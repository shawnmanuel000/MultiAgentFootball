Model: <class 'models.ppo.PPOAgent'>, Dir: BipedalWalkerHardcore-v2
num_envs: 16, state_size: (24,), action_size: (4,), action_space: Box(4,),

import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, DISCOUNT_RATE, NUM_STEPS, ADVANTAGE_DECAY

BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 2					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=torch.scalar_tensor(1), clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) * critic_error.pow(2) * scale
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss.mean())
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage, scale=16*dones.size(0)/len(self.replay_buffer))

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[-3]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 64, Reward: -108.3328 [11.72], Avg: -120.0511 (1.000)
Step: 133, Reward: -107.3103 [6.96], Avg: -117.1611 (1.000)
Step: 209, Reward: -123.2928 [20.57], Avg: -126.0607 (1.000)
Step: 270, Reward: -112.9144 [12.94], Avg: -126.0082 (1.000)
Step: 2270, Reward: -110.6349 [11.06], Avg: -125.1464 (1.000)
Step: 3492, Reward: -108.6471 [8.96], Avg: -123.8895 (1.000)
Step: 5492, Reward: -110.3689 [6.41], Avg: -122.8733 (1.000)
Step: 5553, Reward: -120.8528 [27.57], Avg: -126.0670 (1.000)
Step: 6941, Reward: -125.5196 [32.10], Avg: -129.5724 (1.000)
Step: 7009, Reward: -110.9876 [8.14], Avg: -128.5276 (1.000)
Step: 9009, Reward: -119.0602 [21.75], Avg: -129.6445 (1.000)
Step: 9084, Reward: -105.9453 [6.97], Avg: -128.2504 (1.000)
Step: 9141, Reward: -108.9393 [4.35], Avg: -127.0995 (1.000)
Step: 11141, Reward: -124.5585 [29.91], Avg: -129.0544 (1.000)
Step: 13141, Reward: -131.7138 [35.10], Avg: -131.5719 (1.000)
Step: 13215, Reward: -133.7225 [37.41], Avg: -134.0445 (1.000)
Step: 15215, Reward: -110.2520 [4.94], Avg: -132.9355 (1.000)
Step: 17215, Reward: -101.3540 [10.09], Avg: -131.7416 (1.000)
Step: 19215, Reward: -111.1124 [10.90], Avg: -131.2293 (1.000)
Step: 21215, Reward: -134.5166 [23.40], Avg: -132.5634 (1.000)
Step: 22516, Reward: -146.5840 [9.12], Avg: -133.6654 (1.000)
Step: 24516, Reward: -132.0604 [23.07], Avg: -134.6413 (1.000)
Step: 25159, Reward: -108.6916 [6.51], Avg: -133.7960 (1.000)
Step: 27159, Reward: -121.4744 [17.34], Avg: -134.0050 (1.000)
Step: 29159, Reward: -126.3496 [31.51], Avg: -134.9593 (1.000)
Step: 30303, Reward: -115.9345 [15.14], Avg: -134.8098 (1.000)
Step: 32303, Reward: -140.0413 [29.05], Avg: -136.0795 (1.000)
Step: 34303, Reward: -130.6373 [21.70], Avg: -136.6602 (1.000)
Step: 36303, Reward: -126.2702 [22.37], Avg: -137.0733 (1.000)
Step: 38303, Reward: -129.9230 [25.83], Avg: -137.6959 (1.000)
Step: 39463, Reward: -108.6732 [4.17], Avg: -136.8942 (1.000)
Step: 41463, Reward: -121.4940 [29.96], Avg: -137.3492 (1.000)
Step: 43463, Reward: -112.7997 [17.69], Avg: -137.1414 (1.000)
Step: 44301, Reward: -119.6372 [26.60], Avg: -137.4088 (1.000)
Step: 46301, Reward: -113.3276 [24.79], Avg: -137.4292 (1.000)
Step: 48301, Reward: -134.7429 [30.97], Avg: -138.2150 (1.000)
Step: 50301, Reward: -111.9813 [1.71], Avg: -137.5523 (1.000)
Step: 51622, Reward: -105.8041 [8.39], Avg: -136.9376 (1.000)
Step: 53014, Reward: -103.7867 [24.95], Avg: -136.7273 (1.000)
Step: 55014, Reward: -108.1119 [22.24], Avg: -136.5679 (1.000)
Step: 57014, Reward: -119.6803 [36.73], Avg: -137.0519 (1.000)
Step: 59014, Reward: -104.5736 [3.08], Avg: -136.3520 (1.000)
Step: 61014, Reward: -87.8444 [18.05], Avg: -135.6437 (1.000)
Step: 63014, Reward: -119.2677 [19.08], Avg: -135.7052 (1.000)
Step: 65014, Reward: -97.5803 [4.03], Avg: -134.9474 (1.000)
Step: 67014, Reward: -118.9038 [36.38], Avg: -135.3896 (1.000)
Step: 69014, Reward: -88.7186 [18.55], Avg: -134.7913 (1.000)
Step: 70382, Reward: -102.0373 [21.13], Avg: -134.5490 (1.000)
Step: 72382, Reward: -100.1124 [36.97], Avg: -134.6006 (1.000)
Step: 74184, Reward: -88.3660 [21.20], Avg: -134.1000 (1.000)
Step: 76184, Reward: -115.5440 [17.16], Avg: -134.0727 (1.000)
Step: 78184, Reward: -86.6838 [25.68], Avg: -133.6553 (1.000)
Step: 80184, Reward: -107.9378 [30.58], Avg: -133.7471 (1.000)
Step: 82184, Reward: -117.3169 [20.38], Avg: -133.8203 (1.000)
Step: 84184, Reward: -128.0231 [29.86], Avg: -134.2579 (1.000)
Step: 86081, Reward: -86.3181 [16.44], Avg: -133.6954 (1.000)
Step: 88081, Reward: -108.0360 [13.52], Avg: -133.4824 (1.000)
Step: 90081, Reward: -73.5498 [26.45], Avg: -132.9052 (1.000)
Step: 92081, Reward: -130.3008 [34.30], Avg: -133.4423 (1.000)
Step: 94081, Reward: -111.0333 [40.51], Avg: -133.7440 (1.000)
Step: 96081, Reward: -107.4584 [1.59], Avg: -133.3391 (1.000)
Step: 98081, Reward: -101.2356 [26.26], Avg: -133.2449 (1.000)
Step: 100081, Reward: -101.4058 [44.94], Avg: -133.4529 (1.000)
Step: 100271, Reward: -107.1327 [7.73], Avg: -133.1623 (1.000)
Step: 102271, Reward: -108.6712 [1.75], Avg: -132.8124 (1.000)
Step: 104271, Reward: -100.3028 [19.90], Avg: -132.6215 (1.000)
Step: 105949, Reward: -99.4881 [23.09], Avg: -132.4716 (1.000)
Step: 107949, Reward: -96.5548 [24.97], Avg: -132.3106 (1.000)
Step: 109949, Reward: -100.2052 [14.47], Avg: -132.0551 (1.000)
Step: 111949, Reward: -108.1564 [37.28], Avg: -132.2462 (1.000)
Step: 113949, Reward: -101.7627 [8.44], Avg: -131.9357 (1.000)
Step: 115949, Reward: -77.6444 [34.12], Avg: -131.6555 (1.000)
Step: 116746, Reward: -117.4542 [8.87], Avg: -131.5825 (1.000)
Step: 118746, Reward: -100.2529 [28.89], Avg: -131.5495 (1.000)
Step: 120746, Reward: -87.8740 [43.06], Avg: -131.5412 (1.000)
Step: 122746, Reward: -88.6034 [38.22], Avg: -131.4792 (1.000)
Step: 124746, Reward: -96.1158 [27.21], Avg: -131.3733 (1.000)
Step: 126746, Reward: -92.8730 [15.44], Avg: -131.0777 (1.000)
Step: 128746, Reward: -109.7671 [5.99], Avg: -130.8837 (1.000)
Step: 130746, Reward: -86.1473 [26.71], Avg: -130.6585 (1.000)
Step: 132746, Reward: -83.9095 [20.82], Avg: -130.3384 (1.000)
Step: 134746, Reward: -75.2818 [24.11], Avg: -129.9610 (1.000)
Step: 136746, Reward: -103.1827 [25.62], Avg: -129.9471 (1.000)
Step: 138746, Reward: -100.0275 [26.98], Avg: -129.9121 (1.000)
Step: 140746, Reward: -93.8166 [24.35], Avg: -129.7739 (1.000)
Step: 142746, Reward: -119.5095 [15.86], Avg: -129.8390 (1.000)
Step: 144746, Reward: -105.7456 [5.78], Avg: -129.6285 (1.000)
Step: 146746, Reward: -103.7045 [15.84], Avg: -129.5139 (1.000)
Step: 148746, Reward: -125.7809 [25.95], Avg: -129.7635 (1.000)
Step: 150746, Reward: -87.7842 [24.63], Avg: -129.5707 (1.000)
Step: 151851, Reward: -95.3522 [13.54], Avg: -129.3435 (1.000)
Step: 153166, Reward: -121.1723 [38.93], Avg: -129.6778 (1.000)
Step: 155166, Reward: -79.1949 [36.61], Avg: -129.5286 (1.000)
Step: 157166, Reward: -84.7586 [34.04], Avg: -129.4144 (1.000)
Step: 159166, Reward: -61.2811 [40.41], Avg: -129.1226 (1.000)
Step: 161166, Reward: -90.3353 [17.20], Avg: -128.8977 (1.000)
Step: 163166, Reward: -99.9827 [16.77], Avg: -128.7725 (1.000)
Step: 165166, Reward: -53.8823 [60.29], Avg: -128.6235 (1.000)
Step: 167166, Reward: -79.5591 [28.38], Avg: -128.4145 (1.000)
Step: 169166, Reward: -68.0652 [49.38], Avg: -128.3049 (1.000)
Step: 171166, Reward: -83.5830 [30.01], Avg: -128.1592 (1.000)
Step: 173166, Reward: -83.1467 [32.61], Avg: -128.0376 (1.000)
Step: 175166, Reward: -85.2377 [22.23], Avg: -127.8380 (1.000)
Step: 177166, Reward: -96.8737 [25.61], Avg: -127.7865 (1.000)
Step: 178557, Reward: -74.0347 [33.45], Avg: -127.5932 (1.000)
Step: 180557, Reward: -94.9490 [17.09], Avg: -127.4464 (1.000)
Step: 182557, Reward: -100.5535 [20.71], Avg: -127.3887 (1.000)
Step: 184557, Reward: -78.9841 [47.31], Avg: -127.3785 (1.000)
Step: 186557, Reward: -99.7906 [16.73], Avg: -127.2789 (1.000)
Step: 188557, Reward: -81.5278 [49.16], Avg: -127.3099 (1.000)
Step: 190557, Reward: -106.1083 [7.93], Avg: -127.1903 (1.000)
Step: 192557, Reward: -113.5562 [3.39], Avg: -127.0988 (1.000)
Step: 194557, Reward: -102.7646 [11.05], Avg: -126.9812 (1.000)
Step: 194701, Reward: -88.8377 [30.48], Avg: -126.9141 (1.000)
Step: 196701, Reward: -102.8635 [37.17], Avg: -127.0282 (1.000)
Step: 198701, Reward: -90.6378 [37.82], Avg: -127.0404 (1.000)
Step: 200701, Reward: -97.7171 [37.88], Avg: -127.1136 (1.000)
Step: 202701, Reward: -109.0398 [42.49], Avg: -127.3204 (1.000)
Step: 204701, Reward: -98.5463 [22.30], Avg: -127.2660 (1.000)
Step: 205754, Reward: -75.7376 [35.55], Avg: -127.1329 (1.000)
Step: 207754, Reward: -77.5488 [47.02], Avg: -127.1117 (1.000)
Step: 209754, Reward: -90.1695 [18.55], Avg: -126.9610 (1.000)
Step: 211754, Reward: -104.6029 [13.18], Avg: -126.8863 (1.000)
Step: 213754, Reward: -51.8877 [54.38], Avg: -126.7200 (1.000)
Step: 215754, Reward: -81.8707 [13.99], Avg: -126.4731 (1.000)
Step: 217754, Reward: -97.8641 [14.21], Avg: -126.3588 (1.000)
Step: 219754, Reward: -94.1938 [52.95], Avg: -126.5224 (1.000)
Step: 220067, Reward: -100.6372 [17.88], Avg: -126.4599 (1.000)
Step: 222067, Reward: -80.3175 [42.21], Avg: -126.4294 (1.000)
Step: 222485, Reward: -94.3666 [46.19], Avg: -126.5381 (1.000)
Step: 224485, Reward: -90.6316 [30.67], Avg: -126.4981 (1.000)
Step: 226485, Reward: -75.7131 [27.79], Avg: -126.3239 (1.000)
Step: 228485, Reward: -74.6292 [49.41], Avg: -126.3067 (1.000)
Step: 229435, Reward: -77.4288 [25.00], Avg: -126.1285 (1.000)
Step: 231435, Reward: -75.7915 [38.55], Avg: -126.0412 (1.000)
Step: 233435, Reward: -104.0787 [23.01], Avg: -126.0489 (1.000)
Step: 235435, Reward: -80.2712 [38.24], Avg: -125.9939 (1.000)
Step: 237435, Reward: -89.8543 [21.37], Avg: -125.8868 (1.000)
Step: 239435, Reward: -98.9416 [17.73], Avg: -125.8205 (1.000)
Step: 241435, Reward: -75.4782 [27.88], Avg: -125.6601 (1.000)
Step: 243435, Reward: -86.5985 [43.91], Avg: -125.6945 (1.000)
Step: 245435, Reward: -85.2961 [22.80], Avg: -125.5706 (1.000)
Step: 247435, Reward: -76.1026 [24.13], Avg: -125.3934 (1.000)
Step: 247540, Reward: -93.5314 [29.24], Avg: -125.3752 (1.000)
Step: 247726, Reward: -88.2820 [15.43], Avg: -125.2258 (1.000)
Step: 249726, Reward: -69.7789 [50.40], Avg: -125.1913 (1.000)
Step: 251726, Reward: -44.7746 [60.64], Avg: -125.0567 (1.000)
Step: 253000, Reward: -81.6019 [31.47], Avg: -124.9757 (1.000)
Step: 255000, Reward: -87.2664 [15.86], Avg: -124.8290 (1.000)
Step: 255221, Reward: -96.9069 [8.67], Avg: -124.7007 (1.000)
Step: 255779, Reward: -96.3515 [15.55], Avg: -124.6159 (1.000)
Step: 257779, Reward: -80.5096 [30.97], Avg: -124.5295 (1.000)
Step: 259779, Reward: -121.1269 [68.34], Avg: -124.9540 (1.000)
Step: 260609, Reward: -94.6499 [34.22], Avg: -124.9794 (1.000)
Step: 262609, Reward: -122.1266 [10.72], Avg: -125.0302 (1.000)
Step: 262749, Reward: -67.4079 [66.35], Avg: -125.0861 (1.000)
Step: 264749, Reward: -83.2367 [43.41], Avg: -125.0960 (1.000)
Step: 265451, Reward: -76.2963 [58.99], Avg: -125.1605 (1.000)
Step: 267451, Reward: -96.7830 [61.47], Avg: -125.3686 (1.000)
Step: 269451, Reward: -109.2980 [10.83], Avg: -125.3359 (1.000)
Step: 270561, Reward: -105.3896 [19.55], Avg: -125.3335 (1.000)
Step: 272561, Reward: -79.7396 [41.89], Avg: -125.3106 (1.000)
Step: 273355, Reward: -77.6074 [55.34], Avg: -125.3574 (1.000)
Step: 275355, Reward: -98.6406 [24.84], Avg: -125.3459 (1.000)
Step: 277355, Reward: -88.9150 [27.13], Avg: -125.2896 (1.000)
Step: 277505, Reward: -88.4337 [25.80], Avg: -125.2230 (1.000)
Step: 279505, Reward: -131.8685 [35.10], Avg: -125.4729 (1.000)
Step: 281505, Reward: -93.9415 [44.82], Avg: -125.5520 (1.000)
Step: 283505, Reward: -68.3270 [41.94], Avg: -125.4616 (1.000)
Step: 285505, Reward: -112.2768 [26.22], Avg: -125.5383 (1.000)
Step: 287505, Reward: -91.6599 [18.10], Avg: -125.4460 (1.000)
Step: 289505, Reward: -41.5379 [26.97], Avg: -125.1150 (1.000)
Step: 291256, Reward: -71.5831 [54.19], Avg: -125.1188 (1.000)
Step: 293256, Reward: -76.0364 [33.35], Avg: -125.0284 (1.000)
Step: 295256, Reward: -97.7823 [35.85], Avg: -125.0775 (1.000)
Step: 297256, Reward: -91.9154 [56.29], Avg: -125.2089 (1.000)
Step: 298174, Reward: -65.2346 [33.19], Avg: -125.0576 (1.000)
Step: 300174, Reward: -109.2492 [17.29], Avg: -125.0659 (1.000)
Step: 302174, Reward: -115.9208 [22.88], Avg: -125.1426 (1.000)
Step: 304174, Reward: -123.2133 [9.41], Avg: -125.1841 (1.000)
Step: 306174, Reward: -109.1155 [11.96], Avg: -125.1615 (1.000)
Step: 308174, Reward: -105.0774 [21.31], Avg: -125.1682 (1.000)
Step: 310174, Reward: -108.0414 [17.41], Avg: -125.1698 (1.000)
Step: 312174, Reward: -113.5318 [9.95], Avg: -125.1606 (1.000)
Step: 314174, Reward: -74.5096 [51.08], Avg: -125.1629 (1.000)
Step: 316174, Reward: -109.9505 [21.63], Avg: -125.1974 (1.000)
Step: 318174, Reward: -108.3341 [23.46], Avg: -125.2327 (1.000)
Step: 320174, Reward: -106.1046 [21.52], Avg: -125.2454 (1.000)
Step: 320517, Reward: -102.1499 [24.86], Avg: -125.2547 (1.000)
Step: 322419, Reward: -97.4455 [22.79], Avg: -125.2283 (1.000)
Step: 324419, Reward: -82.0713 [56.63], Avg: -125.2989 (1.000)
Step: 326419, Reward: -108.6928 [17.14], Avg: -125.3017 (1.000)
Step: 328419, Reward: -95.0161 [21.30], Avg: -125.2551 (1.000)
Step: 330419, Reward: -48.5160 [36.98], Avg: -125.0501 (1.000)
Step: 332419, Reward: -108.6828 [32.62], Avg: -125.1335 (1.000)
Step: 333457, Reward: -74.7240 [61.34], Avg: -125.1892 (1.000)
Step: 333647, Reward: -107.1563 [12.52], Avg: -125.1613 (1.000)
Step: 335647, Reward: -110.1294 [7.63], Avg: -125.1239 (1.000)
Step: 337647, Reward: -79.8865 [43.46], Avg: -125.1149 (1.000)
Step: 339647, Reward: -124.3978 [14.28], Avg: -125.1828 (1.000)
Step: 340489, Reward: -123.2776 [18.57], Avg: -125.2657 (1.000)
Step: 342489, Reward: -116.9407 [9.44], Avg: -125.2712 (1.000)
Step: 343057, Reward: -100.4853 [15.54], Avg: -125.2257 (1.000)
Step: 345057, Reward: -122.3083 [10.52], Avg: -125.2629 (1.000)
Step: 347057, Reward: -110.5510 [36.65], Avg: -125.3699 (1.000)
Step: 349057, Reward: -65.0967 [71.47], Avg: -125.4243 (1.000)
Step: 349549, Reward: -73.1804 [29.41], Avg: -125.3140 (1.000)
Step: 351549, Reward: -110.9097 [14.43], Avg: -125.3141 (1.000)
Step: 353549, Reward: -90.2868 [22.14], Avg: -125.2525 (1.000)
Step: 355549, Reward: -79.0180 [35.35], Avg: -125.2006 (1.000)
Step: 357549, Reward: -98.1453 [20.34], Avg: -125.1688 (1.000)
Step: 357691, Reward: -97.0443 [11.53], Avg: -125.0905 (1.000)
Step: 357843, Reward: -86.7522 [33.65], Avg: -125.0685 (1.000)
Step: 359843, Reward: -94.6882 [33.80], Avg: -125.0845 (1.000)
Step: 361843, Reward: -132.5303 [20.57], Avg: -125.2148 (1.000)
Step: 363843, Reward: -101.7726 [37.60], Avg: -125.2804 (1.000)
Step: 365843, Reward: -109.9943 [20.14], Avg: -125.3027 (1.000)
Step: 366076, Reward: -97.9801 [25.90], Avg: -125.2962 (1.000)
Step: 368076, Reward: -108.9095 [40.80], Avg: -125.4077 (1.000)
Step: 370076, Reward: -120.5735 [58.73], Avg: -125.6527 (1.000)
Step: 370236, Reward: -92.5382 [9.91], Avg: -125.5477 (1.000)
Step: 372236, Reward: -75.2952 [61.89], Avg: -125.6001 (1.000)
Step: 372554, Reward: -104.9788 [13.44], Avg: -125.5679 (1.000)
Step: 374554, Reward: -99.2629 [16.07], Avg: -125.5222 (1.000)
Step: 376554, Reward: -70.4236 [26.55], Avg: -125.3953 (1.000)
Step: 378544, Reward: -84.5938 [76.53], Avg: -125.5534 (1.000)
Step: 379000, Reward: -75.6161 [25.25], Avg: -125.4446 (1.000)
Step: 381000, Reward: -83.0610 [32.21], Avg: -125.4000 (1.000)
Step: 383000, Reward: -109.6850 [15.13], Avg: -125.3975 (1.000)
Step: 385000, Reward: -111.7884 [60.56], Avg: -125.6016 (1.000)
Step: 386567, Reward: -74.1303 [65.00], Avg: -125.6602 (1.000)
Step: 388567, Reward: -100.7503 [57.31], Avg: -125.7998 (1.000)
Step: 388767, Reward: -86.7369 [25.74], Avg: -125.7426 (1.000)
Step: 390767, Reward: -113.1369 [63.96], Avg: -125.9621 (1.000)
Step: 392767, Reward: -100.1374 [14.92], Avg: -125.9157 (1.000)
Step: 394767, Reward: -101.0142 [53.94], Avg: -126.0387 (1.000)
Step: 395463, Reward: -59.6695 [65.67], Avg: -126.0358 (1.000)
Step: 397463, Reward: -121.3952 [7.45], Avg: -126.0476 (1.000)
Step: 399463, Reward: -123.0453 [25.75], Avg: -126.1428 (1.000)
Step: 401463, Reward: -97.9575 [6.28], Avg: -126.0515 (1.000)
Step: 401611, Reward: -98.1584 [17.77], Avg: -126.0095 (1.000)
Step: 403611, Reward: -97.4340 [18.24], Avg: -125.9668 (1.000)
Step: 403890, Reward: -114.1809 [46.88], Avg: -126.1112 (1.000)
Step: 405890, Reward: -112.8665 [69.06], Avg: -126.3400 (1.000)
Step: 406776, Reward: -73.8817 [83.70], Avg: -126.4675 (1.000)
Step: 408776, Reward: -113.7578 [13.26], Avg: -126.4697 (1.000)
Step: 410776, Reward: -91.4277 [35.44], Avg: -126.4713 (1.000)
Step: 412776, Reward: -108.3210 [12.93], Avg: -126.4503 (1.000)
Step: 414776, Reward: -105.2314 [43.23], Avg: -126.5387 (1.000)
Step: 416776, Reward: -60.9658 [47.96], Avg: -126.4682 (1.000)
Step: 418776, Reward: -119.6628 [29.64], Avg: -126.5592 (1.000)
Step: 420506, Reward: -121.6799 [31.04], Avg: -126.6630 (1.000)
Step: 422506, Reward: -92.0659 [23.10], Avg: -126.6175 (1.000)
Step: 424506, Reward: -83.0194 [46.99], Avg: -126.6309 (1.000)
Step: 426506, Reward: -96.1875 [32.23], Avg: -126.6379 (1.000)
Step: 428506, Reward: -94.4630 [28.33], Avg: -126.6229 (1.000)
Step: 428675, Reward: -81.3755 [26.10], Avg: -126.5484 (1.000)
Step: 430112, Reward: -99.9040 [16.05], Avg: -126.5073 (1.000)
Step: 430861, Reward: -89.8791 [30.04], Avg: -126.4819 (1.000)
Step: 432861, Reward: -81.5930 [50.34], Avg: -126.5028 (1.000)
Step: 433042, Reward: -118.8435 [10.06], Avg: -126.5120 (1.000)
Step: 435042, Reward: -96.2207 [34.20], Avg: -126.5269 (1.000)
Step: 437042, Reward: -111.7431 [15.30], Avg: -126.5289 (1.000)
Step: 437206, Reward: -127.8383 [25.31], Avg: -126.6297 (1.000)
Step: 439206, Reward: -78.1622 [46.62], Avg: -126.6228 (1.000)
Step: 441206, Reward: -130.4604 [21.07], Avg: -126.7164 (1.000)
Step: 443206, Reward: -62.3550 [29.71], Avg: -126.5866 (1.000)
Step: 444279, Reward: -98.7716 [37.25], Avg: -126.6218 (1.000)
Step: 446279, Reward: -57.2588 [60.71], Avg: -126.5897 (1.000)
Step: 446458, Reward: -90.5571 [27.95], Avg: -126.5597 (1.000)
Step: 448458, Reward: -88.5320 [32.26], Avg: -126.5384 (1.000)
Step: 449719, Reward: -66.6760 [60.24], Avg: -126.5398 (1.000)
Step: 450405, Reward: -110.1659 [25.57], Avg: -126.5735 (1.000)
Step: 451226, Reward: -103.7456 [80.50], Avg: -126.7840 (1.000)
Step: 453226, Reward: -103.2216 [20.74], Avg: -126.7737 (1.000)
Step: 455226, Reward: -64.2789 [40.10], Avg: -126.6926 (1.000)
Step: 457226, Reward: -88.6336 [35.43], Avg: -126.6831 (1.000)
Step: 457363, Reward: -117.3258 [53.84], Avg: -126.8431 (1.000)
Step: 457732, Reward: -90.3081 [16.20], Avg: -126.7702 (1.000)
Step: 458302, Reward: -75.4050 [52.61], Avg: -126.7747 (1.000)
Step: 460302, Reward: -107.7800 [62.90], Avg: -126.9309 (1.000)
Step: 461222, Reward: -92.4178 [34.14], Avg: -126.9296 (1.000)
Step: 463222, Reward: -95.5703 [10.65], Avg: -126.8564 (1.000)
Step: 465222, Reward: -112.6106 [17.59], Avg: -126.8682 (1.000)
Step: 466700, Reward: -86.8290 [62.12], Avg: -126.9457 (1.000)
Step: 468700, Reward: -98.3680 [12.59], Avg: -126.8898 (1.000)
Step: 470700, Reward: -92.2224 [28.13], Avg: -126.8670 (1.000)
Step: 472700, Reward: -115.2872 [17.46], Avg: -126.8874 (1.000)
Step: 473862, Reward: -77.7090 [61.73], Avg: -126.9308 (1.000)
Step: 474493, Reward: -108.8278 [13.59], Avg: -126.9153 (1.000)
Step: 476493, Reward: -136.6497 [33.75], Avg: -127.0647 (1.000)
Step: 476881, Reward: -106.1490 [53.25], Avg: -127.1754 (1.000)
Step: 478356, Reward: -63.5425 [68.56], Avg: -127.1923 (1.000)
Step: 480356, Reward: -103.1258 [26.55], Avg: -127.2007 (1.000)
Step: 482356, Reward: -94.5247 [74.89], Avg: -127.3438 (1.000)
Step: 484356, Reward: -131.3601 [43.27], Avg: -127.5036 (1.000)
Step: 484542, Reward: -92.4222 [67.32], Avg: -127.6121 (1.000)
Step: 486542, Reward: -100.0537 [41.99], Avg: -127.6606 (1.000)
Step: 488542, Reward: -65.3532 [29.33], Avg: -127.5503 (1.000)
Step: 490542, Reward: -107.3331 [21.22], Avg: -127.5536 (1.000)
Step: 492542, Reward: -119.5746 [51.79], Avg: -127.6992 (1.000)
Step: 492757, Reward: -114.6203 [16.59], Avg: -127.7108 (1.000)
Step: 493626, Reward: -61.3925 [42.76], Avg: -127.6330 (1.000)
Step: 495197, Reward: -111.1553 [21.67], Avg: -127.6501 (1.000)
Step: 497197, Reward: -83.1398 [30.45], Avg: -127.6040 (1.000)
Step: 497943, Reward: -77.4554 [56.38], Avg: -127.6244 (1.000)
Step: 498367, Reward: -77.9112 [21.99], Avg: -127.5341 (1.000)
Step: 498686, Reward: -106.0595 [14.33], Avg: -127.5109 (1.000)
Step: 499807, Reward: -100.4429 [38.19], Avg: -127.5469 (1.000)
Step: 501807, Reward: -105.8305 [22.26], Avg: -127.5487 (1.000)
Step: 503807, Reward: -129.0090 [32.74], Avg: -127.6587 (1.000)
Step: 505807, Reward: -84.5089 [18.27], Avg: -127.5789 (1.000)
Step: 506322, Reward: -87.7643 [35.28], Avg: -127.5645 (1.000)
Step: 508322, Reward: -93.3174 [11.13], Avg: -127.4908 (1.000)
Step: 510322, Reward: -76.4866 [38.81], Avg: -127.4521 (1.000)
Step: 512322, Reward: -94.6153 [45.64], Avg: -127.4926 (1.000)
Step: 514322, Reward: -94.3778 [15.85], Avg: -127.4382 (1.000)
Step: 515987, Reward: -77.6583 [49.28], Avg: -127.4366 (1.000)
Step: 516235, Reward: -100.0894 [27.13], Avg: -127.4359 (1.000)
Step: 518235, Reward: -47.1447 [58.58], Avg: -127.3681 (1.000)
Step: 520235, Reward: -59.5872 [80.72], Avg: -127.4084 (1.000)
Step: 520904, Reward: -54.2461 [58.48], Avg: -127.3628 (1.000)
Step: 521086, Reward: -126.9611 [14.51], Avg: -127.4065 (1.000)
Step: 523086, Reward: -84.0607 [45.31], Avg: -127.4125 (1.000)
Step: 525086, Reward: -87.1101 [53.64], Avg: -127.4536 (1.000)
Step: 525362, Reward: -87.8232 [37.35], Avg: -127.4465 (1.000)
Step: 527362, Reward: -86.5742 [25.53], Avg: -127.3996 (1.000)
Step: 529123, Reward: -99.5577 [21.01], Avg: -127.3788 (1.000)
Step: 531123, Reward: -109.4165 [33.33], Avg: -127.4255 (1.000)
Step: 533123, Reward: -103.0284 [26.97], Avg: -127.4333 (1.000)
Step: 535123, Reward: -108.1321 [14.18], Avg: -127.4178 (1.000)
Step: 536435, Reward: -94.7374 [27.92], Avg: -127.4035 (1.000)
Step: 536921, Reward: -115.1142 [23.37], Avg: -127.4367 (1.000)
Step: 538921, Reward: -86.1184 [40.92], Avg: -127.4355 (1.000)
Step: 540921, Reward: -87.7918 [75.46], Avg: -127.5425 (1.000)
Step: 542921, Reward: -94.2257 [36.73], Avg: -127.5526 (1.000)
Step: 544921, Reward: -70.5503 [68.92], Avg: -127.5880 (1.000)
Step: 546921, Reward: -97.8074 [73.45], Avg: -127.7172 (1.000)
Step: 548429, Reward: -105.9598 [19.83], Avg: -127.7115 (1.000)
Step: 550429, Reward: -78.5142 [28.50], Avg: -127.6506 (1.000)
Step: 552429, Reward: -94.8686 [36.62], Avg: -127.6619 (1.000)
Step: 554429, Reward: -81.5744 [13.25], Avg: -127.5658 (1.000)
Step: 555479, Reward: -84.6106 [57.65], Avg: -127.6087 (1.000)
Step: 557479, Reward: -80.8067 [28.62], Avg: -127.5558 (1.000)
Step: 559479, Reward: -115.2063 [48.63], Avg: -127.6610 (1.000)
Step: 560011, Reward: -93.1772 [49.14], Avg: -127.7033 (1.000)
Step: 562011, Reward: -59.2139 [46.78], Avg: -127.6408 (1.000)
Step: 564011, Reward: -99.4796 [21.53], Avg: -127.6217 (1.000)
Step: 565925, Reward: -121.2059 [16.39], Avg: -127.6503 (1.000)
Step: 567425, Reward: -117.9564 [40.13], Avg: -127.7373 (1.000)
Step: 569425, Reward: -100.1408 [16.85], Avg: -127.7067 (1.000)
Step: 571425, Reward: -119.2547 [29.34], Avg: -127.7660 (1.000)
Step: 573425, Reward: -53.2122 [58.41], Avg: -127.7203 (1.000)
Step: 574314, Reward: -86.8100 [63.44], Avg: -127.7839 (1.000)
Step: 576314, Reward: -89.6512 [30.46], Avg: -127.7623 (1.000)
Step: 578314, Reward: -56.0364 [55.39], Avg: -127.7164 (1.000)
Step: 580314, Reward: -127.5816 [33.76], Avg: -127.8106 (1.000)
Step: 581327, Reward: -108.2090 [52.52], Avg: -127.9026 (1.000)
Step: 581460, Reward: -75.6618 [67.52], Avg: -127.9451 (1.000)
Step: 581731, Reward: -104.0109 [40.70], Avg: -127.9917 (1.000)
Step: 582253, Reward: -105.3849 [15.44], Avg: -127.9718 (1.000)
Step: 584253, Reward: -120.2942 [17.54], Avg: -127.9991 (1.000)
Step: 586253, Reward: -118.5321 [52.19], Avg: -128.1168 (1.000)
Step: 587887, Reward: -124.5793 [29.63], Avg: -128.1885 (1.000)
Step: 588365, Reward: -75.2022 [55.07], Avg: -128.1942 (1.000)
Step: 590365, Reward: -99.4856 [38.21], Avg: -128.2201 (1.000)
Step: 592365, Reward: -111.5223 [36.74], Avg: -128.2747 (1.000)
Step: 594365, Reward: -105.8793 [27.51], Avg: -128.2886 (1.000)
Step: 596365, Reward: -43.2478 [74.35], Avg: -128.2597 (1.000)
Step: 598365, Reward: -108.6665 [53.54], Avg: -128.3514 (1.000)
Step: 600365, Reward: -46.7231 [53.51], Avg: -128.2756 (1.000)
Step: 602365, Reward: -90.4952 [42.39], Avg: -128.2880 (1.000)
Step: 604365, Reward: -99.4185 [24.89], Avg: -128.2774 (1.000)
Step: 606365, Reward: -56.7942 [62.71], Avg: -128.2539 (1.000)
Step: 608365, Reward: -40.3627 [64.31], Avg: -128.1910 (1.000)
Step: 610365, Reward: -99.4945 [19.16], Avg: -128.1657 (1.000)
Step: 611833, Reward: -125.2239 [64.91], Avg: -128.3300 (1.000)
Step: 612626, Reward: -116.3935 [13.73], Avg: -128.3348 (1.000)
Step: 614626, Reward: -64.2941 [51.08], Avg: -128.3006 (1.000)
Step: 616626, Reward: -96.2544 [24.61], Avg: -128.2810 (1.000)
Step: 618626, Reward: -99.8038 [20.52], Avg: -128.2601 (1.000)
Step: 620626, Reward: -103.8609 [76.49], Avg: -128.3965 (1.000)
Step: 621100, Reward: -104.6927 [30.85], Avg: -128.4151 (1.000)
Step: 623100, Reward: -115.6298 [29.99], Avg: -128.4599 (1.000)
Step: 625100, Reward: -71.2101 [62.21], Avg: -128.4728 (1.000)
Step: 627100, Reward: -99.3881 [32.19], Avg: -128.4809 (1.000)
Step: 629100, Reward: -116.3617 [20.54], Avg: -128.5026 (1.000)
Step: 631100, Reward: -81.4529 [22.66], Avg: -128.4398 (1.000)
Step: 631699, Reward: -106.0530 [60.94], Avg: -128.5389 (1.000)
Step: 633699, Reward: -90.6973 [41.27], Avg: -128.5477 (1.000)
Step: 635699, Reward: -89.5891 [41.37], Avg: -128.5538 (1.000)
Step: 636746, Reward: -79.2647 [41.76], Avg: -128.5346 (1.000)
Step: 638746, Reward: -100.1573 [34.09], Avg: -128.5492 (1.000)
Step: 640746, Reward: -110.5939 [38.74], Avg: -128.6019 (1.000)
Step: 642095, Reward: -128.3840 [51.30], Avg: -128.7313 (1.000)
Step: 644095, Reward: -67.1767 [47.76], Avg: -128.6964 (1.000)
Step: 644437, Reward: -91.7045 [109.87], Avg: -128.8800 (1.000)
Step: 646437, Reward: -106.2683 [52.67], Avg: -128.9555 (1.000)
Step: 648437, Reward: -106.5772 [22.42], Avg: -128.9556 (1.000)
Step: 650437, Reward: -104.6347 [15.29], Avg: -128.9331 (1.000)
Step: 652437, Reward: -65.4569 [68.10], Avg: -128.9446 (1.000)
Step: 653200, Reward: -77.1923 [28.21], Avg: -128.8860 (1.000)
Step: 655200, Reward: -88.4742 [19.68], Avg: -128.8346 (1.000)
Step: 657200, Reward: -105.2560 [24.63], Avg: -128.8372 (1.000)
Step: 659200, Reward: -124.7554 [48.74], Avg: -128.9474 (1.000)
Step: 659700, Reward: -65.0203 [79.03], Avg: -128.9846 (1.000)
Step: 660975, Reward: -92.6383 [50.62], Avg: -129.0197 (1.000)
Step: 662327, Reward: -68.3738 [71.51], Avg: -129.0463 (1.000)
Step: 664327, Reward: -94.7810 [36.99], Avg: -129.0530 (1.000)
Step: 666327, Reward: -114.0205 [45.93], Avg: -129.1283 (1.000)
Step: 666491, Reward: -67.5510 [42.63], Avg: -129.0822 (1.000)
Step: 666770, Reward: -45.7207 [71.71], Avg: -129.0540 (1.000)
Step: 668770, Reward: -77.1548 [88.37], Avg: -129.1423 (1.000)
Step: 669085, Reward: -82.6240 [82.96], Avg: -129.2303 (1.000)
Step: 671085, Reward: -102.7188 [24.12], Avg: -129.2245 (1.000)
Step: 671321, Reward: -110.3525 [28.79], Avg: -129.2484 (1.000)
Step: 672360, Reward: -96.0187 [36.10], Avg: -129.2553 (1.000)
Step: 674360, Reward: -107.3168 [45.15], Avg: -129.3108 (1.000)
Step: 676360, Reward: -70.6092 [31.70], Avg: -129.2464 (1.000)
Step: 676603, Reward: -125.1987 [22.02], Avg: -129.2891 (1.000)
Step: 678603, Reward: -115.2724 [24.16], Avg: -129.3132 (1.000)
Step: 679023, Reward: -100.2837 [34.77], Avg: -129.3268 (1.000)
Step: 681023, Reward: -98.2417 [27.90], Avg: -129.3193 (1.000)
Step: 683023, Reward: -97.2077 [21.75], Avg: -129.2948 (1.000)
Step: 685023, Reward: -103.9453 [36.04], Avg: -129.3200 (1.000)
Step: 685219, Reward: -123.5576 [17.71], Avg: -129.3480 (1.000)
Step: 687219, Reward: -95.7538 [22.89], Avg: -129.3230 (1.000)
Step: 689219, Reward: -97.4528 [29.04], Avg: -129.3164 (1.000)
Step: 689461, Reward: -102.7499 [17.94], Avg: -129.2962 (1.000)
Step: 690340, Reward: -78.9274 [77.77], Avg: -129.3600 (1.000)
Step: 692340, Reward: -106.2723 [29.56], Avg: -129.3750 (1.000)
Step: 692551, Reward: -103.4628 [13.79], Avg: -129.3469 (1.000)
Step: 694351, Reward: -116.8189 [34.98], Avg: -129.3988 (1.000)
Step: 696351, Reward: -117.9292 [12.93], Avg: -129.4022 (1.000)
Step: 696962, Reward: -112.2695 [23.00], Avg: -129.4157 (1.000)
Step: 698962, Reward: -121.7515 [21.63], Avg: -129.4477 (1.000)
Step: 700962, Reward: -115.0114 [33.96], Avg: -129.4924 (1.000)
Step: 701114, Reward: -79.6370 [57.74], Avg: -129.5104 (1.000)
Step: 701432, Reward: -121.7553 [64.01], Avg: -129.6385 (1.000)
Step: 703432, Reward: -89.7772 [42.03], Avg: -129.6434 (1.000)
Step: 705432, Reward: -101.4200 [31.09], Avg: -129.6499 (1.000)
Step: 705965, Reward: -87.0848 [53.88], Avg: -129.6755 (1.000)
Step: 707261, Reward: -102.6648 [78.81], Avg: -129.7924 (1.000)
Step: 708088, Reward: -107.2056 [28.86], Avg: -129.8066 (1.000)
Step: 710088, Reward: -96.9344 [9.31], Avg: -129.7536 (1.000)
Step: 710754, Reward: -96.2873 [46.31], Avg: -129.7824 (1.000)
Step: 711076, Reward: -113.5923 [31.30], Avg: -129.8162 (1.000)
Step: 713076, Reward: -103.0173 [42.93], Avg: -129.8522 (1.000)
Step: 713375, Reward: -109.7395 [15.93], Avg: -129.8429 (1.000)
Step: 713761, Reward: -116.4730 [29.02], Avg: -129.8777 (1.000)
Step: 714156, Reward: -125.7164 [21.18], Avg: -129.9154 (1.000)
Step: 714933, Reward: -102.2426 [24.99], Avg: -129.9095 (1.000)
Step: 716933, Reward: -123.1252 [12.78], Avg: -129.9227 (1.000)
Step: 718933, Reward: -92.4010 [39.99], Avg: -129.9282 (1.000)
Step: 720933, Reward: -128.9451 [15.98], Avg: -129.9611 (1.000)
Step: 722933, Reward: -130.7738 [32.41], Avg: -130.0340 (1.000)
Step: 724122, Reward: -120.6270 [12.90], Avg: -130.0416 (1.000)
Step: 725048, Reward: -107.9466 [30.10], Avg: -130.0591 (1.000)
Step: 727048, Reward: -99.4753 [29.22], Avg: -130.0561 (1.000)
Step: 727231, Reward: -142.4241 [39.54], Avg: -130.1690 (1.000)
Step: 729231, Reward: -128.4884 [36.00], Avg: -130.2434 (1.000)
Step: 731231, Reward: -74.1926 [36.92], Avg: -130.2020 (1.000)
Step: 731383, Reward: -77.3450 [72.22], Avg: -130.2438 (1.000)
Step: 733383, Reward: -87.9633 [35.41], Avg: -130.2290 (1.000)
Step: 735383, Reward: -103.1563 [27.75], Avg: -130.2305 (1.000)
Step: 735543, Reward: -122.8236 [38.54], Avg: -130.2973 (1.000)
Step: 737261, Reward: -98.7851 [34.70], Avg: -130.3041 (1.000)
Step: 739261, Reward: -124.2145 [55.82], Avg: -130.4104 (1.000)
Step: 741261, Reward: -51.6636 [63.25], Avg: -130.3773 (1.000)
Step: 743261, Reward: -116.2959 [33.40], Avg: -130.4184 (1.000)
Step: 745109, Reward: -135.9096 [40.01], Avg: -130.5150 (1.000)
Step: 745341, Reward: -114.3851 [41.59], Avg: -130.5690 (1.000)
Step: 747341, Reward: -134.1746 [7.47], Avg: -130.5924 (1.000)
Step: 749341, Reward: -121.6237 [25.85], Avg: -130.6280 (1.000)
Step: 750125, Reward: -124.8571 [20.95], Avg: -130.6599 (1.000)
Step: 752125, Reward: -92.5797 [31.72], Avg: -130.6466 (1.000)
Step: 754125, Reward: -116.9423 [25.97], Avg: -130.6723 (1.000)
Step: 755513, Reward: -133.8544 [37.13], Avg: -130.7566 (1.000)
Step: 756827, Reward: -106.2754 [49.66], Avg: -130.8092 (1.000)
Step: 758827, Reward: -131.4155 [31.39], Avg: -130.8759 (1.000)
Step: 759583, Reward: -128.0101 [21.12], Avg: -130.9138 (1.000)
Step: 761583, Reward: -98.9726 [25.27], Avg: -130.9000 (1.000)
Step: 761782, Reward: -124.7965 [57.94], Avg: -131.0073 (1.000)
Step: 763782, Reward: -105.7097 [61.07], Avg: -131.0812 (1.000)
Step: 765782, Reward: -96.7923 [51.18], Avg: -131.1160 (1.000)
Step: 767782, Reward: -124.5696 [17.38], Avg: -131.1383 (1.000)
Step: 769782, Reward: -126.1914 [36.49], Avg: -131.2031 (1.000)
Step: 771016, Reward: -116.9024 [17.50], Avg: -131.2096 (1.000)
Step: 771831, Reward: -127.5070 [16.34], Avg: -131.2355 (1.000)
Step: 773831, Reward: -126.3947 [32.91], Avg: -131.2928 (1.000)
Step: 775831, Reward: -100.1732 [28.58], Avg: -131.2876 (1.000)
Step: 776120, Reward: -110.3294 [79.36], Avg: -131.4063 (1.000)
Step: 776672, Reward: -113.2612 [33.36], Avg: -131.4371 (1.000)
Step: 778672, Reward: -72.1894 [57.23], Avg: -131.4330 (1.000)
Step: 778855, Reward: -117.8539 [29.40], Avg: -131.4650 (1.000)
Step: 780855, Reward: -115.6600 [33.54], Avg: -131.5007 (1.000)
Step: 781282, Reward: -67.1095 [48.67], Avg: -131.4691 (1.000)
Step: 783282, Reward: -97.1194 [68.47], Avg: -131.5376 (1.000)
Step: 785282, Reward: -117.1206 [38.59], Avg: -131.5861 (1.000)
Step: 787282, Reward: -105.9620 [79.27], Avg: -131.6934 (1.000)
Step: 787463, Reward: -111.3359 [18.34], Avg: -131.6893 (1.000)
Step: 788301, Reward: -93.2985 [33.19], Avg: -131.6790 (1.000)
Step: 790301, Reward: -119.3015 [48.90], Avg: -131.7516 (1.000)
Step: 791248, Reward: -67.9814 [72.48], Avg: -131.7689 (1.000)
Step: 793248, Reward: -110.0386 [28.06], Avg: -131.7814 (1.000)
Step: 794208, Reward: -105.2684 [29.22], Avg: -131.7868 (1.000)
Step: 796208, Reward: -89.6994 [37.67], Avg: -131.7781 (1.000)
Step: 798208, Reward: -128.5833 [34.23], Avg: -131.8392 (1.000)
Step: 798847, Reward: -85.8960 [50.41], Avg: -131.8479 (1.000)
Step: 800847, Reward: -109.9332 [10.16], Avg: -131.8249 (1.000)
Step: 802847, Reward: -106.4967 [41.81], Avg: -131.8571 (1.000)
Step: 803873, Reward: -105.7908 [15.26], Avg: -131.8360 (1.000)
Step: 805873, Reward: -129.3859 [34.04], Avg: -131.8976 (1.000)
Step: 806673, Reward: -97.2615 [38.88], Avg: -131.9059 (1.000)
Step: 807215, Reward: -93.0095 [41.22], Avg: -131.9104 (1.000)
Step: 807808, Reward: -33.5531 [92.35], Avg: -131.8987 (1.000)
Step: 809808, Reward: -117.4016 [22.42], Avg: -131.9141 (1.000)
Step: 809937, Reward: -45.9572 [74.17], Avg: -131.8913 (1.000)
Step: 811937, Reward: -72.5434 [14.78], Avg: -131.8054 (1.000)
Step: 813937, Reward: -99.3904 [41.56], Avg: -131.8230 (1.000)
Step: 814236, Reward: -68.3697 [98.56], Avg: -131.8904 (1.000)
Step: 814581, Reward: -102.7133 [25.82], Avg: -131.8840 (1.000)
Step: 815906, Reward: -60.4416 [16.87], Avg: -131.7796 (1.000)
Step: 817699, Reward: -75.7698 [51.83], Avg: -131.7717 (1.000)
Step: 817885, Reward: -68.3725 [40.24], Avg: -131.7276 (1.000)
Step: 819885, Reward: -122.3804 [31.29], Avg: -131.7693 (1.000)
Step: 821885, Reward: -140.4800 [51.34], Avg: -131.8832 (1.000)
Step: 823885, Reward: -116.1118 [35.52], Avg: -131.9206 (1.000)
Step: 825189, Reward: -86.0546 [50.90], Avg: -131.9301 (1.000)
Step: 827189, Reward: -101.4220 [27.36], Avg: -131.9242 (1.000)
Step: 828557, Reward: -99.8440 [7.31], Avg: -131.8775 (1.000)
Step: 830557, Reward: -141.3957 [11.49], Avg: -131.9170 (1.000)
Step: 831745, Reward: -112.1318 [30.39], Avg: -131.9369 (1.000)
Step: 833745, Reward: -38.4710 [79.15], Avg: -131.9101 (1.000)
Step: 835745, Reward: -73.9293 [60.66], Avg: -131.9151 (1.000)
Step: 837745, Reward: -97.8110 [78.62], Avg: -131.9982 (1.000)
Step: 839745, Reward: -80.4214 [49.02], Avg: -131.9934 (1.000)
Step: 841745, Reward: -102.0440 [22.65], Avg: -131.9798 (1.000)
Step: 842483, Reward: -123.2652 [26.74], Avg: -132.0133 (1.000)
Step: 843827, Reward: -93.7327 [13.79], Avg: -131.9679 (1.000)
Step: 845827, Reward: -101.5175 [81.17], Avg: -132.0617 (1.000)
Step: 845945, Reward: -87.6606 [45.24], Avg: -132.0632 (1.000)
Step: 847945, Reward: -83.2538 [19.81], Avg: -132.0098 (1.000)
Step: 849945, Reward: -102.2089 [42.98], Avg: -132.0341 (1.000)
Step: 850241, Reward: -105.6228 [69.44], Avg: -132.1130 (1.000)
Step: 851181, Reward: -92.0038 [36.89], Avg: -132.1071 (1.000)
Step: 853181, Reward: -138.6417 [42.79], Avg: -132.1973 (1.000)
Step: 855181, Reward: -127.6253 [40.32], Avg: -132.2625 (1.000)
Step: 857181, Reward: -98.1070 [37.63], Avg: -132.2688 (1.000)
Step: 857291, Reward: -99.9295 [57.25], Avg: -132.3141 (1.000)
Step: 859291, Reward: -133.9180 [21.69], Avg: -132.3564 (1.000)
Step: 859748, Reward: -119.5378 [16.92], Avg: -132.3638 (1.000)
Step: 861748, Reward: -77.0426 [49.64], Avg: -132.3536 (1.000)
Step: 862846, Reward: -94.0674 [53.80], Avg: -132.3816 (1.000)
Step: 864846, Reward: -118.4492 [20.96], Avg: -132.3942 (1.000)
Step: 866846, Reward: -144.0442 [14.21], Avg: -132.4407 (1.000)
Step: 868846, Reward: -94.0124 [65.60], Avg: -132.4895 (1.000)
Step: 869246, Reward: -104.5958 [47.35], Avg: -132.5244 (1.000)
Step: 871246, Reward: -113.8857 [48.62], Avg: -132.5780 (1.000)
Step: 873246, Reward: -105.5316 [29.40], Avg: -132.5822 (1.000)
Step: 875246, Reward: -134.9658 [22.96], Avg: -132.6274 (1.000)
Step: 877246, Reward: -127.0448 [29.24], Avg: -132.6695 (1.000)
Step: 878469, Reward: -41.4005 [86.70], Avg: -132.6614 (1.000)
Step: 880469, Reward: -112.3461 [63.31], Avg: -132.7376 (1.000)
Step: 882469, Reward: -92.7129 [28.98], Avg: -132.7181 (1.000)
Step: 884469, Reward: -132.2336 [83.81], Avg: -132.8653 (1.000)
Step: 886469, Reward: -105.2374 [37.70], Avg: -132.8831 (1.000)
Step: 888469, Reward: -115.1998 [20.55], Avg: -132.8881 (1.000)
Step: 890469, Reward: -114.2843 [38.18], Avg: -132.9225 (1.000)
Step: 892469, Reward: -53.8198 [50.04], Avg: -132.8715 (1.000)
Step: 893860, Reward: -114.7744 [44.81], Avg: -132.9183 (1.000)
Step: 893981, Reward: -113.4336 [25.71], Avg: -132.9292 (1.000)
Step: 894843, Reward: -100.2421 [36.75], Avg: -132.9363 (1.000)
Step: 896843, Reward: -119.4529 [24.05], Avg: -132.9547 (1.000)
Step: 898843, Reward: -112.1735 [25.01], Avg: -132.9620 (1.000)
Step: 900843, Reward: -132.2510 [63.13], Avg: -133.0704 (1.000)
Step: 902843, Reward: -128.6566 [38.17], Avg: -133.1289 (1.000)
Step: 904843, Reward: -64.1491 [72.28], Avg: -133.1346 (1.000)
Step: 906843, Reward: -129.3241 [22.88], Avg: -133.1676 (1.000)
Step: 908843, Reward: -87.9248 [58.52], Avg: -133.1904 (1.000)
Step: 910843, Reward: -130.5947 [20.21], Avg: -133.2208 (1.000)
Step: 911135, Reward: -90.1802 [29.40], Avg: -133.1973 (1.000)
Step: 911261, Reward: -88.9667 [42.35], Avg: -133.1941 (1.000)
Step: 912128, Reward: -113.0260 [40.06], Avg: -133.2282 (1.000)
Step: 914128, Reward: -125.0353 [16.92], Avg: -133.2431 (1.000)
Step: 916128, Reward: -127.7851 [30.96], Avg: -133.2866 (1.000)
Step: 917252, Reward: -113.3689 [24.54], Avg: -133.2945 (1.000)
Step: 919252, Reward: -130.0807 [38.13], Avg: -133.3538 (1.000)
Step: 919462, Reward: -122.1997 [18.49], Avg: -133.3663 (1.000)
Step: 920472, Reward: -120.4770 [32.04], Avg: -133.3988 (1.000)
Step: 921200, Reward: -121.6308 [23.87], Avg: -133.4192 (1.000)
Step: 921547, Reward: -86.3716 [41.94], Avg: -133.4106 (1.000)
Step: 923547, Reward: -106.0822 [49.20], Avg: -133.4475 (1.000)
Step: 925547, Reward: -122.3098 [28.54], Avg: -133.4768 (1.000)
Step: 926709, Reward: -112.4805 [56.02], Avg: -133.5356 (1.000)
Step: 928673, Reward: -107.2911 [28.69], Avg: -133.5397 (1.000)
Step: 930673, Reward: -74.2564 [61.20], Avg: -133.5429 (1.000)
Step: 932673, Reward: -139.7614 [53.41], Avg: -133.6427 (1.000)
Step: 934673, Reward: -89.0684 [34.67], Avg: -133.6261 (1.000)
Step: 936673, Reward: -110.3548 [39.86], Avg: -133.6538 (1.000)
Step: 936861, Reward: -95.3840 [30.64], Avg: -133.6411 (1.000)
Step: 938861, Reward: -140.6652 [30.12], Avg: -133.7028 (1.000)
Step: 940861, Reward: -100.8692 [51.62], Avg: -133.7340 (1.000)
Step: 942861, Reward: -119.2386 [25.61], Avg: -133.7524 (1.000)
Step: 944392, Reward: -72.3752 [65.29], Avg: -133.7588 (1.000)
Step: 944970, Reward: -138.1011 [47.62], Avg: -133.8446 (1.000)
Step: 946970, Reward: -112.1256 [19.13], Avg: -133.8403 (1.000)
Step: 947211, Reward: -117.6303 [21.61], Avg: -133.8492 (1.000)
Step: 949211, Reward: -82.3599 [46.17], Avg: -133.8404 (1.000)
Step: 951211, Reward: -109.8548 [30.32], Avg: -133.8508 (1.000)
Step: 953211, Reward: -119.0537 [39.71], Avg: -133.8916 (1.000)
Step: 955211, Reward: -93.0163 [20.58], Avg: -133.8584 (1.000)
Step: 955662, Reward: -88.1897 [23.01], Avg: -133.8214 (1.000)
Step: 957662, Reward: -68.2726 [89.93], Avg: -133.8612 (1.000)
Step: 959662, Reward: -95.1641 [17.66], Avg: -133.8269 (1.000)
Step: 961662, Reward: -69.4162 [46.02], Avg: -133.7971 (1.000)
Step: 963662, Reward: -136.0137 [30.12], Avg: -133.8495 (1.000)
Step: 964170, Reward: -29.4156 [55.70], Avg: -133.7706 (1.000)
Step: 966170, Reward: -90.4480 [65.56], Avg: -133.8066 (1.000)
Step: 966416, Reward: -116.8938 [23.84], Avg: -133.8177 (1.000)
Step: 968416, Reward: -91.8716 [52.75], Avg: -133.8351 (1.000)
Step: 968839, Reward: -111.9101 [29.73], Avg: -133.8477 (1.000)
Step: 969165, Reward: -90.2131 [41.44], Avg: -133.8442 (1.000)
Step: 970551, Reward: -99.3942 [47.35], Avg: -133.8648 (1.000)
Step: 970773, Reward: -107.5346 [28.29], Avg: -133.8680 (1.000)
Step: 970939, Reward: -94.4819 [33.47], Avg: -133.8585 (1.000)
Step: 972939, Reward: -122.6231 [28.49], Avg: -133.8860 (1.000)
Step: 973200, Reward: -119.3672 [20.14], Avg: -133.8950 (1.000)
Step: 975200, Reward: -104.8392 [8.84], Avg: -133.8629 (1.000)
Step: 977200, Reward: -85.7828 [55.89], Avg: -133.8753 (1.000)
Step: 979200, Reward: -91.4110 [11.08], Avg: -133.8255 (1.000)
Step: 981200, Reward: -108.0994 [19.16], Avg: -133.8151 (1.000)
Step: 982397, Reward: -112.9999 [31.58], Avg: -133.8321 (1.000)
Step: 984397, Reward: -131.5977 [30.02], Avg: -133.8760 (1.000)
Step: 984861, Reward: -122.5055 [56.75], Avg: -133.9474 (1.000)
Step: 985012, Reward: -120.8460 [46.97], Avg: -134.0007 (1.000)
Step: 985832, Reward: -88.6357 [15.21], Avg: -133.9533 (1.000)
Step: 987748, Reward: -78.6206 [79.35], Avg: -133.9910 (1.000)
Step: 989360, Reward: -105.9343 [38.79], Avg: -134.0078 (1.000)
Step: 989707, Reward: -103.3848 [29.91], Avg: -134.0067 (1.000)
Step: 991707, Reward: -98.1016 [53.89], Avg: -134.0347 (1.000)
Step: 993556, Reward: -119.4672 [50.94], Avg: -134.0914 (1.000)
Step: 994065, Reward: -116.7776 [30.95], Avg: -134.1126 (1.000)
Step: 995336, Reward: -91.4655 [5.77], Avg: -134.0553 (1.000)
Step: 997336, Reward: -50.2245 [95.17], Avg: -134.0729 (1.000)
Step: 999336, Reward: -83.8175 [9.55], Avg: -134.0099 (1.000)
