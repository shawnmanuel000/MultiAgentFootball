Model: <class 'models.ppo.PPOAgent'>, Dir: BipedalWalkerHardcore-v2
num_envs: 16, state_size: (24,), action_size: (4,), action_space: Box(4,),

import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, DISCOUNT_RATE, NUM_STEPS, ADVANTAGE_DECAY

BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 2					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=torch.scalar_tensor(1), clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) * critic_error.pow(2) * scale
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss.mean())
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage, scale=16*dones.size(0)/len(self.replay_buffer))

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[7]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 51, Reward: -108.1448 [5.66], Avg: -113.8023 (1.000)
Step: 122, Reward: -104.1948 [4.54], Avg: -111.2705 (1.000)
Step: 2122, Reward: -104.4050 [7.31], Avg: -111.4189 (1.000)
Step: 2201, Reward: -107.6047 [8.96], Avg: -112.7056 (1.000)
Step: 4201, Reward: -119.5231 [32.90], Avg: -120.6483 (1.000)
Step: 6201, Reward: -108.5099 [6.25], Avg: -119.6666 (1.000)
Step: 8201, Reward: -107.6680 [6.51], Avg: -118.8826 (1.000)
Step: 10201, Reward: -107.7576 [7.09], Avg: -118.3786 (1.000)
Step: 12201, Reward: -122.4061 [25.76], Avg: -121.6881 (1.000)
Step: 12282, Reward: -129.8076 [27.15], Avg: -125.2154 (1.000)
Step: 12334, Reward: -114.0529 [6.86], Avg: -124.8239 (1.000)
Step: 12390, Reward: -131.0053 [35.47], Avg: -128.2946 (1.000)
Step: 12474, Reward: -132.0150 [36.21], Avg: -131.3665 (1.000)
Step: 14474, Reward: -121.0569 [20.70], Avg: -132.1085 (1.000)
Step: 16100, Reward: -133.1206 [27.77], Avg: -134.0273 (1.000)
Step: 18100, Reward: -113.9776 [4.11], Avg: -133.0311 (1.000)
Step: 20100, Reward: -113.3241 [4.49], Avg: -132.1360 (1.000)
Step: 22100, Reward: -123.6002 [26.65], Avg: -133.1425 (1.000)
Step: 24100, Reward: -129.0863 [36.94], Avg: -134.8734 (1.000)
Step: 26100, Reward: -123.6858 [14.42], Avg: -135.0351 (1.000)
Step: 28100, Reward: -122.3410 [21.19], Avg: -135.4396 (1.000)
Step: 30100, Reward: -118.0124 [24.73], Avg: -135.7714 (1.000)
Step: 32100, Reward: -117.0704 [19.04], Avg: -135.7860 (1.000)
Step: 34100, Reward: -104.9147 [22.22], Avg: -135.4257 (1.000)
Step: 35138, Reward: -132.9395 [21.63], Avg: -136.1914 (1.000)
Step: 36199, Reward: -125.7639 [21.88], Avg: -136.6320 (1.000)
Step: 37167, Reward: -127.5648 [20.64], Avg: -137.0606 (1.000)
Step: 39167, Reward: -106.9646 [23.57], Avg: -136.8276 (1.000)
Step: 39225, Reward: -128.2218 [21.85], Avg: -137.2844 (1.000)
Step: 41225, Reward: -110.5346 [16.97], Avg: -136.9582 (1.000)
Step: 43225, Reward: -103.5047 [13.06], Avg: -136.3005 (1.000)
Step: 45225, Reward: -103.1834 [24.83], Avg: -136.0415 (1.000)
Step: 47225, Reward: -116.9728 [26.55], Avg: -136.2682 (1.000)
Step: 47904, Reward: -118.8827 [32.55], Avg: -136.7142 (1.000)
Step: 49904, Reward: -105.9570 [17.30], Avg: -136.3296 (1.000)
Step: 50856, Reward: -131.2365 [42.35], Avg: -137.3645 (1.000)
Step: 52856, Reward: -114.0212 [32.89], Avg: -137.6226 (1.000)
Step: 52921, Reward: -125.6151 [17.44], Avg: -137.7656 (1.000)
Step: 54921, Reward: -121.3033 [33.67], Avg: -138.2067 (1.000)
Step: 55776, Reward: -130.1723 [20.44], Avg: -138.5168 (1.000)
Step: 57776, Reward: -119.4348 [25.18], Avg: -138.6657 (1.000)
Step: 58568, Reward: -114.5763 [43.00], Avg: -139.1160 (1.000)
Step: 60568, Reward: -99.2587 [14.27], Avg: -138.5208 (1.000)
Step: 62568, Reward: -105.5808 [8.34], Avg: -137.9618 (1.000)
Step: 64568, Reward: -110.3062 [30.20], Avg: -138.0183 (1.000)
Step: 66568, Reward: -121.3587 [25.97], Avg: -138.2207 (1.000)
Step: 67933, Reward: -116.8907 [27.94], Avg: -138.3613 (1.000)
Step: 69933, Reward: -108.1605 [9.07], Avg: -137.9211 (1.000)
Step: 71933, Reward: -128.3796 [17.67], Avg: -138.0871 (1.000)
Step: 73933, Reward: -115.5289 [22.22], Avg: -138.0803 (1.000)
Step: 75933, Reward: -128.8400 [54.15], Avg: -138.9610 (1.000)
Step: 77933, Reward: -101.2962 [22.08], Avg: -138.6612 (1.000)
Step: 79933, Reward: -99.7214 [18.60], Avg: -138.2775 (1.000)
Step: 81933, Reward: -101.5239 [17.69], Avg: -137.9245 (1.000)
Step: 83933, Reward: -105.3457 [17.43], Avg: -137.6491 (1.000)
Step: 85933, Reward: -120.3332 [14.81], Avg: -137.6043 (1.000)
Step: 87933, Reward: -108.1455 [9.01], Avg: -137.2455 (1.000)
Step: 89933, Reward: -123.9130 [21.05], Avg: -137.3786 (1.000)
Step: 90816, Reward: -112.7253 [12.99], Avg: -137.1809 (1.000)
Step: 92816, Reward: -82.0718 [23.15], Avg: -136.6482 (1.000)
Step: 94816, Reward: -110.2697 [2.68], Avg: -136.2597 (1.000)
Step: 95261, Reward: -129.4751 [17.08], Avg: -136.4258 (1.000)
Step: 97261, Reward: -122.5631 [34.40], Avg: -136.7518 (1.000)
Step: 99261, Reward: -104.0424 [37.01], Avg: -136.8189 (1.000)
Step: 101261, Reward: -99.2751 [34.08], Avg: -136.7656 (1.000)
Step: 103261, Reward: -114.3331 [5.60], Avg: -136.5105 (1.000)
Step: 104070, Reward: -106.6292 [25.15], Avg: -136.4399 (1.000)
Step: 106070, Reward: -106.8247 [9.80], Avg: -136.1485 (1.000)
Step: 108070, Reward: -111.9223 [8.01], Avg: -135.9135 (1.000)
Step: 110070, Reward: -103.4628 [18.11], Avg: -135.7087 (1.000)
Step: 112070, Reward: -89.6740 [20.59], Avg: -135.3503 (1.000)
Step: 114070, Reward: -118.6171 [8.15], Avg: -135.2310 (1.000)
Step: 116070, Reward: -100.1384 [33.32], Avg: -135.2067 (1.000)
Step: 118070, Reward: -112.8704 [5.51], Avg: -134.9794 (1.000)
Step: 120070, Reward: -104.8245 [16.48], Avg: -134.7971 (1.000)
Step: 122070, Reward: -78.3485 [30.58], Avg: -134.4567 (1.000)
Step: 124070, Reward: -105.3437 [10.28], Avg: -134.2120 (1.000)
Step: 126070, Reward: -104.1594 [22.11], Avg: -134.1103 (1.000)
Step: 127618, Reward: -85.1802 [31.33], Avg: -133.8874 (1.000)
Step: 129618, Reward: -99.2397 [25.92], Avg: -133.7783 (1.000)
Step: 129730, Reward: -126.2959 [26.82], Avg: -134.0170 (1.000)
Step: 131222, Reward: -101.9499 [33.92], Avg: -134.0397 (1.000)
Step: 133222, Reward: -134.4639 [27.41], Avg: -134.3750 (1.000)
Step: 135222, Reward: -82.3946 [46.48], Avg: -134.3095 (1.000)
Step: 137222, Reward: -115.1572 [1.22], Avg: -134.0986 (1.000)
Step: 137307, Reward: -114.7678 [3.11], Avg: -133.9099 (1.000)
Step: 139307, Reward: -101.5966 [14.57], Avg: -133.7059 (1.000)
Step: 141307, Reward: -135.7307 [29.53], Avg: -134.0645 (1.000)
Step: 143307, Reward: -107.7613 [17.05], Avg: -133.9605 (1.000)
Step: 145307, Reward: -111.8031 [4.19], Avg: -133.7609 (1.000)
Step: 147307, Reward: -103.1213 [18.02], Avg: -133.6222 (1.000)
Step: 149307, Reward: -126.5936 [27.32], Avg: -133.8429 (1.000)
Step: 151307, Reward: -108.9306 [8.22], Avg: -133.6634 (1.000)
Step: 153307, Reward: -138.7606 [34.46], Avg: -134.0841 (1.000)
Step: 155307, Reward: -81.1700 [32.79], Avg: -133.8723 (1.000)
Step: 157307, Reward: -87.6145 [46.14], Avg: -133.8711 (1.000)
Step: 158551, Reward: -116.9376 [19.57], Avg: -133.8983 (1.000)
Step: 160551, Reward: -90.7191 [20.92], Avg: -133.6711 (1.000)
Step: 161597, Reward: -52.5348 [25.77], Avg: -133.1118 (1.000)
Step: 163597, Reward: -62.4549 [48.56], Avg: -132.8909 (1.000)
Step: 165597, Reward: -106.3852 [16.77], Avg: -132.7945 (1.000)
Step: 167597, Reward: -56.6054 [31.05], Avg: -132.3519 (1.000)
Step: 169178, Reward: -127.7160 [38.36], Avg: -132.6793 (1.000)
Step: 171178, Reward: -96.6189 [20.40], Avg: -132.5287 (1.000)
Step: 173178, Reward: -103.2523 [12.49], Avg: -132.3689 (1.000)
Step: 175178, Reward: -81.6807 [53.92], Avg: -132.3993 (1.000)
Step: 177178, Reward: -93.9641 [14.65], Avg: -132.1771 (1.000)
Step: 179178, Reward: -84.5508 [31.16], Avg: -132.0246 (1.000)
Step: 181178, Reward: -117.9471 [22.96], Avg: -132.1061 (1.000)
Step: 183178, Reward: -100.1717 [47.33], Avg: -132.2460 (1.000)
Step: 185178, Reward: -64.7279 [40.90], Avg: -132.0063 (1.000)
Step: 187178, Reward: -84.3357 [25.20], Avg: -131.8056 (1.000)
Step: 189178, Reward: -86.2413 [32.39], Avg: -131.6890 (1.000)
Step: 191178, Reward: -95.7223 [22.96], Avg: -131.5750 (1.000)
Step: 191742, Reward: -74.8941 [49.35], Avg: -131.5112 (1.000)
Step: 192892, Reward: -112.8286 [15.08], Avg: -131.4802 (1.000)
Step: 194118, Reward: -79.3526 [46.78], Avg: -131.4345 (1.000)
Step: 196118, Reward: -94.5245 [30.37], Avg: -131.3791 (1.000)
Step: 196331, Reward: -102.3090 [44.82], Avg: -131.5114 (1.000)
Step: 198331, Reward: -88.1392 [32.15], Avg: -131.4179 (1.000)
Step: 199455, Reward: -105.0622 [57.09], Avg: -131.6719 (1.000)
Step: 201455, Reward: -120.2997 [8.95], Avg: -131.6521 (1.000)
Step: 202108, Reward: -91.0162 [29.95], Avg: -131.5652 (1.000)
Step: 204108, Reward: -104.3701 [44.81], Avg: -131.7072 (1.000)
Step: 206108, Reward: -109.1845 [5.75], Avg: -131.5730 (1.000)
Step: 208108, Reward: -59.1833 [54.10], Avg: -131.4279 (1.000)
Step: 208314, Reward: -115.5690 [13.49], Avg: -131.4092 (1.000)
Step: 210314, Reward: -63.3017 [37.34], Avg: -131.1688 (1.000)
Step: 212314, Reward: -115.0006 [5.72], Avg: -131.0878 (1.000)
Step: 212653, Reward: -106.8767 [9.16], Avg: -130.9720 (1.000)
Step: 214653, Reward: -104.3895 [16.39], Avg: -130.8943 (1.000)
Step: 216653, Reward: -91.4702 [18.52], Avg: -130.7359 (1.000)
Step: 216760, Reward: -100.9488 [15.87], Avg: -130.6313 (1.000)
Step: 216829, Reward: -79.3309 [27.99], Avg: -130.4573 (1.000)
Step: 218829, Reward: -97.0622 [25.22], Avg: -130.3968 (1.000)
Step: 220829, Reward: -73.0452 [29.99], Avg: -130.1955 (1.000)
Step: 222829, Reward: -101.6380 [18.03], Avg: -130.1187 (1.000)
Step: 223000, Reward: -82.8992 [17.56], Avg: -129.9038 (1.000)
Step: 225000, Reward: -103.0388 [50.13], Avg: -130.0712 (1.000)
Step: 227000, Reward: -101.6790 [14.92], Avg: -129.9750 (1.000)
Step: 229000, Reward: -105.4875 [24.14], Avg: -129.9725 (1.000)
Step: 229872, Reward: -108.2559 [7.67], Avg: -129.8736 (1.000)
Step: 231872, Reward: -103.3622 [7.83], Avg: -129.7430 (1.000)
Step: 233872, Reward: -125.5164 [26.49], Avg: -129.8976 (1.000)
Step: 235872, Reward: -110.6893 [23.39], Avg: -129.9265 (1.000)
Step: 237872, Reward: -114.9950 [33.76], Avg: -130.0555 (1.000)
Step: 238736, Reward: -82.7673 [51.08], Avg: -130.0812 (1.000)
Step: 240736, Reward: -108.0638 [9.36], Avg: -129.9957 (1.000)
Step: 242736, Reward: -93.2200 [26.19], Avg: -129.9247 (1.000)
Step: 244736, Reward: -90.8174 [27.47], Avg: -129.8470 (1.000)
Step: 246736, Reward: -82.8439 [17.53], Avg: -129.6519 (1.000)
Step: 248736, Reward: -79.4954 [35.05], Avg: -129.5525 (1.000)
Step: 250736, Reward: -91.4342 [20.78], Avg: -129.4392 (1.000)
Step: 252736, Reward: -118.8382 [17.01], Avg: -129.4808 (1.000)
Step: 254736, Reward: -93.2428 [16.83], Avg: -129.3556 (1.000)
Step: 256736, Reward: -115.8633 [41.22], Avg: -129.5334 (1.000)
Step: 258736, Reward: -107.3075 [11.42], Avg: -129.4645 (1.000)
Step: 260736, Reward: -77.9953 [26.76], Avg: -129.3081 (1.000)
Step: 262736, Reward: -81.8047 [46.92], Avg: -129.3044 (1.000)
Step: 264736, Reward: -94.0890 [49.74], Avg: -129.3952 (1.000)
Step: 266220, Reward: -88.0186 [36.59], Avg: -129.3655 (1.000)
Step: 268220, Reward: -89.8703 [33.59], Avg: -129.3290 (1.000)
Step: 270220, Reward: -99.8615 [17.80], Avg: -129.2574 (1.000)
Step: 272220, Reward: -66.0532 [42.35], Avg: -129.1303 (1.000)
Step: 273953, Reward: -76.8432 [57.92], Avg: -129.1644 (1.000)
Step: 275953, Reward: -89.4662 [18.88], Avg: -129.0389 (1.000)
Step: 277953, Reward: -70.7281 [24.42], Avg: -128.8360 (1.000)
Step: 279953, Reward: -80.3507 [41.29], Avg: -128.7931 (1.000)
Step: 281123, Reward: -79.4209 [35.93], Avg: -128.7136 (1.000)
Step: 281270, Reward: -82.0653 [26.38], Avg: -128.5944 (1.000)
Step: 283270, Reward: -72.9360 [51.17], Avg: -128.5681 (1.000)
Step: 285270, Reward: -70.5453 [50.67], Avg: -128.5254 (1.000)
Step: 287270, Reward: -114.2083 [21.90], Avg: -128.5692 (1.000)
Step: 289270, Reward: -79.0781 [46.50], Avg: -128.5520 (1.000)
Step: 291270, Reward: -97.4135 [58.62], Avg: -128.7090 (1.000)
Step: 291859, Reward: -90.2924 [34.48], Avg: -128.6866 (1.000)
Step: 293859, Reward: -82.6958 [55.17], Avg: -128.7385 (1.000)
Step: 295110, Reward: -98.0877 [55.76], Avg: -128.8796 (1.000)
Step: 297110, Reward: -125.2474 [42.10], Avg: -129.0945 (1.000)
Step: 299110, Reward: -71.3167 [32.54], Avg: -128.9543 (1.000)
Step: 301110, Reward: -94.3134 [41.79], Avg: -128.9938 (1.000)
Step: 303110, Reward: -85.6078 [49.85], Avg: -129.0293 (1.000)
Step: 305110, Reward: -76.0696 [58.20], Avg: -129.0579 (1.000)
Step: 307110, Reward: -68.9180 [52.24], Avg: -129.0150 (1.000)
Step: 309110, Reward: -86.9118 [51.25], Avg: -129.0644 (1.000)
Step: 311110, Reward: -103.7145 [15.12], Avg: -129.0094 (1.000)
Step: 313110, Reward: -118.9343 [31.82], Avg: -129.1257 (1.000)
Step: 315110, Reward: -75.3370 [37.48], Avg: -129.0390 (1.000)
Step: 317110, Reward: -98.3326 [10.01], Avg: -128.9295 (1.000)
Step: 319110, Reward: -89.5881 [16.92], Avg: -128.8115 (1.000)
Step: 319951, Reward: -81.9168 [49.42], Avg: -128.8247 (1.000)
Step: 321951, Reward: -120.1332 [16.94], Avg: -128.8676 (1.000)
Step: 322875, Reward: -123.3068 [32.60], Avg: -129.0077 (1.000)
Step: 324034, Reward: -62.2717 [42.70], Avg: -128.8838 (1.000)
Step: 324650, Reward: -103.5130 [45.22], Avg: -128.9856 (1.000)
Step: 325352, Reward: -81.6321 [68.17], Avg: -129.0918 (1.000)
Step: 327352, Reward: -114.4309 [8.10], Avg: -129.0585 (1.000)
Step: 327643, Reward: -122.5025 [21.78], Avg: -129.1354 (1.000)
Step: 329643, Reward: -101.1741 [26.35], Avg: -129.1273 (1.000)
Step: 331475, Reward: -98.7470 [14.34], Avg: -129.0471 (1.000)
Step: 331842, Reward: -69.3838 [45.19], Avg: -128.9751 (1.000)
Step: 331925, Reward: -132.0524 [45.36], Avg: -129.2149 (1.000)
Step: 333925, Reward: -91.3147 [18.81], Avg: -129.1208 (1.000)
Step: 334841, Reward: -110.3817 [38.88], Avg: -129.2196 (1.000)
Step: 336234, Reward: -100.9647 [11.41], Avg: -129.1374 (1.000)
Step: 336552, Reward: -97.1852 [22.53], Avg: -129.0917 (1.000)
Step: 338552, Reward: -103.1336 [39.39], Avg: -129.1566 (1.000)
Step: 340552, Reward: -100.0664 [11.87], Avg: -129.0739 (1.000)
Step: 341102, Reward: -111.7739 [38.13], Avg: -129.1735 (1.000)
Step: 341473, Reward: -106.7806 [9.89], Avg: -129.1140 (1.000)
Step: 341562, Reward: -91.9879 [11.35], Avg: -128.9918 (1.000)
Step: 343562, Reward: -82.1872 [35.05], Avg: -128.9364 (1.000)
Step: 344732, Reward: -101.4412 [19.25], Avg: -128.8976 (1.000)
Step: 346732, Reward: -112.5289 [28.00], Avg: -128.9520 (1.000)
Step: 348732, Reward: -63.9804 [83.19], Avg: -129.0367 (1.000)
Step: 350732, Reward: -79.8740 [27.47], Avg: -128.9363 (1.000)
Step: 352732, Reward: -81.2335 [17.04], Avg: -128.7950 (1.000)
Step: 354732, Reward: -56.1543 [38.76], Avg: -128.6396 (1.000)
Step: 356732, Reward: -99.9850 [15.61], Avg: -128.5800 (1.000)
Step: 356897, Reward: -99.2273 [59.99], Avg: -128.7193 (1.000)
Step: 357251, Reward: -118.2203 [59.13], Avg: -128.9393 (1.000)
Step: 359251, Reward: -99.8367 [14.11], Avg: -128.8718 (1.000)
Step: 361251, Reward: -137.3111 [40.66], Avg: -129.0920 (1.000)
Step: 361578, Reward: -119.9860 [35.66], Avg: -129.2105 (1.000)
Step: 362612, Reward: -102.8098 [15.43], Avg: -129.1617 (1.000)
Step: 363892, Reward: -70.5132 [41.15], Avg: -129.0843 (1.000)
Step: 365892, Reward: -102.0595 [23.23], Avg: -129.0675 (1.000)
Step: 367892, Reward: -91.4202 [46.96], Avg: -129.1084 (1.000)
Step: 369892, Reward: -111.8606 [9.00], Avg: -129.0724 (1.000)
Step: 371892, Reward: -100.9089 [29.66], Avg: -129.0789 (1.000)
Step: 373892, Reward: -111.9885 [2.97], Avg: -129.0178 (1.000)
Step: 375892, Reward: -92.9263 [27.03], Avg: -128.9787 (1.000)
Step: 377892, Reward: -108.6863 [6.34], Avg: -128.9188 (1.000)
Step: 379892, Reward: -104.9030 [15.07], Avg: -128.8806 (1.000)
Step: 381892, Reward: -108.0577 [15.88], Avg: -128.8596 (1.000)
Step: 383892, Reward: -112.9425 [8.37], Avg: -128.8276 (1.000)
Step: 384070, Reward: -114.8541 [0.89], Avg: -128.7724 (1.000)
Step: 386070, Reward: -83.6012 [33.26], Avg: -128.7223 (1.000)
Step: 388070, Reward: -79.8735 [27.21], Avg: -128.6318 (1.000)
Step: 390070, Reward: -70.9107 [67.34], Avg: -128.6719 (1.000)
Step: 392070, Reward: -96.7841 [19.97], Avg: -128.6224 (1.000)
Step: 394070, Reward: -55.8547 [66.39], Avg: -128.5961 (1.000)
Step: 396070, Reward: -99.2563 [21.32], Avg: -128.5631 (1.000)
Step: 397497, Reward: -97.5457 [33.33], Avg: -128.5725 (1.000)
Step: 399497, Reward: -94.8060 [59.61], Avg: -128.6780 (1.000)
Step: 401497, Reward: -75.5013 [40.81], Avg: -128.6278 (1.000)
Step: 403497, Reward: -90.4189 [32.73], Avg: -128.6056 (1.000)
Step: 403917, Reward: -101.8702 [17.71], Avg: -128.5692 (1.000)
Step: 405917, Reward: -83.6588 [47.94], Avg: -128.5813 (1.000)
Step: 406509, Reward: -83.3665 [65.00], Avg: -128.6605 (1.000)
Step: 407616, Reward: -122.0813 [20.84], Avg: -128.7173 (1.000)
Step: 409616, Reward: -96.9038 [42.41], Avg: -128.7594 (1.000)
Step: 409736, Reward: -83.0016 [37.32], Avg: -128.7260 (1.000)
Step: 411736, Reward: -74.9423 [53.52], Avg: -128.7250 (1.000)
Step: 413736, Reward: -126.1410 [39.08], Avg: -128.8681 (1.000)
Step: 415383, Reward: -113.0380 [18.53], Avg: -128.8787 (1.000)
Step: 416316, Reward: -88.9563 [17.09], Avg: -128.7898 (1.000)
Step: 418316, Reward: -96.7829 [23.72], Avg: -128.7577 (1.000)
Step: 418744, Reward: -139.1789 [40.22], Avg: -128.9532 (1.000)
Step: 418874, Reward: -64.6062 [56.67], Avg: -128.9237 (1.000)
Step: 418996, Reward: -96.1059 [28.71], Avg: -128.9080 (1.000)
Step: 419200, Reward: -105.0711 [21.65], Avg: -128.8996 (1.000)
Step: 421200, Reward: -98.1907 [19.77], Avg: -128.8580 (1.000)
Step: 421522, Reward: -124.0996 [35.05], Avg: -128.9728 (1.000)
Step: 423522, Reward: -103.3736 [8.02], Avg: -128.9064 (1.000)
Step: 425522, Reward: -130.3077 [41.08], Avg: -129.0661 (1.000)
Step: 425876, Reward: -95.9685 [12.91], Avg: -128.9905 (1.000)
Step: 427876, Reward: -108.2448 [17.67], Avg: -128.9791 (1.000)
Step: 429876, Reward: -91.2053 [30.87], Avg: -128.9534 (1.000)
Step: 430076, Reward: -86.7626 [24.53], Avg: -128.8880 (1.000)
Step: 431249, Reward: -75.7458 [68.53], Avg: -128.9448 (1.000)
Step: 433249, Reward: -118.4874 [12.22], Avg: -128.9512 (1.000)
Step: 435249, Reward: -82.9813 [52.32], Avg: -128.9745 (1.000)
Step: 437249, Reward: -62.8503 [68.52], Avg: -128.9832 (1.000)
Step: 437425, Reward: -97.4071 [11.58], Avg: -128.9105 (1.000)
Step: 438076, Reward: -72.7865 [46.62], Avg: -128.8761 (1.000)
Step: 438456, Reward: -119.3881 [1.19], Avg: -128.8461 (1.000)
Step: 440456, Reward: -96.9261 [13.34], Avg: -128.7793 (1.000)
Step: 442456, Reward: -69.2475 [48.07], Avg: -128.7382 (1.000)
Step: 444456, Reward: -101.9917 [12.48], Avg: -128.6873 (1.000)
Step: 444658, Reward: -105.3655 [19.54], Avg: -128.6738 (1.000)
Step: 444859, Reward: -97.1372 [11.13], Avg: -128.6014 (1.000)
Step: 445023, Reward: -90.2224 [2.43], Avg: -128.4744 (1.000)
Step: 445524, Reward: -95.3787 [11.57], Avg: -128.3986 (1.000)
Step: 446322, Reward: -77.3032 [28.66], Avg: -128.3199 (1.000)
Step: 448322, Reward: -74.4192 [42.89], Avg: -128.2814 (1.000)
Step: 449319, Reward: -96.7789 [14.02], Avg: -128.2205 (1.000)
Step: 449889, Reward: -98.1915 [32.20], Avg: -128.2281 (1.000)
Step: 451889, Reward: -95.7889 [26.81], Avg: -128.2086 (1.000)
Step: 453889, Reward: -89.5947 [22.24], Avg: -128.1521 (1.000)
Step: 455889, Reward: -67.9686 [53.49], Avg: -128.1291 (1.000)
Step: 457889, Reward: -81.7285 [60.71], Avg: -128.1781 (1.000)
Step: 458068, Reward: -96.3909 [17.16], Avg: -128.1282 (1.000)
Step: 460068, Reward: -93.4499 [13.57], Avg: -128.0564 (1.000)
Step: 462068, Reward: -95.3843 [10.65], Avg: -127.9817 (1.000)
Step: 464068, Reward: -98.2065 [25.99], Avg: -127.9690 (1.000)
Step: 466068, Reward: -101.2466 [15.80], Avg: -127.9322 (1.000)
Step: 466725, Reward: -129.9944 [31.23], Avg: -128.0439 (1.000)
Step: 468725, Reward: -111.4524 [6.45], Avg: -128.0100 (1.000)
Step: 470272, Reward: -93.0998 [51.19], Avg: -128.0642 (1.000)
Step: 472272, Reward: -117.8572 [25.10], Avg: -128.1137 (1.000)
Step: 474272, Reward: -123.7072 [30.88], Avg: -128.2014 (1.000)
Step: 475884, Reward: -109.9673 [23.99], Avg: -128.2204 (1.000)
Step: 477884, Reward: -81.9268 [21.03], Avg: -128.1373 (1.000)
Step: 479884, Reward: -97.2297 [22.62], Avg: -128.1101 (1.000)
Step: 481884, Reward: -85.3204 [19.78], Avg: -128.0349 (1.000)
Step: 483884, Reward: -129.9022 [28.43], Avg: -128.1336 (1.000)
Step: 485884, Reward: -98.8163 [29.88], Avg: -128.1354 (1.000)
Step: 487884, Reward: -125.9840 [16.18], Avg: -128.1808 (1.000)
Step: 489884, Reward: -75.2399 [33.15], Avg: -128.1170 (1.000)
Step: 491884, Reward: -115.0550 [18.43], Avg: -128.1343 (1.000)
Step: 493884, Reward: -117.7654 [18.59], Avg: -128.1606 (1.000)
Step: 495884, Reward: -94.8835 [19.31], Avg: -128.1160 (1.000)
Step: 496357, Reward: -83.1554 [54.86], Avg: -128.1475 (1.000)
Step: 498357, Reward: -61.8190 [41.57], Avg: -128.0689 (1.000)
Step: 500357, Reward: -75.0844 [55.44], Avg: -128.0767 (1.000)
Step: 502357, Reward: -99.1369 [21.78], Avg: -128.0541 (1.000)
Step: 502782, Reward: -112.8859 [38.50], Avg: -128.1275 (1.000)
Step: 504782, Reward: -53.4689 [55.72], Avg: -128.0681 (1.000)
Step: 506782, Reward: -88.6838 [30.35], Avg: -128.0399 (1.000)
Step: 508782, Reward: -105.2949 [15.70], Avg: -128.0180 (1.000)
Step: 510782, Reward: -111.0700 [53.75], Avg: -128.1323 (1.000)
Step: 512782, Reward: -61.1464 [83.38], Avg: -128.1830 (1.000)
Step: 514782, Reward: -79.6092 [45.43], Avg: -128.1733 (1.000)
Step: 516782, Reward: -62.7015 [47.07], Avg: -128.1167 (1.000)
Step: 518782, Reward: -113.1415 [12.73], Avg: -128.1098 (1.000)
Step: 520782, Reward: -80.8420 [61.62], Avg: -128.1537 (1.000)
Step: 522782, Reward: -77.5023 [61.37], Avg: -128.1864 (1.000)
Step: 523928, Reward: -95.5711 [35.14], Avg: -128.1940 (1.000)
Step: 525928, Reward: -110.0923 [8.47], Avg: -128.1649 (1.000)
Step: 527928, Reward: -69.5323 [60.08], Avg: -128.1692 (1.000)
Step: 529164, Reward: -101.4610 [23.44], Avg: -128.1594 (1.000)
Step: 530061, Reward: -55.8436 [69.18], Avg: -128.1500 (1.000)
Step: 532061, Reward: -57.3944 [67.33], Avg: -128.1397 (1.000)
Step: 533968, Reward: -67.9481 [37.55], Avg: -128.0721 (1.000)
Step: 535968, Reward: -57.0441 [60.28], Avg: -128.0401 (1.000)
Step: 537968, Reward: -110.5520 [16.27], Avg: -128.0365 (1.000)
Step: 538788, Reward: -111.4483 [20.02], Avg: -128.0466 (1.000)
Step: 540788, Reward: -81.6859 [18.58], Avg: -127.9647 (1.000)
Step: 541406, Reward: -78.4536 [32.26], Avg: -127.9139 (1.000)
Step: 543406, Reward: -31.8250 [75.30], Avg: -127.8530 (1.000)
Step: 545406, Reward: -79.9253 [48.00], Avg: -127.8532 (1.000)
Step: 547406, Reward: -107.4024 [41.40], Avg: -127.9143 (1.000)
Step: 547858, Reward: -94.0955 [57.73], Avg: -127.9838 (1.000)
Step: 549858, Reward: -99.0959 [26.58], Avg: -127.9771 (1.000)
Step: 551858, Reward: -95.4417 [43.54], Avg: -128.0089 (1.000)
Step: 553858, Reward: -134.2729 [19.88], Avg: -128.0842 (1.000)
Step: 555858, Reward: -89.8384 [15.32], Avg: -128.0183 (1.000)
Step: 557858, Reward: -108.0637 [25.44], Avg: -128.0341 (1.000)
Step: 559858, Reward: -109.0724 [21.68], Avg: -128.0418 (1.000)
Step: 560004, Reward: -117.5780 [14.25], Avg: -128.0526 (1.000)
Step: 562004, Reward: -109.4172 [46.15], Avg: -128.1308 (1.000)
Step: 564004, Reward: -111.2753 [55.32], Avg: -128.2397 (1.000)
Step: 566004, Reward: -93.8206 [24.65], Avg: -128.2121 (1.000)
Step: 568004, Reward: -87.3728 [31.11], Avg: -128.1847 (1.000)
Step: 570004, Reward: -85.3472 [48.31], Avg: -128.2001 (1.000)
Step: 572004, Reward: -93.4571 [37.17], Avg: -128.2069 (1.000)
Step: 574004, Reward: -83.5645 [35.15], Avg: -128.1804 (1.000)
Step: 574463, Reward: -117.5779 [20.52], Avg: -128.2080 (1.000)
Step: 576463, Reward: -119.7127 [18.58], Avg: -128.2360 (1.000)
Step: 578463, Reward: -109.5384 [13.55], Avg: -128.2217 (1.000)
Step: 580463, Reward: -70.1168 [62.43], Avg: -128.2337 (1.000)
Step: 580560, Reward: -97.4463 [25.30], Avg: -128.2186 (1.000)
Step: 581941, Reward: -81.6166 [36.78], Avg: -128.1916 (1.000)
Step: 583357, Reward: -99.4326 [25.45], Avg: -128.1825 (1.000)
Step: 583910, Reward: -97.3238 [34.12], Avg: -128.1914 (1.000)
Step: 585910, Reward: -107.8876 [17.47], Avg: -128.1837 (1.000)
Step: 586365, Reward: -107.8270 [22.51], Avg: -128.1895 (1.000)
Step: 588365, Reward: -53.5924 [63.95], Avg: -128.1607 (1.000)
Step: 590365, Reward: -64.5265 [59.97], Avg: -128.1508 (1.000)
Step: 592365, Reward: -70.7345 [56.64], Avg: -128.1487 (1.000)
Step: 592489, Reward: -125.1815 [35.69], Avg: -128.2366 (1.000)
Step: 594153, Reward: -92.4372 [41.74], Avg: -128.2526 (1.000)
Step: 596153, Reward: -97.0647 [72.39], Avg: -128.3628 (1.000)
Step: 596697, Reward: -101.6777 [25.59], Avg: -128.3598 (1.000)
Step: 597110, Reward: -114.9780 [33.08], Avg: -128.4122 (1.000)
Step: 599110, Reward: -117.2374 [12.82], Avg: -128.4166 (1.000)
Step: 599982, Reward: -78.1083 [42.67], Avg: -128.3964 (1.000)
Step: 601982, Reward: -81.5808 [33.03], Avg: -128.3600 (1.000)
Step: 603982, Reward: -101.6993 [15.52], Avg: -128.3307 (1.000)
Step: 605982, Reward: -125.6670 [49.95], Avg: -128.4548 (1.000)
Step: 607982, Reward: -81.3159 [57.52], Avg: -128.4820 (1.000)
Step: 608638, Reward: -104.6599 [44.75], Avg: -128.5366 (1.000)
Step: 609774, Reward: -76.6018 [19.02], Avg: -128.4509 (1.000)
Step: 610070, Reward: -93.1034 [31.42], Avg: -128.4407 (1.000)
Step: 610269, Reward: -90.8387 [35.77], Avg: -128.4359 (1.000)
Step: 610599, Reward: -99.6492 [22.03], Avg: -128.4185 (1.000)
Step: 612599, Reward: -110.2876 [37.49], Avg: -128.4683 (1.000)
Step: 614599, Reward: -65.3996 [86.11], Avg: -128.5276 (1.000)
Step: 615941, Reward: -104.2004 [68.86], Avg: -128.6418 (1.000)
Step: 617941, Reward: -119.9299 [7.83], Avg: -128.6395 (1.000)
Step: 619941, Reward: -100.6583 [21.97], Avg: -128.6242 (1.000)
Step: 621941, Reward: -82.0924 [20.22], Avg: -128.5572 (1.000)
Step: 622118, Reward: -103.7986 [22.84], Avg: -128.5524 (1.000)
Step: 623985, Reward: -96.6842 [45.96], Avg: -128.5881 (1.000)
Step: 625658, Reward: -89.7442 [24.59], Avg: -128.5521 (1.000)
Step: 625911, Reward: -85.4849 [25.73], Avg: -128.5084 (1.000)
Step: 626096, Reward: -68.4981 [36.83], Avg: -128.4502 (1.000)
Step: 626255, Reward: -99.1013 [25.78], Avg: -128.4412 (1.000)
Step: 626379, Reward: -90.0889 [19.19], Avg: -128.3933 (1.000)
Step: 628379, Reward: -29.0576 [74.41], Avg: -128.3312 (1.000)
Step: 630379, Reward: -99.4585 [18.76], Avg: -128.3060 (1.000)
Step: 630507, Reward: -43.7779 [65.60], Avg: -128.2591 (1.000)
Step: 632507, Reward: -117.1146 [19.58], Avg: -128.2799 (1.000)
Step: 633261, Reward: -132.5441 [19.81], Avg: -128.3394 (1.000)
Step: 635261, Reward: -95.4429 [38.99], Avg: -128.3544 (1.000)
Step: 635711, Reward: -114.5078 [19.63], Avg: -128.3686 (1.000)
Step: 637711, Reward: -62.4270 [64.54], Avg: -128.3652 (1.000)
Step: 639711, Reward: -94.4880 [33.24], Avg: -128.3636 (1.000)
Step: 639889, Reward: -54.9250 [44.82], Avg: -128.2938 (1.000)
Step: 641423, Reward: -98.0815 [33.63], Avg: -128.3021 (1.000)
Step: 642467, Reward: -85.0471 [42.25], Avg: -128.2997 (1.000)
Step: 644467, Reward: -95.0504 [37.20], Avg: -128.3093 (1.000)
Step: 644702, Reward: -78.6919 [26.73], Avg: -128.2540 (1.000)
Step: 644966, Reward: -93.4590 [23.20], Avg: -128.2260 (1.000)
Step: 645373, Reward: -121.4570 [10.42], Avg: -128.2348 (1.000)
Step: 647373, Reward: -71.7913 [52.97], Avg: -128.2265 (1.000)
Step: 649373, Reward: -93.4610 [29.77], Avg: -128.2145 (1.000)
Step: 651373, Reward: -119.9442 [43.75], Avg: -128.2992 (1.000)
Step: 653373, Reward: -116.8202 [36.08], Avg: -128.3578 (1.000)
Step: 654002, Reward: -86.5058 [80.49], Avg: -128.4495 (1.000)
Step: 656002, Reward: -112.3655 [11.49], Avg: -128.4386 (1.000)
Step: 657855, Reward: -92.2414 [19.79], Avg: -128.3998 (1.000)
Step: 659190, Reward: -114.0574 [15.36], Avg: -128.4023 (1.000)
Step: 659962, Reward: -79.9132 [53.44], Avg: -128.4139 (1.000)
Step: 660087, Reward: -67.6288 [34.37], Avg: -128.3519 (1.000)
Step: 661957, Reward: -101.2405 [43.07], Avg: -128.3893 (1.000)
Step: 662351, Reward: -97.3122 [54.41], Avg: -128.4438 (1.000)
Step: 662606, Reward: -117.1945 [23.52], Avg: -128.4724 (1.000)
Step: 662862, Reward: -90.3417 [33.84], Avg: -128.4624 (1.000)
Step: 663642, Reward: -101.9262 [24.28], Avg: -128.4571 (1.000)
Step: 665642, Reward: -88.2043 [27.58], Avg: -128.4278 (1.000)
Step: 666595, Reward: -107.2032 [35.46], Avg: -128.4607 (1.000)
Step: 666859, Reward: -107.7746 [23.59], Avg: -128.4674 (1.000)
Step: 667065, Reward: -89.7663 [11.43], Avg: -128.4047 (1.000)
Step: 667345, Reward: -111.8036 [41.26], Avg: -128.4612 (1.000)
Step: 667466, Reward: -80.2860 [51.69], Avg: -128.4693 (1.000)
Step: 667879, Reward: -97.7206 [19.81], Avg: -128.4443 (1.000)
Step: 669879, Reward: -72.8565 [31.37], Avg: -128.3892 (1.000)
Step: 671879, Reward: -95.5026 [43.70], Avg: -128.4137 (1.000)
Step: 673879, Reward: -115.3723 [36.09], Avg: -128.4660 (1.000)
Step: 675879, Reward: -93.2030 [75.48], Avg: -128.5570 (1.000)
Step: 676443, Reward: -104.5177 [21.67], Avg: -128.5516 (1.000)
Step: 676961, Reward: -97.2964 [26.04], Avg: -128.5399 (1.000)
Step: 678257, Reward: -89.2890 [35.46], Avg: -128.5314 (1.000)
Step: 678726, Reward: -86.7585 [40.08], Avg: -128.5276 (1.000)
Step: 680726, Reward: -106.4971 [53.68], Avg: -128.5984 (1.000)
Step: 682726, Reward: -74.2850 [38.59], Avg: -128.5633 (1.000)
Step: 684726, Reward: -73.8937 [63.11], Avg: -128.5821 (1.000)
Step: 686726, Reward: -117.4564 [34.95], Avg: -128.6350 (1.000)
Step: 687362, Reward: -124.2394 [23.36], Avg: -128.6771 (1.000)
Step: 689362, Reward: -104.5675 [39.12], Avg: -128.7103 (1.000)
Step: 691362, Reward: -106.0288 [28.94], Avg: -128.7241 (1.000)
Step: 693362, Reward: -105.5455 [15.86], Avg: -128.7080 (1.000)
Step: 695362, Reward: -111.5609 [24.16], Avg: -128.7234 (1.000)
Step: 697362, Reward: -113.9725 [19.08], Avg: -128.7329 (1.000)
Step: 699362, Reward: -105.1144 [26.97], Avg: -128.7402 (1.000)
Step: 701362, Reward: -61.4776 [85.10], Avg: -128.7792 (1.000)
Step: 701994, Reward: -107.4531 [18.81], Avg: -128.7737 (1.000)
Step: 703994, Reward: -101.4626 [30.11], Avg: -128.7798 (1.000)
Step: 705994, Reward: -106.2855 [27.57], Avg: -128.7908 (1.000)
Step: 707994, Reward: -99.1211 [21.48], Avg: -128.7730 (1.000)
Step: 709994, Reward: -63.4399 [56.91], Avg: -128.7549 (1.000)
Step: 710620, Reward: -67.5849 [52.62], Avg: -128.7364 (1.000)
Step: 712620, Reward: -124.0429 [17.76], Avg: -128.7645 (1.000)
Step: 714620, Reward: -116.0712 [40.77], Avg: -128.8248 (1.000)
Step: 716620, Reward: -123.4400 [17.53], Avg: -128.8508 (1.000)
Step: 717072, Reward: -90.5559 [30.68], Avg: -128.8345 (1.000)
Step: 719072, Reward: -118.8399 [20.49], Avg: -128.8569 (1.000)
Step: 721072, Reward: -104.7224 [25.39], Avg: -128.8596 (1.000)
Step: 723072, Reward: -110.1707 [32.27], Avg: -128.8884 (1.000)
Step: 725072, Reward: -90.8197 [90.28], Avg: -128.9990 (1.000)
Step: 726316, Reward: -85.5877 [13.92], Avg: -128.9366 (1.000)
Step: 728316, Reward: -70.1082 [40.78], Avg: -128.8986 (1.000)
Step: 730316, Reward: -98.1330 [23.75], Avg: -128.8838 (1.000)
Step: 730464, Reward: -82.7825 [30.01], Avg: -128.8500 (1.000)
Step: 732464, Reward: -78.1787 [20.90], Avg: -128.7876 (1.000)
Step: 734464, Reward: -112.5638 [66.77], Avg: -128.8933 (1.000)
Step: 734779, Reward: -93.2196 [26.24], Avg: -128.8736 (1.000)
Step: 735096, Reward: -63.4413 [87.38], Avg: -128.9193 (1.000)
Step: 736092, Reward: -87.0821 [47.23], Avg: -128.9306 (1.000)
Step: 736201, Reward: -93.6680 [41.38], Avg: -128.9432 (1.000)
Step: 738201, Reward: -119.2923 [76.24], Avg: -129.0811 (1.000)
Step: 740201, Reward: -140.1336 [56.56], Avg: -129.2208 (1.000)
Step: 740835, Reward: -90.7846 [37.76], Avg: -129.2194 (1.000)
Step: 741475, Reward: -62.8626 [25.96], Avg: -129.1363 (1.000)
Step: 742122, Reward: -24.1813 [92.23], Avg: -129.1102 (1.000)
Step: 744122, Reward: -87.9852 [28.31], Avg: -129.0839 (1.000)
Step: 744451, Reward: -83.9039 [38.64], Avg: -129.0705 (1.000)
Step: 744616, Reward: -49.0622 [65.06], Avg: -129.0400 (1.000)
Step: 746616, Reward: -95.7749 [25.11], Avg: -129.0234 (1.000)
Step: 748616, Reward: -69.1397 [44.89], Avg: -128.9929 (1.000)
Step: 750616, Reward: -60.1185 [62.35], Avg: -128.9797 (1.000)
Step: 752616, Reward: -72.3019 [26.98], Avg: -128.9196 (1.000)
Step: 754616, Reward: -114.3318 [29.54], Avg: -128.9498 (1.000)
Step: 754753, Reward: -93.6201 [48.29], Avg: -128.9759 (1.000)
Step: 756753, Reward: -121.7995 [18.25], Avg: -128.9982 (1.000)
Step: 757125, Reward: -85.5430 [23.58], Avg: -128.9583 (1.000)
Step: 758339, Reward: -133.5319 [45.33], Avg: -129.0583 (1.000)
Step: 760339, Reward: -103.4519 [23.93], Avg: -129.0549 (1.000)
Step: 762339, Reward: -77.3180 [43.25], Avg: -129.0380 (1.000)
Step: 762797, Reward: -83.9908 [43.44], Avg: -129.0348 (1.000)
Step: 762973, Reward: -98.0356 [39.22], Avg: -129.0512 (1.000)
Step: 764973, Reward: -126.0989 [39.78], Avg: -129.1242 (1.000)
Step: 766973, Reward: -90.6445 [54.73], Avg: -129.1564 (1.000)
Step: 767450, Reward: -111.2651 [24.46], Avg: -129.1694 (1.000)
Step: 767820, Reward: -98.6873 [26.65], Avg: -129.1618 (1.000)
Step: 769157, Reward: -91.5917 [51.82], Avg: -129.1899 (1.000)
Step: 771157, Reward: -37.5233 [59.16], Avg: -129.1260 (1.000)
Step: 773157, Reward: -108.9722 [20.49], Avg: -129.1267 (1.000)
Step: 773332, Reward: -96.2600 [45.20], Avg: -129.1508 (1.000)
Step: 775332, Reward: -59.7615 [50.53], Avg: -129.1140 (1.000)
Step: 777332, Reward: -117.1487 [67.28], Avg: -129.2218 (1.000)
Step: 777778, Reward: -88.9539 [29.56], Avg: -129.2010 (1.000)
Step: 778021, Reward: -121.1669 [65.52], Avg: -129.3126 (1.000)
Step: 778168, Reward: -109.4642 [29.00], Avg: -129.3304 (1.000)
Step: 780168, Reward: -89.9307 [47.34], Avg: -129.3457 (1.000)
Step: 780386, Reward: -108.7737 [38.66], Avg: -129.3806 (1.000)
Step: 780547, Reward: -94.2748 [68.31], Avg: -129.4446 (1.000)
Step: 782547, Reward: -90.2648 [18.21], Avg: -129.4043 (1.000)
Step: 783078, Reward: -90.5642 [3.75], Avg: -129.3369 (1.000)
Step: 785078, Reward: -119.4434 [47.04], Avg: -129.4081 (1.000)
Step: 787078, Reward: -117.4115 [24.04], Avg: -129.4311 (1.000)
Step: 789078, Reward: -108.7419 [29.08], Avg: -129.4471 (1.000)
Step: 791078, Reward: -120.2934 [34.66], Avg: -129.4957 (1.000)
Step: 793078, Reward: -106.1312 [28.48], Avg: -129.5054 (1.000)
Step: 795078, Reward: -85.7037 [70.65], Avg: -129.5564 (1.000)
Step: 795758, Reward: -103.3750 [21.76], Avg: -129.5480 (1.000)
Step: 797758, Reward: -120.3658 [24.25], Avg: -129.5765 (1.000)
Step: 798811, Reward: -104.9269 [24.81], Avg: -129.5768 (1.000)
Step: 800811, Reward: -88.2953 [44.62], Avg: -129.5831 (1.000)
Step: 801736, Reward: -115.3747 [46.77], Avg: -129.6443 (1.000)
Step: 803736, Reward: -97.2625 [67.94], Avg: -129.7110 (1.000)
Step: 804044, Reward: -109.6764 [25.78], Avg: -129.7217 (1.000)
Step: 804687, Reward: -116.1911 [23.83], Avg: -129.7410 (1.000)
Step: 806687, Reward: -115.1950 [20.86], Avg: -129.7528 (1.000)
Step: 808687, Reward: -140.0432 [37.88], Avg: -129.8425 (1.000)
Step: 810687, Reward: -65.6011 [31.44], Avg: -129.7815 (1.000)
Step: 812687, Reward: -128.6012 [64.13], Avg: -129.8983 (1.000)
Step: 814687, Reward: -96.4612 [17.48], Avg: -129.8687 (1.000)
Step: 815409, Reward: -79.9794 [16.82], Avg: -129.8076 (1.000)
Step: 817409, Reward: -64.8666 [14.67], Avg: -129.7149 (1.000)
Step: 819409, Reward: -93.5494 [20.05], Avg: -129.6852 (1.000)
Step: 821409, Reward: -104.5544 [23.34], Avg: -129.6819 (1.000)
Step: 822417, Reward: -44.5536 [89.87], Avg: -129.6906 (1.000)
Step: 824417, Reward: -86.4755 [72.23], Avg: -129.7437 (1.000)
Step: 824759, Reward: -87.8823 [33.36], Avg: -129.7282 (1.000)
Step: 826759, Reward: -110.1842 [34.53], Avg: -129.7555 (1.000)
Step: 828759, Reward: -99.3021 [30.22], Avg: -129.7551 (1.000)
Step: 828916, Reward: -117.8786 [37.20], Avg: -129.8011 (1.000)
Step: 830916, Reward: -72.2219 [63.47], Avg: -129.8118 (1.000)
Step: 831246, Reward: -86.4806 [33.18], Avg: -129.7934 (1.000)
Step: 832425, Reward: -111.2999 [24.97], Avg: -129.8051 (1.000)
Step: 834425, Reward: -110.8023 [25.09], Avg: -129.8161 (1.000)
Step: 836425, Reward: -94.1147 [61.39], Avg: -129.8624 (1.000)
Step: 838425, Reward: -123.8754 [47.49], Avg: -129.9370 (1.000)
Step: 840425, Reward: -119.8798 [34.05], Avg: -129.9801 (1.000)
Step: 842425, Reward: -110.2396 [36.05], Avg: -130.0093 (1.000)
Step: 843208, Reward: -65.1915 [35.02], Avg: -129.9560 (1.000)
Step: 845208, Reward: -85.2361 [79.38], Avg: -130.0179 (1.000)
Step: 847208, Reward: -40.6407 [71.36], Avg: -129.9858 (1.000)
Step: 848522, Reward: -74.1747 [38.65], Avg: -129.9553 (1.000)
Step: 848689, Reward: -135.1963 [56.07], Avg: -130.0642 (1.000)
Step: 850689, Reward: -90.6348 [17.58], Avg: -130.0255 (1.000)
Step: 852689, Reward: -94.8123 [28.27], Avg: -130.0132 (1.000)
Step: 853025, Reward: -99.0142 [42.48], Avg: -130.0334 (1.000)
Step: 853470, Reward: -57.6073 [74.59], Avg: -130.0373 (1.000)
Step: 855470, Reward: -127.3010 [23.54], Avg: -130.0739 (1.000)
Step: 857470, Reward: -117.2392 [23.30], Avg: -130.0923 (1.000)
Step: 857613, Reward: -102.6737 [45.73], Avg: -130.1244 (1.000)
Step: 858505, Reward: -106.5947 [50.40], Avg: -130.1714 (1.000)
Step: 860505, Reward: -130.6343 [44.11], Avg: -130.2494 (1.000)
Step: 862505, Reward: -118.9867 [22.19], Avg: -130.2684 (1.000)
Step: 864235, Reward: -114.7517 [55.94], Avg: -130.3389 (1.000)
Step: 866235, Reward: -107.5627 [24.09], Avg: -130.3411 (1.000)
Step: 868235, Reward: -96.8303 [48.53], Avg: -130.3672 (1.000)
Step: 868614, Reward: -81.1924 [59.86], Avg: -130.3857 (1.000)
Step: 870614, Reward: -96.5433 [26.57], Avg: -130.3731 (1.000)
Step: 872614, Reward: -120.0887 [34.54], Avg: -130.4150 (1.000)
Step: 874420, Reward: -121.2944 [33.84], Avg: -130.4577 (1.000)
Step: 876420, Reward: -97.7966 [40.39], Avg: -130.4710 (1.000)
Step: 878420, Reward: -114.8090 [28.97], Avg: -130.4938 (1.000)
Step: 880420, Reward: -95.4846 [59.37], Avg: -130.5356 (1.000)
Step: 882420, Reward: -99.4467 [35.12], Avg: -130.5425 (1.000)
Step: 884420, Reward: -89.9559 [47.37], Avg: -130.5541 (1.000)
Step: 886420, Reward: -128.4208 [23.17], Avg: -130.5900 (1.000)
Step: 888420, Reward: -112.3621 [21.88], Avg: -130.5962 (1.000)
Step: 889567, Reward: -85.5530 [17.68], Avg: -130.5497 (1.000)
Step: 890783, Reward: -113.6706 [20.84], Avg: -130.5564 (1.000)
Step: 891746, Reward: -111.7038 [24.47], Avg: -130.5659 (1.000)
Step: 892616, Reward: -129.2585 [24.01], Avg: -130.6043 (1.000)
Step: 892944, Reward: -73.9064 [51.17], Avg: -130.5950 (1.000)
Step: 894944, Reward: -82.6691 [66.95], Avg: -130.6271 (1.000)
Step: 895343, Reward: -110.9466 [54.29], Avg: -130.6854 (1.000)
Step: 895525, Reward: -133.9699 [34.27], Avg: -130.7485 (1.000)
Step: 896804, Reward: -95.2696 [36.63], Avg: -130.7504 (1.000)
Step: 898465, Reward: -129.5358 [28.85], Avg: -130.7967 (1.000)
Step: 899681, Reward: -115.7536 [29.63], Avg: -130.8211 (1.000)
Step: 901681, Reward: -119.8836 [27.97], Avg: -130.8495 (1.000)
Step: 901941, Reward: -92.3979 [44.88], Avg: -130.8602 (1.000)
Step: 902440, Reward: -122.6976 [23.51], Avg: -130.8858 (1.000)
Step: 904440, Reward: -111.4192 [34.31], Avg: -130.9104 (1.000)
Step: 904743, Reward: -111.7246 [41.29], Avg: -130.9471 (1.000)
Step: 906743, Reward: -143.0860 [53.08], Avg: -131.0551 (1.000)
Step: 908743, Reward: -132.6668 [25.74], Avg: -131.1003 (1.000)
Step: 909044, Reward: -140.9703 [17.52], Avg: -131.1455 (1.000)
Step: 911044, Reward: -149.8680 [30.55], Avg: -131.2266 (1.000)
Step: 911444, Reward: -77.0144 [40.73], Avg: -131.2045 (1.000)
Step: 913444, Reward: -114.8153 [29.56], Avg: -131.2261 (1.000)
Step: 915444, Reward: -99.3261 [43.73], Avg: -131.2455 (1.000)
Step: 916143, Reward: -102.5799 [32.16], Avg: -131.2512 (1.000)
Step: 916755, Reward: -65.4090 [46.21], Avg: -131.2191 (1.000)
Step: 918755, Reward: -94.7158 [16.41], Avg: -131.1863 (1.000)
Step: 920755, Reward: -125.4305 [20.48], Avg: -131.2103 (1.000)
Step: 922755, Reward: -110.2319 [38.77], Avg: -131.2393 (1.000)
Step: 923108, Reward: -97.6507 [32.61], Avg: -131.2377 (1.000)
Step: 923239, Reward: -96.9706 [43.92], Avg: -131.2533 (1.000)
Step: 923996, Reward: -85.4154 [37.17], Avg: -131.2393 (1.000)
Step: 925117, Reward: -75.3875 [43.33], Avg: -131.2191 (1.000)
Step: 925914, Reward: -69.1665 [61.55], Avg: -131.2183 (1.000)
Step: 927914, Reward: -107.9591 [17.61], Avg: -131.2092 (1.000)
Step: 928345, Reward: -87.1186 [15.69], Avg: -131.1635 (1.000)
Step: 930345, Reward: -91.6255 [62.47], Avg: -131.2003 (1.000)
Step: 930588, Reward: -136.9550 [29.60], Avg: -131.2569 (1.000)
Step: 930906, Reward: -110.8554 [36.84], Avg: -131.2832 (1.000)
Step: 932906, Reward: -62.4966 [62.40], Avg: -131.2730 (1.000)
Step: 933742, Reward: -110.7115 [25.00], Avg: -131.2801 (1.000)
Step: 933884, Reward: -132.8952 [21.07], Avg: -131.3163 (1.000)
Step: 935884, Reward: -133.7772 [35.06], Avg: -131.3759 (1.000)
Step: 936634, Reward: -135.9541 [39.99], Avg: -131.4466 (1.000)
Step: 938634, Reward: -132.1159 [27.33], Avg: -131.4910 (1.000)
Step: 940240, Reward: -113.1800 [34.87], Avg: -131.5172 (1.000)
Step: 942240, Reward: -108.7410 [33.34], Avg: -131.5339 (1.000)
Step: 944240, Reward: -129.0510 [10.74], Avg: -131.5469 (1.000)
Step: 944932, Reward: -116.1821 [69.78], Avg: -131.6326 (1.000)
Step: 946932, Reward: -106.0040 [8.94], Avg: -131.6064 (1.000)
Step: 948550, Reward: -117.2004 [25.58], Avg: -131.6239 (1.000)
Step: 950550, Reward: -81.6799 [71.82], Avg: -131.6582 (1.000)
Step: 952550, Reward: -133.7053 [24.45], Avg: -131.6997 (1.000)
Step: 954550, Reward: -106.1301 [37.52], Avg: -131.7183 (1.000)
Step: 954953, Reward: -92.2408 [63.38], Avg: -131.7556 (1.000)
Step: 956953, Reward: -124.6336 [24.14], Avg: -131.7821 (1.000)
Step: 957764, Reward: -130.8755 [47.41], Avg: -131.8545 (1.000)
Step: 957939, Reward: -103.9906 [26.74], Avg: -131.8527 (1.000)
Step: 959672, Reward: -142.7383 [45.78], Avg: -131.9406 (1.000)
Step: 961672, Reward: -90.9969 [69.70], Avg: -131.9851 (1.000)
Step: 962934, Reward: -111.2221 [43.69], Avg: -132.0205 (1.000)
Step: 964934, Reward: -149.1097 [55.72], Avg: -132.1329 (1.000)
Step: 966934, Reward: -82.8845 [21.59], Avg: -132.0903 (1.000)
Step: 968934, Reward: -82.3929 [70.62], Avg: -132.1225 (1.000)
Step: 970934, Reward: -86.2410 [63.57], Avg: -132.1496 (1.000)
Step: 971060, Reward: -121.1896 [28.58], Avg: -132.1767 (1.000)
Step: 973060, Reward: -52.1427 [66.83], Avg: -132.1564 (1.000)
Step: 974024, Reward: -110.7476 [26.36], Avg: -132.1640 (1.000)
Step: 976024, Reward: -73.6204 [84.52], Avg: -132.2037 (1.000)
Step: 978024, Reward: -111.9971 [57.02], Avg: -132.2598 (1.000)
Step: 978264, Reward: -113.1741 [35.29], Avg: -132.2845 (1.000)
Step: 978651, Reward: -157.0312 [27.03], Avg: -132.3631 (1.000)
Step: 978852, Reward: -87.8368 [54.79], Avg: -132.3787 (1.000)
Step: 980809, Reward: -68.9602 [52.95], Avg: -132.3629 (1.000)
Step: 981476, Reward: -106.3750 [17.27], Avg: -132.3497 (1.000)
Step: 983476, Reward: -90.2626 [63.12], Avg: -132.3815 (1.000)
Step: 985476, Reward: -96.0321 [13.83], Avg: -132.3475 (1.000)
Step: 985625, Reward: -131.4491 [26.63], Avg: -132.3862 (1.000)
Step: 987625, Reward: -111.8710 [38.08], Avg: -132.4127 (1.000)
Step: 989625, Reward: -112.4595 [38.65], Avg: -132.4407 (1.000)
Step: 991625, Reward: -129.5808 [41.71], Avg: -132.4990 (1.000)
Step: 991976, Reward: -51.9840 [50.18], Avg: -132.4536 (1.000)
Step: 993976, Reward: -88.6741 [15.44], Avg: -132.4112 (1.000)
Step: 995976, Reward: -112.5806 [38.14], Avg: -132.4385 (1.000)
Step: 996315, Reward: -133.3222 [57.29], Avg: -132.5252 (1.000)
Step: 998315, Reward: -136.0404 [23.81], Avg: -132.5659 (1.000)
