Model: <class 'models.ppo.PPOAgent'>, Dir: BipedalWalkerHardcore-v2
num_envs: 16, state_size: (24,), action_size: (4,), action_space: Box(4,),

import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, DISCOUNT_RATE, NUM_STEPS, ADVANTAGE_DECAY

BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 2					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=torch.scalar_tensor(1), clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) * critic_error.pow(2) * scale
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss.mean())
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage, scale=16*dones.size(0)/len(self.replay_buffer))

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[-3]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 1999, Reward: -112.7428 [9.36], Avg: -122.1000 (1.000)
Step: 2103, Reward: -115.1569 [9.46], Avg: -123.3563 (1.000)
Step: 2149, Reward: -116.5384 [10.84], Avg: -124.6976 (1.000)
Step: 4149, Reward: -105.6234 [2.89], Avg: -120.6505 (1.000)
Step: 6149, Reward: -106.8164 [8.39], Avg: -119.5626 (1.000)
Step: 6222, Reward: -133.6989 [36.96], Avg: -128.0779 (1.000)
Step: 8222, Reward: -109.4691 [8.32], Avg: -126.6079 (1.000)
Step: 8292, Reward: -112.6382 [9.37], Avg: -126.0329 (1.000)
Step: 10292, Reward: -110.8515 [8.15], Avg: -125.2511 (1.000)
Step: 12292, Reward: -112.0447 [9.78], Avg: -124.9088 (1.000)
Step: 12374, Reward: -112.3381 [9.48], Avg: -124.6276 (1.000)
Step: 12902, Reward: -126.1639 [22.97], Avg: -126.6695 (1.000)
Step: 12987, Reward: -125.2420 [31.53], Avg: -128.9850 (1.000)
Step: 14987, Reward: -134.4350 [28.96], Avg: -131.4429 (1.000)
Step: 15041, Reward: -114.0319 [16.85], Avg: -131.4056 (1.000)
Step: 17041, Reward: -138.1698 [33.51], Avg: -133.9226 (1.000)
Step: 17095, Reward: -133.6750 [30.88], Avg: -135.7245 (1.000)
Step: 18417, Reward: -112.2822 [5.42], Avg: -134.7231 (1.000)
Step: 19799, Reward: -125.0758 [27.19], Avg: -135.6465 (1.000)
Step: 19868, Reward: -142.2602 [36.09], Avg: -137.7816 (1.000)
Step: 19921, Reward: -116.8611 [19.04], Avg: -137.6919 (1.000)
Step: 21590, Reward: -116.4387 [6.62], Avg: -137.0267 (1.000)
Step: 21654, Reward: -121.6336 [17.95], Avg: -137.1378 (1.000)
Step: 21721, Reward: -115.4362 [6.02], Avg: -136.4843 (1.000)
Step: 23721, Reward: -129.3797 [29.28], Avg: -137.3714 (1.000)
Step: 25443, Reward: -111.8489 [7.84], Avg: -136.6913 (1.000)
Step: 25512, Reward: -122.2012 [15.18], Avg: -136.7168 (1.000)
Step: 27512, Reward: -110.9262 [5.78], Avg: -136.0022 (1.000)
Step: 29111, Reward: -107.6713 [6.37], Avg: -135.2450 (1.000)
Step: 31111, Reward: -139.2207 [26.78], Avg: -136.2701 (1.000)
Step: 32546, Reward: -145.1705 [39.92], Avg: -137.8449 (1.000)
Step: 34546, Reward: -109.4050 [3.20], Avg: -137.0562 (1.000)
Step: 36546, Reward: -123.4784 [28.50], Avg: -137.5083 (1.000)
Step: 38546, Reward: -115.5535 [11.56], Avg: -137.2025 (1.000)
Step: 40546, Reward: -117.1328 [8.56], Avg: -136.8735 (1.000)
Step: 42546, Reward: -136.0586 [31.68], Avg: -137.7309 (1.000)
Step: 44546, Reward: -114.3138 [7.61], Avg: -137.3036 (1.000)
Step: 46546, Reward: -127.0463 [25.35], Avg: -137.7008 (1.000)
Step: 48546, Reward: -116.5833 [6.12], Avg: -137.3163 (1.000)
Step: 50546, Reward: -118.9309 [13.65], Avg: -137.1978 (1.000)
Step: 52546, Reward: -142.2192 [27.22], Avg: -137.9843 (1.000)
Step: 54546, Reward: -144.1851 [33.79], Avg: -138.9364 (1.000)
Step: 56546, Reward: -111.7750 [12.81], Avg: -138.6026 (1.000)
Step: 58546, Reward: -115.7966 [12.49], Avg: -138.3682 (1.000)
Step: 60546, Reward: -123.0901 [30.36], Avg: -138.7033 (1.000)
Step: 62546, Reward: -137.7577 [21.55], Avg: -139.1511 (1.000)
Step: 64546, Reward: -107.4318 [10.90], Avg: -138.7081 (1.000)
Step: 66546, Reward: -104.0537 [15.21], Avg: -138.3031 (1.000)
Step: 68546, Reward: -101.4427 [14.22], Avg: -137.8410 (1.000)
Step: 69394, Reward: -137.7962 [38.13], Avg: -138.6027 (1.000)
Step: 69476, Reward: -106.0936 [30.66], Avg: -138.5664 (1.000)
Step: 70246, Reward: -107.6435 [32.55], Avg: -138.5976 (1.000)
Step: 70328, Reward: -124.4388 [23.15], Avg: -138.7673 (1.000)
Step: 72328, Reward: -121.8469 [13.69], Avg: -138.7075 (1.000)
Step: 73286, Reward: -118.2949 [8.79], Avg: -138.4963 (1.000)
Step: 75286, Reward: -130.2405 [17.11], Avg: -138.6544 (1.000)
Step: 75336, Reward: -121.7145 [17.04], Avg: -138.6562 (1.000)
Step: 77336, Reward: -105.1326 [17.11], Avg: -138.3732 (1.000)
Step: 77396, Reward: -114.9418 [21.74], Avg: -138.3444 (1.000)
Step: 79396, Reward: -123.1210 [13.07], Avg: -138.3085 (1.000)
Step: 81396, Reward: -117.2671 [10.02], Avg: -138.1278 (1.000)
Step: 83396, Reward: -113.2987 [29.32], Avg: -138.2002 (1.000)
Step: 85396, Reward: -109.7226 [15.44], Avg: -137.9934 (1.000)
Step: 87396, Reward: -100.9236 [15.76], Avg: -137.6603 (1.000)
Step: 89396, Reward: -120.6475 [3.77], Avg: -137.4566 (1.000)
Step: 91396, Reward: -119.0425 [24.62], Avg: -137.5507 (1.000)
Step: 93396, Reward: -126.9496 [25.80], Avg: -137.7775 (1.000)
Step: 95396, Reward: -112.2911 [22.33], Avg: -137.7311 (1.000)
Step: 97396, Reward: -121.7123 [4.23], Avg: -137.5603 (1.000)
Step: 99396, Reward: -123.2725 [6.22], Avg: -137.4450 (1.000)
Step: 101396, Reward: -100.6218 [24.39], Avg: -137.2699 (1.000)
Step: 103396, Reward: -90.8701 [19.96], Avg: -136.9028 (1.000)
Step: 105396, Reward: -114.2731 [11.65], Avg: -136.7524 (1.000)
Step: 107396, Reward: -123.9026 [3.69], Avg: -136.6287 (1.000)
Step: 109396, Reward: -116.7429 [7.06], Avg: -136.4576 (1.000)
Step: 111396, Reward: -112.1193 [10.34], Avg: -136.2734 (1.000)
Step: 111463, Reward: -106.4038 [22.28], Avg: -136.1747 (1.000)
Step: 113463, Reward: -105.5767 [36.08], Avg: -136.2451 (1.000)
Step: 115463, Reward: -93.0968 [22.48], Avg: -135.9834 (1.000)
Step: 117463, Reward: -99.8693 [25.39], Avg: -135.8493 (1.000)
Step: 119463, Reward: -114.7662 [5.95], Avg: -135.6626 (1.000)
Step: 121463, Reward: -97.3408 [25.43], Avg: -135.5054 (1.000)
Step: 123463, Reward: -130.3601 [22.13], Avg: -135.7100 (1.000)
Step: 125463, Reward: -115.3582 [15.77], Avg: -135.6555 (1.000)
Step: 126076, Reward: -102.0261 [30.10], Avg: -135.6140 (1.000)
Step: 128076, Reward: -120.7593 [48.44], Avg: -136.0046 (1.000)
Step: 130076, Reward: -96.1620 [21.13], Avg: -135.7895 (1.000)
Step: 132076, Reward: -120.4773 [15.29], Avg: -135.7892 (1.000)
Step: 134076, Reward: -106.5032 [22.77], Avg: -135.7161 (1.000)
Step: 136076, Reward: -110.8696 [10.16], Avg: -135.5529 (1.000)
Step: 138076, Reward: -115.7889 [8.74], Avg: -135.4317 (1.000)
Step: 140076, Reward: -113.9374 [12.57], Avg: -135.3347 (1.000)
Step: 142076, Reward: -121.3073 [21.96], Avg: -135.4200 (1.000)
Step: 142520, Reward: -107.6020 [13.83], Avg: -135.2713 (1.000)
Step: 144520, Reward: -112.4653 [8.17], Avg: -135.1172 (1.000)
Step: 146520, Reward: -130.5561 [37.25], Avg: -135.4577 (1.000)
Step: 147070, Reward: -115.0829 [17.75], Avg: -135.4307 (1.000)
Step: 147513, Reward: -111.6421 [6.83], Avg: -135.2576 (1.000)
Step: 149513, Reward: -134.3276 [34.64], Avg: -135.5981 (1.000)
Step: 151513, Reward: -105.6409 [12.20], Avg: -135.4206 (1.000)
Step: 152474, Reward: -94.1132 [21.49], Avg: -135.2244 (1.000)
Step: 154211, Reward: -124.0866 [46.18], Avg: -135.5679 (1.000)
Step: 156211, Reward: -103.5751 [16.25], Avg: -135.4151 (1.000)
Step: 156523, Reward: -108.4133 [12.33], Avg: -135.2740 (1.000)
Step: 156766, Reward: -113.1684 [10.45], Avg: -135.1630 (1.000)
Step: 158766, Reward: -107.9278 [12.87], Avg: -135.0275 (1.000)
Step: 160766, Reward: -140.3420 [41.62], Avg: -135.4662 (1.000)
Step: 161722, Reward: -108.7782 [50.96], Avg: -135.6909 (1.000)
Step: 163722, Reward: -101.9964 [15.96], Avg: -135.5282 (1.000)
Step: 165722, Reward: -111.4544 [14.86], Avg: -135.4444 (1.000)
Step: 167509, Reward: -106.5035 [11.46], Avg: -135.2869 (1.000)
Step: 169509, Reward: -117.1436 [16.83], Avg: -135.2752 (1.000)
Step: 171509, Reward: -112.8677 [10.36], Avg: -135.1686 (1.000)
Step: 173509, Reward: -125.8055 [25.87], Avg: -135.3134 (1.000)
Step: 174812, Reward: -92.2259 [18.90], Avg: -135.1030 (1.000)
Step: 176812, Reward: -118.7236 [25.39], Avg: -135.1807 (1.000)
Step: 178812, Reward: -112.3135 [24.74], Avg: -135.1968 (1.000)
Step: 180812, Reward: -124.4212 [15.40], Avg: -135.2359 (1.000)
Step: 182812, Reward: -107.9800 [6.74], Avg: -135.0635 (1.000)
Step: 184638, Reward: -112.5731 [16.64], Avg: -135.0148 (1.000)
Step: 186638, Reward: -129.0862 [50.04], Avg: -135.3794 (1.000)
Step: 187703, Reward: -88.6329 [22.97], Avg: -135.1845 (1.000)
Step: 188587, Reward: -78.4799 [29.48], Avg: -134.9632 (1.000)
Step: 190517, Reward: -118.6985 [37.39], Avg: -135.1336 (1.000)
Step: 192045, Reward: -92.7831 [20.86], Avg: -134.9617 (1.000)
Step: 194045, Reward: -151.8772 [32.19], Avg: -135.3514 (1.000)
Step: 195285, Reward: -94.7416 [24.27], Avg: -135.2227 (1.000)
Step: 197285, Reward: -133.0493 [30.25], Avg: -135.4421 (1.000)
Step: 199285, Reward: -81.5226 [58.39], Avg: -135.4767 (1.000)
Step: 199896, Reward: -103.8614 [12.64], Avg: -135.3308 (1.000)
Step: 201896, Reward: -91.1838 [56.04], Avg: -135.4216 (1.000)
Step: 203814, Reward: -124.3809 [11.20], Avg: -135.4228 (1.000)
Step: 205814, Reward: -127.8108 [20.54], Avg: -135.5200 (1.000)
Step: 207814, Reward: -106.1776 [35.82], Avg: -135.5683 (1.000)
Step: 209814, Reward: -111.9399 [43.56], Avg: -135.7159 (1.000)
Step: 211814, Reward: -53.3146 [36.35], Avg: -135.3773 (1.000)
Step: 213814, Reward: -100.6737 [38.35], Avg: -135.4039 (1.000)
Step: 215814, Reward: -117.8717 [14.91], Avg: -135.3849 (1.000)
Step: 217814, Reward: -76.7854 [35.16], Avg: -135.2163 (1.000)
Step: 219814, Reward: -100.8381 [25.28], Avg: -135.1513 (1.000)
Step: 221814, Reward: -117.6859 [12.97], Avg: -135.1194 (1.000)
Step: 223814, Reward: -117.2683 [4.66], Avg: -135.0265 (1.000)
Step: 225814, Reward: -85.0418 [34.15], Avg: -134.9157 (1.000)
Step: 226329, Reward: -77.8320 [25.37], Avg: -134.6955 (1.000)
Step: 228329, Reward: -106.9355 [18.06], Avg: -134.6286 (1.000)
Step: 230329, Reward: -102.0485 [31.88], Avg: -134.6238 (1.000)
Step: 232329, Reward: -109.0895 [18.06], Avg: -134.5730 (1.000)
Step: 234329, Reward: -104.2178 [19.87], Avg: -134.5022 (1.000)
Step: 236329, Reward: -132.9651 [28.00], Avg: -134.6797 (1.000)
Step: 238329, Reward: -97.2015 [27.75], Avg: -134.6149 (1.000)
Step: 240329, Reward: -92.7697 [38.25], Avg: -134.5911 (1.000)
Step: 242329, Reward: -82.5082 [37.04], Avg: -134.4921 (1.000)
Step: 243238, Reward: -101.0506 [50.12], Avg: -134.6011 (1.000)
Step: 243486, Reward: -103.6374 [35.41], Avg: -134.6300 (1.000)
Step: 245486, Reward: -124.2047 [3.68], Avg: -134.5865 (1.000)
Step: 246248, Reward: -83.1298 [66.43], Avg: -134.6824 (1.000)
Step: 246662, Reward: -118.0645 [6.97], Avg: -134.6210 (1.000)
Step: 248662, Reward: -132.4184 [43.10], Avg: -134.8798 (1.000)
Step: 249274, Reward: -128.5763 [25.59], Avg: -135.0012 (1.000)
Step: 251138, Reward: -114.3766 [4.75], Avg: -134.9020 (1.000)
Step: 253138, Reward: -128.4445 [8.58], Avg: -134.9151 (1.000)
Step: 254783, Reward: -121.0261 [14.09], Avg: -134.9164 (1.000)
Step: 256783, Reward: -113.4474 [48.48], Avg: -135.0821 (1.000)
Step: 257120, Reward: -101.0200 [41.65], Avg: -135.1284 (1.000)
Step: 258086, Reward: -127.6079 [12.10], Avg: -135.1561 (1.000)
Step: 258671, Reward: -109.4310 [19.96], Avg: -135.1214 (1.000)
Step: 260671, Reward: -102.4118 [38.25], Avg: -135.1545 (1.000)
Step: 262671, Reward: -100.3038 [24.74], Avg: -135.0943 (1.000)
Step: 264671, Reward: -114.2916 [18.64], Avg: -135.0815 (1.000)
Step: 266671, Reward: -119.9175 [16.23], Avg: -135.0878 (1.000)
Step: 268671, Reward: -100.8342 [41.28], Avg: -135.1289 (1.000)
Step: 270033, Reward: -104.0353 [58.42], Avg: -135.2878 (1.000)
Step: 272033, Reward: -119.2412 [55.60], Avg: -135.5164 (1.000)
Step: 273911, Reward: -101.7501 [28.54], Avg: -135.4864 (1.000)
Step: 275581, Reward: -110.4098 [16.73], Avg: -135.4387 (1.000)
Step: 277002, Reward: -78.0797 [64.77], Avg: -135.4809 (1.000)
Step: 279002, Reward: -42.6465 [29.47], Avg: -135.1228 (1.000)
Step: 281002, Reward: -70.8455 [48.87], Avg: -135.0363 (1.000)
Step: 283002, Reward: -88.9923 [54.51], Avg: -135.0836 (1.000)
Step: 285002, Reward: -54.8275 [59.61], Avg: -134.9689 (1.000)
Step: 287002, Reward: -134.6444 [22.43], Avg: -135.0910 (1.000)
Step: 289002, Reward: -93.6291 [29.68], Avg: -135.0263 (1.000)
Step: 291002, Reward: -75.6655 [46.37], Avg: -134.9553 (1.000)
Step: 293002, Reward: -85.4906 [37.62], Avg: -134.8910 (1.000)
Step: 295002, Reward: -91.2746 [41.75], Avg: -134.8809 (1.000)
Step: 297002, Reward: -121.5381 [16.94], Avg: -134.9002 (1.000)
Step: 298244, Reward: -128.2588 [13.56], Avg: -134.9372 (1.000)
Step: 299518, Reward: -105.3926 [26.23], Avg: -134.9196 (1.000)
Step: 300682, Reward: -109.4880 [19.39], Avg: -134.8876 (1.000)
Step: 302385, Reward: -66.2170 [55.46], Avg: -134.8181 (1.000)
Step: 304385, Reward: -98.8756 [34.68], Avg: -134.8115 (1.000)
Step: 306052, Reward: -123.8507 [38.09], Avg: -134.9528 (1.000)
Step: 308052, Reward: -110.9137 [24.72], Avg: -134.9563 (1.000)
Step: 310052, Reward: -113.8147 [14.06], Avg: -134.9198 (1.000)
Step: 310816, Reward: -116.6673 [10.10], Avg: -134.8780 (1.000)
Step: 312816, Reward: -116.6530 [35.10], Avg: -134.9641 (1.000)
Step: 312989, Reward: -100.1788 [28.58], Avg: -134.9326 (1.000)
Step: 314989, Reward: -93.1766 [54.99], Avg: -134.9994 (1.000)
Step: 315087, Reward: -118.9177 [29.10], Avg: -135.0649 (1.000)
Step: 317087, Reward: -94.9045 [45.62], Avg: -135.0922 (1.000)
Step: 319087, Reward: -121.4154 [36.58], Avg: -135.2061 (1.000)
Step: 319691, Reward: -89.5889 [55.32], Avg: -135.2541 (1.000)
Step: 319858, Reward: -112.4937 [13.30], Avg: -135.2075 (1.000)
Step: 321858, Reward: -128.1990 [23.26], Avg: -135.2872 (1.000)
Step: 323858, Reward: -86.6657 [23.00], Avg: -135.1622 (1.000)
Step: 325436, Reward: -125.9082 [54.26], Avg: -135.3807 (1.000)
Step: 327436, Reward: -100.5312 [55.39], Avg: -135.4800 (1.000)
Step: 329436, Reward: -102.1444 [51.84], Avg: -135.5689 (1.000)
Step: 331436, Reward: -122.6753 [33.61], Avg: -135.6680 (1.000)
Step: 333436, Reward: -127.0953 [45.45], Avg: -135.8436 (1.000)
Step: 335436, Reward: -104.4556 [44.29], Avg: -135.9048 (1.000)
Step: 337436, Reward: -109.9847 [88.21], Avg: -136.1986 (1.000)
Step: 339436, Reward: -94.5146 [50.05], Avg: -136.2378 (1.000)
Step: 341436, Reward: -116.3067 [30.70], Avg: -136.2882 (1.000)
Step: 343436, Reward: -123.2294 [13.06], Avg: -136.2882 (1.000)
Step: 345436, Reward: -120.6895 [41.89], Avg: -136.4099 (1.000)
Step: 347436, Reward: -128.5027 [28.57], Avg: -136.5051 (1.000)
Step: 349436, Reward: -108.9016 [20.07], Avg: -136.4706 (1.000)
Step: 351436, Reward: -92.4397 [22.25], Avg: -136.3712 (1.000)
Step: 353436, Reward: -128.3427 [43.00], Avg: -136.5301 (1.000)
Step: 353796, Reward: -139.9379 [38.58], Avg: -136.7201 (1.000)
Step: 355796, Reward: -113.7208 [33.59], Avg: -136.7678 (1.000)
Step: 357796, Reward: -103.0013 [18.95], Avg: -136.7014 (1.000)
Step: 359796, Reward: -88.7763 [40.20], Avg: -136.6669 (1.000)
Step: 360991, Reward: -133.4468 [12.50], Avg: -136.7081 (1.000)
Step: 362991, Reward: -72.8941 [32.09], Avg: -136.5678 (1.000)
Step: 364991, Reward: -121.6352 [43.70], Avg: -136.6945 (1.000)
Step: 366991, Reward: -94.1181 [57.48], Avg: -136.7599 (1.000)
Step: 368991, Reward: -109.3177 [25.52], Avg: -136.7515 (1.000)
Step: 370991, Reward: -100.3159 [71.74], Avg: -136.9050 (1.000)
Step: 372991, Reward: -74.9710 [33.92], Avg: -136.7837 (1.000)
Step: 374991, Reward: -110.4104 [42.64], Avg: -136.8538 (1.000)
Step: 376991, Reward: -96.9078 [39.92], Avg: -136.8537 (1.000)
Step: 378991, Reward: -89.6693 [35.02], Avg: -136.8017 (1.000)
Step: 380991, Reward: -111.0278 [31.53], Avg: -136.8262 (1.000)
Step: 382991, Reward: -105.6006 [22.69], Avg: -136.7901 (1.000)
Step: 384991, Reward: -114.1333 [19.89], Avg: -136.7784 (1.000)
Step: 386991, Reward: -88.1510 [48.64], Avg: -136.7784 (1.000)
Step: 388991, Reward: -134.7194 [8.31], Avg: -136.8046 (1.000)
Step: 390491, Reward: -107.3620 [25.37], Avg: -136.7876 (1.000)
Step: 392491, Reward: -106.5057 [53.42], Avg: -136.8836 (1.000)
Step: 394491, Reward: -128.3233 [21.67], Avg: -136.9378 (1.000)
Step: 396491, Reward: -87.1964 [43.57], Avg: -136.9124 (1.000)
Step: 398491, Reward: -94.7691 [37.62], Avg: -136.8938 (1.000)
Step: 400491, Reward: -100.1781 [21.81], Avg: -136.8330 (1.000)
Step: 400868, Reward: -115.2432 [39.06], Avg: -136.9041 (1.000)
Step: 402868, Reward: -126.5657 [5.89], Avg: -136.8860 (1.000)
Step: 404868, Reward: -117.2988 [45.71], Avg: -136.9913 (1.000)
Step: 406868, Reward: -108.1012 [34.58], Avg: -137.0142 (1.000)
Step: 408868, Reward: -120.3812 [13.74], Avg: -137.0026 (1.000)
Step: 410868, Reward: -108.3985 [16.15], Avg: -136.9530 (1.000)
Step: 412868, Reward: -134.4403 [53.68], Avg: -137.1561 (1.000)
Step: 413068, Reward: -108.6179 [38.96], Avg: -137.1973 (1.000)
Step: 415068, Reward: -122.0815 [39.08], Avg: -137.2916 (1.000)
Step: 416364, Reward: -113.8430 [18.79], Avg: -137.2733 (1.000)
Step: 418364, Reward: -117.8328 [39.18], Avg: -137.3504 (1.000)
Step: 420364, Reward: -91.0390 [52.29], Avg: -137.3737 (1.000)
Step: 422364, Reward: -96.2228 [21.95], Avg: -137.2993 (1.000)
Step: 424364, Reward: -96.6478 [49.14], Avg: -137.3320 (1.000)
Step: 426364, Reward: -102.9308 [47.12], Avg: -137.3810 (1.000)
Step: 427099, Reward: -124.6026 [19.08], Avg: -137.4051 (1.000)
Step: 429099, Reward: -110.7033 [51.42], Avg: -137.4995 (1.000)
Step: 431099, Reward: -89.9506 [49.91], Avg: -137.5084 (1.000)
Step: 433099, Reward: -120.7167 [23.90], Avg: -137.5354 (1.000)
Step: 434928, Reward: -123.9627 [14.66], Avg: -137.5395 (1.000)
Step: 436928, Reward: -92.2843 [60.10], Avg: -137.5953 (1.000)
Step: 438928, Reward: -117.9842 [20.80], Avg: -137.5997 (1.000)
Step: 440928, Reward: -115.5103 [17.35], Avg: -137.5821 (1.000)
Step: 442928, Reward: -100.9778 [27.34], Avg: -137.5476 (1.000)
Step: 444928, Reward: -81.0768 [43.17], Avg: -137.4984 (1.000)
Step: 446928, Reward: -96.8117 [85.99], Avg: -137.6655 (1.000)
Step: 448928, Reward: -111.9724 [24.92], Avg: -137.6627 (1.000)
Step: 450928, Reward: -135.6489 [0.98], Avg: -137.6589 (1.000)
Step: 452928, Reward: -114.5551 [31.49], Avg: -137.6895 (1.000)
Step: 453649, Reward: -97.7587 [50.96], Avg: -137.7297 (1.000)
Step: 454420, Reward: -106.0446 [43.04], Avg: -137.7708 (1.000)
Step: 456420, Reward: -106.3438 [40.86], Avg: -137.8048 (1.000)
Step: 458420, Reward: -72.5748 [52.60], Avg: -137.7594 (1.000)
Step: 459610, Reward: -116.4312 [24.19], Avg: -137.7697 (1.000)
Step: 461481, Reward: -93.2741 [33.71], Avg: -137.7311 (1.000)
Step: 463481, Reward: -127.7637 [58.96], Avg: -137.9055 (1.000)
Step: 463954, Reward: -114.2450 [56.09], Avg: -138.0205 (1.000)
Step: 465954, Reward: -115.9038 [26.81], Avg: -138.0371 (1.000)
Step: 467954, Reward: -110.5925 [16.69], Avg: -137.9992 (1.000)
Step: 468677, Reward: -112.0320 [16.63], Avg: -137.9665 (1.000)
Step: 470677, Reward: -108.3582 [31.95], Avg: -137.9747 (1.000)
Step: 472677, Reward: -100.4051 [76.12], Avg: -138.1090 (1.000)
Step: 474677, Reward: -120.0811 [23.93], Avg: -138.1295 (1.000)
Step: 475437, Reward: -124.2883 [35.43], Avg: -138.2042 (1.000)
Step: 477066, Reward: -105.3331 [24.11], Avg: -138.1740 (1.000)
Step: 479066, Reward: -125.8549 [16.39], Avg: -138.1880 (1.000)
Step: 481066, Reward: -114.0087 [24.45], Avg: -138.1889 (1.000)
Step: 481503, Reward: -133.1467 [27.81], Avg: -138.2666 (1.000)
Step: 483503, Reward: -144.4699 [16.80], Avg: -138.3448 (1.000)
Step: 484295, Reward: -103.4227 [17.65], Avg: -138.2862 (1.000)
Step: 486295, Reward: -120.5124 [48.29], Avg: -138.3893 (1.000)
Step: 488295, Reward: -145.5707 [42.89], Avg: -138.5579 (1.000)
Step: 489200, Reward: -118.3378 [41.78], Avg: -138.6303 (1.000)
Step: 489506, Reward: -127.9761 [19.60], Avg: -138.6602 (1.000)
Step: 490296, Reward: -117.5646 [21.66], Avg: -138.6621 (1.000)
Step: 492296, Reward: -124.3576 [33.38], Avg: -138.7255 (1.000)
Step: 494296, Reward: -103.1149 [23.36], Avg: -138.6849 (1.000)
Step: 496296, Reward: -130.7260 [22.70], Avg: -138.7335 (1.000)
Step: 498296, Reward: -110.9776 [39.73], Avg: -138.7729 (1.000)
Step: 499374, Reward: -129.9728 [41.13], Avg: -138.8789 (1.000)
Step: 501374, Reward: -126.4898 [20.59], Avg: -138.9057 (1.000)
Step: 502683, Reward: -116.0295 [16.73], Avg: -138.8857 (1.000)
Step: 504683, Reward: -152.7006 [45.67], Avg: -139.0788 (1.000)
Step: 506683, Reward: -119.7054 [51.21], Avg: -139.1818 (1.000)
Step: 507324, Reward: -81.2148 [56.52], Avg: -139.1772 (1.000)
Step: 507933, Reward: -90.2781 [37.70], Avg: -139.1412 (1.000)
Step: 509933, Reward: -105.0604 [46.70], Avg: -139.1816 (1.000)
Step: 510154, Reward: -122.0159 [17.35], Avg: -139.1822 (1.000)
Step: 511065, Reward: -99.1579 [25.68], Avg: -139.1365 (1.000)
Step: 511676, Reward: -70.0271 [27.65], Avg: -139.0049 (1.000)
Step: 512912, Reward: -74.9658 [43.25], Avg: -138.9391 (1.000)
Step: 514912, Reward: -111.8614 [34.20], Avg: -138.9616 (1.000)
Step: 515576, Reward: -136.9413 [35.43], Avg: -139.0667 (1.000)
Step: 515814, Reward: -75.6038 [38.28], Avg: -138.9877 (1.000)
Step: 516610, Reward: -117.2877 [32.61], Avg: -139.0218 (1.000)
Step: 518610, Reward: -117.0491 [23.81], Avg: -139.0276 (1.000)
Step: 520320, Reward: -132.7253 [49.83], Avg: -139.1627 (1.000)
Step: 522320, Reward: -125.5528 [13.11], Avg: -139.1612 (1.000)
Step: 523205, Reward: -113.7135 [24.85], Avg: -139.1593 (1.000)
Step: 525205, Reward: -98.8443 [22.86], Avg: -139.1056 (1.000)
Step: 525425, Reward: -71.7454 [47.66], Avg: -139.0452 (1.000)
Step: 527425, Reward: -52.6562 [47.18], Avg: -138.9253 (1.000)
Step: 529425, Reward: -89.8770 [39.04], Avg: -138.8948 (1.000)
Step: 531425, Reward: -142.7600 [42.17], Avg: -139.0347 (1.000)
Step: 533425, Reward: -105.3445 [31.17], Avg: -139.0270 (1.000)
Step: 535425, Reward: -92.3732 [40.14], Avg: -139.0074 (1.000)
Step: 537425, Reward: -87.2236 [7.82], Avg: -138.8749 (1.000)
Step: 539425, Reward: -84.4180 [15.22], Avg: -138.7571 (1.000)
Step: 541425, Reward: -82.5053 [76.57], Avg: -138.8179 (1.000)
Step: 543425, Reward: -111.5095 [16.22], Avg: -138.7848 (1.000)
Step: 545119, Reward: -106.0619 [59.94], Avg: -138.8658 (1.000)
Step: 545904, Reward: -107.9110 [38.92], Avg: -138.8895 (1.000)
Step: 547904, Reward: -92.0080 [29.50], Avg: -138.8381 (1.000)
Step: 548015, Reward: -86.5502 [38.29], Avg: -138.7968 (1.000)
Step: 548182, Reward: -101.4923 [35.28], Avg: -138.7908 (1.000)
Step: 550182, Reward: -130.9262 [59.46], Avg: -138.9421 (1.000)
Step: 552182, Reward: -129.2935 [14.31], Avg: -138.9558 (1.000)
Step: 552419, Reward: -96.7818 [31.18], Avg: -138.9237 (1.000)
Step: 554419, Reward: -127.9355 [49.91], Avg: -139.0369 (1.000)
Step: 556419, Reward: -138.6801 [13.62], Avg: -139.0753 (1.000)
Step: 556684, Reward: -96.3898 [37.71], Avg: -139.0609 (1.000)
Step: 558684, Reward: -102.1440 [60.10], Avg: -139.1277 (1.000)
Step: 560316, Reward: -138.9307 [5.27], Avg: -139.1423 (1.000)
Step: 561913, Reward: -143.0955 [2.49], Avg: -139.1607 (1.000)
Step: 561958, Reward: -110.6935 [25.20], Avg: -139.1514 (1.000)
Step: 563958, Reward: -126.2797 [20.68], Avg: -139.1737 (1.000)
Step: 565958, Reward: -128.3056 [42.13], Avg: -139.2625 (1.000)
Step: 567958, Reward: -80.3842 [45.65], Avg: -139.2250 (1.000)
Step: 568520, Reward: -124.7254 [26.91], Avg: -139.2600 (1.000)
Step: 570520, Reward: -102.0462 [43.97], Avg: -139.2791 (1.000)
Step: 572520, Reward: -113.4950 [31.20], Avg: -139.2943 (1.000)
Step: 572649, Reward: -132.2976 [19.00], Avg: -139.3279 (1.000)
Step: 572829, Reward: -110.6187 [50.06], Avg: -139.3876 (1.000)
Step: 574829, Reward: -104.5576 [44.21], Avg: -139.4137 (1.000)
Step: 576829, Reward: -110.6463 [41.72], Avg: -139.4497 (1.000)
Step: 578829, Reward: -100.2288 [24.98], Avg: -139.4102 (1.000)
Step: 580829, Reward: -123.4384 [20.30], Avg: -139.4221 (1.000)
Step: 581593, Reward: -92.8233 [46.44], Avg: -139.4217 (1.000)
Step: 583593, Reward: -125.8888 [21.19], Avg: -139.4427 (1.000)
Step: 585593, Reward: -115.8855 [38.51], Avg: -139.4837 (1.000)
Step: 586848, Reward: -92.6258 [32.41], Avg: -139.4442 (1.000)
Step: 587745, Reward: -114.6371 [58.99], Avg: -139.5374 (1.000)
Step: 589745, Reward: -132.7411 [34.42], Avg: -139.6124 (1.000)
Step: 591745, Reward: -112.9456 [56.45], Avg: -139.6931 (1.000)
Step: 593745, Reward: -104.4414 [52.92], Avg: -139.7409 (1.000)
Step: 595745, Reward: -98.8801 [36.37], Avg: -139.7288 (1.000)
Step: 596001, Reward: -93.4765 [42.30], Avg: -139.7181 (1.000)
Step: 598001, Reward: -75.0904 [57.77], Avg: -139.6997 (1.000)
Step: 600001, Reward: -114.7583 [23.69], Avg: -139.6964 (1.000)
Step: 602001, Reward: -102.0818 [21.17], Avg: -139.6525 (1.000)
Step: 604001, Reward: -143.2603 [14.37], Avg: -139.7004 (1.000)
Step: 606001, Reward: -99.4684 [50.08], Avg: -139.7265 (1.000)
Step: 608001, Reward: -105.8833 [36.73], Avg: -139.7341 (1.000)
Step: 610001, Reward: -104.6761 [59.52], Avg: -139.7987 (1.000)
Step: 611063, Reward: -107.2426 [69.41], Avg: -139.8956 (1.000)
Step: 612943, Reward: -60.1217 [33.08], Avg: -139.7731 (1.000)
Step: 614913, Reward: -106.7911 [20.67], Avg: -139.7408 (1.000)
Step: 615537, Reward: -121.2797 [55.46], Avg: -139.8375 (1.000)
Step: 617537, Reward: -103.3307 [49.45], Avg: -139.8712 (1.000)
Step: 619537, Reward: -125.5581 [27.77], Avg: -139.9061 (1.000)
Step: 621537, Reward: -48.8071 [77.20], Avg: -139.8701 (1.000)
Step: 622051, Reward: -132.9496 [22.54], Avg: -139.9104 (1.000)
Step: 624051, Reward: -147.6636 [25.87], Avg: -139.9971 (1.000)
Step: 626051, Reward: -129.6010 [29.68], Avg: -140.0467 (1.000)
Step: 628051, Reward: -141.7982 [6.73], Avg: -140.0684 (1.000)
Step: 630051, Reward: -97.5340 [46.58], Avg: -140.0788 (1.000)
Step: 632051, Reward: -141.5982 [13.76], Avg: -140.1177 (1.000)
Step: 634051, Reward: -126.2284 [25.26], Avg: -140.1467 (1.000)
Step: 636051, Reward: -128.2673 [25.02], Avg: -140.1800 (1.000)
Step: 636494, Reward: -115.8429 [76.74], Avg: -140.3127 (1.000)
Step: 638494, Reward: -77.0361 [67.88], Avg: -140.3243 (1.000)
Step: 640494, Reward: -147.9324 [32.38], Avg: -140.4250 (1.000)
Step: 642494, Reward: -111.8547 [21.16], Avg: -140.4064 (1.000)
Step: 644494, Reward: -133.8272 [11.94], Avg: -140.4198 (1.000)
Step: 646494, Reward: -127.3650 [34.37], Avg: -140.4731 (1.000)
Step: 648494, Reward: -126.9220 [22.33], Avg: -140.4950 (1.000)
Step: 648704, Reward: -100.6459 [52.64], Avg: -140.5268 (1.000)
Step: 649641, Reward: -67.3024 [73.43], Avg: -140.5273 (1.000)
Step: 651641, Reward: -95.5966 [36.87], Avg: -140.5074 (1.000)
Step: 652522, Reward: -109.8056 [35.06], Avg: -140.5181 (1.000)
Step: 654522, Reward: -71.9349 [51.92], Avg: -140.4771 (1.000)
Step: 655184, Reward: -139.3726 [19.76], Avg: -140.5229 (1.000)
Step: 657184, Reward: -107.5414 [43.83], Avg: -140.5495 (1.000)
Step: 659184, Reward: -123.0185 [52.30], Avg: -140.6345 (1.000)
Step: 660368, Reward: -85.9397 [67.75], Avg: -140.6664 (1.000)
Step: 662368, Reward: -129.4877 [24.96], Avg: -140.6999 (1.000)
Step: 664368, Reward: -124.4752 [25.96], Avg: -140.7235 (1.000)
Step: 665355, Reward: -71.1786 [71.54], Avg: -140.7283 (1.000)
Step: 665645, Reward: -89.8522 [48.55], Avg: -140.7227 (1.000)
Step: 667645, Reward: -118.6231 [29.59], Avg: -140.7408 (1.000)
Step: 669645, Reward: -88.9878 [55.95], Avg: -140.7509 (1.000)
Step: 670069, Reward: -107.0293 [22.89], Avg: -140.7249 (1.000)
Step: 672069, Reward: -98.0803 [84.37], Avg: -140.8247 (1.000)
Step: 674069, Reward: -140.0213 [9.14], Avg: -140.8446 (1.000)
Step: 676069, Reward: -110.9812 [34.05], Avg: -140.8546 (1.000)
Step: 678069, Reward: -102.7285 [49.02], Avg: -140.8804 (1.000)
Step: 680069, Reward: -113.4787 [34.30], Avg: -140.8968 (1.000)
Step: 682069, Reward: -154.1951 [20.87], Avg: -140.9776 (1.000)
Step: 684069, Reward: -111.5089 [24.92], Avg: -140.9668 (1.000)
Step: 684579, Reward: -115.0715 [25.02], Avg: -140.9648 (1.000)
Step: 686579, Reward: -158.8128 [40.03], Avg: -141.1007 (1.000)
Step: 688579, Reward: -127.9195 [34.89], Avg: -141.1515 (1.000)
Step: 690579, Reward: -139.7067 [18.19], Avg: -141.1906 (1.000)
Step: 692579, Reward: -165.1201 [28.91], Avg: -141.3138 (1.000)
Step: 693707, Reward: -104.5313 [26.25], Avg: -141.2893 (1.000)
Step: 695707, Reward: -122.8711 [36.06], Avg: -141.3302 (1.000)
Step: 697707, Reward: -113.5727 [48.56], Avg: -141.3784 (1.000)
Step: 699707, Reward: -123.6122 [44.27], Avg: -141.4396 (1.000)
Step: 701707, Reward: -106.3322 [33.98], Avg: -141.4370 (1.000)
Step: 703707, Reward: -106.6878 [24.63], Avg: -141.4137 (1.000)
Step: 705707, Reward: -124.2144 [29.78], Avg: -141.4426 (1.000)
Step: 707707, Reward: -139.5504 [15.99], Avg: -141.4749 (1.000)
Step: 709707, Reward: -154.6691 [22.78], Avg: -141.5570 (1.000)
Step: 711707, Reward: -81.5964 [65.47], Avg: -141.5695 (1.000)
Step: 713707, Reward: -130.3677 [25.85], Avg: -141.6028 (1.000)
Step: 714204, Reward: -137.2223 [36.19], Avg: -141.6749 (1.000)
Step: 716204, Reward: -117.6396 [68.27], Avg: -141.7750 (1.000)
Step: 718204, Reward: -129.3063 [14.45], Avg: -141.7795 (1.000)
Step: 720086, Reward: -106.3151 [52.46], Avg: -141.8178 (1.000)
Step: 722086, Reward: -143.8019 [26.41], Avg: -141.8816 (1.000)
Step: 724086, Reward: -133.4080 [22.82], Avg: -141.9137 (1.000)
Step: 726086, Reward: -98.6424 [46.17], Avg: -141.9202 (1.000)
Step: 726367, Reward: -115.5395 [18.41], Avg: -141.9024 (1.000)
Step: 728367, Reward: -69.9833 [58.62], Avg: -141.8728 (1.000)
Step: 728906, Reward: -125.7266 [42.38], Avg: -141.9311 (1.000)
Step: 730906, Reward: -109.0490 [26.21], Avg: -141.9163 (1.000)
Step: 732906, Reward: -79.1235 [50.99], Avg: -141.8902 (1.000)
Step: 732950, Reward: -135.0458 [13.53], Avg: -141.9050 (1.000)
Step: 734950, Reward: -99.0723 [52.41], Avg: -141.9261 (1.000)
Step: 736950, Reward: -125.0423 [24.05], Avg: -141.9418 (1.000)
Step: 738950, Reward: -113.4658 [22.98], Avg: -141.9298 (1.000)
Step: 739340, Reward: -58.2885 [76.47], Avg: -141.9141 (1.000)
Step: 740326, Reward: -122.2521 [60.40], Avg: -142.0030 (1.000)
Step: 742326, Reward: -113.4531 [34.43], Avg: -142.0158 (1.000)
Step: 744326, Reward: -142.2072 [38.10], Avg: -142.0991 (1.000)
Step: 746326, Reward: -118.4275 [24.49], Avg: -142.1009 (1.000)
Step: 746555, Reward: -131.4655 [23.00], Avg: -142.1276 (1.000)
Step: 748555, Reward: -125.6678 [24.59], Avg: -142.1452 (1.000)
Step: 750555, Reward: -134.5775 [38.42], Avg: -142.2117 (1.000)
Step: 752555, Reward: -127.5912 [28.89], Avg: -142.2424 (1.000)
Step: 753014, Reward: -135.4020 [20.44], Avg: -142.2716 (1.000)
Step: 755014, Reward: -75.7572 [33.27], Avg: -142.2004 (1.000)
Step: 756953, Reward: -99.4785 [57.09], Avg: -142.2311 (1.000)
Step: 758953, Reward: -105.8033 [43.81], Avg: -142.2468 (1.000)
Step: 760122, Reward: -97.4785 [14.91], Avg: -142.1833 (1.000)
Step: 762122, Reward: -113.0698 [61.74], Avg: -142.2526 (1.000)
Step: 764122, Reward: -152.8342 [5.57], Avg: -142.2868 (1.000)
Step: 766122, Reward: -140.9244 [37.42], Avg: -142.3630 (1.000)
Step: 768122, Reward: -126.3452 [28.53], Avg: -142.3894 (1.000)
Step: 770122, Reward: -135.7074 [67.57], Avg: -142.5176 (1.000)
Step: 771034, Reward: -113.4599 [21.93], Avg: -142.5026 (1.000)
Step: 773034, Reward: -103.4618 [40.25], Avg: -142.5052 (1.000)
Step: 775034, Reward: -106.2623 [28.43], Avg: -142.4889 (1.000)
Step: 777034, Reward: -98.3557 [60.90], Avg: -142.5239 (1.000)
Step: 777302, Reward: -144.0739 [13.65], Avg: -142.5555 (1.000)
Step: 777980, Reward: -113.5160 [33.93], Avg: -142.5657 (1.000)
Step: 779980, Reward: -85.0595 [61.03], Avg: -142.5730 (1.000)
Step: 781980, Reward: -142.3931 [30.27], Avg: -142.6353 (1.000)
Step: 782290, Reward: -98.4794 [36.20], Avg: -142.6189 (1.000)
Step: 784290, Reward: -88.6683 [15.08], Avg: -142.5387 (1.000)
Step: 786290, Reward: -89.0836 [81.92], Avg: -142.5973 (1.000)
Step: 787279, Reward: -126.3046 [27.15], Avg: -142.6195 (1.000)
Step: 787773, Reward: -94.5443 [51.13], Avg: -142.6258 (1.000)
Step: 788163, Reward: -119.4296 [29.56], Avg: -142.6388 (1.000)
Step: 789226, Reward: -103.1886 [41.72], Avg: -142.6435 (1.000)
Step: 791226, Reward: -87.2066 [55.71], Avg: -142.6440 (1.000)
Step: 792760, Reward: -124.0893 [25.73], Avg: -142.6586 (1.000)
Step: 793547, Reward: -81.7667 [55.42], Avg: -142.6475 (1.000)
Step: 795547, Reward: -119.4505 [24.30], Avg: -142.6497 (1.000)
Step: 795925, Reward: -116.7079 [34.17], Avg: -142.6664 (1.000)
Step: 796194, Reward: -156.7714 [53.97], Avg: -142.8036 (1.000)
Step: 796419, Reward: -125.1973 [26.90], Avg: -142.8223 (1.000)
Step: 796611, Reward: -93.3460 [57.60], Avg: -142.8386 (1.000)
Step: 798611, Reward: -117.9318 [36.13], Avg: -142.8611 (1.000)
Step: 800611, Reward: -107.4876 [28.38], Avg: -142.8471 (1.000)
Step: 802611, Reward: -142.0176 [40.25], Avg: -142.9258 (1.000)
Step: 803043, Reward: -114.5949 [27.46], Avg: -142.9241 (1.000)
Step: 804084, Reward: -93.5748 [50.52], Avg: -142.9264 (1.000)
Step: 806084, Reward: -155.7902 [2.54], Avg: -142.9570 (1.000)
Step: 808084, Reward: -133.3825 [39.70], Avg: -143.0166 (1.000)
Step: 810084, Reward: -114.1106 [29.29], Avg: -143.0174 (1.000)
Step: 812084, Reward: -131.3520 [50.54], Avg: -143.0941 (1.000)
Step: 814084, Reward: -109.6023 [29.98], Avg: -143.0872 (1.000)
Step: 814237, Reward: -105.1182 [11.27], Avg: -143.0347 (1.000)
Step: 816237, Reward: -137.7052 [21.52], Avg: -143.0665 (1.000)
Step: 816846, Reward: -119.0527 [65.03], Avg: -143.1468 (1.000)
Step: 818846, Reward: -128.6418 [15.60], Avg: -143.1489 (1.000)
Step: 820368, Reward: -112.1571 [37.71], Avg: -143.1620 (1.000)
Step: 822368, Reward: -80.2181 [85.19], Avg: -143.2053 (1.000)
Step: 824368, Reward: -103.6700 [66.55], Avg: -143.2577 (1.000)
Step: 826368, Reward: -116.1579 [47.12], Avg: -143.2965 (1.000)
Step: 828368, Reward: -112.1600 [59.71], Avg: -143.3518 (1.000)
Step: 830368, Reward: -114.9226 [49.60], Avg: -143.3927 (1.000)
Step: 830872, Reward: -71.6056 [73.41], Avg: -143.3958 (1.000)
Step: 832872, Reward: -117.3688 [45.87], Avg: -143.4339 (1.000)
Step: 834872, Reward: -134.7193 [28.02], Avg: -143.4710 (1.000)
Step: 836872, Reward: -128.2852 [54.77], Avg: -143.5468 (1.000)
Step: 838872, Reward: -177.4480 [37.32], Avg: -143.6830 (1.000)
Step: 840872, Reward: -132.7319 [33.84], Avg: -143.7267 (1.000)
Step: 841760, Reward: -131.4369 [27.14], Avg: -143.7550 (1.000)
Step: 843760, Reward: -110.5935 [33.84], Avg: -143.7563 (1.000)
Step: 845760, Reward: -86.1239 [49.53], Avg: -143.7409 (1.000)
Step: 846066, Reward: -110.5612 [33.73], Avg: -143.7419 (1.000)
Step: 846233, Reward: -100.3079 [34.98], Avg: -143.7259 (1.000)
Step: 848233, Reward: -113.4615 [58.18], Avg: -143.7786 (1.000)
Step: 850233, Reward: -105.0159 [71.24], Avg: -143.8398 (1.000)
Step: 852233, Reward: -111.8445 [27.25], Avg: -143.8309 (1.000)
Step: 854233, Reward: -152.1131 [23.11], Avg: -143.8898 (1.000)
Step: 854903, Reward: -130.1774 [29.22], Avg: -143.9188 (1.000)
Step: 856903, Reward: -131.8760 [55.58], Avg: -144.0002 (1.000)
Step: 858903, Reward: -91.7937 [72.41], Avg: -144.0379 (1.000)
Step: 860903, Reward: -108.5587 [57.25], Avg: -144.0784 (1.000)
Step: 862903, Reward: -84.8287 [16.91], Avg: -143.9997 (1.000)
Step: 864270, Reward: -116.9687 [18.53], Avg: -143.9840 (1.000)
Step: 864410, Reward: -136.4447 [27.21], Avg: -144.0204 (1.000)
Step: 866410, Reward: -136.5913 [49.61], Avg: -144.0984 (1.000)
Step: 867473, Reward: -82.8906 [60.01], Avg: -144.0962 (1.000)
Step: 869473, Reward: -149.0870 [10.97], Avg: -144.1256 (1.000)
Step: 871473, Reward: -103.9795 [35.41], Avg: -144.1169 (1.000)
Step: 873473, Reward: -120.1852 [54.13], Avg: -144.1723 (1.000)
Step: 874585, Reward: -105.8319 [57.13], Avg: -144.2067 (1.000)
Step: 875053, Reward: -117.6646 [31.07], Avg: -144.2150 (1.000)
Step: 876504, Reward: -114.3794 [20.08], Avg: -144.1972 (1.000)
Step: 878504, Reward: -101.2572 [101.01], Avg: -144.3029 (1.000)
Step: 880504, Reward: -107.1298 [67.21], Avg: -144.3576 (1.000)
Step: 881335, Reward: -162.3030 [37.15], Avg: -144.4576 (1.000)
Step: 883075, Reward: -91.3635 [39.43], Avg: -144.4328 (1.000)
Step: 884679, Reward: -120.8214 [36.75], Avg: -144.4566 (1.000)
Step: 885187, Reward: -135.6955 [24.64], Avg: -144.4852 (1.000)
Step: 887187, Reward: -127.5671 [39.05], Avg: -144.5251 (1.000)
Step: 889187, Reward: -106.1433 [39.49], Avg: -144.5271 (1.000)
Step: 891187, Reward: -55.4019 [29.30], Avg: -144.4197 (1.000)
Step: 893187, Reward: -123.3261 [35.02], Avg: -144.4446 (1.000)
Step: 895187, Reward: -135.1498 [34.23], Avg: -144.4892 (1.000)
Step: 897187, Reward: -127.9770 [27.54], Avg: -144.5089 (1.000)
Step: 897714, Reward: -108.6930 [36.98], Avg: -144.5110 (1.000)
Step: 899714, Reward: -101.9076 [61.05], Avg: -144.5438 (1.000)
Step: 899897, Reward: -96.3029 [45.58], Avg: -144.5391 (1.000)
Step: 901897, Reward: -125.1058 [54.44], Avg: -144.6012 (1.000)
Step: 902198, Reward: -63.5687 [58.45], Avg: -144.5612 (1.000)
Step: 904198, Reward: -125.8570 [41.71], Avg: -144.6018 (1.000)
Step: 906198, Reward: -101.4979 [38.45], Avg: -144.5936 (1.000)
Step: 908198, Reward: -121.3256 [35.03], Avg: -144.6143 (1.000)
Step: 908982, Reward: -101.4354 [27.04], Avg: -144.5860 (1.000)
Step: 909204, Reward: -87.8783 [21.04], Avg: -144.5234 (1.000)
Step: 909601, Reward: -76.2318 [105.50], Avg: -144.5886 (1.000)
Step: 911601, Reward: -60.8400 [101.59], Avg: -144.6198 (1.000)
Step: 912478, Reward: -56.7516 [56.36], Avg: -144.5648 (1.000)
Step: 912979, Reward: -70.2346 [113.59], Avg: -144.6332 (1.000)
Step: 914979, Reward: -150.9432 [57.37], Avg: -144.7439 (1.000)
Step: 916979, Reward: -49.7480 [75.15], Avg: -144.7095 (1.000)
Step: 917502, Reward: -113.3654 [42.80], Avg: -144.7293 (1.000)
Step: 919502, Reward: -113.6050 [31.14], Avg: -144.7294 (1.000)
Step: 921502, Reward: -121.5323 [19.47], Avg: -144.7229 (1.000)
Step: 923502, Reward: -119.9479 [30.99], Avg: -144.7336 (1.000)
Step: 923969, Reward: -113.0243 [47.47], Avg: -144.7608 (1.000)
Step: 925969, Reward: -121.0361 [24.56], Avg: -144.7622 (1.000)
Step: 926899, Reward: -71.6884 [69.65], Avg: -144.7563 (1.000)
Step: 927318, Reward: -133.7165 [29.87], Avg: -144.7886 (1.000)
Step: 929318, Reward: -119.8143 [40.25], Avg: -144.8147 (1.000)
Step: 931318, Reward: -69.5013 [25.00], Avg: -144.7288 (1.000)
Step: 933111, Reward: -75.0241 [109.05], Avg: -144.7959 (1.000)
Step: 935111, Reward: -69.2278 [58.89], Avg: -144.7675 (1.000)
Step: 937111, Reward: -109.8004 [71.47], Avg: -144.8295 (1.000)
Step: 937569, Reward: -117.0470 [31.80], Avg: -144.8363 (1.000)
Step: 939569, Reward: -71.5375 [53.40], Avg: -144.8026 (1.000)
Step: 939790, Reward: -112.1681 [37.59], Avg: -144.8110 (1.000)
Step: 940582, Reward: -133.0658 [35.57], Avg: -144.8511 (1.000)
Step: 941415, Reward: -131.0205 [28.15], Avg: -144.8753 (1.000)
Step: 943415, Reward: -68.6613 [71.76], Avg: -144.8678 (1.000)
Step: 945415, Reward: -118.9203 [35.30], Avg: -144.8835 (1.000)
Step: 946371, Reward: -133.9202 [31.11], Avg: -144.9172 (1.000)
Step: 948371, Reward: -117.2426 [37.75], Avg: -144.9341 (1.000)
Step: 950371, Reward: -115.9380 [55.95], Avg: -144.9791 (1.000)
Step: 950637, Reward: -65.3530 [73.88], Avg: -144.9695 (1.000)
Step: 951628, Reward: -122.2284 [42.15], Avg: -145.0018 (1.000)
Step: 953628, Reward: -106.7818 [56.62], Avg: -145.0324 (1.000)
Step: 954334, Reward: -122.1526 [36.53], Avg: -145.0550 (1.000)
Step: 956317, Reward: -162.2870 [56.29], Avg: -145.1767 (1.000)
Step: 958317, Reward: -131.7396 [21.67], Avg: -145.1903 (1.000)
Step: 960317, Reward: -87.1616 [78.30], Avg: -145.2238 (1.000)
Step: 960649, Reward: -84.9697 [93.52], Avg: -145.2786 (1.000)
Step: 962649, Reward: -87.8191 [63.89], Avg: -145.2892 (1.000)
Step: 964649, Reward: -106.6276 [52.61], Avg: -145.3121 (1.000)
Step: 966649, Reward: -110.9815 [53.39], Avg: -145.3433 (1.000)
Step: 967030, Reward: -156.8391 [50.26], Avg: -145.4444 (1.000)
Step: 967409, Reward: -115.3966 [40.75], Avg: -145.4619 (1.000)
Step: 969409, Reward: -116.6676 [34.67], Avg: -145.4715 (1.000)
Step: 971409, Reward: -119.5510 [28.30], Avg: -145.4754 (1.000)
Step: 971641, Reward: -132.2510 [29.88], Avg: -145.5024 (1.000)
Step: 973641, Reward: -165.1240 [54.28], Avg: -145.6224 (1.000)
Step: 975641, Reward: -111.0485 [49.94], Avg: -145.6473 (1.000)
Step: 976928, Reward: -97.8360 [65.33], Avg: -145.6757 (1.000)
Step: 978928, Reward: -130.8661 [74.10], Avg: -145.7714 (1.000)
Step: 980928, Reward: -128.1591 [45.89], Avg: -145.8171 (1.000)
Step: 982685, Reward: -129.7021 [47.29], Avg: -145.8673 (1.000)
Step: 984685, Reward: -117.3638 [56.94], Avg: -145.9130 (1.000)
Step: 985220, Reward: -134.7919 [25.06], Avg: -145.9353 (1.000)
Step: 987220, Reward: -120.0730 [28.71], Avg: -145.9399 (1.000)
Step: 989220, Reward: -106.9307 [53.29], Avg: -145.9628 (1.000)
Step: 989678, Reward: -115.6042 [26.23], Avg: -145.9562 (1.000)
Step: 991678, Reward: -124.2365 [28.60], Avg: -145.9671 (1.000)
Step: 993678, Reward: -123.1383 [53.54], Avg: -146.0160 (1.000)
Step: 995216, Reward: -104.4689 [42.95], Avg: -146.0183 (1.000)
Step: 995916, Reward: -144.8495 [29.74], Avg: -146.0636 (1.000)
Step: 996652, Reward: -144.0091 [34.97], Avg: -146.1158 (1.000)
Step: 997498, Reward: -100.3544 [39.63], Avg: -146.1061 (1.000)
Step: 999498, Reward: -137.5810 [36.43], Avg: -146.1502 (1.000)
