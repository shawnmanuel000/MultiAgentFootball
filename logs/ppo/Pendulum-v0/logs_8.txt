Model: <class 'models.ppo.PPOAgent'>, Dir: Pendulum-v0
num_envs: 16, state_size: (3,), action_size: (1,), action_space: Box(1,),

import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, DISCOUNT_RATE, NUM_STEPS, ADVANTAGE_DECAY

BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 2					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=torch.scalar_tensor(1), clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) * critic_error.pow(2) * scale
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss.mean())
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage, scale=16*dones.size(0)/len(self.replay_buffer))

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[3]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 199, Reward: -979.7884 [82.80], Avg: -1062.5928 (1.000)
Step: 399, Reward: -1306.5536 [210.96], Avg: -1290.0550 (1.000)
Step: 599, Reward: -1261.4318 [236.79], Avg: -1359.4434 (1.000)
Step: 799, Reward: -1326.1523 [201.68], Avg: -1401.5408 (1.000)
Step: 999, Reward: -1192.7396 [91.55], Avg: -1378.0900 (1.000)
Step: 1199, Reward: -1156.0318 [128.37], Avg: -1362.4759 (1.000)
Step: 1399, Reward: -1268.6078 [216.93], Avg: -1380.0568 (1.000)
Step: 1599, Reward: -1122.8401 [206.85], Avg: -1373.7609 (1.000)
Step: 1799, Reward: -1094.7135 [166.14], Avg: -1361.2156 (1.000)
Step: 1999, Reward: -1089.2508 [80.82], Avg: -1342.1013 (1.000)
Step: 2199, Reward: -1058.4033 [151.77], Avg: -1330.1077 (1.000)
Step: 2399, Reward: -1047.2529 [144.83], Avg: -1318.6057 (1.000)
Step: 2599, Reward: -1210.4949 [257.52], Avg: -1330.0990 (1.000)
Step: 2799, Reward: -946.4220 [93.90], Avg: -1309.4010 (1.000)
Step: 2999, Reward: -982.4149 [150.75], Avg: -1297.6518 (1.000)
Step: 3199, Reward: -1076.0033 [139.08], Avg: -1292.4911 (1.000)
Step: 3399, Reward: -1108.4622 [192.07], Avg: -1292.9642 (1.000)
Step: 3599, Reward: -1012.0833 [153.10], Avg: -1285.8652 (1.000)
Step: 3799, Reward: -993.0546 [80.54], Avg: -1274.6930 (1.000)
Step: 3999, Reward: -1013.0800 [86.47], Avg: -1265.9358 (1.000)
Step: 4199, Reward: -981.6693 [34.70], Avg: -1254.0519 (1.000)
Step: 4399, Reward: -1118.8096 [234.88], Avg: -1258.5807 (1.000)
Step: 4599, Reward: -965.4510 [103.39], Avg: -1250.3312 (1.000)
Step: 4799, Reward: -1028.6617 [30.89], Avg: -1242.3822 (1.000)
Step: 4999, Reward: -870.3075 [70.47], Avg: -1230.3179 (1.000)
Step: 5199, Reward: -1001.1716 [106.28], Avg: -1225.5924 (1.000)
Step: 5399, Reward: -1054.7307 [44.49], Avg: -1220.9120 (1.000)
Step: 5599, Reward: -991.6849 [76.06], Avg: -1215.4418 (1.000)
Step: 5799, Reward: -1094.4191 [108.54], Avg: -1215.0112 (1.000)
Step: 5999, Reward: -843.6552 [49.13], Avg: -1204.2702 (1.000)
Step: 6199, Reward: -1001.0160 [99.46], Avg: -1200.9219 (1.000)
Step: 6399, Reward: -890.9998 [139.29], Avg: -1195.5895 (1.000)
Step: 6599, Reward: -909.9865 [91.55], Avg: -1189.7091 (1.000)
Step: 6799, Reward: -934.1858 [65.26], Avg: -1184.1131 (1.000)
Step: 6999, Reward: -982.9098 [94.10], Avg: -1181.0531 (1.000)
Step: 7199, Reward: -983.1184 [87.81], Avg: -1177.9941 (1.000)
Step: 7399, Reward: -964.8598 [97.75], Avg: -1174.8756 (1.000)
Step: 7599, Reward: -1051.0285 [42.31], Avg: -1172.7299 (1.000)
Step: 7799, Reward: -1049.0419 [117.87], Avg: -1172.5808 (1.000)
Step: 7999, Reward: -1032.1573 [66.42], Avg: -1170.7306 (1.000)
Step: 8199, Reward: -1081.1923 [96.99], Avg: -1170.9125 (1.000)
Step: 8399, Reward: -994.0980 [79.76], Avg: -1168.6016 (1.000)
Step: 8599, Reward: -1025.8382 [64.33], Avg: -1166.7775 (1.000)
Step: 8799, Reward: -1033.6124 [64.48], Avg: -1165.2164 (1.000)
Step: 8999, Reward: -1027.9389 [19.55], Avg: -1162.6002 (1.000)
Step: 9199, Reward: -1088.2747 [64.90], Avg: -1162.3952 (1.000)
Step: 9399, Reward: -944.0018 [96.08], Avg: -1159.7927 (1.000)
Step: 9599, Reward: -1020.7740 [108.18], Avg: -1159.1503 (1.000)
Step: 9799, Reward: -936.5684 [89.14], Avg: -1156.4270 (1.000)
Step: 9999, Reward: -923.5159 [180.15], Avg: -1155.3719 (1.000)
Step: 10199, Reward: -1007.8615 [139.61], Avg: -1155.2170 (1.000)
Step: 10399, Reward: -929.3758 [41.38], Avg: -1151.6696 (1.000)
Step: 10599, Reward: -939.4384 [64.19], Avg: -1148.8765 (1.000)
Step: 10799, Reward: -920.5302 [109.80], Avg: -1146.6811 (1.000)
Step: 10999, Reward: -991.9860 [59.06], Avg: -1144.9424 (1.000)
Step: 11199, Reward: -930.3335 [54.16], Avg: -1142.0772 (1.000)
Step: 11399, Reward: -926.5653 [88.36], Avg: -1139.8465 (1.000)
Step: 11599, Reward: -866.2235 [67.54], Avg: -1136.2933 (1.000)
Step: 11799, Reward: -899.0625 [96.65], Avg: -1133.9106 (1.000)
Step: 11999, Reward: -931.0435 [75.20], Avg: -1131.7828 (1.000)
Step: 12199, Reward: -850.4751 [41.05], Avg: -1127.8440 (1.000)
Step: 12399, Reward: -810.4219 [50.77], Avg: -1123.5432 (1.000)
Step: 12599, Reward: -959.0215 [78.25], Avg: -1122.1737 (1.000)
Step: 12799, Reward: -917.7726 [54.83], Avg: -1119.8367 (1.000)
Step: 12999, Reward: -849.2404 [70.26], Avg: -1116.7546 (1.000)
Step: 13199, Reward: -937.0449 [67.96], Avg: -1115.0614 (1.000)
Step: 13399, Reward: -977.8574 [48.16], Avg: -1113.7323 (1.000)
Step: 13599, Reward: -861.5728 [62.68], Avg: -1110.9458 (1.000)
Step: 13799, Reward: -856.1168 [127.05], Avg: -1109.0939 (1.000)
Step: 13999, Reward: -823.5619 [126.23], Avg: -1106.8183 (1.000)
Step: 14199, Reward: -960.7258 [60.82], Avg: -1105.6172 (1.000)
Step: 14399, Reward: -815.4154 [56.50], Avg: -1102.3714 (1.000)
Step: 14599, Reward: -860.2097 [107.72], Avg: -1100.5297 (1.000)
Step: 14799, Reward: -770.5467 [77.17], Avg: -1097.1133 (1.000)
Step: 14999, Reward: -807.3714 [59.03], Avg: -1094.0371 (1.000)
Step: 15199, Reward: -925.6816 [96.03], Avg: -1093.0855 (1.000)
Step: 15399, Reward: -841.1734 [80.21], Avg: -1090.8556 (1.000)
Step: 15599, Reward: -910.0872 [68.91], Avg: -1089.4215 (1.000)
Step: 15799, Reward: -878.0886 [17.28], Avg: -1086.9651 (1.000)
Step: 15999, Reward: -798.7640 [54.23], Avg: -1084.0405 (1.000)
Step: 16199, Reward: -881.3211 [66.55], Avg: -1082.3594 (1.000)
Step: 16399, Reward: -830.7044 [136.79], Avg: -1080.9586 (1.000)
Step: 16599, Reward: -870.5308 [109.13], Avg: -1079.7382 (1.000)
Step: 16799, Reward: -812.8923 [79.94], Avg: -1077.5131 (1.000)
Step: 16999, Reward: -819.4118 [50.53], Avg: -1075.0711 (1.000)
Step: 17199, Reward: -843.5706 [53.92], Avg: -1073.0063 (1.000)
Step: 17399, Reward: -844.2579 [81.77], Avg: -1071.3169 (1.000)
Step: 17599, Reward: -882.3967 [52.07], Avg: -1069.7617 (1.000)
Step: 17799, Reward: -822.5500 [60.73], Avg: -1067.6664 (1.000)
Step: 17999, Reward: -730.8291 [94.67], Avg: -1064.9757 (1.000)
Step: 18199, Reward: -814.3828 [61.50], Avg: -1062.8978 (1.000)
Step: 18399, Reward: -735.2994 [48.59], Avg: -1059.8651 (1.000)
Step: 18599, Reward: -756.2192 [69.39], Avg: -1057.3462 (1.000)
Step: 18799, Reward: -743.5897 [67.21], Avg: -1054.7234 (1.000)
Step: 18999, Reward: -729.5860 [83.88], Avg: -1052.1838 (1.000)
Step: 19199, Reward: -806.2895 [57.51], Avg: -1050.2214 (1.000)
Step: 19399, Reward: -753.2835 [17.33], Avg: -1047.3389 (1.000)
Step: 19599, Reward: -756.7101 [83.30], Avg: -1045.2232 (1.000)
Step: 19799, Reward: -733.3412 [49.14], Avg: -1042.5693 (1.000)
Step: 19999, Reward: -728.0555 [51.88], Avg: -1039.9429 (1.000)
Step: 20199, Reward: -719.3221 [85.30], Avg: -1037.6130 (1.000)
Step: 20399, Reward: -772.0139 [76.78], Avg: -1035.7618 (1.000)
Step: 20599, Reward: -813.8850 [39.95], Avg: -1033.9955 (1.000)
Step: 20799, Reward: -712.9296 [45.53], Avg: -1031.3461 (1.000)
Step: 20999, Reward: -749.2739 [11.26], Avg: -1028.7670 (1.000)
Step: 21199, Reward: -729.4870 [48.90], Avg: -1026.4049 (1.000)
Step: 21399, Reward: -742.6151 [15.41], Avg: -1023.8967 (1.000)
Step: 21599, Reward: -788.7694 [40.72], Avg: -1022.0966 (1.000)
Step: 21799, Reward: -687.9099 [58.15], Avg: -1019.5642 (1.000)
Step: 21999, Reward: -750.8711 [96.74], Avg: -1018.0010 (1.000)
Step: 22199, Reward: -730.2840 [54.55], Avg: -1015.9004 (1.000)
Step: 22399, Reward: -723.9816 [85.49], Avg: -1014.0573 (1.000)
Step: 22599, Reward: -750.1741 [62.58], Avg: -1012.2758 (1.000)
Step: 22799, Reward: -710.0538 [60.82], Avg: -1010.1583 (1.000)
Step: 22999, Reward: -711.4841 [40.89], Avg: -1007.9166 (1.000)
Step: 23199, Reward: -756.1759 [35.49], Avg: -1006.0524 (1.000)
Step: 23399, Reward: -736.1778 [83.78], Avg: -1004.4619 (1.000)
Step: 23599, Reward: -700.0372 [63.08], Avg: -1002.4166 (1.000)
Step: 23799, Reward: -708.0681 [43.23], Avg: -1000.3064 (1.000)
Step: 23999, Reward: -670.4147 [58.16], Avg: -998.0420 (1.000)
Step: 24199, Reward: -651.5647 [90.84], Avg: -995.9293 (1.000)
Step: 24399, Reward: -640.2086 [95.05], Avg: -993.7927 (1.000)
Step: 24599, Reward: -672.0040 [71.57], Avg: -991.7583 (1.000)
Step: 24799, Reward: -601.9846 [50.12], Avg: -989.0192 (1.000)
Step: 24999, Reward: -568.0565 [66.65], Avg: -986.1847 (1.000)
Step: 25199, Reward: -538.9902 [79.39], Avg: -983.2656 (1.000)
Step: 25399, Reward: -589.0714 [87.74], Avg: -980.8526 (1.000)
Step: 25599, Reward: -561.8306 [82.00], Avg: -978.2196 (1.000)
Step: 25799, Reward: -604.8080 [89.22], Avg: -976.0166 (1.000)
Step: 25999, Reward: -598.3112 [91.53], Avg: -973.8152 (1.000)
Step: 26199, Reward: -593.8332 [190.46], Avg: -972.3685 (1.000)
Step: 26399, Reward: -566.0417 [120.79], Avg: -970.2054 (1.000)
Step: 26599, Reward: -489.0300 [75.01], Avg: -967.1515 (1.000)
Step: 26799, Reward: -527.4594 [47.64], Avg: -964.2257 (1.000)
Step: 26999, Reward: -452.3047 [59.43], Avg: -960.8740 (1.000)
Step: 27199, Reward: -547.5831 [159.59], Avg: -959.0085 (1.000)
Step: 27399, Reward: -400.7010 [87.93], Avg: -955.5751 (1.000)
Step: 27599, Reward: -598.3672 [141.06], Avg: -954.0088 (1.000)
Step: 27799, Reward: -400.3495 [125.27], Avg: -950.9269 (1.000)
Step: 27999, Reward: -479.5135 [104.52], Avg: -948.3062 (1.000)
Step: 28199, Reward: -371.4775 [189.57], Avg: -945.5597 (1.000)
Step: 28399, Reward: -397.9274 [130.37], Avg: -942.6212 (1.000)
Step: 28599, Reward: -366.3059 [73.34], Avg: -939.1039 (1.000)
Step: 28799, Reward: -452.3001 [242.94], Avg: -937.4104 (1.000)
Step: 28999, Reward: -224.3454 [166.85], Avg: -933.6434 (1.000)
Step: 29199, Reward: -273.2069 [102.45], Avg: -929.8216 (1.000)
Step: 29399, Reward: -206.6583 [175.62], Avg: -926.0967 (1.000)
Step: 29599, Reward: -172.6827 [61.68], Avg: -921.4228 (1.000)
Step: 29799, Reward: -303.3405 [152.02], Avg: -918.2949 (1.000)
Step: 29999, Reward: -123.9051 [76.17], Avg: -913.5068 (1.000)
Step: 30199, Reward: -199.4961 [170.61], Avg: -909.9081 (1.000)
Step: 30399, Reward: -171.3338 [99.88], Avg: -905.7061 (1.000)
Step: 30599, Reward: -147.3214 [89.76], Avg: -901.3360 (1.000)
Step: 30799, Reward: -125.7624 [111.17], Avg: -897.0218 (1.000)
Step: 30999, Reward: -186.5931 [109.91], Avg: -893.1474 (1.000)
Step: 31199, Reward: -165.7990 [81.11], Avg: -889.0048 (1.000)
Step: 31399, Reward: -148.9250 [50.72], Avg: -884.6140 (1.000)
Step: 31599, Reward: -227.5497 [97.93], Avg: -881.0752 (1.000)
Step: 31799, Reward: -193.2810 [98.01], Avg: -877.3659 (1.000)
Step: 31999, Reward: -124.2371 [76.22], Avg: -873.1352 (1.000)
Step: 32199, Reward: -121.8351 [72.18], Avg: -868.9171 (1.000)
Step: 32399, Reward: -121.5991 [75.81], Avg: -864.7720 (1.000)
Step: 32599, Reward: -169.4336 [123.70], Avg: -861.2650 (1.000)
Step: 32799, Reward: -145.3824 [44.58], Avg: -857.1717 (1.000)
Step: 32999, Reward: -148.1604 [118.46], Avg: -853.5926 (1.000)
Step: 33199, Reward: -166.8806 [55.53], Avg: -849.7903 (1.000)
Step: 33399, Reward: -175.8138 [67.24], Avg: -846.1572 (1.000)
Step: 33599, Reward: -175.4215 [59.11], Avg: -842.5165 (1.000)
Step: 33799, Reward: -97.1686 [86.95], Avg: -838.6207 (1.000)
Step: 33999, Reward: -174.4591 [97.78], Avg: -835.2890 (1.000)
Step: 34199, Reward: -222.3852 [91.15], Avg: -832.2379 (1.000)
Step: 34399, Reward: -207.3843 [69.92], Avg: -829.0115 (1.000)
Step: 34599, Reward: -147.8258 [45.62], Avg: -825.3377 (1.000)
Step: 34799, Reward: -214.7431 [114.03], Avg: -822.4839 (1.000)
Step: 34999, Reward: -193.7584 [94.60], Avg: -819.4317 (1.000)
Step: 35199, Reward: -155.5193 [97.74], Avg: -816.2149 (1.000)
Step: 35399, Reward: -78.1760 [59.05], Avg: -812.3788 (1.000)
Step: 35599, Reward: -172.5065 [98.09], Avg: -809.3351 (1.000)
Step: 35799, Reward: -196.2019 [59.86], Avg: -806.2442 (1.000)
Step: 35999, Reward: -251.9734 [161.35], Avg: -804.0612 (1.000)
Step: 36199, Reward: -137.5449 [126.55], Avg: -801.0780 (1.000)
Step: 36399, Reward: -144.7799 [90.08], Avg: -797.9669 (1.000)
Step: 36599, Reward: -188.0940 [79.43], Avg: -795.0683 (1.000)
Step: 36799, Reward: -254.0343 [75.39], Avg: -792.5376 (1.000)
Step: 36999, Reward: -192.9001 [95.63], Avg: -789.8132 (1.000)
Step: 37199, Reward: -148.3464 [93.78], Avg: -786.8687 (1.000)
Step: 37399, Reward: -170.8327 [59.43], Avg: -783.8922 (1.000)
Step: 37599, Reward: -174.8043 [59.55], Avg: -780.9691 (1.000)
Step: 37799, Reward: -120.6459 [131.62], Avg: -778.1718 (1.000)
Step: 37999, Reward: -242.7330 [1.74], Avg: -775.3628 (1.000)
Step: 38199, Reward: -99.6866 [92.73], Avg: -772.3107 (1.000)
Step: 38399, Reward: -147.5271 [90.32], Avg: -769.5271 (1.000)
Step: 38599, Reward: -121.1655 [73.19], Avg: -766.5469 (1.000)
Step: 38799, Reward: -148.7851 [89.96], Avg: -763.8263 (1.000)
Step: 38999, Reward: -152.1891 [95.79], Avg: -761.1809 (1.000)
Step: 39199, Reward: -144.9697 [50.38], Avg: -758.2940 (1.000)
Step: 39399, Reward: -169.2865 [55.42], Avg: -755.5855 (1.000)
Step: 39599, Reward: -220.1566 [139.66], Avg: -753.5866 (1.000)
Step: 39799, Reward: -149.4868 [45.93], Avg: -750.7818 (1.000)
Step: 39999, Reward: -165.3213 [55.43], Avg: -748.1316 (1.000)
Step: 40199, Reward: -280.7365 [105.14], Avg: -746.3293 (1.000)
Step: 40399, Reward: -100.3512 [92.82], Avg: -743.5909 (1.000)
Step: 40599, Reward: -123.8561 [3.39], Avg: -740.5547 (1.000)
Step: 40799, Reward: -242.1497 [80.83], Avg: -738.5078 (1.000)
Step: 40999, Reward: -198.9050 [98.62], Avg: -736.3566 (1.000)
Step: 41199, Reward: -97.6659 [47.92], Avg: -733.4888 (1.000)
Step: 41399, Reward: -124.2008 [77.58], Avg: -730.9202 (1.000)
Step: 41599, Reward: -101.1521 [91.39], Avg: -728.3318 (1.000)
Step: 41799, Reward: -217.0122 [138.90], Avg: -726.5499 (1.000)
Step: 41999, Reward: -126.5426 [2.17], Avg: -723.7031 (1.000)
Step: 42199, Reward: -170.7218 [121.49], Avg: -721.6581 (1.000)
Step: 42399, Reward: -128.4428 [81.41], Avg: -719.2439 (1.000)
Step: 42599, Reward: -147.3766 [89.53], Avg: -716.9794 (1.000)
Step: 42799, Reward: -122.4222 [2.43], Avg: -714.2124 (1.000)
Step: 42999, Reward: -199.3929 [105.63], Avg: -712.3092 (1.000)
Step: 43199, Reward: -119.6677 [73.66], Avg: -709.9066 (1.000)
Step: 43399, Reward: -146.5289 [46.98], Avg: -707.5268 (1.000)
Step: 43599, Reward: -147.2289 [45.55], Avg: -705.1656 (1.000)
Step: 43799, Reward: -125.7388 [83.63], Avg: -702.9017 (1.000)
Step: 43999, Reward: -171.9099 [60.67], Avg: -700.7639 (1.000)
Step: 44199, Reward: -149.6984 [53.84], Avg: -698.5140 (1.000)
Step: 44399, Reward: -173.7370 [104.55], Avg: -696.6211 (1.000)
Step: 44599, Reward: -98.2316 [48.56], Avg: -694.1555 (1.000)
Step: 44799, Reward: -166.7599 [120.47], Avg: -692.3388 (1.000)
Step: 44999, Reward: -122.4437 [74.58], Avg: -690.1374 (1.000)
Step: 45199, Reward: -167.6242 [54.94], Avg: -688.0685 (1.000)
Step: 45399, Reward: -121.1540 [76.29], Avg: -685.9072 (1.000)
Step: 45599, Reward: -121.2261 [75.64], Avg: -683.7622 (1.000)
Step: 45799, Reward: -146.6139 [50.52], Avg: -681.6372 (1.000)
Step: 45999, Reward: -191.5369 [56.60], Avg: -679.7525 (1.000)
Step: 46199, Reward: -166.3145 [84.43], Avg: -677.8953 (1.000)
Step: 46399, Reward: -171.9356 [63.41], Avg: -675.9877 (1.000)
Step: 46599, Reward: -121.0880 [71.05], Avg: -673.9111 (1.000)
Step: 46799, Reward: -123.9866 [77.31], Avg: -671.8914 (1.000)
Step: 46999, Reward: -97.6929 [89.68], Avg: -669.8296 (1.000)
Step: 47199, Reward: -120.8765 [3.89], Avg: -667.5200 (1.000)
Step: 47399, Reward: -187.2985 [135.09], Avg: -666.0638 (1.000)
Step: 47599, Reward: -26.4334 [45.60], Avg: -663.5679 (1.000)
Step: 47799, Reward: -121.4396 [71.49], Avg: -661.5987 (1.000)
Step: 47999, Reward: -172.2520 [56.12], Avg: -659.7936 (1.000)
Step: 48199, Reward: -197.4201 [94.44], Avg: -658.2669 (1.000)
Step: 48399, Reward: -146.7097 [88.16], Avg: -656.5173 (1.000)
Step: 48599, Reward: -147.2672 [43.39], Avg: -654.6002 (1.000)
Step: 48799, Reward: -206.5332 [102.10], Avg: -653.1822 (1.000)
Step: 48999, Reward: -123.1038 [76.73], Avg: -651.3318 (1.000)
Step: 49199, Reward: -176.9613 [64.31], Avg: -649.6649 (1.000)
Step: 49399, Reward: -199.2300 [60.42], Avg: -648.0859 (1.000)
Step: 49599, Reward: -124.7741 [79.12], Avg: -646.2948 (1.000)
Step: 49799, Reward: -172.1801 [59.72], Avg: -644.6305 (1.000)
Step: 49999, Reward: -121.9966 [73.39], Avg: -642.8335 (1.000)
Step: 50199, Reward: -206.0095 [107.50], Avg: -641.5215 (1.000)
Step: 50399, Reward: -144.3141 [40.48], Avg: -639.7091 (1.000)
Step: 50599, Reward: -147.7415 [48.59], Avg: -637.9566 (1.000)
Step: 50799, Reward: -148.8560 [91.23], Avg: -636.3901 (1.000)
Step: 50999, Reward: -146.6361 [90.39], Avg: -634.8240 (1.000)
Step: 51199, Reward: -157.3205 [102.56], Avg: -633.3594 (1.000)
Step: 51399, Reward: -178.6482 [73.62], Avg: -631.8765 (1.000)
Step: 51599, Reward: -146.0658 [44.91], Avg: -630.1676 (1.000)
Step: 51799, Reward: -99.4222 [49.20], Avg: -628.3084 (1.000)
Step: 51999, Reward: -173.0480 [99.12], Avg: -626.9386 (1.000)
Step: 52199, Reward: -149.4109 [89.54], Avg: -625.4520 (1.000)
Step: 52399, Reward: -183.5518 [76.72], Avg: -624.0582 (1.000)
Step: 52599, Reward: -148.6594 [52.81], Avg: -622.4514 (1.000)
Step: 52799, Reward: -145.2020 [89.92], Avg: -620.9842 (1.000)
Step: 52999, Reward: -122.2796 [75.64], Avg: -619.3878 (1.000)
Step: 53199, Reward: -205.0291 [102.92], Avg: -618.2169 (1.000)
Step: 53399, Reward: -168.4999 [120.92], Avg: -616.9855 (1.000)
Step: 53599, Reward: -127.3736 [111.78], Avg: -615.5757 (1.000)
Step: 53799, Reward: -143.3768 [88.27], Avg: -614.1484 (1.000)
Step: 53999, Reward: -118.2477 [72.00], Avg: -612.5784 (1.000)
Step: 54199, Reward: -145.4368 [87.47], Avg: -611.1774 (1.000)
Step: 54399, Reward: -268.8223 [47.72], Avg: -610.0942 (1.000)
Step: 54599, Reward: -189.5468 [88.28], Avg: -608.8771 (1.000)
Step: 54799, Reward: -247.7215 [109.50], Avg: -607.9587 (1.000)
Step: 54999, Reward: -168.8472 [118.60], Avg: -606.7932 (1.000)
Step: 55199, Reward: -191.6956 [118.60], Avg: -605.7189 (1.000)
Step: 55399, Reward: -254.5387 [76.51], Avg: -604.7273 (1.000)
Step: 55599, Reward: -238.4933 [101.99], Avg: -603.7768 (1.000)
Step: 55799, Reward: -171.5568 [98.70], Avg: -602.5813 (1.000)
Step: 55999, Reward: -95.7322 [88.11], Avg: -601.0858 (1.000)
Step: 56199, Reward: -145.4631 [44.99], Avg: -599.6245 (1.000)
Step: 56399, Reward: -148.6828 [89.33], Avg: -598.3422 (1.000)
Step: 56599, Reward: -163.1698 [114.49], Avg: -597.2090 (1.000)
Step: 56799, Reward: -214.5369 [112.64], Avg: -596.2582 (1.000)
Step: 56999, Reward: -290.4306 [96.99], Avg: -595.5255 (1.000)
Step: 57199, Reward: -147.7339 [45.19], Avg: -594.1178 (1.000)
Step: 57399, Reward: -244.2076 [75.09], Avg: -593.1602 (1.000)
Step: 57599, Reward: -176.1811 [59.54], Avg: -591.9191 (1.000)
Step: 57799, Reward: -241.2065 [71.24], Avg: -590.9521 (1.000)
Step: 57999, Reward: -102.1678 [92.87], Avg: -589.5868 (1.000)
Step: 58199, Reward: -194.6229 [96.26], Avg: -588.5604 (1.000)
Step: 58399, Reward: -147.7793 [51.90], Avg: -587.2286 (1.000)
Step: 58599, Reward: -122.1261 [106.93], Avg: -586.0062 (1.000)
Step: 58799, Reward: -192.3523 [118.75], Avg: -585.0711 (1.000)
Step: 58999, Reward: -175.7357 [59.03], Avg: -583.8837 (1.000)
Step: 59199, Reward: -145.9741 [44.63], Avg: -582.5550 (1.000)
Step: 59399, Reward: -214.7466 [86.55], Avg: -581.6080 (1.000)
Step: 59599, Reward: -221.9744 [141.50], Avg: -580.8760 (1.000)
Step: 59799, Reward: -178.5626 [63.72], Avg: -579.7436 (1.000)
Step: 59999, Reward: -234.1903 [124.42], Avg: -579.0065 (1.000)
Step: 60199, Reward: -173.6940 [57.85], Avg: -577.8521 (1.000)
Step: 60399, Reward: -168.2753 [95.83], Avg: -576.8132 (1.000)
Step: 60599, Reward: -169.7889 [93.86], Avg: -575.7797 (1.000)
Step: 60799, Reward: -125.1854 [79.46], Avg: -574.5588 (1.000)
Step: 60999, Reward: -123.5913 [73.90], Avg: -573.3226 (1.000)
Step: 61199, Reward: -199.4440 [93.27], Avg: -572.4055 (1.000)
Step: 61399, Reward: -143.6233 [83.69], Avg: -571.2815 (1.000)
Step: 61599, Reward: -191.6927 [54.90], Avg: -570.2273 (1.000)
Step: 61799, Reward: -124.2254 [76.21], Avg: -569.0305 (1.000)
Step: 61999, Reward: -186.8453 [109.37], Avg: -568.1505 (1.000)
Step: 62199, Reward: -143.2443 [109.39], Avg: -567.1360 (1.000)
Step: 62399, Reward: -214.7167 [75.40], Avg: -566.2481 (1.000)
Step: 62599, Reward: -214.4624 [43.93], Avg: -565.2645 (1.000)
Step: 62799, Reward: -198.2828 [95.08], Avg: -564.3986 (1.000)
Step: 62999, Reward: -131.6751 [88.62], Avg: -563.3062 (1.000)
Step: 63199, Reward: -154.8462 [96.05], Avg: -562.3176 (1.000)
Step: 63399, Reward: -110.4003 [107.09], Avg: -561.2298 (1.000)
Step: 63599, Reward: -147.1785 [49.29], Avg: -560.0827 (1.000)
Step: 63799, Reward: -147.5665 [44.14], Avg: -558.9280 (1.000)
Step: 63999, Reward: -122.3636 [77.33], Avg: -557.8053 (1.000)
Step: 64199, Reward: -173.4021 [94.00], Avg: -556.9006 (1.000)
Step: 64399, Reward: -101.1023 [91.45], Avg: -555.7691 (1.000)
Step: 64599, Reward: -149.9403 [141.08], Avg: -554.9495 (1.000)
Step: 64799, Reward: -147.1766 [45.63], Avg: -553.8318 (1.000)
Step: 64999, Reward: -148.7952 [47.10], Avg: -552.7304 (1.000)
Step: 65199, Reward: -155.0557 [98.25], Avg: -551.8119 (1.000)
Step: 65399, Reward: -150.2031 [52.11], Avg: -550.7431 (1.000)
Step: 65599, Reward: -173.7531 [56.33], Avg: -549.7655 (1.000)
Step: 65799, Reward: -223.2456 [92.08], Avg: -549.0529 (1.000)
Step: 65999, Reward: -100.6632 [49.48], Avg: -547.8441 (1.000)
Step: 66199, Reward: -150.6112 [50.21], Avg: -546.7957 (1.000)
Step: 66399, Reward: -124.5326 [77.43], Avg: -545.7570 (1.000)
Step: 66599, Reward: -221.5781 [53.03], Avg: -544.9428 (1.000)
Step: 66799, Reward: -168.3916 [121.01], Avg: -544.1777 (1.000)
Step: 66999, Reward: -148.6612 [48.81], Avg: -543.1428 (1.000)
Step: 67199, Reward: -195.4963 [89.70], Avg: -542.3751 (1.000)
Step: 67399, Reward: -118.8091 [70.57], Avg: -541.3276 (1.000)
Step: 67599, Reward: -100.1623 [49.45], Avg: -540.1687 (1.000)
Step: 67799, Reward: -149.7348 [119.08], Avg: -539.3683 (1.000)
Step: 67999, Reward: -160.0971 [105.33], Avg: -538.5626 (1.000)
Step: 68199, Reward: -146.5569 [91.35], Avg: -537.6809 (1.000)
Step: 68399, Reward: -110.3809 [109.43], Avg: -536.7514 (1.000)
Step: 68599, Reward: -193.1683 [119.45], Avg: -536.0980 (1.000)
Step: 68799, Reward: -123.2892 [74.09], Avg: -535.1133 (1.000)
Step: 68999, Reward: -146.6046 [45.47], Avg: -534.1190 (1.000)
Step: 69199, Reward: -144.5150 [118.10], Avg: -533.3343 (1.000)
Step: 69399, Reward: -125.1652 [1.35], Avg: -532.1619 (1.000)
Step: 69599, Reward: -125.6746 [79.56], Avg: -531.2225 (1.000)
Step: 69799, Reward: -168.2054 [95.91], Avg: -530.4571 (1.000)
Step: 69999, Reward: -194.5779 [88.94], Avg: -529.7516 (1.000)
Step: 70199, Reward: -172.5254 [59.69], Avg: -528.9039 (1.000)
Step: 70399, Reward: -171.5188 [120.72], Avg: -528.2316 (1.000)
Step: 70599, Reward: -100.4056 [90.36], Avg: -527.2756 (1.000)
Step: 70799, Reward: -97.7774 [47.62], Avg: -526.1968 (1.000)
Step: 70999, Reward: -151.5736 [44.90], Avg: -525.2680 (1.000)
Step: 71199, Reward: -150.6199 [52.40], Avg: -524.3628 (1.000)
Step: 71399, Reward: -171.5195 [97.21], Avg: -523.6468 (1.000)
Step: 71599, Reward: -145.9032 [47.58], Avg: -522.7245 (1.000)
Step: 71799, Reward: -98.9136 [48.94], Avg: -521.6803 (1.000)
Step: 71999, Reward: -174.3348 [60.15], Avg: -520.8826 (1.000)
Step: 72199, Reward: -122.8137 [74.71], Avg: -519.9868 (1.000)
Step: 72399, Reward: -152.6448 [123.12], Avg: -519.3122 (1.000)
Step: 72599, Reward: -151.1862 [54.10], Avg: -518.4471 (1.000)
Step: 72799, Reward: -177.6954 [59.28], Avg: -517.6738 (1.000)
Step: 72999, Reward: -135.3118 [92.69], Avg: -516.8802 (1.000)
Step: 73199, Reward: -154.2562 [59.10], Avg: -516.0509 (1.000)
Step: 73399, Reward: -144.1549 [91.15], Avg: -515.2859 (1.000)
Step: 73599, Reward: -145.9309 [49.26], Avg: -514.4161 (1.000)
Step: 73799, Reward: -166.8753 [116.10], Avg: -513.7889 (1.000)
Step: 73999, Reward: -142.2656 [88.73], Avg: -513.0246 (1.000)
Step: 74199, Reward: -149.3704 [50.98], Avg: -512.1818 (1.000)
Step: 74399, Reward: -196.9411 [99.06], Avg: -511.6007 (1.000)
Step: 74599, Reward: -189.9628 [89.02], Avg: -510.9770 (1.000)
Step: 74799, Reward: -168.0589 [88.58], Avg: -510.2970 (1.000)
Step: 74999, Reward: -215.7743 [82.23], Avg: -509.7309 (1.000)
Step: 75199, Reward: -169.7799 [93.17], Avg: -509.0745 (1.000)
Step: 75399, Reward: -124.3008 [76.98], Avg: -508.2581 (1.000)
Step: 75599, Reward: -173.7688 [62.62], Avg: -507.5389 (1.000)
Step: 75799, Reward: -151.8095 [52.87], Avg: -506.7398 (1.000)
Step: 75999, Reward: -124.6558 [79.62], Avg: -505.9438 (1.000)
Step: 76199, Reward: -177.6574 [102.57], Avg: -505.3514 (1.000)
Step: 76399, Reward: -123.2906 [3.62], Avg: -504.3607 (1.000)
Step: 76599, Reward: -121.3905 [75.98], Avg: -503.5592 (1.000)
Step: 76799, Reward: -125.2721 [83.57], Avg: -502.7917 (1.000)
Step: 76999, Reward: -177.7005 [67.45], Avg: -502.1225 (1.000)
Step: 77199, Reward: -189.7168 [137.81], Avg: -501.6702 (1.000)
Step: 77399, Reward: -200.6296 [100.04], Avg: -501.1508 (1.000)
Step: 77599, Reward: -200.4363 [59.00], Avg: -500.5278 (1.000)
Step: 77799, Reward: -149.0704 [44.66], Avg: -499.7391 (1.000)
Step: 77999, Reward: -146.5905 [46.01], Avg: -498.9516 (1.000)
Step: 78199, Reward: -150.5384 [46.50], Avg: -498.1794 (1.000)
Step: 78399, Reward: -77.0809 [61.63], Avg: -497.2624 (1.000)
Step: 78599, Reward: -78.3171 [62.47], Avg: -496.3554 (1.000)
Step: 78799, Reward: -138.4165 [103.27], Avg: -495.7090 (1.000)
Step: 78999, Reward: -173.6894 [60.36], Avg: -495.0466 (1.000)
Step: 79199, Reward: -218.2604 [99.19], Avg: -494.5981 (1.000)
Step: 79399, Reward: -146.3978 [89.13], Avg: -493.9455 (1.000)
Step: 79599, Reward: -230.9638 [89.55], Avg: -493.5098 (1.000)
Step: 79799, Reward: -169.4895 [56.46], Avg: -492.8392 (1.000)
Step: 79999, Reward: -145.2176 [89.03], Avg: -492.1927 (1.000)
Step: 80199, Reward: -127.4785 [2.54], Avg: -491.2895 (1.000)
Step: 80399, Reward: -119.5023 [72.00], Avg: -490.5438 (1.000)
Step: 80599, Reward: -121.2581 [72.09], Avg: -489.8063 (1.000)
Step: 80799, Reward: -202.7838 [65.63], Avg: -489.2583 (1.000)
Step: 80999, Reward: -162.7008 [106.63], Avg: -488.7153 (1.000)
Step: 81199, Reward: -144.2435 [43.44], Avg: -487.9738 (1.000)
Step: 81399, Reward: -146.8664 [49.22], Avg: -487.2566 (1.000)
Step: 81599, Reward: -74.7353 [95.33], Avg: -486.4792 (1.000)
Step: 81799, Reward: -76.1530 [97.26], Avg: -485.7138 (1.000)
Step: 81999, Reward: -76.6174 [61.11], Avg: -484.8650 (1.000)
Step: 82199, Reward: -147.6164 [46.52], Avg: -484.1576 (1.000)
Step: 82399, Reward: -201.0581 [65.11], Avg: -483.6285 (1.000)
Step: 82599, Reward: -174.5644 [61.81], Avg: -483.0299 (1.000)
Step: 82799, Reward: -124.1649 [2.84], Avg: -482.1699 (1.000)
Step: 82999, Reward: -140.3902 [106.81], Avg: -481.6037 (1.000)
Step: 83199, Reward: -119.8273 [70.78], Avg: -480.9042 (1.000)
Step: 83399, Reward: -159.1104 [107.27], Avg: -480.3898 (1.000)
Step: 83599, Reward: -120.9464 [4.08], Avg: -479.5396 (1.000)
Step: 83799, Reward: -121.0826 [105.55], Avg: -478.9360 (1.000)
Step: 83999, Reward: -193.7193 [58.98], Avg: -478.3974 (1.000)
Step: 84199, Reward: -94.5616 [134.00], Avg: -477.8039 (1.000)
Step: 84399, Reward: -125.3444 [2.77], Avg: -476.9753 (1.000)
Step: 84599, Reward: -124.7325 [80.12], Avg: -476.3320 (1.000)
Step: 84799, Reward: -140.4823 [106.17], Avg: -475.7903 (1.000)
Step: 84999, Reward: -193.6262 [55.73], Avg: -475.2575 (1.000)
Step: 85199, Reward: -121.0836 [73.14], Avg: -474.5978 (1.000)
Step: 85399, Reward: -147.1514 [43.77], Avg: -473.9334 (1.000)
Step: 85599, Reward: -206.1800 [66.77], Avg: -473.4639 (1.000)
Step: 85799, Reward: -164.0760 [111.15], Avg: -473.0018 (1.000)
Step: 85999, Reward: -189.6505 [82.89], Avg: -472.5356 (1.000)
Step: 86199, Reward: -97.2673 [88.36], Avg: -471.8699 (1.000)
Step: 86399, Reward: -168.0903 [119.30], Avg: -471.4428 (1.000)
Step: 86599, Reward: -104.3318 [97.23], Avg: -470.8196 (1.000)
Step: 86799, Reward: -145.3976 [92.28], Avg: -470.2824 (1.000)
Step: 86999, Reward: -122.0761 [76.90], Avg: -469.6587 (1.000)
Step: 87199, Reward: -177.0377 [68.02], Avg: -469.1435 (1.000)
Step: 87399, Reward: -124.3019 [74.88], Avg: -468.5258 (1.000)
Step: 87599, Reward: -96.6824 [47.97], Avg: -467.7863 (1.000)
Step: 87799, Reward: -97.1080 [86.52], Avg: -467.1391 (1.000)
Step: 87999, Reward: -149.8808 [91.79], Avg: -466.6266 (1.000)
Step: 88199, Reward: -145.7571 [46.09], Avg: -466.0035 (1.000)
Step: 88399, Reward: -98.7344 [48.40], Avg: -465.2821 (1.000)
Step: 88599, Reward: -124.4155 [78.13], Avg: -464.6890 (1.000)
Step: 88799, Reward: -71.5217 [56.44], Avg: -463.9306 (1.000)
Step: 88999, Reward: -156.5653 [105.47], Avg: -463.4769 (1.000)
Step: 89199, Reward: -98.7718 [91.26], Avg: -462.8638 (1.000)
Step: 89399, Reward: -126.0445 [3.93], Avg: -462.1191 (1.000)
Step: 89599, Reward: -172.7738 [124.99], Avg: -461.7522 (1.000)
Step: 89799, Reward: -182.1302 [74.67], Avg: -461.2958 (1.000)
Step: 89999, Reward: -217.0927 [50.25], Avg: -460.8647 (1.000)
Step: 90199, Reward: -188.7774 [90.95], Avg: -460.4631 (1.000)
Step: 90399, Reward: -173.0531 [58.17], Avg: -459.9560 (1.000)
Step: 90599, Reward: -193.9452 [56.64], Avg: -459.4938 (1.000)
Step: 90799, Reward: -75.1121 [60.45], Avg: -458.7803 (1.000)
Step: 90999, Reward: -148.5571 [92.36], Avg: -458.3014 (1.000)
Step: 91199, Reward: -140.0325 [45.91], Avg: -457.7042 (1.000)
Step: 91399, Reward: -176.4989 [62.21], Avg: -457.2250 (1.000)
Step: 91599, Reward: -147.1870 [48.71], Avg: -456.6544 (1.000)
Step: 91799, Reward: -170.7424 [94.73], Avg: -456.2378 (1.000)
Step: 91999, Reward: -149.4370 [47.69], Avg: -455.6746 (1.000)
Step: 92199, Reward: -172.1648 [56.45], Avg: -455.1820 (1.000)
Step: 92399, Reward: -192.6453 [119.76], Avg: -454.8730 (1.000)
Step: 92599, Reward: -76.3512 [97.86], Avg: -454.2668 (1.000)
Step: 92799, Reward: -146.6402 [122.16], Avg: -453.8671 (1.000)
Step: 92999, Reward: -168.4237 [142.73], Avg: -453.5602 (1.000)
Step: 93199, Reward: -97.3280 [47.82], Avg: -452.8983 (1.000)
Step: 93399, Reward: -148.3820 [92.55], Avg: -452.4445 (1.000)
Step: 93599, Reward: -249.3909 [114.81], Avg: -452.2559 (1.000)
Step: 93799, Reward: -164.8315 [56.08], Avg: -451.7626 (1.000)
Step: 93999, Reward: -98.5137 [48.82], Avg: -451.1149 (1.000)
Step: 94199, Reward: -125.7074 [77.20], Avg: -450.5879 (1.000)
Step: 94399, Reward: -143.2856 [110.85], Avg: -450.1717 (1.000)
Step: 94599, Reward: -172.3376 [59.25], Avg: -449.7096 (1.000)
Step: 94799, Reward: -166.5892 [115.88], Avg: -449.3568 (1.000)
Step: 94999, Reward: -122.2629 [2.98], Avg: -448.6744 (1.000)
Step: 95199, Reward: -97.5750 [48.34], Avg: -448.0384 (1.000)
Step: 95399, Reward: -97.6466 [48.08], Avg: -447.4046 (1.000)
Step: 95599, Reward: -144.1207 [112.47], Avg: -447.0054 (1.000)
Step: 95799, Reward: -153.5767 [129.05], Avg: -446.6622 (1.000)
Step: 95999, Reward: -196.0050 [93.42], Avg: -446.3346 (1.000)
Step: 96199, Reward: -121.6651 [3.67], Avg: -445.6673 (1.000)
Step: 96399, Reward: -186.8279 [114.99], Avg: -445.3688 (1.000)
Step: 96599, Reward: -165.8183 [91.99], Avg: -444.9805 (1.000)
Step: 96799, Reward: -164.3703 [138.24], Avg: -444.6864 (1.000)
Step: 96999, Reward: -74.0389 [94.91], Avg: -444.1178 (1.000)
Step: 97199, Reward: -168.7228 [121.76], Avg: -443.8017 (1.000)
Step: 97399, Reward: -99.5789 [49.19], Avg: -443.1959 (1.000)
Step: 97599, Reward: -131.4361 [94.73], Avg: -442.7512 (1.000)
Step: 97799, Reward: -220.6320 [92.07], Avg: -442.4852 (1.000)
Step: 97999, Reward: -98.2497 [48.58], Avg: -441.8818 (1.000)
Step: 98199, Reward: -184.3781 [76.18], Avg: -441.5125 (1.000)
Step: 98399, Reward: -98.6017 [48.65], Avg: -440.9144 (1.000)
Step: 98599, Reward: -197.7512 [64.40], Avg: -440.5518 (1.000)
Step: 98799, Reward: -187.8091 [88.73], Avg: -440.2198 (1.000)
Step: 98999, Reward: -121.8804 [4.69], Avg: -439.5862 (1.000)
Step: 99199, Reward: -173.0398 [67.44], Avg: -439.1848 (1.000)
Step: 99399, Reward: -148.5623 [48.13], Avg: -438.6969 (1.000)
Step: 99599, Reward: -175.0764 [58.84], Avg: -438.2857 (1.000)
Step: 99799, Reward: -149.5327 [49.85], Avg: -437.8069 (1.000)
Step: 99999, Reward: -146.1842 [43.81], Avg: -437.3113 (1.000)
