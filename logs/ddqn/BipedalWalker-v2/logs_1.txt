Model: <class 'models.ddqn.DDQNAgent'>, Dir: BipedalWalker-v2
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTQNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020              	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDQNetwork(PTQNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			q_values = self.critic_local(state) if not use_target else self.critic_target(state)
			return q_values.softmax(-1).cpu().numpy() if numpy else q_values.softmax(-1)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			q_values = self.critic_local(state) if not use_target else self.critic_target(state)
			out_dims = q_values.size()[:-1]
			q_values = q_values.reshape(-1, q_values.size(-1))
			q_indices = action.argmax(-1).reshape(-1)
			q_selected = q_values[np.arange(q_indices.size(0)), q_indices].reshape(*out_dims, 1)
			return q_selected.cpu().numpy() if numpy else q_selected
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states)[np.arange(actions.size(0)), actions.argmax(-1)].unsqueeze(-1)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddqn", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddqn", dirname, name)

class DDQNAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDQNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		return action_greedy
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=False, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddqn", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "LunarLander-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[6]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 77, Reward: -110.0945 [3.69], Avg: -113.7800 (1.000)
Step: 273, Reward: -108.2085 [6.91], Avg: -114.4503 (0.990)
Step: 320, Reward: -118.4254 [2.84], Avg: -116.7232 (0.980)
Step: 518, Reward: -113.9738 [5.72], Avg: -117.4658 (0.970)
Step: 557, Reward: -112.9188 [5.87], Avg: -117.7301 (0.961)
Step: 616, Reward: -108.7128 [9.03], Avg: -117.7320 (0.951)
Step: 712, Reward: -107.6572 [5.74], Avg: -117.1134 (0.941)
Step: 835, Reward: -112.3515 [6.25], Avg: -117.2993 (0.932)
Step: 957, Reward: -110.5142 [5.74], Avg: -117.1827 (0.923)
Step: 1007, Reward: -112.9924 [8.14], Avg: -117.5779 (0.914)
Step: 1155, Reward: -104.3162 [5.41], Avg: -116.8644 (0.904)
Step: 1195, Reward: -110.1883 [6.43], Avg: -116.8437 (0.895)
Step: 1495, Reward: -114.2714 [8.44], Avg: -117.2954 (0.886)
Step: 1566, Reward: -103.6394 [4.41], Avg: -116.6349 (0.878)
Step: 1717, Reward: -111.0611 [5.43], Avg: -116.6251 (0.869)
Step: 1776, Reward: -106.7598 [7.27], Avg: -116.4630 (0.860)
Step: 1838, Reward: -107.0053 [10.43], Avg: -116.5200 (0.851)
Step: 1907, Reward: -109.2207 [8.00], Avg: -116.5592 (0.843)
Step: 2161, Reward: -110.3686 [8.02], Avg: -116.6552 (0.835)
Step: 2291, Reward: -105.3832 [4.58], Avg: -116.3204 (0.826)
Step: 2441, Reward: -106.9102 [6.36], Avg: -116.1752 (0.818)
Step: 2511, Reward: -106.9804 [3.87], Avg: -115.9330 (0.810)
Step: 2584, Reward: -104.0992 [8.52], Avg: -115.7891 (0.802)
Step: 2641, Reward: -109.9829 [7.72], Avg: -115.8689 (0.794)
Step: 2792, Reward: -113.3650 [8.58], Avg: -116.1118 (0.786)
Step: 3239, Reward: -106.5633 [6.01], Avg: -115.9755 (0.778)
Step: 3281, Reward: -107.7622 [7.63], Avg: -115.9540 (0.770)
Step: 3390, Reward: -114.2229 [9.57], Avg: -116.2341 (0.762)
Step: 3442, Reward: -102.0546 [6.32], Avg: -115.9632 (0.755)
Step: 3531, Reward: -105.3010 [6.60], Avg: -115.8277 (0.747)
Step: 3650, Reward: -106.0724 [6.92], Avg: -115.7364 (0.740)
Step: 3715, Reward: -105.4272 [9.05], Avg: -115.6970 (0.732)
Step: 3955, Reward: -107.8878 [7.95], Avg: -115.7012 (0.725)
Step: 4021, Reward: -108.2591 [6.36], Avg: -115.6693 (0.718)
Step: 4151, Reward: -110.3098 [4.17], Avg: -115.6354 (0.711)
Step: 4255, Reward: -115.2356 [6.70], Avg: -115.8105 (0.703)
Step: 4346, Reward: -104.9313 [4.19], Avg: -115.6298 (0.696)
Step: 4413, Reward: -109.7029 [8.06], Avg: -115.6859 (0.689)
Step: 4525, Reward: -105.5256 [8.69], Avg: -115.6482 (0.683)
Step: 4589, Reward: -109.7141 [6.09], Avg: -115.6520 (0.676)
Step: 4746, Reward: -106.8602 [8.23], Avg: -115.6383 (0.669)
Step: 4856, Reward: -112.0609 [5.47], Avg: -115.6833 (0.662)
Step: 4944, Reward: -113.4670 [6.45], Avg: -115.7817 (0.656)
Step: 5011, Reward: -109.1846 [5.23], Avg: -115.7506 (0.649)
Step: 5168, Reward: -105.4790 [6.61], Avg: -115.6692 (0.643)
Step: 5244, Reward: -108.0852 [7.91], Avg: -115.6763 (0.636)
Step: 5337, Reward: -106.6206 [5.88], Avg: -115.6088 (0.630)
Step: 6013, Reward: -108.6624 [3.85], Avg: -115.5443 (0.624)
Step: 6106, Reward: -112.3382 [4.80], Avg: -115.5769 (0.617)
Step: 6235, Reward: -109.3908 [8.51], Avg: -115.6233 (0.611)
Step: 6841, Reward: -111.9581 [3.19], Avg: -115.6140 (0.605)
Step: 6881, Reward: -105.0709 [4.68], Avg: -115.5013 (0.599)
Step: 7573, Reward: -111.1001 [3.97], Avg: -115.4932 (0.593)
Step: 7625, Reward: -107.4368 [4.20], Avg: -115.4218 (0.587)
Step: 8005, Reward: -100.9153 [19.89], Avg: -115.5196 (0.581)
Step: 8077, Reward: -105.3873 [4.38], Avg: -115.4169 (0.575)
Step: 8117, Reward: -113.5363 [8.62], Avg: -115.5351 (0.570)
Step: 8211, Reward: -111.0426 [5.70], Avg: -115.5559 (0.564)
Step: 8519, Reward: -114.1405 [7.22], Avg: -115.6542 (0.558)
Step: 8579, Reward: -107.9083 [5.36], Avg: -115.6145 (0.553)
Step: 8675, Reward: -105.4945 [6.81], Avg: -115.5603 (0.547)
Step: 8714, Reward: -113.6445 [8.40], Avg: -115.6648 (0.542)
Step: 8783, Reward: -110.4051 [4.22], Avg: -115.6484 (0.536)
Step: 8890, Reward: -105.9832 [7.38], Avg: -115.6127 (0.531)
Step: 8939, Reward: -106.3569 [4.85], Avg: -115.5449 (0.526)
Step: 9002, Reward: -110.4622 [4.66], Avg: -115.5385 (0.520)
Step: 9071, Reward: -108.7864 [5.84], Avg: -115.5249 (0.515)
Step: 9131, Reward: -105.8421 [4.39], Avg: -115.4470 (0.510)
Step: 9217, Reward: -115.2878 [5.55], Avg: -115.5252 (0.505)
Step: 9312, Reward: -88.5244 [25.02], Avg: -115.4968 (0.500)
Step: 10912, Reward: -105.8017 [5.01], Avg: -115.4308 (0.495)
Step: 10982, Reward: -99.6881 [20.07], Avg: -115.4909 (0.490)
Step: 11119, Reward: -103.5652 [2.13], Avg: -115.3567 (0.485)
Step: 11182, Reward: -105.6671 [6.52], Avg: -115.3139 (0.480)
Step: 11270, Reward: -107.5276 [6.35], Avg: -115.2947 (0.475)
Step: 11327, Reward: -108.7994 [5.98], Avg: -115.2878 (0.471)
Step: 11415, Reward: -106.3419 [4.17], Avg: -115.2258 (0.466)
Step: 11505, Reward: -111.4222 [3.45], Avg: -115.2213 (0.461)
Step: 11581, Reward: -95.8786 [19.50], Avg: -115.2234 (0.457)
Step: 11730, Reward: -103.9881 [3.38], Avg: -115.1252 (0.452)
Step: 11847, Reward: -92.6650 [22.16], Avg: -115.1215 (0.448)
Step: 11916, Reward: -108.2984 [4.47], Avg: -115.0928 (0.443)
Step: 12019, Reward: -109.7465 [4.40], Avg: -115.0814 (0.439)
Step: 13619, Reward: -111.0196 [6.79], Avg: -115.1139 (0.434)
Step: 13680, Reward: -103.0099 [2.25], Avg: -114.9980 (0.430)
Step: 13765, Reward: -112.7432 [4.50], Avg: -115.0241 (0.426)
Step: 13942, Reward: -101.7752 [22.73], Avg: -115.1331 (0.421)
Step: 14060, Reward: -111.2220 [7.07], Avg: -115.1689 (0.417)
Step: 14173, Reward: -108.8461 [4.59], Avg: -115.1495 (0.413)
Step: 14255, Reward: -102.0723 [24.06], Avg: -115.2715 (0.409)
Step: 15855, Reward: -101.2195 [28.80], Avg: -115.4336 (0.405)
Step: 15909, Reward: -108.9373 [4.91], Avg: -115.4163 (0.401)
Step: 17509, Reward: -94.4533 [20.04], Avg: -115.4064 (0.397)
Step: 17600, Reward: -106.4323 [3.09], Avg: -115.3438 (0.393)
Step: 17683, Reward: -108.9591 [3.05], Avg: -115.3087 (0.389)
Step: 17786, Reward: -104.8937 [2.32], Avg: -115.2244 (0.385)
Step: 17972, Reward: -101.1599 [20.68], Avg: -115.2926 (0.381)
Step: 18168, Reward: -94.7793 [22.00], Avg: -115.3077 (0.377)
Step: 18251, Reward: -101.5867 [21.67], Avg: -115.3881 (0.373)
Step: 18424, Reward: -86.5751 [24.47], Avg: -115.3446 (0.370)
Step: 20024, Reward: -90.0133 [27.96], Avg: -115.3706 (0.366)
Step: 21624, Reward: -89.7213 [26.96], Avg: -115.3835 (0.362)
Step: 21728, Reward: -85.3171 [21.89], Avg: -115.3040 (0.359)
Step: 21831, Reward: -90.9820 [31.17], Avg: -115.3699 (0.355)
Step: 21915, Reward: -105.8365 [22.10], Avg: -115.4896 (0.352)
Step: 21973, Reward: -79.0406 [32.72], Avg: -115.4544 (0.348)
Step: 22066, Reward: -102.0814 [25.03], Avg: -115.5634 (0.345)
Step: 22263, Reward: -76.6925 [27.49], Avg: -115.4580 (0.341)
Step: 22429, Reward: -75.2955 [26.29], Avg: -115.3307 (0.338)
Step: 22502, Reward: -107.2331 [3.54], Avg: -115.2893 (0.334)
Step: 22542, Reward: -100.3750 [24.74], Avg: -115.3778 (0.331)
Step: 22604, Reward: -100.1300 [21.43], Avg: -115.4330 (0.328)
Step: 22740, Reward: -92.1936 [27.08], Avg: -115.4670 (0.324)
Step: 22851, Reward: -94.7385 [30.91], Avg: -115.5563 (0.321)
Step: 24451, Reward: -105.7046 [1.25], Avg: -115.4815 (0.318)
Step: 26051, Reward: -97.7532 [24.82], Avg: -115.5426 (0.315)
Step: 26177, Reward: -74.2540 [26.28], Avg: -115.4144 (0.312)
Step: 27777, Reward: -84.0578 [23.43], Avg: -115.3472 (0.309)
Step: 29377, Reward: -83.7820 [26.47], Avg: -115.3044 (0.305)
Step: 30977, Reward: -107.6776 [0.61], Avg: -115.2459 (0.302)
Step: 31029, Reward: -82.1014 [27.43], Avg: -115.1987 (0.299)
Step: 32629, Reward: -85.7533 [25.24], Avg: -115.1642 (0.296)
Step: 32746, Reward: -85.1536 [28.30], Avg: -115.1503 (0.293)
Step: 34346, Reward: -94.8960 [21.73], Avg: -115.1622 (0.290)
Step: 34441, Reward: -90.0952 [31.38], Avg: -115.2128 (0.288)
Step: 34515, Reward: -96.4908 [23.27], Avg: -115.2489 (0.285)
Step: 34561, Reward: -94.6857 [21.00], Avg: -115.2523 (0.282)
Step: 34715, Reward: -98.8028 [25.40], Avg: -115.3222 (0.279)
Step: 34862, Reward: -84.6714 [21.80], Avg: -115.2536 (0.276)
Step: 34998, Reward: -80.5054 [26.34], Avg: -115.1889 (0.273)
Step: 35088, Reward: -83.0509 [25.90], Avg: -115.1413 (0.271)
Step: 35209, Reward: -98.9564 [24.89], Avg: -115.2073 (0.268)
Step: 36809, Reward: -84.0924 [28.07], Avg: -115.1844 (0.265)
Step: 36851, Reward: -106.7753 [4.99], Avg: -115.1589 (0.263)
Step: 38451, Reward: -96.2860 [20.63], Avg: -115.1719 (0.260)
Step: 38529, Reward: -112.5014 [11.41], Avg: -115.2362 (0.257)
Step: 38670, Reward: -60.0232 [20.88], Avg: -114.9856 (0.255)
Step: 40270, Reward: -83.9409 [26.96], Avg: -114.9560 (0.252)
Step: 40424, Reward: -82.3143 [27.01], Avg: -114.9154 (0.250)
Step: 42024, Reward: -95.1662 [21.53], Avg: -114.9281 (0.247)
Step: 43624, Reward: -104.2092 [25.45], Avg: -115.0326 (0.245)
Step: 43767, Reward: -84.6700 [26.75], Avg: -115.0072 (0.242)
Step: 45367, Reward: -86.6426 [26.16], Avg: -114.9918 (0.240)
Step: 45527, Reward: -95.6016 [24.43], Avg: -115.0268 (0.238)
Step: 45614, Reward: -95.2837 [22.28], Avg: -115.0442 (0.235)
Step: 47214, Reward: -98.0398 [27.74], Avg: -115.1178 (0.233)
Step: 47360, Reward: -98.9869 [23.25], Avg: -115.1662 (0.231)
Step: 48960, Reward: -83.7252 [26.23], Avg: -115.1310 (0.228)
Step: 50560, Reward: -83.4007 [28.48], Avg: -115.1092 (0.226)
Step: 50692, Reward: -85.9556 [29.69], Avg: -115.1128 (0.224)
Step: 51001, Reward: -96.6060 [24.02], Avg: -115.1493 (0.221)
Step: 51076, Reward: -80.3819 [30.00], Avg: -115.1180 (0.219)
Step: 52676, Reward: -72.1868 [26.24], Avg: -115.0089 (0.217)
Step: 54276, Reward: -94.4689 [20.32], Avg: -115.0074 (0.215)
Step: 54404, Reward: -87.0328 [27.65], Avg: -115.0053 (0.213)
Step: 56004, Reward: -74.6474 [28.66], Avg: -114.9303 (0.211)
Step: 56150, Reward: -85.3160 [31.13], Avg: -114.9400 (0.208)
Step: 56323, Reward: -85.9451 [25.23], Avg: -114.9162 (0.206)
Step: 56397, Reward: -98.4434 [26.54], Avg: -114.9795 (0.204)
Step: 57997, Reward: -84.4338 [27.60], Avg: -114.9611 (0.202)
Step: 59597, Reward: -84.7346 [28.78], Avg: -114.9521 (0.200)
Step: 59715, Reward: -108.8838 [4.28], Avg: -114.9410 (0.198)
Step: 59958, Reward: -97.0138 [26.93], Avg: -114.9962 (0.196)
Step: 60144, Reward: -91.5965 [34.35], Avg: -115.0630 (0.194)
Step: 60232, Reward: -97.4014 [24.54], Avg: -115.1047 (0.192)
Step: 60381, Reward: -99.0515 [22.90], Avg: -115.1459 (0.190)
Step: 61981, Reward: -74.3297 [29.47], Avg: -115.0780 (0.189)
Step: 63581, Reward: -95.8437 [25.29], Avg: -115.1141 (0.187)
Step: 65181, Reward: -104.2443 [26.90], Avg: -115.2089 (0.185)
Step: 65343, Reward: -96.4672 [23.97], Avg: -115.2397 (0.183)
Step: 65392, Reward: -83.0655 [27.01], Avg: -115.2095 (0.181)
Step: 66992, Reward: -85.2867 [29.10], Avg: -115.2047 (0.179)
Step: 67110, Reward: -95.6167 [25.05], Avg: -115.2363 (0.178)
Step: 67206, Reward: -69.6547 [27.89], Avg: -115.1346 (0.176)
Step: 67346, Reward: -82.8818 [26.82], Avg: -115.1036 (0.174)
Step: 67428, Reward: -82.0112 [30.58], Avg: -115.0893 (0.172)
Step: 67572, Reward: -83.3817 [27.01], Avg: -115.0628 (0.171)
Step: 69172, Reward: -107.1368 [3.43], Avg: -115.0375 (0.169)
Step: 69259, Reward: -88.1379 [30.88], Avg: -115.0598 (0.167)
Step: 70859, Reward: -70.2511 [28.19], Avg: -114.9674 (0.165)
Step: 71038, Reward: -94.4730 [23.03], Avg: -114.9815 (0.164)
Step: 71235, Reward: -95.6880 [22.80], Avg: -115.0007 (0.162)
Step: 71339, Reward: -82.3152 [29.03], Avg: -114.9807 (0.161)
Step: 72939, Reward: -69.9243 [28.89], Avg: -114.8929 (0.159)
Step: 74539, Reward: -81.8871 [27.96], Avg: -114.8656 (0.157)
Step: 74654, Reward: -69.0929 [29.47], Avg: -114.7780 (0.156)
Step: 76254, Reward: -69.8962 [27.79], Avg: -114.6866 (0.154)
Step: 76358, Reward: -80.9268 [30.54], Avg: -114.6694 (0.153)
Step: 76466, Reward: -80.0843 [26.97], Avg: -114.6291 (0.151)
Step: 78066, Reward: -107.8914 [4.06], Avg: -114.6151 (0.150)
Step: 78170, Reward: -43.5797 [1.47], Avg: -114.2509 (0.148)
Step: 78223, Reward: -69.7637 [30.63], Avg: -114.1787 (0.147)
Step: 79823, Reward: -96.0569 [20.62], Avg: -114.1916 (0.145)
Step: 79930, Reward: -68.9996 [31.02], Avg: -114.1186 (0.144)
Step: 81530, Reward: -71.6029 [36.25], Avg: -114.0864 (0.142)
Step: 83130, Reward: -95.3408 [21.18], Avg: -114.0988 (0.141)
Step: 83287, Reward: -94.9760 [22.18], Avg: -114.1143 (0.139)
Step: 84887, Reward: -93.4857 [22.92], Avg: -114.1259 (0.138)
Step: 84977, Reward: -70.7298 [29.32], Avg: -114.0552 (0.137)
Step: 85145, Reward: -95.7350 [21.67], Avg: -114.0719 (0.135)
Step: 85262, Reward: -99.1175 [26.71], Avg: -114.1304 (0.134)
Step: 85344, Reward: -85.6114 [26.85], Avg: -114.1222 (0.133)
Step: 86944, Reward: -87.3988 [31.16], Avg: -114.1440 (0.131)
Step: 87032, Reward: -94.2714 [22.00], Avg: -114.1545 (0.130)
Step: 87124, Reward: -61.6520 [21.73], Avg: -114.0044 (0.129)
Step: 88724, Reward: -71.8596 [25.91], Avg: -113.9256 (0.127)
Step: 88818, Reward: -58.1272 [21.12], Avg: -113.7580 (0.126)
Step: 90418, Reward: -67.8392 [28.58], Avg: -113.6747 (0.125)
Step: 92018, Reward: -81.2367 [28.23], Avg: -113.6545 (0.124)
Step: 92121, Reward: -70.5577 [27.95], Avg: -113.5824 (0.122)
Step: 93721, Reward: -70.6784 [29.16], Avg: -113.5173 (0.121)
Step: 93790, Reward: -56.0103 [26.31], Avg: -113.3701 (0.120)
Step: 93889, Reward: -68.8103 [30.08], Avg: -113.3021 (0.119)
Step: 95489, Reward: -44.6250 [3.49], Avg: -112.9975 (0.118)
Step: 97089, Reward: -58.2521 [30.27], Avg: -112.8837 (0.116)
Step: 98689, Reward: -45.5216 [2.22], Avg: -112.5821 (0.115)
Step: 98781, Reward: -46.5507 [1.79], Avg: -112.2861 (0.114)
Step: 100381, Reward: -92.2022 [24.79], Avg: -112.3077 (0.113)
Step: 100487, Reward: -46.8370 [2.29], Avg: -112.0192 (0.112)
Step: 102087, Reward: -69.0794 [31.43], Avg: -111.9669 (0.111)
Step: 102233, Reward: -82.1015 [29.40], Avg: -111.9647 (0.110)
Step: 103833, Reward: -69.4238 [29.88], Avg: -111.9077 (0.108)
Step: 105433, Reward: -87.4607 [35.01], Avg: -111.9551 (0.107)
Step: 107033, Reward: -42.2024 [1.24], Avg: -111.6492 (0.106)
Step: 108633, Reward: -107.0990 [2.46], Avg: -111.6399 (0.105)
Step: 110233, Reward: -38.8033 [0.70], Avg: -111.3207 (0.104)
Step: 111833, Reward: -83.7902 [32.51], Avg: -111.3427 (0.103)
Step: 113433, Reward: -86.2895 [30.71], Avg: -111.3675 (0.102)
Step: 113667, Reward: -58.4413 [24.43], Avg: -111.2430 (0.101)
Step: 113814, Reward: -68.7955 [25.69], Avg: -111.1702 (0.100)
Step: 113913, Reward: -59.5398 [23.10], Avg: -111.0466 (0.099)
Step: 115513, Reward: -79.9097 [31.69], Avg: -111.0490 (0.098)
Step: 115745, Reward: -56.4334 [25.26], Avg: -110.9231 (0.097)
Step: 115839, Reward: -67.7550 [29.17], Avg: -110.8632 (0.096)
Step: 117439, Reward: -59.8997 [23.05], Avg: -110.7445 (0.095)
Step: 117551, Reward: -81.4456 [32.10], Avg: -110.7563 (0.094)
Step: 119151, Reward: -56.4090 [24.38], Avg: -110.6299 (0.093)
Step: 120751, Reward: -42.9074 [1.50], Avg: -110.3517 (0.092)
Step: 122351, Reward: -55.3929 [28.88], Avg: -110.2425 (0.091)
Step: 123951, Reward: -56.6895 [31.34], Avg: -110.1500 (0.091)
Step: 125551, Reward: -56.5171 [22.15], Avg: -110.0194 (0.090)
Step: 125625, Reward: -53.3428 [25.77], Avg: -109.8916 (0.089)
Step: 125934, Reward: -40.9812 [1.00], Avg: -109.6122 (0.088)
Step: 127534, Reward: -56.4214 [31.37], Avg: -109.5227 (0.087)
Step: 127633, Reward: -39.9527 [0.55], Avg: -109.2410 (0.086)
Step: 129233, Reward: -55.1104 [30.46], Avg: -109.1448 (0.085)
Step: 130833, Reward: -41.1316 [3.45], Avg: -108.8834 (0.084)
Step: 130924, Reward: -55.5271 [29.76], Avg: -108.7883 (0.084)
Step: 130993, Reward: -38.9408 [2.00], Avg: -108.5158 (0.083)
Step: 132593, Reward: -86.6023 [35.31], Avg: -108.5694 (0.082)
Step: 132723, Reward: -72.4114 [35.40], Avg: -108.5664 (0.081)
Step: 132807, Reward: -57.1833 [30.27], Avg: -108.4826 (0.080)
Step: 132904, Reward: -71.8651 [37.86], Avg: -108.4875 (0.079)
Step: 134504, Reward: -40.1457 [1.37], Avg: -108.2238 (0.079)
Step: 136104, Reward: -40.8606 [2.60], Avg: -107.9698 (0.078)
Step: 137704, Reward: -57.5896 [30.15], Avg: -107.8908 (0.077)
Step: 139304, Reward: -56.8035 [31.75], Avg: -107.8156 (0.076)
Step: 140904, Reward: -67.9638 [28.55], Avg: -107.7718 (0.076)
Step: 141079, Reward: -60.1317 [28.30], Avg: -107.6971 (0.075)
Step: 142679, Reward: -55.1816 [31.03], Avg: -107.6145 (0.074)
Step: 144279, Reward: -57.4296 [23.45], Avg: -107.5121 (0.073)
Step: 144434, Reward: -69.3057 [27.54], Avg: -107.4713 (0.073)
Step: 144583, Reward: -57.8348 [23.31], Avg: -107.3712 (0.072)
Step: 146183, Reward: -56.0978 [23.22], Avg: -107.2650 (0.071)
Step: 146306, Reward: -99.5936 [28.81], Avg: -107.3448 (0.070)
Step: 146384, Reward: -80.3701 [30.43], Avg: -107.3577 (0.070)
Step: 147984, Reward: -45.4988 [1.01], Avg: -107.1298 (0.069)
Step: 149584, Reward: -57.6237 [23.40], Avg: -107.0324 (0.068)
Step: 151184, Reward: -44.7665 [1.78], Avg: -106.8076 (0.068)
Step: 151432, Reward: -91.6139 [24.05], Avg: -106.8404 (0.067)
Step: 153032, Reward: -43.8680 [2.30], Avg: -106.6165 (0.066)
Step: 154632, Reward: -43.2085 [1.78], Avg: -106.3899 (0.066)
Step: 156232, Reward: -70.9887 [36.24], Avg: -106.3930 (0.065)
Step: 156350, Reward: -86.1976 [36.47], Avg: -106.4524 (0.064)
Step: 156535, Reward: -54.1315 [25.47], Avg: -106.3547 (0.064)
Step: 158135, Reward: -58.5235 [30.00], Avg: -106.2901 (0.063)
Step: 159735, Reward: -56.1861 [25.72], Avg: -106.2021 (0.062)
Step: 159964, Reward: -42.9461 [1.77], Avg: -105.9809 (0.062)
Step: 161564, Reward: -42.7306 [1.14], Avg: -105.7583 (0.061)
Step: 161701, Reward: -66.6360 [30.50], Avg: -105.7275 (0.061)
Step: 163301, Reward: -55.6347 [25.05], Avg: -105.6384 (0.060)
Step: 163631, Reward: -94.3143 [21.90], Avg: -105.6759 (0.059)
Step: 165231, Reward: -68.2246 [30.20], Avg: -105.6502 (0.059)
Step: 166831, Reward: -56.8836 [29.38], Avg: -105.5820 (0.058)
Step: 168431, Reward: -41.2770 [2.73], Avg: -105.3659 (0.058)
Step: 170031, Reward: -55.7902 [24.03], Avg: -105.2766 (0.057)
Step: 171631, Reward: -67.0806 [31.42], Avg: -105.2530 (0.056)
Step: 173231, Reward: -44.5334 [2.38], Avg: -105.0504 (0.056)
Step: 173352, Reward: -68.9069 [31.03], Avg: -105.0327 (0.055)
Step: 173590, Reward: -71.1930 [34.34], Avg: -105.0345 (0.055)
Step: 175190, Reward: -44.5129 [0.54], Avg: -104.8283 (0.054)
Step: 175310, Reward: -53.7450 [27.29], Avg: -104.7468 (0.054)
Step: 176910, Reward: -43.8595 [2.40], Avg: -104.5472 (0.053)
Step: 177063, Reward: -58.2874 [31.38], Avg: -104.4966 (0.053)
Step: 178663, Reward: -92.9426 [21.76], Avg: -104.5312 (0.052)
Step: 180263, Reward: -58.8827 [25.84], Avg: -104.4643 (0.052)
Step: 180380, Reward: -43.6706 [0.85], Avg: -104.2625 (0.051)
Step: 181980, Reward: -42.4425 [0.94], Avg: -104.0582 (0.051)
Step: 183580, Reward: -72.1816 [32.02], Avg: -104.0587 (0.050)
Step: 185180, Reward: -42.6442 [1.64], Avg: -103.8594 (0.050)
Step: 186780, Reward: -44.7131 [1.49], Avg: -103.6679 (0.049)
Step: 188380, Reward: -42.2006 [2.78], Avg: -103.4735 (0.049)
Step: 188513, Reward: -58.8245 [25.79], Avg: -103.4113 (0.048)
Step: 190113, Reward: -57.7330 [28.77], Avg: -103.3557 (0.048)
Step: 191713, Reward: -43.5004 [0.87], Avg: -103.1623 (0.047)
Step: 193313, Reward: -57.0552 [28.93], Avg: -103.1061 (0.047)
Step: 193560, Reward: -43.4063 [1.67], Avg: -102.9171 (0.046)
Step: 195160, Reward: -41.4077 [0.86], Avg: -102.7202 (0.046)
Step: 196760, Reward: -43.2386 [2.15], Avg: -102.5347 (0.045)
Step: 198360, Reward: -54.1541 [25.92], Avg: -102.4622 (0.045)
Step: 198648, Reward: -55.7136 [25.26], Avg: -102.3931 (0.044)
Step: 200248, Reward: -42.4785 [1.03], Avg: -102.2044 (0.044)
Step: 201848, Reward: -68.8816 [30.16], Avg: -102.1943 (0.043)
Step: 201988, Reward: -78.7118 [30.88], Avg: -102.2178 (0.043)
Step: 202116, Reward: -81.7597 [29.00], Avg: -102.2449 (0.043)
Step: 203716, Reward: -56.7029 [26.05], Avg: -102.1833 (0.042)
Step: 204126, Reward: -42.1186 [0.62], Avg: -101.9957 (0.042)
Step: 205726, Reward: -54.5065 [24.36], Avg: -101.9230 (0.041)
Step: 207326, Reward: -54.2800 [27.49], Avg: -101.8598 (0.041)
Step: 207661, Reward: -57.6903 [31.19], Avg: -101.8192 (0.041)
Step: 209261, Reward: -42.7792 [1.05], Avg: -101.6386 (0.040)
Step: 210861, Reward: -56.7806 [22.73], Avg: -101.5699 (0.040)
Step: 211093, Reward: -42.6256 [0.73], Avg: -101.3897 (0.039)
Step: 212693, Reward: -43.1532 [0.98], Avg: -101.2129 (0.039)
Step: 212846, Reward: -42.8689 [0.29], Avg: -101.0343 (0.039)
Step: 214446, Reward: -42.8380 [1.25], Avg: -100.8596 (0.038)
Step: 216046, Reward: -53.4564 [27.03], Avg: -100.7973 (0.038)
Step: 217646, Reward: -55.6578 [29.78], Avg: -100.7505 (0.037)
Step: 219246, Reward: -114.7312 [0.82], Avg: -100.7955 (0.037)
Step: 219341, Reward: -82.3413 [34.30], Avg: -100.8435 (0.037)
Step: 220485, Reward: -55.6999 [30.52], Avg: -100.7993 (0.036)
Step: 220610, Reward: -55.8172 [29.76], Avg: -100.7535 (0.036)
Step: 222210, Reward: -53.7205 [26.16], Avg: -100.6908 (0.036)
Step: 223810, Reward: -41.3323 [2.55], Avg: -100.5207 (0.035)
Step: 224063, Reward: -42.3839 [0.56], Avg: -100.3488 (0.035)
Step: 225663, Reward: -41.8442 [0.93], Avg: -100.1775 (0.034)
Step: 227263, Reward: -42.2337 [0.52], Avg: -100.0071 (0.034)
Step: 228863, Reward: -66.6355 [32.44], Avg: -100.0043 (0.034)
Step: 230463, Reward: -104.6452 [0.93], Avg: -100.0207 (0.033)
Step: 230570, Reward: -93.6849 [26.09], Avg: -100.0788 (0.033)
Step: 230659, Reward: -104.7838 [0.50], Avg: -100.0941 (0.033)
Step: 230770, Reward: -41.1713 [2.69], Avg: -99.9297 (0.032)
Step: 231041, Reward: -79.8718 [32.30], Avg: -99.9654 (0.032)
Step: 231156, Reward: -53.6896 [26.10], Avg: -99.9067 (0.032)
Step: 232624, Reward: -54.5953 [24.23], Avg: -99.8456 (0.032)
Step: 234224, Reward: -66.8612 [30.40], Avg: -99.8381 (0.031)
Step: 234939, Reward: -53.5234 [24.25], Avg: -99.7745 (0.031)
Step: 236539, Reward: -42.5800 [1.72], Avg: -99.6151 (0.031)
Step: 238139, Reward: -85.7888 [39.05], Avg: -99.6874 (0.030)
Step: 238229, Reward: -105.9036 [31.57], Avg: -99.7954 (0.030)
Step: 239829, Reward: -75.6345 [41.24], Avg: -99.8440 (0.030)
Step: 241429, Reward: -76.7474 [42.88], Avg: -99.9002 (0.029)
Step: 242761, Reward: -42.3163 [1.27], Avg: -99.7407 (0.029)
Step: 244361, Reward: -98.5600 [29.87], Avg: -99.8217 (0.029)
Step: 244473, Reward: -57.8230 [28.79], Avg: -99.7845 (0.029)
Step: 245567, Reward: -78.7380 [28.85], Avg: -99.8065 (0.028)
Step: 245722, Reward: -44.8909 [3.77], Avg: -99.6632 (0.028)
Step: 247322, Reward: -112.1288 [8.18], Avg: -99.7209 (0.028)
Step: 248922, Reward: -68.4554 [34.18], Avg: -99.7290 (0.027)
Step: 249032, Reward: -57.8037 [29.95], Avg: -99.6957 (0.027)
Step: 250632, Reward: -43.1196 [1.65], Avg: -99.5436 (0.027)
Step: 251302, Reward: -41.7182 [0.58], Avg: -99.3855 (0.027)
Step: 252902, Reward: -54.1067 [24.23], Avg: -99.3275 (0.026)
Step: 254502, Reward: -41.7487 [1.67], Avg: -99.1739 (0.026)
Step: 256102, Reward: -42.7468 [2.76], Avg: -99.0269 (0.026)
Step: 257702, Reward: -42.0217 [2.22], Avg: -98.8772 (0.026)
Step: 258887, Reward: -42.0880 [0.61], Avg: -98.7241 (0.025)
Step: 260487, Reward: -42.0774 [0.30], Avg: -98.5710 (0.025)
Step: 260784, Reward: -44.2719 [2.68], Avg: -98.4311 (0.025)
Step: 262384, Reward: -68.8340 [33.59], Avg: -98.4419 (0.025)
Step: 263984, Reward: -56.7041 [27.91], Avg: -98.4046 (0.024)
Step: 265584, Reward: -71.5534 [37.18], Avg: -98.4324 (0.024)
Step: 265841, Reward: -57.2282 [30.54], Avg: -98.4038 (0.024)
Step: 266085, Reward: -73.0422 [36.67], Avg: -98.4340 (0.024)
Step: 267685, Reward: -43.0154 [2.53], Avg: -98.2930 (0.023)
Step: 269285, Reward: -44.4447 [2.57], Avg: -98.1566 (0.023)
Step: 270885, Reward: -73.4864 [36.59], Avg: -98.1882 (0.023)
Step: 272485, Reward: -44.0020 [2.00], Avg: -98.0501 (0.023)
Step: 274085, Reward: -57.7799 [31.52], Avg: -98.0270 (0.022)
Step: 274449, Reward: -42.4512 [3.28], Avg: -97.8894 (0.022)
Step: 276049, Reward: -69.5139 [32.16], Avg: -97.8993 (0.022)
Step: 277649, Reward: -71.5790 [34.89], Avg: -97.9218 (0.022)
Step: 279249, Reward: -52.6036 [28.23], Avg: -97.8772 (0.022)
Step: 280849, Reward: -71.2669 [32.78], Avg: -97.8932 (0.021)
Step: 282449, Reward: -59.8355 [31.51], Avg: -97.8762 (0.021)
Step: 284049, Reward: -107.9386 [3.44], Avg: -97.9112 (0.021)
Step: 284095, Reward: -116.6665 [2.22], Avg: -97.9654 (0.021)
Step: 285695, Reward: -93.1046 [27.09], Avg: -98.0227 (0.020)
Step: 285809, Reward: -44.9577 [3.30], Avg: -97.8947 (0.020)
Step: 287409, Reward: -93.2197 [26.34], Avg: -97.9503 (0.020)
Step: 289009, Reward: -40.6597 [1.24], Avg: -97.8070 (0.020)
Step: 290609, Reward: -67.0729 [33.30], Avg: -97.8135 (0.020)
Step: 292209, Reward: -39.9472 [2.35], Avg: -97.6722 (0.020)
Step: 292335, Reward: -54.4997 [26.52], Avg: -97.6300 (0.020)
Step: 292558, Reward: -40.9942 [2.64], Avg: -97.4933 (0.020)
Step: 294158, Reward: -40.2883 [1.72], Avg: -97.3532 (0.020)
Step: 295758, Reward: -41.5118 [0.49], Avg: -97.2137 (0.020)
Step: 297358, Reward: -62.5619 [38.00], Avg: -97.2222 (0.020)
Step: 298958, Reward: -75.0733 [38.51], Avg: -97.2632 (0.020)
Step: 300558, Reward: -42.4367 [2.84], Avg: -97.1332 (0.020)
Step: 302158, Reward: -59.2457 [31.67], Avg: -97.1177 (0.020)
Step: 303758, Reward: -56.2439 [29.63], Avg: -97.0897 (0.020)
Step: 305358, Reward: -39.7094 [1.52], Avg: -96.9511 (0.020)
Step: 306958, Reward: -75.7849 [40.86], Avg: -96.9999 (0.020)
Step: 308558, Reward: -38.4390 [0.76], Avg: -96.8572 (0.020)
Step: 310158, Reward: -87.3581 [39.87], Avg: -96.9320 (0.020)
Step: 310253, Reward: -106.8181 [0.35], Avg: -96.9571 (0.020)
Step: 310426, Reward: -42.2417 [3.31], Avg: -96.8311 (0.020)
Step: 312026, Reward: -41.1689 [1.58], Avg: -96.6989 (0.020)
Step: 313626, Reward: -86.3478 [35.48], Avg: -96.7602 (0.020)
Step: 315226, Reward: -105.0717 [0.70], Avg: -96.7821 (0.020)
Step: 315437, Reward: -86.2162 [34.30], Avg: -96.8397 (0.020)
Step: 315755, Reward: -40.0262 [1.49], Avg: -96.7058 (0.020)
Step: 317355, Reward: -55.3113 [26.07], Avg: -96.6687 (0.020)
Step: 318955, Reward: -57.0304 [24.22], Avg: -96.6316 (0.020)
Step: 320555, Reward: -42.2830 [3.54], Avg: -96.5095 (0.020)
Step: 320864, Reward: -40.4678 [0.75], Avg: -96.3769 (0.020)
Step: 322464, Reward: -41.1173 [1.52], Avg: -96.2483 (0.020)
Step: 324064, Reward: -58.4821 [33.39], Avg: -96.2379 (0.020)
Step: 325664, Reward: -78.2580 [39.40], Avg: -96.2889 (0.020)
Step: 327264, Reward: -59.6632 [34.96], Avg: -96.2849 (0.020)
Step: 328864, Reward: -41.8228 [1.39], Avg: -96.1591 (0.020)
Step: 330464, Reward: -73.7416 [38.96], Avg: -96.1982 (0.020)
Step: 332064, Reward: -44.7308 [3.21], Avg: -96.0844 (0.020)
Step: 333664, Reward: -57.4868 [25.52], Avg: -96.0537 (0.020)
Step: 335264, Reward: -45.8860 [1.58], Avg: -95.9396 (0.020)
Step: 336864, Reward: -44.1668 [1.95], Avg: -95.8229 (0.020)
Step: 338464, Reward: -43.1045 [3.05], Avg: -95.7069 (0.020)
Step: 340064, Reward: -58.0233 [30.96], Avg: -95.6912 (0.020)
Step: 341664, Reward: -43.6683 [1.25], Avg: -95.5732 (0.020)
Step: 343264, Reward: -56.5062 [23.99], Avg: -95.5382 (0.020)
Step: 344864, Reward: -42.7739 [2.64], Avg: -95.4222 (0.020)
Step: 346464, Reward: -45.2518 [2.76], Avg: -95.3127 (0.020)
Step: 348064, Reward: -44.2346 [2.10], Avg: -95.1998 (0.020)
Step: 349664, Reward: -46.0690 [1.33], Avg: -95.0899 (0.020)
Step: 351264, Reward: -110.2791 [6.19], Avg: -95.1389 (0.020)
Step: 351382, Reward: -92.7241 [24.53], Avg: -95.1895 (0.020)
Step: 351507, Reward: -104.9011 [0.75], Avg: -95.2134 (0.020)
Step: 353107, Reward: -46.0250 [1.83], Avg: -95.1055 (0.020)
Step: 354707, Reward: -90.0795 [37.15], Avg: -95.1786 (0.020)
Step: 354954, Reward: -71.4262 [33.20], Avg: -95.2000 (0.020)
Step: 356554, Reward: -69.3708 [34.32], Avg: -95.2192 (0.020)
Step: 358154, Reward: -59.3531 [32.18], Avg: -95.2109 (0.020)
Step: 358403, Reward: -44.3433 [2.62], Avg: -95.1022 (0.020)
Step: 360003, Reward: -45.3364 [2.56], Avg: -94.9961 (0.020)
Step: 361603, Reward: -43.3406 [1.37], Avg: -94.8834 (0.020)
Step: 363203, Reward: -42.6953 [2.39], Avg: -94.7719 (0.020)
Step: 364803, Reward: -61.0208 [29.84], Avg: -94.7632 (0.020)
Step: 366403, Reward: -58.1031 [31.53], Avg: -94.7518 (0.020)
Step: 368003, Reward: -60.6873 [31.07], Avg: -94.7451 (0.020)
Step: 369603, Reward: -41.6972 [0.83], Avg: -94.6293 (0.020)
Step: 371203, Reward: -42.9050 [2.10], Avg: -94.5195 (0.020)
Step: 372803, Reward: -59.9772 [28.60], Avg: -94.5064 (0.020)
Step: 374403, Reward: -42.4018 [2.53], Avg: -94.3972 (0.020)
Step: 376003, Reward: -58.5405 [28.46], Avg: -94.3810 (0.020)
Step: 377603, Reward: -115.5928 [1.10], Avg: -94.4299 (0.020)
Step: 377739, Reward: -72.1870 [35.46], Avg: -94.4588 (0.020)
Step: 379339, Reward: -60.4633 [30.12], Avg: -94.4504 (0.020)
Step: 380939, Reward: -42.6742 [2.73], Avg: -94.3435 (0.020)
Step: 382539, Reward: -41.5899 [0.95], Avg: -94.2309 (0.020)
Step: 384139, Reward: -87.5137 [36.10], Avg: -94.2946 (0.020)
Step: 384324, Reward: -57.7143 [29.90], Avg: -94.2802 (0.020)
Step: 385924, Reward: -43.3860 [3.12], Avg: -94.1770 (0.020)
Step: 387524, Reward: -42.3091 [1.63], Avg: -94.0687 (0.020)
Step: 389124, Reward: -44.1356 [2.68], Avg: -93.9671 (0.020)
Step: 390724, Reward: -59.9556 [31.45], Avg: -93.9616 (0.020)
Step: 392324, Reward: -43.7839 [1.75], Avg: -93.8579 (0.020)
Step: 393924, Reward: -42.4108 [1.05], Avg: -93.7502 (0.020)
Step: 395524, Reward: -44.1075 [3.16], Avg: -93.6511 (0.020)
Step: 397124, Reward: -43.3774 [2.24], Avg: -93.5489 (0.020)
Step: 398724, Reward: -44.5836 [3.28], Avg: -93.4519 (0.020)
Step: 400324, Reward: -62.3382 [29.12], Avg: -93.4477 (0.020)
Step: 401924, Reward: -46.3238 [1.23], Avg: -93.3506 (0.020)
Step: 403524, Reward: -82.8533 [28.64], Avg: -93.3889 (0.020)
Step: 403622, Reward: -70.5965 [28.77], Avg: -93.4015 (0.020)
Step: 403770, Reward: -59.3357 [24.08], Avg: -93.3805 (0.020)
Step: 404141, Reward: -44.6483 [2.19], Avg: -93.2829 (0.020)
Step: 405741, Reward: -102.5857 [27.47], Avg: -93.3598 (0.020)
Step: 405937, Reward: -111.8413 [4.89], Avg: -93.4086 (0.020)
Step: 406221, Reward: -111.6727 [0.21], Avg: -93.4471 (0.020)
Step: 406292, Reward: -116.4342 [4.11], Avg: -93.5035 (0.020)
Step: 406427, Reward: -109.7101 [3.52], Avg: -93.5444 (0.020)
Step: 406644, Reward: -73.8394 [35.43], Avg: -93.5769 (0.020)
Step: 406750, Reward: -72.8035 [32.51], Avg: -93.6012 (0.020)
Step: 406900, Reward: -58.8816 [29.55], Avg: -93.5905 (0.020)
Step: 408500, Reward: -43.4702 [2.38], Avg: -93.4923 (0.020)
Step: 410100, Reward: -45.3165 [1.81], Avg: -93.3971 (0.020)
Step: 411700, Reward: -95.1003 [24.74], Avg: -93.4513 (0.020)
Step: 413300, Reward: -46.6741 [1.42], Avg: -93.3586 (0.020)
Step: 413509, Reward: -44.0604 [1.67], Avg: -93.2613 (0.020)
Step: 415109, Reward: -44.7484 [1.83], Avg: -93.1663 (0.020)
Step: 416709, Reward: -46.0167 [2.01], Avg: -93.0745 (0.020)
Step: 418309, Reward: -44.3951 [2.18], Avg: -92.9802 (0.020)
Step: 419909, Reward: -43.2824 [2.86], Avg: -92.8854 (0.020)
Step: 421509, Reward: -43.7077 [2.19], Avg: -92.7905 (0.020)
Step: 423109, Reward: -58.1733 [27.43], Avg: -92.7760 (0.020)
Step: 424709, Reward: -44.8489 [2.65], Avg: -92.6849 (0.020)
Step: 426309, Reward: -45.7527 [2.53], Avg: -92.5957 (0.020)
Step: 427909, Reward: -44.0204 [2.74], Avg: -92.5039 (0.020)
Step: 429509, Reward: -56.5640 [23.65], Avg: -92.4793 (0.020)
Step: 431109, Reward: -41.4691 [1.59], Avg: -92.3806 (0.020)
Step: 431291, Reward: -45.7049 [2.26], Avg: -92.2921 (0.020)
Step: 432891, Reward: -42.7537 [2.79], Avg: -92.1992 (0.020)
Step: 434491, Reward: -45.1175 [2.00], Avg: -92.1098 (0.020)
Step: 436091, Reward: -41.8134 [1.19], Avg: -92.0125 (0.020)
Step: 437691, Reward: -57.4358 [31.32], Avg: -92.0061 (0.020)
Step: 439291, Reward: -41.3769 [0.51], Avg: -91.9073 (0.020)
Step: 440891, Reward: -44.4604 [3.86], Avg: -91.8215 (0.020)
Step: 442491, Reward: -43.9463 [2.38], Avg: -91.7321 (0.020)
Step: 444091, Reward: -41.3066 [1.23], Avg: -91.6356 (0.020)
Step: 445691, Reward: -42.6183 [2.28], Avg: -91.5441 (0.020)
Step: 447291, Reward: -43.3042 [2.34], Avg: -91.4545 (0.020)
Step: 448891, Reward: -57.1165 [26.87], Avg: -91.4399 (0.020)
Step: 450491, Reward: -45.5921 [2.41], Avg: -91.3554 (0.020)
Step: 450628, Reward: -59.6709 [30.59], Avg: -91.3533 (0.020)
Step: 452228, Reward: -42.3405 [2.10], Avg: -91.2624 (0.020)
Step: 453828, Reward: -82.3507 [28.41], Avg: -91.3001 (0.020)
Step: 455428, Reward: -57.6439 [24.48], Avg: -91.2824 (0.020)
Step: 455967, Reward: -97.6247 [26.74], Avg: -91.3461 (0.020)
Step: 456095, Reward: -42.9685 [2.54], Avg: -91.2580 (0.020)
Step: 457695, Reward: -44.0709 [2.57], Avg: -91.1723 (0.020)
Step: 459295, Reward: -93.4107 [27.47], Avg: -91.2293 (0.020)
Step: 459478, Reward: -43.2635 [2.00], Avg: -91.1414 (0.020)
Step: 461078, Reward: -69.9327 [34.70], Avg: -91.1671 (0.020)
Step: 461288, Reward: -60.6425 [30.46], Avg: -91.1670 (0.020)
Step: 462888, Reward: -102.6144 [27.00], Avg: -91.2401 (0.020)
Step: 464488, Reward: -55.2152 [24.90], Avg: -91.2190 (0.020)
Step: 466088, Reward: -43.9329 [3.61], Avg: -91.1362 (0.020)
Step: 467688, Reward: -93.6374 [24.57], Avg: -91.1874 (0.020)
Step: 468030, Reward: -96.4861 [27.64], Avg: -91.2496 (0.020)
Step: 468374, Reward: -57.3582 [27.31], Avg: -91.2372 (0.020)
Step: 469974, Reward: -45.5776 [4.17], Avg: -91.1592 (0.020)
Step: 470038, Reward: -56.2043 [27.69], Avg: -91.1455 (0.020)
Step: 471638, Reward: -59.7732 [28.72], Avg: -91.1406 (0.020)
Step: 473238, Reward: -44.7064 [2.34], Avg: -91.0582 (0.020)
Step: 474838, Reward: -43.8683 [3.23], Avg: -90.9761 (0.020)
Step: 476438, Reward: -45.8249 [3.06], Avg: -90.8978 (0.020)
Step: 478038, Reward: -46.0251 [2.77], Avg: -90.8195 (0.020)
Step: 479638, Reward: -57.8233 [24.90], Avg: -90.8045 (0.020)
Step: 481238, Reward: -95.5951 [20.90], Avg: -90.8521 (0.020)
Step: 481369, Reward: -94.2535 [21.43], Avg: -90.8980 (0.020)
Step: 481448, Reward: -105.8491 [0.97], Avg: -90.9273 (0.020)
Step: 481540, Reward: -107.2780 [5.35], Avg: -90.9673 (0.020)
Step: 481686, Reward: -100.9279 [3.10], Avg: -90.9913 (0.020)
Step: 481783, Reward: -102.6471 [1.38], Avg: -91.0152 (0.020)
Step: 481854, Reward: -65.3091 [28.04], Avg: -91.0195 (0.020)
Step: 481967, Reward: -101.7972 [0.92], Avg: -91.0409 (0.020)
Step: 482089, Reward: -102.4741 [0.81], Avg: -91.0632 (0.020)
Step: 482199, Reward: -103.4756 [0.94], Avg: -91.0876 (0.020)
Step: 482299, Reward: -91.5201 [24.37], Avg: -91.1326 (0.020)
Step: 482430, Reward: -78.8658 [30.85], Avg: -91.1664 (0.020)
Step: 482630, Reward: -70.5631 [29.11], Avg: -91.1818 (0.020)
Step: 484230, Reward: -75.8626 [40.50], Avg: -91.2273 (0.020)
Step: 484450, Reward: -75.0451 [33.52], Avg: -91.2586 (0.020)
Step: 486050, Reward: -96.1279 [24.03], Avg: -91.3107 (0.020)
Step: 486229, Reward: -104.5513 [28.51], Avg: -91.3858 (0.020)
Step: 486382, Reward: -102.0662 [29.10], Avg: -91.4572 (0.020)
Step: 486536, Reward: -116.6383 [4.26], Avg: -91.5099 (0.020)
Step: 486668, Reward: -112.6569 [5.66], Avg: -91.5579 (0.020)
Step: 486995, Reward: -42.4623 [3.32], Avg: -91.4761 (0.020)
Step: 488595, Reward: -59.5942 [31.24], Avg: -91.4750 (0.020)
Step: 490195, Reward: -58.9251 [29.64], Avg: -91.4698 (0.020)
Step: 490381, Reward: -44.2847 [3.79], Avg: -91.3927 (0.020)
Step: 491981, Reward: -87.0369 [36.04], Avg: -91.4489 (0.020)
Step: 492162, Reward: -46.9707 [3.13], Avg: -91.3757 (0.020)
Step: 493762, Reward: -56.3965 [27.96], Avg: -91.3633 (0.020)
Step: 495362, Reward: -40.6276 [2.60], Avg: -91.2784 (0.020)
Step: 496962, Reward: -60.0956 [34.03], Avg: -91.2834 (0.020)
Step: 498562, Reward: -54.2333 [24.11], Avg: -91.2607 (0.020)
