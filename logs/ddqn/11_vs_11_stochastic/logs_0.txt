Model: <class 'models.ddqn.DDQNAgent'>, Dir: 11_vs_11_stochastic
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTQNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.95             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDQNetwork(PTQNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			q_values = self.critic_local(state) if not use_target else self.critic_target(state)
			return q_values.cpu().numpy() if numpy else q_values

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			q_values = self.critic_local(state) if not use_target else self.critic_target(state)
			out_dims = q_values.size()[:-1]
			q_values = q_values.reshape(-1, q_values.size(-1))
			q_indices = action.argmax(-1).reshape(-1)
			q_selected = q_values[np.arange(q_indices.size(0)), q_indices].reshape(*out_dims, 1)
			return q_selected.cpu().numpy() if numpy else q_selected
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states)[np.arange(actions.size(0)), actions.argmax(-1)].unsqueeze(-1)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddqn", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddqn", dirname, name)

class DDQNAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDQNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		return action_greedy
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=False, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddqn", choices=["ddqn", "ddpg", "ppo"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=1000, help="Number of steps to train the agent")
args = parser.parse_args()

envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = envs[0]

def make_env(env_name=env_name, log=False):
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		self.agent.network.save_model(dirname, name)

	def load(self, dirname="pytorch", name="best"):
		self.stack.load_model(dirname, name)
		self.agent.network.load_model(dirname, name)
		return self

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s % envs.env.unwrapped._config._scenario_cfg.game_duration == 0:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s//eval_at}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 0, Reward: -2.4000 [0.80], Avg: -3.2000 (1.000)
Step: 3000, Reward: -2.2000 [1.17], Avg: -3.2831 (0.950)
Step: 6000, Reward: -2.6000 [1.02], Avg: -3.3953 (0.902)
Step: 9000, Reward: -3.8000 [1.17], Avg: -3.7880 (0.857)
Step: 12000, Reward: -2.6000 [1.36], Avg: -3.8217 (0.815)
Step: 15000, Reward: -3.2000 [1.60], Avg: -3.9848 (0.774)
Step: 18000, Reward: -1.6000 [1.36], Avg: -3.8379 (0.735)
Step: 21000, Reward: -2.6000 [1.02], Avg: -3.8106 (0.698)
Step: 24000, Reward: -2.0000 [1.26], Avg: -3.7500 (0.663)
Step: 27000, Reward: -2.0000 [1.10], Avg: -3.6845 (0.630)
Step: 30000, Reward: -2.6000 [1.85], Avg: -3.7545 (0.599)
Step: 33000, Reward: -2.8000 [1.72], Avg: -3.8184 (0.569)
Step: 36000, Reward: -1.2000 [0.40], Avg: -3.6477 (0.540)
Step: 39000, Reward: -2.8000 [0.40], Avg: -3.6157 (0.513)
Step: 42000, Reward: -2.4000 [1.62], Avg: -3.6430 (0.488)
Step: 45000, Reward: -3.8000 [1.83], Avg: -3.7674 (0.463)
Step: 48000, Reward: -2.4000 [1.02], Avg: -3.7469 (0.440)
Step: 51000, Reward: -2.8000 [1.47], Avg: -3.7760 (0.418)
Step: 54000, Reward: -2.8000 [1.47], Avg: -3.8020 (0.397)
Step: 57000, Reward: -2.6000 [1.02], Avg: -3.7929 (0.377)
Step: 60000, Reward: -3.2000 [2.04], Avg: -3.8618 (0.358)
Step: 63000, Reward: -1.8000 [0.75], Avg: -3.8021 (0.341)
Step: 66000, Reward: -2.4000 [1.02], Avg: -3.7854 (0.324)
Step: 69000, Reward: -2.0000 [1.26], Avg: -3.7637 (0.307)
Step: 72000, Reward: -1.6000 [0.80], Avg: -3.7092 (0.292)
Step: 75000, Reward: -3.6000 [0.49], Avg: -3.7238 (0.277)
Step: 78000, Reward: -2.0000 [0.63], Avg: -3.6834 (0.264)
Step: 81000, Reward: -2.6000 [1.36], Avg: -3.6932 (0.250)
Step: 84000, Reward: -2.6000 [1.36], Avg: -3.7022 (0.238)
Step: 87000, Reward: -2.6000 [1.85], Avg: -3.7273 (0.226)
Step: 90000, Reward: -2.0000 [1.10], Avg: -3.7069 (0.215)
Step: 93000, Reward: -2.0000 [1.79], Avg: -3.7095 (0.204)
Step: 96000, Reward: -4.2000 [2.04], Avg: -3.7862 (0.194)
Step: 99000, Reward: -2.0000 [0.89], Avg: -3.7599 (0.184)
Step: 102000, Reward: -2.6000 [1.85], Avg: -3.7798 (0.175)
Step: 105000, Reward: -2.4000 [1.85], Avg: -3.7930 (0.166)
Step: 108000, Reward: -2.6000 [0.80], Avg: -3.7824 (0.158)
Step: 111000, Reward: -2.2000 [1.17], Avg: -3.7714 (0.150)
Step: 114000, Reward: -2.4000 [1.85], Avg: -3.7838 (0.142)
Step: 117000, Reward: -3.2000 [1.72], Avg: -3.8122 (0.135)
Step: 120000, Reward: -3.2000 [1.83], Avg: -3.8420 (0.129)
Step: 123000, Reward: -2.8000 [1.72], Avg: -3.8582 (0.122)
Step: 126000, Reward: -2.6000 [1.62], Avg: -3.8667 (0.116)
Step: 129000, Reward: -3.8000 [1.33], Avg: -3.8953 (0.110)
Step: 132000, Reward: -4.2000 [1.17], Avg: -3.9280 (0.105)
Step: 135000, Reward: -2.0000 [0.63], Avg: -3.8998 (0.099)
Step: 138000, Reward: -1.8000 [0.98], Avg: -3.8760 (0.094)
Step: 141000, Reward: -2.0000 [1.41], Avg: -3.8664 (0.090)
Step: 144000, Reward: -2.2000 [1.94], Avg: -3.8720 (0.085)
Step: 147000, Reward: -2.4000 [0.80], Avg: -3.8585 (0.081)
Step: 150000, Reward: -2.6000 [1.85], Avg: -3.8702 (0.077)
Step: 153000, Reward: -3.4000 [1.74], Avg: -3.8947 (0.073)
Step: 156000, Reward: -2.8000 [1.33], Avg: -3.8991 (0.069)
Step: 159000, Reward: -1.8000 [1.17], Avg: -3.8818 (0.066)
Step: 162000, Reward: -2.2000 [0.40], Avg: -3.8585 (0.063)
Step: 165000, Reward: -2.2000 [1.47], Avg: -3.8551 (0.060)
Step: 168000, Reward: -1.4000 [1.85], Avg: -3.8446 (0.057)
Step: 171000, Reward: -2.0000 [1.41], Avg: -3.8372 (0.054)
Step: 174000, Reward: -2.4000 [1.50], Avg: -3.8382 (0.051)
Step: 177000, Reward: -1.6000 [1.02], Avg: -3.8179 (0.048)
Step: 180000, Reward: -2.8000 [1.72], Avg: -3.8294 (0.046)
Step: 183000, Reward: -1.6000 [0.49], Avg: -3.8013 (0.044)
Step: 186000, Reward: -3.2000 [1.60], Avg: -3.8172 (0.042)
Step: 189000, Reward: -3.2000 [0.98], Avg: -3.8228 (0.039)
Step: 192000, Reward: -1.0000 [1.67], Avg: -3.8052 (0.038)
Step: 195000, Reward: -2.4000 [0.49], Avg: -3.7913 (0.036)
Step: 198000, Reward: -3.0000 [0.89], Avg: -3.7928 (0.034)
Step: 201000, Reward: -1.8000 [0.98], Avg: -3.7779 (0.032)
Step: 204000, Reward: -2.4000 [2.87], Avg: -3.7996 (0.031)
Step: 207000, Reward: -1.4000 [1.36], Avg: -3.7847 (0.029)
Step: 210000, Reward: -2.2000 [0.75], Avg: -3.7729 (0.028)
Step: 213000, Reward: -2.0000 [0.89], Avg: -3.7607 (0.026)
Step: 216000, Reward: -3.0000 [0.63], Avg: -3.7589 (0.025)
Step: 219000, Reward: -2.8000 [1.83], Avg: -3.7707 (0.024)
Step: 222000, Reward: -2.0000 [2.10], Avg: -3.7751 (0.022)
Step: 225000, Reward: -1.6000 [0.49], Avg: -3.7529 (0.021)
Step: 228000, Reward: -2.8000 [1.33], Avg: -3.7578 (0.020)
Step: 231000, Reward: -3.8000 [1.17], Avg: -3.7733 (0.020)
Step: 234000, Reward: -2.2000 [1.47], Avg: -3.7720 (0.020)
Step: 237000, Reward: -2.0000 [1.41], Avg: -3.7675 (0.020)
Step: 240000, Reward: -1.8000 [1.17], Avg: -3.7576 (0.020)
Step: 243000, Reward: -2.4000 [1.36], Avg: -3.7576 (0.020)
Step: 246000, Reward: -1.2000 [0.75], Avg: -3.7358 (0.020)
Step: 249000, Reward: -2.0000 [1.67], Avg: -3.7350 (0.020)
Step: 252000, Reward: -1.2000 [0.75], Avg: -3.7140 (0.020)
Step: 255000, Reward: -3.6000 [2.87], Avg: -3.7461 (0.020)
Step: 258000, Reward: -1.8000 [1.17], Avg: -3.7371 (0.020)
Step: 261000, Reward: -0.2000 [0.40], Avg: -3.7015 (0.020)
Step: 264000, Reward: -2.2000 [0.75], Avg: -3.6930 (0.020)
Step: 267000, Reward: -2.0000 [1.41], Avg: -3.6899 (0.020)
Step: 270000, Reward: -1.6000 [1.02], Avg: -3.6781 (0.020)
Step: 273000, Reward: -1.4000 [1.02], Avg: -3.6645 (0.020)
Step: 276000, Reward: -1.6000 [0.49], Avg: -3.6475 (0.020)
Step: 279000, Reward: -3.8000 [1.94], Avg: -3.6698 (0.020)
Step: 282000, Reward: -2.8000 [1.17], Avg: -3.6729 (0.020)
Step: 285000, Reward: -2.8000 [2.14], Avg: -3.6861 (0.020)
Step: 288000, Reward: -2.8000 [0.98], Avg: -3.6870 (0.020)
Step: 291000, Reward: -3.6000 [0.80], Avg: -3.6943 (0.020)
Step: 294000, Reward: -2.2000 [0.98], Avg: -3.6891 (0.020)
Step: 297000, Reward: -2.4000 [0.80], Avg: -3.6842 (0.020)
Step: 300000, Reward: -2.6000 [1.50], Avg: -3.6883 (0.020)
Step: 303000, Reward: -1.6000 [0.80], Avg: -3.6757 (0.020)
Step: 306000, Reward: -1.2000 [0.75], Avg: -3.6589 (0.020)
Step: 309000, Reward: -1.2000 [0.40], Avg: -3.6391 (0.020)
Step: 312000, Reward: -1.4000 [1.50], Avg: -3.6320 (0.020)
Step: 315000, Reward: -2.0000 [1.41], Avg: -3.6300 (0.020)
Step: 318000, Reward: -1.2000 [1.17], Avg: -3.6182 (0.020)
Step: 321000, Reward: -1.8000 [1.33], Avg: -3.6136 (0.020)
Step: 324000, Reward: -1.0000 [0.63], Avg: -3.5954 (0.020)
Step: 327000, Reward: -0.4000 [0.80], Avg: -3.5737 (0.020)
Step: 330000, Reward: -1.6000 [0.80], Avg: -3.5631 (0.020)
Step: 333000, Reward: -1.0000 [0.89], Avg: -3.5482 (0.020)
Step: 336000, Reward: -1.8000 [0.98], Avg: -3.5414 (0.020)
Step: 339000, Reward: -3.4000 [0.80], Avg: -3.5472 (0.020)
Step: 342000, Reward: -1.6000 [1.02], Avg: -3.5391 (0.020)
Step: 345000, Reward: -2.8000 [1.17], Avg: -3.5428 (0.020)
Step: 348000, Reward: -0.6000 [0.80], Avg: -3.5245 (0.020)
Step: 351000, Reward: -3.0000 [3.16], Avg: -3.5468 (0.020)
Step: 354000, Reward: -1.4000 [0.80], Avg: -3.5355 (0.020)
Step: 357000, Reward: -1.0000 [1.10], Avg: -3.5235 (0.020)
Step: 360000, Reward: -2.6000 [1.36], Avg: -3.5271 (0.020)
Step: 363000, Reward: -4.8000 [3.49], Avg: -3.5661 (0.020)
Step: 366000, Reward: -2.8000 [0.75], Avg: -3.5660 (0.020)
Step: 369000, Reward: -2.4000 [1.50], Avg: -3.5686 (0.020)
Step: 372000, Reward: -2.6000 [1.02], Avg: -3.5690 (0.020)
Step: 375000, Reward: -1.0000 [1.41], Avg: -3.5599 (0.020)
Step: 378000, Reward: -1.2000 [1.17], Avg: -3.5505 (0.020)
Step: 381000, Reward: -2.8000 [1.33], Avg: -3.5550 (0.020)
Step: 384000, Reward: -3.0000 [1.10], Avg: -3.5592 (0.020)
Step: 387000, Reward: -2.2000 [1.17], Avg: -3.5577 (0.020)
Step: 390000, Reward: -1.2000 [1.17], Avg: -3.5486 (0.020)
Step: 393000, Reward: -4.4000 [0.49], Avg: -3.5587 (0.020)
Step: 396000, Reward: -1.6000 [1.85], Avg: -3.5580 (0.020)
Step: 399000, Reward: -1.6000 [1.62], Avg: -3.5555 (0.020)
Step: 402000, Reward: -1.6000 [0.80], Avg: -3.5469 (0.020)
Step: 405000, Reward: -2.2000 [1.33], Avg: -3.5468 (0.020)
Step: 408000, Reward: -2.0000 [1.26], Avg: -3.5447 (0.020)
Step: 411000, Reward: -2.0000 [1.41], Avg: -3.5438 (0.020)
Step: 414000, Reward: -1.4000 [1.02], Avg: -3.5357 (0.020)
Step: 417000, Reward: -2.0000 [0.63], Avg: -3.5292 (0.020)
Step: 420000, Reward: -2.6000 [1.36], Avg: -3.5322 (0.020)
Step: 423000, Reward: -3.4000 [0.80], Avg: -3.5370 (0.020)
Step: 426000, Reward: -2.4000 [1.36], Avg: -3.5385 (0.020)
Step: 429000, Reward: -2.0000 [1.67], Avg: -3.5394 (0.020)
Step: 432000, Reward: -3.0000 [1.26], Avg: -3.5444 (0.020)
Step: 435000, Reward: -2.0000 [2.28], Avg: -3.5495 (0.020)
Step: 438000, Reward: -1.4000 [1.36], Avg: -3.5441 (0.020)
Step: 441000, Reward: -2.8000 [1.60], Avg: -3.5499 (0.020)
Step: 444000, Reward: -0.8000 [1.17], Avg: -3.5392 (0.020)
Step: 447000, Reward: -1.2000 [0.75], Avg: -3.5286 (0.020)
Step: 450000, Reward: -3.6000 [2.33], Avg: -3.5445 (0.020)
Step: 453000, Reward: -2.8000 [1.47], Avg: -3.5493 (0.020)
Step: 456000, Reward: -1.6000 [1.36], Avg: -3.5454 (0.020)
Step: 459000, Reward: -2.2000 [1.94], Avg: -3.5493 (0.020)
Step: 462000, Reward: -1.4000 [1.36], Avg: -3.5442 (0.020)
Step: 465000, Reward: -2.4000 [1.50], Avg: -3.5464 (0.020)
Step: 468000, Reward: -1.8000 [1.17], Avg: -3.5427 (0.020)
Step: 471000, Reward: -1.4000 [1.36], Avg: -3.5378 (0.020)
Step: 474000, Reward: -2.2000 [0.75], Avg: -3.5341 (0.020)
Step: 477000, Reward: -2.8000 [1.17], Avg: -3.5368 (0.020)
Step: 480000, Reward: -1.8000 [0.75], Avg: -3.5306 (0.020)
Step: 483000, Reward: -2.4000 [0.80], Avg: -3.5286 (0.020)
Step: 486000, Reward: -2.2000 [0.75], Avg: -3.5250 (0.020)
Step: 489000, Reward: -2.2000 [1.17], Avg: -3.5240 (0.020)
Step: 492000, Reward: -3.0000 [1.41], Avg: -3.5294 (0.020)
Step: 495000, Reward: -2.8000 [2.71], Avg: -3.5414 (0.020)
Step: 498000, Reward: -0.6000 [1.20], Avg: -3.5310 (0.020)
Step: 501000, Reward: -2.2000 [0.98], Avg: -3.5289 (0.020)
Step: 504000, Reward: -2.6000 [1.62], Avg: -3.5330 (0.020)
Step: 507000, Reward: -1.8000 [0.75], Avg: -3.5272 (0.020)
Step: 510000, Reward: -3.2000 [1.17], Avg: -3.5321 (0.020)
Step: 513000, Reward: -1.8000 [1.72], Avg: -3.5320 (0.020)
Step: 516000, Reward: -4.0000 [2.00], Avg: -3.5463 (0.020)
Step: 519000, Reward: -2.4000 [1.36], Avg: -3.5475 (0.020)
Step: 522000, Reward: -1.6000 [1.20], Avg: -3.5432 (0.020)
Step: 525000, Reward: -1.6000 [0.49], Avg: -3.5350 (0.020)
Step: 528000, Reward: -2.4000 [0.49], Avg: -3.5313 (0.020)
Step: 531000, Reward: -1.0000 [1.26], Avg: -3.5242 (0.020)
Step: 534000, Reward: -1.8000 [1.47], Avg: -3.5228 (0.020)
Step: 537000, Reward: -1.2000 [1.33], Avg: -3.5173 (0.020)
Step: 540000, Reward: -2.6000 [1.62], Avg: -3.5212 (0.020)
Step: 543000, Reward: -1.2000 [0.40], Avg: -3.5106 (0.020)
Step: 546000, Reward: -2.6000 [1.02], Avg: -3.5112 (0.020)
Step: 549000, Reward: -3.2000 [1.47], Avg: -3.5175 (0.020)
Step: 552000, Reward: -2.0000 [1.26], Avg: -3.5161 (0.020)
Step: 555000, Reward: -2.0000 [0.63], Avg: -3.5114 (0.020)
Step: 558000, Reward: -1.4000 [0.49], Avg: -3.5027 (0.020)
Step: 561000, Reward: -3.2000 [0.98], Avg: -3.5063 (0.020)
Step: 564000, Reward: -2.4000 [1.62], Avg: -3.5091 (0.020)
Step: 567000, Reward: -0.4000 [0.49], Avg: -3.4953 (0.020)
Step: 570000, Reward: -1.8000 [1.33], Avg: -3.4934 (0.020)
Step: 573000, Reward: -1.2000 [2.04], Avg: -3.4920 (0.020)
Step: 576000, Reward: -1.6000 [1.36], Avg: -3.4893 (0.020)
Step: 579000, Reward: -1.0000 [0.63], Avg: -3.4797 (0.020)
Step: 582000, Reward: -1.8000 [1.17], Avg: -3.4771 (0.020)
Step: 585000, Reward: -1.4000 [0.49], Avg: -3.4690 (0.020)
Step: 588000, Reward: -3.6000 [1.02], Avg: -3.4748 (0.020)
Step: 591000, Reward: -2.6000 [1.85], Avg: -3.4797 (0.020)
Step: 594000, Reward: -1.0000 [0.63], Avg: -3.4705 (0.020)
Step: 597000, Reward: -3.0000 [0.63], Avg: -3.4713 (0.020)
Step: 600000, Reward: -1.4000 [1.02], Avg: -3.4660 (0.020)
Step: 603000, Reward: -3.0000 [1.67], Avg: -3.4720 (0.020)
Step: 606000, Reward: -0.8000 [1.17], Avg: -3.4646 (0.020)
Step: 609000, Reward: -2.6000 [1.62], Avg: -3.4683 (0.020)
Step: 612000, Reward: -1.2000 [1.17], Avg: -3.4630 (0.020)
Step: 615000, Reward: -1.2000 [1.47], Avg: -3.4591 (0.020)
Step: 618000, Reward: -2.2000 [1.17], Avg: -3.4587 (0.020)
Step: 621000, Reward: -1.4000 [0.49], Avg: -3.4511 (0.020)
Step: 624000, Reward: -3.0000 [0.89], Avg: -3.4532 (0.020)
Step: 627000, Reward: -3.4000 [1.50], Avg: -3.4601 (0.020)
Step: 630000, Reward: -2.4000 [1.02], Avg: -3.4599 (0.020)
Step: 633000, Reward: -1.2000 [0.40], Avg: -3.4511 (0.020)
Step: 636000, Reward: -2.0000 [1.26], Avg: -3.4503 (0.020)
Step: 639000, Reward: -2.6000 [2.15], Avg: -3.4564 (0.020)
Step: 642000, Reward: -2.0000 [1.90], Avg: -3.4584 (0.020)
Step: 645000, Reward: -1.6000 [1.02], Avg: -3.4545 (0.020)
Step: 648000, Reward: -0.6000 [0.49], Avg: -3.4436 (0.020)
Step: 651000, Reward: -2.2000 [1.72], Avg: -3.4458 (0.020)
Step: 654000, Reward: -2.4000 [1.36], Avg: -3.4472 (0.020)
Step: 657000, Reward: -1.8000 [1.83], Avg: -3.4481 (0.020)
Step: 660000, Reward: -1.0000 [1.10], Avg: -3.4420 (0.020)
Step: 663000, Reward: -1.8000 [1.94], Avg: -3.4433 (0.020)
Step: 666000, Reward: -2.0000 [1.41], Avg: -3.4432 (0.020)
Step: 669000, Reward: -1.8000 [0.75], Avg: -3.4392 (0.020)
Step: 672000, Reward: -3.4000 [1.36], Avg: -3.4450 (0.020)
Step: 675000, Reward: -1.4000 [1.50], Avg: -3.4426 (0.020)
Step: 678000, Reward: -1.8000 [1.83], Avg: -3.4434 (0.020)
Step: 681000, Reward: -2.2000 [1.33], Avg: -3.4438 (0.020)
Step: 684000, Reward: -2.2000 [1.17], Avg: -3.4435 (0.020)
Step: 687000, Reward: -2.2000 [2.48], Avg: -3.4489 (0.020)
Step: 690000, Reward: -2.6000 [1.96], Avg: -3.4537 (0.020)
Step: 693000, Reward: -2.8000 [0.98], Avg: -3.4551 (0.020)
Step: 696000, Reward: -2.8000 [1.17], Avg: -3.4573 (0.020)
Step: 699000, Reward: -2.8000 [0.98], Avg: -3.4586 (0.020)
Step: 702000, Reward: -2.4000 [0.80], Avg: -3.4575 (0.020)
Step: 705000, Reward: -1.4000 [0.49], Avg: -3.4509 (0.020)
Step: 708000, Reward: -1.8000 [0.75], Avg: -3.4471 (0.020)
Step: 711000, Reward: -1.8000 [0.75], Avg: -3.4433 (0.020)
Step: 714000, Reward: -3.0000 [1.10], Avg: -3.4460 (0.020)
Step: 717000, Reward: -2.2000 [1.17], Avg: -3.4457 (0.020)
Step: 720000, Reward: -2.4000 [2.06], Avg: -3.4499 (0.020)
Step: 723000, Reward: -1.6000 [1.36], Avg: -3.4479 (0.020)
Step: 726000, Reward: -1.8000 [1.60], Avg: -3.4477 (0.020)
Step: 729000, Reward: -2.6000 [1.85], Avg: -3.4518 (0.020)
Step: 732000, Reward: -2.4000 [1.36], Avg: -3.4530 (0.020)
Step: 735000, Reward: -1.4000 [1.02], Avg: -3.4488 (0.020)
Step: 738000, Reward: -2.2000 [0.98], Avg: -3.4478 (0.020)
Step: 741000, Reward: -3.4000 [1.50], Avg: -3.4536 (0.020)
Step: 744000, Reward: -2.4000 [1.02], Avg: -3.4535 (0.020)
Step: 747000, Reward: -1.6000 [1.02], Avg: -3.4501 (0.020)
Step: 750000, Reward: -1.4000 [1.02], Avg: -3.4460 (0.020)
Step: 753000, Reward: -2.2000 [1.83], Avg: -3.4484 (0.020)
Step: 756000, Reward: -1.0000 [1.26], Avg: -3.4437 (0.020)
Step: 759000, Reward: -2.6000 [1.62], Avg: -3.4468 (0.020)
Step: 762000, Reward: -2.2000 [0.75], Avg: -3.4448 (0.020)
Step: 765000, Reward: -1.8000 [1.17], Avg: -3.4429 (0.020)
Step: 768000, Reward: -2.2000 [1.33], Avg: -3.4433 (0.020)
Step: 771000, Reward: -1.8000 [1.17], Avg: -3.4414 (0.020)
Step: 774000, Reward: -2.0000 [0.00], Avg: -3.4358 (0.020)
Step: 777000, Reward: -1.4000 [1.20], Avg: -3.4326 (0.020)
Step: 780000, Reward: -2.0000 [1.26], Avg: -3.4320 (0.020)
Step: 783000, Reward: -1.4000 [0.80], Avg: -3.4273 (0.020)
Step: 786000, Reward: -1.6000 [1.36], Avg: -3.4255 (0.020)
Step: 789000, Reward: -2.4000 [1.74], Avg: -3.4282 (0.020)
Step: 792000, Reward: -1.6000 [1.36], Avg: -3.4264 (0.020)
Step: 795000, Reward: -1.8000 [1.17], Avg: -3.4247 (0.020)
Step: 798000, Reward: -2.8000 [1.47], Avg: -3.4279 (0.020)
Step: 801000, Reward: -1.6000 [0.80], Avg: -3.4240 (0.020)
Step: 804000, Reward: -4.0000 [1.67], Avg: -3.4324 (0.020)
Step: 807000, Reward: -1.6000 [0.80], Avg: -3.4286 (0.020)
Step: 810000, Reward: -1.8000 [0.98], Avg: -3.4262 (0.020)
Step: 813000, Reward: -3.0000 [1.26], Avg: -3.4293 (0.020)
Step: 816000, Reward: -2.8000 [0.75], Avg: -3.4297 (0.020)
Step: 819000, Reward: -0.8000 [0.75], Avg: -3.4228 (0.020)
Step: 822000, Reward: -2.0000 [1.26], Avg: -3.4223 (0.020)
Step: 825000, Reward: -0.6000 [0.80], Avg: -3.4149 (0.020)
Step: 828000, Reward: -0.8000 [0.75], Avg: -3.4082 (0.020)
Step: 831000, Reward: -1.4000 [1.02], Avg: -3.4046 (0.020)
Step: 834000, Reward: -3.0000 [1.26], Avg: -3.4077 (0.020)
Step: 837000, Reward: -0.8000 [0.40], Avg: -3.3998 (0.020)
Step: 840000, Reward: -2.4000 [0.49], Avg: -3.3980 (0.020)
Step: 843000, Reward: -1.2000 [1.17], Avg: -3.3944 (0.020)
Step: 846000, Reward: -2.4000 [2.06], Avg: -3.3981 (0.020)
Step: 849000, Reward: -2.0000 [1.79], Avg: -3.3995 (0.020)
Step: 852000, Reward: -1.8000 [1.17], Avg: -3.3980 (0.020)
Step: 855000, Reward: -2.2000 [1.17], Avg: -3.3979 (0.020)
Step: 858000, Reward: -2.6000 [2.24], Avg: -3.4029 (0.020)
Step: 861000, Reward: -1.8000 [1.33], Avg: -3.4019 (0.020)
Step: 864000, Reward: -1.2000 [0.75], Avg: -3.3969 (0.020)
Step: 867000, Reward: -3.0000 [1.41], Avg: -3.4004 (0.020)
Step: 870000, Reward: -3.6000 [1.85], Avg: -3.4075 (0.020)
Step: 873000, Reward: -2.2000 [0.98], Avg: -3.4067 (0.020)
Step: 876000, Reward: -0.8000 [0.75], Avg: -3.4004 (0.020)
Step: 879000, Reward: -2.2000 [1.60], Avg: -3.4017 (0.020)
Step: 882000, Reward: -1.4000 [2.58], Avg: -3.4037 (0.020)
Step: 885000, Reward: -2.2000 [1.47], Avg: -3.4046 (0.020)
Step: 888000, Reward: -3.4000 [1.62], Avg: -3.4100 (0.020)
Step: 891000, Reward: -1.6000 [1.36], Avg: -3.4085 (0.020)
Step: 894000, Reward: -2.2000 [0.40], Avg: -3.4058 (0.020)
Step: 897000, Reward: -1.4000 [1.02], Avg: -3.4025 (0.020)
Step: 900000, Reward: -2.2000 [0.98], Avg: -3.4018 (0.020)
Step: 903000, Reward: -1.4000 [1.02], Avg: -3.3985 (0.020)
Step: 906000, Reward: -1.6000 [0.49], Avg: -3.3942 (0.020)
Step: 909000, Reward: -2.0000 [1.10], Avg: -3.3932 (0.020)
Step: 912000, Reward: -1.4000 [1.02], Avg: -3.3900 (0.020)
Step: 915000, Reward: -3.0000 [1.10], Avg: -3.3923 (0.020)
Step: 918000, Reward: -2.0000 [1.41], Avg: -3.3924 (0.020)
Step: 921000, Reward: -3.4000 [1.50], Avg: -3.3973 (0.020)
Step: 924000, Reward: -1.0000 [0.63], Avg: -3.3916 (0.020)
Step: 927000, Reward: -2.6000 [1.02], Avg: -3.3923 (0.020)
Step: 930000, Reward: -2.2000 [2.04], Avg: -3.3950 (0.020)
Step: 933000, Reward: -2.6000 [1.36], Avg: -3.3968 (0.020)
Step: 936000, Reward: -1.0000 [1.10], Avg: -3.3927 (0.020)
Step: 939000, Reward: -1.2000 [1.17], Avg: -3.3894 (0.020)
Step: 942000, Reward: -2.2000 [0.98], Avg: -3.3887 (0.020)
Step: 945000, Reward: -2.0000 [0.63], Avg: -3.3863 (0.020)
Step: 948000, Reward: -1.4000 [1.50], Avg: -3.3848 (0.020)
Step: 951000, Reward: -2.4000 [1.50], Avg: -3.3864 (0.020)
Step: 954000, Reward: -2.0000 [1.41], Avg: -3.3865 (0.020)
Step: 957000, Reward: -0.8000 [0.75], Avg: -3.3808 (0.020)
Step: 960000, Reward: -1.6000 [1.02], Avg: -3.3784 (0.020)
Step: 963000, Reward: -2.0000 [2.19], Avg: -3.3809 (0.020)
Step: 966000, Reward: -1.6000 [0.80], Avg: -3.3779 (0.020)
Step: 969000, Reward: -1.6000 [0.49], Avg: -3.3739 (0.020)
Step: 972000, Reward: -3.0000 [1.26], Avg: -3.3766 (0.020)
Step: 975000, Reward: -2.8000 [1.60], Avg: -3.3798 (0.020)
Step: 978000, Reward: -3.8000 [2.04], Avg: -3.3873 (0.020)
Step: 981000, Reward: -2.4000 [1.50], Avg: -3.3889 (0.020)
Step: 984000, Reward: -2.8000 [1.17], Avg: -3.3906 (0.020)
Step: 987000, Reward: -2.6000 [3.26], Avg: -3.3981 (0.020)
Step: 990000, Reward: -1.0000 [1.10], Avg: -3.3942 (0.020)
Step: 993000, Reward: -3.4000 [1.02], Avg: -3.3973 (0.020)
Step: 996000, Reward: -2.8000 [2.32], Avg: -3.4024 (0.020)
Step: 999000, Reward: -1.2000 [1.17], Avg: -3.3993 (0.020)
