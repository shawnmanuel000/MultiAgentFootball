Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: 3_vs_3_custom
num_envs: 32, state_size: [(1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115)], action_size: [[1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19]], action_space: [MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19])], envs: <class 'utils.envs.MPIEnv'>,

import torch
import random
import numpy as np
from models.ppo import PPOActor, PPOCritic, PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: PPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, PPOActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, states_joint, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):
			values = model.get_value(states_joint[:-1], grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean())
			model.soft_copy(model.critic_local, model.critic_target)

			entropy, new_log_prob = model.get_action_probs(state[:-1], action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			self.network.optimize(states, actions, log_probs, states_joint, targets, advantages)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPIEnv, MPI_SIZE
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	envs = (MPIEnv if MPI_SIZE > 1 else EnvManager if type(ports) == list else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, num_envs=envs.num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(envs.num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.0, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=MPI_SIZE if MPI_SIZE > 1 else args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.2 -0.2 -0.2 -0.6 -0.6 -0.6] (1.000)
Step: 999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.1 -0.1 -0.1 -0.3 -0.3 -0.3] (1.000)
Step: 1499, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [0.7746], Avg: [-0.43  -0.43  -0.43  -0.163 -0.163 -0.163] (1.000)
Step: 1999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.545 -0.545 -0.545 -0.145 -0.145 -0.145] (1.000)
Step: 2499, Reward: [-1. -1. -1.  1.  1.  1.] [1.3416], Avg: [-0.815 -0.815 -0.815 -0.095 -0.095 -0.095] (1.000)
Step: 2999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.827 -0.827 -0.827 -0.094 -0.094 -0.094] (1.000)
Step: 3499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.709 -0.709 -0.709 -0.081 -0.081 -0.081] (1.000)
Step: 3999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.732 -0.732 -0.732 -0.082 -0.082 -0.082] (1.000)
Step: 4499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.65  -0.65  -0.65  -0.073 -0.073 -0.073] (1.000)
Step: 4999, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [0.7746], Avg: [-0.694 -0.694 -0.694 -0.054 -0.054 -0.054] (1.000)
Step: 5499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.686 -0.686 -0.686 -0.068 -0.068 -0.068] (1.000)
Step: 5999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.629 -0.629 -0.629 -0.062 -0.062 -0.062] (1.000)
Step: 6499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.626 -0.626 -0.626 -0.073 -0.073 -0.073] (1.000)
Step: 6999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.582 -0.582 -0.582 -0.067 -0.067 -0.067] (1.000)
Step: 7499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.543 -0.543 -0.543 -0.063 -0.063 -0.063] (1.000)
Step: 7999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.509 -0.509 -0.509 -0.059 -0.059 -0.059] (1.000)
Step: 8499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.491 -0.491 -0.491 -0.091 -0.091 -0.091] (1.000)
Step: 8999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.464 -0.464 -0.464 -0.086 -0.086 -0.086] (1.000)
Step: 9499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.471 -0.471 -0.471 -0.092 -0.092 -0.092] (1.000)
Step: 9999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.477 -0.477 -0.477 -0.097 -0.097 -0.097] (1.000)
Step: 10499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.483 -0.483 -0.483 -0.102 -0.102 -0.102] (1.000)
Step: 10999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.488 -0.488 -0.488 -0.107 -0.107 -0.107] (1.000)
Step: 11499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.467 -0.467 -0.467 -0.102 -0.102 -0.102] (1.000)
Step: 11999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.473 -0.473 -0.473 -0.106 -0.106 -0.106] (1.000)
Step: 12499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.454 -0.454 -0.454 -0.102 -0.102 -0.102] (1.000)
Step: 12999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.459 -0.459 -0.459 -0.106 -0.106 -0.106] (1.000)
Step: 13499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.465 -0.465 -0.465 -0.109 -0.109 -0.109] (1.000)
Step: 13999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.469 -0.469 -0.469 -0.112 -0.112 -0.112] (1.000)
Step: 14499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.453 -0.453 -0.453 -0.108 -0.108 -0.108] (1.000)
Step: 14999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.438 -0.438 -0.438 -0.105 -0.105 -0.105] (1.000)
Step: 15499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.443 -0.443 -0.443 -0.108 -0.108 -0.108] (1.000)
Step: 15999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.429 -0.429 -0.429 -0.104 -0.104 -0.104] (1.000)
Step: 16499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.443 -0.443 -0.443 -0.104 -0.104 -0.104] (1.000)
Step: 16999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.43  -0.43  -0.43  -0.101 -0.101 -0.101] (1.000)
Step: 17499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.418 -0.418 -0.418 -0.098 -0.098 -0.098] (1.000)
Step: 17999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.406 -0.406 -0.406 -0.095 -0.095 -0.095] (1.000)
Step: 18499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.396 -0.396 -0.396 -0.093 -0.093 -0.093] (1.000)
Step: 18999, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [0.7746], Avg: [-0.414 -0.414 -0.414 -0.087 -0.087 -0.087] (1.000)
Step: 19499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.419 -0.419 -0.419 -0.09  -0.09  -0.09 ] (1.000)
Step: 19999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.423 -0.423 -0.423 -0.093 -0.093 -0.093] (1.000)
Step: 20499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.413 -0.413 -0.413 -0.091 -0.091 -0.091] (1.000)
Step: 20999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.403 -0.403 -0.403 -0.089 -0.089 -0.089] (1.000)
Step: 21499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.394 -0.394 -0.394 -0.087 -0.087 -0.087] (1.000)
Step: 21999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.385 -0.385 -0.385 -0.085 -0.085 -0.085] (1.000)
Step: 22499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.381 -0.381 -0.381 -0.096 -0.096 -0.096] (1.000)
Step: 22999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.372 -0.372 -0.372 -0.094 -0.094 -0.094] (1.000)
Step: 23499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.364 -0.364 -0.364 -0.092 -0.092 -0.092] (1.000)
Step: 23999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.361 -0.361 -0.361 -0.103 -0.103 -0.103] (1.000)
Step: 24499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.354 -0.354 -0.354 -0.1   -0.1   -0.1  ] (1.000)
Step: 24999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.346 -0.346 -0.346 -0.098 -0.098 -0.098] (1.000)
Step: 25499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.34  -0.34  -0.34  -0.097 -0.097 -0.097] (1.000)
Step: 25999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.333 -0.333 -0.333 -0.095 -0.095 -0.095] (1.000)
Step: 26499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.327 -0.327 -0.327 -0.093 -0.093 -0.093] (1.000)
Step: 26999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.321 -0.321 -0.321 -0.091 -0.091 -0.091] (1.000)
Step: 27499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.315 -0.315 -0.315 -0.09  -0.09  -0.09 ] (1.000)
Step: 27999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.309 -0.309 -0.309 -0.088 -0.088 -0.088] (1.000)
Step: 28499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.304 -0.304 -0.304 -0.086 -0.086 -0.086] (1.000)
Step: 28999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.085 -0.085 -0.085] (1.000)
Step: 29499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.083 -0.083 -0.083] (1.000)
Step: 29999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.082 -0.082 -0.082] (1.000)
Step: 30499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.081 -0.081 -0.081] (1.000)
Step: 30999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.089 -0.089 -0.089] (1.000)
Step: 31499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.088 -0.088 -0.088] (1.000)
Step: 31999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.086 -0.086 -0.086] (1.000)
Step: 32499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.273 -0.273 -0.273 -0.094 -0.094 -0.094] (1.000)
Step: 32999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.093 -0.093 -0.093] (1.000)
Step: 33499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.091 -0.091 -0.091] (1.000)
Step: 33999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.09  -0.09  -0.09 ] (1.000)
Step: 34499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.089 -0.089 -0.089] (1.000)
Step: 34999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.096 -0.096 -0.096] (1.000)
Step: 35499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.103 -0.103 -0.103] (1.000)
Step: 35999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.102 -0.102 -0.102] (1.000)
Step: 36499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.1   -0.1   -0.1  ] (1.000)
Step: 36999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.099 -0.099 -0.099] (1.000)
Step: 37499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.098 -0.098 -0.098] (1.000)
Step: 37999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.096 -0.096 -0.096] (1.000)
Step: 38499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.095 -0.095 -0.095] (1.000)
Step: 38999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.094 -0.094 -0.094] (1.000)
Step: 39499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.093 -0.093 -0.093] (1.000)
Step: 39999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.092 -0.092 -0.092] (1.000)
Step: 40499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.09  -0.09  -0.09 ] (1.000)
Step: 40999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.092 -0.092 -0.092] (1.000)
Step: 41499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.091 -0.091 -0.091] (1.000)
Step: 41999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.09  -0.09  -0.09 ] (1.000)
Step: 42499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.096 -0.096 -0.096] (1.000)
Step: 42999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.101 -0.101 -0.101] (1.000)
Step: 43499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.107 -0.107 -0.107] (1.000)
Step: 43999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.106 -0.106 -0.106] (1.000)
Step: 44499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.105 -0.105 -0.105] (1.000)
Step: 44999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.104 -0.104 -0.104] (1.000)
Step: 45499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.212 -0.212 -0.212 -0.102 -0.102 -0.102] (1.000)
Step: 45999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.21  -0.21  -0.21  -0.101 -0.101 -0.101] (1.000)
Step: 46499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.208 -0.208 -0.208 -0.1   -0.1   -0.1  ] (1.000)
Step: 46999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.206 -0.206 -0.206 -0.099 -0.099 -0.099] (1.000)
Step: 47499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.203 -0.203 -0.203 -0.098 -0.098 -0.098] (1.000)
Step: 47999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.203 -0.203 -0.203 -0.103 -0.103 -0.103] (1.000)
Step: 48499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.203 -0.203 -0.203 -0.108 -0.108 -0.108] (1.000)
Step: 48999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.202 -0.202 -0.202 -0.116 -0.116 -0.116] (1.000)
Step: 49499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.2   -0.2   -0.2   -0.115 -0.115 -0.115] (1.000)
Step: 49999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.198 -0.198 -0.198 -0.114 -0.114 -0.114] (1.000)
Step: 50499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.196 -0.196 -0.196 -0.113 -0.113 -0.113] (1.000)
Step: 50999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.194 -0.194 -0.194 -0.112 -0.112 -0.112] (1.000)
Step: 51499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.192 -0.192 -0.192 -0.111 -0.111 -0.111] (1.000)
Step: 51999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.192 -0.192 -0.192 -0.116 -0.116 -0.116] (1.000)
Step: 52499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.193 -0.193 -0.193 -0.12  -0.12  -0.12 ] (1.000)
Step: 52999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.191 -0.191 -0.191 -0.119 -0.119 -0.119] (1.000)
Step: 53499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.189 -0.189 -0.189 -0.118 -0.118 -0.118] (1.000)
Step: 53999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.187 -0.187 -0.187 -0.117 -0.117 -0.117] (1.000)
Step: 54499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.185 -0.185 -0.185 -0.116 -0.116 -0.116] (1.000)
Step: 54999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.184 -0.184 -0.184 -0.115 -0.115 -0.115] (1.000)
Step: 55499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.184 -0.184 -0.184 -0.119 -0.119 -0.119] (1.000)
Step: 55999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.184 -0.184 -0.184 -0.123 -0.123 -0.123] (1.000)
Step: 56499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.183 -0.183 -0.183 -0.13  -0.13  -0.13 ] (1.000)
Step: 56999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.182 -0.182 -0.182 -0.129 -0.129 -0.129] (1.000)
Step: 57499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.18  -0.18  -0.18  -0.128 -0.128 -0.128] (1.000)
Step: 57999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.18  -0.18  -0.18  -0.132 -0.132 -0.132] (1.000)
Step: 58499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.184 -0.184 -0.184 -0.133 -0.133 -0.133] (1.000)
Step: 58999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.182 -0.182 -0.182 -0.131 -0.131 -0.131] (1.000)
Step: 59499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.182 -0.182 -0.182 -0.135 -0.135 -0.135] (1.000)
Step: 59999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.181 -0.181 -0.181 -0.134 -0.134 -0.134] (1.000)
Step: 60499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.181 -0.181 -0.181 -0.138 -0.138 -0.138] (1.000)
Step: 60999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.18  -0.18  -0.18  -0.137 -0.137 -0.137] (1.000)
Step: 61499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.178 -0.178 -0.178 -0.136 -0.136 -0.136] (1.000)
Step: 61999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.178 -0.178 -0.178 -0.14  -0.14  -0.14 ] (1.000)
Step: 62499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.177 -0.177 -0.177 -0.138 -0.138 -0.138] (1.000)
Step: 62999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.175 -0.175 -0.175 -0.137 -0.137 -0.137] (1.000)
Step: 63499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.176 -0.176 -0.176 -0.141 -0.141 -0.141] (1.000)
Step: 63999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.174 -0.174 -0.174 -0.14  -0.14  -0.14 ] (1.000)
Step: 64499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.173 -0.173 -0.173 -0.139 -0.139 -0.139] (1.000)
Step: 64999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.172 -0.172 -0.172 -0.138 -0.138 -0.138] (1.000)
Step: 65499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.17  -0.17  -0.17  -0.137 -0.137 -0.137] (1.000)
Step: 65999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.169 -0.169 -0.169 -0.136 -0.136 -0.136] (1.000)
Step: 66499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.168 -0.168 -0.168 -0.135 -0.135 -0.135] (1.000)
Step: 66999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.166 -0.166 -0.166 -0.134 -0.134 -0.134] (1.000)
Step: 67499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.165 -0.165 -0.165 -0.133 -0.133 -0.133] (1.000)
Step: 67999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.164 -0.164 -0.164 -0.132 -0.132 -0.132] (1.000)
Step: 68499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.164 -0.164 -0.164 -0.135 -0.135 -0.135] (1.000)
Step: 68999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.163 -0.163 -0.163 -0.134 -0.134 -0.134] (1.000)
Step: 69499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.162 -0.162 -0.162 -0.133 -0.133 -0.133] (1.000)
Step: 69999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.162 -0.162 -0.162 -0.136 -0.136 -0.136] (1.000)
Step: 70499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.161 -0.161 -0.161 -0.135 -0.135 -0.135] (1.000)
Step: 70999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.164 -0.164 -0.164 -0.136 -0.136 -0.136] (1.000)
Step: 71499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.163 -0.163 -0.163 -0.135 -0.135 -0.135] (1.000)
Step: 71999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.162 -0.162 -0.162 -0.134 -0.134 -0.134] (1.000)
Step: 72499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.161 -0.161 -0.161 -0.133 -0.133 -0.133] (1.000)
Step: 72999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.161 -0.161 -0.161 -0.136 -0.136 -0.136] (1.000)
Step: 73499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.16  -0.16  -0.16  -0.135 -0.135 -0.135] (1.000)
Step: 73999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.159 -0.159 -0.159 -0.134 -0.134 -0.134] (1.000)
Step: 74499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.158 -0.158 -0.158 -0.134 -0.134 -0.134] (1.000)
Step: 74999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.157 -0.157 -0.157 -0.133 -0.133 -0.133] (1.000)
Step: 75499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.156 -0.156 -0.156 -0.132 -0.132 -0.132] (1.000)
Step: 75999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.155 -0.155 -0.155 -0.131 -0.131 -0.131] (1.000)
Step: 76499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.154 -0.154 -0.154 -0.13  -0.13  -0.13 ] (1.000)
Step: 76999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.153 -0.153 -0.153 -0.129 -0.129 -0.129] (1.000)
Step: 77499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.152 -0.152 -0.152 -0.128 -0.128 -0.128] (1.000)
Step: 77999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.151 -0.151 -0.151 -0.128 -0.128 -0.128] (1.000)
Step: 78499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.15  -0.15  -0.15  -0.127 -0.127 -0.127] (1.000)
Step: 78999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.15 -0.15 -0.15 -0.13 -0.13 -0.13] (1.000)
Step: 79499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.15  -0.15  -0.15  -0.133 -0.133 -0.133] (1.000)
Step: 79999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.149 -0.149 -0.149 -0.132 -0.132 -0.132] (1.000)
Step: 80499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.148 -0.148 -0.148 -0.131 -0.131 -0.131] (1.000)
Step: 80999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.149 -0.149 -0.149 -0.134 -0.134 -0.134] (1.000)
Step: 81499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.149 -0.149 -0.149 -0.137 -0.137 -0.137] (1.000)
Step: 81999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.149 -0.149 -0.149 -0.14  -0.14  -0.14 ] (1.000)
Step: 82499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.15  -0.15  -0.15  -0.142 -0.142 -0.142] (1.000)
Step: 82999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.149 -0.149 -0.149 -0.142 -0.142 -0.142] (1.000)
Step: 83499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.149 -0.149 -0.149 -0.144 -0.144 -0.144] (1.000)
Step: 83999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.148 -0.148 -0.148 -0.143 -0.143 -0.143] (1.000)
Step: 84499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.147 -0.147 -0.147 -0.143 -0.143 -0.143] (1.000)
Step: 84999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.146 -0.146 -0.146 -0.142 -0.142 -0.142] (1.000)
Step: 85499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.147 -0.147 -0.147 -0.144 -0.144 -0.144] (1.000)
Step: 85999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.15  -0.15  -0.15  -0.147 -0.147 -0.147] (1.000)
Step: 86499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.149 -0.149 -0.149 -0.146 -0.146 -0.146] (1.000)
Step: 86999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.148 -0.148 -0.148 -0.146 -0.146 -0.146] (1.000)
Step: 87499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.147 -0.147 -0.147 -0.145 -0.145 -0.145] (1.000)
Step: 87999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.146 -0.146 -0.146 -0.144 -0.144 -0.144] (1.000)
Step: 88499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.145 -0.145 -0.145 -0.143 -0.143 -0.143] (1.000)
Step: 88999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.145 -0.145 -0.145 -0.142 -0.142 -0.142] (1.000)
Step: 89499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.144 -0.144 -0.144 -0.142 -0.142 -0.142] (1.000)
Step: 89999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.143 -0.143 -0.143 -0.141 -0.141 -0.141] (1.000)
Step: 90499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.142 -0.142 -0.142 -0.14  -0.14  -0.14 ] (1.000)
Step: 90999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.145 -0.145 -0.145 -0.14  -0.14  -0.14 ] (1.000)
Step: 91499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.145 -0.145 -0.145 -0.143 -0.143 -0.143] (1.000)
Step: 91999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.144 -0.144 -0.144 -0.142 -0.142 -0.142] (1.000)
Step: 92499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.143 -0.143 -0.143 -0.141 -0.141 -0.141] (1.000)
Step: 92999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.143 -0.143 -0.143 -0.141 -0.141 -0.141] (1.000)
Step: 93499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.142 -0.142 -0.142 -0.14  -0.14  -0.14 ] (1.000)
Step: 93999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.141 -0.141 -0.141 -0.139 -0.139 -0.139] (1.000)
Step: 94499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.14  -0.14  -0.14  -0.138 -0.138 -0.138] (1.000)
Step: 94999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.14  -0.14  -0.14  -0.138 -0.138 -0.138] (1.000)
Step: 95499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.142 -0.142 -0.142 -0.138 -0.138 -0.138] (1.000)
Step: 95999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.141 -0.141 -0.141 -0.137 -0.137 -0.137] (1.000)
Step: 96499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.141 -0.141 -0.141 -0.136 -0.136 -0.136] (1.000)
Step: 96999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.141 -0.141 -0.141 -0.139 -0.139 -0.139] (1.000)
Step: 97499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.14  -0.14  -0.14  -0.138 -0.138 -0.138] (1.000)
Step: 97999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.139 -0.139 -0.139 -0.137 -0.137 -0.137] (1.000)
Step: 98499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.139 -0.139 -0.139 -0.137 -0.137 -0.137] (1.000)
Step: 98999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.138 -0.138 -0.138 -0.136 -0.136 -0.136] (1.000)
Step: 99499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.137 -0.137 -0.137 -0.135 -0.135 -0.135] (1.000)
Step: 99999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.137 -0.137 -0.137 -0.135 -0.135 -0.135] (1.000)
Step: 100499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.136 -0.136 -0.136 -0.134 -0.134 -0.134] (1.000)
Step: 100999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.135 -0.135 -0.135 -0.133 -0.133 -0.133] (1.000)
Step: 101499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.135 -0.135 -0.135 -0.133 -0.133 -0.133] (1.000)
Step: 101999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.134 -0.134 -0.134 -0.132 -0.132 -0.132] (1.000)
Step: 102499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.134 -0.134 -0.134 -0.134 -0.134 -0.134] (1.000)
Step: 102999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.137 -0.137 -0.137 -0.135 -0.135 -0.135] (1.000)
Step: 103499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.139 -0.139 -0.139 -0.135 -0.135 -0.135] (1.000)
Step: 103999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.139 -0.139 -0.139 -0.137 -0.137 -0.137] (1.000)
Step: 104499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.138 -0.138 -0.138 -0.137 -0.137 -0.137] (1.000)
Step: 104999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.138 -0.138 -0.138 -0.136 -0.136 -0.136] (1.000)
Step: 105499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.137 -0.137 -0.137 -0.135 -0.135 -0.135] (1.000)
Step: 105999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.137 -0.137 -0.137 -0.137 -0.137 -0.137] (1.000)
Step: 106499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.137 -0.137 -0.137 -0.137 -0.137 -0.137] (1.000)
Step: 106999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.136 -0.136 -0.136 -0.136 -0.136 -0.136] (1.000)
Step: 107499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.136 -0.136 -0.136 -0.136 -0.136 -0.136] (1.000)
Step: 107999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.135 -0.135 -0.135 -0.135 -0.135 -0.135] (1.000)
Step: 108499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.135 -0.135 -0.135 -0.137 -0.137 -0.137] (1.000)
Step: 108999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.135 -0.135 -0.135 -0.136 -0.136 -0.136] (1.000)
Step: 109499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.134 -0.134 -0.134 -0.136 -0.136 -0.136] (1.000)
Step: 109999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.133 -0.133 -0.133 -0.135 -0.135 -0.135] (1.000)
Step: 110499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.133 -0.133 -0.133 -0.135 -0.135 -0.135] (1.000)
Step: 110999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.133 -0.133 -0.133 -0.138 -0.138 -0.138] (1.000)
Step: 111499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.133 -0.133 -0.133 -0.14  -0.14  -0.14 ] (1.000)
Step: 111999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.133 -0.133 -0.133 -0.142 -0.142 -0.142] (1.000)
Step: 112499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.133 -0.133 -0.133 -0.144 -0.144 -0.144] (1.000)
Step: 112999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.133 -0.133 -0.133 -0.143 -0.143 -0.143] (1.000)
Step: 113499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.133 -0.133 -0.133 -0.145 -0.145 -0.145] (1.000)
Step: 113999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.133 -0.133 -0.133 -0.145 -0.145 -0.145] (1.000)
Step: 114499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.133 -0.133 -0.133 -0.147 -0.147 -0.147] (1.000)
Step: 114999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.132 -0.132 -0.132 -0.146 -0.146 -0.146] (1.000)
Step: 115499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.132 -0.132 -0.132 -0.146 -0.146 -0.146] (1.000)
Step: 115999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.145 -0.145 -0.145] (1.000)
Step: 116499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.144 -0.144 -0.144] (1.000)
Step: 116999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.131 -0.131 -0.131 -0.146 -0.146 -0.146] (1.000)
Step: 117499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.13 -0.13 -0.13 -0.15 -0.15 -0.15] (1.000)
Step: 117999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.129 -0.129 -0.129 -0.15  -0.15  -0.15 ] (1.000)
Step: 118499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.129 -0.129 -0.129 -0.149 -0.149 -0.149] (1.000)
Step: 118999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.129 -0.129 -0.129 -0.151 -0.151 -0.151] (1.000)
Step: 119499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.129 -0.129 -0.129 -0.15  -0.15  -0.15 ] (1.000)
Step: 119999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.128 -0.128 -0.128 -0.15  -0.15  -0.15 ] (1.000)
Step: 120499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.127 -0.127 -0.127 -0.149 -0.149 -0.149] (1.000)
Step: 120999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.127 -0.127 -0.127 -0.148 -0.148 -0.148] (1.000)
Step: 121499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.127 -0.127 -0.127 -0.15  -0.15  -0.15 ] (1.000)
Step: 121999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.128 -0.128 -0.128 -0.152 -0.152 -0.152] (1.000)
Step: 122499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.127 -0.127 -0.127 -0.151 -0.151 -0.151] (1.000)
Step: 122999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.129 -0.129 -0.129 -0.152 -0.152 -0.152] (1.000)
Step: 123499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.131 -0.131 -0.131 -0.154 -0.154 -0.154] (1.000)
Step: 123999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.13  -0.13  -0.13  -0.153 -0.153 -0.153] (1.000)
Step: 124499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.13  -0.13  -0.13  -0.156 -0.156 -0.156] (1.000)
Step: 124999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.13  -0.13  -0.13  -0.155 -0.155 -0.155] (1.000)
Step: 125499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.132 -0.132 -0.132 -0.157 -0.157 -0.157] (1.000)
Step: 125999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.157 -0.157 -0.157] (1.000)
Step: 126499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.156 -0.156 -0.156] (1.000)
Step: 126999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.132 -0.132 -0.132 -0.16  -0.16  -0.16 ] (1.000)
Step: 127499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.132 -0.132 -0.132 -0.162 -0.162 -0.162] (1.000)
Step: 127999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.132 -0.132 -0.132 -0.164 -0.164 -0.164] (1.000)
Step: 128499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.132 -0.132 -0.132 -0.166 -0.166 -0.166] (1.000)
Step: 128999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.132 -0.132 -0.132 -0.166 -0.166 -0.166] (1.000)
Step: 129499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.165 -0.165 -0.165] (1.000)
Step: 129999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.131 -0.131 -0.131 -0.164 -0.164 -0.164] (1.000)
Step: 130499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.13  -0.13  -0.13  -0.164 -0.164 -0.164] (1.000)
Step: 130999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.13  -0.13  -0.13  -0.163 -0.163 -0.163] (1.000)
Step: 131499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.129 -0.129 -0.129 -0.163 -0.163 -0.163] (1.000)
Step: 131999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.129 -0.129 -0.129 -0.162 -0.162 -0.162] (1.000)
Step: 132499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.128 -0.128 -0.128 -0.165 -0.165 -0.165] (1.000)
Step: 132999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.128 -0.128 -0.128 -0.167 -0.167 -0.167] (1.000)
Step: 133499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.128 -0.128 -0.128 -0.169 -0.169 -0.169] (1.000)
Step: 133999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.129 -0.129 -0.129 -0.17  -0.17  -0.17 ] (1.000)
Step: 134499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.128 -0.128 -0.128 -0.17  -0.17  -0.17 ] (1.000)
Step: 134999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.128 -0.128 -0.128 -0.169 -0.169 -0.169] (1.000)
Step: 135499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.127 -0.127 -0.127 -0.168 -0.168 -0.168] (1.000)
Step: 135999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.127 -0.127 -0.127 -0.17  -0.17  -0.17 ] (1.000)
Step: 136499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.127 -0.127 -0.127 -0.173 -0.173 -0.173] (1.000)
Step: 136999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.127 -0.127 -0.127 -0.175 -0.175 -0.175] (1.000)
Step: 137499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.127 -0.127 -0.127 -0.175 -0.175 -0.175] (1.000)
Step: 137999, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [1.0000], Avg: [-0.127 -0.127 -0.127 -0.179 -0.179 -0.179] (1.000)
Step: 138499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.126 -0.126 -0.126 -0.178 -0.178 -0.178] (1.000)
Step: 138999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.126 -0.126 -0.126 -0.178 -0.178 -0.178] (1.000)
Step: 139499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.126 -0.126 -0.126 -0.177 -0.177 -0.177] (1.000)
Step: 139999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.126 -0.126 -0.126 -0.179 -0.179 -0.179] (1.000)
Step: 140499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.125 -0.125 -0.125 -0.178 -0.178 -0.178] (1.000)
Step: 140999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.125 -0.125 -0.125 -0.177 -0.177 -0.177] (1.000)
Step: 141499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.124 -0.124 -0.124 -0.177 -0.177 -0.177] (1.000)
Step: 141999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.124 -0.124 -0.124 -0.176 -0.176 -0.176] (1.000)
Step: 142499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.124 -0.124 -0.124 -0.179 -0.179 -0.179] (1.000)
Step: 142999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.178 -0.178 -0.178] (1.000)
Step: 143499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.177 -0.177 -0.177] (1.000)
Step: 143999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.123 -0.123 -0.123 -0.179 -0.179 -0.179] (1.000)
Step: 144499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.124 -0.124 -0.124 -0.18  -0.18  -0.18 ] (1.000)
Step: 144999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.18  -0.18  -0.18 ] (1.000)
Step: 145499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.179 -0.179 -0.179] (1.000)
Step: 145999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.178 -0.178 -0.178] (1.000)
Step: 146499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.178 -0.178 -0.178] (1.000)
Step: 146999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.121 -0.121 -0.121 -0.177 -0.177 -0.177] (1.000)
Step: 147499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.121 -0.121 -0.121 -0.177 -0.177 -0.177] (1.000)
Step: 147999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.121 -0.121 -0.121 -0.176 -0.176 -0.176] (1.000)
Step: 148499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.12  -0.12  -0.12  -0.175 -0.175 -0.175] (1.000)
Step: 148999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.121 -0.121 -0.121 -0.177 -0.177 -0.177] (1.000)
Step: 149499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.122 -0.122 -0.122 -0.178 -0.178 -0.178] (1.000)
Step: 149999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.122 -0.122 -0.122 -0.181 -0.181 -0.181] (1.000)
Step: 150499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.122 -0.122 -0.122 -0.182 -0.182 -0.182] (1.000)
Step: 150999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.182 -0.182 -0.182] (1.000)
Step: 151499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.181 -0.181 -0.181] (1.000)
Step: 151999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.121 -0.121 -0.121 -0.18  -0.18  -0.18 ] (1.000)
Step: 152499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.121 -0.121 -0.121 -0.182 -0.182 -0.182] (1.000)
Step: 152999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.121 -0.121 -0.121 -0.181 -0.181 -0.181] (1.000)
Step: 153499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.123 -0.123 -0.123 -0.181 -0.181 -0.181] (1.000)
Step: 153999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.181 -0.181 -0.181] (1.000)
Step: 154499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.122 -0.122 -0.122 -0.183 -0.183 -0.183] (1.000)
Step: 154999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.122 -0.122 -0.122 -0.182 -0.182 -0.182] (1.000)
Step: 155499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.122 -0.122 -0.122 -0.184 -0.184 -0.184] (1.000)
Step: 155999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.124 -0.124 -0.124 -0.184 -0.184 -0.184] (1.000)
Step: 156499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.183 -0.183 -0.183] (1.000)
Step: 156999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.123 -0.123 -0.123 -0.183 -0.183 -0.183] (1.000)
Step: 157499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.124 -0.124 -0.124 -0.183 -0.183 -0.183] (1.000)
Step: 157999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.126 -0.126 -0.126 -0.184 -0.184 -0.184] (1.000)
Step: 158499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.125 -0.125 -0.125 -0.183 -0.183 -0.183] (1.000)
Step: 158999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.125 -0.125 -0.125 -0.183 -0.183 -0.183] (1.000)
Step: 159499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.125 -0.125 -0.125 -0.182 -0.182 -0.182] (1.000)
Step: 159999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.124 -0.124 -0.124 -0.182 -0.182 -0.182] (1.000)
