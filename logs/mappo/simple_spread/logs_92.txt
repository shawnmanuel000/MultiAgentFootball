Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 32
PPO_EPOCHS = 5
TIME_BATCH = 50
MAX_BUFFER_SIZE = 160

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.attention = MultiheadAttention(ACTOR_HIDDEN, 1, 1)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()
		state = self.attention(state)
		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		self.hidden = self.recurrent(state, self.hidden)
		action_mu = self.action_mu(self.hidden).tanh()
		action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			model.actor_local.init_hidden(state.size(0), state.device)
			entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy = torch.stack(entropy).mean()
			# entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-537.825 -537.825 -537.825] [111.6110], Avg: [-537.825 -537.825 -537.825] (1.000)
Step: 2000, Reward: [-538.867 -538.867 -538.867] [149.0196], Avg: [-538.346 -538.346 -538.346] (1.000)
Step: 3000, Reward: [-565.391 -565.391 -565.391] [199.5532], Avg: [-547.361 -547.361 -547.361] (1.000)
Step: 4000, Reward: [-501.594 -501.594 -501.594] [136.4875], Avg: [-535.919 -535.919 -535.919] (1.000)
Step: 5000, Reward: [-469.104 -469.104 -469.104] [81.8413], Avg: [-522.556 -522.556 -522.556] (1.000)
Step: 6000, Reward: [-457.108 -457.108 -457.108] [108.4428], Avg: [-511.648 -511.648 -511.648] (1.000)
Step: 7000, Reward: [-476.232 -476.232 -476.232] [151.6025], Avg: [-506.589 -506.589 -506.589] (1.000)
Step: 8000, Reward: [-491.305 -491.305 -491.305] [69.1974], Avg: [-504.678 -504.678 -504.678] (1.000)
Step: 9000, Reward: [-484.574 -484.574 -484.574] [161.1792], Avg: [-502.444 -502.444 -502.444] (1.000)
Step: 10000, Reward: [-509.494 -509.494 -509.494] [145.0684], Avg: [-503.149 -503.149 -503.149] (1.000)
Step: 11000, Reward: [-518.972 -518.972 -518.972] [162.9224], Avg: [-504.588 -504.588 -504.588] (1.000)
Step: 12000, Reward: [-513.504 -513.504 -513.504] [164.5686], Avg: [-505.331 -505.331 -505.331] (1.000)
Step: 13000, Reward: [-499.895 -499.895 -499.895] [96.7267], Avg: [-504.913 -504.913 -504.913] (1.000)
Step: 14000, Reward: [-423.856 -423.856 -423.856] [62.8318], Avg: [-499.123 -499.123 -499.123] (1.000)
Step: 15000, Reward: [-474.852 -474.852 -474.852] [165.1559], Avg: [-497.505 -497.505 -497.505] (1.000)
Step: 16000, Reward: [-448.891 -448.891 -448.891] [88.2564], Avg: [-494.466 -494.466 -494.466] (1.000)
Step: 17000, Reward: [-468.507 -468.507 -468.507] [114.6570], Avg: [-492.939 -492.939 -492.939] (1.000)
Step: 18000, Reward: [-441.912 -441.912 -441.912] [57.0390], Avg: [-490.105 -490.105 -490.105] (1.000)
Step: 19000, Reward: [-476.127 -476.127 -476.127] [151.8675], Avg: [-489.369 -489.369 -489.369] (1.000)
Step: 20000, Reward: [-454.472 -454.472 -454.472] [118.6535], Avg: [-487.624 -487.624 -487.624] (1.000)
Step: 21000, Reward: [-457.967 -457.967 -457.967] [97.7982], Avg: [-486.212 -486.212 -486.212] (1.000)
Step: 22000, Reward: [-455.013 -455.013 -455.013] [73.2494], Avg: [-484.794 -484.794 -484.794] (1.000)
Step: 23000, Reward: [-469.738 -469.738 -469.738] [122.4541], Avg: [-484.139 -484.139 -484.139] (1.000)
Step: 24000, Reward: [-475.402 -475.402 -475.402] [98.6831], Avg: [-483.775 -483.775 -483.775] (1.000)
Step: 25000, Reward: [-531.245 -531.245 -531.245] [189.4175], Avg: [-485.674 -485.674 -485.674] (1.000)
Step: 26000, Reward: [-441.666 -441.666 -441.666] [89.5639], Avg: [-483.981 -483.981 -483.981] (1.000)
Step: 27000, Reward: [-455.137 -455.137 -455.137] [149.1421], Avg: [-482.913 -482.913 -482.913] (1.000)
Step: 28000, Reward: [-489.802 -489.802 -489.802] [111.0125], Avg: [-483.159 -483.159 -483.159] (1.000)
Step: 29000, Reward: [-469.936 -469.936 -469.936] [77.5228], Avg: [-482.703 -482.703 -482.703] (1.000)
Step: 30000, Reward: [-509.304 -509.304 -509.304] [137.4220], Avg: [-483.59 -483.59 -483.59] (1.000)
Step: 31000, Reward: [-543.909 -543.909 -543.909] [202.4823], Avg: [-485.535 -485.535 -485.535] (1.000)
Step: 32000, Reward: [-508.079 -508.079 -508.079] [127.5709], Avg: [-486.24 -486.24 -486.24] (1.000)
Step: 33000, Reward: [-452.577 -452.577 -452.577] [90.0133], Avg: [-485.22 -485.22 -485.22] (1.000)
Step: 34000, Reward: [-453.912 -453.912 -453.912] [99.4677], Avg: [-484.299 -484.299 -484.299] (1.000)
Step: 35000, Reward: [-506.168 -506.168 -506.168] [164.9456], Avg: [-484.924 -484.924 -484.924] (1.000)
Step: 36000, Reward: [-483.943 -483.943 -483.943] [106.2293], Avg: [-484.897 -484.897 -484.897] (1.000)
Step: 37000, Reward: [-443.954 -443.954 -443.954] [77.4459], Avg: [-483.79 -483.79 -483.79] (1.000)
Step: 38000, Reward: [-429.287 -429.287 -429.287] [84.5978], Avg: [-482.356 -482.356 -482.356] (1.000)
Step: 39000, Reward: [-466.808 -466.808 -466.808] [83.3762], Avg: [-481.957 -481.957 -481.957] (1.000)
Step: 40000, Reward: [-429.293 -429.293 -429.293] [67.3020], Avg: [-480.641 -480.641 -480.641] (1.000)
Step: 41000, Reward: [-451.039 -451.039 -451.039] [112.0732], Avg: [-479.919 -479.919 -479.919] (1.000)
Step: 42000, Reward: [-464.699 -464.699 -464.699] [86.9506], Avg: [-479.556 -479.556 -479.556] (1.000)
Step: 43000, Reward: [-495.946 -495.946 -495.946] [143.5200], Avg: [-479.937 -479.937 -479.937] (1.000)
Step: 44000, Reward: [-493.52 -493.52 -493.52] [161.6702], Avg: [-480.246 -480.246 -480.246] (1.000)
Step: 45000, Reward: [-489.624 -489.624 -489.624] [118.0291], Avg: [-480.454 -480.454 -480.454] (1.000)
Step: 46000, Reward: [-463.582 -463.582 -463.582] [112.6867], Avg: [-480.088 -480.088 -480.088] (1.000)
Step: 47000, Reward: [-467.322 -467.322 -467.322] [120.0344], Avg: [-479.816 -479.816 -479.816] (1.000)
Step: 48000, Reward: [-469.488 -469.488 -469.488] [144.9506], Avg: [-479.601 -479.601 -479.601] (1.000)
Step: 49000, Reward: [-518.778 -518.778 -518.778] [161.5802], Avg: [-480.4 -480.4 -480.4] (1.000)
Step: 50000, Reward: [-439.933 -439.933 -439.933] [44.3758], Avg: [-479.591 -479.591 -479.591] (1.000)
Step: 51000, Reward: [-424.579 -424.579 -424.579] [73.5678], Avg: [-478.512 -478.512 -478.512] (1.000)
Step: 52000, Reward: [-504.849 -504.849 -504.849] [155.8360], Avg: [-479.019 -479.019 -479.019] (1.000)
Step: 53000, Reward: [-483.998 -483.998 -483.998] [83.5509], Avg: [-479.113 -479.113 -479.113] (1.000)
Step: 54000, Reward: [-466.502 -466.502 -466.502] [94.1264], Avg: [-478.879 -478.879 -478.879] (1.000)
Step: 55000, Reward: [-482.331 -482.331 -482.331] [128.9574], Avg: [-478.942 -478.942 -478.942] (1.000)
Step: 56000, Reward: [-467.617 -467.617 -467.617] [128.8720], Avg: [-478.74 -478.74 -478.74] (1.000)
Step: 57000, Reward: [-459.278 -459.278 -459.278] [110.9876], Avg: [-478.398 -478.398 -478.398] (1.000)
Step: 58000, Reward: [-480.67 -480.67 -480.67] [107.8506], Avg: [-478.438 -478.438 -478.438] (1.000)
Step: 59000, Reward: [-445.718 -445.718 -445.718] [124.0357], Avg: [-477.883 -477.883 -477.883] (1.000)
Step: 60000, Reward: [-443.5 -443.5 -443.5] [85.1243], Avg: [-477.31 -477.31 -477.31] (1.000)
Step: 61000, Reward: [-499.348 -499.348 -499.348] [124.2430], Avg: [-477.671 -477.671 -477.671] (1.000)
Step: 62000, Reward: [-456.884 -456.884 -456.884] [74.9947], Avg: [-477.336 -477.336 -477.336] (1.000)
Step: 63000, Reward: [-440.334 -440.334 -440.334] [79.7139], Avg: [-476.749 -476.749 -476.749] (1.000)
Step: 64000, Reward: [-444.551 -444.551 -444.551] [59.0003], Avg: [-476.245 -476.245 -476.245] (1.000)
Step: 65000, Reward: [-505.631 -505.631 -505.631] [139.8122], Avg: [-476.698 -476.698 -476.698] (1.000)
Step: 66000, Reward: [-460.189 -460.189 -460.189] [103.8903], Avg: [-476.447 -476.447 -476.447] (1.000)
Step: 67000, Reward: [-435.318 -435.318 -435.318] [75.8292], Avg: [-475.834 -475.834 -475.834] (1.000)
Step: 68000, Reward: [-422.913 -422.913 -422.913] [88.4459], Avg: [-475.055 -475.055 -475.055] (1.000)
Step: 69000, Reward: [-487.157 -487.157 -487.157] [81.3672], Avg: [-475.231 -475.231 -475.231] (1.000)
Step: 70000, Reward: [-527.876 -527.876 -527.876] [128.4340], Avg: [-475.983 -475.983 -475.983] (1.000)
Step: 71000, Reward: [-442.827 -442.827 -442.827] [56.5139], Avg: [-475.516 -475.516 -475.516] (1.000)
Step: 72000, Reward: [-491.024 -491.024 -491.024] [121.3742], Avg: [-475.731 -475.731 -475.731] (1.000)
Step: 73000, Reward: [-462.871 -462.871 -462.871] [92.3142], Avg: [-475.555 -475.555 -475.555] (1.000)
Step: 74000, Reward: [-470.512 -470.512 -470.512] [111.3017], Avg: [-475.487 -475.487 -475.487] (1.000)
Step: 75000, Reward: [-491.244 -491.244 -491.244] [103.6861], Avg: [-475.697 -475.697 -475.697] (1.000)
Step: 76000, Reward: [-461.794 -461.794 -461.794] [110.6470], Avg: [-475.514 -475.514 -475.514] (1.000)
Step: 77000, Reward: [-437.048 -437.048 -437.048] [92.1746], Avg: [-475.014 -475.014 -475.014] (1.000)
Step: 78000, Reward: [-427.882 -427.882 -427.882] [67.4213], Avg: [-474.41 -474.41 -474.41] (1.000)
Step: 79000, Reward: [-452.329 -452.329 -452.329] [94.6085], Avg: [-474.131 -474.131 -474.131] (1.000)
Step: 80000, Reward: [-468.787 -468.787 -468.787] [113.1414], Avg: [-474.064 -474.064 -474.064] (1.000)
Step: 81000, Reward: [-474.566 -474.566 -474.566] [157.2167], Avg: [-474.07 -474.07 -474.07] (1.000)
Step: 82000, Reward: [-424.011 -424.011 -424.011] [73.2159], Avg: [-473.46 -473.46 -473.46] (1.000)
Step: 83000, Reward: [-371.458 -371.458 -371.458] [61.6786], Avg: [-472.231 -472.231 -472.231] (1.000)
Step: 84000, Reward: [-422.827 -422.827 -422.827] [108.3402], Avg: [-471.643 -471.643 -471.643] (1.000)
Step: 85000, Reward: [-471.334 -471.334 -471.334] [75.4022], Avg: [-471.639 -471.639 -471.639] (1.000)
Step: 86000, Reward: [-454.423 -454.423 -454.423] [126.4443], Avg: [-471.439 -471.439 -471.439] (1.000)
Step: 87000, Reward: [-525.197 -525.197 -525.197] [118.8090], Avg: [-472.057 -472.057 -472.057] (1.000)
Step: 88000, Reward: [-489.258 -489.258 -489.258] [57.3342], Avg: [-472.252 -472.252 -472.252] (1.000)
Step: 89000, Reward: [-460.124 -460.124 -460.124] [114.9262], Avg: [-472.116 -472.116 -472.116] (1.000)
Step: 90000, Reward: [-463.809 -463.809 -463.809] [89.7510], Avg: [-472.024 -472.024 -472.024] (1.000)
Step: 91000, Reward: [-454.132 -454.132 -454.132] [82.2344], Avg: [-471.827 -471.827 -471.827] (1.000)
Step: 92000, Reward: [-418.681 -418.681 -418.681] [84.9244], Avg: [-471.249 -471.249 -471.249] (1.000)
Step: 93000, Reward: [-422.527 -422.527 -422.527] [58.4166], Avg: [-470.725 -470.725 -470.725] (1.000)
Step: 94000, Reward: [-435.949 -435.949 -435.949] [89.0857], Avg: [-470.355 -470.355 -470.355] (1.000)
Step: 95000, Reward: [-460.443 -460.443 -460.443] [75.5517], Avg: [-470.251 -470.251 -470.251] (1.000)
Step: 96000, Reward: [-460.968 -460.968 -460.968] [141.4668], Avg: [-470.154 -470.154 -470.154] (1.000)
Step: 97000, Reward: [-436.978 -436.978 -436.978] [83.5001], Avg: [-469.812 -469.812 -469.812] (1.000)
Step: 98000, Reward: [-463.453 -463.453 -463.453] [108.1568], Avg: [-469.747 -469.747 -469.747] (1.000)
Step: 99000, Reward: [-427.526 -427.526 -427.526] [104.0602], Avg: [-469.321 -469.321 -469.321] (1.000)
Step: 100000, Reward: [-479.098 -479.098 -479.098] [121.5731], Avg: [-469.419 -469.419 -469.419] (1.000)
Step: 101000, Reward: [-430.724 -430.724 -430.724] [124.6303], Avg: [-469.036 -469.036 -469.036] (1.000)
Step: 102000, Reward: [-457.331 -457.331 -457.331] [107.0869], Avg: [-468.921 -468.921 -468.921] (1.000)
Step: 103000, Reward: [-471.007 -471.007 -471.007] [106.8735], Avg: [-468.941 -468.941 -468.941] (1.000)
Step: 104000, Reward: [-475.207 -475.207 -475.207] [72.7100], Avg: [-469.001 -469.001 -469.001] (1.000)
Step: 105000, Reward: [-473.465 -473.465 -473.465] [86.4954], Avg: [-469.044 -469.044 -469.044] (1.000)
Step: 106000, Reward: [-456.254 -456.254 -456.254] [79.2105], Avg: [-468.923 -468.923 -468.923] (1.000)
Step: 107000, Reward: [-462.854 -462.854 -462.854] [88.6391], Avg: [-468.867 -468.867 -468.867] (1.000)
Step: 108000, Reward: [-471.574 -471.574 -471.574] [95.0734], Avg: [-468.892 -468.892 -468.892] (1.000)
Step: 109000, Reward: [-431.431 -431.431 -431.431] [96.0962], Avg: [-468.548 -468.548 -468.548] (1.000)
Step: 110000, Reward: [-466.176 -466.176 -466.176] [114.1166], Avg: [-468.526 -468.526 -468.526] (1.000)
Step: 111000, Reward: [-474.76 -474.76 -474.76] [191.5086], Avg: [-468.583 -468.583 -468.583] (1.000)
Step: 112000, Reward: [-429.398 -429.398 -429.398] [80.2501], Avg: [-468.233 -468.233 -468.233] (1.000)
Step: 113000, Reward: [-505.204 -505.204 -505.204] [121.7003], Avg: [-468.56 -468.56 -468.56] (1.000)
Step: 114000, Reward: [-455.727 -455.727 -455.727] [89.8444], Avg: [-468.447 -468.447 -468.447] (1.000)
Step: 115000, Reward: [-458.211 -458.211 -458.211] [83.5692], Avg: [-468.358 -468.358 -468.358] (1.000)
Step: 116000, Reward: [-447.444 -447.444 -447.444] [70.8408], Avg: [-468.178 -468.178 -468.178] (1.000)
Step: 117000, Reward: [-474.922 -474.922 -474.922] [74.5049], Avg: [-468.236 -468.236 -468.236] (1.000)
Step: 118000, Reward: [-476.702 -476.702 -476.702] [68.3699], Avg: [-468.307 -468.307 -468.307] (1.000)
Step: 119000, Reward: [-450.954 -450.954 -450.954] [98.6587], Avg: [-468.162 -468.162 -468.162] (1.000)
Step: 120000, Reward: [-434.022 -434.022 -434.022] [83.1173], Avg: [-467.877 -467.877 -467.877] (1.000)
Step: 121000, Reward: [-452.866 -452.866 -452.866] [71.6282], Avg: [-467.753 -467.753 -467.753] (1.000)
Step: 122000, Reward: [-477.079 -477.079 -477.079] [86.1717], Avg: [-467.829 -467.829 -467.829] (1.000)
Step: 123000, Reward: [-444.94 -444.94 -444.94] [77.7878], Avg: [-467.643 -467.643 -467.643] (1.000)
Step: 124000, Reward: [-472.379 -472.379 -472.379] [98.4381], Avg: [-467.682 -467.682 -467.682] (1.000)
Step: 125000, Reward: [-443.624 -443.624 -443.624] [95.7047], Avg: [-467.489 -467.489 -467.489] (1.000)
Step: 126000, Reward: [-393.371 -393.371 -393.371] [70.0908], Avg: [-466.901 -466.901 -466.901] (1.000)
Step: 127000, Reward: [-435.202 -435.202 -435.202] [78.3860], Avg: [-466.651 -466.651 -466.651] (1.000)
Step: 128000, Reward: [-473.896 -473.896 -473.896] [72.0574], Avg: [-466.708 -466.708 -466.708] (1.000)
Step: 129000, Reward: [-480.94 -480.94 -480.94] [117.7647], Avg: [-466.818 -466.818 -466.818] (1.000)
Step: 130000, Reward: [-415.099 -415.099 -415.099] [88.8450], Avg: [-466.42 -466.42 -466.42] (1.000)
Step: 131000, Reward: [-463.78 -463.78 -463.78] [98.7226], Avg: [-466.4 -466.4 -466.4] (1.000)
Step: 132000, Reward: [-432.114 -432.114 -432.114] [75.2554], Avg: [-466.14 -466.14 -466.14] (1.000)
Step: 133000, Reward: [-456.303 -456.303 -456.303] [84.2732], Avg: [-466.066 -466.066 -466.066] (1.000)
Step: 134000, Reward: [-444.213 -444.213 -444.213] [81.6673], Avg: [-465.903 -465.903 -465.903] (1.000)
Step: 135000, Reward: [-443.756 -443.756 -443.756] [95.1905], Avg: [-465.739 -465.739 -465.739] (1.000)
Step: 136000, Reward: [-477.09 -477.09 -477.09] [109.7226], Avg: [-465.823 -465.823 -465.823] (1.000)
Step: 137000, Reward: [-460.364 -460.364 -460.364] [84.5803], Avg: [-465.783 -465.783 -465.783] (1.000)
Step: 138000, Reward: [-446.922 -446.922 -446.922] [74.7750], Avg: [-465.646 -465.646 -465.646] (1.000)
Step: 139000, Reward: [-414.026 -414.026 -414.026] [84.0776], Avg: [-465.275 -465.275 -465.275] (1.000)
Step: 140000, Reward: [-479.941 -479.941 -479.941] [104.3011], Avg: [-465.38 -465.38 -465.38] (1.000)
Step: 141000, Reward: [-494.435 -494.435 -494.435] [64.5371], Avg: [-465.586 -465.586 -465.586] (1.000)
Step: 142000, Reward: [-446.689 -446.689 -446.689] [77.9416], Avg: [-465.453 -465.453 -465.453] (1.000)
Step: 143000, Reward: [-445.692 -445.692 -445.692] [63.5941], Avg: [-465.314 -465.314 -465.314] (1.000)
Step: 144000, Reward: [-418.528 -418.528 -418.528] [83.1759], Avg: [-464.99 -464.99 -464.99] (1.000)
Step: 145000, Reward: [-474.219 -474.219 -474.219] [71.8566], Avg: [-465.053 -465.053 -465.053] (1.000)
Step: 146000, Reward: [-469.955 -469.955 -469.955] [104.5420], Avg: [-465.087 -465.087 -465.087] (1.000)
Step: 147000, Reward: [-444.176 -444.176 -444.176] [70.0633], Avg: [-464.945 -464.945 -464.945] (1.000)
Step: 148000, Reward: [-408.767 -408.767 -408.767] [79.5064], Avg: [-464.565 -464.565 -464.565] (1.000)
Step: 149000, Reward: [-453.209 -453.209 -453.209] [93.0800], Avg: [-464.489 -464.489 -464.489] (1.000)
Step: 150000, Reward: [-456.617 -456.617 -456.617] [69.6528], Avg: [-464.436 -464.436 -464.436] (1.000)
Step: 151000, Reward: [-457.204 -457.204 -457.204] [90.5987], Avg: [-464.388 -464.388 -464.388] (1.000)
Step: 152000, Reward: [-462.8 -462.8 -462.8] [100.2314], Avg: [-464.378 -464.378 -464.378] (1.000)
Step: 153000, Reward: [-434.94 -434.94 -434.94] [76.5677], Avg: [-464.185 -464.185 -464.185] (1.000)
Step: 154000, Reward: [-453.806 -453.806 -453.806] [60.0632], Avg: [-464.118 -464.118 -464.118] (1.000)
Step: 155000, Reward: [-473.958 -473.958 -473.958] [92.6618], Avg: [-464.182 -464.182 -464.182] (1.000)
Step: 156000, Reward: [-423.943 -423.943 -423.943] [103.5195], Avg: [-463.924 -463.924 -463.924] (1.000)
Step: 157000, Reward: [-476.615 -476.615 -476.615] [104.2618], Avg: [-464.004 -464.004 -464.004] (1.000)
Step: 158000, Reward: [-437.644 -437.644 -437.644] [97.6173], Avg: [-463.838 -463.838 -463.838] (1.000)
Step: 159000, Reward: [-424.65 -424.65 -424.65] [56.9833], Avg: [-463.591 -463.591 -463.591] (1.000)
Step: 160000, Reward: [-434.765 -434.765 -434.765] [61.4783], Avg: [-463.411 -463.411 -463.411] (1.000)
Step: 161000, Reward: [-443.572 -443.572 -443.572] [85.7637], Avg: [-463.288 -463.288 -463.288] (1.000)
Step: 162000, Reward: [-461.547 -461.547 -461.547] [86.1227], Avg: [-463.277 -463.277 -463.277] (1.000)
Step: 163000, Reward: [-448.529 -448.529 -448.529] [92.0577], Avg: [-463.187 -463.187 -463.187] (1.000)
Step: 164000, Reward: [-456.044 -456.044 -456.044] [69.6894], Avg: [-463.143 -463.143 -463.143] (1.000)
Step: 165000, Reward: [-430.119 -430.119 -430.119] [87.5848], Avg: [-462.943 -462.943 -462.943] (1.000)
Step: 166000, Reward: [-464.45 -464.45 -464.45] [100.7864], Avg: [-462.952 -462.952 -462.952] (1.000)
Step: 167000, Reward: [-457.06 -457.06 -457.06] [65.7085], Avg: [-462.917 -462.917 -462.917] (1.000)
Step: 168000, Reward: [-425.01 -425.01 -425.01] [83.3342], Avg: [-462.691 -462.691 -462.691] (1.000)
Step: 169000, Reward: [-458.732 -458.732 -458.732] [65.2695], Avg: [-462.668 -462.668 -462.668] (1.000)
Step: 170000, Reward: [-447.137 -447.137 -447.137] [68.9224], Avg: [-462.576 -462.576 -462.576] (1.000)
Step: 171000, Reward: [-464.845 -464.845 -464.845] [109.0397], Avg: [-462.589 -462.589 -462.589] (1.000)
Step: 172000, Reward: [-422.686 -422.686 -422.686] [90.9739], Avg: [-462.357 -462.357 -462.357] (1.000)
Step: 173000, Reward: [-504.185 -504.185 -504.185] [115.4113], Avg: [-462.599 -462.599 -462.599] (1.000)
Step: 174000, Reward: [-439.228 -439.228 -439.228] [100.9071], Avg: [-462.465 -462.465 -462.465] (1.000)
Step: 175000, Reward: [-459.547 -459.547 -459.547] [98.5099], Avg: [-462.448 -462.448 -462.448] (1.000)
Step: 176000, Reward: [-419.119 -419.119 -419.119] [76.2608], Avg: [-462.202 -462.202 -462.202] (1.000)
Step: 177000, Reward: [-442.182 -442.182 -442.182] [90.1027], Avg: [-462.089 -462.089 -462.089] (1.000)
Step: 178000, Reward: [-463.016 -463.016 -463.016] [89.7692], Avg: [-462.094 -462.094 -462.094] (1.000)
Step: 179000, Reward: [-458.987 -458.987 -458.987] [88.2201], Avg: [-462.077 -462.077 -462.077] (1.000)
Step: 180000, Reward: [-417.062 -417.062 -417.062] [68.7508], Avg: [-461.827 -461.827 -461.827] (1.000)
Step: 181000, Reward: [-465.456 -465.456 -465.456] [87.1107], Avg: [-461.847 -461.847 -461.847] (1.000)
Step: 182000, Reward: [-429.492 -429.492 -429.492] [93.8608], Avg: [-461.669 -461.669 -461.669] (1.000)
Step: 183000, Reward: [-468.452 -468.452 -468.452] [89.9688], Avg: [-461.706 -461.706 -461.706] (1.000)
Step: 184000, Reward: [-463.764 -463.764 -463.764] [66.1329], Avg: [-461.717 -461.717 -461.717] (1.000)
Step: 185000, Reward: [-485.76 -485.76 -485.76] [119.1764], Avg: [-461.847 -461.847 -461.847] (1.000)
Step: 186000, Reward: [-420.87 -420.87 -420.87] [66.1017], Avg: [-461.627 -461.627 -461.627] (1.000)
Step: 187000, Reward: [-426.055 -426.055 -426.055] [66.0118], Avg: [-461.437 -461.437 -461.437] (1.000)
Step: 188000, Reward: [-481.827 -481.827 -481.827] [91.7130], Avg: [-461.545 -461.545 -461.545] (1.000)
Step: 189000, Reward: [-421.859 -421.859 -421.859] [112.6487], Avg: [-461.335 -461.335 -461.335] (1.000)
Step: 190000, Reward: [-483.948 -483.948 -483.948] [79.0832], Avg: [-461.454 -461.454 -461.454] (1.000)
Step: 191000, Reward: [-435.743 -435.743 -435.743] [96.1887], Avg: [-461.32 -461.32 -461.32] (1.000)
Step: 192000, Reward: [-480.328 -480.328 -480.328] [94.9438], Avg: [-461.419 -461.419 -461.419] (1.000)
Step: 193000, Reward: [-444.399 -444.399 -444.399] [78.8205], Avg: [-461.33 -461.33 -461.33] (1.000)
Step: 194000, Reward: [-492.938 -492.938 -492.938] [130.0955], Avg: [-461.493 -461.493 -461.493] (1.000)
Step: 195000, Reward: [-469.454 -469.454 -469.454] [79.6338], Avg: [-461.534 -461.534 -461.534] (1.000)
Step: 196000, Reward: [-485.677 -485.677 -485.677] [116.5176], Avg: [-461.657 -461.657 -461.657] (1.000)
Step: 197000, Reward: [-420.665 -420.665 -420.665] [71.5809], Avg: [-461.449 -461.449 -461.449] (1.000)
Step: 198000, Reward: [-447.892 -447.892 -447.892] [79.5844], Avg: [-461.381 -461.381 -461.381] (1.000)
Step: 199000, Reward: [-462.05 -462.05 -462.05] [53.1057], Avg: [-461.384 -461.384 -461.384] (1.000)
Step: 200000, Reward: [-435.494 -435.494 -435.494] [77.9388], Avg: [-461.255 -461.255 -461.255] (1.000)
Step: 201000, Reward: [-455.644 -455.644 -455.644] [104.5834], Avg: [-461.227 -461.227 -461.227] (1.000)
Step: 202000, Reward: [-431.958 -431.958 -431.958] [63.7356], Avg: [-461.082 -461.082 -461.082] (1.000)
Step: 203000, Reward: [-487.969 -487.969 -487.969] [105.4913], Avg: [-461.214 -461.214 -461.214] (1.000)
Step: 204000, Reward: [-483.225 -483.225 -483.225] [112.9304], Avg: [-461.322 -461.322 -461.322] (1.000)
Step: 205000, Reward: [-444.893 -444.893 -444.893] [79.5887], Avg: [-461.242 -461.242 -461.242] (1.000)
Step: 206000, Reward: [-470.243 -470.243 -470.243] [86.8898], Avg: [-461.286 -461.286 -461.286] (1.000)
Step: 207000, Reward: [-426.775 -426.775 -426.775] [94.3433], Avg: [-461.119 -461.119 -461.119] (1.000)
Step: 208000, Reward: [-465.462 -465.462 -465.462] [111.1173], Avg: [-461.14 -461.14 -461.14] (1.000)
Step: 209000, Reward: [-450.538 -450.538 -450.538] [105.2624], Avg: [-461.089 -461.089 -461.089] (1.000)
Step: 210000, Reward: [-470.302 -470.302 -470.302] [101.2820], Avg: [-461.133 -461.133 -461.133] (1.000)
Step: 211000, Reward: [-439.337 -439.337 -439.337] [83.0257], Avg: [-461.03 -461.03 -461.03] (1.000)
Step: 212000, Reward: [-461.507 -461.507 -461.507] [66.8202], Avg: [-461.032 -461.032 -461.032] (1.000)
Step: 213000, Reward: [-459.05 -459.05 -459.05] [79.0550], Avg: [-461.023 -461.023 -461.023] (1.000)
Step: 214000, Reward: [-439.602 -439.602 -439.602] [83.4235], Avg: [-460.923 -460.923 -460.923] (1.000)
Step: 215000, Reward: [-439.313 -439.313 -439.313] [64.5197], Avg: [-460.822 -460.822 -460.822] (1.000)
Step: 216000, Reward: [-445.409 -445.409 -445.409] [88.0732], Avg: [-460.751 -460.751 -460.751] (1.000)
Step: 217000, Reward: [-431.006 -431.006 -431.006] [56.9569], Avg: [-460.614 -460.614 -460.614] (1.000)
Step: 218000, Reward: [-508.585 -508.585 -508.585] [91.6648], Avg: [-460.834 -460.834 -460.834] (1.000)
Step: 219000, Reward: [-427.39 -427.39 -427.39] [73.5586], Avg: [-460.681 -460.681 -460.681] (1.000)
Step: 220000, Reward: [-458.343 -458.343 -458.343] [75.1101], Avg: [-460.67 -460.67 -460.67] (1.000)
Step: 221000, Reward: [-423.993 -423.993 -423.993] [104.1958], Avg: [-460.504 -460.504 -460.504] (1.000)
Step: 222000, Reward: [-477.228 -477.228 -477.228] [106.2915], Avg: [-460.58 -460.58 -460.58] (1.000)
Step: 223000, Reward: [-487.568 -487.568 -487.568] [127.7084], Avg: [-460.701 -460.701 -460.701] (1.000)
Step: 224000, Reward: [-464.675 -464.675 -464.675] [78.1944], Avg: [-460.719 -460.719 -460.719] (1.000)
Step: 225000, Reward: [-464.036 -464.036 -464.036] [77.6990], Avg: [-460.733 -460.733 -460.733] (1.000)
Step: 226000, Reward: [-438.869 -438.869 -438.869] [64.5157], Avg: [-460.637 -460.637 -460.637] (1.000)
Step: 227000, Reward: [-498.36 -498.36 -498.36] [116.8723], Avg: [-460.803 -460.803 -460.803] (1.000)
Step: 228000, Reward: [-451.873 -451.873 -451.873] [58.4087], Avg: [-460.764 -460.764 -460.764] (1.000)
Step: 229000, Reward: [-472.248 -472.248 -472.248] [75.6905], Avg: [-460.814 -460.814 -460.814] (1.000)
Step: 230000, Reward: [-418.869 -418.869 -418.869] [65.8049], Avg: [-460.631 -460.631 -460.631] (1.000)
Step: 231000, Reward: [-449.459 -449.459 -449.459] [100.2093], Avg: [-460.583 -460.583 -460.583] (1.000)
Step: 232000, Reward: [-491.9 -491.9 -491.9] [104.5706], Avg: [-460.718 -460.718 -460.718] (1.000)
Step: 233000, Reward: [-441.792 -441.792 -441.792] [100.8130], Avg: [-460.637 -460.637 -460.637] (1.000)
Step: 234000, Reward: [-453.354 -453.354 -453.354] [120.5505], Avg: [-460.606 -460.606 -460.606] (1.000)
Step: 235000, Reward: [-447.24 -447.24 -447.24] [62.6675], Avg: [-460.549 -460.549 -460.549] (1.000)
Step: 236000, Reward: [-487.963 -487.963 -487.963] [72.9491], Avg: [-460.665 -460.665 -460.665] (1.000)
Step: 237000, Reward: [-470.496 -470.496 -470.496] [115.3166], Avg: [-460.706 -460.706 -460.706] (1.000)
Step: 238000, Reward: [-427.647 -427.647 -427.647] [63.9232], Avg: [-460.567 -460.567 -460.567] (1.000)
Step: 239000, Reward: [-481.658 -481.658 -481.658] [92.3061], Avg: [-460.656 -460.656 -460.656] (1.000)
Step: 240000, Reward: [-443.296 -443.296 -443.296] [78.2201], Avg: [-460.583 -460.583 -460.583] (1.000)
Step: 241000, Reward: [-448.697 -448.697 -448.697] [103.3010], Avg: [-460.534 -460.534 -460.534] (1.000)
Step: 242000, Reward: [-464.833 -464.833 -464.833] [84.5011], Avg: [-460.552 -460.552 -460.552] (1.000)
Step: 243000, Reward: [-509.221 -509.221 -509.221] [71.6255], Avg: [-460.752 -460.752 -460.752] (1.000)
Step: 244000, Reward: [-443.75 -443.75 -443.75] [71.0953], Avg: [-460.682 -460.682 -460.682] (1.000)
Step: 245000, Reward: [-428.712 -428.712 -428.712] [70.7063], Avg: [-460.552 -460.552 -460.552] (1.000)
Step: 246000, Reward: [-453.126 -453.126 -453.126] [90.6743], Avg: [-460.522 -460.522 -460.522] (1.000)
Step: 247000, Reward: [-413.784 -413.784 -413.784] [64.8432], Avg: [-460.333 -460.333 -460.333] (1.000)
Step: 248000, Reward: [-437.137 -437.137 -437.137] [78.3786], Avg: [-460.239 -460.239 -460.239] (1.000)
Step: 249000, Reward: [-443.536 -443.536 -443.536] [62.0780], Avg: [-460.172 -460.172 -460.172] (1.000)
Step: 250000, Reward: [-447.9 -447.9 -447.9] [93.7304], Avg: [-460.123 -460.123 -460.123] (1.000)
Step: 251000, Reward: [-446.677 -446.677 -446.677] [89.0853], Avg: [-460.069 -460.069 -460.069] (1.000)
Step: 252000, Reward: [-417.468 -417.468 -417.468] [105.0992], Avg: [-459.9 -459.9 -459.9] (1.000)
Step: 253000, Reward: [-487.335 -487.335 -487.335] [128.0869], Avg: [-460.009 -460.009 -460.009] (1.000)
Step: 254000, Reward: [-433.408 -433.408 -433.408] [99.5230], Avg: [-459.904 -459.904 -459.904] (1.000)
Step: 255000, Reward: [-468.152 -468.152 -468.152] [109.6094], Avg: [-459.936 -459.936 -459.936] (1.000)
Step: 256000, Reward: [-478.32 -478.32 -478.32] [103.3937], Avg: [-460.008 -460.008 -460.008] (1.000)
Step: 257000, Reward: [-461.662 -461.662 -461.662] [97.1010], Avg: [-460.015 -460.015 -460.015] (1.000)
Step: 258000, Reward: [-453.002 -453.002 -453.002] [110.5481], Avg: [-459.987 -459.987 -459.987] (1.000)
Step: 259000, Reward: [-430.949 -430.949 -430.949] [57.0984], Avg: [-459.875 -459.875 -459.875] (1.000)
Step: 260000, Reward: [-453.089 -453.089 -453.089] [73.9659], Avg: [-459.849 -459.849 -459.849] (1.000)
Step: 261000, Reward: [-457.246 -457.246 -457.246] [83.9492], Avg: [-459.839 -459.839 -459.839] (1.000)
Step: 262000, Reward: [-448.713 -448.713 -448.713] [93.4182], Avg: [-459.797 -459.797 -459.797] (1.000)
Step: 263000, Reward: [-480.168 -480.168 -480.168] [101.1167], Avg: [-459.874 -459.874 -459.874] (1.000)
Step: 264000, Reward: [-488.106 -488.106 -488.106] [119.6124], Avg: [-459.981 -459.981 -459.981] (1.000)
Step: 265000, Reward: [-476.939 -476.939 -476.939] [86.5404], Avg: [-460.045 -460.045 -460.045] (1.000)
Step: 266000, Reward: [-430.688 -430.688 -430.688] [78.8909], Avg: [-459.935 -459.935 -459.935] (1.000)
Step: 267000, Reward: [-449.276 -449.276 -449.276] [95.2778], Avg: [-459.895 -459.895 -459.895] (1.000)
Step: 268000, Reward: [-442.694 -442.694 -442.694] [105.1046], Avg: [-459.831 -459.831 -459.831] (1.000)
Step: 269000, Reward: [-455.623 -455.623 -455.623] [119.4929], Avg: [-459.815 -459.815 -459.815] (1.000)
Step: 270000, Reward: [-458.195 -458.195 -458.195] [116.4448], Avg: [-459.809 -459.809 -459.809] (1.000)
Step: 271000, Reward: [-483.916 -483.916 -483.916] [68.1666], Avg: [-459.898 -459.898 -459.898] (1.000)
Step: 272000, Reward: [-480.723 -480.723 -480.723] [110.8639], Avg: [-459.974 -459.974 -459.974] (1.000)
Step: 273000, Reward: [-511.423 -511.423 -511.423] [138.6220], Avg: [-460.163 -460.163 -460.163] (1.000)
Step: 274000, Reward: [-467.086 -467.086 -467.086] [102.4348], Avg: [-460.188 -460.188 -460.188] (1.000)
Step: 275000, Reward: [-467.315 -467.315 -467.315] [99.8074], Avg: [-460.214 -460.214 -460.214] (1.000)
Step: 276000, Reward: [-443.084 -443.084 -443.084] [59.8215], Avg: [-460.152 -460.152 -460.152] (1.000)
Step: 277000, Reward: [-440.381 -440.381 -440.381] [79.4486], Avg: [-460.081 -460.081 -460.081] (1.000)
Step: 278000, Reward: [-479.48 -479.48 -479.48] [89.9750], Avg: [-460.15 -460.15 -460.15] (1.000)
Step: 279000, Reward: [-459.119 -459.119 -459.119] [71.5900], Avg: [-460.147 -460.147 -460.147] (1.000)
Step: 280000, Reward: [-472.87 -472.87 -472.87] [71.1245], Avg: [-460.192 -460.192 -460.192] (1.000)
Step: 281000, Reward: [-443.422 -443.422 -443.422] [47.5965], Avg: [-460.133 -460.133 -460.133] (1.000)
Step: 282000, Reward: [-453.358 -453.358 -453.358] [72.6032], Avg: [-460.108 -460.108 -460.108] (1.000)
Step: 283000, Reward: [-500.035 -500.035 -500.035] [110.8173], Avg: [-460.25 -460.25 -460.25] (1.000)
Step: 284000, Reward: [-476.926 -476.926 -476.926] [104.9816], Avg: [-460.308 -460.308 -460.308] (1.000)
Step: 285000, Reward: [-455.592 -455.592 -455.592] [96.9733], Avg: [-460.292 -460.292 -460.292] (1.000)
Step: 286000, Reward: [-466.577 -466.577 -466.577] [102.9700], Avg: [-460.314 -460.314 -460.314] (1.000)
Step: 287000, Reward: [-450.382 -450.382 -450.382] [72.6541], Avg: [-460.279 -460.279 -460.279] (1.000)
Step: 288000, Reward: [-467.037 -467.037 -467.037] [87.4404], Avg: [-460.303 -460.303 -460.303] (1.000)
Step: 289000, Reward: [-437.533 -437.533 -437.533] [60.6916], Avg: [-460.224 -460.224 -460.224] (1.000)
Step: 290000, Reward: [-470.891 -470.891 -470.891] [90.3383], Avg: [-460.261 -460.261 -460.261] (1.000)
Step: 291000, Reward: [-422.967 -422.967 -422.967] [45.8929], Avg: [-460.132 -460.132 -460.132] (1.000)
Step: 292000, Reward: [-452.116 -452.116 -452.116] [134.5636], Avg: [-460.105 -460.105 -460.105] (1.000)
Step: 293000, Reward: [-414.263 -414.263 -414.263] [61.3963], Avg: [-459.949 -459.949 -459.949] (1.000)
Step: 294000, Reward: [-447.446 -447.446 -447.446] [84.7939], Avg: [-459.906 -459.906 -459.906] (1.000)
Step: 295000, Reward: [-471.094 -471.094 -471.094] [98.6272], Avg: [-459.944 -459.944 -459.944] (1.000)
Step: 296000, Reward: [-445.692 -445.692 -445.692] [85.0060], Avg: [-459.896 -459.896 -459.896] (1.000)
Step: 297000, Reward: [-449.413 -449.413 -449.413] [72.4285], Avg: [-459.86 -459.86 -459.86] (1.000)
Step: 298000, Reward: [-467.5 -467.5 -467.5] [87.6169], Avg: [-459.886 -459.886 -459.886] (1.000)
Step: 299000, Reward: [-468.876 -468.876 -468.876] [126.2859], Avg: [-459.916 -459.916 -459.916] (1.000)
