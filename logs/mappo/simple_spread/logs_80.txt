Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		# self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		# out_dims = state.shape[:-1]
		# state = state.reshape(-1, state.shape[-1])
		# if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		# self.hidden = self.recurrent(state, self.hidden)
		# action_mu = self.action_mu(self.hidden)
		# action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		action_mu = self.action_mu(state)

		action_probs = action_mu.softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action = dist.sample() if action is None else action.argmax(-1)
		action_one_hot = one_hot_from_indices(action, action_probs.size(-1))
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_one_hot, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			# model.actor_local.init_hidden(state.size(0), state.device)
			# entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			# new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			# entropy = torch.stack(entropy).view(1, -1, *advantage.shape[2:])
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=100, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-741.124 -741.124 -741.124] [185.5770], Avg: [-741.124 -741.124 -741.124] (1.000)
Step: 2000, Reward: [-695.98 -695.98 -695.98] [230.7213], Avg: [-718.552 -718.552 -718.552] (1.000)
Step: 3000, Reward: [-627.229 -627.229 -627.229] [121.0675], Avg: [-688.111 -688.111 -688.111] (1.000)
Step: 4000, Reward: [-608.219 -608.219 -608.219] [137.0012], Avg: [-668.138 -668.138 -668.138] (1.000)
Step: 5000, Reward: [-581.458 -581.458 -581.458] [131.7115], Avg: [-650.802 -650.802 -650.802] (1.000)
Step: 6000, Reward: [-572.923 -572.923 -572.923] [118.2611], Avg: [-637.822 -637.822 -637.822] (1.000)
Step: 7000, Reward: [-534.608 -534.608 -534.608] [100.2113], Avg: [-623.077 -623.077 -623.077] (1.000)
Step: 8000, Reward: [-498.063 -498.063 -498.063] [112.0201], Avg: [-607.45 -607.45 -607.45] (1.000)
Step: 9000, Reward: [-527.935 -527.935 -527.935] [100.5740], Avg: [-598.615 -598.615 -598.615] (1.000)
Step: 10000, Reward: [-444.685 -444.685 -444.685] [60.6503], Avg: [-583.222 -583.222 -583.222] (1.000)
Step: 11000, Reward: [-465.183 -465.183 -465.183] [79.3131], Avg: [-572.491 -572.491 -572.491] (1.000)
Step: 12000, Reward: [-471.127 -471.127 -471.127] [112.0283], Avg: [-564.044 -564.044 -564.044] (1.000)
Step: 13000, Reward: [-480.973 -480.973 -480.973] [98.5237], Avg: [-557.654 -557.654 -557.654] (1.000)
Step: 14000, Reward: [-450.8 -450.8 -450.8] [76.2462], Avg: [-550.022 -550.022 -550.022] (1.000)
Step: 15000, Reward: [-452.295 -452.295 -452.295] [65.5633], Avg: [-543.507 -543.507 -543.507] (1.000)
Step: 16000, Reward: [-416.557 -416.557 -416.557] [48.2969], Avg: [-535.572 -535.572 -535.572] (1.000)
Step: 17000, Reward: [-427.577 -427.577 -427.577] [79.9329], Avg: [-529.22 -529.22 -529.22] (1.000)
Step: 18000, Reward: [-437.443 -437.443 -437.443] [83.7607], Avg: [-524.121 -524.121 -524.121] (1.000)
Step: 19000, Reward: [-446.856 -446.856 -446.856] [67.5365], Avg: [-520.054 -520.054 -520.054] (1.000)
Step: 20000, Reward: [-428.067 -428.067 -428.067] [62.8463], Avg: [-515.455 -515.455 -515.455] (1.000)
Step: 21000, Reward: [-445.886 -445.886 -445.886] [77.7349], Avg: [-512.142 -512.142 -512.142] (1.000)
Step: 22000, Reward: [-426.504 -426.504 -426.504] [58.0177], Avg: [-508.25 -508.25 -508.25] (1.000)
Step: 23000, Reward: [-414.167 -414.167 -414.167] [54.4539], Avg: [-504.159 -504.159 -504.159] (1.000)
Step: 24000, Reward: [-450.173 -450.173 -450.173] [57.8895], Avg: [-501.91 -501.91 -501.91] (1.000)
Step: 25000, Reward: [-441.196 -441.196 -441.196] [68.9701], Avg: [-499.481 -499.481 -499.481] (1.000)
Step: 26000, Reward: [-444.58 -444.58 -444.58] [63.5026], Avg: [-497.37 -497.37 -497.37] (1.000)
Step: 27000, Reward: [-453.272 -453.272 -453.272] [79.5679], Avg: [-495.736 -495.736 -495.736] (1.000)
Step: 28000, Reward: [-443.955 -443.955 -443.955] [68.9826], Avg: [-493.887 -493.887 -493.887] (1.000)
Step: 29000, Reward: [-413.78 -413.78 -413.78] [65.5414], Avg: [-491.125 -491.125 -491.125] (1.000)
Step: 30000, Reward: [-467.334 -467.334 -467.334] [60.6326], Avg: [-490.332 -490.332 -490.332] (1.000)
Step: 31000, Reward: [-413.323 -413.323 -413.323] [64.1113], Avg: [-487.848 -487.848 -487.848] (1.000)
Step: 32000, Reward: [-405.594 -405.594 -405.594] [54.7946], Avg: [-485.277 -485.277 -485.277] (1.000)
Step: 33000, Reward: [-411.46 -411.46 -411.46] [66.9985], Avg: [-483.04 -483.04 -483.04] (1.000)
Step: 34000, Reward: [-407.662 -407.662 -407.662] [69.6206], Avg: [-480.823 -480.823 -480.823] (1.000)
Step: 35000, Reward: [-437.473 -437.473 -437.473] [66.0891], Avg: [-479.585 -479.585 -479.585] (1.000)
Step: 36000, Reward: [-439.826 -439.826 -439.826] [59.3742], Avg: [-478.48 -478.48 -478.48] (1.000)
Step: 37000, Reward: [-426.092 -426.092 -426.092] [78.0027], Avg: [-477.064 -477.064 -477.064] (1.000)
Step: 38000, Reward: [-405.021 -405.021 -405.021] [64.8332], Avg: [-475.168 -475.168 -475.168] (1.000)
Step: 39000, Reward: [-419.914 -419.914 -419.914] [62.9689], Avg: [-473.752 -473.752 -473.752] (1.000)
Step: 40000, Reward: [-420.786 -420.786 -420.786] [92.2293], Avg: [-472.428 -472.428 -472.428] (1.000)
Step: 41000, Reward: [-430.262 -430.262 -430.262] [43.6898], Avg: [-471.399 -471.399 -471.399] (1.000)
Step: 42000, Reward: [-407.696 -407.696 -407.696] [74.8238], Avg: [-469.882 -469.882 -469.882] (1.000)
Step: 43000, Reward: [-407.188 -407.188 -407.188] [58.8203], Avg: [-468.424 -468.424 -468.424] (1.000)
Step: 44000, Reward: [-394.74 -394.74 -394.74] [64.8876], Avg: [-466.75 -466.75 -466.75] (1.000)
Step: 45000, Reward: [-425.042 -425.042 -425.042] [75.7318], Avg: [-465.823 -465.823 -465.823] (1.000)
Step: 46000, Reward: [-380.151 -380.151 -380.151] [67.2722], Avg: [-463.96 -463.96 -463.96] (1.000)
Step: 47000, Reward: [-413.644 -413.644 -413.644] [44.1342], Avg: [-462.89 -462.89 -462.89] (1.000)
Step: 48000, Reward: [-403.536 -403.536 -403.536] [54.2964], Avg: [-461.653 -461.653 -461.653] (1.000)
Step: 49000, Reward: [-418.338 -418.338 -418.338] [62.1108], Avg: [-460.769 -460.769 -460.769] (1.000)
Step: 50000, Reward: [-402.053 -402.053 -402.053] [55.7248], Avg: [-459.595 -459.595 -459.595] (1.000)
Step: 51000, Reward: [-421.235 -421.235 -421.235] [54.3613], Avg: [-458.843 -458.843 -458.843] (1.000)
Step: 52000, Reward: [-397.443 -397.443 -397.443] [52.4509], Avg: [-457.662 -457.662 -457.662] (1.000)
Step: 53000, Reward: [-424.558 -424.558 -424.558] [75.5739], Avg: [-457.037 -457.037 -457.037] (1.000)
Step: 54000, Reward: [-424.186 -424.186 -424.186] [61.2600], Avg: [-456.429 -456.429 -456.429] (1.000)
Step: 55000, Reward: [-419.025 -419.025 -419.025] [70.1277], Avg: [-455.749 -455.749 -455.749] (1.000)
Step: 56000, Reward: [-418.813 -418.813 -418.813] [93.0624], Avg: [-455.089 -455.089 -455.089] (1.000)
Step: 57000, Reward: [-401.721 -401.721 -401.721] [59.5842], Avg: [-454.153 -454.153 -454.153] (1.000)
Step: 58000, Reward: [-408.808 -408.808 -408.808] [48.1398], Avg: [-453.371 -453.371 -453.371] (1.000)
Step: 59000, Reward: [-401.811 -401.811 -401.811] [52.1139], Avg: [-452.497 -452.497 -452.497] (1.000)
Step: 60000, Reward: [-405.029 -405.029 -405.029] [64.0398], Avg: [-451.706 -451.706 -451.706] (1.000)
Step: 61000, Reward: [-417.706 -417.706 -417.706] [106.0479], Avg: [-451.149 -451.149 -451.149] (1.000)
Step: 62000, Reward: [-409.21 -409.21 -409.21] [56.0177], Avg: [-450.473 -450.473 -450.473] (1.000)
Step: 63000, Reward: [-375.83 -375.83 -375.83] [57.3621], Avg: [-449.288 -449.288 -449.288] (1.000)
Step: 64000, Reward: [-391.222 -391.222 -391.222] [59.6952], Avg: [-448.38 -448.38 -448.38] (1.000)
Step: 65000, Reward: [-439.699 -439.699 -439.699] [79.7054], Avg: [-448.247 -448.247 -448.247] (1.000)
Step: 66000, Reward: [-382.044 -382.044 -382.044] [33.0345], Avg: [-447.244 -447.244 -447.244] (1.000)
Step: 67000, Reward: [-411.101 -411.101 -411.101] [60.0990], Avg: [-446.704 -446.704 -446.704] (1.000)
Step: 68000, Reward: [-408.819 -408.819 -408.819] [58.2455], Avg: [-446.147 -446.147 -446.147] (1.000)
Step: 69000, Reward: [-410.123 -410.123 -410.123] [65.5446], Avg: [-445.625 -445.625 -445.625] (1.000)
Step: 70000, Reward: [-394.953 -394.953 -394.953] [57.1323], Avg: [-444.901 -444.901 -444.901] (1.000)
Step: 71000, Reward: [-384.427 -384.427 -384.427] [56.4988], Avg: [-444.049 -444.049 -444.049] (1.000)
Step: 72000, Reward: [-400.569 -400.569 -400.569] [59.7796], Avg: [-443.446 -443.446 -443.446] (1.000)
Step: 73000, Reward: [-424.947 -424.947 -424.947] [59.7942], Avg: [-443.192 -443.192 -443.192] (1.000)
Step: 74000, Reward: [-402.968 -402.968 -402.968] [64.9746], Avg: [-442.649 -442.649 -442.649] (1.000)
Step: 75000, Reward: [-402.645 -402.645 -402.645] [65.3058], Avg: [-442.115 -442.115 -442.115] (1.000)
Step: 76000, Reward: [-420.217 -420.217 -420.217] [77.4050], Avg: [-441.827 -441.827 -441.827] (1.000)
Step: 77000, Reward: [-404.517 -404.517 -404.517] [79.8022], Avg: [-441.343 -441.343 -441.343] (1.000)
Step: 78000, Reward: [-428.474 -428.474 -428.474] [44.6331], Avg: [-441.178 -441.178 -441.178] (1.000)
Step: 79000, Reward: [-407.311 -407.311 -407.311] [70.7884], Avg: [-440.749 -440.749 -440.749] (1.000)
Step: 80000, Reward: [-401.082 -401.082 -401.082] [54.0360], Avg: [-440.253 -440.253 -440.253] (1.000)
Step: 81000, Reward: [-395.678 -395.678 -395.678] [63.5277], Avg: [-439.703 -439.703 -439.703] (1.000)
Step: 82000, Reward: [-406.334 -406.334 -406.334] [46.9795], Avg: [-439.296 -439.296 -439.296] (1.000)
Step: 83000, Reward: [-411.679 -411.679 -411.679] [59.4799], Avg: [-438.963 -438.963 -438.963] (1.000)
Step: 84000, Reward: [-409.884 -409.884 -409.884] [64.0484], Avg: [-438.617 -438.617 -438.617] (1.000)
Step: 85000, Reward: [-445.996 -445.996 -445.996] [65.3687], Avg: [-438.704 -438.704 -438.704] (1.000)
Step: 86000, Reward: [-397.013 -397.013 -397.013] [69.8538], Avg: [-438.219 -438.219 -438.219] (1.000)
Step: 87000, Reward: [-400.974 -400.974 -400.974] [70.5298], Avg: [-437.791 -437.791 -437.791] (1.000)
Step: 88000, Reward: [-357.527 -357.527 -357.527] [41.2120], Avg: [-436.879 -436.879 -436.879] (1.000)
Step: 89000, Reward: [-397.484 -397.484 -397.484] [68.6054], Avg: [-436.436 -436.436 -436.436] (1.000)
Step: 90000, Reward: [-397.368 -397.368 -397.368] [71.2729], Avg: [-436.002 -436.002 -436.002] (1.000)
Step: 91000, Reward: [-399.566 -399.566 -399.566] [56.4230], Avg: [-435.602 -435.602 -435.602] (1.000)
Step: 92000, Reward: [-429.041 -429.041 -429.041] [61.9535], Avg: [-435.53 -435.53 -435.53] (1.000)
Step: 93000, Reward: [-406.626 -406.626 -406.626] [68.2610], Avg: [-435.22 -435.22 -435.22] (1.000)
Step: 94000, Reward: [-400.18 -400.18 -400.18] [56.3883], Avg: [-434.847 -434.847 -434.847] (1.000)
Step: 95000, Reward: [-408.301 -408.301 -408.301] [66.2309], Avg: [-434.567 -434.567 -434.567] (1.000)
Step: 96000, Reward: [-383.72 -383.72 -383.72] [65.9246], Avg: [-434.038 -434.038 -434.038] (1.000)
Step: 97000, Reward: [-394.887 -394.887 -394.887] [65.9430], Avg: [-433.634 -433.634 -433.634] (1.000)
Step: 98000, Reward: [-410.973 -410.973 -410.973] [64.8326], Avg: [-433.403 -433.403 -433.403] (1.000)
Step: 99000, Reward: [-406.142 -406.142 -406.142] [73.4313], Avg: [-433.127 -433.127 -433.127] (1.000)
Step: 100000, Reward: [-419.135 -419.135 -419.135] [63.1771], Avg: [-432.988 -432.988 -432.988] (1.000)
Step: 101000, Reward: [-409.259 -409.259 -409.259] [87.7922], Avg: [-432.753 -432.753 -432.753] (1.000)
Step: 102000, Reward: [-400.64 -400.64 -400.64] [65.8748], Avg: [-432.438 -432.438 -432.438] (1.000)
Step: 103000, Reward: [-404.4 -404.4 -404.4] [61.4854], Avg: [-432.166 -432.166 -432.166] (1.000)
Step: 104000, Reward: [-390.66 -390.66 -390.66] [63.1816], Avg: [-431.766 -431.766 -431.766] (1.000)
Step: 105000, Reward: [-404.942 -404.942 -404.942] [65.6228], Avg: [-431.511 -431.511 -431.511] (1.000)
Step: 106000, Reward: [-403.671 -403.671 -403.671] [40.3629], Avg: [-431.248 -431.248 -431.248] (1.000)
Step: 107000, Reward: [-384.165 -384.165 -384.165] [47.4166], Avg: [-430.808 -430.808 -430.808] (1.000)
Step: 108000, Reward: [-403.384 -403.384 -403.384] [61.4654], Avg: [-430.554 -430.554 -430.554] (1.000)
Step: 109000, Reward: [-423.365 -423.365 -423.365] [53.4663], Avg: [-430.488 -430.488 -430.488] (1.000)
Step: 110000, Reward: [-387.196 -387.196 -387.196] [58.9514], Avg: [-430.095 -430.095 -430.095] (1.000)
Step: 111000, Reward: [-401.489 -401.489 -401.489] [75.0912], Avg: [-429.837 -429.837 -429.837] (1.000)
Step: 112000, Reward: [-398.922 -398.922 -398.922] [49.9385], Avg: [-429.561 -429.561 -429.561] (1.000)
Step: 113000, Reward: [-389.226 -389.226 -389.226] [73.0226], Avg: [-429.204 -429.204 -429.204] (1.000)
Step: 114000, Reward: [-407.245 -407.245 -407.245] [68.9669], Avg: [-429.012 -429.012 -429.012] (1.000)
Step: 115000, Reward: [-379.503 -379.503 -379.503] [65.8660], Avg: [-428.581 -428.581 -428.581] (1.000)
Step: 116000, Reward: [-406.966 -406.966 -406.966] [56.5020], Avg: [-428.395 -428.395 -428.395] (1.000)
Step: 117000, Reward: [-393.195 -393.195 -393.195] [68.9176], Avg: [-428.094 -428.094 -428.094] (1.000)
Step: 118000, Reward: [-403.515 -403.515 -403.515] [64.2705], Avg: [-427.886 -427.886 -427.886] (1.000)
Step: 119000, Reward: [-388.493 -388.493 -388.493] [52.7638], Avg: [-427.555 -427.555 -427.555] (1.000)
Step: 120000, Reward: [-405.441 -405.441 -405.441] [72.7098], Avg: [-427.37 -427.37 -427.37] (1.000)
Step: 121000, Reward: [-418.46 -418.46 -418.46] [71.2344], Avg: [-427.297 -427.297 -427.297] (1.000)
Step: 122000, Reward: [-396.577 -396.577 -396.577] [71.4983], Avg: [-427.045 -427.045 -427.045] (1.000)
Step: 123000, Reward: [-389.126 -389.126 -389.126] [54.7374], Avg: [-426.737 -426.737 -426.737] (1.000)
Step: 124000, Reward: [-402.028 -402.028 -402.028] [60.8713], Avg: [-426.537 -426.537 -426.537] (1.000)
Step: 125000, Reward: [-402.783 -402.783 -402.783] [49.3231], Avg: [-426.347 -426.347 -426.347] (1.000)
Step: 126000, Reward: [-388.119 -388.119 -388.119] [71.1023], Avg: [-426.044 -426.044 -426.044] (1.000)
Step: 127000, Reward: [-397.312 -397.312 -397.312] [50.7288], Avg: [-425.818 -425.818 -425.818] (1.000)
Step: 128000, Reward: [-380.239 -380.239 -380.239] [37.0170], Avg: [-425.462 -425.462 -425.462] (1.000)
Step: 129000, Reward: [-380.43 -380.43 -380.43] [59.9931], Avg: [-425.112 -425.112 -425.112] (1.000)
Step: 130000, Reward: [-378.89 -378.89 -378.89] [58.9020], Avg: [-424.757 -424.757 -424.757] (1.000)
Step: 131000, Reward: [-411.762 -411.762 -411.762] [62.5165], Avg: [-424.658 -424.658 -424.658] (1.000)
Step: 132000, Reward: [-394.132 -394.132 -394.132] [39.7236], Avg: [-424.426 -424.426 -424.426] (1.000)
Step: 133000, Reward: [-392.45 -392.45 -392.45] [48.6545], Avg: [-424.186 -424.186 -424.186] (1.000)
Step: 134000, Reward: [-384.982 -384.982 -384.982] [61.4951], Avg: [-423.893 -423.893 -423.893] (1.000)
Step: 135000, Reward: [-421.082 -421.082 -421.082] [55.8275], Avg: [-423.873 -423.873 -423.873] (1.000)
Step: 136000, Reward: [-405.165 -405.165 -405.165] [66.9254], Avg: [-423.735 -423.735 -423.735] (1.000)
Step: 137000, Reward: [-418.44 -418.44 -418.44] [61.4083], Avg: [-423.696 -423.696 -423.696] (1.000)
Step: 138000, Reward: [-403.935 -403.935 -403.935] [58.6130], Avg: [-423.553 -423.553 -423.553] (1.000)
Step: 139000, Reward: [-401.474 -401.474 -401.474] [65.6570], Avg: [-423.394 -423.394 -423.394] (1.000)
Step: 140000, Reward: [-413.07 -413.07 -413.07] [57.5828], Avg: [-423.321 -423.321 -423.321] (1.000)
Step: 141000, Reward: [-367.011 -367.011 -367.011] [74.6350], Avg: [-422.921 -422.921 -422.921] (1.000)
Step: 142000, Reward: [-375.037 -375.037 -375.037] [57.7783], Avg: [-422.584 -422.584 -422.584] (1.000)
Step: 143000, Reward: [-378.537 -378.537 -378.537] [55.2365], Avg: [-422.276 -422.276 -422.276] (1.000)
Step: 144000, Reward: [-425.438 -425.438 -425.438] [46.3515], Avg: [-422.298 -422.298 -422.298] (1.000)
Step: 145000, Reward: [-399.807 -399.807 -399.807] [64.1939], Avg: [-422.143 -422.143 -422.143] (1.000)
Step: 146000, Reward: [-395.234 -395.234 -395.234] [63.0858], Avg: [-421.959 -421.959 -421.959] (1.000)
Step: 147000, Reward: [-349.654 -349.654 -349.654] [67.3440], Avg: [-421.467 -421.467 -421.467] (1.000)
Step: 148000, Reward: [-412.882 -412.882 -412.882] [62.1911], Avg: [-421.409 -421.409 -421.409] (1.000)
Step: 149000, Reward: [-386.406 -386.406 -386.406] [65.4575], Avg: [-421.174 -421.174 -421.174] (1.000)
Step: 150000, Reward: [-377.913 -377.913 -377.913] [65.7324], Avg: [-420.885 -420.885 -420.885] (1.000)
Step: 151000, Reward: [-405.215 -405.215 -405.215] [48.7321], Avg: [-420.782 -420.782 -420.782] (1.000)
Step: 152000, Reward: [-389.478 -389.478 -389.478] [59.3920], Avg: [-420.576 -420.576 -420.576] (1.000)
Step: 153000, Reward: [-385.039 -385.039 -385.039] [68.3606], Avg: [-420.343 -420.343 -420.343] (1.000)
Step: 154000, Reward: [-352.03 -352.03 -352.03] [54.6851], Avg: [-419.9 -419.9 -419.9] (1.000)
Step: 155000, Reward: [-393.107 -393.107 -393.107] [56.8591], Avg: [-419.727 -419.727 -419.727] (1.000)
Step: 156000, Reward: [-371.811 -371.811 -371.811] [34.5319], Avg: [-419.42 -419.42 -419.42] (1.000)
Step: 157000, Reward: [-375.838 -375.838 -375.838] [49.1857], Avg: [-419.142 -419.142 -419.142] (1.000)
Step: 158000, Reward: [-400.232 -400.232 -400.232] [68.0264], Avg: [-419.023 -419.023 -419.023] (1.000)
Step: 159000, Reward: [-401.446 -401.446 -401.446] [70.1460], Avg: [-418.912 -418.912 -418.912] (1.000)
Step: 160000, Reward: [-382.603 -382.603 -382.603] [67.7935], Avg: [-418.685 -418.685 -418.685] (1.000)
Step: 161000, Reward: [-398.576 -398.576 -398.576] [51.0352], Avg: [-418.56 -418.56 -418.56] (1.000)
Step: 162000, Reward: [-419.529 -419.529 -419.529] [69.2970], Avg: [-418.566 -418.566 -418.566] (1.000)
Step: 163000, Reward: [-378.31 -378.31 -378.31] [56.3384], Avg: [-418.319 -418.319 -418.319] (1.000)
Step: 164000, Reward: [-389.475 -389.475 -389.475] [53.4891], Avg: [-418.143 -418.143 -418.143] (1.000)
Step: 165000, Reward: [-369.466 -369.466 -369.466] [56.3347], Avg: [-417.848 -417.848 -417.848] (1.000)
Step: 166000, Reward: [-365.064 -365.064 -365.064] [64.8561], Avg: [-417.53 -417.53 -417.53] (1.000)
Step: 167000, Reward: [-380.705 -380.705 -380.705] [50.3572], Avg: [-417.31 -417.31 -417.31] (1.000)
Step: 168000, Reward: [-374.851 -374.851 -374.851] [79.3870], Avg: [-417.057 -417.057 -417.057] (1.000)
Step: 169000, Reward: [-385.933 -385.933 -385.933] [53.0240], Avg: [-416.873 -416.873 -416.873] (1.000)
Step: 170000, Reward: [-374.207 -374.207 -374.207] [68.1462], Avg: [-416.622 -416.622 -416.622] (1.000)
Step: 171000, Reward: [-371.672 -371.672 -371.672] [57.9841], Avg: [-416.359 -416.359 -416.359] (1.000)
Step: 172000, Reward: [-383.057 -383.057 -383.057] [52.4138], Avg: [-416.165 -416.165 -416.165] (1.000)
Step: 173000, Reward: [-383.254 -383.254 -383.254] [58.3058], Avg: [-415.975 -415.975 -415.975] (1.000)
Step: 174000, Reward: [-381.118 -381.118 -381.118] [61.1532], Avg: [-415.775 -415.775 -415.775] (1.000)
Step: 175000, Reward: [-396.352 -396.352 -396.352] [65.7697], Avg: [-415.664 -415.664 -415.664] (1.000)
Step: 176000, Reward: [-416.264 -416.264 -416.264] [60.8461], Avg: [-415.667 -415.667 -415.667] (1.000)
Step: 177000, Reward: [-374.381 -374.381 -374.381] [49.3255], Avg: [-415.434 -415.434 -415.434] (1.000)
Step: 178000, Reward: [-353.676 -353.676 -353.676] [45.2960], Avg: [-415.087 -415.087 -415.087] (1.000)
Step: 179000, Reward: [-396.859 -396.859 -396.859] [53.6034], Avg: [-414.985 -414.985 -414.985] (1.000)
Step: 180000, Reward: [-375.803 -375.803 -375.803] [59.9447], Avg: [-414.768 -414.768 -414.768] (1.000)
Step: 181000, Reward: [-382.573 -382.573 -382.573] [63.2787], Avg: [-414.59 -414.59 -414.59] (1.000)
Step: 182000, Reward: [-373.132 -373.132 -373.132] [60.4458], Avg: [-414.362 -414.362 -414.362] (1.000)
Step: 183000, Reward: [-374.398 -374.398 -374.398] [60.0289], Avg: [-414.144 -414.144 -414.144] (1.000)
Step: 184000, Reward: [-399.337 -399.337 -399.337] [63.8878], Avg: [-414.063 -414.063 -414.063] (1.000)
Step: 185000, Reward: [-366.919 -366.919 -366.919] [49.8494], Avg: [-413.808 -413.808 -413.808] (1.000)
Step: 186000, Reward: [-373.581 -373.581 -373.581] [59.1940], Avg: [-413.592 -413.592 -413.592] (1.000)
Step: 187000, Reward: [-372.332 -372.332 -372.332] [61.5149], Avg: [-413.371 -413.371 -413.371] (1.000)
Step: 188000, Reward: [-385.89 -385.89 -385.89] [57.1716], Avg: [-413.225 -413.225 -413.225] (1.000)
Step: 189000, Reward: [-366.826 -366.826 -366.826] [54.7402], Avg: [-412.98 -412.98 -412.98] (1.000)
Step: 190000, Reward: [-389.817 -389.817 -389.817] [70.7161], Avg: [-412.858 -412.858 -412.858] (1.000)
Step: 191000, Reward: [-370.449 -370.449 -370.449] [56.8603], Avg: [-412.636 -412.636 -412.636] (1.000)
Step: 192000, Reward: [-367.661 -367.661 -367.661] [76.1699], Avg: [-412.401 -412.401 -412.401] (1.000)
Step: 193000, Reward: [-365.262 -365.262 -365.262] [72.2521], Avg: [-412.157 -412.157 -412.157] (1.000)
Step: 194000, Reward: [-390.32 -390.32 -390.32] [65.9193], Avg: [-412.045 -412.045 -412.045] (1.000)
Step: 195000, Reward: [-381.525 -381.525 -381.525] [55.5372], Avg: [-411.888 -411.888 -411.888] (1.000)
Step: 196000, Reward: [-380.45 -380.45 -380.45] [69.2359], Avg: [-411.728 -411.728 -411.728] (1.000)
Step: 197000, Reward: [-353.245 -353.245 -353.245] [57.8074], Avg: [-411.431 -411.431 -411.431] (1.000)
Step: 198000, Reward: [-370.925 -370.925 -370.925] [50.1589], Avg: [-411.226 -411.226 -411.226] (1.000)
Step: 199000, Reward: [-386.65 -386.65 -386.65] [76.6122], Avg: [-411.103 -411.103 -411.103] (1.000)
