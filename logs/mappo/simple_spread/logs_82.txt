Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		# self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		# out_dims = state.shape[:-1]
		# state = state.reshape(-1, state.shape[-1])
		# if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		# self.hidden = self.recurrent(state, self.hidden)
		# action_mu = self.action_mu(self.hidden)
		# action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		action_mu = self.action_mu(state)

		action_probs = action_mu.softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action = dist.sample() if action is None else action.argmax(-1)
		action_one_hot = one_hot_from_indices(action, action_probs.size(-1))
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_one_hot, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			# model.actor_local.init_hidden(state.size(0), state.device)
			# entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			# new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			# entropy = torch.stack(entropy).view(1, -1, *advantage.shape[2:])
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-626.275 -626.275 -626.275] [174.0664], Avg: [-626.275 -626.275 -626.275] (1.000)
Step: 2000, Reward: [-566.259 -566.259 -566.259] [210.5699], Avg: [-596.267 -596.267 -596.267] (1.000)
Step: 3000, Reward: [-563.786 -563.786 -563.786] [139.6004], Avg: [-585.44 -585.44 -585.44] (1.000)
Step: 4000, Reward: [-638.961 -638.961 -638.961] [192.3198], Avg: [-598.82 -598.82 -598.82] (1.000)
Step: 5000, Reward: [-541.134 -541.134 -541.134] [91.3238], Avg: [-587.283 -587.283 -587.283] (1.000)
Step: 6000, Reward: [-499.042 -499.042 -499.042] [111.2943], Avg: [-572.576 -572.576 -572.576] (1.000)
Step: 7000, Reward: [-475.092 -475.092 -475.092] [90.7789], Avg: [-558.65 -558.65 -558.65] (1.000)
Step: 8000, Reward: [-460.947 -460.947 -460.947] [62.8352], Avg: [-546.437 -546.437 -546.437] (1.000)
Step: 9000, Reward: [-453.726 -453.726 -453.726] [80.0132], Avg: [-536.136 -536.136 -536.136] (1.000)
Step: 10000, Reward: [-429.084 -429.084 -429.084] [49.3455], Avg: [-525.43 -525.43 -525.43] (1.000)
Step: 11000, Reward: [-425.674 -425.674 -425.674] [74.8696], Avg: [-516.362 -516.362 -516.362] (1.000)
Step: 12000, Reward: [-422.735 -422.735 -422.735] [112.0046], Avg: [-508.559 -508.559 -508.559] (1.000)
Step: 13000, Reward: [-429.41 -429.41 -429.41] [80.8781], Avg: [-502.471 -502.471 -502.471] (1.000)
Step: 14000, Reward: [-414.972 -414.972 -414.972] [55.1336], Avg: [-496.221 -496.221 -496.221] (1.000)
Step: 15000, Reward: [-409.469 -409.469 -409.469] [53.6963], Avg: [-490.438 -490.438 -490.438] (1.000)
Step: 16000, Reward: [-415.332 -415.332 -415.332] [81.8172], Avg: [-485.743 -485.743 -485.743] (1.000)
Step: 17000, Reward: [-430.917 -430.917 -430.917] [81.1052], Avg: [-482.518 -482.518 -482.518] (1.000)
Step: 18000, Reward: [-420.507 -420.507 -420.507] [42.7581], Avg: [-479.073 -479.073 -479.073] (1.000)
Step: 19000, Reward: [-396.113 -396.113 -396.113] [31.1453], Avg: [-474.707 -474.707 -474.707] (1.000)
Step: 20000, Reward: [-429.058 -429.058 -429.058] [54.0818], Avg: [-472.424 -472.424 -472.424] (1.000)
Step: 21000, Reward: [-398.921 -398.921 -398.921] [62.5071], Avg: [-468.924 -468.924 -468.924] (1.000)
Step: 22000, Reward: [-377.279 -377.279 -377.279] [59.0813], Avg: [-464.759 -464.759 -464.759] (1.000)
Step: 23000, Reward: [-410.466 -410.466 -410.466] [81.3810], Avg: [-462.398 -462.398 -462.398] (1.000)
Step: 24000, Reward: [-413. -413. -413.] [66.6360], Avg: [-460.34 -460.34 -460.34] (1.000)
Step: 25000, Reward: [-373.676 -373.676 -373.676] [50.4180], Avg: [-456.873 -456.873 -456.873] (1.000)
Step: 26000, Reward: [-414.561 -414.561 -414.561] [72.3780], Avg: [-455.246 -455.246 -455.246] (1.000)
Step: 27000, Reward: [-419.718 -419.718 -419.718] [74.4469], Avg: [-453.93 -453.93 -453.93] (1.000)
Step: 28000, Reward: [-380.593 -380.593 -380.593] [56.2001], Avg: [-451.311 -451.311 -451.311] (1.000)
Step: 29000, Reward: [-403.331 -403.331 -403.331] [68.0106], Avg: [-449.656 -449.656 -449.656] (1.000)
Step: 30000, Reward: [-374.812 -374.812 -374.812] [67.3798], Avg: [-447.162 -447.162 -447.162] (1.000)
Step: 31000, Reward: [-393.114 -393.114 -393.114] [66.6290], Avg: [-445.418 -445.418 -445.418] (1.000)
Step: 32000, Reward: [-384.587 -384.587 -384.587] [56.5666], Avg: [-443.517 -443.517 -443.517] (1.000)
Step: 33000, Reward: [-361.985 -361.985 -361.985] [45.9275], Avg: [-441.046 -441.046 -441.046] (1.000)
Step: 34000, Reward: [-361.733 -361.733 -361.733] [63.8621], Avg: [-438.714 -438.714 -438.714] (1.000)
Step: 35000, Reward: [-381.269 -381.269 -381.269] [51.9856], Avg: [-437.072 -437.072 -437.072] (1.000)
Step: 36000, Reward: [-389.846 -389.846 -389.846] [46.9931], Avg: [-435.761 -435.761 -435.761] (1.000)
Step: 37000, Reward: [-392.997 -392.997 -392.997] [64.7233], Avg: [-434.605 -434.605 -434.605] (1.000)
Step: 38000, Reward: [-376.493 -376.493 -376.493] [44.8712], Avg: [-433.076 -433.076 -433.076] (1.000)
Step: 39000, Reward: [-399.481 -399.481 -399.481] [56.0824], Avg: [-432.214 -432.214 -432.214] (1.000)
Step: 40000, Reward: [-377.806 -377.806 -377.806] [54.8592], Avg: [-430.854 -430.854 -430.854] (1.000)
Step: 41000, Reward: [-369.016 -369.016 -369.016] [53.7905], Avg: [-429.346 -429.346 -429.346] (1.000)
Step: 42000, Reward: [-414.293 -414.293 -414.293] [57.6572], Avg: [-428.987 -428.987 -428.987] (1.000)
Step: 43000, Reward: [-384.588 -384.588 -384.588] [46.7592], Avg: [-427.955 -427.955 -427.955] (1.000)
Step: 44000, Reward: [-377.234 -377.234 -377.234] [73.5945], Avg: [-426.802 -426.802 -426.802] (1.000)
Step: 45000, Reward: [-371.86 -371.86 -371.86] [44.2998], Avg: [-425.581 -425.581 -425.581] (1.000)
Step: 46000, Reward: [-410.308 -410.308 -410.308] [59.4463], Avg: [-425.249 -425.249 -425.249] (1.000)
Step: 47000, Reward: [-355.799 -355.799 -355.799] [35.7070], Avg: [-423.771 -423.771 -423.771] (1.000)
Step: 48000, Reward: [-411.992 -411.992 -411.992] [69.1978], Avg: [-423.526 -423.526 -423.526] (1.000)
Step: 49000, Reward: [-437.5 -437.5 -437.5] [64.0978], Avg: [-423.811 -423.811 -423.811] (1.000)
Step: 50000, Reward: [-402.62 -402.62 -402.62] [70.3391], Avg: [-423.387 -423.387 -423.387] (1.000)
Step: 51000, Reward: [-391.28 -391.28 -391.28] [70.3373], Avg: [-422.758 -422.758 -422.758] (1.000)
Step: 52000, Reward: [-366.149 -366.149 -366.149] [58.1322], Avg: [-421.669 -421.669 -421.669] (1.000)
Step: 53000, Reward: [-425.901 -425.901 -425.901] [64.2355], Avg: [-421.749 -421.749 -421.749] (1.000)
Step: 54000, Reward: [-389.445 -389.445 -389.445] [49.7585], Avg: [-421.151 -421.151 -421.151] (1.000)
Step: 55000, Reward: [-395.916 -395.916 -395.916] [68.6665], Avg: [-420.692 -420.692 -420.692] (1.000)
Step: 56000, Reward: [-416.859 -416.859 -416.859] [45.0749], Avg: [-420.623 -420.623 -420.623] (1.000)
Step: 57000, Reward: [-425.957 -425.957 -425.957] [78.1220], Avg: [-420.717 -420.717 -420.717] (1.000)
Step: 58000, Reward: [-392.581 -392.581 -392.581] [66.0927], Avg: [-420.232 -420.232 -420.232] (1.000)
Step: 59000, Reward: [-385.529 -385.529 -385.529] [49.3285], Avg: [-419.644 -419.644 -419.644] (1.000)
Step: 60000, Reward: [-392.472 -392.472 -392.472] [71.5424], Avg: [-419.191 -419.191 -419.191] (1.000)
Step: 61000, Reward: [-432.803 -432.803 -432.803] [58.9933], Avg: [-419.414 -419.414 -419.414] (1.000)
Step: 62000, Reward: [-397.579 -397.579 -397.579] [56.0772], Avg: [-419.062 -419.062 -419.062] (1.000)
Step: 63000, Reward: [-393.981 -393.981 -393.981] [66.7439], Avg: [-418.664 -418.664 -418.664] (1.000)
Step: 64000, Reward: [-407.48 -407.48 -407.48] [57.7006], Avg: [-418.489 -418.489 -418.489] (1.000)
Step: 65000, Reward: [-433.289 -433.289 -433.289] [73.7583], Avg: [-418.717 -418.717 -418.717] (1.000)
Step: 66000, Reward: [-413.486 -413.486 -413.486] [52.7696], Avg: [-418.637 -418.637 -418.637] (1.000)
Step: 67000, Reward: [-418.941 -418.941 -418.941] [69.0145], Avg: [-418.642 -418.642 -418.642] (1.000)
Step: 68000, Reward: [-441.825 -441.825 -441.825] [60.2045], Avg: [-418.983 -418.983 -418.983] (1.000)
Step: 69000, Reward: [-379.793 -379.793 -379.793] [50.9041], Avg: [-418.415 -418.415 -418.415] (1.000)
Step: 70000, Reward: [-405.218 -405.218 -405.218] [76.2067], Avg: [-418.226 -418.226 -418.226] (1.000)
Step: 71000, Reward: [-370.392 -370.392 -370.392] [65.3158], Avg: [-417.553 -417.553 -417.553] (1.000)
Step: 72000, Reward: [-413.389 -413.389 -413.389] [45.3535], Avg: [-417.495 -417.495 -417.495] (1.000)
Step: 73000, Reward: [-411.357 -411.357 -411.357] [65.4135], Avg: [-417.411 -417.411 -417.411] (1.000)
Step: 74000, Reward: [-391.388 -391.388 -391.388] [61.8368], Avg: [-417.059 -417.059 -417.059] (1.000)
Step: 75000, Reward: [-400.696 -400.696 -400.696] [62.9182], Avg: [-416.841 -416.841 -416.841] (1.000)
Step: 76000, Reward: [-411.7 -411.7 -411.7] [50.3959], Avg: [-416.773 -416.773 -416.773] (1.000)
Step: 77000, Reward: [-401.716 -401.716 -401.716] [58.2650], Avg: [-416.578 -416.578 -416.578] (1.000)
Step: 78000, Reward: [-405.029 -405.029 -405.029] [54.1909], Avg: [-416.43 -416.43 -416.43] (1.000)
Step: 79000, Reward: [-406.391 -406.391 -406.391] [64.6888], Avg: [-416.303 -416.303 -416.303] (1.000)
Step: 80000, Reward: [-390.796 -390.796 -390.796] [42.7121], Avg: [-415.984 -415.984 -415.984] (1.000)
Step: 81000, Reward: [-418.955 -418.955 -418.955] [59.6716], Avg: [-416.02 -416.02 -416.02] (1.000)
Step: 82000, Reward: [-398.261 -398.261 -398.261] [65.8112], Avg: [-415.804 -415.804 -415.804] (1.000)
Step: 83000, Reward: [-423.117 -423.117 -423.117] [65.0287], Avg: [-415.892 -415.892 -415.892] (1.000)
Step: 84000, Reward: [-407.002 -407.002 -407.002] [63.1590], Avg: [-415.786 -415.786 -415.786] (1.000)
Step: 85000, Reward: [-400.14 -400.14 -400.14] [57.8789], Avg: [-415.602 -415.602 -415.602] (1.000)
Step: 86000, Reward: [-438.065 -438.065 -438.065] [86.4845], Avg: [-415.863 -415.863 -415.863] (1.000)
Step: 87000, Reward: [-402.248 -402.248 -402.248] [75.3845], Avg: [-415.707 -415.707 -415.707] (1.000)
Step: 88000, Reward: [-409.761 -409.761 -409.761] [50.9110], Avg: [-415.639 -415.639 -415.639] (1.000)
Step: 89000, Reward: [-411.611 -411.611 -411.611] [52.3700], Avg: [-415.594 -415.594 -415.594] (1.000)
Step: 90000, Reward: [-408.142 -408.142 -408.142] [73.9428], Avg: [-415.511 -415.511 -415.511] (1.000)
Step: 91000, Reward: [-376.66 -376.66 -376.66] [45.9690], Avg: [-415.084 -415.084 -415.084] (1.000)
Step: 92000, Reward: [-405.441 -405.441 -405.441] [83.5492], Avg: [-414.979 -414.979 -414.979] (1.000)
Step: 93000, Reward: [-390.514 -390.514 -390.514] [80.8342], Avg: [-414.716 -414.716 -414.716] (1.000)
Step: 94000, Reward: [-423.197 -423.197 -423.197] [59.9193], Avg: [-414.807 -414.807 -414.807] (1.000)
Step: 95000, Reward: [-406.391 -406.391 -406.391] [55.7542], Avg: [-414.718 -414.718 -414.718] (1.000)
Step: 96000, Reward: [-430.966 -430.966 -430.966] [50.3650], Avg: [-414.887 -414.887 -414.887] (1.000)
Step: 97000, Reward: [-411.184 -411.184 -411.184] [55.9572], Avg: [-414.849 -414.849 -414.849] (1.000)
Step: 98000, Reward: [-376.864 -376.864 -376.864] [52.0151], Avg: [-414.461 -414.461 -414.461] (1.000)
Step: 99000, Reward: [-397.798 -397.798 -397.798] [45.4261], Avg: [-414.293 -414.293 -414.293] (1.000)
Step: 100000, Reward: [-404.24 -404.24 -404.24] [78.3090], Avg: [-414.193 -414.193 -414.193] (1.000)
Step: 101000, Reward: [-384.879 -384.879 -384.879] [69.2267], Avg: [-413.902 -413.902 -413.902] (1.000)
Step: 102000, Reward: [-389.821 -389.821 -389.821] [60.3056], Avg: [-413.666 -413.666 -413.666] (1.000)
Step: 103000, Reward: [-410.702 -410.702 -410.702] [54.4899], Avg: [-413.637 -413.637 -413.637] (1.000)
Step: 104000, Reward: [-414.91 -414.91 -414.91] [80.4047], Avg: [-413.65 -413.65 -413.65] (1.000)
Step: 105000, Reward: [-390.506 -390.506 -390.506] [55.9645], Avg: [-413.429 -413.429 -413.429] (1.000)
Step: 106000, Reward: [-393.413 -393.413 -393.413] [74.7302], Avg: [-413.24 -413.24 -413.24] (1.000)
Step: 107000, Reward: [-355.866 -355.866 -355.866] [53.5914], Avg: [-412.704 -412.704 -412.704] (1.000)
Step: 108000, Reward: [-393.437 -393.437 -393.437] [40.9722], Avg: [-412.526 -412.526 -412.526] (1.000)
Step: 109000, Reward: [-416.172 -416.172 -416.172] [53.9662], Avg: [-412.559 -412.559 -412.559] (1.000)
Step: 110000, Reward: [-403.174 -403.174 -403.174] [57.2975], Avg: [-412.474 -412.474 -412.474] (1.000)
Step: 111000, Reward: [-396.32 -396.32 -396.32] [72.2874], Avg: [-412.328 -412.328 -412.328] (1.000)
Step: 112000, Reward: [-390.732 -390.732 -390.732] [67.3558], Avg: [-412.136 -412.136 -412.136] (1.000)
Step: 113000, Reward: [-398.836 -398.836 -398.836] [64.6476], Avg: [-412.018 -412.018 -412.018] (1.000)
Step: 114000, Reward: [-376.891 -376.891 -376.891] [66.5823], Avg: [-411.71 -411.71 -411.71] (1.000)
Step: 115000, Reward: [-403.762 -403.762 -403.762] [63.0594], Avg: [-411.641 -411.641 -411.641] (1.000)
Step: 116000, Reward: [-399.654 -399.654 -399.654] [70.2056], Avg: [-411.537 -411.537 -411.537] (1.000)
Step: 117000, Reward: [-366.888 -366.888 -366.888] [42.4476], Avg: [-411.156 -411.156 -411.156] (1.000)
Step: 118000, Reward: [-363.72 -363.72 -363.72] [57.7091], Avg: [-410.754 -410.754 -410.754] (1.000)
Step: 119000, Reward: [-384.233 -384.233 -384.233] [62.8173], Avg: [-410.531 -410.531 -410.531] (1.000)
Step: 120000, Reward: [-412.416 -412.416 -412.416] [75.7786], Avg: [-410.547 -410.547 -410.547] (1.000)
Step: 121000, Reward: [-385.832 -385.832 -385.832] [79.8501], Avg: [-410.342 -410.342 -410.342] (1.000)
Step: 122000, Reward: [-410.854 -410.854 -410.854] [56.1350], Avg: [-410.347 -410.347 -410.347] (1.000)
Step: 123000, Reward: [-374.559 -374.559 -374.559] [75.3889], Avg: [-410.056 -410.056 -410.056] (1.000)
Step: 124000, Reward: [-376.749 -376.749 -376.749] [68.4287], Avg: [-409.787 -409.787 -409.787] (1.000)
Step: 125000, Reward: [-380.045 -380.045 -380.045] [46.0057], Avg: [-409.549 -409.549 -409.549] (1.000)
Step: 126000, Reward: [-378.085 -378.085 -378.085] [47.0236], Avg: [-409.299 -409.299 -409.299] (1.000)
Step: 127000, Reward: [-398.497 -398.497 -398.497] [76.4834], Avg: [-409.214 -409.214 -409.214] (1.000)
Step: 128000, Reward: [-392.797 -392.797 -392.797] [49.8328], Avg: [-409.086 -409.086 -409.086] (1.000)
Step: 129000, Reward: [-376.833 -376.833 -376.833] [66.0796], Avg: [-408.836 -408.836 -408.836] (1.000)
Step: 130000, Reward: [-391.062 -391.062 -391.062] [64.4847], Avg: [-408.699 -408.699 -408.699] (1.000)
Step: 131000, Reward: [-364.077 -364.077 -364.077] [38.4848], Avg: [-408.359 -408.359 -408.359] (1.000)
Step: 132000, Reward: [-383.838 -383.838 -383.838] [57.9509], Avg: [-408.173 -408.173 -408.173] (1.000)
Step: 133000, Reward: [-389.419 -389.419 -389.419] [67.1585], Avg: [-408.032 -408.032 -408.032] (1.000)
Step: 134000, Reward: [-378.606 -378.606 -378.606] [62.9410], Avg: [-407.812 -407.812 -407.812] (1.000)
Step: 135000, Reward: [-390.915 -390.915 -390.915] [69.4255], Avg: [-407.687 -407.687 -407.687] (1.000)
Step: 136000, Reward: [-355.912 -355.912 -355.912] [61.7900], Avg: [-407.306 -407.306 -407.306] (1.000)
Step: 137000, Reward: [-356.557 -356.557 -356.557] [46.6082], Avg: [-406.936 -406.936 -406.936] (1.000)
Step: 138000, Reward: [-371.155 -371.155 -371.155] [61.3186], Avg: [-406.677 -406.677 -406.677] (1.000)
Step: 139000, Reward: [-381.933 -381.933 -381.933] [56.5961], Avg: [-406.499 -406.499 -406.499] (1.000)
Step: 140000, Reward: [-369.057 -369.057 -369.057] [64.3584], Avg: [-406.231 -406.231 -406.231] (1.000)
Step: 141000, Reward: [-425.89 -425.89 -425.89] [67.2766], Avg: [-406.371 -406.371 -406.371] (1.000)
Step: 142000, Reward: [-410.701 -410.701 -410.701] [68.2904], Avg: [-406.401 -406.401 -406.401] (1.000)
Step: 143000, Reward: [-395.859 -395.859 -395.859] [61.1563], Avg: [-406.327 -406.327 -406.327] (1.000)
Step: 144000, Reward: [-398.732 -398.732 -398.732] [68.8252], Avg: [-406.275 -406.275 -406.275] (1.000)
Step: 145000, Reward: [-402.809 -402.809 -402.809] [72.8695], Avg: [-406.251 -406.251 -406.251] (1.000)
Step: 146000, Reward: [-385.417 -385.417 -385.417] [59.1228], Avg: [-406.108 -406.108 -406.108] (1.000)
Step: 147000, Reward: [-390.521 -390.521 -390.521] [66.3812], Avg: [-406.002 -406.002 -406.002] (1.000)
Step: 148000, Reward: [-413.21 -413.21 -413.21] [61.6962], Avg: [-406.051 -406.051 -406.051] (1.000)
Step: 149000, Reward: [-402.521 -402.521 -402.521] [62.2438], Avg: [-406.027 -406.027 -406.027] (1.000)
Step: 150000, Reward: [-398.833 -398.833 -398.833] [66.2672], Avg: [-405.979 -405.979 -405.979] (1.000)
Step: 151000, Reward: [-384.666 -384.666 -384.666] [67.7817], Avg: [-405.838 -405.838 -405.838] (1.000)
Step: 152000, Reward: [-385.201 -385.201 -385.201] [39.3811], Avg: [-405.702 -405.702 -405.702] (1.000)
Step: 153000, Reward: [-426.657 -426.657 -426.657] [67.6695], Avg: [-405.839 -405.839 -405.839] (1.000)
Step: 154000, Reward: [-398.324 -398.324 -398.324] [68.0625], Avg: [-405.79 -405.79 -405.79] (1.000)
Step: 155000, Reward: [-388.289 -388.289 -388.289] [62.2894], Avg: [-405.677 -405.677 -405.677] (1.000)
Step: 156000, Reward: [-342.502 -342.502 -342.502] [53.1492], Avg: [-405.272 -405.272 -405.272] (1.000)
Step: 157000, Reward: [-399.516 -399.516 -399.516] [68.9557], Avg: [-405.236 -405.236 -405.236] (1.000)
Step: 158000, Reward: [-400.991 -400.991 -400.991] [61.4114], Avg: [-405.209 -405.209 -405.209] (1.000)
Step: 159000, Reward: [-362.971 -362.971 -362.971] [52.1385], Avg: [-404.943 -404.943 -404.943] (1.000)
Step: 160000, Reward: [-388.974 -388.974 -388.974] [42.4630], Avg: [-404.843 -404.843 -404.843] (1.000)
Step: 161000, Reward: [-378.971 -378.971 -378.971] [52.2414], Avg: [-404.683 -404.683 -404.683] (1.000)
Step: 162000, Reward: [-373.96 -373.96 -373.96] [69.6186], Avg: [-404.493 -404.493 -404.493] (1.000)
Step: 163000, Reward: [-409.247 -409.247 -409.247] [87.1821], Avg: [-404.522 -404.522 -404.522] (1.000)
Step: 164000, Reward: [-379.198 -379.198 -379.198] [49.8967], Avg: [-404.368 -404.368 -404.368] (1.000)
Step: 165000, Reward: [-392.953 -392.953 -392.953] [60.6267], Avg: [-404.299 -404.299 -404.299] (1.000)
Step: 166000, Reward: [-416.912 -416.912 -416.912] [54.1402], Avg: [-404.375 -404.375 -404.375] (1.000)
Step: 167000, Reward: [-396.589 -396.589 -396.589] [64.3798], Avg: [-404.328 -404.328 -404.328] (1.000)
Step: 168000, Reward: [-408.018 -408.018 -408.018] [76.9861], Avg: [-404.35 -404.35 -404.35] (1.000)
Step: 169000, Reward: [-384.203 -384.203 -384.203] [61.6667], Avg: [-404.231 -404.231 -404.231] (1.000)
Step: 170000, Reward: [-371.756 -371.756 -371.756] [48.4278], Avg: [-404.04 -404.04 -404.04] (1.000)
Step: 171000, Reward: [-412.595 -412.595 -412.595] [50.9115], Avg: [-404.09 -404.09 -404.09] (1.000)
Step: 172000, Reward: [-392.212 -392.212 -392.212] [62.1363], Avg: [-404.021 -404.021 -404.021] (1.000)
Step: 173000, Reward: [-393.32 -393.32 -393.32] [71.3108], Avg: [-403.959 -403.959 -403.959] (1.000)
Step: 174000, Reward: [-397.167 -397.167 -397.167] [75.9203], Avg: [-403.92 -403.92 -403.92] (1.000)
Step: 175000, Reward: [-340.459 -340.459 -340.459] [59.3751], Avg: [-403.557 -403.557 -403.557] (1.000)
Step: 176000, Reward: [-396.77 -396.77 -396.77] [53.4603], Avg: [-403.519 -403.519 -403.519] (1.000)
Step: 177000, Reward: [-396.852 -396.852 -396.852] [87.6607], Avg: [-403.481 -403.481 -403.481] (1.000)
Step: 178000, Reward: [-366.059 -366.059 -366.059] [71.0495], Avg: [-403.271 -403.271 -403.271] (1.000)
Step: 179000, Reward: [-391.802 -391.802 -391.802] [61.5513], Avg: [-403.207 -403.207 -403.207] (1.000)
Step: 180000, Reward: [-365.573 -365.573 -365.573] [55.3212], Avg: [-402.998 -402.998 -402.998] (1.000)
Step: 181000, Reward: [-405.486 -405.486 -405.486] [77.5437], Avg: [-403.011 -403.011 -403.011] (1.000)
Step: 182000, Reward: [-366.475 -366.475 -366.475] [50.2565], Avg: [-402.811 -402.811 -402.811] (1.000)
Step: 183000, Reward: [-400.656 -400.656 -400.656] [67.4893], Avg: [-402.799 -402.799 -402.799] (1.000)
Step: 184000, Reward: [-382.01 -382.01 -382.01] [60.1051], Avg: [-402.686 -402.686 -402.686] (1.000)
Step: 185000, Reward: [-388.311 -388.311 -388.311] [56.8185], Avg: [-402.608 -402.608 -402.608] (1.000)
Step: 186000, Reward: [-376.339 -376.339 -376.339] [67.0984], Avg: [-402.467 -402.467 -402.467] (1.000)
Step: 187000, Reward: [-384.776 -384.776 -384.776] [45.5328], Avg: [-402.372 -402.372 -402.372] (1.000)
Step: 188000, Reward: [-390.497 -390.497 -390.497] [77.0681], Avg: [-402.309 -402.309 -402.309] (1.000)
Step: 189000, Reward: [-391.343 -391.343 -391.343] [59.8614], Avg: [-402.251 -402.251 -402.251] (1.000)
Step: 190000, Reward: [-348.922 -348.922 -348.922] [36.7432], Avg: [-401.97 -401.97 -401.97] (1.000)
Step: 191000, Reward: [-399.809 -399.809 -399.809] [77.9776], Avg: [-401.959 -401.959 -401.959] (1.000)
Step: 192000, Reward: [-358.128 -358.128 -358.128] [54.7862], Avg: [-401.731 -401.731 -401.731] (1.000)
Step: 193000, Reward: [-423.461 -423.461 -423.461] [58.0570], Avg: [-401.843 -401.843 -401.843] (1.000)
Step: 194000, Reward: [-365.633 -365.633 -365.633] [68.7742], Avg: [-401.657 -401.657 -401.657] (1.000)
Step: 195000, Reward: [-364.34 -364.34 -364.34] [53.0303], Avg: [-401.465 -401.465 -401.465] (1.000)
Step: 196000, Reward: [-351.51 -351.51 -351.51] [64.8511], Avg: [-401.211 -401.211 -401.211] (1.000)
Step: 197000, Reward: [-390.738 -390.738 -390.738] [57.5356], Avg: [-401.157 -401.157 -401.157] (1.000)
Step: 198000, Reward: [-404.218 -404.218 -404.218] [60.5218], Avg: [-401.173 -401.173 -401.173] (1.000)
Step: 199000, Reward: [-370.539 -370.539 -370.539] [56.7526], Avg: [-401.019 -401.019 -401.019] (1.000)
