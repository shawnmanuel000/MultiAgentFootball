Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		# self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		# out_dims = state.shape[:-1]
		# state = state.reshape(-1, state.shape[-1])
		# if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		# self.hidden = self.recurrent(state, self.hidden)
		# action_mu = self.action_mu(self.hidden).tanh()
		# action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)

		# action_probs = action_mu.softmax(-1)
		# dist = torch.distributions.Categorical(action_probs)
		
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			# model.actor_local.init_hidden(state.size(0), state.device)
			# entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			# new_log_prob = torch.stack(new_log_prob, dim=1)
			# entropy = torch.stack(entropy).mean()
			entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-788.597 -788.597 -788.597] [148.9069], Avg: [-788.597 -788.597 -788.597] (1.000)
Step: 2000, Reward: [-906.639 -906.639 -906.639] [213.0274], Avg: [-847.618 -847.618 -847.618] (1.000)
Step: 3000, Reward: [-746.372 -746.372 -746.372] [235.8619], Avg: [-813.869 -813.869 -813.869] (1.000)
Step: 4000, Reward: [-720.543 -720.543 -720.543] [200.3641], Avg: [-790.538 -790.538 -790.538] (1.000)
Step: 5000, Reward: [-770.218 -770.218 -770.218] [233.7324], Avg: [-786.474 -786.474 -786.474] (1.000)
Step: 6000, Reward: [-704.765 -704.765 -704.765] [232.1887], Avg: [-772.856 -772.856 -772.856] (1.000)
Step: 7000, Reward: [-791.693 -791.693 -791.693] [276.5125], Avg: [-775.547 -775.547 -775.547] (1.000)
Step: 8000, Reward: [-933.055 -933.055 -933.055] [176.9407], Avg: [-795.235 -795.235 -795.235] (1.000)
Step: 9000, Reward: [-764.355 -764.355 -764.355] [203.7402], Avg: [-791.804 -791.804 -791.804] (1.000)
Step: 10000, Reward: [-773.092 -773.092 -773.092] [170.1549], Avg: [-789.933 -789.933 -789.933] (1.000)
Step: 11000, Reward: [-724.57 -724.57 -724.57] [172.7203], Avg: [-783.991 -783.991 -783.991] (1.000)
Step: 12000, Reward: [-732.482 -732.482 -732.482] [242.8094], Avg: [-779.698 -779.698 -779.698] (1.000)
Step: 13000, Reward: [-734.659 -734.659 -734.659] [212.6896], Avg: [-776.234 -776.234 -776.234] (1.000)
Step: 14000, Reward: [-819.355 -819.355 -819.355] [184.4913], Avg: [-779.314 -779.314 -779.314] (1.000)
Step: 15000, Reward: [-707.981 -707.981 -707.981] [197.6683], Avg: [-774.558 -774.558 -774.558] (1.000)
Step: 16000, Reward: [-815.478 -815.478 -815.478] [238.4746], Avg: [-777.116 -777.116 -777.116] (1.000)
Step: 17000, Reward: [-704.95 -704.95 -704.95] [142.4897], Avg: [-772.871 -772.871 -772.871] (1.000)
Step: 18000, Reward: [-830.368 -830.368 -830.368] [207.0650], Avg: [-776.065 -776.065 -776.065] (1.000)
Step: 19000, Reward: [-776.406 -776.406 -776.406] [276.1639], Avg: [-776.083 -776.083 -776.083] (1.000)
Step: 20000, Reward: [-770.607 -770.607 -770.607] [171.9882], Avg: [-775.809 -775.809 -775.809] (1.000)
Step: 21000, Reward: [-720.753 -720.753 -720.753] [196.4842], Avg: [-773.187 -773.187 -773.187] (1.000)
Step: 22000, Reward: [-683.499 -683.499 -683.499] [259.1363], Avg: [-769.111 -769.111 -769.111] (1.000)
Step: 23000, Reward: [-661.449 -661.449 -661.449] [201.2530], Avg: [-764.43 -764.43 -764.43] (1.000)
Step: 24000, Reward: [-582.356 -582.356 -582.356] [129.1151], Avg: [-756.843 -756.843 -756.843] (1.000)
Step: 25000, Reward: [-655.731 -655.731 -655.731] [223.4292], Avg: [-752.799 -752.799 -752.799] (1.000)
Step: 26000, Reward: [-692.798 -692.798 -692.798] [261.7431], Avg: [-750.491 -750.491 -750.491] (1.000)
Step: 27000, Reward: [-658.559 -658.559 -658.559] [216.2694], Avg: [-747.086 -747.086 -747.086] (1.000)
Step: 28000, Reward: [-674.09 -674.09 -674.09] [217.4346], Avg: [-744.479 -744.479 -744.479] (1.000)
Step: 29000, Reward: [-724.935 -724.935 -724.935] [171.1781], Avg: [-743.805 -743.805 -743.805] (1.000)
Step: 30000, Reward: [-602.328 -602.328 -602.328] [164.3911], Avg: [-739.089 -739.089 -739.089] (1.000)
Step: 31000, Reward: [-645.345 -645.345 -645.345] [165.0853], Avg: [-736.065 -736.065 -736.065] (1.000)
Step: 32000, Reward: [-589.793 -589.793 -589.793] [157.1743], Avg: [-731.494 -731.494 -731.494] (1.000)
Step: 33000, Reward: [-613.9 -613.9 -613.9] [218.8122], Avg: [-727.931 -727.931 -727.931] (1.000)
Step: 34000, Reward: [-644.341 -644.341 -644.341] [205.4970], Avg: [-725.472 -725.472 -725.472] (1.000)
Step: 35000, Reward: [-557.05 -557.05 -557.05] [147.0454], Avg: [-720.66 -720.66 -720.66] (1.000)
Step: 36000, Reward: [-566.53 -566.53 -566.53] [145.9226], Avg: [-716.379 -716.379 -716.379] (1.000)
Step: 37000, Reward: [-552.761 -552.761 -552.761] [105.8228], Avg: [-711.957 -711.957 -711.957] (1.000)
Step: 38000, Reward: [-586.322 -586.322 -586.322] [207.8170], Avg: [-708.651 -708.651 -708.651] (1.000)
Step: 39000, Reward: [-542.828 -542.828 -542.828] [237.2389], Avg: [-704.399 -704.399 -704.399] (1.000)
Step: 40000, Reward: [-551.435 -551.435 -551.435] [100.7561], Avg: [-700.575 -700.575 -700.575] (1.000)
Step: 41000, Reward: [-548.407 -548.407 -548.407] [164.8184], Avg: [-696.863 -696.863 -696.863] (1.000)
Step: 42000, Reward: [-503.435 -503.435 -503.435] [101.6199], Avg: [-692.258 -692.258 -692.258] (1.000)
Step: 43000, Reward: [-546.645 -546.645 -546.645] [138.5610], Avg: [-688.872 -688.872 -688.872] (1.000)
Step: 44000, Reward: [-594.418 -594.418 -594.418] [193.0647], Avg: [-686.725 -686.725 -686.725] (1.000)
Step: 45000, Reward: [-543.47 -543.47 -543.47] [91.7662], Avg: [-683.541 -683.541 -683.541] (1.000)
Step: 46000, Reward: [-538.798 -538.798 -538.798] [117.4172], Avg: [-680.395 -680.395 -680.395] (1.000)
Step: 47000, Reward: [-475.097 -475.097 -475.097] [55.4181], Avg: [-676.027 -676.027 -676.027] (1.000)
Step: 48000, Reward: [-531.561 -531.561 -531.561] [84.9697], Avg: [-673.017 -673.017 -673.017] (1.000)
Step: 49000, Reward: [-533.698 -533.698 -533.698] [99.1999], Avg: [-670.174 -670.174 -670.174] (1.000)
Step: 50000, Reward: [-464.933 -464.933 -464.933] [77.8612], Avg: [-666.069 -666.069 -666.069] (1.000)
Step: 51000, Reward: [-528.371 -528.371 -528.371] [173.9957], Avg: [-663.369 -663.369 -663.369] (1.000)
Step: 52000, Reward: [-519.495 -519.495 -519.495] [116.5863], Avg: [-660.602 -660.602 -660.602] (1.000)
Step: 53000, Reward: [-543.351 -543.351 -543.351] [178.8453], Avg: [-658.39 -658.39 -658.39] (1.000)
Step: 54000, Reward: [-510.168 -510.168 -510.168] [100.6990], Avg: [-655.645 -655.645 -655.645] (1.000)
Step: 55000, Reward: [-490.225 -490.225 -490.225] [115.0367], Avg: [-652.637 -652.637 -652.637] (1.000)
Step: 56000, Reward: [-442.995 -442.995 -442.995] [119.7076], Avg: [-648.894 -648.894 -648.894] (1.000)
Step: 57000, Reward: [-439.17 -439.17 -439.17] [48.4071], Avg: [-645.215 -645.215 -645.215] (1.000)
Step: 58000, Reward: [-500.047 -500.047 -500.047] [144.0534], Avg: [-642.712 -642.712 -642.712] (1.000)
Step: 59000, Reward: [-424.688 -424.688 -424.688] [69.8418], Avg: [-639.016 -639.016 -639.016] (1.000)
Step: 60000, Reward: [-494.211 -494.211 -494.211] [200.2476], Avg: [-636.603 -636.603 -636.603] (1.000)
Step: 61000, Reward: [-461.636 -461.636 -461.636] [134.4763], Avg: [-633.735 -633.735 -633.735] (1.000)
Step: 62000, Reward: [-452.592 -452.592 -452.592] [72.8642], Avg: [-630.813 -630.813 -630.813] (1.000)
Step: 63000, Reward: [-480.809 -480.809 -480.809] [106.5559], Avg: [-628.432 -628.432 -628.432] (1.000)
Step: 64000, Reward: [-463.476 -463.476 -463.476] [65.9322], Avg: [-625.854 -625.854 -625.854] (1.000)
Step: 65000, Reward: [-434.33 -434.33 -434.33] [62.1984], Avg: [-622.908 -622.908 -622.908] (1.000)
Step: 66000, Reward: [-512.355 -512.355 -512.355] [123.4546], Avg: [-621.233 -621.233 -621.233] (1.000)
Step: 67000, Reward: [-430.381 -430.381 -430.381] [88.9990], Avg: [-618.384 -618.384 -618.384] (1.000)
Step: 68000, Reward: [-433.894 -433.894 -433.894] [51.0392], Avg: [-615.671 -615.671 -615.671] (1.000)
Step: 69000, Reward: [-442.435 -442.435 -442.435] [78.2306], Avg: [-613.161 -613.161 -613.161] (1.000)
Step: 70000, Reward: [-427.048 -427.048 -427.048] [66.4752], Avg: [-610.502 -610.502 -610.502] (1.000)
Step: 71000, Reward: [-415.365 -415.365 -415.365] [54.6030], Avg: [-607.753 -607.753 -607.753] (1.000)
Step: 72000, Reward: [-487.422 -487.422 -487.422] [102.7945], Avg: [-606.082 -606.082 -606.082] (1.000)
Step: 73000, Reward: [-447.934 -447.934 -447.934] [70.7894], Avg: [-603.916 -603.916 -603.916] (1.000)
Step: 74000, Reward: [-452.22 -452.22 -452.22] [105.0159], Avg: [-601.866 -601.866 -601.866] (1.000)
Step: 75000, Reward: [-444.585 -444.585 -444.585] [114.5654], Avg: [-599.769 -599.769 -599.769] (1.000)
Step: 76000, Reward: [-458.868 -458.868 -458.868] [106.9083], Avg: [-597.915 -597.915 -597.915] (1.000)
Step: 77000, Reward: [-441.127 -441.127 -441.127] [92.1685], Avg: [-595.879 -595.879 -595.879] (1.000)
Step: 78000, Reward: [-465.844 -465.844 -465.844] [108.4448], Avg: [-594.211 -594.211 -594.211] (1.000)
Step: 79000, Reward: [-415.803 -415.803 -415.803] [77.3565], Avg: [-591.953 -591.953 -591.953] (1.000)
Step: 80000, Reward: [-437.324 -437.324 -437.324] [89.6255], Avg: [-590.02 -590.02 -590.02] (1.000)
Step: 81000, Reward: [-426.735 -426.735 -426.735] [70.7416], Avg: [-588.004 -588.004 -588.004] (1.000)
Step: 82000, Reward: [-504.195 -504.195 -504.195] [190.2469], Avg: [-586.982 -586.982 -586.982] (1.000)
Step: 83000, Reward: [-424.258 -424.258 -424.258] [63.0206], Avg: [-585.022 -585.022 -585.022] (1.000)
Step: 84000, Reward: [-406.842 -406.842 -406.842] [53.3670], Avg: [-582.901 -582.901 -582.901] (1.000)
Step: 85000, Reward: [-396.933 -396.933 -396.933] [67.3417], Avg: [-580.713 -580.713 -580.713] (1.000)
Step: 86000, Reward: [-436.85 -436.85 -436.85] [81.4458], Avg: [-579.04 -579.04 -579.04] (1.000)
Step: 87000, Reward: [-415.374 -415.374 -415.374] [53.3983], Avg: [-577.159 -577.159 -577.159] (1.000)
Step: 88000, Reward: [-425.751 -425.751 -425.751] [99.7221], Avg: [-575.438 -575.438 -575.438] (1.000)
Step: 89000, Reward: [-422.309 -422.309 -422.309] [48.6202], Avg: [-573.718 -573.718 -573.718] (1.000)
Step: 90000, Reward: [-395.16 -395.16 -395.16] [66.2983], Avg: [-571.734 -571.734 -571.734] (1.000)
Step: 91000, Reward: [-392.893 -392.893 -392.893] [95.5937], Avg: [-569.768 -569.768 -569.768] (1.000)
Step: 92000, Reward: [-397.365 -397.365 -397.365] [65.5838], Avg: [-567.894 -567.894 -567.894] (1.000)
Step: 93000, Reward: [-436.107 -436.107 -436.107] [77.1368], Avg: [-566.477 -566.477 -566.477] (1.000)
Step: 94000, Reward: [-447.415 -447.415 -447.415] [105.6511], Avg: [-565.211 -565.211 -565.211] (1.000)
Step: 95000, Reward: [-450.938 -450.938 -450.938] [81.3365], Avg: [-564.008 -564.008 -564.008] (1.000)
Step: 96000, Reward: [-435.905 -435.905 -435.905] [69.1980], Avg: [-562.673 -562.673 -562.673] (1.000)
Step: 97000, Reward: [-424.714 -424.714 -424.714] [88.5612], Avg: [-561.251 -561.251 -561.251] (1.000)
Step: 98000, Reward: [-415.036 -415.036 -415.036] [56.8080], Avg: [-559.759 -559.759 -559.759] (1.000)
Step: 99000, Reward: [-433.78 -433.78 -433.78] [48.3101], Avg: [-558.487 -558.487 -558.487] (1.000)
Step: 100000, Reward: [-449.472 -449.472 -449.472] [84.1905], Avg: [-557.397 -557.397 -557.397] (1.000)
Step: 101000, Reward: [-434.516 -434.516 -434.516] [83.8061], Avg: [-556.18 -556.18 -556.18] (1.000)
Step: 102000, Reward: [-433.275 -433.275 -433.275] [100.4901], Avg: [-554.975 -554.975 -554.975] (1.000)
Step: 103000, Reward: [-421.203 -421.203 -421.203] [65.3957], Avg: [-553.676 -553.676 -553.676] (1.000)
Step: 104000, Reward: [-423.571 -423.571 -423.571] [75.6135], Avg: [-552.425 -552.425 -552.425] (1.000)
Step: 105000, Reward: [-417.732 -417.732 -417.732] [83.5350], Avg: [-551.142 -551.142 -551.142] (1.000)
Step: 106000, Reward: [-405.744 -405.744 -405.744] [79.4949], Avg: [-549.771 -549.771 -549.771] (1.000)
Step: 107000, Reward: [-393.283 -393.283 -393.283] [75.1686], Avg: [-548.308 -548.308 -548.308] (1.000)
Step: 108000, Reward: [-430.304 -430.304 -430.304] [73.9515], Avg: [-547.216 -547.216 -547.216] (1.000)
Step: 109000, Reward: [-418.585 -418.585 -418.585] [61.8087], Avg: [-546.035 -546.035 -546.035] (1.000)
Step: 110000, Reward: [-411.98 -411.98 -411.98] [56.6452], Avg: [-544.817 -544.817 -544.817] (1.000)
Step: 111000, Reward: [-420.986 -420.986 -420.986] [72.6499], Avg: [-543.701 -543.701 -543.701] (1.000)
Step: 112000, Reward: [-461.74 -461.74 -461.74] [79.0878], Avg: [-542.969 -542.969 -542.969] (1.000)
Step: 113000, Reward: [-431.037 -431.037 -431.037] [95.4143], Avg: [-541.979 -541.979 -541.979] (1.000)
Step: 114000, Reward: [-419.968 -419.968 -419.968] [66.9899], Avg: [-540.909 -540.909 -540.909] (1.000)
Step: 115000, Reward: [-433.516 -433.516 -433.516] [84.3593], Avg: [-539.975 -539.975 -539.975] (1.000)
Step: 116000, Reward: [-400.89 -400.89 -400.89] [66.8057], Avg: [-538.776 -538.776 -538.776] (1.000)
Step: 117000, Reward: [-446.613 -446.613 -446.613] [146.6394], Avg: [-537.988 -537.988 -537.988] (1.000)
Step: 118000, Reward: [-429.139 -429.139 -429.139] [91.7596], Avg: [-537.066 -537.066 -537.066] (1.000)
Step: 119000, Reward: [-417.094 -417.094 -417.094] [74.0132], Avg: [-536.057 -536.057 -536.057] (1.000)
Step: 120000, Reward: [-407.405 -407.405 -407.405] [51.6106], Avg: [-534.985 -534.985 -534.985] (1.000)
Step: 121000, Reward: [-406.404 -406.404 -406.404] [82.5992], Avg: [-533.923 -533.923 -533.923] (1.000)
Step: 122000, Reward: [-410.435 -410.435 -410.435] [61.5609], Avg: [-532.91 -532.91 -532.91] (1.000)
Step: 123000, Reward: [-412.334 -412.334 -412.334] [79.1382], Avg: [-531.93 -531.93 -531.93] (1.000)
Step: 124000, Reward: [-407.065 -407.065 -407.065] [67.4793], Avg: [-530.923 -530.923 -530.923] (1.000)
Step: 125000, Reward: [-411.83 -411.83 -411.83] [73.2345], Avg: [-529.97 -529.97 -529.97] (1.000)
Step: 126000, Reward: [-400.017 -400.017 -400.017] [89.8403], Avg: [-528.939 -528.939 -528.939] (1.000)
Step: 127000, Reward: [-423.18 -423.18 -423.18] [74.6605], Avg: [-528.106 -528.106 -528.106] (1.000)
Step: 128000, Reward: [-411.874 -411.874 -411.874] [60.8195], Avg: [-527.198 -527.198 -527.198] (1.000)
Step: 129000, Reward: [-401.899 -401.899 -401.899] [55.1530], Avg: [-526.227 -526.227 -526.227] (1.000)
Step: 130000, Reward: [-421.295 -421.295 -421.295] [114.3077], Avg: [-525.42 -525.42 -525.42] (1.000)
Step: 131000, Reward: [-411.935 -411.935 -411.935] [57.2594], Avg: [-524.553 -524.553 -524.553] (1.000)
Step: 132000, Reward: [-389.305 -389.305 -389.305] [69.1043], Avg: [-523.529 -523.529 -523.529] (1.000)
Step: 133000, Reward: [-415.338 -415.338 -415.338] [67.3813], Avg: [-522.715 -522.715 -522.715] (1.000)
Step: 134000, Reward: [-405.111 -405.111 -405.111] [82.6500], Avg: [-521.838 -521.838 -521.838] (1.000)
Step: 135000, Reward: [-409.09 -409.09 -409.09] [91.6972], Avg: [-521.003 -521.003 -521.003] (1.000)
Step: 136000, Reward: [-429.88 -429.88 -429.88] [67.0497], Avg: [-520.333 -520.333 -520.333] (1.000)
Step: 137000, Reward: [-421.815 -421.815 -421.815] [67.4138], Avg: [-519.613 -519.613 -519.613] (1.000)
Step: 138000, Reward: [-401.769 -401.769 -401.769] [72.9134], Avg: [-518.759 -518.759 -518.759] (1.000)
Step: 139000, Reward: [-432.942 -432.942 -432.942] [67.6404], Avg: [-518.142 -518.142 -518.142] (1.000)
Step: 140000, Reward: [-418.694 -418.694 -418.694] [96.7996], Avg: [-517.432 -517.432 -517.432] (1.000)
Step: 141000, Reward: [-393.524 -393.524 -393.524] [71.0488], Avg: [-516.553 -516.553 -516.553] (1.000)
Step: 142000, Reward: [-416.53 -416.53 -416.53] [80.0035], Avg: [-515.849 -515.849 -515.849] (1.000)
Step: 143000, Reward: [-446.296 -446.296 -446.296] [81.8115], Avg: [-515.362 -515.362 -515.362] (1.000)
Step: 144000, Reward: [-401.226 -401.226 -401.226] [79.1799], Avg: [-514.57 -514.57 -514.57] (1.000)
Step: 145000, Reward: [-399.354 -399.354 -399.354] [76.8681], Avg: [-513.775 -513.775 -513.775] (1.000)
Step: 146000, Reward: [-410.56 -410.56 -410.56] [66.9854], Avg: [-513.068 -513.068 -513.068] (1.000)
Step: 147000, Reward: [-406.403 -406.403 -406.403] [82.7957], Avg: [-512.342 -512.342 -512.342] (1.000)
Step: 148000, Reward: [-430.441 -430.441 -430.441] [52.3888], Avg: [-511.789 -511.789 -511.789] (1.000)
Step: 149000, Reward: [-400.351 -400.351 -400.351] [72.1992], Avg: [-511.041 -511.041 -511.041] (1.000)
Step: 150000, Reward: [-415.89 -415.89 -415.89] [59.3523], Avg: [-510.407 -510.407 -510.407] (1.000)
Step: 151000, Reward: [-412.503 -412.503 -412.503] [40.5548], Avg: [-509.758 -509.758 -509.758] (1.000)
Step: 152000, Reward: [-439.857 -439.857 -439.857] [90.0190], Avg: [-509.299 -509.299 -509.299] (1.000)
Step: 153000, Reward: [-430.945 -430.945 -430.945] [75.9078], Avg: [-508.786 -508.786 -508.786] (1.000)
Step: 154000, Reward: [-440.864 -440.864 -440.864] [53.5859], Avg: [-508.345 -508.345 -508.345] (1.000)
Step: 155000, Reward: [-408.848 -408.848 -408.848] [54.4408], Avg: [-507.703 -507.703 -507.703] (1.000)
Step: 156000, Reward: [-425.071 -425.071 -425.071] [68.5513], Avg: [-507.174 -507.174 -507.174] (1.000)
Step: 157000, Reward: [-434.756 -434.756 -434.756] [90.7707], Avg: [-506.713 -506.713 -506.713] (1.000)
Step: 158000, Reward: [-440.576 -440.576 -440.576] [77.7515], Avg: [-506.294 -506.294 -506.294] (1.000)
Step: 159000, Reward: [-437.595 -437.595 -437.595] [82.1348], Avg: [-505.862 -505.862 -505.862] (1.000)
Step: 160000, Reward: [-422.824 -422.824 -422.824] [73.3710], Avg: [-505.343 -505.343 -505.343] (1.000)
Step: 161000, Reward: [-424.61 -424.61 -424.61] [90.2545], Avg: [-504.841 -504.841 -504.841] (1.000)
Step: 162000, Reward: [-419.801 -419.801 -419.801] [73.8069], Avg: [-504.316 -504.316 -504.316] (1.000)
Step: 163000, Reward: [-393.607 -393.607 -393.607] [54.6904], Avg: [-503.637 -503.637 -503.637] (1.000)
Step: 164000, Reward: [-412.983 -412.983 -412.983] [76.5877], Avg: [-503.085 -503.085 -503.085] (1.000)
Step: 165000, Reward: [-408.26 -408.26 -408.26] [76.6142], Avg: [-502.51 -502.51 -502.51] (1.000)
Step: 166000, Reward: [-422.449 -422.449 -422.449] [75.1834], Avg: [-502.028 -502.028 -502.028] (1.000)
Step: 167000, Reward: [-399.162 -399.162 -399.162] [99.1310], Avg: [-501.412 -501.412 -501.412] (1.000)
Step: 168000, Reward: [-414.866 -414.866 -414.866] [70.6356], Avg: [-500.896 -500.896 -500.896] (1.000)
Step: 169000, Reward: [-400.133 -400.133 -400.133] [51.9592], Avg: [-500.3 -500.3 -500.3] (1.000)
Step: 170000, Reward: [-395.626 -395.626 -395.626] [88.0838], Avg: [-499.684 -499.684 -499.684] (1.000)
Step: 171000, Reward: [-402.052 -402.052 -402.052] [64.1044], Avg: [-499.113 -499.113 -499.113] (1.000)
Step: 172000, Reward: [-391.939 -391.939 -391.939] [74.1657], Avg: [-498.49 -498.49 -498.49] (1.000)
Step: 173000, Reward: [-409.383 -409.383 -409.383] [62.0272], Avg: [-497.975 -497.975 -497.975] (1.000)
Step: 174000, Reward: [-409.191 -409.191 -409.191] [58.5993], Avg: [-497.465 -497.465 -497.465] (1.000)
Step: 175000, Reward: [-410.181 -410.181 -410.181] [61.1557], Avg: [-496.966 -496.966 -496.966] (1.000)
Step: 176000, Reward: [-440.676 -440.676 -440.676] [71.6295], Avg: [-496.646 -496.646 -496.646] (1.000)
Step: 177000, Reward: [-395.454 -395.454 -395.454] [67.5418], Avg: [-496.075 -496.075 -496.075] (1.000)
Step: 178000, Reward: [-403.185 -403.185 -403.185] [70.6146], Avg: [-495.553 -495.553 -495.553] (1.000)
Step: 179000, Reward: [-396.866 -396.866 -396.866] [64.1616], Avg: [-495.002 -495.002 -495.002] (1.000)
Step: 180000, Reward: [-419.17 -419.17 -419.17] [75.5342], Avg: [-494.58 -494.58 -494.58] (1.000)
Step: 181000, Reward: [-404.396 -404.396 -404.396] [62.7773], Avg: [-494.082 -494.082 -494.082] (1.000)
Step: 182000, Reward: [-404.956 -404.956 -404.956] [72.3485], Avg: [-493.592 -493.592 -493.592] (1.000)
Step: 183000, Reward: [-403.245 -403.245 -403.245] [87.7097], Avg: [-493.099 -493.099 -493.099] (1.000)
Step: 184000, Reward: [-421.537 -421.537 -421.537] [58.4374], Avg: [-492.71 -492.71 -492.71] (1.000)
Step: 185000, Reward: [-436.203 -436.203 -436.203] [93.2544], Avg: [-492.404 -492.404 -492.404] (1.000)
Step: 186000, Reward: [-403.483 -403.483 -403.483] [63.5594], Avg: [-491.926 -491.926 -491.926] (1.000)
Step: 187000, Reward: [-414.518 -414.518 -414.518] [88.5781], Avg: [-491.512 -491.512 -491.512] (1.000)
Step: 188000, Reward: [-430.258 -430.258 -430.258] [47.5702], Avg: [-491.186 -491.186 -491.186] (1.000)
Step: 189000, Reward: [-382.869 -382.869 -382.869] [62.2452], Avg: [-490.613 -490.613 -490.613] (1.000)
Step: 190000, Reward: [-389.928 -389.928 -389.928] [76.6534], Avg: [-490.083 -490.083 -490.083] (1.000)
Step: 191000, Reward: [-415.916 -415.916 -415.916] [66.6692], Avg: [-489.695 -489.695 -489.695] (1.000)
Step: 192000, Reward: [-406.275 -406.275 -406.275] [73.3204], Avg: [-489.261 -489.261 -489.261] (1.000)
Step: 193000, Reward: [-408.915 -408.915 -408.915] [78.4644], Avg: [-488.844 -488.844 -488.844] (1.000)
Step: 194000, Reward: [-421.231 -421.231 -421.231] [61.2192], Avg: [-488.496 -488.496 -488.496] (1.000)
Step: 195000, Reward: [-413.294 -413.294 -413.294] [53.3665], Avg: [-488.11 -488.11 -488.11] (1.000)
Step: 196000, Reward: [-414.889 -414.889 -414.889] [47.3499], Avg: [-487.737 -487.737 -487.737] (1.000)
Step: 197000, Reward: [-399.949 -399.949 -399.949] [74.4064], Avg: [-487.291 -487.291 -487.291] (1.000)
Step: 198000, Reward: [-430.141 -430.141 -430.141] [72.7823], Avg: [-487.002 -487.002 -487.002] (1.000)
Step: 199000, Reward: [-432.193 -432.193 -432.193] [66.2053], Avg: [-486.727 -486.727 -486.727] (1.000)
