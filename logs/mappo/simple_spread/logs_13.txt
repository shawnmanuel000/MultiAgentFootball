Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 32, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiHeadAttention

ENTROPY_WEIGHT = 0.01			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.2				# The limit of the ratio of new action probabilities to old probabilities

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.attn = MultiHeadAttention(state_size[-1], 8, 64)
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.attn(state)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, states_joint, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):
			values = model.get_value(states_joint[:-1], grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			entropy, new_log_prob = model.get_action_probs(state[:-1], action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			self.network.optimize(states, actions, log_probs, states_joint, targets, advantages)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, save_at=100, checkpoint=True, save_best=False, log=True, render=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, num_envs=envs.num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = rollout(envs, agent, render=True)
			total_rewards.append(np.mean(rollouts, axis=-1) - np.std(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]+np.std(rollouts, axis=-1)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-521.297 -521.297 -521.297] [98.2506], Avg: [-619.548 -619.548 -619.548] (1.000)
Step: 99, Reward: [-501.676 -501.676 -501.676] [105.8229], Avg: [-613.523 -613.523 -613.523] (1.000)
Step: 149, Reward: [-521.881 -521.881 -521.881] [94.5431], Avg: [-614.49 -614.49 -614.49] (1.000)
Step: 199, Reward: [-551.363 -551.363 -551.363] [113.0069], Avg: [-626.96 -626.96 -626.96] (1.000)
Step: 249, Reward: [-556.718 -556.718 -556.718] [161.4806], Avg: [-645.208 -645.208 -645.208] (1.000)
Step: 299, Reward: [-580.351 -580.351 -580.351] [121.5159], Avg: [-654.651 -654.651 -654.651] (1.000)
Step: 349, Reward: [-564.184 -564.184 -564.184] [112.4388], Avg: [-657.79 -657.79 -657.79] (1.000)
Step: 399, Reward: [-595.269 -595.269 -595.269] [157.0700], Avg: [-669.608 -669.608 -669.608] (1.000)
Step: 449, Reward: [-563.89 -563.89 -563.89] [122.5406], Avg: [-671.477 -671.477 -671.477] (1.000)
Step: 499, Reward: [-560.673 -560.673 -560.673] [102.9864], Avg: [-670.696 -670.696 -670.696] (1.000)
Step: 549, Reward: [-554.575 -554.575 -554.575] [121.6567], Avg: [-671.199 -671.199 -671.199] (1.000)
Step: 599, Reward: [-597.256 -597.256 -597.256] [107.6143], Avg: [-674.005 -674.005 -674.005] (1.000)
Step: 649, Reward: [-631.075 -631.075 -631.075] [167.9363], Avg: [-683.621 -683.621 -683.621] (1.000)
Step: 699, Reward: [-574.538 -574.538 -574.538] [130.4104], Avg: [-685.144 -685.144 -685.144] (1.000)
Step: 749, Reward: [-546.681 -546.681 -546.681] [148.9411], Avg: [-685.843 -685.843 -685.843] (1.000)
Step: 799, Reward: [-598.454 -598.454 -598.454] [132.9947], Avg: [-688.693 -688.693 -688.693] (1.000)
Step: 849, Reward: [-595.335 -595.335 -595.335] [149.5711], Avg: [-692. -692. -692.] (1.000)
Step: 899, Reward: [-556.324 -556.324 -556.324] [111.7808], Avg: [-690.672 -690.672 -690.672] (1.000)
Step: 949, Reward: [-547.61 -547.61 -547.61] [128.4095], Avg: [-689.901 -689.901 -689.901] (1.000)
Step: 999, Reward: [-537.453 -537.453 -537.453] [123.2044], Avg: [-688.439 -688.439 -688.439] (1.000)
Step: 1049, Reward: [-543.068 -543.068 -543.068] [121.3340], Avg: [-687.294 -687.294 -687.294] (1.000)
Step: 1099, Reward: [-558.457 -558.457 -558.457] [91.0364], Avg: [-685.576 -685.576 -685.576] (1.000)
Step: 1149, Reward: [-520.25 -520.25 -520.25] [93.4174], Avg: [-682.45 -682.45 -682.45] (1.000)
Step: 1199, Reward: [-525.289 -525.289 -525.289] [134.1503], Avg: [-681.491 -681.491 -681.491] (1.000)
Step: 1249, Reward: [-543.447 -543.447 -543.447] [78.0340], Avg: [-679.09 -679.09 -679.09] (1.000)
Step: 1299, Reward: [-500.584 -500.584 -500.584] [105.0763], Avg: [-676.266 -676.266 -676.266] (1.000)
Step: 1349, Reward: [-480.419 -480.419 -480.419] [86.1022], Avg: [-672.202 -672.202 -672.202] (1.000)
Step: 1399, Reward: [-503.508 -503.508 -503.508] [106.8581], Avg: [-669.993 -669.993 -669.993] (1.000)
Step: 1449, Reward: [-457.185 -457.185 -457.185] [70.7078], Avg: [-665.093 -665.093 -665.093] (1.000)
Step: 1499, Reward: [-492.879 -492.879 -492.879] [86.6274], Avg: [-662.24 -662.24 -662.24] (1.000)
Step: 1549, Reward: [-460.385 -460.385 -460.385] [74.3031], Avg: [-658.126 -658.126 -658.126] (1.000)
Step: 1599, Reward: [-485.222 -485.222 -485.222] [111.9802], Avg: [-656.222 -656.222 -656.222] (1.000)
Step: 1649, Reward: [-498.964 -498.964 -498.964] [116.3217], Avg: [-654.981 -654.981 -654.981] (1.000)
Step: 1699, Reward: [-474.116 -474.116 -474.116] [122.5668], Avg: [-653.267 -653.267 -653.267] (1.000)
Step: 1749, Reward: [-473.086 -473.086 -473.086] [73.4681], Avg: [-650.218 -650.218 -650.218] (1.000)
Step: 1799, Reward: [-483.332 -483.332 -483.332] [82.5504], Avg: [-647.875 -647.875 -647.875] (1.000)
Step: 1849, Reward: [-457.798 -457.798 -457.798] [83.1808], Avg: [-644.986 -644.986 -644.986] (1.000)
Step: 1899, Reward: [-438.172 -438.172 -438.172] [87.6148], Avg: [-641.849 -641.849 -641.849] (1.000)
Step: 1949, Reward: [-468.711 -468.711 -468.711] [82.0064], Avg: [-639.512 -639.512 -639.512] (1.000)
Step: 1999, Reward: [-441.325 -441.325 -441.325] [56.5988], Avg: [-635.973 -635.973 -635.973] (1.000)
Step: 2049, Reward: [-444.844 -444.844 -444.844] [96.0402], Avg: [-633.653 -633.653 -633.653] (1.000)
Step: 2099, Reward: [-446.3 -446.3 -446.3] [86.1598], Avg: [-631.244 -631.244 -631.244] (1.000)
Step: 2149, Reward: [-427.444 -427.444 -427.444] [69.3840], Avg: [-628.118 -628.118 -628.118] (1.000)
Step: 2199, Reward: [-437.759 -437.759 -437.759] [73.9805], Avg: [-625.473 -625.473 -625.473] (1.000)
Step: 2249, Reward: [-434.044 -434.044 -434.044] [74.1340], Avg: [-622.867 -622.867 -622.867] (1.000)
Step: 2299, Reward: [-422.832 -422.832 -422.832] [86.2209], Avg: [-620.392 -620.392 -620.392] (1.000)
Step: 2349, Reward: [-394.835 -394.835 -394.835] [63.4779], Avg: [-616.944 -616.944 -616.944] (1.000)
Step: 2399, Reward: [-447.647 -447.647 -447.647] [72.3680], Avg: [-614.925 -614.925 -614.925] (1.000)
Step: 2449, Reward: [-417.332 -417.332 -417.332] [71.4541], Avg: [-612.35 -612.35 -612.35] (1.000)
Step: 2499, Reward: [-424.944 -424.944 -424.944] [59.0905], Avg: [-609.784 -609.784 -609.784] (1.000)
Step: 2549, Reward: [-405.392 -405.392 -405.392] [72.7896], Avg: [-607.204 -607.204 -607.204] (1.000)
Step: 2599, Reward: [-441.472 -441.472 -441.472] [90.3539], Avg: [-605.754 -605.754 -605.754] (1.000)
Step: 2649, Reward: [-428.237 -428.237 -428.237] [67.3176], Avg: [-603.675 -603.675 -603.675] (1.000)
Step: 2699, Reward: [-420.01 -420.01 -420.01] [64.4682], Avg: [-601.467 -601.467 -601.467] (1.000)
Step: 2749, Reward: [-406.523 -406.523 -406.523] [66.8380], Avg: [-599.138 -599.138 -599.138] (1.000)
Step: 2799, Reward: [-413.945 -413.945 -413.945] [57.0811], Avg: [-596.85 -596.85 -596.85] (1.000)
Step: 2849, Reward: [-431.937 -431.937 -431.937] [61.2554], Avg: [-595.032 -595.032 -595.032] (1.000)
Step: 2899, Reward: [-415.135 -415.135 -415.135] [68.5360], Avg: [-593.112 -593.112 -593.112] (1.000)
Step: 2949, Reward: [-420.453 -420.453 -420.453] [54.9592], Avg: [-591.117 -591.117 -591.117] (1.000)
Step: 2999, Reward: [-415.41 -415.41 -415.41] [67.5729], Avg: [-589.315 -589.315 -589.315] (1.000)
Step: 3049, Reward: [-418.689 -418.689 -418.689] [71.6567], Avg: [-587.692 -587.692 -587.692] (1.000)
Step: 3099, Reward: [-403.677 -403.677 -403.677] [70.2833], Avg: [-585.858 -585.858 -585.858] (1.000)
Step: 3149, Reward: [-415.837 -415.837 -415.837] [79.4213], Avg: [-584.42 -584.42 -584.42] (1.000)
Step: 3199, Reward: [-408.588 -408.588 -408.588] [73.7270], Avg: [-582.824 -582.824 -582.824] (1.000)
Step: 3249, Reward: [-412.16 -412.16 -412.16] [64.3790], Avg: [-581.189 -581.189 -581.189] (1.000)
Step: 3299, Reward: [-388.716 -388.716 -388.716] [46.7051], Avg: [-578.981 -578.981 -578.981] (1.000)
Step: 3349, Reward: [-402.238 -402.238 -402.238] [72.7690], Avg: [-577.429 -577.429 -577.429] (1.000)
Step: 3399, Reward: [-420.784 -420.784 -420.784] [57.4306], Avg: [-575.97 -575.97 -575.97] (1.000)
Step: 3449, Reward: [-415.792 -415.792 -415.792] [78.3997], Avg: [-574.785 -574.785 -574.785] (1.000)
Step: 3499, Reward: [-412.152 -412.152 -412.152] [65.7842], Avg: [-573.401 -573.401 -573.401] (1.000)
Step: 3549, Reward: [-445.413 -445.413 -445.413] [78.3597], Avg: [-572.702 -572.702 -572.702] (1.000)
Step: 3599, Reward: [-407.462 -407.462 -407.462] [61.8450], Avg: [-571.266 -571.266 -571.266] (1.000)
Step: 3649, Reward: [-403.088 -403.088 -403.088] [51.0481], Avg: [-569.662 -569.662 -569.662] (1.000)
Step: 3699, Reward: [-416.814 -416.814 -416.814] [75.9001], Avg: [-568.622 -568.622 -568.622] (1.000)
Step: 3749, Reward: [-413.92 -413.92 -413.92] [62.6656], Avg: [-567.395 -567.395 -567.395] (1.000)
Step: 3799, Reward: [-404.617 -404.617 -404.617] [70.0906], Avg: [-566.175 -566.175 -566.175] (1.000)
Step: 3849, Reward: [-420.109 -420.109 -420.109] [76.4791], Avg: [-565.271 -565.271 -565.271] (1.000)
Step: 3899, Reward: [-421.45 -421.45 -421.45] [51.7632], Avg: [-564.091 -564.091 -564.091] (1.000)
Step: 3949, Reward: [-429.36 -429.36 -429.36] [63.8615], Avg: [-563.194 -563.194 -563.194] (1.000)
Step: 3999, Reward: [-418.117 -418.117 -418.117] [70.3255], Avg: [-562.26 -562.26 -562.26] (1.000)
Step: 4049, Reward: [-423.744 -423.744 -423.744] [62.0529], Avg: [-561.316 -561.316 -561.316] (1.000)
Step: 4099, Reward: [-415.555 -415.555 -415.555] [66.3584], Avg: [-560.347 -560.347 -560.347] (1.000)
Step: 4149, Reward: [-417.366 -417.366 -417.366] [62.0141], Avg: [-559.372 -559.372 -559.372] (1.000)
Step: 4199, Reward: [-407.581 -407.581 -407.581] [72.1431], Avg: [-558.424 -558.424 -558.424] (1.000)
Step: 4249, Reward: [-446.301 -446.301 -446.301] [60.4450], Avg: [-557.816 -557.816 -557.816] (1.000)
Step: 4299, Reward: [-437.274 -437.274 -437.274] [57.5381], Avg: [-557.083 -557.083 -557.083] (1.000)
Step: 4349, Reward: [-441.538 -441.538 -441.538] [63.4463], Avg: [-556.484 -556.484 -556.484] (1.000)
Step: 4399, Reward: [-439.563 -439.563 -439.563] [69.2335], Avg: [-555.942 -555.942 -555.942] (1.000)
Step: 4449, Reward: [-441.92 -441.92 -441.92] [85.0794], Avg: [-555.617 -555.617 -555.617] (1.000)
Step: 4499, Reward: [-436.802 -436.802 -436.802] [70.1380], Avg: [-555.076 -555.076 -555.076] (1.000)
Step: 4549, Reward: [-473.198 -473.198 -473.198] [90.6170], Avg: [-555.172 -555.172 -555.172] (1.000)
Step: 4599, Reward: [-447.364 -447.364 -447.364] [63.8294], Avg: [-554.694 -554.694 -554.694] (1.000)
Step: 4649, Reward: [-486.543 -486.543 -486.543] [77.9697], Avg: [-554.8 -554.8 -554.8] (1.000)
Step: 4699, Reward: [-441.948 -441.948 -441.948] [74.8618], Avg: [-554.396 -554.396 -554.396] (1.000)
Step: 4749, Reward: [-451.191 -451.191 -451.191] [71.4631], Avg: [-554.061 -554.061 -554.061] (1.000)
Step: 4799, Reward: [-452.571 -452.571 -452.571] [73.8933], Avg: [-553.774 -553.774 -553.774] (1.000)
Step: 4849, Reward: [-487.199 -487.199 -487.199] [84.0436], Avg: [-553.954 -553.954 -553.954] (1.000)
Step: 4899, Reward: [-488.891 -488.891 -488.891] [82.8911], Avg: [-554.136 -554.136 -554.136] (1.000)
Step: 4949, Reward: [-523.304 -523.304 -523.304] [114.9071], Avg: [-554.985 -554.985 -554.985] (1.000)
Step: 4999, Reward: [-501.773 -501.773 -501.773] [82.0033], Avg: [-555.273 -555.273 -555.273] (1.000)
Step: 5049, Reward: [-471.169 -471.169 -471.169] [94.0557], Avg: [-555.372 -555.372 -555.372] (1.000)
Step: 5099, Reward: [-515.801 -515.801 -515.801] [104.6923], Avg: [-556.01 -556.01 -556.01] (1.000)
Step: 5149, Reward: [-463.632 -463.632 -463.632] [92.9966], Avg: [-556.016 -556.016 -556.016] (1.000)
Step: 5199, Reward: [-495.489 -495.489 -495.489] [67.7202], Avg: [-556.085 -556.085 -556.085] (1.000)
Step: 5249, Reward: [-497.611 -497.611 -497.611] [92.8766], Avg: [-556.413 -556.413 -556.413] (1.000)
Step: 5299, Reward: [-468.894 -468.894 -468.894] [85.5119], Avg: [-556.394 -556.394 -556.394] (1.000)
Step: 5349, Reward: [-466.161 -466.161 -466.161] [79.5999], Avg: [-556.295 -556.295 -556.295] (1.000)
Step: 5399, Reward: [-466.488 -466.488 -466.488] [79.4843], Avg: [-556.199 -556.199 -556.199] (1.000)
Step: 5449, Reward: [-456.198 -456.198 -456.198] [91.1498], Avg: [-556.118 -556.118 -556.118] (1.000)
Step: 5499, Reward: [-471.866 -471.866 -471.866] [82.1628], Avg: [-556.099 -556.099 -556.099] (1.000)
Step: 5549, Reward: [-484.578 -484.578 -484.578] [75.7788], Avg: [-556.137 -556.137 -556.137] (1.000)
Step: 5599, Reward: [-469.826 -469.826 -469.826] [78.0290], Avg: [-556.063 -556.063 -556.063] (1.000)
Step: 5649, Reward: [-468.603 -468.603 -468.603] [85.1442], Avg: [-556.043 -556.043 -556.043] (1.000)
Step: 5699, Reward: [-444.677 -444.677 -444.677] [76.3506], Avg: [-555.736 -555.736 -555.736] (1.000)
Step: 5749, Reward: [-464.574 -464.574 -464.574] [83.0401], Avg: [-555.665 -555.665 -555.665] (1.000)
Step: 5799, Reward: [-459.992 -459.992 -459.992] [72.9322], Avg: [-555.469 -555.469 -555.469] (1.000)
Step: 5849, Reward: [-456.114 -456.114 -456.114] [96.3665], Avg: [-555.443 -555.443 -555.443] (1.000)
Step: 5899, Reward: [-484.565 -484.565 -484.565] [90.2901], Avg: [-555.608 -555.608 -555.608] (1.000)
Step: 5949, Reward: [-461.293 -461.293 -461.293] [82.1368], Avg: [-555.506 -555.506 -555.506] (1.000)
Step: 5999, Reward: [-436.599 -436.599 -436.599] [75.0582], Avg: [-555.14 -555.14 -555.14] (1.000)
Step: 6049, Reward: [-464.134 -464.134 -464.134] [91.7487], Avg: [-555.146 -555.146 -555.146] (1.000)
Step: 6099, Reward: [-419.218 -419.218 -419.218] [59.0236], Avg: [-554.516 -554.516 -554.516] (1.000)
Step: 6149, Reward: [-432.082 -432.082 -432.082] [71.3840], Avg: [-554.101 -554.101 -554.101] (1.000)
Step: 6199, Reward: [-439.062 -439.062 -439.062] [55.7679], Avg: [-553.623 -553.623 -553.623] (1.000)
Step: 6249, Reward: [-422.977 -422.977 -422.977] [84.1884], Avg: [-553.251 -553.251 -553.251] (1.000)
Step: 6299, Reward: [-438.834 -438.834 -438.834] [76.1624], Avg: [-552.948 -552.948 -552.948] (1.000)
Step: 6349, Reward: [-469.257 -469.257 -469.257] [82.1419], Avg: [-552.935 -552.935 -552.935] (1.000)
Step: 6399, Reward: [-419.468 -419.468 -419.468] [64.6806], Avg: [-552.398 -552.398 -552.398] (1.000)
Step: 6449, Reward: [-438.809 -438.809 -438.809] [80.8729], Avg: [-552.144 -552.144 -552.144] (1.000)
Step: 6499, Reward: [-433.642 -433.642 -433.642] [58.4375], Avg: [-551.682 -551.682 -551.682] (1.000)
Step: 6549, Reward: [-432.522 -432.522 -432.522] [65.4472], Avg: [-551.272 -551.272 -551.272] (1.000)
Step: 6599, Reward: [-454.592 -454.592 -454.592] [84.5186], Avg: [-551.18 -551.18 -551.18] (1.000)
Step: 6649, Reward: [-429.416 -429.416 -429.416] [59.6641], Avg: [-550.713 -550.713 -550.713] (1.000)
Step: 6699, Reward: [-431.117 -431.117 -431.117] [78.6581], Avg: [-550.408 -550.408 -550.408] (1.000)
Step: 6749, Reward: [-439.187 -439.187 -439.187] [58.1683], Avg: [-550.015 -550.015 -550.015] (1.000)
Step: 6799, Reward: [-451.454 -451.454 -451.454] [85.2723], Avg: [-549.917 -549.917 -549.917] (1.000)
Step: 6849, Reward: [-439.318 -439.318 -439.318] [76.2493], Avg: [-549.666 -549.666 -549.666] (1.000)
Step: 6899, Reward: [-430.001 -430.001 -430.001] [58.7517], Avg: [-549.225 -549.225 -549.225] (1.000)
Step: 6949, Reward: [-438.617 -438.617 -438.617] [68.6300], Avg: [-548.923 -548.923 -548.923] (1.000)
Step: 6999, Reward: [-414.841 -414.841 -414.841] [69.3824], Avg: [-548.461 -548.461 -548.461] (1.000)
Step: 7049, Reward: [-438.501 -438.501 -438.501] [62.9754], Avg: [-548.128 -548.128 -548.128] (1.000)
Step: 7099, Reward: [-444.745 -444.745 -444.745] [70.7568], Avg: [-547.898 -547.898 -547.898] (1.000)
Step: 7149, Reward: [-432.167 -432.167 -432.167] [74.7214], Avg: [-547.611 -547.611 -547.611] (1.000)
Step: 7199, Reward: [-443.396 -443.396 -443.396] [59.6328], Avg: [-547.302 -547.302 -547.302] (1.000)
Step: 7249, Reward: [-458.763 -458.763 -458.763] [67.7708], Avg: [-547.158 -547.158 -547.158] (1.000)
Step: 7299, Reward: [-428.91 -428.91 -428.91] [87.5548], Avg: [-546.948 -546.948 -546.948] (1.000)
Step: 7349, Reward: [-459.886 -459.886 -459.886] [61.2934], Avg: [-546.773 -546.773 -546.773] (1.000)
Step: 7399, Reward: [-438.639 -438.639 -438.639] [72.8408], Avg: [-546.534 -546.534 -546.534] (1.000)
Step: 7449, Reward: [-456.544 -456.544 -456.544] [74.0500], Avg: [-546.427 -546.427 -546.427] (1.000)
Step: 7499, Reward: [-460.509 -460.509 -460.509] [83.4382], Avg: [-546.411 -546.411 -546.411] (1.000)
Step: 7549, Reward: [-474.657 -474.657 -474.657] [70.4237], Avg: [-546.402 -546.402 -546.402] (1.000)
Step: 7599, Reward: [-449.775 -449.775 -449.775] [77.4877], Avg: [-546.276 -546.276 -546.276] (1.000)
Step: 7649, Reward: [-454.623 -454.623 -454.623] [67.1571], Avg: [-546.116 -546.116 -546.116] (1.000)
Step: 7699, Reward: [-452.749 -452.749 -452.749] [67.0204], Avg: [-545.945 -545.945 -545.945] (1.000)
Step: 7749, Reward: [-451.153 -451.153 -451.153] [71.5050], Avg: [-545.795 -545.795 -545.795] (1.000)
Step: 7799, Reward: [-469.566 -469.566 -469.566] [75.9916], Avg: [-545.793 -545.793 -545.793] (1.000)
Step: 7849, Reward: [-477.562 -477.562 -477.562] [84.9159], Avg: [-545.899 -545.899 -545.899] (1.000)
Step: 7899, Reward: [-479.131 -479.131 -479.131] [67.8717], Avg: [-545.906 -545.906 -545.906] (1.000)
Step: 7949, Reward: [-459.484 -459.484 -459.484] [63.2523], Avg: [-545.761 -545.761 -545.761] (1.000)
Step: 7999, Reward: [-465.89 -465.89 -465.89] [79.7535], Avg: [-545.76 -545.76 -545.76] (1.000)
Step: 8049, Reward: [-451.973 -451.973 -451.973] [82.4276], Avg: [-545.689 -545.689 -545.689] (1.000)
Step: 8099, Reward: [-486.304 -486.304 -486.304] [77.2331], Avg: [-545.8 -545.8 -545.8] (1.000)
Step: 8149, Reward: [-486.909 -486.909 -486.909] [76.1964], Avg: [-545.906 -545.906 -545.906] (1.000)
Step: 8199, Reward: [-475.27 -475.27 -475.27] [76.8568], Avg: [-545.944 -545.944 -545.944] (1.000)
Step: 8249, Reward: [-467.099 -467.099 -467.099] [78.8507], Avg: [-545.944 -545.944 -545.944] (1.000)
Step: 8299, Reward: [-460.604 -460.604 -460.604] [80.8404], Avg: [-545.917 -545.917 -545.917] (1.000)
Step: 8349, Reward: [-454.344 -454.344 -454.344] [62.4362], Avg: [-545.742 -545.742 -545.742] (1.000)
Step: 8399, Reward: [-471.134 -471.134 -471.134] [77.3530], Avg: [-545.758 -545.758 -545.758] (1.000)
Step: 8449, Reward: [-443.5 -443.5 -443.5] [60.1033], Avg: [-545.509 -545.509 -545.509] (1.000)
Step: 8499, Reward: [-452.081 -452.081 -452.081] [67.1609], Avg: [-545.354 -545.354 -545.354] (1.000)
Step: 8549, Reward: [-452.142 -452.142 -452.142] [69.4169], Avg: [-545.215 -545.215 -545.215] (1.000)
Step: 8599, Reward: [-448.723 -448.723 -448.723] [53.9359], Avg: [-544.968 -544.968 -544.968] (1.000)
Step: 8649, Reward: [-412.38 -412.38 -412.38] [68.9647], Avg: [-544.6 -544.6 -544.6] (1.000)
Step: 8699, Reward: [-442.469 -442.469 -442.469] [80.1982], Avg: [-544.474 -544.474 -544.474] (1.000)
Step: 8749, Reward: [-442.182 -442.182 -442.182] [79.0899], Avg: [-544.342 -544.342 -544.342] (1.000)
Step: 8799, Reward: [-441.421 -441.421 -441.421] [57.8216], Avg: [-544.085 -544.085 -544.085] (1.000)
Step: 8849, Reward: [-427.435 -427.435 -427.435] [69.5521], Avg: [-543.819 -543.819 -543.819] (1.000)
Step: 8899, Reward: [-430.378 -430.378 -430.378] [53.6806], Avg: [-543.483 -543.483 -543.483] (1.000)
Step: 8949, Reward: [-449.652 -449.652 -449.652] [65.5791], Avg: [-543.326 -543.326 -543.326] (1.000)
Step: 8999, Reward: [-437.165 -437.165 -437.165] [56.1109], Avg: [-543.048 -543.048 -543.048] (1.000)
Step: 9049, Reward: [-441.98 -441.98 -441.98] [71.2561], Avg: [-542.883 -542.883 -542.883] (1.000)
Step: 9099, Reward: [-424.372 -424.372 -424.372] [44.9786], Avg: [-542.479 -542.479 -542.479] (1.000)
Step: 9149, Reward: [-438.771 -438.771 -438.771] [56.9232], Avg: [-542.223 -542.223 -542.223] (1.000)
Step: 9199, Reward: [-449.549 -449.549 -449.549] [73.7556], Avg: [-542.12 -542.12 -542.12] (1.000)
Step: 9249, Reward: [-466.894 -466.894 -466.894] [69.7619], Avg: [-542.091 -542.091 -542.091] (1.000)
Step: 9299, Reward: [-450.122 -450.122 -450.122] [69.7263], Avg: [-541.971 -541.971 -541.971] (1.000)
Step: 9349, Reward: [-454.165 -454.165 -454.165] [55.6489], Avg: [-541.799 -541.799 -541.799] (1.000)
Step: 9399, Reward: [-437.554 -437.554 -437.554] [73.4135], Avg: [-541.635 -541.635 -541.635] (1.000)
Step: 9449, Reward: [-430.612 -430.612 -430.612] [59.4494], Avg: [-541.362 -541.362 -541.362] (1.000)
Step: 9499, Reward: [-464.187 -464.187 -464.187] [90.7659], Avg: [-541.434 -541.434 -541.434] (1.000)
Step: 9549, Reward: [-467.975 -467.975 -467.975] [67.4538], Avg: [-541.402 -541.402 -541.402] (1.000)
Step: 9599, Reward: [-444.199 -444.199 -444.199] [67.0989], Avg: [-541.246 -541.246 -541.246] (1.000)
Step: 9649, Reward: [-468.605 -468.605 -468.605] [84.5526], Avg: [-541.307 -541.307 -541.307] (1.000)
Step: 9699, Reward: [-465.053 -465.053 -465.053] [83.4868], Avg: [-541.345 -541.345 -541.345] (1.000)
Step: 9749, Reward: [-455.593 -455.593 -455.593] [70.3873], Avg: [-541.266 -541.266 -541.266] (1.000)
Step: 9799, Reward: [-439.164 -439.164 -439.164] [76.0440], Avg: [-541.133 -541.133 -541.133] (1.000)
Step: 9849, Reward: [-452.402 -452.402 -452.402] [72.3949], Avg: [-541.05 -541.05 -541.05] (1.000)
Step: 9899, Reward: [-467.877 -467.877 -467.877] [71.0316], Avg: [-541.039 -541.039 -541.039] (1.000)
Step: 9949, Reward: [-462.374 -462.374 -462.374] [76.7983], Avg: [-541.03 -541.03 -541.03] (1.000)
Step: 9999, Reward: [-458.851 -458.851 -458.851] [63.6704], Avg: [-540.937 -540.937 -540.937] (1.000)
Step: 10049, Reward: [-456.485 -456.485 -456.485] [70.7455], Avg: [-540.869 -540.869 -540.869] (1.000)
Step: 10099, Reward: [-469.977 -469.977 -469.977] [82.9481], Avg: [-540.929 -540.929 -540.929] (1.000)
Step: 10149, Reward: [-460.262 -460.262 -460.262] [72.7091], Avg: [-540.89 -540.89 -540.89] (1.000)
Step: 10199, Reward: [-471.819 -471.819 -471.819] [73.5241], Avg: [-540.911 -540.911 -540.911] (1.000)
Step: 10249, Reward: [-496.587 -496.587 -496.587] [93.7304], Avg: [-541.152 -541.152 -541.152] (1.000)
Step: 10299, Reward: [-461.69 -461.69 -461.69] [72.4585], Avg: [-541.118 -541.118 -541.118] (1.000)
Step: 10349, Reward: [-449.387 -449.387 -449.387] [78.9987], Avg: [-541.057 -541.057 -541.057] (1.000)
Step: 10399, Reward: [-468.829 -468.829 -468.829] [72.3223], Avg: [-541.057 -541.057 -541.057] (1.000)
Step: 10449, Reward: [-462.696 -462.696 -462.696] [69.5904], Avg: [-541.015 -541.015 -541.015] (1.000)
Step: 10499, Reward: [-482.507 -482.507 -482.507] [86.8286], Avg: [-541.15 -541.15 -541.15] (1.000)
Step: 10549, Reward: [-441.564 -441.564 -441.564] [68.0175], Avg: [-541.001 -541.001 -541.001] (1.000)
Step: 10599, Reward: [-458.218 -458.218 -458.218] [74.2838], Avg: [-540.961 -540.961 -540.961] (1.000)
Step: 10649, Reward: [-471.548 -471.548 -471.548] [49.0346], Avg: [-540.865 -540.865 -540.865] (1.000)
Step: 10699, Reward: [-453.199 -453.199 -453.199] [79.3298], Avg: [-540.826 -540.826 -540.826] (1.000)
Step: 10749, Reward: [-464.35 -464.35 -464.35] [81.6084], Avg: [-540.85 -540.85 -540.85] (1.000)
Step: 10799, Reward: [-455.859 -455.859 -455.859] [79.9048], Avg: [-540.826 -540.826 -540.826] (1.000)
Step: 10849, Reward: [-430.727 -430.727 -430.727] [72.6721], Avg: [-540.654 -540.654 -540.654] (1.000)
Step: 10899, Reward: [-470.052 -470.052 -470.052] [59.5359], Avg: [-540.603 -540.603 -540.603] (1.000)
Step: 10949, Reward: [-460.527 -460.527 -460.527] [70.9430], Avg: [-540.561 -540.561 -540.561] (1.000)
Step: 10999, Reward: [-471.253 -471.253 -471.253] [56.0104], Avg: [-540.501 -540.501 -540.501] (1.000)
Step: 11049, Reward: [-429.725 -429.725 -429.725] [50.1673], Avg: [-540.227 -540.227 -540.227] (1.000)
Step: 11099, Reward: [-476.773 -476.773 -476.773] [87.6743], Avg: [-540.336 -540.336 -540.336] (1.000)
Step: 11149, Reward: [-466.561 -466.561 -466.561] [65.2691], Avg: [-540.298 -540.298 -540.298] (1.000)
Step: 11199, Reward: [-448.841 -448.841 -448.841] [94.9685], Avg: [-540.313 -540.313 -540.313] (1.000)
Step: 11249, Reward: [-464.661 -464.661 -464.661] [63.2601], Avg: [-540.258 -540.258 -540.258] (1.000)
Step: 11299, Reward: [-467.881 -467.881 -467.881] [76.5296], Avg: [-540.277 -540.277 -540.277] (1.000)
Step: 11349, Reward: [-465.257 -465.257 -465.257] [73.4025], Avg: [-540.269 -540.269 -540.269] (1.000)
Step: 11399, Reward: [-469.934 -469.934 -469.934] [83.8955], Avg: [-540.329 -540.329 -540.329] (1.000)
Step: 11449, Reward: [-441.296 -441.296 -441.296] [69.5482], Avg: [-540.2 -540.2 -540.2] (1.000)
Step: 11499, Reward: [-457.015 -457.015 -457.015] [81.0476], Avg: [-540.191 -540.191 -540.191] (1.000)
Step: 11549, Reward: [-429.62 -429.62 -429.62] [85.8704], Avg: [-540.084 -540.084 -540.084] (1.000)
Step: 11599, Reward: [-460.822 -460.822 -460.822] [73.8576], Avg: [-540.061 -540.061 -540.061] (1.000)
Step: 11649, Reward: [-440.686 -440.686 -440.686] [68.0868], Avg: [-539.926 -539.926 -539.926] (1.000)
Step: 11699, Reward: [-436.73 -436.73 -436.73] [68.4593], Avg: [-539.778 -539.778 -539.778] (1.000)
Step: 11749, Reward: [-457.607 -457.607 -457.607] [59.5848], Avg: [-539.682 -539.682 -539.682] (1.000)
Step: 11799, Reward: [-447.033 -447.033 -447.033] [71.9645], Avg: [-539.594 -539.594 -539.594] (1.000)
Step: 11849, Reward: [-450.637 -450.637 -450.637] [78.9851], Avg: [-539.552 -539.552 -539.552] (1.000)
Step: 11899, Reward: [-443.83 -443.83 -443.83] [71.1812], Avg: [-539.449 -539.449 -539.449] (1.000)
Step: 11949, Reward: [-450.32 -450.32 -450.32] [66.5604], Avg: [-539.355 -539.355 -539.355] (1.000)
Step: 11999, Reward: [-451.797 -451.797 -451.797] [71.7181], Avg: [-539.289 -539.289 -539.289] (1.000)
Step: 12049, Reward: [-467.185 -467.185 -467.185] [75.6125], Avg: [-539.303 -539.303 -539.303] (1.000)
Step: 12099, Reward: [-446.568 -446.568 -446.568] [70.0209], Avg: [-539.209 -539.209 -539.209] (1.000)
Step: 12149, Reward: [-466.48 -466.48 -466.48] [78.7436], Avg: [-539.234 -539.234 -539.234] (1.000)
Step: 12199, Reward: [-447.175 -447.175 -447.175] [64.8236], Avg: [-539.122 -539.122 -539.122] (1.000)
Step: 12249, Reward: [-451.863 -451.863 -451.863] [71.1612], Avg: [-539.057 -539.057 -539.057] (1.000)
Step: 12299, Reward: [-439.477 -439.477 -439.477] [75.2720], Avg: [-538.958 -538.958 -538.958] (1.000)
Step: 12349, Reward: [-460.861 -460.861 -460.861] [67.0760], Avg: [-538.913 -538.913 -538.913] (1.000)
Step: 12399, Reward: [-451.741 -451.741 -451.741] [68.7236], Avg: [-538.839 -538.839 -538.839] (1.000)
Step: 12449, Reward: [-458.519 -458.519 -458.519] [81.6262], Avg: [-538.844 -538.844 -538.844] (1.000)
Step: 12499, Reward: [-452.533 -452.533 -452.533] [78.4903], Avg: [-538.813 -538.813 -538.813] (1.000)
Step: 12549, Reward: [-470.747 -470.747 -470.747] [84.3231], Avg: [-538.878 -538.878 -538.878] (1.000)
Step: 12599, Reward: [-459.629 -459.629 -459.629] [72.0253], Avg: [-538.849 -538.849 -538.849] (1.000)
Step: 12649, Reward: [-443.874 -443.874 -443.874] [78.0705], Avg: [-538.782 -538.782 -538.782] (1.000)
Step: 12699, Reward: [-470.205 -470.205 -470.205] [63.5141], Avg: [-538.762 -538.762 -538.762] (1.000)
Step: 12749, Reward: [-444.566 -444.566 -444.566] [78.6452], Avg: [-538.701 -538.701 -538.701] (1.000)
Step: 12799, Reward: [-440.443 -440.443 -440.443] [68.5516], Avg: [-538.585 -538.585 -538.585] (1.000)
Step: 12849, Reward: [-459.825 -459.825 -459.825] [65.2038], Avg: [-538.532 -538.532 -538.532] (1.000)
Step: 12899, Reward: [-453.079 -453.079 -453.079] [66.1609], Avg: [-538.458 -538.458 -538.458] (1.000)
Step: 12949, Reward: [-467.766 -467.766 -467.766] [83.6591], Avg: [-538.508 -538.508 -538.508] (1.000)
Step: 12999, Reward: [-467.119 -467.119 -467.119] [63.1165], Avg: [-538.476 -538.476 -538.476] (1.000)
Step: 13049, Reward: [-448.222 -448.222 -448.222] [74.6792], Avg: [-538.416 -538.416 -538.416] (1.000)
Step: 13099, Reward: [-476.694 -476.694 -476.694] [65.1978], Avg: [-538.429 -538.429 -538.429] (1.000)
Step: 13149, Reward: [-453.571 -453.571 -453.571] [85.5169], Avg: [-538.432 -538.432 -538.432] (1.000)
Step: 13199, Reward: [-460.833 -460.833 -460.833] [79.3914], Avg: [-538.439 -538.439 -538.439] (1.000)
Step: 13249, Reward: [-471.226 -471.226 -471.226] [83.2219], Avg: [-538.499 -538.499 -538.499] (1.000)
Step: 13299, Reward: [-441.634 -441.634 -441.634] [75.0587], Avg: [-538.417 -538.417 -538.417] (1.000)
Step: 13349, Reward: [-436.604 -436.604 -436.604] [66.8820], Avg: [-538.286 -538.286 -538.286] (1.000)
Step: 13399, Reward: [-438.35 -438.35 -438.35] [63.1242], Avg: [-538.149 -538.149 -538.149] (1.000)
Step: 13449, Reward: [-451.047 -451.047 -451.047] [67.9512], Avg: [-538.078 -538.078 -538.078] (1.000)
Step: 13499, Reward: [-454.754 -454.754 -454.754] [82.7474], Avg: [-538.076 -538.076 -538.076] (1.000)
Step: 13549, Reward: [-460.873 -460.873 -460.873] [91.1134], Avg: [-538.127 -538.127 -538.127] (1.000)
Step: 13599, Reward: [-447.811 -447.811 -447.811] [86.9269], Avg: [-538.115 -538.115 -538.115] (1.000)
Step: 13649, Reward: [-450.211 -450.211 -450.211] [78.7156], Avg: [-538.081 -538.081 -538.081] (1.000)
Step: 13699, Reward: [-447.308 -447.308 -447.308] [79.9319], Avg: [-538.041 -538.041 -538.041] (1.000)
Step: 13749, Reward: [-452.782 -452.782 -452.782] [71.7972], Avg: [-537.992 -537.992 -537.992] (1.000)
Step: 13799, Reward: [-433.744 -433.744 -433.744] [74.3131], Avg: [-537.884 -537.884 -537.884] (1.000)
Step: 13849, Reward: [-438.242 -438.242 -438.242] [79.5240], Avg: [-537.811 -537.811 -537.811] (1.000)
Step: 13899, Reward: [-450.929 -450.929 -450.929] [76.3396], Avg: [-537.773 -537.773 -537.773] (1.000)
Step: 13949, Reward: [-441.203 -441.203 -441.203] [87.8701], Avg: [-537.742 -537.742 -537.742] (1.000)
Step: 13999, Reward: [-441.428 -441.428 -441.428] [68.4516], Avg: [-537.643 -537.643 -537.643] (1.000)
Step: 14049, Reward: [-423.554 -423.554 -423.554] [89.9135], Avg: [-537.557 -537.557 -537.557] (1.000)
Step: 14099, Reward: [-460.849 -460.849 -460.849] [64.1684], Avg: [-537.512 -537.512 -537.512] (1.000)
Step: 14149, Reward: [-450.987 -450.987 -450.987] [73.3376], Avg: [-537.466 -537.466 -537.466] (1.000)
Step: 14199, Reward: [-450.527 -450.527 -450.527] [67.9428], Avg: [-537.399 -537.399 -537.399] (1.000)
Step: 14249, Reward: [-451.478 -451.478 -451.478] [72.8917], Avg: [-537.353 -537.353 -537.353] (1.000)
Step: 14299, Reward: [-457.22 -457.22 -457.22] [85.2739], Avg: [-537.371 -537.371 -537.371] (1.000)
Step: 14349, Reward: [-452.381 -452.381 -452.381] [77.8152], Avg: [-537.346 -537.346 -537.346] (1.000)
Step: 14399, Reward: [-439.507 -439.507 -439.507] [78.6055], Avg: [-537.279 -537.279 -537.279] (1.000)
Step: 14449, Reward: [-458.225 -458.225 -458.225] [70.4713], Avg: [-537.249 -537.249 -537.249] (1.000)
Step: 14499, Reward: [-427.415 -427.415 -427.415] [67.5033], Avg: [-537.103 -537.103 -537.103] (1.000)
Step: 14549, Reward: [-452.906 -452.906 -452.906] [80.7190], Avg: [-537.092 -537.092 -537.092] (1.000)
Step: 14599, Reward: [-462.087 -462.087 -462.087] [66.5696], Avg: [-537.063 -537.063 -537.063] (1.000)
Step: 14649, Reward: [-458.964 -458.964 -458.964] [64.8736], Avg: [-537.018 -537.018 -537.018] (1.000)
Step: 14699, Reward: [-446.513 -446.513 -446.513] [60.4534], Avg: [-536.915 -536.915 -536.915] (1.000)
Step: 14749, Reward: [-452.499 -452.499 -452.499] [71.0399], Avg: [-536.87 -536.87 -536.87] (1.000)
Step: 14799, Reward: [-446.759 -446.759 -446.759] [83.7273], Avg: [-536.848 -536.848 -536.848] (1.000)
Step: 14849, Reward: [-439.905 -439.905 -439.905] [62.5941], Avg: [-536.733 -536.733 -536.733] (1.000)
Step: 14899, Reward: [-439.274 -439.274 -439.274] [80.9359], Avg: [-536.677 -536.677 -536.677] (1.000)
Step: 14949, Reward: [-437.951 -437.951 -437.951] [77.9820], Avg: [-536.608 -536.608 -536.608] (1.000)
Step: 14999, Reward: [-459.359 -459.359 -459.359] [65.5135], Avg: [-536.569 -536.569 -536.569] (1.000)
Step: 15049, Reward: [-453.908 -453.908 -453.908] [71.2600], Avg: [-536.531 -536.531 -536.531] (1.000)
Step: 15099, Reward: [-442.012 -442.012 -442.012] [75.8015], Avg: [-536.469 -536.469 -536.469] (1.000)
Step: 15149, Reward: [-480.83 -480.83 -480.83] [66.8174], Avg: [-536.506 -536.506 -536.506] (1.000)
Step: 15199, Reward: [-423.381 -423.381 -423.381] [70.1648], Avg: [-536.365 -536.365 -536.365] (1.000)
Step: 15249, Reward: [-441.999 -441.999 -441.999] [76.3581], Avg: [-536.305 -536.305 -536.305] (1.000)
Step: 15299, Reward: [-437.097 -437.097 -437.097] [67.7056], Avg: [-536.203 -536.203 -536.203] (1.000)
Step: 15349, Reward: [-432.189 -432.189 -432.189] [70.3271], Avg: [-536.093 -536.093 -536.093] (1.000)
Step: 15399, Reward: [-403.096 -403.096 -403.096] [78.6170], Avg: [-535.916 -535.916 -535.916] (1.000)
Step: 15449, Reward: [-422.036 -422.036 -422.036] [62.7397], Avg: [-535.751 -535.751 -535.751] (1.000)
Step: 15499, Reward: [-410.69 -410.69 -410.69] [73.1095], Avg: [-535.583 -535.583 -535.583] (1.000)
Step: 15549, Reward: [-421.975 -421.975 -421.975] [74.2527], Avg: [-535.457 -535.457 -535.457] (1.000)
Step: 15599, Reward: [-426.736 -426.736 -426.736] [81.5250], Avg: [-535.369 -535.369 -535.369] (1.000)
Step: 15649, Reward: [-424.65 -424.65 -424.65] [79.0342], Avg: [-535.268 -535.268 -535.268] (1.000)
Step: 15699, Reward: [-446.88 -446.88 -446.88] [69.3791], Avg: [-535.208 -535.208 -535.208] (1.000)
Step: 15749, Reward: [-412.737 -412.737 -412.737] [59.3596], Avg: [-535.007 -535.007 -535.007] (1.000)
Step: 15799, Reward: [-409.278 -409.278 -409.278] [60.4894], Avg: [-534.801 -534.801 -534.801] (1.000)
Step: 15849, Reward: [-430.889 -430.889 -430.889] [65.9957], Avg: [-534.681 -534.681 -534.681] (1.000)
Step: 15899, Reward: [-447.071 -447.071 -447.071] [77.9921], Avg: [-534.651 -534.651 -534.651] (1.000)
Step: 15949, Reward: [-426.259 -426.259 -426.259] [55.2768], Avg: [-534.484 -534.484 -534.484] (1.000)
Step: 15999, Reward: [-432.94 -432.94 -432.94] [86.1010], Avg: [-534.436 -534.436 -534.436] (1.000)
Step: 16049, Reward: [-429.162 -429.162 -429.162] [61.6020], Avg: [-534.3 -534.3 -534.3] (1.000)
Step: 16099, Reward: [-414.004 -414.004 -414.004] [68.4530], Avg: [-534.139 -534.139 -534.139] (1.000)
Step: 16149, Reward: [-427.604 -427.604 -427.604] [54.9161], Avg: [-533.979 -533.979 -533.979] (1.000)
Step: 16199, Reward: [-422.129 -422.129 -422.129] [66.2593], Avg: [-533.839 -533.839 -533.839] (1.000)
Step: 16249, Reward: [-429.567 -429.567 -429.567] [73.9479], Avg: [-533.745 -533.745 -533.745] (1.000)
Step: 16299, Reward: [-428.571 -428.571 -428.571] [68.9563], Avg: [-533.634 -533.634 -533.634] (1.000)
Step: 16349, Reward: [-446.959 -446.959 -446.959] [69.8220], Avg: [-533.583 -533.583 -533.583] (1.000)
Step: 16399, Reward: [-395.267 -395.267 -395.267] [64.6150], Avg: [-533.358 -533.358 -533.358] (1.000)
Step: 16449, Reward: [-429.032 -429.032 -429.032] [65.8326], Avg: [-533.241 -533.241 -533.241] (1.000)
Step: 16499, Reward: [-449.242 -449.242 -449.242] [71.2492], Avg: [-533.202 -533.202 -533.202] (1.000)
Step: 16549, Reward: [-441.579 -441.579 -441.579] [65.1827], Avg: [-533.122 -533.122 -533.122] (1.000)
Step: 16599, Reward: [-436.205 -436.205 -436.205] [75.3408], Avg: [-533.057 -533.057 -533.057] (1.000)
Step: 16649, Reward: [-439.979 -439.979 -439.979] [69.4369], Avg: [-532.987 -532.987 -532.987] (1.000)
Step: 16699, Reward: [-424.847 -424.847 -424.847] [56.4991], Avg: [-532.832 -532.832 -532.832] (1.000)
Step: 16749, Reward: [-435.509 -435.509 -435.509] [72.4584], Avg: [-532.758 -532.758 -532.758] (1.000)
Step: 16799, Reward: [-425.869 -425.869 -425.869] [72.7197], Avg: [-532.656 -532.656 -532.656] (1.000)
Step: 16849, Reward: [-438.89 -438.89 -438.89] [65.7437], Avg: [-532.573 -532.573 -532.573] (1.000)
Step: 16899, Reward: [-435.865 -435.865 -435.865] [70.2299], Avg: [-532.494 -532.494 -532.494] (1.000)
Step: 16949, Reward: [-428.521 -428.521 -428.521] [64.1034], Avg: [-532.377 -532.377 -532.377] (1.000)
Step: 16999, Reward: [-439.667 -439.667 -439.667] [89.6500], Avg: [-532.368 -532.368 -532.368] (1.000)
Step: 17049, Reward: [-439.616 -439.616 -439.616] [65.0985], Avg: [-532.287 -532.287 -532.287] (1.000)
Step: 17099, Reward: [-425.811 -425.811 -425.811] [81.8660], Avg: [-532.215 -532.215 -532.215] (1.000)
Step: 17149, Reward: [-443.444 -443.444 -443.444] [83.7285], Avg: [-532.2 -532.2 -532.2] (1.000)
Step: 17199, Reward: [-459.852 -459.852 -459.852] [75.1766], Avg: [-532.208 -532.208 -532.208] (1.000)
Step: 17249, Reward: [-448.263 -448.263 -448.263] [76.5512], Avg: [-532.187 -532.187 -532.187] (1.000)
Step: 17299, Reward: [-466.338 -466.338 -466.338] [60.8025], Avg: [-532.172 -532.172 -532.172] (1.000)
Step: 17349, Reward: [-474.22 -474.22 -474.22] [75.0786], Avg: [-532.222 -532.222 -532.222] (1.000)
Step: 17399, Reward: [-459.15 -459.15 -459.15] [62.8667], Avg: [-532.192 -532.192 -532.192] (1.000)
Step: 17449, Reward: [-483.816 -483.816 -483.816] [86.4165], Avg: [-532.301 -532.301 -532.301] (1.000)
Step: 17499, Reward: [-474.373 -474.373 -474.373] [67.3649], Avg: [-532.328 -532.328 -532.328] (1.000)
Step: 17549, Reward: [-464.338 -464.338 -464.338] [88.2701], Avg: [-532.386 -532.386 -532.386] (1.000)
Step: 17599, Reward: [-494.25 -494.25 -494.25] [98.4274], Avg: [-532.557 -532.557 -532.557] (1.000)
Step: 17649, Reward: [-508.483 -508.483 -508.483] [80.8991], Avg: [-532.718 -532.718 -532.718] (1.000)
Step: 17699, Reward: [-491.145 -491.145 -491.145] [86.8958], Avg: [-532.846 -532.846 -532.846] (1.000)
Step: 17749, Reward: [-511.089 -511.089 -511.089] [81.5594], Avg: [-533.015 -533.015 -533.015] (1.000)
Step: 17799, Reward: [-512.227 -512.227 -512.227] [105.5726], Avg: [-533.253 -533.253 -533.253] (1.000)
Step: 17849, Reward: [-498.776 -498.776 -498.776] [80.3627], Avg: [-533.382 -533.382 -533.382] (1.000)
Step: 17899, Reward: [-451.924 -451.924 -451.924] [84.5892], Avg: [-533.39 -533.39 -533.39] (1.000)
Step: 17949, Reward: [-519.358 -519.358 -519.358] [88.3542], Avg: [-533.597 -533.597 -533.597] (1.000)
Step: 17999, Reward: [-524.737 -524.737 -524.737] [80.0578], Avg: [-533.795 -533.795 -533.795] (1.000)
Step: 18049, Reward: [-505.257 -505.257 -505.257] [83.8043], Avg: [-533.948 -533.948 -533.948] (1.000)
Step: 18099, Reward: [-511.017 -511.017 -511.017] [83.3259], Avg: [-534.115 -534.115 -534.115] (1.000)
Step: 18149, Reward: [-525.815 -525.815 -525.815] [80.8523], Avg: [-534.315 -534.315 -534.315] (1.000)
Step: 18199, Reward: [-531.21 -531.21 -531.21] [87.0338], Avg: [-534.545 -534.545 -534.545] (1.000)
Step: 18249, Reward: [-501.729 -501.729 -501.729] [81.8490], Avg: [-534.68 -534.68 -534.68] (1.000)
Step: 18299, Reward: [-470.933 -470.933 -470.933] [70.6421], Avg: [-534.699 -534.699 -534.699] (1.000)
Step: 18349, Reward: [-496.653 -496.653 -496.653] [90.0496], Avg: [-534.84 -534.84 -534.84] (1.000)
Step: 18399, Reward: [-510.464 -510.464 -510.464] [97.2166], Avg: [-535.038 -535.038 -535.038] (1.000)
Step: 18449, Reward: [-512.971 -512.971 -512.971] [89.1475], Avg: [-535.22 -535.22 -535.22] (1.000)
Step: 18499, Reward: [-543.923 -543.923 -543.923] [93.7540], Avg: [-535.497 -535.497 -535.497] (1.000)
Step: 18549, Reward: [-517.166 -517.166 -517.166] [88.2414], Avg: [-535.685 -535.685 -535.685] (1.000)
Step: 18599, Reward: [-524.491 -524.491 -524.491] [104.2311], Avg: [-535.935 -535.935 -535.935] (1.000)
Step: 18649, Reward: [-554.703 -554.703 -554.703] [80.0323], Avg: [-536.2 -536.2 -536.2] (1.000)
Step: 18699, Reward: [-547.84 -547.84 -547.84] [89.3483], Avg: [-536.47 -536.47 -536.47] (1.000)
Step: 18749, Reward: [-514.573 -514.573 -514.573] [74.6284], Avg: [-536.611 -536.611 -536.611] (1.000)
Step: 18799, Reward: [-543.808 -543.808 -543.808] [87.6782], Avg: [-536.863 -536.863 -536.863] (1.000)
Step: 18849, Reward: [-517.183 -517.183 -517.183] [64.0919], Avg: [-536.981 -536.981 -536.981] (1.000)
Step: 18899, Reward: [-527.708 -527.708 -527.708] [98.1042], Avg: [-537.216 -537.216 -537.216] (1.000)
Step: 18949, Reward: [-527.341 -527.341 -527.341] [96.5505], Avg: [-537.445 -537.445 -537.445] (1.000)
Step: 18999, Reward: [-522.085 -522.085 -522.085] [89.1195], Avg: [-537.639 -537.639 -537.639] (1.000)
Step: 19049, Reward: [-502.383 -502.383 -502.383] [90.3141], Avg: [-537.783 -537.783 -537.783] (1.000)
Step: 19099, Reward: [-519.121 -519.121 -519.121] [89.8107], Avg: [-537.97 -537.97 -537.97] (1.000)
Step: 19149, Reward: [-522.14 -522.14 -522.14] [74.5105], Avg: [-538.123 -538.123 -538.123] (1.000)
Step: 19199, Reward: [-507.244 -507.244 -507.244] [90.6067], Avg: [-538.278 -538.278 -538.278] (1.000)
Step: 19249, Reward: [-488.64 -488.64 -488.64] [72.7134], Avg: [-538.338 -538.338 -538.338] (1.000)
Step: 19299, Reward: [-541.333 -541.333 -541.333] [73.0601], Avg: [-538.535 -538.535 -538.535] (1.000)
Step: 19349, Reward: [-532.532 -532.532 -532.532] [85.0040], Avg: [-538.74 -538.74 -538.74] (1.000)
Step: 19399, Reward: [-514.83 -514.83 -514.83] [86.6466], Avg: [-538.901 -538.901 -538.901] (1.000)
Step: 19449, Reward: [-534.729 -534.729 -534.729] [78.3301], Avg: [-539.092 -539.092 -539.092] (1.000)
Step: 19499, Reward: [-574.694 -574.694 -574.694] [118.7761], Avg: [-539.488 -539.488 -539.488] (1.000)
Step: 19549, Reward: [-593.906 -593.906 -593.906] [103.8434], Avg: [-539.892 -539.892 -539.892] (1.000)
Step: 19599, Reward: [-610.513 -610.513 -610.513] [109.1890], Avg: [-540.351 -540.351 -540.351] (1.000)
Step: 19649, Reward: [-552.003 -552.003 -552.003] [85.3153], Avg: [-540.598 -540.598 -540.598] (1.000)
Step: 19699, Reward: [-599.95 -599.95 -599.95] [88.4016], Avg: [-540.973 -540.973 -540.973] (1.000)
Step: 19749, Reward: [-551.257 -551.257 -551.257] [76.7128], Avg: [-541.193 -541.193 -541.193] (1.000)
Step: 19799, Reward: [-548.712 -548.712 -548.712] [78.9806], Avg: [-541.412 -541.412 -541.412] (1.000)
Step: 19849, Reward: [-577.719 -577.719 -577.719] [115.7724], Avg: [-541.795 -541.795 -541.795] (1.000)
Step: 19899, Reward: [-548.635 -548.635 -548.635] [63.0341], Avg: [-541.97 -541.97 -541.97] (1.000)
Step: 19949, Reward: [-543.897 -543.897 -543.897] [77.6435], Avg: [-542.17 -542.17 -542.17] (1.000)
Step: 19999, Reward: [-554.92 -554.92 -554.92] [100.2543], Avg: [-542.452 -542.452 -542.452] (1.000)
Step: 20049, Reward: [-566.679 -566.679 -566.679] [93.7499], Avg: [-542.746 -542.746 -542.746] (1.000)
Step: 20099, Reward: [-578.228 -578.228 -578.228] [91.1594], Avg: [-543.061 -543.061 -543.061] (1.000)
Step: 20149, Reward: [-555.203 -555.203 -555.203] [93.9461], Avg: [-543.325 -543.325 -543.325] (1.000)
Step: 20199, Reward: [-577.069 -577.069 -577.069] [93.1097], Avg: [-543.639 -543.639 -543.639] (1.000)
Step: 20249, Reward: [-561.414 -561.414 -561.414] [89.7783], Avg: [-543.904 -543.904 -543.904] (1.000)
Step: 20299, Reward: [-585.773 -585.773 -585.773] [94.3564], Avg: [-544.24 -544.24 -544.24] (1.000)
Step: 20349, Reward: [-601.868 -601.868 -601.868] [88.5690], Avg: [-544.599 -544.599 -544.599] (1.000)
Step: 20399, Reward: [-630.759 -630.759 -630.759] [111.7305], Avg: [-545.084 -545.084 -545.084] (1.000)
Step: 20449, Reward: [-572.69 -572.69 -572.69] [79.1513], Avg: [-545.345 -545.345 -545.345] (1.000)
Step: 20499, Reward: [-591.465 -591.465 -591.465] [78.2208], Avg: [-545.648 -545.648 -545.648] (1.000)
Step: 20549, Reward: [-600.518 -600.518 -600.518] [98.7034], Avg: [-546.022 -546.022 -546.022] (1.000)
Step: 20599, Reward: [-585.858 -585.858 -585.858] [90.7440], Avg: [-546.339 -546.339 -546.339] (1.000)
Step: 20649, Reward: [-578.906 -578.906 -578.906] [84.0715], Avg: [-546.621 -546.621 -546.621] (1.000)
Step: 20699, Reward: [-576.677 -576.677 -576.677] [103.6195], Avg: [-546.944 -546.944 -546.944] (1.000)
Step: 20749, Reward: [-582.115 -582.115 -582.115] [98.4637], Avg: [-547.266 -547.266 -547.266] (1.000)
Step: 20799, Reward: [-573.789 -573.789 -573.789] [84.1481], Avg: [-547.532 -547.532 -547.532] (1.000)
Step: 20849, Reward: [-564.744 -564.744 -564.744] [97.5426], Avg: [-547.807 -547.807 -547.807] (1.000)
Step: 20899, Reward: [-541.152 -541.152 -541.152] [112.5391], Avg: [-548.061 -548.061 -548.061] (1.000)
Step: 20949, Reward: [-544.384 -544.384 -544.384] [86.0411], Avg: [-548.257 -548.257 -548.257] (1.000)
Step: 20999, Reward: [-534.057 -534.057 -534.057] [86.0174], Avg: [-548.428 -548.428 -548.428] (1.000)
Step: 21049, Reward: [-538.955 -538.955 -538.955] [95.6458], Avg: [-548.633 -548.633 -548.633] (1.000)
Step: 21099, Reward: [-534.691 -534.691 -534.691] [103.2976], Avg: [-548.845 -548.845 -548.845] (1.000)
Step: 21149, Reward: [-536.577 -536.577 -536.577] [81.8800], Avg: [-549.009 -549.009 -549.009] (1.000)
Step: 21199, Reward: [-528.99 -528.99 -528.99] [103.9947], Avg: [-549.207 -549.207 -549.207] (1.000)
Step: 21249, Reward: [-516.906 -516.906 -516.906] [108.4556], Avg: [-549.387 -549.387 -549.387] (1.000)
Step: 21299, Reward: [-578.668 -578.668 -578.668] [89.0280], Avg: [-549.664 -549.664 -549.664] (1.000)
Step: 21349, Reward: [-518.437 -518.437 -518.437] [95.3727], Avg: [-549.814 -549.814 -549.814] (1.000)
Step: 21399, Reward: [-541.443 -541.443 -541.443] [60.0858], Avg: [-549.935 -549.935 -549.935] (1.000)
Step: 21449, Reward: [-577.94 -577.94 -577.94] [107.1422], Avg: [-550.25 -550.25 -550.25] (1.000)
Step: 21499, Reward: [-571.289 -571.289 -571.289] [78.1408], Avg: [-550.481 -550.481 -550.481] (1.000)
Step: 21549, Reward: [-580.977 -580.977 -580.977] [84.6067], Avg: [-550.748 -550.748 -550.748] (1.000)
Step: 21599, Reward: [-575.681 -575.681 -575.681] [88.6443], Avg: [-551.011 -551.011 -551.011] (1.000)
Step: 21649, Reward: [-583.867 -583.867 -583.867] [99.7092], Avg: [-551.317 -551.317 -551.317] (1.000)
Step: 21699, Reward: [-569.404 -569.404 -569.404] [98.1369], Avg: [-551.585 -551.585 -551.585] (1.000)
Step: 21749, Reward: [-551.248 -551.248 -551.248] [102.7938], Avg: [-551.82 -551.82 -551.82] (1.000)
Step: 21799, Reward: [-607.32 -607.32 -607.32] [127.2662], Avg: [-552.24 -552.24 -552.24] (1.000)
Step: 21849, Reward: [-529.357 -529.357 -529.357] [74.4866], Avg: [-552.358 -552.358 -552.358] (1.000)
Step: 21899, Reward: [-527.114 -527.114 -527.114] [87.2082], Avg: [-552.499 -552.499 -552.499] (1.000)
Step: 21949, Reward: [-546.539 -546.539 -546.539] [100.0786], Avg: [-552.714 -552.714 -552.714] (1.000)
Step: 21999, Reward: [-540.893 -540.893 -540.893] [111.7176], Avg: [-552.941 -552.941 -552.941] (1.000)
Step: 22049, Reward: [-547.068 -547.068 -547.068] [114.8399], Avg: [-553.188 -553.188 -553.188] (1.000)
Step: 22099, Reward: [-587.275 -587.275 -587.275] [139.7268], Avg: [-553.581 -553.581 -553.581] (1.000)
Step: 22149, Reward: [-555.161 -555.161 -555.161] [107.7520], Avg: [-553.828 -553.828 -553.828] (1.000)
Step: 22199, Reward: [-566.326 -566.326 -566.326] [126.2359], Avg: [-554.14 -554.14 -554.14] (1.000)
Step: 22249, Reward: [-533.873 -533.873 -533.873] [88.4048], Avg: [-554.293 -554.293 -554.293] (1.000)
Step: 22299, Reward: [-542.635 -542.635 -542.635] [82.1759], Avg: [-554.451 -554.451 -554.451] (1.000)
Step: 22349, Reward: [-555.432 -555.432 -555.432] [107.2718], Avg: [-554.694 -554.694 -554.694] (1.000)
Step: 22399, Reward: [-547.614 -547.614 -547.614] [154.0485], Avg: [-555.022 -555.022 -555.022] (1.000)
Step: 22449, Reward: [-550.945 -550.945 -550.945] [111.3082], Avg: [-555.261 -555.261 -555.261] (1.000)
Step: 22499, Reward: [-547.776 -547.776 -547.776] [117.2997], Avg: [-555.505 -555.505 -555.505] (1.000)
Step: 22549, Reward: [-530.019 -530.019 -530.019] [113.4218], Avg: [-555.7 -555.7 -555.7] (1.000)
Step: 22599, Reward: [-549.402 -549.402 -549.402] [118.2141], Avg: [-555.947 -555.947 -555.947] (1.000)
Step: 22649, Reward: [-517.654 -517.654 -517.654] [110.2058], Avg: [-556.106 -556.106 -556.106] (1.000)
Step: 22699, Reward: [-520.86 -520.86 -520.86] [106.7362], Avg: [-556.263 -556.263 -556.263] (1.000)
Step: 22749, Reward: [-514.939 -514.939 -514.939] [120.9906], Avg: [-556.438 -556.438 -556.438] (1.000)
Step: 22799, Reward: [-527.226 -527.226 -527.226] [95.3828], Avg: [-556.584 -556.584 -556.584] (1.000)
Step: 22849, Reward: [-523.756 -523.756 -523.756] [88.7133], Avg: [-556.706 -556.706 -556.706] (1.000)
Step: 22899, Reward: [-531.554 -531.554 -531.554] [114.6599], Avg: [-556.901 -556.901 -556.901] (1.000)
Step: 22949, Reward: [-530.43 -530.43 -530.43] [105.7286], Avg: [-557.074 -557.074 -557.074] (1.000)
Step: 22999, Reward: [-490.885 -490.885 -490.885] [80.7859], Avg: [-557.106 -557.106 -557.106] (1.000)
Step: 23049, Reward: [-508.038 -508.038 -508.038] [96.4848], Avg: [-557.209 -557.209 -557.209] (1.000)
Step: 23099, Reward: [-508.472 -508.472 -508.472] [61.9910], Avg: [-557.237 -557.237 -557.237] (1.000)
Step: 23149, Reward: [-478.992 -478.992 -478.992] [89.3175], Avg: [-557.261 -557.261 -557.261] (1.000)
Step: 23199, Reward: [-474.56 -474.56 -474.56] [71.0701], Avg: [-557.236 -557.236 -557.236] (1.000)
Step: 23249, Reward: [-487.865 -487.865 -487.865] [94.1125], Avg: [-557.289 -557.289 -557.289] (1.000)
Step: 23299, Reward: [-497.482 -497.482 -497.482] [117.1385], Avg: [-557.412 -557.412 -557.412] (1.000)
Step: 23349, Reward: [-482.702 -482.702 -482.702] [76.9002], Avg: [-557.417 -557.417 -557.417] (1.000)
Step: 23399, Reward: [-449.165 -449.165 -449.165] [76.4659], Avg: [-557.349 -557.349 -557.349] (1.000)
Step: 23449, Reward: [-426.48 -426.48 -426.48] [74.5830], Avg: [-557.229 -557.229 -557.229] (1.000)
Step: 23499, Reward: [-481.482 -481.482 -481.482] [97.8592], Avg: [-557.276 -557.276 -557.276] (1.000)
Step: 23549, Reward: [-448.013 -448.013 -448.013] [64.9515], Avg: [-557.182 -557.182 -557.182] (1.000)
Step: 23599, Reward: [-460.368 -460.368 -460.368] [81.4580], Avg: [-557.149 -557.149 -557.149] (1.000)
Step: 23649, Reward: [-472.054 -472.054 -472.054] [69.3914], Avg: [-557.116 -557.116 -557.116] (1.000)
Step: 23699, Reward: [-467.764 -467.764 -467.764] [81.3492], Avg: [-557.099 -557.099 -557.099] (1.000)
Step: 23749, Reward: [-462.949 -462.949 -462.949] [82.6940], Avg: [-557.075 -557.075 -557.075] (1.000)
Step: 23799, Reward: [-447.013 -447.013 -447.013] [95.3295], Avg: [-557.044 -557.044 -557.044] (1.000)
Step: 23849, Reward: [-490.967 -490.967 -490.967] [100.9910], Avg: [-557.118 -557.118 -557.118] (1.000)
Step: 23899, Reward: [-442.227 -442.227 -442.227] [69.3352], Avg: [-557.022 -557.022 -557.022] (1.000)
Step: 23949, Reward: [-481.333 -481.333 -481.333] [84.1798], Avg: [-557.04 -557.04 -557.04] (1.000)
Step: 23999, Reward: [-464.782 -464.782 -464.782] [78.6054], Avg: [-557.012 -557.012 -557.012] (1.000)
Step: 24049, Reward: [-502.557 -502.557 -502.557] [92.7991], Avg: [-557.091 -557.091 -557.091] (1.000)
Step: 24099, Reward: [-480.659 -480.659 -480.659] [86.8209], Avg: [-557.113 -557.113 -557.113] (1.000)
Step: 24149, Reward: [-496.282 -496.282 -496.282] [113.2869], Avg: [-557.221 -557.221 -557.221] (1.000)
Step: 24199, Reward: [-447.963 -447.963 -447.963] [85.9435], Avg: [-557.173 -557.173 -557.173] (1.000)
Step: 24249, Reward: [-452.329 -452.329 -452.329] [77.3778], Avg: [-557.117 -557.117 -557.117] (1.000)
Step: 24299, Reward: [-464.822 -464.822 -464.822] [91.5217], Avg: [-557.115 -557.115 -557.115] (1.000)
Step: 24349, Reward: [-485.354 -485.354 -485.354] [89.8120], Avg: [-557.152 -557.152 -557.152] (1.000)
Step: 24399, Reward: [-478.755 -478.755 -478.755] [88.6211], Avg: [-557.173 -557.173 -557.173] (1.000)
Step: 24449, Reward: [-481.185 -481.185 -481.185] [67.7660], Avg: [-557.156 -557.156 -557.156] (1.000)
Step: 24499, Reward: [-487.27 -487.27 -487.27] [92.0653], Avg: [-557.201 -557.201 -557.201] (1.000)
Step: 24549, Reward: [-507.054 -507.054 -507.054] [138.4167], Avg: [-557.381 -557.381 -557.381] (1.000)
Step: 24599, Reward: [-479.167 -479.167 -479.167] [100.2339], Avg: [-557.426 -557.426 -557.426] (1.000)
Step: 24649, Reward: [-479.226 -479.226 -479.226] [91.3375], Avg: [-557.453 -557.453 -557.453] (1.000)
Step: 24699, Reward: [-492.638 -492.638 -492.638] [88.1103], Avg: [-557.5 -557.5 -557.5] (1.000)
Step: 24749, Reward: [-450.417 -450.417 -450.417] [64.6600], Avg: [-557.414 -557.414 -557.414] (1.000)
Step: 24799, Reward: [-452.881 -452.881 -452.881] [89.7919], Avg: [-557.384 -557.384 -557.384] (1.000)
Step: 24849, Reward: [-451.928 -451.928 -451.928] [86.2191], Avg: [-557.346 -557.346 -557.346] (1.000)
Step: 24899, Reward: [-447.633 -447.633 -447.633] [65.2601], Avg: [-557.256 -557.256 -557.256] (1.000)
Step: 24949, Reward: [-436.486 -436.486 -436.486] [71.8908], Avg: [-557.158 -557.158 -557.158] (1.000)
Step: 24999, Reward: [-430.823 -430.823 -430.823] [73.4313], Avg: [-557.053 -557.053 -557.053] (1.000)
Step: 25049, Reward: [-445.941 -445.941 -445.941] [69.6480], Avg: [-556.97 -556.97 -556.97] (1.000)
Step: 25099, Reward: [-446.865 -446.865 -446.865] [76.8728], Avg: [-556.904 -556.904 -556.904] (1.000)
Step: 25149, Reward: [-460.953 -460.953 -460.953] [87.9545], Avg: [-556.888 -556.888 -556.888] (1.000)
Step: 25199, Reward: [-450.472 -450.472 -450.472] [80.3448], Avg: [-556.836 -556.836 -556.836] (1.000)
Step: 25249, Reward: [-466.831 -466.831 -466.831] [93.1236], Avg: [-556.842 -556.842 -556.842] (1.000)
Step: 25299, Reward: [-425.127 -425.127 -425.127] [76.7407], Avg: [-556.734 -556.734 -556.734] (1.000)
Step: 25349, Reward: [-444.498 -444.498 -444.498] [78.8839], Avg: [-556.668 -556.668 -556.668] (1.000)
Step: 25399, Reward: [-454.259 -454.259 -454.259] [61.0969], Avg: [-556.586 -556.586 -556.586] (1.000)
Step: 25449, Reward: [-433.922 -433.922 -433.922] [60.7322], Avg: [-556.465 -556.465 -556.465] (1.000)
Step: 25499, Reward: [-462.452 -462.452 -462.452] [83.8689], Avg: [-556.445 -556.445 -556.445] (1.000)
Step: 25549, Reward: [-451.931 -451.931 -451.931] [77.0446], Avg: [-556.391 -556.391 -556.391] (1.000)
Step: 25599, Reward: [-445.465 -445.465 -445.465] [79.4963], Avg: [-556.33 -556.33 -556.33] (1.000)
Step: 25649, Reward: [-444.293 -444.293 -444.293] [62.1239], Avg: [-556.232 -556.232 -556.232] (1.000)
Step: 25699, Reward: [-455.191 -455.191 -455.191] [87.2648], Avg: [-556.206 -556.206 -556.206] (1.000)
Step: 25749, Reward: [-432.02 -432.02 -432.02] [77.7939], Avg: [-556.116 -556.116 -556.116] (1.000)
Step: 25799, Reward: [-442.881 -442.881 -442.881] [77.3421], Avg: [-556.046 -556.046 -556.046] (1.000)
Step: 25849, Reward: [-435.902 -435.902 -435.902] [87.5744], Avg: [-555.983 -555.983 -555.983] (1.000)
Step: 25899, Reward: [-420.058 -420.058 -420.058] [72.5442], Avg: [-555.861 -555.861 -555.861] (1.000)
Step: 25949, Reward: [-445.182 -445.182 -445.182] [71.5926], Avg: [-555.785 -555.785 -555.785] (1.000)
Step: 25999, Reward: [-416.31 -416.31 -416.31] [63.9803], Avg: [-555.64 -555.64 -555.64] (1.000)
Step: 26049, Reward: [-438.688 -438.688 -438.688] [87.8147], Avg: [-555.584 -555.584 -555.584] (1.000)
Step: 26099, Reward: [-408.292 -408.292 -408.292] [80.0200], Avg: [-555.455 -555.455 -555.455] (1.000)
Step: 26149, Reward: [-430.896 -430.896 -430.896] [80.5762], Avg: [-555.371 -555.371 -555.371] (1.000)
Step: 26199, Reward: [-434.194 -434.194 -434.194] [80.5067], Avg: [-555.294 -555.294 -555.294] (1.000)
Step: 26249, Reward: [-399.515 -399.515 -399.515] [76.2308], Avg: [-555.142 -555.142 -555.142] (1.000)
Step: 26299, Reward: [-429.179 -429.179 -429.179] [79.1157], Avg: [-555.053 -555.053 -555.053] (1.000)
Step: 26349, Reward: [-426.654 -426.654 -426.654] [81.5143], Avg: [-554.964 -554.964 -554.964] (1.000)
Step: 26399, Reward: [-428.022 -428.022 -428.022] [98.7939], Avg: [-554.911 -554.911 -554.911] (1.000)
Step: 26449, Reward: [-439.867 -439.867 -439.867] [92.0049], Avg: [-554.867 -554.867 -554.867] (1.000)
Step: 26499, Reward: [-442.388 -442.388 -442.388] [63.4324], Avg: [-554.775 -554.775 -554.775] (1.000)
Step: 26549, Reward: [-443.364 -443.364 -443.364] [89.0627], Avg: [-554.733 -554.733 -554.733] (1.000)
Step: 26599, Reward: [-421.529 -421.529 -421.529] [80.6417], Avg: [-554.634 -554.634 -554.634] (1.000)
Step: 26649, Reward: [-426.047 -426.047 -426.047] [87.7727], Avg: [-554.557 -554.557 -554.557] (1.000)
Step: 26699, Reward: [-469.61 -469.61 -469.61] [86.4465], Avg: [-554.56 -554.56 -554.56] (1.000)
Step: 26749, Reward: [-482.903 -482.903 -482.903] [107.5205], Avg: [-554.627 -554.627 -554.627] (1.000)
Step: 26799, Reward: [-451.814 -451.814 -451.814] [97.8752], Avg: [-554.618 -554.618 -554.618] (1.000)
Step: 26849, Reward: [-436.681 -436.681 -436.681] [65.1684], Avg: [-554.52 -554.52 -554.52] (1.000)
Step: 26899, Reward: [-468.875 -468.875 -468.875] [70.1457], Avg: [-554.491 -554.491 -554.491] (1.000)
Step: 26949, Reward: [-445.065 -445.065 -445.065] [79.5383], Avg: [-554.435 -554.435 -554.435] (1.000)
Step: 26999, Reward: [-471.139 -471.139 -471.139] [85.7173], Avg: [-554.44 -554.44 -554.44] (1.000)
Step: 27049, Reward: [-453.46 -453.46 -453.46] [74.0551], Avg: [-554.39 -554.39 -554.39] (1.000)
Step: 27099, Reward: [-428.609 -428.609 -428.609] [75.3274], Avg: [-554.297 -554.297 -554.297] (1.000)
Step: 27149, Reward: [-412.342 -412.342 -412.342] [70.3220], Avg: [-554.165 -554.165 -554.165] (1.000)
Step: 27199, Reward: [-449.905 -449.905 -449.905] [70.8061], Avg: [-554.104 -554.104 -554.104] (1.000)
Step: 27249, Reward: [-438.286 -438.286 -438.286] [96.9754], Avg: [-554.069 -554.069 -554.069] (1.000)
Step: 27299, Reward: [-432.882 -432.882 -432.882] [67.8291], Avg: [-553.971 -553.971 -553.971] (1.000)
Step: 27349, Reward: [-430.01 -430.01 -430.01] [90.7911], Avg: [-553.911 -553.911 -553.911] (1.000)
Step: 27399, Reward: [-439.429 -439.429 -439.429] [83.2882], Avg: [-553.854 -553.854 -553.854] (1.000)
Step: 27449, Reward: [-412.179 -412.179 -412.179] [73.3047], Avg: [-553.729 -553.729 -553.729] (1.000)
Step: 27499, Reward: [-433.166 -433.166 -433.166] [74.5121], Avg: [-553.645 -553.645 -553.645] (1.000)
Step: 27549, Reward: [-437.786 -437.786 -437.786] [84.7883], Avg: [-553.589 -553.589 -553.589] (1.000)
Step: 27599, Reward: [-420.394 -420.394 -420.394] [78.1977], Avg: [-553.489 -553.489 -553.489] (1.000)
Step: 27649, Reward: [-445.294 -445.294 -445.294] [92.4854], Avg: [-553.461 -553.461 -553.461] (1.000)
Step: 27699, Reward: [-426.511 -426.511 -426.511] [89.5352], Avg: [-553.393 -553.393 -553.393] (1.000)
Step: 27749, Reward: [-452.51 -452.51 -452.51] [86.6608], Avg: [-553.368 -553.368 -553.368] (1.000)
Step: 27799, Reward: [-443.527 -443.527 -443.527] [81.9293], Avg: [-553.318 -553.318 -553.318] (1.000)
Step: 27849, Reward: [-436.437 -436.437 -436.437] [86.7921], Avg: [-553.264 -553.264 -553.264] (1.000)
Step: 27899, Reward: [-472.262 -472.262 -472.262] [72.6988], Avg: [-553.249 -553.249 -553.249] (1.000)
Step: 27949, Reward: [-441.101 -441.101 -441.101] [85.9284], Avg: [-553.202 -553.202 -553.202] (1.000)
Step: 27999, Reward: [-458.962 -458.962 -458.962] [89.8969], Avg: [-553.194 -553.194 -553.194] (1.000)
Step: 28049, Reward: [-418.491 -418.491 -418.491] [86.4941], Avg: [-553.108 -553.108 -553.108] (1.000)
Step: 28099, Reward: [-430.542 -430.542 -430.542] [80.5464], Avg: [-553.033 -553.033 -553.033] (1.000)
Step: 28149, Reward: [-438.711 -438.711 -438.711] [71.9727], Avg: [-552.958 -552.958 -552.958] (1.000)
Step: 28199, Reward: [-459.206 -459.206 -459.206] [70.2750], Avg: [-552.917 -552.917 -552.917] (1.000)
Step: 28249, Reward: [-459.381 -459.381 -459.381] [84.0913], Avg: [-552.9 -552.9 -552.9] (1.000)
Step: 28299, Reward: [-488.338 -488.338 -488.338] [96.0869], Avg: [-552.956 -552.956 -552.956] (1.000)
Step: 28349, Reward: [-474.938 -474.938 -474.938] [94.4299], Avg: [-552.984 -552.984 -552.984] (1.000)
Step: 28399, Reward: [-495.03 -495.03 -495.03] [82.0186], Avg: [-553.027 -553.027 -553.027] (1.000)
Step: 28449, Reward: [-476.875 -476.875 -476.875] [97.9556], Avg: [-553.065 -553.065 -553.065] (1.000)
Step: 28499, Reward: [-471.883 -471.883 -471.883] [84.6173], Avg: [-553.071 -553.071 -553.071] (1.000)
Step: 28549, Reward: [-449.03 -449.03 -449.03] [73.6877], Avg: [-553.018 -553.018 -553.018] (1.000)
Step: 28599, Reward: [-460.346 -460.346 -460.346] [74.4087], Avg: [-552.986 -552.986 -552.986] (1.000)
Step: 28649, Reward: [-485.541 -485.541 -485.541] [89.1510], Avg: [-553.024 -553.024 -553.024] (1.000)
Step: 28699, Reward: [-471.852 -471.852 -471.852] [83.7384], Avg: [-553.028 -553.028 -553.028] (1.000)
Step: 28749, Reward: [-488.244 -488.244 -488.244] [93.0073], Avg: [-553.078 -553.078 -553.078] (1.000)
Step: 28799, Reward: [-475.991 -475.991 -475.991] [102.8958], Avg: [-553.122 -553.122 -553.122] (1.000)
Step: 28849, Reward: [-498.542 -498.542 -498.542] [123.3303], Avg: [-553.241 -553.241 -553.241] (1.000)
Step: 28899, Reward: [-499.656 -499.656 -499.656] [87.1133], Avg: [-553.299 -553.299 -553.299] (1.000)
Step: 28949, Reward: [-497.294 -497.294 -497.294] [95.1491], Avg: [-553.367 -553.367 -553.367] (1.000)
Step: 28999, Reward: [-522.857 -522.857 -522.857] [119.4078], Avg: [-553.52 -553.52 -553.52] (1.000)
Step: 29049, Reward: [-521.455 -521.455 -521.455] [109.9177], Avg: [-553.654 -553.654 -553.654] (1.000)
Step: 29099, Reward: [-533.287 -533.287 -533.287] [106.8432], Avg: [-553.803 -553.803 -553.803] (1.000)
Step: 29149, Reward: [-540.654 -540.654 -540.654] [110.3980], Avg: [-553.97 -553.97 -553.97] (1.000)
Step: 29199, Reward: [-519.223 -519.223 -519.223] [132.1575], Avg: [-554.137 -554.137 -554.137] (1.000)
Step: 29249, Reward: [-538.988 -538.988 -538.988] [101.3922], Avg: [-554.284 -554.284 -554.284] (1.000)
Step: 29299, Reward: [-529.638 -529.638 -529.638] [113.1809], Avg: [-554.435 -554.435 -554.435] (1.000)
Step: 29349, Reward: [-522.27 -522.27 -522.27] [107.8714], Avg: [-554.564 -554.564 -554.564] (1.000)
Step: 29399, Reward: [-574.064 -574.064 -574.064] [141.9198], Avg: [-554.839 -554.839 -554.839] (1.000)
Step: 29449, Reward: [-545.931 -545.931 -545.931] [123.6415], Avg: [-555.033 -555.033 -555.033] (1.000)
Step: 29499, Reward: [-570.001 -570.001 -570.001] [141.0079], Avg: [-555.298 -555.298 -555.298] (1.000)
Step: 29549, Reward: [-586.441 -586.441 -586.441] [136.4142], Avg: [-555.581 -555.581 -555.581] (1.000)
Step: 29599, Reward: [-589.282 -589.282 -589.282] [155.4124], Avg: [-555.901 -555.901 -555.901] (1.000)
Step: 29649, Reward: [-554.458 -554.458 -554.458] [158.0203], Avg: [-556.165 -556.165 -556.165] (1.000)
Step: 29699, Reward: [-639.536 -639.536 -639.536] [188.3480], Avg: [-556.622 -556.622 -556.622] (1.000)
Step: 29749, Reward: [-635.848 -635.848 -635.848] [146.7256], Avg: [-557.002 -557.002 -557.002] (1.000)
Step: 29799, Reward: [-669.166 -669.166 -669.166] [163.8952], Avg: [-557.465 -557.465 -557.465] (1.000)
Step: 29849, Reward: [-729.463 -729.463 -729.463] [218.0192], Avg: [-558.118 -558.118 -558.118] (1.000)
Step: 29899, Reward: [-754.106 -754.106 -754.106] [265.2467], Avg: [-558.89 -558.89 -558.89] (1.000)
Step: 29949, Reward: [-650.77 -650.77 -650.77] [220.0451], Avg: [-559.41 -559.41 -559.41] (1.000)
Step: 29999, Reward: [-579.62 -579.62 -579.62] [182.0379], Avg: [-559.748 -559.748 -559.748] (1.000)
Step: 30049, Reward: [-612.51 -612.51 -612.51] [208.3983], Avg: [-560.182 -560.182 -560.182] (1.000)
Step: 30099, Reward: [-704.303 -704.303 -704.303] [218.8305], Avg: [-560.785 -560.785 -560.785] (1.000)
Step: 30149, Reward: [-606.402 -606.402 -606.402] [222.9689], Avg: [-561.23 -561.23 -561.23] (1.000)
Step: 30199, Reward: [-565.469 -565.469 -565.469] [183.6700], Avg: [-561.541 -561.541 -561.541] (1.000)
Step: 30249, Reward: [-583.594 -583.594 -583.594] [201.0064], Avg: [-561.91 -561.91 -561.91] (1.000)
Step: 30299, Reward: [-702.619 -702.619 -702.619] [286.6779], Avg: [-562.615 -562.615 -562.615] (1.000)
Step: 30349, Reward: [-579.816 -579.816 -579.816] [203.6272], Avg: [-562.979 -562.979 -562.979] (1.000)
Step: 30399, Reward: [-649.883 -649.883 -649.883] [222.1403], Avg: [-563.488 -563.488 -563.488] (1.000)
Step: 30449, Reward: [-650.35 -650.35 -650.35] [262.9172], Avg: [-564.062 -564.062 -564.062] (1.000)
Step: 30499, Reward: [-737.194 -737.194 -737.194] [265.0018], Avg: [-564.78 -564.78 -564.78] (1.000)
Step: 30549, Reward: [-651.541 -651.541 -651.541] [260.3443], Avg: [-565.348 -565.348 -565.348] (1.000)
Step: 30599, Reward: [-650.227 -650.227 -650.227] [272.3435], Avg: [-565.932 -565.932 -565.932] (1.000)
Step: 30649, Reward: [-579.256 -579.256 -579.256] [240.0560], Avg: [-566.345 -566.345 -566.345] (1.000)
Step: 30699, Reward: [-798.227 -798.227 -798.227] [305.4447], Avg: [-567.22 -567.22 -567.22] (1.000)
Step: 30749, Reward: [-735.266 -735.266 -735.266] [296.9108], Avg: [-567.976 -567.976 -567.976] (1.000)
Step: 30799, Reward: [-657.445 -657.445 -657.445] [282.2452], Avg: [-568.58 -568.58 -568.58] (1.000)
Step: 30849, Reward: [-632.275 -632.275 -632.275] [286.4957], Avg: [-569.147 -569.147 -569.147] (1.000)
Step: 30899, Reward: [-617.565 -617.565 -617.565] [287.1821], Avg: [-569.69 -569.69 -569.69] (1.000)
Step: 30949, Reward: [-702.128 -702.128 -702.128] [313.9112], Avg: [-570.412 -570.412 -570.412] (1.000)
Step: 30999, Reward: [-801.382 -801.382 -801.382] [341.7776], Avg: [-571.335 -571.335 -571.335] (1.000)
Step: 31049, Reward: [-772.955 -772.955 -772.955] [370.3246], Avg: [-572.256 -572.256 -572.256] (1.000)
Step: 31099, Reward: [-763.684 -763.684 -763.684] [405.1529], Avg: [-573.215 -573.215 -573.215] (1.000)
Step: 31149, Reward: [-796.694 -796.694 -796.694] [383.7346], Avg: [-574.19 -574.19 -574.19] (1.000)
Step: 31199, Reward: [-930.352 -930.352 -930.352] [443.4996], Avg: [-575.472 -575.472 -575.472] (1.000)
Step: 31249, Reward: [-957.755 -957.755 -957.755] [523.4350], Avg: [-576.921 -576.921 -576.921] (1.000)
Step: 31299, Reward: [-934.903 -934.903 -934.903] [438.0018], Avg: [-578.192 -578.192 -578.192] (1.000)
Step: 31349, Reward: [-949.316 -949.316 -949.316] [494.7254], Avg: [-579.573 -579.573 -579.573] (1.000)
Step: 31399, Reward: [-1041.45 -1041.45 -1041.45] [487.5308], Avg: [-581.085 -581.085 -581.085] (1.000)
Step: 31449, Reward: [-1003.49 -1003.49 -1003.49] [541.0541], Avg: [-582.617 -582.617 -582.617] (1.000)
Step: 31499, Reward: [-959.73 -959.73 -959.73] [582.2646], Avg: [-584.14 -584.14 -584.14] (1.000)
Step: 31549, Reward: [-1041.076 -1041.076 -1041.076] [548.8648], Avg: [-585.734 -585.734 -585.734] (1.000)
Step: 31599, Reward: [-1082.172 -1082.172 -1082.172] [588.6123], Avg: [-587.45 -587.45 -587.45] (1.000)
Step: 31649, Reward: [-1133.46 -1133.46 -1133.46] [566.6107], Avg: [-589.208 -589.208 -589.208] (1.000)
Step: 31699, Reward: [-1007.456 -1007.456 -1007.456] [575.1868], Avg: [-590.775 -590.775 -590.775] (1.000)
Step: 31749, Reward: [-934.893 -934.893 -934.893] [502.8888], Avg: [-592.109 -592.109 -592.109] (1.000)
Step: 31799, Reward: [-957.138 -957.138 -957.138] [513.3901], Avg: [-593.49 -593.49 -593.49] (1.000)
Step: 31849, Reward: [-1113.145 -1113.145 -1113.145] [626.1692], Avg: [-595.289 -595.289 -595.289] (1.000)
Step: 31899, Reward: [-1082.115 -1082.115 -1082.115] [560.4673], Avg: [-596.93 -596.93 -596.93] (1.000)
Step: 31949, Reward: [-934.877 -934.877 -934.877] [546.1536], Avg: [-598.314 -598.314 -598.314] (1.000)
Step: 31999, Reward: [-983.543 -983.543 -983.543] [513.0904], Avg: [-599.718 -599.718 -599.718] (1.000)
Step: 32049, Reward: [-902.175 -902.175 -902.175] [533.1966], Avg: [-601.021 -601.021 -601.021] (1.000)
Step: 32099, Reward: [-927.855 -927.855 -927.855] [537.6515], Avg: [-602.368 -602.368 -602.368] (1.000)
Step: 32149, Reward: [-734.384 -734.384 -734.384] [365.9448], Avg: [-603.142 -603.142 -603.142] (1.000)
Step: 32199, Reward: [-958.312 -958.312 -958.312] [547.1986], Avg: [-604.543 -604.543 -604.543] (1.000)
Step: 32249, Reward: [-887.186 -887.186 -887.186] [553.3285], Avg: [-605.84 -605.84 -605.84] (1.000)
Step: 32299, Reward: [-903.722 -903.722 -903.722] [483.0155], Avg: [-607.048 -607.048 -607.048] (1.000)
Step: 32349, Reward: [-882.573 -882.573 -882.573] [526.1289], Avg: [-608.287 -608.287 -608.287] (1.000)
Step: 32399, Reward: [-751.545 -751.545 -751.545] [451.6562], Avg: [-609.205 -609.205 -609.205] (1.000)
Step: 32449, Reward: [-817.503 -817.503 -817.503] [503.3277], Avg: [-610.302 -610.302 -610.302] (1.000)
Step: 32499, Reward: [-732.986 -732.986 -732.986] [449.4614], Avg: [-611.182 -611.182 -611.182] (1.000)
Step: 32549, Reward: [-748.019 -748.019 -748.019] [419.0297], Avg: [-612.036 -612.036 -612.036] (1.000)
Step: 32599, Reward: [-684.856 -684.856 -684.856] [388.7190], Avg: [-612.744 -612.744 -612.744] (1.000)
Step: 32649, Reward: [-837.669 -837.669 -837.669] [493.0706], Avg: [-613.843 -613.843 -613.843] (1.000)
Step: 32699, Reward: [-648.775 -648.775 -648.775] [350.2053], Avg: [-614.432 -614.432 -614.432] (1.000)
Step: 32749, Reward: [-685.057 -685.057 -685.057] [351.6632], Avg: [-615.077 -615.077 -615.077] (1.000)
Step: 32799, Reward: [-589.294 -589.294 -589.294] [269.2225], Avg: [-615.448 -615.448 -615.448] (1.000)
Step: 32849, Reward: [-720.946 -720.946 -720.946] [338.7673], Avg: [-616.124 -616.124 -616.124] (1.000)
Step: 32899, Reward: [-619.665 -619.665 -619.665] [271.2904], Avg: [-616.542 -616.542 -616.542] (1.000)
Step: 32949, Reward: [-699.192 -699.192 -699.192] [328.8754], Avg: [-617.167 -617.167 -617.167] (1.000)
Step: 32999, Reward: [-654.316 -654.316 -654.316] [363.9585], Avg: [-617.774 -617.774 -617.774] (1.000)
Step: 33049, Reward: [-608.001 -608.001 -608.001] [269.8055], Avg: [-618.168 -618.168 -618.168] (1.000)
Step: 33099, Reward: [-621.014 -621.014 -621.014] [280.1560], Avg: [-618.595 -618.595 -618.595] (1.000)
Step: 33149, Reward: [-658.561 -658.561 -658.561] [314.0749], Avg: [-619.129 -619.129 -619.129] (1.000)
Step: 33199, Reward: [-749.496 -749.496 -749.496] [332.8220], Avg: [-619.827 -619.827 -619.827] (1.000)
Step: 33249, Reward: [-553.803 -553.803 -553.803] [254.5012], Avg: [-620.11 -620.11 -620.11] (1.000)
Step: 33299, Reward: [-625.792 -625.792 -625.792] [252.3253], Avg: [-620.498 -620.498 -620.498] (1.000)
Step: 33349, Reward: [-623.557 -623.557 -623.557] [265.4813], Avg: [-620.9 -620.9 -620.9] (1.000)
Step: 33399, Reward: [-566.605 -566.605 -566.605] [195.7585], Avg: [-621.112 -621.112 -621.112] (1.000)
Step: 33449, Reward: [-664.854 -664.854 -664.854] [273.4733], Avg: [-621.586 -621.586 -621.586] (1.000)
Step: 33499, Reward: [-638.358 -638.358 -638.358] [239.2182], Avg: [-621.968 -621.968 -621.968] (1.000)
Step: 33549, Reward: [-544.671 -544.671 -544.671] [175.8117], Avg: [-622.115 -622.115 -622.115] (1.000)
Step: 33599, Reward: [-654.569 -654.569 -654.569] [244.6840], Avg: [-622.527 -622.527 -622.527] (1.000)
Step: 33649, Reward: [-620.167 -620.167 -620.167] [221.3419], Avg: [-622.853 -622.853 -622.853] (1.000)
Step: 33699, Reward: [-583.495 -583.495 -583.495] [172.5827], Avg: [-623.05 -623.05 -623.05] (1.000)
Step: 33749, Reward: [-581.157 -581.157 -581.157] [178.7909], Avg: [-623.253 -623.253 -623.253] (1.000)
Step: 33799, Reward: [-626.16 -626.16 -626.16] [190.6015], Avg: [-623.539 -623.539 -623.539] (1.000)
Step: 33849, Reward: [-538.453 -538.453 -538.453] [116.9193], Avg: [-623.587 -623.587 -623.587] (1.000)
Step: 33899, Reward: [-583.824 -583.824 -583.824] [120.0097], Avg: [-623.705 -623.705 -623.705] (1.000)
Step: 33949, Reward: [-551.97 -551.97 -551.97] [102.0775], Avg: [-623.75 -623.75 -623.75] (1.000)
Step: 33999, Reward: [-544.434 -544.434 -544.434] [103.8002], Avg: [-623.786 -623.786 -623.786] (1.000)
Step: 34049, Reward: [-532.277 -532.277 -532.277] [108.1432], Avg: [-623.81 -623.81 -623.81] (1.000)
Step: 34099, Reward: [-517.894 -517.894 -517.894] [91.0647], Avg: [-623.788 -623.788 -623.788] (1.000)
Step: 34149, Reward: [-517.618 -517.618 -517.618] [100.2583], Avg: [-623.78 -623.78 -623.78] (1.000)
Step: 34199, Reward: [-518.755 -518.755 -518.755] [81.0610], Avg: [-623.745 -623.745 -623.745] (1.000)
Step: 34249, Reward: [-515.737 -515.737 -515.737] [82.0140], Avg: [-623.707 -623.707 -623.707] (1.000)
Step: 34299, Reward: [-524.865 -524.865 -524.865] [76.9415], Avg: [-623.675 -623.675 -623.675] (1.000)
Step: 34349, Reward: [-505.567 -505.567 -505.567] [93.8245], Avg: [-623.639 -623.639 -623.639] (1.000)
Step: 34399, Reward: [-538.232 -538.232 -538.232] [87.9896], Avg: [-623.643 -623.643 -623.643] (1.000)
Step: 34449, Reward: [-508.103 -508.103 -508.103] [73.3239], Avg: [-623.582 -623.582 -623.582] (1.000)
Step: 34499, Reward: [-511.769 -511.769 -511.769] [72.7296], Avg: [-623.525 -623.525 -623.525] (1.000)
Step: 34549, Reward: [-521.552 -521.552 -521.552] [93.7510], Avg: [-623.513 -623.513 -623.513] (1.000)
Step: 34599, Reward: [-525.982 -525.982 -525.982] [106.7215], Avg: [-623.527 -623.527 -623.527] (1.000)
Step: 34649, Reward: [-515.074 -515.074 -515.074] [101.1301], Avg: [-623.516 -623.516 -623.516] (1.000)
Step: 34699, Reward: [-512.38 -512.38 -512.38] [93.0538], Avg: [-623.49 -623.49 -623.49] (1.000)
Step: 34749, Reward: [-522.72 -522.72 -522.72] [113.8738], Avg: [-623.509 -623.509 -623.509] (1.000)
Step: 34799, Reward: [-511.585 -511.585 -511.585] [86.0415], Avg: [-623.472 -623.472 -623.472] (1.000)
Step: 34849, Reward: [-561.768 -561.768 -561.768] [79.3556], Avg: [-623.497 -623.497 -623.497] (1.000)
Step: 34899, Reward: [-523.628 -523.628 -523.628] [72.4119], Avg: [-623.458 -623.458 -623.458] (1.000)
Step: 34949, Reward: [-540.964 -540.964 -540.964] [115.0280], Avg: [-623.504 -623.504 -623.504] (1.000)
Step: 34999, Reward: [-526.379 -526.379 -526.379] [103.6013], Avg: [-623.513 -623.513 -623.513] (1.000)
Step: 35049, Reward: [-535.531 -535.531 -535.531] [141.1890], Avg: [-623.589 -623.589 -623.589] (1.000)
Step: 35099, Reward: [-495.885 -495.885 -495.885] [101.6474], Avg: [-623.552 -623.552 -623.552] (1.000)
Step: 35149, Reward: [-513.543 -513.543 -513.543] [78.8124], Avg: [-623.508 -623.508 -623.508] (1.000)
Step: 35199, Reward: [-545.583 -545.583 -545.583] [164.5801], Avg: [-623.631 -623.631 -623.631] (1.000)
Step: 35249, Reward: [-515.534 -515.534 -515.534] [102.6985], Avg: [-623.623 -623.623 -623.623] (1.000)
Step: 35299, Reward: [-536.814 -536.814 -536.814] [105.5799], Avg: [-623.65 -623.65 -623.65] (1.000)
Step: 35349, Reward: [-582.407 -582.407 -582.407] [225.5189], Avg: [-623.91 -623.91 -623.91] (1.000)
Step: 35399, Reward: [-514.519 -514.519 -514.519] [93.1782], Avg: [-623.888 -623.888 -623.888] (1.000)
Step: 35449, Reward: [-564.088 -564.088 -564.088] [217.7661], Avg: [-624.11 -624.11 -624.11] (1.000)
Step: 35499, Reward: [-528.157 -528.157 -528.157] [142.7678], Avg: [-624.176 -624.176 -624.176] (1.000)
Step: 35549, Reward: [-565.898 -565.898 -565.898] [137.1784], Avg: [-624.287 -624.287 -624.287] (1.000)
Step: 35599, Reward: [-487.158 -487.158 -487.158] [79.1743], Avg: [-624.206 -624.206 -624.206] (1.000)
Step: 35649, Reward: [-532.402 -532.402 -532.402] [140.2613], Avg: [-624.274 -624.274 -624.274] (1.000)
Step: 35699, Reward: [-523.503 -523.503 -523.503] [198.7540], Avg: [-624.411 -624.411 -624.411] (1.000)
Step: 35749, Reward: [-535.326 -535.326 -535.326] [147.3021], Avg: [-624.492 -624.492 -624.492] (1.000)
