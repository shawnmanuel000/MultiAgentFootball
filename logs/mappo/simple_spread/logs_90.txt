Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		self.hidden = self.recurrent(state, self.hidden)
		action_mu = self.action_mu(self.hidden).tanh()
		action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		# action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)

		# action_probs = action_mu.softmax(-1)
		# dist = torch.distributions.Categorical(action_probs)
		
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			model.actor_local.init_hidden(state.size(0), state.device)
			entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy = torch.stack(entropy).mean()
			# entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-596.688 -596.688 -596.688] [167.1527], Avg: [-596.688 -596.688 -596.688] (1.000)
Step: 2000, Reward: [-521.422 -521.422 -521.422] [87.4445], Avg: [-559.055 -559.055 -559.055] (1.000)
Step: 3000, Reward: [-488.145 -488.145 -488.145] [97.9107], Avg: [-535.418 -535.418 -535.418] (1.000)
Step: 4000, Reward: [-604.535 -604.535 -604.535] [163.2239], Avg: [-552.697 -552.697 -552.697] (1.000)
Step: 5000, Reward: [-587.719 -587.719 -587.719] [168.3961], Avg: [-559.702 -559.702 -559.702] (1.000)
Step: 6000, Reward: [-622.557 -622.557 -622.557] [161.4466], Avg: [-570.178 -570.178 -570.178] (1.000)
Step: 7000, Reward: [-573.206 -573.206 -573.206] [154.2292], Avg: [-570.61 -570.61 -570.61] (1.000)
Step: 8000, Reward: [-565.868 -565.868 -565.868] [102.8249], Avg: [-570.017 -570.017 -570.017] (1.000)
Step: 9000, Reward: [-717.847 -717.847 -717.847] [228.9186], Avg: [-586.443 -586.443 -586.443] (1.000)
Step: 10000, Reward: [-760.045 -760.045 -760.045] [161.4938], Avg: [-603.803 -603.803 -603.803] (1.000)
Step: 11000, Reward: [-610.589 -610.589 -610.589] [213.9048], Avg: [-604.42 -604.42 -604.42] (1.000)
Step: 12000, Reward: [-722.428 -722.428 -722.428] [283.2395], Avg: [-614.254 -614.254 -614.254] (1.000)
Step: 13000, Reward: [-682.989 -682.989 -682.989] [259.4422], Avg: [-619.541 -619.541 -619.541] (1.000)
Step: 14000, Reward: [-619.764 -619.764 -619.764] [214.5248], Avg: [-619.557 -619.557 -619.557] (1.000)
Step: 15000, Reward: [-655.587 -655.587 -655.587] [177.6975], Avg: [-621.959 -621.959 -621.959] (1.000)
Step: 16000, Reward: [-680.473 -680.473 -680.473] [144.2806], Avg: [-625.616 -625.616 -625.616] (1.000)
Step: 17000, Reward: [-800.421 -800.421 -800.421] [203.7354], Avg: [-635.899 -635.899 -635.899] (1.000)
Step: 18000, Reward: [-829.309 -829.309 -829.309] [251.1414], Avg: [-646.644 -646.644 -646.644] (1.000)
Step: 19000, Reward: [-797.264 -797.264 -797.264] [273.9583], Avg: [-654.571 -654.571 -654.571] (1.000)
Step: 20000, Reward: [-889.227 -889.227 -889.227] [160.9240], Avg: [-666.304 -666.304 -666.304] (1.000)
Step: 21000, Reward: [-846.881 -846.881 -846.881] [224.6528], Avg: [-674.903 -674.903 -674.903] (1.000)
Step: 22000, Reward: [-916.44 -916.44 -916.44] [217.3831], Avg: [-685.882 -685.882 -685.882] (1.000)
Step: 23000, Reward: [-938.413 -938.413 -938.413] [286.3279], Avg: [-696.862 -696.862 -696.862] (1.000)
Step: 24000, Reward: [-786.721 -786.721 -786.721] [266.6285], Avg: [-700.606 -700.606 -700.606] (1.000)
Step: 25000, Reward: [-713.471 -713.471 -713.471] [223.2832], Avg: [-701.12 -701.12 -701.12] (1.000)
Step: 26000, Reward: [-904.968 -904.968 -904.968] [310.7380], Avg: [-708.961 -708.961 -708.961] (1.000)
Step: 27000, Reward: [-853.781 -853.781 -853.781] [251.3740], Avg: [-714.324 -714.324 -714.324] (1.000)
Step: 28000, Reward: [-941.858 -941.858 -941.858] [247.4807], Avg: [-722.451 -722.451 -722.451] (1.000)
Step: 29000, Reward: [-785.147 -785.147 -785.147] [266.9723], Avg: [-724.613 -724.613 -724.613] (1.000)
Step: 30000, Reward: [-894.03 -894.03 -894.03] [280.3543], Avg: [-730.26 -730.26 -730.26] (1.000)
Step: 31000, Reward: [-813.306 -813.306 -813.306] [305.7874], Avg: [-732.939 -732.939 -732.939] (1.000)
Step: 32000, Reward: [-787.814 -787.814 -787.814] [254.3472], Avg: [-734.654 -734.654 -734.654] (1.000)
Step: 33000, Reward: [-904.641 -904.641 -904.641] [236.4622], Avg: [-739.805 -739.805 -739.805] (1.000)
Step: 34000, Reward: [-936.871 -936.871 -936.871] [314.6104], Avg: [-745.601 -745.601 -745.601] (1.000)
Step: 35000, Reward: [-855.63 -855.63 -855.63] [226.0994], Avg: [-748.744 -748.744 -748.744] (1.000)
Step: 36000, Reward: [-848.205 -848.205 -848.205] [184.1257], Avg: [-751.507 -751.507 -751.507] (1.000)
Step: 37000, Reward: [-881.318 -881.318 -881.318] [166.0012], Avg: [-755.016 -755.016 -755.016] (1.000)
Step: 38000, Reward: [-885.29 -885.29 -885.29] [218.6864], Avg: [-758.444 -758.444 -758.444] (1.000)
Step: 39000, Reward: [-957.665 -957.665 -957.665] [219.4260], Avg: [-763.552 -763.552 -763.552] (1.000)
Step: 40000, Reward: [-883.786 -883.786 -883.786] [227.4773], Avg: [-766.558 -766.558 -766.558] (1.000)
Step: 41000, Reward: [-940.755 -940.755 -940.755] [222.2593], Avg: [-770.807 -770.807 -770.807] (1.000)
Step: 42000, Reward: [-930.63 -930.63 -930.63] [227.1317], Avg: [-774.612 -774.612 -774.612] (1.000)
Step: 43000, Reward: [-921.29 -921.29 -921.29] [181.8969], Avg: [-778.023 -778.023 -778.023] (1.000)
Step: 44000, Reward: [-920.027 -920.027 -920.027] [155.7503], Avg: [-781.25 -781.25 -781.25] (1.000)
Step: 45000, Reward: [-932.953 -932.953 -932.953] [212.4764], Avg: [-784.622 -784.622 -784.622] (1.000)
Step: 46000, Reward: [-909.581 -909.581 -909.581] [220.3868], Avg: [-787.338 -787.338 -787.338] (1.000)
Step: 47000, Reward: [-886.483 -886.483 -886.483] [251.3289], Avg: [-789.448 -789.448 -789.448] (1.000)
Step: 48000, Reward: [-966.063 -966.063 -966.063] [246.1748], Avg: [-793.127 -793.127 -793.127] (1.000)
Step: 49000, Reward: [-995.717 -995.717 -995.717] [303.0958], Avg: [-797.262 -797.262 -797.262] (1.000)
Step: 50000, Reward: [-916.958 -916.958 -916.958] [245.9433], Avg: [-799.655 -799.655 -799.655] (1.000)
Step: 51000, Reward: [-971.73 -971.73 -971.73] [174.8327], Avg: [-803.03 -803.03 -803.03] (1.000)
Step: 52000, Reward: [-914.67 -914.67 -914.67] [258.5684], Avg: [-805.176 -805.176 -805.176] (1.000)
Step: 53000, Reward: [-909.006 -909.006 -909.006] [220.1098], Avg: [-807.135 -807.135 -807.135] (1.000)
Step: 54000, Reward: [-995.44 -995.44 -995.44] [203.3348], Avg: [-810.623 -810.623 -810.623] (1.000)
Step: 55000, Reward: [-886.852 -886.852 -886.852] [162.7141], Avg: [-812.009 -812.009 -812.009] (1.000)
Step: 56000, Reward: [-901.55 -901.55 -901.55] [142.8874], Avg: [-813.608 -813.608 -813.608] (1.000)
Step: 57000, Reward: [-937.523 -937.523 -937.523] [169.0243], Avg: [-815.782 -815.782 -815.782] (1.000)
Step: 58000, Reward: [-876.935 -876.935 -876.935] [193.1062], Avg: [-816.836 -816.836 -816.836] (1.000)
Step: 59000, Reward: [-883.225 -883.225 -883.225] [183.9896], Avg: [-817.961 -817.961 -817.961] (1.000)
Step: 60000, Reward: [-833.817 -833.817 -833.817] [179.6485], Avg: [-818.225 -818.225 -818.225] (1.000)
Step: 61000, Reward: [-964.2 -964.2 -964.2] [208.2353], Avg: [-820.618 -820.618 -820.618] (1.000)
Step: 62000, Reward: [-918.672 -918.672 -918.672] [174.9392], Avg: [-822.2 -822.2 -822.2] (1.000)
Step: 63000, Reward: [-938.147 -938.147 -938.147] [191.7668], Avg: [-824.04 -824.04 -824.04] (1.000)
Step: 64000, Reward: [-912.38 -912.38 -912.38] [177.9228], Avg: [-825.421 -825.421 -825.421] (1.000)
Step: 65000, Reward: [-884.75 -884.75 -884.75] [275.1110], Avg: [-826.333 -826.333 -826.333] (1.000)
Step: 66000, Reward: [-889.197 -889.197 -889.197] [162.5256], Avg: [-827.286 -827.286 -827.286] (1.000)
Step: 67000, Reward: [-946.554 -946.554 -946.554] [216.7911], Avg: [-829.066 -829.066 -829.066] (1.000)
Step: 68000, Reward: [-878.233 -878.233 -878.233] [207.5260], Avg: [-829.789 -829.789 -829.789] (1.000)
Step: 69000, Reward: [-963.032 -963.032 -963.032] [273.0481], Avg: [-831.72 -831.72 -831.72] (1.000)
Step: 70000, Reward: [-871.142 -871.142 -871.142] [161.6813], Avg: [-832.283 -832.283 -832.283] (1.000)
Step: 71000, Reward: [-954.475 -954.475 -954.475] [158.9975], Avg: [-834.004 -834.004 -834.004] (1.000)
Step: 72000, Reward: [-907.418 -907.418 -907.418] [192.8076], Avg: [-835.024 -835.024 -835.024] (1.000)
Step: 73000, Reward: [-846.148 -846.148 -846.148] [156.3412], Avg: [-835.176 -835.176 -835.176] (1.000)
Step: 74000, Reward: [-938.869 -938.869 -938.869] [195.7675], Avg: [-836.578 -836.578 -836.578] (1.000)
Step: 75000, Reward: [-874.965 -874.965 -874.965] [220.3265], Avg: [-837.089 -837.089 -837.089] (1.000)
Step: 76000, Reward: [-933.03 -933.03 -933.03] [157.5553], Avg: [-838.352 -838.352 -838.352] (1.000)
Step: 77000, Reward: [-1000.454 -1000.454 -1000.454] [189.1700], Avg: [-840.457 -840.457 -840.457] (1.000)
Step: 78000, Reward: [-869.703 -869.703 -869.703] [179.0142], Avg: [-840.832 -840.832 -840.832] (1.000)
Step: 79000, Reward: [-908.141 -908.141 -908.141] [202.7594], Avg: [-841.684 -841.684 -841.684] (1.000)
Step: 80000, Reward: [-1033.831 -1033.831 -1033.831] [185.2627], Avg: [-844.086 -844.086 -844.086] (1.000)
Step: 81000, Reward: [-882.641 -882.641 -882.641] [184.1070], Avg: [-844.562 -844.562 -844.562] (1.000)
Step: 82000, Reward: [-889.759 -889.759 -889.759] [213.0788], Avg: [-845.113 -845.113 -845.113] (1.000)
Step: 83000, Reward: [-874.077 -874.077 -874.077] [243.9819], Avg: [-845.462 -845.462 -845.462] (1.000)
Step: 84000, Reward: [-898.878 -898.878 -898.878] [206.5193], Avg: [-846.098 -846.098 -846.098] (1.000)
Step: 85000, Reward: [-958.853 -958.853 -958.853] [206.5159], Avg: [-847.424 -847.424 -847.424] (1.000)
Step: 86000, Reward: [-897.297 -897.297 -897.297] [153.3618], Avg: [-848.004 -848.004 -848.004] (1.000)
Step: 87000, Reward: [-883.804 -883.804 -883.804] [133.1799], Avg: [-848.416 -848.416 -848.416] (1.000)
Step: 88000, Reward: [-988.921 -988.921 -988.921] [157.3877], Avg: [-850.012 -850.012 -850.012] (1.000)
Step: 89000, Reward: [-958.486 -958.486 -958.486] [213.1389], Avg: [-851.231 -851.231 -851.231] (1.000)
Step: 90000, Reward: [-849.111 -849.111 -849.111] [183.2841], Avg: [-851.208 -851.208 -851.208] (1.000)
Step: 91000, Reward: [-839.322 -839.322 -839.322] [165.6598], Avg: [-851.077 -851.077 -851.077] (1.000)
Step: 92000, Reward: [-810.731 -810.731 -810.731] [180.8373], Avg: [-850.639 -850.639 -850.639] (1.000)
Step: 93000, Reward: [-897.397 -897.397 -897.397] [220.0272], Avg: [-851.141 -851.141 -851.141] (1.000)
Step: 94000, Reward: [-853.248 -853.248 -853.248] [198.5075], Avg: [-851.164 -851.164 -851.164] (1.000)
Step: 95000, Reward: [-953.175 -953.175 -953.175] [192.2259], Avg: [-852.238 -852.238 -852.238] (1.000)
Step: 96000, Reward: [-931.076 -931.076 -931.076] [239.0466], Avg: [-853.059 -853.059 -853.059] (1.000)
Step: 97000, Reward: [-965.055 -965.055 -965.055] [173.5326], Avg: [-854.213 -854.213 -854.213] (1.000)
Step: 98000, Reward: [-1036.487 -1036.487 -1036.487] [194.4604], Avg: [-856.073 -856.073 -856.073] (1.000)
Step: 99000, Reward: [-967.453 -967.453 -967.453] [222.0113], Avg: [-857.198 -857.198 -857.198] (1.000)
Step: 100000, Reward: [-945.152 -945.152 -945.152] [161.8805], Avg: [-858.078 -858.078 -858.078] (1.000)
Step: 101000, Reward: [-948.284 -948.284 -948.284] [166.3589], Avg: [-858.971 -858.971 -858.971] (1.000)
Step: 102000, Reward: [-926.856 -926.856 -926.856] [139.0977], Avg: [-859.637 -859.637 -859.637] (1.000)
Step: 103000, Reward: [-781.574 -781.574 -781.574] [165.9227], Avg: [-858.879 -858.879 -858.879] (1.000)
Step: 104000, Reward: [-867.716 -867.716 -867.716] [188.4077], Avg: [-858.964 -858.964 -858.964] (1.000)
Step: 105000, Reward: [-902.588 -902.588 -902.588] [189.8661], Avg: [-859.379 -859.379 -859.379] (1.000)
Step: 106000, Reward: [-890.706 -890.706 -890.706] [158.1983], Avg: [-859.675 -859.675 -859.675] (1.000)
Step: 107000, Reward: [-844.092 -844.092 -844.092] [200.4724], Avg: [-859.529 -859.529 -859.529] (1.000)
Step: 108000, Reward: [-939.867 -939.867 -939.867] [190.7168], Avg: [-860.273 -860.273 -860.273] (1.000)
Step: 109000, Reward: [-904.595 -904.595 -904.595] [186.1409], Avg: [-860.68 -860.68 -860.68] (1.000)
Step: 110000, Reward: [-969.395 -969.395 -969.395] [201.6884], Avg: [-861.668 -861.668 -861.668] (1.000)
Step: 111000, Reward: [-974.406 -974.406 -974.406] [186.8603], Avg: [-862.683 -862.683 -862.683] (1.000)
Step: 112000, Reward: [-897.014 -897.014 -897.014] [212.4988], Avg: [-862.99 -862.99 -862.99] (1.000)
Step: 113000, Reward: [-911.103 -911.103 -911.103] [218.4866], Avg: [-863.416 -863.416 -863.416] (1.000)
Step: 114000, Reward: [-879.588 -879.588 -879.588] [193.1779], Avg: [-863.558 -863.558 -863.558] (1.000)
Step: 115000, Reward: [-877.597 -877.597 -877.597] [204.4524], Avg: [-863.68 -863.68 -863.68] (1.000)
Step: 116000, Reward: [-885.252 -885.252 -885.252] [248.6615], Avg: [-863.866 -863.866 -863.866] (1.000)
Step: 117000, Reward: [-938.219 -938.219 -938.219] [186.8049], Avg: [-864.501 -864.501 -864.501] (1.000)
Step: 118000, Reward: [-926.885 -926.885 -926.885] [196.4094], Avg: [-865.03 -865.03 -865.03] (1.000)
Step: 119000, Reward: [-965.458 -965.458 -965.458] [197.3913], Avg: [-865.874 -865.874 -865.874] (1.000)
Step: 120000, Reward: [-869.756 -869.756 -869.756] [200.1212], Avg: [-865.906 -865.906 -865.906] (1.000)
Step: 121000, Reward: [-875.95 -875.95 -875.95] [193.2402], Avg: [-865.989 -865.989 -865.989] (1.000)
Step: 122000, Reward: [-874.724 -874.724 -874.724] [266.3653], Avg: [-866.061 -866.061 -866.061] (1.000)
Step: 123000, Reward: [-932.899 -932.899 -932.899] [162.8723], Avg: [-866.604 -866.604 -866.604] (1.000)
Step: 124000, Reward: [-933.705 -933.705 -933.705] [209.3841], Avg: [-867.145 -867.145 -867.145] (1.000)
Step: 125000, Reward: [-817.053 -817.053 -817.053] [209.7088], Avg: [-866.745 -866.745 -866.745] (1.000)
Step: 126000, Reward: [-832.483 -832.483 -832.483] [162.3951], Avg: [-866.473 -866.473 -866.473] (1.000)
Step: 127000, Reward: [-885.081 -885.081 -885.081] [177.6792], Avg: [-866.619 -866.619 -866.619] (1.000)
Step: 128000, Reward: [-937.726 -937.726 -937.726] [120.9174], Avg: [-867.175 -867.175 -867.175] (1.000)
Step: 129000, Reward: [-975.002 -975.002 -975.002] [234.7311], Avg: [-868.011 -868.011 -868.011] (1.000)
Step: 130000, Reward: [-847.527 -847.527 -847.527] [193.7162], Avg: [-867.853 -867.853 -867.853] (1.000)
Step: 131000, Reward: [-861.977 -861.977 -861.977] [184.6662], Avg: [-867.808 -867.808 -867.808] (1.000)
Step: 132000, Reward: [-943.345 -943.345 -943.345] [221.9841], Avg: [-868.38 -868.38 -868.38] (1.000)
Step: 133000, Reward: [-949.509 -949.509 -949.509] [171.1149], Avg: [-868.99 -868.99 -868.99] (1.000)
Step: 134000, Reward: [-865.131 -865.131 -865.131] [177.0520], Avg: [-868.962 -868.962 -868.962] (1.000)
Step: 135000, Reward: [-894.528 -894.528 -894.528] [165.2459], Avg: [-869.151 -869.151 -869.151] (1.000)
Step: 136000, Reward: [-822.774 -822.774 -822.774] [188.8974], Avg: [-868.81 -868.81 -868.81] (1.000)
Step: 137000, Reward: [-882.26 -882.26 -882.26] [177.8585], Avg: [-868.908 -868.908 -868.908] (1.000)
Step: 138000, Reward: [-836.142 -836.142 -836.142] [153.0550], Avg: [-868.671 -868.671 -868.671] (1.000)
Step: 139000, Reward: [-731.515 -731.515 -731.515] [154.5091], Avg: [-867.684 -867.684 -867.684] (1.000)
Step: 140000, Reward: [-890.769 -890.769 -890.769] [137.1825], Avg: [-867.849 -867.849 -867.849] (1.000)
Step: 141000, Reward: [-882.383 -882.383 -882.383] [185.7304], Avg: [-867.952 -867.952 -867.952] (1.000)
Step: 142000, Reward: [-800.218 -800.218 -800.218] [109.3534], Avg: [-867.475 -867.475 -867.475] (1.000)
Step: 143000, Reward: [-908.059 -908.059 -908.059] [176.0749], Avg: [-867.759 -867.759 -867.759] (1.000)
Step: 144000, Reward: [-789.23 -789.23 -789.23] [194.6907], Avg: [-867.213 -867.213 -867.213] (1.000)
Step: 145000, Reward: [-827.127 -827.127 -827.127] [117.0132], Avg: [-866.937 -866.937 -866.937] (1.000)
Step: 146000, Reward: [-967.304 -967.304 -967.304] [167.8848], Avg: [-867.624 -867.624 -867.624] (1.000)
Step: 147000, Reward: [-858.806 -858.806 -858.806] [142.8918], Avg: [-867.564 -867.564 -867.564] (1.000)
Step: 148000, Reward: [-830.149 -830.149 -830.149] [186.4394], Avg: [-867.312 -867.312 -867.312] (1.000)
Step: 149000, Reward: [-930.814 -930.814 -930.814] [234.6700], Avg: [-867.738 -867.738 -867.738] (1.000)
Step: 150000, Reward: [-837.349 -837.349 -837.349] [141.6756], Avg: [-867.535 -867.535 -867.535] (1.000)
Step: 151000, Reward: [-851.051 -851.051 -851.051] [220.7548], Avg: [-867.426 -867.426 -867.426] (1.000)
Step: 152000, Reward: [-869.64 -869.64 -869.64] [156.5831], Avg: [-867.441 -867.441 -867.441] (1.000)
Step: 153000, Reward: [-770.607 -770.607 -770.607] [175.4008], Avg: [-866.808 -866.808 -866.808] (1.000)
Step: 154000, Reward: [-832.669 -832.669 -832.669] [198.0334], Avg: [-866.586 -866.586 -866.586] (1.000)
Step: 155000, Reward: [-849.577 -849.577 -849.577] [192.5968], Avg: [-866.476 -866.476 -866.476] (1.000)
Step: 156000, Reward: [-923.566 -923.566 -923.566] [213.5135], Avg: [-866.842 -866.842 -866.842] (1.000)
Step: 157000, Reward: [-875.55 -875.55 -875.55] [177.4938], Avg: [-866.898 -866.898 -866.898] (1.000)
Step: 158000, Reward: [-835.77 -835.77 -835.77] [189.6168], Avg: [-866.701 -866.701 -866.701] (1.000)
Step: 159000, Reward: [-866.68 -866.68 -866.68] [230.3925], Avg: [-866.701 -866.701 -866.701] (1.000)
Step: 160000, Reward: [-840.751 -840.751 -840.751] [170.8687], Avg: [-866.538 -866.538 -866.538] (1.000)
Step: 161000, Reward: [-774.325 -774.325 -774.325] [175.7008], Avg: [-865.966 -865.966 -865.966] (1.000)
Step: 162000, Reward: [-845.514 -845.514 -845.514] [166.4184], Avg: [-865.839 -865.839 -865.839] (1.000)
Step: 163000, Reward: [-806.983 -806.983 -806.983] [154.6830], Avg: [-865.478 -865.478 -865.478] (1.000)
Step: 164000, Reward: [-852.43 -852.43 -852.43] [174.9375], Avg: [-865.399 -865.399 -865.399] (1.000)
Step: 165000, Reward: [-880.172 -880.172 -880.172] [185.5140], Avg: [-865.488 -865.488 -865.488] (1.000)
Step: 166000, Reward: [-871.191 -871.191 -871.191] [155.8508], Avg: [-865.523 -865.523 -865.523] (1.000)
Step: 167000, Reward: [-840.473 -840.473 -840.473] [238.1407], Avg: [-865.373 -865.373 -865.373] (1.000)
Step: 168000, Reward: [-847.647 -847.647 -847.647] [200.9592], Avg: [-865.267 -865.267 -865.267] (1.000)
Step: 169000, Reward: [-800.674 -800.674 -800.674] [119.0284], Avg: [-864.885 -864.885 -864.885] (1.000)
Step: 170000, Reward: [-842.363 -842.363 -842.363] [180.2397], Avg: [-864.752 -864.752 -864.752] (1.000)
Step: 171000, Reward: [-810.542 -810.542 -810.542] [178.0643], Avg: [-864.435 -864.435 -864.435] (1.000)
Step: 172000, Reward: [-928.431 -928.431 -928.431] [150.8426], Avg: [-864.807 -864.807 -864.807] (1.000)
Step: 173000, Reward: [-889.533 -889.533 -889.533] [256.7827], Avg: [-864.95 -864.95 -864.95] (1.000)
Step: 174000, Reward: [-906.262 -906.262 -906.262] [165.0670], Avg: [-865.188 -865.188 -865.188] (1.000)
Step: 175000, Reward: [-785.26 -785.26 -785.26] [146.4024], Avg: [-864.731 -864.731 -864.731] (1.000)
Step: 176000, Reward: [-782.426 -782.426 -782.426] [169.7098], Avg: [-864.263 -864.263 -864.263] (1.000)
Step: 177000, Reward: [-835.908 -835.908 -835.908] [200.2575], Avg: [-864.103 -864.103 -864.103] (1.000)
Step: 178000, Reward: [-805.386 -805.386 -805.386] [208.2570], Avg: [-863.773 -863.773 -863.773] (1.000)
Step: 179000, Reward: [-834.13 -834.13 -834.13] [173.8098], Avg: [-863.608 -863.608 -863.608] (1.000)
Step: 180000, Reward: [-821.262 -821.262 -821.262] [217.8776], Avg: [-863.373 -863.373 -863.373] (1.000)
Step: 181000, Reward: [-759.278 -759.278 -759.278] [124.8469], Avg: [-862.797 -862.797 -862.797] (1.000)
Step: 182000, Reward: [-794.842 -794.842 -794.842] [147.9526], Avg: [-862.424 -862.424 -862.424] (1.000)
Step: 183000, Reward: [-786.521 -786.521 -786.521] [139.1267], Avg: [-862.009 -862.009 -862.009] (1.000)
Step: 184000, Reward: [-835.623 -835.623 -835.623] [133.2060], Avg: [-861.866 -861.866 -861.866] (1.000)
Step: 185000, Reward: [-840.401 -840.401 -840.401] [140.2865], Avg: [-861.75 -861.75 -861.75] (1.000)
Step: 186000, Reward: [-891.412 -891.412 -891.412] [167.6175], Avg: [-861.909 -861.909 -861.909] (1.000)
Step: 187000, Reward: [-770.013 -770.013 -770.013] [177.1945], Avg: [-861.418 -861.418 -861.418] (1.000)
Step: 188000, Reward: [-810.082 -810.082 -810.082] [201.3255], Avg: [-861.145 -861.145 -861.145] (1.000)
Step: 189000, Reward: [-759.206 -759.206 -759.206] [172.7312], Avg: [-860.605 -860.605 -860.605] (1.000)
Step: 190000, Reward: [-835.903 -835.903 -835.903] [117.6008], Avg: [-860.475 -860.475 -860.475] (1.000)
Step: 191000, Reward: [-924.782 -924.782 -924.782] [199.5946], Avg: [-860.812 -860.812 -860.812] (1.000)
Step: 192000, Reward: [-764.243 -764.243 -764.243] [189.1620], Avg: [-860.309 -860.309 -860.309] (1.000)
Step: 193000, Reward: [-782.895 -782.895 -782.895] [218.9647], Avg: [-859.908 -859.908 -859.908] (1.000)
Step: 194000, Reward: [-761.622 -761.622 -761.622] [175.7988], Avg: [-859.401 -859.401 -859.401] (1.000)
Step: 195000, Reward: [-777.725 -777.725 -777.725] [145.6914], Avg: [-858.983 -858.983 -858.983] (1.000)
Step: 196000, Reward: [-693.574 -693.574 -693.574] [185.4808], Avg: [-858.139 -858.139 -858.139] (1.000)
Step: 197000, Reward: [-697.049 -697.049 -697.049] [136.9787], Avg: [-857.321 -857.321 -857.321] (1.000)
Step: 198000, Reward: [-756.846 -756.846 -756.846] [162.8410], Avg: [-856.813 -856.813 -856.813] (1.000)
Step: 199000, Reward: [-660.71 -660.71 -660.71] [129.9602], Avg: [-855.828 -855.828 -855.828] (1.000)
