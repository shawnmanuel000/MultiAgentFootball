Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		# self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		# out_dims = state.shape[:-1]
		# state = state.reshape(-1, state.shape[-1])
		# if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		# self.hidden = self.recurrent(state, self.hidden)
		# action_mu = self.action_mu(self.hidden)
		# action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		action_mu = self.action_mu(state)

		action_probs = action_mu.softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action = dist.sample() if action is None else action.argmax(-1)
		action_one_hot = one_hot_from_indices(action, action_probs.size(-1))
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_one_hot, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = lambda s,a: MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, self.actor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			# model.actor_local.init_hidden(state.size(0), state.device)
			# entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			# new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			# entropy = torch.stack(entropy).view(1, -1, *advantage.shape[2:])
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=100, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-638.737 -638.737 -638.737] [145.4178], Avg: [-638.737 -638.737 -638.737] (1.000)
Step: 2000, Reward: [-535.696 -535.696 -535.696] [89.2938], Avg: [-587.216 -587.216 -587.216] (1.000)
Step: 3000, Reward: [-531.81 -531.81 -531.81] [81.1733], Avg: [-568.748 -568.748 -568.748] (1.000)
Step: 4000, Reward: [-566.01 -566.01 -566.01] [77.1519], Avg: [-568.063 -568.063 -568.063] (1.000)
Step: 5000, Reward: [-525.134 -525.134 -525.134] [131.8299], Avg: [-559.477 -559.477 -559.477] (1.000)
Step: 6000, Reward: [-565.58 -565.58 -565.58] [111.3594], Avg: [-560.495 -560.495 -560.495] (1.000)
Step: 7000, Reward: [-496.324 -496.324 -496.324] [97.0851], Avg: [-551.327 -551.327 -551.327] (1.000)
Step: 8000, Reward: [-501.183 -501.183 -501.183] [112.0415], Avg: [-545.059 -545.059 -545.059] (1.000)
Step: 9000, Reward: [-504.335 -504.335 -504.335] [82.2103], Avg: [-540.534 -540.534 -540.534] (1.000)
Step: 10000, Reward: [-465.328 -465.328 -465.328] [77.3640], Avg: [-533.014 -533.014 -533.014] (1.000)
Step: 11000, Reward: [-446.256 -446.256 -446.256] [75.4616], Avg: [-525.127 -525.127 -525.127] (1.000)
Step: 12000, Reward: [-428.097 -428.097 -428.097] [65.2987], Avg: [-517.041 -517.041 -517.041] (1.000)
Step: 13000, Reward: [-437.553 -437.553 -437.553] [57.4484], Avg: [-510.926 -510.926 -510.926] (1.000)
Step: 14000, Reward: [-403.842 -403.842 -403.842] [41.8247], Avg: [-503.277 -503.277 -503.277] (1.000)
Step: 15000, Reward: [-414.59 -414.59 -414.59] [66.4093], Avg: [-497.365 -497.365 -497.365] (1.000)
Step: 16000, Reward: [-411.788 -411.788 -411.788] [52.6131], Avg: [-492.016 -492.016 -492.016] (1.000)
Step: 17000, Reward: [-397.743 -397.743 -397.743] [72.3947], Avg: [-486.471 -486.471 -486.471] (1.000)
Step: 18000, Reward: [-400.207 -400.207 -400.207] [67.8049], Avg: [-481.678 -481.678 -481.678] (1.000)
Step: 19000, Reward: [-384.644 -384.644 -384.644] [49.1719], Avg: [-476.571 -476.571 -476.571] (1.000)
Step: 20000, Reward: [-398.295 -398.295 -398.295] [62.6033], Avg: [-472.658 -472.658 -472.658] (1.000)
Step: 21000, Reward: [-411.243 -411.243 -411.243] [61.3096], Avg: [-469.733 -469.733 -469.733] (1.000)
Step: 22000, Reward: [-388.435 -388.435 -388.435] [62.9205], Avg: [-466.038 -466.038 -466.038] (1.000)
Step: 23000, Reward: [-365.37 -365.37 -365.37] [44.7258], Avg: [-461.661 -461.661 -461.661] (1.000)
Step: 24000, Reward: [-382.805 -382.805 -382.805] [65.4197], Avg: [-458.375 -458.375 -458.375] (1.000)
Step: 25000, Reward: [-431.241 -431.241 -431.241] [64.8636], Avg: [-457.29 -457.29 -457.29] (1.000)
Step: 26000, Reward: [-380.592 -380.592 -380.592] [43.2966], Avg: [-454.34 -454.34 -454.34] (1.000)
Step: 27000, Reward: [-387.632 -387.632 -387.632] [48.6640], Avg: [-451.869 -451.869 -451.869] (1.000)
Step: 28000, Reward: [-401.87 -401.87 -401.87] [46.8299], Avg: [-450.084 -450.084 -450.084] (1.000)
Step: 29000, Reward: [-382.947 -382.947 -382.947] [60.1567], Avg: [-447.768 -447.768 -447.768] (1.000)
Step: 30000, Reward: [-371.132 -371.132 -371.132] [60.3994], Avg: [-445.214 -445.214 -445.214] (1.000)
Step: 31000, Reward: [-381.418 -381.418 -381.418] [65.2326], Avg: [-443.156 -443.156 -443.156] (1.000)
Step: 32000, Reward: [-372.229 -372.229 -372.229] [49.5800], Avg: [-440.94 -440.94 -440.94] (1.000)
Step: 33000, Reward: [-390.919 -390.919 -390.919] [59.1319], Avg: [-439.424 -439.424 -439.424] (1.000)
Step: 34000, Reward: [-397.748 -397.748 -397.748] [62.4761], Avg: [-438.198 -438.198 -438.198] (1.000)
Step: 35000, Reward: [-367.602 -367.602 -367.602] [52.0092], Avg: [-436.181 -436.181 -436.181] (1.000)
Step: 36000, Reward: [-386.945 -386.945 -386.945] [50.5323], Avg: [-434.813 -434.813 -434.813] (1.000)
Step: 37000, Reward: [-371.637 -371.637 -371.637] [68.6021], Avg: [-433.106 -433.106 -433.106] (1.000)
Step: 38000, Reward: [-391.81 -391.81 -391.81] [77.5856], Avg: [-432.019 -432.019 -432.019] (1.000)
Step: 39000, Reward: [-370.331 -370.331 -370.331] [47.5652], Avg: [-430.437 -430.437 -430.437] (1.000)
Step: 40000, Reward: [-390.005 -390.005 -390.005] [54.1685], Avg: [-429.427 -429.427 -429.427] (1.000)
Step: 41000, Reward: [-378.07 -378.07 -378.07] [39.7444], Avg: [-428.174 -428.174 -428.174] (1.000)
Step: 42000, Reward: [-355.41 -355.41 -355.41] [67.6860], Avg: [-426.441 -426.441 -426.441] (1.000)
Step: 43000, Reward: [-365.202 -365.202 -365.202] [66.9195], Avg: [-425.017 -425.017 -425.017] (1.000)
Step: 44000, Reward: [-373.942 -373.942 -373.942] [44.8521], Avg: [-423.857 -423.857 -423.857] (1.000)
Step: 45000, Reward: [-386.199 -386.199 -386.199] [46.6569], Avg: [-423.02 -423.02 -423.02] (1.000)
Step: 46000, Reward: [-353.315 -353.315 -353.315] [40.8906], Avg: [-421.504 -421.504 -421.504] (1.000)
Step: 47000, Reward: [-349.47 -349.47 -349.47] [40.0341], Avg: [-419.972 -419.972 -419.972] (1.000)
Step: 48000, Reward: [-363.046 -363.046 -363.046] [44.6819], Avg: [-418.786 -418.786 -418.786] (1.000)
Step: 49000, Reward: [-378.93 -378.93 -378.93] [50.7550], Avg: [-417.972 -417.972 -417.972] (1.000)
Step: 50000, Reward: [-359.689 -359.689 -359.689] [50.3805], Avg: [-416.807 -416.807 -416.807] (1.000)
Step: 51000, Reward: [-364.624 -364.624 -364.624] [53.3199], Avg: [-415.784 -415.784 -415.784] (1.000)
Step: 52000, Reward: [-370.006 -370.006 -370.006] [54.8172], Avg: [-414.903 -414.903 -414.903] (1.000)
Step: 53000, Reward: [-335.509 -335.509 -335.509] [53.1536], Avg: [-413.405 -413.405 -413.405] (1.000)
Step: 54000, Reward: [-374.382 -374.382 -374.382] [64.3051], Avg: [-412.683 -412.683 -412.683] (1.000)
Step: 55000, Reward: [-359.272 -359.272 -359.272] [61.8116], Avg: [-411.711 -411.711 -411.711] (1.000)
Step: 56000, Reward: [-353.64 -353.64 -353.64] [47.7700], Avg: [-410.674 -410.674 -410.674] (1.000)
Step: 57000, Reward: [-365.436 -365.436 -365.436] [44.3220], Avg: [-409.881 -409.881 -409.881] (1.000)
Step: 58000, Reward: [-354.829 -354.829 -354.829] [56.0965], Avg: [-408.932 -408.932 -408.932] (1.000)
Step: 59000, Reward: [-352.803 -352.803 -352.803] [45.7721], Avg: [-407.98 -407.98 -407.98] (1.000)
Step: 60000, Reward: [-381.883 -381.883 -381.883] [58.2292], Avg: [-407.545 -407.545 -407.545] (1.000)
Step: 61000, Reward: [-361.935 -361.935 -361.935] [45.1389], Avg: [-406.798 -406.798 -406.798] (1.000)
Step: 62000, Reward: [-368.279 -368.279 -368.279] [65.3121], Avg: [-406.176 -406.176 -406.176] (1.000)
Step: 63000, Reward: [-389.017 -389.017 -389.017] [72.3371], Avg: [-405.904 -405.904 -405.904] (1.000)
Step: 64000, Reward: [-377.462 -377.462 -377.462] [48.2076], Avg: [-405.46 -405.46 -405.46] (1.000)
Step: 65000, Reward: [-349.878 -349.878 -349.878] [58.8457], Avg: [-404.604 -404.604 -404.604] (1.000)
Step: 66000, Reward: [-359.341 -359.341 -359.341] [54.7411], Avg: [-403.919 -403.919 -403.919] (1.000)
Step: 67000, Reward: [-368.247 -368.247 -368.247] [67.9921], Avg: [-403.386 -403.386 -403.386] (1.000)
Step: 68000, Reward: [-374.179 -374.179 -374.179] [46.6045], Avg: [-402.957 -402.957 -402.957] (1.000)
Step: 69000, Reward: [-371.041 -371.041 -371.041] [53.0682], Avg: [-402.494 -402.494 -402.494] (1.000)
Step: 70000, Reward: [-380.727 -380.727 -380.727] [63.8075], Avg: [-402.183 -402.183 -402.183] (1.000)
Step: 71000, Reward: [-384.708 -384.708 -384.708] [62.1590], Avg: [-401.937 -401.937 -401.937] (1.000)
Step: 72000, Reward: [-370.138 -370.138 -370.138] [58.6931], Avg: [-401.495 -401.495 -401.495] (1.000)
Step: 73000, Reward: [-352.668 -352.668 -352.668] [51.9917], Avg: [-400.827 -400.827 -400.827] (1.000)
Step: 74000, Reward: [-375.945 -375.945 -375.945] [72.1240], Avg: [-400.49 -400.49 -400.49] (1.000)
Step: 75000, Reward: [-349.65 -349.65 -349.65] [55.6966], Avg: [-399.812 -399.812 -399.812] (1.000)
Step: 76000, Reward: [-389.6 -389.6 -389.6] [63.5530], Avg: [-399.678 -399.678 -399.678] (1.000)
Step: 77000, Reward: [-382.381 -382.381 -382.381] [50.8679], Avg: [-399.453 -399.453 -399.453] (1.000)
Step: 78000, Reward: [-375.369 -375.369 -375.369] [46.2549], Avg: [-399.145 -399.145 -399.145] (1.000)
Step: 79000, Reward: [-351.928 -351.928 -351.928] [54.2675], Avg: [-398.547 -398.547 -398.547] (1.000)
Step: 80000, Reward: [-354.27 -354.27 -354.27] [63.1523], Avg: [-397.994 -397.994 -397.994] (1.000)
Step: 81000, Reward: [-363.139 -363.139 -363.139] [49.8216], Avg: [-397.563 -397.563 -397.563] (1.000)
Step: 82000, Reward: [-386.511 -386.511 -386.511] [53.5448], Avg: [-397.428 -397.428 -397.428] (1.000)
Step: 83000, Reward: [-359.653 -359.653 -359.653] [68.0224], Avg: [-396.973 -396.973 -396.973] (1.000)
Step: 84000, Reward: [-367.202 -367.202 -367.202] [58.1095], Avg: [-396.619 -396.619 -396.619] (1.000)
Step: 85000, Reward: [-356.362 -356.362 -356.362] [72.8898], Avg: [-396.145 -396.145 -396.145] (1.000)
Step: 86000, Reward: [-364.336 -364.336 -364.336] [55.5015], Avg: [-395.775 -395.775 -395.775] (1.000)
Step: 87000, Reward: [-385.621 -385.621 -385.621] [54.9981], Avg: [-395.659 -395.659 -395.659] (1.000)
Step: 88000, Reward: [-364.848 -364.848 -364.848] [38.1357], Avg: [-395.309 -395.309 -395.309] (1.000)
Step: 89000, Reward: [-368.177 -368.177 -368.177] [57.6624], Avg: [-395.004 -395.004 -395.004] (1.000)
Step: 90000, Reward: [-359.299 -359.299 -359.299] [39.9336], Avg: [-394.607 -394.607 -394.607] (1.000)
Step: 91000, Reward: [-362.475 -362.475 -362.475] [48.9696], Avg: [-394.254 -394.254 -394.254] (1.000)
Step: 92000, Reward: [-369.273 -369.273 -369.273] [63.6825], Avg: [-393.982 -393.982 -393.982] (1.000)
Step: 93000, Reward: [-367.71 -367.71 -367.71] [29.6103], Avg: [-393.7 -393.7 -393.7] (1.000)
Step: 94000, Reward: [-394.917 -394.917 -394.917] [64.6847], Avg: [-393.713 -393.713 -393.713] (1.000)
Step: 95000, Reward: [-374.202 -374.202 -374.202] [57.0813], Avg: [-393.507 -393.507 -393.507] (1.000)
Step: 96000, Reward: [-379.778 -379.778 -379.778] [53.7536], Avg: [-393.364 -393.364 -393.364] (1.000)
Step: 97000, Reward: [-386.683 -386.683 -386.683] [56.0596], Avg: [-393.296 -393.296 -393.296] (1.000)
Step: 98000, Reward: [-352.949 -352.949 -352.949] [61.7941], Avg: [-392.884 -392.884 -392.884] (1.000)
Step: 99000, Reward: [-386.125 -386.125 -386.125] [42.5203], Avg: [-392.816 -392.816 -392.816] (1.000)
Step: 100000, Reward: [-390.855 -390.855 -390.855] [67.8425], Avg: [-392.796 -392.796 -392.796] (1.000)
Step: 101000, Reward: [-380.827 -380.827 -380.827] [55.6030], Avg: [-392.677 -392.677 -392.677] (1.000)
Step: 102000, Reward: [-372.268 -372.268 -372.268] [53.0555], Avg: [-392.477 -392.477 -392.477] (1.000)
Step: 103000, Reward: [-349.039 -349.039 -349.039] [61.4031], Avg: [-392.056 -392.056 -392.056] (1.000)
Step: 104000, Reward: [-367.219 -367.219 -367.219] [63.3764], Avg: [-391.817 -391.817 -391.817] (1.000)
Step: 105000, Reward: [-402.493 -402.493 -402.493] [54.8429], Avg: [-391.919 -391.919 -391.919] (1.000)
Step: 106000, Reward: [-386.406 -386.406 -386.406] [61.2953], Avg: [-391.866 -391.866 -391.866] (1.000)
Step: 107000, Reward: [-368.133 -368.133 -368.133] [47.0344], Avg: [-391.645 -391.645 -391.645] (1.000)
Step: 108000, Reward: [-374.561 -374.561 -374.561] [50.4780], Avg: [-391.487 -391.487 -391.487] (1.000)
Step: 109000, Reward: [-353.322 -353.322 -353.322] [67.7348], Avg: [-391.136 -391.136 -391.136] (1.000)
Step: 110000, Reward: [-354.386 -354.386 -354.386] [52.3541], Avg: [-390.802 -390.802 -390.802] (1.000)
Step: 111000, Reward: [-391.676 -391.676 -391.676] [57.0886], Avg: [-390.81 -390.81 -390.81] (1.000)
Step: 112000, Reward: [-395.417 -395.417 -395.417] [72.8737], Avg: [-390.851 -390.851 -390.851] (1.000)
Step: 113000, Reward: [-410.08 -410.08 -410.08] [65.5167], Avg: [-391.021 -391.021 -391.021] (1.000)
Step: 114000, Reward: [-366.328 -366.328 -366.328] [65.1185], Avg: [-390.805 -390.805 -390.805] (1.000)
Step: 115000, Reward: [-374.235 -374.235 -374.235] [66.6171], Avg: [-390.661 -390.661 -390.661] (1.000)
Step: 116000, Reward: [-393.487 -393.487 -393.487] [52.2019], Avg: [-390.685 -390.685 -390.685] (1.000)
Step: 117000, Reward: [-389.003 -389.003 -389.003] [56.4338], Avg: [-390.671 -390.671 -390.671] (1.000)
Step: 118000, Reward: [-363.458 -363.458 -363.458] [41.6684], Avg: [-390.44 -390.44 -390.44] (1.000)
Step: 119000, Reward: [-363.249 -363.249 -363.249] [61.4856], Avg: [-390.212 -390.212 -390.212] (1.000)
Step: 120000, Reward: [-379.243 -379.243 -379.243] [60.5766], Avg: [-390.12 -390.12 -390.12] (1.000)
Step: 121000, Reward: [-397.719 -397.719 -397.719] [62.0403], Avg: [-390.183 -390.183 -390.183] (1.000)
Step: 122000, Reward: [-358.815 -358.815 -358.815] [45.6813], Avg: [-389.926 -389.926 -389.926] (1.000)
Step: 123000, Reward: [-371.515 -371.515 -371.515] [57.1986], Avg: [-389.776 -389.776 -389.776] (1.000)
Step: 124000, Reward: [-367.292 -367.292 -367.292] [60.0897], Avg: [-389.595 -389.595 -389.595] (1.000)
Step: 125000, Reward: [-383.166 -383.166 -383.166] [50.1277], Avg: [-389.543 -389.543 -389.543] (1.000)
Step: 126000, Reward: [-368.51 -368.51 -368.51] [76.5014], Avg: [-389.377 -389.377 -389.377] (1.000)
Step: 127000, Reward: [-394.235 -394.235 -394.235] [68.7948], Avg: [-389.415 -389.415 -389.415] (1.000)
Step: 128000, Reward: [-363.72 -363.72 -363.72] [62.5053], Avg: [-389.214 -389.214 -389.214] (1.000)
Step: 129000, Reward: [-373.607 -373.607 -373.607] [59.5421], Avg: [-389.093 -389.093 -389.093] (1.000)
Step: 130000, Reward: [-381.95 -381.95 -381.95] [50.1880], Avg: [-389.038 -389.038 -389.038] (1.000)
Step: 131000, Reward: [-348.192 -348.192 -348.192] [45.9817], Avg: [-388.726 -388.726 -388.726] (1.000)
Step: 132000, Reward: [-359.433 -359.433 -359.433] [66.7095], Avg: [-388.504 -388.504 -388.504] (1.000)
Step: 133000, Reward: [-357.298 -357.298 -357.298] [53.5001], Avg: [-388.27 -388.27 -388.27] (1.000)
Step: 134000, Reward: [-352.131 -352.131 -352.131] [58.0773], Avg: [-388. -388. -388.] (1.000)
Step: 135000, Reward: [-366.918 -366.918 -366.918] [47.1684], Avg: [-387.844 -387.844 -387.844] (1.000)
Step: 136000, Reward: [-349.098 -349.098 -349.098] [56.3916], Avg: [-387.559 -387.559 -387.559] (1.000)
Step: 137000, Reward: [-358.319 -358.319 -358.319] [56.0307], Avg: [-387.346 -387.346 -387.346] (1.000)
Step: 138000, Reward: [-360.93 -360.93 -360.93] [54.8147], Avg: [-387.154 -387.154 -387.154] (1.000)
Step: 139000, Reward: [-360.573 -360.573 -360.573] [57.7159], Avg: [-386.963 -386.963 -386.963] (1.000)
Step: 140000, Reward: [-376.568 -376.568 -376.568] [75.9659], Avg: [-386.889 -386.889 -386.889] (1.000)
Step: 141000, Reward: [-366.619 -366.619 -366.619] [45.9970], Avg: [-386.745 -386.745 -386.745] (1.000)
Step: 142000, Reward: [-383.533 -383.533 -383.533] [54.5181], Avg: [-386.722 -386.722 -386.722] (1.000)
Step: 143000, Reward: [-367.174 -367.174 -367.174] [59.9546], Avg: [-386.586 -386.586 -386.586] (1.000)
Step: 144000, Reward: [-349.284 -349.284 -349.284] [57.1937], Avg: [-386.327 -386.327 -386.327] (1.000)
Step: 145000, Reward: [-350.957 -350.957 -350.957] [51.1182], Avg: [-386.083 -386.083 -386.083] (1.000)
Step: 146000, Reward: [-346.777 -346.777 -346.777] [56.4693], Avg: [-385.813 -385.813 -385.813] (1.000)
Step: 147000, Reward: [-321.094 -321.094 -321.094] [43.8839], Avg: [-385.373 -385.373 -385.373] (1.000)
Step: 148000, Reward: [-382.235 -382.235 -382.235] [61.5895], Avg: [-385.352 -385.352 -385.352] (1.000)
Step: 149000, Reward: [-353.859 -353.859 -353.859] [52.5936], Avg: [-385.141 -385.141 -385.141] (1.000)
Step: 150000, Reward: [-359.964 -359.964 -359.964] [55.6177], Avg: [-384.973 -384.973 -384.973] (1.000)
Step: 151000, Reward: [-390.464 -390.464 -390.464] [67.9605], Avg: [-385.009 -385.009 -385.009] (1.000)
Step: 152000, Reward: [-336.423 -336.423 -336.423] [69.4791], Avg: [-384.689 -384.689 -384.689] (1.000)
Step: 153000, Reward: [-358.018 -358.018 -358.018] [54.7242], Avg: [-384.515 -384.515 -384.515] (1.000)
Step: 154000, Reward: [-379.452 -379.452 -379.452] [57.4919], Avg: [-384.482 -384.482 -384.482] (1.000)
Step: 155000, Reward: [-365.462 -365.462 -365.462] [61.2448], Avg: [-384.36 -384.36 -384.36] (1.000)
Step: 156000, Reward: [-382.002 -382.002 -382.002] [39.1671], Avg: [-384.344 -384.344 -384.344] (1.000)
Step: 157000, Reward: [-394.005 -394.005 -394.005] [75.4904], Avg: [-384.406 -384.406 -384.406] (1.000)
Step: 158000, Reward: [-377.657 -377.657 -377.657] [64.2839], Avg: [-384.363 -384.363 -384.363] (1.000)
Step: 159000, Reward: [-352.773 -352.773 -352.773] [46.3002], Avg: [-384.165 -384.165 -384.165] (1.000)
Step: 160000, Reward: [-329.625 -329.625 -329.625] [56.9779], Avg: [-383.824 -383.824 -383.824] (1.000)
Step: 161000, Reward: [-383.066 -383.066 -383.066] [46.1872], Avg: [-383.819 -383.819 -383.819] (1.000)
Step: 162000, Reward: [-364.675 -364.675 -364.675] [72.9000], Avg: [-383.701 -383.701 -383.701] (1.000)
Step: 163000, Reward: [-339.788 -339.788 -339.788] [50.0707], Avg: [-383.431 -383.431 -383.431] (1.000)
Step: 164000, Reward: [-344.611 -344.611 -344.611] [61.7887], Avg: [-383.195 -383.195 -383.195] (1.000)
Step: 165000, Reward: [-349.995 -349.995 -349.995] [60.8797], Avg: [-382.994 -382.994 -382.994] (1.000)
Step: 166000, Reward: [-367.723 -367.723 -367.723] [72.2833], Avg: [-382.902 -382.902 -382.902] (1.000)
Step: 167000, Reward: [-340.659 -340.659 -340.659] [51.5614], Avg: [-382.649 -382.649 -382.649] (1.000)
Step: 168000, Reward: [-371.581 -371.581 -371.581] [65.4892], Avg: [-382.583 -382.583 -382.583] (1.000)
Step: 169000, Reward: [-370.472 -370.472 -370.472] [57.8933], Avg: [-382.511 -382.511 -382.511] (1.000)
Step: 170000, Reward: [-365.081 -365.081 -365.081] [65.6153], Avg: [-382.409 -382.409 -382.409] (1.000)
Step: 171000, Reward: [-344.015 -344.015 -344.015] [65.7178], Avg: [-382.184 -382.184 -382.184] (1.000)
Step: 172000, Reward: [-379.768 -379.768 -379.768] [56.3934], Avg: [-382.17 -382.17 -382.17] (1.000)
Step: 173000, Reward: [-362.802 -362.802 -362.802] [56.7230], Avg: [-382.058 -382.058 -382.058] (1.000)
Step: 174000, Reward: [-338.708 -338.708 -338.708] [62.4589], Avg: [-381.809 -381.809 -381.809] (1.000)
Step: 175000, Reward: [-342.649 -342.649 -342.649] [49.2122], Avg: [-381.585 -381.585 -381.585] (1.000)
Step: 176000, Reward: [-377.168 -377.168 -377.168] [56.2127], Avg: [-381.56 -381.56 -381.56] (1.000)
Step: 177000, Reward: [-362.039 -362.039 -362.039] [66.8629], Avg: [-381.45 -381.45 -381.45] (1.000)
Step: 178000, Reward: [-374.987 -374.987 -374.987] [56.3629], Avg: [-381.413 -381.413 -381.413] (1.000)
Step: 179000, Reward: [-362.59 -362.59 -362.59] [52.4975], Avg: [-381.308 -381.308 -381.308] (1.000)
Step: 180000, Reward: [-352.477 -352.477 -352.477] [54.0211], Avg: [-381.148 -381.148 -381.148] (1.000)
Step: 181000, Reward: [-355.277 -355.277 -355.277] [68.8151], Avg: [-381.005 -381.005 -381.005] (1.000)
Step: 182000, Reward: [-336.32 -336.32 -336.32] [53.9607], Avg: [-380.76 -380.76 -380.76] (1.000)
Step: 183000, Reward: [-354.734 -354.734 -354.734] [67.5859], Avg: [-380.617 -380.617 -380.617] (1.000)
Step: 184000, Reward: [-358.208 -358.208 -358.208] [65.8951], Avg: [-380.496 -380.496 -380.496] (1.000)
Step: 185000, Reward: [-347.756 -347.756 -347.756] [50.7821], Avg: [-380.319 -380.319 -380.319] (1.000)
Step: 186000, Reward: [-348.973 -348.973 -348.973] [61.9271], Avg: [-380.15 -380.15 -380.15] (1.000)
Step: 187000, Reward: [-357.305 -357.305 -357.305] [67.2334], Avg: [-380.028 -380.028 -380.028] (1.000)
Step: 188000, Reward: [-357.365 -357.365 -357.365] [69.2438], Avg: [-379.907 -379.907 -379.907] (1.000)
Step: 189000, Reward: [-357.813 -357.813 -357.813] [57.6442], Avg: [-379.79 -379.79 -379.79] (1.000)
Step: 190000, Reward: [-366.354 -366.354 -366.354] [52.1940], Avg: [-379.72 -379.72 -379.72] (1.000)
Step: 191000, Reward: [-366.542 -366.542 -366.542] [54.8813], Avg: [-379.651 -379.651 -379.651] (1.000)
Step: 192000, Reward: [-361.931 -361.931 -361.931] [51.8984], Avg: [-379.558 -379.558 -379.558] (1.000)
Step: 193000, Reward: [-345.791 -345.791 -345.791] [63.1473], Avg: [-379.384 -379.384 -379.384] (1.000)
Step: 194000, Reward: [-348.968 -348.968 -348.968] [39.8251], Avg: [-379.227 -379.227 -379.227] (1.000)
Step: 195000, Reward: [-354.937 -354.937 -354.937] [69.2855], Avg: [-379.102 -379.102 -379.102] (1.000)
Step: 196000, Reward: [-362.439 -362.439 -362.439] [62.3338], Avg: [-379.017 -379.017 -379.017] (1.000)
Step: 197000, Reward: [-340.137 -340.137 -340.137] [75.9483], Avg: [-378.82 -378.82 -378.82] (1.000)
Step: 198000, Reward: [-347.742 -347.742 -347.742] [51.2960], Avg: [-378.663 -378.663 -378.663] (1.000)
Step: 199000, Reward: [-349.49 -349.49 -349.49] [63.9977], Avg: [-378.516 -378.516 -378.516] (1.000)
