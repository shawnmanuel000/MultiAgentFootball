Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiHeadAttention

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.attn = MultiHeadAttention(state_size[-1], 4, 32)
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.norm1 = torch.nn.LayerNorm(INPUT_LAYER)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm3 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.recurrent = torch.nn.LSTMCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		state = self.attn(state)
		state = (self.layer1(state)).relu()
		state = (self.layer2(state)).relu()
		state = (self.layer3(state)).relu()
		if self.hidden[0].size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		self.hidden = self.recurrent(state, self.hidden)
		action_mu = self.action_mu(self.hidden[0])
		action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = [torch.zeros([batch_size, ACTOR_HIDDEN]).to(device) for _ in range(2)]

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.attn = MultiHeadAttention(state_size[-1], 4, 32)
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.norm1 = torch.nn.LayerNorm(INPUT_LAYER)
		self.norm2 = torch.nn.LayerNorm(CRITIC_HIDDEN)
		self.norm3 = torch.nn.LayerNorm(CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		state = self.attn(state)
		state = (self.layer1(state)).relu()
		state = (self.layer2(state)).relu()
		state = (self.layer3(state)).relu()
		value = self.value(state)
		value = value.reshape(*out_dims, value.shape[-1])
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.actor = lambda s,a: MAPPOActor(state_size[0], action_size[0])
		self.critic = MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, self.actor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, states_joint, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):
			values = model.get_value(states_joint[:-1], grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			entropy, new_log_prob = model.get_action_probs(state[:-1], action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "mappo", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		# [x.eval() for x in [*[s.actor_local for s in self.network.models], self.network.critic]]
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			# [x.train() for x in [self.network.actor, self.network.critic]]
			# [x.train() for x in [*[s.actor_local for s in self.network.models], self.network.critic]]
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			self.network.optimize(states, actions, log_probs, states_joint, targets, advantages)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, save_at=100, checkpoint=True, save_best=False, log=True, render=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, num_envs=envs.num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%1000==0:#np.any(dones[0]):
			rollouts = rollout(envs, agent, render=True)
			total_rewards.append(np.mean(rollouts, axis=-1) - np.std(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]+np.std(rollouts, axis=-1)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 0, Reward: [-522.368 -522.368 -522.368] [69.1404], Avg: [-591.508 -591.508 -591.508] (1.000)
Step: 1000, Reward: [-631.212 -631.212 -631.212] [135.3677], Avg: [-679.044 -679.044 -679.044] (1.000)
Step: 2000, Reward: [-936.43 -936.43 -936.43] [221.4980], Avg: [-838.672 -838.672 -838.672] (1.000)
Step: 3000, Reward: [-1160.803 -1160.803 -1160.803] [342.1625], Avg: [-1004.745 -1004.745 -1004.745] (1.000)
Step: 4000, Reward: [-1238.927 -1238.927 -1238.927] [267.8161], Avg: [-1105.145 -1105.145 -1105.145] (1.000)
Step: 5000, Reward: [-945.291 -945.291 -945.291] [211.1569], Avg: [-1113.695 -1113.695 -1113.695] (1.000)
Step: 6000, Reward: [-804.531 -804.531 -804.531] [245.6928], Avg: [-1104.628 -1104.628 -1104.628] (1.000)
Step: 7000, Reward: [-641.997 -641.997 -641.997] [176.6141], Avg: [-1068.876 -1068.876 -1068.876] (1.000)
Step: 8000, Reward: [-713.879 -713.879 -713.879] [158.6192], Avg: [-1047.056 -1047.056 -1047.056] (1.000)
Step: 9000, Reward: [-732.231 -732.231 -732.231] [159.9833], Avg: [-1031.572 -1031.572 -1031.572] (1.000)
Step: 10000, Reward: [-757.203 -757.203 -757.203] [178.5194], Avg: [-1022.858 -1022.858 -1022.858] (1.000)
Step: 11000, Reward: [-789.408 -789.408 -789.408] [204.3795], Avg: [-1020.436 -1020.436 -1020.436] (1.000)
Step: 12000, Reward: [-754.272 -754.272 -754.272] [181.6226], Avg: [-1013.933 -1013.933 -1013.933] (1.000)
Step: 13000, Reward: [-742.906 -742.906 -742.906] [159.4394], Avg: [-1005.962 -1005.962 -1005.962] (1.000)
Step: 14000, Reward: [-822.474 -822.474 -822.474] [162.3703], Avg: [-1004.554 -1004.554 -1004.554] (1.000)
Step: 15000, Reward: [-842.112 -842.112 -842.112] [212.4180], Avg: [-1007.678 -1007.678 -1007.678] (1.000)
Step: 16000, Reward: [-818.383 -818.383 -818.383] [194.8660], Avg: [-1008.005 -1008.005 -1008.005] (1.000)
Step: 17000, Reward: [-863.062 -863.062 -863.062] [188.9722], Avg: [-1010.452 -1010.452 -1010.452] (1.000)
Step: 18000, Reward: [-734.1 -734.1 -734.1] [184.0535], Avg: [-1005.594 -1005.594 -1005.594] (1.000)
Step: 19000, Reward: [-740.409 -740.409 -740.409] [179.9333], Avg: [-1001.331 -1001.331 -1001.331] (1.000)
Step: 20000, Reward: [-689.64 -689.64 -689.64] [183.3529], Avg: [-995.22 -995.22 -995.22] (1.000)
Step: 21000, Reward: [-756.304 -756.304 -756.304] [190.6929], Avg: [-993.028 -993.028 -993.028] (1.000)
Step: 22000, Reward: [-663.08 -663.08 -663.08] [140.7250], Avg: [-984.801 -984.801 -984.801] (1.000)
Step: 23000, Reward: [-746.992 -746.992 -746.992] [127.3959], Avg: [-980.2 -980.2 -980.2] (1.000)
Step: 24000, Reward: [-656.584 -656.584 -656.584] [128.5981], Avg: [-972.4 -972.4 -972.4] (1.000)
Step: 25000, Reward: [-668.988 -668.988 -668.988] [152.6449], Avg: [-966.601 -966.601 -966.601] (1.000)
Step: 26000, Reward: [-679.819 -679.819 -679.819] [180.9969], Avg: [-962.683 -962.683 -962.683] (1.000)
Step: 27000, Reward: [-652.976 -652.976 -652.976] [125.7203], Avg: [-956.112 -956.112 -956.112] (1.000)
Step: 28000, Reward: [-680.494 -680.494 -680.494] [151.3128], Avg: [-951.826 -951.826 -951.826] (1.000)
Step: 29000, Reward: [-691.838 -691.838 -691.838] [167.0837], Avg: [-948.729 -948.729 -948.729] (1.000)
Step: 30000, Reward: [-599.752 -599.752 -599.752] [144.1133], Avg: [-942.12 -942.12 -942.12] (1.000)
Step: 31000, Reward: [-710.484 -710.484 -710.484] [115.0976], Avg: [-938.478 -938.478 -938.478] (1.000)
Step: 32000, Reward: [-578.455 -578.455 -578.455] [102.9972], Avg: [-930.69 -930.69 -930.69] (1.000)
Step: 33000, Reward: [-649.415 -649.415 -649.415] [119.3314], Avg: [-925.927 -925.927 -925.927] (1.000)
Step: 34000, Reward: [-678.814 -678.814 -678.814] [160.6631], Avg: [-923.457 -923.457 -923.457] (1.000)
Step: 35000, Reward: [-721.263 -721.263 -721.263] [144.7750], Avg: [-921.862 -921.862 -921.862] (1.000)
Step: 36000, Reward: [-631.006 -631.006 -631.006] [161.5449], Avg: [-918.367 -918.367 -918.367] (1.000)
Step: 37000, Reward: [-642.075 -642.075 -642.075] [120.1516], Avg: [-914.258 -914.258 -914.258] (1.000)
Step: 38000, Reward: [-709.265 -709.265 -709.265] [176.8941], Avg: [-913.537 -913.537 -913.537] (1.000)
Step: 39000, Reward: [-724.05 -724.05 -724.05] [170.3652], Avg: [-913.059 -913.059 -913.059] (1.000)
Step: 40000, Reward: [-674.832 -674.832 -674.832] [162.5534], Avg: [-911.214 -911.214 -911.214] (1.000)
Step: 41000, Reward: [-668.109 -668.109 -668.109] [193.9264], Avg: [-910.043 -910.043 -910.043] (1.000)
Step: 42000, Reward: [-726.233 -726.233 -726.233] [162.4372], Avg: [-909.546 -909.546 -909.546] (1.000)
Step: 43000, Reward: [-740.081 -740.081 -740.081] [199.4504], Avg: [-910.227 -910.227 -910.227] (1.000)
Step: 44000, Reward: [-661.283 -661.283 -661.283] [129.9631], Avg: [-907.583 -907.583 -907.583] (1.000)
Step: 45000, Reward: [-719.983 -719.983 -719.983] [140.6726], Avg: [-906.563 -906.563 -906.563] (1.000)
Step: 46000, Reward: [-721.455 -721.455 -721.455] [163.1911], Avg: [-906.097 -906.097 -906.097] (1.000)
Step: 47000, Reward: [-656.891 -656.891 -656.891] [130.8061], Avg: [-903.63 -903.63 -903.63] (1.000)
Step: 48000, Reward: [-701.495 -701.495 -701.495] [111.9718], Avg: [-901.79 -901.79 -901.79] (1.000)
Step: 49000, Reward: [-681.082 -681.082 -681.082] [113.6218], Avg: [-899.648 -899.648 -899.648] (1.000)
Step: 50000, Reward: [-702.336 -702.336 -702.336] [113.8639], Avg: [-898.012 -898.012 -898.012] (1.000)
Step: 51000, Reward: [-715.523 -715.523 -715.523] [164.8721], Avg: [-897.673 -897.673 -897.673] (1.000)
Step: 52000, Reward: [-669.742 -669.742 -669.742] [170.3818], Avg: [-896.587 -896.587 -896.587] (1.000)
Step: 53000, Reward: [-799.549 -799.549 -799.549] [194.3753], Avg: [-898.39 -898.39 -898.39] (1.000)
Step: 54000, Reward: [-643.395 -643.395 -643.395] [141.0207], Avg: [-896.318 -896.318 -896.318] (1.000)
Step: 55000, Reward: [-719.246 -719.246 -719.246] [185.6507], Avg: [-896.471 -896.471 -896.471] (1.000)
Step: 56000, Reward: [-757.163 -757.163 -757.163] [143.4147], Avg: [-896.543 -896.543 -896.543] (1.000)
Step: 57000, Reward: [-683.322 -683.322 -683.322] [167.9062], Avg: [-895.762 -895.762 -895.762] (1.000)
Step: 58000, Reward: [-712.353 -712.353 -712.353] [151.1715], Avg: [-895.215 -895.215 -895.215] (1.000)
Step: 59000, Reward: [-679.007 -679.007 -679.007] [135.5564], Avg: [-893.871 -893.871 -893.871] (1.000)
Step: 60000, Reward: [-731.757 -731.757 -731.757] [166.9842], Avg: [-893.951 -893.951 -893.951] (1.000)
Step: 61000, Reward: [-791.062 -791.062 -791.062] [206.4204], Avg: [-895.621 -895.621 -895.621] (1.000)
Step: 62000, Reward: [-796.185 -796.185 -796.185] [205.2355], Avg: [-897.3 -897.3 -897.3] (1.000)
Step: 63000, Reward: [-821.891 -821.891 -821.891] [160.1976], Avg: [-898.625 -898.625 -898.625] (1.000)
Step: 64000, Reward: [-800.884 -800.884 -800.884] [153.1906], Avg: [-899.478 -899.478 -899.478] (1.000)
Step: 65000, Reward: [-828.413 -828.413 -828.413] [163.1530], Avg: [-900.873 -900.873 -900.873] (1.000)
Step: 66000, Reward: [-757.378 -757.378 -757.378] [142.2891], Avg: [-900.855 -900.855 -900.855] (1.000)
Step: 67000, Reward: [-655.718 -655.718 -655.718] [208.9259], Avg: [-900.323 -900.323 -900.323] (1.000)
Step: 68000, Reward: [-702.016 -702.016 -702.016] [172.3872], Avg: [-899.947 -899.947 -899.947] (1.000)
Step: 69000, Reward: [-761.094 -761.094 -761.094] [171.0921], Avg: [-900.408 -900.408 -900.408] (1.000)
Step: 70000, Reward: [-639.672 -639.672 -639.672] [112.4988], Avg: [-898.32 -898.32 -898.32] (1.000)
Step: 71000, Reward: [-642.666 -642.666 -642.666] [168.4555], Avg: [-897.109 -897.109 -897.109] (1.000)
Step: 72000, Reward: [-619.124 -619.124 -619.124] [127.7341], Avg: [-895.05 -895.05 -895.05] (1.000)
Step: 73000, Reward: [-680.558 -680.558 -680.558] [120.4958], Avg: [-893.78 -893.78 -893.78] (1.000)
Step: 74000, Reward: [-644.18 -644.18 -644.18] [130.9930], Avg: [-892.199 -892.199 -892.199] (1.000)
Step: 75000, Reward: [-663.495 -663.495 -663.495] [122.6210], Avg: [-890.803 -890.803 -890.803] (1.000)
Step: 76000, Reward: [-691.882 -691.882 -691.882] [160.9641], Avg: [-890.31 -890.31 -890.31] (1.000)
Step: 77000, Reward: [-640.73 -640.73 -640.73] [156.8891], Avg: [-889.122 -889.122 -889.122] (1.000)
Step: 78000, Reward: [-740.263 -740.263 -740.263] [153.3981], Avg: [-889.179 -889.179 -889.179] (1.000)
Step: 79000, Reward: [-745.495 -745.495 -745.495] [126.1214], Avg: [-888.96 -888.96 -888.96] (1.000)
Step: 80000, Reward: [-763.662 -763.662 -763.662] [179.6208], Avg: [-889.63 -889.63 -889.63] (1.000)
Step: 81000, Reward: [-733.599 -733.599 -733.599] [122.8512], Avg: [-889.226 -889.226 -889.226] (1.000)
Step: 82000, Reward: [-737.972 -737.972 -737.972] [172.6456], Avg: [-889.483 -889.483 -889.483] (1.000)
Step: 83000, Reward: [-721.475 -721.475 -721.475] [151.1974], Avg: [-889.283 -889.283 -889.283] (1.000)
Step: 84000, Reward: [-697.081 -697.081 -697.081] [94.2526], Avg: [-888.131 -888.131 -888.131] (1.000)
Step: 85000, Reward: [-759.15 -759.15 -759.15] [211.4595], Avg: [-889.09 -889.09 -889.09] (1.000)
Step: 86000, Reward: [-818.05 -818.05 -818.05] [152.0005], Avg: [-890.021 -890.021 -890.021] (1.000)
Step: 87000, Reward: [-725.17 -725.17 -725.17] [175.4100], Avg: [-890.141 -890.141 -890.141] (1.000)
Step: 88000, Reward: [-757.878 -757.878 -757.878] [178.9286], Avg: [-890.665 -890.665 -890.665] (1.000)
Step: 89000, Reward: [-741.582 -741.582 -741.582] [156.7387], Avg: [-890.75 -890.75 -890.75] (1.000)
Step: 90000, Reward: [-745.055 -745.055 -745.055] [175.5858], Avg: [-891.078 -891.078 -891.078] (1.000)
Step: 91000, Reward: [-649.802 -649.802 -649.802] [153.5903], Avg: [-890.125 -890.125 -890.125] (1.000)
Step: 92000, Reward: [-686.149 -686.149 -686.149] [138.9830], Avg: [-889.426 -889.426 -889.426] (1.000)
Step: 93000, Reward: [-749.723 -749.723 -749.723] [138.1613], Avg: [-889.41 -889.41 -889.41] (1.000)
Step: 94000, Reward: [-670.469 -670.469 -670.469] [120.5950], Avg: [-888.375 -888.375 -888.375] (1.000)
Step: 95000, Reward: [-617.99 -617.99 -617.99] [109.6219], Avg: [-886.7 -886.7 -886.7] (1.000)
Step: 96000, Reward: [-682.889 -682.889 -682.889] [149.2395], Avg: [-886.138 -886.138 -886.138] (1.000)
Step: 97000, Reward: [-742.737 -742.737 -742.737] [184.9210], Avg: [-886.561 -886.561 -886.561] (1.000)
Step: 98000, Reward: [-711.035 -711.035 -711.035] [184.0175], Avg: [-886.647 -886.647 -886.647] (1.000)
Step: 99000, Reward: [-696.323 -696.323 -696.323] [96.6941], Avg: [-885.711 -885.711 -885.711] (1.000)
Step: 100000, Reward: [-731.642 -731.642 -731.642] [154.3863], Avg: [-885.714 -885.714 -885.714] (1.000)
Step: 101000, Reward: [-658.515 -658.515 -658.515] [127.2250], Avg: [-884.734 -884.734 -884.734] (1.000)
Step: 102000, Reward: [-621.425 -621.425 -621.425] [121.8008], Avg: [-883.36 -883.36 -883.36] (1.000)
Step: 103000, Reward: [-737.339 -737.339 -737.339] [135.2387], Avg: [-883.256 -883.256 -883.256] (1.000)
Step: 104000, Reward: [-709.431 -709.431 -709.431] [163.5366], Avg: [-883.158 -883.158 -883.158] (1.000)
Step: 105000, Reward: [-776.168 -776.168 -776.168] [245.2943], Avg: [-884.463 -884.463 -884.463] (1.000)
Step: 106000, Reward: [-724.24 -724.24 -724.24] [146.8344], Avg: [-884.338 -884.338 -884.338] (1.000)
Step: 107000, Reward: [-676.678 -676.678 -676.678] [139.4578], Avg: [-883.706 -883.706 -883.706] (1.000)
Step: 108000, Reward: [-743.365 -743.365 -743.365] [125.6373], Avg: [-883.571 -883.571 -883.571] (1.000)
Step: 109000, Reward: [-763.034 -763.034 -763.034] [162.0888], Avg: [-883.949 -883.949 -883.949] (1.000)
Step: 110000, Reward: [-759.092 -759.092 -759.092] [141.4588], Avg: [-884.099 -884.099 -884.099] (1.000)
Step: 111000, Reward: [-770.25 -770.25 -770.25] [234.4467], Avg: [-885.176 -885.176 -885.176] (1.000)
Step: 112000, Reward: [-757.052 -757.052 -757.052] [175.8205], Avg: [-885.598 -885.598 -885.598] (1.000)
Step: 113000, Reward: [-831.969 -831.969 -831.969] [213.4681], Avg: [-887. -887. -887.] (1.000)
Step: 114000, Reward: [-838.097 -838.097 -838.097] [196.6870], Avg: [-888.285 -888.285 -888.285] (1.000)
Step: 115000, Reward: [-778.539 -778.539 -778.539] [194.7509], Avg: [-889.018 -889.018 -889.018] (1.000)
Step: 116000, Reward: [-732.436 -732.436 -732.436] [175.6893], Avg: [-889.181 -889.181 -889.181] (1.000)
Step: 117000, Reward: [-753.659 -753.659 -753.659] [193.3201], Avg: [-889.671 -889.671 -889.671] (1.000)
Step: 118000, Reward: [-716.8 -716.8 -716.8] [184.9967], Avg: [-889.773 -889.773 -889.773] (1.000)
Step: 119000, Reward: [-726.97 -726.97 -726.97] [144.5732], Avg: [-889.621 -889.621 -889.621] (1.000)
Step: 120000, Reward: [-681.917 -681.917 -681.917] [145.1348], Avg: [-889.104 -889.104 -889.104] (1.000)
Step: 121000, Reward: [-839.979 -839.979 -839.979] [119.7973], Avg: [-889.683 -889.683 -889.683] (1.000)
Step: 122000, Reward: [-735.21 -735.21 -735.21] [155.5216], Avg: [-889.691 -889.691 -889.691] (1.000)
Step: 123000, Reward: [-759.037 -759.037 -759.037] [169.3586], Avg: [-890.004 -890.004 -890.004] (1.000)
Step: 124000, Reward: [-773.527 -773.527 -773.527] [154.8681], Avg: [-890.311 -890.311 -890.311] (1.000)
Step: 125000, Reward: [-793.969 -793.969 -793.969] [177.6544], Avg: [-890.956 -890.956 -890.956] (1.000)
Step: 126000, Reward: [-807.488 -807.488 -807.488] [180.4105], Avg: [-891.719 -891.719 -891.719] (1.000)
Step: 127000, Reward: [-851.941 -851.941 -851.941] [130.0834], Avg: [-892.425 -892.425 -892.425] (1.000)
Step: 128000, Reward: [-851.518 -851.518 -851.518] [222.7196], Avg: [-893.834 -893.834 -893.834] (1.000)
Step: 129000, Reward: [-830.573 -830.573 -830.573] [167.4567], Avg: [-894.636 -894.636 -894.636] (1.000)
Step: 130000, Reward: [-873.608 -873.608 -873.608] [175.0584], Avg: [-895.812 -895.812 -895.812] (1.000)
Step: 131000, Reward: [-855.091 -855.091 -855.091] [194.8515], Avg: [-896.979 -896.979 -896.979] (1.000)
Step: 132000, Reward: [-864.334 -864.334 -864.334] [201.0149], Avg: [-898.245 -898.245 -898.245] (1.000)
Step: 133000, Reward: [-767.186 -767.186 -767.186] [176.9022], Avg: [-898.587 -898.587 -898.587] (1.000)
Step: 134000, Reward: [-692.342 -692.342 -692.342] [130.7780], Avg: [-898.028 -898.028 -898.028] (1.000)
Step: 135000, Reward: [-678.127 -678.127 -678.127] [160.2488], Avg: [-897.59 -897.59 -897.59] (1.000)
Step: 136000, Reward: [-712.677 -712.677 -712.677] [184.8723], Avg: [-897.589 -897.589 -897.589] (1.000)
Step: 137000, Reward: [-696.965 -696.965 -696.965] [180.3008], Avg: [-897.442 -897.442 -897.442] (1.000)
Step: 138000, Reward: [-737.914 -737.914 -737.914] [205.6327], Avg: [-897.774 -897.774 -897.774] (1.000)
Step: 139000, Reward: [-699.921 -699.921 -699.921] [177.6759], Avg: [-897.63 -897.63 -897.63] (1.000)
Step: 140000, Reward: [-679.202 -679.202 -679.202] [168.1434], Avg: [-897.273 -897.273 -897.273] (1.000)
Step: 141000, Reward: [-746.392 -746.392 -746.392] [218.8864], Avg: [-897.752 -897.752 -897.752] (1.000)
Step: 142000, Reward: [-682.102 -682.102 -682.102] [179.8159], Avg: [-897.501 -897.501 -897.501] (1.000)
Step: 143000, Reward: [-730.138 -730.138 -730.138] [167.6561], Avg: [-897.503 -897.503 -897.503] (1.000)
Step: 144000, Reward: [-747.509 -747.509 -747.509] [140.2799], Avg: [-897.436 -897.436 -897.436] (1.000)
Step: 145000, Reward: [-704.564 -704.564 -704.564] [158.4540], Avg: [-897.201 -897.201 -897.201] (1.000)
Step: 146000, Reward: [-737.268 -737.268 -737.268] [183.0591], Avg: [-897.358 -897.358 -897.358] (1.000)
Step: 147000, Reward: [-713.925 -713.925 -713.925] [159.0974], Avg: [-897.194 -897.194 -897.194] (1.000)
Step: 148000, Reward: [-700.662 -700.662 -700.662] [148.4909], Avg: [-896.871 -896.871 -896.871] (1.000)
Step: 149000, Reward: [-690.361 -690.361 -690.361] [125.7238], Avg: [-896.333 -896.333 -896.333] (1.000)
Step: 150000, Reward: [-752.6 -752.6 -752.6] [144.9733], Avg: [-896.341 -896.341 -896.341] (1.000)
Step: 151000, Reward: [-800.481 -800.481 -800.481] [180.3357], Avg: [-896.897 -896.897 -896.897] (1.000)
Step: 152000, Reward: [-763.076 -763.076 -763.076] [158.6866], Avg: [-897.059 -897.059 -897.059] (1.000)
Step: 153000, Reward: [-724.224 -724.224 -724.224] [162.7595], Avg: [-896.994 -896.994 -896.994] (1.000)
Step: 154000, Reward: [-728.37 -728.37 -728.37] [179.6225], Avg: [-897.065 -897.065 -897.065] (1.000)
Step: 155000, Reward: [-761.443 -761.443 -761.443] [189.2701], Avg: [-897.408 -897.408 -897.408] (1.000)
Step: 156000, Reward: [-749.364 -749.364 -749.364] [165.9392], Avg: [-897.522 -897.522 -897.522] (1.000)
Step: 157000, Reward: [-708.46 -708.46 -708.46] [163.2986], Avg: [-897.359 -897.359 -897.359] (1.000)
Step: 158000, Reward: [-736.13 -736.13 -736.13] [220.1646], Avg: [-897.73 -897.73 -897.73] (1.000)
Step: 159000, Reward: [-726.081 -726.081 -726.081] [223.6761], Avg: [-898.055 -898.055 -898.055] (1.000)
Step: 160000, Reward: [-768.1 -768.1 -768.1] [207.5393], Avg: [-898.537 -898.537 -898.537] (1.000)
Step: 161000, Reward: [-721.899 -721.899 -721.899] [192.3892], Avg: [-898.634 -898.634 -898.634] (1.000)
Step: 162000, Reward: [-779.491 -779.491 -779.491] [197.4739], Avg: [-899.115 -899.115 -899.115] (1.000)
Step: 163000, Reward: [-707.424 -707.424 -707.424] [183.6104], Avg: [-899.066 -899.066 -899.066] (1.000)
Step: 164000, Reward: [-720.455 -720.455 -720.455] [157.3216], Avg: [-898.937 -898.937 -898.937] (1.000)
Step: 165000, Reward: [-695.673 -695.673 -695.673] [177.4657], Avg: [-898.781 -898.781 -898.781] (1.000)
Step: 166000, Reward: [-753.491 -753.491 -753.491] [132.5782], Avg: [-898.705 -898.705 -898.705] (1.000)
Step: 167000, Reward: [-736.196 -736.196 -736.196] [122.3226], Avg: [-898.466 -898.466 -898.466] (1.000)
Step: 168000, Reward: [-774.256 -774.256 -774.256] [151.0843], Avg: [-898.625 -898.625 -898.625] (1.000)
Step: 169000, Reward: [-682.798 -682.798 -682.798] [169.7716], Avg: [-898.354 -898.354 -898.354] (1.000)
Step: 170000, Reward: [-686.355 -686.355 -686.355] [163.9969], Avg: [-898.073 -898.073 -898.073] (1.000)
Step: 171000, Reward: [-650.228 -650.228 -650.228] [177.9549], Avg: [-897.667 -897.667 -897.667] (1.000)
Step: 172000, Reward: [-649.288 -649.288 -649.288] [117.4743], Avg: [-896.91 -896.91 -896.91] (1.000)
Step: 173000, Reward: [-648.287 -648.287 -648.287] [157.2043], Avg: [-896.385 -896.385 -896.385] (1.000)
Step: 174000, Reward: [-676.45 -676.45 -676.45] [127.4894], Avg: [-895.857 -895.857 -895.857] (1.000)
Step: 175000, Reward: [-680.686 -680.686 -680.686] [136.4166], Avg: [-895.409 -895.409 -895.409] (1.000)
Step: 176000, Reward: [-650.205 -650.205 -650.205] [132.1578], Avg: [-894.77 -894.77 -894.77] (1.000)
Step: 177000, Reward: [-597.796 -597.796 -597.796] [154.1664], Avg: [-893.968 -893.968 -893.968] (1.000)
Step: 178000, Reward: [-723.852 -723.852 -723.852] [161.3097], Avg: [-893.919 -893.919 -893.919] (1.000)
Step: 179000, Reward: [-680.429 -680.429 -680.429] [138.1212], Avg: [-893.5 -893.5 -893.5] (1.000)
Step: 180000, Reward: [-692.996 -692.996 -692.996] [204.3689], Avg: [-893.522 -893.522 -893.522] (1.000)
Step: 181000, Reward: [-717.63 -717.63 -717.63] [114.2856], Avg: [-893.183 -893.183 -893.183] (1.000)
Step: 182000, Reward: [-715.025 -715.025 -715.025] [164.6555], Avg: [-893.109 -893.109 -893.109] (1.000)
Step: 183000, Reward: [-719.954 -719.954 -719.954] [143.5313], Avg: [-892.948 -892.948 -892.948] (1.000)
Step: 184000, Reward: [-660.092 -660.092 -660.092] [120.2520], Avg: [-892.34 -892.34 -892.34] (1.000)
Step: 185000, Reward: [-698.316 -698.316 -698.316] [133.8492], Avg: [-892.016 -892.016 -892.016] (1.000)
Step: 186000, Reward: [-834.295 -834.295 -834.295] [200.4031], Avg: [-892.779 -892.779 -892.779] (1.000)
Step: 187000, Reward: [-785.993 -785.993 -785.993] [158.7771], Avg: [-893.056 -893.056 -893.056] (1.000)
Step: 188000, Reward: [-752.566 -752.566 -752.566] [181.9933], Avg: [-893.275 -893.275 -893.275] (1.000)
Step: 189000, Reward: [-742.266 -742.266 -742.266] [150.3852], Avg: [-893.272 -893.272 -893.272] (1.000)
Step: 190000, Reward: [-846.676 -846.676 -846.676] [208.4914], Avg: [-894.12 -894.12 -894.12] (1.000)
Step: 191000, Reward: [-776.932 -776.932 -776.932] [168.8501], Avg: [-894.389 -894.389 -894.389] (1.000)
Step: 192000, Reward: [-666.562 -666.562 -666.562] [150.8765], Avg: [-893.99 -893.99 -893.99] (1.000)
Step: 193000, Reward: [-701.079 -701.079 -701.079] [143.5315], Avg: [-893.735 -893.735 -893.735] (1.000)
Step: 194000, Reward: [-714.296 -714.296 -714.296] [221.1884], Avg: [-893.95 -893.95 -893.95] (1.000)
Step: 195000, Reward: [-777.976 -777.976 -777.976] [146.5719], Avg: [-894.106 -894.106 -894.106] (1.000)
Step: 196000, Reward: [-846.481 -846.481 -846.481] [165.7868], Avg: [-894.705 -894.705 -894.705] (1.000)
Step: 197000, Reward: [-771.714 -771.714 -771.714] [165.2416], Avg: [-894.919 -894.919 -894.919] (1.000)
Step: 198000, Reward: [-786.993 -786.993 -786.993] [189.7837], Avg: [-895.33 -895.33 -895.33] (1.000)
Step: 199000, Reward: [-767.775 -767.775 -767.775] [136.5946], Avg: [-895.375 -895.375 -895.375] (1.000)
