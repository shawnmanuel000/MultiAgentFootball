Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])], envs: <class 'utils.envs.EnvManager'>,

import torch
import random
import numpy as np
from models.ppo import PPONetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 640
PPO_EPOCHS = 5
TIME_BATCH = 10
MAX_BUFFER_SIZE = 640

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		# self.attention = MultiheadAttention(ACTOR_HIDDEN, 8, 32)
		# self.attention = torch.nn.modules.MultiheadAttention(1, 1)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()

		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		self.hidden = self.recurrent(state, self.hidden)
		action_mu = self.action_mu(self.hidden)
		action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])

		# action_mu = self.action_mu(state)

		action_probs = action_mu.softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action = dist.sample() if action is None else action.argmax(-1)
		action_one_hot = one_hot_from_indices(action, action_probs.size(-1))
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_one_hot, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: MAPPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			model.actor_local.init_hidden(state.size(0), state.device)
			entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy = torch.stack(entropy).mean()
			# entropy, new_log_prob = model.get_action_probs(state, action, grad=True, numpy=False)
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			# entropy = torch.stack(entropy).view(1, -1, *advantage.shape[2:])
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: list(zip(*[t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1) for t in x]))
			states, actions, log_probs, targets, advantages = [time_split(x) for x in [[s[:-1] for s in states], actions, log_probs, targets, advantages]]
			self.replay_buffer.extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.next_batch(BATCH_SIZE, lambda x: [torch.stack(l) for l in list(zip(*x))])
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env, reward_fn)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=1000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, load=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name), ports)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load=f"{env_name}" if load else "", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=False) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs))
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s>0 and s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (SelfPlayAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)


Step: 1000, Reward: [-528.291 -528.291 -528.291] [125.8287], Avg: [-528.291 -528.291 -528.291] (1.000)
Step: 2000, Reward: [-499.353 -499.353 -499.353] [136.3584], Avg: [-513.822 -513.822 -513.822] (1.000)
Step: 3000, Reward: [-497.82 -497.82 -497.82] [118.6197], Avg: [-508.488 -508.488 -508.488] (1.000)
Step: 4000, Reward: [-490.187 -490.187 -490.187] [95.9469], Avg: [-503.913 -503.913 -503.913] (1.000)
Step: 5000, Reward: [-538.942 -538.942 -538.942] [89.9125], Avg: [-510.918 -510.918 -510.918] (1.000)
Step: 6000, Reward: [-497.149 -497.149 -497.149] [78.6914], Avg: [-508.624 -508.624 -508.624] (1.000)
Step: 7000, Reward: [-507.531 -507.531 -507.531] [88.5814], Avg: [-508.468 -508.468 -508.468] (1.000)
Step: 8000, Reward: [-469.69 -469.69 -469.69] [68.5861], Avg: [-503.62 -503.62 -503.62] (1.000)
Step: 9000, Reward: [-445.666 -445.666 -445.666] [74.7779], Avg: [-497.181 -497.181 -497.181] (1.000)
Step: 10000, Reward: [-453.763 -453.763 -453.763] [93.8805], Avg: [-492.839 -492.839 -492.839] (1.000)
Step: 11000, Reward: [-476.766 -476.766 -476.766] [73.3161], Avg: [-491.378 -491.378 -491.378] (1.000)
Step: 12000, Reward: [-481.738 -481.738 -481.738] [98.4516], Avg: [-490.575 -490.575 -490.575] (1.000)
Step: 13000, Reward: [-466.537 -466.537 -466.537] [52.6334], Avg: [-488.726 -488.726 -488.726] (1.000)
Step: 14000, Reward: [-477.319 -477.319 -477.319] [68.6482], Avg: [-487.911 -487.911 -487.911] (1.000)
Step: 15000, Reward: [-495.511 -495.511 -495.511] [93.3888], Avg: [-488.418 -488.418 -488.418] (1.000)
Step: 16000, Reward: [-499.864 -499.864 -499.864] [101.3077], Avg: [-489.133 -489.133 -489.133] (1.000)
Step: 17000, Reward: [-473.572 -473.572 -473.572] [92.9739], Avg: [-488.218 -488.218 -488.218] (1.000)
Step: 18000, Reward: [-422.714 -422.714 -422.714] [91.3509], Avg: [-484.578 -484.578 -484.578] (1.000)
Step: 19000, Reward: [-481.659 -481.659 -481.659] [66.8634], Avg: [-484.425 -484.425 -484.425] (1.000)
Step: 20000, Reward: [-450.269 -450.269 -450.269] [78.5447], Avg: [-482.717 -482.717 -482.717] (1.000)
Step: 21000, Reward: [-413.464 -413.464 -413.464] [52.5390], Avg: [-479.419 -479.419 -479.419] (1.000)
Step: 22000, Reward: [-453.724 -453.724 -453.724] [81.3051], Avg: [-478.251 -478.251 -478.251] (1.000)
Step: 23000, Reward: [-462.086 -462.086 -462.086] [85.8037], Avg: [-477.548 -477.548 -477.548] (1.000)
Step: 24000, Reward: [-413.125 -413.125 -413.125] [59.4219], Avg: [-474.864 -474.864 -474.864] (1.000)
Step: 25000, Reward: [-429.038 -429.038 -429.038] [47.7703], Avg: [-473.031 -473.031 -473.031] (1.000)
Step: 26000, Reward: [-421.009 -421.009 -421.009] [64.4408], Avg: [-471.03 -471.03 -471.03] (1.000)
Step: 27000, Reward: [-422.404 -422.404 -422.404] [50.8016], Avg: [-469.229 -469.229 -469.229] (1.000)
Step: 28000, Reward: [-399.905 -399.905 -399.905] [85.5184], Avg: [-466.753 -466.753 -466.753] (1.000)
Step: 29000, Reward: [-416.156 -416.156 -416.156] [62.6492], Avg: [-465.009 -465.009 -465.009] (1.000)
Step: 30000, Reward: [-421.97 -421.97 -421.97] [51.8274], Avg: [-463.574 -463.574 -463.574] (1.000)
Step: 31000, Reward: [-449.414 -449.414 -449.414] [62.3584], Avg: [-463.117 -463.117 -463.117] (1.000)
Step: 32000, Reward: [-471.454 -471.454 -471.454] [114.9628], Avg: [-463.378 -463.378 -463.378] (1.000)
Step: 33000, Reward: [-389.144 -389.144 -389.144] [70.2321], Avg: [-461.128 -461.128 -461.128] (1.000)
Step: 34000, Reward: [-386.593 -386.593 -386.593] [76.9250], Avg: [-458.936 -458.936 -458.936] (1.000)
Step: 35000, Reward: [-378.733 -378.733 -378.733] [79.1242], Avg: [-456.645 -456.645 -456.645] (1.000)
Step: 36000, Reward: [-421.415 -421.415 -421.415] [67.0435], Avg: [-455.666 -455.666 -455.666] (1.000)
Step: 37000, Reward: [-429.183 -429.183 -429.183] [66.3605], Avg: [-454.95 -454.95 -454.95] (1.000)
Step: 38000, Reward: [-412.524 -412.524 -412.524] [85.5145], Avg: [-453.834 -453.834 -453.834] (1.000)
Step: 39000, Reward: [-415.365 -415.365 -415.365] [76.4977], Avg: [-452.847 -452.847 -452.847] (1.000)
Step: 40000, Reward: [-457.394 -457.394 -457.394] [76.6118], Avg: [-452.961 -452.961 -452.961] (1.000)
Step: 41000, Reward: [-431.246 -431.246 -431.246] [109.5439], Avg: [-452.431 -452.431 -452.431] (1.000)
Step: 42000, Reward: [-414.652 -414.652 -414.652] [99.7374], Avg: [-451.532 -451.532 -451.532] (1.000)
Step: 43000, Reward: [-410.532 -410.532 -410.532] [67.4148], Avg: [-450.578 -450.578 -450.578] (1.000)
Step: 44000, Reward: [-468.874 -468.874 -468.874] [110.3120], Avg: [-450.994 -450.994 -450.994] (1.000)
Step: 45000, Reward: [-448.057 -448.057 -448.057] [105.6475], Avg: [-450.929 -450.929 -450.929] (1.000)
Step: 46000, Reward: [-473.762 -473.762 -473.762] [94.7757], Avg: [-451.425 -451.425 -451.425] (1.000)
Step: 47000, Reward: [-420.639 -420.639 -420.639] [108.1218], Avg: [-450.77 -450.77 -450.77] (1.000)
Step: 48000, Reward: [-415.755 -415.755 -415.755] [96.6903], Avg: [-450.041 -450.041 -450.041] (1.000)
Step: 49000, Reward: [-415.764 -415.764 -415.764] [91.4054], Avg: [-449.341 -449.341 -449.341] (1.000)
Step: 50000, Reward: [-415.111 -415.111 -415.111] [111.9382], Avg: [-448.657 -448.657 -448.657] (1.000)
Step: 51000, Reward: [-457.652 -457.652 -457.652] [110.5590], Avg: [-448.833 -448.833 -448.833] (1.000)
Step: 52000, Reward: [-443.548 -443.548 -443.548] [108.7211], Avg: [-448.731 -448.731 -448.731] (1.000)
Step: 53000, Reward: [-472.715 -472.715 -472.715] [113.5438], Avg: [-449.184 -449.184 -449.184] (1.000)
Step: 54000, Reward: [-421.916 -421.916 -421.916] [69.5469], Avg: [-448.679 -448.679 -448.679] (1.000)
Step: 55000, Reward: [-458.863 -458.863 -458.863] [91.8051], Avg: [-448.864 -448.864 -448.864] (1.000)
Step: 56000, Reward: [-427.284 -427.284 -427.284] [101.4422], Avg: [-448.479 -448.479 -448.479] (1.000)
Step: 57000, Reward: [-449.408 -449.408 -449.408] [86.2979], Avg: [-448.495 -448.495 -448.495] (1.000)
Step: 58000, Reward: [-403.355 -403.355 -403.355] [74.5136], Avg: [-447.717 -447.717 -447.717] (1.000)
Step: 59000, Reward: [-464.289 -464.289 -464.289] [94.0795], Avg: [-447.998 -447.998 -447.998] (1.000)
Step: 60000, Reward: [-451.104 -451.104 -451.104] [75.4972], Avg: [-448.049 -448.049 -448.049] (1.000)
Step: 61000, Reward: [-403.71 -403.71 -403.71] [61.3996], Avg: [-447.323 -447.323 -447.323] (1.000)
Step: 62000, Reward: [-421.567 -421.567 -421.567] [64.7111], Avg: [-446.907 -446.907 -446.907] (1.000)
Step: 63000, Reward: [-388.603 -388.603 -388.603] [119.6673], Avg: [-445.982 -445.982 -445.982] (1.000)
Step: 64000, Reward: [-409.659 -409.659 -409.659] [67.5860], Avg: [-445.414 -445.414 -445.414] (1.000)
Step: 65000, Reward: [-441.691 -441.691 -441.691] [94.1785], Avg: [-445.357 -445.357 -445.357] (1.000)
Step: 66000, Reward: [-457.686 -457.686 -457.686] [65.6780], Avg: [-445.544 -445.544 -445.544] (1.000)
Step: 67000, Reward: [-408.346 -408.346 -408.346] [82.9855], Avg: [-444.988 -444.988 -444.988] (1.000)
Step: 68000, Reward: [-439.228 -439.228 -439.228] [92.2051], Avg: [-444.904 -444.904 -444.904] (1.000)
Step: 69000, Reward: [-427.186 -427.186 -427.186] [84.9456], Avg: [-444.647 -444.647 -444.647] (1.000)
Step: 70000, Reward: [-434.452 -434.452 -434.452] [100.1359], Avg: [-444.501 -444.501 -444.501] (1.000)
Step: 71000, Reward: [-468.751 -468.751 -468.751] [76.6849], Avg: [-444.843 -444.843 -444.843] (1.000)
Step: 72000, Reward: [-402.174 -402.174 -402.174] [79.6437], Avg: [-444.25 -444.25 -444.25] (1.000)
Step: 73000, Reward: [-431.26 -431.26 -431.26] [55.7263], Avg: [-444.072 -444.072 -444.072] (1.000)
Step: 74000, Reward: [-531.297 -531.297 -531.297] [140.5052], Avg: [-445.251 -445.251 -445.251] (1.000)
Step: 75000, Reward: [-427.678 -427.678 -427.678] [117.3943], Avg: [-445.017 -445.017 -445.017] (1.000)
Step: 76000, Reward: [-455.342 -455.342 -455.342] [89.8885], Avg: [-445.153 -445.153 -445.153] (1.000)
Step: 77000, Reward: [-424.072 -424.072 -424.072] [73.9094], Avg: [-444.879 -444.879 -444.879] (1.000)
Step: 78000, Reward: [-389.139 -389.139 -389.139] [78.0963], Avg: [-444.164 -444.164 -444.164] (1.000)
Step: 79000, Reward: [-399.951 -399.951 -399.951] [64.8927], Avg: [-443.605 -443.605 -443.605] (1.000)
Step: 80000, Reward: [-427.877 -427.877 -427.877] [101.2488], Avg: [-443.408 -443.408 -443.408] (1.000)
Step: 81000, Reward: [-435.76 -435.76 -435.76] [117.0130], Avg: [-443.314 -443.314 -443.314] (1.000)
Step: 82000, Reward: [-420.565 -420.565 -420.565] [109.3152], Avg: [-443.036 -443.036 -443.036] (1.000)
Step: 83000, Reward: [-392.122 -392.122 -392.122] [83.1418], Avg: [-442.423 -442.423 -442.423] (1.000)
Step: 84000, Reward: [-434.205 -434.205 -434.205] [91.0162], Avg: [-442.325 -442.325 -442.325] (1.000)
Step: 85000, Reward: [-505.009 -505.009 -505.009] [89.1063], Avg: [-443.062 -443.062 -443.062] (1.000)
Step: 86000, Reward: [-407.626 -407.626 -407.626] [102.3320], Avg: [-442.65 -442.65 -442.65] (1.000)
Step: 87000, Reward: [-431.289 -431.289 -431.289] [69.6743], Avg: [-442.52 -442.52 -442.52] (1.000)
Step: 88000, Reward: [-430.056 -430.056 -430.056] [85.7501], Avg: [-442.378 -442.378 -442.378] (1.000)
Step: 89000, Reward: [-443.389 -443.389 -443.389] [84.8208], Avg: [-442.389 -442.389 -442.389] (1.000)
Step: 90000, Reward: [-465.98 -465.98 -465.98] [102.0445], Avg: [-442.652 -442.652 -442.652] (1.000)
Step: 91000, Reward: [-421.389 -421.389 -421.389] [101.9649], Avg: [-442.418 -442.418 -442.418] (1.000)
Step: 92000, Reward: [-461.49 -461.49 -461.49] [82.1761], Avg: [-442.625 -442.625 -442.625] (1.000)
Step: 93000, Reward: [-429.738 -429.738 -429.738] [82.0475], Avg: [-442.487 -442.487 -442.487] (1.000)
Step: 94000, Reward: [-446.022 -446.022 -446.022] [108.6796], Avg: [-442.524 -442.524 -442.524] (1.000)
Step: 95000, Reward: [-423.57 -423.57 -423.57] [111.1898], Avg: [-442.325 -442.325 -442.325] (1.000)
Step: 96000, Reward: [-403.491 -403.491 -403.491] [62.3836], Avg: [-441.92 -441.92 -441.92] (1.000)
Step: 97000, Reward: [-421.101 -421.101 -421.101] [92.3781], Avg: [-441.706 -441.706 -441.706] (1.000)
Step: 98000, Reward: [-459.413 -459.413 -459.413] [144.4191], Avg: [-441.886 -441.886 -441.886] (1.000)
Step: 99000, Reward: [-442.459 -442.459 -442.459] [125.6343], Avg: [-441.892 -441.892 -441.892] (1.000)
Step: 100000, Reward: [-396.971 -396.971 -396.971] [86.2139], Avg: [-441.443 -441.443 -441.443] (1.000)
Step: 101000, Reward: [-400.403 -400.403 -400.403] [96.2216], Avg: [-441.036 -441.036 -441.036] (1.000)
Step: 102000, Reward: [-416.064 -416.064 -416.064] [72.2710], Avg: [-440.792 -440.792 -440.792] (1.000)
Step: 103000, Reward: [-445.066 -445.066 -445.066] [107.7216], Avg: [-440.833 -440.833 -440.833] (1.000)
Step: 104000, Reward: [-443.99 -443.99 -443.99] [88.2223], Avg: [-440.863 -440.863 -440.863] (1.000)
Step: 105000, Reward: [-415.822 -415.822 -415.822] [85.8969], Avg: [-440.625 -440.625 -440.625] (1.000)
Step: 106000, Reward: [-405.075 -405.075 -405.075] [52.8047], Avg: [-440.29 -440.29 -440.29] (1.000)
Step: 107000, Reward: [-422.429 -422.429 -422.429] [104.0169], Avg: [-440.123 -440.123 -440.123] (1.000)
Step: 108000, Reward: [-475.003 -475.003 -475.003] [96.9159], Avg: [-440.446 -440.446 -440.446] (1.000)
Step: 109000, Reward: [-448.935 -448.935 -448.935] [111.0119], Avg: [-440.524 -440.524 -440.524] (1.000)
Step: 110000, Reward: [-442.357 -442.357 -442.357] [72.7632], Avg: [-440.54 -440.54 -440.54] (1.000)
Step: 111000, Reward: [-397.46 -397.46 -397.46] [68.8583], Avg: [-440.152 -440.152 -440.152] (1.000)
Step: 112000, Reward: [-450.945 -450.945 -450.945] [77.5769], Avg: [-440.248 -440.248 -440.248] (1.000)
Step: 113000, Reward: [-435.815 -435.815 -435.815] [102.4232], Avg: [-440.209 -440.209 -440.209] (1.000)
Step: 114000, Reward: [-463.639 -463.639 -463.639] [86.8117], Avg: [-440.415 -440.415 -440.415] (1.000)
Step: 115000, Reward: [-419.596 -419.596 -419.596] [137.3133], Avg: [-440.234 -440.234 -440.234] (1.000)
Step: 116000, Reward: [-424.017 -424.017 -424.017] [109.7832], Avg: [-440.094 -440.094 -440.094] (1.000)
Step: 117000, Reward: [-410.364 -410.364 -410.364] [98.5489], Avg: [-439.84 -439.84 -439.84] (1.000)
Step: 118000, Reward: [-465.749 -465.749 -465.749] [93.3232], Avg: [-440.059 -440.059 -440.059] (1.000)
Step: 119000, Reward: [-424.4 -424.4 -424.4] [95.8055], Avg: [-439.928 -439.928 -439.928] (1.000)
Step: 120000, Reward: [-420.953 -420.953 -420.953] [59.2722], Avg: [-439.77 -439.77 -439.77] (1.000)
Step: 121000, Reward: [-459.25 -459.25 -459.25] [68.5734], Avg: [-439.931 -439.931 -439.931] (1.000)
Step: 122000, Reward: [-417.749 -417.749 -417.749] [98.3009], Avg: [-439.749 -439.749 -439.749] (1.000)
Step: 123000, Reward: [-444.666 -444.666 -444.666] [113.6008], Avg: [-439.789 -439.789 -439.789] (1.000)
Step: 124000, Reward: [-417.34 -417.34 -417.34] [94.0544], Avg: [-439.608 -439.608 -439.608] (1.000)
Step: 125000, Reward: [-427.752 -427.752 -427.752] [80.6088], Avg: [-439.513 -439.513 -439.513] (1.000)
Step: 126000, Reward: [-468.104 -468.104 -468.104] [95.2098], Avg: [-439.74 -439.74 -439.74] (1.000)
Step: 127000, Reward: [-474.155 -474.155 -474.155] [107.3950], Avg: [-440.011 -440.011 -440.011] (1.000)
Step: 128000, Reward: [-439.25 -439.25 -439.25] [98.4360], Avg: [-440.005 -440.005 -440.005] (1.000)
Step: 129000, Reward: [-461.78 -461.78 -461.78] [95.2866], Avg: [-440.174 -440.174 -440.174] (1.000)
Step: 130000, Reward: [-395.236 -395.236 -395.236] [119.1882], Avg: [-439.828 -439.828 -439.828] (1.000)
Step: 131000, Reward: [-473.989 -473.989 -473.989] [119.6817], Avg: [-440.089 -440.089 -440.089] (1.000)
Step: 132000, Reward: [-414.591 -414.591 -414.591] [90.7212], Avg: [-439.896 -439.896 -439.896] (1.000)
Step: 133000, Reward: [-474.43 -474.43 -474.43] [111.6775], Avg: [-440.155 -440.155 -440.155] (1.000)
Step: 134000, Reward: [-388.537 -388.537 -388.537] [82.9155], Avg: [-439.77 -439.77 -439.77] (1.000)
Step: 135000, Reward: [-455.825 -455.825 -455.825] [91.0150], Avg: [-439.889 -439.889 -439.889] (1.000)
Step: 136000, Reward: [-414.125 -414.125 -414.125] [80.7105], Avg: [-439.7 -439.7 -439.7] (1.000)
Step: 137000, Reward: [-421.203 -421.203 -421.203] [91.7397], Avg: [-439.565 -439.565 -439.565] (1.000)
Step: 138000, Reward: [-406.016 -406.016 -406.016] [67.9165], Avg: [-439.321 -439.321 -439.321] (1.000)
Step: 139000, Reward: [-441.283 -441.283 -441.283] [89.1486], Avg: [-439.336 -439.336 -439.336] (1.000)
Step: 140000, Reward: [-417.472 -417.472 -417.472] [93.8481], Avg: [-439.179 -439.179 -439.179] (1.000)
Step: 141000, Reward: [-456.968 -456.968 -456.968] [87.5561], Avg: [-439.306 -439.306 -439.306] (1.000)
Step: 142000, Reward: [-454.583 -454.583 -454.583] [107.9483], Avg: [-439.413 -439.413 -439.413] (1.000)
Step: 143000, Reward: [-400.682 -400.682 -400.682] [82.7962], Avg: [-439.142 -439.142 -439.142] (1.000)
Step: 144000, Reward: [-399.627 -399.627 -399.627] [78.3014], Avg: [-438.868 -438.868 -438.868] (1.000)
Step: 145000, Reward: [-447.843 -447.843 -447.843] [81.0900], Avg: [-438.93 -438.93 -438.93] (1.000)
Step: 146000, Reward: [-431.253 -431.253 -431.253] [96.2966], Avg: [-438.877 -438.877 -438.877] (1.000)
Step: 147000, Reward: [-421.583 -421.583 -421.583] [73.8373], Avg: [-438.76 -438.76 -438.76] (1.000)
Step: 148000, Reward: [-449.923 -449.923 -449.923] [80.1683], Avg: [-438.835 -438.835 -438.835] (1.000)
Step: 149000, Reward: [-427.49 -427.49 -427.49] [114.5097], Avg: [-438.759 -438.759 -438.759] (1.000)
Step: 150000, Reward: [-446.476 -446.476 -446.476] [83.7216], Avg: [-438.81 -438.81 -438.81] (1.000)
Step: 151000, Reward: [-425.181 -425.181 -425.181] [93.7430], Avg: [-438.72 -438.72 -438.72] (1.000)
Step: 152000, Reward: [-390.085 -390.085 -390.085] [100.6757], Avg: [-438.4 -438.4 -438.4] (1.000)
Step: 153000, Reward: [-391.07 -391.07 -391.07] [91.8603], Avg: [-438.091 -438.091 -438.091] (1.000)
Step: 154000, Reward: [-447.504 -447.504 -447.504] [64.3576], Avg: [-438.152 -438.152 -438.152] (1.000)
Step: 155000, Reward: [-435.365 -435.365 -435.365] [86.7091], Avg: [-438.134 -438.134 -438.134] (1.000)
Step: 156000, Reward: [-463.682 -463.682 -463.682] [140.6782], Avg: [-438.298 -438.298 -438.298] (1.000)
Step: 157000, Reward: [-450.116 -450.116 -450.116] [71.1816], Avg: [-438.373 -438.373 -438.373] (1.000)
Step: 158000, Reward: [-453.106 -453.106 -453.106] [130.8998], Avg: [-438.466 -438.466 -438.466] (1.000)
Step: 159000, Reward: [-432.938 -432.938 -432.938] [93.3769], Avg: [-438.431 -438.431 -438.431] (1.000)
Step: 160000, Reward: [-401.701 -401.701 -401.701] [94.8540], Avg: [-438.202 -438.202 -438.202] (1.000)
Step: 161000, Reward: [-442.885 -442.885 -442.885] [106.7628], Avg: [-438.231 -438.231 -438.231] (1.000)
Step: 162000, Reward: [-473.576 -473.576 -473.576] [91.6961], Avg: [-438.449 -438.449 -438.449] (1.000)
Step: 163000, Reward: [-393.59 -393.59 -393.59] [78.2683], Avg: [-438.174 -438.174 -438.174] (1.000)
Step: 164000, Reward: [-506.46 -506.46 -506.46] [141.4662], Avg: [-438.59 -438.59 -438.59] (1.000)
Step: 165000, Reward: [-442.987 -442.987 -442.987] [96.9979], Avg: [-438.617 -438.617 -438.617] (1.000)
Step: 166000, Reward: [-412.07 -412.07 -412.07] [85.5232], Avg: [-438.457 -438.457 -438.457] (1.000)
Step: 167000, Reward: [-406.688 -406.688 -406.688] [83.0807], Avg: [-438.267 -438.267 -438.267] (1.000)
Step: 168000, Reward: [-442.478 -442.478 -442.478] [96.1882], Avg: [-438.292 -438.292 -438.292] (1.000)
Step: 169000, Reward: [-418.671 -418.671 -418.671] [81.9319], Avg: [-438.176 -438.176 -438.176] (1.000)
Step: 170000, Reward: [-422.074 -422.074 -422.074] [67.7686], Avg: [-438.081 -438.081 -438.081] (1.000)
Step: 171000, Reward: [-394.873 -394.873 -394.873] [59.0838], Avg: [-437.828 -437.828 -437.828] (1.000)
Step: 172000, Reward: [-437.456 -437.456 -437.456] [105.3283], Avg: [-437.826 -437.826 -437.826] (1.000)
Step: 173000, Reward: [-477.224 -477.224 -477.224] [88.8739], Avg: [-438.054 -438.054 -438.054] (1.000)
Step: 174000, Reward: [-441.473 -441.473 -441.473] [88.4289], Avg: [-438.074 -438.074 -438.074] (1.000)
Step: 175000, Reward: [-394.365 -394.365 -394.365] [88.0257], Avg: [-437.824 -437.824 -437.824] (1.000)
Step: 176000, Reward: [-465.145 -465.145 -465.145] [109.6635], Avg: [-437.979 -437.979 -437.979] (1.000)
Step: 177000, Reward: [-461.671 -461.671 -461.671] [110.9795], Avg: [-438.113 -438.113 -438.113] (1.000)
Step: 178000, Reward: [-406.054 -406.054 -406.054] [94.0117], Avg: [-437.933 -437.933 -437.933] (1.000)
Step: 179000, Reward: [-463.727 -463.727 -463.727] [95.8685], Avg: [-438.077 -438.077 -438.077] (1.000)
Step: 180000, Reward: [-437.928 -437.928 -437.928] [85.7332], Avg: [-438.076 -438.076 -438.076] (1.000)
Step: 181000, Reward: [-437.619 -437.619 -437.619] [77.9733], Avg: [-438.074 -438.074 -438.074] (1.000)
Step: 182000, Reward: [-397.193 -397.193 -397.193] [67.0045], Avg: [-437.849 -437.849 -437.849] (1.000)
Step: 183000, Reward: [-419.603 -419.603 -419.603] [114.8791], Avg: [-437.749 -437.749 -437.749] (1.000)
Step: 184000, Reward: [-429.452 -429.452 -429.452] [93.0161], Avg: [-437.704 -437.704 -437.704] (1.000)
Step: 185000, Reward: [-440.832 -440.832 -440.832] [96.9408], Avg: [-437.721 -437.721 -437.721] (1.000)
Step: 186000, Reward: [-465.777 -465.777 -465.777] [83.9405], Avg: [-437.872 -437.872 -437.872] (1.000)
Step: 187000, Reward: [-434.074 -434.074 -434.074] [65.1600], Avg: [-437.852 -437.852 -437.852] (1.000)
Step: 188000, Reward: [-433.699 -433.699 -433.699] [65.1904], Avg: [-437.829 -437.829 -437.829] (1.000)
Step: 189000, Reward: [-440.689 -440.689 -440.689] [99.9597], Avg: [-437.845 -437.845 -437.845] (1.000)
Step: 190000, Reward: [-446.85 -446.85 -446.85] [65.4241], Avg: [-437.892 -437.892 -437.892] (1.000)
Step: 191000, Reward: [-449.028 -449.028 -449.028] [81.2671], Avg: [-437.95 -437.95 -437.95] (1.000)
Step: 192000, Reward: [-450.546 -450.546 -450.546] [79.1128], Avg: [-438.016 -438.016 -438.016] (1.000)
Step: 193000, Reward: [-461.91 -461.91 -461.91] [94.3164], Avg: [-438.14 -438.14 -438.14] (1.000)
Step: 194000, Reward: [-422.555 -422.555 -422.555] [86.1934], Avg: [-438.059 -438.059 -438.059] (1.000)
Step: 195000, Reward: [-433.618 -433.618 -433.618] [76.7189], Avg: [-438.037 -438.037 -438.037] (1.000)
Step: 196000, Reward: [-416.126 -416.126 -416.126] [84.5960], Avg: [-437.925 -437.925 -437.925] (1.000)
Step: 197000, Reward: [-421.72 -421.72 -421.72] [93.0043], Avg: [-437.843 -437.843 -437.843] (1.000)
Step: 198000, Reward: [-479.776 -479.776 -479.776] [88.7588], Avg: [-438.054 -438.054 -438.054] (1.000)
Step: 199000, Reward: [-500.341 -500.341 -500.341] [152.2607], Avg: [-438.367 -438.367 -438.367] (1.000)
