Model: <class 'multiagent.mappo.MAPPOAgent'>, Dir: 5_vs_5
num_envs: 16,
state_size: [(1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115)],
action_size: [[1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19]],
action_space: [MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19])],
envs: <class 'utils.envs.EnvManager'>,
reward_shape: False,
icm: True,

import torch
import random
import numpy as np
from models.ppo import PPONetwork, PPOCritic
from models.rand import MultiagentReplayBuffer2
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, MultiheadAttention, one_hot_from_indices, gsoftmax

ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.05				# The limit of the ratio of new action probabilities to old probabilities
BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 5					# Number of iterations to sample batches for training
TIME_BATCH = 50					# Number of time steps for RNN BPTT
MAX_BUFFER_SIZE = 960  	    	# Sets the maximum length of the replay buffer

class MAPPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm1 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.norm2 = torch.nn.LayerNorm(ACTOR_HIDDEN)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.attention = MultiheadAttention(ACTOR_HIDDEN, 1, 1)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, action=None, sample=True):
		state = self.norm1(self.layer1(state)).relu()
		state = self.norm2(self.layer2(state)).relu()
		out_dims = state.shape[:-1]
		state = state.reshape(-1, state.shape[-1])
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0), state.device)
		self.hidden = self.recurrent(state, self.hidden)
		state = self.attention(self.hidden)
		action_mu = self.action_mu(state).tanh()
		action_mu = action_mu.reshape(*out_dims, action_mu.shape[-1])
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

	def init_hidden(self, batch_size=1, device=torch.device("cpu")):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN]).to(device)

class MAPPONetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu, name="mappo")
		self.state_size = state_size
		self.action_size = action_size
		self.actor = MAPPOActor(state_size[0], action_size[0])
		self.critic = lambda s,a: PPOCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [PPONetwork(s_size, a_size, lambda s,a: self.actor, self.critic, lr=lr/len(state_size), gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		[[m.train() for m in [model.actor_local, model.critic_local]] for model in self.models]
		if load: self.load_model(load)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_in = [None] * len(state) if action_in is None else action_in
			action_or_entropy, log_prob = map(list, zip(*[model.get_action_probs(s, a, grad=grad, numpy=numpy, sample=sample) for s,a,model in zip(state, action_in, self.models)]))
			return action_or_entropy, log_prob

	def get_value(self, state, grad=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_value(state, grad) for model in self.models]
			return q_value

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		for model, state, action, old_log_prob, target, advantage in zip(self.models, states, actions, old_log_probs, targets, advantages):		
			[m.train() for m in [model.actor_local, model.critic_local]]
			values = model.get_value(states_joint, grad=True)
			critic_error = values - target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			model.actor_local.init_hidden(state.size(0), state.device)
			entropy, new_log_prob = zip(*[model.get_action_probs(state[:,t], action[:,t], grad=True, numpy=False) for t in range(state.size(1))])
			new_log_prob = torch.stack(new_log_prob, dim=1)
			entropy = torch.stack(entropy).mean()
			ratio = (new_log_prob - old_log_prob).exp()
			ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
			advantage = advantage.view(*advantage.shape, *[1]*(len(ratio.shape)-len(advantage.shape)))
			actor_loss = -(torch.min(ratio*advantage, ratio_clipped*advantage) + e_weight*entropy) * scale
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())
			[m.eval() for m in [model.actor_local, model.critic_local]]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MAPPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, MAPPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer2(MAX_BUFFER_SIZE, state_size, action_size, TIME_BATCH)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, self.log_prob, reward, done))
		if np.any(done[0]):
			states, actions, log_probs, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			values = self.network.get_value(states_joint)
			targets, advantages = zip(*[self.compute_gae(value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), value[:-1]) for value,reward,done in zip(values, rewards, dones)])
			time_split = lambda x: [t.view(-1,TIME_BATCH,*t.shape[1:]).transpose(0,1).reshape(TIME_BATCH,-1,*t.shape[2:]).transpose(0,1).cpu().numpy() for t in x]
			states, actions, log_probs, targets, advantages = map(time_split, [[s[:-1] for s in states], actions, log_probs, targets, advantages])
			self.replay_buffer.add(states, actions, log_probs, targets, advantages)
		if len(self.replay_buffer) >= MAX_BUFFER_SIZE:
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				states, actions, log_probs, targets, advantages = self.replay_buffer.sample(BATCH_SIZE, device=self.network.device)
				self.network.optimize(states, actions, log_probs, targets, advantages)
			self.replay_buffer.clear()

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
# env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=10000, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {total_rewards[-1]} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.get_stats()})")

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play() else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="mappo", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step: 0, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.5590], Avg: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] ({'r_i': [0, 0, 0, 0, 0], 'eps': 1.0})
Step: 10000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [0.7500], Avg: [ 0.25  0.25  0.25  0.25  0.25 -0.25 -0.25 -0.25 -0.25 -0.25] ({'r_i': [0.45158062778211505, 0.4510037568470256, 0.4514223832762541, 0.45135634174720485, 0.4517385346818353], 'eps': 1.0})
Step: 20000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.5000], Avg: [ 0.167  0.167  0.167  0.167  0.167 -0.167 -0.167 -0.167 -0.167 -0.167] ({'r_i': [0.43633539370887187, 0.43598597164550384, 0.436041993895856, 0.4359750491348095, 0.43629276062149763], 'eps': 1.0})
Step: 30000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [0.7906], Avg: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] ({'r_i': [0.4267913267125065, 0.42720262888667637, 0.4274065717689033, 0.4267618864903343, 0.4267669859093537], 'eps': 1.0})
Step: 40000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.6614], Avg: [ 0.087  0.087  0.087  0.087  0.087 -0.087 -0.087 -0.087 -0.087 -0.087] ({'r_i': [0.41926041222027405, 0.4194094577412535, 0.419585354141891, 0.41909780864013985, 0.41977364377610177], 'eps': 1.0})
Step: 50000, Reward: [ 0.25  0.25  0.25  0.25  0.25 -0.25 -0.25 -0.25 -0.25 -0.25] [0.6124], Avg: [ 0.115  0.115  0.115  0.115  0.115 -0.115 -0.115 -0.115 -0.115 -0.115] ({'r_i': [0.41490836122373326, 0.41493364038729735, 0.4148590321882059, 0.41461324849864467, 0.4149494321803149], 'eps': 1.0})
Step: 60000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [0.7500], Avg: [ 0.107  0.107  0.107  0.107  0.107 -0.107 -0.107 -0.107 -0.107 -0.107] ({'r_i': [0.41050958616441735, 0.4106957545031716, 0.41071917837435223, 0.4108591560523297, 0.4101343107572757], 'eps': 1.0})
Step: 70000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.7071], Avg: [ 0.094  0.094  0.094  0.094  0.094 -0.094 -0.094 -0.094 -0.094 -0.094] ({'r_i': [0.4082291381551315, 0.40763959867088123, 0.40799679173384273, 0.4076025371567036, 0.4079947890728557], 'eps': 1.0})
Step: 80000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.8292], Avg: [ 0.104  0.104  0.104  0.104  0.104 -0.104 -0.104 -0.104 -0.104 -0.104] ({'r_i': [0.40576621840945964, 0.40602861756676184, 0.4059558503756093, 0.40590782569193595, 0.406244491996761], 'eps': 1.0})
Step: 90000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.8660], Avg: [ 0.094  0.094  0.094  0.094  0.094 -0.094 -0.094 -0.094 -0.094 -0.094] ({'r_i': [0.40518553978202787, 0.40471295298661186, 0.4054961015528728, 0.40465812701021786, 0.40506029818810446], 'eps': 1.0})
Step: 100000, Reward: [ 0.438  0.438  0.438  0.438  0.438 -0.438 -0.438 -0.438 -0.438 -0.438] [0.8292], Avg: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] ({'r_i': [0.4048556418069638, 0.40520162864655457, 0.40557100047098676, 0.4054868283551187, 0.4055675529417478], 'eps': 1.0})
Step: 110000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [0.6614], Avg: [ 0.109  0.109  0.109  0.109  0.109 -0.109 -0.109 -0.109 -0.109 -0.109] ({'r_i': [0.40647615638452894, 0.4058748913338512, 0.40627041822356275, 0.4063936040476482, 0.4063456546833428], 'eps': 1.0})
Step: 120000, Reward: [ 0.562  0.562  0.562  0.562  0.562 -0.562 -0.562 -0.562 -0.562 -0.562] [1.0897], Avg: [ 0.144  0.144  0.144  0.144  0.144 -0.144 -0.144 -0.144 -0.144 -0.144] ({'r_i': [0.4082305256470378, 0.4083927279646547, 0.4081095167243038, 0.40825675489986313, 0.4081669804658159], 'eps': 1.0})
Step: 130000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [0.7071], Avg: [ 0.116  0.116  0.116  0.116  0.116 -0.116 -0.116 -0.116 -0.116 -0.116] ({'r_i': [0.41064085147576407, 0.4103595787780359, 0.41061183787354577, 0.41063719932292586, 0.41103411914082244], 'eps': 1.0})
Step: 140000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [0.9014], Avg: [ 0.112  0.112  0.112  0.112  0.112 -0.112 -0.112 -0.112 -0.112 -0.112] ({'r_i': [0.4129430408574796, 0.4129067240970519, 0.4124736977854692, 0.41304999320276287, 0.41284046504636196], 'eps': 1.0})
Step: 150000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [1.0607], Avg: [ 0.09  0.09  0.09  0.09  0.09 -0.09 -0.09 -0.09 -0.09 -0.09] ({'r_i': [0.4161093183639848, 0.41668560118981013, 0.41683353389877204, 0.4166866027599317, 0.4165061214862508], 'eps': 1.0})
Step: 160000, Reward: [ 0.375  0.375  0.375  0.375  0.375 -0.375 -0.375 -0.375 -0.375 -0.375] [0.7906], Avg: [ 0.107  0.107  0.107  0.107  0.107 -0.107 -0.107 -0.107 -0.107 -0.107] ({'r_i': [0.42002511444377405, 0.4200913699019389, 0.42060814796802076, 0.4201456119434184, 0.41962353667130486], 'eps': 1.0})
Step: 170000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [0.7500], Avg: [ 0.097  0.097  0.097  0.097  0.097 -0.097 -0.097 -0.097 -0.097 -0.097] ({'r_i': [0.42377078355279646, 0.423309168052187, 0.4238681332375078, 0.4244003883973799, 0.42397867999391425], 'eps': 1.0})
Step: 180000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [1.1726], Avg: [ 0.086  0.086  0.086  0.086  0.086 -0.086 -0.086 -0.086 -0.086 -0.086] ({'r_i': [0.42872990260715593, 0.42856837270370063, 0.428580945159231, 0.4285987683523951, 0.42917536952020596], 'eps': 1.0})
Step: 190000, Reward: [ 0.438  0.438  0.438  0.438  0.438 -0.438 -0.438 -0.438 -0.438 -0.438] [0.9682], Avg: [ 0.103  0.103  0.103  0.103  0.103 -0.103 -0.103 -0.103 -0.103 -0.103] ({'r_i': [0.4334053271530817, 0.4333350279137182, 0.4328814650234352, 0.43284470080604986, 0.4330320817371572], 'eps': 1.0})
Step: 200000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [0.7071], Avg: [ 0.092  0.092  0.092  0.092  0.092 -0.092 -0.092 -0.092 -0.092 -0.092] ({'r_i': [0.43696570439838495, 0.4372458824938059, 0.4372207596365673, 0.4368947789004176, 0.4379267799722859], 'eps': 1.0})
Step: 210000, Reward: [ 0.375  0.375  0.375  0.375  0.375 -0.375 -0.375 -0.375 -0.375 -0.375] [1.0000], Avg: [ 0.105  0.105  0.105  0.105  0.105 -0.105 -0.105 -0.105 -0.105 -0.105] ({'r_i': [0.4419585053623887, 0.4418595753803384, 0.4423389780752671, 0.44179069760509687, 0.4428164603346571], 'eps': 1.0})
Step: 220000, Reward: [ 0.438  0.438  0.438  0.438  0.438 -0.438 -0.438 -0.438 -0.438 -0.438] [1.0308], Avg: [ 0.12  0.12  0.12  0.12  0.12 -0.12 -0.12 -0.12 -0.12 -0.12] ({'r_i': [0.4473520024001288, 0.4482346483712447, 0.44831085977651597, 0.44784878159322156, 0.4483939443931191], 'eps': 1.0})
Step: 230000, Reward: [ 0.562  0.562  0.562  0.562  0.562 -0.562 -0.562 -0.562 -0.562 -0.562] [1.1456], Avg: [ 0.138  0.138  0.138  0.138  0.138 -0.138 -0.138 -0.138 -0.138 -0.138] ({'r_i': [0.45239679235199254, 0.45230153692969016, 0.45255353565684825, 0.4525765812243852, 0.4520991560446564], 'eps': 1.0})
Step: 240000, Reward: [ 0.25  0.25  0.25  0.25  0.25 -0.25 -0.25 -0.25 -0.25 -0.25] [1.2748], Avg: [ 0.142  0.142  0.142  0.142  0.142 -0.142 -0.142 -0.142 -0.142 -0.142] ({'r_i': [0.45867605247779286, 0.4587152598676039, 0.4580132535722029, 0.45827557550999337, 0.45835423627356064], 'eps': 1.0})
Step: 250000, Reward: [-0.312 -0.312 -0.312 -0.312 -0.312  0.312  0.312  0.312  0.312  0.312] [1.1456], Avg: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] ({'r_i': [0.4640537626902159, 0.4637464696821343, 0.4638525360642622, 0.4639355558504661, 0.4639343199410683], 'eps': 1.0})
Step: 260000, Reward: [-0.188 -0.188 -0.188 -0.188 -0.188  0.188  0.188  0.188  0.188  0.188] [1.0897], Avg: [ 0.113  0.113  0.113  0.113  0.113 -0.113 -0.113 -0.113 -0.113 -0.113] ({'r_i': [0.4699571204163755, 0.4697536683182439, 0.46949588172329176, 0.4698722735331394, 0.46985312434911936], 'eps': 1.0})
Step: 270000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.7071], Avg: [ 0.109  0.109  0.109  0.109  0.109 -0.109 -0.109 -0.109 -0.109 -0.109] ({'r_i': [0.4749894999403041, 0.47467247441011323, 0.47525408198525354, 0.47586925173411143, 0.4756193619538098], 'eps': 1.0})
Step: 280000, Reward: [ 0.375  0.375  0.375  0.375  0.375 -0.375 -0.375 -0.375 -0.375 -0.375] [1.0000], Avg: [ 0.119  0.119  0.119  0.119  0.119 -0.119 -0.119 -0.119 -0.119 -0.119] ({'r_i': [0.48157244068265176, 0.4812087697750475, 0.48126997315267933, 0.4815631036025265, 0.481214111755809], 'eps': 1.0})
Step: 290000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.9014], Avg: [ 0.121  0.121  0.121  0.121  0.121 -0.121 -0.121 -0.121 -0.121 -0.121] ({'r_i': [0.48683584669635943, 0.4863744562491774, 0.48737854627788896, 0.4872957923130857, 0.4859188686346429], 'eps': 1.0})
Step: 300000, Reward: [-0.438 -0.438 -0.438 -0.438 -0.438  0.438  0.438  0.438  0.438  0.438] [1.1456], Avg: [ 0.103  0.103  0.103  0.103  0.103 -0.103 -0.103 -0.103 -0.103 -0.103] ({'r_i': [0.49276474354955524, 0.493006880429845, 0.4932748918523236, 0.4934264676880945, 0.4933957897882404], 'eps': 1.0})
Step: 310000, Reward: [-0.312 -0.312 -0.312 -0.312 -0.312  0.312  0.312  0.312  0.312  0.312] [1.0308], Avg: [ 0.09  0.09  0.09  0.09  0.09 -0.09 -0.09 -0.09 -0.09 -0.09] ({'r_i': [0.49906210357486264, 0.49891109389190874, 0.49931384965368647, 0.4991128755282197, 0.4994850310430241], 'eps': 1.0})
Step: 320000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.9014], Avg: [ 0.093  0.093  0.093  0.093  0.093 -0.093 -0.093 -0.093 -0.093 -0.093] ({'r_i': [0.5048258908226982, 0.5045923110274081, 0.5041389205992842, 0.5049376464804584, 0.5041368052970308], 'eps': 1.0})
Step: 330000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [1.1456], Avg: [ 0.088  0.088  0.088  0.088  0.088 -0.088 -0.088 -0.088 -0.088 -0.088] ({'r_i': [0.5101062462514965, 0.5107917036082363, 0.5107947348700448, 0.5110942615544967, 0.510438878308128], 'eps': 1.0})
Step: 340000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [1.2247], Avg: [ 0.079  0.079  0.079  0.079  0.079 -0.079 -0.079 -0.079 -0.079 -0.079] ({'r_i': [0.5167839880276265, 0.5170136029834135, 0.5169378931929047, 0.5175643620920213, 0.5180691365480631], 'eps': 1.0})
Step: 350000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [0.9354], Avg: [ 0.073  0.073  0.073  0.073  0.073 -0.073 -0.073 -0.073 -0.073 -0.073] ({'r_i': [0.5227245142111658, 0.5227631398742605, 0.5229179949318576, 0.5223288792519726, 0.5227991097370784], 'eps': 1.0})
Step: 360000, Reward: [-0.5 -0.5 -0.5 -0.5 -0.5  0.5  0.5  0.5  0.5  0.5] [0.8660], Avg: [ 0.057  0.057  0.057  0.057  0.057 -0.057 -0.057 -0.057 -0.057 -0.057] ({'r_i': [0.5288781329063543, 0.5291137444310589, 0.5289837317661538, 0.5291660113045558, 0.5288925235103039], 'eps': 1.0})
Step: 370000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [1.2990], Avg: [ 0.064  0.064  0.064  0.064  0.064 -0.064 -0.064 -0.064 -0.064 -0.064] ({'r_i': [0.5347409390680388, 0.5351461071515352, 0.534322938928122, 0.5345578394475807, 0.5347188235700338], 'eps': 1.0})
Step: 380000, Reward: [-0.312 -0.312 -0.312 -0.312 -0.312  0.312  0.312  0.312  0.312  0.312] [1.2990], Avg: [ 0.054  0.054  0.054  0.054  0.054 -0.054 -0.054 -0.054 -0.054 -0.054] ({'r_i': [0.5408097692455373, 0.541288479649824, 0.5411839463820992, 0.540744665796972, 0.5413502240713375], 'eps': 1.0})
Step: 390000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [0.9014], Avg: [ 0.055  0.055  0.055  0.055  0.055 -0.055 -0.055 -0.055 -0.055 -0.055] ({'r_i': [0.5465455389842779, 0.5471394804356386, 0.5465578286837554, 0.5476513777417713, 0.5465695831889752], 'eps': 1.0})
Step: 400000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.8660], Avg: [ 0.053  0.053  0.053  0.053  0.053 -0.053 -0.053 -0.053 -0.053 -0.053] ({'r_i': [0.5538015016213111, 0.5530975676124604, 0.5531121749015938, 0.5534007553320585, 0.5533789520850405], 'eps': 1.0})
Step: 410000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [0.9014], Avg: [ 0.06  0.06  0.06  0.06  0.06 -0.06 -0.06 -0.06 -0.06 -0.06] ({'r_i': [0.5590684014824219, 0.5601102525800105, 0.5596019586389884, 0.5589972349548608, 0.5589721975188392], 'eps': 1.0})
Step: 420000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1.1726], Avg: [ 0.058  0.058  0.058  0.058  0.058 -0.058 -0.058 -0.058 -0.058 -0.058] ({'r_i': [0.5662520110413509, 0.5665951581374975, 0.5668612555514168, 0.5663654071164395, 0.5664043970698646], 'eps': 1.0})
Step: 430000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [1.0308], Avg: [ 0.058  0.058  0.058  0.058  0.058 -0.058 -0.058 -0.058 -0.058 -0.058] ({'r_i': [0.5750520449076882, 0.5731202302089272, 0.574848708799678, 0.5743430091273557, 0.5742970124806485], 'eps': 1.0})
Step: 440000, Reward: [-0.188 -0.188 -0.188 -0.188 -0.188  0.188  0.188  0.188  0.188  0.188] [0.9682], Avg: [ 0.053  0.053  0.053  0.053  0.053 -0.053 -0.053 -0.053 -0.053 -0.053] ({'r_i': [0.5800794552189504, 0.5802767125462286, 0.5805797943901788, 0.5810317448182031, 0.5802727934487371], 'eps': 1.0})
Step: 450000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.8292], Avg: [ 0.056  0.056  0.056  0.056  0.056 -0.056 -0.056 -0.056 -0.056 -0.056] ({'r_i': [0.5868938833816597, 0.5871062193007675, 0.5870926483370907, 0.5865319931403501, 0.5869508423419901], 'eps': 1.0})
Step: 460000, Reward: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] [1.0000], Avg: [ 0.057  0.057  0.057  0.057  0.057 -0.057 -0.057 -0.057 -0.057 -0.057] ({'r_i': [0.5943127426300715, 0.5947558151195861, 0.5949541949839849, 0.5944080638413628, 0.5949299254055239], 'eps': 1.0})
Step: 470000, Reward: [-0.188 -0.188 -0.188 -0.188 -0.188  0.188  0.188  0.188  0.188  0.188] [1.2500], Avg: [ 0.052  0.052  0.052  0.052  0.052 -0.052 -0.052 -0.052 -0.052 -0.052] ({'r_i': [0.6012938961672027, 0.6010359111690066, 0.6005578626938578, 0.6000736492225486, 0.6013614499737095], 'eps': 1.0})
Step: 480000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [1.1726], Avg: [ 0.048  0.048  0.048  0.048  0.048 -0.048 -0.048 -0.048 -0.048 -0.048] ({'r_i': [0.6089118208848716, 0.6092852196958847, 0.6093234725806008, 0.6083764549287735, 0.6090457651122319], 'eps': 1.0})
Step: 490000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [0.8660], Avg: [ 0.043  0.043  0.043  0.043  0.043 -0.043 -0.043 -0.043 -0.043 -0.043] ({'r_i': [0.617604562728407, 0.6161283336805387, 0.6167887127762143, 0.6169756852761946, 0.6174668130088701], 'eps': 1.0})
Step: 500000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [1.1180], Avg: [ 0.039  0.039  0.039  0.039  0.039 -0.039 -0.039 -0.039 -0.039 -0.039] ({'r_i': [0.6230540257385, 0.6232484308338931, 0.6240019947211258, 0.6229778845310728, 0.6228281846508487], 'eps': 1.0})
Step: 510000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0.7071], Avg: [ 0.038  0.038  0.038  0.038  0.038 -0.038 -0.038 -0.038 -0.038 -0.038] ({'r_i': [0.6303114681458488, 0.6302916260636023, 0.6301206629001923, 0.6301595916274236, 0.6303000011986587], 'eps': 1.0})
Step: 520000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [0.9014], Avg: [ 0.039  0.039  0.039  0.039  0.039 -0.039 -0.039 -0.039 -0.039 -0.039] ({'r_i': [0.6375230142111993, 0.6377465458623548, 0.637570500119848, 0.6371543205496337, 0.63691382176755], 'eps': 1.0})
Step: 530000, Reward: [ 0.438  0.438  0.438  0.438  0.438 -0.438 -0.438 -0.438 -0.438 -0.438] [1.0308], Avg: [ 0.046  0.046  0.046  0.046  0.046 -0.046 -0.046 -0.046 -0.046 -0.046] ({'r_i': [0.644096834378317, 0.6441110114526625, 0.6441742403926327, 0.643668894609695, 0.6437852526259505], 'eps': 1.0})
Step: 540000, Reward: [ 0.188  0.188  0.188  0.188  0.188 -0.188 -0.188 -0.188 -0.188 -0.188] [0.9014], Avg: [ 0.049  0.049  0.049  0.049  0.049 -0.049 -0.049 -0.049 -0.049 -0.049] ({'r_i': [0.6515483530869242, 0.6517296460253031, 0.6510705534496034, 0.6512003721476067, 0.6515894771233435], 'eps': 1.0})
Step: 550000, Reward: [ 0.375  0.375  0.375  0.375  0.375 -0.375 -0.375 -0.375 -0.375 -0.375] [1.0607], Avg: [ 0.055  0.055  0.055  0.055  0.055 -0.055 -0.055 -0.055 -0.055 -0.055] ({'r_i': [0.6594919079723769, 0.6588917882968155, 0.6594135614297767, 0.6591021653739736, 0.6593672290138072], 'eps': 1.0})
Step: 560000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [0.7906], Avg: [ 0.049  0.049  0.049  0.049  0.049 -0.049 -0.049 -0.049 -0.049 -0.049] ({'r_i': [0.6663124044704665, 0.6655989606345796, 0.6652755480110127, 0.6646871558983499, 0.6650967084289424], 'eps': 1.0})
Step: 570000, Reward: [-0.188 -0.188 -0.188 -0.188 -0.188  0.188  0.188  0.188  0.188  0.188] [1.1456], Avg: [ 0.045  0.045  0.045  0.045  0.045 -0.045 -0.045 -0.045 -0.045 -0.045] ({'r_i': [0.6731546083476472, 0.6729172777781884, 0.6729587957093026, 0.6730708077279074, 0.6728751351985072], 'eps': 1.0})
Step: 580000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [1.0308], Avg: [ 0.046  0.046  0.046  0.046  0.046 -0.046 -0.046 -0.046 -0.046 -0.046] ({'r_i': [0.6814181638946758, 0.6809351009606487, 0.680791901901416, 0.6807129994204991, 0.6813902505117779], 'eps': 1.0})
Step: 590000, Reward: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1.1180], Avg: [ 0.045  0.045  0.045  0.045  0.045 -0.045 -0.045 -0.045 -0.045 -0.045] ({'r_i': [0.687505636557264, 0.6873342294117125, 0.6875126823062697, 0.6885247949384566, 0.6875499981915992], 'eps': 1.0})
Step: 600000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [0.9682], Avg: [ 0.045  0.045  0.045  0.045  0.045 -0.045 -0.045 -0.045 -0.045 -0.045] ({'r_i': [0.6947945095575414, 0.6956109444677132, 0.6960436019909103, 0.696710981806197, 0.6952762930224418], 'eps': 1.0})
Step: 610000, Reward: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] [0.9354], Avg: [ 0.046  0.046  0.046  0.046  0.046 -0.046 -0.046 -0.046 -0.046 -0.046] ({'r_i': [0.7031730413389289, 0.7035726066927633, 0.704465911465904, 0.7043970937681281, 0.7044108534963905], 'eps': 1.0})
Step: 620000, Reward: [ 0.25  0.25  0.25  0.25  0.25 -0.25 -0.25 -0.25 -0.25 -0.25] [1.0000], Avg: [ 0.05  0.05  0.05  0.05  0.05 -0.05 -0.05 -0.05 -0.05 -0.05] ({'r_i': [0.7103997554102809, 0.7107098673186782, 0.7101845374707546, 0.7108263488190972, 0.7106744764644343], 'eps': 1.0})
Step: 630000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [0.9014], Avg: [ 0.054  0.054  0.054  0.054  0.054 -0.054 -0.054 -0.054 -0.054 -0.054] ({'r_i': [0.7179125598229777, 0.718402069389238, 0.7183681037724794, 0.7183769855741412, 0.7188857236099431], 'eps': 1.0})
Step: 640000, Reward: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] [1.1180], Avg: [ 0.055  0.055  0.055  0.055  0.055 -0.055 -0.055 -0.055 -0.055 -0.055] ({'r_i': [0.7252978891446773, 0.7260033802158302, 0.7252929622481267, 0.7263806314553755, 0.7254389231874504], 'eps': 1.0})
Step: 650000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [1.3463], Avg: [ 0.059  0.059  0.059  0.059  0.059 -0.059 -0.059 -0.059 -0.059 -0.059] ({'r_i': [0.7308884290938473, 0.7317167488435129, 0.7314688180807151, 0.7318284325428411, 0.7315114725461335], 'eps': 1.0})
Step: 660000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [1.2500], Avg: [ 0.057  0.057  0.057  0.057  0.057 -0.057 -0.057 -0.057 -0.057 -0.057] ({'r_i': [0.7372248488086819, 0.738116340445665, 0.7384181932822879, 0.738048501268068, 0.7383046720995723], 'eps': 1.0})
Step: 670000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [1.2990], Avg: [ 0.057  0.057  0.057  0.057  0.057 -0.057 -0.057 -0.057 -0.057 -0.057] ({'r_i': [0.7436153513672762, 0.7444671360639751, 0.7446451368812057, 0.7455139418231944, 0.744879831618319], 'eps': 1.0})
Step: 680000, Reward: [ 0.312  0.312  0.312  0.312  0.312 -0.312 -0.312 -0.312 -0.312 -0.312] [1.0308], Avg: [ 0.061  0.061  0.061  0.061  0.061 -0.061 -0.061 -0.061 -0.061 -0.061] ({'r_i': [0.7504172599810279, 0.750307922009482, 0.7503813547400965, 0.7500979724133697, 0.7502795437154256], 'eps': 1.0})
Step: 690000, Reward: [ 0.5  0.5  0.5  0.5  0.5 -0.5 -0.5 -0.5 -0.5 -0.5] [1.2748], Avg: [ 0.067  0.067  0.067  0.067  0.067 -0.067 -0.067 -0.067 -0.067 -0.067] ({'r_i': [0.7559529940542611, 0.7551655512188639, 0.7554234243031436, 0.7561830831861978, 0.757054853333432], 'eps': 1.0})
Step: 700000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [0.9354], Avg: [ 0.064  0.064  0.064  0.064  0.064 -0.064 -0.064 -0.064 -0.064 -0.064] ({'r_i': [0.7626652567608074, 0.7620300236519219, 0.7618549012830999, 0.7612462216992864, 0.7619971301510103], 'eps': 1.0})
Step: 710000, Reward: [-0.188 -0.188 -0.188 -0.188 -0.188  0.188  0.188  0.188  0.188  0.188] [1.1456], Avg: [ 0.061  0.061  0.061  0.061  0.061 -0.061 -0.061 -0.061 -0.061 -0.061] ({'r_i': [0.7670836306249514, 0.7674754462138647, 0.7673420055591915, 0.7675363587083606, 0.7683343501136018], 'eps': 1.0})
Step: 720000, Reward: [-0.375 -0.375 -0.375 -0.375 -0.375  0.375  0.375  0.375  0.375  0.375] [1.1180], Avg: [ 0.055  0.055  0.055  0.055  0.055 -0.055 -0.055 -0.055 -0.055 -0.055] ({'r_i': [0.7729168990406519, 0.7731079648711409, 0.7733461737051063, 0.7729221591966924, 0.772545030967022], 'eps': 1.0})
Step: 730000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [0.6124], Avg: [ 0.052  0.052  0.052  0.052  0.052 -0.052 -0.052 -0.052 -0.052 -0.052] ({'r_i': [0.778452510647217, 0.7783379515107307, 0.778280478520888, 0.7781852385239262, 0.7782698341365387], 'eps': 1.0})
Step: 740000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [1.0000], Avg: [ 0.05  0.05  0.05  0.05  0.05 -0.05 -0.05 -0.05 -0.05 -0.05] ({'r_i': [0.7826793852468642, 0.783281270222221, 0.7833052690466866, 0.7837525043213844, 0.7838381897755898], 'eps': 1.0})
Step: 750000, Reward: [-0.438 -0.438 -0.438 -0.438 -0.438  0.438  0.438  0.438  0.438  0.438] [1.0897], Avg: [ 0.044  0.044  0.044  0.044  0.044 -0.044 -0.044 -0.044 -0.044 -0.044] ({'r_i': [0.7871570055855748, 0.7882206944217518, 0.7883866041474199, 0.7883324591711085, 0.7880472658879589], 'eps': 1.0})
Step: 760000, Reward: [ 0.062  0.062  0.062  0.062  0.062 -0.062 -0.062 -0.062 -0.062 -0.062] [1.3919], Avg: [ 0.044  0.044  0.044  0.044  0.044 -0.044 -0.044 -0.044 -0.044 -0.044] ({'r_i': [0.7931413967245672, 0.7939491718898838, 0.7938123505476655, 0.793424621515597, 0.7929685604040407], 'eps': 1.0})
Step: 770000, Reward: [ 0.125  0.125  0.125  0.125  0.125 -0.125 -0.125 -0.125 -0.125 -0.125] [1.1180], Avg: [ 0.045  0.045  0.045  0.045  0.045 -0.045 -0.045 -0.045 -0.045 -0.045] ({'r_i': [0.7968008326658359, 0.797173422550317, 0.7976830884629033, 0.7975729737017924, 0.7970457117226389], 'eps': 1.0})
Step: 780000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [0.9354], Avg: [ 0.043  0.043  0.043  0.043  0.043 -0.043 -0.043 -0.043 -0.043 -0.043] ({'r_i': [0.8034032398837929, 0.8036303078597024, 0.8036205828243013, 0.8032094097396669, 0.8035746132157511], 'eps': 1.0})
Step: 790000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [1.0897], Avg: [ 0.041  0.041  0.041  0.041  0.041 -0.041 -0.041 -0.041 -0.041 -0.041] ({'r_i': [0.8077451839115367, 0.8076013438250457, 0.8075459553985339, 0.8083608304986524, 0.8077918476716927], 'eps': 1.0})
Step: 800000, Reward: [-0.062 -0.062 -0.062 -0.062 -0.062  0.062  0.062  0.062  0.062  0.062] [0.9014], Avg: [ 0.04  0.04  0.04  0.04  0.04 -0.04 -0.04 -0.04 -0.04 -0.04] ({'r_i': [0.8119438039839476, 0.811805849114226, 0.8118262640900081, 0.8117213195888325, 0.8123133332866969], 'eps': 1.0})
Step: 810000, Reward: [-0.125 -0.125 -0.125 -0.125 -0.125  0.125  0.125  0.125  0.125  0.125] [1.0000], Avg: [ 0.038  0.038  0.038  0.038  0.038 -0.038 -0.038 -0.038 -0.038 -0.038] ({'r_i': [0.8166731519009918, 0.8175895071172854, 0.8163602380933395, 0.8172832298697903, 0.8163367993895275], 'eps': 1.0})
Step: 820000, Reward: [ 0.25  0.25  0.25  0.25  0.25 -0.25 -0.25 -0.25 -0.25 -0.25] [1.1180], Avg: [ 0.041  0.041  0.041  0.041  0.041 -0.041 -0.041 -0.041 -0.041 -0.041] ({'r_i': [0.8212741264694164, 0.8211962591411753, 0.8215894871557442, 0.820685476233355, 0.8215638853218407], 'eps': 1.0})
Step: 830000, Reward: [-0.25 -0.25 -0.25 -0.25 -0.25  0.25  0.25  0.25  0.25  0.25] [1.4577], Avg: [ 0.037  0.037  0.037  0.037  0.037 -0.037 -0.037 -0.037 -0.037 -0.037] ({'r_i': [0.8253836695444768, 0.8248320380640733, 0.8255348695666633, 0.824812498847126, 0.8248998796532138], 'eps': 1.0})
