Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_adversary
num_envs: 16, state_size: [(8,), (10,), (10,)], action_size: [[5], [5], [5]], action_space: [Discrete(5), Discrete(5), Discrete(5)],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, REG_LAMBDA, EPS_MIN

BATCH_SIZE = 32					# Number of samples to train on for each train step
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.agents = [DDPGNetwork(s_size, a_size, DDPGActor, lambda s,a: DDPGCritic([np.sum(state_size)], [np.sum(action_size)]), lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(state_size, action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			state = state.transpose(0, -len(self.state_size)).unsqueeze(-len(self.state_size)) if type(self.state_size[0]) == int else state
			action = [agent.get_action(s, use_target, grad, numpy, sample) for s,agent in zip(state, self.agents)]
			return (np.concatenate if numpy else torch.cat)(action, -2) if type(self.action_size[0]) == int else action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			# out_dims = state.size()[:-len(self.state_size)]
			# state = state.view(-1, *state.size()[-len(self.state_size):]).transpose(0, -len(self.state_size)).unsqueeze(-len(self.state_size))
			# action = action.view(-1, *action.size()[-2:]).transpose(0, -2).unsqueeze(-2)
			q_value = [agent.get_q_value(state, action, use_target, grad, numpy) for agent in self.agents]
			return q_value #(np.concatenate if numpy else torch.cat)(q_value, -2).view(*out_dims, -1, 1)

	def optimize(self, states, actions, states_joint, actions_joint, q_targets):
		# states, critic_states, critic_actions, q_targets = [t.permute(2, 0, 1, *range(3,len(t.size()))).detach() for t in [states, critic_states, critic_actions, q_targets]]
		for (i,agent),state,q_target in zip(enumerate(self.agents), states, q_targets):
			q_values = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:-1] - q_target.detach()
			critic_loss = critic_error.pow(2)
			agent.step(agent.critic_optimizer, critic_loss.mean())

			actor_action = agent.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else action.detach() for j,action in enumerate(actions)]
			q_actions = agent.critic_local(states_joint, torch.cat(critic_action, dim=-1))
			actor_loss = -(q_actions - q_values.detach())
			agent.step(agent.actor_optimizer, actor_loss.mean())

			agent.soft_copy(agent.actor_local, agent.actor_target)
			agent.soft_copy(agent.critic_local, agent.critic_target)

	def save_model(self, dirname="pytorch", name="best"):
		[PTACNetwork.save_model(agent, "maddpg", dirname, f"{name}_{i}") for i,agent in enumerate(self.agents)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[PTACNetwork.load_model(agent, "maddpg", dirname, f"{name}_{i}") for i,agent in enumerate(self.agents)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		batch_dim = len(state.shape) - len(self.state_size) if type(state) != list else len(state[0].shape) - len(self.state_size[0])
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state, batch_dim), sample=sample, grad=False, numpy=numpy)
		return action_random if random.random() < eps else action_greedy

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x,2), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state, 1)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True, grad=False))]
			# if dones.size() != rewards.size(): dones = dones.unsqueeze(-1).repeat_interleave(self.state_size[0], dim=-1)

			# states_joint = states.view(*states.size()[:-len(self.state_size)], 1, -1).repeat_interleave(int(self.state_size[0]), dim=-2)
			# actions_joint = actions.view(*actions.size()[:-2], 1, -1).repeat_interleave(int(self.state_size[0]), dim=-2)
			# q_values = self.network.get_q_value(states_joint, actions_joint, grad=False)
			# q_targets, _ = self.compute_gae(q_values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), q_values[:-1])
			# self.network.optimize(states[:-1], states_joint[:-1], actions_joint[:-1], q_targets)
			states_joint = torch.cat(states, dim=-1)
			actions_joint = torch.cat(actions, dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True, grad=False)
			q_targets = [self.compute_gae(q_value[-1], reward, done, q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			self.network.optimize(states, actions, states_joint, actions_joint, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)


Step: 101, Reward: [-1.585  0.165  0.165] [1.1045], Avg: [-2.645 -0.332 -0.332] (0.995)
Step: 203, Reward: [-1.608  0.08   0.08 ] [1.2255], Avg: [-2.611 -0.583 -0.583] (0.990)
Step: 305, Reward: [-1.151  0.366  0.366] [0.8051], Avg: [-2.316 -0.334 -0.334] (0.985)
Step: 407, Reward: [-0.922  0.054  0.054] [0.6980], Avg: [-2.148 -0.334 -0.334] (0.980)
Step: 509, Reward: [-1.075  0.679  0.679] [0.8748], Avg: [-2.001 -0.182 -0.182] (0.975)
Step: 611, Reward: [-0.965  0.405  0.405] [0.7501], Avg: [-1.92  -0.128 -0.128] (0.970)
Step: 713, Reward: [-1.463  0.102  0.102] [1.0393], Avg: [-2.01  -0.161 -0.161] (0.966)
Step: 815, Reward: [-1.23   0.242  0.242] [0.9878], Avg: [-2.012 -0.192 -0.192] (0.961)
Step: 917, Reward: [-0.624  0.008  0.008] [0.5752], Avg: [-1.935 -0.208 -0.208] (0.956)
Step: 1019, Reward: [-1.618  0.367  0.367] [1.1505], Avg: [-2.016 -0.17  -0.17 ] (0.951)
Step: 1121, Reward: [-1.579  0.33   0.33 ] [1.2238], Avg: [-2.081 -0.18  -0.18 ] (0.946)
Step: 1223, Reward: [-2.559  0.54   0.54 ] [1.7040], Avg: [-2.231 -0.164 -0.164] (0.942)
Step: 1325, Reward: [-1.779  0.751  0.751] [1.3938], Avg: [-2.269 -0.138 -0.138] (0.937)
Step: 1427, Reward: [-1.761  0.594  0.594] [1.1884], Avg: [-2.278 -0.105 -0.105] (0.932)
Step: 1529, Reward: [-1.03   0.152  0.152] [0.7211], Avg: [-2.239 -0.109 -0.109] (0.928)
Step: 1631, Reward: [-1.132  0.122  0.122] [0.9837], Avg: [-2.229 -0.137 -0.137] (0.923)
Step: 1733, Reward: [-1.262  0.388  0.388] [0.9265], Avg: [-2.212 -0.129 -0.129] (0.918)
Step: 1835, Reward: [-1.321  0.635  0.635] [1.2028], Avg: [-2.217 -0.122 -0.122] (0.914)
Step: 1937, Reward: [-0.972  0.345  0.345] [0.7673], Avg: [-2.181 -0.118 -0.118] (0.909)
Step: 2039, Reward: [-1.017  0.599  0.599] [1.0964], Avg: [-2.181 -0.108 -0.108] (0.905)
Step: 2141, Reward: [-1.675  0.09   0.09 ] [1.1544], Avg: [-2.214 -0.122 -0.122] (0.900)
Step: 2243, Reward: [-2.386  0.859  0.859] [1.7787], Avg: [-2.287 -0.097 -0.097] (0.896)
Step: 2345, Reward: [-1.037  0.115  0.115] [0.7121], Avg: [-2.256 -0.106 -0.106] (0.891)
Step: 2447, Reward: [-1.26   0.058  0.058] [1.4160], Avg: [-2.304 -0.113 -0.113] (0.887)
Step: 2549, Reward: [-1.072  0.103  0.103] [0.7133], Avg: [-2.274 -0.122 -0.122] (0.882)
Step: 2651, Reward: [-1.022  0.396  0.396] [0.8276], Avg: [-2.251 -0.116 -0.116] (0.878)
Step: 2753, Reward: [-0.824  0.062  0.062] [0.7936], Avg: [-2.219 -0.136 -0.136] (0.873)
Step: 2855, Reward: [-2.874  0.73   0.73 ] [1.7749], Avg: [-2.262 -0.123 -0.123] (0.869)
Step: 2957, Reward: [-1.293  0.243  0.243] [0.9892], Avg: [-2.262 -0.127 -0.127] (0.865)
Step: 3059, Reward: [-1.249  0.26   0.26 ] [1.0478], Avg: [-2.263 -0.133 -0.133] (0.860)
Step: 3161, Reward: [-0.65   0.103  0.103] [0.4925], Avg: [-2.214 -0.139 -0.139] (0.856)
Step: 3263, Reward: [-2.192  0.866  0.866] [1.7060], Avg: [-2.259 -0.12  -0.12 ] (0.852)
Step: 3365, Reward: [-1.431  0.541  0.541] [1.3258], Avg: [-2.273 -0.122 -0.122] (0.848)
Step: 3467, Reward: [-0.13  -0.462 -0.462] [0.5752], Avg: [-2.213 -0.152 -0.152] (0.843)
Step: 3569, Reward: [-1.194  0.395  0.395] [0.8942], Avg: [-2.203 -0.146 -0.146] (0.839)
Step: 3671, Reward: [-1.793  0.517  0.517] [1.5872], Avg: [-2.237 -0.15  -0.15 ] (0.835)
Step: 3773, Reward: [-1.274  0.198  0.198] [1.0324], Avg: [-2.238 -0.157 -0.157] (0.831)
Step: 3875, Reward: [-1.657  0.339  0.339] [1.1919], Avg: [-2.253 -0.154 -0.154] (0.827)
Step: 3977, Reward: [-1.162  0.592  0.592] [1.0527], Avg: [-2.244 -0.151 -0.151] (0.822)
Step: 4079, Reward: [-1.336  0.379  0.379] [0.9223], Avg: [-2.238 -0.144 -0.144] (0.818)
Step: 4181, Reward: [-1.528  0.291  0.291] [1.3881], Avg: [-2.252 -0.157 -0.157] (0.814)
Step: 4283, Reward: [-1.972  0.639  0.639] [1.3643], Avg: [-2.266 -0.147 -0.147] (0.810)
Step: 4385, Reward: [-0.964 -0.05  -0.05 ] [0.8658], Avg: [-2.263 -0.155 -0.155] (0.806)
Step: 4487, Reward: [-1.159  0.416  0.416] [0.7935], Avg: [-2.245 -0.148 -0.148] (0.802)
Step: 4589, Reward: [-1.244  0.424  0.424] [0.9040], Avg: [-2.236 -0.143 -0.143] (0.798)
Step: 4691, Reward: [-2.08   0.574  0.574] [1.4005], Avg: [-2.254 -0.134 -0.134] (0.794)
Step: 4793, Reward: [-1.825  0.793  0.793] [1.4338], Avg: [-2.267 -0.125 -0.125] (0.790)
Step: 4895, Reward: [-1.028  0.371  0.371] [1.0653], Avg: [-2.263 -0.129 -0.129] (0.786)
Step: 4997, Reward: [-0.902  0.127  0.127] [0.8116], Avg: [-2.254 -0.134 -0.134] (0.782)
Step: 5099, Reward: [-1.802  0.367  0.367] [1.5650], Avg: [-2.282 -0.136 -0.136] (0.778)
Step: 5201, Reward: [-0.533 -0.011 -0.011] [0.4423], Avg: [-2.257 -0.14  -0.14 ] (0.774)
Step: 5303, Reward: [-1.396  0.079  0.079] [1.2393], Avg: [-2.271 -0.146 -0.146] (0.771)
Step: 5405, Reward: [-2.295  0.599  0.599] [1.5343], Avg: [-2.292 -0.14  -0.14 ] (0.767)
Step: 5507, Reward: [-0.93   0.107  0.107] [0.8793], Avg: [-2.283 -0.147 -0.147] (0.763)
Step: 5609, Reward: [-2.21   0.845  0.845] [1.6428], Avg: [-2.303 -0.137 -0.137] (0.759)
Step: 5711, Reward: [-1.655  0.68   0.68 ] [1.2065], Avg: [-2.3   -0.132 -0.132] (0.755)
Step: 5813, Reward: [-1.377  0.295  0.295] [1.0449], Avg: [-2.298 -0.135 -0.135] (0.751)
Step: 5915, Reward: [-2.42   0.825  0.825] [1.8967], Avg: [-2.329 -0.13  -0.13 ] (0.748)
Step: 6017, Reward: [-1.07   0.253  0.253] [0.8332], Avg: [-2.321 -0.13  -0.13 ] (0.744)
Step: 6119, Reward: [-1.681  0.324  0.324] [1.2998], Avg: [-2.334 -0.129 -0.129] (0.740)
Step: 6221, Reward: [-2.1    0.582  0.582] [1.4117], Avg: [-2.347 -0.122 -0.122] (0.737)
Step: 6323, Reward: [-1.531  0.551  0.551] [1.2390], Avg: [-2.35  -0.121 -0.121] (0.733)
Step: 6425, Reward: [-0.923  0.051  0.051] [1.0030], Avg: [-2.344 -0.131 -0.131] (0.729)
Step: 6527, Reward: [-1.316  0.167  0.167] [1.1740], Avg: [-2.349 -0.136 -0.136] (0.726)
Step: 6629, Reward: [-1.673  0.117  0.117] [1.0809], Avg: [-2.353 -0.14  -0.14 ] (0.722)
Step: 6731, Reward: [-1.377  0.152  0.152] [1.0357], Avg: [-2.352 -0.145 -0.145] (0.718)
Step: 6833, Reward: [-1.362  0.368  0.368] [0.9857], Avg: [-2.345 -0.146 -0.146] (0.715)
Step: 6935, Reward: [-1.657  0.373  0.373] [1.3493], Avg: [-2.358 -0.144 -0.144] (0.711)
Step: 7037, Reward: [-2.177  0.718  0.718] [1.5976], Avg: [-2.374 -0.138 -0.138] (0.708)
Step: 7139, Reward: [-1.754  0.609  0.609] [1.2020], Avg: [-2.374 -0.132 -0.132] (0.704)
Step: 7241, Reward: [-1.097  0.225  0.225] [1.0092], Avg: [-2.369 -0.137 -0.137] (0.701)
