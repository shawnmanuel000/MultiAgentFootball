Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			target_value = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			actual_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (actual_value - target_value.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			action_probs = agent.get_action(states[i], grad=True, numpy=False)
			action = [gsoftmax(action_probs, hard=True) if i==j else one_hot(model.get_action(state, numpy=False)) for (j,model), state in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -agent.critic_local(states_joint, action_joint).mean() + e_weight*action_probs.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = action_random if random.random() < eps else action_greedy
		return action

	def train(self, state, action, next_state, reward, done):
		self.replay_buffer.push(state, action, next_state, reward, done)
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.time % self.update_freq)==0:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
			self.network.optimize(states, actions, next_states, rewards, dones)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)
		self.time += 1

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-1]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-609.095 -609.095 -609.095] [0.0000], Avg: [-609.095 -609.095 -609.095] (0.950)
Step: 99, Reward: [-376.895 -376.895 -376.895] [0.0000], Avg: [-492.995 -492.995 -492.995] (0.902)
Step: 149, Reward: [-436.498 -436.498 -436.498] [0.0000], Avg: [-474.163 -474.163 -474.163] (0.857)
Step: 199, Reward: [-389.31 -389.31 -389.31] [0.0000], Avg: [-452.949 -452.949 -452.949] (0.815)
Step: 249, Reward: [-345.882 -345.882 -345.882] [0.0000], Avg: [-431.536 -431.536 -431.536] (0.774)
Step: 299, Reward: [-598.994 -598.994 -598.994] [0.0000], Avg: [-459.446 -459.446 -459.446] (0.735)
Step: 349, Reward: [-444.45 -444.45 -444.45] [0.0000], Avg: [-457.303 -457.303 -457.303] (0.698)
Step: 399, Reward: [-641.855 -641.855 -641.855] [0.0000], Avg: [-480.372 -480.372 -480.372] (0.663)
Step: 449, Reward: [-802.854 -802.854 -802.854] [0.0000], Avg: [-516.204 -516.204 -516.204] (0.630)
Step: 499, Reward: [-648.696 -648.696 -648.696] [0.0000], Avg: [-529.453 -529.453 -529.453] (0.599)
Step: 549, Reward: [-421.511 -421.511 -421.511] [0.0000], Avg: [-519.64 -519.64 -519.64] (0.569)
Step: 599, Reward: [-899.107 -899.107 -899.107] [0.0000], Avg: [-551.262 -551.262 -551.262] (0.540)
Step: 649, Reward: [-860.528 -860.528 -860.528] [0.0000], Avg: [-575.052 -575.052 -575.052] (0.513)
Step: 699, Reward: [-471.147 -471.147 -471.147] [0.0000], Avg: [-567.63 -567.63 -567.63] (0.488)
Step: 749, Reward: [-688.698 -688.698 -688.698] [0.0000], Avg: [-575.701 -575.701 -575.701] (0.463)
Step: 799, Reward: [-585.247 -585.247 -585.247] [0.0000], Avg: [-576.298 -576.298 -576.298] (0.440)
Step: 849, Reward: [-603.818 -603.818 -603.818] [0.0000], Avg: [-577.917 -577.917 -577.917] (0.418)
Step: 899, Reward: [-990.75 -990.75 -990.75] [0.0000], Avg: [-600.852 -600.852 -600.852] (0.397)
Step: 949, Reward: [-1232.664 -1232.664 -1232.664] [0.0000], Avg: [-634.105 -634.105 -634.105] (0.377)
Step: 999, Reward: [-914.454 -914.454 -914.454] [0.0000], Avg: [-648.123 -648.123 -648.123] (0.358)
Step: 1049, Reward: [-1067.046 -1067.046 -1067.046] [0.0000], Avg: [-668.071 -668.071 -668.071] (0.341)
Step: 1099, Reward: [-898.79 -898.79 -898.79] [0.0000], Avg: [-678.559 -678.559 -678.559] (0.324)
Step: 1149, Reward: [-875.173 -875.173 -875.173] [0.0000], Avg: [-687.107 -687.107 -687.107] (0.307)
Step: 1199, Reward: [-1275.134 -1275.134 -1275.134] [0.0000], Avg: [-711.608 -711.608 -711.608] (0.292)
Step: 1249, Reward: [-1960.542 -1960.542 -1960.542] [0.0000], Avg: [-761.566 -761.566 -761.566] (0.277)
Step: 1299, Reward: [-1263.961 -1263.961 -1263.961] [0.0000], Avg: [-780.888 -780.888 -780.888] (0.264)
Step: 1349, Reward: [-1820.285 -1820.285 -1820.285] [0.0000], Avg: [-819.385 -819.385 -819.385] (0.250)
Step: 1399, Reward: [-1332.302 -1332.302 -1332.302] [0.0000], Avg: [-837.703 -837.703 -837.703] (0.238)
Step: 1449, Reward: [-922.813 -922.813 -922.813] [0.0000], Avg: [-840.638 -840.638 -840.638] (0.226)
Step: 1499, Reward: [-1239.162 -1239.162 -1239.162] [0.0000], Avg: [-853.922 -853.922 -853.922] (0.215)
Step: 1549, Reward: [-512.231 -512.231 -512.231] [0.0000], Avg: [-842.9 -842.9 -842.9] (0.204)
Step: 1599, Reward: [-882.099 -882.099 -882.099] [0.0000], Avg: [-844.125 -844.125 -844.125] (0.194)
Step: 1649, Reward: [-593.156 -593.156 -593.156] [0.0000], Avg: [-836.52 -836.52 -836.52] (0.184)
Step: 1699, Reward: [-1006.032 -1006.032 -1006.032] [0.0000], Avg: [-841.505 -841.505 -841.505] (0.175)
Step: 1749, Reward: [-454.111 -454.111 -454.111] [0.0000], Avg: [-830.437 -830.437 -830.437] (0.166)
Step: 1799, Reward: [-822.726 -822.726 -822.726] [0.0000], Avg: [-830.223 -830.223 -830.223] (0.158)
Step: 1849, Reward: [-734.105 -734.105 -734.105] [0.0000], Avg: [-827.625 -827.625 -827.625] (0.150)
Step: 1899, Reward: [-469.352 -469.352 -469.352] [0.0000], Avg: [-818.197 -818.197 -818.197] (0.142)
Step: 1949, Reward: [-809.066 -809.066 -809.066] [0.0000], Avg: [-817.963 -817.963 -817.963] (0.135)
Step: 1999, Reward: [-453.058 -453.058 -453.058] [0.0000], Avg: [-808.84 -808.84 -808.84] (0.129)
Step: 2049, Reward: [-437.81 -437.81 -437.81] [0.0000], Avg: [-799.79 -799.79 -799.79] (0.122)
Step: 2099, Reward: [-647.625 -647.625 -647.625] [0.0000], Avg: [-796.167 -796.167 -796.167] (0.116)
Step: 2149, Reward: [-553.603 -553.603 -553.603] [0.0000], Avg: [-790.526 -790.526 -790.526] (0.110)
Step: 2199, Reward: [-505.281 -505.281 -505.281] [0.0000], Avg: [-784.044 -784.044 -784.044] (0.105)
Step: 2249, Reward: [-711.132 -711.132 -711.132] [0.0000], Avg: [-782.423 -782.423 -782.423] (0.099)
Step: 2299, Reward: [-798.055 -798.055 -798.055] [0.0000], Avg: [-782.763 -782.763 -782.763] (0.094)
Step: 2349, Reward: [-514.054 -514.054 -514.054] [0.0000], Avg: [-777.046 -777.046 -777.046] (0.090)
Step: 2399, Reward: [-443.355 -443.355 -443.355] [0.0000], Avg: [-770.094 -770.094 -770.094] (0.085)
Step: 2449, Reward: [-615.544 -615.544 -615.544] [0.0000], Avg: [-766.94 -766.94 -766.94] (0.081)
Step: 2499, Reward: [-399.968 -399.968 -399.968] [0.0000], Avg: [-759.601 -759.601 -759.601] (0.077)
Step: 2549, Reward: [-327.448 -327.448 -327.448] [0.0000], Avg: [-751.127 -751.127 -751.127] (0.073)
Step: 2599, Reward: [-427.283 -427.283 -427.283] [0.0000], Avg: [-744.899 -744.899 -744.899] (0.069)
Step: 2649, Reward: [-665.141 -665.141 -665.141] [0.0000], Avg: [-743.394 -743.394 -743.394] (0.066)
Step: 2699, Reward: [-404.583 -404.583 -404.583] [0.0000], Avg: [-737.12 -737.12 -737.12] (0.063)
Step: 2749, Reward: [-325.78 -325.78 -325.78] [0.0000], Avg: [-729.641 -729.641 -729.641] (0.060)
Step: 2799, Reward: [-413.543 -413.543 -413.543] [0.0000], Avg: [-723.997 -723.997 -723.997] (0.057)
Step: 2849, Reward: [-358.76 -358.76 -358.76] [0.0000], Avg: [-717.589 -717.589 -717.589] (0.054)
Step: 2899, Reward: [-661.508 -661.508 -661.508] [0.0000], Avg: [-716.622 -716.622 -716.622] (0.051)
Step: 2949, Reward: [-524.078 -524.078 -524.078] [0.0000], Avg: [-713.358 -713.358 -713.358] (0.048)
Step: 2999, Reward: [-708.788 -708.788 -708.788] [0.0000], Avg: [-713.282 -713.282 -713.282] (0.046)
Step: 3049, Reward: [-551.235 -551.235 -551.235] [0.0000], Avg: [-710.626 -710.626 -710.626] (0.044)
Step: 3099, Reward: [-294.599 -294.599 -294.599] [0.0000], Avg: [-703.916 -703.916 -703.916] (0.042)
Step: 3149, Reward: [-472.813 -472.813 -472.813] [0.0000], Avg: [-700.247 -700.247 -700.247] (0.039)
Step: 3199, Reward: [-632.993 -632.993 -632.993] [0.0000], Avg: [-699.197 -699.197 -699.197] (0.038)
Step: 3249, Reward: [-546.748 -546.748 -546.748] [0.0000], Avg: [-696.851 -696.851 -696.851] (0.036)
Step: 3299, Reward: [-363.298 -363.298 -363.298] [0.0000], Avg: [-691.797 -691.797 -691.797] (0.034)
Step: 3349, Reward: [-620.01 -620.01 -620.01] [0.0000], Avg: [-690.726 -690.726 -690.726] (0.032)
Step: 3399, Reward: [-772.887 -772.887 -772.887] [0.0000], Avg: [-691.934 -691.934 -691.934] (0.031)
Step: 3449, Reward: [-455.872 -455.872 -455.872] [0.0000], Avg: [-688.513 -688.513 -688.513] (0.029)
Step: 3499, Reward: [-484.067 -484.067 -484.067] [0.0000], Avg: [-685.592 -685.592 -685.592] (0.028)
Step: 3549, Reward: [-657.314 -657.314 -657.314] [0.0000], Avg: [-685.194 -685.194 -685.194] (0.026)
Step: 3599, Reward: [-443.777 -443.777 -443.777] [0.0000], Avg: [-681.841 -681.841 -681.841] (0.025)
Step: 3649, Reward: [-415.852 -415.852 -415.852] [0.0000], Avg: [-678.197 -678.197 -678.197] (0.024)
Step: 3699, Reward: [-484.715 -484.715 -484.715] [0.0000], Avg: [-675.583 -675.583 -675.583] (0.022)
Step: 3749, Reward: [-469.807 -469.807 -469.807] [0.0000], Avg: [-672.839 -672.839 -672.839] (0.021)
Step: 3799, Reward: [-554.299 -554.299 -554.299] [0.0000], Avg: [-671.279 -671.279 -671.279] (0.020)
Step: 3849, Reward: [-451.849 -451.849 -451.849] [0.0000], Avg: [-668.43 -668.43 -668.43] (0.020)
Step: 3899, Reward: [-397.123 -397.123 -397.123] [0.0000], Avg: [-664.951 -664.951 -664.951] (0.020)
Step: 3949, Reward: [-571.287 -571.287 -571.287] [0.0000], Avg: [-663.766 -663.766 -663.766] (0.020)
Step: 3999, Reward: [-459.803 -459.803 -459.803] [0.0000], Avg: [-661.216 -661.216 -661.216] (0.020)
Step: 4049, Reward: [-404.551 -404.551 -404.551] [0.0000], Avg: [-658.047 -658.047 -658.047] (0.020)
Step: 4099, Reward: [-509.74 -509.74 -509.74] [0.0000], Avg: [-656.239 -656.239 -656.239] (0.020)
Step: 4149, Reward: [-429.393 -429.393 -429.393] [0.0000], Avg: [-653.506 -653.506 -653.506] (0.020)
Step: 4199, Reward: [-610.073 -610.073 -610.073] [0.0000], Avg: [-652.989 -652.989 -652.989] (0.020)
Step: 4249, Reward: [-508.698 -508.698 -508.698] [0.0000], Avg: [-651.291 -651.291 -651.291] (0.020)
Step: 4299, Reward: [-429.118 -429.118 -429.118] [0.0000], Avg: [-648.708 -648.708 -648.708] (0.020)
Step: 4349, Reward: [-433.689 -433.689 -433.689] [0.0000], Avg: [-646.236 -646.236 -646.236] (0.020)
Step: 4399, Reward: [-538.502 -538.502 -538.502] [0.0000], Avg: [-645.012 -645.012 -645.012] (0.020)
Step: 4449, Reward: [-571.774 -571.774 -571.774] [0.0000], Avg: [-644.189 -644.189 -644.189] (0.020)
Step: 4499, Reward: [-460.555 -460.555 -460.555] [0.0000], Avg: [-642.149 -642.149 -642.149] (0.020)
Step: 4549, Reward: [-449.209 -449.209 -449.209] [0.0000], Avg: [-640.028 -640.028 -640.028] (0.020)
Step: 4599, Reward: [-615.361 -615.361 -615.361] [0.0000], Avg: [-639.76 -639.76 -639.76] (0.020)
Step: 4649, Reward: [-702.742 -702.742 -702.742] [0.0000], Avg: [-640.438 -640.438 -640.438] (0.020)
Step: 4699, Reward: [-475.69 -475.69 -475.69] [0.0000], Avg: [-638.685 -638.685 -638.685] (0.020)
Step: 4749, Reward: [-504.344 -504.344 -504.344] [0.0000], Avg: [-637.271 -637.271 -637.271] (0.020)
Step: 4799, Reward: [-559.695 -559.695 -559.695] [0.0000], Avg: [-636.463 -636.463 -636.463] (0.020)
Step: 4849, Reward: [-487.559 -487.559 -487.559] [0.0000], Avg: [-634.928 -634.928 -634.928] (0.020)
Step: 4899, Reward: [-781.09 -781.09 -781.09] [0.0000], Avg: [-636.419 -636.419 -636.419] (0.020)
Step: 4949, Reward: [-790.177 -790.177 -790.177] [0.0000], Avg: [-637.972 -637.972 -637.972] (0.020)
Step: 4999, Reward: [-478.454 -478.454 -478.454] [0.0000], Avg: [-636.377 -636.377 -636.377] (0.020)
Step: 5049, Reward: [-499.451 -499.451 -499.451] [0.0000], Avg: [-635.021 -635.021 -635.021] (0.020)
Step: 5099, Reward: [-460.376 -460.376 -460.376] [0.0000], Avg: [-633.309 -633.309 -633.309] (0.020)
Step: 5149, Reward: [-665.376 -665.376 -665.376] [0.0000], Avg: [-633.62 -633.62 -633.62] (0.020)
Step: 5199, Reward: [-626.525 -626.525 -626.525] [0.0000], Avg: [-633.552 -633.552 -633.552] (0.020)
Step: 5249, Reward: [-645.491 -645.491 -645.491] [0.0000], Avg: [-633.666 -633.666 -633.666] (0.020)
Step: 5299, Reward: [-729.351 -729.351 -729.351] [0.0000], Avg: [-634.569 -634.569 -634.569] (0.020)
Step: 5349, Reward: [-1271.781 -1271.781 -1271.781] [0.0000], Avg: [-640.524 -640.524 -640.524] (0.020)
Step: 5399, Reward: [-544.283 -544.283 -544.283] [0.0000], Avg: [-639.633 -639.633 -639.633] (0.020)
Step: 5449, Reward: [-690.472 -690.472 -690.472] [0.0000], Avg: [-640.099 -640.099 -640.099] (0.020)
Step: 5499, Reward: [-811.293 -811.293 -811.293] [0.0000], Avg: [-641.655 -641.655 -641.655] (0.020)
Step: 5549, Reward: [-492.065 -492.065 -492.065] [0.0000], Avg: [-640.308 -640.308 -640.308] (0.020)
Step: 5599, Reward: [-634.291 -634.291 -634.291] [0.0000], Avg: [-640.254 -640.254 -640.254] (0.020)
Step: 5649, Reward: [-499.306 -499.306 -499.306] [0.0000], Avg: [-639.007 -639.007 -639.007] (0.020)
Step: 5699, Reward: [-594.316 -594.316 -594.316] [0.0000], Avg: [-638.615 -638.615 -638.615] (0.020)
Step: 5749, Reward: [-626.023 -626.023 -626.023] [0.0000], Avg: [-638.505 -638.505 -638.505] (0.020)
Step: 5799, Reward: [-560.328 -560.328 -560.328] [0.0000], Avg: [-637.831 -637.831 -637.831] (0.020)
Step: 5849, Reward: [-680.163 -680.163 -680.163] [0.0000], Avg: [-638.193 -638.193 -638.193] (0.020)
Step: 5899, Reward: [-600.771 -600.771 -600.771] [0.0000], Avg: [-637.876 -637.876 -637.876] (0.020)
Step: 5949, Reward: [-480.492 -480.492 -480.492] [0.0000], Avg: [-636.553 -636.553 -636.553] (0.020)
Step: 5999, Reward: [-495.838 -495.838 -495.838] [0.0000], Avg: [-635.381 -635.381 -635.381] (0.020)
Step: 6049, Reward: [-610.352 -610.352 -610.352] [0.0000], Avg: [-635.174 -635.174 -635.174] (0.020)
Step: 6099, Reward: [-480.906 -480.906 -480.906] [0.0000], Avg: [-633.909 -633.909 -633.909] (0.020)
Step: 6149, Reward: [-565.193 -565.193 -565.193] [0.0000], Avg: [-633.351 -633.351 -633.351] (0.020)
Step: 6199, Reward: [-517.654 -517.654 -517.654] [0.0000], Avg: [-632.418 -632.418 -632.418] (0.020)
Step: 6249, Reward: [-465.696 -465.696 -465.696] [0.0000], Avg: [-631.084 -631.084 -631.084] (0.020)
Step: 6299, Reward: [-412.406 -412.406 -412.406] [0.0000], Avg: [-629.348 -629.348 -629.348] (0.020)
Step: 6349, Reward: [-510.753 -510.753 -510.753] [0.0000], Avg: [-628.415 -628.415 -628.415] (0.020)
Step: 6399, Reward: [-389.373 -389.373 -389.373] [0.0000], Avg: [-626.547 -626.547 -626.547] (0.020)
Step: 6449, Reward: [-642.822 -642.822 -642.822] [0.0000], Avg: [-626.673 -626.673 -626.673] (0.020)
Step: 6499, Reward: [-651.676 -651.676 -651.676] [0.0000], Avg: [-626.866 -626.866 -626.866] (0.020)
Step: 6549, Reward: [-681.195 -681.195 -681.195] [0.0000], Avg: [-627.28 -627.28 -627.28] (0.020)
Step: 6599, Reward: [-496.296 -496.296 -496.296] [0.0000], Avg: [-626.288 -626.288 -626.288] (0.020)
Step: 6649, Reward: [-597.131 -597.131 -597.131] [0.0000], Avg: [-626.069 -626.069 -626.069] (0.020)
Step: 6699, Reward: [-694.648 -694.648 -694.648] [0.0000], Avg: [-626.581 -626.581 -626.581] (0.020)
Step: 6749, Reward: [-774.378 -774.378 -774.378] [0.0000], Avg: [-627.675 -627.675 -627.675] (0.020)
Step: 6799, Reward: [-544.922 -544.922 -544.922] [0.0000], Avg: [-627.067 -627.067 -627.067] (0.020)
Step: 6849, Reward: [-780.841 -780.841 -780.841] [0.0000], Avg: [-628.189 -628.189 -628.189] (0.020)
Step: 6899, Reward: [-742.719 -742.719 -742.719] [0.0000], Avg: [-629.019 -629.019 -629.019] (0.020)
Step: 6949, Reward: [-615.088 -615.088 -615.088] [0.0000], Avg: [-628.919 -628.919 -628.919] (0.020)
Step: 6999, Reward: [-685.157 -685.157 -685.157] [0.0000], Avg: [-629.321 -629.321 -629.321] (0.020)
Step: 7049, Reward: [-427.628 -427.628 -427.628] [0.0000], Avg: [-627.89 -627.89 -627.89] (0.020)
Step: 7099, Reward: [-745.775 -745.775 -745.775] [0.0000], Avg: [-628.72 -628.72 -628.72] (0.020)
Step: 7149, Reward: [-720.57 -720.57 -720.57] [0.0000], Avg: [-629.363 -629.363 -629.363] (0.020)
Step: 7199, Reward: [-481.87 -481.87 -481.87] [0.0000], Avg: [-628.338 -628.338 -628.338] (0.020)
Step: 7249, Reward: [-451.331 -451.331 -451.331] [0.0000], Avg: [-627.118 -627.118 -627.118] (0.020)
Step: 7299, Reward: [-620.54 -620.54 -620.54] [0.0000], Avg: [-627.073 -627.073 -627.073] (0.020)
Step: 7349, Reward: [-710.348 -710.348 -710.348] [0.0000], Avg: [-627.639 -627.639 -627.639] (0.020)
Step: 7399, Reward: [-494.156 -494.156 -494.156] [0.0000], Avg: [-626.737 -626.737 -626.737] (0.020)
Step: 7449, Reward: [-619.396 -619.396 -619.396] [0.0000], Avg: [-626.688 -626.688 -626.688] (0.020)
Step: 7499, Reward: [-549.016 -549.016 -549.016] [0.0000], Avg: [-626.17 -626.17 -626.17] (0.020)
Step: 7549, Reward: [-481.812 -481.812 -481.812] [0.0000], Avg: [-625.214 -625.214 -625.214] (0.020)
Step: 7599, Reward: [-588.934 -588.934 -588.934] [0.0000], Avg: [-624.976 -624.976 -624.976] (0.020)
Step: 7649, Reward: [-449.711 -449.711 -449.711] [0.0000], Avg: [-623.83 -623.83 -623.83] (0.020)
Step: 7699, Reward: [-637.341 -637.341 -637.341] [0.0000], Avg: [-623.918 -623.918 -623.918] (0.020)
Step: 7749, Reward: [-400.543 -400.543 -400.543] [0.0000], Avg: [-622.477 -622.477 -622.477] (0.020)
Step: 7799, Reward: [-602.001 -602.001 -602.001] [0.0000], Avg: [-622.345 -622.345 -622.345] (0.020)
Step: 7849, Reward: [-527.839 -527.839 -527.839] [0.0000], Avg: [-621.743 -621.743 -621.743] (0.020)
Step: 7899, Reward: [-627.872 -627.872 -627.872] [0.0000], Avg: [-621.782 -621.782 -621.782] (0.020)
Step: 7949, Reward: [-501.503 -501.503 -501.503] [0.0000], Avg: [-621.026 -621.026 -621.026] (0.020)
Step: 7999, Reward: [-831.132 -831.132 -831.132] [0.0000], Avg: [-622.339 -622.339 -622.339] (0.020)
Step: 8049, Reward: [-512.469 -512.469 -512.469] [0.0000], Avg: [-621.656 -621.656 -621.656] (0.020)
Step: 8099, Reward: [-506.95 -506.95 -506.95] [0.0000], Avg: [-620.948 -620.948 -620.948] (0.020)
Step: 8149, Reward: [-507.042 -507.042 -507.042] [0.0000], Avg: [-620.25 -620.25 -620.25] (0.020)
Step: 8199, Reward: [-455.705 -455.705 -455.705] [0.0000], Avg: [-619.246 -619.246 -619.246] (0.020)
Step: 8249, Reward: [-538.823 -538.823 -538.823] [0.0000], Avg: [-618.759 -618.759 -618.759] (0.020)
Step: 8299, Reward: [-607.739 -607.739 -607.739] [0.0000], Avg: [-618.692 -618.692 -618.692] (0.020)
Step: 8349, Reward: [-425.564 -425.564 -425.564] [0.0000], Avg: [-617.536 -617.536 -617.536] (0.020)
Step: 8399, Reward: [-567.261 -567.261 -567.261] [0.0000], Avg: [-617.237 -617.237 -617.237] (0.020)
Step: 8449, Reward: [-553.109 -553.109 -553.109] [0.0000], Avg: [-616.857 -616.857 -616.857] (0.020)
Step: 8499, Reward: [-718.753 -718.753 -718.753] [0.0000], Avg: [-617.457 -617.457 -617.457] (0.020)
Step: 8549, Reward: [-536.293 -536.293 -536.293] [0.0000], Avg: [-616.982 -616.982 -616.982] (0.020)
Step: 8599, Reward: [-454.994 -454.994 -454.994] [0.0000], Avg: [-616.04 -616.04 -616.04] (0.020)
Step: 8649, Reward: [-419.582 -419.582 -419.582] [0.0000], Avg: [-614.905 -614.905 -614.905] (0.020)
Step: 8699, Reward: [-698.117 -698.117 -698.117] [0.0000], Avg: [-615.383 -615.383 -615.383] (0.020)
Step: 8749, Reward: [-623.454 -623.454 -623.454] [0.0000], Avg: [-615.429 -615.429 -615.429] (0.020)
Step: 8799, Reward: [-437.109 -437.109 -437.109] [0.0000], Avg: [-614.416 -614.416 -614.416] (0.020)
Step: 8849, Reward: [-438.909 -438.909 -438.909] [0.0000], Avg: [-613.424 -613.424 -613.424] (0.020)
Step: 8899, Reward: [-522.393 -522.393 -522.393] [0.0000], Avg: [-612.913 -612.913 -612.913] (0.020)
Step: 8949, Reward: [-424.577 -424.577 -424.577] [0.0000], Avg: [-611.861 -611.861 -611.861] (0.020)
Step: 8999, Reward: [-434.26 -434.26 -434.26] [0.0000], Avg: [-610.874 -610.874 -610.874] (0.020)
Step: 9049, Reward: [-565.395 -565.395 -565.395] [0.0000], Avg: [-610.623 -610.623 -610.623] (0.020)
Step: 9099, Reward: [-611.309 -611.309 -611.309] [0.0000], Avg: [-610.627 -610.627 -610.627] (0.020)
Step: 9149, Reward: [-358.873 -358.873 -358.873] [0.0000], Avg: [-609.251 -609.251 -609.251] (0.020)
Step: 9199, Reward: [-562.904 -562.904 -562.904] [0.0000], Avg: [-608.999 -608.999 -608.999] (0.020)
Step: 9249, Reward: [-576.114 -576.114 -576.114] [0.0000], Avg: [-608.821 -608.821 -608.821] (0.020)
Step: 9299, Reward: [-673.488 -673.488 -673.488] [0.0000], Avg: [-609.169 -609.169 -609.169] (0.020)
Step: 9349, Reward: [-510.189 -510.189 -510.189] [0.0000], Avg: [-608.64 -608.64 -608.64] (0.020)
Step: 9399, Reward: [-606.946 -606.946 -606.946] [0.0000], Avg: [-608.631 -608.631 -608.631] (0.020)
Step: 9449, Reward: [-494.998 -494.998 -494.998] [0.0000], Avg: [-608.029 -608.029 -608.029] (0.020)
Step: 9499, Reward: [-634.638 -634.638 -634.638] [0.0000], Avg: [-608.169 -608.169 -608.169] (0.020)
Step: 9549, Reward: [-590.142 -590.142 -590.142] [0.0000], Avg: [-608.075 -608.075 -608.075] (0.020)
Step: 9599, Reward: [-461.632 -461.632 -461.632] [0.0000], Avg: [-607.312 -607.312 -607.312] (0.020)
Step: 9649, Reward: [-666.472 -666.472 -666.472] [0.0000], Avg: [-607.619 -607.619 -607.619] (0.020)
Step: 9699, Reward: [-454.292 -454.292 -454.292] [0.0000], Avg: [-606.828 -606.828 -606.828] (0.020)
Step: 9749, Reward: [-501.571 -501.571 -501.571] [0.0000], Avg: [-606.289 -606.289 -606.289] (0.020)
Step: 9799, Reward: [-485.793 -485.793 -485.793] [0.0000], Avg: [-605.674 -605.674 -605.674] (0.020)
Step: 9849, Reward: [-641.812 -641.812 -641.812] [0.0000], Avg: [-605.857 -605.857 -605.857] (0.020)
Step: 9899, Reward: [-715.557 -715.557 -715.557] [0.0000], Avg: [-606.411 -606.411 -606.411] (0.020)
Step: 9949, Reward: [-493.735 -493.735 -493.735] [0.0000], Avg: [-605.845 -605.845 -605.845] (0.020)
Step: 9999, Reward: [-560.478 -560.478 -560.478] [0.0000], Avg: [-605.618 -605.618 -605.618] (0.020)
Step: 10049, Reward: [-575.926 -575.926 -575.926] [0.0000], Avg: [-605.471 -605.471 -605.471] (0.020)
Step: 10099, Reward: [-532.068 -532.068 -532.068] [0.0000], Avg: [-605.107 -605.107 -605.107] (0.020)
Step: 10149, Reward: [-651.178 -651.178 -651.178] [0.0000], Avg: [-605.334 -605.334 -605.334] (0.020)
Step: 10199, Reward: [-617.927 -617.927 -617.927] [0.0000], Avg: [-605.396 -605.396 -605.396] (0.020)
Step: 10249, Reward: [-559.852 -559.852 -559.852] [0.0000], Avg: [-605.174 -605.174 -605.174] (0.020)
Step: 10299, Reward: [-504.492 -504.492 -504.492] [0.0000], Avg: [-604.685 -604.685 -604.685] (0.020)
Step: 10349, Reward: [-686.474 -686.474 -686.474] [0.0000], Avg: [-605.08 -605.08 -605.08] (0.020)
Step: 10399, Reward: [-697.507 -697.507 -697.507] [0.0000], Avg: [-605.524 -605.524 -605.524] (0.020)
Step: 10449, Reward: [-250.674 -250.674 -250.674] [0.0000], Avg: [-603.827 -603.827 -603.827] (0.020)
Step: 10499, Reward: [-530.412 -530.412 -530.412] [0.0000], Avg: [-603.477 -603.477 -603.477] (0.020)
Step: 10549, Reward: [-685.041 -685.041 -685.041] [0.0000], Avg: [-603.864 -603.864 -603.864] (0.020)
Step: 10599, Reward: [-568.533 -568.533 -568.533] [0.0000], Avg: [-603.697 -603.697 -603.697] (0.020)
Step: 10649, Reward: [-761.491 -761.491 -761.491] [0.0000], Avg: [-604.438 -604.438 -604.438] (0.020)
Step: 10699, Reward: [-447.84 -447.84 -447.84] [0.0000], Avg: [-603.706 -603.706 -603.706] (0.020)
Step: 10749, Reward: [-731.299 -731.299 -731.299] [0.0000], Avg: [-604.299 -604.299 -604.299] (0.020)
Step: 10799, Reward: [-700.139 -700.139 -700.139] [0.0000], Avg: [-604.743 -604.743 -604.743] (0.020)
Step: 10849, Reward: [-883.991 -883.991 -883.991] [0.0000], Avg: [-606.03 -606.03 -606.03] (0.020)
Step: 10899, Reward: [-686.735 -686.735 -686.735] [0.0000], Avg: [-606.4 -606.4 -606.4] (0.020)
Step: 10949, Reward: [-596.791 -596.791 -596.791] [0.0000], Avg: [-606.356 -606.356 -606.356] (0.020)
Step: 10999, Reward: [-503.092 -503.092 -503.092] [0.0000], Avg: [-605.887 -605.887 -605.887] (0.020)
Step: 11049, Reward: [-669.707 -669.707 -669.707] [0.0000], Avg: [-606.176 -606.176 -606.176] (0.020)
Step: 11099, Reward: [-519.408 -519.408 -519.408] [0.0000], Avg: [-605.785 -605.785 -605.785] (0.020)
Step: 11149, Reward: [-586.303 -586.303 -586.303] [0.0000], Avg: [-605.697 -605.697 -605.697] (0.020)
Step: 11199, Reward: [-527.323 -527.323 -527.323] [0.0000], Avg: [-605.348 -605.348 -605.348] (0.020)
Step: 11249, Reward: [-807.749 -807.749 -807.749] [0.0000], Avg: [-606.247 -606.247 -606.247] (0.020)
Step: 11299, Reward: [-638.49 -638.49 -638.49] [0.0000], Avg: [-606.39 -606.39 -606.39] (0.020)
Step: 11349, Reward: [-606.547 -606.547 -606.547] [0.0000], Avg: [-606.391 -606.391 -606.391] (0.020)
Step: 11399, Reward: [-535.816 -535.816 -535.816] [0.0000], Avg: [-606.081 -606.081 -606.081] (0.020)
Step: 11449, Reward: [-701.235 -701.235 -701.235] [0.0000], Avg: [-606.497 -606.497 -606.497] (0.020)
Step: 11499, Reward: [-431.193 -431.193 -431.193] [0.0000], Avg: [-605.734 -605.734 -605.734] (0.020)
Step: 11549, Reward: [-591.726 -591.726 -591.726] [0.0000], Avg: [-605.674 -605.674 -605.674] (0.020)
Step: 11599, Reward: [-528.944 -528.944 -528.944] [0.0000], Avg: [-605.343 -605.343 -605.343] (0.020)
Step: 11649, Reward: [-524.168 -524.168 -524.168] [0.0000], Avg: [-604.995 -604.995 -604.995] (0.020)
Step: 11699, Reward: [-949.719 -949.719 -949.719] [0.0000], Avg: [-606.468 -606.468 -606.468] (0.020)
Step: 11749, Reward: [-733.164 -733.164 -733.164] [0.0000], Avg: [-607.007 -607.007 -607.007] (0.020)
Step: 11799, Reward: [-476.523 -476.523 -476.523] [0.0000], Avg: [-606.454 -606.454 -606.454] (0.020)
Step: 11849, Reward: [-675.877 -675.877 -675.877] [0.0000], Avg: [-606.747 -606.747 -606.747] (0.020)
Step: 11899, Reward: [-509.021 -509.021 -509.021] [0.0000], Avg: [-606.336 -606.336 -606.336] (0.020)
Step: 11949, Reward: [-795.438 -795.438 -795.438] [0.0000], Avg: [-607.127 -607.127 -607.127] (0.020)
Step: 11999, Reward: [-536.002 -536.002 -536.002] [0.0000], Avg: [-606.831 -606.831 -606.831] (0.020)
Step: 12049, Reward: [-675.737 -675.737 -675.737] [0.0000], Avg: [-607.117 -607.117 -607.117] (0.020)
Step: 12099, Reward: [-696.853 -696.853 -696.853] [0.0000], Avg: [-607.488 -607.488 -607.488] (0.020)
Step: 12149, Reward: [-488.398 -488.398 -488.398] [0.0000], Avg: [-606.998 -606.998 -606.998] (0.020)
Step: 12199, Reward: [-642.476 -642.476 -642.476] [0.0000], Avg: [-607.143 -607.143 -607.143] (0.020)
Step: 12249, Reward: [-652.585 -652.585 -652.585] [0.0000], Avg: [-607.329 -607.329 -607.329] (0.020)
Step: 12299, Reward: [-743.877 -743.877 -743.877] [0.0000], Avg: [-607.884 -607.884 -607.884] (0.020)
Step: 12349, Reward: [-594.267 -594.267 -594.267] [0.0000], Avg: [-607.829 -607.829 -607.829] (0.020)
Step: 12399, Reward: [-492.926 -492.926 -492.926] [0.0000], Avg: [-607.365 -607.365 -607.365] (0.020)
Step: 12449, Reward: [-719.498 -719.498 -719.498] [0.0000], Avg: [-607.816 -607.816 -607.816] (0.020)
Step: 12499, Reward: [-601.318 -601.318 -601.318] [0.0000], Avg: [-607.79 -607.79 -607.79] (0.020)
Step: 12549, Reward: [-467.265 -467.265 -467.265] [0.0000], Avg: [-607.23 -607.23 -607.23] (0.020)
Step: 12599, Reward: [-442.331 -442.331 -442.331] [0.0000], Avg: [-606.575 -606.575 -606.575] (0.020)
Step: 12649, Reward: [-491.733 -491.733 -491.733] [0.0000], Avg: [-606.121 -606.121 -606.121] (0.020)
Step: 12699, Reward: [-450.862 -450.862 -450.862] [0.0000], Avg: [-605.51 -605.51 -605.51] (0.020)
Step: 12749, Reward: [-574.692 -574.692 -574.692] [0.0000], Avg: [-605.389 -605.389 -605.389] (0.020)
Step: 12799, Reward: [-595.856 -595.856 -595.856] [0.0000], Avg: [-605.352 -605.352 -605.352] (0.020)
Step: 12849, Reward: [-628.115 -628.115 -628.115] [0.0000], Avg: [-605.441 -605.441 -605.441] (0.020)
Step: 12899, Reward: [-517.903 -517.903 -517.903] [0.0000], Avg: [-605.101 -605.101 -605.101] (0.020)
Step: 12949, Reward: [-747.544 -747.544 -747.544] [0.0000], Avg: [-605.651 -605.651 -605.651] (0.020)
Step: 12999, Reward: [-685.871 -685.871 -685.871] [0.0000], Avg: [-605.96 -605.96 -605.96] (0.020)
Step: 13049, Reward: [-530.692 -530.692 -530.692] [0.0000], Avg: [-605.672 -605.672 -605.672] (0.020)
Step: 13099, Reward: [-449.103 -449.103 -449.103] [0.0000], Avg: [-605.074 -605.074 -605.074] (0.020)
Step: 13149, Reward: [-668.446 -668.446 -668.446] [0.0000], Avg: [-605.315 -605.315 -605.315] (0.020)
Step: 13199, Reward: [-570.847 -570.847 -570.847] [0.0000], Avg: [-605.184 -605.184 -605.184] (0.020)
Step: 13249, Reward: [-629.931 -629.931 -629.931] [0.0000], Avg: [-605.278 -605.278 -605.278] (0.020)
Step: 13299, Reward: [-523.395 -523.395 -523.395] [0.0000], Avg: [-604.97 -604.97 -604.97] (0.020)
Step: 13349, Reward: [-960.111 -960.111 -960.111] [0.0000], Avg: [-606.3 -606.3 -606.3] (0.020)
Step: 13399, Reward: [-537.475 -537.475 -537.475] [0.0000], Avg: [-606.043 -606.043 -606.043] (0.020)
Step: 13449, Reward: [-525.545 -525.545 -525.545] [0.0000], Avg: [-605.744 -605.744 -605.744] (0.020)
Step: 13499, Reward: [-521.925 -521.925 -521.925] [0.0000], Avg: [-605.434 -605.434 -605.434] (0.020)
Step: 13549, Reward: [-743.026 -743.026 -743.026] [0.0000], Avg: [-605.941 -605.941 -605.941] (0.020)
Step: 13599, Reward: [-454.174 -454.174 -454.174] [0.0000], Avg: [-605.383 -605.383 -605.383] (0.020)
Step: 13649, Reward: [-593.516 -593.516 -593.516] [0.0000], Avg: [-605.34 -605.34 -605.34] (0.020)
Step: 13699, Reward: [-382.486 -382.486 -382.486] [0.0000], Avg: [-604.526 -604.526 -604.526] (0.020)
Step: 13749, Reward: [-624.381 -624.381 -624.381] [0.0000], Avg: [-604.599 -604.599 -604.599] (0.020)
Step: 13799, Reward: [-566.979 -566.979 -566.979] [0.0000], Avg: [-604.462 -604.462 -604.462] (0.020)
Step: 13849, Reward: [-360.118 -360.118 -360.118] [0.0000], Avg: [-603.58 -603.58 -603.58] (0.020)
Step: 13899, Reward: [-551.396 -551.396 -551.396] [0.0000], Avg: [-603.393 -603.393 -603.393] (0.020)
Step: 13949, Reward: [-747.625 -747.625 -747.625] [0.0000], Avg: [-603.909 -603.909 -603.909] (0.020)
Step: 13999, Reward: [-691.813 -691.813 -691.813] [0.0000], Avg: [-604.223 -604.223 -604.223] (0.020)
Step: 14049, Reward: [-662.333 -662.333 -662.333] [0.0000], Avg: [-604.43 -604.43 -604.43] (0.020)
Step: 14099, Reward: [-718.812 -718.812 -718.812] [0.0000], Avg: [-604.836 -604.836 -604.836] (0.020)
Step: 14149, Reward: [-494.201 -494.201 -494.201] [0.0000], Avg: [-604.445 -604.445 -604.445] (0.020)
Step: 14199, Reward: [-413.792 -413.792 -413.792] [0.0000], Avg: [-603.774 -603.774 -603.774] (0.020)
Step: 14249, Reward: [-429.48 -429.48 -429.48] [0.0000], Avg: [-603.162 -603.162 -603.162] (0.020)
Step: 14299, Reward: [-542.439 -542.439 -542.439] [0.0000], Avg: [-602.95 -602.95 -602.95] (0.020)
Step: 14349, Reward: [-753.819 -753.819 -753.819] [0.0000], Avg: [-603.475 -603.475 -603.475] (0.020)
Step: 14399, Reward: [-524.474 -524.474 -524.474] [0.0000], Avg: [-603.201 -603.201 -603.201] (0.020)
Step: 14449, Reward: [-718.719 -718.719 -718.719] [0.0000], Avg: [-603.601 -603.601 -603.601] (0.020)
Step: 14499, Reward: [-813.998 -813.998 -813.998] [0.0000], Avg: [-604.326 -604.326 -604.326] (0.020)
Step: 14549, Reward: [-423.915 -423.915 -423.915] [0.0000], Avg: [-603.706 -603.706 -603.706] (0.020)
Step: 14599, Reward: [-432.493 -432.493 -432.493] [0.0000], Avg: [-603.12 -603.12 -603.12] (0.020)
Step: 14649, Reward: [-491.198 -491.198 -491.198] [0.0000], Avg: [-602.738 -602.738 -602.738] (0.020)
Step: 14699, Reward: [-629.652 -629.652 -629.652] [0.0000], Avg: [-602.83 -602.83 -602.83] (0.020)
Step: 14749, Reward: [-695.345 -695.345 -695.345] [0.0000], Avg: [-603.143 -603.143 -603.143] (0.020)
Step: 14799, Reward: [-779.998 -779.998 -779.998] [0.0000], Avg: [-603.741 -603.741 -603.741] (0.020)
Step: 14849, Reward: [-1141.255 -1141.255 -1141.255] [0.0000], Avg: [-605.55 -605.55 -605.55] (0.020)
Step: 14899, Reward: [-567.991 -567.991 -567.991] [0.0000], Avg: [-605.424 -605.424 -605.424] (0.020)
Step: 14949, Reward: [-661.423 -661.423 -661.423] [0.0000], Avg: [-605.612 -605.612 -605.612] (0.020)
Step: 14999, Reward: [-431.301 -431.301 -431.301] [0.0000], Avg: [-605.031 -605.031 -605.031] (0.020)
Step: 15049, Reward: [-877.02 -877.02 -877.02] [0.0000], Avg: [-605.934 -605.934 -605.934] (0.020)
Step: 15099, Reward: [-692.599 -692.599 -692.599] [0.0000], Avg: [-606.221 -606.221 -606.221] (0.020)
Step: 15149, Reward: [-633.871 -633.871 -633.871] [0.0000], Avg: [-606.313 -606.313 -606.313] (0.020)
Step: 15199, Reward: [-593.64 -593.64 -593.64] [0.0000], Avg: [-606.271 -606.271 -606.271] (0.020)
Step: 15249, Reward: [-651.398 -651.398 -651.398] [0.0000], Avg: [-606.419 -606.419 -606.419] (0.020)
Step: 15299, Reward: [-398.095 -398.095 -398.095] [0.0000], Avg: [-605.738 -605.738 -605.738] (0.020)
Step: 15349, Reward: [-508.761 -508.761 -508.761] [0.0000], Avg: [-605.422 -605.422 -605.422] (0.020)
Step: 15399, Reward: [-360.17 -360.17 -360.17] [0.0000], Avg: [-604.626 -604.626 -604.626] (0.020)
Step: 15449, Reward: [-426.834 -426.834 -426.834] [0.0000], Avg: [-604.05 -604.05 -604.05] (0.020)
Step: 15499, Reward: [-806.97 -806.97 -806.97] [0.0000], Avg: [-604.705 -604.705 -604.705] (0.020)
Step: 15549, Reward: [-684.931 -684.931 -684.931] [0.0000], Avg: [-604.963 -604.963 -604.963] (0.020)
Step: 15599, Reward: [-654.857 -654.857 -654.857] [0.0000], Avg: [-605.123 -605.123 -605.123] (0.020)
Step: 15649, Reward: [-583.095 -583.095 -583.095] [0.0000], Avg: [-605.053 -605.053 -605.053] (0.020)
Step: 15699, Reward: [-607.356 -607.356 -607.356] [0.0000], Avg: [-605.06 -605.06 -605.06] (0.020)
Step: 15749, Reward: [-591.432 -591.432 -591.432] [0.0000], Avg: [-605.017 -605.017 -605.017] (0.020)
Step: 15799, Reward: [-552.694 -552.694 -552.694] [0.0000], Avg: [-604.851 -604.851 -604.851] (0.020)
Step: 15849, Reward: [-766.501 -766.501 -766.501] [0.0000], Avg: [-605.361 -605.361 -605.361] (0.020)
Step: 15899, Reward: [-496.146 -496.146 -496.146] [0.0000], Avg: [-605.018 -605.018 -605.018] (0.020)
Step: 15949, Reward: [-655.175 -655.175 -655.175] [0.0000], Avg: [-605.175 -605.175 -605.175] (0.020)
Step: 15999, Reward: [-474.141 -474.141 -474.141] [0.0000], Avg: [-604.765 -604.765 -604.765] (0.020)
Step: 16049, Reward: [-439.543 -439.543 -439.543] [0.0000], Avg: [-604.251 -604.251 -604.251] (0.020)
Step: 16099, Reward: [-793.668 -793.668 -793.668] [0.0000], Avg: [-604.839 -604.839 -604.839] (0.020)
Step: 16149, Reward: [-392.818 -392.818 -392.818] [0.0000], Avg: [-604.182 -604.182 -604.182] (0.020)
Step: 16199, Reward: [-675.583 -675.583 -675.583] [0.0000], Avg: [-604.403 -604.403 -604.403] (0.020)
Step: 16249, Reward: [-399.792 -399.792 -399.792] [0.0000], Avg: [-603.773 -603.773 -603.773] (0.020)
Step: 16299, Reward: [-446.738 -446.738 -446.738] [0.0000], Avg: [-603.291 -603.291 -603.291] (0.020)
Step: 16349, Reward: [-417.551 -417.551 -417.551] [0.0000], Avg: [-602.723 -602.723 -602.723] (0.020)
Step: 16399, Reward: [-477.199 -477.199 -477.199] [0.0000], Avg: [-602.341 -602.341 -602.341] (0.020)
Step: 16449, Reward: [-793.494 -793.494 -793.494] [0.0000], Avg: [-602.922 -602.922 -602.922] (0.020)
Step: 16499, Reward: [-598.205 -598.205 -598.205] [0.0000], Avg: [-602.908 -602.908 -602.908] (0.020)
Step: 16549, Reward: [-767.821 -767.821 -767.821] [0.0000], Avg: [-603.406 -603.406 -603.406] (0.020)
Step: 16599, Reward: [-643.896 -643.896 -643.896] [0.0000], Avg: [-603.528 -603.528 -603.528] (0.020)
Step: 16649, Reward: [-781.691 -781.691 -781.691] [0.0000], Avg: [-604.063 -604.063 -604.063] (0.020)
Step: 16699, Reward: [-737.012 -737.012 -737.012] [0.0000], Avg: [-604.461 -604.461 -604.461] (0.020)
Step: 16749, Reward: [-521.122 -521.122 -521.122] [0.0000], Avg: [-604.212 -604.212 -604.212] (0.020)
Step: 16799, Reward: [-612.721 -612.721 -612.721] [0.0000], Avg: [-604.237 -604.237 -604.237] (0.020)
Step: 16849, Reward: [-385.921 -385.921 -385.921] [0.0000], Avg: [-603.589 -603.589 -603.589] (0.020)
Step: 16899, Reward: [-657.357 -657.357 -657.357] [0.0000], Avg: [-603.749 -603.749 -603.749] (0.020)
Step: 16949, Reward: [-781.543 -781.543 -781.543] [0.0000], Avg: [-604.273 -604.273 -604.273] (0.020)
Step: 16999, Reward: [-600.63 -600.63 -600.63] [0.0000], Avg: [-604.262 -604.262 -604.262] (0.020)
Step: 17049, Reward: [-735.573 -735.573 -735.573] [0.0000], Avg: [-604.647 -604.647 -604.647] (0.020)
Step: 17099, Reward: [-526.7 -526.7 -526.7] [0.0000], Avg: [-604.419 -604.419 -604.419] (0.020)
Step: 17149, Reward: [-901.524 -901.524 -901.524] [0.0000], Avg: [-605.286 -605.286 -605.286] (0.020)
Step: 17199, Reward: [-633.123 -633.123 -633.123] [0.0000], Avg: [-605.367 -605.367 -605.367] (0.020)
Step: 17249, Reward: [-897.56 -897.56 -897.56] [0.0000], Avg: [-606.214 -606.214 -606.214] (0.020)
Step: 17299, Reward: [-596.648 -596.648 -596.648] [0.0000], Avg: [-606.186 -606.186 -606.186] (0.020)
Step: 17349, Reward: [-588.182 -588.182 -588.182] [0.0000], Avg: [-606.134 -606.134 -606.134] (0.020)
Step: 17399, Reward: [-620.482 -620.482 -620.482] [0.0000], Avg: [-606.175 -606.175 -606.175] (0.020)
Step: 17449, Reward: [-623.436 -623.436 -623.436] [0.0000], Avg: [-606.225 -606.225 -606.225] (0.020)
Step: 17499, Reward: [-552.142 -552.142 -552.142] [0.0000], Avg: [-606.07 -606.07 -606.07] (0.020)
Step: 17549, Reward: [-450.409 -450.409 -450.409] [0.0000], Avg: [-605.627 -605.627 -605.627] (0.020)
Step: 17599, Reward: [-577.916 -577.916 -577.916] [0.0000], Avg: [-605.548 -605.548 -605.548] (0.020)
Step: 17649, Reward: [-460.833 -460.833 -460.833] [0.0000], Avg: [-605.138 -605.138 -605.138] (0.020)
Step: 17699, Reward: [-590.133 -590.133 -590.133] [0.0000], Avg: [-605.096 -605.096 -605.096] (0.020)
Step: 17749, Reward: [-542.271 -542.271 -542.271] [0.0000], Avg: [-604.919 -604.919 -604.919] (0.020)
Step: 17799, Reward: [-390.183 -390.183 -390.183] [0.0000], Avg: [-604.315 -604.315 -604.315] (0.020)
Step: 17849, Reward: [-659.691 -659.691 -659.691] [0.0000], Avg: [-604.471 -604.471 -604.471] (0.020)
Step: 17899, Reward: [-620.346 -620.346 -620.346] [0.0000], Avg: [-604.515 -604.515 -604.515] (0.020)
Step: 17949, Reward: [-678.388 -678.388 -678.388] [0.0000], Avg: [-604.721 -604.721 -604.721] (0.020)
Step: 17999, Reward: [-746.278 -746.278 -746.278] [0.0000], Avg: [-605.114 -605.114 -605.114] (0.020)
Step: 18049, Reward: [-446.925 -446.925 -446.925] [0.0000], Avg: [-604.676 -604.676 -604.676] (0.020)
Step: 18099, Reward: [-925.962 -925.962 -925.962] [0.0000], Avg: [-605.563 -605.563 -605.563] (0.020)
Step: 18149, Reward: [-573.509 -573.509 -573.509] [0.0000], Avg: [-605.475 -605.475 -605.475] (0.020)
Step: 18199, Reward: [-507.214 -507.214 -507.214] [0.0000], Avg: [-605.205 -605.205 -605.205] (0.020)
Step: 18249, Reward: [-569.79 -569.79 -569.79] [0.0000], Avg: [-605.108 -605.108 -605.108] (0.020)
Step: 18299, Reward: [-601.508 -601.508 -601.508] [0.0000], Avg: [-605.098 -605.098 -605.098] (0.020)
Step: 18349, Reward: [-968.949 -968.949 -968.949] [0.0000], Avg: [-606.09 -606.09 -606.09] (0.020)
Step: 18399, Reward: [-589.324 -589.324 -589.324] [0.0000], Avg: [-606.044 -606.044 -606.044] (0.020)
Step: 18449, Reward: [-601.461 -601.461 -601.461] [0.0000], Avg: [-606.032 -606.032 -606.032] (0.020)
Step: 18499, Reward: [-710.452 -710.452 -710.452] [0.0000], Avg: [-606.314 -606.314 -606.314] (0.020)
Step: 18549, Reward: [-580.562 -580.562 -580.562] [0.0000], Avg: [-606.244 -606.244 -606.244] (0.020)
Step: 18599, Reward: [-816.638 -816.638 -816.638] [0.0000], Avg: [-606.81 -606.81 -606.81] (0.020)
Step: 18649, Reward: [-622.071 -622.071 -622.071] [0.0000], Avg: [-606.851 -606.851 -606.851] (0.020)
Step: 18699, Reward: [-466.936 -466.936 -466.936] [0.0000], Avg: [-606.477 -606.477 -606.477] (0.020)
Step: 18749, Reward: [-407.638 -407.638 -407.638] [0.0000], Avg: [-605.947 -605.947 -605.947] (0.020)
Step: 18799, Reward: [-541.235 -541.235 -541.235] [0.0000], Avg: [-605.774 -605.774 -605.774] (0.020)
Step: 18849, Reward: [-572.243 -572.243 -572.243] [0.0000], Avg: [-605.685 -605.685 -605.685] (0.020)
Step: 18899, Reward: [-776.551 -776.551 -776.551] [0.0000], Avg: [-606.138 -606.138 -606.138] (0.020)
Step: 18949, Reward: [-687.722 -687.722 -687.722] [0.0000], Avg: [-606.353 -606.353 -606.353] (0.020)
Step: 18999, Reward: [-837.014 -837.014 -837.014] [0.0000], Avg: [-606.96 -606.96 -606.96] (0.020)
Step: 19049, Reward: [-899.767 -899.767 -899.767] [0.0000], Avg: [-607.728 -607.728 -607.728] (0.020)
Step: 19099, Reward: [-683.374 -683.374 -683.374] [0.0000], Avg: [-607.926 -607.926 -607.926] (0.020)
Step: 19149, Reward: [-596.676 -596.676 -596.676] [0.0000], Avg: [-607.897 -607.897 -607.897] (0.020)
Step: 19199, Reward: [-562.563 -562.563 -562.563] [0.0000], Avg: [-607.779 -607.779 -607.779] (0.020)
Step: 19249, Reward: [-744.166 -744.166 -744.166] [0.0000], Avg: [-608.133 -608.133 -608.133] (0.020)
Step: 19299, Reward: [-518.272 -518.272 -518.272] [0.0000], Avg: [-607.9 -607.9 -607.9] (0.020)
Step: 19349, Reward: [-840.692 -840.692 -840.692] [0.0000], Avg: [-608.502 -608.502 -608.502] (0.020)
Step: 19399, Reward: [-403.762 -403.762 -403.762] [0.0000], Avg: [-607.974 -607.974 -607.974] (0.020)
Step: 19449, Reward: [-565.251 -565.251 -565.251] [0.0000], Avg: [-607.864 -607.864 -607.864] (0.020)
Step: 19499, Reward: [-584.634 -584.634 -584.634] [0.0000], Avg: [-607.805 -607.805 -607.805] (0.020)
Step: 19549, Reward: [-591.199 -591.199 -591.199] [0.0000], Avg: [-607.762 -607.762 -607.762] (0.020)
Step: 19599, Reward: [-498.72 -498.72 -498.72] [0.0000], Avg: [-607.484 -607.484 -607.484] (0.020)
Step: 19649, Reward: [-664.97 -664.97 -664.97] [0.0000], Avg: [-607.63 -607.63 -607.63] (0.020)
Step: 19699, Reward: [-546.593 -546.593 -546.593] [0.0000], Avg: [-607.476 -607.476 -607.476] (0.020)
Step: 19749, Reward: [-641.23 -641.23 -641.23] [0.0000], Avg: [-607.561 -607.561 -607.561] (0.020)
Step: 19799, Reward: [-439.678 -439.678 -439.678] [0.0000], Avg: [-607.137 -607.137 -607.137] (0.020)
Step: 19849, Reward: [-482.235 -482.235 -482.235] [0.0000], Avg: [-606.822 -606.822 -606.822] (0.020)
Step: 19899, Reward: [-505.853 -505.853 -505.853] [0.0000], Avg: [-606.569 -606.569 -606.569] (0.020)
Step: 19949, Reward: [-451.096 -451.096 -451.096] [0.0000], Avg: [-606.179 -606.179 -606.179] (0.020)
Step: 19999, Reward: [-470.504 -470.504 -470.504] [0.0000], Avg: [-605.84 -605.84 -605.84] (0.020)
Step: 20049, Reward: [-569.362 -569.362 -569.362] [0.0000], Avg: [-605.749 -605.749 -605.749] (0.020)
Step: 20099, Reward: [-571.66 -571.66 -571.66] [0.0000], Avg: [-605.664 -605.664 -605.664] (0.020)
Step: 20149, Reward: [-521.4 -521.4 -521.4] [0.0000], Avg: [-605.455 -605.455 -605.455] (0.020)
Step: 20199, Reward: [-365.433 -365.433 -365.433] [0.0000], Avg: [-604.861 -604.861 -604.861] (0.020)
Step: 20249, Reward: [-932.717 -932.717 -932.717] [0.0000], Avg: [-605.67 -605.67 -605.67] (0.020)
Step: 20299, Reward: [-667.069 -667.069 -667.069] [0.0000], Avg: [-605.822 -605.822 -605.822] (0.020)
Step: 20349, Reward: [-630.558 -630.558 -630.558] [0.0000], Avg: [-605.882 -605.882 -605.882] (0.020)
Step: 20399, Reward: [-547.145 -547.145 -547.145] [0.0000], Avg: [-605.738 -605.738 -605.738] (0.020)
Step: 20449, Reward: [-387.407 -387.407 -387.407] [0.0000], Avg: [-605.205 -605.205 -605.205] (0.020)
Step: 20499, Reward: [-693.553 -693.553 -693.553] [0.0000], Avg: [-605.42 -605.42 -605.42] (0.020)
Step: 20549, Reward: [-447.79 -447.79 -447.79] [0.0000], Avg: [-605.037 -605.037 -605.037] (0.020)
Step: 20599, Reward: [-350.714 -350.714 -350.714] [0.0000], Avg: [-604.419 -604.419 -604.419] (0.020)
Step: 20649, Reward: [-1017.955 -1017.955 -1017.955] [0.0000], Avg: [-605.421 -605.421 -605.421] (0.020)
Step: 20699, Reward: [-661.891 -661.891 -661.891] [0.0000], Avg: [-605.557 -605.557 -605.557] (0.020)
Step: 20749, Reward: [-581.492 -581.492 -581.492] [0.0000], Avg: [-605.499 -605.499 -605.499] (0.020)
Step: 20799, Reward: [-367.629 -367.629 -367.629] [0.0000], Avg: [-604.927 -604.927 -604.927] (0.020)
Step: 20849, Reward: [-645.312 -645.312 -645.312] [0.0000], Avg: [-605.024 -605.024 -605.024] (0.020)
Step: 20899, Reward: [-766.658 -766.658 -766.658] [0.0000], Avg: [-605.411 -605.411 -605.411] (0.020)
Step: 20949, Reward: [-431.749 -431.749 -431.749] [0.0000], Avg: [-604.996 -604.996 -604.996] (0.020)
Step: 20999, Reward: [-452.359 -452.359 -452.359] [0.0000], Avg: [-604.633 -604.633 -604.633] (0.020)
Step: 21049, Reward: [-584.227 -584.227 -584.227] [0.0000], Avg: [-604.584 -604.584 -604.584] (0.020)
Step: 21099, Reward: [-687.741 -687.741 -687.741] [0.0000], Avg: [-604.781 -604.781 -604.781] (0.020)
Step: 21149, Reward: [-770.659 -770.659 -770.659] [0.0000], Avg: [-605.174 -605.174 -605.174] (0.020)
Step: 21199, Reward: [-534.06 -534.06 -534.06] [0.0000], Avg: [-605.006 -605.006 -605.006] (0.020)
Step: 21249, Reward: [-458.144 -458.144 -458.144] [0.0000], Avg: [-604.66 -604.66 -604.66] (0.020)
Step: 21299, Reward: [-651.359 -651.359 -651.359] [0.0000], Avg: [-604.77 -604.77 -604.77] (0.020)
Step: 21349, Reward: [-521.045 -521.045 -521.045] [0.0000], Avg: [-604.574 -604.574 -604.574] (0.020)
Step: 21399, Reward: [-680.9 -680.9 -680.9] [0.0000], Avg: [-604.752 -604.752 -604.752] (0.020)
Step: 21449, Reward: [-590.221 -590.221 -590.221] [0.0000], Avg: [-604.718 -604.718 -604.718] (0.020)
Step: 21499, Reward: [-602.286 -602.286 -602.286] [0.0000], Avg: [-604.713 -604.713 -604.713] (0.020)
Step: 21549, Reward: [-577.544 -577.544 -577.544] [0.0000], Avg: [-604.65 -604.65 -604.65] (0.020)
Step: 21599, Reward: [-703.683 -703.683 -703.683] [0.0000], Avg: [-604.879 -604.879 -604.879] (0.020)
Step: 21649, Reward: [-382.684 -382.684 -382.684] [0.0000], Avg: [-604.366 -604.366 -604.366] (0.020)
Step: 21699, Reward: [-567.808 -567.808 -567.808] [0.0000], Avg: [-604.281 -604.281 -604.281] (0.020)
Step: 21749, Reward: [-684.736 -684.736 -684.736] [0.0000], Avg: [-604.466 -604.466 -604.466] (0.020)
Step: 21799, Reward: [-519.266 -519.266 -519.266] [0.0000], Avg: [-604.271 -604.271 -604.271] (0.020)
Step: 21849, Reward: [-489.375 -489.375 -489.375] [0.0000], Avg: [-604.008 -604.008 -604.008] (0.020)
Step: 21899, Reward: [-482.464 -482.464 -482.464] [0.0000], Avg: [-603.731 -603.731 -603.731] (0.020)
Step: 21949, Reward: [-791.929 -791.929 -791.929] [0.0000], Avg: [-604.159 -604.159 -604.159] (0.020)
Step: 21999, Reward: [-709.864 -709.864 -709.864] [0.0000], Avg: [-604.4 -604.4 -604.4] (0.020)
Step: 22049, Reward: [-658.244 -658.244 -658.244] [0.0000], Avg: [-604.522 -604.522 -604.522] (0.020)
Step: 22099, Reward: [-682.379 -682.379 -682.379] [0.0000], Avg: [-604.698 -604.698 -604.698] (0.020)
Step: 22149, Reward: [-743.166 -743.166 -743.166] [0.0000], Avg: [-605.01 -605.01 -605.01] (0.020)
Step: 22199, Reward: [-876.115 -876.115 -876.115] [0.0000], Avg: [-605.621 -605.621 -605.621] (0.020)
Step: 22249, Reward: [-704.19 -704.19 -704.19] [0.0000], Avg: [-605.842 -605.842 -605.842] (0.020)
Step: 22299, Reward: [-601.803 -601.803 -601.803] [0.0000], Avg: [-605.833 -605.833 -605.833] (0.020)
Step: 22349, Reward: [-485.366 -485.366 -485.366] [0.0000], Avg: [-605.564 -605.564 -605.564] (0.020)
Step: 22399, Reward: [-881.115 -881.115 -881.115] [0.0000], Avg: [-606.179 -606.179 -606.179] (0.020)
Step: 22449, Reward: [-597.289 -597.289 -597.289] [0.0000], Avg: [-606.159 -606.159 -606.159] (0.020)
Step: 22499, Reward: [-408.779 -408.779 -408.779] [0.0000], Avg: [-605.721 -605.721 -605.721] (0.020)
Step: 22549, Reward: [-654.409 -654.409 -654.409] [0.0000], Avg: [-605.828 -605.828 -605.828] (0.020)
Step: 22599, Reward: [-598.201 -598.201 -598.201] [0.0000], Avg: [-605.812 -605.812 -605.812] (0.020)
Step: 22649, Reward: [-618.751 -618.751 -618.751] [0.0000], Avg: [-605.84 -605.84 -605.84] (0.020)
Step: 22699, Reward: [-616.595 -616.595 -616.595] [0.0000], Avg: [-605.864 -605.864 -605.864] (0.020)
Step: 22749, Reward: [-681.248 -681.248 -681.248] [0.0000], Avg: [-606.03 -606.03 -606.03] (0.020)
Step: 22799, Reward: [-609.19 -609.19 -609.19] [0.0000], Avg: [-606.036 -606.036 -606.036] (0.020)
Step: 22849, Reward: [-525.738 -525.738 -525.738] [0.0000], Avg: [-605.861 -605.861 -605.861] (0.020)
Step: 22899, Reward: [-700.749 -700.749 -700.749] [0.0000], Avg: [-606.068 -606.068 -606.068] (0.020)
Step: 22949, Reward: [-432.305 -432.305 -432.305] [0.0000], Avg: [-605.689 -605.689 -605.689] (0.020)
Step: 22999, Reward: [-967.075 -967.075 -967.075] [0.0000], Avg: [-606.475 -606.475 -606.475] (0.020)
Step: 23049, Reward: [-567.827 -567.827 -567.827] [0.0000], Avg: [-606.391 -606.391 -606.391] (0.020)
Step: 23099, Reward: [-697.34 -697.34 -697.34] [0.0000], Avg: [-606.588 -606.588 -606.588] (0.020)
Step: 23149, Reward: [-501.642 -501.642 -501.642] [0.0000], Avg: [-606.361 -606.361 -606.361] (0.020)
Step: 23199, Reward: [-657.927 -657.927 -657.927] [0.0000], Avg: [-606.472 -606.472 -606.472] (0.020)
Step: 23249, Reward: [-535.897 -535.897 -535.897] [0.0000], Avg: [-606.321 -606.321 -606.321] (0.020)
Step: 23299, Reward: [-933.447 -933.447 -933.447] [0.0000], Avg: [-607.023 -607.023 -607.023] (0.020)
Step: 23349, Reward: [-599.134 -599.134 -599.134] [0.0000], Avg: [-607.006 -607.006 -607.006] (0.020)
Step: 23399, Reward: [-912.845 -912.845 -912.845] [0.0000], Avg: [-607.659 -607.659 -607.659] (0.020)
Step: 23449, Reward: [-1361.734 -1361.734 -1361.734] [0.0000], Avg: [-609.267 -609.267 -609.267] (0.020)
Step: 23499, Reward: [-672.107 -672.107 -672.107] [0.0000], Avg: [-609.401 -609.401 -609.401] (0.020)
Step: 23549, Reward: [-1426.378 -1426.378 -1426.378] [0.0000], Avg: [-611.135 -611.135 -611.135] (0.020)
Step: 23599, Reward: [-1197.456 -1197.456 -1197.456] [0.0000], Avg: [-612.378 -612.378 -612.378] (0.020)
Step: 23649, Reward: [-1523.194 -1523.194 -1523.194] [0.0000], Avg: [-614.303 -614.303 -614.303] (0.020)
Step: 23699, Reward: [-1488.879 -1488.879 -1488.879] [0.0000], Avg: [-616.148 -616.148 -616.148] (0.020)
Step: 23749, Reward: [-1222.989 -1222.989 -1222.989] [0.0000], Avg: [-617.426 -617.426 -617.426] (0.020)
Step: 23799, Reward: [-1277.65 -1277.65 -1277.65] [0.0000], Avg: [-618.813 -618.813 -618.813] (0.020)
Step: 23849, Reward: [-1905.309 -1905.309 -1905.309] [0.0000], Avg: [-621.51 -621.51 -621.51] (0.020)
Step: 23899, Reward: [-2146.103 -2146.103 -2146.103] [0.0000], Avg: [-624.699 -624.699 -624.699] (0.020)
Step: 23949, Reward: [-1816.302 -1816.302 -1816.302] [0.0000], Avg: [-627.187 -627.187 -627.187] (0.020)
Step: 23999, Reward: [-1186.9 -1186.9 -1186.9] [0.0000], Avg: [-628.353 -628.353 -628.353] (0.020)
Step: 24049, Reward: [-1802.459 -1802.459 -1802.459] [0.0000], Avg: [-630.794 -630.794 -630.794] (0.020)
Step: 24099, Reward: [-1727.224 -1727.224 -1727.224] [0.0000], Avg: [-633.069 -633.069 -633.069] (0.020)
Step: 24149, Reward: [-1095.449 -1095.449 -1095.449] [0.0000], Avg: [-634.026 -634.026 -634.026] (0.020)
Step: 24199, Reward: [-1659.511 -1659.511 -1659.511] [0.0000], Avg: [-636.145 -636.145 -636.145] (0.020)
Step: 24249, Reward: [-1442.347 -1442.347 -1442.347] [0.0000], Avg: [-637.807 -637.807 -637.807] (0.020)
Step: 24299, Reward: [-2029.224 -2029.224 -2029.224] [0.0000], Avg: [-640.67 -640.67 -640.67] (0.020)
Step: 24349, Reward: [-1621.724 -1621.724 -1621.724] [0.0000], Avg: [-642.685 -642.685 -642.685] (0.020)
Step: 24399, Reward: [-1814.936 -1814.936 -1814.936] [0.0000], Avg: [-645.087 -645.087 -645.087] (0.020)
Step: 24449, Reward: [-2057. -2057. -2057.] [0.0000], Avg: [-647.974 -647.974 -647.974] (0.020)
Step: 24499, Reward: [-1960.784 -1960.784 -1960.784] [0.0000], Avg: [-650.654 -650.654 -650.654] (0.020)
Step: 24549, Reward: [-1282.795 -1282.795 -1282.795] [0.0000], Avg: [-651.941 -651.941 -651.941] (0.020)
Step: 24599, Reward: [-1105.861 -1105.861 -1105.861] [0.0000], Avg: [-652.864 -652.864 -652.864] (0.020)
Step: 24649, Reward: [-1144.527 -1144.527 -1144.527] [0.0000], Avg: [-653.861 -653.861 -653.861] (0.020)
Step: 24699, Reward: [-908.485 -908.485 -908.485] [0.0000], Avg: [-654.376 -654.376 -654.376] (0.020)
Step: 24749, Reward: [-680.063 -680.063 -680.063] [0.0000], Avg: [-654.428 -654.428 -654.428] (0.020)
Step: 24799, Reward: [-613.049 -613.049 -613.049] [0.0000], Avg: [-654.345 -654.345 -654.345] (0.020)
Step: 24849, Reward: [-931.959 -931.959 -931.959] [0.0000], Avg: [-654.903 -654.903 -654.903] (0.020)
Step: 24899, Reward: [-1045.273 -1045.273 -1045.273] [0.0000], Avg: [-655.687 -655.687 -655.687] (0.020)
Step: 24949, Reward: [-1042.466 -1042.466 -1042.466] [0.0000], Avg: [-656.462 -656.462 -656.462] (0.020)
Step: 24999, Reward: [-1351.29 -1351.29 -1351.29] [0.0000], Avg: [-657.852 -657.852 -657.852] (0.020)
Step: 25049, Reward: [-1480.513 -1480.513 -1480.513] [0.0000], Avg: [-659.494 -659.494 -659.494] (0.020)
Step: 25099, Reward: [-1176.72 -1176.72 -1176.72] [0.0000], Avg: [-660.524 -660.524 -660.524] (0.020)
Step: 25149, Reward: [-1091.736 -1091.736 -1091.736] [0.0000], Avg: [-661.382 -661.382 -661.382] (0.020)
Step: 25199, Reward: [-1128.187 -1128.187 -1128.187] [0.0000], Avg: [-662.308 -662.308 -662.308] (0.020)
Step: 25249, Reward: [-984.436 -984.436 -984.436] [0.0000], Avg: [-662.946 -662.946 -662.946] (0.020)
Step: 25299, Reward: [-1415.23 -1415.23 -1415.23] [0.0000], Avg: [-664.432 -664.432 -664.432] (0.020)
Step: 25349, Reward: [-1212.353 -1212.353 -1212.353] [0.0000], Avg: [-665.513 -665.513 -665.513] (0.020)
Step: 25399, Reward: [-994.192 -994.192 -994.192] [0.0000], Avg: [-666.16 -666.16 -666.16] (0.020)
Step: 25449, Reward: [-1119.593 -1119.593 -1119.593] [0.0000], Avg: [-667.051 -667.051 -667.051] (0.020)
Step: 25499, Reward: [-978.485 -978.485 -978.485] [0.0000], Avg: [-667.662 -667.662 -667.662] (0.020)
Step: 25549, Reward: [-910.506 -910.506 -910.506] [0.0000], Avg: [-668.137 -668.137 -668.137] (0.020)
Step: 25599, Reward: [-876.817 -876.817 -876.817] [0.0000], Avg: [-668.544 -668.544 -668.544] (0.020)
Step: 25649, Reward: [-1222.233 -1222.233 -1222.233] [0.0000], Avg: [-669.624 -669.624 -669.624] (0.020)
Step: 25699, Reward: [-887.001 -887.001 -887.001] [0.0000], Avg: [-670.047 -670.047 -670.047] (0.020)
Step: 25749, Reward: [-617.715 -617.715 -617.715] [0.0000], Avg: [-669.945 -669.945 -669.945] (0.020)
Step: 25799, Reward: [-946.777 -946.777 -946.777] [0.0000], Avg: [-670.482 -670.482 -670.482] (0.020)
Step: 25849, Reward: [-855.32 -855.32 -855.32] [0.0000], Avg: [-670.839 -670.839 -670.839] (0.020)
Step: 25899, Reward: [-825.314 -825.314 -825.314] [0.0000], Avg: [-671.137 -671.137 -671.137] (0.020)
Step: 25949, Reward: [-1135.578 -1135.578 -1135.578] [0.0000], Avg: [-672.032 -672.032 -672.032] (0.020)
Step: 25999, Reward: [-1389.788 -1389.788 -1389.788] [0.0000], Avg: [-673.412 -673.412 -673.412] (0.020)
Step: 26049, Reward: [-1059.158 -1059.158 -1059.158] [0.0000], Avg: [-674.153 -674.153 -674.153] (0.020)
Step: 26099, Reward: [-931.596 -931.596 -931.596] [0.0000], Avg: [-674.646 -674.646 -674.646] (0.020)
Step: 26149, Reward: [-919.277 -919.277 -919.277] [0.0000], Avg: [-675.114 -675.114 -675.114] (0.020)
Step: 26199, Reward: [-854.627 -854.627 -854.627] [0.0000], Avg: [-675.456 -675.456 -675.456] (0.020)
Step: 26249, Reward: [-610.202 -610.202 -610.202] [0.0000], Avg: [-675.332 -675.332 -675.332] (0.020)
Step: 26299, Reward: [-899.128 -899.128 -899.128] [0.0000], Avg: [-675.758 -675.758 -675.758] (0.020)
Step: 26349, Reward: [-1273.794 -1273.794 -1273.794] [0.0000], Avg: [-676.892 -676.892 -676.892] (0.020)
Step: 26399, Reward: [-697.597 -697.597 -697.597] [0.0000], Avg: [-676.932 -676.932 -676.932] (0.020)
Step: 26449, Reward: [-706.039 -706.039 -706.039] [0.0000], Avg: [-676.987 -676.987 -676.987] (0.020)
Step: 26499, Reward: [-1844.02 -1844.02 -1844.02] [0.0000], Avg: [-679.189 -679.189 -679.189] (0.020)
Step: 26549, Reward: [-1004.226 -1004.226 -1004.226] [0.0000], Avg: [-679.801 -679.801 -679.801] (0.020)
Step: 26599, Reward: [-948.724 -948.724 -948.724] [0.0000], Avg: [-680.306 -680.306 -680.306] (0.020)
Step: 26649, Reward: [-1072.259 -1072.259 -1072.259] [0.0000], Avg: [-681.042 -681.042 -681.042] (0.020)
Step: 26699, Reward: [-1041.494 -1041.494 -1041.494] [0.0000], Avg: [-681.717 -681.717 -681.717] (0.020)
Step: 26749, Reward: [-512.708 -512.708 -512.708] [0.0000], Avg: [-681.401 -681.401 -681.401] (0.020)
Step: 26799, Reward: [-955.02 -955.02 -955.02] [0.0000], Avg: [-681.911 -681.911 -681.911] (0.020)
Step: 26849, Reward: [-1041.409 -1041.409 -1041.409] [0.0000], Avg: [-682.581 -682.581 -682.581] (0.020)
Step: 26899, Reward: [-1047.869 -1047.869 -1047.869] [0.0000], Avg: [-683.26 -683.26 -683.26] (0.020)
Step: 26949, Reward: [-452.215 -452.215 -452.215] [0.0000], Avg: [-682.831 -682.831 -682.831] (0.020)
Step: 26999, Reward: [-647.144 -647.144 -647.144] [0.0000], Avg: [-682.765 -682.765 -682.765] (0.020)
Step: 27049, Reward: [-613.571 -613.571 -613.571] [0.0000], Avg: [-682.637 -682.637 -682.637] (0.020)
Step: 27099, Reward: [-861.748 -861.748 -861.748] [0.0000], Avg: [-682.967 -682.967 -682.967] (0.020)
Step: 27149, Reward: [-1250.372 -1250.372 -1250.372] [0.0000], Avg: [-684.012 -684.012 -684.012] (0.020)
Step: 27199, Reward: [-946.437 -946.437 -946.437] [0.0000], Avg: [-684.495 -684.495 -684.495] (0.020)
Step: 27249, Reward: [-751.079 -751.079 -751.079] [0.0000], Avg: [-684.617 -684.617 -684.617] (0.020)
Step: 27299, Reward: [-595.498 -595.498 -595.498] [0.0000], Avg: [-684.454 -684.454 -684.454] (0.020)
Step: 27349, Reward: [-730.762 -730.762 -730.762] [0.0000], Avg: [-684.538 -684.538 -684.538] (0.020)
Step: 27399, Reward: [-765.321 -765.321 -765.321] [0.0000], Avg: [-684.686 -684.686 -684.686] (0.020)
Step: 27449, Reward: [-1133.281 -1133.281 -1133.281] [0.0000], Avg: [-685.503 -685.503 -685.503] (0.020)
Step: 27499, Reward: [-969.11 -969.11 -969.11] [0.0000], Avg: [-686.018 -686.018 -686.018] (0.020)
Step: 27549, Reward: [-1105.946 -1105.946 -1105.946] [0.0000], Avg: [-686.781 -686.781 -686.781] (0.020)
Step: 27599, Reward: [-938.782 -938.782 -938.782] [0.0000], Avg: [-687.237 -687.237 -687.237] (0.020)
Step: 27649, Reward: [-1385.7 -1385.7 -1385.7] [0.0000], Avg: [-688.5 -688.5 -688.5] (0.020)
Step: 27699, Reward: [-1193.467 -1193.467 -1193.467] [0.0000], Avg: [-689.412 -689.412 -689.412] (0.020)
Step: 27749, Reward: [-502.437 -502.437 -502.437] [0.0000], Avg: [-689.075 -689.075 -689.075] (0.020)
Step: 27799, Reward: [-1340.899 -1340.899 -1340.899] [0.0000], Avg: [-690.247 -690.247 -690.247] (0.020)
Step: 27849, Reward: [-770.374 -770.374 -770.374] [0.0000], Avg: [-690.391 -690.391 -690.391] (0.020)
Step: 27899, Reward: [-814.622 -814.622 -814.622] [0.0000], Avg: [-690.614 -690.614 -690.614] (0.020)
Step: 27949, Reward: [-960.888 -960.888 -960.888] [0.0000], Avg: [-691.097 -691.097 -691.097] (0.020)
Step: 27999, Reward: [-1424.684 -1424.684 -1424.684] [0.0000], Avg: [-692.407 -692.407 -692.407] (0.020)
Step: 28049, Reward: [-2196.951 -2196.951 -2196.951] [0.0000], Avg: [-695.089 -695.089 -695.089] (0.020)
Step: 28099, Reward: [-2018.366 -2018.366 -2018.366] [0.0000], Avg: [-697.444 -697.444 -697.444] (0.020)
Step: 28149, Reward: [-1646.068 -1646.068 -1646.068] [0.0000], Avg: [-699.129 -699.129 -699.129] (0.020)
Step: 28199, Reward: [-1225.42 -1225.42 -1225.42] [0.0000], Avg: [-700.062 -700.062 -700.062] (0.020)
Step: 28249, Reward: [-1904.509 -1904.509 -1904.509] [0.0000], Avg: [-702.193 -702.193 -702.193] (0.020)
Step: 28299, Reward: [-1910.161 -1910.161 -1910.161] [0.0000], Avg: [-704.328 -704.328 -704.328] (0.020)
Step: 28349, Reward: [-1739.49 -1739.49 -1739.49] [0.0000], Avg: [-706.153 -706.153 -706.153] (0.020)
Step: 28399, Reward: [-1789.504 -1789.504 -1789.504] [0.0000], Avg: [-708.061 -708.061 -708.061] (0.020)
Step: 28449, Reward: [-1868.794 -1868.794 -1868.794] [0.0000], Avg: [-710.101 -710.101 -710.101] (0.020)
Step: 28499, Reward: [-2136.488 -2136.488 -2136.488] [0.0000], Avg: [-712.603 -712.603 -712.603] (0.020)
Step: 28549, Reward: [-1269.141 -1269.141 -1269.141] [0.0000], Avg: [-713.578 -713.578 -713.578] (0.020)
Step: 28599, Reward: [-1978.791 -1978.791 -1978.791] [0.0000], Avg: [-715.79 -715.79 -715.79] (0.020)
Step: 28649, Reward: [-1993.513 -1993.513 -1993.513] [0.0000], Avg: [-718.019 -718.019 -718.019] (0.020)
Step: 28699, Reward: [-2030.956 -2030.956 -2030.956] [0.0000], Avg: [-720.307 -720.307 -720.307] (0.020)
