Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7fef64e07c88>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7fef64e07be0>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7fef64e07b00>],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, gumbel_softmax

ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		# self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		# self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state)
		state = self.layer1(state).relu() 
		# state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		# action_sig = self.action_sig(state).exp()
		# epsilon = torch.randn_like(action_sig)
		# action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		action_mu = action_mu.view(*out_dims, -1)
		return gumbel_softmax(action_mu, hard=True)
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.net_state = torch.nn.Linear(state_size[-1]+action_size[-1], CRITIC_HIDDEN)
		# self.net_action = torch.nn.Linear(action_size[-1], INPUT_LAYER)
		# self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state)
		state = self.net_state(state).relu()
		# net_action = self.net_action(action).relu()
		# net_layer = torch.cat([state, net_action], dim=-1)
		# net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(state).relu()
		q_value = self.q_value(net_layer)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:-1] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else action.detach() for j,action in enumerate(actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -(q_actions - q_values.detach()) + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [np.clip((1-eps)*a_greedy + eps*a_random, -1, 1) for a_greedy,a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			self.network.optimize(states, actions, states_joint, actions_joint, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.100               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 10000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False)
	model = MADDPGAgent if type(envs.env.action_space) == list else model
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="ppo", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports)

Step: 49, Reward: [-609.718 -609.718 -609.718] [0.0000], Avg: [-609.718 -609.718 -609.718] (0.995)
Step: 99, Reward: [-872.726 -872.726 -872.726] [0.0000], Avg: [-741.222 -741.222 -741.222] (0.990)
Step: 149, Reward: [-733.315 -733.315 -733.315] [0.0000], Avg: [-738.586 -738.586 -738.586] (0.985)
Step: 199, Reward: [-580.328 -580.328 -580.328] [0.0000], Avg: [-699.022 -699.022 -699.022] (0.980)
Step: 249, Reward: [-645.759 -645.759 -645.759] [0.0000], Avg: [-688.369 -688.369 -688.369] (0.975)
Step: 299, Reward: [-906.82 -906.82 -906.82] [0.0000], Avg: [-724.777 -724.777 -724.777] (0.970)
Step: 349, Reward: [-1343.573 -1343.573 -1343.573] [0.0000], Avg: [-813.177 -813.177 -813.177] (0.966)
Step: 399, Reward: [-670.068 -670.068 -670.068] [0.0000], Avg: [-795.288 -795.288 -795.288] (0.961)
Step: 449, Reward: [-867.295 -867.295 -867.295] [0.0000], Avg: [-803.289 -803.289 -803.289] (0.956)
Step: 499, Reward: [-469.135 -469.135 -469.135] [0.0000], Avg: [-769.874 -769.874 -769.874] (0.951)
Step: 549, Reward: [-650.784 -650.784 -650.784] [0.0000], Avg: [-759.047 -759.047 -759.047] (0.946)
Step: 599, Reward: [-582.634 -582.634 -582.634] [0.0000], Avg: [-744.346 -744.346 -744.346] (0.942)
Step: 649, Reward: [-605.8 -605.8 -605.8] [0.0000], Avg: [-733.689 -733.689 -733.689] (0.937)
Step: 699, Reward: [-813.062 -813.062 -813.062] [0.0000], Avg: [-739.358 -739.358 -739.358] (0.932)
Step: 749, Reward: [-1155.273 -1155.273 -1155.273] [0.0000], Avg: [-767.086 -767.086 -767.086] (0.928)
Step: 799, Reward: [-554.899 -554.899 -554.899] [0.0000], Avg: [-753.824 -753.824 -753.824] (0.923)
Step: 849, Reward: [-1201.985 -1201.985 -1201.985] [0.0000], Avg: [-780.187 -780.187 -780.187] (0.918)
Step: 899, Reward: [-795.402 -795.402 -795.402] [0.0000], Avg: [-781.032 -781.032 -781.032] (0.914)
Step: 949, Reward: [-430.314 -430.314 -430.314] [0.0000], Avg: [-762.573 -762.573 -762.573] (0.909)
Step: 999, Reward: [-844.406 -844.406 -844.406] [0.0000], Avg: [-766.665 -766.665 -766.665] (0.905)
Step: 1049, Reward: [-1369.434 -1369.434 -1369.434] [0.0000], Avg: [-795.368 -795.368 -795.368] (0.900)
Step: 1099, Reward: [-871.254 -871.254 -871.254] [0.0000], Avg: [-798.817 -798.817 -798.817] (0.896)
Step: 1149, Reward: [-966.834 -966.834 -966.834] [0.0000], Avg: [-806.122 -806.122 -806.122] (0.891)
Step: 1199, Reward: [-616.682 -616.682 -616.682] [0.0000], Avg: [-798.229 -798.229 -798.229] (0.887)
Step: 1249, Reward: [-1241.98 -1241.98 -1241.98] [0.0000], Avg: [-815.979 -815.979 -815.979] (0.882)
Step: 1299, Reward: [-569.248 -569.248 -569.248] [0.0000], Avg: [-806.489 -806.489 -806.489] (0.878)
Step: 1349, Reward: [-1037.353 -1037.353 -1037.353] [0.0000], Avg: [-815.04 -815.04 -815.04] (0.873)
Step: 1399, Reward: [-530.364 -530.364 -530.364] [0.0000], Avg: [-804.873 -804.873 -804.873] (0.869)
Step: 1449, Reward: [-839.733 -839.733 -839.733] [0.0000], Avg: [-806.075 -806.075 -806.075] (0.865)
Step: 1499, Reward: [-740.171 -740.171 -740.171] [0.0000], Avg: [-803.878 -803.878 -803.878] (0.860)
Step: 1549, Reward: [-1400.162 -1400.162 -1400.162] [0.0000], Avg: [-823.113 -823.113 -823.113] (0.856)
Step: 1599, Reward: [-673.088 -673.088 -673.088] [0.0000], Avg: [-818.425 -818.425 -818.425] (0.852)
Step: 1649, Reward: [-741.074 -741.074 -741.074] [0.0000], Avg: [-816.081 -816.081 -816.081] (0.848)
Step: 1699, Reward: [-1179.731 -1179.731 -1179.731] [0.0000], Avg: [-826.777 -826.777 -826.777] (0.843)
Step: 1749, Reward: [-739.512 -739.512 -739.512] [0.0000], Avg: [-824.283 -824.283 -824.283] (0.839)
Step: 1799, Reward: [-640.982 -640.982 -640.982] [0.0000], Avg: [-819.192 -819.192 -819.192] (0.835)
Step: 1849, Reward: [-529.92 -529.92 -529.92] [0.0000], Avg: [-811.373 -811.373 -811.373] (0.831)
Step: 1899, Reward: [-802.69 -802.69 -802.69] [0.0000], Avg: [-811.145 -811.145 -811.145] (0.827)
Step: 1949, Reward: [-620.906 -620.906 -620.906] [0.0000], Avg: [-806.267 -806.267 -806.267] (0.822)
Step: 1999, Reward: [-1107.156 -1107.156 -1107.156] [0.0000], Avg: [-813.789 -813.789 -813.789] (0.818)
Step: 2049, Reward: [-926.085 -926.085 -926.085] [0.0000], Avg: [-816.528 -816.528 -816.528] (0.814)
Step: 2099, Reward: [-641.149 -641.149 -641.149] [0.0000], Avg: [-812.352 -812.352 -812.352] (0.810)
Step: 2149, Reward: [-1100.833 -1100.833 -1100.833] [0.0000], Avg: [-819.061 -819.061 -819.061] (0.806)
Step: 2199, Reward: [-798.203 -798.203 -798.203] [0.0000], Avg: [-818.587 -818.587 -818.587] (0.802)
Step: 2249, Reward: [-864.823 -864.823 -864.823] [0.0000], Avg: [-819.615 -819.615 -819.615] (0.798)
Step: 2299, Reward: [-557.437 -557.437 -557.437] [0.0000], Avg: [-813.915 -813.915 -813.915] (0.794)
Step: 2349, Reward: [-618.891 -618.891 -618.891] [0.0000], Avg: [-809.766 -809.766 -809.766] (0.790)
Step: 2399, Reward: [-773.386 -773.386 -773.386] [0.0000], Avg: [-809.008 -809.008 -809.008] (0.786)
Step: 2449, Reward: [-498.096 -498.096 -498.096] [0.0000], Avg: [-802.663 -802.663 -802.663] (0.782)
Step: 2499, Reward: [-656.265 -656.265 -656.265] [0.0000], Avg: [-799.735 -799.735 -799.735] (0.778)
Step: 2549, Reward: [-571.873 -571.873 -571.873] [0.0000], Avg: [-795.267 -795.267 -795.267] (0.774)
Step: 2599, Reward: [-500.242 -500.242 -500.242] [0.0000], Avg: [-789.593 -789.593 -789.593] (0.771)
Step: 2649, Reward: [-578.537 -578.537 -578.537] [0.0000], Avg: [-785.611 -785.611 -785.611] (0.767)
Step: 2699, Reward: [-583.049 -583.049 -583.049] [0.0000], Avg: [-781.86 -781.86 -781.86] (0.763)
Step: 2749, Reward: [-718.99 -718.99 -718.99] [0.0000], Avg: [-780.717 -780.717 -780.717] (0.759)
Step: 2799, Reward: [-696.487 -696.487 -696.487] [0.0000], Avg: [-779.213 -779.213 -779.213] (0.755)
Step: 2849, Reward: [-504.312 -504.312 -504.312] [0.0000], Avg: [-774.39 -774.39 -774.39] (0.751)
Step: 2899, Reward: [-499.272 -499.272 -499.272] [0.0000], Avg: [-769.647 -769.647 -769.647] (0.748)
Step: 2949, Reward: [-387.458 -387.458 -387.458] [0.0000], Avg: [-763.169 -763.169 -763.169] (0.744)
Step: 2999, Reward: [-410.259 -410.259 -410.259] [0.0000], Avg: [-757.287 -757.287 -757.287] (0.740)
Step: 3049, Reward: [-861.054 -861.054 -861.054] [0.0000], Avg: [-758.988 -758.988 -758.988] (0.737)
Step: 3099, Reward: [-644.981 -644.981 -644.981] [0.0000], Avg: [-757.149 -757.149 -757.149] (0.733)
Step: 3149, Reward: [-877.604 -877.604 -877.604] [0.0000], Avg: [-759.061 -759.061 -759.061] (0.729)
Step: 3199, Reward: [-1251.499 -1251.499 -1251.499] [0.0000], Avg: [-766.756 -766.756 -766.756] (0.726)
Step: 3249, Reward: [-447.287 -447.287 -447.287] [0.0000], Avg: [-761.841 -761.841 -761.841] (0.722)
Step: 3299, Reward: [-679.202 -679.202 -679.202] [0.0000], Avg: [-760.589 -760.589 -760.589] (0.718)
Step: 3349, Reward: [-625.826 -625.826 -625.826] [0.0000], Avg: [-758.577 -758.577 -758.577] (0.715)
Step: 3399, Reward: [-604.322 -604.322 -604.322] [0.0000], Avg: [-756.309 -756.309 -756.309] (0.711)
Step: 3449, Reward: [-532.298 -532.298 -532.298] [0.0000], Avg: [-753.062 -753.062 -753.062] (0.708)
Step: 3499, Reward: [-473.778 -473.778 -473.778] [0.0000], Avg: [-749.072 -749.072 -749.072] (0.704)
Step: 3549, Reward: [-337.476 -337.476 -337.476] [0.0000], Avg: [-743.275 -743.275 -743.275] (0.701)
Step: 3599, Reward: [-626.125 -626.125 -626.125] [0.0000], Avg: [-741.648 -741.648 -741.648] (0.697)
Step: 3649, Reward: [-1230.188 -1230.188 -1230.188] [0.0000], Avg: [-748.341 -748.341 -748.341] (0.694)
Step: 3699, Reward: [-487.028 -487.028 -487.028] [0.0000], Avg: [-744.809 -744.809 -744.809] (0.690)
Step: 3749, Reward: [-452.763 -452.763 -452.763] [0.0000], Avg: [-740.915 -740.915 -740.915] (0.687)
Step: 3799, Reward: [-850.204 -850.204 -850.204] [0.0000], Avg: [-742.353 -742.353 -742.353] (0.683)
Step: 3849, Reward: [-454.128 -454.128 -454.128] [0.0000], Avg: [-738.61 -738.61 -738.61] (0.680)
Step: 3899, Reward: [-330.407 -330.407 -330.407] [0.0000], Avg: [-733.377 -733.377 -733.377] (0.676)
Step: 3949, Reward: [-648.99 -648.99 -648.99] [0.0000], Avg: [-732.309 -732.309 -732.309] (0.673)
Step: 3999, Reward: [-1427.648 -1427.648 -1427.648] [0.0000], Avg: [-741. -741. -741.] (0.670)
Step: 4049, Reward: [-741.844 -741.844 -741.844] [0.0000], Avg: [-741.011 -741.011 -741.011] (0.666)
Step: 4099, Reward: [-617.137 -617.137 -617.137] [0.0000], Avg: [-739.5 -739.5 -739.5] (0.663)
Step: 4149, Reward: [-487.572 -487.572 -487.572] [0.0000], Avg: [-736.465 -736.465 -736.465] (0.660)
Step: 4199, Reward: [-539.867 -539.867 -539.867] [0.0000], Avg: [-734.124 -734.124 -734.124] (0.656)
Step: 4249, Reward: [-483.649 -483.649 -483.649] [0.0000], Avg: [-731.178 -731.178 -731.178] (0.653)
Step: 4299, Reward: [-460.402 -460.402 -460.402] [0.0000], Avg: [-728.029 -728.029 -728.029] (0.650)
Step: 4349, Reward: [-1061.692 -1061.692 -1061.692] [0.0000], Avg: [-731.864 -731.864 -731.864] (0.647)
Step: 4399, Reward: [-636.204 -636.204 -636.204] [0.0000], Avg: [-730.777 -730.777 -730.777] (0.643)
Step: 4449, Reward: [-572.323 -572.323 -572.323] [0.0000], Avg: [-728.997 -728.997 -728.997] (0.640)
Step: 4499, Reward: [-460.802 -460.802 -460.802] [0.0000], Avg: [-726.017 -726.017 -726.017] (0.637)
Step: 4549, Reward: [-476.591 -476.591 -476.591] [0.0000], Avg: [-723.276 -723.276 -723.276] (0.634)
Step: 4599, Reward: [-462.076 -462.076 -462.076] [0.0000], Avg: [-720.437 -720.437 -720.437] (0.631)
Step: 4649, Reward: [-773.411 -773.411 -773.411] [0.0000], Avg: [-721.006 -721.006 -721.006] (0.627)
Step: 4699, Reward: [-579.075 -579.075 -579.075] [0.0000], Avg: [-719.497 -719.497 -719.497] (0.624)
Step: 4749, Reward: [-1112.755 -1112.755 -1112.755] [0.0000], Avg: [-723.636 -723.636 -723.636] (0.621)
Step: 4799, Reward: [-323.703 -323.703 -323.703] [0.0000], Avg: [-719.47 -719.47 -719.47] (0.618)
Step: 4849, Reward: [-528.659 -528.659 -528.659] [0.0000], Avg: [-717.503 -717.503 -717.503] (0.615)
Step: 4899, Reward: [-443.176 -443.176 -443.176] [0.0000], Avg: [-714.704 -714.704 -714.704] (0.612)
Step: 4949, Reward: [-315.102 -315.102 -315.102] [0.0000], Avg: [-710.667 -710.667 -710.667] (0.609)
Step: 4999, Reward: [-861.157 -861.157 -861.157] [0.0000], Avg: [-712.172 -712.172 -712.172] (0.606)
Step: 5049, Reward: [-561.52 -561.52 -561.52] [0.0000], Avg: [-710.681 -710.681 -710.681] (0.603)
Step: 5099, Reward: [-507.432 -507.432 -507.432] [0.0000], Avg: [-708.688 -708.688 -708.688] (0.600)
Step: 5149, Reward: [-546.541 -546.541 -546.541] [0.0000], Avg: [-707.114 -707.114 -707.114] (0.597)
Step: 5199, Reward: [-426.933 -426.933 -426.933] [0.0000], Avg: [-704.42 -704.42 -704.42] (0.594)
Step: 5249, Reward: [-580.038 -580.038 -580.038] [0.0000], Avg: [-703.235 -703.235 -703.235] (0.591)
Step: 5299, Reward: [-516.042 -516.042 -516.042] [0.0000], Avg: [-701.469 -701.469 -701.469] (0.588)
Step: 5349, Reward: [-473.301 -473.301 -473.301] [0.0000], Avg: [-699.337 -699.337 -699.337] (0.585)
Step: 5399, Reward: [-514.079 -514.079 -514.079] [0.0000], Avg: [-697.621 -697.621 -697.621] (0.582)
Step: 5449, Reward: [-467.491 -467.491 -467.491] [0.0000], Avg: [-695.51 -695.51 -695.51] (0.579)
Step: 5499, Reward: [-555.212 -555.212 -555.212] [0.0000], Avg: [-694.235 -694.235 -694.235] (0.576)
Step: 5549, Reward: [-580.32 -580.32 -580.32] [0.0000], Avg: [-693.208 -693.208 -693.208] (0.573)
Step: 5599, Reward: [-657.643 -657.643 -657.643] [0.0000], Avg: [-692.891 -692.891 -692.891] (0.570)
Step: 5649, Reward: [-558.025 -558.025 -558.025] [0.0000], Avg: [-691.697 -691.697 -691.697] (0.568)
Step: 5699, Reward: [-627.27 -627.27 -627.27] [0.0000], Avg: [-691.132 -691.132 -691.132] (0.565)
Step: 5749, Reward: [-511.799 -511.799 -511.799] [0.0000], Avg: [-689.573 -689.573 -689.573] (0.562)
Step: 5799, Reward: [-526.065 -526.065 -526.065] [0.0000], Avg: [-688.163 -688.163 -688.163] (0.559)
Step: 5849, Reward: [-608.548 -608.548 -608.548] [0.0000], Avg: [-687.483 -687.483 -687.483] (0.556)
Step: 5899, Reward: [-322.509 -322.509 -322.509] [0.0000], Avg: [-684.39 -684.39 -684.39] (0.554)
Step: 5949, Reward: [-371.175 -371.175 -371.175] [0.0000], Avg: [-681.758 -681.758 -681.758] (0.551)
Step: 5999, Reward: [-423.74 -423.74 -423.74] [0.0000], Avg: [-679.608 -679.608 -679.608] (0.548)
Step: 6049, Reward: [-702.019 -702.019 -702.019] [0.0000], Avg: [-679.793 -679.793 -679.793] (0.545)
Step: 6099, Reward: [-654.328 -654.328 -654.328] [0.0000], Avg: [-679.584 -679.584 -679.584] (0.543)
Step: 6149, Reward: [-479.922 -479.922 -479.922] [0.0000], Avg: [-677.961 -677.961 -677.961] (0.540)
Step: 6199, Reward: [-628.104 -628.104 -628.104] [0.0000], Avg: [-677.559 -677.559 -677.559] (0.537)
Step: 6249, Reward: [-605.646 -605.646 -605.646] [0.0000], Avg: [-676.983 -676.983 -676.983] (0.534)
Step: 6299, Reward: [-534.376 -534.376 -534.376] [0.0000], Avg: [-675.852 -675.852 -675.852] (0.532)
Step: 6349, Reward: [-520.834 -520.834 -520.834] [0.0000], Avg: [-674.631 -674.631 -674.631] (0.529)
Step: 6399, Reward: [-744.091 -744.091 -744.091] [0.0000], Avg: [-675.174 -675.174 -675.174] (0.526)
Step: 6449, Reward: [-469.822 -469.822 -469.822] [0.0000], Avg: [-673.582 -673.582 -673.582] (0.524)
Step: 6499, Reward: [-602.062 -602.062 -602.062] [0.0000], Avg: [-673.032 -673.032 -673.032] (0.521)
Step: 6549, Reward: [-510.533 -510.533 -510.533] [0.0000], Avg: [-671.791 -671.791 -671.791] (0.519)
Step: 6599, Reward: [-556.343 -556.343 -556.343] [0.0000], Avg: [-670.917 -670.917 -670.917] (0.516)
Step: 6649, Reward: [-346.482 -346.482 -346.482] [0.0000], Avg: [-668.477 -668.477 -668.477] (0.513)
Step: 6699, Reward: [-590.955 -590.955 -590.955] [0.0000], Avg: [-667.899 -667.899 -667.899] (0.511)
Step: 6749, Reward: [-333.934 -333.934 -333.934] [0.0000], Avg: [-665.425 -665.425 -665.425] (0.508)
Step: 6799, Reward: [-876.275 -876.275 -876.275] [0.0000], Avg: [-666.975 -666.975 -666.975] (0.506)
Step: 6849, Reward: [-708.654 -708.654 -708.654] [0.0000], Avg: [-667.279 -667.279 -667.279] (0.503)
Step: 6899, Reward: [-526.479 -526.479 -526.479] [0.0000], Avg: [-666.259 -666.259 -666.259] (0.501)
Step: 6949, Reward: [-381.033 -381.033 -381.033] [0.0000], Avg: [-664.207 -664.207 -664.207] (0.498)
Step: 6999, Reward: [-585.361 -585.361 -585.361] [0.0000], Avg: [-663.644 -663.644 -663.644] (0.496)
Step: 7049, Reward: [-451.812 -451.812 -451.812] [0.0000], Avg: [-662.142 -662.142 -662.142] (0.493)
Step: 7099, Reward: [-622.305 -622.305 -622.305] [0.0000], Avg: [-661.861 -661.861 -661.861] (0.491)
Step: 7149, Reward: [-365.263 -365.263 -365.263] [0.0000], Avg: [-659.787 -659.787 -659.787] (0.488)
Step: 7199, Reward: [-532.176 -532.176 -532.176] [0.0000], Avg: [-658.901 -658.901 -658.901] (0.486)
Step: 7249, Reward: [-535.492 -535.492 -535.492] [0.0000], Avg: [-658.05 -658.05 -658.05] (0.483)
Step: 7299, Reward: [-544.039 -544.039 -544.039] [0.0000], Avg: [-657.269 -657.269 -657.269] (0.481)
Step: 7349, Reward: [-543.894 -543.894 -543.894] [0.0000], Avg: [-656.498 -656.498 -656.498] (0.479)
Step: 7399, Reward: [-698.387 -698.387 -698.387] [0.0000], Avg: [-656.781 -656.781 -656.781] (0.476)
Step: 7449, Reward: [-788.198 -788.198 -788.198] [0.0000], Avg: [-657.663 -657.663 -657.663] (0.474)
Step: 7499, Reward: [-627.548 -627.548 -627.548] [0.0000], Avg: [-657.462 -657.462 -657.462] (0.471)
Step: 7549, Reward: [-347.602 -347.602 -347.602] [0.0000], Avg: [-655.41 -655.41 -655.41] (0.469)
Step: 7599, Reward: [-562.586 -562.586 -562.586] [0.0000], Avg: [-654.799 -654.799 -654.799] (0.467)
Step: 7649, Reward: [-556.126 -556.126 -556.126] [0.0000], Avg: [-654.154 -654.154 -654.154] (0.464)
Step: 7699, Reward: [-516.617 -516.617 -516.617] [0.0000], Avg: [-653.261 -653.261 -653.261] (0.462)
Step: 7749, Reward: [-448.442 -448.442 -448.442] [0.0000], Avg: [-651.94 -651.94 -651.94] (0.460)
Step: 7799, Reward: [-425.205 -425.205 -425.205] [0.0000], Avg: [-650.486 -650.486 -650.486] (0.458)
Step: 7849, Reward: [-439.772 -439.772 -439.772] [0.0000], Avg: [-649.144 -649.144 -649.144] (0.455)
Step: 7899, Reward: [-451.344 -451.344 -451.344] [0.0000], Avg: [-647.892 -647.892 -647.892] (0.453)
Step: 7949, Reward: [-623.167 -623.167 -623.167] [0.0000], Avg: [-647.737 -647.737 -647.737] (0.451)
Step: 7999, Reward: [-393.764 -393.764 -393.764] [0.0000], Avg: [-646.149 -646.149 -646.149] (0.448)
Step: 8049, Reward: [-507.493 -507.493 -507.493] [0.0000], Avg: [-645.288 -645.288 -645.288] (0.446)
Step: 8099, Reward: [-480.152 -480.152 -480.152] [0.0000], Avg: [-644.269 -644.269 -644.269] (0.444)
Step: 8149, Reward: [-431.158 -431.158 -431.158] [0.0000], Avg: [-642.961 -642.961 -642.961] (0.442)
Step: 8199, Reward: [-416.416 -416.416 -416.416] [0.0000], Avg: [-641.58 -641.58 -641.58] (0.440)
Step: 8249, Reward: [-461.779 -461.779 -461.779] [0.0000], Avg: [-640.49 -640.49 -640.49] (0.437)
Step: 8299, Reward: [-683.124 -683.124 -683.124] [0.0000], Avg: [-640.747 -640.747 -640.747] (0.435)
Step: 8349, Reward: [-463.741 -463.741 -463.741] [0.0000], Avg: [-639.687 -639.687 -639.687] (0.433)
Step: 8399, Reward: [-656.838 -656.838 -656.838] [0.0000], Avg: [-639.789 -639.789 -639.789] (0.431)
Step: 8449, Reward: [-522.648 -522.648 -522.648] [0.0000], Avg: [-639.096 -639.096 -639.096] (0.429)
Step: 8499, Reward: [-477.822 -477.822 -477.822] [0.0000], Avg: [-638.147 -638.147 -638.147] (0.427)
Step: 8549, Reward: [-385.138 -385.138 -385.138] [0.0000], Avg: [-636.668 -636.668 -636.668] (0.424)
Step: 8599, Reward: [-324.091 -324.091 -324.091] [0.0000], Avg: [-634.851 -634.851 -634.851] (0.422)
Step: 8649, Reward: [-886.516 -886.516 -886.516] [0.0000], Avg: [-636.305 -636.305 -636.305] (0.420)
Step: 8699, Reward: [-556.439 -556.439 -556.439] [0.0000], Avg: [-635.846 -635.846 -635.846] (0.418)
Step: 8749, Reward: [-405.965 -405.965 -405.965] [0.0000], Avg: [-634.533 -634.533 -634.533] (0.416)
Step: 8799, Reward: [-483.749 -483.749 -483.749] [0.0000], Avg: [-633.676 -633.676 -633.676] (0.414)
Step: 8849, Reward: [-363.065 -363.065 -363.065] [0.0000], Avg: [-632.147 -632.147 -632.147] (0.412)
Step: 8899, Reward: [-434.427 -434.427 -434.427] [0.0000], Avg: [-631.036 -631.036 -631.036] (0.410)
Step: 8949, Reward: [-481.136 -481.136 -481.136] [0.0000], Avg: [-630.199 -630.199 -630.199] (0.408)
Step: 8999, Reward: [-795.315 -795.315 -795.315] [0.0000], Avg: [-631.116 -631.116 -631.116] (0.406)
Step: 9049, Reward: [-452.172 -452.172 -452.172] [0.0000], Avg: [-630.128 -630.128 -630.128] (0.404)
Step: 9099, Reward: [-575.375 -575.375 -575.375] [0.0000], Avg: [-629.827 -629.827 -629.827] (0.402)
Step: 9149, Reward: [-416.395 -416.395 -416.395] [0.0000], Avg: [-628.66 -628.66 -628.66] (0.400)
Step: 9199, Reward: [-515.817 -515.817 -515.817] [0.0000], Avg: [-628.047 -628.047 -628.047] (0.398)
Step: 9249, Reward: [-556.38 -556.38 -556.38] [0.0000], Avg: [-627.66 -627.66 -627.66] (0.396)
Step: 9299, Reward: [-465.508 -465.508 -465.508] [0.0000], Avg: [-626.788 -626.788 -626.788] (0.394)
Step: 9349, Reward: [-585.27 -585.27 -585.27] [0.0000], Avg: [-626.566 -626.566 -626.566] (0.392)
Step: 9399, Reward: [-475.517 -475.517 -475.517] [0.0000], Avg: [-625.762 -625.762 -625.762] (0.390)
Step: 9449, Reward: [-418.154 -418.154 -418.154] [0.0000], Avg: [-624.664 -624.664 -624.664] (0.388)
Step: 9499, Reward: [-462.181 -462.181 -462.181] [0.0000], Avg: [-623.809 -623.809 -623.809] (0.386)
Step: 9549, Reward: [-988.295 -988.295 -988.295] [0.0000], Avg: [-625.717 -625.717 -625.717] (0.384)
Step: 9599, Reward: [-445.121 -445.121 -445.121] [0.0000], Avg: [-624.777 -624.777 -624.777] (0.382)
Step: 9649, Reward: [-509.257 -509.257 -509.257] [0.0000], Avg: [-624.178 -624.178 -624.178] (0.380)
Step: 9699, Reward: [-601.634 -601.634 -601.634] [0.0000], Avg: [-624.062 -624.062 -624.062] (0.378)
Step: 9749, Reward: [-858.835 -858.835 -858.835] [0.0000], Avg: [-625.266 -625.266 -625.266] (0.376)
Step: 9799, Reward: [-601.105 -601.105 -601.105] [0.0000], Avg: [-625.143 -625.143 -625.143] (0.374)
Step: 9849, Reward: [-497.155 -497.155 -497.155] [0.0000], Avg: [-624.493 -624.493 -624.493] (0.373)
Step: 9899, Reward: [-458.198 -458.198 -458.198] [0.0000], Avg: [-623.653 -623.653 -623.653] (0.371)
Step: 9949, Reward: [-345.978 -345.978 -345.978] [0.0000], Avg: [-622.258 -622.258 -622.258] (0.369)
Step: 9999, Reward: [-545.33 -545.33 -545.33] [0.0000], Avg: [-621.873 -621.873 -621.873] (0.367)
Step: 10049, Reward: [-870.442 -870.442 -870.442] [0.0000], Avg: [-623.11 -623.11 -623.11] (0.365)
Step: 10099, Reward: [-526.472 -526.472 -526.472] [0.0000], Avg: [-622.631 -622.631 -622.631] (0.363)
Step: 10149, Reward: [-411.781 -411.781 -411.781] [0.0000], Avg: [-621.593 -621.593 -621.593] (0.361)
Step: 10199, Reward: [-485.725 -485.725 -485.725] [0.0000], Avg: [-620.927 -620.927 -620.927] (0.360)
Step: 10249, Reward: [-413.606 -413.606 -413.606] [0.0000], Avg: [-619.915 -619.915 -619.915] (0.358)
Step: 10299, Reward: [-518.995 -518.995 -518.995] [0.0000], Avg: [-619.425 -619.425 -619.425] (0.356)
Step: 10349, Reward: [-466.41 -466.41 -466.41] [0.0000], Avg: [-618.686 -618.686 -618.686] (0.354)
Step: 10399, Reward: [-647.082 -647.082 -647.082] [0.0000], Avg: [-618.823 -618.823 -618.823] (0.353)
Step: 10449, Reward: [-542.164 -542.164 -542.164] [0.0000], Avg: [-618.456 -618.456 -618.456] (0.351)
Step: 10499, Reward: [-384.941 -384.941 -384.941] [0.0000], Avg: [-617.344 -617.344 -617.344] (0.349)
Step: 10549, Reward: [-537.641 -537.641 -537.641] [0.0000], Avg: [-616.966 -616.966 -616.966] (0.347)
Step: 10599, Reward: [-604.47 -604.47 -604.47] [0.0000], Avg: [-616.907 -616.907 -616.907] (0.346)
Step: 10649, Reward: [-577.884 -577.884 -577.884] [0.0000], Avg: [-616.724 -616.724 -616.724] (0.344)
Step: 10699, Reward: [-621.792 -621.792 -621.792] [0.0000], Avg: [-616.748 -616.748 -616.748] (0.342)
Step: 10749, Reward: [-541.281 -541.281 -541.281] [0.0000], Avg: [-616.397 -616.397 -616.397] (0.340)
Step: 10799, Reward: [-688.683 -688.683 -688.683] [0.0000], Avg: [-616.731 -616.731 -616.731] (0.339)
Step: 10849, Reward: [-494.865 -494.865 -494.865] [0.0000], Avg: [-616.17 -616.17 -616.17] (0.337)
Step: 10899, Reward: [-430.434 -430.434 -430.434] [0.0000], Avg: [-615.318 -615.318 -615.318] (0.335)
Step: 10949, Reward: [-335.03 -335.03 -335.03] [0.0000], Avg: [-614.038 -614.038 -614.038] (0.334)
Step: 10999, Reward: [-336.727 -336.727 -336.727] [0.0000], Avg: [-612.777 -612.777 -612.777] (0.332)
Step: 11049, Reward: [-518.893 -518.893 -518.893] [0.0000], Avg: [-612.353 -612.353 -612.353] (0.330)
Step: 11099, Reward: [-779.492 -779.492 -779.492] [0.0000], Avg: [-613.105 -613.105 -613.105] (0.329)
Step: 11149, Reward: [-283.249 -283.249 -283.249] [0.0000], Avg: [-611.626 -611.626 -611.626] (0.327)
Step: 11199, Reward: [-708.396 -708.396 -708.396] [0.0000], Avg: [-612.058 -612.058 -612.058] (0.325)
Step: 11249, Reward: [-874.349 -874.349 -874.349] [0.0000], Avg: [-613.224 -613.224 -613.224] (0.324)
Step: 11299, Reward: [-500.853 -500.853 -500.853] [0.0000], Avg: [-612.727 -612.727 -612.727] (0.322)
Step: 11349, Reward: [-324.105 -324.105 -324.105] [0.0000], Avg: [-611.455 -611.455 -611.455] (0.321)
Step: 11399, Reward: [-621.44 -621.44 -621.44] [0.0000], Avg: [-611.499 -611.499 -611.499] (0.319)
Step: 11449, Reward: [-531.496 -531.496 -531.496] [0.0000], Avg: [-611.15 -611.15 -611.15] (0.317)
Step: 11499, Reward: [-492.367 -492.367 -492.367] [0.0000], Avg: [-610.633 -610.633 -610.633] (0.316)
Step: 11549, Reward: [-463.635 -463.635 -463.635] [0.0000], Avg: [-609.997 -609.997 -609.997] (0.314)
Step: 11599, Reward: [-503.218 -503.218 -503.218] [0.0000], Avg: [-609.537 -609.537 -609.537] (0.313)
Step: 11649, Reward: [-611.906 -611.906 -611.906] [0.0000], Avg: [-609.547 -609.547 -609.547] (0.311)
Step: 11699, Reward: [-412.249 -412.249 -412.249] [0.0000], Avg: [-608.704 -608.704 -608.704] (0.309)
Step: 11749, Reward: [-455.636 -455.636 -455.636] [0.0000], Avg: [-608.052 -608.052 -608.052] (0.308)
Step: 11799, Reward: [-885.016 -885.016 -885.016] [0.0000], Avg: [-609.226 -609.226 -609.226] (0.306)
Step: 11849, Reward: [-725.418 -725.418 -725.418] [0.0000], Avg: [-609.716 -609.716 -609.716] (0.305)
Step: 11899, Reward: [-348.156 -348.156 -348.156] [0.0000], Avg: [-608.617 -608.617 -608.617] (0.303)
Step: 11949, Reward: [-338.243 -338.243 -338.243] [0.0000], Avg: [-607.486 -607.486 -607.486] (0.302)
Step: 11999, Reward: [-463.343 -463.343 -463.343] [0.0000], Avg: [-606.885 -606.885 -606.885] (0.300)
Step: 12049, Reward: [-444.265 -444.265 -444.265] [0.0000], Avg: [-606.211 -606.211 -606.211] (0.299)
Step: 12099, Reward: [-343.39 -343.39 -343.39] [0.0000], Avg: [-605.125 -605.125 -605.125] (0.297)
Step: 12149, Reward: [-567.027 -567.027 -567.027] [0.0000], Avg: [-604.968 -604.968 -604.968] (0.296)
Step: 12199, Reward: [-709.503 -709.503 -709.503] [0.0000], Avg: [-605.396 -605.396 -605.396] (0.294)
Step: 12249, Reward: [-441.674 -441.674 -441.674] [0.0000], Avg: [-604.728 -604.728 -604.728] (0.293)
Step: 12299, Reward: [-417.8 -417.8 -417.8] [0.0000], Avg: [-603.968 -603.968 -603.968] (0.291)
Step: 12349, Reward: [-478.718 -478.718 -478.718] [0.0000], Avg: [-603.461 -603.461 -603.461] (0.290)
Step: 12399, Reward: [-706.241 -706.241 -706.241] [0.0000], Avg: [-603.875 -603.875 -603.875] (0.288)
Step: 12449, Reward: [-522.233 -522.233 -522.233] [0.0000], Avg: [-603.548 -603.548 -603.548] (0.287)
Step: 12499, Reward: [-678.41 -678.41 -678.41] [0.0000], Avg: [-603.847 -603.847 -603.847] (0.286)
Step: 12549, Reward: [-646.945 -646.945 -646.945] [0.0000], Avg: [-604.019 -604.019 -604.019] (0.284)
Step: 12599, Reward: [-513.085 -513.085 -513.085] [0.0000], Avg: [-603.658 -603.658 -603.658] (0.283)
Step: 12649, Reward: [-457.374 -457.374 -457.374] [0.0000], Avg: [-603.08 -603.08 -603.08] (0.281)
Step: 12699, Reward: [-566.265 -566.265 -566.265] [0.0000], Avg: [-602.935 -602.935 -602.935] (0.280)
Step: 12749, Reward: [-785.203 -785.203 -785.203] [0.0000], Avg: [-603.649 -603.649 -603.649] (0.279)
Step: 12799, Reward: [-616.798 -616.798 -616.798] [0.0000], Avg: [-603.701 -603.701 -603.701] (0.277)
Step: 12849, Reward: [-600.484 -600.484 -600.484] [0.0000], Avg: [-603.688 -603.688 -603.688] (0.276)
Step: 12899, Reward: [-622.436 -622.436 -622.436] [0.0000], Avg: [-603.761 -603.761 -603.761] (0.274)
Step: 12949, Reward: [-521.486 -521.486 -521.486] [0.0000], Avg: [-603.443 -603.443 -603.443] (0.273)
Step: 12999, Reward: [-593.28 -593.28 -593.28] [0.0000], Avg: [-603.404 -603.404 -603.404] (0.272)
Step: 13049, Reward: [-491.012 -491.012 -491.012] [0.0000], Avg: [-602.974 -602.974 -602.974] (0.270)
Step: 13099, Reward: [-604.028 -604.028 -604.028] [0.0000], Avg: [-602.978 -602.978 -602.978] (0.269)
Step: 13149, Reward: [-768.83 -768.83 -768.83] [0.0000], Avg: [-603.608 -603.608 -603.608] (0.268)
Step: 13199, Reward: [-647.863 -647.863 -647.863] [0.0000], Avg: [-603.776 -603.776 -603.776] (0.266)
Step: 13249, Reward: [-522.687 -522.687 -522.687] [0.0000], Avg: [-603.47 -603.47 -603.47] (0.265)
Step: 13299, Reward: [-380.229 -380.229 -380.229] [0.0000], Avg: [-602.631 -602.631 -602.631] (0.264)
Step: 13349, Reward: [-297.765 -297.765 -297.765] [0.0000], Avg: [-601.489 -601.489 -601.489] (0.262)
Step: 13399, Reward: [-570.167 -570.167 -570.167] [0.0000], Avg: [-601.372 -601.372 -601.372] (0.261)
Step: 13449, Reward: [-351.244 -351.244 -351.244] [0.0000], Avg: [-600.442 -600.442 -600.442] (0.260)
Step: 13499, Reward: [-503.193 -503.193 -503.193] [0.0000], Avg: [-600.082 -600.082 -600.082] (0.258)
Step: 13549, Reward: [-400.957 -400.957 -400.957] [0.0000], Avg: [-599.347 -599.347 -599.347] (0.257)
Step: 13599, Reward: [-431.213 -431.213 -431.213] [0.0000], Avg: [-598.729 -598.729 -598.729] (0.256)
Step: 13649, Reward: [-358.716 -358.716 -358.716] [0.0000], Avg: [-597.85 -597.85 -597.85] (0.255)
Step: 13699, Reward: [-363.777 -363.777 -363.777] [0.0000], Avg: [-596.996 -596.996 -596.996] (0.253)
Step: 13749, Reward: [-475.562 -475.562 -475.562] [0.0000], Avg: [-596.554 -596.554 -596.554] (0.252)
Step: 13799, Reward: [-620.985 -620.985 -620.985] [0.0000], Avg: [-596.642 -596.642 -596.642] (0.251)
Step: 13849, Reward: [-340.969 -340.969 -340.969] [0.0000], Avg: [-595.719 -595.719 -595.719] (0.249)
Step: 13899, Reward: [-438.487 -438.487 -438.487] [0.0000], Avg: [-595.154 -595.154 -595.154] (0.248)
Step: 13949, Reward: [-404.156 -404.156 -404.156] [0.0000], Avg: [-594.469 -594.469 -594.469] (0.247)
Step: 13999, Reward: [-331.296 -331.296 -331.296] [0.0000], Avg: [-593.529 -593.529 -593.529] (0.246)
Step: 14049, Reward: [-392.165 -392.165 -392.165] [0.0000], Avg: [-592.813 -592.813 -592.813] (0.245)
Step: 14099, Reward: [-398.224 -398.224 -398.224] [0.0000], Avg: [-592.123 -592.123 -592.123] (0.243)
Step: 14149, Reward: [-555.709 -555.709 -555.709] [0.0000], Avg: [-591.994 -591.994 -591.994] (0.242)
Step: 14199, Reward: [-711.815 -711.815 -711.815] [0.0000], Avg: [-592.416 -592.416 -592.416] (0.241)
Step: 14249, Reward: [-428.851 -428.851 -428.851] [0.0000], Avg: [-591.842 -591.842 -591.842] (0.240)
Step: 14299, Reward: [-477.754 -477.754 -477.754] [0.0000], Avg: [-591.443 -591.443 -591.443] (0.238)
Step: 14349, Reward: [-549.389 -549.389 -549.389] [0.0000], Avg: [-591.297 -591.297 -591.297] (0.237)
Step: 14399, Reward: [-445.814 -445.814 -445.814] [0.0000], Avg: [-590.792 -590.792 -590.792] (0.236)
Step: 14449, Reward: [-543.035 -543.035 -543.035] [0.0000], Avg: [-590.626 -590.626 -590.626] (0.235)
Step: 14499, Reward: [-792.936 -792.936 -792.936] [0.0000], Avg: [-591.324 -591.324 -591.324] (0.234)
Step: 14549, Reward: [-408.426 -408.426 -408.426] [0.0000], Avg: [-590.695 -590.695 -590.695] (0.233)
Step: 14599, Reward: [-517.423 -517.423 -517.423] [0.0000], Avg: [-590.444 -590.444 -590.444] (0.231)
Step: 14649, Reward: [-588.745 -588.745 -588.745] [0.0000], Avg: [-590.439 -590.439 -590.439] (0.230)
Step: 14699, Reward: [-742.683 -742.683 -742.683] [0.0000], Avg: [-590.956 -590.956 -590.956] (0.229)
Step: 14749, Reward: [-556.197 -556.197 -556.197] [0.0000], Avg: [-590.839 -590.839 -590.839] (0.228)
Step: 14799, Reward: [-641.678 -641.678 -641.678] [0.0000], Avg: [-591.01 -591.01 -591.01] (0.227)
Step: 14849, Reward: [-584.492 -584.492 -584.492] [0.0000], Avg: [-590.988 -590.988 -590.988] (0.226)
Step: 14899, Reward: [-494.679 -494.679 -494.679] [0.0000], Avg: [-590.665 -590.665 -590.665] (0.225)
Step: 14949, Reward: [-484.719 -484.719 -484.719] [0.0000], Avg: [-590.311 -590.311 -590.311] (0.223)
Step: 14999, Reward: [-507.367 -507.367 -507.367] [0.0000], Avg: [-590.034 -590.034 -590.034] (0.222)
Step: 15049, Reward: [-503.697 -503.697 -503.697] [0.0000], Avg: [-589.748 -589.748 -589.748] (0.221)
Step: 15099, Reward: [-649.781 -649.781 -649.781] [0.0000], Avg: [-589.946 -589.946 -589.946] (0.220)
Step: 15149, Reward: [-500.841 -500.841 -500.841] [0.0000], Avg: [-589.652 -589.652 -589.652] (0.219)
Step: 15199, Reward: [-542.004 -542.004 -542.004] [0.0000], Avg: [-589.496 -589.496 -589.496] (0.218)
Step: 15249, Reward: [-547.106 -547.106 -547.106] [0.0000], Avg: [-589.357 -589.357 -589.357] (0.217)
Step: 15299, Reward: [-530.056 -530.056 -530.056] [0.0000], Avg: [-589.163 -589.163 -589.163] (0.216)
Step: 15349, Reward: [-777.141 -777.141 -777.141] [0.0000], Avg: [-589.775 -589.775 -589.775] (0.215)
Step: 15399, Reward: [-490.111 -490.111 -490.111] [0.0000], Avg: [-589.452 -589.452 -589.452] (0.214)
Step: 15449, Reward: [-783.71 -783.71 -783.71] [0.0000], Avg: [-590.08 -590.08 -590.08] (0.212)
Step: 15499, Reward: [-386.43 -386.43 -386.43] [0.0000], Avg: [-589.423 -589.423 -589.423] (0.211)
Step: 15549, Reward: [-602.61 -602.61 -602.61] [0.0000], Avg: [-589.466 -589.466 -589.466] (0.210)
Step: 15599, Reward: [-524.022 -524.022 -524.022] [0.0000], Avg: [-589.256 -589.256 -589.256] (0.209)
Step: 15649, Reward: [-500.462 -500.462 -500.462] [0.0000], Avg: [-588.972 -588.972 -588.972] (0.208)
Step: 15699, Reward: [-660.645 -660.645 -660.645] [0.0000], Avg: [-589.2 -589.2 -589.2] (0.207)
Step: 15749, Reward: [-536.731 -536.731 -536.731] [0.0000], Avg: [-589.034 -589.034 -589.034] (0.206)
Step: 15799, Reward: [-382.071 -382.071 -382.071] [0.0000], Avg: [-588.379 -588.379 -588.379] (0.205)
Step: 15849, Reward: [-504.625 -504.625 -504.625] [0.0000], Avg: [-588.115 -588.115 -588.115] (0.204)
Step: 15899, Reward: [-394.023 -394.023 -394.023] [0.0000], Avg: [-587.504 -587.504 -587.504] (0.203)
Step: 15949, Reward: [-662.052 -662.052 -662.052] [0.0000], Avg: [-587.738 -587.738 -587.738] (0.202)
Step: 15999, Reward: [-459.686 -459.686 -459.686] [0.0000], Avg: [-587.338 -587.338 -587.338] (0.201)
Step: 16049, Reward: [-632.948 -632.948 -632.948] [0.0000], Avg: [-587.48 -587.48 -587.48] (0.200)
Step: 16099, Reward: [-605.546 -605.546 -605.546] [0.0000], Avg: [-587.536 -587.536 -587.536] (0.199)
Step: 16149, Reward: [-557.946 -557.946 -557.946] [0.0000], Avg: [-587.445 -587.445 -587.445] (0.198)
Step: 16199, Reward: [-878.435 -878.435 -878.435] [0.0000], Avg: [-588.343 -588.343 -588.343] (0.197)
Step: 16249, Reward: [-716.719 -716.719 -716.719] [0.0000], Avg: [-588.738 -588.738 -588.738] (0.196)
Step: 16299, Reward: [-387.161 -387.161 -387.161] [0.0000], Avg: [-588.119 -588.119 -588.119] (0.195)
Step: 16349, Reward: [-750.55 -750.55 -750.55] [0.0000], Avg: [-588.616 -588.616 -588.616] (0.194)
Step: 16399, Reward: [-549.737 -549.737 -549.737] [0.0000], Avg: [-588.498 -588.498 -588.498] (0.193)
Step: 16449, Reward: [-620.053 -620.053 -620.053] [0.0000], Avg: [-588.593 -588.593 -588.593] (0.192)
Step: 16499, Reward: [-753.336 -753.336 -753.336] [0.0000], Avg: [-589.093 -589.093 -589.093] (0.191)
Step: 16549, Reward: [-712.442 -712.442 -712.442] [0.0000], Avg: [-589.465 -589.465 -589.465] (0.190)
Step: 16599, Reward: [-683.338 -683.338 -683.338] [0.0000], Avg: [-589.748 -589.748 -589.748] (0.189)
Step: 16649, Reward: [-655.18 -655.18 -655.18] [0.0000], Avg: [-589.945 -589.945 -589.945] (0.188)
Step: 16699, Reward: [-680.513 -680.513 -680.513] [0.0000], Avg: [-590.216 -590.216 -590.216] (0.187)
Step: 16749, Reward: [-684.714 -684.714 -684.714] [0.0000], Avg: [-590.498 -590.498 -590.498] (0.187)
Step: 16799, Reward: [-521.013 -521.013 -521.013] [0.0000], Avg: [-590.291 -590.291 -590.291] (0.186)
Step: 16849, Reward: [-445.265 -445.265 -445.265] [0.0000], Avg: [-589.861 -589.861 -589.861] (0.185)
Step: 16899, Reward: [-586.187 -586.187 -586.187] [0.0000], Avg: [-589.85 -589.85 -589.85] (0.184)
Step: 16949, Reward: [-602.149 -602.149 -602.149] [0.0000], Avg: [-589.886 -589.886 -589.886] (0.183)
Step: 16999, Reward: [-583.744 -583.744 -583.744] [0.0000], Avg: [-589.868 -589.868 -589.868] (0.182)
Step: 17049, Reward: [-454.845 -454.845 -454.845] [0.0000], Avg: [-589.472 -589.472 -589.472] (0.181)
Step: 17099, Reward: [-463.78 -463.78 -463.78] [0.0000], Avg: [-589.104 -589.104 -589.104] (0.180)
Step: 17149, Reward: [-543.1 -543.1 -543.1] [0.0000], Avg: [-588.97 -588.97 -588.97] (0.179)
Step: 17199, Reward: [-503.622 -503.622 -503.622] [0.0000], Avg: [-588.722 -588.722 -588.722] (0.178)
Step: 17249, Reward: [-478.289 -478.289 -478.289] [0.0000], Avg: [-588.402 -588.402 -588.402] (0.177)
Step: 17299, Reward: [-484.753 -484.753 -484.753] [0.0000], Avg: [-588.103 -588.103 -588.103] (0.177)
Step: 17349, Reward: [-564.631 -564.631 -564.631] [0.0000], Avg: [-588.035 -588.035 -588.035] (0.176)
Step: 17399, Reward: [-512.007 -512.007 -512.007] [0.0000], Avg: [-587.816 -587.816 -587.816] (0.175)
Step: 17449, Reward: [-439.892 -439.892 -439.892] [0.0000], Avg: [-587.393 -587.393 -587.393] (0.174)
Step: 17499, Reward: [-618.856 -618.856 -618.856] [0.0000], Avg: [-587.483 -587.483 -587.483] (0.173)
Step: 17549, Reward: [-502.348 -502.348 -502.348] [0.0000], Avg: [-587.24 -587.24 -587.24] (0.172)
Step: 17599, Reward: [-509.171 -509.171 -509.171] [0.0000], Avg: [-587.018 -587.018 -587.018] (0.171)
Step: 17649, Reward: [-553.249 -553.249 -553.249] [0.0000], Avg: [-586.923 -586.923 -586.923] (0.170)
Step: 17699, Reward: [-793.955 -793.955 -793.955] [0.0000], Avg: [-587.507 -587.507 -587.507] (0.170)
Step: 17749, Reward: [-561.718 -561.718 -561.718] [0.0000], Avg: [-587.435 -587.435 -587.435] (0.169)
Step: 17799, Reward: [-610.025 -610.025 -610.025] [0.0000], Avg: [-587.498 -587.498 -587.498] (0.168)
Step: 17849, Reward: [-502.603 -502.603 -502.603] [0.0000], Avg: [-587.26 -587.26 -587.26] (0.167)
Step: 17899, Reward: [-447.407 -447.407 -447.407] [0.0000], Avg: [-586.87 -586.87 -586.87] (0.166)
Step: 17949, Reward: [-498.233 -498.233 -498.233] [0.0000], Avg: [-586.623 -586.623 -586.623] (0.165)
Step: 17999, Reward: [-560.746 -560.746 -560.746] [0.0000], Avg: [-586.551 -586.551 -586.551] (0.165)
Step: 18049, Reward: [-748.105 -748.105 -748.105] [0.0000], Avg: [-586.998 -586.998 -586.998] (0.164)
Step: 18099, Reward: [-509.156 -509.156 -509.156] [0.0000], Avg: [-586.783 -586.783 -586.783] (0.163)
Step: 18149, Reward: [-528.178 -528.178 -528.178] [0.0000], Avg: [-586.622 -586.622 -586.622] (0.162)
Step: 18199, Reward: [-538.276 -538.276 -538.276] [0.0000], Avg: [-586.489 -586.489 -586.489] (0.161)
Step: 18249, Reward: [-629.321 -629.321 -629.321] [0.0000], Avg: [-586.607 -586.607 -586.607] (0.160)
Step: 18299, Reward: [-457.217 -457.217 -457.217] [0.0000], Avg: [-586.253 -586.253 -586.253] (0.160)
Step: 18349, Reward: [-588.793 -588.793 -588.793] [0.0000], Avg: [-586.26 -586.26 -586.26] (0.159)
Step: 18399, Reward: [-555.401 -555.401 -555.401] [0.0000], Avg: [-586.176 -586.176 -586.176] (0.158)
Step: 18449, Reward: [-412.594 -412.594 -412.594] [0.0000], Avg: [-585.706 -585.706 -585.706] (0.157)
Step: 18499, Reward: [-555.31 -555.31 -555.31] [0.0000], Avg: [-585.623 -585.623 -585.623] (0.157)
Step: 18549, Reward: [-619.7 -619.7 -619.7] [0.0000], Avg: [-585.715 -585.715 -585.715] (0.156)
Step: 18599, Reward: [-493.85 -493.85 -493.85] [0.0000], Avg: [-585.468 -585.468 -585.468] (0.155)
Step: 18649, Reward: [-589.392 -589.392 -589.392] [0.0000], Avg: [-585.479 -585.479 -585.479] (0.154)
Step: 18699, Reward: [-855.105 -855.105 -855.105] [0.0000], Avg: [-586.2 -586.2 -586.2] (0.153)
Step: 18749, Reward: [-560.06 -560.06 -560.06] [0.0000], Avg: [-586.13 -586.13 -586.13] (0.153)
Step: 18799, Reward: [-578.645 -578.645 -578.645] [0.0000], Avg: [-586.11 -586.11 -586.11] (0.152)
Step: 18849, Reward: [-701.23 -701.23 -701.23] [0.0000], Avg: [-586.416 -586.416 -586.416] (0.151)
Step: 18899, Reward: [-841.114 -841.114 -841.114] [0.0000], Avg: [-587.089 -587.089 -587.089] (0.150)
Step: 18949, Reward: [-642.409 -642.409 -642.409] [0.0000], Avg: [-587.235 -587.235 -587.235] (0.150)
Step: 18999, Reward: [-613.453 -613.453 -613.453] [0.0000], Avg: [-587.304 -587.304 -587.304] (0.149)
Step: 19049, Reward: [-624.205 -624.205 -624.205] [0.0000], Avg: [-587.401 -587.401 -587.401] (0.148)
Step: 19099, Reward: [-555.301 -555.301 -555.301] [0.0000], Avg: [-587.317 -587.317 -587.317] (0.147)
Step: 19149, Reward: [-382.011 -382.011 -382.011] [0.0000], Avg: [-586.781 -586.781 -586.781] (0.147)
Step: 19199, Reward: [-461.859 -461.859 -461.859] [0.0000], Avg: [-586.456 -586.456 -586.456] (0.146)
Step: 19249, Reward: [-509.278 -509.278 -509.278] [0.0000], Avg: [-586.255 -586.255 -586.255] (0.145)
Step: 19299, Reward: [-531.35 -531.35 -531.35] [0.0000], Avg: [-586.113 -586.113 -586.113] (0.144)
Step: 19349, Reward: [-503.453 -503.453 -503.453] [0.0000], Avg: [-585.9 -585.9 -585.9] (0.144)
Step: 19399, Reward: [-504.71 -504.71 -504.71] [0.0000], Avg: [-585.69 -585.69 -585.69] (0.143)
Step: 19449, Reward: [-502.608 -502.608 -502.608] [0.0000], Avg: [-585.477 -585.477 -585.477] (0.142)
Step: 19499, Reward: [-544.786 -544.786 -544.786] [0.0000], Avg: [-585.372 -585.372 -585.372] (0.142)
Step: 19549, Reward: [-617.229 -617.229 -617.229] [0.0000], Avg: [-585.454 -585.454 -585.454] (0.141)
Step: 19599, Reward: [-636.94 -636.94 -636.94] [0.0000], Avg: [-585.585 -585.585 -585.585] (0.140)
Step: 19649, Reward: [-602.533 -602.533 -602.533] [0.0000], Avg: [-585.628 -585.628 -585.628] (0.139)
Step: 19699, Reward: [-667.301 -667.301 -667.301] [0.0000], Avg: [-585.836 -585.836 -585.836] (0.139)
Step: 19749, Reward: [-515.479 -515.479 -515.479] [0.0000], Avg: [-585.657 -585.657 -585.657] (0.138)
Step: 19799, Reward: [-409.649 -409.649 -409.649] [0.0000], Avg: [-585.213 -585.213 -585.213] (0.137)
Step: 19849, Reward: [-425.596 -425.596 -425.596] [0.0000], Avg: [-584.811 -584.811 -584.811] (0.137)
Step: 19899, Reward: [-511.274 -511.274 -511.274] [0.0000], Avg: [-584.626 -584.626 -584.626] (0.136)
Step: 19949, Reward: [-396.218 -396.218 -396.218] [0.0000], Avg: [-584.154 -584.154 -584.154] (0.135)
Step: 19999, Reward: [-467.058 -467.058 -467.058] [0.0000], Avg: [-583.861 -583.861 -583.861] (0.135)
Step: 20049, Reward: [-401.041 -401.041 -401.041] [0.0000], Avg: [-583.405 -583.405 -583.405] (0.134)
Step: 20099, Reward: [-661.024 -661.024 -661.024] [0.0000], Avg: [-583.598 -583.598 -583.598] (0.133)
Step: 20149, Reward: [-408.456 -408.456 -408.456] [0.0000], Avg: [-583.164 -583.164 -583.164] (0.133)
Step: 20199, Reward: [-594.758 -594.758 -594.758] [0.0000], Avg: [-583.193 -583.193 -583.193] (0.132)
Step: 20249, Reward: [-350.397 -350.397 -350.397] [0.0000], Avg: [-582.618 -582.618 -582.618] (0.131)
Step: 20299, Reward: [-520.926 -520.926 -520.926] [0.0000], Avg: [-582.466 -582.466 -582.466] (0.131)
Step: 20349, Reward: [-500.632 -500.632 -500.632] [0.0000], Avg: [-582.265 -582.265 -582.265] (0.130)
Step: 20399, Reward: [-643.844 -643.844 -643.844] [0.0000], Avg: [-582.416 -582.416 -582.416] (0.129)
Step: 20449, Reward: [-354.743 -354.743 -354.743] [0.0000], Avg: [-581.859 -581.859 -581.859] (0.129)
Step: 20499, Reward: [-535.647 -535.647 -535.647] [0.0000], Avg: [-581.746 -581.746 -581.746] (0.128)
Step: 20549, Reward: [-423.986 -423.986 -423.986] [0.0000], Avg: [-581.362 -581.362 -581.362] (0.127)
Step: 20599, Reward: [-516.476 -516.476 -516.476] [0.0000], Avg: [-581.205 -581.205 -581.205] (0.127)
Step: 20649, Reward: [-639.347 -639.347 -639.347] [0.0000], Avg: [-581.346 -581.346 -581.346] (0.126)
Step: 20699, Reward: [-460.512 -460.512 -460.512] [0.0000], Avg: [-581.054 -581.054 -581.054] (0.126)
Step: 20749, Reward: [-539.209 -539.209 -539.209] [0.0000], Avg: [-580.953 -580.953 -580.953] (0.125)
Step: 20799, Reward: [-385.205 -385.205 -385.205] [0.0000], Avg: [-580.482 -580.482 -580.482] (0.124)
Step: 20849, Reward: [-524.629 -524.629 -524.629] [0.0000], Avg: [-580.349 -580.349 -580.349] (0.124)
Step: 20899, Reward: [-367.833 -367.833 -367.833] [0.0000], Avg: [-579.84 -579.84 -579.84] (0.123)
Step: 20949, Reward: [-456.18 -456.18 -456.18] [0.0000], Avg: [-579.545 -579.545 -579.545] (0.122)
Step: 20999, Reward: [-452.948 -452.948 -452.948] [0.0000], Avg: [-579.244 -579.244 -579.244] (0.122)
Step: 21049, Reward: [-542.373 -542.373 -542.373] [0.0000], Avg: [-579.156 -579.156 -579.156] (0.121)
Step: 21099, Reward: [-343.951 -343.951 -343.951] [0.0000], Avg: [-578.599 -578.599 -578.599] (0.121)
Step: 21149, Reward: [-440.308 -440.308 -440.308] [0.0000], Avg: [-578.272 -578.272 -578.272] (0.120)
Step: 21199, Reward: [-517.584 -517.584 -517.584] [0.0000], Avg: [-578.129 -578.129 -578.129] (0.119)
Step: 21249, Reward: [-789.458 -789.458 -789.458] [0.0000], Avg: [-578.626 -578.626 -578.626] (0.119)
Step: 21299, Reward: [-478.11 -478.11 -478.11] [0.0000], Avg: [-578.39 -578.39 -578.39] (0.118)
Step: 21349, Reward: [-592.265 -592.265 -592.265] [0.0000], Avg: [-578.422 -578.422 -578.422] (0.118)
Step: 21399, Reward: [-457.981 -457.981 -457.981] [0.0000], Avg: [-578.141 -578.141 -578.141] (0.117)
Step: 21449, Reward: [-641.255 -641.255 -641.255] [0.0000], Avg: [-578.288 -578.288 -578.288] (0.116)
Step: 21499, Reward: [-476.258 -476.258 -476.258] [0.0000], Avg: [-578.051 -578.051 -578.051] (0.116)
Step: 21549, Reward: [-487.15 -487.15 -487.15] [0.0000], Avg: [-577.84 -577.84 -577.84] (0.115)
Step: 21599, Reward: [-478.629 -478.629 -478.629] [0.0000], Avg: [-577.61 -577.61 -577.61] (0.115)
Step: 21649, Reward: [-518.634 -518.634 -518.634] [0.0000], Avg: [-577.474 -577.474 -577.474] (0.114)
Step: 21699, Reward: [-466.72 -466.72 -466.72] [0.0000], Avg: [-577.219 -577.219 -577.219] (0.114)
Step: 21749, Reward: [-604.555 -604.555 -604.555] [0.0000], Avg: [-577.282 -577.282 -577.282] (0.113)
Step: 21799, Reward: [-484.186 -484.186 -484.186] [0.0000], Avg: [-577.068 -577.068 -577.068] (0.112)
Step: 21849, Reward: [-329.037 -329.037 -329.037] [0.0000], Avg: [-576.501 -576.501 -576.501] (0.112)
Step: 21899, Reward: [-548.991 -548.991 -548.991] [0.0000], Avg: [-576.438 -576.438 -576.438] (0.111)
Step: 21949, Reward: [-689.267 -689.267 -689.267] [0.0000], Avg: [-576.695 -576.695 -576.695] (0.111)
Step: 21999, Reward: [-571.758 -571.758 -571.758] [0.0000], Avg: [-576.684 -576.684 -576.684] (0.110)
Step: 22049, Reward: [-440.073 -440.073 -440.073] [0.0000], Avg: [-576.374 -576.374 -576.374] (0.110)
Step: 22099, Reward: [-538.373 -538.373 -538.373] [0.0000], Avg: [-576.288 -576.288 -576.288] (0.109)
Step: 22149, Reward: [-455.147 -455.147 -455.147] [0.0000], Avg: [-576.014 -576.014 -576.014] (0.109)
Step: 22199, Reward: [-381.183 -381.183 -381.183] [0.0000], Avg: [-575.576 -575.576 -575.576] (0.108)
Step: 22249, Reward: [-434.476 -434.476 -434.476] [0.0000], Avg: [-575.258 -575.258 -575.258] (0.107)
Step: 22299, Reward: [-420.668 -420.668 -420.668] [0.0000], Avg: [-574.912 -574.912 -574.912] (0.107)
Step: 22349, Reward: [-281.758 -281.758 -281.758] [0.0000], Avg: [-574.256 -574.256 -574.256] (0.106)
Step: 22399, Reward: [-380.551 -380.551 -380.551] [0.0000], Avg: [-573.824 -573.824 -573.824] (0.106)
Step: 22449, Reward: [-366.968 -366.968 -366.968] [0.0000], Avg: [-573.363 -573.363 -573.363] (0.105)
Step: 22499, Reward: [-411.73 -411.73 -411.73] [0.0000], Avg: [-573.004 -573.004 -573.004] (0.105)
Step: 22549, Reward: [-425.05 -425.05 -425.05] [0.0000], Avg: [-572.676 -572.676 -572.676] (0.104)
Step: 22599, Reward: [-378.366 -378.366 -378.366] [0.0000], Avg: [-572.246 -572.246 -572.246] (0.104)
Step: 22649, Reward: [-598.476 -598.476 -598.476] [0.0000], Avg: [-572.304 -572.304 -572.304] (0.103)
Step: 22699, Reward: [-596.505 -596.505 -596.505] [0.0000], Avg: [-572.357 -572.357 -572.357] (0.103)
Step: 22749, Reward: [-665.796 -665.796 -665.796] [0.0000], Avg: [-572.562 -572.562 -572.562] (0.102)
Step: 22799, Reward: [-364.938 -364.938 -364.938] [0.0000], Avg: [-572.107 -572.107 -572.107] (0.102)
Step: 22849, Reward: [-323.801 -323.801 -323.801] [0.0000], Avg: [-571.564 -571.564 -571.564] (0.101)
Step: 22899, Reward: [-572.63 -572.63 -572.63] [0.0000], Avg: [-571.566 -571.566 -571.566] (0.101)
Step: 22949, Reward: [-431.124 -431.124 -431.124] [0.0000], Avg: [-571.26 -571.26 -571.26] (0.100)
Step: 22999, Reward: [-526.217 -526.217 -526.217] [0.0000], Avg: [-571.162 -571.162 -571.162] (0.100)
Step: 23049, Reward: [-546.755 -546.755 -546.755] [0.0000], Avg: [-571.109 -571.109 -571.109] (0.100)
Step: 23099, Reward: [-568.74 -568.74 -568.74] [0.0000], Avg: [-571.104 -571.104 -571.104] (0.100)
Step: 23149, Reward: [-463.507 -463.507 -463.507] [0.0000], Avg: [-570.872 -570.872 -570.872] (0.100)
Step: 23199, Reward: [-421.84 -421.84 -421.84] [0.0000], Avg: [-570.55 -570.55 -570.55] (0.100)
Step: 23249, Reward: [-411.458 -411.458 -411.458] [0.0000], Avg: [-570.208 -570.208 -570.208] (0.100)
Step: 23299, Reward: [-386.683 -386.683 -386.683] [0.0000], Avg: [-569.815 -569.815 -569.815] (0.100)
Step: 23349, Reward: [-318.203 -318.203 -318.203] [0.0000], Avg: [-569.276 -569.276 -569.276] (0.100)
Step: 23399, Reward: [-431.668 -431.668 -431.668] [0.0000], Avg: [-568.982 -568.982 -568.982] (0.100)
Step: 23449, Reward: [-473.114 -473.114 -473.114] [0.0000], Avg: [-568.777 -568.777 -568.777] (0.100)
Step: 23499, Reward: [-617.709 -617.709 -617.709] [0.0000], Avg: [-568.881 -568.881 -568.881] (0.100)
Step: 23549, Reward: [-465.679 -465.679 -465.679] [0.0000], Avg: [-568.662 -568.662 -568.662] (0.100)
Step: 23599, Reward: [-599.211 -599.211 -599.211] [0.0000], Avg: [-568.727 -568.727 -568.727] (0.100)
Step: 23649, Reward: [-771.171 -771.171 -771.171] [0.0000], Avg: [-569.155 -569.155 -569.155] (0.100)
Step: 23699, Reward: [-1047.276 -1047.276 -1047.276] [0.0000], Avg: [-570.164 -570.164 -570.164] (0.100)
Step: 23749, Reward: [-462.171 -462.171 -462.171] [0.0000], Avg: [-569.936 -569.936 -569.936] (0.100)
Step: 23799, Reward: [-491.932 -491.932 -491.932] [0.0000], Avg: [-569.772 -569.772 -569.772] (0.100)
Step: 23849, Reward: [-550.34 -550.34 -550.34] [0.0000], Avg: [-569.732 -569.732 -569.732] (0.100)
Step: 23899, Reward: [-536.683 -536.683 -536.683] [0.0000], Avg: [-569.663 -569.663 -569.663] (0.100)
Step: 23949, Reward: [-628.642 -628.642 -628.642] [0.0000], Avg: [-569.786 -569.786 -569.786] (0.100)
Step: 23999, Reward: [-561.841 -561.841 -561.841] [0.0000], Avg: [-569.769 -569.769 -569.769] (0.100)
Step: 24049, Reward: [-444.109 -444.109 -444.109] [0.0000], Avg: [-569.508 -569.508 -569.508] (0.100)
Step: 24099, Reward: [-452.866 -452.866 -452.866] [0.0000], Avg: [-569.266 -569.266 -569.266] (0.100)
Step: 24149, Reward: [-425.742 -425.742 -425.742] [0.0000], Avg: [-568.969 -568.969 -568.969] (0.100)
Step: 24199, Reward: [-570.068 -570.068 -570.068] [0.0000], Avg: [-568.971 -568.971 -568.971] (0.100)
Step: 24249, Reward: [-475.68 -475.68 -475.68] [0.0000], Avg: [-568.779 -568.779 -568.779] (0.100)
Step: 24299, Reward: [-560.713 -560.713 -560.713] [0.0000], Avg: [-568.762 -568.762 -568.762] (0.100)
Step: 24349, Reward: [-717.375 -717.375 -717.375] [0.0000], Avg: [-569.067 -569.067 -569.067] (0.100)
Step: 24399, Reward: [-448.509 -448.509 -448.509] [0.0000], Avg: [-568.82 -568.82 -568.82] (0.100)
Step: 24449, Reward: [-505.291 -505.291 -505.291] [0.0000], Avg: [-568.69 -568.69 -568.69] (0.100)
Step: 24499, Reward: [-477.378 -477.378 -477.378] [0.0000], Avg: [-568.504 -568.504 -568.504] (0.100)
Step: 24549, Reward: [-325.993 -325.993 -325.993] [0.0000], Avg: [-568.01 -568.01 -568.01] (0.100)
Step: 24599, Reward: [-524.308 -524.308 -524.308] [0.0000], Avg: [-567.921 -567.921 -567.921] (0.100)
Step: 24649, Reward: [-853.387 -853.387 -853.387] [0.0000], Avg: [-568.5 -568.5 -568.5] (0.100)
Step: 24699, Reward: [-888.038 -888.038 -888.038] [0.0000], Avg: [-569.147 -569.147 -569.147] (0.100)
Step: 24749, Reward: [-414.31 -414.31 -414.31] [0.0000], Avg: [-568.834 -568.834 -568.834] (0.100)
Step: 24799, Reward: [-1039.138 -1039.138 -1039.138] [0.0000], Avg: [-569.782 -569.782 -569.782] (0.100)
Step: 24849, Reward: [-406.673 -406.673 -406.673] [0.0000], Avg: [-569.454 -569.454 -569.454] (0.100)
Step: 24899, Reward: [-580.897 -580.897 -580.897] [0.0000], Avg: [-569.477 -569.477 -569.477] (0.100)
Step: 24949, Reward: [-391.893 -391.893 -391.893] [0.0000], Avg: [-569.121 -569.121 -569.121] (0.100)
Step: 24999, Reward: [-816.869 -816.869 -816.869] [0.0000], Avg: [-569.617 -569.617 -569.617] (0.100)
Step: 25049, Reward: [-470.946 -470.946 -470.946] [0.0000], Avg: [-569.42 -569.42 -569.42] (0.100)
Step: 25099, Reward: [-569.128 -569.128 -569.128] [0.0000], Avg: [-569.419 -569.419 -569.419] (0.100)
Step: 25149, Reward: [-689.109 -689.109 -689.109] [0.0000], Avg: [-569.657 -569.657 -569.657] (0.100)
Step: 25199, Reward: [-429.056 -429.056 -429.056] [0.0000], Avg: [-569.378 -569.378 -569.378] (0.100)
Step: 25249, Reward: [-483.351 -483.351 -483.351] [0.0000], Avg: [-569.208 -569.208 -569.208] (0.100)
Step: 25299, Reward: [-848.736 -848.736 -848.736] [0.0000], Avg: [-569.76 -569.76 -569.76] (0.100)
Step: 25349, Reward: [-309.844 -309.844 -309.844] [0.0000], Avg: [-569.248 -569.248 -569.248] (0.100)
Step: 25399, Reward: [-1004.834 -1004.834 -1004.834] [0.0000], Avg: [-570.105 -570.105 -570.105] (0.100)
Step: 25449, Reward: [-341.612 -341.612 -341.612] [0.0000], Avg: [-569.656 -569.656 -569.656] (0.100)
Step: 25499, Reward: [-775.342 -775.342 -775.342] [0.0000], Avg: [-570.06 -570.06 -570.06] (0.100)
Step: 25549, Reward: [-792.765 -792.765 -792.765] [0.0000], Avg: [-570.495 -570.495 -570.495] (0.100)
Step: 25599, Reward: [-460.622 -460.622 -460.622] [0.0000], Avg: [-570.281 -570.281 -570.281] (0.100)
Step: 25649, Reward: [-443.407 -443.407 -443.407] [0.0000], Avg: [-570.034 -570.034 -570.034] (0.100)
Step: 25699, Reward: [-717.643 -717.643 -717.643] [0.0000], Avg: [-570.321 -570.321 -570.321] (0.100)
Step: 25749, Reward: [-575.361 -575.361 -575.361] [0.0000], Avg: [-570.33 -570.33 -570.33] (0.100)
Step: 25799, Reward: [-376.553 -376.553 -376.553] [0.0000], Avg: [-569.955 -569.955 -569.955] (0.100)
Step: 25849, Reward: [-228.274 -228.274 -228.274] [0.0000], Avg: [-569.294 -569.294 -569.294] (0.100)
Step: 25899, Reward: [-710.85 -710.85 -710.85] [0.0000], Avg: [-569.567 -569.567 -569.567] (0.100)
Step: 25949, Reward: [-546.709 -546.709 -546.709] [0.0000], Avg: [-569.523 -569.523 -569.523] (0.100)
Step: 25999, Reward: [-443.481 -443.481 -443.481] [0.0000], Avg: [-569.281 -569.281 -569.281] (0.100)
Step: 26049, Reward: [-599.067 -599.067 -599.067] [0.0000], Avg: [-569.338 -569.338 -569.338] (0.100)
Step: 26099, Reward: [-675.61 -675.61 -675.61] [0.0000], Avg: [-569.542 -569.542 -569.542] (0.100)
Step: 26149, Reward: [-424.985 -424.985 -424.985] [0.0000], Avg: [-569.265 -569.265 -569.265] (0.100)
Step: 26199, Reward: [-586.225 -586.225 -586.225] [0.0000], Avg: [-569.298 -569.298 -569.298] (0.100)
Step: 26249, Reward: [-492.345 -492.345 -492.345] [0.0000], Avg: [-569.151 -569.151 -569.151] (0.100)
Step: 26299, Reward: [-508.406 -508.406 -508.406] [0.0000], Avg: [-569.036 -569.036 -569.036] (0.100)
Step: 26349, Reward: [-581.764 -581.764 -581.764] [0.0000], Avg: [-569.06 -569.06 -569.06] (0.100)
Step: 26399, Reward: [-467.797 -467.797 -467.797] [0.0000], Avg: [-568.868 -568.868 -568.868] (0.100)
Step: 26449, Reward: [-460.761 -460.761 -460.761] [0.0000], Avg: [-568.664 -568.664 -568.664] (0.100)
Step: 26499, Reward: [-400.311 -400.311 -400.311] [0.0000], Avg: [-568.346 -568.346 -568.346] (0.100)
Step: 26549, Reward: [-436.877 -436.877 -436.877] [0.0000], Avg: [-568.098 -568.098 -568.098] (0.100)
Step: 26599, Reward: [-474.503 -474.503 -474.503] [0.0000], Avg: [-567.922 -567.922 -567.922] (0.100)
Step: 26649, Reward: [-465.946 -465.946 -465.946] [0.0000], Avg: [-567.731 -567.731 -567.731] (0.100)
Step: 26699, Reward: [-471.755 -471.755 -471.755] [0.0000], Avg: [-567.551 -567.551 -567.551] (0.100)
Step: 26749, Reward: [-424.64 -424.64 -424.64] [0.0000], Avg: [-567.284 -567.284 -567.284] (0.100)
Step: 26799, Reward: [-579.441 -579.441 -579.441] [0.0000], Avg: [-567.307 -567.307 -567.307] (0.100)
Step: 26849, Reward: [-560.925 -560.925 -560.925] [0.0000], Avg: [-567.295 -567.295 -567.295] (0.100)
Step: 26899, Reward: [-380.148 -380.148 -380.148] [0.0000], Avg: [-566.947 -566.947 -566.947] (0.100)
Step: 26949, Reward: [-388.198 -388.198 -388.198] [0.0000], Avg: [-566.616 -566.616 -566.616] (0.100)
Step: 26999, Reward: [-557.474 -557.474 -557.474] [0.0000], Avg: [-566.599 -566.599 -566.599] (0.100)
Step: 27049, Reward: [-653.927 -653.927 -653.927] [0.0000], Avg: [-566.76 -566.76 -566.76] (0.100)
Step: 27099, Reward: [-650.337 -650.337 -650.337] [0.0000], Avg: [-566.914 -566.914 -566.914] (0.100)
Step: 27149, Reward: [-523.892 -523.892 -523.892] [0.0000], Avg: [-566.835 -566.835 -566.835] (0.100)
Step: 27199, Reward: [-761.275 -761.275 -761.275] [0.0000], Avg: [-567.192 -567.192 -567.192] (0.100)
Step: 27249, Reward: [-539.373 -539.373 -539.373] [0.0000], Avg: [-567.141 -567.141 -567.141] (0.100)
Step: 27299, Reward: [-552.904 -552.904 -552.904] [0.0000], Avg: [-567.115 -567.115 -567.115] (0.100)
Step: 27349, Reward: [-470.693 -470.693 -470.693] [0.0000], Avg: [-566.939 -566.939 -566.939] (0.100)
Step: 27399, Reward: [-249.615 -249.615 -249.615] [0.0000], Avg: [-566.36 -566.36 -566.36] (0.100)
Step: 27449, Reward: [-472.13 -472.13 -472.13] [0.0000], Avg: [-566.188 -566.188 -566.188] (0.100)
Step: 27499, Reward: [-779.236 -779.236 -779.236] [0.0000], Avg: [-566.576 -566.576 -566.576] (0.100)
Step: 27549, Reward: [-319.76 -319.76 -319.76] [0.0000], Avg: [-566.128 -566.128 -566.128] (0.100)
Step: 27599, Reward: [-387.183 -387.183 -387.183] [0.0000], Avg: [-565.804 -565.804 -565.804] (0.100)
Step: 27649, Reward: [-514.477 -514.477 -514.477] [0.0000], Avg: [-565.711 -565.711 -565.711] (0.100)
Step: 27699, Reward: [-730.608 -730.608 -730.608] [0.0000], Avg: [-566.008 -566.008 -566.008] (0.100)
Step: 27749, Reward: [-449.968 -449.968 -449.968] [0.0000], Avg: [-565.799 -565.799 -565.799] (0.100)
Step: 27799, Reward: [-416.141 -416.141 -416.141] [0.0000], Avg: [-565.53 -565.53 -565.53] (0.100)
Step: 27849, Reward: [-555.901 -555.901 -555.901] [0.0000], Avg: [-565.513 -565.513 -565.513] (0.100)
Step: 27899, Reward: [-359.894 -359.894 -359.894] [0.0000], Avg: [-565.144 -565.144 -565.144] (0.100)
Step: 27949, Reward: [-432.522 -432.522 -432.522] [0.0000], Avg: [-564.907 -564.907 -564.907] (0.100)
Step: 27999, Reward: [-492.267 -492.267 -492.267] [0.0000], Avg: [-564.777 -564.777 -564.777] (0.100)
Step: 28049, Reward: [-460.947 -460.947 -460.947] [0.0000], Avg: [-564.592 -564.592 -564.592] (0.100)
Step: 28099, Reward: [-509.353 -509.353 -509.353] [0.0000], Avg: [-564.494 -564.494 -564.494] (0.100)
Step: 28149, Reward: [-481.558 -481.558 -481.558] [0.0000], Avg: [-564.347 -564.347 -564.347] (0.100)
Step: 28199, Reward: [-724.025 -724.025 -724.025] [0.0000], Avg: [-564.63 -564.63 -564.63] (0.100)
Step: 28249, Reward: [-728.918 -728.918 -728.918] [0.0000], Avg: [-564.921 -564.921 -564.921] (0.100)
Step: 28299, Reward: [-485.597 -485.597 -485.597] [0.0000], Avg: [-564.78 -564.78 -564.78] (0.100)
Step: 28349, Reward: [-527.729 -527.729 -527.729] [0.0000], Avg: [-564.715 -564.715 -564.715] (0.100)
Step: 28399, Reward: [-502.48 -502.48 -502.48] [0.0000], Avg: [-564.606 -564.606 -564.606] (0.100)
Step: 28449, Reward: [-414.249 -414.249 -414.249] [0.0000], Avg: [-564.341 -564.341 -564.341] (0.100)
Step: 28499, Reward: [-859.995 -859.995 -859.995] [0.0000], Avg: [-564.86 -564.86 -564.86] (0.100)
Step: 28549, Reward: [-396.387 -396.387 -396.387] [0.0000], Avg: [-564.565 -564.565 -564.565] (0.100)
Step: 28599, Reward: [-460.165 -460.165 -460.165] [0.0000], Avg: [-564.382 -564.382 -564.382] (0.100)
Step: 28649, Reward: [-638.765 -638.765 -638.765] [0.0000], Avg: [-564.512 -564.512 -564.512] (0.100)
Step: 28699, Reward: [-972.593 -972.593 -972.593] [0.0000], Avg: [-565.223 -565.223 -565.223] (0.100)
Step: 28749, Reward: [-567.464 -567.464 -567.464] [0.0000], Avg: [-565.227 -565.227 -565.227] (0.100)
Step: 28799, Reward: [-845.563 -845.563 -845.563] [0.0000], Avg: [-565.714 -565.714 -565.714] (0.100)
Step: 28849, Reward: [-447.683 -447.683 -447.683] [0.0000], Avg: [-565.509 -565.509 -565.509] (0.100)
Step: 28899, Reward: [-563.693 -563.693 -563.693] [0.0000], Avg: [-565.506 -565.506 -565.506] (0.100)
Step: 28949, Reward: [-949.352 -949.352 -949.352] [0.0000], Avg: [-566.169 -566.169 -566.169] (0.100)
Step: 28999, Reward: [-459.592 -459.592 -459.592] [0.0000], Avg: [-565.985 -565.985 -565.985] (0.100)
Step: 29049, Reward: [-523.367 -523.367 -523.367] [0.0000], Avg: [-565.912 -565.912 -565.912] (0.100)
Step: 29099, Reward: [-493.101 -493.101 -493.101] [0.0000], Avg: [-565.787 -565.787 -565.787] (0.100)
Step: 29149, Reward: [-631.951 -631.951 -631.951] [0.0000], Avg: [-565.9 -565.9 -565.9] (0.100)
Step: 29199, Reward: [-468.618 -468.618 -468.618] [0.0000], Avg: [-565.734 -565.734 -565.734] (0.100)
Step: 29249, Reward: [-874.252 -874.252 -874.252] [0.0000], Avg: [-566.261 -566.261 -566.261] (0.100)
Step: 29299, Reward: [-490.83 -490.83 -490.83] [0.0000], Avg: [-566.132 -566.132 -566.132] (0.100)
Step: 29349, Reward: [-364.019 -364.019 -364.019] [0.0000], Avg: [-565.788 -565.788 -565.788] (0.100)
Step: 29399, Reward: [-509.476 -509.476 -509.476] [0.0000], Avg: [-565.692 -565.692 -565.692] (0.100)
Step: 29449, Reward: [-1019.542 -1019.542 -1019.542] [0.0000], Avg: [-566.463 -566.463 -566.463] (0.100)
Step: 29499, Reward: [-622.317 -622.317 -622.317] [0.0000], Avg: [-566.558 -566.558 -566.558] (0.100)
Step: 29549, Reward: [-679.71 -679.71 -679.71] [0.0000], Avg: [-566.749 -566.749 -566.749] (0.100)
Step: 29599, Reward: [-467.775 -467.775 -467.775] [0.0000], Avg: [-566.582 -566.582 -566.582] (0.100)
Step: 29649, Reward: [-420.934 -420.934 -420.934] [0.0000], Avg: [-566.336 -566.336 -566.336] (0.100)
Step: 29699, Reward: [-328.588 -328.588 -328.588] [0.0000], Avg: [-565.936 -565.936 -565.936] (0.100)
Step: 29749, Reward: [-340.01 -340.01 -340.01] [0.0000], Avg: [-565.556 -565.556 -565.556] (0.100)
Step: 29799, Reward: [-351.764 -351.764 -351.764] [0.0000], Avg: [-565.198 -565.198 -565.198] (0.100)
Step: 29849, Reward: [-342.983 -342.983 -342.983] [0.0000], Avg: [-564.825 -564.825 -564.825] (0.100)
Step: 29899, Reward: [-599.745 -599.745 -599.745] [0.0000], Avg: [-564.884 -564.884 -564.884] (0.100)
Step: 29949, Reward: [-481.373 -481.373 -481.373] [0.0000], Avg: [-564.744 -564.744 -564.744] (0.100)
Step: 29999, Reward: [-585.243 -585.243 -585.243] [0.0000], Avg: [-564.778 -564.778 -564.778] (0.100)
Step: 30049, Reward: [-667.64 -667.64 -667.64] [0.0000], Avg: [-564.95 -564.95 -564.95] (0.100)
Step: 30099, Reward: [-433.346 -433.346 -433.346] [0.0000], Avg: [-564.731 -564.731 -564.731] (0.100)
Step: 30149, Reward: [-464.108 -464.108 -464.108] [0.0000], Avg: [-564.564 -564.564 -564.564] (0.100)
Step: 30199, Reward: [-484.124 -484.124 -484.124] [0.0000], Avg: [-564.431 -564.431 -564.431] (0.100)
Step: 30249, Reward: [-600.46 -600.46 -600.46] [0.0000], Avg: [-564.49 -564.49 -564.49] (0.100)
Step: 30299, Reward: [-664.286 -664.286 -664.286] [0.0000], Avg: [-564.655 -564.655 -564.655] (0.100)
Step: 30349, Reward: [-480.756 -480.756 -480.756] [0.0000], Avg: [-564.517 -564.517 -564.517] (0.100)
Step: 30399, Reward: [-343.971 -343.971 -343.971] [0.0000], Avg: [-564.154 -564.154 -564.154] (0.100)
Step: 30449, Reward: [-364.319 -364.319 -364.319] [0.0000], Avg: [-563.826 -563.826 -563.826] (0.100)
Step: 30499, Reward: [-475.532 -475.532 -475.532] [0.0000], Avg: [-563.681 -563.681 -563.681] (0.100)
Step: 30549, Reward: [-394.923 -394.923 -394.923] [0.0000], Avg: [-563.405 -563.405 -563.405] (0.100)
Step: 30599, Reward: [-452.143 -452.143 -452.143] [0.0000], Avg: [-563.223 -563.223 -563.223] (0.100)
Step: 30649, Reward: [-573.277 -573.277 -573.277] [0.0000], Avg: [-563.24 -563.24 -563.24] (0.100)
Step: 30699, Reward: [-555.597 -555.597 -555.597] [0.0000], Avg: [-563.227 -563.227 -563.227] (0.100)
Step: 30749, Reward: [-432.111 -432.111 -432.111] [0.0000], Avg: [-563.014 -563.014 -563.014] (0.100)
Step: 30799, Reward: [-430.461 -430.461 -430.461] [0.0000], Avg: [-562.799 -562.799 -562.799] (0.100)
Step: 30849, Reward: [-485.288 -485.288 -485.288] [0.0000], Avg: [-562.673 -562.673 -562.673] (0.100)
Step: 30899, Reward: [-410.325 -410.325 -410.325] [0.0000], Avg: [-562.427 -562.427 -562.427] (0.100)
Step: 30949, Reward: [-519.867 -519.867 -519.867] [0.0000], Avg: [-562.358 -562.358 -562.358] (0.100)
Step: 30999, Reward: [-317.629 -317.629 -317.629] [0.0000], Avg: [-561.963 -561.963 -561.963] (0.100)
Step: 31049, Reward: [-396.287 -396.287 -396.287] [0.0000], Avg: [-561.696 -561.696 -561.696] (0.100)
Step: 31099, Reward: [-406.702 -406.702 -406.702] [0.0000], Avg: [-561.447 -561.447 -561.447] (0.100)
Step: 31149, Reward: [-391.614 -391.614 -391.614] [0.0000], Avg: [-561.175 -561.175 -561.175] (0.100)
Step: 31199, Reward: [-486.357 -486.357 -486.357] [0.0000], Avg: [-561.055 -561.055 -561.055] (0.100)
Step: 31249, Reward: [-464.714 -464.714 -464.714] [0.0000], Avg: [-560.901 -560.901 -560.901] (0.100)
Step: 31299, Reward: [-479.29 -479.29 -479.29] [0.0000], Avg: [-560.77 -560.77 -560.77] (0.100)
Step: 31349, Reward: [-628.97 -628.97 -628.97] [0.0000], Avg: [-560.879 -560.879 -560.879] (0.100)
Step: 31399, Reward: [-425.144 -425.144 -425.144] [0.0000], Avg: [-560.663 -560.663 -560.663] (0.100)
Step: 31449, Reward: [-433.836 -433.836 -433.836] [0.0000], Avg: [-560.461 -560.461 -560.461] (0.100)
Step: 31499, Reward: [-554.783 -554.783 -554.783] [0.0000], Avg: [-560.452 -560.452 -560.452] (0.100)
Step: 31549, Reward: [-819.244 -819.244 -819.244] [0.0000], Avg: [-560.862 -560.862 -560.862] (0.100)
Step: 31599, Reward: [-468.249 -468.249 -468.249] [0.0000], Avg: [-560.716 -560.716 -560.716] (0.100)
Step: 31649, Reward: [-384.762 -384.762 -384.762] [0.0000], Avg: [-560.438 -560.438 -560.438] (0.100)
Step: 31699, Reward: [-551.234 -551.234 -551.234] [0.0000], Avg: [-560.423 -560.423 -560.423] (0.100)
Step: 31749, Reward: [-522.95 -522.95 -522.95] [0.0000], Avg: [-560.364 -560.364 -560.364] (0.100)
Step: 31799, Reward: [-347.519 -347.519 -347.519] [0.0000], Avg: [-560.03 -560.03 -560.03] (0.100)
Step: 31849, Reward: [-412.506 -412.506 -412.506] [0.0000], Avg: [-559.798 -559.798 -559.798] (0.100)
Step: 31899, Reward: [-494.997 -494.997 -494.997] [0.0000], Avg: [-559.697 -559.697 -559.697] (0.100)
Step: 31949, Reward: [-742.495 -742.495 -742.495] [0.0000], Avg: [-559.983 -559.983 -559.983] (0.100)
Step: 31999, Reward: [-406.964 -406.964 -406.964] [0.0000], Avg: [-559.743 -559.743 -559.743] (0.100)
Step: 32049, Reward: [-474.607 -474.607 -474.607] [0.0000], Avg: [-559.611 -559.611 -559.611] (0.100)
Step: 32099, Reward: [-489.573 -489.573 -489.573] [0.0000], Avg: [-559.502 -559.502 -559.502] (0.100)
Step: 32149, Reward: [-811.486 -811.486 -811.486] [0.0000], Avg: [-559.893 -559.893 -559.893] (0.100)
Step: 32199, Reward: [-322.27 -322.27 -322.27] [0.0000], Avg: [-559.524 -559.524 -559.524] (0.100)
Step: 32249, Reward: [-488.994 -488.994 -488.994] [0.0000], Avg: [-559.415 -559.415 -559.415] (0.100)
Step: 32299, Reward: [-354.04 -354.04 -354.04] [0.0000], Avg: [-559.097 -559.097 -559.097] (0.100)
Step: 32349, Reward: [-554.333 -554.333 -554.333] [0.0000], Avg: [-559.09 -559.09 -559.09] (0.100)
Step: 32399, Reward: [-452.031 -452.031 -452.031] [0.0000], Avg: [-558.925 -558.925 -558.925] (0.100)
Step: 32449, Reward: [-459.224 -459.224 -459.224] [0.0000], Avg: [-558.771 -558.771 -558.771] (0.100)
Step: 32499, Reward: [-479.37 -479.37 -479.37] [0.0000], Avg: [-558.649 -558.649 -558.649] (0.100)
Step: 32549, Reward: [-394.316 -394.316 -394.316] [0.0000], Avg: [-558.396 -558.396 -558.396] (0.100)
Step: 32599, Reward: [-563.944 -563.944 -563.944] [0.0000], Avg: [-558.405 -558.405 -558.405] (0.100)
Step: 32649, Reward: [-550.82 -550.82 -550.82] [0.0000], Avg: [-558.393 -558.393 -558.393] (0.100)
Step: 32699, Reward: [-455.173 -455.173 -455.173] [0.0000], Avg: [-558.236 -558.236 -558.236] (0.100)
Step: 32749, Reward: [-483.975 -483.975 -483.975] [0.0000], Avg: [-558.122 -558.122 -558.122] (0.100)
Step: 32799, Reward: [-384.662 -384.662 -384.662] [0.0000], Avg: [-557.858 -557.858 -557.858] (0.100)
Step: 32849, Reward: [-345.173 -345.173 -345.173] [0.0000], Avg: [-557.534 -557.534 -557.534] (0.100)
Step: 32899, Reward: [-467.568 -467.568 -467.568] [0.0000], Avg: [-557.397 -557.397 -557.397] (0.100)
Step: 32949, Reward: [-647.031 -647.031 -647.031] [0.0000], Avg: [-557.533 -557.533 -557.533] (0.100)
Step: 32999, Reward: [-451.044 -451.044 -451.044] [0.0000], Avg: [-557.372 -557.372 -557.372] (0.100)
Step: 33049, Reward: [-505.79 -505.79 -505.79] [0.0000], Avg: [-557.294 -557.294 -557.294] (0.100)
Step: 33099, Reward: [-531.549 -531.549 -531.549] [0.0000], Avg: [-557.255 -557.255 -557.255] (0.100)
Step: 33149, Reward: [-514.047 -514.047 -514.047] [0.0000], Avg: [-557.19 -557.19 -557.19] (0.100)
Step: 33199, Reward: [-515.532 -515.532 -515.532] [0.0000], Avg: [-557.127 -557.127 -557.127] (0.100)
Step: 33249, Reward: [-591.588 -591.588 -591.588] [0.0000], Avg: [-557.179 -557.179 -557.179] (0.100)
Step: 33299, Reward: [-463.129 -463.129 -463.129] [0.0000], Avg: [-557.038 -557.038 -557.038] (0.100)
Step: 33349, Reward: [-389.893 -389.893 -389.893] [0.0000], Avg: [-556.787 -556.787 -556.787] (0.100)
Step: 33399, Reward: [-504.774 -504.774 -504.774] [0.0000], Avg: [-556.709 -556.709 -556.709] (0.100)
Step: 33449, Reward: [-837.458 -837.458 -837.458] [0.0000], Avg: [-557.129 -557.129 -557.129] (0.100)
Step: 33499, Reward: [-632.69 -632.69 -632.69] [0.0000], Avg: [-557.242 -557.242 -557.242] (0.100)
Step: 33549, Reward: [-587.736 -587.736 -587.736] [0.0000], Avg: [-557.287 -557.287 -557.287] (0.100)
Step: 33599, Reward: [-428.629 -428.629 -428.629] [0.0000], Avg: [-557.096 -557.096 -557.096] (0.100)
Step: 33649, Reward: [-557.594 -557.594 -557.594] [0.0000], Avg: [-557.096 -557.096 -557.096] (0.100)
Step: 33699, Reward: [-398.913 -398.913 -398.913] [0.0000], Avg: [-556.862 -556.862 -556.862] (0.100)
Step: 33749, Reward: [-471.978 -471.978 -471.978] [0.0000], Avg: [-556.736 -556.736 -556.736] (0.100)
Step: 33799, Reward: [-671.955 -671.955 -671.955] [0.0000], Avg: [-556.906 -556.906 -556.906] (0.100)
Step: 33849, Reward: [-600.399 -600.399 -600.399] [0.0000], Avg: [-556.971 -556.971 -556.971] (0.100)
Step: 33899, Reward: [-541.914 -541.914 -541.914] [0.0000], Avg: [-556.948 -556.948 -556.948] (0.100)
Step: 33949, Reward: [-501.985 -501.985 -501.985] [0.0000], Avg: [-556.867 -556.867 -556.867] (0.100)
Step: 33999, Reward: [-437.152 -437.152 -437.152] [0.0000], Avg: [-556.691 -556.691 -556.691] (0.100)
Step: 34049, Reward: [-412.981 -412.981 -412.981] [0.0000], Avg: [-556.48 -556.48 -556.48] (0.100)
Step: 34099, Reward: [-445.046 -445.046 -445.046] [0.0000], Avg: [-556.317 -556.317 -556.317] (0.100)
Step: 34149, Reward: [-941.157 -941.157 -941.157] [0.0000], Avg: [-556.88 -556.88 -556.88] (0.100)
Step: 34199, Reward: [-670.01 -670.01 -670.01] [0.0000], Avg: [-557.046 -557.046 -557.046] (0.100)
Step: 34249, Reward: [-681.456 -681.456 -681.456] [0.0000], Avg: [-557.227 -557.227 -557.227] (0.100)
Step: 34299, Reward: [-449.491 -449.491 -449.491] [0.0000], Avg: [-557.07 -557.07 -557.07] (0.100)
Step: 34349, Reward: [-356.546 -356.546 -356.546] [0.0000], Avg: [-556.779 -556.779 -556.779] (0.100)
Step: 34399, Reward: [-490.219 -490.219 -490.219] [0.0000], Avg: [-556.682 -556.682 -556.682] (0.100)
Step: 34449, Reward: [-457.268 -457.268 -457.268] [0.0000], Avg: [-556.538 -556.538 -556.538] (0.100)
Step: 34499, Reward: [-682.925 -682.925 -682.925] [0.0000], Avg: [-556.721 -556.721 -556.721] (0.100)
Step: 34549, Reward: [-763.524 -763.524 -763.524] [0.0000], Avg: [-557.02 -557.02 -557.02] (0.100)
Step: 34599, Reward: [-498.786 -498.786 -498.786] [0.0000], Avg: [-556.936 -556.936 -556.936] (0.100)
Step: 34649, Reward: [-457.101 -457.101 -457.101] [0.0000], Avg: [-556.792 -556.792 -556.792] (0.100)
Step: 34699, Reward: [-571.385 -571.385 -571.385] [0.0000], Avg: [-556.813 -556.813 -556.813] (0.100)
Step: 34749, Reward: [-351.735 -351.735 -351.735] [0.0000], Avg: [-556.518 -556.518 -556.518] (0.100)
Step: 34799, Reward: [-450.726 -450.726 -450.726] [0.0000], Avg: [-556.366 -556.366 -556.366] (0.100)
Step: 34849, Reward: [-305.966 -305.966 -305.966] [0.0000], Avg: [-556.006 -556.006 -556.006] (0.100)
Step: 34899, Reward: [-541.772 -541.772 -541.772] [0.0000], Avg: [-555.986 -555.986 -555.986] (0.100)
Step: 34949, Reward: [-348.915 -348.915 -348.915] [0.0000], Avg: [-555.69 -555.69 -555.69] (0.100)
Step: 34999, Reward: [-528.453 -528.453 -528.453] [0.0000], Avg: [-555.651 -555.651 -555.651] (0.100)
Step: 35049, Reward: [-807.148 -807.148 -807.148] [0.0000], Avg: [-556.01 -556.01 -556.01] (0.100)
Step: 35099, Reward: [-443.428 -443.428 -443.428] [0.0000], Avg: [-555.849 -555.849 -555.849] (0.100)
Step: 35149, Reward: [-579.391 -579.391 -579.391] [0.0000], Avg: [-555.883 -555.883 -555.883] (0.100)
Step: 35199, Reward: [-488.613 -488.613 -488.613] [0.0000], Avg: [-555.787 -555.787 -555.787] (0.100)
Step: 35249, Reward: [-376.027 -376.027 -376.027] [0.0000], Avg: [-555.532 -555.532 -555.532] (0.100)
Step: 35299, Reward: [-474.228 -474.228 -474.228] [0.0000], Avg: [-555.417 -555.417 -555.417] (0.100)
Step: 35349, Reward: [-440.813 -440.813 -440.813] [0.0000], Avg: [-555.255 -555.255 -555.255] (0.100)
Step: 35399, Reward: [-261.812 -261.812 -261.812] [0.0000], Avg: [-554.841 -554.841 -554.841] (0.100)
Step: 35449, Reward: [-532.595 -532.595 -532.595] [0.0000], Avg: [-554.809 -554.809 -554.809] (0.100)
Step: 35499, Reward: [-375.125 -375.125 -375.125] [0.0000], Avg: [-554.556 -554.556 -554.556] (0.100)
Step: 35549, Reward: [-517.622 -517.622 -517.622] [0.0000], Avg: [-554.504 -554.504 -554.504] (0.100)
Step: 35599, Reward: [-472.4 -472.4 -472.4] [0.0000], Avg: [-554.389 -554.389 -554.389] (0.100)
Step: 35649, Reward: [-680.227 -680.227 -680.227] [0.0000], Avg: [-554.565 -554.565 -554.565] (0.100)
Step: 35699, Reward: [-381.344 -381.344 -381.344] [0.0000], Avg: [-554.323 -554.323 -554.323] (0.100)
Step: 35749, Reward: [-473.433 -473.433 -473.433] [0.0000], Avg: [-554.21 -554.21 -554.21] (0.100)
Step: 35799, Reward: [-397.645 -397.645 -397.645] [0.0000], Avg: [-553.991 -553.991 -553.991] (0.100)
Step: 35849, Reward: [-473.102 -473.102 -473.102] [0.0000], Avg: [-553.878 -553.878 -553.878] (0.100)
Step: 35899, Reward: [-510.918 -510.918 -510.918] [0.0000], Avg: [-553.818 -553.818 -553.818] (0.100)
Step: 35949, Reward: [-305.64 -305.64 -305.64] [0.0000], Avg: [-553.473 -553.473 -553.473] (0.100)
Step: 35999, Reward: [-448.973 -448.973 -448.973] [0.0000], Avg: [-553.328 -553.328 -553.328] (0.100)
Step: 36049, Reward: [-412.959 -412.959 -412.959] [0.0000], Avg: [-553.133 -553.133 -553.133] (0.100)
Step: 36099, Reward: [-579.476 -579.476 -579.476] [0.0000], Avg: [-553.17 -553.17 -553.17] (0.100)
Step: 36149, Reward: [-405.021 -405.021 -405.021] [0.0000], Avg: [-552.965 -552.965 -552.965] (0.100)
Step: 36199, Reward: [-685.415 -685.415 -685.415] [0.0000], Avg: [-553.148 -553.148 -553.148] (0.100)
Step: 36249, Reward: [-648.583 -648.583 -648.583] [0.0000], Avg: [-553.279 -553.279 -553.279] (0.100)
Step: 36299, Reward: [-875.077 -875.077 -875.077] [0.0000], Avg: [-553.723 -553.723 -553.723] (0.100)
Step: 36349, Reward: [-531.495 -531.495 -531.495] [0.0000], Avg: [-553.692 -553.692 -553.692] (0.100)
Step: 36399, Reward: [-422.541 -422.541 -422.541] [0.0000], Avg: [-553.512 -553.512 -553.512] (0.100)
Step: 36449, Reward: [-499.07 -499.07 -499.07] [0.0000], Avg: [-553.437 -553.437 -553.437] (0.100)
Step: 36499, Reward: [-391.62 -391.62 -391.62] [0.0000], Avg: [-553.216 -553.216 -553.216] (0.100)
Step: 36549, Reward: [-435.956 -435.956 -435.956] [0.0000], Avg: [-553.055 -553.055 -553.055] (0.100)
Step: 36599, Reward: [-412.939 -412.939 -412.939] [0.0000], Avg: [-552.864 -552.864 -552.864] (0.100)
Step: 36649, Reward: [-642.113 -642.113 -642.113] [0.0000], Avg: [-552.986 -552.986 -552.986] (0.100)
Step: 36699, Reward: [-469.921 -469.921 -469.921] [0.0000], Avg: [-552.872 -552.872 -552.872] (0.100)
Step: 36749, Reward: [-1033.679 -1033.679 -1033.679] [0.0000], Avg: [-553.527 -553.527 -553.527] (0.100)
Step: 36799, Reward: [-462.854 -462.854 -462.854] [0.0000], Avg: [-553.403 -553.403 -553.403] (0.100)
Step: 36849, Reward: [-874.89 -874.89 -874.89] [0.0000], Avg: [-553.84 -553.84 -553.84] (0.100)
Step: 36899, Reward: [-390.786 -390.786 -390.786] [0.0000], Avg: [-553.619 -553.619 -553.619] (0.100)
Step: 36949, Reward: [-558.647 -558.647 -558.647] [0.0000], Avg: [-553.625 -553.625 -553.625] (0.100)
Step: 36999, Reward: [-456.558 -456.558 -456.558] [0.0000], Avg: [-553.494 -553.494 -553.494] (0.100)
Step: 37049, Reward: [-1067.153 -1067.153 -1067.153] [0.0000], Avg: [-554.187 -554.187 -554.187] (0.100)
Step: 37099, Reward: [-312.198 -312.198 -312.198] [0.0000], Avg: [-553.861 -553.861 -553.861] (0.100)
Step: 37149, Reward: [-406.456 -406.456 -406.456] [0.0000], Avg: [-553.663 -553.663 -553.663] (0.100)
Step: 37199, Reward: [-551.286 -551.286 -551.286] [0.0000], Avg: [-553.66 -553.66 -553.66] (0.100)
Step: 37249, Reward: [-488.442 -488.442 -488.442] [0.0000], Avg: [-553.572 -553.572 -553.572] (0.100)
Step: 37299, Reward: [-575.846 -575.846 -575.846] [0.0000], Avg: [-553.602 -553.602 -553.602] (0.100)
Step: 37349, Reward: [-596.04 -596.04 -596.04] [0.0000], Avg: [-553.659 -553.659 -553.659] (0.100)
Step: 37399, Reward: [-557.116 -557.116 -557.116] [0.0000], Avg: [-553.663 -553.663 -553.663] (0.100)
Step: 37449, Reward: [-1345.712 -1345.712 -1345.712] [0.0000], Avg: [-554.721 -554.721 -554.721] (0.100)
Step: 37499, Reward: [-958.992 -958.992 -958.992] [0.0000], Avg: [-555.26 -555.26 -555.26] (0.100)
Step: 37549, Reward: [-555.101 -555.101 -555.101] [0.0000], Avg: [-555.26 -555.26 -555.26] (0.100)
Step: 37599, Reward: [-1376.99 -1376.99 -1376.99] [0.0000], Avg: [-556.352 -556.352 -556.352] (0.100)
Step: 37649, Reward: [-399.263 -399.263 -399.263] [0.0000], Avg: [-556.144 -556.144 -556.144] (0.100)
Step: 37699, Reward: [-466.843 -466.843 -466.843] [0.0000], Avg: [-556.025 -556.025 -556.025] (0.100)
Step: 37749, Reward: [-346.45 -346.45 -346.45] [0.0000], Avg: [-555.748 -555.748 -555.748] (0.100)
Step: 37799, Reward: [-556.898 -556.898 -556.898] [0.0000], Avg: [-555.749 -555.749 -555.749] (0.100)
Step: 37849, Reward: [-511.409 -511.409 -511.409] [0.0000], Avg: [-555.691 -555.691 -555.691] (0.100)
Step: 37899, Reward: [-492.869 -492.869 -492.869] [0.0000], Avg: [-555.608 -555.608 -555.608] (0.100)
Step: 37949, Reward: [-281.118 -281.118 -281.118] [0.0000], Avg: [-555.246 -555.246 -555.246] (0.100)
Step: 37999, Reward: [-557.224 -557.224 -557.224] [0.0000], Avg: [-555.249 -555.249 -555.249] (0.100)
Step: 38049, Reward: [-400.634 -400.634 -400.634] [0.0000], Avg: [-555.046 -555.046 -555.046] (0.100)
Step: 38099, Reward: [-685.519 -685.519 -685.519] [0.0000], Avg: [-555.217 -555.217 -555.217] (0.100)
Step: 38149, Reward: [-629.627 -629.627 -629.627] [0.0000], Avg: [-555.314 -555.314 -555.314] (0.100)
Step: 38199, Reward: [-711.177 -711.177 -711.177] [0.0000], Avg: [-555.518 -555.518 -555.518] (0.100)
Step: 38249, Reward: [-672.442 -672.442 -672.442] [0.0000], Avg: [-555.671 -555.671 -555.671] (0.100)
Step: 38299, Reward: [-735.04 -735.04 -735.04] [0.0000], Avg: [-555.905 -555.905 -555.905] (0.100)
Step: 38349, Reward: [-516.411 -516.411 -516.411] [0.0000], Avg: [-555.854 -555.854 -555.854] (0.100)
Step: 38399, Reward: [-423.929 -423.929 -423.929] [0.0000], Avg: [-555.682 -555.682 -555.682] (0.100)
Step: 38449, Reward: [-801.334 -801.334 -801.334] [0.0000], Avg: [-556.002 -556.002 -556.002] (0.100)
Step: 38499, Reward: [-743.942 -743.942 -743.942] [0.0000], Avg: [-556.246 -556.246 -556.246] (0.100)
Step: 38549, Reward: [-461.833 -461.833 -461.833] [0.0000], Avg: [-556.123 -556.123 -556.123] (0.100)
Step: 38599, Reward: [-447.69 -447.69 -447.69] [0.0000], Avg: [-555.983 -555.983 -555.983] (0.100)
Step: 38649, Reward: [-539.604 -539.604 -539.604] [0.0000], Avg: [-555.962 -555.962 -555.962] (0.100)
Step: 38699, Reward: [-704.973 -704.973 -704.973] [0.0000], Avg: [-556.154 -556.154 -556.154] (0.100)
Step: 38749, Reward: [-650.928 -650.928 -650.928] [0.0000], Avg: [-556.276 -556.276 -556.276] (0.100)
Step: 38799, Reward: [-564.974 -564.974 -564.974] [0.0000], Avg: [-556.288 -556.288 -556.288] (0.100)
Step: 38849, Reward: [-488.269 -488.269 -488.269] [0.0000], Avg: [-556.2 -556.2 -556.2] (0.100)
Step: 38899, Reward: [-495.725 -495.725 -495.725] [0.0000], Avg: [-556.122 -556.122 -556.122] (0.100)
Step: 38949, Reward: [-638.869 -638.869 -638.869] [0.0000], Avg: [-556.229 -556.229 -556.229] (0.100)
Step: 38999, Reward: [-606.357 -606.357 -606.357] [0.0000], Avg: [-556.293 -556.293 -556.293] (0.100)
Step: 39049, Reward: [-365.634 -365.634 -365.634] [0.0000], Avg: [-556.049 -556.049 -556.049] (0.100)
Step: 39099, Reward: [-848.358 -848.358 -848.358] [0.0000], Avg: [-556.423 -556.423 -556.423] (0.100)
Step: 39149, Reward: [-420.922 -420.922 -420.922] [0.0000], Avg: [-556.249 -556.249 -556.249] (0.100)
Step: 39199, Reward: [-550.931 -550.931 -550.931] [0.0000], Avg: [-556.243 -556.243 -556.243] (0.100)
Step: 39249, Reward: [-522.994 -522.994 -522.994] [0.0000], Avg: [-556.2 -556.2 -556.2] (0.100)
Step: 39299, Reward: [-529.163 -529.163 -529.163] [0.0000], Avg: [-556.166 -556.166 -556.166] (0.100)
Step: 39349, Reward: [-324.457 -324.457 -324.457] [0.0000], Avg: [-555.871 -555.871 -555.871] (0.100)
Step: 39399, Reward: [-556.01 -556.01 -556.01] [0.0000], Avg: [-555.872 -555.872 -555.872] (0.100)
Step: 39449, Reward: [-478.888 -478.888 -478.888] [0.0000], Avg: [-555.774 -555.774 -555.774] (0.100)
Step: 39499, Reward: [-596.341 -596.341 -596.341] [0.0000], Avg: [-555.825 -555.825 -555.825] (0.100)
Step: 39549, Reward: [-487.402 -487.402 -487.402] [0.0000], Avg: [-555.739 -555.739 -555.739] (0.100)
Step: 39599, Reward: [-651.59 -651.59 -651.59] [0.0000], Avg: [-555.86 -555.86 -555.86] (0.100)
Step: 39649, Reward: [-522.852 -522.852 -522.852] [0.0000], Avg: [-555.818 -555.818 -555.818] (0.100)
Step: 39699, Reward: [-674.121 -674.121 -674.121] [0.0000], Avg: [-555.967 -555.967 -555.967] (0.100)
Step: 39749, Reward: [-414.221 -414.221 -414.221] [0.0000], Avg: [-555.789 -555.789 -555.789] (0.100)
Step: 39799, Reward: [-418.647 -418.647 -418.647] [0.0000], Avg: [-555.617 -555.617 -555.617] (0.100)
Step: 39849, Reward: [-438.216 -438.216 -438.216] [0.0000], Avg: [-555.469 -555.469 -555.469] (0.100)
Step: 39899, Reward: [-585.657 -585.657 -585.657] [0.0000], Avg: [-555.507 -555.507 -555.507] (0.100)
Step: 39949, Reward: [-501.981 -501.981 -501.981] [0.0000], Avg: [-555.44 -555.44 -555.44] (0.100)
Step: 39999, Reward: [-537.585 -537.585 -537.585] [0.0000], Avg: [-555.418 -555.418 -555.418] (0.100)
Step: 40049, Reward: [-661.601 -661.601 -661.601] [0.0000], Avg: [-555.551 -555.551 -555.551] (0.100)
Step: 40099, Reward: [-691.797 -691.797 -691.797] [0.0000], Avg: [-555.72 -555.72 -555.72] (0.100)
Step: 40149, Reward: [-561.876 -561.876 -561.876] [0.0000], Avg: [-555.728 -555.728 -555.728] (0.100)
Step: 40199, Reward: [-469.387 -469.387 -469.387] [0.0000], Avg: [-555.621 -555.621 -555.621] (0.100)
Step: 40249, Reward: [-451.855 -451.855 -451.855] [0.0000], Avg: [-555.492 -555.492 -555.492] (0.100)
Step: 40299, Reward: [-359.791 -359.791 -359.791] [0.0000], Avg: [-555.249 -555.249 -555.249] (0.100)
Step: 40349, Reward: [-487.35 -487.35 -487.35] [0.0000], Avg: [-555.165 -555.165 -555.165] (0.100)
Step: 40399, Reward: [-367.683 -367.683 -367.683] [0.0000], Avg: [-554.933 -554.933 -554.933] (0.100)
Step: 40449, Reward: [-339.999 -339.999 -339.999] [0.0000], Avg: [-554.667 -554.667 -554.667] (0.100)
Step: 40499, Reward: [-483.169 -483.169 -483.169] [0.0000], Avg: [-554.579 -554.579 -554.579] (0.100)
Step: 40549, Reward: [-637.238 -637.238 -637.238] [0.0000], Avg: [-554.681 -554.681 -554.681] (0.100)
Step: 40599, Reward: [-415.179 -415.179 -415.179] [0.0000], Avg: [-554.509 -554.509 -554.509] (0.100)
Step: 40649, Reward: [-466.488 -466.488 -466.488] [0.0000], Avg: [-554.401 -554.401 -554.401] (0.100)
Step: 40699, Reward: [-685.894 -685.894 -685.894] [0.0000], Avg: [-554.562 -554.562 -554.562] (0.100)
Step: 40749, Reward: [-386.057 -386.057 -386.057] [0.0000], Avg: [-554.356 -554.356 -554.356] (0.100)
Step: 40799, Reward: [-611.644 -611.644 -611.644] [0.0000], Avg: [-554.426 -554.426 -554.426] (0.100)
Step: 40849, Reward: [-449.766 -449.766 -449.766] [0.0000], Avg: [-554.298 -554.298 -554.298] (0.100)
Step: 40899, Reward: [-431.598 -431.598 -431.598] [0.0000], Avg: [-554.148 -554.148 -554.148] (0.100)
Step: 40949, Reward: [-991.164 -991.164 -991.164] [0.0000], Avg: [-554.681 -554.681 -554.681] (0.100)
Step: 40999, Reward: [-606.519 -606.519 -606.519] [0.0000], Avg: [-554.744 -554.744 -554.744] (0.100)
Step: 41049, Reward: [-462.499 -462.499 -462.499] [0.0000], Avg: [-554.632 -554.632 -554.632] (0.100)
Step: 41099, Reward: [-730.024 -730.024 -730.024] [0.0000], Avg: [-554.845 -554.845 -554.845] (0.100)
Step: 41149, Reward: [-580.217 -580.217 -580.217] [0.0000], Avg: [-554.876 -554.876 -554.876] (0.100)
Step: 41199, Reward: [-386.208 -386.208 -386.208] [0.0000], Avg: [-554.672 -554.672 -554.672] (0.100)
Step: 41249, Reward: [-250.995 -250.995 -250.995] [0.0000], Avg: [-554.303 -554.303 -554.303] (0.100)
Step: 41299, Reward: [-646.729 -646.729 -646.729] [0.0000], Avg: [-554.415 -554.415 -554.415] (0.100)
Step: 41349, Reward: [-395.716 -395.716 -395.716] [0.0000], Avg: [-554.223 -554.223 -554.223] (0.100)
Step: 41399, Reward: [-948.402 -948.402 -948.402] [0.0000], Avg: [-554.7 -554.7 -554.7] (0.100)
Step: 41449, Reward: [-498.997 -498.997 -498.997] [0.0000], Avg: [-554.632 -554.632 -554.632] (0.100)
Step: 41499, Reward: [-622.218 -622.218 -622.218] [0.0000], Avg: [-554.714 -554.714 -554.714] (0.100)
Step: 41549, Reward: [-392.687 -392.687 -392.687] [0.0000], Avg: [-554.519 -554.519 -554.519] (0.100)
Step: 41599, Reward: [-514.205 -514.205 -514.205] [0.0000], Avg: [-554.47 -554.47 -554.47] (0.100)
Step: 41649, Reward: [-409.614 -409.614 -409.614] [0.0000], Avg: [-554.296 -554.296 -554.296] (0.100)
Step: 41699, Reward: [-359.296 -359.296 -359.296] [0.0000], Avg: [-554.063 -554.063 -554.063] (0.100)
Step: 41749, Reward: [-952.63 -952.63 -952.63] [0.0000], Avg: [-554.54 -554.54 -554.54] (0.100)
Step: 41799, Reward: [-543.13 -543.13 -543.13] [0.0000], Avg: [-554.526 -554.526 -554.526] (0.100)
Step: 41849, Reward: [-514.934 -514.934 -514.934] [0.0000], Avg: [-554.479 -554.479 -554.479] (0.100)
Step: 41899, Reward: [-397.073 -397.073 -397.073] [0.0000], Avg: [-554.291 -554.291 -554.291] (0.100)
Step: 41949, Reward: [-420.39 -420.39 -420.39] [0.0000], Avg: [-554.132 -554.132 -554.132] (0.100)
Step: 41999, Reward: [-375.76 -375.76 -375.76] [0.0000], Avg: [-553.919 -553.919 -553.919] (0.100)
Step: 42049, Reward: [-471.99 -471.99 -471.99] [0.0000], Avg: [-553.822 -553.822 -553.822] (0.100)
Step: 42099, Reward: [-458.841 -458.841 -458.841] [0.0000], Avg: [-553.709 -553.709 -553.709] (0.100)
Step: 42149, Reward: [-527.166 -527.166 -527.166] [0.0000], Avg: [-553.678 -553.678 -553.678] (0.100)
Step: 42199, Reward: [-1016.255 -1016.255 -1016.255] [0.0000], Avg: [-554.226 -554.226 -554.226] (0.100)
Step: 42249, Reward: [-527.706 -527.706 -527.706] [0.0000], Avg: [-554.194 -554.194 -554.194] (0.100)
Step: 42299, Reward: [-516.12 -516.12 -516.12] [0.0000], Avg: [-554.149 -554.149 -554.149] (0.100)
Step: 42349, Reward: [-467.116 -467.116 -467.116] [0.0000], Avg: [-554.046 -554.046 -554.046] (0.100)
Step: 42399, Reward: [-388.899 -388.899 -388.899] [0.0000], Avg: [-553.852 -553.852 -553.852] (0.100)
Step: 42449, Reward: [-511.728 -511.728 -511.728] [0.0000], Avg: [-553.802 -553.802 -553.802] (0.100)
Step: 42499, Reward: [-410.547 -410.547 -410.547] [0.0000], Avg: [-553.634 -553.634 -553.634] (0.100)
Step: 42549, Reward: [-427.614 -427.614 -427.614] [0.0000], Avg: [-553.485 -553.485 -553.485] (0.100)
Step: 42599, Reward: [-474.244 -474.244 -474.244] [0.0000], Avg: [-553.392 -553.392 -553.392] (0.100)
Step: 42649, Reward: [-541.85 -541.85 -541.85] [0.0000], Avg: [-553.379 -553.379 -553.379] (0.100)
Step: 42699, Reward: [-512.769 -512.769 -512.769] [0.0000], Avg: [-553.331 -553.331 -553.331] (0.100)
Step: 42749, Reward: [-403.059 -403.059 -403.059] [0.0000], Avg: [-553.156 -553.156 -553.156] (0.100)
Step: 42799, Reward: [-716.448 -716.448 -716.448] [0.0000], Avg: [-553.346 -553.346 -553.346] (0.100)
Step: 42849, Reward: [-910.495 -910.495 -910.495] [0.0000], Avg: [-553.763 -553.763 -553.763] (0.100)
Step: 42899, Reward: [-1025.161 -1025.161 -1025.161] [0.0000], Avg: [-554.313 -554.313 -554.313] (0.100)
Step: 42949, Reward: [-949.65 -949.65 -949.65] [0.0000], Avg: [-554.773 -554.773 -554.773] (0.100)
Step: 42999, Reward: [-605.002 -605.002 -605.002] [0.0000], Avg: [-554.831 -554.831 -554.831] (0.100)
Step: 43049, Reward: [-445.888 -445.888 -445.888] [0.0000], Avg: [-554.705 -554.705 -554.705] (0.100)
Step: 43099, Reward: [-515.925 -515.925 -515.925] [0.0000], Avg: [-554.66 -554.66 -554.66] (0.100)
Step: 43149, Reward: [-417.922 -417.922 -417.922] [0.0000], Avg: [-554.501 -554.501 -554.501] (0.100)
Step: 43199, Reward: [-1018.318 -1018.318 -1018.318] [0.0000], Avg: [-555.038 -555.038 -555.038] (0.100)
Step: 43249, Reward: [-459.331 -459.331 -459.331] [0.0000], Avg: [-554.927 -554.927 -554.927] (0.100)
Step: 43299, Reward: [-383.72 -383.72 -383.72] [0.0000], Avg: [-554.73 -554.73 -554.73] (0.100)
Step: 43349, Reward: [-260.811 -260.811 -260.811] [0.0000], Avg: [-554.391 -554.391 -554.391] (0.100)
Step: 43399, Reward: [-512.38 -512.38 -512.38] [0.0000], Avg: [-554.342 -554.342 -554.342] (0.100)
Step: 43449, Reward: [-421.366 -421.366 -421.366] [0.0000], Avg: [-554.189 -554.189 -554.189] (0.100)
Step: 43499, Reward: [-561.055 -561.055 -561.055] [0.0000], Avg: [-554.197 -554.197 -554.197] (0.100)
Step: 43549, Reward: [-477.148 -477.148 -477.148] [0.0000], Avg: [-554.109 -554.109 -554.109] (0.100)
Step: 43599, Reward: [-1114.465 -1114.465 -1114.465] [0.0000], Avg: [-554.751 -554.751 -554.751] (0.100)
Step: 43649, Reward: [-583.116 -583.116 -583.116] [0.0000], Avg: [-554.784 -554.784 -554.784] (0.100)
Step: 43699, Reward: [-418.081 -418.081 -418.081] [0.0000], Avg: [-554.627 -554.627 -554.627] (0.100)
Step: 43749, Reward: [-414.906 -414.906 -414.906] [0.0000], Avg: [-554.468 -554.468 -554.468] (0.100)
Step: 43799, Reward: [-688.18 -688.18 -688.18] [0.0000], Avg: [-554.62 -554.62 -554.62] (0.100)
Step: 43849, Reward: [-741.746 -741.746 -741.746] [0.0000], Avg: [-554.834 -554.834 -554.834] (0.100)
Step: 43899, Reward: [-1141.616 -1141.616 -1141.616] [0.0000], Avg: [-555.502 -555.502 -555.502] (0.100)
Step: 43949, Reward: [-343.931 -343.931 -343.931] [0.0000], Avg: [-555.261 -555.261 -555.261] (0.100)
Step: 43999, Reward: [-1251.376 -1251.376 -1251.376] [0.0000], Avg: [-556.052 -556.052 -556.052] (0.100)
Step: 44049, Reward: [-500.75 -500.75 -500.75] [0.0000], Avg: [-555.99 -555.99 -555.99] (0.100)
Step: 44099, Reward: [-262.078 -262.078 -262.078] [0.0000], Avg: [-555.656 -555.656 -555.656] (0.100)
Step: 44149, Reward: [-442.811 -442.811 -442.811] [0.0000], Avg: [-555.529 -555.529 -555.529] (0.100)
Step: 44199, Reward: [-431.909 -431.909 -431.909] [0.0000], Avg: [-555.389 -555.389 -555.389] (0.100)
Step: 44249, Reward: [-384.11 -384.11 -384.11] [0.0000], Avg: [-555.195 -555.195 -555.195] (0.100)
Step: 44299, Reward: [-606.611 -606.611 -606.611] [0.0000], Avg: [-555.253 -555.253 -555.253] (0.100)
Step: 44349, Reward: [-484.23 -484.23 -484.23] [0.0000], Avg: [-555.173 -555.173 -555.173] (0.100)
Step: 44399, Reward: [-354.814 -354.814 -354.814] [0.0000], Avg: [-554.948 -554.948 -554.948] (0.100)
Step: 44449, Reward: [-360.421 -360.421 -360.421] [0.0000], Avg: [-554.729 -554.729 -554.729] (0.100)
Step: 44499, Reward: [-705.907 -705.907 -705.907] [0.0000], Avg: [-554.899 -554.899 -554.899] (0.100)
Step: 44549, Reward: [-563.49 -563.49 -563.49] [0.0000], Avg: [-554.908 -554.908 -554.908] (0.100)
Step: 44599, Reward: [-1494.269 -1494.269 -1494.269] [0.0000], Avg: [-555.961 -555.961 -555.961] (0.100)
Step: 44649, Reward: [-398.881 -398.881 -398.881] [0.0000], Avg: [-555.785 -555.785 -555.785] (0.100)
Step: 44699, Reward: [-369.305 -369.305 -369.305] [0.0000], Avg: [-555.577 -555.577 -555.577] (0.100)
Step: 44749, Reward: [-590.255 -590.255 -590.255] [0.0000], Avg: [-555.616 -555.616 -555.616] (0.100)
Step: 44799, Reward: [-670.559 -670.559 -670.559] [0.0000], Avg: [-555.744 -555.744 -555.744] (0.100)
Step: 44849, Reward: [-586.978 -586.978 -586.978] [0.0000], Avg: [-555.779 -555.779 -555.779] (0.100)
Step: 44899, Reward: [-875.908 -875.908 -875.908] [0.0000], Avg: [-556.135 -556.135 -556.135] (0.100)
Step: 44949, Reward: [-569.261 -569.261 -569.261] [0.0000], Avg: [-556.15 -556.15 -556.15] (0.100)
Step: 44999, Reward: [-345.806 -345.806 -345.806] [0.0000], Avg: [-555.916 -555.916 -555.916] (0.100)
Step: 45049, Reward: [-440.957 -440.957 -440.957] [0.0000], Avg: [-555.788 -555.788 -555.788] (0.100)
Step: 45099, Reward: [-636.972 -636.972 -636.972] [0.0000], Avg: [-555.878 -555.878 -555.878] (0.100)
Step: 45149, Reward: [-538.803 -538.803 -538.803] [0.0000], Avg: [-555.86 -555.86 -555.86] (0.100)
Step: 45199, Reward: [-607.36 -607.36 -607.36] [0.0000], Avg: [-555.917 -555.917 -555.917] (0.100)
Step: 45249, Reward: [-415.337 -415.337 -415.337] [0.0000], Avg: [-555.761 -555.761 -555.761] (0.100)
Step: 45299, Reward: [-456.337 -456.337 -456.337] [0.0000], Avg: [-555.651 -555.651 -555.651] (0.100)
Step: 45349, Reward: [-433.515 -433.515 -433.515] [0.0000], Avg: [-555.517 -555.517 -555.517] (0.100)
Step: 45399, Reward: [-467.778 -467.778 -467.778] [0.0000], Avg: [-555.42 -555.42 -555.42] (0.100)
Step: 45449, Reward: [-524.542 -524.542 -524.542] [0.0000], Avg: [-555.386 -555.386 -555.386] (0.100)
Step: 45499, Reward: [-581.245 -581.245 -581.245] [0.0000], Avg: [-555.415 -555.415 -555.415] (0.100)
Step: 45549, Reward: [-397.533 -397.533 -397.533] [0.0000], Avg: [-555.241 -555.241 -555.241] (0.100)
Step: 45599, Reward: [-567.472 -567.472 -567.472] [0.0000], Avg: [-555.255 -555.255 -555.255] (0.100)
Step: 45649, Reward: [-476.083 -476.083 -476.083] [0.0000], Avg: [-555.168 -555.168 -555.168] (0.100)
Step: 45699, Reward: [-757.217 -757.217 -757.217] [0.0000], Avg: [-555.389 -555.389 -555.389] (0.100)
Step: 45749, Reward: [-412.345 -412.345 -412.345] [0.0000], Avg: [-555.233 -555.233 -555.233] (0.100)
Step: 45799, Reward: [-452.445 -452.445 -452.445] [0.0000], Avg: [-555.121 -555.121 -555.121] (0.100)
Step: 45849, Reward: [-421.06 -421.06 -421.06] [0.0000], Avg: [-554.974 -554.974 -554.974] (0.100)
Step: 45899, Reward: [-667.303 -667.303 -667.303] [0.0000], Avg: [-555.097 -555.097 -555.097] (0.100)
Step: 45949, Reward: [-645.71 -645.71 -645.71] [0.0000], Avg: [-555.195 -555.195 -555.195] (0.100)
Step: 45999, Reward: [-640.788 -640.788 -640.788] [0.0000], Avg: [-555.288 -555.288 -555.288] (0.100)
Step: 46049, Reward: [-503.312 -503.312 -503.312] [0.0000], Avg: [-555.232 -555.232 -555.232] (0.100)
Step: 46099, Reward: [-633.156 -633.156 -633.156] [0.0000], Avg: [-555.316 -555.316 -555.316] (0.100)
Step: 46149, Reward: [-379.05 -379.05 -379.05] [0.0000], Avg: [-555.125 -555.125 -555.125] (0.100)
Step: 46199, Reward: [-490.781 -490.781 -490.781] [0.0000], Avg: [-555.056 -555.056 -555.056] (0.100)
Step: 46249, Reward: [-575.695 -575.695 -575.695] [0.0000], Avg: [-555.078 -555.078 -555.078] (0.100)
Step: 46299, Reward: [-605.962 -605.962 -605.962] [0.0000], Avg: [-555.133 -555.133 -555.133] (0.100)
Step: 46349, Reward: [-629.757 -629.757 -629.757] [0.0000], Avg: [-555.214 -555.214 -555.214] (0.100)
Step: 46399, Reward: [-631.814 -631.814 -631.814] [0.0000], Avg: [-555.296 -555.296 -555.296] (0.100)
Step: 46449, Reward: [-320.999 -320.999 -320.999] [0.0000], Avg: [-555.044 -555.044 -555.044] (0.100)
Step: 46499, Reward: [-423.346 -423.346 -423.346] [0.0000], Avg: [-554.902 -554.902 -554.902] (0.100)
Step: 46549, Reward: [-926.479 -926.479 -926.479] [0.0000], Avg: [-555.301 -555.301 -555.301] (0.100)
Step: 46599, Reward: [-650.04 -650.04 -650.04] [0.0000], Avg: [-555.403 -555.403 -555.403] (0.100)
Step: 46649, Reward: [-494.545 -494.545 -494.545] [0.0000], Avg: [-555.338 -555.338 -555.338] (0.100)
Step: 46699, Reward: [-432.609 -432.609 -432.609] [0.0000], Avg: [-555.206 -555.206 -555.206] (0.100)
Step: 46749, Reward: [-695.277 -695.277 -695.277] [0.0000], Avg: [-555.356 -555.356 -555.356] (0.100)
Step: 46799, Reward: [-1035.738 -1035.738 -1035.738] [0.0000], Avg: [-555.869 -555.869 -555.869] (0.100)
Step: 46849, Reward: [-535.617 -535.617 -535.617] [0.0000], Avg: [-555.848 -555.848 -555.848] (0.100)
Step: 46899, Reward: [-417.118 -417.118 -417.118] [0.0000], Avg: [-555.7 -555.7 -555.7] (0.100)
Step: 46949, Reward: [-389.394 -389.394 -389.394] [0.0000], Avg: [-555.523 -555.523 -555.523] (0.100)
Step: 46999, Reward: [-634.444 -634.444 -634.444] [0.0000], Avg: [-555.607 -555.607 -555.607] (0.100)
Step: 47049, Reward: [-394.513 -394.513 -394.513] [0.0000], Avg: [-555.436 -555.436 -555.436] (0.100)
Step: 47099, Reward: [-568.962 -568.962 -568.962] [0.0000], Avg: [-555.45 -555.45 -555.45] (0.100)
Step: 47149, Reward: [-351.289 -351.289 -351.289] [0.0000], Avg: [-555.233 -555.233 -555.233] (0.100)
Step: 47199, Reward: [-1040.143 -1040.143 -1040.143] [0.0000], Avg: [-555.747 -555.747 -555.747] (0.100)
Step: 47249, Reward: [-460.059 -460.059 -460.059] [0.0000], Avg: [-555.646 -555.646 -555.646] (0.100)
Step: 47299, Reward: [-436.627 -436.627 -436.627] [0.0000], Avg: [-555.52 -555.52 -555.52] (0.100)
Step: 47349, Reward: [-1732.036 -1732.036 -1732.036] [0.0000], Avg: [-556.762 -556.762 -556.762] (0.100)
Step: 47399, Reward: [-705.421 -705.421 -705.421] [0.0000], Avg: [-556.919 -556.919 -556.919] (0.100)
Step: 47449, Reward: [-1694.474 -1694.474 -1694.474] [0.0000], Avg: [-558.118 -558.118 -558.118] (0.100)
Step: 47499, Reward: [-537.943 -537.943 -537.943] [0.0000], Avg: [-558.097 -558.097 -558.097] (0.100)
Step: 47549, Reward: [-515.654 -515.654 -515.654] [0.0000], Avg: [-558.052 -558.052 -558.052] (0.100)
Step: 47599, Reward: [-1245.71 -1245.71 -1245.71] [0.0000], Avg: [-558.774 -558.774 -558.774] (0.100)
Step: 47649, Reward: [-654.63 -654.63 -654.63] [0.0000], Avg: [-558.875 -558.875 -558.875] (0.100)
Step: 47699, Reward: [-594.906 -594.906 -594.906] [0.0000], Avg: [-558.913 -558.913 -558.913] (0.100)
Step: 47749, Reward: [-408.749 -408.749 -408.749] [0.0000], Avg: [-558.756 -558.756 -558.756] (0.100)
Step: 47799, Reward: [-528.153 -528.153 -528.153] [0.0000], Avg: [-558.723 -558.723 -558.723] (0.100)
Step: 47849, Reward: [-634.45 -634.45 -634.45] [0.0000], Avg: [-558.803 -558.803 -558.803] (0.100)
Step: 47899, Reward: [-373.97 -373.97 -373.97] [0.0000], Avg: [-558.61 -558.61 -558.61] (0.100)
