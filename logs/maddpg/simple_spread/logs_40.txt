Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		action = gumbel_softmax(action_mu, hard=True)
		action = action.view(*out_dims, -1)
		return action
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		agent_init_params = []
		# alg_types = [adversary_alg if atype == 'adversary' else agent_alg for atype in env.agent_types]
		for acsp, obsp in zip(action_size, state_size):
			num_in_pol = obsp[-1]
			# if isinstance(acsp, Box):
			#     discrete_action = False
			#     get_shape = lambda x: x.shape[0]
			# else:  # Discrete
			#     discrete_action = True
			#     get_shape = lambda x: x.n
			num_out_pol = acsp[-1]
			# if algtype == "MADDPG":
			num_in_critic = 0
			for oobsp in state_size:
				num_in_critic += oobsp[-1]
			for oacsp in action_size:
				num_in_critic += oacsp[-1]
			# else:
			#     num_in_critic = obsp.shape[0] + get_shape(acsp)
			agent_init_params.append({'num_in_pol': num_in_pol, 'num_out_pol': num_out_pol, 'num_in_critic': num_in_critic})
		# init_dict = {'gamm a': gamma, 'tau': tau, 'lr': lr, 'hidden_dim': hidden_dim, 'alg_types': alg_types, 'agent_init_params': agent_init_params, 'discrete_action': True}
		self.agent = MADDPG(agent_init_params, ["MADDPG"] * len(state_size))
		# self.agent.init_dict = init_dict
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, self.agent.nagents, [obsp[-1] for obsp in state_size], [acsp[-1] for acsp in action_size])

	def get_action(self, state, eps=None, sample=True, numpy=True):
		state = [torch.autograd.Variable(torch.Tensor(np.vstack(state[i])), requires_grad=False) for i in range(self.agent.nagents)]
		torch_agent_actions = self.agent.step(state, explore=True)
		agent_actions = [ac.data.numpy() for ac in torch_agent_actions]
		return agent_actions
		# eps = self.eps if eps is None else eps
		# action_random = super().get_action(state)
		# action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		# action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		# return action

	def train(self, state, action, next_state, reward, done):
		if not hasattr(self, "t"): self.t = 0
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.t % 100)==0):
			self.agent.prep_training(device='gpu' if torch.cuda.is_available() else 'cpu')
			for a_i in range(self.agent.nagents):
				sample = self.replay_buffer.sample(8, to_gpu=False)
				self.agent.update(sample, a_i)
			self.agent.update_all_targets()
			self.agent.prep_rollouts(device='cpu')
		self.t += 1
		# self.buffer.append((state, action, reward, done))
		# if np.any(done[0]) or len(self.buffer) >= self.update_freq:
		# 	states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
		# 	self.buffer.clear()
		# 	next_state = self.to_tensor(next_state)
		# 	states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
		# 	actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
		# 	states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		# 	actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
		# 	q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
		# 	q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
		# 	to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
		# 	states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
		# 	states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
		# 	self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		# if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
		# 	states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
		# 	self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		# if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

MSELoss = torch.nn.MSELoss()


class MADDPG():
	"""
	Wrapper class for DDPG-esque (i.e. also MADDPG) agents in multi-agent task
	"""
	def __init__(self, agent_init_params, alg_types, gamma=0.95, tau=0.01, lr=0.01, hidden_dim=64, discrete_action=True):
		self.nagents = len(alg_types)
		self.alg_types = alg_types
		self.agents = [DDPGAgent(lr=lr, discrete_action=discrete_action, hidden_dim=hidden_dim, **params) for params in agent_init_params]
		self.agent_init_params = agent_init_params
		self.gamma = gamma
		self.tau = tau
		self.lr = lr
		self.discrete_action = discrete_action
		self.pol_dev = 'cpu'  # device for policies
		self.critic_dev = 'cpu'  # device for critics
		self.trgt_pol_dev = 'cpu'  # device for target policies
		self.trgt_critic_dev = 'cpu'  # device for target critics
		self.niter = 0

	@property
	def policies(self):
		return [a.policy for a in self.agents]

	@property
	def target_policies(self):
		return [a.target_policy for a in self.agents]

	def scale_noise(self, scale):
		for a in self.agents:
			a.scale_noise(scale)

	def reset_noise(self):
		for a in self.agents:
			a.reset_noise()

	def step(self, observations, explore=False):
		return [a.step(obs, explore=explore) for a, obs in zip(self.agents, observations)]

	def update(self, sample, agent_i, parallel=False, logger=None):
		obs, acs, rews, next_obs, dones = sample
		curr_agent = self.agents[agent_i]

		curr_agent.critic_optimizer.zero_grad()
		if self.alg_types[agent_i] == 'MADDPG':
			if self.discrete_action: # one-hot encode action
				all_trgt_acs = [one_hot(pi(nobs)) for pi, nobs in zip(self.target_policies, next_obs)]
			else:
				all_trgt_acs = [pi(nobs) for pi, nobs in zip(self.target_policies, next_obs)]
			trgt_vf_in = torch.cat((*next_obs, *all_trgt_acs), dim=1)
		else:  # DDPG
			if self.discrete_action:
				trgt_vf_in = torch.cat((next_obs[agent_i], one_hot(curr_agent.target_policy(next_obs[agent_i]))), dim=1)
			else:
				trgt_vf_in = torch.cat((next_obs[agent_i], curr_agent.target_policy(next_obs[agent_i])), dim=1)
		target_value = (rews[agent_i].view(-1, 1) + self.gamma * curr_agent.target_critic(trgt_vf_in) * (1 - dones[agent_i].view(-1, 1)))

		if self.alg_types[agent_i] == 'MADDPG':
			vf_in = torch.cat((*obs, *acs), dim=1)
		else:  # DDPG
			vf_in = torch.cat((obs[agent_i], acs[agent_i]), dim=1)
		actual_value = curr_agent.critic(vf_in)
		vf_loss = MSELoss(actual_value, target_value.detach())
		vf_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.critic.parameters(), 0.5)
		curr_agent.critic_optimizer.step()
		curr_agent.policy_optimizer.zero_grad()

		if self.discrete_action:
			curr_pol_out = curr_agent.policy(obs[agent_i])
			curr_pol_vf_in = gumbel_softmax(curr_pol_out, hard=True)
		else:
			curr_pol_out = curr_agent.policy(obs[agent_i])
			curr_pol_vf_in = curr_pol_out
		if self.alg_types[agent_i] == 'MADDPG':
			all_pol_acs = []
			for i, pi, ob in zip(range(self.nagents), self.policies, obs):
				if i == agent_i:
					all_pol_acs.append(curr_pol_vf_in)
				elif self.discrete_action:
					all_pol_acs.append(one_hot(pi(ob)))
				else:
					all_pol_acs.append(pi(ob))
			vf_in = torch.cat((*obs, *all_pol_acs), dim=1)
		else:  # DDPG
			vf_in = torch.cat((obs[agent_i], curr_pol_vf_in), dim=1)
		pol_loss = -curr_agent.critic(vf_in).mean()
		pol_loss += (curr_pol_out**2).mean() * 1e-3
		pol_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.policy.parameters(), 0.5)
		curr_agent.policy_optimizer.step()
		if logger is not None:
			logger.add_scalars('agent%i/losses' % agent_i, {'vf_loss': vf_loss, 'pol_loss': pol_loss}, self.niter)

	def update_all_targets(self):
		for a in self.agents:
			# soft_update(a.target_critic, a.critic, self.tau)
			# soft_update(a.target_policy, a.policy, self.tau)
			for target_param, param in zip(a.target_critic.parameters(), a.critic.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
			for target_param, param in zip(a.target_policy.parameters(), a.policy.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
		self.niter += 1

	def prep_training(self, device='gpu'):
		for a in self.agents:
			a.policy.train()
			a.critic.train()
			a.target_policy.train()
			a.target_critic.train()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device
		if not self.critic_dev == device:
			for a in self.agents:
				a.critic = fn(a.critic)
			self.critic_dev = device
		if not self.trgt_pol_dev == device:
			for a in self.agents:
				a.target_policy = fn(a.target_policy)
			self.trgt_pol_dev = device
		if not self.trgt_critic_dev == device:
			for a in self.agents:
				a.target_critic = fn(a.target_critic)
			self.trgt_critic_dev = device

	def prep_rollouts(self, device='cpu'):
		for a in self.agents:
			a.policy.eval()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		# only need main policy for rollouts
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device

	def save(self, filename):
		self.prep_training(device='cpu')  # move parameters to CPU before saving
		save_dict = {'init_dict': self.init_dict, 'agent_params': [a.get_params() for a in self.agents]}
		torch.save(save_dict, filename)

	@classmethod
	def init_from_env(cls, env, agent_alg="MADDPG", adversary_alg="MADDPG", gamma=0.95, tau=0.01, lr=0.01, hidden_dim=64):
		agent_init_params = []
		alg_types = [adversary_alg if atype == 'adversary' else agent_alg for atype in env.agent_types]
		for acsp, obsp, algtype in zip(env.action_space, env.observation_space, alg_types):
			num_in_pol = obsp.shape[0]
			if isinstance(acsp, Box):
				discrete_action = False
				get_shape = lambda x: x.shape[0]
			else:  # Discrete
				discrete_action = True
				get_shape = lambda x: x.n
			num_out_pol = get_shape(acsp)
			if algtype == "MADDPG":
				num_in_critic = 0
				for oobsp in env.observation_space:
					num_in_critic += oobsp.shape[0]
				for oacsp in env.action_space:
					num_in_critic += get_shape(oacsp)
			else:
				num_in_critic = obsp.shape[0] + get_shape(acsp)
			agent_init_params.append({'num_in_pol': num_in_pol, 'num_out_pol': num_out_pol, 'num_in_critic': num_in_critic})
		init_dict = {'gamma': gamma, 'tau': tau, 'lr': lr, 'hidden_dim': hidden_dim, 'alg_types': alg_types, 'agent_init_params': agent_init_params, 'discrete_action': discrete_action}
		instance = cls(**init_dict)
		instance.init_dict = init_dict
		return instance

	@classmethod
	def init_from_save(cls, filename):
		"""
		Instantiate instance of this class from file created by 'save' method
		"""
		save_dict = torch.load(filename)
		instance = cls(**save_dict['init_dict'])
		instance.init_dict = save_dict['init_dict']
		for a, params in zip(instance.agents, save_dict['agent_params']):
			a.load_params(params)
		return instance

class DDPGAgent(object):
	def __init__(self, num_in_pol, num_out_pol, num_in_critic, hidden_dim=64, lr=0.01, discrete_action=True):
		self.policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim,constrain_out=True,discrete_action=discrete_action)
		self.critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim,constrain_out=False)
		self.target_policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim,constrain_out=True,discrete_action=discrete_action)
		self.target_critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim,constrain_out=False)
		# hard_update(self.target_policy, self.policy)
		# hard_update(self.target_critic, self.critic)
		for target_param, param in zip(self.target_policy.parameters(), self.policy.parameters()):
			target_param.data.copy_(param.data)
		for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
			target_param.data.copy_(param.data)
		self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
		self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)
		self.exploration = 0.3  # epsilon for eps-greedy
		self.discrete_action = discrete_action

	def reset_noise(self):
		if not self.discrete_action:
			self.exploration.reset()

	def scale_noise(self, scale):
		if self.discrete_action:
			self.exploration = scale
		else:
			self.exploration.scale = scale

	def step(self, obs, explore=False):
		action = self.policy(obs)
		if explore:
			action = gumbel_softmax(action, hard=True)
		else:
			action = one_hot(action)
		return action

class MLPNetwork(torch.nn.Module):
	def __init__(self, input_dim, out_dim, hidden_dim=64, nonlin=torch.relu, constrain_out=False, norm_in=False, discrete_action=True):
		super(MLPNetwork, self).__init__()

		if norm_in:  # normalize inputs
			self.in_fn = nn.BatchNorm1d(input_dim)
			self.in_fn.weight.data.fill_(1)
			self.in_fn.bias.data.fill_(0)
		else:
			self.in_fn = lambda x: x
		self.fc1 = torch.nn.Linear(input_dim, hidden_dim)
		self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
		self.fc3 = torch.nn.Linear(hidden_dim, out_dim)
		self.nonlin = nonlin
		if constrain_out and not discrete_action:
			# initialize small to prevent saturation
			self.fc3.weight.data.uniform_(-3e-3, 3e-3)
			self.out_fn = torch.tanh
		else:  # logits for discrete action (will softmax later)
			self.out_fn = lambda x: x

	def forward(self, X):
		h1 = self.nonlin(self.fc1(self.in_fn(X)))
		h2 = self.nonlin(self.fc2(h1))
		out = self.out_fn(self.fc3(h2))
		return out
REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-833.424 -833.424 -833.424] [0.0000], Avg: [-833.424 -833.424 -833.424] (1.000)
Step: 99, Reward: [-645.856 -645.856 -645.856] [0.0000], Avg: [-739.64 -739.64 -739.64] (1.000)
Step: 149, Reward: [-497.581 -497.581 -497.581] [0.0000], Avg: [-658.954 -658.954 -658.954] (1.000)
Step: 199, Reward: [-382.008 -382.008 -382.008] [0.0000], Avg: [-589.717 -589.717 -589.717] (1.000)
Step: 249, Reward: [-620.972 -620.972 -620.972] [0.0000], Avg: [-595.968 -595.968 -595.968] (1.000)
Step: 299, Reward: [-956.899 -956.899 -956.899] [0.0000], Avg: [-656.123 -656.123 -656.123] (1.000)
Step: 349, Reward: [-1388.006 -1388.006 -1388.006] [0.0000], Avg: [-760.678 -760.678 -760.678] (1.000)
Step: 399, Reward: [-1221.632 -1221.632 -1221.632] [0.0000], Avg: [-818.297 -818.297 -818.297] (1.000)
Step: 449, Reward: [-1784.532 -1784.532 -1784.532] [0.0000], Avg: [-925.657 -925.657 -925.657] (1.000)
Step: 499, Reward: [-1975.791 -1975.791 -1975.791] [0.0000], Avg: [-1030.67 -1030.67 -1030.67] (1.000)
Step: 549, Reward: [-1151.097 -1151.097 -1151.097] [0.0000], Avg: [-1041.618 -1041.618 -1041.618] (1.000)
Step: 599, Reward: [-1325.027 -1325.027 -1325.027] [0.0000], Avg: [-1065.235 -1065.235 -1065.235] (1.000)
Step: 649, Reward: [-638.82 -638.82 -638.82] [0.0000], Avg: [-1032.434 -1032.434 -1032.434] (1.000)
Step: 699, Reward: [-328.253 -328.253 -328.253] [0.0000], Avg: [-982.136 -982.136 -982.136] (1.000)
Step: 749, Reward: [-593.049 -593.049 -593.049] [0.0000], Avg: [-956.196 -956.196 -956.196] (1.000)
Step: 799, Reward: [-588.832 -588.832 -588.832] [0.0000], Avg: [-933.236 -933.236 -933.236] (1.000)
Step: 849, Reward: [-536.042 -536.042 -536.042] [0.0000], Avg: [-909.872 -909.872 -909.872] (1.000)
Step: 899, Reward: [-636.611 -636.611 -636.611] [0.0000], Avg: [-894.691 -894.691 -894.691] (1.000)
Step: 949, Reward: [-574.505 -574.505 -574.505] [0.0000], Avg: [-877.839 -877.839 -877.839] (1.000)
Step: 999, Reward: [-736.792 -736.792 -736.792] [0.0000], Avg: [-870.786 -870.786 -870.786] (1.000)
Step: 1049, Reward: [-521.812 -521.812 -521.812] [0.0000], Avg: [-854.169 -854.169 -854.169] (1.000)
Step: 1099, Reward: [-721.202 -721.202 -721.202] [0.0000], Avg: [-848.125 -848.125 -848.125] (1.000)
Step: 1149, Reward: [-771.082 -771.082 -771.082] [0.0000], Avg: [-844.775 -844.775 -844.775] (1.000)
Step: 1199, Reward: [-522.75 -522.75 -522.75] [0.0000], Avg: [-831.357 -831.357 -831.357] (1.000)
Step: 1249, Reward: [-773.593 -773.593 -773.593] [0.0000], Avg: [-829.047 -829.047 -829.047] (1.000)
Step: 1299, Reward: [-679.847 -679.847 -679.847] [0.0000], Avg: [-823.308 -823.308 -823.308] (1.000)
Step: 1349, Reward: [-982.115 -982.115 -982.115] [0.0000], Avg: [-829.19 -829.19 -829.19] (1.000)
Step: 1399, Reward: [-663.688 -663.688 -663.688] [0.0000], Avg: [-823.279 -823.279 -823.279] (1.000)
Step: 1449, Reward: [-514.353 -514.353 -514.353] [0.0000], Avg: [-812.627 -812.627 -812.627] (1.000)
Step: 1499, Reward: [-613.446 -613.446 -613.446] [0.0000], Avg: [-805.987 -805.987 -805.987] (1.000)
Step: 1549, Reward: [-517.211 -517.211 -517.211] [0.0000], Avg: [-796.672 -796.672 -796.672] (1.000)
Step: 1599, Reward: [-439.123 -439.123 -439.123] [0.0000], Avg: [-785.498 -785.498 -785.498] (1.000)
Step: 1649, Reward: [-843.321 -843.321 -843.321] [0.0000], Avg: [-787.251 -787.251 -787.251] (1.000)
Step: 1699, Reward: [-764.124 -764.124 -764.124] [0.0000], Avg: [-786.57 -786.57 -786.57] (1.000)
Step: 1749, Reward: [-593.641 -593.641 -593.641] [0.0000], Avg: [-781.058 -781.058 -781.058] (1.000)
Step: 1799, Reward: [-638.042 -638.042 -638.042] [0.0000], Avg: [-777.085 -777.085 -777.085] (1.000)
Step: 1849, Reward: [-301.773 -301.773 -301.773] [0.0000], Avg: [-764.239 -764.239 -764.239] (1.000)
Step: 1899, Reward: [-574.872 -574.872 -574.872] [0.0000], Avg: [-759.256 -759.256 -759.256] (1.000)
Step: 1949, Reward: [-661.603 -661.603 -661.603] [0.0000], Avg: [-756.752 -756.752 -756.752] (1.000)
Step: 1999, Reward: [-428.795 -428.795 -428.795] [0.0000], Avg: [-748.553 -748.553 -748.553] (1.000)
Step: 2049, Reward: [-534.842 -534.842 -534.842] [0.0000], Avg: [-743.341 -743.341 -743.341] (1.000)
Step: 2099, Reward: [-768.221 -768.221 -768.221] [0.0000], Avg: [-743.933 -743.933 -743.933] (1.000)
Step: 2149, Reward: [-601.207 -601.207 -601.207] [0.0000], Avg: [-740.614 -740.614 -740.614] (1.000)
Step: 2199, Reward: [-587.316 -587.316 -587.316] [0.0000], Avg: [-737.13 -737.13 -737.13] (1.000)
Step: 2249, Reward: [-652.627 -652.627 -652.627] [0.0000], Avg: [-735.252 -735.252 -735.252] (1.000)
Step: 2299, Reward: [-535.785 -535.785 -535.785] [0.0000], Avg: [-730.916 -730.916 -730.916] (1.000)
Step: 2349, Reward: [-521.192 -521.192 -521.192] [0.0000], Avg: [-726.453 -726.453 -726.453] (1.000)
Step: 2399, Reward: [-555.558 -555.558 -555.558] [0.0000], Avg: [-722.893 -722.893 -722.893] (1.000)
Step: 2449, Reward: [-515.237 -515.237 -515.237] [0.0000], Avg: [-718.655 -718.655 -718.655] (1.000)
Step: 2499, Reward: [-608.087 -608.087 -608.087] [0.0000], Avg: [-716.444 -716.444 -716.444] (1.000)
Step: 2549, Reward: [-590.975 -590.975 -590.975] [0.0000], Avg: [-713.984 -713.984 -713.984] (1.000)
Step: 2599, Reward: [-467.699 -467.699 -467.699] [0.0000], Avg: [-709.247 -709.247 -709.247] (1.000)
Step: 2649, Reward: [-499.354 -499.354 -499.354] [0.0000], Avg: [-705.287 -705.287 -705.287] (1.000)
Step: 2699, Reward: [-359.288 -359.288 -359.288] [0.0000], Avg: [-698.88 -698.88 -698.88] (1.000)
Step: 2749, Reward: [-389.66 -389.66 -389.66] [0.0000], Avg: [-693.258 -693.258 -693.258] (1.000)
Step: 2799, Reward: [-504.934 -504.934 -504.934] [0.0000], Avg: [-689.895 -689.895 -689.895] (1.000)
Step: 2849, Reward: [-640.724 -640.724 -640.724] [0.0000], Avg: [-689.032 -689.032 -689.032] (1.000)
Step: 2899, Reward: [-754.785 -754.785 -754.785] [0.0000], Avg: [-690.166 -690.166 -690.166] (1.000)
Step: 2949, Reward: [-718.274 -718.274 -718.274] [0.0000], Avg: [-690.642 -690.642 -690.642] (1.000)
Step: 2999, Reward: [-326.217 -326.217 -326.217] [0.0000], Avg: [-684.568 -684.568 -684.568] (1.000)
Step: 3049, Reward: [-479.37 -479.37 -479.37] [0.0000], Avg: [-681.204 -681.204 -681.204] (1.000)
Step: 3099, Reward: [-552.214 -552.214 -552.214] [0.0000], Avg: [-679.124 -679.124 -679.124] (1.000)
Step: 3149, Reward: [-917.875 -917.875 -917.875] [0.0000], Avg: [-682.914 -682.914 -682.914] (1.000)
Step: 3199, Reward: [-586.485 -586.485 -586.485] [0.0000], Avg: [-681.407 -681.407 -681.407] (1.000)
Step: 3249, Reward: [-556.925 -556.925 -556.925] [0.0000], Avg: [-679.492 -679.492 -679.492] (1.000)
Step: 3299, Reward: [-428.129 -428.129 -428.129] [0.0000], Avg: [-675.683 -675.683 -675.683] (1.000)
Step: 3349, Reward: [-752.748 -752.748 -752.748] [0.0000], Avg: [-676.834 -676.834 -676.834] (1.000)
Step: 3399, Reward: [-598.107 -598.107 -598.107] [0.0000], Avg: [-675.676 -675.676 -675.676] (1.000)
Step: 3449, Reward: [-550.771 -550.771 -550.771] [0.0000], Avg: [-673.866 -673.866 -673.866] (1.000)
Step: 3499, Reward: [-806.821 -806.821 -806.821] [0.0000], Avg: [-675.765 -675.765 -675.765] (1.000)
Step: 3549, Reward: [-660.835 -660.835 -660.835] [0.0000], Avg: [-675.555 -675.555 -675.555] (1.000)
Step: 3599, Reward: [-632.002 -632.002 -632.002] [0.0000], Avg: [-674.95 -674.95 -674.95] (1.000)
Step: 3649, Reward: [-907.961 -907.961 -907.961] [0.0000], Avg: [-678.142 -678.142 -678.142] (1.000)
Step: 3699, Reward: [-534.987 -534.987 -534.987] [0.0000], Avg: [-676.207 -676.207 -676.207] (1.000)
Step: 3749, Reward: [-735.74 -735.74 -735.74] [0.0000], Avg: [-677.001 -677.001 -677.001] (1.000)
Step: 3799, Reward: [-546.757 -546.757 -546.757] [0.0000], Avg: [-675.287 -675.287 -675.287] (1.000)
Step: 3849, Reward: [-770.058 -770.058 -770.058] [0.0000], Avg: [-676.518 -676.518 -676.518] (1.000)
Step: 3899, Reward: [-599.843 -599.843 -599.843] [0.0000], Avg: [-675.535 -675.535 -675.535] (1.000)
Step: 3949, Reward: [-832.771 -832.771 -832.771] [0.0000], Avg: [-677.525 -677.525 -677.525] (1.000)
Step: 3999, Reward: [-738.983 -738.983 -738.983] [0.0000], Avg: [-678.294 -678.294 -678.294] (1.000)
Step: 4049, Reward: [-827.793 -827.793 -827.793] [0.0000], Avg: [-680.139 -680.139 -680.139] (1.000)
Step: 4099, Reward: [-801.6 -801.6 -801.6] [0.0000], Avg: [-681.62 -681.62 -681.62] (1.000)
Step: 4149, Reward: [-456.954 -456.954 -456.954] [0.0000], Avg: [-678.914 -678.914 -678.914] (1.000)
Step: 4199, Reward: [-614.556 -614.556 -614.556] [0.0000], Avg: [-678.147 -678.147 -678.147] (1.000)
Step: 4249, Reward: [-549.95 -549.95 -549.95] [0.0000], Avg: [-676.639 -676.639 -676.639] (1.000)
Step: 4299, Reward: [-615.231 -615.231 -615.231] [0.0000], Avg: [-675.925 -675.925 -675.925] (1.000)
Step: 4349, Reward: [-682.193 -682.193 -682.193] [0.0000], Avg: [-675.997 -675.997 -675.997] (1.000)
Step: 4399, Reward: [-671.787 -671.787 -671.787] [0.0000], Avg: [-675.949 -675.949 -675.949] (1.000)
Step: 4449, Reward: [-311.185 -311.185 -311.185] [0.0000], Avg: [-671.851 -671.851 -671.851] (1.000)
Step: 4499, Reward: [-378.098 -378.098 -378.098] [0.0000], Avg: [-668.587 -668.587 -668.587] (1.000)
Step: 4549, Reward: [-570.232 -570.232 -570.232] [0.0000], Avg: [-667.506 -667.506 -667.506] (1.000)
Step: 4599, Reward: [-379.403 -379.403 -379.403] [0.0000], Avg: [-664.375 -664.375 -664.375] (1.000)
Step: 4649, Reward: [-674.864 -674.864 -674.864] [0.0000], Avg: [-664.487 -664.487 -664.487] (1.000)
Step: 4699, Reward: [-646.774 -646.774 -646.774] [0.0000], Avg: [-664.299 -664.299 -664.299] (1.000)
Step: 4749, Reward: [-658.238 -658.238 -658.238] [0.0000], Avg: [-664.235 -664.235 -664.235] (1.000)
Step: 4799, Reward: [-673.403 -673.403 -673.403] [0.0000], Avg: [-664.331 -664.331 -664.331] (1.000)
Step: 4849, Reward: [-593.011 -593.011 -593.011] [0.0000], Avg: [-663.595 -663.595 -663.595] (1.000)
Step: 4899, Reward: [-674.839 -674.839 -674.839] [0.0000], Avg: [-663.71 -663.71 -663.71] (1.000)
Step: 4949, Reward: [-536.891 -536.891 -536.891] [0.0000], Avg: [-662.429 -662.429 -662.429] (1.000)
Step: 4999, Reward: [-483.855 -483.855 -483.855] [0.0000], Avg: [-660.643 -660.643 -660.643] (1.000)
Step: 5049, Reward: [-465.255 -465.255 -465.255] [0.0000], Avg: [-658.709 -658.709 -658.709] (1.000)
Step: 5099, Reward: [-705.944 -705.944 -705.944] [0.0000], Avg: [-659.172 -659.172 -659.172] (1.000)
Step: 5149, Reward: [-720.021 -720.021 -720.021] [0.0000], Avg: [-659.763 -659.763 -659.763] (1.000)
Step: 5199, Reward: [-496.766 -496.766 -496.766] [0.0000], Avg: [-658.195 -658.195 -658.195] (1.000)
Step: 5249, Reward: [-770.904 -770.904 -770.904] [0.0000], Avg: [-659.269 -659.269 -659.269] (1.000)
Step: 5299, Reward: [-541.53 -541.53 -541.53] [0.0000], Avg: [-658.158 -658.158 -658.158] (1.000)
Step: 5349, Reward: [-614.019 -614.019 -614.019] [0.0000], Avg: [-657.746 -657.746 -657.746] (1.000)
Step: 5399, Reward: [-439.358 -439.358 -439.358] [0.0000], Avg: [-655.724 -655.724 -655.724] (1.000)
Step: 5449, Reward: [-499.432 -499.432 -499.432] [0.0000], Avg: [-654.29 -654.29 -654.29] (1.000)
Step: 5499, Reward: [-660.035 -660.035 -660.035] [0.0000], Avg: [-654.342 -654.342 -654.342] (1.000)
Step: 5549, Reward: [-693.608 -693.608 -693.608] [0.0000], Avg: [-654.696 -654.696 -654.696] (1.000)
Step: 5599, Reward: [-465.13 -465.13 -465.13] [0.0000], Avg: [-653.003 -653.003 -653.003] (1.000)
Step: 5649, Reward: [-549.898 -549.898 -549.898] [0.0000], Avg: [-652.091 -652.091 -652.091] (1.000)
Step: 5699, Reward: [-568.35 -568.35 -568.35] [0.0000], Avg: [-651.356 -651.356 -651.356] (1.000)
Step: 5749, Reward: [-571.441 -571.441 -571.441] [0.0000], Avg: [-650.661 -650.661 -650.661] (1.000)
Step: 5799, Reward: [-577.197 -577.197 -577.197] [0.0000], Avg: [-650.028 -650.028 -650.028] (1.000)
Step: 5849, Reward: [-834.424 -834.424 -834.424] [0.0000], Avg: [-651.604 -651.604 -651.604] (1.000)
Step: 5899, Reward: [-865.313 -865.313 -865.313] [0.0000], Avg: [-653.415 -653.415 -653.415] (1.000)
Step: 5949, Reward: [-588.056 -588.056 -588.056] [0.0000], Avg: [-652.866 -652.866 -652.866] (1.000)
Step: 5999, Reward: [-278.617 -278.617 -278.617] [0.0000], Avg: [-649.747 -649.747 -649.747] (1.000)
Step: 6049, Reward: [-940.266 -940.266 -940.266] [0.0000], Avg: [-652.148 -652.148 -652.148] (1.000)
Step: 6099, Reward: [-629.745 -629.745 -629.745] [0.0000], Avg: [-651.964 -651.964 -651.964] (1.000)
Step: 6149, Reward: [-387.095 -387.095 -387.095] [0.0000], Avg: [-649.811 -649.811 -649.811] (1.000)
Step: 6199, Reward: [-766.014 -766.014 -766.014] [0.0000], Avg: [-650.748 -650.748 -650.748] (1.000)
Step: 6249, Reward: [-609.431 -609.431 -609.431] [0.0000], Avg: [-650.418 -650.418 -650.418] (1.000)
Step: 6299, Reward: [-712.095 -712.095 -712.095] [0.0000], Avg: [-650.907 -650.907 -650.907] (1.000)
Step: 6349, Reward: [-399.935 -399.935 -399.935] [0.0000], Avg: [-648.931 -648.931 -648.931] (1.000)
Step: 6399, Reward: [-498.921 -498.921 -498.921] [0.0000], Avg: [-647.759 -647.759 -647.759] (1.000)
Step: 6449, Reward: [-534.847 -534.847 -534.847] [0.0000], Avg: [-646.884 -646.884 -646.884] (1.000)
Step: 6499, Reward: [-588.45 -588.45 -588.45] [0.0000], Avg: [-646.434 -646.434 -646.434] (1.000)
Step: 6549, Reward: [-617.648 -617.648 -617.648] [0.0000], Avg: [-646.214 -646.214 -646.214] (1.000)
Step: 6599, Reward: [-409.044 -409.044 -409.044] [0.0000], Avg: [-644.418 -644.418 -644.418] (1.000)
Step: 6649, Reward: [-580.309 -580.309 -580.309] [0.0000], Avg: [-643.936 -643.936 -643.936] (1.000)
Step: 6699, Reward: [-622.337 -622.337 -622.337] [0.0000], Avg: [-643.774 -643.774 -643.774] (1.000)
Step: 6749, Reward: [-1056.755 -1056.755 -1056.755] [0.0000], Avg: [-646.834 -646.834 -646.834] (1.000)
Step: 6799, Reward: [-510.837 -510.837 -510.837] [0.0000], Avg: [-645.834 -645.834 -645.834] (1.000)
Step: 6849, Reward: [-366.775 -366.775 -366.775] [0.0000], Avg: [-643.797 -643.797 -643.797] (1.000)
Step: 6899, Reward: [-509.732 -509.732 -509.732] [0.0000], Avg: [-642.825 -642.825 -642.825] (1.000)
Step: 6949, Reward: [-415.051 -415.051 -415.051] [0.0000], Avg: [-641.187 -641.187 -641.187] (1.000)
Step: 6999, Reward: [-548.418 -548.418 -548.418] [0.0000], Avg: [-640.524 -640.524 -640.524] (1.000)
Step: 7049, Reward: [-743.849 -743.849 -743.849] [0.0000], Avg: [-641.257 -641.257 -641.257] (1.000)
Step: 7099, Reward: [-468.034 -468.034 -468.034] [0.0000], Avg: [-640.037 -640.037 -640.037] (1.000)
Step: 7149, Reward: [-567.069 -567.069 -567.069] [0.0000], Avg: [-639.527 -639.527 -639.527] (1.000)
Step: 7199, Reward: [-298.289 -298.289 -298.289] [0.0000], Avg: [-637.157 -637.157 -637.157] (1.000)
Step: 7249, Reward: [-822.152 -822.152 -822.152] [0.0000], Avg: [-638.433 -638.433 -638.433] (1.000)
Step: 7299, Reward: [-665.167 -665.167 -665.167] [0.0000], Avg: [-638.616 -638.616 -638.616] (1.000)
Step: 7349, Reward: [-596.532 -596.532 -596.532] [0.0000], Avg: [-638.33 -638.33 -638.33] (1.000)
Step: 7399, Reward: [-603.637 -603.637 -603.637] [0.0000], Avg: [-638.095 -638.095 -638.095] (1.000)
Step: 7449, Reward: [-530.97 -530.97 -530.97] [0.0000], Avg: [-637.376 -637.376 -637.376] (1.000)
Step: 7499, Reward: [-784.905 -784.905 -784.905] [0.0000], Avg: [-638.36 -638.36 -638.36] (1.000)
Step: 7549, Reward: [-705.523 -705.523 -705.523] [0.0000], Avg: [-638.804 -638.804 -638.804] (1.000)
Step: 7599, Reward: [-560.487 -560.487 -560.487] [0.0000], Avg: [-638.289 -638.289 -638.289] (1.000)
Step: 7649, Reward: [-480.053 -480.053 -480.053] [0.0000], Avg: [-637.255 -637.255 -637.255] (1.000)
Step: 7699, Reward: [-582.935 -582.935 -582.935] [0.0000], Avg: [-636.902 -636.902 -636.902] (1.000)
Step: 7749, Reward: [-685.188 -685.188 -685.188] [0.0000], Avg: [-637.214 -637.214 -637.214] (1.000)
Step: 7799, Reward: [-480.688 -480.688 -480.688] [0.0000], Avg: [-636.21 -636.21 -636.21] (1.000)
Step: 7849, Reward: [-537.065 -537.065 -537.065] [0.0000], Avg: [-635.579 -635.579 -635.579] (1.000)
Step: 7899, Reward: [-709.365 -709.365 -709.365] [0.0000], Avg: [-636.046 -636.046 -636.046] (1.000)
Step: 7949, Reward: [-848.194 -848.194 -848.194] [0.0000], Avg: [-637.38 -637.38 -637.38] (1.000)
Step: 7999, Reward: [-692.151 -692.151 -692.151] [0.0000], Avg: [-637.722 -637.722 -637.722] (1.000)
Step: 8049, Reward: [-715.451 -715.451 -715.451] [0.0000], Avg: [-638.205 -638.205 -638.205] (1.000)
Step: 8099, Reward: [-580.882 -580.882 -580.882] [0.0000], Avg: [-637.851 -637.851 -637.851] (1.000)
Step: 8149, Reward: [-677.994 -677.994 -677.994] [0.0000], Avg: [-638.098 -638.098 -638.098] (1.000)
Step: 8199, Reward: [-710.595 -710.595 -710.595] [0.0000], Avg: [-638.54 -638.54 -638.54] (1.000)
Step: 8249, Reward: [-413.911 -413.911 -413.911] [0.0000], Avg: [-637.178 -637.178 -637.178] (1.000)
Step: 8299, Reward: [-628.24 -628.24 -628.24] [0.0000], Avg: [-637.125 -637.125 -637.125] (1.000)
Step: 8349, Reward: [-868.594 -868.594 -868.594] [0.0000], Avg: [-638.511 -638.511 -638.511] (1.000)
Step: 8399, Reward: [-655.854 -655.854 -655.854] [0.0000], Avg: [-638.614 -638.614 -638.614] (1.000)
Step: 8449, Reward: [-478.677 -478.677 -478.677] [0.0000], Avg: [-637.667 -637.667 -637.667] (1.000)
Step: 8499, Reward: [-503.993 -503.993 -503.993] [0.0000], Avg: [-636.881 -636.881 -636.881] (1.000)
Step: 8549, Reward: [-346.403 -346.403 -346.403] [0.0000], Avg: [-635.182 -635.182 -635.182] (1.000)
Step: 8599, Reward: [-448.613 -448.613 -448.613] [0.0000], Avg: [-634.098 -634.098 -634.098] (1.000)
Step: 8649, Reward: [-569.088 -569.088 -569.088] [0.0000], Avg: [-633.722 -633.722 -633.722] (1.000)
Step: 8699, Reward: [-682.074 -682.074 -682.074] [0.0000], Avg: [-634. -634. -634.] (1.000)
Step: 8749, Reward: [-517.597 -517.597 -517.597] [0.0000], Avg: [-633.335 -633.335 -633.335] (1.000)
Step: 8799, Reward: [-645.206 -645.206 -645.206] [0.0000], Avg: [-633.402 -633.402 -633.402] (1.000)
Step: 8849, Reward: [-624.487 -624.487 -624.487] [0.0000], Avg: [-633.352 -633.352 -633.352] (1.000)
Step: 8899, Reward: [-629.577 -629.577 -629.577] [0.0000], Avg: [-633.331 -633.331 -633.331] (1.000)
Step: 8949, Reward: [-650.674 -650.674 -650.674] [0.0000], Avg: [-633.427 -633.427 -633.427] (1.000)
Step: 8999, Reward: [-557.855 -557.855 -557.855] [0.0000], Avg: [-633.008 -633.008 -633.008] (1.000)
Step: 9049, Reward: [-654.346 -654.346 -654.346] [0.0000], Avg: [-633.125 -633.125 -633.125] (1.000)
Step: 9099, Reward: [-783.596 -783.596 -783.596] [0.0000], Avg: [-633.952 -633.952 -633.952] (1.000)
Step: 9149, Reward: [-893.227 -893.227 -893.227] [0.0000], Avg: [-635.369 -635.369 -635.369] (1.000)
Step: 9199, Reward: [-612.034 -612.034 -612.034] [0.0000], Avg: [-635.242 -635.242 -635.242] (1.000)
Step: 9249, Reward: [-563.183 -563.183 -563.183] [0.0000], Avg: [-634.853 -634.853 -634.853] (1.000)
Step: 9299, Reward: [-712.026 -712.026 -712.026] [0.0000], Avg: [-635.268 -635.268 -635.268] (1.000)
Step: 9349, Reward: [-828.883 -828.883 -828.883] [0.0000], Avg: [-636.303 -636.303 -636.303] (1.000)
Step: 9399, Reward: [-686.681 -686.681 -686.681] [0.0000], Avg: [-636.571 -636.571 -636.571] (1.000)
Step: 9449, Reward: [-474.018 -474.018 -474.018] [0.0000], Avg: [-635.711 -635.711 -635.711] (1.000)
Step: 9499, Reward: [-464.246 -464.246 -464.246] [0.0000], Avg: [-634.808 -634.808 -634.808] (1.000)
Step: 9549, Reward: [-380.181 -380.181 -380.181] [0.0000], Avg: [-633.475 -633.475 -633.475] (1.000)
Step: 9599, Reward: [-597.385 -597.385 -597.385] [0.0000], Avg: [-633.287 -633.287 -633.287] (1.000)
Step: 9649, Reward: [-767.064 -767.064 -767.064] [0.0000], Avg: [-633.98 -633.98 -633.98] (1.000)
Step: 9699, Reward: [-867.057 -867.057 -867.057] [0.0000], Avg: [-635.182 -635.182 -635.182] (1.000)
Step: 9749, Reward: [-613.11 -613.11 -613.11] [0.0000], Avg: [-635.069 -635.069 -635.069] (1.000)
Step: 9799, Reward: [-688.941 -688.941 -688.941] [0.0000], Avg: [-635.344 -635.344 -635.344] (1.000)
Step: 9849, Reward: [-506. -506. -506.] [0.0000], Avg: [-634.687 -634.687 -634.687] (1.000)
Step: 9899, Reward: [-670.109 -670.109 -670.109] [0.0000], Avg: [-634.866 -634.866 -634.866] (1.000)
Step: 9949, Reward: [-529.011 -529.011 -529.011] [0.0000], Avg: [-634.334 -634.334 -634.334] (1.000)
Step: 9999, Reward: [-518.124 -518.124 -518.124] [0.0000], Avg: [-633.753 -633.753 -633.753] (1.000)
Step: 10049, Reward: [-480.212 -480.212 -480.212] [0.0000], Avg: [-632.989 -632.989 -632.989] (1.000)
Step: 10099, Reward: [-516.811 -516.811 -516.811] [0.0000], Avg: [-632.414 -632.414 -632.414] (1.000)
Step: 10149, Reward: [-426.418 -426.418 -426.418] [0.0000], Avg: [-631.399 -631.399 -631.399] (1.000)
Step: 10199, Reward: [-515.125 -515.125 -515.125] [0.0000], Avg: [-630.829 -630.829 -630.829] (1.000)
Step: 10249, Reward: [-439.823 -439.823 -439.823] [0.0000], Avg: [-629.897 -629.897 -629.897] (1.000)
Step: 10299, Reward: [-496.063 -496.063 -496.063] [0.0000], Avg: [-629.248 -629.248 -629.248] (1.000)
Step: 10349, Reward: [-329.412 -329.412 -329.412] [0.0000], Avg: [-627.799 -627.799 -627.799] (1.000)
Step: 10399, Reward: [-661.031 -661.031 -661.031] [0.0000], Avg: [-627.959 -627.959 -627.959] (1.000)
Step: 10449, Reward: [-437.076 -437.076 -437.076] [0.0000], Avg: [-627.046 -627.046 -627.046] (1.000)
Step: 10499, Reward: [-687.455 -687.455 -687.455] [0.0000], Avg: [-627.333 -627.333 -627.333] (1.000)
Step: 10549, Reward: [-527.655 -527.655 -527.655] [0.0000], Avg: [-626.861 -626.861 -626.861] (1.000)
Step: 10599, Reward: [-581.126 -581.126 -581.126] [0.0000], Avg: [-626.645 -626.645 -626.645] (1.000)
Step: 10649, Reward: [-371.388 -371.388 -371.388] [0.0000], Avg: [-625.447 -625.447 -625.447] (1.000)
Step: 10699, Reward: [-494.663 -494.663 -494.663] [0.0000], Avg: [-624.836 -624.836 -624.836] (1.000)
Step: 10749, Reward: [-569.589 -569.589 -569.589] [0.0000], Avg: [-624.579 -624.579 -624.579] (1.000)
Step: 10799, Reward: [-520.94 -520.94 -520.94] [0.0000], Avg: [-624.099 -624.099 -624.099] (1.000)
Step: 10849, Reward: [-851.29 -851.29 -851.29] [0.0000], Avg: [-625.146 -625.146 -625.146] (1.000)
Step: 10899, Reward: [-499.668 -499.668 -499.668] [0.0000], Avg: [-624.57 -624.57 -624.57] (1.000)
Step: 10949, Reward: [-493.018 -493.018 -493.018] [0.0000], Avg: [-623.97 -623.97 -623.97] (1.000)
Step: 10999, Reward: [-642.635 -642.635 -642.635] [0.0000], Avg: [-624.054 -624.054 -624.054] (1.000)
Step: 11049, Reward: [-575.403 -575.403 -575.403] [0.0000], Avg: [-623.834 -623.834 -623.834] (1.000)
Step: 11099, Reward: [-610.586 -610.586 -610.586] [0.0000], Avg: [-623.775 -623.775 -623.775] (1.000)
Step: 11149, Reward: [-306.357 -306.357 -306.357] [0.0000], Avg: [-622.351 -622.351 -622.351] (1.000)
Step: 11199, Reward: [-713.743 -713.743 -713.743] [0.0000], Avg: [-622.759 -622.759 -622.759] (1.000)
Step: 11249, Reward: [-630.759 -630.759 -630.759] [0.0000], Avg: [-622.795 -622.795 -622.795] (1.000)
Step: 11299, Reward: [-570.796 -570.796 -570.796] [0.0000], Avg: [-622.565 -622.565 -622.565] (1.000)
Step: 11349, Reward: [-627.482 -627.482 -627.482] [0.0000], Avg: [-622.586 -622.586 -622.586] (1.000)
Step: 11399, Reward: [-802.48 -802.48 -802.48] [0.0000], Avg: [-623.375 -623.375 -623.375] (1.000)
Step: 11449, Reward: [-458.038 -458.038 -458.038] [0.0000], Avg: [-622.653 -622.653 -622.653] (1.000)
Step: 11499, Reward: [-330.32 -330.32 -330.32] [0.0000], Avg: [-621.382 -621.382 -621.382] (1.000)
Step: 11549, Reward: [-573.657 -573.657 -573.657] [0.0000], Avg: [-621.176 -621.176 -621.176] (1.000)
Step: 11599, Reward: [-540.062 -540.062 -540.062] [0.0000], Avg: [-620.826 -620.826 -620.826] (1.000)
Step: 11649, Reward: [-463.651 -463.651 -463.651] [0.0000], Avg: [-620.152 -620.152 -620.152] (1.000)
Step: 11699, Reward: [-343.252 -343.252 -343.252] [0.0000], Avg: [-618.968 -618.968 -618.968] (1.000)
Step: 11749, Reward: [-501.472 -501.472 -501.472] [0.0000], Avg: [-618.468 -618.468 -618.468] (1.000)
Step: 11799, Reward: [-628.435 -628.435 -628.435] [0.0000], Avg: [-618.51 -618.51 -618.51] (1.000)
Step: 11849, Reward: [-600.729 -600.729 -600.729] [0.0000], Avg: [-618.435 -618.435 -618.435] (1.000)
Step: 11899, Reward: [-838.098 -838.098 -838.098] [0.0000], Avg: [-619.358 -619.358 -619.358] (1.000)
Step: 11949, Reward: [-680.333 -680.333 -680.333] [0.0000], Avg: [-619.614 -619.614 -619.614] (1.000)
Step: 11999, Reward: [-597.429 -597.429 -597.429] [0.0000], Avg: [-619.521 -619.521 -619.521] (1.000)
Step: 12049, Reward: [-440.961 -440.961 -440.961] [0.0000], Avg: [-618.78 -618.78 -618.78] (1.000)
Step: 12099, Reward: [-719.172 -719.172 -719.172] [0.0000], Avg: [-619.195 -619.195 -619.195] (1.000)
Step: 12149, Reward: [-738.294 -738.294 -738.294] [0.0000], Avg: [-619.685 -619.685 -619.685] (1.000)
Step: 12199, Reward: [-630.593 -630.593 -630.593] [0.0000], Avg: [-619.73 -619.73 -619.73] (1.000)
Step: 12249, Reward: [-394.171 -394.171 -394.171] [0.0000], Avg: [-618.809 -618.809 -618.809] (1.000)
Step: 12299, Reward: [-819.45 -819.45 -819.45] [0.0000], Avg: [-619.625 -619.625 -619.625] (1.000)
Step: 12349, Reward: [-570.417 -570.417 -570.417] [0.0000], Avg: [-619.426 -619.426 -619.426] (1.000)
Step: 12399, Reward: [-549.449 -549.449 -549.449] [0.0000], Avg: [-619.143 -619.143 -619.143] (1.000)
Step: 12449, Reward: [-655.633 -655.633 -655.633] [0.0000], Avg: [-619.29 -619.29 -619.29] (1.000)
Step: 12499, Reward: [-555.045 -555.045 -555.045] [0.0000], Avg: [-619.033 -619.033 -619.033] (1.000)
Step: 12549, Reward: [-851.826 -851.826 -851.826] [0.0000], Avg: [-619.96 -619.96 -619.96] (1.000)
Step: 12599, Reward: [-498.923 -498.923 -498.923] [0.0000], Avg: [-619.48 -619.48 -619.48] (1.000)
Step: 12649, Reward: [-793.677 -793.677 -793.677] [0.0000], Avg: [-620.169 -620.169 -620.169] (1.000)
Step: 12699, Reward: [-592.594 -592.594 -592.594] [0.0000], Avg: [-620.06 -620.06 -620.06] (1.000)
Step: 12749, Reward: [-698.896 -698.896 -698.896] [0.0000], Avg: [-620.369 -620.369 -620.369] (1.000)
Step: 12799, Reward: [-679.795 -679.795 -679.795] [0.0000], Avg: [-620.601 -620.601 -620.601] (1.000)
Step: 12849, Reward: [-425.518 -425.518 -425.518] [0.0000], Avg: [-619.842 -619.842 -619.842] (1.000)
Step: 12899, Reward: [-450.659 -450.659 -450.659] [0.0000], Avg: [-619.187 -619.187 -619.187] (1.000)
Step: 12949, Reward: [-662.503 -662.503 -662.503] [0.0000], Avg: [-619.354 -619.354 -619.354] (1.000)
Step: 12999, Reward: [-739.452 -739.452 -739.452] [0.0000], Avg: [-619.816 -619.816 -619.816] (1.000)
Step: 13049, Reward: [-742.899 -742.899 -742.899] [0.0000], Avg: [-620.287 -620.287 -620.287] (1.000)
Step: 13099, Reward: [-495.304 -495.304 -495.304] [0.0000], Avg: [-619.81 -619.81 -619.81] (1.000)
Step: 13149, Reward: [-686.481 -686.481 -686.481] [0.0000], Avg: [-620.064 -620.064 -620.064] (1.000)
Step: 13199, Reward: [-710.97 -710.97 -710.97] [0.0000], Avg: [-620.408 -620.408 -620.408] (1.000)
Step: 13249, Reward: [-438.427 -438.427 -438.427] [0.0000], Avg: [-619.721 -619.721 -619.721] (1.000)
Step: 13299, Reward: [-498.595 -498.595 -498.595] [0.0000], Avg: [-619.266 -619.266 -619.266] (1.000)
Step: 13349, Reward: [-576.336 -576.336 -576.336] [0.0000], Avg: [-619.105 -619.105 -619.105] (1.000)
Step: 13399, Reward: [-352.885 -352.885 -352.885] [0.0000], Avg: [-618.112 -618.112 -618.112] (1.000)
Step: 13449, Reward: [-745.567 -745.567 -745.567] [0.0000], Avg: [-618.586 -618.586 -618.586] (1.000)
Step: 13499, Reward: [-838.488 -838.488 -838.488] [0.0000], Avg: [-619.4 -619.4 -619.4] (1.000)
Step: 13549, Reward: [-488.074 -488.074 -488.074] [0.0000], Avg: [-618.916 -618.916 -618.916] (1.000)
Step: 13599, Reward: [-462.539 -462.539 -462.539] [0.0000], Avg: [-618.341 -618.341 -618.341] (1.000)
Step: 13649, Reward: [-869.5 -869.5 -869.5] [0.0000], Avg: [-619.261 -619.261 -619.261] (1.000)
Step: 13699, Reward: [-726.904 -726.904 -726.904] [0.0000], Avg: [-619.654 -619.654 -619.654] (1.000)
Step: 13749, Reward: [-707.158 -707.158 -707.158] [0.0000], Avg: [-619.972 -619.972 -619.972] (1.000)
Step: 13799, Reward: [-638.983 -638.983 -638.983] [0.0000], Avg: [-620.041 -620.041 -620.041] (1.000)
Step: 13849, Reward: [-551.649 -551.649 -551.649] [0.0000], Avg: [-619.794 -619.794 -619.794] (1.000)
Step: 13899, Reward: [-792.556 -792.556 -792.556] [0.0000], Avg: [-620.415 -620.415 -620.415] (1.000)
Step: 13949, Reward: [-915.043 -915.043 -915.043] [0.0000], Avg: [-621.471 -621.471 -621.471] (1.000)
Step: 13999, Reward: [-868.396 -868.396 -868.396] [0.0000], Avg: [-622.353 -622.353 -622.353] (1.000)
Step: 14049, Reward: [-745.696 -745.696 -745.696] [0.0000], Avg: [-622.792 -622.792 -622.792] (1.000)
Step: 14099, Reward: [-415.659 -415.659 -415.659] [0.0000], Avg: [-622.057 -622.057 -622.057] (1.000)
Step: 14149, Reward: [-936.542 -936.542 -936.542] [0.0000], Avg: [-623.169 -623.169 -623.169] (1.000)
Step: 14199, Reward: [-617.698 -617.698 -617.698] [0.0000], Avg: [-623.149 -623.149 -623.149] (1.000)
Step: 14249, Reward: [-710.116 -710.116 -710.116] [0.0000], Avg: [-623.455 -623.455 -623.455] (1.000)
Step: 14299, Reward: [-611.238 -611.238 -611.238] [0.0000], Avg: [-623.412 -623.412 -623.412] (1.000)
Step: 14349, Reward: [-514.745 -514.745 -514.745] [0.0000], Avg: [-623.033 -623.033 -623.033] (1.000)
Step: 14399, Reward: [-648.444 -648.444 -648.444] [0.0000], Avg: [-623.121 -623.121 -623.121] (1.000)
Step: 14449, Reward: [-786.574 -786.574 -786.574] [0.0000], Avg: [-623.687 -623.687 -623.687] (1.000)
Step: 14499, Reward: [-759.219 -759.219 -759.219] [0.0000], Avg: [-624.154 -624.154 -624.154] (1.000)
Step: 14549, Reward: [-1256.46 -1256.46 -1256.46] [0.0000], Avg: [-626.327 -626.327 -626.327] (1.000)
Step: 14599, Reward: [-758.245 -758.245 -758.245] [0.0000], Avg: [-626.779 -626.779 -626.779] (1.000)
Step: 14649, Reward: [-587.578 -587.578 -587.578] [0.0000], Avg: [-626.645 -626.645 -626.645] (1.000)
Step: 14699, Reward: [-1033.202 -1033.202 -1033.202] [0.0000], Avg: [-628.028 -628.028 -628.028] (1.000)
Step: 14749, Reward: [-1350.221 -1350.221 -1350.221] [0.0000], Avg: [-630.476 -630.476 -630.476] (1.000)
Step: 14799, Reward: [-1154.026 -1154.026 -1154.026] [0.0000], Avg: [-632.245 -632.245 -632.245] (1.000)
Step: 14849, Reward: [-638.066 -638.066 -638.066] [0.0000], Avg: [-632.265 -632.265 -632.265] (1.000)
