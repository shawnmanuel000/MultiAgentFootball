Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7f1f91dfd320>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7f1f91dfd3c8>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7f1f91dfd438>],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)
		self.agent = MADDPG(state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.agent.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.t = 0 if not hasattr(self, "t") else self.t + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.agent.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.agent.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0].squeeze(-1) for q_value,reward,done in zip(q_values, rewards, dones)]

			states, actions = map(lambda items: [x[:-1] for x in items], [states, actions])
			states, actions, q_targets = map(lambda items: [x.view(-1, *x.shape[2:]).cpu().numpy() for x in items], [states, actions, q_targets])
			self.replay_buffer.push(states, actions, q_targets)

		if (len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.t % 100)==0):
			states, actions, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.agent.update(states, actions, q_targets)
		"""
			# self.buffer.append((state, action, reward, done))
			# if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			# 	states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			# 	self.buffer.clear()
			# 	next_state = self.to_tensor(next_state)
			# 	states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			# 	actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			# 	states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			# 	actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			# 	q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			# 	q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
				
			# 	to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			# 	states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			# 	states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			# 	self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
			# if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
			# 	states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			# 	self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
			# if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)
		"""

class MADDPG(PTNetwork):
	def __init__(self, state_size, action_size, gamma=0.95, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.gamma = gamma
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.agents = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.agents)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.agents]
			return q_value

	def update(self, states, actions, q_targets):
		for agent_i, curr_agent in enumerate(self.agents):
			target_value = q_targets[agent_i]
			# next_actions = [one_hot(agent.get_action(nobs, numpy=False)) for agent, nobs in zip(self.agents, next_states)]
			# next_states_joint = torch.cat([*next_states], dim=-1)
			# next_actions_joint = torch.cat([*next_actions], dim=-1)
			# target_value = (rewards[agent_i].view(-1, 1, 1) + self.gamma * curr_agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False) * (1 - dones[agent_i].view(-1, 1, 1)))

			states_joint = torch.cat([*states], dim=-1)
			actions_joint = torch.cat([*actions], dim=-1)
			actual_value = curr_agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			vf_loss = (actual_value - target_value.detach()).pow(2).mean()
			curr_agent.step(curr_agent.critic_optimizer, vf_loss, param_norm=curr_agent.critic_local.parameters())
			curr_agent.soft_copy(curr_agent.critic_local, curr_agent.critic_target)

			curr_pol_out = curr_agent.get_action(states[agent_i], grad=True, numpy=False)
			curr_pol_vf_in = gsoftmax(curr_pol_out, hard=True)
			action = [curr_pol_vf_in if i==agent_i else one_hot(agent.get_action(ob, numpy=False)) for (i,agent), ob in zip(enumerate(self.agents), states)]
			action_joint = torch.cat([*action], dim=-1)
			pol_loss = -curr_agent.critic_local(states_joint, action_joint).mean() + 0.001*curr_pol_out.pow(2).mean() 
			curr_agent.step(curr_agent.actor_optimizer, pol_loss, param_norm=curr_agent.actor_local.parameters())
			curr_agent.soft_copy(curr_agent.actor_local, curr_agent.actor_target)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-568.199 -568.199 -568.199] [0.0000], Avg: [-568.199 -568.199 -568.199] (1.000)
Step: 99, Reward: [-485.12 -485.12 -485.12] [0.0000], Avg: [-526.66 -526.66 -526.66] (1.000)
Step: 149, Reward: [-374.679 -374.679 -374.679] [0.0000], Avg: [-476. -476. -476.] (1.000)
Step: 199, Reward: [-579.405 -579.405 -579.405] [0.0000], Avg: [-501.851 -501.851 -501.851] (1.000)
Step: 249, Reward: [-524.525 -524.525 -524.525] [0.0000], Avg: [-506.386 -506.386 -506.386] (1.000)
Step: 299, Reward: [-526.801 -526.801 -526.801] [0.0000], Avg: [-509.788 -509.788 -509.788] (1.000)
Step: 349, Reward: [-482.666 -482.666 -482.666] [0.0000], Avg: [-505.914 -505.914 -505.914] (1.000)
Step: 399, Reward: [-609.336 -609.336 -609.336] [0.0000], Avg: [-518.841 -518.841 -518.841] (1.000)
Step: 449, Reward: [-552.266 -552.266 -552.266] [0.0000], Avg: [-522.555 -522.555 -522.555] (1.000)
Step: 499, Reward: [-405.097 -405.097 -405.097] [0.0000], Avg: [-510.81 -510.81 -510.81] (1.000)
Step: 549, Reward: [-426.801 -426.801 -426.801] [0.0000], Avg: [-503.172 -503.172 -503.172] (1.000)
Step: 599, Reward: [-683.321 -683.321 -683.321] [0.0000], Avg: [-518.185 -518.185 -518.185] (1.000)
Step: 649, Reward: [-548.784 -548.784 -548.784] [0.0000], Avg: [-520.538 -520.538 -520.538] (1.000)
Step: 699, Reward: [-509.084 -509.084 -509.084] [0.0000], Avg: [-519.72 -519.72 -519.72] (1.000)
Step: 749, Reward: [-690.501 -690.501 -690.501] [0.0000], Avg: [-531.106 -531.106 -531.106] (1.000)
Step: 799, Reward: [-370.927 -370.927 -370.927] [0.0000], Avg: [-521.094 -521.094 -521.094] (1.000)
Step: 849, Reward: [-521.668 -521.668 -521.668] [0.0000], Avg: [-521.128 -521.128 -521.128] (1.000)
Step: 899, Reward: [-411.894 -411.894 -411.894] [0.0000], Avg: [-515.06 -515.06 -515.06] (1.000)
Step: 949, Reward: [-513.483 -513.483 -513.483] [0.0000], Avg: [-514.977 -514.977 -514.977] (1.000)
Step: 999, Reward: [-462.174 -462.174 -462.174] [0.0000], Avg: [-512.337 -512.337 -512.337] (1.000)
Step: 1049, Reward: [-405.301 -405.301 -405.301] [0.0000], Avg: [-507.24 -507.24 -507.24] (1.000)
Step: 1099, Reward: [-443.41 -443.41 -443.41] [0.0000], Avg: [-504.338 -504.338 -504.338] (1.000)
Step: 1149, Reward: [-438.319 -438.319 -438.319] [0.0000], Avg: [-501.468 -501.468 -501.468] (1.000)
Step: 1199, Reward: [-595.45 -595.45 -595.45] [0.0000], Avg: [-505.384 -505.384 -505.384] (1.000)
Step: 1249, Reward: [-618.055 -618.055 -618.055] [0.0000], Avg: [-509.891 -509.891 -509.891] (1.000)
Step: 1299, Reward: [-668.064 -668.064 -668.064] [0.0000], Avg: [-515.974 -515.974 -515.974] (1.000)
Step: 1349, Reward: [-791.947 -791.947 -791.947] [0.0000], Avg: [-526.195 -526.195 -526.195] (1.000)
Step: 1399, Reward: [-581.591 -581.591 -581.591] [0.0000], Avg: [-528.174 -528.174 -528.174] (1.000)
Step: 1449, Reward: [-442.016 -442.016 -442.016] [0.0000], Avg: [-525.203 -525.203 -525.203] (1.000)
Step: 1499, Reward: [-474.574 -474.574 -474.574] [0.0000], Avg: [-523.515 -523.515 -523.515] (1.000)
Step: 1549, Reward: [-428.617 -428.617 -428.617] [0.0000], Avg: [-520.454 -520.454 -520.454] (1.000)
Step: 1599, Reward: [-389.698 -389.698 -389.698] [0.0000], Avg: [-516.368 -516.368 -516.368] (1.000)
Step: 1649, Reward: [-558.269 -558.269 -558.269] [0.0000], Avg: [-517.638 -517.638 -517.638] (1.000)
Step: 1699, Reward: [-259.935 -259.935 -259.935] [0.0000], Avg: [-510.058 -510.058 -510.058] (1.000)
Step: 1749, Reward: [-450.758 -450.758 -450.758] [0.0000], Avg: [-508.364 -508.364 -508.364] (1.000)
Step: 1799, Reward: [-556.526 -556.526 -556.526] [0.0000], Avg: [-509.702 -509.702 -509.702] (1.000)
Step: 1849, Reward: [-596.97 -596.97 -596.97] [0.0000], Avg: [-512.06 -512.06 -512.06] (1.000)
Step: 1899, Reward: [-376.422 -376.422 -376.422] [0.0000], Avg: [-508.491 -508.491 -508.491] (1.000)
Step: 1949, Reward: [-628.167 -628.167 -628.167] [0.0000], Avg: [-511.56 -511.56 -511.56] (1.000)
Step: 1999, Reward: [-599.1 -599.1 -599.1] [0.0000], Avg: [-513.748 -513.748 -513.748] (1.000)
Step: 2049, Reward: [-274.062 -274.062 -274.062] [0.0000], Avg: [-507.902 -507.902 -507.902] (1.000)
Step: 2099, Reward: [-434.018 -434.018 -434.018] [0.0000], Avg: [-506.143 -506.143 -506.143] (1.000)
Step: 2149, Reward: [-425.188 -425.188 -425.188] [0.0000], Avg: [-504.26 -504.26 -504.26] (1.000)
Step: 2199, Reward: [-502.305 -502.305 -502.305] [0.0000], Avg: [-504.216 -504.216 -504.216] (1.000)
Step: 2249, Reward: [-530.525 -530.525 -530.525] [0.0000], Avg: [-504.8 -504.8 -504.8] (1.000)
Step: 2299, Reward: [-436.615 -436.615 -436.615] [0.0000], Avg: [-503.318 -503.318 -503.318] (1.000)
Step: 2349, Reward: [-546.828 -546.828 -546.828] [0.0000], Avg: [-504.244 -504.244 -504.244] (1.000)
Step: 2399, Reward: [-517.06 -517.06 -517.06] [0.0000], Avg: [-504.511 -504.511 -504.511] (1.000)
Step: 2449, Reward: [-339.599 -339.599 -339.599] [0.0000], Avg: [-501.145 -501.145 -501.145] (1.000)
Step: 2499, Reward: [-501.785 -501.785 -501.785] [0.0000], Avg: [-501.158 -501.158 -501.158] (1.000)
Step: 2549, Reward: [-555.446 -555.446 -555.446] [0.0000], Avg: [-502.223 -502.223 -502.223] (1.000)
Step: 2599, Reward: [-455.872 -455.872 -455.872] [0.0000], Avg: [-501.331 -501.331 -501.331] (1.000)
Step: 2649, Reward: [-590.093 -590.093 -590.093] [0.0000], Avg: [-503.006 -503.006 -503.006] (1.000)
Step: 2699, Reward: [-550.407 -550.407 -550.407] [0.0000], Avg: [-503.884 -503.884 -503.884] (1.000)
Step: 2749, Reward: [-450.508 -450.508 -450.508] [0.0000], Avg: [-502.913 -502.913 -502.913] (1.000)
Step: 2799, Reward: [-599.095 -599.095 -599.095] [0.0000], Avg: [-504.631 -504.631 -504.631] (1.000)
Step: 2849, Reward: [-734.526 -734.526 -734.526] [0.0000], Avg: [-508.664 -508.664 -508.664] (1.000)
Step: 2899, Reward: [-652.19 -652.19 -652.19] [0.0000], Avg: [-511.139 -511.139 -511.139] (1.000)
Step: 2949, Reward: [-911.253 -911.253 -911.253] [0.0000], Avg: [-517.92 -517.92 -517.92] (1.000)
Step: 2999, Reward: [-682.728 -682.728 -682.728] [0.0000], Avg: [-520.667 -520.667 -520.667] (1.000)
Step: 3049, Reward: [-627.931 -627.931 -627.931] [0.0000], Avg: [-522.426 -522.426 -522.426] (1.000)
Step: 3099, Reward: [-540.263 -540.263 -540.263] [0.0000], Avg: [-522.713 -522.713 -522.713] (1.000)
Step: 3149, Reward: [-659.633 -659.633 -659.633] [0.0000], Avg: [-524.887 -524.887 -524.887] (1.000)
Step: 3199, Reward: [-1640.721 -1640.721 -1640.721] [0.0000], Avg: [-542.321 -542.321 -542.321] (1.000)
Step: 3249, Reward: [-1787.034 -1787.034 -1787.034] [0.0000], Avg: [-561.471 -561.471 -561.471] (1.000)
Step: 3299, Reward: [-1260.924 -1260.924 -1260.924] [0.0000], Avg: [-572.069 -572.069 -572.069] (1.000)
Step: 3349, Reward: [-1871.543 -1871.543 -1871.543] [0.0000], Avg: [-591.464 -591.464 -591.464] (1.000)
Step: 3399, Reward: [-888.639 -888.639 -888.639] [0.0000], Avg: [-595.834 -595.834 -595.834] (1.000)
Step: 3449, Reward: [-786.844 -786.844 -786.844] [0.0000], Avg: [-598.602 -598.602 -598.602] (1.000)
Step: 3499, Reward: [-1474.922 -1474.922 -1474.922] [0.0000], Avg: [-611.121 -611.121 -611.121] (1.000)
Step: 3549, Reward: [-621.892 -621.892 -621.892] [0.0000], Avg: [-611.273 -611.273 -611.273] (1.000)
Step: 3599, Reward: [-1667.757 -1667.757 -1667.757] [0.0000], Avg: [-625.946 -625.946 -625.946] (1.000)
Step: 3649, Reward: [-586.945 -586.945 -586.945] [0.0000], Avg: [-625.412 -625.412 -625.412] (1.000)
Step: 3699, Reward: [-1249.984 -1249.984 -1249.984] [0.0000], Avg: [-633.852 -633.852 -633.852] (1.000)
Step: 3749, Reward: [-723.819 -723.819 -723.819] [0.0000], Avg: [-635.052 -635.052 -635.052] (1.000)
Step: 3799, Reward: [-1392.266 -1392.266 -1392.266] [0.0000], Avg: [-645.015 -645.015 -645.015] (1.000)
Step: 3849, Reward: [-1012.679 -1012.679 -1012.679] [0.0000], Avg: [-649.79 -649.79 -649.79] (1.000)
Step: 3899, Reward: [-806.796 -806.796 -806.796] [0.0000], Avg: [-651.803 -651.803 -651.803] (1.000)
Step: 3949, Reward: [-976.495 -976.495 -976.495] [0.0000], Avg: [-655.913 -655.913 -655.913] (1.000)
Step: 3999, Reward: [-664.444 -664.444 -664.444] [0.0000], Avg: [-656.019 -656.019 -656.019] (1.000)
Step: 4049, Reward: [-662.232 -662.232 -662.232] [0.0000], Avg: [-656.096 -656.096 -656.096] (1.000)
Step: 4099, Reward: [-1026.049 -1026.049 -1026.049] [0.0000], Avg: [-660.608 -660.608 -660.608] (1.000)
Step: 4149, Reward: [-864.215 -864.215 -864.215] [0.0000], Avg: [-663.061 -663.061 -663.061] (1.000)
Step: 4199, Reward: [-497.425 -497.425 -497.425] [0.0000], Avg: [-661.089 -661.089 -661.089] (1.000)
Step: 4249, Reward: [-731.306 -731.306 -731.306] [0.0000], Avg: [-661.915 -661.915 -661.915] (1.000)
Step: 4299, Reward: [-547.841 -547.841 -547.841] [0.0000], Avg: [-660.589 -660.589 -660.589] (1.000)
Step: 4349, Reward: [-674.379 -674.379 -674.379] [0.0000], Avg: [-660.747 -660.747 -660.747] (1.000)
Step: 4399, Reward: [-763.599 -763.599 -763.599] [0.0000], Avg: [-661.916 -661.916 -661.916] (1.000)
Step: 4449, Reward: [-569.089 -569.089 -569.089] [0.0000], Avg: [-660.873 -660.873 -660.873] (1.000)
Step: 4499, Reward: [-365.9 -365.9 -365.9] [0.0000], Avg: [-657.595 -657.595 -657.595] (1.000)
Step: 4549, Reward: [-856.44 -856.44 -856.44] [0.0000], Avg: [-659.781 -659.781 -659.781] (1.000)
Step: 4599, Reward: [-594.859 -594.859 -594.859] [0.0000], Avg: [-659.075 -659.075 -659.075] (1.000)
Step: 4649, Reward: [-520.968 -520.968 -520.968] [0.0000], Avg: [-657.59 -657.59 -657.59] (1.000)
Step: 4699, Reward: [-912.554 -912.554 -912.554] [0.0000], Avg: [-660.302 -660.302 -660.302] (1.000)
Step: 4749, Reward: [-711.443 -711.443 -711.443] [0.0000], Avg: [-660.841 -660.841 -660.841] (1.000)
Step: 4799, Reward: [-980.271 -980.271 -980.271] [0.0000], Avg: [-664.168 -664.168 -664.168] (1.000)
Step: 4849, Reward: [-1514.946 -1514.946 -1514.946] [0.0000], Avg: [-672.939 -672.939 -672.939] (1.000)
Step: 4899, Reward: [-510.4 -510.4 -510.4] [0.0000], Avg: [-671.28 -671.28 -671.28] (1.000)
Step: 4949, Reward: [-886.95 -886.95 -886.95] [0.0000], Avg: [-673.459 -673.459 -673.459] (1.000)
Step: 4999, Reward: [-1271.796 -1271.796 -1271.796] [0.0000], Avg: [-679.442 -679.442 -679.442] (1.000)
Step: 5049, Reward: [-854.438 -854.438 -854.438] [0.0000], Avg: [-681.175 -681.175 -681.175] (1.000)
Step: 5099, Reward: [-2238.025 -2238.025 -2238.025] [0.0000], Avg: [-696.438 -696.438 -696.438] (1.000)
Step: 5149, Reward: [-981.925 -981.925 -981.925] [0.0000], Avg: [-699.21 -699.21 -699.21] (1.000)
Step: 5199, Reward: [-1938.671 -1938.671 -1938.671] [0.0000], Avg: [-711.128 -711.128 -711.128] (1.000)
Step: 5249, Reward: [-1651.487 -1651.487 -1651.487] [0.0000], Avg: [-720.083 -720.083 -720.083] (1.000)
Step: 5299, Reward: [-1562.211 -1562.211 -1562.211] [0.0000], Avg: [-728.028 -728.028 -728.028] (1.000)
Step: 5349, Reward: [-1947.985 -1947.985 -1947.985] [0.0000], Avg: [-739.43 -739.43 -739.43] (1.000)
Step: 5399, Reward: [-1814.144 -1814.144 -1814.144] [0.0000], Avg: [-749.381 -749.381 -749.381] (1.000)
Step: 5449, Reward: [-1318.224 -1318.224 -1318.224] [0.0000], Avg: [-754.599 -754.599 -754.599] (1.000)
Step: 5499, Reward: [-2009.237 -2009.237 -2009.237] [0.0000], Avg: [-766.005 -766.005 -766.005] (1.000)
Step: 5549, Reward: [-1428.827 -1428.827 -1428.827] [0.0000], Avg: [-771.977 -771.977 -771.977] (1.000)
Step: 5599, Reward: [-1637.487 -1637.487 -1637.487] [0.0000], Avg: [-779.704 -779.704 -779.704] (1.000)
Step: 5649, Reward: [-2081.639 -2081.639 -2081.639] [0.0000], Avg: [-791.226 -791.226 -791.226] (1.000)
Step: 5699, Reward: [-1751.127 -1751.127 -1751.127] [0.0000], Avg: [-799.646 -799.646 -799.646] (1.000)
Step: 5749, Reward: [-1595.955 -1595.955 -1595.955] [0.0000], Avg: [-806.57 -806.57 -806.57] (1.000)
Step: 5799, Reward: [-1401.123 -1401.123 -1401.123] [0.0000], Avg: [-811.696 -811.696 -811.696] (1.000)
Step: 5849, Reward: [-1319.716 -1319.716 -1319.716] [0.0000], Avg: [-816.038 -816.038 -816.038] (1.000)
Step: 5899, Reward: [-1267.335 -1267.335 -1267.335] [0.0000], Avg: [-819.862 -819.862 -819.862] (1.000)
Step: 5949, Reward: [-1530.547 -1530.547 -1530.547] [0.0000], Avg: [-825.835 -825.835 -825.835] (1.000)
Step: 5999, Reward: [-1725.244 -1725.244 -1725.244] [0.0000], Avg: [-833.33 -833.33 -833.33] (1.000)
Step: 6049, Reward: [-1555.102 -1555.102 -1555.102] [0.0000], Avg: [-839.295 -839.295 -839.295] (1.000)
Step: 6099, Reward: [-2031.13 -2031.13 -2031.13] [0.0000], Avg: [-849.064 -849.064 -849.064] (1.000)
Step: 6149, Reward: [-1419.521 -1419.521 -1419.521] [0.0000], Avg: [-853.702 -853.702 -853.702] (1.000)
Step: 6199, Reward: [-1371.378 -1371.378 -1371.378] [0.0000], Avg: [-857.877 -857.877 -857.877] (1.000)
Step: 6249, Reward: [-1373.208 -1373.208 -1373.208] [0.0000], Avg: [-861.999 -861.999 -861.999] (1.000)
Step: 6299, Reward: [-1555.404 -1555.404 -1555.404] [0.0000], Avg: [-867.502 -867.502 -867.502] (1.000)
Step: 6349, Reward: [-695.193 -695.193 -695.193] [0.0000], Avg: [-866.146 -866.146 -866.146] (1.000)
Step: 6399, Reward: [-736.98 -736.98 -736.98] [0.0000], Avg: [-865.137 -865.137 -865.137] (1.000)
Step: 6449, Reward: [-951.916 -951.916 -951.916] [0.0000], Avg: [-865.809 -865.809 -865.809] (1.000)
Step: 6499, Reward: [-1168.408 -1168.408 -1168.408] [0.0000], Avg: [-868.137 -868.137 -868.137] (1.000)
Step: 6549, Reward: [-394.279 -394.279 -394.279] [0.0000], Avg: [-864.52 -864.52 -864.52] (1.000)
Step: 6599, Reward: [-488.799 -488.799 -488.799] [0.0000], Avg: [-861.673 -861.673 -861.673] (1.000)
Step: 6649, Reward: [-739.029 -739.029 -739.029] [0.0000], Avg: [-860.751 -860.751 -860.751] (1.000)
Step: 6699, Reward: [-574.225 -574.225 -574.225] [0.0000], Avg: [-858.613 -858.613 -858.613] (1.000)
Step: 6749, Reward: [-521.137 -521.137 -521.137] [0.0000], Avg: [-856.113 -856.113 -856.113] (1.000)
Step: 6799, Reward: [-505.03 -505.03 -505.03] [0.0000], Avg: [-853.532 -853.532 -853.532] (1.000)
Step: 6849, Reward: [-607.171 -607.171 -607.171] [0.0000], Avg: [-851.733 -851.733 -851.733] (1.000)
Step: 6899, Reward: [-400.099 -400.099 -400.099] [0.0000], Avg: [-848.461 -848.461 -848.461] (1.000)
Step: 6949, Reward: [-615.01 -615.01 -615.01] [0.0000], Avg: [-846.781 -846.781 -846.781] (1.000)
Step: 6999, Reward: [-669.983 -669.983 -669.983] [0.0000], Avg: [-845.518 -845.518 -845.518] (1.000)
Step: 7049, Reward: [-387.181 -387.181 -387.181] [0.0000], Avg: [-842.268 -842.268 -842.268] (1.000)
Step: 7099, Reward: [-432.267 -432.267 -432.267] [0.0000], Avg: [-839.38 -839.38 -839.38] (1.000)
Step: 7149, Reward: [-713.346 -713.346 -713.346] [0.0000], Avg: [-838.499 -838.499 -838.499] (1.000)
Step: 7199, Reward: [-558.051 -558.051 -558.051] [0.0000], Avg: [-836.551 -836.551 -836.551] (1.000)
Step: 7249, Reward: [-565.547 -565.547 -565.547] [0.0000], Avg: [-834.682 -834.682 -834.682] (1.000)
Step: 7299, Reward: [-458.556 -458.556 -458.556] [0.0000], Avg: [-832.106 -832.106 -832.106] (1.000)
Step: 7349, Reward: [-444.652 -444.652 -444.652] [0.0000], Avg: [-829.471 -829.471 -829.471] (1.000)
Step: 7399, Reward: [-514.562 -514.562 -514.562] [0.0000], Avg: [-827.343 -827.343 -827.343] (1.000)
Step: 7449, Reward: [-608.937 -608.937 -608.937] [0.0000], Avg: [-825.877 -825.877 -825.877] (1.000)
Step: 7499, Reward: [-481.205 -481.205 -481.205] [0.0000], Avg: [-823.579 -823.579 -823.579] (1.000)
Step: 7549, Reward: [-438.979 -438.979 -438.979] [0.0000], Avg: [-821.032 -821.032 -821.032] (1.000)
Step: 7599, Reward: [-591.239 -591.239 -591.239] [0.0000], Avg: [-819.52 -819.52 -819.52] (1.000)
Step: 7649, Reward: [-798.997 -798.997 -798.997] [0.0000], Avg: [-819.386 -819.386 -819.386] (1.000)
Step: 7699, Reward: [-475.605 -475.605 -475.605] [0.0000], Avg: [-817.154 -817.154 -817.154] (1.000)
Step: 7749, Reward: [-1145.403 -1145.403 -1145.403] [0.0000], Avg: [-819.272 -819.272 -819.272] (1.000)
Step: 7799, Reward: [-407.744 -407.744 -407.744] [0.0000], Avg: [-816.634 -816.634 -816.634] (1.000)
Step: 7849, Reward: [-831.207 -831.207 -831.207] [0.0000], Avg: [-816.726 -816.726 -816.726] (1.000)
Step: 7899, Reward: [-555.149 -555.149 -555.149] [0.0000], Avg: [-815.071 -815.071 -815.071] (1.000)
Step: 7949, Reward: [-436.216 -436.216 -436.216] [0.0000], Avg: [-812.688 -812.688 -812.688] (1.000)
Step: 7999, Reward: [-442.911 -442.911 -442.911] [0.0000], Avg: [-810.377 -810.377 -810.377] (1.000)
Step: 8049, Reward: [-702.914 -702.914 -702.914] [0.0000], Avg: [-809.71 -809.71 -809.71] (1.000)
Step: 8099, Reward: [-533.747 -533.747 -533.747] [0.0000], Avg: [-808.006 -808.006 -808.006] (1.000)
Step: 8149, Reward: [-1089.431 -1089.431 -1089.431] [0.0000], Avg: [-809.733 -809.733 -809.733] (1.000)
Step: 8199, Reward: [-872.972 -872.972 -872.972] [0.0000], Avg: [-810.118 -810.118 -810.118] (1.000)
Step: 8249, Reward: [-943.207 -943.207 -943.207] [0.0000], Avg: [-810.925 -810.925 -810.925] (1.000)
Step: 8299, Reward: [-576.083 -576.083 -576.083] [0.0000], Avg: [-809.51 -809.51 -809.51] (1.000)
Step: 8349, Reward: [-988.323 -988.323 -988.323] [0.0000], Avg: [-810.581 -810.581 -810.581] (1.000)
Step: 8399, Reward: [-527.796 -527.796 -527.796] [0.0000], Avg: [-808.898 -808.898 -808.898] (1.000)
Step: 8449, Reward: [-529.043 -529.043 -529.043] [0.0000], Avg: [-807.242 -807.242 -807.242] (1.000)
Step: 8499, Reward: [-870.885 -870.885 -870.885] [0.0000], Avg: [-807.616 -807.616 -807.616] (1.000)
Step: 8549, Reward: [-1277.722 -1277.722 -1277.722] [0.0000], Avg: [-810.365 -810.365 -810.365] (1.000)
Step: 8599, Reward: [-713.921 -713.921 -713.921] [0.0000], Avg: [-809.804 -809.804 -809.804] (1.000)
Step: 8649, Reward: [-411.93 -411.93 -411.93] [0.0000], Avg: [-807.505 -807.505 -807.505] (1.000)
Step: 8699, Reward: [-497.139 -497.139 -497.139] [0.0000], Avg: [-805.721 -805.721 -805.721] (1.000)
Step: 8749, Reward: [-491.031 -491.031 -491.031] [0.0000], Avg: [-803.923 -803.923 -803.923] (1.000)
Step: 8799, Reward: [-810.368 -810.368 -810.368] [0.0000], Avg: [-803.959 -803.959 -803.959] (1.000)
Step: 8849, Reward: [-672.583 -672.583 -672.583] [0.0000], Avg: [-803.217 -803.217 -803.217] (1.000)
Step: 8899, Reward: [-553.934 -553.934 -553.934] [0.0000], Avg: [-801.817 -801.817 -801.817] (1.000)
Step: 8949, Reward: [-392.101 -392.101 -392.101] [0.0000], Avg: [-799.528 -799.528 -799.528] (1.000)
Step: 8999, Reward: [-571.884 -571.884 -571.884] [0.0000], Avg: [-798.263 -798.263 -798.263] (1.000)
Step: 9049, Reward: [-642.5 -642.5 -642.5] [0.0000], Avg: [-797.402 -797.402 -797.402] (1.000)
Step: 9099, Reward: [-476.604 -476.604 -476.604] [0.0000], Avg: [-795.64 -795.64 -795.64] (1.000)
Step: 9149, Reward: [-690.389 -690.389 -690.389] [0.0000], Avg: [-795.065 -795.065 -795.065] (1.000)
Step: 9199, Reward: [-537.958 -537.958 -537.958] [0.0000], Avg: [-793.667 -793.667 -793.667] (1.000)
Step: 9249, Reward: [-412.411 -412.411 -412.411] [0.0000], Avg: [-791.606 -791.606 -791.606] (1.000)
Step: 9299, Reward: [-381.533 -381.533 -381.533] [0.0000], Avg: [-789.402 -789.402 -789.402] (1.000)
Step: 9349, Reward: [-511.755 -511.755 -511.755] [0.0000], Avg: [-787.917 -787.917 -787.917] (1.000)
Step: 9399, Reward: [-761.063 -761.063 -761.063] [0.0000], Avg: [-787.774 -787.774 -787.774] (1.000)
Step: 9449, Reward: [-538.13 -538.13 -538.13] [0.0000], Avg: [-786.453 -786.453 -786.453] (1.000)
Step: 9499, Reward: [-451.484 -451.484 -451.484] [0.0000], Avg: [-784.69 -784.69 -784.69] (1.000)
Step: 9549, Reward: [-618.798 -618.798 -618.798] [0.0000], Avg: [-783.822 -783.822 -783.822] (1.000)
Step: 9599, Reward: [-411.777 -411.777 -411.777] [0.0000], Avg: [-781.884 -781.884 -781.884] (1.000)
Step: 9649, Reward: [-493.575 -493.575 -493.575] [0.0000], Avg: [-780.39 -780.39 -780.39] (1.000)
Step: 9699, Reward: [-478.56 -478.56 -478.56] [0.0000], Avg: [-778.834 -778.834 -778.834] (1.000)
Step: 9749, Reward: [-338.477 -338.477 -338.477] [0.0000], Avg: [-776.576 -776.576 -776.576] (1.000)
Step: 9799, Reward: [-518.568 -518.568 -518.568] [0.0000], Avg: [-775.26 -775.26 -775.26] (1.000)
Step: 9849, Reward: [-426.23 -426.23 -426.23] [0.0000], Avg: [-773.488 -773.488 -773.488] (1.000)
Step: 9899, Reward: [-460.414 -460.414 -460.414] [0.0000], Avg: [-771.907 -771.907 -771.907] (1.000)
Step: 9949, Reward: [-490.992 -490.992 -490.992] [0.0000], Avg: [-770.495 -770.495 -770.495] (1.000)
Step: 9999, Reward: [-557.072 -557.072 -557.072] [0.0000], Avg: [-769.428 -769.428 -769.428] (1.000)
Step: 10049, Reward: [-474.121 -474.121 -474.121] [0.0000], Avg: [-767.959 -767.959 -767.959] (1.000)
Step: 10099, Reward: [-358.948 -358.948 -358.948] [0.0000], Avg: [-765.934 -765.934 -765.934] (1.000)
Step: 10149, Reward: [-549.864 -549.864 -549.864] [0.0000], Avg: [-764.87 -764.87 -764.87] (1.000)
Step: 10199, Reward: [-622.14 -622.14 -622.14] [0.0000], Avg: [-764.17 -764.17 -764.17] (1.000)
Step: 10249, Reward: [-484.012 -484.012 -484.012] [0.0000], Avg: [-762.803 -762.803 -762.803] (1.000)
Step: 10299, Reward: [-461.89 -461.89 -461.89] [0.0000], Avg: [-761.343 -761.343 -761.343] (1.000)
Step: 10349, Reward: [-563.872 -563.872 -563.872] [0.0000], Avg: [-760.389 -760.389 -760.389] (1.000)
Step: 10399, Reward: [-590.911 -590.911 -590.911] [0.0000], Avg: [-759.574 -759.574 -759.574] (1.000)
Step: 10449, Reward: [-996.554 -996.554 -996.554] [0.0000], Avg: [-760.708 -760.708 -760.708] (1.000)
Step: 10499, Reward: [-461.9 -461.9 -461.9] [0.0000], Avg: [-759.285 -759.285 -759.285] (1.000)
Step: 10549, Reward: [-664.684 -664.684 -664.684] [0.0000], Avg: [-758.837 -758.837 -758.837] (1.000)
Step: 10599, Reward: [-401.789 -401.789 -401.789] [0.0000], Avg: [-757.152 -757.152 -757.152] (1.000)
Step: 10649, Reward: [-784.937 -784.937 -784.937] [0.0000], Avg: [-757.283 -757.283 -757.283] (1.000)
Step: 10699, Reward: [-459.938 -459.938 -459.938] [0.0000], Avg: [-755.893 -755.893 -755.893] (1.000)
Step: 10749, Reward: [-536.443 -536.443 -536.443] [0.0000], Avg: [-754.873 -754.873 -754.873] (1.000)
Step: 10799, Reward: [-581.376 -581.376 -581.376] [0.0000], Avg: [-754.069 -754.069 -754.069] (1.000)
Step: 10849, Reward: [-490.301 -490.301 -490.301] [0.0000], Avg: [-752.854 -752.854 -752.854] (1.000)
Step: 10899, Reward: [-439.675 -439.675 -439.675] [0.0000], Avg: [-751.417 -751.417 -751.417] (1.000)
Step: 10949, Reward: [-577.209 -577.209 -577.209] [0.0000], Avg: [-750.622 -750.622 -750.622] (1.000)
Step: 10999, Reward: [-624.722 -624.722 -624.722] [0.0000], Avg: [-750.05 -750.05 -750.05] (1.000)
Step: 11049, Reward: [-723.438 -723.438 -723.438] [0.0000], Avg: [-749.929 -749.929 -749.929] (1.000)
Step: 11099, Reward: [-601.607 -601.607 -601.607] [0.0000], Avg: [-749.261 -749.261 -749.261] (1.000)
Step: 11149, Reward: [-501.795 -501.795 -501.795] [0.0000], Avg: [-748.151 -748.151 -748.151] (1.000)
Step: 11199, Reward: [-731.802 -731.802 -731.802] [0.0000], Avg: [-748.078 -748.078 -748.078] (1.000)
Step: 11249, Reward: [-619.914 -619.914 -619.914] [0.0000], Avg: [-747.509 -747.509 -747.509] (1.000)
Step: 11299, Reward: [-535.962 -535.962 -535.962] [0.0000], Avg: [-746.573 -746.573 -746.573] (1.000)
Step: 11349, Reward: [-734.787 -734.787 -734.787] [0.0000], Avg: [-746.521 -746.521 -746.521] (1.000)
Step: 11399, Reward: [-851.895 -851.895 -851.895] [0.0000], Avg: [-746.983 -746.983 -746.983] (1.000)
Step: 11449, Reward: [-823.404 -823.404 -823.404] [0.0000], Avg: [-747.317 -747.317 -747.317] (1.000)
Step: 11499, Reward: [-928.404 -928.404 -928.404] [0.0000], Avg: [-748.104 -748.104 -748.104] (1.000)
Step: 11549, Reward: [-1232.617 -1232.617 -1232.617] [0.0000], Avg: [-750.201 -750.201 -750.201] (1.000)
Step: 11599, Reward: [-802.867 -802.867 -802.867] [0.0000], Avg: [-750.428 -750.428 -750.428] (1.000)
Step: 11649, Reward: [-549.725 -549.725 -549.725] [0.0000], Avg: [-749.567 -749.567 -749.567] (1.000)
Step: 11699, Reward: [-982.93 -982.93 -982.93] [0.0000], Avg: [-750.564 -750.564 -750.564] (1.000)
Step: 11749, Reward: [-656.876 -656.876 -656.876] [0.0000], Avg: [-750.166 -750.166 -750.166] (1.000)
Step: 11799, Reward: [-1109.08 -1109.08 -1109.08] [0.0000], Avg: [-751.687 -751.687 -751.687] (1.000)
Step: 11849, Reward: [-1138.55 -1138.55 -1138.55] [0.0000], Avg: [-753.319 -753.319 -753.319] (1.000)
Step: 11899, Reward: [-569.012 -569.012 -569.012] [0.0000], Avg: [-752.544 -752.544 -752.544] (1.000)
Step: 11949, Reward: [-1102.407 -1102.407 -1102.407] [0.0000], Avg: [-754.008 -754.008 -754.008] (1.000)
Step: 11999, Reward: [-1056.028 -1056.028 -1056.028] [0.0000], Avg: [-755.267 -755.267 -755.267] (1.000)
Step: 12049, Reward: [-632.118 -632.118 -632.118] [0.0000], Avg: [-754.756 -754.756 -754.756] (1.000)
Step: 12099, Reward: [-632.823 -632.823 -632.823] [0.0000], Avg: [-754.252 -754.252 -754.252] (1.000)
Step: 12149, Reward: [-471.709 -471.709 -471.709] [0.0000], Avg: [-753.089 -753.089 -753.089] (1.000)
Step: 12199, Reward: [-424.147 -424.147 -424.147] [0.0000], Avg: [-751.741 -751.741 -751.741] (1.000)
Step: 12249, Reward: [-1027.131 -1027.131 -1027.131] [0.0000], Avg: [-752.865 -752.865 -752.865] (1.000)
Step: 12299, Reward: [-517.074 -517.074 -517.074] [0.0000], Avg: [-751.907 -751.907 -751.907] (1.000)
Step: 12349, Reward: [-455.839 -455.839 -455.839] [0.0000], Avg: [-750.708 -750.708 -750.708] (1.000)
Step: 12399, Reward: [-617.054 -617.054 -617.054] [0.0000], Avg: [-750.169 -750.169 -750.169] (1.000)
Step: 12449, Reward: [-383.408 -383.408 -383.408] [0.0000], Avg: [-748.696 -748.696 -748.696] (1.000)
Step: 12499, Reward: [-671.291 -671.291 -671.291] [0.0000], Avg: [-748.386 -748.386 -748.386] (1.000)
Step: 12549, Reward: [-574.711 -574.711 -574.711] [0.0000], Avg: [-747.694 -747.694 -747.694] (1.000)
Step: 12599, Reward: [-549.42 -549.42 -549.42] [0.0000], Avg: [-746.908 -746.908 -746.908] (1.000)
Step: 12649, Reward: [-631.484 -631.484 -631.484] [0.0000], Avg: [-746.451 -746.451 -746.451] (1.000)
Step: 12699, Reward: [-774.639 -774.639 -774.639] [0.0000], Avg: [-746.562 -746.562 -746.562] (1.000)
Step: 12749, Reward: [-557.021 -557.021 -557.021] [0.0000], Avg: [-745.819 -745.819 -745.819] (1.000)
Step: 12799, Reward: [-895.451 -895.451 -895.451] [0.0000], Avg: [-746.404 -746.404 -746.404] (1.000)
Step: 12849, Reward: [-671.939 -671.939 -671.939] [0.0000], Avg: [-746.114 -746.114 -746.114] (1.000)
Step: 12899, Reward: [-480.935 -480.935 -480.935] [0.0000], Avg: [-745.086 -745.086 -745.086] (1.000)
Step: 12949, Reward: [-611.983 -611.983 -611.983] [0.0000], Avg: [-744.572 -744.572 -744.572] (1.000)
Step: 12999, Reward: [-476.853 -476.853 -476.853] [0.0000], Avg: [-743.542 -743.542 -743.542] (1.000)
Step: 13049, Reward: [-628.606 -628.606 -628.606] [0.0000], Avg: [-743.102 -743.102 -743.102] (1.000)
Step: 13099, Reward: [-1019.585 -1019.585 -1019.585] [0.0000], Avg: [-744.157 -744.157 -744.157] (1.000)
Step: 13149, Reward: [-680.09 -680.09 -680.09] [0.0000], Avg: [-743.914 -743.914 -743.914] (1.000)
Step: 13199, Reward: [-520.028 -520.028 -520.028] [0.0000], Avg: [-743.066 -743.066 -743.066] (1.000)
Step: 13249, Reward: [-670.116 -670.116 -670.116] [0.0000], Avg: [-742.79 -742.79 -742.79] (1.000)
Step: 13299, Reward: [-606.022 -606.022 -606.022] [0.0000], Avg: [-742.276 -742.276 -742.276] (1.000)
Step: 13349, Reward: [-676.935 -676.935 -676.935] [0.0000], Avg: [-742.032 -742.032 -742.032] (1.000)
Step: 13399, Reward: [-802.084 -802.084 -802.084] [0.0000], Avg: [-742.256 -742.256 -742.256] (1.000)
Step: 13449, Reward: [-605.673 -605.673 -605.673] [0.0000], Avg: [-741.748 -741.748 -741.748] (1.000)
Step: 13499, Reward: [-433.967 -433.967 -433.967] [0.0000], Avg: [-740.608 -740.608 -740.608] (1.000)
Step: 13549, Reward: [-547.327 -547.327 -547.327] [0.0000], Avg: [-739.895 -739.895 -739.895] (1.000)
Step: 13599, Reward: [-507.914 -507.914 -507.914] [0.0000], Avg: [-739.042 -739.042 -739.042] (1.000)
Step: 13649, Reward: [-479.408 -479.408 -479.408] [0.0000], Avg: [-738.091 -738.091 -738.091] (1.000)
Step: 13699, Reward: [-665.73 -665.73 -665.73] [0.0000], Avg: [-737.827 -737.827 -737.827] (1.000)
Step: 13749, Reward: [-657.57 -657.57 -657.57] [0.0000], Avg: [-737.535 -737.535 -737.535] (1.000)
Step: 13799, Reward: [-334.477 -334.477 -334.477] [0.0000], Avg: [-736.075 -736.075 -736.075] (1.000)
Step: 13849, Reward: [-647.324 -647.324 -647.324] [0.0000], Avg: [-735.754 -735.754 -735.754] (1.000)
Step: 13899, Reward: [-587.3 -587.3 -587.3] [0.0000], Avg: [-735.22 -735.22 -735.22] (1.000)
Step: 13949, Reward: [-504.594 -504.594 -504.594] [0.0000], Avg: [-734.394 -734.394 -734.394] (1.000)
Step: 13999, Reward: [-467.102 -467.102 -467.102] [0.0000], Avg: [-733.439 -733.439 -733.439] (1.000)
Step: 14049, Reward: [-997.018 -997.018 -997.018] [0.0000], Avg: [-734.377 -734.377 -734.377] (1.000)
Step: 14099, Reward: [-449.388 -449.388 -449.388] [0.0000], Avg: [-733.366 -733.366 -733.366] (1.000)
Step: 14149, Reward: [-845.037 -845.037 -845.037] [0.0000], Avg: [-733.761 -733.761 -733.761] (1.000)
Step: 14199, Reward: [-586.99 -586.99 -586.99] [0.0000], Avg: [-733.244 -733.244 -733.244] (1.000)
Step: 14249, Reward: [-598.86 -598.86 -598.86] [0.0000], Avg: [-732.773 -732.773 -732.773] (1.000)
Step: 14299, Reward: [-569.654 -569.654 -569.654] [0.0000], Avg: [-732.202 -732.202 -732.202] (1.000)
Step: 14349, Reward: [-381.611 -381.611 -381.611] [0.0000], Avg: [-730.981 -730.981 -730.981] (1.000)
Step: 14399, Reward: [-620.032 -620.032 -620.032] [0.0000], Avg: [-730.595 -730.595 -730.595] (1.000)
Step: 14449, Reward: [-443.389 -443.389 -443.389] [0.0000], Avg: [-729.602 -729.602 -729.602] (1.000)
Step: 14499, Reward: [-621.809 -621.809 -621.809] [0.0000], Avg: [-729.23 -729.23 -729.23] (1.000)
Step: 14549, Reward: [-564.694 -564.694 -564.694] [0.0000], Avg: [-728.665 -728.665 -728.665] (1.000)
Step: 14599, Reward: [-664.079 -664.079 -664.079] [0.0000], Avg: [-728.443 -728.443 -728.443] (1.000)
Step: 14649, Reward: [-501.935 -501.935 -501.935] [0.0000], Avg: [-727.67 -727.67 -727.67] (1.000)
Step: 14699, Reward: [-575.811 -575.811 -575.811] [0.0000], Avg: [-727.154 -727.154 -727.154] (1.000)
Step: 14749, Reward: [-508.449 -508.449 -508.449] [0.0000], Avg: [-726.412 -726.412 -726.412] (1.000)
Step: 14799, Reward: [-615.124 -615.124 -615.124] [0.0000], Avg: [-726.036 -726.036 -726.036] (1.000)
Step: 14849, Reward: [-416.322 -416.322 -416.322] [0.0000], Avg: [-724.994 -724.994 -724.994] (1.000)
Step: 14899, Reward: [-575.908 -575.908 -575.908] [0.0000], Avg: [-724.493 -724.493 -724.493] (1.000)
Step: 14949, Reward: [-515.753 -515.753 -515.753] [0.0000], Avg: [-723.795 -723.795 -723.795] (1.000)
Step: 14999, Reward: [-619.047 -619.047 -619.047] [0.0000], Avg: [-723.446 -723.446 -723.446] (1.000)
Step: 15049, Reward: [-538.491 -538.491 -538.491] [0.0000], Avg: [-722.832 -722.832 -722.832] (1.000)
Step: 15099, Reward: [-553.034 -553.034 -553.034] [0.0000], Avg: [-722.269 -722.269 -722.269] (1.000)
Step: 15149, Reward: [-735.323 -735.323 -735.323] [0.0000], Avg: [-722.312 -722.312 -722.312] (1.000)
Step: 15199, Reward: [-746.58 -746.58 -746.58] [0.0000], Avg: [-722.392 -722.392 -722.392] (1.000)
Step: 15249, Reward: [-635.173 -635.173 -635.173] [0.0000], Avg: [-722.106 -722.106 -722.106] (1.000)
Step: 15299, Reward: [-553.701 -553.701 -553.701] [0.0000], Avg: [-721.556 -721.556 -721.556] (1.000)
Step: 15349, Reward: [-567.163 -567.163 -567.163] [0.0000], Avg: [-721.053 -721.053 -721.053] (1.000)
Step: 15399, Reward: [-715.359 -715.359 -715.359] [0.0000], Avg: [-721.035 -721.035 -721.035] (1.000)
Step: 15449, Reward: [-515.081 -515.081 -515.081] [0.0000], Avg: [-720.368 -720.368 -720.368] (1.000)
Step: 15499, Reward: [-505.937 -505.937 -505.937] [0.0000], Avg: [-719.676 -719.676 -719.676] (1.000)
Step: 15549, Reward: [-664.862 -664.862 -664.862] [0.0000], Avg: [-719.5 -719.5 -719.5] (1.000)
Step: 15599, Reward: [-450.173 -450.173 -450.173] [0.0000], Avg: [-718.637 -718.637 -718.637] (1.000)
Step: 15649, Reward: [-847.059 -847.059 -847.059] [0.0000], Avg: [-719.047 -719.047 -719.047] (1.000)
Step: 15699, Reward: [-709.332 -709.332 -709.332] [0.0000], Avg: [-719.016 -719.016 -719.016] (1.000)
Step: 15749, Reward: [-910.036 -910.036 -910.036] [0.0000], Avg: [-719.623 -719.623 -719.623] (1.000)
Step: 15799, Reward: [-547.775 -547.775 -547.775] [0.0000], Avg: [-719.079 -719.079 -719.079] (1.000)
Step: 15849, Reward: [-938.138 -938.138 -938.138] [0.0000], Avg: [-719.77 -719.77 -719.77] (1.000)
Step: 15899, Reward: [-592.07 -592.07 -592.07] [0.0000], Avg: [-719.368 -719.368 -719.368] (1.000)
Step: 15949, Reward: [-570.703 -570.703 -570.703] [0.0000], Avg: [-718.902 -718.902 -718.902] (1.000)
Step: 15999, Reward: [-682.323 -682.323 -682.323] [0.0000], Avg: [-718.788 -718.788 -718.788] (1.000)
Step: 16049, Reward: [-658.648 -658.648 -658.648] [0.0000], Avg: [-718.601 -718.601 -718.601] (1.000)
Step: 16099, Reward: [-736.478 -736.478 -736.478] [0.0000], Avg: [-718.656 -718.656 -718.656] (1.000)
Step: 16149, Reward: [-550.062 -550.062 -550.062] [0.0000], Avg: [-718.134 -718.134 -718.134] (1.000)
Step: 16199, Reward: [-663.882 -663.882 -663.882] [0.0000], Avg: [-717.967 -717.967 -717.967] (1.000)
Step: 16249, Reward: [-1037.904 -1037.904 -1037.904] [0.0000], Avg: [-718.951 -718.951 -718.951] (1.000)
Step: 16299, Reward: [-670.035 -670.035 -670.035] [0.0000], Avg: [-718.801 -718.801 -718.801] (1.000)
Step: 16349, Reward: [-675.677 -675.677 -675.677] [0.0000], Avg: [-718.669 -718.669 -718.669] (1.000)
Step: 16399, Reward: [-1211.487 -1211.487 -1211.487] [0.0000], Avg: [-720.172 -720.172 -720.172] (1.000)
Step: 16449, Reward: [-739.795 -739.795 -739.795] [0.0000], Avg: [-720.231 -720.231 -720.231] (1.000)
Step: 16499, Reward: [-1145.894 -1145.894 -1145.894] [0.0000], Avg: [-721.521 -721.521 -721.521] (1.000)
Step: 16549, Reward: [-871.643 -871.643 -871.643] [0.0000], Avg: [-721.975 -721.975 -721.975] (1.000)
Step: 16599, Reward: [-486.986 -486.986 -486.986] [0.0000], Avg: [-721.267 -721.267 -721.267] (1.000)
Step: 16649, Reward: [-722.636 -722.636 -722.636] [0.0000], Avg: [-721.271 -721.271 -721.271] (1.000)
Step: 16699, Reward: [-807.241 -807.241 -807.241] [0.0000], Avg: [-721.528 -721.528 -721.528] (1.000)
Step: 16749, Reward: [-931.166 -931.166 -931.166] [0.0000], Avg: [-722.154 -722.154 -722.154] (1.000)
Step: 16799, Reward: [-838.925 -838.925 -838.925] [0.0000], Avg: [-722.502 -722.502 -722.502] (1.000)
Step: 16849, Reward: [-833.879 -833.879 -833.879] [0.0000], Avg: [-722.832 -722.832 -722.832] (1.000)
Step: 16899, Reward: [-1549.186 -1549.186 -1549.186] [0.0000], Avg: [-725.277 -725.277 -725.277] (1.000)
Step: 16949, Reward: [-904.715 -904.715 -904.715] [0.0000], Avg: [-725.806 -725.806 -725.806] (1.000)
Step: 16999, Reward: [-990.83 -990.83 -990.83] [0.0000], Avg: [-726.586 -726.586 -726.586] (1.000)
Step: 17049, Reward: [-926.375 -926.375 -926.375] [0.0000], Avg: [-727.172 -727.172 -727.172] (1.000)
Step: 17099, Reward: [-1753.012 -1753.012 -1753.012] [0.0000], Avg: [-730.171 -730.171 -730.171] (1.000)
Step: 17149, Reward: [-1008.438 -1008.438 -1008.438] [0.0000], Avg: [-730.983 -730.983 -730.983] (1.000)
Step: 17199, Reward: [-334.003 -334.003 -334.003] [0.0000], Avg: [-729.829 -729.829 -729.829] (1.000)
Step: 17249, Reward: [-1194.47 -1194.47 -1194.47] [0.0000], Avg: [-731.175 -731.175 -731.175] (1.000)
Step: 17299, Reward: [-609.172 -609.172 -609.172] [0.0000], Avg: [-730.823 -730.823 -730.823] (1.000)
Step: 17349, Reward: [-851.183 -851.183 -851.183] [0.0000], Avg: [-731.17 -731.17 -731.17] (1.000)
Step: 17399, Reward: [-761.054 -761.054 -761.054] [0.0000], Avg: [-731.255 -731.255 -731.255] (1.000)
Step: 17449, Reward: [-640.567 -640.567 -640.567] [0.0000], Avg: [-730.996 -730.996 -730.996] (1.000)
Step: 17499, Reward: [-943.073 -943.073 -943.073] [0.0000], Avg: [-731.602 -731.602 -731.602] (1.000)
Step: 17549, Reward: [-1441.921 -1441.921 -1441.921] [0.0000], Avg: [-733.625 -733.625 -733.625] (1.000)
Step: 17599, Reward: [-579.568 -579.568 -579.568] [0.0000], Avg: [-733.188 -733.188 -733.188] (1.000)
Step: 17649, Reward: [-1314.006 -1314.006 -1314.006] [0.0000], Avg: [-734.833 -734.833 -734.833] (1.000)
Step: 17699, Reward: [-941.56 -941.56 -941.56] [0.0000], Avg: [-735.417 -735.417 -735.417] (1.000)
Step: 17749, Reward: [-1396.169 -1396.169 -1396.169] [0.0000], Avg: [-737.278 -737.278 -737.278] (1.000)
Step: 17799, Reward: [-1081.761 -1081.761 -1081.761] [0.0000], Avg: [-738.246 -738.246 -738.246] (1.000)
Step: 17849, Reward: [-695.55 -695.55 -695.55] [0.0000], Avg: [-738.126 -738.126 -738.126] (1.000)
Step: 17899, Reward: [-1517.774 -1517.774 -1517.774] [0.0000], Avg: [-740.304 -740.304 -740.304] (1.000)
Step: 17949, Reward: [-678.788 -678.788 -678.788] [0.0000], Avg: [-740.133 -740.133 -740.133] (1.000)
Step: 17999, Reward: [-1197.764 -1197.764 -1197.764] [0.0000], Avg: [-741.404 -741.404 -741.404] (1.000)
Step: 18049, Reward: [-1063.65 -1063.65 -1063.65] [0.0000], Avg: [-742.297 -742.297 -742.297] (1.000)
Step: 18099, Reward: [-810.316 -810.316 -810.316] [0.0000], Avg: [-742.484 -742.484 -742.484] (1.000)
Step: 18149, Reward: [-601.081 -601.081 -601.081] [0.0000], Avg: [-742.095 -742.095 -742.095] (1.000)
Step: 18199, Reward: [-1436.97 -1436.97 -1436.97] [0.0000], Avg: [-744.004 -744.004 -744.004] (1.000)
Step: 18249, Reward: [-1536.094 -1536.094 -1536.094] [0.0000], Avg: [-746.174 -746.174 -746.174] (1.000)
Step: 18299, Reward: [-748.384 -748.384 -748.384] [0.0000], Avg: [-746.18 -746.18 -746.18] (1.000)
Step: 18349, Reward: [-1345.753 -1345.753 -1345.753] [0.0000], Avg: [-747.814 -747.814 -747.814] (1.000)
Step: 18399, Reward: [-1009.704 -1009.704 -1009.704] [0.0000], Avg: [-748.525 -748.525 -748.525] (1.000)
Step: 18449, Reward: [-687.506 -687.506 -687.506] [0.0000], Avg: [-748.36 -748.36 -748.36] (1.000)
Step: 18499, Reward: [-454.566 -454.566 -454.566] [0.0000], Avg: [-747.566 -747.566 -747.566] (1.000)
Step: 18549, Reward: [-1053.305 -1053.305 -1053.305] [0.0000], Avg: [-748.39 -748.39 -748.39] (1.000)
Step: 18599, Reward: [-1001.898 -1001.898 -1001.898] [0.0000], Avg: [-749.072 -749.072 -749.072] (1.000)
Step: 18649, Reward: [-1301.375 -1301.375 -1301.375] [0.0000], Avg: [-750.552 -750.552 -750.552] (1.000)
Step: 18699, Reward: [-1008.552 -1008.552 -1008.552] [0.0000], Avg: [-751.242 -751.242 -751.242] (1.000)
Step: 18749, Reward: [-476.91 -476.91 -476.91] [0.0000], Avg: [-750.511 -750.511 -750.511] (1.000)
Step: 18799, Reward: [-1586.69 -1586.69 -1586.69] [0.0000], Avg: [-752.734 -752.734 -752.734] (1.000)
Step: 18849, Reward: [-1331.151 -1331.151 -1331.151] [0.0000], Avg: [-754.269 -754.269 -754.269] (1.000)
Step: 18899, Reward: [-1195.745 -1195.745 -1195.745] [0.0000], Avg: [-755.437 -755.437 -755.437] (1.000)
Step: 18949, Reward: [-1207.92 -1207.92 -1207.92] [0.0000], Avg: [-756.631 -756.631 -756.631] (1.000)
Step: 18999, Reward: [-921.136 -921.136 -921.136] [0.0000], Avg: [-757.063 -757.063 -757.063] (1.000)
Step: 19049, Reward: [-558.261 -558.261 -558.261] [0.0000], Avg: [-756.542 -756.542 -756.542] (1.000)
Step: 19099, Reward: [-678.754 -678.754 -678.754] [0.0000], Avg: [-756.338 -756.338 -756.338] (1.000)
Step: 19149, Reward: [-757.585 -757.585 -757.585] [0.0000], Avg: [-756.341 -756.341 -756.341] (1.000)
Step: 19199, Reward: [-994.984 -994.984 -994.984] [0.0000], Avg: [-756.963 -756.963 -756.963] (1.000)
Step: 19249, Reward: [-1271.4 -1271.4 -1271.4] [0.0000], Avg: [-758.299 -758.299 -758.299] (1.000)
Step: 19299, Reward: [-896.656 -896.656 -896.656] [0.0000], Avg: [-758.657 -758.657 -758.657] (1.000)
Step: 19349, Reward: [-1479.755 -1479.755 -1479.755] [0.0000], Avg: [-760.521 -760.521 -760.521] (1.000)
Step: 19399, Reward: [-1212.253 -1212.253 -1212.253] [0.0000], Avg: [-761.685 -761.685 -761.685] (1.000)
Step: 19449, Reward: [-552.188 -552.188 -552.188] [0.0000], Avg: [-761.146 -761.146 -761.146] (1.000)
Step: 19499, Reward: [-1164.376 -1164.376 -1164.376] [0.0000], Avg: [-762.18 -762.18 -762.18] (1.000)
Step: 19549, Reward: [-493.332 -493.332 -493.332] [0.0000], Avg: [-761.493 -761.493 -761.493] (1.000)
Step: 19599, Reward: [-765.3 -765.3 -765.3] [0.0000], Avg: [-761.502 -761.502 -761.502] (1.000)
Step: 19649, Reward: [-545.608 -545.608 -545.608] [0.0000], Avg: [-760.953 -760.953 -760.953] (1.000)
Step: 19699, Reward: [-446.225 -446.225 -446.225] [0.0000], Avg: [-760.154 -760.154 -760.154] (1.000)
Step: 19749, Reward: [-589.93 -589.93 -589.93] [0.0000], Avg: [-759.723 -759.723 -759.723] (1.000)
Step: 19799, Reward: [-522.663 -522.663 -522.663] [0.0000], Avg: [-759.125 -759.125 -759.125] (1.000)
Step: 19849, Reward: [-409.33 -409.33 -409.33] [0.0000], Avg: [-758.244 -758.244 -758.244] (1.000)
Step: 19899, Reward: [-567.991 -567.991 -567.991] [0.0000], Avg: [-757.766 -757.766 -757.766] (1.000)
Step: 19949, Reward: [-395.491 -395.491 -395.491] [0.0000], Avg: [-756.858 -756.858 -756.858] (1.000)
Step: 19999, Reward: [-775.813 -775.813 -775.813] [0.0000], Avg: [-756.905 -756.905 -756.905] (1.000)
Step: 20049, Reward: [-859.775 -859.775 -859.775] [0.0000], Avg: [-757.162 -757.162 -757.162] (1.000)
Step: 20099, Reward: [-592.958 -592.958 -592.958] [0.0000], Avg: [-756.753 -756.753 -756.753] (1.000)
Step: 20149, Reward: [-479.875 -479.875 -479.875] [0.0000], Avg: [-756.066 -756.066 -756.066] (1.000)
Step: 20199, Reward: [-498.474 -498.474 -498.474] [0.0000], Avg: [-755.428 -755.428 -755.428] (1.000)
Step: 20249, Reward: [-797.158 -797.158 -797.158] [0.0000], Avg: [-755.531 -755.531 -755.531] (1.000)
Step: 20299, Reward: [-673.209 -673.209 -673.209] [0.0000], Avg: [-755.329 -755.329 -755.329] (1.000)
Step: 20349, Reward: [-722.108 -722.108 -722.108] [0.0000], Avg: [-755.247 -755.247 -755.247] (1.000)
Step: 20399, Reward: [-516.508 -516.508 -516.508] [0.0000], Avg: [-754.662 -754.662 -754.662] (1.000)
Step: 20449, Reward: [-674.231 -674.231 -674.231] [0.0000], Avg: [-754.465 -754.465 -754.465] (1.000)
Step: 20499, Reward: [-686.66 -686.66 -686.66] [0.0000], Avg: [-754.3 -754.3 -754.3] (1.000)
Step: 20549, Reward: [-556.808 -556.808 -556.808] [0.0000], Avg: [-753.819 -753.819 -753.819] (1.000)
Step: 20599, Reward: [-588.3 -588.3 -588.3] [0.0000], Avg: [-753.418 -753.418 -753.418] (1.000)
Step: 20649, Reward: [-1076.883 -1076.883 -1076.883] [0.0000], Avg: [-754.201 -754.201 -754.201] (1.000)
Step: 20699, Reward: [-798.256 -798.256 -798.256] [0.0000], Avg: [-754.307 -754.307 -754.307] (1.000)
Step: 20749, Reward: [-840.31 -840.31 -840.31] [0.0000], Avg: [-754.514 -754.514 -754.514] (1.000)
Step: 20799, Reward: [-680.425 -680.425 -680.425] [0.0000], Avg: [-754.336 -754.336 -754.336] (1.000)
Step: 20849, Reward: [-1479.322 -1479.322 -1479.322] [0.0000], Avg: [-756.075 -756.075 -756.075] (1.000)
Step: 20899, Reward: [-905.982 -905.982 -905.982] [0.0000], Avg: [-756.434 -756.434 -756.434] (1.000)
Step: 20949, Reward: [-1003.271 -1003.271 -1003.271] [0.0000], Avg: [-757.023 -757.023 -757.023] (1.000)
Step: 20999, Reward: [-784.911 -784.911 -784.911] [0.0000], Avg: [-757.089 -757.089 -757.089] (1.000)
Step: 21049, Reward: [-938.401 -938.401 -938.401] [0.0000], Avg: [-757.52 -757.52 -757.52] (1.000)
Step: 21099, Reward: [-448.237 -448.237 -448.237] [0.0000], Avg: [-756.787 -756.787 -756.787] (1.000)
Step: 21149, Reward: [-1511.472 -1511.472 -1511.472] [0.0000], Avg: [-758.571 -758.571 -758.571] (1.000)
Step: 21199, Reward: [-545.425 -545.425 -545.425] [0.0000], Avg: [-758.068 -758.068 -758.068] (1.000)
Step: 21249, Reward: [-555.475 -555.475 -555.475] [0.0000], Avg: [-757.592 -757.592 -757.592] (1.000)
Step: 21299, Reward: [-531.685 -531.685 -531.685] [0.0000], Avg: [-757.061 -757.061 -757.061] (1.000)
Step: 21349, Reward: [-709.837 -709.837 -709.837] [0.0000], Avg: [-756.951 -756.951 -756.951] (1.000)
Step: 21399, Reward: [-1571.82 -1571.82 -1571.82] [0.0000], Avg: [-758.855 -758.855 -758.855] (1.000)
Step: 21449, Reward: [-1200.56 -1200.56 -1200.56] [0.0000], Avg: [-759.884 -759.884 -759.884] (1.000)
Step: 21499, Reward: [-1105.739 -1105.739 -1105.739] [0.0000], Avg: [-760.689 -760.689 -760.689] (1.000)
Step: 21549, Reward: [-1292.021 -1292.021 -1292.021] [0.0000], Avg: [-761.921 -761.921 -761.921] (1.000)
Step: 21599, Reward: [-534.569 -534.569 -534.569] [0.0000], Avg: [-761.395 -761.395 -761.395] (1.000)
Step: 21649, Reward: [-1430.946 -1430.946 -1430.946] [0.0000], Avg: [-762.941 -762.941 -762.941] (1.000)
Step: 21699, Reward: [-943.628 -943.628 -943.628] [0.0000], Avg: [-763.358 -763.358 -763.358] (1.000)
Step: 21749, Reward: [-974.246 -974.246 -974.246] [0.0000], Avg: [-763.842 -763.842 -763.842] (1.000)
Step: 21799, Reward: [-699.26 -699.26 -699.26] [0.0000], Avg: [-763.694 -763.694 -763.694] (1.000)
Step: 21849, Reward: [-707.325 -707.325 -707.325] [0.0000], Avg: [-763.565 -763.565 -763.565] (1.000)
Step: 21899, Reward: [-1263.435 -1263.435 -1263.435] [0.0000], Avg: [-764.707 -764.707 -764.707] (1.000)
Step: 21949, Reward: [-649.966 -649.966 -649.966] [0.0000], Avg: [-764.445 -764.445 -764.445] (1.000)
Step: 21999, Reward: [-491.9 -491.9 -491.9] [0.0000], Avg: [-763.826 -763.826 -763.826] (1.000)
Step: 22049, Reward: [-1272.142 -1272.142 -1272.142] [0.0000], Avg: [-764.978 -764.978 -764.978] (1.000)
Step: 22099, Reward: [-723.694 -723.694 -723.694] [0.0000], Avg: [-764.885 -764.885 -764.885] (1.000)
Step: 22149, Reward: [-902.781 -902.781 -902.781] [0.0000], Avg: [-765.196 -765.196 -765.196] (1.000)
Step: 22199, Reward: [-661.207 -661.207 -661.207] [0.0000], Avg: [-764.962 -764.962 -764.962] (1.000)
Step: 22249, Reward: [-1120.67 -1120.67 -1120.67] [0.0000], Avg: [-765.761 -765.761 -765.761] (1.000)
Step: 22299, Reward: [-945.27 -945.27 -945.27] [0.0000], Avg: [-766.164 -766.164 -766.164] (1.000)
Step: 22349, Reward: [-927.505 -927.505 -927.505] [0.0000], Avg: [-766.525 -766.525 -766.525] (1.000)
Step: 22399, Reward: [-1567.745 -1567.745 -1567.745] [0.0000], Avg: [-768.313 -768.313 -768.313] (1.000)
Step: 22449, Reward: [-985.422 -985.422 -985.422] [0.0000], Avg: [-768.797 -768.797 -768.797] (1.000)
Step: 22499, Reward: [-1054.154 -1054.154 -1054.154] [0.0000], Avg: [-769.431 -769.431 -769.431] (1.000)
Step: 22549, Reward: [-661.342 -661.342 -661.342] [0.0000], Avg: [-769.191 -769.191 -769.191] (1.000)
Step: 22599, Reward: [-714.851 -714.851 -714.851] [0.0000], Avg: [-769.071 -769.071 -769.071] (1.000)
Step: 22649, Reward: [-548.088 -548.088 -548.088] [0.0000], Avg: [-768.583 -768.583 -768.583] (1.000)
Step: 22699, Reward: [-1344.044 -1344.044 -1344.044] [0.0000], Avg: [-769.851 -769.851 -769.851] (1.000)
Step: 22749, Reward: [-2030.209 -2030.209 -2030.209] [0.0000], Avg: [-772.621 -772.621 -772.621] (1.000)
Step: 22799, Reward: [-763.762 -763.762 -763.762] [0.0000], Avg: [-772.601 -772.601 -772.601] (1.000)
Step: 22849, Reward: [-1915.949 -1915.949 -1915.949] [0.0000], Avg: [-775.103 -775.103 -775.103] (1.000)
Step: 22899, Reward: [-874.366 -874.366 -874.366] [0.0000], Avg: [-775.32 -775.32 -775.32] (1.000)
Step: 22949, Reward: [-737.692 -737.692 -737.692] [0.0000], Avg: [-775.238 -775.238 -775.238] (1.000)
Step: 22999, Reward: [-832.198 -832.198 -832.198] [0.0000], Avg: [-775.362 -775.362 -775.362] (1.000)
Step: 23049, Reward: [-563.27 -563.27 -563.27] [0.0000], Avg: [-774.902 -774.902 -774.902] (1.000)
Step: 23099, Reward: [-771.786 -771.786 -771.786] [0.0000], Avg: [-774.895 -774.895 -774.895] (1.000)
Step: 23149, Reward: [-1043.569 -1043.569 -1043.569] [0.0000], Avg: [-775.475 -775.475 -775.475] (1.000)
Step: 23199, Reward: [-871.16 -871.16 -871.16] [0.0000], Avg: [-775.682 -775.682 -775.682] (1.000)
Step: 23249, Reward: [-716.841 -716.841 -716.841] [0.0000], Avg: [-775.555 -775.555 -775.555] (1.000)
Step: 23299, Reward: [-1354.182 -1354.182 -1354.182] [0.0000], Avg: [-776.797 -776.797 -776.797] (1.000)
Step: 23349, Reward: [-1368.658 -1368.658 -1368.658] [0.0000], Avg: [-778.064 -778.064 -778.064] (1.000)
Step: 23399, Reward: [-750.917 -750.917 -750.917] [0.0000], Avg: [-778.006 -778.006 -778.006] (1.000)
Step: 23449, Reward: [-793.92 -793.92 -793.92] [0.0000], Avg: [-778.04 -778.04 -778.04] (1.000)
Step: 23499, Reward: [-1006.643 -1006.643 -1006.643] [0.0000], Avg: [-778.526 -778.526 -778.526] (1.000)
Step: 23549, Reward: [-1391.085 -1391.085 -1391.085] [0.0000], Avg: [-779.827 -779.827 -779.827] (1.000)
Step: 23599, Reward: [-961.628 -961.628 -961.628] [0.0000], Avg: [-780.212 -780.212 -780.212] (1.000)
Step: 23649, Reward: [-940.136 -940.136 -940.136] [0.0000], Avg: [-780.55 -780.55 -780.55] (1.000)
Step: 23699, Reward: [-1351.889 -1351.889 -1351.889] [0.0000], Avg: [-781.756 -781.756 -781.756] (1.000)
Step: 23749, Reward: [-1090.363 -1090.363 -1090.363] [0.0000], Avg: [-782.405 -782.405 -782.405] (1.000)
Step: 23799, Reward: [-795.179 -795.179 -795.179] [0.0000], Avg: [-782.432 -782.432 -782.432] (1.000)
Step: 23849, Reward: [-755.068 -755.068 -755.068] [0.0000], Avg: [-782.375 -782.375 -782.375] (1.000)
Step: 23899, Reward: [-687.892 -687.892 -687.892] [0.0000], Avg: [-782.177 -782.177 -782.177] (1.000)
Step: 23949, Reward: [-588.188 -588.188 -588.188] [0.0000], Avg: [-781.772 -781.772 -781.772] (1.000)
Step: 23999, Reward: [-648.734 -648.734 -648.734] [0.0000], Avg: [-781.495 -781.495 -781.495] (1.000)
Step: 24049, Reward: [-1132.199 -1132.199 -1132.199] [0.0000], Avg: [-782.224 -782.224 -782.224] (1.000)
Step: 24099, Reward: [-1142.027 -1142.027 -1142.027] [0.0000], Avg: [-782.971 -782.971 -782.971] (1.000)
Step: 24149, Reward: [-886.894 -886.894 -886.894] [0.0000], Avg: [-783.186 -783.186 -783.186] (1.000)
Step: 24199, Reward: [-683.525 -683.525 -683.525] [0.0000], Avg: [-782.98 -782.98 -782.98] (1.000)
Step: 24249, Reward: [-820.818 -820.818 -820.818] [0.0000], Avg: [-783.058 -783.058 -783.058] (1.000)
Step: 24299, Reward: [-594.888 -594.888 -594.888] [0.0000], Avg: [-782.671 -782.671 -782.671] (1.000)
Step: 24349, Reward: [-1451.413 -1451.413 -1451.413] [0.0000], Avg: [-784.044 -784.044 -784.044] (1.000)
Step: 24399, Reward: [-758.106 -758.106 -758.106] [0.0000], Avg: [-783.991 -783.991 -783.991] (1.000)
Step: 24449, Reward: [-712.591 -712.591 -712.591] [0.0000], Avg: [-783.845 -783.845 -783.845] (1.000)
Step: 24499, Reward: [-525.588 -525.588 -525.588] [0.0000], Avg: [-783.318 -783.318 -783.318] (1.000)
Step: 24549, Reward: [-596.982 -596.982 -596.982] [0.0000], Avg: [-782.938 -782.938 -782.938] (1.000)
Step: 24599, Reward: [-2193.075 -2193.075 -2193.075] [0.0000], Avg: [-785.804 -785.804 -785.804] (1.000)
Step: 24649, Reward: [-731.326 -731.326 -731.326] [0.0000], Avg: [-785.694 -785.694 -785.694] (1.000)
Step: 24699, Reward: [-1203.659 -1203.659 -1203.659] [0.0000], Avg: [-786.54 -786.54 -786.54] (1.000)
Step: 24749, Reward: [-934.49 -934.49 -934.49] [0.0000], Avg: [-786.839 -786.839 -786.839] (1.000)
Step: 24799, Reward: [-760.644 -760.644 -760.644] [0.0000], Avg: [-786.786 -786.786 -786.786] (1.000)
Step: 24849, Reward: [-1458.865 -1458.865 -1458.865] [0.0000], Avg: [-788.138 -788.138 -788.138] (1.000)
Step: 24899, Reward: [-597.341 -597.341 -597.341] [0.0000], Avg: [-787.755 -787.755 -787.755] (1.000)
Step: 24949, Reward: [-825.443 -825.443 -825.443] [0.0000], Avg: [-787.831 -787.831 -787.831] (1.000)
Step: 24999, Reward: [-695.866 -695.866 -695.866] [0.0000], Avg: [-787.647 -787.647 -787.647] (1.000)
Step: 25049, Reward: [-972.035 -972.035 -972.035] [0.0000], Avg: [-788.015 -788.015 -788.015] (1.000)
Step: 25099, Reward: [-1319.097 -1319.097 -1319.097] [0.0000], Avg: [-789.073 -789.073 -789.073] (1.000)
Step: 25149, Reward: [-461.936 -461.936 -461.936] [0.0000], Avg: [-788.422 -788.422 -788.422] (1.000)
Step: 25199, Reward: [-887.622 -887.622 -887.622] [0.0000], Avg: [-788.619 -788.619 -788.619] (1.000)
Step: 25249, Reward: [-570.43 -570.43 -570.43] [0.0000], Avg: [-788.187 -788.187 -788.187] (1.000)
Step: 25299, Reward: [-721.864 -721.864 -721.864] [0.0000], Avg: [-788.056 -788.056 -788.056] (1.000)
Step: 25349, Reward: [-546.517 -546.517 -546.517] [0.0000], Avg: [-787.58 -787.58 -787.58] (1.000)
Step: 25399, Reward: [-736.288 -736.288 -736.288] [0.0000], Avg: [-787.479 -787.479 -787.479] (1.000)
Step: 25449, Reward: [-388.454 -388.454 -388.454] [0.0000], Avg: [-786.695 -786.695 -786.695] (1.000)
Step: 25499, Reward: [-662.741 -662.741 -662.741] [0.0000], Avg: [-786.452 -786.452 -786.452] (1.000)
Step: 25549, Reward: [-808.204 -808.204 -808.204] [0.0000], Avg: [-786.494 -786.494 -786.494] (1.000)
Step: 25599, Reward: [-1194.146 -1194.146 -1194.146] [0.0000], Avg: [-787.29 -787.29 -787.29] (1.000)
Step: 25649, Reward: [-514.409 -514.409 -514.409] [0.0000], Avg: [-786.758 -786.758 -786.758] (1.000)
Step: 25699, Reward: [-776.004 -776.004 -776.004] [0.0000], Avg: [-786.737 -786.737 -786.737] (1.000)
Step: 25749, Reward: [-1136.443 -1136.443 -1136.443] [0.0000], Avg: [-787.416 -787.416 -787.416] (1.000)
Step: 25799, Reward: [-673.537 -673.537 -673.537] [0.0000], Avg: [-787.196 -787.196 -787.196] (1.000)
Step: 25849, Reward: [-645.451 -645.451 -645.451] [0.0000], Avg: [-786.922 -786.922 -786.922] (1.000)
Step: 25899, Reward: [-787.978 -787.978 -787.978] [0.0000], Avg: [-786.924 -786.924 -786.924] (1.000)
Step: 25949, Reward: [-654.024 -654.024 -654.024] [0.0000], Avg: [-786.668 -786.668 -786.668] (1.000)
Step: 25999, Reward: [-682.693 -682.693 -682.693] [0.0000], Avg: [-786.468 -786.468 -786.468] (1.000)
Step: 26049, Reward: [-808.145 -808.145 -808.145] [0.0000], Avg: [-786.509 -786.509 -786.509] (1.000)
Step: 26099, Reward: [-682.704 -682.704 -682.704] [0.0000], Avg: [-786.31 -786.31 -786.31] (1.000)
Step: 26149, Reward: [-930.977 -930.977 -930.977] [0.0000], Avg: [-786.587 -786.587 -786.587] (1.000)
Step: 26199, Reward: [-850.632 -850.632 -850.632] [0.0000], Avg: [-786.709 -786.709 -786.709] (1.000)
Step: 26249, Reward: [-555.616 -555.616 -555.616] [0.0000], Avg: [-786.269 -786.269 -786.269] (1.000)
Step: 26299, Reward: [-825.882 -825.882 -825.882] [0.0000], Avg: [-786.344 -786.344 -786.344] (1.000)
Step: 26349, Reward: [-781.705 -781.705 -781.705] [0.0000], Avg: [-786.336 -786.336 -786.336] (1.000)
Step: 26399, Reward: [-738.035 -738.035 -738.035] [0.0000], Avg: [-786.244 -786.244 -786.244] (1.000)
Step: 26449, Reward: [-916.197 -916.197 -916.197] [0.0000], Avg: [-786.49 -786.49 -786.49] (1.000)
Step: 26499, Reward: [-760.058 -760.058 -760.058] [0.0000], Avg: [-786.44 -786.44 -786.44] (1.000)
Step: 26549, Reward: [-423.12 -423.12 -423.12] [0.0000], Avg: [-785.756 -785.756 -785.756] (1.000)
Step: 26599, Reward: [-505.344 -505.344 -505.344] [0.0000], Avg: [-785.229 -785.229 -785.229] (1.000)
Step: 26649, Reward: [-1284.744 -1284.744 -1284.744] [0.0000], Avg: [-786.166 -786.166 -786.166] (1.000)
Step: 26699, Reward: [-788.531 -788.531 -788.531] [0.0000], Avg: [-786.17 -786.17 -786.17] (1.000)
Step: 26749, Reward: [-637.491 -637.491 -637.491] [0.0000], Avg: [-785.892 -785.892 -785.892] (1.000)
Step: 26799, Reward: [-820.162 -820.162 -820.162] [0.0000], Avg: [-785.956 -785.956 -785.956] (1.000)
Step: 26849, Reward: [-1084.917 -1084.917 -1084.917] [0.0000], Avg: [-786.513 -786.513 -786.513] (1.000)
Step: 26899, Reward: [-556.441 -556.441 -556.441] [0.0000], Avg: [-786.085 -786.085 -786.085] (1.000)
Step: 26949, Reward: [-645.059 -645.059 -645.059] [0.0000], Avg: [-785.824 -785.824 -785.824] (1.000)
Step: 26999, Reward: [-954.259 -954.259 -954.259] [0.0000], Avg: [-786.136 -786.136 -786.136] (1.000)
Step: 27049, Reward: [-798.864 -798.864 -798.864] [0.0000], Avg: [-786.159 -786.159 -786.159] (1.000)
Step: 27099, Reward: [-695.593 -695.593 -695.593] [0.0000], Avg: [-785.992 -785.992 -785.992] (1.000)
Step: 27149, Reward: [-1082.209 -1082.209 -1082.209] [0.0000], Avg: [-786.538 -786.538 -786.538] (1.000)
Step: 27199, Reward: [-896.694 -896.694 -896.694] [0.0000], Avg: [-786.74 -786.74 -786.74] (1.000)
Step: 27249, Reward: [-748.643 -748.643 -748.643] [0.0000], Avg: [-786.67 -786.67 -786.67] (1.000)
Step: 27299, Reward: [-783.155 -783.155 -783.155] [0.0000], Avg: [-786.664 -786.664 -786.664] (1.000)
Step: 27349, Reward: [-752.23 -752.23 -752.23] [0.0000], Avg: [-786.601 -786.601 -786.601] (1.000)
Step: 27399, Reward: [-1089.938 -1089.938 -1089.938] [0.0000], Avg: [-787.154 -787.154 -787.154] (1.000)
Step: 27449, Reward: [-849.065 -849.065 -849.065] [0.0000], Avg: [-787.267 -787.267 -787.267] (1.000)
Step: 27499, Reward: [-975.668 -975.668 -975.668] [0.0000], Avg: [-787.61 -787.61 -787.61] (1.000)
Step: 27549, Reward: [-1531.211 -1531.211 -1531.211] [0.0000], Avg: [-788.959 -788.959 -788.959] (1.000)
Step: 27599, Reward: [-393.655 -393.655 -393.655] [0.0000], Avg: [-788.243 -788.243 -788.243] (1.000)
Step: 27649, Reward: [-585.901 -585.901 -585.901] [0.0000], Avg: [-787.877 -787.877 -787.877] (1.000)
Step: 27699, Reward: [-1133.129 -1133.129 -1133.129] [0.0000], Avg: [-788.5 -788.5 -788.5] (1.000)
Step: 27749, Reward: [-1036.982 -1036.982 -1036.982] [0.0000], Avg: [-788.948 -788.948 -788.948] (1.000)
Step: 27799, Reward: [-1196.834 -1196.834 -1196.834] [0.0000], Avg: [-789.682 -789.682 -789.682] (1.000)
Step: 27849, Reward: [-713.481 -713.481 -713.481] [0.0000], Avg: [-789.545 -789.545 -789.545] (1.000)
Step: 27899, Reward: [-910.341 -910.341 -910.341] [0.0000], Avg: [-789.761 -789.761 -789.761] (1.000)
Step: 27949, Reward: [-762.57 -762.57 -762.57] [0.0000], Avg: [-789.713 -789.713 -789.713] (1.000)
Step: 27999, Reward: [-973.709 -973.709 -973.709] [0.0000], Avg: [-790.041 -790.041 -790.041] (1.000)
Step: 28049, Reward: [-932.909 -932.909 -932.909] [0.0000], Avg: [-790.296 -790.296 -790.296] (1.000)
Step: 28099, Reward: [-1366.731 -1366.731 -1366.731] [0.0000], Avg: [-791.322 -791.322 -791.322] (1.000)
Step: 28149, Reward: [-663.154 -663.154 -663.154] [0.0000], Avg: [-791.094 -791.094 -791.094] (1.000)
Step: 28199, Reward: [-699.576 -699.576 -699.576] [0.0000], Avg: [-790.932 -790.932 -790.932] (1.000)
Step: 28249, Reward: [-785.349 -785.349 -785.349] [0.0000], Avg: [-790.922 -790.922 -790.922] (1.000)
Step: 28299, Reward: [-1110.668 -1110.668 -1110.668] [0.0000], Avg: [-791.487 -791.487 -791.487] (1.000)
Step: 28349, Reward: [-976.16 -976.16 -976.16] [0.0000], Avg: [-791.812 -791.812 -791.812] (1.000)
Step: 28399, Reward: [-435.1 -435.1 -435.1] [0.0000], Avg: [-791.184 -791.184 -791.184] (1.000)
Step: 28449, Reward: [-684.235 -684.235 -684.235] [0.0000], Avg: [-790.996 -790.996 -790.996] (1.000)
Step: 28499, Reward: [-713.773 -713.773 -713.773] [0.0000], Avg: [-790.861 -790.861 -790.861] (1.000)
Step: 28549, Reward: [-768.765 -768.765 -768.765] [0.0000], Avg: [-790.822 -790.822 -790.822] (1.000)
Step: 28599, Reward: [-1109.173 -1109.173 -1109.173] [0.0000], Avg: [-791.379 -791.379 -791.379] (1.000)
Step: 28649, Reward: [-1124.588 -1124.588 -1124.588] [0.0000], Avg: [-791.96 -791.96 -791.96] (1.000)
Step: 28699, Reward: [-536.163 -536.163 -536.163] [0.0000], Avg: [-791.515 -791.515 -791.515] (1.000)
Step: 28749, Reward: [-558.719 -558.719 -558.719] [0.0000], Avg: [-791.11 -791.11 -791.11] (1.000)
Step: 28799, Reward: [-636.655 -636.655 -636.655] [0.0000], Avg: [-790.842 -790.842 -790.842] (1.000)
Step: 28849, Reward: [-1103.811 -1103.811 -1103.811] [0.0000], Avg: [-791.384 -791.384 -791.384] (1.000)
Step: 28899, Reward: [-876.061 -876.061 -876.061] [0.0000], Avg: [-791.531 -791.531 -791.531] (1.000)
Step: 28949, Reward: [-463.555 -463.555 -463.555] [0.0000], Avg: [-790.964 -790.964 -790.964] (1.000)
Step: 28999, Reward: [-753.862 -753.862 -753.862] [0.0000], Avg: [-790.9 -790.9 -790.9] (1.000)
Step: 29049, Reward: [-723.012 -723.012 -723.012] [0.0000], Avg: [-790.783 -790.783 -790.783] (1.000)
Step: 29099, Reward: [-530.618 -530.618 -530.618] [0.0000], Avg: [-790.336 -790.336 -790.336] (1.000)
Step: 29149, Reward: [-856.643 -856.643 -856.643] [0.0000], Avg: [-790.45 -790.45 -790.45] (1.000)
Step: 29199, Reward: [-697.435 -697.435 -697.435] [0.0000], Avg: [-790.291 -790.291 -790.291] (1.000)
Step: 29249, Reward: [-854.71 -854.71 -854.71] [0.0000], Avg: [-790.401 -790.401 -790.401] (1.000)
Step: 29299, Reward: [-909.838 -909.838 -909.838] [0.0000], Avg: [-790.605 -790.605 -790.605] (1.000)
Step: 29349, Reward: [-1061.428 -1061.428 -1061.428] [0.0000], Avg: [-791.066 -791.066 -791.066] (1.000)
Step: 29399, Reward: [-513.401 -513.401 -513.401] [0.0000], Avg: [-790.594 -790.594 -790.594] (1.000)
Step: 29449, Reward: [-990.113 -990.113 -990.113] [0.0000], Avg: [-790.933 -790.933 -790.933] (1.000)
Step: 29499, Reward: [-969.169 -969.169 -969.169] [0.0000], Avg: [-791.235 -791.235 -791.235] (1.000)
Step: 29549, Reward: [-761.508 -761.508 -761.508] [0.0000], Avg: [-791.184 -791.184 -791.184] (1.000)
Step: 29599, Reward: [-705.263 -705.263 -705.263] [0.0000], Avg: [-791.039 -791.039 -791.039] (1.000)
Step: 29649, Reward: [-542.308 -542.308 -542.308] [0.0000], Avg: [-790.62 -790.62 -790.62] (1.000)
Step: 29699, Reward: [-805.729 -805.729 -805.729] [0.0000], Avg: [-790.645 -790.645 -790.645] (1.000)
Step: 29749, Reward: [-674.522 -674.522 -674.522] [0.0000], Avg: [-790.45 -790.45 -790.45] (1.000)
Step: 29799, Reward: [-743.555 -743.555 -743.555] [0.0000], Avg: [-790.371 -790.371 -790.371] (1.000)
Step: 29849, Reward: [-673.968 -673.968 -673.968] [0.0000], Avg: [-790.176 -790.176 -790.176] (1.000)
Step: 29899, Reward: [-944.714 -944.714 -944.714] [0.0000], Avg: [-790.435 -790.435 -790.435] (1.000)
Step: 29949, Reward: [-1223.181 -1223.181 -1223.181] [0.0000], Avg: [-791.157 -791.157 -791.157] (1.000)
Step: 29999, Reward: [-647.614 -647.614 -647.614] [0.0000], Avg: [-790.918 -790.918 -790.918] (1.000)
Step: 30049, Reward: [-910.165 -910.165 -910.165] [0.0000], Avg: [-791.116 -791.116 -791.116] (1.000)
Step: 30099, Reward: [-506.443 -506.443 -506.443] [0.0000], Avg: [-790.644 -790.644 -790.644] (1.000)
Step: 30149, Reward: [-683.347 -683.347 -683.347] [0.0000], Avg: [-790.466 -790.466 -790.466] (1.000)
Step: 30199, Reward: [-1152.781 -1152.781 -1152.781] [0.0000], Avg: [-791.065 -791.065 -791.065] (1.000)
Step: 30249, Reward: [-553.887 -553.887 -553.887] [0.0000], Avg: [-790.673 -790.673 -790.673] (1.000)
Step: 30299, Reward: [-494.468 -494.468 -494.468] [0.0000], Avg: [-790.185 -790.185 -790.185] (1.000)
Step: 30349, Reward: [-958.397 -958.397 -958.397] [0.0000], Avg: [-790.462 -790.462 -790.462] (1.000)
Step: 30399, Reward: [-1008.997 -1008.997 -1008.997] [0.0000], Avg: [-790.821 -790.821 -790.821] (1.000)
Step: 30449, Reward: [-451.371 -451.371 -451.371] [0.0000], Avg: [-790.264 -790.264 -790.264] (1.000)
Step: 30499, Reward: [-1008.489 -1008.489 -1008.489] [0.0000], Avg: [-790.622 -790.622 -790.622] (1.000)
Step: 30549, Reward: [-956.506 -956.506 -956.506] [0.0000], Avg: [-790.893 -790.893 -790.893] (1.000)
Step: 30599, Reward: [-1756.917 -1756.917 -1756.917] [0.0000], Avg: [-792.472 -792.472 -792.472] (1.000)
Step: 30649, Reward: [-1385.249 -1385.249 -1385.249] [0.0000], Avg: [-793.439 -793.439 -793.439] (1.000)
Step: 30699, Reward: [-1322.261 -1322.261 -1322.261] [0.0000], Avg: [-794.3 -794.3 -794.3] (1.000)
Step: 30749, Reward: [-1151.059 -1151.059 -1151.059] [0.0000], Avg: [-794.88 -794.88 -794.88] (1.000)
Step: 30799, Reward: [-949.398 -949.398 -949.398] [0.0000], Avg: [-795.131 -795.131 -795.131] (1.000)
Step: 30849, Reward: [-1304.462 -1304.462 -1304.462] [0.0000], Avg: [-795.956 -795.956 -795.956] (1.000)
Step: 30899, Reward: [-850.01 -850.01 -850.01] [0.0000], Avg: [-796.044 -796.044 -796.044] (1.000)
Step: 30949, Reward: [-1074.95 -1074.95 -1074.95] [0.0000], Avg: [-796.494 -796.494 -796.494] (1.000)
Step: 30999, Reward: [-607.22 -607.22 -607.22] [0.0000], Avg: [-796.189 -796.189 -796.189] (1.000)
Step: 31049, Reward: [-1501.432 -1501.432 -1501.432] [0.0000], Avg: [-797.325 -797.325 -797.325] (1.000)
Step: 31099, Reward: [-687.405 -687.405 -687.405] [0.0000], Avg: [-797.148 -797.148 -797.148] (1.000)
Step: 31149, Reward: [-727.875 -727.875 -727.875] [0.0000], Avg: [-797.037 -797.037 -797.037] (1.000)
Step: 31199, Reward: [-1129.695 -1129.695 -1129.695] [0.0000], Avg: [-797.57 -797.57 -797.57] (1.000)
Step: 31249, Reward: [-1066.861 -1066.861 -1066.861] [0.0000], Avg: [-798.001 -798.001 -798.001] (1.000)
Step: 31299, Reward: [-710.556 -710.556 -710.556] [0.0000], Avg: [-797.861 -797.861 -797.861] (1.000)
Step: 31349, Reward: [-434.117 -434.117 -434.117] [0.0000], Avg: [-797.281 -797.281 -797.281] (1.000)
Step: 31399, Reward: [-1137.099 -1137.099 -1137.099] [0.0000], Avg: [-797.822 -797.822 -797.822] (1.000)
Step: 31449, Reward: [-537.475 -537.475 -537.475] [0.0000], Avg: [-797.408 -797.408 -797.408] (1.000)
Step: 31499, Reward: [-646.826 -646.826 -646.826] [0.0000], Avg: [-797.169 -797.169 -797.169] (1.000)
Step: 31549, Reward: [-783.425 -783.425 -783.425] [0.0000], Avg: [-797.147 -797.147 -797.147] (1.000)
Step: 31599, Reward: [-483.739 -483.739 -483.739] [0.0000], Avg: [-796.651 -796.651 -796.651] (1.000)
Step: 31649, Reward: [-410.521 -410.521 -410.521] [0.0000], Avg: [-796.041 -796.041 -796.041] (1.000)
Step: 31699, Reward: [-1274.877 -1274.877 -1274.877] [0.0000], Avg: [-796.797 -796.797 -796.797] (1.000)
Step: 31749, Reward: [-1550.963 -1550.963 -1550.963] [0.0000], Avg: [-797.984 -797.984 -797.984] (1.000)
Step: 31799, Reward: [-609.263 -609.263 -609.263] [0.0000], Avg: [-797.688 -797.688 -797.688] (1.000)
Step: 31849, Reward: [-1024.054 -1024.054 -1024.054] [0.0000], Avg: [-798.043 -798.043 -798.043] (1.000)
Step: 31899, Reward: [-951.976 -951.976 -951.976] [0.0000], Avg: [-798.284 -798.284 -798.284] (1.000)
Step: 31949, Reward: [-854.531 -854.531 -854.531] [0.0000], Avg: [-798.372 -798.372 -798.372] (1.000)
Step: 31999, Reward: [-882.598 -882.598 -882.598] [0.0000], Avg: [-798.504 -798.504 -798.504] (1.000)
Step: 32049, Reward: [-643.861 -643.861 -643.861] [0.0000], Avg: [-798.263 -798.263 -798.263] (1.000)
Step: 32099, Reward: [-997.456 -997.456 -997.456] [0.0000], Avg: [-798.573 -798.573 -798.573] (1.000)
Step: 32149, Reward: [-1002.978 -1002.978 -1002.978] [0.0000], Avg: [-798.891 -798.891 -798.891] (1.000)
Step: 32199, Reward: [-997.414 -997.414 -997.414] [0.0000], Avg: [-799.199 -799.199 -799.199] (1.000)
Step: 32249, Reward: [-1180.215 -1180.215 -1180.215] [0.0000], Avg: [-799.79 -799.79 -799.79] (1.000)
Step: 32299, Reward: [-1032.087 -1032.087 -1032.087] [0.0000], Avg: [-800.149 -800.149 -800.149] (1.000)
Step: 32349, Reward: [-727.89 -727.89 -727.89] [0.0000], Avg: [-800.038 -800.038 -800.038] (1.000)
Step: 32399, Reward: [-791.975 -791.975 -791.975] [0.0000], Avg: [-800.025 -800.025 -800.025] (1.000)
Step: 32449, Reward: [-615.622 -615.622 -615.622] [0.0000], Avg: [-799.741 -799.741 -799.741] (1.000)
Step: 32499, Reward: [-1137.345 -1137.345 -1137.345] [0.0000], Avg: [-800.26 -800.26 -800.26] (1.000)
Step: 32549, Reward: [-345.315 -345.315 -345.315] [0.0000], Avg: [-799.562 -799.562 -799.562] (1.000)
Step: 32599, Reward: [-648.236 -648.236 -648.236] [0.0000], Avg: [-799.33 -799.33 -799.33] (1.000)
Step: 32649, Reward: [-960.319 -960.319 -960.319] [0.0000], Avg: [-799.576 -799.576 -799.576] (1.000)
Step: 32699, Reward: [-1512.117 -1512.117 -1512.117] [0.0000], Avg: [-800.666 -800.666 -800.666] (1.000)
Step: 32749, Reward: [-649.979 -649.979 -649.979] [0.0000], Avg: [-800.436 -800.436 -800.436] (1.000)
Step: 32799, Reward: [-975.61 -975.61 -975.61] [0.0000], Avg: [-800.703 -800.703 -800.703] (1.000)
Step: 32849, Reward: [-1479.155 -1479.155 -1479.155] [0.0000], Avg: [-801.735 -801.735 -801.735] (1.000)
Step: 32899, Reward: [-1048.792 -1048.792 -1048.792] [0.0000], Avg: [-802.111 -802.111 -802.111] (1.000)
Step: 32949, Reward: [-810.255 -810.255 -810.255] [0.0000], Avg: [-802.123 -802.123 -802.123] (1.000)
Step: 32999, Reward: [-1685.185 -1685.185 -1685.185] [0.0000], Avg: [-803.461 -803.461 -803.461] (1.000)
Step: 33049, Reward: [-832.266 -832.266 -832.266] [0.0000], Avg: [-803.505 -803.505 -803.505] (1.000)
Step: 33099, Reward: [-970.302 -970.302 -970.302] [0.0000], Avg: [-803.757 -803.757 -803.757] (1.000)
Step: 33149, Reward: [-1282.204 -1282.204 -1282.204] [0.0000], Avg: [-804.478 -804.478 -804.478] (1.000)
Step: 33199, Reward: [-723.393 -723.393 -723.393] [0.0000], Avg: [-804.356 -804.356 -804.356] (1.000)
Step: 33249, Reward: [-897.863 -897.863 -897.863] [0.0000], Avg: [-804.497 -804.497 -804.497] (1.000)
Step: 33299, Reward: [-658.719 -658.719 -658.719] [0.0000], Avg: [-804.278 -804.278 -804.278] (1.000)
Step: 33349, Reward: [-1200.529 -1200.529 -1200.529] [0.0000], Avg: [-804.872 -804.872 -804.872] (1.000)
Step: 33399, Reward: [-1192.124 -1192.124 -1192.124] [0.0000], Avg: [-805.452 -805.452 -805.452] (1.000)
Step: 33449, Reward: [-561.305 -561.305 -561.305] [0.0000], Avg: [-805.087 -805.087 -805.087] (1.000)
Step: 33499, Reward: [-1313.583 -1313.583 -1313.583] [0.0000], Avg: [-805.846 -805.846 -805.846] (1.000)
Step: 33549, Reward: [-1305.317 -1305.317 -1305.317] [0.0000], Avg: [-806.59 -806.59 -806.59] (1.000)
Step: 33599, Reward: [-614.672 -614.672 -614.672] [0.0000], Avg: [-806.304 -806.304 -806.304] (1.000)
Step: 33649, Reward: [-1203.573 -1203.573 -1203.573] [0.0000], Avg: [-806.895 -806.895 -806.895] (1.000)
Step: 33699, Reward: [-1175.201 -1175.201 -1175.201] [0.0000], Avg: [-807.441 -807.441 -807.441] (1.000)
Step: 33749, Reward: [-1103.913 -1103.913 -1103.913] [0.0000], Avg: [-807.88 -807.88 -807.88] (1.000)
Step: 33799, Reward: [-1436.359 -1436.359 -1436.359] [0.0000], Avg: [-808.81 -808.81 -808.81] (1.000)
Step: 33849, Reward: [-629.609 -629.609 -629.609] [0.0000], Avg: [-808.545 -808.545 -808.545] (1.000)
Step: 33899, Reward: [-1476.049 -1476.049 -1476.049] [0.0000], Avg: [-809.53 -809.53 -809.53] (1.000)
Step: 33949, Reward: [-637.878 -637.878 -637.878] [0.0000], Avg: [-809.277 -809.277 -809.277] (1.000)
Step: 33999, Reward: [-730.432 -730.432 -730.432] [0.0000], Avg: [-809.161 -809.161 -809.161] (1.000)
Step: 34049, Reward: [-879.109 -879.109 -879.109] [0.0000], Avg: [-809.264 -809.264 -809.264] (1.000)
Step: 34099, Reward: [-712.124 -712.124 -712.124] [0.0000], Avg: [-809.121 -809.121 -809.121] (1.000)
Step: 34149, Reward: [-950.764 -950.764 -950.764] [0.0000], Avg: [-809.329 -809.329 -809.329] (1.000)
Step: 34199, Reward: [-1153.573 -1153.573 -1153.573] [0.0000], Avg: [-809.832 -809.832 -809.832] (1.000)
Step: 34249, Reward: [-1315.546 -1315.546 -1315.546] [0.0000], Avg: [-810.57 -810.57 -810.57] (1.000)
Step: 34299, Reward: [-1002.442 -1002.442 -1002.442] [0.0000], Avg: [-810.85 -810.85 -810.85] (1.000)
Step: 34349, Reward: [-1476.05 -1476.05 -1476.05] [0.0000], Avg: [-811.818 -811.818 -811.818] (1.000)
Step: 34399, Reward: [-955.029 -955.029 -955.029] [0.0000], Avg: [-812.026 -812.026 -812.026] (1.000)
Step: 34449, Reward: [-1038.601 -1038.601 -1038.601] [0.0000], Avg: [-812.355 -812.355 -812.355] (1.000)
Step: 34499, Reward: [-956.395 -956.395 -956.395] [0.0000], Avg: [-812.564 -812.564 -812.564] (1.000)
Step: 34549, Reward: [-714.561 -714.561 -714.561] [0.0000], Avg: [-812.422 -812.422 -812.422] (1.000)
Step: 34599, Reward: [-1579.181 -1579.181 -1579.181] [0.0000], Avg: [-813.53 -813.53 -813.53] (1.000)
Step: 34649, Reward: [-809.551 -809.551 -809.551] [0.0000], Avg: [-813.525 -813.525 -813.525] (1.000)
Step: 34699, Reward: [-723.439 -723.439 -723.439] [0.0000], Avg: [-813.395 -813.395 -813.395] (1.000)
Step: 34749, Reward: [-1055.966 -1055.966 -1055.966] [0.0000], Avg: [-813.744 -813.744 -813.744] (1.000)
Step: 34799, Reward: [-628.926 -628.926 -628.926] [0.0000], Avg: [-813.478 -813.478 -813.478] (1.000)
Step: 34849, Reward: [-875.966 -875.966 -875.966] [0.0000], Avg: [-813.568 -813.568 -813.568] (1.000)
Step: 34899, Reward: [-818.494 -818.494 -818.494] [0.0000], Avg: [-813.575 -813.575 -813.575] (1.000)
Step: 34949, Reward: [-649.065 -649.065 -649.065] [0.0000], Avg: [-813.34 -813.34 -813.34] (1.000)
Step: 34999, Reward: [-1085.127 -1085.127 -1085.127] [0.0000], Avg: [-813.728 -813.728 -813.728] (1.000)
Step: 35049, Reward: [-627.555 -627.555 -627.555] [0.0000], Avg: [-813.462 -813.462 -813.462] (1.000)
Step: 35099, Reward: [-692.066 -692.066 -692.066] [0.0000], Avg: [-813.289 -813.289 -813.289] (1.000)
Step: 35149, Reward: [-995.469 -995.469 -995.469] [0.0000], Avg: [-813.548 -813.548 -813.548] (1.000)
Step: 35199, Reward: [-1210.716 -1210.716 -1210.716] [0.0000], Avg: [-814.113 -814.113 -814.113] (1.000)
Step: 35249, Reward: [-701.171 -701.171 -701.171] [0.0000], Avg: [-813.952 -813.952 -813.952] (1.000)
Step: 35299, Reward: [-768.326 -768.326 -768.326] [0.0000], Avg: [-813.888 -813.888 -813.888] (1.000)
Step: 35349, Reward: [-944.849 -944.849 -944.849] [0.0000], Avg: [-814.073 -814.073 -814.073] (1.000)
Step: 35399, Reward: [-917.912 -917.912 -917.912] [0.0000], Avg: [-814.22 -814.22 -814.22] (1.000)
Step: 35449, Reward: [-731.379 -731.379 -731.379] [0.0000], Avg: [-814.103 -814.103 -814.103] (1.000)
Step: 35499, Reward: [-537.096 -537.096 -537.096] [0.0000], Avg: [-813.713 -813.713 -813.713] (1.000)
Step: 35549, Reward: [-1094.131 -1094.131 -1094.131] [0.0000], Avg: [-814.107 -814.107 -814.107] (1.000)
Step: 35599, Reward: [-567.673 -567.673 -567.673] [0.0000], Avg: [-813.761 -813.761 -813.761] (1.000)
Step: 35649, Reward: [-1600.749 -1600.749 -1600.749] [0.0000], Avg: [-814.865 -814.865 -814.865] (1.000)
Step: 35699, Reward: [-859.181 -859.181 -859.181] [0.0000], Avg: [-814.927 -814.927 -814.927] (1.000)
Step: 35749, Reward: [-438.69 -438.69 -438.69] [0.0000], Avg: [-814.401 -814.401 -814.401] (1.000)
Step: 35799, Reward: [-809.681 -809.681 -809.681] [0.0000], Avg: [-814.394 -814.394 -814.394] (1.000)
Step: 35849, Reward: [-568.411 -568.411 -568.411] [0.0000], Avg: [-814.051 -814.051 -814.051] (1.000)
Step: 35899, Reward: [-877.594 -877.594 -877.594] [0.0000], Avg: [-814.139 -814.139 -814.139] (1.000)
Step: 35949, Reward: [-544.814 -544.814 -544.814] [0.0000], Avg: [-813.765 -813.765 -813.765] (1.000)
Step: 35999, Reward: [-1137.025 -1137.025 -1137.025] [0.0000], Avg: [-814.214 -814.214 -814.214] (1.000)
Step: 36049, Reward: [-765.955 -765.955 -765.955] [0.0000], Avg: [-814.147 -814.147 -814.147] (1.000)
Step: 36099, Reward: [-1772.867 -1772.867 -1772.867] [0.0000], Avg: [-815.475 -815.475 -815.475] (1.000)
Step: 36149, Reward: [-1497.295 -1497.295 -1497.295] [0.0000], Avg: [-816.418 -816.418 -816.418] (1.000)
Step: 36199, Reward: [-1118.406 -1118.406 -1118.406] [0.0000], Avg: [-816.835 -816.835 -816.835] (1.000)
Step: 36249, Reward: [-769.699 -769.699 -769.699] [0.0000], Avg: [-816.77 -816.77 -816.77] (1.000)
Step: 36299, Reward: [-998.262 -998.262 -998.262] [0.0000], Avg: [-817.02 -817.02 -817.02] (1.000)
Step: 36349, Reward: [-878.457 -878.457 -878.457] [0.0000], Avg: [-817.104 -817.104 -817.104] (1.000)
Step: 36399, Reward: [-469.392 -469.392 -469.392] [0.0000], Avg: [-816.627 -816.627 -816.627] (1.000)
Step: 36449, Reward: [-816.558 -816.558 -816.558] [0.0000], Avg: [-816.627 -816.627 -816.627] (1.000)
Step: 36499, Reward: [-847.494 -847.494 -847.494] [0.0000], Avg: [-816.669 -816.669 -816.669] (1.000)
Step: 36549, Reward: [-1021.537 -1021.537 -1021.537] [0.0000], Avg: [-816.949 -816.949 -816.949] (1.000)
Step: 36599, Reward: [-870.697 -870.697 -870.697] [0.0000], Avg: [-817.023 -817.023 -817.023] (1.000)
Step: 36649, Reward: [-762.131 -762.131 -762.131] [0.0000], Avg: [-816.948 -816.948 -816.948] (1.000)
Step: 36699, Reward: [-843.128 -843.128 -843.128] [0.0000], Avg: [-816.983 -816.983 -816.983] (1.000)
Step: 36749, Reward: [-427.009 -427.009 -427.009] [0.0000], Avg: [-816.453 -816.453 -816.453] (1.000)
Step: 36799, Reward: [-1373.782 -1373.782 -1373.782] [0.0000], Avg: [-817.21 -817.21 -817.21] (1.000)
Step: 36849, Reward: [-895.816 -895.816 -895.816] [0.0000], Avg: [-817.317 -817.317 -817.317] (1.000)
Step: 36899, Reward: [-1135.397 -1135.397 -1135.397] [0.0000], Avg: [-817.748 -817.748 -817.748] (1.000)
Step: 36949, Reward: [-520.337 -520.337 -520.337] [0.0000], Avg: [-817.345 -817.345 -817.345] (1.000)
Step: 36999, Reward: [-872.779 -872.779 -872.779] [0.0000], Avg: [-817.42 -817.42 -817.42] (1.000)
Step: 37049, Reward: [-1273.567 -1273.567 -1273.567] [0.0000], Avg: [-818.036 -818.036 -818.036] (1.000)
Step: 37099, Reward: [-929.641 -929.641 -929.641] [0.0000], Avg: [-818.186 -818.186 -818.186] (1.000)
Step: 37149, Reward: [-973.928 -973.928 -973.928] [0.0000], Avg: [-818.396 -818.396 -818.396] (1.000)
Step: 37199, Reward: [-988.552 -988.552 -988.552] [0.0000], Avg: [-818.625 -818.625 -818.625] (1.000)
Step: 37249, Reward: [-644.043 -644.043 -644.043] [0.0000], Avg: [-818.39 -818.39 -818.39] (1.000)
Step: 37299, Reward: [-602.435 -602.435 -602.435] [0.0000], Avg: [-818.101 -818.101 -818.101] (1.000)
Step: 37349, Reward: [-1643.973 -1643.973 -1643.973] [0.0000], Avg: [-819.206 -819.206 -819.206] (1.000)
Step: 37399, Reward: [-1273.107 -1273.107 -1273.107] [0.0000], Avg: [-819.813 -819.813 -819.813] (1.000)
Step: 37449, Reward: [-1224.5 -1224.5 -1224.5] [0.0000], Avg: [-820.353 -820.353 -820.353] (1.000)
Step: 37499, Reward: [-758.543 -758.543 -758.543] [0.0000], Avg: [-820.271 -820.271 -820.271] (1.000)
Step: 37549, Reward: [-1184.961 -1184.961 -1184.961] [0.0000], Avg: [-820.757 -820.757 -820.757] (1.000)
Step: 37599, Reward: [-1403.034 -1403.034 -1403.034] [0.0000], Avg: [-821.531 -821.531 -821.531] (1.000)
Step: 37649, Reward: [-1508.466 -1508.466 -1508.466] [0.0000], Avg: [-822.443 -822.443 -822.443] (1.000)
Step: 37699, Reward: [-1330.008 -1330.008 -1330.008] [0.0000], Avg: [-823.116 -823.116 -823.116] (1.000)
Step: 37749, Reward: [-1573.834 -1573.834 -1573.834] [0.0000], Avg: [-824.111 -824.111 -824.111] (1.000)
Step: 37799, Reward: [-1294.661 -1294.661 -1294.661] [0.0000], Avg: [-824.733 -824.733 -824.733] (1.000)
Step: 37849, Reward: [-761.307 -761.307 -761.307] [0.0000], Avg: [-824.649 -824.649 -824.649] (1.000)
Step: 37899, Reward: [-574.806 -574.806 -574.806] [0.0000], Avg: [-824.32 -824.32 -824.32] (1.000)
Step: 37949, Reward: [-801.027 -801.027 -801.027] [0.0000], Avg: [-824.289 -824.289 -824.289] (1.000)
Step: 37999, Reward: [-721.947 -721.947 -721.947] [0.0000], Avg: [-824.154 -824.154 -824.154] (1.000)
Step: 38049, Reward: [-1111.696 -1111.696 -1111.696] [0.0000], Avg: [-824.532 -824.532 -824.532] (1.000)
Step: 38099, Reward: [-1542.367 -1542.367 -1542.367] [0.0000], Avg: [-825.474 -825.474 -825.474] (1.000)
Step: 38149, Reward: [-1553.241 -1553.241 -1553.241] [0.0000], Avg: [-826.428 -826.428 -826.428] (1.000)
Step: 38199, Reward: [-1219.396 -1219.396 -1219.396] [0.0000], Avg: [-826.942 -826.942 -826.942] (1.000)
Step: 38249, Reward: [-716.875 -716.875 -716.875] [0.0000], Avg: [-826.799 -826.799 -826.799] (1.000)
Step: 38299, Reward: [-787.993 -787.993 -787.993] [0.0000], Avg: [-826.748 -826.748 -826.748] (1.000)
Step: 38349, Reward: [-933.299 -933.299 -933.299] [0.0000], Avg: [-826.887 -826.887 -826.887] (1.000)
Step: 38399, Reward: [-792.343 -792.343 -792.343] [0.0000], Avg: [-826.842 -826.842 -826.842] (1.000)
Step: 38449, Reward: [-889.587 -889.587 -889.587] [0.0000], Avg: [-826.923 -826.923 -826.923] (1.000)
Step: 38499, Reward: [-663.262 -663.262 -663.262] [0.0000], Avg: [-826.711 -826.711 -826.711] (1.000)
Step: 38549, Reward: [-1057.487 -1057.487 -1057.487] [0.0000], Avg: [-827.01 -827.01 -827.01] (1.000)
Step: 38599, Reward: [-868.556 -868.556 -868.556] [0.0000], Avg: [-827.064 -827.064 -827.064] (1.000)
Step: 38649, Reward: [-950.731 -950.731 -950.731] [0.0000], Avg: [-827.224 -827.224 -827.224] (1.000)
Step: 38699, Reward: [-1233.804 -1233.804 -1233.804] [0.0000], Avg: [-827.749 -827.749 -827.749] (1.000)
Step: 38749, Reward: [-1232.725 -1232.725 -1232.725] [0.0000], Avg: [-828.272 -828.272 -828.272] (1.000)
Step: 38799, Reward: [-650.332 -650.332 -650.332] [0.0000], Avg: [-828.043 -828.043 -828.043] (1.000)
Step: 38849, Reward: [-707.515 -707.515 -707.515] [0.0000], Avg: [-827.887 -827.887 -827.887] (1.000)
Step: 38899, Reward: [-674.909 -674.909 -674.909] [0.0000], Avg: [-827.691 -827.691 -827.691] (1.000)
Step: 38949, Reward: [-1048.757 -1048.757 -1048.757] [0.0000], Avg: [-827.975 -827.975 -827.975] (1.000)
Step: 38999, Reward: [-1428.393 -1428.393 -1428.393] [0.0000], Avg: [-828.744 -828.744 -828.744] (1.000)
Step: 39049, Reward: [-1396.575 -1396.575 -1396.575] [0.0000], Avg: [-829.471 -829.471 -829.471] (1.000)
Step: 39099, Reward: [-1163.004 -1163.004 -1163.004] [0.0000], Avg: [-829.898 -829.898 -829.898] (1.000)
Step: 39149, Reward: [-2093.358 -2093.358 -2093.358] [0.0000], Avg: [-831.512 -831.512 -831.512] (1.000)
Step: 39199, Reward: [-872.821 -872.821 -872.821] [0.0000], Avg: [-831.564 -831.564 -831.564] (1.000)
Step: 39249, Reward: [-1346.05 -1346.05 -1346.05] [0.0000], Avg: [-832.22 -832.22 -832.22] (1.000)
Step: 39299, Reward: [-949.88 -949.88 -949.88] [0.0000], Avg: [-832.369 -832.369 -832.369] (1.000)
Step: 39349, Reward: [-1131.72 -1131.72 -1131.72] [0.0000], Avg: [-832.75 -832.75 -832.75] (1.000)
Step: 39399, Reward: [-678.76 -678.76 -678.76] [0.0000], Avg: [-832.554 -832.554 -832.554] (1.000)
Step: 39449, Reward: [-900.993 -900.993 -900.993] [0.0000], Avg: [-832.641 -832.641 -832.641] (1.000)
Step: 39499, Reward: [-645.022 -645.022 -645.022] [0.0000], Avg: [-832.403 -832.403 -832.403] (1.000)
Step: 39549, Reward: [-1589.73 -1589.73 -1589.73] [0.0000], Avg: [-833.361 -833.361 -833.361] (1.000)
Step: 39599, Reward: [-643.675 -643.675 -643.675] [0.0000], Avg: [-833.121 -833.121 -833.121] (1.000)
Step: 39649, Reward: [-743.183 -743.183 -743.183] [0.0000], Avg: [-833.008 -833.008 -833.008] (1.000)
Step: 39699, Reward: [-863.133 -863.133 -863.133] [0.0000], Avg: [-833.046 -833.046 -833.046] (1.000)
Step: 39749, Reward: [-1321.899 -1321.899 -1321.899] [0.0000], Avg: [-833.661 -833.661 -833.661] (1.000)
Step: 39799, Reward: [-828.251 -828.251 -828.251] [0.0000], Avg: [-833.654 -833.654 -833.654] (1.000)
Step: 39849, Reward: [-1527.438 -1527.438 -1527.438] [0.0000], Avg: [-834.525 -834.525 -834.525] (1.000)
Step: 39899, Reward: [-737.767 -737.767 -737.767] [0.0000], Avg: [-834.403 -834.403 -834.403] (1.000)
Step: 39949, Reward: [-1292.633 -1292.633 -1292.633] [0.0000], Avg: [-834.977 -834.977 -834.977] (1.000)
Step: 39999, Reward: [-831.876 -831.876 -831.876] [0.0000], Avg: [-834.973 -834.973 -834.973] (1.000)
Step: 40049, Reward: [-600.472 -600.472 -600.472] [0.0000], Avg: [-834.68 -834.68 -834.68] (1.000)
Step: 40099, Reward: [-569.737 -569.737 -569.737] [0.0000], Avg: [-834.35 -834.35 -834.35] (1.000)
Step: 40149, Reward: [-451.223 -451.223 -451.223] [0.0000], Avg: [-833.873 -833.873 -833.873] (1.000)
Step: 40199, Reward: [-1153.168 -1153.168 -1153.168] [0.0000], Avg: [-834.27 -834.27 -834.27] (1.000)
Step: 40249, Reward: [-1152.007 -1152.007 -1152.007] [0.0000], Avg: [-834.665 -834.665 -834.665] (1.000)
Step: 40299, Reward: [-1118.265 -1118.265 -1118.265] [0.0000], Avg: [-835.016 -835.016 -835.016] (1.000)
Step: 40349, Reward: [-1092.338 -1092.338 -1092.338] [0.0000], Avg: [-835.335 -835.335 -835.335] (1.000)
Step: 40399, Reward: [-1231.607 -1231.607 -1231.607] [0.0000], Avg: [-835.826 -835.826 -835.826] (1.000)
Step: 40449, Reward: [-1243.109 -1243.109 -1243.109] [0.0000], Avg: [-836.329 -836.329 -836.329] (1.000)
Step: 40499, Reward: [-928.586 -928.586 -928.586] [0.0000], Avg: [-836.443 -836.443 -836.443] (1.000)
Step: 40549, Reward: [-1100.431 -1100.431 -1100.431] [0.0000], Avg: [-836.769 -836.769 -836.769] (1.000)
Step: 40599, Reward: [-828.156 -828.156 -828.156] [0.0000], Avg: [-836.758 -836.758 -836.758] (1.000)
Step: 40649, Reward: [-846.433 -846.433 -846.433] [0.0000], Avg: [-836.77 -836.77 -836.77] (1.000)
Step: 40699, Reward: [-767.892 -767.892 -767.892] [0.0000], Avg: [-836.685 -836.685 -836.685] (1.000)
Step: 40749, Reward: [-1383.827 -1383.827 -1383.827] [0.0000], Avg: [-837.357 -837.357 -837.357] (1.000)
Step: 40799, Reward: [-908.282 -908.282 -908.282] [0.0000], Avg: [-837.443 -837.443 -837.443] (1.000)
Step: 40849, Reward: [-1092.771 -1092.771 -1092.771] [0.0000], Avg: [-837.756 -837.756 -837.756] (1.000)
Step: 40899, Reward: [-914.709 -914.709 -914.709] [0.0000], Avg: [-837.85 -837.85 -837.85] (1.000)
Step: 40949, Reward: [-1127.61 -1127.61 -1127.61] [0.0000], Avg: [-838.204 -838.204 -838.204] (1.000)
Step: 40999, Reward: [-1580.753 -1580.753 -1580.753] [0.0000], Avg: [-839.109 -839.109 -839.109] (1.000)
Step: 41049, Reward: [-1491.051 -1491.051 -1491.051] [0.0000], Avg: [-839.903 -839.903 -839.903] (1.000)
Step: 41099, Reward: [-1545.412 -1545.412 -1545.412] [0.0000], Avg: [-840.762 -840.762 -840.762] (1.000)
Step: 41149, Reward: [-956.841 -956.841 -956.841] [0.0000], Avg: [-840.903 -840.903 -840.903] (1.000)
Step: 41199, Reward: [-813.422 -813.422 -813.422] [0.0000], Avg: [-840.869 -840.869 -840.869] (1.000)
Step: 41249, Reward: [-898.06 -898.06 -898.06] [0.0000], Avg: [-840.939 -840.939 -840.939] (1.000)
Step: 41299, Reward: [-729.48 -729.48 -729.48] [0.0000], Avg: [-840.804 -840.804 -840.804] (1.000)
Step: 41349, Reward: [-953.382 -953.382 -953.382] [0.0000], Avg: [-840.94 -840.94 -840.94] (1.000)
Step: 41399, Reward: [-995.445 -995.445 -995.445] [0.0000], Avg: [-841.127 -841.127 -841.127] (1.000)
Step: 41449, Reward: [-716.736 -716.736 -716.736] [0.0000], Avg: [-840.977 -840.977 -840.977] (1.000)
Step: 41499, Reward: [-798.14 -798.14 -798.14] [0.0000], Avg: [-840.925 -840.925 -840.925] (1.000)
Step: 41549, Reward: [-669.07 -669.07 -669.07] [0.0000], Avg: [-840.718 -840.718 -840.718] (1.000)
Step: 41599, Reward: [-1107.929 -1107.929 -1107.929] [0.0000], Avg: [-841.039 -841.039 -841.039] (1.000)
Step: 41649, Reward: [-790.435 -790.435 -790.435] [0.0000], Avg: [-840.979 -840.979 -840.979] (1.000)
Step: 41699, Reward: [-928.845 -928.845 -928.845] [0.0000], Avg: [-841.084 -841.084 -841.084] (1.000)
Step: 41749, Reward: [-1143.375 -1143.375 -1143.375] [0.0000], Avg: [-841.446 -841.446 -841.446] (1.000)
Step: 41799, Reward: [-503.886 -503.886 -503.886] [0.0000], Avg: [-841.042 -841.042 -841.042] (1.000)
Step: 41849, Reward: [-1207.241 -1207.241 -1207.241] [0.0000], Avg: [-841.48 -841.48 -841.48] (1.000)
Step: 41899, Reward: [-1101.966 -1101.966 -1101.966] [0.0000], Avg: [-841.79 -841.79 -841.79] (1.000)
Step: 41949, Reward: [-846.399 -846.399 -846.399] [0.0000], Avg: [-841.796 -841.796 -841.796] (1.000)
Step: 41999, Reward: [-828.287 -828.287 -828.287] [0.0000], Avg: [-841.78 -841.78 -841.78] (1.000)
Step: 42049, Reward: [-841.699 -841.699 -841.699] [0.0000], Avg: [-841.78 -841.78 -841.78] (1.000)
Step: 42099, Reward: [-1236.043 -1236.043 -1236.043] [0.0000], Avg: [-842.248 -842.248 -842.248] (1.000)
Step: 42149, Reward: [-706.277 -706.277 -706.277] [0.0000], Avg: [-842.087 -842.087 -842.087] (1.000)
Step: 42199, Reward: [-849.778 -849.778 -849.778] [0.0000], Avg: [-842.096 -842.096 -842.096] (1.000)
Step: 42249, Reward: [-827.165 -827.165 -827.165] [0.0000], Avg: [-842.078 -842.078 -842.078] (1.000)
Step: 42299, Reward: [-879.306 -879.306 -879.306] [0.0000], Avg: [-842.122 -842.122 -842.122] (1.000)
Step: 42349, Reward: [-1374.337 -1374.337 -1374.337] [0.0000], Avg: [-842.751 -842.751 -842.751] (1.000)
Step: 42399, Reward: [-1028.419 -1028.419 -1028.419] [0.0000], Avg: [-842.97 -842.97 -842.97] (1.000)
Step: 42449, Reward: [-656.739 -656.739 -656.739] [0.0000], Avg: [-842.75 -842.75 -842.75] (1.000)
Step: 42499, Reward: [-1202.486 -1202.486 -1202.486] [0.0000], Avg: [-843.173 -843.173 -843.173] (1.000)
Step: 42549, Reward: [-698.812 -698.812 -698.812] [0.0000], Avg: [-843.004 -843.004 -843.004] (1.000)
Step: 42599, Reward: [-736.657 -736.657 -736.657] [0.0000], Avg: [-842.879 -842.879 -842.879] (1.000)
Step: 42649, Reward: [-1306.584 -1306.584 -1306.584] [0.0000], Avg: [-843.423 -843.423 -843.423] (1.000)
Step: 42699, Reward: [-970.724 -970.724 -970.724] [0.0000], Avg: [-843.572 -843.572 -843.572] (1.000)
Step: 42749, Reward: [-821.315 -821.315 -821.315] [0.0000], Avg: [-843.546 -843.546 -843.546] (1.000)
Step: 42799, Reward: [-543.55 -543.55 -543.55] [0.0000], Avg: [-843.195 -843.195 -843.195] (1.000)
Step: 42849, Reward: [-764.355 -764.355 -764.355] [0.0000], Avg: [-843.103 -843.103 -843.103] (1.000)
Step: 42899, Reward: [-615.251 -615.251 -615.251] [0.0000], Avg: [-842.838 -842.838 -842.838] (1.000)
Step: 42949, Reward: [-819.733 -819.733 -819.733] [0.0000], Avg: [-842.811 -842.811 -842.811] (1.000)
Step: 42999, Reward: [-640.521 -640.521 -640.521] [0.0000], Avg: [-842.575 -842.575 -842.575] (1.000)
Step: 43049, Reward: [-537.209 -537.209 -537.209] [0.0000], Avg: [-842.221 -842.221 -842.221] (1.000)
Step: 43099, Reward: [-736.307 -736.307 -736.307] [0.0000], Avg: [-842.098 -842.098 -842.098] (1.000)
Step: 43149, Reward: [-809.95 -809.95 -809.95] [0.0000], Avg: [-842.061 -842.061 -842.061] (1.000)
Step: 43199, Reward: [-711.654 -711.654 -711.654] [0.0000], Avg: [-841.91 -841.91 -841.91] (1.000)
Step: 43249, Reward: [-1095.841 -1095.841 -1095.841] [0.0000], Avg: [-842.203 -842.203 -842.203] (1.000)
Step: 43299, Reward: [-1022.276 -1022.276 -1022.276] [0.0000], Avg: [-842.411 -842.411 -842.411] (1.000)
Step: 43349, Reward: [-676.996 -676.996 -676.996] [0.0000], Avg: [-842.22 -842.22 -842.22] (1.000)
Step: 43399, Reward: [-644.775 -644.775 -644.775] [0.0000], Avg: [-841.993 -841.993 -841.993] (1.000)
Step: 43449, Reward: [-634.266 -634.266 -634.266] [0.0000], Avg: [-841.754 -841.754 -841.754] (1.000)
Step: 43499, Reward: [-1168.15 -1168.15 -1168.15] [0.0000], Avg: [-842.129 -842.129 -842.129] (1.000)
Step: 43549, Reward: [-918.661 -918.661 -918.661] [0.0000], Avg: [-842.217 -842.217 -842.217] (1.000)
Step: 43599, Reward: [-459.14 -459.14 -459.14] [0.0000], Avg: [-841.778 -841.778 -841.778] (1.000)
Step: 43649, Reward: [-733.34 -733.34 -733.34] [0.0000], Avg: [-841.653 -841.653 -841.653] (1.000)
Step: 43699, Reward: [-913.315 -913.315 -913.315] [0.0000], Avg: [-841.735 -841.735 -841.735] (1.000)
Step: 43749, Reward: [-520.537 -520.537 -520.537] [0.0000], Avg: [-841.368 -841.368 -841.368] (1.000)
Step: 43799, Reward: [-956.29 -956.29 -956.29] [0.0000], Avg: [-841.5 -841.5 -841.5] (1.000)
Step: 43849, Reward: [-714.732 -714.732 -714.732] [0.0000], Avg: [-841.355 -841.355 -841.355] (1.000)
Step: 43899, Reward: [-1186.561 -1186.561 -1186.561] [0.0000], Avg: [-841.748 -841.748 -841.748] (1.000)
Step: 43949, Reward: [-1221.414 -1221.414 -1221.414] [0.0000], Avg: [-842.18 -842.18 -842.18] (1.000)
Step: 43999, Reward: [-1182.921 -1182.921 -1182.921] [0.0000], Avg: [-842.567 -842.567 -842.567] (1.000)
Step: 44049, Reward: [-683.574 -683.574 -683.574] [0.0000], Avg: [-842.387 -842.387 -842.387] (1.000)
Step: 44099, Reward: [-774.561 -774.561 -774.561] [0.0000], Avg: [-842.31 -842.31 -842.31] (1.000)
Step: 44149, Reward: [-501.906 -501.906 -501.906] [0.0000], Avg: [-841.924 -841.924 -841.924] (1.000)
Step: 44199, Reward: [-941.721 -941.721 -941.721] [0.0000], Avg: [-842.037 -842.037 -842.037] (1.000)
Step: 44249, Reward: [-905.924 -905.924 -905.924] [0.0000], Avg: [-842.109 -842.109 -842.109] (1.000)
Step: 44299, Reward: [-742.006 -742.006 -742.006] [0.0000], Avg: [-841.996 -841.996 -841.996] (1.000)
Step: 44349, Reward: [-792.619 -792.619 -792.619] [0.0000], Avg: [-841.941 -841.941 -841.941] (1.000)
Step: 44399, Reward: [-555.698 -555.698 -555.698] [0.0000], Avg: [-841.618 -841.618 -841.618] (1.000)
Step: 44449, Reward: [-1098.808 -1098.808 -1098.808] [0.0000], Avg: [-841.908 -841.908 -841.908] (1.000)
Step: 44499, Reward: [-1079.734 -1079.734 -1079.734] [0.0000], Avg: [-842.175 -842.175 -842.175] (1.000)
Step: 44549, Reward: [-910.886 -910.886 -910.886] [0.0000], Avg: [-842.252 -842.252 -842.252] (1.000)
Step: 44599, Reward: [-1171.467 -1171.467 -1171.467] [0.0000], Avg: [-842.621 -842.621 -842.621] (1.000)
Step: 44649, Reward: [-754.56 -754.56 -754.56] [0.0000], Avg: [-842.523 -842.523 -842.523] (1.000)
Step: 44699, Reward: [-704.672 -704.672 -704.672] [0.0000], Avg: [-842.368 -842.368 -842.368] (1.000)
Step: 44749, Reward: [-769.422 -769.422 -769.422] [0.0000], Avg: [-842.287 -842.287 -842.287] (1.000)
Step: 44799, Reward: [-1054.073 -1054.073 -1054.073] [0.0000], Avg: [-842.523 -842.523 -842.523] (1.000)
Step: 44849, Reward: [-1458.804 -1458.804 -1458.804] [0.0000], Avg: [-843.21 -843.21 -843.21] (1.000)
Step: 44899, Reward: [-640.742 -640.742 -640.742] [0.0000], Avg: [-842.985 -842.985 -842.985] (1.000)
Step: 44949, Reward: [-576.402 -576.402 -576.402] [0.0000], Avg: [-842.688 -842.688 -842.688] (1.000)
Step: 44999, Reward: [-659.34 -659.34 -659.34] [0.0000], Avg: [-842.485 -842.485 -842.485] (1.000)
Step: 45049, Reward: [-545.688 -545.688 -545.688] [0.0000], Avg: [-842.155 -842.155 -842.155] (1.000)
Step: 45099, Reward: [-1056.78 -1056.78 -1056.78] [0.0000], Avg: [-842.393 -842.393 -842.393] (1.000)
Step: 45149, Reward: [-1233.527 -1233.527 -1233.527] [0.0000], Avg: [-842.826 -842.826 -842.826] (1.000)
Step: 45199, Reward: [-930.624 -930.624 -930.624] [0.0000], Avg: [-842.923 -842.923 -842.923] (1.000)
Step: 45249, Reward: [-916.425 -916.425 -916.425] [0.0000], Avg: [-843.005 -843.005 -843.005] (1.000)
Step: 45299, Reward: [-577.298 -577.298 -577.298] [0.0000], Avg: [-842.711 -842.711 -842.711] (1.000)
Step: 45349, Reward: [-1639.413 -1639.413 -1639.413] [0.0000], Avg: [-843.59 -843.59 -843.59] (1.000)
Step: 45399, Reward: [-1376.11 -1376.11 -1376.11] [0.0000], Avg: [-844.176 -844.176 -844.176] (1.000)
Step: 45449, Reward: [-755.49 -755.49 -755.49] [0.0000], Avg: [-844.079 -844.079 -844.079] (1.000)
Step: 45499, Reward: [-862.36 -862.36 -862.36] [0.0000], Avg: [-844.099 -844.099 -844.099] (1.000)
Step: 45549, Reward: [-788.218 -788.218 -788.218] [0.0000], Avg: [-844.037 -844.037 -844.037] (1.000)
Step: 45599, Reward: [-1263.918 -1263.918 -1263.918] [0.0000], Avg: [-844.498 -844.498 -844.498] (1.000)
Step: 45649, Reward: [-554.128 -554.128 -554.128] [0.0000], Avg: [-844.18 -844.18 -844.18] (1.000)
Step: 45699, Reward: [-1023.084 -1023.084 -1023.084] [0.0000], Avg: [-844.375 -844.375 -844.375] (1.000)
Step: 45749, Reward: [-1136.862 -1136.862 -1136.862] [0.0000], Avg: [-844.695 -844.695 -844.695] (1.000)
Step: 45799, Reward: [-874.402 -874.402 -874.402] [0.0000], Avg: [-844.728 -844.728 -844.728] (1.000)
Step: 45849, Reward: [-588.606 -588.606 -588.606] [0.0000], Avg: [-844.448 -844.448 -844.448] (1.000)
Step: 45899, Reward: [-816.428 -816.428 -816.428] [0.0000], Avg: [-844.418 -844.418 -844.418] (1.000)
Step: 45949, Reward: [-1806.317 -1806.317 -1806.317] [0.0000], Avg: [-845.464 -845.464 -845.464] (1.000)
Step: 45999, Reward: [-718.226 -718.226 -718.226] [0.0000], Avg: [-845.326 -845.326 -845.326] (1.000)
Step: 46049, Reward: [-847.858 -847.858 -847.858] [0.0000], Avg: [-845.329 -845.329 -845.329] (1.000)
Step: 46099, Reward: [-1915.533 -1915.533 -1915.533] [0.0000], Avg: [-846.49 -846.49 -846.49] (1.000)
Step: 46149, Reward: [-659.43 -659.43 -659.43] [0.0000], Avg: [-846.287 -846.287 -846.287] (1.000)
Step: 46199, Reward: [-546.565 -546.565 -546.565] [0.0000], Avg: [-845.963 -845.963 -845.963] (1.000)
Step: 46249, Reward: [-819.852 -819.852 -819.852] [0.0000], Avg: [-845.934 -845.934 -845.934] (1.000)
Step: 46299, Reward: [-592.485 -592.485 -592.485] [0.0000], Avg: [-845.661 -845.661 -845.661] (1.000)
Step: 46349, Reward: [-380.59 -380.59 -380.59] [0.0000], Avg: [-845.159 -845.159 -845.159] (1.000)
Step: 46399, Reward: [-551.042 -551.042 -551.042] [0.0000], Avg: [-844.842 -844.842 -844.842] (1.000)
Step: 46449, Reward: [-683.442 -683.442 -683.442] [0.0000], Avg: [-844.668 -844.668 -844.668] (1.000)
Step: 46499, Reward: [-983.3 -983.3 -983.3] [0.0000], Avg: [-844.817 -844.817 -844.817] (1.000)
Step: 46549, Reward: [-691.377 -691.377 -691.377] [0.0000], Avg: [-844.653 -844.653 -844.653] (1.000)
Step: 46599, Reward: [-931.6 -931.6 -931.6] [0.0000], Avg: [-844.746 -844.746 -844.746] (1.000)
Step: 46649, Reward: [-871.328 -871.328 -871.328] [0.0000], Avg: [-844.774 -844.774 -844.774] (1.000)
Step: 46699, Reward: [-634.331 -634.331 -634.331] [0.0000], Avg: [-844.549 -844.549 -844.549] (1.000)
Step: 46749, Reward: [-736.783 -736.783 -736.783] [0.0000], Avg: [-844.434 -844.434 -844.434] (1.000)
Step: 46799, Reward: [-577.683 -577.683 -577.683] [0.0000], Avg: [-844.149 -844.149 -844.149] (1.000)
Step: 46849, Reward: [-476.17 -476.17 -476.17] [0.0000], Avg: [-843.756 -843.756 -843.756] (1.000)
Step: 46899, Reward: [-768.719 -768.719 -768.719] [0.0000], Avg: [-843.676 -843.676 -843.676] (1.000)
Step: 46949, Reward: [-425.843 -425.843 -425.843] [0.0000], Avg: [-843.231 -843.231 -843.231] (1.000)
Step: 46999, Reward: [-571.157 -571.157 -571.157] [0.0000], Avg: [-842.942 -842.942 -842.942] (1.000)
Step: 47049, Reward: [-468.802 -468.802 -468.802] [0.0000], Avg: [-842.544 -842.544 -842.544] (1.000)
Step: 47099, Reward: [-473.143 -473.143 -473.143] [0.0000], Avg: [-842.152 -842.152 -842.152] (1.000)
Step: 47149, Reward: [-554.047 -554.047 -554.047] [0.0000], Avg: [-841.846 -841.846 -841.846] (1.000)
Step: 47199, Reward: [-701.291 -701.291 -701.291] [0.0000], Avg: [-841.697 -841.697 -841.697] (1.000)
Step: 47249, Reward: [-441.153 -441.153 -441.153] [0.0000], Avg: [-841.274 -841.274 -841.274] (1.000)
Step: 47299, Reward: [-712.088 -712.088 -712.088] [0.0000], Avg: [-841.137 -841.137 -841.137] (1.000)
Step: 47349, Reward: [-480.203 -480.203 -480.203] [0.0000], Avg: [-840.756 -840.756 -840.756] (1.000)
Step: 47399, Reward: [-778.915 -778.915 -778.915] [0.0000], Avg: [-840.691 -840.691 -840.691] (1.000)
Step: 47449, Reward: [-614.491 -614.491 -614.491] [0.0000], Avg: [-840.452 -840.452 -840.452] (1.000)
Step: 47499, Reward: [-1495.874 -1495.874 -1495.874] [0.0000], Avg: [-841.142 -841.142 -841.142] (1.000)
Step: 47549, Reward: [-825.908 -825.908 -825.908] [0.0000], Avg: [-841.126 -841.126 -841.126] (1.000)
Step: 47599, Reward: [-697.124 -697.124 -697.124] [0.0000], Avg: [-840.975 -840.975 -840.975] (1.000)
Step: 47649, Reward: [-1108.718 -1108.718 -1108.718] [0.0000], Avg: [-841.256 -841.256 -841.256] (1.000)
Step: 47699, Reward: [-929.35 -929.35 -929.35] [0.0000], Avg: [-841.348 -841.348 -841.348] (1.000)
Step: 47749, Reward: [-868.482 -868.482 -868.482] [0.0000], Avg: [-841.377 -841.377 -841.377] (1.000)
Step: 47799, Reward: [-446.94 -446.94 -446.94] [0.0000], Avg: [-840.964 -840.964 -840.964] (1.000)
Step: 47849, Reward: [-673.91 -673.91 -673.91] [0.0000], Avg: [-840.789 -840.789 -840.789] (1.000)
Step: 47899, Reward: [-878.041 -878.041 -878.041] [0.0000], Avg: [-840.828 -840.828 -840.828] (1.000)
Step: 47949, Reward: [-985.105 -985.105 -985.105] [0.0000], Avg: [-840.979 -840.979 -840.979] (1.000)
Step: 47999, Reward: [-1233.749 -1233.749 -1233.749] [0.0000], Avg: [-841.388 -841.388 -841.388] (1.000)
Step: 48049, Reward: [-1666.613 -1666.613 -1666.613] [0.0000], Avg: [-842.247 -842.247 -842.247] (1.000)
Step: 48099, Reward: [-500.307 -500.307 -500.307] [0.0000], Avg: [-841.891 -841.891 -841.891] (1.000)
Step: 48149, Reward: [-1139.235 -1139.235 -1139.235] [0.0000], Avg: [-842.2 -842.2 -842.2] (1.000)
Step: 48199, Reward: [-648.422 -648.422 -648.422] [0.0000], Avg: [-841.999 -841.999 -841.999] (1.000)
Step: 48249, Reward: [-489.49 -489.49 -489.49] [0.0000], Avg: [-841.634 -841.634 -841.634] (1.000)
Step: 48299, Reward: [-560.64 -560.64 -560.64] [0.0000], Avg: [-841.343 -841.343 -841.343] (1.000)
Step: 48349, Reward: [-680.326 -680.326 -680.326] [0.0000], Avg: [-841.176 -841.176 -841.176] (1.000)
Step: 48399, Reward: [-1069.278 -1069.278 -1069.278] [0.0000], Avg: [-841.412 -841.412 -841.412] (1.000)
Step: 48449, Reward: [-1016.723 -1016.723 -1016.723] [0.0000], Avg: [-841.593 -841.593 -841.593] (1.000)
Step: 48499, Reward: [-1267.463 -1267.463 -1267.463] [0.0000], Avg: [-842.032 -842.032 -842.032] (1.000)
Step: 48549, Reward: [-746.115 -746.115 -746.115] [0.0000], Avg: [-841.933 -841.933 -841.933] (1.000)
Step: 48599, Reward: [-1295.88 -1295.88 -1295.88] [0.0000], Avg: [-842.4 -842.4 -842.4] (1.000)
Step: 48649, Reward: [-1180.628 -1180.628 -1180.628] [0.0000], Avg: [-842.748 -842.748 -842.748] (1.000)
Step: 48699, Reward: [-1338.729 -1338.729 -1338.729] [0.0000], Avg: [-843.257 -843.257 -843.257] (1.000)
Step: 48749, Reward: [-822.792 -822.792 -822.792] [0.0000], Avg: [-843.236 -843.236 -843.236] (1.000)
Step: 48799, Reward: [-999.896 -999.896 -999.896] [0.0000], Avg: [-843.396 -843.396 -843.396] (1.000)
Step: 48849, Reward: [-1441.953 -1441.953 -1441.953] [0.0000], Avg: [-844.009 -844.009 -844.009] (1.000)
Step: 48899, Reward: [-1667.993 -1667.993 -1667.993] [0.0000], Avg: [-844.852 -844.852 -844.852] (1.000)
Step: 48949, Reward: [-716.218 -716.218 -716.218] [0.0000], Avg: [-844.72 -844.72 -844.72] (1.000)
Step: 48999, Reward: [-1379.817 -1379.817 -1379.817] [0.0000], Avg: [-845.266 -845.266 -845.266] (1.000)
Step: 49049, Reward: [-1275.205 -1275.205 -1275.205] [0.0000], Avg: [-845.705 -845.705 -845.705] (1.000)
Step: 49099, Reward: [-1328.143 -1328.143 -1328.143] [0.0000], Avg: [-846.196 -846.196 -846.196] (1.000)
Step: 49149, Reward: [-1372.204 -1372.204 -1372.204] [0.0000], Avg: [-846.731 -846.731 -846.731] (1.000)
Step: 49199, Reward: [-818.931 -818.931 -818.931] [0.0000], Avg: [-846.703 -846.703 -846.703] (1.000)
Step: 49249, Reward: [-1066.33 -1066.33 -1066.33] [0.0000], Avg: [-846.926 -846.926 -846.926] (1.000)
Step: 49299, Reward: [-1222.07 -1222.07 -1222.07] [0.0000], Avg: [-847.306 -847.306 -847.306] (1.000)
Step: 49349, Reward: [-1444.464 -1444.464 -1444.464] [0.0000], Avg: [-847.911 -847.911 -847.911] (1.000)
Step: 49399, Reward: [-1144.023 -1144.023 -1144.023] [0.0000], Avg: [-848.211 -848.211 -848.211] (1.000)
Step: 49449, Reward: [-962.556 -962.556 -962.556] [0.0000], Avg: [-848.326 -848.326 -848.326] (1.000)
Step: 49499, Reward: [-778.007 -778.007 -778.007] [0.0000], Avg: [-848.255 -848.255 -848.255] (1.000)
Step: 49549, Reward: [-642.179 -642.179 -642.179] [0.0000], Avg: [-848.047 -848.047 -848.047] (1.000)
Step: 49599, Reward: [-1431.853 -1431.853 -1431.853] [0.0000], Avg: [-848.636 -848.636 -848.636] (1.000)
Step: 49649, Reward: [-775.628 -775.628 -775.628] [0.0000], Avg: [-848.562 -848.562 -848.562] (1.000)
Step: 49699, Reward: [-668.236 -668.236 -668.236] [0.0000], Avg: [-848.381 -848.381 -848.381] (1.000)
Step: 49749, Reward: [-519.719 -519.719 -519.719] [0.0000], Avg: [-848.051 -848.051 -848.051] (1.000)
Step: 49799, Reward: [-948.3 -948.3 -948.3] [0.0000], Avg: [-848.151 -848.151 -848.151] (1.000)
Step: 49849, Reward: [-552.449 -552.449 -552.449] [0.0000], Avg: [-847.855 -847.855 -847.855] (1.000)
Step: 49899, Reward: [-994.368 -994.368 -994.368] [0.0000], Avg: [-848.002 -848.002 -848.002] (1.000)
Step: 49949, Reward: [-1230.811 -1230.811 -1230.811] [0.0000], Avg: [-848.385 -848.385 -848.385] (1.000)
Step: 49999, Reward: [-1091.016 -1091.016 -1091.016] [0.0000], Avg: [-848.627 -848.627 -848.627] (1.000)
Step: 50049, Reward: [-523.331 -523.331 -523.331] [0.0000], Avg: [-848.302 -848.302 -848.302] (1.000)
Step: 50099, Reward: [-1076.048 -1076.048 -1076.048] [0.0000], Avg: [-848.53 -848.53 -848.53] (1.000)
Step: 50149, Reward: [-887.319 -887.319 -887.319] [0.0000], Avg: [-848.568 -848.568 -848.568] (1.000)
Step: 50199, Reward: [-963.624 -963.624 -963.624] [0.0000], Avg: [-848.683 -848.683 -848.683] (1.000)
Step: 50249, Reward: [-757.222 -757.222 -757.222] [0.0000], Avg: [-848.592 -848.592 -848.592] (1.000)
Step: 50299, Reward: [-556.688 -556.688 -556.688] [0.0000], Avg: [-848.302 -848.302 -848.302] (1.000)
Step: 50349, Reward: [-1213.884 -1213.884 -1213.884] [0.0000], Avg: [-848.665 -848.665 -848.665] (1.000)
Step: 50399, Reward: [-821.701 -821.701 -821.701] [0.0000], Avg: [-848.638 -848.638 -848.638] (1.000)
Step: 50449, Reward: [-1312.693 -1312.693 -1312.693] [0.0000], Avg: [-849.098 -849.098 -849.098] (1.000)
Step: 50499, Reward: [-761.905 -761.905 -761.905] [0.0000], Avg: [-849.012 -849.012 -849.012] (1.000)
Step: 50549, Reward: [-1153.299 -1153.299 -1153.299] [0.0000], Avg: [-849.313 -849.313 -849.313] (1.000)
Step: 50599, Reward: [-1147.366 -1147.366 -1147.366] [0.0000], Avg: [-849.607 -849.607 -849.607] (1.000)
Step: 50649, Reward: [-1669.526 -1669.526 -1669.526] [0.0000], Avg: [-850.417 -850.417 -850.417] (1.000)
Step: 50699, Reward: [-721.005 -721.005 -721.005] [0.0000], Avg: [-850.289 -850.289 -850.289] (1.000)
Step: 50749, Reward: [-1535.524 -1535.524 -1535.524] [0.0000], Avg: [-850.964 -850.964 -850.964] (1.000)
Step: 50799, Reward: [-1158.107 -1158.107 -1158.107] [0.0000], Avg: [-851.266 -851.266 -851.266] (1.000)
Step: 50849, Reward: [-758.91 -758.91 -758.91] [0.0000], Avg: [-851.176 -851.176 -851.176] (1.000)
Step: 50899, Reward: [-1277.487 -1277.487 -1277.487] [0.0000], Avg: [-851.594 -851.594 -851.594] (1.000)
Step: 50949, Reward: [-905.304 -905.304 -905.304] [0.0000], Avg: [-851.647 -851.647 -851.647] (1.000)
Step: 50999, Reward: [-1173.977 -1173.977 -1173.977] [0.0000], Avg: [-851.963 -851.963 -851.963] (1.000)
Step: 51049, Reward: [-1468.91 -1468.91 -1468.91] [0.0000], Avg: [-852.567 -852.567 -852.567] (1.000)
Step: 51099, Reward: [-900.349 -900.349 -900.349] [0.0000], Avg: [-852.614 -852.614 -852.614] (1.000)
Step: 51149, Reward: [-823.878 -823.878 -823.878] [0.0000], Avg: [-852.586 -852.586 -852.586] (1.000)
Step: 51199, Reward: [-1306.824 -1306.824 -1306.824] [0.0000], Avg: [-853.03 -853.03 -853.03] (1.000)
Step: 51249, Reward: [-1263.324 -1263.324 -1263.324] [0.0000], Avg: [-853.43 -853.43 -853.43] (1.000)
Step: 51299, Reward: [-1141.007 -1141.007 -1141.007] [0.0000], Avg: [-853.71 -853.71 -853.71] (1.000)
Step: 51349, Reward: [-988.739 -988.739 -988.739] [0.0000], Avg: [-853.842 -853.842 -853.842] (1.000)
Step: 51399, Reward: [-1199.994 -1199.994 -1199.994] [0.0000], Avg: [-854.178 -854.178 -854.178] (1.000)
Step: 51449, Reward: [-1342.381 -1342.381 -1342.381] [0.0000], Avg: [-854.653 -854.653 -854.653] (1.000)
Step: 51499, Reward: [-1611.767 -1611.767 -1611.767] [0.0000], Avg: [-855.388 -855.388 -855.388] (1.000)
Step: 51549, Reward: [-1346.85 -1346.85 -1346.85] [0.0000], Avg: [-855.865 -855.865 -855.865] (1.000)
Step: 51599, Reward: [-561.181 -561.181 -561.181] [0.0000], Avg: [-855.579 -855.579 -855.579] (1.000)
Step: 51649, Reward: [-1113.615 -1113.615 -1113.615] [0.0000], Avg: [-855.829 -855.829 -855.829] (1.000)
Step: 51699, Reward: [-1107.562 -1107.562 -1107.562] [0.0000], Avg: [-856.072 -856.072 -856.072] (1.000)
Step: 51749, Reward: [-1226.872 -1226.872 -1226.872] [0.0000], Avg: [-856.431 -856.431 -856.431] (1.000)
Step: 51799, Reward: [-1393.037 -1393.037 -1393.037] [0.0000], Avg: [-856.949 -856.949 -856.949] (1.000)
Step: 51849, Reward: [-958.979 -958.979 -958.979] [0.0000], Avg: [-857.047 -857.047 -857.047] (1.000)
Step: 51899, Reward: [-1426.272 -1426.272 -1426.272] [0.0000], Avg: [-857.595 -857.595 -857.595] (1.000)
Step: 51949, Reward: [-929.073 -929.073 -929.073] [0.0000], Avg: [-857.664 -857.664 -857.664] (1.000)
Step: 51999, Reward: [-1043.729 -1043.729 -1043.729] [0.0000], Avg: [-857.843 -857.843 -857.843] (1.000)
Step: 52049, Reward: [-1109.563 -1109.563 -1109.563] [0.0000], Avg: [-858.085 -858.085 -858.085] (1.000)
Step: 52099, Reward: [-429.488 -429.488 -429.488] [0.0000], Avg: [-857.673 -857.673 -857.673] (1.000)
Step: 52149, Reward: [-1643.055 -1643.055 -1643.055] [0.0000], Avg: [-858.426 -858.426 -858.426] (1.000)
Step: 52199, Reward: [-1411.053 -1411.053 -1411.053] [0.0000], Avg: [-858.956 -858.956 -858.956] (1.000)
Step: 52249, Reward: [-950.394 -950.394 -950.394] [0.0000], Avg: [-859.043 -859.043 -859.043] (1.000)
Step: 52299, Reward: [-1041.634 -1041.634 -1041.634] [0.0000], Avg: [-859.218 -859.218 -859.218] (1.000)
Step: 52349, Reward: [-1123.658 -1123.658 -1123.658] [0.0000], Avg: [-859.47 -859.47 -859.47] (1.000)
Step: 52399, Reward: [-880.285 -880.285 -880.285] [0.0000], Avg: [-859.49 -859.49 -859.49] (1.000)
Step: 52449, Reward: [-1156.399 -1156.399 -1156.399] [0.0000], Avg: [-859.773 -859.773 -859.773] (1.000)
Step: 52499, Reward: [-1255.718 -1255.718 -1255.718] [0.0000], Avg: [-860.15 -860.15 -860.15] (1.000)
Step: 52549, Reward: [-933.944 -933.944 -933.944] [0.0000], Avg: [-860.221 -860.221 -860.221] (1.000)
Step: 52599, Reward: [-706.94 -706.94 -706.94] [0.0000], Avg: [-860.075 -860.075 -860.075] (1.000)
Step: 52649, Reward: [-668.895 -668.895 -668.895] [0.0000], Avg: [-859.893 -859.893 -859.893] (1.000)
Step: 52699, Reward: [-1036.812 -1036.812 -1036.812] [0.0000], Avg: [-860.061 -860.061 -860.061] (1.000)
Step: 52749, Reward: [-555.206 -555.206 -555.206] [0.0000], Avg: [-859.772 -859.772 -859.772] (1.000)
Step: 52799, Reward: [-1182.646 -1182.646 -1182.646] [0.0000], Avg: [-860.078 -860.078 -860.078] (1.000)
Step: 52849, Reward: [-1513.612 -1513.612 -1513.612] [0.0000], Avg: [-860.696 -860.696 -860.696] (1.000)
Step: 52899, Reward: [-477.749 -477.749 -477.749] [0.0000], Avg: [-860.334 -860.334 -860.334] (1.000)
Step: 52949, Reward: [-1302.49 -1302.49 -1302.49] [0.0000], Avg: [-860.752 -860.752 -860.752] (1.000)
Step: 52999, Reward: [-1302.613 -1302.613 -1302.613] [0.0000], Avg: [-861.169 -861.169 -861.169] (1.000)
Step: 53049, Reward: [-1216.088 -1216.088 -1216.088] [0.0000], Avg: [-861.503 -861.503 -861.503] (1.000)
Step: 53099, Reward: [-1178.469 -1178.469 -1178.469] [0.0000], Avg: [-861.802 -861.802 -861.802] (1.000)
Step: 53149, Reward: [-697.225 -697.225 -697.225] [0.0000], Avg: [-861.647 -861.647 -861.647] (1.000)
Step: 53199, Reward: [-724.113 -724.113 -724.113] [0.0000], Avg: [-861.518 -861.518 -861.518] (1.000)
Step: 53249, Reward: [-884.312 -884.312 -884.312] [0.0000], Avg: [-861.539 -861.539 -861.539] (1.000)
Step: 53299, Reward: [-1199.675 -1199.675 -1199.675] [0.0000], Avg: [-861.856 -861.856 -861.856] (1.000)
Step: 53349, Reward: [-683.877 -683.877 -683.877] [0.0000], Avg: [-861.689 -861.689 -861.689] (1.000)
Step: 53399, Reward: [-693.068 -693.068 -693.068] [0.0000], Avg: [-861.532 -861.532 -861.532] (1.000)
Step: 53449, Reward: [-930.652 -930.652 -930.652] [0.0000], Avg: [-861.596 -861.596 -861.596] (1.000)
Step: 53499, Reward: [-1640.047 -1640.047 -1640.047] [0.0000], Avg: [-862.324 -862.324 -862.324] (1.000)
Step: 53549, Reward: [-1166.639 -1166.639 -1166.639] [0.0000], Avg: [-862.608 -862.608 -862.608] (1.000)
Step: 53599, Reward: [-918.474 -918.474 -918.474] [0.0000], Avg: [-862.66 -862.66 -862.66] (1.000)
Step: 53649, Reward: [-772.892 -772.892 -772.892] [0.0000], Avg: [-862.576 -862.576 -862.576] (1.000)
Step: 53699, Reward: [-858.415 -858.415 -858.415] [0.0000], Avg: [-862.572 -862.572 -862.572] (1.000)
Step: 53749, Reward: [-654.2 -654.2 -654.2] [0.0000], Avg: [-862.379 -862.379 -862.379] (1.000)
Step: 53799, Reward: [-938.42 -938.42 -938.42] [0.0000], Avg: [-862.449 -862.449 -862.449] (1.000)
Step: 53849, Reward: [-1221.195 -1221.195 -1221.195] [0.0000], Avg: [-862.782 -862.782 -862.782] (1.000)
Step: 53899, Reward: [-762.672 -762.672 -762.672] [0.0000], Avg: [-862.69 -862.69 -862.69] (1.000)
Step: 53949, Reward: [-647.75 -647.75 -647.75] [0.0000], Avg: [-862.49 -862.49 -862.49] (1.000)
Step: 53999, Reward: [-880.325 -880.325 -880.325] [0.0000], Avg: [-862.507 -862.507 -862.507] (1.000)
Step: 54049, Reward: [-1009.428 -1009.428 -1009.428] [0.0000], Avg: [-862.643 -862.643 -862.643] (1.000)
Step: 54099, Reward: [-1189.654 -1189.654 -1189.654] [0.0000], Avg: [-862.945 -862.945 -862.945] (1.000)
Step: 54149, Reward: [-893.938 -893.938 -893.938] [0.0000], Avg: [-862.974 -862.974 -862.974] (1.000)
Step: 54199, Reward: [-1624.918 -1624.918 -1624.918] [0.0000], Avg: [-863.676 -863.676 -863.676] (1.000)
Step: 54249, Reward: [-1554.143 -1554.143 -1554.143] [0.0000], Avg: [-864.313 -864.313 -864.313] (1.000)
Step: 54299, Reward: [-552.865 -552.865 -552.865] [0.0000], Avg: [-864.026 -864.026 -864.026] (1.000)
Step: 54349, Reward: [-486.251 -486.251 -486.251] [0.0000], Avg: [-863.679 -863.679 -863.679] (1.000)
Step: 54399, Reward: [-913.232 -913.232 -913.232] [0.0000], Avg: [-863.724 -863.724 -863.724] (1.000)
Step: 54449, Reward: [-1217.887 -1217.887 -1217.887] [0.0000], Avg: [-864.049 -864.049 -864.049] (1.000)
Step: 54499, Reward: [-874.828 -874.828 -874.828] [0.0000], Avg: [-864.059 -864.059 -864.059] (1.000)
Step: 54549, Reward: [-1245.641 -1245.641 -1245.641] [0.0000], Avg: [-864.409 -864.409 -864.409] (1.000)
Step: 54599, Reward: [-728.633 -728.633 -728.633] [0.0000], Avg: [-864.285 -864.285 -864.285] (1.000)
Step: 54649, Reward: [-1287.217 -1287.217 -1287.217] [0.0000], Avg: [-864.672 -864.672 -864.672] (1.000)
Step: 54699, Reward: [-1175.687 -1175.687 -1175.687] [0.0000], Avg: [-864.956 -864.956 -864.956] (1.000)
Step: 54749, Reward: [-885.772 -885.772 -885.772] [0.0000], Avg: [-864.975 -864.975 -864.975] (1.000)
Step: 54799, Reward: [-1210.058 -1210.058 -1210.058] [0.0000], Avg: [-865.29 -865.29 -865.29] (1.000)
Step: 54849, Reward: [-1360.999 -1360.999 -1360.999] [0.0000], Avg: [-865.742 -865.742 -865.742] (1.000)
Step: 54899, Reward: [-632.706 -632.706 -632.706] [0.0000], Avg: [-865.529 -865.529 -865.529] (1.000)
Step: 54949, Reward: [-752.367 -752.367 -752.367] [0.0000], Avg: [-865.426 -865.426 -865.426] (1.000)
Step: 54999, Reward: [-1305.227 -1305.227 -1305.227] [0.0000], Avg: [-865.826 -865.826 -865.826] (1.000)
Step: 55049, Reward: [-910.828 -910.828 -910.828] [0.0000], Avg: [-865.867 -865.867 -865.867] (1.000)
Step: 55099, Reward: [-915.744 -915.744 -915.744] [0.0000], Avg: [-865.912 -865.912 -865.912] (1.000)
Step: 55149, Reward: [-507.434 -507.434 -507.434] [0.0000], Avg: [-865.587 -865.587 -865.587] (1.000)
Step: 55199, Reward: [-1283.159 -1283.159 -1283.159] [0.0000], Avg: [-865.966 -865.966 -865.966] (1.000)
Step: 55249, Reward: [-762.889 -762.889 -762.889] [0.0000], Avg: [-865.872 -865.872 -865.872] (1.000)
Step: 55299, Reward: [-1568.474 -1568.474 -1568.474] [0.0000], Avg: [-866.508 -866.508 -866.508] (1.000)
Step: 55349, Reward: [-487.227 -487.227 -487.227] [0.0000], Avg: [-866.165 -866.165 -866.165] (1.000)
Step: 55399, Reward: [-596.486 -596.486 -596.486] [0.0000], Avg: [-865.922 -865.922 -865.922] (1.000)
Step: 55449, Reward: [-1157.336 -1157.336 -1157.336] [0.0000], Avg: [-866.184 -866.184 -866.184] (1.000)
Step: 55499, Reward: [-1865.271 -1865.271 -1865.271] [0.0000], Avg: [-867.084 -867.084 -867.084] (1.000)
Step: 55549, Reward: [-1091.812 -1091.812 -1091.812] [0.0000], Avg: [-867.287 -867.287 -867.287] (1.000)
Step: 55599, Reward: [-853.587 -853.587 -853.587] [0.0000], Avg: [-867.274 -867.274 -867.274] (1.000)
Step: 55649, Reward: [-1115.413 -1115.413 -1115.413] [0.0000], Avg: [-867.497 -867.497 -867.497] (1.000)
Step: 55699, Reward: [-1248.104 -1248.104 -1248.104] [0.0000], Avg: [-867.839 -867.839 -867.839] (1.000)
Step: 55749, Reward: [-1003.692 -1003.692 -1003.692] [0.0000], Avg: [-867.961 -867.961 -867.961] (1.000)
Step: 55799, Reward: [-1281.791 -1281.791 -1281.791] [0.0000], Avg: [-868.332 -868.332 -868.332] (1.000)
Step: 55849, Reward: [-1520.33 -1520.33 -1520.33] [0.0000], Avg: [-868.915 -868.915 -868.915] (1.000)
Step: 55899, Reward: [-1146.389 -1146.389 -1146.389] [0.0000], Avg: [-869.164 -869.164 -869.164] (1.000)
Step: 55949, Reward: [-1182.89 -1182.89 -1182.89] [0.0000], Avg: [-869.444 -869.444 -869.444] (1.000)
Step: 55999, Reward: [-1919.95 -1919.95 -1919.95] [0.0000], Avg: [-870.382 -870.382 -870.382] (1.000)
Step: 56049, Reward: [-1047.413 -1047.413 -1047.413] [0.0000], Avg: [-870.54 -870.54 -870.54] (1.000)
Step: 56099, Reward: [-1445.773 -1445.773 -1445.773] [0.0000], Avg: [-871.052 -871.052 -871.052] (1.000)
Step: 56149, Reward: [-1865.99 -1865.99 -1865.99] [0.0000], Avg: [-871.938 -871.938 -871.938] (1.000)
Step: 56199, Reward: [-1506.779 -1506.779 -1506.779] [0.0000], Avg: [-872.503 -872.503 -872.503] (1.000)
Step: 56249, Reward: [-1913.089 -1913.089 -1913.089] [0.0000], Avg: [-873.428 -873.428 -873.428] (1.000)
Step: 56299, Reward: [-1387.342 -1387.342 -1387.342] [0.0000], Avg: [-873.885 -873.885 -873.885] (1.000)
Step: 56349, Reward: [-1790.486 -1790.486 -1790.486] [0.0000], Avg: [-874.698 -874.698 -874.698] (1.000)
Step: 56399, Reward: [-1277.138 -1277.138 -1277.138] [0.0000], Avg: [-875.055 -875.055 -875.055] (1.000)
Step: 56449, Reward: [-1533.569 -1533.569 -1533.569] [0.0000], Avg: [-875.638 -875.638 -875.638] (1.000)
Step: 56499, Reward: [-1278.191 -1278.191 -1278.191] [0.0000], Avg: [-875.994 -875.994 -875.994] (1.000)
Step: 56549, Reward: [-1373.084 -1373.084 -1373.084] [0.0000], Avg: [-876.434 -876.434 -876.434] (1.000)
Step: 56599, Reward: [-1285.321 -1285.321 -1285.321] [0.0000], Avg: [-876.795 -876.795 -876.795] (1.000)
Step: 56649, Reward: [-1716.41 -1716.41 -1716.41] [0.0000], Avg: [-877.536 -877.536 -877.536] (1.000)
Step: 56699, Reward: [-1016.718 -1016.718 -1016.718] [0.0000], Avg: [-877.659 -877.659 -877.659] (1.000)
Step: 56749, Reward: [-1739.894 -1739.894 -1739.894] [0.0000], Avg: [-878.418 -878.418 -878.418] (1.000)
Step: 56799, Reward: [-1556.226 -1556.226 -1556.226] [0.0000], Avg: [-879.015 -879.015 -879.015] (1.000)
Step: 56849, Reward: [-1855.89 -1855.89 -1855.89] [0.0000], Avg: [-879.874 -879.874 -879.874] (1.000)
Step: 56899, Reward: [-779.791 -779.791 -779.791] [0.0000], Avg: [-879.786 -879.786 -879.786] (1.000)
Step: 56949, Reward: [-1359.834 -1359.834 -1359.834] [0.0000], Avg: [-880.208 -880.208 -880.208] (1.000)
Step: 56999, Reward: [-1855.916 -1855.916 -1855.916] [0.0000], Avg: [-881.064 -881.064 -881.064] (1.000)
Step: 57049, Reward: [-620.618 -620.618 -620.618] [0.0000], Avg: [-880.835 -880.835 -880.835] (1.000)
Step: 57099, Reward: [-1111.5 -1111.5 -1111.5] [0.0000], Avg: [-881.037 -881.037 -881.037] (1.000)
Step: 57149, Reward: [-925.002 -925.002 -925.002] [0.0000], Avg: [-881.076 -881.076 -881.076] (1.000)
Step: 57199, Reward: [-1167.458 -1167.458 -1167.458] [0.0000], Avg: [-881.326 -881.326 -881.326] (1.000)
Step: 57249, Reward: [-924.772 -924.772 -924.772] [0.0000], Avg: [-881.364 -881.364 -881.364] (1.000)
Step: 57299, Reward: [-811.675 -811.675 -811.675] [0.0000], Avg: [-881.303 -881.303 -881.303] (1.000)
Step: 57349, Reward: [-841.801 -841.801 -841.801] [0.0000], Avg: [-881.269 -881.269 -881.269] (1.000)
Step: 57399, Reward: [-724.976 -724.976 -724.976] [0.0000], Avg: [-881.133 -881.133 -881.133] (1.000)
Step: 57449, Reward: [-1327.498 -1327.498 -1327.498] [0.0000], Avg: [-881.521 -881.521 -881.521] (1.000)
Step: 57499, Reward: [-635.833 -635.833 -635.833] [0.0000], Avg: [-881.307 -881.307 -881.307] (1.000)
Step: 57549, Reward: [-928.254 -928.254 -928.254] [0.0000], Avg: [-881.348 -881.348 -881.348] (1.000)
Step: 57599, Reward: [-924.305 -924.305 -924.305] [0.0000], Avg: [-881.386 -881.386 -881.386] (1.000)
Step: 57649, Reward: [-808.672 -808.672 -808.672] [0.0000], Avg: [-881.323 -881.323 -881.323] (1.000)
Step: 57699, Reward: [-1154.39 -1154.39 -1154.39] [0.0000], Avg: [-881.559 -881.559 -881.559] (1.000)
Step: 57749, Reward: [-598.664 -598.664 -598.664] [0.0000], Avg: [-881.314 -881.314 -881.314] (1.000)
Step: 57799, Reward: [-1359.967 -1359.967 -1359.967] [0.0000], Avg: [-881.728 -881.728 -881.728] (1.000)
Step: 57849, Reward: [-912.634 -912.634 -912.634] [0.0000], Avg: [-881.755 -881.755 -881.755] (1.000)
Step: 57899, Reward: [-804.384 -804.384 -804.384] [0.0000], Avg: [-881.688 -881.688 -881.688] (1.000)
Step: 57949, Reward: [-1347.744 -1347.744 -1347.744] [0.0000], Avg: [-882.09 -882.09 -882.09] (1.000)
Step: 57999, Reward: [-1333.181 -1333.181 -1333.181] [0.0000], Avg: [-882.479 -882.479 -882.479] (1.000)
Step: 58049, Reward: [-1505.879 -1505.879 -1505.879] [0.0000], Avg: [-883.016 -883.016 -883.016] (1.000)
Step: 58099, Reward: [-1643.311 -1643.311 -1643.311] [0.0000], Avg: [-883.67 -883.67 -883.67] (1.000)
Step: 58149, Reward: [-1614.903 -1614.903 -1614.903] [0.0000], Avg: [-884.299 -884.299 -884.299] (1.000)
Step: 58199, Reward: [-1788.43 -1788.43 -1788.43] [0.0000], Avg: [-885.076 -885.076 -885.076] (1.000)
Step: 58249, Reward: [-1517.627 -1517.627 -1517.627] [0.0000], Avg: [-885.619 -885.619 -885.619] (1.000)
Step: 58299, Reward: [-1229.171 -1229.171 -1229.171] [0.0000], Avg: [-885.913 -885.913 -885.913] (1.000)
Step: 58349, Reward: [-1363.493 -1363.493 -1363.493] [0.0000], Avg: [-886.323 -886.323 -886.323] (1.000)
Step: 58399, Reward: [-1022.255 -1022.255 -1022.255] [0.0000], Avg: [-886.439 -886.439 -886.439] (1.000)
Step: 58449, Reward: [-579.197 -579.197 -579.197] [0.0000], Avg: [-886.176 -886.176 -886.176] (1.000)
Step: 58499, Reward: [-841.856 -841.856 -841.856] [0.0000], Avg: [-886.138 -886.138 -886.138] (1.000)
Step: 58549, Reward: [-1101.87 -1101.87 -1101.87] [0.0000], Avg: [-886.323 -886.323 -886.323] (1.000)
Step: 58599, Reward: [-1092.734 -1092.734 -1092.734] [0.0000], Avg: [-886.499 -886.499 -886.499] (1.000)
Step: 58649, Reward: [-651.788 -651.788 -651.788] [0.0000], Avg: [-886.299 -886.299 -886.299] (1.000)
Step: 58699, Reward: [-1083.344 -1083.344 -1083.344] [0.0000], Avg: [-886.467 -886.467 -886.467] (1.000)
Step: 58749, Reward: [-1043.963 -1043.963 -1043.963] [0.0000], Avg: [-886.601 -886.601 -886.601] (1.000)
Step: 58799, Reward: [-812.255 -812.255 -812.255] [0.0000], Avg: [-886.537 -886.537 -886.537] (1.000)
Step: 58849, Reward: [-1090.159 -1090.159 -1090.159] [0.0000], Avg: [-886.71 -886.71 -886.71] (1.000)
Step: 58899, Reward: [-925.41 -925.41 -925.41] [0.0000], Avg: [-886.743 -886.743 -886.743] (1.000)
Step: 58949, Reward: [-825.367 -825.367 -825.367] [0.0000], Avg: [-886.691 -886.691 -886.691] (1.000)
Step: 58999, Reward: [-962.374 -962.374 -962.374] [0.0000], Avg: [-886.755 -886.755 -886.755] (1.000)
Step: 59049, Reward: [-589.908 -589.908 -589.908] [0.0000], Avg: [-886.504 -886.504 -886.504] (1.000)
Step: 59099, Reward: [-915.496 -915.496 -915.496] [0.0000], Avg: [-886.528 -886.528 -886.528] (1.000)
Step: 59149, Reward: [-690.685 -690.685 -690.685] [0.0000], Avg: [-886.363 -886.363 -886.363] (1.000)
Step: 59199, Reward: [-470.26 -470.26 -470.26] [0.0000], Avg: [-886.011 -886.011 -886.011] (1.000)
Step: 59249, Reward: [-652.398 -652.398 -652.398] [0.0000], Avg: [-885.814 -885.814 -885.814] (1.000)
Step: 59299, Reward: [-499.557 -499.557 -499.557] [0.0000], Avg: [-885.489 -885.489 -885.489] (1.000)
Step: 59349, Reward: [-988.724 -988.724 -988.724] [0.0000], Avg: [-885.576 -885.576 -885.576] (1.000)
Step: 59399, Reward: [-972.243 -972.243 -972.243] [0.0000], Avg: [-885.649 -885.649 -885.649] (1.000)
Step: 59449, Reward: [-928.284 -928.284 -928.284] [0.0000], Avg: [-885.684 -885.684 -885.684] (1.000)
Step: 59499, Reward: [-533.762 -533.762 -533.762] [0.0000], Avg: [-885.389 -885.389 -885.389] (1.000)
Step: 59549, Reward: [-894.604 -894.604 -894.604] [0.0000], Avg: [-885.396 -885.396 -885.396] (1.000)
Step: 59599, Reward: [-863.785 -863.785 -863.785] [0.0000], Avg: [-885.378 -885.378 -885.378] (1.000)
Step: 59649, Reward: [-569.611 -569.611 -569.611] [0.0000], Avg: [-885.114 -885.114 -885.114] (1.000)
Step: 59699, Reward: [-542.751 -542.751 -542.751] [0.0000], Avg: [-884.827 -884.827 -884.827] (1.000)
Step: 59749, Reward: [-541.315 -541.315 -541.315] [0.0000], Avg: [-884.539 -884.539 -884.539] (1.000)
Step: 59799, Reward: [-1108.195 -1108.195 -1108.195] [0.0000], Avg: [-884.726 -884.726 -884.726] (1.000)
Step: 59849, Reward: [-1291.658 -1291.658 -1291.658] [0.0000], Avg: [-885.066 -885.066 -885.066] (1.000)
Step: 59899, Reward: [-848.795 -848.795 -848.795] [0.0000], Avg: [-885.036 -885.036 -885.036] (1.000)
Step: 59949, Reward: [-979.403 -979.403 -979.403] [0.0000], Avg: [-885.115 -885.115 -885.115] (1.000)
Step: 59999, Reward: [-674.848 -674.848 -674.848] [0.0000], Avg: [-884.94 -884.94 -884.94] (1.000)
Step: 60049, Reward: [-882.285 -882.285 -882.285] [0.0000], Avg: [-884.937 -884.937 -884.937] (1.000)
Step: 60099, Reward: [-880.941 -880.941 -880.941] [0.0000], Avg: [-884.934 -884.934 -884.934] (1.000)
Step: 60149, Reward: [-604.726 -604.726 -604.726] [0.0000], Avg: [-884.701 -884.701 -884.701] (1.000)
Step: 60199, Reward: [-552.513 -552.513 -552.513] [0.0000], Avg: [-884.425 -884.425 -884.425] (1.000)
Step: 60249, Reward: [-828.694 -828.694 -828.694] [0.0000], Avg: [-884.379 -884.379 -884.379] (1.000)
Step: 60299, Reward: [-361.02 -361.02 -361.02] [0.0000], Avg: [-883.945 -883.945 -883.945] (1.000)
Step: 60349, Reward: [-1333.307 -1333.307 -1333.307] [0.0000], Avg: [-884.317 -884.317 -884.317] (1.000)
Step: 60399, Reward: [-588.305 -588.305 -588.305] [0.0000], Avg: [-884.072 -884.072 -884.072] (1.000)
Step: 60449, Reward: [-764.592 -764.592 -764.592] [0.0000], Avg: [-883.973 -883.973 -883.973] (1.000)
Step: 60499, Reward: [-600.556 -600.556 -600.556] [0.0000], Avg: [-883.739 -883.739 -883.739] (1.000)
Step: 60549, Reward: [-751.577 -751.577 -751.577] [0.0000], Avg: [-883.63 -883.63 -883.63] (1.000)
Step: 60599, Reward: [-536.968 -536.968 -536.968] [0.0000], Avg: [-883.344 -883.344 -883.344] (1.000)
Step: 60649, Reward: [-1293.307 -1293.307 -1293.307] [0.0000], Avg: [-883.682 -883.682 -883.682] (1.000)
Step: 60699, Reward: [-360.947 -360.947 -360.947] [0.0000], Avg: [-883.251 -883.251 -883.251] (1.000)
Step: 60749, Reward: [-489.019 -489.019 -489.019] [0.0000], Avg: [-882.927 -882.927 -882.927] (1.000)
Step: 60799, Reward: [-895.966 -895.966 -895.966] [0.0000], Avg: [-882.938 -882.938 -882.938] (1.000)
Step: 60849, Reward: [-617.344 -617.344 -617.344] [0.0000], Avg: [-882.719 -882.719 -882.719] (1.000)
Step: 60899, Reward: [-755.977 -755.977 -755.977] [0.0000], Avg: [-882.615 -882.615 -882.615] (1.000)
Step: 60949, Reward: [-656.687 -656.687 -656.687] [0.0000], Avg: [-882.43 -882.43 -882.43] (1.000)
Step: 60999, Reward: [-636.354 -636.354 -636.354] [0.0000], Avg: [-882.228 -882.228 -882.228] (1.000)
Step: 61049, Reward: [-544.89 -544.89 -544.89] [0.0000], Avg: [-881.952 -881.952 -881.952] (1.000)
Step: 61099, Reward: [-905.159 -905.159 -905.159] [0.0000], Avg: [-881.971 -881.971 -881.971] (1.000)
Step: 61149, Reward: [-799.262 -799.262 -799.262] [0.0000], Avg: [-881.903 -881.903 -881.903] (1.000)
Step: 61199, Reward: [-590.633 -590.633 -590.633] [0.0000], Avg: [-881.665 -881.665 -881.665] (1.000)
Step: 61249, Reward: [-964.026 -964.026 -964.026] [0.0000], Avg: [-881.733 -881.733 -881.733] (1.000)
Step: 61299, Reward: [-573.221 -573.221 -573.221] [0.0000], Avg: [-881.481 -881.481 -881.481] (1.000)
Step: 61349, Reward: [-1097.209 -1097.209 -1097.209] [0.0000], Avg: [-881.657 -881.657 -881.657] (1.000)
Step: 61399, Reward: [-346.533 -346.533 -346.533] [0.0000], Avg: [-881.221 -881.221 -881.221] (1.000)
Step: 61449, Reward: [-476.169 -476.169 -476.169] [0.0000], Avg: [-880.892 -880.892 -880.892] (1.000)
Step: 61499, Reward: [-778.401 -778.401 -778.401] [0.0000], Avg: [-880.808 -880.808 -880.808] (1.000)
Step: 61549, Reward: [-1136.764 -1136.764 -1136.764] [0.0000], Avg: [-881.016 -881.016 -881.016] (1.000)
Step: 61599, Reward: [-404.889 -404.889 -404.889] [0.0000], Avg: [-880.63 -880.63 -880.63] (1.000)
Step: 61649, Reward: [-616.431 -616.431 -616.431] [0.0000], Avg: [-880.415 -880.415 -880.415] (1.000)
Step: 61699, Reward: [-582.306 -582.306 -582.306] [0.0000], Avg: [-880.174 -880.174 -880.174] (1.000)
Step: 61749, Reward: [-483.446 -483.446 -483.446] [0.0000], Avg: [-879.853 -879.853 -879.853] (1.000)
Step: 61799, Reward: [-538.446 -538.446 -538.446] [0.0000], Avg: [-879.576 -879.576 -879.576] (1.000)
Step: 61849, Reward: [-1197.082 -1197.082 -1197.082] [0.0000], Avg: [-879.833 -879.833 -879.833] (1.000)
Step: 61899, Reward: [-518.364 -518.364 -518.364] [0.0000], Avg: [-879.541 -879.541 -879.541] (1.000)
Step: 61949, Reward: [-682.931 -682.931 -682.931] [0.0000], Avg: [-879.382 -879.382 -879.382] (1.000)
Step: 61999, Reward: [-1040.317 -1040.317 -1040.317] [0.0000], Avg: [-879.512 -879.512 -879.512] (1.000)
Step: 62049, Reward: [-641.915 -641.915 -641.915] [0.0000], Avg: [-879.321 -879.321 -879.321] (1.000)
Step: 62099, Reward: [-405.095 -405.095 -405.095] [0.0000], Avg: [-878.939 -878.939 -878.939] (1.000)
Step: 62149, Reward: [-715.233 -715.233 -715.233] [0.0000], Avg: [-878.807 -878.807 -878.807] (1.000)
Step: 62199, Reward: [-524.214 -524.214 -524.214] [0.0000], Avg: [-878.522 -878.522 -878.522] (1.000)
Step: 62249, Reward: [-617.405 -617.405 -617.405] [0.0000], Avg: [-878.312 -878.312 -878.312] (1.000)
Step: 62299, Reward: [-622.37 -622.37 -622.37] [0.0000], Avg: [-878.107 -878.107 -878.107] (1.000)
Step: 62349, Reward: [-856.599 -856.599 -856.599] [0.0000], Avg: [-878.09 -878.09 -878.09] (1.000)
Step: 62399, Reward: [-621.179 -621.179 -621.179] [0.0000], Avg: [-877.884 -877.884 -877.884] (1.000)
Step: 62449, Reward: [-606.517 -606.517 -606.517] [0.0000], Avg: [-877.667 -877.667 -877.667] (1.000)
Step: 62499, Reward: [-597.528 -597.528 -597.528] [0.0000], Avg: [-877.442 -877.442 -877.442] (1.000)
Step: 62549, Reward: [-518.515 -518.515 -518.515] [0.0000], Avg: [-877.156 -877.156 -877.156] (1.000)
Step: 62599, Reward: [-840.907 -840.907 -840.907] [0.0000], Avg: [-877.127 -877.127 -877.127] (1.000)
Step: 62649, Reward: [-564.857 -564.857 -564.857] [0.0000], Avg: [-876.877 -876.877 -876.877] (1.000)
Step: 62699, Reward: [-606.951 -606.951 -606.951] [0.0000], Avg: [-876.662 -876.662 -876.662] (1.000)
Step: 62749, Reward: [-596.454 -596.454 -596.454] [0.0000], Avg: [-876.439 -876.439 -876.439] (1.000)
Step: 62799, Reward: [-550.521 -550.521 -550.521] [0.0000], Avg: [-876.179 -876.179 -876.179] (1.000)
Step: 62849, Reward: [-538.156 -538.156 -538.156] [0.0000], Avg: [-875.91 -875.91 -875.91] (1.000)
Step: 62899, Reward: [-443.15 -443.15 -443.15] [0.0000], Avg: [-875.566 -875.566 -875.566] (1.000)
Step: 62949, Reward: [-565.22 -565.22 -565.22] [0.0000], Avg: [-875.32 -875.32 -875.32] (1.000)
Step: 62999, Reward: [-591.088 -591.088 -591.088] [0.0000], Avg: [-875.094 -875.094 -875.094] (1.000)
Step: 63049, Reward: [-452.584 -452.584 -452.584] [0.0000], Avg: [-874.759 -874.759 -874.759] (1.000)
Step: 63099, Reward: [-846.788 -846.788 -846.788] [0.0000], Avg: [-874.737 -874.737 -874.737] (1.000)
Step: 63149, Reward: [-456.895 -456.895 -456.895] [0.0000], Avg: [-874.406 -874.406 -874.406] (1.000)
Step: 63199, Reward: [-727.002 -727.002 -727.002] [0.0000], Avg: [-874.29 -874.29 -874.29] (1.000)
Step: 63249, Reward: [-952.659 -952.659 -952.659] [0.0000], Avg: [-874.352 -874.352 -874.352] (1.000)
Step: 63299, Reward: [-492.158 -492.158 -492.158] [0.0000], Avg: [-874.05 -874.05 -874.05] (1.000)
Step: 63349, Reward: [-961.971 -961.971 -961.971] [0.0000], Avg: [-874.119 -874.119 -874.119] (1.000)
Step: 63399, Reward: [-826.271 -826.271 -826.271] [0.0000], Avg: [-874.081 -874.081 -874.081] (1.000)
Step: 63449, Reward: [-872.935 -872.935 -872.935] [0.0000], Avg: [-874.081 -874.081 -874.081] (1.000)
Step: 63499, Reward: [-581.816 -581.816 -581.816] [0.0000], Avg: [-873.85 -873.85 -873.85] (1.000)
Step: 63549, Reward: [-476.005 -476.005 -476.005] [0.0000], Avg: [-873.537 -873.537 -873.537] (1.000)
Step: 63599, Reward: [-508.402 -508.402 -508.402] [0.0000], Avg: [-873.25 -873.25 -873.25] (1.000)
Step: 63649, Reward: [-658.552 -658.552 -658.552] [0.0000], Avg: [-873.082 -873.082 -873.082] (1.000)
Step: 63699, Reward: [-651.765 -651.765 -651.765] [0.0000], Avg: [-872.908 -872.908 -872.908] (1.000)
Step: 63749, Reward: [-546.72 -546.72 -546.72] [0.0000], Avg: [-872.652 -872.652 -872.652] (1.000)
Step: 63799, Reward: [-730.397 -730.397 -730.397] [0.0000], Avg: [-872.541 -872.541 -872.541] (1.000)
Step: 63849, Reward: [-753.586 -753.586 -753.586] [0.0000], Avg: [-872.447 -872.447 -872.447] (1.000)
Step: 63899, Reward: [-819.185 -819.185 -819.185] [0.0000], Avg: [-872.406 -872.406 -872.406] (1.000)
Step: 63949, Reward: [-549.235 -549.235 -549.235] [0.0000], Avg: [-872.153 -872.153 -872.153] (1.000)
Step: 63999, Reward: [-1133.689 -1133.689 -1133.689] [0.0000], Avg: [-872.357 -872.357 -872.357] (1.000)
Step: 64049, Reward: [-967.478 -967.478 -967.478] [0.0000], Avg: [-872.432 -872.432 -872.432] (1.000)
Step: 64099, Reward: [-1126.039 -1126.039 -1126.039] [0.0000], Avg: [-872.63 -872.63 -872.63] (1.000)
Step: 64149, Reward: [-451.522 -451.522 -451.522] [0.0000], Avg: [-872.301 -872.301 -872.301] (1.000)
Step: 64199, Reward: [-598.411 -598.411 -598.411] [0.0000], Avg: [-872.088 -872.088 -872.088] (1.000)
Step: 64249, Reward: [-661.089 -661.089 -661.089] [0.0000], Avg: [-871.924 -871.924 -871.924] (1.000)
Step: 64299, Reward: [-775.874 -775.874 -775.874] [0.0000], Avg: [-871.849 -871.849 -871.849] (1.000)
Step: 64349, Reward: [-615.25 -615.25 -615.25] [0.0000], Avg: [-871.65 -871.65 -871.65] (1.000)
Step: 64399, Reward: [-985.167 -985.167 -985.167] [0.0000], Avg: [-871.738 -871.738 -871.738] (1.000)
Step: 64449, Reward: [-804.55 -804.55 -804.55] [0.0000], Avg: [-871.686 -871.686 -871.686] (1.000)
Step: 64499, Reward: [-428.979 -428.979 -428.979] [0.0000], Avg: [-871.343 -871.343 -871.343] (1.000)
Step: 64549, Reward: [-1223.957 -1223.957 -1223.957] [0.0000], Avg: [-871.616 -871.616 -871.616] (1.000)
Step: 64599, Reward: [-757.701 -757.701 -757.701] [0.0000], Avg: [-871.528 -871.528 -871.528] (1.000)
Step: 64649, Reward: [-553.455 -553.455 -553.455] [0.0000], Avg: [-871.282 -871.282 -871.282] (1.000)
Step: 64699, Reward: [-675.632 -675.632 -675.632] [0.0000], Avg: [-871.13 -871.13 -871.13] (1.000)
Step: 64749, Reward: [-608.398 -608.398 -608.398] [0.0000], Avg: [-870.927 -870.927 -870.927] (1.000)
Step: 64799, Reward: [-717.45 -717.45 -717.45] [0.0000], Avg: [-870.809 -870.809 -870.809] (1.000)
Step: 64849, Reward: [-1087.437 -1087.437 -1087.437] [0.0000], Avg: [-870.976 -870.976 -870.976] (1.000)
Step: 64899, Reward: [-664.402 -664.402 -664.402] [0.0000], Avg: [-870.817 -870.817 -870.817] (1.000)
Step: 64949, Reward: [-1069.328 -1069.328 -1069.328] [0.0000], Avg: [-870.97 -870.97 -870.97] (1.000)
Step: 64999, Reward: [-1122.223 -1122.223 -1122.223] [0.0000], Avg: [-871.163 -871.163 -871.163] (1.000)
Step: 65049, Reward: [-874.91 -874.91 -874.91] [0.0000], Avg: [-871.166 -871.166 -871.166] (1.000)
Step: 65099, Reward: [-554.616 -554.616 -554.616] [0.0000], Avg: [-870.923 -870.923 -870.923] (1.000)
Step: 65149, Reward: [-1073.689 -1073.689 -1073.689] [0.0000], Avg: [-871.078 -871.078 -871.078] (1.000)
Step: 65199, Reward: [-539.952 -539.952 -539.952] [0.0000], Avg: [-870.824 -870.824 -870.824] (1.000)
Step: 65249, Reward: [-758.017 -758.017 -758.017] [0.0000], Avg: [-870.738 -870.738 -870.738] (1.000)
Step: 65299, Reward: [-913.827 -913.827 -913.827] [0.0000], Avg: [-870.771 -870.771 -870.771] (1.000)
Step: 65349, Reward: [-928.646 -928.646 -928.646] [0.0000], Avg: [-870.815 -870.815 -870.815] (1.000)
Step: 65399, Reward: [-668.38 -668.38 -668.38] [0.0000], Avg: [-870.66 -870.66 -870.66] (1.000)
Step: 65449, Reward: [-1301.368 -1301.368 -1301.368] [0.0000], Avg: [-870.99 -870.99 -870.99] (1.000)
Step: 65499, Reward: [-1047.77 -1047.77 -1047.77] [0.0000], Avg: [-871.124 -871.124 -871.124] (1.000)
Step: 65549, Reward: [-1401.593 -1401.593 -1401.593] [0.0000], Avg: [-871.529 -871.529 -871.529] (1.000)
Step: 65599, Reward: [-682.822 -682.822 -682.822] [0.0000], Avg: [-871.385 -871.385 -871.385] (1.000)
Step: 65649, Reward: [-788.086 -788.086 -788.086] [0.0000], Avg: [-871.322 -871.322 -871.322] (1.000)
Step: 65699, Reward: [-619.657 -619.657 -619.657] [0.0000], Avg: [-871.13 -871.13 -871.13] (1.000)
Step: 65749, Reward: [-1048.295 -1048.295 -1048.295] [0.0000], Avg: [-871.265 -871.265 -871.265] (1.000)
Step: 65799, Reward: [-520.358 -520.358 -520.358] [0.0000], Avg: [-870.998 -870.998 -870.998] (1.000)
Step: 65849, Reward: [-776.217 -776.217 -776.217] [0.0000], Avg: [-870.926 -870.926 -870.926] (1.000)
Step: 65899, Reward: [-833.659 -833.659 -833.659] [0.0000], Avg: [-870.898 -870.898 -870.898] (1.000)
Step: 65949, Reward: [-704.704 -704.704 -704.704] [0.0000], Avg: [-870.772 -870.772 -870.772] (1.000)
Step: 65999, Reward: [-807.888 -807.888 -807.888] [0.0000], Avg: [-870.724 -870.724 -870.724] (1.000)
Step: 66049, Reward: [-564.555 -564.555 -564.555] [0.0000], Avg: [-870.493 -870.493 -870.493] (1.000)
Step: 66099, Reward: [-576.01 -576.01 -576.01] [0.0000], Avg: [-870.27 -870.27 -870.27] (1.000)
Step: 66149, Reward: [-644.528 -644.528 -644.528] [0.0000], Avg: [-870.099 -870.099 -870.099] (1.000)
Step: 66199, Reward: [-840.502 -840.502 -840.502] [0.0000], Avg: [-870.077 -870.077 -870.077] (1.000)
Step: 66249, Reward: [-831.006 -831.006 -831.006] [0.0000], Avg: [-870.048 -870.048 -870.048] (1.000)
Step: 66299, Reward: [-630.63 -630.63 -630.63] [0.0000], Avg: [-869.867 -869.867 -869.867] (1.000)
Step: 66349, Reward: [-1054.518 -1054.518 -1054.518] [0.0000], Avg: [-870.006 -870.006 -870.006] (1.000)
Step: 66399, Reward: [-746.343 -746.343 -746.343] [0.0000], Avg: [-869.913 -869.913 -869.913] (1.000)
Step: 66449, Reward: [-419.651 -419.651 -419.651] [0.0000], Avg: [-869.574 -869.574 -869.574] (1.000)
Step: 66499, Reward: [-870.327 -870.327 -870.327] [0.0000], Avg: [-869.575 -869.575 -869.575] (1.000)
Step: 66549, Reward: [-683.334 -683.334 -683.334] [0.0000], Avg: [-869.435 -869.435 -869.435] (1.000)
Step: 66599, Reward: [-596.005 -596.005 -596.005] [0.0000], Avg: [-869.23 -869.23 -869.23] (1.000)
Step: 66649, Reward: [-378.7 -378.7 -378.7] [0.0000], Avg: [-868.862 -868.862 -868.862] (1.000)
Step: 66699, Reward: [-610.203 -610.203 -610.203] [0.0000], Avg: [-868.668 -868.668 -868.668] (1.000)
Step: 66749, Reward: [-679.73 -679.73 -679.73] [0.0000], Avg: [-868.526 -868.526 -868.526] (1.000)
Step: 66799, Reward: [-667.746 -667.746 -667.746] [0.0000], Avg: [-868.376 -868.376 -868.376] (1.000)
Step: 66849, Reward: [-702.84 -702.84 -702.84] [0.0000], Avg: [-868.252 -868.252 -868.252] (1.000)
Step: 66899, Reward: [-536.814 -536.814 -536.814] [0.0000], Avg: [-868.004 -868.004 -868.004] (1.000)
Step: 66949, Reward: [-451.714 -451.714 -451.714] [0.0000], Avg: [-867.693 -867.693 -867.693] (1.000)
Step: 66999, Reward: [-903.453 -903.453 -903.453] [0.0000], Avg: [-867.72 -867.72 -867.72] (1.000)
Step: 67049, Reward: [-768.08 -768.08 -768.08] [0.0000], Avg: [-867.646 -867.646 -867.646] (1.000)
Step: 67099, Reward: [-858.014 -858.014 -858.014] [0.0000], Avg: [-867.639 -867.639 -867.639] (1.000)
Step: 67149, Reward: [-579.592 -579.592 -579.592] [0.0000], Avg: [-867.424 -867.424 -867.424] (1.000)
Step: 67199, Reward: [-584.019 -584.019 -584.019] [0.0000], Avg: [-867.213 -867.213 -867.213] (1.000)
Step: 67249, Reward: [-944.189 -944.189 -944.189] [0.0000], Avg: [-867.271 -867.271 -867.271] (1.000)
Step: 67299, Reward: [-743.657 -743.657 -743.657] [0.0000], Avg: [-867.179 -867.179 -867.179] (1.000)
Step: 67349, Reward: [-622.041 -622.041 -622.041] [0.0000], Avg: [-866.997 -866.997 -866.997] (1.000)
Step: 67399, Reward: [-548.23 -548.23 -548.23] [0.0000], Avg: [-866.76 -866.76 -866.76] (1.000)
Step: 67449, Reward: [-528.284 -528.284 -528.284] [0.0000], Avg: [-866.509 -866.509 -866.509] (1.000)
Step: 67499, Reward: [-477.767 -477.767 -477.767] [0.0000], Avg: [-866.221 -866.221 -866.221] (1.000)
Step: 67549, Reward: [-551.035 -551.035 -551.035] [0.0000], Avg: [-865.988 -865.988 -865.988] (1.000)
Step: 67599, Reward: [-430.096 -430.096 -430.096] [0.0000], Avg: [-865.666 -865.666 -865.666] (1.000)
Step: 67649, Reward: [-705.665 -705.665 -705.665] [0.0000], Avg: [-865.547 -865.547 -865.547] (1.000)
Step: 67699, Reward: [-521.01 -521.01 -521.01] [0.0000], Avg: [-865.293 -865.293 -865.293] (1.000)
Step: 67749, Reward: [-636.524 -636.524 -636.524] [0.0000], Avg: [-865.124 -865.124 -865.124] (1.000)
Step: 67799, Reward: [-908.918 -908.918 -908.918] [0.0000], Avg: [-865.156 -865.156 -865.156] (1.000)
Step: 67849, Reward: [-507.184 -507.184 -507.184] [0.0000], Avg: [-864.893 -864.893 -864.893] (1.000)
Step: 67899, Reward: [-864.846 -864.846 -864.846] [0.0000], Avg: [-864.893 -864.893 -864.893] (1.000)
Step: 67949, Reward: [-934.424 -934.424 -934.424] [0.0000], Avg: [-864.944 -864.944 -864.944] (1.000)
Step: 67999, Reward: [-511.543 -511.543 -511.543] [0.0000], Avg: [-864.684 -864.684 -864.684] (1.000)
Step: 68049, Reward: [-1158.985 -1158.985 -1158.985] [0.0000], Avg: [-864.9 -864.9 -864.9] (1.000)
Step: 68099, Reward: [-1142.768 -1142.768 -1142.768] [0.0000], Avg: [-865.104 -865.104 -865.104] (1.000)
Step: 68149, Reward: [-666.211 -666.211 -666.211] [0.0000], Avg: [-864.958 -864.958 -864.958] (1.000)
Step: 68199, Reward: [-833.432 -833.432 -833.432] [0.0000], Avg: [-864.935 -864.935 -864.935] (1.000)
Step: 68249, Reward: [-479.677 -479.677 -479.677] [0.0000], Avg: [-864.653 -864.653 -864.653] (1.000)
Step: 68299, Reward: [-686.851 -686.851 -686.851] [0.0000], Avg: [-864.523 -864.523 -864.523] (1.000)
Step: 68349, Reward: [-832.718 -832.718 -832.718] [0.0000], Avg: [-864.499 -864.499 -864.499] (1.000)
Step: 68399, Reward: [-816.31 -816.31 -816.31] [0.0000], Avg: [-864.464 -864.464 -864.464] (1.000)
Step: 68449, Reward: [-543.124 -543.124 -543.124] [0.0000], Avg: [-864.229 -864.229 -864.229] (1.000)
Step: 68499, Reward: [-1097.829 -1097.829 -1097.829] [0.0000], Avg: [-864.4 -864.4 -864.4] (1.000)
Step: 68549, Reward: [-821.702 -821.702 -821.702] [0.0000], Avg: [-864.369 -864.369 -864.369] (1.000)
Step: 68599, Reward: [-1120.156 -1120.156 -1120.156] [0.0000], Avg: [-864.555 -864.555 -864.555] (1.000)
Step: 68649, Reward: [-1290.107 -1290.107 -1290.107] [0.0000], Avg: [-864.865 -864.865 -864.865] (1.000)
Step: 68699, Reward: [-595.916 -595.916 -595.916] [0.0000], Avg: [-864.669 -864.669 -864.669] (1.000)
Step: 68749, Reward: [-449.943 -449.943 -449.943] [0.0000], Avg: [-864.368 -864.368 -864.368] (1.000)
Step: 68799, Reward: [-986.55 -986.55 -986.55] [0.0000], Avg: [-864.457 -864.457 -864.457] (1.000)
Step: 68849, Reward: [-416.509 -416.509 -416.509] [0.0000], Avg: [-864.131 -864.131 -864.131] (1.000)
Step: 68899, Reward: [-902.522 -902.522 -902.522] [0.0000], Avg: [-864.159 -864.159 -864.159] (1.000)
Step: 68949, Reward: [-946.763 -946.763 -946.763] [0.0000], Avg: [-864.219 -864.219 -864.219] (1.000)
Step: 68999, Reward: [-762.148 -762.148 -762.148] [0.0000], Avg: [-864.145 -864.145 -864.145] (1.000)
Step: 69049, Reward: [-731.469 -731.469 -731.469] [0.0000], Avg: [-864.049 -864.049 -864.049] (1.000)
Step: 69099, Reward: [-985.512 -985.512 -985.512] [0.0000], Avg: [-864.137 -864.137 -864.137] (1.000)
Step: 69149, Reward: [-822.974 -822.974 -822.974] [0.0000], Avg: [-864.107 -864.107 -864.107] (1.000)
Step: 69199, Reward: [-581.259 -581.259 -581.259] [0.0000], Avg: [-863.903 -863.903 -863.903] (1.000)
Step: 69249, Reward: [-540.398 -540.398 -540.398] [0.0000], Avg: [-863.669 -863.669 -863.669] (1.000)
Step: 69299, Reward: [-824.391 -824.391 -824.391] [0.0000], Avg: [-863.641 -863.641 -863.641] (1.000)
Step: 69349, Reward: [-745.972 -745.972 -745.972] [0.0000], Avg: [-863.556 -863.556 -863.556] (1.000)
Step: 69399, Reward: [-494.389 -494.389 -494.389] [0.0000], Avg: [-863.29 -863.29 -863.29] (1.000)
Step: 69449, Reward: [-617.846 -617.846 -617.846] [0.0000], Avg: [-863.113 -863.113 -863.113] (1.000)
Step: 69499, Reward: [-1245.395 -1245.395 -1245.395] [0.0000], Avg: [-863.388 -863.388 -863.388] (1.000)
Step: 69549, Reward: [-691.492 -691.492 -691.492] [0.0000], Avg: [-863.265 -863.265 -863.265] (1.000)
Step: 69599, Reward: [-777.513 -777.513 -777.513] [0.0000], Avg: [-863.203 -863.203 -863.203] (1.000)
Step: 69649, Reward: [-917.366 -917.366 -917.366] [0.0000], Avg: [-863.242 -863.242 -863.242] (1.000)
Step: 69699, Reward: [-477.309 -477.309 -477.309] [0.0000], Avg: [-862.965 -862.965 -862.965] (1.000)
Step: 69749, Reward: [-604.606 -604.606 -604.606] [0.0000], Avg: [-862.78 -862.78 -862.78] (1.000)
Step: 69799, Reward: [-475.763 -475.763 -475.763] [0.0000], Avg: [-862.503 -862.503 -862.503] (1.000)
Step: 69849, Reward: [-732.64 -732.64 -732.64] [0.0000], Avg: [-862.41 -862.41 -862.41] (1.000)
Step: 69899, Reward: [-560.873 -560.873 -560.873] [0.0000], Avg: [-862.194 -862.194 -862.194] (1.000)
Step: 69949, Reward: [-1198.797 -1198.797 -1198.797] [0.0000], Avg: [-862.435 -862.435 -862.435] (1.000)
Step: 69999, Reward: [-638.116 -638.116 -638.116] [0.0000], Avg: [-862.275 -862.275 -862.275] (1.000)
Step: 70049, Reward: [-742.417 -742.417 -742.417] [0.0000], Avg: [-862.189 -862.189 -862.189] (1.000)
Step: 70099, Reward: [-873.39 -873.39 -873.39] [0.0000], Avg: [-862.197 -862.197 -862.197] (1.000)
Step: 70149, Reward: [-715.155 -715.155 -715.155] [0.0000], Avg: [-862.092 -862.092 -862.092] (1.000)
Step: 70199, Reward: [-816.294 -816.294 -816.294] [0.0000], Avg: [-862.06 -862.06 -862.06] (1.000)
Step: 70249, Reward: [-419.28 -419.28 -419.28] [0.0000], Avg: [-861.744 -861.744 -861.744] (1.000)
Step: 70299, Reward: [-440.132 -440.132 -440.132] [0.0000], Avg: [-861.445 -861.445 -861.445] (1.000)
Step: 70349, Reward: [-659.279 -659.279 -659.279] [0.0000], Avg: [-861.301 -861.301 -861.301] (1.000)
Step: 70399, Reward: [-939.738 -939.738 -939.738] [0.0000], Avg: [-861.357 -861.357 -861.357] (1.000)
Step: 70449, Reward: [-1144.3 -1144.3 -1144.3] [0.0000], Avg: [-861.557 -861.557 -861.557] (1.000)
Step: 70499, Reward: [-807.693 -807.693 -807.693] [0.0000], Avg: [-861.519 -861.519 -861.519] (1.000)
Step: 70549, Reward: [-839.496 -839.496 -839.496] [0.0000], Avg: [-861.504 -861.504 -861.504] (1.000)
Step: 70599, Reward: [-1357.411 -1357.411 -1357.411] [0.0000], Avg: [-861.855 -861.855 -861.855] (1.000)
Step: 70649, Reward: [-831.36 -831.36 -831.36] [0.0000], Avg: [-861.833 -861.833 -861.833] (1.000)
Step: 70699, Reward: [-1283.914 -1283.914 -1283.914] [0.0000], Avg: [-862.132 -862.132 -862.132] (1.000)
Step: 70749, Reward: [-1172.303 -1172.303 -1172.303] [0.0000], Avg: [-862.351 -862.351 -862.351] (1.000)
Step: 70799, Reward: [-934.585 -934.585 -934.585] [0.0000], Avg: [-862.402 -862.402 -862.402] (1.000)
Step: 70849, Reward: [-941.833 -941.833 -941.833] [0.0000], Avg: [-862.458 -862.458 -862.458] (1.000)
Step: 70899, Reward: [-1373.566 -1373.566 -1373.566] [0.0000], Avg: [-862.818 -862.818 -862.818] (1.000)
Step: 70949, Reward: [-1151.294 -1151.294 -1151.294] [0.0000], Avg: [-863.022 -863.022 -863.022] (1.000)
Step: 70999, Reward: [-998.346 -998.346 -998.346] [0.0000], Avg: [-863.117 -863.117 -863.117] (1.000)
Step: 71049, Reward: [-578.48 -578.48 -578.48] [0.0000], Avg: [-862.917 -862.917 -862.917] (1.000)
Step: 71099, Reward: [-1112.554 -1112.554 -1112.554] [0.0000], Avg: [-863.092 -863.092 -863.092] (1.000)
Step: 71149, Reward: [-1178.589 -1178.589 -1178.589] [0.0000], Avg: [-863.314 -863.314 -863.314] (1.000)
Step: 71199, Reward: [-480.079 -480.079 -480.079] [0.0000], Avg: [-863.045 -863.045 -863.045] (1.000)
Step: 71249, Reward: [-717.85 -717.85 -717.85] [0.0000], Avg: [-862.943 -862.943 -862.943] (1.000)
Step: 71299, Reward: [-1085.928 -1085.928 -1085.928] [0.0000], Avg: [-863.099 -863.099 -863.099] (1.000)
Step: 71349, Reward: [-982.288 -982.288 -982.288] [0.0000], Avg: [-863.183 -863.183 -863.183] (1.000)
Step: 71399, Reward: [-576.311 -576.311 -576.311] [0.0000], Avg: [-862.982 -862.982 -862.982] (1.000)
Step: 71449, Reward: [-732.911 -732.911 -732.911] [0.0000], Avg: [-862.891 -862.891 -862.891] (1.000)
Step: 71499, Reward: [-777.281 -777.281 -777.281] [0.0000], Avg: [-862.831 -862.831 -862.831] (1.000)
Step: 71549, Reward: [-1447.621 -1447.621 -1447.621] [0.0000], Avg: [-863.24 -863.24 -863.24] (1.000)
Step: 71599, Reward: [-693.461 -693.461 -693.461] [0.0000], Avg: [-863.121 -863.121 -863.121] (1.000)
Step: 71649, Reward: [-790.497 -790.497 -790.497] [0.0000], Avg: [-863.07 -863.07 -863.07] (1.000)
Step: 71699, Reward: [-522.407 -522.407 -522.407] [0.0000], Avg: [-862.833 -862.833 -862.833] (1.000)
Step: 71749, Reward: [-728.639 -728.639 -728.639] [0.0000], Avg: [-862.739 -862.739 -862.739] (1.000)
Step: 71799, Reward: [-840.861 -840.861 -840.861] [0.0000], Avg: [-862.724 -862.724 -862.724] (1.000)
Step: 71849, Reward: [-773.567 -773.567 -773.567] [0.0000], Avg: [-862.662 -862.662 -862.662] (1.000)
Step: 71899, Reward: [-751.335 -751.335 -751.335] [0.0000], Avg: [-862.585 -862.585 -862.585] (1.000)
Step: 71949, Reward: [-506.144 -506.144 -506.144] [0.0000], Avg: [-862.337 -862.337 -862.337] (1.000)
Step: 71999, Reward: [-920.116 -920.116 -920.116] [0.0000], Avg: [-862.377 -862.377 -862.377] (1.000)
Step: 72049, Reward: [-401.224 -401.224 -401.224] [0.0000], Avg: [-862.057 -862.057 -862.057] (1.000)
Step: 72099, Reward: [-651.718 -651.718 -651.718] [0.0000], Avg: [-861.911 -861.911 -861.911] (1.000)
Step: 72149, Reward: [-693.969 -693.969 -693.969] [0.0000], Avg: [-861.795 -861.795 -861.795] (1.000)
Step: 72199, Reward: [-1087.513 -1087.513 -1087.513] [0.0000], Avg: [-861.951 -861.951 -861.951] (1.000)
Step: 72249, Reward: [-906.59 -906.59 -906.59] [0.0000], Avg: [-861.982 -861.982 -861.982] (1.000)
Step: 72299, Reward: [-717.782 -717.782 -717.782] [0.0000], Avg: [-861.882 -861.882 -861.882] (1.000)
Step: 72349, Reward: [-404.775 -404.775 -404.775] [0.0000], Avg: [-861.566 -861.566 -861.566] (1.000)
Step: 72399, Reward: [-810.825 -810.825 -810.825] [0.0000], Avg: [-861.531 -861.531 -861.531] (1.000)
Step: 72449, Reward: [-502.908 -502.908 -502.908] [0.0000], Avg: [-861.284 -861.284 -861.284] (1.000)
Step: 72499, Reward: [-345.198 -345.198 -345.198] [0.0000], Avg: [-860.928 -860.928 -860.928] (1.000)
Step: 72549, Reward: [-578.54 -578.54 -578.54] [0.0000], Avg: [-860.733 -860.733 -860.733] (1.000)
Step: 72599, Reward: [-640.166 -640.166 -640.166] [0.0000], Avg: [-860.581 -860.581 -860.581] (1.000)
Step: 72649, Reward: [-574.484 -574.484 -574.484] [0.0000], Avg: [-860.385 -860.385 -860.385] (1.000)
Step: 72699, Reward: [-951.477 -951.477 -951.477] [0.0000], Avg: [-860.447 -860.447 -860.447] (1.000)
Step: 72749, Reward: [-498.648 -498.648 -498.648] [0.0000], Avg: [-860.199 -860.199 -860.199] (1.000)
Step: 72799, Reward: [-731.833 -731.833 -731.833] [0.0000], Avg: [-860.11 -860.11 -860.11] (1.000)
Step: 72849, Reward: [-426.748 -426.748 -426.748] [0.0000], Avg: [-859.813 -859.813 -859.813] (1.000)
Step: 72899, Reward: [-785.441 -785.441 -785.441] [0.0000], Avg: [-859.762 -859.762 -859.762] (1.000)
Step: 72949, Reward: [-956.334 -956.334 -956.334] [0.0000], Avg: [-859.828 -859.828 -859.828] (1.000)
Step: 72999, Reward: [-447.952 -447.952 -447.952] [0.0000], Avg: [-859.546 -859.546 -859.546] (1.000)
Step: 73049, Reward: [-508.141 -508.141 -508.141] [0.0000], Avg: [-859.305 -859.305 -859.305] (1.000)
Step: 73099, Reward: [-549.103 -549.103 -549.103] [0.0000], Avg: [-859.093 -859.093 -859.093] (1.000)
Step: 73149, Reward: [-622.2 -622.2 -622.2] [0.0000], Avg: [-858.931 -858.931 -858.931] (1.000)
Step: 73199, Reward: [-733.894 -733.894 -733.894] [0.0000], Avg: [-858.846 -858.846 -858.846] (1.000)
Step: 73249, Reward: [-723.311 -723.311 -723.311] [0.0000], Avg: [-858.753 -858.753 -858.753] (1.000)
Step: 73299, Reward: [-531.622 -531.622 -531.622] [0.0000], Avg: [-858.53 -858.53 -858.53] (1.000)
Step: 73349, Reward: [-670.083 -670.083 -670.083] [0.0000], Avg: [-858.402 -858.402 -858.402] (1.000)
Step: 73399, Reward: [-660.377 -660.377 -660.377] [0.0000], Avg: [-858.267 -858.267 -858.267] (1.000)
Step: 73449, Reward: [-801.299 -801.299 -801.299] [0.0000], Avg: [-858.228 -858.228 -858.228] (1.000)
Step: 73499, Reward: [-610.771 -610.771 -610.771] [0.0000], Avg: [-858.06 -858.06 -858.06] (1.000)
Step: 73549, Reward: [-791.529 -791.529 -791.529] [0.0000], Avg: [-858.015 -858.015 -858.015] (1.000)
Step: 73599, Reward: [-529.781 -529.781 -529.781] [0.0000], Avg: [-857.792 -857.792 -857.792] (1.000)
Step: 73649, Reward: [-1034.775 -1034.775 -1034.775] [0.0000], Avg: [-857.912 -857.912 -857.912] (1.000)
Step: 73699, Reward: [-782.949 -782.949 -782.949] [0.0000], Avg: [-857.861 -857.861 -857.861] (1.000)
Step: 73749, Reward: [-774.902 -774.902 -774.902] [0.0000], Avg: [-857.805 -857.805 -857.805] (1.000)
Step: 73799, Reward: [-681.564 -681.564 -681.564] [0.0000], Avg: [-857.685 -857.685 -857.685] (1.000)
Step: 73849, Reward: [-940.644 -940.644 -940.644] [0.0000], Avg: [-857.741 -857.741 -857.741] (1.000)
Step: 73899, Reward: [-859.529 -859.529 -859.529] [0.0000], Avg: [-857.743 -857.743 -857.743] (1.000)
Step: 73949, Reward: [-611.961 -611.961 -611.961] [0.0000], Avg: [-857.576 -857.576 -857.576] (1.000)
Step: 73999, Reward: [-1250.425 -1250.425 -1250.425] [0.0000], Avg: [-857.842 -857.842 -857.842] (1.000)
Step: 74049, Reward: [-936.659 -936.659 -936.659] [0.0000], Avg: [-857.895 -857.895 -857.895] (1.000)
Step: 74099, Reward: [-864.187 -864.187 -864.187] [0.0000], Avg: [-857.899 -857.899 -857.899] (1.000)
Step: 74149, Reward: [-1411.387 -1411.387 -1411.387] [0.0000], Avg: [-858.273 -858.273 -858.273] (1.000)
Step: 74199, Reward: [-1045.244 -1045.244 -1045.244] [0.0000], Avg: [-858.399 -858.399 -858.399] (1.000)
Step: 74249, Reward: [-798.527 -798.527 -798.527] [0.0000], Avg: [-858.358 -858.358 -858.358] (1.000)
Step: 74299, Reward: [-623.058 -623.058 -623.058] [0.0000], Avg: [-858.2 -858.2 -858.2] (1.000)
Step: 74349, Reward: [-1185.226 -1185.226 -1185.226] [0.0000], Avg: [-858.42 -858.42 -858.42] (1.000)
Step: 74399, Reward: [-1391.813 -1391.813 -1391.813] [0.0000], Avg: [-858.778 -858.778 -858.778] (1.000)
Step: 74449, Reward: [-733.534 -733.534 -733.534] [0.0000], Avg: [-858.694 -858.694 -858.694] (1.000)
Step: 74499, Reward: [-895.173 -895.173 -895.173] [0.0000], Avg: [-858.719 -858.719 -858.719] (1.000)
Step: 74549, Reward: [-1116.936 -1116.936 -1116.936] [0.0000], Avg: [-858.892 -858.892 -858.892] (1.000)
Step: 74599, Reward: [-876.388 -876.388 -876.388] [0.0000], Avg: [-858.904 -858.904 -858.904] (1.000)
Step: 74649, Reward: [-658.121 -658.121 -658.121] [0.0000], Avg: [-858.769 -858.769 -858.769] (1.000)
Step: 74699, Reward: [-628.964 -628.964 -628.964] [0.0000], Avg: [-858.615 -858.615 -858.615] (1.000)
Step: 74749, Reward: [-1727.442 -1727.442 -1727.442] [0.0000], Avg: [-859.196 -859.196 -859.196] (1.000)
Step: 74799, Reward: [-1136.5 -1136.5 -1136.5] [0.0000], Avg: [-859.382 -859.382 -859.382] (1.000)
Step: 74849, Reward: [-1044.42 -1044.42 -1044.42] [0.0000], Avg: [-859.505 -859.505 -859.505] (1.000)
Step: 74899, Reward: [-1012.414 -1012.414 -1012.414] [0.0000], Avg: [-859.608 -859.608 -859.608] (1.000)
Step: 74949, Reward: [-761.646 -761.646 -761.646] [0.0000], Avg: [-859.542 -859.542 -859.542] (1.000)
Step: 74999, Reward: [-759.665 -759.665 -759.665] [0.0000], Avg: [-859.476 -859.476 -859.476] (1.000)
Step: 75049, Reward: [-796.594 -796.594 -796.594] [0.0000], Avg: [-859.434 -859.434 -859.434] (1.000)
Step: 75099, Reward: [-920.135 -920.135 -920.135] [0.0000], Avg: [-859.474 -859.474 -859.474] (1.000)
Step: 75149, Reward: [-538.986 -538.986 -538.986] [0.0000], Avg: [-859.261 -859.261 -859.261] (1.000)
Step: 75199, Reward: [-1010.219 -1010.219 -1010.219] [0.0000], Avg: [-859.361 -859.361 -859.361] (1.000)
Step: 75249, Reward: [-617.994 -617.994 -617.994] [0.0000], Avg: [-859.201 -859.201 -859.201] (1.000)
Step: 75299, Reward: [-568.59 -568.59 -568.59] [0.0000], Avg: [-859.008 -859.008 -859.008] (1.000)
Step: 75349, Reward: [-731.834 -731.834 -731.834] [0.0000], Avg: [-858.923 -858.923 -858.923] (1.000)
Step: 75399, Reward: [-516.817 -516.817 -516.817] [0.0000], Avg: [-858.697 -858.697 -858.697] (1.000)
Step: 75449, Reward: [-1339.796 -1339.796 -1339.796] [0.0000], Avg: [-859.015 -859.015 -859.015] (1.000)
Step: 75499, Reward: [-451.516 -451.516 -451.516] [0.0000], Avg: [-858.746 -858.746 -858.746] (1.000)
Step: 75549, Reward: [-627.549 -627.549 -627.549] [0.0000], Avg: [-858.593 -858.593 -858.593] (1.000)
Step: 75599, Reward: [-636.662 -636.662 -636.662] [0.0000], Avg: [-858.446 -858.446 -858.446] (1.000)
Step: 75649, Reward: [-889.18 -889.18 -889.18] [0.0000], Avg: [-858.466 -858.466 -858.466] (1.000)
Step: 75699, Reward: [-761.701 -761.701 -761.701] [0.0000], Avg: [-858.402 -858.402 -858.402] (1.000)
Step: 75749, Reward: [-962.393 -962.393 -962.393] [0.0000], Avg: [-858.471 -858.471 -858.471] (1.000)
Step: 75799, Reward: [-908.646 -908.646 -908.646] [0.0000], Avg: [-858.504 -858.504 -858.504] (1.000)
Step: 75849, Reward: [-735.763 -735.763 -735.763] [0.0000], Avg: [-858.423 -858.423 -858.423] (1.000)
Step: 75899, Reward: [-1288.75 -1288.75 -1288.75] [0.0000], Avg: [-858.707 -858.707 -858.707] (1.000)
Step: 75949, Reward: [-893.573 -893.573 -893.573] [0.0000], Avg: [-858.729 -858.729 -858.729] (1.000)
Step: 75999, Reward: [-1078.706 -1078.706 -1078.706] [0.0000], Avg: [-858.874 -858.874 -858.874] (1.000)
Step: 76049, Reward: [-865.702 -865.702 -865.702] [0.0000], Avg: [-858.879 -858.879 -858.879] (1.000)
Step: 76099, Reward: [-1019.052 -1019.052 -1019.052] [0.0000], Avg: [-858.984 -858.984 -858.984] (1.000)
Step: 76149, Reward: [-502.83 -502.83 -502.83] [0.0000], Avg: [-858.75 -858.75 -858.75] (1.000)
