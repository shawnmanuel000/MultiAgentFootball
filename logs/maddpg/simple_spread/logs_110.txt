Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024		# How many experience tuples to sample from the buffer for each train step
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
#LEARN_RATE = 0.001
#TARGET_UPDATE_RATE = 0.001

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			q_target = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (q_value - q_target.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			actor_action = agent.get_action(states[i], grad=True, numpy=False)
			action = [gsoftmax(actor_action, hard=True) if j==i else one_hot(model.get_action(ob, numpy=False)) for (j,model), ob in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -(agent.critic_local(states_joint, action_joint)-q_target).mean() + e_weight*actor_action.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.network.optimize(states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-626.072 -626.072 -626.072] [0.0000], Avg: [-626.072 -626.072 -626.072] (1.000)
Step: 99, Reward: [-626.16 -626.16 -626.16] [0.0000], Avg: [-626.116 -626.116 -626.116] (1.000)
Step: 149, Reward: [-474.45 -474.45 -474.45] [0.0000], Avg: [-575.561 -575.561 -575.561] (1.000)
Step: 199, Reward: [-576.631 -576.631 -576.631] [0.0000], Avg: [-575.828 -575.828 -575.828] (1.000)
Step: 249, Reward: [-434.574 -434.574 -434.574] [0.0000], Avg: [-547.577 -547.577 -547.577] (1.000)
Step: 299, Reward: [-388.458 -388.458 -388.458] [0.0000], Avg: [-521.058 -521.058 -521.058] (1.000)
Step: 349, Reward: [-452.726 -452.726 -452.726] [0.0000], Avg: [-511.296 -511.296 -511.296] (1.000)
Step: 399, Reward: [-375.187 -375.187 -375.187] [0.0000], Avg: [-494.282 -494.282 -494.282] (1.000)
Step: 449, Reward: [-432.409 -432.409 -432.409] [0.0000], Avg: [-487.408 -487.408 -487.408] (1.000)
Step: 499, Reward: [-437.614 -437.614 -437.614] [0.0000], Avg: [-482.428 -482.428 -482.428] (1.000)
Step: 549, Reward: [-480.682 -480.682 -480.682] [0.0000], Avg: [-482.269 -482.269 -482.269] (1.000)
Step: 599, Reward: [-467.585 -467.585 -467.585] [0.0000], Avg: [-481.046 -481.046 -481.046] (1.000)
Step: 649, Reward: [-617.347 -617.347 -617.347] [0.0000], Avg: [-491.53 -491.53 -491.53] (1.000)
Step: 699, Reward: [-404.134 -404.134 -404.134] [0.0000], Avg: [-485.288 -485.288 -485.288] (1.000)
Step: 749, Reward: [-575.254 -575.254 -575.254] [0.0000], Avg: [-491.286 -491.286 -491.286] (1.000)
Step: 799, Reward: [-429.792 -429.792 -429.792] [0.0000], Avg: [-487.442 -487.442 -487.442] (1.000)
Step: 849, Reward: [-637.188 -637.188 -637.188] [0.0000], Avg: [-496.251 -496.251 -496.251] (1.000)
Step: 899, Reward: [-523.788 -523.788 -523.788] [0.0000], Avg: [-497.781 -497.781 -497.781] (1.000)
Step: 949, Reward: [-640.289 -640.289 -640.289] [0.0000], Avg: [-505.281 -505.281 -505.281] (1.000)
Step: 999, Reward: [-529.509 -529.509 -529.509] [0.0000], Avg: [-506.493 -506.493 -506.493] (1.000)
Step: 1049, Reward: [-546.702 -546.702 -546.702] [0.0000], Avg: [-508.407 -508.407 -508.407] (1.000)
Step: 1099, Reward: [-401.471 -401.471 -401.471] [0.0000], Avg: [-503.547 -503.547 -503.547] (1.000)
Step: 1149, Reward: [-747.579 -747.579 -747.579] [0.0000], Avg: [-514.157 -514.157 -514.157] (1.000)
Step: 1199, Reward: [-516.722 -516.722 -516.722] [0.0000], Avg: [-514.264 -514.264 -514.264] (1.000)
Step: 1249, Reward: [-349.776 -349.776 -349.776] [0.0000], Avg: [-507.684 -507.684 -507.684] (1.000)
Step: 1299, Reward: [-620.235 -620.235 -620.235] [0.0000], Avg: [-512.013 -512.013 -512.013] (1.000)
Step: 1349, Reward: [-505.7 -505.7 -505.7] [0.0000], Avg: [-511.779 -511.779 -511.779] (1.000)
Step: 1399, Reward: [-459.355 -459.355 -459.355] [0.0000], Avg: [-509.907 -509.907 -509.907] (1.000)
Step: 1449, Reward: [-481.061 -481.061 -481.061] [0.0000], Avg: [-508.912 -508.912 -508.912] (1.000)
Step: 1499, Reward: [-492.115 -492.115 -492.115] [0.0000], Avg: [-508.352 -508.352 -508.352] (1.000)
Step: 1549, Reward: [-592.214 -592.214 -592.214] [0.0000], Avg: [-511.057 -511.057 -511.057] (1.000)
Step: 1599, Reward: [-448.807 -448.807 -448.807] [0.0000], Avg: [-509.112 -509.112 -509.112] (1.000)
Step: 1649, Reward: [-560.29 -560.29 -560.29] [0.0000], Avg: [-510.663 -510.663 -510.663] (1.000)
Step: 1699, Reward: [-538.249 -538.249 -538.249] [0.0000], Avg: [-511.474 -511.474 -511.474] (1.000)
Step: 1749, Reward: [-501.574 -501.574 -501.574] [0.0000], Avg: [-511.191 -511.191 -511.191] (1.000)
Step: 1799, Reward: [-524.604 -524.604 -524.604] [0.0000], Avg: [-511.564 -511.564 -511.564] (1.000)
Step: 1849, Reward: [-555.641 -555.641 -555.641] [0.0000], Avg: [-512.755 -512.755 -512.755] (1.000)
Step: 1899, Reward: [-528.125 -528.125 -528.125] [0.0000], Avg: [-513.16 -513.16 -513.16] (1.000)
Step: 1949, Reward: [-650.939 -650.939 -650.939] [0.0000], Avg: [-516.693 -516.693 -516.693] (1.000)
Step: 1999, Reward: [-493.939 -493.939 -493.939] [0.0000], Avg: [-516.124 -516.124 -516.124] (1.000)
Step: 2049, Reward: [-580.498 -580.498 -580.498] [0.0000], Avg: [-517.694 -517.694 -517.694] (1.000)
Step: 2099, Reward: [-437.617 -437.617 -437.617] [0.0000], Avg: [-515.787 -515.787 -515.787] (1.000)
Step: 2149, Reward: [-484.227 -484.227 -484.227] [0.0000], Avg: [-515.053 -515.053 -515.053] (1.000)
Step: 2199, Reward: [-601.398 -601.398 -601.398] [0.0000], Avg: [-517.016 -517.016 -517.016] (1.000)
Step: 2249, Reward: [-577.295 -577.295 -577.295] [0.0000], Avg: [-518.355 -518.355 -518.355] (1.000)
Step: 2299, Reward: [-660.059 -660.059 -660.059] [0.0000], Avg: [-521.436 -521.436 -521.436] (1.000)
Step: 2349, Reward: [-562.323 -562.323 -562.323] [0.0000], Avg: [-522.306 -522.306 -522.306] (1.000)
Step: 2399, Reward: [-641.469 -641.469 -641.469] [0.0000], Avg: [-524.788 -524.788 -524.788] (1.000)
Step: 2449, Reward: [-393.151 -393.151 -393.151] [0.0000], Avg: [-522.102 -522.102 -522.102] (1.000)
Step: 2499, Reward: [-569.387 -569.387 -569.387] [0.0000], Avg: [-523.047 -523.047 -523.047] (1.000)
Step: 2549, Reward: [-654.518 -654.518 -654.518] [0.0000], Avg: [-525.625 -525.625 -525.625] (1.000)
Step: 2599, Reward: [-514.843 -514.843 -514.843] [0.0000], Avg: [-525.418 -525.418 -525.418] (1.000)
Step: 2649, Reward: [-615.199 -615.199 -615.199] [0.0000], Avg: [-527.112 -527.112 -527.112] (1.000)
Step: 2699, Reward: [-525.134 -525.134 -525.134] [0.0000], Avg: [-527.075 -527.075 -527.075] (1.000)
Step: 2749, Reward: [-487.14 -487.14 -487.14] [0.0000], Avg: [-526.349 -526.349 -526.349] (1.000)
Step: 2799, Reward: [-277.417 -277.417 -277.417] [0.0000], Avg: [-521.904 -521.904 -521.904] (1.000)
Step: 2849, Reward: [-597.97 -597.97 -597.97] [0.0000], Avg: [-523.238 -523.238 -523.238] (1.000)
Step: 2899, Reward: [-641.937 -641.937 -641.937] [0.0000], Avg: [-525.285 -525.285 -525.285] (1.000)
Step: 2949, Reward: [-598.076 -598.076 -598.076] [0.0000], Avg: [-526.519 -526.519 -526.519] (1.000)
Step: 2999, Reward: [-451.627 -451.627 -451.627] [0.0000], Avg: [-525.271 -525.271 -525.271] (1.000)
Step: 3049, Reward: [-722.392 -722.392 -722.392] [0.0000], Avg: [-528.502 -528.502 -528.502] (1.000)
Step: 3099, Reward: [-753.055 -753.055 -753.055] [0.0000], Avg: [-532.124 -532.124 -532.124] (1.000)
Step: 3149, Reward: [-754.925 -754.925 -754.925] [0.0000], Avg: [-535.66 -535.66 -535.66] (1.000)
Step: 3199, Reward: [-499.532 -499.532 -499.532] [0.0000], Avg: [-535.096 -535.096 -535.096] (1.000)
Step: 3249, Reward: [-900.063 -900.063 -900.063] [0.0000], Avg: [-540.711 -540.711 -540.711] (1.000)
Step: 3299, Reward: [-486.488 -486.488 -486.488] [0.0000], Avg: [-539.889 -539.889 -539.889] (1.000)
Step: 3349, Reward: [-581.095 -581.095 -581.095] [0.0000], Avg: [-540.504 -540.504 -540.504] (1.000)
Step: 3399, Reward: [-531.638 -531.638 -531.638] [0.0000], Avg: [-540.374 -540.374 -540.374] (1.000)
Step: 3449, Reward: [-393.749 -393.749 -393.749] [0.0000], Avg: [-538.249 -538.249 -538.249] (1.000)
Step: 3499, Reward: [-531.464 -531.464 -531.464] [0.0000], Avg: [-538.152 -538.152 -538.152] (1.000)
Step: 3549, Reward: [-619.536 -619.536 -619.536] [0.0000], Avg: [-539.298 -539.298 -539.298] (1.000)
Step: 3599, Reward: [-411.417 -411.417 -411.417] [0.0000], Avg: [-537.522 -537.522 -537.522] (1.000)
Step: 3649, Reward: [-641.731 -641.731 -641.731] [0.0000], Avg: [-538.95 -538.95 -538.95] (1.000)
Step: 3699, Reward: [-692.603 -692.603 -692.603] [0.0000], Avg: [-541.026 -541.026 -541.026] (1.000)
Step: 3749, Reward: [-632.768 -632.768 -632.768] [0.0000], Avg: [-542.249 -542.249 -542.249] (1.000)
Step: 3799, Reward: [-533.169 -533.169 -533.169] [0.0000], Avg: [-542.13 -542.13 -542.13] (1.000)
Step: 3849, Reward: [-614.711 -614.711 -614.711] [0.0000], Avg: [-543.072 -543.072 -543.072] (1.000)
Step: 3899, Reward: [-401.639 -401.639 -401.639] [0.0000], Avg: [-541.259 -541.259 -541.259] (1.000)
Step: 3949, Reward: [-580.188 -580.188 -580.188] [0.0000], Avg: [-541.752 -541.752 -541.752] (1.000)
Step: 3999, Reward: [-489.694 -489.694 -489.694] [0.0000], Avg: [-541.101 -541.101 -541.101] (1.000)
Step: 4049, Reward: [-632.297 -632.297 -632.297] [0.0000], Avg: [-542.227 -542.227 -542.227] (1.000)
Step: 4099, Reward: [-411.437 -411.437 -411.437] [0.0000], Avg: [-540.632 -540.632 -540.632] (1.000)
Step: 4149, Reward: [-628.71 -628.71 -628.71] [0.0000], Avg: [-541.693 -541.693 -541.693] (1.000)
Step: 4199, Reward: [-627.068 -627.068 -627.068] [0.0000], Avg: [-542.71 -542.71 -542.71] (1.000)
Step: 4249, Reward: [-483.531 -483.531 -483.531] [0.0000], Avg: [-542.013 -542.013 -542.013] (1.000)
Step: 4299, Reward: [-622.716 -622.716 -622.716] [0.0000], Avg: [-542.952 -542.952 -542.952] (1.000)
Step: 4349, Reward: [-506.045 -506.045 -506.045] [0.0000], Avg: [-542.527 -542.527 -542.527] (1.000)
Step: 4399, Reward: [-609.747 -609.747 -609.747] [0.0000], Avg: [-543.291 -543.291 -543.291] (1.000)
Step: 4449, Reward: [-680.687 -680.687 -680.687] [0.0000], Avg: [-544.835 -544.835 -544.835] (1.000)
Step: 4499, Reward: [-1013.511 -1013.511 -1013.511] [0.0000], Avg: [-550.043 -550.043 -550.043] (1.000)
Step: 4549, Reward: [-494.103 -494.103 -494.103] [0.0000], Avg: [-549.428 -549.428 -549.428] (1.000)
Step: 4599, Reward: [-735.403 -735.403 -735.403] [0.0000], Avg: [-551.449 -551.449 -551.449] (1.000)
Step: 4649, Reward: [-766.807 -766.807 -766.807] [0.0000], Avg: [-553.765 -553.765 -553.765] (1.000)
Step: 4699, Reward: [-930.866 -930.866 -930.866] [0.0000], Avg: [-557.777 -557.777 -557.777] (1.000)
Step: 4749, Reward: [-762.829 -762.829 -762.829] [0.0000], Avg: [-559.935 -559.935 -559.935] (1.000)
Step: 4799, Reward: [-563.696 -563.696 -563.696] [0.0000], Avg: [-559.974 -559.974 -559.974] (1.000)
Step: 4849, Reward: [-784.769 -784.769 -784.769] [0.0000], Avg: [-562.292 -562.292 -562.292] (1.000)
Step: 4899, Reward: [-903.354 -903.354 -903.354] [0.0000], Avg: [-565.772 -565.772 -565.772] (1.000)
Step: 4949, Reward: [-702.852 -702.852 -702.852] [0.0000], Avg: [-567.157 -567.157 -567.157] (1.000)
Step: 4999, Reward: [-428.512 -428.512 -428.512] [0.0000], Avg: [-565.77 -565.77 -565.77] (1.000)
Step: 5049, Reward: [-474.41 -474.41 -474.41] [0.0000], Avg: [-564.866 -564.866 -564.866] (1.000)
Step: 5099, Reward: [-870.85 -870.85 -870.85] [0.0000], Avg: [-567.866 -567.866 -567.866] (1.000)
Step: 5149, Reward: [-487.354 -487.354 -487.354] [0.0000], Avg: [-567.084 -567.084 -567.084] (1.000)
Step: 5199, Reward: [-554.527 -554.527 -554.527] [0.0000], Avg: [-566.963 -566.963 -566.963] (1.000)
Step: 5249, Reward: [-816.992 -816.992 -816.992] [0.0000], Avg: [-569.344 -569.344 -569.344] (1.000)
Step: 5299, Reward: [-720.223 -720.223 -720.223] [0.0000], Avg: [-570.768 -570.768 -570.768] (1.000)
Step: 5349, Reward: [-959.708 -959.708 -959.708] [0.0000], Avg: [-574.403 -574.403 -574.403] (1.000)
Step: 5399, Reward: [-970.667 -970.667 -970.667] [0.0000], Avg: [-578.072 -578.072 -578.072] (1.000)
Step: 5449, Reward: [-695.463 -695.463 -695.463] [0.0000], Avg: [-579.149 -579.149 -579.149] (1.000)
Step: 5499, Reward: [-462.954 -462.954 -462.954] [0.0000], Avg: [-578.093 -578.093 -578.093] (1.000)
Step: 5549, Reward: [-797.802 -797.802 -797.802] [0.0000], Avg: [-580.072 -580.072 -580.072] (1.000)
Step: 5599, Reward: [-460.875 -460.875 -460.875] [0.0000], Avg: [-579.008 -579.008 -579.008] (1.000)
Step: 5649, Reward: [-570.77 -570.77 -570.77] [0.0000], Avg: [-578.935 -578.935 -578.935] (1.000)
Step: 5699, Reward: [-864.029 -864.029 -864.029] [0.0000], Avg: [-581.436 -581.436 -581.436] (1.000)
Step: 5749, Reward: [-590.258 -590.258 -590.258] [0.0000], Avg: [-581.512 -581.512 -581.512] (1.000)
Step: 5799, Reward: [-1033.627 -1033.627 -1033.627] [0.0000], Avg: [-585.41 -585.41 -585.41] (1.000)
Step: 5849, Reward: [-905.945 -905.945 -905.945] [0.0000], Avg: [-588.149 -588.149 -588.149] (1.000)
Step: 5899, Reward: [-514.834 -514.834 -514.834] [0.0000], Avg: [-587.528 -587.528 -587.528] (1.000)
Step: 5949, Reward: [-687.183 -687.183 -687.183] [0.0000], Avg: [-588.366 -588.366 -588.366] (1.000)
Step: 5999, Reward: [-565.785 -565.785 -565.785] [0.0000], Avg: [-588.177 -588.177 -588.177] (1.000)
Step: 6049, Reward: [-653.017 -653.017 -653.017] [0.0000], Avg: [-588.713 -588.713 -588.713] (1.000)
Step: 6099, Reward: [-1216.383 -1216.383 -1216.383] [0.0000], Avg: [-593.858 -593.858 -593.858] (1.000)
Step: 6149, Reward: [-1249.728 -1249.728 -1249.728] [0.0000], Avg: [-599.19 -599.19 -599.19] (1.000)
Step: 6199, Reward: [-828.977 -828.977 -828.977] [0.0000], Avg: [-601.043 -601.043 -601.043] (1.000)
Step: 6249, Reward: [-454.763 -454.763 -454.763] [0.0000], Avg: [-599.873 -599.873 -599.873] (1.000)
Step: 6299, Reward: [-1442.674 -1442.674 -1442.674] [0.0000], Avg: [-606.562 -606.562 -606.562] (1.000)
Step: 6349, Reward: [-691.468 -691.468 -691.468] [0.0000], Avg: [-607.231 -607.231 -607.231] (1.000)
Step: 6399, Reward: [-1511.299 -1511.299 -1511.299] [0.0000], Avg: [-614.294 -614.294 -614.294] (1.000)
Step: 6449, Reward: [-1893.869 -1893.869 -1893.869] [0.0000], Avg: [-624.213 -624.213 -624.213] (1.000)
Step: 6499, Reward: [-675.921 -675.921 -675.921] [0.0000], Avg: [-624.611 -624.611 -624.611] (1.000)
Step: 6549, Reward: [-1451.794 -1451.794 -1451.794] [0.0000], Avg: [-630.925 -630.925 -630.925] (1.000)
Step: 6599, Reward: [-1496.997 -1496.997 -1496.997] [0.0000], Avg: [-637.486 -637.486 -637.486] (1.000)
Step: 6649, Reward: [-1195.999 -1195.999 -1195.999] [0.0000], Avg: [-641.686 -641.686 -641.686] (1.000)
Step: 6699, Reward: [-1521.243 -1521.243 -1521.243] [0.0000], Avg: [-648.249 -648.249 -648.249] (1.000)
Step: 6749, Reward: [-955.968 -955.968 -955.968] [0.0000], Avg: [-650.529 -650.529 -650.529] (1.000)
Step: 6799, Reward: [-988.017 -988.017 -988.017] [0.0000], Avg: [-653.01 -653.01 -653.01] (1.000)
Step: 6849, Reward: [-1368.5 -1368.5 -1368.5] [0.0000], Avg: [-658.233 -658.233 -658.233] (1.000)
Step: 6899, Reward: [-1818.803 -1818.803 -1818.803] [0.0000], Avg: [-666.643 -666.643 -666.643] (1.000)
Step: 6949, Reward: [-793.692 -793.692 -793.692] [0.0000], Avg: [-667.557 -667.557 -667.557] (1.000)
Step: 6999, Reward: [-1512.315 -1512.315 -1512.315] [0.0000], Avg: [-673.591 -673.591 -673.591] (1.000)
Step: 7049, Reward: [-837.842 -837.842 -837.842] [0.0000], Avg: [-674.756 -674.756 -674.756] (1.000)
Step: 7099, Reward: [-980.486 -980.486 -980.486] [0.0000], Avg: [-676.909 -676.909 -676.909] (1.000)
Step: 7149, Reward: [-1739.67 -1739.67 -1739.67] [0.0000], Avg: [-684.341 -684.341 -684.341] (1.000)
Step: 7199, Reward: [-1702.515 -1702.515 -1702.515] [0.0000], Avg: [-691.411 -691.411 -691.411] (1.000)
Step: 7249, Reward: [-1434.299 -1434.299 -1434.299] [0.0000], Avg: [-696.535 -696.535 -696.535] (1.000)
Step: 7299, Reward: [-1438.089 -1438.089 -1438.089] [0.0000], Avg: [-701.614 -701.614 -701.614] (1.000)
Step: 7349, Reward: [-1790.807 -1790.807 -1790.807] [0.0000], Avg: [-709.023 -709.023 -709.023] (1.000)
Step: 7399, Reward: [-1534.702 -1534.702 -1534.702] [0.0000], Avg: [-714.602 -714.602 -714.602] (1.000)
Step: 7449, Reward: [-1561.448 -1561.448 -1561.448] [0.0000], Avg: [-720.286 -720.286 -720.286] (1.000)
Step: 7499, Reward: [-1263.33 -1263.33 -1263.33] [0.0000], Avg: [-723.906 -723.906 -723.906] (1.000)
Step: 7549, Reward: [-1060.122 -1060.122 -1060.122] [0.0000], Avg: [-726.133 -726.133 -726.133] (1.000)
Step: 7599, Reward: [-1717.52 -1717.52 -1717.52] [0.0000], Avg: [-732.655 -732.655 -732.655] (1.000)
Step: 7649, Reward: [-1960.339 -1960.339 -1960.339] [0.0000], Avg: [-740.679 -740.679 -740.679] (1.000)
Step: 7699, Reward: [-1966.628 -1966.628 -1966.628] [0.0000], Avg: [-748.64 -748.64 -748.64] (1.000)
Step: 7749, Reward: [-1609.603 -1609.603 -1609.603] [0.0000], Avg: [-754.194 -754.194 -754.194] (1.000)
Step: 7799, Reward: [-2027.898 -2027.898 -2027.898] [0.0000], Avg: [-762.359 -762.359 -762.359] (1.000)
Step: 7849, Reward: [-2208.292 -2208.292 -2208.292] [0.0000], Avg: [-771.569 -771.569 -771.569] (1.000)
Step: 7899, Reward: [-1453.654 -1453.654 -1453.654] [0.0000], Avg: [-775.886 -775.886 -775.886] (1.000)
Step: 7949, Reward: [-2393.767 -2393.767 -2393.767] [0.0000], Avg: [-786.061 -786.061 -786.061] (1.000)
Step: 7999, Reward: [-1766.065 -1766.065 -1766.065] [0.0000], Avg: [-792.186 -792.186 -792.186] (1.000)
Step: 8049, Reward: [-1395.789 -1395.789 -1395.789] [0.0000], Avg: [-795.935 -795.935 -795.935] (1.000)
Step: 8099, Reward: [-1635.108 -1635.108 -1635.108] [0.0000], Avg: [-801.115 -801.115 -801.115] (1.000)
Step: 8149, Reward: [-1449.208 -1449.208 -1449.208] [0.0000], Avg: [-805.091 -805.091 -805.091] (1.000)
Step: 8199, Reward: [-1843.324 -1843.324 -1843.324] [0.0000], Avg: [-811.422 -811.422 -811.422] (1.000)
Step: 8249, Reward: [-1647.374 -1647.374 -1647.374] [0.0000], Avg: [-816.488 -816.488 -816.488] (1.000)
Step: 8299, Reward: [-1952.614 -1952.614 -1952.614] [0.0000], Avg: [-823.333 -823.333 -823.333] (1.000)
Step: 8349, Reward: [-1859.276 -1859.276 -1859.276] [0.0000], Avg: [-829.536 -829.536 -829.536] (1.000)
Step: 8399, Reward: [-1106.66 -1106.66 -1106.66] [0.0000], Avg: [-831.185 -831.185 -831.185] (1.000)
Step: 8449, Reward: [-1592.687 -1592.687 -1592.687] [0.0000], Avg: [-835.691 -835.691 -835.691] (1.000)
Step: 8499, Reward: [-1934.407 -1934.407 -1934.407] [0.0000], Avg: [-842.154 -842.154 -842.154] (1.000)
Step: 8549, Reward: [-2215.379 -2215.379 -2215.379] [0.0000], Avg: [-850.185 -850.185 -850.185] (1.000)
Step: 8599, Reward: [-1731.465 -1731.465 -1731.465] [0.0000], Avg: [-855.309 -855.309 -855.309] (1.000)
Step: 8649, Reward: [-2118.774 -2118.774 -2118.774] [0.0000], Avg: [-862.612 -862.612 -862.612] (1.000)
Step: 8699, Reward: [-1806.828 -1806.828 -1806.828] [0.0000], Avg: [-868.038 -868.038 -868.038] (1.000)
Step: 8749, Reward: [-1466.241 -1466.241 -1466.241] [0.0000], Avg: [-871.457 -871.457 -871.457] (1.000)
Step: 8799, Reward: [-1432.212 -1432.212 -1432.212] [0.0000], Avg: [-874.643 -874.643 -874.643] (1.000)
Step: 8849, Reward: [-1810.872 -1810.872 -1810.872] [0.0000], Avg: [-879.932 -879.932 -879.932] (1.000)
Step: 8899, Reward: [-2072.209 -2072.209 -2072.209] [0.0000], Avg: [-886.63 -886.63 -886.63] (1.000)
Step: 8949, Reward: [-1831.703 -1831.703 -1831.703] [0.0000], Avg: [-891.91 -891.91 -891.91] (1.000)
Step: 8999, Reward: [-1904.58 -1904.58 -1904.58] [0.0000], Avg: [-897.536 -897.536 -897.536] (1.000)
Step: 9049, Reward: [-1363.301 -1363.301 -1363.301] [0.0000], Avg: [-900.109 -900.109 -900.109] (1.000)
Step: 9099, Reward: [-1195.881 -1195.881 -1195.881] [0.0000], Avg: [-901.735 -901.735 -901.735] (1.000)
Step: 9149, Reward: [-1655.38 -1655.38 -1655.38] [0.0000], Avg: [-905.853 -905.853 -905.853] (1.000)
Step: 9199, Reward: [-1482.698 -1482.698 -1482.698] [0.0000], Avg: [-908.988 -908.988 -908.988] (1.000)
Step: 9249, Reward: [-1635.422 -1635.422 -1635.422] [0.0000], Avg: [-912.914 -912.914 -912.914] (1.000)
Step: 9299, Reward: [-1748.538 -1748.538 -1748.538] [0.0000], Avg: [-917.407 -917.407 -917.407] (1.000)
Step: 9349, Reward: [-1753.597 -1753.597 -1753.597] [0.0000], Avg: [-921.879 -921.879 -921.879] (1.000)
Step: 9399, Reward: [-2187.001 -2187.001 -2187.001] [0.0000], Avg: [-928.608 -928.608 -928.608] (1.000)
Step: 9449, Reward: [-2062.443 -2062.443 -2062.443] [0.0000], Avg: [-934.607 -934.607 -934.607] (1.000)
Step: 9499, Reward: [-1669.111 -1669.111 -1669.111] [0.0000], Avg: [-938.473 -938.473 -938.473] (1.000)
Step: 9549, Reward: [-1475.436 -1475.436 -1475.436] [0.0000], Avg: [-941.284 -941.284 -941.284] (1.000)
Step: 9599, Reward: [-1462.997 -1462.997 -1462.997] [0.0000], Avg: [-944.002 -944.002 -944.002] (1.000)
Step: 9649, Reward: [-1363.26 -1363.26 -1363.26] [0.0000], Avg: [-946.174 -946.174 -946.174] (1.000)
Step: 9699, Reward: [-1679.239 -1679.239 -1679.239] [0.0000], Avg: [-949.953 -949.953 -949.953] (1.000)
Step: 9749, Reward: [-1616.156 -1616.156 -1616.156] [0.0000], Avg: [-953.369 -953.369 -953.369] (1.000)
Step: 9799, Reward: [-1671.48 -1671.48 -1671.48] [0.0000], Avg: [-957.033 -957.033 -957.033] (1.000)
Step: 9849, Reward: [-1847.324 -1847.324 -1847.324] [0.0000], Avg: [-961.552 -961.552 -961.552] (1.000)
Step: 9899, Reward: [-1792.881 -1792.881 -1792.881] [0.0000], Avg: [-965.751 -965.751 -965.751] (1.000)
Step: 9949, Reward: [-1385.857 -1385.857 -1385.857] [0.0000], Avg: [-967.862 -967.862 -967.862] (1.000)
Step: 9999, Reward: [-1739.864 -1739.864 -1739.864] [0.0000], Avg: [-971.722 -971.722 -971.722] (1.000)
Step: 10049, Reward: [-1888.808 -1888.808 -1888.808] [0.0000], Avg: [-976.284 -976.284 -976.284] (1.000)
Step: 10099, Reward: [-1888.815 -1888.815 -1888.815] [0.0000], Avg: [-980.802 -980.802 -980.802] (1.000)
Step: 10149, Reward: [-1299.522 -1299.522 -1299.522] [0.0000], Avg: [-982.372 -982.372 -982.372] (1.000)
Step: 10199, Reward: [-1386.681 -1386.681 -1386.681] [0.0000], Avg: [-984.354 -984.354 -984.354] (1.000)
Step: 10249, Reward: [-1895.651 -1895.651 -1895.651] [0.0000], Avg: [-988.799 -988.799 -988.799] (1.000)
Step: 10299, Reward: [-1555.081 -1555.081 -1555.081] [0.0000], Avg: [-991.548 -991.548 -991.548] (1.000)
Step: 10349, Reward: [-1597.956 -1597.956 -1597.956] [0.0000], Avg: [-994.478 -994.478 -994.478] (1.000)
Step: 10399, Reward: [-1876.077 -1876.077 -1876.077] [0.0000], Avg: [-998.716 -998.716 -998.716] (1.000)
Step: 10449, Reward: [-2009.055 -2009.055 -2009.055] [0.0000], Avg: [-1003.55 -1003.55 -1003.55] (1.000)
Step: 10499, Reward: [-1480.83 -1480.83 -1480.83] [0.0000], Avg: [-1005.823 -1005.823 -1005.823] (1.000)
Step: 10549, Reward: [-1436.857 -1436.857 -1436.857] [0.0000], Avg: [-1007.866 -1007.866 -1007.866] (1.000)
Step: 10599, Reward: [-2059.476 -2059.476 -2059.476] [0.0000], Avg: [-1012.826 -1012.826 -1012.826] (1.000)
Step: 10649, Reward: [-1063.437 -1063.437 -1063.437] [0.0000], Avg: [-1013.064 -1013.064 -1013.064] (1.000)
Step: 10699, Reward: [-1347.148 -1347.148 -1347.148] [0.0000], Avg: [-1014.625 -1014.625 -1014.625] (1.000)
Step: 10749, Reward: [-1282.074 -1282.074 -1282.074] [0.0000], Avg: [-1015.869 -1015.869 -1015.869] (1.000)
Step: 10799, Reward: [-1257.856 -1257.856 -1257.856] [0.0000], Avg: [-1016.989 -1016.989 -1016.989] (1.000)
Step: 10849, Reward: [-1533.21 -1533.21 -1533.21] [0.0000], Avg: [-1019.368 -1019.368 -1019.368] (1.000)
Step: 10899, Reward: [-1244.607 -1244.607 -1244.607] [0.0000], Avg: [-1020.401 -1020.401 -1020.401] (1.000)
Step: 10949, Reward: [-1759.606 -1759.606 -1759.606] [0.0000], Avg: [-1023.777 -1023.777 -1023.777] (1.000)
Step: 10999, Reward: [-1568.681 -1568.681 -1568.681] [0.0000], Avg: [-1026.254 -1026.254 -1026.254] (1.000)
Step: 11049, Reward: [-1433.79 -1433.79 -1433.79] [0.0000], Avg: [-1028.098 -1028.098 -1028.098] (1.000)
Step: 11099, Reward: [-1850.765 -1850.765 -1850.765] [0.0000], Avg: [-1031.803 -1031.803 -1031.803] (1.000)
Step: 11149, Reward: [-1490.628 -1490.628 -1490.628] [0.0000], Avg: [-1033.861 -1033.861 -1033.861] (1.000)
Step: 11199, Reward: [-1787.716 -1787.716 -1787.716] [0.0000], Avg: [-1037.226 -1037.226 -1037.226] (1.000)
Step: 11249, Reward: [-1265.129 -1265.129 -1265.129] [0.0000], Avg: [-1038.239 -1038.239 -1038.239] (1.000)
Step: 11299, Reward: [-1004.306 -1004.306 -1004.306] [0.0000], Avg: [-1038.089 -1038.089 -1038.089] (1.000)
Step: 11349, Reward: [-1402.185 -1402.185 -1402.185] [0.0000], Avg: [-1039.693 -1039.693 -1039.693] (1.000)
Step: 11399, Reward: [-1633.409 -1633.409 -1633.409] [0.0000], Avg: [-1042.297 -1042.297 -1042.297] (1.000)
Step: 11449, Reward: [-1238.808 -1238.808 -1238.808] [0.0000], Avg: [-1043.155 -1043.155 -1043.155] (1.000)
Step: 11499, Reward: [-1376.016 -1376.016 -1376.016] [0.0000], Avg: [-1044.602 -1044.602 -1044.602] (1.000)
Step: 11549, Reward: [-1594.467 -1594.467 -1594.467] [0.0000], Avg: [-1046.983 -1046.983 -1046.983] (1.000)
Step: 11599, Reward: [-1394.29 -1394.29 -1394.29] [0.0000], Avg: [-1048.48 -1048.48 -1048.48] (1.000)
Step: 11649, Reward: [-1619.257 -1619.257 -1619.257] [0.0000], Avg: [-1050.929 -1050.929 -1050.929] (1.000)
Step: 11699, Reward: [-1136.487 -1136.487 -1136.487] [0.0000], Avg: [-1051.295 -1051.295 -1051.295] (1.000)
Step: 11749, Reward: [-1233.966 -1233.966 -1233.966] [0.0000], Avg: [-1052.072 -1052.072 -1052.072] (1.000)
Step: 11799, Reward: [-2183.105 -2183.105 -2183.105] [0.0000], Avg: [-1056.865 -1056.865 -1056.865] (1.000)
Step: 11849, Reward: [-1440.357 -1440.357 -1440.357] [0.0000], Avg: [-1058.483 -1058.483 -1058.483] (1.000)
Step: 11899, Reward: [-1122.908 -1122.908 -1122.908] [0.0000], Avg: [-1058.754 -1058.754 -1058.754] (1.000)
Step: 11949, Reward: [-1432.995 -1432.995 -1432.995] [0.0000], Avg: [-1060.32 -1060.32 -1060.32] (1.000)
Step: 11999, Reward: [-1342.357 -1342.357 -1342.357] [0.0000], Avg: [-1061.495 -1061.495 -1061.495] (1.000)
Step: 12049, Reward: [-1359.196 -1359.196 -1359.196] [0.0000], Avg: [-1062.73 -1062.73 -1062.73] (1.000)
Step: 12099, Reward: [-1732.206 -1732.206 -1732.206] [0.0000], Avg: [-1065.496 -1065.496 -1065.496] (1.000)
Step: 12149, Reward: [-1615.908 -1615.908 -1615.908] [0.0000], Avg: [-1067.762 -1067.762 -1067.762] (1.000)
Step: 12199, Reward: [-1518.905 -1518.905 -1518.905] [0.0000], Avg: [-1069.61 -1069.61 -1069.61] (1.000)
Step: 12249, Reward: [-1665.038 -1665.038 -1665.038] [0.0000], Avg: [-1072.041 -1072.041 -1072.041] (1.000)
Step: 12299, Reward: [-1378.127 -1378.127 -1378.127] [0.0000], Avg: [-1073.285 -1073.285 -1073.285] (1.000)
Step: 12349, Reward: [-1046.529 -1046.529 -1046.529] [0.0000], Avg: [-1073.177 -1073.177 -1073.177] (1.000)
Step: 12399, Reward: [-987.133 -987.133 -987.133] [0.0000], Avg: [-1072.83 -1072.83 -1072.83] (1.000)
Step: 12449, Reward: [-1324.315 -1324.315 -1324.315] [0.0000], Avg: [-1073.84 -1073.84 -1073.84] (1.000)
Step: 12499, Reward: [-1503.295 -1503.295 -1503.295] [0.0000], Avg: [-1075.558 -1075.558 -1075.558] (1.000)
Step: 12549, Reward: [-1303.299 -1303.299 -1303.299] [0.0000], Avg: [-1076.465 -1076.465 -1076.465] (1.000)
Step: 12599, Reward: [-1590.734 -1590.734 -1590.734] [0.0000], Avg: [-1078.506 -1078.506 -1078.506] (1.000)
Step: 12649, Reward: [-1778.919 -1778.919 -1778.919] [0.0000], Avg: [-1081.274 -1081.274 -1081.274] (1.000)
Step: 12699, Reward: [-1293.669 -1293.669 -1293.669] [0.0000], Avg: [-1082.11 -1082.11 -1082.11] (1.000)
Step: 12749, Reward: [-1690.305 -1690.305 -1690.305] [0.0000], Avg: [-1084.495 -1084.495 -1084.495] (1.000)
Step: 12799, Reward: [-1242.452 -1242.452 -1242.452] [0.0000], Avg: [-1085.112 -1085.112 -1085.112] (1.000)
Step: 12849, Reward: [-1673.058 -1673.058 -1673.058] [0.0000], Avg: [-1087.4 -1087.4 -1087.4] (1.000)
Step: 12899, Reward: [-1476.875 -1476.875 -1476.875] [0.0000], Avg: [-1088.91 -1088.91 -1088.91] (1.000)
Step: 12949, Reward: [-1286.816 -1286.816 -1286.816] [0.0000], Avg: [-1089.674 -1089.674 -1089.674] (1.000)
Step: 12999, Reward: [-1820.304 -1820.304 -1820.304] [0.0000], Avg: [-1092.484 -1092.484 -1092.484] (1.000)
Step: 13049, Reward: [-1758.918 -1758.918 -1758.918] [0.0000], Avg: [-1095.037 -1095.037 -1095.037] (1.000)
Step: 13099, Reward: [-1247.468 -1247.468 -1247.468] [0.0000], Avg: [-1095.619 -1095.619 -1095.619] (1.000)
Step: 13149, Reward: [-1409.767 -1409.767 -1409.767] [0.0000], Avg: [-1096.814 -1096.814 -1096.814] (1.000)
Step: 13199, Reward: [-1782.915 -1782.915 -1782.915] [0.0000], Avg: [-1099.412 -1099.412 -1099.412] (1.000)
Step: 13249, Reward: [-1481.553 -1481.553 -1481.553] [0.0000], Avg: [-1100.854 -1100.854 -1100.854] (1.000)
Step: 13299, Reward: [-1368.626 -1368.626 -1368.626] [0.0000], Avg: [-1101.861 -1101.861 -1101.861] (1.000)
Step: 13349, Reward: [-1366.257 -1366.257 -1366.257] [0.0000], Avg: [-1102.851 -1102.851 -1102.851] (1.000)
Step: 13399, Reward: [-1152.468 -1152.468 -1152.468] [0.0000], Avg: [-1103.037 -1103.037 -1103.037] (1.000)
Step: 13449, Reward: [-1395.496 -1395.496 -1395.496] [0.0000], Avg: [-1104.124 -1104.124 -1104.124] (1.000)
Step: 13499, Reward: [-1748.581 -1748.581 -1748.581] [0.0000], Avg: [-1106.511 -1106.511 -1106.511] (1.000)
Step: 13549, Reward: [-1750.715 -1750.715 -1750.715] [0.0000], Avg: [-1108.888 -1108.888 -1108.888] (1.000)
Step: 13599, Reward: [-1910.26 -1910.26 -1910.26] [0.0000], Avg: [-1111.834 -1111.834 -1111.834] (1.000)
Step: 13649, Reward: [-1586.584 -1586.584 -1586.584] [0.0000], Avg: [-1113.573 -1113.573 -1113.573] (1.000)
Step: 13699, Reward: [-1701.175 -1701.175 -1701.175] [0.0000], Avg: [-1115.718 -1115.718 -1115.718] (1.000)
Step: 13749, Reward: [-2081.022 -2081.022 -2081.022] [0.0000], Avg: [-1119.228 -1119.228 -1119.228] (1.000)
Step: 13799, Reward: [-1235.111 -1235.111 -1235.111] [0.0000], Avg: [-1119.648 -1119.648 -1119.648] (1.000)
Step: 13849, Reward: [-1617.345 -1617.345 -1617.345] [0.0000], Avg: [-1121.444 -1121.444 -1121.444] (1.000)
Step: 13899, Reward: [-1354.145 -1354.145 -1354.145] [0.0000], Avg: [-1122.281 -1122.281 -1122.281] (1.000)
Step: 13949, Reward: [-1164.169 -1164.169 -1164.169] [0.0000], Avg: [-1122.432 -1122.432 -1122.432] (1.000)
Step: 13999, Reward: [-1590.273 -1590.273 -1590.273] [0.0000], Avg: [-1124.102 -1124.102 -1124.102] (1.000)
Step: 14049, Reward: [-1398.24 -1398.24 -1398.24] [0.0000], Avg: [-1125.078 -1125.078 -1125.078] (1.000)
Step: 14099, Reward: [-1760.411 -1760.411 -1760.411] [0.0000], Avg: [-1127.331 -1127.331 -1127.331] (1.000)
Step: 14149, Reward: [-1554.484 -1554.484 -1554.484] [0.0000], Avg: [-1128.84 -1128.84 -1128.84] (1.000)
Step: 14199, Reward: [-1239.017 -1239.017 -1239.017] [0.0000], Avg: [-1129.228 -1129.228 -1129.228] (1.000)
Step: 14249, Reward: [-1482.108 -1482.108 -1482.108] [0.0000], Avg: [-1130.466 -1130.466 -1130.466] (1.000)
Step: 14299, Reward: [-1849.255 -1849.255 -1849.255] [0.0000], Avg: [-1132.98 -1132.98 -1132.98] (1.000)
Step: 14349, Reward: [-1781.879 -1781.879 -1781.879] [0.0000], Avg: [-1135.241 -1135.241 -1135.241] (1.000)
Step: 14399, Reward: [-1139.71 -1139.71 -1139.71] [0.0000], Avg: [-1135.256 -1135.256 -1135.256] (1.000)
Step: 14449, Reward: [-2147.213 -2147.213 -2147.213] [0.0000], Avg: [-1138.758 -1138.758 -1138.758] (1.000)
Step: 14499, Reward: [-1221.852 -1221.852 -1221.852] [0.0000], Avg: [-1139.044 -1139.044 -1139.044] (1.000)
Step: 14549, Reward: [-1499.459 -1499.459 -1499.459] [0.0000], Avg: [-1140.283 -1140.283 -1140.283] (1.000)
Step: 14599, Reward: [-2013.431 -2013.431 -2013.431] [0.0000], Avg: [-1143.273 -1143.273 -1143.273] (1.000)
Step: 14649, Reward: [-1648.516 -1648.516 -1648.516] [0.0000], Avg: [-1144.997 -1144.997 -1144.997] (1.000)
Step: 14699, Reward: [-1480.365 -1480.365 -1480.365] [0.0000], Avg: [-1146.138 -1146.138 -1146.138] (1.000)
Step: 14749, Reward: [-1543.116 -1543.116 -1543.116] [0.0000], Avg: [-1147.484 -1147.484 -1147.484] (1.000)
Step: 14799, Reward: [-1327.097 -1327.097 -1327.097] [0.0000], Avg: [-1148.091 -1148.091 -1148.091] (1.000)
Step: 14849, Reward: [-1745.381 -1745.381 -1745.381] [0.0000], Avg: [-1150.102 -1150.102 -1150.102] (1.000)
Step: 14899, Reward: [-1709.57 -1709.57 -1709.57] [0.0000], Avg: [-1151.979 -1151.979 -1151.979] (1.000)
Step: 14949, Reward: [-1773.079 -1773.079 -1773.079] [0.0000], Avg: [-1154.056 -1154.056 -1154.056] (1.000)
Step: 14999, Reward: [-1343.541 -1343.541 -1343.541] [0.0000], Avg: [-1154.688 -1154.688 -1154.688] (1.000)
Step: 15049, Reward: [-1474.616 -1474.616 -1474.616] [0.0000], Avg: [-1155.751 -1155.751 -1155.751] (1.000)
Step: 15099, Reward: [-1823.427 -1823.427 -1823.427] [0.0000], Avg: [-1157.962 -1157.962 -1157.962] (1.000)
Step: 15149, Reward: [-1265.092 -1265.092 -1265.092] [0.0000], Avg: [-1158.315 -1158.315 -1158.315] (1.000)
Step: 15199, Reward: [-1877.395 -1877.395 -1877.395] [0.0000], Avg: [-1160.681 -1160.681 -1160.681] (1.000)
Step: 15249, Reward: [-1370.245 -1370.245 -1370.245] [0.0000], Avg: [-1161.368 -1161.368 -1161.368] (1.000)
Step: 15299, Reward: [-1659.408 -1659.408 -1659.408] [0.0000], Avg: [-1162.995 -1162.995 -1162.995] (1.000)
Step: 15349, Reward: [-1741.906 -1741.906 -1741.906] [0.0000], Avg: [-1164.881 -1164.881 -1164.881] (1.000)
Step: 15399, Reward: [-1401.705 -1401.705 -1401.705] [0.0000], Avg: [-1165.65 -1165.65 -1165.65] (1.000)
Step: 15449, Reward: [-1501.027 -1501.027 -1501.027] [0.0000], Avg: [-1166.735 -1166.735 -1166.735] (1.000)
Step: 15499, Reward: [-1219.446 -1219.446 -1219.446] [0.0000], Avg: [-1166.905 -1166.905 -1166.905] (1.000)
Step: 15549, Reward: [-1470.226 -1470.226 -1470.226] [0.0000], Avg: [-1167.881 -1167.881 -1167.881] (1.000)
Step: 15599, Reward: [-962.108 -962.108 -962.108] [0.0000], Avg: [-1167.221 -1167.221 -1167.221] (1.000)
Step: 15649, Reward: [-1321.785 -1321.785 -1321.785] [0.0000], Avg: [-1167.715 -1167.715 -1167.715] (1.000)
Step: 15699, Reward: [-1596.454 -1596.454 -1596.454] [0.0000], Avg: [-1169.08 -1169.08 -1169.08] (1.000)
Step: 15749, Reward: [-1433.267 -1433.267 -1433.267] [0.0000], Avg: [-1169.919 -1169.919 -1169.919] (1.000)
Step: 15799, Reward: [-1518.294 -1518.294 -1518.294] [0.0000], Avg: [-1171.021 -1171.021 -1171.021] (1.000)
Step: 15849, Reward: [-1260.384 -1260.384 -1260.384] [0.0000], Avg: [-1171.303 -1171.303 -1171.303] (1.000)
Step: 15899, Reward: [-2030.324 -2030.324 -2030.324] [0.0000], Avg: [-1174.005 -1174.005 -1174.005] (1.000)
Step: 15949, Reward: [-1935.988 -1935.988 -1935.988] [0.0000], Avg: [-1176.393 -1176.393 -1176.393] (1.000)
Step: 15999, Reward: [-1588.759 -1588.759 -1588.759] [0.0000], Avg: [-1177.682 -1177.682 -1177.682] (1.000)
Step: 16049, Reward: [-1473.451 -1473.451 -1473.451] [0.0000], Avg: [-1178.603 -1178.603 -1178.603] (1.000)
Step: 16099, Reward: [-1630.976 -1630.976 -1630.976] [0.0000], Avg: [-1180.008 -1180.008 -1180.008] (1.000)
Step: 16149, Reward: [-1493.938 -1493.938 -1493.938] [0.0000], Avg: [-1180.98 -1180.98 -1180.98] (1.000)
Step: 16199, Reward: [-2062.064 -2062.064 -2062.064] [0.0000], Avg: [-1183.7 -1183.7 -1183.7] (1.000)
Step: 16249, Reward: [-1896.47 -1896.47 -1896.47] [0.0000], Avg: [-1185.893 -1185.893 -1185.893] (1.000)
Step: 16299, Reward: [-1093.494 -1093.494 -1093.494] [0.0000], Avg: [-1185.609 -1185.609 -1185.609] (1.000)
Step: 16349, Reward: [-1394. -1394. -1394.] [0.0000], Avg: [-1186.247 -1186.247 -1186.247] (1.000)
Step: 16399, Reward: [-1830.397 -1830.397 -1830.397] [0.0000], Avg: [-1188.21 -1188.21 -1188.21] (1.000)
Step: 16449, Reward: [-1723.638 -1723.638 -1723.638] [0.0000], Avg: [-1189.838 -1189.838 -1189.838] (1.000)
Step: 16499, Reward: [-1722.626 -1722.626 -1722.626] [0.0000], Avg: [-1191.452 -1191.452 -1191.452] (1.000)
Step: 16549, Reward: [-1755.396 -1755.396 -1755.396] [0.0000], Avg: [-1193.156 -1193.156 -1193.156] (1.000)
Step: 16599, Reward: [-1357.005 -1357.005 -1357.005] [0.0000], Avg: [-1193.65 -1193.65 -1193.65] (1.000)
Step: 16649, Reward: [-1419.886 -1419.886 -1419.886] [0.0000], Avg: [-1194.329 -1194.329 -1194.329] (1.000)
Step: 16699, Reward: [-1717.935 -1717.935 -1717.935] [0.0000], Avg: [-1195.897 -1195.897 -1195.897] (1.000)
Step: 16749, Reward: [-1482.427 -1482.427 -1482.427] [0.0000], Avg: [-1196.752 -1196.752 -1196.752] (1.000)
Step: 16799, Reward: [-1089.365 -1089.365 -1089.365] [0.0000], Avg: [-1196.432 -1196.432 -1196.432] (1.000)
Step: 16849, Reward: [-1567.114 -1567.114 -1567.114] [0.0000], Avg: [-1197.532 -1197.532 -1197.532] (1.000)
Step: 16899, Reward: [-1837.54 -1837.54 -1837.54] [0.0000], Avg: [-1199.426 -1199.426 -1199.426] (1.000)
Step: 16949, Reward: [-1453.805 -1453.805 -1453.805] [0.0000], Avg: [-1200.176 -1200.176 -1200.176] (1.000)
Step: 16999, Reward: [-1706.219 -1706.219 -1706.219] [0.0000], Avg: [-1201.665 -1201.665 -1201.665] (1.000)
Step: 17049, Reward: [-1207.865 -1207.865 -1207.865] [0.0000], Avg: [-1201.683 -1201.683 -1201.683] (1.000)
Step: 17099, Reward: [-1633.188 -1633.188 -1633.188] [0.0000], Avg: [-1202.945 -1202.945 -1202.945] (1.000)
Step: 17149, Reward: [-1616.943 -1616.943 -1616.943] [0.0000], Avg: [-1204.152 -1204.152 -1204.152] (1.000)
Step: 17199, Reward: [-1601.22 -1601.22 -1601.22] [0.0000], Avg: [-1205.306 -1205.306 -1205.306] (1.000)
Step: 17249, Reward: [-1466.296 -1466.296 -1466.296] [0.0000], Avg: [-1206.062 -1206.062 -1206.062] (1.000)
Step: 17299, Reward: [-1878.261 -1878.261 -1878.261] [0.0000], Avg: [-1208.005 -1208.005 -1208.005] (1.000)
Step: 17349, Reward: [-1696.875 -1696.875 -1696.875] [0.0000], Avg: [-1209.414 -1209.414 -1209.414] (1.000)
Step: 17399, Reward: [-1803.738 -1803.738 -1803.738] [0.0000], Avg: [-1211.122 -1211.122 -1211.122] (1.000)
Step: 17449, Reward: [-1585.485 -1585.485 -1585.485] [0.0000], Avg: [-1212.194 -1212.194 -1212.194] (1.000)
Step: 17499, Reward: [-1412.608 -1412.608 -1412.608] [0.0000], Avg: [-1212.767 -1212.767 -1212.767] (1.000)
Step: 17549, Reward: [-1404.657 -1404.657 -1404.657] [0.0000], Avg: [-1213.314 -1213.314 -1213.314] (1.000)
Step: 17599, Reward: [-1795.049 -1795.049 -1795.049] [0.0000], Avg: [-1214.966 -1214.966 -1214.966] (1.000)
Step: 17649, Reward: [-1385.281 -1385.281 -1385.281] [0.0000], Avg: [-1215.449 -1215.449 -1215.449] (1.000)
Step: 17699, Reward: [-1387.835 -1387.835 -1387.835] [0.0000], Avg: [-1215.936 -1215.936 -1215.936] (1.000)
Step: 17749, Reward: [-1292.423 -1292.423 -1292.423] [0.0000], Avg: [-1216.151 -1216.151 -1216.151] (1.000)
Step: 17799, Reward: [-1707.339 -1707.339 -1707.339] [0.0000], Avg: [-1217.531 -1217.531 -1217.531] (1.000)
Step: 17849, Reward: [-1553.511 -1553.511 -1553.511] [0.0000], Avg: [-1218.472 -1218.472 -1218.472] (1.000)
Step: 17899, Reward: [-1624.433 -1624.433 -1624.433] [0.0000], Avg: [-1219.606 -1219.606 -1219.606] (1.000)
Step: 17949, Reward: [-2112.108 -2112.108 -2112.108] [0.0000], Avg: [-1222.092 -1222.092 -1222.092] (1.000)
Step: 17999, Reward: [-1591.136 -1591.136 -1591.136] [0.0000], Avg: [-1223.117 -1223.117 -1223.117] (1.000)
Step: 18049, Reward: [-1338.582 -1338.582 -1338.582] [0.0000], Avg: [-1223.437 -1223.437 -1223.437] (1.000)
Step: 18099, Reward: [-1539.853 -1539.853 -1539.853] [0.0000], Avg: [-1224.311 -1224.311 -1224.311] (1.000)
Step: 18149, Reward: [-1516.574 -1516.574 -1516.574] [0.0000], Avg: [-1225.116 -1225.116 -1225.116] (1.000)
Step: 18199, Reward: [-1944.943 -1944.943 -1944.943] [0.0000], Avg: [-1227.094 -1227.094 -1227.094] (1.000)
Step: 18249, Reward: [-1535.985 -1535.985 -1535.985] [0.0000], Avg: [-1227.94 -1227.94 -1227.94] (1.000)
Step: 18299, Reward: [-1839.131 -1839.131 -1839.131] [0.0000], Avg: [-1229.61 -1229.61 -1229.61] (1.000)
Step: 18349, Reward: [-1411.08 -1411.08 -1411.08] [0.0000], Avg: [-1230.105 -1230.105 -1230.105] (1.000)
Step: 18399, Reward: [-1572.444 -1572.444 -1572.444] [0.0000], Avg: [-1231.035 -1231.035 -1231.035] (1.000)
Step: 18449, Reward: [-1181.286 -1181.286 -1181.286] [0.0000], Avg: [-1230.9 -1230.9 -1230.9] (1.000)
Step: 18499, Reward: [-1644.995 -1644.995 -1644.995] [0.0000], Avg: [-1232.019 -1232.019 -1232.019] (1.000)
Step: 18549, Reward: [-1721.146 -1721.146 -1721.146] [0.0000], Avg: [-1233.338 -1233.338 -1233.338] (1.000)
Step: 18599, Reward: [-1635.914 -1635.914 -1635.914] [0.0000], Avg: [-1234.42 -1234.42 -1234.42] (1.000)
Step: 18649, Reward: [-2058.906 -2058.906 -2058.906] [0.0000], Avg: [-1236.63 -1236.63 -1236.63] (1.000)
Step: 18699, Reward: [-1626.389 -1626.389 -1626.389] [0.0000], Avg: [-1237.672 -1237.672 -1237.672] (1.000)
Step: 18749, Reward: [-1624.886 -1624.886 -1624.886] [0.0000], Avg: [-1238.705 -1238.705 -1238.705] (1.000)
Step: 18799, Reward: [-1492.815 -1492.815 -1492.815] [0.0000], Avg: [-1239.381 -1239.381 -1239.381] (1.000)
Step: 18849, Reward: [-1600.005 -1600.005 -1600.005] [0.0000], Avg: [-1240.337 -1240.337 -1240.337] (1.000)
Step: 18899, Reward: [-1954.759 -1954.759 -1954.759] [0.0000], Avg: [-1242.227 -1242.227 -1242.227] (1.000)
Step: 18949, Reward: [-1172.253 -1172.253 -1172.253] [0.0000], Avg: [-1242.043 -1242.043 -1242.043] (1.000)
Step: 18999, Reward: [-1231.491 -1231.491 -1231.491] [0.0000], Avg: [-1242.015 -1242.015 -1242.015] (1.000)
Step: 19049, Reward: [-1279.33 -1279.33 -1279.33] [0.0000], Avg: [-1242.113 -1242.113 -1242.113] (1.000)
Step: 19099, Reward: [-1621.653 -1621.653 -1621.653] [0.0000], Avg: [-1243.106 -1243.106 -1243.106] (1.000)
Step: 19149, Reward: [-1475.693 -1475.693 -1475.693] [0.0000], Avg: [-1243.714 -1243.714 -1243.714] (1.000)
Step: 19199, Reward: [-1722.698 -1722.698 -1722.698] [0.0000], Avg: [-1244.961 -1244.961 -1244.961] (1.000)
Step: 19249, Reward: [-1439.97 -1439.97 -1439.97] [0.0000], Avg: [-1245.468 -1245.468 -1245.468] (1.000)
Step: 19299, Reward: [-1593.876 -1593.876 -1593.876] [0.0000], Avg: [-1246.37 -1246.37 -1246.37] (1.000)
Step: 19349, Reward: [-1388.875 -1388.875 -1388.875] [0.0000], Avg: [-1246.738 -1246.738 -1246.738] (1.000)
Step: 19399, Reward: [-1901.942 -1901.942 -1901.942] [0.0000], Avg: [-1248.427 -1248.427 -1248.427] (1.000)
Step: 19449, Reward: [-1528.085 -1528.085 -1528.085] [0.0000], Avg: [-1249.146 -1249.146 -1249.146] (1.000)
Step: 19499, Reward: [-1768.98 -1768.98 -1768.98] [0.0000], Avg: [-1250.479 -1250.479 -1250.479] (1.000)
Step: 19549, Reward: [-1468.01 -1468.01 -1468.01] [0.0000], Avg: [-1251.035 -1251.035 -1251.035] (1.000)
Step: 19599, Reward: [-2116.335 -2116.335 -2116.335] [0.0000], Avg: [-1253.243 -1253.243 -1253.243] (1.000)
Step: 19649, Reward: [-1608.806 -1608.806 -1608.806] [0.0000], Avg: [-1254.147 -1254.147 -1254.147] (1.000)
Step: 19699, Reward: [-1821.268 -1821.268 -1821.268] [0.0000], Avg: [-1255.587 -1255.587 -1255.587] (1.000)
Step: 19749, Reward: [-1546.93 -1546.93 -1546.93] [0.0000], Avg: [-1256.324 -1256.324 -1256.324] (1.000)
Step: 19799, Reward: [-1519.43 -1519.43 -1519.43] [0.0000], Avg: [-1256.989 -1256.989 -1256.989] (1.000)
Step: 19849, Reward: [-1424.995 -1424.995 -1424.995] [0.0000], Avg: [-1257.412 -1257.412 -1257.412] (1.000)
Step: 19899, Reward: [-1745.966 -1745.966 -1745.966] [0.0000], Avg: [-1258.639 -1258.639 -1258.639] (1.000)
Step: 19949, Reward: [-1304.313 -1304.313 -1304.313] [0.0000], Avg: [-1258.754 -1258.754 -1258.754] (1.000)
Step: 19999, Reward: [-1334.829 -1334.829 -1334.829] [0.0000], Avg: [-1258.944 -1258.944 -1258.944] (1.000)
Step: 20049, Reward: [-1323.65 -1323.65 -1323.65] [0.0000], Avg: [-1259.106 -1259.106 -1259.106] (1.000)
Step: 20099, Reward: [-1402.922 -1402.922 -1402.922] [0.0000], Avg: [-1259.463 -1259.463 -1259.463] (1.000)
Step: 20149, Reward: [-1917.272 -1917.272 -1917.272] [0.0000], Avg: [-1261.096 -1261.096 -1261.096] (1.000)
Step: 20199, Reward: [-1679.015 -1679.015 -1679.015] [0.0000], Avg: [-1262.13 -1262.13 -1262.13] (1.000)
Step: 20249, Reward: [-1047.388 -1047.388 -1047.388] [0.0000], Avg: [-1261.6 -1261.6 -1261.6] (1.000)
Step: 20299, Reward: [-1810.833 -1810.833 -1810.833] [0.0000], Avg: [-1262.953 -1262.953 -1262.953] (1.000)
Step: 20349, Reward: [-1528.299 -1528.299 -1528.299] [0.0000], Avg: [-1263.605 -1263.605 -1263.605] (1.000)
Step: 20399, Reward: [-1620.789 -1620.789 -1620.789] [0.0000], Avg: [-1264.48 -1264.48 -1264.48] (1.000)
Step: 20449, Reward: [-1365.07 -1365.07 -1365.07] [0.0000], Avg: [-1264.726 -1264.726 -1264.726] (1.000)
Step: 20499, Reward: [-1497.356 -1497.356 -1497.356] [0.0000], Avg: [-1265.293 -1265.293 -1265.293] (1.000)
Step: 20549, Reward: [-1227.731 -1227.731 -1227.731] [0.0000], Avg: [-1265.202 -1265.202 -1265.202] (1.000)
Step: 20599, Reward: [-1526.572 -1526.572 -1526.572] [0.0000], Avg: [-1265.836 -1265.836 -1265.836] (1.000)
Step: 20649, Reward: [-1657.855 -1657.855 -1657.855] [0.0000], Avg: [-1266.786 -1266.786 -1266.786] (1.000)
Step: 20699, Reward: [-1460.207 -1460.207 -1460.207] [0.0000], Avg: [-1267.253 -1267.253 -1267.253] (1.000)
Step: 20749, Reward: [-1474.428 -1474.428 -1474.428] [0.0000], Avg: [-1267.752 -1267.752 -1267.752] (1.000)
Step: 20799, Reward: [-1285.212 -1285.212 -1285.212] [0.0000], Avg: [-1267.794 -1267.794 -1267.794] (1.000)
Step: 20849, Reward: [-1845.933 -1845.933 -1845.933] [0.0000], Avg: [-1269.18 -1269.18 -1269.18] (1.000)
Step: 20899, Reward: [-1390.511 -1390.511 -1390.511] [0.0000], Avg: [-1269.471 -1269.471 -1269.471] (1.000)
Step: 20949, Reward: [-1335.357 -1335.357 -1335.357] [0.0000], Avg: [-1269.628 -1269.628 -1269.628] (1.000)
Step: 20999, Reward: [-1507.57 -1507.57 -1507.57] [0.0000], Avg: [-1270.194 -1270.194 -1270.194] (1.000)
Step: 21049, Reward: [-1314.664 -1314.664 -1314.664] [0.0000], Avg: [-1270.3 -1270.3 -1270.3] (1.000)
Step: 21099, Reward: [-1831.245 -1831.245 -1831.245] [0.0000], Avg: [-1271.629 -1271.629 -1271.629] (1.000)
Step: 21149, Reward: [-1639.742 -1639.742 -1639.742] [0.0000], Avg: [-1272.499 -1272.499 -1272.499] (1.000)
Step: 21199, Reward: [-1252.859 -1252.859 -1252.859] [0.0000], Avg: [-1272.453 -1272.453 -1272.453] (1.000)
Step: 21249, Reward: [-1854.579 -1854.579 -1854.579] [0.0000], Avg: [-1273.823 -1273.823 -1273.823] (1.000)
Step: 21299, Reward: [-1459.469 -1459.469 -1459.469] [0.0000], Avg: [-1274.259 -1274.259 -1274.259] (1.000)
Step: 21349, Reward: [-1842.722 -1842.722 -1842.722] [0.0000], Avg: [-1275.59 -1275.59 -1275.59] (1.000)
Step: 21399, Reward: [-1101.008 -1101.008 -1101.008] [0.0000], Avg: [-1275.182 -1275.182 -1275.182] (1.000)
Step: 21449, Reward: [-1619.816 -1619.816 -1619.816] [0.0000], Avg: [-1275.985 -1275.985 -1275.985] (1.000)
Step: 21499, Reward: [-1874.378 -1874.378 -1874.378] [0.0000], Avg: [-1277.377 -1277.377 -1277.377] (1.000)
Step: 21549, Reward: [-1331.026 -1331.026 -1331.026] [0.0000], Avg: [-1277.501 -1277.501 -1277.501] (1.000)
Step: 21599, Reward: [-1951.153 -1951.153 -1951.153] [0.0000], Avg: [-1279.061 -1279.061 -1279.061] (1.000)
Step: 21649, Reward: [-1768.449 -1768.449 -1768.449] [0.0000], Avg: [-1280.191 -1280.191 -1280.191] (1.000)
Step: 21699, Reward: [-1610.03 -1610.03 -1610.03] [0.0000], Avg: [-1280.951 -1280.951 -1280.951] (1.000)
Step: 21749, Reward: [-1331.089 -1331.089 -1331.089] [0.0000], Avg: [-1281.066 -1281.066 -1281.066] (1.000)
Step: 21799, Reward: [-1694.747 -1694.747 -1694.747] [0.0000], Avg: [-1282.015 -1282.015 -1282.015] (1.000)
Step: 21849, Reward: [-1386.804 -1386.804 -1386.804] [0.0000], Avg: [-1282.255 -1282.255 -1282.255] (1.000)
Step: 21899, Reward: [-1494.196 -1494.196 -1494.196] [0.0000], Avg: [-1282.739 -1282.739 -1282.739] (1.000)
Step: 21949, Reward: [-1411.977 -1411.977 -1411.977] [0.0000], Avg: [-1283.033 -1283.033 -1283.033] (1.000)
Step: 21999, Reward: [-1714.762 -1714.762 -1714.762] [0.0000], Avg: [-1284.014 -1284.014 -1284.014] (1.000)
Step: 22049, Reward: [-1458.468 -1458.468 -1458.468] [0.0000], Avg: [-1284.41 -1284.41 -1284.41] (1.000)
Step: 22099, Reward: [-1254.262 -1254.262 -1254.262] [0.0000], Avg: [-1284.342 -1284.342 -1284.342] (1.000)
Step: 22149, Reward: [-1707.534 -1707.534 -1707.534] [0.0000], Avg: [-1285.297 -1285.297 -1285.297] (1.000)
Step: 22199, Reward: [-2016.137 -2016.137 -2016.137] [0.0000], Avg: [-1286.943 -1286.943 -1286.943] (1.000)
Step: 22249, Reward: [-1562.414 -1562.414 -1562.414] [0.0000], Avg: [-1287.562 -1287.562 -1287.562] (1.000)
Step: 22299, Reward: [-1634.203 -1634.203 -1634.203] [0.0000], Avg: [-1288.339 -1288.339 -1288.339] (1.000)
Step: 22349, Reward: [-1520.113 -1520.113 -1520.113] [0.0000], Avg: [-1288.858 -1288.858 -1288.858] (1.000)
Step: 22399, Reward: [-1759.514 -1759.514 -1759.514] [0.0000], Avg: [-1289.908 -1289.908 -1289.908] (1.000)
Step: 22449, Reward: [-1415.226 -1415.226 -1415.226] [0.0000], Avg: [-1290.188 -1290.188 -1290.188] (1.000)
Step: 22499, Reward: [-1613.43 -1613.43 -1613.43] [0.0000], Avg: [-1290.906 -1290.906 -1290.906] (1.000)
Step: 22549, Reward: [-1335.712 -1335.712 -1335.712] [0.0000], Avg: [-1291.005 -1291.005 -1291.005] (1.000)
Step: 22599, Reward: [-1223.3 -1223.3 -1223.3] [0.0000], Avg: [-1290.855 -1290.855 -1290.855] (1.000)
Step: 22649, Reward: [-1342.454 -1342.454 -1342.454] [0.0000], Avg: [-1290.969 -1290.969 -1290.969] (1.000)
Step: 22699, Reward: [-1340.294 -1340.294 -1340.294] [0.0000], Avg: [-1291.078 -1291.078 -1291.078] (1.000)
Step: 22749, Reward: [-1738.356 -1738.356 -1738.356] [0.0000], Avg: [-1292.061 -1292.061 -1292.061] (1.000)
Step: 22799, Reward: [-1572.826 -1572.826 -1572.826] [0.0000], Avg: [-1292.677 -1292.677 -1292.677] (1.000)
Step: 22849, Reward: [-1359.758 -1359.758 -1359.758] [0.0000], Avg: [-1292.824 -1292.824 -1292.824] (1.000)
Step: 22899, Reward: [-1489.066 -1489.066 -1489.066] [0.0000], Avg: [-1293.252 -1293.252 -1293.252] (1.000)
Step: 22949, Reward: [-1708.037 -1708.037 -1708.037] [0.0000], Avg: [-1294.156 -1294.156 -1294.156] (1.000)
Step: 22999, Reward: [-1554.98 -1554.98 -1554.98] [0.0000], Avg: [-1294.723 -1294.723 -1294.723] (1.000)
Step: 23049, Reward: [-1596.196 -1596.196 -1596.196] [0.0000], Avg: [-1295.377 -1295.377 -1295.377] (1.000)
Step: 23099, Reward: [-1924.601 -1924.601 -1924.601] [0.0000], Avg: [-1296.739 -1296.739 -1296.739] (1.000)
Step: 23149, Reward: [-1315.082 -1315.082 -1315.082] [0.0000], Avg: [-1296.778 -1296.778 -1296.778] (1.000)
Step: 23199, Reward: [-1477.169 -1477.169 -1477.169] [0.0000], Avg: [-1297.167 -1297.167 -1297.167] (1.000)
Step: 23249, Reward: [-1512.07 -1512.07 -1512.07] [0.0000], Avg: [-1297.629 -1297.629 -1297.629] (1.000)
Step: 23299, Reward: [-1521.83 -1521.83 -1521.83] [0.0000], Avg: [-1298.11 -1298.11 -1298.11] (1.000)
Step: 23349, Reward: [-1955.187 -1955.187 -1955.187] [0.0000], Avg: [-1299.517 -1299.517 -1299.517] (1.000)
Step: 23399, Reward: [-1400.952 -1400.952 -1400.952] [0.0000], Avg: [-1299.734 -1299.734 -1299.734] (1.000)
Step: 23449, Reward: [-1752.255 -1752.255 -1752.255] [0.0000], Avg: [-1300.699 -1300.699 -1300.699] (1.000)
Step: 23499, Reward: [-1699.312 -1699.312 -1699.312] [0.0000], Avg: [-1301.547 -1301.547 -1301.547] (1.000)
Step: 23549, Reward: [-1641.115 -1641.115 -1641.115] [0.0000], Avg: [-1302.268 -1302.268 -1302.268] (1.000)
Step: 23599, Reward: [-1662.619 -1662.619 -1662.619] [0.0000], Avg: [-1303.031 -1303.031 -1303.031] (1.000)
Step: 23649, Reward: [-1934.841 -1934.841 -1934.841] [0.0000], Avg: [-1304.367 -1304.367 -1304.367] (1.000)
Step: 23699, Reward: [-1273.066 -1273.066 -1273.066] [0.0000], Avg: [-1304.301 -1304.301 -1304.301] (1.000)
Step: 23749, Reward: [-1545.114 -1545.114 -1545.114] [0.0000], Avg: [-1304.808 -1304.808 -1304.808] (1.000)
Step: 23799, Reward: [-1761.971 -1761.971 -1761.971] [0.0000], Avg: [-1305.769 -1305.769 -1305.769] (1.000)
Step: 23849, Reward: [-1470.91 -1470.91 -1470.91] [0.0000], Avg: [-1306.115 -1306.115 -1306.115] (1.000)
Step: 23899, Reward: [-1524.295 -1524.295 -1524.295] [0.0000], Avg: [-1306.571 -1306.571 -1306.571] (1.000)
Step: 23949, Reward: [-1331.842 -1331.842 -1331.842] [0.0000], Avg: [-1306.624 -1306.624 -1306.624] (1.000)
Step: 23999, Reward: [-1434.563 -1434.563 -1434.563] [0.0000], Avg: [-1306.89 -1306.89 -1306.89] (1.000)
Step: 24049, Reward: [-1733.514 -1733.514 -1733.514] [0.0000], Avg: [-1307.777 -1307.777 -1307.777] (1.000)
Step: 24099, Reward: [-1451.146 -1451.146 -1451.146] [0.0000], Avg: [-1308.075 -1308.075 -1308.075] (1.000)
Step: 24149, Reward: [-1721.436 -1721.436 -1721.436] [0.0000], Avg: [-1308.931 -1308.931 -1308.931] (1.000)
Step: 24199, Reward: [-1630.193 -1630.193 -1630.193] [0.0000], Avg: [-1309.594 -1309.594 -1309.594] (1.000)
Step: 24249, Reward: [-1846.243 -1846.243 -1846.243] [0.0000], Avg: [-1310.701 -1310.701 -1310.701] (1.000)
Step: 24299, Reward: [-1553.499 -1553.499 -1553.499] [0.0000], Avg: [-1311.201 -1311.201 -1311.201] (1.000)
Step: 24349, Reward: [-1475.459 -1475.459 -1475.459] [0.0000], Avg: [-1311.538 -1311.538 -1311.538] (1.000)
Step: 24399, Reward: [-1507.977 -1507.977 -1507.977] [0.0000], Avg: [-1311.94 -1311.94 -1311.94] (1.000)
Step: 24449, Reward: [-1496.872 -1496.872 -1496.872] [0.0000], Avg: [-1312.319 -1312.319 -1312.319] (1.000)
Step: 24499, Reward: [-2087.05 -2087.05 -2087.05] [0.0000], Avg: [-1313.9 -1313.9 -1313.9] (1.000)
Step: 24549, Reward: [-1631.559 -1631.559 -1631.559] [0.0000], Avg: [-1314.547 -1314.547 -1314.547] (1.000)
Step: 24599, Reward: [-1272.617 -1272.617 -1272.617] [0.0000], Avg: [-1314.461 -1314.461 -1314.461] (1.000)
Step: 24649, Reward: [-1801.733 -1801.733 -1801.733] [0.0000], Avg: [-1315.45 -1315.45 -1315.45] (1.000)
Step: 24699, Reward: [-1513.365 -1513.365 -1513.365] [0.0000], Avg: [-1315.85 -1315.85 -1315.85] (1.000)
Step: 24749, Reward: [-1471.113 -1471.113 -1471.113] [0.0000], Avg: [-1316.164 -1316.164 -1316.164] (1.000)
Step: 24799, Reward: [-1482.862 -1482.862 -1482.862] [0.0000], Avg: [-1316.5 -1316.5 -1316.5] (1.000)
Step: 24849, Reward: [-1556.826 -1556.826 -1556.826] [0.0000], Avg: [-1316.984 -1316.984 -1316.984] (1.000)
Step: 24899, Reward: [-1352.68 -1352.68 -1352.68] [0.0000], Avg: [-1317.055 -1317.055 -1317.055] (1.000)
Step: 24949, Reward: [-1388.957 -1388.957 -1388.957] [0.0000], Avg: [-1317.199 -1317.199 -1317.199] (1.000)
Step: 24999, Reward: [-1984.955 -1984.955 -1984.955] [0.0000], Avg: [-1318.535 -1318.535 -1318.535] (1.000)
Step: 25049, Reward: [-1301.075 -1301.075 -1301.075] [0.0000], Avg: [-1318.5 -1318.5 -1318.5] (1.000)
Step: 25099, Reward: [-1644.198 -1644.198 -1644.198] [0.0000], Avg: [-1319.149 -1319.149 -1319.149] (1.000)
Step: 25149, Reward: [-1628.929 -1628.929 -1628.929] [0.0000], Avg: [-1319.765 -1319.765 -1319.765] (1.000)
Step: 25199, Reward: [-1780.546 -1780.546 -1780.546] [0.0000], Avg: [-1320.679 -1320.679 -1320.679] (1.000)
Step: 25249, Reward: [-1780.851 -1780.851 -1780.851] [0.0000], Avg: [-1321.59 -1321.59 -1321.59] (1.000)
Step: 25299, Reward: [-1898.459 -1898.459 -1898.459] [0.0000], Avg: [-1322.73 -1322.73 -1322.73] (1.000)
Step: 25349, Reward: [-1773.409 -1773.409 -1773.409] [0.0000], Avg: [-1323.619 -1323.619 -1323.619] (1.000)
Step: 25399, Reward: [-1711.892 -1711.892 -1711.892] [0.0000], Avg: [-1324.384 -1324.384 -1324.384] (1.000)
Step: 25449, Reward: [-1340.258 -1340.258 -1340.258] [0.0000], Avg: [-1324.415 -1324.415 -1324.415] (1.000)
Step: 25499, Reward: [-1918.07 -1918.07 -1918.07] [0.0000], Avg: [-1325.579 -1325.579 -1325.579] (1.000)
Step: 25549, Reward: [-1787.772 -1787.772 -1787.772] [0.0000], Avg: [-1326.483 -1326.483 -1326.483] (1.000)
Step: 25599, Reward: [-1812.086 -1812.086 -1812.086] [0.0000], Avg: [-1327.432 -1327.432 -1327.432] (1.000)
Step: 25649, Reward: [-1905.377 -1905.377 -1905.377] [0.0000], Avg: [-1328.558 -1328.558 -1328.558] (1.000)
Step: 25699, Reward: [-1911.485 -1911.485 -1911.485] [0.0000], Avg: [-1329.692 -1329.692 -1329.692] (1.000)
Step: 25749, Reward: [-1686.744 -1686.744 -1686.744] [0.0000], Avg: [-1330.386 -1330.386 -1330.386] (1.000)
Step: 25799, Reward: [-1244.99 -1244.99 -1244.99] [0.0000], Avg: [-1330.22 -1330.22 -1330.22] (1.000)
Step: 25849, Reward: [-1700.222 -1700.222 -1700.222] [0.0000], Avg: [-1330.936 -1330.936 -1330.936] (1.000)
Step: 25899, Reward: [-1315.744 -1315.744 -1315.744] [0.0000], Avg: [-1330.907 -1330.907 -1330.907] (1.000)
Step: 25949, Reward: [-1814.817 -1814.817 -1814.817] [0.0000], Avg: [-1331.839 -1331.839 -1331.839] (1.000)
Step: 25999, Reward: [-1272.027 -1272.027 -1272.027] [0.0000], Avg: [-1331.724 -1331.724 -1331.724] (1.000)
Step: 26049, Reward: [-1767.955 -1767.955 -1767.955] [0.0000], Avg: [-1332.561 -1332.561 -1332.561] (1.000)
Step: 26099, Reward: [-2110.034 -2110.034 -2110.034] [0.0000], Avg: [-1334.051 -1334.051 -1334.051] (1.000)
Step: 26149, Reward: [-1271.666 -1271.666 -1271.666] [0.0000], Avg: [-1333.931 -1333.931 -1333.931] (1.000)
Step: 26199, Reward: [-1619.768 -1619.768 -1619.768] [0.0000], Avg: [-1334.477 -1334.477 -1334.477] (1.000)
Step: 26249, Reward: [-1713.365 -1713.365 -1713.365] [0.0000], Avg: [-1335.198 -1335.198 -1335.198] (1.000)
Step: 26299, Reward: [-1334.031 -1334.031 -1334.031] [0.0000], Avg: [-1335.196 -1335.196 -1335.196] (1.000)
Step: 26349, Reward: [-1362.911 -1362.911 -1362.911] [0.0000], Avg: [-1335.249 -1335.249 -1335.249] (1.000)
Step: 26399, Reward: [-1995.416 -1995.416 -1995.416] [0.0000], Avg: [-1336.499 -1336.499 -1336.499] (1.000)
Step: 26449, Reward: [-1616.988 -1616.988 -1616.988] [0.0000], Avg: [-1337.029 -1337.029 -1337.029] (1.000)
Step: 26499, Reward: [-1676.827 -1676.827 -1676.827] [0.0000], Avg: [-1337.671 -1337.671 -1337.671] (1.000)
Step: 26549, Reward: [-1609.426 -1609.426 -1609.426] [0.0000], Avg: [-1338.182 -1338.182 -1338.182] (1.000)
Step: 26599, Reward: [-1861.688 -1861.688 -1861.688] [0.0000], Avg: [-1339.166 -1339.166 -1339.166] (1.000)
Step: 26649, Reward: [-1729.414 -1729.414 -1729.414] [0.0000], Avg: [-1339.899 -1339.899 -1339.899] (1.000)
Step: 26699, Reward: [-1239.852 -1239.852 -1239.852] [0.0000], Avg: [-1339.711 -1339.711 -1339.711] (1.000)
Step: 26749, Reward: [-1942.325 -1942.325 -1942.325] [0.0000], Avg: [-1340.838 -1340.838 -1340.838] (1.000)
Step: 26799, Reward: [-1546.381 -1546.381 -1546.381] [0.0000], Avg: [-1341.221 -1341.221 -1341.221] (1.000)
Step: 26849, Reward: [-1653.464 -1653.464 -1653.464] [0.0000], Avg: [-1341.802 -1341.802 -1341.802] (1.000)
Step: 26899, Reward: [-1541.645 -1541.645 -1541.645] [0.0000], Avg: [-1342.174 -1342.174 -1342.174] (1.000)
Step: 26949, Reward: [-1526.938 -1526.938 -1526.938] [0.0000], Avg: [-1342.517 -1342.517 -1342.517] (1.000)
Step: 26999, Reward: [-1300.935 -1300.935 -1300.935] [0.0000], Avg: [-1342.44 -1342.44 -1342.44] (1.000)
Step: 27049, Reward: [-1483.422 -1483.422 -1483.422] [0.0000], Avg: [-1342.7 -1342.7 -1342.7] (1.000)
Step: 27099, Reward: [-1345.841 -1345.841 -1345.841] [0.0000], Avg: [-1342.706 -1342.706 -1342.706] (1.000)
Step: 27149, Reward: [-1637.059 -1637.059 -1637.059] [0.0000], Avg: [-1343.248 -1343.248 -1343.248] (1.000)
Step: 27199, Reward: [-2080.084 -2080.084 -2080.084] [0.0000], Avg: [-1344.603 -1344.603 -1344.603] (1.000)
Step: 27249, Reward: [-1561.683 -1561.683 -1561.683] [0.0000], Avg: [-1345.001 -1345.001 -1345.001] (1.000)
Step: 27299, Reward: [-1375.696 -1375.696 -1375.696] [0.0000], Avg: [-1345.057 -1345.057 -1345.057] (1.000)
Step: 27349, Reward: [-1282.557 -1282.557 -1282.557] [0.0000], Avg: [-1344.943 -1344.943 -1344.943] (1.000)
Step: 27399, Reward: [-1638.067 -1638.067 -1638.067] [0.0000], Avg: [-1345.478 -1345.478 -1345.478] (1.000)
Step: 27449, Reward: [-1424.453 -1424.453 -1424.453] [0.0000], Avg: [-1345.622 -1345.622 -1345.622] (1.000)
Step: 27499, Reward: [-1631.072 -1631.072 -1631.072] [0.0000], Avg: [-1346.141 -1346.141 -1346.141] (1.000)
Step: 27549, Reward: [-1456.703 -1456.703 -1456.703] [0.0000], Avg: [-1346.341 -1346.341 -1346.341] (1.000)
Step: 27599, Reward: [-2053.95 -2053.95 -2053.95] [0.0000], Avg: [-1347.623 -1347.623 -1347.623] (1.000)
Step: 27649, Reward: [-1247.868 -1247.868 -1247.868] [0.0000], Avg: [-1347.443 -1347.443 -1347.443] (1.000)
Step: 27699, Reward: [-1796.515 -1796.515 -1796.515] [0.0000], Avg: [-1348.253 -1348.253 -1348.253] (1.000)
Step: 27749, Reward: [-1552.629 -1552.629 -1552.629] [0.0000], Avg: [-1348.622 -1348.622 -1348.622] (1.000)
Step: 27799, Reward: [-1265.025 -1265.025 -1265.025] [0.0000], Avg: [-1348.471 -1348.471 -1348.471] (1.000)
Step: 27849, Reward: [-1457.083 -1457.083 -1457.083] [0.0000], Avg: [-1348.666 -1348.666 -1348.666] (1.000)
Step: 27899, Reward: [-1465.203 -1465.203 -1465.203] [0.0000], Avg: [-1348.875 -1348.875 -1348.875] (1.000)
Step: 27949, Reward: [-1776.128 -1776.128 -1776.128] [0.0000], Avg: [-1349.64 -1349.64 -1349.64] (1.000)
Step: 27999, Reward: [-1458.011 -1458.011 -1458.011] [0.0000], Avg: [-1349.833 -1349.833 -1349.833] (1.000)
Step: 28049, Reward: [-1601.88 -1601.88 -1601.88] [0.0000], Avg: [-1350.282 -1350.282 -1350.282] (1.000)
Step: 28099, Reward: [-1749.461 -1749.461 -1749.461] [0.0000], Avg: [-1350.993 -1350.993 -1350.993] (1.000)
Step: 28149, Reward: [-1682.13 -1682.13 -1682.13] [0.0000], Avg: [-1351.581 -1351.581 -1351.581] (1.000)
Step: 28199, Reward: [-1155.773 -1155.773 -1155.773] [0.0000], Avg: [-1351.234 -1351.234 -1351.234] (1.000)
Step: 28249, Reward: [-1705.639 -1705.639 -1705.639] [0.0000], Avg: [-1351.861 -1351.861 -1351.861] (1.000)
Step: 28299, Reward: [-1767.798 -1767.798 -1767.798] [0.0000], Avg: [-1352.596 -1352.596 -1352.596] (1.000)
Step: 28349, Reward: [-1421.787 -1421.787 -1421.787] [0.0000], Avg: [-1352.718 -1352.718 -1352.718] (1.000)
Step: 28399, Reward: [-1469.194 -1469.194 -1469.194] [0.0000], Avg: [-1352.923 -1352.923 -1352.923] (1.000)
Step: 28449, Reward: [-1163.727 -1163.727 -1163.727] [0.0000], Avg: [-1352.59 -1352.59 -1352.59] (1.000)
Step: 28499, Reward: [-1696.266 -1696.266 -1696.266] [0.0000], Avg: [-1353.193 -1353.193 -1353.193] (1.000)
Step: 28549, Reward: [-1214.479 -1214.479 -1214.479] [0.0000], Avg: [-1352.95 -1352.95 -1352.95] (1.000)
Step: 28599, Reward: [-1862.314 -1862.314 -1862.314] [0.0000], Avg: [-1353.841 -1353.841 -1353.841] (1.000)
Step: 28649, Reward: [-1544.3 -1544.3 -1544.3] [0.0000], Avg: [-1354.173 -1354.173 -1354.173] (1.000)
Step: 28699, Reward: [-1742.959 -1742.959 -1742.959] [0.0000], Avg: [-1354.851 -1354.851 -1354.851] (1.000)
Step: 28749, Reward: [-1671.811 -1671.811 -1671.811] [0.0000], Avg: [-1355.402 -1355.402 -1355.402] (1.000)
Step: 28799, Reward: [-1422.593 -1422.593 -1422.593] [0.0000], Avg: [-1355.518 -1355.518 -1355.518] (1.000)
Step: 28849, Reward: [-2105.482 -2105.482 -2105.482] [0.0000], Avg: [-1356.818 -1356.818 -1356.818] (1.000)
Step: 28899, Reward: [-1926.093 -1926.093 -1926.093] [0.0000], Avg: [-1357.803 -1357.803 -1357.803] (1.000)
Step: 28949, Reward: [-1795.328 -1795.328 -1795.328] [0.0000], Avg: [-1358.559 -1358.559 -1358.559] (1.000)
Step: 28999, Reward: [-1549.816 -1549.816 -1549.816] [0.0000], Avg: [-1358.889 -1358.889 -1358.889] (1.000)
Step: 29049, Reward: [-1359.144 -1359.144 -1359.144] [0.0000], Avg: [-1358.889 -1358.889 -1358.889] (1.000)
Step: 29099, Reward: [-1429.363 -1429.363 -1429.363] [0.0000], Avg: [-1359.01 -1359.01 -1359.01] (1.000)
Step: 29149, Reward: [-1298.162 -1298.162 -1298.162] [0.0000], Avg: [-1358.906 -1358.906 -1358.906] (1.000)
Step: 29199, Reward: [-1525.375 -1525.375 -1525.375] [0.0000], Avg: [-1359.191 -1359.191 -1359.191] (1.000)
Step: 29249, Reward: [-1889.255 -1889.255 -1889.255] [0.0000], Avg: [-1360.097 -1360.097 -1360.097] (1.000)
Step: 29299, Reward: [-1625.132 -1625.132 -1625.132] [0.0000], Avg: [-1360.549 -1360.549 -1360.549] (1.000)
Step: 29349, Reward: [-1817.499 -1817.499 -1817.499] [0.0000], Avg: [-1361.328 -1361.328 -1361.328] (1.000)
Step: 29399, Reward: [-1506.168 -1506.168 -1506.168] [0.0000], Avg: [-1361.574 -1361.574 -1361.574] (1.000)
Step: 29449, Reward: [-1472.303 -1472.303 -1472.303] [0.0000], Avg: [-1361.762 -1361.762 -1361.762] (1.000)
Step: 29499, Reward: [-1703.707 -1703.707 -1703.707] [0.0000], Avg: [-1362.341 -1362.341 -1362.341] (1.000)
Step: 29549, Reward: [-1432.56 -1432.56 -1432.56] [0.0000], Avg: [-1362.46 -1362.46 -1362.46] (1.000)
Step: 29599, Reward: [-1190.351 -1190.351 -1190.351] [0.0000], Avg: [-1362.17 -1362.17 -1362.17] (1.000)
Step: 29649, Reward: [-1478.562 -1478.562 -1478.562] [0.0000], Avg: [-1362.366 -1362.366 -1362.366] (1.000)
Step: 29699, Reward: [-1343.448 -1343.448 -1343.448] [0.0000], Avg: [-1362.334 -1362.334 -1362.334] (1.000)
Step: 29749, Reward: [-1626.202 -1626.202 -1626.202] [0.0000], Avg: [-1362.777 -1362.777 -1362.777] (1.000)
Step: 29799, Reward: [-1292.268 -1292.268 -1292.268] [0.0000], Avg: [-1362.659 -1362.659 -1362.659] (1.000)
Step: 29849, Reward: [-1608.704 -1608.704 -1608.704] [0.0000], Avg: [-1363.071 -1363.071 -1363.071] (1.000)
Step: 29899, Reward: [-1387.595 -1387.595 -1387.595] [0.0000], Avg: [-1363.112 -1363.112 -1363.112] (1.000)
Step: 29949, Reward: [-1575.27 -1575.27 -1575.27] [0.0000], Avg: [-1363.466 -1363.466 -1363.466] (1.000)
Step: 29999, Reward: [-1362.267 -1362.267 -1362.267] [0.0000], Avg: [-1363.464 -1363.464 -1363.464] (1.000)
Step: 30049, Reward: [-1656.593 -1656.593 -1656.593] [0.0000], Avg: [-1363.952 -1363.952 -1363.952] (1.000)
Step: 30099, Reward: [-1783.444 -1783.444 -1783.444] [0.0000], Avg: [-1364.649 -1364.649 -1364.649] (1.000)
Step: 30149, Reward: [-1535.112 -1535.112 -1535.112] [0.0000], Avg: [-1364.932 -1364.932 -1364.932] (1.000)
Step: 30199, Reward: [-1335.724 -1335.724 -1335.724] [0.0000], Avg: [-1364.883 -1364.883 -1364.883] (1.000)
Step: 30249, Reward: [-1709.809 -1709.809 -1709.809] [0.0000], Avg: [-1365.453 -1365.453 -1365.453] (1.000)
Step: 30299, Reward: [-1892.533 -1892.533 -1892.533] [0.0000], Avg: [-1366.323 -1366.323 -1366.323] (1.000)
Step: 30349, Reward: [-1934.735 -1934.735 -1934.735] [0.0000], Avg: [-1367.26 -1367.26 -1367.26] (1.000)
Step: 30399, Reward: [-1660.211 -1660.211 -1660.211] [0.0000], Avg: [-1367.741 -1367.741 -1367.741] (1.000)
Step: 30449, Reward: [-1366.209 -1366.209 -1366.209] [0.0000], Avg: [-1367.739 -1367.739 -1367.739] (1.000)
Step: 30499, Reward: [-1395.917 -1395.917 -1395.917] [0.0000], Avg: [-1367.785 -1367.785 -1367.785] (1.000)
Step: 30549, Reward: [-1929.499 -1929.499 -1929.499] [0.0000], Avg: [-1368.705 -1368.705 -1368.705] (1.000)
Step: 30599, Reward: [-1382.292 -1382.292 -1382.292] [0.0000], Avg: [-1368.727 -1368.727 -1368.727] (1.000)
Step: 30649, Reward: [-1413.901 -1413.901 -1413.901] [0.0000], Avg: [-1368.8 -1368.8 -1368.8] (1.000)
Step: 30699, Reward: [-1618.193 -1618.193 -1618.193] [0.0000], Avg: [-1369.207 -1369.207 -1369.207] (1.000)
Step: 30749, Reward: [-1730.956 -1730.956 -1730.956] [0.0000], Avg: [-1369.795 -1369.795 -1369.795] (1.000)
Step: 30799, Reward: [-1916.256 -1916.256 -1916.256] [0.0000], Avg: [-1370.682 -1370.682 -1370.682] (1.000)
Step: 30849, Reward: [-1572.912 -1572.912 -1572.912] [0.0000], Avg: [-1371.01 -1371.01 -1371.01] (1.000)
Step: 30899, Reward: [-1601.481 -1601.481 -1601.481] [0.0000], Avg: [-1371.383 -1371.383 -1371.383] (1.000)
Step: 30949, Reward: [-1546.444 -1546.444 -1546.444] [0.0000], Avg: [-1371.665 -1371.665 -1371.665] (1.000)
Step: 30999, Reward: [-1719.148 -1719.148 -1719.148] [0.0000], Avg: [-1372.226 -1372.226 -1372.226] (1.000)
Step: 31049, Reward: [-1801.501 -1801.501 -1801.501] [0.0000], Avg: [-1372.917 -1372.917 -1372.917] (1.000)
Step: 31099, Reward: [-1683.279 -1683.279 -1683.279] [0.0000], Avg: [-1373.416 -1373.416 -1373.416] (1.000)
Step: 31149, Reward: [-1774.14 -1774.14 -1774.14] [0.0000], Avg: [-1374.059 -1374.059 -1374.059] (1.000)
