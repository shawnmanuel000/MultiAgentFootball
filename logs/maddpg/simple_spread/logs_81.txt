Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(10000, state_size, action_size)
		self.agent = MADDPG(state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.agent.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.t = 0 if not hasattr(self, "t") else self.t + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.agent.get_action_probs(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.agent.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1], gamma=0.95)[0].squeeze(-1) for q_value,reward,done in zip(q_values, rewards, dones)]

			states, actions = map(lambda items: [x[:-1] for x in items], [states, actions])
			states, actions, q_targets = map(lambda items: [x.view(-1, *x.shape[2:]).cpu().numpy() for x in items], [states, actions, q_targets])
			self.replay_buffer.push(states, actions, q_targets)

		if (len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.t % 100)==0):
			states, actions, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.agent.update(states, actions, q_targets)

class MADDPG(PTNetwork):
	def __init__(self, state_size, action_size, gamma=0.95, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.gamma = gamma
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.agents = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.agents)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.agents]
			return q_value

	def update(self, states, actions, q_targets):
		for agent_i, curr_agent in enumerate(self.agents):
			target_value = q_targets[agent_i]
			# next_actions = [one_hot(agent.get_action(nobs, numpy=False)) for agent, nobs in zip(self.agents, next_states)]
			# next_states_joint = torch.cat([*next_states], dim=-1)
			# next_actions_joint = torch.cat([*next_actions], dim=-1)
			# next_value = curr_agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			# target_value = (rewards[agent_i].view(-1, 1, 1) + self.gamma * next_value * (1 - dones[agent_i].view(-1, 1, 1)))

			states_joint = torch.cat([*states], dim=-1)
			actions_joint = torch.cat([*actions], dim=-1)
			actual_value = curr_agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			vf_loss = (actual_value - target_value.detach()).pow(2).mean()
			curr_agent.step(curr_agent.critic_optimizer, vf_loss, param_norm=curr_agent.critic_local.parameters())
			curr_agent.soft_copy(curr_agent.critic_local, curr_agent.critic_target)

			curr_pol_out = curr_agent.get_action(states[agent_i], grad=True, numpy=False)
			curr_pol_vf_in = gsoftmax(curr_pol_out, hard=True)
			action = [curr_pol_vf_in if i==agent_i else one_hot(agent.get_action(ob, numpy=False)) for (i,agent), ob in zip(enumerate(self.agents), states)]
			action_joint = torch.cat([*action], dim=-1)
			pol_loss = -curr_agent.critic_local(states_joint, action_joint).mean() + 0.001*curr_pol_out.pow(2).mean() 
			curr_agent.step(curr_agent.actor_optimizer, pol_loss, param_norm=curr_agent.actor_local.parameters())
			curr_agent.soft_copy(curr_agent.actor_local, curr_agent.actor_target)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-441.596 -441.596 -441.596] [0.0000], Avg: [-441.596 -441.596 -441.596] (1.000)
Step: 99, Reward: [-605.051 -605.051 -605.051] [0.0000], Avg: [-523.324 -523.324 -523.324] (1.000)
Step: 149, Reward: [-719.273 -719.273 -719.273] [0.0000], Avg: [-588.64 -588.64 -588.64] (1.000)
Step: 199, Reward: [-554.841 -554.841 -554.841] [0.0000], Avg: [-580.19 -580.19 -580.19] (1.000)
Step: 249, Reward: [-315.965 -315.965 -315.965] [0.0000], Avg: [-527.345 -527.345 -527.345] (1.000)
Step: 299, Reward: [-489.893 -489.893 -489.893] [0.0000], Avg: [-521.103 -521.103 -521.103] (1.000)
Step: 349, Reward: [-424.2 -424.2 -424.2] [0.0000], Avg: [-507.26 -507.26 -507.26] (1.000)
Step: 399, Reward: [-486.136 -486.136 -486.136] [0.0000], Avg: [-504.619 -504.619 -504.619] (1.000)
Step: 449, Reward: [-540.808 -540.808 -540.808] [0.0000], Avg: [-508.64 -508.64 -508.64] (1.000)
Step: 499, Reward: [-465.737 -465.737 -465.737] [0.0000], Avg: [-504.35 -504.35 -504.35] (1.000)
Step: 549, Reward: [-512.52 -512.52 -512.52] [0.0000], Avg: [-505.093 -505.093 -505.093] (1.000)
Step: 599, Reward: [-457.447 -457.447 -457.447] [0.0000], Avg: [-501.122 -501.122 -501.122] (1.000)
Step: 649, Reward: [-326.471 -326.471 -326.471] [0.0000], Avg: [-487.687 -487.687 -487.687] (1.000)
Step: 699, Reward: [-506.865 -506.865 -506.865] [0.0000], Avg: [-489.057 -489.057 -489.057] (1.000)
Step: 749, Reward: [-400.778 -400.778 -400.778] [0.0000], Avg: [-483.172 -483.172 -483.172] (1.000)
Step: 799, Reward: [-569.547 -569.547 -569.547] [0.0000], Avg: [-488.57 -488.57 -488.57] (1.000)
Step: 849, Reward: [-429.948 -429.948 -429.948] [0.0000], Avg: [-485.122 -485.122 -485.122] (1.000)
Step: 899, Reward: [-568.861 -568.861 -568.861] [0.0000], Avg: [-489.774 -489.774 -489.774] (1.000)
Step: 949, Reward: [-732.651 -732.651 -732.651] [0.0000], Avg: [-502.557 -502.557 -502.557] (1.000)
Step: 999, Reward: [-448.338 -448.338 -448.338] [0.0000], Avg: [-499.846 -499.846 -499.846] (1.000)
Step: 1049, Reward: [-666.093 -666.093 -666.093] [0.0000], Avg: [-507.763 -507.763 -507.763] (1.000)
Step: 1099, Reward: [-331.374 -331.374 -331.374] [0.0000], Avg: [-499.745 -499.745 -499.745] (1.000)
Step: 1149, Reward: [-617.877 -617.877 -617.877] [0.0000], Avg: [-504.881 -504.881 -504.881] (1.000)
Step: 1199, Reward: [-1289.123 -1289.123 -1289.123] [0.0000], Avg: [-537.558 -537.558 -537.558] (1.000)
Step: 1249, Reward: [-673.368 -673.368 -673.368] [0.0000], Avg: [-542.99 -542.99 -542.99] (1.000)
Step: 1299, Reward: [-651.027 -651.027 -651.027] [0.0000], Avg: [-547.146 -547.146 -547.146] (1.000)
Step: 1349, Reward: [-851.328 -851.328 -851.328] [0.0000], Avg: [-558.412 -558.412 -558.412] (1.000)
Step: 1399, Reward: [-706.011 -706.011 -706.011] [0.0000], Avg: [-563.683 -563.683 -563.683] (1.000)
Step: 1449, Reward: [-521.556 -521.556 -521.556] [0.0000], Avg: [-562.23 -562.23 -562.23] (1.000)
Step: 1499, Reward: [-595.98 -595.98 -595.98] [0.0000], Avg: [-563.355 -563.355 -563.355] (1.000)
Step: 1549, Reward: [-548.659 -548.659 -548.659] [0.0000], Avg: [-562.881 -562.881 -562.881] (1.000)
Step: 1599, Reward: [-482.219 -482.219 -482.219] [0.0000], Avg: [-560.361 -560.361 -560.361] (1.000)
Step: 1649, Reward: [-402.993 -402.993 -402.993] [0.0000], Avg: [-555.592 -555.592 -555.592] (1.000)
Step: 1699, Reward: [-549.495 -549.495 -549.495] [0.0000], Avg: [-555.413 -555.413 -555.413] (1.000)
Step: 1749, Reward: [-933.751 -933.751 -933.751] [0.0000], Avg: [-566.222 -566.222 -566.222] (1.000)
Step: 1799, Reward: [-505.316 -505.316 -505.316] [0.0000], Avg: [-564.53 -564.53 -564.53] (1.000)
Step: 1849, Reward: [-674.707 -674.707 -674.707] [0.0000], Avg: [-567.508 -567.508 -567.508] (1.000)
Step: 1899, Reward: [-638.859 -638.859 -638.859] [0.0000], Avg: [-569.386 -569.386 -569.386] (1.000)
Step: 1949, Reward: [-588.35 -588.35 -588.35] [0.0000], Avg: [-569.872 -569.872 -569.872] (1.000)
Step: 1999, Reward: [-518.156 -518.156 -518.156] [0.0000], Avg: [-568.579 -568.579 -568.579] (1.000)
Step: 2049, Reward: [-675.265 -675.265 -675.265] [0.0000], Avg: [-571.181 -571.181 -571.181] (1.000)
Step: 2099, Reward: [-554.529 -554.529 -554.529] [0.0000], Avg: [-570.785 -570.785 -570.785] (1.000)
Step: 2149, Reward: [-440.925 -440.925 -440.925] [0.0000], Avg: [-567.765 -567.765 -567.765] (1.000)
Step: 2199, Reward: [-610.705 -610.705 -610.705] [0.0000], Avg: [-568.741 -568.741 -568.741] (1.000)
Step: 2249, Reward: [-647.804 -647.804 -647.804] [0.0000], Avg: [-570.498 -570.498 -570.498] (1.000)
Step: 2299, Reward: [-522.827 -522.827 -522.827] [0.0000], Avg: [-569.461 -569.461 -569.461] (1.000)
Step: 2349, Reward: [-634.191 -634.191 -634.191] [0.0000], Avg: [-570.839 -570.839 -570.839] (1.000)
Step: 2399, Reward: [-531.468 -531.468 -531.468] [0.0000], Avg: [-570.018 -570.018 -570.018] (1.000)
Step: 2449, Reward: [-734.249 -734.249 -734.249] [0.0000], Avg: [-573.37 -573.37 -573.37] (1.000)
Step: 2499, Reward: [-524.385 -524.385 -524.385] [0.0000], Avg: [-572.39 -572.39 -572.39] (1.000)
Step: 2549, Reward: [-501.444 -501.444 -501.444] [0.0000], Avg: [-570.999 -570.999 -570.999] (1.000)
Step: 2599, Reward: [-678.31 -678.31 -678.31] [0.0000], Avg: [-573.063 -573.063 -573.063] (1.000)
Step: 2649, Reward: [-593.081 -593.081 -593.081] [0.0000], Avg: [-573.441 -573.441 -573.441] (1.000)
Step: 2699, Reward: [-464.342 -464.342 -464.342] [0.0000], Avg: [-571.42 -571.42 -571.42] (1.000)
Step: 2749, Reward: [-528.156 -528.156 -528.156] [0.0000], Avg: [-570.634 -570.634 -570.634] (1.000)
Step: 2799, Reward: [-853.089 -853.089 -853.089] [0.0000], Avg: [-575.677 -575.677 -575.677] (1.000)
Step: 2849, Reward: [-862.063 -862.063 -862.063] [0.0000], Avg: [-580.702 -580.702 -580.702] (1.000)
Step: 2899, Reward: [-570.022 -570.022 -570.022] [0.0000], Avg: [-580.518 -580.518 -580.518] (1.000)
Step: 2949, Reward: [-580.402 -580.402 -580.402] [0.0000], Avg: [-580.516 -580.516 -580.516] (1.000)
Step: 2999, Reward: [-493.463 -493.463 -493.463] [0.0000], Avg: [-579.065 -579.065 -579.065] (1.000)
Step: 3049, Reward: [-494.049 -494.049 -494.049] [0.0000], Avg: [-577.671 -577.671 -577.671] (1.000)
Step: 3099, Reward: [-570.625 -570.625 -570.625] [0.0000], Avg: [-577.557 -577.557 -577.557] (1.000)
Step: 3149, Reward: [-656.084 -656.084 -656.084] [0.0000], Avg: [-578.804 -578.804 -578.804] (1.000)
Step: 3199, Reward: [-478.899 -478.899 -478.899] [0.0000], Avg: [-577.243 -577.243 -577.243] (1.000)
Step: 3249, Reward: [-685.312 -685.312 -685.312] [0.0000], Avg: [-578.905 -578.905 -578.905] (1.000)
Step: 3299, Reward: [-584.567 -584.567 -584.567] [0.0000], Avg: [-578.991 -578.991 -578.991] (1.000)
Step: 3349, Reward: [-712.209 -712.209 -712.209] [0.0000], Avg: [-580.98 -580.98 -580.98] (1.000)
Step: 3399, Reward: [-821.788 -821.788 -821.788] [0.0000], Avg: [-584.521 -584.521 -584.521] (1.000)
Step: 3449, Reward: [-622.109 -622.109 -622.109] [0.0000], Avg: [-585.066 -585.066 -585.066] (1.000)
Step: 3499, Reward: [-476.949 -476.949 -476.949] [0.0000], Avg: [-583.521 -583.521 -583.521] (1.000)
Step: 3549, Reward: [-855.755 -855.755 -855.755] [0.0000], Avg: [-587.355 -587.355 -587.355] (1.000)
Step: 3599, Reward: [-601.578 -601.578 -601.578] [0.0000], Avg: [-587.553 -587.553 -587.553] (1.000)
Step: 3649, Reward: [-669.38 -669.38 -669.38] [0.0000], Avg: [-588.674 -588.674 -588.674] (1.000)
Step: 3699, Reward: [-495.395 -495.395 -495.395] [0.0000], Avg: [-587.413 -587.413 -587.413] (1.000)
Step: 3749, Reward: [-644.866 -644.866 -644.866] [0.0000], Avg: [-588.179 -588.179 -588.179] (1.000)
Step: 3799, Reward: [-609.188 -609.188 -609.188] [0.0000], Avg: [-588.456 -588.456 -588.456] (1.000)
Step: 3849, Reward: [-658.784 -658.784 -658.784] [0.0000], Avg: [-589.369 -589.369 -589.369] (1.000)
Step: 3899, Reward: [-386.324 -386.324 -386.324] [0.0000], Avg: [-586.766 -586.766 -586.766] (1.000)
Step: 3949, Reward: [-535.441 -535.441 -535.441] [0.0000], Avg: [-586.116 -586.116 -586.116] (1.000)
Step: 3999, Reward: [-586.323 -586.323 -586.323] [0.0000], Avg: [-586.119 -586.119 -586.119] (1.000)
Step: 4049, Reward: [-712.286 -712.286 -712.286] [0.0000], Avg: [-587.676 -587.676 -587.676] (1.000)
Step: 4099, Reward: [-437.687 -437.687 -437.687] [0.0000], Avg: [-585.847 -585.847 -585.847] (1.000)
Step: 4149, Reward: [-657.131 -657.131 -657.131] [0.0000], Avg: [-586.706 -586.706 -586.706] (1.000)
Step: 4199, Reward: [-421.67 -421.67 -421.67] [0.0000], Avg: [-584.741 -584.741 -584.741] (1.000)
Step: 4249, Reward: [-438.848 -438.848 -438.848] [0.0000], Avg: [-583.025 -583.025 -583.025] (1.000)
Step: 4299, Reward: [-631.882 -631.882 -631.882] [0.0000], Avg: [-583.593 -583.593 -583.593] (1.000)
Step: 4349, Reward: [-426.378 -426.378 -426.378] [0.0000], Avg: [-581.786 -581.786 -581.786] (1.000)
Step: 4399, Reward: [-479.253 -479.253 -479.253] [0.0000], Avg: [-580.621 -580.621 -580.621] (1.000)
Step: 4449, Reward: [-736.904 -736.904 -736.904] [0.0000], Avg: [-582.377 -582.377 -582.377] (1.000)
Step: 4499, Reward: [-621.451 -621.451 -621.451] [0.0000], Avg: [-582.811 -582.811 -582.811] (1.000)
Step: 4549, Reward: [-813.247 -813.247 -813.247] [0.0000], Avg: [-585.343 -585.343 -585.343] (1.000)
Step: 4599, Reward: [-451.069 -451.069 -451.069] [0.0000], Avg: [-583.884 -583.884 -583.884] (1.000)
Step: 4649, Reward: [-777.551 -777.551 -777.551] [0.0000], Avg: [-585.966 -585.966 -585.966] (1.000)
Step: 4699, Reward: [-774.666 -774.666 -774.666] [0.0000], Avg: [-587.974 -587.974 -587.974] (1.000)
Step: 4749, Reward: [-675.259 -675.259 -675.259] [0.0000], Avg: [-588.893 -588.893 -588.893] (1.000)
Step: 4799, Reward: [-526.065 -526.065 -526.065] [0.0000], Avg: [-588.238 -588.238 -588.238] (1.000)
Step: 4849, Reward: [-550.978 -550.978 -550.978] [0.0000], Avg: [-587.854 -587.854 -587.854] (1.000)
Step: 4899, Reward: [-679.008 -679.008 -679.008] [0.0000], Avg: [-588.784 -588.784 -588.784] (1.000)
Step: 4949, Reward: [-478.187 -478.187 -478.187] [0.0000], Avg: [-587.667 -587.667 -587.667] (1.000)
Step: 4999, Reward: [-779.078 -779.078 -779.078] [0.0000], Avg: [-589.581 -589.581 -589.581] (1.000)
Step: 5049, Reward: [-599.249 -599.249 -599.249] [0.0000], Avg: [-589.677 -589.677 -589.677] (1.000)
Step: 5099, Reward: [-622.614 -622.614 -622.614] [0.0000], Avg: [-590. -590. -590.] (1.000)
Step: 5149, Reward: [-358.311 -358.311 -358.311] [0.0000], Avg: [-587.75 -587.75 -587.75] (1.000)
Step: 5199, Reward: [-551.276 -551.276 -551.276] [0.0000], Avg: [-587.4 -587.4 -587.4] (1.000)
Step: 5249, Reward: [-548.373 -548.373 -548.373] [0.0000], Avg: [-587.028 -587.028 -587.028] (1.000)
Step: 5299, Reward: [-668.892 -668.892 -668.892] [0.0000], Avg: [-587.8 -587.8 -587.8] (1.000)
Step: 5349, Reward: [-654.864 -654.864 -654.864] [0.0000], Avg: [-588.427 -588.427 -588.427] (1.000)
Step: 5399, Reward: [-421.244 -421.244 -421.244] [0.0000], Avg: [-586.879 -586.879 -586.879] (1.000)
Step: 5449, Reward: [-470.489 -470.489 -470.489] [0.0000], Avg: [-585.811 -585.811 -585.811] (1.000)
Step: 5499, Reward: [-568.197 -568.197 -568.197] [0.0000], Avg: [-585.651 -585.651 -585.651] (1.000)
Step: 5549, Reward: [-718.959 -718.959 -718.959] [0.0000], Avg: [-586.852 -586.852 -586.852] (1.000)
Step: 5599, Reward: [-531.918 -531.918 -531.918] [0.0000], Avg: [-586.362 -586.362 -586.362] (1.000)
Step: 5649, Reward: [-538.345 -538.345 -538.345] [0.0000], Avg: [-585.937 -585.937 -585.937] (1.000)
Step: 5699, Reward: [-919.345 -919.345 -919.345] [0.0000], Avg: [-588.861 -588.861 -588.861] (1.000)
Step: 5749, Reward: [-633.116 -633.116 -633.116] [0.0000], Avg: [-589.246 -589.246 -589.246] (1.000)
Step: 5799, Reward: [-415.279 -415.279 -415.279] [0.0000], Avg: [-587.746 -587.746 -587.746] (1.000)
Step: 5849, Reward: [-406.967 -406.967 -406.967] [0.0000], Avg: [-586.201 -586.201 -586.201] (1.000)
Step: 5899, Reward: [-651.341 -651.341 -651.341] [0.0000], Avg: [-586.753 -586.753 -586.753] (1.000)
Step: 5949, Reward: [-361.468 -361.468 -361.468] [0.0000], Avg: [-584.86 -584.86 -584.86] (1.000)
Step: 5999, Reward: [-663.985 -663.985 -663.985] [0.0000], Avg: [-585.519 -585.519 -585.519] (1.000)
Step: 6049, Reward: [-608.973 -608.973 -608.973] [0.0000], Avg: [-585.713 -585.713 -585.713] (1.000)
Step: 6099, Reward: [-778.688 -778.688 -778.688] [0.0000], Avg: [-587.295 -587.295 -587.295] (1.000)
Step: 6149, Reward: [-514.762 -514.762 -514.762] [0.0000], Avg: [-586.705 -586.705 -586.705] (1.000)
Step: 6199, Reward: [-761.351 -761.351 -761.351] [0.0000], Avg: [-588.114 -588.114 -588.114] (1.000)
Step: 6249, Reward: [-515.298 -515.298 -515.298] [0.0000], Avg: [-587.531 -587.531 -587.531] (1.000)
Step: 6299, Reward: [-617.111 -617.111 -617.111] [0.0000], Avg: [-587.766 -587.766 -587.766] (1.000)
Step: 6349, Reward: [-614.135 -614.135 -614.135] [0.0000], Avg: [-587.974 -587.974 -587.974] (1.000)
Step: 6399, Reward: [-641.346 -641.346 -641.346] [0.0000], Avg: [-588.391 -588.391 -588.391] (1.000)
Step: 6449, Reward: [-540.814 -540.814 -540.814] [0.0000], Avg: [-588.022 -588.022 -588.022] (1.000)
Step: 6499, Reward: [-828.765 -828.765 -828.765] [0.0000], Avg: [-589.874 -589.874 -589.874] (1.000)
Step: 6549, Reward: [-615.223 -615.223 -615.223] [0.0000], Avg: [-590.067 -590.067 -590.067] (1.000)
Step: 6599, Reward: [-663.777 -663.777 -663.777] [0.0000], Avg: [-590.626 -590.626 -590.626] (1.000)
Step: 6649, Reward: [-648.038 -648.038 -648.038] [0.0000], Avg: [-591.057 -591.057 -591.057] (1.000)
Step: 6699, Reward: [-444. -444. -444.] [0.0000], Avg: [-589.96 -589.96 -589.96] (1.000)
Step: 6749, Reward: [-533.201 -533.201 -533.201] [0.0000], Avg: [-589.539 -589.539 -589.539] (1.000)
Step: 6799, Reward: [-615.146 -615.146 -615.146] [0.0000], Avg: [-589.728 -589.728 -589.728] (1.000)
Step: 6849, Reward: [-496.479 -496.479 -496.479] [0.0000], Avg: [-589.047 -589.047 -589.047] (1.000)
Step: 6899, Reward: [-620.155 -620.155 -620.155] [0.0000], Avg: [-589.272 -589.272 -589.272] (1.000)
Step: 6949, Reward: [-761.953 -761.953 -761.953] [0.0000], Avg: [-590.515 -590.515 -590.515] (1.000)
Step: 6999, Reward: [-587.054 -587.054 -587.054] [0.0000], Avg: [-590.49 -590.49 -590.49] (1.000)
Step: 7049, Reward: [-629.854 -629.854 -629.854] [0.0000], Avg: [-590.769 -590.769 -590.769] (1.000)
Step: 7099, Reward: [-635.716 -635.716 -635.716] [0.0000], Avg: [-591.086 -591.086 -591.086] (1.000)
Step: 7149, Reward: [-635.215 -635.215 -635.215] [0.0000], Avg: [-591.394 -591.394 -591.394] (1.000)
Step: 7199, Reward: [-427.174 -427.174 -427.174] [0.0000], Avg: [-590.254 -590.254 -590.254] (1.000)
Step: 7249, Reward: [-935.581 -935.581 -935.581] [0.0000], Avg: [-592.636 -592.636 -592.636] (1.000)
Step: 7299, Reward: [-526.471 -526.471 -526.471] [0.0000], Avg: [-592.182 -592.182 -592.182] (1.000)
Step: 7349, Reward: [-586.136 -586.136 -586.136] [0.0000], Avg: [-592.141 -592.141 -592.141] (1.000)
Step: 7399, Reward: [-636.286 -636.286 -636.286] [0.0000], Avg: [-592.439 -592.439 -592.439] (1.000)
Step: 7449, Reward: [-576.27 -576.27 -576.27] [0.0000], Avg: [-592.331 -592.331 -592.331] (1.000)
Step: 7499, Reward: [-705.524 -705.524 -705.524] [0.0000], Avg: [-593.086 -593.086 -593.086] (1.000)
Step: 7549, Reward: [-602.878 -602.878 -602.878] [0.0000], Avg: [-593.15 -593.15 -593.15] (1.000)
Step: 7599, Reward: [-473.89 -473.89 -473.89] [0.0000], Avg: [-592.366 -592.366 -592.366] (1.000)
Step: 7649, Reward: [-477.449 -477.449 -477.449] [0.0000], Avg: [-591.615 -591.615 -591.615] (1.000)
Step: 7699, Reward: [-682.559 -682.559 -682.559] [0.0000], Avg: [-592.205 -592.205 -592.205] (1.000)
Step: 7749, Reward: [-500.168 -500.168 -500.168] [0.0000], Avg: [-591.611 -591.611 -591.611] (1.000)
Step: 7799, Reward: [-739.42 -739.42 -739.42] [0.0000], Avg: [-592.559 -592.559 -592.559] (1.000)
Step: 7849, Reward: [-532.304 -532.304 -532.304] [0.0000], Avg: [-592.175 -592.175 -592.175] (1.000)
Step: 7899, Reward: [-510.885 -510.885 -510.885] [0.0000], Avg: [-591.661 -591.661 -591.661] (1.000)
Step: 7949, Reward: [-468.619 -468.619 -468.619] [0.0000], Avg: [-590.887 -590.887 -590.887] (1.000)
Step: 7999, Reward: [-897.441 -897.441 -897.441] [0.0000], Avg: [-592.803 -592.803 -592.803] (1.000)
Step: 8049, Reward: [-466.871 -466.871 -466.871] [0.0000], Avg: [-592.021 -592.021 -592.021] (1.000)
Step: 8099, Reward: [-441.647 -441.647 -441.647] [0.0000], Avg: [-591.092 -591.092 -591.092] (1.000)
Step: 8149, Reward: [-492.379 -492.379 -492.379] [0.0000], Avg: [-590.487 -590.487 -590.487] (1.000)
Step: 8199, Reward: [-456.848 -456.848 -456.848] [0.0000], Avg: [-589.672 -589.672 -589.672] (1.000)
Step: 8249, Reward: [-737.552 -737.552 -737.552] [0.0000], Avg: [-590.568 -590.568 -590.568] (1.000)
Step: 8299, Reward: [-707.675 -707.675 -707.675] [0.0000], Avg: [-591.274 -591.274 -591.274] (1.000)
Step: 8349, Reward: [-330.457 -330.457 -330.457] [0.0000], Avg: [-589.712 -589.712 -589.712] (1.000)
Step: 8399, Reward: [-451.264 -451.264 -451.264] [0.0000], Avg: [-588.888 -588.888 -588.888] (1.000)
Step: 8449, Reward: [-651.012 -651.012 -651.012] [0.0000], Avg: [-589.255 -589.255 -589.255] (1.000)
Step: 8499, Reward: [-573.391 -573.391 -573.391] [0.0000], Avg: [-589.162 -589.162 -589.162] (1.000)
Step: 8549, Reward: [-388.985 -388.985 -388.985] [0.0000], Avg: [-587.991 -587.991 -587.991] (1.000)
Step: 8599, Reward: [-648.902 -648.902 -648.902] [0.0000], Avg: [-588.346 -588.346 -588.346] (1.000)
Step: 8649, Reward: [-756.627 -756.627 -756.627] [0.0000], Avg: [-589.318 -589.318 -589.318] (1.000)
Step: 8699, Reward: [-718.268 -718.268 -718.268] [0.0000], Avg: [-590.059 -590.059 -590.059] (1.000)
Step: 8749, Reward: [-467.793 -467.793 -467.793] [0.0000], Avg: [-589.361 -589.361 -589.361] (1.000)
Step: 8799, Reward: [-511.566 -511.566 -511.566] [0.0000], Avg: [-588.919 -588.919 -588.919] (1.000)
Step: 8849, Reward: [-432.718 -432.718 -432.718] [0.0000], Avg: [-588.036 -588.036 -588.036] (1.000)
Step: 8899, Reward: [-451.842 -451.842 -451.842] [0.0000], Avg: [-587.271 -587.271 -587.271] (1.000)
Step: 8949, Reward: [-701.048 -701.048 -701.048] [0.0000], Avg: [-587.907 -587.907 -587.907] (1.000)
Step: 8999, Reward: [-459.93 -459.93 -459.93] [0.0000], Avg: [-587.196 -587.196 -587.196] (1.000)
Step: 9049, Reward: [-492.726 -492.726 -492.726] [0.0000], Avg: [-586.674 -586.674 -586.674] (1.000)
Step: 9099, Reward: [-816.952 -816.952 -816.952] [0.0000], Avg: [-587.939 -587.939 -587.939] (1.000)
Step: 9149, Reward: [-528.066 -528.066 -528.066] [0.0000], Avg: [-587.612 -587.612 -587.612] (1.000)
Step: 9199, Reward: [-551.933 -551.933 -551.933] [0.0000], Avg: [-587.418 -587.418 -587.418] (1.000)
Step: 9249, Reward: [-644.284 -644.284 -644.284] [0.0000], Avg: [-587.725 -587.725 -587.725] (1.000)
Step: 9299, Reward: [-915.354 -915.354 -915.354] [0.0000], Avg: [-589.487 -589.487 -589.487] (1.000)
Step: 9349, Reward: [-627.335 -627.335 -627.335] [0.0000], Avg: [-589.689 -589.689 -589.689] (1.000)
Step: 9399, Reward: [-621.986 -621.986 -621.986] [0.0000], Avg: [-589.861 -589.861 -589.861] (1.000)
Step: 9449, Reward: [-533.437 -533.437 -533.437] [0.0000], Avg: [-589.562 -589.562 -589.562] (1.000)
Step: 9499, Reward: [-656.132 -656.132 -656.132] [0.0000], Avg: [-589.913 -589.913 -589.913] (1.000)
Step: 9549, Reward: [-306.482 -306.482 -306.482] [0.0000], Avg: [-588.429 -588.429 -588.429] (1.000)
Step: 9599, Reward: [-646.374 -646.374 -646.374] [0.0000], Avg: [-588.731 -588.731 -588.731] (1.000)
Step: 9649, Reward: [-340.395 -340.395 -340.395] [0.0000], Avg: [-587.444 -587.444 -587.444] (1.000)
Step: 9699, Reward: [-550.862 -550.862 -550.862] [0.0000], Avg: [-587.255 -587.255 -587.255] (1.000)
Step: 9749, Reward: [-476.401 -476.401 -476.401] [0.0000], Avg: [-586.687 -586.687 -586.687] (1.000)
Step: 9799, Reward: [-449.721 -449.721 -449.721] [0.0000], Avg: [-585.988 -585.988 -585.988] (1.000)
Step: 9849, Reward: [-517.921 -517.921 -517.921] [0.0000], Avg: [-585.643 -585.643 -585.643] (1.000)
Step: 9899, Reward: [-445.433 -445.433 -445.433] [0.0000], Avg: [-584.934 -584.934 -584.934] (1.000)
Step: 9949, Reward: [-764.013 -764.013 -764.013] [0.0000], Avg: [-585.834 -585.834 -585.834] (1.000)
Step: 9999, Reward: [-564.041 -564.041 -564.041] [0.0000], Avg: [-585.725 -585.725 -585.725] (1.000)
Step: 10049, Reward: [-562.881 -562.881 -562.881] [0.0000], Avg: [-585.612 -585.612 -585.612] (1.000)
Step: 10099, Reward: [-337.663 -337.663 -337.663] [0.0000], Avg: [-584.384 -584.384 -584.384] (1.000)
Step: 10149, Reward: [-649.154 -649.154 -649.154] [0.0000], Avg: [-584.703 -584.703 -584.703] (1.000)
Step: 10199, Reward: [-489.68 -489.68 -489.68] [0.0000], Avg: [-584.237 -584.237 -584.237] (1.000)
Step: 10249, Reward: [-592.689 -592.689 -592.689] [0.0000], Avg: [-584.279 -584.279 -584.279] (1.000)
Step: 10299, Reward: [-497.196 -497.196 -497.196] [0.0000], Avg: [-583.856 -583.856 -583.856] (1.000)
Step: 10349, Reward: [-613.905 -613.905 -613.905] [0.0000], Avg: [-584.001 -584.001 -584.001] (1.000)
Step: 10399, Reward: [-571.88 -571.88 -571.88] [0.0000], Avg: [-583.943 -583.943 -583.943] (1.000)
Step: 10449, Reward: [-395.901 -395.901 -395.901] [0.0000], Avg: [-583.043 -583.043 -583.043] (1.000)
Step: 10499, Reward: [-1092.45 -1092.45 -1092.45] [0.0000], Avg: [-585.469 -585.469 -585.469] (1.000)
Step: 10549, Reward: [-678.672 -678.672 -678.672] [0.0000], Avg: [-585.911 -585.911 -585.911] (1.000)
Step: 10599, Reward: [-614.335 -614.335 -614.335] [0.0000], Avg: [-586.045 -586.045 -586.045] (1.000)
Step: 10649, Reward: [-629.701 -629.701 -629.701] [0.0000], Avg: [-586.25 -586.25 -586.25] (1.000)
Step: 10699, Reward: [-634.505 -634.505 -634.505] [0.0000], Avg: [-586.475 -586.475 -586.475] (1.000)
Step: 10749, Reward: [-720.667 -720.667 -720.667] [0.0000], Avg: [-587.099 -587.099 -587.099] (1.000)
Step: 10799, Reward: [-646.486 -646.486 -646.486] [0.0000], Avg: [-587.374 -587.374 -587.374] (1.000)
Step: 10849, Reward: [-442.203 -442.203 -442.203] [0.0000], Avg: [-586.705 -586.705 -586.705] (1.000)
Step: 10899, Reward: [-366.506 -366.506 -366.506] [0.0000], Avg: [-585.695 -585.695 -585.695] (1.000)
Step: 10949, Reward: [-538.227 -538.227 -538.227] [0.0000], Avg: [-585.478 -585.478 -585.478] (1.000)
Step: 10999, Reward: [-750.17 -750.17 -750.17] [0.0000], Avg: [-586.227 -586.227 -586.227] (1.000)
Step: 11049, Reward: [-638.469 -638.469 -638.469] [0.0000], Avg: [-586.463 -586.463 -586.463] (1.000)
Step: 11099, Reward: [-441.825 -441.825 -441.825] [0.0000], Avg: [-585.812 -585.812 -585.812] (1.000)
Step: 11149, Reward: [-497.874 -497.874 -497.874] [0.0000], Avg: [-585.418 -585.418 -585.418] (1.000)
Step: 11199, Reward: [-570.91 -570.91 -570.91] [0.0000], Avg: [-585.353 -585.353 -585.353] (1.000)
Step: 11249, Reward: [-836.731 -836.731 -836.731] [0.0000], Avg: [-586.47 -586.47 -586.47] (1.000)
Step: 11299, Reward: [-497.82 -497.82 -497.82] [0.0000], Avg: [-586.078 -586.078 -586.078] (1.000)
Step: 11349, Reward: [-478.103 -478.103 -478.103] [0.0000], Avg: [-585.602 -585.602 -585.602] (1.000)
Step: 11399, Reward: [-776.694 -776.694 -776.694] [0.0000], Avg: [-586.44 -586.44 -586.44] (1.000)
Step: 11449, Reward: [-553.028 -553.028 -553.028] [0.0000], Avg: [-586.294 -586.294 -586.294] (1.000)
Step: 11499, Reward: [-586.295 -586.295 -586.295] [0.0000], Avg: [-586.294 -586.294 -586.294] (1.000)
Step: 11549, Reward: [-648.091 -648.091 -648.091] [0.0000], Avg: [-586.562 -586.562 -586.562] (1.000)
Step: 11599, Reward: [-484.298 -484.298 -484.298] [0.0000], Avg: [-586.121 -586.121 -586.121] (1.000)
Step: 11649, Reward: [-658.251 -658.251 -658.251] [0.0000], Avg: [-586.431 -586.431 -586.431] (1.000)
Step: 11699, Reward: [-605.367 -605.367 -605.367] [0.0000], Avg: [-586.512 -586.512 -586.512] (1.000)
Step: 11749, Reward: [-494.792 -494.792 -494.792] [0.0000], Avg: [-586.121 -586.121 -586.121] (1.000)
Step: 11799, Reward: [-426.291 -426.291 -426.291] [0.0000], Avg: [-585.444 -585.444 -585.444] (1.000)
Step: 11849, Reward: [-782.443 -782.443 -782.443] [0.0000], Avg: [-586.275 -586.275 -586.275] (1.000)
Step: 11899, Reward: [-612.628 -612.628 -612.628] [0.0000], Avg: [-586.386 -586.386 -586.386] (1.000)
Step: 11949, Reward: [-434.147 -434.147 -434.147] [0.0000], Avg: [-585.749 -585.749 -585.749] (1.000)
Step: 11999, Reward: [-550.916 -550.916 -550.916] [0.0000], Avg: [-585.604 -585.604 -585.604] (1.000)
Step: 12049, Reward: [-632.278 -632.278 -632.278] [0.0000], Avg: [-585.797 -585.797 -585.797] (1.000)
Step: 12099, Reward: [-644.036 -644.036 -644.036] [0.0000], Avg: [-586.038 -586.038 -586.038] (1.000)
Step: 12149, Reward: [-559.979 -559.979 -559.979] [0.0000], Avg: [-585.931 -585.931 -585.931] (1.000)
Step: 12199, Reward: [-549.844 -549.844 -549.844] [0.0000], Avg: [-585.783 -585.783 -585.783] (1.000)
Step: 12249, Reward: [-562.252 -562.252 -562.252] [0.0000], Avg: [-585.687 -585.687 -585.687] (1.000)
Step: 12299, Reward: [-399.087 -399.087 -399.087] [0.0000], Avg: [-584.928 -584.928 -584.928] (1.000)
Step: 12349, Reward: [-476.512 -476.512 -476.512] [0.0000], Avg: [-584.489 -584.489 -584.489] (1.000)
Step: 12399, Reward: [-745.455 -745.455 -745.455] [0.0000], Avg: [-585.139 -585.139 -585.139] (1.000)
Step: 12449, Reward: [-658.229 -658.229 -658.229] [0.0000], Avg: [-585.432 -585.432 -585.432] (1.000)
Step: 12499, Reward: [-445.784 -445.784 -445.784] [0.0000], Avg: [-584.873 -584.873 -584.873] (1.000)
Step: 12549, Reward: [-450.953 -450.953 -450.953] [0.0000], Avg: [-584.34 -584.34 -584.34] (1.000)
Step: 12599, Reward: [-520.718 -520.718 -520.718] [0.0000], Avg: [-584.087 -584.087 -584.087] (1.000)
Step: 12649, Reward: [-518.122 -518.122 -518.122] [0.0000], Avg: [-583.827 -583.827 -583.827] (1.000)
Step: 12699, Reward: [-691.747 -691.747 -691.747] [0.0000], Avg: [-584.252 -584.252 -584.252] (1.000)
Step: 12749, Reward: [-665.484 -665.484 -665.484] [0.0000], Avg: [-584.57 -584.57 -584.57] (1.000)
Step: 12799, Reward: [-880.622 -880.622 -880.622] [0.0000], Avg: [-585.727 -585.727 -585.727] (1.000)
Step: 12849, Reward: [-639.893 -639.893 -639.893] [0.0000], Avg: [-585.937 -585.937 -585.937] (1.000)
Step: 12899, Reward: [-482.253 -482.253 -482.253] [0.0000], Avg: [-585.536 -585.536 -585.536] (1.000)
Step: 12949, Reward: [-596.634 -596.634 -596.634] [0.0000], Avg: [-585.578 -585.578 -585.578] (1.000)
Step: 12999, Reward: [-455.997 -455.997 -455.997] [0.0000], Avg: [-585.08 -585.08 -585.08] (1.000)
Step: 13049, Reward: [-782.052 -782.052 -782.052] [0.0000], Avg: [-585.835 -585.835 -585.835] (1.000)
Step: 13099, Reward: [-640.609 -640.609 -640.609] [0.0000], Avg: [-586.044 -586.044 -586.044] (1.000)
Step: 13149, Reward: [-784.759 -784.759 -784.759] [0.0000], Avg: [-586.799 -586.799 -586.799] (1.000)
Step: 13199, Reward: [-882.191 -882.191 -882.191] [0.0000], Avg: [-587.918 -587.918 -587.918] (1.000)
Step: 13249, Reward: [-1538.66 -1538.66 -1538.66] [0.0000], Avg: [-591.506 -591.506 -591.506] (1.000)
Step: 13299, Reward: [-1748.63 -1748.63 -1748.63] [0.0000], Avg: [-595.856 -595.856 -595.856] (1.000)
Step: 13349, Reward: [-1872.723 -1872.723 -1872.723] [0.0000], Avg: [-600.638 -600.638 -600.638] (1.000)
Step: 13399, Reward: [-1364.335 -1364.335 -1364.335] [0.0000], Avg: [-603.488 -603.488 -603.488] (1.000)
Step: 13449, Reward: [-1446.037 -1446.037 -1446.037] [0.0000], Avg: [-606.62 -606.62 -606.62] (1.000)
Step: 13499, Reward: [-2433.593 -2433.593 -2433.593] [0.0000], Avg: [-613.387 -613.387 -613.387] (1.000)
Step: 13549, Reward: [-1398.752 -1398.752 -1398.752] [0.0000], Avg: [-616.285 -616.285 -616.285] (1.000)
Step: 13599, Reward: [-1481.984 -1481.984 -1481.984] [0.0000], Avg: [-619.467 -619.467 -619.467] (1.000)
Step: 13649, Reward: [-1354.815 -1354.815 -1354.815] [0.0000], Avg: [-622.161 -622.161 -622.161] (1.000)
Step: 13699, Reward: [-1355.035 -1355.035 -1355.035] [0.0000], Avg: [-624.836 -624.836 -624.836] (1.000)
Step: 13749, Reward: [-966.026 -966.026 -966.026] [0.0000], Avg: [-626.076 -626.076 -626.076] (1.000)
Step: 13799, Reward: [-1191.185 -1191.185 -1191.185] [0.0000], Avg: [-628.124 -628.124 -628.124] (1.000)
Step: 13849, Reward: [-622.614 -622.614 -622.614] [0.0000], Avg: [-628.104 -628.104 -628.104] (1.000)
Step: 13899, Reward: [-740.075 -740.075 -740.075] [0.0000], Avg: [-628.507 -628.507 -628.507] (1.000)
Step: 13949, Reward: [-416.506 -416.506 -416.506] [0.0000], Avg: [-627.747 -627.747 -627.747] (1.000)
Step: 13999, Reward: [-813.609 -813.609 -813.609] [0.0000], Avg: [-628.411 -628.411 -628.411] (1.000)
Step: 14049, Reward: [-632.625 -632.625 -632.625] [0.0000], Avg: [-628.426 -628.426 -628.426] (1.000)
Step: 14099, Reward: [-562.739 -562.739 -562.739] [0.0000], Avg: [-628.193 -628.193 -628.193] (1.000)
Step: 14149, Reward: [-621.378 -621.378 -621.378] [0.0000], Avg: [-628.169 -628.169 -628.169] (1.000)
Step: 14199, Reward: [-554.402 -554.402 -554.402] [0.0000], Avg: [-627.909 -627.909 -627.909] (1.000)
Step: 14249, Reward: [-675.424 -675.424 -675.424] [0.0000], Avg: [-628.076 -628.076 -628.076] (1.000)
Step: 14299, Reward: [-475.931 -475.931 -475.931] [0.0000], Avg: [-627.544 -627.544 -627.544] (1.000)
Step: 14349, Reward: [-665.185 -665.185 -665.185] [0.0000], Avg: [-627.675 -627.675 -627.675] (1.000)
Step: 14399, Reward: [-607.243 -607.243 -607.243] [0.0000], Avg: [-627.604 -627.604 -627.604] (1.000)
Step: 14449, Reward: [-761.395 -761.395 -761.395] [0.0000], Avg: [-628.067 -628.067 -628.067] (1.000)
Step: 14499, Reward: [-512.55 -512.55 -512.55] [0.0000], Avg: [-627.668 -627.668 -627.668] (1.000)
Step: 14549, Reward: [-654.717 -654.717 -654.717] [0.0000], Avg: [-627.761 -627.761 -627.761] (1.000)
Step: 14599, Reward: [-1010.596 -1010.596 -1010.596] [0.0000], Avg: [-629.072 -629.072 -629.072] (1.000)
Step: 14649, Reward: [-587.414 -587.414 -587.414] [0.0000], Avg: [-628.93 -628.93 -628.93] (1.000)
Step: 14699, Reward: [-813.859 -813.859 -813.859] [0.0000], Avg: [-629.559 -629.559 -629.559] (1.000)
Step: 14749, Reward: [-580.66 -580.66 -580.66] [0.0000], Avg: [-629.394 -629.394 -629.394] (1.000)
Step: 14799, Reward: [-509.296 -509.296 -509.296] [0.0000], Avg: [-628.988 -628.988 -628.988] (1.000)
Step: 14849, Reward: [-430.57 -430.57 -430.57] [0.0000], Avg: [-628.32 -628.32 -628.32] (1.000)
Step: 14899, Reward: [-828.063 -828.063 -828.063] [0.0000], Avg: [-628.99 -628.99 -628.99] (1.000)
Step: 14949, Reward: [-454.602 -454.602 -454.602] [0.0000], Avg: [-628.407 -628.407 -628.407] (1.000)
Step: 14999, Reward: [-560.357 -560.357 -560.357] [0.0000], Avg: [-628.18 -628.18 -628.18] (1.000)
Step: 15049, Reward: [-446.151 -446.151 -446.151] [0.0000], Avg: [-627.575 -627.575 -627.575] (1.000)
Step: 15099, Reward: [-558.595 -558.595 -558.595] [0.0000], Avg: [-627.347 -627.347 -627.347] (1.000)
Step: 15149, Reward: [-1366.987 -1366.987 -1366.987] [0.0000], Avg: [-629.788 -629.788 -629.788] (1.000)
Step: 15199, Reward: [-1144.28 -1144.28 -1144.28] [0.0000], Avg: [-631.48 -631.48 -631.48] (1.000)
Step: 15249, Reward: [-1112.853 -1112.853 -1112.853] [0.0000], Avg: [-633.059 -633.059 -633.059] (1.000)
Step: 15299, Reward: [-689.279 -689.279 -689.279] [0.0000], Avg: [-633.242 -633.242 -633.242] (1.000)
Step: 15349, Reward: [-604.577 -604.577 -604.577] [0.0000], Avg: [-633.149 -633.149 -633.149] (1.000)
Step: 15399, Reward: [-435.159 -435.159 -435.159] [0.0000], Avg: [-632.506 -632.506 -632.506] (1.000)
Step: 15449, Reward: [-766.454 -766.454 -766.454] [0.0000], Avg: [-632.94 -632.94 -632.94] (1.000)
Step: 15499, Reward: [-486.861 -486.861 -486.861] [0.0000], Avg: [-632.468 -632.468 -632.468] (1.000)
Step: 15549, Reward: [-625.764 -625.764 -625.764] [0.0000], Avg: [-632.447 -632.447 -632.447] (1.000)
Step: 15599, Reward: [-709.033 -709.033 -709.033] [0.0000], Avg: [-632.692 -632.692 -632.692] (1.000)
Step: 15649, Reward: [-1836.625 -1836.625 -1836.625] [0.0000], Avg: [-636.539 -636.539 -636.539] (1.000)
Step: 15699, Reward: [-1362.505 -1362.505 -1362.505] [0.0000], Avg: [-638.851 -638.851 -638.851] (1.000)
Step: 15749, Reward: [-1552.495 -1552.495 -1552.495] [0.0000], Avg: [-641.751 -641.751 -641.751] (1.000)
Step: 15799, Reward: [-654.942 -654.942 -654.942] [0.0000], Avg: [-641.793 -641.793 -641.793] (1.000)
Step: 15849, Reward: [-1818.155 -1818.155 -1818.155] [0.0000], Avg: [-645.504 -645.504 -645.504] (1.000)
Step: 15899, Reward: [-1847.004 -1847.004 -1847.004] [0.0000], Avg: [-649.282 -649.282 -649.282] (1.000)
Step: 15949, Reward: [-1086.557 -1086.557 -1086.557] [0.0000], Avg: [-650.653 -650.653 -650.653] (1.000)
Step: 15999, Reward: [-1329.201 -1329.201 -1329.201] [0.0000], Avg: [-652.773 -652.773 -652.773] (1.000)
Step: 16049, Reward: [-1689.752 -1689.752 -1689.752] [0.0000], Avg: [-656.004 -656.004 -656.004] (1.000)
Step: 16099, Reward: [-1978.375 -1978.375 -1978.375] [0.0000], Avg: [-660.111 -660.111 -660.111] (1.000)
Step: 16149, Reward: [-1244.956 -1244.956 -1244.956] [0.0000], Avg: [-661.921 -661.921 -661.921] (1.000)
Step: 16199, Reward: [-1833.762 -1833.762 -1833.762] [0.0000], Avg: [-665.538 -665.538 -665.538] (1.000)
Step: 16249, Reward: [-1381.367 -1381.367 -1381.367] [0.0000], Avg: [-667.741 -667.741 -667.741] (1.000)
Step: 16299, Reward: [-1549.052 -1549.052 -1549.052] [0.0000], Avg: [-670.444 -670.444 -670.444] (1.000)
Step: 16349, Reward: [-1845.21 -1845.21 -1845.21] [0.0000], Avg: [-674.037 -674.037 -674.037] (1.000)
Step: 16399, Reward: [-1733.436 -1733.436 -1733.436] [0.0000], Avg: [-677.266 -677.266 -677.266] (1.000)
Step: 16449, Reward: [-2088.188 -2088.188 -2088.188] [0.0000], Avg: [-681.555 -681.555 -681.555] (1.000)
Step: 16499, Reward: [-1723.393 -1723.393 -1723.393] [0.0000], Avg: [-684.712 -684.712 -684.712] (1.000)
Step: 16549, Reward: [-1422.457 -1422.457 -1422.457] [0.0000], Avg: [-686.941 -686.941 -686.941] (1.000)
Step: 16599, Reward: [-2204.346 -2204.346 -2204.346] [0.0000], Avg: [-691.511 -691.511 -691.511] (1.000)
Step: 16649, Reward: [-1515.842 -1515.842 -1515.842] [0.0000], Avg: [-693.987 -693.987 -693.987] (1.000)
Step: 16699, Reward: [-1603.325 -1603.325 -1603.325] [0.0000], Avg: [-696.709 -696.709 -696.709] (1.000)
Step: 16749, Reward: [-1973.824 -1973.824 -1973.824] [0.0000], Avg: [-700.522 -700.522 -700.522] (1.000)
Step: 16799, Reward: [-1918.856 -1918.856 -1918.856] [0.0000], Avg: [-704.148 -704.148 -704.148] (1.000)
Step: 16849, Reward: [-948.774 -948.774 -948.774] [0.0000], Avg: [-704.874 -704.874 -704.874] (1.000)
Step: 16899, Reward: [-1955.228 -1955.228 -1955.228] [0.0000], Avg: [-708.573 -708.573 -708.573] (1.000)
Step: 16949, Reward: [-708.879 -708.879 -708.879] [0.0000], Avg: [-708.574 -708.574 -708.574] (1.000)
Step: 16999, Reward: [-2283.198 -2283.198 -2283.198] [0.0000], Avg: [-713.205 -713.205 -713.205] (1.000)
Step: 17049, Reward: [-692.85 -692.85 -692.85] [0.0000], Avg: [-713.145 -713.145 -713.145] (1.000)
Step: 17099, Reward: [-1662.99 -1662.99 -1662.99] [0.0000], Avg: [-715.923 -715.923 -715.923] (1.000)
Step: 17149, Reward: [-1213.74 -1213.74 -1213.74] [0.0000], Avg: [-717.374 -717.374 -717.374] (1.000)
Step: 17199, Reward: [-1571.544 -1571.544 -1571.544] [0.0000], Avg: [-719.857 -719.857 -719.857] (1.000)
Step: 17249, Reward: [-1734.559 -1734.559 -1734.559] [0.0000], Avg: [-722.798 -722.798 -722.798] (1.000)
Step: 17299, Reward: [-1457.003 -1457.003 -1457.003] [0.0000], Avg: [-724.92 -724.92 -724.92] (1.000)
Step: 17349, Reward: [-1829.68 -1829.68 -1829.68] [0.0000], Avg: [-728.104 -728.104 -728.104] (1.000)
Step: 17399, Reward: [-1745.067 -1745.067 -1745.067] [0.0000], Avg: [-731.026 -731.026 -731.026] (1.000)
Step: 17449, Reward: [-1785.159 -1785.159 -1785.159] [0.0000], Avg: [-734.047 -734.047 -734.047] (1.000)
Step: 17499, Reward: [-1942.439 -1942.439 -1942.439] [0.0000], Avg: [-737.499 -737.499 -737.499] (1.000)
Step: 17549, Reward: [-1860.921 -1860.921 -1860.921] [0.0000], Avg: [-740.7 -740.7 -740.7] (1.000)
Step: 17599, Reward: [-2146.19 -2146.19 -2146.19] [0.0000], Avg: [-744.693 -744.693 -744.693] (1.000)
Step: 17649, Reward: [-1942.313 -1942.313 -1942.313] [0.0000], Avg: [-748.085 -748.085 -748.085] (1.000)
Step: 17699, Reward: [-2176.759 -2176.759 -2176.759] [0.0000], Avg: [-752.121 -752.121 -752.121] (1.000)
Step: 17749, Reward: [-1959.914 -1959.914 -1959.914] [0.0000], Avg: [-755.523 -755.523 -755.523] (1.000)
Step: 17799, Reward: [-1826.591 -1826.591 -1826.591] [0.0000], Avg: [-758.532 -758.532 -758.532] (1.000)
Step: 17849, Reward: [-1275.056 -1275.056 -1275.056] [0.0000], Avg: [-759.979 -759.979 -759.979] (1.000)
Step: 17899, Reward: [-1460.537 -1460.537 -1460.537] [0.0000], Avg: [-761.936 -761.936 -761.936] (1.000)
Step: 17949, Reward: [-934.454 -934.454 -934.454] [0.0000], Avg: [-762.416 -762.416 -762.416] (1.000)
Step: 17999, Reward: [-1158.784 -1158.784 -1158.784] [0.0000], Avg: [-763.517 -763.517 -763.517] (1.000)
Step: 18049, Reward: [-767.432 -767.432 -767.432] [0.0000], Avg: [-763.528 -763.528 -763.528] (1.000)
Step: 18099, Reward: [-796.412 -796.412 -796.412] [0.0000], Avg: [-763.619 -763.619 -763.619] (1.000)
Step: 18149, Reward: [-548.009 -548.009 -548.009] [0.0000], Avg: [-763.025 -763.025 -763.025] (1.000)
Step: 18199, Reward: [-439.351 -439.351 -439.351] [0.0000], Avg: [-762.136 -762.136 -762.136] (1.000)
Step: 18249, Reward: [-708.526 -708.526 -708.526] [0.0000], Avg: [-761.989 -761.989 -761.989] (1.000)
Step: 18299, Reward: [-624.554 -624.554 -624.554] [0.0000], Avg: [-761.613 -761.613 -761.613] (1.000)
Step: 18349, Reward: [-554.749 -554.749 -554.749] [0.0000], Avg: [-761.05 -761.05 -761.05] (1.000)
Step: 18399, Reward: [-459.565 -459.565 -459.565] [0.0000], Avg: [-760.231 -760.231 -760.231] (1.000)
Step: 18449, Reward: [-929.217 -929.217 -929.217] [0.0000], Avg: [-760.688 -760.688 -760.688] (1.000)
Step: 18499, Reward: [-898.468 -898.468 -898.468] [0.0000], Avg: [-761.061 -761.061 -761.061] (1.000)
Step: 18549, Reward: [-657.157 -657.157 -657.157] [0.0000], Avg: [-760.781 -760.781 -760.781] (1.000)
Step: 18599, Reward: [-870.121 -870.121 -870.121] [0.0000], Avg: [-761.075 -761.075 -761.075] (1.000)
Step: 18649, Reward: [-433.776 -433.776 -433.776] [0.0000], Avg: [-760.197 -760.197 -760.197] (1.000)
Step: 18699, Reward: [-645.598 -645.598 -645.598] [0.0000], Avg: [-759.891 -759.891 -759.891] (1.000)
Step: 18749, Reward: [-801.259 -801.259 -801.259] [0.0000], Avg: [-760.001 -760.001 -760.001] (1.000)
Step: 18799, Reward: [-1828.221 -1828.221 -1828.221] [0.0000], Avg: [-762.842 -762.842 -762.842] (1.000)
Step: 18849, Reward: [-785.828 -785.828 -785.828] [0.0000], Avg: [-762.903 -762.903 -762.903] (1.000)
Step: 18899, Reward: [-711.577 -711.577 -711.577] [0.0000], Avg: [-762.767 -762.767 -762.767] (1.000)
Step: 18949, Reward: [-775.453 -775.453 -775.453] [0.0000], Avg: [-762.801 -762.801 -762.801] (1.000)
Step: 18999, Reward: [-931.293 -931.293 -931.293] [0.0000], Avg: [-763.244 -763.244 -763.244] (1.000)
Step: 19049, Reward: [-666.266 -666.266 -666.266] [0.0000], Avg: [-762.99 -762.99 -762.99] (1.000)
Step: 19099, Reward: [-544.714 -544.714 -544.714] [0.0000], Avg: [-762.418 -762.418 -762.418] (1.000)
Step: 19149, Reward: [-391.488 -391.488 -391.488] [0.0000], Avg: [-761.45 -761.45 -761.45] (1.000)
Step: 19199, Reward: [-384.367 -384.367 -384.367] [0.0000], Avg: [-760.468 -760.468 -760.468] (1.000)
Step: 19249, Reward: [-742.012 -742.012 -742.012] [0.0000], Avg: [-760.42 -760.42 -760.42] (1.000)
Step: 19299, Reward: [-1182.126 -1182.126 -1182.126] [0.0000], Avg: [-761.512 -761.512 -761.512] (1.000)
Step: 19349, Reward: [-666.596 -666.596 -666.596] [0.0000], Avg: [-761.267 -761.267 -761.267] (1.000)
Step: 19399, Reward: [-1188.33 -1188.33 -1188.33] [0.0000], Avg: [-762.368 -762.368 -762.368] (1.000)
Step: 19449, Reward: [-642.332 -642.332 -642.332] [0.0000], Avg: [-762.059 -762.059 -762.059] (1.000)
Step: 19499, Reward: [-1409.139 -1409.139 -1409.139] [0.0000], Avg: [-763.718 -763.718 -763.718] (1.000)
Step: 19549, Reward: [-2088.449 -2088.449 -2088.449] [0.0000], Avg: [-767.106 -767.106 -767.106] (1.000)
Step: 19599, Reward: [-1833.275 -1833.275 -1833.275] [0.0000], Avg: [-769.826 -769.826 -769.826] (1.000)
Step: 19649, Reward: [-1771.007 -1771.007 -1771.007] [0.0000], Avg: [-772.374 -772.374 -772.374] (1.000)
Step: 19699, Reward: [-1496.451 -1496.451 -1496.451] [0.0000], Avg: [-774.212 -774.212 -774.212] (1.000)
Step: 19749, Reward: [-1733.879 -1733.879 -1733.879] [0.0000], Avg: [-776.641 -776.641 -776.641] (1.000)
Step: 19799, Reward: [-1982.82 -1982.82 -1982.82] [0.0000], Avg: [-779.687 -779.687 -779.687] (1.000)
Step: 19849, Reward: [-2055.149 -2055.149 -2055.149] [0.0000], Avg: [-782.9 -782.9 -782.9] (1.000)
Step: 19899, Reward: [-1870.251 -1870.251 -1870.251] [0.0000], Avg: [-785.632 -785.632 -785.632] (1.000)
Step: 19949, Reward: [-2092.674 -2092.674 -2092.674] [0.0000], Avg: [-788.908 -788.908 -788.908] (1.000)
Step: 19999, Reward: [-1735.229 -1735.229 -1735.229] [0.0000], Avg: [-791.273 -791.273 -791.273] (1.000)
Step: 20049, Reward: [-1911.854 -1911.854 -1911.854] [0.0000], Avg: [-794.068 -794.068 -794.068] (1.000)
Step: 20099, Reward: [-2124.197 -2124.197 -2124.197] [0.0000], Avg: [-797.377 -797.377 -797.377] (1.000)
Step: 20149, Reward: [-1799.616 -1799.616 -1799.616] [0.0000], Avg: [-799.864 -799.864 -799.864] (1.000)
Step: 20199, Reward: [-1654.56 -1654.56 -1654.56] [0.0000], Avg: [-801.979 -801.979 -801.979] (1.000)
Step: 20249, Reward: [-2275.593 -2275.593 -2275.593] [0.0000], Avg: [-805.618 -805.618 -805.618] (1.000)
Step: 20299, Reward: [-1948.741 -1948.741 -1948.741] [0.0000], Avg: [-808.433 -808.433 -808.433] (1.000)
Step: 20349, Reward: [-1634.142 -1634.142 -1634.142] [0.0000], Avg: [-810.462 -810.462 -810.462] (1.000)
Step: 20399, Reward: [-1951.042 -1951.042 -1951.042] [0.0000], Avg: [-813.258 -813.258 -813.258] (1.000)
Step: 20449, Reward: [-1877.858 -1877.858 -1877.858] [0.0000], Avg: [-815.861 -815.861 -815.861] (1.000)
Step: 20499, Reward: [-1905.047 -1905.047 -1905.047] [0.0000], Avg: [-818.517 -818.517 -818.517] (1.000)
Step: 20549, Reward: [-1670.65 -1670.65 -1670.65] [0.0000], Avg: [-820.59 -820.59 -820.59] (1.000)
Step: 20599, Reward: [-1730.067 -1730.067 -1730.067] [0.0000], Avg: [-822.798 -822.798 -822.798] (1.000)
Step: 20649, Reward: [-1982.938 -1982.938 -1982.938] [0.0000], Avg: [-825.607 -825.607 -825.607] (1.000)
Step: 20699, Reward: [-1861.071 -1861.071 -1861.071] [0.0000], Avg: [-828.108 -828.108 -828.108] (1.000)
Step: 20749, Reward: [-2156.851 -2156.851 -2156.851] [0.0000], Avg: [-831.31 -831.31 -831.31] (1.000)
Step: 20799, Reward: [-1626.818 -1626.818 -1626.818] [0.0000], Avg: [-833.222 -833.222 -833.222] (1.000)
Step: 20849, Reward: [-1856.388 -1856.388 -1856.388] [0.0000], Avg: [-835.676 -835.676 -835.676] (1.000)
Step: 20899, Reward: [-1772.835 -1772.835 -1772.835] [0.0000], Avg: [-837.918 -837.918 -837.918] (1.000)
Step: 20949, Reward: [-2287.501 -2287.501 -2287.501] [0.0000], Avg: [-841.377 -841.377 -841.377] (1.000)
Step: 20999, Reward: [-1628.043 -1628.043 -1628.043] [0.0000], Avg: [-843.25 -843.25 -843.25] (1.000)
Step: 21049, Reward: [-2115.686 -2115.686 -2115.686] [0.0000], Avg: [-846.273 -846.273 -846.273] (1.000)
Step: 21099, Reward: [-2099.559 -2099.559 -2099.559] [0.0000], Avg: [-849.243 -849.243 -849.243] (1.000)
Step: 21149, Reward: [-1681.461 -1681.461 -1681.461] [0.0000], Avg: [-851.21 -851.21 -851.21] (1.000)
Step: 21199, Reward: [-1904.628 -1904.628 -1904.628] [0.0000], Avg: [-853.695 -853.695 -853.695] (1.000)
Step: 21249, Reward: [-2151.738 -2151.738 -2151.738] [0.0000], Avg: [-856.749 -856.749 -856.749] (1.000)
Step: 21299, Reward: [-2128.426 -2128.426 -2128.426] [0.0000], Avg: [-859.734 -859.734 -859.734] (1.000)
Step: 21349, Reward: [-1785.525 -1785.525 -1785.525] [0.0000], Avg: [-861.902 -861.902 -861.902] (1.000)
Step: 21399, Reward: [-1853.308 -1853.308 -1853.308] [0.0000], Avg: [-864.218 -864.218 -864.218] (1.000)
Step: 21449, Reward: [-1951.066 -1951.066 -1951.066] [0.0000], Avg: [-866.752 -866.752 -866.752] (1.000)
Step: 21499, Reward: [-1703.1 -1703.1 -1703.1] [0.0000], Avg: [-868.697 -868.697 -868.697] (1.000)
Step: 21549, Reward: [-1779.65 -1779.65 -1779.65] [0.0000], Avg: [-870.81 -870.81 -870.81] (1.000)
Step: 21599, Reward: [-1591.55 -1591.55 -1591.55] [0.0000], Avg: [-872.479 -872.479 -872.479] (1.000)
Step: 21649, Reward: [-1889.915 -1889.915 -1889.915] [0.0000], Avg: [-874.829 -874.829 -874.829] (1.000)
Step: 21699, Reward: [-686.101 -686.101 -686.101] [0.0000], Avg: [-874.394 -874.394 -874.394] (1.000)
Step: 21749, Reward: [-1101.366 -1101.366 -1101.366] [0.0000], Avg: [-874.916 -874.916 -874.916] (1.000)
Step: 21799, Reward: [-959.596 -959.596 -959.596] [0.0000], Avg: [-875.11 -875.11 -875.11] (1.000)
Step: 21849, Reward: [-560.754 -560.754 -560.754] [0.0000], Avg: [-874.39 -874.39 -874.39] (1.000)
Step: 21899, Reward: [-488.148 -488.148 -488.148] [0.0000], Avg: [-873.509 -873.509 -873.509] (1.000)
Step: 21949, Reward: [-701.59 -701.59 -701.59] [0.0000], Avg: [-873.117 -873.117 -873.117] (1.000)
Step: 21999, Reward: [-650.675 -650.675 -650.675] [0.0000], Avg: [-872.611 -872.611 -872.611] (1.000)
Step: 22049, Reward: [-429.406 -429.406 -429.406] [0.0000], Avg: [-871.606 -871.606 -871.606] (1.000)
Step: 22099, Reward: [-458.121 -458.121 -458.121] [0.0000], Avg: [-870.671 -870.671 -870.671] (1.000)
Step: 22149, Reward: [-770.495 -770.495 -770.495] [0.0000], Avg: [-870.445 -870.445 -870.445] (1.000)
Step: 22199, Reward: [-904.576 -904.576 -904.576] [0.0000], Avg: [-870.522 -870.522 -870.522] (1.000)
Step: 22249, Reward: [-897.182 -897.182 -897.182] [0.0000], Avg: [-870.582 -870.582 -870.582] (1.000)
Step: 22299, Reward: [-435.136 -435.136 -435.136] [0.0000], Avg: [-869.605 -869.605 -869.605] (1.000)
Step: 22349, Reward: [-687.233 -687.233 -687.233] [0.0000], Avg: [-869.197 -869.197 -869.197] (1.000)
Step: 22399, Reward: [-502.114 -502.114 -502.114] [0.0000], Avg: [-868.378 -868.378 -868.378] (1.000)
Step: 22449, Reward: [-567.828 -567.828 -567.828] [0.0000], Avg: [-867.708 -867.708 -867.708] (1.000)
Step: 22499, Reward: [-790.443 -790.443 -790.443] [0.0000], Avg: [-867.537 -867.537 -867.537] (1.000)
Step: 22549, Reward: [-533.681 -533.681 -533.681] [0.0000], Avg: [-866.796 -866.796 -866.796] (1.000)
Step: 22599, Reward: [-677.645 -677.645 -677.645] [0.0000], Avg: [-866.378 -866.378 -866.378] (1.000)
Step: 22649, Reward: [-509.544 -509.544 -509.544] [0.0000], Avg: [-865.59 -865.59 -865.59] (1.000)
Step: 22699, Reward: [-608.311 -608.311 -608.311] [0.0000], Avg: [-865.024 -865.024 -865.024] (1.000)
Step: 22749, Reward: [-712.762 -712.762 -712.762] [0.0000], Avg: [-864.689 -864.689 -864.689] (1.000)
Step: 22799, Reward: [-754.402 -754.402 -754.402] [0.0000], Avg: [-864.447 -864.447 -864.447] (1.000)
Step: 22849, Reward: [-663.52 -663.52 -663.52] [0.0000], Avg: [-864.007 -864.007 -864.007] (1.000)
Step: 22899, Reward: [-618.503 -618.503 -618.503] [0.0000], Avg: [-863.471 -863.471 -863.471] (1.000)
Step: 22949, Reward: [-535.311 -535.311 -535.311] [0.0000], Avg: [-862.756 -862.756 -862.756] (1.000)
Step: 22999, Reward: [-459.218 -459.218 -459.218] [0.0000], Avg: [-861.879 -861.879 -861.879] (1.000)
Step: 23049, Reward: [-572.871 -572.871 -572.871] [0.0000], Avg: [-861.252 -861.252 -861.252] (1.000)
Step: 23099, Reward: [-660.027 -660.027 -660.027] [0.0000], Avg: [-860.817 -860.817 -860.817] (1.000)
Step: 23149, Reward: [-706.627 -706.627 -706.627] [0.0000], Avg: [-860.484 -860.484 -860.484] (1.000)
Step: 23199, Reward: [-499.1 -499.1 -499.1] [0.0000], Avg: [-859.705 -859.705 -859.705] (1.000)
Step: 23249, Reward: [-580.808 -580.808 -580.808] [0.0000], Avg: [-859.105 -859.105 -859.105] (1.000)
Step: 23299, Reward: [-860.644 -860.644 -860.644] [0.0000], Avg: [-859.108 -859.108 -859.108] (1.000)
Step: 23349, Reward: [-503.332 -503.332 -503.332] [0.0000], Avg: [-858.347 -858.347 -858.347] (1.000)
Step: 23399, Reward: [-508.66 -508.66 -508.66] [0.0000], Avg: [-857.599 -857.599 -857.599] (1.000)
Step: 23449, Reward: [-678.07 -678.07 -678.07] [0.0000], Avg: [-857.217 -857.217 -857.217] (1.000)
Step: 23499, Reward: [-401.642 -401.642 -401.642] [0.0000], Avg: [-856.247 -856.247 -856.247] (1.000)
Step: 23549, Reward: [-542.508 -542.508 -542.508] [0.0000], Avg: [-855.581 -855.581 -855.581] (1.000)
Step: 23599, Reward: [-536.184 -536.184 -536.184] [0.0000], Avg: [-854.904 -854.904 -854.904] (1.000)
Step: 23649, Reward: [-436.214 -436.214 -436.214] [0.0000], Avg: [-854.019 -854.019 -854.019] (1.000)
Step: 23699, Reward: [-612.599 -612.599 -612.599] [0.0000], Avg: [-853.51 -853.51 -853.51] (1.000)
Step: 23749, Reward: [-656.549 -656.549 -656.549] [0.0000], Avg: [-853.095 -853.095 -853.095] (1.000)
Step: 23799, Reward: [-816.268 -816.268 -816.268] [0.0000], Avg: [-853.018 -853.018 -853.018] (1.000)
Step: 23849, Reward: [-650.262 -650.262 -650.262] [0.0000], Avg: [-852.593 -852.593 -852.593] (1.000)
Step: 23899, Reward: [-742.446 -742.446 -742.446] [0.0000], Avg: [-852.362 -852.362 -852.362] (1.000)
Step: 23949, Reward: [-480.752 -480.752 -480.752] [0.0000], Avg: [-851.587 -851.587 -851.587] (1.000)
Step: 23999, Reward: [-558.406 -558.406 -558.406] [0.0000], Avg: [-850.976 -850.976 -850.976] (1.000)
Step: 24049, Reward: [-599.187 -599.187 -599.187] [0.0000], Avg: [-850.452 -850.452 -850.452] (1.000)
Step: 24099, Reward: [-366.479 -366.479 -366.479] [0.0000], Avg: [-849.448 -849.448 -849.448] (1.000)
Step: 24149, Reward: [-960.667 -960.667 -960.667] [0.0000], Avg: [-849.679 -849.679 -849.679] (1.000)
Step: 24199, Reward: [-501.076 -501.076 -501.076] [0.0000], Avg: [-848.958 -848.958 -848.958] (1.000)
Step: 24249, Reward: [-457.137 -457.137 -457.137] [0.0000], Avg: [-848.15 -848.15 -848.15] (1.000)
Step: 24299, Reward: [-640.144 -640.144 -640.144] [0.0000], Avg: [-847.722 -847.722 -847.722] (1.000)
Step: 24349, Reward: [-891.203 -891.203 -891.203] [0.0000], Avg: [-847.812 -847.812 -847.812] (1.000)
Step: 24399, Reward: [-788.697 -788.697 -788.697] [0.0000], Avg: [-847.691 -847.691 -847.691] (1.000)
Step: 24449, Reward: [-805.08 -805.08 -805.08] [0.0000], Avg: [-847.603 -847.603 -847.603] (1.000)
Step: 24499, Reward: [-520.666 -520.666 -520.666] [0.0000], Avg: [-846.936 -846.936 -846.936] (1.000)
Step: 24549, Reward: [-733.085 -733.085 -733.085] [0.0000], Avg: [-846.704 -846.704 -846.704] (1.000)
Step: 24599, Reward: [-660.503 -660.503 -660.503] [0.0000], Avg: [-846.326 -846.326 -846.326] (1.000)
Step: 24649, Reward: [-687.718 -687.718 -687.718] [0.0000], Avg: [-846.004 -846.004 -846.004] (1.000)
Step: 24699, Reward: [-566.25 -566.25 -566.25] [0.0000], Avg: [-845.438 -845.438 -845.438] (1.000)
Step: 24749, Reward: [-693.19 -693.19 -693.19] [0.0000], Avg: [-845.13 -845.13 -845.13] (1.000)
Step: 24799, Reward: [-625.01 -625.01 -625.01] [0.0000], Avg: [-844.686 -844.686 -844.686] (1.000)
Step: 24849, Reward: [-657.017 -657.017 -657.017] [0.0000], Avg: [-844.309 -844.309 -844.309] (1.000)
Step: 24899, Reward: [-402.777 -402.777 -402.777] [0.0000], Avg: [-843.422 -843.422 -843.422] (1.000)
Step: 24949, Reward: [-564.071 -564.071 -564.071] [0.0000], Avg: [-842.862 -842.862 -842.862] (1.000)
Step: 24999, Reward: [-398.287 -398.287 -398.287] [0.0000], Avg: [-841.973 -841.973 -841.973] (1.000)
Step: 25049, Reward: [-602.256 -602.256 -602.256] [0.0000], Avg: [-841.495 -841.495 -841.495] (1.000)
Step: 25099, Reward: [-740.212 -740.212 -740.212] [0.0000], Avg: [-841.293 -841.293 -841.293] (1.000)
Step: 25149, Reward: [-334.155 -334.155 -334.155] [0.0000], Avg: [-840.285 -840.285 -840.285] (1.000)
Step: 25199, Reward: [-619.992 -619.992 -619.992] [0.0000], Avg: [-839.848 -839.848 -839.848] (1.000)
Step: 25249, Reward: [-895.914 -895.914 -895.914] [0.0000], Avg: [-839.959 -839.959 -839.959] (1.000)
Step: 25299, Reward: [-514.031 -514.031 -514.031] [0.0000], Avg: [-839.315 -839.315 -839.315] (1.000)
Step: 25349, Reward: [-611.041 -611.041 -611.041] [0.0000], Avg: [-838.864 -838.864 -838.864] (1.000)
Step: 25399, Reward: [-425.786 -425.786 -425.786] [0.0000], Avg: [-838.051 -838.051 -838.051] (1.000)
Step: 25449, Reward: [-553.176 -553.176 -553.176] [0.0000], Avg: [-837.492 -837.492 -837.492] (1.000)
Step: 25499, Reward: [-368.673 -368.673 -368.673] [0.0000], Avg: [-836.572 -836.572 -836.572] (1.000)
Step: 25549, Reward: [-709.482 -709.482 -709.482] [0.0000], Avg: [-836.324 -836.324 -836.324] (1.000)
Step: 25599, Reward: [-396.863 -396.863 -396.863] [0.0000], Avg: [-835.465 -835.465 -835.465] (1.000)
Step: 25649, Reward: [-779.065 -779.065 -779.065] [0.0000], Avg: [-835.355 -835.355 -835.355] (1.000)
Step: 25699, Reward: [-574.16 -574.16 -574.16] [0.0000], Avg: [-834.847 -834.847 -834.847] (1.000)
Step: 25749, Reward: [-694.427 -694.427 -694.427] [0.0000], Avg: [-834.575 -834.575 -834.575] (1.000)
Step: 25799, Reward: [-702.61 -702.61 -702.61] [0.0000], Avg: [-834.319 -834.319 -834.319] (1.000)
Step: 25849, Reward: [-572.701 -572.701 -572.701] [0.0000], Avg: [-833.813 -833.813 -833.813] (1.000)
Step: 25899, Reward: [-551.249 -551.249 -551.249] [0.0000], Avg: [-833.267 -833.267 -833.267] (1.000)
Step: 25949, Reward: [-636.353 -636.353 -636.353] [0.0000], Avg: [-832.888 -832.888 -832.888] (1.000)
Step: 25999, Reward: [-526.247 -526.247 -526.247] [0.0000], Avg: [-832.298 -832.298 -832.298] (1.000)
Step: 26049, Reward: [-556.716 -556.716 -556.716] [0.0000], Avg: [-831.769 -831.769 -831.769] (1.000)
Step: 26099, Reward: [-728.888 -728.888 -728.888] [0.0000], Avg: [-831.572 -831.572 -831.572] (1.000)
Step: 26149, Reward: [-501.25 -501.25 -501.25] [0.0000], Avg: [-830.941 -830.941 -830.941] (1.000)
Step: 26199, Reward: [-421.185 -421.185 -421.185] [0.0000], Avg: [-830.159 -830.159 -830.159] (1.000)
Step: 26249, Reward: [-534.462 -534.462 -534.462] [0.0000], Avg: [-829.595 -829.595 -829.595] (1.000)
Step: 26299, Reward: [-551.71 -551.71 -551.71] [0.0000], Avg: [-829.067 -829.067 -829.067] (1.000)
Step: 26349, Reward: [-384.3 -384.3 -384.3] [0.0000], Avg: [-828.223 -828.223 -828.223] (1.000)
Step: 26399, Reward: [-744.886 -744.886 -744.886] [0.0000], Avg: [-828.065 -828.065 -828.065] (1.000)
Step: 26449, Reward: [-894.732 -894.732 -894.732] [0.0000], Avg: [-828.191 -828.191 -828.191] (1.000)
Step: 26499, Reward: [-606.47 -606.47 -606.47] [0.0000], Avg: [-827.773 -827.773 -827.773] (1.000)
Step: 26549, Reward: [-535.221 -535.221 -535.221] [0.0000], Avg: [-827.222 -827.222 -827.222] (1.000)
Step: 26599, Reward: [-779.64 -779.64 -779.64] [0.0000], Avg: [-827.133 -827.133 -827.133] (1.000)
Step: 26649, Reward: [-1863.999 -1863.999 -1863.999] [0.0000], Avg: [-829.078 -829.078 -829.078] (1.000)
Step: 26699, Reward: [-1996.17 -1996.17 -1996.17] [0.0000], Avg: [-831.263 -831.263 -831.263] (1.000)
Step: 26749, Reward: [-2070.783 -2070.783 -2070.783] [0.0000], Avg: [-833.58 -833.58 -833.58] (1.000)
Step: 26799, Reward: [-2030.611 -2030.611 -2030.611] [0.0000], Avg: [-835.814 -835.814 -835.814] (1.000)
Step: 26849, Reward: [-1837.536 -1837.536 -1837.536] [0.0000], Avg: [-837.679 -837.679 -837.679] (1.000)
Step: 26899, Reward: [-1836.542 -1836.542 -1836.542] [0.0000], Avg: [-839.536 -839.536 -839.536] (1.000)
Step: 26949, Reward: [-1723.893 -1723.893 -1723.893] [0.0000], Avg: [-841.176 -841.176 -841.176] (1.000)
Step: 26999, Reward: [-1792.462 -1792.462 -1792.462] [0.0000], Avg: [-842.938 -842.938 -842.938] (1.000)
Step: 27049, Reward: [-1806.522 -1806.522 -1806.522] [0.0000], Avg: [-844.719 -844.719 -844.719] (1.000)
Step: 27099, Reward: [-1937.791 -1937.791 -1937.791] [0.0000], Avg: [-846.736 -846.736 -846.736] (1.000)
Step: 27149, Reward: [-1397.19 -1397.19 -1397.19] [0.0000], Avg: [-847.75 -847.75 -847.75] (1.000)
Step: 27199, Reward: [-1191.028 -1191.028 -1191.028] [0.0000], Avg: [-848.381 -848.381 -848.381] (1.000)
Step: 27249, Reward: [-979.502 -979.502 -979.502] [0.0000], Avg: [-848.621 -848.621 -848.621] (1.000)
Step: 27299, Reward: [-1197.081 -1197.081 -1197.081] [0.0000], Avg: [-849.259 -849.259 -849.259] (1.000)
Step: 27349, Reward: [-1017.347 -1017.347 -1017.347] [0.0000], Avg: [-849.567 -849.567 -849.567] (1.000)
Step: 27399, Reward: [-934.701 -934.701 -934.701] [0.0000], Avg: [-849.722 -849.722 -849.722] (1.000)
Step: 27449, Reward: [-934.817 -934.817 -934.817] [0.0000], Avg: [-849.877 -849.877 -849.877] (1.000)
Step: 27499, Reward: [-809.934 -809.934 -809.934] [0.0000], Avg: [-849.804 -849.804 -849.804] (1.000)
Step: 27549, Reward: [-455.294 -455.294 -455.294] [0.0000], Avg: [-849.088 -849.088 -849.088] (1.000)
Step: 27599, Reward: [-872.127 -872.127 -872.127] [0.0000], Avg: [-849.13 -849.13 -849.13] (1.000)
Step: 27649, Reward: [-678.585 -678.585 -678.585] [0.0000], Avg: [-848.822 -848.822 -848.822] (1.000)
Step: 27699, Reward: [-505.81 -505.81 -505.81] [0.0000], Avg: [-848.203 -848.203 -848.203] (1.000)
Step: 27749, Reward: [-515.536 -515.536 -515.536] [0.0000], Avg: [-847.603 -847.603 -847.603] (1.000)
Step: 27799, Reward: [-439.208 -439.208 -439.208] [0.0000], Avg: [-846.869 -846.869 -846.869] (1.000)
Step: 27849, Reward: [-673.071 -673.071 -673.071] [0.0000], Avg: [-846.557 -846.557 -846.557] (1.000)
Step: 27899, Reward: [-548.8 -548.8 -548.8] [0.0000], Avg: [-846.023 -846.023 -846.023] (1.000)
Step: 27949, Reward: [-569.665 -569.665 -569.665] [0.0000], Avg: [-845.529 -845.529 -845.529] (1.000)
Step: 27999, Reward: [-509.022 -509.022 -509.022] [0.0000], Avg: [-844.928 -844.928 -844.928] (1.000)
Step: 28049, Reward: [-874.339 -874.339 -874.339] [0.0000], Avg: [-844.98 -844.98 -844.98] (1.000)
Step: 28099, Reward: [-767.837 -767.837 -767.837] [0.0000], Avg: [-844.843 -844.843 -844.843] (1.000)
Step: 28149, Reward: [-507.067 -507.067 -507.067] [0.0000], Avg: [-844.243 -844.243 -844.243] (1.000)
Step: 28199, Reward: [-549.965 -549.965 -549.965] [0.0000], Avg: [-843.721 -843.721 -843.721] (1.000)
Step: 28249, Reward: [-517.403 -517.403 -517.403] [0.0000], Avg: [-843.144 -843.144 -843.144] (1.000)
Step: 28299, Reward: [-662.397 -662.397 -662.397] [0.0000], Avg: [-842.824 -842.824 -842.824] (1.000)
Step: 28349, Reward: [-676.39 -676.39 -676.39] [0.0000], Avg: [-842.531 -842.531 -842.531] (1.000)
Step: 28399, Reward: [-515.681 -515.681 -515.681] [0.0000], Avg: [-841.955 -841.955 -841.955] (1.000)
Step: 28449, Reward: [-453.396 -453.396 -453.396] [0.0000], Avg: [-841.272 -841.272 -841.272] (1.000)
Step: 28499, Reward: [-504.343 -504.343 -504.343] [0.0000], Avg: [-840.681 -840.681 -840.681] (1.000)
Step: 28549, Reward: [-581.232 -581.232 -581.232] [0.0000], Avg: [-840.227 -840.227 -840.227] (1.000)
Step: 28599, Reward: [-600.289 -600.289 -600.289] [0.0000], Avg: [-839.807 -839.807 -839.807] (1.000)
Step: 28649, Reward: [-512.098 -512.098 -512.098] [0.0000], Avg: [-839.236 -839.236 -839.236] (1.000)
Step: 28699, Reward: [-582.725 -582.725 -582.725] [0.0000], Avg: [-838.789 -838.789 -838.789] (1.000)
Step: 28749, Reward: [-751.971 -751.971 -751.971] [0.0000], Avg: [-838.638 -838.638 -838.638] (1.000)
Step: 28799, Reward: [-551.497 -551.497 -551.497] [0.0000], Avg: [-838.139 -838.139 -838.139] (1.000)
Step: 28849, Reward: [-494.798 -494.798 -494.798] [0.0000], Avg: [-837.544 -837.544 -837.544] (1.000)
Step: 28899, Reward: [-777.503 -777.503 -777.503] [0.0000], Avg: [-837.44 -837.44 -837.44] (1.000)
Step: 28949, Reward: [-905.928 -905.928 -905.928] [0.0000], Avg: [-837.559 -837.559 -837.559] (1.000)
Step: 28999, Reward: [-614.475 -614.475 -614.475] [0.0000], Avg: [-837.174 -837.174 -837.174] (1.000)
Step: 29049, Reward: [-692.173 -692.173 -692.173] [0.0000], Avg: [-836.924 -836.924 -836.924] (1.000)
Step: 29099, Reward: [-639.953 -639.953 -639.953] [0.0000], Avg: [-836.586 -836.586 -836.586] (1.000)
Step: 29149, Reward: [-726.279 -726.279 -726.279] [0.0000], Avg: [-836.397 -836.397 -836.397] (1.000)
Step: 29199, Reward: [-494.695 -494.695 -494.695] [0.0000], Avg: [-835.812 -835.812 -835.812] (1.000)
Step: 29249, Reward: [-639.123 -639.123 -639.123] [0.0000], Avg: [-835.475 -835.475 -835.475] (1.000)
Step: 29299, Reward: [-316.05 -316.05 -316.05] [0.0000], Avg: [-834.589 -834.589 -834.589] (1.000)
Step: 29349, Reward: [-646.068 -646.068 -646.068] [0.0000], Avg: [-834.268 -834.268 -834.268] (1.000)
Step: 29399, Reward: [-467.045 -467.045 -467.045] [0.0000], Avg: [-833.643 -833.643 -833.643] (1.000)
Step: 29449, Reward: [-546.282 -546.282 -546.282] [0.0000], Avg: [-833.155 -833.155 -833.155] (1.000)
Step: 29499, Reward: [-495.686 -495.686 -495.686] [0.0000], Avg: [-832.583 -832.583 -832.583] (1.000)
Step: 29549, Reward: [-414.149 -414.149 -414.149] [0.0000], Avg: [-831.875 -831.875 -831.875] (1.000)
Step: 29599, Reward: [-811.272 -811.272 -811.272] [0.0000], Avg: [-831.841 -831.841 -831.841] (1.000)
Step: 29649, Reward: [-486.881 -486.881 -486.881] [0.0000], Avg: [-831.259 -831.259 -831.259] (1.000)
Step: 29699, Reward: [-504.296 -504.296 -504.296] [0.0000], Avg: [-830.708 -830.708 -830.708] (1.000)
Step: 29749, Reward: [-602.166 -602.166 -602.166] [0.0000], Avg: [-830.324 -830.324 -830.324] (1.000)
Step: 29799, Reward: [-559.738 -559.738 -559.738] [0.0000], Avg: [-829.87 -829.87 -829.87] (1.000)
Step: 29849, Reward: [-386.277 -386.277 -386.277] [0.0000], Avg: [-829.127 -829.127 -829.127] (1.000)
Step: 29899, Reward: [-642.142 -642.142 -642.142] [0.0000], Avg: [-828.815 -828.815 -828.815] (1.000)
Step: 29949, Reward: [-459.123 -459.123 -459.123] [0.0000], Avg: [-828.197 -828.197 -828.197] (1.000)
Step: 29999, Reward: [-522.171 -522.171 -522.171] [0.0000], Avg: [-827.687 -827.687 -827.687] (1.000)
Step: 30049, Reward: [-605.087 -605.087 -605.087] [0.0000], Avg: [-827.317 -827.317 -827.317] (1.000)
Step: 30099, Reward: [-855.604 -855.604 -855.604] [0.0000], Avg: [-827.364 -827.364 -827.364] (1.000)
Step: 30149, Reward: [-519.32 -519.32 -519.32] [0.0000], Avg: [-826.853 -826.853 -826.853] (1.000)
Step: 30199, Reward: [-770.884 -770.884 -770.884] [0.0000], Avg: [-826.76 -826.76 -826.76] (1.000)
Step: 30249, Reward: [-426.302 -426.302 -426.302] [0.0000], Avg: [-826.099 -826.099 -826.099] (1.000)
Step: 30299, Reward: [-427.567 -427.567 -427.567] [0.0000], Avg: [-825.441 -825.441 -825.441] (1.000)
Step: 30349, Reward: [-529.86 -529.86 -529.86] [0.0000], Avg: [-824.954 -824.954 -824.954] (1.000)
Step: 30399, Reward: [-462.217 -462.217 -462.217] [0.0000], Avg: [-824.357 -824.357 -824.357] (1.000)
Step: 30449, Reward: [-723.624 -723.624 -723.624] [0.0000], Avg: [-824.192 -824.192 -824.192] (1.000)
Step: 30499, Reward: [-509.382 -509.382 -509.382] [0.0000], Avg: [-823.676 -823.676 -823.676] (1.000)
Step: 30549, Reward: [-617.576 -617.576 -617.576] [0.0000], Avg: [-823.339 -823.339 -823.339] (1.000)
Step: 30599, Reward: [-657.513 -657.513 -657.513] [0.0000], Avg: [-823.068 -823.068 -823.068] (1.000)
Step: 30649, Reward: [-664.471 -664.471 -664.471] [0.0000], Avg: [-822.809 -822.809 -822.809] (1.000)
Step: 30699, Reward: [-623.265 -623.265 -623.265] [0.0000], Avg: [-822.484 -822.484 -822.484] (1.000)
Step: 30749, Reward: [-560.351 -560.351 -560.351] [0.0000], Avg: [-822.058 -822.058 -822.058] (1.000)
Step: 30799, Reward: [-759.776 -759.776 -759.776] [0.0000], Avg: [-821.957 -821.957 -821.957] (1.000)
Step: 30849, Reward: [-1872.927 -1872.927 -1872.927] [0.0000], Avg: [-823.66 -823.66 -823.66] (1.000)
Step: 30899, Reward: [-518.119 -518.119 -518.119] [0.0000], Avg: [-823.165 -823.165 -823.165] (1.000)
Step: 30949, Reward: [-573.546 -573.546 -573.546] [0.0000], Avg: [-822.762 -822.762 -822.762] (1.000)
Step: 30999, Reward: [-437.099 -437.099 -437.099] [0.0000], Avg: [-822.14 -822.14 -822.14] (1.000)
Step: 31049, Reward: [-581.307 -581.307 -581.307] [0.0000], Avg: [-821.752 -821.752 -821.752] (1.000)
Step: 31099, Reward: [-733.872 -733.872 -733.872] [0.0000], Avg: [-821.611 -821.611 -821.611] (1.000)
Step: 31149, Reward: [-860.927 -860.927 -860.927] [0.0000], Avg: [-821.674 -821.674 -821.674] (1.000)
Step: 31199, Reward: [-430.766 -430.766 -430.766] [0.0000], Avg: [-821.048 -821.048 -821.048] (1.000)
Step: 31249, Reward: [-723.015 -723.015 -723.015] [0.0000], Avg: [-820.891 -820.891 -820.891] (1.000)
Step: 31299, Reward: [-1446.381 -1446.381 -1446.381] [0.0000], Avg: [-821.89 -821.89 -821.89] (1.000)
Step: 31349, Reward: [-1980.078 -1980.078 -1980.078] [0.0000], Avg: [-823.737 -823.737 -823.737] (1.000)
Step: 31399, Reward: [-1899.46 -1899.46 -1899.46] [0.0000], Avg: [-825.45 -825.45 -825.45] (1.000)
Step: 31449, Reward: [-1870.943 -1870.943 -1870.943] [0.0000], Avg: [-827.112 -827.112 -827.112] (1.000)
Step: 31499, Reward: [-1729.163 -1729.163 -1729.163] [0.0000], Avg: [-828.544 -828.544 -828.544] (1.000)
Step: 31549, Reward: [-1974.608 -1974.608 -1974.608] [0.0000], Avg: [-830.36 -830.36 -830.36] (1.000)
Step: 31599, Reward: [-1962.168 -1962.168 -1962.168] [0.0000], Avg: [-832.151 -832.151 -832.151] (1.000)
Step: 31649, Reward: [-1841.485 -1841.485 -1841.485] [0.0000], Avg: [-833.746 -833.746 -833.746] (1.000)
Step: 31699, Reward: [-1772.371 -1772.371 -1772.371] [0.0000], Avg: [-835.226 -835.226 -835.226] (1.000)
Step: 31749, Reward: [-2148.547 -2148.547 -2148.547] [0.0000], Avg: [-837.295 -837.295 -837.295] (1.000)
Step: 31799, Reward: [-1989.604 -1989.604 -1989.604] [0.0000], Avg: [-839.106 -839.106 -839.106] (1.000)
Step: 31849, Reward: [-2145.177 -2145.177 -2145.177] [0.0000], Avg: [-841.157 -841.157 -841.157] (1.000)
Step: 31899, Reward: [-1518.427 -1518.427 -1518.427] [0.0000], Avg: [-842.218 -842.218 -842.218] (1.000)
Step: 31949, Reward: [-1742.508 -1742.508 -1742.508] [0.0000], Avg: [-843.627 -843.627 -843.627] (1.000)
Step: 31999, Reward: [-2287.908 -2287.908 -2287.908] [0.0000], Avg: [-845.884 -845.884 -845.884] (1.000)
Step: 32049, Reward: [-2212.29 -2212.29 -2212.29] [0.0000], Avg: [-848.015 -848.015 -848.015] (1.000)
Step: 32099, Reward: [-2033.791 -2033.791 -2033.791] [0.0000], Avg: [-849.862 -849.862 -849.862] (1.000)
Step: 32149, Reward: [-1961.225 -1961.225 -1961.225] [0.0000], Avg: [-851.591 -851.591 -851.591] (1.000)
Step: 32199, Reward: [-1956.77 -1956.77 -1956.77] [0.0000], Avg: [-853.307 -853.307 -853.307] (1.000)
Step: 32249, Reward: [-1840.286 -1840.286 -1840.286] [0.0000], Avg: [-854.837 -854.837 -854.837] (1.000)
Step: 32299, Reward: [-1899.285 -1899.285 -1899.285] [0.0000], Avg: [-856.454 -856.454 -856.454] (1.000)
Step: 32349, Reward: [-1945.585 -1945.585 -1945.585] [0.0000], Avg: [-858.137 -858.137 -858.137] (1.000)
Step: 32399, Reward: [-2095.433 -2095.433 -2095.433] [0.0000], Avg: [-860.047 -860.047 -860.047] (1.000)
Step: 32449, Reward: [-2095.907 -2095.907 -2095.907] [0.0000], Avg: [-861.951 -861.951 -861.951] (1.000)
Step: 32499, Reward: [-1905.737 -1905.737 -1905.737] [0.0000], Avg: [-863.557 -863.557 -863.557] (1.000)
Step: 32549, Reward: [-2013.289 -2013.289 -2013.289] [0.0000], Avg: [-865.323 -865.323 -865.323] (1.000)
Step: 32599, Reward: [-1469.274 -1469.274 -1469.274] [0.0000], Avg: [-866.249 -866.249 -866.249] (1.000)
Step: 32649, Reward: [-1737.589 -1737.589 -1737.589] [0.0000], Avg: [-867.584 -867.584 -867.584] (1.000)
Step: 32699, Reward: [-1380.22 -1380.22 -1380.22] [0.0000], Avg: [-868.367 -868.367 -868.367] (1.000)
Step: 32749, Reward: [-1430.362 -1430.362 -1430.362] [0.0000], Avg: [-869.225 -869.225 -869.225] (1.000)
Step: 32799, Reward: [-1234.797 -1234.797 -1234.797] [0.0000], Avg: [-869.783 -869.783 -869.783] (1.000)
Step: 32849, Reward: [-1856.341 -1856.341 -1856.341] [0.0000], Avg: [-871.284 -871.284 -871.284] (1.000)
Step: 32899, Reward: [-1255.142 -1255.142 -1255.142] [0.0000], Avg: [-871.868 -871.868 -871.868] (1.000)
Step: 32949, Reward: [-1003.272 -1003.272 -1003.272] [0.0000], Avg: [-872.067 -872.067 -872.067] (1.000)
Step: 32999, Reward: [-1442.941 -1442.941 -1442.941] [0.0000], Avg: [-872.932 -872.932 -872.932] (1.000)
Step: 33049, Reward: [-1022.925 -1022.925 -1022.925] [0.0000], Avg: [-873.159 -873.159 -873.159] (1.000)
Step: 33099, Reward: [-1495.011 -1495.011 -1495.011] [0.0000], Avg: [-874.098 -874.098 -874.098] (1.000)
Step: 33149, Reward: [-1406.454 -1406.454 -1406.454] [0.0000], Avg: [-874.901 -874.901 -874.901] (1.000)
Step: 33199, Reward: [-1492.202 -1492.202 -1492.202] [0.0000], Avg: [-875.831 -875.831 -875.831] (1.000)
Step: 33249, Reward: [-2082.266 -2082.266 -2082.266] [0.0000], Avg: [-877.645 -877.645 -877.645] (1.000)
Step: 33299, Reward: [-1132.498 -1132.498 -1132.498] [0.0000], Avg: [-878.028 -878.028 -878.028] (1.000)
Step: 33349, Reward: [-2201.032 -2201.032 -2201.032] [0.0000], Avg: [-880.011 -880.011 -880.011] (1.000)
Step: 33399, Reward: [-981.814 -981.814 -981.814] [0.0000], Avg: [-880.164 -880.164 -880.164] (1.000)
Step: 33449, Reward: [-1633.482 -1633.482 -1633.482] [0.0000], Avg: [-881.29 -881.29 -881.29] (1.000)
Step: 33499, Reward: [-1840.886 -1840.886 -1840.886] [0.0000], Avg: [-882.722 -882.722 -882.722] (1.000)
Step: 33549, Reward: [-2095.733 -2095.733 -2095.733] [0.0000], Avg: [-884.53 -884.53 -884.53] (1.000)
Step: 33599, Reward: [-1424.693 -1424.693 -1424.693] [0.0000], Avg: [-885.334 -885.334 -885.334] (1.000)
Step: 33649, Reward: [-1807.586 -1807.586 -1807.586] [0.0000], Avg: [-886.704 -886.704 -886.704] (1.000)
Step: 33699, Reward: [-1739.671 -1739.671 -1739.671] [0.0000], Avg: [-887.969 -887.969 -887.969] (1.000)
Step: 33749, Reward: [-2142.891 -2142.891 -2142.891] [0.0000], Avg: [-889.829 -889.829 -889.829] (1.000)
Step: 33799, Reward: [-1836.769 -1836.769 -1836.769] [0.0000], Avg: [-891.229 -891.229 -891.229] (1.000)
Step: 33849, Reward: [-2210.257 -2210.257 -2210.257] [0.0000], Avg: [-893.178 -893.178 -893.178] (1.000)
Step: 33899, Reward: [-2044.187 -2044.187 -2044.187] [0.0000], Avg: [-894.875 -894.875 -894.875] (1.000)
Step: 33949, Reward: [-1850.389 -1850.389 -1850.389] [0.0000], Avg: [-896.283 -896.283 -896.283] (1.000)
Step: 33999, Reward: [-1694.147 -1694.147 -1694.147] [0.0000], Avg: [-897.456 -897.456 -897.456] (1.000)
Step: 34049, Reward: [-1542.24 -1542.24 -1542.24] [0.0000], Avg: [-898.403 -898.403 -898.403] (1.000)
Step: 34099, Reward: [-1282.126 -1282.126 -1282.126] [0.0000], Avg: [-898.965 -898.965 -898.965] (1.000)
Step: 34149, Reward: [-1231.355 -1231.355 -1231.355] [0.0000], Avg: [-899.452 -899.452 -899.452] (1.000)
Step: 34199, Reward: [-1365.05 -1365.05 -1365.05] [0.0000], Avg: [-900.133 -900.133 -900.133] (1.000)
Step: 34249, Reward: [-687.542 -687.542 -687.542] [0.0000], Avg: [-899.822 -899.822 -899.822] (1.000)
Step: 34299, Reward: [-968.444 -968.444 -968.444] [0.0000], Avg: [-899.922 -899.922 -899.922] (1.000)
Step: 34349, Reward: [-969.681 -969.681 -969.681] [0.0000], Avg: [-900.024 -900.024 -900.024] (1.000)
Step: 34399, Reward: [-636.711 -636.711 -636.711] [0.0000], Avg: [-899.641 -899.641 -899.641] (1.000)
Step: 34449, Reward: [-639.477 -639.477 -639.477] [0.0000], Avg: [-899.264 -899.264 -899.264] (1.000)
Step: 34499, Reward: [-649.744 -649.744 -649.744] [0.0000], Avg: [-898.902 -898.902 -898.902] (1.000)
Step: 34549, Reward: [-504.822 -504.822 -504.822] [0.0000], Avg: [-898.332 -898.332 -898.332] (1.000)
Step: 34599, Reward: [-1080.279 -1080.279 -1080.279] [0.0000], Avg: [-898.595 -898.595 -898.595] (1.000)
Step: 34649, Reward: [-936.69 -936.69 -936.69] [0.0000], Avg: [-898.65 -898.65 -898.65] (1.000)
Step: 34699, Reward: [-663.751 -663.751 -663.751] [0.0000], Avg: [-898.311 -898.311 -898.311] (1.000)
Step: 34749, Reward: [-610.596 -610.596 -610.596] [0.0000], Avg: [-897.897 -897.897 -897.897] (1.000)
Step: 34799, Reward: [-699.56 -699.56 -699.56] [0.0000], Avg: [-897.612 -897.612 -897.612] (1.000)
Step: 34849, Reward: [-1031.311 -1031.311 -1031.311] [0.0000], Avg: [-897.804 -897.804 -897.804] (1.000)
Step: 34899, Reward: [-1104.666 -1104.666 -1104.666] [0.0000], Avg: [-898.1 -898.1 -898.1] (1.000)
Step: 34949, Reward: [-1682.252 -1682.252 -1682.252] [0.0000], Avg: [-899.222 -899.222 -899.222] (1.000)
Step: 34999, Reward: [-1662.472 -1662.472 -1662.472] [0.0000], Avg: [-900.313 -900.313 -900.313] (1.000)
Step: 35049, Reward: [-1928.599 -1928.599 -1928.599] [0.0000], Avg: [-901.779 -901.779 -901.779] (1.000)
Step: 35099, Reward: [-1918.024 -1918.024 -1918.024] [0.0000], Avg: [-903.227 -903.227 -903.227] (1.000)
Step: 35149, Reward: [-1818.95 -1818.95 -1818.95] [0.0000], Avg: [-904.53 -904.53 -904.53] (1.000)
Step: 35199, Reward: [-1851.416 -1851.416 -1851.416] [0.0000], Avg: [-905.875 -905.875 -905.875] (1.000)
Step: 35249, Reward: [-2205.816 -2205.816 -2205.816] [0.0000], Avg: [-907.719 -907.719 -907.719] (1.000)
Step: 35299, Reward: [-1917.765 -1917.765 -1917.765] [0.0000], Avg: [-909.149 -909.149 -909.149] (1.000)
Step: 35349, Reward: [-1813.346 -1813.346 -1813.346] [0.0000], Avg: [-910.428 -910.428 -910.428] (1.000)
Step: 35399, Reward: [-1962.983 -1962.983 -1962.983] [0.0000], Avg: [-911.915 -911.915 -911.915] (1.000)
Step: 35449, Reward: [-2019.273 -2019.273 -2019.273] [0.0000], Avg: [-913.477 -913.477 -913.477] (1.000)
Step: 35499, Reward: [-2014.768 -2014.768 -2014.768] [0.0000], Avg: [-915.028 -915.028 -915.028] (1.000)
Step: 35549, Reward: [-2056.879 -2056.879 -2056.879] [0.0000], Avg: [-916.634 -916.634 -916.634] (1.000)
Step: 35599, Reward: [-1999.157 -1999.157 -1999.157] [0.0000], Avg: [-918.154 -918.154 -918.154] (1.000)
Step: 35649, Reward: [-1519.665 -1519.665 -1519.665] [0.0000], Avg: [-918.998 -918.998 -918.998] (1.000)
Step: 35699, Reward: [-1769.224 -1769.224 -1769.224] [0.0000], Avg: [-920.189 -920.189 -920.189] (1.000)
Step: 35749, Reward: [-1564.628 -1564.628 -1564.628] [0.0000], Avg: [-921.09 -921.09 -921.09] (1.000)
Step: 35799, Reward: [-1807.031 -1807.031 -1807.031] [0.0000], Avg: [-922.327 -922.327 -922.327] (1.000)
Step: 35849, Reward: [-2056.615 -2056.615 -2056.615] [0.0000], Avg: [-923.909 -923.909 -923.909] (1.000)
Step: 35899, Reward: [-1732.146 -1732.146 -1732.146] [0.0000], Avg: [-925.035 -925.035 -925.035] (1.000)
Step: 35949, Reward: [-2178.752 -2178.752 -2178.752] [0.0000], Avg: [-926.779 -926.779 -926.779] (1.000)
Step: 35999, Reward: [-2072.627 -2072.627 -2072.627] [0.0000], Avg: [-928.37 -928.37 -928.37] (1.000)
Step: 36049, Reward: [-1829.235 -1829.235 -1829.235] [0.0000], Avg: [-929.62 -929.62 -929.62] (1.000)
Step: 36099, Reward: [-1989.348 -1989.348 -1989.348] [0.0000], Avg: [-931.087 -931.087 -931.087] (1.000)
Step: 36149, Reward: [-1932.628 -1932.628 -1932.628] [0.0000], Avg: [-932.473 -932.473 -932.473] (1.000)
Step: 36199, Reward: [-1724.639 -1724.639 -1724.639] [0.0000], Avg: [-933.567 -933.567 -933.567] (1.000)
Step: 36249, Reward: [-1740.265 -1740.265 -1740.265] [0.0000], Avg: [-934.679 -934.679 -934.679] (1.000)
Step: 36299, Reward: [-1730.017 -1730.017 -1730.017] [0.0000], Avg: [-935.775 -935.775 -935.775] (1.000)
Step: 36349, Reward: [-1420.049 -1420.049 -1420.049] [0.0000], Avg: [-936.441 -936.441 -936.441] (1.000)
Step: 36399, Reward: [-1754.2 -1754.2 -1754.2] [0.0000], Avg: [-937.564 -937.564 -937.564] (1.000)
Step: 36449, Reward: [-963.815 -963.815 -963.815] [0.0000], Avg: [-937.6 -937.6 -937.6] (1.000)
Step: 36499, Reward: [-1239.411 -1239.411 -1239.411] [0.0000], Avg: [-938.014 -938.014 -938.014] (1.000)
Step: 36549, Reward: [-1445.515 -1445.515 -1445.515] [0.0000], Avg: [-938.708 -938.708 -938.708] (1.000)
Step: 36599, Reward: [-1177.127 -1177.127 -1177.127] [0.0000], Avg: [-939.034 -939.034 -939.034] (1.000)
Step: 36649, Reward: [-699.393 -699.393 -699.393] [0.0000], Avg: [-938.707 -938.707 -938.707] (1.000)
Step: 36699, Reward: [-912.92 -912.92 -912.92] [0.0000], Avg: [-938.672 -938.672 -938.672] (1.000)
Step: 36749, Reward: [-1123.189 -1123.189 -1123.189] [0.0000], Avg: [-938.923 -938.923 -938.923] (1.000)
Step: 36799, Reward: [-763.999 -763.999 -763.999] [0.0000], Avg: [-938.685 -938.685 -938.685] (1.000)
Step: 36849, Reward: [-957.477 -957.477 -957.477] [0.0000], Avg: [-938.711 -938.711 -938.711] (1.000)
Step: 36899, Reward: [-1021.694 -1021.694 -1021.694] [0.0000], Avg: [-938.823 -938.823 -938.823] (1.000)
Step: 36949, Reward: [-1035.179 -1035.179 -1035.179] [0.0000], Avg: [-938.953 -938.953 -938.953] (1.000)
Step: 36999, Reward: [-885.518 -885.518 -885.518] [0.0000], Avg: [-938.881 -938.881 -938.881] (1.000)
Step: 37049, Reward: [-1345.166 -1345.166 -1345.166] [0.0000], Avg: [-939.43 -939.43 -939.43] (1.000)
Step: 37099, Reward: [-926.637 -926.637 -926.637] [0.0000], Avg: [-939.412 -939.412 -939.412] (1.000)
Step: 37149, Reward: [-989.217 -989.217 -989.217] [0.0000], Avg: [-939.479 -939.479 -939.479] (1.000)
Step: 37199, Reward: [-1319.945 -1319.945 -1319.945] [0.0000], Avg: [-939.991 -939.991 -939.991] (1.000)
Step: 37249, Reward: [-1222.599 -1222.599 -1222.599] [0.0000], Avg: [-940.37 -940.37 -940.37] (1.000)
Step: 37299, Reward: [-840.369 -840.369 -840.369] [0.0000], Avg: [-940.236 -940.236 -940.236] (1.000)
Step: 37349, Reward: [-1392.343 -1392.343 -1392.343] [0.0000], Avg: [-940.841 -940.841 -940.841] (1.000)
Step: 37399, Reward: [-1618.839 -1618.839 -1618.839] [0.0000], Avg: [-941.748 -941.748 -941.748] (1.000)
Step: 37449, Reward: [-759.561 -759.561 -759.561] [0.0000], Avg: [-941.504 -941.504 -941.504] (1.000)
Step: 37499, Reward: [-855.795 -855.795 -855.795] [0.0000], Avg: [-941.39 -941.39 -941.39] (1.000)
Step: 37549, Reward: [-451.27 -451.27 -451.27] [0.0000], Avg: [-940.737 -940.737 -940.737] (1.000)
Step: 37599, Reward: [-770.15 -770.15 -770.15] [0.0000], Avg: [-940.511 -940.511 -940.511] (1.000)
Step: 37649, Reward: [-562.827 -562.827 -562.827] [0.0000], Avg: [-940.009 -940.009 -940.009] (1.000)
Step: 37699, Reward: [-544.615 -544.615 -544.615] [0.0000], Avg: [-939.485 -939.485 -939.485] (1.000)
Step: 37749, Reward: [-1036.008 -1036.008 -1036.008] [0.0000], Avg: [-939.613 -939.613 -939.613] (1.000)
Step: 37799, Reward: [-556.918 -556.918 -556.918] [0.0000], Avg: [-939.106 -939.106 -939.106] (1.000)
Step: 37849, Reward: [-1633.376 -1633.376 -1633.376] [0.0000], Avg: [-940.023 -940.023 -940.023] (1.000)
Step: 37899, Reward: [-1979.074 -1979.074 -1979.074] [0.0000], Avg: [-941.394 -941.394 -941.394] (1.000)
Step: 37949, Reward: [-1152.704 -1152.704 -1152.704] [0.0000], Avg: [-941.673 -941.673 -941.673] (1.000)
Step: 37999, Reward: [-1107.819 -1107.819 -1107.819] [0.0000], Avg: [-941.891 -941.891 -941.891] (1.000)
Step: 38049, Reward: [-1722.768 -1722.768 -1722.768] [0.0000], Avg: [-942.917 -942.917 -942.917] (1.000)
Step: 38099, Reward: [-1594.904 -1594.904 -1594.904] [0.0000], Avg: [-943.773 -943.773 -943.773] (1.000)
Step: 38149, Reward: [-1653.765 -1653.765 -1653.765] [0.0000], Avg: [-944.704 -944.704 -944.704] (1.000)
Step: 38199, Reward: [-2230.406 -2230.406 -2230.406] [0.0000], Avg: [-946.386 -946.386 -946.386] (1.000)
Step: 38249, Reward: [-1467.648 -1467.648 -1467.648] [0.0000], Avg: [-947.068 -947.068 -947.068] (1.000)
Step: 38299, Reward: [-1852.78 -1852.78 -1852.78] [0.0000], Avg: [-948.25 -948.25 -948.25] (1.000)
Step: 38349, Reward: [-1747.376 -1747.376 -1747.376] [0.0000], Avg: [-949.292 -949.292 -949.292] (1.000)
Step: 38399, Reward: [-2095.291 -2095.291 -2095.291] [0.0000], Avg: [-950.784 -950.784 -950.784] (1.000)
Step: 38449, Reward: [-1603.309 -1603.309 -1603.309] [0.0000], Avg: [-951.633 -951.633 -951.633] (1.000)
Step: 38499, Reward: [-2097.041 -2097.041 -2097.041] [0.0000], Avg: [-953.12 -953.12 -953.12] (1.000)
Step: 38549, Reward: [-1542.377 -1542.377 -1542.377] [0.0000], Avg: [-953.885 -953.885 -953.885] (1.000)
Step: 38599, Reward: [-1693.805 -1693.805 -1693.805] [0.0000], Avg: [-954.843 -954.843 -954.843] (1.000)
Step: 38649, Reward: [-961.287 -961.287 -961.287] [0.0000], Avg: [-954.851 -954.851 -954.851] (1.000)
Step: 38699, Reward: [-930.931 -930.931 -930.931] [0.0000], Avg: [-954.82 -954.82 -954.82] (1.000)
Step: 38749, Reward: [-883.743 -883.743 -883.743] [0.0000], Avg: [-954.729 -954.729 -954.729] (1.000)
Step: 38799, Reward: [-751.171 -751.171 -751.171] [0.0000], Avg: [-954.466 -954.466 -954.466] (1.000)
Step: 38849, Reward: [-709.269 -709.269 -709.269] [0.0000], Avg: [-954.151 -954.151 -954.151] (1.000)
Step: 38899, Reward: [-724.953 -724.953 -724.953] [0.0000], Avg: [-953.856 -953.856 -953.856] (1.000)
Step: 38949, Reward: [-619.333 -619.333 -619.333] [0.0000], Avg: [-953.427 -953.427 -953.427] (1.000)
Step: 38999, Reward: [-790.357 -790.357 -790.357] [0.0000], Avg: [-953.218 -953.218 -953.218] (1.000)
Step: 39049, Reward: [-675.841 -675.841 -675.841] [0.0000], Avg: [-952.863 -952.863 -952.863] (1.000)
Step: 39099, Reward: [-587.507 -587.507 -587.507] [0.0000], Avg: [-952.395 -952.395 -952.395] (1.000)
Step: 39149, Reward: [-782.548 -782.548 -782.548] [0.0000], Avg: [-952.178 -952.178 -952.178] (1.000)
Step: 39199, Reward: [-464.708 -464.708 -464.708] [0.0000], Avg: [-951.557 -951.557 -951.557] (1.000)
Step: 39249, Reward: [-579.678 -579.678 -579.678] [0.0000], Avg: [-951.083 -951.083 -951.083] (1.000)
Step: 39299, Reward: [-650.251 -650.251 -650.251] [0.0000], Avg: [-950.7 -950.7 -950.7] (1.000)
Step: 39349, Reward: [-509.546 -509.546 -509.546] [0.0000], Avg: [-950.14 -950.14 -950.14] (1.000)
Step: 39399, Reward: [-629.079 -629.079 -629.079] [0.0000], Avg: [-949.732 -949.732 -949.732] (1.000)
Step: 39449, Reward: [-557.892 -557.892 -557.892] [0.0000], Avg: [-949.236 -949.236 -949.236] (1.000)
Step: 39499, Reward: [-400.326 -400.326 -400.326] [0.0000], Avg: [-948.541 -948.541 -948.541] (1.000)
Step: 39549, Reward: [-361.22 -361.22 -361.22] [0.0000], Avg: [-947.798 -947.798 -947.798] (1.000)
Step: 39599, Reward: [-675.71 -675.71 -675.71] [0.0000], Avg: [-947.455 -947.455 -947.455] (1.000)
Step: 39649, Reward: [-567.761 -567.761 -567.761] [0.0000], Avg: [-946.976 -946.976 -946.976] (1.000)
Step: 39699, Reward: [-605.752 -605.752 -605.752] [0.0000], Avg: [-946.546 -946.546 -946.546] (1.000)
Step: 39749, Reward: [-744.21 -744.21 -744.21] [0.0000], Avg: [-946.292 -946.292 -946.292] (1.000)
Step: 39799, Reward: [-389.226 -389.226 -389.226] [0.0000], Avg: [-945.592 -945.592 -945.592] (1.000)
Step: 39849, Reward: [-559.75 -559.75 -559.75] [0.0000], Avg: [-945.108 -945.108 -945.108] (1.000)
Step: 39899, Reward: [-379.059 -379.059 -379.059] [0.0000], Avg: [-944.398 -944.398 -944.398] (1.000)
Step: 39949, Reward: [-601.936 -601.936 -601.936] [0.0000], Avg: [-943.97 -943.97 -943.97] (1.000)
Step: 39999, Reward: [-511.839 -511.839 -511.839] [0.0000], Avg: [-943.43 -943.43 -943.43] (1.000)
Step: 40049, Reward: [-709.357 -709.357 -709.357] [0.0000], Avg: [-943.137 -943.137 -943.137] (1.000)
Step: 40099, Reward: [-536.772 -536.772 -536.772] [0.0000], Avg: [-942.631 -942.631 -942.631] (1.000)
Step: 40149, Reward: [-736.579 -736.579 -736.579] [0.0000], Avg: [-942.374 -942.374 -942.374] (1.000)
Step: 40199, Reward: [-522.827 -522.827 -522.827] [0.0000], Avg: [-941.852 -941.852 -941.852] (1.000)
Step: 40249, Reward: [-591.801 -591.801 -591.801] [0.0000], Avg: [-941.417 -941.417 -941.417] (1.000)
Step: 40299, Reward: [-865.051 -865.051 -865.051] [0.0000], Avg: [-941.323 -941.323 -941.323] (1.000)
Step: 40349, Reward: [-761.999 -761.999 -761.999] [0.0000], Avg: [-941.1 -941.1 -941.1] (1.000)
Step: 40399, Reward: [-433.896 -433.896 -433.896] [0.0000], Avg: [-940.473 -940.473 -940.473] (1.000)
Step: 40449, Reward: [-624.814 -624.814 -624.814] [0.0000], Avg: [-940.083 -940.083 -940.083] (1.000)
Step: 40499, Reward: [-455.005 -455.005 -455.005] [0.0000], Avg: [-939.484 -939.484 -939.484] (1.000)
Step: 40549, Reward: [-471.081 -471.081 -471.081] [0.0000], Avg: [-938.906 -938.906 -938.906] (1.000)
Step: 40599, Reward: [-356.898 -356.898 -356.898] [0.0000], Avg: [-938.189 -938.189 -938.189] (1.000)
Step: 40649, Reward: [-614.65 -614.65 -614.65] [0.0000], Avg: [-937.791 -937.791 -937.791] (1.000)
Step: 40699, Reward: [-495.634 -495.634 -495.634] [0.0000], Avg: [-937.248 -937.248 -937.248] (1.000)
Step: 40749, Reward: [-850.945 -850.945 -850.945] [0.0000], Avg: [-937.142 -937.142 -937.142] (1.000)
Step: 40799, Reward: [-564.448 -564.448 -564.448] [0.0000], Avg: [-936.686 -936.686 -936.686] (1.000)
Step: 40849, Reward: [-471.96 -471.96 -471.96] [0.0000], Avg: [-936.117 -936.117 -936.117] (1.000)
Step: 40899, Reward: [-560.376 -560.376 -560.376] [0.0000], Avg: [-935.657 -935.657 -935.657] (1.000)
Step: 40949, Reward: [-685.022 -685.022 -685.022] [0.0000], Avg: [-935.351 -935.351 -935.351] (1.000)
Step: 40999, Reward: [-488.947 -488.947 -488.947] [0.0000], Avg: [-934.807 -934.807 -934.807] (1.000)
Step: 41049, Reward: [-470.587 -470.587 -470.587] [0.0000], Avg: [-934.242 -934.242 -934.242] (1.000)
Step: 41099, Reward: [-533.88 -533.88 -533.88] [0.0000], Avg: [-933.755 -933.755 -933.755] (1.000)
Step: 41149, Reward: [-589.496 -589.496 -589.496] [0.0000], Avg: [-933.336 -933.336 -933.336] (1.000)
Step: 41199, Reward: [-490.41 -490.41 -490.41] [0.0000], Avg: [-932.799 -932.799 -932.799] (1.000)
Step: 41249, Reward: [-475.163 -475.163 -475.163] [0.0000], Avg: [-932.244 -932.244 -932.244] (1.000)
Step: 41299, Reward: [-453.694 -453.694 -453.694] [0.0000], Avg: [-931.665 -931.665 -931.665] (1.000)
Step: 41349, Reward: [-382.88 -382.88 -382.88] [0.0000], Avg: [-931.001 -931.001 -931.001] (1.000)
Step: 41399, Reward: [-491.679 -491.679 -491.679] [0.0000], Avg: [-930.47 -930.47 -930.47] (1.000)
Step: 41449, Reward: [-499.637 -499.637 -499.637] [0.0000], Avg: [-929.951 -929.951 -929.951] (1.000)
Step: 41499, Reward: [-432.975 -432.975 -432.975] [0.0000], Avg: [-929.352 -929.352 -929.352] (1.000)
Step: 41549, Reward: [-621.79 -621.79 -621.79] [0.0000], Avg: [-928.982 -928.982 -928.982] (1.000)
Step: 41599, Reward: [-362.245 -362.245 -362.245] [0.0000], Avg: [-928.301 -928.301 -928.301] (1.000)
Step: 41649, Reward: [-750.722 -750.722 -750.722] [0.0000], Avg: [-928.088 -928.088 -928.088] (1.000)
Step: 41699, Reward: [-548.245 -548.245 -548.245] [0.0000], Avg: [-927.632 -927.632 -927.632] (1.000)
Step: 41749, Reward: [-364.135 -364.135 -364.135] [0.0000], Avg: [-926.957 -926.957 -926.957] (1.000)
Step: 41799, Reward: [-617.454 -617.454 -617.454] [0.0000], Avg: [-926.587 -926.587 -926.587] (1.000)
Step: 41849, Reward: [-540.926 -540.926 -540.926] [0.0000], Avg: [-926.126 -926.126 -926.126] (1.000)
Step: 41899, Reward: [-514.65 -514.65 -514.65] [0.0000], Avg: [-925.635 -925.635 -925.635] (1.000)
Step: 41949, Reward: [-486.991 -486.991 -486.991] [0.0000], Avg: [-925.112 -925.112 -925.112] (1.000)
Step: 41999, Reward: [-484.475 -484.475 -484.475] [0.0000], Avg: [-924.588 -924.588 -924.588] (1.000)
Step: 42049, Reward: [-465.851 -465.851 -465.851] [0.0000], Avg: [-924.042 -924.042 -924.042] (1.000)
Step: 42099, Reward: [-502.562 -502.562 -502.562] [0.0000], Avg: [-923.542 -923.542 -923.542] (1.000)
Step: 42149, Reward: [-557.017 -557.017 -557.017] [0.0000], Avg: [-923.107 -923.107 -923.107] (1.000)
Step: 42199, Reward: [-393.37 -393.37 -393.37] [0.0000], Avg: [-922.479 -922.479 -922.479] (1.000)
Step: 42249, Reward: [-511.903 -511.903 -511.903] [0.0000], Avg: [-921.993 -921.993 -921.993] (1.000)
Step: 42299, Reward: [-430.99 -430.99 -430.99] [0.0000], Avg: [-921.413 -921.413 -921.413] (1.000)
Step: 42349, Reward: [-499.068 -499.068 -499.068] [0.0000], Avg: [-920.914 -920.914 -920.914] (1.000)
Step: 42399, Reward: [-561.216 -561.216 -561.216] [0.0000], Avg: [-920.49 -920.49 -920.49] (1.000)
Step: 42449, Reward: [-311.893 -311.893 -311.893] [0.0000], Avg: [-919.773 -919.773 -919.773] (1.000)
Step: 42499, Reward: [-465.159 -465.159 -465.159] [0.0000], Avg: [-919.239 -919.239 -919.239] (1.000)
Step: 42549, Reward: [-531.281 -531.281 -531.281] [0.0000], Avg: [-918.783 -918.783 -918.783] (1.000)
Step: 42599, Reward: [-549.455 -549.455 -549.455] [0.0000], Avg: [-918.349 -918.349 -918.349] (1.000)
Step: 42649, Reward: [-622.784 -622.784 -622.784] [0.0000], Avg: [-918.003 -918.003 -918.003] (1.000)
Step: 42699, Reward: [-605.907 -605.907 -605.907] [0.0000], Avg: [-917.637 -917.637 -917.637] (1.000)
Step: 42749, Reward: [-426.796 -426.796 -426.796] [0.0000], Avg: [-917.063 -917.063 -917.063] (1.000)
Step: 42799, Reward: [-431.257 -431.257 -431.257] [0.0000], Avg: [-916.496 -916.496 -916.496] (1.000)
Step: 42849, Reward: [-500.969 -500.969 -500.969] [0.0000], Avg: [-916.011 -916.011 -916.011] (1.000)
Step: 42899, Reward: [-511.506 -511.506 -511.506] [0.0000], Avg: [-915.539 -915.539 -915.539] (1.000)
Step: 42949, Reward: [-569.06 -569.06 -569.06] [0.0000], Avg: [-915.136 -915.136 -915.136] (1.000)
Step: 42999, Reward: [-473.893 -473.893 -473.893] [0.0000], Avg: [-914.623 -914.623 -914.623] (1.000)
Step: 43049, Reward: [-375.81 -375.81 -375.81] [0.0000], Avg: [-913.997 -913.997 -913.997] (1.000)
Step: 43099, Reward: [-641.634 -641.634 -641.634] [0.0000], Avg: [-913.681 -913.681 -913.681] (1.000)
Step: 43149, Reward: [-489.316 -489.316 -489.316] [0.0000], Avg: [-913.189 -913.189 -913.189] (1.000)
Step: 43199, Reward: [-504.485 -504.485 -504.485] [0.0000], Avg: [-912.716 -912.716 -912.716] (1.000)
Step: 43249, Reward: [-525.334 -525.334 -525.334] [0.0000], Avg: [-912.269 -912.269 -912.269] (1.000)
Step: 43299, Reward: [-503.155 -503.155 -503.155] [0.0000], Avg: [-911.796 -911.796 -911.796] (1.000)
Step: 43349, Reward: [-467.034 -467.034 -467.034] [0.0000], Avg: [-911.283 -911.283 -911.283] (1.000)
Step: 43399, Reward: [-758.23 -758.23 -758.23] [0.0000], Avg: [-911.107 -911.107 -911.107] (1.000)
Step: 43449, Reward: [-722.423 -722.423 -722.423] [0.0000], Avg: [-910.89 -910.89 -910.89] (1.000)
Step: 43499, Reward: [-652.031 -652.031 -652.031] [0.0000], Avg: [-910.592 -910.592 -910.592] (1.000)
Step: 43549, Reward: [-476.275 -476.275 -476.275] [0.0000], Avg: [-910.094 -910.094 -910.094] (1.000)
Step: 43599, Reward: [-586.324 -586.324 -586.324] [0.0000], Avg: [-909.722 -909.722 -909.722] (1.000)
Step: 43649, Reward: [-641.142 -641.142 -641.142] [0.0000], Avg: [-909.415 -909.415 -909.415] (1.000)
Step: 43699, Reward: [-581.302 -581.302 -581.302] [0.0000], Avg: [-909.039 -909.039 -909.039] (1.000)
Step: 43749, Reward: [-547.069 -547.069 -547.069] [0.0000], Avg: [-908.625 -908.625 -908.625] (1.000)
Step: 43799, Reward: [-382.622 -382.622 -382.622] [0.0000], Avg: [-908.025 -908.025 -908.025] (1.000)
Step: 43849, Reward: [-497.353 -497.353 -497.353] [0.0000], Avg: [-907.557 -907.557 -907.557] (1.000)
Step: 43899, Reward: [-653.832 -653.832 -653.832] [0.0000], Avg: [-907.268 -907.268 -907.268] (1.000)
Step: 43949, Reward: [-421.588 -421.588 -421.588] [0.0000], Avg: [-906.715 -906.715 -906.715] (1.000)
Step: 43999, Reward: [-512.606 -512.606 -512.606] [0.0000], Avg: [-906.267 -906.267 -906.267] (1.000)
Step: 44049, Reward: [-539.088 -539.088 -539.088] [0.0000], Avg: [-905.851 -905.851 -905.851] (1.000)
Step: 44099, Reward: [-677.964 -677.964 -677.964] [0.0000], Avg: [-905.592 -905.592 -905.592] (1.000)
Step: 44149, Reward: [-615.459 -615.459 -615.459] [0.0000], Avg: [-905.264 -905.264 -905.264] (1.000)
Step: 44199, Reward: [-595.659 -595.659 -595.659] [0.0000], Avg: [-904.913 -904.913 -904.913] (1.000)
Step: 44249, Reward: [-666.348 -666.348 -666.348] [0.0000], Avg: [-904.644 -904.644 -904.644] (1.000)
Step: 44299, Reward: [-638.998 -638.998 -638.998] [0.0000], Avg: [-904.344 -904.344 -904.344] (1.000)
Step: 44349, Reward: [-449.033 -449.033 -449.033] [0.0000], Avg: [-903.831 -903.831 -903.831] (1.000)
Step: 44399, Reward: [-683.798 -683.798 -683.798] [0.0000], Avg: [-903.583 -903.583 -903.583] (1.000)
Step: 44449, Reward: [-667.147 -667.147 -667.147] [0.0000], Avg: [-903.317 -903.317 -903.317] (1.000)
Step: 44499, Reward: [-593.491 -593.491 -593.491] [0.0000], Avg: [-902.969 -902.969 -902.969] (1.000)
Step: 44549, Reward: [-841.733 -841.733 -841.733] [0.0000], Avg: [-902.9 -902.9 -902.9] (1.000)
Step: 44599, Reward: [-607.97 -607.97 -607.97] [0.0000], Avg: [-902.569 -902.569 -902.569] (1.000)
Step: 44649, Reward: [-766.682 -766.682 -766.682] [0.0000], Avg: [-902.417 -902.417 -902.417] (1.000)
Step: 44699, Reward: [-825.662 -825.662 -825.662] [0.0000], Avg: [-902.331 -902.331 -902.331] (1.000)
Step: 44749, Reward: [-467.87 -467.87 -467.87] [0.0000], Avg: [-901.846 -901.846 -901.846] (1.000)
Step: 44799, Reward: [-1158.937 -1158.937 -1158.937] [0.0000], Avg: [-902.133 -902.133 -902.133] (1.000)
Step: 44849, Reward: [-622.287 -622.287 -622.287] [0.0000], Avg: [-901.821 -901.821 -901.821] (1.000)
Step: 44899, Reward: [-985.797 -985.797 -985.797] [0.0000], Avg: [-901.914 -901.914 -901.914] (1.000)
Step: 44949, Reward: [-434.722 -434.722 -434.722] [0.0000], Avg: [-901.395 -901.395 -901.395] (1.000)
Step: 44999, Reward: [-1528.785 -1528.785 -1528.785] [0.0000], Avg: [-902.092 -902.092 -902.092] (1.000)
Step: 45049, Reward: [-538.835 -538.835 -538.835] [0.0000], Avg: [-901.689 -901.689 -901.689] (1.000)
Step: 45099, Reward: [-718.47 -718.47 -718.47] [0.0000], Avg: [-901.486 -901.486 -901.486] (1.000)
Step: 45149, Reward: [-1011.985 -1011.985 -1011.985] [0.0000], Avg: [-901.608 -901.608 -901.608] (1.000)
Step: 45199, Reward: [-1262.994 -1262.994 -1262.994] [0.0000], Avg: [-902.008 -902.008 -902.008] (1.000)
Step: 45249, Reward: [-1233.186 -1233.186 -1233.186] [0.0000], Avg: [-902.374 -902.374 -902.374] (1.000)
Step: 45299, Reward: [-1468.948 -1468.948 -1468.948] [0.0000], Avg: [-902.999 -902.999 -902.999] (1.000)
Step: 45349, Reward: [-1651.836 -1651.836 -1651.836] [0.0000], Avg: [-903.825 -903.825 -903.825] (1.000)
Step: 45399, Reward: [-802.912 -802.912 -802.912] [0.0000], Avg: [-903.714 -903.714 -903.714] (1.000)
Step: 45449, Reward: [-1495.734 -1495.734 -1495.734] [0.0000], Avg: [-904.365 -904.365 -904.365] (1.000)
Step: 45499, Reward: [-1095.318 -1095.318 -1095.318] [0.0000], Avg: [-904.575 -904.575 -904.575] (1.000)
Step: 45549, Reward: [-1645.963 -1645.963 -1645.963] [0.0000], Avg: [-905.388 -905.388 -905.388] (1.000)
Step: 45599, Reward: [-1476.042 -1476.042 -1476.042] [0.0000], Avg: [-906.014 -906.014 -906.014] (1.000)
Step: 45649, Reward: [-1760.816 -1760.816 -1760.816] [0.0000], Avg: [-906.95 -906.95 -906.95] (1.000)
Step: 45699, Reward: [-1770.793 -1770.793 -1770.793] [0.0000], Avg: [-907.896 -907.896 -907.896] (1.000)
Step: 45749, Reward: [-1580.52 -1580.52 -1580.52] [0.0000], Avg: [-908.631 -908.631 -908.631] (1.000)
Step: 45799, Reward: [-1562.908 -1562.908 -1562.908] [0.0000], Avg: [-909.345 -909.345 -909.345] (1.000)
Step: 45849, Reward: [-1825.303 -1825.303 -1825.303] [0.0000], Avg: [-910.344 -910.344 -910.344] (1.000)
Step: 45899, Reward: [-1757.301 -1757.301 -1757.301] [0.0000], Avg: [-911.266 -911.266 -911.266] (1.000)
Step: 45949, Reward: [-1708.085 -1708.085 -1708.085] [0.0000], Avg: [-912.133 -912.133 -912.133] (1.000)
Step: 45999, Reward: [-1883.245 -1883.245 -1883.245] [0.0000], Avg: [-913.189 -913.189 -913.189] (1.000)
Step: 46049, Reward: [-1538.64 -1538.64 -1538.64] [0.0000], Avg: [-913.868 -913.868 -913.868] (1.000)
Step: 46099, Reward: [-1625.913 -1625.913 -1625.913] [0.0000], Avg: [-914.64 -914.64 -914.64] (1.000)
Step: 46149, Reward: [-1728.851 -1728.851 -1728.851] [0.0000], Avg: [-915.523 -915.523 -915.523] (1.000)
Step: 46199, Reward: [-1512.423 -1512.423 -1512.423] [0.0000], Avg: [-916.169 -916.169 -916.169] (1.000)
Step: 46249, Reward: [-1605.445 -1605.445 -1605.445] [0.0000], Avg: [-916.914 -916.914 -916.914] (1.000)
Step: 46299, Reward: [-1562.006 -1562.006 -1562.006] [0.0000], Avg: [-917.61 -917.61 -917.61] (1.000)
Step: 46349, Reward: [-1514.153 -1514.153 -1514.153] [0.0000], Avg: [-918.254 -918.254 -918.254] (1.000)
Step: 46399, Reward: [-1535.224 -1535.224 -1535.224] [0.0000], Avg: [-918.919 -918.919 -918.919] (1.000)
Step: 46449, Reward: [-1415.818 -1415.818 -1415.818] [0.0000], Avg: [-919.454 -919.454 -919.454] (1.000)
Step: 46499, Reward: [-1613.975 -1613.975 -1613.975] [0.0000], Avg: [-920.2 -920.2 -920.2] (1.000)
Step: 46549, Reward: [-1328.789 -1328.789 -1328.789] [0.0000], Avg: [-920.639 -920.639 -920.639] (1.000)
Step: 46599, Reward: [-1580.327 -1580.327 -1580.327] [0.0000], Avg: [-921.347 -921.347 -921.347] (1.000)
Step: 46649, Reward: [-1467.88 -1467.88 -1467.88] [0.0000], Avg: [-921.933 -921.933 -921.933] (1.000)
Step: 46699, Reward: [-1788.854 -1788.854 -1788.854] [0.0000], Avg: [-922.861 -922.861 -922.861] (1.000)
Step: 46749, Reward: [-1663.904 -1663.904 -1663.904] [0.0000], Avg: [-923.654 -923.654 -923.654] (1.000)
Step: 46799, Reward: [-2069.581 -2069.581 -2069.581] [0.0000], Avg: [-924.878 -924.878 -924.878] (1.000)
Step: 46849, Reward: [-2159.068 -2159.068 -2159.068] [0.0000], Avg: [-926.195 -926.195 -926.195] (1.000)
Step: 46899, Reward: [-1797.107 -1797.107 -1797.107] [0.0000], Avg: [-927.124 -927.124 -927.124] (1.000)
Step: 46949, Reward: [-2107.116 -2107.116 -2107.116] [0.0000], Avg: [-928.38 -928.38 -928.38] (1.000)
Step: 46999, Reward: [-1912.93 -1912.93 -1912.93] [0.0000], Avg: [-929.428 -929.428 -929.428] (1.000)
Step: 47049, Reward: [-1743.351 -1743.351 -1743.351] [0.0000], Avg: [-930.293 -930.293 -930.293] (1.000)
Step: 47099, Reward: [-1828.834 -1828.834 -1828.834] [0.0000], Avg: [-931.246 -931.246 -931.246] (1.000)
Step: 47149, Reward: [-2065.303 -2065.303 -2065.303] [0.0000], Avg: [-932.449 -932.449 -932.449] (1.000)
Step: 47199, Reward: [-2116.426 -2116.426 -2116.426] [0.0000], Avg: [-933.703 -933.703 -933.703] (1.000)
Step: 47249, Reward: [-1647.662 -1647.662 -1647.662] [0.0000], Avg: [-934.459 -934.459 -934.459] (1.000)
Step: 47299, Reward: [-1789.554 -1789.554 -1789.554] [0.0000], Avg: [-935.363 -935.363 -935.363] (1.000)
Step: 47349, Reward: [-1861.108 -1861.108 -1861.108] [0.0000], Avg: [-936.34 -936.34 -936.34] (1.000)
Step: 47399, Reward: [-1785.466 -1785.466 -1785.466] [0.0000], Avg: [-937.236 -937.236 -937.236] (1.000)
Step: 47449, Reward: [-2140.109 -2140.109 -2140.109] [0.0000], Avg: [-938.503 -938.503 -938.503] (1.000)
Step: 47499, Reward: [-1966.587 -1966.587 -1966.587] [0.0000], Avg: [-939.586 -939.586 -939.586] (1.000)
Step: 47549, Reward: [-2375.535 -2375.535 -2375.535] [0.0000], Avg: [-941.096 -941.096 -941.096] (1.000)
Step: 47599, Reward: [-1920.576 -1920.576 -1920.576] [0.0000], Avg: [-942.124 -942.124 -942.124] (1.000)
Step: 47649, Reward: [-1554.769 -1554.769 -1554.769] [0.0000], Avg: [-942.767 -942.767 -942.767] (1.000)
Step: 47699, Reward: [-1843.598 -1843.598 -1843.598] [0.0000], Avg: [-943.712 -943.712 -943.712] (1.000)
Step: 47749, Reward: [-1758.847 -1758.847 -1758.847] [0.0000], Avg: [-944.565 -944.565 -944.565] (1.000)
Step: 47799, Reward: [-1563.74 -1563.74 -1563.74] [0.0000], Avg: [-945.213 -945.213 -945.213] (1.000)
Step: 47849, Reward: [-2086.427 -2086.427 -2086.427] [0.0000], Avg: [-946.405 -946.405 -946.405] (1.000)
Step: 47899, Reward: [-1805.791 -1805.791 -1805.791] [0.0000], Avg: [-947.302 -947.302 -947.302] (1.000)
Step: 47949, Reward: [-2094.019 -2094.019 -2094.019] [0.0000], Avg: [-948.498 -948.498 -948.498] (1.000)
Step: 47999, Reward: [-1743.806 -1743.806 -1743.806] [0.0000], Avg: [-949.326 -949.326 -949.326] (1.000)
Step: 48049, Reward: [-2073.073 -2073.073 -2073.073] [0.0000], Avg: [-950.496 -950.496 -950.496] (1.000)
Step: 48099, Reward: [-2181.042 -2181.042 -2181.042] [0.0000], Avg: [-951.775 -951.775 -951.775] (1.000)
Step: 48149, Reward: [-1950.594 -1950.594 -1950.594] [0.0000], Avg: [-952.812 -952.812 -952.812] (1.000)
Step: 48199, Reward: [-1871.924 -1871.924 -1871.924] [0.0000], Avg: [-953.766 -953.766 -953.766] (1.000)
