Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7fa73d5eda20>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7fa73d5edac8>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7fa73d5edb38>],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01
DISCOUNT_RATE = 0.95

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			target_value = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			actual_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (actual_value - target_value.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			curr_pol_out = agent.get_action(states[i], grad=True, numpy=False)
			curr_pol_vf_in = gsoftmax(curr_pol_out, hard=True)
			action = [curr_pol_vf_in if j==i else one_hot(model.get_action(ob, numpy=False)) for (j,model), ob in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -agent.critic_local(states_joint, action_joint).mean() + 0.001*curr_pol_out.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)
		# self.agent = MADDPG(state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.network.optimize(states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-400.202 -400.202 -400.202] [0.0000], Avg: [-400.202 -400.202 -400.202] (1.000)
Step: 99, Reward: [-376.136 -376.136 -376.136] [0.0000], Avg: [-388.169 -388.169 -388.169] (1.000)
Step: 149, Reward: [-574.762 -574.762 -574.762] [0.0000], Avg: [-450.367 -450.367 -450.367] (1.000)
Step: 199, Reward: [-492.251 -492.251 -492.251] [0.0000], Avg: [-460.838 -460.838 -460.838] (1.000)
Step: 249, Reward: [-531.794 -531.794 -531.794] [0.0000], Avg: [-475.029 -475.029 -475.029] (1.000)
Step: 299, Reward: [-332.853 -332.853 -332.853] [0.0000], Avg: [-451.333 -451.333 -451.333] (1.000)
Step: 349, Reward: [-459.953 -459.953 -459.953] [0.0000], Avg: [-452.564 -452.564 -452.564] (1.000)
Step: 399, Reward: [-610.793 -610.793 -610.793] [0.0000], Avg: [-472.343 -472.343 -472.343] (1.000)
Step: 449, Reward: [-581.331 -581.331 -581.331] [0.0000], Avg: [-484.453 -484.453 -484.453] (1.000)
Step: 499, Reward: [-500.486 -500.486 -500.486] [0.0000], Avg: [-486.056 -486.056 -486.056] (1.000)
Step: 549, Reward: [-494.3 -494.3 -494.3] [0.0000], Avg: [-486.806 -486.806 -486.806] (1.000)
Step: 599, Reward: [-543.51 -543.51 -543.51] [0.0000], Avg: [-491.531 -491.531 -491.531] (1.000)
Step: 649, Reward: [-673.929 -673.929 -673.929] [0.0000], Avg: [-505.562 -505.562 -505.562] (1.000)
Step: 699, Reward: [-408.962 -408.962 -408.962] [0.0000], Avg: [-498.662 -498.662 -498.662] (1.000)
Step: 749, Reward: [-347.957 -347.957 -347.957] [0.0000], Avg: [-488.615 -488.615 -488.615] (1.000)
Step: 799, Reward: [-380.494 -380.494 -380.494] [0.0000], Avg: [-481.857 -481.857 -481.857] (1.000)
Step: 849, Reward: [-575.726 -575.726 -575.726] [0.0000], Avg: [-487.379 -487.379 -487.379] (1.000)
Step: 899, Reward: [-511.163 -511.163 -511.163] [0.0000], Avg: [-488.7 -488.7 -488.7] (1.000)
Step: 949, Reward: [-708.645 -708.645 -708.645] [0.0000], Avg: [-500.276 -500.276 -500.276] (1.000)
Step: 999, Reward: [-495.227 -495.227 -495.227] [0.0000], Avg: [-500.024 -500.024 -500.024] (1.000)
Step: 1049, Reward: [-557.274 -557.274 -557.274] [0.0000], Avg: [-502.75 -502.75 -502.75] (1.000)
Step: 1099, Reward: [-576.324 -576.324 -576.324] [0.0000], Avg: [-506.094 -506.094 -506.094] (1.000)
Step: 1149, Reward: [-507.32 -507.32 -507.32] [0.0000], Avg: [-506.148 -506.148 -506.148] (1.000)
Step: 1199, Reward: [-496.057 -496.057 -496.057] [0.0000], Avg: [-505.727 -505.727 -505.727] (1.000)
Step: 1249, Reward: [-621.618 -621.618 -621.618] [0.0000], Avg: [-510.363 -510.363 -510.363] (1.000)
Step: 1299, Reward: [-630.493 -630.493 -630.493] [0.0000], Avg: [-514.983 -514.983 -514.983] (1.000)
Step: 1349, Reward: [-865.83 -865.83 -865.83] [0.0000], Avg: [-527.977 -527.977 -527.977] (1.000)
Step: 1399, Reward: [-768.746 -768.746 -768.746] [0.0000], Avg: [-536.576 -536.576 -536.576] (1.000)
Step: 1449, Reward: [-819.193 -819.193 -819.193] [0.0000], Avg: [-546.322 -546.322 -546.322] (1.000)
Step: 1499, Reward: [-724.776 -724.776 -724.776] [0.0000], Avg: [-552.27 -552.27 -552.27] (1.000)
Step: 1549, Reward: [-1033.487 -1033.487 -1033.487] [0.0000], Avg: [-567.793 -567.793 -567.793] (1.000)
Step: 1599, Reward: [-730.909 -730.909 -730.909] [0.0000], Avg: [-572.891 -572.891 -572.891] (1.000)
Step: 1649, Reward: [-1054.805 -1054.805 -1054.805] [0.0000], Avg: [-587.494 -587.494 -587.494] (1.000)
Step: 1699, Reward: [-1150.805 -1150.805 -1150.805] [0.0000], Avg: [-604.062 -604.062 -604.062] (1.000)
Step: 1749, Reward: [-1086.34 -1086.34 -1086.34] [0.0000], Avg: [-617.841 -617.841 -617.841] (1.000)
Step: 1799, Reward: [-953.63 -953.63 -953.63] [0.0000], Avg: [-627.169 -627.169 -627.169] (1.000)
Step: 1849, Reward: [-1231.558 -1231.558 -1231.558] [0.0000], Avg: [-643.504 -643.504 -643.504] (1.000)
Step: 1899, Reward: [-918.23 -918.23 -918.23] [0.0000], Avg: [-650.733 -650.733 -650.733] (1.000)
Step: 1949, Reward: [-864.805 -864.805 -864.805] [0.0000], Avg: [-656.222 -656.222 -656.222] (1.000)
Step: 1999, Reward: [-1344.204 -1344.204 -1344.204] [0.0000], Avg: [-673.422 -673.422 -673.422] (1.000)
Step: 2049, Reward: [-682.097 -682.097 -682.097] [0.0000], Avg: [-673.634 -673.634 -673.634] (1.000)
Step: 2099, Reward: [-1452.988 -1452.988 -1452.988] [0.0000], Avg: [-692.19 -692.19 -692.19] (1.000)
Step: 2149, Reward: [-1847.836 -1847.836 -1847.836] [0.0000], Avg: [-719.065 -719.065 -719.065] (1.000)
Step: 2199, Reward: [-1021.821 -1021.821 -1021.821] [0.0000], Avg: [-725.946 -725.946 -725.946] (1.000)
Step: 2249, Reward: [-1477.65 -1477.65 -1477.65] [0.0000], Avg: [-742.65 -742.65 -742.65] (1.000)
Step: 2299, Reward: [-853.946 -853.946 -853.946] [0.0000], Avg: [-745.07 -745.07 -745.07] (1.000)
Step: 2349, Reward: [-581.875 -581.875 -581.875] [0.0000], Avg: [-741.598 -741.598 -741.598] (1.000)
Step: 2399, Reward: [-897.163 -897.163 -897.163] [0.0000], Avg: [-744.839 -744.839 -744.839] (1.000)
Step: 2449, Reward: [-1706.285 -1706.285 -1706.285] [0.0000], Avg: [-764.46 -764.46 -764.46] (1.000)
Step: 2499, Reward: [-1313.181 -1313.181 -1313.181] [0.0000], Avg: [-775.434 -775.434 -775.434] (1.000)
Step: 2549, Reward: [-770.731 -770.731 -770.731] [0.0000], Avg: [-775.342 -775.342 -775.342] (1.000)
Step: 2599, Reward: [-1413.019 -1413.019 -1413.019] [0.0000], Avg: [-787.605 -787.605 -787.605] (1.000)
Step: 2649, Reward: [-1262.324 -1262.324 -1262.324] [0.0000], Avg: [-796.562 -796.562 -796.562] (1.000)
Step: 2699, Reward: [-1139.323 -1139.323 -1139.323] [0.0000], Avg: [-802.91 -802.91 -802.91] (1.000)
Step: 2749, Reward: [-1083.348 -1083.348 -1083.348] [0.0000], Avg: [-808.008 -808.008 -808.008] (1.000)
Step: 2799, Reward: [-1320.121 -1320.121 -1320.121] [0.0000], Avg: [-817.153 -817.153 -817.153] (1.000)
Step: 2849, Reward: [-1093.712 -1093.712 -1093.712] [0.0000], Avg: [-822.005 -822.005 -822.005] (1.000)
Step: 2899, Reward: [-1088.685 -1088.685 -1088.685] [0.0000], Avg: [-826.603 -826.603 -826.603] (1.000)
Step: 2949, Reward: [-953.719 -953.719 -953.719] [0.0000], Avg: [-828.758 -828.758 -828.758] (1.000)
Step: 2999, Reward: [-995.279 -995.279 -995.279] [0.0000], Avg: [-831.533 -831.533 -831.533] (1.000)
Step: 3049, Reward: [-934.165 -934.165 -934.165] [0.0000], Avg: [-833.215 -833.215 -833.215] (1.000)
Step: 3099, Reward: [-873.306 -873.306 -873.306] [0.0000], Avg: [-833.862 -833.862 -833.862] (1.000)
Step: 3149, Reward: [-658.058 -658.058 -658.058] [0.0000], Avg: [-831.072 -831.072 -831.072] (1.000)
Step: 3199, Reward: [-465.62 -465.62 -465.62] [0.0000], Avg: [-825.361 -825.361 -825.361] (1.000)
Step: 3249, Reward: [-430.666 -430.666 -430.666] [0.0000], Avg: [-819.289 -819.289 -819.289] (1.000)
Step: 3299, Reward: [-482.484 -482.484 -482.484] [0.0000], Avg: [-814.186 -814.186 -814.186] (1.000)
Step: 3349, Reward: [-721.695 -721.695 -721.695] [0.0000], Avg: [-812.806 -812.806 -812.806] (1.000)
Step: 3399, Reward: [-605.204 -605.204 -605.204] [0.0000], Avg: [-809.753 -809.753 -809.753] (1.000)
Step: 3449, Reward: [-637.962 -637.962 -637.962] [0.0000], Avg: [-807.263 -807.263 -807.263] (1.000)
Step: 3499, Reward: [-510.587 -510.587 -510.587] [0.0000], Avg: [-803.025 -803.025 -803.025] (1.000)
Step: 3549, Reward: [-483.946 -483.946 -483.946] [0.0000], Avg: [-798.531 -798.531 -798.531] (1.000)
Step: 3599, Reward: [-644.148 -644.148 -644.148] [0.0000], Avg: [-796.386 -796.386 -796.386] (1.000)
Step: 3649, Reward: [-397.16 -397.16 -397.16] [0.0000], Avg: [-790.918 -790.918 -790.918] (1.000)
Step: 3699, Reward: [-724.402 -724.402 -724.402] [0.0000], Avg: [-790.019 -790.019 -790.019] (1.000)
Step: 3749, Reward: [-496.341 -496.341 -496.341] [0.0000], Avg: [-786.103 -786.103 -786.103] (1.000)
Step: 3799, Reward: [-516.707 -516.707 -516.707] [0.0000], Avg: [-782.558 -782.558 -782.558] (1.000)
Step: 3849, Reward: [-680.027 -680.027 -680.027] [0.0000], Avg: [-781.227 -781.227 -781.227] (1.000)
Step: 3899, Reward: [-757.549 -757.549 -757.549] [0.0000], Avg: [-780.923 -780.923 -780.923] (1.000)
Step: 3949, Reward: [-467.179 -467.179 -467.179] [0.0000], Avg: [-776.952 -776.952 -776.952] (1.000)
Step: 3999, Reward: [-612.904 -612.904 -612.904] [0.0000], Avg: [-774.901 -774.901 -774.901] (1.000)
Step: 4049, Reward: [-756.733 -756.733 -756.733] [0.0000], Avg: [-774.677 -774.677 -774.677] (1.000)
Step: 4099, Reward: [-762.285 -762.285 -762.285] [0.0000], Avg: [-774.526 -774.526 -774.526] (1.000)
Step: 4149, Reward: [-566.328 -566.328 -566.328] [0.0000], Avg: [-772.017 -772.017 -772.017] (1.000)
Step: 4199, Reward: [-862.236 -862.236 -862.236] [0.0000], Avg: [-773.091 -773.091 -773.091] (1.000)
Step: 4249, Reward: [-829.028 -829.028 -829.028] [0.0000], Avg: [-773.749 -773.749 -773.749] (1.000)
Step: 4299, Reward: [-1073.865 -1073.865 -1073.865] [0.0000], Avg: [-777.239 -777.239 -777.239] (1.000)
Step: 4349, Reward: [-1008.169 -1008.169 -1008.169] [0.0000], Avg: [-779.893 -779.893 -779.893] (1.000)
Step: 4399, Reward: [-1137.711 -1137.711 -1137.711] [0.0000], Avg: [-783.96 -783.96 -783.96] (1.000)
Step: 4449, Reward: [-928.135 -928.135 -928.135] [0.0000], Avg: [-785.58 -785.58 -785.58] (1.000)
Step: 4499, Reward: [-1432.377 -1432.377 -1432.377] [0.0000], Avg: [-792.766 -792.766 -792.766] (1.000)
Step: 4549, Reward: [-942.098 -942.098 -942.098] [0.0000], Avg: [-794.407 -794.407 -794.407] (1.000)
Step: 4599, Reward: [-719.28 -719.28 -719.28] [0.0000], Avg: [-793.591 -793.591 -793.591] (1.000)
Step: 4649, Reward: [-866.799 -866.799 -866.799] [0.0000], Avg: [-794.378 -794.378 -794.378] (1.000)
Step: 4699, Reward: [-1239.561 -1239.561 -1239.561] [0.0000], Avg: [-799.114 -799.114 -799.114] (1.000)
Step: 4749, Reward: [-582.867 -582.867 -582.867] [0.0000], Avg: [-796.837 -796.837 -796.837] (1.000)
Step: 4799, Reward: [-1076.81 -1076.81 -1076.81] [0.0000], Avg: [-799.754 -799.754 -799.754] (1.000)
Step: 4849, Reward: [-973.811 -973.811 -973.811] [0.0000], Avg: [-801.548 -801.548 -801.548] (1.000)
Step: 4899, Reward: [-817.821 -817.821 -817.821] [0.0000], Avg: [-801.714 -801.714 -801.714] (1.000)
Step: 4949, Reward: [-577.98 -577.98 -577.98] [0.0000], Avg: [-799.454 -799.454 -799.454] (1.000)
Step: 4999, Reward: [-632.484 -632.484 -632.484] [0.0000], Avg: [-797.785 -797.785 -797.785] (1.000)
Step: 5049, Reward: [-562.883 -562.883 -562.883] [0.0000], Avg: [-795.459 -795.459 -795.459] (1.000)
Step: 5099, Reward: [-1003.314 -1003.314 -1003.314] [0.0000], Avg: [-797.497 -797.497 -797.497] (1.000)
Step: 5149, Reward: [-778.053 -778.053 -778.053] [0.0000], Avg: [-797.308 -797.308 -797.308] (1.000)
Step: 5199, Reward: [-797.307 -797.307 -797.307] [0.0000], Avg: [-797.308 -797.308 -797.308] (1.000)
Step: 5249, Reward: [-1217.986 -1217.986 -1217.986] [0.0000], Avg: [-801.314 -801.314 -801.314] (1.000)
Step: 5299, Reward: [-688.665 -688.665 -688.665] [0.0000], Avg: [-800.252 -800.252 -800.252] (1.000)
Step: 5349, Reward: [-483.464 -483.464 -483.464] [0.0000], Avg: [-797.291 -797.291 -797.291] (1.000)
Step: 5399, Reward: [-520.766 -520.766 -520.766] [0.0000], Avg: [-794.731 -794.731 -794.731] (1.000)
Step: 5449, Reward: [-701.424 -701.424 -701.424] [0.0000], Avg: [-793.875 -793.875 -793.875] (1.000)
Step: 5499, Reward: [-429.173 -429.173 -429.173] [0.0000], Avg: [-790.559 -790.559 -790.559] (1.000)
Step: 5549, Reward: [-531.585 -531.585 -531.585] [0.0000], Avg: [-788.226 -788.226 -788.226] (1.000)
Step: 5599, Reward: [-548.55 -548.55 -548.55] [0.0000], Avg: [-786.086 -786.086 -786.086] (1.000)
Step: 5649, Reward: [-583.105 -583.105 -583.105] [0.0000], Avg: [-784.29 -784.29 -784.29] (1.000)
Step: 5699, Reward: [-824.822 -824.822 -824.822] [0.0000], Avg: [-784.645 -784.645 -784.645] (1.000)
Step: 5749, Reward: [-706.987 -706.987 -706.987] [0.0000], Avg: [-783.97 -783.97 -783.97] (1.000)
Step: 5799, Reward: [-553.079 -553.079 -553.079] [0.0000], Avg: [-781.98 -781.98 -781.98] (1.000)
Step: 5849, Reward: [-548.384 -548.384 -548.384] [0.0000], Avg: [-779.983 -779.983 -779.983] (1.000)
Step: 5899, Reward: [-447.749 -447.749 -447.749] [0.0000], Avg: [-777.167 -777.167 -777.167] (1.000)
Step: 5949, Reward: [-832.653 -832.653 -832.653] [0.0000], Avg: [-777.634 -777.634 -777.634] (1.000)
Step: 5999, Reward: [-455.493 -455.493 -455.493] [0.0000], Avg: [-774.949 -774.949 -774.949] (1.000)
Step: 6049, Reward: [-502.162 -502.162 -502.162] [0.0000], Avg: [-772.695 -772.695 -772.695] (1.000)
Step: 6099, Reward: [-669.356 -669.356 -669.356] [0.0000], Avg: [-771.848 -771.848 -771.848] (1.000)
Step: 6149, Reward: [-499.507 -499.507 -499.507] [0.0000], Avg: [-769.634 -769.634 -769.634] (1.000)
Step: 6199, Reward: [-428.972 -428.972 -428.972] [0.0000], Avg: [-766.886 -766.886 -766.886] (1.000)
Step: 6249, Reward: [-684.864 -684.864 -684.864] [0.0000], Avg: [-766.23 -766.23 -766.23] (1.000)
Step: 6299, Reward: [-679.967 -679.967 -679.967] [0.0000], Avg: [-765.546 -765.546 -765.546] (1.000)
Step: 6349, Reward: [-611.874 -611.874 -611.874] [0.0000], Avg: [-764.336 -764.336 -764.336] (1.000)
Step: 6399, Reward: [-512.532 -512.532 -512.532] [0.0000], Avg: [-762.368 -762.368 -762.368] (1.000)
Step: 6449, Reward: [-641.865 -641.865 -641.865] [0.0000], Avg: [-761.434 -761.434 -761.434] (1.000)
Step: 6499, Reward: [-489.629 -489.629 -489.629] [0.0000], Avg: [-759.343 -759.343 -759.343] (1.000)
Step: 6549, Reward: [-652.869 -652.869 -652.869] [0.0000], Avg: [-758.531 -758.531 -758.531] (1.000)
Step: 6599, Reward: [-704.62 -704.62 -704.62] [0.0000], Avg: [-758.122 -758.122 -758.122] (1.000)
Step: 6649, Reward: [-778.891 -778.891 -778.891] [0.0000], Avg: [-758.278 -758.278 -758.278] (1.000)
Step: 6699, Reward: [-606.329 -606.329 -606.329] [0.0000], Avg: [-757.144 -757.144 -757.144] (1.000)
Step: 6749, Reward: [-619.958 -619.958 -619.958] [0.0000], Avg: [-756.128 -756.128 -756.128] (1.000)
Step: 6799, Reward: [-646.747 -646.747 -646.747] [0.0000], Avg: [-755.324 -755.324 -755.324] (1.000)
Step: 6849, Reward: [-570.04 -570.04 -570.04] [0.0000], Avg: [-753.971 -753.971 -753.971] (1.000)
Step: 6899, Reward: [-702.196 -702.196 -702.196] [0.0000], Avg: [-753.596 -753.596 -753.596] (1.000)
Step: 6949, Reward: [-659.661 -659.661 -659.661] [0.0000], Avg: [-752.92 -752.92 -752.92] (1.000)
Step: 6999, Reward: [-597.188 -597.188 -597.188] [0.0000], Avg: [-751.808 -751.808 -751.808] (1.000)
Step: 7049, Reward: [-676.24 -676.24 -676.24] [0.0000], Avg: [-751.272 -751.272 -751.272] (1.000)
Step: 7099, Reward: [-732.297 -732.297 -732.297] [0.0000], Avg: [-751.139 -751.139 -751.139] (1.000)
Step: 7149, Reward: [-497.609 -497.609 -497.609] [0.0000], Avg: [-749.366 -749.366 -749.366] (1.000)
Step: 7199, Reward: [-598.613 -598.613 -598.613] [0.0000], Avg: [-748.319 -748.319 -748.319] (1.000)
Step: 7249, Reward: [-808.118 -808.118 -808.118] [0.0000], Avg: [-748.731 -748.731 -748.731] (1.000)
Step: 7299, Reward: [-405.274 -405.274 -405.274] [0.0000], Avg: [-746.379 -746.379 -746.379] (1.000)
Step: 7349, Reward: [-506.419 -506.419 -506.419] [0.0000], Avg: [-744.746 -744.746 -744.746] (1.000)
Step: 7399, Reward: [-538.253 -538.253 -538.253] [0.0000], Avg: [-743.351 -743.351 -743.351] (1.000)
Step: 7449, Reward: [-480.201 -480.201 -480.201] [0.0000], Avg: [-741.585 -741.585 -741.585] (1.000)
Step: 7499, Reward: [-550.153 -550.153 -550.153] [0.0000], Avg: [-740.309 -740.309 -740.309] (1.000)
Step: 7549, Reward: [-463.76 -463.76 -463.76] [0.0000], Avg: [-738.477 -738.477 -738.477] (1.000)
Step: 7599, Reward: [-414.496 -414.496 -414.496] [0.0000], Avg: [-736.346 -736.346 -736.346] (1.000)
Step: 7649, Reward: [-571.506 -571.506 -571.506] [0.0000], Avg: [-735.268 -735.268 -735.268] (1.000)
Step: 7699, Reward: [-667.352 -667.352 -667.352] [0.0000], Avg: [-734.827 -734.827 -734.827] (1.000)
Step: 7749, Reward: [-374.636 -374.636 -374.636] [0.0000], Avg: [-732.504 -732.504 -732.504] (1.000)
Step: 7799, Reward: [-950.489 -950.489 -950.489] [0.0000], Avg: [-733.901 -733.901 -733.901] (1.000)
Step: 7849, Reward: [-365.742 -365.742 -365.742] [0.0000], Avg: [-731.556 -731.556 -731.556] (1.000)
Step: 7899, Reward: [-633.634 -633.634 -633.634] [0.0000], Avg: [-730.936 -730.936 -730.936] (1.000)
Step: 7949, Reward: [-583.425 -583.425 -583.425] [0.0000], Avg: [-730.009 -730.009 -730.009] (1.000)
Step: 7999, Reward: [-441.178 -441.178 -441.178] [0.0000], Avg: [-728.203 -728.203 -728.203] (1.000)
Step: 8049, Reward: [-621.56 -621.56 -621.56] [0.0000], Avg: [-727.541 -727.541 -727.541] (1.000)
Step: 8099, Reward: [-432.327 -432.327 -432.327] [0.0000], Avg: [-725.719 -725.719 -725.719] (1.000)
Step: 8149, Reward: [-1235.536 -1235.536 -1235.536] [0.0000], Avg: [-728.846 -728.846 -728.846] (1.000)
Step: 8199, Reward: [-660.598 -660.598 -660.598] [0.0000], Avg: [-728.43 -728.43 -728.43] (1.000)
Step: 8249, Reward: [-1667.107 -1667.107 -1667.107] [0.0000], Avg: [-734.119 -734.119 -734.119] (1.000)
Step: 8299, Reward: [-1578.274 -1578.274 -1578.274] [0.0000], Avg: [-739.204 -739.204 -739.204] (1.000)
Step: 8349, Reward: [-1459.616 -1459.616 -1459.616] [0.0000], Avg: [-743.518 -743.518 -743.518] (1.000)
Step: 8399, Reward: [-1702.037 -1702.037 -1702.037] [0.0000], Avg: [-749.224 -749.224 -749.224] (1.000)
Step: 8449, Reward: [-1938.363 -1938.363 -1938.363] [0.0000], Avg: [-756.26 -756.26 -756.26] (1.000)
Step: 8499, Reward: [-1724.859 -1724.859 -1724.859] [0.0000], Avg: [-761.958 -761.958 -761.958] (1.000)
Step: 8549, Reward: [-1368.514 -1368.514 -1368.514] [0.0000], Avg: [-765.505 -765.505 -765.505] (1.000)
Step: 8599, Reward: [-1438.38 -1438.38 -1438.38] [0.0000], Avg: [-769.417 -769.417 -769.417] (1.000)
Step: 8649, Reward: [-1149.635 -1149.635 -1149.635] [0.0000], Avg: [-771.615 -771.615 -771.615] (1.000)
Step: 8699, Reward: [-1268.714 -1268.714 -1268.714] [0.0000], Avg: [-774.472 -774.472 -774.472] (1.000)
Step: 8749, Reward: [-1308.883 -1308.883 -1308.883] [0.0000], Avg: [-777.525 -777.525 -777.525] (1.000)
Step: 8799, Reward: [-1157.871 -1157.871 -1157.871] [0.0000], Avg: [-779.686 -779.686 -779.686] (1.000)
Step: 8849, Reward: [-1243.605 -1243.605 -1243.605] [0.0000], Avg: [-782.307 -782.307 -782.307] (1.000)
Step: 8899, Reward: [-1014.65 -1014.65 -1014.65] [0.0000], Avg: [-783.613 -783.613 -783.613] (1.000)
Step: 8949, Reward: [-909.568 -909.568 -909.568] [0.0000], Avg: [-784.316 -784.316 -784.316] (1.000)
Step: 8999, Reward: [-1489.43 -1489.43 -1489.43] [0.0000], Avg: [-788.234 -788.234 -788.234] (1.000)
Step: 9049, Reward: [-1502.07 -1502.07 -1502.07] [0.0000], Avg: [-792.177 -792.177 -792.177] (1.000)
Step: 9099, Reward: [-795.235 -795.235 -795.235] [0.0000], Avg: [-792.194 -792.194 -792.194] (1.000)
Step: 9149, Reward: [-1283.958 -1283.958 -1283.958] [0.0000], Avg: [-794.882 -794.882 -794.882] (1.000)
Step: 9199, Reward: [-1254.475 -1254.475 -1254.475] [0.0000], Avg: [-797.379 -797.379 -797.379] (1.000)
Step: 9249, Reward: [-526.79 -526.79 -526.79] [0.0000], Avg: [-795.917 -795.917 -795.917] (1.000)
Step: 9299, Reward: [-852.098 -852.098 -852.098] [0.0000], Avg: [-796.219 -796.219 -796.219] (1.000)
Step: 9349, Reward: [-811.263 -811.263 -811.263] [0.0000], Avg: [-796.299 -796.299 -796.299] (1.000)
Step: 9399, Reward: [-672.981 -672.981 -672.981] [0.0000], Avg: [-795.643 -795.643 -795.643] (1.000)
Step: 9449, Reward: [-1181.996 -1181.996 -1181.996] [0.0000], Avg: [-797.687 -797.687 -797.687] (1.000)
Step: 9499, Reward: [-1318.508 -1318.508 -1318.508] [0.0000], Avg: [-800.429 -800.429 -800.429] (1.000)
Step: 9549, Reward: [-1375.238 -1375.238 -1375.238] [0.0000], Avg: [-803.438 -803.438 -803.438] (1.000)
Step: 9599, Reward: [-637.347 -637.347 -637.347] [0.0000], Avg: [-802.573 -802.573 -802.573] (1.000)
Step: 9649, Reward: [-621.484 -621.484 -621.484] [0.0000], Avg: [-801.635 -801.635 -801.635] (1.000)
Step: 9699, Reward: [-650.317 -650.317 -650.317] [0.0000], Avg: [-800.855 -800.855 -800.855] (1.000)
Step: 9749, Reward: [-1720.2 -1720.2 -1720.2] [0.0000], Avg: [-805.569 -805.569 -805.569] (1.000)
Step: 9799, Reward: [-767.243 -767.243 -767.243] [0.0000], Avg: [-805.374 -805.374 -805.374] (1.000)
Step: 9849, Reward: [-1426.122 -1426.122 -1426.122] [0.0000], Avg: [-808.525 -808.525 -808.525] (1.000)
Step: 9899, Reward: [-2046.476 -2046.476 -2046.476] [0.0000], Avg: [-814.777 -814.777 -814.777] (1.000)
Step: 9949, Reward: [-1880.709 -1880.709 -1880.709] [0.0000], Avg: [-820.134 -820.134 -820.134] (1.000)
Step: 9999, Reward: [-1219.22 -1219.22 -1219.22] [0.0000], Avg: [-822.129 -822.129 -822.129] (1.000)
Step: 10049, Reward: [-738.39 -738.39 -738.39] [0.0000], Avg: [-821.712 -821.712 -821.712] (1.000)
Step: 10099, Reward: [-1793.564 -1793.564 -1793.564] [0.0000], Avg: [-826.523 -826.523 -826.523] (1.000)
Step: 10149, Reward: [-2197.48 -2197.48 -2197.48] [0.0000], Avg: [-833.277 -833.277 -833.277] (1.000)
Step: 10199, Reward: [-1860.608 -1860.608 -1860.608] [0.0000], Avg: [-838.313 -838.313 -838.313] (1.000)
Step: 10249, Reward: [-1403.314 -1403.314 -1403.314] [0.0000], Avg: [-841.069 -841.069 -841.069] (1.000)
Step: 10299, Reward: [-2105.966 -2105.966 -2105.966] [0.0000], Avg: [-847.209 -847.209 -847.209] (1.000)
Step: 10349, Reward: [-1943.588 -1943.588 -1943.588] [0.0000], Avg: [-852.506 -852.506 -852.506] (1.000)
Step: 10399, Reward: [-1350.564 -1350.564 -1350.564] [0.0000], Avg: [-854.9 -854.9 -854.9] (1.000)
Step: 10449, Reward: [-1939.204 -1939.204 -1939.204] [0.0000], Avg: [-860.088 -860.088 -860.088] (1.000)
Step: 10499, Reward: [-1596.561 -1596.561 -1596.561] [0.0000], Avg: [-863.595 -863.595 -863.595] (1.000)
Step: 10549, Reward: [-1600.828 -1600.828 -1600.828] [0.0000], Avg: [-867.089 -867.089 -867.089] (1.000)
Step: 10599, Reward: [-2159.134 -2159.134 -2159.134] [0.0000], Avg: [-873.184 -873.184 -873.184] (1.000)
Step: 10649, Reward: [-1796.246 -1796.246 -1796.246] [0.0000], Avg: [-877.518 -877.518 -877.518] (1.000)
Step: 10699, Reward: [-1769.731 -1769.731 -1769.731] [0.0000], Avg: [-881.687 -881.687 -881.687] (1.000)
Step: 10749, Reward: [-2079.641 -2079.641 -2079.641] [0.0000], Avg: [-887.259 -887.259 -887.259] (1.000)
Step: 10799, Reward: [-2108.741 -2108.741 -2108.741] [0.0000], Avg: [-892.914 -892.914 -892.914] (1.000)
Step: 10849, Reward: [-1724.137 -1724.137 -1724.137] [0.0000], Avg: [-896.744 -896.744 -896.744] (1.000)
Step: 10899, Reward: [-509.34 -509.34 -509.34] [0.0000], Avg: [-894.967 -894.967 -894.967] (1.000)
Step: 10949, Reward: [-1226.484 -1226.484 -1226.484] [0.0000], Avg: [-896.481 -896.481 -896.481] (1.000)
Step: 10999, Reward: [-1207.434 -1207.434 -1207.434] [0.0000], Avg: [-897.894 -897.894 -897.894] (1.000)
Step: 11049, Reward: [-535.451 -535.451 -535.451] [0.0000], Avg: [-896.254 -896.254 -896.254] (1.000)
Step: 11099, Reward: [-833.939 -833.939 -833.939] [0.0000], Avg: [-895.974 -895.974 -895.974] (1.000)
Step: 11149, Reward: [-644.654 -644.654 -644.654] [0.0000], Avg: [-894.847 -894.847 -894.847] (1.000)
Step: 11199, Reward: [-634.93 -634.93 -634.93] [0.0000], Avg: [-893.686 -893.686 -893.686] (1.000)
Step: 11249, Reward: [-611.311 -611.311 -611.311] [0.0000], Avg: [-892.431 -892.431 -892.431] (1.000)
Step: 11299, Reward: [-697.493 -697.493 -697.493] [0.0000], Avg: [-891.569 -891.569 -891.569] (1.000)
Step: 11349, Reward: [-754.561 -754.561 -754.561] [0.0000], Avg: [-890.965 -890.965 -890.965] (1.000)
Step: 11399, Reward: [-682.704 -682.704 -682.704] [0.0000], Avg: [-890.052 -890.052 -890.052] (1.000)
Step: 11449, Reward: [-514.233 -514.233 -514.233] [0.0000], Avg: [-888.411 -888.411 -888.411] (1.000)
Step: 11499, Reward: [-597.462 -597.462 -597.462] [0.0000], Avg: [-887.146 -887.146 -887.146] (1.000)
Step: 11549, Reward: [-550.959 -550.959 -550.959] [0.0000], Avg: [-885.69 -885.69 -885.69] (1.000)
Step: 11599, Reward: [-685.672 -685.672 -685.672] [0.0000], Avg: [-884.828 -884.828 -884.828] (1.000)
Step: 11649, Reward: [-509.033 -509.033 -509.033] [0.0000], Avg: [-883.215 -883.215 -883.215] (1.000)
Step: 11699, Reward: [-552.341 -552.341 -552.341] [0.0000], Avg: [-881.801 -881.801 -881.801] (1.000)
Step: 11749, Reward: [-410.299 -410.299 -410.299] [0.0000], Avg: [-879.795 -879.795 -879.795] (1.000)
Step: 11799, Reward: [-456.969 -456.969 -456.969] [0.0000], Avg: [-878.003 -878.003 -878.003] (1.000)
Step: 11849, Reward: [-806.826 -806.826 -806.826] [0.0000], Avg: [-877.703 -877.703 -877.703] (1.000)
Step: 11899, Reward: [-750.936 -750.936 -750.936] [0.0000], Avg: [-877.17 -877.17 -877.17] (1.000)
Step: 11949, Reward: [-569.938 -569.938 -569.938] [0.0000], Avg: [-875.885 -875.885 -875.885] (1.000)
Step: 11999, Reward: [-550.024 -550.024 -550.024] [0.0000], Avg: [-874.527 -874.527 -874.527] (1.000)
Step: 12049, Reward: [-560.789 -560.789 -560.789] [0.0000], Avg: [-873.225 -873.225 -873.225] (1.000)
Step: 12099, Reward: [-367.99 -367.99 -367.99] [0.0000], Avg: [-871.137 -871.137 -871.137] (1.000)
Step: 12149, Reward: [-665.733 -665.733 -665.733] [0.0000], Avg: [-870.292 -870.292 -870.292] (1.000)
Step: 12199, Reward: [-602.147 -602.147 -602.147] [0.0000], Avg: [-869.193 -869.193 -869.193] (1.000)
Step: 12249, Reward: [-446.088 -446.088 -446.088] [0.0000], Avg: [-867.466 -867.466 -867.466] (1.000)
Step: 12299, Reward: [-389.458 -389.458 -389.458] [0.0000], Avg: [-865.523 -865.523 -865.523] (1.000)
Step: 12349, Reward: [-509.85 -509.85 -509.85] [0.0000], Avg: [-864.083 -864.083 -864.083] (1.000)
Step: 12399, Reward: [-483.073 -483.073 -483.073] [0.0000], Avg: [-862.547 -862.547 -862.547] (1.000)
Step: 12449, Reward: [-516.715 -516.715 -516.715] [0.0000], Avg: [-861.158 -861.158 -861.158] (1.000)
Step: 12499, Reward: [-466.408 -466.408 -466.408] [0.0000], Avg: [-859.579 -859.579 -859.579] (1.000)
Step: 12549, Reward: [-523.729 -523.729 -523.729] [0.0000], Avg: [-858.241 -858.241 -858.241] (1.000)
Step: 12599, Reward: [-476.813 -476.813 -476.813] [0.0000], Avg: [-856.727 -856.727 -856.727] (1.000)
Step: 12649, Reward: [-528.829 -528.829 -528.829] [0.0000], Avg: [-855.431 -855.431 -855.431] (1.000)
Step: 12699, Reward: [-383.171 -383.171 -383.171] [0.0000], Avg: [-853.572 -853.572 -853.572] (1.000)
Step: 12749, Reward: [-421.255 -421.255 -421.255] [0.0000], Avg: [-851.877 -851.877 -851.877] (1.000)
Step: 12799, Reward: [-503.779 -503.779 -503.779] [0.0000], Avg: [-850.517 -850.517 -850.517] (1.000)
Step: 12849, Reward: [-436.381 -436.381 -436.381] [0.0000], Avg: [-848.905 -848.905 -848.905] (1.000)
Step: 12899, Reward: [-433.897 -433.897 -433.897] [0.0000], Avg: [-847.297 -847.297 -847.297] (1.000)
Step: 12949, Reward: [-487.855 -487.855 -487.855] [0.0000], Avg: [-845.909 -845.909 -845.909] (1.000)
Step: 12999, Reward: [-431.516 -431.516 -431.516] [0.0000], Avg: [-844.315 -844.315 -844.315] (1.000)
Step: 13049, Reward: [-675.286 -675.286 -675.286] [0.0000], Avg: [-843.668 -843.668 -843.668] (1.000)
Step: 13099, Reward: [-364.372 -364.372 -364.372] [0.0000], Avg: [-841.838 -841.838 -841.838] (1.000)
Step: 13149, Reward: [-362.735 -362.735 -362.735] [0.0000], Avg: [-840.017 -840.017 -840.017] (1.000)
Step: 13199, Reward: [-474.719 -474.719 -474.719] [0.0000], Avg: [-838.633 -838.633 -838.633] (1.000)
Step: 13249, Reward: [-469.483 -469.483 -469.483] [0.0000], Avg: [-837.24 -837.24 -837.24] (1.000)
Step: 13299, Reward: [-441.717 -441.717 -441.717] [0.0000], Avg: [-835.753 -835.753 -835.753] (1.000)
Step: 13349, Reward: [-579.761 -579.761 -579.761] [0.0000], Avg: [-834.794 -834.794 -834.794] (1.000)
Step: 13399, Reward: [-402.368 -402.368 -402.368] [0.0000], Avg: [-833.181 -833.181 -833.181] (1.000)
Step: 13449, Reward: [-643.184 -643.184 -643.184] [0.0000], Avg: [-832.474 -832.474 -832.474] (1.000)
Step: 13499, Reward: [-345.678 -345.678 -345.678] [0.0000], Avg: [-830.671 -830.671 -830.671] (1.000)
Step: 13549, Reward: [-471.792 -471.792 -471.792] [0.0000], Avg: [-829.347 -829.347 -829.347] (1.000)
Step: 13599, Reward: [-437.788 -437.788 -437.788] [0.0000], Avg: [-827.907 -827.907 -827.907] (1.000)
Step: 13649, Reward: [-392.031 -392.031 -392.031] [0.0000], Avg: [-826.311 -826.311 -826.311] (1.000)
Step: 13699, Reward: [-531.991 -531.991 -531.991] [0.0000], Avg: [-825.237 -825.237 -825.237] (1.000)
Step: 13749, Reward: [-434.208 -434.208 -434.208] [0.0000], Avg: [-823.815 -823.815 -823.815] (1.000)
Step: 13799, Reward: [-394.392 -394.392 -394.392] [0.0000], Avg: [-822.259 -822.259 -822.259] (1.000)
Step: 13849, Reward: [-380.158 -380.158 -380.158] [0.0000], Avg: [-820.663 -820.663 -820.663] (1.000)
Step: 13899, Reward: [-438.842 -438.842 -438.842] [0.0000], Avg: [-819.289 -819.289 -819.289] (1.000)
Step: 13949, Reward: [-494.217 -494.217 -494.217] [0.0000], Avg: [-818.124 -818.124 -818.124] (1.000)
Step: 13999, Reward: [-488.625 -488.625 -488.625] [0.0000], Avg: [-816.948 -816.948 -816.948] (1.000)
Step: 14049, Reward: [-437.513 -437.513 -437.513] [0.0000], Avg: [-815.597 -815.597 -815.597] (1.000)
Step: 14099, Reward: [-493.509 -493.509 -493.509] [0.0000], Avg: [-814.455 -814.455 -814.455] (1.000)
Step: 14149, Reward: [-330.693 -330.693 -330.693] [0.0000], Avg: [-812.746 -812.746 -812.746] (1.000)
Step: 14199, Reward: [-372.861 -372.861 -372.861] [0.0000], Avg: [-811.197 -811.197 -811.197] (1.000)
Step: 14249, Reward: [-344.767 -344.767 -344.767] [0.0000], Avg: [-809.56 -809.56 -809.56] (1.000)
Step: 14299, Reward: [-409.168 -409.168 -409.168] [0.0000], Avg: [-808.16 -808.16 -808.16] (1.000)
Step: 14349, Reward: [-441.581 -441.581 -441.581] [0.0000], Avg: [-806.883 -806.883 -806.883] (1.000)
Step: 14399, Reward: [-350.798 -350.798 -350.798] [0.0000], Avg: [-805.299 -805.299 -805.299] (1.000)
Step: 14449, Reward: [-345.609 -345.609 -345.609] [0.0000], Avg: [-803.709 -803.709 -803.709] (1.000)
Step: 14499, Reward: [-465.59 -465.59 -465.59] [0.0000], Avg: [-802.543 -802.543 -802.543] (1.000)
Step: 14549, Reward: [-480.057 -480.057 -480.057] [0.0000], Avg: [-801.435 -801.435 -801.435] (1.000)
Step: 14599, Reward: [-456.985 -456.985 -456.985] [0.0000], Avg: [-800.255 -800.255 -800.255] (1.000)
Step: 14649, Reward: [-439.265 -439.265 -439.265] [0.0000], Avg: [-799.023 -799.023 -799.023] (1.000)
Step: 14699, Reward: [-495.284 -495.284 -495.284] [0.0000], Avg: [-797.99 -797.99 -797.99] (1.000)
Step: 14749, Reward: [-418.719 -418.719 -418.719] [0.0000], Avg: [-796.704 -796.704 -796.704] (1.000)
Step: 14799, Reward: [-344.831 -344.831 -344.831] [0.0000], Avg: [-795.177 -795.177 -795.177] (1.000)
Step: 14849, Reward: [-525.563 -525.563 -525.563] [0.0000], Avg: [-794.27 -794.27 -794.27] (1.000)
Step: 14899, Reward: [-376.303 -376.303 -376.303] [0.0000], Avg: [-792.867 -792.867 -792.867] (1.000)
Step: 14949, Reward: [-465.709 -465.709 -465.709] [0.0000], Avg: [-791.773 -791.773 -791.773] (1.000)
Step: 14999, Reward: [-577.55 -577.55 -577.55] [0.0000], Avg: [-791.059 -791.059 -791.059] (1.000)
Step: 15049, Reward: [-444.017 -444.017 -444.017] [0.0000], Avg: [-789.906 -789.906 -789.906] (1.000)
Step: 15099, Reward: [-317.163 -317.163 -317.163] [0.0000], Avg: [-788.341 -788.341 -788.341] (1.000)
Step: 15149, Reward: [-388.952 -388.952 -388.952] [0.0000], Avg: [-787.022 -787.022 -787.022] (1.000)
Step: 15199, Reward: [-346.123 -346.123 -346.123] [0.0000], Avg: [-785.572 -785.572 -785.572] (1.000)
Step: 15249, Reward: [-445.672 -445.672 -445.672] [0.0000], Avg: [-784.458 -784.458 -784.458] (1.000)
Step: 15299, Reward: [-487.492 -487.492 -487.492] [0.0000], Avg: [-783.487 -783.487 -783.487] (1.000)
Step: 15349, Reward: [-366.175 -366.175 -366.175] [0.0000], Avg: [-782.128 -782.128 -782.128] (1.000)
Step: 15399, Reward: [-449.696 -449.696 -449.696] [0.0000], Avg: [-781.049 -781.049 -781.049] (1.000)
Step: 15449, Reward: [-585.47 -585.47 -585.47] [0.0000], Avg: [-780.416 -780.416 -780.416] (1.000)
Step: 15499, Reward: [-440.1 -440.1 -440.1] [0.0000], Avg: [-779.318 -779.318 -779.318] (1.000)
Step: 15549, Reward: [-558.409 -558.409 -558.409] [0.0000], Avg: [-778.607 -778.607 -778.607] (1.000)
Step: 15599, Reward: [-384.802 -384.802 -384.802] [0.0000], Avg: [-777.345 -777.345 -777.345] (1.000)
Step: 15649, Reward: [-405.599 -405.599 -405.599] [0.0000], Avg: [-776.158 -776.158 -776.158] (1.000)
Step: 15699, Reward: [-497.924 -497.924 -497.924] [0.0000], Avg: [-775.272 -775.272 -775.272] (1.000)
Step: 15749, Reward: [-387.182 -387.182 -387.182] [0.0000], Avg: [-774.039 -774.039 -774.039] (1.000)
Step: 15799, Reward: [-489.758 -489.758 -489.758] [0.0000], Avg: [-773.14 -773.14 -773.14] (1.000)
Step: 15849, Reward: [-482.994 -482.994 -482.994] [0.0000], Avg: [-772.225 -772.225 -772.225] (1.000)
Step: 15899, Reward: [-356.292 -356.292 -356.292] [0.0000], Avg: [-770.917 -770.917 -770.917] (1.000)
Step: 15949, Reward: [-486.033 -486.033 -486.033] [0.0000], Avg: [-770.024 -770.024 -770.024] (1.000)
Step: 15999, Reward: [-371.936 -371.936 -371.936] [0.0000], Avg: [-768.78 -768.78 -768.78] (1.000)
Step: 16049, Reward: [-389.813 -389.813 -389.813] [0.0000], Avg: [-767.599 -767.599 -767.599] (1.000)
Step: 16099, Reward: [-442.692 -442.692 -442.692] [0.0000], Avg: [-766.59 -766.59 -766.59] (1.000)
Step: 16149, Reward: [-411.297 -411.297 -411.297] [0.0000], Avg: [-765.49 -765.49 -765.49] (1.000)
Step: 16199, Reward: [-289.108 -289.108 -289.108] [0.0000], Avg: [-764.02 -764.02 -764.02] (1.000)
Step: 16249, Reward: [-601.144 -601.144 -601.144] [0.0000], Avg: [-763.518 -763.518 -763.518] (1.000)
Step: 16299, Reward: [-427.686 -427.686 -427.686] [0.0000], Avg: [-762.488 -762.488 -762.488] (1.000)
Step: 16349, Reward: [-402.197 -402.197 -402.197] [0.0000], Avg: [-761.387 -761.387 -761.387] (1.000)
Step: 16399, Reward: [-466.994 -466.994 -466.994] [0.0000], Avg: [-760.489 -760.489 -760.489] (1.000)
Step: 16449, Reward: [-365.275 -365.275 -365.275] [0.0000], Avg: [-759.288 -759.288 -759.288] (1.000)
Step: 16499, Reward: [-487.132 -487.132 -487.132] [0.0000], Avg: [-758.463 -758.463 -758.463] (1.000)
Step: 16549, Reward: [-474.885 -474.885 -474.885] [0.0000], Avg: [-757.606 -757.606 -757.606] (1.000)
Step: 16599, Reward: [-392.709 -392.709 -392.709] [0.0000], Avg: [-756.507 -756.507 -756.507] (1.000)
Step: 16649, Reward: [-446.93 -446.93 -446.93] [0.0000], Avg: [-755.578 -755.578 -755.578] (1.000)
Step: 16699, Reward: [-359.712 -359.712 -359.712] [0.0000], Avg: [-754.392 -754.392 -754.392] (1.000)
Step: 16749, Reward: [-452.526 -452.526 -452.526] [0.0000], Avg: [-753.491 -753.491 -753.491] (1.000)
Step: 16799, Reward: [-435.048 -435.048 -435.048] [0.0000], Avg: [-752.543 -752.543 -752.543] (1.000)
Step: 16849, Reward: [-460.198 -460.198 -460.198] [0.0000], Avg: [-751.676 -751.676 -751.676] (1.000)
Step: 16899, Reward: [-443.686 -443.686 -443.686] [0.0000], Avg: [-750.765 -750.765 -750.765] (1.000)
Step: 16949, Reward: [-528.832 -528.832 -528.832] [0.0000], Avg: [-750.11 -750.11 -750.11] (1.000)
Step: 16999, Reward: [-422.883 -422.883 -422.883] [0.0000], Avg: [-749.148 -749.148 -749.148] (1.000)
Step: 17049, Reward: [-358.093 -358.093 -358.093] [0.0000], Avg: [-748.001 -748.001 -748.001] (1.000)
Step: 17099, Reward: [-295.068 -295.068 -295.068] [0.0000], Avg: [-746.676 -746.676 -746.676] (1.000)
Step: 17149, Reward: [-328.875 -328.875 -328.875] [0.0000], Avg: [-745.458 -745.458 -745.458] (1.000)
Step: 17199, Reward: [-378.202 -378.202 -378.202] [0.0000], Avg: [-744.391 -744.391 -744.391] (1.000)
Step: 17249, Reward: [-277.729 -277.729 -277.729] [0.0000], Avg: [-743.038 -743.038 -743.038] (1.000)
Step: 17299, Reward: [-371.875 -371.875 -371.875] [0.0000], Avg: [-741.965 -741.965 -741.965] (1.000)
Step: 17349, Reward: [-455.634 -455.634 -455.634] [0.0000], Avg: [-741.14 -741.14 -741.14] (1.000)
Step: 17399, Reward: [-478.908 -478.908 -478.908] [0.0000], Avg: [-740.387 -740.387 -740.387] (1.000)
Step: 17449, Reward: [-686.554 -686.554 -686.554] [0.0000], Avg: [-740.232 -740.232 -740.232] (1.000)
Step: 17499, Reward: [-327.274 -327.274 -327.274] [0.0000], Avg: [-739.053 -739.053 -739.053] (1.000)
Step: 17549, Reward: [-353.288 -353.288 -353.288] [0.0000], Avg: [-737.954 -737.954 -737.954] (1.000)
Step: 17599, Reward: [-349.398 -349.398 -349.398] [0.0000], Avg: [-736.85 -736.85 -736.85] (1.000)
Step: 17649, Reward: [-367.486 -367.486 -367.486] [0.0000], Avg: [-735.803 -735.803 -735.803] (1.000)
Step: 17699, Reward: [-341.479 -341.479 -341.479] [0.0000], Avg: [-734.689 -734.689 -734.689] (1.000)
Step: 17749, Reward: [-448.77 -448.77 -448.77] [0.0000], Avg: [-733.884 -733.884 -733.884] (1.000)
Step: 17799, Reward: [-410.199 -410.199 -410.199] [0.0000], Avg: [-732.975 -732.975 -732.975] (1.000)
Step: 17849, Reward: [-348.219 -348.219 -348.219] [0.0000], Avg: [-731.897 -731.897 -731.897] (1.000)
Step: 17899, Reward: [-530.145 -530.145 -530.145] [0.0000], Avg: [-731.334 -731.334 -731.334] (1.000)
Step: 17949, Reward: [-436.516 -436.516 -436.516] [0.0000], Avg: [-730.512 -730.512 -730.512] (1.000)
Step: 17999, Reward: [-485.839 -485.839 -485.839] [0.0000], Avg: [-729.833 -729.833 -729.833] (1.000)
Step: 18049, Reward: [-386.699 -386.699 -386.699] [0.0000], Avg: [-728.882 -728.882 -728.882] (1.000)
Step: 18099, Reward: [-409.606 -409.606 -409.606] [0.0000], Avg: [-728. -728. -728.] (1.000)
Step: 18149, Reward: [-696.29 -696.29 -696.29] [0.0000], Avg: [-727.913 -727.913 -727.913] (1.000)
Step: 18199, Reward: [-510.247 -510.247 -510.247] [0.0000], Avg: [-727.315 -727.315 -727.315] (1.000)
Step: 18249, Reward: [-352.475 -352.475 -352.475] [0.0000], Avg: [-726.288 -726.288 -726.288] (1.000)
Step: 18299, Reward: [-597.697 -597.697 -597.697] [0.0000], Avg: [-725.937 -725.937 -725.937] (1.000)
Step: 18349, Reward: [-414.333 -414.333 -414.333] [0.0000], Avg: [-725.087 -725.087 -725.087] (1.000)
Step: 18399, Reward: [-476.299 -476.299 -476.299] [0.0000], Avg: [-724.411 -724.411 -724.411] (1.000)
Step: 18449, Reward: [-590.565 -590.565 -590.565] [0.0000], Avg: [-724.049 -724.049 -724.049] (1.000)
Step: 18499, Reward: [-407.474 -407.474 -407.474] [0.0000], Avg: [-723.193 -723.193 -723.193] (1.000)
Step: 18549, Reward: [-783.635 -783.635 -783.635] [0.0000], Avg: [-723.356 -723.356 -723.356] (1.000)
Step: 18599, Reward: [-369.495 -369.495 -369.495] [0.0000], Avg: [-722.405 -722.405 -722.405] (1.000)
Step: 18649, Reward: [-451.241 -451.241 -451.241] [0.0000], Avg: [-721.678 -721.678 -721.678] (1.000)
Step: 18699, Reward: [-480.523 -480.523 -480.523] [0.0000], Avg: [-721.033 -721.033 -721.033] (1.000)
Step: 18749, Reward: [-537.61 -537.61 -537.61] [0.0000], Avg: [-720.544 -720.544 -720.544] (1.000)
Step: 18799, Reward: [-476.421 -476.421 -476.421] [0.0000], Avg: [-719.895 -719.895 -719.895] (1.000)
Step: 18849, Reward: [-587.769 -587.769 -587.769] [0.0000], Avg: [-719.544 -719.544 -719.544] (1.000)
Step: 18899, Reward: [-402.972 -402.972 -402.972] [0.0000], Avg: [-718.707 -718.707 -718.707] (1.000)
Step: 18949, Reward: [-430.996 -430.996 -430.996] [0.0000], Avg: [-717.947 -717.947 -717.947] (1.000)
Step: 18999, Reward: [-537.613 -537.613 -537.613] [0.0000], Avg: [-717.473 -717.473 -717.473] (1.000)
Step: 19049, Reward: [-484.494 -484.494 -484.494] [0.0000], Avg: [-716.861 -716.861 -716.861] (1.000)
Step: 19099, Reward: [-491.341 -491.341 -491.341] [0.0000], Avg: [-716.271 -716.271 -716.271] (1.000)
Step: 19149, Reward: [-566.638 -566.638 -566.638] [0.0000], Avg: [-715.88 -715.88 -715.88] (1.000)
Step: 19199, Reward: [-427.538 -427.538 -427.538] [0.0000], Avg: [-715.129 -715.129 -715.129] (1.000)
Step: 19249, Reward: [-427.67 -427.67 -427.67] [0.0000], Avg: [-714.383 -714.383 -714.383] (1.000)
Step: 19299, Reward: [-396.601 -396.601 -396.601] [0.0000], Avg: [-713.56 -713.56 -713.56] (1.000)
Step: 19349, Reward: [-376.14 -376.14 -376.14] [0.0000], Avg: [-712.688 -712.688 -712.688] (1.000)
Step: 19399, Reward: [-653.445 -653.445 -653.445] [0.0000], Avg: [-712.535 -712.535 -712.535] (1.000)
Step: 19449, Reward: [-414.815 -414.815 -414.815] [0.0000], Avg: [-711.77 -711.77 -711.77] (1.000)
Step: 19499, Reward: [-407.244 -407.244 -407.244] [0.0000], Avg: [-710.989 -710.989 -710.989] (1.000)
Step: 19549, Reward: [-528.359 -528.359 -528.359] [0.0000], Avg: [-710.522 -710.522 -710.522] (1.000)
Step: 19599, Reward: [-589.909 -589.909 -589.909] [0.0000], Avg: [-710.214 -710.214 -710.214] (1.000)
Step: 19649, Reward: [-464.351 -464.351 -464.351] [0.0000], Avg: [-709.588 -709.588 -709.588] (1.000)
Step: 19699, Reward: [-405.541 -405.541 -405.541] [0.0000], Avg: [-708.817 -708.817 -708.817] (1.000)
Step: 19749, Reward: [-515.698 -515.698 -515.698] [0.0000], Avg: [-708.328 -708.328 -708.328] (1.000)
Step: 19799, Reward: [-380.363 -380.363 -380.363] [0.0000], Avg: [-707.5 -707.5 -707.5] (1.000)
Step: 19849, Reward: [-285.805 -285.805 -285.805] [0.0000], Avg: [-706.437 -706.437 -706.437] (1.000)
Step: 19899, Reward: [-453.597 -453.597 -453.597] [0.0000], Avg: [-705.802 -705.802 -705.802] (1.000)
Step: 19949, Reward: [-406.884 -406.884 -406.884] [0.0000], Avg: [-705.053 -705.053 -705.053] (1.000)
Step: 19999, Reward: [-483.657 -483.657 -483.657] [0.0000], Avg: [-704.499 -704.499 -704.499] (1.000)
Step: 20049, Reward: [-535.397 -535.397 -535.397] [0.0000], Avg: [-704.078 -704.078 -704.078] (1.000)
Step: 20099, Reward: [-410.838 -410.838 -410.838] [0.0000], Avg: [-703.348 -703.348 -703.348] (1.000)
Step: 20149, Reward: [-389.164 -389.164 -389.164] [0.0000], Avg: [-702.569 -702.569 -702.569] (1.000)
Step: 20199, Reward: [-394.507 -394.507 -394.507] [0.0000], Avg: [-701.806 -701.806 -701.806] (1.000)
Step: 20249, Reward: [-394.539 -394.539 -394.539] [0.0000], Avg: [-701.048 -701.048 -701.048] (1.000)
Step: 20299, Reward: [-425.986 -425.986 -425.986] [0.0000], Avg: [-700.37 -700.37 -700.37] (1.000)
Step: 20349, Reward: [-484.673 -484.673 -484.673] [0.0000], Avg: [-699.84 -699.84 -699.84] (1.000)
Step: 20399, Reward: [-486.964 -486.964 -486.964] [0.0000], Avg: [-699.318 -699.318 -699.318] (1.000)
Step: 20449, Reward: [-483.035 -483.035 -483.035] [0.0000], Avg: [-698.789 -698.789 -698.789] (1.000)
Step: 20499, Reward: [-491.101 -491.101 -491.101] [0.0000], Avg: [-698.283 -698.283 -698.283] (1.000)
Step: 20549, Reward: [-440.872 -440.872 -440.872] [0.0000], Avg: [-697.657 -697.657 -697.657] (1.000)
Step: 20599, Reward: [-457.082 -457.082 -457.082] [0.0000], Avg: [-697.073 -697.073 -697.073] (1.000)
Step: 20649, Reward: [-402.47 -402.47 -402.47] [0.0000], Avg: [-696.359 -696.359 -696.359] (1.000)
Step: 20699, Reward: [-477.097 -477.097 -477.097] [0.0000], Avg: [-695.83 -695.83 -695.83] (1.000)
Step: 20749, Reward: [-499.967 -499.967 -499.967] [0.0000], Avg: [-695.358 -695.358 -695.358] (1.000)
Step: 20799, Reward: [-450.611 -450.611 -450.611] [0.0000], Avg: [-694.769 -694.769 -694.769] (1.000)
Step: 20849, Reward: [-405.848 -405.848 -405.848] [0.0000], Avg: [-694.077 -694.077 -694.077] (1.000)
Step: 20899, Reward: [-382.573 -382.573 -382.573] [0.0000], Avg: [-693.331 -693.331 -693.331] (1.000)
Step: 20949, Reward: [-396.907 -396.907 -396.907] [0.0000], Avg: [-692.624 -692.624 -692.624] (1.000)
Step: 20999, Reward: [-364.522 -364.522 -364.522] [0.0000], Avg: [-691.843 -691.843 -691.843] (1.000)
Step: 21049, Reward: [-332.244 -332.244 -332.244] [0.0000], Avg: [-690.989 -690.989 -690.989] (1.000)
Step: 21099, Reward: [-416.129 -416.129 -416.129] [0.0000], Avg: [-690.337 -690.337 -690.337] (1.000)
Step: 21149, Reward: [-477.816 -477.816 -477.816] [0.0000], Avg: [-689.835 -689.835 -689.835] (1.000)
Step: 21199, Reward: [-450.022 -450.022 -450.022] [0.0000], Avg: [-689.269 -689.269 -689.269] (1.000)
Step: 21249, Reward: [-398.969 -398.969 -398.969] [0.0000], Avg: [-688.586 -688.586 -688.586] (1.000)
Step: 21299, Reward: [-391.235 -391.235 -391.235] [0.0000], Avg: [-687.888 -687.888 -687.888] (1.000)
Step: 21349, Reward: [-504.651 -504.651 -504.651] [0.0000], Avg: [-687.459 -687.459 -687.459] (1.000)
Step: 21399, Reward: [-339.939 -339.939 -339.939] [0.0000], Avg: [-686.647 -686.647 -686.647] (1.000)
Step: 21449, Reward: [-427.869 -427.869 -427.869] [0.0000], Avg: [-686.044 -686.044 -686.044] (1.000)
Step: 21499, Reward: [-432.786 -432.786 -432.786] [0.0000], Avg: [-685.455 -685.455 -685.455] (1.000)
Step: 21549, Reward: [-494.908 -494.908 -494.908] [0.0000], Avg: [-685.013 -685.013 -685.013] (1.000)
Step: 21599, Reward: [-646.828 -646.828 -646.828] [0.0000], Avg: [-684.924 -684.924 -684.924] (1.000)
Step: 21649, Reward: [-448.105 -448.105 -448.105] [0.0000], Avg: [-684.377 -684.377 -684.377] (1.000)
Step: 21699, Reward: [-438.57 -438.57 -438.57] [0.0000], Avg: [-683.811 -683.811 -683.811] (1.000)
Step: 21749, Reward: [-430.678 -430.678 -430.678] [0.0000], Avg: [-683.229 -683.229 -683.229] (1.000)
Step: 21799, Reward: [-339.294 -339.294 -339.294] [0.0000], Avg: [-682.44 -682.44 -682.44] (1.000)
Step: 21849, Reward: [-517.763 -517.763 -517.763] [0.0000], Avg: [-682.064 -682.064 -682.064] (1.000)
Step: 21899, Reward: [-544.196 -544.196 -544.196] [0.0000], Avg: [-681.749 -681.749 -681.749] (1.000)
Step: 21949, Reward: [-311.155 -311.155 -311.155] [0.0000], Avg: [-680.905 -680.905 -680.905] (1.000)
Step: 21999, Reward: [-401.761 -401.761 -401.761] [0.0000], Avg: [-680.27 -680.27 -680.27] (1.000)
Step: 22049, Reward: [-474.781 -474.781 -474.781] [0.0000], Avg: [-679.804 -679.804 -679.804] (1.000)
Step: 22099, Reward: [-546.152 -546.152 -546.152] [0.0000], Avg: [-679.502 -679.502 -679.502] (1.000)
Step: 22149, Reward: [-461.907 -461.907 -461.907] [0.0000], Avg: [-679.011 -679.011 -679.011] (1.000)
Step: 22199, Reward: [-344.282 -344.282 -344.282] [0.0000], Avg: [-678.257 -678.257 -678.257] (1.000)
Step: 22249, Reward: [-354.252 -354.252 -354.252] [0.0000], Avg: [-677.529 -677.529 -677.529] (1.000)
Step: 22299, Reward: [-439.897 -439.897 -439.897] [0.0000], Avg: [-676.996 -676.996 -676.996] (1.000)
Step: 22349, Reward: [-342.836 -342.836 -342.836] [0.0000], Avg: [-676.248 -676.248 -676.248] (1.000)
Step: 22399, Reward: [-459.173 -459.173 -459.173] [0.0000], Avg: [-675.764 -675.764 -675.764] (1.000)
Step: 22449, Reward: [-443.207 -443.207 -443.207] [0.0000], Avg: [-675.246 -675.246 -675.246] (1.000)
Step: 22499, Reward: [-416.72 -416.72 -416.72] [0.0000], Avg: [-674.671 -674.671 -674.671] (1.000)
Step: 22549, Reward: [-377.935 -377.935 -377.935] [0.0000], Avg: [-674.013 -674.013 -674.013] (1.000)
Step: 22599, Reward: [-459.772 -459.772 -459.772] [0.0000], Avg: [-673.539 -673.539 -673.539] (1.000)
Step: 22649, Reward: [-434.642 -434.642 -434.642] [0.0000], Avg: [-673.012 -673.012 -673.012] (1.000)
Step: 22699, Reward: [-429.852 -429.852 -429.852] [0.0000], Avg: [-672.476 -672.476 -672.476] (1.000)
Step: 22749, Reward: [-461.714 -461.714 -461.714] [0.0000], Avg: [-672.013 -672.013 -672.013] (1.000)
Step: 22799, Reward: [-410.398 -410.398 -410.398] [0.0000], Avg: [-671.439 -671.439 -671.439] (1.000)
Step: 22849, Reward: [-442.339 -442.339 -442.339] [0.0000], Avg: [-670.938 -670.938 -670.938] (1.000)
Step: 22899, Reward: [-457.478 -457.478 -457.478] [0.0000], Avg: [-670.472 -670.472 -670.472] (1.000)
Step: 22949, Reward: [-479.472 -479.472 -479.472] [0.0000], Avg: [-670.056 -670.056 -670.056] (1.000)
Step: 22999, Reward: [-491.389 -491.389 -491.389] [0.0000], Avg: [-669.668 -669.668 -669.668] (1.000)
Step: 23049, Reward: [-416.444 -416.444 -416.444] [0.0000], Avg: [-669.118 -669.118 -669.118] (1.000)
Step: 23099, Reward: [-371.02 -371.02 -371.02] [0.0000], Avg: [-668.473 -668.473 -668.473] (1.000)
Step: 23149, Reward: [-438.043 -438.043 -438.043] [0.0000], Avg: [-667.975 -667.975 -667.975] (1.000)
Step: 23199, Reward: [-569.41 -569.41 -569.41] [0.0000], Avg: [-667.763 -667.763 -667.763] (1.000)
Step: 23249, Reward: [-470.531 -470.531 -470.531] [0.0000], Avg: [-667.339 -667.339 -667.339] (1.000)
Step: 23299, Reward: [-370.416 -370.416 -370.416] [0.0000], Avg: [-666.702 -666.702 -666.702] (1.000)
Step: 23349, Reward: [-409.163 -409.163 -409.163] [0.0000], Avg: [-666.15 -666.15 -666.15] (1.000)
Step: 23399, Reward: [-435.385 -435.385 -435.385] [0.0000], Avg: [-665.657 -665.657 -665.657] (1.000)
Step: 23449, Reward: [-335.306 -335.306 -335.306] [0.0000], Avg: [-664.953 -664.953 -664.953] (1.000)
Step: 23499, Reward: [-351.929 -351.929 -351.929] [0.0000], Avg: [-664.287 -664.287 -664.287] (1.000)
Step: 23549, Reward: [-508.798 -508.798 -508.798] [0.0000], Avg: [-663.956 -663.956 -663.956] (1.000)
Step: 23599, Reward: [-450.142 -450.142 -450.142] [0.0000], Avg: [-663.503 -663.503 -663.503] (1.000)
Step: 23649, Reward: [-452.237 -452.237 -452.237] [0.0000], Avg: [-663.057 -663.057 -663.057] (1.000)
Step: 23699, Reward: [-428.245 -428.245 -428.245] [0.0000], Avg: [-662.561 -662.561 -662.561] (1.000)
Step: 23749, Reward: [-532.922 -532.922 -532.922] [0.0000], Avg: [-662.289 -662.289 -662.289] (1.000)
Step: 23799, Reward: [-464.503 -464.503 -464.503] [0.0000], Avg: [-661.873 -661.873 -661.873] (1.000)
Step: 23849, Reward: [-440.01 -440.01 -440.01] [0.0000], Avg: [-661.408 -661.408 -661.408] (1.000)
Step: 23899, Reward: [-392.903 -392.903 -392.903] [0.0000], Avg: [-660.846 -660.846 -660.846] (1.000)
Step: 23949, Reward: [-354.02 -354.02 -354.02] [0.0000], Avg: [-660.206 -660.206 -660.206] (1.000)
Step: 23999, Reward: [-372.64 -372.64 -372.64] [0.0000], Avg: [-659.607 -659.607 -659.607] (1.000)
Step: 24049, Reward: [-391.314 -391.314 -391.314] [0.0000], Avg: [-659.049 -659.049 -659.049] (1.000)
Step: 24099, Reward: [-352.544 -352.544 -352.544] [0.0000], Avg: [-658.413 -658.413 -658.413] (1.000)
Step: 24149, Reward: [-383.32 -383.32 -383.32] [0.0000], Avg: [-657.843 -657.843 -657.843] (1.000)
Step: 24199, Reward: [-468.716 -468.716 -468.716] [0.0000], Avg: [-657.453 -657.453 -657.453] (1.000)
Step: 24249, Reward: [-519.399 -519.399 -519.399] [0.0000], Avg: [-657.168 -657.168 -657.168] (1.000)
Step: 24299, Reward: [-414.1 -414.1 -414.1] [0.0000], Avg: [-656.668 -656.668 -656.668] (1.000)
Step: 24349, Reward: [-503.415 -503.415 -503.415] [0.0000], Avg: [-656.353 -656.353 -656.353] (1.000)
Step: 24399, Reward: [-450.438 -450.438 -450.438] [0.0000], Avg: [-655.931 -655.931 -655.931] (1.000)
Step: 24449, Reward: [-473.241 -473.241 -473.241] [0.0000], Avg: [-655.557 -655.557 -655.557] (1.000)
Step: 24499, Reward: [-499.724 -499.724 -499.724] [0.0000], Avg: [-655.239 -655.239 -655.239] (1.000)
Step: 24549, Reward: [-349.919 -349.919 -349.919] [0.0000], Avg: [-654.618 -654.618 -654.618] (1.000)
Step: 24599, Reward: [-608.376 -608.376 -608.376] [0.0000], Avg: [-654.524 -654.524 -654.524] (1.000)
Step: 24649, Reward: [-467.031 -467.031 -467.031] [0.0000], Avg: [-654.143 -654.143 -654.143] (1.000)
Step: 24699, Reward: [-534.828 -534.828 -534.828] [0.0000], Avg: [-653.902 -653.902 -653.902] (1.000)
Step: 24749, Reward: [-483.775 -483.775 -483.775] [0.0000], Avg: [-653.558 -653.558 -653.558] (1.000)
Step: 24799, Reward: [-441.513 -441.513 -441.513] [0.0000], Avg: [-653.131 -653.131 -653.131] (1.000)
Step: 24849, Reward: [-366.151 -366.151 -366.151] [0.0000], Avg: [-652.553 -652.553 -652.553] (1.000)
Step: 24899, Reward: [-391.175 -391.175 -391.175] [0.0000], Avg: [-652.028 -652.028 -652.028] (1.000)
Step: 24949, Reward: [-396.974 -396.974 -396.974] [0.0000], Avg: [-651.517 -651.517 -651.517] (1.000)
Step: 24999, Reward: [-458.342 -458.342 -458.342] [0.0000], Avg: [-651.131 -651.131 -651.131] (1.000)
Step: 25049, Reward: [-447.57 -447.57 -447.57] [0.0000], Avg: [-650.725 -650.725 -650.725] (1.000)
Step: 25099, Reward: [-454.32 -454.32 -454.32] [0.0000], Avg: [-650.333 -650.333 -650.333] (1.000)
Step: 25149, Reward: [-438.406 -438.406 -438.406] [0.0000], Avg: [-649.912 -649.912 -649.912] (1.000)
Step: 25199, Reward: [-546.177 -546.177 -546.177] [0.0000], Avg: [-649.706 -649.706 -649.706] (1.000)
Step: 25249, Reward: [-472.303 -472.303 -472.303] [0.0000], Avg: [-649.355 -649.355 -649.355] (1.000)
Step: 25299, Reward: [-449.653 -449.653 -449.653] [0.0000], Avg: [-648.96 -648.96 -648.96] (1.000)
Step: 25349, Reward: [-412.416 -412.416 -412.416] [0.0000], Avg: [-648.494 -648.494 -648.494] (1.000)
Step: 25399, Reward: [-415.177 -415.177 -415.177] [0.0000], Avg: [-648.034 -648.034 -648.034] (1.000)
Step: 25449, Reward: [-559.896 -559.896 -559.896] [0.0000], Avg: [-647.861 -647.861 -647.861] (1.000)
Step: 25499, Reward: [-693.986 -693.986 -693.986] [0.0000], Avg: [-647.952 -647.952 -647.952] (1.000)
Step: 25549, Reward: [-474.418 -474.418 -474.418] [0.0000], Avg: [-647.612 -647.612 -647.612] (1.000)
Step: 25599, Reward: [-464.836 -464.836 -464.836] [0.0000], Avg: [-647.255 -647.255 -647.255] (1.000)
Step: 25649, Reward: [-458.176 -458.176 -458.176] [0.0000], Avg: [-646.886 -646.886 -646.886] (1.000)
Step: 25699, Reward: [-530.683 -530.683 -530.683] [0.0000], Avg: [-646.66 -646.66 -646.66] (1.000)
Step: 25749, Reward: [-615.279 -615.279 -615.279] [0.0000], Avg: [-646.599 -646.599 -646.599] (1.000)
Step: 25799, Reward: [-445.97 -445.97 -445.97] [0.0000], Avg: [-646.211 -646.211 -646.211] (1.000)
Step: 25849, Reward: [-479.955 -479.955 -479.955] [0.0000], Avg: [-645.889 -645.889 -645.889] (1.000)
Step: 25899, Reward: [-421.255 -421.255 -421.255] [0.0000], Avg: [-645.455 -645.455 -645.455] (1.000)
Step: 25949, Reward: [-602.947 -602.947 -602.947] [0.0000], Avg: [-645.374 -645.374 -645.374] (1.000)
Step: 25999, Reward: [-639.133 -639.133 -639.133] [0.0000], Avg: [-645.362 -645.362 -645.362] (1.000)
Step: 26049, Reward: [-383.272 -383.272 -383.272] [0.0000], Avg: [-644.858 -644.858 -644.858] (1.000)
Step: 26099, Reward: [-540.363 -540.363 -540.363] [0.0000], Avg: [-644.658 -644.658 -644.658] (1.000)
Step: 26149, Reward: [-438.056 -438.056 -438.056] [0.0000], Avg: [-644.263 -644.263 -644.263] (1.000)
Step: 26199, Reward: [-442.38 -442.38 -442.38] [0.0000], Avg: [-643.878 -643.878 -643.878] (1.000)
Step: 26249, Reward: [-333.111 -333.111 -333.111] [0.0000], Avg: [-643.286 -643.286 -643.286] (1.000)
Step: 26299, Reward: [-406.422 -406.422 -406.422] [0.0000], Avg: [-642.836 -642.836 -642.836] (1.000)
Step: 26349, Reward: [-388.26 -388.26 -388.26] [0.0000], Avg: [-642.353 -642.353 -642.353] (1.000)
Step: 26399, Reward: [-398.049 -398.049 -398.049] [0.0000], Avg: [-641.89 -641.89 -641.89] (1.000)
Step: 26449, Reward: [-390.98 -390.98 -390.98] [0.0000], Avg: [-641.416 -641.416 -641.416] (1.000)
Step: 26499, Reward: [-594.756 -594.756 -594.756] [0.0000], Avg: [-641.328 -641.328 -641.328] (1.000)
Step: 26549, Reward: [-423.791 -423.791 -423.791] [0.0000], Avg: [-640.918 -640.918 -640.918] (1.000)
Step: 26599, Reward: [-386.397 -386.397 -386.397] [0.0000], Avg: [-640.44 -640.44 -640.44] (1.000)
Step: 26649, Reward: [-469.14 -469.14 -469.14] [0.0000], Avg: [-640.118 -640.118 -640.118] (1.000)
Step: 26699, Reward: [-317.887 -317.887 -317.887] [0.0000], Avg: [-639.515 -639.515 -639.515] (1.000)
Step: 26749, Reward: [-514.256 -514.256 -514.256] [0.0000], Avg: [-639.281 -639.281 -639.281] (1.000)
Step: 26799, Reward: [-438.979 -438.979 -438.979] [0.0000], Avg: [-638.907 -638.907 -638.907] (1.000)
Step: 26849, Reward: [-519.789 -519.789 -519.789] [0.0000], Avg: [-638.685 -638.685 -638.685] (1.000)
Step: 26899, Reward: [-325.267 -325.267 -325.267] [0.0000], Avg: [-638.102 -638.102 -638.102] (1.000)
Step: 26949, Reward: [-476.784 -476.784 -476.784] [0.0000], Avg: [-637.803 -637.803 -637.803] (1.000)
Step: 26999, Reward: [-465.329 -465.329 -465.329] [0.0000], Avg: [-637.484 -637.484 -637.484] (1.000)
Step: 27049, Reward: [-578.23 -578.23 -578.23] [0.0000], Avg: [-637.374 -637.374 -637.374] (1.000)
Step: 27099, Reward: [-622.716 -622.716 -622.716] [0.0000], Avg: [-637.347 -637.347 -637.347] (1.000)
Step: 27149, Reward: [-508.993 -508.993 -508.993] [0.0000], Avg: [-637.111 -637.111 -637.111] (1.000)
Step: 27199, Reward: [-406.172 -406.172 -406.172] [0.0000], Avg: [-636.686 -636.686 -636.686] (1.000)
Step: 27249, Reward: [-383.483 -383.483 -383.483] [0.0000], Avg: [-636.222 -636.222 -636.222] (1.000)
Step: 27299, Reward: [-514.758 -514.758 -514.758] [0.0000], Avg: [-635.999 -635.999 -635.999] (1.000)
Step: 27349, Reward: [-569.471 -569.471 -569.471] [0.0000], Avg: [-635.878 -635.878 -635.878] (1.000)
Step: 27399, Reward: [-538.907 -538.907 -538.907] [0.0000], Avg: [-635.701 -635.701 -635.701] (1.000)
Step: 27449, Reward: [-504.27 -504.27 -504.27] [0.0000], Avg: [-635.461 -635.461 -635.461] (1.000)
Step: 27499, Reward: [-565.564 -565.564 -565.564] [0.0000], Avg: [-635.334 -635.334 -635.334] (1.000)
Step: 27549, Reward: [-567.674 -567.674 -567.674] [0.0000], Avg: [-635.211 -635.211 -635.211] (1.000)
Step: 27599, Reward: [-587.604 -587.604 -587.604] [0.0000], Avg: [-635.125 -635.125 -635.125] (1.000)
Step: 27649, Reward: [-453.425 -453.425 -453.425] [0.0000], Avg: [-634.797 -634.797 -634.797] (1.000)
Step: 27699, Reward: [-717.348 -717.348 -717.348] [0.0000], Avg: [-634.946 -634.946 -634.946] (1.000)
Step: 27749, Reward: [-520.961 -520.961 -520.961] [0.0000], Avg: [-634.74 -634.74 -634.74] (1.000)
Step: 27799, Reward: [-529.902 -529.902 -529.902] [0.0000], Avg: [-634.552 -634.552 -634.552] (1.000)
Step: 27849, Reward: [-455.78 -455.78 -455.78] [0.0000], Avg: [-634.231 -634.231 -634.231] (1.000)
Step: 27899, Reward: [-546.037 -546.037 -546.037] [0.0000], Avg: [-634.073 -634.073 -634.073] (1.000)
Step: 27949, Reward: [-430.538 -430.538 -430.538] [0.0000], Avg: [-633.709 -633.709 -633.709] (1.000)
Step: 27999, Reward: [-534.734 -534.734 -534.734] [0.0000], Avg: [-633.532 -633.532 -633.532] (1.000)
Step: 28049, Reward: [-505.991 -505.991 -505.991] [0.0000], Avg: [-633.304 -633.304 -633.304] (1.000)
Step: 28099, Reward: [-514.573 -514.573 -514.573] [0.0000], Avg: [-633.093 -633.093 -633.093] (1.000)
Step: 28149, Reward: [-437.894 -437.894 -437.894] [0.0000], Avg: [-632.746 -632.746 -632.746] (1.000)
Step: 28199, Reward: [-488.957 -488.957 -488.957] [0.0000], Avg: [-632.492 -632.492 -632.492] (1.000)
Step: 28249, Reward: [-611.716 -611.716 -611.716] [0.0000], Avg: [-632.455 -632.455 -632.455] (1.000)
Step: 28299, Reward: [-440.961 -440.961 -440.961] [0.0000], Avg: [-632.116 -632.116 -632.116] (1.000)
Step: 28349, Reward: [-613.504 -613.504 -613.504] [0.0000], Avg: [-632.084 -632.084 -632.084] (1.000)
Step: 28399, Reward: [-535.229 -535.229 -535.229] [0.0000], Avg: [-631.913 -631.913 -631.913] (1.000)
Step: 28449, Reward: [-506.894 -506.894 -506.894] [0.0000], Avg: [-631.693 -631.693 -631.693] (1.000)
Step: 28499, Reward: [-409.746 -409.746 -409.746] [0.0000], Avg: [-631.304 -631.304 -631.304] (1.000)
Step: 28549, Reward: [-389.8 -389.8 -389.8] [0.0000], Avg: [-630.881 -630.881 -630.881] (1.000)
Step: 28599, Reward: [-551.752 -551.752 -551.752] [0.0000], Avg: [-630.743 -630.743 -630.743] (1.000)
Step: 28649, Reward: [-571.748 -571.748 -571.748] [0.0000], Avg: [-630.64 -630.64 -630.64] (1.000)
Step: 28699, Reward: [-533.863 -533.863 -533.863] [0.0000], Avg: [-630.471 -630.471 -630.471] (1.000)
Step: 28749, Reward: [-534.9 -534.9 -534.9] [0.0000], Avg: [-630.305 -630.305 -630.305] (1.000)
Step: 28799, Reward: [-616.79 -616.79 -616.79] [0.0000], Avg: [-630.281 -630.281 -630.281] (1.000)
Step: 28849, Reward: [-580.326 -580.326 -580.326] [0.0000], Avg: [-630.195 -630.195 -630.195] (1.000)
Step: 28899, Reward: [-475.446 -475.446 -475.446] [0.0000], Avg: [-629.927 -629.927 -629.927] (1.000)
Step: 28949, Reward: [-674.206 -674.206 -674.206] [0.0000], Avg: [-630.004 -630.004 -630.004] (1.000)
Step: 28999, Reward: [-578.731 -578.731 -578.731] [0.0000], Avg: [-629.915 -629.915 -629.915] (1.000)
Step: 29049, Reward: [-540.064 -540.064 -540.064] [0.0000], Avg: [-629.761 -629.761 -629.761] (1.000)
Step: 29099, Reward: [-634.101 -634.101 -634.101] [0.0000], Avg: [-629.768 -629.768 -629.768] (1.000)
Step: 29149, Reward: [-615.821 -615.821 -615.821] [0.0000], Avg: [-629.744 -629.744 -629.744] (1.000)
Step: 29199, Reward: [-738.81 -738.81 -738.81] [0.0000], Avg: [-629.931 -629.931 -629.931] (1.000)
Step: 29249, Reward: [-570.025 -570.025 -570.025] [0.0000], Avg: [-629.828 -629.828 -629.828] (1.000)
Step: 29299, Reward: [-610.45 -610.45 -610.45] [0.0000], Avg: [-629.795 -629.795 -629.795] (1.000)
Step: 29349, Reward: [-555.172 -555.172 -555.172] [0.0000], Avg: [-629.668 -629.668 -629.668] (1.000)
Step: 29399, Reward: [-594.298 -594.298 -594.298] [0.0000], Avg: [-629.608 -629.608 -629.608] (1.000)
Step: 29449, Reward: [-641.485 -641.485 -641.485] [0.0000], Avg: [-629.628 -629.628 -629.628] (1.000)
Step: 29499, Reward: [-645.49 -645.49 -645.49] [0.0000], Avg: [-629.655 -629.655 -629.655] (1.000)
Step: 29549, Reward: [-510.402 -510.402 -510.402] [0.0000], Avg: [-629.453 -629.453 -629.453] (1.000)
Step: 29599, Reward: [-603.332 -603.332 -603.332] [0.0000], Avg: [-629.409 -629.409 -629.409] (1.000)
Step: 29649, Reward: [-536.084 -536.084 -536.084] [0.0000], Avg: [-629.252 -629.252 -629.252] (1.000)
Step: 29699, Reward: [-450.659 -450.659 -450.659] [0.0000], Avg: [-628.951 -628.951 -628.951] (1.000)
Step: 29749, Reward: [-578.555 -578.555 -578.555] [0.0000], Avg: [-628.867 -628.867 -628.867] (1.000)
Step: 29799, Reward: [-449.391 -449.391 -449.391] [0.0000], Avg: [-628.565 -628.565 -628.565] (1.000)
Step: 29849, Reward: [-431.052 -431.052 -431.052] [0.0000], Avg: [-628.235 -628.235 -628.235] (1.000)
Step: 29899, Reward: [-422.591 -422.591 -422.591] [0.0000], Avg: [-627.891 -627.891 -627.891] (1.000)
Step: 29949, Reward: [-302.666 -302.666 -302.666] [0.0000], Avg: [-627.348 -627.348 -627.348] (1.000)
Step: 29999, Reward: [-440.49 -440.49 -440.49] [0.0000], Avg: [-627.036 -627.036 -627.036] (1.000)
Step: 30049, Reward: [-457.197 -457.197 -457.197] [0.0000], Avg: [-626.754 -626.754 -626.754] (1.000)
Step: 30099, Reward: [-482.964 -482.964 -482.964] [0.0000], Avg: [-626.515 -626.515 -626.515] (1.000)
Step: 30149, Reward: [-467.448 -467.448 -467.448] [0.0000], Avg: [-626.251 -626.251 -626.251] (1.000)
Step: 30199, Reward: [-610.268 -610.268 -610.268] [0.0000], Avg: [-626.225 -626.225 -626.225] (1.000)
Step: 30249, Reward: [-474.304 -474.304 -474.304] [0.0000], Avg: [-625.974 -625.974 -625.974] (1.000)
Step: 30299, Reward: [-434.846 -434.846 -434.846] [0.0000], Avg: [-625.658 -625.658 -625.658] (1.000)
Step: 30349, Reward: [-535.289 -535.289 -535.289] [0.0000], Avg: [-625.509 -625.509 -625.509] (1.000)
Step: 30399, Reward: [-467.774 -467.774 -467.774] [0.0000], Avg: [-625.25 -625.25 -625.25] (1.000)
Step: 30449, Reward: [-460.752 -460.752 -460.752] [0.0000], Avg: [-624.98 -624.98 -624.98] (1.000)
Step: 30499, Reward: [-425.722 -425.722 -425.722] [0.0000], Avg: [-624.653 -624.653 -624.653] (1.000)
Step: 30549, Reward: [-402.288 -402.288 -402.288] [0.0000], Avg: [-624.289 -624.289 -624.289] (1.000)
Step: 30599, Reward: [-415.395 -415.395 -415.395] [0.0000], Avg: [-623.948 -623.948 -623.948] (1.000)
Step: 30649, Reward: [-525.253 -525.253 -525.253] [0.0000], Avg: [-623.787 -623.787 -623.787] (1.000)
Step: 30699, Reward: [-407.125 -407.125 -407.125] [0.0000], Avg: [-623.434 -623.434 -623.434] (1.000)
Step: 30749, Reward: [-555.159 -555.159 -555.159] [0.0000], Avg: [-623.323 -623.323 -623.323] (1.000)
Step: 30799, Reward: [-406.729 -406.729 -406.729] [0.0000], Avg: [-622.971 -622.971 -622.971] (1.000)
Step: 30849, Reward: [-591.52 -591.52 -591.52] [0.0000], Avg: [-622.92 -622.92 -622.92] (1.000)
Step: 30899, Reward: [-581.742 -581.742 -581.742] [0.0000], Avg: [-622.854 -622.854 -622.854] (1.000)
Step: 30949, Reward: [-410.157 -410.157 -410.157] [0.0000], Avg: [-622.51 -622.51 -622.51] (1.000)
Step: 30999, Reward: [-395.657 -395.657 -395.657] [0.0000], Avg: [-622.144 -622.144 -622.144] (1.000)
Step: 31049, Reward: [-337.691 -337.691 -337.691] [0.0000], Avg: [-621.686 -621.686 -621.686] (1.000)
Step: 31099, Reward: [-560.017 -560.017 -560.017] [0.0000], Avg: [-621.587 -621.587 -621.587] (1.000)
Step: 31149, Reward: [-628.8 -628.8 -628.8] [0.0000], Avg: [-621.599 -621.599 -621.599] (1.000)
Step: 31199, Reward: [-467.625 -467.625 -467.625] [0.0000], Avg: [-621.352 -621.352 -621.352] (1.000)
Step: 31249, Reward: [-395.069 -395.069 -395.069] [0.0000], Avg: [-620.99 -620.99 -620.99] (1.000)
Step: 31299, Reward: [-459.308 -459.308 -459.308] [0.0000], Avg: [-620.731 -620.731 -620.731] (1.000)
Step: 31349, Reward: [-396.942 -396.942 -396.942] [0.0000], Avg: [-620.375 -620.375 -620.375] (1.000)
Step: 31399, Reward: [-531.936 -531.936 -531.936] [0.0000], Avg: [-620.234 -620.234 -620.234] (1.000)
Step: 31449, Reward: [-415.299 -415.299 -415.299] [0.0000], Avg: [-619.908 -619.908 -619.908] (1.000)
Step: 31499, Reward: [-378.576 -378.576 -378.576] [0.0000], Avg: [-619.525 -619.525 -619.525] (1.000)
Step: 31549, Reward: [-517.071 -517.071 -517.071] [0.0000], Avg: [-619.362 -619.362 -619.362] (1.000)
Step: 31599, Reward: [-385.153 -385.153 -385.153] [0.0000], Avg: [-618.992 -618.992 -618.992] (1.000)
Step: 31649, Reward: [-393.607 -393.607 -393.607] [0.0000], Avg: [-618.636 -618.636 -618.636] (1.000)
Step: 31699, Reward: [-444.516 -444.516 -444.516] [0.0000], Avg: [-618.361 -618.361 -618.361] (1.000)
Step: 31749, Reward: [-473.483 -473.483 -473.483] [0.0000], Avg: [-618.133 -618.133 -618.133] (1.000)
Step: 31799, Reward: [-509.677 -509.677 -509.677] [0.0000], Avg: [-617.962 -617.962 -617.962] (1.000)
Step: 31849, Reward: [-400.634 -400.634 -400.634] [0.0000], Avg: [-617.621 -617.621 -617.621] (1.000)
Step: 31899, Reward: [-356.029 -356.029 -356.029] [0.0000], Avg: [-617.211 -617.211 -617.211] (1.000)
Step: 31949, Reward: [-411.162 -411.162 -411.162] [0.0000], Avg: [-616.889 -616.889 -616.889] (1.000)
Step: 31999, Reward: [-418.339 -418.339 -418.339] [0.0000], Avg: [-616.579 -616.579 -616.579] (1.000)
Step: 32049, Reward: [-345.008 -345.008 -345.008] [0.0000], Avg: [-616.155 -616.155 -616.155] (1.000)
Step: 32099, Reward: [-359.55 -359.55 -359.55] [0.0000], Avg: [-615.755 -615.755 -615.755] (1.000)
Step: 32149, Reward: [-325.458 -325.458 -325.458] [0.0000], Avg: [-615.304 -615.304 -615.304] (1.000)
Step: 32199, Reward: [-425.176 -425.176 -425.176] [0.0000], Avg: [-615.009 -615.009 -615.009] (1.000)
Step: 32249, Reward: [-448.871 -448.871 -448.871] [0.0000], Avg: [-614.751 -614.751 -614.751] (1.000)
Step: 32299, Reward: [-465.019 -465.019 -465.019] [0.0000], Avg: [-614.519 -614.519 -614.519] (1.000)
Step: 32349, Reward: [-419.223 -419.223 -419.223] [0.0000], Avg: [-614.217 -614.217 -614.217] (1.000)
Step: 32399, Reward: [-409.039 -409.039 -409.039] [0.0000], Avg: [-613.901 -613.901 -613.901] (1.000)
Step: 32449, Reward: [-439.467 -439.467 -439.467] [0.0000], Avg: [-613.632 -613.632 -613.632] (1.000)
Step: 32499, Reward: [-392.732 -392.732 -392.732] [0.0000], Avg: [-613.292 -613.292 -613.292] (1.000)
Step: 32549, Reward: [-373.617 -373.617 -373.617] [0.0000], Avg: [-612.924 -612.924 -612.924] (1.000)
Step: 32599, Reward: [-383.607 -383.607 -383.607] [0.0000], Avg: [-612.572 -612.572 -612.572] (1.000)
Step: 32649, Reward: [-369.266 -369.266 -369.266] [0.0000], Avg: [-612.2 -612.2 -612.2] (1.000)
Step: 32699, Reward: [-617.768 -617.768 -617.768] [0.0000], Avg: [-612.208 -612.208 -612.208] (1.000)
Step: 32749, Reward: [-419.267 -419.267 -419.267] [0.0000], Avg: [-611.914 -611.914 -611.914] (1.000)
Step: 32799, Reward: [-361.081 -361.081 -361.081] [0.0000], Avg: [-611.531 -611.531 -611.531] (1.000)
Step: 32849, Reward: [-498.479 -498.479 -498.479] [0.0000], Avg: [-611.359 -611.359 -611.359] (1.000)
Step: 32899, Reward: [-401.486 -401.486 -401.486] [0.0000], Avg: [-611.04 -611.04 -611.04] (1.000)
Step: 32949, Reward: [-398.845 -398.845 -398.845] [0.0000], Avg: [-610.718 -610.718 -610.718] (1.000)
Step: 32999, Reward: [-438.999 -438.999 -438.999] [0.0000], Avg: [-610.458 -610.458 -610.458] (1.000)
Step: 33049, Reward: [-419.869 -419.869 -419.869] [0.0000], Avg: [-610.17 -610.17 -610.17] (1.000)
Step: 33099, Reward: [-352.155 -352.155 -352.155] [0.0000], Avg: [-609.78 -609.78 -609.78] (1.000)
Step: 33149, Reward: [-411.848 -411.848 -411.848] [0.0000], Avg: [-609.481 -609.481 -609.481] (1.000)
Step: 33199, Reward: [-462.731 -462.731 -462.731] [0.0000], Avg: [-609.26 -609.26 -609.26] (1.000)
Step: 33249, Reward: [-524.34 -524.34 -524.34] [0.0000], Avg: [-609.133 -609.133 -609.133] (1.000)
Step: 33299, Reward: [-324.119 -324.119 -324.119] [0.0000], Avg: [-608.705 -608.705 -608.705] (1.000)
Step: 33349, Reward: [-443.762 -443.762 -443.762] [0.0000], Avg: [-608.457 -608.457 -608.457] (1.000)
Step: 33399, Reward: [-502.388 -502.388 -502.388] [0.0000], Avg: [-608.299 -608.299 -608.299] (1.000)
Step: 33449, Reward: [-434.163 -434.163 -434.163] [0.0000], Avg: [-608.038 -608.038 -608.038] (1.000)
Step: 33499, Reward: [-348.618 -348.618 -348.618] [0.0000], Avg: [-607.651 -607.651 -607.651] (1.000)
Step: 33549, Reward: [-563.684 -563.684 -563.684] [0.0000], Avg: [-607.586 -607.586 -607.586] (1.000)
Step: 33599, Reward: [-342.544 -342.544 -342.544] [0.0000], Avg: [-607.191 -607.191 -607.191] (1.000)
Step: 33649, Reward: [-560.4 -560.4 -560.4] [0.0000], Avg: [-607.122 -607.122 -607.122] (1.000)
Step: 33699, Reward: [-393.114 -393.114 -393.114] [0.0000], Avg: [-606.804 -606.804 -606.804] (1.000)
Step: 33749, Reward: [-455.184 -455.184 -455.184] [0.0000], Avg: [-606.58 -606.58 -606.58] (1.000)
Step: 33799, Reward: [-395.031 -395.031 -395.031] [0.0000], Avg: [-606.267 -606.267 -606.267] (1.000)
Step: 33849, Reward: [-497.601 -497.601 -497.601] [0.0000], Avg: [-606.106 -606.106 -606.106] (1.000)
Step: 33899, Reward: [-453.552 -453.552 -453.552] [0.0000], Avg: [-605.881 -605.881 -605.881] (1.000)
Step: 33949, Reward: [-411.7 -411.7 -411.7] [0.0000], Avg: [-605.595 -605.595 -605.595] (1.000)
Step: 33999, Reward: [-454.936 -454.936 -454.936] [0.0000], Avg: [-605.374 -605.374 -605.374] (1.000)
Step: 34049, Reward: [-497.427 -497.427 -497.427] [0.0000], Avg: [-605.215 -605.215 -605.215] (1.000)
Step: 34099, Reward: [-417.898 -417.898 -417.898] [0.0000], Avg: [-604.94 -604.94 -604.94] (1.000)
Step: 34149, Reward: [-592.906 -592.906 -592.906] [0.0000], Avg: [-604.923 -604.923 -604.923] (1.000)
Step: 34199, Reward: [-572.734 -572.734 -572.734] [0.0000], Avg: [-604.876 -604.876 -604.876] (1.000)
Step: 34249, Reward: [-505.706 -505.706 -505.706] [0.0000], Avg: [-604.731 -604.731 -604.731] (1.000)
Step: 34299, Reward: [-471.584 -471.584 -471.584] [0.0000], Avg: [-604.537 -604.537 -604.537] (1.000)
Step: 34349, Reward: [-462.133 -462.133 -462.133] [0.0000], Avg: [-604.33 -604.33 -604.33] (1.000)
Step: 34399, Reward: [-463.519 -463.519 -463.519] [0.0000], Avg: [-604.125 -604.125 -604.125] (1.000)
Step: 34449, Reward: [-432.461 -432.461 -432.461] [0.0000], Avg: [-603.876 -603.876 -603.876] (1.000)
Step: 34499, Reward: [-737.584 -737.584 -737.584] [0.0000], Avg: [-604.07 -604.07 -604.07] (1.000)
Step: 34549, Reward: [-462.946 -462.946 -462.946] [0.0000], Avg: [-603.865 -603.865 -603.865] (1.000)
Step: 34599, Reward: [-465.821 -465.821 -465.821] [0.0000], Avg: [-603.666 -603.666 -603.666] (1.000)
Step: 34649, Reward: [-468.38 -468.38 -468.38] [0.0000], Avg: [-603.471 -603.471 -603.471] (1.000)
Step: 34699, Reward: [-628.277 -628.277 -628.277] [0.0000], Avg: [-603.506 -603.506 -603.506] (1.000)
Step: 34749, Reward: [-625.229 -625.229 -625.229] [0.0000], Avg: [-603.538 -603.538 -603.538] (1.000)
Step: 34799, Reward: [-452.358 -452.358 -452.358] [0.0000], Avg: [-603.32 -603.32 -603.32] (1.000)
Step: 34849, Reward: [-456.066 -456.066 -456.066] [0.0000], Avg: [-603.109 -603.109 -603.109] (1.000)
Step: 34899, Reward: [-464.258 -464.258 -464.258] [0.0000], Avg: [-602.91 -602.91 -602.91] (1.000)
Step: 34949, Reward: [-448.7 -448.7 -448.7] [0.0000], Avg: [-602.69 -602.69 -602.69] (1.000)
Step: 34999, Reward: [-390.847 -390.847 -390.847] [0.0000], Avg: [-602.387 -602.387 -602.387] (1.000)
Step: 35049, Reward: [-510.719 -510.719 -510.719] [0.0000], Avg: [-602.256 -602.256 -602.256] (1.000)
Step: 35099, Reward: [-448.405 -448.405 -448.405] [0.0000], Avg: [-602.037 -602.037 -602.037] (1.000)
Step: 35149, Reward: [-383.816 -383.816 -383.816] [0.0000], Avg: [-601.727 -601.727 -601.727] (1.000)
Step: 35199, Reward: [-373.247 -373.247 -373.247] [0.0000], Avg: [-601.402 -601.402 -601.402] (1.000)
Step: 35249, Reward: [-390.721 -390.721 -390.721] [0.0000], Avg: [-601.103 -601.103 -601.103] (1.000)
Step: 35299, Reward: [-432.572 -432.572 -432.572] [0.0000], Avg: [-600.864 -600.864 -600.864] (1.000)
Step: 35349, Reward: [-469.394 -469.394 -469.394] [0.0000], Avg: [-600.679 -600.679 -600.679] (1.000)
Step: 35399, Reward: [-403.957 -403.957 -403.957] [0.0000], Avg: [-600.401 -600.401 -600.401] (1.000)
Step: 35449, Reward: [-486.722 -486.722 -486.722] [0.0000], Avg: [-600.24 -600.24 -600.24] (1.000)
Step: 35499, Reward: [-374.577 -374.577 -374.577] [0.0000], Avg: [-599.922 -599.922 -599.922] (1.000)
Step: 35549, Reward: [-467.485 -467.485 -467.485] [0.0000], Avg: [-599.736 -599.736 -599.736] (1.000)
Step: 35599, Reward: [-532.549 -532.549 -532.549] [0.0000], Avg: [-599.642 -599.642 -599.642] (1.000)
Step: 35649, Reward: [-554.298 -554.298 -554.298] [0.0000], Avg: [-599.578 -599.578 -599.578] (1.000)
Step: 35699, Reward: [-386.371 -386.371 -386.371] [0.0000], Avg: [-599.28 -599.28 -599.28] (1.000)
Step: 35749, Reward: [-450.947 -450.947 -450.947] [0.0000], Avg: [-599.072 -599.072 -599.072] (1.000)
Step: 35799, Reward: [-400.886 -400.886 -400.886] [0.0000], Avg: [-598.795 -598.795 -598.795] (1.000)
Step: 35849, Reward: [-464.09 -464.09 -464.09] [0.0000], Avg: [-598.608 -598.608 -598.608] (1.000)
Step: 35899, Reward: [-469.3 -469.3 -469.3] [0.0000], Avg: [-598.427 -598.427 -598.427] (1.000)
Step: 35949, Reward: [-380.271 -380.271 -380.271] [0.0000], Avg: [-598.124 -598.124 -598.124] (1.000)
Step: 35999, Reward: [-442.381 -442.381 -442.381] [0.0000], Avg: [-597.908 -597.908 -597.908] (1.000)
Step: 36049, Reward: [-457.842 -457.842 -457.842] [0.0000], Avg: [-597.713 -597.713 -597.713] (1.000)
Step: 36099, Reward: [-391.296 -391.296 -391.296] [0.0000], Avg: [-597.428 -597.428 -597.428] (1.000)
Step: 36149, Reward: [-558.765 -558.765 -558.765] [0.0000], Avg: [-597.374 -597.374 -597.374] (1.000)
Step: 36199, Reward: [-398.483 -398.483 -398.483] [0.0000], Avg: [-597.099 -597.099 -597.099] (1.000)
Step: 36249, Reward: [-426.371 -426.371 -426.371] [0.0000], Avg: [-596.864 -596.864 -596.864] (1.000)
Step: 36299, Reward: [-310.194 -310.194 -310.194] [0.0000], Avg: [-596.469 -596.469 -596.469] (1.000)
Step: 36349, Reward: [-359.451 -359.451 -359.451] [0.0000], Avg: [-596.143 -596.143 -596.143] (1.000)
Step: 36399, Reward: [-446.371 -446.371 -446.371] [0.0000], Avg: [-595.937 -595.937 -595.937] (1.000)
Step: 36449, Reward: [-374.158 -374.158 -374.158] [0.0000], Avg: [-595.633 -595.633 -595.633] (1.000)
Step: 36499, Reward: [-382.282 -382.282 -382.282] [0.0000], Avg: [-595.341 -595.341 -595.341] (1.000)
Step: 36549, Reward: [-367.384 -367.384 -367.384] [0.0000], Avg: [-595.029 -595.029 -595.029] (1.000)
Step: 36599, Reward: [-400.868 -400.868 -400.868] [0.0000], Avg: [-594.764 -594.764 -594.764] (1.000)
Step: 36649, Reward: [-408.959 -408.959 -408.959] [0.0000], Avg: [-594.51 -594.51 -594.51] (1.000)
Step: 36699, Reward: [-397.128 -397.128 -397.128] [0.0000], Avg: [-594.241 -594.241 -594.241] (1.000)
Step: 36749, Reward: [-495.321 -495.321 -495.321] [0.0000], Avg: [-594.107 -594.107 -594.107] (1.000)
Step: 36799, Reward: [-389.84 -389.84 -389.84] [0.0000], Avg: [-593.829 -593.829 -593.829] (1.000)
Step: 36849, Reward: [-511.618 -511.618 -511.618] [0.0000], Avg: [-593.718 -593.718 -593.718] (1.000)
Step: 36899, Reward: [-463.219 -463.219 -463.219] [0.0000], Avg: [-593.541 -593.541 -593.541] (1.000)
Step: 36949, Reward: [-437.875 -437.875 -437.875] [0.0000], Avg: [-593.33 -593.33 -593.33] (1.000)
Step: 36999, Reward: [-436.598 -436.598 -436.598] [0.0000], Avg: [-593.118 -593.118 -593.118] (1.000)
Step: 37049, Reward: [-419.777 -419.777 -419.777] [0.0000], Avg: [-592.884 -592.884 -592.884] (1.000)
Step: 37099, Reward: [-463.212 -463.212 -463.212] [0.0000], Avg: [-592.71 -592.71 -592.71] (1.000)
Step: 37149, Reward: [-285.96 -285.96 -285.96] [0.0000], Avg: [-592.297 -592.297 -592.297] (1.000)
Step: 37199, Reward: [-463.389 -463.389 -463.389] [0.0000], Avg: [-592.124 -592.124 -592.124] (1.000)
Step: 37249, Reward: [-488.833 -488.833 -488.833] [0.0000], Avg: [-591.985 -591.985 -591.985] (1.000)
Step: 37299, Reward: [-376.464 -376.464 -376.464] [0.0000], Avg: [-591.696 -591.696 -591.696] (1.000)
Step: 37349, Reward: [-451.854 -451.854 -451.854] [0.0000], Avg: [-591.509 -591.509 -591.509] (1.000)
Step: 37399, Reward: [-310.235 -310.235 -310.235] [0.0000], Avg: [-591.133 -591.133 -591.133] (1.000)
Step: 37449, Reward: [-323.128 -323.128 -323.128] [0.0000], Avg: [-590.775 -590.775 -590.775] (1.000)
Step: 37499, Reward: [-421.118 -421.118 -421.118] [0.0000], Avg: [-590.549 -590.549 -590.549] (1.000)
Step: 37549, Reward: [-461.296 -461.296 -461.296] [0.0000], Avg: [-590.377 -590.377 -590.377] (1.000)
Step: 37599, Reward: [-423.086 -423.086 -423.086] [0.0000], Avg: [-590.154 -590.154 -590.154] (1.000)
Step: 37649, Reward: [-389.012 -389.012 -389.012] [0.0000], Avg: [-589.887 -589.887 -589.887] (1.000)
Step: 37699, Reward: [-399.443 -399.443 -399.443] [0.0000], Avg: [-589.634 -589.634 -589.634] (1.000)
Step: 37749, Reward: [-427.883 -427.883 -427.883] [0.0000], Avg: [-589.42 -589.42 -589.42] (1.000)
Step: 37799, Reward: [-322.612 -322.612 -322.612] [0.0000], Avg: [-589.067 -589.067 -589.067] (1.000)
Step: 37849, Reward: [-416.626 -416.626 -416.626] [0.0000], Avg: [-588.839 -588.839 -588.839] (1.000)
Step: 37899, Reward: [-438.404 -438.404 -438.404] [0.0000], Avg: [-588.641 -588.641 -588.641] (1.000)
Step: 37949, Reward: [-458.443 -458.443 -458.443] [0.0000], Avg: [-588.469 -588.469 -588.469] (1.000)
Step: 37999, Reward: [-399.386 -399.386 -399.386] [0.0000], Avg: [-588.221 -588.221 -588.221] (1.000)
Step: 38049, Reward: [-301.232 -301.232 -301.232] [0.0000], Avg: [-587.844 -587.844 -587.844] (1.000)
Step: 38099, Reward: [-408.658 -408.658 -408.658] [0.0000], Avg: [-587.608 -587.608 -587.608] (1.000)
Step: 38149, Reward: [-390.375 -390.375 -390.375] [0.0000], Avg: [-587.35 -587.35 -587.35] (1.000)
Step: 38199, Reward: [-363.454 -363.454 -363.454] [0.0000], Avg: [-587.057 -587.057 -587.057] (1.000)
Step: 38249, Reward: [-390.9 -390.9 -390.9] [0.0000], Avg: [-586.8 -586.8 -586.8] (1.000)
Step: 38299, Reward: [-511.509 -511.509 -511.509] [0.0000], Avg: [-586.702 -586.702 -586.702] (1.000)
Step: 38349, Reward: [-358.44 -358.44 -358.44] [0.0000], Avg: [-586.405 -586.405 -586.405] (1.000)
Step: 38399, Reward: [-376.809 -376.809 -376.809] [0.0000], Avg: [-586.132 -586.132 -586.132] (1.000)
Step: 38449, Reward: [-435.818 -435.818 -435.818] [0.0000], Avg: [-585.936 -585.936 -585.936] (1.000)
Step: 38499, Reward: [-429.773 -429.773 -429.773] [0.0000], Avg: [-585.733 -585.733 -585.733] (1.000)
Step: 38549, Reward: [-469.826 -469.826 -469.826] [0.0000], Avg: [-585.583 -585.583 -585.583] (1.000)
Step: 38599, Reward: [-373.957 -373.957 -373.957] [0.0000], Avg: [-585.309 -585.309 -585.309] (1.000)
Step: 38649, Reward: [-400.8 -400.8 -400.8] [0.0000], Avg: [-585.07 -585.07 -585.07] (1.000)
Step: 38699, Reward: [-423.901 -423.901 -423.901] [0.0000], Avg: [-584.862 -584.862 -584.862] (1.000)
Step: 38749, Reward: [-342.921 -342.921 -342.921] [0.0000], Avg: [-584.55 -584.55 -584.55] (1.000)
Step: 38799, Reward: [-330.856 -330.856 -330.856] [0.0000], Avg: [-584.223 -584.223 -584.223] (1.000)
Step: 38849, Reward: [-466.998 -466.998 -466.998] [0.0000], Avg: [-584.072 -584.072 -584.072] (1.000)
Step: 38899, Reward: [-345.265 -345.265 -345.265] [0.0000], Avg: [-583.765 -583.765 -583.765] (1.000)
Step: 38949, Reward: [-515.214 -515.214 -515.214] [0.0000], Avg: [-583.677 -583.677 -583.677] (1.000)
Step: 38999, Reward: [-385.687 -385.687 -385.687] [0.0000], Avg: [-583.423 -583.423 -583.423] (1.000)
Step: 39049, Reward: [-504.892 -504.892 -504.892] [0.0000], Avg: [-583.323 -583.323 -583.323] (1.000)
Step: 39099, Reward: [-461.087 -461.087 -461.087] [0.0000], Avg: [-583.166 -583.166 -583.166] (1.000)
Step: 39149, Reward: [-471.491 -471.491 -471.491] [0.0000], Avg: [-583.024 -583.024 -583.024] (1.000)
Step: 39199, Reward: [-550.412 -550.412 -550.412] [0.0000], Avg: [-582.982 -582.982 -582.982] (1.000)
Step: 39249, Reward: [-418.215 -418.215 -418.215] [0.0000], Avg: [-582.772 -582.772 -582.772] (1.000)
Step: 39299, Reward: [-386.066 -386.066 -386.066] [0.0000], Avg: [-582.522 -582.522 -582.522] (1.000)
Step: 39349, Reward: [-487.341 -487.341 -487.341] [0.0000], Avg: [-582.401 -582.401 -582.401] (1.000)
Step: 39399, Reward: [-696.968 -696.968 -696.968] [0.0000], Avg: [-582.546 -582.546 -582.546] (1.000)
Step: 39449, Reward: [-445.242 -445.242 -445.242] [0.0000], Avg: [-582.372 -582.372 -582.372] (1.000)
Step: 39499, Reward: [-517.472 -517.472 -517.472] [0.0000], Avg: [-582.29 -582.29 -582.29] (1.000)
Step: 39549, Reward: [-482.85 -482.85 -482.85] [0.0000], Avg: [-582.165 -582.165 -582.165] (1.000)
Step: 39599, Reward: [-857.532 -857.532 -857.532] [0.0000], Avg: [-582.512 -582.512 -582.512] (1.000)
Step: 39649, Reward: [-775.792 -775.792 -775.792] [0.0000], Avg: [-582.756 -582.756 -582.756] (1.000)
Step: 39699, Reward: [-853.173 -853.173 -853.173] [0.0000], Avg: [-583.097 -583.097 -583.097] (1.000)
Step: 39749, Reward: [-851.314 -851.314 -851.314] [0.0000], Avg: [-583.434 -583.434 -583.434] (1.000)
Step: 39799, Reward: [-498.853 -498.853 -498.853] [0.0000], Avg: [-583.328 -583.328 -583.328] (1.000)
Step: 39849, Reward: [-691.04 -691.04 -691.04] [0.0000], Avg: [-583.463 -583.463 -583.463] (1.000)
Step: 39899, Reward: [-956.974 -956.974 -956.974] [0.0000], Avg: [-583.931 -583.931 -583.931] (1.000)
Step: 39949, Reward: [-465.43 -465.43 -465.43] [0.0000], Avg: [-583.783 -583.783 -583.783] (1.000)
Step: 39999, Reward: [-637.58 -637.58 -637.58] [0.0000], Avg: [-583.85 -583.85 -583.85] (1.000)
Step: 40049, Reward: [-641.244 -641.244 -641.244] [0.0000], Avg: [-583.921 -583.921 -583.921] (1.000)
Step: 40099, Reward: [-643.051 -643.051 -643.051] [0.0000], Avg: [-583.995 -583.995 -583.995] (1.000)
Step: 40149, Reward: [-669.575 -669.575 -669.575] [0.0000], Avg: [-584.102 -584.102 -584.102] (1.000)
Step: 40199, Reward: [-819.875 -819.875 -819.875] [0.0000], Avg: [-584.395 -584.395 -584.395] (1.000)
Step: 40249, Reward: [-688.67 -688.67 -688.67] [0.0000], Avg: [-584.525 -584.525 -584.525] (1.000)
Step: 40299, Reward: [-599.369 -599.369 -599.369] [0.0000], Avg: [-584.543 -584.543 -584.543] (1.000)
Step: 40349, Reward: [-680.419 -680.419 -680.419] [0.0000], Avg: [-584.662 -584.662 -584.662] (1.000)
Step: 40399, Reward: [-578.968 -578.968 -578.968] [0.0000], Avg: [-584.655 -584.655 -584.655] (1.000)
Step: 40449, Reward: [-468.838 -468.838 -468.838] [0.0000], Avg: [-584.512 -584.512 -584.512] (1.000)
Step: 40499, Reward: [-740.874 -740.874 -740.874] [0.0000], Avg: [-584.705 -584.705 -584.705] (1.000)
Step: 40549, Reward: [-475.923 -475.923 -475.923] [0.0000], Avg: [-584.57 -584.57 -584.57] (1.000)
Step: 40599, Reward: [-565.41 -565.41 -565.41] [0.0000], Avg: [-584.547 -584.547 -584.547] (1.000)
Step: 40649, Reward: [-434.848 -434.848 -434.848] [0.0000], Avg: [-584.363 -584.363 -584.363] (1.000)
Step: 40699, Reward: [-496.237 -496.237 -496.237] [0.0000], Avg: [-584.254 -584.254 -584.254] (1.000)
Step: 40749, Reward: [-375.074 -375.074 -375.074] [0.0000], Avg: [-583.998 -583.998 -583.998] (1.000)
Step: 40799, Reward: [-480.108 -480.108 -480.108] [0.0000], Avg: [-583.87 -583.87 -583.87] (1.000)
Step: 40849, Reward: [-384.523 -384.523 -384.523] [0.0000], Avg: [-583.626 -583.626 -583.626] (1.000)
Step: 40899, Reward: [-435.862 -435.862 -435.862] [0.0000], Avg: [-583.446 -583.446 -583.446] (1.000)
Step: 40949, Reward: [-524.843 -524.843 -524.843] [0.0000], Avg: [-583.374 -583.374 -583.374] (1.000)
Step: 40999, Reward: [-505.839 -505.839 -505.839] [0.0000], Avg: [-583.28 -583.28 -583.28] (1.000)
Step: 41049, Reward: [-601.867 -601.867 -601.867] [0.0000], Avg: [-583.302 -583.302 -583.302] (1.000)
Step: 41099, Reward: [-481.007 -481.007 -481.007] [0.0000], Avg: [-583.178 -583.178 -583.178] (1.000)
Step: 41149, Reward: [-541.321 -541.321 -541.321] [0.0000], Avg: [-583.127 -583.127 -583.127] (1.000)
Step: 41199, Reward: [-537.028 -537.028 -537.028] [0.0000], Avg: [-583.071 -583.071 -583.071] (1.000)
Step: 41249, Reward: [-429.991 -429.991 -429.991] [0.0000], Avg: [-582.886 -582.886 -582.886] (1.000)
Step: 41299, Reward: [-502.129 -502.129 -502.129] [0.0000], Avg: [-582.788 -582.788 -582.788] (1.000)
Step: 41349, Reward: [-396.109 -396.109 -396.109] [0.0000], Avg: [-582.562 -582.562 -582.562] (1.000)
Step: 41399, Reward: [-630.962 -630.962 -630.962] [0.0000], Avg: [-582.621 -582.621 -582.621] (1.000)
Step: 41449, Reward: [-501.106 -501.106 -501.106] [0.0000], Avg: [-582.522 -582.522 -582.522] (1.000)
Step: 41499, Reward: [-419.272 -419.272 -419.272] [0.0000], Avg: [-582.326 -582.326 -582.326] (1.000)
Step: 41549, Reward: [-562.906 -562.906 -562.906] [0.0000], Avg: [-582.302 -582.302 -582.302] (1.000)
Step: 41599, Reward: [-735.358 -735.358 -735.358] [0.0000], Avg: [-582.486 -582.486 -582.486] (1.000)
Step: 41649, Reward: [-438.427 -438.427 -438.427] [0.0000], Avg: [-582.313 -582.313 -582.313] (1.000)
Step: 41699, Reward: [-707.854 -707.854 -707.854] [0.0000], Avg: [-582.464 -582.464 -582.464] (1.000)
Step: 41749, Reward: [-401.513 -401.513 -401.513] [0.0000], Avg: [-582.247 -582.247 -582.247] (1.000)
Step: 41799, Reward: [-580.866 -580.866 -580.866] [0.0000], Avg: [-582.245 -582.245 -582.245] (1.000)
Step: 41849, Reward: [-558.893 -558.893 -558.893] [0.0000], Avg: [-582.217 -582.217 -582.217] (1.000)
Step: 41899, Reward: [-485.948 -485.948 -485.948] [0.0000], Avg: [-582.103 -582.103 -582.103] (1.000)
Step: 41949, Reward: [-539.331 -539.331 -539.331] [0.0000], Avg: [-582.052 -582.052 -582.052] (1.000)
Step: 41999, Reward: [-758.649 -758.649 -758.649] [0.0000], Avg: [-582.262 -582.262 -582.262] (1.000)
Step: 42049, Reward: [-777.615 -777.615 -777.615] [0.0000], Avg: [-582.494 -582.494 -582.494] (1.000)
Step: 42099, Reward: [-448.746 -448.746 -448.746] [0.0000], Avg: [-582.335 -582.335 -582.335] (1.000)
Step: 42149, Reward: [-602.874 -602.874 -602.874] [0.0000], Avg: [-582.36 -582.36 -582.36] (1.000)
Step: 42199, Reward: [-692.896 -692.896 -692.896] [0.0000], Avg: [-582.491 -582.491 -582.491] (1.000)
Step: 42249, Reward: [-577.968 -577.968 -577.968] [0.0000], Avg: [-582.485 -582.485 -582.485] (1.000)
Step: 42299, Reward: [-691.815 -691.815 -691.815] [0.0000], Avg: [-582.614 -582.614 -582.614] (1.000)
Step: 42349, Reward: [-345.733 -345.733 -345.733] [0.0000], Avg: [-582.335 -582.335 -582.335] (1.000)
Step: 42399, Reward: [-381.16 -381.16 -381.16] [0.0000], Avg: [-582.098 -582.098 -582.098] (1.000)
Step: 42449, Reward: [-496.879 -496.879 -496.879] [0.0000], Avg: [-581.997 -581.997 -581.997] (1.000)
Step: 42499, Reward: [-575.337 -575.337 -575.337] [0.0000], Avg: [-581.989 -581.989 -581.989] (1.000)
Step: 42549, Reward: [-530.856 -530.856 -530.856] [0.0000], Avg: [-581.929 -581.929 -581.929] (1.000)
Step: 42599, Reward: [-398.252 -398.252 -398.252] [0.0000], Avg: [-581.714 -581.714 -581.714] (1.000)
Step: 42649, Reward: [-397.579 -397.579 -397.579] [0.0000], Avg: [-581.498 -581.498 -581.498] (1.000)
Step: 42699, Reward: [-653.709 -653.709 -653.709] [0.0000], Avg: [-581.582 -581.582 -581.582] (1.000)
Step: 42749, Reward: [-540.531 -540.531 -540.531] [0.0000], Avg: [-581.534 -581.534 -581.534] (1.000)
Step: 42799, Reward: [-566.374 -566.374 -566.374] [0.0000], Avg: [-581.517 -581.517 -581.517] (1.000)
Step: 42849, Reward: [-274.148 -274.148 -274.148] [0.0000], Avg: [-581.158 -581.158 -581.158] (1.000)
Step: 42899, Reward: [-358.454 -358.454 -358.454] [0.0000], Avg: [-580.898 -580.898 -580.898] (1.000)
Step: 42949, Reward: [-463.871 -463.871 -463.871] [0.0000], Avg: [-580.762 -580.762 -580.762] (1.000)
Step: 42999, Reward: [-336.634 -336.634 -336.634] [0.0000], Avg: [-580.478 -580.478 -580.478] (1.000)
Step: 43049, Reward: [-381.633 -381.633 -381.633] [0.0000], Avg: [-580.247 -580.247 -580.247] (1.000)
Step: 43099, Reward: [-575.866 -575.866 -575.866] [0.0000], Avg: [-580.242 -580.242 -580.242] (1.000)
Step: 43149, Reward: [-485.871 -485.871 -485.871] [0.0000], Avg: [-580.133 -580.133 -580.133] (1.000)
Step: 43199, Reward: [-429.348 -429.348 -429.348] [0.0000], Avg: [-579.958 -579.958 -579.958] (1.000)
Step: 43249, Reward: [-352.854 -352.854 -352.854] [0.0000], Avg: [-579.696 -579.696 -579.696] (1.000)
Step: 43299, Reward: [-378.462 -378.462 -378.462] [0.0000], Avg: [-579.464 -579.464 -579.464] (1.000)
Step: 43349, Reward: [-367.098 -367.098 -367.098] [0.0000], Avg: [-579.219 -579.219 -579.219] (1.000)
Step: 43399, Reward: [-367.133 -367.133 -367.133] [0.0000], Avg: [-578.974 -578.974 -578.974] (1.000)
Step: 43449, Reward: [-405.911 -405.911 -405.911] [0.0000], Avg: [-578.775 -578.775 -578.775] (1.000)
Step: 43499, Reward: [-470.349 -470.349 -470.349] [0.0000], Avg: [-578.65 -578.65 -578.65] (1.000)
Step: 43549, Reward: [-359.298 -359.298 -359.298] [0.0000], Avg: [-578.399 -578.399 -578.399] (1.000)
Step: 43599, Reward: [-398.344 -398.344 -398.344] [0.0000], Avg: [-578.192 -578.192 -578.192] (1.000)
Step: 43649, Reward: [-347.413 -347.413 -347.413] [0.0000], Avg: [-577.928 -577.928 -577.928] (1.000)
Step: 43699, Reward: [-304.986 -304.986 -304.986] [0.0000], Avg: [-577.615 -577.615 -577.615] (1.000)
Step: 43749, Reward: [-338.242 -338.242 -338.242] [0.0000], Avg: [-577.342 -577.342 -577.342] (1.000)
Step: 43799, Reward: [-315.344 -315.344 -315.344] [0.0000], Avg: [-577.043 -577.043 -577.043] (1.000)
Step: 43849, Reward: [-347.184 -347.184 -347.184] [0.0000], Avg: [-576.781 -576.781 -576.781] (1.000)
Step: 43899, Reward: [-347.766 -347.766 -347.766] [0.0000], Avg: [-576.52 -576.52 -576.52] (1.000)
Step: 43949, Reward: [-464.973 -464.973 -464.973] [0.0000], Avg: [-576.393 -576.393 -576.393] (1.000)
Step: 43999, Reward: [-411.323 -411.323 -411.323] [0.0000], Avg: [-576.205 -576.205 -576.205] (1.000)
Step: 44049, Reward: [-586.428 -586.428 -586.428] [0.0000], Avg: [-576.217 -576.217 -576.217] (1.000)
Step: 44099, Reward: [-430.873 -430.873 -430.873] [0.0000], Avg: [-576.052 -576.052 -576.052] (1.000)
Step: 44149, Reward: [-348.03 -348.03 -348.03] [0.0000], Avg: [-575.794 -575.794 -575.794] (1.000)
Step: 44199, Reward: [-297.282 -297.282 -297.282] [0.0000], Avg: [-575.479 -575.479 -575.479] (1.000)
Step: 44249, Reward: [-410.961 -410.961 -410.961] [0.0000], Avg: [-575.293 -575.293 -575.293] (1.000)
Step: 44299, Reward: [-393.064 -393.064 -393.064] [0.0000], Avg: [-575.087 -575.087 -575.087] (1.000)
Step: 44349, Reward: [-364.593 -364.593 -364.593] [0.0000], Avg: [-574.85 -574.85 -574.85] (1.000)
Step: 44399, Reward: [-410.069 -410.069 -410.069] [0.0000], Avg: [-574.664 -574.664 -574.664] (1.000)
Step: 44449, Reward: [-335.978 -335.978 -335.978] [0.0000], Avg: [-574.396 -574.396 -574.396] (1.000)
Step: 44499, Reward: [-331.482 -331.482 -331.482] [0.0000], Avg: [-574.123 -574.123 -574.123] (1.000)
Step: 44549, Reward: [-476.672 -476.672 -476.672] [0.0000], Avg: [-574.014 -574.014 -574.014] (1.000)
Step: 44599, Reward: [-419.678 -419.678 -419.678] [0.0000], Avg: [-573.841 -573.841 -573.841] (1.000)
Step: 44649, Reward: [-356.265 -356.265 -356.265] [0.0000], Avg: [-573.597 -573.597 -573.597] (1.000)
Step: 44699, Reward: [-366.477 -366.477 -366.477] [0.0000], Avg: [-573.365 -573.365 -573.365] (1.000)
Step: 44749, Reward: [-389.169 -389.169 -389.169] [0.0000], Avg: [-573.16 -573.16 -573.16] (1.000)
Step: 44799, Reward: [-361.277 -361.277 -361.277] [0.0000], Avg: [-572.923 -572.923 -572.923] (1.000)
Step: 44849, Reward: [-363.813 -363.813 -363.813] [0.0000], Avg: [-572.69 -572.69 -572.69] (1.000)
Step: 44899, Reward: [-561.979 -561.979 -561.979] [0.0000], Avg: [-572.678 -572.678 -572.678] (1.000)
Step: 44949, Reward: [-380.007 -380.007 -380.007] [0.0000], Avg: [-572.464 -572.464 -572.464] (1.000)
Step: 44999, Reward: [-418.813 -418.813 -418.813] [0.0000], Avg: [-572.293 -572.293 -572.293] (1.000)
Step: 45049, Reward: [-469.098 -469.098 -469.098] [0.0000], Avg: [-572.178 -572.178 -572.178] (1.000)
Step: 45099, Reward: [-523.327 -523.327 -523.327] [0.0000], Avg: [-572.124 -572.124 -572.124] (1.000)
Step: 45149, Reward: [-417.314 -417.314 -417.314] [0.0000], Avg: [-571.953 -571.953 -571.953] (1.000)
Step: 45199, Reward: [-410.27 -410.27 -410.27] [0.0000], Avg: [-571.774 -571.774 -571.774] (1.000)
Step: 45249, Reward: [-455.689 -455.689 -455.689] [0.0000], Avg: [-571.646 -571.646 -571.646] (1.000)
Step: 45299, Reward: [-371.336 -371.336 -371.336] [0.0000], Avg: [-571.425 -571.425 -571.425] (1.000)
Step: 45349, Reward: [-382.876 -382.876 -382.876] [0.0000], Avg: [-571.217 -571.217 -571.217] (1.000)
Step: 45399, Reward: [-393.332 -393.332 -393.332] [0.0000], Avg: [-571.021 -571.021 -571.021] (1.000)
Step: 45449, Reward: [-392.305 -392.305 -392.305] [0.0000], Avg: [-570.824 -570.824 -570.824] (1.000)
Step: 45499, Reward: [-433.35 -433.35 -433.35] [0.0000], Avg: [-570.673 -570.673 -570.673] (1.000)
Step: 45549, Reward: [-420.892 -420.892 -420.892] [0.0000], Avg: [-570.509 -570.509 -570.509] (1.000)
Step: 45599, Reward: [-450.783 -450.783 -450.783] [0.0000], Avg: [-570.377 -570.377 -570.377] (1.000)
Step: 45649, Reward: [-286.893 -286.893 -286.893] [0.0000], Avg: [-570.067 -570.067 -570.067] (1.000)
Step: 45699, Reward: [-485.294 -485.294 -485.294] [0.0000], Avg: [-569.974 -569.974 -569.974] (1.000)
Step: 45749, Reward: [-328.032 -328.032 -328.032] [0.0000], Avg: [-569.71 -569.71 -569.71] (1.000)
Step: 45799, Reward: [-400.203 -400.203 -400.203] [0.0000], Avg: [-569.525 -569.525 -569.525] (1.000)
Step: 45849, Reward: [-364.924 -364.924 -364.924] [0.0000], Avg: [-569.302 -569.302 -569.302] (1.000)
Step: 45899, Reward: [-366.74 -366.74 -366.74] [0.0000], Avg: [-569.081 -569.081 -569.081] (1.000)
Step: 45949, Reward: [-298.722 -298.722 -298.722] [0.0000], Avg: [-568.787 -568.787 -568.787] (1.000)
Step: 45999, Reward: [-300.729 -300.729 -300.729] [0.0000], Avg: [-568.495 -568.495 -568.495] (1.000)
Step: 46049, Reward: [-357.76 -357.76 -357.76] [0.0000], Avg: [-568.267 -568.267 -568.267] (1.000)
Step: 46099, Reward: [-465.867 -465.867 -465.867] [0.0000], Avg: [-568.156 -568.156 -568.156] (1.000)
Step: 46149, Reward: [-379.631 -379.631 -379.631] [0.0000], Avg: [-567.951 -567.951 -567.951] (1.000)
Step: 46199, Reward: [-415.435 -415.435 -415.435] [0.0000], Avg: [-567.786 -567.786 -567.786] (1.000)
Step: 46249, Reward: [-371.484 -371.484 -371.484] [0.0000], Avg: [-567.574 -567.574 -567.574] (1.000)
Step: 46299, Reward: [-334.756 -334.756 -334.756] [0.0000], Avg: [-567.323 -567.323 -567.323] (1.000)
Step: 46349, Reward: [-342.595 -342.595 -342.595] [0.0000], Avg: [-567.08 -567.08 -567.08] (1.000)
Step: 46399, Reward: [-395.548 -395.548 -395.548] [0.0000], Avg: [-566.895 -566.895 -566.895] (1.000)
Step: 46449, Reward: [-345.407 -345.407 -345.407] [0.0000], Avg: [-566.657 -566.657 -566.657] (1.000)
Step: 46499, Reward: [-439.327 -439.327 -439.327] [0.0000], Avg: [-566.52 -566.52 -566.52] (1.000)
Step: 46549, Reward: [-442.418 -442.418 -442.418] [0.0000], Avg: [-566.387 -566.387 -566.387] (1.000)
Step: 46599, Reward: [-409.199 -409.199 -409.199] [0.0000], Avg: [-566.218 -566.218 -566.218] (1.000)
Step: 46649, Reward: [-322.859 -322.859 -322.859] [0.0000], Avg: [-565.957 -565.957 -565.957] (1.000)
Step: 46699, Reward: [-272.968 -272.968 -272.968] [0.0000], Avg: [-565.644 -565.644 -565.644] (1.000)
Step: 46749, Reward: [-406.494 -406.494 -406.494] [0.0000], Avg: [-565.473 -565.473 -565.473] (1.000)
Step: 46799, Reward: [-285.296 -285.296 -285.296] [0.0000], Avg: [-565.174 -565.174 -565.174] (1.000)
Step: 46849, Reward: [-458.565 -458.565 -458.565] [0.0000], Avg: [-565.06 -565.06 -565.06] (1.000)
Step: 46899, Reward: [-364.771 -364.771 -364.771] [0.0000], Avg: [-564.847 -564.847 -564.847] (1.000)
Step: 46949, Reward: [-379.172 -379.172 -379.172] [0.0000], Avg: [-564.649 -564.649 -564.649] (1.000)
Step: 46999, Reward: [-418.471 -418.471 -418.471] [0.0000], Avg: [-564.493 -564.493 -564.493] (1.000)
Step: 47049, Reward: [-421.856 -421.856 -421.856] [0.0000], Avg: [-564.342 -564.342 -564.342] (1.000)
Step: 47099, Reward: [-391.27 -391.27 -391.27] [0.0000], Avg: [-564.158 -564.158 -564.158] (1.000)
Step: 47149, Reward: [-370.198 -370.198 -370.198] [0.0000], Avg: [-563.952 -563.952 -563.952] (1.000)
Step: 47199, Reward: [-378.834 -378.834 -378.834] [0.0000], Avg: [-563.756 -563.756 -563.756] (1.000)
Step: 47249, Reward: [-477.264 -477.264 -477.264] [0.0000], Avg: [-563.665 -563.665 -563.665] (1.000)
Step: 47299, Reward: [-324.468 -324.468 -324.468] [0.0000], Avg: [-563.412 -563.412 -563.412] (1.000)
Step: 47349, Reward: [-398.249 -398.249 -398.249] [0.0000], Avg: [-563.238 -563.238 -563.238] (1.000)
Step: 47399, Reward: [-405.619 -405.619 -405.619] [0.0000], Avg: [-563.071 -563.071 -563.071] (1.000)
Step: 47449, Reward: [-473.492 -473.492 -473.492] [0.0000], Avg: [-562.977 -562.977 -562.977] (1.000)
Step: 47499, Reward: [-363.611 -363.611 -363.611] [0.0000], Avg: [-562.767 -562.767 -562.767] (1.000)
Step: 47549, Reward: [-456.405 -456.405 -456.405] [0.0000], Avg: [-562.655 -562.655 -562.655] (1.000)
Step: 47599, Reward: [-437.435 -437.435 -437.435] [0.0000], Avg: [-562.524 -562.524 -562.524] (1.000)
Step: 47649, Reward: [-354.13 -354.13 -354.13] [0.0000], Avg: [-562.305 -562.305 -562.305] (1.000)
Step: 47699, Reward: [-301.19 -301.19 -301.19] [0.0000], Avg: [-562.031 -562.031 -562.031] (1.000)
Step: 47749, Reward: [-310.556 -310.556 -310.556] [0.0000], Avg: [-561.768 -561.768 -561.768] (1.000)
Step: 47799, Reward: [-338.357 -338.357 -338.357] [0.0000], Avg: [-561.534 -561.534 -561.534] (1.000)
Step: 47849, Reward: [-489.404 -489.404 -489.404] [0.0000], Avg: [-561.459 -561.459 -561.459] (1.000)
Step: 47899, Reward: [-422.286 -422.286 -422.286] [0.0000], Avg: [-561.314 -561.314 -561.314] (1.000)
Step: 47949, Reward: [-432.003 -432.003 -432.003] [0.0000], Avg: [-561.179 -561.179 -561.179] (1.000)
Step: 47999, Reward: [-500.99 -500.99 -500.99] [0.0000], Avg: [-561.116 -561.116 -561.116] (1.000)
Step: 48049, Reward: [-516.34 -516.34 -516.34] [0.0000], Avg: [-561.069 -561.069 -561.069] (1.000)
Step: 48099, Reward: [-467.816 -467.816 -467.816] [0.0000], Avg: [-560.973 -560.973 -560.973] (1.000)
Step: 48149, Reward: [-301.298 -301.298 -301.298] [0.0000], Avg: [-560.703 -560.703 -560.703] (1.000)
Step: 48199, Reward: [-296.091 -296.091 -296.091] [0.0000], Avg: [-560.428 -560.428 -560.428] (1.000)
Step: 48249, Reward: [-380.208 -380.208 -380.208] [0.0000], Avg: [-560.242 -560.242 -560.242] (1.000)
Step: 48299, Reward: [-454.186 -454.186 -454.186] [0.0000], Avg: [-560.132 -560.132 -560.132] (1.000)
Step: 48349, Reward: [-326.636 -326.636 -326.636] [0.0000], Avg: [-559.89 -559.89 -559.89] (1.000)
Step: 48399, Reward: [-409.015 -409.015 -409.015] [0.0000], Avg: [-559.735 -559.735 -559.735] (1.000)
Step: 48449, Reward: [-468.571 -468.571 -468.571] [0.0000], Avg: [-559.64 -559.64 -559.64] (1.000)
Step: 48499, Reward: [-412.922 -412.922 -412.922] [0.0000], Avg: [-559.489 -559.489 -559.489] (1.000)
Step: 48549, Reward: [-397.809 -397.809 -397.809] [0.0000], Avg: [-559.323 -559.323 -559.323] (1.000)
Step: 48599, Reward: [-358.053 -358.053 -358.053] [0.0000], Avg: [-559.116 -559.116 -559.116] (1.000)
Step: 48649, Reward: [-405.219 -405.219 -405.219] [0.0000], Avg: [-558.957 -558.957 -558.957] (1.000)
Step: 48699, Reward: [-432.336 -432.336 -432.336] [0.0000], Avg: [-558.827 -558.827 -558.827] (1.000)
