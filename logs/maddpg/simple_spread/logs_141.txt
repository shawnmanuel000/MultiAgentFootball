Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread, Date: 13/03/2020 03:13:23
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import torch
import numpy as np
from models.rand import MultiagentReplayBuffer, MultiagentReplayBuffer3
from models.ddpg import DDPGCritic, DDPGNetwork
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, TARGET_UPDATE_RATE, gsoftmax, one_hot

EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
LEARN_RATE = 0.001				# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
ENTROPY_WEIGHT = 1			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 10			# How many experience tuples to sample from the buffer for each train step
MAX_BUFFER_SIZE = 64			# Sets the maximum length of the replay buffer
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return gsoftmax(action, hard=not sample)

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu, name="maddpg")
		self.critic = lambda s,a: DDPGCritic([np.sum([np.prod(s) for s in state_size])], [np.sum([np.prod(a) for a in action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(state_size, action_size)]
		self.action_size = action_size
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_probs = [model.get_action(s, use_target, grad, numpy=numpy, sample=sample) for s,model in zip(state, self.models)]
			return action_probs

	def optimize(self, states, actions, states_joint, actions_joint, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		stats = []
		for i, (agent, state, reward, done) in enumerate(zip(self.models, states, rewards, dones)):
			next_value = agent.get_q_value(states_joint, actions_joint, use_target=True, numpy=False)
			next_value = torch.cat([next_value, torch.zeros_like(next_value[:,-1]).unsqueeze(1)], dim=1)
			q_targets = PTACAgent.compute_ma_gae(reward.unsqueeze(-1), done.unsqueeze(-1), next_value, gamma=0.99)
			q_values = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (q_values - q_targets.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			actor_action = agent.get_action(state, grad=True, numpy=False)
			action = [gsoftmax(actor_action, hard=True) if j==i else one_hot(action) for (j,model), action in zip(enumerate(self.models), actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -(agent.critic_local(states_joint, action_joint)-q_targets).mean() + e_weight*actor_action.pow(2).mean()
			agent.step(agent.actor_optimizer, actor_loss, agent.actor_local.parameters())
			stats.append([x.detach().cpu().numpy() for x in [critic_loss, actor_loss]])
		return np.mean(stats, axis=-1)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, dirname, f"{name}_{i}", self.name) for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, dirname, f"{name}_{i}", self.name) for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)
		self.stats = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [np.tanh((1-eps)*a_greedy + eps*a_random) for a_greedy, a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([one_hot(a).view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			self.replay_buffer.add([self.to_numpy([t.transpose(0,1) for t in x]) for x in (states, actions, [states_joint], [actions_joint], rewards, dones)])
			self.buffer.clear()	
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, states_joint, actions_joint, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			self.stats.append(self.network.optimize(states, actions, states_joint[0], actions_joint[0], rewards, dones, gamma=DISCOUNT_RATE))			
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

	def get_stats(self):
		stats = {k:v for k,v in zip(["critic_loss", "actor_loss"], np.mean(self.stats, axis=0))} if len(self.stats)>0 else {}
		self.stats = []
		return {**stats, **super().get_stats()}

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward,eps: [0.1*(ballr(o[0,88], o[0,89])) + r for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def train(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.selfport is not None or MPI_RANK>0:
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	elif args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	else:
		train(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-505.211 -505.211 -505.211] [129.129], Avg: [-505.211 -505.211 -505.211] (1.0000) <00:00:00> ({r_i: None, r_t: [-9.676 -9.676 -9.676], eps: 1.0})
Step:     100, Reward: [-453.233 -453.233 -453.233] [77.945], Avg: [-479.222 -479.222 -479.222] (0.9801) <00:00:02> ({r_i: None, r_t: [-992.377 -992.377 -992.377], critic_loss: 7709.7939453125, actor_loss: 7691.2880859375, eps: 0.98})
Step:     200, Reward: [-485.815 -485.815 -485.815] [79.062], Avg: [-481.420 -481.420 -481.420] (0.9606) <00:00:04> ({r_i: None, r_t: [-989.291 -989.291 -989.291], critic_loss: 6370.044921875, actor_loss: 6369.3671875, eps: 0.961})
Step:     300, Reward: [-534.832 -534.832 -534.832] [95.998], Avg: [-494.773 -494.773 -494.773] (0.9415) <00:00:06> ({r_i: None, r_t: [-1028.884 -1028.884 -1028.884], critic_loss: 8295.8564453125, actor_loss: 8298.7783203125, eps: 0.941})
Step:     400, Reward: [-523.420 -523.420 -523.420] [130.763], Avg: [-500.502 -500.502 -500.502] (0.9227) <00:00:08> ({r_i: None, r_t: [-1004.014 -1004.014 -1004.014], critic_loss: 7138.24609375, actor_loss: 7129.36181640625, eps: 0.923})
Step:     500, Reward: [-497.618 -497.618 -497.618] [101.245], Avg: [-500.022 -500.022 -500.022] (0.9044) <00:00:11> ({r_i: None, r_t: [-961.505 -961.505 -961.505], critic_loss: 6535.85400390625, actor_loss: 6540.90087890625, eps: 0.904})
Step:     600, Reward: [-472.711 -472.711 -472.711] [87.903], Avg: [-496.120 -496.120 -496.120] (0.8864) <00:00:13> ({r_i: None, r_t: [-946.753 -946.753 -946.753], critic_loss: 7291.794921875, actor_loss: 7264.994140625, eps: 0.886})
Step:     700, Reward: [-471.427 -471.427 -471.427] [57.705], Avg: [-493.033 -493.033 -493.033] (0.8687) <00:00:16> ({r_i: None, r_t: [-986.068 -986.068 -986.068], critic_loss: 5677.494140625, actor_loss: 5657.35986328125, eps: 0.869})
Step:     800, Reward: [-487.206 -487.206 -487.206] [88.505], Avg: [-492.386 -492.386 -492.386] (0.8515) <00:00:18> ({r_i: None, r_t: [-994.507 -994.507 -994.507], critic_loss: 6116.666015625, actor_loss: 6112.68212890625, eps: 0.851})
Step:     900, Reward: [-490.823 -490.823 -490.823] [99.240], Avg: [-492.230 -492.230 -492.230] (0.8345) <00:00:20> ({r_i: None, r_t: [-986.708 -986.708 -986.708], critic_loss: 6845.3017578125, actor_loss: 6807.72802734375, eps: 0.835})
Step:    1000, Reward: [-474.315 -474.315 -474.315] [110.225], Avg: [-490.601 -490.601 -490.601] (0.8179) <00:00:23> ({r_i: None, r_t: [-986.363 -986.363 -986.363], critic_loss: 5074.421875, actor_loss: 4938.1201171875, eps: 0.818})
Step:    1100, Reward: [-488.042 -488.042 -488.042] [83.290], Avg: [-490.388 -490.388 -490.388] (0.8016) <00:00:25> ({r_i: None, r_t: [-1036.288 -1036.288 -1036.288], critic_loss: 6796.8251953125, actor_loss: 6585.16796875, eps: 0.802})
Step:    1200, Reward: [-471.168 -471.168 -471.168] [67.708], Avg: [-488.909 -488.909 -488.909] (0.7857) <00:00:27> ({r_i: None, r_t: [-1051.751 -1051.751 -1051.751], critic_loss: 5593.826171875, actor_loss: 5455.27978515625, eps: 0.786})
Step:    1300, Reward: [-469.611 -469.611 -469.611] [93.621], Avg: [-487.531 -487.531 -487.531] (0.7700) <00:00:30> ({r_i: None, r_t: [-945.633 -945.633 -945.633], critic_loss: 4531.8701171875, actor_loss: 4379.9599609375, eps: 0.77})
Step:    1400, Reward: [-483.245 -483.245 -483.245] [80.864], Avg: [-487.245 -487.245 -487.245] (0.7547) <00:00:32> ({r_i: None, r_t: [-970.611 -970.611 -970.611], critic_loss: 3152.364013671875, actor_loss: 2932.8798828125, eps: 0.755})
Step:    1500, Reward: [-484.924 -484.924 -484.924] [113.085], Avg: [-487.100 -487.100 -487.100] (0.7397) <00:00:34> ({r_i: None, r_t: [-1017.866 -1017.866 -1017.866], critic_loss: 3077.56689453125, actor_loss: 2920.126953125, eps: 0.74})
Step:    1600, Reward: [-516.041 -516.041 -516.041] [112.701], Avg: [-488.802 -488.802 -488.802] (0.7250) <00:00:36> ({r_i: None, r_t: [-929.877 -929.877 -929.877], critic_loss: 1823.2769775390625, actor_loss: 1745.052978515625, eps: 0.725})
Step:    1700, Reward: [-479.586 -479.586 -479.586] [93.444], Avg: [-488.290 -488.290 -488.290] (0.7106) <00:00:39> ({r_i: None, r_t: [-951.183 -951.183 -951.183], critic_loss: 1792.75, actor_loss: 1860.133056640625, eps: 0.711})
Step:    1800, Reward: [-515.719 -515.719 -515.719] [118.036], Avg: [-489.734 -489.734 -489.734] (0.6964) <00:00:41> ({r_i: None, r_t: [-981.991 -981.991 -981.991], critic_loss: 1382.5860595703125, actor_loss: 1450.9019775390625, eps: 0.696})
Step:    1900, Reward: [-469.185 -469.185 -469.185] [61.541], Avg: [-488.707 -488.707 -488.707] (0.6826) <00:00:43> ({r_i: None, r_t: [-1023.560 -1023.560 -1023.560], critic_loss: 1936.989013671875, actor_loss: 2138.031005859375, eps: 0.683})
Step:    2000, Reward: [-499.519 -499.519 -499.519] [81.161], Avg: [-489.221 -489.221 -489.221] (0.6690) <00:00:46> ({r_i: None, r_t: [-952.827 -952.827 -952.827], critic_loss: 2704.15087890625, actor_loss: 2698.02587890625, eps: 0.669})
Step:    2100, Reward: [-556.125 -556.125 -556.125] [116.580], Avg: [-492.263 -492.263 -492.263] (0.6557) <00:00:48> ({r_i: None, r_t: [-1026.658 -1026.658 -1026.658], critic_loss: 1886.1700439453125, actor_loss: 2323.510009765625, eps: 0.656})
Step:    2200, Reward: [-585.743 -585.743 -585.743] [129.696], Avg: [-496.327 -496.327 -496.327] (0.6426) <00:00:50> ({r_i: None, r_t: [-1005.916 -1005.916 -1005.916], critic_loss: 2749.89404296875, actor_loss: 3006.343994140625, eps: 0.643})
Step:    2300, Reward: [-506.450 -506.450 -506.450] [105.549], Avg: [-496.749 -496.749 -496.749] (0.6298) <00:00:53> ({r_i: None, r_t: [-953.819 -953.819 -953.819], critic_loss: 2425.97802734375, actor_loss: 2788.403076171875, eps: 0.63})
Step:    2400, Reward: [-514.905 -514.905 -514.905] [92.208], Avg: [-497.475 -497.475 -497.475] (0.6173) <00:00:55> ({r_i: None, r_t: [-1027.649 -1027.649 -1027.649], critic_loss: 2470.39208984375, actor_loss: 2686.180908203125, eps: 0.617})
Step:    2500, Reward: [-539.537 -539.537 -539.537] [116.969], Avg: [-499.093 -499.093 -499.093] (0.6050) <00:00:57> ({r_i: None, r_t: [-1079.067 -1079.067 -1079.067], critic_loss: 2334.29296875, actor_loss: 2674.654052734375, eps: 0.605})
Step:    2600, Reward: [-526.325 -526.325 -526.325] [147.935], Avg: [-500.101 -500.101 -500.101] (0.5930) <00:01:00> ({r_i: None, r_t: [-1072.403 -1072.403 -1072.403], critic_loss: 1987.4630126953125, actor_loss: 2157.962890625, eps: 0.593})
Step:    2700, Reward: [-562.074 -562.074 -562.074] [157.886], Avg: [-502.315 -502.315 -502.315] (0.5812) <00:01:02> ({r_i: None, r_t: [-1066.057 -1066.057 -1066.057], critic_loss: 2075.7919921875, actor_loss: 2155.60595703125, eps: 0.581})
Step:    2800, Reward: [-599.126 -599.126 -599.126] [132.135], Avg: [-505.653 -505.653 -505.653] (0.5696) <00:01:04> ({r_i: None, r_t: [-1174.944 -1174.944 -1174.944], critic_loss: 3247.464111328125, actor_loss: 3384.60400390625, eps: 0.57})
Step:    2900, Reward: [-543.754 -543.754 -543.754] [71.025], Avg: [-506.923 -506.923 -506.923] (0.5583) <00:01:07> ({r_i: None, r_t: [-1176.672 -1176.672 -1176.672], critic_loss: 2046.053955078125, actor_loss: 2046.0240478515625, eps: 0.558})
Step:    3000, Reward: [-631.802 -631.802 -631.802] [169.473], Avg: [-510.951 -510.951 -510.951] (0.5472) <00:01:09> ({r_i: None, r_t: [-1086.962 -1086.962 -1086.962], critic_loss: 2613.01806640625, actor_loss: 2594.39794921875, eps: 0.547})
Step:    3100, Reward: [-577.241 -577.241 -577.241] [130.268], Avg: [-513.023 -513.023 -513.023] (0.5363) <00:01:11> ({r_i: None, r_t: [-1109.410 -1109.410 -1109.410], critic_loss: 1647.4560546875, actor_loss: 1622.7740478515625, eps: 0.536})
Step:    3200, Reward: [-603.934 -603.934 -603.934] [206.731], Avg: [-515.778 -515.778 -515.778] (0.5256) <00:01:14> ({r_i: None, r_t: [-1088.925 -1088.925 -1088.925], critic_loss: 2067.31103515625, actor_loss: 2099.343994140625, eps: 0.526})
Step:    3300, Reward: [-559.479 -559.479 -559.479] [104.019], Avg: [-517.063 -517.063 -517.063] (0.5151) <00:01:16> ({r_i: None, r_t: [-1132.178 -1132.178 -1132.178], critic_loss: 1300.616943359375, actor_loss: 1277.0179443359375, eps: 0.515})
Step:    3400, Reward: [-523.622 -523.622 -523.622] [93.579], Avg: [-517.251 -517.251 -517.251] (0.5049) <00:01:18> ({r_i: None, r_t: [-1150.906 -1150.906 -1150.906], critic_loss: 2742.00390625, actor_loss: 2663.845947265625, eps: 0.505})
Step:    3500, Reward: [-549.855 -549.855 -549.855] [123.134], Avg: [-518.156 -518.156 -518.156] (0.4948) <00:01:20> ({r_i: None, r_t: [-1121.476 -1121.476 -1121.476], critic_loss: 2059.14990234375, actor_loss: 2029.614990234375, eps: 0.495})
Step:    3600, Reward: [-601.452 -601.452 -601.452] [128.245], Avg: [-520.407 -520.407 -520.407] (0.4850) <00:01:23> ({r_i: None, r_t: [-1126.532 -1126.532 -1126.532], critic_loss: 1822.4649658203125, actor_loss: 1770.654052734375, eps: 0.485})
Step:    3700, Reward: [-641.584 -641.584 -641.584] [127.292], Avg: [-523.596 -523.596 -523.596] (0.4753) <00:01:25> ({r_i: None, r_t: [-1110.552 -1110.552 -1110.552], critic_loss: 1798.8409423828125, actor_loss: 1778.7939453125, eps: 0.475})
Step:    3800, Reward: [-623.048 -623.048 -623.048] [89.944], Avg: [-526.146 -526.146 -526.146] (0.4659) <00:01:27> ({r_i: None, r_t: [-1186.192 -1186.192 -1186.192], critic_loss: 1442.050048828125, actor_loss: 1422.22900390625, eps: 0.466})
Step:    3900, Reward: [-556.050 -556.050 -556.050] [107.721], Avg: [-526.894 -526.894 -526.894] (0.4566) <00:01:30> ({r_i: None, r_t: [-1208.772 -1208.772 -1208.772], critic_loss: 3197.010009765625, actor_loss: 3238.864013671875, eps: 0.457})
Step:    4000, Reward: [-557.213 -557.213 -557.213] [95.604], Avg: [-527.633 -527.633 -527.633] (0.4475) <00:01:32> ({r_i: None, r_t: [-1221.952 -1221.952 -1221.952], critic_loss: 1323.3699951171875, actor_loss: 1265.5760498046875, eps: 0.448})
Step:    4100, Reward: [-600.737 -600.737 -600.737] [138.378], Avg: [-529.374 -529.374 -529.374] (0.4386) <00:01:35> ({r_i: None, r_t: [-1211.143 -1211.143 -1211.143], critic_loss: 2476.7900390625, actor_loss: 2464.662109375, eps: 0.439})
Step:    4200, Reward: [-612.150 -612.150 -612.150] [115.454], Avg: [-531.299 -531.299 -531.299] (0.4299) <00:01:38> ({r_i: None, r_t: [-1114.658 -1114.658 -1114.658], critic_loss: 1805.551025390625, actor_loss: 1733.0260009765625, eps: 0.43})
Step:    4300, Reward: [-562.065 -562.065 -562.065] [92.914], Avg: [-531.998 -531.998 -531.998] (0.4213) <00:01:40> ({r_i: None, r_t: [-1173.624 -1173.624 -1173.624], critic_loss: 1990.717041015625, actor_loss: 1997.5489501953125, eps: 0.421})
Step:    4400, Reward: [-656.551 -656.551 -656.551] [122.751], Avg: [-534.766 -534.766 -534.766] (0.4129) <00:01:42> ({r_i: None, r_t: [-1181.768 -1181.768 -1181.768], critic_loss: 1379.2110595703125, actor_loss: 1417.81494140625, eps: 0.413})
Step:    4500, Reward: [-736.422 -736.422 -736.422] [196.994], Avg: [-539.150 -539.150 -539.150] (0.4047) <00:01:45> ({r_i: None, r_t: [-1207.402 -1207.402 -1207.402], critic_loss: 2239.762939453125, actor_loss: 2356.428955078125, eps: 0.405})
Step:    4600, Reward: [-763.130 -763.130 -763.130] [189.236], Avg: [-543.915 -543.915 -543.915] (0.3967) <00:01:47> ({r_i: None, r_t: [-1393.819 -1393.819 -1393.819], critic_loss: 1912.948974609375, actor_loss: 2067.193115234375, eps: 0.397})
Step:    4700, Reward: [-801.763 -801.763 -801.763] [152.152], Avg: [-549.287 -549.287 -549.287] (0.3888) <00:01:49> ({r_i: None, r_t: [-1384.558 -1384.558 -1384.558], critic_loss: 1810.0360107421875, actor_loss: 1935.72705078125, eps: 0.389})
Step:    4800, Reward: [-955.227 -955.227 -955.227] [204.124], Avg: [-557.572 -557.572 -557.572] (0.3810) <00:01:51> ({r_i: None, r_t: [-1640.781 -1640.781 -1640.781], critic_loss: 3645.72607421875, actor_loss: 3660.218017578125, eps: 0.381})
Step:    4900, Reward: [-949.712 -949.712 -949.712] [132.374], Avg: [-565.415 -565.415 -565.415] (0.3735) <00:01:54> ({r_i: None, r_t: [-1565.688 -1565.688 -1565.688], critic_loss: 2904.672119140625, actor_loss: 2823.491943359375, eps: 0.373})
Step:    5000, Reward: [-983.351 -983.351 -983.351] [183.917], Avg: [-573.609 -573.609 -573.609] (0.3660) <00:01:56> ({r_i: None, r_t: [-1760.707 -1760.707 -1760.707], critic_loss: 3226.635986328125, actor_loss: 3395.0859375, eps: 0.366})
Step:    5100, Reward: [-998.274 -998.274 -998.274] [254.825], Avg: [-581.776 -581.776 -581.776] (0.3587) <00:01:59> ({r_i: None, r_t: [-1803.095 -1803.095 -1803.095], critic_loss: 3897.20703125, actor_loss: 3991.345947265625, eps: 0.359})
Step:    5200, Reward: [-1034.093 -1034.093 -1034.093] [207.408], Avg: [-590.310 -590.310 -590.310] (0.3516) <00:02:01> ({r_i: None, r_t: [-1891.105 -1891.105 -1891.105], critic_loss: 3327.735107421875, actor_loss: 3261.2041015625, eps: 0.352})
Step:    5300, Reward: [-950.816 -950.816 -950.816] [157.839], Avg: [-596.986 -596.986 -596.986] (0.3446) <00:02:03> ({r_i: None, r_t: [-1919.279 -1919.279 -1919.279], critic_loss: 4548.81396484375, actor_loss: 4464.4091796875, eps: 0.345})
Step:    5400, Reward: [-799.792 -799.792 -799.792] [129.867], Avg: [-600.674 -600.674 -600.674] (0.3378) <00:02:05> ({r_i: None, r_t: [-1705.377 -1705.377 -1705.377], critic_loss: 3007.26708984375, actor_loss: 2758.154052734375, eps: 0.338})
Step:    5500, Reward: [-753.991 -753.991 -753.991] [110.138], Avg: [-603.412 -603.412 -603.412] (0.3310) <00:02:08> ({r_i: None, r_t: [-1484.685 -1484.685 -1484.685], critic_loss: 3700.425048828125, actor_loss: 3376.841064453125, eps: 0.331})
Step:    5600, Reward: [-574.962 -574.962 -574.962] [134.407], Avg: [-602.912 -602.912 -602.912] (0.3244) <00:02:10> ({r_i: None, r_t: [-1489.298 -1489.298 -1489.298], critic_loss: 3544.535888671875, actor_loss: 3454.614013671875, eps: 0.324})
Step:    5700, Reward: [-563.817 -563.817 -563.817] [121.852], Avg: [-602.238 -602.238 -602.238] (0.3180) <00:02:12> ({r_i: None, r_t: [-1383.626 -1383.626 -1383.626], critic_loss: 5163.38623046875, actor_loss: 5455.85693359375, eps: 0.318})
Step:    5800, Reward: [-576.242 -576.242 -576.242] [191.012], Avg: [-601.798 -601.798 -601.798] (0.3117) <00:02:15> ({r_i: None, r_t: [-1218.703 -1218.703 -1218.703], critic_loss: 5798.23193359375, actor_loss: 5720.4501953125, eps: 0.312})
Step:    5900, Reward: [-722.426 -722.426 -722.426] [181.601], Avg: [-603.808 -603.808 -603.808] (0.3055) <00:02:17> ({r_i: None, r_t: [-1205.393 -1205.393 -1205.393], critic_loss: 5309.29296875, actor_loss: 5829.68017578125, eps: 0.305})
Step:    6000, Reward: [-834.487 -834.487 -834.487] [239.525], Avg: [-607.590 -607.590 -607.590] (0.2994) <00:02:19> ({r_i: None, r_t: [-1201.615 -1201.615 -1201.615], critic_loss: 3928.9599609375, actor_loss: 4386.84423828125, eps: 0.299})
Step:    6100, Reward: [-1214.251 -1214.251 -1214.251] [297.593], Avg: [-617.375 -617.375 -617.375] (0.2934) <00:02:22> ({r_i: None, r_t: [-1540.672 -1540.672 -1540.672], critic_loss: 3064.6279296875, actor_loss: 2939.9599609375, eps: 0.293})
Step:    6200, Reward: [-1393.699 -1393.699 -1393.699] [264.338], Avg: [-629.697 -629.697 -629.697] (0.2876) <00:02:24> ({r_i: None, r_t: [-2023.543 -2023.543 -2023.543], critic_loss: 8492.7255859375, actor_loss: 7268.080078125, eps: 0.288})
Step:    6300, Reward: [-1522.458 -1522.458 -1522.458] [221.225], Avg: [-643.647 -643.647 -643.647] (0.2819) <00:02:26> ({r_i: None, r_t: [-2510.702 -2510.702 -2510.702], critic_loss: 11785.2548828125, actor_loss: 10941.1904296875, eps: 0.282})
Step:    6400, Reward: [-1585.279 -1585.279 -1585.279] [250.900], Avg: [-658.133 -658.133 -658.133] (0.2763) <00:02:29> ({r_i: None, r_t: [-2867.318 -2867.318 -2867.318], critic_loss: 27704.916015625, actor_loss: 26679.134765625, eps: 0.276})
Step:    6500, Reward: [-1619.689 -1619.689 -1619.689] [299.009], Avg: [-672.702 -672.702 -672.702] (0.2708) <00:02:31> ({r_i: None, r_t: [-3132.975 -3132.975 -3132.975], critic_loss: 31135.724609375, actor_loss: 30292.599609375, eps: 0.271})
Step:    6600, Reward: [-1653.054 -1653.054 -1653.054] [194.858], Avg: [-687.334 -687.334 -687.334] (0.2654) <00:02:33> ({r_i: None, r_t: [-3302.823 -3302.823 -3302.823], critic_loss: 42854.2890625, actor_loss: 41574.6640625, eps: 0.265})
Step:    6700, Reward: [-1760.058 -1760.058 -1760.058] [200.135], Avg: [-703.110 -703.110 -703.110] (0.2601) <00:02:36> ({r_i: None, r_t: [-3446.456 -3446.456 -3446.456], critic_loss: 37905.1484375, actor_loss: 36932.0859375, eps: 0.26})
Step:    6800, Reward: [-1805.933 -1805.933 -1805.933] [184.087], Avg: [-719.093 -719.093 -719.093] (0.2549) <00:02:38> ({r_i: None, r_t: [-3429.234 -3429.234 -3429.234], critic_loss: 35992.53125, actor_loss: 36456.07421875, eps: 0.255})
Step:    6900, Reward: [-1824.440 -1824.440 -1824.440] [146.168], Avg: [-734.883 -734.883 -734.883] (0.2498) <00:02:40> ({r_i: None, r_t: [-3371.551 -3371.551 -3371.551], critic_loss: 25613.943359375, actor_loss: 25372.0, eps: 0.25})
Step:    7000, Reward: [-1726.061 -1726.061 -1726.061] [235.092], Avg: [-748.844 -748.844 -748.844] (0.2449) <00:02:43> ({r_i: None, r_t: [-3553.633 -3553.633 -3553.633], critic_loss: 17875.560546875, actor_loss: 17487.349609375, eps: 0.245})
Step:    7100, Reward: [-1832.024 -1832.024 -1832.024] [238.131], Avg: [-763.888 -763.888 -763.888] (0.2400) <00:02:45> ({r_i: None, r_t: [-3638.829 -3638.829 -3638.829], critic_loss: 24077.904296875, actor_loss: 23606.21484375, eps: 0.24})
Step:    7200, Reward: [-1752.129 -1752.129 -1752.129] [222.496], Avg: [-777.425 -777.425 -777.425] (0.2352) <00:02:47> ({r_i: None, r_t: [-3548.754 -3548.754 -3548.754], critic_loss: 24286.53515625, actor_loss: 23169.5078125, eps: 0.235})
Step:    7300, Reward: [-1801.294 -1801.294 -1801.294] [235.198], Avg: [-791.261 -791.261 -791.261] (0.2305) <00:02:50> ({r_i: None, r_t: [-3582.233 -3582.233 -3582.233], critic_loss: 24700.59765625, actor_loss: 23297.166015625, eps: 0.231})
Step:    7400, Reward: [-1864.912 -1864.912 -1864.912] [161.404], Avg: [-805.577 -805.577 -805.577] (0.2259) <00:02:52> ({r_i: None, r_t: [-3659.306 -3659.306 -3659.306], critic_loss: 28018.26171875, actor_loss: 26810.5625, eps: 0.226})
Step:    7500, Reward: [-1628.715 -1628.715 -1628.715] [302.189], Avg: [-816.408 -816.408 -816.408] (0.2215) <00:02:54> ({r_i: None, r_t: [-3669.677 -3669.677 -3669.677], critic_loss: 34292.74609375, actor_loss: 34680.36328125, eps: 0.221})
Step:    7600, Reward: [-1642.155 -1642.155 -1642.155] [176.874], Avg: [-827.132 -827.132 -827.132] (0.2170) <00:02:57> ({r_i: None, r_t: [-3466.243 -3466.243 -3466.243], critic_loss: 29852.298828125, actor_loss: 30601.20703125, eps: 0.217})
Step:    7700, Reward: [-1204.314 -1204.314 -1204.314] [226.264], Avg: [-831.967 -831.967 -831.967] (0.2127) <00:02:59> ({r_i: None, r_t: [-2985.913 -2985.913 -2985.913], critic_loss: 41709.0078125, actor_loss: 46198.6640625, eps: 0.213})
Step:    7800, Reward: [-934.706 -934.706 -934.706] [227.818], Avg: [-833.268 -833.268 -833.268] (0.2085) <00:03:01> ({r_i: None, r_t: [-2507.251 -2507.251 -2507.251], critic_loss: 58793.78125, actor_loss: 66674.421875, eps: 0.208})
Step:    7900, Reward: [-684.976 -684.976 -684.976] [123.533], Avg: [-831.414 -831.414 -831.414] (0.2043) <00:03:03> ({r_i: None, r_t: [-1898.359 -1898.359 -1898.359], critic_loss: 71358.53125, actor_loss: 82822.2578125, eps: 0.204})
Step:    8000, Reward: [-643.611 -643.611 -643.611] [183.106], Avg: [-829.096 -829.096 -829.096] (0.2003) <00:03:06> ({r_i: None, r_t: [-1389.134 -1389.134 -1389.134], critic_loss: 89516.4609375, actor_loss: 109343.28125, eps: 0.2})
Step:    8100, Reward: [-546.107 -546.107 -546.107] [184.182], Avg: [-825.644 -825.644 -825.644] (0.1963) <00:03:08> ({r_i: None, r_t: [-1381.940 -1381.940 -1381.940], critic_loss: 102513.828125, actor_loss: 125072.7109375, eps: 0.196})
Step:    8200, Reward: [-666.065 -666.065 -666.065] [174.946], Avg: [-823.722 -823.722 -823.722] (0.1924) <00:03:10> ({r_i: None, r_t: [-1282.219 -1282.219 -1282.219], critic_loss: 86865.6875, actor_loss: 108740.765625, eps: 0.192})
Step:    8300, Reward: [-592.278 -592.278 -592.278] [111.286], Avg: [-820.967 -820.967 -820.967] (0.1886) <00:03:13> ({r_i: None, r_t: [-1224.678 -1224.678 -1224.678], critic_loss: 69267.484375, actor_loss: 88012.4921875, eps: 0.189})
Step:    8400, Reward: [-627.986 -627.986 -627.986] [121.921], Avg: [-818.696 -818.696 -818.696] (0.1848) <00:03:15> ({r_i: None, r_t: [-1188.706 -1188.706 -1188.706], critic_loss: 58917.37109375, actor_loss: 77096.6953125, eps: 0.185})
Step:    8500, Reward: [-688.862 -688.862 -688.862] [167.946], Avg: [-817.186 -817.186 -817.186] (0.1811) <00:03:17> ({r_i: None, r_t: [-1229.754 -1229.754 -1229.754], critic_loss: 42720.390625, actor_loss: 58790.46875, eps: 0.181})
Step:    8600, Reward: [-588.067 -588.067 -588.067] [141.669], Avg: [-814.553 -814.553 -814.553] (0.1775) <00:03:20> ({r_i: None, r_t: [-1256.663 -1256.663 -1256.663], critic_loss: 31055.47265625, actor_loss: 42893.98046875, eps: 0.178})
Step:    8700, Reward: [-573.969 -573.969 -573.969] [131.403], Avg: [-811.819 -811.819 -811.819] (0.1740) <00:03:22> ({r_i: None, r_t: [-1288.887 -1288.887 -1288.887], critic_loss: 20520.623046875, actor_loss: 29598.080078125, eps: 0.174})
Step:    8800, Reward: [-588.407 -588.407 -588.407] [109.411], Avg: [-809.309 -809.309 -809.309] (0.1705) <00:03:24> ({r_i: None, r_t: [-1281.614 -1281.614 -1281.614], critic_loss: 13272.912109375, actor_loss: 19650.841796875, eps: 0.171})
Step:    8900, Reward: [-600.113 -600.113 -600.113] [131.166], Avg: [-806.984 -806.984 -806.984] (0.1671) <00:03:26> ({r_i: None, r_t: [-1076.799 -1076.799 -1076.799], critic_loss: 10127.8916015625, actor_loss: 15662.5556640625, eps: 0.167})
Step:    9000, Reward: [-540.610 -540.610 -540.610] [121.627], Avg: [-804.057 -804.057 -804.057] (0.1638) <00:03:29> ({r_i: None, r_t: [-1198.969 -1198.969 -1198.969], critic_loss: 6369.30517578125, actor_loss: 9562.6005859375, eps: 0.164})
Step:    9100, Reward: [-539.522 -539.522 -539.522] [154.912], Avg: [-801.182 -801.182 -801.182] (0.1605) <00:03:31> ({r_i: None, r_t: [-1106.551 -1106.551 -1106.551], critic_loss: 3876.964111328125, actor_loss: 5921.22021484375, eps: 0.161})
Step:    9200, Reward: [-572.865 -572.865 -572.865] [102.475], Avg: [-798.727 -798.727 -798.727] (0.1574) <00:03:34> ({r_i: None, r_t: [-1164.480 -1164.480 -1164.480], critic_loss: 1758.406005859375, actor_loss: 2500.69091796875, eps: 0.157})
Step:    9300, Reward: [-636.611 -636.611 -636.611] [165.134], Avg: [-797.002 -797.002 -797.002] (0.1542) <00:03:36> ({r_i: None, r_t: [-1177.309 -1177.309 -1177.309], critic_loss: 1225.7900390625, actor_loss: 1457.782958984375, eps: 0.154})
Step:    9400, Reward: [-557.994 -557.994 -557.994] [122.661], Avg: [-794.486 -794.486 -794.486] (0.1512) <00:03:38> ({r_i: None, r_t: [-1257.805 -1257.805 -1257.805], critic_loss: 1537.47802734375, actor_loss: 1436.883056640625, eps: 0.151})
Step:    9500, Reward: [-621.333 -621.333 -621.333] [151.005], Avg: [-792.683 -792.683 -792.683] (0.1481) <00:03:40> ({r_i: None, r_t: [-1153.958 -1153.958 -1153.958], critic_loss: 1902.7139892578125, actor_loss: 1659.1419677734375, eps: 0.148})
Step:    9600, Reward: [-581.309 -581.309 -581.309] [129.258], Avg: [-790.503 -790.503 -790.503] (0.1452) <00:03:43> ({r_i: None, r_t: [-1184.919 -1184.919 -1184.919], critic_loss: 2156.8349609375, actor_loss: 1964.0760498046875, eps: 0.145})
Step:    9700, Reward: [-579.816 -579.816 -579.816] [119.140], Avg: [-788.354 -788.354 -788.354] (0.1423) <00:03:45> ({r_i: None, r_t: [-1182.365 -1182.365 -1182.365], critic_loss: 2109.416015625, actor_loss: 1960.0699462890625, eps: 0.142})
Step:    9800, Reward: [-641.984 -641.984 -641.984] [120.184], Avg: [-786.875 -786.875 -786.875] (0.1395) <00:03:47> ({r_i: None, r_t: [-1156.957 -1156.957 -1156.957], critic_loss: 1782.1180419921875, actor_loss: 1307.0179443359375, eps: 0.139})
Step:    9900, Reward: [-591.938 -591.938 -591.938] [117.775], Avg: [-784.926 -784.926 -784.926] (0.1367) <00:03:50> ({r_i: None, r_t: [-1167.086 -1167.086 -1167.086], critic_loss: 3632.944091796875, actor_loss: 2890.7509765625, eps: 0.137})
Step:   10000, Reward: [-585.915 -585.915 -585.915] [117.431], Avg: [-782.955 -782.955 -782.955] (0.1340) <00:03:52> ({r_i: None, r_t: [-1230.471 -1230.471 -1230.471], critic_loss: 4047.780029296875, actor_loss: 2459.238037109375, eps: 0.134})
Step:   10100, Reward: [-599.058 -599.058 -599.058] [167.936], Avg: [-781.152 -781.152 -781.152] (0.1313) <00:03:54> ({r_i: None, r_t: [-1158.941 -1158.941 -1158.941], critic_loss: 4884.90185546875, actor_loss: 3218.3798828125, eps: 0.131})
Step:   10200, Reward: [-578.983 -578.983 -578.983] [118.203], Avg: [-779.190 -779.190 -779.190] (0.1287) <00:03:57> ({r_i: None, r_t: [-1169.426 -1169.426 -1169.426], critic_loss: 3880.699951171875, actor_loss: 2281.587890625, eps: 0.129})
Step:   10300, Reward: [-610.924 -610.924 -610.924] [136.920], Avg: [-777.572 -777.572 -777.572] (0.1261) <00:03:59> ({r_i: None, r_t: [-1113.485 -1113.485 -1113.485], critic_loss: 2684.04296875, actor_loss: 2416.7939453125, eps: 0.126})
Step:   10400, Reward: [-583.705 -583.705 -583.705] [146.197], Avg: [-775.725 -775.725 -775.725] (0.1236) <00:04:01> ({r_i: None, r_t: [-1308.116 -1308.116 -1308.116], critic_loss: 2415.885986328125, actor_loss: 2150.052978515625, eps: 0.124})
Step:   10500, Reward: [-608.549 -608.549 -608.549] [143.259], Avg: [-774.148 -774.148 -774.148] (0.1212) <00:04:04> ({r_i: None, r_t: [-1154.550 -1154.550 -1154.550], critic_loss: 2624.31591796875, actor_loss: 2528.51806640625, eps: 0.121})
Step:   10600, Reward: [-549.738 -549.738 -549.738] [143.555], Avg: [-772.051 -772.051 -772.051] (0.1188) <00:04:06> ({r_i: None, r_t: [-1279.327 -1279.327 -1279.327], critic_loss: 1575.260009765625, actor_loss: 1462.4730224609375, eps: 0.119})
Step:   10700, Reward: [-602.830 -602.830 -602.830] [84.162], Avg: [-770.484 -770.484 -770.484] (0.1164) <00:04:08> ({r_i: None, r_t: [-1190.354 -1190.354 -1190.354], critic_loss: 2318.743896484375, actor_loss: 1898.31494140625, eps: 0.116})
Step:   10800, Reward: [-598.222 -598.222 -598.222] [126.733], Avg: [-768.904 -768.904 -768.904] (0.1141) <00:04:10> ({r_i: None, r_t: [-1260.894 -1260.894 -1260.894], critic_loss: 2329.248046875, actor_loss: 1684.156982421875, eps: 0.114})
Step:   10900, Reward: [-585.921 -585.921 -585.921] [120.413], Avg: [-767.240 -767.240 -767.240] (0.1118) <00:04:13> ({r_i: None, r_t: [-1268.801 -1268.801 -1268.801], critic_loss: 2287.97607421875, actor_loss: 1463.9539794921875, eps: 0.112})
Step:   11000, Reward: [-555.595 -555.595 -555.595] [112.097], Avg: [-765.333 -765.333 -765.333] (0.1096) <00:04:15> ({r_i: None, r_t: [-1193.266 -1193.266 -1193.266], critic_loss: 2969.347900390625, actor_loss: 2090.791015625, eps: 0.11})
Step:   11100, Reward: [-630.021 -630.021 -630.021] [103.054], Avg: [-764.125 -764.125 -764.125] (0.1074) <00:04:17> ({r_i: None, r_t: [-1228.143 -1228.143 -1228.143], critic_loss: 1148.4630126953125, actor_loss: 1055.0860595703125, eps: 0.107})
Step:   11200, Reward: [-602.356 -602.356 -602.356] [160.966], Avg: [-762.694 -762.694 -762.694] (0.1053) <00:04:20> ({r_i: None, r_t: [-1220.871 -1220.871 -1220.871], critic_loss: 1717.241943359375, actor_loss: 1651.51904296875, eps: 0.105})
Step:   11300, Reward: [-568.510 -568.510 -568.510] [98.001], Avg: [-760.990 -760.990 -760.990] (0.1032) <00:04:22> ({r_i: None, r_t: [-1215.899 -1215.899 -1215.899], critic_loss: 1822.4219970703125, actor_loss: 2089.053955078125, eps: 0.103})
Step:   11400, Reward: [-609.685 -609.685 -609.685] [116.012], Avg: [-759.675 -759.675 -759.675] (0.1011) <00:04:24> ({r_i: None, r_t: [-1207.341 -1207.341 -1207.341], critic_loss: 2174.81201171875, actor_loss: 2593.31298828125, eps: 0.101})
Step:   11500, Reward: [-584.349 -584.349 -584.349] [174.719], Avg: [-758.163 -758.163 -758.163] (0.0991) <00:04:27> ({r_i: None, r_t: [-1213.798 -1213.798 -1213.798], critic_loss: 2186.8740234375, actor_loss: 1975.032958984375, eps: 0.099})
Step:   11600, Reward: [-645.269 -645.269 -645.269] [198.152], Avg: [-757.198 -757.198 -757.198] (0.0971) <00:04:29> ({r_i: None, r_t: [-1229.445 -1229.445 -1229.445], critic_loss: 2227.968017578125, actor_loss: 1960.5450439453125, eps: 0.097})
Step:   11700, Reward: [-597.692 -597.692 -597.692] [128.074], Avg: [-755.847 -755.847 -755.847] (0.0952) <00:04:33> ({r_i: None, r_t: [-1170.976 -1170.976 -1170.976], critic_loss: 2006.4610595703125, actor_loss: 1210.85498046875, eps: 0.095})
Step:   11800, Reward: [-599.659 -599.659 -599.659] [85.419], Avg: [-754.534 -754.534 -754.534] (0.0933) <00:04:35> ({r_i: None, r_t: [-1212.951 -1212.951 -1212.951], critic_loss: 1810.43505859375, actor_loss: 1351.758056640625, eps: 0.093})
Step:   11900, Reward: [-577.255 -577.255 -577.255] [110.869], Avg: [-753.057 -753.057 -753.057] (0.0914) <00:04:37> ({r_i: None, r_t: [-1179.306 -1179.306 -1179.306], critic_loss: 1451.343994140625, actor_loss: 1495.43505859375, eps: 0.091})
Step:   12000, Reward: [-587.907 -587.907 -587.907] [124.256], Avg: [-751.692 -751.692 -751.692] (0.0896) <00:04:40> ({r_i: None, r_t: [-1107.851 -1107.851 -1107.851], critic_loss: 992.9509887695312, actor_loss: 868.9249877929688, eps: 0.09})
Step:   12100, Reward: [-502.258 -502.258 -502.258] [115.894], Avg: [-749.647 -749.647 -749.647] (0.0878) <00:04:42> ({r_i: None, r_t: [-1089.982 -1089.982 -1089.982], critic_loss: 1270.8509521484375, actor_loss: 1437.5240478515625, eps: 0.088})
Step:   12200, Reward: [-544.425 -544.425 -544.425] [97.639], Avg: [-747.979 -747.979 -747.979] (0.0861) <00:04:45> ({r_i: None, r_t: [-1198.373 -1198.373 -1198.373], critic_loss: 1425.8360595703125, actor_loss: 1695.7220458984375, eps: 0.086})
Step:   12300, Reward: [-593.421 -593.421 -593.421] [122.847], Avg: [-746.732 -746.732 -746.732] (0.0844) <00:04:47> ({r_i: None, r_t: [-1145.787 -1145.787 -1145.787], critic_loss: 1352.9119873046875, actor_loss: 1360.3599853515625, eps: 0.084})
Step:   12400, Reward: [-568.453 -568.453 -568.453] [151.223], Avg: [-745.306 -745.306 -745.306] (0.0827) <00:04:49> ({r_i: None, r_t: [-1173.918 -1173.918 -1173.918], critic_loss: 1626.511962890625, actor_loss: 2351.93994140625, eps: 0.083})
Step:   12500, Reward: [-598.329 -598.329 -598.329] [104.229], Avg: [-744.140 -744.140 -744.140] (0.0811) <00:04:52> ({r_i: None, r_t: [-1111.143 -1111.143 -1111.143], critic_loss: 1521.6910400390625, actor_loss: 1799.4639892578125, eps: 0.081})
Step:   12600, Reward: [-537.385 -537.385 -537.385] [89.720], Avg: [-742.512 -742.512 -742.512] (0.0794) <00:04:54> ({r_i: None, r_t: [-1100.171 -1100.171 -1100.171], critic_loss: 2367.52392578125, actor_loss: 2727.35009765625, eps: 0.079})
Step:   12700, Reward: [-539.833 -539.833 -539.833] [97.174], Avg: [-740.928 -740.928 -740.928] (0.0779) <00:04:56> ({r_i: None, r_t: [-1130.382 -1130.382 -1130.382], critic_loss: 1859.5400390625, actor_loss: 2511.52001953125, eps: 0.078})
Step:   12800, Reward: [-611.748 -611.748 -611.748] [107.793], Avg: [-739.927 -739.927 -739.927] (0.0763) <00:04:58> ({r_i: None, r_t: [-1142.420 -1142.420 -1142.420], critic_loss: 2714.299072265625, actor_loss: 2709.843017578125, eps: 0.076})
Step:   12900, Reward: [-648.343 -648.343 -648.343] [133.421], Avg: [-739.222 -739.222 -739.222] (0.0748) <00:05:01> ({r_i: None, r_t: [-1300.362 -1300.362 -1300.362], critic_loss: 1885.8050537109375, actor_loss: 1858.677978515625, eps: 0.075})
Step:   13000, Reward: [-615.008 -615.008 -615.008] [122.539], Avg: [-738.274 -738.274 -738.274] (0.0733) <00:05:03> ({r_i: None, r_t: [-1148.426 -1148.426 -1148.426], critic_loss: 4026.089111328125, actor_loss: 4220.76611328125, eps: 0.073})
Step:   13100, Reward: [-561.970 -561.970 -561.970] [114.336], Avg: [-736.939 -736.939 -736.939] (0.0718) <00:05:06> ({r_i: None, r_t: [-1194.347 -1194.347 -1194.347], critic_loss: 2142.0419921875, actor_loss: 2553.18896484375, eps: 0.072})
Step:   13200, Reward: [-597.569 -597.569 -597.569] [194.401], Avg: [-735.891 -735.891 -735.891] (0.0704) <00:05:08> ({r_i: None, r_t: [-1184.422 -1184.422 -1184.422], critic_loss: 1655.583984375, actor_loss: 2187.030029296875, eps: 0.07})
Step:   13300, Reward: [-572.093 -572.093 -572.093] [114.949], Avg: [-734.668 -734.668 -734.668] (0.0690) <00:05:10> ({r_i: None, r_t: [-1184.149 -1184.149 -1184.149], critic_loss: 1732.85205078125, actor_loss: 2755.821044921875, eps: 0.069})
Step:   13400, Reward: [-605.401 -605.401 -605.401] [86.930], Avg: [-733.711 -733.711 -733.711] (0.0676) <00:05:13> ({r_i: None, r_t: [-1221.357 -1221.357 -1221.357], critic_loss: 1062.22802734375, actor_loss: 2277.114990234375, eps: 0.068})
Step:   13500, Reward: [-543.528 -543.528 -543.528] [120.317], Avg: [-732.312 -732.312 -732.312] (0.0663) <00:05:15> ({r_i: None, r_t: [-1174.851 -1174.851 -1174.851], critic_loss: 1419.2559814453125, actor_loss: 1450.4759521484375, eps: 0.066})
Step:   13600, Reward: [-560.936 -560.936 -560.936] [100.750], Avg: [-731.061 -731.061 -731.061] (0.0650) <00:05:17> ({r_i: None, r_t: [-1170.675 -1170.675 -1170.675], critic_loss: 2156.196044921875, actor_loss: 1610.6639404296875, eps: 0.065})
Step:   13700, Reward: [-575.454 -575.454 -575.454] [117.639], Avg: [-729.934 -729.934 -729.934] (0.0637) <00:05:19> ({r_i: None, r_t: [-1224.736 -1224.736 -1224.736], critic_loss: 1806.47998046875, actor_loss: 2167.85400390625, eps: 0.064})
Step:   13800, Reward: [-541.203 -541.203 -541.203] [122.082], Avg: [-728.576 -728.576 -728.576] (0.0624) <00:05:22> ({r_i: None, r_t: [-1108.573 -1108.573 -1108.573], critic_loss: 3247.8720703125, actor_loss: 3246.110107421875, eps: 0.062})
Step:   13900, Reward: [-560.176 -560.176 -560.176] [99.284], Avg: [-727.373 -727.373 -727.373] (0.0612) <00:05:24> ({r_i: None, r_t: [-1213.215 -1213.215 -1213.215], critic_loss: 3715.361083984375, actor_loss: 5332.076171875, eps: 0.061})
Step:   14000, Reward: [-584.167 -584.167 -584.167] [82.747], Avg: [-726.358 -726.358 -726.358] (0.0600) <00:05:26> ({r_i: None, r_t: [-1240.153 -1240.153 -1240.153], critic_loss: 4268.76220703125, actor_loss: 8467.912109375, eps: 0.06})
Step:   14100, Reward: [-567.315 -567.315 -567.315] [117.988], Avg: [-725.238 -725.238 -725.238] (0.0588) <00:05:29> ({r_i: None, r_t: [-1113.307 -1113.307 -1113.307], critic_loss: 3609.48388671875, actor_loss: 9602.7978515625, eps: 0.059})
Step:   14200, Reward: [-583.207 -583.207 -583.207] [115.172], Avg: [-724.244 -724.244 -724.244] (0.0576) <00:05:31> ({r_i: None, r_t: [-1179.971 -1179.971 -1179.971], critic_loss: 3737.6689453125, actor_loss: 11518.8828125, eps: 0.058})
Step:   14300, Reward: [-596.049 -596.049 -596.049] [115.033], Avg: [-723.354 -723.354 -723.354] (0.0565) <00:05:34> ({r_i: None, r_t: [-1191.773 -1191.773 -1191.773], critic_loss: 4232.13818359375, actor_loss: 13724.1318359375, eps: 0.056})
Step:   14400, Reward: [-573.916 -573.916 -573.916] [109.763], Avg: [-722.323 -722.323 -722.323] (0.0553) <00:05:36> ({r_i: None, r_t: [-1105.978 -1105.978 -1105.978], critic_loss: 1934.4610595703125, actor_loss: 10263.9619140625, eps: 0.055})
Step:   14500, Reward: [-638.338 -638.338 -638.338] [129.471], Avg: [-721.748 -721.748 -721.748] (0.0542) <00:05:38> ({r_i: None, r_t: [-1169.637 -1169.637 -1169.637], critic_loss: 1785.1829833984375, actor_loss: 6760.5517578125, eps: 0.054})
Step:   14600, Reward: [-563.735 -563.735 -563.735] [100.307], Avg: [-720.673 -720.673 -720.673] (0.0531) <00:05:40> ({r_i: None, r_t: [-1174.546 -1174.546 -1174.546], critic_loss: 1292.2509765625, actor_loss: 4232.81982421875, eps: 0.053})
Step:   14700, Reward: [-609.395 -609.395 -609.395] [102.173], Avg: [-719.921 -719.921 -719.921] (0.0521) <00:05:43> ({r_i: None, r_t: [-1143.979 -1143.979 -1143.979], critic_loss: 2009.1009521484375, actor_loss: 2932.552001953125, eps: 0.052})
Step:   14800, Reward: [-540.963 -540.963 -540.963] [127.226], Avg: [-718.720 -718.720 -718.720] (0.0511) <00:05:45> ({r_i: None, r_t: [-1141.053 -1141.053 -1141.053], critic_loss: 2282.131103515625, actor_loss: 1456.35205078125, eps: 0.051})
Step:   14900, Reward: [-574.331 -574.331 -574.331] [117.757], Avg: [-717.758 -717.758 -717.758] (0.0500) <00:05:47> ({r_i: None, r_t: [-1216.742 -1216.742 -1216.742], critic_loss: 2758.60498046875, actor_loss: 2041.6820068359375, eps: 0.05})
Step:   15000, Reward: [-584.301 -584.301 -584.301] [116.594], Avg: [-716.874 -716.874 -716.874] (0.0490) <00:05:50> ({r_i: None, r_t: [-1185.459 -1185.459 -1185.459], critic_loss: 2952.718017578125, actor_loss: 3149.843994140625, eps: 0.049})
Step:   15100, Reward: [-666.317 -666.317 -666.317] [141.345], Avg: [-716.541 -716.541 -716.541] (0.0481) <00:05:52> ({r_i: None, r_t: [-1192.447 -1192.447 -1192.447], critic_loss: 2233.18408203125, actor_loss: 3557.659912109375, eps: 0.048})
Step:   15200, Reward: [-626.989 -626.989 -626.989] [115.394], Avg: [-715.956 -715.956 -715.956] (0.0471) <00:05:54> ({r_i: None, r_t: [-1179.328 -1179.328 -1179.328], critic_loss: 1198.0030517578125, actor_loss: 2535.31591796875, eps: 0.047})
Step:   15300, Reward: [-585.036 -585.036 -585.036] [129.873], Avg: [-715.106 -715.106 -715.106] (0.0462) <00:05:57> ({r_i: None, r_t: [-1120.815 -1120.815 -1120.815], critic_loss: 1033.8070068359375, actor_loss: 1643.85498046875, eps: 0.046})
Step:   15400, Reward: [-586.174 -586.174 -586.174] [113.473], Avg: [-714.274 -714.274 -714.274] (0.0453) <00:05:59> ({r_i: None, r_t: [-1156.800 -1156.800 -1156.800], critic_loss: 975.7639770507812, actor_loss: 2110.2060546875, eps: 0.045})
Step:   15500, Reward: [-625.786 -625.786 -625.786] [140.148], Avg: [-713.707 -713.707 -713.707] (0.0444) <00:06:01> ({r_i: None, r_t: [-1222.753 -1222.753 -1222.753], critic_loss: 1035.041015625, actor_loss: 2257.15087890625, eps: 0.044})
Step:   15600, Reward: [-661.681 -661.681 -661.681] [138.047], Avg: [-713.375 -713.375 -713.375] (0.0435) <00:06:04> ({r_i: None, r_t: [-1131.893 -1131.893 -1131.893], critic_loss: 1410.862060546875, actor_loss: 2185.12109375, eps: 0.043})
Step:   15700, Reward: [-609.642 -609.642 -609.642] [126.428], Avg: [-712.719 -712.719 -712.719] (0.0426) <00:06:06> ({r_i: None, r_t: [-1135.988 -1135.988 -1135.988], critic_loss: 886.7069702148438, actor_loss: 1088.2640380859375, eps: 0.043})
Step:   15800, Reward: [-654.907 -654.907 -654.907] [150.350], Avg: [-712.355 -712.355 -712.355] (0.0418) <00:06:08> ({r_i: None, r_t: [-1162.174 -1162.174 -1162.174], critic_loss: 1318.2239990234375, actor_loss: 1827.498046875, eps: 0.042})
Step:   15900, Reward: [-649.243 -649.243 -649.243] [118.536], Avg: [-711.961 -711.961 -711.961] (0.0409) <00:06:11> ({r_i: None, r_t: [-1170.645 -1170.645 -1170.645], critic_loss: 988.801025390625, actor_loss: 1927.98095703125, eps: 0.041})
Step:   16000, Reward: [-546.179 -546.179 -546.179] [124.106], Avg: [-710.931 -710.931 -710.931] (0.0401) <00:06:13> ({r_i: None, r_t: [-1218.001 -1218.001 -1218.001], critic_loss: 985.4290161132812, actor_loss: 1925.759033203125, eps: 0.04})
Step:   16100, Reward: [-554.539 -554.539 -554.539] [109.115], Avg: [-709.966 -709.966 -709.966] (0.0393) <00:06:15> ({r_i: None, r_t: [-1141.961 -1141.961 -1141.961], critic_loss: 986.27099609375, actor_loss: 3394.319091796875, eps: 0.039})
Step:   16200, Reward: [-555.760 -555.760 -555.760] [96.894], Avg: [-709.020 -709.020 -709.020] (0.0385) <00:06:18> ({r_i: None, r_t: [-1107.031 -1107.031 -1107.031], critic_loss: 606.7130126953125, actor_loss: 2184.193115234375, eps: 0.039})
Step:   16300, Reward: [-566.163 -566.163 -566.163] [111.365], Avg: [-708.149 -708.149 -708.149] (0.0378) <00:06:20> ({r_i: None, r_t: [-1099.400 -1099.400 -1099.400], critic_loss: 858.2869873046875, actor_loss: 1357.251953125, eps: 0.038})
Step:   16400, Reward: [-552.050 -552.050 -552.050] [85.502], Avg: [-707.203 -707.203 -707.203] (0.0370) <00:06:22> ({r_i: None, r_t: [-1068.362 -1068.362 -1068.362], critic_loss: 1264.35498046875, actor_loss: 1896.114990234375, eps: 0.037})
Step:   16500, Reward: [-603.140 -603.140 -603.140] [137.433], Avg: [-706.576 -706.576 -706.576] (0.0363) <00:06:24> ({r_i: None, r_t: [-1069.951 -1069.951 -1069.951], critic_loss: 718.6119995117188, actor_loss: 628.2459716796875, eps: 0.036})
Step:   16600, Reward: [-513.608 -513.608 -513.608] [51.268], Avg: [-705.420 -705.420 -705.420] (0.0356) <00:06:27> ({r_i: None, r_t: [-1155.491 -1155.491 -1155.491], critic_loss: 640.0659790039062, actor_loss: 647.6959838867188, eps: 0.036})
Step:   16700, Reward: [-522.113 -522.113 -522.113] [98.263], Avg: [-704.329 -704.329 -704.329] (0.0348) <00:06:29> ({r_i: None, r_t: [-1081.324 -1081.324 -1081.324], critic_loss: 1138.31103515625, actor_loss: 1280.9029541015625, eps: 0.035})
Step:   16800, Reward: [-596.785 -596.785 -596.785] [164.965], Avg: [-703.693 -703.693 -703.693] (0.0342) <00:06:31> ({r_i: None, r_t: [-1088.231 -1088.231 -1088.231], critic_loss: 590.260986328125, actor_loss: 756.8699951171875, eps: 0.034})
Step:   16900, Reward: [-559.697 -559.697 -559.697] [104.484], Avg: [-702.846 -702.846 -702.846] (0.0335) <00:06:34> ({r_i: None, r_t: [-1191.610 -1191.610 -1191.610], critic_loss: 846.2219848632812, actor_loss: 757.1500244140625, eps: 0.033})
Step:   17000, Reward: [-536.192 -536.192 -536.192] [126.365], Avg: [-701.871 -701.871 -701.871] (0.0328) <00:06:36> ({r_i: None, r_t: [-1047.865 -1047.865 -1047.865], critic_loss: 990.0440063476562, actor_loss: 1325.68505859375, eps: 0.033})
Step:   17100, Reward: [-537.653 -537.653 -537.653] [132.316], Avg: [-700.916 -700.916 -700.916] (0.0322) <00:06:39> ({r_i: None, r_t: [-1080.257 -1080.257 -1080.257], critic_loss: 633.5449829101562, actor_loss: 772.8939819335938, eps: 0.032})
Step:   17200, Reward: [-534.419 -534.419 -534.419] [111.494], Avg: [-699.954 -699.954 -699.954] (0.0315) <00:06:41> ({r_i: None, r_t: [-1107.726 -1107.726 -1107.726], critic_loss: 817.5460205078125, actor_loss: 773.6090087890625, eps: 0.032})
Step:   17300, Reward: [-517.564 -517.564 -517.564] [112.779], Avg: [-698.906 -698.906 -698.906] (0.0309) <00:06:43> ({r_i: None, r_t: [-1058.133 -1058.133 -1058.133], critic_loss: 665.5360107421875, actor_loss: 812.0349731445312, eps: 0.031})
Step:   17400, Reward: [-530.048 -530.048 -530.048] [128.981], Avg: [-697.941 -697.941 -697.941] (0.0303) <00:06:45> ({r_i: None, r_t: [-1092.886 -1092.886 -1092.886], critic_loss: 1325.7120361328125, actor_loss: 1396.6400146484375, eps: 0.03})
Step:   17500, Reward: [-506.697 -506.697 -506.697] [96.851], Avg: [-696.854 -696.854 -696.854] (0.0297) <00:06:48> ({r_i: None, r_t: [-1088.543 -1088.543 -1088.543], critic_loss: 975.5670166015625, actor_loss: 1358.990966796875, eps: 0.03})
Step:   17600, Reward: [-503.052 -503.052 -503.052] [99.628], Avg: [-695.759 -695.759 -695.759] (0.0291) <00:06:50> ({r_i: None, r_t: [-975.530 -975.530 -975.530], critic_loss: 1736.2509765625, actor_loss: 2491.5810546875, eps: 0.029})
Step:   17700, Reward: [-478.332 -478.332 -478.332] [88.553], Avg: [-694.538 -694.538 -694.538] (0.0285) <00:06:52> ({r_i: None, r_t: [-958.226 -958.226 -958.226], critic_loss: 1186.969970703125, actor_loss: 1436.052001953125, eps: 0.029})
Step:   17800, Reward: [-452.629 -452.629 -452.629] [105.061], Avg: [-693.186 -693.186 -693.186] (0.0279) <00:06:55> ({r_i: None, r_t: [-956.450 -956.450 -956.450], critic_loss: 697.958984375, actor_loss: 1003.8400268554688, eps: 0.028})
Step:   17900, Reward: [-500.918 -500.918 -500.918] [101.363], Avg: [-692.118 -692.118 -692.118] (0.0274) <00:06:57> ({r_i: None, r_t: [-962.239 -962.239 -962.239], critic_loss: 683.2249755859375, actor_loss: 859.2119750976562, eps: 0.027})
Step:   18000, Reward: [-451.171 -451.171 -451.171] [74.464], Avg: [-690.787 -690.787 -690.787] (0.0268) <00:06:59> ({r_i: None, r_t: [-973.983 -973.983 -973.983], critic_loss: 414.0950012207031, actor_loss: 589.6300048828125, eps: 0.027})
Step:   18100, Reward: [-444.106 -444.106 -444.106] [98.988], Avg: [-689.432 -689.432 -689.432] (0.0263) <00:07:02> ({r_i: None, r_t: [-912.841 -912.841 -912.841], critic_loss: 1326.3409423828125, actor_loss: 1685.7449951171875, eps: 0.026})
Step:   18200, Reward: [-492.982 -492.982 -492.982] [84.884], Avg: [-688.358 -688.358 -688.358] (0.0258) <00:07:04> ({r_i: None, r_t: [-969.042 -969.042 -969.042], critic_loss: 1675.3800048828125, actor_loss: 1942.470947265625, eps: 0.026})
Step:   18300, Reward: [-450.698 -450.698 -450.698] [63.496], Avg: [-687.067 -687.067 -687.067] (0.0253) <00:07:06> ({r_i: None, r_t: [-961.045 -961.045 -961.045], critic_loss: 937.0390014648438, actor_loss: 1425.5579833984375, eps: 0.025})
Step:   18400, Reward: [-492.124 -492.124 -492.124] [80.528], Avg: [-686.013 -686.013 -686.013] (0.0248) <00:07:09> ({r_i: None, r_t: [-935.432 -935.432 -935.432], critic_loss: 597.2069702148438, actor_loss: 823.291015625, eps: 0.025})
Step:   18500, Reward: [-461.692 -461.692 -461.692] [77.563], Avg: [-684.807 -684.807 -684.807] (0.0243) <00:07:11> ({r_i: None, r_t: [-927.125 -927.125 -927.125], critic_loss: 540.4920043945312, actor_loss: 920.4669799804688, eps: 0.024})
Step:   18600, Reward: [-529.962 -529.962 -529.962] [139.029], Avg: [-683.979 -683.979 -683.979] (0.0238) <00:07:13> ({r_i: None, r_t: [-949.717 -949.717 -949.717], critic_loss: 421.7760009765625, actor_loss: 879.22998046875, eps: 0.024})
Step:   18700, Reward: [-465.611 -465.611 -465.611] [95.639], Avg: [-682.817 -682.817 -682.817] (0.0233) <00:07:15> ({r_i: None, r_t: [-939.857 -939.857 -939.857], critic_loss: 518.2020263671875, actor_loss: 850.1240234375, eps: 0.023})
Step:   18800, Reward: [-457.398 -457.398 -457.398] [93.018], Avg: [-681.624 -681.624 -681.624] (0.0228) <00:07:18> ({r_i: None, r_t: [-987.270 -987.270 -987.270], critic_loss: 977.1599731445312, actor_loss: 782.5430297851562, eps: 0.023})
Step:   18900, Reward: [-501.258 -501.258 -501.258] [156.193], Avg: [-680.675 -680.675 -680.675] (0.0224) <00:07:20> ({r_i: None, r_t: [-937.593 -937.593 -937.593], critic_loss: 743.9630126953125, actor_loss: 897.6959838867188, eps: 0.022})
Step:   19000, Reward: [-469.274 -469.274 -469.274] [88.757], Avg: [-679.568 -679.568 -679.568] (0.0219) <00:07:22> ({r_i: None, r_t: [-893.511 -893.511 -893.511], critic_loss: 858.5640258789062, actor_loss: 979.9429931640625, eps: 0.022})
Step:   19100, Reward: [-460.836 -460.836 -460.836] [90.776], Avg: [-678.429 -678.429 -678.429] (0.0215) <00:07:25> ({r_i: None, r_t: [-969.174 -969.174 -969.174], critic_loss: 736.0599975585938, actor_loss: 781.7559814453125, eps: 0.022})
Step:   19200, Reward: [-496.300 -496.300 -496.300] [131.002], Avg: [-677.485 -677.485 -677.485] (0.0211) <00:07:27> ({r_i: None, r_t: [-877.246 -877.246 -877.246], critic_loss: 636.7880249023438, actor_loss: 782.6530151367188, eps: 0.021})
Step:   19300, Reward: [-458.122 -458.122 -458.122] [86.319], Avg: [-676.355 -676.355 -676.355] (0.0207) <00:07:29> ({r_i: None, r_t: [-959.909 -959.909 -959.909], critic_loss: 581.0789794921875, actor_loss: 792.1079711914062, eps: 0.021})
Step:   19400, Reward: [-498.110 -498.110 -498.110] [98.483], Avg: [-675.441 -675.441 -675.441] (0.0203) <00:07:32> ({r_i: None, r_t: [-966.428 -966.428 -966.428], critic_loss: 1234.4110107421875, actor_loss: 1133.4019775390625, eps: 0.02})
Step:   19500, Reward: [-444.275 -444.275 -444.275] [106.674], Avg: [-674.261 -674.261 -674.261] (0.0198) <00:07:34> ({r_i: None, r_t: [-965.800 -965.800 -965.800], critic_loss: 1203.656005859375, actor_loss: 1066.72900390625, eps: 0.02})
Step:   19600, Reward: [-427.713 -427.713 -427.713] [88.146], Avg: [-673.010 -673.010 -673.010] (0.0195) <00:07:36> ({r_i: None, r_t: [-906.334 -906.334 -906.334], critic_loss: 863.1229858398438, actor_loss: 1011.2949829101562, eps: 0.019})
Step:   19700, Reward: [-410.260 -410.260 -410.260] [91.626], Avg: [-671.683 -671.683 -671.683] (0.0191) <00:07:39> ({r_i: None, r_t: [-991.549 -991.549 -991.549], critic_loss: 988.4130249023438, actor_loss: 1040.259033203125, eps: 0.019})
Step:   19800, Reward: [-447.700 -447.700 -447.700] [99.391], Avg: [-670.557 -670.557 -670.557] (0.0187) <00:07:41> ({r_i: None, r_t: [-920.707 -920.707 -920.707], critic_loss: 902.864990234375, actor_loss: 803.5590209960938, eps: 0.019})
Step:   19900, Reward: [-458.477 -458.477 -458.477] [88.711], Avg: [-669.497 -669.497 -669.497] (0.0183) <00:07:44> ({r_i: None, r_t: [-890.616 -890.616 -890.616], critic_loss: 649.4569702148438, actor_loss: 636.4719848632812, eps: 0.018})
Step:   20000, Reward: [-467.697 -467.697 -467.697] [99.210], Avg: [-668.493 -668.493 -668.493] (0.0180) <00:07:46> ({r_i: None, r_t: [-832.813 -832.813 -832.813], critic_loss: 1027.06298828125, actor_loss: 929.8900146484375, eps: 0.018})
Step:   20100, Reward: [-409.421 -409.421 -409.421] [88.001], Avg: [-667.210 -667.210 -667.210] (0.0176) <00:07:48> ({r_i: None, r_t: [-911.710 -911.710 -911.710], critic_loss: 935.1929931640625, actor_loss: 934.281982421875, eps: 0.018})
Step:   20200, Reward: [-424.324 -424.324 -424.324] [81.531], Avg: [-666.014 -666.014 -666.014] (0.0172) <00:07:51> ({r_i: None, r_t: [-856.478 -856.478 -856.478], critic_loss: 1017.0540161132812, actor_loss: 907.0150146484375, eps: 0.017})
Step:   20300, Reward: [-466.109 -466.109 -466.109] [78.175], Avg: [-665.034 -665.034 -665.034] (0.0169) <00:07:53> ({r_i: None, r_t: [-903.900 -903.900 -903.900], critic_loss: 1166.0389404296875, actor_loss: 1089.2840576171875, eps: 0.017})
Step:   20400, Reward: [-474.244 -474.244 -474.244] [79.679], Avg: [-664.103 -664.103 -664.103] (0.0166) <00:07:55> ({r_i: None, r_t: [-831.133 -831.133 -831.133], critic_loss: 1666.968017578125, actor_loss: 1562.571044921875, eps: 0.017})
Step:   20500, Reward: [-405.029 -405.029 -405.029] [59.367], Avg: [-662.845 -662.845 -662.845] (0.0162) <00:07:58> ({r_i: None, r_t: [-895.433 -895.433 -895.433], critic_loss: 861.7540283203125, actor_loss: 794.9459838867188, eps: 0.016})
Step:   20600, Reward: [-441.795 -441.795 -441.795] [78.438], Avg: [-661.778 -661.778 -661.778] (0.0159) <00:08:00> ({r_i: None, r_t: [-923.630 -923.630 -923.630], critic_loss: 1727.0860595703125, actor_loss: 1616.760009765625, eps: 0.016})
Step:   20700, Reward: [-393.930 -393.930 -393.930] [84.838], Avg: [-660.490 -660.490 -660.490] (0.0156) <00:08:02> ({r_i: None, r_t: [-858.226 -858.226 -858.226], critic_loss: 1061.1400146484375, actor_loss: 994.5540161132812, eps: 0.016})
Step:   20800, Reward: [-434.532 -434.532 -434.532] [111.859], Avg: [-659.409 -659.409 -659.409] (0.0153) <00:08:05> ({r_i: None, r_t: [-898.254 -898.254 -898.254], critic_loss: 953.3460083007812, actor_loss: 981.625, eps: 0.015})
Step:   20900, Reward: [-440.012 -440.012 -440.012] [107.030], Avg: [-658.364 -658.364 -658.364] (0.0150) <00:08:07> ({r_i: None, r_t: [-843.285 -843.285 -843.285], critic_loss: 949.968017578125, actor_loss: 938.6170043945312, eps: 0.015})
Step:   21000, Reward: [-438.591 -438.591 -438.591] [122.047], Avg: [-657.322 -657.322 -657.322] (0.0147) <00:08:09> ({r_i: None, r_t: [-891.866 -891.866 -891.866], critic_loss: 1507.4019775390625, actor_loss: 1585.154052734375, eps: 0.015})
Step:   21100, Reward: [-436.009 -436.009 -436.009] [73.467], Avg: [-656.278 -656.278 -656.278] (0.0144) <00:08:12> ({r_i: None, r_t: [-855.614 -855.614 -855.614], critic_loss: 1026.237060546875, actor_loss: 980.6790161132812, eps: 0.014})
Step:   21200, Reward: [-457.007 -457.007 -457.007] [94.913], Avg: [-655.343 -655.343 -655.343] (0.0141) <00:08:14> ({r_i: None, r_t: [-906.530 -906.530 -906.530], critic_loss: 985.9429931640625, actor_loss: 973.4920043945312, eps: 0.014})
Step:   21300, Reward: [-406.121 -406.121 -406.121] [94.296], Avg: [-654.178 -654.178 -654.178] (0.0138) <00:08:16> ({r_i: None, r_t: [-862.357 -862.357 -862.357], critic_loss: 1240.1240234375, actor_loss: 1166.48095703125, eps: 0.014})
Step:   21400, Reward: [-426.587 -426.587 -426.587] [75.423], Avg: [-653.120 -653.120 -653.120] (0.0135) <00:08:19> ({r_i: None, r_t: [-842.650 -842.650 -842.650], critic_loss: 861.9940185546875, actor_loss: 821.1810302734375, eps: 0.014})
Step:   21500, Reward: [-448.890 -448.890 -448.890] [88.178], Avg: [-652.174 -652.174 -652.174] (0.0133) <00:08:21> ({r_i: None, r_t: [-833.593 -833.593 -833.593], critic_loss: 940.7550048828125, actor_loss: 924.593994140625, eps: 0.013})
Step:   21600, Reward: [-420.475 -420.475 -420.475] [81.228], Avg: [-651.107 -651.107 -651.107] (0.0130) <00:08:23> ({r_i: None, r_t: [-869.203 -869.203 -869.203], critic_loss: 998.6740112304688, actor_loss: 932.385009765625, eps: 0.013})
Step:   21700, Reward: [-418.618 -418.618 -418.618] [72.904], Avg: [-650.040 -650.040 -650.040] (0.0128) <00:08:25> ({r_i: None, r_t: [-896.829 -896.829 -896.829], critic_loss: 970.1539916992188, actor_loss: 912.1229858398438, eps: 0.013})
Step:   21800, Reward: [-409.288 -409.288 -409.288] [98.012], Avg: [-648.941 -648.941 -648.941] (0.0125) <00:08:28> ({r_i: None, r_t: [-846.111 -846.111 -846.111], critic_loss: 909.5339965820312, actor_loss: 993.4840087890625, eps: 0.013})
Step:   21900, Reward: [-474.928 -474.928 -474.928] [90.908], Avg: [-648.150 -648.150 -648.150] (0.0123) <00:08:30> ({r_i: None, r_t: [-861.778 -861.778 -861.778], critic_loss: 832.8679809570312, actor_loss: 865.0070190429688, eps: 0.012})
Step:   22000, Reward: [-432.078 -432.078 -432.078] [106.711], Avg: [-647.172 -647.172 -647.172] (0.0120) <00:08:32> ({r_i: None, r_t: [-811.212 -811.212 -811.212], critic_loss: 1107.1800537109375, actor_loss: 1160.2760009765625, eps: 0.012})
Step:   22100, Reward: [-428.499 -428.499 -428.499] [72.583], Avg: [-646.187 -646.187 -646.187] (0.0118) <00:08:35> ({r_i: None, r_t: [-872.960 -872.960 -872.960], critic_loss: 983.6749877929688, actor_loss: 949.4819946289062, eps: 0.012})
Step:   22200, Reward: [-392.085 -392.085 -392.085] [79.328], Avg: [-645.048 -645.048 -645.048] (0.0115) <00:08:37> ({r_i: None, r_t: [-856.354 -856.354 -856.354], critic_loss: 771.8920288085938, actor_loss: 770.7760009765625, eps: 0.012})
Step:   22300, Reward: [-407.785 -407.785 -407.785] [108.501], Avg: [-643.988 -643.988 -643.988] (0.0113) <00:08:39> ({r_i: None, r_t: [-865.165 -865.165 -865.165], critic_loss: 572.39501953125, actor_loss: 579.9299926757812, eps: 0.011})
Step:   22400, Reward: [-470.054 -470.054 -470.054] [109.732], Avg: [-643.215 -643.215 -643.215] (0.0111) <00:08:41> ({r_i: None, r_t: [-823.195 -823.195 -823.195], critic_loss: 1620.6910400390625, actor_loss: 1611.81201171875, eps: 0.011})
Step:   22500, Reward: [-416.204 -416.204 -416.204] [64.562], Avg: [-642.211 -642.211 -642.211] (0.0109) <00:08:44> ({r_i: None, r_t: [-878.680 -878.680 -878.680], critic_loss: 1115.9990234375, actor_loss: 1122.0179443359375, eps: 0.011})
Step:   22600, Reward: [-420.304 -420.304 -420.304] [93.358], Avg: [-641.233 -641.233 -641.233] (0.0106) <00:08:46> ({r_i: None, r_t: [-855.079 -855.079 -855.079], critic_loss: 1098.449951171875, actor_loss: 1129.3800048828125, eps: 0.011})
Step:   22700, Reward: [-420.451 -420.451 -420.451] [98.569], Avg: [-640.265 -640.265 -640.265] (0.0104) <00:08:48> ({r_i: None, r_t: [-830.135 -830.135 -830.135], critic_loss: 927.0059814453125, actor_loss: 947.9450073242188, eps: 0.01})
Step:   22800, Reward: [-443.681 -443.681 -443.681] [56.965], Avg: [-639.407 -639.407 -639.407] (0.0102) <00:08:51> ({r_i: None, r_t: [-906.898 -906.898 -906.898], critic_loss: 1218.989013671875, actor_loss: 1225.458984375, eps: 0.01})
Step:   22900, Reward: [-418.761 -418.761 -418.761] [85.311], Avg: [-638.447 -638.447 -638.447] (0.0100) <00:08:53> ({r_i: None, r_t: [-882.278 -882.278 -882.278], critic_loss: 1103.8499755859375, actor_loss: 1127.3919677734375, eps: 0.01})
Step:   23000, Reward: [-440.036 -440.036 -440.036] [71.728], Avg: [-637.588 -637.588 -637.588] (0.0098) <00:08:55> ({r_i: None, r_t: [-880.512 -880.512 -880.512], critic_loss: 992.9130249023438, actor_loss: 987.7039794921875, eps: 0.01})
Step:   23100, Reward: [-396.551 -396.551 -396.551] [57.927], Avg: [-636.549 -636.549 -636.549] (0.0096) <00:08:58> ({r_i: None, r_t: [-919.779 -919.779 -919.779], critic_loss: 851.4340209960938, actor_loss: 873.3090209960938, eps: 0.01})
Step:   23200, Reward: [-502.935 -502.935 -502.935] [120.766], Avg: [-635.976 -635.976 -635.976] (0.0094) <00:09:00> ({r_i: None, r_t: [-864.812 -864.812 -864.812], critic_loss: 1156.2509765625, actor_loss: 1182.1409912109375, eps: 0.009})
Step:   23300, Reward: [-456.410 -456.410 -456.410] [80.315], Avg: [-635.209 -635.209 -635.209] (0.0092) <00:09:02> ({r_i: None, r_t: [-830.909 -830.909 -830.909], critic_loss: 913.0460205078125, actor_loss: 926.6380004882812, eps: 0.009})
Step:   23400, Reward: [-451.339 -451.339 -451.339] [126.721], Avg: [-634.426 -634.426 -634.426] (0.0091) <00:09:05> ({r_i: None, r_t: [-860.264 -860.264 -860.264], critic_loss: 936.9959716796875, actor_loss: 950.7349853515625, eps: 0.009})
Step:   23500, Reward: [-460.805 -460.805 -460.805] [94.704], Avg: [-633.690 -633.690 -633.690] (0.0089) <00:09:07> ({r_i: None, r_t: [-880.153 -880.153 -880.153], critic_loss: 1266.1619873046875, actor_loss: 1207.073974609375, eps: 0.009})
Step:   23600, Reward: [-435.005 -435.005 -435.005] [84.582], Avg: [-632.852 -632.852 -632.852] (0.0087) <00:09:09> ({r_i: None, r_t: [-849.091 -849.091 -849.091], critic_loss: 788.0150146484375, actor_loss: 807.2899780273438, eps: 0.009})
Step:   23700, Reward: [-453.505 -453.505 -453.505] [95.118], Avg: [-632.099 -632.099 -632.099] (0.0085) <00:09:11> ({r_i: None, r_t: [-856.526 -856.526 -856.526], critic_loss: 1023.6909790039062, actor_loss: 968.2620239257812, eps: 0.009})
Step:   23800, Reward: [-457.797 -457.797 -457.797] [78.506], Avg: [-631.369 -631.369 -631.369] (0.0084) <00:09:14> ({r_i: None, r_t: [-821.990 -821.990 -821.990], critic_loss: 750.676025390625, actor_loss: 743.5029907226562, eps: 0.008})
Step:   23900, Reward: [-442.133 -442.133 -442.133] [87.001], Avg: [-630.581 -630.581 -630.581] (0.0082) <00:09:16> ({r_i: None, r_t: [-922.898 -922.898 -922.898], critic_loss: 848.2239990234375, actor_loss: 820.3150024414062, eps: 0.008})
Step:   24000, Reward: [-444.108 -444.108 -444.108] [134.439], Avg: [-629.807 -629.807 -629.807] (0.0080) <00:09:18> ({r_i: None, r_t: [-881.521 -881.521 -881.521], critic_loss: 1418.8509521484375, actor_loss: 1420.18896484375, eps: 0.008})
Step:   24100, Reward: [-421.447 -421.447 -421.447] [67.427], Avg: [-628.946 -628.946 -628.946] (0.0079) <00:09:21> ({r_i: None, r_t: [-866.202 -866.202 -866.202], critic_loss: 721.6110229492188, actor_loss: 736.1519775390625, eps: 0.008})
Step:   24200, Reward: [-423.325 -423.325 -423.325] [66.233], Avg: [-628.100 -628.100 -628.100] (0.0077) <00:09:23> ({r_i: None, r_t: [-843.068 -843.068 -843.068], critic_loss: 937.9140014648438, actor_loss: 935.989990234375, eps: 0.008})
Step:   24300, Reward: [-423.978 -423.978 -423.978] [60.605], Avg: [-627.263 -627.263 -627.263] (0.0076) <00:09:25> ({r_i: None, r_t: [-905.403 -905.403 -905.403], critic_loss: 681.468994140625, actor_loss: 682.9439697265625, eps: 0.008})
Step:   24400, Reward: [-461.261 -461.261 -461.261] [77.833], Avg: [-626.586 -626.586 -626.586] (0.0074) <00:09:27> ({r_i: None, r_t: [-772.886 -772.886 -772.886], critic_loss: 865.5910034179688, actor_loss: 883.1810302734375, eps: 0.007})
Step:   24500, Reward: [-471.658 -471.658 -471.658] [89.498], Avg: [-625.956 -625.956 -625.956] (0.0073) <00:09:30> ({r_i: None, r_t: [-835.726 -835.726 -835.726], critic_loss: 826.9920043945312, actor_loss: 802.0440063476562, eps: 0.007})
Step:   24600, Reward: [-444.794 -444.794 -444.794] [84.310], Avg: [-625.222 -625.222 -625.222] (0.0071) <00:09:32> ({r_i: None, r_t: [-904.313 -904.313 -904.313], critic_loss: 828.2969970703125, actor_loss: 842.39697265625, eps: 0.007})
Step:   24700, Reward: [-359.906 -359.906 -359.906] [85.863], Avg: [-624.153 -624.153 -624.153] (0.0070) <00:09:34> ({r_i: None, r_t: [-903.761 -903.761 -903.761], critic_loss: 998.31201171875, actor_loss: 985.9860229492188, eps: 0.007})
Step:   24800, Reward: [-428.614 -428.614 -428.614] [116.981], Avg: [-623.367 -623.367 -623.367] (0.0068) <00:09:36> ({r_i: None, r_t: [-832.682 -832.682 -832.682], critic_loss: 1021.52197265625, actor_loss: 996.0609741210938, eps: 0.007})
Step:   24900, Reward: [-425.234 -425.234 -425.234] [88.900], Avg: [-622.575 -622.575 -622.575] (0.0067) <00:09:39> ({r_i: None, r_t: [-841.453 -841.453 -841.453], critic_loss: 708.7440185546875, actor_loss: 711.7780151367188, eps: 0.007})
Step:   25000, Reward: [-492.179 -492.179 -492.179] [104.400], Avg: [-622.055 -622.055 -622.055] (0.0066) <00:09:41> ({r_i: None, r_t: [-867.680 -867.680 -867.680], critic_loss: 730.177001953125, actor_loss: 721.697021484375, eps: 0.007})
Step:   25100, Reward: [-467.882 -467.882 -467.882] [100.996], Avg: [-621.443 -621.443 -621.443] (0.0064) <00:09:44> ({r_i: None, r_t: [-899.842 -899.842 -899.842], critic_loss: 819.8770141601562, actor_loss: 831.1199951171875, eps: 0.006})
Step:   25200, Reward: [-471.249 -471.249 -471.249] [66.175], Avg: [-620.850 -620.850 -620.850] (0.0063) <00:09:46> ({r_i: None, r_t: [-846.213 -846.213 -846.213], critic_loss: 699.6409912109375, actor_loss: 693.301025390625, eps: 0.006})
Step:   25300, Reward: [-439.747 -439.747 -439.747] [95.915], Avg: [-620.137 -620.137 -620.137] (0.0062) <00:09:48> ({r_i: None, r_t: [-953.910 -953.910 -953.910], critic_loss: 971.5889892578125, actor_loss: 967.89697265625, eps: 0.006})
Step:   25400, Reward: [-435.576 -435.576 -435.576] [143.496], Avg: [-619.413 -619.413 -619.413] (0.0061) <00:09:51> ({r_i: None, r_t: [-882.349 -882.349 -882.349], critic_loss: 1308.156982421875, actor_loss: 1347.387939453125, eps: 0.006})
Step:   25500, Reward: [-422.541 -422.541 -422.541] [71.177], Avg: [-618.644 -618.644 -618.644] (0.0059) <00:09:53> ({r_i: None, r_t: [-847.292 -847.292 -847.292], critic_loss: 884.8330078125, actor_loss: 897.552978515625, eps: 0.006})
Step:   25600, Reward: [-479.044 -479.044 -479.044] [74.640], Avg: [-618.101 -618.101 -618.101] (0.0058) <00:09:55> ({r_i: None, r_t: [-866.111 -866.111 -866.111], critic_loss: 932.2410278320312, actor_loss: 949.0260009765625, eps: 0.006})
Step:   25700, Reward: [-392.211 -392.211 -392.211] [69.998], Avg: [-617.225 -617.225 -617.225] (0.0057) <00:09:58> ({r_i: None, r_t: [-866.507 -866.507 -866.507], critic_loss: 901.7249755859375, actor_loss: 890.56298828125, eps: 0.006})
Step:   25800, Reward: [-424.177 -424.177 -424.177] [63.811], Avg: [-616.480 -616.480 -616.480] (0.0056) <00:10:00> ({r_i: None, r_t: [-892.531 -892.531 -892.531], critic_loss: 991.802978515625, actor_loss: 1002.6849975585938, eps: 0.006})
Step:   25900, Reward: [-434.804 -434.804 -434.804] [79.098], Avg: [-615.781 -615.781 -615.781] (0.0055) <00:10:02> ({r_i: None, r_t: [-819.008 -819.008 -819.008], critic_loss: 888.4359741210938, actor_loss: 891.4149780273438, eps: 0.005})
Step:   26000, Reward: [-468.233 -468.233 -468.233] [92.534], Avg: [-615.216 -615.216 -615.216] (0.0054) <00:10:05> ({r_i: None, r_t: [-893.923 -893.923 -893.923], critic_loss: 1074.3360595703125, actor_loss: 1078.5489501953125, eps: 0.005})
Step:   26100, Reward: [-396.747 -396.747 -396.747] [76.338], Avg: [-614.382 -614.382 -614.382] (0.0053) <00:10:07> ({r_i: None, r_t: [-914.546 -914.546 -914.546], critic_loss: 863.4949951171875, actor_loss: 875.2410278320312, eps: 0.005})
Step:   26200, Reward: [-411.350 -411.350 -411.350] [83.701], Avg: [-613.610 -613.610 -613.610] (0.0052) <00:10:09> ({r_i: None, r_t: [-872.723 -872.723 -872.723], critic_loss: 850.197998046875, actor_loss: 858.8499755859375, eps: 0.005})
Step:   26300, Reward: [-416.095 -416.095 -416.095] [100.514], Avg: [-612.862 -612.862 -612.862] (0.0051) <00:10:12> ({r_i: None, r_t: [-862.518 -862.518 -862.518], critic_loss: 752.6199951171875, actor_loss: 774.5590209960938, eps: 0.005})
Step:   26400, Reward: [-457.381 -457.381 -457.381] [111.288], Avg: [-612.275 -612.275 -612.275] (0.0050) <00:10:14> ({r_i: None, r_t: [-822.015 -822.015 -822.015], critic_loss: 669.7310180664062, actor_loss: 695.5579833984375, eps: 0.005})
Step:   26500, Reward: [-413.207 -413.207 -413.207] [78.709], Avg: [-611.527 -611.527 -611.527] (0.0049) <00:10:16> ({r_i: None, r_t: [-919.366 -919.366 -919.366], critic_loss: 1345.2490234375, actor_loss: 1330.7779541015625, eps: 0.005})
Step:   26600, Reward: [-436.354 -436.354 -436.354] [107.060], Avg: [-610.871 -610.871 -610.871] (0.0048) <00:10:18> ({r_i: None, r_t: [-848.449 -848.449 -848.449], critic_loss: 1011.8839721679688, actor_loss: 992.2239990234375, eps: 0.005})
Step:   26700, Reward: [-414.224 -414.224 -414.224] [72.586], Avg: [-610.137 -610.137 -610.137] (0.0047) <00:10:21> ({r_i: None, r_t: [-923.160 -923.160 -923.160], critic_loss: 1094.2120361328125, actor_loss: 1083.156982421875, eps: 0.005})
Step:   26800, Reward: [-462.877 -462.877 -462.877] [106.884], Avg: [-609.590 -609.590 -609.590] (0.0046) <00:10:23> ({r_i: None, r_t: [-889.940 -889.940 -889.940], critic_loss: 746.6270141601562, actor_loss: 755.4099731445312, eps: 0.005})
Step:   26900, Reward: [-470.851 -470.851 -470.851] [81.102], Avg: [-609.076 -609.076 -609.076] (0.0045) <00:10:25> ({r_i: None, r_t: [-910.004 -910.004 -910.004], critic_loss: 1038.3160400390625, actor_loss: 1091.5140380859375, eps: 0.004})
Step:   27000, Reward: [-467.006 -467.006 -467.006] [88.637], Avg: [-608.551 -608.551 -608.551] (0.0044) <00:10:28> ({r_i: None, r_t: [-910.929 -910.929 -910.929], critic_loss: 921.3660278320312, actor_loss: 988.1610107421875, eps: 0.004})
Step:   27100, Reward: [-525.704 -525.704 -525.704] [104.701], Avg: [-608.247 -608.247 -608.247] (0.0043) <00:10:30> ({r_i: None, r_t: [-1056.447 -1056.447 -1056.447], critic_loss: 1919.6529541015625, actor_loss: 1989.81396484375, eps: 0.004})
Step:   27200, Reward: [-527.824 -527.824 -527.824] [86.768], Avg: [-607.952 -607.952 -607.952] (0.0042) <00:10:33> ({r_i: None, r_t: [-922.001 -922.001 -922.001], critic_loss: 1256.10400390625, actor_loss: 1385.614990234375, eps: 0.004})
Step:   27300, Reward: [-451.227 -451.227 -451.227] [91.169], Avg: [-607.380 -607.380 -607.380] (0.0041) <00:10:35> ({r_i: None, r_t: [-960.850 -960.850 -960.850], critic_loss: 986.7899780273438, actor_loss: 1136.3409423828125, eps: 0.004})
Step:   27400, Reward: [-489.492 -489.492 -489.492] [96.282], Avg: [-606.952 -606.952 -606.952] (0.0041) <00:10:37> ({r_i: None, r_t: [-1011.469 -1011.469 -1011.469], critic_loss: 996.052978515625, actor_loss: 1138.4200439453125, eps: 0.004})
Step:   27500, Reward: [-483.026 -483.026 -483.026] [106.723], Avg: [-606.503 -606.503 -606.503] (0.0040) <00:10:39> ({r_i: None, r_t: [-1025.015 -1025.015 -1025.015], critic_loss: 1671.81494140625, actor_loss: 1921.06494140625, eps: 0.004})
Step:   27600, Reward: [-558.216 -558.216 -558.216] [112.589], Avg: [-606.328 -606.328 -606.328] (0.0039) <00:10:42> ({r_i: None, r_t: [-997.139 -997.139 -997.139], critic_loss: 1392.93701171875, actor_loss: 1612.3690185546875, eps: 0.004})
Step:   27700, Reward: [-555.246 -555.246 -555.246] [114.211], Avg: [-606.144 -606.144 -606.144] (0.0038) <00:10:44> ({r_i: None, r_t: [-1094.057 -1094.057 -1094.057], critic_loss: 846.1370239257812, actor_loss: 1032.654052734375, eps: 0.004})
Step:   27800, Reward: [-588.287 -588.287 -588.287] [152.736], Avg: [-606.080 -606.080 -606.080] (0.0037) <00:10:46> ({r_i: None, r_t: [-1101.666 -1101.666 -1101.666], critic_loss: 808.2360229492188, actor_loss: 976.4920043945312, eps: 0.004})
Step:   27900, Reward: [-552.452 -552.452 -552.452] [104.701], Avg: [-605.889 -605.889 -605.889] (0.0037) <00:10:49> ({r_i: None, r_t: [-1050.519 -1050.519 -1050.519], critic_loss: 968.2169799804688, actor_loss: 1193.4019775390625, eps: 0.004})
Step:   28000, Reward: [-577.082 -577.082 -577.082] [125.586], Avg: [-605.786 -605.786 -605.786] (0.0036) <00:10:51> ({r_i: None, r_t: [-1199.971 -1199.971 -1199.971], critic_loss: 830.7410278320312, actor_loss: 903.031982421875, eps: 0.004})
Step:   28100, Reward: [-543.500 -543.500 -543.500] [122.654], Avg: [-605.566 -605.566 -605.566] (0.0035) <00:10:53> ({r_i: None, r_t: [-1122.198 -1122.198 -1122.198], critic_loss: 1375.2850341796875, actor_loss: 1517.5860595703125, eps: 0.004})
Step:   28200, Reward: [-553.558 -553.558 -553.558] [123.137], Avg: [-605.382 -605.382 -605.382] (0.0035) <00:10:56> ({r_i: None, r_t: [-1142.468 -1142.468 -1142.468], critic_loss: 1385.572021484375, actor_loss: 1487.175048828125, eps: 0.003})
Step:   28300, Reward: [-586.965 -586.965 -586.965] [154.587], Avg: [-605.317 -605.317 -605.317] (0.0034) <00:10:58> ({r_i: None, r_t: [-1197.655 -1197.655 -1197.655], critic_loss: 1742.0810546875, actor_loss: 1920.238037109375, eps: 0.003})
Step:   28400, Reward: [-566.841 -566.841 -566.841] [128.598], Avg: [-605.182 -605.182 -605.182] (0.0033) <00:11:00> ({r_i: None, r_t: [-1285.830 -1285.830 -1285.830], critic_loss: 1425.18994140625, actor_loss: 1800.364013671875, eps: 0.003})
Step:   28500, Reward: [-576.947 -576.947 -576.947] [116.159], Avg: [-605.083 -605.083 -605.083] (0.0033) <00:11:03> ({r_i: None, r_t: [-1189.412 -1189.412 -1189.412], critic_loss: 1788.594970703125, actor_loss: 2298.406005859375, eps: 0.003})
