Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread, Date: 13/03/2020 03:04:50
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import torch
import numpy as np
from models.rand import MultiagentReplayBuffer, MultiagentReplayBuffer3
from models.ddpg import DDPGCritic, DDPGNetwork
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, TARGET_UPDATE_RATE, gsoftmax, one_hot

EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
LEARN_RATE = 0.001				# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
ENTROPY_WEIGHT = 0.01			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 10			# How many experience tuples to sample from the buffer for each train step
MAX_BUFFER_SIZE = 64			# Sets the maximum length of the replay buffer
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return gsoftmax(action, hard=not sample)

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu, name="maddpg")
		self.critic = lambda s,a: DDPGCritic([np.sum([np.prod(s) for s in state_size])], [np.sum([np.prod(a) for a in action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(state_size, action_size)]
		self.action_size = action_size
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action_probs = [model.get_action(s, use_target, grad, numpy=numpy, sample=sample) for s,model in zip(state, self.models)]
			return action_probs

	def optimize(self, states, actions, states_joint, actions_joint, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		stats = []
		for i, (agent, state, reward, done) in enumerate(zip(self.models, states, rewards, dones)):
			next_value = agent.get_q_value(states_joint, actions_joint, use_target=True, numpy=False)
			next_value = torch.cat([next_value, torch.zeros_like(next_value[:,-1]).unsqueeze(1)], dim=1)
			q_targets = PTACAgent.compute_ma_gae(reward.unsqueeze(-1), done.unsqueeze(-1), next_value)
			q_values = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (q_values - q_targets.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			actor_action = agent.get_action(state, grad=True, numpy=False)
			action = [gsoftmax(actor_action, hard=True) if j==i else one_hot(action) for (j,model), action in zip(enumerate(self.models), actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -(agent.critic_local(states_joint, action_joint)-q_targets).mean() - e_weight*(actor_action*actor_action.log()).mean()
			agent.step(agent.actor_optimizer, actor_loss, agent.actor_local.parameters())
			stats.append([x.detach().cpu().numpy() for x in [critic_loss, actor_loss]])
		return np.mean(stats, axis=-1)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, dirname, f"{name}_{i}", self.name) for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, dirname, f"{name}_{i}", self.name) for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)
		self.stats = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [np.tanh((1-eps)*a_greedy + eps*a_random) for a_greedy, a_random in zip(action_greedy, action_random)]
		return action_greedy

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([one_hot(a).view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			self.replay_buffer.add([self.to_numpy([t.transpose(0,1) for t in x]) for x in (states, actions, [states_joint], [actions_joint], rewards, dones)])
			self.buffer.clear()	
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, states_joint, actions_joint, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			self.stats.append(self.network.optimize(states, actions, states_joint[0], actions_joint[0], rewards, dones, gamma=DISCOUNT_RATE))			
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

	def get_stats(self):
		stats = {k:v for k,v in zip(["critic_loss", "actor_loss"], np.mean(self.stats, axis=0))} if len(self.stats)>0 else {}
		self.stats = []
		return {**stats, **super().get_stats()}

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward,eps: [0.1*(ballr(o[0,88], o[0,89])) + r for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def train(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.selfport is not None or MPI_RANK>0:
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	elif args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	else:
		train(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-503.608 -503.608 -503.608] [97.277], Avg: [-503.608 -503.608 -503.608] (1.0000) <00:00:00> ({r_i: None, r_t: [-9.241 -9.241 -9.241], eps: 1.0})
Step:     100, Reward: [-556.500 -556.500 -556.500] [182.492], Avg: [-530.054 -530.054 -530.054] (0.9801) <00:00:02> ({r_i: None, r_t: [-1064.978 -1064.978 -1064.978], critic_loss: 13594.51171875, actor_loss: 13583.71875, eps: 0.98})
Step:     200, Reward: [-519.205 -519.205 -519.205] [169.997], Avg: [-526.437 -526.437 -526.437] (0.9606) <00:00:04> ({r_i: None, r_t: [-1067.163 -1067.163 -1067.163], critic_loss: 11407.908203125, actor_loss: 11406.2412109375, eps: 0.961})
Step:     300, Reward: [-551.491 -551.491 -551.491] [117.654], Avg: [-532.701 -532.701 -532.701] (0.9415) <00:00:06> ({r_i: None, r_t: [-1094.923 -1094.923 -1094.923], critic_loss: 11034.337890625, actor_loss: 11040.9384765625, eps: 0.941})
Step:     400, Reward: [-554.577 -554.577 -554.577] [128.377], Avg: [-537.076 -537.076 -537.076] (0.9227) <00:00:08> ({r_i: None, r_t: [-1165.149 -1165.149 -1165.149], critic_loss: 16185.3603515625, actor_loss: 16227.1474609375, eps: 0.923})
Step:     500, Reward: [-608.341 -608.341 -608.341] [144.938], Avg: [-548.953 -548.953 -548.953] (0.9044) <00:00:10> ({r_i: None, r_t: [-1146.640 -1146.640 -1146.640], critic_loss: 10321.1337890625, actor_loss: 10344.9755859375, eps: 0.904})
Step:     600, Reward: [-541.842 -541.842 -541.842] [84.007], Avg: [-547.937 -547.937 -547.937] (0.8864) <00:00:13> ({r_i: None, r_t: [-1102.604 -1102.604 -1102.604], critic_loss: 14011.2578125, actor_loss: 14147.4140625, eps: 0.886})
Step:     700, Reward: [-482.712 -482.712 -482.712] [95.067], Avg: [-539.784 -539.784 -539.784] (0.8687) <00:00:15> ({r_i: None, r_t: [-1269.558 -1269.558 -1269.558], critic_loss: 14550.912109375, actor_loss: 14714.107421875, eps: 0.869})
Step:     800, Reward: [-462.529 -462.529 -462.529] [86.305], Avg: [-531.200 -531.200 -531.200] (0.8515) <00:00:17> ({r_i: None, r_t: [-1070.476 -1070.476 -1070.476], critic_loss: 11361.1220703125, actor_loss: 11357.70703125, eps: 0.851})
Step:     900, Reward: [-562.186 -562.186 -562.186] [93.131], Avg: [-534.299 -534.299 -534.299] (0.8345) <00:00:19> ({r_i: None, r_t: [-982.605 -982.605 -982.605], critic_loss: 8814.392578125, actor_loss: 8934.91796875, eps: 0.835})
Step:    1000, Reward: [-473.112 -473.112 -473.112] [79.635], Avg: [-528.737 -528.737 -528.737] (0.8179) <00:00:21> ({r_i: None, r_t: [-1013.582 -1013.582 -1013.582], critic_loss: 6099.537109375, actor_loss: 6213.755859375, eps: 0.818})
Step:    1100, Reward: [-461.202 -461.202 -461.202] [100.960], Avg: [-523.109 -523.109 -523.109] (0.8016) <00:00:24> ({r_i: None, r_t: [-1029.838 -1029.838 -1029.838], critic_loss: 7142.88623046875, actor_loss: 6938.67578125, eps: 0.802})
Step:    1200, Reward: [-489.982 -489.982 -489.982] [129.768], Avg: [-520.560 -520.560 -520.560] (0.7857) <00:00:26> ({r_i: None, r_t: [-1001.446 -1001.446 -1001.446], critic_loss: 5900.6240234375, actor_loss: 6021.083984375, eps: 0.786})
Step:    1300, Reward: [-437.524 -437.524 -437.524] [99.905], Avg: [-514.629 -514.629 -514.629] (0.7700) <00:00:28> ({r_i: None, r_t: [-973.318 -973.318 -973.318], critic_loss: 5590.9521484375, actor_loss: 5755.18310546875, eps: 0.77})
Step:    1400, Reward: [-474.198 -474.198 -474.198] [107.426], Avg: [-511.934 -511.934 -511.934] (0.7547) <00:00:30> ({r_i: None, r_t: [-994.731 -994.731 -994.731], critic_loss: 4800.92822265625, actor_loss: 5129.09423828125, eps: 0.755})
Step:    1500, Reward: [-526.885 -526.885 -526.885] [94.207], Avg: [-512.868 -512.868 -512.868] (0.7397) <00:00:32> ({r_i: None, r_t: [-959.599 -959.599 -959.599], critic_loss: 5775.84814453125, actor_loss: 6083.9169921875, eps: 0.74})
Step:    1600, Reward: [-470.438 -470.438 -470.438] [80.064], Avg: [-510.372 -510.372 -510.372] (0.7250) <00:00:34> ({r_i: None, r_t: [-1009.803 -1009.803 -1009.803], critic_loss: 6463.9560546875, actor_loss: 6951.10302734375, eps: 0.725})
Step:    1700, Reward: [-533.511 -533.511 -533.511] [100.812], Avg: [-511.658 -511.658 -511.658] (0.7106) <00:00:37> ({r_i: None, r_t: [-981.984 -981.984 -981.984], critic_loss: 5997.10791015625, actor_loss: 6360.1259765625, eps: 0.711})
Step:    1800, Reward: [-485.616 -485.616 -485.616] [110.006], Avg: [-510.287 -510.287 -510.287] (0.6964) <00:00:39> ({r_i: None, r_t: [-988.388 -988.388 -988.388], critic_loss: 6289.43310546875, actor_loss: 6573.748046875, eps: 0.696})
Step:    1900, Reward: [-483.655 -483.655 -483.655] [110.377], Avg: [-508.956 -508.956 -508.956] (0.6826) <00:00:41> ({r_i: None, r_t: [-1013.427 -1013.427 -1013.427], critic_loss: 6024.74609375, actor_loss: 6358.22998046875, eps: 0.683})
Step:    2000, Reward: [-459.787 -459.787 -459.787] [79.049], Avg: [-506.614 -506.614 -506.614] (0.6690) <00:00:43> ({r_i: None, r_t: [-993.394 -993.394 -993.394], critic_loss: 5058.05322265625, actor_loss: 5212.3740234375, eps: 0.669})
Step:    2100, Reward: [-487.086 -487.086 -487.086] [99.718], Avg: [-505.727 -505.727 -505.727] (0.6557) <00:00:46> ({r_i: None, r_t: [-953.491 -953.491 -953.491], critic_loss: 4886.23779296875, actor_loss: 4921.69384765625, eps: 0.656})
Step:    2200, Reward: [-477.151 -477.151 -477.151] [98.147], Avg: [-504.484 -504.484 -504.484] (0.6426) <00:00:48> ({r_i: None, r_t: [-997.062 -997.062 -997.062], critic_loss: 3614.59912109375, actor_loss: 3634.330078125, eps: 0.643})
Step:    2300, Reward: [-527.225 -527.225 -527.225] [124.110], Avg: [-505.432 -505.432 -505.432] (0.6298) <00:00:50> ({r_i: None, r_t: [-1031.214 -1031.214 -1031.214], critic_loss: 3061.47607421875, actor_loss: 3031.867919921875, eps: 0.63})
Step:    2400, Reward: [-480.575 -480.575 -480.575] [95.740], Avg: [-504.437 -504.437 -504.437] (0.6173) <00:00:52> ({r_i: None, r_t: [-991.345 -991.345 -991.345], critic_loss: 3387.80908203125, actor_loss: 3341.867919921875, eps: 0.617})
Step:    2500, Reward: [-502.328 -502.328 -502.328] [98.636], Avg: [-504.356 -504.356 -504.356] (0.6050) <00:00:55> ({r_i: None, r_t: [-979.097 -979.097 -979.097], critic_loss: 2903.93505859375, actor_loss: 2899.969970703125, eps: 0.605})
Step:    2600, Reward: [-497.979 -497.979 -497.979] [107.266], Avg: [-504.120 -504.120 -504.120] (0.5930) <00:00:57> ({r_i: None, r_t: [-1024.317 -1024.317 -1024.317], critic_loss: 2384.110107421875, actor_loss: 2352.531005859375, eps: 0.593})
Step:    2700, Reward: [-472.652 -472.652 -472.652] [107.033], Avg: [-502.996 -502.996 -502.996] (0.5812) <00:00:59> ({r_i: None, r_t: [-948.384 -948.384 -948.384], critic_loss: 2897.885009765625, actor_loss: 2906.489990234375, eps: 0.581})
Step:    2800, Reward: [-533.489 -533.489 -533.489] [147.585], Avg: [-504.048 -504.048 -504.048] (0.5696) <00:01:01> ({r_i: None, r_t: [-951.083 -951.083 -951.083], critic_loss: 1546.43505859375, actor_loss: 1510.843017578125, eps: 0.57})
