Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

EPS_DECAY = 0.999             	# The rate at which eps decays from EPS_MAX to EPS_MIN
LEARN_RATE = 0.001				# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
REPLAY_BATCH_SIZE = 1024		# How many experience tuples to sample from the buffer for each train step
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		for i, agent in enumerate(self.models):
			next_actions = [model.get_action(nobs, grad=False, numpy=False) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			q_target = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (q_value - q_target.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			actor_action = agent.get_action(states[i], grad=True, numpy=False)
			action = [gsoftmax(actor_action, hard=True) if j==i else one_hot(model.get_action(ob, grad=False, numpy=False)) for (j,model), ob in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -(agent.critic_local(states_joint, action_joint)-q_target).mean() + e_weight*actor_action.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		# eps = self.eps if eps is None else eps
		# action_random = super().get_action(state, eps)
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		# action = [np.clip((1-eps)*a_greedy + eps*a_random, -1, 1) for a_greedy, a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, device=self.network.device)
			self.network.optimize(states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(lambda: make_env(env_name), ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if env_name=="3_vs_3_custom" else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.0, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-631.262 -631.262 -631.262] [175.1834], Avg: [-806.445 -806.445 -806.445] (0.999)
Step: 99, Reward: [-491.235 -491.235 -491.235] [89.0332], Avg: [-693.357 -693.357 -693.357] (0.998)
Step: 149, Reward: [-564.95 -564.95 -564.95] [132.7798], Avg: [-694.814 -694.814 -694.814] (0.997)
Step: 199, Reward: [-498.807 -498.807 -498.807] [69.6629], Avg: [-663.228 -663.228 -663.228] (0.996)
Step: 249, Reward: [-494.434 -494.434 -494.434] [59.2595], Avg: [-641.321 -641.321 -641.321] (0.995)
Step: 299, Reward: [-531.246 -531.246 -531.246] [120.2094], Avg: [-643.01 -643.01 -643.01] (0.994)
Step: 349, Reward: [-511.319 -511.319 -511.319] [92.7619], Avg: [-637.449 -637.449 -637.449] (0.993)
Step: 399, Reward: [-486.458 -486.458 -486.458] [37.6991], Avg: [-623.288 -623.288 -623.288] (0.992)
Step: 449, Reward: [-518.854 -518.854 -518.854] [82.8733], Avg: [-620.892 -620.892 -620.892] (0.991)
Step: 499, Reward: [-628.263 -628.263 -628.263] [283.0413], Avg: [-649.933 -649.933 -649.933] (0.990)
Step: 549, Reward: [-619.199 -619.199 -619.199] [84.7942], Avg: [-654.848 -654.848 -654.848] (0.989)
Step: 599, Reward: [-645.922 -645.922 -645.922] [133.5732], Avg: [-665.235 -665.235 -665.235] (0.988)
Step: 649, Reward: [-768.101 -768.101 -768.101] [226.8054], Avg: [-690.594 -690.594 -690.594] (0.987)
Step: 699, Reward: [-682.685 -682.685 -682.685] [119.5353], Avg: [-698.568 -698.568 -698.568] (0.986)
Step: 749, Reward: [-665.514 -665.514 -665.514] [144.6520], Avg: [-706.008 -706.008 -706.008] (0.985)
Step: 799, Reward: [-599.839 -599.839 -599.839] [72.0202], Avg: [-703.873 -703.873 -703.873] (0.984)
Step: 849, Reward: [-745.206 -745.206 -745.206] [146.4547], Avg: [-714.92 -714.92 -714.92] (0.983)
Step: 899, Reward: [-672.47 -672.47 -672.47] [75.8717], Avg: [-716.776 -716.776 -716.776] (0.982)
Step: 949, Reward: [-792.697 -792.697 -792.697] [160.2514], Avg: [-729.207 -729.207 -729.207] (0.981)
Step: 999, Reward: [-807.238 -807.238 -807.238] [260.6501], Avg: [-746.141 -746.141 -746.141] (0.980)
Step: 1049, Reward: [-642.359 -642.359 -642.359] [169.6331], Avg: [-749.276 -749.276 -749.276] (0.979)
Step: 1099, Reward: [-602.677 -602.677 -602.677] [73.3930], Avg: [-745.949 -745.949 -745.949] (0.978)
Step: 1149, Reward: [-914.12 -914.12 -914.12] [300.2946], Avg: [-766.317 -766.317 -766.317] (0.977)
Step: 1199, Reward: [-972.854 -972.854 -972.854] [408.8514], Avg: [-791.958 -791.958 -791.958] (0.976)
Step: 1249, Reward: [-830.481 -830.481 -830.481] [129.6166], Avg: [-798.684 -798.684 -798.684] (0.975)
Step: 1299, Reward: [-983.017 -983.017 -983.017] [292.1345], Avg: [-817.009 -817.009 -817.009] (0.974)
Step: 1349, Reward: [-1047.2 -1047.2 -1047.2] [177.8312], Avg: [-832.121 -832.121 -832.121] (0.973)
Step: 1399, Reward: [-1241.375 -1241.375 -1241.375] [203.3363], Avg: [-853.999 -853.999 -853.999] (0.972)
Step: 1449, Reward: [-1175.523 -1175.523 -1175.523] [278.7035], Avg: [-874.697 -874.697 -874.697] (0.971)
Step: 1499, Reward: [-951.096 -951.096 -951.096] [185.8935], Avg: [-883.44 -883.44 -883.44] (0.970)
Step: 1549, Reward: [-1017.644 -1017.644 -1017.644] [186.5553], Avg: [-893.787 -893.787 -893.787] (0.969)
Step: 1599, Reward: [-1046.275 -1046.275 -1046.275] [120.0067], Avg: [-902.303 -902.303 -902.303] (0.968)
Step: 1649, Reward: [-986.134 -986.134 -986.134] [179.2932], Avg: [-910.276 -910.276 -910.276] (0.968)
Step: 1699, Reward: [-1230.133 -1230.133 -1230.133] [294.7210], Avg: [-928.352 -928.352 -928.352] (0.967)
Step: 1749, Reward: [-970.347 -970.347 -970.347] [218.5909], Avg: [-935.797 -935.797 -935.797] (0.966)
Step: 1799, Reward: [-1206.205 -1206.205 -1206.205] [162.4898], Avg: [-947.822 -947.822 -947.822] (0.965)
Step: 1849, Reward: [-1239.769 -1239.769 -1239.769] [220.9897], Avg: [-961.685 -961.685 -961.685] (0.964)
Step: 1899, Reward: [-1066.678 -1066.678 -1066.678] [162.3699], Avg: [-968.721 -968.721 -968.721] (0.963)
Step: 1949, Reward: [-1156.559 -1156.559 -1156.559] [227.0399], Avg: [-979.359 -979.359 -979.359] (0.962)
Step: 1999, Reward: [-1034.932 -1034.932 -1034.932] [182.1311], Avg: [-985.302 -985.302 -985.302] (0.961)
Step: 2049, Reward: [-1050.227 -1050.227 -1050.227] [178.9957], Avg: [-991.251 -991.251 -991.251] (0.960)
Step: 2099, Reward: [-1118.118 -1118.118 -1118.118] [91.6078], Avg: [-996.453 -996.453 -996.453] (0.959)
Step: 2149, Reward: [-1241.401 -1241.401 -1241.401] [162.8092], Avg: [-1005.935 -1005.935 -1005.935] (0.958)
Step: 2199, Reward: [-1397.547 -1397.547 -1397.547] [137.6734], Avg: [-1017.965 -1017.965 -1017.965] (0.957)
Step: 2249, Reward: [-1356.044 -1356.044 -1356.044] [225.1680], Avg: [-1030.481 -1030.481 -1030.481] (0.956)
Step: 2299, Reward: [-1353.235 -1353.235 -1353.235] [87.9949], Avg: [-1039.411 -1039.411 -1039.411] (0.955)
Step: 2349, Reward: [-1596.04 -1596.04 -1596.04] [200.1473], Avg: [-1055.512 -1055.512 -1055.512] (0.954)
Step: 2399, Reward: [-1510.997 -1510.997 -1510.997] [193.4062], Avg: [-1069.031 -1069.031 -1069.031] (0.953)
Step: 2449, Reward: [-1676.209 -1676.209 -1676.209] [152.5048], Avg: [-1084.534 -1084.534 -1084.534] (0.952)
Step: 2499, Reward: [-1751.237 -1751.237 -1751.237] [338.7698], Avg: [-1104.644 -1104.644 -1104.644] (0.951)
Step: 2549, Reward: [-1721.733 -1721.733 -1721.733] [123.2746], Avg: [-1119.161 -1119.161 -1119.161] (0.950)
Step: 2599, Reward: [-1703.41 -1703.41 -1703.41] [253.2906], Avg: [-1135.267 -1135.267 -1135.267] (0.949)
Step: 2649, Reward: [-1569.41 -1569.41 -1569.41] [465.7697], Avg: [-1152.247 -1152.247 -1152.247] (0.948)
Step: 2699, Reward: [-1722.413 -1722.413 -1722.413] [381.4098], Avg: [-1169.869 -1169.869 -1169.869] (0.947)
Step: 2749, Reward: [-1527.441 -1527.441 -1527.441] [501.0384], Avg: [-1185.48 -1185.48 -1185.48] (0.946)
Step: 2799, Reward: [-1813.369 -1813.369 -1813.369] [292.5971], Avg: [-1201.917 -1201.917 -1201.917] (0.946)
Step: 2849, Reward: [-1690.794 -1690.794 -1690.794] [261.9370], Avg: [-1215.089 -1215.089 -1215.089] (0.945)
Step: 2899, Reward: [-1401.104 -1401.104 -1401.104] [262.4567], Avg: [-1222.821 -1222.821 -1222.821] (0.944)
Step: 2949, Reward: [-1592.343 -1592.343 -1592.343] [327.6969], Avg: [-1234.639 -1234.639 -1234.639] (0.943)
Step: 2999, Reward: [-1475.429 -1475.429 -1475.429] [272.6876], Avg: [-1243.197 -1243.197 -1243.197] (0.942)
Step: 3049, Reward: [-1842.975 -1842.975 -1842.975] [85.0798], Avg: [-1254.424 -1254.424 -1254.424] (0.941)
Step: 3099, Reward: [-1626.7 -1626.7 -1626.7] [238.6405], Avg: [-1264.277 -1264.277 -1264.277] (0.940)
Step: 3149, Reward: [-1407.372 -1407.372 -1407.372] [225.6666], Avg: [-1270.131 -1270.131 -1270.131] (0.939)
Step: 3199, Reward: [-1614.926 -1614.926 -1614.926] [311.3169], Avg: [-1280.382 -1280.382 -1280.382] (0.938)
Step: 3249, Reward: [-1523.983 -1523.983 -1523.983] [266.6820], Avg: [-1288.233 -1288.233 -1288.233] (0.937)
Step: 3299, Reward: [-1675.644 -1675.644 -1675.644] [373.9761], Avg: [-1299.769 -1299.769 -1299.769] (0.936)
Step: 3349, Reward: [-1356.931 -1356.931 -1356.931] [299.2785], Avg: [-1305.089 -1305.089 -1305.089] (0.935)
Step: 3399, Reward: [-1538.554 -1538.554 -1538.554] [305.5842], Avg: [-1313.016 -1313.016 -1313.016] (0.934)
Step: 3449, Reward: [-1293.208 -1293.208 -1293.208] [250.4822], Avg: [-1316.359 -1316.359 -1316.359] (0.933)
Step: 3499, Reward: [-1098.325 -1098.325 -1098.325] [406.6206], Avg: [-1319.054 -1319.054 -1319.054] (0.932)
Step: 3549, Reward: [-1114.282 -1114.282 -1114.282] [427.1475], Avg: [-1322.186 -1322.186 -1322.186] (0.931)
Step: 3599, Reward: [-1638.145 -1638.145 -1638.145] [432.8172], Avg: [-1332.585 -1332.585 -1332.585] (0.930)
Step: 3649, Reward: [-1461.176 -1461.176 -1461.176] [482.8643], Avg: [-1340.961 -1340.961 -1340.961] (0.930)
Step: 3699, Reward: [-1422.962 -1422.962 -1422.962] [214.7687], Avg: [-1344.972 -1344.972 -1344.972] (0.929)
Step: 3749, Reward: [-1582.493 -1582.493 -1582.493] [621.2929], Avg: [-1356.423 -1356.423 -1356.423] (0.928)
Step: 3799, Reward: [-1163.603 -1163.603 -1163.603] [478.5526], Avg: [-1360.182 -1360.182 -1360.182] (0.927)
Step: 3849, Reward: [-955.674 -955.674 -955.674] [200.6854], Avg: [-1357.535 -1357.535 -1357.535] (0.926)
Step: 3899, Reward: [-1546.218 -1546.218 -1546.218] [481.5653], Avg: [-1366.128 -1366.128 -1366.128] (0.925)
Step: 3949, Reward: [-1157.651 -1157.651 -1157.651] [303.1134], Avg: [-1367.326 -1367.326 -1367.326] (0.924)
Step: 3999, Reward: [-1342.457 -1342.457 -1342.457] [331.6529], Avg: [-1371.161 -1371.161 -1371.161] (0.923)
Step: 4049, Reward: [-1400.958 -1400.958 -1400.958] [451.4575], Avg: [-1377.102 -1377.102 -1377.102] (0.922)
Step: 4099, Reward: [-1362.469 -1362.469 -1362.469] [572.3790], Avg: [-1383.904 -1383.904 -1383.904] (0.921)
Step: 4149, Reward: [-1622.205 -1622.205 -1622.205] [178.8124], Avg: [-1388.93 -1388.93 -1388.93] (0.920)
Step: 4199, Reward: [-1561.5 -1561.5 -1561.5] [200.1584], Avg: [-1393.367 -1393.367 -1393.367] (0.919)
Step: 4249, Reward: [-1535.87 -1535.87 -1535.87] [302.4375], Avg: [-1398.601 -1398.601 -1398.601] (0.918)
Step: 4299, Reward: [-1619.605 -1619.605 -1619.605] [175.6681], Avg: [-1403.214 -1403.214 -1403.214] (0.918)
Step: 4349, Reward: [-1654.484 -1654.484 -1654.484] [319.8605], Avg: [-1409.779 -1409.779 -1409.779] (0.917)
Step: 4399, Reward: [-1333.328 -1333.328 -1333.328] [252.0340], Avg: [-1411.774 -1411.774 -1411.774] (0.916)
Step: 4449, Reward: [-1720.719 -1720.719 -1720.719] [133.6405], Avg: [-1416.747 -1416.747 -1416.747] (0.915)
Step: 4499, Reward: [-1776.739 -1776.739 -1776.739] [108.0020], Avg: [-1421.947 -1421.947 -1421.947] (0.914)
Step: 4549, Reward: [-1651.984 -1651.984 -1651.984] [194.3022], Avg: [-1426.61 -1426.61 -1426.61] (0.913)
Step: 4599, Reward: [-1722.54 -1722.54 -1722.54] [195.0699], Avg: [-1431.947 -1431.947 -1431.947] (0.912)
Step: 4649, Reward: [-1936.63 -1936.63 -1936.63] [176.4722], Avg: [-1439.271 -1439.271 -1439.271] (0.911)
Step: 4699, Reward: [-1503.707 -1503.707 -1503.707] [124.1157], Avg: [-1441.277 -1441.277 -1441.277] (0.910)
Step: 4749, Reward: [-1518.673 -1518.673 -1518.673] [147.6463], Avg: [-1443.646 -1443.646 -1443.646] (0.909)
Step: 4799, Reward: [-1673.871 -1673.871 -1673.871] [238.3725], Avg: [-1448.527 -1448.527 -1448.527] (0.908)
Step: 4849, Reward: [-1807.1 -1807.1 -1807.1] [207.1355], Avg: [-1454.359 -1454.359 -1454.359] (0.908)
Step: 4899, Reward: [-1488.943 -1488.943 -1488.943] [86.8934], Avg: [-1455.599 -1455.599 -1455.599] (0.907)
Step: 4949, Reward: [-1720.326 -1720.326 -1720.326] [221.0949], Avg: [-1460.506 -1460.506 -1460.506] (0.906)
Step: 4999, Reward: [-1762.89 -1762.89 -1762.89] [168.6555], Avg: [-1465.216 -1465.216 -1465.216] (0.905)
Step: 5049, Reward: [-1769.655 -1769.655 -1769.655] [147.7425], Avg: [-1469.693 -1469.693 -1469.693] (0.904)
Step: 5099, Reward: [-1804.51 -1804.51 -1804.51] [210.6755], Avg: [-1475.041 -1475.041 -1475.041] (0.903)
Step: 5149, Reward: [-1644.227 -1644.227 -1644.227] [77.8135], Avg: [-1477.439 -1477.439 -1477.439] (0.902)
Step: 5199, Reward: [-1739.041 -1739.041 -1739.041] [199.6210], Avg: [-1481.874 -1481.874 -1481.874] (0.901)
Step: 5249, Reward: [-1791.584 -1791.584 -1791.584] [150.5203], Avg: [-1486.257 -1486.257 -1486.257] (0.900)
Step: 5299, Reward: [-1690.935 -1690.935 -1690.935] [256.2754], Avg: [-1490.606 -1490.606 -1490.606] (0.899)
Step: 5349, Reward: [-1692.856 -1692.856 -1692.856] [143.2529], Avg: [-1493.835 -1493.835 -1493.835] (0.898)
Step: 5399, Reward: [-1727.263 -1727.263 -1727.263] [213.8396], Avg: [-1497.976 -1497.976 -1497.976] (0.898)
Step: 5449, Reward: [-1711.183 -1711.183 -1711.183] [198.4996], Avg: [-1501.753 -1501.753 -1501.753] (0.897)
Step: 5499, Reward: [-1763.2 -1763.2 -1763.2] [233.4530], Avg: [-1506.252 -1506.252 -1506.252] (0.896)
Step: 5549, Reward: [-1557.874 -1557.874 -1557.874] [209.4979], Avg: [-1508.605 -1508.605 -1508.605] (0.895)
Step: 5599, Reward: [-1725.596 -1725.596 -1725.596] [281.4974], Avg: [-1513.056 -1513.056 -1513.056] (0.894)
Step: 5649, Reward: [-1616.625 -1616.625 -1616.625] [163.2252], Avg: [-1515.417 -1515.417 -1515.417] (0.893)
Step: 5699, Reward: [-1541.998 -1541.998 -1541.998] [226.5395], Avg: [-1517.637 -1517.637 -1517.637] (0.892)
Step: 5749, Reward: [-1512.969 -1512.969 -1512.969] [196.8855], Avg: [-1519.308 -1519.308 -1519.308] (0.891)
Step: 5799, Reward: [-1447.666 -1447.666 -1447.666] [189.7904], Avg: [-1520.327 -1520.327 -1520.327] (0.890)
Step: 5849, Reward: [-1511.412 -1511.412 -1511.412] [122.9292], Avg: [-1521.301 -1521.301 -1521.301] (0.890)
Step: 5899, Reward: [-1312.973 -1312.973 -1312.973] [142.2686], Avg: [-1520.742 -1520.742 -1520.742] (0.889)
Step: 5949, Reward: [-1600.263 -1600.263 -1600.263] [264.1712], Avg: [-1523.63 -1523.63 -1523.63] (0.888)
Step: 5999, Reward: [-1557.816 -1557.816 -1557.816] [126.8491], Avg: [-1524.972 -1524.972 -1524.972] (0.887)
Step: 6049, Reward: [-1443.495 -1443.495 -1443.495] [178.7579], Avg: [-1525.776 -1525.776 -1525.776] (0.886)
Step: 6099, Reward: [-1610.728 -1610.728 -1610.728] [182.8835], Avg: [-1527.971 -1527.971 -1527.971] (0.885)
Step: 6149, Reward: [-1673.316 -1673.316 -1673.316] [82.9472], Avg: [-1529.827 -1529.827 -1529.827] (0.884)
Step: 6199, Reward: [-1469.326 -1469.326 -1469.326] [55.5610], Avg: [-1529.787 -1529.787 -1529.787] (0.883)
Step: 6249, Reward: [-1548.174 -1548.174 -1548.174] [207.6945], Avg: [-1531.596 -1531.596 -1531.596] (0.882)
Step: 6299, Reward: [-1396.761 -1396.761 -1396.761] [134.0622], Avg: [-1531.59 -1531.59 -1531.59] (0.882)
Step: 6349, Reward: [-1457.652 -1457.652 -1457.652] [153.1001], Avg: [-1532.213 -1532.213 -1532.213] (0.881)
Step: 6399, Reward: [-1442.004 -1442.004 -1442.004] [205.7847], Avg: [-1533.116 -1533.116 -1533.116] (0.880)
Step: 6449, Reward: [-1326.104 -1326.104 -1326.104] [321.5183], Avg: [-1534.004 -1534.004 -1534.004] (0.879)
Step: 6499, Reward: [-1623.023 -1623.023 -1623.023] [273.7801], Avg: [-1536.795 -1536.795 -1536.795] (0.878)
Step: 6549, Reward: [-1479.653 -1479.653 -1479.653] [197.9395], Avg: [-1537.869 -1537.869 -1537.869] (0.877)
Step: 6599, Reward: [-1588.173 -1588.173 -1588.173] [368.1884], Avg: [-1541.04 -1541.04 -1541.04] (0.876)
Step: 6649, Reward: [-1444.902 -1444.902 -1444.902] [60.9345], Avg: [-1540.775 -1540.775 -1540.775] (0.875)
Step: 6699, Reward: [-1363.045 -1363.045 -1363.045] [173.5169], Avg: [-1540.744 -1540.744 -1540.744] (0.875)
Step: 6749, Reward: [-1514.331 -1514.331 -1514.331] [242.8886], Avg: [-1542.347 -1542.347 -1542.347] (0.874)
Step: 6799, Reward: [-1563.302 -1563.302 -1563.302] [126.1994], Avg: [-1543.429 -1543.429 -1543.429] (0.873)
Step: 6849, Reward: [-1481.126 -1481.126 -1481.126] [131.1064], Avg: [-1543.931 -1543.931 -1543.931] (0.872)
Step: 6899, Reward: [-1432.209 -1432.209 -1432.209] [155.7578], Avg: [-1544.25 -1544.25 -1544.25] (0.871)
Step: 6949, Reward: [-1590.173 -1590.173 -1590.173] [189.8786], Avg: [-1545.947 -1545.947 -1545.947] (0.870)
Step: 6999, Reward: [-1537.958 -1537.958 -1537.958] [282.4749], Avg: [-1547.907 -1547.907 -1547.907] (0.869)
Step: 7049, Reward: [-1510.34 -1510.34 -1510.34] [157.0564], Avg: [-1548.755 -1548.755 -1548.755] (0.868)
Step: 7099, Reward: [-1562.115 -1562.115 -1562.115] [219.0141], Avg: [-1550.391 -1550.391 -1550.391] (0.868)
Step: 7149, Reward: [-1560.206 -1560.206 -1560.206] [215.4442], Avg: [-1551.967 -1551.967 -1551.967] (0.867)
Step: 7199, Reward: [-1356.85 -1356.85 -1356.85] [266.8969], Avg: [-1552.465 -1552.465 -1552.465] (0.866)
Step: 7249, Reward: [-1529.748 -1529.748 -1529.748] [188.4480], Avg: [-1553.608 -1553.608 -1553.608] (0.865)
Step: 7299, Reward: [-1376.623 -1376.623 -1376.623] [188.5326], Avg: [-1553.687 -1553.687 -1553.687] (0.864)
Step: 7349, Reward: [-1545.089 -1545.089 -1545.089] [165.6202], Avg: [-1554.755 -1554.755 -1554.755] (0.863)
Step: 7399, Reward: [-1628.454 -1628.454 -1628.454] [172.5207], Avg: [-1556.419 -1556.419 -1556.419] (0.862)
Step: 7449, Reward: [-1537.026 -1537.026 -1537.026] [132.6270], Avg: [-1557.179 -1557.179 -1557.179] (0.862)
Step: 7499, Reward: [-1483.843 -1483.843 -1483.843] [247.0000], Avg: [-1558.337 -1558.337 -1558.337] (0.861)
Step: 7549, Reward: [-1415.473 -1415.473 -1415.473] [148.4130], Avg: [-1558.373 -1558.373 -1558.373] (0.860)
Step: 7599, Reward: [-1368.585 -1368.585 -1368.585] [108.3064], Avg: [-1557.837 -1557.837 -1557.837] (0.859)
Step: 7649, Reward: [-1583.238 -1583.238 -1583.238] [99.0783], Avg: [-1558.651 -1558.651 -1558.651] (0.858)
Step: 7699, Reward: [-1317.263 -1317.263 -1317.263] [188.7700], Avg: [-1558.309 -1558.309 -1558.309] (0.857)
Step: 7749, Reward: [-1436.358 -1436.358 -1436.358] [156.2652], Avg: [-1558.531 -1558.531 -1558.531] (0.856)
Step: 7799, Reward: [-1631.296 -1631.296 -1631.296] [277.7806], Avg: [-1560.778 -1560.778 -1560.778] (0.855)
Step: 7849, Reward: [-1430.444 -1430.444 -1430.444] [238.0691], Avg: [-1561.464 -1561.464 -1561.464] (0.855)
Step: 7899, Reward: [-1480.496 -1480.496 -1480.496] [200.0029], Avg: [-1562.217 -1562.217 -1562.217] (0.854)
Step: 7949, Reward: [-1458.286 -1458.286 -1458.286] [205.8582], Avg: [-1562.858 -1562.858 -1562.858] (0.853)
Step: 7999, Reward: [-1490.062 -1490.062 -1490.062] [215.1195], Avg: [-1563.748 -1563.748 -1563.748] (0.852)
Step: 8049, Reward: [-1462.46 -1462.46 -1462.46] [289.1324], Avg: [-1564.915 -1564.915 -1564.915] (0.851)
Step: 8099, Reward: [-1740.674 -1740.674 -1740.674] [222.2390], Avg: [-1567.371 -1567.371 -1567.371] (0.850)
Step: 8149, Reward: [-1478.017 -1478.017 -1478.017] [264.3821], Avg: [-1568.445 -1568.445 -1568.445] (0.850)
Step: 8199, Reward: [-1421.225 -1421.225 -1421.225] [229.1286], Avg: [-1568.945 -1568.945 -1568.945] (0.849)
Step: 8249, Reward: [-1469.524 -1469.524 -1469.524] [232.1126], Avg: [-1569.749 -1569.749 -1569.749] (0.848)
Step: 8299, Reward: [-1246.735 -1246.735 -1246.735] [198.7319], Avg: [-1569. -1569. -1569.] (0.847)
Step: 8349, Reward: [-1458.855 -1458.855 -1458.855] [395.7970], Avg: [-1570.711 -1570.711 -1570.711] (0.846)
Step: 8399, Reward: [-1344.082 -1344.082 -1344.082] [280.6605], Avg: [-1571.032 -1571.032 -1571.032] (0.845)
Step: 8449, Reward: [-1248.514 -1248.514 -1248.514] [216.5860], Avg: [-1570.405 -1570.405 -1570.405] (0.844)
Step: 8499, Reward: [-1437.122 -1437.122 -1437.122] [110.9319], Avg: [-1570.274 -1570.274 -1570.274] (0.844)
Step: 8549, Reward: [-1474.089 -1474.089 -1474.089] [203.5088], Avg: [-1570.902 -1570.902 -1570.902] (0.843)
Step: 8599, Reward: [-1464.761 -1464.761 -1464.761] [125.6765], Avg: [-1571.015 -1571.015 -1571.015] (0.842)
Step: 8649, Reward: [-1412.302 -1412.302 -1412.302] [253.4831], Avg: [-1571.563 -1571.563 -1571.563] (0.841)
Step: 8699, Reward: [-1602.536 -1602.536 -1602.536] [319.7456], Avg: [-1573.579 -1573.579 -1573.579] (0.840)
Step: 8749, Reward: [-1614.776 -1614.776 -1614.776] [156.8767], Avg: [-1574.71 -1574.71 -1574.71] (0.839)
Step: 8799, Reward: [-1447.613 -1447.613 -1447.613] [92.6092], Avg: [-1574.515 -1574.515 -1574.515] (0.839)
Step: 8849, Reward: [-1741.811 -1741.811 -1741.811] [200.2511], Avg: [-1576.591 -1576.591 -1576.591] (0.838)
Step: 8899, Reward: [-1449.854 -1449.854 -1449.854] [216.5634], Avg: [-1577.096 -1577.096 -1577.096] (0.837)
Step: 8949, Reward: [-1556.455 -1556.455 -1556.455] [373.5373], Avg: [-1579.067 -1579.067 -1579.067] (0.836)
Step: 8999, Reward: [-1394.552 -1394.552 -1394.552] [160.7476], Avg: [-1578.935 -1578.935 -1578.935] (0.835)
Step: 9049, Reward: [-1678.209 -1678.209 -1678.209] [124.0833], Avg: [-1580.169 -1580.169 -1580.169] (0.834)
Step: 9099, Reward: [-1572.762 -1572.762 -1572.762] [133.0935], Avg: [-1580.86 -1580.86 -1580.86] (0.834)
Step: 9149, Reward: [-1475.799 -1475.799 -1475.799] [279.0667], Avg: [-1581.811 -1581.811 -1581.811] (0.833)
Step: 9199, Reward: [-1238.039 -1238.039 -1238.039] [160.7667], Avg: [-1580.816 -1580.816 -1580.816] (0.832)
Step: 9249, Reward: [-1246.556 -1246.556 -1246.556] [185.4851], Avg: [-1580.012 -1580.012 -1580.012] (0.831)
Step: 9299, Reward: [-1507.178 -1507.178 -1507.178] [143.2956], Avg: [-1580.391 -1580.391 -1580.391] (0.830)
Step: 9349, Reward: [-1451.231 -1451.231 -1451.231] [189.9800], Avg: [-1580.716 -1580.716 -1580.716] (0.829)
Step: 9399, Reward: [-1425.512 -1425.512 -1425.512] [259.0807], Avg: [-1581.268 -1581.268 -1581.268] (0.829)
Step: 9449, Reward: [-1415.814 -1415.814 -1415.814] [68.3176], Avg: [-1580.754 -1580.754 -1580.754] (0.828)
Step: 9499, Reward: [-1498.097 -1498.097 -1498.097] [289.8539], Avg: [-1581.845 -1581.845 -1581.845] (0.827)
Step: 9549, Reward: [-1609.516 -1609.516 -1609.516] [223.7809], Avg: [-1583.161 -1583.161 -1583.161] (0.826)
Step: 9599, Reward: [-1452.014 -1452.014 -1452.014] [208.3812], Avg: [-1583.564 -1583.564 -1583.564] (0.825)
Step: 9649, Reward: [-1513.443 -1513.443 -1513.443] [129.6874], Avg: [-1583.872 -1583.872 -1583.872] (0.824)
Step: 9699, Reward: [-1567.136 -1567.136 -1567.136] [175.0992], Avg: [-1584.689 -1584.689 -1584.689] (0.824)
Step: 9749, Reward: [-1774.423 -1774.423 -1774.423] [227.5815], Avg: [-1586.829 -1586.829 -1586.829] (0.823)
Step: 9799, Reward: [-1584.413 -1584.413 -1584.413] [314.0330], Avg: [-1588.419 -1588.419 -1588.419] (0.822)
Step: 9849, Reward: [-1564.065 -1564.065 -1564.065] [257.2006], Avg: [-1589.601 -1589.601 -1589.601] (0.821)
Step: 9899, Reward: [-1618.884 -1618.884 -1618.884] [300.6859], Avg: [-1591.267 -1591.267 -1591.267] (0.820)
Step: 9949, Reward: [-1775.752 -1775.752 -1775.752] [122.1922], Avg: [-1592.808 -1592.808 -1592.808] (0.819)
Step: 9999, Reward: [-1625.569 -1625.569 -1625.569] [154.5721], Avg: [-1593.745 -1593.745 -1593.745] (0.819)
Step: 10049, Reward: [-1622.521 -1622.521 -1622.521] [240.5977], Avg: [-1595.085 -1595.085 -1595.085] (0.818)
Step: 10099, Reward: [-1666.474 -1666.474 -1666.474] [161.4603], Avg: [-1596.238 -1596.238 -1596.238] (0.817)
Step: 10149, Reward: [-1699.712 -1699.712 -1699.712] [137.8275], Avg: [-1597.426 -1597.426 -1597.426] (0.816)
Step: 10199, Reward: [-1832.003 -1832.003 -1832.003] [157.5274], Avg: [-1599.349 -1599.349 -1599.349] (0.815)
Step: 10249, Reward: [-1680.095 -1680.095 -1680.095] [245.3252], Avg: [-1600.939 -1600.939 -1600.939] (0.815)
Step: 10299, Reward: [-1740.529 -1740.529 -1740.529] [179.9795], Avg: [-1602.49 -1602.49 -1602.49] (0.814)
Step: 10349, Reward: [-1717.542 -1717.542 -1717.542] [109.1888], Avg: [-1603.574 -1603.574 -1603.574] (0.813)
Step: 10399, Reward: [-1676.795 -1676.795 -1676.795] [227.6613], Avg: [-1605.02 -1605.02 -1605.02] (0.812)
Step: 10449, Reward: [-1755.317 -1755.317 -1755.317] [282.0323], Avg: [-1607.089 -1607.089 -1607.089] (0.811)
Step: 10499, Reward: [-1809.422 -1809.422 -1809.422] [237.2680], Avg: [-1609.182 -1609.182 -1609.182] (0.810)
Step: 10549, Reward: [-1587.004 -1587.004 -1587.004] [191.4972], Avg: [-1609.985 -1609.985 -1609.985] (0.810)
Step: 10599, Reward: [-1879.532 -1879.532 -1879.532] [276.5518], Avg: [-1612.561 -1612.561 -1612.561] (0.809)
Step: 10649, Reward: [-1683.117 -1683.117 -1683.117] [360.8002], Avg: [-1614.586 -1614.586 -1614.586] (0.808)
Step: 10699, Reward: [-1815.763 -1815.763 -1815.763] [238.0286], Avg: [-1616.638 -1616.638 -1616.638] (0.807)
Step: 10749, Reward: [-1755.469 -1755.469 -1755.469] [135.6254], Avg: [-1617.915 -1617.915 -1617.915] (0.806)
Step: 10799, Reward: [-1666.505 -1666.505 -1666.505] [134.3241], Avg: [-1618.761 -1618.761 -1618.761] (0.806)
Step: 10849, Reward: [-1700.861 -1700.861 -1700.861] [158.6066], Avg: [-1619.871 -1619.871 -1619.871] (0.805)
Step: 10899, Reward: [-1751.774 -1751.774 -1751.774] [201.4506], Avg: [-1621.4 -1621.4 -1621.4] (0.804)
Step: 10949, Reward: [-1886.612 -1886.612 -1886.612] [283.9579], Avg: [-1623.907 -1623.907 -1623.907] (0.803)
Step: 10999, Reward: [-1817.492 -1817.492 -1817.492] [143.7188], Avg: [-1625.441 -1625.441 -1625.441] (0.802)
Step: 11049, Reward: [-1723.278 -1723.278 -1723.278] [273.1610], Avg: [-1627.119 -1627.119 -1627.119] (0.802)
Step: 11099, Reward: [-1852.606 -1852.606 -1852.606] [181.8481], Avg: [-1628.954 -1628.954 -1628.954] (0.801)
Step: 11149, Reward: [-1917.939 -1917.939 -1917.939] [200.8593], Avg: [-1631.151 -1631.151 -1631.151] (0.800)
Step: 11199, Reward: [-1716.803 -1716.803 -1716.803] [189.0408], Avg: [-1632.377 -1632.377 -1632.377] (0.799)
Step: 11249, Reward: [-1653.246 -1653.246 -1653.246] [135.3986], Avg: [-1633.072 -1633.072 -1633.072] (0.798)
Step: 11299, Reward: [-1693.414 -1693.414 -1693.414] [213.4404], Avg: [-1634.283 -1634.283 -1634.283] (0.798)
Step: 11349, Reward: [-1763.275 -1763.275 -1763.275] [201.4682], Avg: [-1635.739 -1635.739 -1635.739] (0.797)
Step: 11399, Reward: [-1723.59 -1723.59 -1723.59] [207.2698], Avg: [-1637.033 -1637.033 -1637.033] (0.796)
Step: 11449, Reward: [-1651.991 -1651.991 -1651.991] [68.3283], Avg: [-1637.397 -1637.397 -1637.397] (0.795)
Step: 11499, Reward: [-1687.274 -1687.274 -1687.274] [235.4294], Avg: [-1638.637 -1638.637 -1638.637] (0.794)
Step: 11549, Reward: [-1628.25 -1628.25 -1628.25] [173.7479], Avg: [-1639.345 -1639.345 -1639.345] (0.794)
Step: 11599, Reward: [-1714.46 -1714.46 -1714.46] [109.8839], Avg: [-1640.142 -1640.142 -1640.142] (0.793)
Step: 11649, Reward: [-1567.807 -1567.807 -1567.807] [176.1852], Avg: [-1640.588 -1640.588 -1640.588] (0.792)
Step: 11699, Reward: [-1522.04 -1522.04 -1522.04] [107.5120], Avg: [-1640.541 -1640.541 -1640.541] (0.791)
Step: 11749, Reward: [-1527.182 -1527.182 -1527.182] [108.4275], Avg: [-1640.52 -1640.52 -1640.52] (0.790)
Step: 11799, Reward: [-1367.964 -1367.964 -1367.964] [158.5104], Avg: [-1640.036 -1640.036 -1640.036] (0.790)
Step: 11849, Reward: [-1549.716 -1549.716 -1549.716] [132.6000], Avg: [-1640.215 -1640.215 -1640.215] (0.789)
Step: 11899, Reward: [-1727.583 -1727.583 -1727.583] [250.5100], Avg: [-1641.634 -1641.634 -1641.634] (0.788)
Step: 11949, Reward: [-1410.006 -1410.006 -1410.006] [153.3574], Avg: [-1641.307 -1641.307 -1641.307] (0.787)
Step: 11999, Reward: [-1450.008 -1450.008 -1450.008] [250.7087], Avg: [-1641.554 -1641.554 -1641.554] (0.787)
Step: 12049, Reward: [-1468.152 -1468.152 -1468.152] [146.8818], Avg: [-1641.444 -1641.444 -1641.444] (0.786)
Step: 12099, Reward: [-1355.794 -1355.794 -1355.794] [106.8589], Avg: [-1640.706 -1640.706 -1640.706] (0.785)
Step: 12149, Reward: [-1369.549 -1369.549 -1369.549] [196.0075], Avg: [-1640.396 -1640.396 -1640.396] (0.784)
Step: 12199, Reward: [-1575.402 -1575.402 -1575.402] [195.6762], Avg: [-1640.932 -1640.932 -1640.932] (0.783)
Step: 12249, Reward: [-1540.866 -1540.866 -1540.866] [150.0396], Avg: [-1641.136 -1641.136 -1641.136] (0.783)
Step: 12299, Reward: [-1431.062 -1431.062 -1431.062] [163.5420], Avg: [-1640.947 -1640.947 -1640.947] (0.782)
Step: 12349, Reward: [-1360.63 -1360.63 -1360.63] [225.1915], Avg: [-1640.724 -1640.724 -1640.724] (0.781)
Step: 12399, Reward: [-1609.112 -1609.112 -1609.112] [149.5448], Avg: [-1641.199 -1641.199 -1641.199] (0.780)
Step: 12449, Reward: [-1473.753 -1473.753 -1473.753] [214.0529], Avg: [-1641.386 -1641.386 -1641.386] (0.779)
Step: 12499, Reward: [-1381.999 -1381.999 -1381.999] [130.8078], Avg: [-1640.872 -1640.872 -1640.872] (0.779)
Step: 12549, Reward: [-1508.001 -1508.001 -1508.001] [308.1856], Avg: [-1641.57 -1641.57 -1641.57] (0.778)
Step: 12599, Reward: [-1410.747 -1410.747 -1410.747] [197.8668], Avg: [-1641.44 -1641.44 -1641.44] (0.777)
Step: 12649, Reward: [-1665.449 -1665.449 -1665.449] [303.7388], Avg: [-1642.735 -1642.735 -1642.735] (0.776)
Step: 12699, Reward: [-1648.235 -1648.235 -1648.235] [127.5414], Avg: [-1643.259 -1643.259 -1643.259] (0.776)
Step: 12749, Reward: [-1524.376 -1524.376 -1524.376] [118.5779], Avg: [-1643.258 -1643.258 -1643.258] (0.775)
Step: 12799, Reward: [-1574.444 -1574.444 -1574.444] [164.2776], Avg: [-1643.631 -1643.631 -1643.631] (0.774)
Step: 12849, Reward: [-1611.429 -1611.429 -1611.429] [191.8665], Avg: [-1644.252 -1644.252 -1644.252] (0.773)
Step: 12899, Reward: [-1794.781 -1794.781 -1794.781] [72.1891], Avg: [-1645.115 -1645.115 -1645.115] (0.772)
Step: 12949, Reward: [-1478.573 -1478.573 -1478.573] [261.6386], Avg: [-1645.482 -1645.482 -1645.482] (0.772)
Step: 12999, Reward: [-1425.158 -1425.158 -1425.158] [218.4619], Avg: [-1645.475 -1645.475 -1645.475] (0.771)
Step: 13049, Reward: [-1267.943 -1267.943 -1267.943] [198.7193], Avg: [-1644.79 -1644.79 -1644.79] (0.770)
Step: 13099, Reward: [-1507.237 -1507.237 -1507.237] [268.9333], Avg: [-1645.291 -1645.291 -1645.291] (0.769)
Step: 13149, Reward: [-1652.309 -1652.309 -1652.309] [212.2156], Avg: [-1646.125 -1646.125 -1646.125] (0.769)
Step: 13199, Reward: [-1478.186 -1478.186 -1478.186] [257.3181], Avg: [-1646.464 -1646.464 -1646.464] (0.768)
Step: 13249, Reward: [-1714.526 -1714.526 -1714.526] [305.2725], Avg: [-1647.872 -1647.872 -1647.872] (0.767)
Step: 13299, Reward: [-1654.687 -1654.687 -1654.687] [230.1156], Avg: [-1648.763 -1648.763 -1648.763] (0.766)
Step: 13349, Reward: [-1553.672 -1553.672 -1553.672] [227.9910], Avg: [-1649.261 -1649.261 -1649.261] (0.766)
Step: 13399, Reward: [-1587.723 -1587.723 -1587.723] [199.4092], Avg: [-1649.775 -1649.775 -1649.775] (0.765)
Step: 13449, Reward: [-1610.574 -1610.574 -1610.574] [160.4802], Avg: [-1650.226 -1650.226 -1650.226] (0.764)
Step: 13499, Reward: [-1580.281 -1580.281 -1580.281] [239.3507], Avg: [-1650.854 -1650.854 -1650.854] (0.763)
Step: 13549, Reward: [-1531.48 -1531.48 -1531.48] [167.8802], Avg: [-1651.033 -1651.033 -1651.033] (0.763)
Step: 13599, Reward: [-1414.485 -1414.485 -1414.485] [327.9777], Avg: [-1651.369 -1651.369 -1651.369] (0.762)
Step: 13649, Reward: [-1652.596 -1652.596 -1652.596] [208.2510], Avg: [-1652.136 -1652.136 -1652.136] (0.761)
Step: 13699, Reward: [-1391.425 -1391.425 -1391.425] [231.7272], Avg: [-1652.03 -1652.03 -1652.03] (0.760)
Step: 13749, Reward: [-1601.459 -1601.459 -1601.459] [115.1003], Avg: [-1652.265 -1652.265 -1652.265] (0.759)
Step: 13799, Reward: [-1595.645 -1595.645 -1595.645] [297.9687], Avg: [-1653.139 -1653.139 -1653.139] (0.759)
Step: 13849, Reward: [-1535.806 -1535.806 -1535.806] [213.5798], Avg: [-1653.487 -1653.487 -1653.487] (0.758)
Step: 13899, Reward: [-1635.362 -1635.362 -1635.362] [183.8078], Avg: [-1654.083 -1654.083 -1654.083] (0.757)
Step: 13949, Reward: [-1535.95 -1535.95 -1535.95] [365.9406], Avg: [-1654.971 -1654.971 -1654.971] (0.756)
Step: 13999, Reward: [-1643.322 -1643.322 -1643.322] [292.2076], Avg: [-1655.973 -1655.973 -1655.973] (0.756)
Step: 14049, Reward: [-1654.241 -1654.241 -1654.241] [297.2704], Avg: [-1657.025 -1657.025 -1657.025] (0.755)
Step: 14099, Reward: [-1597.749 -1597.749 -1597.749] [209.6408], Avg: [-1657.558 -1657.558 -1657.558] (0.754)
Step: 14149, Reward: [-1507.613 -1507.613 -1507.613] [300.4590], Avg: [-1658.09 -1658.09 -1658.09] (0.753)
Step: 14199, Reward: [-1430.694 -1430.694 -1430.694] [147.1252], Avg: [-1657.807 -1657.807 -1657.807] (0.753)
Step: 14249, Reward: [-1603.991 -1603.991 -1603.991] [181.0737], Avg: [-1658.254 -1658.254 -1658.254] (0.752)
Step: 14299, Reward: [-1581.666 -1581.666 -1581.666] [231.2197], Avg: [-1658.794 -1658.794 -1658.794] (0.751)
Step: 14349, Reward: [-1503.471 -1503.471 -1503.471] [146.2600], Avg: [-1658.763 -1658.763 -1658.763] (0.750)
Step: 14399, Reward: [-1506.813 -1506.813 -1506.813] [225.9389], Avg: [-1659.02 -1659.02 -1659.02] (0.750)
Step: 14449, Reward: [-1321.756 -1321.756 -1321.756] [186.6099], Avg: [-1658.498 -1658.498 -1658.498] (0.749)
Step: 14499, Reward: [-1572.083 -1572.083 -1572.083] [206.2298], Avg: [-1658.912 -1658.912 -1658.912] (0.748)
Step: 14549, Reward: [-1481.143 -1481.143 -1481.143] [244.6424], Avg: [-1659.141 -1659.141 -1659.141] (0.747)
Step: 14599, Reward: [-1355.958 -1355.958 -1355.958] [361.2663], Avg: [-1659.34 -1659.34 -1659.34] (0.747)
Step: 14649, Reward: [-1478.284 -1478.284 -1478.284] [225.6461], Avg: [-1659.492 -1659.492 -1659.492] (0.746)
Step: 14699, Reward: [-1535.291 -1535.291 -1535.291] [228.3816], Avg: [-1659.847 -1659.847 -1659.847] (0.745)
Step: 14749, Reward: [-1264.27 -1264.27 -1264.27] [188.0670], Avg: [-1659.143 -1659.143 -1659.143] (0.744)
Step: 14799, Reward: [-1772.786 -1772.786 -1772.786] [286.4653], Avg: [-1660.495 -1660.495 -1660.495] (0.744)
Step: 14849, Reward: [-1739.206 -1739.206 -1739.206] [179.7841], Avg: [-1661.365 -1661.365 -1661.365] (0.743)
Step: 14899, Reward: [-1540.814 -1540.814 -1540.814] [104.6734], Avg: [-1661.312 -1661.312 -1661.312] (0.742)
Step: 14949, Reward: [-1669.51 -1669.51 -1669.51] [272.0762], Avg: [-1662.25 -1662.25 -1662.25] (0.741)
Step: 14999, Reward: [-1589.503 -1589.503 -1589.503] [161.3672], Avg: [-1662.545 -1662.545 -1662.545] (0.741)
Step: 15049, Reward: [-1455.576 -1455.576 -1455.576] [305.5508], Avg: [-1662.872 -1662.872 -1662.872] (0.740)
Step: 15099, Reward: [-1563.906 -1563.906 -1563.906] [154.5362], Avg: [-1663.056 -1663.056 -1663.056] (0.739)
Step: 15149, Reward: [-1661.239 -1661.239 -1661.239] [251.7784], Avg: [-1663.881 -1663.881 -1663.881] (0.738)
Step: 15199, Reward: [-1410.403 -1410.403 -1410.403] [212.2536], Avg: [-1663.746 -1663.746 -1663.746] (0.738)
Step: 15249, Reward: [-1389.579 -1389.579 -1389.579] [406.4412], Avg: [-1664.179 -1664.179 -1664.179] (0.737)
Step: 15299, Reward: [-1545.02 -1545.02 -1545.02] [320.0320], Avg: [-1664.836 -1664.836 -1664.836] (0.736)
Step: 15349, Reward: [-1487.05 -1487.05 -1487.05] [251.9253], Avg: [-1665.077 -1665.077 -1665.077] (0.736)
Step: 15399, Reward: [-1446.639 -1446.639 -1446.639] [310.5632], Avg: [-1665.377 -1665.377 -1665.377] (0.735)
Step: 15449, Reward: [-1495.847 -1495.847 -1495.847] [360.6880], Avg: [-1665.995 -1665.995 -1665.995] (0.734)
Step: 15499, Reward: [-1489.05 -1489.05 -1489.05] [259.0744], Avg: [-1666.26 -1666.26 -1666.26] (0.733)
Step: 15549, Reward: [-1514.665 -1514.665 -1514.665] [292.8292], Avg: [-1666.714 -1666.714 -1666.714] (0.733)
Step: 15599, Reward: [-1596.065 -1596.065 -1596.065] [184.5676], Avg: [-1667.079 -1667.079 -1667.079] (0.732)
Step: 15649, Reward: [-1606.546 -1606.546 -1606.546] [208.8544], Avg: [-1667.553 -1667.553 -1667.553] (0.731)
Step: 15699, Reward: [-1402.999 -1402.999 -1402.999] [284.3470], Avg: [-1667.616 -1667.616 -1667.616] (0.730)
Step: 15749, Reward: [-1440.512 -1440.512 -1440.512] [366.3796], Avg: [-1668.058 -1668.058 -1668.058] (0.730)
Step: 15799, Reward: [-1216.398 -1216.398 -1216.398] [170.5931], Avg: [-1667.169 -1667.169 -1667.169] (0.729)
Step: 15849, Reward: [-1382.872 -1382.872 -1382.872] [205.0574], Avg: [-1666.919 -1666.919 -1666.919] (0.728)
Step: 15899, Reward: [-1590.497 -1590.497 -1590.497] [310.0184], Avg: [-1667.654 -1667.654 -1667.654] (0.727)
Step: 15949, Reward: [-1469.587 -1469.587 -1469.587] [459.4238], Avg: [-1668.473 -1668.473 -1668.473] (0.727)
Step: 15999, Reward: [-1517.445 -1517.445 -1517.445] [254.3430], Avg: [-1668.796 -1668.796 -1668.796] (0.726)
Step: 16049, Reward: [-1669.216 -1669.216 -1669.216] [403.9620], Avg: [-1670.055 -1670.055 -1670.055] (0.725)
Step: 16099, Reward: [-1719.843 -1719.843 -1719.843] [330.9129], Avg: [-1671.238 -1671.238 -1671.238] (0.725)
Step: 16149, Reward: [-1741.022 -1741.022 -1741.022] [176.7258], Avg: [-1672.001 -1672.001 -1672.001] (0.724)
Step: 16199, Reward: [-1555.275 -1555.275 -1555.275] [383.9152], Avg: [-1672.826 -1672.826 -1672.826] (0.723)
Step: 16249, Reward: [-1598.559 -1598.559 -1598.559] [350.4759], Avg: [-1673.675 -1673.675 -1673.675] (0.722)
Step: 16299, Reward: [-1505.332 -1505.332 -1505.332] [126.8720], Avg: [-1673.548 -1673.548 -1673.548] (0.722)
Step: 16349, Reward: [-1506.1 -1506.1 -1506.1] [304.0834], Avg: [-1673.966 -1673.966 -1673.966] (0.721)
Step: 16399, Reward: [-1584.612 -1584.612 -1584.612] [222.4281], Avg: [-1674.372 -1674.372 -1674.372] (0.720)
Step: 16449, Reward: [-1443.382 -1443.382 -1443.382] [238.2902], Avg: [-1674.394 -1674.394 -1674.394] (0.720)
Step: 16499, Reward: [-1631.327 -1631.327 -1631.327] [285.1374], Avg: [-1675.128 -1675.128 -1675.128] (0.719)
Step: 16549, Reward: [-1338.888 -1338.888 -1338.888] [271.8549], Avg: [-1674.933 -1674.933 -1674.933] (0.718)
Step: 16599, Reward: [-1260.427 -1260.427 -1260.427] [292.3529], Avg: [-1674.565 -1674.565 -1674.565] (0.717)
Step: 16649, Reward: [-1432.337 -1432.337 -1432.337] [292.2978], Avg: [-1674.715 -1674.715 -1674.715] (0.717)
Step: 16699, Reward: [-1193.735 -1193.735 -1193.735] [253.7300], Avg: [-1674.035 -1674.035 -1674.035] (0.716)
Step: 16749, Reward: [-1503.818 -1503.818 -1503.818] [189.2187], Avg: [-1674.092 -1674.092 -1674.092] (0.715)
Step: 16799, Reward: [-1626.141 -1626.141 -1626.141] [421.8834], Avg: [-1675.205 -1675.205 -1675.205] (0.715)
Step: 16849, Reward: [-1519.495 -1519.495 -1519.495] [246.7259], Avg: [-1675.475 -1675.475 -1675.475] (0.714)
Step: 16899, Reward: [-1570.916 -1570.916 -1570.916] [309.4033], Avg: [-1676.081 -1676.081 -1676.081] (0.713)
Step: 16949, Reward: [-1535.54 -1535.54 -1535.54] [416.0788], Avg: [-1676.894 -1676.894 -1676.894] (0.712)
Step: 16999, Reward: [-1568.341 -1568.341 -1568.341] [399.5658], Avg: [-1677.75 -1677.75 -1677.75] (0.712)
Step: 17049, Reward: [-1583.185 -1583.185 -1583.185] [324.1273], Avg: [-1678.423 -1678.423 -1678.423] (0.711)
Step: 17099, Reward: [-1443.36 -1443.36 -1443.36] [426.2604], Avg: [-1678.982 -1678.982 -1678.982] (0.710)
Step: 17149, Reward: [-1495.038 -1495.038 -1495.038] [196.0920], Avg: [-1679.017 -1679.017 -1679.017] (0.710)
Step: 17199, Reward: [-1732.716 -1732.716 -1732.716] [238.5608], Avg: [-1679.867 -1679.867 -1679.867] (0.709)
Step: 17249, Reward: [-1399.843 -1399.843 -1399.843] [361.0593], Avg: [-1680.102 -1680.102 -1680.102] (0.708)
Step: 17299, Reward: [-1460.958 -1460.958 -1460.958] [261.3731], Avg: [-1680.224 -1680.224 -1680.224] (0.707)
Step: 17349, Reward: [-1588.853 -1588.853 -1588.853] [348.1472], Avg: [-1680.964 -1680.964 -1680.964] (0.707)
Step: 17399, Reward: [-1602.05 -1602.05 -1602.05] [180.7614], Avg: [-1681.256 -1681.256 -1681.256] (0.706)
Step: 17449, Reward: [-1459.768 -1459.768 -1459.768] [232.5337], Avg: [-1681.288 -1681.288 -1681.288] (0.705)
Step: 17499, Reward: [-1530.746 -1530.746 -1530.746] [204.4227], Avg: [-1681.442 -1681.442 -1681.442] (0.705)
Step: 17549, Reward: [-1700.225 -1700.225 -1700.225] [246.2763], Avg: [-1682.197 -1682.197 -1682.197] (0.704)
Step: 17599, Reward: [-1513.917 -1513.917 -1513.917] [136.7551], Avg: [-1682.108 -1682.108 -1682.108] (0.703)
Step: 17649, Reward: [-1523.381 -1523.381 -1523.381] [114.0286], Avg: [-1681.981 -1681.981 -1681.981] (0.702)
Step: 17699, Reward: [-1488.281 -1488.281 -1488.281] [310.0985], Avg: [-1682.31 -1682.31 -1682.31] (0.702)
Step: 17749, Reward: [-1344.423 -1344.423 -1344.423] [202.4371], Avg: [-1681.928 -1681.928 -1681.928] (0.701)
Step: 17799, Reward: [-1488.776 -1488.776 -1488.776] [318.5524], Avg: [-1682.28 -1682.28 -1682.28] (0.700)
Step: 17849, Reward: [-1530.099 -1530.099 -1530.099] [373.4991], Avg: [-1682.9 -1682.9 -1682.9] (0.700)
Step: 17899, Reward: [-1410.886 -1410.886 -1410.886] [333.3839], Avg: [-1683.072 -1683.072 -1683.072] (0.699)
Step: 17949, Reward: [-1557.471 -1557.471 -1557.471] [232.6439], Avg: [-1683.37 -1683.37 -1683.37] (0.698)
Step: 17999, Reward: [-1561.567 -1561.567 -1561.567] [256.9645], Avg: [-1683.745 -1683.745 -1683.745] (0.698)
Step: 18049, Reward: [-1396.601 -1396.601 -1396.601] [227.3062], Avg: [-1683.58 -1683.58 -1683.58] (0.697)
Step: 18099, Reward: [-1710.511 -1710.511 -1710.511] [185.9854], Avg: [-1684.168 -1684.168 -1684.168] (0.696)
Step: 18149, Reward: [-1529.057 -1529.057 -1529.057] [218.0005], Avg: [-1684.341 -1684.341 -1684.341] (0.695)
Step: 18199, Reward: [-1550.034 -1550.034 -1550.034] [182.7751], Avg: [-1684.474 -1684.474 -1684.474] (0.695)
Step: 18249, Reward: [-1665.181 -1665.181 -1665.181] [322.9454], Avg: [-1685.306 -1685.306 -1685.306] (0.694)
Step: 18299, Reward: [-1457.548 -1457.548 -1457.548] [219.2221], Avg: [-1685.283 -1685.283 -1685.283] (0.693)
Step: 18349, Reward: [-1700.044 -1700.044 -1700.044] [325.9479], Avg: [-1686.211 -1686.211 -1686.211] (0.693)
Step: 18399, Reward: [-1524.641 -1524.641 -1524.641] [352.4254], Avg: [-1686.73 -1686.73 -1686.73] (0.692)
Step: 18449, Reward: [-1896.183 -1896.183 -1896.183] [242.4215], Avg: [-1687.954 -1687.954 -1687.954] (0.691)
Step: 18499, Reward: [-1802.593 -1802.593 -1802.593] [203.9615], Avg: [-1688.816 -1688.816 -1688.816] (0.691)
Step: 18549, Reward: [-1780.537 -1780.537 -1780.537] [94.3544], Avg: [-1689.317 -1689.317 -1689.317] (0.690)
Step: 18599, Reward: [-1442.266 -1442.266 -1442.266] [225.8585], Avg: [-1689.26 -1689.26 -1689.26] (0.689)
Step: 18649, Reward: [-1582.715 -1582.715 -1582.715] [343.3433], Avg: [-1689.895 -1689.895 -1689.895] (0.689)
Step: 18699, Reward: [-1471.887 -1471.887 -1471.887] [284.3064], Avg: [-1690.072 -1690.072 -1690.072] (0.688)
Step: 18749, Reward: [-1686.232 -1686.232 -1686.232] [263.0544], Avg: [-1690.763 -1690.763 -1690.763] (0.687)
Step: 18799, Reward: [-1624.968 -1624.968 -1624.968] [158.1515], Avg: [-1691.009 -1691.009 -1691.009] (0.686)
Step: 18849, Reward: [-1806.678 -1806.678 -1806.678] [98.7241], Avg: [-1691.578 -1691.578 -1691.578] (0.686)
Step: 18899, Reward: [-1277.702 -1277.702 -1277.702] [197.4540], Avg: [-1691.005 -1691.005 -1691.005] (0.685)
Step: 18949, Reward: [-1686.719 -1686.719 -1686.719] [343.4401], Avg: [-1691.9 -1691.9 -1691.9] (0.684)
Step: 18999, Reward: [-1510.976 -1510.976 -1510.976] [223.6878], Avg: [-1692.013 -1692.013 -1692.013] (0.684)
Step: 19049, Reward: [-1833.139 -1833.139 -1833.139] [344.0200], Avg: [-1693.286 -1693.286 -1693.286] (0.683)
Step: 19099, Reward: [-1746.392 -1746.392 -1746.392] [321.4171], Avg: [-1694.266 -1694.266 -1694.266] (0.682)
Step: 19149, Reward: [-1723.54 -1723.54 -1723.54] [152.9878], Avg: [-1694.742 -1694.742 -1694.742] (0.682)
Step: 19199, Reward: [-1864.878 -1864.878 -1864.878] [249.5350], Avg: [-1695.835 -1695.835 -1695.835] (0.681)
Step: 19249, Reward: [-1830.291 -1830.291 -1830.291] [238.8243], Avg: [-1696.805 -1696.805 -1696.805] (0.680)
Step: 19299, Reward: [-1750.986 -1750.986 -1750.986] [244.8642], Avg: [-1697.579 -1697.579 -1697.579] (0.680)
Step: 19349, Reward: [-1627.618 -1627.618 -1627.618] [138.2104], Avg: [-1697.756 -1697.756 -1697.756] (0.679)
Step: 19399, Reward: [-1487.479 -1487.479 -1487.479] [260.6444], Avg: [-1697.886 -1697.886 -1697.886] (0.678)
Step: 19449, Reward: [-1515.134 -1515.134 -1515.134] [204.2767], Avg: [-1697.941 -1697.941 -1697.941] (0.678)
Step: 19499, Reward: [-1829.099 -1829.099 -1829.099] [96.0920], Avg: [-1698.524 -1698.524 -1698.524] (0.677)
Step: 19549, Reward: [-1746.038 -1746.038 -1746.038] [521.0834], Avg: [-1699.978 -1699.978 -1699.978] (0.676)
Step: 19599, Reward: [-1781.951 -1781.951 -1781.951] [219.6415], Avg: [-1700.747 -1700.747 -1700.747] (0.676)
Step: 19649, Reward: [-1791.552 -1791.552 -1791.552] [241.6726], Avg: [-1701.593 -1701.593 -1701.593] (0.675)
Step: 19699, Reward: [-1757.846 -1757.846 -1757.846] [344.7939], Avg: [-1702.611 -1702.611 -1702.611] (0.674)
Step: 19749, Reward: [-1736.91 -1736.91 -1736.91] [191.9644], Avg: [-1703.184 -1703.184 -1703.184] (0.674)
Step: 19799, Reward: [-1859.775 -1859.775 -1859.775] [331.6578], Avg: [-1704.417 -1704.417 -1704.417] (0.673)
Step: 19849, Reward: [-1784.061 -1784.061 -1784.061] [325.4306], Avg: [-1705.437 -1705.437 -1705.437] (0.672)
Step: 19899, Reward: [-1765.607 -1765.607 -1765.607] [311.5304], Avg: [-1706.371 -1706.371 -1706.371] (0.672)
Step: 19949, Reward: [-1705.963 -1705.963 -1705.963] [191.3412], Avg: [-1706.85 -1706.85 -1706.85] (0.671)
Step: 19999, Reward: [-1659.368 -1659.368 -1659.368] [232.4553], Avg: [-1707.312 -1707.312 -1707.312] (0.670)
Step: 20049, Reward: [-1718.235 -1718.235 -1718.235] [161.5566], Avg: [-1707.742 -1707.742 -1707.742] (0.670)
Step: 20099, Reward: [-1746.981 -1746.981 -1746.981] [263.5906], Avg: [-1708.496 -1708.496 -1708.496] (0.669)
Step: 20149, Reward: [-1897.54 -1897.54 -1897.54] [231.2371], Avg: [-1709.539 -1709.539 -1709.539] (0.668)
Step: 20199, Reward: [-1862.518 -1862.518 -1862.518] [408.3603], Avg: [-1710.928 -1710.928 -1710.928] (0.668)
Step: 20249, Reward: [-1656.014 -1656.014 -1656.014] [252.4493], Avg: [-1711.416 -1711.416 -1711.416] (0.667)
Step: 20299, Reward: [-1755.684 -1755.684 -1755.684] [138.3151], Avg: [-1711.865 -1711.865 -1711.865] (0.666)
Step: 20349, Reward: [-1629.898 -1629.898 -1629.898] [358.5390], Avg: [-1712.545 -1712.545 -1712.545] (0.666)
Step: 20399, Reward: [-1678.01 -1678.01 -1678.01] [199.3919], Avg: [-1712.949 -1712.949 -1712.949] (0.665)
Step: 20449, Reward: [-1612.817 -1612.817 -1612.817] [296.4109], Avg: [-1713.429 -1713.429 -1713.429] (0.664)
Step: 20499, Reward: [-1629.525 -1629.525 -1629.525] [226.7731], Avg: [-1713.777 -1713.777 -1713.777] (0.664)
Step: 20549, Reward: [-1699.635 -1699.635 -1699.635] [215.1480], Avg: [-1714.266 -1714.266 -1714.266] (0.663)
Step: 20599, Reward: [-1712.983 -1712.983 -1712.983] [87.5319], Avg: [-1714.476 -1714.476 -1714.476] (0.662)
Step: 20649, Reward: [-1656.944 -1656.944 -1656.944] [158.8648], Avg: [-1714.721 -1714.721 -1714.721] (0.662)
Step: 20699, Reward: [-1691.075 -1691.075 -1691.075] [179.9095], Avg: [-1715.099 -1715.099 -1715.099] (0.661)
Step: 20749, Reward: [-1660.674 -1660.674 -1660.674] [281.1915], Avg: [-1715.645 -1715.645 -1715.645] (0.660)
Step: 20799, Reward: [-1800.89 -1800.89 -1800.89] [252.7523], Avg: [-1716.458 -1716.458 -1716.458] (0.660)
Step: 20849, Reward: [-1523.764 -1523.764 -1523.764] [305.4390], Avg: [-1716.728 -1716.728 -1716.728] (0.659)
Step: 20899, Reward: [-1662.175 -1662.175 -1662.175] [277.7154], Avg: [-1717.262 -1717.262 -1717.262] (0.658)
Step: 20949, Reward: [-1662.239 -1662.239 -1662.239] [158.2809], Avg: [-1717.508 -1717.508 -1717.508] (0.658)
Step: 20999, Reward: [-1824.587 -1824.587 -1824.587] [144.5553], Avg: [-1718.107 -1718.107 -1718.107] (0.657)
Step: 21049, Reward: [-1547.464 -1547.464 -1547.464] [141.6547], Avg: [-1718.038 -1718.038 -1718.038] (0.656)
Step: 21099, Reward: [-1653.071 -1653.071 -1653.071] [115.9451], Avg: [-1718.159 -1718.159 -1718.159] (0.656)
Step: 21149, Reward: [-1481.932 -1481.932 -1481.932] [247.8227], Avg: [-1718.187 -1718.187 -1718.187] (0.655)
Step: 21199, Reward: [-1491.957 -1491.957 -1491.957] [272.4450], Avg: [-1718.296 -1718.296 -1718.296] (0.654)
Step: 21249, Reward: [-1624.395 -1624.395 -1624.395] [108.7290], Avg: [-1718.331 -1718.331 -1718.331] (0.654)
Step: 21299, Reward: [-1528.793 -1528.793 -1528.793] [180.8163], Avg: [-1718.31 -1718.31 -1718.31] (0.653)
Step: 21349, Reward: [-1500.262 -1500.262 -1500.262] [218.8852], Avg: [-1718.312 -1718.312 -1718.312] (0.652)
Step: 21399, Reward: [-1525.505 -1525.505 -1525.505] [298.9337], Avg: [-1718.56 -1718.56 -1718.56] (0.652)
Step: 21449, Reward: [-1735.116 -1735.116 -1735.116] [362.2639], Avg: [-1719.443 -1719.443 -1719.443] (0.651)
Step: 21499, Reward: [-1595.406 -1595.406 -1595.406] [286.6157], Avg: [-1719.821 -1719.821 -1719.821] (0.650)
Step: 21549, Reward: [-1771.082 -1771.082 -1771.082] [238.6265], Avg: [-1720.494 -1720.494 -1720.494] (0.650)
Step: 21599, Reward: [-1651.791 -1651.791 -1651.791] [315.5058], Avg: [-1721.065 -1721.065 -1721.065] (0.649)
Step: 21649, Reward: [-1712.093 -1712.093 -1712.093] [478.5574], Avg: [-1722.15 -1722.15 -1722.15] (0.648)
Step: 21699, Reward: [-1746.868 -1746.868 -1746.868] [197.4217], Avg: [-1722.661 -1722.661 -1722.661] (0.648)
Step: 21749, Reward: [-1575.136 -1575.136 -1575.136] [176.0166], Avg: [-1722.727 -1722.727 -1722.727] (0.647)
Step: 21799, Reward: [-1638.089 -1638.089 -1638.089] [188.2723], Avg: [-1722.965 -1722.965 -1722.965] (0.646)
Step: 21849, Reward: [-1687.053 -1687.053 -1687.053] [276.7381], Avg: [-1723.516 -1723.516 -1723.516] (0.646)
Step: 21899, Reward: [-1603.653 -1603.653 -1603.653] [396.6673], Avg: [-1724.148 -1724.148 -1724.148] (0.645)
Step: 21949, Reward: [-1632.849 -1632.849 -1632.849] [272.2250], Avg: [-1724.56 -1724.56 -1724.56] (0.645)
Step: 21999, Reward: [-1570.448 -1570.448 -1570.448] [242.3718], Avg: [-1724.76 -1724.76 -1724.76] (0.644)
Step: 22049, Reward: [-1812.068 -1812.068 -1812.068] [232.2715], Avg: [-1725.485 -1725.485 -1725.485] (0.643)
Step: 22099, Reward: [-1867.468 -1867.468 -1867.468] [245.5020], Avg: [-1726.362 -1726.362 -1726.362] (0.643)
Step: 22149, Reward: [-1783.425 -1783.425 -1783.425] [203.9005], Avg: [-1726.951 -1726.951 -1726.951] (0.642)
Step: 22199, Reward: [-1646.867 -1646.867 -1646.867] [391.2132], Avg: [-1727.652 -1727.652 -1727.652] (0.641)
Step: 22249, Reward: [-1702.575 -1702.575 -1702.575] [183.8689], Avg: [-1728.008 -1728.008 -1728.008] (0.641)
Step: 22299, Reward: [-1740.718 -1740.718 -1740.718] [284.6092], Avg: [-1728.675 -1728.675 -1728.675] (0.640)
Step: 22349, Reward: [-1870.937 -1870.937 -1870.937] [178.9887], Avg: [-1729.394 -1729.394 -1729.394] (0.639)
Step: 22399, Reward: [-1679.625 -1679.625 -1679.625] [251.3443], Avg: [-1729.844 -1729.844 -1729.844] (0.639)
Step: 22449, Reward: [-1562.11 -1562.11 -1562.11] [282.7324], Avg: [-1730.1 -1730.1 -1730.1] (0.638)
Step: 22499, Reward: [-1717.504 -1717.504 -1717.504] [333.6757], Avg: [-1730.813 -1730.813 -1730.813] (0.637)
Step: 22549, Reward: [-1508.283 -1508.283 -1508.283] [196.2007], Avg: [-1730.755 -1730.755 -1730.755] (0.637)
Step: 22599, Reward: [-1817.184 -1817.184 -1817.184] [203.2287], Avg: [-1731.396 -1731.396 -1731.396] (0.636)
Step: 22649, Reward: [-1704.605 -1704.605 -1704.605] [350.0531], Avg: [-1732.109 -1732.109 -1732.109] (0.636)
Step: 22699, Reward: [-1677.111 -1677.111 -1677.111] [294.7376], Avg: [-1732.637 -1732.637 -1732.637] (0.635)
Step: 22749, Reward: [-1808.098 -1808.098 -1808.098] [246.0409], Avg: [-1733.344 -1733.344 -1733.344] (0.634)
Step: 22799, Reward: [-1736.669 -1736.669 -1736.669] [127.2632], Avg: [-1733.63 -1733.63 -1733.63] (0.634)
Step: 22849, Reward: [-1622.289 -1622.289 -1622.289] [370.8456], Avg: [-1734.198 -1734.198 -1734.198] (0.633)
Step: 22899, Reward: [-1542.511 -1542.511 -1542.511] [222.6978], Avg: [-1734.266 -1734.266 -1734.266] (0.632)
Step: 22949, Reward: [-1786.901 -1786.901 -1786.901] [170.4250], Avg: [-1734.752 -1734.752 -1734.752] (0.632)
Step: 22999, Reward: [-1679.775 -1679.775 -1679.775] [200.8446], Avg: [-1735.069 -1735.069 -1735.069] (0.631)
Step: 23049, Reward: [-1594.247 -1594.247 -1594.247] [263.6238], Avg: [-1735.335 -1735.335 -1735.335] (0.631)
Step: 23099, Reward: [-1726.544 -1726.544 -1726.544] [218.3302], Avg: [-1735.789 -1735.789 -1735.789] (0.630)
Step: 23149, Reward: [-1737.511 -1737.511 -1737.511] [292.8354], Avg: [-1736.425 -1736.425 -1736.425] (0.629)
Step: 23199, Reward: [-1786.733 -1786.733 -1786.733] [279.8367], Avg: [-1737.137 -1737.137 -1737.137] (0.629)
Step: 23249, Reward: [-1823.098 -1823.098 -1823.098] [417.9749], Avg: [-1738.22 -1738.22 -1738.22] (0.628)
Step: 23299, Reward: [-1745.252 -1745.252 -1745.252] [160.0687], Avg: [-1738.579 -1738.579 -1738.579] (0.627)
Step: 23349, Reward: [-1590.937 -1590.937 -1590.937] [200.1818], Avg: [-1738.691 -1738.691 -1738.691] (0.627)
Step: 23399, Reward: [-1757.856 -1757.856 -1757.856] [82.9797], Avg: [-1738.91 -1738.91 -1738.91] (0.626)
Step: 23449, Reward: [-1752.357 -1752.357 -1752.357] [261.0226], Avg: [-1739.495 -1739.495 -1739.495] (0.625)
Step: 23499, Reward: [-1671.372 -1671.372 -1671.372] [312.5534], Avg: [-1740.015 -1740.015 -1740.015] (0.625)
Step: 23549, Reward: [-1699.792 -1699.792 -1699.792] [157.8555], Avg: [-1740.265 -1740.265 -1740.265] (0.624)
Step: 23599, Reward: [-1661.439 -1661.439 -1661.439] [132.9747], Avg: [-1740.379 -1740.379 -1740.379] (0.624)
Step: 23649, Reward: [-1769.979 -1769.979 -1769.979] [180.4340], Avg: [-1740.823 -1740.823 -1740.823] (0.623)
Step: 23699, Reward: [-1699.749 -1699.749 -1699.749] [167.6195], Avg: [-1741.09 -1741.09 -1741.09] (0.622)
Step: 23749, Reward: [-1675.241 -1675.241 -1675.241] [167.3436], Avg: [-1741.304 -1741.304 -1741.304] (0.622)
Step: 23799, Reward: [-1606.976 -1606.976 -1606.976] [191.3207], Avg: [-1741.424 -1741.424 -1741.424] (0.621)
Step: 23849, Reward: [-1932.871 -1932.871 -1932.871] [307.1001], Avg: [-1742.469 -1742.469 -1742.469] (0.620)
Step: 23899, Reward: [-1898.924 -1898.924 -1898.924] [108.6468], Avg: [-1743.024 -1743.024 -1743.024] (0.620)
Step: 23949, Reward: [-1748.178 -1748.178 -1748.178] [234.7273], Avg: [-1743.524 -1743.524 -1743.524] (0.619)
Step: 23999, Reward: [-1620.436 -1620.436 -1620.436] [266.5537], Avg: [-1743.823 -1743.823 -1743.823] (0.619)
Step: 24049, Reward: [-1571.82 -1571.82 -1571.82] [223.6926], Avg: [-1743.931 -1743.931 -1743.931] (0.618)
Step: 24099, Reward: [-1624.889 -1624.889 -1624.889] [179.3203], Avg: [-1744.056 -1744.056 -1744.056] (0.617)
Step: 24149, Reward: [-1692.792 -1692.792 -1692.792] [146.1616], Avg: [-1744.252 -1744.252 -1744.252] (0.617)
Step: 24199, Reward: [-1565.482 -1565.482 -1565.482] [193.4468], Avg: [-1744.283 -1744.283 -1744.283] (0.616)
Step: 24249, Reward: [-1599.623 -1599.623 -1599.623] [51.6447], Avg: [-1744.091 -1744.091 -1744.091] (0.616)
Step: 24299, Reward: [-1801.86 -1801.86 -1801.86] [231.8822], Avg: [-1744.687 -1744.687 -1744.687] (0.615)
Step: 24349, Reward: [-1707.435 -1707.435 -1707.435] [190.3870], Avg: [-1745.001 -1745.001 -1745.001] (0.614)
Step: 24399, Reward: [-1558.178 -1558.178 -1558.178] [216.8631], Avg: [-1745.063 -1745.063 -1745.063] (0.614)
Step: 24449, Reward: [-1650.438 -1650.438 -1650.438] [144.0155], Avg: [-1745.164 -1745.164 -1745.164] (0.613)
Step: 24499, Reward: [-1724.757 -1724.757 -1724.757] [232.4993], Avg: [-1745.597 -1745.597 -1745.597] (0.612)
Step: 24549, Reward: [-1633.546 -1633.546 -1633.546] [246.8165], Avg: [-1745.871 -1745.871 -1745.871] (0.612)
Step: 24599, Reward: [-1814.786 -1814.786 -1814.786] [159.9372], Avg: [-1746.336 -1746.336 -1746.336] (0.611)
Step: 24649, Reward: [-1893.499 -1893.499 -1893.499] [131.5134], Avg: [-1746.902 -1746.902 -1746.902] (0.611)
Step: 24699, Reward: [-1878.026 -1878.026 -1878.026] [187.9903], Avg: [-1747.548 -1747.548 -1747.548] (0.610)
Step: 24749, Reward: [-1675.889 -1675.889 -1675.889] [272.6575], Avg: [-1747.954 -1747.954 -1747.954] (0.609)
Step: 24799, Reward: [-1778.684 -1778.684 -1778.684] [269.0017], Avg: [-1748.558 -1748.558 -1748.558] (0.609)
Step: 24849, Reward: [-1650.868 -1650.868 -1650.868] [325.2536], Avg: [-1749.016 -1749.016 -1749.016] (0.608)
Step: 24899, Reward: [-1731.807 -1731.807 -1731.807] [266.9073], Avg: [-1749.517 -1749.517 -1749.517] (0.608)
Step: 24949, Reward: [-1763.129 -1763.129 -1763.129] [178.3430], Avg: [-1749.902 -1749.902 -1749.902] (0.607)
Step: 24999, Reward: [-1739.423 -1739.423 -1739.423] [322.7716], Avg: [-1750.526 -1750.526 -1750.526] (0.606)
Step: 25049, Reward: [-1815.965 -1815.965 -1815.965] [66.4646], Avg: [-1750.79 -1750.79 -1750.79] (0.606)
Step: 25099, Reward: [-1502.115 -1502.115 -1502.115] [288.7960], Avg: [-1750.87 -1750.87 -1750.87] (0.605)
Step: 25149, Reward: [-1825.816 -1825.816 -1825.816] [90.8160], Avg: [-1751.199 -1751.199 -1751.199] (0.605)
Step: 25199, Reward: [-1552.102 -1552.102 -1552.102] [284.0250], Avg: [-1751.368 -1751.368 -1751.368] (0.604)
Step: 25249, Reward: [-1711.014 -1711.014 -1711.014] [193.1590], Avg: [-1751.67 -1751.67 -1751.67] (0.603)
Step: 25299, Reward: [-1779.538 -1779.538 -1779.538] [293.2124], Avg: [-1752.305 -1752.305 -1752.305] (0.603)
Step: 25349, Reward: [-1710.361 -1710.361 -1710.361] [42.5202], Avg: [-1752.306 -1752.306 -1752.306] (0.602)
Step: 25399, Reward: [-1478.073 -1478.073 -1478.073] [200.6258], Avg: [-1752.161 -1752.161 -1752.161] (0.602)
Step: 25449, Reward: [-1748.307 -1748.307 -1748.307] [199.3237], Avg: [-1752.545 -1752.545 -1752.545] (0.601)
Step: 25499, Reward: [-2010.396 -2010.396 -2010.396] [205.7020], Avg: [-1753.454 -1753.454 -1753.454] (0.600)
Step: 25549, Reward: [-1699.587 -1699.587 -1699.587] [194.6080], Avg: [-1753.729 -1753.729 -1753.729] (0.600)
Step: 25599, Reward: [-1576.499 -1576.499 -1576.499] [324.1400], Avg: [-1754.016 -1754.016 -1754.016] (0.599)
Step: 25649, Reward: [-1871.917 -1871.917 -1871.917] [134.0100], Avg: [-1754.507 -1754.507 -1754.507] (0.599)
Step: 25699, Reward: [-1784.809 -1784.809 -1784.809] [224.0040], Avg: [-1755.002 -1755.002 -1755.002] (0.598)
Step: 25749, Reward: [-1631.057 -1631.057 -1631.057] [184.8097], Avg: [-1755.12 -1755.12 -1755.12] (0.597)
Step: 25799, Reward: [-1690.161 -1690.161 -1690.161] [142.5599], Avg: [-1755.271 -1755.271 -1755.271] (0.597)
Step: 25849, Reward: [-1359.949 -1359.949 -1359.949] [282.5866], Avg: [-1755.053 -1755.053 -1755.053] (0.596)
Step: 25899, Reward: [-1645.715 -1645.715 -1645.715] [229.2522], Avg: [-1755.284 -1755.284 -1755.284] (0.596)
Step: 25949, Reward: [-1604.03 -1604.03 -1604.03] [160.2658], Avg: [-1755.302 -1755.302 -1755.302] (0.595)
Step: 25999, Reward: [-1521.029 -1521.029 -1521.029] [200.8055], Avg: [-1755.237 -1755.237 -1755.237] (0.594)
Step: 26049, Reward: [-1478.554 -1478.554 -1478.554] [171.9490], Avg: [-1755.036 -1755.036 -1755.036] (0.594)
Step: 26099, Reward: [-1590.552 -1590.552 -1590.552] [278.6120], Avg: [-1755.255 -1755.255 -1755.255] (0.593)
Step: 26149, Reward: [-1443.87 -1443.87 -1443.87] [305.8111], Avg: [-1755.244 -1755.244 -1755.244] (0.593)
Step: 26199, Reward: [-1541.82 -1541.82 -1541.82] [222.8535], Avg: [-1755.262 -1755.262 -1755.262] (0.592)
Step: 26249, Reward: [-1480.911 -1480.911 -1480.911] [166.4639], Avg: [-1755.057 -1755.057 -1755.057] (0.591)
Step: 26299, Reward: [-1752.407 -1752.407 -1752.407] [158.5088], Avg: [-1755.353 -1755.353 -1755.353] (0.591)
Step: 26349, Reward: [-1666.067 -1666.067 -1666.067] [303.4633], Avg: [-1755.759 -1755.759 -1755.759] (0.590)
Step: 26399, Reward: [-1707.892 -1707.892 -1707.892] [117.2790], Avg: [-1755.891 -1755.891 -1755.891] (0.590)
Step: 26449, Reward: [-1786.095 -1786.095 -1786.095] [210.2003], Avg: [-1756.345 -1756.345 -1756.345] (0.589)
Step: 26499, Reward: [-1488.868 -1488.868 -1488.868] [92.1514], Avg: [-1756.015 -1756.015 -1756.015] (0.588)
Step: 26549, Reward: [-1712.263 -1712.263 -1712.263] [110.4659], Avg: [-1756.14 -1756.14 -1756.14] (0.588)
Step: 26599, Reward: [-1882.019 -1882.019 -1882.019] [266.9007], Avg: [-1756.878 -1756.878 -1756.878] (0.587)
Step: 26649, Reward: [-1478.801 -1478.801 -1478.801] [239.0069], Avg: [-1756.805 -1756.805 -1756.805] (0.587)
Step: 26699, Reward: [-1705.904 -1705.904 -1705.904] [69.6625], Avg: [-1756.84 -1756.84 -1756.84] (0.586)
Step: 26749, Reward: [-1563.231 -1563.231 -1563.231] [258.4927], Avg: [-1756.962 -1756.962 -1756.962] (0.586)
Step: 26799, Reward: [-1557.573 -1557.573 -1557.573] [154.0759], Avg: [-1756.877 -1756.877 -1756.877] (0.585)
Step: 26849, Reward: [-1652.022 -1652.022 -1652.022] [171.3251], Avg: [-1757.001 -1757.001 -1757.001] (0.584)
Step: 26899, Reward: [-1864.203 -1864.203 -1864.203] [118.8521], Avg: [-1757.421 -1757.421 -1757.421] (0.584)
Step: 26949, Reward: [-1680.398 -1680.398 -1680.398] [153.3030], Avg: [-1757.562 -1757.562 -1757.562] (0.583)
Step: 26999, Reward: [-1681.927 -1681.927 -1681.927] [258.0855], Avg: [-1757.9 -1757.9 -1757.9] (0.583)
Step: 27049, Reward: [-1619.136 -1619.136 -1619.136] [183.0705], Avg: [-1757.982 -1757.982 -1757.982] (0.582)
Step: 27099, Reward: [-1513.068 -1513.068 -1513.068] [179.9868], Avg: [-1757.862 -1757.862 -1757.862] (0.581)
Step: 27149, Reward: [-1689.789 -1689.789 -1689.789] [131.0817], Avg: [-1757.979 -1757.979 -1757.979] (0.581)
Step: 27199, Reward: [-1632.315 -1632.315 -1632.315] [130.7426], Avg: [-1757.988 -1757.988 -1757.988] (0.580)
Step: 27249, Reward: [-1679.221 -1679.221 -1679.221] [242.7825], Avg: [-1758.289 -1758.289 -1758.289] (0.580)
Step: 27299, Reward: [-1837.996 -1837.996 -1837.996] [120.8082], Avg: [-1758.656 -1758.656 -1758.656] (0.579)
Step: 27349, Reward: [-1722.139 -1722.139 -1722.139] [280.8729], Avg: [-1759.103 -1759.103 -1759.103] (0.579)
Step: 27399, Reward: [-1445.68 -1445.68 -1445.68] [205.8014], Avg: [-1758.906 -1758.906 -1758.906] (0.578)
Step: 27449, Reward: [-1604.828 -1604.828 -1604.828] [125.2399], Avg: [-1758.854 -1758.854 -1758.854] (0.577)
Step: 27499, Reward: [-1570.81 -1570.81 -1570.81] [70.4756], Avg: [-1758.64 -1758.64 -1758.64] (0.577)
Step: 27549, Reward: [-1664.291 -1664.291 -1664.291] [146.0771], Avg: [-1758.734 -1758.734 -1758.734] (0.576)
Step: 27599, Reward: [-1557.113 -1557.113 -1557.113] [292.1754], Avg: [-1758.898 -1758.898 -1758.898] (0.576)
Step: 27649, Reward: [-1817.671 -1817.671 -1817.671] [172.5580], Avg: [-1759.316 -1759.316 -1759.316] (0.575)
Step: 27699, Reward: [-1564.686 -1564.686 -1564.686] [205.2143], Avg: [-1759.335 -1759.335 -1759.335] (0.574)
Step: 27749, Reward: [-1693.396 -1693.396 -1693.396] [265.1235], Avg: [-1759.694 -1759.694 -1759.694] (0.574)
Step: 27799, Reward: [-1646.447 -1646.447 -1646.447] [223.7312], Avg: [-1759.893 -1759.893 -1759.893] (0.573)
Step: 27849, Reward: [-1596.731 -1596.731 -1596.731] [266.0067], Avg: [-1760.078 -1760.078 -1760.078] (0.573)
Step: 27899, Reward: [-1627.563 -1627.563 -1627.563] [157.4446], Avg: [-1760.122 -1760.122 -1760.122] (0.572)
Step: 27949, Reward: [-1710.636 -1710.636 -1710.636] [150.9008], Avg: [-1760.304 -1760.304 -1760.304] (0.572)
Step: 27999, Reward: [-1756.389 -1756.389 -1756.389] [124.1206], Avg: [-1760.518 -1760.518 -1760.518] (0.571)
Step: 28049, Reward: [-1675.2 -1675.2 -1675.2] [179.6430], Avg: [-1760.687 -1760.687 -1760.687] (0.570)
Step: 28099, Reward: [-1729.328 -1729.328 -1729.328] [241.0652], Avg: [-1761.06 -1761.06 -1761.06] (0.570)
Step: 28149, Reward: [-1589.132 -1589.132 -1589.132] [235.9492], Avg: [-1761.173 -1761.173 -1761.173] (0.569)
Step: 28199, Reward: [-1676.312 -1676.312 -1676.312] [274.5269], Avg: [-1761.51 -1761.51 -1761.51] (0.569)
Step: 28249, Reward: [-1627.889 -1627.889 -1627.889] [177.4188], Avg: [-1761.587 -1761.587 -1761.587] (0.568)
Step: 28299, Reward: [-1647.46 -1647.46 -1647.46] [231.8350], Avg: [-1761.795 -1761.795 -1761.795] (0.568)
Step: 28349, Reward: [-1914.839 -1914.839 -1914.839] [116.4246], Avg: [-1762.27 -1762.27 -1762.27] (0.567)
Step: 28399, Reward: [-1829.035 -1829.035 -1829.035] [128.1537], Avg: [-1762.614 -1762.614 -1762.614] (0.566)
Step: 28449, Reward: [-1558.773 -1558.773 -1558.773] [260.1752], Avg: [-1762.713 -1762.713 -1762.713] (0.566)
Step: 28499, Reward: [-1608.211 -1608.211 -1608.211] [308.4069], Avg: [-1762.983 -1762.983 -1762.983] (0.565)
Step: 28549, Reward: [-1673.478 -1673.478 -1673.478] [265.1178], Avg: [-1763.29 -1763.29 -1763.29] (0.565)
Step: 28599, Reward: [-1856.193 -1856.193 -1856.193] [267.8746], Avg: [-1763.921 -1763.921 -1763.921] (0.564)
Step: 28649, Reward: [-1770.408 -1770.408 -1770.408] [215.6851], Avg: [-1764.309 -1764.309 -1764.309] (0.564)
Step: 28699, Reward: [-1939.763 -1939.763 -1939.763] [178.1101], Avg: [-1764.925 -1764.925 -1764.925] (0.563)
Step: 28749, Reward: [-1675.738 -1675.738 -1675.738] [168.3658], Avg: [-1765.062 -1765.062 -1765.062] (0.563)
Step: 28799, Reward: [-1766.308 -1766.308 -1766.308] [160.9146], Avg: [-1765.344 -1765.344 -1765.344] (0.562)
Step: 28849, Reward: [-1646.486 -1646.486 -1646.486] [84.3037], Avg: [-1765.284 -1765.284 -1765.284] (0.561)
Step: 28899, Reward: [-1743.747 -1743.747 -1743.747] [168.6419], Avg: [-1765.538 -1765.538 -1765.538] (0.561)
Step: 28949, Reward: [-1926.908 -1926.908 -1926.908] [78.5574], Avg: [-1765.953 -1765.953 -1765.953] (0.560)
Step: 28999, Reward: [-1756.803 -1756.803 -1756.803] [292.4296], Avg: [-1766.441 -1766.441 -1766.441] (0.560)
Step: 29049, Reward: [-1866.428 -1866.428 -1866.428] [261.1000], Avg: [-1767.063 -1767.063 -1767.063] (0.559)
Step: 29099, Reward: [-1911.066 -1911.066 -1911.066] [175.0570], Avg: [-1767.611 -1767.611 -1767.611] (0.559)
Step: 29149, Reward: [-1673.191 -1673.191 -1673.191] [189.6195], Avg: [-1767.774 -1767.774 -1767.774] (0.558)
Step: 29199, Reward: [-1853.526 -1853.526 -1853.526] [117.2674], Avg: [-1768.122 -1768.122 -1768.122] (0.558)
Step: 29249, Reward: [-1863.728 -1863.728 -1863.728] [139.5546], Avg: [-1768.524 -1768.524 -1768.524] (0.557)
Step: 29299, Reward: [-1896.872 -1896.872 -1896.872] [208.5917], Avg: [-1769.099 -1769.099 -1769.099] (0.556)
Step: 29349, Reward: [-1798.971 -1798.971 -1798.971] [80.7554], Avg: [-1769.287 -1769.287 -1769.287] (0.556)
Step: 29399, Reward: [-1827.631 -1827.631 -1827.631] [107.2635], Avg: [-1769.569 -1769.569 -1769.569] (0.555)
Step: 29449, Reward: [-2031.802 -2031.802 -2031.802] [147.7366], Avg: [-1770.265 -1770.265 -1770.265] (0.555)
Step: 29499, Reward: [-1885.091 -1885.091 -1885.091] [182.3065], Avg: [-1770.769 -1770.769 -1770.769] (0.554)
Step: 29549, Reward: [-1853.503 -1853.503 -1853.503] [116.9468], Avg: [-1771.106 -1771.106 -1771.106] (0.554)
Step: 29599, Reward: [-1850.486 -1850.486 -1850.486] [116.2624], Avg: [-1771.437 -1771.437 -1771.437] (0.553)
Step: 29649, Reward: [-1815.249 -1815.249 -1815.249] [196.9643], Avg: [-1771.843 -1771.843 -1771.843] (0.553)
Step: 29699, Reward: [-1943.996 -1943.996 -1943.996] [108.6112], Avg: [-1772.316 -1772.316 -1772.316] (0.552)
Step: 29749, Reward: [-1946.781 -1946.781 -1946.781] [192.6304], Avg: [-1772.933 -1772.933 -1772.933] (0.551)
Step: 29799, Reward: [-1917.243 -1917.243 -1917.243] [190.6045], Avg: [-1773.495 -1773.495 -1773.495] (0.551)
Step: 29849, Reward: [-1822.55 -1822.55 -1822.55] [127.6919], Avg: [-1773.791 -1773.791 -1773.791] (0.550)
Step: 29899, Reward: [-1910.209 -1910.209 -1910.209] [137.3251], Avg: [-1774.248 -1774.248 -1774.248] (0.550)
Step: 29949, Reward: [-1854.969 -1854.969 -1854.969] [137.5561], Avg: [-1774.613 -1774.613 -1774.613] (0.549)
Step: 29999, Reward: [-1877.124 -1877.124 -1877.124] [365.8877], Avg: [-1775.393 -1775.393 -1775.393] (0.549)
Step: 30049, Reward: [-1630.568 -1630.568 -1630.568] [209.2646], Avg: [-1775.501 -1775.501 -1775.501] (0.548)
Step: 30099, Reward: [-1834.279 -1834.279 -1834.279] [189.1846], Avg: [-1775.913 -1775.913 -1775.913] (0.548)
Step: 30149, Reward: [-1919.719 -1919.719 -1919.719] [134.7560], Avg: [-1776.375 -1776.375 -1776.375] (0.547)
Step: 30199, Reward: [-1909.386 -1909.386 -1909.386] [186.3915], Avg: [-1776.903 -1776.903 -1776.903] (0.546)
Step: 30249, Reward: [-1699.545 -1699.545 -1699.545] [275.3333], Avg: [-1777.231 -1777.231 -1777.231] (0.546)
Step: 30299, Reward: [-1978.59 -1978.59 -1978.59] [178.4159], Avg: [-1777.857 -1777.857 -1777.857] (0.545)
Step: 30349, Reward: [-1985.852 -1985.852 -1985.852] [182.5078], Avg: [-1778.501 -1778.501 -1778.501] (0.545)
Step: 30399, Reward: [-1963.708 -1963.708 -1963.708] [98.1391], Avg: [-1778.967 -1778.967 -1778.967] (0.544)
Step: 30449, Reward: [-1869.171 -1869.171 -1869.171] [166.6023], Avg: [-1779.388 -1779.388 -1779.388] (0.544)
Step: 30499, Reward: [-1929.719 -1929.719 -1929.719] [212.4351], Avg: [-1779.983 -1779.983 -1779.983] (0.543)
Step: 30549, Reward: [-1884.652 -1884.652 -1884.652] [225.3464], Avg: [-1780.523 -1780.523 -1780.523] (0.543)
Step: 30599, Reward: [-1908.607 -1908.607 -1908.607] [250.6010], Avg: [-1781.142 -1781.142 -1781.142] (0.542)
Step: 30649, Reward: [-1760.17 -1760.17 -1760.17] [128.4038], Avg: [-1781.317 -1781.317 -1781.317] (0.542)
Step: 30699, Reward: [-1711.762 -1711.762 -1711.762] [235.4576], Avg: [-1781.587 -1781.587 -1781.587] (0.541)
Step: 30749, Reward: [-1901.493 -1901.493 -1901.493] [351.6767], Avg: [-1782.354 -1782.354 -1782.354] (0.540)
Step: 30799, Reward: [-1884.107 -1884.107 -1884.107] [241.3108], Avg: [-1782.911 -1782.911 -1782.911] (0.540)
Step: 30849, Reward: [-1804.512 -1804.512 -1804.512] [192.6625], Avg: [-1783.258 -1783.258 -1783.258] (0.539)
Step: 30899, Reward: [-1914.337 -1914.337 -1914.337] [160.6611], Avg: [-1783.73 -1783.73 -1783.73] (0.539)
Step: 30949, Reward: [-1682.673 -1682.673 -1682.673] [180.4178], Avg: [-1783.859 -1783.859 -1783.859] (0.538)
Step: 30999, Reward: [-1765.753 -1765.753 -1765.753] [190.1422], Avg: [-1784.136 -1784.136 -1784.136] (0.538)
Step: 31049, Reward: [-1742.683 -1742.683 -1742.683] [139.4767], Avg: [-1784.294 -1784.294 -1784.294] (0.537)
Step: 31099, Reward: [-1871.377 -1871.377 -1871.377] [265.2083], Avg: [-1784.86 -1784.86 -1784.86] (0.537)
Step: 31149, Reward: [-1827.728 -1827.728 -1827.728] [197.9627], Avg: [-1785.247 -1785.247 -1785.247] (0.536)
Step: 31199, Reward: [-1848.157 -1848.157 -1848.157] [118.5780], Avg: [-1785.538 -1785.538 -1785.538] (0.536)
Step: 31249, Reward: [-1771.395 -1771.395 -1771.395] [119.7826], Avg: [-1785.707 -1785.707 -1785.707] (0.535)
Step: 31299, Reward: [-1987.079 -1987.079 -1987.079] [217.3278], Avg: [-1786.376 -1786.376 -1786.376] (0.535)
Step: 31349, Reward: [-1755.033 -1755.033 -1755.033] [180.9290], Avg: [-1786.614 -1786.614 -1786.614] (0.534)
Step: 31399, Reward: [-1853.97 -1853.97 -1853.97] [52.2202], Avg: [-1786.805 -1786.805 -1786.805] (0.533)
Step: 31449, Reward: [-1773.503 -1773.503 -1773.503] [163.4911], Avg: [-1787.043 -1787.043 -1787.043] (0.533)
Step: 31499, Reward: [-1881.758 -1881.758 -1881.758] [153.6923], Avg: [-1787.438 -1787.438 -1787.438] (0.532)
Step: 31549, Reward: [-1802.899 -1802.899 -1802.899] [247.3400], Avg: [-1787.854 -1787.854 -1787.854] (0.532)
Step: 31599, Reward: [-1881.115 -1881.115 -1881.115] [214.9272], Avg: [-1788.342 -1788.342 -1788.342] (0.531)
Step: 31649, Reward: [-1703.835 -1703.835 -1703.835] [105.5846], Avg: [-1788.375 -1788.375 -1788.375] (0.531)
Step: 31699, Reward: [-1913.284 -1913.284 -1913.284] [217.9656], Avg: [-1788.916 -1788.916 -1788.916] (0.530)
Step: 31749, Reward: [-1822.014 -1822.014 -1822.014] [108.4294], Avg: [-1789.139 -1789.139 -1789.139] (0.530)
Step: 31799, Reward: [-1933.806 -1933.806 -1933.806] [99.8090], Avg: [-1789.523 -1789.523 -1789.523] (0.529)
Step: 31849, Reward: [-1783.235 -1783.235 -1783.235] [189.0826], Avg: [-1789.81 -1789.81 -1789.81] (0.529)
Step: 31899, Reward: [-1829.29 -1829.29 -1829.29] [175.6500], Avg: [-1790.147 -1790.147 -1790.147] (0.528)
Step: 31949, Reward: [-1798.254 -1798.254 -1798.254] [109.2096], Avg: [-1790.331 -1790.331 -1790.331] (0.528)
Step: 31999, Reward: [-1753.436 -1753.436 -1753.436] [188.3423], Avg: [-1790.568 -1790.568 -1790.568] (0.527)
Step: 32049, Reward: [-1824.43 -1824.43 -1824.43] [210.1543], Avg: [-1790.948 -1790.948 -1790.948] (0.527)
Step: 32099, Reward: [-1665.453 -1665.453 -1665.453] [132.4263], Avg: [-1790.959 -1790.959 -1790.959] (0.526)
Step: 32149, Reward: [-1909.256 -1909.256 -1909.256] [226.0020], Avg: [-1791.495 -1791.495 -1791.495] (0.526)
Step: 32199, Reward: [-1622.169 -1622.169 -1622.169] [180.0846], Avg: [-1791.511 -1791.511 -1791.511] (0.525)
Step: 32249, Reward: [-1744.583 -1744.583 -1744.583] [295.1297], Avg: [-1791.896 -1791.896 -1791.896] (0.524)
Step: 32299, Reward: [-1855.523 -1855.523 -1855.523] [78.2142], Avg: [-1792.116 -1792.116 -1792.116] (0.524)
Step: 32349, Reward: [-1530.445 -1530.445 -1530.445] [336.0257], Avg: [-1792.231 -1792.231 -1792.231] (0.523)
Step: 32399, Reward: [-1733.281 -1733.281 -1733.281] [152.6611], Avg: [-1792.375 -1792.375 -1792.375] (0.523)
Step: 32449, Reward: [-1797.345 -1797.345 -1797.345] [140.8218], Avg: [-1792.6 -1792.6 -1792.6] (0.522)
Step: 32499, Reward: [-1512.735 -1512.735 -1512.735] [135.3376], Avg: [-1792.377 -1792.377 -1792.377] (0.522)
Step: 32549, Reward: [-1642.156 -1642.156 -1642.156] [225.4001], Avg: [-1792.493 -1792.493 -1792.493] (0.521)
Step: 32599, Reward: [-1805.144 -1805.144 -1805.144] [114.6804], Avg: [-1792.688 -1792.688 -1792.688] (0.521)
Step: 32649, Reward: [-1595.426 -1595.426 -1595.426] [90.0316], Avg: [-1792.524 -1792.524 -1792.524] (0.520)
Step: 32699, Reward: [-1750.466 -1750.466 -1750.466] [137.4531], Avg: [-1792.67 -1792.67 -1792.67] (0.520)
Step: 32749, Reward: [-1713.898 -1713.898 -1713.898] [102.6001], Avg: [-1792.706 -1792.706 -1792.706] (0.519)
Step: 32799, Reward: [-1414.75 -1414.75 -1414.75] [386.4971], Avg: [-1792.719 -1792.719 -1792.719] (0.519)
Step: 32849, Reward: [-1422.603 -1422.603 -1422.603] [219.1583], Avg: [-1792.489 -1792.489 -1792.489] (0.518)
Step: 32899, Reward: [-1801.001 -1801.001 -1801.001] [246.3032], Avg: [-1792.877 -1792.877 -1792.877] (0.518)
Step: 32949, Reward: [-1679.565 -1679.565 -1679.565] [236.9314], Avg: [-1793.064 -1793.064 -1793.064] (0.517)
Step: 32999, Reward: [-1603.967 -1603.967 -1603.967] [252.5215], Avg: [-1793.16 -1793.16 -1793.16] (0.517)
Step: 33049, Reward: [-1633.431 -1633.431 -1633.431] [85.9606], Avg: [-1793.049 -1793.049 -1793.049] (0.516)
Step: 33099, Reward: [-1380.658 -1380.658 -1380.658] [212.4804], Avg: [-1792.747 -1792.747 -1792.747] (0.516)
Step: 33149, Reward: [-1562.671 -1562.671 -1562.671] [251.3221], Avg: [-1792.779 -1792.779 -1792.779] (0.515)
Step: 33199, Reward: [-1584.765 -1584.765 -1584.765] [160.5323], Avg: [-1792.707 -1792.707 -1792.707] (0.515)
Step: 33249, Reward: [-1703.527 -1703.527 -1703.527] [117.7144], Avg: [-1792.75 -1792.75 -1792.75] (0.514)
Step: 33299, Reward: [-1455.95 -1455.95 -1455.95] [89.8272], Avg: [-1792.379 -1792.379 -1792.379] (0.514)
Step: 33349, Reward: [-1571.721 -1571.721 -1571.721] [243.4061], Avg: [-1792.414 -1792.414 -1792.414] (0.513)
Step: 33399, Reward: [-1623.653 -1623.653 -1623.653] [137.7910], Avg: [-1792.367 -1792.367 -1792.367] (0.513)
Step: 33449, Reward: [-1825.444 -1825.444 -1825.444] [180.1094], Avg: [-1792.686 -1792.686 -1792.686] (0.512)
Step: 33499, Reward: [-1573.789 -1573.789 -1573.789] [183.4957], Avg: [-1792.633 -1792.633 -1792.633] (0.512)
Step: 33549, Reward: [-1724.053 -1724.053 -1724.053] [473.7524], Avg: [-1793.237 -1793.237 -1793.237] (0.511)
Step: 33599, Reward: [-1635.927 -1635.927 -1635.927] [159.7330], Avg: [-1793.24 -1793.24 -1793.24] (0.511)
Step: 33649, Reward: [-1731.669 -1731.669 -1731.669] [56.5132], Avg: [-1793.233 -1793.233 -1793.233] (0.510)
Step: 33699, Reward: [-1646.858 -1646.858 -1646.858] [152.1000], Avg: [-1793.241 -1793.241 -1793.241] (0.509)
Step: 33749, Reward: [-1633.049 -1633.049 -1633.049] [251.9972], Avg: [-1793.377 -1793.377 -1793.377] (0.509)
Step: 33799, Reward: [-1859.609 -1859.609 -1859.609] [227.4436], Avg: [-1793.812 -1793.812 -1793.812] (0.508)
Step: 33849, Reward: [-1852.838 -1852.838 -1852.838] [152.7566], Avg: [-1794.125 -1794.125 -1794.125] (0.508)
Step: 33899, Reward: [-1550.924 -1550.924 -1550.924] [163.4107], Avg: [-1794.007 -1794.007 -1794.007] (0.507)
Step: 33949, Reward: [-1597.256 -1597.256 -1597.256] [104.1771], Avg: [-1793.871 -1793.871 -1793.871] (0.507)
Step: 33999, Reward: [-1431.714 -1431.714 -1431.714] [212.1620], Avg: [-1793.65 -1793.65 -1793.65] (0.506)
Step: 34049, Reward: [-1654.839 -1654.839 -1654.839] [327.5782], Avg: [-1793.927 -1793.927 -1793.927] (0.506)
Step: 34099, Reward: [-1422.041 -1422.041 -1422.041] [388.6126], Avg: [-1793.952 -1793.952 -1793.952] (0.505)
Step: 34149, Reward: [-1787.526 -1787.526 -1787.526] [119.4037], Avg: [-1794.117 -1794.117 -1794.117] (0.505)
Step: 34199, Reward: [-1562.778 -1562.778 -1562.778] [110.8272], Avg: [-1793.941 -1793.941 -1793.941] (0.504)
Step: 34249, Reward: [-1576.489 -1576.489 -1576.489] [276.8561], Avg: [-1794.028 -1794.028 -1794.028] (0.504)
Step: 34299, Reward: [-1807.904 -1807.904 -1807.904] [149.1941], Avg: [-1794.265 -1794.265 -1794.265] (0.503)
Step: 34349, Reward: [-1770.954 -1770.954 -1770.954] [146.1014], Avg: [-1794.444 -1794.444 -1794.444] (0.503)
Step: 34399, Reward: [-1645.693 -1645.693 -1645.693] [119.9504], Avg: [-1794.402 -1794.402 -1794.402] (0.502)
Step: 34449, Reward: [-1617.409 -1617.409 -1617.409] [118.0782], Avg: [-1794.317 -1794.317 -1794.317] (0.502)
Step: 34499, Reward: [-1582.012 -1582.012 -1582.012] [332.2207], Avg: [-1794.491 -1794.491 -1794.491] (0.501)
Step: 34549, Reward: [-1666.022 -1666.022 -1666.022] [194.3547], Avg: [-1794.586 -1794.586 -1794.586] (0.501)
Step: 34599, Reward: [-1718.543 -1718.543 -1718.543] [212.6827], Avg: [-1794.783 -1794.783 -1794.783] (0.500)
Step: 34649, Reward: [-1551.487 -1551.487 -1551.487] [131.5500], Avg: [-1794.622 -1794.622 -1794.622] (0.500)
Step: 34699, Reward: [-1529.234 -1529.234 -1529.234] [263.1239], Avg: [-1794.619 -1794.619 -1794.619] (0.499)
Step: 34749, Reward: [-1775.961 -1775.961 -1775.961] [127.6752], Avg: [-1794.776 -1794.776 -1794.776] (0.499)
Step: 34799, Reward: [-1880.751 -1880.751 -1880.751] [196.9487], Avg: [-1795.182 -1795.182 -1795.182] (0.498)
Step: 34849, Reward: [-1730.841 -1730.841 -1730.841] [175.5062], Avg: [-1795.342 -1795.342 -1795.342] (0.498)
Step: 34899, Reward: [-1679.928 -1679.928 -1679.928] [192.3312], Avg: [-1795.452 -1795.452 -1795.452] (0.497)
Step: 34949, Reward: [-1656.201 -1656.201 -1656.201] [308.1762], Avg: [-1795.694 -1795.694 -1795.694] (0.497)
Step: 34999, Reward: [-1567.236 -1567.236 -1567.236] [171.1683], Avg: [-1795.612 -1795.612 -1795.612] (0.496)
Step: 35049, Reward: [-1690.893 -1690.893 -1690.893] [185.4991], Avg: [-1795.727 -1795.727 -1795.727] (0.496)
Step: 35099, Reward: [-1816.057 -1816.057 -1816.057] [266.0506], Avg: [-1796.135 -1796.135 -1796.135] (0.495)
Step: 35149, Reward: [-1568.861 -1568.861 -1568.861] [196.1722], Avg: [-1796.091 -1796.091 -1796.091] (0.495)
Step: 35199, Reward: [-1700.856 -1700.856 -1700.856] [413.8176], Avg: [-1796.543 -1796.543 -1796.543] (0.494)
Step: 35249, Reward: [-1681.429 -1681.429 -1681.429] [200.6135], Avg: [-1796.665 -1796.665 -1796.665] (0.494)
Step: 35299, Reward: [-1517.599 -1517.599 -1517.599] [246.3387], Avg: [-1796.618 -1796.618 -1796.618] (0.493)
Step: 35349, Reward: [-1841.435 -1841.435 -1841.435] [251.4215], Avg: [-1797.037 -1797.037 -1797.037] (0.493)
Step: 35399, Reward: [-1442.717 -1442.717 -1442.717] [286.1819], Avg: [-1796.941 -1796.941 -1796.941] (0.492)
Step: 35449, Reward: [-1837.743 -1837.743 -1837.743] [182.5229], Avg: [-1797.256 -1797.256 -1797.256] (0.492)
Step: 35499, Reward: [-1661.157 -1661.157 -1661.157] [229.0163], Avg: [-1797.387 -1797.387 -1797.387] (0.491)
Step: 35549, Reward: [-1692.788 -1692.788 -1692.788] [233.2050], Avg: [-1797.568 -1797.568 -1797.568] (0.491)
Step: 35599, Reward: [-1764.392 -1764.392 -1764.392] [260.3397], Avg: [-1797.887 -1797.887 -1797.887] (0.490)
Step: 35649, Reward: [-1722.374 -1722.374 -1722.374] [133.4005], Avg: [-1797.968 -1797.968 -1797.968] (0.490)
Step: 35699, Reward: [-1703.924 -1703.924 -1703.924] [249.6286], Avg: [-1798.186 -1798.186 -1798.186] (0.490)
Step: 35749, Reward: [-1723.977 -1723.977 -1723.977] [165.5845], Avg: [-1798.314 -1798.314 -1798.314] (0.489)
Step: 35799, Reward: [-1880.663 -1880.663 -1880.663] [158.7673], Avg: [-1798.65 -1798.65 -1798.65] (0.489)
Step: 35849, Reward: [-1830.409 -1830.409 -1830.409] [207.9321], Avg: [-1798.985 -1798.985 -1798.985] (0.488)
Step: 35899, Reward: [-1699.082 -1699.082 -1699.082] [289.3541], Avg: [-1799.249 -1799.249 -1799.249] (0.488)
Step: 35949, Reward: [-1846.332 -1846.332 -1846.332] [101.8888], Avg: [-1799.456 -1799.456 -1799.456] (0.487)
Step: 35999, Reward: [-1869.098 -1869.098 -1869.098] [161.5599], Avg: [-1799.777 -1799.777 -1799.777] (0.487)
