Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		action = gumbel_softmax(action_mu, hard=True)
		action = action.view(*out_dims, -1)
		return action
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else one_hot(action) for j,action in enumerate(actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
			to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
			states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-506.222 -506.222 -506.222] [0.0000], Avg: [-506.222 -506.222 -506.222] (0.995)
Step: 99, Reward: [-559.407 -559.407 -559.407] [0.0000], Avg: [-532.815 -532.815 -532.815] (0.990)
Step: 149, Reward: [-591.284 -591.284 -591.284] [0.0000], Avg: [-552.304 -552.304 -552.304] (0.985)
Step: 199, Reward: [-569.805 -569.805 -569.805] [0.0000], Avg: [-556.68 -556.68 -556.68] (0.980)
Step: 249, Reward: [-522.229 -522.229 -522.229] [0.0000], Avg: [-549.789 -549.789 -549.789] (0.975)
Step: 299, Reward: [-491.67 -491.67 -491.67] [0.0000], Avg: [-540.103 -540.103 -540.103] (0.970)
Step: 349, Reward: [-665.002 -665.002 -665.002] [0.0000], Avg: [-557.946 -557.946 -557.946] (0.966)
Step: 399, Reward: [-464.669 -464.669 -464.669] [0.0000], Avg: [-546.286 -546.286 -546.286] (0.961)
Step: 449, Reward: [-460.097 -460.097 -460.097] [0.0000], Avg: [-536.709 -536.709 -536.709] (0.956)
Step: 499, Reward: [-540.094 -540.094 -540.094] [0.0000], Avg: [-537.048 -537.048 -537.048] (0.951)
Step: 549, Reward: [-411.751 -411.751 -411.751] [0.0000], Avg: [-525.657 -525.657 -525.657] (0.946)
Step: 599, Reward: [-526.095 -526.095 -526.095] [0.0000], Avg: [-525.694 -525.694 -525.694] (0.942)
Step: 649, Reward: [-407.213 -407.213 -407.213] [0.0000], Avg: [-516.58 -516.58 -516.58] (0.937)
Step: 699, Reward: [-400.15 -400.15 -400.15] [0.0000], Avg: [-508.263 -508.263 -508.263] (0.932)
Step: 749, Reward: [-564.08 -564.08 -564.08] [0.0000], Avg: [-511.985 -511.985 -511.985] (0.928)
Step: 799, Reward: [-666.642 -666.642 -666.642] [0.0000], Avg: [-521.651 -521.651 -521.651] (0.923)
Step: 849, Reward: [-588.496 -588.496 -588.496] [0.0000], Avg: [-525.583 -525.583 -525.583] (0.918)
Step: 899, Reward: [-634.483 -634.483 -634.483] [0.0000], Avg: [-531.633 -531.633 -531.633] (0.914)
Step: 949, Reward: [-572.667 -572.667 -572.667] [0.0000], Avg: [-533.792 -533.792 -533.792] (0.909)
Step: 999, Reward: [-458.229 -458.229 -458.229] [0.0000], Avg: [-530.014 -530.014 -530.014] (0.905)
Step: 1049, Reward: [-448.518 -448.518 -448.518] [0.0000], Avg: [-526.134 -526.134 -526.134] (0.900)
Step: 1099, Reward: [-682.945 -682.945 -682.945] [0.0000], Avg: [-533.261 -533.261 -533.261] (0.896)
Step: 1149, Reward: [-430.723 -430.723 -430.723] [0.0000], Avg: [-528.803 -528.803 -528.803] (0.891)
Step: 1199, Reward: [-608.171 -608.171 -608.171] [0.0000], Avg: [-532.11 -532.11 -532.11] (0.887)
Step: 1249, Reward: [-428.298 -428.298 -428.298] [0.0000], Avg: [-527.958 -527.958 -527.958] (0.882)
Step: 1299, Reward: [-386.008 -386.008 -386.008] [0.0000], Avg: [-522.498 -522.498 -522.498] (0.878)
Step: 1349, Reward: [-502.233 -502.233 -502.233] [0.0000], Avg: [-521.747 -521.747 -521.747] (0.873)
Step: 1399, Reward: [-422.323 -422.323 -422.323] [0.0000], Avg: [-518.197 -518.197 -518.197] (0.869)
Step: 1449, Reward: [-497.246 -497.246 -497.246] [0.0000], Avg: [-517.474 -517.474 -517.474] (0.865)
Step: 1499, Reward: [-494.226 -494.226 -494.226] [0.0000], Avg: [-516.699 -516.699 -516.699] (0.860)
Step: 1549, Reward: [-541.103 -541.103 -541.103] [0.0000], Avg: [-517.486 -517.486 -517.486] (0.856)
Step: 1599, Reward: [-496.727 -496.727 -496.727] [0.0000], Avg: [-516.838 -516.838 -516.838] (0.852)
Step: 1649, Reward: [-583.732 -583.732 -583.732] [0.0000], Avg: [-518.865 -518.865 -518.865] (0.848)
Step: 1699, Reward: [-491.626 -491.626 -491.626] [0.0000], Avg: [-518.064 -518.064 -518.064] (0.843)
Step: 1749, Reward: [-444.044 -444.044 -444.044] [0.0000], Avg: [-515.949 -515.949 -515.949] (0.839)
Step: 1799, Reward: [-417.112 -417.112 -417.112] [0.0000], Avg: [-513.203 -513.203 -513.203] (0.835)
Step: 1849, Reward: [-496.494 -496.494 -496.494] [0.0000], Avg: [-512.752 -512.752 -512.752] (0.831)
Step: 1899, Reward: [-474.854 -474.854 -474.854] [0.0000], Avg: [-511.754 -511.754 -511.754] (0.827)
Step: 1949, Reward: [-426.902 -426.902 -426.902] [0.0000], Avg: [-509.579 -509.579 -509.579] (0.822)
Step: 1999, Reward: [-681.181 -681.181 -681.181] [0.0000], Avg: [-513.869 -513.869 -513.869] (0.818)
Step: 2049, Reward: [-482.776 -482.776 -482.776] [0.0000], Avg: [-513.11 -513.11 -513.11] (0.814)
Step: 2099, Reward: [-407.514 -407.514 -407.514] [0.0000], Avg: [-510.596 -510.596 -510.596] (0.810)
Step: 2149, Reward: [-438.958 -438.958 -438.958] [0.0000], Avg: [-508.93 -508.93 -508.93] (0.806)
Step: 2199, Reward: [-510.439 -510.439 -510.439] [0.0000], Avg: [-508.965 -508.965 -508.965] (0.802)
Step: 2249, Reward: [-474.71 -474.71 -474.71] [0.0000], Avg: [-508.203 -508.203 -508.203] (0.798)
Step: 2299, Reward: [-339.03 -339.03 -339.03] [0.0000], Avg: [-504.526 -504.526 -504.526] (0.794)
Step: 2349, Reward: [-381.71 -381.71 -381.71] [0.0000], Avg: [-501.913 -501.913 -501.913] (0.790)
Step: 2399, Reward: [-512.372 -512.372 -512.372] [0.0000], Avg: [-502.13 -502.13 -502.13] (0.786)
Step: 2449, Reward: [-504.639 -504.639 -504.639] [0.0000], Avg: [-502.182 -502.182 -502.182] (0.782)
Step: 2499, Reward: [-523.833 -523.833 -523.833] [0.0000], Avg: [-502.615 -502.615 -502.615] (0.778)
Step: 2549, Reward: [-351.035 -351.035 -351.035] [0.0000], Avg: [-499.642 -499.642 -499.642] (0.774)
Step: 2599, Reward: [-409.924 -409.924 -409.924] [0.0000], Avg: [-497.917 -497.917 -497.917] (0.771)
Step: 2649, Reward: [-625.035 -625.035 -625.035] [0.0000], Avg: [-500.316 -500.316 -500.316] (0.767)
Step: 2699, Reward: [-479.935 -479.935 -479.935] [0.0000], Avg: [-499.938 -499.938 -499.938] (0.763)
Step: 2749, Reward: [-401.951 -401.951 -401.951] [0.0000], Avg: [-498.157 -498.157 -498.157] (0.759)
Step: 2799, Reward: [-430.904 -430.904 -430.904] [0.0000], Avg: [-496.956 -496.956 -496.956] (0.755)
Step: 2849, Reward: [-465.482 -465.482 -465.482] [0.0000], Avg: [-496.403 -496.403 -496.403] (0.751)
Step: 2899, Reward: [-683.404 -683.404 -683.404] [0.0000], Avg: [-499.628 -499.628 -499.628] (0.748)
Step: 2949, Reward: [-425.611 -425.611 -425.611] [0.0000], Avg: [-498.373 -498.373 -498.373] (0.744)
Step: 2999, Reward: [-400.457 -400.457 -400.457] [0.0000], Avg: [-496.741 -496.741 -496.741] (0.740)
Step: 3049, Reward: [-474.678 -474.678 -474.678] [0.0000], Avg: [-496.379 -496.379 -496.379] (0.737)
Step: 3099, Reward: [-596.056 -596.056 -596.056] [0.0000], Avg: [-497.987 -497.987 -497.987] (0.733)
Step: 3149, Reward: [-530.591 -530.591 -530.591] [0.0000], Avg: [-498.505 -498.505 -498.505] (0.729)
Step: 3199, Reward: [-484.802 -484.802 -484.802] [0.0000], Avg: [-498.291 -498.291 -498.291] (0.726)
Step: 3249, Reward: [-622.223 -622.223 -622.223] [0.0000], Avg: [-500.197 -500.197 -500.197] (0.722)
Step: 3299, Reward: [-623.506 -623.506 -623.506] [0.0000], Avg: [-502.066 -502.066 -502.066] (0.718)
Step: 3349, Reward: [-518.798 -518.798 -518.798] [0.0000], Avg: [-502.315 -502.315 -502.315] (0.715)
Step: 3399, Reward: [-418.768 -418.768 -418.768] [0.0000], Avg: [-501.087 -501.087 -501.087] (0.711)
Step: 3449, Reward: [-521.115 -521.115 -521.115] [0.0000], Avg: [-501.377 -501.377 -501.377] (0.708)
Step: 3499, Reward: [-479.067 -479.067 -479.067] [0.0000], Avg: [-501.058 -501.058 -501.058] (0.704)
Step: 3549, Reward: [-475.451 -475.451 -475.451] [0.0000], Avg: [-500.697 -500.697 -500.697] (0.701)
Step: 3599, Reward: [-363.501 -363.501 -363.501] [0.0000], Avg: [-498.792 -498.792 -498.792] (0.697)
Step: 3649, Reward: [-571.668 -571.668 -571.668] [0.0000], Avg: [-499.79 -499.79 -499.79] (0.694)
Step: 3699, Reward: [-414.668 -414.668 -414.668] [0.0000], Avg: [-498.64 -498.64 -498.64] (0.690)
Step: 3749, Reward: [-358.733 -358.733 -358.733] [0.0000], Avg: [-496.775 -496.775 -496.775] (0.687)
Step: 3799, Reward: [-418.81 -418.81 -418.81] [0.0000], Avg: [-495.749 -495.749 -495.749] (0.683)
Step: 3849, Reward: [-428.3 -428.3 -428.3] [0.0000], Avg: [-494.873 -494.873 -494.873] (0.680)
Step: 3899, Reward: [-611.933 -611.933 -611.933] [0.0000], Avg: [-496.374 -496.374 -496.374] (0.676)
Step: 3949, Reward: [-556.884 -556.884 -556.884] [0.0000], Avg: [-497.139 -497.139 -497.139] (0.673)
Step: 3999, Reward: [-575.963 -575.963 -575.963] [0.0000], Avg: [-498.125 -498.125 -498.125] (0.670)
Step: 4049, Reward: [-482.281 -482.281 -482.281] [0.0000], Avg: [-497.929 -497.929 -497.929] (0.666)
Step: 4099, Reward: [-497.897 -497.897 -497.897] [0.0000], Avg: [-497.929 -497.929 -497.929] (0.663)
Step: 4149, Reward: [-470.098 -470.098 -470.098] [0.0000], Avg: [-497.593 -497.593 -497.593] (0.660)
Step: 4199, Reward: [-577.241 -577.241 -577.241] [0.0000], Avg: [-498.542 -498.542 -498.542] (0.656)
Step: 4249, Reward: [-483.818 -483.818 -483.818] [0.0000], Avg: [-498.368 -498.368 -498.368] (0.653)
Step: 4299, Reward: [-512.435 -512.435 -512.435] [0.0000], Avg: [-498.532 -498.532 -498.532] (0.650)
Step: 4349, Reward: [-511.756 -511.756 -511.756] [0.0000], Avg: [-498.684 -498.684 -498.684] (0.647)
Step: 4399, Reward: [-494.651 -494.651 -494.651] [0.0000], Avg: [-498.638 -498.638 -498.638] (0.643)
Step: 4449, Reward: [-574.362 -574.362 -574.362] [0.0000], Avg: [-499.489 -499.489 -499.489] (0.640)
Step: 4499, Reward: [-810.772 -810.772 -810.772] [0.0000], Avg: [-502.948 -502.948 -502.948] (0.637)
Step: 4549, Reward: [-542.881 -542.881 -542.881] [0.0000], Avg: [-503.387 -503.387 -503.387] (0.634)
Step: 4599, Reward: [-588.693 -588.693 -588.693] [0.0000], Avg: [-504.314 -504.314 -504.314] (0.631)
Step: 4649, Reward: [-434.623 -434.623 -434.623] [0.0000], Avg: [-503.564 -503.564 -503.564] (0.627)
Step: 4699, Reward: [-590.066 -590.066 -590.066] [0.0000], Avg: [-504.485 -504.485 -504.485] (0.624)
Step: 4749, Reward: [-898.925 -898.925 -898.925] [0.0000], Avg: [-508.637 -508.637 -508.637] (0.621)
Step: 4799, Reward: [-621.054 -621.054 -621.054] [0.0000], Avg: [-509.808 -509.808 -509.808] (0.618)
Step: 4849, Reward: [-403.862 -403.862 -403.862] [0.0000], Avg: [-508.715 -508.715 -508.715] (0.615)
Step: 4899, Reward: [-626.503 -626.503 -626.503] [0.0000], Avg: [-509.917 -509.917 -509.917] (0.612)
Step: 4949, Reward: [-865.864 -865.864 -865.864] [0.0000], Avg: [-513.513 -513.513 -513.513] (0.609)
Step: 4999, Reward: [-453.582 -453.582 -453.582] [0.0000], Avg: [-512.913 -512.913 -512.913] (0.606)
Step: 5049, Reward: [-720.029 -720.029 -720.029] [0.0000], Avg: [-514.964 -514.964 -514.964] (0.603)
Step: 5099, Reward: [-484.741 -484.741 -484.741] [0.0000], Avg: [-514.668 -514.668 -514.668] (0.600)
Step: 5149, Reward: [-446.725 -446.725 -446.725] [0.0000], Avg: [-514.008 -514.008 -514.008] (0.597)
Step: 5199, Reward: [-444.097 -444.097 -444.097] [0.0000], Avg: [-513.336 -513.336 -513.336] (0.594)
Step: 5249, Reward: [-610.779 -610.779 -610.779] [0.0000], Avg: [-514.264 -514.264 -514.264] (0.591)
Step: 5299, Reward: [-509.909 -509.909 -509.909] [0.0000], Avg: [-514.223 -514.223 -514.223] (0.588)
Step: 5349, Reward: [-475.575 -475.575 -475.575] [0.0000], Avg: [-513.862 -513.862 -513.862] (0.585)
Step: 5399, Reward: [-510.918 -510.918 -510.918] [0.0000], Avg: [-513.834 -513.834 -513.834] (0.582)
Step: 5449, Reward: [-493.926 -493.926 -493.926] [0.0000], Avg: [-513.652 -513.652 -513.652] (0.579)
Step: 5499, Reward: [-376.345 -376.345 -376.345] [0.0000], Avg: [-512.404 -512.404 -512.404] (0.576)
Step: 5549, Reward: [-581.152 -581.152 -581.152] [0.0000], Avg: [-513.023 -513.023 -513.023] (0.573)
Step: 5599, Reward: [-761.178 -761.178 -761.178] [0.0000], Avg: [-515.239 -515.239 -515.239] (0.570)
Step: 5649, Reward: [-599.473 -599.473 -599.473] [0.0000], Avg: [-515.984 -515.984 -515.984] (0.568)
Step: 5699, Reward: [-552.05 -552.05 -552.05] [0.0000], Avg: [-516.3 -516.3 -516.3] (0.565)
Step: 5749, Reward: [-443.138 -443.138 -443.138] [0.0000], Avg: [-515.664 -515.664 -515.664] (0.562)
Step: 5799, Reward: [-506.913 -506.913 -506.913] [0.0000], Avg: [-515.589 -515.589 -515.589] (0.559)
Step: 5849, Reward: [-585.438 -585.438 -585.438] [0.0000], Avg: [-516.186 -516.186 -516.186] (0.556)
Step: 5899, Reward: [-721.703 -721.703 -721.703] [0.0000], Avg: [-517.927 -517.927 -517.927] (0.554)
Step: 5949, Reward: [-570.109 -570.109 -570.109] [0.0000], Avg: [-518.366 -518.366 -518.366] (0.551)
Step: 5999, Reward: [-536.12 -536.12 -536.12] [0.0000], Avg: [-518.514 -518.514 -518.514] (0.548)
Step: 6049, Reward: [-577.77 -577.77 -577.77] [0.0000], Avg: [-519.004 -519.004 -519.004] (0.545)
Step: 6099, Reward: [-720.188 -720.188 -720.188] [0.0000], Avg: [-520.653 -520.653 -520.653] (0.543)
Step: 6149, Reward: [-711.088 -711.088 -711.088] [0.0000], Avg: [-522.201 -522.201 -522.201] (0.540)
Step: 6199, Reward: [-728.517 -728.517 -728.517] [0.0000], Avg: [-523.865 -523.865 -523.865] (0.537)
Step: 6249, Reward: [-563.29 -563.29 -563.29] [0.0000], Avg: [-524.18 -524.18 -524.18] (0.534)
Step: 6299, Reward: [-610.195 -610.195 -610.195] [0.0000], Avg: [-524.863 -524.863 -524.863] (0.532)
Step: 6349, Reward: [-445.965 -445.965 -445.965] [0.0000], Avg: [-524.242 -524.242 -524.242] (0.529)
Step: 6399, Reward: [-454.818 -454.818 -454.818] [0.0000], Avg: [-523.699 -523.699 -523.699] (0.526)
Step: 6449, Reward: [-425.925 -425.925 -425.925] [0.0000], Avg: [-522.941 -522.941 -522.941] (0.524)
Step: 6499, Reward: [-526.109 -526.109 -526.109] [0.0000], Avg: [-522.966 -522.966 -522.966] (0.521)
Step: 6549, Reward: [-473.226 -473.226 -473.226] [0.0000], Avg: [-522.586 -522.586 -522.586] (0.519)
Step: 6599, Reward: [-614.392 -614.392 -614.392] [0.0000], Avg: [-523.281 -523.281 -523.281] (0.516)
Step: 6649, Reward: [-399.252 -399.252 -399.252] [0.0000], Avg: [-522.349 -522.349 -522.349] (0.513)
Step: 6699, Reward: [-648.125 -648.125 -648.125] [0.0000], Avg: [-523.287 -523.287 -523.287] (0.511)
Step: 6749, Reward: [-527.004 -527.004 -527.004] [0.0000], Avg: [-523.315 -523.315 -523.315] (0.508)
Step: 6799, Reward: [-584.309 -584.309 -584.309] [0.0000], Avg: [-523.764 -523.764 -523.764] (0.506)
Step: 6849, Reward: [-530.816 -530.816 -530.816] [0.0000], Avg: [-523.815 -523.815 -523.815] (0.503)
Step: 6899, Reward: [-491.924 -491.924 -491.924] [0.0000], Avg: [-523.584 -523.584 -523.584] (0.501)
Step: 6949, Reward: [-430.63 -430.63 -430.63] [0.0000], Avg: [-522.915 -522.915 -522.915] (0.498)
Step: 6999, Reward: [-540.893 -540.893 -540.893] [0.0000], Avg: [-523.044 -523.044 -523.044] (0.496)
Step: 7049, Reward: [-455.453 -455.453 -455.453] [0.0000], Avg: [-522.564 -522.564 -522.564] (0.493)
Step: 7099, Reward: [-533.339 -533.339 -533.339] [0.0000], Avg: [-522.64 -522.64 -522.64] (0.491)
Step: 7149, Reward: [-641.811 -641.811 -641.811] [0.0000], Avg: [-523.473 -523.473 -523.473] (0.488)
Step: 7199, Reward: [-626.483 -626.483 -626.483] [0.0000], Avg: [-524.189 -524.189 -524.189] (0.486)
Step: 7249, Reward: [-417.345 -417.345 -417.345] [0.0000], Avg: [-523.452 -523.452 -523.452] (0.483)
Step: 7299, Reward: [-331.477 -331.477 -331.477] [0.0000], Avg: [-522.137 -522.137 -522.137] (0.481)
Step: 7349, Reward: [-439.824 -439.824 -439.824] [0.0000], Avg: [-521.577 -521.577 -521.577] (0.479)
Step: 7399, Reward: [-560.161 -560.161 -560.161] [0.0000], Avg: [-521.838 -521.838 -521.838] (0.476)
Step: 7449, Reward: [-539.058 -539.058 -539.058] [0.0000], Avg: [-521.953 -521.953 -521.953] (0.474)
Step: 7499, Reward: [-469.213 -469.213 -469.213] [0.0000], Avg: [-521.602 -521.602 -521.602] (0.471)
Step: 7549, Reward: [-389.757 -389.757 -389.757] [0.0000], Avg: [-520.729 -520.729 -520.729] (0.469)
Step: 7599, Reward: [-705.597 -705.597 -705.597] [0.0000], Avg: [-521.945 -521.945 -521.945] (0.467)
Step: 7649, Reward: [-714.77 -714.77 -714.77] [0.0000], Avg: [-523.205 -523.205 -523.205] (0.464)
Step: 7699, Reward: [-555.076 -555.076 -555.076] [0.0000], Avg: [-523.412 -523.412 -523.412] (0.462)
Step: 7749, Reward: [-448.183 -448.183 -448.183] [0.0000], Avg: [-522.927 -522.927 -522.927] (0.460)
Step: 7799, Reward: [-644.425 -644.425 -644.425] [0.0000], Avg: [-523.706 -523.706 -523.706] (0.458)
Step: 7849, Reward: [-694.452 -694.452 -694.452] [0.0000], Avg: [-524.793 -524.793 -524.793] (0.455)
Step: 7899, Reward: [-485.021 -485.021 -485.021] [0.0000], Avg: [-524.541 -524.541 -524.541] (0.453)
Step: 7949, Reward: [-571.274 -571.274 -571.274] [0.0000], Avg: [-524.835 -524.835 -524.835] (0.451)
Step: 7999, Reward: [-434.417 -434.417 -434.417] [0.0000], Avg: [-524.27 -524.27 -524.27] (0.448)
Step: 8049, Reward: [-514.035 -514.035 -514.035] [0.0000], Avg: [-524.207 -524.207 -524.207] (0.446)
Step: 8099, Reward: [-384.074 -384.074 -384.074] [0.0000], Avg: [-523.342 -523.342 -523.342] (0.444)
Step: 8149, Reward: [-748.599 -748.599 -748.599] [0.0000], Avg: [-524.724 -524.724 -524.724] (0.442)
Step: 8199, Reward: [-505.969 -505.969 -505.969] [0.0000], Avg: [-524.609 -524.609 -524.609] (0.440)
Step: 8249, Reward: [-532.592 -532.592 -532.592] [0.0000], Avg: [-524.658 -524.658 -524.658] (0.437)
Step: 8299, Reward: [-607.956 -607.956 -607.956] [0.0000], Avg: [-525.159 -525.159 -525.159] (0.435)
Step: 8349, Reward: [-518.476 -518.476 -518.476] [0.0000], Avg: [-525.119 -525.119 -525.119] (0.433)
Step: 8399, Reward: [-571.016 -571.016 -571.016] [0.0000], Avg: [-525.393 -525.393 -525.393] (0.431)
Step: 8449, Reward: [-608.131 -608.131 -608.131] [0.0000], Avg: [-525.882 -525.882 -525.882] (0.429)
Step: 8499, Reward: [-423.206 -423.206 -423.206] [0.0000], Avg: [-525.278 -525.278 -525.278] (0.427)
Step: 8549, Reward: [-597.782 -597.782 -597.782] [0.0000], Avg: [-525.702 -525.702 -525.702] (0.424)
Step: 8599, Reward: [-449.861 -449.861 -449.861] [0.0000], Avg: [-525.261 -525.261 -525.261] (0.422)
Step: 8649, Reward: [-470.485 -470.485 -470.485] [0.0000], Avg: [-524.945 -524.945 -524.945] (0.420)
Step: 8699, Reward: [-548.282 -548.282 -548.282] [0.0000], Avg: [-525.079 -525.079 -525.079] (0.418)
Step: 8749, Reward: [-563.822 -563.822 -563.822] [0.0000], Avg: [-525.3 -525.3 -525.3] (0.416)
Step: 8799, Reward: [-556.377 -556.377 -556.377] [0.0000], Avg: [-525.477 -525.477 -525.477] (0.414)
Step: 8849, Reward: [-472.88 -472.88 -472.88] [0.0000], Avg: [-525.18 -525.18 -525.18] (0.412)
Step: 8899, Reward: [-535.155 -535.155 -535.155] [0.0000], Avg: [-525.236 -525.236 -525.236] (0.410)
Step: 8949, Reward: [-659.159 -659.159 -659.159] [0.0000], Avg: [-525.984 -525.984 -525.984] (0.408)
Step: 8999, Reward: [-579.85 -579.85 -579.85] [0.0000], Avg: [-526.283 -526.283 -526.283] (0.406)
Step: 9049, Reward: [-627.797 -627.797 -627.797] [0.0000], Avg: [-526.844 -526.844 -526.844] (0.404)
Step: 9099, Reward: [-557.072 -557.072 -557.072] [0.0000], Avg: [-527.01 -527.01 -527.01] (0.402)
Step: 9149, Reward: [-709.024 -709.024 -709.024] [0.0000], Avg: [-528.005 -528.005 -528.005] (0.400)
Step: 9199, Reward: [-477.094 -477.094 -477.094] [0.0000], Avg: [-527.728 -527.728 -527.728] (0.398)
Step: 9249, Reward: [-709.75 -709.75 -709.75] [0.0000], Avg: [-528.712 -528.712 -528.712] (0.396)
Step: 9299, Reward: [-539.903 -539.903 -539.903] [0.0000], Avg: [-528.772 -528.772 -528.772] (0.394)
Step: 9349, Reward: [-558.277 -558.277 -558.277] [0.0000], Avg: [-528.93 -528.93 -528.93] (0.392)
Step: 9399, Reward: [-550.722 -550.722 -550.722] [0.0000], Avg: [-529.046 -529.046 -529.046] (0.390)
Step: 9449, Reward: [-456.87 -456.87 -456.87] [0.0000], Avg: [-528.664 -528.664 -528.664] (0.388)
Step: 9499, Reward: [-593.324 -593.324 -593.324] [0.0000], Avg: [-529.004 -529.004 -529.004] (0.386)
Step: 9549, Reward: [-624.466 -624.466 -624.466] [0.0000], Avg: [-529.504 -529.504 -529.504] (0.384)
Step: 9599, Reward: [-838.013 -838.013 -838.013] [0.0000], Avg: [-531.111 -531.111 -531.111] (0.382)
Step: 9649, Reward: [-707.369 -707.369 -707.369] [0.0000], Avg: [-532.024 -532.024 -532.024] (0.380)
Step: 9699, Reward: [-813.735 -813.735 -813.735] [0.0000], Avg: [-533.476 -533.476 -533.476] (0.378)
Step: 9749, Reward: [-505.562 -505.562 -505.562] [0.0000], Avg: [-533.333 -533.333 -533.333] (0.376)
Step: 9799, Reward: [-718.362 -718.362 -718.362] [0.0000], Avg: [-534.277 -534.277 -534.277] (0.374)
Step: 9849, Reward: [-953.316 -953.316 -953.316] [0.0000], Avg: [-536.404 -536.404 -536.404] (0.373)
Step: 9899, Reward: [-942.158 -942.158 -942.158] [0.0000], Avg: [-538.453 -538.453 -538.453] (0.371)
Step: 9949, Reward: [-887.113 -887.113 -887.113] [0.0000], Avg: [-540.205 -540.205 -540.205] (0.369)
Step: 9999, Reward: [-846.329 -846.329 -846.329] [0.0000], Avg: [-541.736 -541.736 -541.736] (0.367)
Step: 10049, Reward: [-885.232 -885.232 -885.232] [0.0000], Avg: [-543.445 -543.445 -543.445] (0.365)
Step: 10099, Reward: [-699.298 -699.298 -699.298] [0.0000], Avg: [-544.216 -544.216 -544.216] (0.363)
Step: 10149, Reward: [-878.973 -878.973 -878.973] [0.0000], Avg: [-545.866 -545.866 -545.866] (0.361)
Step: 10199, Reward: [-951.867 -951.867 -951.867] [0.0000], Avg: [-547.856 -547.856 -547.856] (0.360)
Step: 10249, Reward: [-805.87 -805.87 -805.87] [0.0000], Avg: [-549.114 -549.114 -549.114] (0.358)
Step: 10299, Reward: [-824.106 -824.106 -824.106] [0.0000], Avg: [-550.449 -550.449 -550.449] (0.356)
Step: 10349, Reward: [-543.884 -543.884 -543.884] [0.0000], Avg: [-550.418 -550.418 -550.418] (0.354)
Step: 10399, Reward: [-942.987 -942.987 -942.987] [0.0000], Avg: [-552.305 -552.305 -552.305] (0.353)
Step: 10449, Reward: [-1224.954 -1224.954 -1224.954] [0.0000], Avg: [-555.523 -555.523 -555.523] (0.351)
Step: 10499, Reward: [-1173.487 -1173.487 -1173.487] [0.0000], Avg: [-558.466 -558.466 -558.466] (0.349)
Step: 10549, Reward: [-1068.717 -1068.717 -1068.717] [0.0000], Avg: [-560.884 -560.884 -560.884] (0.347)
Step: 10599, Reward: [-744.964 -744.964 -744.964] [0.0000], Avg: [-561.753 -561.753 -561.753] (0.346)
Step: 10649, Reward: [-1101.91 -1101.91 -1101.91] [0.0000], Avg: [-564.288 -564.288 -564.288] (0.344)
Step: 10699, Reward: [-1213.301 -1213.301 -1213.301] [0.0000], Avg: [-567.321 -567.321 -567.321] (0.342)
Step: 10749, Reward: [-1209.638 -1209.638 -1209.638] [0.0000], Avg: [-570.309 -570.309 -570.309] (0.340)
Step: 10799, Reward: [-1148.475 -1148.475 -1148.475] [0.0000], Avg: [-572.985 -572.985 -572.985] (0.339)
Step: 10849, Reward: [-850.923 -850.923 -850.923] [0.0000], Avg: [-574.266 -574.266 -574.266] (0.337)
Step: 10899, Reward: [-888.614 -888.614 -888.614] [0.0000], Avg: [-575.708 -575.708 -575.708] (0.335)
Step: 10949, Reward: [-844.016 -844.016 -844.016] [0.0000], Avg: [-576.933 -576.933 -576.933] (0.334)
Step: 10999, Reward: [-1000.087 -1000.087 -1000.087] [0.0000], Avg: [-578.857 -578.857 -578.857] (0.332)
Step: 11049, Reward: [-1162.261 -1162.261 -1162.261] [0.0000], Avg: [-581.497 -581.497 -581.497] (0.330)
Step: 11099, Reward: [-1441.305 -1441.305 -1441.305] [0.0000], Avg: [-585.37 -585.37 -585.37] (0.329)
Step: 11149, Reward: [-986.76 -986.76 -986.76] [0.0000], Avg: [-587.17 -587.17 -587.17] (0.327)
Step: 11199, Reward: [-627.33 -627.33 -627.33] [0.0000], Avg: [-587.349 -587.349 -587.349] (0.325)
Step: 11249, Reward: [-1133.266 -1133.266 -1133.266] [0.0000], Avg: [-589.775 -589.775 -589.775] (0.324)
Step: 11299, Reward: [-822.479 -822.479 -822.479] [0.0000], Avg: [-590.805 -590.805 -590.805] (0.322)
Step: 11349, Reward: [-882.933 -882.933 -882.933] [0.0000], Avg: [-592.092 -592.092 -592.092] (0.321)
Step: 11399, Reward: [-1029.871 -1029.871 -1029.871] [0.0000], Avg: [-594.012 -594.012 -594.012] (0.319)
Step: 11449, Reward: [-804.658 -804.658 -804.658] [0.0000], Avg: [-594.932 -594.932 -594.932] (0.317)
Step: 11499, Reward: [-564.727 -564.727 -564.727] [0.0000], Avg: [-594.8 -594.8 -594.8] (0.316)
Step: 11549, Reward: [-644.493 -644.493 -644.493] [0.0000], Avg: [-595.016 -595.016 -595.016] (0.314)
Step: 11599, Reward: [-448.349 -448.349 -448.349] [0.0000], Avg: [-594.383 -594.383 -594.383] (0.313)
Step: 11649, Reward: [-1015.233 -1015.233 -1015.233] [0.0000], Avg: [-596.19 -596.19 -596.19] (0.311)
Step: 11699, Reward: [-1063.251 -1063.251 -1063.251] [0.0000], Avg: [-598.186 -598.186 -598.186] (0.309)
Step: 11749, Reward: [-1544.24 -1544.24 -1544.24] [0.0000], Avg: [-602.211 -602.211 -602.211] (0.308)
Step: 11799, Reward: [-1242.416 -1242.416 -1242.416] [0.0000], Avg: [-604.924 -604.924 -604.924] (0.306)
Step: 11849, Reward: [-1585.912 -1585.912 -1585.912] [0.0000], Avg: [-609.063 -609.063 -609.063] (0.305)
Step: 11899, Reward: [-1167.393 -1167.393 -1167.393] [0.0000], Avg: [-611.409 -611.409 -611.409] (0.303)
Step: 11949, Reward: [-1512.966 -1512.966 -1512.966] [0.0000], Avg: [-615.181 -615.181 -615.181] (0.302)
Step: 11999, Reward: [-1316.912 -1316.912 -1316.912] [0.0000], Avg: [-618.105 -618.105 -618.105] (0.300)
Step: 12049, Reward: [-1037.23 -1037.23 -1037.23] [0.0000], Avg: [-619.844 -619.844 -619.844] (0.299)
Step: 12099, Reward: [-1200.467 -1200.467 -1200.467] [0.0000], Avg: [-622.244 -622.244 -622.244] (0.297)
Step: 12149, Reward: [-1616.184 -1616.184 -1616.184] [0.0000], Avg: [-626.334 -626.334 -626.334] (0.296)
Step: 12199, Reward: [-1279.335 -1279.335 -1279.335] [0.0000], Avg: [-629.01 -629.01 -629.01] (0.294)
Step: 12249, Reward: [-720.999 -720.999 -720.999] [0.0000], Avg: [-629.386 -629.386 -629.386] (0.293)
Step: 12299, Reward: [-1070.36 -1070.36 -1070.36] [0.0000], Avg: [-631.178 -631.178 -631.178] (0.291)
Step: 12349, Reward: [-874.897 -874.897 -874.897] [0.0000], Avg: [-632.165 -632.165 -632.165] (0.290)
Step: 12399, Reward: [-1806.864 -1806.864 -1806.864] [0.0000], Avg: [-636.902 -636.902 -636.902] (0.288)
Step: 12449, Reward: [-1705.083 -1705.083 -1705.083] [0.0000], Avg: [-641.191 -641.191 -641.191] (0.287)
Step: 12499, Reward: [-1124.034 -1124.034 -1124.034] [0.0000], Avg: [-643.123 -643.123 -643.123] (0.286)
Step: 12549, Reward: [-1196.392 -1196.392 -1196.392] [0.0000], Avg: [-645.327 -645.327 -645.327] (0.284)
Step: 12599, Reward: [-1476.621 -1476.621 -1476.621] [0.0000], Avg: [-648.626 -648.626 -648.626] (0.283)
Step: 12649, Reward: [-1405.31 -1405.31 -1405.31] [0.0000], Avg: [-651.617 -651.617 -651.617] (0.281)
Step: 12699, Reward: [-1415.993 -1415.993 -1415.993] [0.0000], Avg: [-654.626 -654.626 -654.626] (0.280)
Step: 12749, Reward: [-1400.849 -1400.849 -1400.849] [0.0000], Avg: [-657.552 -657.552 -657.552] (0.279)
Step: 12799, Reward: [-1189.214 -1189.214 -1189.214] [0.0000], Avg: [-659.629 -659.629 -659.629] (0.277)
Step: 12849, Reward: [-1689.835 -1689.835 -1689.835] [0.0000], Avg: [-663.638 -663.638 -663.638] (0.276)
Step: 12899, Reward: [-949.553 -949.553 -949.553] [0.0000], Avg: [-664.746 -664.746 -664.746] (0.274)
Step: 12949, Reward: [-899.954 -899.954 -899.954] [0.0000], Avg: [-665.654 -665.654 -665.654] (0.273)
Step: 12999, Reward: [-1552.797 -1552.797 -1552.797] [0.0000], Avg: [-669.066 -669.066 -669.066] (0.272)
Step: 13049, Reward: [-1388.952 -1388.952 -1388.952] [0.0000], Avg: [-671.824 -671.824 -671.824] (0.270)
