Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01
DISCOUNT_RATE = 0.95

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			target_value = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			actual_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (actual_value - target_value.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			action_probs = agent.get_action(states[i], grad=True, numpy=False)
			action = [gsoftmax(action_probs, hard=True) if i==j else one_hot(model.get_action(state, numpy=False)) for (j,model), state in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -agent.critic_local(states_joint, action_joint).mean() + e_weight*action_probs.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.replay_buffer.push(state, action, next_state, reward, done)
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.time % self.update_freq)==0:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
			self.network.optimize(states, actions, next_states, rewards, dones)
		self.time += 1

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-506.285 -506.285 -506.285] [0.0000], Avg: [-506.285 -506.285 -506.285] (1.000)
Step: 99, Reward: [-535.114 -535.114 -535.114] [0.0000], Avg: [-520.7 -520.7 -520.7] (1.000)
Step: 149, Reward: [-415.732 -415.732 -415.732] [0.0000], Avg: [-485.711 -485.711 -485.711] (1.000)
Step: 199, Reward: [-457.665 -457.665 -457.665] [0.0000], Avg: [-478.699 -478.699 -478.699] (1.000)
Step: 249, Reward: [-487.029 -487.029 -487.029] [0.0000], Avg: [-480.365 -480.365 -480.365] (1.000)
Step: 299, Reward: [-453.37 -453.37 -453.37] [0.0000], Avg: [-475.866 -475.866 -475.866] (1.000)
Step: 349, Reward: [-521.958 -521.958 -521.958] [0.0000], Avg: [-482.451 -482.451 -482.451] (1.000)
Step: 399, Reward: [-724.166 -724.166 -724.166] [0.0000], Avg: [-512.665 -512.665 -512.665] (1.000)
Step: 449, Reward: [-691.357 -691.357 -691.357] [0.0000], Avg: [-532.52 -532.52 -532.52] (1.000)
Step: 499, Reward: [-560.36 -560.36 -560.36] [0.0000], Avg: [-535.304 -535.304 -535.304] (1.000)
Step: 549, Reward: [-855.161 -855.161 -855.161] [0.0000], Avg: [-564.382 -564.382 -564.382] (1.000)
Step: 599, Reward: [-437.787 -437.787 -437.787] [0.0000], Avg: [-553.832 -553.832 -553.832] (1.000)
Step: 649, Reward: [-551.051 -551.051 -551.051] [0.0000], Avg: [-553.618 -553.618 -553.618] (1.000)
Step: 699, Reward: [-665.75 -665.75 -665.75] [0.0000], Avg: [-561.628 -561.628 -561.628] (1.000)
Step: 749, Reward: [-656.77 -656.77 -656.77] [0.0000], Avg: [-567.97 -567.97 -567.97] (1.000)
Step: 799, Reward: [-558.034 -558.034 -558.034] [0.0000], Avg: [-567.349 -567.349 -567.349] (1.000)
Step: 849, Reward: [-476.115 -476.115 -476.115] [0.0000], Avg: [-561.983 -561.983 -561.983] (1.000)
Step: 899, Reward: [-422.506 -422.506 -422.506] [0.0000], Avg: [-554.234 -554.234 -554.234] (1.000)
Step: 949, Reward: [-660.539 -660.539 -660.539] [0.0000], Avg: [-559.829 -559.829 -559.829] (1.000)
Step: 999, Reward: [-512.923 -512.923 -512.923] [0.0000], Avg: [-557.484 -557.484 -557.484] (1.000)
Step: 1049, Reward: [-334.884 -334.884 -334.884] [0.0000], Avg: [-546.884 -546.884 -546.884] (1.000)
Step: 1099, Reward: [-558.634 -558.634 -558.634] [0.0000], Avg: [-547.418 -547.418 -547.418] (1.000)
Step: 1149, Reward: [-529.855 -529.855 -529.855] [0.0000], Avg: [-546.654 -546.654 -546.654] (1.000)
Step: 1199, Reward: [-386.85 -386.85 -386.85] [0.0000], Avg: [-539.996 -539.996 -539.996] (1.000)
Step: 1249, Reward: [-524.493 -524.493 -524.493] [0.0000], Avg: [-539.376 -539.376 -539.376] (1.000)
Step: 1299, Reward: [-444.352 -444.352 -444.352] [0.0000], Avg: [-535.721 -535.721 -535.721] (1.000)
Step: 1349, Reward: [-872.364 -872.364 -872.364] [0.0000], Avg: [-548.189 -548.189 -548.189] (1.000)
Step: 1399, Reward: [-457.449 -457.449 -457.449] [0.0000], Avg: [-544.948 -544.948 -544.948] (1.000)
Step: 1449, Reward: [-662.973 -662.973 -662.973] [0.0000], Avg: [-549.018 -549.018 -549.018] (1.000)
Step: 1499, Reward: [-869.111 -869.111 -869.111] [0.0000], Avg: [-559.688 -559.688 -559.688] (1.000)
Step: 1549, Reward: [-763.675 -763.675 -763.675] [0.0000], Avg: [-566.268 -566.268 -566.268] (1.000)
Step: 1599, Reward: [-1044.288 -1044.288 -1044.288] [0.0000], Avg: [-581.206 -581.206 -581.206] (1.000)
Step: 1649, Reward: [-914.168 -914.168 -914.168] [0.0000], Avg: [-591.296 -591.296 -591.296] (1.000)
Step: 1699, Reward: [-1115.531 -1115.531 -1115.531] [0.0000], Avg: [-606.715 -606.715 -606.715] (1.000)
Step: 1749, Reward: [-883.07 -883.07 -883.07] [0.0000], Avg: [-614.611 -614.611 -614.611] (1.000)
Step: 1799, Reward: [-1179.34 -1179.34 -1179.34] [0.0000], Avg: [-630.297 -630.297 -630.297] (1.000)
Step: 1849, Reward: [-1103.248 -1103.248 -1103.248] [0.0000], Avg: [-643.08 -643.08 -643.08] (1.000)
Step: 1899, Reward: [-1208.913 -1208.913 -1208.913] [0.0000], Avg: [-657.97 -657.97 -657.97] (1.000)
Step: 1949, Reward: [-1077.246 -1077.246 -1077.246] [0.0000], Avg: [-668.721 -668.721 -668.721] (1.000)
Step: 1999, Reward: [-1224.404 -1224.404 -1224.404] [0.0000], Avg: [-682.613 -682.613 -682.613] (1.000)
Step: 2049, Reward: [-1137.433 -1137.433 -1137.433] [0.0000], Avg: [-693.706 -693.706 -693.706] (1.000)
Step: 2099, Reward: [-922.27 -922.27 -922.27] [0.0000], Avg: [-699.148 -699.148 -699.148] (1.000)
Step: 2149, Reward: [-1235.321 -1235.321 -1235.321] [0.0000], Avg: [-711.617 -711.617 -711.617] (1.000)
Step: 2199, Reward: [-1629.449 -1629.449 -1629.449] [0.0000], Avg: [-732.477 -732.477 -732.477] (1.000)
Step: 2249, Reward: [-1197.633 -1197.633 -1197.633] [0.0000], Avg: [-742.814 -742.814 -742.814] (1.000)
Step: 2299, Reward: [-1468.164 -1468.164 -1468.164] [0.0000], Avg: [-758.582 -758.582 -758.582] (1.000)
Step: 2349, Reward: [-609.234 -609.234 -609.234] [0.0000], Avg: [-755.405 -755.405 -755.405] (1.000)
Step: 2399, Reward: [-990.133 -990.133 -990.133] [0.0000], Avg: [-760.295 -760.295 -760.295] (1.000)
Step: 2449, Reward: [-1161.87 -1161.87 -1161.87] [0.0000], Avg: [-768.49 -768.49 -768.49] (1.000)
Step: 2499, Reward: [-719.584 -719.584 -719.584] [0.0000], Avg: [-767.512 -767.512 -767.512] (1.000)
Step: 2549, Reward: [-742.772 -742.772 -742.772] [0.0000], Avg: [-767.027 -767.027 -767.027] (1.000)
Step: 2599, Reward: [-975.966 -975.966 -975.966] [0.0000], Avg: [-771.045 -771.045 -771.045] (1.000)
Step: 2649, Reward: [-619.385 -619.385 -619.385] [0.0000], Avg: [-768.184 -768.184 -768.184] (1.000)
Step: 2699, Reward: [-937.803 -937.803 -937.803] [0.0000], Avg: [-771.325 -771.325 -771.325] (1.000)
Step: 2749, Reward: [-434.336 -434.336 -434.336] [0.0000], Avg: [-765.198 -765.198 -765.198] (1.000)
Step: 2799, Reward: [-708.874 -708.874 -708.874] [0.0000], Avg: [-764.192 -764.192 -764.192] (1.000)
Step: 2849, Reward: [-1131.802 -1131.802 -1131.802] [0.0000], Avg: [-770.641 -770.641 -770.641] (1.000)
Step: 2899, Reward: [-550.914 -550.914 -550.914] [0.0000], Avg: [-766.853 -766.853 -766.853] (1.000)
Step: 2949, Reward: [-665.39 -665.39 -665.39] [0.0000], Avg: [-765.133 -765.133 -765.133] (1.000)
Step: 2999, Reward: [-750.949 -750.949 -750.949] [0.0000], Avg: [-764.897 -764.897 -764.897] (1.000)
Step: 3049, Reward: [-744.457 -744.457 -744.457] [0.0000], Avg: [-764.562 -764.562 -764.562] (1.000)
Step: 3099, Reward: [-973.007 -973.007 -973.007] [0.0000], Avg: [-767.924 -767.924 -767.924] (1.000)
Step: 3149, Reward: [-720.028 -720.028 -720.028] [0.0000], Avg: [-767.163 -767.163 -767.163] (1.000)
Step: 3199, Reward: [-677.919 -677.919 -677.919] [0.0000], Avg: [-765.769 -765.769 -765.769] (1.000)
Step: 3249, Reward: [-691.627 -691.627 -691.627] [0.0000], Avg: [-764.628 -764.628 -764.628] (1.000)
Step: 3299, Reward: [-666.854 -666.854 -666.854] [0.0000], Avg: [-763.147 -763.147 -763.147] (1.000)
Step: 3349, Reward: [-610.472 -610.472 -610.472] [0.0000], Avg: [-760.868 -760.868 -760.868] (1.000)
Step: 3399, Reward: [-701.724 -701.724 -701.724] [0.0000], Avg: [-759.998 -759.998 -759.998] (1.000)
Step: 3449, Reward: [-671.007 -671.007 -671.007] [0.0000], Avg: [-758.709 -758.709 -758.709] (1.000)
Step: 3499, Reward: [-649.606 -649.606 -649.606] [0.0000], Avg: [-757.15 -757.15 -757.15] (1.000)
Step: 3549, Reward: [-899.973 -899.973 -899.973] [0.0000], Avg: [-759.162 -759.162 -759.162] (1.000)
Step: 3599, Reward: [-897.658 -897.658 -897.658] [0.0000], Avg: [-761.085 -761.085 -761.085] (1.000)
Step: 3649, Reward: [-671.742 -671.742 -671.742] [0.0000], Avg: [-759.861 -759.861 -759.861] (1.000)
Step: 3699, Reward: [-949.655 -949.655 -949.655] [0.0000], Avg: [-762.426 -762.426 -762.426] (1.000)
Step: 3749, Reward: [-1302.905 -1302.905 -1302.905] [0.0000], Avg: [-769.632 -769.632 -769.632] (1.000)
Step: 3799, Reward: [-1561.733 -1561.733 -1561.733] [0.0000], Avg: [-780.055 -780.055 -780.055] (1.000)
Step: 3849, Reward: [-1603.404 -1603.404 -1603.404] [0.0000], Avg: [-790.748 -790.748 -790.748] (1.000)
Step: 3899, Reward: [-1090.952 -1090.952 -1090.952] [0.0000], Avg: [-794.596 -794.596 -794.596] (1.000)
Step: 3949, Reward: [-1461.129 -1461.129 -1461.129] [0.0000], Avg: [-803.034 -803.034 -803.034] (1.000)
Step: 3999, Reward: [-1848.964 -1848.964 -1848.964] [0.0000], Avg: [-816.108 -816.108 -816.108] (1.000)
Step: 4049, Reward: [-1672.84 -1672.84 -1672.84] [0.0000], Avg: [-826.685 -826.685 -826.685] (1.000)
Step: 4099, Reward: [-1305.881 -1305.881 -1305.881] [0.0000], Avg: [-832.529 -832.529 -832.529] (1.000)
Step: 4149, Reward: [-1994.574 -1994.574 -1994.574] [0.0000], Avg: [-846.529 -846.529 -846.529] (1.000)
Step: 4199, Reward: [-1499.326 -1499.326 -1499.326] [0.0000], Avg: [-854.3 -854.3 -854.3] (1.000)
Step: 4249, Reward: [-1602.079 -1602.079 -1602.079] [0.0000], Avg: [-863.098 -863.098 -863.098] (1.000)
Step: 4299, Reward: [-1943.261 -1943.261 -1943.261] [0.0000], Avg: [-875.658 -875.658 -875.658] (1.000)
Step: 4349, Reward: [-1736.312 -1736.312 -1736.312] [0.0000], Avg: [-885.551 -885.551 -885.551] (1.000)
Step: 4399, Reward: [-1726.918 -1726.918 -1726.918] [0.0000], Avg: [-895.111 -895.111 -895.111] (1.000)
Step: 4449, Reward: [-1593.707 -1593.707 -1593.707] [0.0000], Avg: [-902.961 -902.961 -902.961] (1.000)
Step: 4499, Reward: [-1599.324 -1599.324 -1599.324] [0.0000], Avg: [-910.698 -910.698 -910.698] (1.000)
Step: 4549, Reward: [-1594.595 -1594.595 -1594.595] [0.0000], Avg: [-918.214 -918.214 -918.214] (1.000)
Step: 4599, Reward: [-2083.631 -2083.631 -2083.631] [0.0000], Avg: [-930.881 -930.881 -930.881] (1.000)
Step: 4649, Reward: [-1462.967 -1462.967 -1462.967] [0.0000], Avg: [-936.603 -936.603 -936.603] (1.000)
Step: 4699, Reward: [-2038.884 -2038.884 -2038.884] [0.0000], Avg: [-948.329 -948.329 -948.329] (1.000)
Step: 4749, Reward: [-2035.714 -2035.714 -2035.714] [0.0000], Avg: [-959.775 -959.775 -959.775] (1.000)
Step: 4799, Reward: [-1960.274 -1960.274 -1960.274] [0.0000], Avg: [-970.197 -970.197 -970.197] (1.000)
Step: 4849, Reward: [-1173.879 -1173.879 -1173.879] [0.0000], Avg: [-972.297 -972.297 -972.297] (1.000)
Step: 4899, Reward: [-1521.007 -1521.007 -1521.007] [0.0000], Avg: [-977.896 -977.896 -977.896] (1.000)
Step: 4949, Reward: [-1343.407 -1343.407 -1343.407] [0.0000], Avg: [-981.588 -981.588 -981.588] (1.000)
Step: 4999, Reward: [-1845.201 -1845.201 -1845.201] [0.0000], Avg: [-990.224 -990.224 -990.224] (1.000)
Step: 5049, Reward: [-1100.487 -1100.487 -1100.487] [0.0000], Avg: [-991.316 -991.316 -991.316] (1.000)
Step: 5099, Reward: [-1274.866 -1274.866 -1274.866] [0.0000], Avg: [-994.096 -994.096 -994.096] (1.000)
Step: 5149, Reward: [-1162.867 -1162.867 -1162.867] [0.0000], Avg: [-995.734 -995.734 -995.734] (1.000)
Step: 5199, Reward: [-1255.788 -1255.788 -1255.788] [0.0000], Avg: [-998.235 -998.235 -998.235] (1.000)
Step: 5249, Reward: [-1378.819 -1378.819 -1378.819] [0.0000], Avg: [-1001.859 -1001.859 -1001.859] (1.000)
Step: 5299, Reward: [-2166.444 -2166.444 -2166.444] [0.0000], Avg: [-1012.846 -1012.846 -1012.846] (1.000)
Step: 5349, Reward: [-1676.476 -1676.476 -1676.476] [0.0000], Avg: [-1019.048 -1019.048 -1019.048] (1.000)
Step: 5399, Reward: [-1431.527 -1431.527 -1431.527] [0.0000], Avg: [-1022.867 -1022.867 -1022.867] (1.000)
Step: 5449, Reward: [-1828.595 -1828.595 -1828.595] [0.0000], Avg: [-1030.259 -1030.259 -1030.259] (1.000)
Step: 5499, Reward: [-1780.419 -1780.419 -1780.419] [0.0000], Avg: [-1037.079 -1037.079 -1037.079] (1.000)
Step: 5549, Reward: [-1902.583 -1902.583 -1902.583] [0.0000], Avg: [-1044.876 -1044.876 -1044.876] (1.000)
Step: 5599, Reward: [-1354.072 -1354.072 -1354.072] [0.0000], Avg: [-1047.637 -1047.637 -1047.637] (1.000)
Step: 5649, Reward: [-1636.391 -1636.391 -1636.391] [0.0000], Avg: [-1052.847 -1052.847 -1052.847] (1.000)
Step: 5699, Reward: [-2091.805 -2091.805 -2091.805] [0.0000], Avg: [-1061.961 -1061.961 -1061.961] (1.000)
Step: 5749, Reward: [-1193.71 -1193.71 -1193.71] [0.0000], Avg: [-1063.107 -1063.107 -1063.107] (1.000)
Step: 5799, Reward: [-1856.587 -1856.587 -1856.587] [0.0000], Avg: [-1069.947 -1069.947 -1069.947] (1.000)
Step: 5849, Reward: [-2012.838 -2012.838 -2012.838] [0.0000], Avg: [-1078.006 -1078.006 -1078.006] (1.000)
Step: 5899, Reward: [-1089.436 -1089.436 -1089.436] [0.0000], Avg: [-1078.103 -1078.103 -1078.103] (1.000)
Step: 5949, Reward: [-2146.104 -2146.104 -2146.104] [0.0000], Avg: [-1087.077 -1087.077 -1087.077] (1.000)
Step: 5999, Reward: [-2184.308 -2184.308 -2184.308] [0.0000], Avg: [-1096.221 -1096.221 -1096.221] (1.000)
Step: 6049, Reward: [-2003.175 -2003.175 -2003.175] [0.0000], Avg: [-1103.717 -1103.717 -1103.717] (1.000)
Step: 6099, Reward: [-1920.481 -1920.481 -1920.481] [0.0000], Avg: [-1110.411 -1110.411 -1110.411] (1.000)
Step: 6149, Reward: [-1287.149 -1287.149 -1287.149] [0.0000], Avg: [-1111.848 -1111.848 -1111.848] (1.000)
Step: 6199, Reward: [-1407.744 -1407.744 -1407.744] [0.0000], Avg: [-1114.234 -1114.234 -1114.234] (1.000)
Step: 6249, Reward: [-1830.876 -1830.876 -1830.876] [0.0000], Avg: [-1119.968 -1119.968 -1119.968] (1.000)
Step: 6299, Reward: [-1920.871 -1920.871 -1920.871] [0.0000], Avg: [-1126.324 -1126.324 -1126.324] (1.000)
Step: 6349, Reward: [-1319.794 -1319.794 -1319.794] [0.0000], Avg: [-1127.847 -1127.847 -1127.847] (1.000)
Step: 6399, Reward: [-1415.501 -1415.501 -1415.501] [0.0000], Avg: [-1130.095 -1130.095 -1130.095] (1.000)
Step: 6449, Reward: [-1557.923 -1557.923 -1557.923] [0.0000], Avg: [-1133.411 -1133.411 -1133.411] (1.000)
Step: 6499, Reward: [-1036.733 -1036.733 -1036.733] [0.0000], Avg: [-1132.667 -1132.667 -1132.667] (1.000)
Step: 6549, Reward: [-1678.69 -1678.69 -1678.69] [0.0000], Avg: [-1136.836 -1136.836 -1136.836] (1.000)
Step: 6599, Reward: [-1786.564 -1786.564 -1786.564] [0.0000], Avg: [-1141.758 -1141.758 -1141.758] (1.000)
Step: 6649, Reward: [-1985.025 -1985.025 -1985.025] [0.0000], Avg: [-1148.098 -1148.098 -1148.098] (1.000)
Step: 6699, Reward: [-1983.535 -1983.535 -1983.535] [0.0000], Avg: [-1154.333 -1154.333 -1154.333] (1.000)
Step: 6749, Reward: [-1982.086 -1982.086 -1982.086] [0.0000], Avg: [-1160.464 -1160.464 -1160.464] (1.000)
Step: 6799, Reward: [-1789.202 -1789.202 -1789.202] [0.0000], Avg: [-1165.087 -1165.087 -1165.087] (1.000)
Step: 6849, Reward: [-2099.53 -2099.53 -2099.53] [0.0000], Avg: [-1171.908 -1171.908 -1171.908] (1.000)
Step: 6899, Reward: [-2150.166 -2150.166 -2150.166] [0.0000], Avg: [-1178.997 -1178.997 -1178.997] (1.000)
Step: 6949, Reward: [-1948.75 -1948.75 -1948.75] [0.0000], Avg: [-1184.535 -1184.535 -1184.535] (1.000)
Step: 6999, Reward: [-1990.334 -1990.334 -1990.334] [0.0000], Avg: [-1190.29 -1190.29 -1190.29] (1.000)
Step: 7049, Reward: [-848.644 -848.644 -848.644] [0.0000], Avg: [-1187.867 -1187.867 -1187.867] (1.000)
Step: 7099, Reward: [-1803.729 -1803.729 -1803.729] [0.0000], Avg: [-1192.204 -1192.204 -1192.204] (1.000)
Step: 7149, Reward: [-2137.323 -2137.323 -2137.323] [0.0000], Avg: [-1198.814 -1198.814 -1198.814] (1.000)
Step: 7199, Reward: [-1036.53 -1036.53 -1036.53] [0.0000], Avg: [-1197.687 -1197.687 -1197.687] (1.000)
Step: 7249, Reward: [-1358.828 -1358.828 -1358.828] [0.0000], Avg: [-1198.798 -1198.798 -1198.798] (1.000)
Step: 7299, Reward: [-1243.103 -1243.103 -1243.103] [0.0000], Avg: [-1199.101 -1199.101 -1199.101] (1.000)
Step: 7349, Reward: [-1408.463 -1408.463 -1408.463] [0.0000], Avg: [-1200.526 -1200.526 -1200.526] (1.000)
Step: 7399, Reward: [-1638.092 -1638.092 -1638.092] [0.0000], Avg: [-1203.482 -1203.482 -1203.482] (1.000)
Step: 7449, Reward: [-1371.003 -1371.003 -1371.003] [0.0000], Avg: [-1204.606 -1204.606 -1204.606] (1.000)
Step: 7499, Reward: [-1723.726 -1723.726 -1723.726] [0.0000], Avg: [-1208.067 -1208.067 -1208.067] (1.000)
Step: 7549, Reward: [-1992.87 -1992.87 -1992.87] [0.0000], Avg: [-1213.265 -1213.265 -1213.265] (1.000)
Step: 7599, Reward: [-1888.802 -1888.802 -1888.802] [0.0000], Avg: [-1217.709 -1217.709 -1217.709] (1.000)
Step: 7649, Reward: [-1716.694 -1716.694 -1716.694] [0.0000], Avg: [-1220.97 -1220.97 -1220.97] (1.000)
Step: 7699, Reward: [-1049.381 -1049.381 -1049.381] [0.0000], Avg: [-1219.856 -1219.856 -1219.856] (1.000)
Step: 7749, Reward: [-979.187 -979.187 -979.187] [0.0000], Avg: [-1218.303 -1218.303 -1218.303] (1.000)
Step: 7799, Reward: [-698.831 -698.831 -698.831] [0.0000], Avg: [-1214.973 -1214.973 -1214.973] (1.000)
Step: 7849, Reward: [-2138.433 -2138.433 -2138.433] [0.0000], Avg: [-1220.855 -1220.855 -1220.855] (1.000)
Step: 7899, Reward: [-1763.703 -1763.703 -1763.703] [0.0000], Avg: [-1224.291 -1224.291 -1224.291] (1.000)
Step: 7949, Reward: [-974.162 -974.162 -974.162] [0.0000], Avg: [-1222.718 -1222.718 -1222.718] (1.000)
Step: 7999, Reward: [-1692.749 -1692.749 -1692.749] [0.0000], Avg: [-1225.656 -1225.656 -1225.656] (1.000)
Step: 8049, Reward: [-741.402 -741.402 -741.402] [0.0000], Avg: [-1222.648 -1222.648 -1222.648] (1.000)
Step: 8099, Reward: [-869.282 -869.282 -869.282] [0.0000], Avg: [-1220.467 -1220.467 -1220.467] (1.000)
Step: 8149, Reward: [-1281.564 -1281.564 -1281.564] [0.0000], Avg: [-1220.841 -1220.841 -1220.841] (1.000)
Step: 8199, Reward: [-1044.302 -1044.302 -1044.302] [0.0000], Avg: [-1219.765 -1219.765 -1219.765] (1.000)
Step: 8249, Reward: [-1172.236 -1172.236 -1172.236] [0.0000], Avg: [-1219.477 -1219.477 -1219.477] (1.000)
Step: 8299, Reward: [-1767.542 -1767.542 -1767.542] [0.0000], Avg: [-1222.779 -1222.779 -1222.779] (1.000)
Step: 8349, Reward: [-1834.017 -1834.017 -1834.017] [0.0000], Avg: [-1226.439 -1226.439 -1226.439] (1.000)
Step: 8399, Reward: [-1373.408 -1373.408 -1373.408] [0.0000], Avg: [-1227.313 -1227.313 -1227.313] (1.000)
Step: 8449, Reward: [-1692.509 -1692.509 -1692.509] [0.0000], Avg: [-1230.066 -1230.066 -1230.066] (1.000)
Step: 8499, Reward: [-1595.452 -1595.452 -1595.452] [0.0000], Avg: [-1232.215 -1232.215 -1232.215] (1.000)
Step: 8549, Reward: [-1626.476 -1626.476 -1626.476] [0.0000], Avg: [-1234.521 -1234.521 -1234.521] (1.000)
Step: 8599, Reward: [-1154.012 -1154.012 -1154.012] [0.0000], Avg: [-1234.053 -1234.053 -1234.053] (1.000)
Step: 8649, Reward: [-1268.527 -1268.527 -1268.527] [0.0000], Avg: [-1234.252 -1234.252 -1234.252] (1.000)
Step: 8699, Reward: [-1884.784 -1884.784 -1884.784] [0.0000], Avg: [-1237.991 -1237.991 -1237.991] (1.000)
Step: 8749, Reward: [-1566.229 -1566.229 -1566.229] [0.0000], Avg: [-1239.867 -1239.867 -1239.867] (1.000)
Step: 8799, Reward: [-1798.069 -1798.069 -1798.069] [0.0000], Avg: [-1243.038 -1243.038 -1243.038] (1.000)
Step: 8849, Reward: [-1340.419 -1340.419 -1340.419] [0.0000], Avg: [-1243.588 -1243.588 -1243.588] (1.000)
Step: 8899, Reward: [-1096.514 -1096.514 -1096.514] [0.0000], Avg: [-1242.762 -1242.762 -1242.762] (1.000)
Step: 8949, Reward: [-1585.384 -1585.384 -1585.384] [0.0000], Avg: [-1244.676 -1244.676 -1244.676] (1.000)
Step: 8999, Reward: [-927.422 -927.422 -927.422] [0.0000], Avg: [-1242.914 -1242.914 -1242.914] (1.000)
Step: 9049, Reward: [-1401.626 -1401.626 -1401.626] [0.0000], Avg: [-1243.79 -1243.79 -1243.79] (1.000)
Step: 9099, Reward: [-1063.66 -1063.66 -1063.66] [0.0000], Avg: [-1242.801 -1242.801 -1242.801] (1.000)
Step: 9149, Reward: [-753.477 -753.477 -753.477] [0.0000], Avg: [-1240.127 -1240.127 -1240.127] (1.000)
Step: 9199, Reward: [-860.978 -860.978 -860.978] [0.0000], Avg: [-1238.066 -1238.066 -1238.066] (1.000)
Step: 9249, Reward: [-1153.504 -1153.504 -1153.504] [0.0000], Avg: [-1237.609 -1237.609 -1237.609] (1.000)
Step: 9299, Reward: [-1569.436 -1569.436 -1569.436] [0.0000], Avg: [-1239.393 -1239.393 -1239.393] (1.000)
Step: 9349, Reward: [-1745.899 -1745.899 -1745.899] [0.0000], Avg: [-1242.102 -1242.102 -1242.102] (1.000)
Step: 9399, Reward: [-1527.221 -1527.221 -1527.221] [0.0000], Avg: [-1243.618 -1243.618 -1243.618] (1.000)
Step: 9449, Reward: [-868.098 -868.098 -868.098] [0.0000], Avg: [-1241.631 -1241.631 -1241.631] (1.000)
Step: 9499, Reward: [-1835.43 -1835.43 -1835.43] [0.0000], Avg: [-1244.757 -1244.757 -1244.757] (1.000)
Step: 9549, Reward: [-1530.003 -1530.003 -1530.003] [0.0000], Avg: [-1246.25 -1246.25 -1246.25] (1.000)
Step: 9599, Reward: [-817.398 -817.398 -817.398] [0.0000], Avg: [-1244.017 -1244.017 -1244.017] (1.000)
Step: 9649, Reward: [-1070.375 -1070.375 -1070.375] [0.0000], Avg: [-1243.117 -1243.117 -1243.117] (1.000)
Step: 9699, Reward: [-1700.696 -1700.696 -1700.696] [0.0000], Avg: [-1245.476 -1245.476 -1245.476] (1.000)
Step: 9749, Reward: [-1266.02 -1266.02 -1266.02] [0.0000], Avg: [-1245.581 -1245.581 -1245.581] (1.000)
Step: 9799, Reward: [-1117.403 -1117.403 -1117.403] [0.0000], Avg: [-1244.927 -1244.927 -1244.927] (1.000)
Step: 9849, Reward: [-2051.15 -2051.15 -2051.15] [0.0000], Avg: [-1249.019 -1249.019 -1249.019] (1.000)
Step: 9899, Reward: [-916.988 -916.988 -916.988] [0.0000], Avg: [-1247.343 -1247.343 -1247.343] (1.000)
Step: 9949, Reward: [-1307.315 -1307.315 -1307.315] [0.0000], Avg: [-1247.644 -1247.644 -1247.644] (1.000)
Step: 9999, Reward: [-1336.846 -1336.846 -1336.846] [0.0000], Avg: [-1248.09 -1248.09 -1248.09] (1.000)
Step: 10049, Reward: [-1736.505 -1736.505 -1736.505] [0.0000], Avg: [-1250.52 -1250.52 -1250.52] (1.000)
Step: 10099, Reward: [-799.174 -799.174 -799.174] [0.0000], Avg: [-1248.285 -1248.285 -1248.285] (1.000)
Step: 10149, Reward: [-1846.586 -1846.586 -1846.586] [0.0000], Avg: [-1251.233 -1251.233 -1251.233] (1.000)
Step: 10199, Reward: [-1301.839 -1301.839 -1301.839] [0.0000], Avg: [-1251.481 -1251.481 -1251.481] (1.000)
Step: 10249, Reward: [-1416.86 -1416.86 -1416.86] [0.0000], Avg: [-1252.288 -1252.288 -1252.288] (1.000)
Step: 10299, Reward: [-1870.372 -1870.372 -1870.372] [0.0000], Avg: [-1255.288 -1255.288 -1255.288] (1.000)
Step: 10349, Reward: [-1465.445 -1465.445 -1465.445] [0.0000], Avg: [-1256.303 -1256.303 -1256.303] (1.000)
Step: 10399, Reward: [-2012.243 -2012.243 -2012.243] [0.0000], Avg: [-1259.938 -1259.938 -1259.938] (1.000)
Step: 10449, Reward: [-1648.568 -1648.568 -1648.568] [0.0000], Avg: [-1261.797 -1261.797 -1261.797] (1.000)
Step: 10499, Reward: [-1060.499 -1060.499 -1060.499] [0.0000], Avg: [-1260.838 -1260.838 -1260.838] (1.000)
Step: 10549, Reward: [-794.036 -794.036 -794.036] [0.0000], Avg: [-1258.626 -1258.626 -1258.626] (1.000)
Step: 10599, Reward: [-1593.764 -1593.764 -1593.764] [0.0000], Avg: [-1260.207 -1260.207 -1260.207] (1.000)
Step: 10649, Reward: [-1364.102 -1364.102 -1364.102] [0.0000], Avg: [-1260.695 -1260.695 -1260.695] (1.000)
Step: 10699, Reward: [-1827.007 -1827.007 -1827.007] [0.0000], Avg: [-1263.341 -1263.341 -1263.341] (1.000)
Step: 10749, Reward: [-1887.252 -1887.252 -1887.252] [0.0000], Avg: [-1266.243 -1266.243 -1266.243] (1.000)
Step: 10799, Reward: [-1088.671 -1088.671 -1088.671] [0.0000], Avg: [-1265.421 -1265.421 -1265.421] (1.000)
Step: 10849, Reward: [-1199.31 -1199.31 -1199.31] [0.0000], Avg: [-1265.116 -1265.116 -1265.116] (1.000)
Step: 10899, Reward: [-1868.078 -1868.078 -1868.078] [0.0000], Avg: [-1267.882 -1267.882 -1267.882] (1.000)
Step: 10949, Reward: [-1600.084 -1600.084 -1600.084] [0.0000], Avg: [-1269.399 -1269.399 -1269.399] (1.000)
Step: 10999, Reward: [-1332.818 -1332.818 -1332.818] [0.0000], Avg: [-1269.687 -1269.687 -1269.687] (1.000)
Step: 11049, Reward: [-1389.962 -1389.962 -1389.962] [0.0000], Avg: [-1270.231 -1270.231 -1270.231] (1.000)
Step: 11099, Reward: [-1500.696 -1500.696 -1500.696] [0.0000], Avg: [-1271.27 -1271.27 -1271.27] (1.000)
Step: 11149, Reward: [-573.698 -573.698 -573.698] [0.0000], Avg: [-1268.141 -1268.141 -1268.141] (1.000)
Step: 11199, Reward: [-1792.659 -1792.659 -1792.659] [0.0000], Avg: [-1270.483 -1270.483 -1270.483] (1.000)
Step: 11249, Reward: [-1056.326 -1056.326 -1056.326] [0.0000], Avg: [-1269.531 -1269.531 -1269.531] (1.000)
Step: 11299, Reward: [-1151.625 -1151.625 -1151.625] [0.0000], Avg: [-1269.01 -1269.01 -1269.01] (1.000)
Step: 11349, Reward: [-902.767 -902.767 -902.767] [0.0000], Avg: [-1267.396 -1267.396 -1267.396] (1.000)
Step: 11399, Reward: [-1841.959 -1841.959 -1841.959] [0.0000], Avg: [-1269.916 -1269.916 -1269.916] (1.000)
Step: 11449, Reward: [-1743.837 -1743.837 -1743.837] [0.0000], Avg: [-1271.986 -1271.986 -1271.986] (1.000)
Step: 11499, Reward: [-1139.268 -1139.268 -1139.268] [0.0000], Avg: [-1271.409 -1271.409 -1271.409] (1.000)
Step: 11549, Reward: [-1494.325 -1494.325 -1494.325] [0.0000], Avg: [-1272.374 -1272.374 -1272.374] (1.000)
Step: 11599, Reward: [-1018.096 -1018.096 -1018.096] [0.0000], Avg: [-1271.278 -1271.278 -1271.278] (1.000)
Step: 11649, Reward: [-1530.162 -1530.162 -1530.162] [0.0000], Avg: [-1272.389 -1272.389 -1272.389] (1.000)
Step: 11699, Reward: [-1027.203 -1027.203 -1027.203] [0.0000], Avg: [-1271.341 -1271.341 -1271.341] (1.000)
Step: 11749, Reward: [-960.077 -960.077 -960.077] [0.0000], Avg: [-1270.016 -1270.016 -1270.016] (1.000)
Step: 11799, Reward: [-1185.095 -1185.095 -1185.095] [0.0000], Avg: [-1269.657 -1269.657 -1269.657] (1.000)
Step: 11849, Reward: [-1393.401 -1393.401 -1393.401] [0.0000], Avg: [-1270.179 -1270.179 -1270.179] (1.000)
Step: 11899, Reward: [-1003.747 -1003.747 -1003.747] [0.0000], Avg: [-1269.059 -1269.059 -1269.059] (1.000)
Step: 11949, Reward: [-795.276 -795.276 -795.276] [0.0000], Avg: [-1267.077 -1267.077 -1267.077] (1.000)
Step: 11999, Reward: [-1413.136 -1413.136 -1413.136] [0.0000], Avg: [-1267.685 -1267.685 -1267.685] (1.000)
Step: 12049, Reward: [-1215.672 -1215.672 -1215.672] [0.0000], Avg: [-1267.47 -1267.47 -1267.47] (1.000)
Step: 12099, Reward: [-1271.802 -1271.802 -1271.802] [0.0000], Avg: [-1267.488 -1267.488 -1267.488] (1.000)
Step: 12149, Reward: [-970.047 -970.047 -970.047] [0.0000], Avg: [-1266.263 -1266.263 -1266.263] (1.000)
Step: 12199, Reward: [-1382.598 -1382.598 -1382.598] [0.0000], Avg: [-1266.74 -1266.74 -1266.74] (1.000)
Step: 12249, Reward: [-886.564 -886.564 -886.564] [0.0000], Avg: [-1265.189 -1265.189 -1265.189] (1.000)
Step: 12299, Reward: [-1417.214 -1417.214 -1417.214] [0.0000], Avg: [-1265.807 -1265.807 -1265.807] (1.000)
Step: 12349, Reward: [-1150.594 -1150.594 -1150.594] [0.0000], Avg: [-1265.34 -1265.34 -1265.34] (1.000)
Step: 12399, Reward: [-1311.242 -1311.242 -1311.242] [0.0000], Avg: [-1265.525 -1265.525 -1265.525] (1.000)
Step: 12449, Reward: [-822.989 -822.989 -822.989] [0.0000], Avg: [-1263.748 -1263.748 -1263.748] (1.000)
Step: 12499, Reward: [-1595.675 -1595.675 -1595.675] [0.0000], Avg: [-1265.076 -1265.076 -1265.076] (1.000)
Step: 12549, Reward: [-913.708 -913.708 -913.708] [0.0000], Avg: [-1263.676 -1263.676 -1263.676] (1.000)
Step: 12599, Reward: [-749.242 -749.242 -749.242] [0.0000], Avg: [-1261.634 -1261.634 -1261.634] (1.000)
Step: 12649, Reward: [-1254.976 -1254.976 -1254.976] [0.0000], Avg: [-1261.608 -1261.608 -1261.608] (1.000)
Step: 12699, Reward: [-1079.064 -1079.064 -1079.064] [0.0000], Avg: [-1260.889 -1260.889 -1260.889] (1.000)
Step: 12749, Reward: [-656.74 -656.74 -656.74] [0.0000], Avg: [-1258.52 -1258.52 -1258.52] (1.000)
Step: 12799, Reward: [-979.142 -979.142 -979.142] [0.0000], Avg: [-1257.429 -1257.429 -1257.429] (1.000)
Step: 12849, Reward: [-1577.809 -1577.809 -1577.809] [0.0000], Avg: [-1258.675 -1258.675 -1258.675] (1.000)
Step: 12899, Reward: [-1167.501 -1167.501 -1167.501] [0.0000], Avg: [-1258.322 -1258.322 -1258.322] (1.000)
Step: 12949, Reward: [-835.609 -835.609 -835.609] [0.0000], Avg: [-1256.69 -1256.69 -1256.69] (1.000)
Step: 12999, Reward: [-1171.512 -1171.512 -1171.512] [0.0000], Avg: [-1256.362 -1256.362 -1256.362] (1.000)
Step: 13049, Reward: [-851.034 -851.034 -851.034] [0.0000], Avg: [-1254.809 -1254.809 -1254.809] (1.000)
Step: 13099, Reward: [-877.02 -877.02 -877.02] [0.0000], Avg: [-1253.367 -1253.367 -1253.367] (1.000)
Step: 13149, Reward: [-610.067 -610.067 -610.067] [0.0000], Avg: [-1250.921 -1250.921 -1250.921] (1.000)
Step: 13199, Reward: [-853.063 -853.063 -853.063] [0.0000], Avg: [-1249.414 -1249.414 -1249.414] (1.000)
Step: 13249, Reward: [-635.029 -635.029 -635.029] [0.0000], Avg: [-1247.096 -1247.096 -1247.096] (1.000)
Step: 13299, Reward: [-803.648 -803.648 -803.648] [0.0000], Avg: [-1245.429 -1245.429 -1245.429] (1.000)
Step: 13349, Reward: [-704.842 -704.842 -704.842] [0.0000], Avg: [-1243.404 -1243.404 -1243.404] (1.000)
Step: 13399, Reward: [-953.23 -953.23 -953.23] [0.0000], Avg: [-1242.321 -1242.321 -1242.321] (1.000)
Step: 13449, Reward: [-1134.934 -1134.934 -1134.934] [0.0000], Avg: [-1241.922 -1241.922 -1241.922] (1.000)
Step: 13499, Reward: [-1534.758 -1534.758 -1534.758] [0.0000], Avg: [-1243.007 -1243.007 -1243.007] (1.000)
Step: 13549, Reward: [-1618.254 -1618.254 -1618.254] [0.0000], Avg: [-1244.391 -1244.391 -1244.391] (1.000)
Step: 13599, Reward: [-1034.05 -1034.05 -1034.05] [0.0000], Avg: [-1243.618 -1243.618 -1243.618] (1.000)
Step: 13649, Reward: [-1479.815 -1479.815 -1479.815] [0.0000], Avg: [-1244.483 -1244.483 -1244.483] (1.000)
Step: 13699, Reward: [-722.945 -722.945 -722.945] [0.0000], Avg: [-1242.58 -1242.58 -1242.58] (1.000)
Step: 13749, Reward: [-1844.501 -1844.501 -1844.501] [0.0000], Avg: [-1244.769 -1244.769 -1244.769] (1.000)
Step: 13799, Reward: [-716.739 -716.739 -716.739] [0.0000], Avg: [-1242.856 -1242.856 -1242.856] (1.000)
Step: 13849, Reward: [-818.612 -818.612 -818.612] [0.0000], Avg: [-1241.324 -1241.324 -1241.324] (1.000)
Step: 13899, Reward: [-885.947 -885.947 -885.947] [0.0000], Avg: [-1240.046 -1240.046 -1240.046] (1.000)
Step: 13949, Reward: [-1359.074 -1359.074 -1359.074] [0.0000], Avg: [-1240.472 -1240.472 -1240.472] (1.000)
Step: 13999, Reward: [-689.549 -689.549 -689.549] [0.0000], Avg: [-1238.505 -1238.505 -1238.505] (1.000)
Step: 14049, Reward: [-721.97 -721.97 -721.97] [0.0000], Avg: [-1236.667 -1236.667 -1236.667] (1.000)
Step: 14099, Reward: [-681.381 -681.381 -681.381] [0.0000], Avg: [-1234.697 -1234.697 -1234.697] (1.000)
Step: 14149, Reward: [-1913.662 -1913.662 -1913.662] [0.0000], Avg: [-1237.097 -1237.097 -1237.097] (1.000)
Step: 14199, Reward: [-1002.287 -1002.287 -1002.287] [0.0000], Avg: [-1236.27 -1236.27 -1236.27] (1.000)
Step: 14249, Reward: [-726.782 -726.782 -726.782] [0.0000], Avg: [-1234.482 -1234.482 -1234.482] (1.000)
Step: 14299, Reward: [-880.827 -880.827 -880.827] [0.0000], Avg: [-1233.246 -1233.246 -1233.246] (1.000)
Step: 14349, Reward: [-797.892 -797.892 -797.892] [0.0000], Avg: [-1231.729 -1231.729 -1231.729] (1.000)
Step: 14399, Reward: [-1075.955 -1075.955 -1075.955] [0.0000], Avg: [-1231.188 -1231.188 -1231.188] (1.000)
Step: 14449, Reward: [-1352.781 -1352.781 -1352.781] [0.0000], Avg: [-1231.609 -1231.609 -1231.609] (1.000)
Step: 14499, Reward: [-1533.642 -1533.642 -1533.642] [0.0000], Avg: [-1232.65 -1232.65 -1232.65] (1.000)
Step: 14549, Reward: [-976.162 -976.162 -976.162] [0.0000], Avg: [-1231.769 -1231.769 -1231.769] (1.000)
Step: 14599, Reward: [-1217.534 -1217.534 -1217.534] [0.0000], Avg: [-1231.72 -1231.72 -1231.72] (1.000)
Step: 14649, Reward: [-1507.93 -1507.93 -1507.93] [0.0000], Avg: [-1232.663 -1232.663 -1232.663] (1.000)
Step: 14699, Reward: [-1286.084 -1286.084 -1286.084] [0.0000], Avg: [-1232.844 -1232.844 -1232.844] (1.000)
Step: 14749, Reward: [-1753.085 -1753.085 -1753.085] [0.0000], Avg: [-1234.608 -1234.608 -1234.608] (1.000)
Step: 14799, Reward: [-2018.936 -2018.936 -2018.936] [0.0000], Avg: [-1237.258 -1237.258 -1237.258] (1.000)
Step: 14849, Reward: [-657.353 -657.353 -657.353] [0.0000], Avg: [-1235.305 -1235.305 -1235.305] (1.000)
Step: 14899, Reward: [-886.36 -886.36 -886.36] [0.0000], Avg: [-1234.134 -1234.134 -1234.134] (1.000)
Step: 14949, Reward: [-757.258 -757.258 -757.258] [0.0000], Avg: [-1232.539 -1232.539 -1232.539] (1.000)
Step: 14999, Reward: [-832.754 -832.754 -832.754] [0.0000], Avg: [-1231.207 -1231.207 -1231.207] (1.000)
Step: 15049, Reward: [-1595.773 -1595.773 -1595.773] [0.0000], Avg: [-1232.418 -1232.418 -1232.418] (1.000)
Step: 15099, Reward: [-661.003 -661.003 -661.003] [0.0000], Avg: [-1230.526 -1230.526 -1230.526] (1.000)
Step: 15149, Reward: [-1301.201 -1301.201 -1301.201] [0.0000], Avg: [-1230.759 -1230.759 -1230.759] (1.000)
Step: 15199, Reward: [-1389.297 -1389.297 -1389.297] [0.0000], Avg: [-1231.28 -1231.28 -1231.28] (1.000)
Step: 15249, Reward: [-1353.239 -1353.239 -1353.239] [0.0000], Avg: [-1231.68 -1231.68 -1231.68] (1.000)
Step: 15299, Reward: [-1404.871 -1404.871 -1404.871] [0.0000], Avg: [-1232.246 -1232.246 -1232.246] (1.000)
Step: 15349, Reward: [-1010.086 -1010.086 -1010.086] [0.0000], Avg: [-1231.523 -1231.523 -1231.523] (1.000)
Step: 15399, Reward: [-1419.249 -1419.249 -1419.249] [0.0000], Avg: [-1232.132 -1232.132 -1232.132] (1.000)
Step: 15449, Reward: [-987.069 -987.069 -987.069] [0.0000], Avg: [-1231.339 -1231.339 -1231.339] (1.000)
Step: 15499, Reward: [-1535.064 -1535.064 -1535.064] [0.0000], Avg: [-1232.319 -1232.319 -1232.319] (1.000)
Step: 15549, Reward: [-987.935 -987.935 -987.935] [0.0000], Avg: [-1231.533 -1231.533 -1231.533] (1.000)
Step: 15599, Reward: [-1791.297 -1791.297 -1791.297] [0.0000], Avg: [-1233.327 -1233.327 -1233.327] (1.000)
Step: 15649, Reward: [-1053.652 -1053.652 -1053.652] [0.0000], Avg: [-1232.753 -1232.753 -1232.753] (1.000)
Step: 15699, Reward: [-1289.065 -1289.065 -1289.065] [0.0000], Avg: [-1232.932 -1232.932 -1232.932] (1.000)
Step: 15749, Reward: [-723.019 -723.019 -723.019] [0.0000], Avg: [-1231.314 -1231.314 -1231.314] (1.000)
Step: 15799, Reward: [-745.357 -745.357 -745.357] [0.0000], Avg: [-1229.776 -1229.776 -1229.776] (1.000)
Step: 15849, Reward: [-1125.459 -1125.459 -1125.459] [0.0000], Avg: [-1229.447 -1229.447 -1229.447] (1.000)
Step: 15899, Reward: [-938.439 -938.439 -938.439] [0.0000], Avg: [-1228.532 -1228.532 -1228.532] (1.000)
Step: 15949, Reward: [-1157.925 -1157.925 -1157.925] [0.0000], Avg: [-1228.31 -1228.31 -1228.31] (1.000)
Step: 15999, Reward: [-1016.297 -1016.297 -1016.297] [0.0000], Avg: [-1227.648 -1227.648 -1227.648] (1.000)
Step: 16049, Reward: [-1552.847 -1552.847 -1552.847] [0.0000], Avg: [-1228.661 -1228.661 -1228.661] (1.000)
Step: 16099, Reward: [-1031.539 -1031.539 -1031.539] [0.0000], Avg: [-1228.049 -1228.049 -1228.049] (1.000)
Step: 16149, Reward: [-1170.063 -1170.063 -1170.063] [0.0000], Avg: [-1227.869 -1227.869 -1227.869] (1.000)
Step: 16199, Reward: [-618.044 -618.044 -618.044] [0.0000], Avg: [-1225.987 -1225.987 -1225.987] (1.000)
Step: 16249, Reward: [-1433.561 -1433.561 -1433.561] [0.0000], Avg: [-1226.626 -1226.626 -1226.626] (1.000)
Step: 16299, Reward: [-1448.696 -1448.696 -1448.696] [0.0000], Avg: [-1227.307 -1227.307 -1227.307] (1.000)
Step: 16349, Reward: [-920.855 -920.855 -920.855] [0.0000], Avg: [-1226.37 -1226.37 -1226.37] (1.000)
Step: 16399, Reward: [-1038.231 -1038.231 -1038.231] [0.0000], Avg: [-1225.796 -1225.796 -1225.796] (1.000)
Step: 16449, Reward: [-1317.49 -1317.49 -1317.49] [0.0000], Avg: [-1226.075 -1226.075 -1226.075] (1.000)
Step: 16499, Reward: [-1582.068 -1582.068 -1582.068] [0.0000], Avg: [-1227.153 -1227.153 -1227.153] (1.000)
Step: 16549, Reward: [-770.722 -770.722 -770.722] [0.0000], Avg: [-1225.775 -1225.775 -1225.775] (1.000)
Step: 16599, Reward: [-1100.626 -1100.626 -1100.626] [0.0000], Avg: [-1225.398 -1225.398 -1225.398] (1.000)
Step: 16649, Reward: [-863.297 -863.297 -863.297] [0.0000], Avg: [-1224.31 -1224.31 -1224.31] (1.000)
Step: 16699, Reward: [-1250.905 -1250.905 -1250.905] [0.0000], Avg: [-1224.39 -1224.39 -1224.39] (1.000)
Step: 16749, Reward: [-854.32 -854.32 -854.32] [0.0000], Avg: [-1223.285 -1223.285 -1223.285] (1.000)
Step: 16799, Reward: [-979.534 -979.534 -979.534] [0.0000], Avg: [-1222.56 -1222.56 -1222.56] (1.000)
Step: 16849, Reward: [-1192.077 -1192.077 -1192.077] [0.0000], Avg: [-1222.469 -1222.469 -1222.469] (1.000)
Step: 16899, Reward: [-995.432 -995.432 -995.432] [0.0000], Avg: [-1221.798 -1221.798 -1221.798] (1.000)
Step: 16949, Reward: [-1048.83 -1048.83 -1048.83] [0.0000], Avg: [-1221.287 -1221.287 -1221.287] (1.000)
Step: 16999, Reward: [-957.422 -957.422 -957.422] [0.0000], Avg: [-1220.511 -1220.511 -1220.511] (1.000)
Step: 17049, Reward: [-1491.789 -1491.789 -1491.789] [0.0000], Avg: [-1221.307 -1221.307 -1221.307] (1.000)
Step: 17099, Reward: [-1060.019 -1060.019 -1060.019] [0.0000], Avg: [-1220.835 -1220.835 -1220.835] (1.000)
Step: 17149, Reward: [-1176.103 -1176.103 -1176.103] [0.0000], Avg: [-1220.705 -1220.705 -1220.705] (1.000)
Step: 17199, Reward: [-838.302 -838.302 -838.302] [0.0000], Avg: [-1219.593 -1219.593 -1219.593] (1.000)
Step: 17249, Reward: [-698.813 -698.813 -698.813] [0.0000], Avg: [-1218.084 -1218.084 -1218.084] (1.000)
Step: 17299, Reward: [-1269.357 -1269.357 -1269.357] [0.0000], Avg: [-1218.232 -1218.232 -1218.232] (1.000)
Step: 17349, Reward: [-890.917 -890.917 -890.917] [0.0000], Avg: [-1217.288 -1217.288 -1217.288] (1.000)
Step: 17399, Reward: [-1818.518 -1818.518 -1818.518] [0.0000], Avg: [-1219.016 -1219.016 -1219.016] (1.000)
Step: 17449, Reward: [-879.056 -879.056 -879.056] [0.0000], Avg: [-1218.042 -1218.042 -1218.042] (1.000)
Step: 17499, Reward: [-1261.783 -1261.783 -1261.783] [0.0000], Avg: [-1218.167 -1218.167 -1218.167] (1.000)
Step: 17549, Reward: [-971.822 -971.822 -971.822] [0.0000], Avg: [-1217.465 -1217.465 -1217.465] (1.000)
Step: 17599, Reward: [-1508.351 -1508.351 -1508.351] [0.0000], Avg: [-1218.292 -1218.292 -1218.292] (1.000)
Step: 17649, Reward: [-1167.378 -1167.378 -1167.378] [0.0000], Avg: [-1218.147 -1218.147 -1218.147] (1.000)
Step: 17699, Reward: [-789.993 -789.993 -789.993] [0.0000], Avg: [-1216.938 -1216.938 -1216.938] (1.000)
Step: 17749, Reward: [-1689.401 -1689.401 -1689.401] [0.0000], Avg: [-1218.269 -1218.269 -1218.269] (1.000)
Step: 17799, Reward: [-1494.191 -1494.191 -1494.191] [0.0000], Avg: [-1219.044 -1219.044 -1219.044] (1.000)
Step: 17849, Reward: [-2200.74 -2200.74 -2200.74] [0.0000], Avg: [-1221.794 -1221.794 -1221.794] (1.000)
Step: 17899, Reward: [-1096.524 -1096.524 -1096.524] [0.0000], Avg: [-1221.444 -1221.444 -1221.444] (1.000)
Step: 17949, Reward: [-1258.863 -1258.863 -1258.863] [0.0000], Avg: [-1221.548 -1221.548 -1221.548] (1.000)
Step: 17999, Reward: [-1584.82 -1584.82 -1584.82] [0.0000], Avg: [-1222.557 -1222.557 -1222.557] (1.000)
Step: 18049, Reward: [-1460.436 -1460.436 -1460.436] [0.0000], Avg: [-1223.216 -1223.216 -1223.216] (1.000)
Step: 18099, Reward: [-1307.143 -1307.143 -1307.143] [0.0000], Avg: [-1223.448 -1223.448 -1223.448] (1.000)
Step: 18149, Reward: [-649.726 -649.726 -649.726] [0.0000], Avg: [-1221.867 -1221.867 -1221.867] (1.000)
Step: 18199, Reward: [-747.315 -747.315 -747.315] [0.0000], Avg: [-1220.564 -1220.564 -1220.564] (1.000)
Step: 18249, Reward: [-956.989 -956.989 -956.989] [0.0000], Avg: [-1219.842 -1219.842 -1219.842] (1.000)
Step: 18299, Reward: [-1239.209 -1239.209 -1239.209] [0.0000], Avg: [-1219.894 -1219.894 -1219.894] (1.000)
Step: 18349, Reward: [-932.269 -932.269 -932.269] [0.0000], Avg: [-1219.111 -1219.111 -1219.111] (1.000)
Step: 18399, Reward: [-1088.778 -1088.778 -1088.778] [0.0000], Avg: [-1218.757 -1218.757 -1218.757] (1.000)
Step: 18449, Reward: [-1948.213 -1948.213 -1948.213] [0.0000], Avg: [-1220.733 -1220.733 -1220.733] (1.000)
Step: 18499, Reward: [-1133.423 -1133.423 -1133.423] [0.0000], Avg: [-1220.497 -1220.497 -1220.497] (1.000)
Step: 18549, Reward: [-1274.791 -1274.791 -1274.791] [0.0000], Avg: [-1220.644 -1220.644 -1220.644] (1.000)
Step: 18599, Reward: [-1081.613 -1081.613 -1081.613] [0.0000], Avg: [-1220.27 -1220.27 -1220.27] (1.000)
Step: 18649, Reward: [-933.931 -933.931 -933.931] [0.0000], Avg: [-1219.502 -1219.502 -1219.502] (1.000)
Step: 18699, Reward: [-954.519 -954.519 -954.519] [0.0000], Avg: [-1218.794 -1218.794 -1218.794] (1.000)
Step: 18749, Reward: [-941.282 -941.282 -941.282] [0.0000], Avg: [-1218.054 -1218.054 -1218.054] (1.000)
Step: 18799, Reward: [-863.574 -863.574 -863.574] [0.0000], Avg: [-1217.111 -1217.111 -1217.111] (1.000)
Step: 18849, Reward: [-860.61 -860.61 -860.61] [0.0000], Avg: [-1216.165 -1216.165 -1216.165] (1.000)
Step: 18899, Reward: [-866.939 -866.939 -866.939] [0.0000], Avg: [-1215.242 -1215.242 -1215.242] (1.000)
Step: 18949, Reward: [-1012.076 -1012.076 -1012.076] [0.0000], Avg: [-1214.705 -1214.705 -1214.705] (1.000)
Step: 18999, Reward: [-1032.921 -1032.921 -1032.921] [0.0000], Avg: [-1214.227 -1214.227 -1214.227] (1.000)
Step: 19049, Reward: [-1441.735 -1441.735 -1441.735] [0.0000], Avg: [-1214.824 -1214.824 -1214.824] (1.000)
Step: 19099, Reward: [-1011.399 -1011.399 -1011.399] [0.0000], Avg: [-1214.292 -1214.292 -1214.292] (1.000)
Step: 19149, Reward: [-1591.254 -1591.254 -1591.254] [0.0000], Avg: [-1215.276 -1215.276 -1215.276] (1.000)
Step: 19199, Reward: [-1125.534 -1125.534 -1125.534] [0.0000], Avg: [-1215.042 -1215.042 -1215.042] (1.000)
Step: 19249, Reward: [-956.111 -956.111 -956.111] [0.0000], Avg: [-1214.37 -1214.37 -1214.37] (1.000)
Step: 19299, Reward: [-1653.491 -1653.491 -1653.491] [0.0000], Avg: [-1215.507 -1215.507 -1215.507] (1.000)
Step: 19349, Reward: [-848.056 -848.056 -848.056] [0.0000], Avg: [-1214.558 -1214.558 -1214.558] (1.000)
Step: 19399, Reward: [-595.202 -595.202 -595.202] [0.0000], Avg: [-1212.962 -1212.962 -1212.962] (1.000)
Step: 19449, Reward: [-1368.989 -1368.989 -1368.989] [0.0000], Avg: [-1213.363 -1213.363 -1213.363] (1.000)
Step: 19499, Reward: [-1000.386 -1000.386 -1000.386] [0.0000], Avg: [-1212.817 -1212.817 -1212.817] (1.000)
Step: 19549, Reward: [-746.816 -746.816 -746.816] [0.0000], Avg: [-1211.625 -1211.625 -1211.625] (1.000)
Step: 19599, Reward: [-1066.83 -1066.83 -1066.83] [0.0000], Avg: [-1211.255 -1211.255 -1211.255] (1.000)
Step: 19649, Reward: [-938.214 -938.214 -938.214] [0.0000], Avg: [-1210.561 -1210.561 -1210.561] (1.000)
Step: 19699, Reward: [-1876.45 -1876.45 -1876.45] [0.0000], Avg: [-1212.251 -1212.251 -1212.251] (1.000)
Step: 19749, Reward: [-2010.643 -2010.643 -2010.643] [0.0000], Avg: [-1214.272 -1214.272 -1214.272] (1.000)
Step: 19799, Reward: [-908.539 -908.539 -908.539] [0.0000], Avg: [-1213.5 -1213.5 -1213.5] (1.000)
Step: 19849, Reward: [-868.072 -868.072 -868.072] [0.0000], Avg: [-1212.63 -1212.63 -1212.63] (1.000)
Step: 19899, Reward: [-1413.106 -1413.106 -1413.106] [0.0000], Avg: [-1213.133 -1213.133 -1213.133] (1.000)
Step: 19949, Reward: [-1002.11 -1002.11 -1002.11] [0.0000], Avg: [-1212.605 -1212.605 -1212.605] (1.000)
Step: 19999, Reward: [-1014.032 -1014.032 -1014.032] [0.0000], Avg: [-1212.108 -1212.108 -1212.108] (1.000)
Step: 20049, Reward: [-1332.84 -1332.84 -1332.84] [0.0000], Avg: [-1212.409 -1212.409 -1212.409] (1.000)
Step: 20099, Reward: [-1656.572 -1656.572 -1656.572] [0.0000], Avg: [-1213.514 -1213.514 -1213.514] (1.000)
Step: 20149, Reward: [-1733.085 -1733.085 -1733.085] [0.0000], Avg: [-1214.803 -1214.803 -1214.803] (1.000)
Step: 20199, Reward: [-771.221 -771.221 -771.221] [0.0000], Avg: [-1213.705 -1213.705 -1213.705] (1.000)
Step: 20249, Reward: [-1018.969 -1018.969 -1018.969] [0.0000], Avg: [-1213.225 -1213.225 -1213.225] (1.000)
Step: 20299, Reward: [-1049.274 -1049.274 -1049.274] [0.0000], Avg: [-1212.821 -1212.821 -1212.821] (1.000)
Step: 20349, Reward: [-1319.612 -1319.612 -1319.612] [0.0000], Avg: [-1213.083 -1213.083 -1213.083] (1.000)
Step: 20399, Reward: [-1468.37 -1468.37 -1468.37] [0.0000], Avg: [-1213.709 -1213.709 -1213.709] (1.000)
Step: 20449, Reward: [-1342.276 -1342.276 -1342.276] [0.0000], Avg: [-1214.023 -1214.023 -1214.023] (1.000)
Step: 20499, Reward: [-600.476 -600.476 -600.476] [0.0000], Avg: [-1212.527 -1212.527 -1212.527] (1.000)
Step: 20549, Reward: [-1587.069 -1587.069 -1587.069] [0.0000], Avg: [-1213.438 -1213.438 -1213.438] (1.000)
Step: 20599, Reward: [-1744.037 -1744.037 -1744.037] [0.0000], Avg: [-1214.726 -1214.726 -1214.726] (1.000)
Step: 20649, Reward: [-1168.222 -1168.222 -1168.222] [0.0000], Avg: [-1214.613 -1214.613 -1214.613] (1.000)
Step: 20699, Reward: [-662.578 -662.578 -662.578] [0.0000], Avg: [-1213.28 -1213.28 -1213.28] (1.000)
Step: 20749, Reward: [-1158.683 -1158.683 -1158.683] [0.0000], Avg: [-1213.148 -1213.148 -1213.148] (1.000)
Step: 20799, Reward: [-1025.881 -1025.881 -1025.881] [0.0000], Avg: [-1212.698 -1212.698 -1212.698] (1.000)
Step: 20849, Reward: [-1313.43 -1313.43 -1313.43] [0.0000], Avg: [-1212.94 -1212.94 -1212.94] (1.000)
Step: 20899, Reward: [-567.956 -567.956 -567.956] [0.0000], Avg: [-1211.397 -1211.397 -1211.397] (1.000)
Step: 20949, Reward: [-884.17 -884.17 -884.17] [0.0000], Avg: [-1210.616 -1210.616 -1210.616] (1.000)
Step: 20999, Reward: [-703.686 -703.686 -703.686] [0.0000], Avg: [-1209.409 -1209.409 -1209.409] (1.000)
Step: 21049, Reward: [-941.751 -941.751 -941.751] [0.0000], Avg: [-1208.773 -1208.773 -1208.773] (1.000)
Step: 21099, Reward: [-1221.371 -1221.371 -1221.371] [0.0000], Avg: [-1208.803 -1208.803 -1208.803] (1.000)
Step: 21149, Reward: [-596.316 -596.316 -596.316] [0.0000], Avg: [-1207.355 -1207.355 -1207.355] (1.000)
Step: 21199, Reward: [-1115.548 -1115.548 -1115.548] [0.0000], Avg: [-1207.138 -1207.138 -1207.138] (1.000)
Step: 21249, Reward: [-1346.955 -1346.955 -1346.955] [0.0000], Avg: [-1207.467 -1207.467 -1207.467] (1.000)
Step: 21299, Reward: [-793.614 -793.614 -793.614] [0.0000], Avg: [-1206.496 -1206.496 -1206.496] (1.000)
Step: 21349, Reward: [-1785.509 -1785.509 -1785.509] [0.0000], Avg: [-1207.852 -1207.852 -1207.852] (1.000)
Step: 21399, Reward: [-974.471 -974.471 -974.471] [0.0000], Avg: [-1207.307 -1207.307 -1207.307] (1.000)
Step: 21449, Reward: [-1042.229 -1042.229 -1042.229] [0.0000], Avg: [-1206.922 -1206.922 -1206.922] (1.000)
Step: 21499, Reward: [-525.219 -525.219 -525.219] [0.0000], Avg: [-1205.336 -1205.336 -1205.336] (1.000)
Step: 21549, Reward: [-1214.227 -1214.227 -1214.227] [0.0000], Avg: [-1205.357 -1205.357 -1205.357] (1.000)
Step: 21599, Reward: [-1056.156 -1056.156 -1056.156] [0.0000], Avg: [-1205.012 -1205.012 -1205.012] (1.000)
Step: 21649, Reward: [-881.478 -881.478 -881.478] [0.0000], Avg: [-1204.264 -1204.264 -1204.264] (1.000)
Step: 21699, Reward: [-474.524 -474.524 -474.524] [0.0000], Avg: [-1202.583 -1202.583 -1202.583] (1.000)
Step: 21749, Reward: [-1203.713 -1203.713 -1203.713] [0.0000], Avg: [-1202.586 -1202.586 -1202.586] (1.000)
Step: 21799, Reward: [-1100.102 -1100.102 -1100.102] [0.0000], Avg: [-1202.351 -1202.351 -1202.351] (1.000)
Step: 21849, Reward: [-751.072 -751.072 -751.072] [0.0000], Avg: [-1201.318 -1201.318 -1201.318] (1.000)
Step: 21899, Reward: [-971.167 -971.167 -971.167] [0.0000], Avg: [-1200.792 -1200.792 -1200.792] (1.000)
Step: 21949, Reward: [-807.369 -807.369 -807.369] [0.0000], Avg: [-1199.896 -1199.896 -1199.896] (1.000)
Step: 21999, Reward: [-969.633 -969.633 -969.633] [0.0000], Avg: [-1199.373 -1199.373 -1199.373] (1.000)
Step: 22049, Reward: [-838.49 -838.49 -838.49] [0.0000], Avg: [-1198.555 -1198.555 -1198.555] (1.000)
Step: 22099, Reward: [-517.364 -517.364 -517.364] [0.0000], Avg: [-1197.013 -1197.013 -1197.013] (1.000)
Step: 22149, Reward: [-794.008 -794.008 -794.008] [0.0000], Avg: [-1196.104 -1196.104 -1196.104] (1.000)
Step: 22199, Reward: [-806.704 -806.704 -806.704] [0.0000], Avg: [-1195.227 -1195.227 -1195.227] (1.000)
Step: 22249, Reward: [-1446.603 -1446.603 -1446.603] [0.0000], Avg: [-1195.792 -1195.792 -1195.792] (1.000)
Step: 22299, Reward: [-636.163 -636.163 -636.163] [0.0000], Avg: [-1194.537 -1194.537 -1194.537] (1.000)
Step: 22349, Reward: [-860.641 -860.641 -860.641] [0.0000], Avg: [-1193.79 -1193.79 -1193.79] (1.000)
Step: 22399, Reward: [-1192.875 -1192.875 -1192.875] [0.0000], Avg: [-1193.788 -1193.788 -1193.788] (1.000)
Step: 22449, Reward: [-773.795 -773.795 -773.795] [0.0000], Avg: [-1192.852 -1192.852 -1192.852] (1.000)
Step: 22499, Reward: [-1406.411 -1406.411 -1406.411] [0.0000], Avg: [-1193.327 -1193.327 -1193.327] (1.000)
Step: 22549, Reward: [-513.472 -513.472 -513.472] [0.0000], Avg: [-1191.82 -1191.82 -1191.82] (1.000)
Step: 22599, Reward: [-843.197 -843.197 -843.197] [0.0000], Avg: [-1191.048 -1191.048 -1191.048] (1.000)
Step: 22649, Reward: [-1228.675 -1228.675 -1228.675] [0.0000], Avg: [-1191.131 -1191.131 -1191.131] (1.000)
Step: 22699, Reward: [-387.381 -387.381 -387.381] [0.0000], Avg: [-1189.361 -1189.361 -1189.361] (1.000)
Step: 22749, Reward: [-1155.797 -1155.797 -1155.797] [0.0000], Avg: [-1189.287 -1189.287 -1189.287] (1.000)
Step: 22799, Reward: [-711.637 -711.637 -711.637] [0.0000], Avg: [-1188.24 -1188.24 -1188.24] (1.000)
Step: 22849, Reward: [-598.201 -598.201 -598.201] [0.0000], Avg: [-1186.949 -1186.949 -1186.949] (1.000)
Step: 22899, Reward: [-1103.336 -1103.336 -1103.336] [0.0000], Avg: [-1186.766 -1186.766 -1186.766] (1.000)
Step: 22949, Reward: [-1036.08 -1036.08 -1036.08] [0.0000], Avg: [-1186.438 -1186.438 -1186.438] (1.000)
Step: 22999, Reward: [-1279.76 -1279.76 -1279.76] [0.0000], Avg: [-1186.641 -1186.641 -1186.641] (1.000)
Step: 23049, Reward: [-1206.825 -1206.825 -1206.825] [0.0000], Avg: [-1186.684 -1186.684 -1186.684] (1.000)
Step: 23099, Reward: [-1020.083 -1020.083 -1020.083] [0.0000], Avg: [-1186.324 -1186.324 -1186.324] (1.000)
Step: 23149, Reward: [-1149.777 -1149.777 -1149.777] [0.0000], Avg: [-1186.245 -1186.245 -1186.245] (1.000)
Step: 23199, Reward: [-1015.706 -1015.706 -1015.706] [0.0000], Avg: [-1185.877 -1185.877 -1185.877] (1.000)
Step: 23249, Reward: [-1101.606 -1101.606 -1101.606] [0.0000], Avg: [-1185.696 -1185.696 -1185.696] (1.000)
Step: 23299, Reward: [-1160.685 -1160.685 -1160.685] [0.0000], Avg: [-1185.642 -1185.642 -1185.642] (1.000)
Step: 23349, Reward: [-1299.142 -1299.142 -1299.142] [0.0000], Avg: [-1185.885 -1185.885 -1185.885] (1.000)
Step: 23399, Reward: [-683.045 -683.045 -683.045] [0.0000], Avg: [-1184.811 -1184.811 -1184.811] (1.000)
Step: 23449, Reward: [-1319.281 -1319.281 -1319.281] [0.0000], Avg: [-1185.098 -1185.098 -1185.098] (1.000)
Step: 23499, Reward: [-1133.958 -1133.958 -1133.958] [0.0000], Avg: [-1184.989 -1184.989 -1184.989] (1.000)
Step: 23549, Reward: [-1286.687 -1286.687 -1286.687] [0.0000], Avg: [-1185.205 -1185.205 -1185.205] (1.000)
Step: 23599, Reward: [-430.292 -430.292 -430.292] [0.0000], Avg: [-1183.605 -1183.605 -1183.605] (1.000)
Step: 23649, Reward: [-1283.042 -1283.042 -1283.042] [0.0000], Avg: [-1183.816 -1183.816 -1183.816] (1.000)
Step: 23699, Reward: [-615.108 -615.108 -615.108] [0.0000], Avg: [-1182.616 -1182.616 -1182.616] (1.000)
Step: 23749, Reward: [-949.073 -949.073 -949.073] [0.0000], Avg: [-1182.124 -1182.124 -1182.124] (1.000)
Step: 23799, Reward: [-1296.208 -1296.208 -1296.208] [0.0000], Avg: [-1182.364 -1182.364 -1182.364] (1.000)
Step: 23849, Reward: [-586.618 -586.618 -586.618] [0.0000], Avg: [-1181.115 -1181.115 -1181.115] (1.000)
Step: 23899, Reward: [-1214.632 -1214.632 -1214.632] [0.0000], Avg: [-1181.185 -1181.185 -1181.185] (1.000)
Step: 23949, Reward: [-404.652 -404.652 -404.652] [0.0000], Avg: [-1179.564 -1179.564 -1179.564] (1.000)
Step: 23999, Reward: [-678.487 -678.487 -678.487] [0.0000], Avg: [-1178.52 -1178.52 -1178.52] (1.000)
Step: 24049, Reward: [-759.464 -759.464 -759.464] [0.0000], Avg: [-1177.649 -1177.649 -1177.649] (1.000)
Step: 24099, Reward: [-484.066 -484.066 -484.066] [0.0000], Avg: [-1176.21 -1176.21 -1176.21] (1.000)
Step: 24149, Reward: [-1004.034 -1004.034 -1004.034] [0.0000], Avg: [-1175.853 -1175.853 -1175.853] (1.000)
Step: 24199, Reward: [-1153. -1153. -1153.] [0.0000], Avg: [-1175.806 -1175.806 -1175.806] (1.000)
Step: 24249, Reward: [-1721.115 -1721.115 -1721.115] [0.0000], Avg: [-1176.93 -1176.93 -1176.93] (1.000)
Step: 24299, Reward: [-779.841 -779.841 -779.841] [0.0000], Avg: [-1176.113 -1176.113 -1176.113] (1.000)
Step: 24349, Reward: [-877.841 -877.841 -877.841] [0.0000], Avg: [-1175.501 -1175.501 -1175.501] (1.000)
Step: 24399, Reward: [-511.453 -511.453 -511.453] [0.0000], Avg: [-1174.14 -1174.14 -1174.14] (1.000)
Step: 24449, Reward: [-504.527 -504.527 -504.527] [0.0000], Avg: [-1172.771 -1172.771 -1172.771] (1.000)
Step: 24499, Reward: [-675.173 -675.173 -675.173] [0.0000], Avg: [-1171.755 -1171.755 -1171.755] (1.000)
Step: 24549, Reward: [-1184.989 -1184.989 -1184.989] [0.0000], Avg: [-1171.782 -1171.782 -1171.782] (1.000)
Step: 24599, Reward: [-788.151 -788.151 -788.151] [0.0000], Avg: [-1171.003 -1171.003 -1171.003] (1.000)
Step: 24649, Reward: [-1048.186 -1048.186 -1048.186] [0.0000], Avg: [-1170.753 -1170.753 -1170.753] (1.000)
Step: 24699, Reward: [-926.95 -926.95 -926.95] [0.0000], Avg: [-1170.26 -1170.26 -1170.26] (1.000)
Step: 24749, Reward: [-715.081 -715.081 -715.081] [0.0000], Avg: [-1169.34 -1169.34 -1169.34] (1.000)
Step: 24799, Reward: [-542.193 -542.193 -542.193] [0.0000], Avg: [-1168.076 -1168.076 -1168.076] (1.000)
Step: 24849, Reward: [-869.548 -869.548 -869.548] [0.0000], Avg: [-1167.475 -1167.475 -1167.475] (1.000)
Step: 24899, Reward: [-1143.088 -1143.088 -1143.088] [0.0000], Avg: [-1167.426 -1167.426 -1167.426] (1.000)
Step: 24949, Reward: [-531.862 -531.862 -531.862] [0.0000], Avg: [-1166.153 -1166.153 -1166.153] (1.000)
Step: 24999, Reward: [-677.313 -677.313 -677.313] [0.0000], Avg: [-1165.175 -1165.175 -1165.175] (1.000)
Step: 25049, Reward: [-633.959 -633.959 -633.959] [0.0000], Avg: [-1164.115 -1164.115 -1164.115] (1.000)
Step: 25099, Reward: [-765.309 -765.309 -765.309] [0.0000], Avg: [-1163.32 -1163.32 -1163.32] (1.000)
Step: 25149, Reward: [-878.25 -878.25 -878.25] [0.0000], Avg: [-1162.753 -1162.753 -1162.753] (1.000)
Step: 25199, Reward: [-979.781 -979.781 -979.781] [0.0000], Avg: [-1162.39 -1162.39 -1162.39] (1.000)
Step: 25249, Reward: [-389.227 -389.227 -389.227] [0.0000], Avg: [-1160.859 -1160.859 -1160.859] (1.000)
Step: 25299, Reward: [-479.761 -479.761 -479.761] [0.0000], Avg: [-1159.513 -1159.513 -1159.513] (1.000)
Step: 25349, Reward: [-1205.844 -1205.844 -1205.844] [0.0000], Avg: [-1159.605 -1159.605 -1159.605] (1.000)
Step: 25399, Reward: [-580.019 -580.019 -580.019] [0.0000], Avg: [-1158.464 -1158.464 -1158.464] (1.000)
Step: 25449, Reward: [-698.334 -698.334 -698.334] [0.0000], Avg: [-1157.56 -1157.56 -1157.56] (1.000)
Step: 25499, Reward: [-588.73 -588.73 -588.73] [0.0000], Avg: [-1156.444 -1156.444 -1156.444] (1.000)
Step: 25549, Reward: [-1178.683 -1178.683 -1178.683] [0.0000], Avg: [-1156.488 -1156.488 -1156.488] (1.000)
Step: 25599, Reward: [-617.407 -617.407 -617.407] [0.0000], Avg: [-1155.435 -1155.435 -1155.435] (1.000)
Step: 25649, Reward: [-724.598 -724.598 -724.598] [0.0000], Avg: [-1154.595 -1154.595 -1154.595] (1.000)
Step: 25699, Reward: [-1364.803 -1364.803 -1364.803] [0.0000], Avg: [-1155.004 -1155.004 -1155.004] (1.000)
Step: 25749, Reward: [-703.067 -703.067 -703.067] [0.0000], Avg: [-1154.127 -1154.127 -1154.127] (1.000)
Step: 25799, Reward: [-357.98 -357.98 -357.98] [0.0000], Avg: [-1152.584 -1152.584 -1152.584] (1.000)
Step: 25849, Reward: [-738.716 -738.716 -738.716] [0.0000], Avg: [-1151.783 -1151.783 -1151.783] (1.000)
Step: 25899, Reward: [-1289.07 -1289.07 -1289.07] [0.0000], Avg: [-1152.048 -1152.048 -1152.048] (1.000)
Step: 25949, Reward: [-659.832 -659.832 -659.832] [0.0000], Avg: [-1151.1 -1151.1 -1151.1] (1.000)
Step: 25999, Reward: [-1080.087 -1080.087 -1080.087] [0.0000], Avg: [-1150.963 -1150.963 -1150.963] (1.000)
Step: 26049, Reward: [-1040.038 -1040.038 -1040.038] [0.0000], Avg: [-1150.75 -1150.75 -1150.75] (1.000)
Step: 26099, Reward: [-483.888 -483.888 -483.888] [0.0000], Avg: [-1149.473 -1149.473 -1149.473] (1.000)
Step: 26149, Reward: [-941.844 -941.844 -941.844] [0.0000], Avg: [-1149.076 -1149.076 -1149.076] (1.000)
Step: 26199, Reward: [-765.085 -765.085 -765.085] [0.0000], Avg: [-1148.343 -1148.343 -1148.343] (1.000)
Step: 26249, Reward: [-651.559 -651.559 -651.559] [0.0000], Avg: [-1147.397 -1147.397 -1147.397] (1.000)
Step: 26299, Reward: [-973.512 -973.512 -973.512] [0.0000], Avg: [-1147.066 -1147.066 -1147.066] (1.000)
Step: 26349, Reward: [-550.313 -550.313 -550.313] [0.0000], Avg: [-1145.934 -1145.934 -1145.934] (1.000)
Step: 26399, Reward: [-921.646 -921.646 -921.646] [0.0000], Avg: [-1145.509 -1145.509 -1145.509] (1.000)
Step: 26449, Reward: [-879.313 -879.313 -879.313] [0.0000], Avg: [-1145.006 -1145.006 -1145.006] (1.000)
Step: 26499, Reward: [-1040.979 -1040.979 -1040.979] [0.0000], Avg: [-1144.81 -1144.81 -1144.81] (1.000)
Step: 26549, Reward: [-509.674 -509.674 -509.674] [0.0000], Avg: [-1143.614 -1143.614 -1143.614] (1.000)
Step: 26599, Reward: [-549.059 -549.059 -549.059] [0.0000], Avg: [-1142.496 -1142.496 -1142.496] (1.000)
Step: 26649, Reward: [-432.592 -432.592 -432.592] [0.0000], Avg: [-1141.164 -1141.164 -1141.164] (1.000)
Step: 26699, Reward: [-706.653 -706.653 -706.653] [0.0000], Avg: [-1140.35 -1140.35 -1140.35] (1.000)
Step: 26749, Reward: [-1101.607 -1101.607 -1101.607] [0.0000], Avg: [-1140.278 -1140.278 -1140.278] (1.000)
Step: 26799, Reward: [-1137.078 -1137.078 -1137.078] [0.0000], Avg: [-1140.272 -1140.272 -1140.272] (1.000)
Step: 26849, Reward: [-554.684 -554.684 -554.684] [0.0000], Avg: [-1139.181 -1139.181 -1139.181] (1.000)
Step: 26899, Reward: [-777.447 -777.447 -777.447] [0.0000], Avg: [-1138.509 -1138.509 -1138.509] (1.000)
Step: 26949, Reward: [-684.54 -684.54 -684.54] [0.0000], Avg: [-1137.667 -1137.667 -1137.667] (1.000)
Step: 26999, Reward: [-1477.832 -1477.832 -1477.832] [0.0000], Avg: [-1138.297 -1138.297 -1138.297] (1.000)
Step: 27049, Reward: [-1078.457 -1078.457 -1078.457] [0.0000], Avg: [-1138.186 -1138.186 -1138.186] (1.000)
Step: 27099, Reward: [-766.095 -766.095 -766.095] [0.0000], Avg: [-1137.5 -1137.5 -1137.5] (1.000)
Step: 27149, Reward: [-1166.322 -1166.322 -1166.322] [0.0000], Avg: [-1137.553 -1137.553 -1137.553] (1.000)
Step: 27199, Reward: [-1160.164 -1160.164 -1160.164] [0.0000], Avg: [-1137.594 -1137.594 -1137.594] (1.000)
Step: 27249, Reward: [-724.855 -724.855 -724.855] [0.0000], Avg: [-1136.837 -1136.837 -1136.837] (1.000)
Step: 27299, Reward: [-576.067 -576.067 -576.067] [0.0000], Avg: [-1135.81 -1135.81 -1135.81] (1.000)
Step: 27349, Reward: [-937.921 -937.921 -937.921] [0.0000], Avg: [-1135.448 -1135.448 -1135.448] (1.000)
Step: 27399, Reward: [-1660.004 -1660.004 -1660.004] [0.0000], Avg: [-1136.405 -1136.405 -1136.405] (1.000)
Step: 27449, Reward: [-837.504 -837.504 -837.504] [0.0000], Avg: [-1135.861 -1135.861 -1135.861] (1.000)
Step: 27499, Reward: [-1599.948 -1599.948 -1599.948] [0.0000], Avg: [-1136.705 -1136.705 -1136.705] (1.000)
Step: 27549, Reward: [-1211.424 -1211.424 -1211.424] [0.0000], Avg: [-1136.84 -1136.84 -1136.84] (1.000)
Step: 27599, Reward: [-765.189 -765.189 -765.189] [0.0000], Avg: [-1136.167 -1136.167 -1136.167] (1.000)
Step: 27649, Reward: [-1682.281 -1682.281 -1682.281] [0.0000], Avg: [-1137.155 -1137.155 -1137.155] (1.000)
Step: 27699, Reward: [-1086.849 -1086.849 -1086.849] [0.0000], Avg: [-1137.064 -1137.064 -1137.064] (1.000)
Step: 27749, Reward: [-716.129 -716.129 -716.129] [0.0000], Avg: [-1136.305 -1136.305 -1136.305] (1.000)
Step: 27799, Reward: [-1733.561 -1733.561 -1733.561] [0.0000], Avg: [-1137.38 -1137.38 -1137.38] (1.000)
Step: 27849, Reward: [-790.775 -790.775 -790.775] [0.0000], Avg: [-1136.757 -1136.757 -1136.757] (1.000)
Step: 27899, Reward: [-821.888 -821.888 -821.888] [0.0000], Avg: [-1136.193 -1136.193 -1136.193] (1.000)
Step: 27949, Reward: [-1001.599 -1001.599 -1001.599] [0.0000], Avg: [-1135.952 -1135.952 -1135.952] (1.000)
Step: 27999, Reward: [-540.568 -540.568 -540.568] [0.0000], Avg: [-1134.889 -1134.889 -1134.889] (1.000)
Step: 28049, Reward: [-1459.528 -1459.528 -1459.528] [0.0000], Avg: [-1135.468 -1135.468 -1135.468] (1.000)
Step: 28099, Reward: [-1552.103 -1552.103 -1552.103] [0.0000], Avg: [-1136.209 -1136.209 -1136.209] (1.000)
Step: 28149, Reward: [-1465.582 -1465.582 -1465.582] [0.0000], Avg: [-1136.794 -1136.794 -1136.794] (1.000)
Step: 28199, Reward: [-605.96 -605.96 -605.96] [0.0000], Avg: [-1135.853 -1135.853 -1135.853] (1.000)
Step: 28249, Reward: [-719.749 -719.749 -719.749] [0.0000], Avg: [-1135.116 -1135.116 -1135.116] (1.000)
Step: 28299, Reward: [-952.893 -952.893 -952.893] [0.0000], Avg: [-1134.795 -1134.795 -1134.795] (1.000)
Step: 28349, Reward: [-1386.537 -1386.537 -1386.537] [0.0000], Avg: [-1135.238 -1135.238 -1135.238] (1.000)
Step: 28399, Reward: [-1261.238 -1261.238 -1261.238] [0.0000], Avg: [-1135.46 -1135.46 -1135.46] (1.000)
Step: 28449, Reward: [-1006.105 -1006.105 -1006.105] [0.0000], Avg: [-1135.233 -1135.233 -1135.233] (1.000)
Step: 28499, Reward: [-983.891 -983.891 -983.891] [0.0000], Avg: [-1134.967 -1134.967 -1134.967] (1.000)
Step: 28549, Reward: [-867.425 -867.425 -867.425] [0.0000], Avg: [-1134.499 -1134.499 -1134.499] (1.000)
Step: 28599, Reward: [-588.842 -588.842 -588.842] [0.0000], Avg: [-1133.545 -1133.545 -1133.545] (1.000)
Step: 28649, Reward: [-1431.162 -1431.162 -1431.162] [0.0000], Avg: [-1134.064 -1134.064 -1134.064] (1.000)
Step: 28699, Reward: [-1031.472 -1031.472 -1031.472] [0.0000], Avg: [-1133.886 -1133.886 -1133.886] (1.000)
Step: 28749, Reward: [-1758.863 -1758.863 -1758.863] [0.0000], Avg: [-1134.973 -1134.973 -1134.973] (1.000)
Step: 28799, Reward: [-1025.607 -1025.607 -1025.607] [0.0000], Avg: [-1134.783 -1134.783 -1134.783] (1.000)
Step: 28849, Reward: [-1114.409 -1114.409 -1114.409] [0.0000], Avg: [-1134.747 -1134.747 -1134.747] (1.000)
Step: 28899, Reward: [-702.287 -702.287 -702.287] [0.0000], Avg: [-1133.999 -1133.999 -1133.999] (1.000)
Step: 28949, Reward: [-652.781 -652.781 -652.781] [0.0000], Avg: [-1133.168 -1133.168 -1133.168] (1.000)
Step: 28999, Reward: [-936.586 -936.586 -936.586] [0.0000], Avg: [-1132.829 -1132.829 -1132.829] (1.000)
Step: 29049, Reward: [-1121.937 -1121.937 -1121.937] [0.0000], Avg: [-1132.81 -1132.81 -1132.81] (1.000)
Step: 29099, Reward: [-834.345 -834.345 -834.345] [0.0000], Avg: [-1132.298 -1132.298 -1132.298] (1.000)
Step: 29149, Reward: [-1115.614 -1115.614 -1115.614] [0.0000], Avg: [-1132.269 -1132.269 -1132.269] (1.000)
Step: 29199, Reward: [-819.206 -819.206 -819.206] [0.0000], Avg: [-1131.733 -1131.733 -1131.733] (1.000)
Step: 29249, Reward: [-1092.165 -1092.165 -1092.165] [0.0000], Avg: [-1131.665 -1131.665 -1131.665] (1.000)
Step: 29299, Reward: [-838.149 -838.149 -838.149] [0.0000], Avg: [-1131.164 -1131.164 -1131.164] (1.000)
Step: 29349, Reward: [-1460.205 -1460.205 -1460.205] [0.0000], Avg: [-1131.725 -1131.725 -1131.725] (1.000)
Step: 29399, Reward: [-1200.787 -1200.787 -1200.787] [0.0000], Avg: [-1131.842 -1131.842 -1131.842] (1.000)
Step: 29449, Reward: [-880.97 -880.97 -880.97] [0.0000], Avg: [-1131.416 -1131.416 -1131.416] (1.000)
Step: 29499, Reward: [-615.169 -615.169 -615.169] [0.0000], Avg: [-1130.541 -1130.541 -1130.541] (1.000)
Step: 29549, Reward: [-1030.655 -1030.655 -1030.655] [0.0000], Avg: [-1130.372 -1130.372 -1130.372] (1.000)
Step: 29599, Reward: [-524.534 -524.534 -524.534] [0.0000], Avg: [-1129.349 -1129.349 -1129.349] (1.000)
Step: 29649, Reward: [-1118.347 -1118.347 -1118.347] [0.0000], Avg: [-1129.33 -1129.33 -1129.33] (1.000)
Step: 29699, Reward: [-849.101 -849.101 -849.101] [0.0000], Avg: [-1128.859 -1128.859 -1128.859] (1.000)
Step: 29749, Reward: [-783.589 -783.589 -783.589] [0.0000], Avg: [-1128.278 -1128.278 -1128.278] (1.000)
Step: 29799, Reward: [-1071.577 -1071.577 -1071.577] [0.0000], Avg: [-1128.183 -1128.183 -1128.183] (1.000)
Step: 29849, Reward: [-855.349 -855.349 -855.349] [0.0000], Avg: [-1127.726 -1127.726 -1127.726] (1.000)
Step: 29899, Reward: [-1088.076 -1088.076 -1088.076] [0.0000], Avg: [-1127.66 -1127.66 -1127.66] (1.000)
Step: 29949, Reward: [-1482.291 -1482.291 -1482.291] [0.0000], Avg: [-1128.252 -1128.252 -1128.252] (1.000)
Step: 29999, Reward: [-1495.175 -1495.175 -1495.175] [0.0000], Avg: [-1128.864 -1128.864 -1128.864] (1.000)
Step: 30049, Reward: [-1339.071 -1339.071 -1339.071] [0.0000], Avg: [-1129.213 -1129.213 -1129.213] (1.000)
Step: 30099, Reward: [-1432.573 -1432.573 -1432.573] [0.0000], Avg: [-1129.717 -1129.717 -1129.717] (1.000)
Step: 30149, Reward: [-1232.215 -1232.215 -1232.215] [0.0000], Avg: [-1129.887 -1129.887 -1129.887] (1.000)
Step: 30199, Reward: [-1199.849 -1199.849 -1199.849] [0.0000], Avg: [-1130.003 -1130.003 -1130.003] (1.000)
Step: 30249, Reward: [-1489.146 -1489.146 -1489.146] [0.0000], Avg: [-1130.597 -1130.597 -1130.597] (1.000)
Step: 30299, Reward: [-1491.548 -1491.548 -1491.548] [0.0000], Avg: [-1131.192 -1131.192 -1131.192] (1.000)
Step: 30349, Reward: [-1236.236 -1236.236 -1236.236] [0.0000], Avg: [-1131.365 -1131.365 -1131.365] (1.000)
Step: 30399, Reward: [-1013.693 -1013.693 -1013.693] [0.0000], Avg: [-1131.172 -1131.172 -1131.172] (1.000)
Step: 30449, Reward: [-1247.718 -1247.718 -1247.718] [0.0000], Avg: [-1131.363 -1131.363 -1131.363] (1.000)
Step: 30499, Reward: [-1421.795 -1421.795 -1421.795] [0.0000], Avg: [-1131.839 -1131.839 -1131.839] (1.000)
Step: 30549, Reward: [-1057.461 -1057.461 -1057.461] [0.0000], Avg: [-1131.718 -1131.718 -1131.718] (1.000)
Step: 30599, Reward: [-1289.816 -1289.816 -1289.816] [0.0000], Avg: [-1131.976 -1131.976 -1131.976] (1.000)
Step: 30649, Reward: [-1228.215 -1228.215 -1228.215] [0.0000], Avg: [-1132.133 -1132.133 -1132.133] (1.000)
Step: 30699, Reward: [-1338.746 -1338.746 -1338.746] [0.0000], Avg: [-1132.469 -1132.469 -1132.469] (1.000)
Step: 30749, Reward: [-738.924 -738.924 -738.924] [0.0000], Avg: [-1131.829 -1131.829 -1131.829] (1.000)
Step: 30799, Reward: [-850.288 -850.288 -850.288] [0.0000], Avg: [-1131.372 -1131.372 -1131.372] (1.000)
Step: 30849, Reward: [-1162.769 -1162.769 -1162.769] [0.0000], Avg: [-1131.423 -1131.423 -1131.423] (1.000)
Step: 30899, Reward: [-1218.127 -1218.127 -1218.127] [0.0000], Avg: [-1131.564 -1131.564 -1131.564] (1.000)
Step: 30949, Reward: [-1108.259 -1108.259 -1108.259] [0.0000], Avg: [-1131.526 -1131.526 -1131.526] (1.000)
Step: 30999, Reward: [-1673.939 -1673.939 -1673.939] [0.0000], Avg: [-1132.401 -1132.401 -1132.401] (1.000)
Step: 31049, Reward: [-1263.895 -1263.895 -1263.895] [0.0000], Avg: [-1132.613 -1132.613 -1132.613] (1.000)
Step: 31099, Reward: [-974.243 -974.243 -974.243] [0.0000], Avg: [-1132.358 -1132.358 -1132.358] (1.000)
Step: 31149, Reward: [-1129.688 -1129.688 -1129.688] [0.0000], Avg: [-1132.354 -1132.354 -1132.354] (1.000)
Step: 31199, Reward: [-1414.629 -1414.629 -1414.629] [0.0000], Avg: [-1132.806 -1132.806 -1132.806] (1.000)
Step: 31249, Reward: [-1060.555 -1060.555 -1060.555] [0.0000], Avg: [-1132.69 -1132.69 -1132.69] (1.000)
Step: 31299, Reward: [-1275.72 -1275.72 -1275.72] [0.0000], Avg: [-1132.919 -1132.919 -1132.919] (1.000)
Step: 31349, Reward: [-823.522 -823.522 -823.522] [0.0000], Avg: [-1132.425 -1132.425 -1132.425] (1.000)
Step: 31399, Reward: [-1032.48 -1032.48 -1032.48] [0.0000], Avg: [-1132.266 -1132.266 -1132.266] (1.000)
Step: 31449, Reward: [-1059.242 -1059.242 -1059.242] [0.0000], Avg: [-1132.15 -1132.15 -1132.15] (1.000)
Step: 31499, Reward: [-697.583 -697.583 -697.583] [0.0000], Avg: [-1131.46 -1131.46 -1131.46] (1.000)
Step: 31549, Reward: [-907.642 -907.642 -907.642] [0.0000], Avg: [-1131.106 -1131.106 -1131.106] (1.000)
Step: 31599, Reward: [-1492.279 -1492.279 -1492.279] [0.0000], Avg: [-1131.677 -1131.677 -1131.677] (1.000)
Step: 31649, Reward: [-1074.445 -1074.445 -1074.445] [0.0000], Avg: [-1131.587 -1131.587 -1131.587] (1.000)
Step: 31699, Reward: [-972.944 -972.944 -972.944] [0.0000], Avg: [-1131.337 -1131.337 -1131.337] (1.000)
Step: 31749, Reward: [-1229.125 -1229.125 -1229.125] [0.0000], Avg: [-1131.491 -1131.491 -1131.491] (1.000)
Step: 31799, Reward: [-820.858 -820.858 -820.858] [0.0000], Avg: [-1131.002 -1131.002 -1131.002] (1.000)
Step: 31849, Reward: [-1038.04 -1038.04 -1038.04] [0.0000], Avg: [-1130.856 -1130.856 -1130.856] (1.000)
Step: 31899, Reward: [-674.906 -674.906 -674.906] [0.0000], Avg: [-1130.142 -1130.142 -1130.142] (1.000)
Step: 31949, Reward: [-902.782 -902.782 -902.782] [0.0000], Avg: [-1129.786 -1129.786 -1129.786] (1.000)
Step: 31999, Reward: [-990.477 -990.477 -990.477] [0.0000], Avg: [-1129.568 -1129.568 -1129.568] (1.000)
Step: 32049, Reward: [-1119.203 -1119.203 -1119.203] [0.0000], Avg: [-1129.552 -1129.552 -1129.552] (1.000)
Step: 32099, Reward: [-1195.933 -1195.933 -1195.933] [0.0000], Avg: [-1129.655 -1129.655 -1129.655] (1.000)
Step: 32149, Reward: [-1037.439 -1037.439 -1037.439] [0.0000], Avg: [-1129.512 -1129.512 -1129.512] (1.000)
Step: 32199, Reward: [-1015.254 -1015.254 -1015.254] [0.0000], Avg: [-1129.334 -1129.334 -1129.334] (1.000)
Step: 32249, Reward: [-1440.018 -1440.018 -1440.018] [0.0000], Avg: [-1129.816 -1129.816 -1129.816] (1.000)
Step: 32299, Reward: [-834.242 -834.242 -834.242] [0.0000], Avg: [-1129.359 -1129.359 -1129.359] (1.000)
Step: 32349, Reward: [-1413.827 -1413.827 -1413.827] [0.0000], Avg: [-1129.798 -1129.798 -1129.798] (1.000)
Step: 32399, Reward: [-652.826 -652.826 -652.826] [0.0000], Avg: [-1129.062 -1129.062 -1129.062] (1.000)
Step: 32449, Reward: [-886.765 -886.765 -886.765] [0.0000], Avg: [-1128.689 -1128.689 -1128.689] (1.000)
Step: 32499, Reward: [-706.521 -706.521 -706.521] [0.0000], Avg: [-1128.039 -1128.039 -1128.039] (1.000)
Step: 32549, Reward: [-923.938 -923.938 -923.938] [0.0000], Avg: [-1127.726 -1127.726 -1127.726] (1.000)
Step: 32599, Reward: [-991.059 -991.059 -991.059] [0.0000], Avg: [-1127.516 -1127.516 -1127.516] (1.000)
Step: 32649, Reward: [-1064.868 -1064.868 -1064.868] [0.0000], Avg: [-1127.42 -1127.42 -1127.42] (1.000)
Step: 32699, Reward: [-1006.61 -1006.61 -1006.61] [0.0000], Avg: [-1127.236 -1127.236 -1127.236] (1.000)
Step: 32749, Reward: [-1166.253 -1166.253 -1166.253] [0.0000], Avg: [-1127.295 -1127.295 -1127.295] (1.000)
Step: 32799, Reward: [-1067.943 -1067.943 -1067.943] [0.0000], Avg: [-1127.205 -1127.205 -1127.205] (1.000)
Step: 32849, Reward: [-988.148 -988.148 -988.148] [0.0000], Avg: [-1126.993 -1126.993 -1126.993] (1.000)
Step: 32899, Reward: [-1041.994 -1041.994 -1041.994] [0.0000], Avg: [-1126.864 -1126.864 -1126.864] (1.000)
Step: 32949, Reward: [-643.389 -643.389 -643.389] [0.0000], Avg: [-1126.13 -1126.13 -1126.13] (1.000)
Step: 32999, Reward: [-713.286 -713.286 -713.286] [0.0000], Avg: [-1125.505 -1125.505 -1125.505] (1.000)
Step: 33049, Reward: [-1302.66 -1302.66 -1302.66] [0.0000], Avg: [-1125.773 -1125.773 -1125.773] (1.000)
Step: 33099, Reward: [-1143.554 -1143.554 -1143.554] [0.0000], Avg: [-1125.8 -1125.8 -1125.8] (1.000)
Step: 33149, Reward: [-893.433 -893.433 -893.433] [0.0000], Avg: [-1125.449 -1125.449 -1125.449] (1.000)
Step: 33199, Reward: [-975.03 -975.03 -975.03] [0.0000], Avg: [-1125.223 -1125.223 -1125.223] (1.000)
Step: 33249, Reward: [-1669.23 -1669.23 -1669.23] [0.0000], Avg: [-1126.041 -1126.041 -1126.041] (1.000)
Step: 33299, Reward: [-1311.27 -1311.27 -1311.27] [0.0000], Avg: [-1126.319 -1126.319 -1126.319] (1.000)
Step: 33349, Reward: [-580.222 -580.222 -580.222] [0.0000], Avg: [-1125.5 -1125.5 -1125.5] (1.000)
Step: 33399, Reward: [-1384.534 -1384.534 -1384.534] [0.0000], Avg: [-1125.888 -1125.888 -1125.888] (1.000)
Step: 33449, Reward: [-849.833 -849.833 -849.833] [0.0000], Avg: [-1125.475 -1125.475 -1125.475] (1.000)
Step: 33499, Reward: [-1474.509 -1474.509 -1474.509] [0.0000], Avg: [-1125.996 -1125.996 -1125.996] (1.000)
Step: 33549, Reward: [-967.87 -967.87 -967.87] [0.0000], Avg: [-1125.76 -1125.76 -1125.76] (1.000)
Step: 33599, Reward: [-683.143 -683.143 -683.143] [0.0000], Avg: [-1125.102 -1125.102 -1125.102] (1.000)
Step: 33649, Reward: [-805.46 -805.46 -805.46] [0.0000], Avg: [-1124.627 -1124.627 -1124.627] (1.000)
Step: 33699, Reward: [-613.753 -613.753 -613.753] [0.0000], Avg: [-1123.869 -1123.869 -1123.869] (1.000)
Step: 33749, Reward: [-700.275 -700.275 -700.275] [0.0000], Avg: [-1123.241 -1123.241 -1123.241] (1.000)
Step: 33799, Reward: [-1211.805 -1211.805 -1211.805] [0.0000], Avg: [-1123.372 -1123.372 -1123.372] (1.000)
Step: 33849, Reward: [-470.05 -470.05 -470.05] [0.0000], Avg: [-1122.407 -1122.407 -1122.407] (1.000)
Step: 33899, Reward: [-1180.935 -1180.935 -1180.935] [0.0000], Avg: [-1122.494 -1122.494 -1122.494] (1.000)
Step: 33949, Reward: [-1029.819 -1029.819 -1029.819] [0.0000], Avg: [-1122.357 -1122.357 -1122.357] (1.000)
Step: 33999, Reward: [-1098.753 -1098.753 -1098.753] [0.0000], Avg: [-1122.322 -1122.322 -1122.322] (1.000)
Step: 34049, Reward: [-1143.664 -1143.664 -1143.664] [0.0000], Avg: [-1122.354 -1122.354 -1122.354] (1.000)
Step: 34099, Reward: [-440.785 -440.785 -440.785] [0.0000], Avg: [-1121.354 -1121.354 -1121.354] (1.000)
Step: 34149, Reward: [-1217.958 -1217.958 -1217.958] [0.0000], Avg: [-1121.496 -1121.496 -1121.496] (1.000)
Step: 34199, Reward: [-626.693 -626.693 -626.693] [0.0000], Avg: [-1120.772 -1120.772 -1120.772] (1.000)
Step: 34249, Reward: [-634.816 -634.816 -634.816] [0.0000], Avg: [-1120.063 -1120.063 -1120.063] (1.000)
Step: 34299, Reward: [-1256.702 -1256.702 -1256.702] [0.0000], Avg: [-1120.262 -1120.262 -1120.262] (1.000)
Step: 34349, Reward: [-1244.739 -1244.739 -1244.739] [0.0000], Avg: [-1120.443 -1120.443 -1120.443] (1.000)
Step: 34399, Reward: [-1080.54 -1080.54 -1080.54] [0.0000], Avg: [-1120.385 -1120.385 -1120.385] (1.000)
Step: 34449, Reward: [-864.681 -864.681 -864.681] [0.0000], Avg: [-1120.014 -1120.014 -1120.014] (1.000)
Step: 34499, Reward: [-951.701 -951.701 -951.701] [0.0000], Avg: [-1119.77 -1119.77 -1119.77] (1.000)
Step: 34549, Reward: [-352.941 -352.941 -352.941] [0.0000], Avg: [-1118.661 -1118.661 -1118.661] (1.000)
Step: 34599, Reward: [-1504.548 -1504.548 -1504.548] [0.0000], Avg: [-1119.218 -1119.218 -1119.218] (1.000)
Step: 34649, Reward: [-1450.878 -1450.878 -1450.878] [0.0000], Avg: [-1119.697 -1119.697 -1119.697] (1.000)
Step: 34699, Reward: [-683.546 -683.546 -683.546] [0.0000], Avg: [-1119.068 -1119.068 -1119.068] (1.000)
Step: 34749, Reward: [-1709.672 -1709.672 -1709.672] [0.0000], Avg: [-1119.918 -1119.918 -1119.918] (1.000)
Step: 34799, Reward: [-765.433 -765.433 -765.433] [0.0000], Avg: [-1119.409 -1119.409 -1119.409] (1.000)
Step: 34849, Reward: [-1846.751 -1846.751 -1846.751] [0.0000], Avg: [-1120.452 -1120.452 -1120.452] (1.000)
Step: 34899, Reward: [-412.578 -412.578 -412.578] [0.0000], Avg: [-1119.438 -1119.438 -1119.438] (1.000)
Step: 34949, Reward: [-1191.145 -1191.145 -1191.145] [0.0000], Avg: [-1119.541 -1119.541 -1119.541] (1.000)
Step: 34999, Reward: [-1738.683 -1738.683 -1738.683] [0.0000], Avg: [-1120.425 -1120.425 -1120.425] (1.000)
Step: 35049, Reward: [-1713.49 -1713.49 -1713.49] [0.0000], Avg: [-1121.271 -1121.271 -1121.271] (1.000)
Step: 35099, Reward: [-635.478 -635.478 -635.478] [0.0000], Avg: [-1120.579 -1120.579 -1120.579] (1.000)
Step: 35149, Reward: [-1523.707 -1523.707 -1523.707] [0.0000], Avg: [-1121.153 -1121.153 -1121.153] (1.000)
Step: 35199, Reward: [-1111.223 -1111.223 -1111.223] [0.0000], Avg: [-1121.139 -1121.139 -1121.139] (1.000)
Step: 35249, Reward: [-1011.595 -1011.595 -1011.595] [0.0000], Avg: [-1120.983 -1120.983 -1120.983] (1.000)
Step: 35299, Reward: [-1221.074 -1221.074 -1221.074] [0.0000], Avg: [-1121.125 -1121.125 -1121.125] (1.000)
Step: 35349, Reward: [-989.171 -989.171 -989.171] [0.0000], Avg: [-1120.938 -1120.938 -1120.938] (1.000)
Step: 35399, Reward: [-1510.239 -1510.239 -1510.239] [0.0000], Avg: [-1121.488 -1121.488 -1121.488] (1.000)
Step: 35449, Reward: [-613.221 -613.221 -613.221] [0.0000], Avg: [-1120.771 -1120.771 -1120.771] (1.000)
Step: 35499, Reward: [-970.459 -970.459 -970.459] [0.0000], Avg: [-1120.56 -1120.56 -1120.56] (1.000)
Step: 35549, Reward: [-1536.086 -1536.086 -1536.086] [0.0000], Avg: [-1121.144 -1121.144 -1121.144] (1.000)
Step: 35599, Reward: [-1711.013 -1711.013 -1711.013] [0.0000], Avg: [-1121.973 -1121.973 -1121.973] (1.000)
Step: 35649, Reward: [-1178.223 -1178.223 -1178.223] [0.0000], Avg: [-1122.051 -1122.051 -1122.051] (1.000)
Step: 35699, Reward: [-1658.314 -1658.314 -1658.314] [0.0000], Avg: [-1122.802 -1122.802 -1122.802] (1.000)
Step: 35749, Reward: [-1695.102 -1695.102 -1695.102] [0.0000], Avg: [-1123.603 -1123.603 -1123.603] (1.000)
Step: 35799, Reward: [-1724.083 -1724.083 -1724.083] [0.0000], Avg: [-1124.442 -1124.442 -1124.442] (1.000)
Step: 35849, Reward: [-1567.937 -1567.937 -1567.937] [0.0000], Avg: [-1125.06 -1125.06 -1125.06] (1.000)
Step: 35899, Reward: [-1260.545 -1260.545 -1260.545] [0.0000], Avg: [-1125.249 -1125.249 -1125.249] (1.000)
Step: 35949, Reward: [-1837.648 -1837.648 -1837.648] [0.0000], Avg: [-1126.24 -1126.24 -1126.24] (1.000)
Step: 35999, Reward: [-833.609 -833.609 -833.609] [0.0000], Avg: [-1125.833 -1125.833 -1125.833] (1.000)
Step: 36049, Reward: [-972.269 -972.269 -972.269] [0.0000], Avg: [-1125.62 -1125.62 -1125.62] (1.000)
Step: 36099, Reward: [-1259.36 -1259.36 -1259.36] [0.0000], Avg: [-1125.805 -1125.805 -1125.805] (1.000)
Step: 36149, Reward: [-1947.161 -1947.161 -1947.161] [0.0000], Avg: [-1126.941 -1126.941 -1126.941] (1.000)
Step: 36199, Reward: [-909.148 -909.148 -909.148] [0.0000], Avg: [-1126.641 -1126.641 -1126.641] (1.000)
Step: 36249, Reward: [-1361.255 -1361.255 -1361.255] [0.0000], Avg: [-1126.964 -1126.964 -1126.964] (1.000)
Step: 36299, Reward: [-1764.318 -1764.318 -1764.318] [0.0000], Avg: [-1127.842 -1127.842 -1127.842] (1.000)
Step: 36349, Reward: [-1358.735 -1358.735 -1358.735] [0.0000], Avg: [-1128.16 -1128.16 -1128.16] (1.000)
Step: 36399, Reward: [-1388.068 -1388.068 -1388.068] [0.0000], Avg: [-1128.517 -1128.517 -1128.517] (1.000)
Step: 36449, Reward: [-1138.347 -1138.347 -1138.347] [0.0000], Avg: [-1128.53 -1128.53 -1128.53] (1.000)
Step: 36499, Reward: [-1057.529 -1057.529 -1057.529] [0.0000], Avg: [-1128.433 -1128.433 -1128.433] (1.000)
Step: 36549, Reward: [-656.807 -656.807 -656.807] [0.0000], Avg: [-1127.788 -1127.788 -1127.788] (1.000)
Step: 36599, Reward: [-1943.572 -1943.572 -1943.572] [0.0000], Avg: [-1128.902 -1128.902 -1128.902] (1.000)
Step: 36649, Reward: [-1696.432 -1696.432 -1696.432] [0.0000], Avg: [-1129.677 -1129.677 -1129.677] (1.000)
Step: 36699, Reward: [-1675.864 -1675.864 -1675.864] [0.0000], Avg: [-1130.421 -1130.421 -1130.421] (1.000)
Step: 36749, Reward: [-1725.864 -1725.864 -1725.864] [0.0000], Avg: [-1131.231 -1131.231 -1131.231] (1.000)
Step: 36799, Reward: [-1649.819 -1649.819 -1649.819] [0.0000], Avg: [-1131.935 -1131.935 -1131.935] (1.000)
Step: 36849, Reward: [-1437.904 -1437.904 -1437.904] [0.0000], Avg: [-1132.351 -1132.351 -1132.351] (1.000)
Step: 36899, Reward: [-1670.078 -1670.078 -1670.078] [0.0000], Avg: [-1133.079 -1133.079 -1133.079] (1.000)
Step: 36949, Reward: [-1351.675 -1351.675 -1351.675] [0.0000], Avg: [-1133.375 -1133.375 -1133.375] (1.000)
Step: 36999, Reward: [-1898.573 -1898.573 -1898.573] [0.0000], Avg: [-1134.409 -1134.409 -1134.409] (1.000)
Step: 37049, Reward: [-2083.259 -2083.259 -2083.259] [0.0000], Avg: [-1135.69 -1135.69 -1135.69] (1.000)
Step: 37099, Reward: [-1837.796 -1837.796 -1837.796] [0.0000], Avg: [-1136.636 -1136.636 -1136.636] (1.000)
Step: 37149, Reward: [-1158.111 -1158.111 -1158.111] [0.0000], Avg: [-1136.665 -1136.665 -1136.665] (1.000)
Step: 37199, Reward: [-1896.172 -1896.172 -1896.172] [0.0000], Avg: [-1137.686 -1137.686 -1137.686] (1.000)
Step: 37249, Reward: [-1500.944 -1500.944 -1500.944] [0.0000], Avg: [-1138.173 -1138.173 -1138.173] (1.000)
Step: 37299, Reward: [-1772.612 -1772.612 -1772.612] [0.0000], Avg: [-1139.024 -1139.024 -1139.024] (1.000)
Step: 37349, Reward: [-1171.268 -1171.268 -1171.268] [0.0000], Avg: [-1139.067 -1139.067 -1139.067] (1.000)
Step: 37399, Reward: [-1578.989 -1578.989 -1578.989] [0.0000], Avg: [-1139.655 -1139.655 -1139.655] (1.000)
Step: 37449, Reward: [-1067.842 -1067.842 -1067.842] [0.0000], Avg: [-1139.559 -1139.559 -1139.559] (1.000)
Step: 37499, Reward: [-1880.832 -1880.832 -1880.832] [0.0000], Avg: [-1140.547 -1140.547 -1140.547] (1.000)
Step: 37549, Reward: [-736.736 -736.736 -736.736] [0.0000], Avg: [-1140.01 -1140.01 -1140.01] (1.000)
Step: 37599, Reward: [-1155.484 -1155.484 -1155.484] [0.0000], Avg: [-1140.03 -1140.03 -1140.03] (1.000)
Step: 37649, Reward: [-1069.854 -1069.854 -1069.854] [0.0000], Avg: [-1139.937 -1139.937 -1139.937] (1.000)
Step: 37699, Reward: [-1074.982 -1074.982 -1074.982] [0.0000], Avg: [-1139.851 -1139.851 -1139.851] (1.000)
Step: 37749, Reward: [-537.166 -537.166 -537.166] [0.0000], Avg: [-1139.053 -1139.053 -1139.053] (1.000)
Step: 37799, Reward: [-1439.095 -1439.095 -1439.095] [0.0000], Avg: [-1139.449 -1139.449 -1139.449] (1.000)
Step: 37849, Reward: [-914.861 -914.861 -914.861] [0.0000], Avg: [-1139.153 -1139.153 -1139.153] (1.000)
Step: 37899, Reward: [-1128.565 -1128.565 -1128.565] [0.0000], Avg: [-1139.139 -1139.139 -1139.139] (1.000)
Step: 37949, Reward: [-733.711 -733.711 -733.711] [0.0000], Avg: [-1138.605 -1138.605 -1138.605] (1.000)
