Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		# self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		# action_sig = self.action_sig(state).tanh().exp()
		# epsilon = torch.randn_like(action_sig)
		# action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		action = gumbel_softmax(action_mu, hard=True)
		action = action.view(*out_dims, -1)
		return action
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		# self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		# out_dims = state.size()[:-1]
		# state = state.view(int(np.prod(out_dims)), -1)
		# state = self.norm(state)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		# q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else one_hot(action) for j,action in enumerate(actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -(q_actions - q_values.detach()) + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
			to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
			states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=True, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		print(actions)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-477.325 -477.325 -477.325] [0.0000], Avg: [-477.325 -477.325 -477.325] (0.995)
Step: 99, Reward: [-537.906 -537.906 -537.906] [0.0000], Avg: [-507.616 -507.616 -507.616] (0.990)
Step: 149, Reward: [-509.999 -509.999 -509.999] [0.0000], Avg: [-508.41 -508.41 -508.41] (0.985)
Step: 199, Reward: [-580.978 -580.978 -580.978] [0.0000], Avg: [-526.552 -526.552 -526.552] (0.980)
Step: 249, Reward: [-591.085 -591.085 -591.085] [0.0000], Avg: [-539.459 -539.459 -539.459] (0.975)
Step: 299, Reward: [-512.244 -512.244 -512.244] [0.0000], Avg: [-534.923 -534.923 -534.923] (0.970)
Step: 349, Reward: [-433.365 -433.365 -433.365] [0.0000], Avg: [-520.415 -520.415 -520.415] (0.966)
Step: 399, Reward: [-416.144 -416.144 -416.144] [0.0000], Avg: [-507.381 -507.381 -507.381] (0.961)
Step: 449, Reward: [-351.507 -351.507 -351.507] [0.0000], Avg: [-490.062 -490.062 -490.062] (0.956)
Step: 499, Reward: [-454.98 -454.98 -454.98] [0.0000], Avg: [-486.553 -486.553 -486.553] (0.951)
Step: 549, Reward: [-505.998 -505.998 -505.998] [0.0000], Avg: [-488.321 -488.321 -488.321] (0.946)
Step: 599, Reward: [-401.289 -401.289 -401.289] [0.0000], Avg: [-481.068 -481.068 -481.068] (0.942)
Step: 649, Reward: [-636.89 -636.89 -636.89] [0.0000], Avg: [-493.055 -493.055 -493.055] (0.937)
Step: 699, Reward: [-473.881 -473.881 -473.881] [0.0000], Avg: [-491.685 -491.685 -491.685] (0.932)
Step: 749, Reward: [-556.236 -556.236 -556.236] [0.0000], Avg: [-495.989 -495.989 -495.989] (0.928)
Step: 799, Reward: [-528.601 -528.601 -528.601] [0.0000], Avg: [-498.027 -498.027 -498.027] (0.923)
Step: 849, Reward: [-547.019 -547.019 -547.019] [0.0000], Avg: [-500.909 -500.909 -500.909] (0.918)
Step: 899, Reward: [-460.865 -460.865 -460.865] [0.0000], Avg: [-498.684 -498.684 -498.684] (0.914)
Step: 949, Reward: [-679.582 -679.582 -679.582] [0.0000], Avg: [-508.205 -508.205 -508.205] (0.909)
Step: 999, Reward: [-370.935 -370.935 -370.935] [0.0000], Avg: [-501.342 -501.342 -501.342] (0.905)
Step: 1049, Reward: [-520.757 -520.757 -520.757] [0.0000], Avg: [-502.266 -502.266 -502.266] (0.900)
Step: 1099, Reward: [-449.835 -449.835 -449.835] [0.0000], Avg: [-499.883 -499.883 -499.883] (0.896)
Step: 1149, Reward: [-635.437 -635.437 -635.437] [0.0000], Avg: [-505.777 -505.777 -505.777] (0.891)
Step: 1199, Reward: [-556.954 -556.954 -556.954] [0.0000], Avg: [-507.909 -507.909 -507.909] (0.887)
Step: 1249, Reward: [-451.217 -451.217 -451.217] [0.0000], Avg: [-505.641 -505.641 -505.641] (0.882)
Step: 1299, Reward: [-466.783 -466.783 -466.783] [0.0000], Avg: [-504.147 -504.147 -504.147] (0.878)
Step: 1349, Reward: [-558.105 -558.105 -558.105] [0.0000], Avg: [-506.145 -506.145 -506.145] (0.873)
Step: 1399, Reward: [-397.663 -397.663 -397.663] [0.0000], Avg: [-502.271 -502.271 -502.271] (0.869)
Step: 1449, Reward: [-397.825 -397.825 -397.825] [0.0000], Avg: [-498.669 -498.669 -498.669] (0.865)
Step: 1499, Reward: [-469.52 -469.52 -469.52] [0.0000], Avg: [-497.698 -497.698 -497.698] (0.860)
Step: 1549, Reward: [-496.25 -496.25 -496.25] [0.0000], Avg: [-497.651 -497.651 -497.651] (0.856)
Step: 1599, Reward: [-508.571 -508.571 -508.571] [0.0000], Avg: [-497.992 -497.992 -497.992] (0.852)
Step: 1649, Reward: [-766.381 -766.381 -766.381] [0.0000], Avg: [-506.125 -506.125 -506.125] (0.848)
Step: 1699, Reward: [-474.239 -474.239 -474.239] [0.0000], Avg: [-505.187 -505.187 -505.187] (0.843)
Step: 1749, Reward: [-391.104 -391.104 -391.104] [0.0000], Avg: [-501.928 -501.928 -501.928] (0.839)
Step: 1799, Reward: [-574.73 -574.73 -574.73] [0.0000], Avg: [-503.95 -503.95 -503.95] (0.835)
Step: 1849, Reward: [-517.039 -517.039 -517.039] [0.0000], Avg: [-504.304 -504.304 -504.304] (0.831)
Step: 1899, Reward: [-535.962 -535.962 -535.962] [0.0000], Avg: [-505.137 -505.137 -505.137] (0.827)
Step: 1949, Reward: [-450.417 -450.417 -450.417] [0.0000], Avg: [-503.734 -503.734 -503.734] (0.822)
Step: 1999, Reward: [-654.949 -654.949 -654.949] [0.0000], Avg: [-507.514 -507.514 -507.514] (0.818)
Step: 2049, Reward: [-501.112 -501.112 -501.112] [0.0000], Avg: [-507.358 -507.358 -507.358] (0.814)
Step: 2099, Reward: [-439.736 -439.736 -439.736] [0.0000], Avg: [-505.748 -505.748 -505.748] (0.810)
Step: 2149, Reward: [-442.797 -442.797 -442.797] [0.0000], Avg: [-504.284 -504.284 -504.284] (0.806)
Step: 2199, Reward: [-539.339 -539.339 -539.339] [0.0000], Avg: [-505.081 -505.081 -505.081] (0.802)
Step: 2249, Reward: [-539.097 -539.097 -539.097] [0.0000], Avg: [-505.837 -505.837 -505.837] (0.798)
Step: 2299, Reward: [-694.242 -694.242 -694.242] [0.0000], Avg: [-509.932 -509.932 -509.932] (0.794)
Step: 2349, Reward: [-395.743 -395.743 -395.743] [0.0000], Avg: [-507.503 -507.503 -507.503] (0.790)
Step: 2399, Reward: [-427.172 -427.172 -427.172] [0.0000], Avg: [-505.829 -505.829 -505.829] (0.786)
Step: 2449, Reward: [-575.701 -575.701 -575.701] [0.0000], Avg: [-507.255 -507.255 -507.255] (0.782)
Step: 2499, Reward: [-378.671 -378.671 -378.671] [0.0000], Avg: [-504.684 -504.684 -504.684] (0.778)
Step: 2549, Reward: [-545.691 -545.691 -545.691] [0.0000], Avg: [-505.488 -505.488 -505.488] (0.774)
Step: 2599, Reward: [-578.392 -578.392 -578.392] [0.0000], Avg: [-506.89 -506.89 -506.89] (0.771)
Step: 2649, Reward: [-583.61 -583.61 -583.61] [0.0000], Avg: [-508.337 -508.337 -508.337] (0.767)
Step: 2699, Reward: [-551.56 -551.56 -551.56] [0.0000], Avg: [-509.138 -509.138 -509.138] (0.763)
Step: 2749, Reward: [-452.768 -452.768 -452.768] [0.0000], Avg: [-508.113 -508.113 -508.113] (0.759)
Step: 2799, Reward: [-516.943 -516.943 -516.943] [0.0000], Avg: [-508.27 -508.27 -508.27] (0.755)
Step: 2849, Reward: [-573.588 -573.588 -573.588] [0.0000], Avg: [-509.416 -509.416 -509.416] (0.751)
Step: 2899, Reward: [-566.106 -566.106 -566.106] [0.0000], Avg: [-510.394 -510.394 -510.394] (0.748)
Step: 2949, Reward: [-411.932 -411.932 -411.932] [0.0000], Avg: [-508.725 -508.725 -508.725] (0.744)
Step: 2999, Reward: [-474.61 -474.61 -474.61] [0.0000], Avg: [-508.156 -508.156 -508.156] (0.740)
Step: 3049, Reward: [-548.044 -548.044 -548.044] [0.0000], Avg: [-508.81 -508.81 -508.81] (0.737)
Step: 3099, Reward: [-498.729 -498.729 -498.729] [0.0000], Avg: [-508.648 -508.648 -508.648] (0.733)
Step: 3149, Reward: [-398.047 -398.047 -398.047] [0.0000], Avg: [-506.892 -506.892 -506.892] (0.729)
Step: 3199, Reward: [-477.094 -477.094 -477.094] [0.0000], Avg: [-506.426 -506.426 -506.426] (0.726)
Step: 3249, Reward: [-500.594 -500.594 -500.594] [0.0000], Avg: [-506.337 -506.337 -506.337] (0.722)
Step: 3299, Reward: [-332.884 -332.884 -332.884] [0.0000], Avg: [-503.709 -503.709 -503.709] (0.718)
Step: 3349, Reward: [-573.129 -573.129 -573.129] [0.0000], Avg: [-504.745 -504.745 -504.745] (0.715)
Step: 3399, Reward: [-589.182 -589.182 -589.182] [0.0000], Avg: [-505.987 -505.987 -505.987] (0.711)
Step: 3449, Reward: [-365.804 -365.804 -365.804] [0.0000], Avg: [-503.955 -503.955 -503.955] (0.708)
Step: 3499, Reward: [-496.361 -496.361 -496.361] [0.0000], Avg: [-503.846 -503.846 -503.846] (0.704)
Step: 3549, Reward: [-490.214 -490.214 -490.214] [0.0000], Avg: [-503.654 -503.654 -503.654] (0.701)
Step: 3599, Reward: [-565.472 -565.472 -565.472] [0.0000], Avg: [-504.513 -504.513 -504.513] (0.697)
Step: 3649, Reward: [-780.942 -780.942 -780.942] [0.0000], Avg: [-508.3 -508.3 -508.3] (0.694)
Step: 3699, Reward: [-684.326 -684.326 -684.326] [0.0000], Avg: [-510.678 -510.678 -510.678] (0.690)
Step: 3749, Reward: [-542.668 -542.668 -542.668] [0.0000], Avg: [-511.105 -511.105 -511.105] (0.687)
Step: 3799, Reward: [-489.199 -489.199 -489.199] [0.0000], Avg: [-510.817 -510.817 -510.817] (0.683)
Step: 3849, Reward: [-321.077 -321.077 -321.077] [0.0000], Avg: [-508.353 -508.353 -508.353] (0.680)
Step: 3899, Reward: [-357.559 -357.559 -357.559] [0.0000], Avg: [-506.419 -506.419 -506.419] (0.676)
Step: 3949, Reward: [-502.684 -502.684 -502.684] [0.0000], Avg: [-506.372 -506.372 -506.372] (0.673)
Step: 3999, Reward: [-537.995 -537.995 -537.995] [0.0000], Avg: [-506.767 -506.767 -506.767] (0.670)
Step: 4049, Reward: [-520.807 -520.807 -520.807] [0.0000], Avg: [-506.941 -506.941 -506.941] (0.666)
Step: 4099, Reward: [-757.428 -757.428 -757.428] [0.0000], Avg: [-509.995 -509.995 -509.995] (0.663)
Step: 4149, Reward: [-549.894 -549.894 -549.894] [0.0000], Avg: [-510.476 -510.476 -510.476] (0.660)
Step: 4199, Reward: [-554.58 -554.58 -554.58] [0.0000], Avg: [-511.001 -511.001 -511.001] (0.656)
Step: 4249, Reward: [-511.764 -511.764 -511.764] [0.0000], Avg: [-511.01 -511.01 -511.01] (0.653)
Step: 4299, Reward: [-503.356 -503.356 -503.356] [0.0000], Avg: [-510.921 -510.921 -510.921] (0.650)
Step: 4349, Reward: [-622.227 -622.227 -622.227] [0.0000], Avg: [-512.2 -512.2 -512.2] (0.647)
Step: 4399, Reward: [-591.195 -591.195 -591.195] [0.0000], Avg: [-513.098 -513.098 -513.098] (0.643)
Step: 4449, Reward: [-492.769 -492.769 -492.769] [0.0000], Avg: [-512.87 -512.87 -512.87] (0.640)
Step: 4499, Reward: [-645.297 -645.297 -645.297] [0.0000], Avg: [-514.341 -514.341 -514.341] (0.637)
Step: 4549, Reward: [-541.028 -541.028 -541.028] [0.0000], Avg: [-514.634 -514.634 -514.634] (0.634)
Step: 4599, Reward: [-734.512 -734.512 -734.512] [0.0000], Avg: [-517.024 -517.024 -517.024] (0.631)
Step: 4649, Reward: [-510.021 -510.021 -510.021] [0.0000], Avg: [-516.949 -516.949 -516.949] (0.627)
Step: 4699, Reward: [-358.659 -358.659 -358.659] [0.0000], Avg: [-515.265 -515.265 -515.265] (0.624)
Step: 4749, Reward: [-665.82 -665.82 -665.82] [0.0000], Avg: [-516.85 -516.85 -516.85] (0.621)
Step: 4799, Reward: [-535.846 -535.846 -535.846] [0.0000], Avg: [-517.048 -517.048 -517.048] (0.618)
Step: 4849, Reward: [-620.907 -620.907 -620.907] [0.0000], Avg: [-518.118 -518.118 -518.118] (0.615)
Step: 4899, Reward: [-516.17 -516.17 -516.17] [0.0000], Avg: [-518.099 -518.099 -518.099] (0.612)
Step: 4949, Reward: [-559.283 -559.283 -559.283] [0.0000], Avg: [-518.515 -518.515 -518.515] (0.609)
Step: 4999, Reward: [-583.658 -583.658 -583.658] [0.0000], Avg: [-519.166 -519.166 -519.166] (0.606)
Step: 5049, Reward: [-397.252 -397.252 -397.252] [0.0000], Avg: [-517.959 -517.959 -517.959] (0.603)
Step: 5099, Reward: [-358.847 -358.847 -358.847] [0.0000], Avg: [-516.399 -516.399 -516.399] (0.600)
Step: 5149, Reward: [-470.249 -470.249 -470.249] [0.0000], Avg: [-515.951 -515.951 -515.951] (0.597)
Step: 5199, Reward: [-637.975 -637.975 -637.975] [0.0000], Avg: [-517.124 -517.124 -517.124] (0.594)
Step: 5249, Reward: [-760.683 -760.683 -760.683] [0.0000], Avg: [-519.444 -519.444 -519.444] (0.591)
Step: 5299, Reward: [-491.595 -491.595 -491.595] [0.0000], Avg: [-519.181 -519.181 -519.181] (0.588)
Step: 5349, Reward: [-443.069 -443.069 -443.069] [0.0000], Avg: [-518.47 -518.47 -518.47] (0.585)
Step: 5399, Reward: [-595.956 -595.956 -595.956] [0.0000], Avg: [-519.187 -519.187 -519.187] (0.582)
Step: 5449, Reward: [-520.637 -520.637 -520.637] [0.0000], Avg: [-519.201 -519.201 -519.201] (0.579)
Step: 5499, Reward: [-469.862 -469.862 -469.862] [0.0000], Avg: [-518.752 -518.752 -518.752] (0.576)
Step: 5549, Reward: [-421.196 -421.196 -421.196] [0.0000], Avg: [-517.873 -517.873 -517.873] (0.573)
Step: 5599, Reward: [-341.153 -341.153 -341.153] [0.0000], Avg: [-516.295 -516.295 -516.295] (0.570)
Step: 5649, Reward: [-393.462 -393.462 -393.462] [0.0000], Avg: [-515.208 -515.208 -515.208] (0.568)
Step: 5699, Reward: [-397.783 -397.783 -397.783] [0.0000], Avg: [-514.178 -514.178 -514.178] (0.565)
Step: 5749, Reward: [-725.106 -725.106 -725.106] [0.0000], Avg: [-516.012 -516.012 -516.012] (0.562)
Step: 5799, Reward: [-506.819 -506.819 -506.819] [0.0000], Avg: [-515.933 -515.933 -515.933] (0.559)
Step: 5849, Reward: [-454.299 -454.299 -454.299] [0.0000], Avg: [-515.406 -515.406 -515.406] (0.556)
Step: 5899, Reward: [-596.267 -596.267 -596.267] [0.0000], Avg: [-516.092 -516.092 -516.092] (0.554)
Step: 5949, Reward: [-503.941 -503.941 -503.941] [0.0000], Avg: [-515.99 -515.99 -515.99] (0.551)
Step: 5999, Reward: [-538.092 -538.092 -538.092] [0.0000], Avg: [-516.174 -516.174 -516.174] (0.548)
Step: 6049, Reward: [-386.803 -386.803 -386.803] [0.0000], Avg: [-515.105 -515.105 -515.105] (0.545)
Step: 6099, Reward: [-343.487 -343.487 -343.487] [0.0000], Avg: [-513.698 -513.698 -513.698] (0.543)
Step: 6149, Reward: [-408.061 -408.061 -408.061] [0.0000], Avg: [-512.839 -512.839 -512.839] (0.540)
Step: 6199, Reward: [-585.376 -585.376 -585.376] [0.0000], Avg: [-513.424 -513.424 -513.424] (0.537)
Step: 6249, Reward: [-557.839 -557.839 -557.839] [0.0000], Avg: [-513.779 -513.779 -513.779] (0.534)
Step: 6299, Reward: [-376.973 -376.973 -376.973] [0.0000], Avg: [-512.694 -512.694 -512.694] (0.532)
Step: 6349, Reward: [-523.949 -523.949 -523.949] [0.0000], Avg: [-512.782 -512.782 -512.782] (0.529)
Step: 6399, Reward: [-646.837 -646.837 -646.837] [0.0000], Avg: [-513.829 -513.829 -513.829] (0.526)
Step: 6449, Reward: [-441.466 -441.466 -441.466] [0.0000], Avg: [-513.268 -513.268 -513.268] (0.524)
Step: 6499, Reward: [-725.193 -725.193 -725.193] [0.0000], Avg: [-514.899 -514.899 -514.899] (0.521)
Step: 6549, Reward: [-695.21 -695.21 -695.21] [0.0000], Avg: [-516.275 -516.275 -516.275] (0.519)
Step: 6599, Reward: [-517.367 -517.367 -517.367] [0.0000], Avg: [-516.283 -516.283 -516.283] (0.516)
Step: 6649, Reward: [-375.432 -375.432 -375.432] [0.0000], Avg: [-515.224 -515.224 -515.224] (0.513)
Step: 6699, Reward: [-447.118 -447.118 -447.118] [0.0000], Avg: [-514.716 -514.716 -514.716] (0.511)
Step: 6749, Reward: [-380.771 -380.771 -380.771] [0.0000], Avg: [-513.724 -513.724 -513.724] (0.508)
Step: 6799, Reward: [-413.578 -413.578 -413.578] [0.0000], Avg: [-512.988 -512.988 -512.988] (0.506)
Step: 6849, Reward: [-580.005 -580.005 -580.005] [0.0000], Avg: [-513.477 -513.477 -513.477] (0.503)
Step: 6899, Reward: [-772.228 -772.228 -772.228] [0.0000], Avg: [-515.352 -515.352 -515.352] (0.501)
Step: 6949, Reward: [-819.698 -819.698 -819.698] [0.0000], Avg: [-517.541 -517.541 -517.541] (0.498)
Step: 6999, Reward: [-656.48 -656.48 -656.48] [0.0000], Avg: [-518.534 -518.534 -518.534] (0.496)
Step: 7049, Reward: [-644.123 -644.123 -644.123] [0.0000], Avg: [-519.424 -519.424 -519.424] (0.493)
Step: 7099, Reward: [-471.457 -471.457 -471.457] [0.0000], Avg: [-519.087 -519.087 -519.087] (0.491)
Step: 7149, Reward: [-532.542 -532.542 -532.542] [0.0000], Avg: [-519.181 -519.181 -519.181] (0.488)
Step: 7199, Reward: [-442.693 -442.693 -442.693] [0.0000], Avg: [-518.65 -518.65 -518.65] (0.486)
Step: 7249, Reward: [-515.787 -515.787 -515.787] [0.0000], Avg: [-518.63 -518.63 -518.63] (0.483)
Step: 7299, Reward: [-420.654 -420.654 -420.654] [0.0000], Avg: [-517.959 -517.959 -517.959] (0.481)
Step: 7349, Reward: [-449.197 -449.197 -449.197] [0.0000], Avg: [-517.491 -517.491 -517.491] (0.479)
Step: 7399, Reward: [-570.937 -570.937 -570.937] [0.0000], Avg: [-517.852 -517.852 -517.852] (0.476)
Step: 7449, Reward: [-462.701 -462.701 -462.701] [0.0000], Avg: [-517.482 -517.482 -517.482] (0.474)
Step: 7499, Reward: [-513.955 -513.955 -513.955] [0.0000], Avg: [-517.458 -517.458 -517.458] (0.471)
Step: 7549, Reward: [-497.914 -497.914 -497.914] [0.0000], Avg: [-517.329 -517.329 -517.329] (0.469)
Step: 7599, Reward: [-560.075 -560.075 -560.075] [0.0000], Avg: [-517.61 -517.61 -517.61] (0.467)
Step: 7649, Reward: [-418.525 -418.525 -418.525] [0.0000], Avg: [-516.963 -516.963 -516.963] (0.464)
Step: 7699, Reward: [-728.377 -728.377 -728.377] [0.0000], Avg: [-518.335 -518.335 -518.335] (0.462)
Step: 7749, Reward: [-594.252 -594.252 -594.252] [0.0000], Avg: [-518.825 -518.825 -518.825] (0.460)
Step: 7799, Reward: [-681.088 -681.088 -681.088] [0.0000], Avg: [-519.865 -519.865 -519.865] (0.458)
Step: 7849, Reward: [-475.694 -475.694 -475.694] [0.0000], Avg: [-519.584 -519.584 -519.584] (0.455)
Step: 7899, Reward: [-627.552 -627.552 -627.552] [0.0000], Avg: [-520.267 -520.267 -520.267] (0.453)
Step: 7949, Reward: [-758.988 -758.988 -758.988] [0.0000], Avg: [-521.769 -521.769 -521.769] (0.451)
Step: 7999, Reward: [-626.046 -626.046 -626.046] [0.0000], Avg: [-522.42 -522.42 -522.42] (0.448)
Step: 8049, Reward: [-546.036 -546.036 -546.036] [0.0000], Avg: [-522.567 -522.567 -522.567] (0.446)
Step: 8099, Reward: [-833.756 -833.756 -833.756] [0.0000], Avg: [-524.488 -524.488 -524.488] (0.444)
Step: 8149, Reward: [-779.888 -779.888 -779.888] [0.0000], Avg: [-526.055 -526.055 -526.055] (0.442)
Step: 8199, Reward: [-818.934 -818.934 -818.934] [0.0000], Avg: [-527.841 -527.841 -527.841] (0.440)
Step: 8249, Reward: [-512.809 -512.809 -512.809] [0.0000], Avg: [-527.75 -527.75 -527.75] (0.437)
Step: 8299, Reward: [-607.739 -607.739 -607.739] [0.0000], Avg: [-528.232 -528.232 -528.232] (0.435)
Step: 8349, Reward: [-554.429 -554.429 -554.429] [0.0000], Avg: [-528.388 -528.388 -528.388] (0.433)
Step: 8399, Reward: [-751.729 -751.729 -751.729] [0.0000], Avg: [-529.718 -529.718 -529.718] (0.431)
Step: 8449, Reward: [-576.262 -576.262 -576.262] [0.0000], Avg: [-529.993 -529.993 -529.993] (0.429)
Step: 8499, Reward: [-423.556 -423.556 -423.556] [0.0000], Avg: [-529.367 -529.367 -529.367] (0.427)
Step: 8549, Reward: [-648.07 -648.07 -648.07] [0.0000], Avg: [-530.061 -530.061 -530.061] (0.424)
Step: 8599, Reward: [-580.179 -580.179 -580.179] [0.0000], Avg: [-530.353 -530.353 -530.353] (0.422)
Step: 8649, Reward: [-635.714 -635.714 -635.714] [0.0000], Avg: [-530.962 -530.962 -530.962] (0.420)
Step: 8699, Reward: [-646.538 -646.538 -646.538] [0.0000], Avg: [-531.626 -531.626 -531.626] (0.418)
Step: 8749, Reward: [-573.84 -573.84 -573.84] [0.0000], Avg: [-531.867 -531.867 -531.867] (0.416)
Step: 8799, Reward: [-516.096 -516.096 -516.096] [0.0000], Avg: [-531.778 -531.778 -531.778] (0.414)
Step: 8849, Reward: [-523.974 -523.974 -523.974] [0.0000], Avg: [-531.733 -531.733 -531.733] (0.412)
Step: 8899, Reward: [-475.286 -475.286 -475.286] [0.0000], Avg: [-531.416 -531.416 -531.416] (0.410)
Step: 8949, Reward: [-505.847 -505.847 -505.847] [0.0000], Avg: [-531.273 -531.273 -531.273] (0.408)
Step: 8999, Reward: [-494.551 -494.551 -494.551] [0.0000], Avg: [-531.069 -531.069 -531.069] (0.406)
Step: 9049, Reward: [-628.478 -628.478 -628.478] [0.0000], Avg: [-531.608 -531.608 -531.608] (0.404)
Step: 9099, Reward: [-626.012 -626.012 -626.012] [0.0000], Avg: [-532.126 -532.126 -532.126] (0.402)
Step: 9149, Reward: [-517.835 -517.835 -517.835] [0.0000], Avg: [-532.048 -532.048 -532.048] (0.400)
Step: 9199, Reward: [-623.331 -623.331 -623.331] [0.0000], Avg: [-532.544 -532.544 -532.544] (0.398)
Step: 9249, Reward: [-620.677 -620.677 -620.677] [0.0000], Avg: [-533.021 -533.021 -533.021] (0.396)
Step: 9299, Reward: [-417.396 -417.396 -417.396] [0.0000], Avg: [-532.399 -532.399 -532.399] (0.394)
Step: 9349, Reward: [-615.625 -615.625 -615.625] [0.0000], Avg: [-532.844 -532.844 -532.844] (0.392)
Step: 9399, Reward: [-768.915 -768.915 -768.915] [0.0000], Avg: [-534.1 -534.1 -534.1] (0.390)
Step: 9449, Reward: [-533.889 -533.889 -533.889] [0.0000], Avg: [-534.099 -534.099 -534.099] (0.388)
Step: 9499, Reward: [-623.126 -623.126 -623.126] [0.0000], Avg: [-534.567 -534.567 -534.567] (0.386)
Step: 9549, Reward: [-442.184 -442.184 -442.184] [0.0000], Avg: [-534.084 -534.084 -534.084] (0.384)
Step: 9599, Reward: [-1311.413 -1311.413 -1311.413] [0.0000], Avg: [-538.132 -538.132 -538.132] (0.382)
Step: 9649, Reward: [-903.453 -903.453 -903.453] [0.0000], Avg: [-540.025 -540.025 -540.025] (0.380)
Step: 9699, Reward: [-1701.801 -1701.801 -1701.801] [0.0000], Avg: [-546.014 -546.014 -546.014] (0.378)
Step: 9749, Reward: [-1368.237 -1368.237 -1368.237] [0.0000], Avg: [-550.23 -550.23 -550.23] (0.376)
Step: 9799, Reward: [-1170.893 -1170.893 -1170.893] [0.0000], Avg: [-553.397 -553.397 -553.397] (0.374)
Step: 9849, Reward: [-1295.907 -1295.907 -1295.907] [0.0000], Avg: [-557.166 -557.166 -557.166] (0.373)
Step: 9899, Reward: [-1132.295 -1132.295 -1132.295] [0.0000], Avg: [-560.071 -560.071 -560.071] (0.371)
Step: 9949, Reward: [-1452.323 -1452.323 -1452.323] [0.0000], Avg: [-564.554 -564.554 -564.554] (0.369)
Step: 9999, Reward: [-1226.526 -1226.526 -1226.526] [0.0000], Avg: [-567.864 -567.864 -567.864] (0.367)
Step: 10049, Reward: [-1454.328 -1454.328 -1454.328] [0.0000], Avg: [-572.274 -572.274 -572.274] (0.365)
Step: 10099, Reward: [-1303.133 -1303.133 -1303.133] [0.0000], Avg: [-575.892 -575.892 -575.892] (0.363)
Step: 10149, Reward: [-1260.3 -1260.3 -1260.3] [0.0000], Avg: [-579.264 -579.264 -579.264] (0.361)
Step: 10199, Reward: [-1168.902 -1168.902 -1168.902] [0.0000], Avg: [-582.154 -582.154 -582.154] (0.360)
Step: 10249, Reward: [-1453.277 -1453.277 -1453.277] [0.0000], Avg: [-586.404 -586.404 -586.404] (0.358)
Step: 10299, Reward: [-1367.022 -1367.022 -1367.022] [0.0000], Avg: [-590.193 -590.193 -590.193] (0.356)
Step: 10349, Reward: [-1207.479 -1207.479 -1207.479] [0.0000], Avg: [-593.175 -593.175 -593.175] (0.354)
Step: 10399, Reward: [-1418.261 -1418.261 -1418.261] [0.0000], Avg: [-597.142 -597.142 -597.142] (0.353)
Step: 10449, Reward: [-1174.55 -1174.55 -1174.55] [0.0000], Avg: [-599.905 -599.905 -599.905] (0.351)
Step: 10499, Reward: [-1264.496 -1264.496 -1264.496] [0.0000], Avg: [-603.069 -603.069 -603.069] (0.349)
Step: 10549, Reward: [-1195.45 -1195.45 -1195.45] [0.0000], Avg: [-605.877 -605.877 -605.877] (0.347)
Step: 10599, Reward: [-1291.734 -1291.734 -1291.734] [0.0000], Avg: [-609.112 -609.112 -609.112] (0.346)
Step: 10649, Reward: [-1749.673 -1749.673 -1749.673] [0.0000], Avg: [-614.467 -614.467 -614.467] (0.344)
Step: 10699, Reward: [-967.093 -967.093 -967.093] [0.0000], Avg: [-616.115 -616.115 -616.115] (0.342)
Step: 10749, Reward: [-1201.404 -1201.404 -1201.404] [0.0000], Avg: [-618.837 -618.837 -618.837] (0.340)
Step: 10799, Reward: [-1450.495 -1450.495 -1450.495] [0.0000], Avg: [-622.687 -622.687 -622.687] (0.339)
Step: 10849, Reward: [-1399.362 -1399.362 -1399.362] [0.0000], Avg: [-626.266 -626.266 -626.266] (0.337)
Step: 10899, Reward: [-817.673 -817.673 -817.673] [0.0000], Avg: [-627.144 -627.144 -627.144] (0.335)
Step: 10949, Reward: [-1745.984 -1745.984 -1745.984] [0.0000], Avg: [-632.253 -632.253 -632.253] (0.334)
Step: 10999, Reward: [-1538.788 -1538.788 -1538.788] [0.0000], Avg: [-636.374 -636.374 -636.374] (0.332)
Step: 11049, Reward: [-1486.378 -1486.378 -1486.378] [0.0000], Avg: [-640.22 -640.22 -640.22] (0.330)
Step: 11099, Reward: [-1569.413 -1569.413 -1569.413] [0.0000], Avg: [-644.405 -644.405 -644.405] (0.329)
Step: 11149, Reward: [-1589.884 -1589.884 -1589.884] [0.0000], Avg: [-648.645 -648.645 -648.645] (0.327)
Step: 11199, Reward: [-1689.596 -1689.596 -1689.596] [0.0000], Avg: [-653.292 -653.292 -653.292] (0.325)
Step: 11249, Reward: [-1453.52 -1453.52 -1453.52] [0.0000], Avg: [-656.849 -656.849 -656.849] (0.324)
Step: 11299, Reward: [-1564.75 -1564.75 -1564.75] [0.0000], Avg: [-660.866 -660.866 -660.866] (0.322)
Step: 11349, Reward: [-1552.812 -1552.812 -1552.812] [0.0000], Avg: [-664.796 -664.796 -664.796] (0.321)
Step: 11399, Reward: [-1754.219 -1754.219 -1754.219] [0.0000], Avg: [-669.574 -669.574 -669.574] (0.319)
Step: 11449, Reward: [-1360.743 -1360.743 -1360.743] [0.0000], Avg: [-672.592 -672.592 -672.592] (0.317)
Step: 11499, Reward: [-1324.63 -1324.63 -1324.63] [0.0000], Avg: [-675.427 -675.427 -675.427] (0.316)
Step: 11549, Reward: [-1279.208 -1279.208 -1279.208] [0.0000], Avg: [-678.041 -678.041 -678.041] (0.314)
Step: 11599, Reward: [-1272.067 -1272.067 -1272.067] [0.0000], Avg: [-680.601 -680.601 -680.601] (0.313)
Step: 11649, Reward: [-1566.54 -1566.54 -1566.54] [0.0000], Avg: [-684.403 -684.403 -684.403] (0.311)
Step: 11699, Reward: [-1478.389 -1478.389 -1478.389] [0.0000], Avg: [-687.796 -687.796 -687.796] (0.309)
Step: 11749, Reward: [-1497.001 -1497.001 -1497.001] [0.0000], Avg: [-691.24 -691.24 -691.24] (0.308)
Step: 11799, Reward: [-1602.181 -1602.181 -1602.181] [0.0000], Avg: [-695.1 -695.1 -695.1] (0.306)
Step: 11849, Reward: [-1524.468 -1524.468 -1524.468] [0.0000], Avg: [-698.599 -698.599 -698.599] (0.305)
Step: 11899, Reward: [-1695.527 -1695.527 -1695.527] [0.0000], Avg: [-702.788 -702.788 -702.788] (0.303)
Step: 11949, Reward: [-1703.194 -1703.194 -1703.194] [0.0000], Avg: [-706.974 -706.974 -706.974] (0.302)
Step: 11999, Reward: [-1582.471 -1582.471 -1582.471] [0.0000], Avg: [-710.622 -710.622 -710.622] (0.300)
Step: 12049, Reward: [-1454.674 -1454.674 -1454.674] [0.0000], Avg: [-713.709 -713.709 -713.709] (0.299)
Step: 12099, Reward: [-1861.531 -1861.531 -1861.531] [0.0000], Avg: [-718.452 -718.452 -718.452] (0.297)
Step: 12149, Reward: [-1453.491 -1453.491 -1453.491] [0.0000], Avg: [-721.477 -721.477 -721.477] (0.296)
Step: 12199, Reward: [-1838.698 -1838.698 -1838.698] [0.0000], Avg: [-726.056 -726.056 -726.056] (0.294)
Step: 12249, Reward: [-1775.109 -1775.109 -1775.109] [0.0000], Avg: [-730.338 -730.338 -730.338] (0.293)
Step: 12299, Reward: [-1754.789 -1754.789 -1754.789] [0.0000], Avg: [-734.502 -734.502 -734.502] (0.291)
Step: 12349, Reward: [-1505.006 -1505.006 -1505.006] [0.0000], Avg: [-737.622 -737.622 -737.622] (0.290)
Step: 12399, Reward: [-1738.069 -1738.069 -1738.069] [0.0000], Avg: [-741.656 -741.656 -741.656] (0.288)
Step: 12449, Reward: [-1640.119 -1640.119 -1640.119] [0.0000], Avg: [-745.264 -745.264 -745.264] (0.287)
Step: 12499, Reward: [-1887.887 -1887.887 -1887.887] [0.0000], Avg: [-749.834 -749.834 -749.834] (0.286)
Step: 12549, Reward: [-1783.982 -1783.982 -1783.982] [0.0000], Avg: [-753.954 -753.954 -753.954] (0.284)
Step: 12599, Reward: [-1649.517 -1649.517 -1649.517] [0.0000], Avg: [-757.508 -757.508 -757.508] (0.283)
Step: 12649, Reward: [-1565.854 -1565.854 -1565.854] [0.0000], Avg: [-760.703 -760.703 -760.703] (0.281)
Step: 12699, Reward: [-1667.628 -1667.628 -1667.628] [0.0000], Avg: [-764.274 -764.274 -764.274] (0.280)
Step: 12749, Reward: [-1787.904 -1787.904 -1787.904] [0.0000], Avg: [-768.288 -768.288 -768.288] (0.279)
Step: 12799, Reward: [-2013.463 -2013.463 -2013.463] [0.0000], Avg: [-773.152 -773.152 -773.152] (0.277)
Step: 12849, Reward: [-1866.138 -1866.138 -1866.138] [0.0000], Avg: [-777.405 -777.405 -777.405] (0.276)
Step: 12899, Reward: [-1550.232 -1550.232 -1550.232] [0.0000], Avg: [-780.4 -780.4 -780.4] (0.274)
Step: 12949, Reward: [-1848.698 -1848.698 -1848.698] [0.0000], Avg: [-784.525 -784.525 -784.525] (0.273)
Step: 12999, Reward: [-1580.117 -1580.117 -1580.117] [0.0000], Avg: [-787.585 -787.585 -787.585] (0.272)
Step: 13049, Reward: [-1761.798 -1761.798 -1761.798] [0.0000], Avg: [-791.318 -791.318 -791.318] (0.270)
Step: 13099, Reward: [-1412.995 -1412.995 -1412.995] [0.0000], Avg: [-793.691 -793.691 -793.691] (0.269)
Step: 13149, Reward: [-1931.374 -1931.374 -1931.374] [0.0000], Avg: [-798.016 -798.016 -798.016] (0.268)
Step: 13199, Reward: [-2202.562 -2202.562 -2202.562] [0.0000], Avg: [-803.337 -803.337 -803.337] (0.266)
Step: 13249, Reward: [-1809.584 -1809.584 -1809.584] [0.0000], Avg: [-807.134 -807.134 -807.134] (0.265)
Step: 13299, Reward: [-1882.447 -1882.447 -1882.447] [0.0000], Avg: [-811.176 -811.176 -811.176] (0.264)
Step: 13349, Reward: [-1571.212 -1571.212 -1571.212] [0.0000], Avg: [-814.023 -814.023 -814.023] (0.262)
Step: 13399, Reward: [-1976.371 -1976.371 -1976.371] [0.0000], Avg: [-818.36 -818.36 -818.36] (0.261)
Step: 13449, Reward: [-1616.637 -1616.637 -1616.637] [0.0000], Avg: [-821.328 -821.328 -821.328] (0.260)
Step: 13499, Reward: [-1914.998 -1914.998 -1914.998] [0.0000], Avg: [-825.378 -825.378 -825.378] (0.258)
Step: 13549, Reward: [-1689.056 -1689.056 -1689.056] [0.0000], Avg: [-828.565 -828.565 -828.565] (0.257)
Step: 13599, Reward: [-1515.436 -1515.436 -1515.436] [0.0000], Avg: [-831.09 -831.09 -831.09] (0.256)
Step: 13649, Reward: [-1853.061 -1853.061 -1853.061] [0.0000], Avg: [-834.834 -834.834 -834.834] (0.255)
Step: 13699, Reward: [-1747.839 -1747.839 -1747.839] [0.0000], Avg: [-838.166 -838.166 -838.166] (0.253)
Step: 13749, Reward: [-2016.062 -2016.062 -2016.062] [0.0000], Avg: [-842.449 -842.449 -842.449] (0.252)
Step: 13799, Reward: [-2033.265 -2033.265 -2033.265] [0.0000], Avg: [-846.764 -846.764 -846.764] (0.251)
Step: 13849, Reward: [-1445.239 -1445.239 -1445.239] [0.0000], Avg: [-848.924 -848.924 -848.924] (0.249)
Step: 13899, Reward: [-1795.776 -1795.776 -1795.776] [0.0000], Avg: [-852.33 -852.33 -852.33] (0.248)
Step: 13949, Reward: [-1889.834 -1889.834 -1889.834] [0.0000], Avg: [-856.049 -856.049 -856.049] (0.247)
Step: 13999, Reward: [-1685.178 -1685.178 -1685.178] [0.0000], Avg: [-859.01 -859.01 -859.01] (0.246)
Step: 14049, Reward: [-1710.824 -1710.824 -1710.824] [0.0000], Avg: [-862.042 -862.042 -862.042] (0.245)
Step: 14099, Reward: [-2094.505 -2094.505 -2094.505] [0.0000], Avg: [-866.412 -866.412 -866.412] (0.243)
Step: 14149, Reward: [-1648.46 -1648.46 -1648.46] [0.0000], Avg: [-869.175 -869.175 -869.175] (0.242)
Step: 14199, Reward: [-1962.737 -1962.737 -1962.737] [0.0000], Avg: [-873.026 -873.026 -873.026] (0.241)
Step: 14249, Reward: [-1898.152 -1898.152 -1898.152] [0.0000], Avg: [-876.623 -876.623 -876.623] (0.240)
Step: 14299, Reward: [-1946.047 -1946.047 -1946.047] [0.0000], Avg: [-880.362 -880.362 -880.362] (0.238)
Step: 14349, Reward: [-1777.504 -1777.504 -1777.504] [0.0000], Avg: [-883.488 -883.488 -883.488] (0.237)
Step: 14399, Reward: [-1985.974 -1985.974 -1985.974] [0.0000], Avg: [-887.316 -887.316 -887.316] (0.236)
Step: 14449, Reward: [-1745.49 -1745.49 -1745.49] [0.0000], Avg: [-890.286 -890.286 -890.286] (0.235)
Step: 14499, Reward: [-1890.684 -1890.684 -1890.684] [0.0000], Avg: [-893.735 -893.735 -893.735] (0.234)
Step: 14549, Reward: [-1978.82 -1978.82 -1978.82] [0.0000], Avg: [-897.464 -897.464 -897.464] (0.233)
Step: 14599, Reward: [-1536.803 -1536.803 -1536.803] [0.0000], Avg: [-899.654 -899.654 -899.654] (0.231)
Step: 14649, Reward: [-1836.107 -1836.107 -1836.107] [0.0000], Avg: [-902.85 -902.85 -902.85] (0.230)
Step: 14699, Reward: [-1729.908 -1729.908 -1729.908] [0.0000], Avg: [-905.663 -905.663 -905.663] (0.229)
Step: 14749, Reward: [-1760.758 -1760.758 -1760.758] [0.0000], Avg: [-908.561 -908.561 -908.561] (0.228)
Step: 14799, Reward: [-2232.097 -2232.097 -2232.097] [0.0000], Avg: [-913.033 -913.033 -913.033] (0.227)
Step: 14849, Reward: [-1878.087 -1878.087 -1878.087] [0.0000], Avg: [-916.282 -916.282 -916.282] (0.226)
Step: 14899, Reward: [-2028.661 -2028.661 -2028.661] [0.0000], Avg: [-920.015 -920.015 -920.015] (0.225)
Step: 14949, Reward: [-2299.718 -2299.718 -2299.718] [0.0000], Avg: [-924.629 -924.629 -924.629] (0.223)
Step: 14999, Reward: [-1927.323 -1927.323 -1927.323] [0.0000], Avg: [-927.972 -927.972 -927.972] (0.222)
Step: 15049, Reward: [-2229.653 -2229.653 -2229.653] [0.0000], Avg: [-932.296 -932.296 -932.296] (0.221)
Step: 15099, Reward: [-2300.609 -2300.609 -2300.609] [0.0000], Avg: [-936.827 -936.827 -936.827] (0.220)
Step: 15149, Reward: [-2098.691 -2098.691 -2098.691] [0.0000], Avg: [-940.662 -940.662 -940.662] (0.219)
Step: 15199, Reward: [-1942.183 -1942.183 -1942.183] [0.0000], Avg: [-943.956 -943.956 -943.956] (0.218)
Step: 15249, Reward: [-1696.577 -1696.577 -1696.577] [0.0000], Avg: [-946.424 -946.424 -946.424] (0.217)
Step: 15299, Reward: [-2153.407 -2153.407 -2153.407] [0.0000], Avg: [-950.368 -950.368 -950.368] (0.216)
Step: 15349, Reward: [-1866.148 -1866.148 -1866.148] [0.0000], Avg: [-953.351 -953.351 -953.351] (0.215)
Step: 15399, Reward: [-2356.266 -2356.266 -2356.266] [0.0000], Avg: [-957.906 -957.906 -957.906] (0.214)
Step: 15449, Reward: [-1857.227 -1857.227 -1857.227] [0.0000], Avg: [-960.816 -960.816 -960.816] (0.212)
Step: 15499, Reward: [-1864.766 -1864.766 -1864.766] [0.0000], Avg: [-963.732 -963.732 -963.732] (0.211)
Step: 15549, Reward: [-1851.177 -1851.177 -1851.177] [0.0000], Avg: [-966.586 -966.586 -966.586] (0.210)
Step: 15599, Reward: [-2224.261 -2224.261 -2224.261] [0.0000], Avg: [-970.617 -970.617 -970.617] (0.209)
Step: 15649, Reward: [-2158.763 -2158.763 -2158.763] [0.0000], Avg: [-974.413 -974.413 -974.413] (0.208)
Step: 15699, Reward: [-2032.629 -2032.629 -2032.629] [0.0000], Avg: [-977.783 -977.783 -977.783] (0.207)
Step: 15749, Reward: [-1899.946 -1899.946 -1899.946] [0.0000], Avg: [-980.711 -980.711 -980.711] (0.206)
Step: 15799, Reward: [-1822.621 -1822.621 -1822.621] [0.0000], Avg: [-983.375 -983.375 -983.375] (0.205)
Step: 15849, Reward: [-2069.506 -2069.506 -2069.506] [0.0000], Avg: [-986.801 -986.801 -986.801] (0.204)
Step: 15899, Reward: [-1988.448 -1988.448 -1988.448] [0.0000], Avg: [-989.951 -989.951 -989.951] (0.203)
Step: 15949, Reward: [-1681.915 -1681.915 -1681.915] [0.0000], Avg: [-992.12 -992.12 -992.12] (0.202)
Step: 15999, Reward: [-1936.174 -1936.174 -1936.174] [0.0000], Avg: [-995.07 -995.07 -995.07] (0.201)
Step: 16049, Reward: [-1739.73 -1739.73 -1739.73] [0.0000], Avg: [-997.39 -997.39 -997.39] (0.200)
Step: 16099, Reward: [-1760.257 -1760.257 -1760.257] [0.0000], Avg: [-999.759 -999.759 -999.759] (0.199)
Step: 16149, Reward: [-2028.234 -2028.234 -2028.234] [0.0000], Avg: [-1002.943 -1002.943 -1002.943] (0.198)
Step: 16199, Reward: [-1620.113 -1620.113 -1620.113] [0.0000], Avg: [-1004.848 -1004.848 -1004.848] (0.197)
Step: 16249, Reward: [-2056.974 -2056.974 -2056.974] [0.0000], Avg: [-1008.085 -1008.085 -1008.085] (0.196)
Step: 16299, Reward: [-1876.997 -1876.997 -1876.997] [0.0000], Avg: [-1010.751 -1010.751 -1010.751] (0.195)
Step: 16349, Reward: [-1959.874 -1959.874 -1959.874] [0.0000], Avg: [-1013.653 -1013.653 -1013.653] (0.194)
Step: 16399, Reward: [-2083.903 -2083.903 -2083.903] [0.0000], Avg: [-1016.916 -1016.916 -1016.916] (0.193)
Step: 16449, Reward: [-1836.85 -1836.85 -1836.85] [0.0000], Avg: [-1019.409 -1019.409 -1019.409] (0.192)
Step: 16499, Reward: [-1853.984 -1853.984 -1853.984] [0.0000], Avg: [-1021.938 -1021.938 -1021.938] (0.191)
Step: 16549, Reward: [-2002.536 -2002.536 -2002.536] [0.0000], Avg: [-1024.9 -1024.9 -1024.9] (0.190)
Step: 16599, Reward: [-2144.845 -2144.845 -2144.845] [0.0000], Avg: [-1028.273 -1028.273 -1028.273] (0.189)
Step: 16649, Reward: [-1795.287 -1795.287 -1795.287] [0.0000], Avg: [-1030.577 -1030.577 -1030.577] (0.188)
Step: 16699, Reward: [-2116.115 -2116.115 -2116.115] [0.0000], Avg: [-1033.827 -1033.827 -1033.827] (0.187)
Step: 16749, Reward: [-1770.177 -1770.177 -1770.177] [0.0000], Avg: [-1036.025 -1036.025 -1036.025] (0.187)
Step: 16799, Reward: [-1716.227 -1716.227 -1716.227] [0.0000], Avg: [-1038.049 -1038.049 -1038.049] (0.186)
Step: 16849, Reward: [-1926.477 -1926.477 -1926.477] [0.0000], Avg: [-1040.686 -1040.686 -1040.686] (0.185)
Step: 16899, Reward: [-2263.131 -2263.131 -2263.131] [0.0000], Avg: [-1044.302 -1044.302 -1044.302] (0.184)
Step: 16949, Reward: [-2176.243 -2176.243 -2176.243] [0.0000], Avg: [-1047.641 -1047.641 -1047.641] (0.183)
Step: 16999, Reward: [-2217.771 -2217.771 -2217.771] [0.0000], Avg: [-1051.083 -1051.083 -1051.083] (0.182)
Step: 17049, Reward: [-1954.754 -1954.754 -1954.754] [0.0000], Avg: [-1053.733 -1053.733 -1053.733] (0.181)
Step: 17099, Reward: [-1472.47 -1472.47 -1472.47] [0.0000], Avg: [-1054.957 -1054.957 -1054.957] (0.180)
Step: 17149, Reward: [-1863.308 -1863.308 -1863.308] [0.0000], Avg: [-1057.314 -1057.314 -1057.314] (0.179)
Step: 17199, Reward: [-2101.477 -2101.477 -2101.477] [0.0000], Avg: [-1060.349 -1060.349 -1060.349] (0.178)
Step: 17249, Reward: [-1942.185 -1942.185 -1942.185] [0.0000], Avg: [-1062.905 -1062.905 -1062.905] (0.177)
Step: 17299, Reward: [-1715.823 -1715.823 -1715.823] [0.0000], Avg: [-1064.793 -1064.793 -1064.793] (0.177)
Step: 17349, Reward: [-1925.093 -1925.093 -1925.093] [0.0000], Avg: [-1067.272 -1067.272 -1067.272] (0.176)
Step: 17399, Reward: [-1837.11 -1837.11 -1837.11] [0.0000], Avg: [-1069.484 -1069.484 -1069.484] (0.175)
Step: 17449, Reward: [-2006.595 -2006.595 -2006.595] [0.0000], Avg: [-1072.169 -1072.169 -1072.169] (0.174)
Step: 17499, Reward: [-2215.242 -2215.242 -2215.242] [0.0000], Avg: [-1075.435 -1075.435 -1075.435] (0.173)
Step: 17549, Reward: [-1986.883 -1986.883 -1986.883] [0.0000], Avg: [-1078.032 -1078.032 -1078.032] (0.172)
Step: 17599, Reward: [-1624.833 -1624.833 -1624.833] [0.0000], Avg: [-1079.585 -1079.585 -1079.585] (0.171)
Step: 17649, Reward: [-1818.633 -1818.633 -1818.633] [0.0000], Avg: [-1081.679 -1081.679 -1081.679] (0.170)
Step: 17699, Reward: [-2163.63 -2163.63 -2163.63] [0.0000], Avg: [-1084.735 -1084.735 -1084.735] (0.170)
Step: 17749, Reward: [-2065.253 -2065.253 -2065.253] [0.0000], Avg: [-1087.497 -1087.497 -1087.497] (0.169)
Step: 17799, Reward: [-1759.031 -1759.031 -1759.031] [0.0000], Avg: [-1089.383 -1089.383 -1089.383] (0.168)
Step: 17849, Reward: [-2083.116 -2083.116 -2083.116] [0.0000], Avg: [-1092.167 -1092.167 -1092.167] (0.167)
Step: 17899, Reward: [-2106.068 -2106.068 -2106.068] [0.0000], Avg: [-1094.999 -1094.999 -1094.999] (0.166)
Step: 17949, Reward: [-1937.043 -1937.043 -1937.043] [0.0000], Avg: [-1097.345 -1097.345 -1097.345] (0.165)
Step: 17999, Reward: [-2123.973 -2123.973 -2123.973] [0.0000], Avg: [-1100.196 -1100.196 -1100.196] (0.165)
Step: 18049, Reward: [-2024.254 -2024.254 -2024.254] [0.0000], Avg: [-1102.756 -1102.756 -1102.756] (0.164)
Step: 18099, Reward: [-2077. -2077. -2077.] [0.0000], Avg: [-1105.447 -1105.447 -1105.447] (0.163)
Step: 18149, Reward: [-1918.005 -1918.005 -1918.005] [0.0000], Avg: [-1107.686 -1107.686 -1107.686] (0.162)
Step: 18199, Reward: [-2045.613 -2045.613 -2045.613] [0.0000], Avg: [-1110.263 -1110.263 -1110.263] (0.161)
Step: 18249, Reward: [-2061.028 -2061.028 -2061.028] [0.0000], Avg: [-1112.867 -1112.867 -1112.867] (0.160)
Step: 18299, Reward: [-1818.235 -1818.235 -1818.235] [0.0000], Avg: [-1114.795 -1114.795 -1114.795] (0.160)
Step: 18349, Reward: [-1886.04 -1886.04 -1886.04] [0.0000], Avg: [-1116.896 -1116.896 -1116.896] (0.159)
Step: 18399, Reward: [-2082.249 -2082.249 -2082.249] [0.0000], Avg: [-1119.519 -1119.519 -1119.519] (0.158)
Step: 18449, Reward: [-2201.664 -2201.664 -2201.664] [0.0000], Avg: [-1122.452 -1122.452 -1122.452] (0.157)
Step: 18499, Reward: [-1795.224 -1795.224 -1795.224] [0.0000], Avg: [-1124.27 -1124.27 -1124.27] (0.157)
Step: 18549, Reward: [-1760.128 -1760.128 -1760.128] [0.0000], Avg: [-1125.984 -1125.984 -1125.984] (0.156)
Step: 18599, Reward: [-1883.2 -1883.2 -1883.2] [0.0000], Avg: [-1128.02 -1128.02 -1128.02] (0.155)
Step: 18649, Reward: [-1847.806 -1847.806 -1847.806] [0.0000], Avg: [-1129.95 -1129.95 -1129.95] (0.154)
Step: 18699, Reward: [-1882.081 -1882.081 -1882.081] [0.0000], Avg: [-1131.961 -1131.961 -1131.961] (0.153)
Step: 18749, Reward: [-1808.389 -1808.389 -1808.389] [0.0000], Avg: [-1133.764 -1133.764 -1133.764] (0.153)
Step: 18799, Reward: [-2197.361 -2197.361 -2197.361] [0.0000], Avg: [-1136.593 -1136.593 -1136.593] (0.152)
Step: 18849, Reward: [-1870.746 -1870.746 -1870.746] [0.0000], Avg: [-1138.54 -1138.54 -1138.54] (0.151)
Step: 18899, Reward: [-1808.075 -1808.075 -1808.075] [0.0000], Avg: [-1140.312 -1140.312 -1140.312] (0.150)
Step: 18949, Reward: [-1850.836 -1850.836 -1850.836] [0.0000], Avg: [-1142.186 -1142.186 -1142.186] (0.150)
Step: 18999, Reward: [-2009.062 -2009.062 -2009.062] [0.0000], Avg: [-1144.468 -1144.468 -1144.468] (0.149)
Step: 19049, Reward: [-1896.152 -1896.152 -1896.152] [0.0000], Avg: [-1146.441 -1146.441 -1146.441] (0.148)
Step: 19099, Reward: [-2053.154 -2053.154 -2053.154] [0.0000], Avg: [-1148.814 -1148.814 -1148.814] (0.147)
Step: 19149, Reward: [-2108.339 -2108.339 -2108.339] [0.0000], Avg: [-1151.319 -1151.319 -1151.319] (0.147)
Step: 19199, Reward: [-2385.764 -2385.764 -2385.764] [0.0000], Avg: [-1154.534 -1154.534 -1154.534] (0.146)
Step: 19249, Reward: [-2038.132 -2038.132 -2038.132] [0.0000], Avg: [-1156.829 -1156.829 -1156.829] (0.145)
Step: 19299, Reward: [-1686.684 -1686.684 -1686.684] [0.0000], Avg: [-1158.202 -1158.202 -1158.202] (0.144)
Step: 19349, Reward: [-1847.028 -1847.028 -1847.028] [0.0000], Avg: [-1159.982 -1159.982 -1159.982] (0.144)
Step: 19399, Reward: [-2501.108 -2501.108 -2501.108] [0.0000], Avg: [-1163.438 -1163.438 -1163.438] (0.143)
Step: 19449, Reward: [-2172.335 -2172.335 -2172.335] [0.0000], Avg: [-1166.032 -1166.032 -1166.032] (0.142)
Step: 19499, Reward: [-1823.495 -1823.495 -1823.495] [0.0000], Avg: [-1167.718 -1167.718 -1167.718] (0.142)
Step: 19549, Reward: [-1608.6 -1608.6 -1608.6] [0.0000], Avg: [-1168.845 -1168.845 -1168.845] (0.141)
Step: 19599, Reward: [-1887.411 -1887.411 -1887.411] [0.0000], Avg: [-1170.678 -1170.678 -1170.678] (0.140)
Step: 19649, Reward: [-2066.779 -2066.779 -2066.779] [0.0000], Avg: [-1172.959 -1172.959 -1172.959] (0.139)
Step: 19699, Reward: [-2103.705 -2103.705 -2103.705] [0.0000], Avg: [-1175.321 -1175.321 -1175.321] (0.139)
Step: 19749, Reward: [-1977.529 -1977.529 -1977.529] [0.0000], Avg: [-1177.352 -1177.352 -1177.352] (0.138)
Step: 19799, Reward: [-1801.784 -1801.784 -1801.784] [0.0000], Avg: [-1178.929 -1178.929 -1178.929] (0.137)
Step: 19849, Reward: [-2162.79 -2162.79 -2162.79] [0.0000], Avg: [-1181.407 -1181.407 -1181.407] (0.137)
Step: 19899, Reward: [-1782.076 -1782.076 -1782.076] [0.0000], Avg: [-1182.916 -1182.916 -1182.916] (0.136)
Step: 19949, Reward: [-2048.67 -2048.67 -2048.67] [0.0000], Avg: [-1185.086 -1185.086 -1185.086] (0.135)
Step: 19999, Reward: [-1870.901 -1870.901 -1870.901] [0.0000], Avg: [-1186.8 -1186.8 -1186.8] (0.135)
Step: 20049, Reward: [-1843.072 -1843.072 -1843.072] [0.0000], Avg: [-1188.437 -1188.437 -1188.437] (0.134)
Step: 20099, Reward: [-2134.346 -2134.346 -2134.346] [0.0000], Avg: [-1190.79 -1190.79 -1190.79] (0.133)
Step: 20149, Reward: [-1796.715 -1796.715 -1796.715] [0.0000], Avg: [-1192.294 -1192.294 -1192.294] (0.133)
Step: 20199, Reward: [-2697.8 -2697.8 -2697.8] [0.0000], Avg: [-1196.02 -1196.02 -1196.02] (0.132)
Step: 20249, Reward: [-2058.933 -2058.933 -2058.933] [0.0000], Avg: [-1198.151 -1198.151 -1198.151] (0.131)
Step: 20299, Reward: [-2054.151 -2054.151 -2054.151] [0.0000], Avg: [-1200.259 -1200.259 -1200.259] (0.131)
Step: 20349, Reward: [-1715.549 -1715.549 -1715.549] [0.0000], Avg: [-1201.525 -1201.525 -1201.525] (0.130)
Step: 20399, Reward: [-2281.729 -2281.729 -2281.729] [0.0000], Avg: [-1204.173 -1204.173 -1204.173] (0.129)
Step: 20449, Reward: [-2052.048 -2052.048 -2052.048] [0.0000], Avg: [-1206.246 -1206.246 -1206.246] (0.129)
Step: 20499, Reward: [-2055.323 -2055.323 -2055.323] [0.0000], Avg: [-1208.317 -1208.317 -1208.317] (0.128)
Step: 20549, Reward: [-2207.102 -2207.102 -2207.102] [0.0000], Avg: [-1210.747 -1210.747 -1210.747] (0.127)
Step: 20599, Reward: [-2244.787 -2244.787 -2244.787] [0.0000], Avg: [-1213.257 -1213.257 -1213.257] (0.127)
Step: 20649, Reward: [-1757.474 -1757.474 -1757.474] [0.0000], Avg: [-1214.574 -1214.574 -1214.574] (0.126)
Step: 20699, Reward: [-2077.8 -2077.8 -2077.8] [0.0000], Avg: [-1216.659 -1216.659 -1216.659] (0.126)
Step: 20749, Reward: [-1912.563 -1912.563 -1912.563] [0.0000], Avg: [-1218.336 -1218.336 -1218.336] (0.125)
Step: 20799, Reward: [-1732.988 -1732.988 -1732.988] [0.0000], Avg: [-1219.573 -1219.573 -1219.573] (0.124)
Step: 20849, Reward: [-2133.24 -2133.24 -2133.24] [0.0000], Avg: [-1221.764 -1221.764 -1221.764] (0.124)
Step: 20899, Reward: [-2155.699 -2155.699 -2155.699] [0.0000], Avg: [-1223.999 -1223.999 -1223.999] (0.123)
Step: 20949, Reward: [-1733.456 -1733.456 -1733.456] [0.0000], Avg: [-1225.215 -1225.215 -1225.215] (0.122)
Step: 20999, Reward: [-2022.361 -2022.361 -2022.361] [0.0000], Avg: [-1227.113 -1227.113 -1227.113] (0.122)
Step: 21049, Reward: [-408.45 -408.45 -408.45] [0.0000], Avg: [-1225.168 -1225.168 -1225.168] (0.121)
Step: 21099, Reward: [-621.375 -621.375 -621.375] [0.0000], Avg: [-1223.737 -1223.737 -1223.737] (0.121)
Step: 21149, Reward: [-530.431 -530.431 -530.431] [0.0000], Avg: [-1222.098 -1222.098 -1222.098] (0.120)
Step: 21199, Reward: [-550.55 -550.55 -550.55] [0.0000], Avg: [-1220.514 -1220.514 -1220.514] (0.119)
Step: 21249, Reward: [-436.461 -436.461 -436.461] [0.0000], Avg: [-1218.67 -1218.67 -1218.67] (0.119)
Step: 21299, Reward: [-500.437 -500.437 -500.437] [0.0000], Avg: [-1216.984 -1216.984 -1216.984] (0.118)
Step: 21349, Reward: [-596.074 -596.074 -596.074] [0.0000], Avg: [-1215.529 -1215.529 -1215.529] (0.118)
Step: 21399, Reward: [-967.114 -967.114 -967.114] [0.0000], Avg: [-1214.949 -1214.949 -1214.949] (0.117)
Step: 21449, Reward: [-545.981 -545.981 -545.981] [0.0000], Avg: [-1213.39 -1213.39 -1213.39] (0.116)
Step: 21499, Reward: [-459.597 -459.597 -459.597] [0.0000], Avg: [-1211.637 -1211.637 -1211.637] (0.116)
Step: 21549, Reward: [-561.078 -561.078 -561.078] [0.0000], Avg: [-1210.127 -1210.127 -1210.127] (0.115)
Step: 21599, Reward: [-676.749 -676.749 -676.749] [0.0000], Avg: [-1208.893 -1208.893 -1208.893] (0.115)
Step: 21649, Reward: [-563.858 -563.858 -563.858] [0.0000], Avg: [-1207.403 -1207.403 -1207.403] (0.114)
Step: 21699, Reward: [-355.587 -355.587 -355.587] [0.0000], Avg: [-1205.44 -1205.44 -1205.44] (0.114)
Step: 21749, Reward: [-750.18 -750.18 -750.18] [0.0000], Avg: [-1204.394 -1204.394 -1204.394] (0.113)
Step: 21799, Reward: [-638.435 -638.435 -638.435] [0.0000], Avg: [-1203.096 -1203.096 -1203.096] (0.112)
Step: 21849, Reward: [-503.033 -503.033 -503.033] [0.0000], Avg: [-1201.494 -1201.494 -1201.494] (0.112)
Step: 21899, Reward: [-625.65 -625.65 -625.65] [0.0000], Avg: [-1200.179 -1200.179 -1200.179] (0.111)
Step: 21949, Reward: [-537.01 -537.01 -537.01] [0.0000], Avg: [-1198.668 -1198.668 -1198.668] (0.111)
Step: 21999, Reward: [-579.295 -579.295 -579.295] [0.0000], Avg: [-1197.261 -1197.261 -1197.261] (0.110)
Step: 22049, Reward: [-792.703 -792.703 -792.703] [0.0000], Avg: [-1196.343 -1196.343 -1196.343] (0.110)
Step: 22099, Reward: [-632.198 -632.198 -632.198] [0.0000], Avg: [-1195.067 -1195.067 -1195.067] (0.109)
Step: 22149, Reward: [-629.148 -629.148 -629.148] [0.0000], Avg: [-1193.789 -1193.789 -1193.789] (0.109)
Step: 22199, Reward: [-712.47 -712.47 -712.47] [0.0000], Avg: [-1192.705 -1192.705 -1192.705] (0.108)
Step: 22249, Reward: [-656.125 -656.125 -656.125] [0.0000], Avg: [-1191.499 -1191.499 -1191.499] (0.107)
Step: 22299, Reward: [-520.235 -520.235 -520.235] [0.0000], Avg: [-1189.994 -1189.994 -1189.994] (0.107)
Step: 22349, Reward: [-646.541 -646.541 -646.541] [0.0000], Avg: [-1188.779 -1188.779 -1188.779] (0.106)
Step: 22399, Reward: [-609.448 -609.448 -609.448] [0.0000], Avg: [-1187.485 -1187.485 -1187.485] (0.106)
Step: 22449, Reward: [-546.346 -546.346 -546.346] [0.0000], Avg: [-1186.058 -1186.058 -1186.058] (0.105)
Step: 22499, Reward: [-517.774 -517.774 -517.774] [0.0000], Avg: [-1184.572 -1184.572 -1184.572] (0.105)
Step: 22549, Reward: [-420.566 -420.566 -420.566] [0.0000], Avg: [-1182.878 -1182.878 -1182.878] (0.104)
Step: 22599, Reward: [-696.685 -696.685 -696.685] [0.0000], Avg: [-1181.803 -1181.803 -1181.803] (0.104)
Step: 22649, Reward: [-719.187 -719.187 -719.187] [0.0000], Avg: [-1180.782 -1180.782 -1180.782] (0.103)
Step: 22699, Reward: [-497.676 -497.676 -497.676] [0.0000], Avg: [-1179.277 -1179.277 -1179.277] (0.103)
Step: 22749, Reward: [-997.255 -997.255 -997.255] [0.0000], Avg: [-1178.877 -1178.877 -1178.877] (0.102)
Step: 22799, Reward: [-657.801 -657.801 -657.801] [0.0000], Avg: [-1177.734 -1177.734 -1177.734] (0.102)
Step: 22849, Reward: [-481.995 -481.995 -481.995] [0.0000], Avg: [-1176.212 -1176.212 -1176.212] (0.101)
Step: 22899, Reward: [-874.88 -874.88 -874.88] [0.0000], Avg: [-1175.554 -1175.554 -1175.554] (0.101)
Step: 22949, Reward: [-535.056 -535.056 -535.056] [0.0000], Avg: [-1174.158 -1174.158 -1174.158] (0.100)
Step: 22999, Reward: [-921.413 -921.413 -921.413] [0.0000], Avg: [-1173.609 -1173.609 -1173.609] (0.100)
Step: 23049, Reward: [-650.47 -650.47 -650.47] [0.0000], Avg: [-1172.474 -1172.474 -1172.474] (0.099)
Step: 23099, Reward: [-500.115 -500.115 -500.115] [0.0000], Avg: [-1171.019 -1171.019 -1171.019] (0.099)
Step: 23149, Reward: [-828.547 -828.547 -828.547] [0.0000], Avg: [-1170.279 -1170.279 -1170.279] (0.098)
Step: 23199, Reward: [-562.743 -562.743 -562.743] [0.0000], Avg: [-1168.97 -1168.97 -1168.97] (0.098)
Step: 23249, Reward: [-621.689 -621.689 -621.689] [0.0000], Avg: [-1167.793 -1167.793 -1167.793] (0.097)
Step: 23299, Reward: [-443.604 -443.604 -443.604] [0.0000], Avg: [-1166.239 -1166.239 -1166.239] (0.097)
Step: 23349, Reward: [-432.051 -432.051 -432.051] [0.0000], Avg: [-1164.667 -1164.667 -1164.667] (0.096)
Step: 23399, Reward: [-441.071 -441.071 -441.071] [0.0000], Avg: [-1163.121 -1163.121 -1163.121] (0.096)
Step: 23449, Reward: [-794.584 -794.584 -794.584] [0.0000], Avg: [-1162.335 -1162.335 -1162.335] (0.095)
Step: 23499, Reward: [-469.196 -469.196 -469.196] [0.0000], Avg: [-1160.86 -1160.86 -1160.86] (0.095)
Step: 23549, Reward: [-352.249 -352.249 -352.249] [0.0000], Avg: [-1159.143 -1159.143 -1159.143] (0.094)
Step: 23599, Reward: [-516.862 -516.862 -516.862] [0.0000], Avg: [-1157.782 -1157.782 -1157.782] (0.094)
Step: 23649, Reward: [-646.184 -646.184 -646.184] [0.0000], Avg: [-1156.701 -1156.701 -1156.701] (0.093)
Step: 23699, Reward: [-603.483 -603.483 -603.483] [0.0000], Avg: [-1155.534 -1155.534 -1155.534] (0.093)
Step: 23749, Reward: [-431.767 -431.767 -431.767] [0.0000], Avg: [-1154.01 -1154.01 -1154.01] (0.092)
Step: 23799, Reward: [-819.334 -819.334 -819.334] [0.0000], Avg: [-1153.307 -1153.307 -1153.307] (0.092)
Step: 23849, Reward: [-505.876 -505.876 -505.876] [0.0000], Avg: [-1151.95 -1151.95 -1151.95] (0.092)
Step: 23899, Reward: [-668.38 -668.38 -668.38] [0.0000], Avg: [-1150.938 -1150.938 -1150.938] (0.091)
Step: 23949, Reward: [-1003.502 -1003.502 -1003.502] [0.0000], Avg: [-1150.63 -1150.63 -1150.63] (0.091)
Step: 23999, Reward: [-482.898 -482.898 -482.898] [0.0000], Avg: [-1149.239 -1149.239 -1149.239] (0.090)
Step: 24049, Reward: [-473.78 -473.78 -473.78] [0.0000], Avg: [-1147.835 -1147.835 -1147.835] (0.090)
Step: 24099, Reward: [-456.19 -456.19 -456.19] [0.0000], Avg: [-1146.4 -1146.4 -1146.4] (0.089)
Step: 24149, Reward: [-613.69 -613.69 -613.69] [0.0000], Avg: [-1145.297 -1145.297 -1145.297] (0.089)
Step: 24199, Reward: [-450.61 -450.61 -450.61] [0.0000], Avg: [-1143.862 -1143.862 -1143.862] (0.088)
Step: 24249, Reward: [-714.386 -714.386 -714.386] [0.0000], Avg: [-1142.976 -1142.976 -1142.976] (0.088)
Step: 24299, Reward: [-556.118 -556.118 -556.118] [0.0000], Avg: [-1141.769 -1141.769 -1141.769] (0.088)
Step: 24349, Reward: [-543.236 -543.236 -543.236] [0.0000], Avg: [-1140.54 -1140.54 -1140.54] (0.087)
Step: 24399, Reward: [-714.005 -714.005 -714.005] [0.0000], Avg: [-1139.665 -1139.665 -1139.665] (0.087)
Step: 24449, Reward: [-699.526 -699.526 -699.526] [0.0000], Avg: [-1138.765 -1138.765 -1138.765] (0.086)
Step: 24499, Reward: [-329.014 -329.014 -329.014] [0.0000], Avg: [-1137.113 -1137.113 -1137.113] (0.086)
Step: 24549, Reward: [-605.919 -605.919 -605.919] [0.0000], Avg: [-1136.031 -1136.031 -1136.031] (0.085)
Step: 24599, Reward: [-460.728 -460.728 -460.728] [0.0000], Avg: [-1134.658 -1134.658 -1134.658] (0.085)
Step: 24649, Reward: [-424.667 -424.667 -424.667] [0.0000], Avg: [-1133.218 -1133.218 -1133.218] (0.084)
Step: 24699, Reward: [-561.673 -561.673 -561.673] [0.0000], Avg: [-1132.061 -1132.061 -1132.061] (0.084)
Step: 24749, Reward: [-679.697 -679.697 -679.697] [0.0000], Avg: [-1131.147 -1131.147 -1131.147] (0.084)
Step: 24799, Reward: [-525.057 -525.057 -525.057] [0.0000], Avg: [-1129.925 -1129.925 -1129.925] (0.083)
Step: 24849, Reward: [-654.438 -654.438 -654.438] [0.0000], Avg: [-1128.969 -1128.969 -1128.969] (0.083)
Step: 24899, Reward: [-553.493 -553.493 -553.493] [0.0000], Avg: [-1127.813 -1127.813 -1127.813] (0.082)
Step: 24949, Reward: [-627.577 -627.577 -627.577] [0.0000], Avg: [-1126.811 -1126.811 -1126.811] (0.082)
Step: 24999, Reward: [-384.616 -384.616 -384.616] [0.0000], Avg: [-1125.326 -1125.326 -1125.326] (0.082)
Step: 25049, Reward: [-396.295 -396.295 -396.295] [0.0000], Avg: [-1123.871 -1123.871 -1123.871] (0.081)
Step: 25099, Reward: [-644.073 -644.073 -644.073] [0.0000], Avg: [-1122.915 -1122.915 -1122.915] (0.081)
Step: 25149, Reward: [-586.571 -586.571 -586.571] [0.0000], Avg: [-1121.849 -1121.849 -1121.849] (0.080)
Step: 25199, Reward: [-671.382 -671.382 -671.382] [0.0000], Avg: [-1120.955 -1120.955 -1120.955] (0.080)
Step: 25249, Reward: [-413.037 -413.037 -413.037] [0.0000], Avg: [-1119.553 -1119.553 -1119.553] (0.080)
Step: 25299, Reward: [-634.676 -634.676 -634.676] [0.0000], Avg: [-1118.595 -1118.595 -1118.595] (0.079)
Step: 25349, Reward: [-682.802 -682.802 -682.802] [0.0000], Avg: [-1117.736 -1117.736 -1117.736] (0.079)
Step: 25399, Reward: [-694.568 -694.568 -694.568] [0.0000], Avg: [-1116.903 -1116.903 -1116.903] (0.078)
Step: 25449, Reward: [-616.568 -616.568 -616.568] [0.0000], Avg: [-1115.92 -1115.92 -1115.92] (0.078)
Step: 25499, Reward: [-435.053 -435.053 -435.053] [0.0000], Avg: [-1114.585 -1114.585 -1114.585] (0.078)
Step: 25549, Reward: [-578.415 -578.415 -578.415] [0.0000], Avg: [-1113.535 -1113.535 -1113.535] (0.077)
Step: 25599, Reward: [-304.698 -304.698 -304.698] [0.0000], Avg: [-1111.956 -1111.956 -1111.956] (0.077)
Step: 25649, Reward: [-478.318 -478.318 -478.318] [0.0000], Avg: [-1110.72 -1110.72 -1110.72] (0.076)
Step: 25699, Reward: [-510.938 -510.938 -510.938] [0.0000], Avg: [-1109.554 -1109.554 -1109.554] (0.076)
