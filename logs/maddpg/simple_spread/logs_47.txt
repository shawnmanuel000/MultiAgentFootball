Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		action = gumbel_softmax(action_mu, hard=True)
		action = action.view(*out_dims, -1)
		return action
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		# agent_init_params = []
		# for acsp, obsp in zip(action_size, state_size):
		# 	num_in_pol = obsp[-1]
		# 	num_out_pol = acsp[-1]
		# 	num_in_critic = 0
		# 	for oobsp in state_size:
		# 		num_in_critic += oobsp[-1]
		# 	for oacsp in action_size:
		# 		num_in_critic += oacsp[-1]
		# 	agent_init_params.append({'num_in_pol': num_in_pol, 'num_out_pol': num_out_pol, 'num_in_critic': num_in_critic})
		# self.agent = MADDPG(agent_init_params, ["MADDPG"] * len(state_size))
		# self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, self.agent.nagents, [obsp[-1] for obsp in state_size], [acsp[-1] for acsp in action_size])

	def get_action(self, state, eps=None, sample=True, numpy=True):
		# state = [torch.autograd.Variable(torch.Tensor(np.vstack(state[i])), requires_grad=False) for i in range(self.agent.nagents)]
		# torch_agent_actions = self.agent.step(state, explore=True)
		# agent_actions = [ac.data.numpy() for ac in torch_agent_actions]
		# return agent_actions
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		# if not hasattr(self, "t"): self.t = 0
		# self.replay_buffer.push(state, action, next_state, reward, done)
		# if (len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.t % 100)==0):
		# 	self.agent.prep_training(device='cpu')
		# 	for a_i in range(self.agent.nagents):
		# 		sample = self.replay_buffer.sample(8, to_gpu=False)
		# 		self.agent.update(sample, a_i)
		# 	self.agent.update_all_targets()
		# 	self.agent.prep_rollouts(device='cpu')
		# self.t += 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
			to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
			states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

MSELoss = torch.nn.MSELoss()


class MADDPG():
	"""
	Wrapper class for DDPG-esque (i.e. also MADDPG) agents in multi-agent task
	"""
	def __init__(self, agent_init_params, alg_types, gamma=0.95, tau=0.01, lr=0.01, hidden_dim=64, discrete_action=True):
		self.nagents = len(alg_types)
		self.alg_types = alg_types
		self.agents = [DDPGAgent(lr=lr, discrete_action=discrete_action, hidden_dim=hidden_dim, **params) for params in agent_init_params]
		self.agent_init_params = agent_init_params
		self.gamma = gamma
		self.tau = tau
		self.lr = lr
		self.discrete_action = discrete_action
		self.pol_dev = 'cpu'  # device for policies
		self.critic_dev = 'cpu'  # device for critics
		self.trgt_pol_dev = 'cpu'  # device for target policies
		self.trgt_critic_dev = 'cpu'  # device for target critics
		self.niter = 0

	@property
	def policies(self):
		return [a.policy for a in self.agents]

	@property
	def target_policies(self):
		return [a.target_policy for a in self.agents]

	def scale_noise(self, scale):
		for a in self.agents:
			a.scale_noise(scale)

	def reset_noise(self):
		for a in self.agents:
			a.reset_noise()

	def step(self, observations, explore=False):
		return [a.step(obs, explore=explore) for a, obs in zip(self.agents, observations)]

	def update(self, sample, agent_i, parallel=False, logger=None):
		obs, acs, rews, next_obs, dones = sample
		curr_agent = self.agents[agent_i]

		curr_agent.critic_optimizer.zero_grad()
		if self.alg_types[agent_i] == 'MADDPG':
			if self.discrete_action: # one-hot encode action
				all_trgt_acs = [one_hot(pi(nobs)) for pi, nobs in zip(self.target_policies, next_obs)]
			else:
				all_trgt_acs = [pi(nobs) for pi, nobs in zip(self.target_policies, next_obs)]
			trgt_vf_in = torch.cat((*next_obs, *all_trgt_acs), dim=1)
		else:  # DDPG
			if self.discrete_action:
				trgt_vf_in = torch.cat((next_obs[agent_i], one_hot(curr_agent.target_policy(next_obs[agent_i]))), dim=1)
			else:
				trgt_vf_in = torch.cat((next_obs[agent_i], curr_agent.target_policy(next_obs[agent_i])), dim=1)
		target_value = (rews[agent_i].view(-1, 1) + self.gamma * curr_agent.target_critic(trgt_vf_in) * (1 - dones[agent_i].view(-1, 1)))

		if self.alg_types[agent_i] == 'MADDPG':
			vf_in = torch.cat((*obs, *acs), dim=1)
		else:  # DDPG
			vf_in = torch.cat((obs[agent_i], acs[agent_i]), dim=1)
		actual_value = curr_agent.critic(vf_in)
		vf_loss = MSELoss(actual_value, target_value.detach())
		vf_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.critic.parameters(), 0.5)
		curr_agent.critic_optimizer.step()
		curr_agent.policy_optimizer.zero_grad()

		if self.discrete_action:
			curr_pol_out = curr_agent.policy(obs[agent_i])
			curr_pol_vf_in = gumbel_softmax(curr_pol_out, hard=True)
		else:
			curr_pol_out = curr_agent.policy(obs[agent_i])
			curr_pol_vf_in = curr_pol_out
		if self.alg_types[agent_i] == 'MADDPG':
			all_pol_acs = []
			for i, pi, ob in zip(range(self.nagents), self.policies, obs):
				if i == agent_i:
					all_pol_acs.append(curr_pol_vf_in)
				elif self.discrete_action:
					all_pol_acs.append(one_hot(pi(ob)))
				else:
					all_pol_acs.append(pi(ob))
			vf_in = torch.cat((*obs, *all_pol_acs), dim=1)
		else:  # DDPG
			vf_in = torch.cat((obs[agent_i], curr_pol_vf_in), dim=1)
		pol_loss = -curr_agent.critic(vf_in).mean()
		pol_loss += (curr_pol_out**2).mean() * 1e-3
		pol_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.policy.parameters(), 0.5)
		curr_agent.policy_optimizer.step()
		if logger is not None:
			logger.add_scalars('agent%i/losses' % agent_i, {'vf_loss': vf_loss, 'pol_loss': pol_loss}, self.niter)

	def update_all_targets(self):
		for a in self.agents:
			# soft_update(a.target_critic, a.critic, self.tau)
			# soft_update(a.target_policy, a.policy, self.tau)
			for target_param, param in zip(a.target_critic.parameters(), a.critic.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
			for target_param, param in zip(a.target_policy.parameters(), a.policy.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
		self.niter += 1

	def prep_training(self, device='gpu'):
		for a in self.agents:
			a.policy.train()
			a.critic.train()
			a.target_policy.train()
			a.target_critic.train()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device
		if not self.critic_dev == device:
			for a in self.agents:
				a.critic = fn(a.critic)
			self.critic_dev = device
		if not self.trgt_pol_dev == device:
			for a in self.agents:
				a.target_policy = fn(a.target_policy)
			self.trgt_pol_dev = device
		if not self.trgt_critic_dev == device:
			for a in self.agents:
				a.target_critic = fn(a.target_critic)
			self.trgt_critic_dev = device

	def prep_rollouts(self, device='cpu'):
		for a in self.agents:
			a.policy.eval()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		# only need main policy for rollouts
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device

	def save(self, filename):
		self.prep_training(device='cpu')  # move parameters to CPU before saving
		save_dict = {'init_dict': self.init_dict, 'agent_params': [a.get_params() for a in self.agents]}
		torch.save(save_dict, filename)

	@classmethod
	def init_from_env(cls, env, agent_alg="MADDPG", adversary_alg="MADDPG", gamma=0.95, tau=0.01, lr=0.01, hidden_dim=64):
		agent_init_params = []
		alg_types = [adversary_alg if atype == 'adversary' else agent_alg for atype in env.agent_types]
		for acsp, obsp, algtype in zip(env.action_space, env.observation_space, alg_types):
			num_in_pol = obsp.shape[0]
			if isinstance(acsp, Box):
				discrete_action = False
				get_shape = lambda x: x.shape[0]
			else:  # Discrete
				discrete_action = True
				get_shape = lambda x: x.n
			num_out_pol = get_shape(acsp)
			if algtype == "MADDPG":
				num_in_critic = 0
				for oobsp in env.observation_space:
					num_in_critic += oobsp.shape[0]
				for oacsp in env.action_space:
					num_in_critic += get_shape(oacsp)
			else:
				num_in_critic = obsp.shape[0] + get_shape(acsp)
			agent_init_params.append({'num_in_pol': num_in_pol, 'num_out_pol': num_out_pol, 'num_in_critic': num_in_critic})
		init_dict = {'gamma': gamma, 'tau': tau, 'lr': lr, 'hidden_dim': hidden_dim, 'alg_types': alg_types, 'agent_init_params': agent_init_params, 'discrete_action': discrete_action}
		instance = cls(**init_dict)
		instance.init_dict = init_dict
		return instance

	@classmethod
	def init_from_save(cls, filename):
		"""
		Instantiate instance of this class from file created by 'save' method
		"""
		save_dict = torch.load(filename)
		instance = cls(**save_dict['init_dict'])
		instance.init_dict = save_dict['init_dict']
		for a, params in zip(instance.agents, save_dict['agent_params']):
			a.load_params(params)
		return instance

class DDPGAgent(object):
	def __init__(self, num_in_pol, num_out_pol, num_in_critic, hidden_dim=64, lr=0.01, discrete_action=True):
		self.policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim,constrain_out=True,discrete_action=discrete_action)
		self.critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim,constrain_out=False)
		self.target_policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim,constrain_out=True,discrete_action=discrete_action)
		self.target_critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim,constrain_out=False)
		# hard_update(self.target_policy, self.policy)
		# hard_update(self.target_critic, self.critic)
		for target_param, param in zip(self.target_policy.parameters(), self.policy.parameters()):
			target_param.data.copy_(param.data)
		for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
			target_param.data.copy_(param.data)
		self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
		self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)
		self.exploration = 0.3  # epsilon for eps-greedy
		self.discrete_action = discrete_action

	def reset_noise(self):
		if not self.discrete_action:
			self.exploration.reset()

	def scale_noise(self, scale):
		if self.discrete_action:
			self.exploration = scale
		else:
			self.exploration.scale = scale

	def step(self, obs, explore=False):
		action = self.policy(obs)
		if explore:
			action = gumbel_softmax(action, hard=True)
		else:
			action = one_hot(action)
		return action

class MLPNetwork(torch.nn.Module):
	def __init__(self, input_dim, out_dim, hidden_dim=64, nonlin=torch.relu, constrain_out=False, norm_in=False, discrete_action=True):
		super(MLPNetwork, self).__init__()

		if norm_in:  # normalize inputs
			self.in_fn = nn.BatchNorm1d(input_dim)
			self.in_fn.weight.data.fill_(1)
			self.in_fn.bias.data.fill_(0)
		else:
			self.in_fn = lambda x: x
		self.fc1 = torch.nn.Linear(input_dim, hidden_dim)
		self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
		self.fc3 = torch.nn.Linear(hidden_dim, out_dim)
		self.nonlin = nonlin
		if constrain_out and not discrete_action:
			# initialize small to prevent saturation
			self.fc3.weight.data.uniform_(-3e-3, 3e-3)
			self.out_fn = torch.tanh
		else:  # logits for discrete action (will softmax later)
			self.out_fn = lambda x: x

	def forward(self, X):
		h1 = self.nonlin(self.fc1(self.in_fn(X)))
		h2 = self.nonlin(self.fc2(h1))
		out = self.out_fn(self.fc3(h2))
		return out
REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-373.309 -373.309 -373.309] [0.0000], Avg: [-373.309 -373.309 -373.309] (0.995)
Step: 99, Reward: [-369.485 -369.485 -369.485] [0.0000], Avg: [-371.397 -371.397 -371.397] (0.990)
Step: 149, Reward: [-681.664 -681.664 -681.664] [0.0000], Avg: [-474.819 -474.819 -474.819] (0.985)
Step: 199, Reward: [-345.203 -345.203 -345.203] [0.0000], Avg: [-442.415 -442.415 -442.415] (0.980)
Step: 249, Reward: [-477.095 -477.095 -477.095] [0.0000], Avg: [-449.351 -449.351 -449.351] (0.975)
Step: 299, Reward: [-398.238 -398.238 -398.238] [0.0000], Avg: [-440.832 -440.832 -440.832] (0.970)
Step: 349, Reward: [-522.671 -522.671 -522.671] [0.0000], Avg: [-452.523 -452.523 -452.523] (0.966)
Step: 399, Reward: [-719.33 -719.33 -719.33] [0.0000], Avg: [-485.874 -485.874 -485.874] (0.961)
Step: 449, Reward: [-520.887 -520.887 -520.887] [0.0000], Avg: [-489.765 -489.765 -489.765] (0.956)
Step: 499, Reward: [-691.039 -691.039 -691.039] [0.0000], Avg: [-509.892 -509.892 -509.892] (0.951)
Step: 549, Reward: [-492.872 -492.872 -492.872] [0.0000], Avg: [-508.345 -508.345 -508.345] (0.946)
Step: 599, Reward: [-485.226 -485.226 -485.226] [0.0000], Avg: [-506.418 -506.418 -506.418] (0.942)
Step: 649, Reward: [-525.293 -525.293 -525.293] [0.0000], Avg: [-507.87 -507.87 -507.87] (0.937)
Step: 699, Reward: [-528.48 -528.48 -528.48] [0.0000], Avg: [-509.342 -509.342 -509.342] (0.932)
Step: 749, Reward: [-638.616 -638.616 -638.616] [0.0000], Avg: [-517.96 -517.96 -517.96] (0.928)
Step: 799, Reward: [-492.453 -492.453 -492.453] [0.0000], Avg: [-516.366 -516.366 -516.366] (0.923)
Step: 849, Reward: [-405.284 -405.284 -405.284] [0.0000], Avg: [-509.832 -509.832 -509.832] (0.918)
Step: 899, Reward: [-567.835 -567.835 -567.835] [0.0000], Avg: [-513.054 -513.054 -513.054] (0.914)
Step: 949, Reward: [-437. -437. -437.] [0.0000], Avg: [-509.051 -509.051 -509.051] (0.909)
Step: 999, Reward: [-508.225 -508.225 -508.225] [0.0000], Avg: [-509.01 -509.01 -509.01] (0.905)
Step: 1049, Reward: [-522.19 -522.19 -522.19] [0.0000], Avg: [-509.638 -509.638 -509.638] (0.900)
Step: 1099, Reward: [-428.68 -428.68 -428.68] [0.0000], Avg: [-505.958 -505.958 -505.958] (0.896)
Step: 1149, Reward: [-427.878 -427.878 -427.878] [0.0000], Avg: [-502.563 -502.563 -502.563] (0.891)
Step: 1199, Reward: [-342.651 -342.651 -342.651] [0.0000], Avg: [-495.9 -495.9 -495.9] (0.887)
Step: 1249, Reward: [-427.329 -427.329 -427.329] [0.0000], Avg: [-493.157 -493.157 -493.157] (0.882)
Step: 1299, Reward: [-399.765 -399.765 -399.765] [0.0000], Avg: [-489.565 -489.565 -489.565] (0.878)
Step: 1349, Reward: [-427.399 -427.399 -427.399] [0.0000], Avg: [-487.263 -487.263 -487.263] (0.873)
Step: 1399, Reward: [-551.634 -551.634 -551.634] [0.0000], Avg: [-489.562 -489.562 -489.562] (0.869)
Step: 1449, Reward: [-409.815 -409.815 -409.815] [0.0000], Avg: [-486.812 -486.812 -486.812] (0.865)
Step: 1499, Reward: [-548.599 -548.599 -548.599] [0.0000], Avg: [-488.871 -488.871 -488.871] (0.860)
Step: 1549, Reward: [-541.985 -541.985 -541.985] [0.0000], Avg: [-490.585 -490.585 -490.585] (0.856)
Step: 1599, Reward: [-519.282 -519.282 -519.282] [0.0000], Avg: [-491.482 -491.482 -491.482] (0.852)
Step: 1649, Reward: [-546.918 -546.918 -546.918] [0.0000], Avg: [-493.161 -493.161 -493.161] (0.848)
Step: 1699, Reward: [-613.731 -613.731 -613.731] [0.0000], Avg: [-496.708 -496.708 -496.708] (0.843)
Step: 1749, Reward: [-644.784 -644.784 -644.784] [0.0000], Avg: [-500.938 -500.938 -500.938] (0.839)
Step: 1799, Reward: [-401.364 -401.364 -401.364] [0.0000], Avg: [-498.172 -498.172 -498.172] (0.835)
Step: 1849, Reward: [-487.872 -487.872 -487.872] [0.0000], Avg: [-497.894 -497.894 -497.894] (0.831)
Step: 1899, Reward: [-476.259 -476.259 -476.259] [0.0000], Avg: [-497.325 -497.325 -497.325] (0.827)
Step: 1949, Reward: [-457.824 -457.824 -457.824] [0.0000], Avg: [-496.312 -496.312 -496.312] (0.822)
Step: 1999, Reward: [-439.287 -439.287 -439.287] [0.0000], Avg: [-494.886 -494.886 -494.886] (0.818)
Step: 2049, Reward: [-444.136 -444.136 -444.136] [0.0000], Avg: [-493.648 -493.648 -493.648] (0.814)
Step: 2099, Reward: [-484.547 -484.547 -484.547] [0.0000], Avg: [-493.432 -493.432 -493.432] (0.810)
Step: 2149, Reward: [-372.723 -372.723 -372.723] [0.0000], Avg: [-490.625 -490.625 -490.625] (0.806)
Step: 2199, Reward: [-308.037 -308.037 -308.037] [0.0000], Avg: [-486.475 -486.475 -486.475] (0.802)
Step: 2249, Reward: [-510.665 -510.665 -510.665] [0.0000], Avg: [-487.012 -487.012 -487.012] (0.798)
Step: 2299, Reward: [-618.516 -618.516 -618.516] [0.0000], Avg: [-489.871 -489.871 -489.871] (0.794)
Step: 2349, Reward: [-448.406 -448.406 -448.406] [0.0000], Avg: [-488.989 -488.989 -488.989] (0.790)
Step: 2399, Reward: [-484.502 -484.502 -484.502] [0.0000], Avg: [-488.895 -488.895 -488.895] (0.786)
Step: 2449, Reward: [-430.951 -430.951 -430.951] [0.0000], Avg: [-487.713 -487.713 -487.713] (0.782)
Step: 2499, Reward: [-591.626 -591.626 -591.626] [0.0000], Avg: [-489.791 -489.791 -489.791] (0.778)
Step: 2549, Reward: [-444.447 -444.447 -444.447] [0.0000], Avg: [-488.902 -488.902 -488.902] (0.774)
Step: 2599, Reward: [-543.224 -543.224 -543.224] [0.0000], Avg: [-489.947 -489.947 -489.947] (0.771)
Step: 2649, Reward: [-328.017 -328.017 -328.017] [0.0000], Avg: [-486.891 -486.891 -486.891] (0.767)
Step: 2699, Reward: [-541.787 -541.787 -541.787] [0.0000], Avg: [-487.908 -487.908 -487.908] (0.763)
Step: 2749, Reward: [-392.076 -392.076 -392.076] [0.0000], Avg: [-486.166 -486.166 -486.166] (0.759)
Step: 2799, Reward: [-497.151 -497.151 -497.151] [0.0000], Avg: [-486.362 -486.362 -486.362] (0.755)
Step: 2849, Reward: [-540.371 -540.371 -540.371] [0.0000], Avg: [-487.309 -487.309 -487.309] (0.751)
Step: 2899, Reward: [-671.959 -671.959 -671.959] [0.0000], Avg: [-490.493 -490.493 -490.493] (0.748)
Step: 2949, Reward: [-611.793 -611.793 -611.793] [0.0000], Avg: [-492.549 -492.549 -492.549] (0.744)
Step: 2999, Reward: [-576.645 -576.645 -576.645] [0.0000], Avg: [-493.95 -493.95 -493.95] (0.740)
Step: 3049, Reward: [-404.016 -404.016 -404.016] [0.0000], Avg: [-492.476 -492.476 -492.476] (0.737)
Step: 3099, Reward: [-394.64 -394.64 -394.64] [0.0000], Avg: [-490.898 -490.898 -490.898] (0.733)
Step: 3149, Reward: [-340.967 -340.967 -340.967] [0.0000], Avg: [-488.518 -488.518 -488.518] (0.729)
Step: 3199, Reward: [-368.639 -368.639 -368.639] [0.0000], Avg: [-486.645 -486.645 -486.645] (0.726)
Step: 3249, Reward: [-725.471 -725.471 -725.471] [0.0000], Avg: [-490.319 -490.319 -490.319] (0.722)
Step: 3299, Reward: [-395.958 -395.958 -395.958] [0.0000], Avg: [-488.89 -488.89 -488.89] (0.718)
Step: 3349, Reward: [-416.766 -416.766 -416.766] [0.0000], Avg: [-487.813 -487.813 -487.813] (0.715)
Step: 3399, Reward: [-537.353 -537.353 -537.353] [0.0000], Avg: [-488.542 -488.542 -488.542] (0.711)
Step: 3449, Reward: [-487.533 -487.533 -487.533] [0.0000], Avg: [-488.527 -488.527 -488.527] (0.708)
Step: 3499, Reward: [-446.023 -446.023 -446.023] [0.0000], Avg: [-487.92 -487.92 -487.92] (0.704)
Step: 3549, Reward: [-609.193 -609.193 -609.193] [0.0000], Avg: [-489.628 -489.628 -489.628] (0.701)
Step: 3599, Reward: [-485.765 -485.765 -485.765] [0.0000], Avg: [-489.574 -489.574 -489.574] (0.697)
Step: 3649, Reward: [-599.595 -599.595 -599.595] [0.0000], Avg: [-491.082 -491.082 -491.082] (0.694)
Step: 3699, Reward: [-495.759 -495.759 -495.759] [0.0000], Avg: [-491.145 -491.145 -491.145] (0.690)
Step: 3749, Reward: [-609.921 -609.921 -609.921] [0.0000], Avg: [-492.728 -492.728 -492.728] (0.687)
Step: 3799, Reward: [-351.662 -351.662 -351.662] [0.0000], Avg: [-490.872 -490.872 -490.872] (0.683)
Step: 3849, Reward: [-464.473 -464.473 -464.473] [0.0000], Avg: [-490.529 -490.529 -490.529] (0.680)
Step: 3899, Reward: [-528.427 -528.427 -528.427] [0.0000], Avg: [-491.015 -491.015 -491.015] (0.676)
Step: 3949, Reward: [-413.515 -413.515 -413.515] [0.0000], Avg: [-490.034 -490.034 -490.034] (0.673)
Step: 3999, Reward: [-648.685 -648.685 -648.685] [0.0000], Avg: [-492.017 -492.017 -492.017] (0.670)
Step: 4049, Reward: [-451.738 -451.738 -451.738] [0.0000], Avg: [-491.52 -491.52 -491.52] (0.666)
Step: 4099, Reward: [-497.697 -497.697 -497.697] [0.0000], Avg: [-491.595 -491.595 -491.595] (0.663)
Step: 4149, Reward: [-635.879 -635.879 -635.879] [0.0000], Avg: [-493.334 -493.334 -493.334] (0.660)
Step: 4199, Reward: [-456.02 -456.02 -456.02] [0.0000], Avg: [-492.89 -492.89 -492.89] (0.656)
Step: 55349, Reward: [-455.664 -455.664 -455.664] [0.0000], Avg: [-523.475 -523.475 -523.475] (1.000)
Step: 55399, Reward: [-628.508 -628.508 -628.508] [0.0000], Avg: [-523.57 -523.57 -523.57] (1.000)
Step: 4399, Reward: [-881.38 -881.38 -881.38] [0.0000], Avg: [-494.184 -494.184 -494.184] (0.643)
Step: 4449, Reward: [-975.801 -975.801 -975.801] [0.0000], Avg: [-499.595 -499.595 -499.595] (0.640)
Step: 4499, Reward: [-686.586 -686.586 -686.586] [0.0000], Avg: [-501.673 -501.673 -501.673] (0.637)
Step: 4549, Reward: [-643.447 -643.447 -643.447] [0.0000], Avg: [-503.231 -503.231 -503.231] (0.634)
Step: 4599, Reward: [-473.656 -473.656 -473.656] [0.0000], Avg: [-502.91 -502.91 -502.91] (0.631)
Step: 4649, Reward: [-431.363 -431.363 -431.363] [0.0000], Avg: [-502.14 -502.14 -502.14] (0.627)
Step: 4699, Reward: [-692.445 -692.445 -692.445] [0.0000], Avg: [-504.165 -504.165 -504.165] (0.624)
Step: 4749, Reward: [-427.256 -427.256 -427.256] [0.0000], Avg: [-503.355 -503.355 -503.355] (0.621)
Step: 4799, Reward: [-472.71 -472.71 -472.71] [0.0000], Avg: [-503.036 -503.036 -503.036] (0.618)
Step: 4849, Reward: [-464.507 -464.507 -464.507] [0.0000], Avg: [-502.639 -502.639 -502.639] (0.615)
Step: 4899, Reward: [-790.245 -790.245 -790.245] [0.0000], Avg: [-505.574 -505.574 -505.574] (0.612)
Step: 4949, Reward: [-577.145 -577.145 -577.145] [0.0000], Avg: [-506.296 -506.296 -506.296] (0.609)
Step: 4999, Reward: [-876.507 -876.507 -876.507] [0.0000], Avg: [-509.999 -509.999 -509.999] (0.606)
Step: 5049, Reward: [-601.729 -601.729 -601.729] [0.0000], Avg: [-510.907 -510.907 -510.907] (0.603)
Step: 5099, Reward: [-578.339 -578.339 -578.339] [0.0000], Avg: [-511.568 -511.568 -511.568] (0.600)
Step: 5149, Reward: [-438.032 -438.032 -438.032] [0.0000], Avg: [-510.854 -510.854 -510.854] (0.597)
Step: 5199, Reward: [-638.495 -638.495 -638.495] [0.0000], Avg: [-512.081 -512.081 -512.081] (0.594)
Step: 5249, Reward: [-904.764 -904.764 -904.764] [0.0000], Avg: [-515.821 -515.821 -515.821] (0.591)
Step: 5299, Reward: [-501.13 -501.13 -501.13] [0.0000], Avg: [-515.683 -515.683 -515.683] (0.588)
Step: 5349, Reward: [-656.056 -656.056 -656.056] [0.0000], Avg: [-516.994 -516.994 -516.994] (0.585)
Step: 5399, Reward: [-601.249 -601.249 -601.249] [0.0000], Avg: [-517.775 -517.775 -517.775] (0.582)
Step: 5449, Reward: [-485.828 -485.828 -485.828] [0.0000], Avg: [-517.481 -517.481 -517.481] (0.579)
Step: 5499, Reward: [-556.171 -556.171 -556.171] [0.0000], Avg: [-517.833 -517.833 -517.833] (0.576)
Step: 5549, Reward: [-1001.404 -1001.404 -1001.404] [0.0000], Avg: [-522.19 -522.19 -522.19] (0.573)
Step: 5599, Reward: [-477.069 -477.069 -477.069] [0.0000], Avg: [-521.787 -521.787 -521.787] (0.570)
Step: 5649, Reward: [-554.789 -554.789 -554.789] [0.0000], Avg: [-522.079 -522.079 -522.079] (0.568)
Step: 5699, Reward: [-526.314 -526.314 -526.314] [0.0000], Avg: [-522.116 -522.116 -522.116] (0.565)
Step: 5749, Reward: [-494.847 -494.847 -494.847] [0.0000], Avg: [-521.879 -521.879 -521.879] (0.562)
Step: 5799, Reward: [-506.655 -506.655 -506.655] [0.0000], Avg: [-521.748 -521.748 -521.748] (0.559)
Step: 5849, Reward: [-505.11 -505.11 -505.11] [0.0000], Avg: [-521.605 -521.605 -521.605] (0.556)
Step: 5899, Reward: [-529.304 -529.304 -529.304] [0.0000], Avg: [-521.671 -521.671 -521.671] (0.554)
Step: 5949, Reward: [-650.964 -650.964 -650.964] [0.0000], Avg: [-522.757 -522.757 -522.757] (0.551)
Step: 5999, Reward: [-472.524 -472.524 -472.524] [0.0000], Avg: [-522.339 -522.339 -522.339] (0.548)
Step: 6049, Reward: [-504.871 -504.871 -504.871] [0.0000], Avg: [-522.194 -522.194 -522.194] (0.545)
Step: 6099, Reward: [-470.951 -470.951 -470.951] [0.0000], Avg: [-521.774 -521.774 -521.774] (0.543)
Step: 6149, Reward: [-609.519 -609.519 -609.519] [0.0000], Avg: [-522.488 -522.488 -522.488] (0.540)
Step: 6199, Reward: [-537.153 -537.153 -537.153] [0.0000], Avg: [-522.606 -522.606 -522.606] (0.537)
Step: 6249, Reward: [-648.291 -648.291 -648.291] [0.0000], Avg: [-523.611 -523.611 -523.611] (0.534)
Step: 6299, Reward: [-417.984 -417.984 -417.984] [0.0000], Avg: [-522.773 -522.773 -522.773] (0.532)
Step: 6349, Reward: [-470.937 -470.937 -470.937] [0.0000], Avg: [-522.365 -522.365 -522.365] (0.529)
Step: 6399, Reward: [-791.91 -791.91 -791.91] [0.0000], Avg: [-524.471 -524.471 -524.471] (0.526)
Step: 6449, Reward: [-693.32 -693.32 -693.32] [0.0000], Avg: [-525.78 -525.78 -525.78] (0.524)
Step: 6499, Reward: [-414.573 -414.573 -414.573] [0.0000], Avg: [-524.924 -524.924 -524.924] (0.521)
Step: 6549, Reward: [-617.329 -617.329 -617.329] [0.0000], Avg: [-525.63 -525.63 -525.63] (0.519)
Step: 6599, Reward: [-760.769 -760.769 -760.769] [0.0000], Avg: [-527.411 -527.411 -527.411] (0.516)
Step: 6649, Reward: [-476.804 -476.804 -476.804] [0.0000], Avg: [-527.03 -527.03 -527.03] (0.513)
Step: 6699, Reward: [-453.729 -453.729 -453.729] [0.0000], Avg: [-526.483 -526.483 -526.483] (0.511)
Step: 6749, Reward: [-696.822 -696.822 -696.822] [0.0000], Avg: [-527.745 -527.745 -527.745] (0.508)
Step: 6799, Reward: [-505.434 -505.434 -505.434] [0.0000], Avg: [-527.581 -527.581 -527.581] (0.506)
Step: 6849, Reward: [-609.353 -609.353 -609.353] [0.0000], Avg: [-528.178 -528.178 -528.178] (0.503)
Step: 6899, Reward: [-772.873 -772.873 -772.873] [0.0000], Avg: [-529.951 -529.951 -529.951] (0.501)
Step: 6949, Reward: [-661.698 -661.698 -661.698] [0.0000], Avg: [-530.899 -530.899 -530.899] (0.498)
Step: 6999, Reward: [-599.038 -599.038 -599.038] [0.0000], Avg: [-531.386 -531.386 -531.386] (0.496)
Step: 7049, Reward: [-482.003 -482.003 -482.003] [0.0000], Avg: [-531.035 -531.035 -531.035] (0.493)
Step: 7099, Reward: [-602.348 -602.348 -602.348] [0.0000], Avg: [-531.538 -531.538 -531.538] (0.491)
Step: 7149, Reward: [-369.427 -369.427 -369.427] [0.0000], Avg: [-530.404 -530.404 -530.404] (0.488)
Step: 7199, Reward: [-577.897 -577.897 -577.897] [0.0000], Avg: [-530.734 -530.734 -530.734] (0.486)
Step: 7249, Reward: [-478.568 -478.568 -478.568] [0.0000], Avg: [-530.374 -530.374 -530.374] (0.483)
Step: 7299, Reward: [-599.096 -599.096 -599.096] [0.0000], Avg: [-530.845 -530.845 -530.845] (0.481)
Step: 7349, Reward: [-729.216 -729.216 -729.216] [0.0000], Avg: [-532.194 -532.194 -532.194] (0.479)
Step: 7399, Reward: [-472.289 -472.289 -472.289] [0.0000], Avg: [-531.789 -531.789 -531.789] (0.476)
Step: 7449, Reward: [-686.169 -686.169 -686.169] [0.0000], Avg: [-532.826 -532.826 -532.826] (0.474)
Step: 7499, Reward: [-378.09 -378.09 -378.09] [0.0000], Avg: [-531.794 -531.794 -531.794] (0.471)
Step: 7549, Reward: [-707.379 -707.379 -707.379] [0.0000], Avg: [-532.957 -532.957 -532.957] (0.469)
Step: 7599, Reward: [-806.373 -806.373 -806.373] [0.0000], Avg: [-534.756 -534.756 -534.756] (0.467)
Step: 7649, Reward: [-473.059 -473.059 -473.059] [0.0000], Avg: [-534.352 -534.352 -534.352] (0.464)
Step: 7699, Reward: [-662.063 -662.063 -662.063] [0.0000], Avg: [-535.182 -535.182 -535.182] (0.462)
Step: 7749, Reward: [-760.618 -760.618 -760.618] [0.0000], Avg: [-536.636 -536.636 -536.636] (0.460)
Step: 7799, Reward: [-670.869 -670.869 -670.869] [0.0000], Avg: [-537.497 -537.497 -537.497] (0.458)
Step: 7849, Reward: [-869.141 -869.141 -869.141] [0.0000], Avg: [-539.609 -539.609 -539.609] (0.455)
Step: 7899, Reward: [-452.049 -452.049 -452.049] [0.0000], Avg: [-539.055 -539.055 -539.055] (0.453)
Step: 7949, Reward: [-629.27 -629.27 -629.27] [0.0000], Avg: [-539.622 -539.622 -539.622] (0.451)
Step: 7999, Reward: [-518.203 -518.203 -518.203] [0.0000], Avg: [-539.488 -539.488 -539.488] (0.448)
Step: 8049, Reward: [-351.24 -351.24 -351.24] [0.0000], Avg: [-538.319 -538.319 -538.319] (0.446)
Step: 8099, Reward: [-605.346 -605.346 -605.346] [0.0000], Avg: [-538.733 -538.733 -538.733] (0.444)
Step: 8149, Reward: [-642.817 -642.817 -642.817] [0.0000], Avg: [-539.371 -539.371 -539.371] (0.442)
Step: 8199, Reward: [-523.767 -523.767 -523.767] [0.0000], Avg: [-539.276 -539.276 -539.276] (0.440)
Step: 8249, Reward: [-652.488 -652.488 -652.488] [0.0000], Avg: [-539.962 -539.962 -539.962] (0.437)
Step: 8299, Reward: [-511.623 -511.623 -511.623] [0.0000], Avg: [-539.792 -539.792 -539.792] (0.435)
Step: 8349, Reward: [-586.921 -586.921 -586.921] [0.0000], Avg: [-540.074 -540.074 -540.074] (0.433)
Step: 8399, Reward: [-589.225 -589.225 -589.225] [0.0000], Avg: [-540.366 -540.366 -540.366] (0.431)
Step: 8449, Reward: [-585.077 -585.077 -585.077] [0.0000], Avg: [-540.631 -540.631 -540.631] (0.429)
Step: 8499, Reward: [-417.28 -417.28 -417.28] [0.0000], Avg: [-539.905 -539.905 -539.905] (0.427)
Step: 8549, Reward: [-991.058 -991.058 -991.058] [0.0000], Avg: [-542.544 -542.544 -542.544] (0.424)
Step: 8599, Reward: [-793.612 -793.612 -793.612] [0.0000], Avg: [-544.003 -544.003 -544.003] (0.422)
Step: 8649, Reward: [-700.63 -700.63 -700.63] [0.0000], Avg: [-544.909 -544.909 -544.909] (0.420)
Step: 8699, Reward: [-855.343 -855.343 -855.343] [0.0000], Avg: [-546.693 -546.693 -546.693] (0.418)
Step: 8749, Reward: [-423.491 -423.491 -423.491] [0.0000], Avg: [-545.989 -545.989 -545.989] (0.416)
Step: 8799, Reward: [-604.656 -604.656 -604.656] [0.0000], Avg: [-546.322 -546.322 -546.322] (0.414)
Step: 8849, Reward: [-734.741 -734.741 -734.741] [0.0000], Avg: [-547.387 -547.387 -547.387] (0.412)
Step: 8899, Reward: [-695.653 -695.653 -695.653] [0.0000], Avg: [-548.22 -548.22 -548.22] (0.410)
Step: 8949, Reward: [-717.484 -717.484 -717.484] [0.0000], Avg: [-549.165 -549.165 -549.165] (0.408)
Step: 8999, Reward: [-701.226 -701.226 -701.226] [0.0000], Avg: [-550.01 -550.01 -550.01] (0.406)
Step: 9049, Reward: [-656.297 -656.297 -656.297] [0.0000], Avg: [-550.597 -550.597 -550.597] (0.404)
Step: 9099, Reward: [-796.676 -796.676 -796.676] [0.0000], Avg: [-551.949 -551.949 -551.949] (0.402)
Step: 9149, Reward: [-703.97 -703.97 -703.97] [0.0000], Avg: [-552.78 -552.78 -552.78] (0.400)
Step: 9199, Reward: [-972.332 -972.332 -972.332] [0.0000], Avg: [-555.06 -555.06 -555.06] (0.398)
Step: 9249, Reward: [-698.106 -698.106 -698.106] [0.0000], Avg: [-555.833 -555.833 -555.833] (0.396)
Step: 9299, Reward: [-520.047 -520.047 -520.047] [0.0000], Avg: [-555.641 -555.641 -555.641] (0.394)
Step: 9349, Reward: [-443.471 -443.471 -443.471] [0.0000], Avg: [-555.041 -555.041 -555.041] (0.392)
Step: 9399, Reward: [-414.812 -414.812 -414.812] [0.0000], Avg: [-554.295 -554.295 -554.295] (0.390)
Step: 9449, Reward: [-646.981 -646.981 -646.981] [0.0000], Avg: [-554.786 -554.786 -554.786] (0.388)
Step: 9499, Reward: [-397.153 -397.153 -397.153] [0.0000], Avg: [-553.956 -553.956 -553.956] (0.386)
Step: 9549, Reward: [-467.589 -467.589 -467.589] [0.0000], Avg: [-553.504 -553.504 -553.504] (0.384)
Step: 9599, Reward: [-664.794 -664.794 -664.794] [0.0000], Avg: [-554.083 -554.083 -554.083] (0.382)
Step: 9649, Reward: [-638.461 -638.461 -638.461] [0.0000], Avg: [-554.521 -554.521 -554.521] (0.380)
Step: 9699, Reward: [-572.783 -572.783 -572.783] [0.0000], Avg: [-554.615 -554.615 -554.615] (0.378)
Step: 9749, Reward: [-694.768 -694.768 -694.768] [0.0000], Avg: [-555.334 -555.334 -555.334] (0.376)
Step: 9799, Reward: [-568.118 -568.118 -568.118] [0.0000], Avg: [-555.399 -555.399 -555.399] (0.374)
Step: 9849, Reward: [-650.367 -650.367 -650.367] [0.0000], Avg: [-555.881 -555.881 -555.881] (0.373)
Step: 9899, Reward: [-609.46 -609.46 -609.46] [0.0000], Avg: [-556.151 -556.151 -556.151] (0.371)
Step: 9949, Reward: [-515.458 -515.458 -515.458] [0.0000], Avg: [-555.947 -555.947 -555.947] (0.369)
Step: 9999, Reward: [-960.279 -960.279 -960.279] [0.0000], Avg: [-557.969 -557.969 -557.969] (0.367)
Step: 10049, Reward: [-528.191 -528.191 -528.191] [0.0000], Avg: [-557.82 -557.82 -557.82] (0.365)
Step: 10099, Reward: [-1257.521 -1257.521 -1257.521] [0.0000], Avg: [-561.284 -561.284 -561.284] (0.363)
Step: 10149, Reward: [-422.569 -422.569 -422.569] [0.0000], Avg: [-560.601 -560.601 -560.601] (0.361)
Step: 10199, Reward: [-944.852 -944.852 -944.852] [0.0000], Avg: [-562.485 -562.485 -562.485] (0.360)
Step: 10249, Reward: [-588.564 -588.564 -588.564] [0.0000], Avg: [-562.612 -562.612 -562.612] (0.358)
Step: 10299, Reward: [-650.531 -650.531 -650.531] [0.0000], Avg: [-563.039 -563.039 -563.039] (0.356)
Step: 10349, Reward: [-542.132 -542.132 -542.132] [0.0000], Avg: [-562.938 -562.938 -562.938] (0.354)
Step: 10399, Reward: [-581.745 -581.745 -581.745] [0.0000], Avg: [-563.028 -563.028 -563.028] (0.353)
Step: 10449, Reward: [-633.459 -633.459 -633.459] [0.0000], Avg: [-563.365 -563.365 -563.365] (0.351)
Step: 10499, Reward: [-613.036 -613.036 -613.036] [0.0000], Avg: [-563.602 -563.602 -563.602] (0.349)
Step: 10549, Reward: [-513.63 -513.63 -513.63] [0.0000], Avg: [-563.365 -563.365 -563.365] (0.347)
Step: 10599, Reward: [-441.906 -441.906 -441.906] [0.0000], Avg: [-562.792 -562.792 -562.792] (0.346)
Step: 10649, Reward: [-483.498 -483.498 -483.498] [0.0000], Avg: [-562.419 -562.419 -562.419] (0.344)
Step: 10699, Reward: [-705.277 -705.277 -705.277] [0.0000], Avg: [-563.087 -563.087 -563.087] (0.342)
Step: 10749, Reward: [-429.621 -429.621 -429.621] [0.0000], Avg: [-562.466 -562.466 -562.466] (0.340)
Step: 10799, Reward: [-687.844 -687.844 -687.844] [0.0000], Avg: [-563.047 -563.047 -563.047] (0.339)
Step: 10849, Reward: [-586.777 -586.777 -586.777] [0.0000], Avg: [-563.156 -563.156 -563.156] (0.337)
Step: 10899, Reward: [-477.122 -477.122 -477.122] [0.0000], Avg: [-562.761 -562.761 -562.761] (0.335)
Step: 10949, Reward: [-539.705 -539.705 -539.705] [0.0000], Avg: [-562.656 -562.656 -562.656] (0.334)
Step: 10999, Reward: [-539.552 -539.552 -539.552] [0.0000], Avg: [-562.551 -562.551 -562.551] (0.332)
Step: 11049, Reward: [-540.815 -540.815 -540.815] [0.0000], Avg: [-562.453 -562.453 -562.453] (0.330)
Step: 11099, Reward: [-675.545 -675.545 -675.545] [0.0000], Avg: [-562.962 -562.962 -562.962] (0.329)
Step: 11149, Reward: [-1036.311 -1036.311 -1036.311] [0.0000], Avg: [-565.085 -565.085 -565.085] (0.327)
Step: 11199, Reward: [-744.352 -744.352 -744.352] [0.0000], Avg: [-565.885 -565.885 -565.885] (0.325)
Step: 11249, Reward: [-1080.534 -1080.534 -1080.534] [0.0000], Avg: [-568.172 -568.172 -568.172] (0.324)
Step: 11299, Reward: [-448.889 -448.889 -448.889] [0.0000], Avg: [-567.645 -567.645 -567.645] (0.322)
Step: 11349, Reward: [-654.425 -654.425 -654.425] [0.0000], Avg: [-568.027 -568.027 -568.027] (0.321)
Step: 11399, Reward: [-389.69 -389.69 -389.69] [0.0000], Avg: [-567.245 -567.245 -567.245] (0.319)
Step: 11449, Reward: [-560.049 -560.049 -560.049] [0.0000], Avg: [-567.213 -567.213 -567.213] (0.317)
Step: 11499, Reward: [-576.765 -576.765 -576.765] [0.0000], Avg: [-567.255 -567.255 -567.255] (0.316)
Step: 11549, Reward: [-865.796 -865.796 -865.796] [0.0000], Avg: [-568.547 -568.547 -568.547] (0.314)
Step: 11599, Reward: [-639.638 -639.638 -639.638] [0.0000], Avg: [-568.854 -568.854 -568.854] (0.313)
Step: 11649, Reward: [-669.444 -669.444 -669.444] [0.0000], Avg: [-569.285 -569.285 -569.285] (0.311)
Step: 11699, Reward: [-792.855 -792.855 -792.855] [0.0000], Avg: [-570.241 -570.241 -570.241] (0.309)
Step: 11749, Reward: [-527.909 -527.909 -527.909] [0.0000], Avg: [-570.061 -570.061 -570.061] (0.308)
Step: 11799, Reward: [-465.14 -465.14 -465.14] [0.0000], Avg: [-569.616 -569.616 -569.616] (0.306)
Step: 11849, Reward: [-640.169 -640.169 -640.169] [0.0000], Avg: [-569.914 -569.914 -569.914] (0.305)
Step: 11899, Reward: [-653.645 -653.645 -653.645] [0.0000], Avg: [-570.266 -570.266 -570.266] (0.303)
Step: 11949, Reward: [-658.534 -658.534 -658.534] [0.0000], Avg: [-570.635 -570.635 -570.635] (0.302)
Step: 11999, Reward: [-909.847 -909.847 -909.847] [0.0000], Avg: [-572.048 -572.048 -572.048] (0.300)
Step: 12049, Reward: [-721.317 -721.317 -721.317] [0.0000], Avg: [-572.668 -572.668 -572.668] (0.299)
Step: 12099, Reward: [-1490.114 -1490.114 -1490.114] [0.0000], Avg: [-576.459 -576.459 -576.459] (0.297)
Step: 12149, Reward: [-1754.383 -1754.383 -1754.383] [0.0000], Avg: [-581.306 -581.306 -581.306] (0.296)
Step: 12199, Reward: [-1791.54 -1791.54 -1791.54] [0.0000], Avg: [-586.266 -586.266 -586.266] (0.294)
Step: 12249, Reward: [-1308.668 -1308.668 -1308.668] [0.0000], Avg: [-589.215 -589.215 -589.215] (0.293)
Step: 12299, Reward: [-1457.576 -1457.576 -1457.576] [0.0000], Avg: [-592.745 -592.745 -592.745] (0.291)
Step: 12349, Reward: [-1806.04 -1806.04 -1806.04] [0.0000], Avg: [-597.657 -597.657 -597.657] (0.290)
Step: 12399, Reward: [-423.103 -423.103 -423.103] [0.0000], Avg: [-596.953 -596.953 -596.953] (0.288)
Step: 12449, Reward: [-549.139 -549.139 -549.139] [0.0000], Avg: [-596.761 -596.761 -596.761] (0.287)
Step: 12499, Reward: [-522.958 -522.958 -522.958] [0.0000], Avg: [-596.466 -596.466 -596.466] (0.286)
Step: 12549, Reward: [-685.247 -685.247 -685.247] [0.0000], Avg: [-596.819 -596.819 -596.819] (0.284)
Step: 12599, Reward: [-510.31 -510.31 -510.31] [0.0000], Avg: [-596.476 -596.476 -596.476] (0.283)
Step: 12649, Reward: [-709.159 -709.159 -709.159] [0.0000], Avg: [-596.922 -596.922 -596.922] (0.281)
Step: 12699, Reward: [-472.366 -472.366 -472.366] [0.0000], Avg: [-596.431 -596.431 -596.431] (0.280)
Step: 12749, Reward: [-376.867 -376.867 -376.867] [0.0000], Avg: [-595.57 -595.57 -595.57] (0.279)
Step: 12799, Reward: [-428.387 -428.387 -428.387] [0.0000], Avg: [-594.917 -594.917 -594.917] (0.277)
Step: 12849, Reward: [-509.974 -509.974 -509.974] [0.0000], Avg: [-594.587 -594.587 -594.587] (0.276)
Step: 12899, Reward: [-559.463 -559.463 -559.463] [0.0000], Avg: [-594.45 -594.45 -594.45] (0.274)
Step: 12949, Reward: [-526.908 -526.908 -526.908] [0.0000], Avg: [-594.19 -594.19 -594.19] (0.273)
Step: 12999, Reward: [-715.352 -715.352 -715.352] [0.0000], Avg: [-594.656 -594.656 -594.656] (0.272)
Step: 13049, Reward: [-384.109 -384.109 -384.109] [0.0000], Avg: [-593.849 -593.849 -593.849] (0.270)
Step: 13099, Reward: [-428.721 -428.721 -428.721] [0.0000], Avg: [-593.219 -593.219 -593.219] (0.269)
Step: 13149, Reward: [-424.916 -424.916 -424.916] [0.0000], Avg: [-592.579 -592.579 -592.579] (0.268)
Step: 13199, Reward: [-355.759 -355.759 -355.759] [0.0000], Avg: [-591.682 -591.682 -591.682] (0.266)
Step: 13249, Reward: [-311.665 -311.665 -311.665] [0.0000], Avg: [-590.625 -590.625 -590.625] (0.265)
Step: 13299, Reward: [-551.317 -551.317 -551.317] [0.0000], Avg: [-590.477 -590.477 -590.477] (0.264)
Step: 13349, Reward: [-517.691 -517.691 -517.691] [0.0000], Avg: [-590.205 -590.205 -590.205] (0.262)
Step: 13399, Reward: [-439.488 -439.488 -439.488] [0.0000], Avg: [-589.642 -589.642 -589.642] (0.261)
Step: 13449, Reward: [-445.031 -445.031 -445.031] [0.0000], Avg: [-589.105 -589.105 -589.105] (0.260)
Step: 13499, Reward: [-482.912 -482.912 -482.912] [0.0000], Avg: [-588.711 -588.711 -588.711] (0.258)
Step: 13549, Reward: [-514.031 -514.031 -514.031] [0.0000], Avg: [-588.436 -588.436 -588.436] (0.257)
Step: 13599, Reward: [-367.557 -367.557 -367.557] [0.0000], Avg: [-587.624 -587.624 -587.624] (0.256)
Step: 13649, Reward: [-360.81 -360.81 -360.81] [0.0000], Avg: [-586.793 -586.793 -586.793] (0.255)
Step: 13699, Reward: [-448.741 -448.741 -448.741] [0.0000], Avg: [-586.289 -586.289 -586.289] (0.253)
Step: 13749, Reward: [-608.042 -608.042 -608.042] [0.0000], Avg: [-586.368 -586.368 -586.368] (0.252)
Step: 13799, Reward: [-445.389 -445.389 -445.389] [0.0000], Avg: [-585.857 -585.857 -585.857] (0.251)
Step: 13849, Reward: [-390.928 -390.928 -390.928] [0.0000], Avg: [-585.154 -585.154 -585.154] (0.249)
Step: 13899, Reward: [-453.607 -453.607 -453.607] [0.0000], Avg: [-584.681 -584.681 -584.681] (0.248)
Step: 13949, Reward: [-517.291 -517.291 -517.291] [0.0000], Avg: [-584.439 -584.439 -584.439] (0.247)
Step: 13999, Reward: [-600.699 -600.699 -600.699] [0.0000], Avg: [-584.497 -584.497 -584.497] (0.246)
Step: 14049, Reward: [-548.323 -548.323 -548.323] [0.0000], Avg: [-584.368 -584.368 -584.368] (0.245)
Step: 14099, Reward: [-463.467 -463.467 -463.467] [0.0000], Avg: [-583.94 -583.94 -583.94] (0.243)
Step: 14149, Reward: [-794.211 -794.211 -794.211] [0.0000], Avg: [-584.683 -584.683 -584.683] (0.242)
Step: 14199, Reward: [-432.673 -432.673 -432.673] [0.0000], Avg: [-584.147 -584.147 -584.147] (0.241)
Step: 14249, Reward: [-608.969 -608.969 -608.969] [0.0000], Avg: [-584.234 -584.234 -584.234] (0.240)
Step: 14299, Reward: [-668.998 -668.998 -668.998] [0.0000], Avg: [-584.531 -584.531 -584.531] (0.238)
Step: 14349, Reward: [-266.579 -266.579 -266.579] [0.0000], Avg: [-583.423 -583.423 -583.423] (0.237)
Step: 14399, Reward: [-377.593 -377.593 -377.593] [0.0000], Avg: [-582.708 -582.708 -582.708] (0.236)
Step: 14449, Reward: [-751.211 -751.211 -751.211] [0.0000], Avg: [-583.291 -583.291 -583.291] (0.235)
Step: 14499, Reward: [-713.791 -713.791 -713.791] [0.0000], Avg: [-583.741 -583.741 -583.741] (0.234)
Step: 14549, Reward: [-429.167 -429.167 -429.167] [0.0000], Avg: [-583.21 -583.21 -583.21] (0.233)
Step: 14599, Reward: [-527.725 -527.725 -527.725] [0.0000], Avg: [-583.02 -583.02 -583.02] (0.231)
Step: 14649, Reward: [-604.948 -604.948 -604.948] [0.0000], Avg: [-583.095 -583.095 -583.095] (0.230)
Step: 14699, Reward: [-454.381 -454.381 -454.381] [0.0000], Avg: [-582.657 -582.657 -582.657] (0.229)
Step: 14749, Reward: [-468.861 -468.861 -468.861] [0.0000], Avg: [-582.271 -582.271 -582.271] (0.228)
Step: 14799, Reward: [-1013.938 -1013.938 -1013.938] [0.0000], Avg: [-583.73 -583.73 -583.73] (0.227)
Step: 14849, Reward: [-599.004 -599.004 -599.004] [0.0000], Avg: [-583.781 -583.781 -583.781] (0.226)
Step: 14899, Reward: [-764.736 -764.736 -764.736] [0.0000], Avg: [-584.388 -584.388 -584.388] (0.225)
Step: 14949, Reward: [-792.09 -792.09 -792.09] [0.0000], Avg: [-585.083 -585.083 -585.083] (0.223)
Step: 14999, Reward: [-517.647 -517.647 -517.647] [0.0000], Avg: [-584.858 -584.858 -584.858] (0.222)
Step: 15049, Reward: [-682.206 -682.206 -682.206] [0.0000], Avg: [-585.182 -585.182 -585.182] (0.221)
Step: 15099, Reward: [-654.267 -654.267 -654.267] [0.0000], Avg: [-585.41 -585.41 -585.41] (0.220)
Step: 15149, Reward: [-420.716 -420.716 -420.716] [0.0000], Avg: [-584.867 -584.867 -584.867] (0.219)
Step: 15199, Reward: [-481.308 -481.308 -481.308] [0.0000], Avg: [-584.526 -584.526 -584.526] (0.218)
Step: 15249, Reward: [-531.071 -531.071 -531.071] [0.0000], Avg: [-584.351 -584.351 -584.351] (0.217)
Step: 15299, Reward: [-907.351 -907.351 -907.351] [0.0000], Avg: [-585.407 -585.407 -585.407] (0.216)
Step: 15349, Reward: [-756.593 -756.593 -756.593] [0.0000], Avg: [-585.964 -585.964 -585.964] (0.215)
Step: 15399, Reward: [-586.568 -586.568 -586.568] [0.0000], Avg: [-585.966 -585.966 -585.966] (0.214)
Step: 15449, Reward: [-628.721 -628.721 -628.721] [0.0000], Avg: [-586.105 -586.105 -586.105] (0.212)
Step: 15499, Reward: [-581.785 -581.785 -581.785] [0.0000], Avg: [-586.091 -586.091 -586.091] (0.211)
Step: 15549, Reward: [-1111.941 -1111.941 -1111.941] [0.0000], Avg: [-587.781 -587.781 -587.781] (0.210)
Step: 15599, Reward: [-375.396 -375.396 -375.396] [0.0000], Avg: [-587.101 -587.101 -587.101] (0.209)
Step: 15649, Reward: [-809.003 -809.003 -809.003] [0.0000], Avg: [-587.81 -587.81 -587.81] (0.208)
Step: 15699, Reward: [-783.972 -783.972 -783.972] [0.0000], Avg: [-588.434 -588.434 -588.434] (0.207)
Step: 15749, Reward: [-993.059 -993.059 -993.059] [0.0000], Avg: [-589.719 -589.719 -589.719] (0.206)
Step: 15799, Reward: [-1623.95 -1623.95 -1623.95] [0.0000], Avg: [-592.992 -592.992 -592.992] (0.205)
Step: 15849, Reward: [-1923.746 -1923.746 -1923.746] [0.0000], Avg: [-597.19 -597.19 -597.19] (0.204)
Step: 15899, Reward: [-1149.096 -1149.096 -1149.096] [0.0000], Avg: [-598.925 -598.925 -598.925] (0.203)
Step: 15949, Reward: [-1457.184 -1457.184 -1457.184] [0.0000], Avg: [-601.616 -601.616 -601.616] (0.202)
Step: 15999, Reward: [-2245.079 -2245.079 -2245.079] [0.0000], Avg: [-606.752 -606.752 -606.752] (0.201)
Step: 16049, Reward: [-515.502 -515.502 -515.502] [0.0000], Avg: [-606.467 -606.467 -606.467] (0.200)
Step: 16099, Reward: [-372.752 -372.752 -372.752] [0.0000], Avg: [-605.741 -605.741 -605.741] (0.199)
Step: 16149, Reward: [-705.353 -705.353 -705.353] [0.0000], Avg: [-606.05 -606.05 -606.05] (0.198)
Step: 16199, Reward: [-991.151 -991.151 -991.151] [0.0000], Avg: [-607.238 -607.238 -607.238] (0.197)
Step: 16249, Reward: [-621.202 -621.202 -621.202] [0.0000], Avg: [-607.281 -607.281 -607.281] (0.196)
Step: 16299, Reward: [-562.292 -562.292 -562.292] [0.0000], Avg: [-607.143 -607.143 -607.143] (0.195)
Step: 16349, Reward: [-571.465 -571.465 -571.465] [0.0000], Avg: [-607.034 -607.034 -607.034] (0.194)
Step: 16399, Reward: [-430.233 -430.233 -430.233] [0.0000], Avg: [-606.495 -606.495 -606.495] (0.193)
Step: 16449, Reward: [-681.875 -681.875 -681.875] [0.0000], Avg: [-606.724 -606.724 -606.724] (0.192)
Step: 16499, Reward: [-654.429 -654.429 -654.429] [0.0000], Avg: [-606.869 -606.869 -606.869] (0.191)
Step: 16549, Reward: [-538.49 -538.49 -538.49] [0.0000], Avg: [-606.662 -606.662 -606.662] (0.190)
Step: 16599, Reward: [-448.407 -448.407 -448.407] [0.0000], Avg: [-606.186 -606.186 -606.186] (0.189)
Step: 16649, Reward: [-698.425 -698.425 -698.425] [0.0000], Avg: [-606.463 -606.463 -606.463] (0.188)
Step: 16699, Reward: [-632.517 -632.517 -632.517] [0.0000], Avg: [-606.541 -606.541 -606.541] (0.187)
Step: 16749, Reward: [-634.185 -634.185 -634.185] [0.0000], Avg: [-606.623 -606.623 -606.623] (0.187)
Step: 16799, Reward: [-419.172 -419.172 -419.172] [0.0000], Avg: [-606.065 -606.065 -606.065] (0.186)
Step: 16849, Reward: [-453.722 -453.722 -453.722] [0.0000], Avg: [-605.613 -605.613 -605.613] (0.185)
Step: 16899, Reward: [-510.076 -510.076 -510.076] [0.0000], Avg: [-605.331 -605.331 -605.331] (0.184)
Step: 16949, Reward: [-718.788 -718.788 -718.788] [0.0000], Avg: [-605.665 -605.665 -605.665] (0.183)
Step: 16999, Reward: [-771.925 -771.925 -771.925] [0.0000], Avg: [-606.154 -606.154 -606.154] (0.182)
Step: 17049, Reward: [-402.184 -402.184 -402.184] [0.0000], Avg: [-605.556 -605.556 -605.556] (0.181)
Step: 17099, Reward: [-771.676 -771.676 -771.676] [0.0000], Avg: [-606.042 -606.042 -606.042] (0.180)
Step: 17149, Reward: [-718.84 -718.84 -718.84] [0.0000], Avg: [-606.371 -606.371 -606.371] (0.179)
Step: 17199, Reward: [-643.053 -643.053 -643.053] [0.0000], Avg: [-606.477 -606.477 -606.477] (0.178)
Step: 17249, Reward: [-447.889 -447.889 -447.889] [0.0000], Avg: [-606.018 -606.018 -606.018] (0.177)
Step: 17299, Reward: [-923.128 -923.128 -923.128] [0.0000], Avg: [-606.934 -606.934 -606.934] (0.177)
Step: 17349, Reward: [-488.611 -488.611 -488.611] [0.0000], Avg: [-606.593 -606.593 -606.593] (0.176)
Step: 17399, Reward: [-638.767 -638.767 -638.767] [0.0000], Avg: [-606.686 -606.686 -606.686] (0.175)
Step: 17449, Reward: [-1076.835 -1076.835 -1076.835] [0.0000], Avg: [-608.033 -608.033 -608.033] (0.174)
Step: 17499, Reward: [-441.306 -441.306 -441.306] [0.0000], Avg: [-607.556 -607.556 -607.556] (0.173)
Step: 17549, Reward: [-632.089 -632.089 -632.089] [0.0000], Avg: [-607.626 -607.626 -607.626] (0.172)
Step: 17599, Reward: [-775.378 -775.378 -775.378] [0.0000], Avg: [-608.103 -608.103 -608.103] (0.171)
Step: 17649, Reward: [-551.361 -551.361 -551.361] [0.0000], Avg: [-607.942 -607.942 -607.942] (0.170)
Step: 17699, Reward: [-893.562 -893.562 -893.562] [0.0000], Avg: [-608.749 -608.749 -608.749] (0.170)
Step: 17749, Reward: [-717.16 -717.16 -717.16] [0.0000], Avg: [-609.054 -609.054 -609.054] (0.169)
Step: 17799, Reward: [-720.691 -720.691 -720.691] [0.0000], Avg: [-609.368 -609.368 -609.368] (0.168)
Step: 17849, Reward: [-669.667 -669.667 -669.667] [0.0000], Avg: [-609.537 -609.537 -609.537] (0.167)
Step: 17899, Reward: [-642.653 -642.653 -642.653] [0.0000], Avg: [-609.629 -609.629 -609.629] (0.166)
Step: 17949, Reward: [-722.063 -722.063 -722.063] [0.0000], Avg: [-609.943 -609.943 -609.943] (0.165)
Step: 17999, Reward: [-383.07 -383.07 -383.07] [0.0000], Avg: [-609.312 -609.312 -609.312] (0.165)
Step: 18049, Reward: [-688.188 -688.188 -688.188] [0.0000], Avg: [-609.531 -609.531 -609.531] (0.164)
Step: 18099, Reward: [-342.236 -342.236 -342.236] [0.0000], Avg: [-608.792 -608.792 -608.792] (0.163)
Step: 18149, Reward: [-1564.216 -1564.216 -1564.216] [0.0000], Avg: [-611.424 -611.424 -611.424] (0.162)
Step: 18199, Reward: [-484.568 -484.568 -484.568] [0.0000], Avg: [-611.076 -611.076 -611.076] (0.161)
Step: 18249, Reward: [-2037.117 -2037.117 -2037.117] [0.0000], Avg: [-614.983 -614.983 -614.983] (0.160)
Step: 18299, Reward: [-1850.494 -1850.494 -1850.494] [0.0000], Avg: [-618.359 -618.359 -618.359] (0.160)
Step: 18349, Reward: [-2044.74 -2044.74 -2044.74] [0.0000], Avg: [-622.245 -622.245 -622.245] (0.159)
Step: 18399, Reward: [-1669.251 -1669.251 -1669.251] [0.0000], Avg: [-625.09 -625.09 -625.09] (0.158)
Step: 18449, Reward: [-1845.571 -1845.571 -1845.571] [0.0000], Avg: [-628.398 -628.398 -628.398] (0.157)
Step: 18499, Reward: [-1866.638 -1866.638 -1866.638] [0.0000], Avg: [-631.745 -631.745 -631.745] (0.157)
Step: 18549, Reward: [-2116.856 -2116.856 -2116.856] [0.0000], Avg: [-635.748 -635.748 -635.748] (0.156)
Step: 18599, Reward: [-1816.901 -1816.901 -1816.901] [0.0000], Avg: [-638.923 -638.923 -638.923] (0.155)
Step: 18649, Reward: [-1866.588 -1866.588 -1866.588] [0.0000], Avg: [-642.214 -642.214 -642.214] (0.154)
Step: 18699, Reward: [-1956.595 -1956.595 -1956.595] [0.0000], Avg: [-645.728 -645.728 -645.728] (0.153)
Step: 18749, Reward: [-1746.689 -1746.689 -1746.689] [0.0000], Avg: [-648.664 -648.664 -648.664] (0.153)
Step: 18799, Reward: [-2062.618 -2062.618 -2062.618] [0.0000], Avg: [-652.425 -652.425 -652.425] (0.152)
Step: 18849, Reward: [-2071.09 -2071.09 -2071.09] [0.0000], Avg: [-656.188 -656.188 -656.188] (0.151)
Step: 18899, Reward: [-1234.365 -1234.365 -1234.365] [0.0000], Avg: [-657.717 -657.717 -657.717] (0.150)
Step: 18949, Reward: [-1883.482 -1883.482 -1883.482] [0.0000], Avg: [-660.952 -660.952 -660.952] (0.150)
Step: 18999, Reward: [-1686.85 -1686.85 -1686.85] [0.0000], Avg: [-663.651 -663.651 -663.651] (0.149)
Step: 19049, Reward: [-2050.623 -2050.623 -2050.623] [0.0000], Avg: [-667.292 -667.292 -667.292] (0.148)
Step: 19099, Reward: [-2055.946 -2055.946 -2055.946] [0.0000], Avg: [-670.927 -670.927 -670.927] (0.147)
Step: 19149, Reward: [-1412.036 -1412.036 -1412.036] [0.0000], Avg: [-672.862 -672.862 -672.862] (0.147)
Step: 19199, Reward: [-1284.432 -1284.432 -1284.432] [0.0000], Avg: [-674.455 -674.455 -674.455] (0.146)
Step: 19249, Reward: [-1714.416 -1714.416 -1714.416] [0.0000], Avg: [-677.156 -677.156 -677.156] (0.145)
Step: 19299, Reward: [-1472.392 -1472.392 -1472.392] [0.0000], Avg: [-679.216 -679.216 -679.216] (0.144)
Step: 19349, Reward: [-1044.743 -1044.743 -1044.743] [0.0000], Avg: [-680.16 -680.16 -680.16] (0.144)
Step: 19399, Reward: [-1714.6 -1714.6 -1714.6] [0.0000], Avg: [-682.827 -682.827 -682.827] (0.143)
Step: 19449, Reward: [-2015.264 -2015.264 -2015.264] [0.0000], Avg: [-686.252 -686.252 -686.252] (0.142)
Step: 19499, Reward: [-1947.97 -1947.97 -1947.97] [0.0000], Avg: [-689.487 -689.487 -689.487] (0.142)
Step: 19549, Reward: [-1814.396 -1814.396 -1814.396] [0.0000], Avg: [-692.364 -692.364 -692.364] (0.141)
Step: 19599, Reward: [-2336.194 -2336.194 -2336.194] [0.0000], Avg: [-696.557 -696.557 -696.557] (0.140)
Step: 19649, Reward: [-1907.488 -1907.488 -1907.488] [0.0000], Avg: [-699.639 -699.639 -699.639] (0.139)
Step: 19699, Reward: [-1401.681 -1401.681 -1401.681] [0.0000], Avg: [-701.421 -701.421 -701.421] (0.139)
Step: 19749, Reward: [-1924.759 -1924.759 -1924.759] [0.0000], Avg: [-704.518 -704.518 -704.518] (0.138)
Step: 19799, Reward: [-1715.75 -1715.75 -1715.75] [0.0000], Avg: [-707.071 -707.071 -707.071] (0.137)
Step: 19849, Reward: [-1736.417 -1736.417 -1736.417] [0.0000], Avg: [-709.664 -709.664 -709.664] (0.137)
Step: 19899, Reward: [-2215.8 -2215.8 -2215.8] [0.0000], Avg: [-713.448 -713.448 -713.448] (0.136)
Step: 19949, Reward: [-1480.548 -1480.548 -1480.548] [0.0000], Avg: [-715.371 -715.371 -715.371] (0.135)
Step: 19999, Reward: [-1938.433 -1938.433 -1938.433] [0.0000], Avg: [-718.428 -718.428 -718.428] (0.135)
Step: 20049, Reward: [-1452.221 -1452.221 -1452.221] [0.0000], Avg: [-720.258 -720.258 -720.258] (0.134)
Step: 20099, Reward: [-1315.083 -1315.083 -1315.083] [0.0000], Avg: [-721.738 -721.738 -721.738] (0.133)
Step: 20149, Reward: [-1897.187 -1897.187 -1897.187] [0.0000], Avg: [-724.655 -724.655 -724.655] (0.133)
Step: 20199, Reward: [-1460.652 -1460.652 -1460.652] [0.0000], Avg: [-726.477 -726.477 -726.477] (0.132)
Step: 20249, Reward: [-1494.399 -1494.399 -1494.399] [0.0000], Avg: [-728.373 -728.373 -728.373] (0.131)
Step: 20299, Reward: [-1849.36 -1849.36 -1849.36] [0.0000], Avg: [-731.134 -731.134 -731.134] (0.131)
Step: 20349, Reward: [-1785.005 -1785.005 -1785.005] [0.0000], Avg: [-733.723 -733.723 -733.723] (0.130)
Step: 20399, Reward: [-2153.617 -2153.617 -2153.617] [0.0000], Avg: [-737.203 -737.203 -737.203] (0.129)
Step: 20449, Reward: [-1591.936 -1591.936 -1591.936] [0.0000], Avg: [-739.293 -739.293 -739.293] (0.129)
Step: 20499, Reward: [-1655.688 -1655.688 -1655.688] [0.0000], Avg: [-741.528 -741.528 -741.528] (0.128)
Step: 20549, Reward: [-1483.273 -1483.273 -1483.273] [0.0000], Avg: [-743.333 -743.333 -743.333] (0.127)
Step: 20599, Reward: [-1933.845 -1933.845 -1933.845] [0.0000], Avg: [-746.222 -746.222 -746.222] (0.127)
Step: 20649, Reward: [-1999.525 -1999.525 -1999.525] [0.0000], Avg: [-749.257 -749.257 -749.257] (0.126)
Step: 20699, Reward: [-1500.9 -1500.9 -1500.9] [0.0000], Avg: [-751.073 -751.073 -751.073] (0.126)
Step: 20749, Reward: [-1644.281 -1644.281 -1644.281] [0.0000], Avg: [-753.225 -753.225 -753.225] (0.125)
Step: 20799, Reward: [-1585.363 -1585.363 -1585.363] [0.0000], Avg: [-755.225 -755.225 -755.225] (0.124)
Step: 20849, Reward: [-1731.738 -1731.738 -1731.738] [0.0000], Avg: [-757.567 -757.567 -757.567] (0.124)
Step: 20899, Reward: [-1514.678 -1514.678 -1514.678] [0.0000], Avg: [-759.378 -759.378 -759.378] (0.123)
Step: 20949, Reward: [-1824.666 -1824.666 -1824.666] [0.0000], Avg: [-761.921 -761.921 -761.921] (0.122)
Step: 20999, Reward: [-1378.31 -1378.31 -1378.31] [0.0000], Avg: [-763.388 -763.388 -763.388] (0.122)
Step: 21049, Reward: [-1666.66 -1666.66 -1666.66] [0.0000], Avg: [-765.534 -765.534 -765.534] (0.121)
Step: 21099, Reward: [-1726.545 -1726.545 -1726.545] [0.0000], Avg: [-767.811 -767.811 -767.811] (0.121)
Step: 21149, Reward: [-1582.887 -1582.887 -1582.887] [0.0000], Avg: [-769.738 -769.738 -769.738] (0.120)
Step: 21199, Reward: [-1749.816 -1749.816 -1749.816] [0.0000], Avg: [-772.05 -772.05 -772.05] (0.119)
Step: 21249, Reward: [-1560.269 -1560.269 -1560.269] [0.0000], Avg: [-773.904 -773.904 -773.904] (0.119)
Step: 21299, Reward: [-2063.438 -2063.438 -2063.438] [0.0000], Avg: [-776.931 -776.931 -776.931] (0.118)
Step: 21349, Reward: [-1857.198 -1857.198 -1857.198] [0.0000], Avg: [-779.461 -779.461 -779.461] (0.118)
Step: 21399, Reward: [-1790.852 -1790.852 -1790.852] [0.0000], Avg: [-781.824 -781.824 -781.824] (0.117)
Step: 21449, Reward: [-1773.257 -1773.257 -1773.257] [0.0000], Avg: [-784.135 -784.135 -784.135] (0.116)
Step: 21499, Reward: [-1750.538 -1750.538 -1750.538] [0.0000], Avg: [-786.383 -786.383 -786.383] (0.116)
Step: 21549, Reward: [-2239.83 -2239.83 -2239.83] [0.0000], Avg: [-789.755 -789.755 -789.755] (0.115)
Step: 21599, Reward: [-1767.609 -1767.609 -1767.609] [0.0000], Avg: [-792.019 -792.019 -792.019] (0.115)
Step: 21649, Reward: [-1943.447 -1943.447 -1943.447] [0.0000], Avg: [-794.678 -794.678 -794.678] (0.114)
Step: 21699, Reward: [-2331.082 -2331.082 -2331.082] [0.0000], Avg: [-798.218 -798.218 -798.218] (0.114)
Step: 21749, Reward: [-1670.933 -1670.933 -1670.933] [0.0000], Avg: [-800.224 -800.224 -800.224] (0.113)
Step: 21799, Reward: [-2339.778 -2339.778 -2339.778] [0.0000], Avg: [-803.755 -803.755 -803.755] (0.112)
Step: 21849, Reward: [-1959.298 -1959.298 -1959.298] [0.0000], Avg: [-806.399 -806.399 -806.399] (0.112)
Step: 21899, Reward: [-1612.687 -1612.687 -1612.687] [0.0000], Avg: [-808.24 -808.24 -808.24] (0.111)
Step: 21949, Reward: [-1982.351 -1982.351 -1982.351] [0.0000], Avg: [-810.915 -810.915 -810.915] (0.111)
Step: 21999, Reward: [-1800.324 -1800.324 -1800.324] [0.0000], Avg: [-813.163 -813.163 -813.163] (0.110)
Step: 22049, Reward: [-2380.457 -2380.457 -2380.457] [0.0000], Avg: [-816.717 -816.717 -816.717] (0.110)
Step: 22099, Reward: [-1862.049 -1862.049 -1862.049] [0.0000], Avg: [-819.082 -819.082 -819.082] (0.109)
Step: 22149, Reward: [-2081.683 -2081.683 -2081.683] [0.0000], Avg: [-821.933 -821.933 -821.933] (0.109)
Step: 22199, Reward: [-2155.707 -2155.707 -2155.707] [0.0000], Avg: [-824.937 -824.937 -824.937] (0.108)
Step: 22249, Reward: [-1928.875 -1928.875 -1928.875] [0.0000], Avg: [-827.417 -827.417 -827.417] (0.107)
Step: 22299, Reward: [-2123.591 -2123.591 -2123.591] [0.0000], Avg: [-830.323 -830.323 -830.323] (0.107)
Step: 22349, Reward: [-2142.154 -2142.154 -2142.154] [0.0000], Avg: [-833.258 -833.258 -833.258] (0.106)
Step: 22399, Reward: [-1663.067 -1663.067 -1663.067] [0.0000], Avg: [-835.11 -835.11 -835.11] (0.106)
Step: 22449, Reward: [-2076.186 -2076.186 -2076.186] [0.0000], Avg: [-837.875 -837.875 -837.875] (0.105)
Step: 22499, Reward: [-2098.764 -2098.764 -2098.764] [0.0000], Avg: [-840.677 -840.677 -840.677] (0.105)
Step: 22549, Reward: [-1950.474 -1950.474 -1950.474] [0.0000], Avg: [-843.137 -843.137 -843.137] (0.104)
Step: 22599, Reward: [-2257.641 -2257.641 -2257.641] [0.0000], Avg: [-846.267 -846.267 -846.267] (0.104)
Step: 22649, Reward: [-2290.175 -2290.175 -2290.175] [0.0000], Avg: [-849.454 -849.454 -849.454] (0.103)
Step: 22699, Reward: [-2205.629 -2205.629 -2205.629] [0.0000], Avg: [-852.441 -852.441 -852.441] (0.103)
Step: 22749, Reward: [-2076.158 -2076.158 -2076.158] [0.0000], Avg: [-855.131 -855.131 -855.131] (0.102)
Step: 22799, Reward: [-1987.31 -1987.31 -1987.31] [0.0000], Avg: [-857.614 -857.614 -857.614] (0.102)
Step: 22849, Reward: [-2457.07 -2457.07 -2457.07] [0.0000], Avg: [-861.114 -861.114 -861.114] (0.101)
Step: 22899, Reward: [-1857.811 -1857.811 -1857.811] [0.0000], Avg: [-863.29 -863.29 -863.29] (0.101)
Step: 22949, Reward: [-1952.542 -1952.542 -1952.542] [0.0000], Avg: [-865.663 -865.663 -865.663] (0.100)
Step: 22999, Reward: [-2089.506 -2089.506 -2089.506] [0.0000], Avg: [-868.323 -868.323 -868.323] (0.100)
Step: 23049, Reward: [-2342.165 -2342.165 -2342.165] [0.0000], Avg: [-871.52 -871.52 -871.52] (0.099)
Step: 23099, Reward: [-2130.587 -2130.587 -2130.587] [0.0000], Avg: [-874.246 -874.246 -874.246] (0.099)
Step: 23149, Reward: [-2424.787 -2424.787 -2424.787] [0.0000], Avg: [-877.595 -877.595 -877.595] (0.098)
Step: 23199, Reward: [-1743.609 -1743.609 -1743.609] [0.0000], Avg: [-879.461 -879.461 -879.461] (0.098)
Step: 23249, Reward: [-2093.906 -2093.906 -2093.906] [0.0000], Avg: [-882.073 -882.073 -882.073] (0.097)
Step: 23299, Reward: [-1989.498 -1989.498 -1989.498] [0.0000], Avg: [-884.449 -884.449 -884.449] (0.097)
Step: 23349, Reward: [-435.112 -435.112 -435.112] [0.0000], Avg: [-883.487 -883.487 -883.487] (0.096)
Step: 23399, Reward: [-623.085 -623.085 -623.085] [0.0000], Avg: [-882.931 -882.931 -882.931] (0.096)
Step: 23449, Reward: [-459.98 -459.98 -459.98] [0.0000], Avg: [-882.029 -882.029 -882.029] (0.095)
Step: 23499, Reward: [-751.382 -751.382 -751.382] [0.0000], Avg: [-881.751 -881.751 -881.751] (0.095)
Step: 23549, Reward: [-442.72 -442.72 -442.72] [0.0000], Avg: [-880.819 -880.819 -880.819] (0.094)
Step: 23599, Reward: [-468.402 -468.402 -468.402] [0.0000], Avg: [-879.945 -879.945 -879.945] (0.094)
Step: 23649, Reward: [-796.247 -796.247 -796.247] [0.0000], Avg: [-879.768 -879.768 -879.768] (0.093)
Step: 23699, Reward: [-520.368 -520.368 -520.368] [0.0000], Avg: [-879.01 -879.01 -879.01] (0.093)
Step: 23749, Reward: [-609.251 -609.251 -609.251] [0.0000], Avg: [-878.442 -878.442 -878.442] (0.092)
Step: 23799, Reward: [-811.985 -811.985 -811.985] [0.0000], Avg: [-878.302 -878.302 -878.302] (0.092)
Step: 23849, Reward: [-555.303 -555.303 -555.303] [0.0000], Avg: [-877.625 -877.625 -877.625] (0.092)
Step: 23899, Reward: [-607.09 -607.09 -607.09] [0.0000], Avg: [-877.059 -877.059 -877.059] (0.091)
Step: 23949, Reward: [-567.658 -567.658 -567.658] [0.0000], Avg: [-876.413 -876.413 -876.413] (0.091)
Step: 23999, Reward: [-751.667 -751.667 -751.667] [0.0000], Avg: [-876.153 -876.153 -876.153] (0.090)
Step: 24049, Reward: [-637.216 -637.216 -637.216] [0.0000], Avg: [-875.657 -875.657 -875.657] (0.090)
Step: 24099, Reward: [-540.774 -540.774 -540.774] [0.0000], Avg: [-874.962 -874.962 -874.962] (0.089)
Step: 24149, Reward: [-514.5 -514.5 -514.5] [0.0000], Avg: [-874.215 -874.215 -874.215] (0.089)
Step: 24199, Reward: [-570.945 -570.945 -570.945] [0.0000], Avg: [-873.589 -873.589 -873.589] (0.088)
Step: 24249, Reward: [-525.997 -525.997 -525.997] [0.0000], Avg: [-872.872 -872.872 -872.872] (0.088)
Step: 24299, Reward: [-951.691 -951.691 -951.691] [0.0000], Avg: [-873.034 -873.034 -873.034] (0.088)
Step: 24349, Reward: [-748.524 -748.524 -748.524] [0.0000], Avg: [-872.779 -872.779 -872.779] (0.087)
Step: 24399, Reward: [-1281.274 -1281.274 -1281.274] [0.0000], Avg: [-873.616 -873.616 -873.616] (0.087)
Step: 24449, Reward: [-787.714 -787.714 -787.714] [0.0000], Avg: [-873.44 -873.44 -873.44] (0.086)
Step: 24499, Reward: [-874.069 -874.069 -874.069] [0.0000], Avg: [-873.441 -873.441 -873.441] (0.086)
Step: 24549, Reward: [-742.51 -742.51 -742.51] [0.0000], Avg: [-873.175 -873.175 -873.175] (0.085)
Step: 24599, Reward: [-787.866 -787.866 -787.866] [0.0000], Avg: [-873.001 -873.001 -873.001] (0.085)
Step: 24649, Reward: [-558.605 -558.605 -558.605] [0.0000], Avg: [-872.364 -872.364 -872.364] (0.084)
Step: 24699, Reward: [-687.191 -687.191 -687.191] [0.0000], Avg: [-871.989 -871.989 -871.989] (0.084)
Step: 24749, Reward: [-586.9 -586.9 -586.9] [0.0000], Avg: [-871.413 -871.413 -871.413] (0.084)
Step: 24799, Reward: [-587.622 -587.622 -587.622] [0.0000], Avg: [-870.841 -870.841 -870.841] (0.083)
Step: 24849, Reward: [-901.119 -901.119 -901.119] [0.0000], Avg: [-870.902 -870.902 -870.902] (0.083)
Step: 24899, Reward: [-853.157 -853.157 -853.157] [0.0000], Avg: [-870.866 -870.866 -870.866] (0.082)
Step: 24949, Reward: [-613.342 -613.342 -613.342] [0.0000], Avg: [-870.35 -870.35 -870.35] (0.082)
Step: 24999, Reward: [-680.249 -680.249 -680.249] [0.0000], Avg: [-869.97 -869.97 -869.97] (0.082)
Step: 25049, Reward: [-698.468 -698.468 -698.468] [0.0000], Avg: [-869.627 -869.627 -869.627] (0.081)
Step: 25099, Reward: [-485.936 -485.936 -485.936] [0.0000], Avg: [-868.863 -868.863 -868.863] (0.081)
Step: 25149, Reward: [-582.024 -582.024 -582.024] [0.0000], Avg: [-868.293 -868.293 -868.293] (0.080)
Step: 25199, Reward: [-815.623 -815.623 -815.623] [0.0000], Avg: [-868.188 -868.188 -868.188] (0.080)
Step: 25249, Reward: [-698.824 -698.824 -698.824] [0.0000], Avg: [-867.853 -867.853 -867.853] (0.080)
Step: 25299, Reward: [-760.62 -760.62 -760.62] [0.0000], Avg: [-867.641 -867.641 -867.641] (0.079)
Step: 25349, Reward: [-660.453 -660.453 -660.453] [0.0000], Avg: [-867.232 -867.232 -867.232] (0.079)
Step: 25399, Reward: [-655.479 -655.479 -655.479] [0.0000], Avg: [-866.815 -866.815 -866.815] (0.078)
Step: 25449, Reward: [-906.844 -906.844 -906.844] [0.0000], Avg: [-866.894 -866.894 -866.894] (0.078)
Step: 25499, Reward: [-1008.586 -1008.586 -1008.586] [0.0000], Avg: [-867.172 -867.172 -867.172] (0.078)
Step: 25549, Reward: [-2043.353 -2043.353 -2043.353] [0.0000], Avg: [-869.474 -869.474 -869.474] (0.077)
Step: 25599, Reward: [-2063.484 -2063.484 -2063.484] [0.0000], Avg: [-871.806 -871.806 -871.806] (0.077)
Step: 25649, Reward: [-2420.411 -2420.411 -2420.411] [0.0000], Avg: [-874.824 -874.824 -874.824] (0.076)
Step: 25699, Reward: [-2164.09 -2164.09 -2164.09] [0.0000], Avg: [-877.333 -877.333 -877.333] (0.076)
Step: 25749, Reward: [-2050.9 -2050.9 -2050.9] [0.0000], Avg: [-879.611 -879.611 -879.611] (0.076)
Step: 25799, Reward: [-1905.447 -1905.447 -1905.447] [0.0000], Avg: [-881.6 -881.6 -881.6] (0.075)
Step: 25849, Reward: [-2127.588 -2127.588 -2127.588] [0.0000], Avg: [-884.01 -884.01 -884.01] (0.075)
Step: 25899, Reward: [-1705.221 -1705.221 -1705.221] [0.0000], Avg: [-885.595 -885.595 -885.595] (0.075)
Step: 25949, Reward: [-1947.414 -1947.414 -1947.414] [0.0000], Avg: [-887.641 -887.641 -887.641] (0.074)
Step: 25999, Reward: [-1885.593 -1885.593 -1885.593] [0.0000], Avg: [-889.56 -889.56 -889.56] (0.074)
Step: 26049, Reward: [-1913.326 -1913.326 -1913.326] [0.0000], Avg: [-891.525 -891.525 -891.525] (0.073)
Step: 26099, Reward: [-2203.689 -2203.689 -2203.689] [0.0000], Avg: [-894.039 -894.039 -894.039] (0.073)
Step: 26149, Reward: [-2257.392 -2257.392 -2257.392] [0.0000], Avg: [-896.645 -896.645 -896.645] (0.073)
Step: 26199, Reward: [-1761.346 -1761.346 -1761.346] [0.0000], Avg: [-898.296 -898.296 -898.296] (0.072)
Step: 26249, Reward: [-2135.187 -2135.187 -2135.187] [0.0000], Avg: [-900.652 -900.652 -900.652] (0.072)
Step: 26299, Reward: [-2048.534 -2048.534 -2048.534] [0.0000], Avg: [-902.834 -902.834 -902.834] (0.072)
Step: 26349, Reward: [-1847.595 -1847.595 -1847.595] [0.0000], Avg: [-904.627 -904.627 -904.627] (0.071)
Step: 26399, Reward: [-1867.343 -1867.343 -1867.343] [0.0000], Avg: [-906.45 -906.45 -906.45] (0.071)
Step: 26449, Reward: [-1806.352 -1806.352 -1806.352] [0.0000], Avg: [-908.151 -908.151 -908.151] (0.071)
Step: 26499, Reward: [-2115.777 -2115.777 -2115.777] [0.0000], Avg: [-910.43 -910.43 -910.43] (0.070)
Step: 26549, Reward: [-1953.208 -1953.208 -1953.208] [0.0000], Avg: [-912.393 -912.393 -912.393] (0.070)
Step: 26599, Reward: [-1892.423 -1892.423 -1892.423] [0.0000], Avg: [-914.236 -914.236 -914.236] (0.069)
Step: 26649, Reward: [-1976.724 -1976.724 -1976.724] [0.0000], Avg: [-916.229 -916.229 -916.229] (0.069)
Step: 26699, Reward: [-2078.903 -2078.903 -2078.903] [0.0000], Avg: [-918.406 -918.406 -918.406] (0.069)
Step: 26749, Reward: [-1811.946 -1811.946 -1811.946] [0.0000], Avg: [-920.077 -920.077 -920.077] (0.068)
Step: 26799, Reward: [-1972.467 -1972.467 -1972.467] [0.0000], Avg: [-922.04 -922.04 -922.04] (0.068)
Step: 26849, Reward: [-1980.007 -1980.007 -1980.007] [0.0000], Avg: [-924.01 -924.01 -924.01] (0.068)
Step: 26899, Reward: [-1955.033 -1955.033 -1955.033] [0.0000], Avg: [-925.926 -925.926 -925.926] (0.067)
Step: 26949, Reward: [-2034.96 -2034.96 -2034.96] [0.0000], Avg: [-927.984 -927.984 -927.984] (0.067)
Step: 26999, Reward: [-2287.998 -2287.998 -2287.998] [0.0000], Avg: [-930.503 -930.503 -930.503] (0.067)
Step: 27049, Reward: [-2197.696 -2197.696 -2197.696] [0.0000], Avg: [-932.845 -932.845 -932.845] (0.066)
Step: 27099, Reward: [-1904.998 -1904.998 -1904.998] [0.0000], Avg: [-934.639 -934.639 -934.639] (0.066)
Step: 27149, Reward: [-2287.022 -2287.022 -2287.022] [0.0000], Avg: [-937.129 -937.129 -937.129] (0.066)
Step: 27199, Reward: [-1879.78 -1879.78 -1879.78] [0.0000], Avg: [-938.862 -938.862 -938.862] (0.065)
Step: 27249, Reward: [-2285.073 -2285.073 -2285.073] [0.0000], Avg: [-941.332 -941.332 -941.332] (0.065)
Step: 27299, Reward: [-2072.388 -2072.388 -2072.388] [0.0000], Avg: [-943.404 -943.404 -943.404] (0.065)
Step: 27349, Reward: [-1849.496 -1849.496 -1849.496] [0.0000], Avg: [-945.06 -945.06 -945.06] (0.064)
Step: 27399, Reward: [-2149.453 -2149.453 -2149.453] [0.0000], Avg: [-947.258 -947.258 -947.258] (0.064)
Step: 27449, Reward: [-2013.529 -2013.529 -2013.529] [0.0000], Avg: [-949.2 -949.2 -949.2] (0.064)
Step: 27499, Reward: [-2370.233 -2370.233 -2370.233] [0.0000], Avg: [-951.784 -951.784 -951.784] (0.063)
Step: 27549, Reward: [-1873.851 -1873.851 -1873.851] [0.0000], Avg: [-953.457 -953.457 -953.457] (0.063)
Step: 27599, Reward: [-2382.168 -2382.168 -2382.168] [0.0000], Avg: [-956.045 -956.045 -956.045] (0.063)
Step: 27649, Reward: [-1784.794 -1784.794 -1784.794] [0.0000], Avg: [-957.544 -957.544 -957.544] (0.063)
Step: 27699, Reward: [-1899.061 -1899.061 -1899.061] [0.0000], Avg: [-959.244 -959.244 -959.244] (0.062)
Step: 27749, Reward: [-2126.144 -2126.144 -2126.144] [0.0000], Avg: [-961.346 -961.346 -961.346] (0.062)
Step: 27799, Reward: [-2036.598 -2036.598 -2036.598] [0.0000], Avg: [-963.28 -963.28 -963.28] (0.062)
Step: 27849, Reward: [-2138.687 -2138.687 -2138.687] [0.0000], Avg: [-965.39 -965.39 -965.39] (0.061)
Step: 27899, Reward: [-1882.956 -1882.956 -1882.956] [0.0000], Avg: [-967.035 -967.035 -967.035] (0.061)
Step: 27949, Reward: [-2151.484 -2151.484 -2151.484] [0.0000], Avg: [-969.154 -969.154 -969.154] (0.061)
Step: 27999, Reward: [-2120.665 -2120.665 -2120.665] [0.0000], Avg: [-971.21 -971.21 -971.21] (0.060)
Step: 28049, Reward: [-2121.847 -2121.847 -2121.847] [0.0000], Avg: [-973.261 -973.261 -973.261] (0.060)
Step: 28099, Reward: [-1727.664 -1727.664 -1727.664] [0.0000], Avg: [-974.603 -974.603 -974.603] (0.060)
Step: 28149, Reward: [-1703.276 -1703.276 -1703.276] [0.0000], Avg: [-975.897 -975.897 -975.897] (0.059)
Step: 28199, Reward: [-1764.922 -1764.922 -1764.922] [0.0000], Avg: [-977.296 -977.296 -977.296] (0.059)
Step: 28249, Reward: [-1330.428 -1330.428 -1330.428] [0.0000], Avg: [-977.921 -977.921 -977.921] (0.059)
Step: 28299, Reward: [-1724.374 -1724.374 -1724.374] [0.0000], Avg: [-979.24 -979.24 -979.24] (0.059)
Step: 28349, Reward: [-1859.461 -1859.461 -1859.461] [0.0000], Avg: [-980.793 -980.793 -980.793] (0.058)
Step: 28399, Reward: [-1667.378 -1667.378 -1667.378] [0.0000], Avg: [-982.001 -982.001 -982.001] (0.058)
Step: 28449, Reward: [-2122.484 -2122.484 -2122.484] [0.0000], Avg: [-984.006 -984.006 -984.006] (0.058)
Step: 28499, Reward: [-2168.186 -2168.186 -2168.186] [0.0000], Avg: [-986.083 -986.083 -986.083] (0.057)
Step: 28549, Reward: [-2203.621 -2203.621 -2203.621] [0.0000], Avg: [-988.216 -988.216 -988.216] (0.057)
Step: 28599, Reward: [-1500.275 -1500.275 -1500.275] [0.0000], Avg: [-989.111 -989.111 -989.111] (0.057)
Step: 28649, Reward: [-1919.459 -1919.459 -1919.459] [0.0000], Avg: [-990.734 -990.734 -990.734] (0.057)
Step: 28699, Reward: [-1780.611 -1780.611 -1780.611] [0.0000], Avg: [-992.111 -992.111 -992.111] (0.056)
Step: 28749, Reward: [-1182.999 -1182.999 -1182.999] [0.0000], Avg: [-992.443 -992.443 -992.443] (0.056)
Step: 28799, Reward: [-1929.425 -1929.425 -1929.425] [0.0000], Avg: [-994.069 -994.069 -994.069] (0.056)
Step: 28849, Reward: [-1668.647 -1668.647 -1668.647] [0.0000], Avg: [-995.238 -995.238 -995.238] (0.055)
Step: 28899, Reward: [-2071.868 -2071.868 -2071.868] [0.0000], Avg: [-997.101 -997.101 -997.101] (0.055)
Step: 28949, Reward: [-1762.516 -1762.516 -1762.516] [0.0000], Avg: [-998.423 -998.423 -998.423] (0.055)
Step: 28999, Reward: [-1362.727 -1362.727 -1362.727] [0.0000], Avg: [-999.051 -999.051 -999.051] (0.055)
Step: 29049, Reward: [-2156.639 -2156.639 -2156.639] [0.0000], Avg: [-1001.044 -1001.044 -1001.044] (0.054)
Step: 29099, Reward: [-2243.237 -2243.237 -2243.237] [0.0000], Avg: [-1003.178 -1003.178 -1003.178] (0.054)
Step: 29149, Reward: [-2010.041 -2010.041 -2010.041] [0.0000], Avg: [-1004.905 -1004.905 -1004.905] (0.054)
Step: 29199, Reward: [-1807.313 -1807.313 -1807.313] [0.0000], Avg: [-1006.279 -1006.279 -1006.279] (0.054)
Step: 29249, Reward: [-2029.732 -2029.732 -2029.732] [0.0000], Avg: [-1008.028 -1008.028 -1008.028] (0.053)
Step: 29299, Reward: [-1499.417 -1499.417 -1499.417] [0.0000], Avg: [-1008.867 -1008.867 -1008.867] (0.053)
Step: 29349, Reward: [-2265.697 -2265.697 -2265.697] [0.0000], Avg: [-1011.008 -1011.008 -1011.008] (0.053)
Step: 29399, Reward: [-1906.579 -1906.579 -1906.579] [0.0000], Avg: [-1012.531 -1012.531 -1012.531] (0.052)
Step: 29449, Reward: [-2305.965 -2305.965 -2305.965] [0.0000], Avg: [-1014.727 -1014.727 -1014.727] (0.052)
Step: 29499, Reward: [-1837.237 -1837.237 -1837.237] [0.0000], Avg: [-1016.121 -1016.121 -1016.121] (0.052)
Step: 29549, Reward: [-2205.795 -2205.795 -2205.795] [0.0000], Avg: [-1018.134 -1018.134 -1018.134] (0.052)
Step: 29599, Reward: [-2307.682 -2307.682 -2307.682] [0.0000], Avg: [-1020.312 -1020.312 -1020.312] (0.051)
Step: 29649, Reward: [-1916.27 -1916.27 -1916.27] [0.0000], Avg: [-1021.823 -1021.823 -1021.823] (0.051)
Step: 29699, Reward: [-2196.267 -2196.267 -2196.267] [0.0000], Avg: [-1023.801 -1023.801 -1023.801] (0.051)
Step: 29749, Reward: [-2200.679 -2200.679 -2200.679] [0.0000], Avg: [-1025.778 -1025.778 -1025.778] (0.051)
Step: 29799, Reward: [-2077.043 -2077.043 -2077.043] [0.0000], Avg: [-1027.542 -1027.542 -1027.542] (0.050)
Step: 29849, Reward: [-1613.786 -1613.786 -1613.786] [0.0000], Avg: [-1028.524 -1028.524 -1028.524] (0.050)
Step: 29899, Reward: [-1936.483 -1936.483 -1936.483] [0.0000], Avg: [-1030.043 -1030.043 -1030.043] (0.050)
Step: 29949, Reward: [-1884.446 -1884.446 -1884.446] [0.0000], Avg: [-1031.469 -1031.469 -1031.469] (0.050)
Step: 29999, Reward: [-2209.767 -2209.767 -2209.767] [0.0000], Avg: [-1033.433 -1033.433 -1033.433] (0.049)
Step: 30049, Reward: [-1751.709 -1751.709 -1751.709] [0.0000], Avg: [-1034.628 -1034.628 -1034.628] (0.049)
Step: 30099, Reward: [-2033.418 -2033.418 -2033.418] [0.0000], Avg: [-1036.287 -1036.287 -1036.287] (0.049)
Step: 30149, Reward: [-2015.435 -2015.435 -2015.435] [0.0000], Avg: [-1037.911 -1037.911 -1037.911] (0.049)
Step: 30199, Reward: [-1760.035 -1760.035 -1760.035] [0.0000], Avg: [-1039.106 -1039.106 -1039.106] (0.048)
Step: 30249, Reward: [-1979.046 -1979.046 -1979.046] [0.0000], Avg: [-1040.66 -1040.66 -1040.66] (0.048)
Step: 30299, Reward: [-2097.549 -2097.549 -2097.549] [0.0000], Avg: [-1042.404 -1042.404 -1042.404] (0.048)
Step: 30349, Reward: [-2050.081 -2050.081 -2050.081] [0.0000], Avg: [-1044.064 -1044.064 -1044.064] (0.048)
Step: 30399, Reward: [-1919.788 -1919.788 -1919.788] [0.0000], Avg: [-1045.505 -1045.505 -1045.505] (0.047)
Step: 30449, Reward: [-1837.604 -1837.604 -1837.604] [0.0000], Avg: [-1046.805 -1046.805 -1046.805] (0.047)
Step: 30499, Reward: [-2143.146 -2143.146 -2143.146] [0.0000], Avg: [-1048.603 -1048.603 -1048.603] (0.047)
Step: 30549, Reward: [-1986.236 -1986.236 -1986.236] [0.0000], Avg: [-1050.137 -1050.137 -1050.137] (0.047)
Step: 30599, Reward: [-1890.398 -1890.398 -1890.398] [0.0000], Avg: [-1051.51 -1051.51 -1051.51] (0.047)
Step: 30649, Reward: [-2289.871 -2289.871 -2289.871] [0.0000], Avg: [-1053.53 -1053.53 -1053.53] (0.046)
Step: 30699, Reward: [-2005.55 -2005.55 -2005.55] [0.0000], Avg: [-1055.081 -1055.081 -1055.081] (0.046)
Step: 30749, Reward: [-1835.821 -1835.821 -1835.821] [0.0000], Avg: [-1056.35 -1056.35 -1056.35] (0.046)
Step: 30799, Reward: [-1903.937 -1903.937 -1903.937] [0.0000], Avg: [-1057.726 -1057.726 -1057.726] (0.046)
Step: 30849, Reward: [-1734.835 -1734.835 -1734.835] [0.0000], Avg: [-1058.824 -1058.824 -1058.824] (0.045)
Step: 30899, Reward: [-1917.6 -1917.6 -1917.6] [0.0000], Avg: [-1060.213 -1060.213 -1060.213] (0.045)
Step: 30949, Reward: [-2054.221 -2054.221 -2054.221] [0.0000], Avg: [-1061.819 -1061.819 -1061.819] (0.045)
Step: 30999, Reward: [-2212.662 -2212.662 -2212.662] [0.0000], Avg: [-1063.675 -1063.675 -1063.675] (0.045)
Step: 31049, Reward: [-2094.991 -2094.991 -2094.991] [0.0000], Avg: [-1065.336 -1065.336 -1065.336] (0.044)
Step: 31099, Reward: [-1929.763 -1929.763 -1929.763] [0.0000], Avg: [-1066.726 -1066.726 -1066.726] (0.044)
Step: 31149, Reward: [-1811.999 -1811.999 -1811.999] [0.0000], Avg: [-1067.922 -1067.922 -1067.922] (0.044)
Step: 31199, Reward: [-2106.56 -2106.56 -2106.56] [0.0000], Avg: [-1069.586 -1069.586 -1069.586] (0.044)
Step: 31249, Reward: [-1380.273 -1380.273 -1380.273] [0.0000], Avg: [-1070.084 -1070.084 -1070.084] (0.044)
Step: 31299, Reward: [-1863.693 -1863.693 -1863.693] [0.0000], Avg: [-1071.351 -1071.351 -1071.351] (0.043)
Step: 31349, Reward: [-1726.451 -1726.451 -1726.451] [0.0000], Avg: [-1072.396 -1072.396 -1072.396] (0.043)
Step: 31399, Reward: [-1683.868 -1683.868 -1683.868] [0.0000], Avg: [-1073.37 -1073.37 -1073.37] (0.043)
Step: 31449, Reward: [-1946.947 -1946.947 -1946.947] [0.0000], Avg: [-1074.759 -1074.759 -1074.759] (0.043)
Step: 31499, Reward: [-1963.306 -1963.306 -1963.306] [0.0000], Avg: [-1076.169 -1076.169 -1076.169] (0.043)
Step: 31549, Reward: [-1950.948 -1950.948 -1950.948] [0.0000], Avg: [-1077.555 -1077.555 -1077.555] (0.042)
Step: 31599, Reward: [-1606.716 -1606.716 -1606.716] [0.0000], Avg: [-1078.393 -1078.393 -1078.393] (0.042)
Step: 31649, Reward: [-1592.908 -1592.908 -1592.908] [0.0000], Avg: [-1079.206 -1079.206 -1079.206] (0.042)
Step: 31699, Reward: [-1891.26 -1891.26 -1891.26] [0.0000], Avg: [-1080.486 -1080.486 -1080.486] (0.042)
Step: 31749, Reward: [-1778.439 -1778.439 -1778.439] [0.0000], Avg: [-1081.585 -1081.585 -1081.585] (0.041)
Step: 31799, Reward: [-1960.757 -1960.757 -1960.757] [0.0000], Avg: [-1082.968 -1082.968 -1082.968] (0.041)
Step: 31849, Reward: [-1556.734 -1556.734 -1556.734] [0.0000], Avg: [-1083.712 -1083.712 -1083.712] (0.041)
Step: 31899, Reward: [-1924.36 -1924.36 -1924.36] [0.0000], Avg: [-1085.029 -1085.029 -1085.029] (0.041)
Step: 31949, Reward: [-1880.408 -1880.408 -1880.408] [0.0000], Avg: [-1086.274 -1086.274 -1086.274] (0.041)
Step: 31999, Reward: [-1686.526 -1686.526 -1686.526] [0.0000], Avg: [-1087.212 -1087.212 -1087.212] (0.040)
Step: 32049, Reward: [-1493.06 -1493.06 -1493.06] [0.0000], Avg: [-1087.845 -1087.845 -1087.845] (0.040)
Step: 32099, Reward: [-1768.223 -1768.223 -1768.223] [0.0000], Avg: [-1088.905 -1088.905 -1088.905] (0.040)
Step: 32149, Reward: [-1747.535 -1747.535 -1747.535] [0.0000], Avg: [-1089.929 -1089.929 -1089.929] (0.040)
Step: 32199, Reward: [-1763.371 -1763.371 -1763.371] [0.0000], Avg: [-1090.975 -1090.975 -1090.975] (0.040)
Step: 32249, Reward: [-1585.079 -1585.079 -1585.079] [0.0000], Avg: [-1091.741 -1091.741 -1091.741] (0.039)
Step: 32299, Reward: [-1756.988 -1756.988 -1756.988] [0.0000], Avg: [-1092.771 -1092.771 -1092.771] (0.039)
Step: 32349, Reward: [-1946.96 -1946.96 -1946.96] [0.0000], Avg: [-1094.091 -1094.091 -1094.091] (0.039)
