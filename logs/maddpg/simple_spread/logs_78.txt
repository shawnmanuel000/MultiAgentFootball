Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7f64590691d0>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7f6459069278>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7f64590692e8>],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)
		self.agent = MADDPG(state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.agent.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.t = 0 if not hasattr(self, "t") else self.t + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.agent.get_action_probs(next_state))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.agent.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1], gamma=0.95)[0].squeeze(-1) for q_value,reward,done in zip(q_values, rewards, dones)]

			states, actions = map(lambda items: [x[:-1] for x in items], [states, actions])
			states, actions, q_targets = map(lambda items: [x.view(-1, *x.shape[2:]).cpu().numpy() for x in items], [states, actions, q_targets])
			self.replay_buffer.push(states, actions, q_targets)

		if (len(self.replay_buffer) >= REPLAY_BATCH_SIZE and (self.t % 100)==0):
			states, actions, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.agent.update(states, actions, q_targets)
		"""
			# self.buffer.append((state, action, reward, done))
			# if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			# 	states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			# 	self.buffer.clear()
			# 	next_state = self.to_tensor(next_state)
			# 	states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			# 	actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			# 	states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			# 	actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			# 	q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			# 	q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
				
			# 	to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			# 	states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			# 	states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			# 	self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
			# if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
			# 	states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			# 	self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
			# if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)
		"""

class MADDPG(PTNetwork):
	def __init__(self, state_size, action_size, gamma=0.95, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.gamma = gamma
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.agents = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.agents)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.agents]
			return q_value

	def update(self, states, actions, q_targets):
		for agent_i, curr_agent in enumerate(self.agents):
			target_value = q_targets[agent_i]
			# next_actions = [one_hot(agent.get_action(nobs, numpy=False)) for agent, nobs in zip(self.agents, next_states)]
			# next_states_joint = torch.cat([*next_states], dim=-1)
			# next_actions_joint = torch.cat([*next_actions], dim=-1)
			# target_value = (rewards[agent_i].view(-1, 1, 1) + self.gamma * curr_agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False) * (1 - dones[agent_i].view(-1, 1, 1)))

			states_joint = torch.cat([*states], dim=-1)
			actions_joint = torch.cat([*actions], dim=-1)
			actual_value = curr_agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			vf_loss = (actual_value - target_value.detach()).pow(2).mean()
			curr_agent.step(curr_agent.critic_optimizer, vf_loss, param_norm=curr_agent.critic_local.parameters())
			curr_agent.soft_copy(curr_agent.critic_local, curr_agent.critic_target)

			curr_pol_out = curr_agent.get_action(states[agent_i], grad=True, numpy=False)
			curr_pol_vf_in = gsoftmax(curr_pol_out, hard=True)
			action = [curr_pol_vf_in if i==agent_i else one_hot(agent.get_action(ob, numpy=False)) for (i,agent), ob in zip(enumerate(self.agents), states)]
			action_joint = torch.cat([*action], dim=-1)
			pol_loss = -curr_agent.critic_local(states_joint, action_joint).mean() + 0.001*curr_pol_out.pow(2).mean() 
			curr_agent.step(curr_agent.actor_optimizer, pol_loss, param_norm=curr_agent.actor_local.parameters())
			curr_agent.soft_copy(curr_agent.actor_local, curr_agent.actor_target)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-479.96 -479.96 -479.96] [0.0000], Avg: [-479.96 -479.96 -479.96] (1.000)
Step: 99, Reward: [-800.029 -800.029 -800.029] [0.0000], Avg: [-639.994 -639.994 -639.994] (1.000)
Step: 149, Reward: [-614.395 -614.395 -614.395] [0.0000], Avg: [-631.461 -631.461 -631.461] (1.000)
Step: 199, Reward: [-624.1 -624.1 -624.1] [0.0000], Avg: [-629.621 -629.621 -629.621] (1.000)
Step: 249, Reward: [-566.794 -566.794 -566.794] [0.0000], Avg: [-617.056 -617.056 -617.056] (1.000)
Step: 299, Reward: [-441.567 -441.567 -441.567] [0.0000], Avg: [-587.808 -587.808 -587.808] (1.000)
Step: 349, Reward: [-521.519 -521.519 -521.519] [0.0000], Avg: [-578.338 -578.338 -578.338] (1.000)
Step: 399, Reward: [-633.664 -633.664 -633.664] [0.0000], Avg: [-585.253 -585.253 -585.253] (1.000)
Step: 449, Reward: [-465.866 -465.866 -465.866] [0.0000], Avg: [-571.988 -571.988 -571.988] (1.000)
Step: 499, Reward: [-490.627 -490.627 -490.627] [0.0000], Avg: [-563.852 -563.852 -563.852] (1.000)
Step: 549, Reward: [-525.61 -525.61 -525.61] [0.0000], Avg: [-560.375 -560.375 -560.375] (1.000)
Step: 599, Reward: [-522.477 -522.477 -522.477] [0.0000], Avg: [-557.217 -557.217 -557.217] (1.000)
Step: 649, Reward: [-471.516 -471.516 -471.516] [0.0000], Avg: [-550.625 -550.625 -550.625] (1.000)
Step: 699, Reward: [-436.825 -436.825 -436.825] [0.0000], Avg: [-542.496 -542.496 -542.496] (1.000)
Step: 749, Reward: [-471.932 -471.932 -471.932] [0.0000], Avg: [-537.792 -537.792 -537.792] (1.000)
Step: 799, Reward: [-418.952 -418.952 -418.952] [0.0000], Avg: [-530.365 -530.365 -530.365] (1.000)
Step: 849, Reward: [-458.276 -458.276 -458.276] [0.0000], Avg: [-526.124 -526.124 -526.124] (1.000)
Step: 899, Reward: [-436.907 -436.907 -436.907] [0.0000], Avg: [-521.168 -521.168 -521.168] (1.000)
Step: 949, Reward: [-696.28 -696.28 -696.28] [0.0000], Avg: [-530.384 -530.384 -530.384] (1.000)
Step: 999, Reward: [-446.623 -446.623 -446.623] [0.0000], Avg: [-526.196 -526.196 -526.196] (1.000)
Step: 1049, Reward: [-346.197 -346.197 -346.197] [0.0000], Avg: [-517.625 -517.625 -517.625] (1.000)
Step: 1099, Reward: [-521.907 -521.907 -521.907] [0.0000], Avg: [-517.819 -517.819 -517.819] (1.000)
Step: 1149, Reward: [-508.523 -508.523 -508.523] [0.0000], Avg: [-517.415 -517.415 -517.415] (1.000)
Step: 1199, Reward: [-567.083 -567.083 -567.083] [0.0000], Avg: [-519.485 -519.485 -519.485] (1.000)
Step: 1249, Reward: [-442.716 -442.716 -442.716] [0.0000], Avg: [-516.414 -516.414 -516.414] (1.000)
Step: 1299, Reward: [-670.347 -670.347 -670.347] [0.0000], Avg: [-522.334 -522.334 -522.334] (1.000)
Step: 1349, Reward: [-655.768 -655.768 -655.768] [0.0000], Avg: [-527.276 -527.276 -527.276] (1.000)
Step: 1399, Reward: [-592.61 -592.61 -592.61] [0.0000], Avg: [-529.61 -529.61 -529.61] (1.000)
Step: 1449, Reward: [-631.318 -631.318 -631.318] [0.0000], Avg: [-533.117 -533.117 -533.117] (1.000)
Step: 1499, Reward: [-1837.026 -1837.026 -1837.026] [0.0000], Avg: [-576.58 -576.58 -576.58] (1.000)
Step: 1549, Reward: [-1571.562 -1571.562 -1571.562] [0.0000], Avg: [-608.677 -608.677 -608.677] (1.000)
Step: 1599, Reward: [-1985.423 -1985.423 -1985.423] [0.0000], Avg: [-651.7 -651.7 -651.7] (1.000)
Step: 1649, Reward: [-1704.283 -1704.283 -1704.283] [0.0000], Avg: [-683.596 -683.596 -683.596] (1.000)
Step: 1699, Reward: [-1836.404 -1836.404 -1836.404] [0.0000], Avg: [-717.503 -717.503 -717.503] (1.000)
Step: 1749, Reward: [-1703.912 -1703.912 -1703.912] [0.0000], Avg: [-745.686 -745.686 -745.686] (1.000)
Step: 1799, Reward: [-1746.836 -1746.836 -1746.836] [0.0000], Avg: [-773.495 -773.495 -773.495] (1.000)
Step: 1849, Reward: [-1956.12 -1956.12 -1956.12] [0.0000], Avg: [-805.458 -805.458 -805.458] (1.000)
Step: 1899, Reward: [-1931.022 -1931.022 -1931.022] [0.0000], Avg: [-835.078 -835.078 -835.078] (1.000)
Step: 1949, Reward: [-2298.134 -2298.134 -2298.134] [0.0000], Avg: [-872.593 -872.593 -872.593] (1.000)
Step: 1999, Reward: [-2046.604 -2046.604 -2046.604] [0.0000], Avg: [-901.943 -901.943 -901.943] (1.000)
Step: 2049, Reward: [-1819.354 -1819.354 -1819.354] [0.0000], Avg: [-924.319 -924.319 -924.319] (1.000)
Step: 2099, Reward: [-1509.158 -1509.158 -1509.158] [0.0000], Avg: [-938.243 -938.243 -938.243] (1.000)
Step: 2149, Reward: [-1937.813 -1937.813 -1937.813] [0.0000], Avg: [-961.489 -961.489 -961.489] (1.000)
Step: 2199, Reward: [-1959.25 -1959.25 -1959.25] [0.0000], Avg: [-984.166 -984.166 -984.166] (1.000)
Step: 2249, Reward: [-1775.908 -1775.908 -1775.908] [0.0000], Avg: [-1001.76 -1001.76 -1001.76] (1.000)
Step: 2299, Reward: [-1659.29 -1659.29 -1659.29] [0.0000], Avg: [-1016.054 -1016.054 -1016.054] (1.000)
Step: 2349, Reward: [-1747.798 -1747.798 -1747.798] [0.0000], Avg: [-1031.623 -1031.623 -1031.623] (1.000)
Step: 2399, Reward: [-1660.642 -1660.642 -1660.642] [0.0000], Avg: [-1044.728 -1044.728 -1044.728] (1.000)
Step: 2449, Reward: [-1379.455 -1379.455 -1379.455] [0.0000], Avg: [-1051.559 -1051.559 -1051.559] (1.000)
Step: 2499, Reward: [-1718.68 -1718.68 -1718.68] [0.0000], Avg: [-1064.901 -1064.901 -1064.901] (1.000)
Step: 2549, Reward: [-1657.661 -1657.661 -1657.661] [0.0000], Avg: [-1076.524 -1076.524 -1076.524] (1.000)
Step: 2599, Reward: [-2112.7 -2112.7 -2112.7] [0.0000], Avg: [-1096.45 -1096.45 -1096.45] (1.000)
Step: 2649, Reward: [-1834.349 -1834.349 -1834.349] [0.0000], Avg: [-1110.373 -1110.373 -1110.373] (1.000)
Step: 2699, Reward: [-1933.351 -1933.351 -1933.351] [0.0000], Avg: [-1125.613 -1125.613 -1125.613] (1.000)
Step: 2749, Reward: [-1717.049 -1717.049 -1717.049] [0.0000], Avg: [-1136.367 -1136.367 -1136.367] (1.000)
Step: 2799, Reward: [-2241.864 -2241.864 -2241.864] [0.0000], Avg: [-1156.108 -1156.108 -1156.108] (1.000)
Step: 2849, Reward: [-1967.138 -1967.138 -1967.138] [0.0000], Avg: [-1170.336 -1170.336 -1170.336] (1.000)
Step: 2899, Reward: [-2334.167 -2334.167 -2334.167] [0.0000], Avg: [-1190.402 -1190.402 -1190.402] (1.000)
Step: 2949, Reward: [-1775.906 -1775.906 -1775.906] [0.0000], Avg: [-1200.326 -1200.326 -1200.326] (1.000)
Step: 2999, Reward: [-1249.292 -1249.292 -1249.292] [0.0000], Avg: [-1201.142 -1201.142 -1201.142] (1.000)
Step: 3049, Reward: [-1817.692 -1817.692 -1817.692] [0.0000], Avg: [-1211.25 -1211.25 -1211.25] (1.000)
Step: 3099, Reward: [-1660.442 -1660.442 -1660.442] [0.0000], Avg: [-1218.495 -1218.495 -1218.495] (1.000)
Step: 3149, Reward: [-786.239 -786.239 -786.239] [0.0000], Avg: [-1211.633 -1211.633 -1211.633] (1.000)
Step: 3199, Reward: [-1872.329 -1872.329 -1872.329] [0.0000], Avg: [-1221.957 -1221.957 -1221.957] (1.000)
Step: 3249, Reward: [-932.698 -932.698 -932.698] [0.0000], Avg: [-1217.507 -1217.507 -1217.507] (1.000)
Step: 3299, Reward: [-913.086 -913.086 -913.086] [0.0000], Avg: [-1212.894 -1212.894 -1212.894] (1.000)
Step: 3349, Reward: [-1615.625 -1615.625 -1615.625] [0.0000], Avg: [-1218.905 -1218.905 -1218.905] (1.000)
Step: 3399, Reward: [-555.405 -555.405 -555.405] [0.0000], Avg: [-1209.148 -1209.148 -1209.148] (1.000)
Step: 3449, Reward: [-877.168 -877.168 -877.168] [0.0000], Avg: [-1204.337 -1204.337 -1204.337] (1.000)
Step: 3499, Reward: [-1013.126 -1013.126 -1013.126] [0.0000], Avg: [-1201.605 -1201.605 -1201.605] (1.000)
Step: 3549, Reward: [-740.063 -740.063 -740.063] [0.0000], Avg: [-1195.104 -1195.104 -1195.104] (1.000)
Step: 3599, Reward: [-911.573 -911.573 -911.573] [0.0000], Avg: [-1191.166 -1191.166 -1191.166] (1.000)
Step: 3649, Reward: [-854.379 -854.379 -854.379] [0.0000], Avg: [-1186.553 -1186.553 -1186.553] (1.000)
Step: 3699, Reward: [-680.907 -680.907 -680.907] [0.0000], Avg: [-1179.72 -1179.72 -1179.72] (1.000)
Step: 3749, Reward: [-527.365 -527.365 -527.365] [0.0000], Avg: [-1171.022 -1171.022 -1171.022] (1.000)
Step: 3799, Reward: [-742.124 -742.124 -742.124] [0.0000], Avg: [-1165.378 -1165.378 -1165.378] (1.000)
Step: 3849, Reward: [-406.016 -406.016 -406.016] [0.0000], Avg: [-1155.517 -1155.517 -1155.517] (1.000)
Step: 3899, Reward: [-861.601 -861.601 -861.601] [0.0000], Avg: [-1151.748 -1151.748 -1151.748] (1.000)
Step: 3949, Reward: [-525.888 -525.888 -525.888] [0.0000], Avg: [-1143.826 -1143.826 -1143.826] (1.000)
Step: 3999, Reward: [-404.044 -404.044 -404.044] [0.0000], Avg: [-1134.579 -1134.579 -1134.579] (1.000)
Step: 4049, Reward: [-501.909 -501.909 -501.909] [0.0000], Avg: [-1126.768 -1126.768 -1126.768] (1.000)
Step: 4099, Reward: [-911.598 -911.598 -911.598] [0.0000], Avg: [-1124.144 -1124.144 -1124.144] (1.000)
Step: 4149, Reward: [-740.04 -740.04 -740.04] [0.0000], Avg: [-1119.516 -1119.516 -1119.516] (1.000)
Step: 4199, Reward: [-509.384 -509.384 -509.384] [0.0000], Avg: [-1112.253 -1112.253 -1112.253] (1.000)
Step: 4249, Reward: [-410.087 -410.087 -410.087] [0.0000], Avg: [-1103.992 -1103.992 -1103.992] (1.000)
Step: 4299, Reward: [-538.374 -538.374 -538.374] [0.0000], Avg: [-1097.415 -1097.415 -1097.415] (1.000)
Step: 4349, Reward: [-507.278 -507.278 -507.278] [0.0000], Avg: [-1090.632 -1090.632 -1090.632] (1.000)
Step: 4399, Reward: [-561.236 -561.236 -561.236] [0.0000], Avg: [-1084.616 -1084.616 -1084.616] (1.000)
Step: 4449, Reward: [-402.909 -402.909 -402.909] [0.0000], Avg: [-1076.956 -1076.956 -1076.956] (1.000)
Step: 4499, Reward: [-543.796 -543.796 -543.796] [0.0000], Avg: [-1071.032 -1071.032 -1071.032] (1.000)
Step: 4549, Reward: [-651.885 -651.885 -651.885] [0.0000], Avg: [-1066.426 -1066.426 -1066.426] (1.000)
Step: 4599, Reward: [-596.45 -596.45 -596.45] [0.0000], Avg: [-1061.318 -1061.318 -1061.318] (1.000)
Step: 4649, Reward: [-524.578 -524.578 -524.578] [0.0000], Avg: [-1055.547 -1055.547 -1055.547] (1.000)
Step: 4699, Reward: [-938.987 -938.987 -938.987] [0.0000], Avg: [-1054.307 -1054.307 -1054.307] (1.000)
Step: 4749, Reward: [-749.048 -749.048 -749.048] [0.0000], Avg: [-1051.093 -1051.093 -1051.093] (1.000)
Step: 4799, Reward: [-355.784 -355.784 -355.784] [0.0000], Avg: [-1043.851 -1043.851 -1043.851] (1.000)
Step: 4849, Reward: [-618.619 -618.619 -618.619] [0.0000], Avg: [-1039.467 -1039.467 -1039.467] (1.000)
Step: 4899, Reward: [-439.302 -439.302 -439.302] [0.0000], Avg: [-1033.343 -1033.343 -1033.343] (1.000)
Step: 4949, Reward: [-791.602 -791.602 -791.602] [0.0000], Avg: [-1030.901 -1030.901 -1030.901] (1.000)
Step: 4999, Reward: [-1116.001 -1116.001 -1116.001] [0.0000], Avg: [-1031.752 -1031.752 -1031.752] (1.000)
Step: 5049, Reward: [-637.398 -637.398 -637.398] [0.0000], Avg: [-1027.847 -1027.847 -1027.847] (1.000)
Step: 5099, Reward: [-583.001 -583.001 -583.001] [0.0000], Avg: [-1023.486 -1023.486 -1023.486] (1.000)
Step: 5149, Reward: [-563.373 -563.373 -563.373] [0.0000], Avg: [-1019.019 -1019.019 -1019.019] (1.000)
Step: 5199, Reward: [-1195.875 -1195.875 -1195.875] [0.0000], Avg: [-1020.719 -1020.719 -1020.719] (1.000)
Step: 5249, Reward: [-626.022 -626.022 -626.022] [0.0000], Avg: [-1016.96 -1016.96 -1016.96] (1.000)
Step: 5299, Reward: [-1360.647 -1360.647 -1360.647] [0.0000], Avg: [-1020.203 -1020.203 -1020.203] (1.000)
Step: 5349, Reward: [-645.198 -645.198 -645.198] [0.0000], Avg: [-1016.698 -1016.698 -1016.698] (1.000)
Step: 5399, Reward: [-668.18 -668.18 -668.18] [0.0000], Avg: [-1013.471 -1013.471 -1013.471] (1.000)
Step: 5449, Reward: [-998.717 -998.717 -998.717] [0.0000], Avg: [-1013.336 -1013.336 -1013.336] (1.000)
Step: 5499, Reward: [-697.702 -697.702 -697.702] [0.0000], Avg: [-1010.466 -1010.466 -1010.466] (1.000)
Step: 5549, Reward: [-677.274 -677.274 -677.274] [0.0000], Avg: [-1007.465 -1007.465 -1007.465] (1.000)
Step: 5599, Reward: [-520.984 -520.984 -520.984] [0.0000], Avg: [-1003.121 -1003.121 -1003.121] (1.000)
Step: 5649, Reward: [-667.112 -667.112 -667.112] [0.0000], Avg: [-1000.147 -1000.147 -1000.147] (1.000)
Step: 5699, Reward: [-798.809 -798.809 -798.809] [0.0000], Avg: [-998.381 -998.381 -998.381] (1.000)
Step: 5749, Reward: [-664.02 -664.02 -664.02] [0.0000], Avg: [-995.474 -995.474 -995.474] (1.000)
Step: 5799, Reward: [-636.804 -636.804 -636.804] [0.0000], Avg: [-992.382 -992.382 -992.382] (1.000)
Step: 5849, Reward: [-655.242 -655.242 -655.242] [0.0000], Avg: [-989.5 -989.5 -989.5] (1.000)
Step: 5899, Reward: [-814.496 -814.496 -814.496] [0.0000], Avg: [-988.017 -988.017 -988.017] (1.000)
Step: 5949, Reward: [-963.979 -963.979 -963.979] [0.0000], Avg: [-987.815 -987.815 -987.815] (1.000)
Step: 5999, Reward: [-592.28 -592.28 -592.28] [0.0000], Avg: [-984.519 -984.519 -984.519] (1.000)
Step: 6049, Reward: [-860.585 -860.585 -860.585] [0.0000], Avg: [-983.495 -983.495 -983.495] (1.000)
Step: 6099, Reward: [-1114.78 -1114.78 -1114.78] [0.0000], Avg: [-984.571 -984.571 -984.571] (1.000)
Step: 6149, Reward: [-931.842 -931.842 -931.842] [0.0000], Avg: [-984.142 -984.142 -984.142] (1.000)
Step: 6199, Reward: [-1101.196 -1101.196 -1101.196] [0.0000], Avg: [-985.086 -985.086 -985.086] (1.000)
Step: 6249, Reward: [-1486.448 -1486.448 -1486.448] [0.0000], Avg: [-989.097 -989.097 -989.097] (1.000)
Step: 6299, Reward: [-829.347 -829.347 -829.347] [0.0000], Avg: [-987.829 -987.829 -987.829] (1.000)
Step: 6349, Reward: [-1031.662 -1031.662 -1031.662] [0.0000], Avg: [-988.174 -988.174 -988.174] (1.000)
Step: 6399, Reward: [-710.852 -710.852 -710.852] [0.0000], Avg: [-986.008 -986.008 -986.008] (1.000)
Step: 6449, Reward: [-672.464 -672.464 -672.464] [0.0000], Avg: [-983.577 -983.577 -983.577] (1.000)
Step: 6499, Reward: [-1287.337 -1287.337 -1287.337] [0.0000], Avg: [-985.914 -985.914 -985.914] (1.000)
Step: 6549, Reward: [-1111.94 -1111.94 -1111.94] [0.0000], Avg: [-986.876 -986.876 -986.876] (1.000)
Step: 6599, Reward: [-1052.2 -1052.2 -1052.2] [0.0000], Avg: [-987.371 -987.371 -987.371] (1.000)
Step: 6649, Reward: [-576.009 -576.009 -576.009] [0.0000], Avg: [-984.278 -984.278 -984.278] (1.000)
Step: 6699, Reward: [-502.41 -502.41 -502.41] [0.0000], Avg: [-980.682 -980.682 -980.682] (1.000)
Step: 6749, Reward: [-783.384 -783.384 -783.384] [0.0000], Avg: [-979.22 -979.22 -979.22] (1.000)
Step: 6799, Reward: [-1409.379 -1409.379 -1409.379] [0.0000], Avg: [-982.383 -982.383 -982.383] (1.000)
Step: 6849, Reward: [-1513.674 -1513.674 -1513.674] [0.0000], Avg: [-986.261 -986.261 -986.261] (1.000)
Step: 6899, Reward: [-1163.412 -1163.412 -1163.412] [0.0000], Avg: [-987.545 -987.545 -987.545] (1.000)
Step: 6949, Reward: [-1660.656 -1660.656 -1660.656] [0.0000], Avg: [-992.388 -992.388 -992.388] (1.000)
Step: 6999, Reward: [-1087.666 -1087.666 -1087.666] [0.0000], Avg: [-993.068 -993.068 -993.068] (1.000)
Step: 7049, Reward: [-1454.924 -1454.924 -1454.924] [0.0000], Avg: [-996.344 -996.344 -996.344] (1.000)
Step: 7099, Reward: [-2014.438 -2014.438 -2014.438] [0.0000], Avg: [-1003.513 -1003.513 -1003.513] (1.000)
Step: 7149, Reward: [-1852.546 -1852.546 -1852.546] [0.0000], Avg: [-1009.451 -1009.451 -1009.451] (1.000)
Step: 7199, Reward: [-2159.503 -2159.503 -2159.503] [0.0000], Avg: [-1017.437 -1017.437 -1017.437] (1.000)
Step: 7249, Reward: [-1573.412 -1573.412 -1573.412] [0.0000], Avg: [-1021.271 -1021.271 -1021.271] (1.000)
Step: 7299, Reward: [-1279.572 -1279.572 -1279.572] [0.0000], Avg: [-1023.041 -1023.041 -1023.041] (1.000)
Step: 7349, Reward: [-1052.876 -1052.876 -1052.876] [0.0000], Avg: [-1023.244 -1023.244 -1023.244] (1.000)
Step: 7399, Reward: [-1317.852 -1317.852 -1317.852] [0.0000], Avg: [-1025.234 -1025.234 -1025.234] (1.000)
Step: 7449, Reward: [-2212.733 -2212.733 -2212.733] [0.0000], Avg: [-1033.204 -1033.204 -1033.204] (1.000)
Step: 7499, Reward: [-1644.237 -1644.237 -1644.237] [0.0000], Avg: [-1037.278 -1037.278 -1037.278] (1.000)
Step: 7549, Reward: [-1903.181 -1903.181 -1903.181] [0.0000], Avg: [-1043.012 -1043.012 -1043.012] (1.000)
Step: 7599, Reward: [-1526.57 -1526.57 -1526.57] [0.0000], Avg: [-1046.193 -1046.193 -1046.193] (1.000)
Step: 7649, Reward: [-1225.858 -1225.858 -1225.858] [0.0000], Avg: [-1047.368 -1047.368 -1047.368] (1.000)
Step: 7699, Reward: [-1952.605 -1952.605 -1952.605] [0.0000], Avg: [-1053.246 -1053.246 -1053.246] (1.000)
Step: 7749, Reward: [-2131.827 -2131.827 -2131.827] [0.0000], Avg: [-1060.204 -1060.204 -1060.204] (1.000)
Step: 7799, Reward: [-2125.208 -2125.208 -2125.208] [0.0000], Avg: [-1067.031 -1067.031 -1067.031] (1.000)
Step: 7849, Reward: [-2274.298 -2274.298 -2274.298] [0.0000], Avg: [-1074.721 -1074.721 -1074.721] (1.000)
Step: 7899, Reward: [-1718.016 -1718.016 -1718.016] [0.0000], Avg: [-1078.792 -1078.792 -1078.792] (1.000)
Step: 7949, Reward: [-1796.694 -1796.694 -1796.694] [0.0000], Avg: [-1083.307 -1083.307 -1083.307] (1.000)
Step: 7999, Reward: [-1633.184 -1633.184 -1633.184] [0.0000], Avg: [-1086.744 -1086.744 -1086.744] (1.000)
Step: 8049, Reward: [-1550.01 -1550.01 -1550.01] [0.0000], Avg: [-1089.622 -1089.622 -1089.622] (1.000)
Step: 8099, Reward: [-1684.4 -1684.4 -1684.4] [0.0000], Avg: [-1093.293 -1093.293 -1093.293] (1.000)
Step: 8149, Reward: [-2073.865 -2073.865 -2073.865] [0.0000], Avg: [-1099.309 -1099.309 -1099.309] (1.000)
Step: 8199, Reward: [-2020.024 -2020.024 -2020.024] [0.0000], Avg: [-1104.923 -1104.923 -1104.923] (1.000)
Step: 8249, Reward: [-1938.929 -1938.929 -1938.929] [0.0000], Avg: [-1109.978 -1109.978 -1109.978] (1.000)
Step: 8299, Reward: [-1566.352 -1566.352 -1566.352] [0.0000], Avg: [-1112.727 -1112.727 -1112.727] (1.000)
Step: 8349, Reward: [-1690.705 -1690.705 -1690.705] [0.0000], Avg: [-1116.188 -1116.188 -1116.188] (1.000)
Step: 8399, Reward: [-1615.909 -1615.909 -1615.909] [0.0000], Avg: [-1119.162 -1119.162 -1119.162] (1.000)
Step: 8449, Reward: [-1921.77 -1921.77 -1921.77] [0.0000], Avg: [-1123.911 -1123.911 -1123.911] (1.000)
Step: 8499, Reward: [-1739.716 -1739.716 -1739.716] [0.0000], Avg: [-1127.534 -1127.534 -1127.534] (1.000)
Step: 8549, Reward: [-1894.736 -1894.736 -1894.736] [0.0000], Avg: [-1132.02 -1132.02 -1132.02] (1.000)
Step: 8599, Reward: [-1826.662 -1826.662 -1826.662] [0.0000], Avg: [-1136.059 -1136.059 -1136.059] (1.000)
Step: 8649, Reward: [-1776.63 -1776.63 -1776.63] [0.0000], Avg: [-1139.762 -1139.762 -1139.762] (1.000)
Step: 8699, Reward: [-1848.364 -1848.364 -1848.364] [0.0000], Avg: [-1143.834 -1143.834 -1143.834] (1.000)
Step: 8749, Reward: [-2113.812 -2113.812 -2113.812] [0.0000], Avg: [-1149.377 -1149.377 -1149.377] (1.000)
Step: 8799, Reward: [-2146.713 -2146.713 -2146.713] [0.0000], Avg: [-1155.044 -1155.044 -1155.044] (1.000)
Step: 8849, Reward: [-1818.343 -1818.343 -1818.343] [0.0000], Avg: [-1158.791 -1158.791 -1158.791] (1.000)
Step: 8899, Reward: [-1831.238 -1831.238 -1831.238] [0.0000], Avg: [-1162.569 -1162.569 -1162.569] (1.000)
Step: 8949, Reward: [-2004.251 -2004.251 -2004.251] [0.0000], Avg: [-1167.271 -1167.271 -1167.271] (1.000)
Step: 8999, Reward: [-2151.852 -2151.852 -2151.852] [0.0000], Avg: [-1172.741 -1172.741 -1172.741] (1.000)
Step: 9049, Reward: [-1687.935 -1687.935 -1687.935] [0.0000], Avg: [-1175.587 -1175.587 -1175.587] (1.000)
Step: 9099, Reward: [-1933.826 -1933.826 -1933.826] [0.0000], Avg: [-1179.753 -1179.753 -1179.753] (1.000)
Step: 9149, Reward: [-1662.208 -1662.208 -1662.208] [0.0000], Avg: [-1182.39 -1182.39 -1182.39] (1.000)
Step: 9199, Reward: [-2017.493 -2017.493 -2017.493] [0.0000], Avg: [-1186.928 -1186.928 -1186.928] (1.000)
Step: 9249, Reward: [-1955.089 -1955.089 -1955.089] [0.0000], Avg: [-1191.081 -1191.081 -1191.081] (1.000)
Step: 9299, Reward: [-1939.963 -1939.963 -1939.963] [0.0000], Avg: [-1195.107 -1195.107 -1195.107] (1.000)
Step: 9349, Reward: [-1674.934 -1674.934 -1674.934] [0.0000], Avg: [-1197.673 -1197.673 -1197.673] (1.000)
Step: 9399, Reward: [-2210.684 -2210.684 -2210.684] [0.0000], Avg: [-1203.061 -1203.061 -1203.061] (1.000)
Step: 9449, Reward: [-1988.622 -1988.622 -1988.622] [0.0000], Avg: [-1207.217 -1207.217 -1207.217] (1.000)
Step: 9499, Reward: [-1989.369 -1989.369 -1989.369] [0.0000], Avg: [-1211.334 -1211.334 -1211.334] (1.000)
Step: 9549, Reward: [-1941.47 -1941.47 -1941.47] [0.0000], Avg: [-1215.157 -1215.157 -1215.157] (1.000)
Step: 9599, Reward: [-1670.631 -1670.631 -1670.631] [0.0000], Avg: [-1217.529 -1217.529 -1217.529] (1.000)
Step: 9649, Reward: [-1610.075 -1610.075 -1610.075] [0.0000], Avg: [-1219.563 -1219.563 -1219.563] (1.000)
Step: 9699, Reward: [-2006.188 -2006.188 -2006.188] [0.0000], Avg: [-1223.618 -1223.618 -1223.618] (1.000)
Step: 9749, Reward: [-1894.031 -1894.031 -1894.031] [0.0000], Avg: [-1227.056 -1227.056 -1227.056] (1.000)
Step: 9799, Reward: [-1743.853 -1743.853 -1743.853] [0.0000], Avg: [-1229.692 -1229.692 -1229.692] (1.000)
Step: 9849, Reward: [-1820.865 -1820.865 -1820.865] [0.0000], Avg: [-1232.693 -1232.693 -1232.693] (1.000)
Step: 9899, Reward: [-1990.275 -1990.275 -1990.275] [0.0000], Avg: [-1236.519 -1236.519 -1236.519] (1.000)
Step: 9949, Reward: [-1281.951 -1281.951 -1281.951] [0.0000], Avg: [-1236.748 -1236.748 -1236.748] (1.000)
Step: 9999, Reward: [-2268.225 -2268.225 -2268.225] [0.0000], Avg: [-1241.905 -1241.905 -1241.905] (1.000)
Step: 10049, Reward: [-1939.152 -1939.152 -1939.152] [0.0000], Avg: [-1245.374 -1245.374 -1245.374] (1.000)
Step: 10099, Reward: [-2075.93 -2075.93 -2075.93] [0.0000], Avg: [-1249.486 -1249.486 -1249.486] (1.000)
Step: 10149, Reward: [-1846.438 -1846.438 -1846.438] [0.0000], Avg: [-1252.426 -1252.426 -1252.426] (1.000)
Step: 10199, Reward: [-2002.791 -2002.791 -2002.791] [0.0000], Avg: [-1256.105 -1256.105 -1256.105] (1.000)
Step: 10249, Reward: [-2161.148 -2161.148 -2161.148] [0.0000], Avg: [-1260.519 -1260.519 -1260.519] (1.000)
Step: 10299, Reward: [-2070.056 -2070.056 -2070.056] [0.0000], Avg: [-1264.449 -1264.449 -1264.449] (1.000)
Step: 10349, Reward: [-1980.265 -1980.265 -1980.265] [0.0000], Avg: [-1267.907 -1267.907 -1267.907] (1.000)
Step: 10399, Reward: [-2214.159 -2214.159 -2214.159] [0.0000], Avg: [-1272.457 -1272.457 -1272.457] (1.000)
Step: 10449, Reward: [-1928.046 -1928.046 -1928.046] [0.0000], Avg: [-1275.593 -1275.593 -1275.593] (1.000)
Step: 10499, Reward: [-1775.683 -1775.683 -1775.683] [0.0000], Avg: [-1277.975 -1277.975 -1277.975] (1.000)
Step: 10549, Reward: [-1566.438 -1566.438 -1566.438] [0.0000], Avg: [-1279.342 -1279.342 -1279.342] (1.000)
Step: 10599, Reward: [-1783.501 -1783.501 -1783.501] [0.0000], Avg: [-1281.72 -1281.72 -1281.72] (1.000)
Step: 10649, Reward: [-1726.916 -1726.916 -1726.916] [0.0000], Avg: [-1283.81 -1283.81 -1283.81] (1.000)
Step: 10699, Reward: [-2315.265 -2315.265 -2315.265] [0.0000], Avg: [-1288.63 -1288.63 -1288.63] (1.000)
Step: 10749, Reward: [-2014.009 -2014.009 -2014.009] [0.0000], Avg: [-1292.004 -1292.004 -1292.004] (1.000)
Step: 10799, Reward: [-1800.685 -1800.685 -1800.685] [0.0000], Avg: [-1294.359 -1294.359 -1294.359] (1.000)
Step: 10849, Reward: [-1920.684 -1920.684 -1920.684] [0.0000], Avg: [-1297.245 -1297.245 -1297.245] (1.000)
Step: 10899, Reward: [-1165.97 -1165.97 -1165.97] [0.0000], Avg: [-1296.643 -1296.643 -1296.643] (1.000)
Step: 10949, Reward: [-2064.462 -2064.462 -2064.462] [0.0000], Avg: [-1300.149 -1300.149 -1300.149] (1.000)
Step: 10999, Reward: [-2210.918 -2210.918 -2210.918] [0.0000], Avg: [-1304.289 -1304.289 -1304.289] (1.000)
Step: 11049, Reward: [-1998.536 -1998.536 -1998.536] [0.0000], Avg: [-1307.43 -1307.43 -1307.43] (1.000)
Step: 11099, Reward: [-1720.169 -1720.169 -1720.169] [0.0000], Avg: [-1309.289 -1309.289 -1309.289] (1.000)
Step: 11149, Reward: [-2343.738 -2343.738 -2343.738] [0.0000], Avg: [-1313.928 -1313.928 -1313.928] (1.000)
Step: 11199, Reward: [-1394.419 -1394.419 -1394.419] [0.0000], Avg: [-1314.288 -1314.288 -1314.288] (1.000)
Step: 11249, Reward: [-1985.487 -1985.487 -1985.487] [0.0000], Avg: [-1317.271 -1317.271 -1317.271] (1.000)
Step: 11299, Reward: [-1929.737 -1929.737 -1929.737] [0.0000], Avg: [-1319.981 -1319.981 -1319.981] (1.000)
Step: 11349, Reward: [-1897.237 -1897.237 -1897.237] [0.0000], Avg: [-1322.524 -1322.524 -1322.524] (1.000)
Step: 11399, Reward: [-2174.721 -2174.721 -2174.721] [0.0000], Avg: [-1326.261 -1326.261 -1326.261] (1.000)
Step: 11449, Reward: [-1866.863 -1866.863 -1866.863] [0.0000], Avg: [-1328.622 -1328.622 -1328.622] (1.000)
Step: 11499, Reward: [-2391.828 -2391.828 -2391.828] [0.0000], Avg: [-1333.245 -1333.245 -1333.245] (1.000)
Step: 11549, Reward: [-1716.095 -1716.095 -1716.095] [0.0000], Avg: [-1334.902 -1334.902 -1334.902] (1.000)
Step: 11599, Reward: [-1883.892 -1883.892 -1883.892] [0.0000], Avg: [-1337.268 -1337.268 -1337.268] (1.000)
Step: 11649, Reward: [-1921.948 -1921.948 -1921.948] [0.0000], Avg: [-1339.778 -1339.778 -1339.778] (1.000)
Step: 11699, Reward: [-2242.846 -2242.846 -2242.846] [0.0000], Avg: [-1343.637 -1343.637 -1343.637] (1.000)
Step: 11749, Reward: [-1758.474 -1758.474 -1758.474] [0.0000], Avg: [-1345.402 -1345.402 -1345.402] (1.000)
Step: 11799, Reward: [-2038.267 -2038.267 -2038.267] [0.0000], Avg: [-1348.338 -1348.338 -1348.338] (1.000)
Step: 11849, Reward: [-2045.686 -2045.686 -2045.686] [0.0000], Avg: [-1351.281 -1351.281 -1351.281] (1.000)
Step: 11899, Reward: [-2185.179 -2185.179 -2185.179] [0.0000], Avg: [-1354.784 -1354.784 -1354.784] (1.000)
Step: 11949, Reward: [-1895.522 -1895.522 -1895.522] [0.0000], Avg: [-1357.047 -1357.047 -1357.047] (1.000)
Step: 11999, Reward: [-2321.833 -2321.833 -2321.833] [0.0000], Avg: [-1361.067 -1361.067 -1361.067] (1.000)
Step: 12049, Reward: [-2191.139 -2191.139 -2191.139] [0.0000], Avg: [-1364.511 -1364.511 -1364.511] (1.000)
Step: 12099, Reward: [-2131.68 -2131.68 -2131.68] [0.0000], Avg: [-1367.681 -1367.681 -1367.681] (1.000)
Step: 12149, Reward: [-1841.825 -1841.825 -1841.825] [0.0000], Avg: [-1369.632 -1369.632 -1369.632] (1.000)
Step: 12199, Reward: [-2114.406 -2114.406 -2114.406] [0.0000], Avg: [-1372.685 -1372.685 -1372.685] (1.000)
Step: 12249, Reward: [-1629.119 -1629.119 -1629.119] [0.0000], Avg: [-1373.731 -1373.731 -1373.731] (1.000)
Step: 12299, Reward: [-2465.975 -2465.975 -2465.975] [0.0000], Avg: [-1378.171 -1378.171 -1378.171] (1.000)
Step: 12349, Reward: [-1981.919 -1981.919 -1981.919] [0.0000], Avg: [-1380.616 -1380.616 -1380.616] (1.000)
Step: 12399, Reward: [-1712.994 -1712.994 -1712.994] [0.0000], Avg: [-1381.956 -1381.956 -1381.956] (1.000)
Step: 12449, Reward: [-2128.329 -2128.329 -2128.329] [0.0000], Avg: [-1384.953 -1384.953 -1384.953] (1.000)
Step: 12499, Reward: [-1708.469 -1708.469 -1708.469] [0.0000], Avg: [-1386.248 -1386.248 -1386.248] (1.000)
Step: 12549, Reward: [-2007.073 -2007.073 -2007.073] [0.0000], Avg: [-1388.721 -1388.721 -1388.721] (1.000)
Step: 12599, Reward: [-1919.392 -1919.392 -1919.392] [0.0000], Avg: [-1390.827 -1390.827 -1390.827] (1.000)
Step: 12649, Reward: [-1759.123 -1759.123 -1759.123] [0.0000], Avg: [-1392.282 -1392.282 -1392.282] (1.000)
Step: 12699, Reward: [-1779.807 -1779.807 -1779.807] [0.0000], Avg: [-1393.808 -1393.808 -1393.808] (1.000)
Step: 12749, Reward: [-1945.838 -1945.838 -1945.838] [0.0000], Avg: [-1395.973 -1395.973 -1395.973] (1.000)
Step: 12799, Reward: [-2126.334 -2126.334 -2126.334] [0.0000], Avg: [-1398.826 -1398.826 -1398.826] (1.000)
Step: 12849, Reward: [-1881.532 -1881.532 -1881.532] [0.0000], Avg: [-1400.704 -1400.704 -1400.704] (1.000)
Step: 12899, Reward: [-1891.266 -1891.266 -1891.266] [0.0000], Avg: [-1402.606 -1402.606 -1402.606] (1.000)
Step: 12949, Reward: [-2298.9 -2298.9 -2298.9] [0.0000], Avg: [-1406.066 -1406.066 -1406.066] (1.000)
Step: 12999, Reward: [-2050.217 -2050.217 -2050.217] [0.0000], Avg: [-1408.544 -1408.544 -1408.544] (1.000)
Step: 13049, Reward: [-1913.1 -1913.1 -1913.1] [0.0000], Avg: [-1410.477 -1410.477 -1410.477] (1.000)
Step: 13099, Reward: [-2111.943 -2111.943 -2111.943] [0.0000], Avg: [-1413.154 -1413.154 -1413.154] (1.000)
Step: 13149, Reward: [-1574.188 -1574.188 -1574.188] [0.0000], Avg: [-1413.767 -1413.767 -1413.767] (1.000)
Step: 13199, Reward: [-1728.353 -1728.353 -1728.353] [0.0000], Avg: [-1414.958 -1414.958 -1414.958] (1.000)
Step: 13249, Reward: [-1530.519 -1530.519 -1530.519] [0.0000], Avg: [-1415.394 -1415.394 -1415.394] (1.000)
Step: 13299, Reward: [-1679.033 -1679.033 -1679.033] [0.0000], Avg: [-1416.385 -1416.385 -1416.385] (1.000)
Step: 13349, Reward: [-1409.019 -1409.019 -1409.019] [0.0000], Avg: [-1416.358 -1416.358 -1416.358] (1.000)
Step: 13399, Reward: [-1800.32 -1800.32 -1800.32] [0.0000], Avg: [-1417.79 -1417.79 -1417.79] (1.000)
Step: 13449, Reward: [-1576.143 -1576.143 -1576.143] [0.0000], Avg: [-1418.379 -1418.379 -1418.379] (1.000)
Step: 13499, Reward: [-1019.08 -1019.08 -1019.08] [0.0000], Avg: [-1416.9 -1416.9 -1416.9] (1.000)
Step: 13549, Reward: [-1532.283 -1532.283 -1532.283] [0.0000], Avg: [-1417.326 -1417.326 -1417.326] (1.000)
Step: 13599, Reward: [-1311.818 -1311.818 -1311.818] [0.0000], Avg: [-1416.938 -1416.938 -1416.938] (1.000)
Step: 13649, Reward: [-1279.374 -1279.374 -1279.374] [0.0000], Avg: [-1416.434 -1416.434 -1416.434] (1.000)
Step: 13699, Reward: [-1576.389 -1576.389 -1576.389] [0.0000], Avg: [-1417.018 -1417.018 -1417.018] (1.000)
Step: 13749, Reward: [-1537.756 -1537.756 -1537.756] [0.0000], Avg: [-1417.457 -1417.457 -1417.457] (1.000)
Step: 13799, Reward: [-1241.163 -1241.163 -1241.163] [0.0000], Avg: [-1416.818 -1416.818 -1416.818] (1.000)
Step: 13849, Reward: [-1562.882 -1562.882 -1562.882] [0.0000], Avg: [-1417.346 -1417.346 -1417.346] (1.000)
Step: 13899, Reward: [-1858.685 -1858.685 -1858.685] [0.0000], Avg: [-1418.933 -1418.933 -1418.933] (1.000)
Step: 13949, Reward: [-1975.521 -1975.521 -1975.521] [0.0000], Avg: [-1420.928 -1420.928 -1420.928] (1.000)
Step: 13999, Reward: [-1840.028 -1840.028 -1840.028] [0.0000], Avg: [-1422.425 -1422.425 -1422.425] (1.000)
Step: 14049, Reward: [-2078.522 -2078.522 -2078.522] [0.0000], Avg: [-1424.76 -1424.76 -1424.76] (1.000)
Step: 14099, Reward: [-1965.944 -1965.944 -1965.944] [0.0000], Avg: [-1426.679 -1426.679 -1426.679] (1.000)
Step: 14149, Reward: [-2070.262 -2070.262 -2070.262] [0.0000], Avg: [-1428.953 -1428.953 -1428.953] (1.000)
Step: 14199, Reward: [-1960.541 -1960.541 -1960.541] [0.0000], Avg: [-1430.825 -1430.825 -1430.825] (1.000)
Step: 14249, Reward: [-1938.315 -1938.315 -1938.315] [0.0000], Avg: [-1432.605 -1432.605 -1432.605] (1.000)
Step: 14299, Reward: [-2019.205 -2019.205 -2019.205] [0.0000], Avg: [-1434.656 -1434.656 -1434.656] (1.000)
Step: 14349, Reward: [-1830.037 -1830.037 -1830.037] [0.0000], Avg: [-1436.034 -1436.034 -1436.034] (1.000)
Step: 14399, Reward: [-1758.859 -1758.859 -1758.859] [0.0000], Avg: [-1437.155 -1437.155 -1437.155] (1.000)
Step: 14449, Reward: [-2324.827 -2324.827 -2324.827] [0.0000], Avg: [-1440.227 -1440.227 -1440.227] (1.000)
Step: 14499, Reward: [-1948.63 -1948.63 -1948.63] [0.0000], Avg: [-1441.98 -1441.98 -1441.98] (1.000)
Step: 14549, Reward: [-1838.573 -1838.573 -1838.573] [0.0000], Avg: [-1443.343 -1443.343 -1443.343] (1.000)
Step: 14599, Reward: [-1953.157 -1953.157 -1953.157] [0.0000], Avg: [-1445.088 -1445.088 -1445.088] (1.000)
Step: 14649, Reward: [-1943.85 -1943.85 -1943.85] [0.0000], Avg: [-1446.791 -1446.791 -1446.791] (1.000)
Step: 14699, Reward: [-2159.912 -2159.912 -2159.912] [0.0000], Avg: [-1449.216 -1449.216 -1449.216] (1.000)
Step: 14749, Reward: [-1933.697 -1933.697 -1933.697] [0.0000], Avg: [-1450.859 -1450.859 -1450.859] (1.000)
Step: 14799, Reward: [-1985.756 -1985.756 -1985.756] [0.0000], Avg: [-1452.666 -1452.666 -1452.666] (1.000)
Step: 14849, Reward: [-1945.449 -1945.449 -1945.449] [0.0000], Avg: [-1454.325 -1454.325 -1454.325] (1.000)
Step: 14899, Reward: [-1622.312 -1622.312 -1622.312] [0.0000], Avg: [-1454.889 -1454.889 -1454.889] (1.000)
Step: 14949, Reward: [-2417.767 -2417.767 -2417.767] [0.0000], Avg: [-1458.109 -1458.109 -1458.109] (1.000)
Step: 14999, Reward: [-1981.239 -1981.239 -1981.239] [0.0000], Avg: [-1459.853 -1459.853 -1459.853] (1.000)
Step: 15049, Reward: [-1804.227 -1804.227 -1804.227] [0.0000], Avg: [-1460.997 -1460.997 -1460.997] (1.000)
Step: 15099, Reward: [-1730.278 -1730.278 -1730.278] [0.0000], Avg: [-1461.888 -1461.888 -1461.888] (1.000)
Step: 15149, Reward: [-2206.098 -2206.098 -2206.098] [0.0000], Avg: [-1464.345 -1464.345 -1464.345] (1.000)
Step: 15199, Reward: [-1785.269 -1785.269 -1785.269] [0.0000], Avg: [-1465.4 -1465.4 -1465.4] (1.000)
Step: 15249, Reward: [-1703.305 -1703.305 -1703.305] [0.0000], Avg: [-1466.18 -1466.18 -1466.18] (1.000)
Step: 15299, Reward: [-1909.997 -1909.997 -1909.997] [0.0000], Avg: [-1467.631 -1467.631 -1467.631] (1.000)
Step: 15349, Reward: [-1720.212 -1720.212 -1720.212] [0.0000], Avg: [-1468.453 -1468.453 -1468.453] (1.000)
Step: 15399, Reward: [-1936.894 -1936.894 -1936.894] [0.0000], Avg: [-1469.974 -1469.974 -1469.974] (1.000)
Step: 15449, Reward: [-2206.094 -2206.094 -2206.094] [0.0000], Avg: [-1472.357 -1472.357 -1472.357] (1.000)
Step: 15499, Reward: [-1984.762 -1984.762 -1984.762] [0.0000], Avg: [-1474.01 -1474.01 -1474.01] (1.000)
Step: 15549, Reward: [-1887.292 -1887.292 -1887.292] [0.0000], Avg: [-1475.338 -1475.338 -1475.338] (1.000)
Step: 15599, Reward: [-2069.447 -2069.447 -2069.447] [0.0000], Avg: [-1477.243 -1477.243 -1477.243] (1.000)
Step: 15649, Reward: [-2082.293 -2082.293 -2082.293] [0.0000], Avg: [-1479.176 -1479.176 -1479.176] (1.000)
Step: 15699, Reward: [-1843.552 -1843.552 -1843.552] [0.0000], Avg: [-1480.336 -1480.336 -1480.336] (1.000)
Step: 15749, Reward: [-1950.565 -1950.565 -1950.565] [0.0000], Avg: [-1481.829 -1481.829 -1481.829] (1.000)
Step: 15799, Reward: [-2094.427 -2094.427 -2094.427] [0.0000], Avg: [-1483.767 -1483.767 -1483.767] (1.000)
Step: 15849, Reward: [-2236.834 -2236.834 -2236.834] [0.0000], Avg: [-1486.143 -1486.143 -1486.143] (1.000)
Step: 15899, Reward: [-1594.428 -1594.428 -1594.428] [0.0000], Avg: [-1486.484 -1486.484 -1486.484] (1.000)
Step: 15949, Reward: [-1956.877 -1956.877 -1956.877] [0.0000], Avg: [-1487.958 -1487.958 -1487.958] (1.000)
Step: 15999, Reward: [-1938.898 -1938.898 -1938.898] [0.0000], Avg: [-1489.367 -1489.367 -1489.367] (1.000)
Step: 16049, Reward: [-2026.683 -2026.683 -2026.683] [0.0000], Avg: [-1491.041 -1491.041 -1491.041] (1.000)
Step: 16099, Reward: [-2373.483 -2373.483 -2373.483] [0.0000], Avg: [-1493.782 -1493.782 -1493.782] (1.000)
Step: 16149, Reward: [-1945.609 -1945.609 -1945.609] [0.0000], Avg: [-1495.181 -1495.181 -1495.181] (1.000)
Step: 16199, Reward: [-1851.334 -1851.334 -1851.334] [0.0000], Avg: [-1496.28 -1496.28 -1496.28] (1.000)
Step: 16249, Reward: [-2053.708 -2053.708 -2053.708] [0.0000], Avg: [-1497.995 -1497.995 -1497.995] (1.000)
Step: 16299, Reward: [-1769.835 -1769.835 -1769.835] [0.0000], Avg: [-1498.829 -1498.829 -1498.829] (1.000)
Step: 16349, Reward: [-1720.379 -1720.379 -1720.379] [0.0000], Avg: [-1499.506 -1499.506 -1499.506] (1.000)
Step: 16399, Reward: [-1804.984 -1804.984 -1804.984] [0.0000], Avg: [-1500.438 -1500.438 -1500.438] (1.000)
Step: 16449, Reward: [-2072.631 -2072.631 -2072.631] [0.0000], Avg: [-1502.177 -1502.177 -1502.177] (1.000)
Step: 16499, Reward: [-1870.117 -1870.117 -1870.117] [0.0000], Avg: [-1503.292 -1503.292 -1503.292] (1.000)
Step: 16549, Reward: [-1664.087 -1664.087 -1664.087] [0.0000], Avg: [-1503.778 -1503.778 -1503.778] (1.000)
Step: 16599, Reward: [-1775.726 -1775.726 -1775.726] [0.0000], Avg: [-1504.597 -1504.597 -1504.597] (1.000)
Step: 16649, Reward: [-2019.685 -2019.685 -2019.685] [0.0000], Avg: [-1506.144 -1506.144 -1506.144] (1.000)
Step: 16699, Reward: [-1782.315 -1782.315 -1782.315] [0.0000], Avg: [-1506.97 -1506.97 -1506.97] (1.000)
Step: 16749, Reward: [-2071.828 -2071.828 -2071.828] [0.0000], Avg: [-1508.657 -1508.657 -1508.657] (1.000)
Step: 16799, Reward: [-1835.419 -1835.419 -1835.419] [0.0000], Avg: [-1509.629 -1509.629 -1509.629] (1.000)
Step: 16849, Reward: [-1993.321 -1993.321 -1993.321] [0.0000], Avg: [-1511.064 -1511.064 -1511.064] (1.000)
Step: 16899, Reward: [-1901.879 -1901.879 -1901.879] [0.0000], Avg: [-1512.221 -1512.221 -1512.221] (1.000)
Step: 16949, Reward: [-1893.445 -1893.445 -1893.445] [0.0000], Avg: [-1513.345 -1513.345 -1513.345] (1.000)
Step: 16999, Reward: [-2099.127 -2099.127 -2099.127] [0.0000], Avg: [-1515.068 -1515.068 -1515.068] (1.000)
Step: 17049, Reward: [-1919.223 -1919.223 -1919.223] [0.0000], Avg: [-1516.253 -1516.253 -1516.253] (1.000)
Step: 17099, Reward: [-1918.192 -1918.192 -1918.192] [0.0000], Avg: [-1517.429 -1517.429 -1517.429] (1.000)
Step: 17149, Reward: [-1987.628 -1987.628 -1987.628] [0.0000], Avg: [-1518.799 -1518.799 -1518.799] (1.000)
Step: 17199, Reward: [-1923.047 -1923.047 -1923.047] [0.0000], Avg: [-1519.975 -1519.975 -1519.975] (1.000)
Step: 17249, Reward: [-1871.397 -1871.397 -1871.397] [0.0000], Avg: [-1520.993 -1520.993 -1520.993] (1.000)
Step: 17299, Reward: [-1990.484 -1990.484 -1990.484] [0.0000], Avg: [-1522.35 -1522.35 -1522.35] (1.000)
Step: 17349, Reward: [-1897.28 -1897.28 -1897.28] [0.0000], Avg: [-1523.431 -1523.431 -1523.431] (1.000)
Step: 17399, Reward: [-1841.513 -1841.513 -1841.513] [0.0000], Avg: [-1524.345 -1524.345 -1524.345] (1.000)
Step: 17449, Reward: [-2167.806 -2167.806 -2167.806] [0.0000], Avg: [-1526.188 -1526.188 -1526.188] (1.000)
Step: 17499, Reward: [-1840.864 -1840.864 -1840.864] [0.0000], Avg: [-1527.087 -1527.087 -1527.087] (1.000)
Step: 17549, Reward: [-1925.33 -1925.33 -1925.33] [0.0000], Avg: [-1528.222 -1528.222 -1528.222] (1.000)
Step: 17599, Reward: [-1831.455 -1831.455 -1831.455] [0.0000], Avg: [-1529.083 -1529.083 -1529.083] (1.000)
Step: 17649, Reward: [-2088.267 -2088.267 -2088.267] [0.0000], Avg: [-1530.668 -1530.668 -1530.668] (1.000)
Step: 17699, Reward: [-1819.412 -1819.412 -1819.412] [0.0000], Avg: [-1531.483 -1531.483 -1531.483] (1.000)
Step: 17749, Reward: [-1802.753 -1802.753 -1802.753] [0.0000], Avg: [-1532.247 -1532.247 -1532.247] (1.000)
Step: 17799, Reward: [-1655.534 -1655.534 -1655.534] [0.0000], Avg: [-1532.594 -1532.594 -1532.594] (1.000)
Step: 17849, Reward: [-1919.173 -1919.173 -1919.173] [0.0000], Avg: [-1533.677 -1533.677 -1533.677] (1.000)
Step: 17899, Reward: [-2013.374 -2013.374 -2013.374] [0.0000], Avg: [-1535.016 -1535.016 -1535.016] (1.000)
Step: 17949, Reward: [-2037.629 -2037.629 -2037.629] [0.0000], Avg: [-1536.417 -1536.417 -1536.417] (1.000)
Step: 17999, Reward: [-1853.096 -1853.096 -1853.096] [0.0000], Avg: [-1537.296 -1537.296 -1537.296] (1.000)
Step: 18049, Reward: [-1882.653 -1882.653 -1882.653] [0.0000], Avg: [-1538.253 -1538.253 -1538.253] (1.000)
Step: 18099, Reward: [-1916.465 -1916.465 -1916.465] [0.0000], Avg: [-1539.298 -1539.298 -1539.298] (1.000)
Step: 18149, Reward: [-1689.92 -1689.92 -1689.92] [0.0000], Avg: [-1539.713 -1539.713 -1539.713] (1.000)
Step: 18199, Reward: [-1911.777 -1911.777 -1911.777] [0.0000], Avg: [-1540.735 -1540.735 -1540.735] (1.000)
Step: 18249, Reward: [-2077.253 -2077.253 -2077.253] [0.0000], Avg: [-1542.205 -1542.205 -1542.205] (1.000)
Step: 18299, Reward: [-2079.035 -2079.035 -2079.035] [0.0000], Avg: [-1543.671 -1543.671 -1543.671] (1.000)
Step: 18349, Reward: [-1784.38 -1784.38 -1784.38] [0.0000], Avg: [-1544.327 -1544.327 -1544.327] (1.000)
Step: 18399, Reward: [-1814.838 -1814.838 -1814.838] [0.0000], Avg: [-1545.062 -1545.062 -1545.062] (1.000)
Step: 18449, Reward: [-1569.431 -1569.431 -1569.431] [0.0000], Avg: [-1545.128 -1545.128 -1545.128] (1.000)
Step: 18499, Reward: [-2071.174 -2071.174 -2071.174] [0.0000], Avg: [-1546.55 -1546.55 -1546.55] (1.000)
Step: 18549, Reward: [-1815.382 -1815.382 -1815.382] [0.0000], Avg: [-1547.275 -1547.275 -1547.275] (1.000)
Step: 18599, Reward: [-1928.266 -1928.266 -1928.266] [0.0000], Avg: [-1548.299 -1548.299 -1548.299] (1.000)
Step: 18649, Reward: [-1901.677 -1901.677 -1901.677] [0.0000], Avg: [-1549.246 -1549.246 -1549.246] (1.000)
Step: 18699, Reward: [-1721.955 -1721.955 -1721.955] [0.0000], Avg: [-1549.708 -1549.708 -1549.708] (1.000)
Step: 18749, Reward: [-1947.83 -1947.83 -1947.83] [0.0000], Avg: [-1550.77 -1550.77 -1550.77] (1.000)
Step: 18799, Reward: [-1769.666 -1769.666 -1769.666] [0.0000], Avg: [-1551.352 -1551.352 -1551.352] (1.000)
Step: 18849, Reward: [-1996.327 -1996.327 -1996.327] [0.0000], Avg: [-1552.532 -1552.532 -1552.532] (1.000)
Step: 18899, Reward: [-1737.349 -1737.349 -1737.349] [0.0000], Avg: [-1553.021 -1553.021 -1553.021] (1.000)
Step: 18949, Reward: [-1810.32 -1810.32 -1810.32] [0.0000], Avg: [-1553.7 -1553.7 -1553.7] (1.000)
Step: 18999, Reward: [-2006.424 -2006.424 -2006.424] [0.0000], Avg: [-1554.891 -1554.891 -1554.891] (1.000)
Step: 19049, Reward: [-2081.365 -2081.365 -2081.365] [0.0000], Avg: [-1556.273 -1556.273 -1556.273] (1.000)
Step: 19099, Reward: [-1927.132 -1927.132 -1927.132] [0.0000], Avg: [-1557.244 -1557.244 -1557.244] (1.000)
Step: 19149, Reward: [-1874.294 -1874.294 -1874.294] [0.0000], Avg: [-1558.072 -1558.072 -1558.072] (1.000)
Step: 19199, Reward: [-1647.713 -1647.713 -1647.713] [0.0000], Avg: [-1558.305 -1558.305 -1558.305] (1.000)
Step: 19249, Reward: [-2184.035 -2184.035 -2184.035] [0.0000], Avg: [-1559.931 -1559.931 -1559.931] (1.000)
Step: 19299, Reward: [-1941.992 -1941.992 -1941.992] [0.0000], Avg: [-1560.92 -1560.92 -1560.92] (1.000)
Step: 19349, Reward: [-1838.898 -1838.898 -1838.898] [0.0000], Avg: [-1561.639 -1561.639 -1561.639] (1.000)
Step: 19399, Reward: [-1803.024 -1803.024 -1803.024] [0.0000], Avg: [-1562.261 -1562.261 -1562.261] (1.000)
Step: 19449, Reward: [-1700.595 -1700.595 -1700.595] [0.0000], Avg: [-1562.616 -1562.616 -1562.616] (1.000)
Step: 19499, Reward: [-2037.052 -2037.052 -2037.052] [0.0000], Avg: [-1563.833 -1563.833 -1563.833] (1.000)
Step: 19549, Reward: [-2134.094 -2134.094 -2134.094] [0.0000], Avg: [-1565.291 -1565.291 -1565.291] (1.000)
Step: 19599, Reward: [-1691.492 -1691.492 -1691.492] [0.0000], Avg: [-1565.613 -1565.613 -1565.613] (1.000)
Step: 19649, Reward: [-1856.266 -1856.266 -1856.266] [0.0000], Avg: [-1566.353 -1566.353 -1566.353] (1.000)
Step: 19699, Reward: [-2084.638 -2084.638 -2084.638] [0.0000], Avg: [-1567.668 -1567.668 -1567.668] (1.000)
Step: 19749, Reward: [-2127.558 -2127.558 -2127.558] [0.0000], Avg: [-1569.086 -1569.086 -1569.086] (1.000)
Step: 19799, Reward: [-1905.502 -1905.502 -1905.502] [0.0000], Avg: [-1569.935 -1569.935 -1569.935] (1.000)
Step: 19849, Reward: [-1702.353 -1702.353 -1702.353] [0.0000], Avg: [-1570.269 -1570.269 -1570.269] (1.000)
Step: 19899, Reward: [-1989.669 -1989.669 -1989.669] [0.0000], Avg: [-1571.323 -1571.323 -1571.323] (1.000)
Step: 19949, Reward: [-2166.78 -2166.78 -2166.78] [0.0000], Avg: [-1572.815 -1572.815 -1572.815] (1.000)
Step: 19999, Reward: [-1988.595 -1988.595 -1988.595] [0.0000], Avg: [-1573.854 -1573.854 -1573.854] (1.000)
Step: 20049, Reward: [-1713.13 -1713.13 -1713.13] [0.0000], Avg: [-1574.202 -1574.202 -1574.202] (1.000)
Step: 20099, Reward: [-1767.944 -1767.944 -1767.944] [0.0000], Avg: [-1574.684 -1574.684 -1574.684] (1.000)
Step: 20149, Reward: [-2165.439 -2165.439 -2165.439] [0.0000], Avg: [-1576.15 -1576.15 -1576.15] (1.000)
Step: 20199, Reward: [-1898.306 -1898.306 -1898.306] [0.0000], Avg: [-1576.947 -1576.947 -1576.947] (1.000)
Step: 20249, Reward: [-1904.145 -1904.145 -1904.145] [0.0000], Avg: [-1577.755 -1577.755 -1577.755] (1.000)
Step: 20299, Reward: [-1832.763 -1832.763 -1832.763] [0.0000], Avg: [-1578.383 -1578.383 -1578.383] (1.000)
Step: 20349, Reward: [-1961.377 -1961.377 -1961.377] [0.0000], Avg: [-1579.324 -1579.324 -1579.324] (1.000)
Step: 20399, Reward: [-1737.564 -1737.564 -1737.564] [0.0000], Avg: [-1579.712 -1579.712 -1579.712] (1.000)
Step: 20449, Reward: [-1934.205 -1934.205 -1934.205] [0.0000], Avg: [-1580.579 -1580.579 -1580.579] (1.000)
Step: 20499, Reward: [-1955.963 -1955.963 -1955.963] [0.0000], Avg: [-1581.494 -1581.494 -1581.494] (1.000)
Step: 20549, Reward: [-2125.138 -2125.138 -2125.138] [0.0000], Avg: [-1582.817 -1582.817 -1582.817] (1.000)
Step: 20599, Reward: [-2068.338 -2068.338 -2068.338] [0.0000], Avg: [-1583.995 -1583.995 -1583.995] (1.000)
Step: 20649, Reward: [-1729.134 -1729.134 -1729.134] [0.0000], Avg: [-1584.347 -1584.347 -1584.347] (1.000)
Step: 20699, Reward: [-2222.665 -2222.665 -2222.665] [0.0000], Avg: [-1585.889 -1585.889 -1585.889] (1.000)
Step: 20749, Reward: [-1791.628 -1791.628 -1791.628] [0.0000], Avg: [-1586.384 -1586.384 -1586.384] (1.000)
Step: 20799, Reward: [-1779.793 -1779.793 -1779.793] [0.0000], Avg: [-1586.849 -1586.849 -1586.849] (1.000)
Step: 20849, Reward: [-1510.215 -1510.215 -1510.215] [0.0000], Avg: [-1586.666 -1586.666 -1586.666] (1.000)
Step: 20899, Reward: [-2141.657 -2141.657 -2141.657] [0.0000], Avg: [-1587.993 -1587.993 -1587.993] (1.000)
Step: 20949, Reward: [-1727.198 -1727.198 -1727.198] [0.0000], Avg: [-1588.326 -1588.326 -1588.326] (1.000)
Step: 20999, Reward: [-1634.499 -1634.499 -1634.499] [0.0000], Avg: [-1588.435 -1588.435 -1588.435] (1.000)
Step: 21049, Reward: [-1666.509 -1666.509 -1666.509] [0.0000], Avg: [-1588.621 -1588.621 -1588.621] (1.000)
Step: 21099, Reward: [-1627.409 -1627.409 -1627.409] [0.0000], Avg: [-1588.713 -1588.713 -1588.713] (1.000)
Step: 21149, Reward: [-1315.301 -1315.301 -1315.301] [0.0000], Avg: [-1588.066 -1588.066 -1588.066] (1.000)
Step: 21199, Reward: [-1478.713 -1478.713 -1478.713] [0.0000], Avg: [-1587.809 -1587.809 -1587.809] (1.000)
Step: 21249, Reward: [-1586.364 -1586.364 -1586.364] [0.0000], Avg: [-1587.805 -1587.805 -1587.805] (1.000)
Step: 21299, Reward: [-1796.505 -1796.505 -1796.505] [0.0000], Avg: [-1588.295 -1588.295 -1588.295] (1.000)
Step: 21349, Reward: [-1515.93 -1515.93 -1515.93] [0.0000], Avg: [-1588.126 -1588.126 -1588.126] (1.000)
Step: 21399, Reward: [-1806.239 -1806.239 -1806.239] [0.0000], Avg: [-1588.635 -1588.635 -1588.635] (1.000)
Step: 21449, Reward: [-1415.853 -1415.853 -1415.853] [0.0000], Avg: [-1588.232 -1588.232 -1588.232] (1.000)
Step: 21499, Reward: [-1725.352 -1725.352 -1725.352] [0.0000], Avg: [-1588.551 -1588.551 -1588.551] (1.000)
Step: 21549, Reward: [-1904.999 -1904.999 -1904.999] [0.0000], Avg: [-1589.286 -1589.286 -1589.286] (1.000)
Step: 21599, Reward: [-1765.18 -1765.18 -1765.18] [0.0000], Avg: [-1589.693 -1589.693 -1589.693] (1.000)
Step: 21649, Reward: [-1692.854 -1692.854 -1692.854] [0.0000], Avg: [-1589.931 -1589.931 -1589.931] (1.000)
Step: 21699, Reward: [-2074.748 -2074.748 -2074.748] [0.0000], Avg: [-1591.048 -1591.048 -1591.048] (1.000)
Step: 21749, Reward: [-2042.464 -2042.464 -2042.464] [0.0000], Avg: [-1592.086 -1592.086 -1592.086] (1.000)
Step: 21799, Reward: [-1694.227 -1694.227 -1694.227] [0.0000], Avg: [-1592.32 -1592.32 -1592.32] (1.000)
Step: 21849, Reward: [-1671.522 -1671.522 -1671.522] [0.0000], Avg: [-1592.501 -1592.501 -1592.501] (1.000)
Step: 21899, Reward: [-1662.828 -1662.828 -1662.828] [0.0000], Avg: [-1592.662 -1592.662 -1592.662] (1.000)
Step: 21949, Reward: [-1901.885 -1901.885 -1901.885] [0.0000], Avg: [-1593.366 -1593.366 -1593.366] (1.000)
Step: 21999, Reward: [-1541.195 -1541.195 -1541.195] [0.0000], Avg: [-1593.248 -1593.248 -1593.248] (1.000)
Step: 22049, Reward: [-2269.186 -2269.186 -2269.186] [0.0000], Avg: [-1594.78 -1594.78 -1594.78] (1.000)
Step: 22099, Reward: [-1726.654 -1726.654 -1726.654] [0.0000], Avg: [-1595.079 -1595.079 -1595.079] (1.000)
Step: 22149, Reward: [-1684.528 -1684.528 -1684.528] [0.0000], Avg: [-1595.281 -1595.281 -1595.281] (1.000)
Step: 22199, Reward: [-2217.747 -2217.747 -2217.747] [0.0000], Avg: [-1596.683 -1596.683 -1596.683] (1.000)
Step: 22249, Reward: [-1774.517 -1774.517 -1774.517] [0.0000], Avg: [-1597.082 -1597.082 -1597.082] (1.000)
Step: 22299, Reward: [-1765.6 -1765.6 -1765.6] [0.0000], Avg: [-1597.46 -1597.46 -1597.46] (1.000)
Step: 22349, Reward: [-1848.903 -1848.903 -1848.903] [0.0000], Avg: [-1598.023 -1598.023 -1598.023] (1.000)
Step: 22399, Reward: [-1681.59 -1681.59 -1681.59] [0.0000], Avg: [-1598.209 -1598.209 -1598.209] (1.000)
Step: 22449, Reward: [-1477.734 -1477.734 -1477.734] [0.0000], Avg: [-1597.941 -1597.941 -1597.941] (1.000)
Step: 22499, Reward: [-1832.311 -1832.311 -1832.311] [0.0000], Avg: [-1598.462 -1598.462 -1598.462] (1.000)
Step: 22549, Reward: [-1871.432 -1871.432 -1871.432] [0.0000], Avg: [-1599.067 -1599.067 -1599.067] (1.000)
Step: 22599, Reward: [-1393.298 -1393.298 -1393.298] [0.0000], Avg: [-1598.612 -1598.612 -1598.612] (1.000)
Step: 22649, Reward: [-1547.123 -1547.123 -1547.123] [0.0000], Avg: [-1598.498 -1598.498 -1598.498] (1.000)
Step: 22699, Reward: [-1485.907 -1485.907 -1485.907] [0.0000], Avg: [-1598.25 -1598.25 -1598.25] (1.000)
Step: 22749, Reward: [-1780.004 -1780.004 -1780.004] [0.0000], Avg: [-1598.649 -1598.649 -1598.649] (1.000)
Step: 22799, Reward: [-1914.327 -1914.327 -1914.327] [0.0000], Avg: [-1599.342 -1599.342 -1599.342] (1.000)
Step: 22849, Reward: [-1907.109 -1907.109 -1907.109] [0.0000], Avg: [-1600.015 -1600.015 -1600.015] (1.000)
Step: 22899, Reward: [-2238.582 -2238.582 -2238.582] [0.0000], Avg: [-1601.409 -1601.409 -1601.409] (1.000)
Step: 22949, Reward: [-2000.828 -2000.828 -2000.828] [0.0000], Avg: [-1602.28 -1602.28 -1602.28] (1.000)
Step: 22999, Reward: [-1857.911 -1857.911 -1857.911] [0.0000], Avg: [-1602.835 -1602.835 -1602.835] (1.000)
Step: 23049, Reward: [-1555.243 -1555.243 -1555.243] [0.0000], Avg: [-1602.732 -1602.732 -1602.732] (1.000)
Step: 23099, Reward: [-1556.506 -1556.506 -1556.506] [0.0000], Avg: [-1602.632 -1602.632 -1602.632] (1.000)
Step: 23149, Reward: [-1881.413 -1881.413 -1881.413] [0.0000], Avg: [-1603.234 -1603.234 -1603.234] (1.000)
Step: 23199, Reward: [-1918.018 -1918.018 -1918.018] [0.0000], Avg: [-1603.913 -1603.913 -1603.913] (1.000)
Step: 23249, Reward: [-1660.249 -1660.249 -1660.249] [0.0000], Avg: [-1604.034 -1604.034 -1604.034] (1.000)
Step: 23299, Reward: [-1941.103 -1941.103 -1941.103] [0.0000], Avg: [-1604.757 -1604.757 -1604.757] (1.000)
Step: 23349, Reward: [-2100.533 -2100.533 -2100.533] [0.0000], Avg: [-1605.819 -1605.819 -1605.819] (1.000)
Step: 23399, Reward: [-2041.532 -2041.532 -2041.532] [0.0000], Avg: [-1606.75 -1606.75 -1606.75] (1.000)
Step: 23449, Reward: [-2315.366 -2315.366 -2315.366] [0.0000], Avg: [-1608.261 -1608.261 -1608.261] (1.000)
Step: 23499, Reward: [-1804.489 -1804.489 -1804.489] [0.0000], Avg: [-1608.678 -1608.678 -1608.678] (1.000)
Step: 23549, Reward: [-1736.604 -1736.604 -1736.604] [0.0000], Avg: [-1608.95 -1608.95 -1608.95] (1.000)
Step: 23599, Reward: [-1936.065 -1936.065 -1936.065] [0.0000], Avg: [-1609.643 -1609.643 -1609.643] (1.000)
Step: 23649, Reward: [-2321.713 -2321.713 -2321.713] [0.0000], Avg: [-1611.148 -1611.148 -1611.148] (1.000)
Step: 23699, Reward: [-1753.562 -1753.562 -1753.562] [0.0000], Avg: [-1611.449 -1611.449 -1611.449] (1.000)
Step: 23749, Reward: [-1781.925 -1781.925 -1781.925] [0.0000], Avg: [-1611.808 -1611.808 -1611.808] (1.000)
Step: 23799, Reward: [-2204.691 -2204.691 -2204.691] [0.0000], Avg: [-1613.053 -1613.053 -1613.053] (1.000)
Step: 23849, Reward: [-2009.247 -2009.247 -2009.247] [0.0000], Avg: [-1613.884 -1613.884 -1613.884] (1.000)
Step: 23899, Reward: [-2303.864 -2303.864 -2303.864] [0.0000], Avg: [-1615.327 -1615.327 -1615.327] (1.000)
Step: 23949, Reward: [-1676.82 -1676.82 -1676.82] [0.0000], Avg: [-1615.456 -1615.456 -1615.456] (1.000)
Step: 23999, Reward: [-2166.411 -2166.411 -2166.411] [0.0000], Avg: [-1616.603 -1616.603 -1616.603] (1.000)
Step: 24049, Reward: [-1851.919 -1851.919 -1851.919] [0.0000], Avg: [-1617.093 -1617.093 -1617.093] (1.000)
Step: 24099, Reward: [-2001.035 -2001.035 -2001.035] [0.0000], Avg: [-1617.889 -1617.889 -1617.889] (1.000)
Step: 24149, Reward: [-2150.021 -2150.021 -2150.021] [0.0000], Avg: [-1618.991 -1618.991 -1618.991] (1.000)
Step: 24199, Reward: [-1677.933 -1677.933 -1677.933] [0.0000], Avg: [-1619.113 -1619.113 -1619.113] (1.000)
Step: 24249, Reward: [-1715.997 -1715.997 -1715.997] [0.0000], Avg: [-1619.312 -1619.312 -1619.312] (1.000)
Step: 24299, Reward: [-2335.726 -2335.726 -2335.726] [0.0000], Avg: [-1620.787 -1620.787 -1620.787] (1.000)
Step: 24349, Reward: [-1196.495 -1196.495 -1196.495] [0.0000], Avg: [-1619.915 -1619.915 -1619.915] (1.000)
Step: 24399, Reward: [-2009.098 -2009.098 -2009.098] [0.0000], Avg: [-1620.713 -1620.713 -1620.713] (1.000)
Step: 24449, Reward: [-1809.76 -1809.76 -1809.76] [0.0000], Avg: [-1621.099 -1621.099 -1621.099] (1.000)
Step: 24499, Reward: [-1897.25 -1897.25 -1897.25] [0.0000], Avg: [-1621.663 -1621.663 -1621.663] (1.000)
Step: 24549, Reward: [-1700.936 -1700.936 -1700.936] [0.0000], Avg: [-1621.824 -1621.824 -1621.824] (1.000)
Step: 24599, Reward: [-1860.417 -1860.417 -1860.417] [0.0000], Avg: [-1622.309 -1622.309 -1622.309] (1.000)
Step: 24649, Reward: [-1636.368 -1636.368 -1636.368] [0.0000], Avg: [-1622.338 -1622.338 -1622.338] (1.000)
Step: 24699, Reward: [-1874.685 -1874.685 -1874.685] [0.0000], Avg: [-1622.849 -1622.849 -1622.849] (1.000)
Step: 24749, Reward: [-2104.033 -2104.033 -2104.033] [0.0000], Avg: [-1623.821 -1623.821 -1623.821] (1.000)
Step: 24799, Reward: [-1651.944 -1651.944 -1651.944] [0.0000], Avg: [-1623.877 -1623.877 -1623.877] (1.000)
Step: 24849, Reward: [-1402.403 -1402.403 -1402.403] [0.0000], Avg: [-1623.432 -1623.432 -1623.432] (1.000)
Step: 24899, Reward: [-2174.759 -2174.759 -2174.759] [0.0000], Avg: [-1624.539 -1624.539 -1624.539] (1.000)
Step: 24949, Reward: [-1817.316 -1817.316 -1817.316] [0.0000], Avg: [-1624.925 -1624.925 -1624.925] (1.000)
Step: 24999, Reward: [-2071.062 -2071.062 -2071.062] [0.0000], Avg: [-1625.818 -1625.818 -1625.818] (1.000)
Step: 25049, Reward: [-1726.992 -1726.992 -1726.992] [0.0000], Avg: [-1626.019 -1626.019 -1626.019] (1.000)
Step: 25099, Reward: [-1946.268 -1946.268 -1946.268] [0.0000], Avg: [-1626.657 -1626.657 -1626.657] (1.000)
Step: 25149, Reward: [-1690.215 -1690.215 -1690.215] [0.0000], Avg: [-1626.784 -1626.784 -1626.784] (1.000)
Step: 25199, Reward: [-1758.015 -1758.015 -1758.015] [0.0000], Avg: [-1627.044 -1627.044 -1627.044] (1.000)
Step: 25249, Reward: [-1760.288 -1760.288 -1760.288] [0.0000], Avg: [-1627.308 -1627.308 -1627.308] (1.000)
Step: 25299, Reward: [-1999.99 -1999.99 -1999.99] [0.0000], Avg: [-1628.045 -1628.045 -1628.045] (1.000)
Step: 25349, Reward: [-1787.936 -1787.936 -1787.936] [0.0000], Avg: [-1628.36 -1628.36 -1628.36] (1.000)
Step: 25399, Reward: [-1612.354 -1612.354 -1612.354] [0.0000], Avg: [-1628.328 -1628.328 -1628.328] (1.000)
Step: 25449, Reward: [-1242.745 -1242.745 -1242.745] [0.0000], Avg: [-1627.571 -1627.571 -1627.571] (1.000)
Step: 25499, Reward: [-1540.522 -1540.522 -1540.522] [0.0000], Avg: [-1627.4 -1627.4 -1627.4] (1.000)
Step: 25549, Reward: [-1725.197 -1725.197 -1725.197] [0.0000], Avg: [-1627.592 -1627.592 -1627.592] (1.000)
Step: 25599, Reward: [-1811.465 -1811.465 -1811.465] [0.0000], Avg: [-1627.951 -1627.951 -1627.951] (1.000)
Step: 25649, Reward: [-1795.427 -1795.427 -1795.427] [0.0000], Avg: [-1628.277 -1628.277 -1628.277] (1.000)
Step: 25699, Reward: [-1971.289 -1971.289 -1971.289] [0.0000], Avg: [-1628.944 -1628.944 -1628.944] (1.000)
Step: 25749, Reward: [-1482.279 -1482.279 -1482.279] [0.0000], Avg: [-1628.66 -1628.66 -1628.66] (1.000)
Step: 25799, Reward: [-1697.636 -1697.636 -1697.636] [0.0000], Avg: [-1628.793 -1628.793 -1628.793] (1.000)
Step: 25849, Reward: [-1519.232 -1519.232 -1519.232] [0.0000], Avg: [-1628.581 -1628.581 -1628.581] (1.000)
Step: 25899, Reward: [-1821.048 -1821.048 -1821.048] [0.0000], Avg: [-1628.953 -1628.953 -1628.953] (1.000)
Step: 25949, Reward: [-1655.084 -1655.084 -1655.084] [0.0000], Avg: [-1629.003 -1629.003 -1629.003] (1.000)
Step: 25999, Reward: [-1866.98 -1866.98 -1866.98] [0.0000], Avg: [-1629.461 -1629.461 -1629.461] (1.000)
Step: 26049, Reward: [-2034.531 -2034.531 -2034.531] [0.0000], Avg: [-1630.238 -1630.238 -1630.238] (1.000)
Step: 26099, Reward: [-2106.511 -2106.511 -2106.511] [0.0000], Avg: [-1631.151 -1631.151 -1631.151] (1.000)
Step: 26149, Reward: [-1569.326 -1569.326 -1569.326] [0.0000], Avg: [-1631.033 -1631.033 -1631.033] (1.000)
Step: 26199, Reward: [-2112.175 -2112.175 -2112.175] [0.0000], Avg: [-1631.951 -1631.951 -1631.951] (1.000)
Step: 26249, Reward: [-1413.911 -1413.911 -1413.911] [0.0000], Avg: [-1631.536 -1631.536 -1631.536] (1.000)
Step: 26299, Reward: [-1862.513 -1862.513 -1862.513] [0.0000], Avg: [-1631.975 -1631.975 -1631.975] (1.000)
Step: 26349, Reward: [-1793.176 -1793.176 -1793.176] [0.0000], Avg: [-1632.281 -1632.281 -1632.281] (1.000)
Step: 26399, Reward: [-1646.132 -1646.132 -1646.132] [0.0000], Avg: [-1632.307 -1632.307 -1632.307] (1.000)
Step: 26449, Reward: [-1871.208 -1871.208 -1871.208] [0.0000], Avg: [-1632.758 -1632.758 -1632.758] (1.000)
Step: 26499, Reward: [-1287.07 -1287.07 -1287.07] [0.0000], Avg: [-1632.106 -1632.106 -1632.106] (1.000)
Step: 26549, Reward: [-1982.478 -1982.478 -1982.478] [0.0000], Avg: [-1632.766 -1632.766 -1632.766] (1.000)
Step: 26599, Reward: [-1776.269 -1776.269 -1776.269] [0.0000], Avg: [-1633.036 -1633.036 -1633.036] (1.000)
Step: 26649, Reward: [-1022.477 -1022.477 -1022.477] [0.0000], Avg: [-1631.89 -1631.89 -1631.89] (1.000)
Step: 26699, Reward: [-1269.673 -1269.673 -1269.673] [0.0000], Avg: [-1631.212 -1631.212 -1631.212] (1.000)
Step: 26749, Reward: [-1470.086 -1470.086 -1470.086] [0.0000], Avg: [-1630.911 -1630.911 -1630.911] (1.000)
Step: 26799, Reward: [-1450.372 -1450.372 -1450.372] [0.0000], Avg: [-1630.574 -1630.574 -1630.574] (1.000)
Step: 26849, Reward: [-1420.07 -1420.07 -1420.07] [0.0000], Avg: [-1630.182 -1630.182 -1630.182] (1.000)
Step: 26899, Reward: [-1389.557 -1389.557 -1389.557] [0.0000], Avg: [-1629.735 -1629.735 -1629.735] (1.000)
Step: 26949, Reward: [-1647.253 -1647.253 -1647.253] [0.0000], Avg: [-1629.767 -1629.767 -1629.767] (1.000)
Step: 26999, Reward: [-1865.03 -1865.03 -1865.03] [0.0000], Avg: [-1630.203 -1630.203 -1630.203] (1.000)
Step: 27049, Reward: [-1692.381 -1692.381 -1692.381] [0.0000], Avg: [-1630.318 -1630.318 -1630.318] (1.000)
Step: 27099, Reward: [-1739.395 -1739.395 -1739.395] [0.0000], Avg: [-1630.519 -1630.519 -1630.519] (1.000)
Step: 27149, Reward: [-1918.25 -1918.25 -1918.25] [0.0000], Avg: [-1631.049 -1631.049 -1631.049] (1.000)
Step: 27199, Reward: [-1101.479 -1101.479 -1101.479] [0.0000], Avg: [-1630.075 -1630.075 -1630.075] (1.000)
Step: 27249, Reward: [-1973.681 -1973.681 -1973.681] [0.0000], Avg: [-1630.706 -1630.706 -1630.706] (1.000)
Step: 27299, Reward: [-1895.117 -1895.117 -1895.117] [0.0000], Avg: [-1631.19 -1631.19 -1631.19] (1.000)
Step: 27349, Reward: [-1474.365 -1474.365 -1474.365] [0.0000], Avg: [-1630.903 -1630.903 -1630.903] (1.000)
Step: 27399, Reward: [-1432.305 -1432.305 -1432.305] [0.0000], Avg: [-1630.541 -1630.541 -1630.541] (1.000)
Step: 27449, Reward: [-1676.891 -1676.891 -1676.891] [0.0000], Avg: [-1630.626 -1630.626 -1630.626] (1.000)
Step: 27499, Reward: [-1972.414 -1972.414 -1972.414] [0.0000], Avg: [-1631.247 -1631.247 -1631.247] (1.000)
Step: 27549, Reward: [-1818.212 -1818.212 -1818.212] [0.0000], Avg: [-1631.586 -1631.586 -1631.586] (1.000)
Step: 27599, Reward: [-2082.889 -2082.889 -2082.889] [0.0000], Avg: [-1632.404 -1632.404 -1632.404] (1.000)
Step: 27649, Reward: [-1368.395 -1368.395 -1368.395] [0.0000], Avg: [-1631.926 -1631.926 -1631.926] (1.000)
Step: 27699, Reward: [-1867.95 -1867.95 -1867.95] [0.0000], Avg: [-1632.352 -1632.352 -1632.352] (1.000)
Step: 27749, Reward: [-895.8 -895.8 -895.8] [0.0000], Avg: [-1631.025 -1631.025 -1631.025] (1.000)
Step: 27799, Reward: [-1471.137 -1471.137 -1471.137] [0.0000], Avg: [-1630.738 -1630.738 -1630.738] (1.000)
Step: 27849, Reward: [-1822.165 -1822.165 -1822.165] [0.0000], Avg: [-1631.081 -1631.081 -1631.081] (1.000)
Step: 27899, Reward: [-1938.118 -1938.118 -1938.118] [0.0000], Avg: [-1631.632 -1631.632 -1631.632] (1.000)
Step: 27949, Reward: [-1880.367 -1880.367 -1880.367] [0.0000], Avg: [-1632.077 -1632.077 -1632.077] (1.000)
Step: 27999, Reward: [-2073.071 -2073.071 -2073.071] [0.0000], Avg: [-1632.864 -1632.864 -1632.864] (1.000)
Step: 28049, Reward: [-2249.883 -2249.883 -2249.883] [0.0000], Avg: [-1633.964 -1633.964 -1633.964] (1.000)
Step: 28099, Reward: [-1368.109 -1368.109 -1368.109] [0.0000], Avg: [-1633.491 -1633.491 -1633.491] (1.000)
Step: 28149, Reward: [-1389.123 -1389.123 -1389.123] [0.0000], Avg: [-1633.057 -1633.057 -1633.057] (1.000)
Step: 28199, Reward: [-1754.973 -1754.973 -1754.973] [0.0000], Avg: [-1633.273 -1633.273 -1633.273] (1.000)
Step: 28249, Reward: [-1086.205 -1086.205 -1086.205] [0.0000], Avg: [-1632.305 -1632.305 -1632.305] (1.000)
Step: 28299, Reward: [-1351.296 -1351.296 -1351.296] [0.0000], Avg: [-1631.808 -1631.808 -1631.808] (1.000)
Step: 28349, Reward: [-1118.43 -1118.43 -1118.43] [0.0000], Avg: [-1630.903 -1630.903 -1630.903] (1.000)
Step: 28399, Reward: [-1493.409 -1493.409 -1493.409] [0.0000], Avg: [-1630.661 -1630.661 -1630.661] (1.000)
Step: 28449, Reward: [-1335.721 -1335.721 -1335.721] [0.0000], Avg: [-1630.142 -1630.142 -1630.142] (1.000)
Step: 28499, Reward: [-1359.963 -1359.963 -1359.963] [0.0000], Avg: [-1629.668 -1629.668 -1629.668] (1.000)
Step: 28549, Reward: [-1265.868 -1265.868 -1265.868] [0.0000], Avg: [-1629.031 -1629.031 -1629.031] (1.000)
Step: 28599, Reward: [-1933.46 -1933.46 -1933.46] [0.0000], Avg: [-1629.564 -1629.564 -1629.564] (1.000)
Step: 28649, Reward: [-1369.719 -1369.719 -1369.719] [0.0000], Avg: [-1629.11 -1629.11 -1629.11] (1.000)
Step: 28699, Reward: [-1817.807 -1817.807 -1817.807] [0.0000], Avg: [-1629.439 -1629.439 -1629.439] (1.000)
Step: 28749, Reward: [-1789.872 -1789.872 -1789.872] [0.0000], Avg: [-1629.718 -1629.718 -1629.718] (1.000)
Step: 28799, Reward: [-826.39 -826.39 -826.39] [0.0000], Avg: [-1628.323 -1628.323 -1628.323] (1.000)
Step: 28849, Reward: [-1479.556 -1479.556 -1479.556] [0.0000], Avg: [-1628.065 -1628.065 -1628.065] (1.000)
Step: 28899, Reward: [-1179.931 -1179.931 -1179.931] [0.0000], Avg: [-1627.29 -1627.29 -1627.29] (1.000)
Step: 28949, Reward: [-1389.665 -1389.665 -1389.665] [0.0000], Avg: [-1626.88 -1626.88 -1626.88] (1.000)
Step: 28999, Reward: [-1374.7 -1374.7 -1374.7] [0.0000], Avg: [-1626.445 -1626.445 -1626.445] (1.000)
Step: 29049, Reward: [-1330.608 -1330.608 -1330.608] [0.0000], Avg: [-1625.936 -1625.936 -1625.936] (1.000)
Step: 29099, Reward: [-1327.002 -1327.002 -1327.002] [0.0000], Avg: [-1625.422 -1625.422 -1625.422] (1.000)
Step: 29149, Reward: [-1581.581 -1581.581 -1581.581] [0.0000], Avg: [-1625.347 -1625.347 -1625.347] (1.000)
Step: 29199, Reward: [-1210.203 -1210.203 -1210.203] [0.0000], Avg: [-1624.636 -1624.636 -1624.636] (1.000)
Step: 29249, Reward: [-1386.941 -1386.941 -1386.941] [0.0000], Avg: [-1624.23 -1624.23 -1624.23] (1.000)
Step: 29299, Reward: [-873.75 -873.75 -873.75] [0.0000], Avg: [-1622.949 -1622.949 -1622.949] (1.000)
Step: 29349, Reward: [-1454.287 -1454.287 -1454.287] [0.0000], Avg: [-1622.662 -1622.662 -1622.662] (1.000)
Step: 29399, Reward: [-904.06 -904.06 -904.06] [0.0000], Avg: [-1621.44 -1621.44 -1621.44] (1.000)
Step: 29449, Reward: [-1283.145 -1283.145 -1283.145] [0.0000], Avg: [-1620.865 -1620.865 -1620.865] (1.000)
Step: 29499, Reward: [-2191.031 -2191.031 -2191.031] [0.0000], Avg: [-1621.832 -1621.832 -1621.832] (1.000)
Step: 29549, Reward: [-1045.096 -1045.096 -1045.096] [0.0000], Avg: [-1620.856 -1620.856 -1620.856] (1.000)
Step: 29599, Reward: [-1324.108 -1324.108 -1324.108] [0.0000], Avg: [-1620.354 -1620.354 -1620.354] (1.000)
Step: 29649, Reward: [-1754.737 -1754.737 -1754.737] [0.0000], Avg: [-1620.581 -1620.581 -1620.581] (1.000)
Step: 29699, Reward: [-1490.737 -1490.737 -1490.737] [0.0000], Avg: [-1620.362 -1620.362 -1620.362] (1.000)
Step: 29749, Reward: [-1735.706 -1735.706 -1735.706] [0.0000], Avg: [-1620.556 -1620.556 -1620.556] (1.000)
Step: 29799, Reward: [-1059.448 -1059.448 -1059.448] [0.0000], Avg: [-1619.615 -1619.615 -1619.615] (1.000)
Step: 29849, Reward: [-1091.457 -1091.457 -1091.457] [0.0000], Avg: [-1618.73 -1618.73 -1618.73] (1.000)
Step: 29899, Reward: [-1782.548 -1782.548 -1782.548] [0.0000], Avg: [-1619.004 -1619.004 -1619.004] (1.000)
Step: 29949, Reward: [-1758.007 -1758.007 -1758.007] [0.0000], Avg: [-1619.236 -1619.236 -1619.236] (1.000)
Step: 29999, Reward: [-1839.612 -1839.612 -1839.612] [0.0000], Avg: [-1619.603 -1619.603 -1619.603] (1.000)
Step: 30049, Reward: [-1607.873 -1607.873 -1607.873] [0.0000], Avg: [-1619.584 -1619.584 -1619.584] (1.000)
Step: 30099, Reward: [-2080.97 -2080.97 -2080.97] [0.0000], Avg: [-1620.35 -1620.35 -1620.35] (1.000)
Step: 30149, Reward: [-1713.382 -1713.382 -1713.382] [0.0000], Avg: [-1620.505 -1620.505 -1620.505] (1.000)
Step: 30199, Reward: [-1157.609 -1157.609 -1157.609] [0.0000], Avg: [-1619.738 -1619.738 -1619.738] (1.000)
Step: 30249, Reward: [-1558.244 -1558.244 -1558.244] [0.0000], Avg: [-1619.637 -1619.637 -1619.637] (1.000)
Step: 30299, Reward: [-1527.902 -1527.902 -1527.902] [0.0000], Avg: [-1619.485 -1619.485 -1619.485] (1.000)
Step: 30349, Reward: [-1163.939 -1163.939 -1163.939] [0.0000], Avg: [-1618.735 -1618.735 -1618.735] (1.000)
Step: 30399, Reward: [-2172.947 -2172.947 -2172.947] [0.0000], Avg: [-1619.646 -1619.646 -1619.646] (1.000)
Step: 30449, Reward: [-1584.841 -1584.841 -1584.841] [0.0000], Avg: [-1619.589 -1619.589 -1619.589] (1.000)
Step: 30499, Reward: [-1666.71 -1666.71 -1666.71] [0.0000], Avg: [-1619.666 -1619.666 -1619.666] (1.000)
Step: 30549, Reward: [-1567.2 -1567.2 -1567.2] [0.0000], Avg: [-1619.58 -1619.58 -1619.58] (1.000)
Step: 30599, Reward: [-1913.908 -1913.908 -1913.908] [0.0000], Avg: [-1620.061 -1620.061 -1620.061] (1.000)
Step: 30649, Reward: [-1300.04 -1300.04 -1300.04] [0.0000], Avg: [-1619.539 -1619.539 -1619.539] (1.000)
Step: 30699, Reward: [-1325.404 -1325.404 -1325.404] [0.0000], Avg: [-1619.06 -1619.06 -1619.06] (1.000)
Step: 30749, Reward: [-1652.91 -1652.91 -1652.91] [0.0000], Avg: [-1619.115 -1619.115 -1619.115] (1.000)
Step: 30799, Reward: [-1577.236 -1577.236 -1577.236] [0.0000], Avg: [-1619.047 -1619.047 -1619.047] (1.000)
Step: 30849, Reward: [-1559.937 -1559.937 -1559.937] [0.0000], Avg: [-1618.952 -1618.952 -1618.952] (1.000)
Step: 30899, Reward: [-1376.441 -1376.441 -1376.441] [0.0000], Avg: [-1618.559 -1618.559 -1618.559] (1.000)
Step: 30949, Reward: [-2279.284 -2279.284 -2279.284] [0.0000], Avg: [-1619.627 -1619.627 -1619.627] (1.000)
Step: 30999, Reward: [-1469.947 -1469.947 -1469.947] [0.0000], Avg: [-1619.385 -1619.385 -1619.385] (1.000)
Step: 31049, Reward: [-1826.497 -1826.497 -1826.497] [0.0000], Avg: [-1619.719 -1619.719 -1619.719] (1.000)
Step: 31099, Reward: [-1789.568 -1789.568 -1789.568] [0.0000], Avg: [-1619.992 -1619.992 -1619.992] (1.000)
Step: 31149, Reward: [-1160.623 -1160.623 -1160.623] [0.0000], Avg: [-1619.254 -1619.254 -1619.254] (1.000)
Step: 31199, Reward: [-1724.737 -1724.737 -1724.737] [0.0000], Avg: [-1619.423 -1619.423 -1619.423] (1.000)
Step: 31249, Reward: [-2094.014 -2094.014 -2094.014] [0.0000], Avg: [-1620.183 -1620.183 -1620.183] (1.000)
Step: 31299, Reward: [-1251.518 -1251.518 -1251.518] [0.0000], Avg: [-1619.594 -1619.594 -1619.594] (1.000)
Step: 31349, Reward: [-1248.753 -1248.753 -1248.753] [0.0000], Avg: [-1619.002 -1619.002 -1619.002] (1.000)
Step: 31399, Reward: [-1843.393 -1843.393 -1843.393] [0.0000], Avg: [-1619.36 -1619.36 -1619.36] (1.000)
Step: 31449, Reward: [-1889.533 -1889.533 -1889.533] [0.0000], Avg: [-1619.789 -1619.789 -1619.789] (1.000)
Step: 31499, Reward: [-1883.083 -1883.083 -1883.083] [0.0000], Avg: [-1620.207 -1620.207 -1620.207] (1.000)
Step: 31549, Reward: [-1572.395 -1572.395 -1572.395] [0.0000], Avg: [-1620.131 -1620.131 -1620.131] (1.000)
Step: 31599, Reward: [-1725.496 -1725.496 -1725.496] [0.0000], Avg: [-1620.298 -1620.298 -1620.298] (1.000)
Step: 31649, Reward: [-2078.53 -2078.53 -2078.53] [0.0000], Avg: [-1621.022 -1621.022 -1621.022] (1.000)
Step: 31699, Reward: [-1814.12 -1814.12 -1814.12] [0.0000], Avg: [-1621.327 -1621.327 -1621.327] (1.000)
Step: 31749, Reward: [-1579.358 -1579.358 -1579.358] [0.0000], Avg: [-1621.26 -1621.26 -1621.26] (1.000)
Step: 31799, Reward: [-931.586 -931.586 -931.586] [0.0000], Avg: [-1620.176 -1620.176 -1620.176] (1.000)
Step: 31849, Reward: [-1312.82 -1312.82 -1312.82] [0.0000], Avg: [-1619.694 -1619.694 -1619.694] (1.000)
Step: 31899, Reward: [-1187.139 -1187.139 -1187.139] [0.0000], Avg: [-1619.016 -1619.016 -1619.016] (1.000)
Step: 31949, Reward: [-1463.687 -1463.687 -1463.687] [0.0000], Avg: [-1618.773 -1618.773 -1618.773] (1.000)
Step: 31999, Reward: [-940.252 -940.252 -940.252] [0.0000], Avg: [-1617.712 -1617.712 -1617.712] (1.000)
Step: 32049, Reward: [-872.037 -872.037 -872.037] [0.0000], Avg: [-1616.549 -1616.549 -1616.549] (1.000)
Step: 32099, Reward: [-1374.656 -1374.656 -1374.656] [0.0000], Avg: [-1616.172 -1616.172 -1616.172] (1.000)
Step: 32149, Reward: [-1232.669 -1232.669 -1232.669] [0.0000], Avg: [-1615.576 -1615.576 -1615.576] (1.000)
Step: 32199, Reward: [-976.957 -976.957 -976.957] [0.0000], Avg: [-1614.584 -1614.584 -1614.584] (1.000)
Step: 32249, Reward: [-1986.377 -1986.377 -1986.377] [0.0000], Avg: [-1615.161 -1615.161 -1615.161] (1.000)
Step: 32299, Reward: [-1335.536 -1335.536 -1335.536] [0.0000], Avg: [-1614.728 -1614.728 -1614.728] (1.000)
Step: 32349, Reward: [-1636.536 -1636.536 -1636.536] [0.0000], Avg: [-1614.761 -1614.761 -1614.761] (1.000)
Step: 32399, Reward: [-1268.808 -1268.808 -1268.808] [0.0000], Avg: [-1614.228 -1614.228 -1614.228] (1.000)
Step: 32449, Reward: [-1065.937 -1065.937 -1065.937] [0.0000], Avg: [-1613.383 -1613.383 -1613.383] (1.000)
Step: 32499, Reward: [-1363.249 -1363.249 -1363.249] [0.0000], Avg: [-1612.998 -1612.998 -1612.998] (1.000)
Step: 32549, Reward: [-910.052 -910.052 -910.052] [0.0000], Avg: [-1611.918 -1611.918 -1611.918] (1.000)
Step: 32599, Reward: [-1530.294 -1530.294 -1530.294] [0.0000], Avg: [-1611.793 -1611.793 -1611.793] (1.000)
Step: 32649, Reward: [-1650.585 -1650.585 -1650.585] [0.0000], Avg: [-1611.852 -1611.852 -1611.852] (1.000)
Step: 32699, Reward: [-1683.185 -1683.185 -1683.185] [0.0000], Avg: [-1611.961 -1611.961 -1611.961] (1.000)
Step: 32749, Reward: [-1035.787 -1035.787 -1035.787] [0.0000], Avg: [-1611.082 -1611.082 -1611.082] (1.000)
Step: 32799, Reward: [-1433.882 -1433.882 -1433.882] [0.0000], Avg: [-1610.812 -1610.812 -1610.812] (1.000)
Step: 32849, Reward: [-721.372 -721.372 -721.372] [0.0000], Avg: [-1609.458 -1609.458 -1609.458] (1.000)
Step: 32899, Reward: [-1372.154 -1372.154 -1372.154] [0.0000], Avg: [-1609.097 -1609.097 -1609.097] (1.000)
Step: 32949, Reward: [-764.72 -764.72 -764.72] [0.0000], Avg: [-1607.816 -1607.816 -1607.816] (1.000)
Step: 32999, Reward: [-1686.767 -1686.767 -1686.767] [0.0000], Avg: [-1607.936 -1607.936 -1607.936] (1.000)
Step: 33049, Reward: [-2064.293 -2064.293 -2064.293] [0.0000], Avg: [-1608.626 -1608.626 -1608.626] (1.000)
Step: 33099, Reward: [-1196.081 -1196.081 -1196.081] [0.0000], Avg: [-1608.003 -1608.003 -1608.003] (1.000)
Step: 33149, Reward: [-1817.594 -1817.594 -1817.594] [0.0000], Avg: [-1608.319 -1608.319 -1608.319] (1.000)
Step: 33199, Reward: [-1470.619 -1470.619 -1470.619] [0.0000], Avg: [-1608.112 -1608.112 -1608.112] (1.000)
Step: 33249, Reward: [-1694.308 -1694.308 -1694.308] [0.0000], Avg: [-1608.241 -1608.241 -1608.241] (1.000)
Step: 33299, Reward: [-962.221 -962.221 -962.221] [0.0000], Avg: [-1607.271 -1607.271 -1607.271] (1.000)
Step: 33349, Reward: [-1583.61 -1583.61 -1583.61] [0.0000], Avg: [-1607.236 -1607.236 -1607.236] (1.000)
Step: 33399, Reward: [-1150.996 -1150.996 -1150.996] [0.0000], Avg: [-1606.553 -1606.553 -1606.553] (1.000)
Step: 33449, Reward: [-1725.98 -1725.98 -1725.98] [0.0000], Avg: [-1606.731 -1606.731 -1606.731] (1.000)
Step: 33499, Reward: [-1189.081 -1189.081 -1189.081] [0.0000], Avg: [-1606.108 -1606.108 -1606.108] (1.000)
Step: 33549, Reward: [-1982.72 -1982.72 -1982.72] [0.0000], Avg: [-1606.669 -1606.669 -1606.669] (1.000)
Step: 33599, Reward: [-1910.069 -1910.069 -1910.069] [0.0000], Avg: [-1607.121 -1607.121 -1607.121] (1.000)
Step: 33649, Reward: [-2296.273 -2296.273 -2296.273] [0.0000], Avg: [-1608.145 -1608.145 -1608.145] (1.000)
Step: 33699, Reward: [-1070.407 -1070.407 -1070.407] [0.0000], Avg: [-1607.347 -1607.347 -1607.347] (1.000)
Step: 33749, Reward: [-1235.062 -1235.062 -1235.062] [0.0000], Avg: [-1606.795 -1606.795 -1606.795] (1.000)
Step: 33799, Reward: [-1953.848 -1953.848 -1953.848] [0.0000], Avg: [-1607.309 -1607.309 -1607.309] (1.000)
Step: 33849, Reward: [-1810.911 -1810.911 -1810.911] [0.0000], Avg: [-1607.609 -1607.609 -1607.609] (1.000)
Step: 33899, Reward: [-1109.648 -1109.648 -1109.648] [0.0000], Avg: [-1606.875 -1606.875 -1606.875] (1.000)
Step: 33949, Reward: [-1581.544 -1581.544 -1581.544] [0.0000], Avg: [-1606.838 -1606.838 -1606.838] (1.000)
Step: 33999, Reward: [-1830.083 -1830.083 -1830.083] [0.0000], Avg: [-1607.166 -1607.166 -1607.166] (1.000)
Step: 34049, Reward: [-1071.654 -1071.654 -1071.654] [0.0000], Avg: [-1606.38 -1606.38 -1606.38] (1.000)
Step: 34099, Reward: [-1860.999 -1860.999 -1860.999] [0.0000], Avg: [-1606.753 -1606.753 -1606.753] (1.000)
Step: 34149, Reward: [-1519.33 -1519.33 -1519.33] [0.0000], Avg: [-1606.625 -1606.625 -1606.625] (1.000)
Step: 34199, Reward: [-885.395 -885.395 -885.395] [0.0000], Avg: [-1605.57 -1605.57 -1605.57] (1.000)
Step: 34249, Reward: [-1409.608 -1409.608 -1409.608] [0.0000], Avg: [-1605.284 -1605.284 -1605.284] (1.000)
Step: 34299, Reward: [-1152.89 -1152.89 -1152.89] [0.0000], Avg: [-1604.625 -1604.625 -1604.625] (1.000)
Step: 34349, Reward: [-2100.696 -2100.696 -2100.696] [0.0000], Avg: [-1605.347 -1605.347 -1605.347] (1.000)
Step: 34399, Reward: [-1101.725 -1101.725 -1101.725] [0.0000], Avg: [-1604.615 -1604.615 -1604.615] (1.000)
Step: 34449, Reward: [-1529.648 -1529.648 -1529.648] [0.0000], Avg: [-1604.506 -1604.506 -1604.506] (1.000)
Step: 34499, Reward: [-2006.398 -2006.398 -2006.398] [0.0000], Avg: [-1605.089 -1605.089 -1605.089] (1.000)
Step: 34549, Reward: [-1512.354 -1512.354 -1512.354] [0.0000], Avg: [-1604.954 -1604.954 -1604.954] (1.000)
Step: 34599, Reward: [-1326.521 -1326.521 -1326.521] [0.0000], Avg: [-1604.552 -1604.552 -1604.552] (1.000)
Step: 34649, Reward: [-1879.309 -1879.309 -1879.309] [0.0000], Avg: [-1604.949 -1604.949 -1604.949] (1.000)
Step: 34699, Reward: [-2300.415 -2300.415 -2300.415] [0.0000], Avg: [-1605.951 -1605.951 -1605.951] (1.000)
Step: 34749, Reward: [-1498.162 -1498.162 -1498.162] [0.0000], Avg: [-1605.796 -1605.796 -1605.796] (1.000)
Step: 34799, Reward: [-2023.037 -2023.037 -2023.037] [0.0000], Avg: [-1606.395 -1606.395 -1606.395] (1.000)
Step: 34849, Reward: [-1840.889 -1840.889 -1840.889] [0.0000], Avg: [-1606.731 -1606.731 -1606.731] (1.000)
Step: 34899, Reward: [-1423.917 -1423.917 -1423.917] [0.0000], Avg: [-1606.47 -1606.47 -1606.47] (1.000)
Step: 34949, Reward: [-1833.028 -1833.028 -1833.028] [0.0000], Avg: [-1606.794 -1606.794 -1606.794] (1.000)
Step: 34999, Reward: [-2306.236 -2306.236 -2306.236] [0.0000], Avg: [-1607.793 -1607.793 -1607.793] (1.000)
Step: 35049, Reward: [-2011.458 -2011.458 -2011.458] [0.0000], Avg: [-1608.369 -1608.369 -1608.369] (1.000)
Step: 35099, Reward: [-1382.737 -1382.737 -1382.737] [0.0000], Avg: [-1608.047 -1608.047 -1608.047] (1.000)
Step: 35149, Reward: [-1994.834 -1994.834 -1994.834] [0.0000], Avg: [-1608.597 -1608.597 -1608.597] (1.000)
Step: 35199, Reward: [-1290.292 -1290.292 -1290.292] [0.0000], Avg: [-1608.145 -1608.145 -1608.145] (1.000)
Step: 35249, Reward: [-1309.059 -1309.059 -1309.059] [0.0000], Avg: [-1607.721 -1607.721 -1607.721] (1.000)
Step: 35299, Reward: [-1972.916 -1972.916 -1972.916] [0.0000], Avg: [-1608.238 -1608.238 -1608.238] (1.000)
Step: 35349, Reward: [-2083.995 -2083.995 -2083.995] [0.0000], Avg: [-1608.911 -1608.911 -1608.911] (1.000)
Step: 35399, Reward: [-1990.581 -1990.581 -1990.581] [0.0000], Avg: [-1609.45 -1609.45 -1609.45] (1.000)
Step: 35449, Reward: [-1797.145 -1797.145 -1797.145] [0.0000], Avg: [-1609.715 -1609.715 -1609.715] (1.000)
Step: 35499, Reward: [-2073.097 -2073.097 -2073.097] [0.0000], Avg: [-1610.368 -1610.368 -1610.368] (1.000)
Step: 35549, Reward: [-2103.437 -2103.437 -2103.437] [0.0000], Avg: [-1611.061 -1611.061 -1611.061] (1.000)
Step: 35599, Reward: [-1600.945 -1600.945 -1600.945] [0.0000], Avg: [-1611.047 -1611.047 -1611.047] (1.000)
Step: 35649, Reward: [-1819.464 -1819.464 -1819.464] [0.0000], Avg: [-1611.339 -1611.339 -1611.339] (1.000)
Step: 35699, Reward: [-2023.053 -2023.053 -2023.053] [0.0000], Avg: [-1611.916 -1611.916 -1611.916] (1.000)
Step: 35749, Reward: [-2360.689 -2360.689 -2360.689] [0.0000], Avg: [-1612.963 -1612.963 -1612.963] (1.000)
Step: 35799, Reward: [-2062.487 -2062.487 -2062.487] [0.0000], Avg: [-1613.591 -1613.591 -1613.591] (1.000)
Step: 35849, Reward: [-1669.979 -1669.979 -1669.979] [0.0000], Avg: [-1613.67 -1613.67 -1613.67] (1.000)
Step: 35899, Reward: [-1281.961 -1281.961 -1281.961] [0.0000], Avg: [-1613.208 -1613.208 -1613.208] (1.000)
Step: 35949, Reward: [-2167.886 -2167.886 -2167.886] [0.0000], Avg: [-1613.979 -1613.979 -1613.979] (1.000)
Step: 35999, Reward: [-2211.258 -2211.258 -2211.258] [0.0000], Avg: [-1614.809 -1614.809 -1614.809] (1.000)
Step: 36049, Reward: [-1555.5 -1555.5 -1555.5] [0.0000], Avg: [-1614.726 -1614.726 -1614.726] (1.000)
Step: 36099, Reward: [-1667.112 -1667.112 -1667.112] [0.0000], Avg: [-1614.799 -1614.799 -1614.799] (1.000)
Step: 36149, Reward: [-1170.155 -1170.155 -1170.155] [0.0000], Avg: [-1614.184 -1614.184 -1614.184] (1.000)
Step: 36199, Reward: [-1616.568 -1616.568 -1616.568] [0.0000], Avg: [-1614.187 -1614.187 -1614.187] (1.000)
Step: 36249, Reward: [-1850.934 -1850.934 -1850.934] [0.0000], Avg: [-1614.514 -1614.514 -1614.514] (1.000)
Step: 36299, Reward: [-1672.21 -1672.21 -1672.21] [0.0000], Avg: [-1614.593 -1614.593 -1614.593] (1.000)
Step: 36349, Reward: [-1757.606 -1757.606 -1757.606] [0.0000], Avg: [-1614.79 -1614.79 -1614.79] (1.000)
Step: 36399, Reward: [-2130.544 -2130.544 -2130.544] [0.0000], Avg: [-1615.499 -1615.499 -1615.499] (1.000)
Step: 36449, Reward: [-1194.135 -1194.135 -1194.135] [0.0000], Avg: [-1614.921 -1614.921 -1614.921] (1.000)
Step: 36499, Reward: [-1645.052 -1645.052 -1645.052] [0.0000], Avg: [-1614.962 -1614.962 -1614.962] (1.000)
Step: 36549, Reward: [-1865.801 -1865.801 -1865.801] [0.0000], Avg: [-1615.305 -1615.305 -1615.305] (1.000)
Step: 36599, Reward: [-1578.167 -1578.167 -1578.167] [0.0000], Avg: [-1615.254 -1615.254 -1615.254] (1.000)
Step: 36649, Reward: [-1661.168 -1661.168 -1661.168] [0.0000], Avg: [-1615.317 -1615.317 -1615.317] (1.000)
Step: 36699, Reward: [-1673.101 -1673.101 -1673.101] [0.0000], Avg: [-1615.396 -1615.396 -1615.396] (1.000)
Step: 36749, Reward: [-2002.342 -2002.342 -2002.342] [0.0000], Avg: [-1615.922 -1615.922 -1615.922] (1.000)
Step: 36799, Reward: [-1677.194 -1677.194 -1677.194] [0.0000], Avg: [-1616.005 -1616.005 -1616.005] (1.000)
Step: 36849, Reward: [-2199.249 -2199.249 -2199.249] [0.0000], Avg: [-1616.797 -1616.797 -1616.797] (1.000)
Step: 36899, Reward: [-2044.088 -2044.088 -2044.088] [0.0000], Avg: [-1617.376 -1617.376 -1617.376] (1.000)
Step: 36949, Reward: [-1719.944 -1719.944 -1719.944] [0.0000], Avg: [-1617.514 -1617.514 -1617.514] (1.000)
Step: 36999, Reward: [-2014.205 -2014.205 -2014.205] [0.0000], Avg: [-1618.05 -1618.05 -1618.05] (1.000)
Step: 37049, Reward: [-1468.893 -1468.893 -1468.893] [0.0000], Avg: [-1617.849 -1617.849 -1617.849] (1.000)
Step: 37099, Reward: [-1520.11 -1520.11 -1520.11] [0.0000], Avg: [-1617.717 -1617.717 -1617.717] (1.000)
Step: 37149, Reward: [-1876.189 -1876.189 -1876.189] [0.0000], Avg: [-1618.065 -1618.065 -1618.065] (1.000)
Step: 37199, Reward: [-1586.699 -1586.699 -1586.699] [0.0000], Avg: [-1618.023 -1618.023 -1618.023] (1.000)
Step: 37249, Reward: [-1604.498 -1604.498 -1604.498] [0.0000], Avg: [-1618.005 -1618.005 -1618.005] (1.000)
Step: 37299, Reward: [-1455.5 -1455.5 -1455.5] [0.0000], Avg: [-1617.787 -1617.787 -1617.787] (1.000)
Step: 37349, Reward: [-1661.478 -1661.478 -1661.478] [0.0000], Avg: [-1617.846 -1617.846 -1617.846] (1.000)
Step: 37399, Reward: [-1884.975 -1884.975 -1884.975] [0.0000], Avg: [-1618.203 -1618.203 -1618.203] (1.000)
Step: 37449, Reward: [-1586.242 -1586.242 -1586.242] [0.0000], Avg: [-1618.16 -1618.16 -1618.16] (1.000)
Step: 37499, Reward: [-2025.566 -2025.566 -2025.566] [0.0000], Avg: [-1618.703 -1618.703 -1618.703] (1.000)
Step: 37549, Reward: [-1585.586 -1585.586 -1585.586] [0.0000], Avg: [-1618.659 -1618.659 -1618.659] (1.000)
Step: 37599, Reward: [-1288.26 -1288.26 -1288.26] [0.0000], Avg: [-1618.22 -1618.22 -1618.22] (1.000)
Step: 37649, Reward: [-1466.407 -1466.407 -1466.407] [0.0000], Avg: [-1618.018 -1618.018 -1618.018] (1.000)
Step: 37699, Reward: [-2057.065 -2057.065 -2057.065] [0.0000], Avg: [-1618.601 -1618.601 -1618.601] (1.000)
Step: 37749, Reward: [-1675.673 -1675.673 -1675.673] [0.0000], Avg: [-1618.676 -1618.676 -1618.676] (1.000)
Step: 37799, Reward: [-1492.099 -1492.099 -1492.099] [0.0000], Avg: [-1618.509 -1618.509 -1618.509] (1.000)
Step: 37849, Reward: [-1466.436 -1466.436 -1466.436] [0.0000], Avg: [-1618.308 -1618.308 -1618.308] (1.000)
Step: 37899, Reward: [-1257.47 -1257.47 -1257.47] [0.0000], Avg: [-1617.832 -1617.832 -1617.832] (1.000)
Step: 37949, Reward: [-1053.022 -1053.022 -1053.022] [0.0000], Avg: [-1617.088 -1617.088 -1617.088] (1.000)
Step: 37999, Reward: [-1325.009 -1325.009 -1325.009] [0.0000], Avg: [-1616.703 -1616.703 -1616.703] (1.000)
Step: 38049, Reward: [-1808.074 -1808.074 -1808.074] [0.0000], Avg: [-1616.955 -1616.955 -1616.955] (1.000)
Step: 38099, Reward: [-1469.529 -1469.529 -1469.529] [0.0000], Avg: [-1616.761 -1616.761 -1616.761] (1.000)
Step: 38149, Reward: [-951.762 -951.762 -951.762] [0.0000], Avg: [-1615.89 -1615.89 -1615.89] (1.000)
Step: 38199, Reward: [-1778.493 -1778.493 -1778.493] [0.0000], Avg: [-1616.103 -1616.103 -1616.103] (1.000)
Step: 38249, Reward: [-2144.145 -2144.145 -2144.145] [0.0000], Avg: [-1616.793 -1616.793 -1616.793] (1.000)
Step: 38299, Reward: [-1710.964 -1710.964 -1710.964] [0.0000], Avg: [-1616.916 -1616.916 -1616.916] (1.000)
Step: 38349, Reward: [-1529.866 -1529.866 -1529.866] [0.0000], Avg: [-1616.802 -1616.802 -1616.802] (1.000)
Step: 38399, Reward: [-2173.553 -2173.553 -2173.553] [0.0000], Avg: [-1617.527 -1617.527 -1617.527] (1.000)
Step: 38449, Reward: [-1583.42 -1583.42 -1583.42] [0.0000], Avg: [-1617.483 -1617.483 -1617.483] (1.000)
Step: 38499, Reward: [-1467.658 -1467.658 -1467.658] [0.0000], Avg: [-1617.288 -1617.288 -1617.288] (1.000)
Step: 38549, Reward: [-761.58 -761.58 -761.58] [0.0000], Avg: [-1616.178 -1616.178 -1616.178] (1.000)
Step: 38599, Reward: [-2158.848 -2158.848 -2158.848] [0.0000], Avg: [-1616.881 -1616.881 -1616.881] (1.000)
Step: 38649, Reward: [-1712.761 -1712.761 -1712.761] [0.0000], Avg: [-1617.005 -1617.005 -1617.005] (1.000)
Step: 38699, Reward: [-1609.202 -1609.202 -1609.202] [0.0000], Avg: [-1616.995 -1616.995 -1616.995] (1.000)
Step: 38749, Reward: [-1915.461 -1915.461 -1915.461] [0.0000], Avg: [-1617.38 -1617.38 -1617.38] (1.000)
Step: 38799, Reward: [-2227.881 -2227.881 -2227.881] [0.0000], Avg: [-1618.167 -1618.167 -1618.167] (1.000)
Step: 38849, Reward: [-1951.227 -1951.227 -1951.227] [0.0000], Avg: [-1618.596 -1618.596 -1618.596] (1.000)
Step: 38899, Reward: [-1872.126 -1872.126 -1872.126] [0.0000], Avg: [-1618.922 -1618.922 -1618.922] (1.000)
Step: 38949, Reward: [-1619.574 -1619.574 -1619.574] [0.0000], Avg: [-1618.923 -1618.923 -1618.923] (1.000)
Step: 38999, Reward: [-1522.886 -1522.886 -1522.886] [0.0000], Avg: [-1618.799 -1618.799 -1618.799] (1.000)
Step: 39049, Reward: [-2177.547 -2177.547 -2177.547] [0.0000], Avg: [-1619.515 -1619.515 -1619.515] (1.000)
Step: 39099, Reward: [-1226.667 -1226.667 -1226.667] [0.0000], Avg: [-1619.012 -1619.012 -1619.012] (1.000)
Step: 39149, Reward: [-2203.041 -2203.041 -2203.041] [0.0000], Avg: [-1619.758 -1619.758 -1619.758] (1.000)
Step: 39199, Reward: [-694.411 -694.411 -694.411] [0.0000], Avg: [-1618.578 -1618.578 -1618.578] (1.000)
Step: 39249, Reward: [-2011.989 -2011.989 -2011.989] [0.0000], Avg: [-1619.079 -1619.079 -1619.079] (1.000)
Step: 39299, Reward: [-1408.597 -1408.597 -1408.597] [0.0000], Avg: [-1618.811 -1618.811 -1618.811] (1.000)
Step: 39349, Reward: [-951.27 -951.27 -951.27] [0.0000], Avg: [-1617.963 -1617.963 -1617.963] (1.000)
Step: 39399, Reward: [-2352.801 -2352.801 -2352.801] [0.0000], Avg: [-1618.896 -1618.896 -1618.896] (1.000)
Step: 39449, Reward: [-1823.687 -1823.687 -1823.687] [0.0000], Avg: [-1619.155 -1619.155 -1619.155] (1.000)
Step: 39499, Reward: [-2194.252 -2194.252 -2194.252] [0.0000], Avg: [-1619.883 -1619.883 -1619.883] (1.000)
Step: 39549, Reward: [-1722.75 -1722.75 -1722.75] [0.0000], Avg: [-1620.013 -1620.013 -1620.013] (1.000)
Step: 39599, Reward: [-1987.091 -1987.091 -1987.091] [0.0000], Avg: [-1620.477 -1620.477 -1620.477] (1.000)
Step: 39649, Reward: [-1760.242 -1760.242 -1760.242] [0.0000], Avg: [-1620.653 -1620.653 -1620.653] (1.000)
Step: 39699, Reward: [-1575.302 -1575.302 -1575.302] [0.0000], Avg: [-1620.596 -1620.596 -1620.596] (1.000)
Step: 39749, Reward: [-1136.545 -1136.545 -1136.545] [0.0000], Avg: [-1619.987 -1619.987 -1619.987] (1.000)
Step: 39799, Reward: [-1769.871 -1769.871 -1769.871] [0.0000], Avg: [-1620.175 -1620.175 -1620.175] (1.000)
Step: 39849, Reward: [-1885.974 -1885.974 -1885.974] [0.0000], Avg: [-1620.509 -1620.509 -1620.509] (1.000)
Step: 39899, Reward: [-1905.382 -1905.382 -1905.382] [0.0000], Avg: [-1620.866 -1620.866 -1620.866] (1.000)
Step: 39949, Reward: [-903.008 -903.008 -903.008] [0.0000], Avg: [-1619.967 -1619.967 -1619.967] (1.000)
Step: 39999, Reward: [-1817.393 -1817.393 -1817.393] [0.0000], Avg: [-1620.214 -1620.214 -1620.214] (1.000)
Step: 40049, Reward: [-1210.511 -1210.511 -1210.511] [0.0000], Avg: [-1619.703 -1619.703 -1619.703] (1.000)
Step: 40099, Reward: [-935.579 -935.579 -935.579] [0.0000], Avg: [-1618.85 -1618.85 -1618.85] (1.000)
Step: 40149, Reward: [-1869.838 -1869.838 -1869.838] [0.0000], Avg: [-1619.162 -1619.162 -1619.162] (1.000)
Step: 40199, Reward: [-1738.545 -1738.545 -1738.545] [0.0000], Avg: [-1619.311 -1619.311 -1619.311] (1.000)
Step: 40249, Reward: [-1455.387 -1455.387 -1455.387] [0.0000], Avg: [-1619.107 -1619.107 -1619.107] (1.000)
Step: 40299, Reward: [-1989.428 -1989.428 -1989.428] [0.0000], Avg: [-1619.567 -1619.567 -1619.567] (1.000)
Step: 40349, Reward: [-1731.401 -1731.401 -1731.401] [0.0000], Avg: [-1619.705 -1619.705 -1619.705] (1.000)
Step: 40399, Reward: [-1974.339 -1974.339 -1974.339] [0.0000], Avg: [-1620.144 -1620.144 -1620.144] (1.000)
Step: 40449, Reward: [-1063.216 -1063.216 -1063.216] [0.0000], Avg: [-1619.456 -1619.456 -1619.456] (1.000)
Step: 40499, Reward: [-2046.66 -2046.66 -2046.66] [0.0000], Avg: [-1619.983 -1619.983 -1619.983] (1.000)
Step: 40549, Reward: [-1648.089 -1648.089 -1648.089] [0.0000], Avg: [-1620.018 -1620.018 -1620.018] (1.000)
Step: 40599, Reward: [-1674.806 -1674.806 -1674.806] [0.0000], Avg: [-1620.085 -1620.085 -1620.085] (1.000)
Step: 40649, Reward: [-2220.298 -2220.298 -2220.298] [0.0000], Avg: [-1620.823 -1620.823 -1620.823] (1.000)
Step: 40699, Reward: [-1465.416 -1465.416 -1465.416] [0.0000], Avg: [-1620.633 -1620.633 -1620.633] (1.000)
Step: 40749, Reward: [-1723.124 -1723.124 -1723.124] [0.0000], Avg: [-1620.758 -1620.758 -1620.758] (1.000)
Step: 40799, Reward: [-1764.929 -1764.929 -1764.929] [0.0000], Avg: [-1620.935 -1620.935 -1620.935] (1.000)
Step: 40849, Reward: [-1861.143 -1861.143 -1861.143] [0.0000], Avg: [-1621.229 -1621.229 -1621.229] (1.000)
Step: 40899, Reward: [-1734.698 -1734.698 -1734.698] [0.0000], Avg: [-1621.368 -1621.368 -1621.368] (1.000)
Step: 40949, Reward: [-1993.303 -1993.303 -1993.303] [0.0000], Avg: [-1621.822 -1621.822 -1621.822] (1.000)
Step: 40999, Reward: [-621.518 -621.518 -621.518] [0.0000], Avg: [-1620.602 -1620.602 -1620.602] (1.000)
Step: 41049, Reward: [-1120.236 -1120.236 -1120.236] [0.0000], Avg: [-1619.992 -1619.992 -1619.992] (1.000)
Step: 41099, Reward: [-1923.866 -1923.866 -1923.866] [0.0000], Avg: [-1620.362 -1620.362 -1620.362] (1.000)
Step: 41149, Reward: [-1343.031 -1343.031 -1343.031] [0.0000], Avg: [-1620.025 -1620.025 -1620.025] (1.000)
Step: 41199, Reward: [-1619.8 -1619.8 -1619.8] [0.0000], Avg: [-1620.025 -1620.025 -1620.025] (1.000)
Step: 41249, Reward: [-1538.407 -1538.407 -1538.407] [0.0000], Avg: [-1619.926 -1619.926 -1619.926] (1.000)
Step: 41299, Reward: [-2039.383 -2039.383 -2039.383] [0.0000], Avg: [-1620.434 -1620.434 -1620.434] (1.000)
Step: 41349, Reward: [-1507.748 -1507.748 -1507.748] [0.0000], Avg: [-1620.298 -1620.298 -1620.298] (1.000)
Step: 41399, Reward: [-805.28 -805.28 -805.28] [0.0000], Avg: [-1619.313 -1619.313 -1619.313] (1.000)
Step: 41449, Reward: [-1763.991 -1763.991 -1763.991] [0.0000], Avg: [-1619.488 -1619.488 -1619.488] (1.000)
Step: 41499, Reward: [-1699.907 -1699.907 -1699.907] [0.0000], Avg: [-1619.585 -1619.585 -1619.585] (1.000)
Step: 41549, Reward: [-1323.547 -1323.547 -1323.547] [0.0000], Avg: [-1619.228 -1619.228 -1619.228] (1.000)
Step: 41599, Reward: [-1472.819 -1472.819 -1472.819] [0.0000], Avg: [-1619.052 -1619.052 -1619.052] (1.000)
Step: 41649, Reward: [-1624.478 -1624.478 -1624.478] [0.0000], Avg: [-1619.059 -1619.059 -1619.059] (1.000)
Step: 41699, Reward: [-1849.43 -1849.43 -1849.43] [0.0000], Avg: [-1619.335 -1619.335 -1619.335] (1.000)
Step: 41749, Reward: [-2219.914 -2219.914 -2219.914] [0.0000], Avg: [-1620.054 -1620.054 -1620.054] (1.000)
Step: 41799, Reward: [-567.881 -567.881 -567.881] [0.0000], Avg: [-1618.796 -1618.796 -1618.796] (1.000)
Step: 41849, Reward: [-960.321 -960.321 -960.321] [0.0000], Avg: [-1618.009 -1618.009 -1618.009] (1.000)
Step: 41899, Reward: [-2237.26 -2237.26 -2237.26] [0.0000], Avg: [-1618.748 -1618.748 -1618.748] (1.000)
Step: 41949, Reward: [-1597.458 -1597.458 -1597.458] [0.0000], Avg: [-1618.723 -1618.723 -1618.723] (1.000)
Step: 41999, Reward: [-1628.191 -1628.191 -1628.191] [0.0000], Avg: [-1618.734 -1618.734 -1618.734] (1.000)
Step: 42049, Reward: [-2011.471 -2011.471 -2011.471] [0.0000], Avg: [-1619.201 -1619.201 -1619.201] (1.000)
Step: 42099, Reward: [-1053.569 -1053.569 -1053.569] [0.0000], Avg: [-1618.529 -1618.529 -1618.529] (1.000)
Step: 42149, Reward: [-2151.517 -2151.517 -2151.517] [0.0000], Avg: [-1619.161 -1619.161 -1619.161] (1.000)
Step: 42199, Reward: [-2095.681 -2095.681 -2095.681] [0.0000], Avg: [-1619.726 -1619.726 -1619.726] (1.000)
Step: 42249, Reward: [-673.071 -673.071 -673.071] [0.0000], Avg: [-1618.606 -1618.606 -1618.606] (1.000)
Step: 42299, Reward: [-1416.591 -1416.591 -1416.591] [0.0000], Avg: [-1618.367 -1618.367 -1618.367] (1.000)
Step: 42349, Reward: [-1405.125 -1405.125 -1405.125] [0.0000], Avg: [-1618.115 -1618.115 -1618.115] (1.000)
Step: 42399, Reward: [-1911.932 -1911.932 -1911.932] [0.0000], Avg: [-1618.462 -1618.462 -1618.462] (1.000)
Step: 42449, Reward: [-2174.838 -2174.838 -2174.838] [0.0000], Avg: [-1619.117 -1619.117 -1619.117] (1.000)
Step: 42499, Reward: [-1285.911 -1285.911 -1285.911] [0.0000], Avg: [-1618.725 -1618.725 -1618.725] (1.000)
Step: 42549, Reward: [-1152.243 -1152.243 -1152.243] [0.0000], Avg: [-1618.177 -1618.177 -1618.177] (1.000)
Step: 42599, Reward: [-1154.194 -1154.194 -1154.194] [0.0000], Avg: [-1617.632 -1617.632 -1617.632] (1.000)
Step: 42649, Reward: [-2027.975 -2027.975 -2027.975] [0.0000], Avg: [-1618.113 -1618.113 -1618.113] (1.000)
Step: 42699, Reward: [-1333.74 -1333.74 -1333.74] [0.0000], Avg: [-1617.78 -1617.78 -1617.78] (1.000)
Step: 42749, Reward: [-1000.998 -1000.998 -1000.998] [0.0000], Avg: [-1617.059 -1617.059 -1617.059] (1.000)
Step: 42799, Reward: [-1664.145 -1664.145 -1664.145] [0.0000], Avg: [-1617.114 -1617.114 -1617.114] (1.000)
Step: 42849, Reward: [-1597.752 -1597.752 -1597.752] [0.0000], Avg: [-1617.091 -1617.091 -1617.091] (1.000)
Step: 42899, Reward: [-927.156 -927.156 -927.156] [0.0000], Avg: [-1616.287 -1616.287 -1616.287] (1.000)
Step: 42949, Reward: [-1841.034 -1841.034 -1841.034] [0.0000], Avg: [-1616.549 -1616.549 -1616.549] (1.000)
Step: 42999, Reward: [-2287.895 -2287.895 -2287.895] [0.0000], Avg: [-1617.33 -1617.33 -1617.33] (1.000)
Step: 43049, Reward: [-1102.477 -1102.477 -1102.477] [0.0000], Avg: [-1616.732 -1616.732 -1616.732] (1.000)
Step: 43099, Reward: [-2014.448 -2014.448 -2014.448] [0.0000], Avg: [-1617.193 -1617.193 -1617.193] (1.000)
Step: 43149, Reward: [-1588.552 -1588.552 -1588.552] [0.0000], Avg: [-1617.16 -1617.16 -1617.16] (1.000)
Step: 43199, Reward: [-1429.478 -1429.478 -1429.478] [0.0000], Avg: [-1616.943 -1616.943 -1616.943] (1.000)
Step: 43249, Reward: [-2123.425 -2123.425 -2123.425] [0.0000], Avg: [-1617.528 -1617.528 -1617.528] (1.000)
Step: 43299, Reward: [-1424.969 -1424.969 -1424.969] [0.0000], Avg: [-1617.306 -1617.306 -1617.306] (1.000)
Step: 43349, Reward: [-1246.986 -1246.986 -1246.986] [0.0000], Avg: [-1616.879 -1616.879 -1616.879] (1.000)
Step: 43399, Reward: [-1492.709 -1492.709 -1492.709] [0.0000], Avg: [-1616.736 -1616.736 -1616.736] (1.000)
Step: 43449, Reward: [-1576.674 -1576.674 -1576.674] [0.0000], Avg: [-1616.689 -1616.689 -1616.689] (1.000)
Step: 43499, Reward: [-1254.356 -1254.356 -1254.356] [0.0000], Avg: [-1616.273 -1616.273 -1616.273] (1.000)
Step: 43549, Reward: [-2108.142 -2108.142 -2108.142] [0.0000], Avg: [-1616.838 -1616.838 -1616.838] (1.000)
Step: 43599, Reward: [-2004.034 -2004.034 -2004.034] [0.0000], Avg: [-1617.282 -1617.282 -1617.282] (1.000)
Step: 43649, Reward: [-1789.34 -1789.34 -1789.34] [0.0000], Avg: [-1617.479 -1617.479 -1617.479] (1.000)
Step: 43699, Reward: [-2076.11 -2076.11 -2076.11] [0.0000], Avg: [-1618.004 -1618.004 -1618.004] (1.000)
Step: 43749, Reward: [-1917.99 -1917.99 -1917.99] [0.0000], Avg: [-1618.346 -1618.346 -1618.346] (1.000)
Step: 43799, Reward: [-1153.105 -1153.105 -1153.105] [0.0000], Avg: [-1617.815 -1617.815 -1617.815] (1.000)
Step: 43849, Reward: [-1433.668 -1433.668 -1433.668] [0.0000], Avg: [-1617.605 -1617.605 -1617.605] (1.000)
Step: 43899, Reward: [-865.958 -865.958 -865.958] [0.0000], Avg: [-1616.749 -1616.749 -1616.749] (1.000)
Step: 43949, Reward: [-1746.601 -1746.601 -1746.601] [0.0000], Avg: [-1616.897 -1616.897 -1616.897] (1.000)
Step: 43999, Reward: [-1474.835 -1474.835 -1474.835] [0.0000], Avg: [-1616.736 -1616.736 -1616.736] (1.000)
Step: 44049, Reward: [-1514.898 -1514.898 -1514.898] [0.0000], Avg: [-1616.62 -1616.62 -1616.62] (1.000)
Step: 44099, Reward: [-1534.699 -1534.699 -1534.699] [0.0000], Avg: [-1616.527 -1616.527 -1616.527] (1.000)
Step: 44149, Reward: [-1074.737 -1074.737 -1074.737] [0.0000], Avg: [-1615.913 -1615.913 -1615.913] (1.000)
Step: 44199, Reward: [-1374.409 -1374.409 -1374.409] [0.0000], Avg: [-1615.64 -1615.64 -1615.64] (1.000)
Step: 44249, Reward: [-1025.514 -1025.514 -1025.514] [0.0000], Avg: [-1614.973 -1614.973 -1614.973] (1.000)
Step: 44299, Reward: [-2067.264 -2067.264 -2067.264] [0.0000], Avg: [-1615.484 -1615.484 -1615.484] (1.000)
Step: 44349, Reward: [-1800.653 -1800.653 -1800.653] [0.0000], Avg: [-1615.693 -1615.693 -1615.693] (1.000)
Step: 44399, Reward: [-1732.953 -1732.953 -1732.953] [0.0000], Avg: [-1615.825 -1615.825 -1615.825] (1.000)
Step: 44449, Reward: [-1452.279 -1452.279 -1452.279] [0.0000], Avg: [-1615.641 -1615.641 -1615.641] (1.000)
Step: 44499, Reward: [-1838.031 -1838.031 -1838.031] [0.0000], Avg: [-1615.891 -1615.891 -1615.891] (1.000)
Step: 44549, Reward: [-1466.133 -1466.133 -1466.133] [0.0000], Avg: [-1615.723 -1615.723 -1615.723] (1.000)
Step: 44599, Reward: [-1083.732 -1083.732 -1083.732] [0.0000], Avg: [-1615.126 -1615.126 -1615.126] (1.000)
Step: 44649, Reward: [-1804.757 -1804.757 -1804.757] [0.0000], Avg: [-1615.339 -1615.339 -1615.339] (1.000)
Step: 44699, Reward: [-1546.396 -1546.396 -1546.396] [0.0000], Avg: [-1615.261 -1615.261 -1615.261] (1.000)
Step: 44749, Reward: [-1150.817 -1150.817 -1150.817] [0.0000], Avg: [-1614.742 -1614.742 -1614.742] (1.000)
Step: 44799, Reward: [-1886.057 -1886.057 -1886.057] [0.0000], Avg: [-1615.045 -1615.045 -1615.045] (1.000)
Step: 44849, Reward: [-1814.359 -1814.359 -1814.359] [0.0000], Avg: [-1615.267 -1615.267 -1615.267] (1.000)
Step: 44899, Reward: [-1861.62 -1861.62 -1861.62] [0.0000], Avg: [-1615.542 -1615.542 -1615.542] (1.000)
Step: 44949, Reward: [-2018.388 -2018.388 -2018.388] [0.0000], Avg: [-1615.99 -1615.99 -1615.99] (1.000)
Step: 44999, Reward: [-1649.367 -1649.367 -1649.367] [0.0000], Avg: [-1616.027 -1616.027 -1616.027] (1.000)
Step: 45049, Reward: [-976.67 -976.67 -976.67] [0.0000], Avg: [-1615.317 -1615.317 -1615.317] (1.000)
Step: 45099, Reward: [-2372.955 -2372.955 -2372.955] [0.0000], Avg: [-1616.157 -1616.157 -1616.157] (1.000)
Step: 45149, Reward: [-691.897 -691.897 -691.897] [0.0000], Avg: [-1615.134 -1615.134 -1615.134] (1.000)
Step: 45199, Reward: [-1867.247 -1867.247 -1867.247] [0.0000], Avg: [-1615.413 -1615.413 -1615.413] (1.000)
Step: 45249, Reward: [-2098.322 -2098.322 -2098.322] [0.0000], Avg: [-1615.946 -1615.946 -1615.946] (1.000)
Step: 45299, Reward: [-1357.933 -1357.933 -1357.933] [0.0000], Avg: [-1615.662 -1615.662 -1615.662] (1.000)
Step: 45349, Reward: [-1623.648 -1623.648 -1623.648] [0.0000], Avg: [-1615.67 -1615.67 -1615.67] (1.000)
Step: 45399, Reward: [-1215.509 -1215.509 -1215.509] [0.0000], Avg: [-1615.23 -1615.23 -1615.23] (1.000)
Step: 45449, Reward: [-651.313 -651.313 -651.313] [0.0000], Avg: [-1614.169 -1614.169 -1614.169] (1.000)
Step: 45499, Reward: [-1557.155 -1557.155 -1557.155] [0.0000], Avg: [-1614.107 -1614.107 -1614.107] (1.000)
Step: 45549, Reward: [-1794.903 -1794.903 -1794.903] [0.0000], Avg: [-1614.305 -1614.305 -1614.305] (1.000)
Step: 45599, Reward: [-2018.029 -2018.029 -2018.029] [0.0000], Avg: [-1614.748 -1614.748 -1614.748] (1.000)
Step: 45649, Reward: [-1064.325 -1064.325 -1064.325] [0.0000], Avg: [-1614.145 -1614.145 -1614.145] (1.000)
Step: 45699, Reward: [-1363.246 -1363.246 -1363.246] [0.0000], Avg: [-1613.87 -1613.87 -1613.87] (1.000)
Step: 45749, Reward: [-2049.339 -2049.339 -2049.339] [0.0000], Avg: [-1614.346 -1614.346 -1614.346] (1.000)
Step: 45799, Reward: [-1810.793 -1810.793 -1810.793] [0.0000], Avg: [-1614.561 -1614.561 -1614.561] (1.000)
Step: 45849, Reward: [-1793.695 -1793.695 -1793.695] [0.0000], Avg: [-1614.756 -1614.756 -1614.756] (1.000)
Step: 45899, Reward: [-1730.175 -1730.175 -1730.175] [0.0000], Avg: [-1614.882 -1614.882 -1614.882] (1.000)
Step: 45949, Reward: [-1493.963 -1493.963 -1493.963] [0.0000], Avg: [-1614.75 -1614.75 -1614.75] (1.000)
Step: 45999, Reward: [-1965.396 -1965.396 -1965.396] [0.0000], Avg: [-1615.131 -1615.131 -1615.131] (1.000)
Step: 46049, Reward: [-1795.791 -1795.791 -1795.791] [0.0000], Avg: [-1615.327 -1615.327 -1615.327] (1.000)
Step: 46099, Reward: [-1287.976 -1287.976 -1287.976] [0.0000], Avg: [-1614.972 -1614.972 -1614.972] (1.000)
Step: 46149, Reward: [-1024.521 -1024.521 -1024.521] [0.0000], Avg: [-1614.333 -1614.333 -1614.333] (1.000)
Step: 46199, Reward: [-1252.822 -1252.822 -1252.822] [0.0000], Avg: [-1613.941 -1613.941 -1613.941] (1.000)
Step: 46249, Reward: [-1427.32 -1427.32 -1427.32] [0.0000], Avg: [-1613.74 -1613.74 -1613.74] (1.000)
Step: 46299, Reward: [-1524.868 -1524.868 -1524.868] [0.0000], Avg: [-1613.644 -1613.644 -1613.644] (1.000)
Step: 46349, Reward: [-1440.215 -1440.215 -1440.215] [0.0000], Avg: [-1613.457 -1613.457 -1613.457] (1.000)
Step: 46399, Reward: [-1828.219 -1828.219 -1828.219] [0.0000], Avg: [-1613.688 -1613.688 -1613.688] (1.000)
Step: 46449, Reward: [-775.923 -775.923 -775.923] [0.0000], Avg: [-1612.786 -1612.786 -1612.786] (1.000)
Step: 46499, Reward: [-1744.184 -1744.184 -1744.184] [0.0000], Avg: [-1612.928 -1612.928 -1612.928] (1.000)
Step: 46549, Reward: [-1779.353 -1779.353 -1779.353] [0.0000], Avg: [-1613.106 -1613.106 -1613.106] (1.000)
Step: 46599, Reward: [-2040.929 -2040.929 -2040.929] [0.0000], Avg: [-1613.565 -1613.565 -1613.565] (1.000)
Step: 46649, Reward: [-1836.607 -1836.607 -1836.607] [0.0000], Avg: [-1613.804 -1613.804 -1613.804] (1.000)
Step: 46699, Reward: [-1719.39 -1719.39 -1719.39] [0.0000], Avg: [-1613.917 -1613.917 -1613.917] (1.000)
Step: 46749, Reward: [-1318.638 -1318.638 -1318.638] [0.0000], Avg: [-1613.602 -1613.602 -1613.602] (1.000)
Step: 46799, Reward: [-1299.514 -1299.514 -1299.514] [0.0000], Avg: [-1613.266 -1613.266 -1613.266] (1.000)
Step: 46849, Reward: [-1166.474 -1166.474 -1166.474] [0.0000], Avg: [-1612.789 -1612.789 -1612.789] (1.000)
Step: 46899, Reward: [-1717.585 -1717.585 -1717.585] [0.0000], Avg: [-1612.901 -1612.901 -1612.901] (1.000)
Step: 46949, Reward: [-933.726 -933.726 -933.726] [0.0000], Avg: [-1612.178 -1612.178 -1612.178] (1.000)
Step: 46999, Reward: [-1572.098 -1572.098 -1572.098] [0.0000], Avg: [-1612.135 -1612.135 -1612.135] (1.000)
Step: 47049, Reward: [-1262.784 -1262.784 -1262.784] [0.0000], Avg: [-1611.764 -1611.764 -1611.764] (1.000)
Step: 47099, Reward: [-1600.703 -1600.703 -1600.703] [0.0000], Avg: [-1611.752 -1611.752 -1611.752] (1.000)
Step: 47149, Reward: [-889.738 -889.738 -889.738] [0.0000], Avg: [-1610.986 -1610.986 -1610.986] (1.000)
Step: 47199, Reward: [-1556.693 -1556.693 -1556.693] [0.0000], Avg: [-1610.929 -1610.929 -1610.929] (1.000)
Step: 47249, Reward: [-1708.548 -1708.548 -1708.548] [0.0000], Avg: [-1611.032 -1611.032 -1611.032] (1.000)
Step: 47299, Reward: [-1235.513 -1235.513 -1235.513] [0.0000], Avg: [-1610.635 -1610.635 -1610.635] (1.000)
Step: 47349, Reward: [-1669.502 -1669.502 -1669.502] [0.0000], Avg: [-1610.697 -1610.697 -1610.697] (1.000)
Step: 47399, Reward: [-1621.973 -1621.973 -1621.973] [0.0000], Avg: [-1610.709 -1610.709 -1610.709] (1.000)
Step: 47449, Reward: [-1548.117 -1548.117 -1548.117] [0.0000], Avg: [-1610.643 -1610.643 -1610.643] (1.000)
Step: 47499, Reward: [-1832.463 -1832.463 -1832.463] [0.0000], Avg: [-1610.877 -1610.877 -1610.877] (1.000)
Step: 47549, Reward: [-1695.437 -1695.437 -1695.437] [0.0000], Avg: [-1610.966 -1610.966 -1610.966] (1.000)
Step: 47599, Reward: [-981.799 -981.799 -981.799] [0.0000], Avg: [-1610.305 -1610.305 -1610.305] (1.000)
Step: 47649, Reward: [-556.75 -556.75 -556.75] [0.0000], Avg: [-1609.199 -1609.199 -1609.199] (1.000)
Step: 47699, Reward: [-1638.24 -1638.24 -1638.24] [0.0000], Avg: [-1609.23 -1609.23 -1609.23] (1.000)
Step: 47749, Reward: [-613.144 -613.144 -613.144] [0.0000], Avg: [-1608.187 -1608.187 -1608.187] (1.000)
Step: 47799, Reward: [-1561.97 -1561.97 -1561.97] [0.0000], Avg: [-1608.138 -1608.138 -1608.138] (1.000)
Step: 47849, Reward: [-1497.314 -1497.314 -1497.314] [0.0000], Avg: [-1608.023 -1608.023 -1608.023] (1.000)
Step: 47899, Reward: [-1350.082 -1350.082 -1350.082] [0.0000], Avg: [-1607.753 -1607.753 -1607.753] (1.000)
Step: 47949, Reward: [-1249.145 -1249.145 -1249.145] [0.0000], Avg: [-1607.379 -1607.379 -1607.379] (1.000)
Step: 47999, Reward: [-1366.302 -1366.302 -1366.302] [0.0000], Avg: [-1607.128 -1607.128 -1607.128] (1.000)
Step: 48049, Reward: [-1445.8 -1445.8 -1445.8] [0.0000], Avg: [-1606.96 -1606.96 -1606.96] (1.000)
Step: 48099, Reward: [-790.973 -790.973 -790.973] [0.0000], Avg: [-1606.112 -1606.112 -1606.112] (1.000)
Step: 48149, Reward: [-891.523 -891.523 -891.523] [0.0000], Avg: [-1605.37 -1605.37 -1605.37] (1.000)
Step: 48199, Reward: [-1401.431 -1401.431 -1401.431] [0.0000], Avg: [-1605.159 -1605.159 -1605.159] (1.000)
Step: 48249, Reward: [-450.351 -450.351 -450.351] [0.0000], Avg: [-1603.962 -1603.962 -1603.962] (1.000)
Step: 48299, Reward: [-696.29 -696.29 -696.29] [0.0000], Avg: [-1603.022 -1603.022 -1603.022] (1.000)
Step: 48349, Reward: [-1310.114 -1310.114 -1310.114] [0.0000], Avg: [-1602.719 -1602.719 -1602.719] (1.000)
Step: 48399, Reward: [-660.406 -660.406 -660.406] [0.0000], Avg: [-1601.746 -1601.746 -1601.746] (1.000)
Step: 48449, Reward: [-1506.433 -1506.433 -1506.433] [0.0000], Avg: [-1601.648 -1601.648 -1601.648] (1.000)
Step: 48499, Reward: [-1054.228 -1054.228 -1054.228] [0.0000], Avg: [-1601.083 -1601.083 -1601.083] (1.000)
Step: 48549, Reward: [-741.335 -741.335 -741.335] [0.0000], Avg: [-1600.198 -1600.198 -1600.198] (1.000)
Step: 48599, Reward: [-688.438 -688.438 -688.438] [0.0000], Avg: [-1599.26 -1599.26 -1599.26] (1.000)
Step: 48649, Reward: [-1494.905 -1494.905 -1494.905] [0.0000], Avg: [-1599.153 -1599.153 -1599.153] (1.000)
Step: 48699, Reward: [-724.325 -724.325 -724.325] [0.0000], Avg: [-1598.254 -1598.254 -1598.254] (1.000)
Step: 48749, Reward: [-2041.246 -2041.246 -2041.246] [0.0000], Avg: [-1598.709 -1598.709 -1598.709] (1.000)
Step: 48799, Reward: [-1318.842 -1318.842 -1318.842] [0.0000], Avg: [-1598.422 -1598.422 -1598.422] (1.000)
Step: 48849, Reward: [-1526.877 -1526.877 -1526.877] [0.0000], Avg: [-1598.349 -1598.349 -1598.349] (1.000)
Step: 48899, Reward: [-1580.253 -1580.253 -1580.253] [0.0000], Avg: [-1598.33 -1598.33 -1598.33] (1.000)
Step: 48949, Reward: [-2094.034 -2094.034 -2094.034] [0.0000], Avg: [-1598.837 -1598.837 -1598.837] (1.000)
Step: 48999, Reward: [-1233.195 -1233.195 -1233.195] [0.0000], Avg: [-1598.463 -1598.463 -1598.463] (1.000)
Step: 49049, Reward: [-456.943 -456.943 -456.943] [0.0000], Avg: [-1597.3 -1597.3 -1597.3] (1.000)
Step: 49099, Reward: [-892.607 -892.607 -892.607] [0.0000], Avg: [-1596.582 -1596.582 -1596.582] (1.000)
Step: 49149, Reward: [-1720.869 -1720.869 -1720.869] [0.0000], Avg: [-1596.709 -1596.709 -1596.709] (1.000)
Step: 49199, Reward: [-843.373 -843.373 -843.373] [0.0000], Avg: [-1595.943 -1595.943 -1595.943] (1.000)
Step: 49249, Reward: [-1982.8 -1982.8 -1982.8] [0.0000], Avg: [-1596.336 -1596.336 -1596.336] (1.000)
Step: 49299, Reward: [-1138.297 -1138.297 -1138.297] [0.0000], Avg: [-1595.871 -1595.871 -1595.871] (1.000)
Step: 49349, Reward: [-2267.69 -2267.69 -2267.69] [0.0000], Avg: [-1596.552 -1596.552 -1596.552] (1.000)
Step: 49399, Reward: [-486.351 -486.351 -486.351] [0.0000], Avg: [-1595.428 -1595.428 -1595.428] (1.000)
Step: 49449, Reward: [-2425.421 -2425.421 -2425.421] [0.0000], Avg: [-1596.267 -1596.267 -1596.267] (1.000)
Step: 49499, Reward: [-1778.835 -1778.835 -1778.835] [0.0000], Avg: [-1596.452 -1596.452 -1596.452] (1.000)
Step: 49549, Reward: [-1985.672 -1985.672 -1985.672] [0.0000], Avg: [-1596.845 -1596.845 -1596.845] (1.000)
Step: 49599, Reward: [-1427.667 -1427.667 -1427.667] [0.0000], Avg: [-1596.674 -1596.674 -1596.674] (1.000)
Step: 49649, Reward: [-1950.44 -1950.44 -1950.44] [0.0000], Avg: [-1597.03 -1597.03 -1597.03] (1.000)
Step: 49699, Reward: [-2240.936 -2240.936 -2240.936] [0.0000], Avg: [-1597.678 -1597.678 -1597.678] (1.000)
Step: 49749, Reward: [-1694.446 -1694.446 -1694.446] [0.0000], Avg: [-1597.775 -1597.775 -1597.775] (1.000)
Step: 49799, Reward: [-1820.59 -1820.59 -1820.59] [0.0000], Avg: [-1597.999 -1597.999 -1597.999] (1.000)
Step: 49849, Reward: [-1537.888 -1537.888 -1537.888] [0.0000], Avg: [-1597.939 -1597.939 -1597.939] (1.000)
Step: 49899, Reward: [-1665.054 -1665.054 -1665.054] [0.0000], Avg: [-1598.006 -1598.006 -1598.006] (1.000)
Step: 49949, Reward: [-1862.12 -1862.12 -1862.12] [0.0000], Avg: [-1598.27 -1598.27 -1598.27] (1.000)
Step: 49999, Reward: [-1946.269 -1946.269 -1946.269] [0.0000], Avg: [-1598.618 -1598.618 -1598.618] (1.000)
Step: 50049, Reward: [-2019.061 -2019.061 -2019.061] [0.0000], Avg: [-1599.038 -1599.038 -1599.038] (1.000)
Step: 50099, Reward: [-1583.313 -1583.313 -1583.313] [0.0000], Avg: [-1599.023 -1599.023 -1599.023] (1.000)
Step: 50149, Reward: [-1833.034 -1833.034 -1833.034] [0.0000], Avg: [-1599.256 -1599.256 -1599.256] (1.000)
Step: 50199, Reward: [-1572.535 -1572.535 -1572.535] [0.0000], Avg: [-1599.229 -1599.229 -1599.229] (1.000)
Step: 50249, Reward: [-1719.845 -1719.845 -1719.845] [0.0000], Avg: [-1599.349 -1599.349 -1599.349] (1.000)
Step: 50299, Reward: [-1780.153 -1780.153 -1780.153] [0.0000], Avg: [-1599.529 -1599.529 -1599.529] (1.000)
Step: 50349, Reward: [-2133.877 -2133.877 -2133.877] [0.0000], Avg: [-1600.06 -1600.06 -1600.06] (1.000)
Step: 50399, Reward: [-2235.469 -2235.469 -2235.469] [0.0000], Avg: [-1600.69 -1600.69 -1600.69] (1.000)
Step: 50449, Reward: [-1552.959 -1552.959 -1552.959] [0.0000], Avg: [-1600.643 -1600.643 -1600.643] (1.000)
Step: 50499, Reward: [-1526.556 -1526.556 -1526.556] [0.0000], Avg: [-1600.57 -1600.57 -1600.57] (1.000)
Step: 50549, Reward: [-1599.234 -1599.234 -1599.234] [0.0000], Avg: [-1600.568 -1600.568 -1600.568] (1.000)
Step: 50599, Reward: [-2057.172 -2057.172 -2057.172] [0.0000], Avg: [-1601.019 -1601.019 -1601.019] (1.000)
Step: 50649, Reward: [-1076.574 -1076.574 -1076.574] [0.0000], Avg: [-1600.502 -1600.502 -1600.502] (1.000)
Step: 50699, Reward: [-1823.288 -1823.288 -1823.288] [0.0000], Avg: [-1600.721 -1600.721 -1600.721] (1.000)
Step: 50749, Reward: [-1749.638 -1749.638 -1749.638] [0.0000], Avg: [-1600.868 -1600.868 -1600.868] (1.000)
Step: 50799, Reward: [-1851.097 -1851.097 -1851.097] [0.0000], Avg: [-1601.114 -1601.114 -1601.114] (1.000)
Step: 50849, Reward: [-1897.427 -1897.427 -1897.427] [0.0000], Avg: [-1601.406 -1601.406 -1601.406] (1.000)
Step: 50899, Reward: [-1390.011 -1390.011 -1390.011] [0.0000], Avg: [-1601.198 -1601.198 -1601.198] (1.000)
Step: 50949, Reward: [-2177.562 -2177.562 -2177.562] [0.0000], Avg: [-1601.764 -1601.764 -1601.764] (1.000)
Step: 50999, Reward: [-1780.56 -1780.56 -1780.56] [0.0000], Avg: [-1601.939 -1601.939 -1601.939] (1.000)
Step: 51049, Reward: [-1430.1 -1430.1 -1430.1] [0.0000], Avg: [-1601.771 -1601.771 -1601.771] (1.000)
Step: 51099, Reward: [-1774.823 -1774.823 -1774.823] [0.0000], Avg: [-1601.94 -1601.94 -1601.94] (1.000)
Step: 51149, Reward: [-2052.206 -2052.206 -2052.206] [0.0000], Avg: [-1602.38 -1602.38 -1602.38] (1.000)
Step: 51199, Reward: [-1878.464 -1878.464 -1878.464] [0.0000], Avg: [-1602.65 -1602.65 -1602.65] (1.000)
Step: 51249, Reward: [-2064.704 -2064.704 -2064.704] [0.0000], Avg: [-1603.101 -1603.101 -1603.101] (1.000)
Step: 51299, Reward: [-2165.878 -2165.878 -2165.878] [0.0000], Avg: [-1603.649 -1603.649 -1603.649] (1.000)
Step: 51349, Reward: [-1786.764 -1786.764 -1786.764] [0.0000], Avg: [-1603.827 -1603.827 -1603.827] (1.000)
Step: 51399, Reward: [-1574.421 -1574.421 -1574.421] [0.0000], Avg: [-1603.799 -1603.799 -1603.799] (1.000)
Step: 51449, Reward: [-1214.916 -1214.916 -1214.916] [0.0000], Avg: [-1603.421 -1603.421 -1603.421] (1.000)
Step: 51499, Reward: [-1623.259 -1623.259 -1623.259] [0.0000], Avg: [-1603.44 -1603.44 -1603.44] (1.000)
Step: 51549, Reward: [-1947.059 -1947.059 -1947.059] [0.0000], Avg: [-1603.773 -1603.773 -1603.773] (1.000)
Step: 51599, Reward: [-1751.186 -1751.186 -1751.186] [0.0000], Avg: [-1603.916 -1603.916 -1603.916] (1.000)
Step: 51649, Reward: [-2207.873 -2207.873 -2207.873] [0.0000], Avg: [-1604.501 -1604.501 -1604.501] (1.000)
Step: 51699, Reward: [-1933.578 -1933.578 -1933.578] [0.0000], Avg: [-1604.819 -1604.819 -1604.819] (1.000)
Step: 51749, Reward: [-2200.073 -2200.073 -2200.073] [0.0000], Avg: [-1605.394 -1605.394 -1605.394] (1.000)
Step: 51799, Reward: [-1673.674 -1673.674 -1673.674] [0.0000], Avg: [-1605.46 -1605.46 -1605.46] (1.000)
Step: 51849, Reward: [-1555.69 -1555.69 -1555.69] [0.0000], Avg: [-1605.412 -1605.412 -1605.412] (1.000)
Step: 51899, Reward: [-2076.894 -2076.894 -2076.894] [0.0000], Avg: [-1605.866 -1605.866 -1605.866] (1.000)
Step: 51949, Reward: [-1866.499 -1866.499 -1866.499] [0.0000], Avg: [-1606.117 -1606.117 -1606.117] (1.000)
Step: 51999, Reward: [-2166.965 -2166.965 -2166.965] [0.0000], Avg: [-1606.657 -1606.657 -1606.657] (1.000)
Step: 52049, Reward: [-1704.051 -1704.051 -1704.051] [0.0000], Avg: [-1606.75 -1606.75 -1606.75] (1.000)
Step: 52099, Reward: [-1998.9 -1998.9 -1998.9] [0.0000], Avg: [-1607.126 -1607.126 -1607.126] (1.000)
Step: 52149, Reward: [-1559.61 -1559.61 -1559.61] [0.0000], Avg: [-1607.081 -1607.081 -1607.081] (1.000)
Step: 52199, Reward: [-1779.621 -1779.621 -1779.621] [0.0000], Avg: [-1607.246 -1607.246 -1607.246] (1.000)
Step: 52249, Reward: [-2036.324 -2036.324 -2036.324] [0.0000], Avg: [-1607.657 -1607.657 -1607.657] (1.000)
Step: 52299, Reward: [-1419.566 -1419.566 -1419.566] [0.0000], Avg: [-1607.477 -1607.477 -1607.477] (1.000)
Step: 52349, Reward: [-1894.397 -1894.397 -1894.397] [0.0000], Avg: [-1607.751 -1607.751 -1607.751] (1.000)
Step: 52399, Reward: [-2018.72 -2018.72 -2018.72] [0.0000], Avg: [-1608.143 -1608.143 -1608.143] (1.000)
Step: 52449, Reward: [-1622.238 -1622.238 -1622.238] [0.0000], Avg: [-1608.157 -1608.157 -1608.157] (1.000)
Step: 52499, Reward: [-1828.384 -1828.384 -1828.384] [0.0000], Avg: [-1608.366 -1608.366 -1608.366] (1.000)
Step: 52549, Reward: [-1526.152 -1526.152 -1526.152] [0.0000], Avg: [-1608.288 -1608.288 -1608.288] (1.000)
Step: 52599, Reward: [-1968.651 -1968.651 -1968.651] [0.0000], Avg: [-1608.631 -1608.631 -1608.631] (1.000)
Step: 52649, Reward: [-1871.395 -1871.395 -1871.395] [0.0000], Avg: [-1608.88 -1608.88 -1608.88] (1.000)
Step: 52699, Reward: [-1951.616 -1951.616 -1951.616] [0.0000], Avg: [-1609.205 -1609.205 -1609.205] (1.000)
Step: 52749, Reward: [-2183.973 -2183.973 -2183.973] [0.0000], Avg: [-1609.75 -1609.75 -1609.75] (1.000)
Step: 52799, Reward: [-2159.38 -2159.38 -2159.38] [0.0000], Avg: [-1610.271 -1610.271 -1610.271] (1.000)
Step: 52849, Reward: [-1661.598 -1661.598 -1661.598] [0.0000], Avg: [-1610.319 -1610.319 -1610.319] (1.000)
Step: 52899, Reward: [-2027.843 -2027.843 -2027.843] [0.0000], Avg: [-1610.714 -1610.714 -1610.714] (1.000)
Step: 52949, Reward: [-2129.703 -2129.703 -2129.703] [0.0000], Avg: [-1611.204 -1611.204 -1611.204] (1.000)
Step: 52999, Reward: [-1948.421 -1948.421 -1948.421] [0.0000], Avg: [-1611.522 -1611.522 -1611.522] (1.000)
Step: 53049, Reward: [-1704.891 -1704.891 -1704.891] [0.0000], Avg: [-1611.61 -1611.61 -1611.61] (1.000)
Step: 53099, Reward: [-1886.659 -1886.659 -1886.659] [0.0000], Avg: [-1611.869 -1611.869 -1611.869] (1.000)
Step: 53149, Reward: [-1933.224 -1933.224 -1933.224] [0.0000], Avg: [-1612.171 -1612.171 -1612.171] (1.000)
Step: 53199, Reward: [-2058.616 -2058.616 -2058.616] [0.0000], Avg: [-1612.591 -1612.591 -1612.591] (1.000)
Step: 53249, Reward: [-1838.476 -1838.476 -1838.476] [0.0000], Avg: [-1612.803 -1612.803 -1612.803] (1.000)
Step: 53299, Reward: [-2159.62 -2159.62 -2159.62] [0.0000], Avg: [-1613.316 -1613.316 -1613.316] (1.000)
Step: 53349, Reward: [-1810.499 -1810.499 -1810.499] [0.0000], Avg: [-1613.501 -1613.501 -1613.501] (1.000)
Step: 53399, Reward: [-1758.005 -1758.005 -1758.005] [0.0000], Avg: [-1613.636 -1613.636 -1613.636] (1.000)
Step: 53449, Reward: [-1938.394 -1938.394 -1938.394] [0.0000], Avg: [-1613.94 -1613.94 -1613.94] (1.000)
Step: 53499, Reward: [-1801.843 -1801.843 -1801.843] [0.0000], Avg: [-1614.116 -1614.116 -1614.116] (1.000)
Step: 53549, Reward: [-2202.569 -2202.569 -2202.569] [0.0000], Avg: [-1614.665 -1614.665 -1614.665] (1.000)
Step: 53599, Reward: [-2026.923 -2026.923 -2026.923] [0.0000], Avg: [-1615.05 -1615.05 -1615.05] (1.000)
Step: 53649, Reward: [-1792.234 -1792.234 -1792.234] [0.0000], Avg: [-1615.215 -1615.215 -1615.215] (1.000)
Step: 53699, Reward: [-1877.052 -1877.052 -1877.052] [0.0000], Avg: [-1615.458 -1615.458 -1615.458] (1.000)
Step: 53749, Reward: [-1816.295 -1816.295 -1816.295] [0.0000], Avg: [-1615.645 -1615.645 -1615.645] (1.000)
Step: 53799, Reward: [-1875.17 -1875.17 -1875.17] [0.0000], Avg: [-1615.886 -1615.886 -1615.886] (1.000)
Step: 53849, Reward: [-1922.504 -1922.504 -1922.504] [0.0000], Avg: [-1616.171 -1616.171 -1616.171] (1.000)
Step: 53899, Reward: [-1708.976 -1708.976 -1708.976] [0.0000], Avg: [-1616.257 -1616.257 -1616.257] (1.000)
Step: 53949, Reward: [-1747.155 -1747.155 -1747.155] [0.0000], Avg: [-1616.379 -1616.379 -1616.379] (1.000)
Step: 53999, Reward: [-2083.237 -2083.237 -2083.237] [0.0000], Avg: [-1616.811 -1616.811 -1616.811] (1.000)
Step: 54049, Reward: [-2027.408 -2027.408 -2027.408] [0.0000], Avg: [-1617.191 -1617.191 -1617.191] (1.000)
Step: 54099, Reward: [-1941.558 -1941.558 -1941.558] [0.0000], Avg: [-1617.49 -1617.49 -1617.49] (1.000)
Step: 54149, Reward: [-1840.425 -1840.425 -1840.425] [0.0000], Avg: [-1617.696 -1617.696 -1617.696] (1.000)
Step: 54199, Reward: [-1883.481 -1883.481 -1883.481] [0.0000], Avg: [-1617.942 -1617.942 -1617.942] (1.000)
Step: 54249, Reward: [-1909.512 -1909.512 -1909.512] [0.0000], Avg: [-1618.21 -1618.21 -1618.21] (1.000)
Step: 54299, Reward: [-1672.31 -1672.31 -1672.31] [0.0000], Avg: [-1618.26 -1618.26 -1618.26] (1.000)
Step: 54349, Reward: [-1774.295 -1774.295 -1774.295] [0.0000], Avg: [-1618.404 -1618.404 -1618.404] (1.000)
Step: 54399, Reward: [-2114.165 -2114.165 -2114.165] [0.0000], Avg: [-1618.859 -1618.859 -1618.859] (1.000)
Step: 54449, Reward: [-1741.297 -1741.297 -1741.297] [0.0000], Avg: [-1618.972 -1618.972 -1618.972] (1.000)
Step: 54499, Reward: [-1764.237 -1764.237 -1764.237] [0.0000], Avg: [-1619.105 -1619.105 -1619.105] (1.000)
Step: 54549, Reward: [-1532.515 -1532.515 -1532.515] [0.0000], Avg: [-1619.026 -1619.026 -1619.026] (1.000)
Step: 54599, Reward: [-2163.971 -2163.971 -2163.971] [0.0000], Avg: [-1619.525 -1619.525 -1619.525] (1.000)
Step: 54649, Reward: [-2205.744 -2205.744 -2205.744] [0.0000], Avg: [-1620.061 -1620.061 -1620.061] (1.000)
Step: 54699, Reward: [-1923.287 -1923.287 -1923.287] [0.0000], Avg: [-1620.338 -1620.338 -1620.338] (1.000)
Step: 54749, Reward: [-1693.72 -1693.72 -1693.72] [0.0000], Avg: [-1620.405 -1620.405 -1620.405] (1.000)
Step: 54799, Reward: [-1988.089 -1988.089 -1988.089] [0.0000], Avg: [-1620.741 -1620.741 -1620.741] (1.000)
Step: 54849, Reward: [-2040.888 -2040.888 -2040.888] [0.0000], Avg: [-1621.124 -1621.124 -1621.124] (1.000)
Step: 54899, Reward: [-2130.766 -2130.766 -2130.766] [0.0000], Avg: [-1621.588 -1621.588 -1621.588] (1.000)
Step: 54949, Reward: [-1782.655 -1782.655 -1782.655] [0.0000], Avg: [-1621.734 -1621.734 -1621.734] (1.000)
Step: 54999, Reward: [-2092.523 -2092.523 -2092.523] [0.0000], Avg: [-1622.162 -1622.162 -1622.162] (1.000)
Step: 55049, Reward: [-2023.452 -2023.452 -2023.452] [0.0000], Avg: [-1622.527 -1622.527 -1622.527] (1.000)
Step: 55099, Reward: [-2207.191 -2207.191 -2207.191] [0.0000], Avg: [-1623.057 -1623.057 -1623.057] (1.000)
Step: 55149, Reward: [-1951.869 -1951.869 -1951.869] [0.0000], Avg: [-1623.355 -1623.355 -1623.355] (1.000)
Step: 55199, Reward: [-2449.361 -2449.361 -2449.361] [0.0000], Avg: [-1624.104 -1624.104 -1624.104] (1.000)
Step: 55249, Reward: [-1789.099 -1789.099 -1789.099] [0.0000], Avg: [-1624.253 -1624.253 -1624.253] (1.000)
Step: 55299, Reward: [-1861.832 -1861.832 -1861.832] [0.0000], Avg: [-1624.468 -1624.468 -1624.468] (1.000)
Step: 55349, Reward: [-1444.061 -1444.061 -1444.061] [0.0000], Avg: [-1624.305 -1624.305 -1624.305] (1.000)
Step: 55399, Reward: [-2103.246 -2103.246 -2103.246] [0.0000], Avg: [-1624.737 -1624.737 -1624.737] (1.000)
Step: 55449, Reward: [-1845.81 -1845.81 -1845.81] [0.0000], Avg: [-1624.936 -1624.936 -1624.936] (1.000)
Step: 55499, Reward: [-1904.674 -1904.674 -1904.674] [0.0000], Avg: [-1625.188 -1625.188 -1625.188] (1.000)
Step: 55549, Reward: [-1547.446 -1547.446 -1547.446] [0.0000], Avg: [-1625.118 -1625.118 -1625.118] (1.000)
Step: 55599, Reward: [-2095.39 -2095.39 -2095.39] [0.0000], Avg: [-1625.541 -1625.541 -1625.541] (1.000)
Step: 55649, Reward: [-1803.628 -1803.628 -1803.628] [0.0000], Avg: [-1625.701 -1625.701 -1625.701] (1.000)
Step: 55699, Reward: [-1447.207 -1447.207 -1447.207] [0.0000], Avg: [-1625.541 -1625.541 -1625.541] (1.000)
Step: 55749, Reward: [-2280.066 -2280.066 -2280.066] [0.0000], Avg: [-1626.128 -1626.128 -1626.128] (1.000)
Step: 55799, Reward: [-2088.419 -2088.419 -2088.419] [0.0000], Avg: [-1626.542 -1626.542 -1626.542] (1.000)
Step: 55849, Reward: [-1950.667 -1950.667 -1950.667] [0.0000], Avg: [-1626.833 -1626.833 -1626.833] (1.000)
Step: 55899, Reward: [-1667.459 -1667.459 -1667.459] [0.0000], Avg: [-1626.869 -1626.869 -1626.869] (1.000)
Step: 55949, Reward: [-1439.573 -1439.573 -1439.573] [0.0000], Avg: [-1626.702 -1626.702 -1626.702] (1.000)
Step: 55999, Reward: [-2015.398 -2015.398 -2015.398] [0.0000], Avg: [-1627.049 -1627.049 -1627.049] (1.000)
Step: 56049, Reward: [-1851.719 -1851.719 -1851.719] [0.0000], Avg: [-1627.249 -1627.249 -1627.249] (1.000)
Step: 56099, Reward: [-1909.319 -1909.319 -1909.319] [0.0000], Avg: [-1627.5 -1627.5 -1627.5] (1.000)
Step: 56149, Reward: [-1577.003 -1577.003 -1577.003] [0.0000], Avg: [-1627.455 -1627.455 -1627.455] (1.000)
Step: 56199, Reward: [-2238.536 -2238.536 -2238.536] [0.0000], Avg: [-1627.999 -1627.999 -1627.999] (1.000)
Step: 56249, Reward: [-1925.261 -1925.261 -1925.261] [0.0000], Avg: [-1628.263 -1628.263 -1628.263] (1.000)
Step: 56299, Reward: [-1775.03 -1775.03 -1775.03] [0.0000], Avg: [-1628.394 -1628.394 -1628.394] (1.000)
Step: 56349, Reward: [-2336.143 -2336.143 -2336.143] [0.0000], Avg: [-1629.022 -1629.022 -1629.022] (1.000)
Step: 56399, Reward: [-1980.107 -1980.107 -1980.107] [0.0000], Avg: [-1629.333 -1629.333 -1629.333] (1.000)
Step: 56449, Reward: [-2141.127 -2141.127 -2141.127] [0.0000], Avg: [-1629.786 -1629.786 -1629.786] (1.000)
Step: 56499, Reward: [-2180.901 -2180.901 -2180.901] [0.0000], Avg: [-1630.274 -1630.274 -1630.274] (1.000)
Step: 56549, Reward: [-1799.665 -1799.665 -1799.665] [0.0000], Avg: [-1630.424 -1630.424 -1630.424] (1.000)
Step: 56599, Reward: [-1298.748 -1298.748 -1298.748] [0.0000], Avg: [-1630.131 -1630.131 -1630.131] (1.000)
Step: 56649, Reward: [-2185.715 -2185.715 -2185.715] [0.0000], Avg: [-1630.621 -1630.621 -1630.621] (1.000)
Step: 56699, Reward: [-1656.541 -1656.541 -1656.541] [0.0000], Avg: [-1630.644 -1630.644 -1630.644] (1.000)
Step: 56749, Reward: [-2062.008 -2062.008 -2062.008] [0.0000], Avg: [-1631.024 -1631.024 -1631.024] (1.000)
Step: 56799, Reward: [-1952.475 -1952.475 -1952.475] [0.0000], Avg: [-1631.307 -1631.307 -1631.307] (1.000)
Step: 56849, Reward: [-1934.411 -1934.411 -1934.411] [0.0000], Avg: [-1631.574 -1631.574 -1631.574] (1.000)
Step: 56899, Reward: [-1755.366 -1755.366 -1755.366] [0.0000], Avg: [-1631.682 -1631.682 -1631.682] (1.000)
Step: 56949, Reward: [-2130.457 -2130.457 -2130.457] [0.0000], Avg: [-1632.12 -1632.12 -1632.12] (1.000)
Step: 56999, Reward: [-1590.211 -1590.211 -1590.211] [0.0000], Avg: [-1632.083 -1632.083 -1632.083] (1.000)
Step: 57049, Reward: [-2209.418 -2209.418 -2209.418] [0.0000], Avg: [-1632.589 -1632.589 -1632.589] (1.000)
Step: 57099, Reward: [-1931.602 -1931.602 -1931.602] [0.0000], Avg: [-1632.851 -1632.851 -1632.851] (1.000)
Step: 57149, Reward: [-1669.004 -1669.004 -1669.004] [0.0000], Avg: [-1632.883 -1632.883 -1632.883] (1.000)
Step: 57199, Reward: [-1689.394 -1689.394 -1689.394] [0.0000], Avg: [-1632.932 -1632.932 -1632.932] (1.000)
Step: 57249, Reward: [-1090.678 -1090.678 -1090.678] [0.0000], Avg: [-1632.459 -1632.459 -1632.459] (1.000)
Step: 57299, Reward: [-1846.009 -1846.009 -1846.009] [0.0000], Avg: [-1632.645 -1632.645 -1632.645] (1.000)
Step: 57349, Reward: [-1797.639 -1797.639 -1797.639] [0.0000], Avg: [-1632.789 -1632.789 -1632.789] (1.000)
Step: 57399, Reward: [-1729.769 -1729.769 -1729.769] [0.0000], Avg: [-1632.873 -1632.873 -1632.873] (1.000)
Step: 57449, Reward: [-2063.102 -2063.102 -2063.102] [0.0000], Avg: [-1633.248 -1633.248 -1633.248] (1.000)
Step: 57499, Reward: [-1978.643 -1978.643 -1978.643] [0.0000], Avg: [-1633.548 -1633.548 -1633.548] (1.000)
Step: 57549, Reward: [-2182.445 -2182.445 -2182.445] [0.0000], Avg: [-1634.025 -1634.025 -1634.025] (1.000)
Step: 57599, Reward: [-2029.224 -2029.224 -2029.224] [0.0000], Avg: [-1634.368 -1634.368 -1634.368] (1.000)
Step: 57649, Reward: [-2004.084 -2004.084 -2004.084] [0.0000], Avg: [-1634.689 -1634.689 -1634.689] (1.000)
Step: 57699, Reward: [-1791.662 -1791.662 -1791.662] [0.0000], Avg: [-1634.825 -1634.825 -1634.825] (1.000)
Step: 57749, Reward: [-1755.458 -1755.458 -1755.458] [0.0000], Avg: [-1634.929 -1634.929 -1634.929] (1.000)
Step: 57799, Reward: [-2004.423 -2004.423 -2004.423] [0.0000], Avg: [-1635.249 -1635.249 -1635.249] (1.000)
Step: 57849, Reward: [-2036.95 -2036.95 -2036.95] [0.0000], Avg: [-1635.596 -1635.596 -1635.596] (1.000)
Step: 57899, Reward: [-2127.298 -2127.298 -2127.298] [0.0000], Avg: [-1636.021 -1636.021 -1636.021] (1.000)
Step: 57949, Reward: [-1852.802 -1852.802 -1852.802] [0.0000], Avg: [-1636.208 -1636.208 -1636.208] (1.000)
Step: 57999, Reward: [-1818.198 -1818.198 -1818.198] [0.0000], Avg: [-1636.365 -1636.365 -1636.365] (1.000)
Step: 58049, Reward: [-2239.117 -2239.117 -2239.117] [0.0000], Avg: [-1636.884 -1636.884 -1636.884] (1.000)
Step: 58099, Reward: [-2144.025 -2144.025 -2144.025] [0.0000], Avg: [-1637.32 -1637.32 -1637.32] (1.000)
Step: 58149, Reward: [-1090.147 -1090.147 -1090.147] [0.0000], Avg: [-1636.85 -1636.85 -1636.85] (1.000)
Step: 58199, Reward: [-1833.489 -1833.489 -1833.489] [0.0000], Avg: [-1637.019 -1637.019 -1637.019] (1.000)
Step: 58249, Reward: [-1905.58 -1905.58 -1905.58] [0.0000], Avg: [-1637.249 -1637.249 -1637.249] (1.000)
Step: 58299, Reward: [-1877.84 -1877.84 -1877.84] [0.0000], Avg: [-1637.456 -1637.456 -1637.456] (1.000)
Step: 58349, Reward: [-2326.035 -2326.035 -2326.035] [0.0000], Avg: [-1638.046 -1638.046 -1638.046] (1.000)
Step: 58399, Reward: [-1667.472 -1667.472 -1667.472] [0.0000], Avg: [-1638.071 -1638.071 -1638.071] (1.000)
Step: 58449, Reward: [-1873.097 -1873.097 -1873.097] [0.0000], Avg: [-1638.272 -1638.272 -1638.272] (1.000)
Step: 58499, Reward: [-1420.56 -1420.56 -1420.56] [0.0000], Avg: [-1638.086 -1638.086 -1638.086] (1.000)
Step: 58549, Reward: [-1504.785 -1504.785 -1504.785] [0.0000], Avg: [-1637.972 -1637.972 -1637.972] (1.000)
Step: 58599, Reward: [-1705.898 -1705.898 -1705.898] [0.0000], Avg: [-1638.03 -1638.03 -1638.03] (1.000)
Step: 58649, Reward: [-2301.346 -2301.346 -2301.346] [0.0000], Avg: [-1638.595 -1638.595 -1638.595] (1.000)
Step: 58699, Reward: [-1722.316 -1722.316 -1722.316] [0.0000], Avg: [-1638.667 -1638.667 -1638.667] (1.000)
Step: 58749, Reward: [-1399.009 -1399.009 -1399.009] [0.0000], Avg: [-1638.463 -1638.463 -1638.463] (1.000)
Step: 58799, Reward: [-1815.186 -1815.186 -1815.186] [0.0000], Avg: [-1638.613 -1638.613 -1638.613] (1.000)
Step: 58849, Reward: [-1978.018 -1978.018 -1978.018] [0.0000], Avg: [-1638.901 -1638.901 -1638.901] (1.000)
Step: 58899, Reward: [-1140.741 -1140.741 -1140.741] [0.0000], Avg: [-1638.478 -1638.478 -1638.478] (1.000)
Step: 58949, Reward: [-2180.21 -2180.21 -2180.21] [0.0000], Avg: [-1638.938 -1638.938 -1638.938] (1.000)
Step: 58999, Reward: [-1001.158 -1001.158 -1001.158] [0.0000], Avg: [-1638.397 -1638.397 -1638.397] (1.000)
Step: 59049, Reward: [-1485.078 -1485.078 -1485.078] [0.0000], Avg: [-1638.268 -1638.268 -1638.268] (1.000)
Step: 59099, Reward: [-1571.961 -1571.961 -1571.961] [0.0000], Avg: [-1638.212 -1638.212 -1638.212] (1.000)
Step: 59149, Reward: [-2133.622 -2133.622 -2133.622] [0.0000], Avg: [-1638.63 -1638.63 -1638.63] (1.000)
Step: 59199, Reward: [-1967.337 -1967.337 -1967.337] [0.0000], Avg: [-1638.908 -1638.908 -1638.908] (1.000)
Step: 59249, Reward: [-1845.914 -1845.914 -1845.914] [0.0000], Avg: [-1639.083 -1639.083 -1639.083] (1.000)
Step: 59299, Reward: [-1599.969 -1599.969 -1599.969] [0.0000], Avg: [-1639.05 -1639.05 -1639.05] (1.000)
Step: 59349, Reward: [-2196.935 -2196.935 -2196.935] [0.0000], Avg: [-1639.52 -1639.52 -1639.52] (1.000)
Step: 59399, Reward: [-1769.945 -1769.945 -1769.945] [0.0000], Avg: [-1639.629 -1639.629 -1639.629] (1.000)
Step: 59449, Reward: [-1952.707 -1952.707 -1952.707] [0.0000], Avg: [-1639.893 -1639.893 -1639.893] (1.000)
Step: 59499, Reward: [-1591.656 -1591.656 -1591.656] [0.0000], Avg: [-1639.852 -1639.852 -1639.852] (1.000)
Step: 59549, Reward: [-1793.434 -1793.434 -1793.434] [0.0000], Avg: [-1639.981 -1639.981 -1639.981] (1.000)
Step: 59599, Reward: [-1460.818 -1460.818 -1460.818] [0.0000], Avg: [-1639.831 -1639.831 -1639.831] (1.000)
Step: 59649, Reward: [-1834.543 -1834.543 -1834.543] [0.0000], Avg: [-1639.994 -1639.994 -1639.994] (1.000)
Step: 59699, Reward: [-1579.923 -1579.923 -1579.923] [0.0000], Avg: [-1639.944 -1639.944 -1639.944] (1.000)
Step: 59749, Reward: [-1808.569 -1808.569 -1808.569] [0.0000], Avg: [-1640.085 -1640.085 -1640.085] (1.000)
Step: 59799, Reward: [-1317.31 -1317.31 -1317.31] [0.0000], Avg: [-1639.815 -1639.815 -1639.815] (1.000)
Step: 59849, Reward: [-2119.162 -2119.162 -2119.162] [0.0000], Avg: [-1640.215 -1640.215 -1640.215] (1.000)
Step: 59899, Reward: [-1634.53 -1634.53 -1634.53] [0.0000], Avg: [-1640.211 -1640.211 -1640.211] (1.000)
Step: 59949, Reward: [-1807.669 -1807.669 -1807.669] [0.0000], Avg: [-1640.35 -1640.35 -1640.35] (1.000)
Step: 59999, Reward: [-1916.857 -1916.857 -1916.857] [0.0000], Avg: [-1640.581 -1640.581 -1640.581] (1.000)
Step: 60049, Reward: [-1895.838 -1895.838 -1895.838] [0.0000], Avg: [-1640.793 -1640.793 -1640.793] (1.000)
Step: 60099, Reward: [-1571.876 -1571.876 -1571.876] [0.0000], Avg: [-1640.736 -1640.736 -1640.736] (1.000)
Step: 60149, Reward: [-1794.866 -1794.866 -1794.866] [0.0000], Avg: [-1640.864 -1640.864 -1640.864] (1.000)
Step: 60199, Reward: [-2189.771 -2189.771 -2189.771] [0.0000], Avg: [-1641.32 -1641.32 -1641.32] (1.000)
Step: 60249, Reward: [-1445.079 -1445.079 -1445.079] [0.0000], Avg: [-1641.157 -1641.157 -1641.157] (1.000)
Step: 60299, Reward: [-1516.431 -1516.431 -1516.431] [0.0000], Avg: [-1641.054 -1641.054 -1641.054] (1.000)
Step: 60349, Reward: [-1406.193 -1406.193 -1406.193] [0.0000], Avg: [-1640.859 -1640.859 -1640.859] (1.000)
Step: 60399, Reward: [-1975.319 -1975.319 -1975.319] [0.0000], Avg: [-1641.136 -1641.136 -1641.136] (1.000)
Step: 60449, Reward: [-1795.484 -1795.484 -1795.484] [0.0000], Avg: [-1641.264 -1641.264 -1641.264] (1.000)
Step: 60499, Reward: [-1661.183 -1661.183 -1661.183] [0.0000], Avg: [-1641.28 -1641.28 -1641.28] (1.000)
Step: 60549, Reward: [-2167.618 -2167.618 -2167.618] [0.0000], Avg: [-1641.715 -1641.715 -1641.715] (1.000)
Step: 60599, Reward: [-1955.27 -1955.27 -1955.27] [0.0000], Avg: [-1641.973 -1641.973 -1641.973] (1.000)
Step: 60649, Reward: [-1814.784 -1814.784 -1814.784] [0.0000], Avg: [-1642.116 -1642.116 -1642.116] (1.000)
Step: 60699, Reward: [-1590.161 -1590.161 -1590.161] [0.0000], Avg: [-1642.073 -1642.073 -1642.073] (1.000)
Step: 60749, Reward: [-1937.899 -1937.899 -1937.899] [0.0000], Avg: [-1642.317 -1642.317 -1642.317] (1.000)
Step: 60799, Reward: [-1859.534 -1859.534 -1859.534] [0.0000], Avg: [-1642.495 -1642.495 -1642.495] (1.000)
Step: 60849, Reward: [-1351.325 -1351.325 -1351.325] [0.0000], Avg: [-1642.256 -1642.256 -1642.256] (1.000)
Step: 60899, Reward: [-2194.455 -2194.455 -2194.455] [0.0000], Avg: [-1642.709 -1642.709 -1642.709] (1.000)
Step: 60949, Reward: [-2070.085 -2070.085 -2070.085] [0.0000], Avg: [-1643.06 -1643.06 -1643.06] (1.000)
Step: 60999, Reward: [-1876.691 -1876.691 -1876.691] [0.0000], Avg: [-1643.251 -1643.251 -1643.251] (1.000)
Step: 61049, Reward: [-1975.388 -1975.388 -1975.388] [0.0000], Avg: [-1643.523 -1643.523 -1643.523] (1.000)
Step: 61099, Reward: [-1573.256 -1573.256 -1573.256] [0.0000], Avg: [-1643.466 -1643.466 -1643.466] (1.000)
Step: 61149, Reward: [-1898.79 -1898.79 -1898.79] [0.0000], Avg: [-1643.675 -1643.675 -1643.675] (1.000)
Step: 61199, Reward: [-1631.77 -1631.77 -1631.77] [0.0000], Avg: [-1643.665 -1643.665 -1643.665] (1.000)
Step: 61249, Reward: [-1430.959 -1430.959 -1430.959] [0.0000], Avg: [-1643.491 -1643.491 -1643.491] (1.000)
Step: 61299, Reward: [-1869.249 -1869.249 -1869.249] [0.0000], Avg: [-1643.676 -1643.676 -1643.676] (1.000)
Step: 61349, Reward: [-1718.719 -1718.719 -1718.719] [0.0000], Avg: [-1643.737 -1643.737 -1643.737] (1.000)
Step: 61399, Reward: [-1800.512 -1800.512 -1800.512] [0.0000], Avg: [-1643.864 -1643.864 -1643.864] (1.000)
Step: 61449, Reward: [-1249.609 -1249.609 -1249.609] [0.0000], Avg: [-1643.544 -1643.544 -1643.544] (1.000)
Step: 61499, Reward: [-1553.934 -1553.934 -1553.934] [0.0000], Avg: [-1643.471 -1643.471 -1643.471] (1.000)
Step: 61549, Reward: [-1503.49 -1503.49 -1503.49] [0.0000], Avg: [-1643.357 -1643.357 -1643.357] (1.000)
Step: 61599, Reward: [-1492.065 -1492.065 -1492.065] [0.0000], Avg: [-1643.234 -1643.234 -1643.234] (1.000)
Step: 61649, Reward: [-1721.725 -1721.725 -1721.725] [0.0000], Avg: [-1643.298 -1643.298 -1643.298] (1.000)
Step: 61699, Reward: [-2110.485 -2110.485 -2110.485] [0.0000], Avg: [-1643.676 -1643.676 -1643.676] (1.000)
Step: 61749, Reward: [-2096.158 -2096.158 -2096.158] [0.0000], Avg: [-1644.043 -1644.043 -1644.043] (1.000)
Step: 61799, Reward: [-2252.714 -2252.714 -2252.714] [0.0000], Avg: [-1644.535 -1644.535 -1644.535] (1.000)
Step: 61849, Reward: [-1435.805 -1435.805 -1435.805] [0.0000], Avg: [-1644.367 -1644.367 -1644.367] (1.000)
Step: 61899, Reward: [-1305.496 -1305.496 -1305.496] [0.0000], Avg: [-1644.093 -1644.093 -1644.093] (1.000)
Step: 61949, Reward: [-1621.604 -1621.604 -1621.604] [0.0000], Avg: [-1644.075 -1644.075 -1644.075] (1.000)
Step: 61999, Reward: [-1599.005 -1599.005 -1599.005] [0.0000], Avg: [-1644.038 -1644.038 -1644.038] (1.000)
Step: 62049, Reward: [-1403.069 -1403.069 -1403.069] [0.0000], Avg: [-1643.844 -1643.844 -1643.844] (1.000)
Step: 62099, Reward: [-2172.717 -2172.717 -2172.717] [0.0000], Avg: [-1644.27 -1644.27 -1644.27] (1.000)
Step: 62149, Reward: [-1609.211 -1609.211 -1609.211] [0.0000], Avg: [-1644.242 -1644.242 -1644.242] (1.000)
Step: 62199, Reward: [-777.988 -777.988 -777.988] [0.0000], Avg: [-1643.545 -1643.545 -1643.545] (1.000)
Step: 62249, Reward: [-1996.131 -1996.131 -1996.131] [0.0000], Avg: [-1643.829 -1643.829 -1643.829] (1.000)
Step: 62299, Reward: [-1889.81 -1889.81 -1889.81] [0.0000], Avg: [-1644.026 -1644.026 -1644.026] (1.000)
Step: 62349, Reward: [-1635.68 -1635.68 -1635.68] [0.0000], Avg: [-1644.019 -1644.019 -1644.019] (1.000)
Step: 62399, Reward: [-1693.232 -1693.232 -1693.232] [0.0000], Avg: [-1644.059 -1644.059 -1644.059] (1.000)
Step: 62449, Reward: [-833.92 -833.92 -833.92] [0.0000], Avg: [-1643.41 -1643.41 -1643.41] (1.000)
Step: 62499, Reward: [-1627.7 -1627.7 -1627.7] [0.0000], Avg: [-1643.398 -1643.398 -1643.398] (1.000)
Step: 62549, Reward: [-1621.203 -1621.203 -1621.203] [0.0000], Avg: [-1643.38 -1643.38 -1643.38] (1.000)
Step: 62599, Reward: [-1757.555 -1757.555 -1757.555] [0.0000], Avg: [-1643.471 -1643.471 -1643.471] (1.000)
Step: 62649, Reward: [-1307.964 -1307.964 -1307.964] [0.0000], Avg: [-1643.203 -1643.203 -1643.203] (1.000)
Step: 62699, Reward: [-1337.147 -1337.147 -1337.147] [0.0000], Avg: [-1642.959 -1642.959 -1642.959] (1.000)
Step: 62749, Reward: [-1851.705 -1851.705 -1851.705] [0.0000], Avg: [-1643.126 -1643.126 -1643.126] (1.000)
Step: 62799, Reward: [-1395.886 -1395.886 -1395.886] [0.0000], Avg: [-1642.929 -1642.929 -1642.929] (1.000)
Step: 62849, Reward: [-1276.679 -1276.679 -1276.679] [0.0000], Avg: [-1642.637 -1642.637 -1642.637] (1.000)
Step: 62899, Reward: [-1828.229 -1828.229 -1828.229] [0.0000], Avg: [-1642.785 -1642.785 -1642.785] (1.000)
Step: 62949, Reward: [-1766.102 -1766.102 -1766.102] [0.0000], Avg: [-1642.883 -1642.883 -1642.883] (1.000)
Step: 62999, Reward: [-1503.162 -1503.162 -1503.162] [0.0000], Avg: [-1642.772 -1642.772 -1642.772] (1.000)
Step: 63049, Reward: [-2162.644 -2162.644 -2162.644] [0.0000], Avg: [-1643.184 -1643.184 -1643.184] (1.000)
Step: 63099, Reward: [-1817.035 -1817.035 -1817.035] [0.0000], Avg: [-1643.322 -1643.322 -1643.322] (1.000)
Step: 63149, Reward: [-1847.76 -1847.76 -1847.76] [0.0000], Avg: [-1643.484 -1643.484 -1643.484] (1.000)
Step: 63199, Reward: [-1221.268 -1221.268 -1221.268] [0.0000], Avg: [-1643.15 -1643.15 -1643.15] (1.000)
Step: 63249, Reward: [-1694.097 -1694.097 -1694.097] [0.0000], Avg: [-1643.19 -1643.19 -1643.19] (1.000)
Step: 63299, Reward: [-2279.681 -2279.681 -2279.681] [0.0000], Avg: [-1643.693 -1643.693 -1643.693] (1.000)
Step: 63349, Reward: [-1556.106 -1556.106 -1556.106] [0.0000], Avg: [-1643.624 -1643.624 -1643.624] (1.000)
Step: 63399, Reward: [-1925.918 -1925.918 -1925.918] [0.0000], Avg: [-1643.846 -1643.846 -1643.846] (1.000)
Step: 63449, Reward: [-1547.172 -1547.172 -1547.172] [0.0000], Avg: [-1643.77 -1643.77 -1643.77] (1.000)
Step: 63499, Reward: [-1588.272 -1588.272 -1588.272] [0.0000], Avg: [-1643.726 -1643.726 -1643.726] (1.000)
Step: 63549, Reward: [-1970.86 -1970.86 -1970.86] [0.0000], Avg: [-1643.984 -1643.984 -1643.984] (1.000)
Step: 63599, Reward: [-1561.927 -1561.927 -1561.927] [0.0000], Avg: [-1643.919 -1643.919 -1643.919] (1.000)
Step: 63649, Reward: [-1810.233 -1810.233 -1810.233] [0.0000], Avg: [-1644.05 -1644.05 -1644.05] (1.000)
Step: 63699, Reward: [-1818.626 -1818.626 -1818.626] [0.0000], Avg: [-1644.187 -1644.187 -1644.187] (1.000)
Step: 63749, Reward: [-1776.3 -1776.3 -1776.3] [0.0000], Avg: [-1644.291 -1644.291 -1644.291] (1.000)
Step: 63799, Reward: [-2008.898 -2008.898 -2008.898] [0.0000], Avg: [-1644.576 -1644.576 -1644.576] (1.000)
Step: 63849, Reward: [-1949.613 -1949.613 -1949.613] [0.0000], Avg: [-1644.815 -1644.815 -1644.815] (1.000)
Step: 63899, Reward: [-1713.683 -1713.683 -1713.683] [0.0000], Avg: [-1644.869 -1644.869 -1644.869] (1.000)
Step: 63949, Reward: [-2010.82 -2010.82 -2010.82] [0.0000], Avg: [-1645.155 -1645.155 -1645.155] (1.000)
Step: 63999, Reward: [-1597.673 -1597.673 -1597.673] [0.0000], Avg: [-1645.118 -1645.118 -1645.118] (1.000)
Step: 64049, Reward: [-2113.203 -2113.203 -2113.203] [0.0000], Avg: [-1645.484 -1645.484 -1645.484] (1.000)
Step: 64099, Reward: [-1573.674 -1573.674 -1573.674] [0.0000], Avg: [-1645.428 -1645.428 -1645.428] (1.000)
Step: 64149, Reward: [-1587.102 -1587.102 -1587.102] [0.0000], Avg: [-1645.382 -1645.382 -1645.382] (1.000)
Step: 64199, Reward: [-1478.985 -1478.985 -1478.985] [0.0000], Avg: [-1645.252 -1645.252 -1645.252] (1.000)
Step: 64249, Reward: [-1178.58 -1178.58 -1178.58] [0.0000], Avg: [-1644.889 -1644.889 -1644.889] (1.000)
Step: 64299, Reward: [-1631.714 -1631.714 -1631.714] [0.0000], Avg: [-1644.879 -1644.879 -1644.879] (1.000)
Step: 64349, Reward: [-1611.486 -1611.486 -1611.486] [0.0000], Avg: [-1644.853 -1644.853 -1644.853] (1.000)
Step: 64399, Reward: [-1749.6 -1749.6 -1749.6] [0.0000], Avg: [-1644.934 -1644.934 -1644.934] (1.000)
Step: 64449, Reward: [-1797.594 -1797.594 -1797.594] [0.0000], Avg: [-1645.053 -1645.053 -1645.053] (1.000)
Step: 64499, Reward: [-1576.763 -1576.763 -1576.763] [0.0000], Avg: [-1645. -1645. -1645.] (1.000)
Step: 64549, Reward: [-2039.616 -2039.616 -2039.616] [0.0000], Avg: [-1645.306 -1645.306 -1645.306] (1.000)
Step: 64599, Reward: [-1918.495 -1918.495 -1918.495] [0.0000], Avg: [-1645.517 -1645.517 -1645.517] (1.000)
Step: 64649, Reward: [-1502.203 -1502.203 -1502.203] [0.0000], Avg: [-1645.406 -1645.406 -1645.406] (1.000)
Step: 64699, Reward: [-1520.602 -1520.602 -1520.602] [0.0000], Avg: [-1645.31 -1645.31 -1645.31] (1.000)
Step: 64749, Reward: [-1487.962 -1487.962 -1487.962] [0.0000], Avg: [-1645.188 -1645.188 -1645.188] (1.000)
Step: 64799, Reward: [-1847.793 -1847.793 -1847.793] [0.0000], Avg: [-1645.345 -1645.345 -1645.345] (1.000)
Step: 64849, Reward: [-1749.195 -1749.195 -1749.195] [0.0000], Avg: [-1645.425 -1645.425 -1645.425] (1.000)
Step: 64899, Reward: [-2110.704 -2110.704 -2110.704] [0.0000], Avg: [-1645.783 -1645.783 -1645.783] (1.000)
Step: 64949, Reward: [-1639.356 -1639.356 -1639.356] [0.0000], Avg: [-1645.778 -1645.778 -1645.778] (1.000)
Step: 64999, Reward: [-1597.229 -1597.229 -1597.229] [0.0000], Avg: [-1645.741 -1645.741 -1645.741] (1.000)
Step: 65049, Reward: [-1268.711 -1268.711 -1268.711] [0.0000], Avg: [-1645.451 -1645.451 -1645.451] (1.000)
Step: 65099, Reward: [-1357.57 -1357.57 -1357.57] [0.0000], Avg: [-1645.23 -1645.23 -1645.23] (1.000)
Step: 65149, Reward: [-1792.804 -1792.804 -1792.804] [0.0000], Avg: [-1645.343 -1645.343 -1645.343] (1.000)
Step: 65199, Reward: [-2107.534 -2107.534 -2107.534] [0.0000], Avg: [-1645.698 -1645.698 -1645.698] (1.000)
Step: 65249, Reward: [-1491.933 -1491.933 -1491.933] [0.0000], Avg: [-1645.58 -1645.58 -1645.58] (1.000)
Step: 65299, Reward: [-1450.636 -1450.636 -1450.636] [0.0000], Avg: [-1645.431 -1645.431 -1645.431] (1.000)
Step: 65349, Reward: [-1784.558 -1784.558 -1784.558] [0.0000], Avg: [-1645.537 -1645.537 -1645.537] (1.000)
Step: 65399, Reward: [-1783.114 -1783.114 -1783.114] [0.0000], Avg: [-1645.642 -1645.642 -1645.642] (1.000)
Step: 65449, Reward: [-1756.455 -1756.455 -1756.455] [0.0000], Avg: [-1645.727 -1645.727 -1645.727] (1.000)
Step: 65499, Reward: [-1841.051 -1841.051 -1841.051] [0.0000], Avg: [-1645.876 -1645.876 -1645.876] (1.000)
Step: 65549, Reward: [-1724.139 -1724.139 -1724.139] [0.0000], Avg: [-1645.936 -1645.936 -1645.936] (1.000)
Step: 65599, Reward: [-1504.993 -1504.993 -1504.993] [0.0000], Avg: [-1645.828 -1645.828 -1645.828] (1.000)
Step: 65649, Reward: [-1786.9 -1786.9 -1786.9] [0.0000], Avg: [-1645.936 -1645.936 -1645.936] (1.000)
Step: 65699, Reward: [-1272.061 -1272.061 -1272.061] [0.0000], Avg: [-1645.651 -1645.651 -1645.651] (1.000)
Step: 65749, Reward: [-1671.903 -1671.903 -1671.903] [0.0000], Avg: [-1645.671 -1645.671 -1645.671] (1.000)
Step: 65799, Reward: [-1265.031 -1265.031 -1265.031] [0.0000], Avg: [-1645.382 -1645.382 -1645.382] (1.000)
Step: 65849, Reward: [-1321.432 -1321.432 -1321.432] [0.0000], Avg: [-1645.136 -1645.136 -1645.136] (1.000)
Step: 65899, Reward: [-2005.219 -2005.219 -2005.219] [0.0000], Avg: [-1645.409 -1645.409 -1645.409] (1.000)
Step: 65949, Reward: [-1227.01 -1227.01 -1227.01] [0.0000], Avg: [-1645.092 -1645.092 -1645.092] (1.000)
Step: 65999, Reward: [-1771.618 -1771.618 -1771.618] [0.0000], Avg: [-1645.188 -1645.188 -1645.188] (1.000)
Step: 66049, Reward: [-1403.731 -1403.731 -1403.731] [0.0000], Avg: [-1645.005 -1645.005 -1645.005] (1.000)
Step: 66099, Reward: [-1907.037 -1907.037 -1907.037] [0.0000], Avg: [-1645.203 -1645.203 -1645.203] (1.000)
Step: 66149, Reward: [-1698.256 -1698.256 -1698.256] [0.0000], Avg: [-1645.243 -1645.243 -1645.243] (1.000)
Step: 66199, Reward: [-1464.485 -1464.485 -1464.485] [0.0000], Avg: [-1645.107 -1645.107 -1645.107] (1.000)
Step: 66249, Reward: [-1732.758 -1732.758 -1732.758] [0.0000], Avg: [-1645.173 -1645.173 -1645.173] (1.000)
Step: 66299, Reward: [-1860.776 -1860.776 -1860.776] [0.0000], Avg: [-1645.335 -1645.335 -1645.335] (1.000)
Step: 66349, Reward: [-1712.05 -1712.05 -1712.05] [0.0000], Avg: [-1645.386 -1645.386 -1645.386] (1.000)
Step: 66399, Reward: [-2335.423 -2335.423 -2335.423] [0.0000], Avg: [-1645.905 -1645.905 -1645.905] (1.000)
Step: 66449, Reward: [-1801.663 -1801.663 -1801.663] [0.0000], Avg: [-1646.023 -1646.023 -1646.023] (1.000)
Step: 66499, Reward: [-1760.773 -1760.773 -1760.773] [0.0000], Avg: [-1646.109 -1646.109 -1646.109] (1.000)
Step: 66549, Reward: [-1523.662 -1523.662 -1523.662] [0.0000], Avg: [-1646.017 -1646.017 -1646.017] (1.000)
Step: 66599, Reward: [-1550.591 -1550.591 -1550.591] [0.0000], Avg: [-1645.945 -1645.945 -1645.945] (1.000)
Step: 66649, Reward: [-1880.934 -1880.934 -1880.934] [0.0000], Avg: [-1646.121 -1646.121 -1646.121] (1.000)
Step: 66699, Reward: [-1343.488 -1343.488 -1343.488] [0.0000], Avg: [-1645.895 -1645.895 -1645.895] (1.000)
Step: 66749, Reward: [-1483.82 -1483.82 -1483.82] [0.0000], Avg: [-1645.773 -1645.773 -1645.773] (1.000)
Step: 66799, Reward: [-1825.32 -1825.32 -1825.32] [0.0000], Avg: [-1645.908 -1645.908 -1645.908] (1.000)
Step: 66849, Reward: [-1549.34 -1549.34 -1549.34] [0.0000], Avg: [-1645.835 -1645.835 -1645.835] (1.000)
Step: 66899, Reward: [-1618.651 -1618.651 -1618.651] [0.0000], Avg: [-1645.815 -1645.815 -1645.815] (1.000)
Step: 66949, Reward: [-1592.973 -1592.973 -1592.973] [0.0000], Avg: [-1645.776 -1645.776 -1645.776] (1.000)
Step: 66999, Reward: [-1319.406 -1319.406 -1319.406] [0.0000], Avg: [-1645.532 -1645.532 -1645.532] (1.000)
Step: 67049, Reward: [-1471.037 -1471.037 -1471.037] [0.0000], Avg: [-1645.402 -1645.402 -1645.402] (1.000)
Step: 67099, Reward: [-1784.38 -1784.38 -1784.38] [0.0000], Avg: [-1645.505 -1645.505 -1645.505] (1.000)
Step: 67149, Reward: [-1677.088 -1677.088 -1677.088] [0.0000], Avg: [-1645.529 -1645.529 -1645.529] (1.000)
Step: 67199, Reward: [-1508.927 -1508.927 -1508.927] [0.0000], Avg: [-1645.427 -1645.427 -1645.427] (1.000)
Step: 67249, Reward: [-1680.538 -1680.538 -1680.538] [0.0000], Avg: [-1645.453 -1645.453 -1645.453] (1.000)
Step: 67299, Reward: [-1722.683 -1722.683 -1722.683] [0.0000], Avg: [-1645.511 -1645.511 -1645.511] (1.000)
Step: 67349, Reward: [-1671.648 -1671.648 -1671.648] [0.0000], Avg: [-1645.53 -1645.53 -1645.53] (1.000)
Step: 67399, Reward: [-1573.264 -1573.264 -1573.264] [0.0000], Avg: [-1645.477 -1645.477 -1645.477] (1.000)
Step: 67449, Reward: [-1720.444 -1720.444 -1720.444] [0.0000], Avg: [-1645.532 -1645.532 -1645.532] (1.000)
Step: 67499, Reward: [-1482.273 -1482.273 -1482.273] [0.0000], Avg: [-1645.411 -1645.411 -1645.411] (1.000)
Step: 67549, Reward: [-2152.2 -2152.2 -2152.2] [0.0000], Avg: [-1645.786 -1645.786 -1645.786] (1.000)
Step: 67599, Reward: [-1755.882 -1755.882 -1755.882] [0.0000], Avg: [-1645.868 -1645.868 -1645.868] (1.000)
Step: 67649, Reward: [-1634.506 -1634.506 -1634.506] [0.0000], Avg: [-1645.859 -1645.859 -1645.859] (1.000)
Step: 67699, Reward: [-1679.891 -1679.891 -1679.891] [0.0000], Avg: [-1645.885 -1645.885 -1645.885] (1.000)
Step: 67749, Reward: [-1154.376 -1154.376 -1154.376] [0.0000], Avg: [-1645.522 -1645.522 -1645.522] (1.000)
Step: 67799, Reward: [-1543.467 -1543.467 -1543.467] [0.0000], Avg: [-1645.447 -1645.447 -1645.447] (1.000)
Step: 67849, Reward: [-1865.214 -1865.214 -1865.214] [0.0000], Avg: [-1645.608 -1645.608 -1645.608] (1.000)
Step: 67899, Reward: [-1691.458 -1691.458 -1691.458] [0.0000], Avg: [-1645.642 -1645.642 -1645.642] (1.000)
Step: 67949, Reward: [-1672.2 -1672.2 -1672.2] [0.0000], Avg: [-1645.662 -1645.662 -1645.662] (1.000)
Step: 67999, Reward: [-1663.334 -1663.334 -1663.334] [0.0000], Avg: [-1645.675 -1645.675 -1645.675] (1.000)
Step: 68049, Reward: [-1593.52 -1593.52 -1593.52] [0.0000], Avg: [-1645.636 -1645.636 -1645.636] (1.000)
Step: 68099, Reward: [-1330.829 -1330.829 -1330.829] [0.0000], Avg: [-1645.405 -1645.405 -1645.405] (1.000)
Step: 68149, Reward: [-1503.854 -1503.854 -1503.854] [0.0000], Avg: [-1645.301 -1645.301 -1645.301] (1.000)
Step: 68199, Reward: [-2269.314 -2269.314 -2269.314] [0.0000], Avg: [-1645.759 -1645.759 -1645.759] (1.000)
Step: 68249, Reward: [-1541.292 -1541.292 -1541.292] [0.0000], Avg: [-1645.682 -1645.682 -1645.682] (1.000)
Step: 68299, Reward: [-1319.253 -1319.253 -1319.253] [0.0000], Avg: [-1645.443 -1645.443 -1645.443] (1.000)
Step: 68349, Reward: [-1703.164 -1703.164 -1703.164] [0.0000], Avg: [-1645.486 -1645.486 -1645.486] (1.000)
Step: 68399, Reward: [-1997.326 -1997.326 -1997.326] [0.0000], Avg: [-1645.743 -1645.743 -1645.743] (1.000)
Step: 68449, Reward: [-1197.989 -1197.989 -1197.989] [0.0000], Avg: [-1645.416 -1645.416 -1645.416] (1.000)
Step: 68499, Reward: [-1541.253 -1541.253 -1541.253] [0.0000], Avg: [-1645.34 -1645.34 -1645.34] (1.000)
Step: 68549, Reward: [-1689.972 -1689.972 -1689.972] [0.0000], Avg: [-1645.372 -1645.372 -1645.372] (1.000)
Step: 68599, Reward: [-1080.13 -1080.13 -1080.13] [0.0000], Avg: [-1644.96 -1644.96 -1644.96] (1.000)
Step: 68649, Reward: [-1724.976 -1724.976 -1724.976] [0.0000], Avg: [-1645.019 -1645.019 -1645.019] (1.000)
Step: 68699, Reward: [-1727.375 -1727.375 -1727.375] [0.0000], Avg: [-1645.079 -1645.079 -1645.079] (1.000)
Step: 68749, Reward: [-1682.918 -1682.918 -1682.918] [0.0000], Avg: [-1645.106 -1645.106 -1645.106] (1.000)
Step: 68799, Reward: [-1371.182 -1371.182 -1371.182] [0.0000], Avg: [-1644.907 -1644.907 -1644.907] (1.000)
Step: 68849, Reward: [-1025.426 -1025.426 -1025.426] [0.0000], Avg: [-1644.457 -1644.457 -1644.457] (1.000)
Step: 68899, Reward: [-2093.963 -2093.963 -2093.963] [0.0000], Avg: [-1644.783 -1644.783 -1644.783] (1.000)
Step: 68949, Reward: [-1766.992 -1766.992 -1766.992] [0.0000], Avg: [-1644.872 -1644.872 -1644.872] (1.000)
Step: 68999, Reward: [-2254.313 -2254.313 -2254.313] [0.0000], Avg: [-1645.314 -1645.314 -1645.314] (1.000)
Step: 69049, Reward: [-1660.799 -1660.799 -1660.799] [0.0000], Avg: [-1645.325 -1645.325 -1645.325] (1.000)
Step: 69099, Reward: [-1114.757 -1114.757 -1114.757] [0.0000], Avg: [-1644.941 -1644.941 -1644.941] (1.000)
Step: 69149, Reward: [-1828.457 -1828.457 -1828.457] [0.0000], Avg: [-1645.074 -1645.074 -1645.074] (1.000)
Step: 69199, Reward: [-1520.416 -1520.416 -1520.416] [0.0000], Avg: [-1644.983 -1644.983 -1644.983] (1.000)
Step: 69249, Reward: [-1443.237 -1443.237 -1443.237] [0.0000], Avg: [-1644.838 -1644.838 -1644.838] (1.000)
Step: 69299, Reward: [-1720.792 -1720.792 -1720.792] [0.0000], Avg: [-1644.893 -1644.893 -1644.893] (1.000)
Step: 69349, Reward: [-1804.838 -1804.838 -1804.838] [0.0000], Avg: [-1645.008 -1645.008 -1645.008] (1.000)
Step: 69399, Reward: [-2164.43 -2164.43 -2164.43] [0.0000], Avg: [-1645.382 -1645.382 -1645.382] (1.000)
Step: 69449, Reward: [-935.003 -935.003 -935.003] [0.0000], Avg: [-1644.871 -1644.871 -1644.871] (1.000)
Step: 69499, Reward: [-1693.805 -1693.805 -1693.805] [0.0000], Avg: [-1644.906 -1644.906 -1644.906] (1.000)
Step: 69549, Reward: [-2038.246 -2038.246 -2038.246] [0.0000], Avg: [-1645.189 -1645.189 -1645.189] (1.000)
Step: 69599, Reward: [-1466.085 -1466.085 -1466.085] [0.0000], Avg: [-1645.06 -1645.06 -1645.06] (1.000)
Step: 69649, Reward: [-1411.02 -1411.02 -1411.02] [0.0000], Avg: [-1644.892 -1644.892 -1644.892] (1.000)
Step: 69699, Reward: [-1217.69 -1217.69 -1217.69] [0.0000], Avg: [-1644.586 -1644.586 -1644.586] (1.000)
Step: 69749, Reward: [-1703.844 -1703.844 -1703.844] [0.0000], Avg: [-1644.628 -1644.628 -1644.628] (1.000)
Step: 69799, Reward: [-1995.406 -1995.406 -1995.406] [0.0000], Avg: [-1644.879 -1644.879 -1644.879] (1.000)
Step: 69849, Reward: [-1721.694 -1721.694 -1721.694] [0.0000], Avg: [-1644.934 -1644.934 -1644.934] (1.000)
Step: 69899, Reward: [-2078.214 -2078.214 -2078.214] [0.0000], Avg: [-1645.244 -1645.244 -1645.244] (1.000)
Step: 69949, Reward: [-2090.353 -2090.353 -2090.353] [0.0000], Avg: [-1645.562 -1645.562 -1645.562] (1.000)
Step: 69999, Reward: [-1445.928 -1445.928 -1445.928] [0.0000], Avg: [-1645.42 -1645.42 -1645.42] (1.000)
Step: 70049, Reward: [-1446.057 -1446.057 -1446.057] [0.0000], Avg: [-1645.278 -1645.278 -1645.278] (1.000)
Step: 70099, Reward: [-1062.64 -1062.64 -1062.64] [0.0000], Avg: [-1644.862 -1644.862 -1644.862] (1.000)
Step: 70149, Reward: [-1163.244 -1163.244 -1163.244] [0.0000], Avg: [-1644.519 -1644.519 -1644.519] (1.000)
Step: 70199, Reward: [-1535.206 -1535.206 -1535.206] [0.0000], Avg: [-1644.441 -1644.441 -1644.441] (1.000)
Step: 70249, Reward: [-1583.963 -1583.963 -1583.963] [0.0000], Avg: [-1644.398 -1644.398 -1644.398] (1.000)
Step: 70299, Reward: [-1450.054 -1450.054 -1450.054] [0.0000], Avg: [-1644.26 -1644.26 -1644.26] (1.000)
Step: 70349, Reward: [-1385.904 -1385.904 -1385.904] [0.0000], Avg: [-1644.076 -1644.076 -1644.076] (1.000)
Step: 70399, Reward: [-1367.094 -1367.094 -1367.094] [0.0000], Avg: [-1643.879 -1643.879 -1643.879] (1.000)
Step: 70449, Reward: [-1301.74 -1301.74 -1301.74] [0.0000], Avg: [-1643.636 -1643.636 -1643.636] (1.000)
Step: 70499, Reward: [-1548.729 -1548.729 -1548.729] [0.0000], Avg: [-1643.569 -1643.569 -1643.569] (1.000)
Step: 70549, Reward: [-1580.202 -1580.202 -1580.202] [0.0000], Avg: [-1643.524 -1643.524 -1643.524] (1.000)
Step: 70599, Reward: [-2028.972 -2028.972 -2028.972] [0.0000], Avg: [-1643.797 -1643.797 -1643.797] (1.000)
Step: 70649, Reward: [-1420.487 -1420.487 -1420.487] [0.0000], Avg: [-1643.639 -1643.639 -1643.639] (1.000)
Step: 70699, Reward: [-1633.868 -1633.868 -1633.868] [0.0000], Avg: [-1643.632 -1643.632 -1643.632] (1.000)
Step: 70749, Reward: [-1739.388 -1739.388 -1739.388] [0.0000], Avg: [-1643.7 -1643.7 -1643.7] (1.000)
Step: 70799, Reward: [-1765.868 -1765.868 -1765.868] [0.0000], Avg: [-1643.786 -1643.786 -1643.786] (1.000)
Step: 70849, Reward: [-1810.944 -1810.944 -1810.944] [0.0000], Avg: [-1643.904 -1643.904 -1643.904] (1.000)
Step: 70899, Reward: [-1991.256 -1991.256 -1991.256] [0.0000], Avg: [-1644.149 -1644.149 -1644.149] (1.000)
Step: 70949, Reward: [-1664.149 -1664.149 -1664.149] [0.0000], Avg: [-1644.163 -1644.163 -1644.163] (1.000)
Step: 70999, Reward: [-1230.511 -1230.511 -1230.511] [0.0000], Avg: [-1643.872 -1643.872 -1643.872] (1.000)
Step: 71049, Reward: [-1637.805 -1637.805 -1637.805] [0.0000], Avg: [-1643.868 -1643.868 -1643.868] (1.000)
Step: 71099, Reward: [-1560.976 -1560.976 -1560.976] [0.0000], Avg: [-1643.809 -1643.809 -1643.809] (1.000)
Step: 71149, Reward: [-2019.016 -2019.016 -2019.016] [0.0000], Avg: [-1644.073 -1644.073 -1644.073] (1.000)
Step: 71199, Reward: [-1512.217 -1512.217 -1512.217] [0.0000], Avg: [-1643.98 -1643.98 -1643.98] (1.000)
Step: 71249, Reward: [-1182.416 -1182.416 -1182.416] [0.0000], Avg: [-1643.656 -1643.656 -1643.656] (1.000)
Step: 71299, Reward: [-1910.353 -1910.353 -1910.353] [0.0000], Avg: [-1643.843 -1643.843 -1643.843] (1.000)
Step: 71349, Reward: [-1710.536 -1710.536 -1710.536] [0.0000], Avg: [-1643.89 -1643.89 -1643.89] (1.000)
Step: 71399, Reward: [-2039.291 -2039.291 -2039.291] [0.0000], Avg: [-1644.167 -1644.167 -1644.167] (1.000)
Step: 71449, Reward: [-853.662 -853.662 -853.662] [0.0000], Avg: [-1643.614 -1643.614 -1643.614] (1.000)
Step: 71499, Reward: [-2321.876 -2321.876 -2321.876] [0.0000], Avg: [-1644.088 -1644.088 -1644.088] (1.000)
Step: 71549, Reward: [-1579.666 -1579.666 -1579.666] [0.0000], Avg: [-1644.043 -1644.043 -1644.043] (1.000)
Step: 71599, Reward: [-1707.452 -1707.452 -1707.452] [0.0000], Avg: [-1644.087 -1644.087 -1644.087] (1.000)
Step: 71649, Reward: [-771.288 -771.288 -771.288] [0.0000], Avg: [-1643.478 -1643.478 -1643.478] (1.000)
Step: 71699, Reward: [-948.325 -948.325 -948.325] [0.0000], Avg: [-1642.994 -1642.994 -1642.994] (1.000)
Step: 71749, Reward: [-1211.648 -1211.648 -1211.648] [0.0000], Avg: [-1642.693 -1642.693 -1642.693] (1.000)
Step: 71799, Reward: [-1885.17 -1885.17 -1885.17] [0.0000], Avg: [-1642.862 -1642.862 -1642.862] (1.000)
Step: 71849, Reward: [-2046.973 -2046.973 -2046.973] [0.0000], Avg: [-1643.143 -1643.143 -1643.143] (1.000)
Step: 71899, Reward: [-1357.35 -1357.35 -1357.35] [0.0000], Avg: [-1642.944 -1642.944 -1642.944] (1.000)
Step: 71949, Reward: [-1149.877 -1149.877 -1149.877] [0.0000], Avg: [-1642.602 -1642.602 -1642.602] (1.000)
Step: 71999, Reward: [-2085.104 -2085.104 -2085.104] [0.0000], Avg: [-1642.909 -1642.909 -1642.909] (1.000)
Step: 72049, Reward: [-1871.567 -1871.567 -1871.567] [0.0000], Avg: [-1643.068 -1643.068 -1643.068] (1.000)
Step: 72099, Reward: [-1684.574 -1684.574 -1684.574] [0.0000], Avg: [-1643.097 -1643.097 -1643.097] (1.000)
Step: 72149, Reward: [-1765.837 -1765.837 -1765.837] [0.0000], Avg: [-1643.182 -1643.182 -1643.182] (1.000)
Step: 72199, Reward: [-2136.188 -2136.188 -2136.188] [0.0000], Avg: [-1643.523 -1643.523 -1643.523] (1.000)
Step: 72249, Reward: [-1451.337 -1451.337 -1451.337] [0.0000], Avg: [-1643.39 -1643.39 -1643.39] (1.000)
Step: 72299, Reward: [-798.3 -798.3 -798.3] [0.0000], Avg: [-1642.806 -1642.806 -1642.806] (1.000)
Step: 72349, Reward: [-1711.722 -1711.722 -1711.722] [0.0000], Avg: [-1642.853 -1642.853 -1642.853] (1.000)
Step: 72399, Reward: [-1483.361 -1483.361 -1483.361] [0.0000], Avg: [-1642.743 -1642.743 -1642.743] (1.000)
Step: 72449, Reward: [-1615.326 -1615.326 -1615.326] [0.0000], Avg: [-1642.724 -1642.724 -1642.724] (1.000)
Step: 72499, Reward: [-741.262 -741.262 -741.262] [0.0000], Avg: [-1642.102 -1642.102 -1642.102] (1.000)
Step: 72549, Reward: [-1548.361 -1548.361 -1548.361] [0.0000], Avg: [-1642.038 -1642.038 -1642.038] (1.000)
Step: 72599, Reward: [-862.966 -862.966 -862.966] [0.0000], Avg: [-1641.501 -1641.501 -1641.501] (1.000)
Step: 72649, Reward: [-1188.164 -1188.164 -1188.164] [0.0000], Avg: [-1641.189 -1641.189 -1641.189] (1.000)
Step: 72699, Reward: [-1190.407 -1190.407 -1190.407] [0.0000], Avg: [-1640.879 -1640.879 -1640.879] (1.000)
Step: 72749, Reward: [-1786.503 -1786.503 -1786.503] [0.0000], Avg: [-1640.979 -1640.979 -1640.979] (1.000)
Step: 72799, Reward: [-1853.67 -1853.67 -1853.67] [0.0000], Avg: [-1641.125 -1641.125 -1641.125] (1.000)
Step: 72849, Reward: [-1689.884 -1689.884 -1689.884] [0.0000], Avg: [-1641.159 -1641.159 -1641.159] (1.000)
Step: 72899, Reward: [-1284.756 -1284.756 -1284.756] [0.0000], Avg: [-1640.914 -1640.914 -1640.914] (1.000)
Step: 72949, Reward: [-1200.401 -1200.401 -1200.401] [0.0000], Avg: [-1640.612 -1640.612 -1640.612] (1.000)
Step: 72999, Reward: [-1724.86 -1724.86 -1724.86] [0.0000], Avg: [-1640.67 -1640.67 -1640.67] (1.000)
Step: 73049, Reward: [-1614.151 -1614.151 -1614.151] [0.0000], Avg: [-1640.652 -1640.652 -1640.652] (1.000)
Step: 73099, Reward: [-1188.416 -1188.416 -1188.416] [0.0000], Avg: [-1640.343 -1640.343 -1640.343] (1.000)
Step: 73149, Reward: [-1201.466 -1201.466 -1201.466] [0.0000], Avg: [-1640.043 -1640.043 -1640.043] (1.000)
Step: 73199, Reward: [-1400.74 -1400.74 -1400.74] [0.0000], Avg: [-1639.879 -1639.879 -1639.879] (1.000)
Step: 73249, Reward: [-689.228 -689.228 -689.228] [0.0000], Avg: [-1639.23 -1639.23 -1639.23] (1.000)
Step: 73299, Reward: [-2112.306 -2112.306 -2112.306] [0.0000], Avg: [-1639.553 -1639.553 -1639.553] (1.000)
Step: 73349, Reward: [-1535.343 -1535.343 -1535.343] [0.0000], Avg: [-1639.482 -1639.482 -1639.482] (1.000)
Step: 73399, Reward: [-1797.888 -1797.888 -1797.888] [0.0000], Avg: [-1639.59 -1639.59 -1639.59] (1.000)
Step: 73449, Reward: [-878.802 -878.802 -878.802] [0.0000], Avg: [-1639.072 -1639.072 -1639.072] (1.000)
Step: 73499, Reward: [-1602.185 -1602.185 -1602.185] [0.0000], Avg: [-1639.047 -1639.047 -1639.047] (1.000)
Step: 73549, Reward: [-1839.263 -1839.263 -1839.263] [0.0000], Avg: [-1639.183 -1639.183 -1639.183] (1.000)
Step: 73599, Reward: [-2004.989 -2004.989 -2004.989] [0.0000], Avg: [-1639.432 -1639.432 -1639.432] (1.000)
Step: 73649, Reward: [-1793.759 -1793.759 -1793.759] [0.0000], Avg: [-1639.536 -1639.536 -1639.536] (1.000)
Step: 73699, Reward: [-1408.449 -1408.449 -1408.449] [0.0000], Avg: [-1639.38 -1639.38 -1639.38] (1.000)
Step: 73749, Reward: [-2105.72 -2105.72 -2105.72] [0.0000], Avg: [-1639.696 -1639.696 -1639.696] (1.000)
Step: 73799, Reward: [-1464.377 -1464.377 -1464.377] [0.0000], Avg: [-1639.577 -1639.577 -1639.577] (1.000)
Step: 73849, Reward: [-1630.871 -1630.871 -1630.871] [0.0000], Avg: [-1639.571 -1639.571 -1639.571] (1.000)
Step: 73899, Reward: [-2100.932 -2100.932 -2100.932] [0.0000], Avg: [-1639.883 -1639.883 -1639.883] (1.000)
Step: 73949, Reward: [-2071.182 -2071.182 -2071.182] [0.0000], Avg: [-1640.175 -1640.175 -1640.175] (1.000)
Step: 73999, Reward: [-1453.702 -1453.702 -1453.702] [0.0000], Avg: [-1640.049 -1640.049 -1640.049] (1.000)
Step: 74049, Reward: [-1746.679 -1746.679 -1746.679] [0.0000], Avg: [-1640.121 -1640.121 -1640.121] (1.000)
Step: 74099, Reward: [-1135.958 -1135.958 -1135.958] [0.0000], Avg: [-1639.781 -1639.781 -1639.781] (1.000)
Step: 74149, Reward: [-2007.146 -2007.146 -2007.146] [0.0000], Avg: [-1640.028 -1640.028 -1640.028] (1.000)
Step: 74199, Reward: [-1943.346 -1943.346 -1943.346] [0.0000], Avg: [-1640.233 -1640.233 -1640.233] (1.000)
Step: 74249, Reward: [-1450.648 -1450.648 -1450.648] [0.0000], Avg: [-1640.105 -1640.105 -1640.105] (1.000)
Step: 74299, Reward: [-1610.593 -1610.593 -1610.593] [0.0000], Avg: [-1640.085 -1640.085 -1640.085] (1.000)
Step: 74349, Reward: [-1950.476 -1950.476 -1950.476] [0.0000], Avg: [-1640.294 -1640.294 -1640.294] (1.000)
Step: 74399, Reward: [-1771.397 -1771.397 -1771.397] [0.0000], Avg: [-1640.382 -1640.382 -1640.382] (1.000)
Step: 74449, Reward: [-1737.9 -1737.9 -1737.9] [0.0000], Avg: [-1640.448 -1640.448 -1640.448] (1.000)
Step: 74499, Reward: [-1877.499 -1877.499 -1877.499] [0.0000], Avg: [-1640.607 -1640.607 -1640.607] (1.000)
Step: 74549, Reward: [-1575.071 -1575.071 -1575.071] [0.0000], Avg: [-1640.563 -1640.563 -1640.563] (1.000)
Step: 74599, Reward: [-1544.463 -1544.463 -1544.463] [0.0000], Avg: [-1640.498 -1640.498 -1640.498] (1.000)
Step: 74649, Reward: [-1757.098 -1757.098 -1757.098] [0.0000], Avg: [-1640.576 -1640.576 -1640.576] (1.000)
Step: 74699, Reward: [-2207.935 -2207.935 -2207.935] [0.0000], Avg: [-1640.956 -1640.956 -1640.956] (1.000)
Step: 74749, Reward: [-1988.828 -1988.828 -1988.828] [0.0000], Avg: [-1641.189 -1641.189 -1641.189] (1.000)
Step: 74799, Reward: [-1437.711 -1437.711 -1437.711] [0.0000], Avg: [-1641.053 -1641.053 -1641.053] (1.000)
Step: 74849, Reward: [-2051.513 -2051.513 -2051.513] [0.0000], Avg: [-1641.327 -1641.327 -1641.327] (1.000)
Step: 74899, Reward: [-1593.054 -1593.054 -1593.054] [0.0000], Avg: [-1641.295 -1641.295 -1641.295] (1.000)
Step: 74949, Reward: [-1166.594 -1166.594 -1166.594] [0.0000], Avg: [-1640.978 -1640.978 -1640.978] (1.000)
Step: 74999, Reward: [-1831.854 -1831.854 -1831.854] [0.0000], Avg: [-1641.105 -1641.105 -1641.105] (1.000)
Step: 75049, Reward: [-1169.511 -1169.511 -1169.511] [0.0000], Avg: [-1640.791 -1640.791 -1640.791] (1.000)
Step: 75099, Reward: [-1503.9 -1503.9 -1503.9] [0.0000], Avg: [-1640.7 -1640.7 -1640.7] (1.000)
Step: 75149, Reward: [-1439.971 -1439.971 -1439.971] [0.0000], Avg: [-1640.566 -1640.566 -1640.566] (1.000)
Step: 75199, Reward: [-1676.753 -1676.753 -1676.753] [0.0000], Avg: [-1640.591 -1640.591 -1640.591] (1.000)
Step: 75249, Reward: [-1637.166 -1637.166 -1637.166] [0.0000], Avg: [-1640.588 -1640.588 -1640.588] (1.000)
Step: 75299, Reward: [-2190.098 -2190.098 -2190.098] [0.0000], Avg: [-1640.953 -1640.953 -1640.953] (1.000)
Step: 75349, Reward: [-1262.956 -1262.956 -1262.956] [0.0000], Avg: [-1640.702 -1640.702 -1640.702] (1.000)
Step: 75399, Reward: [-1914.516 -1914.516 -1914.516] [0.0000], Avg: [-1640.884 -1640.884 -1640.884] (1.000)
Step: 75449, Reward: [-1704.271 -1704.271 -1704.271] [0.0000], Avg: [-1640.926 -1640.926 -1640.926] (1.000)
Step: 75499, Reward: [-1584.216 -1584.216 -1584.216] [0.0000], Avg: [-1640.888 -1640.888 -1640.888] (1.000)
Step: 75549, Reward: [-993.262 -993.262 -993.262] [0.0000], Avg: [-1640.46 -1640.46 -1640.46] (1.000)
Step: 75599, Reward: [-1084.939 -1084.939 -1084.939] [0.0000], Avg: [-1640.092 -1640.092 -1640.092] (1.000)
Step: 75649, Reward: [-1044.266 -1044.266 -1044.266] [0.0000], Avg: [-1639.698 -1639.698 -1639.698] (1.000)
Step: 75699, Reward: [-905.473 -905.473 -905.473] [0.0000], Avg: [-1639.214 -1639.214 -1639.214] (1.000)
Step: 75749, Reward: [-1385.725 -1385.725 -1385.725] [0.0000], Avg: [-1639.046 -1639.046 -1639.046] (1.000)
Step: 75799, Reward: [-1965.301 -1965.301 -1965.301] [0.0000], Avg: [-1639.261 -1639.261 -1639.261] (1.000)
Step: 75849, Reward: [-1416.581 -1416.581 -1416.581] [0.0000], Avg: [-1639.115 -1639.115 -1639.115] (1.000)
Step: 75899, Reward: [-1577.296 -1577.296 -1577.296] [0.0000], Avg: [-1639.074 -1639.074 -1639.074] (1.000)
Step: 75949, Reward: [-1775.933 -1775.933 -1775.933] [0.0000], Avg: [-1639.164 -1639.164 -1639.164] (1.000)
Step: 75999, Reward: [-1225.589 -1225.589 -1225.589] [0.0000], Avg: [-1638.892 -1638.892 -1638.892] (1.000)
Step: 76049, Reward: [-1347.751 -1347.751 -1347.751] [0.0000], Avg: [-1638.701 -1638.701 -1638.701] (1.000)
Step: 76099, Reward: [-2007.941 -2007.941 -2007.941] [0.0000], Avg: [-1638.943 -1638.943 -1638.943] (1.000)
Step: 76149, Reward: [-1240.12 -1240.12 -1240.12] [0.0000], Avg: [-1638.681 -1638.681 -1638.681] (1.000)
Step: 76199, Reward: [-1707.1 -1707.1 -1707.1] [0.0000], Avg: [-1638.726 -1638.726 -1638.726] (1.000)
Step: 76249, Reward: [-1409.949 -1409.949 -1409.949] [0.0000], Avg: [-1638.576 -1638.576 -1638.576] (1.000)
Step: 76299, Reward: [-1917.813 -1917.813 -1917.813] [0.0000], Avg: [-1638.759 -1638.759 -1638.759] (1.000)
Step: 76349, Reward: [-1726.361 -1726.361 -1726.361] [0.0000], Avg: [-1638.816 -1638.816 -1638.816] (1.000)
Step: 76399, Reward: [-1624.405 -1624.405 -1624.405] [0.0000], Avg: [-1638.807 -1638.807 -1638.807] (1.000)
Step: 76449, Reward: [-1700.158 -1700.158 -1700.158] [0.0000], Avg: [-1638.847 -1638.847 -1638.847] (1.000)
Step: 76499, Reward: [-1727.361 -1727.361 -1727.361] [0.0000], Avg: [-1638.905 -1638.905 -1638.905] (1.000)
Step: 76549, Reward: [-1999.752 -1999.752 -1999.752] [0.0000], Avg: [-1639.141 -1639.141 -1639.141] (1.000)
Step: 76599, Reward: [-1363.843 -1363.843 -1363.843] [0.0000], Avg: [-1638.961 -1638.961 -1638.961] (1.000)
Step: 76649, Reward: [-1363.582 -1363.582 -1363.582] [0.0000], Avg: [-1638.781 -1638.781 -1638.781] (1.000)
Step: 76699, Reward: [-1826.841 -1826.841 -1826.841] [0.0000], Avg: [-1638.904 -1638.904 -1638.904] (1.000)
Step: 76749, Reward: [-1168.114 -1168.114 -1168.114] [0.0000], Avg: [-1638.597 -1638.597 -1638.597] (1.000)
Step: 76799, Reward: [-1298.851 -1298.851 -1298.851] [0.0000], Avg: [-1638.376 -1638.376 -1638.376] (1.000)
Step: 76849, Reward: [-1543.362 -1543.362 -1543.362] [0.0000], Avg: [-1638.314 -1638.314 -1638.314] (1.000)
Step: 76899, Reward: [-1709.812 -1709.812 -1709.812] [0.0000], Avg: [-1638.361 -1638.361 -1638.361] (1.000)
Step: 76949, Reward: [-1147.022 -1147.022 -1147.022] [0.0000], Avg: [-1638.041 -1638.041 -1638.041] (1.000)
Step: 76999, Reward: [-1796.926 -1796.926 -1796.926] [0.0000], Avg: [-1638.145 -1638.145 -1638.145] (1.000)
Step: 77049, Reward: [-1545.5 -1545.5 -1545.5] [0.0000], Avg: [-1638.085 -1638.085 -1638.085] (1.000)
Step: 77099, Reward: [-1753.273 -1753.273 -1753.273] [0.0000], Avg: [-1638.159 -1638.159 -1638.159] (1.000)
Step: 77149, Reward: [-1226.72 -1226.72 -1226.72] [0.0000], Avg: [-1637.893 -1637.893 -1637.893] (1.000)
Step: 77199, Reward: [-1539.107 -1539.107 -1539.107] [0.0000], Avg: [-1637.829 -1637.829 -1637.829] (1.000)
Step: 77249, Reward: [-2089.427 -2089.427 -2089.427] [0.0000], Avg: [-1638.121 -1638.121 -1638.121] (1.000)
Step: 77299, Reward: [-1815.868 -1815.868 -1815.868] [0.0000], Avg: [-1638.236 -1638.236 -1638.236] (1.000)
Step: 77349, Reward: [-1533.987 -1533.987 -1533.987] [0.0000], Avg: [-1638.168 -1638.168 -1638.168] (1.000)
Step: 77399, Reward: [-1703.246 -1703.246 -1703.246] [0.0000], Avg: [-1638.211 -1638.211 -1638.211] (1.000)
Step: 77449, Reward: [-1376.483 -1376.483 -1376.483] [0.0000], Avg: [-1638.042 -1638.042 -1638.042] (1.000)
Step: 77499, Reward: [-2081.814 -2081.814 -2081.814] [0.0000], Avg: [-1638.328 -1638.328 -1638.328] (1.000)
Step: 77549, Reward: [-1240.4 -1240.4 -1240.4] [0.0000], Avg: [-1638.071 -1638.071 -1638.071] (1.000)
Step: 77599, Reward: [-1545.151 -1545.151 -1545.151] [0.0000], Avg: [-1638.011 -1638.011 -1638.011] (1.000)
Step: 77649, Reward: [-1158.076 -1158.076 -1158.076] [0.0000], Avg: [-1637.702 -1637.702 -1637.702] (1.000)
Step: 77699, Reward: [-1916.361 -1916.361 -1916.361] [0.0000], Avg: [-1637.882 -1637.882 -1637.882] (1.000)
Step: 77749, Reward: [-1934.37 -1934.37 -1934.37] [0.0000], Avg: [-1638.072 -1638.072 -1638.072] (1.000)
Step: 77799, Reward: [-1583.655 -1583.655 -1583.655] [0.0000], Avg: [-1638.037 -1638.037 -1638.037] (1.000)
Step: 77849, Reward: [-1700.834 -1700.834 -1700.834] [0.0000], Avg: [-1638.078 -1638.078 -1638.078] (1.000)
Step: 77899, Reward: [-1736.324 -1736.324 -1736.324] [0.0000], Avg: [-1638.141 -1638.141 -1638.141] (1.000)
Step: 77949, Reward: [-1707.089 -1707.089 -1707.089] [0.0000], Avg: [-1638.185 -1638.185 -1638.185] (1.000)
Step: 77999, Reward: [-1508.916 -1508.916 -1508.916] [0.0000], Avg: [-1638.102 -1638.102 -1638.102] (1.000)
Step: 78049, Reward: [-2025.054 -2025.054 -2025.054] [0.0000], Avg: [-1638.35 -1638.35 -1638.35] (1.000)
Step: 78099, Reward: [-1243.05 -1243.05 -1243.05] [0.0000], Avg: [-1638.097 -1638.097 -1638.097] (1.000)
Step: 78149, Reward: [-1878.05 -1878.05 -1878.05] [0.0000], Avg: [-1638.251 -1638.251 -1638.251] (1.000)
Step: 78199, Reward: [-898.945 -898.945 -898.945] [0.0000], Avg: [-1637.778 -1637.778 -1637.778] (1.000)
Step: 78249, Reward: [-873.211 -873.211 -873.211] [0.0000], Avg: [-1637.289 -1637.289 -1637.289] (1.000)
Step: 78299, Reward: [-1371.138 -1371.138 -1371.138] [0.0000], Avg: [-1637.119 -1637.119 -1637.119] (1.000)
Step: 78349, Reward: [-1427.498 -1427.498 -1427.498] [0.0000], Avg: [-1636.986 -1636.986 -1636.986] (1.000)
Step: 78399, Reward: [-1161.748 -1161.748 -1161.748] [0.0000], Avg: [-1636.682 -1636.682 -1636.682] (1.000)
Step: 78449, Reward: [-1787.066 -1787.066 -1787.066] [0.0000], Avg: [-1636.778 -1636.778 -1636.778] (1.000)
Step: 78499, Reward: [-1946.312 -1946.312 -1946.312] [0.0000], Avg: [-1636.975 -1636.975 -1636.975] (1.000)
Step: 78549, Reward: [-1352.035 -1352.035 -1352.035] [0.0000], Avg: [-1636.794 -1636.794 -1636.794] (1.000)
Step: 78599, Reward: [-1708.944 -1708.944 -1708.944] [0.0000], Avg: [-1636.84 -1636.84 -1636.84] (1.000)
Step: 78649, Reward: [-1959.892 -1959.892 -1959.892] [0.0000], Avg: [-1637.045 -1637.045 -1637.045] (1.000)
Step: 78699, Reward: [-2371.73 -2371.73 -2371.73] [0.0000], Avg: [-1637.512 -1637.512 -1637.512] (1.000)
Step: 78749, Reward: [-790.559 -790.559 -790.559] [0.0000], Avg: [-1636.974 -1636.974 -1636.974] (1.000)
Step: 78799, Reward: [-1712.436 -1712.436 -1712.436] [0.0000], Avg: [-1637.022 -1637.022 -1637.022] (1.000)
Step: 78849, Reward: [-1366.392 -1366.392 -1366.392] [0.0000], Avg: [-1636.851 -1636.851 -1636.851] (1.000)
Step: 78899, Reward: [-947.386 -947.386 -947.386] [0.0000], Avg: [-1636.414 -1636.414 -1636.414] (1.000)
Step: 78949, Reward: [-1667.048 -1667.048 -1667.048] [0.0000], Avg: [-1636.433 -1636.433 -1636.433] (1.000)
Step: 78999, Reward: [-1642.177 -1642.177 -1642.177] [0.0000], Avg: [-1636.437 -1636.437 -1636.437] (1.000)
Step: 79049, Reward: [-2079.172 -2079.172 -2079.172] [0.0000], Avg: [-1636.717 -1636.717 -1636.717] (1.000)
Step: 79099, Reward: [-1736.608 -1736.608 -1736.608] [0.0000], Avg: [-1636.78 -1636.78 -1636.78] (1.000)
Step: 79149, Reward: [-839.383 -839.383 -839.383] [0.0000], Avg: [-1636.276 -1636.276 -1636.276] (1.000)
Step: 79199, Reward: [-2113.478 -2113.478 -2113.478] [0.0000], Avg: [-1636.577 -1636.577 -1636.577] (1.000)
Step: 79249, Reward: [-912.886 -912.886 -912.886] [0.0000], Avg: [-1636.121 -1636.121 -1636.121] (1.000)
Step: 79299, Reward: [-1667.457 -1667.457 -1667.457] [0.0000], Avg: [-1636.141 -1636.141 -1636.141] (1.000)
Step: 79349, Reward: [-1850.416 -1850.416 -1850.416] [0.0000], Avg: [-1636.276 -1636.276 -1636.276] (1.000)
Step: 79399, Reward: [-1493.671 -1493.671 -1493.671] [0.0000], Avg: [-1636.186 -1636.186 -1636.186] (1.000)
Step: 79449, Reward: [-2117.687 -2117.687 -2117.687] [0.0000], Avg: [-1636.489 -1636.489 -1636.489] (1.000)
Step: 79499, Reward: [-1887.051 -1887.051 -1887.051] [0.0000], Avg: [-1636.646 -1636.646 -1636.646] (1.000)
Step: 79549, Reward: [-1075.488 -1075.488 -1075.488] [0.0000], Avg: [-1636.294 -1636.294 -1636.294] (1.000)
Step: 79599, Reward: [-991.915 -991.915 -991.915] [0.0000], Avg: [-1635.889 -1635.889 -1635.889] (1.000)
Step: 79649, Reward: [-1727.491 -1727.491 -1727.491] [0.0000], Avg: [-1635.946 -1635.946 -1635.946] (1.000)
Step: 79699, Reward: [-1612.257 -1612.257 -1612.257] [0.0000], Avg: [-1635.932 -1635.932 -1635.932] (1.000)
Step: 79749, Reward: [-1663.354 -1663.354 -1663.354] [0.0000], Avg: [-1635.949 -1635.949 -1635.949] (1.000)
Step: 79799, Reward: [-1657.934 -1657.934 -1657.934] [0.0000], Avg: [-1635.963 -1635.963 -1635.963] (1.000)
Step: 79849, Reward: [-2113.316 -2113.316 -2113.316] [0.0000], Avg: [-1636.261 -1636.261 -1636.261] (1.000)
Step: 79899, Reward: [-1898.142 -1898.142 -1898.142] [0.0000], Avg: [-1636.425 -1636.425 -1636.425] (1.000)
Step: 79949, Reward: [-926.437 -926.437 -926.437] [0.0000], Avg: [-1635.981 -1635.981 -1635.981] (1.000)
Step: 79999, Reward: [-1226.685 -1226.685 -1226.685] [0.0000], Avg: [-1635.726 -1635.726 -1635.726] (1.000)
Step: 80049, Reward: [-822.858 -822.858 -822.858] [0.0000], Avg: [-1635.218 -1635.218 -1635.218] (1.000)
Step: 80099, Reward: [-881.535 -881.535 -881.535] [0.0000], Avg: [-1634.747 -1634.747 -1634.747] (1.000)
Step: 80149, Reward: [-1706.763 -1706.763 -1706.763] [0.0000], Avg: [-1634.792 -1634.792 -1634.792] (1.000)
Step: 80199, Reward: [-1613.559 -1613.559 -1613.559] [0.0000], Avg: [-1634.779 -1634.779 -1634.779] (1.000)
Step: 80249, Reward: [-1860.217 -1860.217 -1860.217] [0.0000], Avg: [-1634.92 -1634.92 -1634.92] (1.000)
Step: 80299, Reward: [-836.069 -836.069 -836.069] [0.0000], Avg: [-1634.422 -1634.422 -1634.422] (1.000)
Step: 80349, Reward: [-1934.003 -1934.003 -1934.003] [0.0000], Avg: [-1634.609 -1634.609 -1634.609] (1.000)
Step: 80399, Reward: [-1983.502 -1983.502 -1983.502] [0.0000], Avg: [-1634.825 -1634.825 -1634.825] (1.000)
Step: 80449, Reward: [-1579.139 -1579.139 -1579.139] [0.0000], Avg: [-1634.791 -1634.791 -1634.791] (1.000)
Step: 80499, Reward: [-1251.225 -1251.225 -1251.225] [0.0000], Avg: [-1634.553 -1634.553 -1634.553] (1.000)
Step: 80549, Reward: [-911.576 -911.576 -911.576] [0.0000], Avg: [-1634.104 -1634.104 -1634.104] (1.000)
Step: 80599, Reward: [-2011.763 -2011.763 -2011.763] [0.0000], Avg: [-1634.338 -1634.338 -1634.338] (1.000)
Step: 80649, Reward: [-1811.946 -1811.946 -1811.946] [0.0000], Avg: [-1634.448 -1634.448 -1634.448] (1.000)
Step: 80699, Reward: [-1048.426 -1048.426 -1048.426] [0.0000], Avg: [-1634.085 -1634.085 -1634.085] (1.000)
Step: 80749, Reward: [-1818.097 -1818.097 -1818.097] [0.0000], Avg: [-1634.199 -1634.199 -1634.199] (1.000)
Step: 80799, Reward: [-1432.824 -1432.824 -1432.824] [0.0000], Avg: [-1634.074 -1634.074 -1634.074] (1.000)
Step: 80849, Reward: [-2211.825 -2211.825 -2211.825] [0.0000], Avg: [-1634.432 -1634.432 -1634.432] (1.000)
Step: 80899, Reward: [-1536.527 -1536.527 -1536.527] [0.0000], Avg: [-1634.371 -1634.371 -1634.371] (1.000)
Step: 80949, Reward: [-1998.881 -1998.881 -1998.881] [0.0000], Avg: [-1634.596 -1634.596 -1634.596] (1.000)
Step: 80999, Reward: [-1048.467 -1048.467 -1048.467] [0.0000], Avg: [-1634.235 -1634.235 -1634.235] (1.000)
Step: 81049, Reward: [-2089.692 -2089.692 -2089.692] [0.0000], Avg: [-1634.516 -1634.516 -1634.516] (1.000)
Step: 81099, Reward: [-2108.134 -2108.134 -2108.134] [0.0000], Avg: [-1634.808 -1634.808 -1634.808] (1.000)
Step: 81149, Reward: [-1443.792 -1443.792 -1443.792] [0.0000], Avg: [-1634.69 -1634.69 -1634.69] (1.000)
Step: 81199, Reward: [-1552.974 -1552.974 -1552.974] [0.0000], Avg: [-1634.64 -1634.64 -1634.64] (1.000)
Step: 81249, Reward: [-1472.536 -1472.536 -1472.536] [0.0000], Avg: [-1634.54 -1634.54 -1634.54] (1.000)
Step: 81299, Reward: [-1034.629 -1034.629 -1034.629] [0.0000], Avg: [-1634.171 -1634.171 -1634.171] (1.000)
Step: 81349, Reward: [-1647.223 -1647.223 -1647.223] [0.0000], Avg: [-1634.179 -1634.179 -1634.179] (1.000)
Step: 81399, Reward: [-2008.125 -2008.125 -2008.125] [0.0000], Avg: [-1634.409 -1634.409 -1634.409] (1.000)
Step: 81449, Reward: [-892.027 -892.027 -892.027] [0.0000], Avg: [-1633.953 -1633.953 -1633.953] (1.000)
Step: 81499, Reward: [-1784.852 -1784.852 -1784.852] [0.0000], Avg: [-1634.045 -1634.045 -1634.045] (1.000)
Step: 81549, Reward: [-1870.246 -1870.246 -1870.246] [0.0000], Avg: [-1634.19 -1634.19 -1634.19] (1.000)
Step: 81599, Reward: [-1922.839 -1922.839 -1922.839] [0.0000], Avg: [-1634.367 -1634.367 -1634.367] (1.000)
Step: 81649, Reward: [-831.514 -831.514 -831.514] [0.0000], Avg: [-1633.875 -1633.875 -1633.875] (1.000)
Step: 81699, Reward: [-1341.383 -1341.383 -1341.383] [0.0000], Avg: [-1633.696 -1633.696 -1633.696] (1.000)
Step: 81749, Reward: [-2010.914 -2010.914 -2010.914] [0.0000], Avg: [-1633.927 -1633.927 -1633.927] (1.000)
Step: 81799, Reward: [-2117.821 -2117.821 -2117.821] [0.0000], Avg: [-1634.223 -1634.223 -1634.223] (1.000)
Step: 81849, Reward: [-1980.979 -1980.979 -1980.979] [0.0000], Avg: [-1634.435 -1634.435 -1634.435] (1.000)
Step: 81899, Reward: [-1761.8 -1761.8 -1761.8] [0.0000], Avg: [-1634.513 -1634.513 -1634.513] (1.000)
Step: 81949, Reward: [-2203.943 -2203.943 -2203.943] [0.0000], Avg: [-1634.86 -1634.86 -1634.86] (1.000)
Step: 81999, Reward: [-1706.393 -1706.393 -1706.393] [0.0000], Avg: [-1634.904 -1634.904 -1634.904] (1.000)
Step: 82049, Reward: [-1609.233 -1609.233 -1609.233] [0.0000], Avg: [-1634.888 -1634.888 -1634.888] (1.000)
Step: 82099, Reward: [-1047.452 -1047.452 -1047.452] [0.0000], Avg: [-1634.53 -1634.53 -1634.53] (1.000)
Step: 82149, Reward: [-561.028 -561.028 -561.028] [0.0000], Avg: [-1633.877 -1633.877 -1633.877] (1.000)
Step: 82199, Reward: [-2124.915 -2124.915 -2124.915] [0.0000], Avg: [-1634.176 -1634.176 -1634.176] (1.000)
Step: 82249, Reward: [-2189.5 -2189.5 -2189.5] [0.0000], Avg: [-1634.513 -1634.513 -1634.513] (1.000)
Step: 82299, Reward: [-1772.004 -1772.004 -1772.004] [0.0000], Avg: [-1634.597 -1634.597 -1634.597] (1.000)
Step: 82349, Reward: [-1679.423 -1679.423 -1679.423] [0.0000], Avg: [-1634.624 -1634.624 -1634.624] (1.000)
Step: 82399, Reward: [-2245.61 -2245.61 -2245.61] [0.0000], Avg: [-1634.995 -1634.995 -1634.995] (1.000)
Step: 82449, Reward: [-1446.512 -1446.512 -1446.512] [0.0000], Avg: [-1634.88 -1634.88 -1634.88] (1.000)
Step: 82499, Reward: [-1353.027 -1353.027 -1353.027] [0.0000], Avg: [-1634.709 -1634.709 -1634.709] (1.000)
Step: 82549, Reward: [-2157.556 -2157.556 -2157.556] [0.0000], Avg: [-1635.026 -1635.026 -1635.026] (1.000)
Step: 82599, Reward: [-1824.187 -1824.187 -1824.187] [0.0000], Avg: [-1635.141 -1635.141 -1635.141] (1.000)
Step: 82649, Reward: [-1688.009 -1688.009 -1688.009] [0.0000], Avg: [-1635.173 -1635.173 -1635.173] (1.000)
Step: 82699, Reward: [-2163.559 -2163.559 -2163.559] [0.0000], Avg: [-1635.492 -1635.492 -1635.492] (1.000)
Step: 82749, Reward: [-2154.467 -2154.467 -2154.467] [0.0000], Avg: [-1635.806 -1635.806 -1635.806] (1.000)
Step: 82799, Reward: [-1707.607 -1707.607 -1707.607] [0.0000], Avg: [-1635.849 -1635.849 -1635.849] (1.000)
Step: 82849, Reward: [-1556.973 -1556.973 -1556.973] [0.0000], Avg: [-1635.801 -1635.801 -1635.801] (1.000)
Step: 82899, Reward: [-1679.223 -1679.223 -1679.223] [0.0000], Avg: [-1635.828 -1635.828 -1635.828] (1.000)
Step: 82949, Reward: [-1121.944 -1121.944 -1121.944] [0.0000], Avg: [-1635.518 -1635.518 -1635.518] (1.000)
Step: 82999, Reward: [-1787.398 -1787.398 -1787.398] [0.0000], Avg: [-1635.609 -1635.609 -1635.609] (1.000)
Step: 83049, Reward: [-1933.167 -1933.167 -1933.167] [0.0000], Avg: [-1635.788 -1635.788 -1635.788] (1.000)
Step: 83099, Reward: [-1484.635 -1484.635 -1484.635] [0.0000], Avg: [-1635.698 -1635.698 -1635.698] (1.000)
Step: 83149, Reward: [-1725.25 -1725.25 -1725.25] [0.0000], Avg: [-1635.751 -1635.751 -1635.751] (1.000)
Step: 83199, Reward: [-1715.961 -1715.961 -1715.961] [0.0000], Avg: [-1635.8 -1635.8 -1635.8] (1.000)
Step: 83249, Reward: [-2245.797 -2245.797 -2245.797] [0.0000], Avg: [-1636.166 -1636.166 -1636.166] (1.000)
Step: 83299, Reward: [-1411.222 -1411.222 -1411.222] [0.0000], Avg: [-1636.031 -1636.031 -1636.031] (1.000)
Step: 83349, Reward: [-1640.222 -1640.222 -1640.222] [0.0000], Avg: [-1636.033 -1636.033 -1636.033] (1.000)
Step: 83399, Reward: [-1912.191 -1912.191 -1912.191] [0.0000], Avg: [-1636.199 -1636.199 -1636.199] (1.000)
Step: 83449, Reward: [-2049.3 -2049.3 -2049.3] [0.0000], Avg: [-1636.447 -1636.447 -1636.447] (1.000)
Step: 83499, Reward: [-1806.397 -1806.397 -1806.397] [0.0000], Avg: [-1636.548 -1636.548 -1636.548] (1.000)
Step: 83549, Reward: [-1971.298 -1971.298 -1971.298] [0.0000], Avg: [-1636.749 -1636.749 -1636.749] (1.000)
Step: 83599, Reward: [-1129.119 -1129.119 -1129.119] [0.0000], Avg: [-1636.445 -1636.445 -1636.445] (1.000)
Step: 83649, Reward: [-1542.669 -1542.669 -1542.669] [0.0000], Avg: [-1636.389 -1636.389 -1636.389] (1.000)
Step: 83699, Reward: [-1506.91 -1506.91 -1506.91] [0.0000], Avg: [-1636.312 -1636.312 -1636.312] (1.000)
Step: 83749, Reward: [-1790.495 -1790.495 -1790.495] [0.0000], Avg: [-1636.404 -1636.404 -1636.404] (1.000)
Step: 83799, Reward: [-1817.554 -1817.554 -1817.554] [0.0000], Avg: [-1636.512 -1636.512 -1636.512] (1.000)
Step: 83849, Reward: [-1765.041 -1765.041 -1765.041] [0.0000], Avg: [-1636.588 -1636.588 -1636.588] (1.000)
Step: 83899, Reward: [-1750.769 -1750.769 -1750.769] [0.0000], Avg: [-1636.656 -1636.656 -1636.656] (1.000)
Step: 83949, Reward: [-2066.017 -2066.017 -2066.017] [0.0000], Avg: [-1636.912 -1636.912 -1636.912] (1.000)
Step: 83999, Reward: [-1814.424 -1814.424 -1814.424] [0.0000], Avg: [-1637.018 -1637.018 -1637.018] (1.000)
Step: 84049, Reward: [-1388.436 -1388.436 -1388.436] [0.0000], Avg: [-1636.87 -1636.87 -1636.87] (1.000)
Step: 84099, Reward: [-1560.709 -1560.709 -1560.709] [0.0000], Avg: [-1636.825 -1636.825 -1636.825] (1.000)
Step: 84149, Reward: [-1623.927 -1623.927 -1623.927] [0.0000], Avg: [-1636.817 -1636.817 -1636.817] (1.000)
Step: 84199, Reward: [-1070.461 -1070.461 -1070.461] [0.0000], Avg: [-1636.481 -1636.481 -1636.481] (1.000)
Step: 84249, Reward: [-2056.805 -2056.805 -2056.805] [0.0000], Avg: [-1636.73 -1636.73 -1636.73] (1.000)
Step: 84299, Reward: [-1697.246 -1697.246 -1697.246] [0.0000], Avg: [-1636.766 -1636.766 -1636.766] (1.000)
Step: 84349, Reward: [-1433.667 -1433.667 -1433.667] [0.0000], Avg: [-1636.646 -1636.646 -1636.646] (1.000)
Step: 84399, Reward: [-2253.6 -2253.6 -2253.6] [0.0000], Avg: [-1637.011 -1637.011 -1637.011] (1.000)
Step: 84449, Reward: [-1426.454 -1426.454 -1426.454] [0.0000], Avg: [-1636.886 -1636.886 -1636.886] (1.000)
Step: 84499, Reward: [-1353.955 -1353.955 -1353.955] [0.0000], Avg: [-1636.719 -1636.719 -1636.719] (1.000)
Step: 84549, Reward: [-1656.416 -1656.416 -1656.416] [0.0000], Avg: [-1636.731 -1636.731 -1636.731] (1.000)
Step: 84599, Reward: [-1908.204 -1908.204 -1908.204] [0.0000], Avg: [-1636.891 -1636.891 -1636.891] (1.000)
Step: 84649, Reward: [-1844.088 -1844.088 -1844.088] [0.0000], Avg: [-1637.014 -1637.014 -1637.014] (1.000)
Step: 84699, Reward: [-1802.502 -1802.502 -1802.502] [0.0000], Avg: [-1637.111 -1637.111 -1637.111] (1.000)
Step: 84749, Reward: [-1541.749 -1541.749 -1541.749] [0.0000], Avg: [-1637.055 -1637.055 -1637.055] (1.000)
Step: 84799, Reward: [-1893.145 -1893.145 -1893.145] [0.0000], Avg: [-1637.206 -1637.206 -1637.206] (1.000)
Step: 84849, Reward: [-2006.167 -2006.167 -2006.167] [0.0000], Avg: [-1637.423 -1637.423 -1637.423] (1.000)
Step: 84899, Reward: [-1631.319 -1631.319 -1631.319] [0.0000], Avg: [-1637.42 -1637.42 -1637.42] (1.000)
Step: 84949, Reward: [-2033.313 -2033.313 -2033.313] [0.0000], Avg: [-1637.653 -1637.653 -1637.653] (1.000)
Step: 84999, Reward: [-1874.948 -1874.948 -1874.948] [0.0000], Avg: [-1637.792 -1637.792 -1637.792] (1.000)
Step: 85049, Reward: [-1956.936 -1956.936 -1956.936] [0.0000], Avg: [-1637.98 -1637.98 -1637.98] (1.000)
Step: 85099, Reward: [-1905.411 -1905.411 -1905.411] [0.0000], Avg: [-1638.137 -1638.137 -1638.137] (1.000)
Step: 85149, Reward: [-2284.202 -2284.202 -2284.202] [0.0000], Avg: [-1638.517 -1638.517 -1638.517] (1.000)
Step: 85199, Reward: [-1828.496 -1828.496 -1828.496] [0.0000], Avg: [-1638.628 -1638.628 -1638.628] (1.000)
Step: 85249, Reward: [-2058.193 -2058.193 -2058.193] [0.0000], Avg: [-1638.874 -1638.874 -1638.874] (1.000)
Step: 85299, Reward: [-2133.993 -2133.993 -2133.993] [0.0000], Avg: [-1639.164 -1639.164 -1639.164] (1.000)
Step: 85349, Reward: [-1635.475 -1635.475 -1635.475] [0.0000], Avg: [-1639.162 -1639.162 -1639.162] (1.000)
Step: 85399, Reward: [-1955.212 -1955.212 -1955.212] [0.0000], Avg: [-1639.347 -1639.347 -1639.347] (1.000)
Step: 85449, Reward: [-1844.907 -1844.907 -1844.907] [0.0000], Avg: [-1639.467 -1639.467 -1639.467] (1.000)
Step: 85499, Reward: [-1610.164 -1610.164 -1610.164] [0.0000], Avg: [-1639.45 -1639.45 -1639.45] (1.000)
Step: 85549, Reward: [-1827.569 -1827.569 -1827.569] [0.0000], Avg: [-1639.56 -1639.56 -1639.56] (1.000)
Step: 85599, Reward: [-1843.334 -1843.334 -1843.334] [0.0000], Avg: [-1639.679 -1639.679 -1639.679] (1.000)
Step: 85649, Reward: [-1942.319 -1942.319 -1942.319] [0.0000], Avg: [-1639.856 -1639.856 -1639.856] (1.000)
Step: 85699, Reward: [-1298.281 -1298.281 -1298.281] [0.0000], Avg: [-1639.657 -1639.657 -1639.657] (1.000)
Step: 85749, Reward: [-2003.017 -2003.017 -2003.017] [0.0000], Avg: [-1639.869 -1639.869 -1639.869] (1.000)
Step: 85799, Reward: [-1838.502 -1838.502 -1838.502] [0.0000], Avg: [-1639.984 -1639.984 -1639.984] (1.000)
Step: 85849, Reward: [-1455.201 -1455.201 -1455.201] [0.0000], Avg: [-1639.877 -1639.877 -1639.877] (1.000)
Step: 85899, Reward: [-1842.49 -1842.49 -1842.49] [0.0000], Avg: [-1639.995 -1639.995 -1639.995] (1.000)
Step: 85949, Reward: [-1901.308 -1901.308 -1901.308] [0.0000], Avg: [-1640.147 -1640.147 -1640.147] (1.000)
Step: 85999, Reward: [-1658.874 -1658.874 -1658.874] [0.0000], Avg: [-1640.158 -1640.158 -1640.158] (1.000)
Step: 86049, Reward: [-1771.374 -1771.374 -1771.374] [0.0000], Avg: [-1640.234 -1640.234 -1640.234] (1.000)
Step: 86099, Reward: [-1995.919 -1995.919 -1995.919] [0.0000], Avg: [-1640.44 -1640.44 -1640.44] (1.000)
Step: 86149, Reward: [-1462.932 -1462.932 -1462.932] [0.0000], Avg: [-1640.337 -1640.337 -1640.337] (1.000)
Step: 86199, Reward: [-2020.272 -2020.272 -2020.272] [0.0000], Avg: [-1640.558 -1640.558 -1640.558] (1.000)
Step: 86249, Reward: [-1870.885 -1870.885 -1870.885] [0.0000], Avg: [-1640.691 -1640.691 -1640.691] (1.000)
Step: 86299, Reward: [-1519.361 -1519.361 -1519.361] [0.0000], Avg: [-1640.621 -1640.621 -1640.621] (1.000)
Step: 86349, Reward: [-2070.397 -2070.397 -2070.397] [0.0000], Avg: [-1640.87 -1640.87 -1640.87] (1.000)
Step: 86399, Reward: [-2138.636 -2138.636 -2138.636] [0.0000], Avg: [-1641.158 -1641.158 -1641.158] (1.000)
Step: 86449, Reward: [-1978.825 -1978.825 -1978.825] [0.0000], Avg: [-1641.353 -1641.353 -1641.353] (1.000)
Step: 86499, Reward: [-1670.281 -1670.281 -1670.281] [0.0000], Avg: [-1641.37 -1641.37 -1641.37] (1.000)
Step: 86549, Reward: [-1515.625 -1515.625 -1515.625] [0.0000], Avg: [-1641.297 -1641.297 -1641.297] (1.000)
Step: 86599, Reward: [-1572.814 -1572.814 -1572.814] [0.0000], Avg: [-1641.258 -1641.258 -1641.258] (1.000)
Step: 86649, Reward: [-1474.836 -1474.836 -1474.836] [0.0000], Avg: [-1641.162 -1641.162 -1641.162] (1.000)
Step: 86699, Reward: [-1781.103 -1781.103 -1781.103] [0.0000], Avg: [-1641.242 -1641.242 -1641.242] (1.000)
Step: 86749, Reward: [-2069.03 -2069.03 -2069.03] [0.0000], Avg: [-1641.489 -1641.489 -1641.489] (1.000)
Step: 86799, Reward: [-1421.159 -1421.159 -1421.159] [0.0000], Avg: [-1641.362 -1641.362 -1641.362] (1.000)
Step: 86849, Reward: [-2308.585 -2308.585 -2308.585] [0.0000], Avg: [-1641.746 -1641.746 -1641.746] (1.000)
Step: 86899, Reward: [-2207.46 -2207.46 -2207.46] [0.0000], Avg: [-1642.072 -1642.072 -1642.072] (1.000)
Step: 86949, Reward: [-1782.554 -1782.554 -1782.554] [0.0000], Avg: [-1642.152 -1642.152 -1642.152] (1.000)
Step: 86999, Reward: [-1761.597 -1761.597 -1761.597] [0.0000], Avg: [-1642.221 -1642.221 -1642.221] (1.000)
Step: 87049, Reward: [-1984.996 -1984.996 -1984.996] [0.0000], Avg: [-1642.418 -1642.418 -1642.418] (1.000)
Step: 87099, Reward: [-1349.463 -1349.463 -1349.463] [0.0000], Avg: [-1642.25 -1642.25 -1642.25] (1.000)
Step: 87149, Reward: [-1975.511 -1975.511 -1975.511] [0.0000], Avg: [-1642.441 -1642.441 -1642.441] (1.000)
Step: 87199, Reward: [-2016.548 -2016.548 -2016.548] [0.0000], Avg: [-1642.655 -1642.655 -1642.655] (1.000)
Step: 87249, Reward: [-1540.744 -1540.744 -1540.744] [0.0000], Avg: [-1642.597 -1642.597 -1642.597] (1.000)
Step: 87299, Reward: [-1865.774 -1865.774 -1865.774] [0.0000], Avg: [-1642.725 -1642.725 -1642.725] (1.000)
Step: 87349, Reward: [-2116.038 -2116.038 -2116.038] [0.0000], Avg: [-1642.996 -1642.996 -1642.996] (1.000)
Step: 87399, Reward: [-2250.176 -2250.176 -2250.176] [0.0000], Avg: [-1643.343 -1643.343 -1643.343] (1.000)
Step: 87449, Reward: [-1467.567 -1467.567 -1467.567] [0.0000], Avg: [-1643.243 -1643.243 -1643.243] (1.000)
Step: 87499, Reward: [-1880.32 -1880.32 -1880.32] [0.0000], Avg: [-1643.378 -1643.378 -1643.378] (1.000)
Step: 87549, Reward: [-1507.106 -1507.106 -1507.106] [0.0000], Avg: [-1643.3 -1643.3 -1643.3] (1.000)
Step: 87599, Reward: [-2330.009 -2330.009 -2330.009] [0.0000], Avg: [-1643.692 -1643.692 -1643.692] (1.000)
Step: 87649, Reward: [-1216.047 -1216.047 -1216.047] [0.0000], Avg: [-1643.448 -1643.448 -1643.448] (1.000)
Step: 87699, Reward: [-1487.525 -1487.525 -1487.525] [0.0000], Avg: [-1643.359 -1643.359 -1643.359] (1.000)
Step: 87749, Reward: [-1466.782 -1466.782 -1466.782] [0.0000], Avg: [-1643.259 -1643.259 -1643.259] (1.000)
Step: 87799, Reward: [-1449.335 -1449.335 -1449.335] [0.0000], Avg: [-1643.148 -1643.148 -1643.148] (1.000)
Step: 87849, Reward: [-1597.562 -1597.562 -1597.562] [0.0000], Avg: [-1643.122 -1643.122 -1643.122] (1.000)
Step: 87899, Reward: [-1310.171 -1310.171 -1310.171] [0.0000], Avg: [-1642.933 -1642.933 -1642.933] (1.000)
Step: 87949, Reward: [-1640.915 -1640.915 -1640.915] [0.0000], Avg: [-1642.932 -1642.932 -1642.932] (1.000)
Step: 87999, Reward: [-1918.236 -1918.236 -1918.236] [0.0000], Avg: [-1643.088 -1643.088 -1643.088] (1.000)
Step: 88049, Reward: [-1528.338 -1528.338 -1528.338] [0.0000], Avg: [-1643.023 -1643.023 -1643.023] (1.000)
Step: 88099, Reward: [-1738.087 -1738.087 -1738.087] [0.0000], Avg: [-1643.077 -1643.077 -1643.077] (1.000)
Step: 88149, Reward: [-1438.037 -1438.037 -1438.037] [0.0000], Avg: [-1642.961 -1642.961 -1642.961] (1.000)
Step: 88199, Reward: [-1472.279 -1472.279 -1472.279] [0.0000], Avg: [-1642.864 -1642.864 -1642.864] (1.000)
Step: 88249, Reward: [-1638.022 -1638.022 -1638.022] [0.0000], Avg: [-1642.861 -1642.861 -1642.861] (1.000)
Step: 88299, Reward: [-1870.552 -1870.552 -1870.552] [0.0000], Avg: [-1642.99 -1642.99 -1642.99] (1.000)
Step: 88349, Reward: [-2235.624 -2235.624 -2235.624] [0.0000], Avg: [-1643.326 -1643.326 -1643.326] (1.000)
Step: 88399, Reward: [-2057.483 -2057.483 -2057.483] [0.0000], Avg: [-1643.56 -1643.56 -1643.56] (1.000)
Step: 88449, Reward: [-1369.346 -1369.346 -1369.346] [0.0000], Avg: [-1643.405 -1643.405 -1643.405] (1.000)
Step: 88499, Reward: [-1446.779 -1446.779 -1446.779] [0.0000], Avg: [-1643.294 -1643.294 -1643.294] (1.000)
Step: 88549, Reward: [-1692.179 -1692.179 -1692.179] [0.0000], Avg: [-1643.321 -1643.321 -1643.321] (1.000)
Step: 88599, Reward: [-1809.654 -1809.654 -1809.654] [0.0000], Avg: [-1643.415 -1643.415 -1643.415] (1.000)
Step: 88649, Reward: [-1632.633 -1632.633 -1632.633] [0.0000], Avg: [-1643.409 -1643.409 -1643.409] (1.000)
Step: 88699, Reward: [-2242.967 -2242.967 -2242.967] [0.0000], Avg: [-1643.747 -1643.747 -1643.747] (1.000)
Step: 88749, Reward: [-1754.062 -1754.062 -1754.062] [0.0000], Avg: [-1643.809 -1643.809 -1643.809] (1.000)
Step: 88799, Reward: [-1796.884 -1796.884 -1796.884] [0.0000], Avg: [-1643.895 -1643.895 -1643.895] (1.000)
Step: 88849, Reward: [-1618.536 -1618.536 -1618.536] [0.0000], Avg: [-1643.881 -1643.881 -1643.881] (1.000)
Step: 88899, Reward: [-1239.92 -1239.92 -1239.92] [0.0000], Avg: [-1643.654 -1643.654 -1643.654] (1.000)
Step: 88949, Reward: [-2072.941 -2072.941 -2072.941] [0.0000], Avg: [-1643.895 -1643.895 -1643.895] (1.000)
Step: 88999, Reward: [-1619.947 -1619.947 -1619.947] [0.0000], Avg: [-1643.882 -1643.882 -1643.882] (1.000)
Step: 89049, Reward: [-1696.021 -1696.021 -1696.021] [0.0000], Avg: [-1643.911 -1643.911 -1643.911] (1.000)
Step: 89099, Reward: [-2067.072 -2067.072 -2067.072] [0.0000], Avg: [-1644.149 -1644.149 -1644.149] (1.000)
Step: 89149, Reward: [-1554.399 -1554.399 -1554.399] [0.0000], Avg: [-1644.098 -1644.098 -1644.098] (1.000)
Step: 89199, Reward: [-1239.129 -1239.129 -1239.129] [0.0000], Avg: [-1643.871 -1643.871 -1643.871] (1.000)
Step: 89249, Reward: [-1192.592 -1192.592 -1192.592] [0.0000], Avg: [-1643.618 -1643.618 -1643.618] (1.000)
Step: 89299, Reward: [-1684.654 -1684.654 -1684.654] [0.0000], Avg: [-1643.641 -1643.641 -1643.641] (1.000)
Step: 89349, Reward: [-1758.556 -1758.556 -1758.556] [0.0000], Avg: [-1643.706 -1643.706 -1643.706] (1.000)
Step: 89399, Reward: [-1363.067 -1363.067 -1363.067] [0.0000], Avg: [-1643.549 -1643.549 -1643.549] (1.000)
Step: 89449, Reward: [-1866.348 -1866.348 -1866.348] [0.0000], Avg: [-1643.673 -1643.673 -1643.673] (1.000)
Step: 89499, Reward: [-1755.312 -1755.312 -1755.312] [0.0000], Avg: [-1643.736 -1643.736 -1643.736] (1.000)
Step: 89549, Reward: [-1528.296 -1528.296 -1528.296] [0.0000], Avg: [-1643.671 -1643.671 -1643.671] (1.000)
Step: 89599, Reward: [-1932.497 -1932.497 -1932.497] [0.0000], Avg: [-1643.832 -1643.832 -1643.832] (1.000)
Step: 89649, Reward: [-2139.852 -2139.852 -2139.852] [0.0000], Avg: [-1644.109 -1644.109 -1644.109] (1.000)
Step: 89699, Reward: [-1308.337 -1308.337 -1308.337] [0.0000], Avg: [-1643.922 -1643.922 -1643.922] (1.000)
Step: 89749, Reward: [-1582.643 -1582.643 -1582.643] [0.0000], Avg: [-1643.888 -1643.888 -1643.888] (1.000)
Step: 89799, Reward: [-1968.244 -1968.244 -1968.244] [0.0000], Avg: [-1644.068 -1644.068 -1644.068] (1.000)
Step: 89849, Reward: [-1585.818 -1585.818 -1585.818] [0.0000], Avg: [-1644.036 -1644.036 -1644.036] (1.000)
Step: 89899, Reward: [-2121.628 -2121.628 -2121.628] [0.0000], Avg: [-1644.302 -1644.302 -1644.302] (1.000)
Step: 89949, Reward: [-1314.382 -1314.382 -1314.382] [0.0000], Avg: [-1644.118 -1644.118 -1644.118] (1.000)
Step: 89999, Reward: [-1642.429 -1642.429 -1642.429] [0.0000], Avg: [-1644.117 -1644.117 -1644.117] (1.000)
Step: 90049, Reward: [-1406.655 -1406.655 -1406.655] [0.0000], Avg: [-1643.985 -1643.985 -1643.985] (1.000)
Step: 90099, Reward: [-1306.046 -1306.046 -1306.046] [0.0000], Avg: [-1643.798 -1643.798 -1643.798] (1.000)
Step: 90149, Reward: [-2231.876 -2231.876 -2231.876] [0.0000], Avg: [-1644.124 -1644.124 -1644.124] (1.000)
Step: 90199, Reward: [-2228.305 -2228.305 -2228.305] [0.0000], Avg: [-1644.448 -1644.448 -1644.448] (1.000)
Step: 90249, Reward: [-1591.465 -1591.465 -1591.465] [0.0000], Avg: [-1644.418 -1644.418 -1644.418] (1.000)
Step: 90299, Reward: [-1247.563 -1247.563 -1247.563] [0.0000], Avg: [-1644.199 -1644.199 -1644.199] (1.000)
Step: 90349, Reward: [-1634.577 -1634.577 -1634.577] [0.0000], Avg: [-1644.193 -1644.193 -1644.193] (1.000)
Step: 90399, Reward: [-1658.598 -1658.598 -1658.598] [0.0000], Avg: [-1644.201 -1644.201 -1644.201] (1.000)
Step: 90449, Reward: [-1705.955 -1705.955 -1705.955] [0.0000], Avg: [-1644.235 -1644.235 -1644.235] (1.000)
Step: 90499, Reward: [-1212.204 -1212.204 -1212.204] [0.0000], Avg: [-1643.997 -1643.997 -1643.997] (1.000)
Step: 90549, Reward: [-1708.344 -1708.344 -1708.344] [0.0000], Avg: [-1644.032 -1644.032 -1644.032] (1.000)
Step: 90599, Reward: [-2376.461 -2376.461 -2376.461] [0.0000], Avg: [-1644.437 -1644.437 -1644.437] (1.000)
Step: 90649, Reward: [-1928.26 -1928.26 -1928.26] [0.0000], Avg: [-1644.593 -1644.593 -1644.593] (1.000)
Step: 90699, Reward: [-1723.229 -1723.229 -1723.229] [0.0000], Avg: [-1644.636 -1644.636 -1644.636] (1.000)
Step: 90749, Reward: [-1352.444 -1352.444 -1352.444] [0.0000], Avg: [-1644.475 -1644.475 -1644.475] (1.000)
Step: 90799, Reward: [-1370.135 -1370.135 -1370.135] [0.0000], Avg: [-1644.324 -1644.324 -1644.324] (1.000)
Step: 90849, Reward: [-2227.452 -2227.452 -2227.452] [0.0000], Avg: [-1644.645 -1644.645 -1644.645] (1.000)
Step: 90899, Reward: [-1226.525 -1226.525 -1226.525] [0.0000], Avg: [-1644.415 -1644.415 -1644.415] (1.000)
Step: 90949, Reward: [-1822.419 -1822.419 -1822.419] [0.0000], Avg: [-1644.513 -1644.513 -1644.513] (1.000)
Step: 90999, Reward: [-1775.82 -1775.82 -1775.82] [0.0000], Avg: [-1644.585 -1644.585 -1644.585] (1.000)
Step: 91049, Reward: [-1498.537 -1498.537 -1498.537] [0.0000], Avg: [-1644.505 -1644.505 -1644.505] (1.000)
Step: 91099, Reward: [-2011.379 -2011.379 -2011.379] [0.0000], Avg: [-1644.706 -1644.706 -1644.706] (1.000)
Step: 91149, Reward: [-1768.91 -1768.91 -1768.91] [0.0000], Avg: [-1644.775 -1644.775 -1644.775] (1.000)
Step: 91199, Reward: [-1892.409 -1892.409 -1892.409] [0.0000], Avg: [-1644.91 -1644.91 -1644.91] (1.000)
Step: 91249, Reward: [-1221.107 -1221.107 -1221.107] [0.0000], Avg: [-1644.678 -1644.678 -1644.678] (1.000)
Step: 91299, Reward: [-1638.91 -1638.91 -1638.91] [0.0000], Avg: [-1644.675 -1644.675 -1644.675] (1.000)
Step: 91349, Reward: [-2265.54 -2265.54 -2265.54] [0.0000], Avg: [-1645.015 -1645.015 -1645.015] (1.000)
Step: 91399, Reward: [-2136.972 -2136.972 -2136.972] [0.0000], Avg: [-1645.284 -1645.284 -1645.284] (1.000)
Step: 91449, Reward: [-2075.498 -2075.498 -2075.498] [0.0000], Avg: [-1645.519 -1645.519 -1645.519] (1.000)
Step: 91499, Reward: [-1478.979 -1478.979 -1478.979] [0.0000], Avg: [-1645.428 -1645.428 -1645.428] (1.000)
Step: 91549, Reward: [-1118.166 -1118.166 -1118.166] [0.0000], Avg: [-1645.14 -1645.14 -1645.14] (1.000)
Step: 91599, Reward: [-1401.754 -1401.754 -1401.754] [0.0000], Avg: [-1645.007 -1645.007 -1645.007] (1.000)
Step: 91649, Reward: [-1962.659 -1962.659 -1962.659] [0.0000], Avg: [-1645.181 -1645.181 -1645.181] (1.000)
Step: 91699, Reward: [-1259.982 -1259.982 -1259.982] [0.0000], Avg: [-1644.971 -1644.971 -1644.971] (1.000)
Step: 91749, Reward: [-1373.885 -1373.885 -1373.885] [0.0000], Avg: [-1644.823 -1644.823 -1644.823] (1.000)
Step: 91799, Reward: [-2061.718 -2061.718 -2061.718] [0.0000], Avg: [-1645.05 -1645.05 -1645.05] (1.000)
Step: 91849, Reward: [-2077.695 -2077.695 -2077.695] [0.0000], Avg: [-1645.285 -1645.285 -1645.285] (1.000)
Step: 91899, Reward: [-1496.381 -1496.381 -1496.381] [0.0000], Avg: [-1645.204 -1645.204 -1645.204] (1.000)
Step: 91949, Reward: [-1657.642 -1657.642 -1657.642] [0.0000], Avg: [-1645.211 -1645.211 -1645.211] (1.000)
Step: 91999, Reward: [-1171.747 -1171.747 -1171.747] [0.0000], Avg: [-1644.954 -1644.954 -1644.954] (1.000)
Step: 92049, Reward: [-2088.387 -2088.387 -2088.387] [0.0000], Avg: [-1645.195 -1645.195 -1645.195] (1.000)
Step: 92099, Reward: [-2075.408 -2075.408 -2075.408] [0.0000], Avg: [-1645.428 -1645.428 -1645.428] (1.000)
Step: 92149, Reward: [-1061.403 -1061.403 -1061.403] [0.0000], Avg: [-1645.111 -1645.111 -1645.111] (1.000)
Step: 92199, Reward: [-1371.777 -1371.777 -1371.777] [0.0000], Avg: [-1644.963 -1644.963 -1644.963] (1.000)
Step: 92249, Reward: [-1838.927 -1838.927 -1838.927] [0.0000], Avg: [-1645.068 -1645.068 -1645.068] (1.000)
Step: 92299, Reward: [-1845.908 -1845.908 -1845.908] [0.0000], Avg: [-1645.177 -1645.177 -1645.177] (1.000)
Step: 92349, Reward: [-1399.031 -1399.031 -1399.031] [0.0000], Avg: [-1645.044 -1645.044 -1645.044] (1.000)
Step: 92399, Reward: [-2232.972 -2232.972 -2232.972] [0.0000], Avg: [-1645.362 -1645.362 -1645.362] (1.000)
Step: 92449, Reward: [-1884.476 -1884.476 -1884.476] [0.0000], Avg: [-1645.491 -1645.491 -1645.491] (1.000)
Step: 92499, Reward: [-1441.447 -1441.447 -1441.447] [0.0000], Avg: [-1645.381 -1645.381 -1645.381] (1.000)
Step: 92549, Reward: [-710.634 -710.634 -710.634] [0.0000], Avg: [-1644.876 -1644.876 -1644.876] (1.000)
Step: 92599, Reward: [-1608.661 -1608.661 -1608.661] [0.0000], Avg: [-1644.856 -1644.856 -1644.856] (1.000)
Step: 92649, Reward: [-1642.061 -1642.061 -1642.061] [0.0000], Avg: [-1644.855 -1644.855 -1644.855] (1.000)
Step: 92699, Reward: [-1806.247 -1806.247 -1806.247] [0.0000], Avg: [-1644.942 -1644.942 -1644.942] (1.000)
Step: 92749, Reward: [-1933.674 -1933.674 -1933.674] [0.0000], Avg: [-1645.098 -1645.098 -1645.098] (1.000)
Step: 92799, Reward: [-1323.089 -1323.089 -1323.089] [0.0000], Avg: [-1644.924 -1644.924 -1644.924] (1.000)
Step: 92849, Reward: [-1619.886 -1619.886 -1619.886] [0.0000], Avg: [-1644.911 -1644.911 -1644.911] (1.000)
Step: 92899, Reward: [-1637.985 -1637.985 -1637.985] [0.0000], Avg: [-1644.907 -1644.907 -1644.907] (1.000)
Step: 92949, Reward: [-859.983 -859.983 -859.983] [0.0000], Avg: [-1644.485 -1644.485 -1644.485] (1.000)
Step: 92999, Reward: [-1870.753 -1870.753 -1870.753] [0.0000], Avg: [-1644.606 -1644.606 -1644.606] (1.000)
Step: 93049, Reward: [-1960.17 -1960.17 -1960.17] [0.0000], Avg: [-1644.776 -1644.776 -1644.776] (1.000)
Step: 93099, Reward: [-1152.025 -1152.025 -1152.025] [0.0000], Avg: [-1644.511 -1644.511 -1644.511] (1.000)
Step: 93149, Reward: [-1650.557 -1650.557 -1650.557] [0.0000], Avg: [-1644.515 -1644.515 -1644.515] (1.000)
Step: 93199, Reward: [-1471.879 -1471.879 -1471.879] [0.0000], Avg: [-1644.422 -1644.422 -1644.422] (1.000)
Step: 93249, Reward: [-1571.227 -1571.227 -1571.227] [0.0000], Avg: [-1644.383 -1644.383 -1644.383] (1.000)
Step: 93299, Reward: [-2269.571 -2269.571 -2269.571] [0.0000], Avg: [-1644.718 -1644.718 -1644.718] (1.000)
Step: 93349, Reward: [-1597.797 -1597.797 -1597.797] [0.0000], Avg: [-1644.693 -1644.693 -1644.693] (1.000)
Step: 93399, Reward: [-1637.406 -1637.406 -1637.406] [0.0000], Avg: [-1644.689 -1644.689 -1644.689] (1.000)
Step: 93449, Reward: [-1727.673 -1727.673 -1727.673] [0.0000], Avg: [-1644.733 -1644.733 -1644.733] (1.000)
Step: 93499, Reward: [-1911.794 -1911.794 -1911.794] [0.0000], Avg: [-1644.876 -1644.876 -1644.876] (1.000)
Step: 93549, Reward: [-1450.513 -1450.513 -1450.513] [0.0000], Avg: [-1644.772 -1644.772 -1644.772] (1.000)
Step: 93599, Reward: [-1923.194 -1923.194 -1923.194] [0.0000], Avg: [-1644.921 -1644.921 -1644.921] (1.000)
Step: 93649, Reward: [-1603.178 -1603.178 -1603.178] [0.0000], Avg: [-1644.898 -1644.898 -1644.898] (1.000)
Step: 93699, Reward: [-1891.637 -1891.637 -1891.637] [0.0000], Avg: [-1645.03 -1645.03 -1645.03] (1.000)
Step: 93749, Reward: [-2264.3 -2264.3 -2264.3] [0.0000], Avg: [-1645.36 -1645.36 -1645.36] (1.000)
Step: 93799, Reward: [-2117.127 -2117.127 -2117.127] [0.0000], Avg: [-1645.612 -1645.612 -1645.612] (1.000)
Step: 93849, Reward: [-1785.128 -1785.128 -1785.128] [0.0000], Avg: [-1645.686 -1645.686 -1645.686] (1.000)
Step: 93899, Reward: [-1863.627 -1863.627 -1863.627] [0.0000], Avg: [-1645.802 -1645.802 -1645.802] (1.000)
Step: 93949, Reward: [-1224.309 -1224.309 -1224.309] [0.0000], Avg: [-1645.578 -1645.578 -1645.578] (1.000)
Step: 93999, Reward: [-1575.885 -1575.885 -1575.885] [0.0000], Avg: [-1645.541 -1645.541 -1645.541] (1.000)
Step: 94049, Reward: [-1519.951 -1519.951 -1519.951] [0.0000], Avg: [-1645.474 -1645.474 -1645.474] (1.000)
Step: 94099, Reward: [-1510.516 -1510.516 -1510.516] [0.0000], Avg: [-1645.402 -1645.402 -1645.402] (1.000)
Step: 94149, Reward: [-1607.459 -1607.459 -1607.459] [0.0000], Avg: [-1645.382 -1645.382 -1645.382] (1.000)
Step: 94199, Reward: [-2050.689 -2050.689 -2050.689] [0.0000], Avg: [-1645.597 -1645.597 -1645.597] (1.000)
Step: 94249, Reward: [-1988.892 -1988.892 -1988.892] [0.0000], Avg: [-1645.78 -1645.78 -1645.78] (1.000)
Step: 94299, Reward: [-1751.889 -1751.889 -1751.889] [0.0000], Avg: [-1645.836 -1645.836 -1645.836] (1.000)
Step: 94349, Reward: [-2064.823 -2064.823 -2064.823] [0.0000], Avg: [-1646.058 -1646.058 -1646.058] (1.000)
Step: 94399, Reward: [-1888.365 -1888.365 -1888.365] [0.0000], Avg: [-1646.186 -1646.186 -1646.186] (1.000)
Step: 94449, Reward: [-2187.016 -2187.016 -2187.016] [0.0000], Avg: [-1646.472 -1646.472 -1646.472] (1.000)
Step: 94499, Reward: [-1924.214 -1924.214 -1924.214] [0.0000], Avg: [-1646.619 -1646.619 -1646.619] (1.000)
Step: 94549, Reward: [-893.764 -893.764 -893.764] [0.0000], Avg: [-1646.221 -1646.221 -1646.221] (1.000)
Step: 94599, Reward: [-1714.663 -1714.663 -1714.663] [0.0000], Avg: [-1646.257 -1646.257 -1646.257] (1.000)
Step: 94649, Reward: [-1507.412 -1507.412 -1507.412] [0.0000], Avg: [-1646.184 -1646.184 -1646.184] (1.000)
Step: 94699, Reward: [-740.29 -740.29 -740.29] [0.0000], Avg: [-1645.706 -1645.706 -1645.706] (1.000)
Step: 94749, Reward: [-1168.128 -1168.128 -1168.128] [0.0000], Avg: [-1645.454 -1645.454 -1645.454] (1.000)
Step: 94799, Reward: [-1617.015 -1617.015 -1617.015] [0.0000], Avg: [-1645.439 -1645.439 -1645.439] (1.000)
Step: 94849, Reward: [-1587.126 -1587.126 -1587.126] [0.0000], Avg: [-1645.408 -1645.408 -1645.408] (1.000)
Step: 94899, Reward: [-1024.707 -1024.707 -1024.707] [0.0000], Avg: [-1645.081 -1645.081 -1645.081] (1.000)
Step: 94949, Reward: [-1427.368 -1427.368 -1427.368] [0.0000], Avg: [-1644.966 -1644.966 -1644.966] (1.000)
Step: 94999, Reward: [-1903.004 -1903.004 -1903.004] [0.0000], Avg: [-1645.102 -1645.102 -1645.102] (1.000)
Step: 95049, Reward: [-1593.346 -1593.346 -1593.346] [0.0000], Avg: [-1645.075 -1645.075 -1645.075] (1.000)
Step: 95099, Reward: [-1345.802 -1345.802 -1345.802] [0.0000], Avg: [-1644.918 -1644.918 -1644.918] (1.000)
Step: 95149, Reward: [-1562.444 -1562.444 -1562.444] [0.0000], Avg: [-1644.874 -1644.874 -1644.874] (1.000)
Step: 95199, Reward: [-1954.308 -1954.308 -1954.308] [0.0000], Avg: [-1645.037 -1645.037 -1645.037] (1.000)
Step: 95249, Reward: [-2140.972 -2140.972 -2140.972] [0.0000], Avg: [-1645.297 -1645.297 -1645.297] (1.000)
Step: 95299, Reward: [-2020.576 -2020.576 -2020.576] [0.0000], Avg: [-1645.494 -1645.494 -1645.494] (1.000)
Step: 95349, Reward: [-1423.518 -1423.518 -1423.518] [0.0000], Avg: [-1645.378 -1645.378 -1645.378] (1.000)
Step: 95399, Reward: [-1159.212 -1159.212 -1159.212] [0.0000], Avg: [-1645.123 -1645.123 -1645.123] (1.000)
Step: 95449, Reward: [-1897.428 -1897.428 -1897.428] [0.0000], Avg: [-1645.255 -1645.255 -1645.255] (1.000)
Step: 95499, Reward: [-1649.65 -1649.65 -1649.65] [0.0000], Avg: [-1645.257 -1645.257 -1645.257] (1.000)
Step: 95549, Reward: [-2245.668 -2245.668 -2245.668] [0.0000], Avg: [-1645.571 -1645.571 -1645.571] (1.000)
Step: 95599, Reward: [-1485.493 -1485.493 -1485.493] [0.0000], Avg: [-1645.488 -1645.488 -1645.488] (1.000)
Step: 95649, Reward: [-2218.465 -2218.465 -2218.465] [0.0000], Avg: [-1645.787 -1645.787 -1645.787] (1.000)
Step: 95699, Reward: [-1658.292 -1658.292 -1658.292] [0.0000], Avg: [-1645.794 -1645.794 -1645.794] (1.000)
Step: 95749, Reward: [-1395.015 -1395.015 -1395.015] [0.0000], Avg: [-1645.663 -1645.663 -1645.663] (1.000)
Step: 95799, Reward: [-1348. -1348. -1348.] [0.0000], Avg: [-1645.507 -1645.507 -1645.507] (1.000)
Step: 95849, Reward: [-1745.131 -1745.131 -1745.131] [0.0000], Avg: [-1645.559 -1645.559 -1645.559] (1.000)
Step: 95899, Reward: [-1654.364 -1654.364 -1654.364] [0.0000], Avg: [-1645.564 -1645.564 -1645.564] (1.000)
Step: 95949, Reward: [-1758.093 -1758.093 -1758.093] [0.0000], Avg: [-1645.623 -1645.623 -1645.623] (1.000)
Step: 95999, Reward: [-1197.522 -1197.522 -1197.522] [0.0000], Avg: [-1645.389 -1645.389 -1645.389] (1.000)
Step: 96049, Reward: [-2090.498 -2090.498 -2090.498] [0.0000], Avg: [-1645.621 -1645.621 -1645.621] (1.000)
Step: 96099, Reward: [-1845.306 -1845.306 -1845.306] [0.0000], Avg: [-1645.725 -1645.725 -1645.725] (1.000)
Step: 96149, Reward: [-1310.932 -1310.932 -1310.932] [0.0000], Avg: [-1645.551 -1645.551 -1645.551] (1.000)
Step: 96199, Reward: [-1168.614 -1168.614 -1168.614] [0.0000], Avg: [-1645.303 -1645.303 -1645.303] (1.000)
Step: 96249, Reward: [-1861.915 -1861.915 -1861.915] [0.0000], Avg: [-1645.415 -1645.415 -1645.415] (1.000)
Step: 96299, Reward: [-2122.976 -2122.976 -2122.976] [0.0000], Avg: [-1645.663 -1645.663 -1645.663] (1.000)
Step: 96349, Reward: [-1499.697 -1499.697 -1499.697] [0.0000], Avg: [-1645.588 -1645.588 -1645.588] (1.000)
Step: 96399, Reward: [-2013.849 -2013.849 -2013.849] [0.0000], Avg: [-1645.779 -1645.779 -1645.779] (1.000)
Step: 96449, Reward: [-1611.993 -1611.993 -1611.993] [0.0000], Avg: [-1645.761 -1645.761 -1645.761] (1.000)
Step: 96499, Reward: [-2002.15 -2002.15 -2002.15] [0.0000], Avg: [-1645.946 -1645.946 -1645.946] (1.000)
Step: 96549, Reward: [-1516.874 -1516.874 -1516.874] [0.0000], Avg: [-1645.879 -1645.879 -1645.879] (1.000)
Step: 96599, Reward: [-1271.995 -1271.995 -1271.995] [0.0000], Avg: [-1645.685 -1645.685 -1645.685] (1.000)
Step: 96649, Reward: [-1686.777 -1686.777 -1686.777] [0.0000], Avg: [-1645.707 -1645.707 -1645.707] (1.000)
Step: 96699, Reward: [-1672.533 -1672.533 -1672.533] [0.0000], Avg: [-1645.721 -1645.721 -1645.721] (1.000)
Step: 96749, Reward: [-2070.967 -2070.967 -2070.967] [0.0000], Avg: [-1645.94 -1645.94 -1645.94] (1.000)
Step: 96799, Reward: [-1281.502 -1281.502 -1281.502] [0.0000], Avg: [-1645.752 -1645.752 -1645.752] (1.000)
Step: 96849, Reward: [-1435.945 -1435.945 -1435.945] [0.0000], Avg: [-1645.644 -1645.644 -1645.644] (1.000)
Step: 96899, Reward: [-1297.402 -1297.402 -1297.402] [0.0000], Avg: [-1645.464 -1645.464 -1645.464] (1.000)
Step: 96949, Reward: [-1983.877 -1983.877 -1983.877] [0.0000], Avg: [-1645.639 -1645.639 -1645.639] (1.000)
Step: 96999, Reward: [-1956.104 -1956.104 -1956.104] [0.0000], Avg: [-1645.799 -1645.799 -1645.799] (1.000)
Step: 97049, Reward: [-1564.923 -1564.923 -1564.923] [0.0000], Avg: [-1645.757 -1645.757 -1645.757] (1.000)
Step: 97099, Reward: [-1907.621 -1907.621 -1907.621] [0.0000], Avg: [-1645.892 -1645.892 -1645.892] (1.000)
Step: 97149, Reward: [-1694.197 -1694.197 -1694.197] [0.0000], Avg: [-1645.917 -1645.917 -1645.917] (1.000)
Step: 97199, Reward: [-1782.422 -1782.422 -1782.422] [0.0000], Avg: [-1645.987 -1645.987 -1645.987] (1.000)
Step: 97249, Reward: [-1157.414 -1157.414 -1157.414] [0.0000], Avg: [-1645.736 -1645.736 -1645.736] (1.000)
Step: 97299, Reward: [-1584.117 -1584.117 -1584.117] [0.0000], Avg: [-1645.704 -1645.704 -1645.704] (1.000)
Step: 97349, Reward: [-1309.855 -1309.855 -1309.855] [0.0000], Avg: [-1645.532 -1645.532 -1645.532] (1.000)
Step: 97399, Reward: [-1560.585 -1560.585 -1560.585] [0.0000], Avg: [-1645.488 -1645.488 -1645.488] (1.000)
Step: 97449, Reward: [-2164.248 -2164.248 -2164.248] [0.0000], Avg: [-1645.754 -1645.754 -1645.754] (1.000)
Step: 97499, Reward: [-1334.054 -1334.054 -1334.054] [0.0000], Avg: [-1645.594 -1645.594 -1645.594] (1.000)
Step: 97549, Reward: [-1658.229 -1658.229 -1658.229] [0.0000], Avg: [-1645.601 -1645.601 -1645.601] (1.000)
Step: 97599, Reward: [-2209.414 -2209.414 -2209.414] [0.0000], Avg: [-1645.89 -1645.89 -1645.89] (1.000)
Step: 97649, Reward: [-1403.051 -1403.051 -1403.051] [0.0000], Avg: [-1645.765 -1645.765 -1645.765] (1.000)
Step: 97699, Reward: [-1246.364 -1246.364 -1246.364] [0.0000], Avg: [-1645.561 -1645.561 -1645.561] (1.000)
Step: 97749, Reward: [-1268.351 -1268.351 -1268.351] [0.0000], Avg: [-1645.368 -1645.368 -1645.368] (1.000)
Step: 97799, Reward: [-1233.33 -1233.33 -1233.33] [0.0000], Avg: [-1645.157 -1645.157 -1645.157] (1.000)
Step: 97849, Reward: [-1333.26 -1333.26 -1333.26] [0.0000], Avg: [-1644.998 -1644.998 -1644.998] (1.000)
Step: 97899, Reward: [-1253.92 -1253.92 -1253.92] [0.0000], Avg: [-1644.798 -1644.798 -1644.798] (1.000)
Step: 97949, Reward: [-1986.483 -1986.483 -1986.483] [0.0000], Avg: [-1644.973 -1644.973 -1644.973] (1.000)
Step: 97999, Reward: [-1266.624 -1266.624 -1266.624] [0.0000], Avg: [-1644.78 -1644.78 -1644.78] (1.000)
Step: 98049, Reward: [-1580.495 -1580.495 -1580.495] [0.0000], Avg: [-1644.747 -1644.747 -1644.747] (1.000)
Step: 98099, Reward: [-2045.986 -2045.986 -2045.986] [0.0000], Avg: [-1644.951 -1644.951 -1644.951] (1.000)
Step: 98149, Reward: [-2001.342 -2001.342 -2001.342] [0.0000], Avg: [-1645.133 -1645.133 -1645.133] (1.000)
Step: 98199, Reward: [-2200.517 -2200.517 -2200.517] [0.0000], Avg: [-1645.416 -1645.416 -1645.416] (1.000)
Step: 98249, Reward: [-997.126 -997.126 -997.126] [0.0000], Avg: [-1645.086 -1645.086 -1645.086] (1.000)
Step: 98299, Reward: [-1411.605 -1411.605 -1411.605] [0.0000], Avg: [-1644.967 -1644.967 -1644.967] (1.000)
Step: 98349, Reward: [-1201.288 -1201.288 -1201.288] [0.0000], Avg: [-1644.741 -1644.741 -1644.741] (1.000)
Step: 98399, Reward: [-1389.933 -1389.933 -1389.933] [0.0000], Avg: [-1644.612 -1644.612 -1644.612] (1.000)
Step: 98449, Reward: [-2232.442 -2232.442 -2232.442] [0.0000], Avg: [-1644.91 -1644.91 -1644.91] (1.000)
Step: 98499, Reward: [-1585.524 -1585.524 -1585.524] [0.0000], Avg: [-1644.88 -1644.88 -1644.88] (1.000)
Step: 98549, Reward: [-1738.087 -1738.087 -1738.087] [0.0000], Avg: [-1644.928 -1644.928 -1644.928] (1.000)
Step: 98599, Reward: [-1028.526 -1028.526 -1028.526] [0.0000], Avg: [-1644.615 -1644.615 -1644.615] (1.000)
Step: 98649, Reward: [-1236.979 -1236.979 -1236.979] [0.0000], Avg: [-1644.408 -1644.408 -1644.408] (1.000)
Step: 98699, Reward: [-1344.761 -1344.761 -1344.761] [0.0000], Avg: [-1644.257 -1644.257 -1644.257] (1.000)
Step: 98749, Reward: [-1719.746 -1719.746 -1719.746] [0.0000], Avg: [-1644.295 -1644.295 -1644.295] (1.000)
Step: 98799, Reward: [-1697.877 -1697.877 -1697.877] [0.0000], Avg: [-1644.322 -1644.322 -1644.322] (1.000)
Step: 98849, Reward: [-1794.073 -1794.073 -1794.073] [0.0000], Avg: [-1644.398 -1644.398 -1644.398] (1.000)
Step: 98899, Reward: [-1847.786 -1847.786 -1847.786] [0.0000], Avg: [-1644.5 -1644.5 -1644.5] (1.000)
Step: 98949, Reward: [-1938.125 -1938.125 -1938.125] [0.0000], Avg: [-1644.649 -1644.649 -1644.649] (1.000)
Step: 98999, Reward: [-1743.99 -1743.99 -1743.99] [0.0000], Avg: [-1644.699 -1644.699 -1644.699] (1.000)
Step: 99049, Reward: [-1638.08 -1638.08 -1638.08] [0.0000], Avg: [-1644.696 -1644.696 -1644.696] (1.000)
Step: 99099, Reward: [-1683.343 -1683.343 -1683.343] [0.0000], Avg: [-1644.715 -1644.715 -1644.715] (1.000)
Step: 99149, Reward: [-1525.485 -1525.485 -1525.485] [0.0000], Avg: [-1644.655 -1644.655 -1644.655] (1.000)
Step: 99199, Reward: [-1555.899 -1555.899 -1555.899] [0.0000], Avg: [-1644.61 -1644.61 -1644.61] (1.000)
Step: 99249, Reward: [-1703.973 -1703.973 -1703.973] [0.0000], Avg: [-1644.64 -1644.64 -1644.64] (1.000)
Step: 99299, Reward: [-1450.696 -1450.696 -1450.696] [0.0000], Avg: [-1644.543 -1644.543 -1644.543] (1.000)
Step: 99349, Reward: [-1438.351 -1438.351 -1438.351] [0.0000], Avg: [-1644.439 -1644.439 -1644.439] (1.000)
Step: 99399, Reward: [-1452.195 -1452.195 -1452.195] [0.0000], Avg: [-1644.342 -1644.342 -1644.342] (1.000)
Step: 99449, Reward: [-1041.972 -1041.972 -1041.972] [0.0000], Avg: [-1644.039 -1644.039 -1644.039] (1.000)
Step: 99499, Reward: [-1530.283 -1530.283 -1530.283] [0.0000], Avg: [-1643.982 -1643.982 -1643.982] (1.000)
Step: 99549, Reward: [-1760.46 -1760.46 -1760.46] [0.0000], Avg: [-1644.041 -1644.041 -1644.041] (1.000)
Step: 99599, Reward: [-1928.642 -1928.642 -1928.642] [0.0000], Avg: [-1644.183 -1644.183 -1644.183] (1.000)
Step: 99649, Reward: [-1612.974 -1612.974 -1612.974] [0.0000], Avg: [-1644.168 -1644.168 -1644.168] (1.000)
Step: 99699, Reward: [-1217.054 -1217.054 -1217.054] [0.0000], Avg: [-1643.954 -1643.954 -1643.954] (1.000)
Step: 99749, Reward: [-1378.089 -1378.089 -1378.089] [0.0000], Avg: [-1643.82 -1643.82 -1643.82] (1.000)
Step: 99799, Reward: [-2147.081 -2147.081 -2147.081] [0.0000], Avg: [-1644.072 -1644.072 -1644.072] (1.000)
Step: 99849, Reward: [-2077.186 -2077.186 -2077.186] [0.0000], Avg: [-1644.289 -1644.289 -1644.289] (1.000)
Step: 99899, Reward: [-1649.456 -1649.456 -1649.456] [0.0000], Avg: [-1644.292 -1644.292 -1644.292] (1.000)
Step: 99949, Reward: [-1574.793 -1574.793 -1574.793] [0.0000], Avg: [-1644.257 -1644.257 -1644.257] (1.000)
Step: 99999, Reward: [-1086.798 -1086.798 -1086.798] [0.0000], Avg: [-1643.978 -1643.978 -1643.978] (1.000)
