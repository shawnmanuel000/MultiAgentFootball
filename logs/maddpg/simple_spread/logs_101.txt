Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01
DISCOUNT_RATE = 0.95

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			target_value = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			actual_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (actual_value - target_value.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			curr_pol_out = agent.get_action(states[i], grad=True, numpy=False)
			curr_pol_vf_in = gsoftmax(curr_pol_out, hard=True)
			action = [curr_pol_vf_in if j==i else one_hot(model.get_action(ob, numpy=False)) for (j,model), ob in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -agent.critic_local(states_joint, action_joint).mean() + 0.001*curr_pol_out.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.network.optimize(states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-386.874 -386.874 -386.874] [0.0000], Avg: [-386.874 -386.874 -386.874] (1.000)
Step: 99, Reward: [-479.993 -479.993 -479.993] [0.0000], Avg: [-433.434 -433.434 -433.434] (1.000)
Step: 149, Reward: [-452.471 -452.471 -452.471] [0.0000], Avg: [-439.779 -439.779 -439.779] (1.000)
Step: 199, Reward: [-478.131 -478.131 -478.131] [0.0000], Avg: [-449.367 -449.367 -449.367] (1.000)
Step: 249, Reward: [-668.956 -668.956 -668.956] [0.0000], Avg: [-493.285 -493.285 -493.285] (1.000)
Step: 299, Reward: [-1010.23 -1010.23 -1010.23] [0.0000], Avg: [-579.442 -579.442 -579.442] (1.000)
Step: 349, Reward: [-582.957 -582.957 -582.957] [0.0000], Avg: [-579.944 -579.944 -579.944] (1.000)
Step: 399, Reward: [-465.467 -465.467 -465.467] [0.0000], Avg: [-565.635 -565.635 -565.635] (1.000)
Step: 449, Reward: [-449.765 -449.765 -449.765] [0.0000], Avg: [-552.76 -552.76 -552.76] (1.000)
Step: 499, Reward: [-396.876 -396.876 -396.876] [0.0000], Avg: [-537.172 -537.172 -537.172] (1.000)
Step: 549, Reward: [-373.472 -373.472 -373.472] [0.0000], Avg: [-522.29 -522.29 -522.29] (1.000)
Step: 599, Reward: [-513.296 -513.296 -513.296] [0.0000], Avg: [-521.54 -521.54 -521.54] (1.000)
Step: 649, Reward: [-458.716 -458.716 -458.716] [0.0000], Avg: [-516.708 -516.708 -516.708] (1.000)
Step: 699, Reward: [-629.134 -629.134 -629.134] [0.0000], Avg: [-524.738 -524.738 -524.738] (1.000)
Step: 749, Reward: [-426.02 -426.02 -426.02] [0.0000], Avg: [-518.157 -518.157 -518.157] (1.000)
Step: 799, Reward: [-418.182 -418.182 -418.182] [0.0000], Avg: [-511.909 -511.909 -511.909] (1.000)
Step: 849, Reward: [-367.923 -367.923 -367.923] [0.0000], Avg: [-503.439 -503.439 -503.439] (1.000)
Step: 899, Reward: [-471.305 -471.305 -471.305] [0.0000], Avg: [-501.654 -501.654 -501.654] (1.000)
Step: 949, Reward: [-554.554 -554.554 -554.554] [0.0000], Avg: [-504.438 -504.438 -504.438] (1.000)
Step: 999, Reward: [-287.699 -287.699 -287.699] [0.0000], Avg: [-493.601 -493.601 -493.601] (1.000)
Step: 1049, Reward: [-530.77 -530.77 -530.77] [0.0000], Avg: [-495.371 -495.371 -495.371] (1.000)
Step: 1099, Reward: [-383.031 -383.031 -383.031] [0.0000], Avg: [-490.265 -490.265 -490.265] (1.000)
Step: 1149, Reward: [-600.587 -600.587 -600.587] [0.0000], Avg: [-495.061 -495.061 -495.061] (1.000)
Step: 1199, Reward: [-701.952 -701.952 -701.952] [0.0000], Avg: [-503.682 -503.682 -503.682] (1.000)
Step: 1249, Reward: [-585.382 -585.382 -585.382] [0.0000], Avg: [-506.95 -506.95 -506.95] (1.000)
Step: 1299, Reward: [-852.258 -852.258 -852.258] [0.0000], Avg: [-520.231 -520.231 -520.231] (1.000)
Step: 1349, Reward: [-509.503 -509.503 -509.503] [0.0000], Avg: [-519.833 -519.833 -519.833] (1.000)
Step: 1399, Reward: [-581.386 -581.386 -581.386] [0.0000], Avg: [-522.032 -522.032 -522.032] (1.000)
Step: 1449, Reward: [-531.189 -531.189 -531.189] [0.0000], Avg: [-522.347 -522.347 -522.347] (1.000)
Step: 1499, Reward: [-785.5 -785.5 -785.5] [0.0000], Avg: [-531.119 -531.119 -531.119] (1.000)
Step: 1549, Reward: [-570.282 -570.282 -570.282] [0.0000], Avg: [-532.383 -532.383 -532.383] (1.000)
Step: 1599, Reward: [-491.763 -491.763 -491.763] [0.0000], Avg: [-531.113 -531.113 -531.113] (1.000)
Step: 1649, Reward: [-461.902 -461.902 -461.902] [0.0000], Avg: [-529.016 -529.016 -529.016] (1.000)
Step: 1699, Reward: [-595.594 -595.594 -595.594] [0.0000], Avg: [-530.974 -530.974 -530.974] (1.000)
Step: 1749, Reward: [-403.22 -403.22 -403.22] [0.0000], Avg: [-527.324 -527.324 -527.324] (1.000)
Step: 1799, Reward: [-517.043 -517.043 -517.043] [0.0000], Avg: [-527.038 -527.038 -527.038] (1.000)
Step: 1849, Reward: [-542.613 -542.613 -542.613] [0.0000], Avg: [-527.459 -527.459 -527.459] (1.000)
Step: 1899, Reward: [-723.198 -723.198 -723.198] [0.0000], Avg: [-532.61 -532.61 -532.61] (1.000)
Step: 1949, Reward: [-447.842 -447.842 -447.842] [0.0000], Avg: [-530.437 -530.437 -530.437] (1.000)
Step: 1999, Reward: [-727.56 -727.56 -727.56] [0.0000], Avg: [-535.365 -535.365 -535.365] (1.000)
Step: 2049, Reward: [-595.981 -595.981 -595.981] [0.0000], Avg: [-536.843 -536.843 -536.843] (1.000)
Step: 2099, Reward: [-599.849 -599.849 -599.849] [0.0000], Avg: [-538.343 -538.343 -538.343] (1.000)
Step: 2149, Reward: [-538.238 -538.238 -538.238] [0.0000], Avg: [-538.341 -538.341 -538.341] (1.000)
Step: 2199, Reward: [-709.981 -709.981 -709.981] [0.0000], Avg: [-542.242 -542.242 -542.242] (1.000)
Step: 2249, Reward: [-579.536 -579.536 -579.536] [0.0000], Avg: [-543.071 -543.071 -543.071] (1.000)
Step: 2299, Reward: [-828.862 -828.862 -828.862] [0.0000], Avg: [-549.284 -549.284 -549.284] (1.000)
Step: 2349, Reward: [-1271.685 -1271.685 -1271.685] [0.0000], Avg: [-564.654 -564.654 -564.654] (1.000)
Step: 2399, Reward: [-1139.962 -1139.962 -1139.962] [0.0000], Avg: [-576.639 -576.639 -576.639] (1.000)
Step: 2449, Reward: [-1430.017 -1430.017 -1430.017] [0.0000], Avg: [-594.055 -594.055 -594.055] (1.000)
Step: 2499, Reward: [-1462.682 -1462.682 -1462.682] [0.0000], Avg: [-611.428 -611.428 -611.428] (1.000)
Step: 2549, Reward: [-1554.877 -1554.877 -1554.877] [0.0000], Avg: [-629.927 -629.927 -629.927] (1.000)
Step: 2599, Reward: [-1561.288 -1561.288 -1561.288] [0.0000], Avg: [-647.838 -647.838 -647.838] (1.000)
Step: 2649, Reward: [-1266.668 -1266.668 -1266.668] [0.0000], Avg: [-659.514 -659.514 -659.514] (1.000)
Step: 2699, Reward: [-927.7 -927.7 -927.7] [0.0000], Avg: [-664.48 -664.48 -664.48] (1.000)
Step: 2749, Reward: [-869.109 -869.109 -869.109] [0.0000], Avg: [-668.201 -668.201 -668.201] (1.000)
Step: 2799, Reward: [-839.495 -839.495 -839.495] [0.0000], Avg: [-671.259 -671.259 -671.259] (1.000)
Step: 2849, Reward: [-1137.031 -1137.031 -1137.031] [0.0000], Avg: [-679.431 -679.431 -679.431] (1.000)
Step: 2899, Reward: [-1169.423 -1169.423 -1169.423] [0.0000], Avg: [-687.879 -687.879 -687.879] (1.000)
Step: 2949, Reward: [-1061.463 -1061.463 -1061.463] [0.0000], Avg: [-694.211 -694.211 -694.211] (1.000)
Step: 2999, Reward: [-566.139 -566.139 -566.139] [0.0000], Avg: [-692.076 -692.076 -692.076] (1.000)
Step: 3049, Reward: [-488.307 -488.307 -488.307] [0.0000], Avg: [-688.736 -688.736 -688.736] (1.000)
Step: 3099, Reward: [-572.283 -572.283 -572.283] [0.0000], Avg: [-686.858 -686.858 -686.858] (1.000)
Step: 3149, Reward: [-589.955 -589.955 -589.955] [0.0000], Avg: [-685.319 -685.319 -685.319] (1.000)
Step: 3199, Reward: [-737.052 -737.052 -737.052] [0.0000], Avg: [-686.128 -686.128 -686.128] (1.000)
Step: 3249, Reward: [-474.533 -474.533 -474.533] [0.0000], Avg: [-682.872 -682.872 -682.872] (1.000)
Step: 3299, Reward: [-642.701 -642.701 -642.701] [0.0000], Avg: [-682.264 -682.264 -682.264] (1.000)
Step: 3349, Reward: [-506.751 -506.751 -506.751] [0.0000], Avg: [-679.644 -679.644 -679.644] (1.000)
Step: 3399, Reward: [-567.191 -567.191 -567.191] [0.0000], Avg: [-677.991 -677.991 -677.991] (1.000)
Step: 3449, Reward: [-558.941 -558.941 -558.941] [0.0000], Avg: [-676.265 -676.265 -676.265] (1.000)
Step: 3499, Reward: [-744.8 -744.8 -744.8] [0.0000], Avg: [-677.244 -677.244 -677.244] (1.000)
Step: 3549, Reward: [-477.116 -477.116 -477.116] [0.0000], Avg: [-674.426 -674.426 -674.426] (1.000)
Step: 3599, Reward: [-486.552 -486.552 -486.552] [0.0000], Avg: [-671.816 -671.816 -671.816] (1.000)
Step: 3649, Reward: [-403.46 -403.46 -403.46] [0.0000], Avg: [-668.14 -668.14 -668.14] (1.000)
Step: 3699, Reward: [-499.255 -499.255 -499.255] [0.0000], Avg: [-665.858 -665.858 -665.858] (1.000)
Step: 3749, Reward: [-523.457 -523.457 -523.457] [0.0000], Avg: [-663.959 -663.959 -663.959] (1.000)
Step: 3799, Reward: [-765.163 -765.163 -765.163] [0.0000], Avg: [-665.291 -665.291 -665.291] (1.000)
Step: 3849, Reward: [-481.651 -481.651 -481.651] [0.0000], Avg: [-662.906 -662.906 -662.906] (1.000)
Step: 3899, Reward: [-381.319 -381.319 -381.319] [0.0000], Avg: [-659.296 -659.296 -659.296] (1.000)
Step: 3949, Reward: [-466.584 -466.584 -466.584] [0.0000], Avg: [-656.856 -656.856 -656.856] (1.000)
Step: 3999, Reward: [-666.459 -666.459 -666.459] [0.0000], Avg: [-656.976 -656.976 -656.976] (1.000)
Step: 4049, Reward: [-540.289 -540.289 -540.289] [0.0000], Avg: [-655.536 -655.536 -655.536] (1.000)
Step: 4099, Reward: [-585.809 -585.809 -585.809] [0.0000], Avg: [-654.685 -654.685 -654.685] (1.000)
Step: 4149, Reward: [-546.419 -546.419 -546.419] [0.0000], Avg: [-653.381 -653.381 -653.381] (1.000)
Step: 4199, Reward: [-538.044 -538.044 -538.044] [0.0000], Avg: [-652.008 -652.008 -652.008] (1.000)
Step: 4249, Reward: [-614.915 -614.915 -614.915] [0.0000], Avg: [-651.572 -651.572 -651.572] (1.000)
Step: 4299, Reward: [-500.816 -500.816 -500.816] [0.0000], Avg: [-649.819 -649.819 -649.819] (1.000)
Step: 4349, Reward: [-461.113 -461.113 -461.113] [0.0000], Avg: [-647.65 -647.65 -647.65] (1.000)
Step: 4399, Reward: [-560.711 -560.711 -560.711] [0.0000], Avg: [-646.662 -646.662 -646.662] (1.000)
Step: 4449, Reward: [-626.586 -626.586 -626.586] [0.0000], Avg: [-646.436 -646.436 -646.436] (1.000)
Step: 4499, Reward: [-396.386 -396.386 -396.386] [0.0000], Avg: [-643.658 -643.658 -643.658] (1.000)
Step: 4549, Reward: [-450.59 -450.59 -450.59] [0.0000], Avg: [-641.536 -641.536 -641.536] (1.000)
Step: 4599, Reward: [-471.412 -471.412 -471.412] [0.0000], Avg: [-639.687 -639.687 -639.687] (1.000)
Step: 4649, Reward: [-680.768 -680.768 -680.768] [0.0000], Avg: [-640.129 -640.129 -640.129] (1.000)
Step: 4699, Reward: [-545.459 -545.459 -545.459] [0.0000], Avg: [-639.122 -639.122 -639.122] (1.000)
Step: 4749, Reward: [-664.032 -664.032 -664.032] [0.0000], Avg: [-639.384 -639.384 -639.384] (1.000)
Step: 4799, Reward: [-654.171 -654.171 -654.171] [0.0000], Avg: [-639.538 -639.538 -639.538] (1.000)
Step: 4849, Reward: [-451.139 -451.139 -451.139] [0.0000], Avg: [-637.596 -637.596 -637.596] (1.000)
Step: 4899, Reward: [-734.022 -734.022 -734.022] [0.0000], Avg: [-638.58 -638.58 -638.58] (1.000)
Step: 4949, Reward: [-421.456 -421.456 -421.456] [0.0000], Avg: [-636.386 -636.386 -636.386] (1.000)
Step: 4999, Reward: [-360.15 -360.15 -360.15] [0.0000], Avg: [-633.624 -633.624 -633.624] (1.000)
Step: 5049, Reward: [-418.23 -418.23 -418.23] [0.0000], Avg: [-631.491 -631.491 -631.491] (1.000)
Step: 5099, Reward: [-586.981 -586.981 -586.981] [0.0000], Avg: [-631.055 -631.055 -631.055] (1.000)
Step: 5149, Reward: [-335.1 -335.1 -335.1] [0.0000], Avg: [-628.182 -628.182 -628.182] (1.000)
Step: 5199, Reward: [-389.285 -389.285 -389.285] [0.0000], Avg: [-625.885 -625.885 -625.885] (1.000)
Step: 5249, Reward: [-648.095 -648.095 -648.095] [0.0000], Avg: [-626.096 -626.096 -626.096] (1.000)
Step: 5299, Reward: [-419.478 -419.478 -419.478] [0.0000], Avg: [-624.147 -624.147 -624.147] (1.000)
Step: 5349, Reward: [-519.102 -519.102 -519.102] [0.0000], Avg: [-623.165 -623.165 -623.165] (1.000)
Step: 5399, Reward: [-430.496 -430.496 -430.496] [0.0000], Avg: [-621.381 -621.381 -621.381] (1.000)
Step: 5449, Reward: [-474.662 -474.662 -474.662] [0.0000], Avg: [-620.035 -620.035 -620.035] (1.000)
Step: 5499, Reward: [-623.389 -623.389 -623.389] [0.0000], Avg: [-620.066 -620.066 -620.066] (1.000)
Step: 5549, Reward: [-528.604 -528.604 -528.604] [0.0000], Avg: [-619.242 -619.242 -619.242] (1.000)
Step: 5599, Reward: [-770.861 -770.861 -770.861] [0.0000], Avg: [-620.595 -620.595 -620.595] (1.000)
Step: 5649, Reward: [-425.129 -425.129 -425.129] [0.0000], Avg: [-618.866 -618.866 -618.866] (1.000)
Step: 5699, Reward: [-414.836 -414.836 -414.836] [0.0000], Avg: [-617.076 -617.076 -617.076] (1.000)
Step: 5749, Reward: [-347.613 -347.613 -347.613] [0.0000], Avg: [-614.733 -614.733 -614.733] (1.000)
Step: 5799, Reward: [-417.582 -417.582 -417.582] [0.0000], Avg: [-613.033 -613.033 -613.033] (1.000)
Step: 5849, Reward: [-374.296 -374.296 -374.296] [0.0000], Avg: [-610.993 -610.993 -610.993] (1.000)
Step: 5899, Reward: [-527.783 -527.783 -527.783] [0.0000], Avg: [-610.287 -610.287 -610.287] (1.000)
Step: 5949, Reward: [-598.104 -598.104 -598.104] [0.0000], Avg: [-610.185 -610.185 -610.185] (1.000)
Step: 5999, Reward: [-644.671 -644.671 -644.671] [0.0000], Avg: [-610.472 -610.472 -610.472] (1.000)
Step: 6049, Reward: [-414.616 -414.616 -414.616] [0.0000], Avg: [-608.854 -608.854 -608.854] (1.000)
Step: 6099, Reward: [-388.897 -388.897 -388.897] [0.0000], Avg: [-607.051 -607.051 -607.051] (1.000)
Step: 6149, Reward: [-466.971 -466.971 -466.971] [0.0000], Avg: [-605.912 -605.912 -605.912] (1.000)
Step: 6199, Reward: [-617.619 -617.619 -617.619] [0.0000], Avg: [-606.006 -606.006 -606.006] (1.000)
Step: 6249, Reward: [-466.032 -466.032 -466.032] [0.0000], Avg: [-604.887 -604.887 -604.887] (1.000)
Step: 6299, Reward: [-562.404 -562.404 -562.404] [0.0000], Avg: [-604.549 -604.549 -604.549] (1.000)
Step: 6349, Reward: [-433.91 -433.91 -433.91] [0.0000], Avg: [-603.206 -603.206 -603.206] (1.000)
Step: 6399, Reward: [-556.535 -556.535 -556.535] [0.0000], Avg: [-602.841 -602.841 -602.841] (1.000)
Step: 6449, Reward: [-512.008 -512.008 -512.008] [0.0000], Avg: [-602.137 -602.137 -602.137] (1.000)
Step: 6499, Reward: [-717.177 -717.177 -717.177] [0.0000], Avg: [-603.022 -603.022 -603.022] (1.000)
Step: 6549, Reward: [-316.732 -316.732 -316.732] [0.0000], Avg: [-600.837 -600.837 -600.837] (1.000)
Step: 6599, Reward: [-509.63 -509.63 -509.63] [0.0000], Avg: [-600.146 -600.146 -600.146] (1.000)
Step: 6649, Reward: [-371.009 -371.009 -371.009] [0.0000], Avg: [-598.423 -598.423 -598.423] (1.000)
Step: 6699, Reward: [-575.526 -575.526 -575.526] [0.0000], Avg: [-598.252 -598.252 -598.252] (1.000)
Step: 6749, Reward: [-444.968 -444.968 -444.968] [0.0000], Avg: [-597.117 -597.117 -597.117] (1.000)
Step: 6799, Reward: [-706.73 -706.73 -706.73] [0.0000], Avg: [-597.923 -597.923 -597.923] (1.000)
Step: 6849, Reward: [-502.638 -502.638 -502.638] [0.0000], Avg: [-597.227 -597.227 -597.227] (1.000)
Step: 6899, Reward: [-441.508 -441.508 -441.508] [0.0000], Avg: [-596.099 -596.099 -596.099] (1.000)
Step: 6949, Reward: [-366.232 -366.232 -366.232] [0.0000], Avg: [-594.445 -594.445 -594.445] (1.000)
Step: 6999, Reward: [-402.745 -402.745 -402.745] [0.0000], Avg: [-593.076 -593.076 -593.076] (1.000)
Step: 7049, Reward: [-613.312 -613.312 -613.312] [0.0000], Avg: [-593.219 -593.219 -593.219] (1.000)
Step: 7099, Reward: [-448.029 -448.029 -448.029] [0.0000], Avg: [-592.197 -592.197 -592.197] (1.000)
Step: 7149, Reward: [-647.37 -647.37 -647.37] [0.0000], Avg: [-592.582 -592.582 -592.582] (1.000)
Step: 7199, Reward: [-366.66 -366.66 -366.66] [0.0000], Avg: [-591.014 -591.014 -591.014] (1.000)
Step: 7249, Reward: [-499.779 -499.779 -499.779] [0.0000], Avg: [-590.384 -590.384 -590.384] (1.000)
Step: 7299, Reward: [-460.495 -460.495 -460.495] [0.0000], Avg: [-589.495 -589.495 -589.495] (1.000)
Step: 7349, Reward: [-357.59 -357.59 -357.59] [0.0000], Avg: [-587.917 -587.917 -587.917] (1.000)
Step: 7399, Reward: [-415.113 -415.113 -415.113] [0.0000], Avg: [-586.75 -586.75 -586.75] (1.000)
Step: 7449, Reward: [-594.859 -594.859 -594.859] [0.0000], Avg: [-586.804 -586.804 -586.804] (1.000)
Step: 7499, Reward: [-419.257 -419.257 -419.257] [0.0000], Avg: [-585.687 -585.687 -585.687] (1.000)
Step: 7549, Reward: [-457.163 -457.163 -457.163] [0.0000], Avg: [-584.836 -584.836 -584.836] (1.000)
Step: 7599, Reward: [-444.962 -444.962 -444.962] [0.0000], Avg: [-583.916 -583.916 -583.916] (1.000)
Step: 7649, Reward: [-513.842 -513.842 -513.842] [0.0000], Avg: [-583.458 -583.458 -583.458] (1.000)
Step: 7699, Reward: [-425.578 -425.578 -425.578] [0.0000], Avg: [-582.432 -582.432 -582.432] (1.000)
Step: 7749, Reward: [-439.303 -439.303 -439.303] [0.0000], Avg: [-581.509 -581.509 -581.509] (1.000)
Step: 7799, Reward: [-463.119 -463.119 -463.119] [0.0000], Avg: [-580.75 -580.75 -580.75] (1.000)
Step: 7849, Reward: [-446.766 -446.766 -446.766] [0.0000], Avg: [-579.897 -579.897 -579.897] (1.000)
Step: 7899, Reward: [-448.076 -448.076 -448.076] [0.0000], Avg: [-579.062 -579.062 -579.062] (1.000)
Step: 7949, Reward: [-556.328 -556.328 -556.328] [0.0000], Avg: [-578.919 -578.919 -578.919] (1.000)
Step: 7999, Reward: [-536.537 -536.537 -536.537] [0.0000], Avg: [-578.655 -578.655 -578.655] (1.000)
Step: 8049, Reward: [-501.452 -501.452 -501.452] [0.0000], Avg: [-578.175 -578.175 -578.175] (1.000)
Step: 8099, Reward: [-602.317 -602.317 -602.317] [0.0000], Avg: [-578.324 -578.324 -578.324] (1.000)
Step: 8149, Reward: [-404.644 -404.644 -404.644] [0.0000], Avg: [-577.258 -577.258 -577.258] (1.000)
Step: 8199, Reward: [-621.296 -621.296 -621.296] [0.0000], Avg: [-577.527 -577.527 -577.527] (1.000)
Step: 8249, Reward: [-523.889 -523.889 -523.889] [0.0000], Avg: [-577.202 -577.202 -577.202] (1.000)
Step: 8299, Reward: [-589.203 -589.203 -589.203] [0.0000], Avg: [-577.274 -577.274 -577.274] (1.000)
Step: 8349, Reward: [-623.857 -623.857 -623.857] [0.0000], Avg: [-577.553 -577.553 -577.553] (1.000)
Step: 8399, Reward: [-644.736 -644.736 -644.736] [0.0000], Avg: [-577.953 -577.953 -577.953] (1.000)
Step: 8449, Reward: [-783.741 -783.741 -783.741] [0.0000], Avg: [-579.171 -579.171 -579.171] (1.000)
Step: 8499, Reward: [-602.145 -602.145 -602.145] [0.0000], Avg: [-579.306 -579.306 -579.306] (1.000)
Step: 8549, Reward: [-485.954 -485.954 -485.954] [0.0000], Avg: [-578.76 -578.76 -578.76] (1.000)
Step: 8599, Reward: [-483.709 -483.709 -483.709] [0.0000], Avg: [-578.207 -578.207 -578.207] (1.000)
Step: 8649, Reward: [-414.909 -414.909 -414.909] [0.0000], Avg: [-577.263 -577.263 -577.263] (1.000)
Step: 8699, Reward: [-531.76 -531.76 -531.76] [0.0000], Avg: [-577.002 -577.002 -577.002] (1.000)
Step: 8749, Reward: [-457.316 -457.316 -457.316] [0.0000], Avg: [-576.318 -576.318 -576.318] (1.000)
Step: 8799, Reward: [-682.437 -682.437 -682.437] [0.0000], Avg: [-576.921 -576.921 -576.921] (1.000)
Step: 8849, Reward: [-533.942 -533.942 -533.942] [0.0000], Avg: [-576.678 -576.678 -576.678] (1.000)
Step: 8899, Reward: [-650.617 -650.617 -650.617] [0.0000], Avg: [-577.094 -577.094 -577.094] (1.000)
Step: 8949, Reward: [-612.945 -612.945 -612.945] [0.0000], Avg: [-577.294 -577.294 -577.294] (1.000)
Step: 8999, Reward: [-739.197 -739.197 -739.197] [0.0000], Avg: [-578.193 -578.193 -578.193] (1.000)
Step: 9049, Reward: [-539.034 -539.034 -539.034] [0.0000], Avg: [-577.977 -577.977 -577.977] (1.000)
Step: 9099, Reward: [-468.77 -468.77 -468.77] [0.0000], Avg: [-577.377 -577.377 -577.377] (1.000)
Step: 9149, Reward: [-587.815 -587.815 -587.815] [0.0000], Avg: [-577.434 -577.434 -577.434] (1.000)
Step: 9199, Reward: [-523.17 -523.17 -523.17] [0.0000], Avg: [-577.139 -577.139 -577.139] (1.000)
Step: 9249, Reward: [-549.257 -549.257 -549.257] [0.0000], Avg: [-576.988 -576.988 -576.988] (1.000)
Step: 9299, Reward: [-453.2 -453.2 -453.2] [0.0000], Avg: [-576.323 -576.323 -576.323] (1.000)
Step: 9349, Reward: [-556.7 -556.7 -556.7] [0.0000], Avg: [-576.218 -576.218 -576.218] (1.000)
Step: 9399, Reward: [-507.376 -507.376 -507.376] [0.0000], Avg: [-575.852 -575.852 -575.852] (1.000)
Step: 9449, Reward: [-429.969 -429.969 -429.969] [0.0000], Avg: [-575.08 -575.08 -575.08] (1.000)
Step: 9499, Reward: [-408.583 -408.583 -408.583] [0.0000], Avg: [-574.203 -574.203 -574.203] (1.000)
Step: 9549, Reward: [-515.215 -515.215 -515.215] [0.0000], Avg: [-573.895 -573.895 -573.895] (1.000)
Step: 9599, Reward: [-385.94 -385.94 -385.94] [0.0000], Avg: [-572.916 -572.916 -572.916] (1.000)
Step: 9649, Reward: [-457.931 -457.931 -457.931] [0.0000], Avg: [-572.32 -572.32 -572.32] (1.000)
Step: 9699, Reward: [-488.512 -488.512 -488.512] [0.0000], Avg: [-571.888 -571.888 -571.888] (1.000)
Step: 9749, Reward: [-359.85 -359.85 -359.85] [0.0000], Avg: [-570.801 -570.801 -570.801] (1.000)
Step: 9799, Reward: [-369.673 -369.673 -369.673] [0.0000], Avg: [-569.774 -569.774 -569.774] (1.000)
Step: 9849, Reward: [-485.158 -485.158 -485.158] [0.0000], Avg: [-569.345 -569.345 -569.345] (1.000)
Step: 9899, Reward: [-393.485 -393.485 -393.485] [0.0000], Avg: [-568.457 -568.457 -568.457] (1.000)
Step: 9949, Reward: [-452.889 -452.889 -452.889] [0.0000], Avg: [-567.876 -567.876 -567.876] (1.000)
Step: 9999, Reward: [-353.34 -353.34 -353.34] [0.0000], Avg: [-566.803 -566.803 -566.803] (1.000)
Step: 10049, Reward: [-424.835 -424.835 -424.835] [0.0000], Avg: [-566.097 -566.097 -566.097] (1.000)
Step: 10099, Reward: [-338.441 -338.441 -338.441] [0.0000], Avg: [-564.97 -564.97 -564.97] (1.000)
Step: 10149, Reward: [-477.818 -477.818 -477.818] [0.0000], Avg: [-564.541 -564.541 -564.541] (1.000)
Step: 10199, Reward: [-450.311 -450.311 -450.311] [0.0000], Avg: [-563.981 -563.981 -563.981] (1.000)
Step: 10249, Reward: [-463.297 -463.297 -463.297] [0.0000], Avg: [-563.49 -563.49 -563.49] (1.000)
Step: 10299, Reward: [-508.298 -508.298 -508.298] [0.0000], Avg: [-563.222 -563.222 -563.222] (1.000)
Step: 10349, Reward: [-366.468 -366.468 -366.468] [0.0000], Avg: [-562.271 -562.271 -562.271] (1.000)
Step: 10399, Reward: [-455.881 -455.881 -455.881] [0.0000], Avg: [-561.76 -561.76 -561.76] (1.000)
Step: 10449, Reward: [-384.103 -384.103 -384.103] [0.0000], Avg: [-560.91 -560.91 -560.91] (1.000)
Step: 10499, Reward: [-377.847 -377.847 -377.847] [0.0000], Avg: [-560.038 -560.038 -560.038] (1.000)
Step: 10549, Reward: [-513.235 -513.235 -513.235] [0.0000], Avg: [-559.816 -559.816 -559.816] (1.000)
Step: 10599, Reward: [-363.454 -363.454 -363.454] [0.0000], Avg: [-558.89 -558.89 -558.89] (1.000)
Step: 10649, Reward: [-420.322 -420.322 -420.322] [0.0000], Avg: [-558.239 -558.239 -558.239] (1.000)
Step: 10699, Reward: [-562.992 -562.992 -562.992] [0.0000], Avg: [-558.261 -558.261 -558.261] (1.000)
Step: 10749, Reward: [-299.244 -299.244 -299.244] [0.0000], Avg: [-557.057 -557.057 -557.057] (1.000)
Step: 10799, Reward: [-461.847 -461.847 -461.847] [0.0000], Avg: [-556.616 -556.616 -556.616] (1.000)
Step: 10849, Reward: [-375.372 -375.372 -375.372] [0.0000], Avg: [-555.781 -555.781 -555.781] (1.000)
Step: 10899, Reward: [-271.171 -271.171 -271.171] [0.0000], Avg: [-554.475 -554.475 -554.475] (1.000)
Step: 10949, Reward: [-484.148 -484.148 -484.148] [0.0000], Avg: [-554.154 -554.154 -554.154] (1.000)
Step: 10999, Reward: [-563.688 -563.688 -563.688] [0.0000], Avg: [-554.197 -554.197 -554.197] (1.000)
Step: 11049, Reward: [-409.574 -409.574 -409.574] [0.0000], Avg: [-553.543 -553.543 -553.543] (1.000)
Step: 11099, Reward: [-321.091 -321.091 -321.091] [0.0000], Avg: [-552.496 -552.496 -552.496] (1.000)
Step: 11149, Reward: [-321.262 -321.262 -321.262] [0.0000], Avg: [-551.459 -551.459 -551.459] (1.000)
Step: 11199, Reward: [-548.568 -548.568 -548.568] [0.0000], Avg: [-551.446 -551.446 -551.446] (1.000)
Step: 11249, Reward: [-525.497 -525.497 -525.497] [0.0000], Avg: [-551.331 -551.331 -551.331] (1.000)
Step: 11299, Reward: [-466.921 -466.921 -466.921] [0.0000], Avg: [-550.957 -550.957 -550.957] (1.000)
Step: 11349, Reward: [-402.897 -402.897 -402.897] [0.0000], Avg: [-550.305 -550.305 -550.305] (1.000)
Step: 11399, Reward: [-411.321 -411.321 -411.321] [0.0000], Avg: [-549.695 -549.695 -549.695] (1.000)
Step: 11449, Reward: [-439.991 -439.991 -439.991] [0.0000], Avg: [-549.216 -549.216 -549.216] (1.000)
Step: 11499, Reward: [-457.093 -457.093 -457.093] [0.0000], Avg: [-548.816 -548.816 -548.816] (1.000)
Step: 11549, Reward: [-366.705 -366.705 -366.705] [0.0000], Avg: [-548.027 -548.027 -548.027] (1.000)
Step: 11599, Reward: [-473.409 -473.409 -473.409] [0.0000], Avg: [-547.706 -547.706 -547.706] (1.000)
Step: 11649, Reward: [-486.328 -486.328 -486.328] [0.0000], Avg: [-547.442 -547.442 -547.442] (1.000)
Step: 11699, Reward: [-425.104 -425.104 -425.104] [0.0000], Avg: [-546.92 -546.92 -546.92] (1.000)
Step: 11749, Reward: [-402.506 -402.506 -402.506] [0.0000], Avg: [-546.305 -546.305 -546.305] (1.000)
Step: 11799, Reward: [-718.547 -718.547 -718.547] [0.0000], Avg: [-547.035 -547.035 -547.035] (1.000)
Step: 11849, Reward: [-602.903 -602.903 -602.903] [0.0000], Avg: [-547.271 -547.271 -547.271] (1.000)
Step: 11899, Reward: [-505.356 -505.356 -505.356] [0.0000], Avg: [-547.095 -547.095 -547.095] (1.000)
Step: 11949, Reward: [-446.509 -446.509 -446.509] [0.0000], Avg: [-546.674 -546.674 -546.674] (1.000)
Step: 11999, Reward: [-507.212 -507.212 -507.212] [0.0000], Avg: [-546.509 -546.509 -546.509] (1.000)
Step: 12049, Reward: [-464.968 -464.968 -464.968] [0.0000], Avg: [-546.171 -546.171 -546.171] (1.000)
Step: 12099, Reward: [-465.162 -465.162 -465.162] [0.0000], Avg: [-545.836 -545.836 -545.836] (1.000)
Step: 12149, Reward: [-539.07 -539.07 -539.07] [0.0000], Avg: [-545.808 -545.808 -545.808] (1.000)
Step: 12199, Reward: [-601.572 -601.572 -601.572] [0.0000], Avg: [-546.037 -546.037 -546.037] (1.000)
Step: 12249, Reward: [-546.688 -546.688 -546.688] [0.0000], Avg: [-546.04 -546.04 -546.04] (1.000)
Step: 12299, Reward: [-446.242 -446.242 -446.242] [0.0000], Avg: [-545.634 -545.634 -545.634] (1.000)
Step: 12349, Reward: [-416.374 -416.374 -416.374] [0.0000], Avg: [-545.111 -545.111 -545.111] (1.000)
Step: 12399, Reward: [-476.469 -476.469 -476.469] [0.0000], Avg: [-544.834 -544.834 -544.834] (1.000)
Step: 12449, Reward: [-482.702 -482.702 -482.702] [0.0000], Avg: [-544.584 -544.584 -544.584] (1.000)
Step: 12499, Reward: [-559.921 -559.921 -559.921] [0.0000], Avg: [-544.646 -544.646 -544.646] (1.000)
Step: 12549, Reward: [-366.664 -366.664 -366.664] [0.0000], Avg: [-543.936 -543.936 -543.936] (1.000)
Step: 12599, Reward: [-516.666 -516.666 -516.666] [0.0000], Avg: [-543.828 -543.828 -543.828] (1.000)
Step: 12649, Reward: [-574.184 -574.184 -574.184] [0.0000], Avg: [-543.948 -543.948 -543.948] (1.000)
Step: 12699, Reward: [-411.435 -411.435 -411.435] [0.0000], Avg: [-543.427 -543.427 -543.427] (1.000)
Step: 12749, Reward: [-544.995 -544.995 -544.995] [0.0000], Avg: [-543.433 -543.433 -543.433] (1.000)
Step: 12799, Reward: [-642.777 -642.777 -642.777] [0.0000], Avg: [-543.821 -543.821 -543.821] (1.000)
Step: 12849, Reward: [-657.82 -657.82 -657.82] [0.0000], Avg: [-544.264 -544.264 -544.264] (1.000)
Step: 12899, Reward: [-696.629 -696.629 -696.629] [0.0000], Avg: [-544.855 -544.855 -544.855] (1.000)
Step: 12949, Reward: [-437.486 -437.486 -437.486] [0.0000], Avg: [-544.44 -544.44 -544.44] (1.000)
Step: 12999, Reward: [-399.191 -399.191 -399.191] [0.0000], Avg: [-543.882 -543.882 -543.882] (1.000)
Step: 13049, Reward: [-745.549 -745.549 -745.549] [0.0000], Avg: [-544.654 -544.654 -544.654] (1.000)
Step: 13099, Reward: [-518.501 -518.501 -518.501] [0.0000], Avg: [-544.555 -544.555 -544.555] (1.000)
Step: 13149, Reward: [-552.689 -552.689 -552.689] [0.0000], Avg: [-544.585 -544.585 -544.585] (1.000)
Step: 13199, Reward: [-650.891 -650.891 -650.891] [0.0000], Avg: [-544.988 -544.988 -544.988] (1.000)
Step: 13249, Reward: [-425.556 -425.556 -425.556] [0.0000], Avg: [-544.537 -544.537 -544.537] (1.000)
Step: 13299, Reward: [-785.282 -785.282 -785.282] [0.0000], Avg: [-545.442 -545.442 -545.442] (1.000)
Step: 13349, Reward: [-574.608 -574.608 -574.608] [0.0000], Avg: [-545.552 -545.552 -545.552] (1.000)
Step: 13399, Reward: [-326.351 -326.351 -326.351] [0.0000], Avg: [-544.734 -544.734 -544.734] (1.000)
Step: 13449, Reward: [-453.944 -453.944 -453.944] [0.0000], Avg: [-544.396 -544.396 -544.396] (1.000)
Step: 13499, Reward: [-540.785 -540.785 -540.785] [0.0000], Avg: [-544.383 -544.383 -544.383] (1.000)
Step: 13549, Reward: [-526.94 -526.94 -526.94] [0.0000], Avg: [-544.319 -544.319 -544.319] (1.000)
Step: 13599, Reward: [-746.106 -746.106 -746.106] [0.0000], Avg: [-545.06 -545.06 -545.06] (1.000)
Step: 13649, Reward: [-553.631 -553.631 -553.631] [0.0000], Avg: [-545.092 -545.092 -545.092] (1.000)
Step: 13699, Reward: [-468.544 -468.544 -468.544] [0.0000], Avg: [-544.812 -544.812 -544.812] (1.000)
Step: 13749, Reward: [-393.091 -393.091 -393.091] [0.0000], Avg: [-544.261 -544.261 -544.261] (1.000)
Step: 13799, Reward: [-541.512 -541.512 -541.512] [0.0000], Avg: [-544.251 -544.251 -544.251] (1.000)
Step: 13849, Reward: [-912.974 -912.974 -912.974] [0.0000], Avg: [-545.582 -545.582 -545.582] (1.000)
Step: 13899, Reward: [-533.066 -533.066 -533.066] [0.0000], Avg: [-545.537 -545.537 -545.537] (1.000)
Step: 13949, Reward: [-513.841 -513.841 -513.841] [0.0000], Avg: [-545.423 -545.423 -545.423] (1.000)
Step: 13999, Reward: [-604.888 -604.888 -604.888] [0.0000], Avg: [-545.636 -545.636 -545.636] (1.000)
Step: 14049, Reward: [-569.409 -569.409 -569.409] [0.0000], Avg: [-545.72 -545.72 -545.72] (1.000)
Step: 14099, Reward: [-604.236 -604.236 -604.236] [0.0000], Avg: [-545.928 -545.928 -545.928] (1.000)
Step: 14149, Reward: [-479.585 -479.585 -479.585] [0.0000], Avg: [-545.693 -545.693 -545.693] (1.000)
Step: 14199, Reward: [-420.083 -420.083 -420.083] [0.0000], Avg: [-545.251 -545.251 -545.251] (1.000)
Step: 14249, Reward: [-424.781 -424.781 -424.781] [0.0000], Avg: [-544.828 -544.828 -544.828] (1.000)
Step: 14299, Reward: [-636.101 -636.101 -636.101] [0.0000], Avg: [-545.147 -545.147 -545.147] (1.000)
Step: 14349, Reward: [-699.889 -699.889 -699.889] [0.0000], Avg: [-545.687 -545.687 -545.687] (1.000)
Step: 14399, Reward: [-788.576 -788.576 -788.576] [0.0000], Avg: [-546.53 -546.53 -546.53] (1.000)
Step: 14449, Reward: [-606.553 -606.553 -606.553] [0.0000], Avg: [-546.738 -546.738 -546.738] (1.000)
Step: 14499, Reward: [-455.038 -455.038 -455.038] [0.0000], Avg: [-546.421 -546.421 -546.421] (1.000)
Step: 14549, Reward: [-578.584 -578.584 -578.584] [0.0000], Avg: [-546.532 -546.532 -546.532] (1.000)
Step: 14599, Reward: [-566.954 -566.954 -566.954] [0.0000], Avg: [-546.602 -546.602 -546.602] (1.000)
Step: 14649, Reward: [-793.891 -793.891 -793.891] [0.0000], Avg: [-547.446 -547.446 -547.446] (1.000)
Step: 14699, Reward: [-412.155 -412.155 -412.155] [0.0000], Avg: [-546.986 -546.986 -546.986] (1.000)
Step: 14749, Reward: [-575.358 -575.358 -575.358] [0.0000], Avg: [-547.082 -547.082 -547.082] (1.000)
Step: 14799, Reward: [-506.598 -506.598 -506.598] [0.0000], Avg: [-546.945 -546.945 -546.945] (1.000)
Step: 14849, Reward: [-504.796 -504.796 -504.796] [0.0000], Avg: [-546.803 -546.803 -546.803] (1.000)
Step: 14899, Reward: [-542.783 -542.783 -542.783] [0.0000], Avg: [-546.79 -546.79 -546.79] (1.000)
Step: 14949, Reward: [-561.124 -561.124 -561.124] [0.0000], Avg: [-546.838 -546.838 -546.838] (1.000)
Step: 14999, Reward: [-610.829 -610.829 -610.829] [0.0000], Avg: [-547.051 -547.051 -547.051] (1.000)
Step: 15049, Reward: [-777.374 -777.374 -777.374] [0.0000], Avg: [-547.816 -547.816 -547.816] (1.000)
Step: 15099, Reward: [-484.948 -484.948 -484.948] [0.0000], Avg: [-547.608 -547.608 -547.608] (1.000)
Step: 15149, Reward: [-775.687 -775.687 -775.687] [0.0000], Avg: [-548.361 -548.361 -548.361] (1.000)
Step: 15199, Reward: [-620.385 -620.385 -620.385] [0.0000], Avg: [-548.598 -548.598 -548.598] (1.000)
Step: 15249, Reward: [-479.037 -479.037 -479.037] [0.0000], Avg: [-548.37 -548.37 -548.37] (1.000)
Step: 15299, Reward: [-436.883 -436.883 -436.883] [0.0000], Avg: [-548.005 -548.005 -548.005] (1.000)
Step: 15349, Reward: [-630.622 -630.622 -630.622] [0.0000], Avg: [-548.274 -548.274 -548.274] (1.000)
Step: 15399, Reward: [-702.082 -702.082 -702.082] [0.0000], Avg: [-548.774 -548.774 -548.774] (1.000)
Step: 15449, Reward: [-455.956 -455.956 -455.956] [0.0000], Avg: [-548.473 -548.473 -548.473] (1.000)
Step: 15499, Reward: [-587.196 -587.196 -587.196] [0.0000], Avg: [-548.598 -548.598 -548.598] (1.000)
Step: 15549, Reward: [-698.4 -698.4 -698.4] [0.0000], Avg: [-549.08 -549.08 -549.08] (1.000)
Step: 15599, Reward: [-386.312 -386.312 -386.312] [0.0000], Avg: [-548.558 -548.558 -548.558] (1.000)
Step: 15649, Reward: [-464.612 -464.612 -464.612] [0.0000], Avg: [-548.29 -548.29 -548.29] (1.000)
Step: 15699, Reward: [-604.623 -604.623 -604.623] [0.0000], Avg: [-548.469 -548.469 -548.469] (1.000)
Step: 15749, Reward: [-487.734 -487.734 -487.734] [0.0000], Avg: [-548.277 -548.277 -548.277] (1.000)
Step: 15799, Reward: [-438.713 -438.713 -438.713] [0.0000], Avg: [-547.93 -547.93 -547.93] (1.000)
Step: 15849, Reward: [-561.478 -561.478 -561.478] [0.0000], Avg: [-547.973 -547.973 -547.973] (1.000)
Step: 15899, Reward: [-357.97 -357.97 -357.97] [0.0000], Avg: [-547.375 -547.375 -547.375] (1.000)
Step: 15949, Reward: [-403.952 -403.952 -403.952] [0.0000], Avg: [-546.926 -546.926 -546.926] (1.000)
Step: 15999, Reward: [-436.598 -436.598 -436.598] [0.0000], Avg: [-546.581 -546.581 -546.581] (1.000)
Step: 16049, Reward: [-437.154 -437.154 -437.154] [0.0000], Avg: [-546.24 -546.24 -546.24] (1.000)
Step: 16099, Reward: [-537.146 -537.146 -537.146] [0.0000], Avg: [-546.212 -546.212 -546.212] (1.000)
Step: 16149, Reward: [-465.23 -465.23 -465.23] [0.0000], Avg: [-545.961 -545.961 -545.961] (1.000)
Step: 16199, Reward: [-587.022 -587.022 -587.022] [0.0000], Avg: [-546.088 -546.088 -546.088] (1.000)
Step: 16249, Reward: [-727.703 -727.703 -727.703] [0.0000], Avg: [-546.647 -546.647 -546.647] (1.000)
Step: 16299, Reward: [-486.554 -486.554 -486.554] [0.0000], Avg: [-546.462 -546.462 -546.462] (1.000)
Step: 16349, Reward: [-627.286 -627.286 -627.286] [0.0000], Avg: [-546.709 -546.709 -546.709] (1.000)
Step: 16399, Reward: [-596.872 -596.872 -596.872] [0.0000], Avg: [-546.862 -546.862 -546.862] (1.000)
Step: 16449, Reward: [-692.454 -692.454 -692.454] [0.0000], Avg: [-547.305 -547.305 -547.305] (1.000)
Step: 16499, Reward: [-573.66 -573.66 -573.66] [0.0000], Avg: [-547.385 -547.385 -547.385] (1.000)
Step: 16549, Reward: [-498.542 -498.542 -498.542] [0.0000], Avg: [-547.237 -547.237 -547.237] (1.000)
Step: 16599, Reward: [-428.566 -428.566 -428.566] [0.0000], Avg: [-546.88 -546.88 -546.88] (1.000)
Step: 16649, Reward: [-420.904 -420.904 -420.904] [0.0000], Avg: [-546.501 -546.501 -546.501] (1.000)
Step: 16699, Reward: [-417.823 -417.823 -417.823] [0.0000], Avg: [-546.116 -546.116 -546.116] (1.000)
Step: 16749, Reward: [-593.917 -593.917 -593.917] [0.0000], Avg: [-546.259 -546.259 -546.259] (1.000)
Step: 16799, Reward: [-511.179 -511.179 -511.179] [0.0000], Avg: [-546.154 -546.154 -546.154] (1.000)
Step: 16849, Reward: [-580.345 -580.345 -580.345] [0.0000], Avg: [-546.256 -546.256 -546.256] (1.000)
Step: 16899, Reward: [-635.389 -635.389 -635.389] [0.0000], Avg: [-546.52 -546.52 -546.52] (1.000)
Step: 16949, Reward: [-617.598 -617.598 -617.598] [0.0000], Avg: [-546.729 -546.729 -546.729] (1.000)
Step: 16999, Reward: [-651.374 -651.374 -651.374] [0.0000], Avg: [-547.037 -547.037 -547.037] (1.000)
Step: 17049, Reward: [-584.776 -584.776 -584.776] [0.0000], Avg: [-547.148 -547.148 -547.148] (1.000)
Step: 17099, Reward: [-533.656 -533.656 -533.656] [0.0000], Avg: [-547.108 -547.108 -547.108] (1.000)
Step: 17149, Reward: [-476.06 -476.06 -476.06] [0.0000], Avg: [-546.901 -546.901 -546.901] (1.000)
Step: 17199, Reward: [-632.381 -632.381 -632.381] [0.0000], Avg: [-547.15 -547.15 -547.15] (1.000)
Step: 17249, Reward: [-591.297 -591.297 -591.297] [0.0000], Avg: [-547.278 -547.278 -547.278] (1.000)
Step: 17299, Reward: [-414.941 -414.941 -414.941] [0.0000], Avg: [-546.895 -546.895 -546.895] (1.000)
Step: 17349, Reward: [-451.076 -451.076 -451.076] [0.0000], Avg: [-546.619 -546.619 -546.619] (1.000)
Step: 17399, Reward: [-597.258 -597.258 -597.258] [0.0000], Avg: [-546.764 -546.764 -546.764] (1.000)
Step: 17449, Reward: [-531.268 -531.268 -531.268] [0.0000], Avg: [-546.72 -546.72 -546.72] (1.000)
Step: 17499, Reward: [-760.96 -760.96 -760.96] [0.0000], Avg: [-547.332 -547.332 -547.332] (1.000)
Step: 17549, Reward: [-670.289 -670.289 -670.289] [0.0000], Avg: [-547.682 -547.682 -547.682] (1.000)
Step: 17599, Reward: [-620.897 -620.897 -620.897] [0.0000], Avg: [-547.89 -547.89 -547.89] (1.000)
Step: 17649, Reward: [-1248.489 -1248.489 -1248.489] [0.0000], Avg: [-549.875 -549.875 -549.875] (1.000)
Step: 17699, Reward: [-971.902 -971.902 -971.902] [0.0000], Avg: [-551.067 -551.067 -551.067] (1.000)
Step: 17749, Reward: [-729.794 -729.794 -729.794] [0.0000], Avg: [-551.571 -551.571 -551.571] (1.000)
Step: 17799, Reward: [-751.407 -751.407 -751.407] [0.0000], Avg: [-552.132 -552.132 -552.132] (1.000)
Step: 17849, Reward: [-548.003 -548.003 -548.003] [0.0000], Avg: [-552.121 -552.121 -552.121] (1.000)
Step: 17899, Reward: [-959.607 -959.607 -959.607] [0.0000], Avg: [-553.259 -553.259 -553.259] (1.000)
Step: 17949, Reward: [-833.178 -833.178 -833.178] [0.0000], Avg: [-554.039 -554.039 -554.039] (1.000)
Step: 17999, Reward: [-1114.246 -1114.246 -1114.246] [0.0000], Avg: [-555.595 -555.595 -555.595] (1.000)
Step: 18049, Reward: [-1017.682 -1017.682 -1017.682] [0.0000], Avg: [-556.875 -556.875 -556.875] (1.000)
Step: 18099, Reward: [-768.172 -768.172 -768.172] [0.0000], Avg: [-557.458 -557.458 -557.458] (1.000)
Step: 18149, Reward: [-755.114 -755.114 -755.114] [0.0000], Avg: [-558.003 -558.003 -558.003] (1.000)
Step: 18199, Reward: [-1011.801 -1011.801 -1011.801] [0.0000], Avg: [-559.25 -559.25 -559.25] (1.000)
Step: 18249, Reward: [-1125.877 -1125.877 -1125.877] [0.0000], Avg: [-560.802 -560.802 -560.802] (1.000)
Step: 18299, Reward: [-1178.581 -1178.581 -1178.581] [0.0000], Avg: [-562.49 -562.49 -562.49] (1.000)
Step: 18349, Reward: [-693.381 -693.381 -693.381] [0.0000], Avg: [-562.847 -562.847 -562.847] (1.000)
Step: 18399, Reward: [-685.249 -685.249 -685.249] [0.0000], Avg: [-563.179 -563.179 -563.179] (1.000)
Step: 18449, Reward: [-1033.447 -1033.447 -1033.447] [0.0000], Avg: [-564.454 -564.454 -564.454] (1.000)
Step: 18499, Reward: [-1075.87 -1075.87 -1075.87] [0.0000], Avg: [-565.836 -565.836 -565.836] (1.000)
Step: 18549, Reward: [-837.201 -837.201 -837.201] [0.0000], Avg: [-566.567 -566.567 -566.567] (1.000)
Step: 18599, Reward: [-724.779 -724.779 -724.779] [0.0000], Avg: [-566.993 -566.993 -566.993] (1.000)
Step: 18649, Reward: [-924.075 -924.075 -924.075] [0.0000], Avg: [-567.95 -567.95 -567.95] (1.000)
Step: 18699, Reward: [-835.716 -835.716 -835.716] [0.0000], Avg: [-568.666 -568.666 -568.666] (1.000)
Step: 18749, Reward: [-1232.566 -1232.566 -1232.566] [0.0000], Avg: [-570.436 -570.436 -570.436] (1.000)
Step: 18799, Reward: [-1055.98 -1055.98 -1055.98] [0.0000], Avg: [-571.728 -571.728 -571.728] (1.000)
Step: 18849, Reward: [-1145.032 -1145.032 -1145.032] [0.0000], Avg: [-573.248 -573.248 -573.248] (1.000)
Step: 18899, Reward: [-980.091 -980.091 -980.091] [0.0000], Avg: [-574.325 -574.325 -574.325] (1.000)
Step: 18949, Reward: [-999.05 -999.05 -999.05] [0.0000], Avg: [-575.445 -575.445 -575.445] (1.000)
Step: 18999, Reward: [-653.394 -653.394 -653.394] [0.0000], Avg: [-575.65 -575.65 -575.65] (1.000)
Step: 19049, Reward: [-689.861 -689.861 -689.861] [0.0000], Avg: [-575.95 -575.95 -575.95] (1.000)
Step: 19099, Reward: [-706.753 -706.753 -706.753] [0.0000], Avg: [-576.293 -576.293 -576.293] (1.000)
Step: 19149, Reward: [-692.642 -692.642 -692.642] [0.0000], Avg: [-576.596 -576.596 -576.596] (1.000)
Step: 19199, Reward: [-689.489 -689.489 -689.489] [0.0000], Avg: [-576.89 -576.89 -576.89] (1.000)
Step: 19249, Reward: [-796.093 -796.093 -796.093] [0.0000], Avg: [-577.46 -577.46 -577.46] (1.000)
Step: 19299, Reward: [-578.867 -578.867 -578.867] [0.0000], Avg: [-577.463 -577.463 -577.463] (1.000)
Step: 19349, Reward: [-630.596 -630.596 -630.596] [0.0000], Avg: [-577.601 -577.601 -577.601] (1.000)
Step: 19399, Reward: [-646.562 -646.562 -646.562] [0.0000], Avg: [-577.778 -577.778 -577.778] (1.000)
Step: 19449, Reward: [-861.813 -861.813 -861.813] [0.0000], Avg: [-578.509 -578.509 -578.509] (1.000)
Step: 19499, Reward: [-916.051 -916.051 -916.051] [0.0000], Avg: [-579.374 -579.374 -579.374] (1.000)
Step: 19549, Reward: [-887.277 -887.277 -887.277] [0.0000], Avg: [-580.161 -580.161 -580.161] (1.000)
Step: 19599, Reward: [-868.04 -868.04 -868.04] [0.0000], Avg: [-580.896 -580.896 -580.896] (1.000)
Step: 19649, Reward: [-1671.021 -1671.021 -1671.021] [0.0000], Avg: [-583.67 -583.67 -583.67] (1.000)
Step: 19699, Reward: [-833.492 -833.492 -833.492] [0.0000], Avg: [-584.304 -584.304 -584.304] (1.000)
Step: 19749, Reward: [-840.566 -840.566 -840.566] [0.0000], Avg: [-584.953 -584.953 -584.953] (1.000)
Step: 19799, Reward: [-809.891 -809.891 -809.891] [0.0000], Avg: [-585.521 -585.521 -585.521] (1.000)
Step: 19849, Reward: [-649.728 -649.728 -649.728] [0.0000], Avg: [-585.682 -585.682 -585.682] (1.000)
Step: 19899, Reward: [-1328.112 -1328.112 -1328.112] [0.0000], Avg: [-587.548 -587.548 -587.548] (1.000)
Step: 19949, Reward: [-788.558 -788.558 -788.558] [0.0000], Avg: [-588.051 -588.051 -588.051] (1.000)
Step: 19999, Reward: [-828.126 -828.126 -828.126] [0.0000], Avg: [-588.652 -588.652 -588.652] (1.000)
Step: 20049, Reward: [-1218.157 -1218.157 -1218.157] [0.0000], Avg: [-590.222 -590.222 -590.222] (1.000)
Step: 20099, Reward: [-1081.584 -1081.584 -1081.584] [0.0000], Avg: [-591.444 -591.444 -591.444] (1.000)
Step: 20149, Reward: [-1105.946 -1105.946 -1105.946] [0.0000], Avg: [-592.72 -592.72 -592.72] (1.000)
Step: 20199, Reward: [-785.09 -785.09 -785.09] [0.0000], Avg: [-593.197 -593.197 -593.197] (1.000)
Step: 20249, Reward: [-670.538 -670.538 -670.538] [0.0000], Avg: [-593.388 -593.388 -593.388] (1.000)
Step: 20299, Reward: [-1071.237 -1071.237 -1071.237] [0.0000], Avg: [-594.565 -594.565 -594.565] (1.000)
Step: 20349, Reward: [-998.19 -998.19 -998.19] [0.0000], Avg: [-595.556 -595.556 -595.556] (1.000)
Step: 20399, Reward: [-771.252 -771.252 -771.252] [0.0000], Avg: [-595.987 -595.987 -595.987] (1.000)
Step: 20449, Reward: [-936.236 -936.236 -936.236] [0.0000], Avg: [-596.819 -596.819 -596.819] (1.000)
Step: 20499, Reward: [-923.81 -923.81 -923.81] [0.0000], Avg: [-597.616 -597.616 -597.616] (1.000)
Step: 20549, Reward: [-882.69 -882.69 -882.69] [0.0000], Avg: [-598.31 -598.31 -598.31] (1.000)
Step: 20599, Reward: [-1031.069 -1031.069 -1031.069] [0.0000], Avg: [-599.36 -599.36 -599.36] (1.000)
Step: 20649, Reward: [-984.728 -984.728 -984.728] [0.0000], Avg: [-600.293 -600.293 -600.293] (1.000)
Step: 20699, Reward: [-588.888 -588.888 -588.888] [0.0000], Avg: [-600.266 -600.266 -600.266] (1.000)
Step: 20749, Reward: [-595.694 -595.694 -595.694] [0.0000], Avg: [-600.255 -600.255 -600.255] (1.000)
Step: 20799, Reward: [-504.416 -504.416 -504.416] [0.0000], Avg: [-600.025 -600.025 -600.025] (1.000)
Step: 20849, Reward: [-623.187 -623.187 -623.187] [0.0000], Avg: [-600.08 -600.08 -600.08] (1.000)
Step: 20899, Reward: [-678.952 -678.952 -678.952] [0.0000], Avg: [-600.269 -600.269 -600.269] (1.000)
Step: 20949, Reward: [-449.069 -449.069 -449.069] [0.0000], Avg: [-599.908 -599.908 -599.908] (1.000)
Step: 20999, Reward: [-758.852 -758.852 -758.852] [0.0000], Avg: [-600.286 -600.286 -600.286] (1.000)
Step: 21049, Reward: [-651.109 -651.109 -651.109] [0.0000], Avg: [-600.407 -600.407 -600.407] (1.000)
Step: 21099, Reward: [-611.347 -611.347 -611.347] [0.0000], Avg: [-600.433 -600.433 -600.433] (1.000)
Step: 21149, Reward: [-593.873 -593.873 -593.873] [0.0000], Avg: [-600.417 -600.417 -600.417] (1.000)
Step: 21199, Reward: [-395.994 -395.994 -395.994] [0.0000], Avg: [-599.935 -599.935 -599.935] (1.000)
Step: 21249, Reward: [-486.82 -486.82 -486.82] [0.0000], Avg: [-599.669 -599.669 -599.669] (1.000)
Step: 21299, Reward: [-462.178 -462.178 -462.178] [0.0000], Avg: [-599.346 -599.346 -599.346] (1.000)
Step: 21349, Reward: [-496.021 -496.021 -496.021] [0.0000], Avg: [-599.104 -599.104 -599.104] (1.000)
Step: 21399, Reward: [-397.941 -397.941 -397.941] [0.0000], Avg: [-598.634 -598.634 -598.634] (1.000)
Step: 21449, Reward: [-572.447 -572.447 -572.447] [0.0000], Avg: [-598.573 -598.573 -598.573] (1.000)
Step: 21499, Reward: [-555.767 -555.767 -555.767] [0.0000], Avg: [-598.474 -598.474 -598.474] (1.000)
Step: 21549, Reward: [-382.719 -382.719 -382.719] [0.0000], Avg: [-597.973 -597.973 -597.973] (1.000)
Step: 21599, Reward: [-411.161 -411.161 -411.161] [0.0000], Avg: [-597.541 -597.541 -597.541] (1.000)
Step: 21649, Reward: [-499.128 -499.128 -499.128] [0.0000], Avg: [-597.314 -597.314 -597.314] (1.000)
Step: 21699, Reward: [-603.241 -603.241 -603.241] [0.0000], Avg: [-597.327 -597.327 -597.327] (1.000)
Step: 21749, Reward: [-387.426 -387.426 -387.426] [0.0000], Avg: [-596.845 -596.845 -596.845] (1.000)
Step: 21799, Reward: [-580.026 -580.026 -580.026] [0.0000], Avg: [-596.806 -596.806 -596.806] (1.000)
Step: 21849, Reward: [-340.124 -340.124 -340.124] [0.0000], Avg: [-596.219 -596.219 -596.219] (1.000)
Step: 21899, Reward: [-534.006 -534.006 -534.006] [0.0000], Avg: [-596.077 -596.077 -596.077] (1.000)
Step: 21949, Reward: [-441.906 -441.906 -441.906] [0.0000], Avg: [-595.725 -595.725 -595.725] (1.000)
Step: 21999, Reward: [-504.685 -504.685 -504.685] [0.0000], Avg: [-595.519 -595.519 -595.519] (1.000)
Step: 22049, Reward: [-373.077 -373.077 -373.077] [0.0000], Avg: [-595.014 -595.014 -595.014] (1.000)
Step: 22099, Reward: [-500.977 -500.977 -500.977] [0.0000], Avg: [-594.801 -594.801 -594.801] (1.000)
Step: 22149, Reward: [-399.681 -399.681 -399.681] [0.0000], Avg: [-594.361 -594.361 -594.361] (1.000)
Step: 22199, Reward: [-378.524 -378.524 -378.524] [0.0000], Avg: [-593.875 -593.875 -593.875] (1.000)
Step: 22249, Reward: [-405.893 -405.893 -405.893] [0.0000], Avg: [-593.452 -593.452 -593.452] (1.000)
Step: 22299, Reward: [-393.052 -393.052 -393.052] [0.0000], Avg: [-593.003 -593.003 -593.003] (1.000)
Step: 22349, Reward: [-484.842 -484.842 -484.842] [0.0000], Avg: [-592.761 -592.761 -592.761] (1.000)
Step: 22399, Reward: [-497.077 -497.077 -497.077] [0.0000], Avg: [-592.548 -592.548 -592.548] (1.000)
Step: 22449, Reward: [-411.213 -411.213 -411.213] [0.0000], Avg: [-592.144 -592.144 -592.144] (1.000)
Step: 22499, Reward: [-407.026 -407.026 -407.026] [0.0000], Avg: [-591.732 -591.732 -591.732] (1.000)
Step: 22549, Reward: [-329.224 -329.224 -329.224] [0.0000], Avg: [-591.15 -591.15 -591.15] (1.000)
Step: 22599, Reward: [-500.53 -500.53 -500.53] [0.0000], Avg: [-590.95 -590.95 -590.95] (1.000)
Step: 22649, Reward: [-464.687 -464.687 -464.687] [0.0000], Avg: [-590.671 -590.671 -590.671] (1.000)
Step: 22699, Reward: [-571.923 -571.923 -571.923] [0.0000], Avg: [-590.63 -590.63 -590.63] (1.000)
Step: 22749, Reward: [-417.075 -417.075 -417.075] [0.0000], Avg: [-590.248 -590.248 -590.248] (1.000)
Step: 22799, Reward: [-525.25 -525.25 -525.25] [0.0000], Avg: [-590.106 -590.106 -590.106] (1.000)
Step: 22849, Reward: [-619.668 -619.668 -619.668] [0.0000], Avg: [-590.17 -590.17 -590.17] (1.000)
Step: 22899, Reward: [-576.834 -576.834 -576.834] [0.0000], Avg: [-590.141 -590.141 -590.141] (1.000)
Step: 22949, Reward: [-545.487 -545.487 -545.487] [0.0000], Avg: [-590.044 -590.044 -590.044] (1.000)
Step: 22999, Reward: [-453.766 -453.766 -453.766] [0.0000], Avg: [-589.748 -589.748 -589.748] (1.000)
Step: 23049, Reward: [-654.117 -654.117 -654.117] [0.0000], Avg: [-589.887 -589.887 -589.887] (1.000)
Step: 23099, Reward: [-342.703 -342.703 -342.703] [0.0000], Avg: [-589.352 -589.352 -589.352] (1.000)
Step: 23149, Reward: [-569.68 -569.68 -569.68] [0.0000], Avg: [-589.31 -589.31 -589.31] (1.000)
Step: 23199, Reward: [-351.972 -351.972 -351.972] [0.0000], Avg: [-588.798 -588.798 -588.798] (1.000)
Step: 23249, Reward: [-365.006 -365.006 -365.006] [0.0000], Avg: [-588.317 -588.317 -588.317] (1.000)
Step: 23299, Reward: [-463.085 -463.085 -463.085] [0.0000], Avg: [-588.048 -588.048 -588.048] (1.000)
Step: 23349, Reward: [-438.969 -438.969 -438.969] [0.0000], Avg: [-587.729 -587.729 -587.729] (1.000)
Step: 23399, Reward: [-605.244 -605.244 -605.244] [0.0000], Avg: [-587.767 -587.767 -587.767] (1.000)
Step: 23449, Reward: [-629.146 -629.146 -629.146] [0.0000], Avg: [-587.855 -587.855 -587.855] (1.000)
Step: 23499, Reward: [-793.081 -793.081 -793.081] [0.0000], Avg: [-588.291 -588.291 -588.291] (1.000)
Step: 23549, Reward: [-711.456 -711.456 -711.456] [0.0000], Avg: [-588.553 -588.553 -588.553] (1.000)
Step: 23599, Reward: [-509.149 -509.149 -509.149] [0.0000], Avg: [-588.385 -588.385 -588.385] (1.000)
Step: 23649, Reward: [-576.261 -576.261 -576.261] [0.0000], Avg: [-588.359 -588.359 -588.359] (1.000)
Step: 23699, Reward: [-673.295 -673.295 -673.295] [0.0000], Avg: [-588.538 -588.538 -588.538] (1.000)
Step: 23749, Reward: [-809.156 -809.156 -809.156] [0.0000], Avg: [-589.003 -589.003 -589.003] (1.000)
Step: 23799, Reward: [-868.412 -868.412 -868.412] [0.0000], Avg: [-589.59 -589.59 -589.59] (1.000)
Step: 23849, Reward: [-712.818 -712.818 -712.818] [0.0000], Avg: [-589.848 -589.848 -589.848] (1.000)
Step: 23899, Reward: [-857.375 -857.375 -857.375] [0.0000], Avg: [-590.408 -590.408 -590.408] (1.000)
Step: 23949, Reward: [-670.334 -670.334 -670.334] [0.0000], Avg: [-590.575 -590.575 -590.575] (1.000)
Step: 23999, Reward: [-1101.824 -1101.824 -1101.824] [0.0000], Avg: [-591.64 -591.64 -591.64] (1.000)
Step: 24049, Reward: [-660.347 -660.347 -660.347] [0.0000], Avg: [-591.783 -591.783 -591.783] (1.000)
Step: 24099, Reward: [-606.153 -606.153 -606.153] [0.0000], Avg: [-591.812 -591.812 -591.812] (1.000)
Step: 24149, Reward: [-581.332 -581.332 -581.332] [0.0000], Avg: [-591.791 -591.791 -591.791] (1.000)
Step: 24199, Reward: [-648.025 -648.025 -648.025] [0.0000], Avg: [-591.907 -591.907 -591.907] (1.000)
Step: 24249, Reward: [-886.747 -886.747 -886.747] [0.0000], Avg: [-592.515 -592.515 -592.515] (1.000)
Step: 24299, Reward: [-910.625 -910.625 -910.625] [0.0000], Avg: [-593.169 -593.169 -593.169] (1.000)
Step: 24349, Reward: [-825.668 -825.668 -825.668] [0.0000], Avg: [-593.647 -593.647 -593.647] (1.000)
Step: 24399, Reward: [-766.954 -766.954 -766.954] [0.0000], Avg: [-594.002 -594.002 -594.002] (1.000)
Step: 24449, Reward: [-597.123 -597.123 -597.123] [0.0000], Avg: [-594.008 -594.008 -594.008] (1.000)
Step: 24499, Reward: [-1101.594 -1101.594 -1101.594] [0.0000], Avg: [-595.044 -595.044 -595.044] (1.000)
Step: 24549, Reward: [-582.293 -582.293 -582.293] [0.0000], Avg: [-595.018 -595.018 -595.018] (1.000)
Step: 24599, Reward: [-783.202 -783.202 -783.202] [0.0000], Avg: [-595.401 -595.401 -595.401] (1.000)
Step: 24649, Reward: [-721.822 -721.822 -721.822] [0.0000], Avg: [-595.657 -595.657 -595.657] (1.000)
Step: 24699, Reward: [-580.595 -580.595 -580.595] [0.0000], Avg: [-595.627 -595.627 -595.627] (1.000)
Step: 24749, Reward: [-563.588 -563.588 -563.588] [0.0000], Avg: [-595.562 -595.562 -595.562] (1.000)
Step: 24799, Reward: [-689.495 -689.495 -689.495] [0.0000], Avg: [-595.751 -595.751 -595.751] (1.000)
Step: 24849, Reward: [-566.768 -566.768 -566.768] [0.0000], Avg: [-595.693 -595.693 -595.693] (1.000)
Step: 24899, Reward: [-1007.509 -1007.509 -1007.509] [0.0000], Avg: [-596.52 -596.52 -596.52] (1.000)
Step: 24949, Reward: [-651.42 -651.42 -651.42] [0.0000], Avg: [-596.63 -596.63 -596.63] (1.000)
Step: 24999, Reward: [-534.444 -534.444 -534.444] [0.0000], Avg: [-596.506 -596.506 -596.506] (1.000)
Step: 25049, Reward: [-838.899 -838.899 -838.899] [0.0000], Avg: [-596.989 -596.989 -596.989] (1.000)
Step: 25099, Reward: [-725.042 -725.042 -725.042] [0.0000], Avg: [-597.244 -597.244 -597.244] (1.000)
Step: 25149, Reward: [-423.191 -423.191 -423.191] [0.0000], Avg: [-596.898 -596.898 -596.898] (1.000)
Step: 25199, Reward: [-647.215 -647.215 -647.215] [0.0000], Avg: [-596.998 -596.998 -596.998] (1.000)
Step: 25249, Reward: [-584.361 -584.361 -584.361] [0.0000], Avg: [-596.973 -596.973 -596.973] (1.000)
Step: 25299, Reward: [-621.206 -621.206 -621.206] [0.0000], Avg: [-597.021 -597.021 -597.021] (1.000)
Step: 25349, Reward: [-643.43 -643.43 -643.43] [0.0000], Avg: [-597.113 -597.113 -597.113] (1.000)
Step: 25399, Reward: [-737.433 -737.433 -737.433] [0.0000], Avg: [-597.389 -597.389 -597.389] (1.000)
Step: 25449, Reward: [-465.99 -465.99 -465.99] [0.0000], Avg: [-597.131 -597.131 -597.131] (1.000)
Step: 25499, Reward: [-437.482 -437.482 -437.482] [0.0000], Avg: [-596.818 -596.818 -596.818] (1.000)
Step: 25549, Reward: [-786.814 -786.814 -786.814] [0.0000], Avg: [-597.189 -597.189 -597.189] (1.000)
Step: 25599, Reward: [-648.202 -648.202 -648.202] [0.0000], Avg: [-597.289 -597.289 -597.289] (1.000)
Step: 25649, Reward: [-672.364 -672.364 -672.364] [0.0000], Avg: [-597.435 -597.435 -597.435] (1.000)
Step: 25699, Reward: [-1119.175 -1119.175 -1119.175] [0.0000], Avg: [-598.451 -598.451 -598.451] (1.000)
Step: 25749, Reward: [-665.521 -665.521 -665.521] [0.0000], Avg: [-598.581 -598.581 -598.581] (1.000)
Step: 25799, Reward: [-424.766 -424.766 -424.766] [0.0000], Avg: [-598.244 -598.244 -598.244] (1.000)
Step: 25849, Reward: [-629.455 -629.455 -629.455] [0.0000], Avg: [-598.304 -598.304 -598.304] (1.000)
Step: 25899, Reward: [-822.551 -822.551 -822.551] [0.0000], Avg: [-598.737 -598.737 -598.737] (1.000)
Step: 25949, Reward: [-517.767 -517.767 -517.767] [0.0000], Avg: [-598.581 -598.581 -598.581] (1.000)
Step: 25999, Reward: [-511.225 -511.225 -511.225] [0.0000], Avg: [-598.413 -598.413 -598.413] (1.000)
Step: 26049, Reward: [-681.117 -681.117 -681.117] [0.0000], Avg: [-598.572 -598.572 -598.572] (1.000)
Step: 26099, Reward: [-754.531 -754.531 -754.531] [0.0000], Avg: [-598.871 -598.871 -598.871] (1.000)
Step: 26149, Reward: [-701.486 -701.486 -701.486] [0.0000], Avg: [-599.067 -599.067 -599.067] (1.000)
Step: 26199, Reward: [-704.459 -704.459 -704.459] [0.0000], Avg: [-599.268 -599.268 -599.268] (1.000)
Step: 26249, Reward: [-626.64 -626.64 -626.64] [0.0000], Avg: [-599.32 -599.32 -599.32] (1.000)
Step: 26299, Reward: [-597.391 -597.391 -597.391] [0.0000], Avg: [-599.317 -599.317 -599.317] (1.000)
Step: 26349, Reward: [-651.184 -651.184 -651.184] [0.0000], Avg: [-599.415 -599.415 -599.415] (1.000)
Step: 26399, Reward: [-613.162 -613.162 -613.162] [0.0000], Avg: [-599.441 -599.441 -599.441] (1.000)
Step: 26449, Reward: [-704.848 -704.848 -704.848] [0.0000], Avg: [-599.64 -599.64 -599.64] (1.000)
Step: 26499, Reward: [-500.328 -500.328 -500.328] [0.0000], Avg: [-599.453 -599.453 -599.453] (1.000)
Step: 26549, Reward: [-651.661 -651.661 -651.661] [0.0000], Avg: [-599.551 -599.551 -599.551] (1.000)
Step: 26599, Reward: [-644.564 -644.564 -644.564] [0.0000], Avg: [-599.636 -599.636 -599.636] (1.000)
Step: 26649, Reward: [-551.733 -551.733 -551.733] [0.0000], Avg: [-599.546 -599.546 -599.546] (1.000)
Step: 26699, Reward: [-617.249 -617.249 -617.249] [0.0000], Avg: [-599.579 -599.579 -599.579] (1.000)
Step: 26749, Reward: [-542.012 -542.012 -542.012] [0.0000], Avg: [-599.471 -599.471 -599.471] (1.000)
Step: 26799, Reward: [-616.877 -616.877 -616.877] [0.0000], Avg: [-599.504 -599.504 -599.504] (1.000)
Step: 26849, Reward: [-562.493 -562.493 -562.493] [0.0000], Avg: [-599.435 -599.435 -599.435] (1.000)
Step: 26899, Reward: [-691.344 -691.344 -691.344] [0.0000], Avg: [-599.606 -599.606 -599.606] (1.000)
Step: 26949, Reward: [-570.444 -570.444 -570.444] [0.0000], Avg: [-599.552 -599.552 -599.552] (1.000)
Step: 26999, Reward: [-608.158 -608.158 -608.158] [0.0000], Avg: [-599.568 -599.568 -599.568] (1.000)
Step: 27049, Reward: [-680.417 -680.417 -680.417] [0.0000], Avg: [-599.717 -599.717 -599.717] (1.000)
Step: 27099, Reward: [-585.116 -585.116 -585.116] [0.0000], Avg: [-599.69 -599.69 -599.69] (1.000)
Step: 27149, Reward: [-684.17 -684.17 -684.17] [0.0000], Avg: [-599.846 -599.846 -599.846] (1.000)
Step: 27199, Reward: [-432.23 -432.23 -432.23] [0.0000], Avg: [-599.538 -599.538 -599.538] (1.000)
Step: 27249, Reward: [-382.266 -382.266 -382.266] [0.0000], Avg: [-599.139 -599.139 -599.139] (1.000)
Step: 27299, Reward: [-568.628 -568.628 -568.628] [0.0000], Avg: [-599.083 -599.083 -599.083] (1.000)
Step: 27349, Reward: [-332.434 -332.434 -332.434] [0.0000], Avg: [-598.596 -598.596 -598.596] (1.000)
Step: 27399, Reward: [-506.33 -506.33 -506.33] [0.0000], Avg: [-598.427 -598.427 -598.427] (1.000)
Step: 27449, Reward: [-531.405 -531.405 -531.405] [0.0000], Avg: [-598.305 -598.305 -598.305] (1.000)
Step: 27499, Reward: [-482.036 -482.036 -482.036] [0.0000], Avg: [-598.094 -598.094 -598.094] (1.000)
Step: 27549, Reward: [-709.08 -709.08 -709.08] [0.0000], Avg: [-598.295 -598.295 -598.295] (1.000)
Step: 27599, Reward: [-514.364 -514.364 -514.364] [0.0000], Avg: [-598.143 -598.143 -598.143] (1.000)
Step: 27649, Reward: [-694.829 -694.829 -694.829] [0.0000], Avg: [-598.318 -598.318 -598.318] (1.000)
Step: 27699, Reward: [-779.339 -779.339 -779.339] [0.0000], Avg: [-598.645 -598.645 -598.645] (1.000)
Step: 27749, Reward: [-648.904 -648.904 -648.904] [0.0000], Avg: [-598.735 -598.735 -598.735] (1.000)
Step: 27799, Reward: [-601.184 -601.184 -601.184] [0.0000], Avg: [-598.74 -598.74 -598.74] (1.000)
Step: 27849, Reward: [-434.821 -434.821 -434.821] [0.0000], Avg: [-598.445 -598.445 -598.445] (1.000)
Step: 27899, Reward: [-644.805 -644.805 -644.805] [0.0000], Avg: [-598.528 -598.528 -598.528] (1.000)
Step: 27949, Reward: [-906.934 -906.934 -906.934] [0.0000], Avg: [-599.08 -599.08 -599.08] (1.000)
Step: 27999, Reward: [-854.822 -854.822 -854.822] [0.0000], Avg: [-599.537 -599.537 -599.537] (1.000)
Step: 28049, Reward: [-1306.873 -1306.873 -1306.873] [0.0000], Avg: [-600.798 -600.798 -600.798] (1.000)
Step: 28099, Reward: [-991.584 -991.584 -991.584] [0.0000], Avg: [-601.493 -601.493 -601.493] (1.000)
Step: 28149, Reward: [-1051.164 -1051.164 -1051.164] [0.0000], Avg: [-602.292 -602.292 -602.292] (1.000)
Step: 28199, Reward: [-926.755 -926.755 -926.755] [0.0000], Avg: [-602.867 -602.867 -602.867] (1.000)
Step: 28249, Reward: [-1162.693 -1162.693 -1162.693] [0.0000], Avg: [-603.858 -603.858 -603.858] (1.000)
Step: 28299, Reward: [-912.209 -912.209 -912.209] [0.0000], Avg: [-604.403 -604.403 -604.403] (1.000)
Step: 28349, Reward: [-509.039 -509.039 -509.039] [0.0000], Avg: [-604.235 -604.235 -604.235] (1.000)
Step: 28399, Reward: [-759.242 -759.242 -759.242] [0.0000], Avg: [-604.507 -604.507 -604.507] (1.000)
Step: 28449, Reward: [-803.718 -803.718 -803.718] [0.0000], Avg: [-604.858 -604.858 -604.858] (1.000)
Step: 28499, Reward: [-715.65 -715.65 -715.65] [0.0000], Avg: [-605.052 -605.052 -605.052] (1.000)
Step: 28549, Reward: [-977.923 -977.923 -977.923] [0.0000], Avg: [-605.705 -605.705 -605.705] (1.000)
Step: 28599, Reward: [-645.91 -645.91 -645.91] [0.0000], Avg: [-605.775 -605.775 -605.775] (1.000)
Step: 28649, Reward: [-527.144 -527.144 -527.144] [0.0000], Avg: [-605.638 -605.638 -605.638] (1.000)
Step: 28699, Reward: [-751.563 -751.563 -751.563] [0.0000], Avg: [-605.892 -605.892 -605.892] (1.000)
Step: 28749, Reward: [-569.081 -569.081 -569.081] [0.0000], Avg: [-605.828 -605.828 -605.828] (1.000)
Step: 28799, Reward: [-854.612 -854.612 -854.612] [0.0000], Avg: [-606.26 -606.26 -606.26] (1.000)
Step: 28849, Reward: [-678.705 -678.705 -678.705] [0.0000], Avg: [-606.386 -606.386 -606.386] (1.000)
Step: 28899, Reward: [-785.354 -785.354 -785.354] [0.0000], Avg: [-606.695 -606.695 -606.695] (1.000)
Step: 28949, Reward: [-721.323 -721.323 -721.323] [0.0000], Avg: [-606.893 -606.893 -606.893] (1.000)
Step: 28999, Reward: [-601.024 -601.024 -601.024] [0.0000], Avg: [-606.883 -606.883 -606.883] (1.000)
Step: 29049, Reward: [-715.432 -715.432 -715.432] [0.0000], Avg: [-607.07 -607.07 -607.07] (1.000)
Step: 29099, Reward: [-690.231 -690.231 -690.231] [0.0000], Avg: [-607.213 -607.213 -607.213] (1.000)
Step: 29149, Reward: [-720.102 -720.102 -720.102] [0.0000], Avg: [-607.406 -607.406 -607.406] (1.000)
Step: 29199, Reward: [-390.732 -390.732 -390.732] [0.0000], Avg: [-607.035 -607.035 -607.035] (1.000)
Step: 29249, Reward: [-660.701 -660.701 -660.701] [0.0000], Avg: [-607.127 -607.127 -607.127] (1.000)
Step: 29299, Reward: [-646.295 -646.295 -646.295] [0.0000], Avg: [-607.194 -607.194 -607.194] (1.000)
Step: 29349, Reward: [-572.129 -572.129 -572.129] [0.0000], Avg: [-607.134 -607.134 -607.134] (1.000)
Step: 29399, Reward: [-667.027 -667.027 -667.027] [0.0000], Avg: [-607.236 -607.236 -607.236] (1.000)
Step: 29449, Reward: [-736.983 -736.983 -736.983] [0.0000], Avg: [-607.456 -607.456 -607.456] (1.000)
Step: 29499, Reward: [-653.12 -653.12 -653.12] [0.0000], Avg: [-607.534 -607.534 -607.534] (1.000)
Step: 29549, Reward: [-413.998 -413.998 -413.998] [0.0000], Avg: [-607.206 -607.206 -607.206] (1.000)
Step: 29599, Reward: [-597.47 -597.47 -597.47] [0.0000], Avg: [-607.19 -607.19 -607.19] (1.000)
Step: 29649, Reward: [-637.143 -637.143 -637.143] [0.0000], Avg: [-607.24 -607.24 -607.24] (1.000)
Step: 29699, Reward: [-822.074 -822.074 -822.074] [0.0000], Avg: [-607.602 -607.602 -607.602] (1.000)
Step: 29749, Reward: [-1093.343 -1093.343 -1093.343] [0.0000], Avg: [-608.418 -608.418 -608.418] (1.000)
Step: 29799, Reward: [-935.457 -935.457 -935.457] [0.0000], Avg: [-608.967 -608.967 -608.967] (1.000)
Step: 29849, Reward: [-842.574 -842.574 -842.574] [0.0000], Avg: [-609.359 -609.359 -609.359] (1.000)
Step: 29899, Reward: [-1003.291 -1003.291 -1003.291] [0.0000], Avg: [-610.017 -610.017 -610.017] (1.000)
Step: 29949, Reward: [-924.722 -924.722 -924.722] [0.0000], Avg: [-610.543 -610.543 -610.543] (1.000)
Step: 29999, Reward: [-1213.948 -1213.948 -1213.948] [0.0000], Avg: [-611.548 -611.548 -611.548] (1.000)
Step: 30049, Reward: [-1330.886 -1330.886 -1330.886] [0.0000], Avg: [-612.745 -612.745 -612.745] (1.000)
Step: 30099, Reward: [-1722.55 -1722.55 -1722.55] [0.0000], Avg: [-614.589 -614.589 -614.589] (1.000)
Step: 30149, Reward: [-1417.771 -1417.771 -1417.771] [0.0000], Avg: [-615.921 -615.921 -615.921] (1.000)
Step: 30199, Reward: [-1211.099 -1211.099 -1211.099] [0.0000], Avg: [-616.906 -616.906 -616.906] (1.000)
Step: 30249, Reward: [-1509.187 -1509.187 -1509.187] [0.0000], Avg: [-618.381 -618.381 -618.381] (1.000)
Step: 30299, Reward: [-1642.601 -1642.601 -1642.601] [0.0000], Avg: [-620.071 -620.071 -620.071] (1.000)
Step: 30349, Reward: [-1326.517 -1326.517 -1326.517] [0.0000], Avg: [-621.235 -621.235 -621.235] (1.000)
Step: 30399, Reward: [-1414.612 -1414.612 -1414.612] [0.0000], Avg: [-622.54 -622.54 -622.54] (1.000)
Step: 30449, Reward: [-1968.291 -1968.291 -1968.291] [0.0000], Avg: [-624.75 -624.75 -624.75] (1.000)
Step: 30499, Reward: [-1545.726 -1545.726 -1545.726] [0.0000], Avg: [-626.259 -626.259 -626.259] (1.000)
Step: 30549, Reward: [-1903.246 -1903.246 -1903.246] [0.0000], Avg: [-628.349 -628.349 -628.349] (1.000)
Step: 30599, Reward: [-1935.68 -1935.68 -1935.68] [0.0000], Avg: [-630.486 -630.486 -630.486] (1.000)
Step: 30649, Reward: [-1739.23 -1739.23 -1739.23] [0.0000], Avg: [-632.294 -632.294 -632.294] (1.000)
Step: 30699, Reward: [-1387.862 -1387.862 -1387.862] [0.0000], Avg: [-633.525 -633.525 -633.525] (1.000)
Step: 30749, Reward: [-1613.32 -1613.32 -1613.32] [0.0000], Avg: [-635.118 -635.118 -635.118] (1.000)
Step: 30799, Reward: [-898.163 -898.163 -898.163] [0.0000], Avg: [-635.545 -635.545 -635.545] (1.000)
Step: 30849, Reward: [-1662.662 -1662.662 -1662.662] [0.0000], Avg: [-637.21 -637.21 -637.21] (1.000)
Step: 30899, Reward: [-1782.814 -1782.814 -1782.814] [0.0000], Avg: [-639.063 -639.063 -639.063] (1.000)
Step: 30949, Reward: [-1620.242 -1620.242 -1620.242] [0.0000], Avg: [-640.649 -640.649 -640.649] (1.000)
Step: 30999, Reward: [-1145.212 -1145.212 -1145.212] [0.0000], Avg: [-641.462 -641.462 -641.462] (1.000)
Step: 31049, Reward: [-2133.852 -2133.852 -2133.852] [0.0000], Avg: [-643.866 -643.866 -643.866] (1.000)
Step: 31099, Reward: [-1297.713 -1297.713 -1297.713] [0.0000], Avg: [-644.917 -644.917 -644.917] (1.000)
Step: 31149, Reward: [-1379.758 -1379.758 -1379.758] [0.0000], Avg: [-646.096 -646.096 -646.096] (1.000)
Step: 31199, Reward: [-1173.586 -1173.586 -1173.586] [0.0000], Avg: [-646.942 -646.942 -646.942] (1.000)
Step: 31249, Reward: [-1424.082 -1424.082 -1424.082] [0.0000], Avg: [-648.185 -648.185 -648.185] (1.000)
Step: 31299, Reward: [-1003.735 -1003.735 -1003.735] [0.0000], Avg: [-648.753 -648.753 -648.753] (1.000)
Step: 31349, Reward: [-1154.059 -1154.059 -1154.059] [0.0000], Avg: [-649.559 -649.559 -649.559] (1.000)
Step: 31399, Reward: [-927.206 -927.206 -927.206] [0.0000], Avg: [-650.001 -650.001 -650.001] (1.000)
Step: 31449, Reward: [-743.625 -743.625 -743.625] [0.0000], Avg: [-650.15 -650.15 -650.15] (1.000)
Step: 31499, Reward: [-738.611 -738.611 -738.611] [0.0000], Avg: [-650.29 -650.29 -650.29] (1.000)
Step: 31549, Reward: [-714.055 -714.055 -714.055] [0.0000], Avg: [-650.391 -650.391 -650.391] (1.000)
Step: 31599, Reward: [-752.261 -752.261 -752.261] [0.0000], Avg: [-650.553 -650.553 -650.553] (1.000)
Step: 31649, Reward: [-696.3 -696.3 -696.3] [0.0000], Avg: [-650.625 -650.625 -650.625] (1.000)
Step: 31699, Reward: [-555.362 -555.362 -555.362] [0.0000], Avg: [-650.475 -650.475 -650.475] (1.000)
Step: 31749, Reward: [-1158.449 -1158.449 -1158.449] [0.0000], Avg: [-651.275 -651.275 -651.275] (1.000)
Step: 31799, Reward: [-925.773 -925.773 -925.773] [0.0000], Avg: [-651.706 -651.706 -651.706] (1.000)
Step: 31849, Reward: [-1228.513 -1228.513 -1228.513] [0.0000], Avg: [-652.612 -652.612 -652.612] (1.000)
Step: 31899, Reward: [-1723.728 -1723.728 -1723.728] [0.0000], Avg: [-654.29 -654.29 -654.29] (1.000)
Step: 31949, Reward: [-1635.533 -1635.533 -1635.533] [0.0000], Avg: [-655.826 -655.826 -655.826] (1.000)
Step: 31999, Reward: [-1323.604 -1323.604 -1323.604] [0.0000], Avg: [-656.869 -656.869 -656.869] (1.000)
Step: 32049, Reward: [-1454.46 -1454.46 -1454.46] [0.0000], Avg: [-658.114 -658.114 -658.114] (1.000)
Step: 32099, Reward: [-1621.217 -1621.217 -1621.217] [0.0000], Avg: [-659.614 -659.614 -659.614] (1.000)
Step: 32149, Reward: [-1700.846 -1700.846 -1700.846] [0.0000], Avg: [-661.233 -661.233 -661.233] (1.000)
Step: 32199, Reward: [-1915.383 -1915.383 -1915.383] [0.0000], Avg: [-663.181 -663.181 -663.181] (1.000)
Step: 32249, Reward: [-1752.26 -1752.26 -1752.26] [0.0000], Avg: [-664.869 -664.869 -664.869] (1.000)
Step: 32299, Reward: [-1684.648 -1684.648 -1684.648] [0.0000], Avg: [-666.448 -666.448 -666.448] (1.000)
Step: 32349, Reward: [-1774.334 -1774.334 -1774.334] [0.0000], Avg: [-668.16 -668.16 -668.16] (1.000)
Step: 32399, Reward: [-1962.032 -1962.032 -1962.032] [0.0000], Avg: [-670.157 -670.157 -670.157] (1.000)
Step: 32449, Reward: [-1003.941 -1003.941 -1003.941] [0.0000], Avg: [-670.671 -670.671 -670.671] (1.000)
Step: 32499, Reward: [-2140.614 -2140.614 -2140.614] [0.0000], Avg: [-672.933 -672.933 -672.933] (1.000)
Step: 32549, Reward: [-1896.574 -1896.574 -1896.574] [0.0000], Avg: [-674.812 -674.812 -674.812] (1.000)
Step: 32599, Reward: [-1531.771 -1531.771 -1531.771] [0.0000], Avg: [-676.127 -676.127 -676.127] (1.000)
Step: 32649, Reward: [-1757.162 -1757.162 -1757.162] [0.0000], Avg: [-677.782 -677.782 -677.782] (1.000)
Step: 32699, Reward: [-1620.128 -1620.128 -1620.128] [0.0000], Avg: [-679.223 -679.223 -679.223] (1.000)
Step: 32749, Reward: [-1821.695 -1821.695 -1821.695] [0.0000], Avg: [-680.967 -680.967 -680.967] (1.000)
Step: 32799, Reward: [-2084.377 -2084.377 -2084.377] [0.0000], Avg: [-683.107 -683.107 -683.107] (1.000)
Step: 32849, Reward: [-1850.684 -1850.684 -1850.684] [0.0000], Avg: [-684.884 -684.884 -684.884] (1.000)
Step: 32899, Reward: [-1840.795 -1840.795 -1840.795] [0.0000], Avg: [-686.64 -686.64 -686.64] (1.000)
Step: 32949, Reward: [-1915.054 -1915.054 -1915.054] [0.0000], Avg: [-688.504 -688.504 -688.504] (1.000)
Step: 32999, Reward: [-2062.788 -2062.788 -2062.788] [0.0000], Avg: [-690.587 -690.587 -690.587] (1.000)
Step: 33049, Reward: [-2025.955 -2025.955 -2025.955] [0.0000], Avg: [-692.607 -692.607 -692.607] (1.000)
Step: 33099, Reward: [-1661.332 -1661.332 -1661.332] [0.0000], Avg: [-694.07 -694.07 -694.07] (1.000)
Step: 33149, Reward: [-1472.501 -1472.501 -1472.501] [0.0000], Avg: [-695.244 -695.244 -695.244] (1.000)
Step: 33199, Reward: [-1962.691 -1962.691 -1962.691] [0.0000], Avg: [-697.153 -697.153 -697.153] (1.000)
Step: 33249, Reward: [-2044.542 -2044.542 -2044.542] [0.0000], Avg: [-699.179 -699.179 -699.179] (1.000)
Step: 33299, Reward: [-1742.554 -1742.554 -1742.554] [0.0000], Avg: [-700.746 -700.746 -700.746] (1.000)
Step: 33349, Reward: [-1987.969 -1987.969 -1987.969] [0.0000], Avg: [-702.676 -702.676 -702.676] (1.000)
Step: 33399, Reward: [-1375.824 -1375.824 -1375.824] [0.0000], Avg: [-703.684 -703.684 -703.684] (1.000)
Step: 33449, Reward: [-1701.747 -1701.747 -1701.747] [0.0000], Avg: [-705.175 -705.175 -705.175] (1.000)
Step: 33499, Reward: [-1614.121 -1614.121 -1614.121] [0.0000], Avg: [-706.532 -706.532 -706.532] (1.000)
Step: 33549, Reward: [-1647.177 -1647.177 -1647.177] [0.0000], Avg: [-707.934 -707.934 -707.934] (1.000)
Step: 33599, Reward: [-1834.643 -1834.643 -1834.643] [0.0000], Avg: [-709.611 -709.611 -709.611] (1.000)
Step: 33649, Reward: [-1468.111 -1468.111 -1468.111] [0.0000], Avg: [-710.738 -710.738 -710.738] (1.000)
Step: 33699, Reward: [-1811.064 -1811.064 -1811.064] [0.0000], Avg: [-712.37 -712.37 -712.37] (1.000)
Step: 33749, Reward: [-1516.418 -1516.418 -1516.418] [0.0000], Avg: [-713.561 -713.561 -713.561] (1.000)
Step: 33799, Reward: [-1883.537 -1883.537 -1883.537] [0.0000], Avg: [-715.292 -715.292 -715.292] (1.000)
Step: 33849, Reward: [-1627.296 -1627.296 -1627.296] [0.0000], Avg: [-716.639 -716.639 -716.639] (1.000)
Step: 33899, Reward: [-1366.974 -1366.974 -1366.974] [0.0000], Avg: [-717.598 -717.598 -717.598] (1.000)
Step: 33949, Reward: [-1816.882 -1816.882 -1816.882] [0.0000], Avg: [-719.217 -719.217 -719.217] (1.000)
Step: 33999, Reward: [-1158.05 -1158.05 -1158.05] [0.0000], Avg: [-719.863 -719.863 -719.863] (1.000)
Step: 34049, Reward: [-1169.64 -1169.64 -1169.64] [0.0000], Avg: [-720.523 -720.523 -720.523] (1.000)
Step: 34099, Reward: [-1779.624 -1779.624 -1779.624] [0.0000], Avg: [-722.076 -722.076 -722.076] (1.000)
Step: 34149, Reward: [-1621.45 -1621.45 -1621.45] [0.0000], Avg: [-723.393 -723.393 -723.393] (1.000)
Step: 34199, Reward: [-1569.852 -1569.852 -1569.852] [0.0000], Avg: [-724.63 -724.63 -724.63] (1.000)
Step: 34249, Reward: [-1651.263 -1651.263 -1651.263] [0.0000], Avg: [-725.983 -725.983 -725.983] (1.000)
Step: 34299, Reward: [-1624.083 -1624.083 -1624.083] [0.0000], Avg: [-727.292 -727.292 -727.292] (1.000)
Step: 34349, Reward: [-1531.95 -1531.95 -1531.95] [0.0000], Avg: [-728.464 -728.464 -728.464] (1.000)
Step: 34399, Reward: [-1875.583 -1875.583 -1875.583] [0.0000], Avg: [-730.131 -730.131 -730.131] (1.000)
Step: 34449, Reward: [-1201.472 -1201.472 -1201.472] [0.0000], Avg: [-730.815 -730.815 -730.815] (1.000)
Step: 34499, Reward: [-1553.025 -1553.025 -1553.025] [0.0000], Avg: [-732.007 -732.007 -732.007] (1.000)
Step: 34549, Reward: [-933.434 -933.434 -933.434] [0.0000], Avg: [-732.298 -732.298 -732.298] (1.000)
Step: 34599, Reward: [-1671.551 -1671.551 -1671.551] [0.0000], Avg: [-733.655 -733.655 -733.655] (1.000)
Step: 34649, Reward: [-1284.461 -1284.461 -1284.461] [0.0000], Avg: [-734.45 -734.45 -734.45] (1.000)
Step: 34699, Reward: [-1543.287 -1543.287 -1543.287] [0.0000], Avg: [-735.616 -735.616 -735.616] (1.000)
Step: 34749, Reward: [-1228.946 -1228.946 -1228.946] [0.0000], Avg: [-736.326 -736.326 -736.326] (1.000)
Step: 34799, Reward: [-1656.504 -1656.504 -1656.504] [0.0000], Avg: [-737.648 -737.648 -737.648] (1.000)
Step: 34849, Reward: [-1136.625 -1136.625 -1136.625] [0.0000], Avg: [-738.22 -738.22 -738.22] (1.000)
Step: 34899, Reward: [-558.354 -558.354 -558.354] [0.0000], Avg: [-737.962 -737.962 -737.962] (1.000)
Step: 34949, Reward: [-1218.362 -1218.362 -1218.362] [0.0000], Avg: [-738.65 -738.65 -738.65] (1.000)
Step: 34999, Reward: [-718.827 -718.827 -718.827] [0.0000], Avg: [-738.621 -738.621 -738.621] (1.000)
Step: 35049, Reward: [-506.288 -506.288 -506.288] [0.0000], Avg: [-738.29 -738.29 -738.29] (1.000)
Step: 35099, Reward: [-586.043 -586.043 -586.043] [0.0000], Avg: [-738.073 -738.073 -738.073] (1.000)
Step: 35149, Reward: [-602.347 -602.347 -602.347] [0.0000], Avg: [-737.88 -737.88 -737.88] (1.000)
Step: 35199, Reward: [-504.124 -504.124 -504.124] [0.0000], Avg: [-737.548 -737.548 -737.548] (1.000)
Step: 35249, Reward: [-563.54 -563.54 -563.54] [0.0000], Avg: [-737.301 -737.301 -737.301] (1.000)
Step: 35299, Reward: [-528.262 -528.262 -528.262] [0.0000], Avg: [-737.005 -737.005 -737.005] (1.000)
Step: 35349, Reward: [-622.985 -622.985 -622.985] [0.0000], Avg: [-736.844 -736.844 -736.844] (1.000)
Step: 35399, Reward: [-533.787 -533.787 -533.787] [0.0000], Avg: [-736.557 -736.557 -736.557] (1.000)
Step: 35449, Reward: [-731.681 -731.681 -731.681] [0.0000], Avg: [-736.55 -736.55 -736.55] (1.000)
Step: 35499, Reward: [-837.279 -837.279 -837.279] [0.0000], Avg: [-736.692 -736.692 -736.692] (1.000)
Step: 35549, Reward: [-476.025 -476.025 -476.025] [0.0000], Avg: [-736.325 -736.325 -736.325] (1.000)
Step: 35599, Reward: [-589.222 -589.222 -589.222] [0.0000], Avg: [-736.119 -736.119 -736.119] (1.000)
Step: 35649, Reward: [-376.092 -376.092 -376.092] [0.0000], Avg: [-735.614 -735.614 -735.614] (1.000)
Step: 35699, Reward: [-622.735 -622.735 -622.735] [0.0000], Avg: [-735.456 -735.456 -735.456] (1.000)
Step: 35749, Reward: [-545.36 -545.36 -545.36] [0.0000], Avg: [-735.19 -735.19 -735.19] (1.000)
Step: 35799, Reward: [-833.554 -833.554 -833.554] [0.0000], Avg: [-735.327 -735.327 -735.327] (1.000)
Step: 35849, Reward: [-941.41 -941.41 -941.41] [0.0000], Avg: [-735.615 -735.615 -735.615] (1.000)
Step: 35899, Reward: [-536.449 -536.449 -536.449] [0.0000], Avg: [-735.337 -735.337 -735.337] (1.000)
Step: 35949, Reward: [-496.07 -496.07 -496.07] [0.0000], Avg: [-735.004 -735.004 -735.004] (1.000)
Step: 35999, Reward: [-583.295 -583.295 -583.295] [0.0000], Avg: [-734.794 -734.794 -734.794] (1.000)
Step: 36049, Reward: [-442.846 -442.846 -442.846] [0.0000], Avg: [-734.389 -734.389 -734.389] (1.000)
Step: 36099, Reward: [-1204.859 -1204.859 -1204.859] [0.0000], Avg: [-735.04 -735.04 -735.04] (1.000)
Step: 36149, Reward: [-511.94 -511.94 -511.94] [0.0000], Avg: [-734.732 -734.732 -734.732] (1.000)
Step: 36199, Reward: [-472.483 -472.483 -472.483] [0.0000], Avg: [-734.37 -734.37 -734.37] (1.000)
Step: 36249, Reward: [-832.225 -832.225 -832.225] [0.0000], Avg: [-734.505 -734.505 -734.505] (1.000)
Step: 36299, Reward: [-894.161 -894.161 -894.161] [0.0000], Avg: [-734.724 -734.724 -734.724] (1.000)
Step: 36349, Reward: [-1287.885 -1287.885 -1287.885] [0.0000], Avg: [-735.485 -735.485 -735.485] (1.000)
Step: 36399, Reward: [-589.648 -589.648 -589.648] [0.0000], Avg: [-735.285 -735.285 -735.285] (1.000)
Step: 36449, Reward: [-1106.95 -1106.95 -1106.95] [0.0000], Avg: [-735.795 -735.795 -735.795] (1.000)
Step: 36499, Reward: [-1612.27 -1612.27 -1612.27] [0.0000], Avg: [-736.996 -736.996 -736.996] (1.000)
Step: 36549, Reward: [-1490.053 -1490.053 -1490.053] [0.0000], Avg: [-738.026 -738.026 -738.026] (1.000)
Step: 36599, Reward: [-1200.083 -1200.083 -1200.083] [0.0000], Avg: [-738.657 -738.657 -738.657] (1.000)
Step: 36649, Reward: [-1491.743 -1491.743 -1491.743] [0.0000], Avg: [-739.684 -739.684 -739.684] (1.000)
Step: 36699, Reward: [-1013.266 -1013.266 -1013.266] [0.0000], Avg: [-740.057 -740.057 -740.057] (1.000)
Step: 36749, Reward: [-862.829 -862.829 -862.829] [0.0000], Avg: [-740.224 -740.224 -740.224] (1.000)
Step: 36799, Reward: [-1347.154 -1347.154 -1347.154] [0.0000], Avg: [-741.049 -741.049 -741.049] (1.000)
Step: 36849, Reward: [-1234.473 -1234.473 -1234.473] [0.0000], Avg: [-741.718 -741.718 -741.718] (1.000)
Step: 36899, Reward: [-860.398 -860.398 -860.398] [0.0000], Avg: [-741.879 -741.879 -741.879] (1.000)
Step: 36949, Reward: [-1225.778 -1225.778 -1225.778] [0.0000], Avg: [-742.534 -742.534 -742.534] (1.000)
Step: 36999, Reward: [-1189.944 -1189.944 -1189.944] [0.0000], Avg: [-743.138 -743.138 -743.138] (1.000)
Step: 37049, Reward: [-763.372 -763.372 -763.372] [0.0000], Avg: [-743.166 -743.166 -743.166] (1.000)
Step: 37099, Reward: [-745.07 -745.07 -745.07] [0.0000], Avg: [-743.168 -743.168 -743.168] (1.000)
Step: 37149, Reward: [-1484.951 -1484.951 -1484.951] [0.0000], Avg: [-744.167 -744.167 -744.167] (1.000)
Step: 37199, Reward: [-1474.656 -1474.656 -1474.656] [0.0000], Avg: [-745.149 -745.149 -745.149] (1.000)
Step: 37249, Reward: [-946.25 -946.25 -946.25] [0.0000], Avg: [-745.418 -745.418 -745.418] (1.000)
Step: 37299, Reward: [-1240.356 -1240.356 -1240.356] [0.0000], Avg: [-746.082 -746.082 -746.082] (1.000)
Step: 37349, Reward: [-714.848 -714.848 -714.848] [0.0000], Avg: [-746.04 -746.04 -746.04] (1.000)
Step: 37399, Reward: [-1695.478 -1695.478 -1695.478] [0.0000], Avg: [-747.309 -747.309 -747.309] (1.000)
Step: 37449, Reward: [-1388.138 -1388.138 -1388.138] [0.0000], Avg: [-748.165 -748.165 -748.165] (1.000)
Step: 37499, Reward: [-983.651 -983.651 -983.651] [0.0000], Avg: [-748.479 -748.479 -748.479] (1.000)
Step: 37549, Reward: [-1598.255 -1598.255 -1598.255] [0.0000], Avg: [-749.61 -749.61 -749.61] (1.000)
Step: 37599, Reward: [-1693.551 -1693.551 -1693.551] [0.0000], Avg: [-750.866 -750.866 -750.866] (1.000)
Step: 37649, Reward: [-1791.946 -1791.946 -1791.946] [0.0000], Avg: [-752.248 -752.248 -752.248] (1.000)
Step: 37699, Reward: [-1652.731 -1652.731 -1652.731] [0.0000], Avg: [-753.443 -753.443 -753.443] (1.000)
Step: 37749, Reward: [-1660.149 -1660.149 -1660.149] [0.0000], Avg: [-754.644 -754.644 -754.644] (1.000)
Step: 37799, Reward: [-1729.096 -1729.096 -1729.096] [0.0000], Avg: [-755.932 -755.932 -755.932] (1.000)
Step: 37849, Reward: [-1947.598 -1947.598 -1947.598] [0.0000], Avg: [-757.507 -757.507 -757.507] (1.000)
Step: 37899, Reward: [-1412.098 -1412.098 -1412.098] [0.0000], Avg: [-758.37 -758.37 -758.37] (1.000)
Step: 37949, Reward: [-1672.284 -1672.284 -1672.284] [0.0000], Avg: [-759.574 -759.574 -759.574] (1.000)
Step: 37999, Reward: [-1769.239 -1769.239 -1769.239] [0.0000], Avg: [-760.903 -760.903 -760.903] (1.000)
Step: 38049, Reward: [-1803.263 -1803.263 -1803.263] [0.0000], Avg: [-762.273 -762.273 -762.273] (1.000)
Step: 38099, Reward: [-2099.326 -2099.326 -2099.326] [0.0000], Avg: [-764.027 -764.027 -764.027] (1.000)
Step: 38149, Reward: [-1969.363 -1969.363 -1969.363] [0.0000], Avg: [-765.607 -765.607 -765.607] (1.000)
Step: 38199, Reward: [-1909.928 -1909.928 -1909.928] [0.0000], Avg: [-767.105 -767.105 -767.105] (1.000)
Step: 38249, Reward: [-1368.282 -1368.282 -1368.282] [0.0000], Avg: [-767.891 -767.891 -767.891] (1.000)
Step: 38299, Reward: [-694.16 -694.16 -694.16] [0.0000], Avg: [-767.794 -767.794 -767.794] (1.000)
Step: 38349, Reward: [-2046.233 -2046.233 -2046.233] [0.0000], Avg: [-769.461 -769.461 -769.461] (1.000)
Step: 38399, Reward: [-743.353 -743.353 -743.353] [0.0000], Avg: [-769.427 -769.427 -769.427] (1.000)
Step: 38449, Reward: [-786.6 -786.6 -786.6] [0.0000], Avg: [-769.45 -769.45 -769.45] (1.000)
Step: 38499, Reward: [-1805.853 -1805.853 -1805.853] [0.0000], Avg: [-770.795 -770.795 -770.795] (1.000)
Step: 38549, Reward: [-1294.689 -1294.689 -1294.689] [0.0000], Avg: [-771.475 -771.475 -771.475] (1.000)
