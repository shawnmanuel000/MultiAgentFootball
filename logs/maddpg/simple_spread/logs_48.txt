Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7faa326ad320>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7faa326ad3c8>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7faa326ad438>],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		action = gumbel_softmax(action_mu, hard=True)
		action = action.view(*out_dims, -1)
		return action
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state) if state.shape[0]>1 else state
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else other.get_action(states[j], numpy=False) for j,other in enumerate(self.models)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -q_actions.mean() + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		agent_init_params = []
		for acsp, obsp in zip(action_size, state_size):
			num_in_pol = obsp[-1]
			num_out_pol = acsp[-1]
			num_in_critic = 0
			for oobsp in state_size:
				num_in_critic += oobsp[-1]
			for oacsp in action_size:
				num_in_critic += oacsp[-1]
			agent_init_params.append({'num_in_pol': num_in_pol, 'num_out_pol': num_out_pol, 'num_in_critic': num_in_critic})
		self.agent = MADDPG(agent_init_params, ["MADDPG"] * len(state_size))
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, self.agent.nagents, [obsp[-1] for obsp in state_size], [acsp[-1] for acsp in action_size])

	def get_action(self, state, eps=None, sample=True, numpy=True):
		state = [torch.autograd.Variable(torch.Tensor(np.vstack(state[i])), requires_grad=False) for i in range(self.agent.nagents)]
		torch_agent_actions = self.agent.step(state, explore=True)
		agent_actions = [ac.data.numpy() for ac in torch_agent_actions]
		return agent_actions
		# eps = self.eps if eps is None else eps
		# action_random = super().get_action(state)
		# action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		# action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		# return action

	def train(self, state, action, next_state, reward, done):
		if not hasattr(self, "t"): self.t = 0
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (len(self.replay_buffer) >= 1024 and (self.t % 100)==0):
			self.agent.prep_training(device='cpu')
			for a_i in range(self.agent.nagents):
				sample = self.replay_buffer.sample(1024, to_gpu=False)
				self.agent.update(sample, a_i)
			self.agent.update_all_targets()
			self.agent.prep_rollouts(device='cpu')
		self.t += 1
		# self.buffer.append((state, action, reward, done))
		# if np.any(done[0]) or len(self.buffer) >= self.update_freq:
		# 	states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
		# 	self.buffer.clear()
		# 	next_state = self.to_tensor(next_state)
		# 	states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
		# 	actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
		# 	states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
		# 	actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
		# 	q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
		# 	q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
		# 	to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
		# 	states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
		# 	states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
		# 	self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		# if len(self.replay_buffer) > REPLAY_BATCH_SIZE:
		# 	states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
		# 	self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		# if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

MSELoss = torch.nn.MSELoss()


class MADDPG():
	"""
	Wrapper class for DDPG-esque (i.e. also MADDPG) agents in multi-agent task
	"""
	def __init__(self, agent_init_params, alg_types, gamma=0.95, tau=0.01, lr=0.01, hidden_dim=64, discrete_action=True):
		self.nagents = len(alg_types)
		self.alg_types = alg_types
		self.agents = [DDPGAgent(lr=lr, discrete_action=discrete_action, hidden_dim=hidden_dim, **params) for params in agent_init_params]
		self.agent_init_params = agent_init_params
		self.gamma = gamma
		self.tau = tau
		self.lr = lr
		self.pol_dev = 'cpu'  # device for policies
		self.critic_dev = 'cpu'  # device for critics
		self.trgt_pol_dev = 'cpu'  # device for target policies
		self.trgt_critic_dev = 'cpu'  # device for target critics
		self.niter = 0

	@property
	def policies(self):
		return [a.policy for a in self.agents]

	@property
	def target_policies(self):
		return [a.target_policy for a in self.agents]

	def step(self, observations, explore=False):
		return [a.step(obs, explore=explore) for a, obs in zip(self.agents, observations)]

	def update(self, sample, agent_i, parallel=False, logger=None):
		obs, acs, rews, next_obs, dones = sample
		curr_agent = self.agents[agent_i]

		curr_agent.critic_optimizer.zero_grad()
		all_trgt_acs = [one_hot(pi(nobs)) for pi, nobs in zip(self.target_policies, next_obs)]
		trgt_vf_in = torch.cat((*next_obs, *all_trgt_acs), dim=1)
		target_value = (rews[agent_i].view(-1, 1) + self.gamma * curr_agent.target_critic(trgt_vf_in) * (1 - dones[agent_i].view(-1, 1)))

		vf_in = torch.cat((*obs, *acs), dim=1)
		actual_value = curr_agent.critic(vf_in)
		vf_loss = (actual_value - target_value.detach()).pow(2).mean()
		vf_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.critic.parameters(), 0.5)
		curr_agent.critic_optimizer.step()

		curr_agent.policy_optimizer.zero_grad()
		curr_pol_out = curr_agent.policy(obs[agent_i])
		curr_pol_vf_in = gumbel_softmax(curr_pol_out, hard=True)
		all_pol_acs = [curr_pol_vf_in if i==agent_i else one_hot(pi(ob)) for i, pi, ob in zip(range(self.nagents), self.policies, obs)]
		vf_in = torch.cat((*obs, *all_pol_acs), dim=1)
		pol_loss = -curr_agent.critic(vf_in).mean()
		pol_loss += (curr_pol_out**2).mean() * 1e-3
		pol_loss.backward()
		torch.nn.utils.clip_grad_norm(curr_agent.policy.parameters(), 0.5)
		curr_agent.policy_optimizer.step()

	def update_all_targets(self):
		for a in self.agents:
			for target_param, param in zip(a.target_critic.parameters(), a.critic.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
			for target_param, param in zip(a.target_policy.parameters(), a.policy.parameters()):
				target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)
		self.niter += 1

	def prep_training(self, device='gpu'):
		for a in self.agents:
			a.policy.train()
			a.critic.train()
			a.target_policy.train()
			a.target_critic.train()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device
		if not self.critic_dev == device:
			for a in self.agents:
				a.critic = fn(a.critic)
			self.critic_dev = device
		if not self.trgt_pol_dev == device:
			for a in self.agents:
				a.target_policy = fn(a.target_policy)
			self.trgt_pol_dev = device
		if not self.trgt_critic_dev == device:
			for a in self.agents:
				a.target_critic = fn(a.target_critic)
			self.trgt_critic_dev = device

	def prep_rollouts(self, device='cpu'):
		for a in self.agents:
			a.policy.eval()
		if device == 'gpu':
			fn = lambda x: x.cuda()
		else:
			fn = lambda x: x.cpu()
		# only need main policy for rollouts
		if not self.pol_dev == device:
			for a in self.agents:
				a.policy = fn(a.policy)
			self.pol_dev = device

class DDPGAgent(object):
	def __init__(self, num_in_pol, num_out_pol, num_in_critic, hidden_dim=64, lr=0.01, discrete_action=True):
		self.policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim, constrain_out=True, discrete_action=discrete_action)
		self.critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim, constrain_out=False)
		self.target_policy = MLPNetwork(num_in_pol, num_out_pol,hidden_dim=hidden_dim, constrain_out=True, discrete_action=discrete_action)
		self.target_critic = MLPNetwork(num_in_critic, 1,hidden_dim=hidden_dim, constrain_out=False)
		for target_param, param in zip(self.target_policy.parameters(), self.policy.parameters()):
			target_param.data.copy_(param.data)
		for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):
			target_param.data.copy_(param.data)
		self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)
		self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)

	def step(self, obs, explore=False):
		action = self.policy(obs)
		if explore:
			action = gumbel_softmax(action, hard=True)
		else:
			action = one_hot(action)
		return action

class MLPNetwork(torch.nn.Module):
	def __init__(self, input_dim, out_dim, hidden_dim=64, nonlin=torch.relu, constrain_out=False, norm_in=False, discrete_action=True):
		super(MLPNetwork, self).__init__()

		if norm_in:  # normalize inputs
			self.in_fn = nn.BatchNorm1d(input_dim)
			self.in_fn.weight.data.fill_(1)
			self.in_fn.bias.data.fill_(0)
		else:
			self.in_fn = lambda x: x
		self.fc1 = torch.nn.Linear(input_dim, hidden_dim)
		self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
		self.fc3 = torch.nn.Linear(hidden_dim, out_dim)
		self.nonlin = nonlin
		if constrain_out and not discrete_action:
			self.fc3.weight.data.uniform_(-3e-3, 3e-3)
			self.out_fn = torch.tanh
			raise EnvironmentError()
		else:  # logits for discrete action (will softmax later)
			self.out_fn = lambda x: x

	def forward(self, X):
		h1 = self.nonlin(self.fc1(self.in_fn(X)))
		h2 = self.nonlin(self.fc2(h1))
		out = self.out_fn(self.fc3(h2))
		return out
REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.900             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)
# np.random.seed(1)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-410.402 -410.402 -410.402] [0.0000], Avg: [-410.402 -410.402 -410.402] (1.000)
Step: 99, Reward: [-464.065 -464.065 -464.065] [0.0000], Avg: [-437.234 -437.234 -437.234] (1.000)
Step: 149, Reward: [-640.495 -640.495 -640.495] [0.0000], Avg: [-504.987 -504.987 -504.987] (1.000)
Step: 199, Reward: [-602.42 -602.42 -602.42] [0.0000], Avg: [-529.346 -529.346 -529.346] (1.000)
Step: 249, Reward: [-500.329 -500.329 -500.329] [0.0000], Avg: [-523.542 -523.542 -523.542] (1.000)
Step: 299, Reward: [-467.475 -467.475 -467.475] [0.0000], Avg: [-514.198 -514.198 -514.198] (1.000)
Step: 349, Reward: [-636.641 -636.641 -636.641] [0.0000], Avg: [-531.69 -531.69 -531.69] (1.000)
Step: 399, Reward: [-371.535 -371.535 -371.535] [0.0000], Avg: [-511.67 -511.67 -511.67] (1.000)
Step: 449, Reward: [-458.018 -458.018 -458.018] [0.0000], Avg: [-505.709 -505.709 -505.709] (1.000)
Step: 499, Reward: [-679.649 -679.649 -679.649] [0.0000], Avg: [-523.103 -523.103 -523.103] (1.000)
Step: 549, Reward: [-442.248 -442.248 -442.248] [0.0000], Avg: [-515.753 -515.753 -515.753] (1.000)
Step: 599, Reward: [-422.338 -422.338 -422.338] [0.0000], Avg: [-507.968 -507.968 -507.968] (1.000)
Step: 649, Reward: [-492.482 -492.482 -492.482] [0.0000], Avg: [-506.777 -506.777 -506.777] (1.000)
Step: 699, Reward: [-437.041 -437.041 -437.041] [0.0000], Avg: [-501.796 -501.796 -501.796] (1.000)
Step: 749, Reward: [-482.519 -482.519 -482.519] [0.0000], Avg: [-500.511 -500.511 -500.511] (1.000)
Step: 799, Reward: [-549.229 -549.229 -549.229] [0.0000], Avg: [-503.555 -503.555 -503.555] (1.000)
Step: 849, Reward: [-389.975 -389.975 -389.975] [0.0000], Avg: [-496.874 -496.874 -496.874] (1.000)
Step: 899, Reward: [-445.314 -445.314 -445.314] [0.0000], Avg: [-494.01 -494.01 -494.01] (1.000)
Step: 949, Reward: [-570.259 -570.259 -570.259] [0.0000], Avg: [-498.023 -498.023 -498.023] (1.000)
Step: 999, Reward: [-429.916 -429.916 -429.916] [0.0000], Avg: [-494.618 -494.618 -494.618] (1.000)
Step: 1049, Reward: [-489.207 -489.207 -489.207] [0.0000], Avg: [-494.36 -494.36 -494.36] (1.000)
Step: 1099, Reward: [-382.499 -382.499 -382.499] [0.0000], Avg: [-489.275 -489.275 -489.275] (1.000)
Step: 1149, Reward: [-520.172 -520.172 -520.172] [0.0000], Avg: [-490.619 -490.619 -490.619] (1.000)
Step: 1199, Reward: [-431.653 -431.653 -431.653] [0.0000], Avg: [-488.162 -488.162 -488.162] (1.000)
Step: 1249, Reward: [-514.006 -514.006 -514.006] [0.0000], Avg: [-489.196 -489.196 -489.196] (1.000)
Step: 1299, Reward: [-403.02 -403.02 -403.02] [0.0000], Avg: [-485.881 -485.881 -485.881] (1.000)
Step: 1349, Reward: [-852.726 -852.726 -852.726] [0.0000], Avg: [-499.468 -499.468 -499.468] (1.000)
Step: 1399, Reward: [-609.447 -609.447 -609.447] [0.0000], Avg: [-503.396 -503.396 -503.396] (1.000)
Step: 1449, Reward: [-1076.901 -1076.901 -1076.901] [0.0000], Avg: [-523.172 -523.172 -523.172] (1.000)
Step: 1499, Reward: [-358.107 -358.107 -358.107] [0.0000], Avg: [-517.67 -517.67 -517.67] (1.000)
Step: 1549, Reward: [-779.641 -779.641 -779.641] [0.0000], Avg: [-526.12 -526.12 -526.12] (1.000)
Step: 1599, Reward: [-711.448 -711.448 -711.448] [0.0000], Avg: [-531.912 -531.912 -531.912] (1.000)
Step: 1649, Reward: [-702.89 -702.89 -702.89] [0.0000], Avg: [-537.093 -537.093 -537.093] (1.000)
Step: 1699, Reward: [-466.89 -466.89 -466.89] [0.0000], Avg: [-535.028 -535.028 -535.028] (1.000)
Step: 1749, Reward: [-709.299 -709.299 -709.299] [0.0000], Avg: [-540.007 -540.007 -540.007] (1.000)
Step: 1799, Reward: [-596.593 -596.593 -596.593] [0.0000], Avg: [-541.579 -541.579 -541.579] (1.000)
Step: 1849, Reward: [-579.326 -579.326 -579.326] [0.0000], Avg: [-542.599 -542.599 -542.599] (1.000)
Step: 1899, Reward: [-352.458 -352.458 -352.458] [0.0000], Avg: [-537.596 -537.596 -537.596] (1.000)
Step: 1949, Reward: [-536.504 -536.504 -536.504] [0.0000], Avg: [-537.568 -537.568 -537.568] (1.000)
Step: 1999, Reward: [-381.623 -381.623 -381.623] [0.0000], Avg: [-533.669 -533.669 -533.669] (1.000)
Step: 2049, Reward: [-522.64 -522.64 -522.64] [0.0000], Avg: [-533.4 -533.4 -533.4] (1.000)
Step: 2099, Reward: [-424.875 -424.875 -424.875] [0.0000], Avg: [-530.816 -530.816 -530.816] (1.000)
Step: 2149, Reward: [-397.242 -397.242 -397.242] [0.0000], Avg: [-527.71 -527.71 -527.71] (1.000)
Step: 2199, Reward: [-733.713 -733.713 -733.713] [0.0000], Avg: [-532.392 -532.392 -532.392] (1.000)
Step: 2249, Reward: [-641.896 -641.896 -641.896] [0.0000], Avg: [-534.825 -534.825 -534.825] (1.000)
Step: 2299, Reward: [-507.593 -507.593 -507.593] [0.0000], Avg: [-534.233 -534.233 -534.233] (1.000)
Step: 2349, Reward: [-429.91 -429.91 -429.91] [0.0000], Avg: [-532.013 -532.013 -532.013] (1.000)
Step: 2399, Reward: [-516.289 -516.289 -516.289] [0.0000], Avg: [-531.686 -531.686 -531.686] (1.000)
Step: 2449, Reward: [-611.587 -611.587 -611.587] [0.0000], Avg: [-533.317 -533.317 -533.317] (1.000)
Step: 2499, Reward: [-444.831 -444.831 -444.831] [0.0000], Avg: [-531.547 -531.547 -531.547] (1.000)
Step: 2549, Reward: [-469.312 -469.312 -469.312] [0.0000], Avg: [-530.327 -530.327 -530.327] (1.000)
Step: 2599, Reward: [-530.703 -530.703 -530.703] [0.0000], Avg: [-530.334 -530.334 -530.334] (1.000)
Step: 2649, Reward: [-523.969 -523.969 -523.969] [0.0000], Avg: [-530.214 -530.214 -530.214] (1.000)
Step: 2699, Reward: [-619.048 -619.048 -619.048] [0.0000], Avg: [-531.859 -531.859 -531.859] (1.000)
Step: 2749, Reward: [-454.478 -454.478 -454.478] [0.0000], Avg: [-530.452 -530.452 -530.452] (1.000)
Step: 2799, Reward: [-577.983 -577.983 -577.983] [0.0000], Avg: [-531.301 -531.301 -531.301] (1.000)
Step: 2849, Reward: [-674.716 -674.716 -674.716] [0.0000], Avg: [-533.817 -533.817 -533.817] (1.000)
Step: 2899, Reward: [-681.052 -681.052 -681.052] [0.0000], Avg: [-536.355 -536.355 -536.355] (1.000)
Step: 2949, Reward: [-625.258 -625.258 -625.258] [0.0000], Avg: [-537.862 -537.862 -537.862] (1.000)
Step: 2999, Reward: [-424.373 -424.373 -424.373] [0.0000], Avg: [-535.971 -535.971 -535.971] (1.000)
Step: 3049, Reward: [-723.548 -723.548 -723.548] [0.0000], Avg: [-539.046 -539.046 -539.046] (1.000)
Step: 3099, Reward: [-820.531 -820.531 -820.531] [0.0000], Avg: [-543.586 -543.586 -543.586] (1.000)
Step: 3149, Reward: [-552.97 -552.97 -552.97] [0.0000], Avg: [-543.735 -543.735 -543.735] (1.000)
Step: 3199, Reward: [-515.609 -515.609 -515.609] [0.0000], Avg: [-543.295 -543.295 -543.295] (1.000)
Step: 3249, Reward: [-811.181 -811.181 -811.181] [0.0000], Avg: [-547.416 -547.416 -547.416] (1.000)
Step: 3299, Reward: [-548.221 -548.221 -548.221] [0.0000], Avg: [-547.429 -547.429 -547.429] (1.000)
Step: 3349, Reward: [-455.414 -455.414 -455.414] [0.0000], Avg: [-546.055 -546.055 -546.055] (1.000)
Step: 3399, Reward: [-897.935 -897.935 -897.935] [0.0000], Avg: [-551.23 -551.23 -551.23] (1.000)
Step: 3449, Reward: [-395.796 -395.796 -395.796] [0.0000], Avg: [-548.977 -548.977 -548.977] (1.000)
Step: 3499, Reward: [-947.4 -947.4 -947.4] [0.0000], Avg: [-554.669 -554.669 -554.669] (1.000)
Step: 3549, Reward: [-623.23 -623.23 -623.23] [0.0000], Avg: [-555.635 -555.635 -555.635] (1.000)
Step: 3599, Reward: [-608.107 -608.107 -608.107] [0.0000], Avg: [-556.364 -556.364 -556.364] (1.000)
Step: 3649, Reward: [-687.425 -687.425 -687.425] [0.0000], Avg: [-558.159 -558.159 -558.159] (1.000)
Step: 3699, Reward: [-495.852 -495.852 -495.852] [0.0000], Avg: [-557.317 -557.317 -557.317] (1.000)
Step: 3749, Reward: [-709.688 -709.688 -709.688] [0.0000], Avg: [-559.349 -559.349 -559.349] (1.000)
Step: 3799, Reward: [-833.415 -833.415 -833.415] [0.0000], Avg: [-562.955 -562.955 -562.955] (1.000)
Step: 3849, Reward: [-490.468 -490.468 -490.468] [0.0000], Avg: [-562.013 -562.013 -562.013] (1.000)
Step: 3899, Reward: [-662.234 -662.234 -662.234] [0.0000], Avg: [-563.298 -563.298 -563.298] (1.000)
Step: 3949, Reward: [-693.235 -693.235 -693.235] [0.0000], Avg: [-564.943 -564.943 -564.943] (1.000)
Step: 3999, Reward: [-540.774 -540.774 -540.774] [0.0000], Avg: [-564.641 -564.641 -564.641] (1.000)
Step: 4049, Reward: [-745.637 -745.637 -745.637] [0.0000], Avg: [-566.875 -566.875 -566.875] (1.000)
Step: 4099, Reward: [-566.057 -566.057 -566.057] [0.0000], Avg: [-566.865 -566.865 -566.865] (1.000)
Step: 4149, Reward: [-734.841 -734.841 -734.841] [0.0000], Avg: [-568.889 -568.889 -568.889] (1.000)
Step: 4199, Reward: [-569.516 -569.516 -569.516] [0.0000], Avg: [-568.897 -568.897 -568.897] (1.000)
Step: 4249, Reward: [-1101.617 -1101.617 -1101.617] [0.0000], Avg: [-575.164 -575.164 -575.164] (1.000)
Step: 4299, Reward: [-887.144 -887.144 -887.144] [0.0000], Avg: [-578.792 -578.792 -578.792] (1.000)
Step: 4349, Reward: [-626.914 -626.914 -626.914] [0.0000], Avg: [-579.345 -579.345 -579.345] (1.000)
Step: 4399, Reward: [-655.448 -655.448 -655.448] [0.0000], Avg: [-580.21 -580.21 -580.21] (1.000)
Step: 4449, Reward: [-629.186 -629.186 -629.186] [0.0000], Avg: [-580.76 -580.76 -580.76] (1.000)
Step: 4499, Reward: [-642.226 -642.226 -642.226] [0.0000], Avg: [-581.443 -581.443 -581.443] (1.000)
Step: 4549, Reward: [-584.281 -584.281 -584.281] [0.0000], Avg: [-581.474 -581.474 -581.474] (1.000)
Step: 4599, Reward: [-806.427 -806.427 -806.427] [0.0000], Avg: [-583.919 -583.919 -583.919] (1.000)
Step: 4649, Reward: [-552.597 -552.597 -552.597] [0.0000], Avg: [-583.582 -583.582 -583.582] (1.000)
Step: 4699, Reward: [-741.586 -741.586 -741.586] [0.0000], Avg: [-585.263 -585.263 -585.263] (1.000)
Step: 4749, Reward: [-763.713 -763.713 -763.713] [0.0000], Avg: [-587.142 -587.142 -587.142] (1.000)
Step: 4799, Reward: [-854.059 -854.059 -854.059] [0.0000], Avg: [-589.922 -589.922 -589.922] (1.000)
Step: 4849, Reward: [-1029.588 -1029.588 -1029.588] [0.0000], Avg: [-594.455 -594.455 -594.455] (1.000)
Step: 4899, Reward: [-782.063 -782.063 -782.063] [0.0000], Avg: [-596.369 -596.369 -596.369] (1.000)
Step: 4949, Reward: [-702.287 -702.287 -702.287] [0.0000], Avg: [-597.439 -597.439 -597.439] (1.000)
Step: 4999, Reward: [-696.801 -696.801 -696.801] [0.0000], Avg: [-598.433 -598.433 -598.433] (1.000)
Step: 5049, Reward: [-644.66 -644.66 -644.66] [0.0000], Avg: [-598.89 -598.89 -598.89] (1.000)
Step: 5099, Reward: [-985.009 -985.009 -985.009] [0.0000], Avg: [-602.676 -602.676 -602.676] (1.000)
Step: 5149, Reward: [-704.145 -704.145 -704.145] [0.0000], Avg: [-603.661 -603.661 -603.661] (1.000)
Step: 5199, Reward: [-795.105 -795.105 -795.105] [0.0000], Avg: [-605.502 -605.502 -605.502] (1.000)
Step: 5249, Reward: [-578.424 -578.424 -578.424] [0.0000], Avg: [-605.244 -605.244 -605.244] (1.000)
Step: 5299, Reward: [-666.217 -666.217 -666.217] [0.0000], Avg: [-605.819 -605.819 -605.819] (1.000)
Step: 5349, Reward: [-684.536 -684.536 -684.536] [0.0000], Avg: [-606.555 -606.555 -606.555] (1.000)
Step: 5399, Reward: [-786.078 -786.078 -786.078] [0.0000], Avg: [-608.217 -608.217 -608.217] (1.000)
Step: 5449, Reward: [-788.14 -788.14 -788.14] [0.0000], Avg: [-609.868 -609.868 -609.868] (1.000)
Step: 5499, Reward: [-405.9 -405.9 -405.9] [0.0000], Avg: [-608.013 -608.013 -608.013] (1.000)
Step: 5549, Reward: [-773.752 -773.752 -773.752] [0.0000], Avg: [-609.506 -609.506 -609.506] (1.000)
Step: 5599, Reward: [-715.694 -715.694 -715.694] [0.0000], Avg: [-610.455 -610.455 -610.455] (1.000)
Step: 5649, Reward: [-582.395 -582.395 -582.395] [0.0000], Avg: [-610.206 -610.206 -610.206] (1.000)
Step: 5699, Reward: [-701.262 -701.262 -701.262] [0.0000], Avg: [-611.005 -611.005 -611.005] (1.000)
Step: 5749, Reward: [-384.636 -384.636 -384.636] [0.0000], Avg: [-609.037 -609.037 -609.037] (1.000)
Step: 5799, Reward: [-498.6 -498.6 -498.6] [0.0000], Avg: [-608.085 -608.085 -608.085] (1.000)
Step: 5849, Reward: [-766.327 -766.327 -766.327] [0.0000], Avg: [-609.437 -609.437 -609.437] (1.000)
Step: 5899, Reward: [-577.987 -577.987 -577.987] [0.0000], Avg: [-609.171 -609.171 -609.171] (1.000)
Step: 5949, Reward: [-395.594 -395.594 -395.594] [0.0000], Avg: [-607.376 -607.376 -607.376] (1.000)
Step: 5999, Reward: [-604.431 -604.431 -604.431] [0.0000], Avg: [-607.351 -607.351 -607.351] (1.000)
Step: 6049, Reward: [-445.616 -445.616 -445.616] [0.0000], Avg: [-606.015 -606.015 -606.015] (1.000)
Step: 6099, Reward: [-383.251 -383.251 -383.251] [0.0000], Avg: [-604.189 -604.189 -604.189] (1.000)
Step: 6149, Reward: [-562.181 -562.181 -562.181] [0.0000], Avg: [-603.847 -603.847 -603.847] (1.000)
Step: 6199, Reward: [-482.672 -482.672 -482.672] [0.0000], Avg: [-602.87 -602.87 -602.87] (1.000)
Step: 6249, Reward: [-354.928 -354.928 -354.928] [0.0000], Avg: [-600.886 -600.886 -600.886] (1.000)
Step: 6299, Reward: [-652.928 -652.928 -652.928] [0.0000], Avg: [-601.299 -601.299 -601.299] (1.000)
Step: 6349, Reward: [-414.719 -414.719 -414.719] [0.0000], Avg: [-599.83 -599.83 -599.83] (1.000)
Step: 6399, Reward: [-405.54 -405.54 -405.54] [0.0000], Avg: [-598.312 -598.312 -598.312] (1.000)
Step: 6449, Reward: [-570.171 -570.171 -570.171] [0.0000], Avg: [-598.094 -598.094 -598.094] (1.000)
Step: 6499, Reward: [-475.405 -475.405 -475.405] [0.0000], Avg: [-597.15 -597.15 -597.15] (1.000)
Step: 6549, Reward: [-339.469 -339.469 -339.469] [0.0000], Avg: [-595.183 -595.183 -595.183] (1.000)
Step: 6599, Reward: [-509.949 -509.949 -509.949] [0.0000], Avg: [-594.538 -594.538 -594.538] (1.000)
Step: 6649, Reward: [-401.981 -401.981 -401.981] [0.0000], Avg: [-593.09 -593.09 -593.09] (1.000)
Step: 6699, Reward: [-527.578 -527.578 -527.578] [0.0000], Avg: [-592.601 -592.601 -592.601] (1.000)
Step: 6749, Reward: [-394.573 -394.573 -394.573] [0.0000], Avg: [-591.134 -591.134 -591.134] (1.000)
Step: 6799, Reward: [-439.053 -439.053 -439.053] [0.0000], Avg: [-590.016 -590.016 -590.016] (1.000)
Step: 6849, Reward: [-456.162 -456.162 -456.162] [0.0000], Avg: [-589.039 -589.039 -589.039] (1.000)
Step: 6899, Reward: [-412.512 -412.512 -412.512] [0.0000], Avg: [-587.76 -587.76 -587.76] (1.000)
Step: 6949, Reward: [-629.772 -629.772 -629.772] [0.0000], Avg: [-588.062 -588.062 -588.062] (1.000)
Step: 6999, Reward: [-449.284 -449.284 -449.284] [0.0000], Avg: [-587.071 -587.071 -587.071] (1.000)
Step: 7049, Reward: [-480.457 -480.457 -480.457] [0.0000], Avg: [-586.315 -586.315 -586.315] (1.000)
Step: 7099, Reward: [-453.151 -453.151 -453.151] [0.0000], Avg: [-585.377 -585.377 -585.377] (1.000)
Step: 7149, Reward: [-339.907 -339.907 -339.907] [0.0000], Avg: [-583.66 -583.66 -583.66] (1.000)
Step: 7199, Reward: [-386.175 -386.175 -386.175] [0.0000], Avg: [-582.289 -582.289 -582.289] (1.000)
Step: 7249, Reward: [-504.216 -504.216 -504.216] [0.0000], Avg: [-581.75 -581.75 -581.75] (1.000)
Step: 7299, Reward: [-413.758 -413.758 -413.758] [0.0000], Avg: [-580.6 -580.6 -580.6] (1.000)
Step: 7349, Reward: [-574.178 -574.178 -574.178] [0.0000], Avg: [-580.556 -580.556 -580.556] (1.000)
Step: 7399, Reward: [-525.49 -525.49 -525.49] [0.0000], Avg: [-580.184 -580.184 -580.184] (1.000)
Step: 7449, Reward: [-512.865 -512.865 -512.865] [0.0000], Avg: [-579.732 -579.732 -579.732] (1.000)
Step: 7499, Reward: [-488.631 -488.631 -488.631] [0.0000], Avg: [-579.125 -579.125 -579.125] (1.000)
Step: 7549, Reward: [-593.646 -593.646 -593.646] [0.0000], Avg: [-579.221 -579.221 -579.221] (1.000)
Step: 7599, Reward: [-450.031 -450.031 -450.031] [0.0000], Avg: [-578.371 -578.371 -578.371] (1.000)
Step: 7649, Reward: [-454.982 -454.982 -454.982] [0.0000], Avg: [-577.565 -577.565 -577.565] (1.000)
Step: 7699, Reward: [-489.978 -489.978 -489.978] [0.0000], Avg: [-576.996 -576.996 -576.996] (1.000)
Step: 7749, Reward: [-514.628 -514.628 -514.628] [0.0000], Avg: [-576.593 -576.593 -576.593] (1.000)
Step: 7799, Reward: [-376.34 -376.34 -376.34] [0.0000], Avg: [-575.31 -575.31 -575.31] (1.000)
Step: 7849, Reward: [-407.418 -407.418 -407.418] [0.0000], Avg: [-574.24 -574.24 -574.24] (1.000)
Step: 7899, Reward: [-580.066 -580.066 -580.066] [0.0000], Avg: [-574.277 -574.277 -574.277] (1.000)
Step: 7949, Reward: [-472.381 -472.381 -472.381] [0.0000], Avg: [-573.636 -573.636 -573.636] (1.000)
Step: 7999, Reward: [-545.648 -545.648 -545.648] [0.0000], Avg: [-573.461 -573.461 -573.461] (1.000)
Step: 8049, Reward: [-464.171 -464.171 -464.171] [0.0000], Avg: [-572.783 -572.783 -572.783] (1.000)
Step: 8099, Reward: [-455.036 -455.036 -455.036] [0.0000], Avg: [-572.056 -572.056 -572.056] (1.000)
Step: 8149, Reward: [-471.213 -471.213 -471.213] [0.0000], Avg: [-571.437 -571.437 -571.437] (1.000)
Step: 8199, Reward: [-739.24 -739.24 -739.24] [0.0000], Avg: [-572.46 -572.46 -572.46] (1.000)
Step: 8249, Reward: [-694.688 -694.688 -694.688] [0.0000], Avg: [-573.201 -573.201 -573.201] (1.000)
Step: 8299, Reward: [-419.021 -419.021 -419.021] [0.0000], Avg: [-572.272 -572.272 -572.272] (1.000)
Step: 8349, Reward: [-424.35 -424.35 -424.35] [0.0000], Avg: [-571.387 -571.387 -571.387] (1.000)
Step: 8399, Reward: [-696.562 -696.562 -696.562] [0.0000], Avg: [-572.132 -572.132 -572.132] (1.000)
Step: 8449, Reward: [-542.053 -542.053 -542.053] [0.0000], Avg: [-571.954 -571.954 -571.954] (1.000)
Step: 8499, Reward: [-576.428 -576.428 -576.428] [0.0000], Avg: [-571.98 -571.98 -571.98] (1.000)
Step: 8549, Reward: [-908.272 -908.272 -908.272] [0.0000], Avg: [-573.947 -573.947 -573.947] (1.000)
Step: 8599, Reward: [-483.779 -483.779 -483.779] [0.0000], Avg: [-573.422 -573.422 -573.422] (1.000)
Step: 8649, Reward: [-810.217 -810.217 -810.217] [0.0000], Avg: [-574.791 -574.791 -574.791] (1.000)
Step: 8699, Reward: [-493.566 -493.566 -493.566] [0.0000], Avg: [-574.324 -574.324 -574.324] (1.000)
Step: 8749, Reward: [-637.919 -637.919 -637.919] [0.0000], Avg: [-574.688 -574.688 -574.688] (1.000)
Step: 8799, Reward: [-451.456 -451.456 -451.456] [0.0000], Avg: [-573.988 -573.988 -573.988] (1.000)
Step: 8849, Reward: [-503.24 -503.24 -503.24] [0.0000], Avg: [-573.588 -573.588 -573.588] (1.000)
Step: 8899, Reward: [-460.847 -460.847 -460.847] [0.0000], Avg: [-572.954 -572.954 -572.954] (1.000)
Step: 8949, Reward: [-459.612 -459.612 -459.612] [0.0000], Avg: [-572.321 -572.321 -572.321] (1.000)
Step: 8999, Reward: [-666.074 -666.074 -666.074] [0.0000], Avg: [-572.842 -572.842 -572.842] (1.000)
Step: 9049, Reward: [-393.536 -393.536 -393.536] [0.0000], Avg: [-571.851 -571.851 -571.851] (1.000)
Step: 9099, Reward: [-334.365 -334.365 -334.365] [0.0000], Avg: [-570.547 -570.547 -570.547] (1.000)
Step: 9149, Reward: [-480.538 -480.538 -480.538] [0.0000], Avg: [-570.055 -570.055 -570.055] (1.000)
Step: 9199, Reward: [-510.559 -510.559 -510.559] [0.0000], Avg: [-569.731 -569.731 -569.731] (1.000)
Step: 9249, Reward: [-517.023 -517.023 -517.023] [0.0000], Avg: [-569.446 -569.446 -569.446] (1.000)
Step: 9299, Reward: [-416.317 -416.317 -416.317] [0.0000], Avg: [-568.623 -568.623 -568.623] (1.000)
Step: 9349, Reward: [-415.846 -415.846 -415.846] [0.0000], Avg: [-567.806 -567.806 -567.806] (1.000)
Step: 9399, Reward: [-412.463 -412.463 -412.463] [0.0000], Avg: [-566.98 -566.98 -566.98] (1.000)
Step: 9449, Reward: [-487.865 -487.865 -487.865] [0.0000], Avg: [-566.561 -566.561 -566.561] (1.000)
Step: 9499, Reward: [-439.878 -439.878 -439.878] [0.0000], Avg: [-565.895 -565.895 -565.895] (1.000)
Step: 9549, Reward: [-512.2 -512.2 -512.2] [0.0000], Avg: [-565.613 -565.613 -565.613] (1.000)
Step: 9599, Reward: [-497.925 -497.925 -497.925] [0.0000], Avg: [-565.261 -565.261 -565.261] (1.000)
Step: 9649, Reward: [-410.092 -410.092 -410.092] [0.0000], Avg: [-564.457 -564.457 -564.457] (1.000)
Step: 9699, Reward: [-435.298 -435.298 -435.298] [0.0000], Avg: [-563.791 -563.791 -563.791] (1.000)
Step: 9749, Reward: [-502.304 -502.304 -502.304] [0.0000], Avg: [-563.476 -563.476 -563.476] (1.000)
Step: 9799, Reward: [-394.041 -394.041 -394.041] [0.0000], Avg: [-562.611 -562.611 -562.611] (1.000)
Step: 9849, Reward: [-392.439 -392.439 -392.439] [0.0000], Avg: [-561.748 -561.748 -561.748] (1.000)
Step: 9899, Reward: [-368.882 -368.882 -368.882] [0.0000], Avg: [-560.773 -560.773 -560.773] (1.000)
Step: 9949, Reward: [-496.238 -496.238 -496.238] [0.0000], Avg: [-560.449 -560.449 -560.449] (1.000)
Step: 9999, Reward: [-415.844 -415.844 -415.844] [0.0000], Avg: [-559.726 -559.726 -559.726] (1.000)
Step: 10049, Reward: [-392.623 -392.623 -392.623] [0.0000], Avg: [-558.895 -558.895 -558.895] (1.000)
Step: 10099, Reward: [-470.954 -470.954 -470.954] [0.0000], Avg: [-558.459 -558.459 -558.459] (1.000)
Step: 10149, Reward: [-526.594 -526.594 -526.594] [0.0000], Avg: [-558.302 -558.302 -558.302] (1.000)
Step: 10199, Reward: [-345.959 -345.959 -345.959] [0.0000], Avg: [-557.262 -557.262 -557.262] (1.000)
Step: 10249, Reward: [-527.356 -527.356 -527.356] [0.0000], Avg: [-557.116 -557.116 -557.116] (1.000)
Step: 10299, Reward: [-518.841 -518.841 -518.841] [0.0000], Avg: [-556.93 -556.93 -556.93] (1.000)
Step: 10349, Reward: [-493.941 -493.941 -493.941] [0.0000], Avg: [-556.626 -556.626 -556.626] (1.000)
Step: 10399, Reward: [-397.672 -397.672 -397.672] [0.0000], Avg: [-555.861 -555.861 -555.861] (1.000)
Step: 10449, Reward: [-586.94 -586.94 -586.94] [0.0000], Avg: [-556.01 -556.01 -556.01] (1.000)
Step: 10499, Reward: [-435.175 -435.175 -435.175] [0.0000], Avg: [-555.435 -555.435 -555.435] (1.000)
Step: 10549, Reward: [-542.377 -542.377 -542.377] [0.0000], Avg: [-555.373 -555.373 -555.373] (1.000)
Step: 10599, Reward: [-510.875 -510.875 -510.875] [0.0000], Avg: [-555.163 -555.163 -555.163] (1.000)
Step: 10649, Reward: [-348.664 -348.664 -348.664] [0.0000], Avg: [-554.193 -554.193 -554.193] (1.000)
Step: 10699, Reward: [-414.136 -414.136 -414.136] [0.0000], Avg: [-553.539 -553.539 -553.539] (1.000)
Step: 10749, Reward: [-423.992 -423.992 -423.992] [0.0000], Avg: [-552.936 -552.936 -552.936] (1.000)
Step: 10799, Reward: [-816.118 -816.118 -816.118] [0.0000], Avg: [-554.155 -554.155 -554.155] (1.000)
Step: 10849, Reward: [-572.46 -572.46 -572.46] [0.0000], Avg: [-554.239 -554.239 -554.239] (1.000)
Step: 10899, Reward: [-475.126 -475.126 -475.126] [0.0000], Avg: [-553.876 -553.876 -553.876] (1.000)
Step: 10949, Reward: [-440.762 -440.762 -440.762] [0.0000], Avg: [-553.36 -553.36 -553.36] (1.000)
Step: 10999, Reward: [-555.86 -555.86 -555.86] [0.0000], Avg: [-553.371 -553.371 -553.371] (1.000)
Step: 11049, Reward: [-555.822 -555.822 -555.822] [0.0000], Avg: [-553.382 -553.382 -553.382] (1.000)
Step: 11099, Reward: [-556.729 -556.729 -556.729] [0.0000], Avg: [-553.397 -553.397 -553.397] (1.000)
Step: 11149, Reward: [-454.359 -454.359 -454.359] [0.0000], Avg: [-552.953 -552.953 -552.953] (1.000)
Step: 11199, Reward: [-509.516 -509.516 -509.516] [0.0000], Avg: [-552.759 -552.759 -552.759] (1.000)
Step: 11249, Reward: [-385.712 -385.712 -385.712] [0.0000], Avg: [-552.017 -552.017 -552.017] (1.000)
Step: 11299, Reward: [-410.217 -410.217 -410.217] [0.0000], Avg: [-551.389 -551.389 -551.389] (1.000)
Step: 11349, Reward: [-580.588 -580.588 -580.588] [0.0000], Avg: [-551.518 -551.518 -551.518] (1.000)
Step: 11399, Reward: [-452.486 -452.486 -452.486] [0.0000], Avg: [-551.084 -551.084 -551.084] (1.000)
Step: 11449, Reward: [-500.743 -500.743 -500.743] [0.0000], Avg: [-550.864 -550.864 -550.864] (1.000)
Step: 11499, Reward: [-721.541 -721.541 -721.541] [0.0000], Avg: [-551.606 -551.606 -551.606] (1.000)
Step: 11549, Reward: [-598.902 -598.902 -598.902] [0.0000], Avg: [-551.811 -551.811 -551.811] (1.000)
Step: 11599, Reward: [-586.499 -586.499 -586.499] [0.0000], Avg: [-551.96 -551.96 -551.96] (1.000)
Step: 11649, Reward: [-407.025 -407.025 -407.025] [0.0000], Avg: [-551.338 -551.338 -551.338] (1.000)
Step: 11699, Reward: [-394.444 -394.444 -394.444] [0.0000], Avg: [-550.668 -550.668 -550.668] (1.000)
Step: 11749, Reward: [-535.639 -535.639 -535.639] [0.0000], Avg: [-550.604 -550.604 -550.604] (1.000)
Step: 11799, Reward: [-543.368 -543.368 -543.368] [0.0000], Avg: [-550.573 -550.573 -550.573] (1.000)
Step: 11849, Reward: [-440.714 -440.714 -440.714] [0.0000], Avg: [-550.11 -550.11 -550.11] (1.000)
Step: 11899, Reward: [-607.525 -607.525 -607.525] [0.0000], Avg: [-550.351 -550.351 -550.351] (1.000)
Step: 11949, Reward: [-435.573 -435.573 -435.573] [0.0000], Avg: [-549.871 -549.871 -549.871] (1.000)
Step: 11999, Reward: [-660. -660. -660.] [0.0000], Avg: [-550.329 -550.329 -550.329] (1.000)
Step: 12049, Reward: [-481.004 -481.004 -481.004] [0.0000], Avg: [-550.042 -550.042 -550.042] (1.000)
Step: 12099, Reward: [-625.731 -625.731 -625.731] [0.0000], Avg: [-550.355 -550.355 -550.355] (1.000)
Step: 12149, Reward: [-550.134 -550.134 -550.134] [0.0000], Avg: [-550.354 -550.354 -550.354] (1.000)
Step: 12199, Reward: [-468.185 -468.185 -468.185] [0.0000], Avg: [-550.017 -550.017 -550.017] (1.000)
Step: 12249, Reward: [-787.234 -787.234 -787.234] [0.0000], Avg: [-550.985 -550.985 -550.985] (1.000)
Step: 12299, Reward: [-505.902 -505.902 -505.902] [0.0000], Avg: [-550.802 -550.802 -550.802] (1.000)
Step: 12349, Reward: [-810.705 -810.705 -810.705] [0.0000], Avg: [-551.854 -551.854 -551.854] (1.000)
Step: 12399, Reward: [-765.768 -765.768 -765.768] [0.0000], Avg: [-552.717 -552.717 -552.717] (1.000)
Step: 12449, Reward: [-531.577 -531.577 -531.577] [0.0000], Avg: [-552.632 -552.632 -552.632] (1.000)
Step: 12499, Reward: [-689.289 -689.289 -689.289] [0.0000], Avg: [-553.178 -553.178 -553.178] (1.000)
Step: 12549, Reward: [-660.064 -660.064 -660.064] [0.0000], Avg: [-553.604 -553.604 -553.604] (1.000)
Step: 12599, Reward: [-576.349 -576.349 -576.349] [0.0000], Avg: [-553.694 -553.694 -553.694] (1.000)
Step: 12649, Reward: [-547.474 -547.474 -547.474] [0.0000], Avg: [-553.67 -553.67 -553.67] (1.000)
Step: 12699, Reward: [-522.711 -522.711 -522.711] [0.0000], Avg: [-553.548 -553.548 -553.548] (1.000)
Step: 12749, Reward: [-365.841 -365.841 -365.841] [0.0000], Avg: [-552.812 -552.812 -552.812] (1.000)
Step: 12799, Reward: [-395.385 -395.385 -395.385] [0.0000], Avg: [-552.197 -552.197 -552.197] (1.000)
Step: 12849, Reward: [-516.474 -516.474 -516.474] [0.0000], Avg: [-552.058 -552.058 -552.058] (1.000)
Step: 12899, Reward: [-445.071 -445.071 -445.071] [0.0000], Avg: [-551.643 -551.643 -551.643] (1.000)
Step: 12949, Reward: [-396.008 -396.008 -396.008] [0.0000], Avg: [-551.042 -551.042 -551.042] (1.000)
Step: 12999, Reward: [-518.5 -518.5 -518.5] [0.0000], Avg: [-550.917 -550.917 -550.917] (1.000)
Step: 13049, Reward: [-429.627 -429.627 -429.627] [0.0000], Avg: [-550.452 -550.452 -550.452] (1.000)
Step: 13099, Reward: [-563.422 -563.422 -563.422] [0.0000], Avg: [-550.502 -550.502 -550.502] (1.000)
Step: 13149, Reward: [-528.472 -528.472 -528.472] [0.0000], Avg: [-550.418 -550.418 -550.418] (1.000)
Step: 13199, Reward: [-639.253 -639.253 -639.253] [0.0000], Avg: [-550.755 -550.755 -550.755] (1.000)
Step: 13249, Reward: [-436.987 -436.987 -436.987] [0.0000], Avg: [-550.325 -550.325 -550.325] (1.000)
Step: 13299, Reward: [-430.64 -430.64 -430.64] [0.0000], Avg: [-549.875 -549.875 -549.875] (1.000)
Step: 13349, Reward: [-503.626 -503.626 -503.626] [0.0000], Avg: [-549.702 -549.702 -549.702] (1.000)
Step: 13399, Reward: [-426.146 -426.146 -426.146] [0.0000], Avg: [-549.241 -549.241 -549.241] (1.000)
Step: 13449, Reward: [-561.812 -561.812 -561.812] [0.0000], Avg: [-549.288 -549.288 -549.288] (1.000)
Step: 13499, Reward: [-308.168 -308.168 -308.168] [0.0000], Avg: [-548.395 -548.395 -548.395] (1.000)
Step: 13549, Reward: [-494.941 -494.941 -494.941] [0.0000], Avg: [-548.198 -548.198 -548.198] (1.000)
Step: 13599, Reward: [-487.183 -487.183 -487.183] [0.0000], Avg: [-547.973 -547.973 -547.973] (1.000)
Step: 13649, Reward: [-664.451 -664.451 -664.451] [0.0000], Avg: [-548.4 -548.4 -548.4] (1.000)
Step: 13699, Reward: [-316.292 -316.292 -316.292] [0.0000], Avg: [-547.553 -547.553 -547.553] (1.000)
Step: 13749, Reward: [-332.869 -332.869 -332.869] [0.0000], Avg: [-546.772 -546.772 -546.772] (1.000)
Step: 13799, Reward: [-404.674 -404.674 -404.674] [0.0000], Avg: [-546.257 -546.257 -546.257] (1.000)
Step: 13849, Reward: [-560.868 -560.868 -560.868] [0.0000], Avg: [-546.31 -546.31 -546.31] (1.000)
Step: 13899, Reward: [-547.64 -547.64 -547.64] [0.0000], Avg: [-546.315 -546.315 -546.315] (1.000)
Step: 13949, Reward: [-729.925 -729.925 -729.925] [0.0000], Avg: [-546.973 -546.973 -546.973] (1.000)
Step: 13999, Reward: [-377.972 -377.972 -377.972] [0.0000], Avg: [-546.369 -546.369 -546.369] (1.000)
Step: 14049, Reward: [-1235.976 -1235.976 -1235.976] [0.0000], Avg: [-548.824 -548.824 -548.824] (1.000)
Step: 14099, Reward: [-956.982 -956.982 -956.982] [0.0000], Avg: [-550.271 -550.271 -550.271] (1.000)
Step: 14149, Reward: [-414.123 -414.123 -414.123] [0.0000], Avg: [-549.79 -549.79 -549.79] (1.000)
Step: 14199, Reward: [-442.238 -442.238 -442.238] [0.0000], Avg: [-549.411 -549.411 -549.411] (1.000)
Step: 14249, Reward: [-589.734 -589.734 -589.734] [0.0000], Avg: [-549.553 -549.553 -549.553] (1.000)
Step: 14299, Reward: [-407.491 -407.491 -407.491] [0.0000], Avg: [-549.056 -549.056 -549.056] (1.000)
Step: 14349, Reward: [-503.073 -503.073 -503.073] [0.0000], Avg: [-548.896 -548.896 -548.896] (1.000)
Step: 14399, Reward: [-454.216 -454.216 -454.216] [0.0000], Avg: [-548.567 -548.567 -548.567] (1.000)
Step: 14449, Reward: [-506.145 -506.145 -506.145] [0.0000], Avg: [-548.42 -548.42 -548.42] (1.000)
Step: 14499, Reward: [-320.833 -320.833 -320.833] [0.0000], Avg: [-547.635 -547.635 -547.635] (1.000)
Step: 14549, Reward: [-356.069 -356.069 -356.069] [0.0000], Avg: [-546.977 -546.977 -546.977] (1.000)
Step: 14599, Reward: [-520.559 -520.559 -520.559] [0.0000], Avg: [-546.887 -546.887 -546.887] (1.000)
Step: 14649, Reward: [-721.72 -721.72 -721.72] [0.0000], Avg: [-547.483 -547.483 -547.483] (1.000)
Step: 14699, Reward: [-487.369 -487.369 -487.369] [0.0000], Avg: [-547.279 -547.279 -547.279] (1.000)
Step: 14749, Reward: [-417.777 -417.777 -417.777] [0.0000], Avg: [-546.84 -546.84 -546.84] (1.000)
Step: 14799, Reward: [-531.127 -531.127 -531.127] [0.0000], Avg: [-546.787 -546.787 -546.787] (1.000)
Step: 14849, Reward: [-446.465 -446.465 -446.465] [0.0000], Avg: [-546.449 -546.449 -546.449] (1.000)
Step: 14899, Reward: [-412.588 -412.588 -412.588] [0.0000], Avg: [-546. -546. -546.] (1.000)
Step: 14949, Reward: [-525.128 -525.128 -525.128] [0.0000], Avg: [-545.93 -545.93 -545.93] (1.000)
Step: 14999, Reward: [-480.203 -480.203 -480.203] [0.0000], Avg: [-545.711 -545.711 -545.711] (1.000)
Step: 15049, Reward: [-423.848 -423.848 -423.848] [0.0000], Avg: [-545.306 -545.306 -545.306] (1.000)
Step: 15099, Reward: [-530.002 -530.002 -530.002] [0.0000], Avg: [-545.255 -545.255 -545.255] (1.000)
Step: 15149, Reward: [-404.599 -404.599 -404.599] [0.0000], Avg: [-544.791 -544.791 -544.791] (1.000)
Step: 15199, Reward: [-382.599 -382.599 -382.599] [0.0000], Avg: [-544.258 -544.258 -544.258] (1.000)
Step: 15249, Reward: [-404.033 -404.033 -404.033] [0.0000], Avg: [-543.798 -543.798 -543.798] (1.000)
Step: 15299, Reward: [-309.639 -309.639 -309.639] [0.0000], Avg: [-543.033 -543.033 -543.033] (1.000)
Step: 15349, Reward: [-371.818 -371.818 -371.818] [0.0000], Avg: [-542.475 -542.475 -542.475] (1.000)
Step: 15399, Reward: [-569.613 -569.613 -569.613] [0.0000], Avg: [-542.563 -542.563 -542.563] (1.000)
Step: 15449, Reward: [-399.967 -399.967 -399.967] [0.0000], Avg: [-542.102 -542.102 -542.102] (1.000)
Step: 15499, Reward: [-479.867 -479.867 -479.867] [0.0000], Avg: [-541.901 -541.901 -541.901] (1.000)
Step: 15549, Reward: [-421.231 -421.231 -421.231] [0.0000], Avg: [-541.513 -541.513 -541.513] (1.000)
Step: 15599, Reward: [-474.204 -474.204 -474.204] [0.0000], Avg: [-541.297 -541.297 -541.297] (1.000)
Step: 15649, Reward: [-387.684 -387.684 -387.684] [0.0000], Avg: [-540.806 -540.806 -540.806] (1.000)
Step: 15699, Reward: [-396.688 -396.688 -396.688] [0.0000], Avg: [-540.347 -540.347 -540.347] (1.000)
Step: 15749, Reward: [-340.619 -340.619 -340.619] [0.0000], Avg: [-539.713 -539.713 -539.713] (1.000)
Step: 15799, Reward: [-485.109 -485.109 -485.109] [0.0000], Avg: [-539.54 -539.54 -539.54] (1.000)
Step: 15849, Reward: [-411.384 -411.384 -411.384] [0.0000], Avg: [-539.136 -539.136 -539.136] (1.000)
Step: 15899, Reward: [-415.74 -415.74 -415.74] [0.0000], Avg: [-538.748 -538.748 -538.748] (1.000)
Step: 15949, Reward: [-443.673 -443.673 -443.673] [0.0000], Avg: [-538.45 -538.45 -538.45] (1.000)
Step: 15999, Reward: [-672.286 -672.286 -672.286] [0.0000], Avg: [-538.868 -538.868 -538.868] (1.000)
Step: 16049, Reward: [-737.661 -737.661 -737.661] [0.0000], Avg: [-539.488 -539.488 -539.488] (1.000)
Step: 16099, Reward: [-430.533 -430.533 -430.533] [0.0000], Avg: [-539.149 -539.149 -539.149] (1.000)
Step: 16149, Reward: [-348.519 -348.519 -348.519] [0.0000], Avg: [-538.559 -538.559 -538.559] (1.000)
Step: 16199, Reward: [-497.286 -497.286 -497.286] [0.0000], Avg: [-538.432 -538.432 -538.432] (1.000)
Step: 16249, Reward: [-527.653 -527.653 -527.653] [0.0000], Avg: [-538.398 -538.398 -538.398] (1.000)
Step: 16299, Reward: [-434.037 -434.037 -434.037] [0.0000], Avg: [-538.078 -538.078 -538.078] (1.000)
Step: 16349, Reward: [-383.712 -383.712 -383.712] [0.0000], Avg: [-537.606 -537.606 -537.606] (1.000)
Step: 16399, Reward: [-405.832 -405.832 -405.832] [0.0000], Avg: [-537.205 -537.205 -537.205] (1.000)
Step: 16449, Reward: [-491.727 -491.727 -491.727] [0.0000], Avg: [-537.066 -537.066 -537.066] (1.000)
Step: 16499, Reward: [-431.132 -431.132 -431.132] [0.0000], Avg: [-536.745 -536.745 -536.745] (1.000)
Step: 16549, Reward: [-407.591 -407.591 -407.591] [0.0000], Avg: [-536.355 -536.355 -536.355] (1.000)
Step: 16599, Reward: [-679.187 -679.187 -679.187] [0.0000], Avg: [-536.785 -536.785 -536.785] (1.000)
Step: 16649, Reward: [-351.344 -351.344 -351.344] [0.0000], Avg: [-536.228 -536.228 -536.228] (1.000)
Step: 16699, Reward: [-368.441 -368.441 -368.441] [0.0000], Avg: [-535.726 -535.726 -535.726] (1.000)
Step: 16749, Reward: [-437.288 -437.288 -437.288] [0.0000], Avg: [-535.432 -535.432 -535.432] (1.000)
Step: 16799, Reward: [-452.885 -452.885 -452.885] [0.0000], Avg: [-535.187 -535.187 -535.187] (1.000)
Step: 16849, Reward: [-723.788 -723.788 -723.788] [0.0000], Avg: [-535.746 -535.746 -535.746] (1.000)
Step: 16899, Reward: [-414.799 -414.799 -414.799] [0.0000], Avg: [-535.388 -535.388 -535.388] (1.000)
Step: 16949, Reward: [-398.54 -398.54 -398.54] [0.0000], Avg: [-534.985 -534.985 -534.985] (1.000)
Step: 16999, Reward: [-456.503 -456.503 -456.503] [0.0000], Avg: [-534.754 -534.754 -534.754] (1.000)
Step: 17049, Reward: [-417.117 -417.117 -417.117] [0.0000], Avg: [-534.409 -534.409 -534.409] (1.000)
Step: 17099, Reward: [-350.148 -350.148 -350.148] [0.0000], Avg: [-533.87 -533.87 -533.87] (1.000)
Step: 17149, Reward: [-517.732 -517.732 -517.732] [0.0000], Avg: [-533.823 -533.823 -533.823] (1.000)
Step: 17199, Reward: [-381.959 -381.959 -381.959] [0.0000], Avg: [-533.382 -533.382 -533.382] (1.000)
Step: 17249, Reward: [-326.696 -326.696 -326.696] [0.0000], Avg: [-532.783 -532.783 -532.783] (1.000)
Step: 17299, Reward: [-371.448 -371.448 -371.448] [0.0000], Avg: [-532.316 -532.316 -532.316] (1.000)
Step: 17349, Reward: [-448.408 -448.408 -448.408] [0.0000], Avg: [-532.074 -532.074 -532.074] (1.000)
Step: 17399, Reward: [-517.292 -517.292 -517.292] [0.0000], Avg: [-532.032 -532.032 -532.032] (1.000)
Step: 17449, Reward: [-407.045 -407.045 -407.045] [0.0000], Avg: [-531.674 -531.674 -531.674] (1.000)
Step: 17499, Reward: [-387.915 -387.915 -387.915] [0.0000], Avg: [-531.263 -531.263 -531.263] (1.000)
Step: 17549, Reward: [-412.861 -412.861 -412.861] [0.0000], Avg: [-530.926 -530.926 -530.926] (1.000)
Step: 17599, Reward: [-455.268 -455.268 -455.268] [0.0000], Avg: [-530.711 -530.711 -530.711] (1.000)
Step: 17649, Reward: [-671.016 -671.016 -671.016] [0.0000], Avg: [-531.108 -531.108 -531.108] (1.000)
Step: 17699, Reward: [-353.024 -353.024 -353.024] [0.0000], Avg: [-530.605 -530.605 -530.605] (1.000)
Step: 17749, Reward: [-637.596 -637.596 -637.596] [0.0000], Avg: [-530.907 -530.907 -530.907] (1.000)
Step: 17799, Reward: [-388.914 -388.914 -388.914] [0.0000], Avg: [-530.508 -530.508 -530.508] (1.000)
Step: 17849, Reward: [-603.431 -603.431 -603.431] [0.0000], Avg: [-530.712 -530.712 -530.712] (1.000)
Step: 17899, Reward: [-464.734 -464.734 -464.734] [0.0000], Avg: [-530.528 -530.528 -530.528] (1.000)
Step: 17949, Reward: [-381.12 -381.12 -381.12] [0.0000], Avg: [-530.112 -530.112 -530.112] (1.000)
Step: 17999, Reward: [-337.136 -337.136 -337.136] [0.0000], Avg: [-529.575 -529.575 -529.575] (1.000)
Step: 18049, Reward: [-342.548 -342.548 -342.548] [0.0000], Avg: [-529.057 -529.057 -529.057] (1.000)
Step: 18099, Reward: [-488.866 -488.866 -488.866] [0.0000], Avg: [-528.946 -528.946 -528.946] (1.000)
Step: 18149, Reward: [-373.329 -373.329 -373.329] [0.0000], Avg: [-528.518 -528.518 -528.518] (1.000)
Step: 18199, Reward: [-396.882 -396.882 -396.882] [0.0000], Avg: [-528.156 -528.156 -528.156] (1.000)
Step: 18249, Reward: [-450.709 -450.709 -450.709] [0.0000], Avg: [-527.944 -527.944 -527.944] (1.000)
Step: 18299, Reward: [-321.487 -321.487 -321.487] [0.0000], Avg: [-527.38 -527.38 -527.38] (1.000)
Step: 18349, Reward: [-411.578 -411.578 -411.578] [0.0000], Avg: [-527.064 -527.064 -527.064] (1.000)
Step: 18399, Reward: [-411.004 -411.004 -411.004] [0.0000], Avg: [-526.749 -526.749 -526.749] (1.000)
Step: 18449, Reward: [-457.962 -457.962 -457.962] [0.0000], Avg: [-526.562 -526.562 -526.562] (1.000)
Step: 18499, Reward: [-436.685 -436.685 -436.685] [0.0000], Avg: [-526.32 -526.32 -526.32] (1.000)
Step: 18549, Reward: [-423.862 -423.862 -423.862] [0.0000], Avg: [-526.043 -526.043 -526.043] (1.000)
Step: 18599, Reward: [-529.187 -529.187 -529.187] [0.0000], Avg: [-526.052 -526.052 -526.052] (1.000)
Step: 18649, Reward: [-422.506 -422.506 -422.506] [0.0000], Avg: [-525.774 -525.774 -525.774] (1.000)
Step: 18699, Reward: [-433.087 -433.087 -433.087] [0.0000], Avg: [-525.526 -525.526 -525.526] (1.000)
Step: 18749, Reward: [-357.713 -357.713 -357.713] [0.0000], Avg: [-525.079 -525.079 -525.079] (1.000)
Step: 18799, Reward: [-416.417 -416.417 -416.417] [0.0000], Avg: [-524.79 -524.79 -524.79] (1.000)
Step: 18849, Reward: [-448.946 -448.946 -448.946] [0.0000], Avg: [-524.589 -524.589 -524.589] (1.000)
Step: 18899, Reward: [-269.171 -269.171 -269.171] [0.0000], Avg: [-523.913 -523.913 -523.913] (1.000)
Step: 18949, Reward: [-409.561 -409.561 -409.561] [0.0000], Avg: [-523.611 -523.611 -523.611] (1.000)
Step: 18999, Reward: [-355.465 -355.465 -355.465] [0.0000], Avg: [-523.169 -523.169 -523.169] (1.000)
Step: 19049, Reward: [-646.889 -646.889 -646.889] [0.0000], Avg: [-523.494 -523.494 -523.494] (1.000)
Step: 19099, Reward: [-537.848 -537.848 -537.848] [0.0000], Avg: [-523.531 -523.531 -523.531] (1.000)
Step: 19149, Reward: [-542.799 -542.799 -542.799] [0.0000], Avg: [-523.581 -523.581 -523.581] (1.000)
Step: 19199, Reward: [-397.013 -397.013 -397.013] [0.0000], Avg: [-523.252 -523.252 -523.252] (1.000)
Step: 19249, Reward: [-478.351 -478.351 -478.351] [0.0000], Avg: [-523.135 -523.135 -523.135] (1.000)
Step: 19299, Reward: [-454.235 -454.235 -454.235] [0.0000], Avg: [-522.957 -522.957 -522.957] (1.000)
Step: 19349, Reward: [-381.213 -381.213 -381.213] [0.0000], Avg: [-522.59 -522.59 -522.59] (1.000)
Step: 19399, Reward: [-438.322 -438.322 -438.322] [0.0000], Avg: [-522.373 -522.373 -522.373] (1.000)
Step: 19449, Reward: [-454.392 -454.392 -454.392] [0.0000], Avg: [-522.198 -522.198 -522.198] (1.000)
Step: 19499, Reward: [-377.398 -377.398 -377.398] [0.0000], Avg: [-521.827 -521.827 -521.827] (1.000)
Step: 19549, Reward: [-303.826 -303.826 -303.826] [0.0000], Avg: [-521.27 -521.27 -521.27] (1.000)
Step: 19599, Reward: [-436.469 -436.469 -436.469] [0.0000], Avg: [-521.053 -521.053 -521.053] (1.000)
Step: 19649, Reward: [-478.254 -478.254 -478.254] [0.0000], Avg: [-520.944 -520.944 -520.944] (1.000)
Step: 19699, Reward: [-418.513 -418.513 -418.513] [0.0000], Avg: [-520.684 -520.684 -520.684] (1.000)
Step: 19749, Reward: [-424.734 -424.734 -424.734] [0.0000], Avg: [-520.441 -520.441 -520.441] (1.000)
Step: 19799, Reward: [-494.699 -494.699 -494.699] [0.0000], Avg: [-520.376 -520.376 -520.376] (1.000)
Step: 19849, Reward: [-346.407 -346.407 -346.407] [0.0000], Avg: [-519.938 -519.938 -519.938] (1.000)
Step: 19899, Reward: [-420.25 -420.25 -420.25] [0.0000], Avg: [-519.688 -519.688 -519.688] (1.000)
Step: 19949, Reward: [-354.629 -354.629 -354.629] [0.0000], Avg: [-519.274 -519.274 -519.274] (1.000)
Step: 19999, Reward: [-399.633 -399.633 -399.633] [0.0000], Avg: [-518.975 -518.975 -518.975] (1.000)
Step: 20049, Reward: [-326.144 -326.144 -326.144] [0.0000], Avg: [-518.494 -518.494 -518.494] (1.000)
Step: 20099, Reward: [-338.822 -338.822 -338.822] [0.0000], Avg: [-518.047 -518.047 -518.047] (1.000)
Step: 20149, Reward: [-392.59 -392.59 -392.59] [0.0000], Avg: [-517.736 -517.736 -517.736] (1.000)
Step: 20199, Reward: [-417.026 -417.026 -417.026] [0.0000], Avg: [-517.487 -517.487 -517.487] (1.000)
Step: 20249, Reward: [-389.675 -389.675 -389.675] [0.0000], Avg: [-517.171 -517.171 -517.171] (1.000)
Step: 20299, Reward: [-355.081 -355.081 -355.081] [0.0000], Avg: [-516.772 -516.772 -516.772] (1.000)
Step: 20349, Reward: [-416.004 -416.004 -416.004] [0.0000], Avg: [-516.524 -516.524 -516.524] (1.000)
Step: 20399, Reward: [-306.014 -306.014 -306.014] [0.0000], Avg: [-516.008 -516.008 -516.008] (1.000)
Step: 20449, Reward: [-333.884 -333.884 -333.884] [0.0000], Avg: [-515.563 -515.563 -515.563] (1.000)
Step: 20499, Reward: [-390.537 -390.537 -390.537] [0.0000], Avg: [-515.258 -515.258 -515.258] (1.000)
Step: 20549, Reward: [-332.946 -332.946 -332.946] [0.0000], Avg: [-514.814 -514.814 -514.814] (1.000)
Step: 20599, Reward: [-480.633 -480.633 -480.633] [0.0000], Avg: [-514.731 -514.731 -514.731] (1.000)
Step: 20649, Reward: [-322.572 -322.572 -322.572] [0.0000], Avg: [-514.266 -514.266 -514.266] (1.000)
Step: 20699, Reward: [-525.053 -525.053 -525.053] [0.0000], Avg: [-514.292 -514.292 -514.292] (1.000)
Step: 20749, Reward: [-335.444 -335.444 -335.444] [0.0000], Avg: [-513.861 -513.861 -513.861] (1.000)
Step: 20799, Reward: [-491.725 -491.725 -491.725] [0.0000], Avg: [-513.808 -513.808 -513.808] (1.000)
Step: 20849, Reward: [-422.94 -422.94 -422.94] [0.0000], Avg: [-513.59 -513.59 -513.59] (1.000)
Step: 20899, Reward: [-507.852 -507.852 -507.852] [0.0000], Avg: [-513.576 -513.576 -513.576] (1.000)
Step: 20949, Reward: [-316.494 -316.494 -316.494] [0.0000], Avg: [-513.106 -513.106 -513.106] (1.000)
Step: 20999, Reward: [-373.622 -373.622 -373.622] [0.0000], Avg: [-512.774 -512.774 -512.774] (1.000)
Step: 21049, Reward: [-307.19 -307.19 -307.19] [0.0000], Avg: [-512.286 -512.286 -512.286] (1.000)
Step: 21099, Reward: [-321.418 -321.418 -321.418] [0.0000], Avg: [-511.833 -511.833 -511.833] (1.000)
Step: 21149, Reward: [-406.591 -406.591 -406.591] [0.0000], Avg: [-511.585 -511.585 -511.585] (1.000)
Step: 21199, Reward: [-432.704 -432.704 -432.704] [0.0000], Avg: [-511.399 -511.399 -511.399] (1.000)
Step: 21249, Reward: [-333.209 -333.209 -333.209] [0.0000], Avg: [-510.979 -510.979 -510.979] (1.000)
Step: 21299, Reward: [-534.003 -534.003 -534.003] [0.0000], Avg: [-511.033 -511.033 -511.033] (1.000)
Step: 21349, Reward: [-325.412 -325.412 -325.412] [0.0000], Avg: [-510.599 -510.599 -510.599] (1.000)
Step: 21399, Reward: [-411.463 -411.463 -411.463] [0.0000], Avg: [-510.367 -510.367 -510.367] (1.000)
Step: 21449, Reward: [-402.289 -402.289 -402.289] [0.0000], Avg: [-510.115 -510.115 -510.115] (1.000)
Step: 21499, Reward: [-462.31 -462.31 -462.31] [0.0000], Avg: [-510.004 -510.004 -510.004] (1.000)
Step: 21549, Reward: [-346.754 -346.754 -346.754] [0.0000], Avg: [-509.625 -509.625 -509.625] (1.000)
Step: 21599, Reward: [-538.777 -538.777 -538.777] [0.0000], Avg: [-509.693 -509.693 -509.693] (1.000)
Step: 21649, Reward: [-322.737 -322.737 -322.737] [0.0000], Avg: [-509.261 -509.261 -509.261] (1.000)
Step: 21699, Reward: [-416.735 -416.735 -416.735] [0.0000], Avg: [-509.048 -509.048 -509.048] (1.000)
Step: 21749, Reward: [-470.792 -470.792 -470.792] [0.0000], Avg: [-508.96 -508.96 -508.96] (1.000)
Step: 21799, Reward: [-333.102 -333.102 -333.102] [0.0000], Avg: [-508.556 -508.556 -508.556] (1.000)
Step: 21849, Reward: [-327.961 -327.961 -327.961] [0.0000], Avg: [-508.143 -508.143 -508.143] (1.000)
Step: 21899, Reward: [-343.768 -343.768 -343.768] [0.0000], Avg: [-507.768 -507.768 -507.768] (1.000)
Step: 21949, Reward: [-325.426 -325.426 -325.426] [0.0000], Avg: [-507.352 -507.352 -507.352] (1.000)
Step: 21999, Reward: [-460.636 -460.636 -460.636] [0.0000], Avg: [-507.246 -507.246 -507.246] (1.000)
Step: 22049, Reward: [-352.084 -352.084 -352.084] [0.0000], Avg: [-506.894 -506.894 -506.894] (1.000)
Step: 22099, Reward: [-467.834 -467.834 -467.834] [0.0000], Avg: [-506.806 -506.806 -506.806] (1.000)
Step: 22149, Reward: [-332.055 -332.055 -332.055] [0.0000], Avg: [-506.412 -506.412 -506.412] (1.000)
Step: 22199, Reward: [-379.04 -379.04 -379.04] [0.0000], Avg: [-506.125 -506.125 -506.125] (1.000)
Step: 22249, Reward: [-375.648 -375.648 -375.648] [0.0000], Avg: [-505.831 -505.831 -505.831] (1.000)
Step: 22299, Reward: [-527.704 -527.704 -527.704] [0.0000], Avg: [-505.881 -505.881 -505.881] (1.000)
Step: 22349, Reward: [-440.227 -440.227 -440.227] [0.0000], Avg: [-505.734 -505.734 -505.734] (1.000)
Step: 22399, Reward: [-402.818 -402.818 -402.818] [0.0000], Avg: [-505.504 -505.504 -505.504] (1.000)
Step: 22449, Reward: [-369.58 -369.58 -369.58] [0.0000], Avg: [-505.201 -505.201 -505.201] (1.000)
Step: 22499, Reward: [-469.032 -469.032 -469.032] [0.0000], Avg: [-505.121 -505.121 -505.121] (1.000)
Step: 22549, Reward: [-268.177 -268.177 -268.177] [0.0000], Avg: [-504.595 -504.595 -504.595] (1.000)
Step: 22599, Reward: [-344.795 -344.795 -344.795] [0.0000], Avg: [-504.242 -504.242 -504.242] (1.000)
Step: 22649, Reward: [-440.167 -440.167 -440.167] [0.0000], Avg: [-504.1 -504.1 -504.1] (1.000)
Step: 22699, Reward: [-419.932 -419.932 -419.932] [0.0000], Avg: [-503.915 -503.915 -503.915] (1.000)
Step: 22749, Reward: [-338.827 -338.827 -338.827] [0.0000], Avg: [-503.552 -503.552 -503.552] (1.000)
Step: 22799, Reward: [-370.722 -370.722 -370.722] [0.0000], Avg: [-503.261 -503.261 -503.261] (1.000)
Step: 22849, Reward: [-445.816 -445.816 -445.816] [0.0000], Avg: [-503.135 -503.135 -503.135] (1.000)
Step: 22899, Reward: [-415.287 -415.287 -415.287] [0.0000], Avg: [-502.943 -502.943 -502.943] (1.000)
Step: 22949, Reward: [-694.781 -694.781 -694.781] [0.0000], Avg: [-503.361 -503.361 -503.361] (1.000)
Step: 22999, Reward: [-403.562 -403.562 -403.562] [0.0000], Avg: [-503.144 -503.144 -503.144] (1.000)
Step: 23049, Reward: [-433.98 -433.98 -433.98] [0.0000], Avg: [-502.994 -502.994 -502.994] (1.000)
Step: 23099, Reward: [-354.756 -354.756 -354.756] [0.0000], Avg: [-502.674 -502.674 -502.674] (1.000)
Step: 23149, Reward: [-433.687 -433.687 -433.687] [0.0000], Avg: [-502.525 -502.525 -502.525] (1.000)
Step: 23199, Reward: [-428.433 -428.433 -428.433] [0.0000], Avg: [-502.365 -502.365 -502.365] (1.000)
Step: 23249, Reward: [-485.781 -485.781 -485.781] [0.0000], Avg: [-502.329 -502.329 -502.329] (1.000)
Step: 23299, Reward: [-358.532 -358.532 -358.532] [0.0000], Avg: [-502.021 -502.021 -502.021] (1.000)
Step: 23349, Reward: [-509.126 -509.126 -509.126] [0.0000], Avg: [-502.036 -502.036 -502.036] (1.000)
Step: 23399, Reward: [-313.677 -313.677 -313.677] [0.0000], Avg: [-501.633 -501.633 -501.633] (1.000)
Step: 23449, Reward: [-392.364 -392.364 -392.364] [0.0000], Avg: [-501.4 -501.4 -501.4] (1.000)
Step: 23499, Reward: [-568.702 -568.702 -568.702] [0.0000], Avg: [-501.544 -501.544 -501.544] (1.000)
Step: 23549, Reward: [-361.502 -361.502 -361.502] [0.0000], Avg: [-501.246 -501.246 -501.246] (1.000)
Step: 23599, Reward: [-382.55 -382.55 -382.55] [0.0000], Avg: [-500.995 -500.995 -500.995] (1.000)
Step: 23649, Reward: [-405.077 -405.077 -405.077] [0.0000], Avg: [-500.792 -500.792 -500.792] (1.000)
Step: 23699, Reward: [-358.972 -358.972 -358.972] [0.0000], Avg: [-500.493 -500.493 -500.493] (1.000)
Step: 23749, Reward: [-629.426 -629.426 -629.426] [0.0000], Avg: [-500.764 -500.764 -500.764] (1.000)
Step: 23799, Reward: [-335.563 -335.563 -335.563] [0.0000], Avg: [-500.417 -500.417 -500.417] (1.000)
Step: 23849, Reward: [-270.054 -270.054 -270.054] [0.0000], Avg: [-499.934 -499.934 -499.934] (1.000)
Step: 23899, Reward: [-490.517 -490.517 -490.517] [0.0000], Avg: [-499.915 -499.915 -499.915] (1.000)
Step: 23949, Reward: [-434.819 -434.819 -434.819] [0.0000], Avg: [-499.779 -499.779 -499.779] (1.000)
Step: 23999, Reward: [-418.436 -418.436 -418.436] [0.0000], Avg: [-499.609 -499.609 -499.609] (1.000)
Step: 24049, Reward: [-391.769 -391.769 -391.769] [0.0000], Avg: [-499.385 -499.385 -499.385] (1.000)
Step: 24099, Reward: [-385.081 -385.081 -385.081] [0.0000], Avg: [-499.148 -499.148 -499.148] (1.000)
Step: 24149, Reward: [-454.384 -454.384 -454.384] [0.0000], Avg: [-499.055 -499.055 -499.055] (1.000)
Step: 24199, Reward: [-338.138 -338.138 -338.138] [0.0000], Avg: [-498.723 -498.723 -498.723] (1.000)
Step: 24249, Reward: [-285.431 -285.431 -285.431] [0.0000], Avg: [-498.283 -498.283 -498.283] (1.000)
Step: 24299, Reward: [-490.213 -490.213 -490.213] [0.0000], Avg: [-498.266 -498.266 -498.266] (1.000)
Step: 24349, Reward: [-463.474 -463.474 -463.474] [0.0000], Avg: [-498.195 -498.195 -498.195] (1.000)
Step: 24399, Reward: [-344.47 -344.47 -344.47] [0.0000], Avg: [-497.88 -497.88 -497.88] (1.000)
Step: 24449, Reward: [-281.607 -281.607 -281.607] [0.0000], Avg: [-497.438 -497.438 -497.438] (1.000)
Step: 24499, Reward: [-541.027 -541.027 -541.027] [0.0000], Avg: [-497.526 -497.526 -497.526] (1.000)
Step: 24549, Reward: [-404.817 -404.817 -404.817] [0.0000], Avg: [-497.338 -497.338 -497.338] (1.000)
Step: 24599, Reward: [-290.327 -290.327 -290.327] [0.0000], Avg: [-496.917 -496.917 -496.917] (1.000)
Step: 24649, Reward: [-468.052 -468.052 -468.052] [0.0000], Avg: [-496.858 -496.858 -496.858] (1.000)
Step: 24699, Reward: [-447.08 -447.08 -447.08] [0.0000], Avg: [-496.758 -496.758 -496.758] (1.000)
Step: 24749, Reward: [-455.544 -455.544 -455.544] [0.0000], Avg: [-496.674 -496.674 -496.674] (1.000)
Step: 24799, Reward: [-390.2 -390.2 -390.2] [0.0000], Avg: [-496.46 -496.46 -496.46] (1.000)
Step: 24849, Reward: [-328.37 -328.37 -328.37] [0.0000], Avg: [-496.121 -496.121 -496.121] (1.000)
Step: 24899, Reward: [-322.091 -322.091 -322.091] [0.0000], Avg: [-495.772 -495.772 -495.772] (1.000)
Step: 24949, Reward: [-611.956 -611.956 -611.956] [0.0000], Avg: [-496.005 -496.005 -496.005] (1.000)
Step: 24999, Reward: [-468.797 -468.797 -468.797] [0.0000], Avg: [-495.95 -495.95 -495.95] (1.000)
Step: 25049, Reward: [-493.534 -493.534 -493.534] [0.0000], Avg: [-495.946 -495.946 -495.946] (1.000)
Step: 25099, Reward: [-427.412 -427.412 -427.412] [0.0000], Avg: [-495.809 -495.809 -495.809] (1.000)
Step: 25149, Reward: [-586.443 -586.443 -586.443] [0.0000], Avg: [-495.989 -495.989 -495.989] (1.000)
Step: 25199, Reward: [-409.294 -409.294 -409.294] [0.0000], Avg: [-495.817 -495.817 -495.817] (1.000)
Step: 25249, Reward: [-369.167 -369.167 -369.167] [0.0000], Avg: [-495.566 -495.566 -495.566] (1.000)
Step: 25299, Reward: [-392.997 -392.997 -392.997] [0.0000], Avg: [-495.364 -495.364 -495.364] (1.000)
Step: 25349, Reward: [-470.663 -470.663 -470.663] [0.0000], Avg: [-495.315 -495.315 -495.315] (1.000)
Step: 25399, Reward: [-550.585 -550.585 -550.585] [0.0000], Avg: [-495.424 -495.424 -495.424] (1.000)
Step: 25449, Reward: [-452.668 -452.668 -452.668] [0.0000], Avg: [-495.34 -495.34 -495.34] (1.000)
Step: 25499, Reward: [-426.634 -426.634 -426.634] [0.0000], Avg: [-495.205 -495.205 -495.205] (1.000)
Step: 25549, Reward: [-384.09 -384.09 -384.09] [0.0000], Avg: [-494.988 -494.988 -494.988] (1.000)
Step: 25599, Reward: [-470.342 -470.342 -470.342] [0.0000], Avg: [-494.94 -494.94 -494.94] (1.000)
Step: 25649, Reward: [-309.191 -309.191 -309.191] [0.0000], Avg: [-494.577 -494.577 -494.577] (1.000)
Step: 25699, Reward: [-378.986 -378.986 -378.986] [0.0000], Avg: [-494.353 -494.353 -494.353] (1.000)
Step: 25749, Reward: [-363.92 -363.92 -363.92] [0.0000], Avg: [-494.099 -494.099 -494.099] (1.000)
Step: 25799, Reward: [-445.657 -445.657 -445.657] [0.0000], Avg: [-494.005 -494.005 -494.005] (1.000)
Step: 25849, Reward: [-342.993 -342.993 -342.993] [0.0000], Avg: [-493.713 -493.713 -493.713] (1.000)
Step: 25899, Reward: [-458.951 -458.951 -458.951] [0.0000], Avg: [-493.646 -493.646 -493.646] (1.000)
Step: 25949, Reward: [-332.247 -332.247 -332.247] [0.0000], Avg: [-493.335 -493.335 -493.335] (1.000)
Step: 25999, Reward: [-335.764 -335.764 -335.764] [0.0000], Avg: [-493.032 -493.032 -493.032] (1.000)
Step: 26049, Reward: [-338.993 -338.993 -338.993] [0.0000], Avg: [-492.737 -492.737 -492.737] (1.000)
Step: 26099, Reward: [-379.195 -379.195 -379.195] [0.0000], Avg: [-492.519 -492.519 -492.519] (1.000)
Step: 26149, Reward: [-410.024 -410.024 -410.024] [0.0000], Avg: [-492.361 -492.361 -492.361] (1.000)
Step: 26199, Reward: [-402.38 -402.38 -402.38] [0.0000], Avg: [-492.19 -492.19 -492.19] (1.000)
Step: 26249, Reward: [-368.647 -368.647 -368.647] [0.0000], Avg: [-491.954 -491.954 -491.954] (1.000)
Step: 26299, Reward: [-337.743 -337.743 -337.743] [0.0000], Avg: [-491.661 -491.661 -491.661] (1.000)
Step: 26349, Reward: [-374.922 -374.922 -374.922] [0.0000], Avg: [-491.44 -491.44 -491.44] (1.000)
Step: 26399, Reward: [-390.983 -390.983 -390.983] [0.0000], Avg: [-491.249 -491.249 -491.249] (1.000)
Step: 26449, Reward: [-444.714 -444.714 -444.714] [0.0000], Avg: [-491.161 -491.161 -491.161] (1.000)
Step: 26499, Reward: [-332.661 -332.661 -332.661] [0.0000], Avg: [-490.862 -490.862 -490.862] (1.000)
Step: 26549, Reward: [-400.022 -400.022 -400.022] [0.0000], Avg: [-490.691 -490.691 -490.691] (1.000)
Step: 26599, Reward: [-420.111 -420.111 -420.111] [0.0000], Avg: [-490.559 -490.559 -490.559] (1.000)
Step: 26649, Reward: [-405.766 -405.766 -405.766] [0.0000], Avg: [-490.399 -490.399 -490.399] (1.000)
Step: 26699, Reward: [-371.896 -371.896 -371.896] [0.0000], Avg: [-490.178 -490.178 -490.178] (1.000)
Step: 26749, Reward: [-331.448 -331.448 -331.448] [0.0000], Avg: [-489.881 -489.881 -489.881] (1.000)
Step: 26799, Reward: [-472.624 -472.624 -472.624] [0.0000], Avg: [-489.849 -489.849 -489.849] (1.000)
Step: 26849, Reward: [-407.535 -407.535 -407.535] [0.0000], Avg: [-489.695 -489.695 -489.695] (1.000)
Step: 26899, Reward: [-448.079 -448.079 -448.079] [0.0000], Avg: [-489.618 -489.618 -489.618] (1.000)
Step: 26949, Reward: [-311.272 -311.272 -311.272] [0.0000], Avg: [-489.287 -489.287 -489.287] (1.000)
Step: 26999, Reward: [-328.963 -328.963 -328.963] [0.0000], Avg: [-488.99 -488.99 -488.99] (1.000)
Step: 27049, Reward: [-302.099 -302.099 -302.099] [0.0000], Avg: [-488.645 -488.645 -488.645] (1.000)
Step: 27099, Reward: [-471.078 -471.078 -471.078] [0.0000], Avg: [-488.612 -488.612 -488.612] (1.000)
Step: 27149, Reward: [-334.51 -334.51 -334.51] [0.0000], Avg: [-488.329 -488.329 -488.329] (1.000)
Step: 27199, Reward: [-479.89 -479.89 -479.89] [0.0000], Avg: [-488.313 -488.313 -488.313] (1.000)
Step: 27249, Reward: [-357.008 -357.008 -357.008] [0.0000], Avg: [-488.072 -488.072 -488.072] (1.000)
Step: 27299, Reward: [-428.208 -428.208 -428.208] [0.0000], Avg: [-487.962 -487.962 -487.962] (1.000)
Step: 27349, Reward: [-255.207 -255.207 -255.207] [0.0000], Avg: [-487.537 -487.537 -487.537] (1.000)
Step: 27399, Reward: [-483.829 -483.829 -483.829] [0.0000], Avg: [-487.53 -487.53 -487.53] (1.000)
Step: 27449, Reward: [-421.793 -421.793 -421.793] [0.0000], Avg: [-487.41 -487.41 -487.41] (1.000)
Step: 27499, Reward: [-461.116 -461.116 -461.116] [0.0000], Avg: [-487.363 -487.363 -487.363] (1.000)
Step: 27549, Reward: [-296.258 -296.258 -296.258] [0.0000], Avg: [-487.016 -487.016 -487.016] (1.000)
Step: 27599, Reward: [-430.511 -430.511 -430.511] [0.0000], Avg: [-486.913 -486.913 -486.913] (1.000)
Step: 27649, Reward: [-391.838 -391.838 -391.838] [0.0000], Avg: [-486.742 -486.742 -486.742] (1.000)
Step: 27699, Reward: [-328.083 -328.083 -328.083] [0.0000], Avg: [-486.455 -486.455 -486.455] (1.000)
Step: 27749, Reward: [-336.112 -336.112 -336.112] [0.0000], Avg: [-486.184 -486.184 -486.184] (1.000)
Step: 27799, Reward: [-369.552 -369.552 -369.552] [0.0000], Avg: [-485.975 -485.975 -485.975] (1.000)
Step: 27849, Reward: [-434.743 -434.743 -434.743] [0.0000], Avg: [-485.883 -485.883 -485.883] (1.000)
Step: 27899, Reward: [-457.544 -457.544 -457.544] [0.0000], Avg: [-485.832 -485.832 -485.832] (1.000)
Step: 27949, Reward: [-330.591 -330.591 -330.591] [0.0000], Avg: [-485.554 -485.554 -485.554] (1.000)
Step: 27999, Reward: [-339.107 -339.107 -339.107] [0.0000], Avg: [-485.293 -485.293 -485.293] (1.000)
Step: 28049, Reward: [-412.056 -412.056 -412.056] [0.0000], Avg: [-485.162 -485.162 -485.162] (1.000)
Step: 28099, Reward: [-441.259 -441.259 -441.259] [0.0000], Avg: [-485.084 -485.084 -485.084] (1.000)
Step: 28149, Reward: [-446.989 -446.989 -446.989] [0.0000], Avg: [-485.016 -485.016 -485.016] (1.000)
Step: 28199, Reward: [-328.697 -328.697 -328.697] [0.0000], Avg: [-484.739 -484.739 -484.739] (1.000)
Step: 28249, Reward: [-281.522 -281.522 -281.522] [0.0000], Avg: [-484.379 -484.379 -484.379] (1.000)
Step: 28299, Reward: [-382.609 -382.609 -382.609] [0.0000], Avg: [-484.2 -484.2 -484.2] (1.000)
Step: 28349, Reward: [-307.82 -307.82 -307.82] [0.0000], Avg: [-483.888 -483.888 -483.888] (1.000)
Step: 28399, Reward: [-366.714 -366.714 -366.714] [0.0000], Avg: [-483.682 -483.682 -483.682] (1.000)
Step: 28449, Reward: [-466.686 -466.686 -466.686] [0.0000], Avg: [-483.652 -483.652 -483.652] (1.000)
Step: 28499, Reward: [-436.5 -436.5 -436.5] [0.0000], Avg: [-483.57 -483.57 -483.57] (1.000)
Step: 28549, Reward: [-375.255 -375.255 -375.255] [0.0000], Avg: [-483.38 -483.38 -483.38] (1.000)
Step: 28599, Reward: [-357.224 -357.224 -357.224] [0.0000], Avg: [-483.159 -483.159 -483.159] (1.000)
Step: 28649, Reward: [-361.524 -361.524 -361.524] [0.0000], Avg: [-482.947 -482.947 -482.947] (1.000)
Step: 28699, Reward: [-355.325 -355.325 -355.325] [0.0000], Avg: [-482.725 -482.725 -482.725] (1.000)
Step: 28749, Reward: [-448.235 -448.235 -448.235] [0.0000], Avg: [-482.665 -482.665 -482.665] (1.000)
Step: 28799, Reward: [-325.113 -325.113 -325.113] [0.0000], Avg: [-482.391 -482.391 -482.391] (1.000)
Step: 28849, Reward: [-381.167 -381.167 -381.167] [0.0000], Avg: [-482.216 -482.216 -482.216] (1.000)
Step: 28899, Reward: [-406.923 -406.923 -406.923] [0.0000], Avg: [-482.086 -482.086 -482.086] (1.000)
Step: 28949, Reward: [-581.312 -581.312 -581.312] [0.0000], Avg: [-482.257 -482.257 -482.257] (1.000)
Step: 28999, Reward: [-391.494 -391.494 -391.494] [0.0000], Avg: [-482.1 -482.1 -482.1] (1.000)
Step: 29049, Reward: [-366.857 -366.857 -366.857] [0.0000], Avg: [-481.902 -481.902 -481.902] (1.000)
Step: 29099, Reward: [-339.96 -339.96 -339.96] [0.0000], Avg: [-481.658 -481.658 -481.658] (1.000)
Step: 29149, Reward: [-389.599 -389.599 -389.599] [0.0000], Avg: [-481.5 -481.5 -481.5] (1.000)
Step: 29199, Reward: [-428.194 -428.194 -428.194] [0.0000], Avg: [-481.409 -481.409 -481.409] (1.000)
Step: 29249, Reward: [-313.674 -313.674 -313.674] [0.0000], Avg: [-481.122 -481.122 -481.122] (1.000)
Step: 29299, Reward: [-410.07 -410.07 -410.07] [0.0000], Avg: [-481.001 -481.001 -481.001] (1.000)
Step: 29349, Reward: [-341.775 -341.775 -341.775] [0.0000], Avg: [-480.764 -480.764 -480.764] (1.000)
Step: 29399, Reward: [-360.136 -360.136 -360.136] [0.0000], Avg: [-480.559 -480.559 -480.559] (1.000)
Step: 29449, Reward: [-415.525 -415.525 -415.525] [0.0000], Avg: [-480.448 -480.448 -480.448] (1.000)
Step: 29499, Reward: [-375.648 -375.648 -375.648] [0.0000], Avg: [-480.271 -480.271 -480.271] (1.000)
Step: 29549, Reward: [-467.007 -467.007 -467.007] [0.0000], Avg: [-480.248 -480.248 -480.248] (1.000)
Step: 29599, Reward: [-461.375 -461.375 -461.375] [0.0000], Avg: [-480.216 -480.216 -480.216] (1.000)
Step: 29649, Reward: [-476.137 -476.137 -476.137] [0.0000], Avg: [-480.209 -480.209 -480.209] (1.000)
Step: 29699, Reward: [-426.759 -426.759 -426.759] [0.0000], Avg: [-480.119 -480.119 -480.119] (1.000)
Step: 29749, Reward: [-427.655 -427.655 -427.655] [0.0000], Avg: [-480.031 -480.031 -480.031] (1.000)
Step: 29799, Reward: [-452.644 -452.644 -452.644] [0.0000], Avg: [-479.985 -479.985 -479.985] (1.000)
Step: 29849, Reward: [-382.474 -382.474 -382.474] [0.0000], Avg: [-479.822 -479.822 -479.822] (1.000)
Step: 29899, Reward: [-390.277 -390.277 -390.277] [0.0000], Avg: [-479.672 -479.672 -479.672] (1.000)
Step: 29949, Reward: [-327.368 -327.368 -327.368] [0.0000], Avg: [-479.418 -479.418 -479.418] (1.000)
Step: 29999, Reward: [-321.55 -321.55 -321.55] [0.0000], Avg: [-479.155 -479.155 -479.155] (1.000)
Step: 30049, Reward: [-346.152 -346.152 -346.152] [0.0000], Avg: [-478.934 -478.934 -478.934] (1.000)
Step: 30099, Reward: [-370.178 -370.178 -370.178] [0.0000], Avg: [-478.753 -478.753 -478.753] (1.000)
Step: 30149, Reward: [-424.337 -424.337 -424.337] [0.0000], Avg: [-478.663 -478.663 -478.663] (1.000)
Step: 30199, Reward: [-368.212 -368.212 -368.212] [0.0000], Avg: [-478.48 -478.48 -478.48] (1.000)
Step: 30249, Reward: [-406.776 -406.776 -406.776] [0.0000], Avg: [-478.361 -478.361 -478.361] (1.000)
Step: 30299, Reward: [-325.87 -325.87 -325.87] [0.0000], Avg: [-478.11 -478.11 -478.11] (1.000)
Step: 30349, Reward: [-434.133 -434.133 -434.133] [0.0000], Avg: [-478.037 -478.037 -478.037] (1.000)
Step: 30399, Reward: [-477.739 -477.739 -477.739] [0.0000], Avg: [-478.037 -478.037 -478.037] (1.000)
Step: 30449, Reward: [-492.107 -492.107 -492.107] [0.0000], Avg: [-478.06 -478.06 -478.06] (1.000)
Step: 30499, Reward: [-456.606 -456.606 -456.606] [0.0000], Avg: [-478.025 -478.025 -478.025] (1.000)
Step: 30549, Reward: [-376.734 -376.734 -376.734] [0.0000], Avg: [-477.859 -477.859 -477.859] (1.000)
Step: 30599, Reward: [-365.036 -365.036 -365.036] [0.0000], Avg: [-477.675 -477.675 -477.675] (1.000)
Step: 30649, Reward: [-309.313 -309.313 -309.313] [0.0000], Avg: [-477.4 -477.4 -477.4] (1.000)
Step: 30699, Reward: [-409.444 -409.444 -409.444] [0.0000], Avg: [-477.289 -477.289 -477.289] (1.000)
Step: 30749, Reward: [-517.62 -517.62 -517.62] [0.0000], Avg: [-477.355 -477.355 -477.355] (1.000)
Step: 30799, Reward: [-445.244 -445.244 -445.244] [0.0000], Avg: [-477.303 -477.303 -477.303] (1.000)
Step: 30849, Reward: [-419.551 -419.551 -419.551] [0.0000], Avg: [-477.209 -477.209 -477.209] (1.000)
Step: 30899, Reward: [-416.516 -416.516 -416.516] [0.0000], Avg: [-477.111 -477.111 -477.111] (1.000)
Step: 30949, Reward: [-441.452 -441.452 -441.452] [0.0000], Avg: [-477.053 -477.053 -477.053] (1.000)
Step: 30999, Reward: [-465.45 -465.45 -465.45] [0.0000], Avg: [-477.034 -477.034 -477.034] (1.000)
Step: 31049, Reward: [-500.647 -500.647 -500.647] [0.0000], Avg: [-477.073 -477.073 -477.073] (1.000)
Step: 31099, Reward: [-491.68 -491.68 -491.68] [0.0000], Avg: [-477.096 -477.096 -477.096] (1.000)
Step: 31149, Reward: [-339.778 -339.778 -339.778] [0.0000], Avg: [-476.876 -476.876 -476.876] (1.000)
Step: 31199, Reward: [-459.229 -459.229 -459.229] [0.0000], Avg: [-476.847 -476.847 -476.847] (1.000)
Step: 31249, Reward: [-399.453 -399.453 -399.453] [0.0000], Avg: [-476.723 -476.723 -476.723] (1.000)
Step: 31299, Reward: [-576.879 -576.879 -576.879] [0.0000], Avg: [-476.883 -476.883 -476.883] (1.000)
Step: 31349, Reward: [-370.278 -370.278 -370.278] [0.0000], Avg: [-476.713 -476.713 -476.713] (1.000)
Step: 31399, Reward: [-379.15 -379.15 -379.15] [0.0000], Avg: [-476.558 -476.558 -476.558] (1.000)
Step: 31449, Reward: [-307.596 -307.596 -307.596] [0.0000], Avg: [-476.289 -476.289 -476.289] (1.000)
Step: 31499, Reward: [-356.858 -356.858 -356.858] [0.0000], Avg: [-476.1 -476.1 -476.1] (1.000)
Step: 31549, Reward: [-327.454 -327.454 -327.454] [0.0000], Avg: [-475.864 -475.864 -475.864] (1.000)
Step: 31599, Reward: [-354.589 -354.589 -354.589] [0.0000], Avg: [-475.672 -475.672 -475.672] (1.000)
Step: 31649, Reward: [-345.766 -345.766 -345.766] [0.0000], Avg: [-475.467 -475.467 -475.467] (1.000)
Step: 31699, Reward: [-283.56 -283.56 -283.56] [0.0000], Avg: [-475.165 -475.165 -475.165] (1.000)
Step: 31749, Reward: [-357.815 -357.815 -357.815] [0.0000], Avg: [-474.98 -474.98 -474.98] (1.000)
Step: 31799, Reward: [-314.664 -314.664 -314.664] [0.0000], Avg: [-474.728 -474.728 -474.728] (1.000)
Step: 31849, Reward: [-489.915 -489.915 -489.915] [0.0000], Avg: [-474.751 -474.751 -474.751] (1.000)
Step: 31899, Reward: [-270.078 -270.078 -270.078] [0.0000], Avg: [-474.431 -474.431 -474.431] (1.000)
Step: 31949, Reward: [-412.266 -412.266 -412.266] [0.0000], Avg: [-474.333 -474.333 -474.333] (1.000)
Step: 31999, Reward: [-389.12 -389.12 -389.12] [0.0000], Avg: [-474.2 -474.2 -474.2] (1.000)
Step: 32049, Reward: [-529.635 -529.635 -529.635] [0.0000], Avg: [-474.287 -474.287 -474.287] (1.000)
Step: 32099, Reward: [-387.94 -387.94 -387.94] [0.0000], Avg: [-474.152 -474.152 -474.152] (1.000)
Step: 32149, Reward: [-392.602 -392.602 -392.602] [0.0000], Avg: [-474.025 -474.025 -474.025] (1.000)
Step: 32199, Reward: [-448.353 -448.353 -448.353] [0.0000], Avg: [-473.986 -473.986 -473.986] (1.000)
Step: 32249, Reward: [-452.354 -452.354 -452.354] [0.0000], Avg: [-473.952 -473.952 -473.952] (1.000)
Step: 32299, Reward: [-486.968 -486.968 -486.968] [0.0000], Avg: [-473.972 -473.972 -473.972] (1.000)
Step: 32349, Reward: [-439.243 -439.243 -439.243] [0.0000], Avg: [-473.918 -473.918 -473.918] (1.000)
Step: 32399, Reward: [-354.401 -354.401 -354.401] [0.0000], Avg: [-473.734 -473.734 -473.734] (1.000)
Step: 32449, Reward: [-469.02 -469.02 -469.02] [0.0000], Avg: [-473.727 -473.727 -473.727] (1.000)
Step: 32499, Reward: [-468.014 -468.014 -468.014] [0.0000], Avg: [-473.718 -473.718 -473.718] (1.000)
Step: 32549, Reward: [-346.594 -346.594 -346.594] [0.0000], Avg: [-473.523 -473.523 -473.523] (1.000)
Step: 32599, Reward: [-440.172 -440.172 -440.172] [0.0000], Avg: [-473.472 -473.472 -473.472] (1.000)
Step: 32649, Reward: [-286.99 -286.99 -286.99] [0.0000], Avg: [-473.186 -473.186 -473.186] (1.000)
Step: 32699, Reward: [-440.278 -440.278 -440.278] [0.0000], Avg: [-473.136 -473.136 -473.136] (1.000)
Step: 32749, Reward: [-513.467 -513.467 -513.467] [0.0000], Avg: [-473.197 -473.197 -473.197] (1.000)
Step: 32799, Reward: [-337.911 -337.911 -337.911] [0.0000], Avg: [-472.991 -472.991 -472.991] (1.000)
Step: 32849, Reward: [-394.87 -394.87 -394.87] [0.0000], Avg: [-472.872 -472.872 -472.872] (1.000)
Step: 32899, Reward: [-372.915 -372.915 -372.915] [0.0000], Avg: [-472.72 -472.72 -472.72] (1.000)
Step: 32949, Reward: [-337.725 -337.725 -337.725] [0.0000], Avg: [-472.515 -472.515 -472.515] (1.000)
Step: 32999, Reward: [-320.007 -320.007 -320.007] [0.0000], Avg: [-472.284 -472.284 -472.284] (1.000)
Step: 33049, Reward: [-329.138 -329.138 -329.138] [0.0000], Avg: [-472.068 -472.068 -472.068] (1.000)
Step: 33099, Reward: [-497.188 -497.188 -497.188] [0.0000], Avg: [-472.106 -472.106 -472.106] (1.000)
Step: 33149, Reward: [-422.375 -422.375 -422.375] [0.0000], Avg: [-472.031 -472.031 -472.031] (1.000)
Step: 33199, Reward: [-389.769 -389.769 -389.769] [0.0000], Avg: [-471.907 -471.907 -471.907] (1.000)
Step: 33249, Reward: [-549.186 -549.186 -549.186] [0.0000], Avg: [-472.023 -472.023 -472.023] (1.000)
Step: 33299, Reward: [-464.796 -464.796 -464.796] [0.0000], Avg: [-472.012 -472.012 -472.012] (1.000)
Step: 33349, Reward: [-369.523 -369.523 -369.523] [0.0000], Avg: [-471.858 -471.858 -471.858] (1.000)
Step: 33399, Reward: [-331.659 -331.659 -331.659] [0.0000], Avg: [-471.649 -471.649 -471.649] (1.000)
Step: 33449, Reward: [-360.852 -360.852 -360.852] [0.0000], Avg: [-471.483 -471.483 -471.483] (1.000)
Step: 33499, Reward: [-296.959 -296.959 -296.959] [0.0000], Avg: [-471.222 -471.222 -471.222] (1.000)
Step: 33549, Reward: [-317.677 -317.677 -317.677] [0.0000], Avg: [-470.994 -470.994 -470.994] (1.000)
Step: 33599, Reward: [-460.336 -460.336 -460.336] [0.0000], Avg: [-470.978 -470.978 -470.978] (1.000)
Step: 33649, Reward: [-783.937 -783.937 -783.937] [0.0000], Avg: [-471.443 -471.443 -471.443] (1.000)
Step: 33699, Reward: [-505.655 -505.655 -505.655] [0.0000], Avg: [-471.494 -471.494 -471.494] (1.000)
Step: 33749, Reward: [-511.21 -511.21 -511.21] [0.0000], Avg: [-471.552 -471.552 -471.552] (1.000)
Step: 33799, Reward: [-482.853 -482.853 -482.853] [0.0000], Avg: [-471.569 -471.569 -471.569] (1.000)
Step: 33849, Reward: [-414.153 -414.153 -414.153] [0.0000], Avg: [-471.484 -471.484 -471.484] (1.000)
Step: 33899, Reward: [-489.897 -489.897 -489.897] [0.0000], Avg: [-471.511 -471.511 -471.511] (1.000)
Step: 33949, Reward: [-561.41 -561.41 -561.41] [0.0000], Avg: [-471.644 -471.644 -471.644] (1.000)
Step: 33999, Reward: [-454.052 -454.052 -454.052] [0.0000], Avg: [-471.618 -471.618 -471.618] (1.000)
Step: 34049, Reward: [-521.191 -521.191 -521.191] [0.0000], Avg: [-471.691 -471.691 -471.691] (1.000)
Step: 34099, Reward: [-405.114 -405.114 -405.114] [0.0000], Avg: [-471.593 -471.593 -471.593] (1.000)
Step: 34149, Reward: [-429.669 -429.669 -429.669] [0.0000], Avg: [-471.532 -471.532 -471.532] (1.000)
Step: 34199, Reward: [-454.375 -454.375 -454.375] [0.0000], Avg: [-471.507 -471.507 -471.507] (1.000)
Step: 34249, Reward: [-289.963 -289.963 -289.963] [0.0000], Avg: [-471.242 -471.242 -471.242] (1.000)
Step: 34299, Reward: [-318.759 -318.759 -318.759] [0.0000], Avg: [-471.019 -471.019 -471.019] (1.000)
Step: 34349, Reward: [-305.381 -305.381 -305.381] [0.0000], Avg: [-470.778 -470.778 -470.778] (1.000)
Step: 34399, Reward: [-353.212 -353.212 -353.212] [0.0000], Avg: [-470.607 -470.607 -470.607] (1.000)
Step: 34449, Reward: [-325.909 -325.909 -325.909] [0.0000], Avg: [-470.397 -470.397 -470.397] (1.000)
Step: 34499, Reward: [-431.346 -431.346 -431.346] [0.0000], Avg: [-470.341 -470.341 -470.341] (1.000)
Step: 34549, Reward: [-288.017 -288.017 -288.017] [0.0000], Avg: [-470.077 -470.077 -470.077] (1.000)
Step: 34599, Reward: [-509.135 -509.135 -509.135] [0.0000], Avg: [-470.133 -470.133 -470.133] (1.000)
Step: 34649, Reward: [-338.822 -338.822 -338.822] [0.0000], Avg: [-469.944 -469.944 -469.944] (1.000)
Step: 34699, Reward: [-458.466 -458.466 -458.466] [0.0000], Avg: [-469.927 -469.927 -469.927] (1.000)
Step: 34749, Reward: [-339.049 -339.049 -339.049] [0.0000], Avg: [-469.739 -469.739 -469.739] (1.000)
Step: 34799, Reward: [-462.24 -462.24 -462.24] [0.0000], Avg: [-469.728 -469.728 -469.728] (1.000)
Step: 34849, Reward: [-494.912 -494.912 -494.912] [0.0000], Avg: [-469.764 -469.764 -469.764] (1.000)
Step: 34899, Reward: [-322.259 -322.259 -322.259] [0.0000], Avg: [-469.553 -469.553 -469.553] (1.000)
Step: 34949, Reward: [-555.937 -555.937 -555.937] [0.0000], Avg: [-469.677 -469.677 -469.677] (1.000)
Step: 34999, Reward: [-421.147 -421.147 -421.147] [0.0000], Avg: [-469.607 -469.607 -469.607] (1.000)
Step: 35049, Reward: [-413.57 -413.57 -413.57] [0.0000], Avg: [-469.527 -469.527 -469.527] (1.000)
Step: 35099, Reward: [-433.444 -433.444 -433.444] [0.0000], Avg: [-469.476 -469.476 -469.476] (1.000)
Step: 35149, Reward: [-260.094 -260.094 -260.094] [0.0000], Avg: [-469.178 -469.178 -469.178] (1.000)
Step: 35199, Reward: [-369.66 -369.66 -369.66] [0.0000], Avg: [-469.037 -469.037 -469.037] (1.000)
Step: 35249, Reward: [-479.935 -479.935 -479.935] [0.0000], Avg: [-469.052 -469.052 -469.052] (1.000)
Step: 35299, Reward: [-614.217 -614.217 -614.217] [0.0000], Avg: [-469.258 -469.258 -469.258] (1.000)
Step: 35349, Reward: [-430.625 -430.625 -430.625] [0.0000], Avg: [-469.203 -469.203 -469.203] (1.000)
Step: 35399, Reward: [-337.091 -337.091 -337.091] [0.0000], Avg: [-469.017 -469.017 -469.017] (1.000)
Step: 35449, Reward: [-432.782 -432.782 -432.782] [0.0000], Avg: [-468.966 -468.966 -468.966] (1.000)
Step: 35499, Reward: [-362.312 -362.312 -362.312] [0.0000], Avg: [-468.815 -468.815 -468.815] (1.000)
Step: 35549, Reward: [-394.546 -394.546 -394.546] [0.0000], Avg: [-468.711 -468.711 -468.711] (1.000)
Step: 35599, Reward: [-358.495 -358.495 -358.495] [0.0000], Avg: [-468.556 -468.556 -468.556] (1.000)
Step: 35649, Reward: [-477.761 -477.761 -477.761] [0.0000], Avg: [-468.569 -468.569 -468.569] (1.000)
Step: 35699, Reward: [-377.132 -377.132 -377.132] [0.0000], Avg: [-468.441 -468.441 -468.441] (1.000)
Step: 35749, Reward: [-391.373 -391.373 -391.373] [0.0000], Avg: [-468.333 -468.333 -468.333] (1.000)
Step: 35799, Reward: [-257.702 -257.702 -257.702] [0.0000], Avg: [-468.039 -468.039 -468.039] (1.000)
Step: 35849, Reward: [-337.33 -337.33 -337.33] [0.0000], Avg: [-467.857 -467.857 -467.857] (1.000)
Step: 35899, Reward: [-296.495 -296.495 -296.495] [0.0000], Avg: [-467.618 -467.618 -467.618] (1.000)
Step: 35949, Reward: [-293.281 -293.281 -293.281] [0.0000], Avg: [-467.376 -467.376 -467.376] (1.000)
Step: 35999, Reward: [-451.173 -451.173 -451.173] [0.0000], Avg: [-467.353 -467.353 -467.353] (1.000)
Step: 36049, Reward: [-342.051 -342.051 -342.051] [0.0000], Avg: [-467.179 -467.179 -467.179] (1.000)
Step: 36099, Reward: [-605.104 -605.104 -605.104] [0.0000], Avg: [-467.37 -467.37 -467.37] (1.000)
Step: 36149, Reward: [-314.028 -314.028 -314.028] [0.0000], Avg: [-467.158 -467.158 -467.158] (1.000)
Step: 36199, Reward: [-351.918 -351.918 -351.918] [0.0000], Avg: [-466.999 -466.999 -466.999] (1.000)
Step: 36249, Reward: [-385.367 -385.367 -385.367] [0.0000], Avg: [-466.886 -466.886 -466.886] (1.000)
Step: 36299, Reward: [-310.752 -310.752 -310.752] [0.0000], Avg: [-466.671 -466.671 -466.671] (1.000)
Step: 36349, Reward: [-472.449 -472.449 -472.449] [0.0000], Avg: [-466.679 -466.679 -466.679] (1.000)
Step: 36399, Reward: [-418.076 -418.076 -418.076] [0.0000], Avg: [-466.613 -466.613 -466.613] (1.000)
Step: 36449, Reward: [-459.587 -459.587 -459.587] [0.0000], Avg: [-466.603 -466.603 -466.603] (1.000)
Step: 36499, Reward: [-463.578 -463.578 -463.578] [0.0000], Avg: [-466.599 -466.599 -466.599] (1.000)
Step: 36549, Reward: [-419.866 -419.866 -419.866] [0.0000], Avg: [-466.535 -466.535 -466.535] (1.000)
Step: 36599, Reward: [-259.872 -259.872 -259.872] [0.0000], Avg: [-466.252 -466.252 -466.252] (1.000)
Step: 36649, Reward: [-375.16 -375.16 -375.16] [0.0000], Avg: [-466.128 -466.128 -466.128] (1.000)
Step: 36699, Reward: [-382.776 -382.776 -382.776] [0.0000], Avg: [-466.015 -466.015 -466.015] (1.000)
Step: 36749, Reward: [-363.779 -363.779 -363.779] [0.0000], Avg: [-465.876 -465.876 -465.876] (1.000)
Step: 36799, Reward: [-465.398 -465.398 -465.398] [0.0000], Avg: [-465.875 -465.875 -465.875] (1.000)
Step: 36849, Reward: [-421.879 -421.879 -421.879] [0.0000], Avg: [-465.815 -465.815 -465.815] (1.000)
Step: 36899, Reward: [-479.893 -479.893 -479.893] [0.0000], Avg: [-465.834 -465.834 -465.834] (1.000)
Step: 36949, Reward: [-456.151 -456.151 -456.151] [0.0000], Avg: [-465.821 -465.821 -465.821] (1.000)
Step: 36999, Reward: [-466.651 -466.651 -466.651] [0.0000], Avg: [-465.822 -465.822 -465.822] (1.000)
Step: 37049, Reward: [-459.549 -459.549 -459.549] [0.0000], Avg: [-465.814 -465.814 -465.814] (1.000)
Step: 37099, Reward: [-698.861 -698.861 -698.861] [0.0000], Avg: [-466.128 -466.128 -466.128] (1.000)
Step: 37149, Reward: [-358.39 -358.39 -358.39] [0.0000], Avg: [-465.983 -465.983 -465.983] (1.000)
Step: 37199, Reward: [-526.349 -526.349 -526.349] [0.0000], Avg: [-466.064 -466.064 -466.064] (1.000)
Step: 37249, Reward: [-570.098 -570.098 -570.098] [0.0000], Avg: [-466.204 -466.204 -466.204] (1.000)
Step: 37299, Reward: [-487.435 -487.435 -487.435] [0.0000], Avg: [-466.232 -466.232 -466.232] (1.000)
Step: 37349, Reward: [-343.224 -343.224 -343.224] [0.0000], Avg: [-466.067 -466.067 -466.067] (1.000)
Step: 37399, Reward: [-255.289 -255.289 -255.289] [0.0000], Avg: [-465.786 -465.786 -465.786] (1.000)
Step: 37449, Reward: [-441.27 -441.27 -441.27] [0.0000], Avg: [-465.753 -465.753 -465.753] (1.000)
Step: 37499, Reward: [-346.085 -346.085 -346.085] [0.0000], Avg: [-465.593 -465.593 -465.593] (1.000)
Step: 37549, Reward: [-358.729 -358.729 -358.729] [0.0000], Avg: [-465.451 -465.451 -465.451] (1.000)
Step: 37599, Reward: [-342.332 -342.332 -342.332] [0.0000], Avg: [-465.287 -465.287 -465.287] (1.000)
Step: 37649, Reward: [-431.415 -431.415 -431.415] [0.0000], Avg: [-465.242 -465.242 -465.242] (1.000)
Step: 37699, Reward: [-336.581 -336.581 -336.581] [0.0000], Avg: [-465.072 -465.072 -465.072] (1.000)
Step: 37749, Reward: [-421.717 -421.717 -421.717] [0.0000], Avg: [-465.014 -465.014 -465.014] (1.000)
Step: 37799, Reward: [-273.951 -273.951 -273.951] [0.0000], Avg: [-464.762 -464.762 -464.762] (1.000)
Step: 37849, Reward: [-315.843 -315.843 -315.843] [0.0000], Avg: [-464.565 -464.565 -464.565] (1.000)
Step: 37899, Reward: [-304.809 -304.809 -304.809] [0.0000], Avg: [-464.354 -464.354 -464.354] (1.000)
Step: 37949, Reward: [-378.544 -378.544 -378.544] [0.0000], Avg: [-464.241 -464.241 -464.241] (1.000)
Step: 37999, Reward: [-408.917 -408.917 -408.917] [0.0000], Avg: [-464.168 -464.168 -464.168] (1.000)
Step: 38049, Reward: [-632.909 -632.909 -632.909] [0.0000], Avg: [-464.39 -464.39 -464.39] (1.000)
Step: 38099, Reward: [-350.687 -350.687 -350.687] [0.0000], Avg: [-464.241 -464.241 -464.241] (1.000)
Step: 38149, Reward: [-344.469 -344.469 -344.469] [0.0000], Avg: [-464.084 -464.084 -464.084] (1.000)
Step: 38199, Reward: [-380.629 -380.629 -380.629] [0.0000], Avg: [-463.975 -463.975 -463.975] (1.000)
Step: 38249, Reward: [-423.291 -423.291 -423.291] [0.0000], Avg: [-463.921 -463.921 -463.921] (1.000)
Step: 38299, Reward: [-366.88 -366.88 -366.88] [0.0000], Avg: [-463.795 -463.795 -463.795] (1.000)
Step: 38349, Reward: [-445.876 -445.876 -445.876] [0.0000], Avg: [-463.771 -463.771 -463.771] (1.000)
Step: 38399, Reward: [-411.87 -411.87 -411.87] [0.0000], Avg: [-463.704 -463.704 -463.704] (1.000)
Step: 38449, Reward: [-396.134 -396.134 -396.134] [0.0000], Avg: [-463.616 -463.616 -463.616] (1.000)
Step: 38499, Reward: [-419.235 -419.235 -419.235] [0.0000], Avg: [-463.558 -463.558 -463.558] (1.000)
Step: 38549, Reward: [-377.354 -377.354 -377.354] [0.0000], Avg: [-463.446 -463.446 -463.446] (1.000)
Step: 38599, Reward: [-335.278 -335.278 -335.278] [0.0000], Avg: [-463.28 -463.28 -463.28] (1.000)
Step: 38649, Reward: [-303.39 -303.39 -303.39] [0.0000], Avg: [-463.074 -463.074 -463.074] (1.000)
Step: 38699, Reward: [-421.001 -421.001 -421.001] [0.0000], Avg: [-463.019 -463.019 -463.019] (1.000)
Step: 38749, Reward: [-315.428 -315.428 -315.428] [0.0000], Avg: [-462.829 -462.829 -462.829] (1.000)
Step: 38799, Reward: [-444.406 -444.406 -444.406] [0.0000], Avg: [-462.805 -462.805 -462.805] (1.000)
Step: 38849, Reward: [-375.977 -375.977 -375.977] [0.0000], Avg: [-462.693 -462.693 -462.693] (1.000)
Step: 38899, Reward: [-465.534 -465.534 -465.534] [0.0000], Avg: [-462.697 -462.697 -462.697] (1.000)
Step: 38949, Reward: [-377.778 -377.778 -377.778] [0.0000], Avg: [-462.588 -462.588 -462.588] (1.000)
Step: 38999, Reward: [-393.208 -393.208 -393.208] [0.0000], Avg: [-462.499 -462.499 -462.499] (1.000)
Step: 39049, Reward: [-365.249 -365.249 -365.249] [0.0000], Avg: [-462.374 -462.374 -462.374] (1.000)
Step: 39099, Reward: [-374.059 -374.059 -374.059] [0.0000], Avg: [-462.262 -462.262 -462.262] (1.000)
Step: 39149, Reward: [-419.64 -419.64 -419.64] [0.0000], Avg: [-462.207 -462.207 -462.207] (1.000)
Step: 39199, Reward: [-426.528 -426.528 -426.528] [0.0000], Avg: [-462.162 -462.162 -462.162] (1.000)
Step: 39249, Reward: [-398.519 -398.519 -398.519] [0.0000], Avg: [-462.081 -462.081 -462.081] (1.000)
Step: 39299, Reward: [-398.638 -398.638 -398.638] [0.0000], Avg: [-462. -462. -462.] (1.000)
Step: 39349, Reward: [-499.721 -499.721 -499.721] [0.0000], Avg: [-462.048 -462.048 -462.048] (1.000)
Step: 39399, Reward: [-356.545 -356.545 -356.545] [0.0000], Avg: [-461.914 -461.914 -461.914] (1.000)
Step: 39449, Reward: [-453.655 -453.655 -453.655] [0.0000], Avg: [-461.903 -461.903 -461.903] (1.000)
Step: 39499, Reward: [-415.869 -415.869 -415.869] [0.0000], Avg: [-461.845 -461.845 -461.845] (1.000)
Step: 39549, Reward: [-437.73 -437.73 -437.73] [0.0000], Avg: [-461.815 -461.815 -461.815] (1.000)
Step: 39599, Reward: [-361.649 -361.649 -361.649] [0.0000], Avg: [-461.688 -461.688 -461.688] (1.000)
Step: 39649, Reward: [-304.115 -304.115 -304.115] [0.0000], Avg: [-461.489 -461.489 -461.489] (1.000)
Step: 39699, Reward: [-353.757 -353.757 -353.757] [0.0000], Avg: [-461.354 -461.354 -461.354] (1.000)
Step: 39749, Reward: [-312.022 -312.022 -312.022] [0.0000], Avg: [-461.166 -461.166 -461.166] (1.000)
Step: 39799, Reward: [-455.732 -455.732 -455.732] [0.0000], Avg: [-461.159 -461.159 -461.159] (1.000)
Step: 39849, Reward: [-325.292 -325.292 -325.292] [0.0000], Avg: [-460.989 -460.989 -460.989] (1.000)
Step: 39899, Reward: [-358.44 -358.44 -358.44] [0.0000], Avg: [-460.86 -460.86 -460.86] (1.000)
Step: 39949, Reward: [-347.043 -347.043 -347.043] [0.0000], Avg: [-460.718 -460.718 -460.718] (1.000)
Step: 39999, Reward: [-351.842 -351.842 -351.842] [0.0000], Avg: [-460.582 -460.582 -460.582] (1.000)
Step: 40049, Reward: [-415.667 -415.667 -415.667] [0.0000], Avg: [-460.526 -460.526 -460.526] (1.000)
Step: 40099, Reward: [-274.606 -274.606 -274.606] [0.0000], Avg: [-460.294 -460.294 -460.294] (1.000)
Step: 40149, Reward: [-393.589 -393.589 -393.589] [0.0000], Avg: [-460.211 -460.211 -460.211] (1.000)
Step: 40199, Reward: [-268.016 -268.016 -268.016] [0.0000], Avg: [-459.972 -459.972 -459.972] (1.000)
Step: 40249, Reward: [-530.132 -530.132 -530.132] [0.0000], Avg: [-460.059 -460.059 -460.059] (1.000)
Step: 40299, Reward: [-368.56 -368.56 -368.56] [0.0000], Avg: [-459.945 -459.945 -459.945] (1.000)
Step: 40349, Reward: [-365.553 -365.553 -365.553] [0.0000], Avg: [-459.828 -459.828 -459.828] (1.000)
Step: 40399, Reward: [-505.376 -505.376 -505.376] [0.0000], Avg: [-459.885 -459.885 -459.885] (1.000)
Step: 40449, Reward: [-345.454 -345.454 -345.454] [0.0000], Avg: [-459.743 -459.743 -459.743] (1.000)
Step: 40499, Reward: [-559.486 -559.486 -559.486] [0.0000], Avg: [-459.866 -459.866 -459.866] (1.000)
Step: 40549, Reward: [-470.063 -470.063 -470.063] [0.0000], Avg: [-459.879 -459.879 -459.879] (1.000)
Step: 40599, Reward: [-616.71 -616.71 -616.71] [0.0000], Avg: [-460.072 -460.072 -460.072] (1.000)
Step: 40649, Reward: [-376.365 -376.365 -376.365] [0.0000], Avg: [-459.969 -459.969 -459.969] (1.000)
Step: 40699, Reward: [-345.669 -345.669 -345.669] [0.0000], Avg: [-459.829 -459.829 -459.829] (1.000)
Step: 40749, Reward: [-457.806 -457.806 -457.806] [0.0000], Avg: [-459.826 -459.826 -459.826] (1.000)
Step: 40799, Reward: [-443.597 -443.597 -443.597] [0.0000], Avg: [-459.806 -459.806 -459.806] (1.000)
Step: 40849, Reward: [-305.772 -305.772 -305.772] [0.0000], Avg: [-459.618 -459.618 -459.618] (1.000)
Step: 40899, Reward: [-329.387 -329.387 -329.387] [0.0000], Avg: [-459.459 -459.459 -459.459] (1.000)
Step: 40949, Reward: [-592.078 -592.078 -592.078] [0.0000], Avg: [-459.62 -459.62 -459.62] (1.000)
Step: 40999, Reward: [-435.894 -435.894 -435.894] [0.0000], Avg: [-459.592 -459.592 -459.592] (1.000)
Step: 41049, Reward: [-298.33 -298.33 -298.33] [0.0000], Avg: [-459.395 -459.395 -459.395] (1.000)
Step: 41099, Reward: [-331.667 -331.667 -331.667] [0.0000], Avg: [-459.24 -459.24 -459.24] (1.000)
Step: 41149, Reward: [-359.033 -359.033 -359.033] [0.0000], Avg: [-459.118 -459.118 -459.118] (1.000)
Step: 41199, Reward: [-413.369 -413.369 -413.369] [0.0000], Avg: [-459.062 -459.062 -459.062] (1.000)
Step: 41249, Reward: [-325.821 -325.821 -325.821] [0.0000], Avg: [-458.901 -458.901 -458.901] (1.000)
Step: 41299, Reward: [-355.14 -355.14 -355.14] [0.0000], Avg: [-458.775 -458.775 -458.775] (1.000)
Step: 41349, Reward: [-306.914 -306.914 -306.914] [0.0000], Avg: [-458.592 -458.592 -458.592] (1.000)
Step: 41399, Reward: [-403.483 -403.483 -403.483] [0.0000], Avg: [-458.525 -458.525 -458.525] (1.000)
Step: 41449, Reward: [-334.294 -334.294 -334.294] [0.0000], Avg: [-458.375 -458.375 -458.375] (1.000)
Step: 41499, Reward: [-298.423 -298.423 -298.423] [0.0000], Avg: [-458.183 -458.183 -458.183] (1.000)
Step: 41549, Reward: [-343.13 -343.13 -343.13] [0.0000], Avg: [-458.044 -458.044 -458.044] (1.000)
Step: 41599, Reward: [-284.296 -284.296 -284.296] [0.0000], Avg: [-457.835 -457.835 -457.835] (1.000)
Step: 41649, Reward: [-395.69 -395.69 -395.69] [0.0000], Avg: [-457.761 -457.761 -457.761] (1.000)
Step: 41699, Reward: [-309.632 -309.632 -309.632] [0.0000], Avg: [-457.583 -457.583 -457.583] (1.000)
Step: 41749, Reward: [-303.378 -303.378 -303.378] [0.0000], Avg: [-457.398 -457.398 -457.398] (1.000)
Step: 41799, Reward: [-344.013 -344.013 -344.013] [0.0000], Avg: [-457.263 -457.263 -457.263] (1.000)
Step: 41849, Reward: [-342.925 -342.925 -342.925] [0.0000], Avg: [-457.126 -457.126 -457.126] (1.000)
Step: 41899, Reward: [-416.497 -416.497 -416.497] [0.0000], Avg: [-457.078 -457.078 -457.078] (1.000)
Step: 41949, Reward: [-332.348 -332.348 -332.348] [0.0000], Avg: [-456.929 -456.929 -456.929] (1.000)
Step: 41999, Reward: [-612.937 -612.937 -612.937] [0.0000], Avg: [-457.115 -457.115 -457.115] (1.000)
Step: 42049, Reward: [-437.509 -437.509 -437.509] [0.0000], Avg: [-457.091 -457.091 -457.091] (1.000)
Step: 42099, Reward: [-335.167 -335.167 -335.167] [0.0000], Avg: [-456.947 -456.947 -456.947] (1.000)
Step: 42149, Reward: [-348.068 -348.068 -348.068] [0.0000], Avg: [-456.817 -456.817 -456.817] (1.000)
Step: 42199, Reward: [-286.074 -286.074 -286.074] [0.0000], Avg: [-456.615 -456.615 -456.615] (1.000)
Step: 42249, Reward: [-338.925 -338.925 -338.925] [0.0000], Avg: [-456.476 -456.476 -456.476] (1.000)
Step: 42299, Reward: [-368.221 -368.221 -368.221] [0.0000], Avg: [-456.372 -456.372 -456.372] (1.000)
Step: 42349, Reward: [-322.607 -322.607 -322.607] [0.0000], Avg: [-456.214 -456.214 -456.214] (1.000)
Step: 42399, Reward: [-509.834 -509.834 -509.834] [0.0000], Avg: [-456.277 -456.277 -456.277] (1.000)
Step: 42449, Reward: [-291.13 -291.13 -291.13] [0.0000], Avg: [-456.082 -456.082 -456.082] (1.000)
Step: 42499, Reward: [-444.167 -444.167 -444.167] [0.0000], Avg: [-456.068 -456.068 -456.068] (1.000)
Step: 42549, Reward: [-330.918 -330.918 -330.918] [0.0000], Avg: [-455.921 -455.921 -455.921] (1.000)
Step: 42599, Reward: [-359.898 -359.898 -359.898] [0.0000], Avg: [-455.809 -455.809 -455.809] (1.000)
Step: 42649, Reward: [-433.508 -433.508 -433.508] [0.0000], Avg: [-455.782 -455.782 -455.782] (1.000)
Step: 42699, Reward: [-336.091 -336.091 -336.091] [0.0000], Avg: [-455.642 -455.642 -455.642] (1.000)
Step: 42749, Reward: [-313.149 -313.149 -313.149] [0.0000], Avg: [-455.476 -455.476 -455.476] (1.000)
Step: 42799, Reward: [-299.119 -299.119 -299.119] [0.0000], Avg: [-455.293 -455.293 -455.293] (1.000)
Step: 42849, Reward: [-290.257 -290.257 -290.257] [0.0000], Avg: [-455.1 -455.1 -455.1] (1.000)
Step: 42899, Reward: [-421.862 -421.862 -421.862] [0.0000], Avg: [-455.062 -455.062 -455.062] (1.000)
Step: 42949, Reward: [-410.704 -410.704 -410.704] [0.0000], Avg: [-455.01 -455.01 -455.01] (1.000)
Step: 42999, Reward: [-352.486 -352.486 -352.486] [0.0000], Avg: [-454.891 -454.891 -454.891] (1.000)
Step: 43049, Reward: [-322.064 -322.064 -322.064] [0.0000], Avg: [-454.737 -454.737 -454.737] (1.000)
Step: 43099, Reward: [-408.827 -408.827 -408.827] [0.0000], Avg: [-454.683 -454.683 -454.683] (1.000)
Step: 43149, Reward: [-329.502 -329.502 -329.502] [0.0000], Avg: [-454.538 -454.538 -454.538] (1.000)
Step: 43199, Reward: [-387.62 -387.62 -387.62] [0.0000], Avg: [-454.461 -454.461 -454.461] (1.000)
Step: 43249, Reward: [-366.507 -366.507 -366.507] [0.0000], Avg: [-454.359 -454.359 -454.359] (1.000)
Step: 43299, Reward: [-305.323 -305.323 -305.323] [0.0000], Avg: [-454.187 -454.187 -454.187] (1.000)
Step: 43349, Reward: [-472.623 -472.623 -472.623] [0.0000], Avg: [-454.208 -454.208 -454.208] (1.000)
Step: 43399, Reward: [-273.725 -273.725 -273.725] [0.0000], Avg: [-454. -454. -454.] (1.000)
Step: 43449, Reward: [-332.747 -332.747 -332.747] [0.0000], Avg: [-453.861 -453.861 -453.861] (1.000)
Step: 43499, Reward: [-397.416 -397.416 -397.416] [0.0000], Avg: [-453.796 -453.796 -453.796] (1.000)
Step: 43549, Reward: [-353.12 -353.12 -353.12] [0.0000], Avg: [-453.68 -453.68 -453.68] (1.000)
Step: 43599, Reward: [-348.189 -348.189 -348.189] [0.0000], Avg: [-453.559 -453.559 -453.559] (1.000)
Step: 43649, Reward: [-346.338 -346.338 -346.338] [0.0000], Avg: [-453.437 -453.437 -453.437] (1.000)
Step: 43699, Reward: [-446.081 -446.081 -446.081] [0.0000], Avg: [-453.428 -453.428 -453.428] (1.000)
Step: 43749, Reward: [-366.622 -366.622 -366.622] [0.0000], Avg: [-453.329 -453.329 -453.329] (1.000)
Step: 43799, Reward: [-437.724 -437.724 -437.724] [0.0000], Avg: [-453.311 -453.311 -453.311] (1.000)
Step: 43849, Reward: [-366.492 -366.492 -366.492] [0.0000], Avg: [-453.212 -453.212 -453.212] (1.000)
Step: 43899, Reward: [-441.918 -441.918 -441.918] [0.0000], Avg: [-453.199 -453.199 -453.199] (1.000)
Step: 43949, Reward: [-500.96 -500.96 -500.96] [0.0000], Avg: [-453.254 -453.254 -453.254] (1.000)
Step: 43999, Reward: [-326.796 -326.796 -326.796] [0.0000], Avg: [-453.11 -453.11 -453.11] (1.000)
Step: 44049, Reward: [-293.455 -293.455 -293.455] [0.0000], Avg: [-452.929 -452.929 -452.929] (1.000)
Step: 44099, Reward: [-261.554 -261.554 -261.554] [0.0000], Avg: [-452.712 -452.712 -452.712] (1.000)
Step: 44149, Reward: [-302.649 -302.649 -302.649] [0.0000], Avg: [-452.542 -452.542 -452.542] (1.000)
Step: 44199, Reward: [-390.302 -390.302 -390.302] [0.0000], Avg: [-452.471 -452.471 -452.471] (1.000)
Step: 44249, Reward: [-399.717 -399.717 -399.717] [0.0000], Avg: [-452.412 -452.412 -452.412] (1.000)
Step: 44299, Reward: [-386.335 -386.335 -386.335] [0.0000], Avg: [-452.337 -452.337 -452.337] (1.000)
Step: 44349, Reward: [-342.925 -342.925 -342.925] [0.0000], Avg: [-452.214 -452.214 -452.214] (1.000)
Step: 44399, Reward: [-397.927 -397.927 -397.927] [0.0000], Avg: [-452.153 -452.153 -452.153] (1.000)
Step: 44449, Reward: [-476.031 -476.031 -476.031] [0.0000], Avg: [-452.179 -452.179 -452.179] (1.000)
Step: 44499, Reward: [-429.069 -429.069 -429.069] [0.0000], Avg: [-452.154 -452.154 -452.154] (1.000)
Step: 44549, Reward: [-491.831 -491.831 -491.831] [0.0000], Avg: [-452.198 -452.198 -452.198] (1.000)
Step: 44599, Reward: [-449.254 -449.254 -449.254] [0.0000], Avg: [-452.195 -452.195 -452.195] (1.000)
Step: 44649, Reward: [-472.91 -472.91 -472.91] [0.0000], Avg: [-452.218 -452.218 -452.218] (1.000)
Step: 44699, Reward: [-425.554 -425.554 -425.554] [0.0000], Avg: [-452.188 -452.188 -452.188] (1.000)
Step: 44749, Reward: [-322.579 -322.579 -322.579] [0.0000], Avg: [-452.043 -452.043 -452.043] (1.000)
Step: 44799, Reward: [-390.536 -390.536 -390.536] [0.0000], Avg: [-451.975 -451.975 -451.975] (1.000)
Step: 44849, Reward: [-402.69 -402.69 -402.69] [0.0000], Avg: [-451.92 -451.92 -451.92] (1.000)
Step: 44899, Reward: [-397.475 -397.475 -397.475] [0.0000], Avg: [-451.859 -451.859 -451.859] (1.000)
Step: 44949, Reward: [-372.36 -372.36 -372.36] [0.0000], Avg: [-451.771 -451.771 -451.771] (1.000)
Step: 44999, Reward: [-276.702 -276.702 -276.702] [0.0000], Avg: [-451.576 -451.576 -451.576] (1.000)
Step: 45049, Reward: [-407.863 -407.863 -407.863] [0.0000], Avg: [-451.528 -451.528 -451.528] (1.000)
Step: 45099, Reward: [-343.608 -343.608 -343.608] [0.0000], Avg: [-451.408 -451.408 -451.408] (1.000)
Step: 45149, Reward: [-431.42 -431.42 -431.42] [0.0000], Avg: [-451.386 -451.386 -451.386] (1.000)
Step: 45199, Reward: [-320.432 -320.432 -320.432] [0.0000], Avg: [-451.241 -451.241 -451.241] (1.000)
Step: 45249, Reward: [-291.891 -291.891 -291.891] [0.0000], Avg: [-451.065 -451.065 -451.065] (1.000)
Step: 45299, Reward: [-448.452 -448.452 -448.452] [0.0000], Avg: [-451.062 -451.062 -451.062] (1.000)
Step: 45349, Reward: [-426.317 -426.317 -426.317] [0.0000], Avg: [-451.035 -451.035 -451.035] (1.000)
Step: 45399, Reward: [-430.609 -430.609 -430.609] [0.0000], Avg: [-451.012 -451.012 -451.012] (1.000)
Step: 45449, Reward: [-322.292 -322.292 -322.292] [0.0000], Avg: [-450.871 -450.871 -450.871] (1.000)
Step: 45499, Reward: [-420.864 -420.864 -420.864] [0.0000], Avg: [-450.838 -450.838 -450.838] (1.000)
Step: 45549, Reward: [-486.918 -486.918 -486.918] [0.0000], Avg: [-450.877 -450.877 -450.877] (1.000)
Step: 45599, Reward: [-386.757 -386.757 -386.757] [0.0000], Avg: [-450.807 -450.807 -450.807] (1.000)
Step: 45649, Reward: [-512.937 -512.937 -512.937] [0.0000], Avg: [-450.875 -450.875 -450.875] (1.000)
Step: 45699, Reward: [-324.256 -324.256 -324.256] [0.0000], Avg: [-450.736 -450.736 -450.736] (1.000)
Step: 45749, Reward: [-381.329 -381.329 -381.329] [0.0000], Avg: [-450.661 -450.661 -450.661] (1.000)
Step: 45799, Reward: [-516.022 -516.022 -516.022] [0.0000], Avg: [-450.732 -450.732 -450.732] (1.000)
Step: 45849, Reward: [-455.412 -455.412 -455.412] [0.0000], Avg: [-450.737 -450.737 -450.737] (1.000)
Step: 45899, Reward: [-397.243 -397.243 -397.243] [0.0000], Avg: [-450.679 -450.679 -450.679] (1.000)
Step: 45949, Reward: [-270.691 -270.691 -270.691] [0.0000], Avg: [-450.483 -450.483 -450.483] (1.000)
Step: 45999, Reward: [-428.658 -428.658 -428.658] [0.0000], Avg: [-450.459 -450.459 -450.459] (1.000)
Step: 46049, Reward: [-370.672 -370.672 -370.672] [0.0000], Avg: [-450.373 -450.373 -450.373] (1.000)
Step: 46099, Reward: [-293.333 -293.333 -293.333] [0.0000], Avg: [-450.202 -450.202 -450.202] (1.000)
Step: 46149, Reward: [-487.01 -487.01 -487.01] [0.0000], Avg: [-450.242 -450.242 -450.242] (1.000)
Step: 46199, Reward: [-296.381 -296.381 -296.381] [0.0000], Avg: [-450.076 -450.076 -450.076] (1.000)
Step: 46249, Reward: [-354.335 -354.335 -354.335] [0.0000], Avg: [-449.972 -449.972 -449.972] (1.000)
Step: 46299, Reward: [-326.665 -326.665 -326.665] [0.0000], Avg: [-449.839 -449.839 -449.839] (1.000)
Step: 46349, Reward: [-407.896 -407.896 -407.896] [0.0000], Avg: [-449.794 -449.794 -449.794] (1.000)
Step: 46399, Reward: [-320.405 -320.405 -320.405] [0.0000], Avg: [-449.654 -449.654 -449.654] (1.000)
Step: 46449, Reward: [-423.81 -423.81 -423.81] [0.0000], Avg: [-449.626 -449.626 -449.626] (1.000)
Step: 46499, Reward: [-507.701 -507.701 -507.701] [0.0000], Avg: [-449.689 -449.689 -449.689] (1.000)
Step: 46549, Reward: [-306.523 -306.523 -306.523] [0.0000], Avg: [-449.535 -449.535 -449.535] (1.000)
Step: 46599, Reward: [-321.272 -321.272 -321.272] [0.0000], Avg: [-449.398 -449.398 -449.398] (1.000)
Step: 46649, Reward: [-301.764 -301.764 -301.764] [0.0000], Avg: [-449.239 -449.239 -449.239] (1.000)
Step: 46699, Reward: [-446.142 -446.142 -446.142] [0.0000], Avg: [-449.236 -449.236 -449.236] (1.000)
Step: 46749, Reward: [-277.431 -277.431 -277.431] [0.0000], Avg: [-449.052 -449.052 -449.052] (1.000)
Step: 46799, Reward: [-440.084 -440.084 -440.084] [0.0000], Avg: [-449.043 -449.043 -449.043] (1.000)
Step: 46849, Reward: [-397.937 -397.937 -397.937] [0.0000], Avg: [-448.988 -448.988 -448.988] (1.000)
Step: 46899, Reward: [-319.998 -319.998 -319.998] [0.0000], Avg: [-448.851 -448.851 -448.851] (1.000)
Step: 46949, Reward: [-283.959 -283.959 -283.959] [0.0000], Avg: [-448.675 -448.675 -448.675] (1.000)
Step: 46999, Reward: [-307.527 -307.527 -307.527] [0.0000], Avg: [-448.525 -448.525 -448.525] (1.000)
Step: 47049, Reward: [-268.698 -268.698 -268.698] [0.0000], Avg: [-448.334 -448.334 -448.334] (1.000)
Step: 47099, Reward: [-346.025 -346.025 -346.025] [0.0000], Avg: [-448.225 -448.225 -448.225] (1.000)
Step: 47149, Reward: [-435.218 -435.218 -435.218] [0.0000], Avg: [-448.211 -448.211 -448.211] (1.000)
Step: 47199, Reward: [-362.992 -362.992 -362.992] [0.0000], Avg: [-448.121 -448.121 -448.121] (1.000)
Step: 47249, Reward: [-273.033 -273.033 -273.033] [0.0000], Avg: [-447.936 -447.936 -447.936] (1.000)
Step: 47299, Reward: [-365.006 -365.006 -365.006] [0.0000], Avg: [-447.848 -447.848 -447.848] (1.000)
Step: 47349, Reward: [-519.771 -519.771 -519.771] [0.0000], Avg: [-447.924 -447.924 -447.924] (1.000)
Step: 47399, Reward: [-476.017 -476.017 -476.017] [0.0000], Avg: [-447.954 -447.954 -447.954] (1.000)
Step: 47449, Reward: [-464.872 -464.872 -464.872] [0.0000], Avg: [-447.972 -447.972 -447.972] (1.000)
Step: 47499, Reward: [-440.597 -440.597 -440.597] [0.0000], Avg: [-447.964 -447.964 -447.964] (1.000)
Step: 47549, Reward: [-519.863 -519.863 -519.863] [0.0000], Avg: [-448.039 -448.039 -448.039] (1.000)
Step: 47599, Reward: [-450.805 -450.805 -450.805] [0.0000], Avg: [-448.042 -448.042 -448.042] (1.000)
Step: 47649, Reward: [-460.995 -460.995 -460.995] [0.0000], Avg: [-448.056 -448.056 -448.056] (1.000)
Step: 47699, Reward: [-417.015 -417.015 -417.015] [0.0000], Avg: [-448.023 -448.023 -448.023] (1.000)
Step: 47749, Reward: [-333.022 -333.022 -333.022] [0.0000], Avg: [-447.903 -447.903 -447.903] (1.000)
Step: 47799, Reward: [-430.94 -430.94 -430.94] [0.0000], Avg: [-447.885 -447.885 -447.885] (1.000)
Step: 47849, Reward: [-466.545 -466.545 -466.545] [0.0000], Avg: [-447.905 -447.905 -447.905] (1.000)
Step: 47899, Reward: [-340.003 -340.003 -340.003] [0.0000], Avg: [-447.792 -447.792 -447.792] (1.000)
Step: 47949, Reward: [-415.857 -415.857 -415.857] [0.0000], Avg: [-447.759 -447.759 -447.759] (1.000)
Step: 47999, Reward: [-426.524 -426.524 -426.524] [0.0000], Avg: [-447.737 -447.737 -447.737] (1.000)
Step: 48049, Reward: [-286.726 -286.726 -286.726] [0.0000], Avg: [-447.569 -447.569 -447.569] (1.000)
Step: 48099, Reward: [-362.996 -362.996 -362.996] [0.0000], Avg: [-447.481 -447.481 -447.481] (1.000)
Step: 48149, Reward: [-356.032 -356.032 -356.032] [0.0000], Avg: [-447.386 -447.386 -447.386] (1.000)
Step: 48199, Reward: [-367.866 -367.866 -367.866] [0.0000], Avg: [-447.304 -447.304 -447.304] (1.000)
Step: 48249, Reward: [-395.461 -395.461 -395.461] [0.0000], Avg: [-447.25 -447.25 -447.25] (1.000)
Step: 48299, Reward: [-300.862 -300.862 -300.862] [0.0000], Avg: [-447.098 -447.098 -447.098] (1.000)
Step: 48349, Reward: [-388.806 -388.806 -388.806] [0.0000], Avg: [-447.038 -447.038 -447.038] (1.000)
Step: 48399, Reward: [-369.327 -369.327 -369.327] [0.0000], Avg: [-446.958 -446.958 -446.958] (1.000)
Step: 48449, Reward: [-346.44 -346.44 -346.44] [0.0000], Avg: [-446.854 -446.854 -446.854] (1.000)
Step: 48499, Reward: [-491.888 -491.888 -491.888] [0.0000], Avg: [-446.901 -446.901 -446.901] (1.000)
Step: 48549, Reward: [-473.877 -473.877 -473.877] [0.0000], Avg: [-446.928 -446.928 -446.928] (1.000)
Step: 48599, Reward: [-320.587 -320.587 -320.587] [0.0000], Avg: [-446.798 -446.798 -446.798] (1.000)
Step: 48649, Reward: [-338.818 -338.818 -338.818] [0.0000], Avg: [-446.687 -446.687 -446.687] (1.000)
Step: 48699, Reward: [-365.905 -365.905 -365.905] [0.0000], Avg: [-446.604 -446.604 -446.604] (1.000)
Step: 48749, Reward: [-471.592 -471.592 -471.592] [0.0000], Avg: [-446.63 -446.63 -446.63] (1.000)
Step: 48799, Reward: [-456.616 -456.616 -456.616] [0.0000], Avg: [-446.64 -446.64 -446.64] (1.000)
Step: 48849, Reward: [-337.081 -337.081 -337.081] [0.0000], Avg: [-446.528 -446.528 -446.528] (1.000)
Step: 48899, Reward: [-319.527 -319.527 -319.527] [0.0000], Avg: [-446.398 -446.398 -446.398] (1.000)
Step: 48949, Reward: [-384.451 -384.451 -384.451] [0.0000], Avg: [-446.335 -446.335 -446.335] (1.000)
Step: 48999, Reward: [-447.482 -447.482 -447.482] [0.0000], Avg: [-446.336 -446.336 -446.336] (1.000)
Step: 49049, Reward: [-492.664 -492.664 -492.664] [0.0000], Avg: [-446.383 -446.383 -446.383] (1.000)
Step: 49099, Reward: [-434.588 -434.588 -434.588] [0.0000], Avg: [-446.371 -446.371 -446.371] (1.000)
Step: 49149, Reward: [-401.402 -401.402 -401.402] [0.0000], Avg: [-446.326 -446.326 -446.326] (1.000)
Step: 49199, Reward: [-397.757 -397.757 -397.757] [0.0000], Avg: [-446.276 -446.276 -446.276] (1.000)
Step: 49249, Reward: [-390.568 -390.568 -390.568] [0.0000], Avg: [-446.22 -446.22 -446.22] (1.000)
Step: 49299, Reward: [-419.684 -419.684 -419.684] [0.0000], Avg: [-446.193 -446.193 -446.193] (1.000)
Step: 49349, Reward: [-445.075 -445.075 -445.075] [0.0000], Avg: [-446.192 -446.192 -446.192] (1.000)
Step: 49399, Reward: [-450.237 -450.237 -450.237] [0.0000], Avg: [-446.196 -446.196 -446.196] (1.000)
Step: 49449, Reward: [-376.572 -376.572 -376.572] [0.0000], Avg: [-446.125 -446.125 -446.125] (1.000)
Step: 49499, Reward: [-381.346 -381.346 -381.346] [0.0000], Avg: [-446.06 -446.06 -446.06] (1.000)
Step: 49549, Reward: [-501.07 -501.07 -501.07] [0.0000], Avg: [-446.115 -446.115 -446.115] (1.000)
Step: 49599, Reward: [-335.177 -335.177 -335.177] [0.0000], Avg: [-446.004 -446.004 -446.004] (1.000)
Step: 49649, Reward: [-478.114 -478.114 -478.114] [0.0000], Avg: [-446.036 -446.036 -446.036] (1.000)
Step: 49699, Reward: [-468.505 -468.505 -468.505] [0.0000], Avg: [-446.059 -446.059 -446.059] (1.000)
Step: 49749, Reward: [-386.585 -386.585 -386.585] [0.0000], Avg: [-445.999 -445.999 -445.999] (1.000)
Step: 49799, Reward: [-458.079 -458.079 -458.079] [0.0000], Avg: [-446.011 -446.011 -446.011] (1.000)
Step: 49849, Reward: [-295.079 -295.079 -295.079] [0.0000], Avg: [-445.86 -445.86 -445.86] (1.000)
Step: 49899, Reward: [-471.901 -471.901 -471.901] [0.0000], Avg: [-445.886 -445.886 -445.886] (1.000)
Step: 49949, Reward: [-366.265 -366.265 -366.265] [0.0000], Avg: [-445.806 -445.806 -445.806] (1.000)
Step: 49999, Reward: [-418.077 -418.077 -418.077] [0.0000], Avg: [-445.778 -445.778 -445.778] (1.000)
Step: 50049, Reward: [-463.538 -463.538 -463.538] [0.0000], Avg: [-445.796 -445.796 -445.796] (1.000)
Step: 50099, Reward: [-441.104 -441.104 -441.104] [0.0000], Avg: [-445.791 -445.791 -445.791] (1.000)
Step: 50149, Reward: [-343.996 -343.996 -343.996] [0.0000], Avg: [-445.69 -445.69 -445.69] (1.000)
Step: 50199, Reward: [-285.511 -285.511 -285.511] [0.0000], Avg: [-445.53 -445.53 -445.53] (1.000)
Step: 50249, Reward: [-360.314 -360.314 -360.314] [0.0000], Avg: [-445.445 -445.445 -445.445] (1.000)
Step: 50299, Reward: [-274.233 -274.233 -274.233] [0.0000], Avg: [-445.275 -445.275 -445.275] (1.000)
Step: 50349, Reward: [-354.591 -354.591 -354.591] [0.0000], Avg: [-445.185 -445.185 -445.185] (1.000)
Step: 50399, Reward: [-346.325 -346.325 -346.325] [0.0000], Avg: [-445.087 -445.087 -445.087] (1.000)
Step: 50449, Reward: [-382.239 -382.239 -382.239] [0.0000], Avg: [-445.025 -445.025 -445.025] (1.000)
Step: 50499, Reward: [-389.56 -389.56 -389.56] [0.0000], Avg: [-444.97 -444.97 -444.97] (1.000)
Step: 50549, Reward: [-352.041 -352.041 -352.041] [0.0000], Avg: [-444.878 -444.878 -444.878] (1.000)
Step: 50599, Reward: [-502.312 -502.312 -502.312] [0.0000], Avg: [-444.935 -444.935 -444.935] (1.000)
Step: 50649, Reward: [-344.986 -344.986 -344.986] [0.0000], Avg: [-444.836 -444.836 -444.836] (1.000)
Step: 50699, Reward: [-420.637 -420.637 -420.637] [0.0000], Avg: [-444.812 -444.812 -444.812] (1.000)
Step: 50749, Reward: [-381.281 -381.281 -381.281] [0.0000], Avg: [-444.75 -444.75 -444.75] (1.000)
Step: 50799, Reward: [-312.633 -312.633 -312.633] [0.0000], Avg: [-444.62 -444.62 -444.62] (1.000)
Step: 50849, Reward: [-475.883 -475.883 -475.883] [0.0000], Avg: [-444.65 -444.65 -444.65] (1.000)
Step: 50899, Reward: [-434.247 -434.247 -434.247] [0.0000], Avg: [-444.64 -444.64 -444.64] (1.000)
Step: 50949, Reward: [-356.285 -356.285 -356.285] [0.0000], Avg: [-444.553 -444.553 -444.553] (1.000)
Step: 50999, Reward: [-370.935 -370.935 -370.935] [0.0000], Avg: [-444.481 -444.481 -444.481] (1.000)
Step: 51049, Reward: [-268.929 -268.929 -268.929] [0.0000], Avg: [-444.309 -444.309 -444.309] (1.000)
Step: 51099, Reward: [-329.202 -329.202 -329.202] [0.0000], Avg: [-444.197 -444.197 -444.197] (1.000)
Step: 51149, Reward: [-355.607 -355.607 -355.607] [0.0000], Avg: [-444.11 -444.11 -444.11] (1.000)
Step: 51199, Reward: [-435.96 -435.96 -435.96] [0.0000], Avg: [-444.102 -444.102 -444.102] (1.000)
Step: 51249, Reward: [-446.624 -446.624 -446.624] [0.0000], Avg: [-444.105 -444.105 -444.105] (1.000)
Step: 51299, Reward: [-421.582 -421.582 -421.582] [0.0000], Avg: [-444.083 -444.083 -444.083] (1.000)
Step: 51349, Reward: [-478.37 -478.37 -478.37] [0.0000], Avg: [-444.116 -444.116 -444.116] (1.000)
Step: 51399, Reward: [-350.319 -350.319 -350.319] [0.0000], Avg: [-444.025 -444.025 -444.025] (1.000)
Step: 51449, Reward: [-385.397 -385.397 -385.397] [0.0000], Avg: [-443.968 -443.968 -443.968] (1.000)
Step: 51499, Reward: [-452.341 -452.341 -452.341] [0.0000], Avg: [-443.976 -443.976 -443.976] (1.000)
Step: 51549, Reward: [-495.867 -495.867 -495.867] [0.0000], Avg: [-444.026 -444.026 -444.026] (1.000)
Step: 51599, Reward: [-447.227 -447.227 -447.227] [0.0000], Avg: [-444.029 -444.029 -444.029] (1.000)
Step: 51649, Reward: [-490.36 -490.36 -490.36] [0.0000], Avg: [-444.074 -444.074 -444.074] (1.000)
Step: 51699, Reward: [-363.65 -363.65 -363.65] [0.0000], Avg: [-443.996 -443.996 -443.996] (1.000)
Step: 51749, Reward: [-455.633 -455.633 -455.633] [0.0000], Avg: [-444.008 -444.008 -444.008] (1.000)
Step: 51799, Reward: [-352.226 -352.226 -352.226] [0.0000], Avg: [-443.919 -443.919 -443.919] (1.000)
Step: 51849, Reward: [-462.807 -462.807 -462.807] [0.0000], Avg: [-443.937 -443.937 -443.937] (1.000)
Step: 51899, Reward: [-472.617 -472.617 -472.617] [0.0000], Avg: [-443.965 -443.965 -443.965] (1.000)
Step: 51949, Reward: [-361.743 -361.743 -361.743] [0.0000], Avg: [-443.886 -443.886 -443.886] (1.000)
Step: 51999, Reward: [-304.168 -304.168 -304.168] [0.0000], Avg: [-443.751 -443.751 -443.751] (1.000)
Step: 52049, Reward: [-319.032 -319.032 -319.032] [0.0000], Avg: [-443.632 -443.632 -443.632] (1.000)
Step: 52099, Reward: [-317.442 -317.442 -317.442] [0.0000], Avg: [-443.511 -443.511 -443.511] (1.000)
Step: 52149, Reward: [-387.347 -387.347 -387.347] [0.0000], Avg: [-443.457 -443.457 -443.457] (1.000)
Step: 52199, Reward: [-367.187 -367.187 -367.187] [0.0000], Avg: [-443.384 -443.384 -443.384] (1.000)
Step: 52249, Reward: [-384.166 -384.166 -384.166] [0.0000], Avg: [-443.327 -443.327 -443.327] (1.000)
Step: 52299, Reward: [-319.156 -319.156 -319.156] [0.0000], Avg: [-443.208 -443.208 -443.208] (1.000)
Step: 52349, Reward: [-359.563 -359.563 -359.563] [0.0000], Avg: [-443.128 -443.128 -443.128] (1.000)
Step: 52399, Reward: [-495.733 -495.733 -495.733] [0.0000], Avg: [-443.179 -443.179 -443.179] (1.000)
Step: 52449, Reward: [-372.666 -372.666 -372.666] [0.0000], Avg: [-443.111 -443.111 -443.111] (1.000)
Step: 52499, Reward: [-336.839 -336.839 -336.839] [0.0000], Avg: [-443.01 -443.01 -443.01] (1.000)
Step: 52549, Reward: [-459.862 -459.862 -459.862] [0.0000], Avg: [-443.026 -443.026 -443.026] (1.000)
Step: 52599, Reward: [-291.181 -291.181 -291.181] [0.0000], Avg: [-442.882 -442.882 -442.882] (1.000)
Step: 52649, Reward: [-353.62 -353.62 -353.62] [0.0000], Avg: [-442.797 -442.797 -442.797] (1.000)
Step: 52699, Reward: [-292.668 -292.668 -292.668] [0.0000], Avg: [-442.655 -442.655 -442.655] (1.000)
Step: 52749, Reward: [-398.478 -398.478 -398.478] [0.0000], Avg: [-442.613 -442.613 -442.613] (1.000)
Step: 52799, Reward: [-413.992 -413.992 -413.992] [0.0000], Avg: [-442.586 -442.586 -442.586] (1.000)
Step: 52849, Reward: [-371.74 -371.74 -371.74] [0.0000], Avg: [-442.519 -442.519 -442.519] (1.000)
Step: 52899, Reward: [-538.186 -538.186 -538.186] [0.0000], Avg: [-442.609 -442.609 -442.609] (1.000)
Step: 52949, Reward: [-495.233 -495.233 -495.233] [0.0000], Avg: [-442.659 -442.659 -442.659] (1.000)
Step: 52999, Reward: [-391.675 -391.675 -391.675] [0.0000], Avg: [-442.611 -442.611 -442.611] (1.000)
Step: 53049, Reward: [-447.04 -447.04 -447.04] [0.0000], Avg: [-442.615 -442.615 -442.615] (1.000)
Step: 53099, Reward: [-344.389 -344.389 -344.389] [0.0000], Avg: [-442.522 -442.522 -442.522] (1.000)
Step: 53149, Reward: [-499.127 -499.127 -499.127] [0.0000], Avg: [-442.576 -442.576 -442.576] (1.000)
Step: 53199, Reward: [-324.825 -324.825 -324.825] [0.0000], Avg: [-442.465 -442.465 -442.465] (1.000)
Step: 53249, Reward: [-320.181 -320.181 -320.181] [0.0000], Avg: [-442.35 -442.35 -442.35] (1.000)
Step: 53299, Reward: [-367.414 -367.414 -367.414] [0.0000], Avg: [-442.28 -442.28 -442.28] (1.000)
Step: 53349, Reward: [-335.473 -335.473 -335.473] [0.0000], Avg: [-442.18 -442.18 -442.18] (1.000)
Step: 53399, Reward: [-345.232 -345.232 -345.232] [0.0000], Avg: [-442.089 -442.089 -442.089] (1.000)
Step: 53449, Reward: [-463.666 -463.666 -463.666] [0.0000], Avg: [-442.109 -442.109 -442.109] (1.000)
Step: 53499, Reward: [-460.993 -460.993 -460.993] [0.0000], Avg: [-442.127 -442.127 -442.127] (1.000)
Step: 53549, Reward: [-408.367 -408.367 -408.367] [0.0000], Avg: [-442.095 -442.095 -442.095] (1.000)
Step: 53599, Reward: [-340.082 -340.082 -340.082] [0.0000], Avg: [-442. -442. -442.] (1.000)
Step: 53649, Reward: [-416.097 -416.097 -416.097] [0.0000], Avg: [-441.976 -441.976 -441.976] (1.000)
Step: 53699, Reward: [-351.679 -351.679 -351.679] [0.0000], Avg: [-441.892 -441.892 -441.892] (1.000)
Step: 53749, Reward: [-309.318 -309.318 -309.318] [0.0000], Avg: [-441.769 -441.769 -441.769] (1.000)
Step: 53799, Reward: [-293.498 -293.498 -293.498] [0.0000], Avg: [-441.631 -441.631 -441.631] (1.000)
Step: 53849, Reward: [-314.037 -314.037 -314.037] [0.0000], Avg: [-441.512 -441.512 -441.512] (1.000)
Step: 53899, Reward: [-373.558 -373.558 -373.558] [0.0000], Avg: [-441.449 -441.449 -441.449] (1.000)
Step: 53949, Reward: [-332.216 -332.216 -332.216] [0.0000], Avg: [-441.348 -441.348 -441.348] (1.000)
Step: 53999, Reward: [-354.272 -354.272 -354.272] [0.0000], Avg: [-441.267 -441.267 -441.267] (1.000)
Step: 54049, Reward: [-424.624 -424.624 -424.624] [0.0000], Avg: [-441.252 -441.252 -441.252] (1.000)
Step: 54099, Reward: [-424.395 -424.395 -424.395] [0.0000], Avg: [-441.236 -441.236 -441.236] (1.000)
Step: 54149, Reward: [-391.764 -391.764 -391.764] [0.0000], Avg: [-441.191 -441.191 -441.191] (1.000)
Step: 54199, Reward: [-371.605 -371.605 -371.605] [0.0000], Avg: [-441.126 -441.126 -441.126] (1.000)
Step: 54249, Reward: [-297.692 -297.692 -297.692] [0.0000], Avg: [-440.994 -440.994 -440.994] (1.000)
Step: 54299, Reward: [-307.454 -307.454 -307.454] [0.0000], Avg: [-440.871 -440.871 -440.871] (1.000)
Step: 54349, Reward: [-445.2 -445.2 -445.2] [0.0000], Avg: [-440.875 -440.875 -440.875] (1.000)
Step: 54399, Reward: [-370.608 -370.608 -370.608] [0.0000], Avg: [-440.811 -440.811 -440.811] (1.000)
Step: 54449, Reward: [-461.654 -461.654 -461.654] [0.0000], Avg: [-440.83 -440.83 -440.83] (1.000)
Step: 54499, Reward: [-391.104 -391.104 -391.104] [0.0000], Avg: [-440.784 -440.784 -440.784] (1.000)
Step: 54549, Reward: [-378.316 -378.316 -378.316] [0.0000], Avg: [-440.727 -440.727 -440.727] (1.000)
Step: 54599, Reward: [-336.361 -336.361 -336.361] [0.0000], Avg: [-440.631 -440.631 -440.631] (1.000)
Step: 54649, Reward: [-478.732 -478.732 -478.732] [0.0000], Avg: [-440.666 -440.666 -440.666] (1.000)
Step: 54699, Reward: [-425.262 -425.262 -425.262] [0.0000], Avg: [-440.652 -440.652 -440.652] (1.000)
Step: 54749, Reward: [-270.744 -270.744 -270.744] [0.0000], Avg: [-440.497 -440.497 -440.497] (1.000)
Step: 54799, Reward: [-348.706 -348.706 -348.706] [0.0000], Avg: [-440.413 -440.413 -440.413] (1.000)
Step: 54849, Reward: [-368.865 -368.865 -368.865] [0.0000], Avg: [-440.348 -440.348 -440.348] (1.000)
Step: 54899, Reward: [-427.324 -427.324 -427.324] [0.0000], Avg: [-440.336 -440.336 -440.336] (1.000)
Step: 54949, Reward: [-350.849 -350.849 -350.849] [0.0000], Avg: [-440.255 -440.255 -440.255] (1.000)
Step: 54999, Reward: [-339.002 -339.002 -339.002] [0.0000], Avg: [-440.163 -440.163 -440.163] (1.000)
Step: 55049, Reward: [-513.908 -513.908 -513.908] [0.0000], Avg: [-440.23 -440.23 -440.23] (1.000)
Step: 55099, Reward: [-351.847 -351.847 -351.847] [0.0000], Avg: [-440.149 -440.149 -440.149] (1.000)
Step: 55149, Reward: [-304.033 -304.033 -304.033] [0.0000], Avg: [-440.026 -440.026 -440.026] (1.000)
Step: 55199, Reward: [-370.659 -370.659 -370.659] [0.0000], Avg: [-439.963 -439.963 -439.963] (1.000)
Step: 55249, Reward: [-405.64 -405.64 -405.64] [0.0000], Avg: [-439.932 -439.932 -439.932] (1.000)
Step: 55299, Reward: [-503.091 -503.091 -503.091] [0.0000], Avg: [-439.989 -439.989 -439.989] (1.000)
Step: 55349, Reward: [-404.28 -404.28 -404.28] [0.0000], Avg: [-439.957 -439.957 -439.957] (1.000)
Step: 55399, Reward: [-281.136 -281.136 -281.136] [0.0000], Avg: [-439.814 -439.814 -439.814] (1.000)
Step: 55449, Reward: [-293.032 -293.032 -293.032] [0.0000], Avg: [-439.681 -439.681 -439.681] (1.000)
Step: 55499, Reward: [-393.23 -393.23 -393.23] [0.0000], Avg: [-439.639 -439.639 -439.639] (1.000)
Step: 55549, Reward: [-338.989 -338.989 -338.989] [0.0000], Avg: [-439.549 -439.549 -439.549] (1.000)
Step: 55599, Reward: [-310.182 -310.182 -310.182] [0.0000], Avg: [-439.433 -439.433 -439.433] (1.000)
Step: 55649, Reward: [-388.075 -388.075 -388.075] [0.0000], Avg: [-439.386 -439.386 -439.386] (1.000)
Step: 55699, Reward: [-414.222 -414.222 -414.222] [0.0000], Avg: [-439.364 -439.364 -439.364] (1.000)
Step: 55749, Reward: [-274.789 -274.789 -274.789] [0.0000], Avg: [-439.216 -439.216 -439.216] (1.000)
Step: 55799, Reward: [-436.468 -436.468 -436.468] [0.0000], Avg: [-439.214 -439.214 -439.214] (1.000)
Step: 55849, Reward: [-536.138 -536.138 -536.138] [0.0000], Avg: [-439.301 -439.301 -439.301] (1.000)
Step: 55899, Reward: [-389.652 -389.652 -389.652] [0.0000], Avg: [-439.256 -439.256 -439.256] (1.000)
Step: 55949, Reward: [-383.951 -383.951 -383.951] [0.0000], Avg: [-439.207 -439.207 -439.207] (1.000)
Step: 55999, Reward: [-278.482 -278.482 -278.482] [0.0000], Avg: [-439.063 -439.063 -439.063] (1.000)
Step: 56049, Reward: [-374.269 -374.269 -374.269] [0.0000], Avg: [-439.005 -439.005 -439.005] (1.000)
Step: 56099, Reward: [-327.905 -327.905 -327.905] [0.0000], Avg: [-438.906 -438.906 -438.906] (1.000)
Step: 56149, Reward: [-404.941 -404.941 -404.941] [0.0000], Avg: [-438.876 -438.876 -438.876] (1.000)
Step: 56199, Reward: [-398.551 -398.551 -398.551] [0.0000], Avg: [-438.84 -438.84 -438.84] (1.000)
Step: 56249, Reward: [-383.249 -383.249 -383.249] [0.0000], Avg: [-438.791 -438.791 -438.791] (1.000)
Step: 56299, Reward: [-275.26 -275.26 -275.26] [0.0000], Avg: [-438.646 -438.646 -438.646] (1.000)
Step: 56349, Reward: [-330.917 -330.917 -330.917] [0.0000], Avg: [-438.55 -438.55 -438.55] (1.000)
Step: 56399, Reward: [-410.203 -410.203 -410.203] [0.0000], Avg: [-438.525 -438.525 -438.525] (1.000)
Step: 56449, Reward: [-355.153 -355.153 -355.153] [0.0000], Avg: [-438.451 -438.451 -438.451] (1.000)
Step: 56499, Reward: [-339.35 -339.35 -339.35] [0.0000], Avg: [-438.363 -438.363 -438.363] (1.000)
Step: 56549, Reward: [-294.405 -294.405 -294.405] [0.0000], Avg: [-438.236 -438.236 -438.236] (1.000)
Step: 56599, Reward: [-365.497 -365.497 -365.497] [0.0000], Avg: [-438.172 -438.172 -438.172] (1.000)
Step: 56649, Reward: [-290.229 -290.229 -290.229] [0.0000], Avg: [-438.041 -438.041 -438.041] (1.000)
Step: 56699, Reward: [-330.96 -330.96 -330.96] [0.0000], Avg: [-437.947 -437.947 -437.947] (1.000)
Step: 56749, Reward: [-330.075 -330.075 -330.075] [0.0000], Avg: [-437.852 -437.852 -437.852] (1.000)
Step: 56799, Reward: [-360.165 -360.165 -360.165] [0.0000], Avg: [-437.783 -437.783 -437.783] (1.000)
Step: 56849, Reward: [-400.134 -400.134 -400.134] [0.0000], Avg: [-437.75 -437.75 -437.75] (1.000)
Step: 56899, Reward: [-344.522 -344.522 -344.522] [0.0000], Avg: [-437.668 -437.668 -437.668] (1.000)
Step: 56949, Reward: [-300.895 -300.895 -300.895] [0.0000], Avg: [-437.548 -437.548 -437.548] (1.000)
Step: 56999, Reward: [-312.854 -312.854 -312.854] [0.0000], Avg: [-437.439 -437.439 -437.439] (1.000)
Step: 57049, Reward: [-489.476 -489.476 -489.476] [0.0000], Avg: [-437.484 -437.484 -437.484] (1.000)
Step: 57099, Reward: [-399.865 -399.865 -399.865] [0.0000], Avg: [-437.452 -437.452 -437.452] (1.000)
Step: 57149, Reward: [-315.414 -315.414 -315.414] [0.0000], Avg: [-437.345 -437.345 -437.345] (1.000)
Step: 57199, Reward: [-275.007 -275.007 -275.007] [0.0000], Avg: [-437.203 -437.203 -437.203] (1.000)
Step: 57249, Reward: [-356.343 -356.343 -356.343] [0.0000], Avg: [-437.132 -437.132 -437.132] (1.000)
Step: 57299, Reward: [-263.327 -263.327 -263.327] [0.0000], Avg: [-436.981 -436.981 -436.981] (1.000)
Step: 57349, Reward: [-405.384 -405.384 -405.384] [0.0000], Avg: [-436.953 -436.953 -436.953] (1.000)
Step: 57399, Reward: [-320.444 -320.444 -320.444] [0.0000], Avg: [-436.852 -436.852 -436.852] (1.000)
Step: 57449, Reward: [-402.637 -402.637 -402.637] [0.0000], Avg: [-436.822 -436.822 -436.822] (1.000)
Step: 57499, Reward: [-389.904 -389.904 -389.904] [0.0000], Avg: [-436.781 -436.781 -436.781] (1.000)
Step: 57549, Reward: [-432.27 -432.27 -432.27] [0.0000], Avg: [-436.777 -436.777 -436.777] (1.000)
Step: 57599, Reward: [-295.91 -295.91 -295.91] [0.0000], Avg: [-436.655 -436.655 -436.655] (1.000)
Step: 57649, Reward: [-455.854 -455.854 -455.854] [0.0000], Avg: [-436.671 -436.671 -436.671] (1.000)
Step: 57699, Reward: [-400.562 -400.562 -400.562] [0.0000], Avg: [-436.64 -436.64 -436.64] (1.000)
Step: 57749, Reward: [-354.593 -354.593 -354.593] [0.0000], Avg: [-436.569 -436.569 -436.569] (1.000)
Step: 57799, Reward: [-285.618 -285.618 -285.618] [0.0000], Avg: [-436.439 -436.439 -436.439] (1.000)
Step: 57849, Reward: [-366.464 -366.464 -366.464] [0.0000], Avg: [-436.378 -436.378 -436.378] (1.000)
Step: 57899, Reward: [-387.228 -387.228 -387.228] [0.0000], Avg: [-436.336 -436.336 -436.336] (1.000)
Step: 57949, Reward: [-348.471 -348.471 -348.471] [0.0000], Avg: [-436.26 -436.26 -436.26] (1.000)
Step: 57999, Reward: [-279.273 -279.273 -279.273] [0.0000], Avg: [-436.124 -436.124 -436.124] (1.000)
Step: 58049, Reward: [-343.114 -343.114 -343.114] [0.0000], Avg: [-436.044 -436.044 -436.044] (1.000)
Step: 58099, Reward: [-394.821 -394.821 -394.821] [0.0000], Avg: [-436.009 -436.009 -436.009] (1.000)
Step: 58149, Reward: [-357.302 -357.302 -357.302] [0.0000], Avg: [-435.941 -435.941 -435.941] (1.000)
Step: 58199, Reward: [-319.285 -319.285 -319.285] [0.0000], Avg: [-435.841 -435.841 -435.841] (1.000)
Step: 58249, Reward: [-334.476 -334.476 -334.476] [0.0000], Avg: [-435.754 -435.754 -435.754] (1.000)
Step: 58299, Reward: [-349.744 -349.744 -349.744] [0.0000], Avg: [-435.68 -435.68 -435.68] (1.000)
Step: 58349, Reward: [-376.378 -376.378 -376.378] [0.0000], Avg: [-435.629 -435.629 -435.629] (1.000)
Step: 58399, Reward: [-429.373 -429.373 -429.373] [0.0000], Avg: [-435.624 -435.624 -435.624] (1.000)
Step: 58449, Reward: [-331.58 -331.58 -331.58] [0.0000], Avg: [-435.535 -435.535 -435.535] (1.000)
Step: 58499, Reward: [-456.91 -456.91 -456.91] [0.0000], Avg: [-435.553 -435.553 -435.553] (1.000)
Step: 58549, Reward: [-320.483 -320.483 -320.483] [0.0000], Avg: [-435.455 -435.455 -435.455] (1.000)
Step: 58599, Reward: [-559.77 -559.77 -559.77] [0.0000], Avg: [-435.561 -435.561 -435.561] (1.000)
Step: 58649, Reward: [-379.856 -379.856 -379.856] [0.0000], Avg: [-435.514 -435.514 -435.514] (1.000)
Step: 58699, Reward: [-412.479 -412.479 -412.479] [0.0000], Avg: [-435.494 -435.494 -435.494] (1.000)
Step: 58749, Reward: [-433.608 -433.608 -433.608] [0.0000], Avg: [-435.492 -435.492 -435.492] (1.000)
Step: 58799, Reward: [-334.04 -334.04 -334.04] [0.0000], Avg: [-435.406 -435.406 -435.406] (1.000)
Step: 58849, Reward: [-281.999 -281.999 -281.999] [0.0000], Avg: [-435.276 -435.276 -435.276] (1.000)
Step: 58899, Reward: [-275.525 -275.525 -275.525] [0.0000], Avg: [-435.14 -435.14 -435.14] (1.000)
Step: 58949, Reward: [-377.934 -377.934 -377.934] [0.0000], Avg: [-435.092 -435.092 -435.092] (1.000)
Step: 58999, Reward: [-357.693 -357.693 -357.693] [0.0000], Avg: [-435.026 -435.026 -435.026] (1.000)
Step: 59049, Reward: [-242.03 -242.03 -242.03] [0.0000], Avg: [-434.863 -434.863 -434.863] (1.000)
Step: 59099, Reward: [-368.75 -368.75 -368.75] [0.0000], Avg: [-434.807 -434.807 -434.807] (1.000)
Step: 59149, Reward: [-521.042 -521.042 -521.042] [0.0000], Avg: [-434.88 -434.88 -434.88] (1.000)
Step: 59199, Reward: [-353.563 -353.563 -353.563] [0.0000], Avg: [-434.811 -434.811 -434.811] (1.000)
Step: 59249, Reward: [-400.26 -400.26 -400.26] [0.0000], Avg: [-434.782 -434.782 -434.782] (1.000)
Step: 59299, Reward: [-417.966 -417.966 -417.966] [0.0000], Avg: [-434.768 -434.768 -434.768] (1.000)
Step: 59349, Reward: [-438.855 -438.855 -438.855] [0.0000], Avg: [-434.771 -434.771 -434.771] (1.000)
Step: 59399, Reward: [-314.016 -314.016 -314.016] [0.0000], Avg: [-434.669 -434.669 -434.669] (1.000)
Step: 59449, Reward: [-417.924 -417.924 -417.924] [0.0000], Avg: [-434.655 -434.655 -434.655] (1.000)
Step: 59499, Reward: [-352.039 -352.039 -352.039] [0.0000], Avg: [-434.586 -434.586 -434.586] (1.000)
Step: 59549, Reward: [-388.687 -388.687 -388.687] [0.0000], Avg: [-434.547 -434.547 -434.547] (1.000)
Step: 59599, Reward: [-418.737 -418.737 -418.737] [0.0000], Avg: [-434.534 -434.534 -434.534] (1.000)
Step: 59649, Reward: [-443.721 -443.721 -443.721] [0.0000], Avg: [-434.542 -434.542 -434.542] (1.000)
Step: 59699, Reward: [-364.569 -364.569 -364.569] [0.0000], Avg: [-434.483 -434.483 -434.483] (1.000)
Step: 59749, Reward: [-403.037 -403.037 -403.037] [0.0000], Avg: [-434.457 -434.457 -434.457] (1.000)
Step: 59799, Reward: [-509.054 -509.054 -509.054] [0.0000], Avg: [-434.519 -434.519 -434.519] (1.000)
Step: 59849, Reward: [-419.566 -419.566 -419.566] [0.0000], Avg: [-434.507 -434.507 -434.507] (1.000)
Step: 59899, Reward: [-326.088 -326.088 -326.088] [0.0000], Avg: [-434.416 -434.416 -434.416] (1.000)
Step: 59949, Reward: [-331.241 -331.241 -331.241] [0.0000], Avg: [-434.33 -434.33 -434.33] (1.000)
Step: 59999, Reward: [-276.996 -276.996 -276.996] [0.0000], Avg: [-434.199 -434.199 -434.199] (1.000)
Step: 60049, Reward: [-444.128 -444.128 -444.128] [0.0000], Avg: [-434.207 -434.207 -434.207] (1.000)
Step: 60099, Reward: [-420.833 -420.833 -420.833] [0.0000], Avg: [-434.196 -434.196 -434.196] (1.000)
Step: 60149, Reward: [-411.183 -411.183 -411.183] [0.0000], Avg: [-434.177 -434.177 -434.177] (1.000)
Step: 60199, Reward: [-381.754 -381.754 -381.754] [0.0000], Avg: [-434.134 -434.134 -434.134] (1.000)
Step: 60249, Reward: [-405.98 -405.98 -405.98] [0.0000], Avg: [-434.11 -434.11 -434.11] (1.000)
Step: 60299, Reward: [-412.693 -412.693 -412.693] [0.0000], Avg: [-434.092 -434.092 -434.092] (1.000)
Step: 60349, Reward: [-370.122 -370.122 -370.122] [0.0000], Avg: [-434.039 -434.039 -434.039] (1.000)
Step: 60399, Reward: [-350.441 -350.441 -350.441] [0.0000], Avg: [-433.97 -433.97 -433.97] (1.000)
Step: 60449, Reward: [-508.3 -508.3 -508.3] [0.0000], Avg: [-434.032 -434.032 -434.032] (1.000)
Step: 60499, Reward: [-484.201 -484.201 -484.201] [0.0000], Avg: [-434.073 -434.073 -434.073] (1.000)
Step: 60549, Reward: [-386.025 -386.025 -386.025] [0.0000], Avg: [-434.033 -434.033 -434.033] (1.000)
Step: 60599, Reward: [-344.778 -344.778 -344.778] [0.0000], Avg: [-433.96 -433.96 -433.96] (1.000)
Step: 60649, Reward: [-324.281 -324.281 -324.281] [0.0000], Avg: [-433.869 -433.869 -433.869] (1.000)
Step: 60699, Reward: [-396.111 -396.111 -396.111] [0.0000], Avg: [-433.838 -433.838 -433.838] (1.000)
Step: 60749, Reward: [-366.027 -366.027 -366.027] [0.0000], Avg: [-433.782 -433.782 -433.782] (1.000)
Step: 60799, Reward: [-325.859 -325.859 -325.859] [0.0000], Avg: [-433.694 -433.694 -433.694] (1.000)
Step: 60849, Reward: [-275.383 -275.383 -275.383] [0.0000], Avg: [-433.564 -433.564 -433.564] (1.000)
Step: 60899, Reward: [-291.48 -291.48 -291.48] [0.0000], Avg: [-433.447 -433.447 -433.447] (1.000)
Step: 60949, Reward: [-340.391 -340.391 -340.391] [0.0000], Avg: [-433.371 -433.371 -433.371] (1.000)
Step: 60999, Reward: [-475.229 -475.229 -475.229] [0.0000], Avg: [-433.405 -433.405 -433.405] (1.000)
Step: 61049, Reward: [-338.836 -338.836 -338.836] [0.0000], Avg: [-433.328 -433.328 -433.328] (1.000)
Step: 61099, Reward: [-355.387 -355.387 -355.387] [0.0000], Avg: [-433.264 -433.264 -433.264] (1.000)
Step: 61149, Reward: [-383.693 -383.693 -383.693] [0.0000], Avg: [-433.223 -433.223 -433.223] (1.000)
Step: 61199, Reward: [-350.91 -350.91 -350.91] [0.0000], Avg: [-433.156 -433.156 -433.156] (1.000)
Step: 61249, Reward: [-478.7 -478.7 -478.7] [0.0000], Avg: [-433.193 -433.193 -433.193] (1.000)
Step: 61299, Reward: [-297.092 -297.092 -297.092] [0.0000], Avg: [-433.082 -433.082 -433.082] (1.000)
Step: 61349, Reward: [-371.658 -371.658 -371.658] [0.0000], Avg: [-433.032 -433.032 -433.032] (1.000)
Step: 61399, Reward: [-356.031 -356.031 -356.031] [0.0000], Avg: [-432.969 -432.969 -432.969] (1.000)
Step: 61449, Reward: [-359.389 -359.389 -359.389] [0.0000], Avg: [-432.909 -432.909 -432.909] (1.000)
Step: 61499, Reward: [-406.273 -406.273 -406.273] [0.0000], Avg: [-432.888 -432.888 -432.888] (1.000)
Step: 61549, Reward: [-263.634 -263.634 -263.634] [0.0000], Avg: [-432.75 -432.75 -432.75] (1.000)
Step: 61599, Reward: [-426.307 -426.307 -426.307] [0.0000], Avg: [-432.745 -432.745 -432.745] (1.000)
Step: 61649, Reward: [-298.722 -298.722 -298.722] [0.0000], Avg: [-432.636 -432.636 -432.636] (1.000)
Step: 61699, Reward: [-388.771 -388.771 -388.771] [0.0000], Avg: [-432.601 -432.601 -432.601] (1.000)
Step: 61749, Reward: [-382.06 -382.06 -382.06] [0.0000], Avg: [-432.56 -432.56 -432.56] (1.000)
Step: 61799, Reward: [-446.906 -446.906 -446.906] [0.0000], Avg: [-432.572 -432.572 -432.572] (1.000)
Step: 61849, Reward: [-341.288 -341.288 -341.288] [0.0000], Avg: [-432.498 -432.498 -432.498] (1.000)
Step: 61899, Reward: [-262.055 -262.055 -262.055] [0.0000], Avg: [-432.36 -432.36 -432.36] (1.000)
Step: 61949, Reward: [-351.593 -351.593 -351.593] [0.0000], Avg: [-432.295 -432.295 -432.295] (1.000)
Step: 61999, Reward: [-276.051 -276.051 -276.051] [0.0000], Avg: [-432.169 -432.169 -432.169] (1.000)
Step: 62049, Reward: [-387.303 -387.303 -387.303] [0.0000], Avg: [-432.133 -432.133 -432.133] (1.000)
Step: 62099, Reward: [-287.229 -287.229 -287.229] [0.0000], Avg: [-432.016 -432.016 -432.016] (1.000)
Step: 62149, Reward: [-306.271 -306.271 -306.271] [0.0000], Avg: [-431.915 -431.915 -431.915] (1.000)
Step: 62199, Reward: [-353.269 -353.269 -353.269] [0.0000], Avg: [-431.852 -431.852 -431.852] (1.000)
Step: 62249, Reward: [-372.95 -372.95 -372.95] [0.0000], Avg: [-431.804 -431.804 -431.804] (1.000)
Step: 62299, Reward: [-505.229 -505.229 -505.229] [0.0000], Avg: [-431.863 -431.863 -431.863] (1.000)
Step: 62349, Reward: [-375.506 -375.506 -375.506] [0.0000], Avg: [-431.818 -431.818 -431.818] (1.000)
Step: 62399, Reward: [-330.671 -330.671 -330.671] [0.0000], Avg: [-431.737 -431.737 -431.737] (1.000)
Step: 62449, Reward: [-269.853 -269.853 -269.853] [0.0000], Avg: [-431.607 -431.607 -431.607] (1.000)
Step: 62499, Reward: [-393.419 -393.419 -393.419] [0.0000], Avg: [-431.577 -431.577 -431.577] (1.000)
Step: 62549, Reward: [-439.087 -439.087 -439.087] [0.0000], Avg: [-431.583 -431.583 -431.583] (1.000)
Step: 62599, Reward: [-326.946 -326.946 -326.946] [0.0000], Avg: [-431.499 -431.499 -431.499] (1.000)
Step: 62649, Reward: [-503.356 -503.356 -503.356] [0.0000], Avg: [-431.557 -431.557 -431.557] (1.000)
Step: 62699, Reward: [-424.066 -424.066 -424.066] [0.0000], Avg: [-431.551 -431.551 -431.551] (1.000)
Step: 62749, Reward: [-261.059 -261.059 -261.059] [0.0000], Avg: [-431.415 -431.415 -431.415] (1.000)
Step: 62799, Reward: [-457.063 -457.063 -457.063] [0.0000], Avg: [-431.435 -431.435 -431.435] (1.000)
Step: 62849, Reward: [-330.789 -330.789 -330.789] [0.0000], Avg: [-431.355 -431.355 -431.355] (1.000)
Step: 62899, Reward: [-353.159 -353.159 -353.159] [0.0000], Avg: [-431.293 -431.293 -431.293] (1.000)
Step: 62949, Reward: [-476.864 -476.864 -476.864] [0.0000], Avg: [-431.329 -431.329 -431.329] (1.000)
Step: 62999, Reward: [-307.08 -307.08 -307.08] [0.0000], Avg: [-431.231 -431.231 -431.231] (1.000)
Step: 63049, Reward: [-366.026 -366.026 -366.026] [0.0000], Avg: [-431.179 -431.179 -431.179] (1.000)
Step: 63099, Reward: [-544.918 -544.918 -544.918] [0.0000], Avg: [-431.269 -431.269 -431.269] (1.000)
Step: 63149, Reward: [-475.514 -475.514 -475.514] [0.0000], Avg: [-431.304 -431.304 -431.304] (1.000)
Step: 63199, Reward: [-303.686 -303.686 -303.686] [0.0000], Avg: [-431.203 -431.203 -431.203] (1.000)
Step: 63249, Reward: [-453.691 -453.691 -453.691] [0.0000], Avg: [-431.221 -431.221 -431.221] (1.000)
Step: 63299, Reward: [-383.094 -383.094 -383.094] [0.0000], Avg: [-431.183 -431.183 -431.183] (1.000)
Step: 63349, Reward: [-377.988 -377.988 -377.988] [0.0000], Avg: [-431.141 -431.141 -431.141] (1.000)
Step: 63399, Reward: [-457.385 -457.385 -457.385] [0.0000], Avg: [-431.162 -431.162 -431.162] (1.000)
Step: 63449, Reward: [-437.864 -437.864 -437.864] [0.0000], Avg: [-431.167 -431.167 -431.167] (1.000)
Step: 63499, Reward: [-410.172 -410.172 -410.172] [0.0000], Avg: [-431.15 -431.15 -431.15] (1.000)
Step: 63549, Reward: [-322.366 -322.366 -322.366] [0.0000], Avg: [-431.065 -431.065 -431.065] (1.000)
Step: 63599, Reward: [-338.451 -338.451 -338.451] [0.0000], Avg: [-430.992 -430.992 -430.992] (1.000)
Step: 63649, Reward: [-437.426 -437.426 -437.426] [0.0000], Avg: [-430.997 -430.997 -430.997] (1.000)
Step: 63699, Reward: [-412.35 -412.35 -412.35] [0.0000], Avg: [-430.982 -430.982 -430.982] (1.000)
Step: 63749, Reward: [-375.274 -375.274 -375.274] [0.0000], Avg: [-430.939 -430.939 -430.939] (1.000)
Step: 63799, Reward: [-439.693 -439.693 -439.693] [0.0000], Avg: [-430.946 -430.946 -430.946] (1.000)
Step: 63849, Reward: [-355.512 -355.512 -355.512] [0.0000], Avg: [-430.886 -430.886 -430.886] (1.000)
Step: 63899, Reward: [-361.584 -361.584 -361.584] [0.0000], Avg: [-430.832 -430.832 -430.832] (1.000)
Step: 63949, Reward: [-408.687 -408.687 -408.687] [0.0000], Avg: [-430.815 -430.815 -430.815] (1.000)
Step: 63999, Reward: [-286.698 -286.698 -286.698] [0.0000], Avg: [-430.702 -430.702 -430.702] (1.000)
Step: 64049, Reward: [-324.217 -324.217 -324.217] [0.0000], Avg: [-430.619 -430.619 -430.619] (1.000)
Step: 64099, Reward: [-296.067 -296.067 -296.067] [0.0000], Avg: [-430.514 -430.514 -430.514] (1.000)
Step: 64149, Reward: [-304.723 -304.723 -304.723] [0.0000], Avg: [-430.416 -430.416 -430.416] (1.000)
Step: 64199, Reward: [-358.582 -358.582 -358.582] [0.0000], Avg: [-430.36 -430.36 -430.36] (1.000)
Step: 64249, Reward: [-348.698 -348.698 -348.698] [0.0000], Avg: [-430.297 -430.297 -430.297] (1.000)
Step: 64299, Reward: [-452.23 -452.23 -452.23] [0.0000], Avg: [-430.314 -430.314 -430.314] (1.000)
Step: 64349, Reward: [-346.398 -346.398 -346.398] [0.0000], Avg: [-430.249 -430.249 -430.249] (1.000)
Step: 64399, Reward: [-316.421 -316.421 -316.421] [0.0000], Avg: [-430.16 -430.16 -430.16] (1.000)
Step: 64449, Reward: [-429.955 -429.955 -429.955] [0.0000], Avg: [-430.16 -430.16 -430.16] (1.000)
Step: 64499, Reward: [-577.484 -577.484 -577.484] [0.0000], Avg: [-430.274 -430.274 -430.274] (1.000)
Step: 64549, Reward: [-432.001 -432.001 -432.001] [0.0000], Avg: [-430.276 -430.276 -430.276] (1.000)
Step: 64599, Reward: [-486.914 -486.914 -486.914] [0.0000], Avg: [-430.319 -430.319 -430.319] (1.000)
Step: 64649, Reward: [-341.161 -341.161 -341.161] [0.0000], Avg: [-430.25 -430.25 -430.25] (1.000)
Step: 64699, Reward: [-360.731 -360.731 -360.731] [0.0000], Avg: [-430.197 -430.197 -430.197] (1.000)
Step: 64749, Reward: [-479.091 -479.091 -479.091] [0.0000], Avg: [-430.234 -430.234 -430.234] (1.000)
Step: 64799, Reward: [-400.231 -400.231 -400.231] [0.0000], Avg: [-430.211 -430.211 -430.211] (1.000)
Step: 64849, Reward: [-367.756 -367.756 -367.756] [0.0000], Avg: [-430.163 -430.163 -430.163] (1.000)
Step: 64899, Reward: [-289.524 -289.524 -289.524] [0.0000], Avg: [-430.055 -430.055 -430.055] (1.000)
Step: 64949, Reward: [-450.339 -450.339 -450.339] [0.0000], Avg: [-430.07 -430.07 -430.07] (1.000)
Step: 64999, Reward: [-398.827 -398.827 -398.827] [0.0000], Avg: [-430.046 -430.046 -430.046] (1.000)
Step: 65049, Reward: [-380.768 -380.768 -380.768] [0.0000], Avg: [-430.009 -430.009 -430.009] (1.000)
Step: 65099, Reward: [-499.508 -499.508 -499.508] [0.0000], Avg: [-430.062 -430.062 -430.062] (1.000)
Step: 65149, Reward: [-375.323 -375.323 -375.323] [0.0000], Avg: [-430.02 -430.02 -430.02] (1.000)
Step: 65199, Reward: [-543.1 -543.1 -543.1] [0.0000], Avg: [-430.107 -430.107 -430.107] (1.000)
Step: 65249, Reward: [-311.284 -311.284 -311.284] [0.0000], Avg: [-430.016 -430.016 -430.016] (1.000)
Step: 65299, Reward: [-428.812 -428.812 -428.812] [0.0000], Avg: [-430.015 -430.015 -430.015] (1.000)
Step: 65349, Reward: [-385.078 -385.078 -385.078] [0.0000], Avg: [-429.98 -429.98 -429.98] (1.000)
Step: 65399, Reward: [-337.587 -337.587 -337.587] [0.0000], Avg: [-429.91 -429.91 -429.91] (1.000)
Step: 65449, Reward: [-424.023 -424.023 -424.023] [0.0000], Avg: [-429.905 -429.905 -429.905] (1.000)
Step: 65499, Reward: [-500.017 -500.017 -500.017] [0.0000], Avg: [-429.959 -429.959 -429.959] (1.000)
Step: 65549, Reward: [-436.359 -436.359 -436.359] [0.0000], Avg: [-429.964 -429.964 -429.964] (1.000)
Step: 65599, Reward: [-482.708 -482.708 -482.708] [0.0000], Avg: [-430.004 -430.004 -430.004] (1.000)
Step: 65649, Reward: [-364.361 -364.361 -364.361] [0.0000], Avg: [-429.954 -429.954 -429.954] (1.000)
Step: 65699, Reward: [-285.273 -285.273 -285.273] [0.0000], Avg: [-429.844 -429.844 -429.844] (1.000)
Step: 65749, Reward: [-384.241 -384.241 -384.241] [0.0000], Avg: [-429.809 -429.809 -429.809] (1.000)
Step: 65799, Reward: [-524.286 -524.286 -524.286] [0.0000], Avg: [-429.881 -429.881 -429.881] (1.000)
Step: 65849, Reward: [-487.491 -487.491 -487.491] [0.0000], Avg: [-429.924 -429.924 -429.924] (1.000)
Step: 65899, Reward: [-507.875 -507.875 -507.875] [0.0000], Avg: [-429.984 -429.984 -429.984] (1.000)
Step: 65949, Reward: [-380.059 -380.059 -380.059] [0.0000], Avg: [-429.946 -429.946 -429.946] (1.000)
Step: 65999, Reward: [-311.329 -311.329 -311.329] [0.0000], Avg: [-429.856 -429.856 -429.856] (1.000)
Step: 66049, Reward: [-509.295 -509.295 -509.295] [0.0000], Avg: [-429.916 -429.916 -429.916] (1.000)
Step: 66099, Reward: [-309.848 -309.848 -309.848] [0.0000], Avg: [-429.825 -429.825 -429.825] (1.000)
Step: 66149, Reward: [-327.905 -327.905 -327.905] [0.0000], Avg: [-429.748 -429.748 -429.748] (1.000)
Step: 66199, Reward: [-287.176 -287.176 -287.176] [0.0000], Avg: [-429.641 -429.641 -429.641] (1.000)
Step: 66249, Reward: [-349.815 -349.815 -349.815] [0.0000], Avg: [-429.58 -429.58 -429.58] (1.000)
Step: 66299, Reward: [-313.277 -313.277 -313.277] [0.0000], Avg: [-429.493 -429.493 -429.493] (1.000)
Step: 66349, Reward: [-350.594 -350.594 -350.594] [0.0000], Avg: [-429.433 -429.433 -429.433] (1.000)
Step: 66399, Reward: [-403.512 -403.512 -403.512] [0.0000], Avg: [-429.414 -429.414 -429.414] (1.000)
Step: 66449, Reward: [-401.16 -401.16 -401.16] [0.0000], Avg: [-429.392 -429.392 -429.392] (1.000)
Step: 66499, Reward: [-480.453 -480.453 -480.453] [0.0000], Avg: [-429.431 -429.431 -429.431] (1.000)
