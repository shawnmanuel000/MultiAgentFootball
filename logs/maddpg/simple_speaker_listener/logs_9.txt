Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_speaker_listener
num_envs: 16, state_size: [(1, 3), (1, 11)], action_size: [[1, 3], [1, 5]], action_space: [MultiDiscrete([3]), MultiDiscrete([5])],

import torch
import random
import numpy as np
from models.rand import MultiagentReplayBuffer
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, LEARN_RATE, DISCOUNT_RATE, EPS_MIN, EPS_DECAY, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, MAX_BUFFER_SIZE, TARGET_UPDATE_RATE, gsoftmax, one_hot

REPLAY_BATCH_SIZE = 1024
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		return action_mu
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=False, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = lambda s,a: MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)

	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [gsoftmax(model.get_action(s, use_target, grad, numpy=False), hard=True) for s,model in zip(state, self.models)]
			return [a.cpu().numpy() if numpy else a for a in action]

	def optimize(self, states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE, e_weight=ENTROPY_WEIGHT):
		for i, agent in enumerate(self.models):
			next_actions = [one_hot(model.get_action(nobs, numpy=False)) for model, nobs in zip(self.models, next_states)]
			next_states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(next_states, self.state_size)], dim=-1)
			next_actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(next_actions, self.action_size)], dim=-1)
			next_value = agent.get_q_value(next_states_joint, next_actions_joint, use_target=True, numpy=False)
			q_target = (rewards[i].view(-1, 1) + gamma * next_value * (1 - dones[i].view(-1, 1)))

			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_value = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_loss = (q_value - q_target.detach()).pow(2).mean()
			agent.step(agent.critic_optimizer, critic_loss, param_norm=agent.critic_local.parameters())
			agent.soft_copy(agent.critic_local, agent.critic_target)

			actor_action = agent.get_action(states[i], grad=True, numpy=False)
			action = [gsoftmax(actor_action, hard=True) if j==i else one_hot(model.get_action(ob, numpy=False)) for (j,model), ob in zip(enumerate(self.models), states)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(action, self.action_size)], dim=-1)
			actor_loss = -(agent.critic_local(states_joint, action_joint)-q_target).mean() + e_weight*actor_action.pow(2).mean() 
			agent.step(agent.actor_optimizer, actor_loss, param_norm=agent.actor_local.parameters())
			agent.soft_copy(agent.actor_local, agent.actor_target)

	def save_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.save_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		[PTACNetwork.load_model(model, "maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		action = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		return action

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.replay_buffer.push(state, action, next_state, reward, done)
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			states, actions, next_states, rewards, dones = self.replay_buffer.sample(REPLAY_BATCH_SIZE, to_gpu=False)
			self.network.optimize(states, actions, next_states, rewards, dones, gamma=DISCOUNT_RATE)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-44.447 -44.447] [0.0000], Avg: [-44.447 -44.447] (1.000)
Step: 99, Reward: [-135.016 -135.016] [0.0000], Avg: [-89.732 -89.732] (1.000)
Step: 149, Reward: [-810.248 -810.248] [0.0000], Avg: [-329.904 -329.904] (1.000)
Step: 199, Reward: [-314.089 -314.089] [0.0000], Avg: [-325.95 -325.95] (1.000)
Step: 249, Reward: [-454.612 -454.612] [0.0000], Avg: [-351.682 -351.682] (1.000)
Step: 299, Reward: [-162.909 -162.909] [0.0000], Avg: [-320.22 -320.22] (1.000)
Step: 349, Reward: [-353.602 -353.602] [0.0000], Avg: [-324.989 -324.989] (1.000)
Step: 399, Reward: [-158.248 -158.248] [0.0000], Avg: [-304.146 -304.146] (1.000)
Step: 449, Reward: [-835.409 -835.409] [0.0000], Avg: [-363.176 -363.176] (1.000)
Step: 499, Reward: [-1651.572 -1651.572] [0.0000], Avg: [-492.015 -492.015] (1.000)
Step: 549, Reward: [-1582.54 -1582.54] [0.0000], Avg: [-591.154 -591.154] (1.000)
Step: 599, Reward: [-931.219 -931.219] [0.0000], Avg: [-619.493 -619.493] (1.000)
Step: 649, Reward: [-1687.753 -1687.753] [0.0000], Avg: [-701.666 -701.666] (1.000)
Step: 699, Reward: [-3057.281 -3057.281] [0.0000], Avg: [-869.925 -869.925] (1.000)
Step: 749, Reward: [-2700.109 -2700.109] [0.0000], Avg: [-991.937 -991.937] (1.000)
Step: 799, Reward: [-2730.541 -2730.541] [0.0000], Avg: [-1100.6 -1100.6] (1.000)
Step: 849, Reward: [-4052.169 -4052.169] [0.0000], Avg: [-1274.221 -1274.221] (1.000)
Step: 899, Reward: [-2713.711 -2713.711] [0.0000], Avg: [-1354.193 -1354.193] (1.000)
Step: 949, Reward: [-931.408 -931.408] [0.0000], Avg: [-1331.941 -1331.941] (1.000)
Step: 999, Reward: [-3624.636 -3624.636] [0.0000], Avg: [-1446.576 -1446.576] (1.000)
Step: 1049, Reward: [-2749.631 -2749.631] [0.0000], Avg: [-1508.626 -1508.626] (1.000)
Step: 1099, Reward: [-266.725 -266.725] [0.0000], Avg: [-1452.176 -1452.176] (1.000)
Step: 1149, Reward: [-400.917 -400.917] [0.0000], Avg: [-1406.469 -1406.469] (1.000)
Step: 1199, Reward: [-208.781 -208.781] [0.0000], Avg: [-1356.566 -1356.566] (1.000)
Step: 1249, Reward: [-523.469 -523.469] [0.0000], Avg: [-1323.242 -1323.242] (1.000)
Step: 1299, Reward: [-766.536 -766.536] [0.0000], Avg: [-1301.83 -1301.83] (1.000)
Step: 1349, Reward: [-600.386 -600.386] [0.0000], Avg: [-1275.851 -1275.851] (1.000)
Step: 1399, Reward: [-672.294 -672.294] [0.0000], Avg: [-1254.295 -1254.295] (1.000)
Step: 1449, Reward: [-813.206 -813.206] [0.0000], Avg: [-1239.085 -1239.085] (1.000)
Step: 1499, Reward: [-1406.302 -1406.302] [0.0000], Avg: [-1244.659 -1244.659] (1.000)
Step: 1549, Reward: [-1868.549 -1868.549] [0.0000], Avg: [-1264.784 -1264.784] (1.000)
Step: 1599, Reward: [-1900.799 -1900.799] [0.0000], Avg: [-1284.66 -1284.66] (1.000)
Step: 1649, Reward: [-2455.126 -2455.126] [0.0000], Avg: [-1320.129 -1320.129] (1.000)
Step: 1699, Reward: [-1571.783 -1571.783] [0.0000], Avg: [-1327.53 -1327.53] (1.000)
Step: 1749, Reward: [-2010.282 -2010.282] [0.0000], Avg: [-1347.037 -1347.037] (1.000)
Step: 1799, Reward: [-2556.848 -2556.848] [0.0000], Avg: [-1380.643 -1380.643] (1.000)
Step: 1849, Reward: [-3044.313 -3044.313] [0.0000], Avg: [-1425.607 -1425.607] (1.000)
Step: 1899, Reward: [-4160.701 -4160.701] [0.0000], Avg: [-1497.583 -1497.583] (1.000)
Step: 1949, Reward: [-3610.581 -3610.581] [0.0000], Avg: [-1551.763 -1551.763] (1.000)
Step: 1999, Reward: [-1257.876 -1257.876] [0.0000], Avg: [-1544.416 -1544.416] (1.000)
Step: 2049, Reward: [-2017.469 -2017.469] [0.0000], Avg: [-1555.954 -1555.954] (1.000)
Step: 2099, Reward: [-3734.719 -3734.719] [0.0000], Avg: [-1607.829 -1607.829] (1.000)
Step: 2149, Reward: [-1785.903 -1785.903] [0.0000], Avg: [-1611.97 -1611.97] (1.000)
Step: 2199, Reward: [-1303.903 -1303.903] [0.0000], Avg: [-1604.969 -1604.969] (1.000)
Step: 2249, Reward: [-1549.94 -1549.94] [0.0000], Avg: [-1603.746 -1603.746] (1.000)
Step: 2299, Reward: [-3590.378 -3590.378] [0.0000], Avg: [-1646.933 -1646.933] (1.000)
Step: 2349, Reward: [-2367.448 -2367.448] [0.0000], Avg: [-1662.263 -1662.263] (1.000)
Step: 2399, Reward: [-2398.313 -2398.313] [0.0000], Avg: [-1677.598 -1677.598] (1.000)
Step: 2449, Reward: [-763.239 -763.239] [0.0000], Avg: [-1658.937 -1658.937] (1.000)
Step: 2499, Reward: [-848.766 -848.766] [0.0000], Avg: [-1642.734 -1642.734] (1.000)
Step: 2549, Reward: [-2002.27 -2002.27] [0.0000], Avg: [-1649.784 -1649.784] (1.000)
Step: 2599, Reward: [-2269.387 -2269.387] [0.0000], Avg: [-1661.699 -1661.699] (1.000)
Step: 2649, Reward: [-2299.772 -2299.772] [0.0000], Avg: [-1673.738 -1673.738] (1.000)
Step: 2699, Reward: [-1713.842 -1713.842] [0.0000], Avg: [-1674.481 -1674.481] (1.000)
Step: 2749, Reward: [-1066.476 -1066.476] [0.0000], Avg: [-1663.426 -1663.426] (1.000)
Step: 2799, Reward: [-757.057 -757.057] [0.0000], Avg: [-1647.241 -1647.241] (1.000)
Step: 2849, Reward: [-346.795 -346.795] [0.0000], Avg: [-1624.426 -1624.426] (1.000)
Step: 2899, Reward: [-758.686 -758.686] [0.0000], Avg: [-1609.5 -1609.5] (1.000)
Step: 2949, Reward: [-1017.999 -1017.999] [0.0000], Avg: [-1599.474 -1599.474] (1.000)
Step: 2999, Reward: [-782.475 -782.475] [0.0000], Avg: [-1585.858 -1585.858] (1.000)
Step: 3049, Reward: [-830.936 -830.936] [0.0000], Avg: [-1573.482 -1573.482] (1.000)
Step: 3099, Reward: [-359.089 -359.089] [0.0000], Avg: [-1553.895 -1553.895] (1.000)
Step: 3149, Reward: [-627.99 -627.99] [0.0000], Avg: [-1539.198 -1539.198] (1.000)
Step: 3199, Reward: [-673.612 -673.612] [0.0000], Avg: [-1525.673 -1525.673] (1.000)
Step: 3249, Reward: [-596.035 -596.035] [0.0000], Avg: [-1511.371 -1511.371] (1.000)
Step: 3299, Reward: [-1774.661 -1774.661] [0.0000], Avg: [-1515.36 -1515.36] (1.000)
Step: 3349, Reward: [-385.583 -385.583] [0.0000], Avg: [-1498.498 -1498.498] (1.000)
Step: 3399, Reward: [-659.544 -659.544] [0.0000], Avg: [-1486.16 -1486.16] (1.000)
Step: 3449, Reward: [-433.762 -433.762] [0.0000], Avg: [-1470.908 -1470.908] (1.000)
Step: 3499, Reward: [-174.583 -174.583] [0.0000], Avg: [-1452.389 -1452.389] (1.000)
Step: 3549, Reward: [-460.099 -460.099] [0.0000], Avg: [-1438.413 -1438.413] (1.000)
Step: 3599, Reward: [-903.108 -903.108] [0.0000], Avg: [-1430.979 -1430.979] (1.000)
Step: 3649, Reward: [-186.214 -186.214] [0.0000], Avg: [-1413.927 -1413.927] (1.000)
Step: 3699, Reward: [-478.712 -478.712] [0.0000], Avg: [-1401.289 -1401.289] (1.000)
Step: 3749, Reward: [-679.484 -679.484] [0.0000], Avg: [-1391.665 -1391.665] (1.000)
Step: 3799, Reward: [-92.329 -92.329] [0.0000], Avg: [-1374.568 -1374.568] (1.000)
Step: 3849, Reward: [-727.903 -727.903] [0.0000], Avg: [-1366.17 -1366.17] (1.000)
Step: 3899, Reward: [-93.635 -93.635] [0.0000], Avg: [-1349.856 -1349.856] (1.000)
Step: 3949, Reward: [-984.112 -984.112] [0.0000], Avg: [-1345.226 -1345.226] (1.000)
Step: 3999, Reward: [-481.116 -481.116] [0.0000], Avg: [-1334.425 -1334.425] (1.000)
Step: 4049, Reward: [-675.901 -675.901] [0.0000], Avg: [-1326.295 -1326.295] (1.000)
Step: 4099, Reward: [-504.428 -504.428] [0.0000], Avg: [-1316.272 -1316.272] (1.000)
Step: 4149, Reward: [-370.819 -370.819] [0.0000], Avg: [-1304.881 -1304.881] (1.000)
Step: 4199, Reward: [-111.098 -111.098] [0.0000], Avg: [-1290.669 -1290.669] (1.000)
Step: 4249, Reward: [-447.125 -447.125] [0.0000], Avg: [-1280.745 -1280.745] (1.000)
Step: 4299, Reward: [-425.86 -425.86] [0.0000], Avg: [-1270.805 -1270.805] (1.000)
Step: 4349, Reward: [-460.354 -460.354] [0.0000], Avg: [-1261.489 -1261.489] (1.000)
Step: 4399, Reward: [-176.021 -176.021] [0.0000], Avg: [-1249.154 -1249.154] (1.000)
Step: 4449, Reward: [-261.244 -261.244] [0.0000], Avg: [-1238.054 -1238.054] (1.000)
Step: 4499, Reward: [-484.609 -484.609] [0.0000], Avg: [-1229.683 -1229.683] (1.000)
Step: 4549, Reward: [-389.745 -389.745] [0.0000], Avg: [-1220.452 -1220.452] (1.000)
Step: 4599, Reward: [-158.31 -158.31] [0.0000], Avg: [-1208.907 -1208.907] (1.000)
Step: 4649, Reward: [-140.566 -140.566] [0.0000], Avg: [-1197.42 -1197.42] (1.000)
Step: 4699, Reward: [-576.16 -576.16] [0.0000], Avg: [-1190.811 -1190.811] (1.000)
Step: 4749, Reward: [-535.935 -535.935] [0.0000], Avg: [-1183.917 -1183.917] (1.000)
Step: 4799, Reward: [-187.371 -187.371] [0.0000], Avg: [-1173.537 -1173.537] (1.000)
Step: 4849, Reward: [-278.27 -278.27] [0.0000], Avg: [-1164.307 -1164.307] (1.000)
Step: 4899, Reward: [-225.877 -225.877] [0.0000], Avg: [-1154.731 -1154.731] (1.000)
Step: 4949, Reward: [-795.184 -795.184] [0.0000], Avg: [-1151.099 -1151.099] (1.000)
Step: 4999, Reward: [-457.244 -457.244] [0.0000], Avg: [-1144.161 -1144.161] (1.000)
Step: 5049, Reward: [-858.489 -858.489] [0.0000], Avg: [-1141.332 -1141.332] (1.000)
Step: 5099, Reward: [-837.084 -837.084] [0.0000], Avg: [-1138.35 -1138.35] (1.000)
Step: 5149, Reward: [-616.948 -616.948] [0.0000], Avg: [-1133.287 -1133.287] (1.000)
Step: 5199, Reward: [-835.564 -835.564] [0.0000], Avg: [-1130.425 -1130.425] (1.000)
Step: 5249, Reward: [-307.481 -307.481] [0.0000], Avg: [-1122.587 -1122.587] (1.000)
Step: 5299, Reward: [-481.279 -481.279] [0.0000], Avg: [-1116.537 -1116.537] (1.000)
Step: 5349, Reward: [-1514.906 -1514.906] [0.0000], Avg: [-1120.26 -1120.26] (1.000)
Step: 5399, Reward: [-791.385 -791.385] [0.0000], Avg: [-1117.215 -1117.215] (1.000)
Step: 5449, Reward: [-1746.396 -1746.396] [0.0000], Avg: [-1122.987 -1122.987] (1.000)
Step: 5499, Reward: [-836.73 -836.73] [0.0000], Avg: [-1120.385 -1120.385] (1.000)
Step: 5549, Reward: [-1454.384 -1454.384] [0.0000], Avg: [-1123.394 -1123.394] (1.000)
Step: 5599, Reward: [-1512.84 -1512.84] [0.0000], Avg: [-1126.871 -1126.871] (1.000)
Step: 5649, Reward: [-1069.954 -1069.954] [0.0000], Avg: [-1126.367 -1126.367] (1.000)
Step: 5699, Reward: [-1373.2 -1373.2] [0.0000], Avg: [-1128.533 -1128.533] (1.000)
Step: 5749, Reward: [-1602.544 -1602.544] [0.0000], Avg: [-1132.655 -1132.655] (1.000)
Step: 5799, Reward: [-1130.052 -1130.052] [0.0000], Avg: [-1132.632 -1132.632] (1.000)
Step: 5849, Reward: [-1389.262 -1389.262] [0.0000], Avg: [-1134.826 -1134.826] (1.000)
Step: 5899, Reward: [-939.675 -939.675] [0.0000], Avg: [-1133.172 -1133.172] (1.000)
Step: 5949, Reward: [-616.408 -616.408] [0.0000], Avg: [-1128.829 -1128.829] (1.000)
Step: 5999, Reward: [-625.69 -625.69] [0.0000], Avg: [-1124.636 -1124.636] (1.000)
Step: 6049, Reward: [-451.999 -451.999] [0.0000], Avg: [-1119.077 -1119.077] (1.000)
Step: 6099, Reward: [-717.471 -717.471] [0.0000], Avg: [-1115.785 -1115.785] (1.000)
Step: 6149, Reward: [-266.742 -266.742] [0.0000], Avg: [-1108.883 -1108.883] (1.000)
Step: 6199, Reward: [-653.807 -653.807] [0.0000], Avg: [-1105.213 -1105.213] (1.000)
Step: 6249, Reward: [-663.654 -663.654] [0.0000], Avg: [-1101.68 -1101.68] (1.000)
Step: 6299, Reward: [-643.877 -643.877] [0.0000], Avg: [-1098.047 -1098.047] (1.000)
Step: 6349, Reward: [-133.79 -133.79] [0.0000], Avg: [-1090.454 -1090.454] (1.000)
Step: 6399, Reward: [-233.445 -233.445] [0.0000], Avg: [-1083.759 -1083.759] (1.000)
Step: 6449, Reward: [-515.963 -515.963] [0.0000], Avg: [-1079.357 -1079.357] (1.000)
Step: 6499, Reward: [-438.883 -438.883] [0.0000], Avg: [-1074.431 -1074.431] (1.000)
Step: 6549, Reward: [-518.177 -518.177] [0.0000], Avg: [-1070.184 -1070.184] (1.000)
Step: 6599, Reward: [-360.782 -360.782] [0.0000], Avg: [-1064.81 -1064.81] (1.000)
Step: 6649, Reward: [-397.97 -397.97] [0.0000], Avg: [-1059.796 -1059.796] (1.000)
Step: 6699, Reward: [-138.134 -138.134] [0.0000], Avg: [-1052.918 -1052.918] (1.000)
Step: 6749, Reward: [-190.711 -190.711] [0.0000], Avg: [-1046.532 -1046.532] (1.000)
Step: 6799, Reward: [-226.446 -226.446] [0.0000], Avg: [-1040.502 -1040.502] (1.000)
Step: 6849, Reward: [-140.599 -140.599] [0.0000], Avg: [-1033.933 -1033.933] (1.000)
Step: 6899, Reward: [-198.095 -198.095] [0.0000], Avg: [-1027.876 -1027.876] (1.000)
Step: 6949, Reward: [-409.467 -409.467] [0.0000], Avg: [-1023.427 -1023.427] (1.000)
Step: 6999, Reward: [-381.963 -381.963] [0.0000], Avg: [-1018.845 -1018.845] (1.000)
Step: 7049, Reward: [-288.43 -288.43] [0.0000], Avg: [-1013.665 -1013.665] (1.000)
Step: 7099, Reward: [-128.101 -128.101] [0.0000], Avg: [-1007.429 -1007.429] (1.000)
Step: 7149, Reward: [-314.723 -314.723] [0.0000], Avg: [-1002.585 -1002.585] (1.000)
Step: 7199, Reward: [-413.719 -413.719] [0.0000], Avg: [-998.495 -998.495] (1.000)
Step: 7249, Reward: [-753.416 -753.416] [0.0000], Avg: [-996.805 -996.805] (1.000)
Step: 7299, Reward: [-667.082 -667.082] [0.0000], Avg: [-994.547 -994.547] (1.000)
Step: 7349, Reward: [-1401.234 -1401.234] [0.0000], Avg: [-997.313 -997.313] (1.000)
Step: 7399, Reward: [-488.632 -488.632] [0.0000], Avg: [-993.876 -993.876] (1.000)
Step: 7449, Reward: [-1079.017 -1079.017] [0.0000], Avg: [-994.448 -994.448] (1.000)
Step: 7499, Reward: [-241.923 -241.923] [0.0000], Avg: [-989.431 -989.431] (1.000)
Step: 7549, Reward: [-967.489 -967.489] [0.0000], Avg: [-989.285 -989.285] (1.000)
Step: 7599, Reward: [-563.732 -563.732] [0.0000], Avg: [-986.486 -986.486] (1.000)
Step: 7649, Reward: [-1319.727 -1319.727] [0.0000], Avg: [-988.664 -988.664] (1.000)
Step: 7699, Reward: [-860.57 -860.57] [0.0000], Avg: [-987.832 -987.832] (1.000)
Step: 7749, Reward: [-796.083 -796.083] [0.0000], Avg: [-986.595 -986.595] (1.000)
Step: 7799, Reward: [-867.231 -867.231] [0.0000], Avg: [-985.83 -985.83] (1.000)
Step: 7849, Reward: [-1585.963 -1585.963] [0.0000], Avg: [-989.652 -989.652] (1.000)
Step: 7899, Reward: [-1002.462 -1002.462] [0.0000], Avg: [-989.733 -989.733] (1.000)
Step: 7949, Reward: [-1300.302 -1300.302] [0.0000], Avg: [-991.687 -991.687] (1.000)
Step: 7999, Reward: [-1853.932 -1853.932] [0.0000], Avg: [-997.076 -997.076] (1.000)
Step: 8049, Reward: [-1914.567 -1914.567] [0.0000], Avg: [-1002.774 -1002.774] (1.000)
Step: 8099, Reward: [-1894.925 -1894.925] [0.0000], Avg: [-1008.281 -1008.281] (1.000)
Step: 8149, Reward: [-2050.473 -2050.473] [0.0000], Avg: [-1014.675 -1014.675] (1.000)
Step: 8199, Reward: [-982.771 -982.771] [0.0000], Avg: [-1014.481 -1014.481] (1.000)
Step: 8249, Reward: [-1374.948 -1374.948] [0.0000], Avg: [-1016.665 -1016.665] (1.000)
Step: 8299, Reward: [-2210.268 -2210.268] [0.0000], Avg: [-1023.856 -1023.856] (1.000)
Step: 8349, Reward: [-1049.84 -1049.84] [0.0000], Avg: [-1024.011 -1024.011] (1.000)
Step: 8399, Reward: [-1497.438 -1497.438] [0.0000], Avg: [-1026.829 -1026.829] (1.000)
Step: 8449, Reward: [-892.842 -892.842] [0.0000], Avg: [-1026.037 -1026.037] (1.000)
Step: 8499, Reward: [-1119.415 -1119.415] [0.0000], Avg: [-1026.586 -1026.586] (1.000)
Step: 8549, Reward: [-1156.561 -1156.561] [0.0000], Avg: [-1027.346 -1027.346] (1.000)
Step: 8599, Reward: [-867.408 -867.408] [0.0000], Avg: [-1026.416 -1026.416] (1.000)
Step: 8649, Reward: [-2569.226 -2569.226] [0.0000], Avg: [-1035.334 -1035.334] (1.000)
Step: 8699, Reward: [-783.238 -783.238] [0.0000], Avg: [-1033.885 -1033.885] (1.000)
Step: 8749, Reward: [-975.154 -975.154] [0.0000], Avg: [-1033.55 -1033.55] (1.000)
Step: 8799, Reward: [-463.473 -463.473] [0.0000], Avg: [-1030.311 -1030.311] (1.000)
Step: 8849, Reward: [-594.1 -594.1] [0.0000], Avg: [-1027.846 -1027.846] (1.000)
Step: 8899, Reward: [-968.293 -968.293] [0.0000], Avg: [-1027.511 -1027.511] (1.000)
Step: 8949, Reward: [-681.164 -681.164] [0.0000], Avg: [-1025.577 -1025.577] (1.000)
Step: 8999, Reward: [-1013.119 -1013.119] [0.0000], Avg: [-1025.507 -1025.507] (1.000)
Step: 9049, Reward: [-1428.639 -1428.639] [0.0000], Avg: [-1027.735 -1027.735] (1.000)
Step: 9099, Reward: [-710.071 -710.071] [0.0000], Avg: [-1025.989 -1025.989] (1.000)
Step: 9149, Reward: [-600.072 -600.072] [0.0000], Avg: [-1023.662 -1023.662] (1.000)
Step: 9199, Reward: [-855.583 -855.583] [0.0000], Avg: [-1022.748 -1022.748] (1.000)
Step: 9249, Reward: [-597.131 -597.131] [0.0000], Avg: [-1020.448 -1020.448] (1.000)
Step: 9299, Reward: [-812.889 -812.889] [0.0000], Avg: [-1019.332 -1019.332] (1.000)
Step: 9349, Reward: [-738.185 -738.185] [0.0000], Avg: [-1017.828 -1017.828] (1.000)
Step: 9399, Reward: [-1052.816 -1052.816] [0.0000], Avg: [-1018.014 -1018.014] (1.000)
Step: 9449, Reward: [-843.227 -843.227] [0.0000], Avg: [-1017.09 -1017.09] (1.000)
Step: 9499, Reward: [-738.782 -738.782] [0.0000], Avg: [-1015.625 -1015.625] (1.000)
Step: 9549, Reward: [-688.628 -688.628] [0.0000], Avg: [-1013.913 -1013.913] (1.000)
Step: 9599, Reward: [-782.175 -782.175] [0.0000], Avg: [-1012.706 -1012.706] (1.000)
Step: 9649, Reward: [-724.773 -724.773] [0.0000], Avg: [-1011.214 -1011.214] (1.000)
Step: 9699, Reward: [-990.062 -990.062] [0.0000], Avg: [-1011.105 -1011.105] (1.000)
Step: 9749, Reward: [-1016.192 -1016.192] [0.0000], Avg: [-1011.131 -1011.131] (1.000)
Step: 9799, Reward: [-719.749 -719.749] [0.0000], Avg: [-1009.644 -1009.644] (1.000)
Step: 9849, Reward: [-442.191 -442.191] [0.0000], Avg: [-1006.764 -1006.764] (1.000)
Step: 9899, Reward: [-962.82 -962.82] [0.0000], Avg: [-1006.542 -1006.542] (1.000)
Step: 9949, Reward: [-498.658 -498.658] [0.0000], Avg: [-1003.99 -1003.99] (1.000)
Step: 9999, Reward: [-327.167 -327.167] [0.0000], Avg: [-1000.606 -1000.606] (1.000)
Step: 10049, Reward: [-888.642 -888.642] [0.0000], Avg: [-1000.049 -1000.049] (1.000)
Step: 10099, Reward: [-448.814 -448.814] [0.0000], Avg: [-997.32 -997.32] (1.000)
Step: 10149, Reward: [-773.158 -773.158] [0.0000], Avg: [-996.216 -996.216] (1.000)
Step: 10199, Reward: [-282.838 -282.838] [0.0000], Avg: [-992.719 -992.719] (1.000)
Step: 10249, Reward: [-846.586 -846.586] [0.0000], Avg: [-992.006 -992.006] (1.000)
Step: 10299, Reward: [-729.925 -729.925] [0.0000], Avg: [-990.733 -990.733] (1.000)
Step: 10349, Reward: [-1441.508 -1441.508] [0.0000], Avg: [-992.911 -992.911] (1.000)
Step: 10399, Reward: [-721.732 -721.732] [0.0000], Avg: [-991.607 -991.607] (1.000)
Step: 10449, Reward: [-1872.924 -1872.924] [0.0000], Avg: [-995.824 -995.824] (1.000)
Step: 10499, Reward: [-984.398 -984.398] [0.0000], Avg: [-995.77 -995.77] (1.000)
Step: 10549, Reward: [-1781.968 -1781.968] [0.0000], Avg: [-999.496 -999.496] (1.000)
Step: 10599, Reward: [-1443.977 -1443.977] [0.0000], Avg: [-1001.592 -1001.592] (1.000)
Step: 10649, Reward: [-1373.611 -1373.611] [0.0000], Avg: [-1003.339 -1003.339] (1.000)
Step: 10699, Reward: [-1982.947 -1982.947] [0.0000], Avg: [-1007.917 -1007.917] (1.000)
Step: 10749, Reward: [-1649.291 -1649.291] [0.0000], Avg: [-1010.9 -1010.9] (1.000)
Step: 10799, Reward: [-1302.703 -1302.703] [0.0000], Avg: [-1012.251 -1012.251] (1.000)
Step: 10849, Reward: [-3158.267 -3158.267] [0.0000], Avg: [-1022.14 -1022.14] (1.000)
Step: 10899, Reward: [-1629.203 -1629.203] [0.0000], Avg: [-1024.925 -1024.925] (1.000)
Step: 10949, Reward: [-1601.8 -1601.8] [0.0000], Avg: [-1027.559 -1027.559] (1.000)
Step: 10999, Reward: [-1033.868 -1033.868] [0.0000], Avg: [-1027.588 -1027.588] (1.000)
Step: 11049, Reward: [-2186.198 -2186.198] [0.0000], Avg: [-1032.83 -1032.83] (1.000)
Step: 11099, Reward: [-2365.803 -2365.803] [0.0000], Avg: [-1038.835 -1038.835] (1.000)
Step: 11149, Reward: [-1540.769 -1540.769] [0.0000], Avg: [-1041.085 -1041.085] (1.000)
Step: 11199, Reward: [-1316.549 -1316.549] [0.0000], Avg: [-1042.315 -1042.315] (1.000)
Step: 11249, Reward: [-684.66 -684.66] [0.0000], Avg: [-1040.726 -1040.726] (1.000)
Step: 11299, Reward: [-2697.908 -2697.908] [0.0000], Avg: [-1048.058 -1048.058] (1.000)
Step: 11349, Reward: [-1552.569 -1552.569] [0.0000], Avg: [-1050.281 -1050.281] (1.000)
Step: 11399, Reward: [-1735.042 -1735.042] [0.0000], Avg: [-1053.284 -1053.284] (1.000)
Step: 11449, Reward: [-1223.217 -1223.217] [0.0000], Avg: [-1054.026 -1054.026] (1.000)
Step: 11499, Reward: [-1529.375 -1529.375] [0.0000], Avg: [-1056.093 -1056.093] (1.000)
Step: 11549, Reward: [-1242.835 -1242.835] [0.0000], Avg: [-1056.901 -1056.901] (1.000)
Step: 11599, Reward: [-1280.084 -1280.084] [0.0000], Avg: [-1057.863 -1057.863] (1.000)
Step: 11649, Reward: [-1418.279 -1418.279] [0.0000], Avg: [-1059.41 -1059.41] (1.000)
Step: 11699, Reward: [-1562.657 -1562.657] [0.0000], Avg: [-1061.561 -1061.561] (1.000)
Step: 11749, Reward: [-1048.228 -1048.228] [0.0000], Avg: [-1061.504 -1061.504] (1.000)
Step: 11799, Reward: [-897.391 -897.391] [0.0000], Avg: [-1060.809 -1060.809] (1.000)
Step: 11849, Reward: [-1164.057 -1164.057] [0.0000], Avg: [-1061.244 -1061.244] (1.000)
Step: 11899, Reward: [-1398.918 -1398.918] [0.0000], Avg: [-1062.663 -1062.663] (1.000)
Step: 11949, Reward: [-895.283 -895.283] [0.0000], Avg: [-1061.963 -1061.963] (1.000)
Step: 11999, Reward: [-1189.855 -1189.855] [0.0000], Avg: [-1062.496 -1062.496] (1.000)
Step: 12049, Reward: [-427.796 -427.796] [0.0000], Avg: [-1059.862 -1059.862] (1.000)
Step: 12099, Reward: [-1628.727 -1628.727] [0.0000], Avg: [-1062.213 -1062.213] (1.000)
Step: 12149, Reward: [-1236.655 -1236.655] [0.0000], Avg: [-1062.931 -1062.931] (1.000)
Step: 12199, Reward: [-1258.396 -1258.396] [0.0000], Avg: [-1063.732 -1063.732] (1.000)
Step: 12249, Reward: [-858.602 -858.602] [0.0000], Avg: [-1062.894 -1062.894] (1.000)
Step: 12299, Reward: [-1621.974 -1621.974] [0.0000], Avg: [-1065.167 -1065.167] (1.000)
Step: 12349, Reward: [-1049.125 -1049.125] [0.0000], Avg: [-1065.102 -1065.102] (1.000)
Step: 12399, Reward: [-1105.768 -1105.768] [0.0000], Avg: [-1065.266 -1065.266] (1.000)
Step: 12449, Reward: [-899.014 -899.014] [0.0000], Avg: [-1064.599 -1064.599] (1.000)
Step: 12499, Reward: [-1281.88 -1281.88] [0.0000], Avg: [-1065.468 -1065.468] (1.000)
Step: 12549, Reward: [-1108.279 -1108.279] [0.0000], Avg: [-1065.638 -1065.638] (1.000)
Step: 12599, Reward: [-2269.147 -2269.147] [0.0000], Avg: [-1070.414 -1070.414] (1.000)
Step: 12649, Reward: [-1081.867 -1081.867] [0.0000], Avg: [-1070.459 -1070.459] (1.000)
Step: 12699, Reward: [-683.642 -683.642] [0.0000], Avg: [-1068.936 -1068.936] (1.000)
Step: 12749, Reward: [-869.952 -869.952] [0.0000], Avg: [-1068.156 -1068.156] (1.000)
Step: 12799, Reward: [-1344.105 -1344.105] [0.0000], Avg: [-1069.234 -1069.234] (1.000)
Step: 12849, Reward: [-1692.969 -1692.969] [0.0000], Avg: [-1071.661 -1071.661] (1.000)
Step: 12899, Reward: [-780.486 -780.486] [0.0000], Avg: [-1070.532 -1070.532] (1.000)
Step: 12949, Reward: [-814.579 -814.579] [0.0000], Avg: [-1069.544 -1069.544] (1.000)
Step: 12999, Reward: [-849.864 -849.864] [0.0000], Avg: [-1068.699 -1068.699] (1.000)
Step: 13049, Reward: [-1334.076 -1334.076] [0.0000], Avg: [-1069.716 -1069.716] (1.000)
Step: 13099, Reward: [-989.817 -989.817] [0.0000], Avg: [-1069.411 -1069.411] (1.000)
Step: 13149, Reward: [-1203.117 -1203.117] [0.0000], Avg: [-1069.919 -1069.919] (1.000)
Step: 13199, Reward: [-292.96 -292.96] [0.0000], Avg: [-1066.976 -1066.976] (1.000)
Step: 13249, Reward: [-1287.264 -1287.264] [0.0000], Avg: [-1067.808 -1067.808] (1.000)
Step: 13299, Reward: [-517.263 -517.263] [0.0000], Avg: [-1065.738 -1065.738] (1.000)
Step: 13349, Reward: [-584.643 -584.643] [0.0000], Avg: [-1063.936 -1063.936] (1.000)
Step: 13399, Reward: [-425.615 -425.615] [0.0000], Avg: [-1061.554 -1061.554] (1.000)
Step: 13449, Reward: [-1119.82 -1119.82] [0.0000], Avg: [-1061.771 -1061.771] (1.000)
Step: 13499, Reward: [-1288.121 -1288.121] [0.0000], Avg: [-1062.609 -1062.609] (1.000)
Step: 13549, Reward: [-1151.982 -1151.982] [0.0000], Avg: [-1062.939 -1062.939] (1.000)
Step: 13599, Reward: [-1416.12 -1416.12] [0.0000], Avg: [-1064.237 -1064.237] (1.000)
Step: 13649, Reward: [-1039.93 -1039.93] [0.0000], Avg: [-1064.148 -1064.148] (1.000)
Step: 13699, Reward: [-963.986 -963.986] [0.0000], Avg: [-1063.783 -1063.783] (1.000)
Step: 13749, Reward: [-2287.943 -2287.943] [0.0000], Avg: [-1068.234 -1068.234] (1.000)
Step: 13799, Reward: [-1920.883 -1920.883] [0.0000], Avg: [-1071.324 -1071.324] (1.000)
Step: 13849, Reward: [-1347.613 -1347.613] [0.0000], Avg: [-1072.321 -1072.321] (1.000)
Step: 13899, Reward: [-1512.834 -1512.834] [0.0000], Avg: [-1073.906 -1073.906] (1.000)
Step: 13949, Reward: [-999.961 -999.961] [0.0000], Avg: [-1073.641 -1073.641] (1.000)
Step: 13999, Reward: [-1432.725 -1432.725] [0.0000], Avg: [-1074.923 -1074.923] (1.000)
Step: 14049, Reward: [-914.158 -914.158] [0.0000], Avg: [-1074.351 -1074.351] (1.000)
Step: 14099, Reward: [-1113.818 -1113.818] [0.0000], Avg: [-1074.491 -1074.491] (1.000)
Step: 14149, Reward: [-565.652 -565.652] [0.0000], Avg: [-1072.693 -1072.693] (1.000)
Step: 14199, Reward: [-867.518 -867.518] [0.0000], Avg: [-1071.97 -1071.97] (1.000)
Step: 14249, Reward: [-1627.346 -1627.346] [0.0000], Avg: [-1073.919 -1073.919] (1.000)
Step: 14299, Reward: [-3135.51 -3135.51] [0.0000], Avg: [-1081.128 -1081.128] (1.000)
Step: 14349, Reward: [-3252.01 -3252.01] [0.0000], Avg: [-1088.692 -1088.692] (1.000)
Step: 14399, Reward: [-1474.155 -1474.155] [0.0000], Avg: [-1090.03 -1090.03] (1.000)
Step: 14449, Reward: [-1920.642 -1920.642] [0.0000], Avg: [-1092.904 -1092.904] (1.000)
Step: 14499, Reward: [-2113.907 -2113.907] [0.0000], Avg: [-1096.425 -1096.425] (1.000)
Step: 14549, Reward: [-925.297 -925.297] [0.0000], Avg: [-1095.837 -1095.837] (1.000)
Step: 14599, Reward: [-1122.638 -1122.638] [0.0000], Avg: [-1095.928 -1095.928] (1.000)
Step: 14649, Reward: [-1622.708 -1622.708] [0.0000], Avg: [-1097.726 -1097.726] (1.000)
Step: 14699, Reward: [-1441.165 -1441.165] [0.0000], Avg: [-1098.895 -1098.895] (1.000)
Step: 14749, Reward: [-1624.003 -1624.003] [0.0000], Avg: [-1100.675 -1100.675] (1.000)
Step: 14799, Reward: [-1513.859 -1513.859] [0.0000], Avg: [-1102.07 -1102.07] (1.000)
Step: 14849, Reward: [-1731.258 -1731.258] [0.0000], Avg: [-1104.189 -1104.189] (1.000)
Step: 14899, Reward: [-945.516 -945.516] [0.0000], Avg: [-1103.656 -1103.656] (1.000)
Step: 14949, Reward: [-1320.27 -1320.27] [0.0000], Avg: [-1104.381 -1104.381] (1.000)
Step: 14999, Reward: [-1135.492 -1135.492] [0.0000], Avg: [-1104.485 -1104.485] (1.000)
Step: 15049, Reward: [-767.042 -767.042] [0.0000], Avg: [-1103.364 -1103.364] (1.000)
Step: 15099, Reward: [-619.276 -619.276] [0.0000], Avg: [-1101.761 -1101.761] (1.000)
Step: 15149, Reward: [-753.371 -753.371] [0.0000], Avg: [-1100.611 -1100.611] (1.000)
Step: 15199, Reward: [-484.631 -484.631] [0.0000], Avg: [-1098.585 -1098.585] (1.000)
Step: 15249, Reward: [-484.157 -484.157] [0.0000], Avg: [-1096.57 -1096.57] (1.000)
Step: 15299, Reward: [-357.999 -357.999] [0.0000], Avg: [-1094.156 -1094.156] (1.000)
Step: 15349, Reward: [-1010.734 -1010.734] [0.0000], Avg: [-1093.885 -1093.885] (1.000)
Step: 15399, Reward: [-682.433 -682.433] [0.0000], Avg: [-1092.549 -1092.549] (1.000)
Step: 15449, Reward: [-702.626 -702.626] [0.0000], Avg: [-1091.287 -1091.287] (1.000)
Step: 15499, Reward: [-840.062 -840.062] [0.0000], Avg: [-1090.477 -1090.477] (1.000)
Step: 15549, Reward: [-700.408 -700.408] [0.0000], Avg: [-1089.222 -1089.222] (1.000)
Step: 15599, Reward: [-278.625 -278.625] [0.0000], Avg: [-1086.624 -1086.624] (1.000)
Step: 15649, Reward: [-573.272 -573.272] [0.0000], Avg: [-1084.984 -1084.984] (1.000)
Step: 15699, Reward: [-553.847 -553.847] [0.0000], Avg: [-1083.293 -1083.293] (1.000)
Step: 15749, Reward: [-431.175 -431.175] [0.0000], Avg: [-1081.222 -1081.222] (1.000)
Step: 15799, Reward: [-1597.773 -1597.773] [0.0000], Avg: [-1082.857 -1082.857] (1.000)
Step: 15849, Reward: [-692.728 -692.728] [0.0000], Avg: [-1081.626 -1081.626] (1.000)
Step: 15899, Reward: [-577.661 -577.661] [0.0000], Avg: [-1080.042 -1080.042] (1.000)
Step: 15949, Reward: [-818.655 -818.655] [0.0000], Avg: [-1079.222 -1079.222] (1.000)
Step: 15999, Reward: [-520.407 -520.407] [0.0000], Avg: [-1077.476 -1077.476] (1.000)
Step: 16049, Reward: [-730.955 -730.955] [0.0000], Avg: [-1076.396 -1076.396] (1.000)
Step: 16099, Reward: [-305.554 -305.554] [0.0000], Avg: [-1074.002 -1074.002] (1.000)
Step: 16149, Reward: [-342.625 -342.625] [0.0000], Avg: [-1071.738 -1071.738] (1.000)
Step: 16199, Reward: [-524.084 -524.084] [0.0000], Avg: [-1070.048 -1070.048] (1.000)
Step: 16249, Reward: [-461.093 -461.093] [0.0000], Avg: [-1068.174 -1068.174] (1.000)
Step: 16299, Reward: [-346.882 -346.882] [0.0000], Avg: [-1065.962 -1065.962] (1.000)
Step: 16349, Reward: [-588.17 -588.17] [0.0000], Avg: [-1064.5 -1064.5] (1.000)
Step: 16399, Reward: [-950.187 -950.187] [0.0000], Avg: [-1064.152 -1064.152] (1.000)
Step: 16449, Reward: [-874.318 -874.318] [0.0000], Avg: [-1063.575 -1063.575] (1.000)
Step: 16499, Reward: [-773.79 -773.79] [0.0000], Avg: [-1062.697 -1062.697] (1.000)
Step: 16549, Reward: [-970.175 -970.175] [0.0000], Avg: [-1062.417 -1062.417] (1.000)
Step: 16599, Reward: [-1208.744 -1208.744] [0.0000], Avg: [-1062.858 -1062.858] (1.000)
Step: 16649, Reward: [-575.359 -575.359] [0.0000], Avg: [-1061.394 -1061.394] (1.000)
Step: 16699, Reward: [-2240.564 -2240.564] [0.0000], Avg: [-1064.924 -1064.924] (1.000)
Step: 16749, Reward: [-1177.32 -1177.32] [0.0000], Avg: [-1065.26 -1065.26] (1.000)
Step: 16799, Reward: [-767.344 -767.344] [0.0000], Avg: [-1064.373 -1064.373] (1.000)
Step: 16849, Reward: [-1494.742 -1494.742] [0.0000], Avg: [-1065.65 -1065.65] (1.000)
Step: 16899, Reward: [-1670.934 -1670.934] [0.0000], Avg: [-1067.441 -1067.441] (1.000)
Step: 16949, Reward: [-1961.853 -1961.853] [0.0000], Avg: [-1070.08 -1070.08] (1.000)
Step: 16999, Reward: [-1737.692 -1737.692] [0.0000], Avg: [-1072.043 -1072.043] (1.000)
Step: 17049, Reward: [-996.125 -996.125] [0.0000], Avg: [-1071.82 -1071.82] (1.000)
Step: 17099, Reward: [-2610.39 -2610.39] [0.0000], Avg: [-1076.319 -1076.319] (1.000)
Step: 17149, Reward: [-1369.797 -1369.797] [0.0000], Avg: [-1077.175 -1077.175] (1.000)
Step: 17199, Reward: [-1555.336 -1555.336] [0.0000], Avg: [-1078.565 -1078.565] (1.000)
Step: 17249, Reward: [-1609.968 -1609.968] [0.0000], Avg: [-1080.105 -1080.105] (1.000)
Step: 17299, Reward: [-1783.618 -1783.618] [0.0000], Avg: [-1082.138 -1082.138] (1.000)
Step: 17349, Reward: [-1388.386 -1388.386] [0.0000], Avg: [-1083.021 -1083.021] (1.000)
Step: 17399, Reward: [-1426.494 -1426.494] [0.0000], Avg: [-1084.008 -1084.008] (1.000)
Step: 17449, Reward: [-3271.914 -3271.914] [0.0000], Avg: [-1090.277 -1090.277] (1.000)
Step: 17499, Reward: [-1989.269 -1989.269] [0.0000], Avg: [-1092.846 -1092.846] (1.000)
Step: 17549, Reward: [-1158.441 -1158.441] [0.0000], Avg: [-1093.032 -1093.032] (1.000)
Step: 17599, Reward: [-1874.828 -1874.828] [0.0000], Avg: [-1095.253 -1095.253] (1.000)
Step: 17649, Reward: [-1532.774 -1532.774] [0.0000], Avg: [-1096.493 -1096.493] (1.000)
Step: 17699, Reward: [-1284.68 -1284.68] [0.0000], Avg: [-1097.025 -1097.025] (1.000)
Step: 17749, Reward: [-1517.911 -1517.911] [0.0000], Avg: [-1098.21 -1098.21] (1.000)
Step: 17799, Reward: [-691.995 -691.995] [0.0000], Avg: [-1097.069 -1097.069] (1.000)
Step: 17849, Reward: [-2373.437 -2373.437] [0.0000], Avg: [-1100.644 -1100.644] (1.000)
Step: 17899, Reward: [-2040.672 -2040.672] [0.0000], Avg: [-1103.27 -1103.27] (1.000)
Step: 17949, Reward: [-1856.146 -1856.146] [0.0000], Avg: [-1105.367 -1105.367] (1.000)
Step: 17999, Reward: [-1624.09 -1624.09] [0.0000], Avg: [-1106.808 -1106.808] (1.000)
Step: 18049, Reward: [-927.946 -927.946] [0.0000], Avg: [-1106.313 -1106.313] (1.000)
Step: 18099, Reward: [-1199.689 -1199.689] [0.0000], Avg: [-1106.571 -1106.571] (1.000)
Step: 18149, Reward: [-919.084 -919.084] [0.0000], Avg: [-1106.054 -1106.054] (1.000)
Step: 18199, Reward: [-1625.842 -1625.842] [0.0000], Avg: [-1107.482 -1107.482] (1.000)
Step: 18249, Reward: [-860.933 -860.933] [0.0000], Avg: [-1106.807 -1106.807] (1.000)
Step: 18299, Reward: [-606.515 -606.515] [0.0000], Avg: [-1105.44 -1105.44] (1.000)
Step: 18349, Reward: [-843.856 -843.856] [0.0000], Avg: [-1104.727 -1104.727] (1.000)
Step: 18399, Reward: [-851.457 -851.457] [0.0000], Avg: [-1104.039 -1104.039] (1.000)
Step: 18449, Reward: [-1061.878 -1061.878] [0.0000], Avg: [-1103.924 -1103.924] (1.000)
Step: 18499, Reward: [-1117.586 -1117.586] [0.0000], Avg: [-1103.961 -1103.961] (1.000)
Step: 18549, Reward: [-399.576 -399.576] [0.0000], Avg: [-1102.063 -1102.063] (1.000)
Step: 18599, Reward: [-1247.261 -1247.261] [0.0000], Avg: [-1102.453 -1102.453] (1.000)
Step: 18649, Reward: [-891.248 -891.248] [0.0000], Avg: [-1101.887 -1101.887] (1.000)
Step: 18699, Reward: [-453.204 -453.204] [0.0000], Avg: [-1100.152 -1100.152] (1.000)
Step: 18749, Reward: [-756.783 -756.783] [0.0000], Avg: [-1099.237 -1099.237] (1.000)
Step: 18799, Reward: [-505.887 -505.887] [0.0000], Avg: [-1097.659 -1097.659] (1.000)
Step: 18849, Reward: [-532.479 -532.479] [0.0000], Avg: [-1096.16 -1096.16] (1.000)
Step: 18899, Reward: [-659.016 -659.016] [0.0000], Avg: [-1095.003 -1095.003] (1.000)
Step: 18949, Reward: [-667.09 -667.09] [0.0000], Avg: [-1093.874 -1093.874] (1.000)
Step: 18999, Reward: [-676.364 -676.364] [0.0000], Avg: [-1092.775 -1092.775] (1.000)
Step: 19049, Reward: [-649.437 -649.437] [0.0000], Avg: [-1091.612 -1091.612] (1.000)
Step: 19099, Reward: [-746.251 -746.251] [0.0000], Avg: [-1090.708 -1090.708] (1.000)
Step: 19149, Reward: [-680.429 -680.429] [0.0000], Avg: [-1089.636 -1089.636] (1.000)
Step: 19199, Reward: [-797.985 -797.985] [0.0000], Avg: [-1088.877 -1088.877] (1.000)
Step: 19249, Reward: [-1036.896 -1036.896] [0.0000], Avg: [-1088.742 -1088.742] (1.000)
Step: 19299, Reward: [-609.987 -609.987] [0.0000], Avg: [-1087.502 -1087.502] (1.000)
Step: 19349, Reward: [-1248.469 -1248.469] [0.0000], Avg: [-1087.918 -1087.918] (1.000)
Step: 19399, Reward: [-958.931 -958.931] [0.0000], Avg: [-1087.585 -1087.585] (1.000)
Step: 19449, Reward: [-1390.978 -1390.978] [0.0000], Avg: [-1088.365 -1088.365] (1.000)
Step: 19499, Reward: [-502.731 -502.731] [0.0000], Avg: [-1086.863 -1086.863] (1.000)
Step: 19549, Reward: [-852.607 -852.607] [0.0000], Avg: [-1086.264 -1086.264] (1.000)
Step: 19599, Reward: [-968.858 -968.858] [0.0000], Avg: [-1085.965 -1085.965] (1.000)
Step: 19649, Reward: [-1387.843 -1387.843] [0.0000], Avg: [-1086.733 -1086.733] (1.000)
Step: 19699, Reward: [-916.871 -916.871] [0.0000], Avg: [-1086.302 -1086.302] (1.000)
Step: 19749, Reward: [-1101.833 -1101.833] [0.0000], Avg: [-1086.341 -1086.341] (1.000)
Step: 19799, Reward: [-740.869 -740.869] [0.0000], Avg: [-1085.469 -1085.469] (1.000)
Step: 19849, Reward: [-1143.206 -1143.206] [0.0000], Avg: [-1085.614 -1085.614] (1.000)
Step: 19899, Reward: [-1648.887 -1648.887] [0.0000], Avg: [-1087.029 -1087.029] (1.000)
Step: 19949, Reward: [-1827.996 -1827.996] [0.0000], Avg: [-1088.886 -1088.886] (1.000)
Step: 19999, Reward: [-742.298 -742.298] [0.0000], Avg: [-1088.02 -1088.02] (1.000)
Step: 20049, Reward: [-979.015 -979.015] [0.0000], Avg: [-1087.748 -1087.748] (1.000)
Step: 20099, Reward: [-1192.164 -1192.164] [0.0000], Avg: [-1088.008 -1088.008] (1.000)
Step: 20149, Reward: [-977.402 -977.402] [0.0000], Avg: [-1087.733 -1087.733] (1.000)
Step: 20199, Reward: [-1016.272 -1016.272] [0.0000], Avg: [-1087.557 -1087.557] (1.000)
Step: 20249, Reward: [-777.703 -777.703] [0.0000], Avg: [-1086.791 -1086.791] (1.000)
Step: 20299, Reward: [-765.519 -765.519] [0.0000], Avg: [-1086. -1086.] (1.000)
Step: 20349, Reward: [-835.189 -835.189] [0.0000], Avg: [-1085.384 -1085.384] (1.000)
Step: 20399, Reward: [-1044.767 -1044.767] [0.0000], Avg: [-1085.284 -1085.284] (1.000)
Step: 20449, Reward: [-858.872 -858.872] [0.0000], Avg: [-1084.731 -1084.731] (1.000)
Step: 20499, Reward: [-1064.078 -1064.078] [0.0000], Avg: [-1084.68 -1084.68] (1.000)
Step: 20549, Reward: [-670.474 -670.474] [0.0000], Avg: [-1083.673 -1083.673] (1.000)
Step: 20599, Reward: [-1038.286 -1038.286] [0.0000], Avg: [-1083.562 -1083.562] (1.000)
Step: 20649, Reward: [-1338.436 -1338.436] [0.0000], Avg: [-1084.18 -1084.18] (1.000)
