Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_speaker_listener
num_envs: 16, state_size: [(1, 3), (1, 11)], action_size: [[1, 3], [1, 5]], action_space: [<gym.spaces.multi_discrete.MultiDiscrete object at 0x7fcba16f7b00>, <gym.spaces.multi_discrete.MultiDiscrete object at 0x7fcba16f7be0>],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACAgent, LEARN_RATE, NUM_STEPS, EPS_MIN, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, gumbel_softmax, one_hot

REPLAY_BATCH_SIZE = 8
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
LEARN_RATE = 0.01
TARGET_UPDATE_RATE = 0.01

class MADDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		# self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state)
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		action_mu = self.action_mu(state)
		# action_sig = self.action_sig(state).exp()
		# epsilon = torch.randn_like(action_sig)
		# action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		action = action_mu.view(*out_dims, -1)
		return gumbel_softmax(action, hard=False)
	
class MADDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.norm = torch.nn.BatchNorm1d(state_size[-1]+action_size[-1])
		self.norm.weight.data.fill_(1)
		self.norm.bias.data.fill_(0)
		self.layer1 = torch.nn.Linear(state_size[-1]+action_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = torch.cat([state, action], -1)
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		state = self.norm(state)
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_value = self.q_value(state)
		q_value = q_value.view(*out_dims, -1)
		return q_value

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=None):
		super().__init__(tau=tau, gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.critic = MADDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		self.models = [DDPGNetwork(s_size, a_size, MADDPGActor, lambda s,a: self.critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.get_action(s, use_target, grad, numpy, sample) for s,model in zip(state, self.models)]
			return action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_value = [model.get_q_value(state, action, use_target, grad, numpy) for model in self.models]
			return q_value

	def optimize(self, states, actions, states_joint, actions_joint, q_targets, e_weight=ENTROPY_WEIGHT):
		for (i,model),state,q_target in zip(enumerate(self.models), states, q_targets):
			q_values = model.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:q_target.size(0)] - q_target.detach()
			critic_loss = critic_error.pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), param_norm=model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			actor_action = model.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else one_hot(action) for j,action in enumerate(actions)]
			action_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(critic_action, self.action_size)], dim=-1)
			q_actions = model.critic_local(states_joint, action_joint)
			actor_loss = -(q_actions - q_values.detach()) + e_weight*actor_action.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean(), param_norm=model.actor_local.parameters())
			model.soft_copy(model.actor_local, model.actor_target)

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("maddpg", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state), sample=sample, numpy=numpy)
		action = [(1-eps)*a_greedy + eps*a_random for a_greedy,a_random in zip(action_greedy, action_random)]
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_joint = torch.cat([a.view(*a.size()[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions, self.action_size)], dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True)
			q_targets = [self.compute_gae(q_value[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			
			to_stack = lambda items: list(zip(*[x.view(-1, *x.shape[2:]).cpu().numpy() for x in items]))
			states, actions, states_joint, actions_joint = map(lambda items: [x[:-1] for x in items], [states, actions, [states_joint], [actions_joint]])
			states, actions, states_joint, actions_joint, q_targets = map(to_stack, [states, actions, states_joint, actions_joint, q_targets])
			self.replay_buffer.extend(list(zip(states, actions, states_joint, actions_joint, q_targets)), shuffle=False)	
		if len(self.replay_buffer) > 0:
			states, actions, states_joint, actions_joint, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)[0]
			self.network.optimize(states, actions, states_joint[0], actions_joint[0], q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-4]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False, env_name=env_name)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-988.635 -988.635] [0.0000], Avg: [-988.635 -988.635] (0.995)
Step: 99, Reward: [-455.043 -455.043] [0.0000], Avg: [-721.839 -721.839] (0.990)
Step: 149, Reward: [-686.967 -686.967] [0.0000], Avg: [-710.215 -710.215] (0.985)
Step: 199, Reward: [-859.941 -859.941] [0.0000], Avg: [-747.646 -747.646] (0.980)
Step: 249, Reward: [-846.462 -846.462] [0.0000], Avg: [-767.409 -767.409] (0.975)
Step: 299, Reward: [-332.028 -332.028] [0.0000], Avg: [-694.846 -694.846] (0.970)
Step: 349, Reward: [-290.839 -290.839] [0.0000], Avg: [-637.131 -637.131] (0.966)
Step: 399, Reward: [-505.918 -505.918] [0.0000], Avg: [-620.729 -620.729] (0.961)
Step: 449, Reward: [-225.31 -225.31] [0.0000], Avg: [-576.794 -576.794] (0.956)
Step: 499, Reward: [-2374.098 -2374.098] [0.0000], Avg: [-756.524 -756.524] (0.951)
Step: 549, Reward: [-1209.498 -1209.498] [0.0000], Avg: [-797.703 -797.703] (0.946)
Step: 599, Reward: [-4055.463 -4055.463] [0.0000], Avg: [-1069.183 -1069.183] (0.942)
Step: 649, Reward: [-675.432 -675.432] [0.0000], Avg: [-1038.895 -1038.895] (0.937)
Step: 699, Reward: [-802.63 -802.63] [0.0000], Avg: [-1022.019 -1022.019] (0.932)
Step: 749, Reward: [-491.981 -491.981] [0.0000], Avg: [-986.683 -986.683] (0.928)
Step: 799, Reward: [-544.345 -544.345] [0.0000], Avg: [-959.037 -959.037] (0.923)
Step: 849, Reward: [-1701.669 -1701.669] [0.0000], Avg: [-1002.721 -1002.721] (0.918)
Step: 899, Reward: [-1928.827 -1928.827] [0.0000], Avg: [-1054.171 -1054.171] (0.914)
Step: 949, Reward: [-2064.72 -2064.72] [0.0000], Avg: [-1107.358 -1107.358] (0.909)
Step: 999, Reward: [-651.878 -651.878] [0.0000], Avg: [-1084.584 -1084.584] (0.905)
Step: 1049, Reward: [-274.649 -274.649] [0.0000], Avg: [-1046.016 -1046.016] (0.900)
Step: 1099, Reward: [-218.464 -218.464] [0.0000], Avg: [-1008.4 -1008.4] (0.896)
Step: 1149, Reward: [-1206.7 -1206.7] [0.0000], Avg: [-1017.022 -1017.022] (0.891)
Step: 1199, Reward: [-2096.686 -2096.686] [0.0000], Avg: [-1062.008 -1062.008] (0.887)
Step: 1249, Reward: [-1202.289 -1202.289] [0.0000], Avg: [-1067.619 -1067.619] (0.882)
Step: 1299, Reward: [-32.624 -32.624] [0.0000], Avg: [-1027.811 -1027.811] (0.878)
Step: 1349, Reward: [-45.1 -45.1] [0.0000], Avg: [-991.415 -991.415] (0.873)
Step: 1399, Reward: [-1427.132 -1427.132] [0.0000], Avg: [-1006.976 -1006.976] (0.869)
Step: 1449, Reward: [-1219.513 -1219.513] [0.0000], Avg: [-1014.305 -1014.305] (0.865)
Step: 1499, Reward: [-543.48 -543.48] [0.0000], Avg: [-998.611 -998.611] (0.860)
Step: 1549, Reward: [-267.552 -267.552] [0.0000], Avg: [-975.028 -975.028] (0.856)
Step: 1599, Reward: [-83.453 -83.453] [0.0000], Avg: [-947.166 -947.166] (0.852)
Step: 1649, Reward: [-1004.829 -1004.829] [0.0000], Avg: [-948.914 -948.914] (0.848)
Step: 1699, Reward: [-2112.506 -2112.506] [0.0000], Avg: [-983.137 -983.137] (0.843)
Step: 1749, Reward: [-335.491 -335.491] [0.0000], Avg: [-964.633 -964.633] (0.839)
Step: 1799, Reward: [-721.287 -721.287] [0.0000], Avg: [-957.873 -957.873] (0.835)
Step: 1849, Reward: [-524.088 -524.088] [0.0000], Avg: [-946.149 -946.149] (0.831)
Step: 1899, Reward: [-1130.866 -1130.866] [0.0000], Avg: [-951.01 -951.01] (0.827)
Step: 1949, Reward: [-2106.595 -2106.595] [0.0000], Avg: [-980.641 -980.641] (0.822)
Step: 1999, Reward: [-3591.829 -3591.829] [0.0000], Avg: [-1045.92 -1045.92] (0.818)
Step: 2049, Reward: [-2830.201 -2830.201] [0.0000], Avg: [-1089.439 -1089.439] (0.814)
Step: 2099, Reward: [-1569.327 -1569.327] [0.0000], Avg: [-1100.865 -1100.865] (0.810)
Step: 2149, Reward: [-1627.998 -1627.998] [0.0000], Avg: [-1113.124 -1113.124] (0.806)
Step: 2199, Reward: [-1888.306 -1888.306] [0.0000], Avg: [-1130.742 -1130.742] (0.802)
Step: 2249, Reward: [-1707.061 -1707.061] [0.0000], Avg: [-1143.549 -1143.549] (0.798)
Step: 2299, Reward: [-1379.491 -1379.491] [0.0000], Avg: [-1148.678 -1148.678] (0.794)
Step: 2349, Reward: [-1098.264 -1098.264] [0.0000], Avg: [-1147.606 -1147.606] (0.790)
Step: 2399, Reward: [-376.648 -376.648] [0.0000], Avg: [-1131.544 -1131.544] (0.786)
Step: 2449, Reward: [-1255.689 -1255.689] [0.0000], Avg: [-1134.078 -1134.078] (0.782)
Step: 2499, Reward: [-4721.79 -4721.79] [0.0000], Avg: [-1205.832 -1205.832] (0.778)
Step: 2549, Reward: [-934.587 -934.587] [0.0000], Avg: [-1200.513 -1200.513] (0.774)
Step: 2599, Reward: [-839.034 -839.034] [0.0000], Avg: [-1193.562 -1193.562] (0.771)
Step: 2649, Reward: [-1663.528 -1663.528] [0.0000], Avg: [-1202.429 -1202.429] (0.767)
Step: 2699, Reward: [-1016.549 -1016.549] [0.0000], Avg: [-1198.987 -1198.987] (0.763)
Step: 2749, Reward: [-1259.617 -1259.617] [0.0000], Avg: [-1200.089 -1200.089] (0.759)
Step: 2799, Reward: [-680.86 -680.86] [0.0000], Avg: [-1190.817 -1190.817] (0.755)
Step: 2849, Reward: [-387.994 -387.994] [0.0000], Avg: [-1176.733 -1176.733] (0.751)
Step: 2899, Reward: [-1223.072 -1223.072] [0.0000], Avg: [-1177.532 -1177.532] (0.748)
Step: 2949, Reward: [-227.257 -227.257] [0.0000], Avg: [-1161.425 -1161.425] (0.744)
Step: 2999, Reward: [-753.647 -753.647] [0.0000], Avg: [-1154.629 -1154.629] (0.740)
Step: 3049, Reward: [-2830.692 -2830.692] [0.0000], Avg: [-1182.105 -1182.105] (0.737)
Step: 3099, Reward: [-1985.49 -1985.49] [0.0000], Avg: [-1195.063 -1195.063] (0.733)
Step: 3149, Reward: [-2227.311 -2227.311] [0.0000], Avg: [-1211.448 -1211.448] (0.729)
Step: 3199, Reward: [-1114.341 -1114.341] [0.0000], Avg: [-1209.931 -1209.931] (0.726)
Step: 3249, Reward: [-885.751 -885.751] [0.0000], Avg: [-1204.943 -1204.943] (0.722)
Step: 3299, Reward: [-1053.355 -1053.355] [0.0000], Avg: [-1202.647 -1202.647] (0.718)
Step: 3349, Reward: [-1442.655 -1442.655] [0.0000], Avg: [-1206.229 -1206.229] (0.715)
Step: 3399, Reward: [-678.169 -678.169] [0.0000], Avg: [-1198.463 -1198.463] (0.711)
Step: 3449, Reward: [-1279.162 -1279.162] [0.0000], Avg: [-1199.633 -1199.633] (0.708)
Step: 3499, Reward: [-694.722 -694.722] [0.0000], Avg: [-1192.42 -1192.42] (0.704)
Step: 3549, Reward: [-643.538 -643.538] [0.0000], Avg: [-1184.689 -1184.689] (0.701)
Step: 3599, Reward: [-1033.471 -1033.471] [0.0000], Avg: [-1182.589 -1182.589] (0.697)
Step: 3649, Reward: [-802.112 -802.112] [0.0000], Avg: [-1177.377 -1177.377] (0.694)
Step: 3699, Reward: [-3941. -3941.] [0.0000], Avg: [-1214.723 -1214.723] (0.690)
Step: 3749, Reward: [-2765.277 -2765.277] [0.0000], Avg: [-1235.397 -1235.397] (0.687)
Step: 3799, Reward: [-1610.461 -1610.461] [0.0000], Avg: [-1240.332 -1240.332] (0.683)
Step: 3849, Reward: [-1400.677 -1400.677] [0.0000], Avg: [-1242.415 -1242.415] (0.680)
Step: 3899, Reward: [-1792.425 -1792.425] [0.0000], Avg: [-1249.466 -1249.466] (0.676)
Step: 3949, Reward: [-2089.134 -2089.134] [0.0000], Avg: [-1260.095 -1260.095] (0.673)
Step: 3999, Reward: [-268.108 -268.108] [0.0000], Avg: [-1247.695 -1247.695] (0.670)
Step: 4049, Reward: [-1353.381 -1353.381] [0.0000], Avg: [-1249. -1249.] (0.666)
Step: 4099, Reward: [-801.44 -801.44] [0.0000], Avg: [-1243.542 -1243.542] (0.663)
Step: 4149, Reward: [-702.227 -702.227] [0.0000], Avg: [-1237.02 -1237.02] (0.660)
Step: 4199, Reward: [-343.096 -343.096] [0.0000], Avg: [-1226.378 -1226.378] (0.656)
Step: 4249, Reward: [-843.196 -843.196] [0.0000], Avg: [-1221.87 -1221.87] (0.653)
Step: 4299, Reward: [-2932.453 -2932.453] [0.0000], Avg: [-1241.76 -1241.76] (0.650)
Step: 4349, Reward: [-2452.151 -2452.151] [0.0000], Avg: [-1255.673 -1255.673] (0.647)
Step: 4399, Reward: [-471.39 -471.39] [0.0000], Avg: [-1246.76 -1246.76] (0.643)
Step: 4449, Reward: [-1155.891 -1155.891] [0.0000], Avg: [-1245.739 -1245.739] (0.640)
Step: 4499, Reward: [-1561.828 -1561.828] [0.0000], Avg: [-1249.252 -1249.252] (0.637)
Step: 4549, Reward: [-2488.216 -2488.216] [0.0000], Avg: [-1262.867 -1262.867] (0.634)
Step: 4599, Reward: [-157.276 -157.276] [0.0000], Avg: [-1250.849 -1250.849] (0.631)
Step: 4649, Reward: [-455.778 -455.778] [0.0000], Avg: [-1242.3 -1242.3] (0.627)
Step: 4699, Reward: [-442.967 -442.967] [0.0000], Avg: [-1233.797 -1233.797] (0.624)
Step: 4749, Reward: [-1030.18 -1030.18] [0.0000], Avg: [-1231.653 -1231.653] (0.621)
Step: 4799, Reward: [-227.476 -227.476] [0.0000], Avg: [-1221.193 -1221.193] (0.618)
Step: 4849, Reward: [-953.661 -953.661] [0.0000], Avg: [-1218.435 -1218.435] (0.615)
Step: 4899, Reward: [-636.153 -636.153] [0.0000], Avg: [-1212.493 -1212.493] (0.612)
Step: 4949, Reward: [-414.038 -414.038] [0.0000], Avg: [-1204.428 -1204.428] (0.609)
Step: 4999, Reward: [-1450.102 -1450.102] [0.0000], Avg: [-1206.885 -1206.885] (0.606)
Step: 5049, Reward: [-1451.289 -1451.289] [0.0000], Avg: [-1209.305 -1209.305] (0.603)
Step: 5099, Reward: [-3044.753 -3044.753] [0.0000], Avg: [-1227.299 -1227.299] (0.600)
Step: 5149, Reward: [-857.796 -857.796] [0.0000], Avg: [-1223.712 -1223.712] (0.597)
Step: 5199, Reward: [-2245.049 -2245.049] [0.0000], Avg: [-1233.532 -1233.532] (0.594)
Step: 5249, Reward: [-2011.972 -2011.972] [0.0000], Avg: [-1240.946 -1240.946] (0.591)
Step: 5299, Reward: [-3172.849 -3172.849] [0.0000], Avg: [-1259.172 -1259.172] (0.588)
Step: 5349, Reward: [-2512.594 -2512.594] [0.0000], Avg: [-1270.886 -1270.886] (0.585)
Step: 5399, Reward: [-172.821 -172.821] [0.0000], Avg: [-1260.719 -1260.719] (0.582)
Step: 5449, Reward: [-109.567 -109.567] [0.0000], Avg: [-1250.158 -1250.158] (0.579)
Step: 5499, Reward: [-2122.574 -2122.574] [0.0000], Avg: [-1258.089 -1258.089] (0.576)
Step: 5549, Reward: [-3223.007 -3223.007] [0.0000], Avg: [-1275.791 -1275.791] (0.573)
Step: 5599, Reward: [-1389.501 -1389.501] [0.0000], Avg: [-1276.806 -1276.806] (0.570)
Step: 5649, Reward: [-2305.498 -2305.498] [0.0000], Avg: [-1285.909 -1285.909] (0.568)
Step: 5699, Reward: [-923.34 -923.34] [0.0000], Avg: [-1282.729 -1282.729] (0.565)
Step: 5749, Reward: [-256.819 -256.819] [0.0000], Avg: [-1273.808 -1273.808] (0.562)
Step: 5799, Reward: [-2131.922 -2131.922] [0.0000], Avg: [-1281.206 -1281.206] (0.559)
Step: 5849, Reward: [-510.762 -510.762] [0.0000], Avg: [-1274.621 -1274.621] (0.556)
Step: 5899, Reward: [-924.521 -924.521] [0.0000], Avg: [-1271.654 -1271.654] (0.554)
Step: 5949, Reward: [-738.177 -738.177] [0.0000], Avg: [-1267.171 -1267.171] (0.551)
Step: 5999, Reward: [-257.218 -257.218] [0.0000], Avg: [-1258.754 -1258.754] (0.548)
Step: 6049, Reward: [-3090.212 -3090.212] [0.0000], Avg: [-1273.89 -1273.89] (0.545)
Step: 6099, Reward: [-185. -185.] [0.0000], Avg: [-1264.965 -1264.965] (0.543)
Step: 6149, Reward: [-2634.145 -2634.145] [0.0000], Avg: [-1276.097 -1276.097] (0.540)
Step: 6199, Reward: [-777.691 -777.691] [0.0000], Avg: [-1272.077 -1272.077] (0.537)
Step: 6249, Reward: [-381.219 -381.219] [0.0000], Avg: [-1264.95 -1264.95] (0.534)
Step: 6299, Reward: [-390.875 -390.875] [0.0000], Avg: [-1258.013 -1258.013] (0.532)
Step: 6349, Reward: [-424.882 -424.882] [0.0000], Avg: [-1251.453 -1251.453] (0.529)
Step: 6399, Reward: [-866.756 -866.756] [0.0000], Avg: [-1248.448 -1248.448] (0.526)
Step: 6449, Reward: [-133.466 -133.466] [0.0000], Avg: [-1239.804 -1239.804] (0.524)
Step: 6499, Reward: [-648.217 -648.217] [0.0000], Avg: [-1235.254 -1235.254] (0.521)
Step: 6549, Reward: [-258.994 -258.994] [0.0000], Avg: [-1227.801 -1227.801] (0.519)
Step: 6599, Reward: [-461.836 -461.836] [0.0000], Avg: [-1221.999 -1221.999] (0.516)
Step: 6649, Reward: [-444.853 -444.853] [0.0000], Avg: [-1216.155 -1216.155] (0.513)
Step: 6699, Reward: [-111.47 -111.47] [0.0000], Avg: [-1207.911 -1207.911] (0.511)
Step: 6749, Reward: [-1273.969 -1273.969] [0.0000], Avg: [-1208.401 -1208.401] (0.508)
Step: 6799, Reward: [-335.576 -335.576] [0.0000], Avg: [-1201.983 -1201.983] (0.506)
Step: 6849, Reward: [-706.828 -706.828] [0.0000], Avg: [-1198.369 -1198.369] (0.503)
Step: 6899, Reward: [-656.293 -656.293] [0.0000], Avg: [-1194.441 -1194.441] (0.501)
Step: 6949, Reward: [-759.481 -759.481] [0.0000], Avg: [-1191.311 -1191.311] (0.498)
Step: 6999, Reward: [-942.371 -942.371] [0.0000], Avg: [-1189.533 -1189.533] (0.496)
Step: 7049, Reward: [-1868.761 -1868.761] [0.0000], Avg: [-1194.35 -1194.35] (0.493)
Step: 7099, Reward: [-403.824 -403.824] [0.0000], Avg: [-1188.783 -1188.783] (0.491)
Step: 7149, Reward: [-828.42 -828.42] [0.0000], Avg: [-1186.263 -1186.263] (0.488)
Step: 7199, Reward: [-443.534 -443.534] [0.0000], Avg: [-1181.105 -1181.105] (0.486)
Step: 7249, Reward: [-445.346 -445.346] [0.0000], Avg: [-1176.031 -1176.031] (0.483)
Step: 7299, Reward: [-2990.352 -2990.352] [0.0000], Avg: [-1188.458 -1188.458] (0.481)
Step: 7349, Reward: [-136.014 -136.014] [0.0000], Avg: [-1181.299 -1181.299] (0.479)
Step: 7399, Reward: [-366.254 -366.254] [0.0000], Avg: [-1175.792 -1175.792] (0.476)
Step: 7449, Reward: [-100.288 -100.288] [0.0000], Avg: [-1168.573 -1168.573] (0.474)
Step: 7499, Reward: [-620.665 -620.665] [0.0000], Avg: [-1164.921 -1164.921] (0.471)
Step: 7549, Reward: [-171.304 -171.304] [0.0000], Avg: [-1158.34 -1158.34] (0.469)
Step: 7599, Reward: [-396.418 -396.418] [0.0000], Avg: [-1153.328 -1153.328] (0.467)
Step: 7649, Reward: [-1577.561 -1577.561] [0.0000], Avg: [-1156.101 -1156.101] (0.464)
Step: 7699, Reward: [-547.129 -547.129] [0.0000], Avg: [-1152.146 -1152.146] (0.462)
Step: 7749, Reward: [-1045.888 -1045.888] [0.0000], Avg: [-1151.461 -1151.461] (0.460)
Step: 7799, Reward: [-106.542 -106.542] [0.0000], Avg: [-1144.762 -1144.762] (0.458)
Step: 7849, Reward: [-112.189 -112.189] [0.0000], Avg: [-1138.186 -1138.186] (0.455)
Step: 7899, Reward: [-180.964 -180.964] [0.0000], Avg: [-1132.127 -1132.127] (0.453)
Step: 7949, Reward: [-240.678 -240.678] [0.0000], Avg: [-1126.521 -1126.521] (0.451)
Step: 7999, Reward: [-250.251 -250.251] [0.0000], Avg: [-1121.044 -1121.044] (0.448)
Step: 8049, Reward: [-539.45 -539.45] [0.0000], Avg: [-1117.432 -1117.432] (0.446)
Step: 8099, Reward: [-410.472 -410.472] [0.0000], Avg: [-1113.068 -1113.068] (0.444)
Step: 8149, Reward: [-599.481 -599.481] [0.0000], Avg: [-1109.917 -1109.917] (0.442)
Step: 8199, Reward: [-78.994 -78.994] [0.0000], Avg: [-1103.631 -1103.631] (0.440)
Step: 8249, Reward: [-136.56 -136.56] [0.0000], Avg: [-1097.77 -1097.77] (0.437)
Step: 8299, Reward: [-332.641 -332.641] [0.0000], Avg: [-1093.16 -1093.16] (0.435)
Step: 8349, Reward: [-1007.665 -1007.665] [0.0000], Avg: [-1092.648 -1092.648] (0.433)
Step: 8399, Reward: [-571.56 -571.56] [0.0000], Avg: [-1089.547 -1089.547] (0.431)
Step: 8449, Reward: [-599.794 -599.794] [0.0000], Avg: [-1086.649 -1086.649] (0.429)
Step: 8499, Reward: [-237.75 -237.75] [0.0000], Avg: [-1081.655 -1081.655] (0.427)
Step: 8549, Reward: [-482.291 -482.291] [0.0000], Avg: [-1078.15 -1078.15] (0.424)
Step: 8599, Reward: [-265.49 -265.49] [0.0000], Avg: [-1073.425 -1073.425] (0.422)
Step: 8649, Reward: [-240.994 -240.994] [0.0000], Avg: [-1068.614 -1068.614] (0.420)
Step: 8699, Reward: [-602.703 -602.703] [0.0000], Avg: [-1065.936 -1065.936] (0.418)
Step: 8749, Reward: [-398.002 -398.002] [0.0000], Avg: [-1062.119 -1062.119] (0.416)
Step: 8799, Reward: [-234.718 -234.718] [0.0000], Avg: [-1057.418 -1057.418] (0.414)
Step: 8849, Reward: [-372.714 -372.714] [0.0000], Avg: [-1053.55 -1053.55] (0.412)
Step: 8899, Reward: [-214.525 -214.525] [0.0000], Avg: [-1048.836 -1048.836] (0.410)
Step: 8949, Reward: [-229.663 -229.663] [0.0000], Avg: [-1044.26 -1044.26] (0.408)
Step: 8999, Reward: [-258.513 -258.513] [0.0000], Avg: [-1039.894 -1039.894] (0.406)
Step: 9049, Reward: [-250.745 -250.745] [0.0000], Avg: [-1035.535 -1035.535] (0.404)
Step: 9099, Reward: [-10.598 -10.598] [0.0000], Avg: [-1029.903 -1029.903] (0.402)
Step: 9149, Reward: [-65.001 -65.001] [0.0000], Avg: [-1024.63 -1024.63] (0.400)
Step: 9199, Reward: [-182.969 -182.969] [0.0000], Avg: [-1020.056 -1020.056] (0.398)
Step: 9249, Reward: [-338.743 -338.743] [0.0000], Avg: [-1016.373 -1016.373] (0.396)
Step: 9299, Reward: [-260.161 -260.161] [0.0000], Avg: [-1012.308 -1012.308] (0.394)
Step: 9349, Reward: [-91.225 -91.225] [0.0000], Avg: [-1007.382 -1007.382] (0.392)
Step: 9399, Reward: [-91.934 -91.934] [0.0000], Avg: [-1002.513 -1002.513] (0.390)
Step: 9449, Reward: [-92.628 -92.628] [0.0000], Avg: [-997.698 -997.698] (0.388)
Step: 9499, Reward: [-38.166 -38.166] [0.0000], Avg: [-992.648 -992.648] (0.386)
Step: 9549, Reward: [-134.539 -134.539] [0.0000], Avg: [-988.156 -988.156] (0.384)
Step: 9599, Reward: [-192.859 -192.859] [0.0000], Avg: [-984.013 -984.013] (0.382)
Step: 9649, Reward: [-34.597 -34.597] [0.0000], Avg: [-979.094 -979.094] (0.380)
Step: 9699, Reward: [-30.467 -30.467] [0.0000], Avg: [-974.204 -974.204] (0.378)
Step: 9749, Reward: [-114.816 -114.816] [0.0000], Avg: [-969.797 -969.797] (0.376)
Step: 9799, Reward: [-116.324 -116.324] [0.0000], Avg: [-965.443 -965.443] (0.374)
Step: 9849, Reward: [-142.841 -142.841] [0.0000], Avg: [-961.267 -961.267] (0.373)
Step: 9899, Reward: [-9.337 -9.337] [0.0000], Avg: [-956.459 -956.459] (0.371)
Step: 9949, Reward: [-430.951 -430.951] [0.0000], Avg: [-953.819 -953.819] (0.369)
Step: 9999, Reward: [-82.744 -82.744] [0.0000], Avg: [-949.463 -949.463] (0.367)
Step: 10049, Reward: [-73.063 -73.063] [0.0000], Avg: [-945.103 -945.103] (0.365)
Step: 10099, Reward: [-12.709 -12.709] [0.0000], Avg: [-940.487 -940.487] (0.363)
Step: 10149, Reward: [-132.715 -132.715] [0.0000], Avg: [-936.508 -936.508] (0.361)
Step: 10199, Reward: [-259.15 -259.15] [0.0000], Avg: [-933.188 -933.188] (0.360)
Step: 10249, Reward: [-330.629 -330.629] [0.0000], Avg: [-930.248 -930.248] (0.358)
Step: 10299, Reward: [-37.506 -37.506] [0.0000], Avg: [-925.915 -925.915] (0.356)
Step: 10349, Reward: [-161.255 -161.255] [0.0000], Avg: [-922.221 -922.221] (0.354)
Step: 10399, Reward: [-222.898 -222.898] [0.0000], Avg: [-918.859 -918.859] (0.353)
Step: 10449, Reward: [-286.206 -286.206] [0.0000], Avg: [-915.831 -915.831] (0.351)
Step: 10499, Reward: [-32.179 -32.179] [0.0000], Avg: [-911.624 -911.624] (0.349)
Step: 10549, Reward: [-103.922 -103.922] [0.0000], Avg: [-907.796 -907.796] (0.347)
Step: 10599, Reward: [-196.117 -196.117] [0.0000], Avg: [-904.439 -904.439] (0.346)
Step: 10649, Reward: [-201.776 -201.776] [0.0000], Avg: [-901.14 -901.14] (0.344)
Step: 10699, Reward: [-49.591 -49.591] [0.0000], Avg: [-897.161 -897.161] (0.342)
Step: 10749, Reward: [-201.319 -201.319] [0.0000], Avg: [-893.924 -893.924] (0.340)
Step: 10799, Reward: [-445.283 -445.283] [0.0000], Avg: [-891.847 -891.847] (0.339)
Step: 10849, Reward: [-212.691 -212.691] [0.0000], Avg: [-888.717 -888.717] (0.337)
Step: 10899, Reward: [-46.971 -46.971] [0.0000], Avg: [-884.856 -884.856] (0.335)
Step: 10949, Reward: [-476.775 -476.775] [0.0000], Avg: [-882.993 -882.993] (0.334)
Step: 10999, Reward: [-148.309 -148.309] [0.0000], Avg: [-879.653 -879.653] (0.332)
Step: 11049, Reward: [-110.784 -110.784] [0.0000], Avg: [-876.174 -876.174] (0.330)
Step: 11099, Reward: [-139.304 -139.304] [0.0000], Avg: [-872.855 -872.855] (0.329)
Step: 11149, Reward: [-333.549 -333.549] [0.0000], Avg: [-870.437 -870.437] (0.327)
Step: 11199, Reward: [-65.466 -65.466] [0.0000], Avg: [-866.843 -866.843] (0.325)
Step: 11249, Reward: [-1062.986 -1062.986] [0.0000], Avg: [-867.715 -867.715] (0.324)
Step: 11299, Reward: [-1389.927 -1389.927] [0.0000], Avg: [-870.025 -870.025] (0.322)
Step: 11349, Reward: [-1776.444 -1776.444] [0.0000], Avg: [-874.018 -874.018] (0.321)
Step: 11399, Reward: [-1116.007 -1116.007] [0.0000], Avg: [-875.08 -875.08] (0.319)
Step: 11449, Reward: [-1092.61 -1092.61] [0.0000], Avg: [-876.03 -876.03] (0.317)
Step: 11499, Reward: [-2473.204 -2473.204] [0.0000], Avg: [-882.974 -882.974] (0.316)
Step: 11549, Reward: [-839.533 -839.533] [0.0000], Avg: [-882.786 -882.786] (0.314)
Step: 11599, Reward: [-1651.234 -1651.234] [0.0000], Avg: [-886.098 -886.098] (0.313)
Step: 11649, Reward: [-1500.257 -1500.257] [0.0000], Avg: [-888.734 -888.734] (0.311)
Step: 11699, Reward: [-1533.088 -1533.088] [0.0000], Avg: [-891.488 -891.488] (0.309)
Step: 11749, Reward: [-2214.784 -2214.784] [0.0000], Avg: [-897.119 -897.119] (0.308)
Step: 11799, Reward: [-1846.222 -1846.222] [0.0000], Avg: [-901.14 -901.14] (0.306)
Step: 11849, Reward: [-3178.508 -3178.508] [0.0000], Avg: [-910.749 -910.749] (0.305)
Step: 11899, Reward: [-1796.455 -1796.455] [0.0000], Avg: [-914.471 -914.471] (0.303)
Step: 11949, Reward: [-2211.274 -2211.274] [0.0000], Avg: [-919.897 -919.897] (0.302)
Step: 11999, Reward: [-3443.616 -3443.616] [0.0000], Avg: [-930.412 -930.412] (0.300)
Step: 12049, Reward: [-2046.291 -2046.291] [0.0000], Avg: [-935.043 -935.043] (0.299)
Step: 12099, Reward: [-1863.183 -1863.183] [0.0000], Avg: [-938.878 -938.878] (0.297)
Step: 12149, Reward: [-3183.815 -3183.815] [0.0000], Avg: [-948.116 -948.116] (0.296)
Step: 12199, Reward: [-2078.377 -2078.377] [0.0000], Avg: [-952.748 -952.748] (0.294)
Step: 12249, Reward: [-3503.907 -3503.907] [0.0000], Avg: [-963.161 -963.161] (0.293)
Step: 12299, Reward: [-2682.262 -2682.262] [0.0000], Avg: [-970.15 -970.15] (0.291)
Step: 12349, Reward: [-4114.65 -4114.65] [0.0000], Avg: [-982.88 -982.88] (0.290)
Step: 12399, Reward: [-2273.379 -2273.379] [0.0000], Avg: [-988.084 -988.084] (0.288)
Step: 12449, Reward: [-3878.54 -3878.54] [0.0000], Avg: [-999.692 -999.692] (0.287)
Step: 12499, Reward: [-2693.582 -2693.582] [0.0000], Avg: [-1006.468 -1006.468] (0.286)
Step: 12549, Reward: [-3355.355 -3355.355] [0.0000], Avg: [-1015.826 -1015.826] (0.284)
Step: 12599, Reward: [-3482.451 -3482.451] [0.0000], Avg: [-1025.614 -1025.614] (0.283)
Step: 12649, Reward: [-1918.377 -1918.377] [0.0000], Avg: [-1029.143 -1029.143] (0.281)
Step: 12699, Reward: [-2125.025 -2125.025] [0.0000], Avg: [-1033.457 -1033.457] (0.280)
Step: 12749, Reward: [-2968.491 -2968.491] [0.0000], Avg: [-1041.046 -1041.046] (0.279)
Step: 12799, Reward: [-2757.526 -2757.526] [0.0000], Avg: [-1047.751 -1047.751] (0.277)
Step: 12849, Reward: [-2548.825 -2548.825] [0.0000], Avg: [-1053.591 -1053.591] (0.276)
Step: 12899, Reward: [-2466.317 -2466.317] [0.0000], Avg: [-1059.067 -1059.067] (0.274)
Step: 12949, Reward: [-1967.684 -1967.684] [0.0000], Avg: [-1062.575 -1062.575] (0.273)
Step: 12999, Reward: [-4247.151 -4247.151] [0.0000], Avg: [-1074.824 -1074.824] (0.272)
Step: 13049, Reward: [-3212.349 -3212.349] [0.0000], Avg: [-1083.013 -1083.013] (0.270)
Step: 13099, Reward: [-2567.753 -2567.753] [0.0000], Avg: [-1088.68 -1088.68] (0.269)
Step: 13149, Reward: [-3230.187 -3230.187] [0.0000], Avg: [-1096.823 -1096.823] (0.268)
Step: 13199, Reward: [-4037.937 -4037.937] [0.0000], Avg: [-1107.964 -1107.964] (0.266)
Step: 13249, Reward: [-3013.274 -3013.274] [0.0000], Avg: [-1115.153 -1115.153] (0.265)
Step: 13299, Reward: [-3382.649 -3382.649] [0.0000], Avg: [-1123.678 -1123.678] (0.264)
Step: 13349, Reward: [-2480.671 -2480.671] [0.0000], Avg: [-1128.76 -1128.76] (0.262)
Step: 13399, Reward: [-4852.425 -4852.425] [0.0000], Avg: [-1142.654 -1142.654] (0.261)
Step: 13449, Reward: [-3111.586 -3111.586] [0.0000], Avg: [-1149.974 -1149.974] (0.260)
Step: 13499, Reward: [-2352.707 -2352.707] [0.0000], Avg: [-1154.428 -1154.428] (0.258)
Step: 13549, Reward: [-2266.252 -2266.252] [0.0000], Avg: [-1158.531 -1158.531] (0.257)
Step: 13599, Reward: [-4065.886 -4065.886] [0.0000], Avg: [-1169.22 -1169.22] (0.256)
Step: 13649, Reward: [-1907.924 -1907.924] [0.0000], Avg: [-1171.926 -1171.926] (0.255)
Step: 13699, Reward: [-2712.73 -2712.73] [0.0000], Avg: [-1177.549 -1177.549] (0.253)
Step: 13749, Reward: [-2724.082 -2724.082] [0.0000], Avg: [-1183.173 -1183.173] (0.252)
Step: 13799, Reward: [-3598.055 -3598.055] [0.0000], Avg: [-1191.923 -1191.923] (0.251)
Step: 13849, Reward: [-3825.657 -3825.657] [0.0000], Avg: [-1201.431 -1201.431] (0.249)
Step: 13899, Reward: [-2884.184 -2884.184] [0.0000], Avg: [-1207.484 -1207.484] (0.248)
Step: 13949, Reward: [-2704.322 -2704.322] [0.0000], Avg: [-1212.849 -1212.849] (0.247)
Step: 13999, Reward: [-3081.442 -3081.442] [0.0000], Avg: [-1219.522 -1219.522] (0.246)
Step: 14049, Reward: [-3009.085 -3009.085] [0.0000], Avg: [-1225.891 -1225.891] (0.245)
Step: 14099, Reward: [-4500.619 -4500.619] [0.0000], Avg: [-1237.503 -1237.503] (0.243)
Step: 14149, Reward: [-2850.494 -2850.494] [0.0000], Avg: [-1243.203 -1243.203] (0.242)
Step: 14199, Reward: [-2044.287 -2044.287] [0.0000], Avg: [-1246.024 -1246.024] (0.241)
Step: 14249, Reward: [-3835.908 -3835.908] [0.0000], Avg: [-1255.111 -1255.111] (0.240)
Step: 14299, Reward: [-2733.344 -2733.344] [0.0000], Avg: [-1260.28 -1260.28] (0.238)
Step: 14349, Reward: [-2209.129 -2209.129] [0.0000], Avg: [-1263.586 -1263.586] (0.237)
Step: 14399, Reward: [-2506.013 -2506.013] [0.0000], Avg: [-1267.9 -1267.9] (0.236)
Step: 14449, Reward: [-2733.727 -2733.727] [0.0000], Avg: [-1272.972 -1272.972] (0.235)
Step: 14499, Reward: [-4634.951 -4634.951] [0.0000], Avg: [-1284.565 -1284.565] (0.234)
Step: 14549, Reward: [-2989.828 -2989.828] [0.0000], Avg: [-1290.425 -1290.425] (0.233)
Step: 14599, Reward: [-1730.183 -1730.183] [0.0000], Avg: [-1291.931 -1291.931] (0.231)
Step: 14649, Reward: [-4011.868 -4011.868] [0.0000], Avg: [-1301.214 -1301.214] (0.230)
Step: 14699, Reward: [-2421.253 -2421.253] [0.0000], Avg: [-1305.024 -1305.024] (0.229)
Step: 14749, Reward: [-3895.437 -3895.437] [0.0000], Avg: [-1313.805 -1313.805] (0.228)
Step: 14799, Reward: [-1608.027 -1608.027] [0.0000], Avg: [-1314.799 -1314.799] (0.227)
Step: 14849, Reward: [-2519.35 -2519.35] [0.0000], Avg: [-1318.854 -1318.854] (0.226)
Step: 14899, Reward: [-3901.513 -3901.513] [0.0000], Avg: [-1327.521 -1327.521] (0.225)
Step: 14949, Reward: [-3174.496 -3174.496] [0.0000], Avg: [-1333.698 -1333.698] (0.223)
Step: 14999, Reward: [-3830.533 -3830.533] [0.0000], Avg: [-1342.021 -1342.021] (0.222)
Step: 15049, Reward: [-3884.783 -3884.783] [0.0000], Avg: [-1350.469 -1350.469] (0.221)
Step: 15099, Reward: [-4262.346 -4262.346] [0.0000], Avg: [-1360.111 -1360.111] (0.220)
Step: 15149, Reward: [-1925.375 -1925.375] [0.0000], Avg: [-1361.976 -1361.976] (0.219)
Step: 15199, Reward: [-4856.194 -4856.194] [0.0000], Avg: [-1373.47 -1373.47] (0.218)
Step: 15249, Reward: [-2480.736 -2480.736] [0.0000], Avg: [-1377.101 -1377.101] (0.217)
Step: 15299, Reward: [-2251.367 -2251.367] [0.0000], Avg: [-1379.958 -1379.958] (0.216)
Step: 15349, Reward: [-3455.174 -3455.174] [0.0000], Avg: [-1386.717 -1386.717] (0.215)
Step: 15399, Reward: [-3416.635 -3416.635] [0.0000], Avg: [-1393.308 -1393.308] (0.214)
Step: 15449, Reward: [-3092.806 -3092.806] [0.0000], Avg: [-1398.808 -1398.808] (0.212)
Step: 15499, Reward: [-3629.959 -3629.959] [0.0000], Avg: [-1406.005 -1406.005] (0.211)
Step: 15549, Reward: [-2470.393 -2470.393] [0.0000], Avg: [-1409.428 -1409.428] (0.210)
Step: 15599, Reward: [-3113.455 -3113.455] [0.0000], Avg: [-1414.889 -1414.889] (0.209)
Step: 15649, Reward: [-3379.447 -3379.447] [0.0000], Avg: [-1421.166 -1421.166] (0.208)
Step: 15699, Reward: [-1971.227 -1971.227] [0.0000], Avg: [-1422.918 -1422.918] (0.207)
Step: 15749, Reward: [-1804.645 -1804.645] [0.0000], Avg: [-1424.13 -1424.13] (0.206)
Step: 15799, Reward: [-2851.367 -2851.367] [0.0000], Avg: [-1428.646 -1428.646] (0.205)
Step: 15849, Reward: [-2844.115 -2844.115] [0.0000], Avg: [-1433.111 -1433.111] (0.204)
Step: 15899, Reward: [-2380.197 -2380.197] [0.0000], Avg: [-1436.09 -1436.09] (0.203)
Step: 15949, Reward: [-2063.749 -2063.749] [0.0000], Avg: [-1438.057 -1438.057] (0.202)
Step: 15999, Reward: [-3399.31 -3399.31] [0.0000], Avg: [-1444.186 -1444.186] (0.201)
Step: 16049, Reward: [-2224.82 -2224.82] [0.0000], Avg: [-1446.618 -1446.618] (0.200)
Step: 16099, Reward: [-2435.883 -2435.883] [0.0000], Avg: [-1449.69 -1449.69] (0.199)
Step: 16149, Reward: [-3057.817 -3057.817] [0.0000], Avg: [-1454.669 -1454.669] (0.198)
Step: 16199, Reward: [-2403.068 -2403.068] [0.0000], Avg: [-1457.596 -1457.596] (0.197)
Step: 16249, Reward: [-4528.431 -4528.431] [0.0000], Avg: [-1467.045 -1467.045] (0.196)
Step: 16299, Reward: [-2628.625 -2628.625] [0.0000], Avg: [-1470.608 -1470.608] (0.195)
Step: 16349, Reward: [-4823.366 -4823.366] [0.0000], Avg: [-1480.861 -1480.861] (0.194)
Step: 16399, Reward: [-3043.503 -3043.503] [0.0000], Avg: [-1485.625 -1485.625] (0.193)
Step: 16449, Reward: [-3604.297 -3604.297] [0.0000], Avg: [-1492.065 -1492.065] (0.192)
Step: 16499, Reward: [-3645.488 -3645.488] [0.0000], Avg: [-1498.59 -1498.59] (0.191)
Step: 16549, Reward: [-3578.725 -3578.725] [0.0000], Avg: [-1504.875 -1504.875] (0.190)
Step: 16599, Reward: [-2771.352 -2771.352] [0.0000], Avg: [-1508.69 -1508.69] (0.189)
Step: 16649, Reward: [-3610.883 -3610.883] [0.0000], Avg: [-1515.002 -1515.002] (0.188)
Step: 16699, Reward: [-2650.928 -2650.928] [0.0000], Avg: [-1518.403 -1518.403] (0.187)
Step: 16749, Reward: [-3588.256 -3588.256] [0.0000], Avg: [-1524.582 -1524.582] (0.187)
Step: 16799, Reward: [-2301.693 -2301.693] [0.0000], Avg: [-1526.895 -1526.895] (0.186)
Step: 16849, Reward: [-3118.739 -3118.739] [0.0000], Avg: [-1531.618 -1531.618] (0.185)
Step: 16899, Reward: [-3052.466 -3052.466] [0.0000], Avg: [-1536.118 -1536.118] (0.184)
Step: 16949, Reward: [-2903.182 -2903.182] [0.0000], Avg: [-1540.151 -1540.151] (0.183)
Step: 16999, Reward: [-2835.626 -2835.626] [0.0000], Avg: [-1543.961 -1543.961] (0.182)
Step: 17049, Reward: [-1808.369 -1808.369] [0.0000], Avg: [-1544.736 -1544.736] (0.181)
Step: 17099, Reward: [-2947.647 -2947.647] [0.0000], Avg: [-1548.838 -1548.838] (0.180)
Step: 17149, Reward: [-4331.274 -4331.274] [0.0000], Avg: [-1556.95 -1556.95] (0.179)
Step: 17199, Reward: [-3265.073 -3265.073] [0.0000], Avg: [-1561.916 -1561.916] (0.178)
Step: 17249, Reward: [-3315.978 -3315.978] [0.0000], Avg: [-1567. -1567.] (0.177)
Step: 17299, Reward: [-2372.403 -2372.403] [0.0000], Avg: [-1569.328 -1569.328] (0.177)
Step: 17349, Reward: [-3276.324 -3276.324] [0.0000], Avg: [-1574.247 -1574.247] (0.176)
Step: 17399, Reward: [-3585.02 -3585.02] [0.0000], Avg: [-1580.025 -1580.025] (0.175)
Step: 17449, Reward: [-2739.735 -2739.735] [0.0000], Avg: [-1583.348 -1583.348] (0.174)
Step: 17499, Reward: [-2046.696 -2046.696] [0.0000], Avg: [-1584.672 -1584.672] (0.173)
Step: 17549, Reward: [-2674.154 -2674.154] [0.0000], Avg: [-1587.776 -1587.776] (0.172)
Step: 17599, Reward: [-3197.689 -3197.689] [0.0000], Avg: [-1592.35 -1592.35] (0.171)
Step: 17649, Reward: [-3317.915 -3317.915] [0.0000], Avg: [-1597.238 -1597.238] (0.170)
Step: 17699, Reward: [-2958.113 -2958.113] [0.0000], Avg: [-1601.082 -1601.082] (0.170)
Step: 17749, Reward: [-3813.832 -3813.832] [0.0000], Avg: [-1607.315 -1607.315] (0.169)
Step: 17799, Reward: [-4804.603 -4804.603] [0.0000], Avg: [-1616.296 -1616.296] (0.168)
Step: 17849, Reward: [-4707.559 -4707.559] [0.0000], Avg: [-1624.955 -1624.955] (0.167)
Step: 17899, Reward: [-3176.694 -3176.694] [0.0000], Avg: [-1629.29 -1629.29] (0.166)
Step: 17949, Reward: [-6.804 -6.804] [0.0000], Avg: [-1624.77 -1624.77] (0.165)
Step: 17999, Reward: [-2498.537 -2498.537] [0.0000], Avg: [-1627.198 -1627.198] (0.165)
Step: 18049, Reward: [-3772.216 -3772.216] [0.0000], Avg: [-1633.139 -1633.139] (0.164)
Step: 18099, Reward: [-2516.228 -2516.228] [0.0000], Avg: [-1635.579 -1635.579] (0.163)
Step: 18149, Reward: [-3157.912 -3157.912] [0.0000], Avg: [-1639.773 -1639.773] (0.162)
Step: 18199, Reward: [-2257.815 -2257.815] [0.0000], Avg: [-1641.471 -1641.471] (0.161)
Step: 18249, Reward: [-3454.051 -3454.051] [0.0000], Avg: [-1646.437 -1646.437] (0.160)
Step: 18299, Reward: [-3467.689 -3467.689] [0.0000], Avg: [-1651.413 -1651.413] (0.160)
Step: 18349, Reward: [-2492.176 -2492.176] [0.0000], Avg: [-1653.704 -1653.704] (0.159)
Step: 18399, Reward: [-2450.776 -2450.776] [0.0000], Avg: [-1655.87 -1655.87] (0.158)
Step: 18449, Reward: [-2569.539 -2569.539] [0.0000], Avg: [-1658.346 -1658.346] (0.157)
Step: 18499, Reward: [-4509.281 -4509.281] [0.0000], Avg: [-1666.051 -1666.051] (0.157)
Step: 18549, Reward: [-1862.596 -1862.596] [0.0000], Avg: [-1666.581 -1666.581] (0.156)
Step: 18599, Reward: [-2597.622 -2597.622] [0.0000], Avg: [-1669.083 -1669.083] (0.155)
Step: 18649, Reward: [-2248.036 -2248.036] [0.0000], Avg: [-1670.636 -1670.636] (0.154)
Step: 18699, Reward: [-2731.05 -2731.05] [0.0000], Avg: [-1673.471 -1673.471] (0.153)
Step: 18749, Reward: [-3011.846 -3011.846] [0.0000], Avg: [-1677.04 -1677.04] (0.153)
Step: 18799, Reward: [-3616.165 -3616.165] [0.0000], Avg: [-1682.197 -1682.197] (0.152)
Step: 18849, Reward: [-1846.622 -1846.622] [0.0000], Avg: [-1682.633 -1682.633] (0.151)
Step: 18899, Reward: [-3678.209 -3678.209] [0.0000], Avg: [-1687.913 -1687.913] (0.150)
Step: 18949, Reward: [-2040.071 -2040.071] [0.0000], Avg: [-1688.842 -1688.842] (0.150)
Step: 18999, Reward: [-3209.328 -3209.328] [0.0000], Avg: [-1692.843 -1692.843] (0.149)
Step: 19049, Reward: [-2656.67 -2656.67] [0.0000], Avg: [-1695.373 -1695.373] (0.148)
Step: 19099, Reward: [-2650.541 -2650.541] [0.0000], Avg: [-1697.873 -1697.873] (0.147)
Step: 19149, Reward: [-3461.235 -3461.235] [0.0000], Avg: [-1702.477 -1702.477] (0.147)
Step: 19199, Reward: [-3592.394 -3592.394] [0.0000], Avg: [-1707.399 -1707.399] (0.146)
Step: 19249, Reward: [-4126.742 -4126.742] [0.0000], Avg: [-1713.683 -1713.683] (0.145)
Step: 19299, Reward: [-2443.009 -2443.009] [0.0000], Avg: [-1715.572 -1715.572] (0.144)
Step: 19349, Reward: [-2076.787 -2076.787] [0.0000], Avg: [-1716.506 -1716.506] (0.144)
Step: 19399, Reward: [-2370.175 -2370.175] [0.0000], Avg: [-1718.19 -1718.19] (0.143)
Step: 19449, Reward: [-3002.611 -3002.611] [0.0000], Avg: [-1721.492 -1721.492] (0.142)
Step: 19499, Reward: [-3354.618 -3354.618] [0.0000], Avg: [-1725.68 -1725.68] (0.142)
Step: 19549, Reward: [-2487.614 -2487.614] [0.0000], Avg: [-1727.628 -1727.628] (0.141)
Step: 19599, Reward: [-2494.317 -2494.317] [0.0000], Avg: [-1729.584 -1729.584] (0.140)
Step: 19649, Reward: [-3798.59 -3798.59] [0.0000], Avg: [-1734.849 -1734.849] (0.139)
Step: 19699, Reward: [-3046.07 -3046.07] [0.0000], Avg: [-1738.177 -1738.177] (0.139)
Step: 19749, Reward: [-2969.518 -2969.518] [0.0000], Avg: [-1741.294 -1741.294] (0.138)
Step: 19799, Reward: [-4052.537 -4052.537] [0.0000], Avg: [-1747.131 -1747.131] (0.137)
Step: 19849, Reward: [-3035.827 -3035.827] [0.0000], Avg: [-1750.377 -1750.377] (0.137)
Step: 19899, Reward: [-3562.595 -3562.595] [0.0000], Avg: [-1754.93 -1754.93] (0.136)
Step: 19949, Reward: [-2637.989 -2637.989] [0.0000], Avg: [-1757.143 -1757.143] (0.135)
Step: 19999, Reward: [-3708.518 -3708.518] [0.0000], Avg: [-1762.022 -1762.022] (0.135)
Step: 20049, Reward: [-2900.572 -2900.572] [0.0000], Avg: [-1764.861 -1764.861] (0.134)
Step: 20099, Reward: [-3069.669 -3069.669] [0.0000], Avg: [-1768.107 -1768.107] (0.133)
Step: 20149, Reward: [-3400.538 -3400.538] [0.0000], Avg: [-1772.158 -1772.158] (0.133)
Step: 20199, Reward: [-1711.347 -1711.347] [0.0000], Avg: [-1772.007 -1772.007] (0.132)
Step: 20249, Reward: [-2729.077 -2729.077] [0.0000], Avg: [-1774.37 -1774.37] (0.131)
Step: 20299, Reward: [-3044.156 -3044.156] [0.0000], Avg: [-1777.498 -1777.498] (0.131)
Step: 20349, Reward: [-3796.618 -3796.618] [0.0000], Avg: [-1782.459 -1782.459] (0.130)
Step: 20399, Reward: [-3729.505 -3729.505] [0.0000], Avg: [-1787.231 -1787.231] (0.129)
Step: 20449, Reward: [-1718.443 -1718.443] [0.0000], Avg: [-1787.063 -1787.063] (0.129)
Step: 20499, Reward: [-2827.121 -2827.121] [0.0000], Avg: [-1789.599 -1789.599] (0.128)
Step: 20549, Reward: [-2487.732 -2487.732] [0.0000], Avg: [-1791.298 -1791.298] (0.127)
Step: 20599, Reward: [-3862.528 -3862.528] [0.0000], Avg: [-1796.325 -1796.325] (0.127)
Step: 20649, Reward: [-2825.68 -2825.68] [0.0000], Avg: [-1798.818 -1798.818] (0.126)
Step: 20699, Reward: [-3057.237 -3057.237] [0.0000], Avg: [-1801.857 -1801.857] (0.126)
Step: 20749, Reward: [-1979.601 -1979.601] [0.0000], Avg: [-1802.286 -1802.286] (0.125)
Step: 20799, Reward: [-3937.835 -3937.835] [0.0000], Avg: [-1807.419 -1807.419] (0.124)
Step: 20849, Reward: [-2333.965 -2333.965] [0.0000], Avg: [-1808.682 -1808.682] (0.124)
Step: 20899, Reward: [-4759.268 -4759.268] [0.0000], Avg: [-1815.741 -1815.741] (0.123)
Step: 20949, Reward: [-3255.916 -3255.916] [0.0000], Avg: [-1819.178 -1819.178] (0.122)
Step: 20999, Reward: [-1928.961 -1928.961] [0.0000], Avg: [-1819.439 -1819.439] (0.122)
Step: 21049, Reward: [-4523.4 -4523.4] [0.0000], Avg: [-1825.862 -1825.862] (0.121)
Step: 21099, Reward: [-3509.832 -3509.832] [0.0000], Avg: [-1829.852 -1829.852] (0.121)
Step: 21149, Reward: [-2571.563 -2571.563] [0.0000], Avg: [-1831.606 -1831.606] (0.120)
Step: 21199, Reward: [-2847.836 -2847.836] [0.0000], Avg: [-1834.003 -1834.003] (0.119)
Step: 21249, Reward: [-1799.507 -1799.507] [0.0000], Avg: [-1833.921 -1833.921] (0.119)
Step: 21299, Reward: [-2663.647 -2663.647] [0.0000], Avg: [-1835.869 -1835.869] (0.118)
Step: 21349, Reward: [-4007.265 -4007.265] [0.0000], Avg: [-1840.954 -1840.954] (0.118)
Step: 21399, Reward: [-2945.565 -2945.565] [0.0000], Avg: [-1843.535 -1843.535] (0.117)
Step: 21449, Reward: [-1576.472 -1576.472] [0.0000], Avg: [-1842.913 -1842.913] (0.116)
Step: 21499, Reward: [-2877.542 -2877.542] [0.0000], Avg: [-1845.319 -1845.319] (0.116)
Step: 21549, Reward: [-3501.114 -3501.114] [0.0000], Avg: [-1849.161 -1849.161] (0.115)
Step: 21599, Reward: [-3001.016 -3001.016] [0.0000], Avg: [-1851.827 -1851.827] (0.115)
Step: 21649, Reward: [-1624.969 -1624.969] [0.0000], Avg: [-1851.303 -1851.303] (0.114)
Step: 21699, Reward: [-2467.172 -2467.172] [0.0000], Avg: [-1852.722 -1852.722] (0.114)
Step: 21749, Reward: [-2273.078 -2273.078] [0.0000], Avg: [-1853.688 -1853.688] (0.113)
Step: 21799, Reward: [-2409.684 -2409.684] [0.0000], Avg: [-1854.964 -1854.964] (0.112)
Step: 21849, Reward: [-2834.91 -2834.91] [0.0000], Avg: [-1857.206 -1857.206] (0.112)
Step: 21899, Reward: [-2101.737 -2101.737] [0.0000], Avg: [-1857.764 -1857.764] (0.111)
Step: 21949, Reward: [-2055.912 -2055.912] [0.0000], Avg: [-1858.216 -1858.216] (0.111)
Step: 21999, Reward: [-3113.718 -3113.718] [0.0000], Avg: [-1861.069 -1861.069] (0.110)
Step: 22049, Reward: [-2905.455 -2905.455] [0.0000], Avg: [-1863.437 -1863.437] (0.110)
Step: 22099, Reward: [-2148.376 -2148.376] [0.0000], Avg: [-1864.082 -1864.082] (0.109)
Step: 22149, Reward: [-4074.955 -4074.955] [0.0000], Avg: [-1869.073 -1869.073] (0.109)
Step: 22199, Reward: [-3921.521 -3921.521] [0.0000], Avg: [-1873.695 -1873.695] (0.108)
Step: 22249, Reward: [-3264.323 -3264.323] [0.0000], Avg: [-1876.82 -1876.82] (0.107)
Step: 22299, Reward: [-3366.382 -3366.382] [0.0000], Avg: [-1880.16 -1880.16] (0.107)
Step: 22349, Reward: [-3660.883 -3660.883] [0.0000], Avg: [-1884.144 -1884.144] (0.106)
Step: 22399, Reward: [-2063.861 -2063.861] [0.0000], Avg: [-1884.545 -1884.545] (0.106)
Step: 22449, Reward: [-2560.144 -2560.144] [0.0000], Avg: [-1886.05 -1886.05] (0.105)
Step: 22499, Reward: [-2088.386 -2088.386] [0.0000], Avg: [-1886.499 -1886.499] (0.105)
Step: 22549, Reward: [-2288.528 -2288.528] [0.0000], Avg: [-1887.391 -1887.391] (0.104)
Step: 22599, Reward: [-2570.196 -2570.196] [0.0000], Avg: [-1888.901 -1888.901] (0.104)
Step: 22649, Reward: [-2424.475 -2424.475] [0.0000], Avg: [-1890.084 -1890.084] (0.103)
Step: 22699, Reward: [-2720.381 -2720.381] [0.0000], Avg: [-1891.913 -1891.913] (0.103)
Step: 22749, Reward: [-3035.676 -3035.676] [0.0000], Avg: [-1894.426 -1894.426] (0.102)
Step: 22799, Reward: [-3404.324 -3404.324] [0.0000], Avg: [-1897.737 -1897.737] (0.102)
Step: 22849, Reward: [-3695.606 -3695.606] [0.0000], Avg: [-1901.672 -1901.672] (0.101)
Step: 22899, Reward: [-2773.894 -2773.894] [0.0000], Avg: [-1903.576 -1903.576] (0.101)
Step: 22949, Reward: [-3878.956 -3878.956] [0.0000], Avg: [-1907.88 -1907.88] (0.100)
Step: 22999, Reward: [-3595.136 -3595.136] [0.0000], Avg: [-1911.548 -1911.548] (0.100)
Step: 23049, Reward: [-2803.574 -2803.574] [0.0000], Avg: [-1913.483 -1913.483] (0.099)
Step: 23099, Reward: [-2324.729 -2324.729] [0.0000], Avg: [-1914.373 -1914.373] (0.099)
Step: 23149, Reward: [-1793.949 -1793.949] [0.0000], Avg: [-1914.113 -1914.113] (0.098)
Step: 23199, Reward: [-2907.134 -2907.134] [0.0000], Avg: [-1916.253 -1916.253] (0.098)
Step: 23249, Reward: [-4188.413 -4188.413] [0.0000], Avg: [-1921.139 -1921.139] (0.097)
Step: 23299, Reward: [-3571.226 -3571.226] [0.0000], Avg: [-1924.68 -1924.68] (0.097)
Step: 23349, Reward: [-2439.487 -2439.487] [0.0000], Avg: [-1925.782 -1925.782] (0.096)
Step: 23399, Reward: [-2780.017 -2780.017] [0.0000], Avg: [-1927.608 -1927.608] (0.096)
Step: 23449, Reward: [-2510.867 -2510.867] [0.0000], Avg: [-1928.851 -1928.851] (0.095)
Step: 23499, Reward: [-3414.283 -3414.283] [0.0000], Avg: [-1932.012 -1932.012] (0.095)
Step: 23549, Reward: [-2946.041 -2946.041] [0.0000], Avg: [-1934.165 -1934.165] (0.094)
Step: 23599, Reward: [-3639.394 -3639.394] [0.0000], Avg: [-1937.778 -1937.778] (0.094)
Step: 23649, Reward: [-1809.591 -1809.591] [0.0000], Avg: [-1937.507 -1937.507] (0.093)
Step: 23699, Reward: [-1642.572 -1642.572] [0.0000], Avg: [-1936.884 -1936.884] (0.093)
Step: 23749, Reward: [-1932.183 -1932.183] [0.0000], Avg: [-1936.874 -1936.874] (0.092)
Step: 23799, Reward: [-3834.312 -3834.312] [0.0000], Avg: [-1940.861 -1940.861] (0.092)
Step: 23849, Reward: [-2147.051 -2147.051] [0.0000], Avg: [-1941.293 -1941.293] (0.092)
Step: 23899, Reward: [-3123.201 -3123.201] [0.0000], Avg: [-1943.765 -1943.765] (0.091)
Step: 23949, Reward: [-3201.711 -3201.711] [0.0000], Avg: [-1946.392 -1946.392] (0.091)
Step: 23999, Reward: [-1751.798 -1751.798] [0.0000], Avg: [-1945.986 -1945.986] (0.090)
Step: 24049, Reward: [-3876.201 -3876.201] [0.0000], Avg: [-1949.999 -1949.999] (0.090)
Step: 24099, Reward: [-2074.989 -2074.989] [0.0000], Avg: [-1950.258 -1950.258] (0.089)
Step: 24149, Reward: [-2051.673 -2051.673] [0.0000], Avg: [-1950.468 -1950.468] (0.089)
Step: 24199, Reward: [-2535.795 -2535.795] [0.0000], Avg: [-1951.678 -1951.678] (0.088)
Step: 24249, Reward: [-4066.08 -4066.08] [0.0000], Avg: [-1956.037 -1956.037] (0.088)
Step: 24299, Reward: [-3000.673 -3000.673] [0.0000], Avg: [-1958.187 -1958.187] (0.088)
Step: 24349, Reward: [-2851.308 -2851.308] [0.0000], Avg: [-1960.021 -1960.021] (0.087)
Step: 24399, Reward: [-4284.136 -4284.136] [0.0000], Avg: [-1964.783 -1964.783] (0.087)
Step: 24449, Reward: [-3792.283 -3792.283] [0.0000], Avg: [-1968.521 -1968.521] (0.086)
Step: 24499, Reward: [-3176.021 -3176.021] [0.0000], Avg: [-1970.985 -1970.985] (0.086)
Step: 24549, Reward: [-3078.213 -3078.213] [0.0000], Avg: [-1973.24 -1973.24] (0.085)
Step: 24599, Reward: [-4179.986 -4179.986] [0.0000], Avg: [-1977.725 -1977.725] (0.085)
Step: 24649, Reward: [-1734.175 -1734.175] [0.0000], Avg: [-1977.231 -1977.231] (0.084)
Step: 24699, Reward: [-2878.225 -2878.225] [0.0000], Avg: [-1979.055 -1979.055] (0.084)
Step: 24749, Reward: [-2679.356 -2679.356] [0.0000], Avg: [-1980.47 -1980.47] (0.084)
Step: 24799, Reward: [-2920.223 -2920.223] [0.0000], Avg: [-1982.364 -1982.364] (0.083)
Step: 24849, Reward: [-2012.095 -2012.095] [0.0000], Avg: [-1982.424 -1982.424] (0.083)
Step: 24899, Reward: [-2984.839 -2984.839] [0.0000], Avg: [-1984.437 -1984.437] (0.082)
Step: 24949, Reward: [-2284.828 -2284.828] [0.0000], Avg: [-1985.039 -1985.039] (0.082)
Step: 24999, Reward: [-2850.988 -2850.988] [0.0000], Avg: [-1986.771 -1986.771] (0.082)
Step: 25049, Reward: [-3379.741 -3379.741] [0.0000], Avg: [-1989.551 -1989.551] (0.081)
Step: 25099, Reward: [-2868.997 -2868.997] [0.0000], Avg: [-1991.303 -1991.303] (0.081)
Step: 25149, Reward: [-3108.445 -3108.445] [0.0000], Avg: [-1993.524 -1993.524] (0.080)
Step: 25199, Reward: [-3279.619 -3279.619] [0.0000], Avg: [-1996.076 -1996.076] (0.080)
Step: 25249, Reward: [-2859.196 -2859.196] [0.0000], Avg: [-1997.785 -1997.785] (0.080)
Step: 25299, Reward: [-1722.478 -1722.478] [0.0000], Avg: [-1997.241 -1997.241] (0.079)
Step: 25349, Reward: [-4662.333 -4662.333] [0.0000], Avg: [-2002.498 -2002.498] (0.079)
Step: 25399, Reward: [-2581.065 -2581.065] [0.0000], Avg: [-2003.637 -2003.637] (0.078)
Step: 25449, Reward: [-3132.869 -3132.869] [0.0000], Avg: [-2005.855 -2005.855] (0.078)
Step: 25499, Reward: [-4385.484 -4385.484] [0.0000], Avg: [-2010.521 -2010.521] (0.078)
Step: 25549, Reward: [-2489.75 -2489.75] [0.0000], Avg: [-2011.459 -2011.459] (0.077)
Step: 25599, Reward: [-3281.035 -3281.035] [0.0000], Avg: [-2013.938 -2013.938] (0.077)
Step: 25649, Reward: [-3004.158 -3004.158] [0.0000], Avg: [-2015.869 -2015.869] (0.076)
Step: 25699, Reward: [-2680.033 -2680.033] [0.0000], Avg: [-2017.161 -2017.161] (0.076)
Step: 25749, Reward: [-3174.763 -3174.763] [0.0000], Avg: [-2019.409 -2019.409] (0.076)
Step: 25799, Reward: [-2836.531 -2836.531] [0.0000], Avg: [-2020.992 -2020.992] (0.075)
Step: 25849, Reward: [-4197.186 -4197.186] [0.0000], Avg: [-2025.201 -2025.201] (0.075)
Step: 25899, Reward: [-4041.193 -4041.193] [0.0000], Avg: [-2029.093 -2029.093] (0.075)
Step: 25949, Reward: [-2142.154 -2142.154] [0.0000], Avg: [-2029.311 -2029.311] (0.074)
Step: 25999, Reward: [-4122.196 -4122.196] [0.0000], Avg: [-2033.336 -2033.336] (0.074)
Step: 26049, Reward: [-1588.525 -1588.525] [0.0000], Avg: [-2032.482 -2032.482] (0.073)
Step: 26099, Reward: [-3410.955 -3410.955] [0.0000], Avg: [-2035.123 -2035.123] (0.073)
Step: 26149, Reward: [-2443.533 -2443.533] [0.0000], Avg: [-2035.904 -2035.904] (0.073)
Step: 26199, Reward: [-2911.987 -2911.987] [0.0000], Avg: [-2037.576 -2037.576] (0.072)
Step: 26249, Reward: [-2238.787 -2238.787] [0.0000], Avg: [-2037.959 -2037.959] (0.072)
Step: 26299, Reward: [-2745.344 -2745.344] [0.0000], Avg: [-2039.304 -2039.304] (0.072)
Step: 26349, Reward: [-1854.615 -1854.615] [0.0000], Avg: [-2038.953 -2038.953] (0.071)
Step: 26399, Reward: [-2661.62 -2661.62] [0.0000], Avg: [-2040.133 -2040.133] (0.071)
Step: 26449, Reward: [-3063.981 -3063.981] [0.0000], Avg: [-2042.068 -2042.068] (0.071)
Step: 26499, Reward: [-3151.929 -3151.929] [0.0000], Avg: [-2044.162 -2044.162] (0.070)
Step: 26549, Reward: [-4053.587 -4053.587] [0.0000], Avg: [-2047.946 -2047.946] (0.070)
Step: 26599, Reward: [-3131.254 -3131.254] [0.0000], Avg: [-2049.983 -2049.983] (0.069)
Step: 26649, Reward: [-4088.168 -4088.168] [0.0000], Avg: [-2053.807 -2053.807] (0.069)
Step: 26699, Reward: [-2760.579 -2760.579] [0.0000], Avg: [-2055.13 -2055.13] (0.069)
Step: 26749, Reward: [-2342.984 -2342.984] [0.0000], Avg: [-2055.668 -2055.668] (0.068)
Step: 26799, Reward: [-3717.72 -3717.72] [0.0000], Avg: [-2058.769 -2058.769] (0.068)
Step: 26849, Reward: [-1615.613 -1615.613] [0.0000], Avg: [-2057.944 -2057.944] (0.068)
Step: 26899, Reward: [-2807.62 -2807.62] [0.0000], Avg: [-2059.337 -2059.337] (0.067)
Step: 26949, Reward: [-3888.056 -3888.056] [0.0000], Avg: [-2062.73 -2062.73] (0.067)
Step: 26999, Reward: [-2340.223 -2340.223] [0.0000], Avg: [-2063.244 -2063.244] (0.067)
Step: 27049, Reward: [-4161.818 -4161.818] [0.0000], Avg: [-2067.123 -2067.123] (0.066)
Step: 27099, Reward: [-3021.227 -3021.227] [0.0000], Avg: [-2068.883 -2068.883] (0.066)
Step: 27149, Reward: [-2556.53 -2556.53] [0.0000], Avg: [-2069.782 -2069.782] (0.066)
Step: 27199, Reward: [-1984.006 -1984.006] [0.0000], Avg: [-2069.624 -2069.624] (0.065)
Step: 27249, Reward: [-3688.294 -3688.294] [0.0000], Avg: [-2072.594 -2072.594] (0.065)
Step: 27299, Reward: [-2877.867 -2877.867] [0.0000], Avg: [-2074.069 -2074.069] (0.065)
Step: 27349, Reward: [-3325.646 -3325.646] [0.0000], Avg: [-2076.357 -2076.357] (0.064)
Step: 27399, Reward: [-2601.568 -2601.568] [0.0000], Avg: [-2077.315 -2077.315] (0.064)
Step: 27449, Reward: [-3296.603 -3296.603] [0.0000], Avg: [-2079.536 -2079.536] (0.064)
Step: 27499, Reward: [-3319.58 -3319.58] [0.0000], Avg: [-2081.791 -2081.791] (0.063)
Step: 27549, Reward: [-3288.597 -3288.597] [0.0000], Avg: [-2083.981 -2083.981] (0.063)
Step: 27599, Reward: [-3310.583 -3310.583] [0.0000], Avg: [-2086.203 -2086.203] (0.063)
Step: 27649, Reward: [-4067.966 -4067.966] [0.0000], Avg: [-2089.787 -2089.787] (0.063)
Step: 27699, Reward: [-1951.674 -1951.674] [0.0000], Avg: [-2089.537 -2089.537] (0.062)
Step: 27749, Reward: [-3066.636 -3066.636] [0.0000], Avg: [-2091.298 -2091.298] (0.062)
Step: 27799, Reward: [-4291.088 -4291.088] [0.0000], Avg: [-2095.254 -2095.254] (0.062)
Step: 27849, Reward: [-3653.472 -3653.472] [0.0000], Avg: [-2098.052 -2098.052] (0.061)
Step: 27899, Reward: [-2851.465 -2851.465] [0.0000], Avg: [-2099.402 -2099.402] (0.061)
Step: 27949, Reward: [-3385.931 -3385.931] [0.0000], Avg: [-2101.704 -2101.704] (0.061)
Step: 27999, Reward: [-3406.245 -3406.245] [0.0000], Avg: [-2104.033 -2104.033] (0.060)
Step: 28049, Reward: [-4473.266 -4473.266] [0.0000], Avg: [-2108.256 -2108.256] (0.060)
Step: 28099, Reward: [-3943.187 -3943.187] [0.0000], Avg: [-2111.521 -2111.521] (0.060)
Step: 28149, Reward: [-3315.326 -3315.326] [0.0000], Avg: [-2113.66 -2113.66] (0.059)
Step: 28199, Reward: [-3072.281 -3072.281] [0.0000], Avg: [-2115.359 -2115.359] (0.059)
Step: 28249, Reward: [-2734.789 -2734.789] [0.0000], Avg: [-2116.456 -2116.456] (0.059)
Step: 28299, Reward: [-2402.304 -2402.304] [0.0000], Avg: [-2116.961 -2116.961] (0.059)
Step: 28349, Reward: [-4464.916 -4464.916] [0.0000], Avg: [-2121.102 -2121.102] (0.058)
Step: 28399, Reward: [-3075.055 -3075.055] [0.0000], Avg: [-2122.781 -2122.781] (0.058)
Step: 28449, Reward: [-4295.403 -4295.403] [0.0000], Avg: [-2126.6 -2126.6] (0.058)
Step: 28499, Reward: [-3735.461 -3735.461] [0.0000], Avg: [-2129.422 -2129.422] (0.057)
Step: 28549, Reward: [-2643.072 -2643.072] [0.0000], Avg: [-2130.322 -2130.322] (0.057)
Step: 28599, Reward: [-3656.086 -3656.086] [0.0000], Avg: [-2132.989 -2132.989] (0.057)
Step: 28649, Reward: [-1817.889 -1817.889] [0.0000], Avg: [-2132.439 -2132.439] (0.057)
Step: 28699, Reward: [-3942.408 -3942.408] [0.0000], Avg: [-2135.592 -2135.592] (0.056)
Step: 28749, Reward: [-3705.973 -3705.973] [0.0000], Avg: [-2138.323 -2138.323] (0.056)
Step: 28799, Reward: [-4371.726 -4371.726] [0.0000], Avg: [-2142.201 -2142.201] (0.056)
Step: 28849, Reward: [-2288.538 -2288.538] [0.0000], Avg: [-2142.455 -2142.455] (0.055)
Step: 28899, Reward: [-2048.573 -2048.573] [0.0000], Avg: [-2142.292 -2142.292] (0.055)
Step: 28949, Reward: [-3301.524 -3301.524] [0.0000], Avg: [-2144.294 -2144.294] (0.055)
Step: 28999, Reward: [-4549.486 -4549.486] [0.0000], Avg: [-2148.441 -2148.441] (0.055)
Step: 29049, Reward: [-3009.97 -3009.97] [0.0000], Avg: [-2149.924 -2149.924] (0.054)
Step: 29099, Reward: [-3589.131 -3589.131] [0.0000], Avg: [-2152.397 -2152.397] (0.054)
Step: 29149, Reward: [-3288.989 -3288.989] [0.0000], Avg: [-2154.346 -2154.346] (0.054)
Step: 29199, Reward: [-2089.028 -2089.028] [0.0000], Avg: [-2154.235 -2154.235] (0.054)
Step: 29249, Reward: [-2680.122 -2680.122] [0.0000], Avg: [-2155.134 -2155.134] (0.053)
Step: 29299, Reward: [-2391.746 -2391.746] [0.0000], Avg: [-2155.537 -2155.537] (0.053)
Step: 29349, Reward: [-3525.044 -3525.044] [0.0000], Avg: [-2157.87 -2157.87] (0.053)
Step: 29399, Reward: [-2774.012 -2774.012] [0.0000], Avg: [-2158.918 -2158.918] (0.052)
Step: 29449, Reward: [-2899.896 -2899.896] [0.0000], Avg: [-2160.176 -2160.176] (0.052)
Step: 29499, Reward: [-2172.586 -2172.586] [0.0000], Avg: [-2160.197 -2160.197] (0.052)
Step: 29549, Reward: [-3020.232 -3020.232] [0.0000], Avg: [-2161.652 -2161.652] (0.052)
Step: 29599, Reward: [-2691.101 -2691.101] [0.0000], Avg: [-2162.547 -2162.547] (0.051)
Step: 29649, Reward: [-2429.784 -2429.784] [0.0000], Avg: [-2162.997 -2162.997] (0.051)
Step: 29699, Reward: [-4040.758 -4040.758] [0.0000], Avg: [-2166.159 -2166.159] (0.051)
Step: 29749, Reward: [-1904.251 -1904.251] [0.0000], Avg: [-2165.719 -2165.719] (0.051)
Step: 29799, Reward: [-2910.229 -2910.229] [0.0000], Avg: [-2166.968 -2166.968] (0.050)
Step: 29849, Reward: [-3094.074 -3094.074] [0.0000], Avg: [-2168.521 -2168.521] (0.050)
Step: 29899, Reward: [-2745.041 -2745.041] [0.0000], Avg: [-2169.485 -2169.485] (0.050)
Step: 29949, Reward: [-3553.115 -3553.115] [0.0000], Avg: [-2171.795 -2171.795] (0.050)
Step: 29999, Reward: [-2927.405 -2927.405] [0.0000], Avg: [-2173.054 -2173.054] (0.049)
Step: 30049, Reward: [-3913.905 -3913.905] [0.0000], Avg: [-2175.951 -2175.951] (0.049)
Step: 30099, Reward: [-4430.861 -4430.861] [0.0000], Avg: [-2179.696 -2179.696] (0.049)
Step: 30149, Reward: [-4224.518 -4224.518] [0.0000], Avg: [-2183.087 -2183.087] (0.049)
Step: 30199, Reward: [-4868.551 -4868.551] [0.0000], Avg: [-2187.533 -2187.533] (0.048)
Step: 30249, Reward: [-2953.614 -2953.614] [0.0000], Avg: [-2188.8 -2188.8] (0.048)
Step: 30299, Reward: [-2332.584 -2332.584] [0.0000], Avg: [-2189.037 -2189.037] (0.048)
Step: 30349, Reward: [-2328.345 -2328.345] [0.0000], Avg: [-2189.266 -2189.266] (0.048)
Step: 30399, Reward: [-2411.319 -2411.319] [0.0000], Avg: [-2189.632 -2189.632] (0.047)
Step: 30449, Reward: [-4132.655 -4132.655] [0.0000], Avg: [-2192.822 -2192.822] (0.047)
Step: 30499, Reward: [-4139.91 -4139.91] [0.0000], Avg: [-2196.014 -2196.014] (0.047)
Step: 30549, Reward: [-2058.47 -2058.47] [0.0000], Avg: [-2195.789 -2195.789] (0.047)
Step: 30599, Reward: [-2670.27 -2670.27] [0.0000], Avg: [-2196.564 -2196.564] (0.047)
Step: 30649, Reward: [-2389.648 -2389.648] [0.0000], Avg: [-2196.879 -2196.879] (0.046)
Step: 30699, Reward: [-4594.154 -4594.154] [0.0000], Avg: [-2200.784 -2200.784] (0.046)
Step: 30749, Reward: [-1903.058 -1903.058] [0.0000], Avg: [-2200.3 -2200.3] (0.046)
Step: 30799, Reward: [-2780.094 -2780.094] [0.0000], Avg: [-2201.241 -2201.241] (0.046)
Step: 30849, Reward: [-2146.683 -2146.683] [0.0000], Avg: [-2201.152 -2201.152] (0.045)
Step: 30899, Reward: [-2670.858 -2670.858] [0.0000], Avg: [-2201.912 -2201.912] (0.045)
Step: 30949, Reward: [-3294.002 -3294.002] [0.0000], Avg: [-2203.677 -2203.677] (0.045)
Step: 30999, Reward: [-2297.975 -2297.975] [0.0000], Avg: [-2203.829 -2203.829] (0.045)
Step: 31049, Reward: [-3443.535 -3443.535] [0.0000], Avg: [-2205.825 -2205.825] (0.044)
Step: 31099, Reward: [-2265.668 -2265.668] [0.0000], Avg: [-2205.921 -2205.921] (0.044)
Step: 31149, Reward: [-2343.053 -2343.053] [0.0000], Avg: [-2206.141 -2206.141] (0.044)
Step: 31199, Reward: [-2345.145 -2345.145] [0.0000], Avg: [-2206.364 -2206.364] (0.044)
Step: 31249, Reward: [-2534.534 -2534.534] [0.0000], Avg: [-2206.889 -2206.889] (0.044)
Step: 31299, Reward: [-1412.899 -1412.899] [0.0000], Avg: [-2205.621 -2205.621] (0.043)
Step: 31349, Reward: [-3119.025 -3119.025] [0.0000], Avg: [-2207.078 -2207.078] (0.043)
Step: 31399, Reward: [-2800.121 -2800.121] [0.0000], Avg: [-2208.022 -2208.022] (0.043)
Step: 31449, Reward: [-2712.733 -2712.733] [0.0000], Avg: [-2208.824 -2208.824] (0.043)
Step: 31499, Reward: [-2072.061 -2072.061] [0.0000], Avg: [-2208.607 -2208.607] (0.043)
Step: 31549, Reward: [-3083.723 -3083.723] [0.0000], Avg: [-2209.994 -2209.994] (0.042)
Step: 31599, Reward: [-3678.705 -3678.705] [0.0000], Avg: [-2212.318 -2212.318] (0.042)
Step: 31649, Reward: [-3234.498 -3234.498] [0.0000], Avg: [-2213.933 -2213.933] (0.042)
Step: 31699, Reward: [-1588.091 -1588.091] [0.0000], Avg: [-2212.946 -2212.946] (0.042)
Step: 31749, Reward: [-3125.812 -3125.812] [0.0000], Avg: [-2214.383 -2214.383] (0.041)
Step: 31799, Reward: [-4481.55 -4481.55] [0.0000], Avg: [-2217.948 -2217.948] (0.041)
Step: 31849, Reward: [-3040.763 -3040.763] [0.0000], Avg: [-2219.24 -2219.24] (0.041)
Step: 31899, Reward: [-3929.087 -3929.087] [0.0000], Avg: [-2221.92 -2221.92] (0.041)
Step: 31949, Reward: [-3488.764 -3488.764] [0.0000], Avg: [-2223.902 -2223.902] (0.041)
Step: 31999, Reward: [-2945.785 -2945.785] [0.0000], Avg: [-2225.03 -2225.03] (0.040)
Step: 32049, Reward: [-2154.571 -2154.571] [0.0000], Avg: [-2224.92 -2224.92] (0.040)
Step: 32099, Reward: [-2309.605 -2309.605] [0.0000], Avg: [-2225.052 -2225.052] (0.040)
Step: 32149, Reward: [-3890.566 -3890.566] [0.0000], Avg: [-2227.643 -2227.643] (0.040)
Step: 32199, Reward: [-4107.222 -4107.222] [0.0000], Avg: [-2230.561 -2230.561] (0.040)
Step: 32249, Reward: [-1706.665 -1706.665] [0.0000], Avg: [-2229.749 -2229.749] (0.039)
Step: 32299, Reward: [-4417.792 -4417.792] [0.0000], Avg: [-2233.136 -2233.136] (0.039)
Step: 32349, Reward: [-2606.989 -2606.989] [0.0000], Avg: [-2233.714 -2233.714] (0.039)
Step: 32399, Reward: [-2737.122 -2737.122] [0.0000], Avg: [-2234.491 -2234.491] (0.039)
Step: 32449, Reward: [-1773.339 -1773.339] [0.0000], Avg: [-2233.78 -2233.78] (0.039)
Step: 32499, Reward: [-2381.357 -2381.357] [0.0000], Avg: [-2234.007 -2234.007] (0.038)
Step: 32549, Reward: [-2628.136 -2628.136] [0.0000], Avg: [-2234.613 -2234.613] (0.038)
Step: 32599, Reward: [-2739.764 -2739.764] [0.0000], Avg: [-2235.387 -2235.387] (0.038)
Step: 32649, Reward: [-2374.903 -2374.903] [0.0000], Avg: [-2235.601 -2235.601] (0.038)
Step: 32699, Reward: [-2192.238 -2192.238] [0.0000], Avg: [-2235.535 -2235.535] (0.038)
Step: 32749, Reward: [-2681.282 -2681.282] [0.0000], Avg: [-2236.215 -2236.215] (0.038)
Step: 32799, Reward: [-2776.305 -2776.305] [0.0000], Avg: [-2237.039 -2237.039] (0.037)
Step: 32849, Reward: [-2773.764 -2773.764] [0.0000], Avg: [-2237.855 -2237.855] (0.037)
Step: 32899, Reward: [-3313.222 -3313.222] [0.0000], Avg: [-2239.49 -2239.49] (0.037)
Step: 32949, Reward: [-2205.348 -2205.348] [0.0000], Avg: [-2239.438 -2239.438] (0.037)
Step: 32999, Reward: [-2908.641 -2908.641] [0.0000], Avg: [-2240.452 -2240.452] (0.037)
Step: 33049, Reward: [-2834.338 -2834.338] [0.0000], Avg: [-2241.35 -2241.35] (0.036)
Step: 33099, Reward: [-3139.191 -3139.191] [0.0000], Avg: [-2242.707 -2242.707] (0.036)
Step: 33149, Reward: [-2371.848 -2371.848] [0.0000], Avg: [-2242.901 -2242.901] (0.036)
Step: 33199, Reward: [-3401.381 -3401.381] [0.0000], Avg: [-2244.646 -2244.646] (0.036)
Step: 33249, Reward: [-3151. -3151.] [0.0000], Avg: [-2246.009 -2246.009] (0.036)
Step: 33299, Reward: [-2925.951 -2925.951] [0.0000], Avg: [-2247.03 -2247.03] (0.035)
Step: 33349, Reward: [-2176.751 -2176.751] [0.0000], Avg: [-2246.925 -2246.925] (0.035)
Step: 33399, Reward: [-3593.642 -3593.642] [0.0000], Avg: [-2248.941 -2248.941] (0.035)
Step: 33449, Reward: [-3087.341 -3087.341] [0.0000], Avg: [-2250.194 -2250.194] (0.035)
Step: 33499, Reward: [-3716.339 -3716.339] [0.0000], Avg: [-2252.382 -2252.382] (0.035)
Step: 33549, Reward: [-4181.76 -4181.76] [0.0000], Avg: [-2255.257 -2255.257] (0.035)
Step: 33599, Reward: [-2443.631 -2443.631] [0.0000], Avg: [-2255.538 -2255.538] (0.034)
Step: 33649, Reward: [-3710.6 -3710.6] [0.0000], Avg: [-2257.7 -2257.7] (0.034)
Step: 33699, Reward: [-2697.667 -2697.667] [0.0000], Avg: [-2258.353 -2258.353] (0.034)
Step: 33749, Reward: [-2651.537 -2651.537] [0.0000], Avg: [-2258.935 -2258.935] (0.034)
Step: 33799, Reward: [-1965.877 -1965.877] [0.0000], Avg: [-2258.502 -2258.502] (0.034)
Step: 33849, Reward: [-1974.049 -1974.049] [0.0000], Avg: [-2258.081 -2258.081] (0.034)
Step: 33899, Reward: [-2137.864 -2137.864] [0.0000], Avg: [-2257.904 -2257.904] (0.033)
Step: 33949, Reward: [-2130.618 -2130.618] [0.0000], Avg: [-2257.717 -2257.717] (0.033)
Step: 33999, Reward: [-3672.911 -3672.911] [0.0000], Avg: [-2259.798 -2259.798] (0.033)
Step: 34049, Reward: [-1664.185 -1664.185] [0.0000], Avg: [-2258.923 -2258.923] (0.033)
Step: 34099, Reward: [-3053.974 -3053.974] [0.0000], Avg: [-2260.089 -2260.089] (0.033)
Step: 34149, Reward: [-3093.614 -3093.614] [0.0000], Avg: [-2261.309 -2261.309] (0.033)
Step: 34199, Reward: [-2701.02 -2701.02] [0.0000], Avg: [-2261.952 -2261.952] (0.032)
Step: 34249, Reward: [-2685.61 -2685.61] [0.0000], Avg: [-2262.571 -2262.571] (0.032)
Step: 34299, Reward: [-2302.507 -2302.507] [0.0000], Avg: [-2262.629 -2262.629] (0.032)
Step: 34349, Reward: [-2819.423 -2819.423] [0.0000], Avg: [-2263.439 -2263.439] (0.032)
Step: 34399, Reward: [-3521.596 -3521.596] [0.0000], Avg: [-2265.268 -2265.268] (0.032)
Step: 34449, Reward: [-3045.379 -3045.379] [0.0000], Avg: [-2266.4 -2266.4] (0.032)
Step: 34499, Reward: [-3839.832 -3839.832] [0.0000], Avg: [-2268.681 -2268.681] (0.031)
Step: 34549, Reward: [-3694.085 -3694.085] [0.0000], Avg: [-2270.743 -2270.743] (0.031)
Step: 34599, Reward: [-3974.619 -3974.619] [0.0000], Avg: [-2273.206 -2273.206] (0.031)
Step: 34649, Reward: [-1823.959 -1823.959] [0.0000], Avg: [-2272.557 -2272.557] (0.031)
Step: 34699, Reward: [-1631.329 -1631.329] [0.0000], Avg: [-2271.634 -2271.634] (0.031)
Step: 34749, Reward: [-2685.564 -2685.564] [0.0000], Avg: [-2272.229 -2272.229] (0.031)
Step: 34799, Reward: [-3882.655 -3882.655] [0.0000], Avg: [-2274.543 -2274.543] (0.031)
Step: 34849, Reward: [-3889.793 -3889.793] [0.0000], Avg: [-2276.86 -2276.86] (0.030)
Step: 34899, Reward: [-3342.887 -3342.887] [0.0000], Avg: [-2278.388 -2278.388] (0.030)
Step: 34949, Reward: [-3094.586 -3094.586] [0.0000], Avg: [-2279.555 -2279.555] (0.030)
Step: 34999, Reward: [-2438.348 -2438.348] [0.0000], Avg: [-2279.782 -2279.782] (0.030)
Step: 35049, Reward: [-2022.099 -2022.099] [0.0000], Avg: [-2279.415 -2279.415] (0.030)
Step: 35099, Reward: [-3348.491 -3348.491] [0.0000], Avg: [-2280.937 -2280.937] (0.030)
Step: 35149, Reward: [-2851.602 -2851.602] [0.0000], Avg: [-2281.749 -2281.749] (0.029)
Step: 35199, Reward: [-2078.726 -2078.726] [0.0000], Avg: [-2281.461 -2281.461] (0.029)
Step: 35249, Reward: [-2639.158 -2639.158] [0.0000], Avg: [-2281.968 -2281.968] (0.029)
Step: 35299, Reward: [-2356.379 -2356.379] [0.0000], Avg: [-2282.074 -2282.074] (0.029)
Step: 35349, Reward: [-2473.856 -2473.856] [0.0000], Avg: [-2282.345 -2282.345] (0.029)
Step: 35399, Reward: [-2828.716 -2828.716] [0.0000], Avg: [-2283.117 -2283.117] (0.029)
Step: 35449, Reward: [-3943.696 -3943.696] [0.0000], Avg: [-2285.459 -2285.459] (0.029)
Step: 35499, Reward: [-51.204 -51.204] [0.0000], Avg: [-2282.312 -2282.312] (0.028)
Step: 35549, Reward: [-3087.012 -3087.012] [0.0000], Avg: [-2283.444 -2283.444] (0.028)
Step: 35599, Reward: [-3213.648 -3213.648] [0.0000], Avg: [-2284.75 -2284.75] (0.028)
Step: 35649, Reward: [-2516.094 -2516.094] [0.0000], Avg: [-2285.075 -2285.075] (0.028)
Step: 35699, Reward: [-3616.232 -3616.232] [0.0000], Avg: [-2286.939 -2286.939] (0.028)
Step: 35749, Reward: [-3280.405 -3280.405] [0.0000], Avg: [-2288.328 -2288.328] (0.028)
Step: 35799, Reward: [-2628.028 -2628.028] [0.0000], Avg: [-2288.803 -2288.803] (0.028)
Step: 35849, Reward: [-4303.796 -4303.796] [0.0000], Avg: [-2291.613 -2291.613] (0.027)
Step: 35899, Reward: [-2729.795 -2729.795] [0.0000], Avg: [-2292.223 -2292.223] (0.027)
Step: 35949, Reward: [-2891.212 -2891.212] [0.0000], Avg: [-2293.057 -2293.057] (0.027)
Step: 35999, Reward: [-3453.576 -3453.576] [0.0000], Avg: [-2294.668 -2294.668] (0.027)
Step: 36049, Reward: [-2033.866 -2033.866] [0.0000], Avg: [-2294.307 -2294.307] (0.027)
Step: 36099, Reward: [-3109.06 -3109.06] [0.0000], Avg: [-2295.435 -2295.435] (0.027)
Step: 36149, Reward: [-2503.628 -2503.628] [0.0000], Avg: [-2295.723 -2295.723] (0.027)
Step: 36199, Reward: [-2051.823 -2051.823] [0.0000], Avg: [-2295.386 -2295.386] (0.027)
Step: 36249, Reward: [-1562.559 -1562.559] [0.0000], Avg: [-2294.375 -2294.375] (0.026)
Step: 36299, Reward: [-3091.743 -3091.743] [0.0000], Avg: [-2295.474 -2295.474] (0.026)
Step: 36349, Reward: [-3873.304 -3873.304] [0.0000], Avg: [-2297.644 -2297.644] (0.026)
Step: 36399, Reward: [-2141.46 -2141.46] [0.0000], Avg: [-2297.429 -2297.429] (0.026)
Step: 36449, Reward: [-2703.653 -2703.653] [0.0000], Avg: [-2297.987 -2297.987] (0.026)
Step: 36499, Reward: [-3285.274 -3285.274] [0.0000], Avg: [-2299.339 -2299.339] (0.026)
Step: 36549, Reward: [-3283.062 -3283.062] [0.0000], Avg: [-2300.685 -2300.685] (0.026)
Step: 36599, Reward: [-2153.971 -2153.971] [0.0000], Avg: [-2300.484 -2300.484] (0.025)
Step: 36649, Reward: [-3262.459 -3262.459] [0.0000], Avg: [-2301.797 -2301.797] (0.025)
Step: 36699, Reward: [-3686.474 -3686.474] [0.0000], Avg: [-2303.683 -2303.683] (0.025)
Step: 36749, Reward: [-3472.82 -3472.82] [0.0000], Avg: [-2305.274 -2305.274] (0.025)
Step: 36799, Reward: [-4302.164 -4302.164] [0.0000], Avg: [-2307.987 -2307.987] (0.025)
Step: 36849, Reward: [-1902.861 -1902.861] [0.0000], Avg: [-2307.437 -2307.437] (0.025)
Step: 36899, Reward: [-3187.514 -3187.514] [0.0000], Avg: [-2308.63 -2308.63] (0.025)
Step: 36949, Reward: [-3386.221 -3386.221] [0.0000], Avg: [-2310.088 -2310.088] (0.025)
Step: 36999, Reward: [-2725.908 -2725.908] [0.0000], Avg: [-2310.65 -2310.65] (0.024)
Step: 37049, Reward: [-2130.814 -2130.814] [0.0000], Avg: [-2310.407 -2310.407] (0.024)
Step: 37099, Reward: [-3690.509 -3690.509] [0.0000], Avg: [-2312.267 -2312.267] (0.024)
Step: 37149, Reward: [-2986.384 -2986.384] [0.0000], Avg: [-2313.175 -2313.175] (0.024)
Step: 37199, Reward: [-2811.658 -2811.658] [0.0000], Avg: [-2313.845 -2313.845] (0.024)
Step: 37249, Reward: [-3079.722 -3079.722] [0.0000], Avg: [-2314.873 -2314.873] (0.024)
Step: 37299, Reward: [-3116.803 -3116.803] [0.0000], Avg: [-2315.948 -2315.948] (0.024)
Step: 37349, Reward: [-3428.583 -3428.583] [0.0000], Avg: [-2317.437 -2317.437] (0.024)
Step: 37399, Reward: [-2595.226 -2595.226] [0.0000], Avg: [-2317.808 -2317.808] (0.024)
Step: 37449, Reward: [-3744.705 -3744.705] [0.0000], Avg: [-2319.714 -2319.714] (0.023)
Step: 37499, Reward: [-2004.172 -2004.172] [0.0000], Avg: [-2319.293 -2319.293] (0.023)
Step: 37549, Reward: [-1845.234 -1845.234] [0.0000], Avg: [-2318.662 -2318.662] (0.023)
Step: 37599, Reward: [-4332.897 -4332.897] [0.0000], Avg: [-2321.34 -2321.34] (0.023)
Step: 37649, Reward: [-3013.405 -3013.405] [0.0000], Avg: [-2322.259 -2322.259] (0.023)
Step: 37699, Reward: [-4710.319 -4710.319] [0.0000], Avg: [-2325.426 -2325.426] (0.023)
Step: 37749, Reward: [-4309.1 -4309.1] [0.0000], Avg: [-2328.054 -2328.054] (0.023)
Step: 37799, Reward: [-2133.893 -2133.893] [0.0000], Avg: [-2327.797 -2327.797] (0.023)
Step: 37849, Reward: [-2444.235 -2444.235] [0.0000], Avg: [-2327.951 -2327.951] (0.022)
Step: 37899, Reward: [-4450.498 -4450.498] [0.0000], Avg: [-2330.751 -2330.751] (0.022)
Step: 37949, Reward: [-3182.403 -3182.403] [0.0000], Avg: [-2331.873 -2331.873] (0.022)
Step: 37999, Reward: [-3098.663 -3098.663] [0.0000], Avg: [-2332.882 -2332.882] (0.022)
Step: 38049, Reward: [-4644.48 -4644.48] [0.0000], Avg: [-2335.92 -2335.92] (0.022)
Step: 38099, Reward: [-3493.663 -3493.663] [0.0000], Avg: [-2337.439 -2337.439] (0.022)
Step: 38149, Reward: [-2999.744 -2999.744] [0.0000], Avg: [-2338.307 -2338.307] (0.022)
Step: 38199, Reward: [-4073.679 -4073.679] [0.0000], Avg: [-2340.578 -2340.578] (0.022)
Step: 38249, Reward: [-3177.831 -3177.831] [0.0000], Avg: [-2341.673 -2341.673] (0.022)
Step: 38299, Reward: [-3324.779 -3324.779] [0.0000], Avg: [-2342.956 -2342.956] (0.022)
Step: 38349, Reward: [-3213.116 -3213.116] [0.0000], Avg: [-2344.091 -2344.091] (0.021)
Step: 38399, Reward: [-4107.837 -4107.837] [0.0000], Avg: [-2346.387 -2346.387] (0.021)
Step: 38449, Reward: [-2229.217 -2229.217] [0.0000], Avg: [-2346.235 -2346.235] (0.021)
Step: 38499, Reward: [-2447.261 -2447.261] [0.0000], Avg: [-2346.366 -2346.366] (0.021)
Step: 38549, Reward: [-4775.974 -4775.974] [0.0000], Avg: [-2349.517 -2349.517] (0.021)
Step: 38599, Reward: [-3608.349 -3608.349] [0.0000], Avg: [-2351.148 -2351.148] (0.021)
Step: 38649, Reward: [-3646.581 -3646.581] [0.0000], Avg: [-2352.824 -2352.824] (0.021)
Step: 38699, Reward: [-2562.989 -2562.989] [0.0000], Avg: [-2353.095 -2353.095] (0.021)
Step: 38749, Reward: [-1982.761 -1982.761] [0.0000], Avg: [-2352.617 -2352.617] (0.021)
Step: 38799, Reward: [-2198.323 -2198.323] [0.0000], Avg: [-2352.419 -2352.419] (0.020)
Step: 38849, Reward: [-2519.157 -2519.157] [0.0000], Avg: [-2352.633 -2352.633] (0.020)
Step: 38899, Reward: [-2338.272 -2338.272] [0.0000], Avg: [-2352.615 -2352.615] (0.020)
Step: 38949, Reward: [-3138.501 -3138.501] [0.0000], Avg: [-2353.624 -2353.624] (0.020)
Step: 38999, Reward: [-2618.303 -2618.303] [0.0000], Avg: [-2353.963 -2353.963] (0.020)
Step: 39049, Reward: [-2338.598 -2338.598] [0.0000], Avg: [-2353.943 -2353.943] (0.020)
Step: 39099, Reward: [-4290.986 -4290.986] [0.0000], Avg: [-2356.42 -2356.42] (0.020)
Step: 39149, Reward: [-1825.218 -1825.218] [0.0000], Avg: [-2355.742 -2355.742] (0.020)
Step: 39199, Reward: [-2316.746 -2316.746] [0.0000], Avg: [-2355.692 -2355.692] (0.020)
Step: 39249, Reward: [-4823.683 -4823.683] [0.0000], Avg: [-2358.836 -2358.836] (0.020)
Step: 39299, Reward: [-2941.022 -2941.022] [0.0000], Avg: [-2359.577 -2359.577] (0.020)
Step: 39349, Reward: [-3667.53 -3667.53] [0.0000], Avg: [-2361.239 -2361.239] (0.020)
Step: 39399, Reward: [-4102.068 -4102.068] [0.0000], Avg: [-2363.448 -2363.448] (0.020)
Step: 39449, Reward: [-2974.37 -2974.37] [0.0000], Avg: [-2364.222 -2364.222] (0.020)
Step: 39499, Reward: [-1566.513 -1566.513] [0.0000], Avg: [-2363.212 -2363.212] (0.020)
Step: 39549, Reward: [-3785.612 -3785.612] [0.0000], Avg: [-2365.011 -2365.011] (0.020)
Step: 39599, Reward: [-3597.568 -3597.568] [0.0000], Avg: [-2366.567 -2366.567] (0.020)
Step: 39649, Reward: [-2512.956 -2512.956] [0.0000], Avg: [-2366.752 -2366.752] (0.020)
Step: 39699, Reward: [-3186.735 -3186.735] [0.0000], Avg: [-2367.784 -2367.784] (0.020)
Step: 39749, Reward: [-2675.302 -2675.302] [0.0000], Avg: [-2368.171 -2368.171] (0.020)
Step: 39799, Reward: [-2993.312 -2993.312] [0.0000], Avg: [-2368.956 -2368.956] (0.020)
Step: 39849, Reward: [-2543.862 -2543.862] [0.0000], Avg: [-2369.176 -2369.176] (0.020)
Step: 39899, Reward: [-1853.841 -1853.841] [0.0000], Avg: [-2368.53 -2368.53] (0.020)
Step: 39949, Reward: [-3080.599 -3080.599] [0.0000], Avg: [-2369.421 -2369.421] (0.020)
Step: 39999, Reward: [-2916.354 -2916.354] [0.0000], Avg: [-2370.105 -2370.105] (0.020)
Step: 40049, Reward: [-1757.171 -1757.171] [0.0000], Avg: [-2369.34 -2369.34] (0.020)
Step: 40099, Reward: [-4706.998 -4706.998] [0.0000], Avg: [-2372.255 -2372.255] (0.020)
Step: 40149, Reward: [-3415.448 -3415.448] [0.0000], Avg: [-2373.554 -2373.554] (0.020)
Step: 40199, Reward: [-3919.04 -3919.04] [0.0000], Avg: [-2375.476 -2375.476] (0.020)
Step: 40249, Reward: [-3360.679 -3360.679] [0.0000], Avg: [-2376.7 -2376.7] (0.020)
Step: 40299, Reward: [-2738.813 -2738.813] [0.0000], Avg: [-2377.149 -2377.149] (0.020)
Step: 40349, Reward: [-2774.722 -2774.722] [0.0000], Avg: [-2377.642 -2377.642] (0.020)
Step: 40399, Reward: [-2126.215 -2126.215] [0.0000], Avg: [-2377.33 -2377.33] (0.020)
Step: 40449, Reward: [-4827.245 -4827.245] [0.0000], Avg: [-2380.359 -2380.359] (0.020)
Step: 40499, Reward: [-4261.393 -4261.393] [0.0000], Avg: [-2382.681 -2382.681] (0.020)
Step: 40549, Reward: [-3062.221 -3062.221] [0.0000], Avg: [-2383.519 -2383.519] (0.020)
Step: 40599, Reward: [-2272.54 -2272.54] [0.0000], Avg: [-2383.382 -2383.382] (0.020)
Step: 40649, Reward: [-2280.642 -2280.642] [0.0000], Avg: [-2383.256 -2383.256] (0.020)
Step: 40699, Reward: [-2254.298 -2254.298] [0.0000], Avg: [-2383.098 -2383.098] (0.020)
Step: 40749, Reward: [-3079.064 -3079.064] [0.0000], Avg: [-2383.951 -2383.951] (0.020)
Step: 40799, Reward: [-3300.738 -3300.738] [0.0000], Avg: [-2385.075 -2385.075] (0.020)
Step: 40849, Reward: [-3123.259 -3123.259] [0.0000], Avg: [-2385.979 -2385.979] (0.020)
Step: 40899, Reward: [-2518.507 -2518.507] [0.0000], Avg: [-2386.141 -2386.141] (0.020)
Step: 40949, Reward: [-3757.937 -3757.937] [0.0000], Avg: [-2387.815 -2387.815] (0.020)
Step: 40999, Reward: [-1929.677 -1929.677] [0.0000], Avg: [-2387.257 -2387.257] (0.020)
Step: 41049, Reward: [-3131.16 -3131.16] [0.0000], Avg: [-2388.163 -2388.163] (0.020)
Step: 41099, Reward: [-3668.563 -3668.563] [0.0000], Avg: [-2389.721 -2389.721] (0.020)
Step: 41149, Reward: [-4612.8 -4612.8] [0.0000], Avg: [-2392.422 -2392.422] (0.020)
Step: 41199, Reward: [-2595.405 -2595.405] [0.0000], Avg: [-2392.668 -2392.668] (0.020)
Step: 41249, Reward: [-3310.306 -3310.306] [0.0000], Avg: [-2393.78 -2393.78] (0.020)
Step: 41299, Reward: [-3787.38 -3787.38] [0.0000], Avg: [-2395.468 -2395.468] (0.020)
Step: 41349, Reward: [-3951.833 -3951.833] [0.0000], Avg: [-2397.349 -2397.349] (0.020)
Step: 41399, Reward: [-1643.785 -1643.785] [0.0000], Avg: [-2396.439 -2396.439] (0.020)
Step: 41449, Reward: [-4452.666 -4452.666] [0.0000], Avg: [-2398.92 -2398.92] (0.020)
Step: 41499, Reward: [-4554.368 -4554.368] [0.0000], Avg: [-2401.517 -2401.517] (0.020)
Step: 41549, Reward: [-2898.587 -2898.587] [0.0000], Avg: [-2402.115 -2402.115] (0.020)
Step: 41599, Reward: [-3369.764 -3369.764] [0.0000], Avg: [-2403.278 -2403.278] (0.020)
Step: 41649, Reward: [-3057.86 -3057.86] [0.0000], Avg: [-2404.064 -2404.064] (0.020)
Step: 41699, Reward: [-3475.316 -3475.316] [0.0000], Avg: [-2405.348 -2405.348] (0.020)
Step: 41749, Reward: [-2693.093 -2693.093] [0.0000], Avg: [-2405.693 -2405.693] (0.020)
Step: 41799, Reward: [-3702.501 -3702.501] [0.0000], Avg: [-2407.244 -2407.244] (0.020)
Step: 41849, Reward: [-2897.536 -2897.536] [0.0000], Avg: [-2407.83 -2407.83] (0.020)
Step: 41899, Reward: [-2359.824 -2359.824] [0.0000], Avg: [-2407.772 -2407.772] (0.020)
Step: 41949, Reward: [-2727.21 -2727.21] [0.0000], Avg: [-2408.153 -2408.153] (0.020)
Step: 41999, Reward: [-4486.794 -4486.794] [0.0000], Avg: [-2410.628 -2410.628] (0.020)
Step: 42049, Reward: [-2425.694 -2425.694] [0.0000], Avg: [-2410.646 -2410.646] (0.020)
Step: 42099, Reward: [-4789.818 -4789.818] [0.0000], Avg: [-2413.471 -2413.471] (0.020)
Step: 42149, Reward: [-3686.346 -3686.346] [0.0000], Avg: [-2414.981 -2414.981] (0.020)
Step: 42199, Reward: [-3539.021 -3539.021] [0.0000], Avg: [-2416.313 -2416.313] (0.020)
Step: 42249, Reward: [-3092.46 -3092.46] [0.0000], Avg: [-2417.113 -2417.113] (0.020)
Step: 42299, Reward: [-2781.649 -2781.649] [0.0000], Avg: [-2417.544 -2417.544] (0.020)
Step: 42349, Reward: [-2025.313 -2025.313] [0.0000], Avg: [-2417.081 -2417.081] (0.020)
Step: 42399, Reward: [-2200.984 -2200.984] [0.0000], Avg: [-2416.826 -2416.826] (0.020)
Step: 42449, Reward: [-2446.21 -2446.21] [0.0000], Avg: [-2416.861 -2416.861] (0.020)
Step: 42499, Reward: [-2660.831 -2660.831] [0.0000], Avg: [-2417.148 -2417.148] (0.020)
Step: 42549, Reward: [-2998.353 -2998.353] [0.0000], Avg: [-2417.831 -2417.831] (0.020)
Step: 42599, Reward: [-2817.161 -2817.161] [0.0000], Avg: [-2418.299 -2418.299] (0.020)
Step: 42649, Reward: [-3419.161 -3419.161] [0.0000], Avg: [-2419.473 -2419.473] (0.020)
Step: 42699, Reward: [-3541.128 -3541.128] [0.0000], Avg: [-2420.786 -2420.786] (0.020)
Step: 42749, Reward: [-4252.861 -4252.861] [0.0000], Avg: [-2422.929 -2422.929] (0.020)
Step: 42799, Reward: [-4137.126 -4137.126] [0.0000], Avg: [-2424.932 -2424.932] (0.020)
Step: 42849, Reward: [-2731.047 -2731.047] [0.0000], Avg: [-2425.289 -2425.289] (0.020)
Step: 42899, Reward: [-3256.535 -3256.535] [0.0000], Avg: [-2426.258 -2426.258] (0.020)
Step: 42949, Reward: [-3545.739 -3545.739] [0.0000], Avg: [-2427.561 -2427.561] (0.020)
Step: 42999, Reward: [-4128.409 -4128.409] [0.0000], Avg: [-2429.539 -2429.539] (0.020)
Step: 43049, Reward: [-2917.971 -2917.971] [0.0000], Avg: [-2430.106 -2430.106] (0.020)
Step: 43099, Reward: [-3286.368 -3286.368] [0.0000], Avg: [-2431.099 -2431.099] (0.020)
Step: 43149, Reward: [-2666.766 -2666.766] [0.0000], Avg: [-2431.372 -2431.372] (0.020)
Step: 43199, Reward: [-2792.443 -2792.443] [0.0000], Avg: [-2431.79 -2431.79] (0.020)
Step: 43249, Reward: [-1755.973 -1755.973] [0.0000], Avg: [-2431.009 -2431.009] (0.020)
Step: 43299, Reward: [-2062.247 -2062.247] [0.0000], Avg: [-2430.583 -2430.583] (0.020)
Step: 43349, Reward: [-3175.879 -3175.879] [0.0000], Avg: [-2431.443 -2431.443] (0.020)
Step: 43399, Reward: [-3088.961 -3088.961] [0.0000], Avg: [-2432.2 -2432.2] (0.020)
Step: 43449, Reward: [-2361.987 -2361.987] [0.0000], Avg: [-2432.119 -2432.119] (0.020)
Step: 43499, Reward: [-2815.847 -2815.847] [0.0000], Avg: [-2432.56 -2432.56] (0.020)
Step: 43549, Reward: [-2668.592 -2668.592] [0.0000], Avg: [-2432.831 -2432.831] (0.020)
Step: 43599, Reward: [-2069.684 -2069.684] [0.0000], Avg: [-2432.415 -2432.415] (0.020)
Step: 43649, Reward: [-3577.851 -3577.851] [0.0000], Avg: [-2433.727 -2433.727] (0.020)
Step: 43699, Reward: [-2490.459 -2490.459] [0.0000], Avg: [-2433.792 -2433.792] (0.020)
Step: 43749, Reward: [-3268.821 -3268.821] [0.0000], Avg: [-2434.746 -2434.746] (0.020)
Step: 43799, Reward: [-3985.734 -3985.734] [0.0000], Avg: [-2436.517 -2436.517] (0.020)
Step: 43849, Reward: [-2193.122 -2193.122] [0.0000], Avg: [-2436.239 -2436.239] (0.020)
Step: 43899, Reward: [-3362.971 -3362.971] [0.0000], Avg: [-2437.295 -2437.295] (0.020)
Step: 43949, Reward: [-2677.833 -2677.833] [0.0000], Avg: [-2437.568 -2437.568] (0.020)
Step: 43999, Reward: [-1743.724 -1743.724] [0.0000], Avg: [-2436.78 -2436.78] (0.020)
Step: 44049, Reward: [-2913.749 -2913.749] [0.0000], Avg: [-2437.321 -2437.321] (0.020)
Step: 44099, Reward: [-2746.47 -2746.47] [0.0000], Avg: [-2437.672 -2437.672] (0.020)
Step: 44149, Reward: [-2395.374 -2395.374] [0.0000], Avg: [-2437.624 -2437.624] (0.020)
Step: 44199, Reward: [-2249.182 -2249.182] [0.0000], Avg: [-2437.411 -2437.411] (0.020)
Step: 44249, Reward: [-3132.202 -3132.202] [0.0000], Avg: [-2438.196 -2438.196] (0.020)
Step: 44299, Reward: [-3910.653 -3910.653] [0.0000], Avg: [-2439.858 -2439.858] (0.020)
Step: 44349, Reward: [-2903.961 -2903.961] [0.0000], Avg: [-2440.381 -2440.381] (0.020)
Step: 44399, Reward: [-2546.547 -2546.547] [0.0000], Avg: [-2440.501 -2440.501] (0.020)
Step: 44449, Reward: [-1997.877 -1997.877] [0.0000], Avg: [-2440.003 -2440.003] (0.020)
Step: 44499, Reward: [-3318.18 -3318.18] [0.0000], Avg: [-2440.989 -2440.989] (0.020)
Step: 44549, Reward: [-3461.592 -3461.592] [0.0000], Avg: [-2442.135 -2442.135] (0.020)
Step: 44599, Reward: [-2599.305 -2599.305] [0.0000], Avg: [-2442.311 -2442.311] (0.020)
Step: 44649, Reward: [-2751.06 -2751.06] [0.0000], Avg: [-2442.657 -2442.657] (0.020)
Step: 44699, Reward: [-2956.47 -2956.47] [0.0000], Avg: [-2443.232 -2443.232] (0.020)
Step: 44749, Reward: [-2447.236 -2447.236] [0.0000], Avg: [-2443.236 -2443.236] (0.020)
Step: 44799, Reward: [-1562.16 -1562.16] [0.0000], Avg: [-2442.253 -2442.253] (0.020)
Step: 44849, Reward: [-1915.853 -1915.853] [0.0000], Avg: [-2441.666 -2441.666] (0.020)
Step: 44899, Reward: [-2623.136 -2623.136] [0.0000], Avg: [-2441.868 -2441.868] (0.020)
Step: 44949, Reward: [-2482.894 -2482.894] [0.0000], Avg: [-2441.914 -2441.914] (0.020)
Step: 44999, Reward: [-2387.877 -2387.877] [0.0000], Avg: [-2441.854 -2441.854] (0.020)
Step: 45049, Reward: [-3780.442 -3780.442] [0.0000], Avg: [-2443.339 -2443.339] (0.020)
Step: 45099, Reward: [-4704.238 -4704.238] [0.0000], Avg: [-2445.846 -2445.846] (0.020)
Step: 45149, Reward: [-3240.542 -3240.542] [0.0000], Avg: [-2446.726 -2446.726] (0.020)
Step: 45199, Reward: [-2648.419 -2648.419] [0.0000], Avg: [-2446.949 -2446.949] (0.020)
Step: 45249, Reward: [-2622.769 -2622.769] [0.0000], Avg: [-2447.143 -2447.143] (0.020)
Step: 45299, Reward: [-3276.293 -3276.293] [0.0000], Avg: [-2448.058 -2448.058] (0.020)
Step: 45349, Reward: [-4628.08 -4628.08] [0.0000], Avg: [-2450.462 -2450.462] (0.020)
Step: 45399, Reward: [-2298.137 -2298.137] [0.0000], Avg: [-2450.294 -2450.294] (0.020)
Step: 45449, Reward: [-2978.712 -2978.712] [0.0000], Avg: [-2450.875 -2450.875] (0.020)
Step: 45499, Reward: [-2500.496 -2500.496] [0.0000], Avg: [-2450.93 -2450.93] (0.020)
Step: 45549, Reward: [-3073.033 -3073.033] [0.0000], Avg: [-2451.613 -2451.613] (0.020)
Step: 45599, Reward: [-2386.926 -2386.926] [0.0000], Avg: [-2451.542 -2451.542] (0.020)
Step: 45649, Reward: [-3548.43 -3548.43] [0.0000], Avg: [-2452.743 -2452.743] (0.020)
Step: 45699, Reward: [-3745.699 -3745.699] [0.0000], Avg: [-2454.158 -2454.158] (0.020)
Step: 45749, Reward: [-1787.686 -1787.686] [0.0000], Avg: [-2453.43 -2453.43] (0.020)
Step: 45799, Reward: [-3387.106 -3387.106] [0.0000], Avg: [-2454.449 -2454.449] (0.020)
Step: 45849, Reward: [-2356.034 -2356.034] [0.0000], Avg: [-2454.342 -2454.342] (0.020)
Step: 45899, Reward: [-4703.442 -4703.442] [0.0000], Avg: [-2456.792 -2456.792] (0.020)
Step: 45949, Reward: [-3140.087 -3140.087] [0.0000], Avg: [-2457.535 -2457.535] (0.020)
Step: 45999, Reward: [-2415.022 -2415.022] [0.0000], Avg: [-2457.489 -2457.489] (0.020)
Step: 46049, Reward: [-3888.692 -3888.692] [0.0000], Avg: [-2459.043 -2459.043] (0.020)
Step: 46099, Reward: [-2149.513 -2149.513] [0.0000], Avg: [-2458.707 -2458.707] (0.020)
Step: 46149, Reward: [-3246.408 -3246.408] [0.0000], Avg: [-2459.561 -2459.561] (0.020)
Step: 46199, Reward: [-2852.24 -2852.24] [0.0000], Avg: [-2459.986 -2459.986] (0.020)
Step: 46249, Reward: [-3527.597 -3527.597] [0.0000], Avg: [-2461.14 -2461.14] (0.020)
Step: 46299, Reward: [-1780.907 -1780.907] [0.0000], Avg: [-2460.405 -2460.405] (0.020)
Step: 46349, Reward: [-2768.15 -2768.15] [0.0000], Avg: [-2460.737 -2460.737] (0.020)
Step: 46399, Reward: [-4322.03 -4322.03] [0.0000], Avg: [-2462.743 -2462.743] (0.020)
Step: 46449, Reward: [-3552.837 -3552.837] [0.0000], Avg: [-2463.916 -2463.916] (0.020)
Step: 46499, Reward: [-1628.047 -1628.047] [0.0000], Avg: [-2463.017 -2463.017] (0.020)
Step: 46549, Reward: [-4642.103 -4642.103] [0.0000], Avg: [-2465.358 -2465.358] (0.020)
Step: 46599, Reward: [-3479.741 -3479.741] [0.0000], Avg: [-2466.446 -2466.446] (0.020)
Step: 46649, Reward: [-3511.004 -3511.004] [0.0000], Avg: [-2467.566 -2467.566] (0.020)
Step: 46699, Reward: [-3763.795 -3763.795] [0.0000], Avg: [-2468.954 -2468.954] (0.020)
Step: 46749, Reward: [-2727.028 -2727.028] [0.0000], Avg: [-2469.23 -2469.23] (0.020)
Step: 46799, Reward: [-2993.423 -2993.423] [0.0000], Avg: [-2469.79 -2469.79] (0.020)
Step: 46849, Reward: [-3214.665 -3214.665] [0.0000], Avg: [-2470.585 -2470.585] (0.020)
Step: 46899, Reward: [-2433.755 -2433.755] [0.0000], Avg: [-2470.546 -2470.546] (0.020)
Step: 46949, Reward: [-2019.407 -2019.407] [0.0000], Avg: [-2470.065 -2470.065] (0.020)
Step: 46999, Reward: [-2104.531 -2104.531] [0.0000], Avg: [-2469.676 -2469.676] (0.020)
Step: 47049, Reward: [-1978.596 -1978.596] [0.0000], Avg: [-2469.154 -2469.154] (0.020)
Step: 47099, Reward: [-2943.032 -2943.032] [0.0000], Avg: [-2469.657 -2469.657] (0.020)
Step: 47149, Reward: [-3111.201 -3111.201] [0.0000], Avg: [-2470.338 -2470.338] (0.020)
Step: 47199, Reward: [-3255.929 -3255.929] [0.0000], Avg: [-2471.17 -2471.17] (0.020)
Step: 47249, Reward: [-3213.816 -3213.816] [0.0000], Avg: [-2471.956 -2471.956] (0.020)
Step: 47299, Reward: [-2586.176 -2586.176] [0.0000], Avg: [-2472.077 -2472.077] (0.020)
Step: 47349, Reward: [-3006.944 -3006.944] [0.0000], Avg: [-2472.641 -2472.641] (0.020)
Step: 47399, Reward: [-3052.829 -3052.829] [0.0000], Avg: [-2473.253 -2473.253] (0.020)
Step: 47449, Reward: [-2989.244 -2989.244] [0.0000], Avg: [-2473.797 -2473.797] (0.020)
Step: 47499, Reward: [-3292.972 -3292.972] [0.0000], Avg: [-2474.659 -2474.659] (0.020)
Step: 47549, Reward: [-2417.725 -2417.725] [0.0000], Avg: [-2474.599 -2474.599] (0.020)
Step: 47599, Reward: [-2560.009 -2560.009] [0.0000], Avg: [-2474.689 -2474.689] (0.020)
Step: 47649, Reward: [-3003.238 -3003.238] [0.0000], Avg: [-2475.244 -2475.244] (0.020)
Step: 47699, Reward: [-3639.587 -3639.587] [0.0000], Avg: [-2476.464 -2476.464] (0.020)
Step: 47749, Reward: [-3825.601 -3825.601] [0.0000], Avg: [-2477.877 -2477.877] (0.020)
Step: 47799, Reward: [-4287.44 -4287.44] [0.0000], Avg: [-2479.77 -2479.77] (0.020)
Step: 47849, Reward: [-2136.862 -2136.862] [0.0000], Avg: [-2479.412 -2479.412] (0.020)
Step: 47899, Reward: [-3132.089 -3132.089] [0.0000], Avg: [-2480.093 -2480.093] (0.020)
Step: 47949, Reward: [-2010.007 -2010.007] [0.0000], Avg: [-2479.603 -2479.603] (0.020)
Step: 47999, Reward: [-3193.55 -3193.55] [0.0000], Avg: [-2480.346 -2480.346] (0.020)
Step: 48049, Reward: [-3939.453 -3939.453] [0.0000], Avg: [-2481.865 -2481.865] (0.020)
Step: 48099, Reward: [-3903.328 -3903.328] [0.0000], Avg: [-2483.342 -2483.342] (0.020)
Step: 48149, Reward: [-4605.48 -4605.48] [0.0000], Avg: [-2485.546 -2485.546] (0.020)
Step: 48199, Reward: [-3311.523 -3311.523] [0.0000], Avg: [-2486.403 -2486.403] (0.020)
Step: 48249, Reward: [-2616.979 -2616.979] [0.0000], Avg: [-2486.538 -2486.538] (0.020)
Step: 48299, Reward: [-3574.806 -3574.806] [0.0000], Avg: [-2487.665 -2487.665] (0.020)
Step: 48349, Reward: [-3777.404 -3777.404] [0.0000], Avg: [-2488.998 -2488.998] (0.020)
Step: 48399, Reward: [-2935.995 -2935.995] [0.0000], Avg: [-2489.46 -2489.46] (0.020)
Step: 48449, Reward: [-3830.67 -3830.67] [0.0000], Avg: [-2490.844 -2490.844] (0.020)
Step: 48499, Reward: [-3358.546 -3358.546] [0.0000], Avg: [-2491.739 -2491.739] (0.020)
Step: 48549, Reward: [-2714.835 -2714.835] [0.0000], Avg: [-2491.969 -2491.969] (0.020)
Step: 48599, Reward: [-2825.591 -2825.591] [0.0000], Avg: [-2492.312 -2492.312] (0.020)
Step: 48649, Reward: [-3847.593 -3847.593] [0.0000], Avg: [-2493.705 -2493.705] (0.020)
Step: 48699, Reward: [-3608.174 -3608.174] [0.0000], Avg: [-2494.849 -2494.849] (0.020)
Step: 48749, Reward: [-3882.476 -3882.476] [0.0000], Avg: [-2496.272 -2496.272] (0.020)
Step: 48799, Reward: [-1907.084 -1907.084] [0.0000], Avg: [-2495.668 -2495.668] (0.020)
Step: 48849, Reward: [-2130.535 -2130.535] [0.0000], Avg: [-2495.295 -2495.295] (0.020)
Step: 48899, Reward: [-4369.621 -4369.621] [0.0000], Avg: [-2497.211 -2497.211] (0.020)
Step: 48949, Reward: [-2509.032 -2509.032] [0.0000], Avg: [-2497.223 -2497.223] (0.020)
Step: 48999, Reward: [-2915.523 -2915.523] [0.0000], Avg: [-2497.65 -2497.65] (0.020)
Step: 49049, Reward: [-3489.576 -3489.576] [0.0000], Avg: [-2498.661 -2498.661] (0.020)
Step: 49099, Reward: [-3251.918 -3251.918] [0.0000], Avg: [-2499.428 -2499.428] (0.020)
Step: 49149, Reward: [-2330.261 -2330.261] [0.0000], Avg: [-2499.256 -2499.256] (0.020)
Step: 49199, Reward: [-2191.447 -2191.447] [0.0000], Avg: [-2498.943 -2498.943] (0.020)
Step: 49249, Reward: [-1868.562 -1868.562] [0.0000], Avg: [-2498.303 -2498.303] (0.020)
Step: 49299, Reward: [-2295.546 -2295.546] [0.0000], Avg: [-2498.098 -2498.098] (0.020)
Step: 49349, Reward: [-4397.106 -4397.106] [0.0000], Avg: [-2500.022 -2500.022] (0.020)
Step: 49399, Reward: [-2600.888 -2600.888] [0.0000], Avg: [-2500.124 -2500.124] (0.020)
Step: 49449, Reward: [-2687.47 -2687.47] [0.0000], Avg: [-2500.313 -2500.313] (0.020)
Step: 49499, Reward: [-4045.553 -4045.553] [0.0000], Avg: [-2501.874 -2501.874] (0.020)
Step: 49549, Reward: [-3656.033 -3656.033] [0.0000], Avg: [-2503.039 -2503.039] (0.020)
Step: 49599, Reward: [-1787.738 -1787.738] [0.0000], Avg: [-2502.318 -2502.318] (0.020)
Step: 49649, Reward: [-2547.717 -2547.717] [0.0000], Avg: [-2502.363 -2502.363] (0.020)
Step: 49699, Reward: [-1715.741 -1715.741] [0.0000], Avg: [-2501.572 -2501.572] (0.020)
Step: 49749, Reward: [-3445.231 -3445.231] [0.0000], Avg: [-2502.521 -2502.521] (0.020)
Step: 49799, Reward: [-2220.98 -2220.98] [0.0000], Avg: [-2502.238 -2502.238] (0.020)
Step: 49849, Reward: [-2927.007 -2927.007] [0.0000], Avg: [-2502.664 -2502.664] (0.020)
Step: 49899, Reward: [-3354.179 -3354.179] [0.0000], Avg: [-2503.517 -2503.517] (0.020)
Step: 49949, Reward: [-2725.703 -2725.703] [0.0000], Avg: [-2503.74 -2503.74] (0.020)
Step: 49999, Reward: [-3363.317 -3363.317] [0.0000], Avg: [-2504.599 -2504.599] (0.020)
Step: 50049, Reward: [-2626.022 -2626.022] [0.0000], Avg: [-2504.72 -2504.72] (0.020)
Step: 50099, Reward: [-2923.316 -2923.316] [0.0000], Avg: [-2505.138 -2505.138] (0.020)
Step: 50149, Reward: [-3273.994 -3273.994] [0.0000], Avg: [-2505.905 -2505.905] (0.020)
Step: 50199, Reward: [-3786.843 -3786.843] [0.0000], Avg: [-2507.181 -2507.181] (0.020)
Step: 50249, Reward: [-2963.604 -2963.604] [0.0000], Avg: [-2507.635 -2507.635] (0.020)
Step: 50299, Reward: [-2433.307 -2433.307] [0.0000], Avg: [-2507.561 -2507.561] (0.020)
Step: 50349, Reward: [-3784.791 -3784.791] [0.0000], Avg: [-2508.829 -2508.829] (0.020)
Step: 50399, Reward: [-1684.939 -1684.939] [0.0000], Avg: [-2508.012 -2508.012] (0.020)
Step: 50449, Reward: [-2179.664 -2179.664] [0.0000], Avg: [-2507.686 -2507.686] (0.020)
Step: 50499, Reward: [-2841.927 -2841.927] [0.0000], Avg: [-2508.017 -2508.017] (0.020)
Step: 50549, Reward: [-2700.272 -2700.272] [0.0000], Avg: [-2508.208 -2508.208] (0.020)
Step: 50599, Reward: [-3503.022 -3503.022] [0.0000], Avg: [-2509.191 -2509.191] (0.020)
Step: 50649, Reward: [-4350.861 -4350.861] [0.0000], Avg: [-2511.009 -2511.009] (0.020)
Step: 50699, Reward: [-3104.012 -3104.012] [0.0000], Avg: [-2511.593 -2511.593] (0.020)
Step: 50749, Reward: [-2770.662 -2770.662] [0.0000], Avg: [-2511.849 -2511.849] (0.020)
Step: 50799, Reward: [-3107.321 -3107.321] [0.0000], Avg: [-2512.435 -2512.435] (0.020)
Step: 50849, Reward: [-2756.254 -2756.254] [0.0000], Avg: [-2512.674 -2512.674] (0.020)
Step: 50899, Reward: [-2794.512 -2794.512] [0.0000], Avg: [-2512.951 -2512.951] (0.020)
Step: 50949, Reward: [-2397.127 -2397.127] [0.0000], Avg: [-2512.838 -2512.838] (0.020)
Step: 50999, Reward: [-3109.94 -3109.94] [0.0000], Avg: [-2513.423 -2513.423] (0.020)
Step: 51049, Reward: [-2556.374 -2556.374] [0.0000], Avg: [-2513.465 -2513.465] (0.020)
Step: 51099, Reward: [-4486.172 -4486.172] [0.0000], Avg: [-2515.395 -2515.395] (0.020)
Step: 51149, Reward: [-3028.445 -3028.445] [0.0000], Avg: [-2515.897 -2515.897] (0.020)
Step: 51199, Reward: [-4220.568 -4220.568] [0.0000], Avg: [-2517.562 -2517.562] (0.020)
Step: 51249, Reward: [-3886.545 -3886.545] [0.0000], Avg: [-2518.897 -2518.897] (0.020)
Step: 51299, Reward: [-3505.304 -3505.304] [0.0000], Avg: [-2519.859 -2519.859] (0.020)
Step: 51349, Reward: [-3893.997 -3893.997] [0.0000], Avg: [-2521.197 -2521.197] (0.020)
Step: 51399, Reward: [-3131.771 -3131.771] [0.0000], Avg: [-2521.791 -2521.791] (0.020)
Step: 51449, Reward: [-4103.329 -4103.329] [0.0000], Avg: [-2523.328 -2523.328] (0.020)
Step: 51499, Reward: [-2888.89 -2888.89] [0.0000], Avg: [-2523.682 -2523.682] (0.020)
Step: 51549, Reward: [-3084.461 -3084.461] [0.0000], Avg: [-2524.226 -2524.226] (0.020)
Step: 51599, Reward: [-3195.271 -3195.271] [0.0000], Avg: [-2524.877 -2524.877] (0.020)
Step: 51649, Reward: [-2476.509 -2476.509] [0.0000], Avg: [-2524.83 -2524.83] (0.020)
Step: 51699, Reward: [-2332.204 -2332.204] [0.0000], Avg: [-2524.643 -2524.643] (0.020)
Step: 51749, Reward: [-2780.374 -2780.374] [0.0000], Avg: [-2524.891 -2524.891] (0.020)
Step: 51799, Reward: [-4003.974 -4003.974] [0.0000], Avg: [-2526.318 -2526.318] (0.020)
Step: 51849, Reward: [-2116.917 -2116.917] [0.0000], Avg: [-2525.923 -2525.923] (0.020)
Step: 51899, Reward: [-2326.477 -2326.477] [0.0000], Avg: [-2525.731 -2525.731] (0.020)
Step: 51949, Reward: [-2079.339 -2079.339] [0.0000], Avg: [-2525.302 -2525.302] (0.020)
Step: 51999, Reward: [-2762.508 -2762.508] [0.0000], Avg: [-2525.53 -2525.53] (0.020)
Step: 52049, Reward: [-4855.467 -4855.467] [0.0000], Avg: [-2527.768 -2527.768] (0.020)
Step: 52099, Reward: [-3751.213 -3751.213] [0.0000], Avg: [-2528.942 -2528.942] (0.020)
Step: 52149, Reward: [-2233.036 -2233.036] [0.0000], Avg: [-2528.658 -2528.658] (0.020)
Step: 52199, Reward: [-2892.908 -2892.908] [0.0000], Avg: [-2529.007 -2529.007] (0.020)
Step: 52249, Reward: [-2512.791 -2512.791] [0.0000], Avg: [-2528.992 -2528.992] (0.020)
Step: 52299, Reward: [-1987.723 -1987.723] [0.0000], Avg: [-2528.474 -2528.474] (0.020)
Step: 52349, Reward: [-2796.317 -2796.317] [0.0000], Avg: [-2528.73 -2528.73] (0.020)
Step: 52399, Reward: [-1701.148 -1701.148] [0.0000], Avg: [-2527.94 -2527.94] (0.020)
Step: 52449, Reward: [-1461.918 -1461.918] [0.0000], Avg: [-2526.924 -2526.924] (0.020)
Step: 52499, Reward: [-3318.024 -3318.024] [0.0000], Avg: [-2527.678 -2527.678] (0.020)
Step: 52549, Reward: [-3662.866 -3662.866] [0.0000], Avg: [-2528.758 -2528.758] (0.020)
Step: 52599, Reward: [-3846.203 -3846.203] [0.0000], Avg: [-2530.01 -2530.01] (0.020)
Step: 52649, Reward: [-2985.366 -2985.366] [0.0000], Avg: [-2530.442 -2530.442] (0.020)
Step: 52699, Reward: [-3475.416 -3475.416] [0.0000], Avg: [-2531.339 -2531.339] (0.020)
Step: 52749, Reward: [-3044.315 -3044.315] [0.0000], Avg: [-2531.825 -2531.825] (0.020)
Step: 52799, Reward: [-3659.764 -3659.764] [0.0000], Avg: [-2532.893 -2532.893] (0.020)
Step: 52849, Reward: [-2792.501 -2792.501] [0.0000], Avg: [-2533.139 -2533.139] (0.020)
Step: 52899, Reward: [-3735.917 -3735.917] [0.0000], Avg: [-2534.276 -2534.276] (0.020)
Step: 52949, Reward: [-2085.619 -2085.619] [0.0000], Avg: [-2533.852 -2533.852] (0.020)
Step: 52999, Reward: [-2939.381 -2939.381] [0.0000], Avg: [-2534.235 -2534.235] (0.020)
Step: 53049, Reward: [-2721.52 -2721.52] [0.0000], Avg: [-2534.411 -2534.411] (0.020)
Step: 53099, Reward: [-3440.471 -3440.471] [0.0000], Avg: [-2535.264 -2535.264] (0.020)
Step: 53149, Reward: [-2884.246 -2884.246] [0.0000], Avg: [-2535.593 -2535.593] (0.020)
Step: 53199, Reward: [-3478.484 -3478.484] [0.0000], Avg: [-2536.479 -2536.479] (0.020)
Step: 53249, Reward: [-2868.87 -2868.87] [0.0000], Avg: [-2536.791 -2536.791] (0.020)
Step: 53299, Reward: [-4029.53 -4029.53] [0.0000], Avg: [-2538.191 -2538.191] (0.020)
Step: 53349, Reward: [-3301.26 -3301.26] [0.0000], Avg: [-2538.906 -2538.906] (0.020)
Step: 53399, Reward: [-3170.73 -3170.73] [0.0000], Avg: [-2539.498 -2539.498] (0.020)
Step: 53449, Reward: [-2902.426 -2902.426] [0.0000], Avg: [-2539.838 -2539.838] (0.020)
Step: 53499, Reward: [-3201.514 -3201.514] [0.0000], Avg: [-2540.456 -2540.456] (0.020)
Step: 53549, Reward: [-4311.11 -4311.11] [0.0000], Avg: [-2542.109 -2542.109] (0.020)
Step: 53599, Reward: [-2435.415 -2435.415] [0.0000], Avg: [-2542.01 -2542.01] (0.020)
Step: 53649, Reward: [-2590.499 -2590.499] [0.0000], Avg: [-2542.055 -2542.055] (0.020)
Step: 53699, Reward: [-2571.612 -2571.612] [0.0000], Avg: [-2542.082 -2542.082] (0.020)
Step: 53749, Reward: [-2941.007 -2941.007] [0.0000], Avg: [-2542.454 -2542.454] (0.020)
Step: 53799, Reward: [-4068.402 -4068.402] [0.0000], Avg: [-2543.872 -2543.872] (0.020)
Step: 53849, Reward: [-2970.556 -2970.556] [0.0000], Avg: [-2544.268 -2544.268] (0.020)
Step: 53899, Reward: [-1607.481 -1607.481] [0.0000], Avg: [-2543.399 -2543.399] (0.020)
Step: 53949, Reward: [-2079.819 -2079.819] [0.0000], Avg: [-2542.969 -2542.969] (0.020)
Step: 53999, Reward: [-726.038 -726.038] [0.0000], Avg: [-2541.287 -2541.287] (0.020)
Step: 54049, Reward: [-2859.363 -2859.363] [0.0000], Avg: [-2541.581 -2541.581] (0.020)
Step: 54099, Reward: [-2429.426 -2429.426] [0.0000], Avg: [-2541.477 -2541.477] (0.020)
Step: 54149, Reward: [-2831.665 -2831.665] [0.0000], Avg: [-2541.745 -2541.745] (0.020)
Step: 54199, Reward: [-3112.91 -3112.91] [0.0000], Avg: [-2542.272 -2542.272] (0.020)
Step: 54249, Reward: [-2387.871 -2387.871] [0.0000], Avg: [-2542.13 -2542.13] (0.020)
Step: 54299, Reward: [-3111.149 -3111.149] [0.0000], Avg: [-2542.654 -2542.654] (0.020)
Step: 54349, Reward: [-2612.91 -2612.91] [0.0000], Avg: [-2542.719 -2542.719] (0.020)
Step: 54399, Reward: [-2176.635 -2176.635] [0.0000], Avg: [-2542.382 -2542.382] (0.020)
Step: 54449, Reward: [-2378.003 -2378.003] [0.0000], Avg: [-2542.231 -2542.231] (0.020)
Step: 54499, Reward: [-3408.853 -3408.853] [0.0000], Avg: [-2543.026 -2543.026] (0.020)
Step: 54549, Reward: [-3085.43 -3085.43] [0.0000], Avg: [-2543.523 -2543.523] (0.020)
Step: 54599, Reward: [-2294.462 -2294.462] [0.0000], Avg: [-2543.295 -2543.295] (0.020)
Step: 54649, Reward: [-2957.345 -2957.345] [0.0000], Avg: [-2543.674 -2543.674] (0.020)
Step: 54699, Reward: [-2534.077 -2534.077] [0.0000], Avg: [-2543.665 -2543.665] (0.020)
Step: 54749, Reward: [-4462.874 -4462.874] [0.0000], Avg: [-2545.418 -2545.418] (0.020)
Step: 54799, Reward: [-1973.195 -1973.195] [0.0000], Avg: [-2544.896 -2544.896] (0.020)
Step: 54849, Reward: [-4979.149 -4979.149] [0.0000], Avg: [-2547.115 -2547.115] (0.020)
Step: 54899, Reward: [-2534.616 -2534.616] [0.0000], Avg: [-2547.104 -2547.104] (0.020)
Step: 54949, Reward: [-3052.528 -3052.528] [0.0000], Avg: [-2547.563 -2547.563] (0.020)
Step: 54999, Reward: [-2647.205 -2647.205] [0.0000], Avg: [-2547.654 -2547.654] (0.020)
Step: 55049, Reward: [-2401.749 -2401.749] [0.0000], Avg: [-2547.522 -2547.522] (0.020)
Step: 55099, Reward: [-3242.696 -3242.696] [0.0000], Avg: [-2548.152 -2548.152] (0.020)
Step: 55149, Reward: [-3627.604 -3627.604] [0.0000], Avg: [-2549.131 -2549.131] (0.020)
Step: 55199, Reward: [-3569.006 -3569.006] [0.0000], Avg: [-2550.055 -2550.055] (0.020)
Step: 55249, Reward: [-3504.265 -3504.265] [0.0000], Avg: [-2550.918 -2550.918] (0.020)
Step: 55299, Reward: [-3067.409 -3067.409] [0.0000], Avg: [-2551.385 -2551.385] (0.020)
Step: 55349, Reward: [-2900.786 -2900.786] [0.0000], Avg: [-2551.701 -2551.701] (0.020)
Step: 55399, Reward: [-3144.309 -3144.309] [0.0000], Avg: [-2552.236 -2552.236] (0.020)
Step: 55449, Reward: [-2436.556 -2436.556] [0.0000], Avg: [-2552.132 -2552.132] (0.020)
Step: 55499, Reward: [-4224.746 -4224.746] [0.0000], Avg: [-2553.638 -2553.638] (0.020)
Step: 55549, Reward: [-1781.675 -1781.675] [0.0000], Avg: [-2552.944 -2552.944] (0.020)
Step: 55599, Reward: [-2526.054 -2526.054] [0.0000], Avg: [-2552.919 -2552.919] (0.020)
Step: 55649, Reward: [-4194.392 -4194.392] [0.0000], Avg: [-2554.394 -2554.394] (0.020)
Step: 55699, Reward: [-4288.757 -4288.757] [0.0000], Avg: [-2555.951 -2555.951] (0.020)
Step: 55749, Reward: [-1857.934 -1857.934] [0.0000], Avg: [-2555.325 -2555.325] (0.020)
Step: 55799, Reward: [-2897.843 -2897.843] [0.0000], Avg: [-2555.632 -2555.632] (0.020)
Step: 55849, Reward: [-4052.851 -4052.851] [0.0000], Avg: [-2556.972 -2556.972] (0.020)
Step: 55899, Reward: [-2043.908 -2043.908] [0.0000], Avg: [-2556.513 -2556.513] (0.020)
Step: 55949, Reward: [-2962.792 -2962.792] [0.0000], Avg: [-2556.877 -2556.877] (0.020)
Step: 55999, Reward: [-3054.784 -3054.784] [0.0000], Avg: [-2557.321 -2557.321] (0.020)
Step: 56049, Reward: [-2403.219 -2403.219] [0.0000], Avg: [-2557.184 -2557.184] (0.020)
Step: 56099, Reward: [-3891.562 -3891.562] [0.0000], Avg: [-2558.373 -2558.373] (0.020)
Step: 56149, Reward: [-2684.865 -2684.865] [0.0000], Avg: [-2558.486 -2558.486] (0.020)
Step: 56199, Reward: [-2937.737 -2937.737] [0.0000], Avg: [-2558.823 -2558.823] (0.020)
Step: 56249, Reward: [-2008.1 -2008.1] [0.0000], Avg: [-2558.333 -2558.333] (0.020)
Step: 56299, Reward: [-1909.764 -1909.764] [0.0000], Avg: [-2557.757 -2557.757] (0.020)
Step: 56349, Reward: [-3931.017 -3931.017] [0.0000], Avg: [-2558.976 -2558.976] (0.020)
Step: 56399, Reward: [-2539.653 -2539.653] [0.0000], Avg: [-2558.959 -2558.959] (0.020)
Step: 56449, Reward: [-3689.707 -3689.707] [0.0000], Avg: [-2559.96 -2559.96] (0.020)
Step: 56499, Reward: [-2965.87 -2965.87] [0.0000], Avg: [-2560.32 -2560.32] (0.020)
Step: 56549, Reward: [-2298.409 -2298.409] [0.0000], Avg: [-2560.088 -2560.088] (0.020)
Step: 56599, Reward: [-1704.139 -1704.139] [0.0000], Avg: [-2559.332 -2559.332] (0.020)
Step: 56649, Reward: [-3107.622 -3107.622] [0.0000], Avg: [-2559.816 -2559.816] (0.020)
Step: 56699, Reward: [-2822.425 -2822.425] [0.0000], Avg: [-2560.047 -2560.047] (0.020)
Step: 56749, Reward: [-3982.974 -3982.974] [0.0000], Avg: [-2561.301 -2561.301] (0.020)
Step: 56799, Reward: [-3027.594 -3027.594] [0.0000], Avg: [-2561.711 -2561.711] (0.020)
Step: 56849, Reward: [-2986.587 -2986.587] [0.0000], Avg: [-2562.085 -2562.085] (0.020)
Step: 56899, Reward: [-3668.598 -3668.598] [0.0000], Avg: [-2563.057 -2563.057] (0.020)
Step: 56949, Reward: [-2697.951 -2697.951] [0.0000], Avg: [-2563.176 -2563.176] (0.020)
Step: 56999, Reward: [-4473.272 -4473.272] [0.0000], Avg: [-2564.851 -2564.851] (0.020)
Step: 57049, Reward: [-4597.195 -4597.195] [0.0000], Avg: [-2566.633 -2566.633] (0.020)
Step: 57099, Reward: [-2861.711 -2861.711] [0.0000], Avg: [-2566.891 -2566.891] (0.020)
Step: 57149, Reward: [-3531.705 -3531.705] [0.0000], Avg: [-2567.735 -2567.735] (0.020)
Step: 57199, Reward: [-2936.718 -2936.718] [0.0000], Avg: [-2568.058 -2568.058] (0.020)
Step: 57249, Reward: [-3323.552 -3323.552] [0.0000], Avg: [-2568.718 -2568.718] (0.020)
Step: 57299, Reward: [-2452.303 -2452.303] [0.0000], Avg: [-2568.616 -2568.616] (0.020)
Step: 57349, Reward: [-2882.582 -2882.582] [0.0000], Avg: [-2568.89 -2568.89] (0.020)
Step: 57399, Reward: [-2750.959 -2750.959] [0.0000], Avg: [-2569.048 -2569.048] (0.020)
Step: 57449, Reward: [-2188.059 -2188.059] [0.0000], Avg: [-2568.717 -2568.717] (0.020)
Step: 57499, Reward: [-2850.571 -2850.571] [0.0000], Avg: [-2568.962 -2568.962] (0.020)
Step: 57549, Reward: [-2398.872 -2398.872] [0.0000], Avg: [-2568.814 -2568.814] (0.020)
Step: 57599, Reward: [-2904.106 -2904.106] [0.0000], Avg: [-2569.105 -2569.105] (0.020)
Step: 57649, Reward: [-2400.686 -2400.686] [0.0000], Avg: [-2568.959 -2568.959] (0.020)
Step: 57699, Reward: [-2955.283 -2955.283] [0.0000], Avg: [-2569.294 -2569.294] (0.020)
Step: 57749, Reward: [-2899.439 -2899.439] [0.0000], Avg: [-2569.58 -2569.58] (0.020)
Step: 57799, Reward: [-2492.421 -2492.421] [0.0000], Avg: [-2569.513 -2569.513] (0.020)
Step: 57849, Reward: [-3336.582 -3336.582] [0.0000], Avg: [-2570.176 -2570.176] (0.020)
Step: 57899, Reward: [-1842.824 -1842.824] [0.0000], Avg: [-2569.548 -2569.548] (0.020)
Step: 57949, Reward: [-3185.633 -3185.633] [0.0000], Avg: [-2570.079 -2570.079] (0.020)
Step: 57999, Reward: [-2914.054 -2914.054] [0.0000], Avg: [-2570.376 -2570.376] (0.020)
Step: 58049, Reward: [-2538.547 -2538.547] [0.0000], Avg: [-2570.348 -2570.348] (0.020)
Step: 58099, Reward: [-4909.264 -4909.264] [0.0000], Avg: [-2572.361 -2572.361] (0.020)
Step: 58149, Reward: [-2331.595 -2331.595] [0.0000], Avg: [-2572.154 -2572.154] (0.020)
Step: 58199, Reward: [-2521.2 -2521.2] [0.0000], Avg: [-2572.11 -2572.11] (0.020)
Step: 58249, Reward: [-3343.59 -3343.59] [0.0000], Avg: [-2572.773 -2572.773] (0.020)
Step: 58299, Reward: [-3061.768 -3061.768] [0.0000], Avg: [-2573.192 -2573.192] (0.020)
Step: 58349, Reward: [-2653.68 -2653.68] [0.0000], Avg: [-2573.261 -2573.261] (0.020)
Step: 58399, Reward: [-2129.914 -2129.914] [0.0000], Avg: [-2572.881 -2572.881] (0.020)
Step: 58449, Reward: [-4006.555 -4006.555] [0.0000], Avg: [-2574.108 -2574.108] (0.020)
Step: 58499, Reward: [-3272.638 -3272.638] [0.0000], Avg: [-2574.705 -2574.705] (0.020)
Step: 58549, Reward: [-2767.842 -2767.842] [0.0000], Avg: [-2574.87 -2574.87] (0.020)
Step: 58599, Reward: [-2643.429 -2643.429] [0.0000], Avg: [-2574.928 -2574.928] (0.020)
Step: 58649, Reward: [-3804.335 -3804.335] [0.0000], Avg: [-2575.976 -2575.976] (0.020)
Step: 58699, Reward: [-2507.067 -2507.067] [0.0000], Avg: [-2575.918 -2575.918] (0.020)
Step: 58749, Reward: [-2500.954 -2500.954] [0.0000], Avg: [-2575.854 -2575.854] (0.020)
Step: 58799, Reward: [-2596.401 -2596.401] [0.0000], Avg: [-2575.871 -2575.871] (0.020)
Step: 58849, Reward: [-4818.614 -4818.614] [0.0000], Avg: [-2577.777 -2577.777] (0.020)
Step: 58899, Reward: [-4606.704 -4606.704] [0.0000], Avg: [-2579.499 -2579.499] (0.020)
Step: 58949, Reward: [-2939.956 -2939.956] [0.0000], Avg: [-2579.805 -2579.805] (0.020)
Step: 58999, Reward: [-3631.613 -3631.613] [0.0000], Avg: [-2580.696 -2580.696] (0.020)
Step: 59049, Reward: [-4186.109 -4186.109] [0.0000], Avg: [-2582.056 -2582.056] (0.020)
Step: 59099, Reward: [-2383.359 -2383.359] [0.0000], Avg: [-2581.888 -2581.888] (0.020)
Step: 59149, Reward: [-1880.881 -1880.881] [0.0000], Avg: [-2581.295 -2581.295] (0.020)
Step: 59199, Reward: [-2672.961 -2672.961] [0.0000], Avg: [-2581.372 -2581.372] (0.020)
Step: 59249, Reward: [-2261.649 -2261.649] [0.0000], Avg: [-2581.103 -2581.103] (0.020)
Step: 59299, Reward: [-3646.991 -3646.991] [0.0000], Avg: [-2582.001 -2582.001] (0.020)
Step: 59349, Reward: [-4493.512 -4493.512] [0.0000], Avg: [-2583.612 -2583.612] (0.020)
Step: 59399, Reward: [-2972.851 -2972.851] [0.0000], Avg: [-2583.939 -2583.939] (0.020)
Step: 59449, Reward: [-1858.557 -1858.557] [0.0000], Avg: [-2583.329 -2583.329] (0.020)
Step: 59499, Reward: [-2042.452 -2042.452] [0.0000], Avg: [-2582.875 -2582.875] (0.020)
Step: 59549, Reward: [-3525.555 -3525.555] [0.0000], Avg: [-2583.666 -2583.666] (0.020)
Step: 59599, Reward: [-2866.839 -2866.839] [0.0000], Avg: [-2583.904 -2583.904] (0.020)
Step: 59649, Reward: [-2848.93 -2848.93] [0.0000], Avg: [-2584.126 -2584.126] (0.020)
Step: 59699, Reward: [-3969.524 -3969.524] [0.0000], Avg: [-2585.286 -2585.286] (0.020)
Step: 59749, Reward: [-5212.879 -5212.879] [0.0000], Avg: [-2587.485 -2587.485] (0.020)
Step: 59799, Reward: [-2583.518 -2583.518] [0.0000], Avg: [-2587.482 -2587.482] (0.020)
Step: 59849, Reward: [-3693.971 -3693.971] [0.0000], Avg: [-2588.406 -2588.406] (0.020)
Step: 59899, Reward: [-1845.576 -1845.576] [0.0000], Avg: [-2587.786 -2587.786] (0.020)
Step: 59949, Reward: [-3000.518 -3000.518] [0.0000], Avg: [-2588.13 -2588.13] (0.020)
Step: 59999, Reward: [-3163.417 -3163.417] [0.0000], Avg: [-2588.61 -2588.61] (0.020)
Step: 60049, Reward: [-2892.695 -2892.695] [0.0000], Avg: [-2588.863 -2588.863] (0.020)
Step: 60099, Reward: [-2784.85 -2784.85] [0.0000], Avg: [-2589.026 -2589.026] (0.020)
Step: 60149, Reward: [-4417.85 -4417.85] [0.0000], Avg: [-2590.546 -2590.546] (0.020)
Step: 60199, Reward: [-3484.48 -3484.48] [0.0000], Avg: [-2591.289 -2591.289] (0.020)
Step: 60249, Reward: [-2601.937 -2601.937] [0.0000], Avg: [-2591.297 -2591.297] (0.020)
Step: 60299, Reward: [-3140.898 -3140.898] [0.0000], Avg: [-2591.753 -2591.753] (0.020)
Step: 60349, Reward: [-3125.823 -3125.823] [0.0000], Avg: [-2592.196 -2592.196] (0.020)
Step: 60399, Reward: [-2528.644 -2528.644] [0.0000], Avg: [-2592.143 -2592.143] (0.020)
Step: 60449, Reward: [-4739.35 -4739.35] [0.0000], Avg: [-2593.919 -2593.919] (0.020)
Step: 60499, Reward: [-4220.706 -4220.706] [0.0000], Avg: [-2595.264 -2595.264] (0.020)
Step: 60549, Reward: [-3404.628 -3404.628] [0.0000], Avg: [-2595.932 -2595.932] (0.020)
Step: 60599, Reward: [-3157.249 -3157.249] [0.0000], Avg: [-2596.395 -2596.395] (0.020)
Step: 60649, Reward: [-3768.007 -3768.007] [0.0000], Avg: [-2597.361 -2597.361] (0.020)
Step: 60699, Reward: [-2652.152 -2652.152] [0.0000], Avg: [-2597.406 -2597.406] (0.020)
Step: 60749, Reward: [-2520.832 -2520.832] [0.0000], Avg: [-2597.343 -2597.343] (0.020)
Step: 60799, Reward: [-2399.685 -2399.685] [0.0000], Avg: [-2597.18 -2597.18] (0.020)
Step: 60849, Reward: [-2184.591 -2184.591] [0.0000], Avg: [-2596.841 -2596.841] (0.020)
Step: 60899, Reward: [-2262.676 -2262.676] [0.0000], Avg: [-2596.567 -2596.567] (0.020)
Step: 60949, Reward: [-2270.499 -2270.499] [0.0000], Avg: [-2596.3 -2596.3] (0.020)
Step: 60999, Reward: [-3877.658 -3877.658] [0.0000], Avg: [-2597.35 -2597.35] (0.020)
Step: 61049, Reward: [-3812.072 -3812.072] [0.0000], Avg: [-2598.345 -2598.345] (0.020)
Step: 61099, Reward: [-1819.21 -1819.21] [0.0000], Avg: [-2597.707 -2597.707] (0.020)
Step: 61149, Reward: [-4682.462 -4682.462] [0.0000], Avg: [-2599.412 -2599.412] (0.020)
Step: 61199, Reward: [-4083.734 -4083.734] [0.0000], Avg: [-2600.624 -2600.624] (0.020)
Step: 61249, Reward: [-4282.815 -4282.815] [0.0000], Avg: [-2601.998 -2601.998] (0.020)
Step: 61299, Reward: [-1915.996 -1915.996] [0.0000], Avg: [-2601.438 -2601.438] (0.020)
Step: 61349, Reward: [-3007.643 -3007.643] [0.0000], Avg: [-2601.769 -2601.769] (0.020)
Step: 61399, Reward: [-2826.531 -2826.531] [0.0000], Avg: [-2601.952 -2601.952] (0.020)
Step: 61449, Reward: [-1743.246 -1743.246] [0.0000], Avg: [-2601.253 -2601.253] (0.020)
Step: 61499, Reward: [-3836.205 -3836.205] [0.0000], Avg: [-2602.258 -2602.258] (0.020)
Step: 61549, Reward: [-2103.119 -2103.119] [0.0000], Avg: [-2601.852 -2601.852] (0.020)
Step: 61599, Reward: [-3307.601 -3307.601] [0.0000], Avg: [-2602.425 -2602.425] (0.020)
Step: 61649, Reward: [-4479.356 -4479.356] [0.0000], Avg: [-2603.947 -2603.947] (0.020)
Step: 61699, Reward: [-4356.935 -4356.935] [0.0000], Avg: [-2605.368 -2605.368] (0.020)
Step: 61749, Reward: [-2069.611 -2069.611] [0.0000], Avg: [-2604.934 -2604.934] (0.020)
Step: 61799, Reward: [-3008.81 -3008.81] [0.0000], Avg: [-2605.261 -2605.261] (0.020)
Step: 61849, Reward: [-1656.578 -1656.578] [0.0000], Avg: [-2604.494 -2604.494] (0.020)
Step: 61899, Reward: [-2532.081 -2532.081] [0.0000], Avg: [-2604.435 -2604.435] (0.020)
Step: 61949, Reward: [-2147.49 -2147.49] [0.0000], Avg: [-2604.066 -2604.066] (0.020)
Step: 61999, Reward: [-4104.639 -4104.639] [0.0000], Avg: [-2605.277 -2605.277] (0.020)
Step: 62049, Reward: [-4223.108 -4223.108] [0.0000], Avg: [-2606.58 -2606.58] (0.020)
Step: 62099, Reward: [-3035.058 -3035.058] [0.0000], Avg: [-2606.925 -2606.925] (0.020)
Step: 62149, Reward: [-3086.435 -3086.435] [0.0000], Avg: [-2607.311 -2607.311] (0.020)
Step: 62199, Reward: [-2274.812 -2274.812] [0.0000], Avg: [-2607.044 -2607.044] (0.020)
Step: 62249, Reward: [-3975.336 -3975.336] [0.0000], Avg: [-2608.143 -2608.143] (0.020)
Step: 62299, Reward: [-2602.785 -2602.785] [0.0000], Avg: [-2608.138 -2608.138] (0.020)
Step: 62349, Reward: [-2537.077 -2537.077] [0.0000], Avg: [-2608.081 -2608.081] (0.020)
Step: 62399, Reward: [-1729.929 -1729.929] [0.0000], Avg: [-2607.378 -2607.378] (0.020)
Step: 62449, Reward: [-3478.236 -3478.236] [0.0000], Avg: [-2608.075 -2608.075] (0.020)
Step: 62499, Reward: [-4148.569 -4148.569] [0.0000], Avg: [-2609.307 -2609.307] (0.020)
Step: 62549, Reward: [-2655.993 -2655.993] [0.0000], Avg: [-2609.345 -2609.345] (0.020)
Step: 62599, Reward: [-3762.745 -3762.745] [0.0000], Avg: [-2610.266 -2610.266] (0.020)
Step: 62649, Reward: [-3442.235 -3442.235] [0.0000], Avg: [-2610.93 -2610.93] (0.020)
Step: 62699, Reward: [-4025.252 -4025.252] [0.0000], Avg: [-2612.058 -2612.058] (0.020)
Step: 62749, Reward: [-4035.266 -4035.266] [0.0000], Avg: [-2613.192 -2613.192] (0.020)
Step: 62799, Reward: [-3294.023 -3294.023] [0.0000], Avg: [-2613.734 -2613.734] (0.020)
Step: 62849, Reward: [-3973.477 -3973.477] [0.0000], Avg: [-2614.816 -2614.816] (0.020)
Step: 62899, Reward: [-2603.68 -2603.68] [0.0000], Avg: [-2614.807 -2614.807] (0.020)
Step: 62949, Reward: [-4233.203 -4233.203] [0.0000], Avg: [-2616.092 -2616.092] (0.020)
Step: 62999, Reward: [-2433.327 -2433.327] [0.0000], Avg: [-2615.947 -2615.947] (0.020)
Step: 63049, Reward: [-2455.467 -2455.467] [0.0000], Avg: [-2615.82 -2615.82] (0.020)
Step: 63099, Reward: [-3451.816 -3451.816] [0.0000], Avg: [-2616.482 -2616.482] (0.020)
Step: 63149, Reward: [-2442.365 -2442.365] [0.0000], Avg: [-2616.345 -2616.345] (0.020)
Step: 63199, Reward: [-3298.013 -3298.013] [0.0000], Avg: [-2616.884 -2616.884] (0.020)
Step: 63249, Reward: [-2841.823 -2841.823] [0.0000], Avg: [-2617.062 -2617.062] (0.020)
Step: 63299, Reward: [-3376.012 -3376.012] [0.0000], Avg: [-2617.661 -2617.661] (0.020)
Step: 63349, Reward: [-2994.857 -2994.857] [0.0000], Avg: [-2617.959 -2617.959] (0.020)
Step: 63399, Reward: [-3394.909 -3394.909] [0.0000], Avg: [-2618.572 -2618.572] (0.020)
Step: 63449, Reward: [-3298.368 -3298.368] [0.0000], Avg: [-2619.107 -2619.107] (0.020)
Step: 63499, Reward: [-4184.498 -4184.498] [0.0000], Avg: [-2620.34 -2620.34] (0.020)
Step: 63549, Reward: [-3854.522 -3854.522] [0.0000], Avg: [-2621.311 -2621.311] (0.020)
Step: 63599, Reward: [-2838.242 -2838.242] [0.0000], Avg: [-2621.481 -2621.481] (0.020)
Step: 63649, Reward: [-2981.258 -2981.258] [0.0000], Avg: [-2621.764 -2621.764] (0.020)
Step: 63699, Reward: [-2978.943 -2978.943] [0.0000], Avg: [-2622.044 -2622.044] (0.020)
Step: 63749, Reward: [-4770.663 -4770.663] [0.0000], Avg: [-2623.73 -2623.73] (0.020)
Step: 63799, Reward: [-1594.018 -1594.018] [0.0000], Avg: [-2622.923 -2622.923] (0.020)
Step: 63849, Reward: [-2468.441 -2468.441] [0.0000], Avg: [-2622.802 -2622.802] (0.020)
Step: 63899, Reward: [-1905.42 -1905.42] [0.0000], Avg: [-2622.24 -2622.24] (0.020)
Step: 63949, Reward: [-4806.478 -4806.478] [0.0000], Avg: [-2623.948 -2623.948] (0.020)
Step: 63999, Reward: [-3337.454 -3337.454] [0.0000], Avg: [-2624.506 -2624.506] (0.020)
Step: 64049, Reward: [-3596.09 -3596.09] [0.0000], Avg: [-2625.264 -2625.264] (0.020)
Step: 64099, Reward: [-3637.469 -3637.469] [0.0000], Avg: [-2626.054 -2626.054] (0.020)
Step: 64149, Reward: [-2055.031 -2055.031] [0.0000], Avg: [-2625.608 -2625.608] (0.020)
Step: 64199, Reward: [-2968.532 -2968.532] [0.0000], Avg: [-2625.876 -2625.876] (0.020)
Step: 64249, Reward: [-2466.643 -2466.643] [0.0000], Avg: [-2625.752 -2625.752] (0.020)
Step: 64299, Reward: [-2936.968 -2936.968] [0.0000], Avg: [-2625.994 -2625.994] (0.020)
Step: 64349, Reward: [-4113.644 -4113.644] [0.0000], Avg: [-2627.15 -2627.15] (0.020)
Step: 64399, Reward: [-2947.919 -2947.919] [0.0000], Avg: [-2627.399 -2627.399] (0.020)
Step: 64449, Reward: [-3559.758 -3559.758] [0.0000], Avg: [-2628.122 -2628.122] (0.020)
Step: 64499, Reward: [-2601.089 -2601.089] [0.0000], Avg: [-2628.101 -2628.101] (0.020)
Step: 64549, Reward: [-3269.427 -3269.427] [0.0000], Avg: [-2628.598 -2628.598] (0.020)
Step: 64599, Reward: [-2654.185 -2654.185] [0.0000], Avg: [-2628.618 -2628.618] (0.020)
Step: 64649, Reward: [-2465.882 -2465.882] [0.0000], Avg: [-2628.492 -2628.492] (0.020)
Step: 64699, Reward: [-2909.291 -2909.291] [0.0000], Avg: [-2628.709 -2628.709] (0.020)
Step: 64749, Reward: [-3452.689 -3452.689] [0.0000], Avg: [-2629.345 -2629.345] (0.020)
Step: 64799, Reward: [-2962.258 -2962.258] [0.0000], Avg: [-2629.602 -2629.602] (0.020)
Step: 64849, Reward: [-3468.274 -3468.274] [0.0000], Avg: [-2630.248 -2630.248] (0.020)
Step: 64899, Reward: [-3750.866 -3750.866] [0.0000], Avg: [-2631.112 -2631.112] (0.020)
Step: 64949, Reward: [-3119.543 -3119.543] [0.0000], Avg: [-2631.488 -2631.488] (0.020)
Step: 64999, Reward: [-2726.561 -2726.561] [0.0000], Avg: [-2631.561 -2631.561] (0.020)
Step: 65049, Reward: [-3151.318 -3151.318] [0.0000], Avg: [-2631.96 -2631.96] (0.020)
Step: 65099, Reward: [-3142.7 -3142.7] [0.0000], Avg: [-2632.353 -2632.353] (0.020)
Step: 65149, Reward: [-2483.454 -2483.454] [0.0000], Avg: [-2632.238 -2632.238] (0.020)
Step: 65199, Reward: [-2248.064 -2248.064] [0.0000], Avg: [-2631.944 -2631.944] (0.020)
Step: 65249, Reward: [-2638.276 -2638.276] [0.0000], Avg: [-2631.949 -2631.949] (0.020)
Step: 65299, Reward: [-1696.231 -1696.231] [0.0000], Avg: [-2631.232 -2631.232] (0.020)
Step: 65349, Reward: [-2035.075 -2035.075] [0.0000], Avg: [-2630.776 -2630.776] (0.020)
Step: 65399, Reward: [-4402.504 -4402.504] [0.0000], Avg: [-2632.131 -2632.131] (0.020)
Step: 65449, Reward: [-2713.145 -2713.145] [0.0000], Avg: [-2632.192 -2632.192] (0.020)
Step: 65499, Reward: [-2802.034 -2802.034] [0.0000], Avg: [-2632.322 -2632.322] (0.020)
Step: 65549, Reward: [-2814.057 -2814.057] [0.0000], Avg: [-2632.461 -2632.461] (0.020)
Step: 65599, Reward: [-3416.769 -3416.769] [0.0000], Avg: [-2633.059 -2633.059] (0.020)
Step: 65649, Reward: [-4136.579 -4136.579] [0.0000], Avg: [-2634.204 -2634.204] (0.020)
Step: 65699, Reward: [-4043.96 -4043.96] [0.0000], Avg: [-2635.277 -2635.277] (0.020)
Step: 65749, Reward: [-3856.534 -3856.534] [0.0000], Avg: [-2636.205 -2636.205] (0.020)
Step: 65799, Reward: [-2104.924 -2104.924] [0.0000], Avg: [-2635.802 -2635.802] (0.020)
Step: 65849, Reward: [-3518. -3518.] [0.0000], Avg: [-2636.471 -2636.471] (0.020)
Step: 65899, Reward: [-3294.889 -3294.889] [0.0000], Avg: [-2636.971 -2636.971] (0.020)
Step: 65949, Reward: [-1751.112 -1751.112] [0.0000], Avg: [-2636.299 -2636.299] (0.020)
Step: 65999, Reward: [-3963.828 -3963.828] [0.0000], Avg: [-2637.305 -2637.305] (0.020)
Step: 66049, Reward: [-2130.079 -2130.079] [0.0000], Avg: [-2636.921 -2636.921] (0.020)
Step: 66099, Reward: [-2392.62 -2392.62] [0.0000], Avg: [-2636.736 -2636.736] (0.020)
Step: 66149, Reward: [-4708.232 -4708.232] [0.0000], Avg: [-2638.302 -2638.302] (0.020)
Step: 66199, Reward: [-3441.988 -3441.988] [0.0000], Avg: [-2638.909 -2638.909] (0.020)
Step: 66249, Reward: [-4124.937 -4124.937] [0.0000], Avg: [-2640.031 -2640.031] (0.020)
Step: 66299, Reward: [-2804.983 -2804.983] [0.0000], Avg: [-2640.155 -2640.155] (0.020)
Step: 66349, Reward: [-4337.587 -4337.587] [0.0000], Avg: [-2641.434 -2641.434] (0.020)
Step: 66399, Reward: [-2150.239 -2150.239] [0.0000], Avg: [-2641.064 -2641.064] (0.020)
Step: 66449, Reward: [-1961.276 -1961.276] [0.0000], Avg: [-2640.553 -2640.553] (0.020)
Step: 66499, Reward: [-1922.874 -1922.874] [0.0000], Avg: [-2640.013 -2640.013] (0.020)
Step: 66549, Reward: [-2867.815 -2867.815] [0.0000], Avg: [-2640.184 -2640.184] (0.020)
Step: 66599, Reward: [-2557.13 -2557.13] [0.0000], Avg: [-2640.122 -2640.122] (0.020)
Step: 66649, Reward: [-3832.145 -3832.145] [0.0000], Avg: [-2641.016 -2641.016] (0.020)
Step: 66699, Reward: [-3118.555 -3118.555] [0.0000], Avg: [-2641.374 -2641.374] (0.020)
Step: 66749, Reward: [-3878.663 -3878.663] [0.0000], Avg: [-2642.301 -2642.301] (0.020)
Step: 66799, Reward: [-3081.398 -3081.398] [0.0000], Avg: [-2642.63 -2642.63] (0.020)
Step: 66849, Reward: [-2918.297 -2918.297] [0.0000], Avg: [-2642.836 -2642.836] (0.020)
Step: 66899, Reward: [-1692.932 -1692.932] [0.0000], Avg: [-2642.126 -2642.126] (0.020)
Step: 66949, Reward: [-3221.031 -3221.031] [0.0000], Avg: [-2642.558 -2642.558] (0.020)
Step: 66999, Reward: [-3396.612 -3396.612] [0.0000], Avg: [-2643.121 -2643.121] (0.020)
Step: 67049, Reward: [-2811.566 -2811.566] [0.0000], Avg: [-2643.247 -2643.247] (0.020)
Step: 67099, Reward: [-3616.284 -3616.284] [0.0000], Avg: [-2643.972 -2643.972] (0.020)
Step: 67149, Reward: [-2817.222 -2817.222] [0.0000], Avg: [-2644.101 -2644.101] (0.020)
Step: 67199, Reward: [-3254.369 -3254.369] [0.0000], Avg: [-2644.555 -2644.555] (0.020)
Step: 67249, Reward: [-2703.869 -2703.869] [0.0000], Avg: [-2644.599 -2644.599] (0.020)
Step: 67299, Reward: [-1992.515 -1992.515] [0.0000], Avg: [-2644.114 -2644.114] (0.020)
Step: 67349, Reward: [-2964.872 -2964.872] [0.0000], Avg: [-2644.352 -2644.352] (0.020)
Step: 67399, Reward: [-2006.878 -2006.878] [0.0000], Avg: [-2643.88 -2643.88] (0.020)
Step: 67449, Reward: [-3996.778 -3996.778] [0.0000], Avg: [-2644.882 -2644.882] (0.020)
Step: 67499, Reward: [-2344.63 -2344.63] [0.0000], Avg: [-2644.66 -2644.66] (0.020)
Step: 67549, Reward: [-2973.668 -2973.668] [0.0000], Avg: [-2644.904 -2644.904] (0.020)
Step: 67599, Reward: [-2922.254 -2922.254] [0.0000], Avg: [-2645.109 -2645.109] (0.020)
Step: 67649, Reward: [-2330.451 -2330.451] [0.0000], Avg: [-2644.876 -2644.876] (0.020)
Step: 67699, Reward: [-3879.256 -3879.256] [0.0000], Avg: [-2645.788 -2645.788] (0.020)
Step: 67749, Reward: [-1890.441 -1890.441] [0.0000], Avg: [-2645.23 -2645.23] (0.020)
Step: 67799, Reward: [-3919.286 -3919.286] [0.0000], Avg: [-2646.17 -2646.17] (0.020)
Step: 67849, Reward: [-3414.902 -3414.902] [0.0000], Avg: [-2646.736 -2646.736] (0.020)
Step: 67899, Reward: [-3718.445 -3718.445] [0.0000], Avg: [-2647.526 -2647.526] (0.020)
Step: 67949, Reward: [-1939.383 -1939.383] [0.0000], Avg: [-2647.004 -2647.004] (0.020)
Step: 67999, Reward: [-2511.961 -2511.961] [0.0000], Avg: [-2646.905 -2646.905] (0.020)
Step: 68049, Reward: [-3207.896 -3207.896] [0.0000], Avg: [-2647.317 -2647.317] (0.020)
Step: 68099, Reward: [-3065.542 -3065.542] [0.0000], Avg: [-2647.624 -2647.624] (0.020)
Step: 68149, Reward: [-4134.557 -4134.557] [0.0000], Avg: [-2648.715 -2648.715] (0.020)
Step: 68199, Reward: [-3261.728 -3261.728] [0.0000], Avg: [-2649.165 -2649.165] (0.020)
Step: 68249, Reward: [-4163.46 -4163.46] [0.0000], Avg: [-2650.274 -2650.274] (0.020)
Step: 68299, Reward: [-3775.558 -3775.558] [0.0000], Avg: [-2651.098 -2651.098] (0.020)
Step: 68349, Reward: [-2427.329 -2427.329] [0.0000], Avg: [-2650.934 -2650.934] (0.020)
Step: 68399, Reward: [-4576.238 -4576.238] [0.0000], Avg: [-2652.342 -2652.342] (0.020)
Step: 68449, Reward: [-3906.391 -3906.391] [0.0000], Avg: [-2653.258 -2653.258] (0.020)
Step: 68499, Reward: [-3571.173 -3571.173] [0.0000], Avg: [-2653.928 -2653.928] (0.020)
Step: 68549, Reward: [-2472.356 -2472.356] [0.0000], Avg: [-2653.795 -2653.795] (0.020)
Step: 68599, Reward: [-3403.424 -3403.424] [0.0000], Avg: [-2654.342 -2654.342] (0.020)
Step: 68649, Reward: [-2800.228 -2800.228] [0.0000], Avg: [-2654.448 -2654.448] (0.020)
Step: 68699, Reward: [-2590.496 -2590.496] [0.0000], Avg: [-2654.401 -2654.401] (0.020)
Step: 68749, Reward: [-1815.685 -1815.685] [0.0000], Avg: [-2653.791 -2653.791] (0.020)
Step: 68799, Reward: [-3098.441 -3098.441] [0.0000], Avg: [-2654.115 -2654.115] (0.020)
Step: 68849, Reward: [-1767.537 -1767.537] [0.0000], Avg: [-2653.471 -2653.471] (0.020)
Step: 68899, Reward: [-1649.011 -1649.011] [0.0000], Avg: [-2652.742 -2652.742] (0.020)
Step: 68949, Reward: [-2794.211 -2794.211] [0.0000], Avg: [-2652.844 -2652.844] (0.020)
Step: 68999, Reward: [-2769.269 -2769.269] [0.0000], Avg: [-2652.929 -2652.929] (0.020)
Step: 69049, Reward: [-3541.726 -3541.726] [0.0000], Avg: [-2653.572 -2653.572] (0.020)
Step: 69099, Reward: [-2660.749 -2660.749] [0.0000], Avg: [-2653.577 -2653.577] (0.020)
Step: 69149, Reward: [-2649.864 -2649.864] [0.0000], Avg: [-2653.575 -2653.575] (0.020)
Step: 69199, Reward: [-2983.373 -2983.373] [0.0000], Avg: [-2653.813 -2653.813] (0.020)
Step: 69249, Reward: [-1947.353 -1947.353] [0.0000], Avg: [-2653.303 -2653.303] (0.020)
Step: 69299, Reward: [-3718.104 -3718.104] [0.0000], Avg: [-2654.071 -2654.071] (0.020)
Step: 69349, Reward: [-2765.18 -2765.18] [0.0000], Avg: [-2654.151 -2654.151] (0.020)
Step: 69399, Reward: [-3867.927 -3867.927] [0.0000], Avg: [-2655.026 -2655.026] (0.020)
Step: 69449, Reward: [-3586.74 -3586.74] [0.0000], Avg: [-2655.697 -2655.697] (0.020)
Step: 69499, Reward: [-2660.63 -2660.63] [0.0000], Avg: [-2655.7 -2655.7] (0.020)
Step: 69549, Reward: [-1872.391 -1872.391] [0.0000], Avg: [-2655.137 -2655.137] (0.020)
Step: 69599, Reward: [-3578.556 -3578.556] [0.0000], Avg: [-2655.8 -2655.8] (0.020)
Step: 69649, Reward: [-2709.459 -2709.459] [0.0000], Avg: [-2655.839 -2655.839] (0.020)
Step: 69699, Reward: [-2903.62 -2903.62] [0.0000], Avg: [-2656.017 -2656.017] (0.020)
Step: 69749, Reward: [-2733.426 -2733.426] [0.0000], Avg: [-2656.072 -2656.072] (0.020)
Step: 69799, Reward: [-4129.028 -4129.028] [0.0000], Avg: [-2657.127 -2657.127] (0.020)
Step: 69849, Reward: [-3461.927 -3461.927] [0.0000], Avg: [-2657.703 -2657.703] (0.020)
Step: 69899, Reward: [-4402.754 -4402.754] [0.0000], Avg: [-2658.952 -2658.952] (0.020)
Step: 69949, Reward: [-3889.755 -3889.755] [0.0000], Avg: [-2659.831 -2659.831] (0.020)
Step: 69999, Reward: [-2812.469 -2812.469] [0.0000], Avg: [-2659.94 -2659.94] (0.020)
Step: 70049, Reward: [-1900.749 -1900.749] [0.0000], Avg: [-2659.399 -2659.399] (0.020)
Step: 70099, Reward: [-2617.514 -2617.514] [0.0000], Avg: [-2659.369 -2659.369] (0.020)
Step: 70149, Reward: [-2459.467 -2459.467] [0.0000], Avg: [-2659.226 -2659.226] (0.020)
Step: 70199, Reward: [-2412.823 -2412.823] [0.0000], Avg: [-2659.051 -2659.051] (0.020)
Step: 70249, Reward: [-1725.989 -1725.989] [0.0000], Avg: [-2658.387 -2658.387] (0.020)
Step: 70299, Reward: [-3668.23 -3668.23] [0.0000], Avg: [-2659.105 -2659.105] (0.020)
Step: 70349, Reward: [-4281.458 -4281.458] [0.0000], Avg: [-2660.258 -2660.258] (0.020)
Step: 70399, Reward: [-1933.663 -1933.663] [0.0000], Avg: [-2659.742 -2659.742] (0.020)
Step: 70449, Reward: [-2533.94 -2533.94] [0.0000], Avg: [-2659.653 -2659.653] (0.020)
Step: 70499, Reward: [-2748.835 -2748.835] [0.0000], Avg: [-2659.716 -2659.716] (0.020)
Step: 70549, Reward: [-1788.141 -1788.141] [0.0000], Avg: [-2659.098 -2659.098] (0.020)
Step: 70599, Reward: [-3221.849 -3221.849] [0.0000], Avg: [-2659.497 -2659.497] (0.020)
Step: 70649, Reward: [-3694.476 -3694.476] [0.0000], Avg: [-2660.229 -2660.229] (0.020)
Step: 70699, Reward: [-3289.028 -3289.028] [0.0000], Avg: [-2660.674 -2660.674] (0.020)
Step: 70749, Reward: [-4702.152 -4702.152] [0.0000], Avg: [-2662.117 -2662.117] (0.020)
Step: 70799, Reward: [-2706.687 -2706.687] [0.0000], Avg: [-2662.148 -2662.148] (0.020)
Step: 70849, Reward: [-4441.544 -4441.544] [0.0000], Avg: [-2663.404 -2663.404] (0.020)
Step: 70899, Reward: [-3419.018 -3419.018] [0.0000], Avg: [-2663.937 -2663.937] (0.020)
Step: 70949, Reward: [-3145.256 -3145.256] [0.0000], Avg: [-2664.276 -2664.276] (0.020)
Step: 70999, Reward: [-3064.353 -3064.353] [0.0000], Avg: [-2664.558 -2664.558] (0.020)
Step: 71049, Reward: [-3874.654 -3874.654] [0.0000], Avg: [-2665.409 -2665.409] (0.020)
Step: 71099, Reward: [-2610.968 -2610.968] [0.0000], Avg: [-2665.371 -2665.371] (0.020)
Step: 71149, Reward: [-2577.716 -2577.716] [0.0000], Avg: [-2665.309 -2665.309] (0.020)
Step: 71199, Reward: [-3061.334 -3061.334] [0.0000], Avg: [-2665.587 -2665.587] (0.020)
Step: 71249, Reward: [-3762.317 -3762.317] [0.0000], Avg: [-2666.357 -2666.357] (0.020)
Step: 71299, Reward: [-2783.643 -2783.643] [0.0000], Avg: [-2666.439 -2666.439] (0.020)
Step: 71349, Reward: [-3873.606 -3873.606] [0.0000], Avg: [-2667.285 -2667.285] (0.020)
Step: 71399, Reward: [-3328.563 -3328.563] [0.0000], Avg: [-2667.748 -2667.748] (0.020)
Step: 71449, Reward: [-2486.059 -2486.059] [0.0000], Avg: [-2667.621 -2667.621] (0.020)
Step: 71499, Reward: [-2988.453 -2988.453] [0.0000], Avg: [-2667.846 -2667.846] (0.020)
Step: 71549, Reward: [-3781.692 -3781.692] [0.0000], Avg: [-2668.624 -2668.624] (0.020)
Step: 71599, Reward: [-3437.895 -3437.895] [0.0000], Avg: [-2669.161 -2669.161] (0.020)
Step: 71649, Reward: [-2207.675 -2207.675] [0.0000], Avg: [-2668.839 -2668.839] (0.020)
Step: 71699, Reward: [-3895.472 -3895.472] [0.0000], Avg: [-2669.694 -2669.694] (0.020)
Step: 71749, Reward: [-2438.208 -2438.208] [0.0000], Avg: [-2669.533 -2669.533] (0.020)
Step: 71799, Reward: [-3145.383 -3145.383] [0.0000], Avg: [-2669.865 -2669.865] (0.020)
Step: 71849, Reward: [-4057.025 -4057.025] [0.0000], Avg: [-2670.83 -2670.83] (0.020)
Step: 71899, Reward: [-4709.124 -4709.124] [0.0000], Avg: [-2672.247 -2672.247] (0.020)
Step: 71949, Reward: [-2509.083 -2509.083] [0.0000], Avg: [-2672.134 -2672.134] (0.020)
Step: 71999, Reward: [-1850.126 -1850.126] [0.0000], Avg: [-2671.563 -2671.563] (0.020)
Step: 72049, Reward: [-4174.648 -4174.648] [0.0000], Avg: [-2672.606 -2672.606] (0.020)
Step: 72099, Reward: [-3193.127 -3193.127] [0.0000], Avg: [-2672.967 -2672.967] (0.020)
Step: 72149, Reward: [-2839.109 -2839.109] [0.0000], Avg: [-2673.082 -2673.082] (0.020)
Step: 72199, Reward: [-1669.862 -1669.862] [0.0000], Avg: [-2672.387 -2672.387] (0.020)
Step: 72249, Reward: [-1706.126 -1706.126] [0.0000], Avg: [-2671.719 -2671.719] (0.020)
Step: 72299, Reward: [-2079.752 -2079.752] [0.0000], Avg: [-2671.309 -2671.309] (0.020)
Step: 72349, Reward: [-3709.427 -3709.427] [0.0000], Avg: [-2672.027 -2672.027] (0.020)
Step: 72399, Reward: [-2478.436 -2478.436] [0.0000], Avg: [-2671.893 -2671.893] (0.020)
Step: 72449, Reward: [-3012.58 -3012.58] [0.0000], Avg: [-2672.128 -2672.128] (0.020)
Step: 72499, Reward: [-2864.963 -2864.963] [0.0000], Avg: [-2672.261 -2672.261] (0.020)
Step: 72549, Reward: [-2928.101 -2928.101] [0.0000], Avg: [-2672.438 -2672.438] (0.020)
Step: 72599, Reward: [-2186.713 -2186.713] [0.0000], Avg: [-2672.103 -2672.103] (0.020)
Step: 72649, Reward: [-3660.273 -3660.273] [0.0000], Avg: [-2672.783 -2672.783] (0.020)
Step: 72699, Reward: [-3828.406 -3828.406] [0.0000], Avg: [-2673.578 -2673.578] (0.020)
Step: 72749, Reward: [-2475.661 -2475.661] [0.0000], Avg: [-2673.442 -2673.442] (0.020)
Step: 72799, Reward: [-2473.818 -2473.818] [0.0000], Avg: [-2673.305 -2673.305] (0.020)
Step: 72849, Reward: [-3144.223 -3144.223] [0.0000], Avg: [-2673.628 -2673.628] (0.020)
Step: 72899, Reward: [-3214.919 -3214.919] [0.0000], Avg: [-2673.999 -2673.999] (0.020)
Step: 72949, Reward: [-2606.13 -2606.13] [0.0000], Avg: [-2673.953 -2673.953] (0.020)
Step: 72999, Reward: [-2161.581 -2161.581] [0.0000], Avg: [-2673.602 -2673.602] (0.020)
Step: 73049, Reward: [-3078.212 -3078.212] [0.0000], Avg: [-2673.879 -2673.879] (0.020)
Step: 73099, Reward: [-4192.658 -4192.658] [0.0000], Avg: [-2674.918 -2674.918] (0.020)
Step: 73149, Reward: [-3131.483 -3131.483] [0.0000], Avg: [-2675.23 -2675.23] (0.020)
Step: 73199, Reward: [-3243.86 -3243.86] [0.0000], Avg: [-2675.618 -2675.618] (0.020)
Step: 73249, Reward: [-3672.77 -3672.77] [0.0000], Avg: [-2676.299 -2676.299] (0.020)
Step: 73299, Reward: [-2662.284 -2662.284] [0.0000], Avg: [-2676.289 -2676.289] (0.020)
Step: 73349, Reward: [-3113.676 -3113.676] [0.0000], Avg: [-2676.587 -2676.587] (0.020)
Step: 73399, Reward: [-3378.264 -3378.264] [0.0000], Avg: [-2677.065 -2677.065] (0.020)
Step: 73449, Reward: [-2848.608 -2848.608] [0.0000], Avg: [-2677.182 -2677.182] (0.020)
Step: 73499, Reward: [-2129.613 -2129.613] [0.0000], Avg: [-2676.81 -2676.81] (0.020)
Step: 73549, Reward: [-2291.551 -2291.551] [0.0000], Avg: [-2676.548 -2676.548] (0.020)
Step: 73599, Reward: [-3056.636 -3056.636] [0.0000], Avg: [-2676.806 -2676.806] (0.020)
Step: 73649, Reward: [-4463.218 -4463.218] [0.0000], Avg: [-2678.019 -2678.019] (0.020)
Step: 73699, Reward: [-3952.767 -3952.767] [0.0000], Avg: [-2678.883 -2678.883] (0.020)
Step: 73749, Reward: [-3422.86 -3422.86] [0.0000], Avg: [-2679.388 -2679.388] (0.020)
Step: 73799, Reward: [-2006.745 -2006.745] [0.0000], Avg: [-2678.932 -2678.932] (0.020)
Step: 73849, Reward: [-2572.506 -2572.506] [0.0000], Avg: [-2678.86 -2678.86] (0.020)
Step: 73899, Reward: [-3577.953 -3577.953] [0.0000], Avg: [-2679.468 -2679.468] (0.020)
Step: 73949, Reward: [-2737.6 -2737.6] [0.0000], Avg: [-2679.508 -2679.508] (0.020)
Step: 73999, Reward: [-3614.522 -3614.522] [0.0000], Avg: [-2680.139 -2680.139] (0.020)
Step: 74049, Reward: [-2617.669 -2617.669] [0.0000], Avg: [-2680.097 -2680.097] (0.020)
Step: 74099, Reward: [-4592.649 -4592.649] [0.0000], Avg: [-2681.388 -2681.388] (0.020)
Step: 74149, Reward: [-3627.601 -3627.601] [0.0000], Avg: [-2682.026 -2682.026] (0.020)
Step: 74199, Reward: [-2749.477 -2749.477] [0.0000], Avg: [-2682.071 -2682.071] (0.020)
Step: 74249, Reward: [-3028.675 -3028.675] [0.0000], Avg: [-2682.305 -2682.305] (0.020)
Step: 74299, Reward: [-3165.306 -3165.306] [0.0000], Avg: [-2682.63 -2682.63] (0.020)
Step: 74349, Reward: [-2575.232 -2575.232] [0.0000], Avg: [-2682.558 -2682.558] (0.020)
Step: 74399, Reward: [-2669.595 -2669.595] [0.0000], Avg: [-2682.549 -2682.549] (0.020)
Step: 74449, Reward: [-3489.721 -3489.721] [0.0000], Avg: [-2683.091 -2683.091] (0.020)
Step: 74499, Reward: [-5149.897 -5149.897] [0.0000], Avg: [-2684.746 -2684.746] (0.020)
Step: 74549, Reward: [-3029.152 -3029.152] [0.0000], Avg: [-2684.977 -2684.977] (0.020)
Step: 74599, Reward: [-2340.236 -2340.236] [0.0000], Avg: [-2684.746 -2684.746] (0.020)
Step: 74649, Reward: [-3905.1 -3905.1] [0.0000], Avg: [-2685.564 -2685.564] (0.020)
Step: 74699, Reward: [-3034.283 -3034.283] [0.0000], Avg: [-2685.797 -2685.797] (0.020)
Step: 74749, Reward: [-3016.534 -3016.534] [0.0000], Avg: [-2686.018 -2686.018] (0.020)
Step: 74799, Reward: [-3661.658 -3661.658] [0.0000], Avg: [-2686.671 -2686.671] (0.020)
Step: 74849, Reward: [-2442.407 -2442.407] [0.0000], Avg: [-2686.507 -2686.507] (0.020)
Step: 74899, Reward: [-3794.867 -3794.867] [0.0000], Avg: [-2687.247 -2687.247] (0.020)
Step: 74949, Reward: [-4338.544 -4338.544] [0.0000], Avg: [-2688.349 -2688.349] (0.020)
Step: 74999, Reward: [-2726.978 -2726.978] [0.0000], Avg: [-2688.375 -2688.375] (0.020)
Step: 75049, Reward: [-2524.496 -2524.496] [0.0000], Avg: [-2688.266 -2688.266] (0.020)
Step: 75099, Reward: [-1870.885 -1870.885] [0.0000], Avg: [-2687.721 -2687.721] (0.020)
Step: 75149, Reward: [-3075.075 -3075.075] [0.0000], Avg: [-2687.979 -2687.979] (0.020)
Step: 75199, Reward: [-4486.124 -4486.124] [0.0000], Avg: [-2689.175 -2689.175] (0.020)
Step: 75249, Reward: [-3636.285 -3636.285] [0.0000], Avg: [-2689.804 -2689.804] (0.020)
Step: 75299, Reward: [-3841.087 -3841.087] [0.0000], Avg: [-2690.568 -2690.568] (0.020)
Step: 75349, Reward: [-4677.954 -4677.954] [0.0000], Avg: [-2691.887 -2691.887] (0.020)
Step: 75399, Reward: [-3396.815 -3396.815] [0.0000], Avg: [-2692.355 -2692.355] (0.020)
Step: 75449, Reward: [-1904.598 -1904.598] [0.0000], Avg: [-2691.833 -2691.833] (0.020)
Step: 75499, Reward: [-2658.792 -2658.792] [0.0000], Avg: [-2691.811 -2691.811] (0.020)
Step: 75549, Reward: [-3409.351 -3409.351] [0.0000], Avg: [-2692.286 -2692.286] (0.020)
Step: 75599, Reward: [-1672.036 -1672.036] [0.0000], Avg: [-2691.611 -2691.611] (0.020)
Step: 75649, Reward: [-3730.997 -3730.997] [0.0000], Avg: [-2692.298 -2692.298] (0.020)
Step: 75699, Reward: [-2442.913 -2442.913] [0.0000], Avg: [-2692.133 -2692.133] (0.020)
Step: 75749, Reward: [-1788.622 -1788.622] [0.0000], Avg: [-2691.537 -2691.537] (0.020)
Step: 75799, Reward: [-2194.509 -2194.509] [0.0000], Avg: [-2691.209 -2691.209] (0.020)
Step: 75849, Reward: [-2740.14 -2740.14] [0.0000], Avg: [-2691.241 -2691.241] (0.020)
Step: 75899, Reward: [-2496.54 -2496.54] [0.0000], Avg: [-2691.113 -2691.113] (0.020)
Step: 75949, Reward: [-3950.955 -3950.955] [0.0000], Avg: [-2691.942 -2691.942] (0.020)
Step: 75999, Reward: [-1794.928 -1794.928] [0.0000], Avg: [-2691.352 -2691.352] (0.020)
Step: 76049, Reward: [-2806.917 -2806.917] [0.0000], Avg: [-2691.428 -2691.428] (0.020)
Step: 76099, Reward: [-2919.877 -2919.877] [0.0000], Avg: [-2691.578 -2691.578] (0.020)
Step: 76149, Reward: [-3953.001 -3953.001] [0.0000], Avg: [-2692.406 -2692.406] (0.020)
Step: 76199, Reward: [-2866.275 -2866.275] [0.0000], Avg: [-2692.52 -2692.52] (0.020)
Step: 76249, Reward: [-3164.95 -3164.95] [0.0000], Avg: [-2692.83 -2692.83] (0.020)
Step: 76299, Reward: [-3517.57 -3517.57] [0.0000], Avg: [-2693.371 -2693.371] (0.020)
Step: 76349, Reward: [-1612.603 -1612.603] [0.0000], Avg: [-2692.663 -2692.663] (0.020)
Step: 76399, Reward: [-2888.321 -2888.321] [0.0000], Avg: [-2692.791 -2692.791] (0.020)
Step: 76449, Reward: [-2071. -2071.] [0.0000], Avg: [-2692.384 -2692.384] (0.020)
Step: 76499, Reward: [-3145.775 -3145.775] [0.0000], Avg: [-2692.681 -2692.681] (0.020)
Step: 76549, Reward: [-2980.073 -2980.073] [0.0000], Avg: [-2692.868 -2692.868] (0.020)
Step: 76599, Reward: [-2747.846 -2747.846] [0.0000], Avg: [-2692.904 -2692.904] (0.020)
Step: 76649, Reward: [-2725.081 -2725.081] [0.0000], Avg: [-2692.925 -2692.925] (0.020)
Step: 76699, Reward: [-2761.898 -2761.898] [0.0000], Avg: [-2692.97 -2692.97] (0.020)
Step: 76749, Reward: [-3035.744 -3035.744] [0.0000], Avg: [-2693.194 -2693.194] (0.020)
Step: 76799, Reward: [-2353.472 -2353.472] [0.0000], Avg: [-2692.972 -2692.972] (0.020)
Step: 76849, Reward: [-3147.471 -3147.471] [0.0000], Avg: [-2693.268 -2693.268] (0.020)
Step: 76899, Reward: [-2395.537 -2395.537] [0.0000], Avg: [-2693.074 -2693.074] (0.020)
Step: 76949, Reward: [-1851.919 -1851.919] [0.0000], Avg: [-2692.528 -2692.528] (0.020)
Step: 76999, Reward: [-2617.687 -2617.687] [0.0000], Avg: [-2692.479 -2692.479] (0.020)
Step: 77049, Reward: [-2968.429 -2968.429] [0.0000], Avg: [-2692.658 -2692.658] (0.020)
Step: 77099, Reward: [-2889.789 -2889.789] [0.0000], Avg: [-2692.786 -2692.786] (0.020)
Step: 77149, Reward: [-4482.935 -4482.935] [0.0000], Avg: [-2693.946 -2693.946] (0.020)
Step: 77199, Reward: [-3670.136 -3670.136] [0.0000], Avg: [-2694.579 -2694.579] (0.020)
Step: 77249, Reward: [-3043.213 -3043.213] [0.0000], Avg: [-2694.804 -2694.804] (0.020)
Step: 77299, Reward: [-2824.81 -2824.81] [0.0000], Avg: [-2694.888 -2694.888] (0.020)
Step: 77349, Reward: [-2188.915 -2188.915] [0.0000], Avg: [-2694.561 -2694.561] (0.020)
Step: 77399, Reward: [-3335.331 -3335.331] [0.0000], Avg: [-2694.975 -2694.975] (0.020)
Step: 77449, Reward: [-3330.962 -3330.962] [0.0000], Avg: [-2695.386 -2695.386] (0.020)
Step: 77499, Reward: [-2815.329 -2815.329] [0.0000], Avg: [-2695.463 -2695.463] (0.020)
Step: 77549, Reward: [-2310.696 -2310.696] [0.0000], Avg: [-2695.215 -2695.215] (0.020)
Step: 77599, Reward: [-3642.549 -3642.549] [0.0000], Avg: [-2695.826 -2695.826] (0.020)
Step: 77649, Reward: [-2083.626 -2083.626] [0.0000], Avg: [-2695.431 -2695.431] (0.020)
Step: 77699, Reward: [-1689.12 -1689.12] [0.0000], Avg: [-2694.784 -2694.784] (0.020)
Step: 77749, Reward: [-2915.258 -2915.258] [0.0000], Avg: [-2694.926 -2694.926] (0.020)
Step: 77799, Reward: [-2171.8 -2171.8] [0.0000], Avg: [-2694.589 -2694.589] (0.020)
Step: 77849, Reward: [-3661.751 -3661.751] [0.0000], Avg: [-2695.211 -2695.211] (0.020)
Step: 77899, Reward: [-3639.387 -3639.387] [0.0000], Avg: [-2695.817 -2695.817] (0.020)
Step: 77949, Reward: [-1728.291 -1728.291] [0.0000], Avg: [-2695.196 -2695.196] (0.020)
Step: 77999, Reward: [-1702.519 -1702.519] [0.0000], Avg: [-2694.56 -2694.56] (0.020)
Step: 78049, Reward: [-4773.12 -4773.12] [0.0000], Avg: [-2695.891 -2695.891] (0.020)
Step: 78099, Reward: [-2586.785 -2586.785] [0.0000], Avg: [-2695.821 -2695.821] (0.020)
Step: 78149, Reward: [-3190.686 -3190.686] [0.0000], Avg: [-2696.138 -2696.138] (0.020)
