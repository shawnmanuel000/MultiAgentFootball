Model: <class 'multiagent.maddpg.MADDPGAgent'>, Dir: simple_speaker_listener
num_envs: 16, state_size: [(3,), (11,)], action_size: [[3], [5]], action_space: [Discrete(3), Discrete(5)],

import torch
import random
import numpy as np
from models.ddpg import DDPGActor, DDPGCritic, DDPGNetwork
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, REG_LAMBDA, EPS_MIN

BATCH_SIZE = 32					# Number of samples to train on for each train step
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN

class MADDPGNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = state_size
		self.action_size = action_size
		self.agents = [DDPGNetwork(s_size, a_size, DDPGActor, lambda s,a: DDPGCritic([np.sum(state_size)], [np.sum(action_size)]), lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(state_size, action_size)]
		
	def get_action_probs(self, state, use_target=False, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			state = state.transpose(0, -len(self.state_size)).unsqueeze(-len(self.state_size)) if type(self.state_size[0]) == int else state
			action = [agent.get_action(s, use_target, grad, numpy, sample) for s,agent in zip(state, self.agents)]
			return (np.concatenate if numpy else torch.cat)(action, -2) if type(self.action_size[0]) == int else action

	def get_q_value(self, state, action, use_target=False, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			# out_dims = state.size()[:-len(self.state_size)]
			# state = state.view(-1, *state.size()[-len(self.state_size):]).transpose(0, -len(self.state_size)).unsqueeze(-len(self.state_size))
			# action = action.view(-1, *action.size()[-2:]).transpose(0, -2).unsqueeze(-2)
			q_value = [agent.get_q_value(state, action, use_target, grad, numpy) for agent in self.agents]
			return q_value #(np.concatenate if numpy else torch.cat)(q_value, -2).view(*out_dims, -1, 1)

	def optimize(self, states, actions, states_joint, actions_joint, q_targets):
		# states, critic_states, critic_actions, q_targets = [t.permute(2, 0, 1, *range(3,len(t.size()))).detach() for t in [states, critic_states, critic_actions, q_targets]]
		for (i,agent),state,q_target in zip(enumerate(self.agents), states, q_targets):
			q_values = agent.get_q_value(states_joint, actions_joint, grad=True, numpy=False)
			critic_error = q_values[:-1] - q_target.detach()
			critic_loss = critic_error.pow(2)
			agent.step(agent.critic_optimizer, critic_loss.mean())

			actor_action = agent.get_action(state, grad=True, numpy=False)
			critic_action = [actor_action if j==i else action.detach() for j,action in enumerate(actions)]
			q_actions = agent.critic_local(states_joint, torch.cat(critic_action, dim=-1))
			actor_loss = -(q_actions - q_values.detach())
			agent.step(agent.actor_optimizer, actor_loss.mean())

			agent.soft_copy(agent.actor_local, agent.actor_target)
			agent.soft_copy(agent.critic_local, agent.critic_target)

	def save_model(self, dirname="pytorch", name="best"):
		[PTACNetwork.save_model(agent, "maddpg", dirname, f"{name}_{i}") for i,agent in enumerate(self.agents)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[PTACNetwork.load_model(agent, "maddpg", dirname, f"{name}_{i}") for i,agent in enumerate(self.agents)]

class MADDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, MADDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		batch_dim = 0 if type(state) == list else 1# len(state[0].shape) - len(self.state_size[0])
		action_random = super().get_action(state)
		action_greedy = self.network.get_action_probs(self.to_tensor(state, batch_dim), sample=sample, grad=False, numpy=numpy)
		return action_random if random.random() < eps else action_greedy

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(lambda x: self.to_tensor(x,2), zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state, 1)
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, next_state)]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.network.get_action_probs(next_state, use_target=True, grad=False))]
			# if dones.size() != rewards.size(): dones = dones.unsqueeze(-1).repeat_interleave(self.state_size[0], dim=-1)

			# states_joint = states.view(*states.size()[:-len(self.state_size)], 1, -1).repeat_interleave(int(self.state_size[0]), dim=-2)
			# actions_joint = actions.view(*actions.size()[:-2], 1, -1).repeat_interleave(int(self.state_size[0]), dim=-2)
			# q_values = self.network.get_q_value(states_joint, actions_joint, grad=False)
			# q_targets, _ = self.compute_gae(q_values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), q_values[:-1])
			# self.network.optimize(states[:-1], states_joint[:-1], actions_joint[:-1], q_targets)
			states_joint = torch.cat(states, dim=-1)
			actions_joint = torch.cat(actions, dim=-1)
			q_values = self.network.get_q_value(states_joint, actions_joint, use_target=True, grad=False)
			q_targets = [self.compute_gae(q_value[-1], reward, done, q_value[:-1])[0] for q_value,reward,done in zip(q_values, rewards, dones)]
			self.network.optimize(states, actions, states_joint, actions_joint, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)


Step: 51, Reward: [-1234.852 -1234.852] [835.6009], Avg: [-2070.453 -2070.453] (0.995)
Step: 103, Reward: [-950.528 -950.528] [747.7782], Avg: [-1884.38 -1884.38] (0.990)
Step: 155, Reward: [-1057.139 -1057.139] [199.7352], Avg: [-1675.211 -1675.211] (0.985)
Step: 207, Reward: [-1313.167 -1313.167] [749.3108], Avg: [-1772.028 -1772.028] (0.980)
Step: 259, Reward: [-1474.114 -1474.114] [765.1476], Avg: [-1865.475 -1865.475] (0.975)
Step: 311, Reward: [-760. -760.] [70.4758], Avg: [-1692.975 -1692.975] (0.970)
Step: 363, Reward: [-1385.738 -1385.738] [486.5845], Avg: [-1718.596 -1718.596] (0.966)
Step: 415, Reward: [-1503.311 -1503.311] [689.5130], Avg: [-1777.874 -1777.874] (0.961)
Step: 467, Reward: [-2414.046 -2414.046] [915.5453], Avg: [-1950.287 -1950.287] (0.956)
Step: 519, Reward: [-809.508 -809.508] [286.1692], Avg: [-1864.826 -1864.826] (0.951)
Step: 571, Reward: [-728.199 -728.199] [558.4526], Avg: [-1812.265 -1812.265] (0.946)
Step: 623, Reward: [-216.881 -216.881] [110.5270], Avg: [-1688.527 -1688.527] (0.942)
Step: 675, Reward: [-1674.742 -1674.742] [968.8980], Avg: [-1761.997 -1761.997] (0.937)
Step: 727, Reward: [-2965.385 -2965.385] [42.0921], Avg: [-1850.96 -1850.96] (0.932)
Step: 779, Reward: [-1675.418 -1675.418] [1182.7735], Avg: [-1918.109 -1918.109] (0.928)
Step: 831, Reward: [-421.996 -421.996] [269.1097], Avg: [-1841.421 -1841.421] (0.923)
