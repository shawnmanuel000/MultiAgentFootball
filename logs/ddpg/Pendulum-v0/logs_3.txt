Model: <class 'models.ddpg.DDPGAgent'>, Dir: Pendulum-v0
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.98             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "Acrobot-v1", "Pendulum-v0"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[2]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:# or s % envs.env.spec.max_episode_steps == 0:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 199, Reward: -1299.8202 [87.66], Avg: -1387.4782 (0.980)
Step: 399, Reward: -1446.6284 [69.53], Avg: -1451.8196 (0.960)
Step: 599, Reward: -1395.6252 [131.23], Avg: -1476.8303 (0.941)
Step: 799, Reward: -1424.3164 [172.03], Avg: -1506.7093 (0.922)
Step: 999, Reward: -1247.1365 [108.40], Avg: -1476.4743 (0.904)
Step: 1199, Reward: -1251.2435 [94.48], Avg: -1454.6822 (0.886)
Step: 1399, Reward: -1241.7179 [152.19], Avg: -1445.9995 (0.868)
Step: 1599, Reward: -1245.7589 [188.32], Avg: -1444.5089 (0.851)
Step: 1799, Reward: -1200.3530 [139.61], Avg: -1432.8927 (0.834)
Step: 1999, Reward: -1202.4209 [261.11], Avg: -1435.9564 (0.817)
Step: 2199, Reward: -1307.6338 [293.58], Avg: -1450.9801 (0.801)
Step: 2399, Reward: -1235.7849 [116.26], Avg: -1442.7354 (0.785)
Step: 2599, Reward: -1262.8633 [184.86], Avg: -1443.1189 (0.769)
Step: 2799, Reward: -1225.7833 [153.63], Avg: -1438.5686 (0.754)
Step: 2999, Reward: -1324.6060 [309.54], Avg: -1451.6069 (0.739)
Step: 3199, Reward: -1283.9118 [125.99], Avg: -1449.0006 (0.724)
Step: 3399, Reward: -1337.8128 [144.20], Avg: -1450.9422 (0.709)
Step: 3599, Reward: -1382.1948 [161.73], Avg: -1456.1081 (0.695)
Step: 3799, Reward: -1224.8656 [194.11], Avg: -1454.1537 (0.681)
Step: 3999, Reward: -1299.9505 [234.96], Avg: -1458.1916 (0.668)
Step: 4199, Reward: -1169.6838 [33.67], Avg: -1446.0565 (0.654)
Step: 4399, Reward: -1376.0294 [255.37], Avg: -1454.4813 (0.641)
Step: 4599, Reward: -1263.8613 [147.52], Avg: -1452.6074 (0.628)
Step: 4799, Reward: -1225.2832 [228.05], Avg: -1452.6377 (0.616)
Step: 4999, Reward: -1106.1425 [116.05], Avg: -1443.4198 (0.603)
Step: 5199, Reward: -1075.6699 [103.76], Avg: -1433.2662 (0.591)
Step: 5399, Reward: -1208.9490 [298.86], Avg: -1436.0269 (0.580)
Step: 5599, Reward: -1231.0849 [196.00], Avg: -1435.7077 (0.568)
Step: 5799, Reward: -1075.9311 [269.88], Avg: -1432.6077 (0.557)
Step: 5999, Reward: -1262.5019 [209.88], Avg: -1433.9334 (0.545)
Step: 6199, Reward: -1085.5721 [101.10], Avg: -1425.9572 (0.535)
Step: 6399, Reward: -1411.5174 [276.82], Avg: -1434.1567 (0.524)
Step: 6599, Reward: -1203.1733 [233.11], Avg: -1434.2210 (0.513)
Step: 6799, Reward: -986.0604 [84.33], Avg: -1423.5201 (0.503)
Step: 6999, Reward: -1339.6919 [236.25], Avg: -1427.8751 (0.493)
Step: 7199, Reward: -1315.8611 [230.32], Avg: -1431.1614 (0.483)
Step: 7399, Reward: -1328.8789 [74.29], Avg: -1430.4049 (0.474)
Step: 7599, Reward: -1184.3299 [69.26], Avg: -1425.7518 (0.464)
Step: 7799, Reward: -1349.5875 [190.74], Avg: -1428.6896 (0.455)
Step: 7999, Reward: -1349.9680 [168.96], Avg: -1430.9456 (0.446)
Step: 8199, Reward: -940.6100 [115.92], Avg: -1421.8135 (0.437)
Step: 8399, Reward: -1117.9331 [144.43], Avg: -1418.0172 (0.428)
Step: 8599, Reward: -1334.0965 [287.00], Avg: -1422.7399 (0.419)
Step: 8799, Reward: -1139.4910 [161.26], Avg: -1419.9674 (0.411)
Step: 8999, Reward: -1026.7765 [198.90], Avg: -1415.6498 (0.403)
Step: 9199, Reward: -1085.3866 [219.07], Avg: -1413.2327 (0.395)
Step: 9399, Reward: -1073.6058 [164.41], Avg: -1409.5047 (0.387)
Step: 9599, Reward: -1031.6556 [109.89], Avg: -1403.9223 (0.379)
Step: 9799, Reward: -1097.7201 [113.70], Avg: -1399.9936 (0.372)
Step: 9999, Reward: -1037.0700 [99.76], Avg: -1394.7304 (0.364)
Step: 10199, Reward: -1051.5155 [151.34], Avg: -1390.9681 (0.357)
Step: 10399, Reward: -995.1411 [87.80], Avg: -1385.0445 (0.350)
Step: 10599, Reward: -962.2993 [58.47], Avg: -1378.1715 (0.343)
Step: 10799, Reward: -952.6550 [36.54], Avg: -1370.9682 (0.336)
Step: 10999, Reward: -941.2229 [112.22], Avg: -1365.1951 (0.329)
Step: 11199, Reward: -898.5305 [80.34], Avg: -1358.2964 (0.323)
Step: 11399, Reward: -817.3406 [105.11], Avg: -1350.6500 (0.316)
Step: 11599, Reward: -783.2586 [110.20], Avg: -1342.7673 (0.310)
Step: 11799, Reward: -715.6029 [47.04], Avg: -1332.9347 (0.304)
Step: 11999, Reward: -885.2179 [70.46], Avg: -1326.6470 (0.298)
Step: 12199, Reward: -846.8196 [87.63], Avg: -1320.2176 (0.292)
Step: 12399, Reward: -844.7373 [73.33], Avg: -1313.7314 (0.286)
Step: 12599, Reward: -885.0752 [119.01], Avg: -1308.8164 (0.280)
Step: 12799, Reward: -1280.6365 [70.72], Avg: -1309.4811 (0.274)
Step: 12999, Reward: -913.1549 [65.36], Avg: -1304.3893 (0.269)
Step: 13199, Reward: -1337.7870 [36.66], Avg: -1305.4507 (0.264)
Step: 13399, Reward: -1394.1446 [20.37], Avg: -1307.0785 (0.258)
Step: 13599, Reward: -1438.4492 [27.79], Avg: -1309.4191 (0.253)
Step: 13799, Reward: -704.6266 [150.44], Avg: -1302.8343 (0.248)
Step: 13999, Reward: -972.4079 [119.38], Avg: -1299.8193 (0.243)
Step: 14199, Reward: -1047.0265 [128.89], Avg: -1298.0742 (0.238)
Step: 14399, Reward: -979.7445 [110.76], Avg: -1295.1913 (0.233)
Step: 14599, Reward: -881.7038 [35.09], Avg: -1290.0077 (0.229)
Step: 14799, Reward: -1046.3345 [162.36], Avg: -1288.9089 (0.224)
Step: 14999, Reward: -975.0463 [87.59], Avg: -1285.8920 (0.220)
Step: 15199, Reward: -893.3059 [83.30], Avg: -1281.8224 (0.215)
Step: 15399, Reward: -945.1617 [91.22], Avg: -1278.6349 (0.211)
Step: 15599, Reward: -926.3166 [83.61], Avg: -1275.1899 (0.207)
Step: 15799, Reward: -813.7204 [53.49], Avg: -1270.0256 (0.203)
Step: 15999, Reward: -922.5558 [56.50], Avg: -1266.3884 (0.199)
Step: 16199, Reward: -956.2692 [105.19], Avg: -1263.8584 (0.195)
Step: 16399, Reward: -795.3388 [112.08], Avg: -1259.5116 (0.191)
Step: 16599, Reward: -832.4638 [138.57], Avg: -1256.0360 (0.187)
Step: 16799, Reward: -707.7026 [120.12], Avg: -1250.9382 (0.183)
Step: 16999, Reward: -733.6822 [171.79], Avg: -1246.8740 (0.180)
Step: 17199, Reward: -593.4688 [192.00], Avg: -1241.5088 (0.176)
Step: 17399, Reward: -801.3167 [89.10], Avg: -1237.4732 (0.172)
Step: 17599, Reward: -891.4634 [82.74], Avg: -1234.4815 (0.169)
Step: 17799, Reward: -801.7141 [55.72], Avg: -1230.2450 (0.166)
Step: 17999, Reward: -795.7424 [100.83], Avg: -1226.5376 (0.162)
Step: 18199, Reward: -855.7368 [52.66], Avg: -1223.0415 (0.159)
Step: 18399, Reward: -876.0509 [33.18], Avg: -1219.6305 (0.156)
Step: 18599, Reward: -814.3276 [47.10], Avg: -1215.7789 (0.153)
Step: 18799, Reward: -605.5366 [19.73], Avg: -1209.4968 (0.150)
Step: 18999, Reward: -517.8629 [82.43], Avg: -1203.0842 (0.147)
Step: 19199, Reward: -385.7333 [138.44], Avg: -1196.0122 (0.144)
Step: 19399, Reward: -551.5965 [116.37], Avg: -1190.5685 (0.141)
Step: 19599, Reward: -603.3694 [185.84], Avg: -1186.4729 (0.138)
Step: 19799, Reward: -366.1952 [264.09], Avg: -1180.8549 (0.135)
Step: 19999, Reward: -673.2349 [121.83], Avg: -1176.9970 (0.133)
Step: 20199, Reward: -547.2121 [196.19], Avg: -1172.7040 (0.130)
Step: 20399, Reward: -878.0817 [153.98], Avg: -1171.3251 (0.127)
Step: 20599, Reward: -654.4802 [41.81], Avg: -1166.7131 (0.125)
Step: 20799, Reward: -983.3949 [63.55], Avg: -1165.5615 (0.122)
Step: 20999, Reward: -1032.7834 [14.32], Avg: -1164.4333 (0.120)
Step: 21199, Reward: -434.0451 [210.30], Avg: -1159.5268 (0.117)
Step: 21399, Reward: -447.8372 [59.63], Avg: -1153.4328 (0.115)
Step: 21599, Reward: -760.6533 [74.79], Avg: -1150.4884 (0.113)
Step: 21799, Reward: -271.4849 [124.23], Avg: -1143.5639 (0.111)
Step: 21999, Reward: -587.8925 [82.95], Avg: -1139.2664 (0.108)
Step: 22199, Reward: -388.8965 [47.45], Avg: -1132.9338 (0.106)
Step: 22399, Reward: -406.0729 [173.94], Avg: -1127.9970 (0.104)
Step: 22599, Reward: -511.0604 [133.75], Avg: -1123.7211 (0.102)
Step: 22799, Reward: -321.4115 [146.90], Avg: -1117.9719 (0.100)
Step: 22999, Reward: -353.8793 [167.60], Avg: -1112.7850 (0.098)
Step: 23199, Reward: -280.2393 [163.68], Avg: -1107.0189 (0.096)
Step: 23399, Reward: -359.7697 [163.25], Avg: -1102.0274 (0.094)
Step: 23599, Reward: -339.8167 [140.15], Avg: -1096.7557 (0.092)
Step: 23799, Reward: -336.4034 [109.11], Avg: -1091.2831 (0.090)
Step: 23999, Reward: -220.1076 [153.02], Avg: -1085.2985 (0.089)
Step: 24199, Reward: -398.0655 [180.98], Avg: -1081.1146 (0.087)
Step: 24399, Reward: -249.1328 [133.70], Avg: -1075.3910 (0.085)
Step: 24599, Reward: -350.3552 [142.95], Avg: -1070.6586 (0.083)
Step: 24799, Reward: -273.4086 [95.86], Avg: -1065.0022 (0.082)
Step: 24999, Reward: -168.3750 [161.36], Avg: -1059.1200 (0.080)
Step: 25199, Reward: -371.4582 [109.74], Avg: -1054.5334 (0.078)
Step: 25399, Reward: -432.7550 [157.46], Avg: -1050.8773 (0.077)
Step: 25599, Reward: -187.2387 [169.58], Avg: -1045.4550 (0.075)
Step: 25799, Reward: -266.2006 [156.17], Avg: -1040.6249 (0.074)
Step: 25999, Reward: -190.2590 [117.70], Avg: -1034.9890 (0.072)
Step: 26199, Reward: -287.7049 [95.96], Avg: -1030.0170 (0.071)
Step: 26399, Reward: -193.3975 [158.54], Avg: -1024.8801 (0.069)
Step: 26599, Reward: -201.5239 [149.51], Avg: -1019.8136 (0.068)
Step: 26799, Reward: -271.5243 [48.11], Avg: -1014.5884 (0.067)
Step: 26999, Reward: -192.0155 [56.28], Avg: -1008.9121 (0.065)
Step: 27199, Reward: -281.4264 [155.76], Avg: -1004.7082 (0.064)
Step: 27399, Reward: -303.3623 [200.37], Avg: -1001.0515 (0.063)
Step: 27599, Reward: -140.6870 [132.94], Avg: -995.7803 (0.062)
Step: 27799, Reward: -316.0779 [61.28], Avg: -991.3312 (0.060)
Step: 27999, Reward: -167.1144 [117.90], Avg: -986.2860 (0.059)
Step: 28199, Reward: -282.6817 [56.52], Avg: -981.6968 (0.058)
Step: 28399, Reward: -170.7656 [57.41], Avg: -976.3903 (0.057)
Step: 28599, Reward: -193.5380 [124.35], Avg: -971.7854 (0.056)
Step: 28799, Reward: -181.8265 [151.66], Avg: -967.3528 (0.055)
Step: 28999, Reward: -190.5165 [91.51], Avg: -962.6264 (0.053)
Step: 29199, Reward: -189.5177 [58.26], Avg: -957.7302 (0.052)
Step: 29399, Reward: -168.8924 [57.84], Avg: -952.7574 (0.051)
Step: 29599, Reward: -301.2570 [138.99], Avg: -949.2945 (0.050)
Step: 29799, Reward: -166.6131 [93.37], Avg: -944.6682 (0.049)
Step: 29999, Reward: -95.4428 [87.02], Avg: -939.5868 (0.048)
Step: 30199, Reward: -257.2840 [106.13], Avg: -935.7711 (0.047)
Step: 30399, Reward: -180.0708 [121.20], Avg: -931.5967 (0.046)
Step: 30599, Reward: -183.3779 [113.87], Avg: -927.4507 (0.045)
Step: 30799, Reward: -142.7466 [86.70], Avg: -922.9182 (0.045)
Step: 30999, Reward: -120.0744 [126.61], Avg: -918.5554 (0.044)
Step: 31199, Reward: -73.1647 [59.06], Avg: -913.5148 (0.043)
Step: 31399, Reward: -163.1520 [50.45], Avg: -909.0568 (0.042)
Step: 31599, Reward: -118.8305 [125.95], Avg: -904.8525 (0.041)
Step: 31799, Reward: -122.4019 [74.50], Avg: -900.4000 (0.040)
Step: 31999, Reward: -145.3141 [47.65], Avg: -895.9785 (0.039)
Step: 32199, Reward: -190.2673 [58.21], Avg: -891.9568 (0.039)
Step: 32399, Reward: -291.1656 [285.29], Avg: -890.0093 (0.038)
Step: 32599, Reward: -211.0558 [43.70], Avg: -886.1120 (0.037)
Step: 32799, Reward: -488.8275 [325.51], Avg: -885.6743 (0.036)
Step: 32999, Reward: -142.6892 [41.61], Avg: -881.4236 (0.036)
Step: 33199, Reward: -421.9416 [317.14], Avg: -880.5661 (0.035)
Step: 33399, Reward: -213.0967 [47.52], Avg: -876.8538 (0.034)
Step: 33599, Reward: -294.6153 [301.96], Avg: -875.1855 (0.034)
Step: 33799, Reward: -169.0973 [124.60], Avg: -871.7448 (0.033)
Step: 33999, Reward: -121.6535 [75.11], Avg: -867.7743 (0.032)
Step: 34199, Reward: -359.9951 [337.80], Avg: -866.7803 (0.032)
Step: 34399, Reward: -470.2758 [312.87], Avg: -866.2940 (0.031)
Step: 34599, Reward: -316.2843 [291.29], Avg: -864.7985 (0.030)
Step: 34799, Reward: -579.8361 [454.23], Avg: -865.7713 (0.030)
Step: 34999, Reward: -146.1368 [88.82], Avg: -862.1667 (0.029)
Step: 35199, Reward: -502.4677 [386.83], Avg: -862.3208 (0.029)
Step: 35399, Reward: -199.1546 [102.51], Avg: -859.1533 (0.028)
Step: 35599, Reward: -163.7787 [117.40], Avg: -855.9063 (0.027)
Step: 35799, Reward: -300.2493 [372.77], Avg: -854.8845 (0.027)
Step: 35999, Reward: -185.6144 [86.87], Avg: -851.6490 (0.026)
Step: 36199, Reward: -179.8715 [80.27], Avg: -848.3810 (0.026)
Step: 36399, Reward: -24.5362 [48.28], Avg: -844.1196 (0.025)
Step: 36599, Reward: -165.8768 [93.19], Avg: -840.9225 (0.025)
Step: 36799, Reward: -96.9264 [48.18], Avg: -837.1410 (0.024)
Step: 36999, Reward: -115.2731 [101.81], Avg: -833.7893 (0.024)
Step: 37199, Reward: -96.7576 [47.10], Avg: -830.0799 (0.023)
Step: 37399, Reward: -168.8580 [53.92], Avg: -826.8324 (0.023)
Step: 37599, Reward: -120.0989 [105.00], Avg: -823.6316 (0.022)
Step: 37799, Reward: -188.4851 [85.60], Avg: -820.7240 (0.022)
Step: 37999, Reward: -220.1765 [100.49], Avg: -818.0921 (0.022)
Step: 38199, Reward: -165.3397 [53.70], Avg: -814.9557 (0.021)
Step: 38399, Reward: -74.7836 [58.80], Avg: -811.4069 (0.021)
Step: 38599, Reward: -133.5088 [119.65], Avg: -808.5145 (0.020)
Step: 38799, Reward: -209.9675 [113.00], Avg: -806.0116 (0.020)
Step: 38999, Reward: -99.2007 [48.38], Avg: -802.6351 (0.020)
Step: 39199, Reward: -124.8188 [108.59], Avg: -799.7309 (0.020)
Step: 39399, Reward: -170.5799 [95.09], Avg: -797.0199 (0.020)
Step: 39599, Reward: -98.7961 [48.41], Avg: -793.7380 (0.020)
Step: 39799, Reward: -149.7373 [46.81], Avg: -790.7370 (0.020)
Step: 39999, Reward: -140.3859 [111.09], Avg: -788.0407 (0.020)
Step: 40199, Reward: -220.3085 [120.72], Avg: -785.8168 (0.020)
Step: 40399, Reward: -240.7028 [110.88], Avg: -783.6671 (0.020)
Step: 40599, Reward: -192.6113 [61.17], Avg: -781.0568 (0.020)
Step: 40799, Reward: -122.7179 [75.03], Avg: -778.1975 (0.020)
Step: 40999, Reward: -142.0226 [110.03], Avg: -775.6309 (0.020)
Step: 41199, Reward: -211.3016 [136.28], Avg: -773.5530 (0.020)
Step: 41399, Reward: -169.4842 [96.81], Avg: -771.1025 (0.020)
Step: 41599, Reward: -170.9595 [122.69], Avg: -768.8071 (0.020)
Step: 41799, Reward: -188.3857 [88.58], Avg: -766.4538 (0.020)
Step: 41999, Reward: -100.2753 [46.87], Avg: -763.5047 (0.020)
Step: 42199, Reward: -129.2495 [143.88], Avg: -761.1806 (0.020)
Step: 42399, Reward: -213.2391 [80.63], Avg: -758.9764 (0.020)
Step: 42599, Reward: -97.0792 [47.50], Avg: -756.0919 (0.020)
Step: 42799, Reward: -221.5521 [127.67], Avg: -754.1906 (0.020)
Step: 42999, Reward: -219.3851 [94.83], Avg: -752.1442 (0.020)
Step: 43199, Reward: -173.7479 [57.34], Avg: -749.7319 (0.020)
Step: 43399, Reward: -205.8240 [78.03], Avg: -747.5850 (0.020)
Step: 43599, Reward: -170.5033 [119.58], Avg: -745.4864 (0.020)
Step: 43799, Reward: -144.5161 [41.75], Avg: -742.9328 (0.020)
Step: 43999, Reward: -102.7168 [93.58], Avg: -740.4481 (0.020)
Step: 44199, Reward: -144.2424 [42.43], Avg: -737.9423 (0.020)
Step: 44399, Reward: -95.4617 [84.94], Avg: -735.4309 (0.020)
Step: 44599, Reward: -206.4753 [111.38], Avg: -733.5584 (0.020)
Step: 44799, Reward: -199.4600 [65.66], Avg: -731.4671 (0.020)
Step: 44999, Reward: -74.1364 [58.76], Avg: -728.8068 (0.020)
Step: 45199, Reward: -209.3044 [104.47], Avg: -726.9704 (0.020)
Step: 45399, Reward: -216.7602 [118.45], Avg: -725.2446 (0.020)
Step: 45599, Reward: -144.1261 [135.83], Avg: -723.2915 (0.020)
Step: 45799, Reward: -148.3818 [94.38], Avg: -721.1931 (0.020)
Step: 45999, Reward: -198.9304 [101.26], Avg: -719.3627 (0.020)
Step: 46199, Reward: -155.8819 [105.59], Avg: -717.3805 (0.020)
Step: 46399, Reward: -195.1533 [61.74], Avg: -715.3957 (0.020)
Step: 46599, Reward: -188.8607 [88.77], Avg: -713.5168 (0.020)
Step: 46799, Reward: -211.3459 [80.37], Avg: -711.7142 (0.020)
Step: 46999, Reward: -120.6383 [72.07], Avg: -709.5057 (0.020)
Step: 47199, Reward: -134.9749 [123.88], Avg: -707.5962 (0.020)
Step: 47399, Reward: -170.0551 [58.42], Avg: -705.5746 (0.020)
Step: 47599, Reward: -252.2149 [73.50], Avg: -703.9785 (0.020)
Step: 47799, Reward: -73.8166 [58.28], Avg: -701.5857 (0.020)
Step: 47999, Reward: -188.0548 [117.30], Avg: -699.9348 (0.020)
Step: 48199, Reward: -163.4669 [55.47], Avg: -697.9389 (0.020)
Step: 48399, Reward: -186.4311 [115.09], Avg: -696.3008 (0.020)
Step: 48599, Reward: -141.3909 [88.05], Avg: -694.3796 (0.020)
Step: 48799, Reward: -138.7295 [83.15], Avg: -692.4431 (0.020)
Step: 48999, Reward: -234.0388 [101.59], Avg: -690.9868 (0.020)
Step: 49199, Reward: -163.5562 [81.57], Avg: -689.1743 (0.020)
Step: 49399, Reward: -120.9889 [72.55], Avg: -687.1677 (0.020)
Step: 49599, Reward: -92.3910 [85.87], Avg: -685.1156 (0.020)
Step: 49799, Reward: -175.5671 [71.30], Avg: -683.3556 (0.020)
Step: 49999, Reward: -144.4820 [43.98], Avg: -681.3760 (0.020)
Step: 50199, Reward: -95.9817 [47.35], Avg: -679.2324 (0.020)
Step: 50399, Reward: -184.2765 [117.55], Avg: -677.7347 (0.020)
Step: 50599, Reward: -164.3269 [54.92], Avg: -675.9225 (0.020)
Step: 50799, Reward: -167.4374 [52.95], Avg: -674.1291 (0.020)
Step: 50999, Reward: -122.5570 [3.32], Avg: -671.9791 (0.020)
Step: 51199, Reward: -96.2073 [46.13], Avg: -669.9102 (0.020)
Step: 51399, Reward: -181.8612 [108.99], Avg: -668.4353 (0.020)
Step: 51599, Reward: -164.3565 [92.23], Avg: -666.8389 (0.020)
Step: 51799, Reward: -140.4346 [86.87], Avg: -665.1419 (0.020)
Step: 51999, Reward: -70.8469 [57.20], Avg: -663.0761 (0.020)
Step: 52199, Reward: -140.1647 [84.84], Avg: -661.3977 (0.020)
Step: 52399, Reward: -222.1473 [91.03], Avg: -660.0686 (0.020)
Step: 52599, Reward: -163.7927 [58.26], Avg: -658.4032 (0.020)
Step: 52799, Reward: -94.1410 [47.07], Avg: -656.4441 (0.020)
Step: 52999, Reward: -138.0110 [130.05], Avg: -654.9785 (0.020)
Step: 53199, Reward: -95.2621 [47.14], Avg: -653.0515 (0.020)
Step: 53399, Reward: -165.9617 [55.56], Avg: -651.4353 (0.020)
Step: 53599, Reward: -137.0667 [128.17], Avg: -649.9943 (0.020)
Step: 53799, Reward: -241.7676 [132.70], Avg: -648.9700 (0.020)
Step: 53999, Reward: -259.9273 [81.22], Avg: -647.8299 (0.020)
Step: 54199, Reward: -188.4208 [57.15], Avg: -646.3456 (0.020)
Step: 54399, Reward: -271.0084 [87.15], Avg: -645.2861 (0.020)
Step: 54599, Reward: -145.4883 [88.94], Avg: -643.7811 (0.020)
Step: 54799, Reward: -71.9921 [56.08], Avg: -641.8990 (0.020)
Step: 54999, Reward: -117.4061 [73.37], Avg: -640.2585 (0.020)
Step: 55199, Reward: -138.8808 [85.69], Avg: -638.7524 (0.020)
Step: 55399, Reward: -181.6739 [80.11], Avg: -637.3915 (0.020)
Step: 55599, Reward: -208.4224 [133.36], Avg: -636.3281 (0.020)
Step: 55799, Reward: -95.6140 [88.62], Avg: -634.7077 (0.020)
Step: 55999, Reward: -146.1414 [91.73], Avg: -633.2905 (0.020)
Step: 56199, Reward: -145.7010 [47.54], Avg: -631.7244 (0.020)
Step: 56399, Reward: -234.0613 [69.92], Avg: -630.5622 (0.020)
Step: 56599, Reward: -119.5096 [76.01], Avg: -629.0250 (0.020)
Step: 56799, Reward: -164.2027 [116.28], Avg: -627.7977 (0.020)
Step: 56999, Reward: -141.1757 [39.21], Avg: -626.2278 (0.020)
Step: 57199, Reward: -74.7565 [59.93], Avg: -624.5092 (0.020)
Step: 57399, Reward: -183.5212 [55.31], Avg: -623.1653 (0.020)
Step: 57599, Reward: -97.1989 [87.90], Avg: -621.6443 (0.020)
Step: 57799, Reward: -122.1716 [75.85], Avg: -620.1785 (0.020)
Step: 57999, Reward: -120.9838 [106.53], Avg: -618.8244 (0.020)
Step: 58199, Reward: -120.6457 [73.80], Avg: -617.3661 (0.020)
Step: 58399, Reward: -97.2553 [48.41], Avg: -615.7507 (0.020)
Step: 58599, Reward: -144.7261 [46.61], Avg: -614.3022 (0.020)
Step: 58799, Reward: -167.6115 [60.22], Avg: -612.9877 (0.020)
Step: 58999, Reward: -122.1631 [77.83], Avg: -611.5877 (0.020)
Step: 59199, Reward: -98.4995 [48.50], Avg: -610.0181 (0.020)
Step: 59399, Reward: -143.4418 [46.04], Avg: -608.6022 (0.020)
Step: 59599, Reward: -139.2449 [129.45], Avg: -607.4616 (0.020)
Step: 59799, Reward: -145.3894 [44.10], Avg: -606.0637 (0.020)
Step: 59999, Reward: -141.5258 [43.80], Avg: -604.6612 (0.020)
Step: 60199, Reward: -141.0521 [114.79], Avg: -603.5024 (0.020)
Step: 60399, Reward: -140.3591 [47.81], Avg: -602.1271 (0.020)
Step: 60599, Reward: -191.8925 [99.32], Avg: -601.1010 (0.020)
Step: 60799, Reward: -119.8447 [1.51], Avg: -599.5229 (0.020)
Step: 60999, Reward: -212.9982 [81.56], Avg: -598.5230 (0.020)
Step: 61199, Reward: -162.3747 [109.94], Avg: -597.4570 (0.020)
Step: 61399, Reward: -120.6353 [1.95], Avg: -595.9101 (0.020)
Step: 61599, Reward: -135.1374 [104.60], Avg: -594.7537 (0.020)
Step: 61799, Reward: -119.9657 [74.85], Avg: -593.4594 (0.020)
Step: 61999, Reward: -137.7757 [81.34], Avg: -592.2519 (0.020)
Step: 62199, Reward: -119.9769 [2.92], Avg: -590.7427 (0.020)
Step: 62399, Reward: -95.7521 [88.38], Avg: -589.4395 (0.020)
Step: 62599, Reward: -93.0625 [85.47], Avg: -588.1267 (0.020)
Step: 62799, Reward: -237.0935 [102.75], Avg: -587.3360 (0.020)
Step: 62999, Reward: -140.4772 [86.28], Avg: -586.1913 (0.020)
Step: 63199, Reward: -165.5296 [55.11], Avg: -585.0345 (0.020)
Step: 63399, Reward: -97.1267 [46.47], Avg: -583.6419 (0.020)
Step: 63599, Reward: -195.2317 [58.24], Avg: -582.6037 (0.020)
Step: 63799, Reward: -168.2534 [56.82], Avg: -581.4829 (0.020)
Step: 63999, Reward: -146.0958 [48.14], Avg: -580.2727 (0.020)
Step: 64199, Reward: -98.8550 [49.11], Avg: -578.9260 (0.020)
Step: 64399, Reward: -160.9859 [88.10], Avg: -577.9016 (0.020)
Step: 64599, Reward: -120.9681 [74.59], Avg: -576.7179 (0.020)
Step: 64799, Reward: -167.7306 [53.34], Avg: -575.6202 (0.020)
Step: 64999, Reward: -121.2862 [72.39], Avg: -574.4450 (0.020)
Step: 65199, Reward: -191.6384 [55.05], Avg: -573.4396 (0.020)
Step: 65399, Reward: -209.5335 [109.86], Avg: -572.6627 (0.020)
Step: 65599, Reward: -97.4152 [89.82], Avg: -571.4876 (0.020)
Step: 65799, Reward: -183.7904 [81.50], Avg: -570.5569 (0.020)
Step: 65999, Reward: -199.7849 [102.25], Avg: -569.7432 (0.020)
Step: 66199, Reward: -207.4589 [82.32], Avg: -568.8974 (0.020)
Step: 66399, Reward: -148.4664 [50.08], Avg: -567.7819 (0.020)
Step: 66599, Reward: -123.3270 [74.76], Avg: -566.6717 (0.020)
Step: 66799, Reward: -145.3372 [87.56], Avg: -565.6724 (0.020)
Step: 66999, Reward: -159.4027 [104.70], Avg: -564.7722 (0.020)
Step: 67199, Reward: -156.2874 [106.82], Avg: -563.8743 (0.020)
Step: 67399, Reward: -151.4122 [97.87], Avg: -562.9408 (0.020)
Step: 67599, Reward: -142.7475 [46.72], Avg: -561.8358 (0.020)
Step: 67799, Reward: -171.9570 [58.23], Avg: -560.8575 (0.020)
Step: 67999, Reward: -121.8670 [76.94], Avg: -559.7927 (0.020)
Step: 68199, Reward: -241.6941 [67.23], Avg: -559.0570 (0.020)
Step: 68399, Reward: -142.0877 [46.06], Avg: -557.9724 (0.020)
Step: 68599, Reward: -101.9717 [50.16], Avg: -556.7892 (0.020)
Step: 68799, Reward: -189.6570 [61.10], Avg: -555.8996 (0.020)
Step: 68999, Reward: -124.5995 [3.38], Avg: -554.6592 (0.020)
Step: 69199, Reward: -170.9771 [120.59], Avg: -553.8988 (0.020)
Step: 69399, Reward: -75.8884 [60.12], Avg: -552.6946 (0.020)
Step: 69599, Reward: -164.8961 [56.31], Avg: -551.7420 (0.020)
Step: 69799, Reward: -168.1003 [54.52], Avg: -550.7990 (0.020)
Step: 69999, Reward: -171.0876 [57.71], Avg: -549.8790 (0.020)
Step: 70199, Reward: -167.6174 [55.05], Avg: -548.9467 (0.020)
Step: 70399, Reward: -188.0186 [55.97], Avg: -548.0804 (0.020)
Step: 70599, Reward: -165.4526 [51.55], Avg: -547.1425 (0.020)
Step: 70799, Reward: -143.9405 [115.97], Avg: -546.3311 (0.020)
Step: 70999, Reward: -147.8029 [54.70], Avg: -545.3625 (0.020)
Step: 71199, Reward: -169.0540 [96.03], Avg: -544.5753 (0.020)
Step: 71399, Reward: -173.2566 [56.64], Avg: -543.6938 (0.020)
Step: 71599, Reward: -100.5188 [48.90], Avg: -542.5925 (0.020)
Step: 71799, Reward: -96.6076 [47.57], Avg: -541.4827 (0.020)
Step: 71999, Reward: -139.7309 [41.69], Avg: -540.4825 (0.020)
Step: 72199, Reward: -144.4395 [42.61], Avg: -539.5035 (0.020)
Step: 72399, Reward: -191.2454 [90.48], Avg: -538.7914 (0.020)
Step: 72599, Reward: -208.3704 [129.33], Avg: -538.2374 (0.020)
Step: 72799, Reward: -196.0319 [61.60], Avg: -537.4665 (0.020)
Step: 72999, Reward: -188.5832 [55.56], Avg: -536.6629 (0.020)
Step: 73199, Reward: -144.0679 [47.31], Avg: -535.7195 (0.020)
Step: 73399, Reward: -119.1188 [72.14], Avg: -534.7809 (0.020)
Step: 73599, Reward: -119.4405 [2.03], Avg: -533.6578 (0.020)
Step: 73799, Reward: -156.8273 [130.16], Avg: -532.9893 (0.020)
Step: 73999, Reward: -211.7261 [113.48], Avg: -532.4277 (0.020)
Step: 74199, Reward: -121.4177 [4.97], Avg: -531.3333 (0.020)
Step: 74399, Reward: -122.6898 [5.04], Avg: -530.2483 (0.020)
Step: 74599, Reward: -178.3091 [73.11], Avg: -529.5008 (0.020)
Step: 74799, Reward: -98.2708 [48.28], Avg: -528.4768 (0.020)
Step: 74999, Reward: -141.5897 [46.26], Avg: -527.5685 (0.020)
Step: 75199, Reward: -142.7563 [42.08], Avg: -526.6570 (0.020)
Step: 75399, Reward: -232.4544 [99.45], Avg: -526.1404 (0.020)
Step: 75599, Reward: -117.4632 [70.82], Avg: -525.2466 (0.020)
Step: 75799, Reward: -142.7459 [87.60], Avg: -524.4685 (0.020)
Step: 75999, Reward: -187.4020 [83.81], Avg: -523.8020 (0.020)
Step: 76199, Reward: -148.1185 [53.64], Avg: -522.9568 (0.020)
Step: 76399, Reward: -169.8574 [57.00], Avg: -522.1816 (0.020)
Step: 76599, Reward: -148.1926 [50.70], Avg: -521.3375 (0.020)
Step: 76799, Reward: -187.9117 [120.68], Avg: -520.7835 (0.020)
Step: 76999, Reward: -164.0520 [86.01], Avg: -520.0803 (0.020)
Step: 77199, Reward: -163.2767 [57.31], Avg: -519.3045 (0.020)
Step: 77399, Reward: -150.9578 [61.87], Avg: -518.5125 (0.020)
Step: 77599, Reward: -190.6421 [93.84], Avg: -517.9094 (0.020)
Step: 77799, Reward: -121.4945 [76.87], Avg: -517.0879 (0.020)
Step: 77999, Reward: -213.2336 [83.28], Avg: -516.5223 (0.020)
Step: 78199, Reward: -208.7671 [82.53], Avg: -515.9463 (0.020)
Step: 78399, Reward: -183.0441 [83.03], Avg: -515.3088 (0.020)
Step: 78599, Reward: -189.4971 [116.95], Avg: -514.7774 (0.020)
Step: 78799, Reward: -235.9100 [101.72], Avg: -514.3278 (0.020)
Step: 78999, Reward: -97.5965 [48.61], Avg: -513.3958 (0.020)
Step: 79199, Reward: -73.9901 [58.81], Avg: -512.4347 (0.020)
Step: 79399, Reward: -72.4295 [57.49], Avg: -511.4712 (0.020)
Step: 79599, Reward: -123.3709 [76.24], Avg: -510.6877 (0.020)
Step: 79799, Reward: -121.6616 [73.20], Avg: -509.8961 (0.020)
Step: 79999, Reward: -173.8261 [64.39], Avg: -509.2169 (0.020)
Step: 80199, Reward: -183.8295 [78.88], Avg: -508.6022 (0.020)
Step: 80399, Reward: -117.4926 [103.06], Avg: -507.8856 (0.020)
Step: 80599, Reward: -237.9502 [109.59], Avg: -507.4878 (0.020)
Step: 80799, Reward: -147.4531 [49.83], Avg: -506.7199 (0.020)
Step: 80999, Reward: -170.0686 [96.85], Avg: -506.1278 (0.020)
Step: 81199, Reward: -209.2123 [70.62], Avg: -505.5704 (0.020)
Step: 81399, Reward: -143.3587 [44.06], Avg: -504.7887 (0.020)
Step: 81599, Reward: -157.2867 [109.50], Avg: -504.2054 (0.020)
Step: 81799, Reward: -207.2077 [72.63], Avg: -503.6568 (0.020)
Step: 81999, Reward: -168.1714 [58.84], Avg: -502.9821 (0.020)
Step: 82199, Reward: -176.9944 [67.10], Avg: -502.3522 (0.020)
Step: 82399, Reward: -73.9395 [58.73], Avg: -501.4549 (0.020)
Step: 82599, Reward: -120.1475 [1.56], Avg: -500.5354 (0.020)
Step: 82799, Reward: -149.4826 [49.54], Avg: -499.8071 (0.020)
Step: 82999, Reward: -218.2957 [89.42], Avg: -499.3442 (0.020)
Step: 83199, Reward: -120.1289 [71.86], Avg: -498.6054 (0.020)
Step: 83399, Reward: -71.7717 [92.58], Avg: -497.8038 (0.020)
Step: 83599, Reward: -148.6415 [49.06], Avg: -497.0859 (0.020)
Step: 83799, Reward: -146.5860 [41.64], Avg: -496.3488 (0.020)
Step: 83999, Reward: -142.6045 [43.70], Avg: -495.6106 (0.020)
Step: 84199, Reward: -97.8971 [48.60], Avg: -494.7813 (0.020)
Step: 84399, Reward: -73.2425 [57.68], Avg: -493.9191 (0.020)
Step: 84599, Reward: -148.9015 [54.30], Avg: -493.2318 (0.020)
Step: 84799, Reward: -208.7827 [46.84], Avg: -492.6714 (0.020)
Step: 84999, Reward: -148.2809 [45.07], Avg: -491.9671 (0.020)
Step: 85199, Reward: -188.2220 [88.89], Avg: -491.4628 (0.020)
Step: 85399, Reward: -121.2406 [72.56], Avg: -490.7657 (0.020)
Step: 85599, Reward: -192.1108 [54.29], Avg: -490.1947 (0.020)
Step: 85799, Reward: -194.4674 [91.91], Avg: -489.7196 (0.020)
Step: 85999, Reward: -100.2684 [49.65], Avg: -488.9294 (0.020)
Step: 86199, Reward: -123.0523 [74.43], Avg: -488.2532 (0.020)
Step: 86399, Reward: -120.6015 [75.44], Avg: -487.5768 (0.020)
Step: 86599, Reward: -170.8039 [91.23], Avg: -487.0559 (0.020)
Step: 86799, Reward: -103.0106 [51.10], Avg: -486.2887 (0.020)
Step: 86999, Reward: -145.2861 [42.82], Avg: -485.6033 (0.020)
Step: 87199, Reward: -121.3820 [74.86], Avg: -484.9396 (0.020)
Step: 87399, Reward: -264.3315 [89.88], Avg: -484.6404 (0.020)
Step: 87599, Reward: -98.2774 [48.62], Avg: -483.8693 (0.020)
Step: 87799, Reward: -142.6830 [86.88], Avg: -483.2901 (0.020)
Step: 87999, Reward: -117.9285 [74.40], Avg: -482.6288 (0.020)
Step: 88199, Reward: -143.9606 [45.70], Avg: -481.9645 (0.020)
Step: 88399, Reward: -147.3870 [42.29], Avg: -481.3032 (0.020)
Step: 88599, Reward: -215.8250 [90.70], Avg: -480.9086 (0.020)
Step: 88799, Reward: -186.1110 [115.42], Avg: -480.5046 (0.020)
Step: 88999, Reward: -121.0508 [4.48], Avg: -479.7069 (0.020)
Step: 89199, Reward: -226.4056 [88.28], Avg: -479.3369 (0.020)
Step: 89399, Reward: -163.0324 [85.11], Avg: -478.8197 (0.020)
Step: 89599, Reward: -146.7818 [41.42], Avg: -478.1710 (0.020)
Step: 89799, Reward: -236.2040 [103.34], Avg: -477.8623 (0.020)
Step: 89999, Reward: -159.3079 [106.58], Avg: -477.3912 (0.020)
Step: 90199, Reward: -120.3703 [75.06], Avg: -476.7660 (0.020)
Step: 90399, Reward: -119.6928 [75.21], Avg: -476.1424 (0.020)
Step: 90599, Reward: -176.7286 [103.16], Avg: -475.7092 (0.020)
Step: 90799, Reward: -98.4434 [48.43], Avg: -474.9849 (0.020)
Step: 90999, Reward: -124.5088 [79.64], Avg: -474.3896 (0.020)
Step: 91199, Reward: -237.8293 [67.96], Avg: -474.0199 (0.020)
Step: 91399, Reward: -101.0270 [49.81], Avg: -473.3127 (0.020)
Step: 91599, Reward: -141.4491 [86.46], Avg: -472.7769 (0.020)
Step: 91799, Reward: -51.6059 [61.02], Avg: -471.9923 (0.020)
Step: 91999, Reward: -169.6560 [95.51], Avg: -471.5426 (0.020)
Step: 92199, Reward: -146.7898 [89.04], Avg: -471.0313 (0.020)
Step: 92399, Reward: -119.0418 [72.53], Avg: -470.4264 (0.020)
Step: 92599, Reward: -188.5879 [56.29], Avg: -469.9393 (0.020)
Step: 92799, Reward: -96.5288 [86.93], Avg: -469.3219 (0.020)
Step: 92999, Reward: -140.9683 [41.91], Avg: -468.7059 (0.020)
Step: 93199, Reward: -166.4117 [85.96], Avg: -468.2416 (0.020)
Step: 93399, Reward: -74.7146 [58.74], Avg: -467.5248 (0.020)
Step: 93599, Reward: -125.8092 [110.43], Avg: -467.0306 (0.020)
Step: 93799, Reward: -215.1316 [93.38], Avg: -466.6926 (0.020)
Step: 93999, Reward: -124.8195 [2.44], Avg: -465.9704 (0.020)
Step: 94199, Reward: -121.5050 [72.92], Avg: -465.3939 (0.020)
Step: 94399, Reward: -220.4373 [52.68], Avg: -464.9865 (0.020)
Step: 94599, Reward: -96.3958 [85.95], Avg: -464.3889 (0.020)
Step: 94799, Reward: -147.3226 [46.97], Avg: -463.8191 (0.020)
Step: 94999, Reward: -145.4292 [46.36], Avg: -463.2464 (0.020)
Step: 95199, Reward: -188.2402 [56.99], Avg: -462.7884 (0.020)
Step: 95399, Reward: -98.3285 [87.78], Avg: -462.2084 (0.020)
Step: 95599, Reward: -143.5862 [46.62], Avg: -461.6393 (0.020)
Step: 95799, Reward: -136.5550 [104.13], Avg: -461.1780 (0.020)
Step: 95999, Reward: -166.6877 [57.55], Avg: -460.6844 (0.020)
Step: 96199, Reward: -171.9206 [124.85], Avg: -460.3436 (0.020)
Step: 96399, Reward: -170.5150 [61.41], Avg: -459.8697 (0.020)
Step: 96599, Reward: -164.8167 [57.18], Avg: -459.3772 (0.020)
Step: 96799, Reward: -98.5100 [47.88], Avg: -458.7305 (0.020)
Step: 96999, Reward: -207.7529 [78.03], Avg: -458.3739 (0.020)
Step: 97199, Reward: -72.5865 [94.57], Avg: -457.7747 (0.020)
Step: 97399, Reward: -123.4977 [74.06], Avg: -457.2404 (0.020)
Step: 97599, Reward: -122.0510 [0.70], Avg: -456.5550 (0.020)
Step: 97799, Reward: -216.2735 [92.80], Avg: -456.2534 (0.020)
Step: 97999, Reward: -124.6270 [5.87], Avg: -455.5886 (0.020)
Step: 98199, Reward: -122.0680 [76.89], Avg: -455.0659 (0.020)
Step: 98399, Reward: -123.1742 [3.25], Avg: -454.3980 (0.020)
Step: 98599, Reward: -169.5939 [60.50], Avg: -453.9430 (0.020)
Step: 98799, Reward: -145.6034 [46.09], Avg: -453.4121 (0.020)
Step: 98999, Reward: -192.0544 [54.64], Avg: -452.9945 (0.020)
Step: 99199, Reward: -165.0130 [54.41], Avg: -452.5236 (0.020)
Step: 99399, Reward: -122.5206 [75.05], Avg: -452.0106 (0.020)
Step: 99599, Reward: -146.3080 [43.83], Avg: -451.4847 (0.020)
Step: 99799, Reward: -169.0344 [125.05], Avg: -451.1693 (0.020)
Step: 99999, Reward: -175.1850 [69.57], Avg: -450.7565 (0.020)
