Model: <class 'models.ddpg.DDPGAgent'>, Dir: Pendulum-v0
num_envs: 16, state_size: (3,), action_size: (1,), action_space: Box(1,),

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, EPS_DECAY, REPLAY_BATCH_SIZE

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, decay=EPS_DECAY, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=False)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[3]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 199, Reward: -1288.2235 [136.31], Avg: -1424.5359 (0.980)
Step: 399, Reward: -1358.8622 [179.98], Avg: -1481.6882 (0.960)
Step: 599, Reward: -1330.4382 [45.70], Avg: -1446.5062 (0.941)
Step: 799, Reward: -1274.5215 [86.24], Avg: -1425.0692 (0.922)
Step: 999, Reward: -1242.5206 [102.70], Avg: -1409.0995 (0.904)
Step: 1199, Reward: -1311.2414 [69.58], Avg: -1404.3865 (0.886)
Step: 1399, Reward: -1264.1989 [173.94], Avg: -1409.2081 (0.868)
Step: 1599, Reward: -1285.8937 [136.31], Avg: -1410.8322 (0.851)
Step: 1799, Reward: -1375.2209 [135.38], Avg: -1421.9181 (0.834)
Step: 1999, Reward: -1270.1317 [118.15], Avg: -1418.5541 (0.817)
Step: 2199, Reward: -1271.0263 [170.48], Avg: -1420.6408 (0.801)
Step: 2399, Reward: -1402.7887 [173.83], Avg: -1433.6389 (0.785)
Step: 2599, Reward: -1303.9084 [86.64], Avg: -1430.3244 (0.769)
Step: 2799, Reward: -1645.8569 [167.90], Avg: -1457.7127 (0.754)
Step: 2999, Reward: -1369.8076 [175.51], Avg: -1463.5531 (0.739)
Step: 3199, Reward: -1423.7261 [109.22], Avg: -1467.8900 (0.724)
Step: 3399, Reward: -1365.1449 [104.29], Avg: -1467.9809 (0.709)
Step: 3599, Reward: -1219.4932 [159.79], Avg: -1463.0530 (0.695)
Step: 3799, Reward: -1429.9304 [166.74], Avg: -1470.0855 (0.681)
Step: 3999, Reward: -1094.9555 [98.05], Avg: -1456.2318 (0.668)
Step: 4199, Reward: -1463.3658 [240.02], Avg: -1468.0009 (0.654)
Step: 4399, Reward: -1227.3921 [276.82], Avg: -1469.6470 (0.641)
Step: 4599, Reward: -1282.7927 [193.88], Avg: -1469.9525 (0.628)
Step: 4799, Reward: -1243.1498 [64.05], Avg: -1463.1711 (0.616)
Step: 4999, Reward: -1461.2133 [214.89], Avg: -1471.6884 (0.603)
Step: 5199, Reward: -1491.9650 [80.46], Avg: -1475.5630 (0.591)
Step: 5399, Reward: -1411.7522 [335.14], Avg: -1485.6123 (0.580)
Step: 5599, Reward: -1276.4286 [203.31], Avg: -1485.4026 (0.568)
Step: 5799, Reward: -1306.1225 [258.11], Avg: -1488.1209 (0.557)
Step: 5999, Reward: -1178.7472 [183.32], Avg: -1483.9191 (0.545)
Step: 6199, Reward: -1279.1677 [165.91], Avg: -1482.6663 (0.535)
Step: 6399, Reward: -1124.3280 [145.29], Avg: -1476.0085 (0.524)
Step: 6599, Reward: -1174.8500 [199.52], Avg: -1472.9286 (0.513)
Step: 6799, Reward: -1237.5709 [204.29], Avg: -1472.0150 (0.503)
Step: 6999, Reward: -1497.1881 [209.84], Avg: -1478.7297 (0.493)
Step: 7199, Reward: -1300.2105 [191.69], Avg: -1479.0956 (0.483)
Step: 7399, Reward: -1565.8365 [61.16], Avg: -1483.0929 (0.474)
Step: 7599, Reward: -1327.2503 [210.20], Avg: -1484.5233 (0.464)
Step: 7799, Reward: -1175.6622 [122.04], Avg: -1479.7329 (0.455)
Step: 7999, Reward: -1394.0119 [156.77], Avg: -1481.5091 (0.446)
Step: 8199, Reward: -1320.3047 [131.77], Avg: -1480.7911 (0.437)
Step: 8399, Reward: -1406.2963 [172.57], Avg: -1483.1263 (0.428)
Step: 8599, Reward: -1284.0337 [206.92], Avg: -1483.3084 (0.419)
Step: 8799, Reward: -1306.9317 [91.78], Avg: -1481.3856 (0.411)
Step: 8999, Reward: -1489.0095 [156.52], Avg: -1485.0333 (0.403)
Step: 9199, Reward: -1292.2245 [178.42], Avg: -1484.7204 (0.395)
Step: 9399, Reward: -1284.2524 [168.91], Avg: -1484.0491 (0.387)
Step: 9599, Reward: -1182.9907 [73.86], Avg: -1479.3158 (0.379)
Step: 9799, Reward: -1396.2034 [192.47], Avg: -1481.5475 (0.372)
Step: 9999, Reward: -1039.4627 [167.85], Avg: -1476.0628 (0.364)
Step: 10199, Reward: -1209.8083 [204.12], Avg: -1474.8444 (0.357)
Step: 10399, Reward: -1015.5740 [144.88], Avg: -1468.7984 (0.350)
Step: 10599, Reward: -1359.4793 [230.86], Avg: -1471.0916 (0.343)
Step: 10799, Reward: -1319.8479 [173.47], Avg: -1471.5032 (0.336)
Step: 10999, Reward: -1304.1399 [17.64], Avg: -1468.7810 (0.329)
Step: 11199, Reward: -1341.7916 [173.07], Avg: -1469.6039 (0.323)
Step: 11399, Reward: -1238.7199 [191.50], Avg: -1468.9129 (0.316)
Step: 11599, Reward: -1235.2585 [80.99], Avg: -1466.2807 (0.310)
Step: 11799, Reward: -1398.6842 [181.11], Avg: -1468.2047 (0.304)
Step: 11999, Reward: -1028.6296 [72.27], Avg: -1462.0830 (0.298)
Step: 12199, Reward: -1081.3990 [100.05], Avg: -1457.4824 (0.292)
Step: 12399, Reward: -929.8784 [66.06], Avg: -1450.0382 (0.286)
Step: 12599, Reward: -946.4162 [89.31], Avg: -1443.4618 (0.280)
Step: 12799, Reward: -1296.0070 [60.59], Avg: -1442.1046 (0.274)
Step: 12999, Reward: -938.2385 [52.17], Avg: -1435.1554 (0.269)
Step: 13199, Reward: -1224.2507 [44.51], Avg: -1432.6342 (0.264)
Step: 13399, Reward: -1266.3434 [31.52], Avg: -1430.6228 (0.258)
Step: 13599, Reward: -1175.3793 [38.42], Avg: -1427.4343 (0.253)
Step: 13799, Reward: -1092.7384 [104.98], Avg: -1424.1050 (0.248)
Step: 13999, Reward: -1232.4389 [228.05], Avg: -1424.6247 (0.243)
Step: 14199, Reward: -1125.9124 [70.52], Avg: -1421.4108 (0.238)
Step: 14399, Reward: -988.5678 [44.74], Avg: -1416.0205 (0.233)
Step: 14599, Reward: -1141.1738 [42.82], Avg: -1412.8420 (0.229)
Step: 14799, Reward: -1060.4567 [89.14], Avg: -1409.2846 (0.224)
Step: 14999, Reward: -946.4823 [39.42], Avg: -1403.6395 (0.220)
Step: 15199, Reward: -978.1950 [51.93], Avg: -1398.7249 (0.215)
Step: 15399, Reward: -915.5431 [87.41], Avg: -1393.5850 (0.211)
Step: 15599, Reward: -900.0166 [107.10], Avg: -1388.6303 (0.207)
Step: 15799, Reward: -914.7070 [57.69], Avg: -1383.3615 (0.203)
Step: 15999, Reward: -1038.5186 [32.45], Avg: -1379.4566 (0.199)
Step: 16199, Reward: -965.5595 [101.01], Avg: -1375.5938 (0.195)
Step: 16399, Reward: -1047.5863 [28.43], Avg: -1371.9404 (0.191)
Step: 16599, Reward: -924.6247 [106.81], Avg: -1367.8379 (0.187)
Step: 16799, Reward: -993.8269 [109.63], Avg: -1364.6905 (0.183)
Step: 16999, Reward: -955.9367 [98.62], Avg: -1361.0419 (0.180)
Step: 17199, Reward: -1082.1431 [137.34], Avg: -1359.3959 (0.176)
Step: 17399, Reward: -1118.9956 [266.90], Avg: -1359.7005 (0.172)
Step: 17599, Reward: -917.2763 [45.38], Avg: -1355.1887 (0.169)
Step: 17799, Reward: -1060.1898 [37.56], Avg: -1352.2960 (0.166)
Step: 17999, Reward: -1075.0011 [32.75], Avg: -1349.5789 (0.162)
Step: 18199, Reward: -1002.8166 [93.53], Avg: -1346.7961 (0.159)
Step: 18399, Reward: -1034.4572 [35.73], Avg: -1343.7895 (0.156)
Step: 18599, Reward: -1031.5121 [45.99], Avg: -1340.9262 (0.153)
Step: 18799, Reward: -865.1927 [17.52], Avg: -1336.0515 (0.150)
Step: 18999, Reward: -865.8009 [69.91], Avg: -1331.8374 (0.147)
Step: 19199, Reward: -882.4910 [10.27], Avg: -1327.2637 (0.144)
Step: 19399, Reward: -1196.6972 [53.31], Avg: -1326.4672 (0.141)
Step: 19599, Reward: -1111.6786 [29.34], Avg: -1324.5749 (0.138)
Step: 19799, Reward: -1124.9298 [46.01], Avg: -1323.0230 (0.135)
Step: 19999, Reward: -1097.7594 [42.84], Avg: -1321.1988 (0.133)
Step: 20199, Reward: -942.4537 [42.48], Avg: -1317.8695 (0.130)
Step: 20399, Reward: -881.5342 [4.54], Avg: -1313.6361 (0.127)
Step: 20599, Reward: -869.9455 [68.76], Avg: -1309.9960 (0.125)
Step: 20799, Reward: -888.3048 [79.28], Avg: -1306.7036 (0.122)
Step: 20999, Reward: -910.9357 [77.94], Avg: -1303.6767 (0.120)
Step: 21199, Reward: -850.9594 [37.15], Avg: -1299.7562 (0.117)
Step: 21399, Reward: -886.9772 [82.63], Avg: -1296.6707 (0.115)
Step: 21599, Reward: -857.7141 [29.53], Avg: -1292.8797 (0.113)
Step: 21799, Reward: -959.5486 [57.54], Avg: -1290.3495 (0.111)
Step: 21999, Reward: -888.7668 [21.78], Avg: -1286.8968 (0.108)
Step: 22199, Reward: -992.0257 [86.65], Avg: -1285.0210 (0.106)
Step: 22399, Reward: -1003.7033 [43.34], Avg: -1282.8961 (0.104)
Step: 22599, Reward: -996.3954 [46.20], Avg: -1280.7696 (0.102)
Step: 22799, Reward: -839.1407 [57.10], Avg: -1277.3965 (0.100)
Step: 22999, Reward: -941.4005 [54.21], Avg: -1274.9462 (0.098)
Step: 23199, Reward: -926.2044 [53.58], Avg: -1272.4017 (0.096)
Step: 23399, Reward: -911.6345 [73.91], Avg: -1269.9499 (0.094)
Step: 23599, Reward: -863.5379 [12.93], Avg: -1266.6154 (0.092)
Step: 23799, Reward: -937.9273 [93.82], Avg: -1264.6417 (0.090)
Step: 23999, Reward: -839.5461 [119.02], Avg: -1262.0911 (0.089)
Step: 24199, Reward: -809.0978 [54.18], Avg: -1258.7952 (0.087)
Step: 24399, Reward: -836.9826 [79.69], Avg: -1255.9909 (0.085)
Step: 24599, Reward: -868.8580 [123.12], Avg: -1253.8445 (0.083)
Step: 24799, Reward: -719.4034 [44.18], Avg: -1249.8907 (0.082)
Step: 24999, Reward: -855.6592 [59.99], Avg: -1247.2167 (0.080)
Step: 25199, Reward: -850.6999 [10.46], Avg: -1244.1528 (0.078)
Step: 25399, Reward: -968.2181 [69.90], Avg: -1242.5305 (0.077)
Step: 25599, Reward: -887.4506 [105.28], Avg: -1240.5789 (0.075)
Step: 25799, Reward: -912.0168 [106.56], Avg: -1238.8580 (0.074)
Step: 25999, Reward: -941.3265 [73.69], Avg: -1237.1362 (0.072)
Step: 26199, Reward: -849.1970 [62.81], Avg: -1234.6543 (0.071)
Step: 26399, Reward: -954.6167 [110.58], Avg: -1233.3706 (0.069)
Step: 26599, Reward: -749.9524 [107.12], Avg: -1230.5413 (0.068)
Step: 26799, Reward: -815.5044 [75.37], Avg: -1228.0064 (0.067)
Step: 26999, Reward: -791.8021 [51.72], Avg: -1225.1584 (0.065)
Step: 27199, Reward: -829.2671 [40.07], Avg: -1222.5421 (0.064)
Step: 27399, Reward: -803.0956 [47.67], Avg: -1219.8284 (0.063)
Step: 27599, Reward: -692.4953 [44.94], Avg: -1216.3328 (0.062)
Step: 27799, Reward: -724.8731 [101.92], Avg: -1213.5303 (0.060)
Step: 27999, Reward: -682.2037 [85.12], Avg: -1210.3431 (0.059)
Step: 28199, Reward: -782.6812 [56.97], Avg: -1207.7141 (0.058)
Step: 28399, Reward: -783.9342 [83.07], Avg: -1205.3148 (0.057)
Step: 28599, Reward: -701.1554 [70.03], Avg: -1202.2789 (0.056)
Step: 28799, Reward: -917.3587 [54.50], Avg: -1200.6787 (0.055)
Step: 28999, Reward: -857.1330 [63.89], Avg: -1198.7501 (0.053)
Step: 29199, Reward: -741.8384 [74.65], Avg: -1196.1318 (0.052)
Step: 29399, Reward: -834.1839 [45.54], Avg: -1193.9794 (0.051)
Step: 29599, Reward: -774.5954 [49.81], Avg: -1191.4823 (0.050)
Step: 29799, Reward: -774.6808 [46.56], Avg: -1188.9975 (0.049)
Step: 29999, Reward: -823.6927 [57.51], Avg: -1186.9455 (0.048)
Step: 30199, Reward: -755.3247 [115.96], Avg: -1184.8551 (0.047)
Step: 30399, Reward: -696.3298 [50.81], Avg: -1181.9754 (0.046)
Step: 30599, Reward: -664.4337 [93.71], Avg: -1179.2053 (0.045)
Step: 30799, Reward: -829.4502 [37.82], Avg: -1177.1797 (0.045)
Step: 30999, Reward: -870.9075 [81.70], Avg: -1175.7309 (0.044)
Step: 31199, Reward: -749.5831 [112.71], Avg: -1173.7217 (0.043)
Step: 31399, Reward: -764.7175 [80.65], Avg: -1171.6303 (0.042)
Step: 31599, Reward: -755.8106 [47.21], Avg: -1169.2973 (0.041)
Step: 31799, Reward: -804.1993 [47.59], Avg: -1167.3004 (0.040)
Step: 31999, Reward: -915.9952 [35.74], Avg: -1165.9531 (0.039)
Step: 32199, Reward: -723.7396 [79.64], Avg: -1163.7011 (0.039)
Step: 32399, Reward: -866.7095 [82.01], Avg: -1162.3741 (0.038)
Step: 32599, Reward: -809.7322 [51.39], Avg: -1160.5259 (0.037)
Step: 32799, Reward: -741.3614 [9.02], Avg: -1158.0250 (0.036)
Step: 32999, Reward: -730.0581 [61.70], Avg: -1155.8052 (0.036)
Step: 33199, Reward: -742.0311 [85.53], Avg: -1153.8279 (0.035)
Step: 33399, Reward: -806.0555 [53.09], Avg: -1152.0633 (0.034)
Step: 33599, Reward: -844.3805 [36.05], Avg: -1150.4465 (0.034)
Step: 33799, Reward: -883.3534 [4.36], Avg: -1148.8919 (0.033)
Step: 33999, Reward: -921.4368 [45.03], Avg: -1147.8187 (0.032)
Step: 34199, Reward: -848.7291 [39.87], Avg: -1146.3029 (0.032)
Step: 34399, Reward: -939.1685 [35.55], Avg: -1145.3053 (0.031)
Step: 34599, Reward: -983.5771 [15.73], Avg: -1144.4613 (0.030)
Step: 34799, Reward: -909.6077 [40.90], Avg: -1143.3467 (0.030)
Step: 34999, Reward: -862.5419 [86.44], Avg: -1142.2360 (0.029)
Step: 35199, Reward: -852.1394 [67.52], Avg: -1140.9714 (0.029)
Step: 35399, Reward: -770.1750 [45.55], Avg: -1139.1338 (0.028)
Step: 35599, Reward: -742.7965 [3.29], Avg: -1136.9257 (0.027)
Step: 35799, Reward: -739.8504 [91.53], Avg: -1135.2188 (0.027)
Step: 35999, Reward: -691.7464 [110.08], Avg: -1133.3666 (0.026)
Step: 36199, Reward: -642.4159 [154.19], Avg: -1131.5060 (0.026)
Step: 36399, Reward: -572.3336 [56.14], Avg: -1128.7421 (0.025)
Step: 36599, Reward: -808.8263 [49.88], Avg: -1127.2665 (0.025)
Step: 36799, Reward: -846.2404 [65.36], Avg: -1126.0944 (0.024)
Step: 36999, Reward: -869.8218 [70.08], Avg: -1125.0879 (0.024)
Step: 37199, Reward: -906.8889 [47.82], Avg: -1124.1719 (0.023)
Step: 37399, Reward: -856.0420 [81.56], Avg: -1123.1742 (0.023)
Step: 37599, Reward: -949.8460 [116.47], Avg: -1122.8717 (0.022)
Step: 37799, Reward: -941.6464 [44.66], Avg: -1122.1492 (0.022)
Step: 37999, Reward: -779.8222 [50.81], Avg: -1120.6148 (0.022)
Step: 38199, Reward: -799.0895 [47.63], Avg: -1119.1808 (0.021)
Step: 38399, Reward: -641.6489 [60.14], Avg: -1117.0069 (0.021)
Step: 38599, Reward: -688.5990 [127.11], Avg: -1115.4458 (0.020)
Step: 38799, Reward: -725.0579 [60.10], Avg: -1113.7433 (0.020)
Step: 38999, Reward: -679.9043 [51.27], Avg: -1111.7814 (0.020)
Step: 39199, Reward: -708.7758 [78.66], Avg: -1110.1265 (0.020)
Step: 39399, Reward: -852.5079 [61.91], Avg: -1109.1331 (0.020)
Step: 39599, Reward: -774.2232 [35.80], Avg: -1107.6224 (0.020)
Step: 39799, Reward: -769.4991 [43.13], Avg: -1106.1401 (0.020)
Step: 39999, Reward: -757.1559 [45.09], Avg: -1104.6206 (0.020)
Step: 40199, Reward: -694.4164 [50.53], Avg: -1102.8312 (0.020)
Step: 40399, Reward: -672.4561 [42.40], Avg: -1100.9105 (0.020)
Step: 40599, Reward: -726.9714 [43.18], Avg: -1099.2812 (0.020)
Step: 40799, Reward: -787.0230 [45.56], Avg: -1097.9738 (0.020)
Step: 40999, Reward: -795.8539 [45.68], Avg: -1096.7229 (0.020)
Step: 41199, Reward: -794.8529 [45.82], Avg: -1095.4799 (0.020)
Step: 41399, Reward: -862.0095 [60.40], Avg: -1094.6439 (0.020)
Step: 41599, Reward: -928.4054 [35.21], Avg: -1094.0139 (0.020)
Step: 41799, Reward: -831.4841 [82.85], Avg: -1093.1542 (0.020)
Step: 41999, Reward: -776.0707 [34.00], Avg: -1091.8062 (0.020)
Step: 42199, Reward: -880.9735 [44.64], Avg: -1091.0185 (0.020)
Step: 42399, Reward: -687.4330 [46.13], Avg: -1089.3324 (0.020)
Step: 42599, Reward: -625.6588 [4.37], Avg: -1087.1761 (0.020)
Step: 42799, Reward: -691.4064 [78.96], Avg: -1085.6957 (0.020)
Step: 42999, Reward: -735.3186 [76.18], Avg: -1084.4203 (0.020)
Step: 43199, Reward: -766.1474 [40.81], Avg: -1083.1358 (0.020)
Step: 43399, Reward: -770.4946 [33.58], Avg: -1081.8498 (0.020)
Step: 43599, Reward: -719.5092 [84.49], Avg: -1080.5752 (0.020)
Step: 43799, Reward: -829.6281 [68.96], Avg: -1079.7442 (0.020)
Step: 43999, Reward: -850.2811 [61.75], Avg: -1078.9819 (0.020)
Step: 44199, Reward: -826.0534 [64.10], Avg: -1078.1274 (0.020)
Step: 44399, Reward: -721.8476 [60.12], Avg: -1076.7934 (0.020)
Step: 44599, Reward: -720.9399 [70.15], Avg: -1075.5122 (0.020)
Step: 44799, Reward: -857.7090 [51.77], Avg: -1074.7710 (0.020)
Step: 44999, Reward: -838.8554 [45.44], Avg: -1073.9244 (0.020)
Step: 45199, Reward: -890.3215 [3.91], Avg: -1073.1293 (0.020)
Step: 45399, Reward: -916.1277 [142.81], Avg: -1073.0668 (0.020)
Step: 45599, Reward: -798.4575 [79.73], Avg: -1072.2121 (0.020)
Step: 45799, Reward: -816.5645 [51.83], Avg: -1071.3220 (0.020)
Step: 45999, Reward: -807.3796 [47.41], Avg: -1070.3806 (0.020)
Step: 46199, Reward: -804.9027 [55.29], Avg: -1069.4707 (0.020)
Step: 46399, Reward: -776.9059 [38.71], Avg: -1068.3765 (0.020)
Step: 46599, Reward: -778.2962 [43.57], Avg: -1067.3185 (0.020)
Step: 46799, Reward: -688.3672 [86.85], Avg: -1066.0702 (0.020)
Step: 46999, Reward: -630.8065 [52.95], Avg: -1064.4433 (0.020)
Step: 47199, Reward: -744.3727 [72.66], Avg: -1063.3949 (0.020)
Step: 47399, Reward: -702.6309 [33.56], Avg: -1062.0143 (0.020)
Step: 47599, Reward: -820.7713 [43.25], Avg: -1061.1824 (0.020)
Step: 47799, Reward: -731.4026 [91.35], Avg: -1060.1848 (0.020)
Step: 47999, Reward: -797.6102 [30.92], Avg: -1059.2196 (0.020)
Step: 48199, Reward: -713.4974 [42.86], Avg: -1057.9629 (0.020)
Step: 48399, Reward: -665.3441 [114.46], Avg: -1056.8135 (0.020)
Step: 48599, Reward: -751.8302 [70.94], Avg: -1055.8504 (0.020)
Step: 48799, Reward: -692.9684 [57.47], Avg: -1054.5987 (0.020)
Step: 48999, Reward: -735.2691 [72.26], Avg: -1053.5903 (0.020)
Step: 49199, Reward: -701.6898 [79.23], Avg: -1052.4818 (0.020)
Step: 49399, Reward: -785.9326 [50.60], Avg: -1051.6076 (0.020)
Step: 49599, Reward: -798.1240 [69.07], Avg: -1050.8639 (0.020)
Step: 49799, Reward: -767.8646 [49.59], Avg: -1049.9266 (0.020)
Step: 49999, Reward: -848.3804 [30.50], Avg: -1049.2424 (0.020)
Step: 50199, Reward: -760.5881 [37.79], Avg: -1048.2429 (0.020)
Step: 50399, Reward: -818.8151 [63.43], Avg: -1047.5842 (0.020)
Step: 50599, Reward: -738.0862 [58.87], Avg: -1046.5936 (0.020)
Step: 50799, Reward: -679.8169 [47.84], Avg: -1045.3379 (0.020)
Step: 50999, Reward: -755.4724 [10.85], Avg: -1044.2438 (0.020)
Step: 51199, Reward: -698.8965 [81.13], Avg: -1043.2116 (0.020)
Step: 51399, Reward: -686.9981 [49.54], Avg: -1042.0183 (0.020)
Step: 51599, Reward: -715.2155 [114.64], Avg: -1041.1960 (0.020)
Step: 51799, Reward: -769.4817 [41.86], Avg: -1040.3085 (0.020)
Step: 51999, Reward: -721.5700 [27.17], Avg: -1039.1871 (0.020)
Step: 52199, Reward: -811.5325 [92.62], Avg: -1038.6697 (0.020)
Step: 52399, Reward: -738.6193 [87.93], Avg: -1037.8601 (0.020)
Step: 52599, Reward: -719.8398 [87.40], Avg: -1036.9832 (0.020)
Step: 52799, Reward: -736.4923 [61.54], Avg: -1036.0781 (0.020)
Step: 52999, Reward: -838.7814 [48.11], Avg: -1035.5151 (0.020)
Step: 53199, Reward: -803.3030 [71.49], Avg: -1034.9109 (0.020)
Step: 53399, Reward: -653.6217 [94.92], Avg: -1033.8384 (0.020)
Step: 53599, Reward: -760.2016 [16.11], Avg: -1032.8775 (0.020)
Step: 53799, Reward: -817.1627 [37.23], Avg: -1032.2140 (0.020)
Step: 53999, Reward: -774.0622 [55.69], Avg: -1031.4641 (0.020)
Step: 54199, Reward: -770.1594 [42.28], Avg: -1030.6559 (0.020)
Step: 54399, Reward: -781.3892 [65.53], Avg: -1029.9804 (0.020)
Step: 54599, Reward: -774.9185 [39.19], Avg: -1029.1897 (0.020)
Step: 54799, Reward: -854.1636 [60.32], Avg: -1028.7710 (0.020)
Step: 54999, Reward: -759.2905 [61.30], Avg: -1028.0140 (0.020)
Step: 55199, Reward: -839.5014 [44.59], Avg: -1027.4925 (0.020)
Step: 55399, Reward: -854.0739 [121.79], Avg: -1027.3062 (0.020)
Step: 55599, Reward: -932.2638 [34.01], Avg: -1027.0866 (0.020)
Step: 55799, Reward: -915.1502 [32.13], Avg: -1026.8006 (0.020)
Step: 55999, Reward: -918.1203 [44.55], Avg: -1026.5716 (0.020)
Step: 56199, Reward: -942.2186 [45.09], Avg: -1026.4318 (0.020)
Step: 56399, Reward: -737.9028 [67.08], Avg: -1025.6466 (0.020)
Step: 56599, Reward: -902.9802 [29.76], Avg: -1025.3183 (0.020)
Step: 56799, Reward: -939.7832 [33.11], Avg: -1025.1336 (0.020)
Step: 56999, Reward: -919.0322 [28.57], Avg: -1024.8616 (0.020)
Step: 57199, Reward: -883.5754 [66.79], Avg: -1024.6011 (0.020)
Step: 57399, Reward: -910.3962 [89.32], Avg: -1024.5144 (0.020)
Step: 57599, Reward: -819.8713 [63.35], Avg: -1024.0238 (0.020)
Step: 57799, Reward: -803.8430 [89.59], Avg: -1023.5719 (0.020)
Step: 57999, Reward: -853.0459 [66.76], Avg: -1023.2141 (0.020)
Step: 58199, Reward: -721.7673 [53.64], Avg: -1022.3625 (0.020)
Step: 58399, Reward: -835.6503 [104.58], Avg: -1022.0813 (0.020)
Step: 58599, Reward: -766.0042 [78.84], Avg: -1021.4764 (0.020)
Step: 58799, Reward: -843.5706 [38.44], Avg: -1021.0020 (0.020)
Step: 58999, Reward: -752.2843 [72.00], Avg: -1020.3351 (0.020)
Step: 59199, Reward: -803.2178 [63.72], Avg: -1019.8169 (0.020)
Step: 59399, Reward: -890.9493 [13.37], Avg: -1019.4280 (0.020)
Step: 59599, Reward: -739.8206 [89.61], Avg: -1018.7904 (0.020)
Step: 59799, Reward: -774.9071 [86.42], Avg: -1018.2638 (0.020)
Step: 59999, Reward: -761.6843 [76.37], Avg: -1017.6631 (0.020)
Step: 60199, Reward: -981.1917 [22.94], Avg: -1017.6181 (0.020)
Step: 60399, Reward: -864.7721 [43.98], Avg: -1017.2577 (0.020)
Step: 60599, Reward: -825.2854 [46.60], Avg: -1016.7779 (0.020)
Step: 60799, Reward: -928.5769 [35.75], Avg: -1016.6053 (0.020)
Step: 60999, Reward: -768.7770 [88.15], Avg: -1016.0818 (0.020)
Step: 61199, Reward: -797.4833 [84.41], Avg: -1015.6433 (0.020)
Step: 61399, Reward: -765.1255 [93.66], Avg: -1015.1323 (0.020)
Step: 61599, Reward: -721.0727 [59.08], Avg: -1014.3694 (0.020)
Step: 61799, Reward: -689.1607 [85.90], Avg: -1013.5949 (0.020)
Step: 61999, Reward: -730.4706 [25.31], Avg: -1012.7633 (0.020)
Step: 62199, Reward: -642.9097 [77.97], Avg: -1011.8248 (0.020)
Step: 62399, Reward: -785.3615 [43.05], Avg: -1011.2369 (0.020)
Step: 62599, Reward: -690.8574 [65.75], Avg: -1010.4234 (0.020)
Step: 62799, Reward: -657.0865 [46.34], Avg: -1009.4457 (0.020)
Step: 62999, Reward: -738.9933 [34.25], Avg: -1008.6958 (0.020)
Step: 63199, Reward: -771.4514 [40.58], Avg: -1008.0735 (0.020)
Step: 63399, Reward: -766.4191 [37.37], Avg: -1007.4291 (0.020)
Step: 63599, Reward: -702.6423 [63.98], Avg: -1006.6718 (0.020)
Step: 63799, Reward: -653.1835 [119.52], Avg: -1005.9384 (0.020)
Step: 63999, Reward: -628.4463 [72.86], Avg: -1004.9864 (0.020)
Step: 64199, Reward: -774.6850 [46.95], Avg: -1004.4152 (0.020)
Step: 64399, Reward: -775.8511 [40.19], Avg: -1003.8302 (0.020)
Step: 64599, Reward: -878.7052 [6.79], Avg: -1003.4638 (0.020)
Step: 64799, Reward: -780.9720 [45.38], Avg: -1002.9172 (0.020)
Step: 64999, Reward: -766.2759 [40.99], Avg: -1002.3152 (0.020)
Step: 65199, Reward: -754.3675 [3.42], Avg: -1001.5651 (0.020)
Step: 65399, Reward: -729.8435 [65.38], Avg: -1000.9341 (0.020)
Step: 65599, Reward: -800.8704 [55.99], Avg: -1000.4948 (0.020)
Step: 65799, Reward: -790.4540 [42.99], Avg: -999.9871 (0.020)
Step: 65999, Reward: -719.4641 [88.01], Avg: -999.4037 (0.020)
Step: 66199, Reward: -804.0369 [45.93], Avg: -998.9523 (0.020)
Step: 66399, Reward: -695.2472 [55.00], Avg: -998.2031 (0.020)
Step: 66599, Reward: -839.4565 [41.99], Avg: -997.8525 (0.020)
Step: 66799, Reward: -744.6599 [14.52], Avg: -997.1379 (0.020)
Step: 66999, Reward: -773.1301 [53.78], Avg: -996.6298 (0.020)
Step: 67199, Reward: -772.9303 [37.11], Avg: -996.0745 (0.020)
Step: 67399, Reward: -864.5015 [38.64], Avg: -995.7987 (0.020)
Step: 67599, Reward: -773.5990 [44.44], Avg: -995.2728 (0.020)
Step: 67799, Reward: -831.2672 [37.95], Avg: -994.9010 (0.020)
Step: 67999, Reward: -681.8781 [85.35], Avg: -994.2313 (0.020)
Step: 68199, Reward: -728.7876 [51.34], Avg: -993.6035 (0.020)
Step: 68399, Reward: -795.6001 [48.23], Avg: -993.1655 (0.020)
Step: 68599, Reward: -760.3292 [19.45], Avg: -992.5434 (0.020)
Step: 68799, Reward: -692.3121 [53.22], Avg: -991.8253 (0.020)
Step: 68999, Reward: -729.6641 [12.86], Avg: -991.1027 (0.020)
Step: 69199, Reward: -741.0497 [40.30], Avg: -990.4965 (0.020)
Step: 69399, Reward: -638.0548 [44.08], Avg: -989.6078 (0.020)
Step: 69599, Reward: -750.4517 [24.29], Avg: -988.9904 (0.020)
Step: 69799, Reward: -770.6399 [65.33], Avg: -988.5520 (0.020)
Step: 69999, Reward: -747.3580 [43.43], Avg: -987.9869 (0.020)
Step: 70199, Reward: -684.5976 [50.63], Avg: -987.2668 (0.020)
Step: 70399, Reward: -790.9741 [48.44], Avg: -986.8468 (0.020)
Step: 70599, Reward: -772.5611 [33.80], Avg: -986.3355 (0.020)
Step: 70799, Reward: -623.9083 [71.45], Avg: -985.5135 (0.020)
Step: 70999, Reward: -827.6612 [51.63], Avg: -985.2143 (0.020)
Step: 71199, Reward: -996.1443 [35.63], Avg: -985.3451 (0.020)
Step: 71399, Reward: -925.1060 [39.43], Avg: -985.2868 (0.020)
Step: 71599, Reward: -990.6801 [39.72], Avg: -985.4128 (0.020)
Step: 71799, Reward: -990.1676 [101.32], Avg: -985.7083 (0.020)
Step: 71999, Reward: -909.9277 [88.82], Avg: -985.7445 (0.020)
Step: 72199, Reward: -868.7556 [9.80], Avg: -985.4476 (0.020)
Step: 72399, Reward: -864.0578 [51.96], Avg: -985.2558 (0.020)
Step: 72599, Reward: -827.0619 [42.58], Avg: -984.9373 (0.020)
Step: 72799, Reward: -738.0019 [76.72], Avg: -984.4697 (0.020)
Step: 72999, Reward: -926.0336 [39.30], Avg: -984.4173 (0.020)
Step: 73199, Reward: -776.6444 [80.62], Avg: -984.0699 (0.020)
Step: 73399, Reward: -814.0935 [47.83], Avg: -983.7371 (0.020)
Step: 73599, Reward: -860.0347 [60.53], Avg: -983.5654 (0.020)
Step: 73799, Reward: -632.4251 [29.25], Avg: -982.6931 (0.020)
Step: 73999, Reward: -786.0772 [39.61], Avg: -982.2687 (0.020)
Step: 74199, Reward: -882.2897 [85.80], Avg: -982.2305 (0.020)
Step: 74399, Reward: -778.1751 [100.48], Avg: -981.9521 (0.020)
Step: 74599, Reward: -641.2007 [45.62], Avg: -981.1608 (0.020)
Step: 74799, Reward: -675.6508 [36.32], Avg: -980.4411 (0.020)
Step: 74999, Reward: -705.9015 [66.69], Avg: -979.8868 (0.020)
Step: 75199, Reward: -680.7723 [79.87], Avg: -979.3037 (0.020)
Step: 75399, Reward: -649.1056 [42.08], Avg: -978.5395 (0.020)
Step: 75599, Reward: -781.3913 [52.43], Avg: -978.1566 (0.020)
Step: 75799, Reward: -737.0202 [58.00], Avg: -977.6734 (0.020)
Step: 75999, Reward: -769.4398 [37.88], Avg: -977.2251 (0.020)
Step: 76199, Reward: -738.9720 [67.57], Avg: -976.7771 (0.020)
Step: 76399, Reward: -712.2616 [35.75], Avg: -976.1782 (0.020)
Step: 76599, Reward: -691.3556 [59.13], Avg: -975.5890 (0.020)
Step: 76799, Reward: -676.3777 [42.58], Avg: -974.9207 (0.020)
Step: 76999, Reward: -662.8775 [45.38], Avg: -974.2280 (0.020)
Step: 77199, Reward: -696.8777 [92.00], Avg: -973.7479 (0.020)
Step: 77399, Reward: -794.4474 [50.93], Avg: -973.4161 (0.020)
Step: 77599, Reward: -767.1234 [39.09], Avg: -972.9852 (0.020)
Step: 77799, Reward: -670.3177 [58.66], Avg: -972.3579 (0.020)
Step: 77999, Reward: -687.7118 [81.03], Avg: -971.8358 (0.020)
Step: 78199, Reward: -702.0814 [60.64], Avg: -971.3010 (0.020)
Step: 78399, Reward: -726.9711 [61.17], Avg: -970.8338 (0.020)
Step: 78599, Reward: -691.5157 [67.07], Avg: -970.2937 (0.020)
Step: 78799, Reward: -705.5874 [42.30], Avg: -969.7292 (0.020)
Step: 78999, Reward: -740.2803 [67.97], Avg: -969.3204 (0.020)
Step: 79199, Reward: -740.3226 [2.99], Avg: -968.7497 (0.020)
Step: 79399, Reward: -795.1023 [44.34], Avg: -968.4240 (0.020)
Step: 79599, Reward: -754.6396 [79.75], Avg: -968.0872 (0.020)
Step: 79799, Reward: -808.1521 [49.96], Avg: -967.8116 (0.020)
Step: 79999, Reward: -733.9549 [62.88], Avg: -967.3841 (0.020)
Step: 80199, Reward: -769.2530 [71.11], Avg: -967.0674 (0.020)
Step: 80399, Reward: -792.1697 [57.19], Avg: -966.7746 (0.020)
Step: 80599, Reward: -780.7281 [46.37], Avg: -966.4280 (0.020)
Step: 80799, Reward: -752.5718 [54.39], Avg: -966.0333 (0.020)
Step: 80999, Reward: -844.3667 [58.69], Avg: -965.8778 (0.020)
Step: 81199, Reward: -854.1381 [64.33], Avg: -965.7610 (0.020)
Step: 81399, Reward: -715.5351 [47.02], Avg: -965.2617 (0.020)
Step: 81599, Reward: -776.7815 [37.57], Avg: -964.8918 (0.020)
Step: 81799, Reward: -739.9792 [56.64], Avg: -964.4804 (0.020)
Step: 81999, Reward: -746.7426 [0.93], Avg: -963.9516 (0.020)
Step: 82199, Reward: -730.8613 [65.07], Avg: -963.5428 (0.020)
Step: 82399, Reward: -670.2311 [48.66], Avg: -962.9490 (0.020)
Step: 82599, Reward: -796.9575 [45.58], Avg: -962.6574 (0.020)
Step: 82799, Reward: -793.7227 [46.32], Avg: -962.3613 (0.020)
Step: 82999, Reward: -821.5228 [34.04], Avg: -962.1039 (0.020)
Step: 83199, Reward: -706.2670 [44.96], Avg: -961.5970 (0.020)
Step: 83399, Reward: -723.1097 [50.35], Avg: -961.1459 (0.020)
Step: 83599, Reward: -859.4393 [22.45], Avg: -960.9562 (0.020)
Step: 83799, Reward: -807.0122 [43.88], Avg: -960.6935 (0.020)
Step: 83999, Reward: -791.5659 [99.35], Avg: -960.5274 (0.020)
Step: 84199, Reward: -782.2569 [44.55], Avg: -960.2098 (0.020)
Step: 84399, Reward: -768.6327 [42.71], Avg: -959.8570 (0.020)
Step: 84599, Reward: -797.5115 [106.01], Avg: -959.7238 (0.020)
Step: 84799, Reward: -838.4334 [39.63], Avg: -959.5312 (0.020)
Step: 84999, Reward: -775.6600 [74.81], Avg: -959.2746 (0.020)
Step: 85199, Reward: -673.4097 [40.83], Avg: -958.6994 (0.020)
Step: 85399, Reward: -833.2026 [31.09], Avg: -958.4783 (0.020)
Step: 85599, Reward: -839.2039 [64.52], Avg: -958.3504 (0.020)
Step: 85799, Reward: -896.9691 [62.16], Avg: -958.3522 (0.020)
Step: 85999, Reward: -892.9273 [33.53], Avg: -958.2780 (0.020)
Step: 86199, Reward: -786.4857 [138.40], Avg: -958.2005 (0.020)
Step: 86399, Reward: -743.1299 [92.99], Avg: -957.9180 (0.020)
Step: 86599, Reward: -805.8974 [42.36], Avg: -957.6647 (0.020)
Step: 86799, Reward: -717.1125 [53.58], Avg: -957.2339 (0.020)
Step: 86999, Reward: -779.6096 [47.84], Avg: -956.9355 (0.020)
Step: 87199, Reward: -815.2807 [78.13], Avg: -956.7898 (0.020)
Step: 87399, Reward: -779.6862 [40.27], Avg: -956.4767 (0.020)
Step: 87599, Reward: -714.3167 [43.28], Avg: -956.0226 (0.020)
Step: 87799, Reward: -627.1074 [72.09], Avg: -955.4376 (0.020)
Step: 87999, Reward: -651.6618 [36.57], Avg: -954.8303 (0.020)
Step: 88199, Reward: -777.0877 [77.69], Avg: -954.6034 (0.020)
Step: 88399, Reward: -704.2943 [45.84], Avg: -954.1408 (0.020)
Step: 88599, Reward: -759.3097 [50.17], Avg: -953.8143 (0.020)
Step: 88799, Reward: -790.4115 [46.72], Avg: -953.5515 (0.020)
Step: 88999, Reward: -744.0016 [3.25], Avg: -953.0879 (0.020)
Step: 89199, Reward: -771.8083 [80.09], Avg: -952.8610 (0.020)
Step: 89399, Reward: -738.9996 [66.15], Avg: -952.5306 (0.020)
Step: 89599, Reward: -653.2406 [58.70], Avg: -951.9935 (0.020)
Step: 89799, Reward: -766.7772 [77.30], Avg: -951.7532 (0.020)
Step: 89999, Reward: -728.3709 [17.32], Avg: -951.2952 (0.020)
Step: 90199, Reward: -613.7865 [59.02], Avg: -950.6778 (0.020)
Step: 90399, Reward: -781.3556 [42.52], Avg: -950.3972 (0.020)
Step: 90599, Reward: -706.6039 [43.12], Avg: -949.9542 (0.020)
Step: 90799, Reward: -747.1376 [25.75], Avg: -949.5642 (0.020)
Step: 90999, Reward: -743.0057 [84.29], Avg: -949.2955 (0.020)
Step: 91199, Reward: -666.5687 [53.03], Avg: -948.7918 (0.020)
Step: 91399, Reward: -743.3600 [88.36], Avg: -948.5356 (0.020)
Step: 91599, Reward: -790.4675 [43.03], Avg: -948.2844 (0.020)
Step: 91799, Reward: -828.2672 [46.77], Avg: -948.1249 (0.020)
Step: 91999, Reward: -783.4719 [39.82], Avg: -947.8535 (0.020)
Step: 92199, Reward: -708.1214 [55.18], Avg: -947.4531 (0.020)
Step: 92399, Reward: -630.9722 [27.94], Avg: -946.8286 (0.020)
Step: 92599, Reward: -599.3742 [82.94], Avg: -946.2573 (0.020)
Step: 92799, Reward: -709.5372 [75.11], Avg: -945.9090 (0.020)
Step: 92999, Reward: -639.5459 [45.02], Avg: -945.3470 (0.020)
Step: 93199, Reward: -672.9695 [48.46], Avg: -944.8664 (0.020)
Step: 93399, Reward: -740.5795 [69.22], Avg: -944.5772 (0.020)
Step: 93599, Reward: -785.8337 [57.44], Avg: -944.3607 (0.020)
Step: 93799, Reward: -677.9869 [86.09], Avg: -943.9764 (0.020)
Step: 93999, Reward: -809.4754 [42.77], Avg: -943.7812 (0.020)
Step: 94199, Reward: -821.1318 [31.43], Avg: -943.5875 (0.020)
Step: 94399, Reward: -669.0518 [41.25], Avg: -943.0933 (0.020)
Step: 94599, Reward: -745.2722 [15.83], Avg: -942.7085 (0.020)
Step: 94799, Reward: -682.8349 [82.77], Avg: -942.3349 (0.020)
Step: 94999, Reward: -707.3542 [41.62], Avg: -941.9278 (0.020)
Step: 95199, Reward: -794.5065 [51.78], Avg: -941.7269 (0.020)
Step: 95399, Reward: -841.3334 [37.56], Avg: -941.5951 (0.020)
Step: 95599, Reward: -853.0090 [62.48], Avg: -941.5405 (0.020)
Step: 95799, Reward: -724.6652 [76.38], Avg: -941.2472 (0.020)
Step: 95999, Reward: -728.5355 [68.02], Avg: -940.9458 (0.020)
Step: 96199, Reward: -837.2623 [72.29], Avg: -940.8805 (0.020)
Step: 96399, Reward: -807.4951 [34.99], Avg: -940.6764 (0.020)
Step: 96599, Reward: -850.6009 [18.31], Avg: -940.5278 (0.020)
Step: 96799, Reward: -815.9143 [46.45], Avg: -940.3663 (0.020)
Step: 96999, Reward: -720.7110 [79.47], Avg: -940.0772 (0.020)
Step: 97199, Reward: -759.0772 [47.51], Avg: -939.8026 (0.020)
Step: 97399, Reward: -875.7826 [80.74], Avg: -939.8369 (0.020)
Step: 97599, Reward: -792.1197 [54.66], Avg: -939.6462 (0.020)
Step: 97799, Reward: -813.6688 [47.69], Avg: -939.4861 (0.020)
Step: 97999, Reward: -879.3254 [32.30], Avg: -939.4292 (0.020)
Step: 98199, Reward: -845.0259 [44.72], Avg: -939.3281 (0.020)
Step: 98399, Reward: -828.6502 [48.58], Avg: -939.2018 (0.020)
Step: 98599, Reward: -764.1325 [19.46], Avg: -938.8862 (0.020)
Step: 98799, Reward: -746.5917 [66.36], Avg: -938.6313 (0.020)
Step: 98999, Reward: -727.7433 [51.74], Avg: -938.3098 (0.020)
Step: 99199, Reward: -734.9113 [64.75], Avg: -938.0303 (0.020)
Step: 99399, Reward: -780.2272 [39.62], Avg: -937.7925 (0.020)
Step: 99599, Reward: -786.6085 [60.58], Avg: -937.6105 (0.020)
Step: 99799, Reward: -752.6397 [59.04], Avg: -937.3582 (0.020)
Step: 99999, Reward: -800.2084 [48.06], Avg: -937.1800 (0.020)
