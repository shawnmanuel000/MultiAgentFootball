Model: <class 'models.ddpg.DDPGAgent'>, Dir: Pendulum-v0
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.98             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "Acrobot-v1", "Pendulum-v0"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[2]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0] or s+1 % envs.env.spec.max_episode_steps == 0:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 199, Reward: -1432.0551 [252.44], Avg: -1684.4918 (0.980)
Step: 399, Reward: -1202.6908 [90.46], Avg: -1488.8210 (0.960)
Step: 599, Reward: -1315.5197 [48.10], Avg: -1447.0870 (0.941)
Step: 799, Reward: -1253.7343 [209.86], Avg: -1451.2130 (0.922)
Step: 999, Reward: -1285.5884 [33.15], Avg: -1424.7182 (0.904)
Step: 1199, Reward: -1399.0990 [201.40], Avg: -1454.0158 (0.886)
Step: 1399, Reward: -1473.1412 [151.92], Avg: -1478.4509 (0.868)
Step: 1599, Reward: -1238.6299 [208.67], Avg: -1474.5568 (0.851)
Step: 1799, Reward: -1365.6152 [260.58], Avg: -1491.4055 (0.834)
Step: 1999, Reward: -1162.0595 [44.21], Avg: -1462.8918 (0.817)
Step: 2199, Reward: -1416.9559 [186.65], Avg: -1475.6839 (0.801)
Step: 2399, Reward: -1147.8537 [240.06], Avg: -1468.3696 (0.785)
Step: 2599, Reward: -1205.2241 [179.84], Avg: -1461.9611 (0.769)
Step: 2799, Reward: -1183.6754 [89.83], Avg: -1448.5001 (0.754)
Step: 2999, Reward: -1344.1636 [300.76], Avg: -1461.5947 (0.739)
Step: 3199, Reward: -1278.7312 [165.14], Avg: -1460.4868 (0.724)
Step: 3399, Reward: -1289.6114 [161.40], Avg: -1459.9295 (0.709)
Step: 3599, Reward: -1234.8505 [103.33], Avg: -1453.1657 (0.695)
Step: 3799, Reward: -1210.2947 [120.85], Avg: -1446.7436 (0.681)
Step: 3999, Reward: -1391.5361 [236.37], Avg: -1455.8020 (0.668)
Step: 4199, Reward: -1378.4169 [163.62], Avg: -1459.9084 (0.654)
Step: 4399, Reward: -1309.2957 [193.53], Avg: -1461.8592 (0.641)
Step: 4599, Reward: -1358.3533 [220.33], Avg: -1466.9386 (0.628)
Step: 4799, Reward: -1242.6873 [55.22], Avg: -1459.8957 (0.616)
Step: 4999, Reward: -1234.4992 [210.55], Avg: -1459.3016 (0.603)
Step: 5199, Reward: -1350.9830 [109.82], Avg: -1459.3592 (0.591)
Step: 5399, Reward: -1287.7575 [97.94], Avg: -1456.6312 (0.580)
Step: 5599, Reward: -1107.6902 [84.95], Avg: -1447.2031 (0.568)
Step: 5799, Reward: -1246.4551 [258.86], Avg: -1449.2070 (0.557)
Step: 5999, Reward: -1311.8033 [251.23], Avg: -1453.0013 (0.545)
Step: 6199, Reward: -1300.8141 [109.95], Avg: -1451.6388 (0.535)
Step: 6399, Reward: -1327.8472 [258.97], Avg: -1455.8631 (0.524)
Step: 6599, Reward: -1426.5066 [290.40], Avg: -1463.7737 (0.513)
Step: 6799, Reward: -1487.5013 [167.68], Avg: -1469.4034 (0.503)
Step: 6999, Reward: -1071.3970 [143.19], Avg: -1462.1230 (0.493)
Step: 7199, Reward: -1294.5664 [258.13], Avg: -1464.6388 (0.483)
Step: 7399, Reward: -1332.4550 [180.68], Avg: -1465.9495 (0.474)
Step: 7599, Reward: -1103.8355 [195.90], Avg: -1461.5754 (0.464)
Step: 7799, Reward: -1203.4108 [278.58], Avg: -1462.0987 (0.455)
Step: 7999, Reward: -1274.0710 [203.32], Avg: -1462.4811 (0.446)
Step: 8199, Reward: -1410.7060 [245.50], Avg: -1467.2061 (0.437)
Step: 8399, Reward: -1280.1455 [100.04], Avg: -1465.1342 (0.428)
Step: 8599, Reward: -1066.5144 [91.22], Avg: -1457.9853 (0.419)
Step: 8799, Reward: -1087.1718 [30.48], Avg: -1450.2505 (0.411)
Step: 8999, Reward: -1036.8491 [103.22], Avg: -1443.3576 (0.403)
Step: 9199, Reward: -1070.5952 [251.13], Avg: -1440.7134 (0.395)
Step: 9399, Reward: -954.8078 [84.17], Avg: -1432.1659 (0.387)
Step: 9599, Reward: -1049.2035 [17.90], Avg: -1424.5605 (0.379)
Step: 9799, Reward: -1015.9510 [44.63], Avg: -1417.1324 (0.372)
Step: 9999, Reward: -974.0419 [90.25], Avg: -1410.0755 (0.364)
Step: 10199, Reward: -857.3642 [111.25], Avg: -1401.4195 (0.357)
Step: 10399, Reward: -953.0373 [39.15], Avg: -1393.5497 (0.350)
Step: 10599, Reward: -898.7905 [54.92], Avg: -1385.2508 (0.343)
Step: 10799, Reward: -933.4107 [146.73], Avg: -1379.6005 (0.336)
Step: 10999, Reward: -1065.4247 [121.63], Avg: -1376.0997 (0.329)
Step: 11199, Reward: -942.2353 [86.25], Avg: -1369.8923 (0.323)
Step: 11399, Reward: -843.4887 [145.25], Avg: -1363.2054 (0.316)
Step: 11599, Reward: -944.2853 [138.12], Avg: -1358.3641 (0.310)
Step: 11799, Reward: -919.2531 [104.49], Avg: -1352.6926 (0.304)
Step: 11999, Reward: -799.2265 [117.80], Avg: -1345.4315 (0.298)
Step: 12199, Reward: -691.9328 [55.54], Avg: -1335.6289 (0.292)
Step: 12399, Reward: -672.4280 [189.55], Avg: -1327.9894 (0.286)
Step: 12599, Reward: -541.9718 [150.79], Avg: -1317.9064 (0.280)
Step: 12799, Reward: -697.6747 [131.54], Avg: -1310.2706 (0.274)
Step: 12999, Reward: -829.1658 [135.89], Avg: -1304.9596 (0.269)
Step: 13199, Reward: -992.2070 [100.30], Avg: -1301.7406 (0.264)
Step: 13399, Reward: -956.1324 [249.58], Avg: -1300.3073 (0.258)
Step: 13599, Reward: -914.8381 [121.75], Avg: -1296.4291 (0.253)
Step: 13799, Reward: -869.4350 [122.26], Avg: -1292.0128 (0.248)
Step: 13999, Reward: -966.1497 [85.00], Avg: -1288.5719 (0.243)
Step: 14199, Reward: -875.9685 [47.51], Avg: -1283.4297 (0.238)
Step: 14399, Reward: -926.7097 [123.65], Avg: -1280.1927 (0.233)
Step: 14599, Reward: -754.5741 [62.33], Avg: -1273.8462 (0.229)
Step: 14799, Reward: -881.0740 [90.44], Avg: -1269.7607 (0.224)
Step: 14999, Reward: -894.3878 [41.51], Avg: -1265.3091 (0.220)
Step: 15199, Reward: -932.0080 [49.26], Avg: -1261.5717 (0.215)
Step: 15399, Reward: -868.5722 [53.01], Avg: -1257.1563 (0.211)
Step: 15599, Reward: -904.5674 [81.42], Avg: -1253.6798 (0.207)
Step: 15799, Reward: -784.9722 [57.87], Avg: -1248.4794 (0.203)
Step: 15999, Reward: -872.3853 [153.44], Avg: -1245.6963 (0.199)
Step: 16199, Reward: -882.6255 [39.41], Avg: -1241.7005 (0.195)
Step: 16399, Reward: -912.3522 [112.65], Avg: -1239.0578 (0.191)
Step: 16599, Reward: -838.3159 [119.90], Avg: -1235.6742 (0.187)
Step: 16799, Reward: -707.0511 [131.74], Avg: -1230.9494 (0.183)
Step: 16999, Reward: -790.9558 [68.07], Avg: -1226.5738 (0.180)
Step: 17199, Reward: -590.6298 [89.63], Avg: -1220.2213 (0.176)
Step: 17399, Reward: -402.9427 [163.62], Avg: -1212.7080 (0.172)
Step: 17599, Reward: -454.7034 [122.42], Avg: -1205.4854 (0.169)
Step: 17799, Reward: -348.8694 [42.02], Avg: -1196.3326 (0.166)
Step: 17999, Reward: -401.3553 [121.40], Avg: -1188.8484 (0.162)
Step: 18199, Reward: -321.5118 [143.18], Avg: -1180.8906 (0.159)
Step: 18399, Reward: -243.1714 [105.19], Avg: -1171.8414 (0.156)
Step: 18599, Reward: -287.1325 [116.03], Avg: -1163.5761 (0.153)
Step: 18799, Reward: -342.9783 [88.41], Avg: -1155.7868 (0.150)
Step: 18999, Reward: -362.1087 [123.95], Avg: -1148.7370 (0.147)
Step: 19199, Reward: -169.2683 [152.92], Avg: -1140.1271 (0.144)
Step: 19399, Reward: -530.6679 [488.63], Avg: -1138.8815 (0.141)
Step: 19599, Reward: -392.7919 [232.36], Avg: -1133.6393 (0.138)
Step: 19799, Reward: -600.0395 [195.72], Avg: -1130.2264 (0.135)
Step: 19999, Reward: -142.5066 [40.22], Avg: -1120.7514 (0.133)
Step: 20199, Reward: -102.3622 [90.00], Avg: -1111.5595 (0.130)
Step: 20399, Reward: -300.0171 [88.58], Avg: -1104.4716 (0.127)
Step: 20599, Reward: -168.7091 [54.21], Avg: -1095.9128 (0.125)
Step: 20799, Reward: -370.2843 [113.12], Avg: -1090.0233 (0.122)
Step: 20999, Reward: -1029.6935 [42.15], Avg: -1089.8501 (0.120)
Step: 21199, Reward: -969.7373 [21.92], Avg: -1088.9238 (0.117)
Step: 21399, Reward: -167.4590 [159.00], Avg: -1081.7980 (0.115)
Step: 21599, Reward: -195.0586 [94.90], Avg: -1074.4662 (0.113)
Step: 21799, Reward: -163.8902 [55.60], Avg: -1066.6224 (0.111)
Step: 21999, Reward: -172.9035 [102.13], Avg: -1059.4261 (0.108)
Step: 22199, Reward: -163.7389 [116.51], Avg: -1052.4065 (0.106)
Step: 22399, Reward: -215.7901 [78.88], Avg: -1045.6410 (0.104)
Step: 22599, Reward: -102.5420 [50.26], Avg: -1037.7397 (0.102)
Step: 22799, Reward: -147.6132 [88.35], Avg: -1030.7065 (0.100)
Step: 22999, Reward: -169.2076 [55.31], Avg: -1023.6962 (0.098)
Step: 23199, Reward: -149.1245 [49.56], Avg: -1016.5841 (0.096)
Step: 23399, Reward: -167.3540 [116.46], Avg: -1010.3211 (0.094)
Step: 23599, Reward: -191.5173 [59.52], Avg: -1003.8865 (0.092)
Step: 23799, Reward: -250.8048 [72.08], Avg: -998.1638 (0.090)
Step: 23999, Reward: -166.3008 [57.79], Avg: -991.7131 (0.089)
Step: 24199, Reward: -142.5326 [42.43], Avg: -985.0458 (0.087)
Step: 24399, Reward: -199.1833 [149.35], Avg: -979.8285 (0.085)
Step: 24599, Reward: -388.1401 [183.62], Avg: -976.5109 (0.083)
Step: 24799, Reward: -161.0723 [110.22], Avg: -970.8236 (0.082)
Step: 24999, Reward: -213.9773 [136.70], Avg: -965.8625 (0.080)
Step: 25199, Reward: -271.0442 [163.96], Avg: -961.6493 (0.078)
Step: 25399, Reward: -295.5263 [90.53], Avg: -957.1170 (0.077)
Step: 25599, Reward: -146.9351 [51.08], Avg: -951.1865 (0.075)
Step: 25799, Reward: -140.5640 [114.27], Avg: -945.7885 (0.074)
Step: 25999, Reward: -283.4312 [88.49], Avg: -941.3741 (0.072)
Step: 26199, Reward: -166.8887 [121.50], Avg: -936.3895 (0.071)
Step: 26399, Reward: -98.7765 [49.41], Avg: -930.4182 (0.069)
Step: 26599, Reward: -120.5178 [126.45], Avg: -925.2795 (0.068)
Step: 26799, Reward: -445.3041 [207.09], Avg: -923.2430 (0.067)
Step: 26999, Reward: -201.9734 [221.98], Avg: -919.5446 (0.065)
Step: 27199, Reward: -222.5070 [91.77], Avg: -915.0941 (0.064)
Step: 27399, Reward: -180.0087 [55.82], Avg: -910.1360 (0.063)
Step: 27599, Reward: -147.0065 [105.24], Avg: -905.3687 (0.062)
Step: 27799, Reward: -248.7750 [75.47], Avg: -901.1880 (0.060)
Step: 27999, Reward: -145.2208 [48.51], Avg: -896.1347 (0.059)
Step: 28199, Reward: -97.7395 [86.39], Avg: -891.0850 (0.058)
Step: 28399, Reward: -122.5733 [70.51], Avg: -886.1695 (0.057)
Step: 28599, Reward: -121.7861 [72.38], Avg: -881.3303 (0.056)
Step: 28799, Reward: -198.0389 [96.49], Avg: -877.2553 (0.055)
Step: 28999, Reward: -236.8768 [104.04], Avg: -873.5564 (0.053)
Step: 29199, Reward: -171.9040 [55.22], Avg: -869.1288 (0.052)
Step: 29399, Reward: -189.4531 [93.95], Avg: -865.1442 (0.051)
Step: 29599, Reward: -211.8852 [124.02], Avg: -861.5683 (0.050)
Step: 29799, Reward: -122.8676 [3.29], Avg: -856.6326 (0.049)
Step: 29999, Reward: -120.1472 [71.53], Avg: -852.1996 (0.048)
Step: 30199, Reward: -188.5412 [118.66], Avg: -848.5903 (0.047)
Step: 30399, Reward: -148.2789 [118.24], Avg: -844.7609 (0.046)
Step: 30599, Reward: -121.5122 [75.97], Avg: -840.5303 (0.045)
Step: 30799, Reward: -186.9219 [86.72], Avg: -836.8492 (0.045)
Step: 30999, Reward: -214.4108 [112.28], Avg: -833.5578 (0.044)
Step: 31199, Reward: -122.9237 [76.10], Avg: -829.4903 (0.043)
Step: 31399, Reward: -169.4615 [90.31], Avg: -825.8615 (0.042)
Step: 31599, Reward: -185.0616 [81.65], Avg: -822.3226 (0.041)
Step: 31799, Reward: -121.1911 [72.65], Avg: -818.3698 (0.040)
Step: 31999, Reward: -124.5649 [2.94], Avg: -814.0519 (0.039)
Step: 32199, Reward: -163.2265 [90.64], Avg: -810.5725 (0.039)
Step: 32399, Reward: -142.1542 [83.56], Avg: -806.9623 (0.038)
Step: 32599, Reward: -185.1969 [115.86], Avg: -803.8586 (0.037)
Step: 32799, Reward: -216.5995 [137.45], Avg: -801.1158 (0.036)
Step: 32999, Reward: -194.4384 [100.73], Avg: -798.0495 (0.036)
Step: 33199, Reward: -157.9984 [72.70], Avg: -794.6317 (0.035)
Step: 33399, Reward: -97.4810 [47.95], Avg: -790.7443 (0.034)
Step: 33599, Reward: -205.6893 [75.64], Avg: -787.7120 (0.034)
Step: 33799, Reward: -168.8821 [53.82], Avg: -784.3687 (0.033)
Step: 33999, Reward: -95.7142 [46.83], Avg: -780.5933 (0.032)
Step: 34199, Reward: -227.1090 [65.21], Avg: -777.7379 (0.032)
Step: 34399, Reward: -144.4950 [88.49], Avg: -774.5707 (0.031)
Step: 34599, Reward: -143.8944 [44.56], Avg: -771.1827 (0.030)
Step: 34799, Reward: -177.7264 [78.68], Avg: -768.2242 (0.030)
Step: 34999, Reward: -141.9872 [110.73], Avg: -765.2785 (0.029)
Step: 35199, Reward: -191.0209 [59.14], Avg: -762.3517 (0.029)
Step: 35399, Reward: -96.3903 [47.86], Avg: -758.8596 (0.028)
Step: 35599, Reward: -208.9761 [105.72], Avg: -756.3643 (0.027)
Step: 35799, Reward: -172.3766 [124.78], Avg: -753.7989 (0.027)
Step: 35999, Reward: -160.7205 [76.19], Avg: -750.9273 (0.026)
Step: 36199, Reward: -141.8402 [108.14], Avg: -748.1596 (0.026)
Step: 36399, Reward: -120.2258 [72.58], Avg: -745.1082 (0.025)
Step: 36599, Reward: -117.8048 [104.19], Avg: -742.2496 (0.025)
Step: 36799, Reward: -121.3679 [1.00], Avg: -738.8807 (0.024)
Step: 36999, Reward: -139.6194 [134.89], Avg: -736.3706 (0.024)
Step: 37199, Reward: -142.2256 [111.65], Avg: -733.7765 (0.023)
Step: 37399, Reward: -161.2272 [78.23], Avg: -731.1331 (0.023)
Step: 37599, Reward: -121.0321 [3.51], Avg: -727.9065 (0.022)
Step: 37799, Reward: -161.7992 [112.06], Avg: -725.5041 (0.022)
Step: 37999, Reward: -98.0634 [48.13], Avg: -722.4551 (0.022)
Step: 38199, Reward: -145.4262 [46.06], Avg: -719.6752 (0.021)
Step: 38399, Reward: -248.7983 [79.03], Avg: -717.6344 (0.021)
Step: 38599, Reward: -95.5406 [47.45], Avg: -714.6569 (0.020)
Step: 38799, Reward: -96.6742 [47.88], Avg: -711.7183 (0.020)
Step: 38999, Reward: -95.4539 [47.22], Avg: -708.8001 (0.020)
Step: 39199, Reward: -143.5594 [46.78], Avg: -706.1549 (0.020)
Step: 39399, Reward: -118.9020 [1.00], Avg: -703.1790 (0.020)
Step: 39599, Reward: -119.9666 [74.11], Avg: -700.6077 (0.020)
Step: 39799, Reward: -117.8384 [126.38], Avg: -698.3143 (0.020)
Step: 39999, Reward: -119.0915 [2.19], Avg: -695.4291 (0.020)
Step: 40199, Reward: -183.6942 [81.44], Avg: -693.2884 (0.020)
Step: 40399, Reward: -97.7699 [48.20], Avg: -690.5789 (0.020)
Step: 40599, Reward: -159.7021 [76.69], Avg: -688.3415 (0.020)
Step: 40799, Reward: -223.1223 [56.39], Avg: -686.3374 (0.020)
Step: 40999, Reward: -119.1325 [74.08], Avg: -683.9320 (0.020)
Step: 41199, Reward: -142.0736 [47.96], Avg: -681.5344 (0.020)
Step: 41399, Reward: -137.9493 [84.76], Avg: -679.3179 (0.020)
Step: 41599, Reward: -221.5435 [86.96], Avg: -677.5351 (0.020)
Step: 41799, Reward: -116.7038 [103.70], Avg: -675.3479 (0.020)
Step: 41999, Reward: -163.3324 [88.98], Avg: -673.3334 (0.020)
Step: 42199, Reward: -182.1520 [83.90], Avg: -671.4032 (0.020)
Step: 42399, Reward: -73.4678 [59.08], Avg: -668.8614 (0.020)
Step: 42599, Reward: -141.8647 [42.97], Avg: -666.5890 (0.020)
Step: 42799, Reward: -162.5412 [52.44], Avg: -664.4787 (0.020)
Step: 42999, Reward: -200.8060 [71.69], Avg: -662.6555 (0.020)
Step: 43199, Reward: -160.1050 [79.54], Avg: -660.6971 (0.020)
Step: 43399, Reward: -74.5713 [59.40], Avg: -658.2698 (0.020)
Step: 43599, Reward: -144.6468 [44.91], Avg: -656.1198 (0.020)
Step: 43799, Reward: -166.7622 [54.05], Avg: -654.1321 (0.020)
Step: 43999, Reward: -74.0947 [58.99], Avg: -651.7637 (0.020)
Step: 44199, Reward: -162.9431 [51.83], Avg: -649.7864 (0.020)
Step: 44399, Reward: -99.3239 [89.32], Avg: -647.7091 (0.020)
Step: 44599, Reward: -164.4668 [53.12], Avg: -645.7803 (0.020)
Step: 44799, Reward: -120.9579 [75.53], Avg: -643.7745 (0.020)
Step: 44999, Reward: -158.1992 [78.09], Avg: -641.9635 (0.020)
Step: 45199, Reward: -99.0722 [89.25], Avg: -639.9563 (0.020)
Step: 45399, Reward: -139.8381 [85.70], Avg: -638.1306 (0.020)
Step: 45599, Reward: -121.5894 [1.88], Avg: -635.8733 (0.020)
Step: 45799, Reward: -194.6200 [93.83], Avg: -634.3562 (0.020)
Step: 45999, Reward: -167.2832 [94.84], Avg: -632.7378 (0.020)
Step: 46199, Reward: -116.9945 [69.95], Avg: -630.8080 (0.020)
Step: 46399, Reward: -72.6788 [57.25], Avg: -628.6490 (0.020)
Step: 46599, Reward: -199.4832 [97.97], Avg: -627.2276 (0.020)
Step: 46799, Reward: -145.0937 [37.36], Avg: -625.3268 (0.020)
Step: 46999, Reward: -184.7633 [114.44], Avg: -623.9391 (0.020)
Step: 47199, Reward: -146.6103 [43.79], Avg: -622.1020 (0.020)
Step: 47399, Reward: -117.7604 [73.72], Avg: -620.2850 (0.020)
Step: 47599, Reward: -144.4242 [45.06], Avg: -618.4749 (0.020)
Step: 47799, Reward: -141.2742 [44.59], Avg: -616.6648 (0.020)
Step: 47999, Reward: -164.0923 [91.27], Avg: -615.1594 (0.020)
Step: 48199, Reward: -162.9253 [89.86], Avg: -613.6558 (0.020)
Step: 48399, Reward: -165.1951 [93.99], Avg: -612.1910 (0.020)
Step: 48599, Reward: -122.7402 [75.24], Avg: -610.4865 (0.020)
Step: 48799, Reward: -200.8426 [67.78], Avg: -609.0854 (0.020)
Step: 48999, Reward: -168.6032 [55.87], Avg: -607.5155 (0.020)
Step: 49199, Reward: -168.0048 [124.25], Avg: -606.2340 (0.020)
Step: 49399, Reward: -146.0217 [118.09], Avg: -604.8488 (0.020)
Step: 49599, Reward: -187.2828 [81.58], Avg: -603.4941 (0.020)
Step: 49799, Reward: -186.8438 [53.76], Avg: -602.0367 (0.020)
Step: 49999, Reward: -48.6265 [56.94], Avg: -600.0508 (0.020)
Step: 50199, Reward: -152.7747 [100.45], Avg: -598.6690 (0.020)
Step: 50399, Reward: -159.7235 [50.39], Avg: -597.1271 (0.020)
Step: 50599, Reward: -146.2964 [90.94], Avg: -595.7046 (0.020)
Step: 50799, Reward: -140.7649 [44.11], Avg: -594.0872 (0.020)
Step: 50999, Reward: -168.8495 [95.74], Avg: -592.7950 (0.020)
Step: 51199, Reward: -116.0863 [69.82], Avg: -591.2056 (0.020)
Step: 51399, Reward: -95.5786 [46.44], Avg: -589.4578 (0.020)
Step: 51599, Reward: -171.7145 [98.62], Avg: -588.2209 (0.020)
Step: 51799, Reward: -137.7215 [83.70], Avg: -586.8047 (0.020)
Step: 51999, Reward: -186.3492 [55.24], Avg: -585.4769 (0.020)
Step: 52199, Reward: -71.9243 [57.86], Avg: -583.7310 (0.020)
Step: 52399, Reward: -144.2060 [89.33], Avg: -582.3944 (0.020)
Step: 52599, Reward: -148.0657 [95.58], Avg: -581.1063 (0.020)
Step: 52799, Reward: -73.4535 [97.06], Avg: -579.5511 (0.020)
Step: 52999, Reward: -94.4335 [134.87], Avg: -578.2294 (0.020)
Step: 53199, Reward: -181.6680 [85.36], Avg: -577.0595 (0.020)
Step: 53399, Reward: -120.5587 [74.49], Avg: -575.6287 (0.020)
Step: 53599, Reward: -140.4517 [44.91], Avg: -574.1725 (0.020)
Step: 53799, Reward: -164.3226 [113.67], Avg: -573.0714 (0.020)
Step: 53999, Reward: -165.9757 [96.61], Avg: -571.9215 (0.020)
Step: 54199, Reward: -121.3699 [74.12], Avg: -570.5324 (0.020)
Step: 54399, Reward: -119.6114 [1.70], Avg: -568.8809 (0.020)
Step: 54599, Reward: -190.7103 [120.72], Avg: -567.9379 (0.020)
Step: 54799, Reward: -188.1125 [84.81], Avg: -566.8612 (0.020)
Step: 54999, Reward: -165.3113 [53.58], Avg: -565.5958 (0.020)
Step: 55199, Reward: -166.0132 [55.59], Avg: -564.3495 (0.020)
Step: 55399, Reward: -121.9569 [71.19], Avg: -563.0094 (0.020)
Step: 55599, Reward: -238.1238 [62.75], Avg: -562.0665 (0.020)
Step: 55799, Reward: -98.2716 [47.87], Avg: -560.5757 (0.020)
Step: 55999, Reward: -100.2166 [48.65], Avg: -559.1053 (0.020)
Step: 56199, Reward: -123.0124 [76.33], Avg: -557.8250 (0.020)
Step: 56399, Reward: -144.6913 [44.65], Avg: -556.5183 (0.020)
Step: 56599, Reward: -119.4313 [73.96], Avg: -555.2352 (0.020)
Step: 56799, Reward: -166.4421 [54.23], Avg: -554.0571 (0.020)
Step: 56999, Reward: -116.8371 [70.53], Avg: -552.7705 (0.020)
Step: 57199, Reward: -205.4928 [102.98], Avg: -551.9163 (0.020)
Step: 57399, Reward: -167.4541 [57.19], Avg: -550.7760 (0.020)
Step: 57599, Reward: -179.8214 [106.93], Avg: -549.8593 (0.020)
Step: 57799, Reward: -167.6705 [57.03], Avg: -548.7341 (0.020)
Step: 57999, Reward: -173.7833 [99.60], Avg: -547.7846 (0.020)
Step: 58199, Reward: -178.0722 [69.56], Avg: -546.7532 (0.020)
Step: 58399, Reward: -144.9816 [45.03], Avg: -545.5315 (0.020)
Step: 58599, Reward: -118.8164 [103.34], Avg: -544.4278 (0.020)
Step: 58799, Reward: -143.3734 [38.73], Avg: -543.1954 (0.020)
Step: 58999, Reward: -96.0903 [47.78], Avg: -541.8418 (0.020)
Step: 59199, Reward: -93.5450 [83.82], Avg: -540.6104 (0.020)
Step: 59399, Reward: -140.2713 [106.69], Avg: -539.6217 (0.020)
Step: 59599, Reward: -140.9798 [44.54], Avg: -538.4334 (0.020)
Step: 59799, Reward: -162.9355 [157.51], Avg: -537.7044 (0.020)
Step: 59999, Reward: -190.9258 [98.93], Avg: -536.8782 (0.020)
Step: 60199, Reward: -136.0828 [109.94], Avg: -535.9119 (0.020)
Step: 60399, Reward: -152.4689 [62.29], Avg: -534.8485 (0.020)
Step: 60599, Reward: -119.9702 [74.57], Avg: -533.7254 (0.020)
Step: 60799, Reward: -94.3057 [47.02], Avg: -532.4346 (0.020)
Step: 60999, Reward: -117.1343 [70.08], Avg: -531.3028 (0.020)
Step: 61199, Reward: -122.2530 [2.76], Avg: -529.9750 (0.020)
Step: 61399, Reward: -142.4115 [48.67], Avg: -528.8711 (0.020)
Step: 61599, Reward: -98.1604 [48.07], Avg: -527.6288 (0.020)
Step: 61799, Reward: -135.7362 [105.92], Avg: -526.7033 (0.020)
Step: 61999, Reward: -141.9235 [44.27], Avg: -525.6049 (0.020)
Step: 62199, Reward: -97.2957 [47.92], Avg: -524.3818 (0.020)
Step: 62399, Reward: -139.6809 [110.81], Avg: -523.5039 (0.020)
Step: 62599, Reward: -141.4411 [44.96], Avg: -522.4269 (0.020)
Step: 62799, Reward: -215.9349 [85.70], Avg: -521.7237 (0.020)
Step: 62999, Reward: -69.9622 [56.36], Avg: -520.4685 (0.020)
Step: 63199, Reward: -139.9541 [86.88], Avg: -519.5393 (0.020)
Step: 63399, Reward: -48.0487 [58.18], Avg: -518.2354 (0.020)
Step: 63599, Reward: -93.3420 [85.46], Avg: -517.1681 (0.020)
Step: 63799, Reward: -189.0005 [53.78], Avg: -516.3079 (0.020)
Step: 63999, Reward: -146.5869 [91.89], Avg: -515.4397 (0.020)
Step: 64199, Reward: -163.9110 [111.72], Avg: -514.6926 (0.020)
Step: 64399, Reward: -168.1331 [119.27], Avg: -513.9868 (0.020)
Step: 64599, Reward: -120.6997 [74.96], Avg: -513.0012 (0.020)
Step: 64799, Reward: -94.2295 [114.06], Avg: -512.0608 (0.020)
Step: 64999, Reward: -161.9080 [87.97], Avg: -511.2540 (0.020)
Step: 65199, Reward: -169.6889 [67.33], Avg: -510.4128 (0.020)
Step: 65399, Reward: -133.3728 [98.84], Avg: -509.5621 (0.020)
Step: 65599, Reward: -117.9636 [72.32], Avg: -508.5887 (0.020)
Step: 65799, Reward: -92.8618 [82.92], Avg: -507.5771 (0.020)
Step: 65999, Reward: -140.2242 [44.66], Avg: -506.5992 (0.020)
Step: 66199, Reward: -166.1742 [95.78], Avg: -505.8601 (0.020)
Step: 66399, Reward: -138.6210 [111.98], Avg: -505.0913 (0.020)
Step: 66599, Reward: -119.8781 [73.35], Avg: -504.1547 (0.020)
Step: 66799, Reward: -223.2516 [61.08], Avg: -503.4966 (0.020)
Step: 66999, Reward: -185.9397 [86.01], Avg: -502.8054 (0.020)
Step: 67199, Reward: -139.9501 [133.40], Avg: -502.1225 (0.020)
Step: 67399, Reward: -146.6060 [90.23], Avg: -501.3353 (0.020)
Step: 67599, Reward: -141.1562 [136.79], Avg: -500.6743 (0.020)
Step: 67799, Reward: -161.7207 [50.67], Avg: -499.8240 (0.020)
Step: 67999, Reward: -188.8993 [116.76], Avg: -499.2529 (0.020)
Step: 68199, Reward: -163.3110 [54.40], Avg: -498.4272 (0.020)
Step: 68399, Reward: -165.9666 [53.53], Avg: -497.6117 (0.020)
Step: 68599, Reward: -169.8328 [64.17], Avg: -496.8431 (0.020)
Step: 68799, Reward: -165.5749 [54.55], Avg: -496.0387 (0.020)
Step: 68999, Reward: -136.1555 [103.17], Avg: -495.2946 (0.020)
Step: 69199, Reward: -114.6257 [68.69], Avg: -494.3929 (0.020)
Step: 69399, Reward: -120.9720 [74.04], Avg: -493.5302 (0.020)
Step: 69599, Reward: -96.6653 [48.21], Avg: -492.5283 (0.020)
Step: 69799, Reward: -120.5159 [2.51], Avg: -491.4695 (0.020)
Step: 69999, Reward: -162.2676 [84.22], Avg: -490.7696 (0.020)
Step: 70199, Reward: -73.5937 [59.28], Avg: -489.7499 (0.020)
Step: 70399, Reward: -142.9291 [44.63], Avg: -488.8914 (0.020)
Step: 70599, Reward: -138.0179 [131.22], Avg: -488.2692 (0.020)
Step: 70799, Reward: -142.7578 [48.76], Avg: -487.4309 (0.020)
Step: 70999, Reward: -269.2156 [86.74], Avg: -487.0605 (0.020)
Step: 71199, Reward: -142.9641 [89.65], Avg: -486.3458 (0.020)
Step: 71399, Reward: -189.4114 [92.72], Avg: -485.7738 (0.020)
Step: 71599, Reward: -115.0406 [101.21], Avg: -485.0209 (0.020)
Step: 71799, Reward: -190.5164 [60.31], Avg: -484.3686 (0.020)
Step: 71999, Reward: -164.7261 [56.02], Avg: -483.6363 (0.020)
Step: 72199, Reward: -96.6145 [47.67], Avg: -482.6963 (0.020)
Step: 72399, Reward: -94.4793 [47.07], Avg: -481.7539 (0.020)
Step: 72599, Reward: -124.1982 [114.45], Avg: -481.0841 (0.020)
Step: 72799, Reward: -119.1454 [76.16], Avg: -480.2990 (0.020)
Step: 72999, Reward: -166.0733 [56.19], Avg: -479.5921 (0.020)
Step: 73199, Reward: -188.7296 [97.78], Avg: -479.0646 (0.020)
Step: 73399, Reward: -192.4107 [60.65], Avg: -478.4487 (0.020)
Step: 73599, Reward: -115.3645 [101.28], Avg: -477.7373 (0.020)
Step: 73799, Reward: -229.8469 [99.12], Avg: -477.3341 (0.020)
Step: 73999, Reward: -184.5158 [84.02], Avg: -476.7698 (0.020)
Step: 74199, Reward: -228.2635 [65.72], Avg: -476.2771 (0.020)
Step: 74399, Reward: -188.7266 [87.31], Avg: -475.7388 (0.020)
Step: 74599, Reward: -119.8175 [1.34], Avg: -474.7882 (0.020)
Step: 74799, Reward: -198.4235 [99.53], Avg: -474.3154 (0.020)
Step: 74999, Reward: -141.7310 [38.66], Avg: -473.5316 (0.020)
Step: 75199, Reward: -157.4713 [131.49], Avg: -473.0407 (0.020)
Step: 75399, Reward: -186.8191 [88.63], Avg: -472.5166 (0.020)
Step: 75599, Reward: -164.4241 [54.70], Avg: -471.8463 (0.020)
Step: 75799, Reward: -397.8453 [553.91], Avg: -473.1125 (0.020)
Step: 75999, Reward: -144.2297 [48.04], Avg: -472.3735 (0.020)
Step: 76199, Reward: -168.2918 [92.69], Avg: -471.8186 (0.020)
Step: 76399, Reward: -116.5759 [2.40], Avg: -470.8949 (0.020)
Step: 76599, Reward: -117.1059 [70.78], Avg: -470.1560 (0.020)
Step: 76799, Reward: -97.0101 [48.34], Avg: -469.3102 (0.020)
Step: 76999, Reward: -158.5149 [106.44], Avg: -468.7794 (0.020)
Step: 77199, Reward: -49.1074 [58.86], Avg: -467.8446 (0.020)
Step: 77399, Reward: -123.7145 [73.69], Avg: -467.1458 (0.020)
Step: 77599, Reward: -76.3229 [95.06], Avg: -466.3835 (0.020)
Step: 77799, Reward: -122.2706 [102.62], Avg: -465.7627 (0.020)
Step: 77999, Reward: -123.0766 [74.08], Avg: -465.0740 (0.020)
Step: 78199, Reward: -148.5138 [141.79], Avg: -464.6270 (0.020)
Step: 78399, Reward: -167.4123 [92.45], Avg: -464.1046 (0.020)
Step: 78599, Reward: -142.1289 [40.77], Avg: -463.3891 (0.020)
Step: 78799, Reward: -121.2307 [74.35], Avg: -462.7094 (0.020)
Step: 78999, Reward: -147.9566 [42.77], Avg: -462.0208 (0.020)
Step: 79199, Reward: -144.9651 [42.57], Avg: -461.3277 (0.020)
Step: 79399, Reward: -99.7882 [47.87], Avg: -460.5376 (0.020)
Step: 79599, Reward: -188.6335 [76.70], Avg: -460.0471 (0.020)
Step: 79799, Reward: -193.7803 [56.74], Avg: -459.5220 (0.020)
Step: 79999, Reward: -197.8756 [57.81], Avg: -459.0124 (0.020)
Step: 80199, Reward: -194.1158 [119.87], Avg: -458.6507 (0.020)
Step: 80399, Reward: -151.1158 [96.08], Avg: -458.1247 (0.020)
Step: 80599, Reward: -193.7635 [117.89], Avg: -457.7613 (0.020)
Step: 80799, Reward: -206.7262 [108.42], Avg: -457.4083 (0.020)
Step: 80999, Reward: -216.2886 [112.15], Avg: -457.0898 (0.020)
Step: 81199, Reward: -97.7266 [90.32], Avg: -456.4271 (0.020)
Step: 81399, Reward: -218.0136 [88.94], Avg: -456.0599 (0.020)
Step: 81599, Reward: -118.9732 [3.19], Avg: -455.2415 (0.020)
Step: 81799, Reward: -149.6393 [124.07], Avg: -454.7977 (0.020)
Step: 81999, Reward: -199.3790 [96.01], Avg: -454.4089 (0.020)
Step: 82199, Reward: -75.6143 [59.86], Avg: -453.6329 (0.020)
Step: 82399, Reward: -193.2926 [95.16], Avg: -453.2320 (0.020)
Step: 82599, Reward: -214.3096 [108.41], Avg: -452.9160 (0.020)
Step: 82799, Reward: -118.7368 [74.34], Avg: -452.2883 (0.020)
Step: 82999, Reward: -167.3078 [92.80], Avg: -451.8253 (0.020)
Step: 83199, Reward: -93.7419 [84.72], Avg: -451.1681 (0.020)
Step: 83399, Reward: -154.7329 [62.18], Avg: -450.6064 (0.020)
Step: 83599, Reward: -192.1555 [60.03], Avg: -450.1317 (0.020)
Step: 83799, Reward: -188.6910 [84.77], Avg: -449.7100 (0.020)
Step: 83999, Reward: -100.0145 [48.47], Avg: -448.9928 (0.020)
Step: 84199, Reward: -150.5934 [95.87], Avg: -448.5117 (0.020)
Step: 84399, Reward: -99.2427 [92.87], Avg: -447.9042 (0.020)
Step: 84599, Reward: -75.7330 [59.59], Avg: -447.1652 (0.020)
Step: 84799, Reward: -147.9183 [46.31], Avg: -446.5686 (0.020)
Step: 84999, Reward: -148.9083 [91.51], Avg: -446.0836 (0.020)
Step: 85199, Reward: -144.5087 [44.00], Avg: -445.4789 (0.020)
Step: 85399, Reward: -121.9906 [108.74], Avg: -444.9760 (0.020)
Step: 85599, Reward: -120.6134 [107.35], Avg: -444.4690 (0.020)
Step: 85799, Reward: -166.2746 [53.51], Avg: -443.9453 (0.020)
Step: 85999, Reward: -188.6158 [85.78], Avg: -443.5509 (0.020)
Step: 86199, Reward: -118.8849 [74.72], Avg: -442.9710 (0.020)
Step: 86399, Reward: -163.4433 [85.17], Avg: -442.5211 (0.020)
Step: 86599, Reward: -188.7699 [92.50], Avg: -442.1487 (0.020)
Step: 86799, Reward: -142.4305 [110.56], Avg: -441.7129 (0.020)
Step: 86999, Reward: -119.4352 [72.51], Avg: -441.1387 (0.020)
Step: 87199, Reward: -122.6143 [75.64], Avg: -440.5816 (0.020)
Step: 87399, Reward: -167.5498 [90.10], Avg: -440.1630 (0.020)
Step: 87599, Reward: -157.9584 [101.43], Avg: -439.7503 (0.020)
Step: 87799, Reward: -77.2325 [57.25], Avg: -439.0549 (0.020)
Step: 87999, Reward: -105.9359 [95.99], Avg: -438.5160 (0.020)
Step: 88199, Reward: -166.8518 [55.07], Avg: -438.0249 (0.020)
Step: 88399, Reward: -122.8371 [105.47], Avg: -437.5504 (0.020)
Step: 88599, Reward: -143.9693 [43.54], Avg: -436.9860 (0.020)
Step: 88799, Reward: -185.7207 [110.73], Avg: -436.6694 (0.020)
Step: 88999, Reward: -174.9810 [122.52], Avg: -436.3567 (0.020)
Step: 89199, Reward: -169.8367 [56.67], Avg: -435.8862 (0.020)
Step: 89399, Reward: -123.4269 [1.07], Avg: -435.1896 (0.020)
Step: 89599, Reward: -172.8149 [95.32], Avg: -434.8167 (0.020)
Step: 89799, Reward: -121.1893 [71.10], Avg: -434.2766 (0.020)
Step: 89999, Reward: -155.4278 [63.12], Avg: -433.7972 (0.020)
Step: 90199, Reward: -166.2051 [56.28], Avg: -433.3286 (0.020)
Step: 90399, Reward: -100.5182 [48.44], Avg: -432.6995 (0.020)
Step: 90599, Reward: -192.6959 [91.59], Avg: -432.3719 (0.020)
Step: 90799, Reward: -104.3194 [48.95], Avg: -431.7571 (0.020)
Step: 90999, Reward: -212.7940 [85.12], Avg: -431.4630 (0.020)
Step: 91199, Reward: -185.8154 [80.35], Avg: -431.1005 (0.020)
Step: 91399, Reward: -98.5129 [47.99], Avg: -430.4777 (0.020)
Step: 91599, Reward: -214.7772 [48.62], Avg: -430.1129 (0.020)
Step: 91799, Reward: -168.8967 [90.03], Avg: -429.7399 (0.020)
Step: 91999, Reward: -122.1676 [2.27], Avg: -429.0762 (0.020)
Step: 92199, Reward: -146.2043 [43.97], Avg: -428.5580 (0.020)
Step: 92399, Reward: -125.3605 [1.53], Avg: -427.9051 (0.020)
Step: 92599, Reward: -123.5404 [72.26], Avg: -427.4038 (0.020)
Step: 92799, Reward: -122.8117 [3.31], Avg: -426.7545 (0.020)
Step: 92999, Reward: -141.3504 [82.08], Avg: -426.3172 (0.020)
Step: 93199, Reward: -169.0334 [53.01], Avg: -425.8788 (0.020)
Step: 93399, Reward: -144.5964 [135.95], Avg: -425.5676 (0.020)
Step: 93599, Reward: -173.2443 [120.93], Avg: -425.2869 (0.020)
Step: 93799, Reward: -196.8083 [58.12], Avg: -424.9236 (0.020)
Step: 93999, Reward: -152.3060 [89.04], Avg: -424.5331 (0.020)
Step: 94199, Reward: -173.2209 [92.98], Avg: -424.1969 (0.020)
Step: 94399, Reward: -162.6858 [104.85], Avg: -423.8650 (0.020)
Step: 94599, Reward: -125.9749 [73.00], Avg: -423.3895 (0.020)
Step: 94799, Reward: -168.6098 [94.72], Avg: -423.0518 (0.020)
Step: 94999, Reward: -148.1248 [47.79], Avg: -422.5737 (0.020)
Step: 95199, Reward: -120.7285 [73.54], Avg: -422.0940 (0.020)
Step: 95399, Reward: -167.3244 [53.33], Avg: -421.6717 (0.020)
Step: 95599, Reward: -145.0469 [86.45], Avg: -421.2739 (0.020)
Step: 95799, Reward: -190.9931 [58.05], Avg: -420.9143 (0.020)
Step: 95999, Reward: -143.5984 [45.88], Avg: -420.4322 (0.020)
Step: 96199, Reward: -124.6213 [77.89], Avg: -419.9791 (0.020)
Step: 96399, Reward: -154.8995 [98.99], Avg: -419.6345 (0.020)
Step: 96599, Reward: -235.8651 [71.41], Avg: -419.4019 (0.020)
Step: 96799, Reward: -116.8128 [68.68], Avg: -418.9186 (0.020)
Step: 96999, Reward: -133.9554 [96.77], Avg: -418.5306 (0.020)
Step: 97199, Reward: -214.0327 [82.15], Avg: -418.2788 (0.020)
Step: 97399, Reward: -97.5314 [47.94], Avg: -417.7186 (0.020)
Step: 97599, Reward: -191.9355 [119.24], Avg: -417.5003 (0.020)
Step: 97799, Reward: -245.1193 [76.09], Avg: -417.3034 (0.020)
Step: 97999, Reward: -103.2125 [49.59], Avg: -416.7636 (0.020)
Step: 98199, Reward: -118.8409 [72.44], Avg: -416.3044 (0.020)
Step: 98399, Reward: -189.9682 [83.97], Avg: -416.0150 (0.020)
Step: 98599, Reward: -189.5456 [87.99], Avg: -415.7341 (0.020)
Step: 98799, Reward: -120.8870 [106.76], Avg: -415.3534 (0.020)
Step: 98999, Reward: -73.8381 [57.82], Avg: -414.7803 (0.020)
Step: 99199, Reward: -123.2668 [76.65], Avg: -414.3471 (0.020)
Step: 99399, Reward: -168.8760 [59.84], Avg: -413.9736 (0.020)
Step: 99599, Reward: -190.0620 [56.06], Avg: -413.6365 (0.020)
Step: 99799, Reward: -164.8826 [53.55], Avg: -413.2453 (0.020)
Step: 99999, Reward: -122.8351 [107.31], Avg: -412.8791 (0.020)
