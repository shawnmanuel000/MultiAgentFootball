Model: <class 'models.ddpg.DDPGAgent'>, Dir: Pendulum-v0
num_envs: 16, state_size: (3,), action_size: (1,), action_space: Box(1,),

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, EPS_DECAY, REPLAY_BATCH_SIZE

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, decay=EPS_DECAY, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=False)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddpg", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[3]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class AsyncAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = AsyncAgent(envs.state_size, envs.action_size, num_envs, model)
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.stack.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 199, Reward: -1410.5005 [55.98], Avg: -1466.4803 (0.980)
Step: 399, Reward: -1302.6656 [62.59], Avg: -1415.8686 (0.960)
Step: 599, Reward: -1203.2283 [148.32], Avg: -1394.4298 (0.941)
Step: 799, Reward: -1417.0687 [215.25], Avg: -1453.9027 (0.922)
Step: 999, Reward: -1270.7204 [152.06], Avg: -1447.6778 (0.904)
Step: 1199, Reward: -1357.7017 [174.86], Avg: -1461.8251 (0.886)
Step: 1399, Reward: -1330.0555 [134.22], Avg: -1462.1754 (0.868)
Step: 1599, Reward: -1314.3069 [239.37], Avg: -1473.6135 (0.851)
Step: 1799, Reward: -1199.4727 [54.07], Avg: -1449.1608 (0.834)
Step: 1999, Reward: -1413.1156 [186.32], Avg: -1464.1882 (0.817)
Step: 2199, Reward: -1226.8233 [85.79], Avg: -1450.4083 (0.801)
Step: 2399, Reward: -1295.4342 [198.19], Avg: -1454.0095 (0.785)
Step: 2599, Reward: -1172.7411 [108.75], Avg: -1440.7390 (0.769)
Step: 2799, Reward: -1176.3740 [251.82], Avg: -1439.8427 (0.754)
Step: 2999, Reward: -1255.2513 [207.43], Avg: -1441.3654 (0.739)
Step: 3199, Reward: -1288.8410 [180.69], Avg: -1443.1255 (0.724)
Step: 3399, Reward: -1246.2521 [126.22], Avg: -1438.9694 (0.709)
Step: 3599, Reward: -1285.8871 [258.95], Avg: -1444.8511 (0.695)
Step: 3799, Reward: -1170.2108 [94.99], Avg: -1435.3958 (0.681)
Step: 3999, Reward: -1461.0988 [149.87], Avg: -1444.1742 (0.668)
Step: 4199, Reward: -1195.8370 [145.04], Avg: -1439.2551 (0.654)
Step: 4399, Reward: -1349.0300 [222.10], Avg: -1445.2495 (0.641)
Step: 4599, Reward: -1164.5308 [228.03], Avg: -1442.9587 (0.628)
Step: 4799, Reward: -1208.0526 [351.50], Avg: -1447.8169 (0.616)
Step: 4999, Reward: -1242.7536 [297.18], Avg: -1451.5014 (0.603)
Step: 5199, Reward: -1184.2954 [245.51], Avg: -1450.6670 (0.591)
Step: 5399, Reward: -1075.8231 [54.46], Avg: -1438.8009 (0.580)
Step: 5599, Reward: -1163.7083 [186.59], Avg: -1435.6401 (0.568)
Step: 5799, Reward: -968.2480 [161.36], Avg: -1425.0874 (0.557)
Step: 5999, Reward: -917.8146 [64.47], Avg: -1410.3272 (0.545)
Step: 6199, Reward: -989.4369 [194.85], Avg: -1403.0355 (0.535)
Step: 6399, Reward: -948.9310 [88.09], Avg: -1391.5975 (0.524)
Step: 6599, Reward: -924.3878 [126.55], Avg: -1381.2744 (0.513)
Step: 6799, Reward: -1001.9474 [86.64], Avg: -1372.6660 (0.503)
Step: 6999, Reward: -930.1493 [103.57], Avg: -1362.9817 (0.493)
Step: 7199, Reward: -877.6980 [74.34], Avg: -1351.5666 (0.483)
Step: 7399, Reward: -1204.3581 [332.89], Avg: -1356.5849 (0.474)
Step: 7599, Reward: -1372.4495 [135.73], Avg: -1360.5741 (0.464)
Step: 7799, Reward: -1052.8164 [81.49], Avg: -1354.7723 (0.455)
Step: 7999, Reward: -1251.1090 [321.04], Avg: -1360.2068 (0.446)
Step: 8199, Reward: -1004.2916 [194.83], Avg: -1356.2778 (0.437)
Step: 8399, Reward: -931.3549 [82.67], Avg: -1348.1291 (0.428)
Step: 8599, Reward: -964.3199 [92.36], Avg: -1341.3511 (0.419)
Step: 8799, Reward: -1003.9174 [108.60], Avg: -1336.1504 (0.411)
Step: 8999, Reward: -936.9741 [110.96], Avg: -1329.7455 (0.403)
Step: 9199, Reward: -888.3721 [39.74], Avg: -1321.0143 (0.395)
Step: 9399, Reward: -885.8151 [45.79], Avg: -1312.7290 (0.387)
Step: 9599, Reward: -970.7167 [202.06], Avg: -1309.8134 (0.379)
Step: 9799, Reward: -1038.5216 [190.92], Avg: -1308.1732 (0.372)
Step: 9999, Reward: -931.5215 [94.04], Avg: -1302.5209 (0.364)
Step: 10199, Reward: -930.6127 [109.97], Avg: -1297.3848 (0.357)
Step: 10399, Reward: -895.2037 [165.08], Avg: -1292.8251 (0.350)
Step: 10599, Reward: -1008.0212 [185.92], Avg: -1290.9594 (0.343)
Step: 10799, Reward: -883.4488 [35.76], Avg: -1284.0750 (0.336)
Step: 10999, Reward: -959.8941 [55.88], Avg: -1279.1968 (0.329)
Step: 11199, Reward: -877.9395 [37.95], Avg: -1272.7091 (0.323)
Step: 11399, Reward: -909.7159 [63.32], Avg: -1267.4516 (0.316)
Step: 11599, Reward: -893.3944 [125.73], Avg: -1263.1700 (0.310)
Step: 11799, Reward: -839.7470 [71.57], Avg: -1257.2063 (0.304)
Step: 11999, Reward: -800.7333 [128.45], Avg: -1251.7393 (0.298)
Step: 12199, Reward: -778.8033 [104.90], Avg: -1245.7060 (0.292)
Step: 12399, Reward: -764.4045 [91.34], Avg: -1239.4162 (0.286)
Step: 12599, Reward: -634.3596 [82.55], Avg: -1231.1225 (0.280)
Step: 12799, Reward: -558.4119 [65.23], Avg: -1221.6306 (0.274)
Step: 12999, Reward: -597.0050 [47.83], Avg: -1212.7568 (0.269)
Step: 13199, Reward: -247.9524 [152.74], Avg: -1200.4529 (0.264)
Step: 13399, Reward: -515.9412 [178.21], Avg: -1192.8962 (0.258)
Step: 13599, Reward: -300.9512 [174.13], Avg: -1182.3400 (0.253)
Step: 13799, Reward: -496.0412 [188.90], Avg: -1175.1313 (0.248)
Step: 13999, Reward: -324.9932 [165.24], Avg: -1165.3470 (0.243)
Step: 14199, Reward: -362.0469 [201.11], Avg: -1156.8655 (0.238)
Step: 14399, Reward: -309.2685 [138.23], Avg: -1147.0132 (0.233)
Step: 14599, Reward: -246.1272 [75.78], Avg: -1135.7104 (0.229)
Step: 14799, Reward: -420.3927 [99.12], Avg: -1127.3833 (0.224)
Step: 14999, Reward: -221.4979 [94.51], Avg: -1116.5650 (0.220)
Step: 15199, Reward: -322.8640 [218.31], Avg: -1108.9941 (0.215)
Step: 15399, Reward: -255.8945 [227.95], Avg: -1100.8753 (0.211)
Step: 15599, Reward: -270.9400 [126.29], Avg: -1091.8542 (0.207)
Step: 15799, Reward: -246.5134 [77.05], Avg: -1082.1290 (0.203)
Step: 15999, Reward: -220.5099 [161.44], Avg: -1073.3767 (0.199)
Step: 16199, Reward: -481.0433 [221.61], Avg: -1068.7999 (0.195)
Step: 16399, Reward: -223.9963 [119.55], Avg: -1059.9553 (0.191)
Step: 16599, Reward: -231.1914 [159.27], Avg: -1051.8890 (0.187)
Step: 16799, Reward: -272.1339 [139.09], Avg: -1044.2621 (0.183)
Step: 16999, Reward: -341.4617 [253.29], Avg: -1038.9737 (0.180)
Step: 17199, Reward: -165.2267 [114.65], Avg: -1030.1470 (0.176)
Step: 17399, Reward: -197.7244 [99.35], Avg: -1021.7209 (0.172)
Step: 17599, Reward: -170.9241 [93.37], Avg: -1013.1138 (0.169)
Step: 17799, Reward: -121.0810 [4.12], Avg: -1003.1373 (0.166)
Step: 17999, Reward: -129.3876 [141.40], Avg: -995.0000 (0.162)
Step: 18199, Reward: -197.3977 [141.07], Avg: -987.7854 (0.159)
Step: 18399, Reward: -144.2409 [109.79], Avg: -979.8098 (0.156)
Step: 18599, Reward: -220.3075 [89.44], Avg: -972.6049 (0.153)
Step: 18799, Reward: -240.7617 [72.45], Avg: -965.5900 (0.150)
Step: 18999, Reward: -166.4097 [52.64], Avg: -957.7318 (0.147)
Step: 19199, Reward: -196.7035 [93.09], Avg: -950.7741 (0.144)
Step: 19399, Reward: -134.7453 [125.18], Avg: -943.6519 (0.141)
Step: 19599, Reward: -120.2142 [105.08], Avg: -936.3217 (0.138)
Step: 19799, Reward: -152.6910 [94.88], Avg: -929.3647 (0.135)
Step: 19999, Reward: -121.7226 [78.07], Avg: -922.0689 (0.133)
Step: 20199, Reward: -125.3217 [2.69], Avg: -914.2070 (0.130)
Step: 20399, Reward: -162.2270 [111.05], Avg: -907.9234 (0.127)
Step: 20599, Reward: -149.6753 [121.77], Avg: -901.7440 (0.125)
Step: 20799, Reward: -203.8077 [151.19], Avg: -896.4868 (0.122)
Step: 20999, Reward: -120.8232 [76.59], Avg: -889.8289 (0.120)
Step: 21199, Reward: -199.4857 [99.53], Avg: -884.2553 (0.117)
Step: 21399, Reward: -242.5767 [72.88], Avg: -878.9394 (0.115)
Step: 21599, Reward: -165.6579 [81.99], Avg: -873.0941 (0.113)
Step: 21799, Reward: -562.3765 [412.39], Avg: -874.0269 (0.111)
Step: 21999, Reward: -193.1333 [57.03], Avg: -868.3554 (0.108)
Step: 22199, Reward: -119.1686 [71.54], Avg: -862.2505 (0.106)
Step: 22399, Reward: -442.2942 [219.57], Avg: -860.4613 (0.104)
Step: 22599, Reward: -900.3450 [38.36], Avg: -861.1537 (0.102)
Step: 22799, Reward: -168.6710 [114.08], Avg: -856.0800 (0.100)
Step: 22999, Reward: -123.9155 [110.70], Avg: -850.6759 (0.098)
Step: 23199, Reward: -146.5841 [112.27], Avg: -845.5740 (0.096)
Step: 23399, Reward: -149.4919 [87.19], Avg: -840.3699 (0.094)
Step: 23599, Reward: -127.9943 [78.12], Avg: -834.9948 (0.092)
Step: 23799, Reward: -223.1949 [121.25], Avg: -830.8725 (0.090)
Step: 23999, Reward: -261.4480 [174.04], Avg: -827.5777 (0.089)
Step: 24199, Reward: -99.5896 [87.36], Avg: -822.2832 (0.087)
Step: 24399, Reward: -148.2130 [86.46], Avg: -817.4667 (0.085)
Step: 24599, Reward: -195.1911 [88.63], Avg: -813.1281 (0.083)
Step: 24799, Reward: -101.6856 [91.29], Avg: -808.1269 (0.082)
Step: 24999, Reward: -257.0855 [104.96], Avg: -804.5583 (0.080)
Step: 25199, Reward: -99.1222 [47.58], Avg: -799.3372 (0.078)
Step: 25399, Reward: -77.2771 [58.06], Avg: -794.1089 (0.077)
Step: 25599, Reward: -170.3182 [88.20], Avg: -789.9246 (0.075)
Step: 25799, Reward: -202.3698 [65.80], Avg: -785.8800 (0.074)
Step: 25999, Reward: -123.2436 [70.25], Avg: -781.3232 (0.072)
Step: 26199, Reward: -209.7458 [104.00], Avg: -777.7539 (0.071)
Step: 26399, Reward: -136.7790 [103.27], Avg: -773.6803 (0.069)
Step: 26599, Reward: -123.7359 [128.25], Avg: -769.7578 (0.068)
Step: 26799, Reward: -178.3162 [58.72], Avg: -765.7823 (0.067)
Step: 26999, Reward: -170.8810 [85.02], Avg: -762.0053 (0.065)
Step: 27199, Reward: -102.5034 [87.46], Avg: -757.7992 (0.064)
Step: 27399, Reward: -174.2161 [93.60], Avg: -754.2226 (0.063)
Step: 27599, Reward: -173.0490 [51.65], Avg: -750.3855 (0.062)
Step: 27799, Reward: -145.6913 [102.55], Avg: -746.7729 (0.060)
Step: 27999, Reward: -197.6260 [120.71], Avg: -743.7127 (0.059)
Step: 28199, Reward: -178.4203 [55.98], Avg: -740.1005 (0.058)
Step: 28399, Reward: -214.7014 [78.10], Avg: -736.9506 (0.057)
Step: 28599, Reward: -278.4855 [105.43], Avg: -734.4818 (0.056)
Step: 28799, Reward: -181.7592 [55.28], Avg: -731.0273 (0.055)
Step: 28999, Reward: -198.0837 [88.70], Avg: -727.9636 (0.053)
Step: 29199, Reward: -132.8982 [72.00], Avg: -724.3810 (0.052)
Step: 29399, Reward: -168.0736 [54.48], Avg: -720.9672 (0.051)
Step: 29599, Reward: -174.3857 [106.82], Avg: -717.9958 (0.050)
Step: 29799, Reward: -142.7250 [111.97], Avg: -714.8865 (0.049)
Step: 29999, Reward: -192.9899 [80.42], Avg: -711.9433 (0.048)
Step: 30199, Reward: -123.3869 [77.03], Avg: -708.5557 (0.047)
Step: 30399, Reward: -165.7963 [56.29], Avg: -705.3552 (0.046)
Step: 30599, Reward: -140.1153 [129.91], Avg: -702.5099 (0.045)
Step: 30799, Reward: -152.9976 [102.21], Avg: -699.6053 (0.045)
Step: 30999, Reward: -161.5094 [112.18], Avg: -696.8575 (0.044)
Step: 31199, Reward: -75.4136 [59.42], Avg: -693.2548 (0.043)
Step: 31399, Reward: -143.7838 [88.07], Avg: -690.3160 (0.042)
Step: 31599, Reward: -120.1669 [1.52], Avg: -686.7171 (0.041)
Step: 31799, Reward: -192.0898 [91.92], Avg: -684.1843 (0.040)
Step: 31999, Reward: -121.4334 [72.94], Avg: -681.1230 (0.039)
Step: 32199, Reward: -120.8289 [74.49], Avg: -678.1056 (0.039)
Step: 32399, Reward: -163.5747 [92.84], Avg: -675.5025 (0.038)
Step: 32599, Reward: -200.0660 [75.44], Avg: -673.0486 (0.037)
Step: 32799, Reward: -92.1206 [131.53], Avg: -670.3083 (0.036)
Step: 32999, Reward: -163.5343 [78.51], Avg: -667.7127 (0.036)
Step: 33199, Reward: -142.7536 [113.45], Avg: -665.2338 (0.035)
Step: 33399, Reward: -166.5367 [122.51], Avg: -662.9811 (0.034)
Step: 33599, Reward: -182.2924 [110.81], Avg: -660.7795 (0.034)
Step: 33799, Reward: -100.5851 [92.87], Avg: -658.0142 (0.033)
Step: 33999, Reward: -142.9740 [43.44], Avg: -655.2401 (0.032)
Step: 34199, Reward: -175.5754 [64.83], Avg: -652.8142 (0.032)
Step: 34399, Reward: -54.7767 [60.60], Avg: -649.6896 (0.031)
Step: 34599, Reward: -171.5298 [99.84], Avg: -647.5027 (0.030)
Step: 34799, Reward: -96.6214 [47.32], Avg: -644.6087 (0.030)
Step: 34999, Reward: -171.7011 [91.33], Avg: -642.4282 (0.029)
Step: 35199, Reward: -216.4316 [161.91], Avg: -640.9277 (0.029)
Step: 35399, Reward: -220.4528 [136.73], Avg: -639.3246 (0.028)
Step: 35599, Reward: -237.4453 [103.67], Avg: -637.6493 (0.027)
Step: 35799, Reward: -193.5309 [91.33], Avg: -635.6784 (0.027)
Step: 35999, Reward: -100.0694 [90.29], Avg: -633.2044 (0.026)
Step: 36199, Reward: -116.6205 [69.22], Avg: -630.7328 (0.026)
Step: 36399, Reward: -171.7834 [55.57], Avg: -628.5164 (0.025)
Step: 36599, Reward: -213.1143 [63.64], Avg: -626.5942 (0.025)
Step: 36799, Reward: -145.0542 [135.66], Avg: -624.7144 (0.024)
Step: 36999, Reward: -97.3913 [85.15], Avg: -622.3243 (0.024)
Step: 37199, Reward: -184.6495 [133.07], Avg: -620.6866 (0.023)
Step: 37399, Reward: -145.2455 [83.40], Avg: -618.5902 (0.023)
Step: 37599, Reward: -145.7534 [87.11], Avg: -616.5384 (0.022)
Step: 37799, Reward: -204.3070 [74.48], Avg: -614.7514 (0.022)
Step: 37999, Reward: -144.4953 [109.57], Avg: -612.8531 (0.022)
Step: 38199, Reward: -125.9691 [76.32], Avg: -610.7036 (0.021)
Step: 38399, Reward: -151.6566 [113.16], Avg: -608.9021 (0.021)
Step: 38599, Reward: -145.7413 [41.97], Avg: -606.7197 (0.020)
Step: 38799, Reward: -215.9508 [85.38], Avg: -605.1456 (0.020)
Step: 38999, Reward: -146.6429 [42.41], Avg: -603.0117 (0.020)
Step: 39199, Reward: -174.8468 [119.16], Avg: -601.4352 (0.020)
Step: 39399, Reward: -144.3169 [84.65], Avg: -599.5444 (0.020)
Step: 39599, Reward: -186.6453 [81.92], Avg: -597.8728 (0.020)
Step: 39799, Reward: -120.2188 [68.70], Avg: -595.8177 (0.020)
Step: 39999, Reward: -101.1105 [49.29], Avg: -593.5907 (0.020)
Step: 40199, Reward: -206.5097 [69.68], Avg: -592.0116 (0.020)
Step: 40399, Reward: -138.4937 [129.70], Avg: -590.4085 (0.020)
Step: 40599, Reward: -246.2177 [104.65], Avg: -589.2285 (0.020)
Step: 40799, Reward: -171.1674 [54.12], Avg: -587.4445 (0.020)
Step: 40999, Reward: -188.4178 [90.35], Avg: -585.9388 (0.020)
Step: 41199, Reward: -101.7215 [49.92], Avg: -583.8305 (0.020)
Step: 41399, Reward: -141.6945 [37.55], Avg: -581.8760 (0.020)
Step: 41599, Reward: -100.6666 [48.84], Avg: -579.7973 (0.020)
Step: 41799, Reward: -147.9873 [46.96], Avg: -577.9559 (0.020)
Step: 41999, Reward: -161.6155 [80.67], Avg: -576.3574 (0.020)
Step: 42199, Reward: -167.8006 [55.06], Avg: -574.6821 (0.020)
Step: 42399, Reward: -74.1008 [59.14], Avg: -572.5998 (0.020)
Step: 42599, Reward: -186.5848 [91.97], Avg: -571.2193 (0.020)
Step: 42799, Reward: -122.1971 [4.41], Avg: -569.1417 (0.020)
Step: 42999, Reward: -170.0162 [119.18], Avg: -567.8397 (0.020)
Step: 43199, Reward: -150.0151 [50.97], Avg: -566.1413 (0.020)
Step: 43399, Reward: -149.0710 [48.77], Avg: -564.4440 (0.020)
Step: 43599, Reward: -146.9314 [88.29], Avg: -562.9338 (0.020)
Step: 43799, Reward: -170.7610 [95.20], Avg: -561.5778 (0.020)
Step: 43999, Reward: -123.8621 [75.44], Avg: -559.9311 (0.020)
Step: 44199, Reward: -187.4625 [118.89], Avg: -558.7837 (0.020)
Step: 44399, Reward: -206.1368 [105.06], Avg: -557.6684 (0.020)
Step: 44599, Reward: -118.3924 [123.39], Avg: -556.2519 (0.020)
Step: 44799, Reward: -97.5603 [47.84], Avg: -554.4177 (0.020)
Step: 44999, Reward: -116.0469 [143.43], Avg: -553.1069 (0.020)
Step: 45199, Reward: -234.0576 [68.84], Avg: -551.9998 (0.020)
Step: 45399, Reward: -207.5689 [103.71], Avg: -550.9393 (0.020)
Step: 45599, Reward: -98.2643 [48.29], Avg: -549.1657 (0.020)
Step: 45799, Reward: -233.5061 [100.83], Avg: -548.2276 (0.020)
Step: 45999, Reward: -222.4246 [88.31], Avg: -547.1950 (0.020)
Step: 46199, Reward: -165.7039 [83.36], Avg: -545.9044 (0.020)
Step: 46399, Reward: -197.0300 [92.42], Avg: -544.7990 (0.020)
Step: 46599, Reward: -155.4574 [102.77], Avg: -543.5691 (0.020)
Step: 46799, Reward: -165.1935 [52.87], Avg: -542.1780 (0.020)
Step: 46999, Reward: -235.2775 [101.79], Avg: -541.3052 (0.020)
Step: 47199, Reward: -188.9657 [117.17], Avg: -540.3088 (0.020)
Step: 47399, Reward: -122.0752 [71.99], Avg: -538.8478 (0.020)
Step: 47599, Reward: -190.1175 [90.85], Avg: -537.7643 (0.020)
Step: 47799, Reward: -135.0603 [98.38], Avg: -536.4909 (0.020)
Step: 47999, Reward: -164.0672 [51.44], Avg: -535.1535 (0.020)
Step: 48199, Reward: -141.8767 [112.84], Avg: -533.9899 (0.020)
Step: 48399, Reward: -229.6155 [100.19], Avg: -533.1461 (0.020)
Step: 48599, Reward: -165.7061 [55.61], Avg: -531.8629 (0.020)
Step: 48799, Reward: -165.5694 [118.96], Avg: -530.8492 (0.020)
Step: 48999, Reward: -142.6044 [106.63], Avg: -529.6998 (0.020)
Step: 49199, Reward: -167.1094 [92.50], Avg: -528.6018 (0.020)
Step: 49399, Reward: -207.4762 [87.07], Avg: -527.6542 (0.020)
Step: 49599, Reward: -120.5632 [3.74], Avg: -526.0278 (0.020)
Step: 49799, Reward: -174.6474 [73.08], Avg: -524.9101 (0.020)
Step: 49999, Reward: -95.9710 [47.79], Avg: -523.3855 (0.020)
Step: 50199, Reward: -160.4774 [80.22], Avg: -522.2593 (0.020)
Step: 50399, Reward: -97.9274 [48.66], Avg: -520.7685 (0.020)
Step: 50599, Reward: -117.8941 [70.52], Avg: -519.4549 (0.020)
Step: 50799, Reward: -212.0310 [114.76], Avg: -518.6963 (0.020)
Step: 50999, Reward: -140.9444 [43.81], Avg: -517.3868 (0.020)
Step: 51199, Reward: -162.0976 [80.96], Avg: -516.3152 (0.020)
Step: 51399, Reward: -95.0854 [46.93], Avg: -514.8588 (0.020)
Step: 51599, Reward: -142.4565 [85.95], Avg: -513.7485 (0.020)
Step: 51799, Reward: -145.6583 [52.27], Avg: -512.5291 (0.020)
Step: 51999, Reward: -118.8525 [69.43], Avg: -511.2820 (0.020)
Step: 52199, Reward: -144.6131 [87.04], Avg: -510.2106 (0.020)
Step: 52399, Reward: -201.1906 [67.22], Avg: -509.2877 (0.020)
Step: 52599, Reward: -148.4943 [44.65], Avg: -508.0856 (0.020)
Step: 52799, Reward: -146.9789 [116.21], Avg: -507.1580 (0.020)
Step: 52999, Reward: -75.6371 [61.11], Avg: -505.7602 (0.020)
Step: 53199, Reward: -166.1923 [120.12], Avg: -504.9352 (0.020)
Step: 53399, Reward: -157.4104 [68.38], Avg: -503.8897 (0.020)
Step: 53599, Reward: -205.9169 [74.26], Avg: -503.0549 (0.020)
Step: 53799, Reward: -120.1665 [72.94], Avg: -501.9027 (0.020)
Step: 53999, Reward: -143.5164 [86.07], Avg: -500.8941 (0.020)
Step: 54199, Reward: -97.1254 [47.91], Avg: -499.5810 (0.020)
Step: 54399, Reward: -118.7695 [69.98], Avg: -498.4382 (0.020)
Step: 54599, Reward: -120.9196 [3.48], Avg: -497.0681 (0.020)
Step: 54799, Reward: -178.7653 [78.12], Avg: -496.1915 (0.020)
Step: 54999, Reward: -139.5354 [134.90], Avg: -495.3851 (0.020)
Step: 55199, Reward: -141.6801 [40.28], Avg: -494.2495 (0.020)
Step: 55399, Reward: -93.3885 [86.19], Avg: -493.1135 (0.020)
Step: 55599, Reward: -99.1123 [48.17], Avg: -491.8695 (0.020)
Step: 55799, Reward: -233.4277 [70.90], Avg: -491.1973 (0.020)
Step: 55999, Reward: -166.9109 [99.03], Avg: -490.3928 (0.020)
Step: 56199, Reward: -169.5795 [95.49], Avg: -489.5910 (0.020)
Step: 56399, Reward: -117.0035 [71.95], Avg: -488.5249 (0.020)
Step: 56599, Reward: -125.1530 [4.72], Avg: -487.2575 (0.020)
Step: 56799, Reward: -122.4018 [4.05], Avg: -485.9871 (0.020)
Step: 56999, Reward: -165.6704 [82.48], Avg: -485.1526 (0.020)
Step: 57199, Reward: -209.5550 [39.64], Avg: -484.3275 (0.020)
Step: 57399, Reward: -149.8672 [41.22], Avg: -483.3058 (0.020)
Step: 57599, Reward: -143.8669 [43.32], Avg: -482.2776 (0.020)
Step: 57799, Reward: -97.2034 [48.03], Avg: -481.1114 (0.020)
Step: 57999, Reward: -178.1695 [76.90], Avg: -480.3319 (0.020)
Step: 58199, Reward: -121.1437 [6.17], Avg: -479.1188 (0.020)
Step: 58399, Reward: -212.0356 [83.81], Avg: -478.4912 (0.020)
Step: 58599, Reward: -167.9894 [91.00], Avg: -477.7420 (0.020)
Step: 58799, Reward: -200.5871 [74.35], Avg: -477.0522 (0.020)
Step: 58999, Reward: -136.2979 [102.11], Avg: -476.2432 (0.020)
Step: 59199, Reward: -122.4905 [72.06], Avg: -475.2916 (0.020)
Step: 59399, Reward: -242.4801 [70.13], Avg: -474.7438 (0.020)
Step: 59599, Reward: -143.8559 [88.90], Avg: -473.9318 (0.020)
Step: 59799, Reward: -190.5234 [88.45], Avg: -473.2797 (0.020)
Step: 59999, Reward: -168.3564 [93.67], Avg: -472.5756 (0.020)
Step: 60199, Reward: -195.5290 [89.27], Avg: -471.9517 (0.020)
Step: 60399, Reward: -145.2684 [41.81], Avg: -471.0084 (0.020)
Step: 60599, Reward: -171.1839 [142.75], Avg: -470.4900 (0.020)
Step: 60799, Reward: -170.5714 [121.72], Avg: -469.9039 (0.020)
Step: 60999, Reward: -173.0219 [100.19], Avg: -469.2589 (0.020)
Step: 61199, Reward: -121.0859 [73.21], Avg: -468.3604 (0.020)
Step: 61399, Reward: -166.3860 [52.43], Avg: -467.5475 (0.020)
Step: 61599, Reward: -173.4260 [96.74], Avg: -466.9067 (0.020)
Step: 61799, Reward: -77.4382 [60.05], Avg: -465.8406 (0.020)
Step: 61999, Reward: -99.1027 [47.92], Avg: -464.8121 (0.020)
Step: 62199, Reward: -116.0687 [98.20], Avg: -464.0065 (0.020)
Step: 62399, Reward: -170.3388 [94.95], Avg: -463.3696 (0.020)
Step: 62599, Reward: -210.8828 [108.27], Avg: -462.9088 (0.020)
Step: 62799, Reward: -166.9230 [116.39], Avg: -462.3369 (0.020)
Step: 62999, Reward: -196.7564 [89.94], Avg: -461.7793 (0.020)
Step: 63199, Reward: -198.3269 [92.38], Avg: -461.2379 (0.020)
Step: 63399, Reward: -76.3216 [59.97], Avg: -460.2129 (0.020)
Step: 63599, Reward: -166.7547 [90.73], Avg: -459.5754 (0.020)
Step: 63799, Reward: -140.8878 [133.23], Avg: -458.9940 (0.020)
Step: 63999, Reward: -140.2653 [44.30], Avg: -458.1364 (0.020)
Step: 64199, Reward: -117.3426 [69.10], Avg: -457.2900 (0.020)
Step: 64399, Reward: -187.4801 [89.78], Avg: -456.7309 (0.020)
Step: 64599, Reward: -97.9658 [48.26], Avg: -455.7696 (0.020)
Step: 64799, Reward: -164.0885 [51.35], Avg: -455.0279 (0.020)
Step: 64999, Reward: -162.1594 [113.74], Avg: -454.4767 (0.020)
Step: 65199, Reward: -205.2316 [102.32], Avg: -454.0260 (0.020)
Step: 65399, Reward: -161.3057 [155.17], Avg: -453.6054 (0.020)
Step: 65599, Reward: -163.5706 [84.80], Avg: -452.9797 (0.020)
Step: 65799, Reward: -163.5486 [116.40], Avg: -452.4537 (0.020)
Step: 65999, Reward: -123.5318 [3.75], Avg: -451.4683 (0.020)
Step: 66199, Reward: -171.0829 [59.92], Avg: -450.8023 (0.020)
Step: 66399, Reward: -98.7568 [49.00], Avg: -449.8895 (0.020)
Step: 66599, Reward: -120.6583 [77.37], Avg: -449.1332 (0.020)
Step: 66799, Reward: -243.7391 [75.41], Avg: -448.7440 (0.020)
Step: 66999, Reward: -165.8781 [57.40], Avg: -448.0710 (0.020)
Step: 67199, Reward: -149.4531 [45.94], Avg: -447.3189 (0.020)
Step: 67399, Reward: -73.2807 [59.34], Avg: -446.3851 (0.020)
Step: 67599, Reward: -188.6286 [92.11], Avg: -445.8950 (0.020)
Step: 67799, Reward: -120.5515 [68.98], Avg: -445.1388 (0.020)
Step: 67999, Reward: -212.4007 [86.96], Avg: -444.7100 (0.020)
Step: 68199, Reward: -190.6028 [83.07], Avg: -444.2085 (0.020)
Step: 68399, Reward: -166.5315 [51.10], Avg: -443.5460 (0.020)
Step: 68599, Reward: -166.0174 [85.07], Avg: -442.9849 (0.020)
Step: 68799, Reward: -102.9872 [48.54], Avg: -442.1376 (0.020)
Step: 68999, Reward: -170.4956 [118.65], Avg: -441.6942 (0.020)
Step: 69199, Reward: -166.0682 [86.99], Avg: -441.1490 (0.020)
Step: 69399, Reward: -147.2102 [85.69], Avg: -440.5488 (0.020)
Step: 69599, Reward: -145.8796 [83.23], Avg: -439.9413 (0.020)
Step: 69799, Reward: -147.6054 [116.19], Avg: -439.4365 (0.020)
Step: 69999, Reward: -262.5429 [92.53], Avg: -439.1955 (0.020)
Step: 70199, Reward: -138.5680 [82.95], Avg: -438.5753 (0.020)
Step: 70399, Reward: -146.2627 [111.19], Avg: -438.0608 (0.020)
Step: 70599, Reward: -101.0070 [48.21], Avg: -437.2425 (0.020)
Step: 70799, Reward: -71.6602 [56.59], Avg: -436.3696 (0.020)
Step: 70999, Reward: -168.6517 [59.70], Avg: -435.7837 (0.020)
Step: 71199, Reward: -148.6549 [44.45], Avg: -435.1020 (0.020)
Step: 71399, Reward: -94.6775 [85.81], Avg: -434.3888 (0.020)
Step: 71599, Reward: -146.8504 [90.51], Avg: -433.8384 (0.020)
Step: 71799, Reward: -167.3913 [53.37], Avg: -433.2449 (0.020)
Step: 71999, Reward: -77.0687 [95.67], Avg: -432.5213 (0.020)
Step: 72199, Reward: -166.1052 [52.21], Avg: -431.9279 (0.020)
Step: 72399, Reward: -142.7067 [39.13], Avg: -431.2371 (0.020)
Step: 72599, Reward: -145.3797 [47.44], Avg: -430.5803 (0.020)
Step: 72799, Reward: -123.5455 [76.04], Avg: -429.9457 (0.020)
Step: 72999, Reward: -233.1662 [98.19], Avg: -429.6756 (0.020)
Step: 73199, Reward: -179.6823 [105.63], Avg: -429.2811 (0.020)
Step: 73399, Reward: -170.2923 [92.58], Avg: -428.8277 (0.020)
Step: 73599, Reward: -141.6562 [87.24], Avg: -428.2844 (0.020)
Step: 73799, Reward: -143.3744 [87.30], Avg: -427.7489 (0.020)
Step: 73999, Reward: -190.5082 [57.73], Avg: -427.2637 (0.020)
Step: 74199, Reward: -191.7235 [60.05], Avg: -426.7907 (0.020)
Step: 74399, Reward: -98.9424 [89.62], Avg: -426.1503 (0.020)
Step: 74599, Reward: -118.3146 [75.44], Avg: -425.5272 (0.020)
Step: 74799, Reward: -147.2340 [98.05], Avg: -425.0453 (0.020)
Step: 74999, Reward: -375.0301 [242.47], Avg: -425.5585 (0.020)
Step: 75199, Reward: -171.2634 [113.40], Avg: -425.1838 (0.020)
Step: 75399, Reward: -145.0915 [131.48], Avg: -424.7896 (0.020)
Step: 75599, Reward: -192.9882 [89.49], Avg: -424.4131 (0.020)
Step: 75799, Reward: -152.2331 [51.20], Avg: -423.8301 (0.020)
Step: 75999, Reward: -123.4431 [2.70], Avg: -423.0467 (0.020)
Step: 76199, Reward: -142.6621 [108.07], Avg: -422.5944 (0.020)
Step: 76399, Reward: -190.7138 [87.02], Avg: -422.2152 (0.020)
Step: 76599, Reward: -143.0161 [104.15], Avg: -421.7581 (0.020)
Step: 76799, Reward: -185.7719 [72.68], Avg: -421.3329 (0.020)
Step: 76999, Reward: -168.8511 [84.52], Avg: -420.8966 (0.020)
Step: 77199, Reward: -105.0510 [50.66], Avg: -420.2096 (0.020)
Step: 77399, Reward: -147.5621 [48.72], Avg: -419.6309 (0.020)
Step: 77599, Reward: -185.9102 [130.16], Avg: -419.3640 (0.020)
Step: 77799, Reward: -145.9921 [109.29], Avg: -418.9422 (0.020)
Step: 77999, Reward: -199.2070 [93.28], Avg: -418.6180 (0.020)
Step: 78199, Reward: -166.5083 [118.17], Avg: -418.2754 (0.020)
Step: 78399, Reward: -209.9417 [79.90], Avg: -417.9478 (0.020)
Step: 78599, Reward: -168.4550 [53.72], Avg: -417.4497 (0.020)
Step: 78799, Reward: -143.2469 [46.87], Avg: -416.8727 (0.020)
Step: 78999, Reward: -99.3127 [48.05], Avg: -416.1903 (0.020)
Step: 79199, Reward: -161.5182 [104.09], Avg: -415.8101 (0.020)
Step: 79399, Reward: -145.0115 [47.14], Avg: -415.2467 (0.020)
Step: 79599, Reward: -120.4770 [69.54], Avg: -414.6808 (0.020)
Step: 79799, Reward: -118.6068 [125.64], Avg: -414.2537 (0.020)
Step: 79999, Reward: -170.7338 [94.91], Avg: -413.8822 (0.020)
Step: 80199, Reward: -146.5987 [46.11], Avg: -413.3306 (0.020)
Step: 80399, Reward: -262.7854 [82.24], Avg: -413.1607 (0.020)
Step: 80599, Reward: -139.1898 [39.80], Avg: -412.5796 (0.020)
Step: 80799, Reward: -103.1784 [50.14], Avg: -411.9379 (0.020)
Step: 80999, Reward: -258.8239 [44.29], Avg: -411.6692 (0.020)
Step: 81199, Reward: -172.1263 [96.83], Avg: -411.3177 (0.020)
Step: 81399, Reward: -124.9103 [76.30], Avg: -410.8014 (0.020)
Step: 81599, Reward: -169.5425 [48.87], Avg: -410.3299 (0.020)
Step: 81799, Reward: -212.9407 [46.89], Avg: -409.9619 (0.020)
Step: 81999, Reward: -215.1016 [81.49], Avg: -409.6854 (0.020)
Step: 82199, Reward: -101.1109 [48.99], Avg: -409.0538 (0.020)
Step: 82399, Reward: -146.2811 [48.36], Avg: -408.5334 (0.020)
Step: 82599, Reward: -215.7012 [115.89], Avg: -408.3471 (0.020)
Step: 82799, Reward: -183.6442 [114.24], Avg: -408.0803 (0.020)
Step: 82999, Reward: -148.4620 [49.92], Avg: -407.5750 (0.020)
Step: 83199, Reward: -212.9843 [110.83], Avg: -407.3736 (0.020)
Step: 83399, Reward: -188.1367 [52.82], Avg: -406.9746 (0.020)
Step: 83599, Reward: -119.2599 [68.59], Avg: -406.4503 (0.020)
Step: 83799, Reward: -96.4227 [85.92], Avg: -405.9155 (0.020)
Step: 83999, Reward: -149.8949 [46.86], Avg: -405.4175 (0.020)
Step: 84199, Reward: -237.2339 [71.31], Avg: -405.1874 (0.020)
Step: 84399, Reward: -161.8108 [151.28], Avg: -404.9691 (0.020)
Step: 84599, Reward: -176.6379 [98.87], Avg: -404.6631 (0.020)
Step: 84799, Reward: -149.2291 [48.99], Avg: -404.1762 (0.020)
Step: 84999, Reward: -162.1189 [80.29], Avg: -403.7955 (0.020)
Step: 85199, Reward: -146.7576 [39.72], Avg: -403.2854 (0.020)
Step: 85399, Reward: -168.4894 [140.86], Avg: -403.0654 (0.020)
Step: 85599, Reward: -170.2286 [55.52], Avg: -402.6511 (0.020)
Step: 85799, Reward: -120.8975 [125.87], Avg: -402.2878 (0.020)
Step: 85999, Reward: -196.2111 [95.29], Avg: -402.0301 (0.020)
Step: 86199, Reward: -144.8393 [89.99], Avg: -401.6422 (0.020)
Step: 86399, Reward: -217.7536 [116.22], Avg: -401.4855 (0.020)
Step: 86599, Reward: -168.7602 [54.54], Avg: -401.0740 (0.020)
Step: 86799, Reward: -127.5384 [77.06], Avg: -400.6213 (0.020)
Step: 86999, Reward: -192.0338 [54.10], Avg: -400.2662 (0.020)
Step: 87199, Reward: -126.9824 [75.99], Avg: -399.8137 (0.020)
Step: 87399, Reward: -96.7581 [85.76], Avg: -399.3165 (0.020)
Step: 87599, Reward: -124.5778 [110.01], Avg: -398.9404 (0.020)
Step: 87799, Reward: -138.5459 [42.59], Avg: -398.4442 (0.020)
Step: 87999, Reward: -236.5236 [98.15], Avg: -398.2993 (0.020)
Step: 88199, Reward: -122.8281 [71.30], Avg: -397.8363 (0.020)
Step: 88399, Reward: -169.6981 [117.36], Avg: -397.5857 (0.020)
Step: 88599, Reward: -76.2404 [59.61], Avg: -396.9949 (0.020)
Step: 88799, Reward: -162.3090 [49.63], Avg: -396.5781 (0.020)
Step: 88999, Reward: -169.7307 [54.92], Avg: -396.1917 (0.020)
Step: 89199, Reward: -73.4618 [58.53], Avg: -395.5994 (0.020)
Step: 89399, Reward: -147.3030 [51.62], Avg: -395.1594 (0.020)
Step: 89599, Reward: -160.2245 [151.16], Avg: -394.9724 (0.020)
Step: 89799, Reward: -120.4544 [106.79], Avg: -394.5988 (0.020)
Step: 89999, Reward: -119.1847 [103.12], Avg: -394.2160 (0.020)
Step: 90199, Reward: -211.5323 [88.37], Avg: -394.0068 (0.020)
Step: 90399, Reward: -235.5871 [98.71], Avg: -393.8747 (0.020)
Step: 90599, Reward: -199.9122 [101.62], Avg: -393.6709 (0.020)
Step: 90799, Reward: -194.2198 [94.71], Avg: -393.4402 (0.020)
Step: 90999, Reward: -137.2772 [126.07], Avg: -393.1543 (0.020)
Step: 91199, Reward: -178.8555 [129.90], Avg: -392.9692 (0.020)
Step: 91399, Reward: -189.5430 [52.44], Avg: -392.6388 (0.020)
Step: 91599, Reward: -116.8618 [71.76], Avg: -392.1934 (0.020)
Step: 91799, Reward: -170.5948 [57.13], Avg: -391.8350 (0.020)
Step: 91999, Reward: -146.2319 [42.56], Avg: -391.3936 (0.020)
Step: 92199, Reward: -140.2828 [42.25], Avg: -390.9406 (0.020)
Step: 92399, Reward: -121.1584 [129.53], Avg: -390.6370 (0.020)
Step: 92599, Reward: -136.8670 [108.22], Avg: -390.3227 (0.020)
Step: 92799, Reward: -181.5938 [74.44], Avg: -390.0332 (0.020)
Step: 92999, Reward: -122.9135 [3.01], Avg: -389.4653 (0.020)
Step: 93199, Reward: -142.8636 [42.44], Avg: -389.0272 (0.020)
Step: 93399, Reward: -144.2318 [42.30], Avg: -388.5935 (0.020)
Step: 93599, Reward: -188.9374 [86.18], Avg: -388.3511 (0.020)
Step: 93799, Reward: -140.6548 [38.87], Avg: -387.9058 (0.020)
Step: 93999, Reward: -96.8981 [48.32], Avg: -387.3895 (0.020)
Step: 94199, Reward: -97.1887 [89.84], Avg: -386.9641 (0.020)
Step: 94399, Reward: -95.7039 [83.90], Avg: -386.5248 (0.020)
Step: 94599, Reward: -138.1800 [83.31], Avg: -386.1759 (0.020)
Step: 94799, Reward: -93.3441 [82.86], Avg: -385.7329 (0.020)
Step: 94999, Reward: -170.2463 [59.61], Avg: -385.4047 (0.020)
Step: 95199, Reward: -73.6212 [57.93], Avg: -384.8714 (0.020)
Step: 95399, Reward: -74.6669 [58.28], Avg: -384.3433 (0.020)
Step: 95599, Reward: -97.7274 [84.93], Avg: -383.9213 (0.020)
Step: 95799, Reward: -137.3254 [99.23], Avg: -383.6137 (0.020)
Step: 95999, Reward: -148.6434 [50.40], Avg: -383.2292 (0.020)
Step: 96199, Reward: -143.1473 [45.68], Avg: -382.8250 (0.020)
Step: 96399, Reward: -179.8037 [75.46], Avg: -382.5603 (0.020)
Step: 96599, Reward: -215.6632 [130.97], Avg: -382.4859 (0.020)
Step: 96799, Reward: -117.0045 [69.69], Avg: -382.0814 (0.020)
Step: 96999, Reward: -183.9926 [154.35], Avg: -381.9912 (0.020)
Step: 97199, Reward: -198.8454 [124.22], Avg: -381.8700 (0.020)
Step: 97399, Reward: -196.1805 [96.94], Avg: -381.6877 (0.020)
Step: 97599, Reward: -142.8579 [39.65], Avg: -381.2796 (0.020)
Step: 97799, Reward: -142.9744 [85.84], Avg: -380.9678 (0.020)
Step: 97999, Reward: -188.0623 [90.97], Avg: -380.7597 (0.020)
Step: 98199, Reward: -144.1645 [111.34], Avg: -380.5046 (0.020)
Step: 98399, Reward: -76.4198 [60.08], Avg: -380.0087 (0.020)
Step: 98599, Reward: -139.6488 [82.51], Avg: -379.6885 (0.020)
Step: 98799, Reward: -190.7825 [53.95], Avg: -379.4153 (0.020)
Step: 98999, Reward: -100.5860 [50.35], Avg: -378.9538 (0.020)
Step: 99199, Reward: -143.2555 [43.31], Avg: -378.5659 (0.020)
Step: 99399, Reward: -119.8106 [75.81], Avg: -378.1978 (0.020)
Step: 99599, Reward: -97.8586 [48.86], Avg: -377.7330 (0.020)
Step: 99799, Reward: -48.0461 [56.71], Avg: -377.1859 (0.020)
Step: 99999, Reward: -206.6840 [86.65], Avg: -377.0182 (0.020)
