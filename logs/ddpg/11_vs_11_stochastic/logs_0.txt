Model: <class 'models.ddpg.DDPGAgent'>, Dir: 11_vs_11_stochastic
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.98             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddqn", choices=["ddqn", "ddpg", "ppo"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=1000, help="Number of steps to train the agent")
args = parser.parse_args()

envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = envs[0]

def make_env(env_name=env_name, log=False):
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		self.agent.network.save_model(dirname, name)

	def load(self, dirname="pytorch", name="best"):
		self.stack.load_model(dirname, name)
		self.agent.network.load_model(dirname, name)
		return self

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s % envs.env.unwrapped._config._scenario_cfg.game_duration == 0:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 0, Reward: -3.4000 [1.02], Avg: -4.4198 (1.000)
Step: 3000, Reward: -2.6000 [1.85], Avg: -4.4373 (0.980)
Step: 6000, Reward: -3.2000 [0.75], Avg: -4.2743 (0.960)
Step: 9000, Reward: -2.6000 [1.62], Avg: -4.2619 (0.941)
Step: 12000, Reward: -1.6000 [1.36], Avg: -4.0008 (0.922)
Step: 15000, Reward: -3.4000 [1.50], Avg: -4.1501 (0.904)
Step: 18000, Reward: -2.4000 [1.62], Avg: -4.1322 (0.886)
Step: 21000, Reward: -3.0000 [1.41], Avg: -4.1675 (0.868)
Step: 24000, Reward: -1.8000 [1.33], Avg: -4.0518 (0.851)
Step: 27000, Reward: -2.2000 [1.83], Avg: -4.0499 (0.834)
Step: 30000, Reward: -2.8000 [0.75], Avg: -4.0043 (0.817)
Step: 33000, Reward: -2.6000 [1.02], Avg: -3.9723 (0.801)
Step: 36000, Reward: -1.4000 [1.02], Avg: -3.8529 (0.785)
Step: 39000, Reward: -1.6000 [2.33], Avg: -3.8586 (0.769)
Step: 42000, Reward: -2.2000 [1.60], Avg: -3.8547 (0.754)
Step: 45000, Reward: -1.0000 [0.89], Avg: -3.7321 (0.739)
Step: 48000, Reward: -2.0000 [1.41], Avg: -3.7134 (0.724)
Step: 51000, Reward: -1.4000 [0.49], Avg: -3.6121 (0.709)
Step: 54000, Reward: -2.6000 [2.06], Avg: -3.6672 (0.695)
Step: 57000, Reward: -1.6000 [1.20], Avg: -3.6239 (0.681)
Step: 60000, Reward: -1.4000 [1.02], Avg: -3.5665 (0.668)
Step: 63000, Reward: -1.6000 [1.02], Avg: -3.5235 (0.654)
Step: 66000, Reward: -3.4000 [3.14], Avg: -3.6545 (0.641)
Step: 69000, Reward: -2.4000 [1.62], Avg: -3.6699 (0.628)
Step: 72000, Reward: -1.6000 [1.20], Avg: -3.6352 (0.616)
Step: 75000, Reward: -2.2000 [1.17], Avg: -3.6248 (0.603)
Step: 78000, Reward: -2.6000 [1.20], Avg: -3.6313 (0.591)
Step: 81000, Reward: -1.8000 [2.23], Avg: -3.6454 (0.580)
Step: 84000, Reward: -4.6000 [1.36], Avg: -3.7251 (0.568)
Step: 87000, Reward: -2.8000 [1.83], Avg: -3.7554 (0.557)
Step: 90000, Reward: -1.8000 [0.98], Avg: -3.7239 (0.545)
Step: 93000, Reward: -2.0000 [1.41], Avg: -3.7142 (0.535)
Step: 96000, Reward: -1.4000 [0.49], Avg: -3.6590 (0.524)
Step: 99000, Reward: -2.4000 [1.50], Avg: -3.6659 (0.513)
Step: 102000, Reward: -1.2000 [1.17], Avg: -3.6288 (0.503)
Step: 105000, Reward: -2.0000 [0.63], Avg: -3.6011 (0.493)
Step: 108000, Reward: -1.6000 [1.62], Avg: -3.5910 (0.483)
Step: 111000, Reward: -3.0000 [1.67], Avg: -3.6194 (0.474)
Step: 114000, Reward: -2.8000 [1.72], Avg: -3.6425 (0.464)
Step: 117000, Reward: -2.8000 [0.40], Avg: -3.6315 (0.455)
Step: 120000, Reward: -4.2000 [1.47], Avg: -3.6812 (0.446)
Step: 123000, Reward: -2.2000 [1.33], Avg: -3.6775 (0.437)
Step: 126000, Reward: -2.2000 [1.60], Avg: -3.6804 (0.428)
Step: 129000, Reward: -2.4000 [1.50], Avg: -3.6853 (0.419)
Step: 132000, Reward: -0.8000 [0.98], Avg: -3.6429 (0.411)
Step: 135000, Reward: -2.6000 [1.62], Avg: -3.6556 (0.403)
Step: 138000, Reward: -1.4000 [1.85], Avg: -3.6471 (0.395)
Step: 141000, Reward: -1.8000 [1.72], Avg: -3.6444 (0.387)
Step: 144000, Reward: -1.4000 [1.74], Avg: -3.6342 (0.379)
Step: 147000, Reward: -1.8000 [1.83], Avg: -3.6342 (0.372)
Step: 150000, Reward: -2.0000 [1.90], Avg: -3.6393 (0.364)
Step: 153000, Reward: -2.6000 [1.74], Avg: -3.6529 (0.357)
Step: 156000, Reward: -0.6000 [1.20], Avg: -3.6179 (0.350)
Step: 159000, Reward: -1.8000 [1.33], Avg: -3.6088 (0.343)
Step: 162000, Reward: -2.6000 [1.62], Avg: -3.6200 (0.336)
Step: 165000, Reward: -1.2000 [1.47], Avg: -3.6031 (0.329)
Step: 168000, Reward: -1.4000 [1.02], Avg: -3.5823 (0.323)
Step: 171000, Reward: -1.2000 [1.17], Avg: -3.5613 (0.316)
Step: 174000, Reward: -3.8000 [2.40], Avg: -3.6060 (0.310)
Step: 177000, Reward: -1.8000 [1.94], Avg: -3.6083 (0.304)
Step: 180000, Reward: -1.4000 [1.20], Avg: -3.5917 (0.298)
Step: 183000, Reward: -1.2000 [0.75], Avg: -3.5652 (0.292)
Step: 186000, Reward: -1.0000 [0.89], Avg: -3.5387 (0.286)
Step: 189000, Reward: -1.2000 [1.94], Avg: -3.5325 (0.280)
Step: 192000, Reward: -1.6000 [1.02], Avg: -3.5184 (0.274)
Step: 195000, Reward: -0.4000 [0.80], Avg: -3.4833 (0.269)
Step: 198000, Reward: -2.6000 [1.50], Avg: -3.4925 (0.264)
Step: 201000, Reward: -0.6000 [0.80], Avg: -3.4617 (0.258)
Step: 204000, Reward: -1.4000 [1.50], Avg: -3.4535 (0.253)
Step: 207000, Reward: -1.8000 [1.47], Avg: -3.4509 (0.248)
Step: 210000, Reward: -1.8000 [1.47], Avg: -3.4483 (0.243)
Step: 213000, Reward: -1.8000 [1.72], Avg: -3.4493 (0.238)
Step: 216000, Reward: -2.2000 [1.94], Avg: -3.4588 (0.233)
Step: 219000, Reward: -1.4000 [1.50], Avg: -3.4512 (0.229)
Step: 222000, Reward: -1.8000 [1.33], Avg: -3.4468 (0.224)
Step: 225000, Reward: -2.6000 [2.06], Avg: -3.4628 (0.220)
Step: 228000, Reward: -1.2000 [1.17], Avg: -3.4486 (0.215)
Step: 231000, Reward: -2.2000 [1.94], Avg: -3.4574 (0.211)
Step: 234000, Reward: -0.8000 [1.17], Avg: -3.4385 (0.207)
Step: 237000, Reward: -3.0000 [3.52], Avg: -3.4771 (0.203)
Step: 240000, Reward: -2.4000 [1.62], Avg: -3.4838 (0.199)
Step: 243000, Reward: -0.6000 [0.49], Avg: -3.4546 (0.195)
Step: 246000, Reward: -1.0000 [1.55], Avg: -3.4437 (0.191)
Step: 249000, Reward: -1.0000 [1.55], Avg: -3.4331 (0.187)
Step: 252000, Reward: -2.0000 [1.79], Avg: -3.4373 (0.183)
Step: 255000, Reward: -2.8000 [2.32], Avg: -3.4568 (0.180)
Step: 258000, Reward: -1.8000 [1.83], Avg: -3.4588 (0.176)
Step: 261000, Reward: -1.8000 [0.98], Avg: -3.4511 (0.172)
Step: 264000, Reward: -1.6000 [2.24], Avg: -3.4555 (0.169)
Step: 267000, Reward: -0.8000 [0.75], Avg: -3.4343 (0.166)
Step: 270000, Reward: -2.0000 [1.79], Avg: -3.4382 (0.162)
Step: 273000, Reward: -1.2000 [1.47], Avg: -3.4299 (0.159)
Step: 276000, Reward: -2.8000 [2.71], Avg: -3.4523 (0.156)
Step: 279000, Reward: -3.0000 [1.10], Avg: -3.4591 (0.153)
Step: 282000, Reward: -1.6000 [1.36], Avg: -3.4538 (0.150)
Step: 285000, Reward: -2.0000 [1.41], Avg: -3.4534 (0.147)
Step: 288000, Reward: -2.2000 [0.75], Avg: -3.4482 (0.144)
Step: 291000, Reward: -2.6000 [0.80], Avg: -3.4477 (0.141)
Step: 294000, Reward: -0.8000 [0.75], Avg: -3.4285 (0.138)
Step: 297000, Reward: -2.6000 [2.33], Avg: -3.4435 (0.135)
Step: 300000, Reward: -1.4000 [2.33], Avg: -3.4464 (0.133)
Step: 303000, Reward: -2.8000 [1.33], Avg: -3.4531 (0.130)
Step: 306000, Reward: -1.4000 [1.36], Avg: -3.4463 (0.127)
Step: 309000, Reward: -2.2000 [2.23], Avg: -3.4557 (0.125)
Step: 312000, Reward: -0.8000 [1.60], Avg: -3.4457 (0.122)
Step: 315000, Reward: -2.0000 [1.67], Avg: -3.4478 (0.120)
Step: 318000, Reward: -1.4000 [0.80], Avg: -3.4362 (0.117)
Step: 321000, Reward: -1.8000 [0.75], Avg: -3.4280 (0.115)
Step: 324000, Reward: -4.4000 [2.06], Avg: -3.4558 (0.113)
Step: 327000, Reward: -2.6000 [0.80], Avg: -3.4553 (0.111)
Step: 330000, Reward: -3.0000 [3.69], Avg: -3.4844 (0.108)
Step: 333000, Reward: -1.6000 [1.36], Avg: -3.4797 (0.106)
Step: 336000, Reward: -1.6000 [0.80], Avg: -3.4701 (0.104)
Step: 339000, Reward: -2.8000 [2.32], Avg: -3.4845 (0.102)
Step: 342000, Reward: -1.6000 [1.96], Avg: -3.4852 (0.100)
Step: 345000, Reward: -1.0000 [0.63], Avg: -3.4692 (0.098)
Step: 348000, Reward: -2.4000 [1.74], Avg: -3.4750 (0.096)
Step: 351000, Reward: -1.2000 [1.17], Avg: -3.4656 (0.094)
Step: 354000, Reward: -2.6000 [1.74], Avg: -3.4730 (0.092)
Step: 357000, Reward: -1.4000 [1.50], Avg: -3.4682 (0.090)
Step: 360000, Reward: -2.2000 [1.72], Avg: -3.4719 (0.089)
Step: 363000, Reward: -0.8000 [0.98], Avg: -3.4580 (0.087)
Step: 366000, Reward: -1.8000 [2.23], Avg: -3.4627 (0.085)
Step: 369000, Reward: -1.0000 [1.55], Avg: -3.4553 (0.083)
Step: 372000, Reward: -3.2000 [1.94], Avg: -3.4688 (0.082)
Step: 375000, Reward: -4.2000 [1.17], Avg: -3.4838 (0.080)
Step: 378000, Reward: -1.4000 [1.96], Avg: -3.4828 (0.078)
Step: 381000, Reward: -0.6000 [0.49], Avg: -3.4641 (0.077)
Step: 384000, Reward: -1.4000 [1.02], Avg: -3.4561 (0.075)
Step: 387000, Reward: -1.8000 [1.17], Avg: -3.4523 (0.074)
Step: 390000, Reward: -1.8000 [2.23], Avg: -3.4567 (0.072)
Step: 393000, Reward: -2.8000 [0.98], Avg: -3.4591 (0.071)
Step: 396000, Reward: -0.6000 [0.49], Avg: -3.4413 (0.069)
Step: 399000, Reward: -1.8000 [1.17], Avg: -3.4378 (0.068)
Step: 402000, Reward: -1.4000 [1.20], Avg: -3.4316 (0.067)
Step: 405000, Reward: -1.8000 [1.33], Avg: -3.4293 (0.065)
Step: 408000, Reward: -0.8000 [0.75], Avg: -3.4156 (0.064)
Step: 411000, Reward: -1.0000 [1.26], Avg: -3.4072 (0.063)
Step: 414000, Reward: -0.2000 [0.40], Avg: -3.3871 (0.062)
Step: 417000, Reward: -1.6000 [2.24], Avg: -3.3903 (0.060)
Step: 420000, Reward: -3.4000 [1.85], Avg: -3.4035 (0.059)
Step: 423000, Reward: -1.6000 [1.36], Avg: -3.4004 (0.058)
Step: 426000, Reward: -2.6000 [2.42], Avg: -3.4117 (0.057)
Step: 429000, Reward: -2.4000 [1.36], Avg: -3.4141 (0.056)
Step: 432000, Reward: -2.4000 [3.20], Avg: -3.4292 (0.055)
Step: 435000, Reward: -0.8000 [1.60], Avg: -3.4221 (0.053)
Step: 438000, Reward: -2.6000 [2.42], Avg: -3.4330 (0.052)
Step: 441000, Reward: -1.6000 [1.62], Avg: -3.4316 (0.051)
Step: 444000, Reward: -1.4000 [0.80], Avg: -3.4233 (0.050)
Step: 447000, Reward: -2.2000 [1.33], Avg: -3.4240 (0.049)
Step: 450000, Reward: -1.6000 [1.50], Avg: -3.4218 (0.048)
Step: 453000, Reward: -2.0000 [2.61], Avg: -3.4296 (0.047)
Step: 456000, Reward: -2.2000 [1.72], Avg: -3.4328 (0.046)
Step: 459000, Reward: -0.8000 [0.75], Avg: -3.4206 (0.045)
Step: 462000, Reward: -2.2000 [1.47], Avg: -3.4222 (0.045)
Step: 465000, Reward: -1.2000 [1.60], Avg: -3.4182 (0.044)
Step: 468000, Reward: -2.4000 [1.62], Avg: -3.4221 (0.043)
Step: 471000, Reward: -2.2000 [1.72], Avg: -3.4252 (0.042)
Step: 474000, Reward: -0.6000 [0.80], Avg: -3.4125 (0.041)
Step: 477000, Reward: -3.2000 [1.33], Avg: -3.4195 (0.040)
Step: 480000, Reward: -0.8000 [1.17], Avg: -3.4104 (0.039)
Step: 483000, Reward: -0.8000 [0.40], Avg: -3.3968 (0.039)
Step: 486000, Reward: -2.0000 [1.90], Avg: -3.3999 (0.038)
Step: 489000, Reward: -2.6000 [2.15], Avg: -3.4081 (0.037)
Step: 492000, Reward: -2.8000 [2.79], Avg: -3.4213 (0.036)
Step: 495000, Reward: -1.2000 [1.47], Avg: -3.4168 (0.036)
Step: 498000, Reward: -1.4000 [1.36], Avg: -3.4128 (0.035)
Step: 501000, Reward: -2.8000 [1.47], Avg: -3.4179 (0.034)
Step: 504000, Reward: -5.0000 [2.83], Avg: -3.4440 (0.034)
Step: 507000, Reward: -2.0000 [2.61], Avg: -3.4509 (0.033)
Step: 510000, Reward: -2.8000 [1.72], Avg: -3.4571 (0.032)
Step: 513000, Reward: -1.2000 [0.98], Avg: -3.4497 (0.032)
Step: 516000, Reward: -1.4000 [1.36], Avg: -3.4457 (0.031)
Step: 519000, Reward: -3.0000 [1.10], Avg: -3.4494 (0.030)
Step: 522000, Reward: -2.2000 [2.04], Avg: -3.4539 (0.030)
Step: 525000, Reward: -4.4000 [3.01], Avg: -3.4764 (0.029)
Step: 528000, Reward: -1.0000 [0.63], Avg: -3.4660 (0.029)
Step: 531000, Reward: -3.4000 [1.02], Avg: -3.4713 (0.028)
Step: 534000, Reward: -0.6000 [1.20], Avg: -3.4620 (0.027)
Step: 537000, Reward: -2.0000 [1.79], Avg: -3.4638 (0.027)
Step: 540000, Reward: -2.0000 [1.26], Avg: -3.4627 (0.026)
Step: 543000, Reward: -0.8000 [1.17], Avg: -3.4545 (0.026)
Step: 546000, Reward: -0.8000 [0.75], Avg: -3.4441 (0.025)
Step: 549000, Reward: -1.2000 [1.17], Avg: -3.4382 (0.025)
Step: 552000, Reward: -0.6000 [0.80], Avg: -3.4272 (0.024)
Step: 555000, Reward: -1.4000 [1.50], Avg: -3.4244 (0.024)
Step: 558000, Reward: -2.4000 [1.74], Avg: -3.4282 (0.023)
Step: 561000, Reward: -1.8000 [2.64], Avg: -3.4336 (0.023)
Step: 564000, Reward: -1.8000 [1.72], Avg: -3.4340 (0.022)
Step: 567000, Reward: -1.2000 [1.17], Avg: -3.4284 (0.022)
Step: 570000, Reward: -2.0000 [1.79], Avg: -3.4303 (0.022)
Step: 573000, Reward: -2.0000 [0.89], Avg: -3.4275 (0.021)
Step: 576000, Reward: -1.2000 [1.47], Avg: -3.4236 (0.021)
Step: 579000, Reward: -1.6000 [2.06], Avg: -3.4248 (0.020)
Step: 582000, Reward: -2.4000 [2.73], Avg: -3.4335 (0.020)
Step: 585000, Reward: -1.4000 [1.36], Avg: -3.4301 (0.020)
Step: 588000, Reward: -1.8000 [1.83], Avg: -3.4311 (0.020)
Step: 591000, Reward: -2.4000 [2.06], Avg: -3.4363 (0.020)
Step: 594000, Reward: -2.4000 [1.74], Avg: -3.4398 (0.020)
Step: 597000, Reward: -2.0000 [1.26], Avg: -3.4390 (0.020)
Step: 600000, Reward: -0.8000 [0.75], Avg: -3.4296 (0.020)
Step: 603000, Reward: -1.6000 [1.62], Avg: -3.4286 (0.020)
Step: 606000, Reward: -1.2000 [1.47], Avg: -3.4248 (0.020)
Step: 609000, Reward: -3.4000 [2.42], Avg: -3.4365 (0.020)
Step: 612000, Reward: -2.2000 [1.94], Avg: -3.4400 (0.020)
Step: 615000, Reward: -1.2000 [1.17], Avg: -3.4348 (0.020)
Step: 618000, Reward: -1.8000 [1.94], Avg: -3.4362 (0.020)
Step: 621000, Reward: -1.0000 [1.55], Avg: -3.4320 (0.020)
Step: 624000, Reward: -2.6000 [2.58], Avg: -3.4403 (0.020)
Step: 627000, Reward: -1.4000 [1.85], Avg: -3.4394 (0.020)
Step: 630000, Reward: -1.6000 [0.80], Avg: -3.4345 (0.020)
Step: 633000, Reward: -2.4000 [3.38], Avg: -3.4456 (0.020)
Step: 636000, Reward: -1.6000 [1.62], Avg: -3.4445 (0.020)
Step: 639000, Reward: -1.8000 [2.14], Avg: -3.4468 (0.020)
Step: 642000, Reward: -1.6000 [1.36], Avg: -3.4445 (0.020)
Step: 645000, Reward: -3.0000 [2.19], Avg: -3.4526 (0.020)
Step: 648000, Reward: -1.4000 [2.80], Avg: -3.4561 (0.020)
Step: 651000, Reward: -4.0000 [2.37], Avg: -3.4694 (0.020)
Step: 654000, Reward: -3.6000 [2.24], Avg: -3.4803 (0.020)
Step: 657000, Reward: -0.8000 [0.75], Avg: -3.4715 (0.020)
Step: 660000, Reward: -1.6000 [1.74], Avg: -3.4709 (0.020)
Step: 663000, Reward: -1.2000 [2.40], Avg: -3.4715 (0.020)
Step: 666000, Reward: -1.8000 [1.47], Avg: -3.4706 (0.020)
Step: 669000, Reward: -0.6000 [0.49], Avg: -3.4600 (0.020)
Step: 672000, Reward: -2.8000 [1.17], Avg: -3.4622 (0.020)
Step: 675000, Reward: -0.8000 [0.75], Avg: -3.4537 (0.020)
Step: 678000, Reward: -2.6000 [1.62], Avg: -3.4571 (0.020)
Step: 681000, Reward: -0.8000 [1.17], Avg: -3.4506 (0.020)
Step: 684000, Reward: 0.0000 [0.00], Avg: -3.4355 (0.020)
Step: 687000, Reward: -1.2000 [1.47], Avg: -3.4322 (0.020)
Step: 690000, Reward: -2.2000 [1.33], Avg: -3.4326 (0.020)
Step: 693000, Reward: -3.2000 [2.14], Avg: -3.4408 (0.020)
Step: 696000, Reward: -1.6000 [1.20], Avg: -3.4381 (0.020)
Step: 699000, Reward: -0.8000 [1.17], Avg: -3.4318 (0.020)
Step: 702000, Reward: -2.6000 [1.62], Avg: -3.4351 (0.020)
Step: 705000, Reward: -2.2000 [1.47], Avg: -3.4361 (0.020)
Step: 708000, Reward: -2.4000 [2.58], Avg: -3.4426 (0.020)
Step: 711000, Reward: -0.4000 [0.49], Avg: -3.4319 (0.020)
Step: 714000, Reward: -1.8000 [1.17], Avg: -3.4300 (0.020)
Step: 717000, Reward: -2.8000 [2.32], Avg: -3.4370 (0.020)
Step: 720000, Reward: -1.4000 [2.33], Avg: -3.4382 (0.020)
Step: 723000, Reward: -1.6000 [1.36], Avg: -3.4362 (0.020)
Step: 726000, Reward: -2.6000 [1.85], Avg: -3.4404 (0.020)
Step: 729000, Reward: -2.8000 [1.17], Avg: -3.4426 (0.020)
Step: 732000, Reward: -2.0000 [1.90], Avg: -3.4444 (0.020)
Step: 735000, Reward: -2.0000 [1.67], Avg: -3.4454 (0.020)
Step: 738000, Reward: -2.4000 [1.85], Avg: -3.4486 (0.020)
Step: 741000, Reward: -1.0000 [1.26], Avg: -3.4439 (0.020)
Step: 744000, Reward: -0.8000 [1.17], Avg: -3.4379 (0.020)
Step: 747000, Reward: -2.4000 [1.36], Avg: -3.4392 (0.020)
Step: 750000, Reward: -2.2000 [2.40], Avg: -3.4438 (0.020)
Step: 753000, Reward: -1.0000 [1.10], Avg: -3.4385 (0.020)
Step: 756000, Reward: -2.2000 [1.33], Avg: -3.4388 (0.020)
Step: 759000, Reward: -1.4000 [1.50], Avg: -3.4367 (0.020)
Step: 762000, Reward: -2.4000 [2.94], Avg: -3.4442 (0.020)
Step: 765000, Reward: -0.8000 [0.75], Avg: -3.4367 (0.020)
Step: 768000, Reward: -1.0000 [0.63], Avg: -3.4297 (0.020)
Step: 771000, Reward: -1.0000 [1.55], Avg: -3.4263 (0.020)
Step: 774000, Reward: -2.2000 [1.17], Avg: -3.4261 (0.020)
Step: 777000, Reward: -1.2000 [1.17], Avg: -3.4220 (0.020)
Step: 780000, Reward: -3.8000 [2.48], Avg: -3.4330 (0.020)
Step: 783000, Reward: -1.2000 [1.47], Avg: -3.4300 (0.020)
Step: 786000, Reward: -1.2000 [1.17], Avg: -3.4260 (0.020)
Step: 789000, Reward: -0.8000 [0.40], Avg: -3.4176 (0.020)
Step: 792000, Reward: -1.8000 [2.64], Avg: -3.4214 (0.020)
Step: 795000, Reward: -0.4000 [0.49], Avg: -3.4119 (0.020)
Step: 798000, Reward: -0.6000 [0.80], Avg: -3.4044 (0.020)
Step: 801000, Reward: -0.6000 [0.80], Avg: -3.3969 (0.020)
Step: 804000, Reward: -2.4000 [2.50], Avg: -3.4025 (0.020)
Step: 807000, Reward: -1.2000 [1.94], Avg: -3.4015 (0.020)
Step: 810000, Reward: -1.8000 [0.75], Avg: -3.3983 (0.020)
Step: 813000, Reward: -1.6000 [1.02], Avg: -3.3955 (0.020)
Step: 816000, Reward: -3.6000 [2.87], Avg: -3.4067 (0.020)
Step: 819000, Reward: -1.4000 [1.50], Avg: -3.4049 (0.020)
Step: 822000, Reward: -2.6000 [1.50], Avg: -3.4074 (0.020)
Step: 825000, Reward: -0.6000 [0.49], Avg: -3.3990 (0.020)
Step: 828000, Reward: -0.8000 [0.98], Avg: -3.3932 (0.020)
Step: 831000, Reward: -1.6000 [1.50], Avg: -3.3921 (0.020)
Step: 834000, Reward: -1.0000 [2.00], Avg: -3.3907 (0.020)
Step: 837000, Reward: -2.2000 [1.94], Avg: -3.3934 (0.020)
Step: 840000, Reward: -1.4000 [1.50], Avg: -3.3916 (0.020)
Step: 843000, Reward: -1.4000 [1.50], Avg: -3.3898 (0.020)
Step: 846000, Reward: -2.2000 [1.33], Avg: -3.3903 (0.020)
Step: 849000, Reward: -2.6000 [1.96], Avg: -3.3944 (0.020)
Step: 852000, Reward: -0.8000 [0.98], Avg: -3.3888 (0.020)
Step: 855000, Reward: -2.2000 [1.33], Avg: -3.3893 (0.020)
Step: 858000, Reward: -0.2000 [0.40], Avg: -3.3795 (0.020)
Step: 861000, Reward: -1.6000 [1.36], Avg: -3.3781 (0.020)
Step: 864000, Reward: -1.8000 [1.83], Avg: -3.3790 (0.020)
Step: 867000, Reward: -1.2000 [1.17], Avg: -3.3755 (0.020)
Step: 870000, Reward: -2.8000 [2.14], Avg: -3.3808 (0.020)
Step: 873000, Reward: -1.6000 [2.24], Avg: -3.3824 (0.020)
Step: 876000, Reward: -3.0000 [1.67], Avg: -3.3868 (0.020)
Step: 879000, Reward: -0.2000 [0.40], Avg: -3.3773 (0.020)
Step: 882000, Reward: -2.0000 [1.90], Avg: -3.3791 (0.020)
Step: 885000, Reward: -3.2000 [1.94], Avg: -3.3850 (0.020)
Step: 888000, Reward: -3.6000 [2.87], Avg: -3.3954 (0.020)
Step: 891000, Reward: -1.4000 [1.96], Avg: -3.3953 (0.020)
Step: 894000, Reward: -2.8000 [1.94], Avg: -3.3998 (0.020)
Step: 897000, Reward: -2.0000 [1.26], Avg: -3.3994 (0.020)
Step: 900000, Reward: -0.8000 [1.17], Avg: -3.3946 (0.020)
Step: 903000, Reward: -3.2000 [2.14], Avg: -3.4010 (0.020)
Step: 906000, Reward: -2.0000 [2.19], Avg: -3.4036 (0.020)
Step: 909000, Reward: -1.8000 [1.72], Avg: -3.4040 (0.020)
Step: 912000, Reward: -0.4000 [0.49], Avg: -3.3958 (0.020)
Step: 915000, Reward: -1.4000 [1.50], Avg: -3.3941 (0.020)
Step: 918000, Reward: -2.4000 [1.85], Avg: -3.3969 (0.020)
Step: 921000, Reward: -2.8000 [2.48], Avg: -3.4031 (0.020)
Step: 924000, Reward: -1.0000 [1.10], Avg: -3.3988 (0.020)
Step: 927000, Reward: -1.4000 [1.50], Avg: -3.3972 (0.020)
Step: 930000, Reward: -0.6000 [0.80], Avg: -3.3908 (0.020)
Step: 933000, Reward: -1.8000 [1.47], Avg: -3.3904 (0.020)
Step: 936000, Reward: -3.2000 [2.04], Avg: -3.3963 (0.020)
Step: 939000, Reward: -2.0000 [1.41], Avg: -3.3964 (0.020)
Step: 942000, Reward: -1.8000 [1.94], Avg: -3.3975 (0.020)
Step: 945000, Reward: -0.8000 [1.17], Avg: -3.3929 (0.020)
Step: 948000, Reward: -2.4000 [1.50], Avg: -3.3945 (0.020)
Step: 951000, Reward: -2.0000 [1.67], Avg: -3.3954 (0.020)
Step: 954000, Reward: -2.4000 [2.06], Avg: -3.3987 (0.020)
Step: 957000, Reward: -3.6000 [1.36], Avg: -3.4036 (0.020)
Step: 960000, Reward: -2.2000 [1.72], Avg: -3.4052 (0.020)
Step: 963000, Reward: -1.4000 [1.20], Avg: -3.4027 (0.020)
Step: 966000, Reward: -1.2000 [1.47], Avg: -3.4004 (0.020)
Step: 969000, Reward: -1.6000 [2.33], Avg: -3.4021 (0.020)
Step: 972000, Reward: -2.4000 [1.62], Avg: -3.4040 (0.020)
Step: 975000, Reward: -1.6000 [0.49], Avg: -3.4000 (0.020)
Step: 978000, Reward: -0.8000 [0.75], Avg: -3.3943 (0.020)
Step: 981000, Reward: -2.4000 [2.06], Avg: -3.3975 (0.020)
Step: 984000, Reward: -0.8000 [1.17], Avg: -3.3932 (0.020)
Step: 987000, Reward: -0.8000 [1.60], Avg: -3.3902 (0.020)
Step: 990000, Reward: -0.6000 [0.49], Avg: -3.3832 (0.020)
Step: 993000, Reward: -2.8000 [2.71], Avg: -3.3896 (0.020)
Step: 996000, Reward: -1.4000 [2.33], Avg: -3.3907 (0.020)
Step: 999000, Reward: -1.4000 [1.20], Avg: -3.3883 (0.020)
