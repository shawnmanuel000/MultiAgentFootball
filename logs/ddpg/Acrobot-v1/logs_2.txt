Model: <class 'models.ddpg.DDPGAgent'>, Dir: Acrobot-v1
num_envs: 16,

import os
import math
import torch
import random
import numpy as np
from models.rand import RandomAgent, PrioritizedReplayBuffer, ReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS

EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.98             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

class DDPGActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, sample=True):
		state = self.layer1(state).relu() 
		state = self.layer2(state).relu() 
		state = self.layer3(state).relu() 
		action_mu = self.action_mu(state)
		action_sig = self.action_sig(state).exp()
		epsilon = torch.randn_like(action_sig)
		action = action_mu + epsilon.mul(action_sig) if sample else action_mu
		return action.tanh()
	
class DDPGCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.net_state = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.net_action = torch.nn.Linear(*action_size, INPUT_LAYER)
		self.net_layer1 = torch.nn.Linear(2*INPUT_LAYER, CRITIC_HIDDEN)
		self.net_layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, action):
		state = self.net_state(state).relu()
		net_action = self.net_action(action).relu()
		net_layer = torch.cat([state, net_action], dim=-1)
		net_layer = self.net_layer1(net_layer).relu()
		net_layer = self.net_layer2(net_layer).relu()
		q_value = self.q_value(net_layer)
		return q_value

class DDPGNetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None): 
		super().__init__(state_size, action_size, DDPGActor, DDPGCritic, lr=lr, gpu=gpu, load=load)

	def get_action(self, state, use_target=False, numpy=True, sample=True):
		with torch.no_grad():
			actor = self.actor_local if not use_target else self.actor_target
			return actor(state, sample).cpu().numpy() if numpy else actor(state, sample)

	def get_q_value(self, state, action, use_target=False, numpy=True):
		with torch.no_grad():
			critic = self.critic_local if not use_target else self.critic_target
			return critic(state, action).cpu().numpy() if numpy else critic(state, action)
	
	def optimize(self, states, actions, q_targets, importances=1):
		q_values = self.critic_local(states, actions)
		critic_error = q_values - q_targets.detach()
		critic_loss = importances.to(self.device) * critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		q_actions = self.critic_local(states, self.actor_local(states))
		actor_loss = -(q_actions - q_values.detach())
		self.step(self.actor_optimizer, actor_loss.mean())
		
		self.soft_copy(self.actor_local, self.actor_target)
		self.soft_copy(self.critic_local, self.critic_target)
		return critic_error.cpu().detach().numpy().squeeze(-1)
	
	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ddpg", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ddpg", dirname, name)

class DDPGAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, DDPGNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, e_greedy=False):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state, eps)
		if e_greedy and random.random() < eps: return action_random
		action_greedy = self.network.get_action(self.to_tensor(state), sample=sample)
		action = action_greedy if e_greedy else np.clip((1-eps)*action_greedy + eps*action_random, -1, 1)
		return action
		
	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if done[0]:#len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.5):
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()	
			next_state = self.to_tensor(next_state)
			next_action = self.network.get_action(next_state, use_target=True, numpy=False)
			values = self.network.get_q_value(states, actions, use_target=True, numpy=False)
			next_value = self.network.get_q_value(next_state, next_action, use_target=True, numpy=False)
			targets, _ = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, targets = [x.view(x.size(0)*x.size(1), *x.size()[2:]).cpu().numpy() for x in (states, actions, targets)]
			self.replay_buffer.extend(list(zip(states, actions, targets)), shuffle=True)	
		if len(self.replay_buffer) > 0:
			(states, actions, targets), indices, importances = self.replay_buffer.sample(REPLAY_BATCH_SIZE, dtype=self.to_tensor)
			errors = self.network.optimize(states, actions, targets, importances**(1-self.eps))
			self.replay_buffer.update_priorities(indices, errors)
			if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0002           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000 				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

import gym
import argparse
import numpy as np
# import gfootball.env as ggym
from collections import deque
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, ImgStack, RawStack
from utils.misc import Logger, rollout

parser = argparse.ArgumentParser(description="A3C Trainer")
parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
parser.add_argument("--model", type=str, default="ddqn", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
args = parser.parse_args()

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "LunarLander-v2"]
gfb_envs = ["11_vs_11_stochastic", "academy_empty_goal_close"]
env_name = gym_envs[2]

def make_env(env_name=env_name, log=False):
	if env_name in gym_envs: return gym.make(env_name)
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=False)
	env.spec = gym.envs.registration.EnvSpec(env_name + "-v0", max_episode_steps=env.unwrapped._config._scenario_cfg.game_duration)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space.n}")
	return env

class PixelAgent(RandomAgent):
	def __init__(self, state_size, action_size, num_envs, agent, load="", gpu=True, train=True):
		super().__init__(state_size, action_size)
		statemodel = RawStack if len(state_size) == 1 else ImgStack
		self.stack = statemodel(state_size, num_envs, load=load, gpu=gpu)
		self.agent = agent(self.stack.state_size, action_size, load="" if train else load, gpu=gpu)

	def get_env_action(self, env, state, eps=None, sample=True):
		state = self.stack.get_state(state)
		env_action, action = self.agent.get_env_action(env, state, eps, sample)
		return env_action, action, state

	def train(self, state, action, next_state, reward, done):
		next_state = self.stack.get_state(next_state)
		self.agent.train(state, action, next_state, reward, done)

	def reset(self, num_envs=None):
		num_envs = self.stack.num_envs if num_envs is None else num_envs
		self.stack.reset(num_envs, restore=False)
		return self

	def save_model(self, dirname="pytorch", name="best"):
		if hasattr(self.agent, "network"): self.agent.network.save_model(dirname, name)

def run(model, steps=10000, ports=16, eval_at=1000):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	logger = Logger(model, env_name, num_envs=num_envs)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports)
	agent = PixelAgent(envs.state_size, envs.action_size, num_envs, model)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		agent.reset(num_envs)
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if dones[0]:
			rollouts = [rollout(envs.env, agent.reset(1)) for _ in range(5)]
			test_reward = np.mean(rollouts) - np.std(rollouts)
			total_rewards.append(test_reward)
			agent.save_model(env_name, "checkpoint")
			if env_name in gfb_envs and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts):.4f} [{np.std(rollouts):.2f}], Avg: {np.mean(total_rewards):.4f} ({agent.agent.eps:.3f})")

if __name__ == "__main__":
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		if len(args.workerports) == 1: args.workerports = args.workerports[0]
		run(model, args.steps, args.workerports)
	print(f"Training finished")

Step: 499, Reward: -500.0000 [0.00], Avg: -500.0000 (0.980)
Step: 714, Reward: -500.0000 [0.00], Avg: -500.0000 (0.960)
Step: 1214, Reward: -500.0000 [0.00], Avg: -500.0000 (0.941)
Step: 1714, Reward: -498.6000 [2.80], Avg: -500.3500 (0.922)
Step: 2214, Reward: -500.0000 [0.00], Avg: -500.2800 (0.904)
Step: 2714, Reward: -500.0000 [0.00], Avg: -500.2333 (0.886)
Step: 3214, Reward: -466.2000 [67.60], Avg: -505.0286 (0.868)
Step: 3714, Reward: -500.0000 [0.00], Avg: -504.4000 (0.851)
Step: 4214, Reward: -500.0000 [0.00], Avg: -503.9111 (0.834)
Step: 4714, Reward: -500.0000 [0.00], Avg: -503.5200 (0.817)
Step: 5214, Reward: -500.0000 [0.00], Avg: -503.2000 (0.801)
Step: 5714, Reward: -500.0000 [0.00], Avg: -502.9333 (0.785)
Step: 6214, Reward: -500.0000 [0.00], Avg: -502.7077 (0.769)
Step: 6714, Reward: -500.0000 [0.00], Avg: -502.5143 (0.754)
Step: 7214, Reward: -500.0000 [0.00], Avg: -502.3467 (0.739)
Step: 7637, Reward: -500.0000 [0.00], Avg: -502.2000 (0.724)
Step: 8137, Reward: -500.0000 [0.00], Avg: -502.0706 (0.709)
Step: 8637, Reward: -500.0000 [0.00], Avg: -501.9556 (0.695)
Step: 9137, Reward: -500.0000 [0.00], Avg: -501.8526 (0.681)
Step: 9637, Reward: -500.0000 [0.00], Avg: -501.7600 (0.668)
Step: 10137, Reward: -500.0000 [0.00], Avg: -501.6762 (0.654)
Step: 10637, Reward: -500.0000 [0.00], Avg: -501.6000 (0.641)
Step: 11137, Reward: -500.0000 [0.00], Avg: -501.5304 (0.628)
Step: 11637, Reward: -500.0000 [0.00], Avg: -501.4667 (0.616)
Step: 12137, Reward: -500.0000 [0.00], Avg: -501.4080 (0.603)
Step: 12637, Reward: -500.0000 [0.00], Avg: -501.3538 (0.591)
Step: 13117, Reward: -500.0000 [0.00], Avg: -501.3037 (0.580)
Step: 13617, Reward: -500.0000 [0.00], Avg: -501.2571 (0.568)
Step: 14117, Reward: -500.0000 [0.00], Avg: -501.2138 (0.557)
Step: 14617, Reward: -500.0000 [0.00], Avg: -501.1733 (0.545)
Step: 15117, Reward: -500.0000 [0.00], Avg: -501.1355 (0.535)
Step: 15617, Reward: -500.0000 [0.00], Avg: -501.1000 (0.524)
Step: 16117, Reward: -500.0000 [0.00], Avg: -501.0667 (0.513)
Step: 16617, Reward: -500.0000 [0.00], Avg: -501.0353 (0.503)
Step: 17117, Reward: -500.0000 [0.00], Avg: -501.0057 (0.493)
Step: 17617, Reward: -500.0000 [0.00], Avg: -500.9778 (0.483)
Step: 18117, Reward: -500.0000 [0.00], Avg: -500.9514 (0.474)
Step: 18617, Reward: -500.0000 [0.00], Avg: -500.9263 (0.464)
Step: 19117, Reward: -500.0000 [0.00], Avg: -500.9026 (0.455)
Step: 19617, Reward: -500.0000 [0.00], Avg: -500.8800 (0.446)
Step: 20117, Reward: -500.0000 [0.00], Avg: -500.8585 (0.437)
Step: 20617, Reward: -500.0000 [0.00], Avg: -500.8381 (0.428)
Step: 21117, Reward: -500.0000 [0.00], Avg: -500.8186 (0.419)
Step: 21617, Reward: -500.0000 [0.00], Avg: -500.8000 (0.411)
Step: 22117, Reward: -500.0000 [0.00], Avg: -500.7822 (0.403)
Step: 22617, Reward: -500.0000 [0.00], Avg: -500.7652 (0.395)
Step: 23114, Reward: -500.0000 [0.00], Avg: -500.7489 (0.387)
Step: 23614, Reward: -500.0000 [0.00], Avg: -500.7333 (0.379)
Step: 24114, Reward: -500.0000 [0.00], Avg: -500.7184 (0.372)
Step: 24614, Reward: -500.0000 [0.00], Avg: -500.7040 (0.364)
Step: 25114, Reward: -500.0000 [0.00], Avg: -500.6902 (0.357)
Step: 25538, Reward: -500.0000 [0.00], Avg: -500.6769 (0.350)
Step: 26038, Reward: -500.0000 [0.00], Avg: -500.6642 (0.343)
Step: 26538, Reward: -500.0000 [0.00], Avg: -500.6519 (0.336)
Step: 27038, Reward: -500.0000 [0.00], Avg: -500.6400 (0.329)
Step: 27482, Reward: -500.0000 [0.00], Avg: -500.6286 (0.323)
Step: 27982, Reward: -500.0000 [0.00], Avg: -500.6175 (0.316)
Step: 28482, Reward: -500.0000 [0.00], Avg: -500.6069 (0.310)
Step: 28982, Reward: -500.0000 [0.00], Avg: -500.5966 (0.304)
Step: 29482, Reward: -500.0000 [0.00], Avg: -500.5867 (0.298)
Step: 29982, Reward: -500.0000 [0.00], Avg: -500.5770 (0.292)
Step: 30482, Reward: -500.0000 [0.00], Avg: -500.5677 (0.286)
Step: 30982, Reward: -500.0000 [0.00], Avg: -500.5587 (0.280)
Step: 31482, Reward: -500.0000 [0.00], Avg: -500.5500 (0.274)
Step: 31982, Reward: -500.0000 [0.00], Avg: -500.5415 (0.269)
Step: 32482, Reward: -500.0000 [0.00], Avg: -500.5333 (0.264)
Step: 32982, Reward: -500.0000 [0.00], Avg: -500.5254 (0.258)
Step: 33482, Reward: -500.0000 [0.00], Avg: -500.5176 (0.253)
Step: 33982, Reward: -500.0000 [0.00], Avg: -500.5101 (0.248)
Step: 34482, Reward: -500.0000 [0.00], Avg: -500.5029 (0.243)
Step: 34982, Reward: -500.0000 [0.00], Avg: -500.4958 (0.238)
Step: 35482, Reward: -500.0000 [0.00], Avg: -500.4889 (0.233)
Step: 35982, Reward: -500.0000 [0.00], Avg: -500.4822 (0.229)
Step: 36482, Reward: -500.0000 [0.00], Avg: -500.4757 (0.224)
Step: 36982, Reward: -500.0000 [0.00], Avg: -500.4693 (0.220)
Step: 37482, Reward: -500.0000 [0.00], Avg: -500.4632 (0.215)
Step: 37982, Reward: -500.0000 [0.00], Avg: -500.4571 (0.211)
Step: 38482, Reward: -500.0000 [0.00], Avg: -500.4513 (0.207)
Step: 38982, Reward: -500.0000 [0.00], Avg: -500.4456 (0.203)
Step: 39482, Reward: -500.0000 [0.00], Avg: -500.4400 (0.199)
Step: 39982, Reward: -500.0000 [0.00], Avg: -500.4346 (0.195)
Step: 40482, Reward: -500.0000 [0.00], Avg: -500.4293 (0.191)
Step: 40982, Reward: -500.0000 [0.00], Avg: -500.4241 (0.187)
Step: 41482, Reward: -500.0000 [0.00], Avg: -500.4190 (0.183)
Step: 41982, Reward: -500.0000 [0.00], Avg: -500.4141 (0.180)
Step: 42482, Reward: -500.0000 [0.00], Avg: -500.4093 (0.176)
Step: 42982, Reward: -500.0000 [0.00], Avg: -500.4046 (0.172)
Step: 43482, Reward: -500.0000 [0.00], Avg: -500.4000 (0.169)
Step: 43982, Reward: -500.0000 [0.00], Avg: -500.3955 (0.166)
Step: 44482, Reward: -500.0000 [0.00], Avg: -500.3911 (0.162)
Step: 44982, Reward: -500.0000 [0.00], Avg: -500.3868 (0.159)
Step: 45482, Reward: -500.0000 [0.00], Avg: -500.3826 (0.156)
Step: 45982, Reward: -500.0000 [0.00], Avg: -500.3785 (0.153)
Step: 46482, Reward: -500.0000 [0.00], Avg: -500.3745 (0.150)
Step: 46982, Reward: -500.0000 [0.00], Avg: -500.3705 (0.147)
Step: 47482, Reward: -500.0000 [0.00], Avg: -500.3667 (0.144)
Step: 47982, Reward: -500.0000 [0.00], Avg: -500.3629 (0.141)
Step: 48482, Reward: -500.0000 [0.00], Avg: -500.3592 (0.138)
Step: 48982, Reward: -500.0000 [0.00], Avg: -500.3556 (0.135)
Step: 49482, Reward: -500.0000 [0.00], Avg: -500.3520 (0.133)
Step: 49982, Reward: -491.4000 [17.20], Avg: -500.4337 (0.130)
Step: 50482, Reward: -500.0000 [0.00], Avg: -500.4294 (0.127)
Step: 50982, Reward: -500.0000 [0.00], Avg: -500.4252 (0.125)
Step: 51320, Reward: -500.0000 [0.00], Avg: -500.4212 (0.122)
Step: 51809, Reward: -500.0000 [0.00], Avg: -500.4171 (0.120)
Step: 52309, Reward: -500.0000 [0.00], Avg: -500.4132 (0.117)
Step: 52809, Reward: -466.4000 [50.76], Avg: -500.5698 (0.115)
Step: 53309, Reward: -500.0000 [0.00], Avg: -500.5645 (0.113)
Step: 53809, Reward: -500.0000 [0.00], Avg: -500.5593 (0.111)
Step: 54309, Reward: -476.8000 [32.83], Avg: -500.6418 (0.108)
Step: 54809, Reward: -495.6000 [8.80], Avg: -500.6756 (0.106)
Step: 55309, Reward: -500.0000 [0.00], Avg: -500.6696 (0.104)
Step: 55809, Reward: -500.0000 [0.00], Avg: -500.6637 (0.102)
Step: 56309, Reward: -500.0000 [0.00], Avg: -500.6578 (0.100)
Step: 56809, Reward: -500.0000 [0.00], Avg: -500.6521 (0.098)
Step: 57309, Reward: -500.0000 [0.00], Avg: -500.6465 (0.096)
Step: 57809, Reward: -500.0000 [0.00], Avg: -500.6410 (0.094)
Step: 58068, Reward: -500.0000 [0.00], Avg: -500.6355 (0.092)
Step: 58343, Reward: -500.0000 [0.00], Avg: -500.6302 (0.090)
Step: 58697, Reward: -500.0000 [0.00], Avg: -500.6249 (0.089)
Step: 58829, Reward: -500.0000 [0.00], Avg: -500.6198 (0.087)
Step: 58926, Reward: -500.0000 [0.00], Avg: -500.6147 (0.085)
Step: 59426, Reward: -500.0000 [0.00], Avg: -500.6097 (0.083)
Step: 59926, Reward: -500.0000 [0.00], Avg: -500.6048 (0.082)
Step: 60426, Reward: -500.0000 [0.00], Avg: -500.6000 (0.080)
Step: 60926, Reward: -500.0000 [0.00], Avg: -500.5952 (0.078)
Step: 61081, Reward: -219.0000 [42.77], Avg: -498.7147 (0.077)
Step: 61359, Reward: -500.0000 [0.00], Avg: -498.7247 (0.075)
Step: 61859, Reward: -500.0000 [0.00], Avg: -498.7346 (0.074)
Step: 62306, Reward: -500.0000 [0.00], Avg: -498.7443 (0.072)
Step: 62806, Reward: -500.0000 [0.00], Avg: -498.7539 (0.071)
Step: 63306, Reward: -500.0000 [0.00], Avg: -498.7634 (0.069)
Step: 63700, Reward: -500.0000 [0.00], Avg: -498.7727 (0.068)
Step: 64200, Reward: -144.8000 [20.94], Avg: -496.2873 (0.067)
Step: 64402, Reward: -500.0000 [0.00], Avg: -496.3149 (0.065)
Step: 64902, Reward: -134.2000 [20.93], Avg: -493.8062 (0.064)
Step: 65402, Reward: -500.0000 [0.00], Avg: -493.8514 (0.063)
Step: 65902, Reward: -500.0000 [0.00], Avg: -493.8959 (0.062)
Step: 66402, Reward: -500.0000 [0.00], Avg: -493.9398 (0.060)
Step: 66902, Reward: -500.0000 [0.00], Avg: -493.9831 (0.059)
Step: 67402, Reward: -500.0000 [0.00], Avg: -494.0258 (0.058)
Step: 67902, Reward: -500.0000 [0.00], Avg: -494.0679 (0.057)
Step: 68402, Reward: -500.0000 [0.00], Avg: -494.1094 (0.056)
Step: 68902, Reward: -500.0000 [0.00], Avg: -494.1503 (0.055)
Step: 69402, Reward: -500.0000 [0.00], Avg: -494.1906 (0.053)
Step: 69892, Reward: -117.8000 [41.58], Avg: -491.8974 (0.052)
Step: 70392, Reward: -500.0000 [0.00], Avg: -491.9525 (0.051)
Step: 70892, Reward: -500.0000 [0.00], Avg: -492.0069 (0.050)
Step: 71392, Reward: -500.0000 [0.00], Avg: -492.0605 (0.049)
Step: 71892, Reward: -500.0000 [0.00], Avg: -492.1135 (0.048)
Step: 72189, Reward: -500.0000 [0.00], Avg: -492.1657 (0.047)
Step: 72461, Reward: -500.0000 [0.00], Avg: -492.2172 (0.046)
Step: 72961, Reward: -500.0000 [0.00], Avg: -492.2681 (0.045)
Step: 73461, Reward: -500.0000 [0.00], Avg: -492.3183 (0.045)
Step: 73961, Reward: -500.0000 [0.00], Avg: -492.3679 (0.044)
Step: 74461, Reward: -500.0000 [0.00], Avg: -492.4168 (0.043)
Step: 74961, Reward: -500.0000 [0.00], Avg: -492.4651 (0.042)
Step: 75193, Reward: -96.4000 [36.71], Avg: -490.1907 (0.041)
Step: 75293, Reward: -81.0000 [7.87], Avg: -487.6667 (0.040)
Step: 75390, Reward: -115.2000 [33.11], Avg: -485.5457 (0.039)
Step: 75563, Reward: -88.4000 [10.38], Avg: -483.1435 (0.039)
Step: 75646, Reward: -98.4000 [39.68], Avg: -481.0135 (0.038)
Step: 75753, Reward: -111.0000 [52.65], Avg: -479.0664 (0.037)
Step: 75853, Reward: -90.8000 [11.02], Avg: -476.7661 (0.036)
Step: 75946, Reward: -130.2000 [72.34], Avg: -475.1041 (0.036)
Step: 76208, Reward: -116.2000 [42.60], Avg: -473.1987 (0.035)
Step: 76370, Reward: -263.4000 [194.33], Avg: -473.1061 (0.034)
Step: 76570, Reward: -178.4000 [161.37], Avg: -472.3124 (0.034)
Step: 76666, Reward: -82.8000 [8.52], Avg: -470.0580 (0.033)
Step: 76740, Reward: -341.4000 [194.60], Avg: -470.4458 (0.032)
Step: 76891, Reward: -110.0000 [22.46], Avg: -468.4693 (0.032)
Step: 77029, Reward: -98.8000 [29.22], Avg: -466.4899 (0.031)
Step: 77121, Reward: -103.4000 [32.96], Avg: -464.5817 (0.030)
Step: 77223, Reward: -81.8000 [6.40], Avg: -462.4186 (0.030)
Step: 77310, Reward: -89.0000 [5.62], Avg: -460.3169 (0.029)
Step: 77437, Reward: -127.2000 [75.53], Avg: -458.8533 (0.029)
Step: 77526, Reward: -91.0000 [9.08], Avg: -456.8264 (0.028)
Step: 77629, Reward: -118.0000 [32.66], Avg: -455.1063 (0.027)
Step: 77717, Reward: -84.4000 [7.76], Avg: -453.0787 (0.027)
Step: 77829, Reward: -87.0000 [9.82], Avg: -451.0995 (0.026)
Step: 77915, Reward: -89.4000 [8.69], Avg: -449.1491 (0.026)
Step: 77999, Reward: -127.8000 [54.58], Avg: -447.6834 (0.025)
Step: 78125, Reward: -93.6000 [6.62], Avg: -445.7847 (0.025)
Step: 78235, Reward: -99.2000 [10.67], Avg: -443.9590 (0.024)
Step: 78328, Reward: -112.2000 [24.11], Avg: -442.2961 (0.024)
Step: 78418, Reward: -98.4000 [9.48], Avg: -440.4981 (0.023)
Step: 78503, Reward: -107.2000 [21.48], Avg: -438.8306 (0.023)
Step: 78600, Reward: -92.4000 [1.50], Avg: -436.9959 (0.022)
Step: 78690, Reward: -96.6000 [7.50], Avg: -435.2345 (0.022)
Step: 78776, Reward: -90.6000 [8.36], Avg: -433.4646 (0.022)
Step: 78993, Reward: -93.2000 [11.11], Avg: -431.7413 (0.021)
Step: 79079, Reward: -104.6000 [21.58], Avg: -430.1499 (0.021)
Step: 79184, Reward: -98.2000 [10.32], Avg: -428.4834 (0.020)
Step: 79288, Reward: -119.0000 [31.80], Avg: -427.0520 (0.020)
Step: 79435, Reward: -89.0000 [8.29], Avg: -425.3610 (0.020)
Step: 79514, Reward: -94.0000 [10.86], Avg: -423.7258 (0.020)
Step: 79609, Reward: -107.2000 [23.74], Avg: -422.2395 (0.020)
Step: 79694, Reward: -95.4000 [7.00], Avg: -420.6242 (0.020)
Step: 79795, Reward: -97.6000 [6.86], Avg: -419.0354 (0.020)
Step: 79875, Reward: -106.8000 [4.31], Avg: -417.4958 (0.020)
Step: 79970, Reward: -90.6000 [7.45], Avg: -415.9065 (0.020)
Step: 80049, Reward: -102.6000 [19.85], Avg: -414.4537 (0.020)
Step: 80149, Reward: -93.4000 [11.52], Avg: -412.9289 (0.020)
Step: 80240, Reward: -101.0000 [10.47], Avg: -411.4511 (0.020)
Step: 80333, Reward: -88.2000 [6.58], Avg: -409.9064 (0.020)
Step: 80413, Reward: -94.0000 [10.99], Avg: -408.4263 (0.020)
Step: 80503, Reward: -107.6000 [28.19], Avg: -407.1092 (0.020)
Step: 80595, Reward: -112.2000 [49.72], Avg: -405.9304 (0.020)
Step: 80679, Reward: -111.6000 [21.49], Avg: -404.6249 (0.020)
Step: 80780, Reward: -110.8000 [43.46], Avg: -403.4327 (0.020)
Step: 81036, Reward: -89.4000 [12.63], Avg: -402.0043 (0.020)
Step: 81154, Reward: -89.2000 [7.44], Avg: -400.5639 (0.020)
Step: 81226, Reward: -87.2000 [4.45], Avg: -399.1135 (0.020)
Step: 81310, Reward: -86.2000 [7.25], Avg: -397.6852 (0.020)
Step: 81395, Reward: -96.4000 [20.51], Avg: -396.3793 (0.020)
Step: 81492, Reward: -98.4000 [23.13], Avg: -395.1068 (0.020)
Step: 81575, Reward: -100.4000 [12.77], Avg: -393.8076 (0.020)
Step: 81716, Reward: -156.4000 [17.72], Avg: -392.7998 (0.020)
Step: 81812, Reward: -105.4000 [29.63], Avg: -391.6228 (0.020)
Step: 81931, Reward: -90.8000 [18.37], Avg: -390.3389 (0.020)
Step: 82011, Reward: -88.6000 [10.25], Avg: -389.0199 (0.020)
Step: 82111, Reward: -85.4000 [6.31], Avg: -387.6807 (0.020)
Step: 82192, Reward: -105.2000 [32.90], Avg: -386.5615 (0.020)
Step: 82288, Reward: -83.8000 [7.52], Avg: -385.2434 (0.020)
Step: 82373, Reward: -83.4000 [10.17], Avg: -383.9471 (0.020)
Step: 82453, Reward: -84.2000 [6.24], Avg: -382.6484 (0.020)
Step: 82549, Reward: -89.6000 [9.41], Avg: -381.3989 (0.020)
Step: 82645, Reward: -78.6000 [1.74], Avg: -380.0785 (0.020)
Step: 82724, Reward: -90.0000 [7.87], Avg: -378.8462 (0.020)
Step: 82825, Reward: -92.2000 [8.91], Avg: -377.6386 (0.020)
Step: 82955, Reward: -90.8000 [5.23], Avg: -376.4195 (0.020)
Step: 83070, Reward: -130.0000 [83.86], Avg: -375.7189 (0.020)
Step: 83183, Reward: -87.4000 [8.04], Avg: -374.5160 (0.020)
Step: 83286, Reward: -99.0000 [11.97], Avg: -373.3897 (0.020)
Step: 83387, Reward: -97.8000 [15.51], Avg: -372.2830 (0.020)
Step: 83491, Reward: -90.2000 [6.85], Avg: -371.1167 (0.020)
Step: 83587, Reward: -90.2000 [8.26], Avg: -369.9663 (0.020)
Step: 83683, Reward: -92.4000 [9.05], Avg: -368.8380 (0.020)
Step: 83780, Reward: -109.4000 [31.09], Avg: -367.8826 (0.020)
Step: 83868, Reward: -102.0000 [5.69], Avg: -366.7985 (0.020)
Step: 83954, Reward: -92.4000 [3.61], Avg: -365.6749 (0.020)
Step: 84056, Reward: -113.8000 [58.23], Avg: -364.8747 (0.020)
Step: 84141, Reward: -99.2000 [17.54], Avg: -363.8536 (0.020)
Step: 84221, Reward: -102.2000 [28.32], Avg: -362.8973 (0.020)
Step: 84337, Reward: -93.0000 [7.69], Avg: -361.8271 (0.020)
Step: 84462, Reward: -93.2000 [9.43], Avg: -360.7734 (0.020)
Step: 84567, Reward: -102.0000 [20.52], Avg: -359.8088 (0.020)
Step: 84646, Reward: -88.4000 [8.50], Avg: -358.7487 (0.020)
Step: 84732, Reward: -81.8000 [6.97], Avg: -357.6645 (0.020)
Step: 84813, Reward: -99.8000 [11.96], Avg: -356.6808 (0.020)
Step: 84912, Reward: -113.0000 [27.24], Avg: -355.8185 (0.020)
Step: 84992, Reward: -116.4000 [30.34], Avg: -354.9888 (0.020)
Step: 85105, Reward: -89.2000 [8.18], Avg: -353.9706 (0.020)
Step: 85183, Reward: -117.6000 [33.65], Avg: -353.1725 (0.020)
Step: 85273, Reward: -97.4000 [24.55], Avg: -352.2657 (0.020)
Step: 85353, Reward: -116.4000 [37.69], Avg: -351.4916 (0.020)
Step: 85444, Reward: -92.6000 [14.61], Avg: -350.5411 (0.020)
Step: 85539, Reward: -100.2000 [23.03], Avg: -349.6600 (0.020)
Step: 85681, Reward: -124.8000 [54.55], Avg: -349.0024 (0.020)
Step: 85847, Reward: -100.4000 [15.12], Avg: -348.1044 (0.020)
Step: 85927, Reward: -85.8000 [6.88], Avg: -347.1258 (0.020)
Step: 86019, Reward: -92.0000 [11.70], Avg: -346.1967 (0.020)
Step: 86118, Reward: -91.6000 [12.52], Avg: -345.2762 (0.020)
Step: 86198, Reward: -82.8000 [3.12], Avg: -344.2938 (0.020)
Step: 86276, Reward: -86.6000 [7.23], Avg: -343.3487 (0.020)
Step: 86372, Reward: -89.2000 [7.28], Avg: -342.4206 (0.020)
Step: 86471, Reward: -91.0000 [13.34], Avg: -341.5289 (0.020)
Step: 86574, Reward: -79.6000 [4.45], Avg: -340.5682 (0.020)
Step: 86654, Reward: -105.0000 [41.82], Avg: -339.8479 (0.020)
Step: 86747, Reward: -105.4000 [27.05], Avg: -339.0798 (0.020)
Step: 86832, Reward: -107.8000 [25.73], Avg: -338.3213 (0.020)
Step: 86922, Reward: -84.4000 [7.79], Avg: -337.4164 (0.020)
Step: 87002, Reward: -95.6000 [24.82], Avg: -336.6215 (0.020)
Step: 87088, Reward: -100.4000 [37.68], Avg: -335.8969 (0.020)
Step: 87194, Reward: -85.6000 [6.77], Avg: -335.0114 (0.020)
Step: 87295, Reward: -107.6000 [28.85], Avg: -334.2919 (0.020)
Step: 87389, Reward: -101.0000 [8.92], Avg: -333.4819 (0.020)
Step: 87493, Reward: -94.2000 [14.03], Avg: -332.6717 (0.020)
Step: 87578, Reward: -96.4000 [6.59], Avg: -331.8485 (0.020)
Step: 87659, Reward: -102.4000 [30.70], Avg: -331.1387 (0.020)
Step: 87795, Reward: -142.2000 [44.28], Avg: -330.6239 (0.020)
Step: 87909, Reward: -120.8000 [56.13], Avg: -330.0788 (0.020)
Step: 88028, Reward: -95.4000 [8.80], Avg: -329.2807 (0.020)
Step: 88140, Reward: -82.6000 [3.61], Avg: -328.4248 (0.020)
Step: 88233, Reward: -90.8000 [7.98], Avg: -327.6191 (0.020)
Step: 88318, Reward: -112.6000 [32.25], Avg: -326.9800 (0.020)
Step: 88404, Reward: -97.2000 [9.28], Avg: -326.2117 (0.020)
Step: 88501, Reward: -88.8000 [6.58], Avg: -325.4102 (0.020)
Step: 88593, Reward: -103.0000 [21.02], Avg: -324.7134 (0.020)
Step: 88685, Reward: -115.8000 [39.59], Avg: -324.1295 (0.020)
Step: 88804, Reward: -97.2000 [8.59], Avg: -323.3792 (0.020)
Step: 89071, Reward: -101.0000 [7.13], Avg: -322.6421 (0.020)
Step: 89165, Reward: -90.2000 [11.75], Avg: -321.8889 (0.020)
Step: 89267, Reward: -109.0000 [27.72], Avg: -321.2590 (0.020)
Step: 89353, Reward: -91.4000 [7.47], Avg: -320.5052 (0.020)
Step: 89444, Reward: -90.0000 [7.67], Avg: -319.7523 (0.020)
Step: 89543, Reward: -100.0000 [9.23], Avg: -319.0435 (0.020)
Step: 89662, Reward: -102.4000 [19.05], Avg: -318.3805 (0.020)
Step: 89773, Reward: -91.6000 [7.36], Avg: -317.6466 (0.020)
Step: 89867, Reward: -95.4000 [18.08], Avg: -316.9661 (0.020)
Step: 89959, Reward: -106.8000 [26.67], Avg: -316.3565 (0.020)
Step: 90044, Reward: -115.6000 [36.05], Avg: -315.8111 (0.020)
Step: 90167, Reward: -96.4000 [9.00], Avg: -315.1167 (0.020)
Step: 90253, Reward: -122.4000 [72.44], Avg: -314.7210 (0.020)
Step: 90333, Reward: -109.0000 [32.69], Avg: -314.1537 (0.020)
Step: 90407, Reward: -88.0000 [13.48], Avg: -313.4587 (0.020)
Step: 90517, Reward: -100.6000 [39.06], Avg: -312.8926 (0.020)
Step: 90600, Reward: -109.0000 [54.56], Avg: -312.4077 (0.020)
Step: 90686, Reward: -93.0000 [11.52], Avg: -311.7350 (0.020)
Step: 90788, Reward: -89.8000 [4.02], Avg: -311.0320 (0.020)
Step: 90888, Reward: -89.0000 [8.29], Avg: -310.3447 (0.020)
Step: 91003, Reward: -83.8000 [6.01], Avg: -309.6379 (0.020)
Step: 91099, Reward: -124.2000 [45.57], Avg: -309.1910 (0.020)
Step: 91215, Reward: -98.6000 [11.83], Avg: -308.5580 (0.020)
Step: 91295, Reward: -92.6000 [8.33], Avg: -307.8989 (0.020)
Step: 91388, Reward: -86.2000 [14.40], Avg: -307.2429 (0.020)
Step: 91580, Reward: -99.4000 [12.21], Avg: -306.6258 (0.020)
Step: 91704, Reward: -93.6000 [8.40], Avg: -305.9823 (0.020)
Step: 91784, Reward: -113.6000 [46.08], Avg: -305.5237 (0.020)
Step: 91901, Reward: -91.8000 [7.03], Avg: -304.8777 (0.020)
Step: 91981, Reward: -90.6000 [7.50], Avg: -304.2336 (0.020)
Step: 92074, Reward: -90.4000 [7.66], Avg: -303.5933 (0.020)
Step: 92176, Reward: -83.6000 [4.54], Avg: -302.9262 (0.020)
Step: 92280, Reward: -91.0000 [6.42], Avg: -302.2919 (0.020)
Step: 92379, Reward: -91.6000 [6.71], Avg: -301.6643 (0.020)
Step: 92489, Reward: -95.0000 [15.27], Avg: -301.0772 (0.020)
Step: 92569, Reward: -93.0000 [6.39], Avg: -300.4604 (0.020)
Step: 92654, Reward: -114.4000 [45.86], Avg: -300.0330 (0.020)
Step: 92748, Reward: -79.0000 [0.00], Avg: -299.3612 (0.020)
Step: 92841, Reward: -92.8000 [14.89], Avg: -298.7803 (0.020)
Step: 92940, Reward: -93.4000 [9.22], Avg: -298.1877 (0.020)
Step: 93034, Reward: -107.8000 [37.31], Avg: -297.7266 (0.020)
Step: 93119, Reward: -90.0000 [4.15], Avg: -297.1153 (0.020)
Step: 93204, Reward: -90.0000 [10.30], Avg: -296.5260 (0.020)
Step: 93283, Reward: -84.0000 [6.32], Avg: -295.9105 (0.020)
Step: 93361, Reward: -85.8000 [4.53], Avg: -295.2986 (0.020)
Step: 93455, Reward: -95.0000 [17.45], Avg: -294.7560 (0.020)
Step: 93559, Reward: -96.0000 [12.21], Avg: -294.2041 (0.020)
Step: 93652, Reward: -90.0000 [11.82], Avg: -293.6366 (0.020)
Step: 93755, Reward: -98.6000 [11.00], Avg: -293.0954 (0.020)
Step: 93841, Reward: -91.0000 [3.58], Avg: -292.5132 (0.020)
Step: 93940, Reward: -96.2000 [6.58], Avg: -291.9584 (0.020)
Step: 94026, Reward: -98.4000 [11.64], Avg: -291.4280 (0.020)
Step: 94111, Reward: -95.4000 [11.84], Avg: -290.8926 (0.020)
Step: 94197, Reward: -96.2000 [8.16], Avg: -290.3519 (0.020)
Step: 94294, Reward: -94.2000 [7.36], Avg: -289.8063 (0.020)
Step: 94389, Reward: -89.0000 [4.69], Avg: -289.2411 (0.020)
Step: 94489, Reward: -98.8000 [9.33], Avg: -288.7207 (0.020)
Step: 94575, Reward: -94.2000 [11.44], Avg: -288.1961 (0.020)
Step: 94658, Reward: -96.0000 [10.68], Avg: -287.6775 (0.020)
Step: 94744, Reward: -88.6000 [6.15], Avg: -287.1278 (0.020)
Step: 94835, Reward: -86.2000 [5.49], Avg: -286.5726 (0.020)
Step: 94925, Reward: -89.8000 [4.12], Avg: -286.0268 (0.020)
Step: 95030, Reward: -94.0000 [9.32], Avg: -285.5107 (0.020)
Step: 95118, Reward: -96.4000 [8.26], Avg: -285.0013 (0.020)
Step: 95196, Reward: -94.4000 [10.67], Avg: -284.4959 (0.020)
Step: 95299, Reward: -95.2000 [15.01], Avg: -284.0077 (0.020)
Step: 95404, Reward: -89.0000 [6.07], Avg: -283.4799 (0.020)
Step: 95489, Reward: -98.6000 [11.84], Avg: -282.9979 (0.020)
Step: 95580, Reward: -125.6000 [56.19], Avg: -282.7168 (0.020)
Step: 95699, Reward: -95.2000 [5.11], Avg: -282.2115 (0.020)
Step: 95807, Reward: -92.2000 [8.91], Avg: -281.7112 (0.020)
Step: 95903, Reward: -93.4000 [6.68], Avg: -281.2109 (0.020)
Step: 95991, Reward: -100.4000 [5.43], Avg: -280.7290 (0.020)
Step: 96092, Reward: -95.0000 [5.59], Avg: -280.2355 (0.020)
Step: 96180, Reward: -101.0000 [40.34], Avg: -279.8560 (0.020)
Step: 96269, Reward: -91.2000 [7.44], Avg: -279.3622 (0.020)
Step: 96355, Reward: -92.8000 [9.11], Avg: -278.8800 (0.020)
Step: 96439, Reward: -90.8000 [5.27], Avg: -278.3846 (0.020)
Step: 96550, Reward: -96.0000 [13.78], Avg: -277.9289 (0.020)
Step: 96636, Reward: -100.4000 [13.17], Avg: -277.4859 (0.020)
Step: 96714, Reward: -91.2000 [6.05], Avg: -277.0014 (0.020)
Step: 96799, Reward: -98.0000 [4.60], Avg: -276.5338 (0.020)
Step: 96887, Reward: -92.8000 [8.70], Avg: -276.0658 (0.020)
Step: 96997, Reward: -98.4000 [6.95], Avg: -275.6106 (0.020)
Step: 97099, Reward: -86.2000 [2.79], Avg: -275.1142 (0.020)
Step: 97184, Reward: -87.2000 [12.70], Avg: -274.6495 (0.020)
Step: 97268, Reward: -90.6000 [11.64], Avg: -274.1934 (0.020)
Step: 97354, Reward: -98.6000 [11.86], Avg: -273.7613 (0.020)
Step: 97456, Reward: -148.8000 [83.13], Avg: -273.6513 (0.020)
Step: 97558, Reward: -90.2000 [8.16], Avg: -273.1912 (0.020)
Step: 97643, Reward: -97.2000 [12.69], Avg: -272.7637 (0.020)
Step: 97742, Reward: -90.8000 [7.30], Avg: -272.3077 (0.020)
Step: 97936, Reward: -88.0000 [9.80], Avg: -271.8532 (0.020)
Step: 98039, Reward: -99.0000 [15.74], Avg: -271.4451 (0.020)
Step: 98137, Reward: -89.4000 [5.89], Avg: -270.9887 (0.020)
Step: 98241, Reward: -92.0000 [8.07], Avg: -270.5471 (0.020)
Step: 98341, Reward: -89.0000 [9.88], Avg: -270.1047 (0.020)
Step: 98454, Reward: -82.0000 [9.55], Avg: -269.6456 (0.020)
Step: 98550, Reward: -131.4000 [56.89], Avg: -269.4370 (0.020)
Step: 98654, Reward: -95.6000 [7.94], Avg: -269.0128 (0.020)
Step: 98762, Reward: -107.2000 [31.49], Avg: -268.6803 (0.020)
Step: 98850, Reward: -85.8000 [11.55], Avg: -268.2443 (0.020)
Step: 98936, Reward: -90.2000 [16.63], Avg: -267.8347 (0.020)
Step: 99023, Reward: -106.2000 [19.00], Avg: -267.4736 (0.020)
Step: 99111, Reward: -94.4000 [14.53], Avg: -267.0732 (0.020)
Step: 99206, Reward: -86.2000 [11.70], Avg: -266.6471 (0.020)
Step: 99294, Reward: -92.2000 [12.89], Avg: -266.2412 (0.020)
Step: 99377, Reward: -89.2000 [8.42], Avg: -265.8186 (0.020)
Step: 99485, Reward: -84.4000 [8.73], Avg: -265.3868 (0.020)
Step: 99579, Reward: -80.0000 [5.87], Avg: -264.9392 (0.020)
Step: 99670, Reward: -88.8000 [6.05], Avg: -264.5160 (0.020)
Step: 99760, Reward: -88.8000 [11.89], Avg: -264.1095 (0.020)
Step: 99858, Reward: -97.2000 [5.46], Avg: -263.7099 (0.020)
Step: 99958, Reward: -78.8000 [5.84], Avg: -263.2677 (0.020)
