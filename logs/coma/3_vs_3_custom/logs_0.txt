Model: <class 'multiagent.coma.COMAAgent'>, Dir: 3_vs_3_custom
num_envs: 16, state_size: (3, 115), action_size: [19], action_space: MultiDiscrete([19 19 19]),

import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, REG_LAMBDA, EPS_MIN

BATCH_SIZE = 32					# Number of samples to train on for each train step

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		out_dims = state.size()[:-1]
		state = state.view(np.prod(out_dims), -1)
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
		self.hidden = self.recurrent(state, self.hidden)
		action_probs = self.action_probs(self.hidden).softmax(-1)
		action_probs = action_probs.view(*out_dims, -1)
		return action_probs

	def init_hidden(self, batch_size=1):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, *action_size)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		q_values = self.q_values(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		actor_input = [np.sum(state_size) + action_size[0]]
		self.actor_local = COMAActor(actor_input, action_size)
		self.actor_target = COMAActor(actor_input, action_size)
		self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=lr, weight_decay=REG_LAMBDA)
		
		critic_input = [np.sum(state_size) + 2*action_size[0]*state_size[0]]
		self.critic_local = COMACritic(critic_input, action_size)
		self.critic_target = COMACritic(critic_input, action_size)
		self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=lr, weight_decay=REG_LAMBDA)
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action_probs = self.actor_local(state.to(self.device), sample)
			return action_probs.cpu().numpy() if numpy else action_probs

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = self.critic_local(state.to(self.device))
			return q_values

	def optimize(self, states, actions, actor_inputs, critic_inputs, q_values, q_selecteds, q_targets):
		for t in reversed(range(q_targets.size(0))):
			q_values[t] = self.get_value(critic_inputs[t])
			q_selected = torch.gather(q_values[t], dim=-1, index=actions[t].argmax(-1, keepdims=True))
			critic_loss = (q_selected - q_targets[t].detach()).pow(2)
			self.step(self.critic_optimizer, critic_loss.mean(), retain=True)

		hidden = self.actor_local.hidden
		action_probs = torch.stack([self.get_action_probs(actor_inputs[t]) for t in range(q_targets.size(0))], dim=0)
		baseline = (action_probs * q_values[:-1]).sum(-1, keepdims=True).detach()
		log_probs = torch.gather(action_probs, dim=-1, index=actions[:-1].argmax(-1, keepdims=True)).log()
		advantages = (q_selecteds[:-1] - baseline).detach()
		actor_loss = (advantages * log_probs).sum()
		self.step(self.actor_optimizer, actor_loss.mean())
		self.actor_local.hidden = hidden

	def save_model(self, dirname="pytorch", name="best"):
		PTACNetwork.save_model(self, "coma", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		PTACNetwork.load_model(self, "coma", dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, save=True):
		if not hasattr(self, "action"): self.action = np.zeros([*state.shape[:-1], self.action_size[0]])
		eps = self.eps if eps is None else eps
		batch_size = state.shape[0] if len(state.shape) > len(self.state_size) else 1
		agent_ids = np.repeat(np.expand_dims(np.eye(self.state_size[0]), 0), repeats=batch_size, axis=0).squeeze()
		last_action = self.action if len(state.shape) == len(self.action.shape) else np.zeros([*state.shape[:-1], self.action_size[0]])
		inputs = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
		action_greedy = self.network.get_action_probs(inputs, sample=sample, grad=False, numpy=save)
		action = (np.random.rand if save else torch.rand)(*action_greedy.shape) if random.random() < eps else action_greedy
		if save: self.action = action.astype(np.float32)
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if done[0] or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			states = torch.cat([states, next_state.unsqueeze(0)], dim=0)
			actions = torch.cat([actions, self.get_action(next_state, save=False).unsqueeze(0)], dim=0)
			dones = dones.unsqueeze(-1).repeat_interleave(self.state_size[0], dim=-1)

			action_indices = actions.argmax(-1, keepdims=True)
			actions_one_hot = torch.zeros_like(actions)
			actions_one_hot.scatter_(-1, action_indices, 1)
			actions_joint_one_hot = actions_one_hot.view(*actions.size()[:-2], 1, -1).repeat_interleave(self.state_size[0], dim=-2)
			agent_mask = (1-torch.eye(self.state_size[0])).view(-1, 1).repeat(1, self.action_size[0]).view(1, 1, self.state_size[0], -1)
			last_actions = torch.cat([torch.zeros_like(actions_one_hot[0:1]), actions_one_hot[:-1]], dim=0)
			last_actions_joint = last_actions.view(*actions.size()[:-2], 1, -1).repeat_interleave(self.state_size[0], dim=-2)
			agent_ids = torch.eye(self.state_size[0]).unsqueeze(0).unsqueeze(0).expand(*actions.size()[:2], -1, -1)
			critic_inputs = torch.cat([states, actions_joint_one_hot * agent_mask, last_actions_joint, agent_ids], dim=-1)
			actor_inputs = torch.cat([states, last_actions, agent_ids], dim=-1)

			q_values = self.network.get_value(critic_inputs, grad=False)
			q_selecteds = torch.gather(q_values, dim=-1, index=actions.argmax(-1, keepdims=True))
			q_targets, _ = self.compute_gae(q_selecteds[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), q_selecteds[:-1])
			self.network.optimize(states, actions, actor_inputs, critic_inputs, q_values, q_selecteds, q_targets)
		if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)


Step: 399, Reward: -0.2000 [0.40], Avg: -0.6000 (0.980)
Step: 799, Reward: -0.4000 [0.49], Avg: -0.7449 (0.960)
Step: 1199, Reward: 0.0000 [0.00], Avg: -0.4966 (0.941)
Step: 1599, Reward: -0.2000 [0.40], Avg: -0.5225 (0.922)
Step: 1999, Reward: -0.4000 [0.49], Avg: -0.5960 (0.904)
Step: 2399, Reward: -0.4000 [0.49], Avg: -0.6449 (0.886)
Step: 2799, Reward: -0.4000 [0.49], Avg: -0.6799 (0.868)
Step: 3199, Reward: 0.0000 [0.00], Avg: -0.5949 (0.851)
Step: 3599, Reward: -0.4000 [0.49], Avg: -0.6277 (0.834)
Step: 3999, Reward: -0.8000 [0.40], Avg: -0.6849 (0.817)
Step: 4399, Reward: -0.2000 [0.40], Avg: -0.6772 (0.801)
Step: 4799, Reward: -0.4000 [0.80], Avg: -0.7208 (0.785)
Step: 5199, Reward: -0.2000 [0.40], Avg: -0.7115 (0.769)
Step: 5599, Reward: -0.6000 [0.80], Avg: -0.7607 (0.754)
Step: 5999, Reward: -0.2000 [0.40], Avg: -0.7500 (0.739)
Step: 6399, Reward: 0.0000 [0.00], Avg: -0.7031 (0.724)
Step: 6799, Reward: -0.6000 [0.80], Avg: -0.7441 (0.709)
Step: 7199, Reward: -0.2000 [0.40], Avg: -0.7361 (0.695)
Step: 7599, Reward: -0.6000 [0.80], Avg: -0.7710 (0.681)
Step: 7999, Reward: -0.2000 [0.40], Avg: -0.7625 (0.668)
Step: 8399, Reward: -0.8000 [0.75], Avg: -0.7999 (0.654)
Step: 8799, Reward: -0.2000 [0.40], Avg: -0.7908 (0.641)
Step: 9199, Reward: -0.2000 [0.40], Avg: -0.7825 (0.628)
Step: 9599, Reward: -0.2000 [0.40], Avg: -0.7749 (0.616)
Step: 9999, Reward: -0.4000 [0.49], Avg: -0.7795 (0.603)
Step: 10399, Reward: 0.0000 [0.00], Avg: -0.7495 (0.591)
Step: 10799, Reward: 0.0000 [0.00], Avg: -0.7218 (0.580)
Step: 11199, Reward: -0.2000 [0.40], Avg: -0.7174 (0.568)
Step: 11599, Reward: -0.2000 [0.40], Avg: -0.7134 (0.557)
Step: 11999, Reward: -0.2000 [0.40], Avg: -0.7096 (0.545)
Step: 12399, Reward: -0.4000 [0.49], Avg: -0.7154 (0.535)
Step: 12799, Reward: -0.4000 [0.49], Avg: -0.7209 (0.524)
Step: 13199, Reward: -0.4000 [0.49], Avg: -0.7260 (0.513)
Step: 13599, Reward: -0.2000 [0.40], Avg: -0.7223 (0.503)
Step: 13999, Reward: 0.0000 [0.00], Avg: -0.7016 (0.493)
Step: 14399, Reward: -0.6000 [0.80], Avg: -0.7210 (0.483)
Step: 14799, Reward: -0.4000 [0.80], Avg: -0.7340 (0.474)
Step: 15199, Reward: -0.2000 [0.40], Avg: -0.7305 (0.464)
Step: 15599, Reward: -0.2000 [0.40], Avg: -0.7271 (0.455)
Step: 15999, Reward: -0.4000 [0.49], Avg: -0.7312 (0.446)
Step: 16399, Reward: -0.4000 [0.49], Avg: -0.7351 (0.437)
Step: 16799, Reward: -1.0000 [0.89], Avg: -0.7627 (0.428)
Step: 17199, Reward: -0.4000 [0.49], Avg: -0.7656 (0.419)
Step: 17599, Reward: 0.0000 [0.00], Avg: -0.7482 (0.411)
Step: 17999, Reward: -0.4000 [0.49], Avg: -0.7514 (0.403)
Step: 18399, Reward: -0.4000 [0.80], Avg: -0.7611 (0.395)
Step: 18799, Reward: -0.6000 [0.80], Avg: -0.7747 (0.387)
Step: 19199, Reward: -0.4000 [0.49], Avg: -0.7771 (0.379)
Step: 19599, Reward: -0.6000 [0.49], Avg: -0.7835 (0.372)
Step: 19999, Reward: -0.2000 [0.40], Avg: -0.7798 (0.364)
Step: 20399, Reward: -0.2000 [0.40], Avg: -0.7763 (0.357)
