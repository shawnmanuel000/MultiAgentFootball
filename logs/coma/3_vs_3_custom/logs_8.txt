Model: <class 'multiagent.coma.COMAAgent'>, Dir: 3_vs_3_custom
num_envs: 16, state_size: [(1, 115), (1, 115), (1, 115), (1, 115), (1, 115), (1, 115)], action_size: [[1, 19], [1, 19], [1, 19], [1, 19], [1, 19], [1, 19]], action_space: [MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19]), MultiDiscrete([19])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 512
EPS_MAX = 1.0
EPS_MIN = 0.01
EPS_DECAY = 0.999
NUM_ENVS = 16
EPISODE_LIMIT = 500
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		self.episode_batch.update({"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy(), actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			self.episode_batch.update({"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
		self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

	def save_models(self, path):
		self.mac.save_models(path)
		torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
		torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
		torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

	def load_models(self, path):
		self.mac.load_models(path)
		self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
		self.target_critic.load_state_dict(self.critic.state_dict())
		self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
		self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def load_state(self, other_mac):
		self.agent.load_state_dict(other_mac.agent.state_dict())

	def cuda(self):
		self.agent.cuda()

	def save_models(self, path):
		torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

	def load_models(self, path):
		self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		h = self.rnn(x, h_in)
		q = self.fc2(h)
		return q, h

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.970             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
# env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(lambda: make_env(env_name), ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [0. 0. 0. 0. 0. 0.] (1.000)
Step: 999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [0. 0. 0. 0. 0. 0.] (1.000)
Step: 1499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.2   -0.2   -0.2   -0.067 -0.067 -0.067] (1.000)
Step: 1999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.387 -0.387 -0.387 -0.187 -0.187 -0.187] (1.000)
Step: 2499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.31 -0.31 -0.31 -0.15 -0.15 -0.15] (1.000)
Step: 2999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.125 -0.125 -0.125] (1.000)
Step: 3499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.107 -0.107 -0.107] (1.000)
Step: 3999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.244 -0.244 -0.244 -0.244 -0.244 -0.244] (1.000)
Step: 4499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.216 -0.216 -0.216] (1.000)
Step: 4999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.195 -0.195 -0.195 -0.195 -0.195 -0.195] (1.000)
Step: 5499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.232 -0.232 -0.232 -0.195 -0.195 -0.195] (1.000)
Step: 5999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.196 -0.196 -0.196] (1.000)
Step: 6499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.273 -0.273 -0.273 -0.273 -0.273 -0.273] (1.000)
Step: 6999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.253 -0.253 -0.253] (1.000)
Step: 7499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.25  -0.25  -0.25  -0.277 -0.277 -0.277] (1.000)
Step: 7999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.272 -0.272 -0.272 -0.272 -0.272 -0.272] (1.000)
Step: 8499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.268 -0.268 -0.268] (1.000)
Step: 8999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.253 -0.253 -0.253] (1.000)
Step: 9499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.239 -0.239 -0.239] (1.000)
Step: 9999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.257 -0.257 -0.257] (1.000)
Step: 10499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.274 -0.274 -0.274] (1.000)
Step: 10999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.272 -0.272 -0.272 -0.29  -0.29  -0.29 ] (1.000)
Step: 11499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.277 -0.277 -0.277] (1.000)
Step: 11999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.266 -0.266 -0.266] (1.000)
Step: 12499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.247 -0.247 -0.247 -0.279 -0.279 -0.279] (1.000)
Step: 12999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.241 -0.241 -0.241 -0.303 -0.303 -0.303] (1.000)
Step: 13499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.292 -0.292 -0.292] (1.000)
Step: 13999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.281 -0.281 -0.281] (1.000)
Step: 14499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.292 -0.292 -0.292] (1.000)
Step: 14999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.302 -0.302 -0.302] (1.000)
Step: 15499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.293 -0.293 -0.293] (1.000)
Step: 15999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.302 -0.302 -0.302] (1.000)
Step: 16499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.226 -0.226 -0.226 -0.299 -0.299 -0.299] (1.000)
Step: 16999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22 -0.22 -0.22 -0.29 -0.29 -0.29] (1.000)
Step: 17499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.216 -0.216 -0.216 -0.307 -0.307 -0.307] (1.000)
Step: 17999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.21  -0.21  -0.21  -0.299 -0.299 -0.299] (1.000)
Step: 18499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.219 -0.219 -0.219 -0.316 -0.316 -0.316] (1.000)
Step: 18999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.229 -0.229 -0.229 -0.313 -0.313 -0.313] (1.000)
Step: 19499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.305 -0.305 -0.305] (1.000)
Step: 19999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.313 -0.313 -0.313] (1.000)
Step: 20499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.232 -0.232 -0.232 -0.31  -0.31  -0.31 ] (1.000)
Step: 20999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.303 -0.303 -0.303] (1.000)
Step: 21499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.235 -0.235 -0.235 -0.3   -0.3   -0.3  ] (1.000)
Step: 21999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.243 -0.243 -0.243 -0.298 -0.298 -0.298] (1.000)
Step: 22499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.291 -0.291 -0.291] (1.000)
Step: 22999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.285 -0.285 -0.285] (1.000)
Step: 23499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.23  -0.23  -0.23  -0.298 -0.298 -0.298] (1.000)
Step: 23999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.292 -0.292 -0.292] (1.000)
Step: 24499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.239 -0.239 -0.239 -0.288 -0.288 -0.288] (1.000)
Step: 24999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.282 -0.282 -0.282] (1.000)
Step: 25499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.233 -0.233 -0.233 -0.288 -0.288 -0.288] (1.000)
Step: 25999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.282 -0.282 -0.282] (1.000)
Step: 26499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.277 -0.277 -0.277] (1.000)
Step: 26999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.283 -0.283 -0.283] (1.000)
Step: 27499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.231 -0.231 -0.231 -0.282 -0.282 -0.282] (1.000)
Step: 27999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.237 -0.237 -0.237 -0.28  -0.28  -0.28 ] (1.000)
Step: 28499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.279 -0.279 -0.279] (1.000)
Step: 28999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.274 -0.274 -0.274] (1.000)
Step: 29499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.269 -0.269 -0.269] (1.000)
Step: 29999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.265 -0.265 -0.265] (1.000)
Step: 30499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.26  -0.26  -0.26 ] (1.000)
Step: 30999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.234 -0.234 -0.234 -0.259 -0.259 -0.259] (1.000)
Step: 31499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.24  -0.24  -0.24  -0.259 -0.259 -0.259] (1.000)
Step: 31999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.245 -0.245 -0.245 -0.258 -0.258 -0.258] (1.000)
Step: 32499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.254 -0.254 -0.254] (1.000)
Step: 32999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.25  -0.25  -0.25 ] (1.000)
Step: 33499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.243 -0.243 -0.243 -0.249 -0.249 -0.249] (1.000)
Step: 33999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.242 -0.242 -0.242 -0.254 -0.254 -0.254] (1.000)
Step: 34499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.251 -0.251 -0.251] (1.000)
Step: 34999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.245 -0.245 -0.245 -0.256 -0.256 -0.256] (1.000)
Step: 35499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.252 -0.252 -0.252] (1.000)
Step: 35999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.239 -0.239 -0.239 -0.261 -0.261 -0.261] (1.000)
Step: 36499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.239 -0.239 -0.239 -0.266 -0.266 -0.266] (1.000)
Step: 36999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.238 -0.238 -0.238 -0.27  -0.27  -0.27 ] (1.000)
Step: 37499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.243 -0.243 -0.243 -0.269 -0.269 -0.269] (1.000)
Step: 37999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.266 -0.266 -0.266] (1.000)
Step: 38499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.265 -0.265 -0.265] (1.000)
Step: 38999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.262 -0.262 -0.262] (1.000)
Step: 39499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.246 -0.246 -0.246 -0.261 -0.261 -0.261] (1.000)
Step: 39999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.258 -0.258 -0.258] (1.000)
Step: 40499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.254 -0.254 -0.254] (1.000)
Step: 40999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.251 -0.251 -0.251] (1.000)
Step: 41499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.236 -0.236 -0.236 -0.256 -0.256 -0.256] (1.000)
Step: 41999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.255 -0.255 -0.255] (1.000)
Step: 42499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.245 -0.245 -0.245 -0.254 -0.254 -0.254] (1.000)
Step: 42999, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [1.0000], Avg: [-0.258 -0.258 -0.258 -0.254 -0.254 -0.254] (1.000)
Step: 43499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.253 -0.253 -0.253] (1.000)
Step: 43999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.252 -0.252 -0.252] (1.000)
Step: 44499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.25  -0.25  -0.25 ] (1.000)
Step: 44999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.27  -0.27  -0.27  -0.248 -0.248 -0.248] (1.000)
Step: 45499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.269 -0.269 -0.269 -0.252 -0.252 -0.252] (1.000)
Step: 45999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.249 -0.249 -0.249] (1.000)
Step: 46499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.246 -0.246 -0.246] (1.000)
Step: 46999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.244 -0.244 -0.244] (1.000)
Step: 47499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.241 -0.241 -0.241] (1.000)
Step: 47999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.239 -0.239 -0.239] (1.000)
Step: 48499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.238 -0.238 -0.238] (1.000)
Step: 48999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.268 -0.268 -0.268 -0.24  -0.24  -0.24 ] (1.000)
Step: 49499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.272 -0.272 -0.272 -0.239 -0.239 -0.239] (1.000)
Step: 49999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.237 -0.237 -0.237] (1.000)
Step: 50499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.235 -0.235 -0.235] (1.000)
Step: 50999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.232 -0.232 -0.232] (1.000)
Step: 51499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.232 -0.232 -0.232] (1.000)
Step: 51999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.27  -0.27  -0.27  -0.232 -0.232 -0.232] (1.000)
Step: 52499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.274 -0.274 -0.274 -0.236 -0.236 -0.236] (1.000)
Step: 52999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.273 -0.273 -0.273 -0.239 -0.239 -0.239] (1.000)
Step: 53499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.237 -0.237 -0.237] (1.000)
Step: 53999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.27 -0.27 -0.27 -0.24 -0.24 -0.24] (1.000)
Step: 54499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.238 -0.238 -0.238] (1.000)
Step: 54999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.236 -0.236 -0.236] (1.000)
Step: 55499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.264 -0.264 -0.264 -0.239 -0.239 -0.239] (1.000)
Step: 55999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.264 -0.264 -0.264 -0.242 -0.242 -0.242] (1.000)
Step: 56499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.245 -0.245 -0.245] (1.000)
Step: 56999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.243 -0.243 -0.243] (1.000)
Step: 57499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.241 -0.241 -0.241] (1.000)
Step: 57999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.239 -0.239 -0.239] (1.000)
Step: 58499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.242 -0.242 -0.242] (1.000)
Step: 58999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.245 -0.245 -0.245] (1.000)
Step: 59499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.245 -0.245 -0.245] (1.000)
Step: 59999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.243 -0.243 -0.243] (1.000)
Step: 60499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.241 -0.241 -0.241] (1.000)
Step: 60999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.239 -0.239 -0.239] (1.000)
Step: 61499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.237 -0.237 -0.237] (1.000)
Step: 61999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 62499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 62999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.247 -0.247 -0.247 -0.241 -0.241 -0.241] (1.000)
Step: 63499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.239 -0.239 -0.239] (1.000)
Step: 63999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.237 -0.237 -0.237] (1.000)
Step: 64499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.235 -0.235 -0.235] (1.000)
Step: 64999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.243 -0.243 -0.243 -0.243 -0.243 -0.243] (1.000)
Step: 65499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.242 -0.242 -0.242 -0.245 -0.245 -0.245] (1.000)
Step: 65999, Reward: [0. 0. 0. 0. 0. 0.] [1.0954], Avg: [-0.249 -0.249 -0.249 -0.252 -0.252 -0.252] (1.000)
Step: 66499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.25  -0.25  -0.25 ] (1.000)
Step: 66999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.248 -0.248 -0.248] (1.000)
Step: 67499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.246 -0.246 -0.246] (1.000)
Step: 67999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.25  -0.25  -0.25  -0.247 -0.247 -0.247] (1.000)
Step: 68499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.25 -0.25 -0.25 -0.25 -0.25 -0.25] (1.000)
Step: 68999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.248 -0.248 -0.248] (1.000)
Step: 69499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.246 -0.246 -0.246] (1.000)
Step: 69999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.244 -0.244 -0.244] (1.000)
Step: 70499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.249 -0.249 -0.249 -0.243 -0.243 -0.243] (1.000)
Step: 70999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.243 -0.243 -0.243] (1.000)
Step: 71499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.256 -0.256 -0.256 -0.242 -0.242 -0.242] (1.000)
Step: 71999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.24  -0.24  -0.24 ] (1.000)
Step: 72499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.24  -0.24  -0.24 ] (1.000)
Step: 72999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.24  -0.24  -0.24 ] (1.000)
Step: 73499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.238 -0.238 -0.238] (1.000)
Step: 73999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.237 -0.237 -0.237] (1.000)
Step: 74499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.239 -0.239 -0.239] (1.000)
Step: 74999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.237 -0.237 -0.237] (1.000)
Step: 75499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.236 -0.236 -0.236] (1.000)
Step: 75999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.236 -0.236 -0.236] (1.000)
Step: 76499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.235 -0.235 -0.235] (1.000)
Step: 76999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [1.0000], Avg: [-0.262 -0.262 -0.262 -0.239 -0.239 -0.239] (1.000)
Step: 77499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.237 -0.237 -0.237] (1.000)
Step: 77999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.236 -0.236 -0.236] (1.000)
Step: 78499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.234 -0.234 -0.234] (1.000)
Step: 78999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.234 -0.234 -0.234] (1.000)
Step: 79499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.233 -0.233 -0.233] (1.000)
Step: 79999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.231 -0.231 -0.231] (1.000)
Step: 80499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.23  -0.23  -0.23 ] (1.000)
Step: 80999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.228 -0.228 -0.228] (1.000)
Step: 81499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.257 -0.257 -0.257 -0.23  -0.23  -0.23 ] (1.000)
Step: 81999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.229 -0.229 -0.229] (1.000)
Step: 82499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.227 -0.227 -0.227] (1.000)
Step: 82999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.226 -0.226 -0.226] (1.000)
Step: 83499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.225 -0.225 -0.225] (1.000)
Step: 83999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.225 -0.225 -0.225] (1.000)
Step: 84499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.227 -0.227 -0.227] (1.000)
Step: 84999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.229 -0.229 -0.229] (1.000)
Step: 85499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.229 -0.229 -0.229] (1.000)
Step: 85999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.231 -0.231 -0.231] (1.000)
Step: 86499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.23  -0.23  -0.23 ] (1.000)
Step: 86999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.23  -0.23  -0.23 ] (1.000)
Step: 87499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.229 -0.229 -0.229] (1.000)
Step: 87999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.229 -0.229 -0.229] (1.000)
Step: 88499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.227 -0.227 -0.227] (1.000)
Step: 88999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.226 -0.226 -0.226] (1.000)
Step: 89499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.225 -0.225 -0.225] (1.000)
Step: 89999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.227 -0.227 -0.227] (1.000)
Step: 90499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.229 -0.229 -0.229] (1.000)
Step: 90999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.231 -0.231 -0.231] (1.000)
Step: 91499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.231 -0.231 -0.231] (1.000)
Step: 91999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.233 -0.233 -0.233] (1.000)
Step: 92499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.231 -0.231 -0.231] (1.000)
Step: 92999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.235 -0.235 -0.235] (1.000)
Step: 93499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.234 -0.234 -0.234] (1.000)
Step: 93999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.234 -0.234 -0.234] (1.000)
Step: 94499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.232 -0.232 -0.232] (1.000)
Step: 94999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.231 -0.231 -0.231] (1.000)
Step: 95499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.231 -0.231 -0.231] (1.000)
Step: 95999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.23  -0.23  -0.23 ] (1.000)
Step: 96499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.229 -0.229 -0.229] (1.000)
Step: 96999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.23  -0.23  -0.23 ] (1.000)
Step: 97499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.25  -0.25  -0.25  -0.234 -0.234 -0.234] (1.000)
Step: 97999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.252 -0.252 -0.252 -0.238 -0.238 -0.238] (1.000)
Step: 98499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.236 -0.236 -0.236] (1.000)
Step: 98999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [1.0954], Avg: [-0.252 -0.252 -0.252 -0.242 -0.242 -0.242] (1.000)
Step: 99499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.244 -0.244 -0.244] (1.000)
Step: 99999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.244 -0.244 -0.244] (1.000)
Step: 100499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.256 -0.256 -0.256 -0.246 -0.246 -0.246] (1.000)
Step: 100999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.248 -0.248 -0.248] (1.000)
Step: 101499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.249 -0.249 -0.249] (1.000)
Step: 101999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.248 -0.248 -0.248] (1.000)
Step: 102499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.247 -0.247 -0.247] (1.000)
Step: 102999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.249 -0.249 -0.249] (1.000)
Step: 103499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.248 -0.248 -0.248] (1.000)
Step: 103999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.246 -0.246 -0.246] (1.000)
Step: 104499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.251 -0.251 -0.251 -0.249 -0.249 -0.249] (1.000)
Step: 104999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.251 -0.251 -0.251] (1.000)
Step: 105499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25 -0.25 -0.25 -0.25 -0.25 -0.25] (1.000)
Step: 105999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.25  -0.25  -0.25 ] (1.000)
Step: 106499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.251 -0.251 -0.251 -0.253 -0.253 -0.253] (1.000)
Step: 106999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.251 -0.251 -0.251] (1.000)
Step: 107499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.253 -0.253 -0.253] (1.000)
Step: 107999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.252 -0.252 -0.252] (1.000)
Step: 108499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.251 -0.251 -0.251] (1.000)
Step: 108999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.25  -0.25  -0.25 ] (1.000)
Step: 109499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.248 -0.248 -0.248] (1.000)
Step: 109999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.244 -0.244 -0.244 -0.251 -0.251 -0.251] (1.000)
Step: 110499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.247 -0.247 -0.247 -0.251 -0.251 -0.251] (1.000)
Step: 110999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.25  -0.25  -0.25 ] (1.000)
Step: 111499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.244 -0.244 -0.244 -0.253 -0.253 -0.253] (1.000)
Step: 111999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.252 -0.252 -0.252] (1.000)
Step: 112499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.246 -0.246 -0.246 -0.251 -0.251 -0.251] (1.000)
Step: 112999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.25  -0.25  -0.25 ] (1.000)
Step: 113499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.247 -0.247 -0.247 -0.25  -0.25  -0.25 ] (1.000)
Step: 113999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.25  -0.25  -0.25 ] (1.000)
Step: 114499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.25  -0.25  -0.25  -0.252 -0.252 -0.252] (1.000)
Step: 114999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.25  -0.25  -0.25  -0.253 -0.253 -0.253] (1.000)
Step: 115499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.249 -0.249 -0.249 -0.256 -0.256 -0.256] (1.000)
Step: 115999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.255 -0.255 -0.255] (1.000)
Step: 116499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.256 -0.256 -0.256] (1.000)
Step: 116999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.256 -0.256 -0.256] (1.000)
Step: 117499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.255 -0.255 -0.255] (1.000)
Step: 117999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.251 -0.251 -0.251 -0.254 -0.254 -0.254] (1.000)
Step: 118499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.253 -0.253 -0.253] (1.000)
Step: 118999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.25  -0.25  -0.25  -0.255 -0.255 -0.255] (1.000)
Step: 119499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.254 -0.254 -0.254] (1.000)
Step: 119999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.25  -0.25  -0.25  -0.253 -0.253 -0.253] (1.000)
Step: 120499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.253 -0.253 -0.253] (1.000)
Step: 120999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.253 -0.253 -0.253] (1.000)
Step: 121499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.254 -0.254 -0.254] (1.000)
Step: 121999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.253 -0.253 -0.253] (1.000)
Step: 122499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.252 -0.252 -0.252] (1.000)
Step: 122999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.251 -0.251 -0.251] (1.000)
Step: 123499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.251 -0.251 -0.251] (1.000)
Step: 123999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25 -0.25 -0.25 -0.25 -0.25 -0.25] (1.000)
Step: 124499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.252 -0.252 -0.252 -0.252 -0.252 -0.252] (1.000)
Step: 124999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.253 -0.253 -0.253 -0.253 -0.253 -0.253] (1.000)
Step: 125499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.252 -0.252 -0.252] (1.000)
Step: 125999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.252 -0.252 -0.252] (1.000)
Step: 126499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.251 -0.251 -0.251] (1.000)
Step: 126999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.25  -0.25  -0.25 ] (1.000)
Step: 127499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.251 -0.251 -0.251] (1.000)
Step: 127999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.251 -0.251 -0.251] (1.000)
Step: 128499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.252 -0.252 -0.252] (1.000)
Step: 128999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.252 -0.252 -0.252] (1.000)
Step: 129499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.252 -0.252 -0.252] (1.000)
Step: 129999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.254 -0.254 -0.254] (1.000)
Step: 130499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.254 -0.254 -0.254] (1.000)
Step: 130999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.253 -0.253 -0.253] (1.000)
Step: 131499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.252 -0.252 -0.252] (1.000)
Step: 131999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.251 -0.251 -0.251] (1.000)
Step: 132499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.251 -0.251 -0.251] (1.000)
Step: 132999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.251 -0.251 -0.251] (1.000)
Step: 133499, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [1.0000], Avg: [-0.26  -0.26  -0.26  -0.251 -0.251 -0.251] (1.000)
Step: 133999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.252 -0.252 -0.252] (1.000)
Step: 134499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.252 -0.252 -0.252] (1.000)
Step: 134999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.26  -0.26  -0.26  -0.254 -0.254 -0.254] (1.000)
Step: 135499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [1.0000], Avg: [-0.26  -0.26  -0.26  -0.258 -0.258 -0.258] (1.000)
Step: 135999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.258 -0.258 -0.258] (1.000)
Step: 136499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.259 -0.259 -0.259] (1.000)
Step: 136999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.259 -0.259 -0.259] (1.000)
Step: 137499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.258 -0.258 -0.258] (1.000)
Step: 137999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.26  -0.26  -0.26  -0.259 -0.259 -0.259] (1.000)
Step: 138499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.258 -0.258 -0.258] (1.000)
Step: 138999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.258 -0.258 -0.258] (1.000)
Step: 139499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.262 -0.262 -0.262 -0.259 -0.259 -0.259] (1.000)
Step: 139999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.26  -0.26  -0.26 ] (1.000)
Step: 140499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.259 -0.259 -0.259] (1.000)
Step: 140999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.259 -0.259 -0.259] (1.000)
Step: 141499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.258 -0.258 -0.258] (1.000)
Step: 141999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.257 -0.257 -0.257] (1.000)
Step: 142499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.256 -0.256 -0.256] (1.000)
Step: 142999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.256 -0.256 -0.256] (1.000)
Step: 143499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.257 -0.257 -0.257] (1.000)
Step: 143999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.257 -0.257 -0.257] (1.000)
Step: 144499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.256 -0.256 -0.256] (1.000)
Step: 144999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.258 -0.258 -0.258] (1.000)
Step: 145499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.257 -0.257 -0.257] (1.000)
Step: 145999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.257 -0.257 -0.257] (1.000)
Step: 146499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.256 -0.256 -0.256] (1.000)
Step: 146999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.259 -0.259 -0.259 -0.258 -0.258 -0.258] (1.000)
Step: 147499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.259 -0.259 -0.259] (1.000)
Step: 147999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.259 -0.259 -0.259] (1.000)
Step: 148499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.258 -0.258 -0.258] (1.000)
Step: 148999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.259 -0.259 -0.259] (1.000)
Step: 149499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.258 -0.258 -0.258] (1.000)
Step: 149999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.259 -0.259 -0.259] (1.000)
Step: 150499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.258 -0.258 -0.258] (1.000)
Step: 150999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.26  -0.26  -0.26 ] (1.000)
Step: 151499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.259 -0.259 -0.259] (1.000)
Step: 151999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.259 -0.259 -0.259] (1.000)
Step: 152499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.26  -0.26  -0.26 ] (1.000)
Step: 152999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.262 -0.262 -0.262] (1.000)
Step: 153499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.261 -0.261 -0.261] (1.000)
Step: 153999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.262 -0.262 -0.262] (1.000)
Step: 154499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.259 -0.259 -0.259 -0.263 -0.263 -0.263] (1.000)
Step: 154999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.264 -0.264 -0.264] (1.000)
Step: 155499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.264 -0.264 -0.264] (1.000)
Step: 155999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.265 -0.265 -0.265] (1.000)
Step: 156499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.264 -0.264 -0.264] (1.000)
Step: 156999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.265 -0.265 -0.265] (1.000)
Step: 157499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.264 -0.264 -0.264] (1.000)
Step: 157999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.263 -0.263 -0.263] (1.000)
Step: 158499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 158999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 159499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.256 -0.256 -0.256 -0.265 -0.265 -0.265] (1.000)
Step: 159999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 160499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.264 -0.264 -0.264] (1.000)
Step: 160999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 161499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.258 -0.258 -0.258 -0.263 -0.263 -0.263] (1.000)
Step: 161999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.263 -0.263 -0.263] (1.000)
Step: 162499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.262 -0.262 -0.262] (1.000)
Step: 162999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.261 -0.261 -0.261] (1.000)
Step: 163499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.261 -0.261 -0.261 -0.262 -0.262 -0.262] (1.000)
Step: 163999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.261 -0.261 -0.261] (1.000)
Step: 164499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.262 -0.262 -0.262] (1.000)
Step: 164999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.262 -0.262 -0.262] (1.000)
Step: 165499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.261 -0.261 -0.261] (1.000)
Step: 165999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.261 -0.261 -0.261] (1.000)
Step: 166499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.262 -0.262 -0.262 -0.262 -0.262 -0.262] (1.000)
Step: 166999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.263 -0.263 -0.263] (1.000)
Step: 167499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.264 -0.264 -0.264] (1.000)
Step: 167999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.263 -0.263 -0.263] (1.000)
Step: 168499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.26  -0.26  -0.26  -0.265 -0.265 -0.265] (1.000)
Step: 168999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.264 -0.264 -0.264] (1.000)
Step: 169499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.264 -0.264 -0.264] (1.000)
Step: 169999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.262 -0.262 -0.262 -0.264 -0.264 -0.264] (1.000)
Step: 170499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.263 -0.263 -0.263] (1.000)
Step: 170999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.263 -0.263 -0.263 -0.265 -0.265 -0.265] (1.000)
Step: 171499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.266 -0.266 -0.266] (1.000)
Step: 171999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.265 -0.265 -0.265] (1.000)
Step: 172499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.265 -0.265 -0.265] (1.000)
Step: 172999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.264 -0.264 -0.264] (1.000)
Step: 173499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.263 -0.263 -0.263] (1.000)
Step: 173999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.262 -0.262 -0.262] (1.000)
Step: 174499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.262 -0.262 -0.262] (1.000)
Step: 174999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.261 -0.261 -0.261] (1.000)
Step: 175499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.26  -0.26  -0.26  -0.263 -0.263 -0.263] (1.000)
Step: 175999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.262 -0.262 -0.262] (1.000)
Step: 176499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.263 -0.263 -0.263] (1.000)
Step: 176999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.262 -0.262 -0.262] (1.000)
Step: 177499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [1.0000], Avg: [-0.258 -0.258 -0.258 -0.265 -0.265 -0.265] (1.000)
Step: 177999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.259 -0.259 -0.259 -0.266 -0.266 -0.266] (1.000)
Step: 178499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.266 -0.266 -0.266] (1.000)
Step: 178999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.265 -0.265 -0.265] (1.000)
Step: 179499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.265 -0.265 -0.265] (1.000)
Step: 179999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.264 -0.264 -0.264] (1.000)
Step: 180499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.263 -0.263 -0.263] (1.000)
Step: 180999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.262 -0.262 -0.262] (1.000)
Step: 181499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.263 -0.263 -0.263] (1.000)
Step: 181999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.263 -0.263 -0.263] (1.000)
Step: 182499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.263 -0.263 -0.263] (1.000)
Step: 182999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 183499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.262 -0.262 -0.262] (1.000)
Step: 183999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.263 -0.263 -0.263] (1.000)
Step: 184499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.262 -0.262 -0.262] (1.000)
Step: 184999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.262 -0.262 -0.262] (1.000)
Step: 185499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.252 -0.252 -0.252 -0.263 -0.263 -0.263] (1.000)
Step: 185999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.263 -0.263 -0.263] (1.000)
Step: 186499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 186999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 187499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.264 -0.264 -0.264] (1.000)
Step: 187999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.265 -0.265 -0.265] (1.000)
Step: 188499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 188999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.264 -0.264 -0.264] (1.000)
Step: 189499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 189999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.264 -0.264 -0.264] (1.000)
Step: 190499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.264 -0.264 -0.264] (1.000)
Step: 190999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 191499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.255 -0.255 -0.255 -0.265 -0.265 -0.265] (1.000)
Step: 191999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.256 -0.256 -0.256 -0.266 -0.266 -0.266] (1.000)
Step: 192499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.265 -0.265 -0.265] (1.000)
Step: 192999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 193499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.264 -0.264 -0.264] (1.000)
Step: 193999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 194499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.265 -0.265 -0.265] (1.000)
Step: 194999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.266 -0.266 -0.266] (1.000)
Step: 195499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.266 -0.266 -0.266] (1.000)
Step: 195999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.266 -0.266 -0.266] (1.000)
Step: 196499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.265 -0.265 -0.265] (1.000)
Step: 196999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.265 -0.265 -0.265] (1.000)
Step: 197499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 197999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.266 -0.266 -0.266] (1.000)
Step: 198499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 198999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 199499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.264 -0.264 -0.264] (1.000)
Step: 199999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.264 -0.264 -0.264] (1.000)
Step: 200499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.265 -0.265 -0.265] (1.000)
Step: 200999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.265 -0.265 -0.265] (1.000)
Step: 201499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.264 -0.264 -0.264] (1.000)
Step: 201999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.264 -0.264 -0.264] (1.000)
Step: 202499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.263 -0.263 -0.263] (1.000)
Step: 202999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.253 -0.253 -0.253 -0.262 -0.262 -0.262] (1.000)
Step: 203499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.262 -0.262 -0.262] (1.000)
Step: 203999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.253 -0.253 -0.253 -0.263 -0.263 -0.263] (1.000)
Step: 204499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.264 -0.264 -0.264] (1.000)
Step: 204999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.264 -0.264 -0.264] (1.000)
Step: 205499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.264 -0.264 -0.264] (1.000)
Step: 205999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.263 -0.263 -0.263] (1.000)
Step: 206499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.263 -0.263 -0.263] (1.000)
Step: 206999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.262 -0.262 -0.262] (1.000)
Step: 207499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.261 -0.261 -0.261] (1.000)
Step: 207999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.261 -0.261 -0.261] (1.000)
Step: 208499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.252 -0.252 -0.252 -0.262 -0.262 -0.262] (1.000)
Step: 208999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.262 -0.262 -0.262] (1.000)
Step: 209499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.261 -0.261 -0.261] (1.000)
Step: 209999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.262 -0.262 -0.262] (1.000)
Step: 210499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.253 -0.253 -0.253 -0.263 -0.263 -0.263] (1.000)
Step: 210999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.262 -0.262 -0.262] (1.000)
Step: 211499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.262 -0.262 -0.262] (1.000)
Step: 211999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.262 -0.262 -0.262] (1.000)
Step: 212499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.262 -0.262 -0.262] (1.000)
Step: 212999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.262 -0.262 -0.262] (1.000)
Step: 213499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.262 -0.262 -0.262] (1.000)
Step: 213999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 214499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 214999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.263 -0.263 -0.263] (1.000)
Step: 215499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 215999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.263 -0.263 -0.263] (1.000)
Step: 216499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.264 -0.264 -0.264] (1.000)
Step: 216999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.264 -0.264 -0.264] (1.000)
Step: 217499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.256 -0.256 -0.256 -0.265 -0.265 -0.265] (1.000)
Step: 217999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.266 -0.266 -0.266] (1.000)
Step: 218499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.257 -0.257 -0.257 -0.265 -0.265 -0.265] (1.000)
Step: 218999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.257 -0.257 -0.257 -0.267 -0.267 -0.267] (1.000)
Step: 219499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.267 -0.267 -0.267] (1.000)
Step: 219999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.266 -0.266 -0.266] (1.000)
Step: 220499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [1.0000], Avg: [-0.258 -0.258 -0.258 -0.267 -0.267 -0.267] (1.000)
Step: 220999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.259 -0.259 -0.259 -0.268 -0.268 -0.268] (1.000)
Step: 221499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.268 -0.268 -0.268] (1.000)
Step: 221999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.267 -0.267 -0.267] (1.000)
Step: 222499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.26  -0.26  -0.26  -0.268 -0.268 -0.268] (1.000)
Step: 222999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.268 -0.268 -0.268] (1.000)
Step: 223499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.262 -0.262 -0.262 -0.269 -0.269 -0.269] (1.000)
Step: 223999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.269 -0.269 -0.269] (1.000)
Step: 224499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.268 -0.268 -0.268] (1.000)
Step: 224999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.268 -0.268 -0.268] (1.000)
Step: 225499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.269 -0.269 -0.269] (1.000)
Step: 225999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.268 -0.268 -0.268] (1.000)
Step: 226499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.268 -0.268 -0.268] (1.000)
Step: 226999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.267 -0.267 -0.267] (1.000)
Step: 227499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.267 -0.267 -0.267] (1.000)
Step: 227999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.267 -0.267 -0.267] (1.000)
Step: 228499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.267 -0.267 -0.267] (1.000)
Step: 228999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.267 -0.267 -0.267] (1.000)
Step: 229499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.267 -0.267 -0.267] (1.000)
Step: 229999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.268 -0.268 -0.268] (1.000)
Step: 230499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.268 -0.268 -0.268] (1.000)
Step: 230999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [1.0954], Avg: [-0.264 -0.264 -0.264 -0.269 -0.269 -0.269] (1.000)
Step: 231499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.265 -0.265 -0.265 -0.27  -0.27  -0.27 ] (1.000)
Step: 231999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.27  -0.27  -0.27 ] (1.000)
Step: 232499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.27  -0.27  -0.27 ] (1.000)
Step: 232999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.271 -0.271 -0.271] (1.000)
Step: 233499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.27  -0.27  -0.27 ] (1.000)
Step: 233999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.271 -0.271 -0.271] (1.000)
Step: 234499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.27  -0.27  -0.27 ] (1.000)
Step: 234999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.266 -0.266 -0.266 -0.272 -0.272 -0.272] (1.000)
Step: 235499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.273 -0.273 -0.273] (1.000)
Step: 235999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.274 -0.274 -0.274] (1.000)
Step: 236499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.268 -0.268 -0.268 -0.273 -0.273 -0.273] (1.000)
Step: 236999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 237499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.268 -0.268 -0.268 -0.273 -0.273 -0.273] (1.000)
Step: 237999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.272 -0.272 -0.272] (1.000)
Step: 238499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 238999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.267 -0.267 -0.267 -0.275 -0.275 -0.275] (1.000)
Step: 239499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.274 -0.274 -0.274] (1.000)
Step: 239999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.274 -0.274 -0.274] (1.000)
Step: 240499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 240999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.267 -0.267 -0.267 -0.275 -0.275 -0.275] (1.000)
Step: 241499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.274 -0.274 -0.274] (1.000)
Step: 241999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.273 -0.273 -0.273] (1.000)
Step: 242499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 242999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 243499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 243999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 244499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.273 -0.273 -0.273] (1.000)
Step: 244999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 245499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.272 -0.272 -0.272] (1.000)
Step: 245999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.271 -0.271 -0.271] (1.000)
Step: 246499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.271 -0.271 -0.271] (1.000)
Step: 246999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [1.0000], Avg: [-0.268 -0.268 -0.268 -0.273 -0.273 -0.273] (1.000)
Step: 247499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 247999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.273 -0.273 -0.273] (1.000)
Step: 248499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.267 -0.267 -0.267 -0.275 -0.275 -0.275] (1.000)
Step: 248999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.275 -0.275 -0.275] (1.000)
Step: 249499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.268 -0.268 -0.268 -0.276 -0.276 -0.276] (1.000)
Step: 249999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.275 -0.275 -0.275] (1.000)
Step: 250499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.269 -0.269 -0.269 -0.276 -0.276 -0.276] (1.000)
Step: 250999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.27  -0.27  -0.27  -0.276 -0.276 -0.276] (1.000)
Step: 251499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.271 -0.271 -0.271 -0.277 -0.277 -0.277] (1.000)
Step: 251999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.271 -0.271 -0.271 -0.278 -0.278 -0.278] (1.000)
Step: 252499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.272 -0.272 -0.272 -0.278 -0.278 -0.278] (1.000)
Step: 252999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.272 -0.272 -0.272 -0.278 -0.278 -0.278] (1.000)
Step: 253499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.273 -0.273 -0.273 -0.279 -0.279 -0.279] (1.000)
Step: 253999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.278 -0.278 -0.278] (1.000)
Step: 254499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.278 -0.278 -0.278] (1.000)
Step: 254999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.272 -0.272 -0.272 -0.278 -0.278 -0.278] (1.000)
Step: 255499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.273 -0.273 -0.273 -0.278 -0.278 -0.278] (1.000)
Step: 255999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.274 -0.274 -0.274 -0.278 -0.278 -0.278] (1.000)
Step: 256499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.274 -0.274 -0.274 -0.278 -0.278 -0.278] (1.000)
Step: 256999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.274 -0.274 -0.274 -0.279 -0.279 -0.279] (1.000)
Step: 257499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.274 -0.274 -0.274 -0.279 -0.279 -0.279] (1.000)
Step: 257999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.279 -0.279 -0.279] (1.000)
Step: 258499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.279 -0.279 -0.279] (1.000)
Step: 258999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.28  -0.28  -0.28 ] (1.000)
Step: 259499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.278 -0.278 -0.278 -0.279 -0.279 -0.279] (1.000)
Step: 259999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.28  -0.28  -0.28 ] (1.000)
Step: 260499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.28  -0.28  -0.28 ] (1.000)
Step: 260999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.28  -0.28  -0.28 ] (1.000)
Step: 261499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.281 -0.281 -0.281] (1.000)
Step: 261999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.28  -0.28  -0.28 ] (1.000)
Step: 262499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.281 -0.281 -0.281] (1.000)
Step: 262999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.28  -0.28  -0.28 ] (1.000)
Step: 263499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.281 -0.281 -0.281] (1.000)
Step: 263999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.28  -0.28  -0.28 ] (1.000)
Step: 264499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.28  -0.28  -0.28 ] (1.000)
Step: 264999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.281 -0.281 -0.281] (1.000)
Step: 265499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.281 -0.281 -0.281] (1.000)
Step: 265999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.28  -0.28  -0.28 ] (1.000)
Step: 266499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.281 -0.281 -0.281] (1.000)
Step: 266999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.281 -0.281 -0.281] (1.000)
Step: 267499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.281 -0.281 -0.281] (1.000)
Step: 267999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.28  -0.28  -0.28 ] (1.000)
Step: 268499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.28  -0.28  -0.28 ] (1.000)
Step: 268999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.28  -0.28  -0.28 ] (1.000)
Step: 269499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.281 -0.281 -0.281] (1.000)
Step: 269999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.282 -0.282 -0.282] (1.000)
Step: 270499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.278 -0.278 -0.278 -0.281 -0.281 -0.281] (1.000)
Step: 270999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.282 -0.282 -0.282] (1.000)
Step: 271499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [1.0000], Avg: [-0.277 -0.277 -0.277 -0.284 -0.284 -0.284] (1.000)
Step: 271999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.283 -0.283 -0.283] (1.000)
Step: 272499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.283 -0.283 -0.283] (1.000)
Step: 272999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.284 -0.284 -0.284] (1.000)
Step: 273499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.283 -0.283 -0.283] (1.000)
Step: 273999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.284 -0.284 -0.284] (1.000)
Step: 274499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.283 -0.283 -0.283] (1.000)
Step: 274999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.283 -0.283 -0.283] (1.000)
Step: 275499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.282 -0.282 -0.282] (1.000)
Step: 275999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.282 -0.282 -0.282] (1.000)
Step: 276499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.274 -0.274 -0.274 -0.282 -0.282 -0.282] (1.000)
Step: 276999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.274 -0.274 -0.274 -0.284 -0.284 -0.284] (1.000)
Step: 277499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.284 -0.284 -0.284] (1.000)
Step: 277999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.284 -0.284 -0.284] (1.000)
Step: 278499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.283 -0.283 -0.283] (1.000)
Step: 278999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.283 -0.283 -0.283] (1.000)
Step: 279499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.284 -0.284 -0.284] (1.000)
Step: 279999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.284 -0.284 -0.284] (1.000)
Step: 280499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.274 -0.274 -0.274 -0.284 -0.284 -0.284] (1.000)
Step: 280999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.284 -0.284 -0.284] (1.000)
Step: 281499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.285 -0.285 -0.285] (1.000)
Step: 281999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.285 -0.285 -0.285] (1.000)
Step: 282499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.285 -0.285 -0.285] (1.000)
Step: 282999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.284 -0.284 -0.284] (1.000)
Step: 283499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.284 -0.284 -0.284] (1.000)
Step: 283999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.285 -0.285 -0.285] (1.000)
Step: 284499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.285 -0.285 -0.285] (1.000)
Step: 284999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.286 -0.286 -0.286] (1.000)
Step: 285499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [1.0000], Avg: [-0.278 -0.278 -0.278 -0.287 -0.287 -0.287] (1.000)
Step: 285999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.286 -0.286 -0.286] (1.000)
Step: 286499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.278 -0.278 -0.278 -0.287 -0.287 -0.287] (1.000)
Step: 286999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.287 -0.287 -0.287] (1.000)
Step: 287499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.287 -0.287 -0.287] (1.000)
Step: 287999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.286 -0.286 -0.286] (1.000)
Step: 288499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.277 -0.277 -0.277 -0.287 -0.287 -0.287] (1.000)
Step: 288999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.278 -0.278 -0.278 -0.288 -0.288 -0.288] (1.000)
Step: 289499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.288 -0.288 -0.288] (1.000)
Step: 289999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.278 -0.278 -0.278 -0.289 -0.289 -0.289] (1.000)
Step: 290499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.279 -0.279 -0.279 -0.289 -0.289 -0.289] (1.000)
Step: 290999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 291499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 291999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 292499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28 -0.28 -0.28 -0.29 -0.29 -0.29] (1.000)
Step: 292999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.29  -0.29  -0.29 ] (1.000)
Step: 293499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.29  -0.29  -0.29 ] (1.000)
Step: 293999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28 -0.28 -0.28 -0.29 -0.29 -0.29] (1.000)
Step: 294499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.28  -0.28  -0.28  -0.291 -0.291 -0.291] (1.000)
Step: 294999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.29  -0.29  -0.29 ] (1.000)
Step: 295499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.29  -0.29  -0.29 ] (1.000)
Step: 295999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.279 -0.279 -0.279 -0.289 -0.289 -0.289] (1.000)
Step: 296499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.289 -0.289 -0.289] (1.000)
Step: 296999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.288 -0.288 -0.288] (1.000)
Step: 297499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.289 -0.289 -0.289] (1.000)
Step: 297999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 298499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 298999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 299499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.289 -0.289 -0.289] (1.000)
Step: 299999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28 -0.28 -0.28 -0.29 -0.29 -0.29] (1.000)
Step: 300499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.28 -0.28 -0.28 -0.29 -0.29 -0.29] (1.000)
Step: 300999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.291 -0.291 -0.291] (1.000)
Step: 301499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.279 -0.279 -0.279 -0.292 -0.292 -0.292] (1.000)
Step: 301999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.292 -0.292 -0.292] (1.000)
Step: 302499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.279 -0.279 -0.279 -0.292 -0.292 -0.292] (1.000)
Step: 302999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.28  -0.28  -0.28  -0.292 -0.292 -0.292] (1.000)
Step: 303499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.292 -0.292 -0.292] (1.000)
Step: 303999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.293 -0.293 -0.293] (1.000)
Step: 304499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.281 -0.281 -0.281 -0.293 -0.293 -0.293] (1.000)
Step: 304999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.281 -0.281 -0.281 -0.294 -0.294 -0.294] (1.000)
Step: 305499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.282 -0.282 -0.282 -0.294 -0.294 -0.294] (1.000)
Step: 305999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.294 -0.294 -0.294] (1.000)
Step: 306499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.294 -0.294 -0.294] (1.000)
Step: 306999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.294 -0.294 -0.294] (1.000)
Step: 307499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.294 -0.294 -0.294] (1.000)
Step: 307999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.293 -0.293 -0.293] (1.000)
Step: 308499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.293 -0.293 -0.293] (1.000)
Step: 308999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.294 -0.294 -0.294] (1.000)
Step: 309499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.294 -0.294 -0.294] (1.000)
Step: 309999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.294 -0.294 -0.294] (1.000)
Step: 310499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.282 -0.282 -0.282 -0.295 -0.295 -0.295] (1.000)
Step: 310999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.295 -0.295 -0.295] (1.000)
Step: 311499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.284 -0.284 -0.284 -0.295 -0.295 -0.295] (1.000)
Step: 311999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.294 -0.294 -0.294] (1.000)
Step: 312499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.294 -0.294 -0.294] (1.000)
Step: 312999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.294 -0.294 -0.294] (1.000)
Step: 313499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.295 -0.295 -0.295] (1.000)
Step: 313999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.284 -0.284 -0.284 -0.295 -0.295 -0.295] (1.000)
Step: 314499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.294 -0.294 -0.294] (1.000)
Step: 314999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.294 -0.294 -0.294] (1.000)
Step: 315499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.294 -0.294 -0.294] (1.000)
Step: 315999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.293 -0.293 -0.293] (1.000)
Step: 316499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.285 -0.285 -0.285 -0.294 -0.294 -0.294] (1.000)
Step: 316999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.294 -0.294 -0.294] (1.000)
Step: 317499, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 317999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 318499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 318999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.295 -0.295 -0.295] (1.000)
Step: 319499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.296 -0.296 -0.296] (1.000)
Step: 319999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.283 -0.283 -0.283 -0.297 -0.297 -0.297] (1.000)
Step: 320499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.296 -0.296 -0.296] (1.000)
Step: 320999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.283 -0.283 -0.283 -0.297 -0.297 -0.297] (1.000)
Step: 321499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.296 -0.296 -0.296] (1.000)
Step: 321999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.296 -0.296 -0.296] (1.000)
Step: 322499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 322999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.295 -0.295 -0.295] (1.000)
Step: 323499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 323999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.296 -0.296 -0.296] (1.000)
Step: 324499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.284 -0.284 -0.284 -0.297 -0.297 -0.297] (1.000)
Step: 324999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.297 -0.297 -0.297] (1.000)
Step: 325499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.297 -0.297 -0.297] (1.000)
Step: 325999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.297 -0.297 -0.297] (1.000)
Step: 326499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 326999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 327499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.283 -0.283 -0.283 -0.296 -0.296 -0.296] (1.000)
Step: 327999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.296 -0.296 -0.296] (1.000)
Step: 328499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.285 -0.285 -0.285 -0.296 -0.296 -0.296] (1.000)
Step: 328999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.296 -0.296 -0.296] (1.000)
Step: 329499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.296 -0.296 -0.296] (1.000)
Step: 329999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.286 -0.286 -0.286 -0.297 -0.297 -0.297] (1.000)
Step: 330499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 330999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.297 -0.297 -0.297] (1.000)
Step: 331499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 331999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 332499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.298 -0.298 -0.298] (1.000)
Step: 332999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 333499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.285 -0.285 -0.285 -0.299 -0.299 -0.299] (1.000)
Step: 333999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 334499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.286 -0.286 -0.286 -0.299 -0.299 -0.299] (1.000)
Step: 334999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 335499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.298 -0.298 -0.298] (1.000)
Step: 335999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 336499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 336999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 337499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.287 -0.287 -0.287 -0.297 -0.297 -0.297] (1.000)
Step: 337999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.297 -0.297 -0.297] (1.000)
Step: 338499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.286 -0.286 -0.286 -0.297 -0.297 -0.297] (1.000)
Step: 338999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 339499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.286 -0.286 -0.286 -0.299 -0.299 -0.299] (1.000)
Step: 339999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.298 -0.298 -0.298] (1.000)
Step: 340499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.286 -0.286 -0.286 -0.299 -0.299 -0.299] (1.000)
Step: 340999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.287 -0.287 -0.287 -0.299 -0.299 -0.299] (1.000)
Step: 341499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.287 -0.287 -0.287 -0.298 -0.298 -0.298] (1.000)
Step: 341999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.287 -0.287 -0.287 -0.299 -0.299 -0.299] (1.000)
Step: 342499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.299 -0.299 -0.299] (1.000)
Step: 342999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.286 -0.286 -0.286 -0.299 -0.299 -0.299] (1.000)
Step: 343499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.287 -0.287 -0.287 -0.3   -0.3   -0.3  ] (1.000)
Step: 343999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.288 -0.288 -0.288 -0.299 -0.299 -0.299] (1.000)
Step: 344499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.288 -0.288 -0.288 -0.3   -0.3   -0.3  ] (1.000)
Step: 344999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.288 -0.288 -0.288 -0.3   -0.3   -0.3  ] (1.000)
Step: 345499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.3   -0.3   -0.3  ] (1.000)
Step: 345999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.288 -0.288 -0.288 -0.301 -0.301 -0.301] (1.000)
Step: 346499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.288 -0.288 -0.288 -0.302 -0.302 -0.302] (1.000)
Step: 346999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 347499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.301 -0.301 -0.301] (1.000)
Step: 347999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 348499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 348999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 349499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 349999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 350499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 350999, Reward: [ 0.6  0.6  0.6 -0.6 -0.6 -0.6] [0.7746], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 351499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 351999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.302 -0.302 -0.302] (1.000)
Step: 352499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 352999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 353499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 353999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29 -0.29 -0.29 -0.3  -0.3  -0.3 ] (1.000)
Step: 354499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.3   -0.3   -0.3  ] (1.000)
Step: 354999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29 -0.29 -0.29 -0.3  -0.3  -0.3 ] (1.000)
Step: 355499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [1.0000], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 355999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 356499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 356999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.29  -0.29  -0.29  -0.302 -0.302 -0.302] (1.000)
Step: 357499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 357999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 358499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.289 -0.289 -0.289 -0.303 -0.303 -0.303] (1.000)
Step: 358999, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [1.0000], Avg: [-0.291 -0.291 -0.291 -0.303 -0.303 -0.303] (1.000)
Step: 359499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.303 -0.303 -0.303] (1.000)
Step: 359999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.303 -0.303 -0.303] (1.000)
Step: 360499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.291 -0.291 -0.291 -0.304 -0.304 -0.304] (1.000)
Step: 360999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.304 -0.304 -0.304] (1.000)
Step: 361499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.304 -0.304 -0.304] (1.000)
Step: 361999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.303 -0.303 -0.303] (1.000)
Step: 362499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.303 -0.303 -0.303] (1.000)
Step: 362999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.303 -0.303 -0.303] (1.000)
Step: 363499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.29  -0.29  -0.29  -0.303 -0.303 -0.303] (1.000)
Step: 363999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.303 -0.303 -0.303] (1.000)
Step: 364499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.29  -0.29  -0.29  -0.303 -0.303 -0.303] (1.000)
Step: 364999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.303 -0.303 -0.303] (1.000)
Step: 365499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 365999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 366499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 366999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 367499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 367999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.302 -0.302 -0.302] (1.000)
Step: 368499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 368999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 369499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 369999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.301 -0.301 -0.301] (1.000)
Step: 370499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.289 -0.289 -0.289 -0.302 -0.302 -0.302] (1.000)
Step: 370999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.301 -0.301 -0.301] (1.000)
Step: 371499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.289 -0.289 -0.289 -0.301 -0.301 -0.301] (1.000)
Step: 371999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 372499, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [1.0000], Avg: [-0.291 -0.291 -0.291 -0.301 -0.301 -0.301] (1.000)
Step: 372999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.292 -0.292 -0.292 -0.301 -0.301 -0.301] (1.000)
Step: 373499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.302 -0.302 -0.302] (1.000)
Step: 373999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.291 -0.291 -0.291 -0.302 -0.302 -0.302] (1.000)
Step: 374499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.302 -0.302 -0.302] (1.000)
Step: 374999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.302 -0.302 -0.302] (1.000)
Step: 375499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.301 -0.301 -0.301] (1.000)
Step: 375999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 376499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 376999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.29  -0.29  -0.29  -0.301 -0.301 -0.301] (1.000)
Step: 377499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29 -0.29 -0.29 -0.3  -0.3  -0.3 ] (1.000)
Step: 377999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [1.0000], Avg: [-0.291 -0.291 -0.291 -0.301 -0.301 -0.301] (1.000)
Step: 378499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.292 -0.292 -0.292 -0.301 -0.301 -0.301] (1.000)
Step: 378999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.3   -0.3   -0.3  ] (1.000)
Step: 379499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.292 -0.292 -0.292 -0.3   -0.3   -0.3  ] (1.000)
Step: 379999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.292 -0.292 -0.292 -0.3   -0.3   -0.3  ] (1.000)
Step: 380499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.3   -0.3   -0.3  ] (1.000)
Step: 380999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.292 -0.292 -0.292 -0.299 -0.299 -0.299] (1.000)
Step: 381499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [1.0000], Avg: [-0.293 -0.293 -0.293 -0.301 -0.301 -0.301] (1.000)
Step: 381999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.294 -0.294 -0.294 -0.3   -0.3   -0.3  ] (1.000)
Step: 382499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.3   -0.3   -0.3  ] (1.000)
Step: 382999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.294 -0.294 -0.294 -0.3   -0.3   -0.3  ] (1.000)
Step: 383499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.294 -0.294 -0.294 -0.301 -0.301 -0.301] (1.000)
Step: 383999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.294 -0.294 -0.294 -0.301 -0.301 -0.301] (1.000)
Step: 384499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.294 -0.294 -0.294 -0.302 -0.302 -0.302] (1.000)
Step: 384999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.294 -0.294 -0.294 -0.302 -0.302 -0.302] (1.000)
Step: 385499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.294 -0.294 -0.294 -0.303 -0.303 -0.303] (1.000)
Step: 385999, Reward: [-0.6 -0.6 -0.6  0.6  0.6  0.6] [0.7746], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 386499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.295 -0.295 -0.295 -0.303 -0.303 -0.303] (1.000)
Step: 386999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 387499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 387999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 388499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.301 -0.301 -0.301] (1.000)
Step: 388999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.301 -0.301 -0.301] (1.000)
Step: 389499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.301 -0.301 -0.301] (1.000)
Step: 389999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.294 -0.294 -0.294 -0.301 -0.301 -0.301] (1.000)
Step: 390499, Reward: [0. 0. 0. 0. 0. 0.] [1.2649], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 390999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.303 -0.303 -0.303] (1.000)
Step: 391499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 391999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 392499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 392999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.302 -0.302 -0.302] (1.000)
Step: 393499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.302 -0.302 -0.302] (1.000)
Step: 393999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 394499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 394999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 395499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 395999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 396499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 396999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 397499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 397999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 398499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 398999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 399499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 399999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.302 -0.302 -0.302] (1.000)
Step: 400499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 400999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 401499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 401999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 402499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 402999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 403499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.296 -0.296 -0.296 -0.303 -0.303 -0.303] (1.000)
Step: 403999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 404499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.302 -0.302 -0.302] (1.000)
Step: 404999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.302 -0.302 -0.302] (1.000)
Step: 405499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 405999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 406499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 406999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.298 -0.298 -0.298 -0.303 -0.303 -0.303] (1.000)
Step: 407499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 407999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 408499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.305 -0.305 -0.305] (1.000)
Step: 408999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.304 -0.304 -0.304] (1.000)
Step: 409499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.305 -0.305 -0.305] (1.000)
Step: 409999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.305 -0.305 -0.305] (1.000)
Step: 410499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.305 -0.305 -0.305] (1.000)
Step: 410999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.305 -0.305 -0.305] (1.000)
Step: 411499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 411999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.304 -0.304 -0.304] (1.000)
Step: 412499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.298 -0.298 -0.298 -0.304 -0.304 -0.304] (1.000)
Step: 412999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.304 -0.304 -0.304] (1.000)
Step: 413499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 413999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.303 -0.303 -0.303] (1.000)
Step: 414499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [1.0000], Avg: [-0.298 -0.298 -0.298 -0.303 -0.303 -0.303] (1.000)
Step: 414999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.303 -0.303 -0.303] (1.000)
Step: 415499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.304 -0.304 -0.304] (1.000)
Step: 415999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.303 -0.303 -0.303] (1.000)
Step: 416499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.303 -0.303 -0.303] (1.000)
Step: 416999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.3   -0.3   -0.3   -0.303 -0.303 -0.303] (1.000)
Step: 417499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.3   -0.3   -0.3   -0.304 -0.304 -0.304] (1.000)
Step: 417999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.301 -0.301 -0.301 -0.304 -0.304 -0.304] (1.000)
Step: 418499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.304 -0.304 -0.304] (1.000)
Step: 418999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.301 -0.301 -0.301 -0.304 -0.304 -0.304] (1.000)
Step: 419499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.302 -0.302 -0.302 -0.304 -0.304 -0.304] (1.000)
Step: 419999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.301 -0.301 -0.301 -0.304 -0.304 -0.304] (1.000)
Step: 420499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.301 -0.301 -0.301 -0.303 -0.303 -0.303] (1.000)
Step: 420999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.303 -0.303 -0.303] (1.000)
Step: 421499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 421999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 422499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 422999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 423499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 423999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.301 -0.301 -0.301] (1.000)
Step: 424499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.302 -0.302 -0.302] (1.000)
Step: 424999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.302 -0.302 -0.302] (1.000)
Step: 425499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.302 -0.302 -0.302] (1.000)
Step: 425999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.301 -0.301 -0.301 -0.301 -0.301 -0.301] (1.000)
Step: 426499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.301 -0.301 -0.301] (1.000)
Step: 426999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.301 -0.301 -0.301 -0.301 -0.301 -0.301] (1.000)
Step: 427499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3 -0.3 -0.3 -0.3 -0.3 -0.3] (1.000)
Step: 427999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3 -0.3 -0.3 -0.3 -0.3 -0.3] (1.000)
Step: 428499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.301 -0.301 -0.301 -0.3   -0.3   -0.3  ] (1.000)
Step: 428999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.301 -0.301 -0.301] (1.000)
Step: 429499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3 -0.3 -0.3 -0.3 -0.3 -0.3] (1.000)
Step: 429999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.301 -0.301 -0.301] (1.000)
Step: 430499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3 -0.3 -0.3 -0.3 -0.3 -0.3] (1.000)
Step: 430999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.3   -0.3   -0.3  ] (1.000)
Step: 431499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.3   -0.3   -0.3  ] (1.000)
Step: 431999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.299 -0.299 -0.299] (1.000)
Step: 432499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.299 -0.299 -0.299] (1.000)
Step: 432999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.299 -0.299 -0.299] (1.000)
Step: 433499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 433999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 434499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 434999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 435499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 435999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 436499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 436999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.299 -0.299 -0.299] (1.000)
Step: 437499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 437999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.298 -0.298 -0.298] (1.000)
Step: 438499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 438999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.301 -0.301 -0.301 -0.298 -0.298 -0.298] (1.000)
Step: 439499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.301 -0.301 -0.301 -0.299 -0.299 -0.299] (1.000)
Step: 439999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 440499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.299 -0.299 -0.299] (1.000)
Step: 440999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 441499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.298 -0.298 -0.298] (1.000)
Step: 441999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 442499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.298 -0.298 -0.298] (1.000)
Step: 442999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 443499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 443999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 444499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 444999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 445499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 445999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 446499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 446999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 447499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [1.0000], Avg: [-0.3   -0.3   -0.3   -0.298 -0.298 -0.298] (1.000)
Step: 447999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.298 -0.298 -0.298] (1.000)
Step: 448499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.298 -0.298 -0.298] (1.000)
Step: 448999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 449499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 449999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 450499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 450999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 451499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 451999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 452499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 452999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 453499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 453999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 454499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 454999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 455499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 455999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 456499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 456999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 457499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 457999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.297 -0.297 -0.297] (1.000)
Step: 458499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 458999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 459499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 459999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 460499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 460999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.295 -0.295 -0.295] (1.000)
Step: 461499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 461999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.295 -0.295 -0.295] (1.000)
Step: 462499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 462999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [1.0954], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 463499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 463999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 464499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 464999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.295 -0.295 -0.295] (1.000)
Step: 465499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 465999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 466499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 466999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 467499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 467999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 468499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 468999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 469499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 469999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 470499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 470999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 471499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 471999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 472499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 472999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 473499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 473999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 474499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 474999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 475499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 475999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 476499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 476999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.295 -0.295 -0.295] (1.000)
Step: 477499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 477999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 478499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 478999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.297 -0.297 -0.297] (1.000)
Step: 479499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 479999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 480499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 480999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.296 -0.296 -0.296] (1.000)
Step: 481499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 481999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 482499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 482999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 483499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 483999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 484499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 484999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 485499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 485999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 486499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 486999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 487499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 487999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 488499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 488999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 489499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 489999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 490499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 490999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.301 -0.301 -0.301 -0.296 -0.296 -0.296] (1.000)
Step: 491499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 491999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 492499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 492999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.3   -0.3   -0.3   -0.296 -0.296 -0.296] (1.000)
Step: 493499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 493999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 494499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 494999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 495499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.296 -0.296 -0.296] (1.000)
Step: 495999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 496499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 496999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 497499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 497999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.299 -0.299 -0.299 -0.295 -0.295 -0.295] (1.000)
Step: 498499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.294 -0.294 -0.294] (1.000)
Step: 498999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.294 -0.294 -0.294] (1.000)
Step: 499499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.294 -0.294 -0.294] (1.000)
Step: 499999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.294 -0.294 -0.294] (1.000)
Step: 500499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.293 -0.293 -0.293] (1.000)
Step: 500999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.293 -0.293 -0.293] (1.000)
Step: 501499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.293 -0.293 -0.293] (1.000)
Step: 501999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.293 -0.293 -0.293] (1.000)
Step: 502499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.293 -0.293 -0.293] (1.000)
Step: 502999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 503499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 503999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 504499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.299 -0.299 -0.299 -0.292 -0.292 -0.292] (1.000)
Step: 504999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 505499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.291 -0.291 -0.291] (1.000)
Step: 505999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.291 -0.291 -0.291] (1.000)
Step: 506499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.8944], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 506999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.298 -0.298 -0.298 -0.292 -0.292 -0.292] (1.000)
Step: 507499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 507999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 508499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 508999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 509499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 509999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 510499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 510999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.297 -0.297 -0.297 -0.291 -0.291 -0.291] (1.000)
Step: 511499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.29  -0.29  -0.29 ] (1.000)
Step: 511999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.297 -0.297 -0.297 -0.29  -0.29  -0.29 ] (1.000)
Step: 512499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.29  -0.29  -0.29 ] (1.000)
Step: 512999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.289 -0.289 -0.289] (1.000)
Step: 513499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.289 -0.289 -0.289] (1.000)
Step: 513999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.289 -0.289 -0.289] (1.000)
Step: 514499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.289 -0.289 -0.289] (1.000)
Step: 514999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.289 -0.289 -0.289] (1.000)
Step: 515499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.288 -0.288 -0.288] (1.000)
Step: 515999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.288 -0.288 -0.288] (1.000)
Step: 516499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.296 -0.296 -0.296 -0.288 -0.288 -0.288] (1.000)
Step: 516999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.296 -0.296 -0.296 -0.288 -0.288 -0.288] (1.000)
Step: 517499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.288 -0.288 -0.288] (1.000)
Step: 517999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 518499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 518999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 519499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 519999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.286 -0.286 -0.286] (1.000)
Step: 520499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.286 -0.286 -0.286] (1.000)
Step: 520999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 521499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 521999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 522499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 522999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.295 -0.295 -0.295 -0.286 -0.286 -0.286] (1.000)
Step: 523499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.295 -0.295 -0.295 -0.287 -0.287 -0.287] (1.000)
Step: 523999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.286 -0.286 -0.286] (1.000)
Step: 524499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.286 -0.286 -0.286] (1.000)
Step: 524999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.294 -0.294 -0.294 -0.286 -0.286 -0.286] (1.000)
Step: 525499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 525999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 526499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.293 -0.293 -0.293 -0.286 -0.286 -0.286] (1.000)
Step: 526999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 527499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 527999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 528499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 528999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.284 -0.284 -0.284] (1.000)
Step: 529499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.293 -0.293 -0.293 -0.285 -0.285 -0.285] (1.000)
Step: 529999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.284 -0.284 -0.284] (1.000)
Step: 530499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.284 -0.284 -0.284] (1.000)
Step: 530999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.284 -0.284 -0.284] (1.000)
Step: 531499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.292 -0.292 -0.292 -0.284 -0.284 -0.284] (1.000)
Step: 531999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.283 -0.283 -0.283] (1.000)
Step: 532499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.291 -0.291 -0.291 -0.284 -0.284 -0.284] (1.000)
Step: 532999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.283 -0.283 -0.283] (1.000)
Step: 533499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.291 -0.291 -0.291 -0.283 -0.283 -0.283] (1.000)
Step: 533999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.283 -0.283 -0.283] (1.000)
Step: 534499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.283 -0.283 -0.283] (1.000)
Step: 534999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.282 -0.282 -0.282] (1.000)
Step: 535499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.29  -0.29  -0.29  -0.282 -0.282 -0.282] (1.000)
Step: 535999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.282 -0.282 -0.282] (1.000)
Step: 536499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.282 -0.282 -0.282] (1.000)
Step: 536999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.289 -0.289 -0.289 -0.281 -0.281 -0.281] (1.000)
Step: 537499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.281 -0.281 -0.281] (1.000)
Step: 537999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.281 -0.281 -0.281] (1.000)
Step: 538499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 538999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 539499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 539999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 540499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.28  -0.28  -0.28 ] (1.000)
Step: 540999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 541499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.288 -0.288 -0.288 -0.28  -0.28  -0.28 ] (1.000)
Step: 541999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.28  -0.28  -0.28 ] (1.000)
Step: 542499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 542999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 543499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 543999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 544499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 544999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 545499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.279 -0.279 -0.279] (1.000)
Step: 545999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.287 -0.287 -0.287 -0.278 -0.278 -0.278] (1.000)
Step: 546499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.278 -0.278 -0.278] (1.000)
Step: 546999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.278 -0.278 -0.278] (1.000)
Step: 547499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.286 -0.286 -0.286 -0.278 -0.278 -0.278] (1.000)
Step: 547999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.286 -0.286 -0.286 -0.278 -0.278 -0.278] (1.000)
Step: 548499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.278 -0.278 -0.278] (1.000)
Step: 548999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.277 -0.277 -0.277] (1.000)
Step: 549499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.277 -0.277 -0.277] (1.000)
Step: 549999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.285 -0.285 -0.285 -0.278 -0.278 -0.278] (1.000)
Step: 550499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.285 -0.285 -0.285 -0.277 -0.277 -0.277] (1.000)
Step: 550999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.277 -0.277 -0.277] (1.000)
Step: 551499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.277 -0.277 -0.277] (1.000)
Step: 551999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.277 -0.277 -0.277] (1.000)
Step: 552499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.276 -0.276 -0.276] (1.000)
Step: 552999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.284 -0.284 -0.284 -0.276 -0.276 -0.276] (1.000)
Step: 553499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.284 -0.284 -0.284 -0.276 -0.276 -0.276] (1.000)
Step: 553999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.276 -0.276 -0.276] (1.000)
Step: 554499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 554999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 555499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 555999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 556499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 556999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.7746], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 557499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 557999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 558499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 558999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 559499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 559999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.283 -0.283 -0.283 -0.275 -0.275 -0.275] (1.000)
Step: 560499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.275 -0.275 -0.275] (1.000)
Step: 560999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.274 -0.274 -0.274] (1.000)
Step: 561499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.274 -0.274 -0.274] (1.000)
Step: 561999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.282 -0.282 -0.282 -0.274 -0.274 -0.274] (1.000)
Step: 562499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.282 -0.282 -0.282 -0.274 -0.274 -0.274] (1.000)
Step: 562999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.274 -0.274 -0.274] (1.000)
Step: 563499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.274 -0.274 -0.274] (1.000)
Step: 563999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 564499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 564999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 565499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 565999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 566499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 566999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 567499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.273 -0.273 -0.273] (1.000)
Step: 567999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.273 -0.273 -0.273] (1.000)
Step: 568499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 568999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 569499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 569999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 570499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 570999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 571499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 571999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 572499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 572999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 573499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.272 -0.272 -0.272] (1.000)
Step: 573999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 574499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 574999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.281 -0.281 -0.281 -0.272 -0.272 -0.272] (1.000)
Step: 575499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.271 -0.271 -0.271] (1.000)
Step: 575999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.281 -0.281 -0.281 -0.271 -0.271 -0.271] (1.000)
Step: 576499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 576999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 577499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 577999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 578499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.28  -0.28  -0.28  -0.271 -0.271 -0.271] (1.000)
Step: 578999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.27  -0.27  -0.27 ] (1.000)
Step: 579499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.27  -0.27  -0.27 ] (1.000)
Step: 579999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.279 -0.279 -0.279 -0.27  -0.27  -0.27 ] (1.000)
Step: 580499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.27  -0.27  -0.27 ] (1.000)
Step: 580999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.279 -0.279 -0.279 -0.27  -0.27  -0.27 ] (1.000)
Step: 581499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.27  -0.27  -0.27 ] (1.000)
Step: 581999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 582499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 582999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 583499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 583999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 584499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 584999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.269 -0.269 -0.269] (1.000)
Step: 585499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.269 -0.269 -0.269] (1.000)
Step: 585999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 586499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 586999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.278 -0.278 -0.278 -0.268 -0.268 -0.268] (1.000)
Step: 587499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.278 -0.278 -0.278 -0.268 -0.268 -0.268] (1.000)
Step: 587999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 588499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 588999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 589499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.268 -0.268 -0.268] (1.000)
Step: 589999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.267 -0.267 -0.267] (1.000)
Step: 590499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.277 -0.277 -0.277 -0.267 -0.267 -0.267] (1.000)
Step: 590999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.267 -0.267 -0.267] (1.000)
Step: 591499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.267 -0.267 -0.267] (1.000)
Step: 591999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.267 -0.267 -0.267] (1.000)
Step: 592499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 592999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.266 -0.266 -0.266] (1.000)
Step: 593499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.267 -0.267 -0.267] (1.000)
Step: 593999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.267 -0.267 -0.267] (1.000)
Step: 594499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 594999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.267 -0.267 -0.267] (1.000)
Step: 595499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 595999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 596499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.267 -0.267 -0.267] (1.000)
Step: 596999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.277 -0.277 -0.277 -0.267 -0.267 -0.267] (1.000)
Step: 597499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 597999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 598499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 598999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.266 -0.266 -0.266] (1.000)
Step: 599499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.265 -0.265 -0.265] (1.000)
Step: 599999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.265 -0.265 -0.265] (1.000)
Step: 600499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.265 -0.265 -0.265] (1.000)
Step: 600999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.265 -0.265 -0.265] (1.000)
Step: 601499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.265 -0.265 -0.265] (1.000)
Step: 601999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.265 -0.265 -0.265] (1.000)
Step: 602499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.264 -0.264 -0.264] (1.000)
Step: 602999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 603499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 603999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 604499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 604999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.264 -0.264 -0.264] (1.000)
Step: 605499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 605999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.276 -0.276 -0.276 -0.264 -0.264 -0.264] (1.000)
Step: 606499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 606999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 607499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 607999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 608499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 608999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 609499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 609999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 610499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 610999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.262 -0.262 -0.262] (1.000)
Step: 611499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.262 -0.262 -0.262] (1.000)
Step: 611999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.275 -0.275 -0.275 -0.263 -0.263 -0.263] (1.000)
Step: 612499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.275 -0.275 -0.275 -0.262 -0.262 -0.262] (1.000)
Step: 612999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.262 -0.262 -0.262] (1.000)
Step: 613499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.275 -0.275 -0.275 -0.262 -0.262 -0.262] (1.000)
Step: 613999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.262 -0.262 -0.262] (1.000)
Step: 614499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.262 -0.262 -0.262] (1.000)
Step: 614999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.261 -0.261 -0.261] (1.000)
Step: 615499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.261 -0.261 -0.261] (1.000)
Step: 615999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.274 -0.274 -0.274 -0.261 -0.261 -0.261] (1.000)
Step: 616499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.261 -0.261 -0.261] (1.000)
Step: 616999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 617499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 617999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 618499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 618999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 619499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 619999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.26  -0.26  -0.26 ] (1.000)
Step: 620499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.273 -0.273 -0.273 -0.259 -0.259 -0.259] (1.000)
Step: 620999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.259 -0.259 -0.259] (1.000)
Step: 621499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.272 -0.272 -0.272 -0.26  -0.26  -0.26 ] (1.000)
Step: 621999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.259 -0.259 -0.259] (1.000)
Step: 622499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.259 -0.259 -0.259] (1.000)
Step: 622999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.272 -0.272 -0.272 -0.259 -0.259 -0.259] (1.000)
Step: 623499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.271 -0.271 -0.271 -0.259 -0.259 -0.259] (1.000)
Step: 623999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.271 -0.271 -0.271 -0.259 -0.259 -0.259] (1.000)
Step: 624499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.271 -0.271 -0.271 -0.259 -0.259 -0.259] (1.000)
Step: 624999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.271 -0.271 -0.271 -0.259 -0.259 -0.259] (1.000)
Step: 625499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.271 -0.271 -0.271 -0.258 -0.258 -0.258] (1.000)
Step: 625999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.271 -0.271 -0.271 -0.258 -0.258 -0.258] (1.000)
Step: 626499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.271 -0.271 -0.271 -0.258 -0.258 -0.258] (1.000)
Step: 626999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.258 -0.258 -0.258] (1.000)
Step: 627499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.258 -0.258 -0.258] (1.000)
Step: 627999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.258 -0.258 -0.258] (1.000)
Step: 628499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 628999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 629499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 629999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 630499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 630999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.27  -0.27  -0.27  -0.257 -0.257 -0.257] (1.000)
Step: 631499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.257 -0.257 -0.257] (1.000)
Step: 631999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.257 -0.257 -0.257] (1.000)
Step: 632499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.256 -0.256 -0.256] (1.000)
Step: 632999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.269 -0.269 -0.269 -0.257 -0.257 -0.257] (1.000)
Step: 633499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.257 -0.257 -0.257] (1.000)
Step: 633999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.256 -0.256 -0.256] (1.000)
Step: 634499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.256 -0.256 -0.256] (1.000)
Step: 634999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.256 -0.256 -0.256] (1.000)
Step: 635499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.269 -0.269 -0.269 -0.256 -0.256 -0.256] (1.000)
Step: 635999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.269 -0.269 -0.269 -0.256 -0.256 -0.256] (1.000)
Step: 636499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 636999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.269 -0.269 -0.269 -0.255 -0.255 -0.255] (1.000)
Step: 637499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 637999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 638499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 638999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 639499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 639999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 640499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.255 -0.255 -0.255] (1.000)
Step: 640999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 641499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 641999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.254 -0.254 -0.254] (1.000)
Step: 642499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.254 -0.254 -0.254] (1.000)
Step: 642999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.254 -0.254 -0.254] (1.000)
Step: 643499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 643999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 644499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 644999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 645499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 645999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 646499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.254 -0.254 -0.254] (1.000)
Step: 646999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.254 -0.254 -0.254] (1.000)
Step: 647499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 647999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 648499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 648999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 649499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 649999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 650499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 650999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 651499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.268 -0.268 -0.268 -0.253 -0.253 -0.253] (1.000)
Step: 651999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 652499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 652999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 653499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 653999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 654499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 654999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.253 -0.253 -0.253] (1.000)
Step: 655499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 655999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 656499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 656999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 657499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.252 -0.252 -0.252] (1.000)
Step: 657999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 658499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 658999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.267 -0.267 -0.267 -0.252 -0.252 -0.252] (1.000)
Step: 659499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 659999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 660499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 660999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 661499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 661999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 662499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.251 -0.251 -0.251] (1.000)
Step: 662999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 663499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 663999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.251 -0.251 -0.251] (1.000)
Step: 664499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 664999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 665499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.25  -0.25  -0.25 ] (1.000)
Step: 665999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.25  -0.25  -0.25 ] (1.000)
Step: 666499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.25  -0.25  -0.25 ] (1.000)
Step: 666999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.25  -0.25  -0.25 ] (1.000)
Step: 667499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 667999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.25  -0.25  -0.25 ] (1.000)
Step: 668499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 668999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 669499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 669999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.265 -0.265 -0.265 -0.251 -0.251 -0.251] (1.000)
Step: 670499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.8944], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 670999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 671499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 671999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 672499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 672999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 673499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 673999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 674499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 674999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.266 -0.266 -0.266 -0.251 -0.251 -0.251] (1.000)
Step: 675499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 675999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 676499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 676999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 677499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.265 -0.265 -0.265 -0.25  -0.25  -0.25 ] (1.000)
Step: 677999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.249 -0.249 -0.249] (1.000)
Step: 678499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.249 -0.249 -0.249] (1.000)
Step: 678999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.249 -0.249 -0.249] (1.000)
Step: 679499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.249 -0.249 -0.249] (1.000)
Step: 679999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.264 -0.264 -0.264 -0.249 -0.249 -0.249] (1.000)
Step: 680499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.249 -0.249 -0.249] (1.000)
Step: 680999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 681499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 681999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 682499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 682999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 683499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 683999, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 684499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 684999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 685499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 685999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 686499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 686999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 687499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.263 -0.263 -0.263 -0.248 -0.248 -0.248] (1.000)
Step: 687999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 688499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 688999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 689499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 689999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 690499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.262 -0.262 -0.262 -0.247 -0.247 -0.247] (1.000)
Step: 690999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 691499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 691999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.246 -0.246 -0.246] (1.000)
Step: 692499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 692999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.246 -0.246 -0.246] (1.000)
Step: 693499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.246 -0.246 -0.246] (1.000)
Step: 693999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.246 -0.246 -0.246] (1.000)
Step: 694499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 694999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 695499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 695999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 696499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.261 -0.261 -0.261 -0.247 -0.247 -0.247] (1.000)
Step: 696999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 697499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 697999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 698499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 698999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 699499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.247 -0.247 -0.247] (1.000)
Step: 699999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.26  -0.26  -0.26  -0.246 -0.246 -0.246] (1.000)
Step: 700499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.246 -0.246 -0.246] (1.000)
Step: 700999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.247 -0.247 -0.247] (1.000)
Step: 701499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.246 -0.246 -0.246] (1.000)
Step: 701999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.246 -0.246 -0.246] (1.000)
Step: 702499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.246 -0.246 -0.246] (1.000)
Step: 702999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.246 -0.246 -0.246] (1.000)
Step: 703499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.247 -0.247 -0.247] (1.000)
Step: 703999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.259 -0.259 -0.259 -0.247 -0.247 -0.247] (1.000)
Step: 704499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.259 -0.259 -0.259 -0.247 -0.247 -0.247] (1.000)
Step: 704999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 705499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 705999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.248 -0.248 -0.248] (1.000)
Step: 706499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 706999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.258 -0.258 -0.258 -0.248 -0.248 -0.248] (1.000)
Step: 707499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 707999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 708499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.258 -0.258 -0.258 -0.247 -0.247 -0.247] (1.000)
Step: 708999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.247 -0.247 -0.247] (1.000)
Step: 709499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.247 -0.247 -0.247] (1.000)
Step: 709999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.247 -0.247 -0.247] (1.000)
Step: 710499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.246 -0.246 -0.246] (1.000)
Step: 710999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.246 -0.246 -0.246] (1.000)
Step: 711499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.246 -0.246 -0.246] (1.000)
Step: 711999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.257 -0.257 -0.257 -0.246 -0.246 -0.246] (1.000)
Step: 712499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.257 -0.257 -0.257 -0.246 -0.246 -0.246] (1.000)
Step: 712999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.246 -0.246 -0.246] (1.000)
Step: 713499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.246 -0.246 -0.246] (1.000)
Step: 713999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.245 -0.245 -0.245] (1.000)
Step: 714499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.256 -0.256 -0.256 -0.246 -0.246 -0.246] (1.000)
Step: 714999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.245 -0.245 -0.245] (1.000)
Step: 715499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.256 -0.256 -0.256 -0.245 -0.245 -0.245] (1.000)
Step: 715999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.245 -0.245 -0.245] (1.000)
Step: 716499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.255 -0.255 -0.255 -0.246 -0.246 -0.246] (1.000)
Step: 716999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.255 -0.255 -0.255 -0.246 -0.246 -0.246] (1.000)
Step: 717499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.246 -0.246 -0.246] (1.000)
Step: 717999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.245 -0.245 -0.245] (1.000)
Step: 718499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.255 -0.255 -0.255 -0.246 -0.246 -0.246] (1.000)
Step: 718999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.246 -0.246 -0.246] (1.000)
Step: 719499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.245 -0.245 -0.245] (1.000)
Step: 719999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.255 -0.255 -0.255 -0.245 -0.245 -0.245] (1.000)
Step: 720499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 720999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 721499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 721999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 722499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 722999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 723499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.254 -0.254 -0.254 -0.244 -0.244 -0.244] (1.000)
Step: 723999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.254 -0.254 -0.254 -0.244 -0.244 -0.244] (1.000)
Step: 724499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.254 -0.254 -0.254 -0.245 -0.245 -0.245] (1.000)
Step: 724999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.245 -0.245 -0.245] (1.000)
Step: 725499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.244 -0.244 -0.244] (1.000)
Step: 725999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.253 -0.253 -0.253 -0.245 -0.245 -0.245] (1.000)
Step: 726499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.245 -0.245 -0.245] (1.000)
Step: 726999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.244 -0.244 -0.244] (1.000)
Step: 727499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.244 -0.244 -0.244] (1.000)
Step: 727999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.253 -0.253 -0.253 -0.244 -0.244 -0.244] (1.000)
Step: 728499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.244 -0.244 -0.244] (1.000)
Step: 728999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.244 -0.244 -0.244] (1.000)
Step: 729499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.244 -0.244 -0.244] (1.000)
Step: 729999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.243 -0.243 -0.243] (1.000)
Step: 730499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.244 -0.244 -0.244] (1.000)
Step: 730999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.243 -0.243 -0.243] (1.000)
Step: 731499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.252 -0.252 -0.252 -0.243 -0.243 -0.243] (1.000)
Step: 731999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.243 -0.243 -0.243] (1.000)
Step: 732499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.252 -0.252 -0.252 -0.243 -0.243 -0.243] (1.000)
Step: 732999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.243 -0.243 -0.243] (1.000)
Step: 733499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.243 -0.243 -0.243] (1.000)
Step: 733999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.243 -0.243 -0.243] (1.000)
Step: 734499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.242 -0.242 -0.242] (1.000)
Step: 734999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.242 -0.242 -0.242] (1.000)
Step: 735499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.251 -0.251 -0.251 -0.242 -0.242 -0.242] (1.000)
Step: 735999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.242 -0.242 -0.242] (1.000)
Step: 736499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.251 -0.251 -0.251 -0.242 -0.242 -0.242] (1.000)
Step: 736999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.242 -0.242 -0.242] (1.000)
Step: 737499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.242 -0.242 -0.242] (1.000)
Step: 737999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.241 -0.241 -0.241] (1.000)
Step: 738499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.241 -0.241 -0.241] (1.000)
Step: 738999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.241 -0.241 -0.241] (1.000)
Step: 739499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.25  -0.25  -0.25  -0.241 -0.241 -0.241] (1.000)
Step: 739999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.241 -0.241 -0.241] (1.000)
Step: 740499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.241 -0.241 -0.241] (1.000)
Step: 740999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.241 -0.241 -0.241] (1.000)
Step: 741499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.241 -0.241 -0.241] (1.000)
Step: 741999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.241 -0.241 -0.241] (1.000)
Step: 742499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 742999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 743499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 743999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 744499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 744999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 745499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.7746], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 745999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 746499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 746999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 747499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 747999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 748499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 748999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 749499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 749999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 750499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 750999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 751499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 751999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 752499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 752999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.249 -0.249 -0.249 -0.24  -0.24  -0.24 ] (1.000)
Step: 753499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 753999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 754499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 754999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 755499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.24  -0.24  -0.24 ] (1.000)
Step: 755999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 756499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 756999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 757499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 757999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 758499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 758999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 759499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 759999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 760499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.248 -0.248 -0.248 -0.239 -0.239 -0.239] (1.000)
Step: 760999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 761499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.239 -0.239 -0.239] (1.000)
Step: 761999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 762499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 762999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 763499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 763999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 764499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.247 -0.247 -0.247 -0.238 -0.238 -0.238] (1.000)
Step: 764999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 765499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.237 -0.237 -0.237] (1.000)
Step: 765999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 766499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 766999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 767499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 767999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 768499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 768999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 769499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.246 -0.246 -0.246 -0.238 -0.238 -0.238] (1.000)
Step: 769999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.238 -0.238 -0.238] (1.000)
Step: 770499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.237 -0.237 -0.237] (1.000)
Step: 770999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.237 -0.237 -0.237] (1.000)
Step: 771499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.237 -0.237 -0.237] (1.000)
Step: 771999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.237 -0.237 -0.237] (1.000)
Step: 772499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.245 -0.245 -0.245 -0.237 -0.237 -0.237] (1.000)
Step: 772999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.237 -0.237 -0.237] (1.000)
Step: 773499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.237 -0.237 -0.237] (1.000)
Step: 773999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 774499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 774999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 775499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 775999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 776499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 776999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 777499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 777999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 778499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.244 -0.244 -0.244 -0.236 -0.236 -0.236] (1.000)
Step: 778999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 779499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 779999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.235 -0.235 -0.235] (1.000)
Step: 780499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 780999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.244 -0.244 -0.244 -0.235 -0.235 -0.235] (1.000)
Step: 781499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 781999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 782499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 782999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.235 -0.235 -0.235] (1.000)
Step: 783499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.234 -0.234 -0.234] (1.000)
Step: 783999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.243 -0.243 -0.243 -0.234 -0.234 -0.234] (1.000)
Step: 784499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.243 -0.243 -0.243 -0.234 -0.234 -0.234] (1.000)
Step: 784999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 785499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 785999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 786499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 786999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 787499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 787999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 788499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.242 -0.242 -0.242 -0.234 -0.234 -0.234] (1.000)
Step: 788999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.234 -0.234 -0.234] (1.000)
Step: 789499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.234 -0.234 -0.234] (1.000)
Step: 789999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.234 -0.234 -0.234] (1.000)
Step: 790499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 790999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 791499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 791999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 792499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 792999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 793499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 793999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 794499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 794999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 795499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 795999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 796499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 796999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 797499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 797999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 798499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 798999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 799499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.241 -0.241 -0.241 -0.234 -0.234 -0.234] (1.000)
Step: 799999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.234 -0.234 -0.234] (1.000)
Step: 800499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 800999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.241 -0.241 -0.241 -0.233 -0.233 -0.233] (1.000)
Step: 801499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 801999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 802499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 802999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 803499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 803999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.232 -0.232 -0.232] (1.000)
Step: 804499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.24  -0.24  -0.24  -0.232 -0.232 -0.232] (1.000)
Step: 804999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 805499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 805999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.233 -0.233 -0.233] (1.000)
Step: 806499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.232 -0.232 -0.232] (1.000)
Step: 806999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.24  -0.24  -0.24  -0.232 -0.232 -0.232] (1.000)
Step: 807499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.232 -0.232 -0.232] (1.000)
Step: 807999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.232 -0.232 -0.232] (1.000)
Step: 808499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.232 -0.232 -0.232] (1.000)
Step: 808999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.232 -0.232 -0.232] (1.000)
Step: 809499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.232 -0.232 -0.232] (1.000)
Step: 809999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.231 -0.231 -0.231] (1.000)
Step: 810499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 810999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.239 -0.239 -0.239 -0.231 -0.231 -0.231] (1.000)
Step: 811499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.239 -0.239 -0.239 -0.231 -0.231 -0.231] (1.000)
Step: 811999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.231 -0.231 -0.231] (1.000)
Step: 812499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.239 -0.239 -0.239 -0.231 -0.231 -0.231] (1.000)
Step: 812999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 813499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 813999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 814499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 814999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 815499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.231 -0.231 -0.231] (1.000)
Step: 815999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.23  -0.23  -0.23 ] (1.000)
Step: 816499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.238 -0.238 -0.238 -0.23  -0.23  -0.23 ] (1.000)
Step: 816999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 817499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.237 -0.237 -0.237 -0.231 -0.231 -0.231] (1.000)
Step: 817999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.237 -0.237 -0.237 -0.231 -0.231 -0.231] (1.000)
Step: 818499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.231 -0.231 -0.231] (1.000)
Step: 818999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 819499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 819999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 820499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 820999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.237 -0.237 -0.237 -0.23  -0.23  -0.23 ] (1.000)
Step: 821499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.23  -0.23  -0.23 ] (1.000)
Step: 821999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.23  -0.23  -0.23 ] (1.000)
Step: 822499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.23  -0.23  -0.23 ] (1.000)
Step: 822999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.23  -0.23  -0.23 ] (1.000)
Step: 823499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.23  -0.23  -0.23 ] (1.000)
Step: 823999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.229 -0.229 -0.229] (1.000)
Step: 824499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.236 -0.236 -0.236 -0.229 -0.229 -0.229] (1.000)
Step: 824999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 825499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 825999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 826499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 826999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 827499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 827999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 828499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 828999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.235 -0.235 -0.235 -0.229 -0.229 -0.229] (1.000)
Step: 829499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.228 -0.228 -0.228] (1.000)
Step: 829999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.235 -0.235 -0.235 -0.228 -0.228 -0.228] (1.000)
Step: 830499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 830999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 831499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 831999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 832499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 832999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 833499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 833999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 834499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 834999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 835499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.234 -0.234 -0.234 -0.228 -0.228 -0.228] (1.000)
Step: 835999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 836499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 836999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 837499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 837999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 838499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 838999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 839499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 839999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 840499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 840999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.233 -0.233 -0.233 -0.227 -0.227 -0.227] (1.000)
Step: 841499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 841999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.232 -0.232 -0.232 -0.227 -0.227 -0.227] (1.000)
Step: 842499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.227 -0.227 -0.227] (1.000)
Step: 842999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 843499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 843999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 844499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 844999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 845499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 845999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 846499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.226 -0.226 -0.226] (1.000)
Step: 846999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.232 -0.232 -0.232 -0.226 -0.226 -0.226] (1.000)
Step: 847499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.232 -0.232 -0.232 -0.225 -0.225 -0.225] (1.000)
Step: 847999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 848499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 848999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 849499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 849999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 850499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.225 -0.225 -0.225] (1.000)
Step: 850999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.231 -0.231 -0.231 -0.224 -0.224 -0.224] (1.000)
Step: 851499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 851999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 852499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 852999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 853499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 853999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 854499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 854999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 855499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.23  -0.23  -0.23  -0.224 -0.224 -0.224] (1.000)
Step: 855999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.224 -0.224 -0.224] (1.000)
Step: 856499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.224 -0.224 -0.224] (1.000)
Step: 856999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.224 -0.224 -0.224] (1.000)
Step: 857499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.229 -0.229 -0.229 -0.224 -0.224 -0.224] (1.000)
Step: 857999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 858499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 858999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 859499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 859999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 860499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 860999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.229 -0.229 -0.229 -0.223 -0.223 -0.223] (1.000)
Step: 861499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 861999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 862499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 862999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 863499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 863999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 864499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 864999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.223 -0.223 -0.223] (1.000)
Step: 865499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 865999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 866499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 866999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 867499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 867999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 868499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 868999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 869499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 869999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 870499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 870999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.228 -0.228 -0.228 -0.222 -0.222 -0.222] (1.000)
Step: 871499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.222 -0.222 -0.222] (1.000)
Step: 871999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.222 -0.222 -0.222] (1.000)
Step: 872499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.222 -0.222 -0.222] (1.000)
Step: 872999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 873499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 873999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 874499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 874999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 875499, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 875999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 876499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 876999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 877499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 877999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.227 -0.227 -0.227 -0.221 -0.221 -0.221] (1.000)
Step: 878499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.221 -0.221 -0.221] (1.000)
Step: 878999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.221 -0.221 -0.221] (1.000)
Step: 879499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.221 -0.221 -0.221] (1.000)
Step: 879999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 880499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 880999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 881499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 881999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 882499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 882999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 883499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 883999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 884499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 884999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 885499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 885999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 886499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.226 -0.226 -0.226 -0.22  -0.22  -0.22 ] (1.000)
Step: 886999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.22  -0.22  -0.22 ] (1.000)
Step: 887499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 887999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 888499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 888999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 889499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 889999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 890499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 890999, Reward: [0. 0. 0. 0. 0. 0.] [0.6325], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 891499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 891999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 892499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 892999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 893499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 893999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 894499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 894999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 895499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.225 -0.225 -0.225 -0.219 -0.219 -0.219] (1.000)
Step: 895999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 896499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 896999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 897499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 897999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 898499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 898999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 899499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 899999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 900499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.219 -0.219 -0.219] (1.000)
Step: 900999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 901499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 901999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 902499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 902999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 903499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 903999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.224 -0.224 -0.224 -0.218 -0.218 -0.218] (1.000)
Step: 904499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.218 -0.218 -0.218] (1.000)
Step: 904999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.218 -0.218 -0.218] (1.000)
Step: 905499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.218 -0.218 -0.218] (1.000)
Step: 905999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 906499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 906999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 907499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 907999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 908499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 908999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 909499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 909999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 910499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 910999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 911499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 911999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.223 -0.223 -0.223 -0.217 -0.217 -0.217] (1.000)
Step: 912499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 912999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 913499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.216 -0.216 -0.216] (1.000)
Step: 913999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 914499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 914999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 915499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 915999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 916499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 916999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 917499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 917999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 918499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 918999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 919499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 919999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.217 -0.217 -0.217] (1.000)
Step: 920499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.217 -0.217 -0.217] (1.000)
Step: 920999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.217 -0.217 -0.217] (1.000)
Step: 921499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 921999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 922499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.222 -0.222 -0.222 -0.216 -0.216 -0.216] (1.000)
Step: 922999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 923499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 923999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 924499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 924999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 925499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 925999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.216 -0.216 -0.216] (1.000)
Step: 926499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.221 -0.221 -0.221 -0.215 -0.215 -0.215] (1.000)
Step: 926999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 927499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 927999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 928499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 928999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 929499, Reward: [-0.4 -0.4 -0.4  0.4  0.4  0.4] [0.6325], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 929999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 930499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 930999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.221 -0.221 -0.221 -0.215 -0.215 -0.215] (1.000)
Step: 931499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 931999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 932499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 932999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 933499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 933999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 934499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 934999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 935499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 935999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 936499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 936999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 937499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 937999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 938499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 938999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 939499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 939999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 940499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 940999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 941499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 941999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 942499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 942999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.22  -0.22  -0.22  -0.215 -0.215 -0.215] (1.000)
Step: 943499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 943999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.22  -0.22  -0.22  -0.214 -0.214 -0.214] (1.000)
Step: 944499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 944999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 945499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 945999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 946499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 946999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 947499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 947999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 948499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 948999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 949499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 949999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 950499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 950999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 951499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 951999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 952499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.219 -0.219 -0.219 -0.214 -0.214 -0.214] (1.000)
Step: 952999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 953499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 953999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 954499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 954999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 955499, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 955999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 956499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 956999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 957499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 957999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.214 -0.214 -0.214] (1.000)
Step: 958499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 958999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.218 -0.218 -0.218 -0.214 -0.214 -0.214] (1.000)
Step: 959499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.214 -0.214 -0.214] (1.000)
Step: 959999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.214 -0.214 -0.214] (1.000)
Step: 960499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 960999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 961499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 961999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 962499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 962999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 963499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 963999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 964499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 964999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 965499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.217 -0.217 -0.217 -0.213 -0.213 -0.213] (1.000)
Step: 965999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 966499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 966999, Reward: [ 0.4  0.4  0.4 -0.4 -0.4 -0.4] [0.6325], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 967499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 967999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 968499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 968999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 969499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 969999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 970499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.213 -0.213 -0.213] (1.000)
Step: 970999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 971499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 971999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 972499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 972999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 973499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 973999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 974499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 974999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 975499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 975999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 976499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.216 -0.216 -0.216 -0.212 -0.212 -0.212] (1.000)
Step: 976999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.212 -0.212 -0.212] (1.000)
Step: 977499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.212 -0.212 -0.212] (1.000)
Step: 977999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.212 -0.212 -0.212] (1.000)
Step: 978499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 978999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 979499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 979999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 980499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 980999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 981499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 981999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 982499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 982999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 983499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 983999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 984499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 984999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 985499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 985999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.211 -0.211 -0.211] (1.000)
Step: 986499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.21  -0.21  -0.21 ] (1.000)
Step: 986999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.215 -0.215 -0.215 -0.21  -0.21  -0.21 ] (1.000)
Step: 987499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 987999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 988499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 988999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 989499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 989999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 990499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 990999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 991499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 991999, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 992499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 992999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 993499, Reward: [-0.2 -0.2 -0.2  0.2  0.2  0.2] [0.4472], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 993999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 994499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 994999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 995499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.214 -0.214 -0.214 -0.209 -0.209 -0.209] (1.000)
Step: 995999, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.214 -0.214 -0.214 -0.21  -0.21  -0.21 ] (1.000)
Step: 996499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 996999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 997499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 997999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 998499, Reward: [ 0.2  0.2  0.2 -0.2 -0.2 -0.2] [0.4472], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 998999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 999499, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
Step: 999999, Reward: [0. 0. 0. 0. 0. 0.] [0.0000], Avg: [-0.213 -0.213 -0.213 -0.209 -0.209 -0.209] (1.000)
