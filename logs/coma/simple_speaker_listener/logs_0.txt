Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_speaker_listener
num_envs: 16, state_size: [(3,), (11,)], action_size: [[3], [5]], action_space: [Discrete(3), Discrete(5)],

import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, REG_LAMBDA, EPS_MIN

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		out_dims = state.size()[:-1]
		state = state.view(int(np.prod(out_dims)), -1)
		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
		self.hidden = self.recurrent(state, self.hidden)
		action_probs = self.action_probs(self.hidden).softmax(-1)
		action_probs = action_probs.view(*out_dims, -1)
		return action_probs

	def init_hidden(self, batch_size=1):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		q_values = self.q_values(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		# self.state_size = state_size
		# self.action_size = action_size
		# actor_input = [np.sum(self.state_size) + self.action_size[-1]]
		# critic_input = [np.sum(self.state_size) + 2*self.action_size[-1]*self.state_size[0]]
		# self.critic = DDPGCritic([np.sum([np.prod(s) for s in self.state_size])], [np.sum([np.prod(a) for a in self.action_size])])
		# critic_input = [np.sum([np.prod(s) for s in self.state_size]) + np.max([s[-1] for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size])]
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

	def get_value(self, state, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			for t in reversed(range(q_target.size(0))):
				q_value[t] = model.critic_local(critic_input[t])
				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
				critic_loss = (q_select - q_target[t].detach()).pow(2)
				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

			hidden = model.actor_local.hidden
			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = (advantages * log_probs).sum()
			model.step(model.actor_optimizer, actor_loss.mean())
			model.actor_local.hidden = hidden

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = self.to_tensor(state)
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids.squeeze(-1) if n_agents==1 else agent_ids], axis=-1), device=self.network.device).float()
			actor_inputs.append(actor_input)
		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
		action = action_random if numpy and random.random() < eps else action_greedy
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()

			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state, 1))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a.argmax(-1, keepdims=True), a_size[-1]) for a,a_size in zip(actions, self.action_size)]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states, actions, last_actions = [[x.unsqueeze(-2) if n_agent==1 else x for x,n_agent in zip(inputs, n_agents)] for inputs in [states, actions, last_actions]]
			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

def one_hot(indices, depth):
	shape = [*indices.shape[:-1], depth]
	one_hot = torch.zeros(shape)
	one_hot.scatter_(-1,indices, 1)
	return one_hot
REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 10					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 10000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[4]
env_name = gfb_envs[-4]
env_name = ptc_envs[-4]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return env #FootballTeamEnv(env)

def run(model, steps=10000, ports=16, eval_at=1000, checkpoint=False, save_best=False, log=True, render=True):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(make_env, ports, render=False)
	model = COMAAgent if type(envs.env.action_space) == gym.spaces.MultiDiscrete else COMAAgent if type(envs.env.action_space) == list else model
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(1)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and total_rewards[-1] >= max(total_rewards): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model):
	envs = EnsembleEnv(make_env, 0, log=True, render=True)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="ppo", choices=["ddqn", "ddpg", "ppo", "rand"], help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--test", action="store_true", help="Whether to show a trial run")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	model = DDPGAgent if args.model == "ddpg" else PPOAgent if args.model == "ppo" else DDQNAgent if args.model == "ddqn" else RandomAgent
	if args.test:
		trial(model)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports)

Step: 49, Reward: [-2414.06 -2414.06] [0.0000], Avg: [-2414.06 -2414.06] (0.995)
Step: 99, Reward: [-161.832 -161.832] [0.0000], Avg: [-1287.946 -1287.946] (0.990)
Step: 149, Reward: [-301.926 -301.926] [0.0000], Avg: [-959.273 -959.273] (0.985)
Step: 199, Reward: [-1329.695 -1329.695] [0.0000], Avg: [-1051.878 -1051.878] (0.980)
Step: 249, Reward: [-949.23 -949.23] [0.0000], Avg: [-1031.349 -1031.349] (0.975)
Step: 299, Reward: [-1229.594 -1229.594] [0.0000], Avg: [-1064.39 -1064.39] (0.970)
Step: 349, Reward: [-110.227 -110.227] [0.0000], Avg: [-928.081 -928.081] (0.966)
Step: 399, Reward: [-1286.563 -1286.563] [0.0000], Avg: [-972.891 -972.891] (0.961)
Step: 449, Reward: [-825.17 -825.17] [0.0000], Avg: [-956.478 -956.478] (0.956)
Step: 499, Reward: [-836.273 -836.273] [0.0000], Avg: [-944.457 -944.457] (0.951)
Step: 549, Reward: [-1253.179 -1253.179] [0.0000], Avg: [-972.523 -972.523] (0.946)
Step: 599, Reward: [-1743.767 -1743.767] [0.0000], Avg: [-1036.793 -1036.793] (0.942)
Step: 649, Reward: [-198.77 -198.77] [0.0000], Avg: [-972.33 -972.33] (0.937)
Step: 699, Reward: [-346.185 -346.185] [0.0000], Avg: [-927.605 -927.605] (0.932)
Step: 749, Reward: [-802.249 -802.249] [0.0000], Avg: [-919.248 -919.248] (0.928)
Step: 799, Reward: [-1218.363 -1218.363] [0.0000], Avg: [-937.943 -937.943] (0.923)
Step: 849, Reward: [-449.322 -449.322] [0.0000], Avg: [-909.2 -909.2] (0.918)
Step: 899, Reward: [-191.601 -191.601] [0.0000], Avg: [-869.334 -869.334] (0.914)
Step: 949, Reward: [-3413.79 -3413.79] [0.0000], Avg: [-1003.252 -1003.252] (0.909)
Step: 999, Reward: [-617.163 -617.163] [0.0000], Avg: [-983.948 -983.948] (0.905)
Step: 1049, Reward: [-1039.05 -1039.05] [0.0000], Avg: [-986.572 -986.572] (0.900)
Step: 1099, Reward: [-503.059 -503.059] [0.0000], Avg: [-964.594 -964.594] (0.896)
Step: 1149, Reward: [-683.055 -683.055] [0.0000], Avg: [-952.353 -952.353] (0.891)
Step: 1199, Reward: [-202.661 -202.661] [0.0000], Avg: [-921.116 -921.116] (0.887)
Step: 1249, Reward: [-841.229 -841.229] [0.0000], Avg: [-917.921 -917.921] (0.882)
Step: 1299, Reward: [-133.915 -133.915] [0.0000], Avg: [-887.766 -887.766] (0.878)
Step: 1349, Reward: [-1518.186 -1518.186] [0.0000], Avg: [-911.115 -911.115] (0.873)
Step: 1399, Reward: [-2173.736 -2173.736] [0.0000], Avg: [-956.209 -956.209] (0.869)
Step: 1449, Reward: [-613.432 -613.432] [0.0000], Avg: [-944.389 -944.389] (0.865)
Step: 1499, Reward: [-100.502 -100.502] [0.0000], Avg: [-916.259 -916.259] (0.860)
Step: 1549, Reward: [-2125.664 -2125.664] [0.0000], Avg: [-955.273 -955.273] (0.856)
Step: 1599, Reward: [-1226.698 -1226.698] [0.0000], Avg: [-963.755 -963.755] (0.852)
Step: 1649, Reward: [-1656.613 -1656.613] [0.0000], Avg: [-984.75 -984.75] (0.848)
Step: 1699, Reward: [-867.431 -867.431] [0.0000], Avg: [-981.3 -981.3] (0.843)
Step: 1749, Reward: [-269.024 -269.024] [0.0000], Avg: [-960.949 -960.949] (0.839)
Step: 1799, Reward: [-173.705 -173.705] [0.0000], Avg: [-939.081 -939.081] (0.835)
Step: 1849, Reward: [-1575.559 -1575.559] [0.0000], Avg: [-956.283 -956.283] (0.831)
Step: 1899, Reward: [-870.065 -870.065] [0.0000], Avg: [-954.014 -954.014] (0.827)
Step: 1949, Reward: [-1570.041 -1570.041] [0.0000], Avg: [-969.81 -969.81] (0.822)
Step: 1999, Reward: [-899.667 -899.667] [0.0000], Avg: [-968.056 -968.056] (0.818)
Step: 2049, Reward: [-454.205 -454.205] [0.0000], Avg: [-955.523 -955.523] (0.814)
Step: 2099, Reward: [-445.273 -445.273] [0.0000], Avg: [-943.374 -943.374] (0.810)
Step: 2149, Reward: [-1410.939 -1410.939] [0.0000], Avg: [-954.248 -954.248] (0.806)
Step: 2199, Reward: [-2260.596 -2260.596] [0.0000], Avg: [-983.938 -983.938] (0.802)
Step: 2249, Reward: [-950.711 -950.711] [0.0000], Avg: [-983.199 -983.199] (0.798)
Step: 2299, Reward: [-354.103 -354.103] [0.0000], Avg: [-969.523 -969.523] (0.794)
Step: 2349, Reward: [-714.338 -714.338] [0.0000], Avg: [-964.094 -964.094] (0.790)
Step: 2399, Reward: [-440.265 -440.265] [0.0000], Avg: [-953.181 -953.181] (0.786)
Step: 2449, Reward: [-1545.546 -1545.546] [0.0000], Avg: [-965.27 -965.27] (0.782)
Step: 2499, Reward: [-61.922 -61.922] [0.0000], Avg: [-947.203 -947.203] (0.778)
Step: 2549, Reward: [-529.655 -529.655] [0.0000], Avg: [-939.016 -939.016] (0.774)
Step: 2599, Reward: [-35.187 -35.187] [0.0000], Avg: [-921.634 -921.634] (0.771)
Step: 2649, Reward: [-312.745 -312.745] [0.0000], Avg: [-910.146 -910.146] (0.767)
Step: 2699, Reward: [-1196.131 -1196.131] [0.0000], Avg: [-915.442 -915.442] (0.763)
Step: 2749, Reward: [-861.772 -861.772] [0.0000], Avg: [-914.466 -914.466] (0.759)
Step: 2799, Reward: [-1187.352 -1187.352] [0.0000], Avg: [-919.339 -919.339] (0.755)
Step: 2849, Reward: [-268.343 -268.343] [0.0000], Avg: [-907.918 -907.918] (0.751)
Step: 2899, Reward: [-1380.527 -1380.527] [0.0000], Avg: [-916.067 -916.067] (0.748)
Step: 2949, Reward: [-375.993 -375.993] [0.0000], Avg: [-906.913 -906.913] (0.744)
Step: 2999, Reward: [-383.817 -383.817] [0.0000], Avg: [-898.195 -898.195] (0.740)
Step: 3049, Reward: [-1090.478 -1090.478] [0.0000], Avg: [-901.347 -901.347] (0.737)
Step: 3099, Reward: [-1332.952 -1332.952] [0.0000], Avg: [-908.308 -908.308] (0.733)
Step: 3149, Reward: [-1738.328 -1738.328] [0.0000], Avg: [-921.483 -921.483] (0.729)
Step: 3199, Reward: [-77.915 -77.915] [0.0000], Avg: [-908.302 -908.302] (0.726)
Step: 3249, Reward: [-2182.454 -2182.454] [0.0000], Avg: [-927.905 -927.905] (0.722)
Step: 3299, Reward: [-765.834 -765.834] [0.0000], Avg: [-925.449 -925.449] (0.718)
Step: 3349, Reward: [-43.161 -43.161] [0.0000], Avg: [-912.281 -912.281] (0.715)
Step: 3399, Reward: [-1360.954 -1360.954] [0.0000], Avg: [-918.879 -918.879] (0.711)
Step: 3449, Reward: [-220.168 -220.168] [0.0000], Avg: [-908.752 -908.752] (0.708)
Step: 3499, Reward: [-1577.244 -1577.244] [0.0000], Avg: [-918.302 -918.302] (0.704)
Step: 3549, Reward: [-341.05 -341.05] [0.0000], Avg: [-910.172 -910.172] (0.701)
Step: 3599, Reward: [-1030.563 -1030.563] [0.0000], Avg: [-911.844 -911.844] (0.697)
Step: 3649, Reward: [-1190.983 -1190.983] [0.0000], Avg: [-915.668 -915.668] (0.694)
Step: 3699, Reward: [-258.23 -258.23] [0.0000], Avg: [-906.784 -906.784] (0.690)
Step: 3749, Reward: [-42.44 -42.44] [0.0000], Avg: [-895.259 -895.259] (0.687)
Step: 3799, Reward: [-696.949 -696.949] [0.0000], Avg: [-892.65 -892.65] (0.683)
Step: 3849, Reward: [-355.321 -355.321] [0.0000], Avg: [-885.671 -885.671] (0.680)
Step: 3899, Reward: [-385.045 -385.045] [0.0000], Avg: [-879.253 -879.253] (0.676)
Step: 3949, Reward: [-207.964 -207.964] [0.0000], Avg: [-870.756 -870.756] (0.673)
Step: 3999, Reward: [-225.475 -225.475] [0.0000], Avg: [-862.69 -862.69] (0.670)
Step: 4049, Reward: [-397.637 -397.637] [0.0000], Avg: [-856.948 -856.948] (0.666)
Step: 4099, Reward: [-113.785 -113.785] [0.0000], Avg: [-847.885 -847.885] (0.663)
Step: 4149, Reward: [-189.343 -189.343] [0.0000], Avg: [-839.951 -839.951] (0.660)
Step: 4199, Reward: [-124.754 -124.754] [0.0000], Avg: [-831.437 -831.437] (0.656)
Step: 4249, Reward: [-1099.359 -1099.359] [0.0000], Avg: [-834.589 -834.589] (0.653)
Step: 4299, Reward: [-518.402 -518.402] [0.0000], Avg: [-830.912 -830.912] (0.650)
Step: 4349, Reward: [-1523.692 -1523.692] [0.0000], Avg: [-838.875 -838.875] (0.647)
Step: 4399, Reward: [-718.644 -718.644] [0.0000], Avg: [-837.509 -837.509] (0.643)
Step: 4449, Reward: [-388.444 -388.444] [0.0000], Avg: [-832.463 -832.463] (0.640)
Step: 4499, Reward: [-160.596 -160.596] [0.0000], Avg: [-824.998 -824.998] (0.637)
Step: 4549, Reward: [-117.411 -117.411] [0.0000], Avg: [-817.223 -817.223] (0.634)
Step: 4599, Reward: [-2210.751 -2210.751] [0.0000], Avg: [-832.37 -832.37] (0.631)
Step: 4649, Reward: [-2051.157 -2051.157] [0.0000], Avg: [-845.475 -845.475] (0.627)
Step: 4699, Reward: [-2184.359 -2184.359] [0.0000], Avg: [-859.718 -859.718] (0.624)
Step: 4749, Reward: [-3359.276 -3359.276] [0.0000], Avg: [-886.029 -886.029] (0.621)
Step: 4799, Reward: [-1813.345 -1813.345] [0.0000], Avg: [-895.689 -895.689] (0.618)
Step: 4849, Reward: [-1457.405 -1457.405] [0.0000], Avg: [-901.48 -901.48] (0.615)
Step: 4899, Reward: [-912.71 -912.71] [0.0000], Avg: [-901.594 -901.594] (0.612)
Step: 4949, Reward: [-266.132 -266.132] [0.0000], Avg: [-895.176 -895.176] (0.609)
Step: 4999, Reward: [-459.146 -459.146] [0.0000], Avg: [-890.815 -890.815] (0.606)
Step: 5049, Reward: [-624.255 -624.255] [0.0000], Avg: [-888.176 -888.176] (0.603)
Step: 5099, Reward: [-90.185 -90.185] [0.0000], Avg: [-880.353 -880.353] (0.600)
Step: 5149, Reward: [-1646.446 -1646.446] [0.0000], Avg: [-887.79 -887.79] (0.597)
Step: 5199, Reward: [-2029.691 -2029.691] [0.0000], Avg: [-898.77 -898.77] (0.594)
Step: 5249, Reward: [-840.053 -840.053] [0.0000], Avg: [-898.211 -898.211] (0.591)
Step: 5299, Reward: [-1493.823 -1493.823] [0.0000], Avg: [-903.83 -903.83] (0.588)
Step: 5349, Reward: [-3089.799 -3089.799] [0.0000], Avg: [-924.26 -924.26] (0.585)
Step: 5399, Reward: [-151.473 -151.473] [0.0000], Avg: [-917.104 -917.104] (0.582)
Step: 5449, Reward: [-258.757 -258.757] [0.0000], Avg: [-911.064 -911.064] (0.579)
Step: 5499, Reward: [-219.128 -219.128] [0.0000], Avg: [-904.774 -904.774] (0.576)
Step: 5549, Reward: [-116.226 -116.226] [0.0000], Avg: [-897.67 -897.67] (0.573)
Step: 5599, Reward: [-1718.841 -1718.841] [0.0000], Avg: [-905.002 -905.002] (0.570)
Step: 5649, Reward: [-484.574 -484.574] [0.0000], Avg: [-901.281 -901.281] (0.568)
Step: 5699, Reward: [-2788.313 -2788.313] [0.0000], Avg: [-917.834 -917.834] (0.565)
Step: 5749, Reward: [-4723.768 -4723.768] [0.0000], Avg: [-950.929 -950.929] (0.562)
Step: 5799, Reward: [-1254.674 -1254.674] [0.0000], Avg: [-953.548 -953.548] (0.559)
Step: 5849, Reward: [-1655.53 -1655.53] [0.0000], Avg: [-959.548 -959.548] (0.556)
Step: 5899, Reward: [-2422.286 -2422.286] [0.0000], Avg: [-971.944 -971.944] (0.554)
Step: 5949, Reward: [-774.202 -774.202] [0.0000], Avg: [-970.282 -970.282] (0.551)
Step: 5999, Reward: [-428.054 -428.054] [0.0000], Avg: [-965.763 -965.763] (0.548)
Step: 6049, Reward: [-1485.371 -1485.371] [0.0000], Avg: [-970.058 -970.058] (0.545)
Step: 6099, Reward: [-1122.413 -1122.413] [0.0000], Avg: [-971.306 -971.306] (0.543)
Step: 6149, Reward: [-1010.313 -1010.313] [0.0000], Avg: [-971.624 -971.624] (0.540)
Step: 6199, Reward: [-1133.603 -1133.603] [0.0000], Avg: [-972.93 -972.93] (0.537)
Step: 6249, Reward: [-588.021 -588.021] [0.0000], Avg: [-969.851 -969.851] (0.534)
Step: 6299, Reward: [-1039.659 -1039.659] [0.0000], Avg: [-970.405 -970.405] (0.532)
Step: 6349, Reward: [-279.192 -279.192] [0.0000], Avg: [-964.962 -964.962] (0.529)
Step: 6399, Reward: [-1724.813 -1724.813] [0.0000], Avg: [-970.898 -970.898] (0.526)
Step: 6449, Reward: [-1113.933 -1113.933] [0.0000], Avg: [-972.007 -972.007] (0.524)
Step: 6499, Reward: [-539.543 -539.543] [0.0000], Avg: [-968.681 -968.681] (0.521)
Step: 6549, Reward: [-1084.659 -1084.659] [0.0000], Avg: [-969.566 -969.566] (0.519)
Step: 6599, Reward: [-163.485 -163.485] [0.0000], Avg: [-963.459 -963.459] (0.516)
Step: 6649, Reward: [-1515.359 -1515.359] [0.0000], Avg: [-967.609 -967.609] (0.513)
Step: 6699, Reward: [-116.914 -116.914] [0.0000], Avg: [-961.26 -961.26] (0.511)
Step: 6749, Reward: [-1026.93 -1026.93] [0.0000], Avg: [-961.747 -961.747] (0.508)
Step: 6799, Reward: [-1069.252 -1069.252] [0.0000], Avg: [-962.537 -962.537] (0.506)
Step: 6849, Reward: [-1044.463 -1044.463] [0.0000], Avg: [-963.135 -963.135] (0.503)
Step: 6899, Reward: [-1648.805 -1648.805] [0.0000], Avg: [-968.104 -968.104] (0.501)
Step: 6949, Reward: [-39.822 -39.822] [0.0000], Avg: [-961.426 -961.426] (0.498)
Step: 6999, Reward: [-809.972 -809.972] [0.0000], Avg: [-960.344 -960.344] (0.496)
Step: 7049, Reward: [-1293.355 -1293.355] [0.0000], Avg: [-962.706 -962.706] (0.493)
Step: 7099, Reward: [-954.883 -954.883] [0.0000], Avg: [-962.65 -962.65] (0.491)
Step: 7149, Reward: [-216.667 -216.667] [0.0000], Avg: [-957.434 -957.434] (0.488)
Step: 7199, Reward: [-514.979 -514.979] [0.0000], Avg: [-954.361 -954.361] (0.486)
Step: 7249, Reward: [-1157.98 -1157.98] [0.0000], Avg: [-955.765 -955.765] (0.483)
Step: 7299, Reward: [-770.447 -770.447] [0.0000], Avg: [-954.496 -954.496] (0.481)
Step: 7349, Reward: [-1628.254 -1628.254] [0.0000], Avg: [-959.08 -959.08] (0.479)
Step: 7399, Reward: [-1446.144 -1446.144] [0.0000], Avg: [-962.371 -962.371] (0.476)
Step: 7449, Reward: [-1303.436 -1303.436] [0.0000], Avg: [-964.66 -964.66] (0.474)
Step: 7499, Reward: [-606.609 -606.609] [0.0000], Avg: [-962.273 -962.273] (0.471)
Step: 7549, Reward: [-645.304 -645.304] [0.0000], Avg: [-960.173 -960.173] (0.469)
Step: 7599, Reward: [-1386.687 -1386.687] [0.0000], Avg: [-962.979 -962.979] (0.467)
Step: 7649, Reward: [-854.409 -854.409] [0.0000], Avg: [-962.27 -962.27] (0.464)
Step: 7699, Reward: [-509.494 -509.494] [0.0000], Avg: [-959.33 -959.33] (0.462)
Step: 7749, Reward: [-1200.215 -1200.215] [0.0000], Avg: [-960.884 -960.884] (0.460)
Step: 7799, Reward: [-2473.103 -2473.103] [0.0000], Avg: [-970.578 -970.578] (0.458)
Step: 7849, Reward: [-1521.173 -1521.173] [0.0000], Avg: [-974.085 -974.085] (0.455)
Step: 7899, Reward: [-612.36 -612.36] [0.0000], Avg: [-971.795 -971.795] (0.453)
Step: 7949, Reward: [-2058.752 -2058.752] [0.0000], Avg: [-978.631 -978.631] (0.451)
Step: 7999, Reward: [-224.169 -224.169] [0.0000], Avg: [-973.916 -973.916] (0.448)
Step: 8049, Reward: [-641.607 -641.607] [0.0000], Avg: [-971.852 -971.852] (0.446)
Step: 8099, Reward: [-1371.439 -1371.439] [0.0000], Avg: [-974.318 -974.318] (0.444)
Step: 8149, Reward: [-2441.063 -2441.063] [0.0000], Avg: [-983.317 -983.317] (0.442)
Step: 8199, Reward: [-677.307 -677.307] [0.0000], Avg: [-981.451 -981.451] (0.440)
Step: 8249, Reward: [-2106.778 -2106.778] [0.0000], Avg: [-988.271 -988.271] (0.437)
Step: 8299, Reward: [-2294.174 -2294.174] [0.0000], Avg: [-996.138 -996.138] (0.435)
Step: 8349, Reward: [-1581.766 -1581.766] [0.0000], Avg: [-999.645 -999.645] (0.433)
Step: 8399, Reward: [-1203.007 -1203.007] [0.0000], Avg: [-1000.855 -1000.855] (0.431)
Step: 8449, Reward: [-554.092 -554.092] [0.0000], Avg: [-998.212 -998.212] (0.429)
Step: 8499, Reward: [-721.095 -721.095] [0.0000], Avg: [-996.582 -996.582] (0.427)
Step: 8549, Reward: [-819.464 -819.464] [0.0000], Avg: [-995.546 -995.546] (0.424)
Step: 8599, Reward: [-1264.843 -1264.843] [0.0000], Avg: [-997.112 -997.112] (0.422)
Step: 8649, Reward: [-1165.062 -1165.062] [0.0000], Avg: [-998.082 -998.082] (0.420)
Step: 8699, Reward: [-2795.091 -2795.091] [0.0000], Avg: [-1008.41 -1008.41] (0.418)
Step: 8749, Reward: [-1390.085 -1390.085] [0.0000], Avg: [-1010.591 -1010.591] (0.416)
Step: 8799, Reward: [-1403.931 -1403.931] [0.0000], Avg: [-1012.826 -1012.826] (0.414)
Step: 8849, Reward: [-1051.605 -1051.605] [0.0000], Avg: [-1013.045 -1013.045] (0.412)
Step: 8899, Reward: [-548.839 -548.839] [0.0000], Avg: [-1010.437 -1010.437] (0.410)
Step: 8949, Reward: [-1075.902 -1075.902] [0.0000], Avg: [-1010.803 -1010.803] (0.408)
Step: 8999, Reward: [-2425.565 -2425.565] [0.0000], Avg: [-1018.663 -1018.663] (0.406)
Step: 9049, Reward: [-1768.753 -1768.753] [0.0000], Avg: [-1022.807 -1022.807] (0.404)
Step: 9099, Reward: [-569.962 -569.962] [0.0000], Avg: [-1020.319 -1020.319] (0.402)
Step: 9149, Reward: [-449.204 -449.204] [0.0000], Avg: [-1017.198 -1017.198] (0.400)
Step: 9199, Reward: [-2055.873 -2055.873] [0.0000], Avg: [-1022.843 -1022.843] (0.398)
Step: 9249, Reward: [-324.218 -324.218] [0.0000], Avg: [-1019.066 -1019.066] (0.396)
Step: 9299, Reward: [-1668.625 -1668.625] [0.0000], Avg: [-1022.559 -1022.559] (0.394)
Step: 9349, Reward: [-838.439 -838.439] [0.0000], Avg: [-1021.574 -1021.574] (0.392)
Step: 9399, Reward: [-443.618 -443.618] [0.0000], Avg: [-1018.5 -1018.5] (0.390)
Step: 9449, Reward: [-2344.693 -2344.693] [0.0000], Avg: [-1025.517 -1025.517] (0.388)
Step: 9499, Reward: [-466.523 -466.523] [0.0000], Avg: [-1022.575 -1022.575] (0.386)
Step: 9549, Reward: [-870.455 -870.455] [0.0000], Avg: [-1021.778 -1021.778] (0.384)
Step: 9599, Reward: [-860.432 -860.432] [0.0000], Avg: [-1020.938 -1020.938] (0.382)
Step: 9649, Reward: [-1407.11 -1407.11] [0.0000], Avg: [-1022.939 -1022.939] (0.380)
Step: 9699, Reward: [-902.486 -902.486] [0.0000], Avg: [-1022.318 -1022.318] (0.378)
Step: 9749, Reward: [-688.128 -688.128] [0.0000], Avg: [-1020.604 -1020.604] (0.376)
Step: 9799, Reward: [-1220.146 -1220.146] [0.0000], Avg: [-1021.622 -1021.622] (0.374)
Step: 9849, Reward: [-1121.673 -1121.673] [0.0000], Avg: [-1022.13 -1022.13] (0.373)
Step: 9899, Reward: [-2030.796 -2030.796] [0.0000], Avg: [-1027.224 -1027.224] (0.371)
Step: 9949, Reward: [-1820.938 -1820.938] [0.0000], Avg: [-1031.213 -1031.213] (0.369)
Step: 9999, Reward: [-898.347 -898.347] [0.0000], Avg: [-1030.548 -1030.548] (0.367)
Step: 10049, Reward: [-1415.432 -1415.432] [0.0000], Avg: [-1032.463 -1032.463] (0.365)
Step: 10099, Reward: [-953.166 -953.166] [0.0000], Avg: [-1032.071 -1032.071] (0.363)
Step: 10149, Reward: [-976.916 -976.916] [0.0000], Avg: [-1031.799 -1031.799] (0.361)
Step: 10199, Reward: [-2473.15 -2473.15] [0.0000], Avg: [-1038.864 -1038.864] (0.360)
Step: 10249, Reward: [-723.427 -723.427] [0.0000], Avg: [-1037.326 -1037.326] (0.358)
Step: 10299, Reward: [-740.521 -740.521] [0.0000], Avg: [-1035.885 -1035.885] (0.356)
Step: 10349, Reward: [-1477.358 -1477.358] [0.0000], Avg: [-1038.018 -1038.018] (0.354)
Step: 10399, Reward: [-972.746 -972.746] [0.0000], Avg: [-1037.704 -1037.704] (0.353)
Step: 10449, Reward: [-1361.551 -1361.551] [0.0000], Avg: [-1039.253 -1039.253] (0.351)
Step: 10499, Reward: [-550.571 -550.571] [0.0000], Avg: [-1036.926 -1036.926] (0.349)
Step: 10549, Reward: [-1816.104 -1816.104] [0.0000], Avg: [-1040.619 -1040.619] (0.347)
Step: 10599, Reward: [-1620.667 -1620.667] [0.0000], Avg: [-1043.355 -1043.355] (0.346)
Step: 10649, Reward: [-626.432 -626.432] [0.0000], Avg: [-1041.398 -1041.398] (0.344)
Step: 10699, Reward: [-922.935 -922.935] [0.0000], Avg: [-1040.844 -1040.844] (0.342)
Step: 10749, Reward: [-1032.928 -1032.928] [0.0000], Avg: [-1040.807 -1040.807] (0.340)
Step: 10799, Reward: [-1025.76 -1025.76] [0.0000], Avg: [-1040.738 -1040.738] (0.339)
Step: 10849, Reward: [-671.784 -671.784] [0.0000], Avg: [-1039.037 -1039.037] (0.337)
Step: 10899, Reward: [-1961.615 -1961.615] [0.0000], Avg: [-1043.269 -1043.269] (0.335)
Step: 10949, Reward: [-2504.396 -2504.396] [0.0000], Avg: [-1049.941 -1049.941] (0.334)
Step: 10999, Reward: [-645.216 -645.216] [0.0000], Avg: [-1048.102 -1048.102] (0.332)
Step: 11049, Reward: [-2784.519 -2784.519] [0.0000], Avg: [-1055.959 -1055.959] (0.330)
Step: 11099, Reward: [-1341.902 -1341.902] [0.0000], Avg: [-1057.247 -1057.247] (0.329)
Step: 11149, Reward: [-423.89 -423.89] [0.0000], Avg: [-1054.407 -1054.407] (0.327)
Step: 11199, Reward: [-236.015 -236.015] [0.0000], Avg: [-1050.753 -1050.753] (0.325)
Step: 11249, Reward: [-1663.435 -1663.435] [0.0000], Avg: [-1053.476 -1053.476] (0.324)
Step: 11299, Reward: [-1800.659 -1800.659] [0.0000], Avg: [-1056.782 -1056.782] (0.322)
Step: 11349, Reward: [-1690.127 -1690.127] [0.0000], Avg: [-1059.572 -1059.572] (0.321)
Step: 11399, Reward: [-1630.356 -1630.356] [0.0000], Avg: [-1062.076 -1062.076] (0.319)
Step: 11449, Reward: [-2188.149 -2188.149] [0.0000], Avg: [-1066.993 -1066.993] (0.317)
Step: 11499, Reward: [-1935.395 -1935.395] [0.0000], Avg: [-1070.769 -1070.769] (0.316)
Step: 11549, Reward: [-1796.055 -1796.055] [0.0000], Avg: [-1073.908 -1073.908] (0.314)
Step: 11599, Reward: [-2141.948 -2141.948] [0.0000], Avg: [-1078.512 -1078.512] (0.313)
Step: 11649, Reward: [-1707.724 -1707.724] [0.0000], Avg: [-1081.213 -1081.213] (0.311)
Step: 11699, Reward: [-866.641 -866.641] [0.0000], Avg: [-1080.296 -1080.296] (0.309)
Step: 11749, Reward: [-1468.393 -1468.393] [0.0000], Avg: [-1081.947 -1081.947] (0.308)
Step: 11799, Reward: [-1047.669 -1047.669] [0.0000], Avg: [-1081.802 -1081.802] (0.306)
Step: 11849, Reward: [-521.829 -521.829] [0.0000], Avg: [-1079.439 -1079.439] (0.305)
Step: 11899, Reward: [-1624.663 -1624.663] [0.0000], Avg: [-1081.73 -1081.73] (0.303)
Step: 11949, Reward: [-1397.606 -1397.606] [0.0000], Avg: [-1083.052 -1083.052] (0.302)
Step: 11999, Reward: [-1477.792 -1477.792] [0.0000], Avg: [-1084.696 -1084.696] (0.300)
Step: 12049, Reward: [-1503.451 -1503.451] [0.0000], Avg: [-1086.434 -1086.434] (0.299)
Step: 12099, Reward: [-3186.581 -3186.581] [0.0000], Avg: [-1095.112 -1095.112] (0.297)
Step: 12149, Reward: [-1623.846 -1623.846] [0.0000], Avg: [-1097.288 -1097.288] (0.296)
Step: 12199, Reward: [-1895.708 -1895.708] [0.0000], Avg: [-1100.56 -1100.56] (0.294)
Step: 12249, Reward: [-1201.327 -1201.327] [0.0000], Avg: [-1100.972 -1100.972] (0.293)
Step: 12299, Reward: [-955.092 -955.092] [0.0000], Avg: [-1100.379 -1100.379] (0.291)
Step: 12349, Reward: [-2223.222 -2223.222] [0.0000], Avg: [-1104.924 -1104.924] (0.290)
Step: 12399, Reward: [-2246.085 -2246.085] [0.0000], Avg: [-1109.526 -1109.526] (0.288)
Step: 12449, Reward: [-2189.057 -2189.057] [0.0000], Avg: [-1113.861 -1113.861] (0.287)
Step: 12499, Reward: [-779.713 -779.713] [0.0000], Avg: [-1112.525 -1112.525] (0.286)
Step: 12549, Reward: [-1447.864 -1447.864] [0.0000], Avg: [-1113.861 -1113.861] (0.284)
Step: 12599, Reward: [-594.302 -594.302] [0.0000], Avg: [-1111.799 -1111.799] (0.283)
Step: 12649, Reward: [-3983.674 -3983.674] [0.0000], Avg: [-1123.15 -1123.15] (0.281)
Step: 12699, Reward: [-1461.34 -1461.34] [0.0000], Avg: [-1124.482 -1124.482] (0.280)
Step: 12749, Reward: [-1697.739 -1697.739] [0.0000], Avg: [-1126.73 -1126.73] (0.279)
Step: 12799, Reward: [-1250.55 -1250.55] [0.0000], Avg: [-1127.214 -1127.214] (0.277)
Step: 12849, Reward: [-3536.723 -3536.723] [0.0000], Avg: [-1136.589 -1136.589] (0.276)
Step: 12899, Reward: [-3094.991 -3094.991] [0.0000], Avg: [-1144.18 -1144.18] (0.274)
Step: 12949, Reward: [-1649.36 -1649.36] [0.0000], Avg: [-1146.13 -1146.13] (0.273)
Step: 12999, Reward: [-2359.208 -2359.208] [0.0000], Avg: [-1150.796 -1150.796] (0.272)
Step: 13049, Reward: [-2608.297 -2608.297] [0.0000], Avg: [-1156.38 -1156.38] (0.270)
Step: 13099, Reward: [-1184.831 -1184.831] [0.0000], Avg: [-1156.489 -1156.489] (0.269)
Step: 13149, Reward: [-363.375 -363.375] [0.0000], Avg: [-1153.473 -1153.473] (0.268)
Step: 13199, Reward: [-1131.516 -1131.516] [0.0000], Avg: [-1153.39 -1153.39] (0.266)
Step: 13249, Reward: [-1198.991 -1198.991] [0.0000], Avg: [-1153.562 -1153.562] (0.265)
Step: 13299, Reward: [-2236.011 -2236.011] [0.0000], Avg: [-1157.631 -1157.631] (0.264)
Step: 13349, Reward: [-2060.837 -2060.837] [0.0000], Avg: [-1161.014 -1161.014] (0.262)
Step: 13399, Reward: [-2867.798 -2867.798] [0.0000], Avg: [-1167.383 -1167.383] (0.261)
Step: 13449, Reward: [-1198.697 -1198.697] [0.0000], Avg: [-1167.499 -1167.499] (0.260)
Step: 13499, Reward: [-1545.877 -1545.877] [0.0000], Avg: [-1168.901 -1168.901] (0.258)
Step: 13549, Reward: [-1379.074 -1379.074] [0.0000], Avg: [-1169.676 -1169.676] (0.257)
Step: 13599, Reward: [-2317.102 -2317.102] [0.0000], Avg: [-1173.895 -1173.895] (0.256)
Step: 13649, Reward: [-780.955 -780.955] [0.0000], Avg: [-1172.455 -1172.455] (0.255)
Step: 13699, Reward: [-1833.608 -1833.608] [0.0000], Avg: [-1174.868 -1174.868] (0.253)
Step: 13749, Reward: [-1072.152 -1072.152] [0.0000], Avg: [-1174.495 -1174.495] (0.252)
Step: 13799, Reward: [-469.901 -469.901] [0.0000], Avg: [-1171.942 -1171.942] (0.251)
Step: 13849, Reward: [-3426.051 -3426.051] [0.0000], Avg: [-1180.08 -1180.08] (0.249)
Step: 13899, Reward: [-1096.508 -1096.508] [0.0000], Avg: [-1179.779 -1179.779] (0.248)
Step: 13949, Reward: [-1342.379 -1342.379] [0.0000], Avg: [-1180.362 -1180.362] (0.247)
Step: 13999, Reward: [-2181.318 -2181.318] [0.0000], Avg: [-1183.937 -1183.937] (0.246)
Step: 14049, Reward: [-2188.652 -2188.652] [0.0000], Avg: [-1187.512 -1187.512] (0.245)
Step: 14099, Reward: [-892.992 -892.992] [0.0000], Avg: [-1186.468 -1186.468] (0.243)
Step: 14149, Reward: [-2977.753 -2977.753] [0.0000], Avg: [-1192.797 -1192.797] (0.242)
Step: 14199, Reward: [-1551.485 -1551.485] [0.0000], Avg: [-1194.06 -1194.06] (0.241)
Step: 14249, Reward: [-2631.905 -2631.905] [0.0000], Avg: [-1199.105 -1199.105] (0.240)
Step: 14299, Reward: [-1516.935 -1516.935] [0.0000], Avg: [-1200.217 -1200.217] (0.238)
Step: 14349, Reward: [-2023.182 -2023.182] [0.0000], Avg: [-1203.084 -1203.084] (0.237)
Step: 14399, Reward: [-2355.539 -2355.539] [0.0000], Avg: [-1207.086 -1207.086] (0.236)
Step: 14449, Reward: [-1977.709 -1977.709] [0.0000], Avg: [-1209.752 -1209.752] (0.235)
Step: 14499, Reward: [-2390.48 -2390.48] [0.0000], Avg: [-1213.824 -1213.824] (0.234)
Step: 14549, Reward: [-3241.318 -3241.318] [0.0000], Avg: [-1220.791 -1220.791] (0.233)
Step: 14599, Reward: [-1046.489 -1046.489] [0.0000], Avg: [-1220.194 -1220.194] (0.231)
Step: 14649, Reward: [-2073.801 -2073.801] [0.0000], Avg: [-1223.107 -1223.107] (0.230)
Step: 14699, Reward: [-4083.971 -4083.971] [0.0000], Avg: [-1232.838 -1232.838] (0.229)
Step: 14749, Reward: [-445.44 -445.44] [0.0000], Avg: [-1230.169 -1230.169] (0.228)
Step: 14799, Reward: [-1497.524 -1497.524] [0.0000], Avg: [-1231.072 -1231.072] (0.227)
Step: 14849, Reward: [-1308.406 -1308.406] [0.0000], Avg: [-1231.333 -1231.333] (0.226)
Step: 14899, Reward: [-1771.021 -1771.021] [0.0000], Avg: [-1233.144 -1233.144] (0.225)
Step: 14949, Reward: [-1790.518 -1790.518] [0.0000], Avg: [-1235.008 -1235.008] (0.223)
Step: 14999, Reward: [-1631.002 -1631.002] [0.0000], Avg: [-1236.328 -1236.328] (0.222)
Step: 15049, Reward: [-3250.288 -3250.288] [0.0000], Avg: [-1243.019 -1243.019] (0.221)
Step: 15099, Reward: [-2686.443 -2686.443] [0.0000], Avg: [-1247.798 -1247.798] (0.220)
Step: 15149, Reward: [-1509.96 -1509.96] [0.0000], Avg: [-1248.664 -1248.664] (0.219)
Step: 15199, Reward: [-2293.019 -2293.019] [0.0000], Avg: [-1252.099 -1252.099] (0.218)
Step: 15249, Reward: [-1748.283 -1748.283] [0.0000], Avg: [-1253.726 -1253.726] (0.217)
Step: 15299, Reward: [-1382.404 -1382.404] [0.0000], Avg: [-1254.146 -1254.146] (0.216)
Step: 15349, Reward: [-3915.228 -3915.228] [0.0000], Avg: [-1262.814 -1262.814] (0.215)
Step: 15399, Reward: [-1798.654 -1798.654] [0.0000], Avg: [-1264.554 -1264.554] (0.214)
Step: 15449, Reward: [-2802.87 -2802.87] [0.0000], Avg: [-1269.532 -1269.532] (0.212)
Step: 15499, Reward: [-1741.664 -1741.664] [0.0000], Avg: [-1271.055 -1271.055] (0.211)
Step: 15549, Reward: [-1119.509 -1119.509] [0.0000], Avg: [-1270.568 -1270.568] (0.210)
Step: 15599, Reward: [-1965.755 -1965.755] [0.0000], Avg: [-1272.796 -1272.796] (0.209)
Step: 15649, Reward: [-1433.294 -1433.294] [0.0000], Avg: [-1273.309 -1273.309] (0.208)
Step: 15699, Reward: [-1715.127 -1715.127] [0.0000], Avg: [-1274.716 -1274.716] (0.207)
Step: 15749, Reward: [-1585.252 -1585.252] [0.0000], Avg: [-1275.702 -1275.702] (0.206)
Step: 15799, Reward: [-2354.244 -2354.244] [0.0000], Avg: [-1279.115 -1279.115] (0.205)
Step: 15849, Reward: [-1266.562 -1266.562] [0.0000], Avg: [-1279.075 -1279.075] (0.204)
Step: 15899, Reward: [-2471.562 -2471.562] [0.0000], Avg: [-1282.825 -1282.825] (0.203)
Step: 15949, Reward: [-1153.166 -1153.166] [0.0000], Avg: [-1282.419 -1282.419] (0.202)
Step: 15999, Reward: [-1474.358 -1474.358] [0.0000], Avg: [-1283.019 -1283.019] (0.201)
Step: 16049, Reward: [-668.484 -668.484] [0.0000], Avg: [-1281.104 -1281.104] (0.200)
Step: 16099, Reward: [-2731.707 -2731.707] [0.0000], Avg: [-1285.609 -1285.609] (0.199)
Step: 16149, Reward: [-1248.858 -1248.858] [0.0000], Avg: [-1285.496 -1285.496] (0.198)
Step: 16199, Reward: [-2129.066 -2129.066] [0.0000], Avg: [-1288.099 -1288.099] (0.197)
Step: 16249, Reward: [-2630.051 -2630.051] [0.0000], Avg: [-1292.228 -1292.228] (0.196)
Step: 16299, Reward: [-1232.402 -1232.402] [0.0000], Avg: [-1292.045 -1292.045] (0.195)
Step: 16349, Reward: [-2209.142 -2209.142] [0.0000], Avg: [-1294.849 -1294.849] (0.194)
Step: 16399, Reward: [-1492.204 -1492.204] [0.0000], Avg: [-1295.451 -1295.451] (0.193)
Step: 16449, Reward: [-3114.24 -3114.24] [0.0000], Avg: [-1300.979 -1300.979] (0.192)
Step: 16499, Reward: [-1243.502 -1243.502] [0.0000], Avg: [-1300.805 -1300.805] (0.191)
Step: 16549, Reward: [-1577.234 -1577.234] [0.0000], Avg: [-1301.64 -1301.64] (0.190)
Step: 16599, Reward: [-1762.102 -1762.102] [0.0000], Avg: [-1303.027 -1303.027] (0.189)
Step: 16649, Reward: [-1822.297 -1822.297] [0.0000], Avg: [-1304.586 -1304.586] (0.188)
Step: 16699, Reward: [-1991.484 -1991.484] [0.0000], Avg: [-1306.643 -1306.643] (0.187)
Step: 16749, Reward: [-1217.772 -1217.772] [0.0000], Avg: [-1306.378 -1306.378] (0.187)
Step: 16799, Reward: [-2858.615 -2858.615] [0.0000], Avg: [-1310.998 -1310.998] (0.186)
Step: 16849, Reward: [-3472.065 -3472.065] [0.0000], Avg: [-1317.41 -1317.41] (0.185)
Step: 16899, Reward: [-2423.253 -2423.253] [0.0000], Avg: [-1320.682 -1320.682] (0.184)
Step: 16949, Reward: [-1828.99 -1828.99] [0.0000], Avg: [-1322.181 -1322.181] (0.183)
Step: 16999, Reward: [-1030.509 -1030.509] [0.0000], Avg: [-1321.323 -1321.323] (0.182)
Step: 17049, Reward: [-2011.342 -2011.342] [0.0000], Avg: [-1323.347 -1323.347] (0.181)
Step: 17099, Reward: [-1693.191 -1693.191] [0.0000], Avg: [-1324.428 -1324.428] (0.180)
Step: 17149, Reward: [-3165.844 -3165.844] [0.0000], Avg: [-1329.797 -1329.797] (0.179)
Step: 17199, Reward: [-2846.089 -2846.089] [0.0000], Avg: [-1334.205 -1334.205] (0.178)
Step: 17249, Reward: [-900.738 -900.738] [0.0000], Avg: [-1332.948 -1332.948] (0.177)
Step: 17299, Reward: [-2574.087 -2574.087] [0.0000], Avg: [-1336.535 -1336.535] (0.177)
Step: 17349, Reward: [-2113.298 -2113.298] [0.0000], Avg: [-1338.774 -1338.774] (0.176)
Step: 17399, Reward: [-1735.849 -1735.849] [0.0000], Avg: [-1339.915 -1339.915] (0.175)
Step: 17449, Reward: [-1258.727 -1258.727] [0.0000], Avg: [-1339.682 -1339.682] (0.174)
Step: 17499, Reward: [-2266.005 -2266.005] [0.0000], Avg: [-1342.329 -1342.329] (0.173)
Step: 17549, Reward: [-1679.83 -1679.83] [0.0000], Avg: [-1343.291 -1343.291] (0.172)
Step: 17599, Reward: [-3124.058 -3124.058] [0.0000], Avg: [-1348.35 -1348.35] (0.171)
Step: 17649, Reward: [-1866.153 -1866.153] [0.0000], Avg: [-1349.816 -1349.816] (0.170)
Step: 17699, Reward: [-2148.123 -2148.123] [0.0000], Avg: [-1352.071 -1352.071] (0.170)
Step: 17749, Reward: [-2233.376 -2233.376] [0.0000], Avg: [-1354.554 -1354.554] (0.169)
Step: 17799, Reward: [-3076.543 -3076.543] [0.0000], Avg: [-1359.391 -1359.391] (0.168)
Step: 17849, Reward: [-3583.298 -3583.298] [0.0000], Avg: [-1365.621 -1365.621] (0.167)
Step: 17899, Reward: [-2412.173 -2412.173] [0.0000], Avg: [-1368.544 -1368.544] (0.166)
Step: 17949, Reward: [-1113.212 -1113.212] [0.0000], Avg: [-1367.833 -1367.833] (0.165)
Step: 17999, Reward: [-1396.3 -1396.3] [0.0000], Avg: [-1367.912 -1367.912] (0.165)
Step: 18049, Reward: [-1983.691 -1983.691] [0.0000], Avg: [-1369.617 -1369.617] (0.164)
Step: 18099, Reward: [-2191.56 -2191.56] [0.0000], Avg: [-1371.888 -1371.888] (0.163)
Step: 18149, Reward: [-863.305 -863.305] [0.0000], Avg: [-1370.487 -1370.487] (0.162)
Step: 18199, Reward: [-2306.642 -2306.642] [0.0000], Avg: [-1373.059 -1373.059] (0.161)
Step: 18249, Reward: [-3190.521 -3190.521] [0.0000], Avg: [-1378.038 -1378.038] (0.160)
Step: 18299, Reward: [-2392.371 -2392.371] [0.0000], Avg: [-1380.81 -1380.81] (0.160)
Step: 18349, Reward: [-1875.999 -1875.999] [0.0000], Avg: [-1382.159 -1382.159] (0.159)
Step: 18399, Reward: [-2374.798 -2374.798] [0.0000], Avg: [-1384.856 -1384.856] (0.158)
Step: 18449, Reward: [-2739.476 -2739.476] [0.0000], Avg: [-1388.527 -1388.527] (0.157)
Step: 18499, Reward: [-1881.335 -1881.335] [0.0000], Avg: [-1389.859 -1389.859] (0.157)
Step: 18549, Reward: [-2563.252 -2563.252] [0.0000], Avg: [-1393.022 -1393.022] (0.156)
Step: 18599, Reward: [-2504.076 -2504.076] [0.0000], Avg: [-1396.009 -1396.009] (0.155)
Step: 18649, Reward: [-2624.591 -2624.591] [0.0000], Avg: [-1399.302 -1399.302] (0.154)
Step: 18699, Reward: [-2026.154 -2026.154] [0.0000], Avg: [-1400.979 -1400.979] (0.153)
Step: 18749, Reward: [-3321.288 -3321.288] [0.0000], Avg: [-1406.099 -1406.099] (0.153)
Step: 18799, Reward: [-2141.112 -2141.112] [0.0000], Avg: [-1408.054 -1408.054] (0.152)
Step: 18849, Reward: [-1806.531 -1806.531] [0.0000], Avg: [-1409.111 -1409.111] (0.151)
Step: 18899, Reward: [-4155.87 -4155.87] [0.0000], Avg: [-1416.378 -1416.378] (0.150)
Step: 18949, Reward: [-3047.331 -3047.331] [0.0000], Avg: [-1420.681 -1420.681] (0.150)
Step: 18999, Reward: [-2365.442 -2365.442] [0.0000], Avg: [-1423.167 -1423.167] (0.149)
Step: 19049, Reward: [-1592.319 -1592.319] [0.0000], Avg: [-1423.611 -1423.611] (0.148)
Step: 19099, Reward: [-2570.678 -2570.678] [0.0000], Avg: [-1426.614 -1426.614] (0.147)
Step: 19149, Reward: [-1689.129 -1689.129] [0.0000], Avg: [-1427.299 -1427.299] (0.147)
Step: 19199, Reward: [-1710.587 -1710.587] [0.0000], Avg: [-1428.037 -1428.037] (0.146)
Step: 19249, Reward: [-3289.238 -3289.238] [0.0000], Avg: [-1432.871 -1432.871] (0.145)
Step: 19299, Reward: [-2359.509 -2359.509] [0.0000], Avg: [-1435.272 -1435.272] (0.144)
Step: 19349, Reward: [-1726.436 -1726.436] [0.0000], Avg: [-1436.024 -1436.024] (0.144)
Step: 19399, Reward: [-2083.304 -2083.304] [0.0000], Avg: [-1437.693 -1437.693] (0.143)
Step: 19449, Reward: [-4208.946 -4208.946] [0.0000], Avg: [-1444.817 -1444.817] (0.142)
Step: 19499, Reward: [-2312.519 -2312.519] [0.0000], Avg: [-1447.042 -1447.042] (0.142)
Step: 19549, Reward: [-2045.044 -2045.044] [0.0000], Avg: [-1448.571 -1448.571] (0.141)
Step: 19599, Reward: [-2235.187 -2235.187] [0.0000], Avg: [-1450.578 -1450.578] (0.140)
Step: 19649, Reward: [-1714.947 -1714.947] [0.0000], Avg: [-1451.25 -1451.25] (0.139)
Step: 19699, Reward: [-862.1 -862.1] [0.0000], Avg: [-1449.755 -1449.755] (0.139)
Step: 19749, Reward: [-2704.077 -2704.077] [0.0000], Avg: [-1452.931 -1452.931] (0.138)
Step: 19799, Reward: [-1488.254 -1488.254] [0.0000], Avg: [-1453.02 -1453.02] (0.137)
Step: 19849, Reward: [-2288.411 -2288.411] [0.0000], Avg: [-1455.124 -1455.124] (0.137)
Step: 19899, Reward: [-1673.43 -1673.43] [0.0000], Avg: [-1455.673 -1455.673] (0.136)
Step: 19949, Reward: [-1691.605 -1691.605] [0.0000], Avg: [-1456.264 -1456.264] (0.135)
Step: 19999, Reward: [-1672.584 -1672.584] [0.0000], Avg: [-1456.805 -1456.805] (0.135)
Step: 20049, Reward: [-2854.099 -2854.099] [0.0000], Avg: [-1460.289 -1460.289] (0.134)
Step: 20099, Reward: [-2657.635 -2657.635] [0.0000], Avg: [-1463.268 -1463.268] (0.133)
Step: 20149, Reward: [-2565.862 -2565.862] [0.0000], Avg: [-1466.004 -1466.004] (0.133)
Step: 20199, Reward: [-2941.415 -2941.415] [0.0000], Avg: [-1469.656 -1469.656] (0.132)
Step: 20249, Reward: [-2079.989 -2079.989] [0.0000], Avg: [-1471.163 -1471.163] (0.131)
Step: 20299, Reward: [-1536.21 -1536.21] [0.0000], Avg: [-1471.323 -1471.323] (0.131)
Step: 20349, Reward: [-2697.392 -2697.392] [0.0000], Avg: [-1474.335 -1474.335] (0.130)
Step: 20399, Reward: [-1477.609 -1477.609] [0.0000], Avg: [-1474.343 -1474.343] (0.129)
Step: 20449, Reward: [-3314.93 -3314.93] [0.0000], Avg: [-1478.844 -1478.844] (0.129)
Step: 20499, Reward: [-1733.884 -1733.884] [0.0000], Avg: [-1479.466 -1479.466] (0.128)
Step: 20549, Reward: [-903.378 -903.378] [0.0000], Avg: [-1478.064 -1478.064] (0.127)
Step: 20599, Reward: [-1730.068 -1730.068] [0.0000], Avg: [-1478.676 -1478.676] (0.127)
Step: 20649, Reward: [-2145.794 -2145.794] [0.0000], Avg: [-1480.291 -1480.291] (0.126)
Step: 20699, Reward: [-1895.059 -1895.059] [0.0000], Avg: [-1481.293 -1481.293] (0.126)
Step: 20749, Reward: [-1533.858 -1533.858] [0.0000], Avg: [-1481.419 -1481.419] (0.125)
Step: 20799, Reward: [-2284.936 -2284.936] [0.0000], Avg: [-1483.351 -1483.351] (0.124)
Step: 20849, Reward: [-1877.213 -1877.213] [0.0000], Avg: [-1484.295 -1484.295] (0.124)
Step: 20899, Reward: [-1438.143 -1438.143] [0.0000], Avg: [-1484.185 -1484.185] (0.123)
Step: 20949, Reward: [-1707.71 -1707.71] [0.0000], Avg: [-1484.718 -1484.718] (0.122)
Step: 20999, Reward: [-2670.831 -2670.831] [0.0000], Avg: [-1487.543 -1487.543] (0.122)
Step: 21049, Reward: [-1997.697 -1997.697] [0.0000], Avg: [-1488.754 -1488.754] (0.121)
Step: 21099, Reward: [-4762.04 -4762.04] [0.0000], Avg: [-1496.511 -1496.511] (0.121)
Step: 21149, Reward: [-1619.394 -1619.394] [0.0000], Avg: [-1496.801 -1496.801] (0.120)
Step: 21199, Reward: [-2194.311 -2194.311] [0.0000], Avg: [-1498.447 -1498.447] (0.119)
Step: 21249, Reward: [-2795.312 -2795.312] [0.0000], Avg: [-1501.498 -1501.498] (0.119)
Step: 21299, Reward: [-1159.095 -1159.095] [0.0000], Avg: [-1500.694 -1500.694] (0.118)
Step: 21349, Reward: [-1937.926 -1937.926] [0.0000], Avg: [-1501.718 -1501.718] (0.118)
Step: 21399, Reward: [-2651.634 -2651.634] [0.0000], Avg: [-1504.405 -1504.405] (0.117)
Step: 21449, Reward: [-2851.055 -2851.055] [0.0000], Avg: [-1507.544 -1507.544] (0.116)
Step: 21499, Reward: [-1344.359 -1344.359] [0.0000], Avg: [-1507.164 -1507.164] (0.116)
Step: 21549, Reward: [-3222.357 -3222.357] [0.0000], Avg: [-1511.144 -1511.144] (0.115)
Step: 21599, Reward: [-2621.727 -2621.727] [0.0000], Avg: [-1513.715 -1513.715] (0.115)
Step: 21649, Reward: [-2227.326 -2227.326] [0.0000], Avg: [-1515.363 -1515.363] (0.114)
Step: 21699, Reward: [-2671.38 -2671.38] [0.0000], Avg: [-1518.026 -1518.026] (0.114)
Step: 21749, Reward: [-3430.604 -3430.604] [0.0000], Avg: [-1522.423 -1522.423] (0.113)
Step: 21799, Reward: [-2021.671 -2021.671] [0.0000], Avg: [-1523.568 -1523.568] (0.112)
Step: 21849, Reward: [-2657.168 -2657.168] [0.0000], Avg: [-1526.162 -1526.162] (0.112)
Step: 21899, Reward: [-2653.392 -2653.392] [0.0000], Avg: [-1528.736 -1528.736] (0.111)
Step: 21949, Reward: [-3009.776 -3009.776] [0.0000], Avg: [-1532.11 -1532.11] (0.111)
Step: 21999, Reward: [-2932.526 -2932.526] [0.0000], Avg: [-1535.292 -1535.292] (0.110)
Step: 22049, Reward: [-2414.209 -2414.209] [0.0000], Avg: [-1537.285 -1537.285] (0.110)
Step: 22099, Reward: [-4323.687 -4323.687] [0.0000], Avg: [-1543.589 -1543.589] (0.109)
Step: 22149, Reward: [-1211.274 -1211.274] [0.0000], Avg: [-1542.839 -1542.839] (0.109)
Step: 22199, Reward: [-2309.029 -2309.029] [0.0000], Avg: [-1544.565 -1544.565] (0.108)
Step: 22249, Reward: [-2793.349 -2793.349] [0.0000], Avg: [-1547.371 -1547.371] (0.107)
Step: 22299, Reward: [-3609.062 -3609.062] [0.0000], Avg: [-1551.994 -1551.994] (0.107)
Step: 22349, Reward: [-1857.803 -1857.803] [0.0000], Avg: [-1552.678 -1552.678] (0.106)
Step: 22399, Reward: [-3024.633 -3024.633] [0.0000], Avg: [-1555.964 -1555.964] (0.106)
Step: 22449, Reward: [-3034.049 -3034.049] [0.0000], Avg: [-1559.256 -1559.256] (0.105)
Step: 22499, Reward: [-2063.039 -2063.039] [0.0000], Avg: [-1560.375 -1560.375] (0.105)
Step: 22549, Reward: [-1758.153 -1758.153] [0.0000], Avg: [-1560.814 -1560.814] (0.104)
Step: 22599, Reward: [-2722.234 -2722.234] [0.0000], Avg: [-1563.383 -1563.383] (0.104)
Step: 22649, Reward: [-2569.262 -2569.262] [0.0000], Avg: [-1565.604 -1565.604] (0.103)
Step: 22699, Reward: [-2751.232 -2751.232] [0.0000], Avg: [-1568.215 -1568.215] (0.103)
Step: 22749, Reward: [-3542.673 -3542.673] [0.0000], Avg: [-1572.555 -1572.555] (0.102)
Step: 22799, Reward: [-2360.554 -2360.554] [0.0000], Avg: [-1574.283 -1574.283] (0.102)
Step: 22849, Reward: [-3238.129 -3238.129] [0.0000], Avg: [-1577.923 -1577.923] (0.101)
Step: 22899, Reward: [-2882.212 -2882.212] [0.0000], Avg: [-1580.771 -1580.771] (0.101)
Step: 22949, Reward: [-2996.749 -2996.749] [0.0000], Avg: [-1583.856 -1583.856] (0.100)
Step: 22999, Reward: [-3593.499 -3593.499] [0.0000], Avg: [-1588.225 -1588.225] (0.100)
Step: 23049, Reward: [-2520.51 -2520.51] [0.0000], Avg: [-1590.247 -1590.247] (0.099)
Step: 23099, Reward: [-3355.853 -3355.853] [0.0000], Avg: [-1594.069 -1594.069] (0.099)
Step: 23149, Reward: [-2889.535 -2889.535] [0.0000], Avg: [-1596.867 -1596.867] (0.098)
Step: 23199, Reward: [-3485.659 -3485.659] [0.0000], Avg: [-1600.938 -1600.938] (0.098)
Step: 23249, Reward: [-2972.486 -2972.486] [0.0000], Avg: [-1603.887 -1603.887] (0.097)
Step: 23299, Reward: [-2996.549 -2996.549] [0.0000], Avg: [-1606.876 -1606.876] (0.097)
Step: 23349, Reward: [-3957.569 -3957.569] [0.0000], Avg: [-1611.909 -1611.909] (0.096)
Step: 23399, Reward: [-1656.994 -1656.994] [0.0000], Avg: [-1612.006 -1612.006] (0.096)
Step: 23449, Reward: [-2293.012 -2293.012] [0.0000], Avg: [-1613.458 -1613.458] (0.095)
Step: 23499, Reward: [-1823.147 -1823.147] [0.0000], Avg: [-1613.904 -1613.904] (0.095)
Step: 23549, Reward: [-2171.382 -2171.382] [0.0000], Avg: [-1615.087 -1615.087] (0.094)
Step: 23599, Reward: [-1600.449 -1600.449] [0.0000], Avg: [-1615.056 -1615.056] (0.094)
Step: 23649, Reward: [-1968.473 -1968.473] [0.0000], Avg: [-1615.804 -1615.804] (0.093)
Step: 23699, Reward: [-1755.973 -1755.973] [0.0000], Avg: [-1616.099 -1616.099] (0.093)
Step: 23749, Reward: [-2188.222 -2188.222] [0.0000], Avg: [-1617.304 -1617.304] (0.092)
Step: 23799, Reward: [-2824.86 -2824.86] [0.0000], Avg: [-1619.841 -1619.841] (0.092)
Step: 23849, Reward: [-4595.828 -4595.828] [0.0000], Avg: [-1626.08 -1626.08] (0.092)
Step: 23899, Reward: [-2786.615 -2786.615] [0.0000], Avg: [-1628.507 -1628.507] (0.091)
Step: 23949, Reward: [-1190.453 -1190.453] [0.0000], Avg: [-1627.593 -1627.593] (0.091)
Step: 23999, Reward: [-2824.709 -2824.709] [0.0000], Avg: [-1630.087 -1630.087] (0.090)
Step: 24049, Reward: [-1727.618 -1727.618] [0.0000], Avg: [-1630.29 -1630.29] (0.090)
Step: 24099, Reward: [-103.863 -103.863] [0.0000], Avg: [-1627.123 -1627.123] (0.089)
Step: 24149, Reward: [-332.086 -332.086] [0.0000], Avg: [-1624.442 -1624.442] (0.089)
Step: 24199, Reward: [-128.893 -128.893] [0.0000], Avg: [-1621.352 -1621.352] (0.088)
Step: 24249, Reward: [-115.179 -115.179] [0.0000], Avg: [-1618.246 -1618.246] (0.088)
Step: 24299, Reward: [-132.037 -132.037] [0.0000], Avg: [-1615.188 -1615.188] (0.088)
Step: 24349, Reward: [-44.652 -44.652] [0.0000], Avg: [-1611.963 -1611.963] (0.087)
Step: 24399, Reward: [-27.12 -27.12] [0.0000], Avg: [-1608.716 -1608.716] (0.087)
Step: 24449, Reward: [-197.372 -197.372] [0.0000], Avg: [-1605.829 -1605.829] (0.086)
Step: 24499, Reward: [-219.521 -219.521] [0.0000], Avg: [-1603. -1603.] (0.086)
Step: 24549, Reward: [-13.384 -13.384] [0.0000], Avg: [-1599.763 -1599.763] (0.085)
Step: 24599, Reward: [-57.609 -57.609] [0.0000], Avg: [-1596.628 -1596.628] (0.085)
Step: 24649, Reward: [-301.182 -301.182] [0.0000], Avg: [-1594.001 -1594.001] (0.084)
Step: 24699, Reward: [-211.338 -211.338] [0.0000], Avg: [-1591.202 -1591.202] (0.084)
Step: 24749, Reward: [-131.508 -131.508] [0.0000], Avg: [-1588.253 -1588.253] (0.084)
Step: 24799, Reward: [-29.906 -29.906] [0.0000], Avg: [-1585.111 -1585.111] (0.083)
Step: 24849, Reward: [-180.773 -180.773] [0.0000], Avg: [-1582.285 -1582.285] (0.083)
Step: 24899, Reward: [-50.416 -50.416] [0.0000], Avg: [-1579.209 -1579.209] (0.082)
Step: 24949, Reward: [-57.674 -57.674] [0.0000], Avg: [-1576.16 -1576.16] (0.082)
Step: 24999, Reward: [-1.987 -1.987] [0.0000], Avg: [-1573.012 -1573.012] (0.082)
Step: 25049, Reward: [-335.62 -335.62] [0.0000], Avg: [-1570.542 -1570.542] (0.081)
Step: 25099, Reward: [-2.98 -2.98] [0.0000], Avg: [-1567.419 -1567.419] (0.081)
Step: 25149, Reward: [-26.42 -26.42] [0.0000], Avg: [-1564.356 -1564.356] (0.080)
Step: 25199, Reward: [-60.879 -60.879] [0.0000], Avg: [-1561.373 -1561.373] (0.080)
Step: 25249, Reward: [-53.134 -53.134] [0.0000], Avg: [-1558.386 -1558.386] (0.080)
Step: 25299, Reward: [-65.851 -65.851] [0.0000], Avg: [-1555.436 -1555.436] (0.079)
Step: 25349, Reward: [-13.096 -13.096] [0.0000], Avg: [-1552.394 -1552.394] (0.079)
Step: 25399, Reward: [-61.867 -61.867] [0.0000], Avg: [-1549.46 -1549.46] (0.078)
Step: 25449, Reward: [-127.657 -127.657] [0.0000], Avg: [-1546.667 -1546.667] (0.078)
Step: 25499, Reward: [-146.9 -146.9] [0.0000], Avg: [-1543.922 -1543.922] (0.078)
Step: 25549, Reward: [-385.828 -385.828] [0.0000], Avg: [-1541.656 -1541.656] (0.077)
Step: 25599, Reward: [-256.935 -256.935] [0.0000], Avg: [-1539.147 -1539.147] (0.077)
Step: 25649, Reward: [-16.633 -16.633] [0.0000], Avg: [-1536.179 -1536.179] (0.076)
Step: 25699, Reward: [-217.734 -217.734] [0.0000], Avg: [-1533.614 -1533.614] (0.076)
Step: 25749, Reward: [-313.411 -313.411] [0.0000], Avg: [-1531.244 -1531.244] (0.076)
Step: 25799, Reward: [-46.06 -46.06] [0.0000], Avg: [-1528.366 -1528.366] (0.075)
Step: 25849, Reward: [-3.998 -3.998] [0.0000], Avg: [-1525.418 -1525.418] (0.075)
Step: 25899, Reward: [-271.625 -271.625] [0.0000], Avg: [-1522.997 -1522.997] (0.075)
Step: 25949, Reward: [-64.23 -64.23] [0.0000], Avg: [-1520.186 -1520.186] (0.074)
Step: 25999, Reward: [-108.083 -108.083] [0.0000], Avg: [-1517.471 -1517.471] (0.074)
Step: 26049, Reward: [-129.034 -129.034] [0.0000], Avg: [-1514.806 -1514.806] (0.073)
Step: 26099, Reward: [-103.517 -103.517] [0.0000], Avg: [-1512.102 -1512.102] (0.073)
Step: 26149, Reward: [-141.062 -141.062] [0.0000], Avg: [-1509.481 -1509.481] (0.073)
Step: 26199, Reward: [-140.779 -140.779] [0.0000], Avg: [-1506.869 -1506.869] (0.072)
Step: 26249, Reward: [-11.947 -11.947] [0.0000], Avg: [-1504.021 -1504.021] (0.072)
Step: 26299, Reward: [-331.799 -331.799] [0.0000], Avg: [-1501.793 -1501.793] (0.072)
Step: 26349, Reward: [-36.804 -36.804] [0.0000], Avg: [-1499.013 -1499.013] (0.071)
Step: 26399, Reward: [-72.641 -72.641] [0.0000], Avg: [-1496.311 -1496.311] (0.071)
Step: 26449, Reward: [-542.971 -542.971] [0.0000], Avg: [-1494.509 -1494.509] (0.071)
Step: 26499, Reward: [-16.956 -16.956] [0.0000], Avg: [-1491.721 -1491.721] (0.070)
Step: 26549, Reward: [-25.283 -25.283] [0.0000], Avg: [-1488.96 -1488.96] (0.070)
Step: 26599, Reward: [-171.458 -171.458] [0.0000], Avg: [-1486.483 -1486.483] (0.069)
Step: 26649, Reward: [-176.36 -176.36] [0.0000], Avg: [-1484.025 -1484.025] (0.069)
Step: 26699, Reward: [-213.437 -213.437] [0.0000], Avg: [-1481.646 -1481.646] (0.069)
Step: 26749, Reward: [-114.257 -114.257] [0.0000], Avg: [-1479.09 -1479.09] (0.068)
Step: 26799, Reward: [-302.697 -302.697] [0.0000], Avg: [-1476.895 -1476.895] (0.068)
Step: 26849, Reward: [-87.052 -87.052] [0.0000], Avg: [-1474.307 -1474.307] (0.068)
Step: 26899, Reward: [-251.944 -251.944] [0.0000], Avg: [-1472.035 -1472.035] (0.067)
Step: 26949, Reward: [-248.058 -248.058] [0.0000], Avg: [-1469.764 -1469.764] (0.067)
Step: 26999, Reward: [-83.175 -83.175] [0.0000], Avg: [-1467.196 -1467.196] (0.067)
Step: 27049, Reward: [-251.677 -251.677] [0.0000], Avg: [-1464.95 -1464.95] (0.066)
Step: 27099, Reward: [-23.114 -23.114] [0.0000], Avg: [-1462.289 -1462.289] (0.066)
Step: 27149, Reward: [-60.086 -60.086] [0.0000], Avg: [-1459.707 -1459.707] (0.066)
Step: 27199, Reward: [-8.979 -8.979] [0.0000], Avg: [-1457.04 -1457.04] (0.065)
Step: 27249, Reward: [-291.361 -291.361] [0.0000], Avg: [-1454.901 -1454.901] (0.065)
Step: 27299, Reward: [-102.991 -102.991] [0.0000], Avg: [-1452.425 -1452.425] (0.065)
Step: 27349, Reward: [-53.106 -53.106] [0.0000], Avg: [-1449.867 -1449.867] (0.064)
Step: 27399, Reward: [-250.562 -250.562] [0.0000], Avg: [-1447.679 -1447.679] (0.064)
Step: 27449, Reward: [-261.157 -261.157] [0.0000], Avg: [-1445.517 -1445.517] (0.064)
Step: 27499, Reward: [-14.09 -14.09] [0.0000], Avg: [-1442.915 -1442.915] (0.063)
Step: 27549, Reward: [-110.475 -110.475] [0.0000], Avg: [-1440.497 -1440.497] (0.063)
Step: 27599, Reward: [-413.169 -413.169] [0.0000], Avg: [-1438.635 -1438.635] (0.063)
Step: 27649, Reward: [-109.936 -109.936] [0.0000], Avg: [-1436.233 -1436.233] (0.063)
Step: 27699, Reward: [-63.186 -63.186] [0.0000], Avg: [-1433.754 -1433.754] (0.062)
Step: 27749, Reward: [-362.181 -362.181] [0.0000], Avg: [-1431.824 -1431.824] (0.062)
Step: 27799, Reward: [-47.266 -47.266] [0.0000], Avg: [-1429.333 -1429.333] (0.062)
Step: 27849, Reward: [-193.893 -193.893] [0.0000], Avg: [-1427.115 -1427.115] (0.061)
Step: 27899, Reward: [-275.942 -275.942] [0.0000], Avg: [-1425.052 -1425.052] (0.061)
Step: 27949, Reward: [-392.636 -392.636] [0.0000], Avg: [-1423.205 -1423.205] (0.061)
Step: 27999, Reward: [-62.346 -62.346] [0.0000], Avg: [-1420.775 -1420.775] (0.060)
Step: 28049, Reward: [-251.016 -251.016] [0.0000], Avg: [-1418.69 -1418.69] (0.060)
Step: 28099, Reward: [-57.816 -57.816] [0.0000], Avg: [-1416.269 -1416.269] (0.060)
Step: 28149, Reward: [-105.182 -105.182] [0.0000], Avg: [-1413.94 -1413.94] (0.059)
Step: 28199, Reward: [-138.781 -138.781] [0.0000], Avg: [-1411.679 -1411.679] (0.059)
Step: 28249, Reward: [-393.409 -393.409] [0.0000], Avg: [-1409.877 -1409.877] (0.059)
Step: 28299, Reward: [-460.172 -460.172] [0.0000], Avg: [-1408.199 -1408.199] (0.059)
Step: 28349, Reward: [-467.284 -467.284] [0.0000], Avg: [-1406.539 -1406.539] (0.058)
Step: 28399, Reward: [-95.069 -95.069] [0.0000], Avg: [-1404.23 -1404.23] (0.058)
Step: 28449, Reward: [-182.964 -182.964] [0.0000], Avg: [-1402.084 -1402.084] (0.058)
Step: 28499, Reward: [-15.376 -15.376] [0.0000], Avg: [-1399.651 -1399.651] (0.057)
Step: 28549, Reward: [-17.087 -17.087] [0.0000], Avg: [-1397.23 -1397.23] (0.057)
Step: 28599, Reward: [-242.616 -242.616] [0.0000], Avg: [-1395.211 -1395.211] (0.057)
Step: 28649, Reward: [-40.834 -40.834] [0.0000], Avg: [-1392.848 -1392.848] (0.057)
Step: 28699, Reward: [-290.108 -290.108] [0.0000], Avg: [-1390.927 -1390.927] (0.056)
Step: 28749, Reward: [-328.236 -328.236] [0.0000], Avg: [-1389.078 -1389.078] (0.056)
Step: 28799, Reward: [-36.191 -36.191] [0.0000], Avg: [-1386.73 -1386.73] (0.056)
Step: 28849, Reward: [-379.686 -379.686] [0.0000], Avg: [-1384.984 -1384.984] (0.055)
Step: 28899, Reward: [-367.283 -367.283] [0.0000], Avg: [-1383.224 -1383.224] (0.055)
Step: 28949, Reward: [-68.88 -68.88] [0.0000], Avg: [-1380.954 -1380.954] (0.055)
Step: 28999, Reward: [-235.97 -235.97] [0.0000], Avg: [-1378.98 -1378.98] (0.055)
Step: 29049, Reward: [-249.521 -249.521] [0.0000], Avg: [-1377.036 -1377.036] (0.054)
Step: 29099, Reward: [-28.651 -28.651] [0.0000], Avg: [-1374.719 -1374.719] (0.054)
Step: 29149, Reward: [-24.656 -24.656] [0.0000], Avg: [-1372.403 -1372.403] (0.054)
Step: 29199, Reward: [-235.095 -235.095] [0.0000], Avg: [-1370.456 -1370.456] (0.054)
Step: 29249, Reward: [-249.116 -249.116] [0.0000], Avg: [-1368.539 -1368.539] (0.053)
Step: 29299, Reward: [-217.731 -217.731] [0.0000], Avg: [-1366.575 -1366.575] (0.053)
Step: 29349, Reward: [-205.806 -205.806] [0.0000], Avg: [-1364.597 -1364.597] (0.053)
Step: 29399, Reward: [-128.28 -128.28] [0.0000], Avg: [-1362.495 -1362.495] (0.052)
Step: 29449, Reward: [-280.839 -280.839] [0.0000], Avg: [-1360.658 -1360.658] (0.052)
Step: 29499, Reward: [-39.046 -39.046] [0.0000], Avg: [-1358.418 -1358.418] (0.052)
Step: 29549, Reward: [-5.62 -5.62] [0.0000], Avg: [-1356.129 -1356.129] (0.052)
Step: 29599, Reward: [-291.454 -291.454] [0.0000], Avg: [-1354.331 -1354.331] (0.051)
Step: 29649, Reward: [-27.379 -27.379] [0.0000], Avg: [-1352.093 -1352.093] (0.051)
Step: 29699, Reward: [-177.381 -177.381] [0.0000], Avg: [-1350.116 -1350.116] (0.051)
Step: 29749, Reward: [-73.247 -73.247] [0.0000], Avg: [-1347.97 -1347.97] (0.051)
Step: 29799, Reward: [-176.939 -176.939] [0.0000], Avg: [-1346.005 -1346.005] (0.050)
Step: 29849, Reward: [-138.011 -138.011] [0.0000], Avg: [-1343.981 -1343.981] (0.050)
Step: 29899, Reward: [-23.759 -23.759] [0.0000], Avg: [-1341.774 -1341.774] (0.050)
Step: 29949, Reward: [-174.931 -174.931] [0.0000], Avg: [-1339.826 -1339.826] (0.050)
Step: 29999, Reward: [-151.786 -151.786] [0.0000], Avg: [-1337.846 -1337.846] (0.049)
Step: 30049, Reward: [-119.727 -119.727] [0.0000], Avg: [-1335.819 -1335.819] (0.049)
Step: 30099, Reward: [-33.539 -33.539] [0.0000], Avg: [-1333.656 -1333.656] (0.049)
Step: 30149, Reward: [-39.613 -39.613] [0.0000], Avg: [-1331.51 -1331.51] (0.049)
Step: 30199, Reward: [-13.711 -13.711] [0.0000], Avg: [-1329.328 -1329.328] (0.048)
Step: 30249, Reward: [-30.323 -30.323] [0.0000], Avg: [-1327.181 -1327.181] (0.048)
Step: 30299, Reward: [-105.601 -105.601] [0.0000], Avg: [-1325.165 -1325.165] (0.048)
Step: 30349, Reward: [-23.183 -23.183] [0.0000], Avg: [-1323.02 -1323.02] (0.048)
Step: 30399, Reward: [-156.723 -156.723] [0.0000], Avg: [-1321.102 -1321.102] (0.047)
Step: 30449, Reward: [-167.877 -167.877] [0.0000], Avg: [-1319.208 -1319.208] (0.047)
Step: 30499, Reward: [-227.532 -227.532] [0.0000], Avg: [-1317.418 -1317.418] (0.047)
Step: 30549, Reward: [-152.39 -152.39] [0.0000], Avg: [-1315.512 -1315.512] (0.047)
Step: 30599, Reward: [-188.22 -188.22] [0.0000], Avg: [-1313.67 -1313.67] (0.047)
Step: 30649, Reward: [-21.124 -21.124] [0.0000], Avg: [-1311.561 -1311.561] (0.046)
Step: 30699, Reward: [-240.295 -240.295] [0.0000], Avg: [-1309.816 -1309.816] (0.046)
Step: 30749, Reward: [-102.187 -102.187] [0.0000], Avg: [-1307.853 -1307.853] (0.046)
Step: 30799, Reward: [-118.311 -118.311] [0.0000], Avg: [-1305.922 -1305.922] (0.046)
Step: 30849, Reward: [-81.459 -81.459] [0.0000], Avg: [-1303.937 -1303.937] (0.045)
Step: 30899, Reward: [-241.002 -241.002] [0.0000], Avg: [-1302.217 -1302.217] (0.045)
Step: 30949, Reward: [-50.29 -50.29] [0.0000], Avg: [-1300.195 -1300.195] (0.045)
Step: 30999, Reward: [-279.835 -279.835] [0.0000], Avg: [-1298.549 -1298.549] (0.045)
Step: 31049, Reward: [-151.837 -151.837] [0.0000], Avg: [-1296.702 -1296.702] (0.044)
Step: 31099, Reward: [-65.217 -65.217] [0.0000], Avg: [-1294.722 -1294.722] (0.044)
Step: 31149, Reward: [-155.117 -155.117] [0.0000], Avg: [-1292.893 -1292.893] (0.044)
Step: 31199, Reward: [-196.833 -196.833] [0.0000], Avg: [-1291.137 -1291.137] (0.044)
Step: 31249, Reward: [-713.643 -713.643] [0.0000], Avg: [-1290.213 -1290.213] (0.044)
Step: 31299, Reward: [-165.8 -165.8] [0.0000], Avg: [-1288.417 -1288.417] (0.043)
Step: 31349, Reward: [-95.009 -95.009] [0.0000], Avg: [-1286.513 -1286.513] (0.043)
Step: 31399, Reward: [-117.134 -117.134] [0.0000], Avg: [-1284.651 -1284.651] (0.043)
Step: 31449, Reward: [-246.112 -246.112] [0.0000], Avg: [-1283. -1283.] (0.043)
Step: 31499, Reward: [-111.799 -111.799] [0.0000], Avg: [-1281.141 -1281.141] (0.043)
Step: 31549, Reward: [-81.39 -81.39] [0.0000], Avg: [-1279.24 -1279.24] (0.042)
Step: 31599, Reward: [-231.535 -231.535] [0.0000], Avg: [-1277.582 -1277.582] (0.042)
Step: 31649, Reward: [-10.487 -10.487] [0.0000], Avg: [-1275.58 -1275.58] (0.042)
Step: 31699, Reward: [-97.359 -97.359] [0.0000], Avg: [-1273.722 -1273.722] (0.042)
Step: 31749, Reward: [-25.203 -25.203] [0.0000], Avg: [-1271.756 -1271.756] (0.041)
Step: 31799, Reward: [-46.93 -46.93] [0.0000], Avg: [-1269.83 -1269.83] (0.041)
Step: 31849, Reward: [-25.808 -25.808] [0.0000], Avg: [-1267.877 -1267.877] (0.041)
Step: 31899, Reward: [-34.396 -34.396] [0.0000], Avg: [-1265.943 -1265.943] (0.041)
Step: 31949, Reward: [-4.44 -4.44] [0.0000], Avg: [-1263.969 -1263.969] (0.041)
Step: 31999, Reward: [-78.084 -78.084] [0.0000], Avg: [-1262.116 -1262.116] (0.040)
Step: 32049, Reward: [-49.024 -49.024] [0.0000], Avg: [-1260.224 -1260.224] (0.040)
Step: 32099, Reward: [-199.829 -199.829] [0.0000], Avg: [-1258.572 -1258.572] (0.040)
Step: 32149, Reward: [-149.622 -149.622] [0.0000], Avg: [-1256.847 -1256.847] (0.040)
Step: 32199, Reward: [-141.796 -141.796] [0.0000], Avg: [-1255.116 -1255.116] (0.040)
Step: 32249, Reward: [-464.059 -464.059] [0.0000], Avg: [-1253.89 -1253.89] (0.039)
Step: 32299, Reward: [-88.431 -88.431] [0.0000], Avg: [-1252.085 -1252.085] (0.039)
Step: 32349, Reward: [-2.872 -2.872] [0.0000], Avg: [-1250.155 -1250.155] (0.039)
Step: 32399, Reward: [-20.843 -20.843] [0.0000], Avg: [-1248.258 -1248.258] (0.039)
Step: 32449, Reward: [-30.43 -30.43] [0.0000], Avg: [-1246.381 -1246.381] (0.039)
Step: 32499, Reward: [-111.057 -111.057] [0.0000], Avg: [-1244.634 -1244.634] (0.038)
Step: 32549, Reward: [-39.758 -39.758] [0.0000], Avg: [-1242.784 -1242.784] (0.038)
Step: 32599, Reward: [-252.319 -252.319] [0.0000], Avg: [-1241.265 -1241.265] (0.038)
Step: 32649, Reward: [-70.553 -70.553] [0.0000], Avg: [-1239.472 -1239.472] (0.038)
Step: 32699, Reward: [-215.996 -215.996] [0.0000], Avg: [-1237.907 -1237.907] (0.038)
Step: 32749, Reward: [-0.037 -0.037] [0.0000], Avg: [-1236.017 -1236.017] (0.038)
Step: 32799, Reward: [-10.339 -10.339] [0.0000], Avg: [-1234.149 -1234.149] (0.037)
Step: 32849, Reward: [-104.448 -104.448] [0.0000], Avg: [-1232.429 -1232.429] (0.037)
Step: 32899, Reward: [-154.259 -154.259] [0.0000], Avg: [-1230.79 -1230.79] (0.037)
Step: 32949, Reward: [-137.484 -137.484] [0.0000], Avg: [-1229.131 -1229.131] (0.037)
Step: 32999, Reward: [-18.993 -18.993] [0.0000], Avg: [-1227.298 -1227.298] (0.037)
Step: 33049, Reward: [-17.958 -17.958] [0.0000], Avg: [-1225.468 -1225.468] (0.036)
Step: 33099, Reward: [-24.905 -24.905] [0.0000], Avg: [-1223.655 -1223.655] (0.036)
Step: 33149, Reward: [-200.211 -200.211] [0.0000], Avg: [-1222.111 -1222.111] (0.036)
Step: 33199, Reward: [-190.114 -190.114] [0.0000], Avg: [-1220.557 -1220.557] (0.036)
Step: 33249, Reward: [-76.423 -76.423] [0.0000], Avg: [-1218.836 -1218.836] (0.036)
Step: 33299, Reward: [-69.334 -69.334] [0.0000], Avg: [-1217.11 -1217.11] (0.035)
Step: 33349, Reward: [-62.903 -62.903] [0.0000], Avg: [-1215.38 -1215.38] (0.035)
Step: 33399, Reward: [-182.89 -182.89] [0.0000], Avg: [-1213.834 -1213.834] (0.035)
Step: 33449, Reward: [-9.795 -9.795] [0.0000], Avg: [-1212.035 -1212.035] (0.035)
Step: 33499, Reward: [-159.644 -159.644] [0.0000], Avg: [-1210.464 -1210.464] (0.035)
Step: 33549, Reward: [-350.159 -350.159] [0.0000], Avg: [-1209.182 -1209.182] (0.035)
Step: 33599, Reward: [-83.42 -83.42] [0.0000], Avg: [-1207.506 -1207.506] (0.034)
Step: 33649, Reward: [-49.353 -49.353] [0.0000], Avg: [-1205.786 -1205.786] (0.034)
Step: 33699, Reward: [-26.262 -26.262] [0.0000], Avg: [-1204.036 -1204.036] (0.034)
Step: 33749, Reward: [-15.11 -15.11] [0.0000], Avg: [-1202.274 -1202.274] (0.034)
Step: 33799, Reward: [-131.239 -131.239] [0.0000], Avg: [-1200.69 -1200.69] (0.034)
Step: 33849, Reward: [-80.872 -80.872] [0.0000], Avg: [-1199.036 -1199.036] (0.034)
Step: 33899, Reward: [-368.181 -368.181] [0.0000], Avg: [-1197.81 -1197.81] (0.033)
Step: 33949, Reward: [-115.52 -115.52] [0.0000], Avg: [-1196.216 -1196.216] (0.033)
Step: 33999, Reward: [-3.736 -3.736] [0.0000], Avg: [-1194.463 -1194.463] (0.033)
Step: 34049, Reward: [-65.775 -65.775] [0.0000], Avg: [-1192.805 -1192.805] (0.033)
Step: 34099, Reward: [-222.821 -222.821] [0.0000], Avg: [-1191.383 -1191.383] (0.033)
Step: 34149, Reward: [-32.123 -32.123] [0.0000], Avg: [-1189.686 -1189.686] (0.033)
Step: 34199, Reward: [-64.518 -64.518] [0.0000], Avg: [-1188.041 -1188.041] (0.032)
Step: 34249, Reward: [-22.714 -22.714] [0.0000], Avg: [-1186.34 -1186.34] (0.032)
Step: 34299, Reward: [-142.421 -142.421] [0.0000], Avg: [-1184.818 -1184.818] (0.032)
Step: 34349, Reward: [-173.653 -173.653] [0.0000], Avg: [-1183.346 -1183.346] (0.032)
Step: 34399, Reward: [-188.866 -188.866] [0.0000], Avg: [-1181.9 -1181.9] (0.032)
Step: 34449, Reward: [-390.282 -390.282] [0.0000], Avg: [-1180.752 -1180.752] (0.032)
Step: 34499, Reward: [-58.194 -58.194] [0.0000], Avg: [-1179.125 -1179.125] (0.031)
Step: 34549, Reward: [-228.3 -228.3] [0.0000], Avg: [-1177.749 -1177.749] (0.031)
Step: 34599, Reward: [-94.741 -94.741] [0.0000], Avg: [-1176.184 -1176.184] (0.031)
Step: 34649, Reward: [-126.506 -126.506] [0.0000], Avg: [-1174.669 -1174.669] (0.031)
Step: 34699, Reward: [-300.86 -300.86] [0.0000], Avg: [-1173.41 -1173.41] (0.031)
Step: 34749, Reward: [-121.626 -121.626] [0.0000], Avg: [-1171.896 -1171.896] (0.031)
Step: 34799, Reward: [-142.29 -142.29] [0.0000], Avg: [-1170.417 -1170.417] (0.031)
Step: 34849, Reward: [-39.489 -39.489] [0.0000], Avg: [-1168.795 -1168.795] (0.030)
Step: 34899, Reward: [-9.42 -9.42] [0.0000], Avg: [-1167.134 -1167.134] (0.030)
Step: 34949, Reward: [-39.696 -39.696] [0.0000], Avg: [-1165.521 -1165.521] (0.030)
Step: 34999, Reward: [-47.242 -47.242] [0.0000], Avg: [-1163.923 -1163.923] (0.030)
Step: 35049, Reward: [-124.33 -124.33] [0.0000], Avg: [-1162.44 -1162.44] (0.030)
Step: 35099, Reward: [-260.323 -260.323] [0.0000], Avg: [-1161.155 -1161.155] (0.030)
Step: 35149, Reward: [-338.992 -338.992] [0.0000], Avg: [-1159.986 -1159.986] (0.029)
Step: 35199, Reward: [-152.259 -152.259] [0.0000], Avg: [-1158.554 -1158.554] (0.029)
Step: 35249, Reward: [-91.618 -91.618] [0.0000], Avg: [-1157.041 -1157.041] (0.029)
Step: 35299, Reward: [-27.707 -27.707] [0.0000], Avg: [-1155.441 -1155.441] (0.029)
Step: 35349, Reward: [-77.15 -77.15] [0.0000], Avg: [-1153.916 -1153.916] (0.029)
Step: 35399, Reward: [-8.214 -8.214] [0.0000], Avg: [-1152.298 -1152.298] (0.029)
Step: 35449, Reward: [-546.306 -546.306] [0.0000], Avg: [-1151.443 -1151.443] (0.029)
Step: 35499, Reward: [-126.39 -126.39] [0.0000], Avg: [-1149.999 -1149.999] (0.028)
Step: 35549, Reward: [-152.267 -152.267] [0.0000], Avg: [-1148.596 -1148.596] (0.028)
Step: 35599, Reward: [-138.894 -138.894] [0.0000], Avg: [-1147.178 -1147.178] (0.028)
Step: 35649, Reward: [-476.023 -476.023] [0.0000], Avg: [-1146.237 -1146.237] (0.028)
Step: 35699, Reward: [-182.434 -182.434] [0.0000], Avg: [-1144.887 -1144.887] (0.028)
Step: 35749, Reward: [-20.756 -20.756] [0.0000], Avg: [-1143.314 -1143.314] (0.028)
Step: 35799, Reward: [-305.215 -305.215] [0.0000], Avg: [-1142.144 -1142.144] (0.028)
Step: 35849, Reward: [-22.366 -22.366] [0.0000], Avg: [-1140.582 -1140.582] (0.027)
Step: 35899, Reward: [-371.166 -371.166] [0.0000], Avg: [-1139.511 -1139.511] (0.027)
Step: 35949, Reward: [-357.266 -357.266] [0.0000], Avg: [-1138.423 -1138.423] (0.027)
Step: 35999, Reward: [-80.792 -80.792] [0.0000], Avg: [-1136.954 -1136.954] (0.027)
Step: 36049, Reward: [-23.955 -23.955] [0.0000], Avg: [-1135.41 -1135.41] (0.027)
Step: 36099, Reward: [-53.106 -53.106] [0.0000], Avg: [-1133.911 -1133.911] (0.027)
Step: 36149, Reward: [-11.923 -11.923] [0.0000], Avg: [-1132.359 -1132.359] (0.027)
Step: 36199, Reward: [-73.636 -73.636] [0.0000], Avg: [-1130.897 -1130.897] (0.027)
Step: 36249, Reward: [-23.643 -23.643] [0.0000], Avg: [-1129.37 -1129.37] (0.026)
Step: 36299, Reward: [-161.584 -161.584] [0.0000], Avg: [-1128.036 -1128.036] (0.026)
Step: 36349, Reward: [-371.842 -371.842] [0.0000], Avg: [-1126.996 -1126.996] (0.026)
Step: 36399, Reward: [-98.083 -98.083] [0.0000], Avg: [-1125.583 -1125.583] (0.026)
Step: 36449, Reward: [-64.585 -64.585] [0.0000], Avg: [-1124.128 -1124.128] (0.026)
Step: 36499, Reward: [-53.08 -53.08] [0.0000], Avg: [-1122.66 -1122.66] (0.026)
Step: 36549, Reward: [-42.81 -42.81] [0.0000], Avg: [-1121.183 -1121.183] (0.026)
Step: 36599, Reward: [-211.05 -211.05] [0.0000], Avg: [-1119.94 -1119.94] (0.025)
Step: 36649, Reward: [-70.12 -70.12] [0.0000], Avg: [-1118.508 -1118.508] (0.025)
Step: 36699, Reward: [-178.273 -178.273] [0.0000], Avg: [-1117.227 -1117.227] (0.025)
Step: 36749, Reward: [-8.144 -8.144] [0.0000], Avg: [-1115.718 -1115.718] (0.025)
Step: 36799, Reward: [-49.948 -49.948] [0.0000], Avg: [-1114.27 -1114.27] (0.025)
Step: 36849, Reward: [-185.909 -185.909] [0.0000], Avg: [-1113.01 -1113.01] (0.025)
Step: 36899, Reward: [-78.419 -78.419] [0.0000], Avg: [-1111.608 -1111.608] (0.025)
Step: 36949, Reward: [-67.095 -67.095] [0.0000], Avg: [-1110.195 -1110.195] (0.025)
Step: 36999, Reward: [-276.641 -276.641] [0.0000], Avg: [-1109.068 -1109.068] (0.024)
Step: 37049, Reward: [-193.099 -193.099] [0.0000], Avg: [-1107.832 -1107.832] (0.024)
Step: 37099, Reward: [-26.896 -26.896] [0.0000], Avg: [-1106.375 -1106.375] (0.024)
Step: 37149, Reward: [-169.51 -169.51] [0.0000], Avg: [-1105.114 -1105.114] (0.024)
Step: 37199, Reward: [-46.105 -46.105] [0.0000], Avg: [-1103.691 -1103.691] (0.024)
Step: 37249, Reward: [-7.719 -7.719] [0.0000], Avg: [-1102.22 -1102.22] (0.024)
Step: 37299, Reward: [-417.43 -417.43] [0.0000], Avg: [-1101.302 -1101.302] (0.024)
Step: 37349, Reward: [-38.723 -38.723] [0.0000], Avg: [-1099.879 -1099.879] (0.024)
Step: 37399, Reward: [-12.447 -12.447] [0.0000], Avg: [-1098.426 -1098.426] (0.024)
Step: 37449, Reward: [-279.418 -279.418] [0.0000], Avg: [-1097.332 -1097.332] (0.023)
Step: 37499, Reward: [-61.207 -61.207] [0.0000], Avg: [-1095.951 -1095.951] (0.023)
Step: 37549, Reward: [-141.228 -141.228] [0.0000], Avg: [-1094.679 -1094.679] (0.023)
Step: 37599, Reward: [-193.88 -193.88] [0.0000], Avg: [-1093.482 -1093.482] (0.023)
Step: 37649, Reward: [-190.204 -190.204] [0.0000], Avg: [-1092.282 -1092.282] (0.023)
Step: 37699, Reward: [-127.041 -127.041] [0.0000], Avg: [-1091.002 -1091.002] (0.023)
Step: 37749, Reward: [-115.413 -115.413] [0.0000], Avg: [-1089.71 -1089.71] (0.023)
Step: 37799, Reward: [-11.814 -11.814] [0.0000], Avg: [-1088.284 -1088.284] (0.023)
Step: 37849, Reward: [-80.277 -80.277] [0.0000], Avg: [-1086.952 -1086.952] (0.022)
Step: 37899, Reward: [-125.143 -125.143] [0.0000], Avg: [-1085.683 -1085.683] (0.022)
Step: 37949, Reward: [-100.733 -100.733] [0.0000], Avg: [-1084.386 -1084.386] (0.022)
Step: 37999, Reward: [-142.664 -142.664] [0.0000], Avg: [-1083.147 -1083.147] (0.022)
Step: 38049, Reward: [-0.516 -0.516] [0.0000], Avg: [-1081.724 -1081.724] (0.022)
Step: 38099, Reward: [-144.821 -144.821] [0.0000], Avg: [-1080.494 -1080.494] (0.022)
Step: 38149, Reward: [-80.752 -80.752] [0.0000], Avg: [-1079.184 -1079.184] (0.022)
Step: 38199, Reward: [-23.954 -23.954] [0.0000], Avg: [-1077.803 -1077.803] (0.022)
Step: 38249, Reward: [-296.76 -296.76] [0.0000], Avg: [-1076.782 -1076.782] (0.022)
Step: 38299, Reward: [-152.456 -152.456] [0.0000], Avg: [-1075.575 -1075.575] (0.022)
Step: 38349, Reward: [-37.868 -37.868] [0.0000], Avg: [-1074.222 -1074.222] (0.021)
Step: 38399, Reward: [-16.325 -16.325] [0.0000], Avg: [-1072.845 -1072.845] (0.021)
Step: 38449, Reward: [-39.935 -39.935] [0.0000], Avg: [-1071.502 -1071.502] (0.021)
Step: 38499, Reward: [-392.925 -392.925] [0.0000], Avg: [-1070.62 -1070.62] (0.021)
Step: 38549, Reward: [-2.615 -2.615] [0.0000], Avg: [-1069.235 -1069.235] (0.021)
Step: 38599, Reward: [-150.638 -150.638] [0.0000], Avg: [-1068.045 -1068.045] (0.021)
Step: 38649, Reward: [-63.014 -63.014] [0.0000], Avg: [-1066.745 -1066.745] (0.021)
Step: 38699, Reward: [-267.789 -267.789] [0.0000], Avg: [-1065.713 -1065.713] (0.021)
Step: 38749, Reward: [-271.574 -271.574] [0.0000], Avg: [-1064.688 -1064.688] (0.021)
Step: 38799, Reward: [-95.49 -95.49] [0.0000], Avg: [-1063.439 -1063.439] (0.020)
Step: 38849, Reward: [-215.825 -215.825] [0.0000], Avg: [-1062.348 -1062.348] (0.020)
Step: 38899, Reward: [-3.042 -3.042] [0.0000], Avg: [-1060.987 -1060.987] (0.020)
Step: 38949, Reward: [-18.973 -18.973] [0.0000], Avg: [-1059.649 -1059.649] (0.020)
Step: 38999, Reward: [-304.508 -304.508] [0.0000], Avg: [-1058.681 -1058.681] (0.020)
Step: 39049, Reward: [-99.764 -99.764] [0.0000], Avg: [-1057.453 -1057.453] (0.020)
Step: 39099, Reward: [-85.654 -85.654] [0.0000], Avg: [-1056.211 -1056.211] (0.020)
Step: 39149, Reward: [-2.377 -2.377] [0.0000], Avg: [-1054.865 -1054.865] (0.020)
Step: 39199, Reward: [-197.814 -197.814] [0.0000], Avg: [-1053.771 -1053.771] (0.020)
Step: 39249, Reward: [-42.749 -42.749] [0.0000], Avg: [-1052.484 -1052.484] (0.020)
Step: 39299, Reward: [-449.895 -449.895] [0.0000], Avg: [-1051.717 -1051.717] (0.020)
Step: 39349, Reward: [-94.663 -94.663] [0.0000], Avg: [-1050.501 -1050.501] (0.020)
Step: 39399, Reward: [-140.574 -140.574] [0.0000], Avg: [-1049.346 -1049.346] (0.020)
Step: 39449, Reward: [-42.346 -42.346] [0.0000], Avg: [-1048.07 -1048.07] (0.020)
Step: 39499, Reward: [-112.177 -112.177] [0.0000], Avg: [-1046.885 -1046.885] (0.020)
Step: 39549, Reward: [-62.985 -62.985] [0.0000], Avg: [-1045.641 -1045.641] (0.020)
Step: 39599, Reward: [-122.045 -122.045] [0.0000], Avg: [-1044.475 -1044.475] (0.020)
Step: 39649, Reward: [-205.371 -205.371] [0.0000], Avg: [-1043.417 -1043.417] (0.020)
Step: 39699, Reward: [-141.343 -141.343] [0.0000], Avg: [-1042.281 -1042.281] (0.020)
Step: 39749, Reward: [-54.396 -54.396] [0.0000], Avg: [-1041.038 -1041.038] (0.020)
Step: 39799, Reward: [-177.187 -177.187] [0.0000], Avg: [-1039.953 -1039.953] (0.020)
Step: 39849, Reward: [-0.354 -0.354] [0.0000], Avg: [-1038.649 -1038.649] (0.020)
Step: 39899, Reward: [-191.141 -191.141] [0.0000], Avg: [-1037.587 -1037.587] (0.020)
Step: 39949, Reward: [-174.18 -174.18] [0.0000], Avg: [-1036.506 -1036.506] (0.020)
Step: 39999, Reward: [-26.292 -26.292] [0.0000], Avg: [-1035.243 -1035.243] (0.020)
Step: 40049, Reward: [-8.189 -8.189] [0.0000], Avg: [-1033.961 -1033.961] (0.020)
Step: 40099, Reward: [-9.919 -9.919] [0.0000], Avg: [-1032.684 -1032.684] (0.020)
Step: 40149, Reward: [-64.483 -64.483] [0.0000], Avg: [-1031.478 -1031.478] (0.020)
Step: 40199, Reward: [-96.208 -96.208] [0.0000], Avg: [-1030.315 -1030.315] (0.020)
Step: 40249, Reward: [-221.242 -221.242] [0.0000], Avg: [-1029.31 -1029.31] (0.020)
Step: 40299, Reward: [-139.012 -139.012] [0.0000], Avg: [-1028.205 -1028.205] (0.020)
Step: 40349, Reward: [-92.164 -92.164] [0.0000], Avg: [-1027.046 -1027.046] (0.020)
Step: 40399, Reward: [-192.967 -192.967] [0.0000], Avg: [-1026.013 -1026.013] (0.020)
Step: 40449, Reward: [-141.083 -141.083] [0.0000], Avg: [-1024.919 -1024.919] (0.020)
Step: 40499, Reward: [-1.749 -1.749] [0.0000], Avg: [-1023.656 -1023.656] (0.020)
Step: 40549, Reward: [-107.484 -107.484] [0.0000], Avg: [-1022.527 -1022.527] (0.020)
Step: 40599, Reward: [-280.074 -280.074] [0.0000], Avg: [-1021.612 -1021.612] (0.020)
Step: 40649, Reward: [-0.455 -0.455] [0.0000], Avg: [-1020.356 -1020.356] (0.020)
Step: 40699, Reward: [-1.293 -1.293] [0.0000], Avg: [-1019.104 -1019.104] (0.020)
Step: 40749, Reward: [-359.549 -359.549] [0.0000], Avg: [-1018.295 -1018.295] (0.020)
Step: 40799, Reward: [-138.693 -138.693] [0.0000], Avg: [-1017.217 -1017.217] (0.020)
Step: 40849, Reward: [-45.879 -45.879] [0.0000], Avg: [-1016.028 -1016.028] (0.020)
Step: 40899, Reward: [-119.967 -119.967] [0.0000], Avg: [-1014.933 -1014.933] (0.020)
Step: 40949, Reward: [-81.047 -81.047] [0.0000], Avg: [-1013.792 -1013.792] (0.020)
Step: 40999, Reward: [-575.461 -575.461] [0.0000], Avg: [-1013.258 -1013.258] (0.020)
Step: 41049, Reward: [-137.778 -137.778] [0.0000], Avg: [-1012.192 -1012.192] (0.020)
Step: 41099, Reward: [-303.08 -303.08] [0.0000], Avg: [-1011.329 -1011.329] (0.020)
Step: 41149, Reward: [-17.057 -17.057] [0.0000], Avg: [-1010.121 -1010.121] (0.020)
Step: 41199, Reward: [-68.547 -68.547] [0.0000], Avg: [-1008.978 -1008.978] (0.020)
Step: 41249, Reward: [-10.097 -10.097] [0.0000], Avg: [-1007.767 -1007.767] (0.020)
Step: 41299, Reward: [-164.211 -164.211] [0.0000], Avg: [-1006.746 -1006.746] (0.020)
Step: 41349, Reward: [-136.609 -136.609] [0.0000], Avg: [-1005.694 -1005.694] (0.020)
Step: 41399, Reward: [-48.687 -48.687] [0.0000], Avg: [-1004.538 -1004.538] (0.020)
Step: 41449, Reward: [-218.698 -218.698] [0.0000], Avg: [-1003.59 -1003.59] (0.020)
Step: 41499, Reward: [-332.935 -332.935] [0.0000], Avg: [-1002.782 -1002.782] (0.020)
Step: 41549, Reward: [-241.74 -241.74] [0.0000], Avg: [-1001.866 -1001.866] (0.020)
Step: 41599, Reward: [-53.302 -53.302] [0.0000], Avg: [-1000.726 -1000.726] (0.020)
Step: 41649, Reward: [-406.811 -406.811] [0.0000], Avg: [-1000.013 -1000.013] (0.020)
Step: 41699, Reward: [-109.263 -109.263] [0.0000], Avg: [-998.945 -998.945] (0.020)
Step: 41749, Reward: [-314.888 -314.888] [0.0000], Avg: [-998.126 -998.126] (0.020)
Step: 41799, Reward: [-235.907 -235.907] [0.0000], Avg: [-997.214 -997.214] (0.020)
Step: 41849, Reward: [-24.254 -24.254] [0.0000], Avg: [-996.052 -996.052] (0.020)
Step: 41899, Reward: [-270.281 -270.281] [0.0000], Avg: [-995.186 -995.186] (0.020)
Step: 41949, Reward: [-54.615 -54.615] [0.0000], Avg: [-994.065 -994.065] (0.020)
Step: 41999, Reward: [-136.167 -136.167] [0.0000], Avg: [-993.043 -993.043] (0.020)
Step: 42049, Reward: [-41.458 -41.458] [0.0000], Avg: [-991.912 -991.912] (0.020)
Step: 42099, Reward: [-29.364 -29.364] [0.0000], Avg: [-990.769 -990.769] (0.020)
Step: 42149, Reward: [-11.196 -11.196] [0.0000], Avg: [-989.607 -989.607] (0.020)
Step: 42199, Reward: [-101.145 -101.145] [0.0000], Avg: [-988.554 -988.554] (0.020)
Step: 42249, Reward: [-7.69 -7.69] [0.0000], Avg: [-987.393 -987.393] (0.020)
Step: 42299, Reward: [-166.407 -166.407] [0.0000], Avg: [-986.423 -986.423] (0.020)
Step: 42349, Reward: [-64.75 -64.75] [0.0000], Avg: [-985.335 -985.335] (0.020)
Step: 42399, Reward: [-23.201 -23.201] [0.0000], Avg: [-984.2 -984.2] (0.020)
Step: 42449, Reward: [-314.227 -314.227] [0.0000], Avg: [-983.411 -983.411] (0.020)
Step: 42499, Reward: [-9.436 -9.436] [0.0000], Avg: [-982.265 -982.265] (0.020)
Step: 42549, Reward: [-3.603 -3.603] [0.0000], Avg: [-981.115 -981.115] (0.020)
Step: 42599, Reward: [-566.2 -566.2] [0.0000], Avg: [-980.628 -980.628] (0.020)
Step: 42649, Reward: [-12.498 -12.498] [0.0000], Avg: [-979.493 -979.493] (0.020)
Step: 42699, Reward: [-37.395 -37.395] [0.0000], Avg: [-978.39 -978.39] (0.020)
Step: 42749, Reward: [-192.014 -192.014] [0.0000], Avg: [-977.47 -977.47] (0.020)
Step: 42799, Reward: [-2.205 -2.205] [0.0000], Avg: [-976.331 -976.331] (0.020)
Step: 42849, Reward: [-336.432 -336.432] [0.0000], Avg: [-975.584 -975.584] (0.020)
Step: 42899, Reward: [-9.159 -9.159] [0.0000], Avg: [-974.458 -974.458] (0.020)
Step: 42949, Reward: [-18.746 -18.746] [0.0000], Avg: [-973.345 -973.345] (0.020)
Step: 42999, Reward: [-32.157 -32.157] [0.0000], Avg: [-972.251 -972.251] (0.020)
Step: 43049, Reward: [-8.984 -8.984] [0.0000], Avg: [-971.132 -971.132] (0.020)
Step: 43099, Reward: [-149.802 -149.802] [0.0000], Avg: [-970.179 -970.179] (0.020)
Step: 43149, Reward: [-87.719 -87.719] [0.0000], Avg: [-969.157 -969.157] (0.020)
Step: 43199, Reward: [-107.666 -107.666] [0.0000], Avg: [-968.16 -968.16] (0.020)
Step: 43249, Reward: [-78.534 -78.534] [0.0000], Avg: [-967.131 -967.131] (0.020)
Step: 43299, Reward: [-269.286 -269.286] [0.0000], Avg: [-966.325 -966.325] (0.020)
Step: 43349, Reward: [-86.633 -86.633] [0.0000], Avg: [-965.311 -965.311] (0.020)
Step: 43399, Reward: [-74.821 -74.821] [0.0000], Avg: [-964.285 -964.285] (0.020)
Step: 43449, Reward: [-63.152 -63.152] [0.0000], Avg: [-963.248 -963.248] (0.020)
Step: 43499, Reward: [-137.738 -137.738] [0.0000], Avg: [-962.299 -962.299] (0.020)
Step: 43549, Reward: [-231.77 -231.77] [0.0000], Avg: [-961.46 -961.46] (0.020)
Step: 43599, Reward: [-12.311 -12.311] [0.0000], Avg: [-960.372 -960.372] (0.020)
Step: 43649, Reward: [-19.73 -19.73] [0.0000], Avg: [-959.294 -959.294] (0.020)
Step: 43699, Reward: [-232.326 -232.326] [0.0000], Avg: [-958.462 -958.462] (0.020)
Step: 43749, Reward: [-231.584 -231.584] [0.0000], Avg: [-957.632 -957.632] (0.020)
Step: 43799, Reward: [-45.04 -45.04] [0.0000], Avg: [-956.59 -956.59] (0.020)
Step: 43849, Reward: [-159.876 -159.876] [0.0000], Avg: [-955.681 -955.681] (0.020)
Step: 43899, Reward: [-91.892 -91.892] [0.0000], Avg: [-954.698 -954.698] (0.020)
Step: 43949, Reward: [-440.384 -440.384] [0.0000], Avg: [-954.113 -954.113] (0.020)
Step: 43999, Reward: [-105.688 -105.688] [0.0000], Avg: [-953.148 -953.148] (0.020)
Step: 44049, Reward: [-34.702 -34.702] [0.0000], Avg: [-952.106 -952.106] (0.020)
Step: 44099, Reward: [-18.9 -18.9] [0.0000], Avg: [-951.048 -951.048] (0.020)
Step: 44149, Reward: [-141.77 -141.77] [0.0000], Avg: [-950.131 -950.131] (0.020)
Step: 44199, Reward: [-57.308 -57.308] [0.0000], Avg: [-949.121 -949.121] (0.020)
Step: 44249, Reward: [-52.649 -52.649] [0.0000], Avg: [-948.108 -948.108] (0.020)
Step: 44299, Reward: [-177.532 -177.532] [0.0000], Avg: [-947.239 -947.239] (0.020)
Step: 44349, Reward: [-150.526 -150.526] [0.0000], Avg: [-946.34 -946.34] (0.020)
Step: 44399, Reward: [-129.321 -129.321] [0.0000], Avg: [-945.42 -945.42] (0.020)
Step: 44449, Reward: [-292.766 -292.766] [0.0000], Avg: [-944.686 -944.686] (0.020)
Step: 44499, Reward: [-197.696 -197.696] [0.0000], Avg: [-943.847 -943.847] (0.020)
Step: 44549, Reward: [-435.642 -435.642] [0.0000], Avg: [-943.277 -943.277] (0.020)
Step: 44599, Reward: [-48.64 -48.64] [0.0000], Avg: [-942.274 -942.274] (0.020)
Step: 44649, Reward: [-145.636 -145.636] [0.0000], Avg: [-941.381 -941.381] (0.020)
Step: 44699, Reward: [-101.76 -101.76] [0.0000], Avg: [-940.442 -940.442] (0.020)
Step: 44749, Reward: [-224.68 -224.68] [0.0000], Avg: [-939.643 -939.643] (0.020)
Step: 44799, Reward: [-306.912 -306.912] [0.0000], Avg: [-938.936 -938.936] (0.020)
Step: 44849, Reward: [-65.742 -65.742] [0.0000], Avg: [-937.963 -937.963] (0.020)
Step: 44899, Reward: [-69.067 -69.067] [0.0000], Avg: [-936.995 -936.995] (0.020)
Step: 44949, Reward: [-65.428 -65.428] [0.0000], Avg: [-936.026 -936.026] (0.020)
Step: 44999, Reward: [-173.739 -173.739] [0.0000], Avg: [-935.179 -935.179] (0.020)
Step: 45049, Reward: [-375.211 -375.211] [0.0000], Avg: [-934.557 -934.557] (0.020)
Step: 45099, Reward: [-142.61 -142.61] [0.0000], Avg: [-933.679 -933.679] (0.020)
Step: 45149, Reward: [-21.021 -21.021] [0.0000], Avg: [-932.669 -932.669] (0.020)
Step: 45199, Reward: [-68.823 -68.823] [0.0000], Avg: [-931.713 -931.713] (0.020)
Step: 45249, Reward: [-66.904 -66.904] [0.0000], Avg: [-930.758 -930.758] (0.020)
Step: 45299, Reward: [-124.92 -124.92] [0.0000], Avg: [-929.868 -929.868] (0.020)
Step: 45349, Reward: [-7.759 -7.759] [0.0000], Avg: [-928.851 -928.851] (0.020)
Step: 45399, Reward: [-110.507 -110.507] [0.0000], Avg: [-927.95 -927.95] (0.020)
Step: 45449, Reward: [-98.454 -98.454] [0.0000], Avg: [-927.038 -927.038] (0.020)
Step: 45499, Reward: [-136.074 -136.074] [0.0000], Avg: [-926.168 -926.168] (0.020)
Step: 45549, Reward: [-51.811 -51.811] [0.0000], Avg: [-925.209 -925.209] (0.020)
Step: 45599, Reward: [-90.556 -90.556] [0.0000], Avg: [-924.293 -924.293] (0.020)
Step: 45649, Reward: [-41.018 -41.018] [0.0000], Avg: [-923.326 -923.326] (0.020)
Step: 45699, Reward: [-234.283 -234.283] [0.0000], Avg: [-922.572 -922.572] (0.020)
Step: 45749, Reward: [-127.831 -127.831] [0.0000], Avg: [-921.704 -921.704] (0.020)
Step: 45799, Reward: [-166.504 -166.504] [0.0000], Avg: [-920.879 -920.879] (0.020)
Step: 45849, Reward: [-90.063 -90.063] [0.0000], Avg: [-919.973 -919.973] (0.020)
Step: 45899, Reward: [-243.41 -243.41] [0.0000], Avg: [-919.236 -919.236] (0.020)
Step: 45949, Reward: [-53.739 -53.739] [0.0000], Avg: [-918.294 -918.294] (0.020)
Step: 45999, Reward: [-56.181 -56.181] [0.0000], Avg: [-917.357 -917.357] (0.020)
Step: 46049, Reward: [-35.381 -35.381] [0.0000], Avg: [-916.4 -916.4] (0.020)
Step: 46099, Reward: [-89.392 -89.392] [0.0000], Avg: [-915.503 -915.503] (0.020)
Step: 46149, Reward: [-462.791 -462.791] [0.0000], Avg: [-915.012 -915.012] (0.020)
Step: 46199, Reward: [-260.845 -260.845] [0.0000], Avg: [-914.304 -914.304] (0.020)
Step: 46249, Reward: [-168.311 -168.311] [0.0000], Avg: [-913.498 -913.498] (0.020)
Step: 46299, Reward: [-121.887 -121.887] [0.0000], Avg: [-912.643 -912.643] (0.020)
Step: 46349, Reward: [-163.824 -163.824] [0.0000], Avg: [-911.835 -911.835] (0.020)
Step: 46399, Reward: [-259.905 -259.905] [0.0000], Avg: [-911.133 -911.133] (0.020)
Step: 46449, Reward: [-163.457 -163.457] [0.0000], Avg: [-910.328 -910.328] (0.020)
Step: 46499, Reward: [-69.109 -69.109] [0.0000], Avg: [-909.423 -909.423] (0.020)
Step: 46549, Reward: [-252.678 -252.678] [0.0000], Avg: [-908.718 -908.718] (0.020)
Step: 46599, Reward: [-53.026 -53.026] [0.0000], Avg: [-907.8 -907.8] (0.020)
Step: 46649, Reward: [-190.859 -190.859] [0.0000], Avg: [-907.031 -907.031] (0.020)
Step: 46699, Reward: [-438.025 -438.025] [0.0000], Avg: [-906.529 -906.529] (0.020)
Step: 46749, Reward: [-97.712 -97.712] [0.0000], Avg: [-905.664 -905.664] (0.020)
Step: 46799, Reward: [-75.521 -75.521] [0.0000], Avg: [-904.777 -904.777] (0.020)
Step: 46849, Reward: [-402.163 -402.163] [0.0000], Avg: [-904.241 -904.241] (0.020)
Step: 46899, Reward: [-333.195 -333.195] [0.0000], Avg: [-903.632 -903.632] (0.020)
Step: 46949, Reward: [-112.604 -112.604] [0.0000], Avg: [-902.79 -902.79] (0.020)
Step: 46999, Reward: [-143.622 -143.622] [0.0000], Avg: [-901.982 -901.982] (0.020)
Step: 47049, Reward: [-108.941 -108.941] [0.0000], Avg: [-901.139 -901.139] (0.020)
Step: 47099, Reward: [-200.681 -200.681] [0.0000], Avg: [-900.396 -900.396] (0.020)
Step: 47149, Reward: [-49.634 -49.634] [0.0000], Avg: [-899.493 -899.493] (0.020)
Step: 47199, Reward: [-43.007 -43.007] [0.0000], Avg: [-898.586 -898.586] (0.020)
Step: 47249, Reward: [-217.967 -217.967] [0.0000], Avg: [-897.866 -897.866] (0.020)
Step: 47299, Reward: [-203.25 -203.25] [0.0000], Avg: [-897.132 -897.132] (0.020)
Step: 47349, Reward: [-13.088 -13.088] [0.0000], Avg: [-896.198 -896.198] (0.020)
Step: 47399, Reward: [-1.683 -1.683] [0.0000], Avg: [-895.254 -895.254] (0.020)
Step: 47449, Reward: [-23.832 -23.832] [0.0000], Avg: [-894.336 -894.336] (0.020)
Step: 47499, Reward: [-12.83 -12.83] [0.0000], Avg: [-893.408 -893.408] (0.020)
Step: 47549, Reward: [-216.549 -216.549] [0.0000], Avg: [-892.697 -892.697] (0.020)
Step: 47599, Reward: [-387.272 -387.272] [0.0000], Avg: [-892.166 -892.166] (0.020)
Step: 47649, Reward: [-61.694 -61.694] [0.0000], Avg: [-891.294 -891.294] (0.020)
Step: 47699, Reward: [-99.698 -99.698] [0.0000], Avg: [-890.464 -890.464] (0.020)
Step: 47749, Reward: [-173.757 -173.757] [0.0000], Avg: [-889.714 -889.714] (0.020)
Step: 47799, Reward: [-4.536 -4.536] [0.0000], Avg: [-888.788 -888.788] (0.020)
Step: 47849, Reward: [-13.983 -13.983] [0.0000], Avg: [-887.874 -887.874] (0.020)
Step: 47899, Reward: [-191.888 -191.888] [0.0000], Avg: [-887.147 -887.147] (0.020)
Step: 47949, Reward: [-18.831 -18.831] [0.0000], Avg: [-886.242 -886.242] (0.020)
Step: 47999, Reward: [-41.876 -41.876] [0.0000], Avg: [-885.362 -885.362] (0.020)
Step: 48049, Reward: [-171.179 -171.179] [0.0000], Avg: [-884.619 -884.619] (0.020)
Step: 48099, Reward: [-394.368 -394.368] [0.0000], Avg: [-884.11 -884.11] (0.020)
Step: 48149, Reward: [-169.532 -169.532] [0.0000], Avg: [-883.368 -883.368] (0.020)
Step: 48199, Reward: [-46.568 -46.568] [0.0000], Avg: [-882.5 -882.5] (0.020)
Step: 48249, Reward: [-26.459 -26.459] [0.0000], Avg: [-881.613 -881.613] (0.020)
Step: 48299, Reward: [-63.53 -63.53] [0.0000], Avg: [-880.766 -880.766] (0.020)
Step: 48349, Reward: [-23.738 -23.738] [0.0000], Avg: [-879.879 -879.879] (0.020)
Step: 48399, Reward: [-148.195 -148.195] [0.0000], Avg: [-879.124 -879.124] (0.020)
Step: 48449, Reward: [-361.923 -361.923] [0.0000], Avg: [-878.59 -878.59] (0.020)
Step: 48499, Reward: [-289.663 -289.663] [0.0000], Avg: [-877.983 -877.983] (0.020)
Step: 48549, Reward: [-83.9 -83.9] [0.0000], Avg: [-877.165 -877.165] (0.020)
Step: 48599, Reward: [-84.779 -84.779] [0.0000], Avg: [-876.35 -876.35] (0.020)
Step: 48649, Reward: [-7.047 -7.047] [0.0000], Avg: [-875.456 -875.456] (0.020)
Step: 48699, Reward: [-191.699 -191.699] [0.0000], Avg: [-874.754 -874.754] (0.020)
Step: 48749, Reward: [-69.111 -69.111] [0.0000], Avg: [-873.928 -873.928] (0.020)
Step: 48799, Reward: [-509.406 -509.406] [0.0000], Avg: [-873.554 -873.554] (0.020)
Step: 48849, Reward: [-3.204 -3.204] [0.0000], Avg: [-872.664 -872.664] (0.020)
Step: 48899, Reward: [-5.994 -5.994] [0.0000], Avg: [-871.777 -871.777] (0.020)
Step: 48949, Reward: [-132.065 -132.065] [0.0000], Avg: [-871.022 -871.022] (0.020)
Step: 48999, Reward: [-24.848 -24.848] [0.0000], Avg: [-870.158 -870.158] (0.020)
Step: 49049, Reward: [-24.782 -24.782] [0.0000], Avg: [-869.297 -869.297] (0.020)
Step: 49099, Reward: [-41.642 -41.642] [0.0000], Avg: [-868.454 -868.454] (0.020)
Step: 49149, Reward: [-140.085 -140.085] [0.0000], Avg: [-867.713 -867.713] (0.020)
Step: 49199, Reward: [-32.024 -32.024] [0.0000], Avg: [-866.864 -866.864] (0.020)
Step: 49249, Reward: [-299.582 -299.582] [0.0000], Avg: [-866.288 -866.288] (0.020)
Step: 49299, Reward: [-95.218 -95.218] [0.0000], Avg: [-865.506 -865.506] (0.020)
Step: 49349, Reward: [-141.542 -141.542] [0.0000], Avg: [-864.772 -864.772] (0.020)
Step: 49399, Reward: [-3.168 -3.168] [0.0000], Avg: [-863.9 -863.9] (0.020)
Step: 49449, Reward: [-87.092 -87.092] [0.0000], Avg: [-863.115 -863.115] (0.020)
Step: 49499, Reward: [-121.479 -121.479] [0.0000], Avg: [-862.365 -862.365] (0.020)
Step: 49549, Reward: [-187.707 -187.707] [0.0000], Avg: [-861.685 -861.685] (0.020)
Step: 49599, Reward: [-187.457 -187.457] [0.0000], Avg: [-861.005 -861.005] (0.020)
Step: 49649, Reward: [-36.683 -36.683] [0.0000], Avg: [-860.175 -860.175] (0.020)
Step: 49699, Reward: [-0.088 -0.088] [0.0000], Avg: [-859.31 -859.31] (0.020)
Step: 49749, Reward: [-226.082 -226.082] [0.0000], Avg: [-858.673 -858.673] (0.020)
Step: 49799, Reward: [-18.961 -18.961] [0.0000], Avg: [-857.83 -857.83] (0.020)
Step: 49849, Reward: [-108.951 -108.951] [0.0000], Avg: [-857.079 -857.079] (0.020)
Step: 49899, Reward: [-18.764 -18.764] [0.0000], Avg: [-856.239 -856.239] (0.020)
Step: 49949, Reward: [-171.697 -171.697] [0.0000], Avg: [-855.554 -855.554] (0.020)
Step: 49999, Reward: [-140.907 -140.907] [0.0000], Avg: [-854.839 -854.839] (0.020)
Step: 50049, Reward: [-61.759 -61.759] [0.0000], Avg: [-854.047 -854.047] (0.020)
Step: 50099, Reward: [-12.246 -12.246] [0.0000], Avg: [-853.207 -853.207] (0.020)
Step: 50149, Reward: [-8.194 -8.194] [0.0000], Avg: [-852.364 -852.364] (0.020)
Step: 50199, Reward: [-49.45 -49.45] [0.0000], Avg: [-851.564 -851.564] (0.020)
Step: 50249, Reward: [-91.208 -91.208] [0.0000], Avg: [-850.808 -850.808] (0.020)
Step: 50299, Reward: [-93.952 -93.952] [0.0000], Avg: [-850.056 -850.056] (0.020)
Step: 50349, Reward: [-3.525 -3.525] [0.0000], Avg: [-849.215 -849.215] (0.020)
Step: 50399, Reward: [-324.143 -324.143] [0.0000], Avg: [-848.694 -848.694] (0.020)
Step: 50449, Reward: [-129.861 -129.861] [0.0000], Avg: [-847.982 -847.982] (0.020)
Step: 50499, Reward: [-230.842 -230.842] [0.0000], Avg: [-847.371 -847.371] (0.020)
Step: 50549, Reward: [-64.941 -64.941] [0.0000], Avg: [-846.597 -846.597] (0.020)
Step: 50599, Reward: [-16.735 -16.735] [0.0000], Avg: [-845.777 -845.777] (0.020)
Step: 50649, Reward: [-11.214 -11.214] [0.0000], Avg: [-844.953 -844.953] (0.020)
Step: 50699, Reward: [-192.622 -192.622] [0.0000], Avg: [-844.309 -844.309] (0.020)
Step: 50749, Reward: [-315.222 -315.222] [0.0000], Avg: [-843.788 -843.788] (0.020)
Step: 50799, Reward: [-99.217 -99.217] [0.0000], Avg: [-843.055 -843.055] (0.020)
Step: 50849, Reward: [-115.037 -115.037] [0.0000], Avg: [-842.34 -842.34] (0.020)
Step: 50899, Reward: [-242.966 -242.966] [0.0000], Avg: [-841.751 -841.751] (0.020)
Step: 50949, Reward: [-68.594 -68.594] [0.0000], Avg: [-840.992 -840.992] (0.020)
Step: 50999, Reward: [-77.388 -77.388] [0.0000], Avg: [-840.243 -840.243] (0.020)
Step: 51049, Reward: [-55.652 -55.652] [0.0000], Avg: [-839.475 -839.475] (0.020)
Step: 51099, Reward: [-45.088 -45.088] [0.0000], Avg: [-838.698 -838.698] (0.020)
Step: 51149, Reward: [-19.035 -19.035] [0.0000], Avg: [-837.896 -837.896] (0.020)
Step: 51199, Reward: [-22.283 -22.283] [0.0000], Avg: [-837.1 -837.1] (0.020)
Step: 51249, Reward: [-272.945 -272.945] [0.0000], Avg: [-836.549 -836.549] (0.020)
Step: 51299, Reward: [-100.834 -100.834] [0.0000], Avg: [-835.832 -835.832] (0.020)
Step: 51349, Reward: [-252.472 -252.472] [0.0000], Avg: [-835.264 -835.264] (0.020)
Step: 51399, Reward: [-13.567 -13.567] [0.0000], Avg: [-834.465 -834.465] (0.020)
Step: 51449, Reward: [-89.735 -89.735] [0.0000], Avg: [-833.741 -833.741] (0.020)
Step: 51499, Reward: [-210.785 -210.785] [0.0000], Avg: [-833.137 -833.137] (0.020)
Step: 51549, Reward: [-33.769 -33.769] [0.0000], Avg: [-832.361 -832.361] (0.020)
Step: 51599, Reward: [-329.845 -329.845] [0.0000], Avg: [-831.874 -831.874] (0.020)
Step: 51649, Reward: [-9.616 -9.616] [0.0000], Avg: [-831.078 -831.078] (0.020)
Step: 51699, Reward: [-295.65 -295.65] [0.0000], Avg: [-830.56 -830.56] (0.020)
Step: 51749, Reward: [-87.89 -87.89] [0.0000], Avg: [-829.843 -829.843] (0.020)
Step: 51799, Reward: [-101.353 -101.353] [0.0000], Avg: [-829.14 -829.14] (0.020)
Step: 51849, Reward: [-63.185 -63.185] [0.0000], Avg: [-828.401 -828.401] (0.020)
Step: 51899, Reward: [-32.827 -32.827] [0.0000], Avg: [-827.635 -827.635] (0.020)
Step: 51949, Reward: [-201.221 -201.221] [0.0000], Avg: [-827.032 -827.032] (0.020)
Step: 51999, Reward: [-75.875 -75.875] [0.0000], Avg: [-826.309 -826.309] (0.020)
Step: 52049, Reward: [-143.286 -143.286] [0.0000], Avg: [-825.653 -825.653] (0.020)
Step: 52099, Reward: [-109.024 -109.024] [0.0000], Avg: [-824.966 -824.966] (0.020)
Step: 52149, Reward: [-43.107 -43.107] [0.0000], Avg: [-824.216 -824.216] (0.020)
Step: 52199, Reward: [-260.373 -260.373] [0.0000], Avg: [-823.676 -823.676] (0.020)
Step: 52249, Reward: [-101.595 -101.595] [0.0000], Avg: [-822.985 -822.985] (0.020)
Step: 52299, Reward: [-40.727 -40.727] [0.0000], Avg: [-822.237 -822.237] (0.020)
Step: 52349, Reward: [-138.915 -138.915] [0.0000], Avg: [-821.584 -821.584] (0.020)
Step: 52399, Reward: [-195.377 -195.377] [0.0000], Avg: [-820.987 -820.987] (0.020)
Step: 52449, Reward: [-81.081 -81.081] [0.0000], Avg: [-820.282 -820.282] (0.020)
Step: 52499, Reward: [-59.8 -59.8] [0.0000], Avg: [-819.557 -819.557] (0.020)
Step: 52549, Reward: [-184.639 -184.639] [0.0000], Avg: [-818.953 -818.953] (0.020)
Step: 52599, Reward: [-166.571 -166.571] [0.0000], Avg: [-818.333 -818.333] (0.020)
Step: 52649, Reward: [-138.567 -138.567] [0.0000], Avg: [-817.687 -817.687] (0.020)
Step: 52699, Reward: [-169.806 -169.806] [0.0000], Avg: [-817.073 -817.073] (0.020)
Step: 52749, Reward: [-14.625 -14.625] [0.0000], Avg: [-816.312 -816.312] (0.020)
Step: 52799, Reward: [-93.719 -93.719] [0.0000], Avg: [-815.628 -815.628] (0.020)
Step: 52849, Reward: [-61.668 -61.668] [0.0000], Avg: [-814.915 -814.915] (0.020)
Step: 52899, Reward: [-4.736 -4.736] [0.0000], Avg: [-814.149 -814.149] (0.020)
Step: 52949, Reward: [-71.982 -71.982] [0.0000], Avg: [-813.448 -813.448] (0.020)
Step: 52999, Reward: [-288.981 -288.981] [0.0000], Avg: [-812.953 -812.953] (0.020)
Step: 53049, Reward: [-308.613 -308.613] [0.0000], Avg: [-812.478 -812.478] (0.020)
Step: 53099, Reward: [-160.366 -160.366] [0.0000], Avg: [-811.864 -811.864] (0.020)
Step: 53149, Reward: [-25.629 -25.629] [0.0000], Avg: [-811.124 -811.124] (0.020)
Step: 53199, Reward: [-131.538 -131.538] [0.0000], Avg: [-810.486 -810.486] (0.020)
Step: 53249, Reward: [-21.01 -21.01] [0.0000], Avg: [-809.744 -809.744] (0.020)
Step: 53299, Reward: [-63.21 -63.21] [0.0000], Avg: [-809.044 -809.044] (0.020)
Step: 53349, Reward: [-52.196 -52.196] [0.0000], Avg: [-808.335 -808.335] (0.020)
Step: 53399, Reward: [-43.543 -43.543] [0.0000], Avg: [-807.618 -807.618] (0.020)
Step: 53449, Reward: [-13.348 -13.348] [0.0000], Avg: [-806.875 -806.875] (0.020)
Step: 53499, Reward: [-7.174 -7.174] [0.0000], Avg: [-806.128 -806.128] (0.020)
Step: 53549, Reward: [-3.284 -3.284] [0.0000], Avg: [-805.378 -805.378] (0.020)
Step: 53599, Reward: [-26.657 -26.657] [0.0000], Avg: [-804.652 -804.652] (0.020)
Step: 53649, Reward: [-3.307 -3.307] [0.0000], Avg: [-803.905 -803.905] (0.020)
Step: 53699, Reward: [-5.04 -5.04] [0.0000], Avg: [-803.161 -803.161] (0.020)
Step: 53749, Reward: [-7.902 -7.902] [0.0000], Avg: [-802.422 -802.422] (0.020)
Step: 53799, Reward: [-36.309 -36.309] [0.0000], Avg: [-801.71 -801.71] (0.020)
Step: 53849, Reward: [-129.257 -129.257] [0.0000], Avg: [-801.085 -801.085] (0.020)
Step: 53899, Reward: [-1.898 -1.898] [0.0000], Avg: [-800.344 -800.344] (0.020)
Step: 53949, Reward: [-120.802 -120.802] [0.0000], Avg: [-799.714 -799.714] (0.020)
Step: 53999, Reward: [-7.107 -7.107] [0.0000], Avg: [-798.98 -798.98] (0.020)
Step: 54049, Reward: [-69.842 -69.842] [0.0000], Avg: [-798.306 -798.306] (0.020)
Step: 54099, Reward: [-14.089 -14.089] [0.0000], Avg: [-797.581 -797.581] (0.020)
Step: 54149, Reward: [-55.088 -55.088] [0.0000], Avg: [-796.895 -796.895] (0.020)
Step: 54199, Reward: [-260.21 -260.21] [0.0000], Avg: [-796.4 -796.4] (0.020)
Step: 54249, Reward: [-40.218 -40.218] [0.0000], Avg: [-795.703 -795.703] (0.020)
Step: 54299, Reward: [-23.63 -23.63] [0.0000], Avg: [-794.992 -794.992] (0.020)
Step: 54349, Reward: [-96.951 -96.951] [0.0000], Avg: [-794.35 -794.35] (0.020)
Step: 54399, Reward: [-248.643 -248.643] [0.0000], Avg: [-793.849 -793.849] (0.020)
Step: 54449, Reward: [-181.103 -181.103] [0.0000], Avg: [-793.286 -793.286] (0.020)
Step: 54499, Reward: [-211.86 -211.86] [0.0000], Avg: [-792.753 -792.753] (0.020)
Step: 54549, Reward: [-199.238 -199.238] [0.0000], Avg: [-792.209 -792.209] (0.020)
Step: 54599, Reward: [-418.686 -418.686] [0.0000], Avg: [-791.866 -791.866] (0.020)
Step: 54649, Reward: [-8.384 -8.384] [0.0000], Avg: [-791.15 -791.15] (0.020)
Step: 54699, Reward: [-21.863 -21.863] [0.0000], Avg: [-790.446 -790.446] (0.020)
Step: 54749, Reward: [-315.557 -315.557] [0.0000], Avg: [-790.013 -790.013] (0.020)
Step: 54799, Reward: [-6.945 -6.945] [0.0000], Avg: [-789.298 -789.298] (0.020)
Step: 54849, Reward: [-301.738 -301.738] [0.0000], Avg: [-788.854 -788.854] (0.020)
Step: 54899, Reward: [-21.131 -21.131] [0.0000], Avg: [-788.155 -788.155] (0.020)
Step: 54949, Reward: [-43.335 -43.335] [0.0000], Avg: [-787.477 -787.477] (0.020)
Step: 54999, Reward: [-288.218 -288.218] [0.0000], Avg: [-787.023 -787.023] (0.020)
Step: 55049, Reward: [-116.443 -116.443] [0.0000], Avg: [-786.414 -786.414] (0.020)
Step: 55099, Reward: [-31.545 -31.545] [0.0000], Avg: [-785.729 -785.729] (0.020)
Step: 55149, Reward: [-22.558 -22.558] [0.0000], Avg: [-785.037 -785.037] (0.020)
Step: 55199, Reward: [-80.905 -80.905] [0.0000], Avg: [-784.399 -784.399] (0.020)
Step: 55249, Reward: [-257.22 -257.22] [0.0000], Avg: [-783.922 -783.922] (0.020)
Step: 55299, Reward: [-63.241 -63.241] [0.0000], Avg: [-783.271 -783.271] (0.020)
Step: 55349, Reward: [-1.114 -1.114] [0.0000], Avg: [-782.564 -782.564] (0.020)
Step: 55399, Reward: [-91.624 -91.624] [0.0000], Avg: [-781.94 -781.94] (0.020)
Step: 55449, Reward: [-209.53 -209.53] [0.0000], Avg: [-781.424 -781.424] (0.020)
Step: 55499, Reward: [-212.868 -212.868] [0.0000], Avg: [-780.912 -780.912] (0.020)
Step: 55549, Reward: [-317.39 -317.39] [0.0000], Avg: [-780.495 -780.495] (0.020)
Step: 55599, Reward: [-340.541 -340.541] [0.0000], Avg: [-780.099 -780.099] (0.020)
Step: 55649, Reward: [-111.558 -111.558] [0.0000], Avg: [-779.499 -779.499] (0.020)
Step: 55699, Reward: [-299.827 -299.827] [0.0000], Avg: [-779.068 -779.068] (0.020)
Step: 55749, Reward: [-154.746 -154.746] [0.0000], Avg: [-778.508 -778.508] (0.020)
Step: 55799, Reward: [-110.458 -110.458] [0.0000], Avg: [-777.909 -777.909] (0.020)
Step: 55849, Reward: [-235.111 -235.111] [0.0000], Avg: [-777.423 -777.423] (0.020)
Step: 55899, Reward: [-107.511 -107.511] [0.0000], Avg: [-776.824 -776.824] (0.020)
Step: 55949, Reward: [-18.545 -18.545] [0.0000], Avg: [-776.147 -776.147] (0.020)
Step: 55999, Reward: [-183.428 -183.428] [0.0000], Avg: [-775.617 -775.617] (0.020)
Step: 56049, Reward: [-271.99 -271.99] [0.0000], Avg: [-775.168 -775.168] (0.020)
Step: 56099, Reward: [-0.276 -0.276] [0.0000], Avg: [-774.478 -774.478] (0.020)
Step: 56149, Reward: [-16.653 -16.653] [0.0000], Avg: [-773.803 -773.803] (0.020)
Step: 56199, Reward: [-45.614 -45.614] [0.0000], Avg: [-773.155 -773.155] (0.020)
Step: 56249, Reward: [-251.326 -251.326] [0.0000], Avg: [-772.691 -772.691] (0.020)
Step: 56299, Reward: [-133.84 -133.84] [0.0000], Avg: [-772.124 -772.124] (0.020)
Step: 56349, Reward: [-65.451 -65.451] [0.0000], Avg: [-771.497 -771.497] (0.020)
Step: 56399, Reward: [-299.018 -299.018] [0.0000], Avg: [-771.078 -771.078] (0.020)
Step: 56449, Reward: [-38.312 -38.312] [0.0000], Avg: [-770.429 -770.429] (0.020)
Step: 56499, Reward: [-223.462 -223.462] [0.0000], Avg: [-769.945 -769.945] (0.020)
Step: 56549, Reward: [-3.842 -3.842] [0.0000], Avg: [-769.267 -769.267] (0.020)
Step: 56599, Reward: [-137.403 -137.403] [0.0000], Avg: [-768.709 -768.709] (0.020)
Step: 56649, Reward: [-78.045 -78.045] [0.0000], Avg: [-768.1 -768.1] (0.020)
Step: 56699, Reward: [-220.157 -220.157] [0.0000], Avg: [-767.616 -767.616] (0.020)
Step: 56749, Reward: [-60.928 -60.928] [0.0000], Avg: [-766.994 -766.994] (0.020)
Step: 56799, Reward: [-514.709 -514.709] [0.0000], Avg: [-766.772 -766.772] (0.020)
Step: 56849, Reward: [-82.799 -82.799] [0.0000], Avg: [-766.17 -766.17] (0.020)
Step: 56899, Reward: [-62.638 -62.638] [0.0000], Avg: [-765.552 -765.552] (0.020)
Step: 56949, Reward: [-127.388 -127.388] [0.0000], Avg: [-764.992 -764.992] (0.020)
Step: 56999, Reward: [-229.885 -229.885] [0.0000], Avg: [-764.522 -764.522] (0.020)
Step: 57049, Reward: [-142.695 -142.695] [0.0000], Avg: [-763.977 -763.977] (0.020)
Step: 57099, Reward: [-225.377 -225.377] [0.0000], Avg: [-763.506 -763.506] (0.020)
Step: 57149, Reward: [-110.24 -110.24] [0.0000], Avg: [-762.934 -762.934] (0.020)
Step: 57199, Reward: [-32.073 -32.073] [0.0000], Avg: [-762.295 -762.295] (0.020)
Step: 57249, Reward: [-126.691 -126.691] [0.0000], Avg: [-761.74 -761.74] (0.020)
Step: 57299, Reward: [-133.912 -133.912] [0.0000], Avg: [-761.192 -761.192] (0.020)
Step: 57349, Reward: [-77.931 -77.931] [0.0000], Avg: [-760.596 -760.596] (0.020)
Step: 57399, Reward: [-272.015 -272.015] [0.0000], Avg: [-760.171 -760.171] (0.020)
Step: 57449, Reward: [-304.753 -304.753] [0.0000], Avg: [-759.775 -759.775] (0.020)
Step: 57499, Reward: [-281.413 -281.413] [0.0000], Avg: [-759.359 -759.359] (0.020)
Step: 57549, Reward: [-381.474 -381.474] [0.0000], Avg: [-759.03 -759.03] (0.020)
Step: 57599, Reward: [-162.426 -162.426] [0.0000], Avg: [-758.512 -758.512] (0.020)
Step: 57649, Reward: [-70.738 -70.738] [0.0000], Avg: [-757.916 -757.916] (0.020)
Step: 57699, Reward: [-303.152 -303.152] [0.0000], Avg: [-757.522 -757.522] (0.020)
Step: 57749, Reward: [-62.37 -62.37] [0.0000], Avg: [-756.92 -756.92] (0.020)
Step: 57799, Reward: [-324.951 -324.951] [0.0000], Avg: [-756.546 -756.546] (0.020)
Step: 57849, Reward: [-47.217 -47.217] [0.0000], Avg: [-755.933 -755.933] (0.020)
Step: 57899, Reward: [-58.736 -58.736] [0.0000], Avg: [-755.331 -755.331] (0.020)
Step: 57949, Reward: [-10.416 -10.416] [0.0000], Avg: [-754.688 -754.688] (0.020)
Step: 57999, Reward: [-127.277 -127.277] [0.0000], Avg: [-754.148 -754.148] (0.020)
Step: 58049, Reward: [-21.244 -21.244] [0.0000], Avg: [-753.516 -753.516] (0.020)
Step: 58099, Reward: [-339.182 -339.182] [0.0000], Avg: [-753.16 -753.16] (0.020)
Step: 58149, Reward: [-99.794 -99.794] [0.0000], Avg: [-752.598 -752.598] (0.020)
Step: 58199, Reward: [-215.88 -215.88] [0.0000], Avg: [-752.137 -752.137] (0.020)
Step: 58249, Reward: [-103.237 -103.237] [0.0000], Avg: [-751.58 -751.58] (0.020)
Step: 58299, Reward: [-128.53 -128.53] [0.0000], Avg: [-751.045 -751.045] (0.020)
Step: 58349, Reward: [-131.387 -131.387] [0.0000], Avg: [-750.514 -750.514] (0.020)
Step: 58399, Reward: [-99.609 -99.609] [0.0000], Avg: [-749.957 -749.957] (0.020)
Step: 58449, Reward: [-370.694 -370.694] [0.0000], Avg: [-749.633 -749.633] (0.020)
Step: 58499, Reward: [-57.949 -57.949] [0.0000], Avg: [-749.042 -749.042] (0.020)
Step: 58549, Reward: [-274.52 -274.52] [0.0000], Avg: [-748.636 -748.636] (0.020)
Step: 58599, Reward: [-2.092 -2.092] [0.0000], Avg: [-747.999 -747.999] (0.020)
Step: 58649, Reward: [-16.336 -16.336] [0.0000], Avg: [-747.376 -747.376] (0.020)
Step: 58699, Reward: [-169.046 -169.046] [0.0000], Avg: [-746.883 -746.883] (0.020)
Step: 58749, Reward: [-239.134 -239.134] [0.0000], Avg: [-746.451 -746.451] (0.020)
Step: 58799, Reward: [-0.716 -0.716] [0.0000], Avg: [-745.817 -745.817] (0.020)
Step: 58849, Reward: [-78.216 -78.216] [0.0000], Avg: [-745.25 -745.25] (0.020)
Step: 58899, Reward: [-88.129 -88.129] [0.0000], Avg: [-744.692 -744.692] (0.020)
Step: 58949, Reward: [-42.215 -42.215] [0.0000], Avg: [-744.096 -744.096] (0.020)
Step: 58999, Reward: [-78.459 -78.459] [0.0000], Avg: [-743.532 -743.532] (0.020)
Step: 59049, Reward: [-297.198 -297.198] [0.0000], Avg: [-743.154 -743.154] (0.020)
Step: 59099, Reward: [-87.042 -87.042] [0.0000], Avg: [-742.599 -742.599] (0.020)
Step: 59149, Reward: [-326.579 -326.579] [0.0000], Avg: [-742.247 -742.247] (0.020)
Step: 59199, Reward: [-119.638 -119.638] [0.0000], Avg: [-741.721 -741.721] (0.020)
Step: 59249, Reward: [-67.427 -67.427] [0.0000], Avg: [-741.152 -741.152] (0.020)
Step: 59299, Reward: [-209.549 -209.549] [0.0000], Avg: [-740.704 -740.704] (0.020)
Step: 59349, Reward: [-87.808 -87.808] [0.0000], Avg: [-740.154 -740.154] (0.020)
Step: 59399, Reward: [-56.752 -56.752] [0.0000], Avg: [-739.579 -739.579] (0.020)
Step: 59449, Reward: [-154.473 -154.473] [0.0000], Avg: [-739.087 -739.087] (0.020)
Step: 59499, Reward: [-106.916 -106.916] [0.0000], Avg: [-738.555 -738.555] (0.020)
Step: 59549, Reward: [-37.669 -37.669] [0.0000], Avg: [-737.967 -737.967] (0.020)
Step: 59599, Reward: [-138.255 -138.255] [0.0000], Avg: [-737.464 -737.464] (0.020)
Step: 59649, Reward: [-366.181 -366.181] [0.0000], Avg: [-737.153 -737.153] (0.020)
Step: 59699, Reward: [-7.967 -7.967] [0.0000], Avg: [-736.542 -736.542] (0.020)
Step: 59749, Reward: [-423.743 -423.743] [0.0000], Avg: [-736.28 -736.28] (0.020)
Step: 59799, Reward: [-63.521 -63.521] [0.0000], Avg: [-735.718 -735.718] (0.020)
Step: 59849, Reward: [-7.507 -7.507] [0.0000], Avg: [-735.109 -735.109] (0.020)
Step: 59899, Reward: [-43.098 -43.098] [0.0000], Avg: [-734.532 -734.532] (0.020)
Step: 59949, Reward: [-349.84 -349.84] [0.0000], Avg: [-734.211 -734.211] (0.020)
Step: 59999, Reward: [-1.783 -1.783] [0.0000], Avg: [-733.6 -733.6] (0.020)
Step: 60049, Reward: [-428.123 -428.123] [0.0000], Avg: [-733.346 -733.346] (0.020)
Step: 60099, Reward: [-459.304 -459.304] [0.0000], Avg: [-733.118 -733.118] (0.020)
Step: 60149, Reward: [-230.309 -230.309] [0.0000], Avg: [-732.7 -732.7] (0.020)
Step: 60199, Reward: [-37.865 -37.865] [0.0000], Avg: [-732.123 -732.123] (0.020)
Step: 60249, Reward: [-52.946 -52.946] [0.0000], Avg: [-731.559 -731.559] (0.020)
Step: 60299, Reward: [-166.495 -166.495] [0.0000], Avg: [-731.091 -731.091] (0.020)
Step: 60349, Reward: [-5.127 -5.127] [0.0000], Avg: [-730.489 -730.489] (0.020)
Step: 60399, Reward: [-16.196 -16.196] [0.0000], Avg: [-729.898 -729.898] (0.020)
Step: 60449, Reward: [-323.743 -323.743] [0.0000], Avg: [-729.562 -729.562] (0.020)
Step: 60499, Reward: [-116.253 -116.253] [0.0000], Avg: [-729.055 -729.055] (0.020)
Step: 60549, Reward: [-268.096 -268.096] [0.0000], Avg: [-728.675 -728.675] (0.020)
Step: 60599, Reward: [-15.459 -15.459] [0.0000], Avg: [-728.086 -728.086] (0.020)
Step: 60649, Reward: [-7.182 -7.182] [0.0000], Avg: [-727.492 -727.492] (0.020)
Step: 60699, Reward: [-342.796 -342.796] [0.0000], Avg: [-727.175 -727.175] (0.020)
Step: 60749, Reward: [-54.811 -54.811] [0.0000], Avg: [-726.622 -726.622] (0.020)
Step: 60799, Reward: [-14.186 -14.186] [0.0000], Avg: [-726.036 -726.036] (0.020)
Step: 60849, Reward: [-52.663 -52.663] [0.0000], Avg: [-725.482 -725.482] (0.020)
Step: 60899, Reward: [-183.809 -183.809] [0.0000], Avg: [-725.038 -725.038] (0.020)
Step: 60949, Reward: [-109.027 -109.027] [0.0000], Avg: [-724.532 -724.532] (0.020)
Step: 60999, Reward: [-86.981 -86.981] [0.0000], Avg: [-724.01 -724.01] (0.020)
Step: 61049, Reward: [-350.722 -350.722] [0.0000], Avg: [-723.704 -723.704] (0.020)
Step: 61099, Reward: [-187.974 -187.974] [0.0000], Avg: [-723.266 -723.266] (0.020)
Step: 61149, Reward: [-133.496 -133.496] [0.0000], Avg: [-722.783 -722.783] (0.020)
Step: 61199, Reward: [-28.971 -28.971] [0.0000], Avg: [-722.216 -722.216] (0.020)
Step: 61249, Reward: [-187.175 -187.175] [0.0000], Avg: [-721.78 -721.78] (0.020)
Step: 61299, Reward: [-136.986 -136.986] [0.0000], Avg: [-721.303 -721.303] (0.020)
Step: 61349, Reward: [-412.596 -412.596] [0.0000], Avg: [-721.051 -721.051] (0.020)
Step: 61399, Reward: [-8.909 -8.909] [0.0000], Avg: [-720.471 -720.471] (0.020)
Step: 61449, Reward: [-79.743 -79.743] [0.0000], Avg: [-719.95 -719.95] (0.020)
Step: 61499, Reward: [-275.729 -275.729] [0.0000], Avg: [-719.589 -719.589] (0.020)
Step: 61549, Reward: [-271.007 -271.007] [0.0000], Avg: [-719.224 -719.224] (0.020)
Step: 61599, Reward: [-234.015 -234.015] [0.0000], Avg: [-718.83 -718.83] (0.020)
Step: 61649, Reward: [-25.59 -25.59] [0.0000], Avg: [-718.268 -718.268] (0.020)
Step: 61699, Reward: [-107.395 -107.395] [0.0000], Avg: [-717.773 -717.773] (0.020)
Step: 61749, Reward: [-84.867 -84.867] [0.0000], Avg: [-717.261 -717.261] (0.020)
Step: 61799, Reward: [-27.116 -27.116] [0.0000], Avg: [-716.702 -716.702] (0.020)
Step: 61849, Reward: [-254.045 -254.045] [0.0000], Avg: [-716.328 -716.328] (0.020)
Step: 61899, Reward: [-6.473 -6.473] [0.0000], Avg: [-715.755 -715.755] (0.020)
Step: 61949, Reward: [-78.591 -78.591] [0.0000], Avg: [-715.241 -715.241] (0.020)
Step: 61999, Reward: [-1.228 -1.228] [0.0000], Avg: [-714.665 -714.665] (0.020)
Step: 62049, Reward: [-130.319 -130.319] [0.0000], Avg: [-714.194 -714.194] (0.020)
Step: 62099, Reward: [-194.595 -194.595] [0.0000], Avg: [-713.776 -713.776] (0.020)
Step: 62149, Reward: [-30.416 -30.416] [0.0000], Avg: [-713.226 -713.226] (0.020)
Step: 62199, Reward: [-218.354 -218.354] [0.0000], Avg: [-712.828 -712.828] (0.020)
Step: 62249, Reward: [-313.432 -313.432] [0.0000], Avg: [-712.507 -712.507] (0.020)
Step: 62299, Reward: [-377.626 -377.626] [0.0000], Avg: [-712.238 -712.238] (0.020)
Step: 62349, Reward: [-17.555 -17.555] [0.0000], Avg: [-711.681 -711.681] (0.020)
Step: 62399, Reward: [-57.696 -57.696] [0.0000], Avg: [-711.157 -711.157] (0.020)
Step: 62449, Reward: [-77.452 -77.452] [0.0000], Avg: [-710.65 -710.65] (0.020)
Step: 62499, Reward: [-91.277 -91.277] [0.0000], Avg: [-710.155 -710.155] (0.020)
Step: 62549, Reward: [-125.455 -125.455] [0.0000], Avg: [-709.687 -709.687] (0.020)
Step: 62599, Reward: [-6.37 -6.37] [0.0000], Avg: [-709.125 -709.125] (0.020)
Step: 62649, Reward: [-146.226 -146.226] [0.0000], Avg: [-708.676 -708.676] (0.020)
Step: 62699, Reward: [-236.98 -236.98] [0.0000], Avg: [-708.3 -708.3] (0.020)
Step: 62749, Reward: [-4.601 -4.601] [0.0000], Avg: [-707.739 -707.739] (0.020)
Step: 62799, Reward: [-78.927 -78.927] [0.0000], Avg: [-707.239 -707.239] (0.020)
Step: 62849, Reward: [-18.746 -18.746] [0.0000], Avg: [-706.691 -706.691] (0.020)
Step: 62899, Reward: [-82.482 -82.482] [0.0000], Avg: [-706.195 -706.195] (0.020)
Step: 62949, Reward: [-161.366 -161.366] [0.0000], Avg: [-705.762 -705.762] (0.020)
Step: 62999, Reward: [-71.652 -71.652] [0.0000], Avg: [-705.259 -705.259] (0.020)
Step: 63049, Reward: [-19.892 -19.892] [0.0000], Avg: [-704.715 -704.715] (0.020)
Step: 63099, Reward: [-101.21 -101.21] [0.0000], Avg: [-704.237 -704.237] (0.020)
Step: 63149, Reward: [-210.454 -210.454] [0.0000], Avg: [-703.846 -703.846] (0.020)
Step: 63199, Reward: [-164.885 -164.885] [0.0000], Avg: [-703.42 -703.42] (0.020)
Step: 63249, Reward: [-21.676 -21.676] [0.0000], Avg: [-702.881 -702.881] (0.020)
Step: 63299, Reward: [-13.998 -13.998] [0.0000], Avg: [-702.337 -702.337] (0.020)
Step: 63349, Reward: [-290.612 -290.612] [0.0000], Avg: [-702.012 -702.012] (0.020)
Step: 63399, Reward: [-4.09 -4.09] [0.0000], Avg: [-701.461 -701.461] (0.020)
Step: 63449, Reward: [-65.779 -65.779] [0.0000], Avg: [-700.96 -700.96] (0.020)
Step: 63499, Reward: [-209.933 -209.933] [0.0000], Avg: [-700.574 -700.574] (0.020)
Step: 63549, Reward: [-68.165 -68.165] [0.0000], Avg: [-700.076 -700.076] (0.020)
Step: 63599, Reward: [-9.907 -9.907] [0.0000], Avg: [-699.533 -699.533] (0.020)
Step: 63649, Reward: [-207.689 -207.689] [0.0000], Avg: [-699.147 -699.147] (0.020)
Step: 63699, Reward: [-154.733 -154.733] [0.0000], Avg: [-698.72 -698.72] (0.020)
Step: 63749, Reward: [-111.01 -111.01] [0.0000], Avg: [-698.259 -698.259] (0.020)
Step: 63799, Reward: [-283.32 -283.32] [0.0000], Avg: [-697.934 -697.934] (0.020)
Step: 63849, Reward: [-180.735 -180.735] [0.0000], Avg: [-697.529 -697.529] (0.020)
Step: 63899, Reward: [-22.304 -22.304] [0.0000], Avg: [-697. -697.] (0.020)
Step: 63949, Reward: [-109.695 -109.695] [0.0000], Avg: [-696.541 -696.541] (0.020)
Step: 63999, Reward: [-1.844 -1.844] [0.0000], Avg: [-695.998 -695.998] (0.020)
Step: 64049, Reward: [-32.014 -32.014] [0.0000], Avg: [-695.48 -695.48] (0.020)
Step: 64099, Reward: [-28.084 -28.084] [0.0000], Avg: [-694.959 -694.959] (0.020)
Step: 64149, Reward: [-205.605 -205.605] [0.0000], Avg: [-694.578 -694.578] (0.020)
Step: 64199, Reward: [-46.241 -46.241] [0.0000], Avg: [-694.073 -694.073] (0.020)
Step: 64249, Reward: [-181.495 -181.495] [0.0000], Avg: [-693.674 -693.674] (0.020)
Step: 64299, Reward: [-237.18 -237.18] [0.0000], Avg: [-693.319 -693.319] (0.020)
Step: 64349, Reward: [-2.359 -2.359] [0.0000], Avg: [-692.782 -692.782] (0.020)
Step: 64399, Reward: [-47.232 -47.232] [0.0000], Avg: [-692.281 -692.281] (0.020)
Step: 64449, Reward: [-245.578 -245.578] [0.0000], Avg: [-691.935 -691.935] (0.020)
Step: 64499, Reward: [-164.675 -164.675] [0.0000], Avg: [-691.526 -691.526] (0.020)
Step: 64549, Reward: [-117.066 -117.066] [0.0000], Avg: [-691.081 -691.081] (0.020)
Step: 64599, Reward: [-33.394 -33.394] [0.0000], Avg: [-690.572 -690.572] (0.020)
Step: 64649, Reward: [-222.951 -222.951] [0.0000], Avg: [-690.21 -690.21] (0.020)
Step: 64699, Reward: [-10.329 -10.329] [0.0000], Avg: [-689.685 -689.685] (0.020)
Step: 64749, Reward: [-3.286 -3.286] [0.0000], Avg: [-689.155 -689.155] (0.020)
Step: 64799, Reward: [-15.606 -15.606] [0.0000], Avg: [-688.635 -688.635] (0.020)
Step: 64849, Reward: [-130.102 -130.102] [0.0000], Avg: [-688.204 -688.204] (0.020)
Step: 64899, Reward: [-226.263 -226.263] [0.0000], Avg: [-687.848 -687.848] (0.020)
Step: 64949, Reward: [-399.548 -399.548] [0.0000], Avg: [-687.627 -687.627] (0.020)
Step: 64999, Reward: [-20.21 -20.21] [0.0000], Avg: [-687.113 -687.113] (0.020)
Step: 65049, Reward: [-159.836 -159.836] [0.0000], Avg: [-686.708 -686.708] (0.020)
Step: 65099, Reward: [-174.272 -174.272] [0.0000], Avg: [-686.314 -686.314] (0.020)
Step: 65149, Reward: [-373.183 -373.183] [0.0000], Avg: [-686.074 -686.074] (0.020)
Step: 65199, Reward: [-20.224 -20.224] [0.0000], Avg: [-685.563 -685.563] (0.020)
Step: 65249, Reward: [-173.185 -173.185] [0.0000], Avg: [-685.171 -685.171] (0.020)
Step: 65299, Reward: [-90.357 -90.357] [0.0000], Avg: [-684.715 -684.715] (0.020)
Step: 65349, Reward: [-10.952 -10.952] [0.0000], Avg: [-684.2 -684.2] (0.020)
Step: 65399, Reward: [-200.18 -200.18] [0.0000], Avg: [-683.83 -683.83] (0.020)
Step: 65449, Reward: [-104.24 -104.24] [0.0000], Avg: [-683.387 -683.387] (0.020)
Step: 65499, Reward: [-44.151 -44.151] [0.0000], Avg: [-682.899 -682.899] (0.020)
Step: 65549, Reward: [-93.226 -93.226] [0.0000], Avg: [-682.449 -682.449] (0.020)
Step: 65599, Reward: [-7.284 -7.284] [0.0000], Avg: [-681.935 -681.935] (0.020)
Step: 65649, Reward: [-144.973 -144.973] [0.0000], Avg: [-681.526 -681.526] (0.020)
Step: 65699, Reward: [-24.919 -24.919] [0.0000], Avg: [-681.026 -681.026] (0.020)
Step: 65749, Reward: [-25.047 -25.047] [0.0000], Avg: [-680.527 -680.527] (0.020)
Step: 65799, Reward: [-110.54 -110.54] [0.0000], Avg: [-680.094 -680.094] (0.020)
Step: 65849, Reward: [-45.834 -45.834] [0.0000], Avg: [-679.612 -679.612] (0.020)
Step: 65899, Reward: [-134.087 -134.087] [0.0000], Avg: [-679.198 -679.198] (0.020)
Step: 65949, Reward: [-355.836 -355.836] [0.0000], Avg: [-678.953 -678.953] (0.020)
Step: 65999, Reward: [-105.856 -105.856] [0.0000], Avg: [-678.519 -678.519] (0.020)
Step: 66049, Reward: [-10.443 -10.443] [0.0000], Avg: [-678.013 -678.013] (0.020)
Step: 66099, Reward: [-325.314 -325.314] [0.0000], Avg: [-677.747 -677.747] (0.020)
Step: 66149, Reward: [-144.271 -144.271] [0.0000], Avg: [-677.343 -677.343] (0.020)
Step: 66199, Reward: [-133.489 -133.489] [0.0000], Avg: [-676.933 -676.933] (0.020)
Step: 66249, Reward: [-214.091 -214.091] [0.0000], Avg: [-676.583 -676.583] (0.020)
Step: 66299, Reward: [-170.187 -170.187] [0.0000], Avg: [-676.201 -676.201] (0.020)
Step: 66349, Reward: [-3.336 -3.336] [0.0000], Avg: [-675.694 -675.694] (0.020)
Step: 66399, Reward: [-133.782 -133.782] [0.0000], Avg: [-675.286 -675.286] (0.020)
Step: 66449, Reward: [-79.764 -79.764] [0.0000], Avg: [-674.838 -674.838] (0.020)
Step: 66499, Reward: [-25.431 -25.431] [0.0000], Avg: [-674.35 -674.35] (0.020)
Step: 66549, Reward: [-407.898 -407.898] [0.0000], Avg: [-674.15 -674.15] (0.020)
Step: 66599, Reward: [-47.363 -47.363] [0.0000], Avg: [-673.679 -673.679] (0.020)
Step: 66649, Reward: [-56.644 -56.644] [0.0000], Avg: [-673.216 -673.216] (0.020)
Step: 66699, Reward: [-19.711 -19.711] [0.0000], Avg: [-672.726 -672.726] (0.020)
Step: 66749, Reward: [-139.946 -139.946] [0.0000], Avg: [-672.327 -672.327] (0.020)
Step: 66799, Reward: [-121.444 -121.444] [0.0000], Avg: [-671.915 -671.915] (0.020)
Step: 66849, Reward: [-2.116 -2.116] [0.0000], Avg: [-671.414 -671.414] (0.020)
Step: 66899, Reward: [-46.27 -46.27] [0.0000], Avg: [-670.947 -670.947] (0.020)
Step: 66949, Reward: [-35.051 -35.051] [0.0000], Avg: [-670.472 -670.472] (0.020)
Step: 66999, Reward: [-96.849 -96.849] [0.0000], Avg: [-670.044 -670.044] (0.020)
Step: 67049, Reward: [-16.561 -16.561] [0.0000], Avg: [-669.556 -669.556] (0.020)
Step: 67099, Reward: [-168.636 -168.636] [0.0000], Avg: [-669.183 -669.183] (0.020)
Step: 67149, Reward: [-110.064 -110.064] [0.0000], Avg: [-668.767 -668.767] (0.020)
Step: 67199, Reward: [-44.938 -44.938] [0.0000], Avg: [-668.303 -668.303] (0.020)
Step: 67249, Reward: [-7.704 -7.704] [0.0000], Avg: [-667.812 -667.812] (0.020)
Step: 67299, Reward: [-384.42 -384.42] [0.0000], Avg: [-667.601 -667.601] (0.020)
Step: 67349, Reward: [-168.98 -168.98] [0.0000], Avg: [-667.231 -667.231] (0.020)
Step: 67399, Reward: [-85.982 -85.982] [0.0000], Avg: [-666.8 -666.8] (0.020)
Step: 67449, Reward: [-155.409 -155.409] [0.0000], Avg: [-666.421 -666.421] (0.020)
Step: 67499, Reward: [-143.542 -143.542] [0.0000], Avg: [-666.033 -666.033] (0.020)
Step: 67549, Reward: [-146.447 -146.447] [0.0000], Avg: [-665.649 -665.649] (0.020)
Step: 67599, Reward: [-128.397 -128.397] [0.0000], Avg: [-665.251 -665.251] (0.020)
Step: 67649, Reward: [-47.625 -47.625] [0.0000], Avg: [-664.795 -664.795] (0.020)
Step: 67699, Reward: [-18.417 -18.417] [0.0000], Avg: [-664.317 -664.317] (0.020)
Step: 67749, Reward: [-255.983 -255.983] [0.0000], Avg: [-664.016 -664.016] (0.020)
Step: 67799, Reward: [-202.064 -202.064] [0.0000], Avg: [-663.675 -663.675] (0.020)
Step: 67849, Reward: [-31.679 -31.679] [0.0000], Avg: [-663.21 -663.21] (0.020)
Step: 67899, Reward: [-92.149 -92.149] [0.0000], Avg: [-662.789 -662.789] (0.020)
Step: 67949, Reward: [-2.896 -2.896] [0.0000], Avg: [-662.304 -662.304] (0.020)
Step: 67999, Reward: [-258.588 -258.588] [0.0000], Avg: [-662.007 -662.007] (0.020)
Step: 68049, Reward: [-9.243 -9.243] [0.0000], Avg: [-661.527 -661.527] (0.020)
Step: 68099, Reward: [-145.615 -145.615] [0.0000], Avg: [-661.148 -661.148] (0.020)
Step: 68149, Reward: [-122.022 -122.022] [0.0000], Avg: [-660.753 -660.753] (0.020)
Step: 68199, Reward: [-48.315 -48.315] [0.0000], Avg: [-660.304 -660.304] (0.020)
Step: 68249, Reward: [-16.872 -16.872] [0.0000], Avg: [-659.832 -659.832] (0.020)
Step: 68299, Reward: [-217.645 -217.645] [0.0000], Avg: [-659.509 -659.509] (0.020)
Step: 68349, Reward: [-27.513 -27.513] [0.0000], Avg: [-659.046 -659.046] (0.020)
Step: 68399, Reward: [-146.733 -146.733] [0.0000], Avg: [-658.672 -658.672] (0.020)
Step: 68449, Reward: [-219.108 -219.108] [0.0000], Avg: [-658.351 -658.351] (0.020)
Step: 68499, Reward: [-120.681 -120.681] [0.0000], Avg: [-657.958 -657.958] (0.020)
Step: 68549, Reward: [-154.061 -154.061] [0.0000], Avg: [-657.591 -657.591] (0.020)
Step: 68599, Reward: [-14.801 -14.801] [0.0000], Avg: [-657.122 -657.122] (0.020)
Step: 68649, Reward: [-20.953 -20.953] [0.0000], Avg: [-656.659 -656.659] (0.020)
Step: 68699, Reward: [-120.943 -120.943] [0.0000], Avg: [-656.269 -656.269] (0.020)
Step: 68749, Reward: [-224.3 -224.3] [0.0000], Avg: [-655.955 -655.955] (0.020)
Step: 68799, Reward: [-107.001 -107.001] [0.0000], Avg: [-655.556 -655.556] (0.020)
Step: 68849, Reward: [-307.569 -307.569] [0.0000], Avg: [-655.303 -655.303] (0.020)
Step: 68899, Reward: [-168.792 -168.792] [0.0000], Avg: [-654.95 -654.95] (0.020)
Step: 68949, Reward: [-271.182 -271.182] [0.0000], Avg: [-654.672 -654.672] (0.020)
Step: 68999, Reward: [-79.703 -79.703] [0.0000], Avg: [-654.255 -654.255] (0.020)
Step: 69049, Reward: [-2.596 -2.596] [0.0000], Avg: [-653.783 -653.783] (0.020)
Step: 69099, Reward: [-214.542 -214.542] [0.0000], Avg: [-653.466 -653.466] (0.020)
Step: 69149, Reward: [-184.496 -184.496] [0.0000], Avg: [-653.126 -653.126] (0.020)
Step: 69199, Reward: [-65.641 -65.641] [0.0000], Avg: [-652.702 -652.702] (0.020)
Step: 69249, Reward: [-296.166 -296.166] [0.0000], Avg: [-652.445 -652.445] (0.020)
Step: 69299, Reward: [-200.921 -200.921] [0.0000], Avg: [-652.119 -652.119] (0.020)
Step: 69349, Reward: [-37.543 -37.543] [0.0000], Avg: [-651.676 -651.676] (0.020)
Step: 69399, Reward: [-66.047 -66.047] [0.0000], Avg: [-651.254 -651.254] (0.020)
Step: 69449, Reward: [-40.581 -40.581] [0.0000], Avg: [-650.814 -650.814] (0.020)
Step: 69499, Reward: [-106.836 -106.836] [0.0000], Avg: [-650.423 -650.423] (0.020)
Step: 69549, Reward: [-152.676 -152.676] [0.0000], Avg: [-650.065 -650.065] (0.020)
Step: 69599, Reward: [-104.754 -104.754] [0.0000], Avg: [-649.673 -649.673] (0.020)
Step: 69649, Reward: [-106.079 -106.079] [0.0000], Avg: [-649.283 -649.283] (0.020)
Step: 69699, Reward: [-75.017 -75.017] [0.0000], Avg: [-648.871 -648.871] (0.020)
Step: 69749, Reward: [-125.627 -125.627] [0.0000], Avg: [-648.496 -648.496] (0.020)
Step: 69799, Reward: [-90.463 -90.463] [0.0000], Avg: [-648.096 -648.096] (0.020)
Step: 69849, Reward: [-11.094 -11.094] [0.0000], Avg: [-647.64 -647.64] (0.020)
Step: 69899, Reward: [-257.949 -257.949] [0.0000], Avg: [-647.361 -647.361] (0.020)
Step: 69949, Reward: [-37.226 -37.226] [0.0000], Avg: [-646.925 -646.925] (0.020)
Step: 69999, Reward: [-3.73 -3.73] [0.0000], Avg: [-646.466 -646.466] (0.020)
Step: 70049, Reward: [-70.773 -70.773] [0.0000], Avg: [-646.055 -646.055] (0.020)
Step: 70099, Reward: [-62.735 -62.735] [0.0000], Avg: [-645.639 -645.639] (0.020)
Step: 70149, Reward: [-13.427 -13.427] [0.0000], Avg: [-645.188 -645.188] (0.020)
Step: 70199, Reward: [-5.648 -5.648] [0.0000], Avg: [-644.733 -644.733] (0.020)
Step: 70249, Reward: [-178.516 -178.516] [0.0000], Avg: [-644.401 -644.401] (0.020)
Step: 70299, Reward: [-176.028 -176.028] [0.0000], Avg: [-644.068 -644.068] (0.020)
Step: 70349, Reward: [-161.363 -161.363] [0.0000], Avg: [-643.725 -643.725] (0.020)
Step: 70399, Reward: [-68.632 -68.632] [0.0000], Avg: [-643.316 -643.316] (0.020)
Step: 70449, Reward: [-161.863 -161.863] [0.0000], Avg: [-642.975 -642.975] (0.020)
Step: 70499, Reward: [-1.711 -1.711] [0.0000], Avg: [-642.52 -642.52] (0.020)
Step: 70549, Reward: [-185.605 -185.605] [0.0000], Avg: [-642.196 -642.196] (0.020)
Step: 70599, Reward: [-185.748 -185.748] [0.0000], Avg: [-641.873 -641.873] (0.020)
Step: 70649, Reward: [-15.137 -15.137] [0.0000], Avg: [-641.429 -641.429] (0.020)
Step: 70699, Reward: [-127.925 -127.925] [0.0000], Avg: [-641.066 -641.066] (0.020)
Step: 70749, Reward: [-469.826 -469.826] [0.0000], Avg: [-640.945 -640.945] (0.020)
Step: 70799, Reward: [-153.249 -153.249] [0.0000], Avg: [-640.601 -640.601] (0.020)
Step: 70849, Reward: [-120.074 -120.074] [0.0000], Avg: [-640.233 -640.233] (0.020)
Step: 70899, Reward: [-111.19 -111.19] [0.0000], Avg: [-639.86 -639.86] (0.020)
Step: 70949, Reward: [-266.868 -266.868] [0.0000], Avg: [-639.597 -639.597] (0.020)
Step: 70999, Reward: [-207.402 -207.402] [0.0000], Avg: [-639.293 -639.293] (0.020)
Step: 71049, Reward: [-412.458 -412.458] [0.0000], Avg: [-639.133 -639.133] (0.020)
Step: 71099, Reward: [-221.889 -221.889] [0.0000], Avg: [-638.84 -638.84] (0.020)
Step: 71149, Reward: [-13.301 -13.301] [0.0000], Avg: [-638.4 -638.4] (0.020)
Step: 71199, Reward: [-9.359 -9.359] [0.0000], Avg: [-637.959 -637.959] (0.020)
Step: 71249, Reward: [-449.172 -449.172] [0.0000], Avg: [-637.826 -637.826] (0.020)
Step: 71299, Reward: [-123.043 -123.043] [0.0000], Avg: [-637.465 -637.465] (0.020)
Step: 71349, Reward: [-170.714 -170.714] [0.0000], Avg: [-637.138 -637.138] (0.020)
Step: 71399, Reward: [-213.58 -213.58] [0.0000], Avg: [-636.841 -636.841] (0.020)
Step: 71449, Reward: [-8.942 -8.942] [0.0000], Avg: [-636.402 -636.402] (0.020)
Step: 71499, Reward: [-288.086 -288.086] [0.0000], Avg: [-636.158 -636.158] (0.020)
Step: 71549, Reward: [-15.661 -15.661] [0.0000], Avg: [-635.725 -635.725] (0.020)
Step: 71599, Reward: [-132.646 -132.646] [0.0000], Avg: [-635.373 -635.373] (0.020)
Step: 71649, Reward: [-352.474 -352.474] [0.0000], Avg: [-635.176 -635.176] (0.020)
Step: 71699, Reward: [-32.695 -32.695] [0.0000], Avg: [-634.756 -634.756] (0.020)
Step: 71749, Reward: [-4.268 -4.268] [0.0000], Avg: [-634.317 -634.317] (0.020)
Step: 71799, Reward: [-137.686 -137.686] [0.0000], Avg: [-633.971 -633.971] (0.020)
Step: 71849, Reward: [-56.494 -56.494] [0.0000], Avg: [-633.569 -633.569] (0.020)
Step: 71899, Reward: [-465.148 -465.148] [0.0000], Avg: [-633.452 -633.452] (0.020)
Step: 71949, Reward: [-156.666 -156.666] [0.0000], Avg: [-633.12 -633.12] (0.020)
Step: 71999, Reward: [-309.564 -309.564] [0.0000], Avg: [-632.896 -632.896] (0.020)
Step: 72049, Reward: [-106.813 -106.813] [0.0000], Avg: [-632.531 -632.531] (0.020)
Step: 72099, Reward: [-163.315 -163.315] [0.0000], Avg: [-632.205 -632.205] (0.020)
Step: 72149, Reward: [-1.42 -1.42] [0.0000], Avg: [-631.768 -631.768] (0.020)
Step: 72199, Reward: [-87.303 -87.303] [0.0000], Avg: [-631.391 -631.391] (0.020)
Step: 72249, Reward: [-85.604 -85.604] [0.0000], Avg: [-631.013 -631.013] (0.020)
Step: 72299, Reward: [-140.904 -140.904] [0.0000], Avg: [-630.674 -630.674] (0.020)
Step: 72349, Reward: [-270.27 -270.27] [0.0000], Avg: [-630.425 -630.425] (0.020)
Step: 72399, Reward: [-120.249 -120.249] [0.0000], Avg: [-630.073 -630.073] (0.020)
Step: 72449, Reward: [-18.351 -18.351] [0.0000], Avg: [-629.651 -629.651] (0.020)
Step: 72499, Reward: [-372.692 -372.692] [0.0000], Avg: [-629.474 -629.474] (0.020)
Step: 72549, Reward: [-70.338 -70.338] [0.0000], Avg: [-629.088 -629.088] (0.020)
Step: 72599, Reward: [-29.058 -29.058] [0.0000], Avg: [-628.675 -628.675] (0.020)
Step: 72649, Reward: [-433.317 -433.317] [0.0000], Avg: [-628.541 -628.541] (0.020)
Step: 72699, Reward: [-83.782 -83.782] [0.0000], Avg: [-628.166 -628.166] (0.020)
Step: 72749, Reward: [-295.575 -295.575] [0.0000], Avg: [-627.937 -627.937] (0.020)
Step: 72799, Reward: [-66.77 -66.77] [0.0000], Avg: [-627.552 -627.552] (0.020)
Step: 72849, Reward: [-292.74 -292.74] [0.0000], Avg: [-627.322 -627.322] (0.020)
Step: 72899, Reward: [-252.853 -252.853] [0.0000], Avg: [-627.065 -627.065] (0.020)
Step: 72949, Reward: [-63.757 -63.757] [0.0000], Avg: [-626.679 -626.679] (0.020)
Step: 72999, Reward: [-221.072 -221.072] [0.0000], Avg: [-626.401 -626.401] (0.020)
Step: 73049, Reward: [-75.411 -75.411] [0.0000], Avg: [-626.024 -626.024] (0.020)
Step: 73099, Reward: [-140.913 -140.913] [0.0000], Avg: [-625.692 -625.692] (0.020)
Step: 73149, Reward: [-339.983 -339.983] [0.0000], Avg: [-625.497 -625.497] (0.020)
Step: 73199, Reward: [-157.871 -157.871] [0.0000], Avg: [-625.178 -625.178] (0.020)
Step: 73249, Reward: [-183.814 -183.814] [0.0000], Avg: [-624.876 -624.876] (0.020)
Step: 73299, Reward: [-53.145 -53.145] [0.0000], Avg: [-624.486 -624.486] (0.020)
Step: 73349, Reward: [-461.431 -461.431] [0.0000], Avg: [-624.375 -624.375] (0.020)
Step: 73399, Reward: [-20.439 -20.439] [0.0000], Avg: [-623.964 -623.964] (0.020)
Step: 73449, Reward: [-220.449 -220.449] [0.0000], Avg: [-623.689 -623.689] (0.020)
Step: 73499, Reward: [-11.292 -11.292] [0.0000], Avg: [-623.273 -623.273] (0.020)
Step: 73549, Reward: [-204.565 -204.565] [0.0000], Avg: [-622.988 -622.988] (0.020)
Step: 73599, Reward: [-78.047 -78.047] [0.0000], Avg: [-622.618 -622.618] (0.020)
Step: 73649, Reward: [-227.334 -227.334] [0.0000], Avg: [-622.349 -622.349] (0.020)
Step: 73699, Reward: [-26.992 -26.992] [0.0000], Avg: [-621.945 -621.945] (0.020)
Step: 73749, Reward: [-141.292 -141.292] [0.0000], Avg: [-621.62 -621.62] (0.020)
Step: 73799, Reward: [-232.251 -232.251] [0.0000], Avg: [-621.356 -621.356] (0.020)
Step: 73849, Reward: [-87.155 -87.155] [0.0000], Avg: [-620.994 -620.994] (0.020)
Step: 73899, Reward: [-181.232 -181.232] [0.0000], Avg: [-620.697 -620.697] (0.020)
Step: 73949, Reward: [-494.848 -494.848] [0.0000], Avg: [-620.611 -620.611] (0.020)
Step: 73999, Reward: [-153.053 -153.053] [0.0000], Avg: [-620.296 -620.296] (0.020)
Step: 74049, Reward: [-208.409 -208.409] [0.0000], Avg: [-620.017 -620.017] (0.020)
Step: 74099, Reward: [-131.44 -131.44] [0.0000], Avg: [-619.688 -619.688] (0.020)
Step: 74149, Reward: [-79.354 -79.354] [0.0000], Avg: [-619.323 -619.323] (0.020)
Step: 74199, Reward: [-6.722 -6.722] [0.0000], Avg: [-618.911 -618.911] (0.020)
Step: 74249, Reward: [-73.197 -73.197] [0.0000], Avg: [-618.543 -618.543] (0.020)
Step: 74299, Reward: [-257.913 -257.913] [0.0000], Avg: [-618.3 -618.3] (0.020)
Step: 74349, Reward: [-108.875 -108.875] [0.0000], Avg: [-617.958 -617.958] (0.020)
Step: 74399, Reward: [-60.045 -60.045] [0.0000], Avg: [-617.583 -617.583] (0.020)
Step: 74449, Reward: [-166.067 -166.067] [0.0000], Avg: [-617.28 -617.28] (0.020)
Step: 74499, Reward: [-184.764 -184.764] [0.0000], Avg: [-616.989 -616.989] (0.020)
Step: 74549, Reward: [-176.444 -176.444] [0.0000], Avg: [-616.694 -616.694] (0.020)
Step: 74599, Reward: [-92.543 -92.543] [0.0000], Avg: [-616.343 -616.343] (0.020)
Step: 74649, Reward: [-49.987 -49.987] [0.0000], Avg: [-615.963 -615.963] (0.020)
Step: 74699, Reward: [-329.744 -329.744] [0.0000], Avg: [-615.772 -615.772] (0.020)
Step: 74749, Reward: [-138.024 -138.024] [0.0000], Avg: [-615.452 -615.452] (0.020)
Step: 74799, Reward: [-232.707 -232.707] [0.0000], Avg: [-615.196 -615.196] (0.020)
Step: 74849, Reward: [-99.417 -99.417] [0.0000], Avg: [-614.852 -614.852] (0.020)
Step: 74899, Reward: [-213.047 -213.047] [0.0000], Avg: [-614.584 -614.584] (0.020)
Step: 74949, Reward: [-3.749 -3.749] [0.0000], Avg: [-614.176 -614.176] (0.020)
Step: 74999, Reward: [-108.101 -108.101] [0.0000], Avg: [-613.839 -613.839] (0.020)
Step: 75049, Reward: [-334.709 -334.709] [0.0000], Avg: [-613.653 -613.653] (0.020)
Step: 75099, Reward: [-1.678 -1.678] [0.0000], Avg: [-613.245 -613.245] (0.020)
Step: 75149, Reward: [-95.727 -95.727] [0.0000], Avg: [-612.901 -612.901] (0.020)
Step: 75199, Reward: [-66. -66.] [0.0000], Avg: [-612.537 -612.537] (0.020)
Step: 75249, Reward: [-179.019 -179.019] [0.0000], Avg: [-612.249 -612.249] (0.020)
Step: 75299, Reward: [-196.433 -196.433] [0.0000], Avg: [-611.973 -611.973] (0.020)
Step: 75349, Reward: [-196.64 -196.64] [0.0000], Avg: [-611.698 -611.698] (0.020)
Step: 75399, Reward: [-150.841 -150.841] [0.0000], Avg: [-611.392 -611.392] (0.020)
Step: 75449, Reward: [-8.664 -8.664] [0.0000], Avg: [-610.993 -610.993] (0.020)
Step: 75499, Reward: [-12.94 -12.94] [0.0000], Avg: [-610.596 -610.596] (0.020)
Step: 75549, Reward: [-172.192 -172.192] [0.0000], Avg: [-610.306 -610.306] (0.020)
Step: 75599, Reward: [-195.81 -195.81] [0.0000], Avg: [-610.032 -610.032] (0.020)
Step: 75649, Reward: [-6.823 -6.823] [0.0000], Avg: [-609.634 -609.634] (0.020)
Step: 75699, Reward: [-60.139 -60.139] [0.0000], Avg: [-609.271 -609.271] (0.020)
Step: 75749, Reward: [-203.736 -203.736] [0.0000], Avg: [-609.003 -609.003] (0.020)
Step: 75799, Reward: [-31.004 -31.004] [0.0000], Avg: [-608.622 -608.622] (0.020)
Step: 75849, Reward: [-306.752 -306.752] [0.0000], Avg: [-608.423 -608.423] (0.020)
Step: 75899, Reward: [-278.988 -278.988] [0.0000], Avg: [-608.206 -608.206] (0.020)
Step: 75949, Reward: [-236.888 -236.888] [0.0000], Avg: [-607.961 -607.961] (0.020)
Step: 75999, Reward: [-28.59 -28.59] [0.0000], Avg: [-607.58 -607.58] (0.020)
Step: 76049, Reward: [-195.021 -195.021] [0.0000], Avg: [-607.309 -607.309] (0.020)
Step: 76099, Reward: [-195.23 -195.23] [0.0000], Avg: [-607.038 -607.038] (0.020)
Step: 76149, Reward: [-224.34 -224.34] [0.0000], Avg: [-606.787 -606.787] (0.020)
Step: 76199, Reward: [-0.046 -0.046] [0.0000], Avg: [-606.389 -606.389] (0.020)
Step: 76249, Reward: [-12.957 -12.957] [0.0000], Avg: [-605.999 -605.999] (0.020)
Step: 76299, Reward: [-9.872 -9.872] [0.0000], Avg: [-605.609 -605.609] (0.020)
Step: 76349, Reward: [-125.016 -125.016] [0.0000], Avg: [-605.294 -605.294] (0.020)
Step: 76399, Reward: [-105.349 -105.349] [0.0000], Avg: [-604.967 -604.967] (0.020)
Step: 76449, Reward: [-9.515 -9.515] [0.0000], Avg: [-604.577 -604.577] (0.020)
Step: 76499, Reward: [-258.608 -258.608] [0.0000], Avg: [-604.351 -604.351] (0.020)
Step: 76549, Reward: [-30.668 -30.668] [0.0000], Avg: [-603.977 -603.977] (0.020)
Step: 76599, Reward: [-15.296 -15.296] [0.0000], Avg: [-603.592 -603.592] (0.020)
Step: 76649, Reward: [-51.251 -51.251] [0.0000], Avg: [-603.232 -603.232] (0.020)
Step: 76699, Reward: [-286.228 -286.228] [0.0000], Avg: [-603.025 -603.025] (0.020)
Step: 76749, Reward: [-27.402 -27.402] [0.0000], Avg: [-602.65 -602.65] (0.020)
Step: 76799, Reward: [-70.908 -70.908] [0.0000], Avg: [-602.304 -602.304] (0.020)
Step: 76849, Reward: [-3.473 -3.473] [0.0000], Avg: [-601.915 -601.915] (0.020)
Step: 76899, Reward: [-17.73 -17.73] [0.0000], Avg: [-601.535 -601.535] (0.020)
Step: 76949, Reward: [-212.221 -212.221] [0.0000], Avg: [-601.282 -601.282] (0.020)
Step: 76999, Reward: [-182.328 -182.328] [0.0000], Avg: [-601.01 -601.01] (0.020)
Step: 77049, Reward: [-287.088 -287.088] [0.0000], Avg: [-600.806 -600.806] (0.020)
Step: 77099, Reward: [-36.258 -36.258] [0.0000], Avg: [-600.44 -600.44] (0.020)
Step: 77149, Reward: [-206.893 -206.893] [0.0000], Avg: [-600.185 -600.185] (0.020)
Step: 77199, Reward: [-8.605 -8.605] [0.0000], Avg: [-599.802 -599.802] (0.020)
Step: 77249, Reward: [-49.021 -49.021] [0.0000], Avg: [-599.445 -599.445] (0.020)
Step: 77299, Reward: [-9.811 -9.811] [0.0000], Avg: [-599.064 -599.064] (0.020)
Step: 77349, Reward: [-163.953 -163.953] [0.0000], Avg: [-598.783 -598.783] (0.020)
Step: 77399, Reward: [-52.077 -52.077] [0.0000], Avg: [-598.429 -598.429] (0.020)
Step: 77449, Reward: [-218.005 -218.005] [0.0000], Avg: [-598.184 -598.184] (0.020)
Step: 77499, Reward: [-91.558 -91.558] [0.0000], Avg: [-597.857 -597.857] (0.020)
Step: 77549, Reward: [-332.29 -332.29] [0.0000], Avg: [-597.686 -597.686] (0.020)
Step: 77599, Reward: [-293.51 -293.51] [0.0000], Avg: [-597.49 -597.49] (0.020)
Step: 77649, Reward: [-190.732 -190.732] [0.0000], Avg: [-597.228 -597.228] (0.020)
Step: 77699, Reward: [-125.186 -125.186] [0.0000], Avg: [-596.924 -596.924] (0.020)
Step: 77749, Reward: [-41.72 -41.72] [0.0000], Avg: [-596.567 -596.567] (0.020)
Step: 77799, Reward: [-59.106 -59.106] [0.0000], Avg: [-596.222 -596.222] (0.020)
Step: 77849, Reward: [-56.622 -56.622] [0.0000], Avg: [-595.875 -595.875] (0.020)
Step: 77899, Reward: [-180.767 -180.767] [0.0000], Avg: [-595.609 -595.609] (0.020)
Step: 77949, Reward: [-66.538 -66.538] [0.0000], Avg: [-595.269 -595.269] (0.020)
Step: 77999, Reward: [-162.757 -162.757] [0.0000], Avg: [-594.992 -594.992] (0.020)
Step: 78049, Reward: [-42.17 -42.17] [0.0000], Avg: [-594.638 -594.638] (0.020)
Step: 78099, Reward: [-23.464 -23.464] [0.0000], Avg: [-594.272 -594.272] (0.020)
Step: 78149, Reward: [-132.968 -132.968] [0.0000], Avg: [-593.977 -593.977] (0.020)
Step: 78199, Reward: [-2.944 -2.944] [0.0000], Avg: [-593.599 -593.599] (0.020)
Step: 78249, Reward: [-196.609 -196.609] [0.0000], Avg: [-593.345 -593.345] (0.020)
Step: 78299, Reward: [-97.118 -97.118] [0.0000], Avg: [-593.029 -593.029] (0.020)
Step: 78349, Reward: [-100.323 -100.323] [0.0000], Avg: [-592.714 -592.714] (0.020)
Step: 78399, Reward: [-23.019 -23.019] [0.0000], Avg: [-592.351 -592.351] (0.020)
Step: 78449, Reward: [-230.306 -230.306] [0.0000], Avg: [-592.12 -592.12] (0.020)
Step: 78499, Reward: [-72.253 -72.253] [0.0000], Avg: [-591.789 -591.789] (0.020)
Step: 78549, Reward: [-93.076 -93.076] [0.0000], Avg: [-591.472 -591.472] (0.020)
Step: 78599, Reward: [-56.072 -56.072] [0.0000], Avg: [-591.131 -591.131] (0.020)
Step: 78649, Reward: [-46.963 -46.963] [0.0000], Avg: [-590.785 -590.785] (0.020)
Step: 78699, Reward: [-10.86 -10.86] [0.0000], Avg: [-590.417 -590.417] (0.020)
Step: 78749, Reward: [-451.266 -451.266] [0.0000], Avg: [-590.328 -590.328] (0.020)
Step: 78799, Reward: [-42.619 -42.619] [0.0000], Avg: [-589.981 -589.981] (0.020)
Step: 78849, Reward: [-1.115 -1.115] [0.0000], Avg: [-589.607 -589.607] (0.020)
Step: 78899, Reward: [-37.919 -37.919] [0.0000], Avg: [-589.258 -589.258] (0.020)
Step: 78949, Reward: [-230.657 -230.657] [0.0000], Avg: [-589.031 -589.031] (0.020)
Step: 78999, Reward: [-368.201 -368.201] [0.0000], Avg: [-588.891 -588.891] (0.020)
Step: 79049, Reward: [-406.09 -406.09] [0.0000], Avg: [-588.775 -588.775] (0.020)
Step: 79099, Reward: [-40.481 -40.481] [0.0000], Avg: [-588.429 -588.429] (0.020)
Step: 79149, Reward: [-21.515 -21.515] [0.0000], Avg: [-588.07 -588.07] (0.020)
Step: 79199, Reward: [-152.649 -152.649] [0.0000], Avg: [-587.796 -587.796] (0.020)
Step: 79249, Reward: [-251.673 -251.673] [0.0000], Avg: [-587.584 -587.584] (0.020)
Step: 79299, Reward: [-177.879 -177.879] [0.0000], Avg: [-587.325 -587.325] (0.020)
Step: 79349, Reward: [-135.211 -135.211] [0.0000], Avg: [-587.04 -587.04] (0.020)
Step: 79399, Reward: [-53.432 -53.432] [0.0000], Avg: [-586.704 -586.704] (0.020)
Step: 79449, Reward: [-119.131 -119.131] [0.0000], Avg: [-586.41 -586.41] (0.020)
Step: 79499, Reward: [-500.049 -500.049] [0.0000], Avg: [-586.356 -586.356] (0.020)
Step: 79549, Reward: [-74.118 -74.118] [0.0000], Avg: [-586.034 -586.034] (0.020)
Step: 79599, Reward: [-130.731 -130.731] [0.0000], Avg: [-585.748 -585.748] (0.020)
Step: 79649, Reward: [-40.903 -40.903] [0.0000], Avg: [-585.406 -585.406] (0.020)
Step: 79699, Reward: [-119.084 -119.084] [0.0000], Avg: [-585.113 -585.113] (0.020)
Step: 79749, Reward: [-14.804 -14.804] [0.0000], Avg: [-584.756 -584.756] (0.020)
Step: 79799, Reward: [-107.846 -107.846] [0.0000], Avg: [-584.457 -584.457] (0.020)
Step: 79849, Reward: [-81.169 -81.169] [0.0000], Avg: [-584.142 -584.142] (0.020)
Step: 79899, Reward: [-264.445 -264.445] [0.0000], Avg: [-583.942 -583.942] (0.020)
Step: 79949, Reward: [-288.611 -288.611] [0.0000], Avg: [-583.757 -583.757] (0.020)
Step: 79999, Reward: [-144.24 -144.24] [0.0000], Avg: [-583.482 -583.482] (0.020)
Step: 80049, Reward: [-30.598 -30.598] [0.0000], Avg: [-583.137 -583.137] (0.020)
Step: 80099, Reward: [-78.632 -78.632] [0.0000], Avg: [-582.822 -582.822] (0.020)
Step: 80149, Reward: [-320.212 -320.212] [0.0000], Avg: [-582.658 -582.658] (0.020)
Step: 80199, Reward: [-239.998 -239.998] [0.0000], Avg: [-582.444 -582.444] (0.020)
Step: 80249, Reward: [-301.905 -301.905] [0.0000], Avg: [-582.27 -582.27] (0.020)
Step: 80299, Reward: [-285.68 -285.68] [0.0000], Avg: [-582.085 -582.085] (0.020)
Step: 80349, Reward: [-139.089 -139.089] [0.0000], Avg: [-581.809 -581.809] (0.020)
Step: 80399, Reward: [-172.77 -172.77] [0.0000], Avg: [-581.555 -581.555] (0.020)
Step: 80449, Reward: [-72.389 -72.389] [0.0000], Avg: [-581.239 -581.239] (0.020)
Step: 80499, Reward: [-207.741 -207.741] [0.0000], Avg: [-581.007 -581.007] (0.020)
Step: 80549, Reward: [-17.783 -17.783] [0.0000], Avg: [-580.657 -580.657] (0.020)
Step: 80599, Reward: [-20.06 -20.06] [0.0000], Avg: [-580.309 -580.309] (0.020)
Step: 80649, Reward: [-133.433 -133.433] [0.0000], Avg: [-580.032 -580.032] (0.020)
Step: 80699, Reward: [-328.699 -328.699] [0.0000], Avg: [-579.876 -579.876] (0.020)
Step: 80749, Reward: [-182.508 -182.508] [0.0000], Avg: [-579.63 -579.63] (0.020)
Step: 80799, Reward: [-315.113 -315.113] [0.0000], Avg: [-579.467 -579.467] (0.020)
Step: 80849, Reward: [-175.668 -175.668] [0.0000], Avg: [-579.217 -579.217] (0.020)
Step: 80899, Reward: [-24.109 -24.109] [0.0000], Avg: [-578.874 -578.874] (0.020)
Step: 80949, Reward: [-40.939 -40.939] [0.0000], Avg: [-578.542 -578.542] (0.020)
Step: 80999, Reward: [-3.21 -3.21] [0.0000], Avg: [-578.186 -578.186] (0.020)
Step: 81049, Reward: [-142.698 -142.698] [0.0000], Avg: [-577.918 -577.918] (0.020)
Step: 81099, Reward: [-75.781 -75.781] [0.0000], Avg: [-577.608 -577.608] (0.020)
Step: 81149, Reward: [-191.849 -191.849] [0.0000], Avg: [-577.371 -577.371] (0.020)
Step: 81199, Reward: [-0.67 -0.67] [0.0000], Avg: [-577.015 -577.015] (0.020)
Step: 81249, Reward: [-12.644 -12.644] [0.0000], Avg: [-576.668 -576.668] (0.020)
Step: 81299, Reward: [-61.571 -61.571] [0.0000], Avg: [-576.351 -576.351] (0.020)
Step: 81349, Reward: [-13.185 -13.185] [0.0000], Avg: [-576.005 -576.005] (0.020)
Step: 81399, Reward: [-241.48 -241.48] [0.0000], Avg: [-575.8 -575.8] (0.020)
Step: 81449, Reward: [-13.868 -13.868] [0.0000], Avg: [-575.455 -575.455] (0.020)
Step: 81499, Reward: [-101.242 -101.242] [0.0000], Avg: [-575.164 -575.164] (0.020)
Step: 81549, Reward: [-76.677 -76.677] [0.0000], Avg: [-574.858 -574.858] (0.020)
Step: 81599, Reward: [-153.358 -153.358] [0.0000], Avg: [-574.6 -574.6] (0.020)
Step: 81649, Reward: [-21.239 -21.239] [0.0000], Avg: [-574.261 -574.261] (0.020)
Step: 81699, Reward: [-78.775 -78.775] [0.0000], Avg: [-573.958 -573.958] (0.020)
Step: 81749, Reward: [-34.4 -34.4] [0.0000], Avg: [-573.628 -573.628] (0.020)
Step: 81799, Reward: [-147.437 -147.437] [0.0000], Avg: [-573.367 -573.367] (0.020)
Step: 81849, Reward: [-7.99 -7.99] [0.0000], Avg: [-573.022 -573.022] (0.020)
Step: 81899, Reward: [-217.184 -217.184] [0.0000], Avg: [-572.805 -572.805] (0.020)
Step: 81949, Reward: [-390.904 -390.904] [0.0000], Avg: [-572.694 -572.694] (0.020)
Step: 81999, Reward: [-281.746 -281.746] [0.0000], Avg: [-572.516 -572.516] (0.020)
Step: 82049, Reward: [-303.975 -303.975] [0.0000], Avg: [-572.353 -572.353] (0.020)
Step: 82099, Reward: [-90.25 -90.25] [0.0000], Avg: [-572.059 -572.059] (0.020)
Step: 82149, Reward: [-116.468 -116.468] [0.0000], Avg: [-571.782 -571.782] (0.020)
Step: 82199, Reward: [-198.269 -198.269] [0.0000], Avg: [-571.555 -571.555] (0.020)
Step: 82249, Reward: [-212.574 -212.574] [0.0000], Avg: [-571.336 -571.336] (0.020)
Step: 82299, Reward: [-35.638 -35.638] [0.0000], Avg: [-571.011 -571.011] (0.020)
Step: 82349, Reward: [-188.433 -188.433] [0.0000], Avg: [-570.779 -570.779] (0.020)
Step: 82399, Reward: [-159.845 -159.845] [0.0000], Avg: [-570.529 -570.529] (0.020)
Step: 82449, Reward: [-26.996 -26.996] [0.0000], Avg: [-570.2 -570.2] (0.020)
Step: 82499, Reward: [-413.393 -413.393] [0.0000], Avg: [-570.105 -570.105] (0.020)
Step: 82549, Reward: [-128.683 -128.683] [0.0000], Avg: [-569.837 -569.837] (0.020)
Step: 82599, Reward: [-81.185 -81.185] [0.0000], Avg: [-569.541 -569.541] (0.020)
Step: 82649, Reward: [-22.221 -22.221] [0.0000], Avg: [-569.21 -569.21] (0.020)
Step: 82699, Reward: [-247.342 -247.342] [0.0000], Avg: [-569.016 -569.016] (0.020)
Step: 82749, Reward: [-236.022 -236.022] [0.0000], Avg: [-568.815 -568.815] (0.020)
Step: 82799, Reward: [-155.529 -155.529] [0.0000], Avg: [-568.565 -568.565] (0.020)
Step: 82849, Reward: [-193.298 -193.298] [0.0000], Avg: [-568.338 -568.338] (0.020)
Step: 82899, Reward: [-27.256 -27.256] [0.0000], Avg: [-568.012 -568.012] (0.020)
Step: 82949, Reward: [-87.324 -87.324] [0.0000], Avg: [-567.722 -567.722] (0.020)
Step: 82999, Reward: [-220.629 -220.629] [0.0000], Avg: [-567.513 -567.513] (0.020)
Step: 83049, Reward: [-75.292 -75.292] [0.0000], Avg: [-567.217 -567.217] (0.020)
Step: 83099, Reward: [-623.191 -623.191] [0.0000], Avg: [-567.251 -567.251] (0.020)
Step: 83149, Reward: [-32.184 -32.184] [0.0000], Avg: [-566.929 -566.929] (0.020)
Step: 83199, Reward: [-127.803 -127.803] [0.0000], Avg: [-566.665 -566.665] (0.020)
Step: 83249, Reward: [-92.009 -92.009] [0.0000], Avg: [-566.38 -566.38] (0.020)
Step: 83299, Reward: [-245.779 -245.779] [0.0000], Avg: [-566.187 -566.187] (0.020)
Step: 83349, Reward: [-55.055 -55.055] [0.0000], Avg: [-565.881 -565.881] (0.020)
Step: 83399, Reward: [-35.191 -35.191] [0.0000], Avg: [-565.563 -565.563] (0.020)
Step: 83449, Reward: [-182.509 -182.509] [0.0000], Avg: [-565.333 -565.333] (0.020)
Step: 83499, Reward: [-4.666 -4.666] [0.0000], Avg: [-564.997 -564.997] (0.020)
Step: 83549, Reward: [-114.735 -114.735] [0.0000], Avg: [-564.728 -564.728] (0.020)
Step: 83599, Reward: [-9.89 -9.89] [0.0000], Avg: [-564.396 -564.396] (0.020)
Step: 83649, Reward: [-24.391 -24.391] [0.0000], Avg: [-564.073 -564.073] (0.020)
Step: 83699, Reward: [-152.394 -152.394] [0.0000], Avg: [-563.827 -563.827] (0.020)
Step: 83749, Reward: [-93.206 -93.206] [0.0000], Avg: [-563.546 -563.546] (0.020)
Step: 83799, Reward: [-89.1 -89.1] [0.0000], Avg: [-563.263 -563.263] (0.020)
Step: 83849, Reward: [-85.79 -85.79] [0.0000], Avg: [-562.979 -562.979] (0.020)
Step: 83899, Reward: [-294.207 -294.207] [0.0000], Avg: [-562.819 -562.819] (0.020)
Step: 83949, Reward: [-342.02 -342.02] [0.0000], Avg: [-562.687 -562.687] (0.020)
Step: 83999, Reward: [-130.382 -130.382] [0.0000], Avg: [-562.43 -562.43] (0.020)
Step: 84049, Reward: [-10.562 -10.562] [0.0000], Avg: [-562.101 -562.101] (0.020)
Step: 84099, Reward: [-9.549 -9.549] [0.0000], Avg: [-561.773 -561.773] (0.020)
Step: 84149, Reward: [-178.463 -178.463] [0.0000], Avg: [-561.545 -561.545] (0.020)
Step: 84199, Reward: [-168.163 -168.163] [0.0000], Avg: [-561.312 -561.312] (0.020)
Step: 84249, Reward: [-44.362 -44.362] [0.0000], Avg: [-561.005 -561.005] (0.020)
Step: 84299, Reward: [-219.609 -219.609] [0.0000], Avg: [-560.802 -560.802] (0.020)
Step: 84349, Reward: [-303.615 -303.615] [0.0000], Avg: [-560.65 -560.65] (0.020)
Step: 84399, Reward: [-156.961 -156.961] [0.0000], Avg: [-560.411 -560.411] (0.020)
Step: 84449, Reward: [-19.825 -19.825] [0.0000], Avg: [-560.091 -560.091] (0.020)
Step: 84499, Reward: [-107.704 -107.704] [0.0000], Avg: [-559.823 -559.823] (0.020)
Step: 84549, Reward: [-34.661 -34.661] [0.0000], Avg: [-559.512 -559.512] (0.020)
Step: 84599, Reward: [-275.428 -275.428] [0.0000], Avg: [-559.344 -559.344] (0.020)
Step: 84649, Reward: [-306.983 -306.983] [0.0000], Avg: [-559.195 -559.195] (0.020)
Step: 84699, Reward: [-171.628 -171.628] [0.0000], Avg: [-558.967 -558.967] (0.020)
Step: 84749, Reward: [-222.422 -222.422] [0.0000], Avg: [-558.768 -558.768] (0.020)
Step: 84799, Reward: [-140.678 -140.678] [0.0000], Avg: [-558.522 -558.522] (0.020)
Step: 84849, Reward: [-39.377 -39.377] [0.0000], Avg: [-558.216 -558.216] (0.020)
Step: 84899, Reward: [-51.522 -51.522] [0.0000], Avg: [-557.917 -557.917] (0.020)
Step: 84949, Reward: [-35.231 -35.231] [0.0000], Avg: [-557.61 -557.61] (0.020)
Step: 84999, Reward: [-45.045 -45.045] [0.0000], Avg: [-557.308 -557.308] (0.020)
Step: 85049, Reward: [-167.204 -167.204] [0.0000], Avg: [-557.079 -557.079] (0.020)
Step: 85099, Reward: [-41.988 -41.988] [0.0000], Avg: [-556.776 -556.776] (0.020)
Step: 85149, Reward: [-8.719 -8.719] [0.0000], Avg: [-556.454 -556.454] (0.020)
Step: 85199, Reward: [-80.418 -80.418] [0.0000], Avg: [-556.175 -556.175] (0.020)
Step: 85249, Reward: [-58.915 -58.915] [0.0000], Avg: [-555.883 -555.883] (0.020)
Step: 85299, Reward: [-212.338 -212.338] [0.0000], Avg: [-555.682 -555.682] (0.020)
Step: 85349, Reward: [-5.98 -5.98] [0.0000], Avg: [-555.36 -555.36] (0.020)
Step: 85399, Reward: [-73.159 -73.159] [0.0000], Avg: [-555.078 -555.078] (0.020)
Step: 85449, Reward: [-22.694 -22.694] [0.0000], Avg: [-554.766 -554.766] (0.020)
Step: 85499, Reward: [-468.68 -468.68] [0.0000], Avg: [-554.716 -554.716] (0.020)
Step: 85549, Reward: [-167.765 -167.765] [0.0000], Avg: [-554.49 -554.49] (0.020)
Step: 85599, Reward: [-283.835 -283.835] [0.0000], Avg: [-554.331 -554.331] (0.020)
Step: 85649, Reward: [-179.861 -179.861] [0.0000], Avg: [-554.113 -554.113] (0.020)
Step: 85699, Reward: [-47.693 -47.693] [0.0000], Avg: [-553.817 -553.817] (0.020)
Step: 85749, Reward: [-215.634 -215.634] [0.0000], Avg: [-553.62 -553.62] (0.020)
Step: 85799, Reward: [-290.268 -290.268] [0.0000], Avg: [-553.467 -553.467] (0.020)
Step: 85849, Reward: [-4.755 -4.755] [0.0000], Avg: [-553.147 -553.147] (0.020)
Step: 85899, Reward: [-70.103 -70.103] [0.0000], Avg: [-552.866 -552.866] (0.020)
Step: 85949, Reward: [-6.577 -6.577] [0.0000], Avg: [-552.548 -552.548] (0.020)
Step: 85999, Reward: [-323.223 -323.223] [0.0000], Avg: [-552.415 -552.415] (0.020)
Step: 86049, Reward: [-134.722 -134.722] [0.0000], Avg: [-552.172 -552.172] (0.020)
Step: 86099, Reward: [-87.392 -87.392] [0.0000], Avg: [-551.902 -551.902] (0.020)
Step: 86149, Reward: [-217.472 -217.472] [0.0000], Avg: [-551.708 -551.708] (0.020)
Step: 86199, Reward: [-15.447 -15.447] [0.0000], Avg: [-551.397 -551.397] (0.020)
Step: 86249, Reward: [-300.855 -300.855] [0.0000], Avg: [-551.252 -551.252] (0.020)
Step: 86299, Reward: [-146.306 -146.306] [0.0000], Avg: [-551.017 -551.017] (0.020)
Step: 86349, Reward: [-145.39 -145.39] [0.0000], Avg: [-550.782 -550.782] (0.020)
Step: 86399, Reward: [-20.627 -20.627] [0.0000], Avg: [-550.476 -550.476] (0.020)
Step: 86449, Reward: [-11.086 -11.086] [0.0000], Avg: [-550.164 -550.164] (0.020)
Step: 86499, Reward: [-11.713 -11.713] [0.0000], Avg: [-549.852 -549.852] (0.020)
Step: 86549, Reward: [-121.116 -121.116] [0.0000], Avg: [-549.605 -549.605] (0.020)
Step: 86599, Reward: [-318.328 -318.328] [0.0000], Avg: [-549.471 -549.471] (0.020)
Step: 86649, Reward: [-176.831 -176.831] [0.0000], Avg: [-549.256 -549.256] (0.020)
Step: 86699, Reward: [-305.577 -305.577] [0.0000], Avg: [-549.116 -549.116] (0.020)
Step: 86749, Reward: [-341.767 -341.767] [0.0000], Avg: [-548.996 -548.996] (0.020)
Step: 86799, Reward: [-8.728 -8.728] [0.0000], Avg: [-548.685 -548.685] (0.020)
Step: 86849, Reward: [-75.894 -75.894] [0.0000], Avg: [-548.413 -548.413] (0.020)
Step: 86899, Reward: [-73.547 -73.547] [0.0000], Avg: [-548.139 -548.139] (0.020)
Step: 86949, Reward: [-233.049 -233.049] [0.0000], Avg: [-547.958 -547.958] (0.020)
Step: 86999, Reward: [-0.013 -0.013] [0.0000], Avg: [-547.643 -547.643] (0.020)
Step: 87049, Reward: [-60.412 -60.412] [0.0000], Avg: [-547.363 -547.363] (0.020)
Step: 87099, Reward: [-94.468 -94.468] [0.0000], Avg: [-547.103 -547.103] (0.020)
Step: 87149, Reward: [-185.218 -185.218] [0.0000], Avg: [-546.896 -546.896] (0.020)
Step: 87199, Reward: [-79.142 -79.142] [0.0000], Avg: [-546.628 -546.628] (0.020)
Step: 87249, Reward: [-126.262 -126.262] [0.0000], Avg: [-546.387 -546.387] (0.020)
Step: 87299, Reward: [-101.126 -101.126] [0.0000], Avg: [-546.132 -546.132] (0.020)
Step: 87349, Reward: [-197.901 -197.901] [0.0000], Avg: [-545.932 -545.932] (0.020)
Step: 87399, Reward: [-388.042 -388.042] [0.0000], Avg: [-545.842 -545.842] (0.020)
Step: 87449, Reward: [-298.657 -298.657] [0.0000], Avg: [-545.701 -545.701] (0.020)
Step: 87499, Reward: [-292.665 -292.665] [0.0000], Avg: [-545.556 -545.556] (0.020)
Step: 87549, Reward: [-224.302 -224.302] [0.0000], Avg: [-545.373 -545.373] (0.020)
Step: 87599, Reward: [-3.438 -3.438] [0.0000], Avg: [-545.063 -545.063] (0.020)
Step: 87649, Reward: [-181.483 -181.483] [0.0000], Avg: [-544.856 -544.856] (0.020)
Step: 87699, Reward: [-289.458 -289.458] [0.0000], Avg: [-544.71 -544.71] (0.020)
Step: 87749, Reward: [-273.326 -273.326] [0.0000], Avg: [-544.556 -544.556] (0.020)
Step: 87799, Reward: [-154.972 -154.972] [0.0000], Avg: [-544.334 -544.334] (0.020)
Step: 87849, Reward: [-117.158 -117.158] [0.0000], Avg: [-544.091 -544.091] (0.020)
Step: 87899, Reward: [-406.412 -406.412] [0.0000], Avg: [-544.012 -544.012] (0.020)
Step: 87949, Reward: [-8.075 -8.075] [0.0000], Avg: [-543.708 -543.708] (0.020)
Step: 87999, Reward: [-32.833 -32.833] [0.0000], Avg: [-543.417 -543.417] (0.020)
Step: 88049, Reward: [-1.929 -1.929] [0.0000], Avg: [-543.11 -543.11] (0.020)
Step: 88099, Reward: [-127.972 -127.972] [0.0000], Avg: [-542.874 -542.874] (0.020)
Step: 88149, Reward: [-228.834 -228.834] [0.0000], Avg: [-542.696 -542.696] (0.020)
Step: 88199, Reward: [-15.56 -15.56] [0.0000], Avg: [-542.397 -542.397] (0.020)
Step: 88249, Reward: [-95.1 -95.1] [0.0000], Avg: [-542.144 -542.144] (0.020)
Step: 88299, Reward: [-31.83 -31.83] [0.0000], Avg: [-541.855 -541.855] (0.020)
Step: 88349, Reward: [-33.345 -33.345] [0.0000], Avg: [-541.567 -541.567] (0.020)
Step: 88399, Reward: [-117.588 -117.588] [0.0000], Avg: [-541.327 -541.327] (0.020)
Step: 88449, Reward: [-215.289 -215.289] [0.0000], Avg: [-541.143 -541.143] (0.020)
Step: 88499, Reward: [-57.269 -57.269] [0.0000], Avg: [-540.87 -540.87] (0.020)
Step: 88549, Reward: [-211.418 -211.418] [0.0000], Avg: [-540.684 -540.684] (0.020)
Step: 88599, Reward: [-87.927 -87.927] [0.0000], Avg: [-540.428 -540.428] (0.020)
Step: 88649, Reward: [-399.766 -399.766] [0.0000], Avg: [-540.349 -540.349] (0.020)
Step: 88699, Reward: [-130.926 -130.926] [0.0000], Avg: [-540.118 -540.118] (0.020)
Step: 88749, Reward: [-107.616 -107.616] [0.0000], Avg: [-539.874 -539.874] (0.020)
Step: 88799, Reward: [-342.579 -342.579] [0.0000], Avg: [-539.763 -539.763] (0.020)
Step: 88849, Reward: [-8.658 -8.658] [0.0000], Avg: [-539.464 -539.464] (0.020)
Step: 88899, Reward: [-25.515 -25.515] [0.0000], Avg: [-539.175 -539.175] (0.020)
Step: 88949, Reward: [-26.491 -26.491] [0.0000], Avg: [-538.887 -538.887] (0.020)
Step: 88999, Reward: [-1.68 -1.68] [0.0000], Avg: [-538.585 -538.585] (0.020)
Step: 89049, Reward: [-213.118 -213.118] [0.0000], Avg: [-538.403 -538.403] (0.020)
Step: 89099, Reward: [-29.228 -29.228] [0.0000], Avg: [-538.117 -538.117] (0.020)
Step: 89149, Reward: [-67.328 -67.328] [0.0000], Avg: [-537.853 -537.853] (0.020)
Step: 89199, Reward: [-322.995 -322.995] [0.0000], Avg: [-537.732 -537.732] (0.020)
Step: 89249, Reward: [-272.123 -272.123] [0.0000], Avg: [-537.584 -537.584] (0.020)
Step: 89299, Reward: [-81.886 -81.886] [0.0000], Avg: [-537.328 -537.328] (0.020)
Step: 89349, Reward: [-42.164 -42.164] [0.0000], Avg: [-537.051 -537.051] (0.020)
Step: 89399, Reward: [-153.542 -153.542] [0.0000], Avg: [-536.837 -536.837] (0.020)
Step: 89449, Reward: [-84.052 -84.052] [0.0000], Avg: [-536.584 -536.584] (0.020)
Step: 89499, Reward: [-160.581 -160.581] [0.0000], Avg: [-536.374 -536.374] (0.020)
Step: 89549, Reward: [-119.531 -119.531] [0.0000], Avg: [-536.141 -536.141] (0.020)
Step: 89599, Reward: [-115.775 -115.775] [0.0000], Avg: [-535.906 -535.906] (0.020)
Step: 89649, Reward: [-85.911 -85.911] [0.0000], Avg: [-535.655 -535.655] (0.020)
Step: 89699, Reward: [-132.54 -132.54] [0.0000], Avg: [-535.431 -535.431] (0.020)
Step: 89749, Reward: [-233.465 -233.465] [0.0000], Avg: [-535.263 -535.263] (0.020)
Step: 89799, Reward: [-226.236 -226.236] [0.0000], Avg: [-535.09 -535.09] (0.020)
Step: 89849, Reward: [-100.215 -100.215] [0.0000], Avg: [-534.848 -534.848] (0.020)
Step: 89899, Reward: [-128.949 -128.949] [0.0000], Avg: [-534.623 -534.623] (0.020)
Step: 89949, Reward: [-143.644 -143.644] [0.0000], Avg: [-534.405 -534.405] (0.020)
Step: 89999, Reward: [-40.716 -40.716] [0.0000], Avg: [-534.131 -534.131] (0.020)
Step: 90049, Reward: [-51.512 -51.512] [0.0000], Avg: [-533.863 -533.863] (0.020)
Step: 90099, Reward: [-32.503 -32.503] [0.0000], Avg: [-533.585 -533.585] (0.020)
Step: 90149, Reward: [-236.563 -236.563] [0.0000], Avg: [-533.42 -533.42] (0.020)
Step: 90199, Reward: [-38.99 -38.99] [0.0000], Avg: [-533.146 -533.146] (0.020)
Step: 90249, Reward: [-118.996 -118.996] [0.0000], Avg: [-532.917 -532.917] (0.020)
Step: 90299, Reward: [-4.32 -4.32] [0.0000], Avg: [-532.624 -532.624] (0.020)
Step: 90349, Reward: [-137.633 -137.633] [0.0000], Avg: [-532.405 -532.405] (0.020)
Step: 90399, Reward: [-144.835 -144.835] [0.0000], Avg: [-532.191 -532.191] (0.020)
Step: 90449, Reward: [-185.776 -185.776] [0.0000], Avg: [-532. -532.] (0.020)
Step: 90499, Reward: [-97.395 -97.395] [0.0000], Avg: [-531.759 -531.759] (0.020)
Step: 90549, Reward: [-378.074 -378.074] [0.0000], Avg: [-531.675 -531.675] (0.020)
Step: 90599, Reward: [-301.327 -301.327] [0.0000], Avg: [-531.547 -531.547] (0.020)
Step: 90649, Reward: [-180.014 -180.014] [0.0000], Avg: [-531.354 -531.354] (0.020)
Step: 90699, Reward: [-188.617 -188.617] [0.0000], Avg: [-531.165 -531.165] (0.020)
Step: 90749, Reward: [-145.847 -145.847] [0.0000], Avg: [-530.952 -530.952] (0.020)
Step: 90799, Reward: [-230.658 -230.658] [0.0000], Avg: [-530.787 -530.787] (0.020)
Step: 90849, Reward: [-473.327 -473.327] [0.0000], Avg: [-530.755 -530.755] (0.020)
Step: 90899, Reward: [-0.613 -0.613] [0.0000], Avg: [-530.464 -530.464] (0.020)
Step: 90949, Reward: [-19.034 -19.034] [0.0000], Avg: [-530.183 -530.183] (0.020)
Step: 90999, Reward: [-54.233 -54.233] [0.0000], Avg: [-529.921 -529.921] (0.020)
Step: 91049, Reward: [-134.442 -134.442] [0.0000], Avg: [-529.704 -529.704] (0.020)
Step: 91099, Reward: [-1.815 -1.815] [0.0000], Avg: [-529.414 -529.414] (0.020)
Step: 91149, Reward: [-3.924 -3.924] [0.0000], Avg: [-529.126 -529.126] (0.020)
Step: 91199, Reward: [-10.148 -10.148] [0.0000], Avg: [-528.841 -528.841] (0.020)
Step: 91249, Reward: [-123.431 -123.431] [0.0000], Avg: [-528.619 -528.619] (0.020)
Step: 91299, Reward: [-285.838 -285.838] [0.0000], Avg: [-528.486 -528.486] (0.020)
Step: 91349, Reward: [-34.129 -34.129] [0.0000], Avg: [-528.216 -528.216] (0.020)
Step: 91399, Reward: [-88.689 -88.689] [0.0000], Avg: [-527.975 -527.975] (0.020)
Step: 91449, Reward: [-152.529 -152.529] [0.0000], Avg: [-527.77 -527.77] (0.020)
Step: 91499, Reward: [-75.563 -75.563] [0.0000], Avg: [-527.523 -527.523] (0.020)
Step: 91549, Reward: [-125.446 -125.446] [0.0000], Avg: [-527.303 -527.303] (0.020)
Step: 91599, Reward: [-187.768 -187.768] [0.0000], Avg: [-527.118 -527.118] (0.020)
Step: 91649, Reward: [-376.979 -376.979] [0.0000], Avg: [-527.036 -527.036] (0.020)
Step: 91699, Reward: [-19.202 -19.202] [0.0000], Avg: [-526.759 -526.759] (0.020)
Step: 91749, Reward: [-85.823 -85.823] [0.0000], Avg: [-526.519 -526.519] (0.020)
Step: 91799, Reward: [-201.305 -201.305] [0.0000], Avg: [-526.342 -526.342] (0.020)
Step: 91849, Reward: [-228.493 -228.493] [0.0000], Avg: [-526.18 -526.18] (0.020)
Step: 91899, Reward: [-290.202 -290.202] [0.0000], Avg: [-526.051 -526.051] (0.020)
Step: 91949, Reward: [-11.44 -11.44] [0.0000], Avg: [-525.771 -525.771] (0.020)
Step: 91999, Reward: [-225.909 -225.909] [0.0000], Avg: [-525.608 -525.608] (0.020)
Step: 92049, Reward: [-72.22 -72.22] [0.0000], Avg: [-525.362 -525.362] (0.020)
Step: 92099, Reward: [-196.217 -196.217] [0.0000], Avg: [-525.183 -525.183] (0.020)
Step: 92149, Reward: [-340.112 -340.112] [0.0000], Avg: [-525.083 -525.083] (0.020)
Step: 92199, Reward: [-219.081 -219.081] [0.0000], Avg: [-524.917 -524.917] (0.020)
Step: 92249, Reward: [-27.153 -27.153] [0.0000], Avg: [-524.647 -524.647] (0.020)
Step: 92299, Reward: [-371.67 -371.67] [0.0000], Avg: [-524.564 -524.564] (0.020)
Step: 92349, Reward: [-1.518 -1.518] [0.0000], Avg: [-524.281 -524.281] (0.020)
Step: 92399, Reward: [-98.891 -98.891] [0.0000], Avg: [-524.051 -524.051] (0.020)
Step: 92449, Reward: [-88.665 -88.665] [0.0000], Avg: [-523.815 -523.815] (0.020)
Step: 92499, Reward: [-56.569 -56.569] [0.0000], Avg: [-523.563 -523.563] (0.020)
Step: 92549, Reward: [-248.056 -248.056] [0.0000], Avg: [-523.414 -523.414] (0.020)
Step: 92599, Reward: [-58.75 -58.75] [0.0000], Avg: [-523.163 -523.163] (0.020)
Step: 92649, Reward: [-71.873 -71.873] [0.0000], Avg: [-522.92 -522.92] (0.020)
Step: 92699, Reward: [-79.767 -79.767] [0.0000], Avg: [-522.681 -522.681] (0.020)
Step: 92749, Reward: [-273.325 -273.325] [0.0000], Avg: [-522.546 -522.546] (0.020)
Step: 92799, Reward: [-126.883 -126.883] [0.0000], Avg: [-522.333 -522.333] (0.020)
Step: 92849, Reward: [-127.448 -127.448] [0.0000], Avg: [-522.12 -522.12] (0.020)
Step: 92899, Reward: [-65.716 -65.716] [0.0000], Avg: [-521.875 -521.875] (0.020)
Step: 92949, Reward: [-108.627 -108.627] [0.0000], Avg: [-521.652 -521.652] (0.020)
Step: 92999, Reward: [-264.867 -264.867] [0.0000], Avg: [-521.514 -521.514] (0.020)
Step: 93049, Reward: [-179.087 -179.087] [0.0000], Avg: [-521.33 -521.33] (0.020)
Step: 93099, Reward: [-91.744 -91.744] [0.0000], Avg: [-521.1 -521.1] (0.020)
Step: 93149, Reward: [-50.786 -50.786] [0.0000], Avg: [-520.847 -520.847] (0.020)
Step: 93199, Reward: [-297.737 -297.737] [0.0000], Avg: [-520.728 -520.728] (0.020)
Step: 93249, Reward: [-26.823 -26.823] [0.0000], Avg: [-520.463 -520.463] (0.020)
Step: 93299, Reward: [-225.555 -225.555] [0.0000], Avg: [-520.305 -520.305] (0.020)
Step: 93349, Reward: [-117.667 -117.667] [0.0000], Avg: [-520.089 -520.089] (0.020)
Step: 93399, Reward: [-202.002 -202.002] [0.0000], Avg: [-519.919 -519.919] (0.020)
Step: 93449, Reward: [-10.284 -10.284] [0.0000], Avg: [-519.646 -519.646] (0.020)
Step: 93499, Reward: [-119.918 -119.918] [0.0000], Avg: [-519.432 -519.432] (0.020)
Step: 93549, Reward: [-436.303 -436.303] [0.0000], Avg: [-519.388 -519.388] (0.020)
Step: 93599, Reward: [-398.048 -398.048] [0.0000], Avg: [-519.323 -519.323] (0.020)
Step: 93649, Reward: [-139.056 -139.056] [0.0000], Avg: [-519.12 -519.12] (0.020)
Step: 93699, Reward: [-154.673 -154.673] [0.0000], Avg: [-518.926 -518.926] (0.020)
Step: 93749, Reward: [-168.761 -168.761] [0.0000], Avg: [-518.739 -518.739] (0.020)
Step: 93799, Reward: [-9.423 -9.423] [0.0000], Avg: [-518.467 -518.467] (0.020)
Step: 93849, Reward: [-25.185 -25.185] [0.0000], Avg: [-518.204 -518.204] (0.020)
Step: 93899, Reward: [-451.267 -451.267] [0.0000], Avg: [-518.169 -518.169] (0.020)
Step: 93949, Reward: [-180.378 -180.378] [0.0000], Avg: [-517.989 -517.989] (0.020)
Step: 93999, Reward: [-82.83 -82.83] [0.0000], Avg: [-517.758 -517.758] (0.020)
Step: 94049, Reward: [-35.122 -35.122] [0.0000], Avg: [-517.501 -517.501] (0.020)
Step: 94099, Reward: [-33.163 -33.163] [0.0000], Avg: [-517.244 -517.244] (0.020)
Step: 94149, Reward: [-146.133 -146.133] [0.0000], Avg: [-517.047 -517.047] (0.020)
Step: 94199, Reward: [-17.28 -17.28] [0.0000], Avg: [-516.781 -516.781] (0.020)
Step: 94249, Reward: [-200.778 -200.778] [0.0000], Avg: [-516.614 -516.614] (0.020)
Step: 94299, Reward: [-206.952 -206.952] [0.0000], Avg: [-516.449 -516.449] (0.020)
Step: 94349, Reward: [-259.34 -259.34] [0.0000], Avg: [-516.313 -516.313] (0.020)
Step: 94399, Reward: [-172.829 -172.829] [0.0000], Avg: [-516.131 -516.131] (0.020)
Step: 94449, Reward: [-7.782 -7.782] [0.0000], Avg: [-515.862 -515.862] (0.020)
Step: 94499, Reward: [-2.025 -2.025] [0.0000], Avg: [-515.59 -515.59] (0.020)
Step: 94549, Reward: [-68.435 -68.435] [0.0000], Avg: [-515.354 -515.354] (0.020)
Step: 94599, Reward: [-58.861 -58.861] [0.0000], Avg: [-515.113 -515.113] (0.020)
Step: 94649, Reward: [-144.757 -144.757] [0.0000], Avg: [-514.917 -514.917] (0.020)
Step: 94699, Reward: [-384.418 -384.418] [0.0000], Avg: [-514.848 -514.848] (0.020)
Step: 94749, Reward: [-25.119 -25.119] [0.0000], Avg: [-514.59 -514.59] (0.020)
Step: 94799, Reward: [-41.539 -41.539] [0.0000], Avg: [-514.34 -514.34] (0.020)
Step: 94849, Reward: [-90.561 -90.561] [0.0000], Avg: [-514.117 -514.117] (0.020)
Step: 94899, Reward: [-153.673 -153.673] [0.0000], Avg: [-513.927 -513.927] (0.020)
Step: 94949, Reward: [-76.176 -76.176] [0.0000], Avg: [-513.696 -513.696] (0.020)
Step: 94999, Reward: [-87.296 -87.296] [0.0000], Avg: [-513.472 -513.472] (0.020)
Step: 95049, Reward: [-389.607 -389.607] [0.0000], Avg: [-513.407 -513.407] (0.020)
Step: 95099, Reward: [-18.832 -18.832] [0.0000], Avg: [-513.147 -513.147] (0.020)
Step: 95149, Reward: [-142.189 -142.189] [0.0000], Avg: [-512.952 -512.952] (0.020)
Step: 95199, Reward: [-237.483 -237.483] [0.0000], Avg: [-512.807 -512.807] (0.020)
Step: 95249, Reward: [-158.907 -158.907] [0.0000], Avg: [-512.621 -512.621] (0.020)
Step: 95299, Reward: [-62.534 -62.534] [0.0000], Avg: [-512.385 -512.385] (0.020)
Step: 95349, Reward: [-48.874 -48.874] [0.0000], Avg: [-512.142 -512.142] (0.020)
Step: 95399, Reward: [-57.72 -57.72] [0.0000], Avg: [-511.904 -511.904] (0.020)
Step: 95449, Reward: [-3.962 -3.962] [0.0000], Avg: [-511.638 -511.638] (0.020)
Step: 95499, Reward: [-36.099 -36.099] [0.0000], Avg: [-511.389 -511.389] (0.020)
Step: 95549, Reward: [-112.448 -112.448] [0.0000], Avg: [-511.18 -511.18] (0.020)
Step: 95599, Reward: [-35.203 -35.203] [0.0000], Avg: [-510.931 -510.931] (0.020)
Step: 95649, Reward: [-103.189 -103.189] [0.0000], Avg: [-510.718 -510.718] (0.020)
Step: 95699, Reward: [-0.497 -0.497] [0.0000], Avg: [-510.451 -510.451] (0.020)
Step: 95749, Reward: [-156.941 -156.941] [0.0000], Avg: [-510.267 -510.267] (0.020)
Step: 95799, Reward: [-38.817 -38.817] [0.0000], Avg: [-510.021 -510.021] (0.020)
Step: 95849, Reward: [-105.05 -105.05] [0.0000], Avg: [-509.81 -509.81] (0.020)
Step: 95899, Reward: [-226.787 -226.787] [0.0000], Avg: [-509.662 -509.662] (0.020)
Step: 95949, Reward: [-42.891 -42.891] [0.0000], Avg: [-509.419 -509.419] (0.020)
Step: 95999, Reward: [-35.289 -35.289] [0.0000], Avg: [-509.172 -509.172] (0.020)
Step: 96049, Reward: [-47.523 -47.523] [0.0000], Avg: [-508.931 -508.931] (0.020)
Step: 96099, Reward: [-3.415 -3.415] [0.0000], Avg: [-508.668 -508.668] (0.020)
Step: 96149, Reward: [-135.052 -135.052] [0.0000], Avg: [-508.474 -508.474] (0.020)
Step: 96199, Reward: [-212.838 -212.838] [0.0000], Avg: [-508.321 -508.321] (0.020)
Step: 96249, Reward: [-37.199 -37.199] [0.0000], Avg: [-508.076 -508.076] (0.020)
Step: 96299, Reward: [-59.381 -59.381] [0.0000], Avg: [-507.843 -507.843] (0.020)
Step: 96349, Reward: [-14.2 -14.2] [0.0000], Avg: [-507.587 -507.587] (0.020)
Step: 96399, Reward: [-244.293 -244.293] [0.0000], Avg: [-507.45 -507.45] (0.020)
Step: 96449, Reward: [-194.545 -194.545] [0.0000], Avg: [-507.288 -507.288] (0.020)
Step: 96499, Reward: [-121.275 -121.275] [0.0000], Avg: [-507.088 -507.088] (0.020)
Step: 96549, Reward: [-134.687 -134.687] [0.0000], Avg: [-506.895 -506.895] (0.020)
Step: 96599, Reward: [-174.263 -174.263] [0.0000], Avg: [-506.723 -506.723] (0.020)
Step: 96649, Reward: [-238.252 -238.252] [0.0000], Avg: [-506.584 -506.584] (0.020)
Step: 96699, Reward: [-340.34 -340.34] [0.0000], Avg: [-506.498 -506.498] (0.020)
Step: 96749, Reward: [-241.77 -241.77] [0.0000], Avg: [-506.361 -506.361] (0.020)
Step: 96799, Reward: [-251.284 -251.284] [0.0000], Avg: [-506.229 -506.229] (0.020)
Step: 96849, Reward: [-121.468 -121.468] [0.0000], Avg: [-506.031 -506.031] (0.020)
Step: 96899, Reward: [-81.976 -81.976] [0.0000], Avg: [-505.812 -505.812] (0.020)
Step: 96949, Reward: [-293.257 -293.257] [0.0000], Avg: [-505.702 -505.702] (0.020)
Step: 96999, Reward: [-147.446 -147.446] [0.0000], Avg: [-505.518 -505.518] (0.020)
Step: 97049, Reward: [-301.654 -301.654] [0.0000], Avg: [-505.413 -505.413] (0.020)
Step: 97099, Reward: [-157.771 -157.771] [0.0000], Avg: [-505.234 -505.234] (0.020)
Step: 97149, Reward: [-73.461 -73.461] [0.0000], Avg: [-505.011 -505.011] (0.020)
Step: 97199, Reward: [-152.724 -152.724] [0.0000], Avg: [-504.83 -504.83] (0.020)
Step: 97249, Reward: [-80.596 -80.596] [0.0000], Avg: [-504.612 -504.612] (0.020)
Step: 97299, Reward: [-201.927 -201.927] [0.0000], Avg: [-504.457 -504.457] (0.020)
Step: 97349, Reward: [-133.475 -133.475] [0.0000], Avg: [-504.266 -504.266] (0.020)
Step: 97399, Reward: [-339.757 -339.757] [0.0000], Avg: [-504.182 -504.182] (0.020)
Step: 97449, Reward: [-110.825 -110.825] [0.0000], Avg: [-503.98 -503.98] (0.020)
Step: 97499, Reward: [-341.535 -341.535] [0.0000], Avg: [-503.896 -503.896] (0.020)
Step: 97549, Reward: [-23.221 -23.221] [0.0000], Avg: [-503.65 -503.65] (0.020)
Step: 97599, Reward: [-7.171 -7.171] [0.0000], Avg: [-503.396 -503.396] (0.020)
Step: 97649, Reward: [-108.219 -108.219] [0.0000], Avg: [-503.193 -503.193] (0.020)
Step: 97699, Reward: [-61.988 -61.988] [0.0000], Avg: [-502.968 -502.968] (0.020)
Step: 97749, Reward: [-228.2 -228.2] [0.0000], Avg: [-502.827 -502.827] (0.020)
Step: 97799, Reward: [-173.646 -173.646] [0.0000], Avg: [-502.659 -502.659] (0.020)
Step: 97849, Reward: [-100.899 -100.899] [0.0000], Avg: [-502.453 -502.453] (0.020)
Step: 97899, Reward: [-71.816 -71.816] [0.0000], Avg: [-502.233 -502.233] (0.020)
Step: 97949, Reward: [-459.433 -459.433] [0.0000], Avg: [-502.212 -502.212] (0.020)
Step: 97999, Reward: [-318.617 -318.617] [0.0000], Avg: [-502.118 -502.118] (0.020)
Step: 98049, Reward: [-15.012 -15.012] [0.0000], Avg: [-501.87 -501.87] (0.020)
Step: 98099, Reward: [-113.382 -113.382] [0.0000], Avg: [-501.672 -501.672] (0.020)
Step: 98149, Reward: [-141.212 -141.212] [0.0000], Avg: [-501.488 -501.488] (0.020)
Step: 98199, Reward: [-170.875 -170.875] [0.0000], Avg: [-501.32 -501.32] (0.020)
Step: 98249, Reward: [-32.617 -32.617] [0.0000], Avg: [-501.081 -501.081] (0.020)
Step: 98299, Reward: [-87.45 -87.45] [0.0000], Avg: [-500.871 -500.871] (0.020)
Step: 98349, Reward: [-166.957 -166.957] [0.0000], Avg: [-500.701 -500.701] (0.020)
Step: 98399, Reward: [-20.462 -20.462] [0.0000], Avg: [-500.457 -500.457] (0.020)
