Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 1, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = "cpu" if not gpu else "gpu"
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		eps = self.eps if eps is None else eps
		pre_transition_data = {"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}
		self.episode_batch.update(pre_transition_data, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=len(state[0].shape)==len(self.state_size[0]))
		return list(zip(*one_hot_from_indices(actions, self.action_size[0][-1]).data.numpy()))

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			last_data = {"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}
			self.episode_batch.update(last_data, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
    def __init__(self, out_dim):
        self.out_dim = out_dim

    def transform(self, tensor):
        y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
        y_onehot.scatter_(-1, tensor.long(), 1)
        return y_onehot.float()

    def infer_output_info(self, vshape_in, dtype_in):
        return (self.out_dim,), torch.float32

class COMALearner:
    def __init__(self, mac, scheme, n_agents, n_actions):
        self.n_agents = n_agents
        self.n_actions = n_actions
        self.last_target_update_step = 0
        self.mac = mac
        self.critic_training_steps = 0
        self.critic = COMACritic(scheme, self.n_agents, self.n_actions)
        self.critic_params = list(self.critic.parameters())
        self.agent_params = list(mac.parameters())
        self.params = self.agent_params + self.critic_params
        self.target_critic = copy.deepcopy(self.critic)
        self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
        self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

    def train(self, batch):
        # Get the relevant quantities
        bs = batch.batch_size
        max_t = batch.max_seq_length
        rewards = batch["reward"][:, :-1]
        actions = batch["actions"][:, :]
        done = batch["done"][:, :-1].float()
        mask = batch["filled"][:, :-1].float()
        mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
        critic_mask = mask.clone()
        mask = mask.repeat(1, 1, self.n_agents).view(-1)
        q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
        actions = actions[:,:-1]
        mac_out = []
        self.mac.init_hidden(batch.batch_size)
        for t in range(batch.max_seq_length - 1):
            agent_outs = self.mac.forward(batch, t=t)
            mac_out.append(agent_outs)
        mac_out = torch.stack(mac_out, dim=1)  # Concat over time
        # Mask out unavailable actions, renormalise (as in action selection)
        q_vals = q_vals.reshape(-1, self.n_actions)
        pi = mac_out.view(-1, self.n_actions)
        baseline = (pi * q_vals).sum(-1).detach()
        q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
        pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
        pi_taken[mask == 0] = 1.0
        log_pi_taken = torch.log(pi_taken)
        advantages = (q_taken - baseline).detach()
        coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
        self.agent_optimiser.zero_grad()
        coma_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
        self.agent_optimiser.step()
        if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
            self._update_targets()
            self.last_target_update_step = self.critic_training_steps

    def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
        target_q_vals = self.target_critic(batch)[:, :]
        targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
        targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
        q_vals = torch.zeros_like(target_q_vals)[:, :-1]
        for t in reversed(range(rewards.size(1))):
            mask_t = mask[:, t].expand(-1, self.n_agents)
            if mask_t.sum() == 0:
                continue
            q_t = self.critic(batch, t)
            q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
            q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
            targets_t = targets[:, t]
            td_error = (q_taken - targets_t.detach())
            # 0-out the targets that came from padded data
            masked_td_error = td_error * mask_t
            loss = (masked_td_error ** 2).sum() / mask_t.sum()
            self.critic_optimiser.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
            self.critic_optimiser.step()
            self.critic_training_steps += 1
        return q_vals

    def _update_targets(self):
        self.target_critic.load_state_dict(self.critic.state_dict())

    def cuda(self):
        self.mac.cuda()
        self.critic.cuda()
        self.target_critic.cuda()

    def save_models(self, path):
        self.mac.save_models(path)
        torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
        torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
        torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

    def load_models(self, path):
        self.mac.load_models(path)
        self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
        self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
    # Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
    # Initialise  last  lambda -return  for  not  done  episodes
    ret = target_qs.new_zeros(*target_qs.shape)
    ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
    # Backwards  recursive  update  of the "forward  view"
    for t in range(ret.shape[1] - 2, -1,  -1):
        ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
    # Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
    return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
    def __init__(self, scheme, n_agents, n_actions):
        super(COMACritic, self).__init__()
        self.n_actions = n_actions
        self.n_agents = n_agents
        input_shape = self._get_input_shape(scheme)
        self.output_type = "q"
        self.fc1 = torch.nn.Linear(input_shape, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc3 = torch.nn.Linear(128, self.n_actions)

    def forward(self, batch, t=None):
        inputs = self._build_inputs(batch, t=t)
        x = torch.relu(self.fc1(inputs))
        x = torch.relu(self.fc2(x))
        q = self.fc3(x)
        return q

    def _build_inputs(self, batch, t=None):
        bs = batch.batch_size
        max_t = batch.max_seq_length if t is None else 1
        ts = slice(None) if t is None else slice(t, t+1)
        inputs = []
        inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
        inputs.append(batch["obs"][:, ts])
        actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
        agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
        agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
        inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
        # last actions
        if t == 0:
            inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
        elif isinstance(t, int):
            inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
        else:
            last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
            last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
            inputs.append(last_actions)

        inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
        inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
        return inputs

    def _get_input_shape(self, scheme):
        input_shape = scheme["state"]["vshape"]
        input_shape += scheme["obs"]["vshape"]
        input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
        input_shape += self.n_agents
        return input_shape

class BasicMAC:
    def __init__(self, scheme, groups, n_agents, n_actions):
        self.n_agents = n_agents
        self.n_actions = n_actions
        self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions)
        self.action_selector = MultinomialActionSelector()
        self.hidden_states = None

    def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
        agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
        chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
        return chosen_actions

    def forward(self, ep_batch, t, test_mode=False):
        agent_inputs = self._build_inputs(ep_batch, t)
        agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
        agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
        if not test_mode:
            epsilon_action_num = agent_outs.size(-1)
            agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs) * self.action_selector.epsilon/epsilon_action_num)
        return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

    def init_hidden(self, batch_size):
        self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

    def parameters(self):
        return self.agent.parameters()

    def load_state(self, other_mac):
        self.agent.load_state_dict(other_mac.agent.state_dict())

    def cuda(self):
        self.agent.cuda()

    def save_models(self, path):
        torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

    def load_models(self, path):
        self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

    def _build_inputs(self, batch, t):
        bs = batch.batch_size
        inputs = []
        inputs.append(batch["obs"][:, t])  # b1av
        inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
        inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
        inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
        return inputs

    def _get_input_shape(self, scheme):
        input_shape = scheme["obs"]["vshape"]
        input_shape += scheme["actions_onehot"]["vshape"][0]
        input_shape += self.n_agents
        return input_shape

class RNNAgent(torch.nn.Module):
    def __init__(self, input_shape, output_shape):
        super(RNNAgent, self).__init__()
        self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
        self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
        self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

    def init_hidden(self):
        return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

    def forward(self, inputs, hidden_state):
        x = torch.relu(self.fc1(inputs))
        h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
        h = self.rnn(x, h_in)
        q = self.fc2(h)
        return q, h

class MultinomialActionSelector():
    def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
        self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
        self.epsilon = self.schedule.eval(0)

    def select_action(self, agent_inputs, t_env, test_mode=False):
        self.epsilon = self.schedule.eval(t_env)
        masked_policies = agent_inputs.clone()
        picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
        return picked_actions

class DecayThenFlatSchedule():
    def __init__(self, start, finish, time_length, decay="exp"):
        self.start = start
        self.finish = finish
        self.time_length = time_length
        self.delta = (self.start - self.finish) / self.time_length
        self.decay = decay
        if self.decay in ["exp"]:
            self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

    def eval(self, T):
        if self.decay in ["linear"]:
            return max(self.finish, self.start - self.delta * T)
        elif self.decay in ["exp"]:
            return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
    def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
        self.scheme = scheme.copy()
        self.groups = groups
        self.batch_size = batch_size
        self.max_seq_length = max_seq_length
        self.preprocess = {} if preprocess is None else preprocess
        self.device = device

        if data is not None:
            self.data = data
        else:
            self.data = SN()
            self.data.transition_data = {}
            self.data.episode_data = {}
            self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

    def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
        if preprocess is not None:
            for k in preprocess:
                assert k in scheme
                new_k = preprocess[k][0]
                transforms = preprocess[k][1]
                vshape = self.scheme[k]["vshape"]
                dtype = self.scheme[k]["dtype"]
                for transform in transforms:
                    vshape, dtype = transform.infer_output_info(vshape, dtype)
                self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
                if "group" in self.scheme[k]:
                    self.scheme[new_k]["group"] = self.scheme[k]["group"]
                if "episode_const" in self.scheme[k]:
                    self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

        assert "filled" not in scheme, '"filled" is a reserved key for masking.'
        scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

        for field_key, field_info in scheme.items():
            assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
            vshape = field_info["vshape"]
            episode_const = field_info.get("episode_const", False)
            group = field_info.get("group", None)
            dtype = field_info.get("dtype", torch.float32)

            if isinstance(vshape, int):
                vshape = (vshape,)
            if group:
                assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
                shape = (groups[group], *vshape)
            else:
                shape = vshape
            if episode_const:
                self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype, device=self.device)
            else:
                self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype, device=self.device)

    def extend(self, scheme, groups=None):
        self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

    def to(self, device):
        for k, v in self.data.transition_data.items():
            self.data.transition_data[k] = v.to(device)
        for k, v in self.data.episode_data.items():
            self.data.episode_data[k] = v.to(device)
        self.device = device

    def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
        slices = self._parse_slices((bs, ts))
        for k, v in data.items():
            if k in self.data.transition_data:
                target = self.data.transition_data
                if mark_filled:
                    target["filled"][slices] = 1
                    mark_filled = False
                _slices = slices
            elif k in self.data.episode_data:
                target = self.data.episode_data
                _slices = slices[0]
            else:
                raise KeyError("{} not found in transition or episode data".format(k))

            dtype = self.scheme[k].get("dtype", torch.float32)
            v = torch.tensor(v, dtype=dtype, device=self.device)
            self._check_safe_view(v, target[k][_slices])
            target[k][_slices] = v.view_as(target[k][_slices])

            if k in self.preprocess:
                new_k = self.preprocess[k][0]
                v = target[k][_slices]
                for transform in self.preprocess[k][1]:
                    v = transform.transform(v)
                target[new_k][_slices] = v.view_as(target[new_k][_slices])

    def _check_safe_view(self, v, dest):
        idx = len(v.shape) - 1
        for s in dest.shape[::-1]:
            if v.shape[idx] != s:
                if s != 1:
                    raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
            else:
                idx -= 1

    def __getitem__(self, item):
        if isinstance(item, str):
            if item in self.data.episode_data:
                return self.data.episode_data[item]
            elif item in self.data.transition_data:
                return self.data.transition_data[item]
            else:
                raise ValueError
        elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
            new_data = self._new_data_sn()
            for key in item:
                if key in self.data.transition_data:
                    new_data.transition_data[key] = self.data.transition_data[key]
                elif key in self.data.episode_data:
                    new_data.episode_data[key] = self.data.episode_data[key]
                else:
                    raise KeyError("Unrecognised key {}".format(key))

            # Update the scheme to only have the requested keys
            new_scheme = {key: self.scheme[key] for key in item}
            new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
                          for key in item if "group" in self.scheme[key]}
            ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
            return ret
        else:
            item = self._parse_slices(item)
            new_data = self._new_data_sn()
            for k, v in self.data.transition_data.items():
                new_data.transition_data[k] = v[item]
            for k, v in self.data.episode_data.items():
                new_data.episode_data[k] = v[item[0]]

            ret_bs = self._get_num_items(item[0], self.batch_size)
            ret_max_t = self._get_num_items(item[1], self.max_seq_length)

            ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
            return ret

    def _get_num_items(self, indexing_item, max_size):
        if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
            return len(indexing_item)
        elif isinstance(indexing_item, slice):
            _range = indexing_item.indices(max_size)
            return 1 + (_range[1] - _range[0] - 1)//_range[2]

    def _new_data_sn(self):
        new_data = SN()
        new_data.transition_data = {}
        new_data.episode_data = {}
        return new_data

    def _parse_slices(self, items):
        parsed = []
        # Only batch slice given, add full time slice
        if (isinstance(items, slice)  # slice a:b
            or isinstance(items, int)  # int i
            or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
            ):
            items = (items, slice(None))

        # Need the time indexing to be contiguous
        if isinstance(items[1], list):
            raise IndexError("Indexing across Time must be contiguous")

        for item in items:
            #TODO: stronger checks to ensure only supported options get through
            if isinstance(item, int):
                # Convert single indices to slices
                parsed.append(slice(item, item+1))
            else:
                # Leave slices and lists as is
                parsed.append(item)
        return parsed

    def max_t_filled(self):
        return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
    def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
        super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
        self.buffer_size = buffer_size  # same as self.batch_size but more explicit
        self.buffer_index = 0
        self.episodes_in_buffer = 0

    def insert_episode_batch(self, ep_batch):
        if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
            self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
            self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
            self.buffer_index = (self.buffer_index + ep_batch.batch_size)
            self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
            self.buffer_index = self.buffer_index % self.buffer_size
            assert self.buffer_index < self.buffer_size
        else:
            buffer_left = self.buffer_size - self.buffer_index
            self.insert_episode_batch(ep_batch[0:buffer_left, :])
            self.insert_episode_batch(ep_batch[buffer_left:, :])

    def can_sample(self, batch_size):
        return self.episodes_in_buffer >= batch_size

    def sample(self, batch_size):
        assert self.can_sample(batch_size)
        if self.episodes_in_buffer == batch_size:
            return self[:batch_size]
        else:
            ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
            return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
# env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		envs.envs[0].render()
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[1], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-1215.035 -1215.035 -1215.035] [618.0036], Avg: [-1833.039 -1833.039 -1833.039] (1.000)
Step: 99, Reward: [-2165.422 -2165.422 -2165.422] [193.3499], Avg: [-2095.905 -2095.905 -2095.905] (1.000)
Step: 149, Reward: [-2201.052 -2201.052 -2201.052] [89.0778], Avg: [-2160.647 -2160.647 -2160.647] (1.000)
Step: 199, Reward: [-1945.347 -1945.347 -1945.347] [90.7954], Avg: [-2129.521 -2129.521 -2129.521] (1.000)
Step: 249, Reward: [-1977.578 -1977.578 -1977.578] [210.7637], Avg: [-2141.285 -2141.285 -2141.285] (1.000)
Step: 299, Reward: [-1702.468 -1702.468 -1702.468] [342.1487], Avg: [-2125.174 -2125.174 -2125.174] (1.000)
Step: 349, Reward: [-1333.198 -1333.198 -1333.198] [330.1101], Avg: [-2059.193 -2059.193 -2059.193] (1.000)
Step: 399, Reward: [-893.078 -893.078 -893.078] [237.1148], Avg: [-1943.068 -1943.068 -1943.068] (1.000)
Step: 449, Reward: [-1239.229 -1239.229 -1239.229] [280.5407], Avg: [-1896.035 -1896.035 -1896.035] (1.000)
Step: 499, Reward: [-2021.063 -2021.063 -2021.063] [335.5254], Avg: [-1942.09 -1942.09 -1942.09] (1.000)
Step: 549, Reward: [-1800.586 -1800.586 -1800.586] [344.1462], Avg: [-1960.512 -1960.512 -1960.512] (1.000)
Step: 599, Reward: [-2050.994 -2050.994 -2050.994] [98.6374], Avg: [-1976.272 -1976.272 -1976.272] (1.000)
Step: 649, Reward: [-2112.293 -2112.293 -2112.293] [241.8697], Avg: [-2005.341 -2005.341 -2005.341] (1.000)
Step: 699, Reward: [-2094.19 -2094.19 -2094.19] [237.7590], Avg: [-2028.67 -2028.67 -2028.67] (1.000)
Step: 749, Reward: [-2114.863 -2114.863 -2114.863] [190.7054], Avg: [-2047.13 -2047.13 -2047.13] (1.000)
Step: 799, Reward: [-2030.062 -2030.062 -2030.062] [116.9257], Avg: [-2053.371 -2053.371 -2053.371] (1.000)
Step: 849, Reward: [-2140.38 -2140.38 -2140.38] [259.6291], Avg: [-2073.761 -2073.761 -2073.761] (1.000)
Step: 899, Reward: [-2100.83 -2100.83 -2100.83] [198.8785], Avg: [-2086.314 -2086.314 -2086.314] (1.000)
Step: 949, Reward: [-2194.07 -2194.07 -2194.07] [135.4242], Avg: [-2099.113 -2099.113 -2099.113] (1.000)
Step: 999, Reward: [-1978.279 -1978.279 -1978.279] [144.0822], Avg: [-2100.275 -2100.275 -2100.275] (1.000)
Step: 1049, Reward: [-1965.663 -1965.663 -1965.663] [242.8772], Avg: [-2105.431 -2105.431 -2105.431] (1.000)
Step: 1099, Reward: [-2029.253 -2029.253 -2029.253] [164.9118], Avg: [-2109.464 -2109.464 -2109.464] (1.000)
Step: 1149, Reward: [-1510.76 -1510.76 -1510.76] [156.1376], Avg: [-2090.222 -2090.222 -2090.222] (1.000)
Step: 1199, Reward: [-2197.494 -2197.494 -2197.494] [207.3265], Avg: [-2103.33 -2103.33 -2103.33] (1.000)
Step: 1249, Reward: [-2013.331 -2013.331 -2013.331] [200.1735], Avg: [-2107.737 -2107.737 -2107.737] (1.000)
Step: 1299, Reward: [-2043.192 -2043.192 -2043.192] [120.9762], Avg: [-2109.908 -2109.908 -2109.908] (1.000)
Step: 1349, Reward: [-1993.231 -1993.231 -1993.231] [122.2375], Avg: [-2110.114 -2110.114 -2110.114] (1.000)
Step: 1399, Reward: [-1956.699 -1956.699 -1956.699] [114.3471], Avg: [-2108.718 -2108.718 -2108.718] (1.000)
Step: 1449, Reward: [-1976.607 -1976.607 -1976.607] [136.6133], Avg: [-2108.874 -2108.874 -2108.874] (1.000)
Step: 1499, Reward: [-2040.591 -2040.591 -2040.591] [196.9756], Avg: [-2113.163 -2113.163 -2113.163] (1.000)
Step: 1549, Reward: [-2075.301 -2075.301 -2075.301] [124.9369], Avg: [-2115.972 -2115.972 -2115.972] (1.000)
Step: 1599, Reward: [-2033.447 -2033.447 -2033.447] [204.1098], Avg: [-2119.772 -2119.772 -2119.772] (1.000)
Step: 1649, Reward: [-2158.558 -2158.558 -2158.558] [139.0053], Avg: [-2125.159 -2125.159 -2125.159] (1.000)
Step: 1699, Reward: [-1630.347 -1630.347 -1630.347] [98.6230], Avg: [-2113.507 -2113.507 -2113.507] (1.000)
Step: 1749, Reward: [-1778.905 -1778.905 -1778.905] [232.1046], Avg: [-2110.578 -2110.578 -2110.578] (1.000)
Step: 1799, Reward: [-1881.152 -1881.152 -1881.152] [305.1985], Avg: [-2112.683 -2112.683 -2112.683] (1.000)
Step: 1849, Reward: [-1144.871 -1144.871 -1144.871] [659.6821], Avg: [-2104.355 -2104.355 -2104.355] (1.000)
Step: 1899, Reward: [-418.137 -418.137 -418.137] [74.6830], Avg: [-2061.946 -2061.946 -2061.946] (1.000)
Step: 1949, Reward: [-1078.693 -1078.693 -1078.693] [266.1655], Avg: [-2043.56 -2043.56 -2043.56] (1.000)
Step: 1999, Reward: [-444.095 -444.095 -444.095] [136.4946], Avg: [-2006.985 -2006.985 -2006.985] (1.000)
Step: 2049, Reward: [-420.128 -420.128 -420.128] [38.9883], Avg: [-1969.232 -1969.232 -1969.232] (1.000)
Step: 2099, Reward: [-1007.96 -1007.96 -1007.96] [142.9570], Avg: [-1949.749 -1949.749 -1949.749] (1.000)
Step: 2149, Reward: [-2020.364 -2020.364 -2020.364] [223.4407], Avg: [-1956.587 -1956.587 -1956.587] (1.000)
Step: 2199, Reward: [-1957.735 -1957.735 -1957.735] [126.9361], Avg: [-1959.498 -1959.498 -1959.498] (1.000)
Step: 2249, Reward: [-2037.729 -2037.729 -2037.729] [145.6888], Avg: [-1964.474 -1964.474 -1964.474] (1.000)
Step: 2299, Reward: [-1994.098 -1994.098 -1994.098] [200.5825], Avg: [-1969.479 -1969.479 -1969.479] (1.000)
Step: 2349, Reward: [-1982.306 -1982.306 -1982.306] [204.7761], Avg: [-1974.109 -1974.109 -1974.109] (1.000)
Step: 2399, Reward: [-2114.308 -2114.308 -2114.308] [264.6310], Avg: [-1982.543 -1982.543 -1982.543] (1.000)
Step: 2449, Reward: [-1875.7 -1875.7 -1875.7] [192.8204], Avg: [-1984.297 -1984.297 -1984.297] (1.000)
Step: 2499, Reward: [-1497.465 -1497.465 -1497.465] [195.7481], Avg: [-1978.475 -1978.475 -1978.475] (1.000)
Step: 2549, Reward: [-1226.184 -1226.184 -1226.184] [179.9138], Avg: [-1967.252 -1967.252 -1967.252] (1.000)
Step: 2599, Reward: [-1216.819 -1216.819 -1216.819] [140.4211], Avg: [-1955.521 -1955.521 -1955.521] (1.000)
Step: 2649, Reward: [-1384.899 -1384.899 -1384.899] [164.9121], Avg: [-1947.866 -1947.866 -1947.866] (1.000)
Step: 2699, Reward: [-1907.8 -1907.8 -1907.8] [190.0136], Avg: [-1950.643 -1950.643 -1950.643] (1.000)
Step: 2749, Reward: [-1939.097 -1939.097 -1939.097] [210.5256], Avg: [-1954.261 -1954.261 -1954.261] (1.000)
Step: 2799, Reward: [-1922.513 -1922.513 -1922.513] [112.1284], Avg: [-1955.696 -1955.696 -1955.696] (1.000)
Step: 2849, Reward: [-2050.482 -2050.482 -2050.482] [68.4065], Avg: [-1958.559 -1958.559 -1958.559] (1.000)
Step: 2899, Reward: [-1943.793 -1943.793 -1943.793] [122.3891], Avg: [-1960.415 -1960.415 -1960.415] (1.000)
Step: 2949, Reward: [-1965.665 -1965.665 -1965.665] [129.5061], Avg: [-1962.699 -1962.699 -1962.699] (1.000)
Step: 2999, Reward: [-2064.338 -2064.338 -2064.338] [195.3369], Avg: [-1967.649 -1967.649 -1967.649] (1.000)
Step: 3049, Reward: [-1980.031 -1980.031 -1980.031] [85.7249], Avg: [-1969.257 -1969.257 -1969.257] (1.000)
Step: 3099, Reward: [-1899.522 -1899.522 -1899.522] [162.5252], Avg: [-1970.754 -1970.754 -1970.754] (1.000)
Step: 3149, Reward: [-2209.223 -2209.223 -2209.223] [241.8283], Avg: [-1978.377 -1978.377 -1978.377] (1.000)
Step: 3199, Reward: [-2149.306 -2149.306 -2149.306] [147.2372], Avg: [-1983.349 -1983.349 -1983.349] (1.000)
Step: 3249, Reward: [-2039.24 -2039.24 -2039.24] [107.9237], Avg: [-1985.869 -1985.869 -1985.869] (1.000)
Step: 3299, Reward: [-1936.387 -1936.387 -1936.387] [117.2324], Avg: [-1986.895 -1986.895 -1986.895] (1.000)
Step: 3349, Reward: [-2135.599 -2135.599 -2135.599] [338.1684], Avg: [-1994.162 -1994.162 -1994.162] (1.000)
Step: 3399, Reward: [-2064.451 -2064.451 -2064.451] [130.0428], Avg: [-1997.108 -1997.108 -1997.108] (1.000)
Step: 3449, Reward: [-2058.589 -2058.589 -2058.589] [219.8102], Avg: [-2001.185 -2001.185 -2001.185] (1.000)
Step: 3499, Reward: [-1407.184 -1407.184 -1407.184] [177.2011], Avg: [-1995.231 -1995.231 -1995.231] (1.000)
Step: 3549, Reward: [-801.403 -801.403 -801.403] [177.8600], Avg: [-1980.921 -1980.921 -1980.921] (1.000)
Step: 3599, Reward: [-546.833 -546.833 -546.833] [53.3426], Avg: [-1961.744 -1961.744 -1961.744] (1.000)
Step: 3649, Reward: [-496.314 -496.314 -496.314] [54.2161], Avg: [-1942.412 -1942.412 -1942.412] (1.000)
Step: 3699, Reward: [-504.577 -504.577 -504.577] [75.4383], Avg: [-1924.002 -1924.002 -1924.002] (1.000)
Step: 3749, Reward: [-447.526 -447.526 -447.526] [56.6865], Avg: [-1905.071 -1905.071 -1905.071] (1.000)
Step: 3799, Reward: [-363.918 -363.918 -363.918] [33.9719], Avg: [-1885.24 -1885.24 -1885.24] (1.000)
Step: 3849, Reward: [-460.405 -460.405 -460.405] [25.4948], Avg: [-1867.067 -1867.067 -1867.067] (1.000)
Step: 3899, Reward: [-469.035 -469.035 -469.035] [60.9323], Avg: [-1849.924 -1849.924 -1849.924] (1.000)
Step: 3949, Reward: [-512.027 -512.027 -512.027] [20.1653], Avg: [-1833.244 -1833.244 -1833.244] (1.000)
Step: 3999, Reward: [-576.279 -576.279 -576.279] [50.1286], Avg: [-1818.159 -1818.159 -1818.159] (1.000)
Step: 4049, Reward: [-937.31 -937.31 -937.31] [85.2327], Avg: [-1808.336 -1808.336 -1808.336] (1.000)
Step: 4099, Reward: [-977.616 -977.616 -977.616] [75.6795], Avg: [-1799.128 -1799.128 -1799.128] (1.000)
Step: 4149, Reward: [-2187.7 -2187.7 -2187.7] [173.8180], Avg: [-1805.904 -1805.904 -1805.904] (1.000)
Step: 4199, Reward: [-2049.011 -2049.011 -2049.011] [245.1590], Avg: [-1811.717 -1811.717 -1811.717] (1.000)
Step: 4249, Reward: [-2068.915 -2068.915 -2068.915] [60.9762], Avg: [-1815.46 -1815.46 -1815.46] (1.000)
Step: 4299, Reward: [-1956.902 -1956.902 -1956.902] [121.9296], Avg: [-1818.523 -1818.523 -1818.523] (1.000)
Step: 4349, Reward: [-1983.491 -1983.491 -1983.491] [138.9924], Avg: [-1822.016 -1822.016 -1822.016] (1.000)
Step: 4399, Reward: [-2099.019 -2099.019 -2099.019] [130.2276], Avg: [-1826.644 -1826.644 -1826.644] (1.000)
Step: 4449, Reward: [-2003.518 -2003.518 -2003.518] [83.8029], Avg: [-1829.573 -1829.573 -1829.573] (1.000)
Step: 4499, Reward: [-1946.216 -1946.216 -1946.216] [157.0514], Avg: [-1832.614 -1832.614 -1832.614] (1.000)
Step: 4549, Reward: [-2069.511 -2069.511 -2069.511] [133.9934], Avg: [-1836.69 -1836.69 -1836.69] (1.000)
Step: 4599, Reward: [-2130.2 -2130.2 -2130.2] [187.8744], Avg: [-1841.922 -1841.922 -1841.922] (1.000)
Step: 4649, Reward: [-2085.319 -2085.319 -2085.319] [245.6510], Avg: [-1847.181 -1847.181 -1847.181] (1.000)
Step: 4699, Reward: [-952.693 -952.693 -952.693] [130.9651], Avg: [-1839.058 -1839.058 -1839.058] (1.000)
Step: 4749, Reward: [-603.616 -603.616 -603.616] [77.1239], Avg: [-1826.865 -1826.865 -1826.865] (1.000)
Step: 4799, Reward: [-519.11 -519.11 -519.11] [78.9249], Avg: [-1814.065 -1814.065 -1814.065] (1.000)
Step: 4849, Reward: [-439.783 -439.783 -439.783] [34.5828], Avg: [-1800.254 -1800.254 -1800.254] (1.000)
Step: 4899, Reward: [-439.668 -439.668 -439.668] [35.5131], Avg: [-1786.733 -1786.733 -1786.733] (1.000)
Step: 4949, Reward: [-391.509 -391.509 -391.509] [41.6550], Avg: [-1773.06 -1773.06 -1773.06] (1.000)
Step: 4999, Reward: [-432.645 -432.645 -432.645] [60.3129], Avg: [-1760.259 -1760.259 -1760.259] (1.000)
Step: 5049, Reward: [-391.325 -391.325 -391.325] [48.8670], Avg: [-1747.189 -1747.189 -1747.189] (1.000)
Step: 5099, Reward: [-452.839 -452.839 -452.839] [67.0322], Avg: [-1735.157 -1735.157 -1735.157] (1.000)
Step: 5149, Reward: [-430.551 -430.551 -430.551] [27.9609], Avg: [-1722.762 -1722.762 -1722.762] (1.000)
Step: 5199, Reward: [-446.855 -446.855 -446.855] [34.0042], Avg: [-1710.821 -1710.821 -1710.821] (1.000)
Step: 5249, Reward: [-485.928 -485.928 -485.928] [50.4802], Avg: [-1699.636 -1699.636 -1699.636] (1.000)
Step: 5299, Reward: [-546.723 -546.723 -546.723] [53.9966], Avg: [-1689.269 -1689.269 -1689.269] (1.000)
Step: 5349, Reward: [-577.93 -577.93 -577.93] [137.5547], Avg: [-1680.168 -1680.168 -1680.168] (1.000)
Step: 5399, Reward: [-587.998 -587.998 -587.998] [54.7774], Avg: [-1670.562 -1670.562 -1670.562] (1.000)
Step: 5449, Reward: [-639.395 -639.395 -639.395] [110.1328], Avg: [-1662.112 -1662.112 -1662.112] (1.000)
Step: 5499, Reward: [-594.254 -594.254 -594.254] [66.2633], Avg: [-1653.007 -1653.007 -1653.007] (1.000)
Step: 5549, Reward: [-649.697 -649.697 -649.697] [55.4096], Avg: [-1644.467 -1644.467 -1644.467] (1.000)
Step: 5599, Reward: [-1145.407 -1145.407 -1145.407] [330.1183], Avg: [-1642.959 -1642.959 -1642.959] (1.000)
Step: 5649, Reward: [-1194.3 -1194.3 -1194.3] [181.8621], Avg: [-1640.598 -1640.598 -1640.598] (1.000)
Step: 5699, Reward: [-842.431 -842.431 -842.431] [87.9347], Avg: [-1634.368 -1634.368 -1634.368] (1.000)
Step: 5749, Reward: [-607.51 -607.51 -607.51] [150.2090], Avg: [-1626.745 -1626.745 -1626.745] (1.000)
Step: 5799, Reward: [-623.941 -623.941 -623.941] [91.2990], Avg: [-1618.887 -1618.887 -1618.887] (1.000)
Step: 5849, Reward: [-541.792 -541.792 -541.792] [62.6260], Avg: [-1610.216 -1610.216 -1610.216] (1.000)
Step: 5899, Reward: [-459.916 -459.916 -459.916] [62.8124], Avg: [-1601. -1601. -1601.] (1.000)
Step: 5949, Reward: [-435.031 -435.031 -435.031] [47.9650], Avg: [-1591.605 -1591.605 -1591.605] (1.000)
Step: 5999, Reward: [-394.345 -394.345 -394.345] [55.9370], Avg: [-1582.094 -1582.094 -1582.094] (1.000)
Step: 6049, Reward: [-483.777 -483.777 -483.777] [106.5343], Avg: [-1573.898 -1573.898 -1573.898] (1.000)
Step: 6099, Reward: [-431.512 -431.512 -431.512] [84.8782], Avg: [-1565.23 -1565.23 -1565.23] (1.000)
Step: 6149, Reward: [-396.911 -396.911 -396.911] [44.7271], Avg: [-1556.095 -1556.095 -1556.095] (1.000)
Step: 6199, Reward: [-487.815 -487.815 -487.815] [64.8404], Avg: [-1548.003 -1548.003 -1548.003] (1.000)
Step: 6249, Reward: [-469.217 -469.217 -469.217] [36.0995], Avg: [-1539.661 -1539.661 -1539.661] (1.000)
Step: 6299, Reward: [-447.13 -447.13 -447.13] [86.2323], Avg: [-1531.675 -1531.675 -1531.675] (1.000)
Step: 6349, Reward: [-480.195 -480.195 -480.195] [57.2023], Avg: [-1523.846 -1523.846 -1523.846] (1.000)
Step: 6399, Reward: [-496.053 -496.053 -496.053] [59.5566], Avg: [-1516.281 -1516.281 -1516.281] (1.000)
Step: 6449, Reward: [-536.88 -536.88 -536.88] [86.9968], Avg: [-1509.363 -1509.363 -1509.363] (1.000)
Step: 6499, Reward: [-472.051 -472.051 -472.051] [68.8426], Avg: [-1501.914 -1501.914 -1501.914] (1.000)
Step: 6549, Reward: [-459.044 -459.044 -459.044] [46.8631], Avg: [-1494.311 -1494.311 -1494.311] (1.000)
Step: 6599, Reward: [-524.866 -524.866 -524.866] [33.3065], Avg: [-1487.219 -1487.219 -1487.219] (1.000)
Step: 6649, Reward: [-489.774 -489.774 -489.774] [34.3537], Avg: [-1479.977 -1479.977 -1479.977] (1.000)
Step: 6699, Reward: [-545.215 -545.215 -545.215] [44.6456], Avg: [-1473.335 -1473.335 -1473.335] (1.000)
Step: 6749, Reward: [-631.943 -631.943 -631.943] [66.9076], Avg: [-1467.598 -1467.598 -1467.598] (1.000)
Step: 6799, Reward: [-762.343 -762.343 -762.343] [145.5616], Avg: [-1463.482 -1463.482 -1463.482] (1.000)
Step: 6849, Reward: [-639.528 -639.528 -639.528] [73.5497], Avg: [-1458.005 -1458.005 -1458.005] (1.000)
Step: 6899, Reward: [-501.901 -501.901 -501.901] [106.1824], Avg: [-1451.846 -1451.846 -1451.846] (1.000)
Step: 6949, Reward: [-466.982 -466.982 -466.982] [91.5883], Avg: [-1445.42 -1445.42 -1445.42] (1.000)
Step: 6999, Reward: [-451.363 -451.363 -451.363] [40.6033], Avg: [-1438.609 -1438.609 -1438.609] (1.000)
Step: 7049, Reward: [-498.721 -498.721 -498.721] [92.6404], Avg: [-1432.6 -1432.6 -1432.6] (1.000)
Step: 7099, Reward: [-399.774 -399.774 -399.774] [55.5019], Avg: [-1425.718 -1425.718 -1425.718] (1.000)
Step: 7149, Reward: [-447.146 -447.146 -447.146] [76.5994], Avg: [-1419.41 -1419.41 -1419.41] (1.000)
Step: 7199, Reward: [-496.493 -496.493 -496.493] [50.6343], Avg: [-1413.353 -1413.353 -1413.353] (1.000)
Step: 7249, Reward: [-417.092 -417.092 -417.092] [58.2407], Avg: [-1406.884 -1406.884 -1406.884] (1.000)
Step: 7299, Reward: [-468.077 -468.077 -468.077] [70.1299], Avg: [-1400.934 -1400.934 -1400.934] (1.000)
Step: 7349, Reward: [-446.428 -446.428 -446.428] [84.1502], Avg: [-1395.013 -1395.013 -1395.013] (1.000)
Step: 7399, Reward: [-417.386 -417.386 -417.386] [68.4415], Avg: [-1388.87 -1388.87 -1388.87] (1.000)
Step: 7449, Reward: [-437.84 -437.84 -437.84] [44.0129], Avg: [-1382.783 -1382.783 -1382.783] (1.000)
Step: 7499, Reward: [-503.348 -503.348 -503.348] [96.8970], Avg: [-1377.566 -1377.566 -1377.566] (1.000)
Step: 7549, Reward: [-463.662 -463.662 -463.662] [86.6652], Avg: [-1372.087 -1372.087 -1372.087] (1.000)
Step: 7599, Reward: [-480.232 -480.232 -480.232] [70.6439], Avg: [-1366.685 -1366.685 -1366.685] (1.000)
Step: 7649, Reward: [-471.42 -471.42 -471.42] [23.1090], Avg: [-1360.984 -1360.984 -1360.984] (1.000)
Step: 7699, Reward: [-481.229 -481.229 -481.229] [51.0246], Avg: [-1355.603 -1355.603 -1355.603] (1.000)
Step: 7749, Reward: [-584.633 -584.633 -584.633] [108.2396], Avg: [-1351.327 -1351.327 -1351.327] (1.000)
Step: 7799, Reward: [-439.606 -439.606 -439.606] [45.5854], Avg: [-1345.775 -1345.775 -1345.775] (1.000)
Step: 7849, Reward: [-610.127 -610.127 -610.127] [156.0627], Avg: [-1342.083 -1342.083 -1342.083] (1.000)
Step: 7899, Reward: [-518.589 -518.589 -518.589] [58.4872], Avg: [-1337.242 -1337.242 -1337.242] (1.000)
Step: 7949, Reward: [-524.373 -524.373 -524.373] [65.4634], Avg: [-1332.541 -1332.541 -1332.541] (1.000)
Step: 7999, Reward: [-502.903 -502.903 -502.903] [55.1923], Avg: [-1327.701 -1327.701 -1327.701] (1.000)
Step: 8049, Reward: [-575.198 -575.198 -575.198] [82.2416], Avg: [-1323.537 -1323.537 -1323.537] (1.000)
Step: 8099, Reward: [-662.363 -662.363 -662.363] [104.0240], Avg: [-1320.098 -1320.098 -1320.098] (1.000)
Step: 8149, Reward: [-663.364 -663.364 -663.364] [105.0908], Avg: [-1316.714 -1316.714 -1316.714] (1.000)
Step: 8199, Reward: [-668.532 -668.532 -668.532] [128.9013], Avg: [-1313.548 -1313.548 -1313.548] (1.000)
Step: 8249, Reward: [-571.502 -571.502 -571.502] [63.3082], Avg: [-1309.434 -1309.434 -1309.434] (1.000)
Step: 8299, Reward: [-975.714 -975.714 -975.714] [340.3810], Avg: [-1309.474 -1309.474 -1309.474] (1.000)
Step: 8349, Reward: [-584.972 -584.972 -584.972] [155.2046], Avg: [-1306.065 -1306.065 -1306.065] (1.000)
Step: 8399, Reward: [-665.862 -665.862 -665.862] [90.8748], Avg: [-1302.795 -1302.795 -1302.795] (1.000)
Step: 8449, Reward: [-688.265 -688.265 -688.265] [143.2532], Avg: [-1300.007 -1300.007 -1300.007] (1.000)
Step: 8499, Reward: [-643.883 -643.883 -643.883] [82.2253], Avg: [-1296.631 -1296.631 -1296.631] (1.000)
Step: 8549, Reward: [-574.953 -574.953 -574.953] [137.3775], Avg: [-1293.214 -1293.214 -1293.214] (1.000)
Step: 8599, Reward: [-657.163 -657.163 -657.163] [150.3937], Avg: [-1290.39 -1290.39 -1290.39] (1.000)
Step: 8649, Reward: [-564.367 -564.367 -564.367] [128.8773], Avg: [-1286.939 -1286.939 -1286.939] (1.000)
Step: 8699, Reward: [-556.103 -556.103 -556.103] [171.0977], Avg: [-1283.722 -1283.722 -1283.722] (1.000)
Step: 8749, Reward: [-479.967 -479.967 -479.967] [114.2384], Avg: [-1279.782 -1279.782 -1279.782] (1.000)
Step: 8799, Reward: [-552.936 -552.936 -552.936] [47.2494], Avg: [-1275.92 -1275.92 -1275.92] (1.000)
Step: 8849, Reward: [-446.504 -446.504 -446.504] [44.8273], Avg: [-1271.488 -1271.488 -1271.488] (1.000)
Step: 8899, Reward: [-956.167 -956.167 -956.167] [406.4644], Avg: [-1272. -1272. -1272.] (1.000)
Step: 8949, Reward: [-659.105 -659.105 -659.105] [101.0868], Avg: [-1269.14 -1269.14 -1269.14] (1.000)
Step: 8999, Reward: [-603.073 -603.073 -603.073] [102.7811], Avg: [-1266.011 -1266.011 -1266.011] (1.000)
Step: 9049, Reward: [-600.856 -600.856 -600.856] [185.1282], Avg: [-1263.359 -1263.359 -1263.359] (1.000)
Step: 9099, Reward: [-661.966 -661.966 -661.966] [200.9569], Avg: [-1261.159 -1261.159 -1261.159] (1.000)
Step: 9149, Reward: [-626.039 -626.039 -626.039] [159.2439], Avg: [-1258.558 -1258.558 -1258.558] (1.000)
Step: 9199, Reward: [-843.018 -843.018 -843.018] [213.2963], Avg: [-1257.459 -1257.459 -1257.459] (1.000)
Step: 9249, Reward: [-672.951 -672.951 -672.951] [104.4471], Avg: [-1254.864 -1254.864 -1254.864] (1.000)
Step: 9299, Reward: [-572.595 -572.595 -572.595] [107.9325], Avg: [-1251.776 -1251.776 -1251.776] (1.000)
Step: 9349, Reward: [-605.082 -605.082 -605.082] [127.2953], Avg: [-1248.999 -1248.999 -1248.999] (1.000)
Step: 9399, Reward: [-476.783 -476.783 -476.783] [58.7407], Avg: [-1245.204 -1245.204 -1245.204] (1.000)
Step: 9449, Reward: [-524.365 -524.365 -524.365] [123.3783], Avg: [-1242.043 -1242.043 -1242.043] (1.000)
Step: 9499, Reward: [-582.325 -582.325 -582.325] [166.6438], Avg: [-1239.447 -1239.447 -1239.447] (1.000)
Step: 9549, Reward: [-493.529 -493.529 -493.529] [85.9302], Avg: [-1235.992 -1235.992 -1235.992] (1.000)
Step: 9599, Reward: [-511.376 -511.376 -511.376] [126.7298], Avg: [-1232.878 -1232.878 -1232.878] (1.000)
Step: 9649, Reward: [-555.826 -555.826 -555.826] [93.4974], Avg: [-1229.854 -1229.854 -1229.854] (1.000)
Step: 9699, Reward: [-590.069 -590.069 -590.069] [107.5893], Avg: [-1227.111 -1227.111 -1227.111] (1.000)
Step: 9749, Reward: [-594.993 -594.993 -594.993] [118.5733], Avg: [-1224.478 -1224.478 -1224.478] (1.000)
Step: 9799, Reward: [-780.67 -780.67 -780.67] [127.0340], Avg: [-1222.861 -1222.861 -1222.861] (1.000)
Step: 9849, Reward: [-710.921 -710.921 -710.921] [214.2457], Avg: [-1221.35 -1221.35 -1221.35] (1.000)
Step: 9899, Reward: [-823.615 -823.615 -823.615] [159.1362], Avg: [-1220.145 -1220.145 -1220.145] (1.000)
Step: 9949, Reward: [-534.507 -534.507 -534.507] [75.2486], Avg: [-1217.078 -1217.078 -1217.078] (1.000)
Step: 9999, Reward: [-621.466 -621.466 -621.466] [180.4155], Avg: [-1215.002 -1215.002 -1215.002] (1.000)
Step: 10049, Reward: [-710.884 -710.884 -710.884] [185.2498], Avg: [-1213.416 -1213.416 -1213.416] (1.000)
Step: 10099, Reward: [-607.073 -607.073 -607.073] [179.8147], Avg: [-1211.304 -1211.304 -1211.304] (1.000)
Step: 10149, Reward: [-547.152 -547.152 -547.152] [150.9339], Avg: [-1208.776 -1208.776 -1208.776] (1.000)
Step: 10199, Reward: [-588.311 -588.311 -588.311] [114.9080], Avg: [-1206.298 -1206.298 -1206.298] (1.000)
Step: 10249, Reward: [-984.038 -984.038 -984.038] [472.9989], Avg: [-1207.521 -1207.521 -1207.521] (1.000)
Step: 10299, Reward: [-909.956 -909.956 -909.956] [319.0966], Avg: [-1207.625 -1207.625 -1207.625] (1.000)
Step: 10349, Reward: [-683.47 -683.47 -683.47] [197.5068], Avg: [-1206.047 -1206.047 -1206.047] (1.000)
Step: 10399, Reward: [-803.25 -803.25 -803.25] [380.6310], Avg: [-1205.941 -1205.941 -1205.941] (1.000)
Step: 10449, Reward: [-512.103 -512.103 -512.103] [107.8279], Avg: [-1203.137 -1203.137 -1203.137] (1.000)
Step: 10499, Reward: [-787.93 -787.93 -787.93] [242.8808], Avg: [-1202.316 -1202.316 -1202.316] (1.000)
Step: 10549, Reward: [-654.063 -654.063 -654.063] [120.2749], Avg: [-1200.288 -1200.288 -1200.288] (1.000)
Step: 10599, Reward: [-677.842 -677.842 -677.842] [233.8605], Avg: [-1198.927 -1198.927 -1198.927] (1.000)
Step: 10649, Reward: [-739.264 -739.264 -739.264] [252.3099], Avg: [-1197.953 -1197.953 -1197.953] (1.000)
Step: 10699, Reward: [-719.511 -719.511 -719.511] [226.7274], Avg: [-1196.777 -1196.777 -1196.777] (1.000)
Step: 10749, Reward: [-676.983 -676.983 -676.983] [237.0549], Avg: [-1195.462 -1195.462 -1195.462] (1.000)
Step: 10799, Reward: [-641.94 -641.94 -641.94] [151.0548], Avg: [-1193.599 -1193.599 -1193.599] (1.000)
Step: 10849, Reward: [-486.457 -486.457 -486.457] [37.3073], Avg: [-1190.512 -1190.512 -1190.512] (1.000)
Step: 10899, Reward: [-691.143 -691.143 -691.143] [264.3800], Avg: [-1189.434 -1189.434 -1189.434] (1.000)
Step: 10949, Reward: [-776.699 -776.699 -776.699] [193.8892], Avg: [-1188.435 -1188.435 -1188.435] (1.000)
Step: 10999, Reward: [-802.71 -802.71 -802.71] [372.9528], Avg: [-1188.377 -1188.377 -1188.377] (1.000)
Step: 11049, Reward: [-707.329 -707.329 -707.329] [278.8993], Avg: [-1187.462 -1187.462 -1187.462] (1.000)
Step: 11099, Reward: [-746.144 -746.144 -746.144] [182.1629], Avg: [-1186.294 -1186.294 -1186.294] (1.000)
Step: 11149, Reward: [-839.03 -839.03 -839.03] [297.0298], Avg: [-1186.069 -1186.069 -1186.069] (1.000)
Step: 11199, Reward: [-520.349 -520.349 -520.349] [145.1658], Avg: [-1183.745 -1183.745 -1183.745] (1.000)
Step: 11249, Reward: [-873.014 -873.014 -873.014] [362.3510], Avg: [-1183.975 -1183.975 -1183.975] (1.000)
Step: 11299, Reward: [-844.776 -844.776 -844.776] [172.8787], Avg: [-1183.239 -1183.239 -1183.239] (1.000)
Step: 11349, Reward: [-578.044 -578.044 -578.044] [164.7158], Avg: [-1181.298 -1181.298 -1181.298] (1.000)
Step: 11399, Reward: [-826.102 -826.102 -826.102] [298.4517], Avg: [-1181.049 -1181.049 -1181.049] (1.000)
Step: 11449, Reward: [-903.713 -903.713 -903.713] [323.8857], Avg: [-1181.253 -1181.253 -1181.253] (1.000)
Step: 11499, Reward: [-793.407 -793.407 -793.407] [302.6068], Avg: [-1180.882 -1180.882 -1180.882] (1.000)
Step: 11549, Reward: [-894.627 -894.627 -894.627] [460.8402], Avg: [-1181.638 -1181.638 -1181.638] (1.000)
Step: 11599, Reward: [-941.895 -941.895 -941.895] [449.2346], Avg: [-1182.541 -1182.541 -1182.541] (1.000)
Step: 11649, Reward: [-1067.35 -1067.35 -1067.35] [590.1429], Avg: [-1184.579 -1184.579 -1184.579] (1.000)
Step: 11699, Reward: [-1110.203 -1110.203 -1110.203] [463.7951], Avg: [-1186.244 -1186.244 -1186.244] (1.000)
Step: 11749, Reward: [-870.455 -870.455 -870.455] [646.8561], Avg: [-1187.652 -1187.652 -1187.652] (1.000)
Step: 11799, Reward: [-913.506 -913.506 -913.506] [220.2980], Avg: [-1187.424 -1187.424 -1187.424] (1.000)
Step: 11849, Reward: [-956.93 -956.93 -956.93] [605.7727], Avg: [-1189.008 -1189.008 -1189.008] (1.000)
Step: 11899, Reward: [-1196.417 -1196.417 -1196.417] [410.0549], Avg: [-1190.762 -1190.762 -1190.762] (1.000)
Step: 11949, Reward: [-1185.655 -1185.655 -1185.655] [354.8602], Avg: [-1192.225 -1192.225 -1192.225] (1.000)
Step: 11999, Reward: [-977.865 -977.865 -977.865] [398.7645], Avg: [-1192.993 -1192.993 -1192.993] (1.000)
Step: 12049, Reward: [-932.043 -932.043 -932.043] [292.9223], Avg: [-1193.126 -1193.126 -1193.126] (1.000)
Step: 12099, Reward: [-716.208 -716.208 -716.208] [100.2700], Avg: [-1191.57 -1191.57 -1191.57] (1.000)
Step: 12149, Reward: [-725.074 -725.074 -725.074] [207.2580], Avg: [-1190.503 -1190.503 -1190.503] (1.000)
Step: 12199, Reward: [-873.454 -873.454 -873.454] [258.3354], Avg: [-1190.262 -1190.262 -1190.262] (1.000)
Step: 12249, Reward: [-885.823 -885.823 -885.823] [277.6335], Avg: [-1190.153 -1190.153 -1190.153] (1.000)
Step: 12299, Reward: [-897.672 -897.672 -897.672] [310.8440], Avg: [-1190.227 -1190.227 -1190.227] (1.000)
Step: 12349, Reward: [-694.51 -694.51 -694.51] [114.5138], Avg: [-1188.684 -1188.684 -1188.684] (1.000)
Step: 12399, Reward: [-1111.912 -1111.912 -1111.912] [586.2360], Avg: [-1190.738 -1190.738 -1190.738] (1.000)
Step: 12449, Reward: [-900.241 -900.241 -900.241] [326.9113], Avg: [-1190.885 -1190.885 -1190.885] (1.000)
Step: 12499, Reward: [-965.37 -965.37 -965.37] [463.2472], Avg: [-1191.836 -1191.836 -1191.836] (1.000)
Step: 12549, Reward: [-898.4 -898.4 -898.4] [257.7516], Avg: [-1191.693 -1191.693 -1191.693] (1.000)
Step: 12599, Reward: [-1039.856 -1039.856 -1039.856] [595.2919], Avg: [-1193.453 -1193.453 -1193.453] (1.000)
Step: 12649, Reward: [-856.757 -856.757 -856.757] [329.8631], Avg: [-1193.426 -1193.426 -1193.426] (1.000)
Step: 12699, Reward: [-743.861 -743.861 -743.861] [286.6915], Avg: [-1192.785 -1192.785 -1192.785] (1.000)
Step: 12749, Reward: [-574.404 -574.404 -574.404] [85.8436], Avg: [-1190.697 -1190.697 -1190.697] (1.000)
Step: 12799, Reward: [-1078.343 -1078.343 -1078.343] [323.0998], Avg: [-1191.52 -1191.52 -1191.52] (1.000)
Step: 12849, Reward: [-823.264 -823.264 -823.264] [389.6875], Avg: [-1191.603 -1191.603 -1191.603] (1.000)
Step: 12899, Reward: [-750.115 -750.115 -750.115] [200.2200], Avg: [-1190.668 -1190.668 -1190.668] (1.000)
Step: 12949, Reward: [-748.964 -748.964 -748.964] [268.8883], Avg: [-1190.001 -1190.001 -1190.001] (1.000)
Step: 12999, Reward: [-625.424 -625.424 -625.424] [60.5726], Avg: [-1188.062 -1188.062 -1188.062] (1.000)
Step: 13049, Reward: [-897.256 -897.256 -897.256] [279.1780], Avg: [-1188.018 -1188.018 -1188.018] (1.000)
Step: 13099, Reward: [-535.29 -535.29 -535.29] [115.1081], Avg: [-1185.966 -1185.966 -1185.966] (1.000)
Step: 13149, Reward: [-781.152 -781.152 -781.152] [203.3264], Avg: [-1185.2 -1185.2 -1185.2] (1.000)
Step: 13199, Reward: [-536.47 -536.47 -536.47] [133.3963], Avg: [-1183.248 -1183.248 -1183.248] (1.000)
Step: 13249, Reward: [-603.562 -603.562 -603.562] [245.7069], Avg: [-1181.987 -1181.987 -1181.987] (1.000)
Step: 13299, Reward: [-609.918 -609.918 -609.918] [252.9394], Avg: [-1180.788 -1180.788 -1180.788] (1.000)
Step: 13349, Reward: [-506.734 -506.734 -506.734] [105.3315], Avg: [-1178.658 -1178.658 -1178.658] (1.000)
Step: 13399, Reward: [-666.413 -666.413 -666.413] [124.6263], Avg: [-1177.211 -1177.211 -1177.211] (1.000)
Step: 13449, Reward: [-519.793 -519.793 -519.793] [47.7000], Avg: [-1174.945 -1174.945 -1174.945] (1.000)
Step: 13499, Reward: [-614.276 -614.276 -614.276] [115.9552], Avg: [-1173.297 -1173.297 -1173.297] (1.000)
Step: 13549, Reward: [-629.098 -629.098 -629.098] [86.3968], Avg: [-1171.608 -1171.608 -1171.608] (1.000)
Step: 13599, Reward: [-617.357 -617.357 -617.357] [89.3308], Avg: [-1169.899 -1169.899 -1169.899] (1.000)
Step: 13649, Reward: [-680.526 -680.526 -680.526] [186.4931], Avg: [-1168.789 -1168.789 -1168.789] (1.000)
Step: 13699, Reward: [-724.904 -724.904 -724.904] [269.5517], Avg: [-1168.153 -1168.153 -1168.153] (1.000)
Step: 13749, Reward: [-984.045 -984.045 -984.045] [281.8609], Avg: [-1168.509 -1168.509 -1168.509] (1.000)
Step: 13799, Reward: [-769.575 -769.575 -769.575] [271.4723], Avg: [-1168.047 -1168.047 -1168.047] (1.000)
Step: 13849, Reward: [-617.942 -617.942 -617.942] [196.7482], Avg: [-1166.771 -1166.771 -1166.771] (1.000)
Step: 13899, Reward: [-777.312 -777.312 -777.312] [220.3342], Avg: [-1166.163 -1166.163 -1166.163] (1.000)
Step: 13949, Reward: [-797.032 -797.032 -797.032] [195.5612], Avg: [-1165.541 -1165.541 -1165.541] (1.000)
Step: 13999, Reward: [-720.889 -720.889 -720.889] [234.0107], Avg: [-1164.788 -1164.788 -1164.788] (1.000)
Step: 14049, Reward: [-694.637 -694.637 -694.637] [130.6999], Avg: [-1163.58 -1163.58 -1163.58] (1.000)
Step: 14099, Reward: [-793.118 -793.118 -793.118] [346.7470], Avg: [-1163.496 -1163.496 -1163.496] (1.000)
Step: 14149, Reward: [-643.569 -643.569 -643.569] [195.1365], Avg: [-1162.349 -1162.349 -1162.349] (1.000)
Step: 14199, Reward: [-615.526 -615.526 -615.526] [109.2849], Avg: [-1160.808 -1160.808 -1160.808] (1.000)
Step: 14249, Reward: [-654.736 -654.736 -654.736] [105.5681], Avg: [-1159.403 -1159.403 -1159.403] (1.000)
Step: 14299, Reward: [-639.09 -639.09 -639.09] [154.4282], Avg: [-1158.123 -1158.123 -1158.123] (1.000)
Step: 14349, Reward: [-697.293 -697.293 -697.293] [248.1848], Avg: [-1157.382 -1157.382 -1157.382] (1.000)
Step: 14399, Reward: [-567.513 -567.513 -567.513] [98.8447], Avg: [-1155.678 -1155.678 -1155.678] (1.000)
Step: 14449, Reward: [-683.585 -683.585 -683.585] [245.0478], Avg: [-1154.892 -1154.892 -1154.892] (1.000)
Step: 14499, Reward: [-613.887 -613.887 -613.887] [62.6973], Avg: [-1153.243 -1153.243 -1153.243] (1.000)
Step: 14549, Reward: [-882.422 -882.422 -882.422] [312.8812], Avg: [-1153.387 -1153.387 -1153.387] (1.000)
Step: 14599, Reward: [-640.903 -640.903 -640.903] [110.3888], Avg: [-1152.01 -1152.01 -1152.01] (1.000)
Step: 14649, Reward: [-717.995 -717.995 -717.995] [196.5660], Avg: [-1151.2 -1151.2 -1151.2] (1.000)
Step: 14699, Reward: [-714.198 -714.198 -714.198] [125.8047], Avg: [-1150.141 -1150.141 -1150.141] (1.000)
Step: 14749, Reward: [-636.377 -636.377 -636.377] [51.1918], Avg: [-1148.573 -1148.573 -1148.573] (1.000)
Step: 14799, Reward: [-697.921 -697.921 -697.921] [314.0828], Avg: [-1148.112 -1148.112 -1148.112] (1.000)
Step: 14849, Reward: [-563.962 -563.962 -563.962] [105.0393], Avg: [-1146.499 -1146.499 -1146.499] (1.000)
Step: 14899, Reward: [-819.589 -819.589 -819.589] [291.4118], Avg: [-1146.379 -1146.379 -1146.379] (1.000)
Step: 14949, Reward: [-849.097 -849.097 -849.097] [442.3704], Avg: [-1146.865 -1146.865 -1146.865] (1.000)
Step: 14999, Reward: [-685.639 -685.639 -685.639] [171.1577], Avg: [-1145.898 -1145.898 -1145.898] (1.000)
Step: 15049, Reward: [-590.895 -590.895 -590.895] [89.0564], Avg: [-1144.35 -1144.35 -1144.35] (1.000)
Step: 15099, Reward: [-587.351 -587.351 -587.351] [108.5870], Avg: [-1142.865 -1142.865 -1142.865] (1.000)
Step: 15149, Reward: [-640.508 -640.508 -640.508] [102.1248], Avg: [-1141.544 -1141.544 -1141.544] (1.000)
Step: 15199, Reward: [-614.653 -614.653 -614.653] [88.8516], Avg: [-1140.103 -1140.103 -1140.103] (1.000)
Step: 15249, Reward: [-657.958 -657.958 -657.958] [96.7160], Avg: [-1138.84 -1138.84 -1138.84] (1.000)
Step: 15299, Reward: [-577.656 -577.656 -577.656] [83.0040], Avg: [-1137.277 -1137.277 -1137.277] (1.000)
Step: 15349, Reward: [-491.339 -491.339 -491.339] [51.2122], Avg: [-1135.34 -1135.34 -1135.34] (1.000)
Step: 15399, Reward: [-767.873 -767.873 -767.873] [251.0658], Avg: [-1134.962 -1134.962 -1134.962] (1.000)
Step: 15449, Reward: [-629.262 -629.262 -629.262] [112.2971], Avg: [-1133.689 -1133.689 -1133.689] (1.000)
Step: 15499, Reward: [-611.666 -611.666 -611.666] [182.1075], Avg: [-1132.592 -1132.592 -1132.592] (1.000)
Step: 15549, Reward: [-731.406 -731.406 -731.406] [157.5483], Avg: [-1131.809 -1131.809 -1131.809] (1.000)
Step: 15599, Reward: [-701.276 -701.276 -701.276] [176.7956], Avg: [-1130.995 -1130.995 -1130.995] (1.000)
Step: 15649, Reward: [-707.818 -707.818 -707.818] [284.5029], Avg: [-1130.552 -1130.552 -1130.552] (1.000)
Step: 15699, Reward: [-788.736 -788.736 -788.736] [181.0997], Avg: [-1130.04 -1130.04 -1130.04] (1.000)
Step: 15749, Reward: [-704.544 -704.544 -704.544] [130.5554], Avg: [-1129.104 -1129.104 -1129.104] (1.000)
Step: 15799, Reward: [-626.263 -626.263 -626.263] [194.5696], Avg: [-1128.129 -1128.129 -1128.129] (1.000)
Step: 15849, Reward: [-563.852 -563.852 -563.852] [201.1703], Avg: [-1126.983 -1126.983 -1126.983] (1.000)
Step: 15899, Reward: [-526.286 -526.286 -526.286] [71.4471], Avg: [-1125.319 -1125.319 -1125.319] (1.000)
Step: 15949, Reward: [-863.618 -863.618 -863.618] [337.3352], Avg: [-1125.556 -1125.556 -1125.556] (1.000)
Step: 15999, Reward: [-1082.187 -1082.187 -1082.187] [348.9659], Avg: [-1126.511 -1126.511 -1126.511] (1.000)
Step: 16049, Reward: [-614.805 -614.805 -614.805] [180.7480], Avg: [-1125.48 -1125.48 -1125.48] (1.000)
Step: 16099, Reward: [-1037.25 -1037.25 -1037.25] [464.5481], Avg: [-1126.649 -1126.649 -1126.649] (1.000)
Step: 16149, Reward: [-958.666 -958.666 -958.666] [445.6337], Avg: [-1127.508 -1127.508 -1127.508] (1.000)
Step: 16199, Reward: [-1053.652 -1053.652 -1053.652] [233.7296], Avg: [-1128.002 -1128.002 -1128.002] (1.000)
Step: 16249, Reward: [-873.683 -873.683 -873.683] [319.2974], Avg: [-1128.202 -1128.202 -1128.202] (1.000)
Step: 16299, Reward: [-748.479 -748.479 -748.479] [319.2524], Avg: [-1128.016 -1128.016 -1128.016] (1.000)
Step: 16349, Reward: [-1506.973 -1506.973 -1506.973] [524.6410], Avg: [-1130.779 -1130.779 -1130.779] (1.000)
Step: 16399, Reward: [-949.733 -949.733 -949.733] [565.2876], Avg: [-1131.951 -1131.951 -1131.951] (1.000)
Step: 16449, Reward: [-1724.035 -1724.035 -1724.035] [473.0927], Avg: [-1135.188 -1135.188 -1135.188] (1.000)
Step: 16499, Reward: [-1256.252 -1256.252 -1256.252] [494.6734], Avg: [-1137.054 -1137.054 -1137.054] (1.000)
Step: 16549, Reward: [-877.529 -877.529 -877.529] [528.7114], Avg: [-1137.868 -1137.868 -1137.868] (1.000)
Step: 16599, Reward: [-1142.223 -1142.223 -1142.223] [483.8473], Avg: [-1139.338 -1139.338 -1139.338] (1.000)
Step: 16649, Reward: [-1215.48 -1215.48 -1215.48] [402.4075], Avg: [-1140.775 -1140.775 -1140.775] (1.000)
Step: 16699, Reward: [-1055.235 -1055.235 -1055.235] [572.5577], Avg: [-1142.233 -1142.233 -1142.233] (1.000)
Step: 16749, Reward: [-923.696 -923.696 -923.696] [242.0224], Avg: [-1142.303 -1142.303 -1142.303] (1.000)
Step: 16799, Reward: [-1394.837 -1394.837 -1394.837] [390.5180], Avg: [-1144.217 -1144.217 -1144.217] (1.000)
Step: 16849, Reward: [-955.971 -955.971 -955.971] [445.4601], Avg: [-1144.981 -1144.981 -1144.981] (1.000)
Step: 16899, Reward: [-1154.983 -1154.983 -1154.983] [624.0164], Avg: [-1146.856 -1146.856 -1146.856] (1.000)
Step: 16949, Reward: [-770.314 -770.314 -770.314] [139.9699], Avg: [-1146.158 -1146.158 -1146.158] (1.000)
Step: 16999, Reward: [-1364.678 -1364.678 -1364.678] [537.2290], Avg: [-1148.381 -1148.381 -1148.381] (1.000)
Step: 17049, Reward: [-848.776 -848.776 -848.776] [134.0235], Avg: [-1147.896 -1147.896 -1147.896] (1.000)
Step: 17099, Reward: [-1132.842 -1132.842 -1132.842] [530.8791], Avg: [-1149.404 -1149.404 -1149.404] (1.000)
Step: 17149, Reward: [-1063.623 -1063.623 -1063.623] [508.0038], Avg: [-1150.635 -1150.635 -1150.635] (1.000)
Step: 17199, Reward: [-866.162 -866.162 -866.162] [96.6057], Avg: [-1150.089 -1150.089 -1150.089] (1.000)
Step: 17249, Reward: [-999.587 -999.587 -999.587] [472.1361], Avg: [-1151.021 -1151.021 -1151.021] (1.000)
Step: 17299, Reward: [-1417.732 -1417.732 -1417.732] [394.3048], Avg: [-1152.932 -1152.932 -1152.932] (1.000)
Step: 17349, Reward: [-953.741 -953.741 -953.741] [458.9301], Avg: [-1153.68 -1153.68 -1153.68] (1.000)
Step: 17399, Reward: [-946.809 -946.809 -946.809] [321.3674], Avg: [-1154.009 -1154.009 -1154.009] (1.000)
Step: 17449, Reward: [-936.916 -936.916 -936.916] [347.2485], Avg: [-1154.382 -1154.382 -1154.382] (1.000)
Step: 17499, Reward: [-833.038 -833.038 -833.038] [332.8498], Avg: [-1154.415 -1154.415 -1154.415] (1.000)
Step: 17549, Reward: [-708.026 -708.026 -708.026] [253.9719], Avg: [-1153.867 -1153.867 -1153.867] (1.000)
Step: 17599, Reward: [-764.029 -764.029 -764.029] [270.6250], Avg: [-1153.528 -1153.528 -1153.528] (1.000)
Step: 17649, Reward: [-628.895 -628.895 -628.895] [157.7095], Avg: [-1152.489 -1152.489 -1152.489] (1.000)
Step: 17699, Reward: [-587.252 -587.252 -587.252] [195.4114], Avg: [-1151.444 -1151.444 -1151.444] (1.000)
Step: 17749, Reward: [-638.028 -638.028 -638.028] [90.9250], Avg: [-1150.254 -1150.254 -1150.254] (1.000)
Step: 17799, Reward: [-583.167 -583.167 -583.167] [84.4953], Avg: [-1148.898 -1148.898 -1148.898] (1.000)
Step: 17849, Reward: [-848.233 -848.233 -848.233] [407.2386], Avg: [-1149.197 -1149.197 -1149.197] (1.000)
Step: 17899, Reward: [-575.473 -575.473 -575.473] [107.2603], Avg: [-1147.894 -1147.894 -1147.894] (1.000)
Step: 17949, Reward: [-820.058 -820.058 -820.058] [286.6930], Avg: [-1147.779 -1147.779 -1147.779] (1.000)
Step: 17999, Reward: [-522.389 -522.389 -522.389] [102.3178], Avg: [-1146.326 -1146.326 -1146.326] (1.000)
Step: 18049, Reward: [-501.042 -501.042 -501.042] [59.0363], Avg: [-1144.702 -1144.702 -1144.702] (1.000)
Step: 18099, Reward: [-956.149 -956.149 -956.149] [377.1008], Avg: [-1145.223 -1145.223 -1145.223] (1.000)
Step: 18149, Reward: [-758.636 -758.636 -758.636] [180.9747], Avg: [-1144.657 -1144.657 -1144.657] (1.000)
Step: 18199, Reward: [-784.045 -784.045 -784.045] [211.8814], Avg: [-1144.248 -1144.248 -1144.248] (1.000)
Step: 18249, Reward: [-663.465 -663.465 -663.465] [188.2160], Avg: [-1143.446 -1143.446 -1143.446] (1.000)
Step: 18299, Reward: [-669.604 -669.604 -669.604] [202.2270], Avg: [-1142.704 -1142.704 -1142.704] (1.000)
Step: 18349, Reward: [-621.737 -621.737 -621.737] [174.3731], Avg: [-1141.76 -1141.76 -1141.76] (1.000)
Step: 18399, Reward: [-699.494 -699.494 -699.494] [161.0726], Avg: [-1140.996 -1140.996 -1140.996] (1.000)
Step: 18449, Reward: [-657.155 -657.155 -657.155] [144.3423], Avg: [-1140.076 -1140.076 -1140.076] (1.000)
Step: 18499, Reward: [-673.176 -673.176 -673.176] [110.9784], Avg: [-1139.114 -1139.114 -1139.114] (1.000)
Step: 18549, Reward: [-651.167 -651.167 -651.167] [192.4677], Avg: [-1138.317 -1138.317 -1138.317] (1.000)
Step: 18599, Reward: [-756.774 -756.774 -756.774] [139.4509], Avg: [-1137.667 -1137.667 -1137.667] (1.000)
Step: 18649, Reward: [-624.637 -624.637 -624.637] [140.9583], Avg: [-1136.669 -1136.669 -1136.669] (1.000)
Step: 18699, Reward: [-663.518 -663.518 -663.518] [175.3174], Avg: [-1135.873 -1135.873 -1135.873] (1.000)
Step: 18749, Reward: [-708.296 -708.296 -708.296] [224.7138], Avg: [-1135.332 -1135.332 -1135.332] (1.000)
Step: 18799, Reward: [-658.511 -658.511 -658.511] [304.4956], Avg: [-1134.873 -1134.873 -1134.873] (1.000)
Step: 18849, Reward: [-702.76 -702.76 -702.76] [215.5685], Avg: [-1134.299 -1134.299 -1134.299] (1.000)
Step: 18899, Reward: [-665.425 -665.425 -665.425] [194.3581], Avg: [-1133.573 -1133.573 -1133.573] (1.000)
Step: 18949, Reward: [-665.909 -665.909 -665.909] [204.0602], Avg: [-1132.877 -1132.877 -1132.877] (1.000)
Step: 18999, Reward: [-585.805 -585.805 -585.805] [93.6947], Avg: [-1131.684 -1131.684 -1131.684] (1.000)
Step: 19049, Reward: [-686.649 -686.649 -686.649] [234.0832], Avg: [-1131.131 -1131.131 -1131.131] (1.000)
Step: 19099, Reward: [-830.803 -830.803 -830.803] [152.3840], Avg: [-1130.743 -1130.743 -1130.743] (1.000)
Step: 19149, Reward: [-680.76 -680.76 -680.76] [166.3734], Avg: [-1130.003 -1130.003 -1130.003] (1.000)
Step: 19199, Reward: [-640.406 -640.406 -640.406] [121.6016], Avg: [-1129.044 -1129.044 -1129.044] (1.000)
Step: 19249, Reward: [-768.004 -768.004 -768.004] [134.7265], Avg: [-1128.457 -1128.457 -1128.457] (1.000)
Step: 19299, Reward: [-733.637 -733.637 -733.637] [99.9333], Avg: [-1127.693 -1127.693 -1127.693] (1.000)
Step: 19349, Reward: [-772.809 -772.809 -772.809] [68.1348], Avg: [-1126.952 -1126.952 -1126.952] (1.000)
Step: 19399, Reward: [-905.815 -905.815 -905.815] [119.6733], Avg: [-1126.69 -1126.69 -1126.69] (1.000)
Step: 19449, Reward: [-1188.625 -1188.625 -1188.625] [366.0268], Avg: [-1127.79 -1127.79 -1127.79] (1.000)
Step: 19499, Reward: [-1453.011 -1453.011 -1453.011] [589.9780], Avg: [-1130.137 -1130.137 -1130.137] (1.000)
Step: 19549, Reward: [-2171.135 -2171.135 -2171.135] [233.8178], Avg: [-1133.397 -1133.397 -1133.397] (1.000)
Step: 19599, Reward: [-1962.176 -1962.176 -1962.176] [313.6094], Avg: [-1136.312 -1136.312 -1136.312] (1.000)
Step: 19649, Reward: [-1833.738 -1833.738 -1833.738] [359.4534], Avg: [-1139.001 -1139.001 -1139.001] (1.000)
Step: 19699, Reward: [-1185.013 -1185.013 -1185.013] [301.9923], Avg: [-1139.884 -1139.884 -1139.884] (1.000)
Step: 19749, Reward: [-1134.998 -1134.998 -1134.998] [403.0495], Avg: [-1140.892 -1140.892 -1140.892] (1.000)
Step: 19799, Reward: [-953.111 -953.111 -953.111] [282.8718], Avg: [-1141.132 -1141.132 -1141.132] (1.000)
Step: 19849, Reward: [-697.981 -697.981 -697.981] [152.9008], Avg: [-1140.401 -1140.401 -1140.401] (1.000)
Step: 19899, Reward: [-676.94 -676.94 -676.94] [123.5565], Avg: [-1139.547 -1139.547 -1139.547] (1.000)
Step: 19949, Reward: [-685.515 -685.515 -685.515] [158.7059], Avg: [-1138.807 -1138.807 -1138.807] (1.000)
Step: 19999, Reward: [-676.333 -676.333 -676.333] [196.4305], Avg: [-1138.142 -1138.142 -1138.142] (1.000)
Step: 20049, Reward: [-548.706 -548.706 -548.706] [94.0248], Avg: [-1136.906 -1136.906 -1136.906] (1.000)
Step: 20099, Reward: [-634.045 -634.045 -634.045] [100.3827], Avg: [-1135.905 -1135.905 -1135.905] (1.000)
Step: 20149, Reward: [-596.992 -596.992 -596.992] [142.5022], Avg: [-1134.922 -1134.922 -1134.922] (1.000)
Step: 20199, Reward: [-659.068 -659.068 -659.068] [101.3841], Avg: [-1133.995 -1133.995 -1133.995] (1.000)
Step: 20249, Reward: [-530.104 -530.104 -530.104] [98.4492], Avg: [-1132.747 -1132.747 -1132.747] (1.000)
Step: 20299, Reward: [-576.304 -576.304 -576.304] [77.8667], Avg: [-1131.568 -1131.568 -1131.568] (1.000)
Step: 20349, Reward: [-479.304 -479.304 -479.304] [32.3238], Avg: [-1130.045 -1130.045 -1130.045] (1.000)
Step: 20399, Reward: [-591.68 -591.68 -591.68] [194.6597], Avg: [-1129.202 -1129.202 -1129.202] (1.000)
Step: 20449, Reward: [-542.786 -542.786 -542.786] [102.4857], Avg: [-1128.019 -1128.019 -1128.019] (1.000)
Step: 20499, Reward: [-612.656 -612.656 -612.656] [107.5562], Avg: [-1127.024 -1127.024 -1127.024] (1.000)
Step: 20549, Reward: [-480.322 -480.322 -480.322] [96.7800], Avg: [-1125.686 -1125.686 -1125.686] (1.000)
Step: 20599, Reward: [-488.082 -488.082 -488.082] [64.1250], Avg: [-1124.295 -1124.295 -1124.295] (1.000)
Step: 20649, Reward: [-554.757 -554.757 -554.757] [132.0287], Avg: [-1123.235 -1123.235 -1123.235] (1.000)
Step: 20699, Reward: [-544.581 -544.581 -544.581] [104.3353], Avg: [-1122.089 -1122.089 -1122.089] (1.000)
Step: 20749, Reward: [-506.188 -506.188 -506.188] [78.5495], Avg: [-1120.795 -1120.795 -1120.795] (1.000)
Step: 20799, Reward: [-426.201 -426.201 -426.201] [54.4207], Avg: [-1119.256 -1119.256 -1119.256] (1.000)
Step: 20849, Reward: [-534.404 -534.404 -534.404] [130.2424], Avg: [-1118.166 -1118.166 -1118.166] (1.000)
Step: 20899, Reward: [-458.13 -458.13 -458.13] [60.0055], Avg: [-1116.73 -1116.73 -1116.73] (1.000)
Step: 20949, Reward: [-461.522 -461.522 -461.522] [43.5117], Avg: [-1115.27 -1115.27 -1115.27] (1.000)
Step: 20999, Reward: [-419.481 -419.481 -419.481] [58.7078], Avg: [-1113.753 -1113.753 -1113.753] (1.000)
Step: 21049, Reward: [-399.283 -399.283 -399.283] [47.1161], Avg: [-1112.168 -1112.168 -1112.168] (1.000)
Step: 21099, Reward: [-448.053 -448.053 -448.053] [66.8112], Avg: [-1110.753 -1110.753 -1110.753] (1.000)
Step: 21149, Reward: [-466.352 -466.352 -466.352] [86.9008], Avg: [-1109.435 -1109.435 -1109.435] (1.000)
Step: 21199, Reward: [-439.385 -439.385 -439.385] [48.2989], Avg: [-1107.968 -1107.968 -1107.968] (1.000)
Step: 21249, Reward: [-460.956 -460.956 -460.956] [83.5512], Avg: [-1106.643 -1106.643 -1106.643] (1.000)
Step: 21299, Reward: [-500.904 -500.904 -500.904] [109.4507], Avg: [-1105.478 -1105.478 -1105.478] (1.000)
Step: 21349, Reward: [-498.102 -498.102 -498.102] [96.1044], Avg: [-1104.28 -1104.28 -1104.28] (1.000)
Step: 21399, Reward: [-411.355 -411.355 -411.355] [45.8519], Avg: [-1102.768 -1102.768 -1102.768] (1.000)
Step: 21449, Reward: [-452.51 -452.51 -452.51] [70.9181], Avg: [-1101.418 -1101.418 -1101.418] (1.000)
Step: 21499, Reward: [-453.618 -453.618 -453.618] [82.8964], Avg: [-1100.104 -1100.104 -1100.104] (1.000)
Step: 21549, Reward: [-506.383 -506.383 -506.383] [68.7716], Avg: [-1098.886 -1098.886 -1098.886] (1.000)
Step: 21599, Reward: [-409.359 -409.359 -409.359] [77.2334], Avg: [-1097.469 -1097.469 -1097.469] (1.000)
Step: 21649, Reward: [-439.229 -439.229 -439.229] [46.7299], Avg: [-1096.057 -1096.057 -1096.057] (1.000)
Step: 21699, Reward: [-438.435 -438.435 -438.435] [42.5703], Avg: [-1094.639 -1094.639 -1094.639] (1.000)
Step: 21749, Reward: [-449.203 -449.203 -449.203] [58.2038], Avg: [-1093.29 -1093.29 -1093.29] (1.000)
Step: 21799, Reward: [-473.434 -473.434 -473.434] [39.7485], Avg: [-1091.959 -1091.959 -1091.959] (1.000)
Step: 21849, Reward: [-422.787 -422.787 -422.787] [31.3180], Avg: [-1090.499 -1090.499 -1090.499] (1.000)
Step: 21899, Reward: [-489.037 -489.037 -489.037] [51.0436], Avg: [-1089.243 -1089.243 -1089.243] (1.000)
Step: 21949, Reward: [-471.199 -471.199 -471.199] [45.9060], Avg: [-1087.939 -1087.939 -1087.939] (1.000)
Step: 21999, Reward: [-408.635 -408.635 -408.635] [67.1862], Avg: [-1086.548 -1086.548 -1086.548] (1.000)
Step: 22049, Reward: [-511.061 -511.061 -511.061] [46.2124], Avg: [-1085.348 -1085.348 -1085.348] (1.000)
Step: 22099, Reward: [-373.786 -373.786 -373.786] [47.9449], Avg: [-1083.847 -1083.847 -1083.847] (1.000)
Step: 22149, Reward: [-408.377 -408.377 -408.377] [118.3803], Avg: [-1082.589 -1082.589 -1082.589] (1.000)
Step: 22199, Reward: [-443.792 -443.792 -443.792] [138.4899], Avg: [-1081.462 -1081.462 -1081.462] (1.000)
Step: 22249, Reward: [-401.645 -401.645 -401.645] [45.7890], Avg: [-1080.038 -1080.038 -1080.038] (1.000)
Step: 22299, Reward: [-460.211 -460.211 -460.211] [60.0415], Avg: [-1078.782 -1078.782 -1078.782] (1.000)
Step: 22349, Reward: [-454.531 -454.531 -454.531] [38.9762], Avg: [-1077.473 -1077.473 -1077.473] (1.000)
Step: 22399, Reward: [-433.52 -433.52 -433.52] [36.9779], Avg: [-1076.118 -1076.118 -1076.118] (1.000)
Step: 22449, Reward: [-434.401 -434.401 -434.401] [14.3453], Avg: [-1074.721 -1074.721 -1074.721] (1.000)
Step: 22499, Reward: [-372.585 -372.585 -372.585] [67.3104], Avg: [-1073.31 -1073.31 -1073.31] (1.000)
Step: 22549, Reward: [-385.44 -385.44 -385.44] [42.2958], Avg: [-1071.879 -1071.879 -1071.879] (1.000)
Step: 22599, Reward: [-458.028 -458.028 -458.028] [85.7841], Avg: [-1070.711 -1070.711 -1070.711] (1.000)
Step: 22649, Reward: [-399.09 -399.09 -399.09] [72.5493], Avg: [-1069.388 -1069.388 -1069.388] (1.000)
Step: 22699, Reward: [-574.793 -574.793 -574.793] [117.2965], Avg: [-1068.557 -1068.557 -1068.557] (1.000)
Step: 22749, Reward: [-414.195 -414.195 -414.195] [38.6443], Avg: [-1067.204 -1067.204 -1067.204] (1.000)
Step: 22799, Reward: [-392.61 -392.61 -392.61] [48.1389], Avg: [-1065.83 -1065.83 -1065.83] (1.000)
Step: 22849, Reward: [-459.41 -459.41 -459.41] [34.9840], Avg: [-1064.58 -1064.58 -1064.58] (1.000)
Step: 22899, Reward: [-352.592 -352.592 -352.592] [43.8830], Avg: [-1063.121 -1063.121 -1063.121] (1.000)
Step: 22949, Reward: [-393.253 -393.253 -393.253] [69.5361], Avg: [-1061.813 -1061.813 -1061.813] (1.000)
Step: 22999, Reward: [-422.514 -422.514 -422.514] [91.0068], Avg: [-1060.621 -1060.621 -1060.621] (1.000)
Step: 23049, Reward: [-417.197 -417.197 -417.197] [77.6838], Avg: [-1059.394 -1059.394 -1059.394] (1.000)
Step: 23099, Reward: [-369.939 -369.939 -369.939] [39.2473], Avg: [-1057.986 -1057.986 -1057.986] (1.000)
Step: 23149, Reward: [-466.019 -466.019 -466.019] [107.1429], Avg: [-1056.939 -1056.939 -1056.939] (1.000)
Step: 23199, Reward: [-484.599 -484.599 -484.599] [85.5576], Avg: [-1055.89 -1055.89 -1055.89] (1.000)
Step: 23249, Reward: [-446.024 -446.024 -446.024] [63.7331], Avg: [-1054.716 -1054.716 -1054.716] (1.000)
Step: 23299, Reward: [-436.452 -436.452 -436.452] [43.0290], Avg: [-1053.481 -1053.481 -1053.481] (1.000)
Step: 23349, Reward: [-416.727 -416.727 -416.727] [33.4500], Avg: [-1052.189 -1052.189 -1052.189] (1.000)
Step: 23399, Reward: [-432.138 -432.138 -432.138] [46.0206], Avg: [-1050.963 -1050.963 -1050.963] (1.000)
Step: 23449, Reward: [-418.955 -418.955 -418.955] [162.5656], Avg: [-1049.962 -1049.962 -1049.962] (1.000)
Step: 23499, Reward: [-424.203 -424.203 -424.203] [91.7506], Avg: [-1048.826 -1048.826 -1048.826] (1.000)
Step: 23549, Reward: [-506.012 -506.012 -506.012] [124.8965], Avg: [-1047.938 -1047.938 -1047.938] (1.000)
Step: 23599, Reward: [-433.939 -433.939 -433.939] [75.3621], Avg: [-1046.797 -1046.797 -1046.797] (1.000)
Step: 23649, Reward: [-407.089 -407.089 -407.089] [24.6785], Avg: [-1045.497 -1045.497 -1045.497] (1.000)
Step: 23699, Reward: [-445.839 -445.839 -445.839] [63.8061], Avg: [-1044.366 -1044.366 -1044.366] (1.000)
Step: 23749, Reward: [-566.274 -566.274 -566.274] [127.1990], Avg: [-1043.628 -1043.628 -1043.628] (1.000)
Step: 23799, Reward: [-379.05 -379.05 -379.05] [96.6037], Avg: [-1042.435 -1042.435 -1042.435] (1.000)
Step: 23849, Reward: [-387.038 -387.038 -387.038] [41.3393], Avg: [-1041.147 -1041.147 -1041.147] (1.000)
Step: 23899, Reward: [-432.384 -432.384 -432.384] [64.2434], Avg: [-1040.008 -1040.008 -1040.008] (1.000)
Step: 23949, Reward: [-452.461 -452.461 -452.461] [52.9434], Avg: [-1038.892 -1038.892 -1038.892] (1.000)
Step: 23999, Reward: [-469.978 -469.978 -469.978] [75.6216], Avg: [-1037.864 -1037.864 -1037.864] (1.000)
Step: 24049, Reward: [-485.597 -485.597 -485.597] [142.8775], Avg: [-1037.013 -1037.013 -1037.013] (1.000)
Step: 24099, Reward: [-446.055 -446.055 -446.055] [59.2442], Avg: [-1035.91 -1035.91 -1035.91] (1.000)
Step: 24149, Reward: [-372.26 -372.26 -372.26] [63.0403], Avg: [-1034.667 -1034.667 -1034.667] (1.000)
Step: 24199, Reward: [-394.941 -394.941 -394.941] [34.4490], Avg: [-1033.416 -1033.416 -1033.416] (1.000)
Step: 24249, Reward: [-353.995 -353.995 -353.995] [29.4244], Avg: [-1032.076 -1032.076 -1032.076] (1.000)
Step: 24299, Reward: [-443.385 -443.385 -443.385] [67.5756], Avg: [-1031.003 -1031.003 -1031.003] (1.000)
Step: 24349, Reward: [-408.283 -408.283 -408.283] [94.1308], Avg: [-1029.918 -1029.918 -1029.918] (1.000)
Step: 24399, Reward: [-423.024 -423.024 -423.024] [65.7219], Avg: [-1028.809 -1028.809 -1028.809] (1.000)
Step: 24449, Reward: [-417.897 -417.897 -417.897] [32.5570], Avg: [-1027.626 -1027.626 -1027.626] (1.000)
Step: 24499, Reward: [-353.97 -353.97 -353.97] [48.0520], Avg: [-1026.35 -1026.35 -1026.35] (1.000)
Step: 24549, Reward: [-509.208 -509.208 -509.208] [106.2775], Avg: [-1025.513 -1025.513 -1025.513] (1.000)
Step: 24599, Reward: [-345.225 -345.225 -345.225] [41.1343], Avg: [-1024.214 -1024.214 -1024.214] (1.000)
Step: 24649, Reward: [-437.14 -437.14 -437.14] [65.8178], Avg: [-1023.156 -1023.156 -1023.156] (1.000)
Step: 24699, Reward: [-468.941 -468.941 -468.941] [85.2551], Avg: [-1022.207 -1022.207 -1022.207] (1.000)
Step: 24749, Reward: [-519.702 -519.702 -519.702] [111.2678], Avg: [-1021.417 -1021.417 -1021.417] (1.000)
Step: 24799, Reward: [-451.358 -451.358 -451.358] [82.0225], Avg: [-1020.433 -1020.433 -1020.433] (1.000)
Step: 24849, Reward: [-482.07 -482.07 -482.07] [136.7677], Avg: [-1019.625 -1019.625 -1019.625] (1.000)
Step: 24899, Reward: [-475.328 -475.328 -475.328] [57.9958], Avg: [-1018.648 -1018.648 -1018.648] (1.000)
Step: 24949, Reward: [-535.174 -535.174 -535.174] [59.6939], Avg: [-1017.799 -1017.799 -1017.799] (1.000)
Step: 24999, Reward: [-543.814 -543.814 -543.814] [111.6487], Avg: [-1017.074 -1017.074 -1017.074] (1.000)
Step: 25049, Reward: [-498.282 -498.282 -498.282] [141.5085], Avg: [-1016.321 -1016.321 -1016.321] (1.000)
Step: 25099, Reward: [-468.718 -468.718 -468.718] [35.7357], Avg: [-1015.302 -1015.302 -1015.302] (1.000)
Step: 25149, Reward: [-425.431 -425.431 -425.431] [34.7069], Avg: [-1014.198 -1014.198 -1014.198] (1.000)
Step: 25199, Reward: [-741.096 -741.096 -741.096] [104.3009], Avg: [-1013.863 -1013.863 -1013.863] (1.000)
Step: 25249, Reward: [-575.261 -575.261 -575.261] [77.8075], Avg: [-1013.149 -1013.149 -1013.149] (1.000)
Step: 25299, Reward: [-867.436 -867.436 -867.436] [84.3177], Avg: [-1013.027 -1013.027 -1013.027] (1.000)
Step: 25349, Reward: [-501.05 -501.05 -501.05] [76.3025], Avg: [-1012.168 -1012.168 -1012.168] (1.000)
Step: 25399, Reward: [-565.715 -565.715 -565.715] [90.7809], Avg: [-1011.468 -1011.468 -1011.468] (1.000)
Step: 25449, Reward: [-569.625 -569.625 -569.625] [73.9000], Avg: [-1010.745 -1010.745 -1010.745] (1.000)
Step: 25499, Reward: [-514.157 -514.157 -514.157] [55.6242], Avg: [-1009.88 -1009.88 -1009.88] (1.000)
Step: 25549, Reward: [-497.044 -497.044 -497.044] [68.1612], Avg: [-1009.01 -1009.01 -1009.01] (1.000)
Step: 25599, Reward: [-485.434 -485.434 -485.434] [73.9694], Avg: [-1008.132 -1008.132 -1008.132] (1.000)
Step: 25649, Reward: [-473.449 -473.449 -473.449] [73.9398], Avg: [-1007.234 -1007.234 -1007.234] (1.000)
Step: 25699, Reward: [-580.709 -580.709 -580.709] [127.6822], Avg: [-1006.652 -1006.652 -1006.652] (1.000)
Step: 25749, Reward: [-454.266 -454.266 -454.266] [71.3998], Avg: [-1005.718 -1005.718 -1005.718] (1.000)
Step: 25799, Reward: [-509.908 -509.908 -509.908] [90.7620], Avg: [-1004.933 -1004.933 -1004.933] (1.000)
Step: 25849, Reward: [-453.969 -453.969 -453.969] [26.7401], Avg: [-1003.919 -1003.919 -1003.919] (1.000)
Step: 25899, Reward: [-518.319 -518.319 -518.319] [65.2522], Avg: [-1003.108 -1003.108 -1003.108] (1.000)
Step: 25949, Reward: [-550.331 -550.331 -550.331] [33.3200], Avg: [-1002.3 -1002.3 -1002.3] (1.000)
Step: 25999, Reward: [-520.279 -520.279 -520.279] [70.4308], Avg: [-1001.508 -1001.508 -1001.508] (1.000)
Step: 26049, Reward: [-415.865 -415.865 -415.865] [74.3481], Avg: [-1000.527 -1000.527 -1000.527] (1.000)
Step: 26099, Reward: [-423.448 -423.448 -423.448] [91.2604], Avg: [-999.596 -999.596 -999.596] (1.000)
Step: 26149, Reward: [-438.307 -438.307 -438.307] [62.6745], Avg: [-998.643 -998.643 -998.643] (1.000)
Step: 26199, Reward: [-454.378 -454.378 -454.378] [100.0598], Avg: [-997.795 -997.795 -997.795] (1.000)
Step: 26249, Reward: [-385.706 -385.706 -385.706] [45.9080], Avg: [-996.717 -996.717 -996.717] (1.000)
Step: 26299, Reward: [-461.433 -461.433 -461.433] [107.6773], Avg: [-995.904 -995.904 -995.904] (1.000)
Step: 26349, Reward: [-472.43 -472.43 -472.43] [100.5462], Avg: [-995.101 -995.101 -995.101] (1.000)
Step: 26399, Reward: [-409.627 -409.627 -409.627] [10.4267], Avg: [-994.012 -994.012 -994.012] (1.000)
Step: 26449, Reward: [-402.682 -402.682 -402.682] [41.9861], Avg: [-992.974 -992.974 -992.974] (1.000)
Step: 26499, Reward: [-440.285 -440.285 -440.285] [61.3863], Avg: [-992.047 -992.047 -992.047] (1.000)
Step: 26549, Reward: [-389.782 -389.782 -389.782] [52.8490], Avg: [-991.012 -991.012 -991.012] (1.000)
Step: 26599, Reward: [-413.362 -413.362 -413.362] [40.6958], Avg: [-990.003 -990.003 -990.003] (1.000)
Step: 26649, Reward: [-464.734 -464.734 -464.734] [92.4478], Avg: [-989.191 -989.191 -989.191] (1.000)
Step: 26699, Reward: [-408.064 -408.064 -408.064] [62.3061], Avg: [-988.219 -988.219 -988.219] (1.000)
Step: 26749, Reward: [-439.015 -439.015 -439.015] [38.8433], Avg: [-987.265 -987.265 -987.265] (1.000)
Step: 26799, Reward: [-437.126 -437.126 -437.126] [103.5392], Avg: [-986.432 -986.432 -986.432] (1.000)
Step: 26849, Reward: [-421.156 -421.156 -421.156] [75.2167], Avg: [-985.519 -985.519 -985.519] (1.000)
Step: 26899, Reward: [-396.645 -396.645 -396.645] [46.4937], Avg: [-984.511 -984.511 -984.511] (1.000)
Step: 26949, Reward: [-448.48 -448.48 -448.48] [33.4203], Avg: [-983.579 -983.579 -983.579] (1.000)
Step: 26999, Reward: [-390.103 -390.103 -390.103] [47.1975], Avg: [-982.567 -982.567 -982.567] (1.000)
Step: 27049, Reward: [-348.54 -348.54 -348.54] [49.6417], Avg: [-981.487 -981.487 -981.487] (1.000)
Step: 27099, Reward: [-417.511 -417.511 -417.511] [102.5954], Avg: [-980.636 -980.636 -980.636] (1.000)
Step: 27149, Reward: [-407.683 -407.683 -407.683] [63.1469], Avg: [-979.697 -979.697 -979.697] (1.000)
Step: 27199, Reward: [-357.058 -357.058 -357.058] [94.4360], Avg: [-978.726 -978.726 -978.726] (1.000)
Step: 27249, Reward: [-426.072 -426.072 -426.072] [77.0701], Avg: [-977.853 -977.853 -977.853] (1.000)
Step: 27299, Reward: [-382.249 -382.249 -382.249] [48.8905], Avg: [-976.852 -976.852 -976.852] (1.000)
Step: 27349, Reward: [-409.942 -409.942 -409.942] [57.1391], Avg: [-975.92 -975.92 -975.92] (1.000)
Step: 27399, Reward: [-390.765 -390.765 -390.765] [28.4558], Avg: [-974.904 -974.904 -974.904] (1.000)
Step: 27449, Reward: [-446.143 -446.143 -446.143] [33.9454], Avg: [-974.003 -974.003 -974.003] (1.000)
Step: 27499, Reward: [-362.495 -362.495 -362.495] [62.4648], Avg: [-973.004 -973.004 -973.004] (1.000)
Step: 27549, Reward: [-419.893 -419.893 -419.893] [135.2221], Avg: [-972.246 -972.246 -972.246] (1.000)
Step: 27599, Reward: [-404.923 -404.923 -404.923] [72.1796], Avg: [-971.349 -971.349 -971.349] (1.000)
Step: 27649, Reward: [-459.183 -459.183 -459.183] [87.3541], Avg: [-970.581 -970.581 -970.581] (1.000)
Step: 27699, Reward: [-422.345 -422.345 -422.345] [49.4054], Avg: [-969.68 -969.68 -969.68] (1.000)
Step: 27749, Reward: [-344.063 -344.063 -344.063] [70.7344], Avg: [-968.681 -968.681 -968.681] (1.000)
Step: 27799, Reward: [-370.006 -370.006 -370.006] [66.1880], Avg: [-967.723 -967.723 -967.723] (1.000)
Step: 27849, Reward: [-460.997 -460.997 -460.997] [62.7023], Avg: [-966.926 -966.926 -966.926] (1.000)
Step: 27899, Reward: [-371.072 -371.072 -371.072] [63.3568], Avg: [-965.971 -965.971 -965.971] (1.000)
Step: 27949, Reward: [-408.098 -408.098 -408.098] [51.1456], Avg: [-965.065 -965.065 -965.065] (1.000)
Step: 27999, Reward: [-446.889 -446.889 -446.889] [40.3710], Avg: [-964.212 -964.212 -964.212] (1.000)
Step: 28049, Reward: [-405.249 -405.249 -405.249] [35.6452], Avg: [-963.279 -963.279 -963.279] (1.000)
Step: 28099, Reward: [-387.185 -387.185 -387.185] [64.4651], Avg: [-962.369 -962.369 -962.369] (1.000)
Step: 28149, Reward: [-434.203 -434.203 -434.203] [55.8906], Avg: [-961.53 -961.53 -961.53] (1.000)
Step: 28199, Reward: [-479.376 -479.376 -479.376] [107.0436], Avg: [-960.865 -960.865 -960.865] (1.000)
Step: 28249, Reward: [-472.702 -472.702 -472.702] [101.3801], Avg: [-960.18 -960.18 -960.18] (1.000)
Step: 28299, Reward: [-434.669 -434.669 -434.669] [46.7734], Avg: [-959.334 -959.334 -959.334] (1.000)
Step: 28349, Reward: [-482.51 -482.51 -482.51] [36.7485], Avg: [-958.558 -958.558 -958.558] (1.000)
Step: 28399, Reward: [-469.364 -469.364 -469.364] [25.5468], Avg: [-957.742 -957.742 -957.742] (1.000)
Step: 28449, Reward: [-394.403 -394.403 -394.403] [75.0237], Avg: [-956.884 -956.884 -956.884] (1.000)
Step: 28499, Reward: [-409.288 -409.288 -409.288] [65.9666], Avg: [-956.039 -956.039 -956.039] (1.000)
Step: 28549, Reward: [-488.006 -488.006 -488.006] [49.5384], Avg: [-955.306 -955.306 -955.306] (1.000)
Step: 28599, Reward: [-399.63 -399.63 -399.63] [96.1908], Avg: [-954.502 -954.502 -954.502] (1.000)
Step: 28649, Reward: [-527.42 -527.42 -527.42] [171.1955], Avg: [-954.056 -954.056 -954.056] (1.000)
Step: 28699, Reward: [-477.165 -477.165 -477.165] [85.9932], Avg: [-953.375 -953.375 -953.375] (1.000)
Step: 28749, Reward: [-436.954 -436.954 -436.954] [117.2635], Avg: [-952.681 -952.681 -952.681] (1.000)
Step: 28799, Reward: [-434.478 -434.478 -434.478] [61.6644], Avg: [-951.888 -951.888 -951.888] (1.000)
Step: 28849, Reward: [-367.203 -367.203 -367.203] [45.3618], Avg: [-950.953 -950.953 -950.953] (1.000)
Step: 28899, Reward: [-395.089 -395.089 -395.089] [88.5581], Avg: [-950.145 -950.145 -950.145] (1.000)
Step: 28949, Reward: [-389.787 -389.787 -389.787] [69.5680], Avg: [-949.297 -949.297 -949.297] (1.000)
Step: 28999, Reward: [-381.561 -381.561 -381.561] [53.1986], Avg: [-948.41 -948.41 -948.41] (1.000)
Step: 29049, Reward: [-377.295 -377.295 -377.295] [74.5281], Avg: [-947.555 -947.555 -947.555] (1.000)
Step: 29099, Reward: [-435.975 -435.975 -435.975] [91.8668], Avg: [-946.834 -946.834 -946.834] (1.000)
Step: 29149, Reward: [-446.143 -446.143 -446.143] [100.7843], Avg: [-946.148 -946.148 -946.148] (1.000)
Step: 29199, Reward: [-427.966 -427.966 -427.966] [133.8460], Avg: [-945.49 -945.49 -945.49] (1.000)
Step: 29249, Reward: [-436.708 -436.708 -436.708] [64.4000], Avg: [-944.73 -944.73 -944.73] (1.000)
Step: 29299, Reward: [-383.573 -383.573 -383.573] [49.4226], Avg: [-943.857 -943.857 -943.857] (1.000)
Step: 29349, Reward: [-449.007 -449.007 -449.007] [103.0491], Avg: [-943.19 -943.19 -943.19] (1.000)
Step: 29399, Reward: [-515.052 -515.052 -515.052] [108.5609], Avg: [-942.646 -942.646 -942.646] (1.000)
Step: 29449, Reward: [-422.37 -422.37 -422.37] [62.0526], Avg: [-941.868 -941.868 -941.868] (1.000)
Step: 29499, Reward: [-506.683 -506.683 -506.683] [119.0943], Avg: [-941.333 -941.333 -941.333] (1.000)
Step: 29549, Reward: [-586.385 -586.385 -586.385] [65.0617], Avg: [-940.842 -940.842 -940.842] (1.000)
Step: 29599, Reward: [-566.842 -566.842 -566.842] [164.3833], Avg: [-940.488 -940.488 -940.488] (1.000)
Step: 29649, Reward: [-569.108 -569.108 -569.108] [78.8996], Avg: [-939.995 -939.995 -939.995] (1.000)
Step: 29699, Reward: [-523.934 -523.934 -523.934] [37.0918], Avg: [-939.357 -939.357 -939.357] (1.000)
Step: 29749, Reward: [-702.362 -702.362 -702.362] [129.5568], Avg: [-939.176 -939.176 -939.176] (1.000)
Step: 29799, Reward: [-741.867 -741.867 -741.867] [130.9704], Avg: [-939.065 -939.065 -939.065] (1.000)
Step: 29849, Reward: [-853.227 -853.227 -853.227] [190.4750], Avg: [-939.24 -939.24 -939.24] (1.000)
Step: 29899, Reward: [-945.731 -945.731 -945.731] [192.5655], Avg: [-939.573 -939.573 -939.573] (1.000)
Step: 29949, Reward: [-1032.314 -1032.314 -1032.314] [212.1039], Avg: [-940.082 -940.082 -940.082] (1.000)
Step: 29999, Reward: [-841.739 -841.739 -841.739] [286.8381], Avg: [-940.396 -940.396 -940.396] (1.000)
Step: 30049, Reward: [-994.502 -994.502 -994.502] [251.5835], Avg: [-940.905 -940.905 -940.905] (1.000)
Step: 30099, Reward: [-987.204 -987.204 -987.204] [130.6129], Avg: [-941.199 -941.199 -941.199] (1.000)
Step: 30149, Reward: [-1002.32 -1002.32 -1002.32] [276.4648], Avg: [-941.758 -941.758 -941.758] (1.000)
Step: 30199, Reward: [-885.404 -885.404 -885.404] [207.1494], Avg: [-942.008 -942.008 -942.008] (1.000)
Step: 30249, Reward: [-833.684 -833.684 -833.684] [211.6198], Avg: [-942.179 -942.179 -942.179] (1.000)
Step: 30299, Reward: [-871.746 -871.746 -871.746] [197.5089], Avg: [-942.389 -942.389 -942.389] (1.000)
Step: 30349, Reward: [-801.359 -801.359 -801.359] [151.9396], Avg: [-942.407 -942.407 -942.407] (1.000)
Step: 30399, Reward: [-777.489 -777.489 -777.489] [176.7763], Avg: [-942.426 -942.426 -942.426] (1.000)
Step: 30449, Reward: [-989.222 -989.222 -989.222] [169.7878], Avg: [-942.782 -942.782 -942.782] (1.000)
Step: 30499, Reward: [-1089.327 -1089.327 -1089.327] [280.6301], Avg: [-943.482 -943.482 -943.482] (1.000)
Step: 30549, Reward: [-1122.853 -1122.853 -1122.853] [292.4338], Avg: [-944.254 -944.254 -944.254] (1.000)
Step: 30599, Reward: [-851.176 -851.176 -851.176] [151.3744], Avg: [-944.349 -944.349 -944.349] (1.000)
Step: 30649, Reward: [-786.817 -786.817 -786.817] [131.4123], Avg: [-944.307 -944.307 -944.307] (1.000)
Step: 30699, Reward: [-940.108 -940.108 -940.108] [161.0444], Avg: [-944.562 -944.562 -944.562] (1.000)
Step: 30749, Reward: [-736.128 -736.128 -736.128] [168.0091], Avg: [-944.497 -944.497 -944.497] (1.000)
Step: 30799, Reward: [-748.108 -748.108 -748.108] [75.3951], Avg: [-944.3 -944.3 -944.3] (1.000)
Step: 30849, Reward: [-714.52 -714.52 -714.52] [136.7469], Avg: [-944.149 -944.149 -944.149] (1.000)
Step: 30899, Reward: [-703.733 -703.733 -703.733] [125.7983], Avg: [-943.964 -943.964 -943.964] (1.000)
Step: 30949, Reward: [-705.969 -705.969 -705.969] [263.8634], Avg: [-944.006 -944.006 -944.006] (1.000)
Step: 30999, Reward: [-619.635 -619.635 -619.635] [61.6082], Avg: [-943.582 -943.582 -943.582] (1.000)
Step: 31049, Reward: [-558.463 -558.463 -558.463] [76.7508], Avg: [-943.085 -943.085 -943.085] (1.000)
Step: 31099, Reward: [-518.978 -518.978 -518.978] [105.9962], Avg: [-942.574 -942.574 -942.574] (1.000)
Step: 31149, Reward: [-537.473 -537.473 -537.473] [79.0685], Avg: [-942.05 -942.05 -942.05] (1.000)
Step: 31199, Reward: [-500.01 -500.01 -500.01] [120.1184], Avg: [-941.535 -941.535 -941.535] (1.000)
Step: 31249, Reward: [-485.044 -485.044 -485.044] [115.7744], Avg: [-940.989 -940.989 -940.989] (1.000)
Step: 31299, Reward: [-454.602 -454.602 -454.602] [87.0738], Avg: [-940.352 -940.352 -940.352] (1.000)
Step: 31349, Reward: [-441.147 -441.147 -441.147] [48.4905], Avg: [-939.633 -939.633 -939.633] (1.000)
Step: 31399, Reward: [-418.213 -418.213 -418.213] [94.4105], Avg: [-938.953 -938.953 -938.953] (1.000)
Step: 31449, Reward: [-383.106 -383.106 -383.106] [64.4749], Avg: [-938.172 -938.172 -938.172] (1.000)
Step: 31499, Reward: [-385.998 -385.998 -385.998] [53.6700], Avg: [-937.38 -937.38 -937.38] (1.000)
Step: 31549, Reward: [-384.169 -384.169 -384.169] [55.4759], Avg: [-936.591 -936.591 -936.591] (1.000)
Step: 31599, Reward: [-409.427 -409.427 -409.427] [63.4032], Avg: [-935.858 -935.858 -935.858] (1.000)
Step: 31649, Reward: [-385.382 -385.382 -385.382] [62.3194], Avg: [-935.087 -935.087 -935.087] (1.000)
Step: 31699, Reward: [-409.832 -409.832 -409.832] [94.4126], Avg: [-934.407 -934.407 -934.407] (1.000)
Step: 31749, Reward: [-390.095 -390.095 -390.095] [72.7859], Avg: [-933.664 -933.664 -933.664] (1.000)
Step: 31799, Reward: [-424.034 -424.034 -424.034] [25.7385], Avg: [-932.904 -932.904 -932.904] (1.000)
Step: 31849, Reward: [-359.872 -359.872 -359.872] [13.0711], Avg: [-932.024 -932.024 -932.024] (1.000)
Step: 31899, Reward: [-464.369 -464.369 -464.369] [85.8444], Avg: [-931.426 -931.426 -931.426] (1.000)
Step: 31949, Reward: [-380.695 -380.695 -380.695] [84.7020], Avg: [-930.697 -930.697 -930.697] (1.000)
Step: 31999, Reward: [-434.737 -434.737 -434.737] [62.1227], Avg: [-930.019 -930.019 -930.019] (1.000)
Step: 32049, Reward: [-373.049 -373.049 -373.049] [52.6942], Avg: [-929.232 -929.232 -929.232] (1.000)
Step: 32099, Reward: [-417.527 -417.527 -417.527] [71.6037], Avg: [-928.547 -928.547 -928.547] (1.000)
Step: 32149, Reward: [-498.74 -498.74 -498.74] [129.1388], Avg: [-928.079 -928.079 -928.079] (1.000)
Step: 32199, Reward: [-501.953 -501.953 -501.953] [41.0569], Avg: [-927.481 -927.481 -927.481] (1.000)
Step: 32249, Reward: [-445.906 -445.906 -445.906] [31.6993], Avg: [-926.784 -926.784 -926.784] (1.000)
Step: 32299, Reward: [-472.637 -472.637 -472.637] [35.4191], Avg: [-926.135 -926.135 -926.135] (1.000)
Step: 32349, Reward: [-425.991 -425.991 -425.991] [70.9074], Avg: [-925.472 -925.472 -925.472] (1.000)
Step: 32399, Reward: [-445.959 -445.959 -445.959] [43.5678], Avg: [-924.799 -924.799 -924.799] (1.000)
Step: 32449, Reward: [-506.41 -506.41 -506.41] [48.8996], Avg: [-924.23 -924.23 -924.23] (1.000)
Step: 32499, Reward: [-499.28 -499.28 -499.28] [81.5227], Avg: [-923.702 -923.702 -923.702] (1.000)
Step: 32549, Reward: [-499.085 -499.085 -499.085] [40.5682], Avg: [-923.112 -923.112 -923.112] (1.000)
Step: 32599, Reward: [-478.57 -478.57 -478.57] [113.4662], Avg: [-922.604 -922.604 -922.604] (1.000)
Step: 32649, Reward: [-525.396 -525.396 -525.396] [86.8000], Avg: [-922.129 -922.129 -922.129] (1.000)
Step: 32699, Reward: [-604.995 -604.995 -604.995] [78.2819], Avg: [-921.763 -921.763 -921.763] (1.000)
Step: 32749, Reward: [-581.325 -581.325 -581.325] [65.2556], Avg: [-921.343 -921.343 -921.343] (1.000)
Step: 32799, Reward: [-601.697 -601.697 -601.697] [83.7059], Avg: [-920.984 -920.984 -920.984] (1.000)
Step: 32849, Reward: [-677.18 -677.18 -677.18] [78.3542], Avg: [-920.732 -920.732 -920.732] (1.000)
Step: 32899, Reward: [-875.389 -875.389 -875.389] [57.7613], Avg: [-920.751 -920.751 -920.751] (1.000)
Step: 32949, Reward: [-982.978 -982.978 -982.978] [73.5564], Avg: [-920.957 -920.957 -920.957] (1.000)
Step: 32999, Reward: [-790.79 -790.79 -790.79] [137.0519], Avg: [-920.967 -920.967 -920.967] (1.000)
Step: 33049, Reward: [-937.675 -937.675 -937.675] [85.0834], Avg: [-921.121 -921.121 -921.121] (1.000)
Step: 33099, Reward: [-981.859 -981.859 -981.859] [58.4403], Avg: [-921.301 -921.301 -921.301] (1.000)
Step: 33149, Reward: [-848.522 -848.522 -848.522] [126.5541], Avg: [-921.382 -921.382 -921.382] (1.000)
Step: 33199, Reward: [-700.203 -700.203 -700.203] [101.8691], Avg: [-921.202 -921.202 -921.202] (1.000)
Step: 33249, Reward: [-625.741 -625.741 -625.741] [89.6989], Avg: [-920.893 -920.893 -920.893] (1.000)
Step: 33299, Reward: [-583.064 -583.064 -583.064] [26.9324], Avg: [-920.426 -920.426 -920.426] (1.000)
Step: 33349, Reward: [-541.775 -541.775 -541.775] [42.1374], Avg: [-919.922 -919.922 -919.922] (1.000)
Step: 33399, Reward: [-505.189 -505.189 -505.189] [53.9976], Avg: [-919.382 -919.382 -919.382] (1.000)
Step: 33449, Reward: [-512.433 -512.433 -512.433] [87.9352], Avg: [-918.905 -918.905 -918.905] (1.000)
Step: 33499, Reward: [-671.202 -671.202 -671.202] [62.1735], Avg: [-918.628 -918.628 -918.628] (1.000)
Step: 33549, Reward: [-597.646 -597.646 -597.646] [41.4684], Avg: [-918.211 -918.211 -918.211] (1.000)
Step: 33599, Reward: [-535.926 -535.926 -535.926] [82.0194], Avg: [-917.765 -917.765 -917.765] (1.000)
Step: 33649, Reward: [-512.324 -512.324 -512.324] [104.8825], Avg: [-917.318 -917.318 -917.318] (1.000)
Step: 33699, Reward: [-486.403 -486.403 -486.403] [65.7368], Avg: [-916.776 -916.776 -916.776] (1.000)
Step: 33749, Reward: [-484.652 -484.652 -484.652] [61.2677], Avg: [-916.227 -916.227 -916.227] (1.000)
Step: 33799, Reward: [-447.944 -447.944 -447.944] [84.3013], Avg: [-915.659 -915.659 -915.659] (1.000)
Step: 33849, Reward: [-478.684 -478.684 -478.684] [67.6996], Avg: [-915.113 -915.113 -915.113] (1.000)
Step: 33899, Reward: [-451.804 -451.804 -451.804] [42.2944], Avg: [-914.492 -914.492 -914.492] (1.000)
Step: 33949, Reward: [-426.918 -426.918 -426.918] [47.1942], Avg: [-913.844 -913.844 -913.844] (1.000)
Step: 33999, Reward: [-485.156 -485.156 -485.156] [72.5993], Avg: [-913.32 -913.32 -913.32] (1.000)
Step: 34049, Reward: [-419.178 -419.178 -419.178] [40.8370], Avg: [-912.654 -912.654 -912.654] (1.000)
Step: 34099, Reward: [-415.622 -415.622 -415.622] [38.1290], Avg: [-911.982 -911.982 -911.982] (1.000)
Step: 34149, Reward: [-445.015 -445.015 -445.015] [87.3082], Avg: [-911.426 -911.426 -911.426] (1.000)
Step: 34199, Reward: [-370.55 -370.55 -370.55] [57.2384], Avg: [-910.719 -910.719 -910.719] (1.000)
Step: 34249, Reward: [-456.836 -456.836 -456.836] [73.3498], Avg: [-910.163 -910.163 -910.163] (1.000)
Step: 34299, Reward: [-397.457 -397.457 -397.457] [69.4564], Avg: [-909.517 -909.517 -909.517] (1.000)
Step: 34349, Reward: [-402.138 -402.138 -402.138] [77.4126], Avg: [-908.891 -908.891 -908.891] (1.000)
Step: 34399, Reward: [-398.671 -398.671 -398.671] [29.5978], Avg: [-908.193 -908.193 -908.193] (1.000)
Step: 34449, Reward: [-402.987 -402.987 -402.987] [14.2757], Avg: [-907.48 -907.48 -907.48] (1.000)
Step: 34499, Reward: [-458.295 -458.295 -458.295] [41.9595], Avg: [-906.89 -906.89 -906.89] (1.000)
Step: 34549, Reward: [-456.592 -456.592 -456.592] [94.8739], Avg: [-906.375 -906.375 -906.375] (1.000)
Step: 34599, Reward: [-434.152 -434.152 -434.152] [102.4337], Avg: [-905.841 -905.841 -905.841] (1.000)
Step: 34649, Reward: [-359.554 -359.554 -359.554] [71.7681], Avg: [-905.156 -905.156 -905.156] (1.000)
Step: 34699, Reward: [-500.606 -500.606 -500.606] [65.0390], Avg: [-904.667 -904.667 -904.667] (1.000)
Step: 34749, Reward: [-402.833 -402.833 -402.833] [38.6702], Avg: [-904.001 -904.001 -904.001] (1.000)
Step: 34799, Reward: [-383.928 -383.928 -383.928] [58.3115], Avg: [-903.337 -903.337 -903.337] (1.000)
Step: 34849, Reward: [-522.552 -522.552 -522.552] [68.7679], Avg: [-902.89 -902.89 -902.89] (1.000)
Step: 34899, Reward: [-433.95 -433.95 -433.95] [59.2979], Avg: [-902.303 -902.303 -902.303] (1.000)
Step: 34949, Reward: [-394.054 -394.054 -394.054] [110.8345], Avg: [-901.734 -901.734 -901.734] (1.000)
Step: 34999, Reward: [-423.365 -423.365 -423.365] [63.1810], Avg: [-901.141 -901.141 -901.141] (1.000)
Step: 35049, Reward: [-381.443 -381.443 -381.443] [58.0010], Avg: [-900.482 -900.482 -900.482] (1.000)
Step: 35099, Reward: [-430.744 -430.744 -430.744] [67.0474], Avg: [-899.909 -899.909 -899.909] (1.000)
Step: 35149, Reward: [-467.085 -467.085 -467.085] [65.6279], Avg: [-899.386 -899.386 -899.386] (1.000)
Step: 35199, Reward: [-431.013 -431.013 -431.013] [71.5759], Avg: [-898.823 -898.823 -898.823] (1.000)
Step: 35249, Reward: [-373.072 -373.072 -373.072] [64.4269], Avg: [-898.168 -898.168 -898.168] (1.000)
Step: 35299, Reward: [-386.518 -386.518 -386.518] [61.5545], Avg: [-897.531 -897.531 -897.531] (1.000)
Step: 35349, Reward: [-374.87 -374.87 -374.87] [34.8737], Avg: [-896.841 -896.841 -896.841] (1.000)
Step: 35399, Reward: [-427.587 -427.587 -427.587] [81.1624], Avg: [-896.293 -896.293 -896.293] (1.000)
Step: 35449, Reward: [-394.114 -394.114 -394.114] [66.7341], Avg: [-895.679 -895.679 -895.679] (1.000)
Step: 35499, Reward: [-440.514 -440.514 -440.514] [99.5982], Avg: [-895.178 -895.178 -895.178] (1.000)
Step: 35549, Reward: [-451.298 -451.298 -451.298] [116.8491], Avg: [-894.718 -894.718 -894.718] (1.000)
Step: 35599, Reward: [-457.158 -457.158 -457.158] [53.4396], Avg: [-894.178 -894.178 -894.178] (1.000)
Step: 35649, Reward: [-431.525 -431.525 -431.525] [42.3767], Avg: [-893.589 -893.589 -893.589] (1.000)
Step: 35699, Reward: [-430.866 -430.866 -430.866] [125.5647], Avg: [-893.117 -893.117 -893.117] (1.000)
Step: 35749, Reward: [-361.681 -361.681 -361.681] [73.1842], Avg: [-892.476 -892.476 -892.476] (1.000)
Step: 35799, Reward: [-398.519 -398.519 -398.519] [21.9710], Avg: [-891.817 -891.817 -891.817] (1.000)
Step: 35849, Reward: [-389.805 -389.805 -389.805] [45.9729], Avg: [-891.181 -891.181 -891.181] (1.000)
Step: 35899, Reward: [-439.031 -439.031 -439.031] [74.6317], Avg: [-890.655 -890.655 -890.655] (1.000)
Step: 35949, Reward: [-423.3 -423.3 -423.3] [61.0552], Avg: [-890.09 -890.09 -890.09] (1.000)
Step: 35999, Reward: [-381.401 -381.401 -381.401] [83.3593], Avg: [-889.499 -889.499 -889.499] (1.000)
Step: 36049, Reward: [-423.854 -423.854 -423.854] [67.2942], Avg: [-888.946 -888.946 -888.946] (1.000)
Step: 36099, Reward: [-414.648 -414.648 -414.648] [42.7187], Avg: [-888.349 -888.349 -888.349] (1.000)
Step: 36149, Reward: [-397.141 -397.141 -397.141] [30.4104], Avg: [-887.711 -887.711 -887.711] (1.000)
Step: 36199, Reward: [-450.544 -450.544 -450.544] [99.2043], Avg: [-887.245 -887.245 -887.245] (1.000)
Step: 36249, Reward: [-459.643 -459.643 -459.643] [73.9873], Avg: [-886.757 -886.757 -886.757] (1.000)
Step: 36299, Reward: [-438.165 -438.165 -438.165] [104.5942], Avg: [-886.283 -886.283 -886.283] (1.000)
Step: 36349, Reward: [-388.119 -388.119 -388.119] [47.8847], Avg: [-885.664 -885.664 -885.664] (1.000)
Step: 36399, Reward: [-361.734 -361.734 -361.734] [56.6737], Avg: [-885.022 -885.022 -885.022] (1.000)
Step: 36449, Reward: [-395.974 -395.974 -395.974] [19.8607], Avg: [-884.378 -884.378 -884.378] (1.000)
Step: 36499, Reward: [-461.412 -461.412 -461.412] [53.8807], Avg: [-883.873 -883.873 -883.873] (1.000)
Step: 36549, Reward: [-406.269 -406.269 -406.269] [44.3147], Avg: [-883.28 -883.28 -883.28] (1.000)
Step: 36599, Reward: [-397.733 -397.733 -397.733] [99.0302], Avg: [-882.752 -882.752 -882.752] (1.000)
Step: 36649, Reward: [-460.992 -460.992 -460.992] [61.6965], Avg: [-882.261 -882.261 -882.261] (1.000)
Step: 36699, Reward: [-462.165 -462.165 -462.165] [49.7478], Avg: [-881.756 -881.756 -881.756] (1.000)
Step: 36749, Reward: [-409.303 -409.303 -409.303] [43.9060], Avg: [-881.173 -881.173 -881.173] (1.000)
Step: 36799, Reward: [-435.727 -435.727 -435.727] [23.6099], Avg: [-880.6 -880.6 -880.6] (1.000)
Step: 36849, Reward: [-473.36 -473.36 -473.36] [57.3696], Avg: [-880.125 -880.125 -880.125] (1.000)
Step: 36899, Reward: [-424.811 -424.811 -424.811] [41.0373], Avg: [-879.564 -879.564 -879.564] (1.000)
Step: 36949, Reward: [-503.083 -503.083 -503.083] [78.4743], Avg: [-879.161 -879.161 -879.161] (1.000)
Step: 36999, Reward: [-464.433 -464.433 -464.433] [57.1699], Avg: [-878.677 -878.677 -878.677] (1.000)
Step: 37049, Reward: [-487.605 -487.605 -487.605] [45.0904], Avg: [-878.21 -878.21 -878.21] (1.000)
Step: 37099, Reward: [-442.069 -442.069 -442.069] [43.8551], Avg: [-877.682 -877.682 -877.682] (1.000)
Step: 37149, Reward: [-485.312 -485.312 -485.312] [56.8131], Avg: [-877.23 -877.23 -877.23] (1.000)
Step: 37199, Reward: [-458.474 -458.474 -458.474] [73.3104], Avg: [-876.766 -876.766 -876.766] (1.000)
Step: 37249, Reward: [-438.236 -438.236 -438.236] [75.2964], Avg: [-876.278 -876.278 -876.278] (1.000)
Step: 37299, Reward: [-426.489 -426.489 -426.489] [33.9781], Avg: [-875.721 -875.721 -875.721] (1.000)
Step: 37349, Reward: [-486.868 -486.868 -486.868] [41.8294], Avg: [-875.256 -875.256 -875.256] (1.000)
Step: 37399, Reward: [-456.91 -456.91 -456.91] [77.0822], Avg: [-874.8 -874.8 -874.8] (1.000)
Step: 37449, Reward: [-423.796 -423.796 -423.796] [56.9961], Avg: [-874.274 -874.274 -874.274] (1.000)
Step: 37499, Reward: [-492.085 -492.085 -492.085] [101.2574], Avg: [-873.899 -873.899 -873.899] (1.000)
Step: 37549, Reward: [-492.944 -492.944 -492.944] [49.1838], Avg: [-873.458 -873.458 -873.458] (1.000)
Step: 37599, Reward: [-441.531 -441.531 -441.531] [51.4744], Avg: [-872.952 -872.952 -872.952] (1.000)
Step: 37649, Reward: [-377.75 -377.75 -377.75] [48.4772], Avg: [-872.359 -872.359 -872.359] (1.000)
Step: 37699, Reward: [-420.107 -420.107 -420.107] [86.0901], Avg: [-871.873 -871.873 -871.873] (1.000)
Step: 37749, Reward: [-442.553 -442.553 -442.553] [57.3731], Avg: [-871.38 -871.38 -871.38] (1.000)
Step: 37799, Reward: [-431.996 -431.996 -431.996] [70.3889], Avg: [-870.892 -870.892 -870.892] (1.000)
Step: 37849, Reward: [-543.964 -543.964 -543.964] [210.8161], Avg: [-870.739 -870.739 -870.739] (1.000)
Step: 37899, Reward: [-419.903 -419.903 -419.903] [38.0213], Avg: [-870.194 -870.194 -870.194] (1.000)
Step: 37949, Reward: [-459.205 -459.205 -459.205] [60.3138], Avg: [-869.732 -869.732 -869.732] (1.000)
Step: 37999, Reward: [-451.187 -451.187 -451.187] [67.2501], Avg: [-869.27 -869.27 -869.27] (1.000)
Step: 38049, Reward: [-436.683 -436.683 -436.683] [56.1833], Avg: [-868.775 -868.775 -868.775] (1.000)
Step: 38099, Reward: [-481.404 -481.404 -481.404] [62.5881], Avg: [-868.349 -868.349 -868.349] (1.000)
Step: 38149, Reward: [-462.956 -462.956 -462.956] [90.9354], Avg: [-867.937 -867.937 -867.937] (1.000)
Step: 38199, Reward: [-592.708 -592.708 -592.708] [244.7935], Avg: [-867.897 -867.897 -867.897] (1.000)
Step: 38249, Reward: [-412.363 -412.363 -412.363] [41.3302], Avg: [-867.356 -867.356 -867.356] (1.000)
Step: 38299, Reward: [-604.108 -604.108 -604.108] [137.8173], Avg: [-867.192 -867.192 -867.192] (1.000)
Step: 38349, Reward: [-459.072 -459.072 -459.072] [41.9546], Avg: [-866.715 -866.715 -866.715] (1.000)
Step: 38399, Reward: [-609.313 -609.313 -609.313] [150.9072], Avg: [-866.576 -866.576 -866.576] (1.000)
Step: 38449, Reward: [-488.644 -488.644 -488.644] [53.5929], Avg: [-866.154 -866.154 -866.154] (1.000)
Step: 38499, Reward: [-606.167 -606.167 -606.167] [154.9559], Avg: [-866.018 -866.018 -866.018] (1.000)
Step: 38549, Reward: [-573.121 -573.121 -573.121] [92.4035], Avg: [-865.758 -865.758 -865.758] (1.000)
Step: 38599, Reward: [-597.941 -597.941 -597.941] [78.6212], Avg: [-865.513 -865.513 -865.513] (1.000)
Step: 38649, Reward: [-491.321 -491.321 -491.321] [131.1055], Avg: [-865.198 -865.198 -865.198] (1.000)
Step: 38699, Reward: [-616.802 -616.802 -616.802] [61.7358], Avg: [-864.957 -864.957 -864.957] (1.000)
Step: 38749, Reward: [-567.294 -567.294 -567.294] [196.6520], Avg: [-864.827 -864.827 -864.827] (1.000)
Step: 38799, Reward: [-610.029 -610.029 -610.029] [76.5650], Avg: [-864.597 -864.597 -864.597] (1.000)
Step: 38849, Reward: [-689.496 -689.496 -689.496] [109.9360], Avg: [-864.513 -864.513 -864.513] (1.000)
Step: 38899, Reward: [-664.886 -664.886 -664.886] [126.6270], Avg: [-864.419 -864.419 -864.419] (1.000)
Step: 38949, Reward: [-577.645 -577.645 -577.645] [62.8820], Avg: [-864.132 -864.132 -864.132] (1.000)
Step: 38999, Reward: [-582.236 -582.236 -582.236] [72.3376], Avg: [-863.863 -863.863 -863.863] (1.000)
Step: 39049, Reward: [-590.165 -590.165 -590.165] [117.9541], Avg: [-863.664 -863.664 -863.664] (1.000)
Step: 39099, Reward: [-615.528 -615.528 -615.528] [107.9989], Avg: [-863.485 -863.485 -863.485] (1.000)
Step: 39149, Reward: [-644.714 -644.714 -644.714] [234.5999], Avg: [-863.505 -863.505 -863.505] (1.000)
Step: 39199, Reward: [-792.947 -792.947 -792.947] [389.2254], Avg: [-863.911 -863.911 -863.911] (1.000)
Step: 39249, Reward: [-729.714 -729.714 -729.714] [239.8858], Avg: [-864.046 -864.046 -864.046] (1.000)
Step: 39299, Reward: [-616.572 -616.572 -616.572] [154.5069], Avg: [-863.928 -863.928 -863.928] (1.000)
Step: 39349, Reward: [-512.857 -512.857 -512.857] [66.2228], Avg: [-863.566 -863.566 -863.566] (1.000)
Step: 39399, Reward: [-804.693 -804.693 -804.693] [294.8375], Avg: [-863.865 -863.865 -863.865] (1.000)
Step: 39449, Reward: [-830.661 -830.661 -830.661] [181.9408], Avg: [-864.054 -864.054 -864.054] (1.000)
Step: 39499, Reward: [-1082.968 -1082.968 -1082.968] [394.5331], Avg: [-864.83 -864.83 -864.83] (1.000)
Step: 39549, Reward: [-833.596 -833.596 -833.596] [191.5230], Avg: [-865.033 -865.033 -865.033] (1.000)
Step: 39599, Reward: [-711.765 -711.765 -711.765] [327.8648], Avg: [-865.253 -865.253 -865.253] (1.000)
Step: 39649, Reward: [-576.355 -576.355 -576.355] [169.2538], Avg: [-865.102 -865.102 -865.102] (1.000)
Step: 39699, Reward: [-836.139 -836.139 -836.139] [245.5523], Avg: [-865.375 -865.375 -865.375] (1.000)
Step: 39749, Reward: [-669.275 -669.275 -669.275] [131.4329], Avg: [-865.294 -865.294 -865.294] (1.000)
Step: 39799, Reward: [-766.074 -766.074 -766.074] [298.5080], Avg: [-865.544 -865.544 -865.544] (1.000)
Step: 39849, Reward: [-744.521 -744.521 -744.521] [319.8624], Avg: [-865.794 -865.794 -865.794] (1.000)
Step: 39899, Reward: [-722.572 -722.572 -722.572] [123.9821], Avg: [-865.769 -865.769 -865.769] (1.000)
Step: 39949, Reward: [-825.446 -825.446 -825.446] [225.6982], Avg: [-866.001 -866.001 -866.001] (1.000)
Step: 39999, Reward: [-681.357 -681.357 -681.357] [185.6529], Avg: [-866.003 -866.003 -866.003] (1.000)
Step: 40049, Reward: [-635.688 -635.688 -635.688] [283.1645], Avg: [-866.069 -866.069 -866.069] (1.000)
Step: 40099, Reward: [-636.374 -636.374 -636.374] [306.5212], Avg: [-866.165 -866.165 -866.165] (1.000)
Step: 40149, Reward: [-789.166 -789.166 -789.166] [281.4630], Avg: [-866.419 -866.419 -866.419] (1.000)
Step: 40199, Reward: [-808.672 -808.672 -808.672] [289.0819], Avg: [-866.707 -866.707 -866.707] (1.000)
Step: 40249, Reward: [-893.594 -893.594 -893.594] [334.5302], Avg: [-867.156 -867.156 -867.156] (1.000)
Step: 40299, Reward: [-656.531 -656.531 -656.531] [177.2207], Avg: [-867.114 -867.114 -867.114] (1.000)
Step: 40349, Reward: [-857.587 -857.587 -857.587] [253.9304], Avg: [-867.417 -867.417 -867.417] (1.000)
Step: 40399, Reward: [-930.549 -930.549 -930.549] [467.1232], Avg: [-868.074 -868.074 -868.074] (1.000)
Step: 40449, Reward: [-621.14 -621.14 -621.14] [96.8744], Avg: [-867.888 -867.888 -867.888] (1.000)
Step: 40499, Reward: [-702.403 -702.403 -702.403] [304.0151], Avg: [-868.059 -868.059 -868.059] (1.000)
Step: 40549, Reward: [-643.57 -643.57 -643.57] [185.7063], Avg: [-868.011 -868.011 -868.011] (1.000)
Step: 40599, Reward: [-509.35 -509.35 -509.35] [45.6132], Avg: [-867.626 -867.626 -867.626] (1.000)
Step: 40649, Reward: [-610.3 -610.3 -610.3] [246.5475], Avg: [-867.612 -867.612 -867.612] (1.000)
Step: 40699, Reward: [-581.158 -581.158 -581.158] [208.8077], Avg: [-867.517 -867.517 -867.517] (1.000)
Step: 40749, Reward: [-603.165 -603.165 -603.165] [243.3964], Avg: [-867.491 -867.491 -867.491] (1.000)
Step: 40799, Reward: [-516.342 -516.342 -516.342] [83.2358], Avg: [-867.163 -867.163 -867.163] (1.000)
Step: 40849, Reward: [-624.842 -624.842 -624.842] [95.4667], Avg: [-866.983 -866.983 -866.983] (1.000)
Step: 40899, Reward: [-610.909 -610.909 -610.909] [155.0842], Avg: [-866.86 -866.86 -866.86] (1.000)
Step: 40949, Reward: [-470.025 -470.025 -470.025] [73.7199], Avg: [-866.465 -866.465 -866.465] (1.000)
Step: 40999, Reward: [-520.733 -520.733 -520.733] [80.4065], Avg: [-866.142 -866.142 -866.142] (1.000)
Step: 41049, Reward: [-506.887 -506.887 -506.887] [29.2944], Avg: [-865.74 -865.74 -865.74] (1.000)
Step: 41099, Reward: [-529.426 -529.426 -529.426] [96.8614], Avg: [-865.449 -865.449 -865.449] (1.000)
Step: 41149, Reward: [-462.478 -462.478 -462.478] [52.4109], Avg: [-865.023 -865.023 -865.023] (1.000)
Step: 41199, Reward: [-478.839 -478.839 -478.839] [107.3807], Avg: [-864.684 -864.684 -864.684] (1.000)
Step: 41249, Reward: [-587.146 -587.146 -587.146] [124.9767], Avg: [-864.499 -864.499 -864.499] (1.000)
Step: 41299, Reward: [-422.182 -422.182 -422.182] [82.7962], Avg: [-864.064 -864.064 -864.064] (1.000)
Step: 41349, Reward: [-624.117 -624.117 -624.117] [81.6680], Avg: [-863.873 -863.873 -863.873] (1.000)
Step: 41399, Reward: [-522.833 -522.833 -522.833] [107.1677], Avg: [-863.59 -863.59 -863.59] (1.000)
Step: 41449, Reward: [-566.856 -566.856 -566.856] [144.8705], Avg: [-863.407 -863.407 -863.407] (1.000)
Step: 41499, Reward: [-456.703 -456.703 -456.703] [76.6063], Avg: [-863.009 -863.009 -863.009] (1.000)
Step: 41549, Reward: [-452.43 -452.43 -452.43] [51.1262], Avg: [-862.577 -862.577 -862.577] (1.000)
Step: 41599, Reward: [-513.765 -513.765 -513.765] [54.1097], Avg: [-862.223 -862.223 -862.223] (1.000)
Step: 41649, Reward: [-443.147 -443.147 -443.147] [79.0792], Avg: [-861.814 -861.814 -861.814] (1.000)
Step: 41699, Reward: [-508.644 -508.644 -508.644] [47.7573], Avg: [-861.448 -861.448 -861.448] (1.000)
Step: 41749, Reward: [-584.362 -584.362 -584.362] [63.7582], Avg: [-861.193 -861.193 -861.193] (1.000)
Step: 41799, Reward: [-557.764 -557.764 -557.764] [96.3033], Avg: [-860.945 -860.945 -860.945] (1.000)
Step: 41849, Reward: [-632.13 -632.13 -632.13] [78.1488], Avg: [-860.765 -860.765 -860.765] (1.000)
Step: 41899, Reward: [-672.762 -672.762 -672.762] [25.3862], Avg: [-860.571 -860.571 -860.571] (1.000)
Step: 41949, Reward: [-817.115 -817.115 -817.115] [76.3299], Avg: [-860.61 -860.61 -860.61] (1.000)
Step: 41999, Reward: [-1025.603 -1025.603 -1025.603] [84.4743], Avg: [-860.907 -860.907 -860.907] (1.000)
Step: 42049, Reward: [-1217.384 -1217.384 -1217.384] [171.5189], Avg: [-861.535 -861.535 -861.535] (1.000)
Step: 42099, Reward: [-1212.334 -1212.334 -1212.334] [149.6943], Avg: [-862.129 -862.129 -862.129] (1.000)
Step: 42149, Reward: [-1284.353 -1284.353 -1284.353] [208.2785], Avg: [-862.877 -862.877 -862.877] (1.000)
Step: 42199, Reward: [-1226.582 -1226.582 -1226.582] [109.2960], Avg: [-863.438 -863.438 -863.438] (1.000)
Step: 42249, Reward: [-1454.686 -1454.686 -1454.686] [266.8195], Avg: [-864.453 -864.453 -864.453] (1.000)
Step: 42299, Reward: [-1314.417 -1314.417 -1314.417] [168.7717], Avg: [-865.184 -865.184 -865.184] (1.000)
Step: 42349, Reward: [-1270.846 -1270.846 -1270.846] [171.1004], Avg: [-865.865 -865.865 -865.865] (1.000)
Step: 42399, Reward: [-1223.51 -1223.51 -1223.51] [128.6764], Avg: [-866.439 -866.439 -866.439] (1.000)
Step: 42449, Reward: [-1137.595 -1137.595 -1137.595] [124.1536], Avg: [-866.904 -866.904 -866.904] (1.000)
Step: 42499, Reward: [-1199.274 -1199.274 -1199.274] [153.0813], Avg: [-867.476 -867.476 -867.476] (1.000)
Step: 42549, Reward: [-1045.507 -1045.507 -1045.507] [77.8278], Avg: [-867.776 -867.776 -867.776] (1.000)
Step: 42599, Reward: [-883.797 -883.797 -883.797] [250.4394], Avg: [-868.089 -868.089 -868.089] (1.000)
Step: 42649, Reward: [-900.002 -900.002 -900.002] [155.8155], Avg: [-868.309 -868.309 -868.309] (1.000)
Step: 42699, Reward: [-962.793 -962.793 -962.793] [121.4574], Avg: [-868.562 -868.562 -868.562] (1.000)
Step: 42749, Reward: [-985.092 -985.092 -985.092] [180.3761], Avg: [-868.909 -868.909 -868.909] (1.000)
Step: 42799, Reward: [-1081.614 -1081.614 -1081.614] [126.2455], Avg: [-869.305 -869.305 -869.305] (1.000)
Step: 42849, Reward: [-1098.345 -1098.345 -1098.345] [128.0622], Avg: [-869.722 -869.722 -869.722] (1.000)
Step: 42899, Reward: [-1005.723 -1005.723 -1005.723] [130.8023], Avg: [-870.033 -870.033 -870.033] (1.000)
Step: 42949, Reward: [-1173.278 -1173.278 -1173.278] [115.5382], Avg: [-870.52 -870.52 -870.52] (1.000)
Step: 42999, Reward: [-834.408 -834.408 -834.408] [115.1645], Avg: [-870.612 -870.612 -870.612] (1.000)
Step: 43049, Reward: [-642.735 -642.735 -642.735] [108.5284], Avg: [-870.474 -870.474 -870.474] (1.000)
Step: 43099, Reward: [-647.644 -647.644 -647.644] [102.9019], Avg: [-870.335 -870.335 -870.335] (1.000)
Step: 43149, Reward: [-609.501 -609.501 -609.501] [54.3177], Avg: [-870.095 -870.095 -870.095] (1.000)
Step: 43199, Reward: [-610.036 -610.036 -610.036] [51.6210], Avg: [-869.854 -869.854 -869.854] (1.000)
Step: 43249, Reward: [-530.09 -530.09 -530.09] [31.5455], Avg: [-869.498 -869.498 -869.498] (1.000)
Step: 43299, Reward: [-528.178 -528.178 -528.178] [39.2172], Avg: [-869.149 -869.149 -869.149] (1.000)
Step: 43349, Reward: [-552.626 -552.626 -552.626] [57.8531], Avg: [-868.85 -868.85 -868.85] (1.000)
Step: 43399, Reward: [-557.139 -557.139 -557.139] [82.3535], Avg: [-868.586 -868.586 -868.586] (1.000)
Step: 43449, Reward: [-519.096 -519.096 -519.096] [59.8573], Avg: [-868.253 -868.253 -868.253] (1.000)
Step: 43499, Reward: [-470.116 -470.116 -470.116] [55.6477], Avg: [-867.859 -867.859 -867.859] (1.000)
Step: 43549, Reward: [-462.993 -462.993 -462.993] [56.0494], Avg: [-867.459 -867.459 -867.459] (1.000)
Step: 43599, Reward: [-392.652 -392.652 -392.652] [30.5525], Avg: [-866.949 -866.949 -866.949] (1.000)
Step: 43649, Reward: [-509.526 -509.526 -509.526] [19.8606], Avg: [-866.563 -866.563 -866.563] (1.000)
Step: 43699, Reward: [-463.612 -463.612 -463.612] [100.9657], Avg: [-866.217 -866.217 -866.217] (1.000)
Step: 43749, Reward: [-465.046 -465.046 -465.046] [67.1952], Avg: [-865.835 -865.835 -865.835] (1.000)
Step: 43799, Reward: [-479.657 -479.657 -479.657] [84.9824], Avg: [-865.492 -865.492 -865.492] (1.000)
Step: 43849, Reward: [-414.816 -414.816 -414.816] [56.0322], Avg: [-865.042 -865.042 -865.042] (1.000)
Step: 43899, Reward: [-446.589 -446.589 -446.589] [79.2302], Avg: [-864.655 -864.655 -864.655] (1.000)
Step: 43949, Reward: [-459.68 -459.68 -459.68] [104.3031], Avg: [-864.313 -864.313 -864.313] (1.000)
Step: 43999, Reward: [-483.681 -483.681 -483.681] [85.9464], Avg: [-863.978 -863.978 -863.978] (1.000)
Step: 44049, Reward: [-406.577 -406.577 -406.577] [36.5955], Avg: [-863.501 -863.501 -863.501] (1.000)
Step: 44099, Reward: [-460.805 -460.805 -460.805] [42.6760], Avg: [-863.093 -863.093 -863.093] (1.000)
Step: 44149, Reward: [-442.274 -442.274 -442.274] [31.5692], Avg: [-862.652 -862.652 -862.652] (1.000)
Step: 44199, Reward: [-433.479 -433.479 -433.479] [66.1172], Avg: [-862.241 -862.241 -862.241] (1.000)
Step: 44249, Reward: [-401.887 -401.887 -401.887] [51.2308], Avg: [-861.779 -861.779 -861.779] (1.000)
Step: 44299, Reward: [-456.517 -456.517 -456.517] [125.8062], Avg: [-861.463 -861.463 -861.463] (1.000)
Step: 44349, Reward: [-375.71 -375.71 -375.71] [47.4913], Avg: [-860.969 -860.969 -860.969] (1.000)
Step: 44399, Reward: [-442.468 -442.468 -442.468] [77.4016], Avg: [-860.585 -860.585 -860.585] (1.000)
Step: 44449, Reward: [-375.286 -375.286 -375.286] [30.4986], Avg: [-860.073 -860.073 -860.073] (1.000)
Step: 44499, Reward: [-394.728 -394.728 -394.728] [44.8185], Avg: [-859.601 -859.601 -859.601] (1.000)
Step: 44549, Reward: [-423.085 -423.085 -423.085] [46.3693], Avg: [-859.163 -859.163 -859.163] (1.000)
Step: 44599, Reward: [-404.728 -404.728 -404.728] [114.8703], Avg: [-858.782 -858.782 -858.782] (1.000)
Step: 44649, Reward: [-405.599 -405.599 -405.599] [53.4916], Avg: [-858.335 -858.335 -858.335] (1.000)
Step: 44699, Reward: [-369.088 -369.088 -369.088] [64.8073], Avg: [-857.86 -857.86 -857.86] (1.000)
Step: 44749, Reward: [-394.644 -394.644 -394.644] [55.8962], Avg: [-857.405 -857.405 -857.405] (1.000)
Step: 44799, Reward: [-476.77 -476.77 -476.77] [97.1628], Avg: [-857.089 -857.089 -857.089] (1.000)
Step: 44849, Reward: [-357.386 -357.386 -357.386] [38.5455], Avg: [-856.574 -856.574 -856.574] (1.000)
Step: 44899, Reward: [-387.226 -387.226 -387.226] [44.9928], Avg: [-856.102 -856.102 -856.102] (1.000)
Step: 44949, Reward: [-401.828 -401.828 -401.828] [93.2950], Avg: [-855.7 -855.7 -855.7] (1.000)
Step: 44999, Reward: [-363.757 -363.757 -363.757] [66.4332], Avg: [-855.228 -855.228 -855.228] (1.000)
Step: 45049, Reward: [-374.618 -374.618 -374.618] [52.3756], Avg: [-854.752 -854.752 -854.752] (1.000)
Step: 45099, Reward: [-438.178 -438.178 -438.178] [38.6120], Avg: [-854.333 -854.333 -854.333] (1.000)
Step: 45149, Reward: [-444.42 -444.42 -444.42] [58.7804], Avg: [-853.944 -853.944 -853.944] (1.000)
Step: 45199, Reward: [-456.727 -456.727 -456.727] [110.4080], Avg: [-853.627 -853.627 -853.627] (1.000)
Step: 45249, Reward: [-451.276 -451.276 -451.276] [66.1378], Avg: [-853.256 -853.256 -853.256] (1.000)
Step: 45299, Reward: [-432.715 -432.715 -432.715] [76.4917], Avg: [-852.876 -852.876 -852.876] (1.000)
Step: 45349, Reward: [-376.99 -376.99 -376.99] [47.5817], Avg: [-852.404 -852.404 -852.404] (1.000)
Step: 45399, Reward: [-408.352 -408.352 -408.352] [43.6089], Avg: [-851.963 -851.963 -851.963] (1.000)
Step: 45449, Reward: [-405.448 -405.448 -405.448] [88.6522], Avg: [-851.569 -851.569 -851.569] (1.000)
Step: 45499, Reward: [-384.834 -384.834 -384.834] [77.6228], Avg: [-851.141 -851.141 -851.141] (1.000)
Step: 45549, Reward: [-401.07 -401.07 -401.07] [31.6418], Avg: [-850.682 -850.682 -850.682] (1.000)
Step: 45599, Reward: [-368.357 -368.357 -368.357] [66.3180], Avg: [-850.226 -850.226 -850.226] (1.000)
Step: 45649, Reward: [-331.929 -331.929 -331.929] [56.0424], Avg: [-849.72 -849.72 -849.72] (1.000)
Step: 45699, Reward: [-397.988 -397.988 -397.988] [77.0602], Avg: [-849.31 -849.31 -849.31] (1.000)
Step: 45749, Reward: [-375.37 -375.37 -375.37] [79.1383], Avg: [-848.878 -848.878 -848.878] (1.000)
Step: 45799, Reward: [-413.903 -413.903 -413.903] [52.0844], Avg: [-848.46 -848.46 -848.46] (1.000)
Step: 45849, Reward: [-417.516 -417.516 -417.516] [93.4464], Avg: [-848.092 -848.092 -848.092] (1.000)
Step: 45899, Reward: [-429.492 -429.492 -429.492] [35.5957], Avg: [-847.675 -847.675 -847.675] (1.000)
Step: 45949, Reward: [-387.949 -387.949 -387.949] [46.7053], Avg: [-847.226 -847.226 -847.226] (1.000)
Step: 45999, Reward: [-366.207 -366.207 -366.207] [67.0700], Avg: [-846.776 -846.776 -846.776] (1.000)
Step: 46049, Reward: [-416.622 -416.622 -416.622] [38.1381], Avg: [-846.35 -846.35 -846.35] (1.000)
Step: 46099, Reward: [-310.537 -310.537 -310.537] [58.0609], Avg: [-845.832 -845.832 -845.832] (1.000)
Step: 46149, Reward: [-405.4 -405.4 -405.4] [46.7418], Avg: [-845.405 -845.405 -845.405] (1.000)
Step: 46199, Reward: [-406.561 -406.561 -406.561] [82.0103], Avg: [-845.019 -845.019 -845.019] (1.000)
Step: 46249, Reward: [-411.689 -411.689 -411.689] [19.4815], Avg: [-844.572 -844.572 -844.572] (1.000)
Step: 46299, Reward: [-415.778 -415.778 -415.778] [87.6511], Avg: [-844.203 -844.203 -844.203] (1.000)
Step: 46349, Reward: [-433.044 -433.044 -433.044] [77.9716], Avg: [-843.844 -843.844 -843.844] (1.000)
Step: 46399, Reward: [-403.119 -403.119 -403.119] [52.4379], Avg: [-843.425 -843.425 -843.425] (1.000)
Step: 46449, Reward: [-424.329 -424.329 -424.329] [92.4588], Avg: [-843.074 -843.074 -843.074] (1.000)
Step: 46499, Reward: [-410.211 -410.211 -410.211] [28.5958], Avg: [-842.639 -842.639 -842.639] (1.000)
Step: 46549, Reward: [-343.197 -343.197 -343.197] [40.2644], Avg: [-842.146 -842.146 -842.146] (1.000)
Step: 46599, Reward: [-441.098 -441.098 -441.098] [66.1987], Avg: [-841.787 -841.787 -841.787] (1.000)
Step: 46649, Reward: [-355.898 -355.898 -355.898] [25.2469], Avg: [-841.293 -841.293 -841.293] (1.000)
Step: 46699, Reward: [-400.185 -400.185 -400.185] [27.0518], Avg: [-840.85 -840.85 -840.85] (1.000)
Step: 46749, Reward: [-403.068 -403.068 -403.068] [73.4833], Avg: [-840.46 -840.46 -840.46] (1.000)
Step: 46799, Reward: [-370.374 -370.374 -370.374] [28.2879], Avg: [-839.988 -839.988 -839.988] (1.000)
Step: 46849, Reward: [-387.016 -387.016 -387.016] [67.3673], Avg: [-839.576 -839.576 -839.576] (1.000)
Step: 46899, Reward: [-345.884 -345.884 -345.884] [72.3386], Avg: [-839.127 -839.127 -839.127] (1.000)
Step: 46949, Reward: [-339.604 -339.604 -339.604] [46.3527], Avg: [-838.645 -838.645 -838.645] (1.000)
Step: 46999, Reward: [-450.574 -450.574 -450.574] [93.9993], Avg: [-838.332 -838.332 -838.332] (1.000)
Step: 47049, Reward: [-442.005 -442.005 -442.005] [72.2999], Avg: [-837.987 -837.987 -837.987] (1.000)
Step: 47099, Reward: [-445.046 -445.046 -445.046] [68.8941], Avg: [-837.643 -837.643 -837.643] (1.000)
Step: 47149, Reward: [-452.324 -452.324 -452.324] [79.4762], Avg: [-837.319 -837.319 -837.319] (1.000)
Step: 47199, Reward: [-389.776 -389.776 -389.776] [103.9366], Avg: [-836.955 -836.955 -836.955] (1.000)
Step: 47249, Reward: [-413.34 -413.34 -413.34] [65.8316], Avg: [-836.577 -836.577 -836.577] (1.000)
Step: 47299, Reward: [-348.731 -348.731 -348.731] [64.7609], Avg: [-836.129 -836.129 -836.129] (1.000)
Step: 47349, Reward: [-365.365 -365.365 -365.365] [48.1844], Avg: [-835.683 -835.683 -835.683] (1.000)
Step: 47399, Reward: [-442.76 -442.76 -442.76] [104.9763], Avg: [-835.379 -835.379 -835.379] (1.000)
Step: 47449, Reward: [-422.556 -422.556 -422.556] [116.7248], Avg: [-835.067 -835.067 -835.067] (1.000)
Step: 47499, Reward: [-459.058 -459.058 -459.058] [56.5160], Avg: [-834.731 -834.731 -834.731] (1.000)
Step: 47549, Reward: [-379.031 -379.031 -379.031] [72.3354], Avg: [-834.328 -834.328 -834.328] (1.000)
Step: 47599, Reward: [-479.913 -479.913 -479.913] [34.4098], Avg: [-833.992 -833.992 -833.992] (1.000)
Step: 47649, Reward: [-392.126 -392.126 -392.126] [107.6484], Avg: [-833.641 -833.641 -833.641] (1.000)
Step: 47699, Reward: [-428.51 -428.51 -428.51] [101.3122], Avg: [-833.323 -833.323 -833.323] (1.000)
Step: 47749, Reward: [-451.018 -451.018 -451.018] [105.2966], Avg: [-833.033 -833.033 -833.033] (1.000)
Step: 47799, Reward: [-356.388 -356.388 -356.388] [76.5824], Avg: [-832.614 -832.614 -832.614] (1.000)
Step: 47849, Reward: [-457.452 -457.452 -457.452] [68.8836], Avg: [-832.294 -832.294 -832.294] (1.000)
Step: 47899, Reward: [-338.378 -338.378 -338.378] [44.4978], Avg: [-831.825 -831.825 -831.825] (1.000)
Step: 47949, Reward: [-431.371 -431.371 -431.371] [73.3816], Avg: [-831.484 -831.484 -831.484] (1.000)
Step: 47999, Reward: [-324.091 -324.091 -324.091] [97.3996], Avg: [-831.057 -831.057 -831.057] (1.000)
Step: 48049, Reward: [-365.379 -365.379 -365.379] [65.6165], Avg: [-830.64 -830.64 -830.64] (1.000)
Step: 48099, Reward: [-417.683 -417.683 -417.683] [59.4519], Avg: [-830.273 -830.273 -830.273] (1.000)
Step: 48149, Reward: [-429.541 -429.541 -429.541] [36.1931], Avg: [-829.894 -829.894 -829.894] (1.000)
Step: 48199, Reward: [-370.637 -370.637 -370.637] [58.8104], Avg: [-829.479 -829.479 -829.479] (1.000)
Step: 48249, Reward: [-374.979 -374.979 -374.979] [55.8758], Avg: [-829.066 -829.066 -829.066] (1.000)
Step: 48299, Reward: [-421.995 -421.995 -421.995] [76.1287], Avg: [-828.723 -828.723 -828.723] (1.000)
Step: 48349, Reward: [-403.669 -403.669 -403.669] [54.4646], Avg: [-828.34 -828.34 -828.34] (1.000)
Step: 48399, Reward: [-418.353 -418.353 -418.353] [97.2670], Avg: [-828.017 -828.017 -828.017] (1.000)
Step: 48449, Reward: [-392.29 -392.29 -392.29] [26.0402], Avg: [-827.594 -827.594 -827.594] (1.000)
Step: 48499, Reward: [-377.166 -377.166 -377.166] [52.3085], Avg: [-827.184 -827.184 -827.184] (1.000)
Step: 48549, Reward: [-441.736 -441.736 -441.736] [81.4535], Avg: [-826.871 -826.871 -826.871] (1.000)
Step: 48599, Reward: [-388.321 -388.321 -388.321] [60.4751], Avg: [-826.482 -826.482 -826.482] (1.000)
Step: 48649, Reward: [-329.446 -329.446 -329.446] [56.6654], Avg: [-826.029 -826.029 -826.029] (1.000)
Step: 48699, Reward: [-416.658 -416.658 -416.658] [57.4293], Avg: [-825.668 -825.668 -825.668] (1.000)
Step: 48749, Reward: [-436.19 -436.19 -436.19] [71.0580], Avg: [-825.341 -825.341 -825.341] (1.000)
Step: 48799, Reward: [-404.682 -404.682 -404.682] [45.2174], Avg: [-824.957 -824.957 -824.957] (1.000)
Step: 48849, Reward: [-415.104 -415.104 -415.104] [54.9181], Avg: [-824.593 -824.593 -824.593] (1.000)
Step: 48899, Reward: [-405.984 -405.984 -405.984] [33.9090], Avg: [-824.2 -824.2 -824.2] (1.000)
Step: 48949, Reward: [-386.452 -386.452 -386.452] [43.5550], Avg: [-823.797 -823.797 -823.797] (1.000)
Step: 48999, Reward: [-376.563 -376.563 -376.563] [66.2537], Avg: [-823.409 -823.409 -823.409] (1.000)
Step: 49049, Reward: [-367.093 -367.093 -367.093] [59.2941], Avg: [-823.004 -823.004 -823.004] (1.000)
Step: 49099, Reward: [-404.638 -404.638 -404.638] [65.3908], Avg: [-822.644 -822.644 -822.644] (1.000)
Step: 49149, Reward: [-366.046 -366.046 -366.046] [82.7309], Avg: [-822.264 -822.264 -822.264] (1.000)
Step: 49199, Reward: [-392.155 -392.155 -392.155] [117.5433], Avg: [-821.946 -821.946 -821.946] (1.000)
Step: 49249, Reward: [-439.156 -439.156 -439.156] [80.1024], Avg: [-821.639 -821.639 -821.639] (1.000)
Step: 49299, Reward: [-386.694 -386.694 -386.694] [81.8970], Avg: [-821.281 -821.281 -821.281] (1.000)
Step: 49349, Reward: [-447.272 -447.272 -447.272] [40.7496], Avg: [-820.943 -820.943 -820.943] (1.000)
Step: 49399, Reward: [-475.714 -475.714 -475.714] [90.3636], Avg: [-820.685 -820.685 -820.685] (1.000)
Step: 49449, Reward: [-417.604 -417.604 -417.604] [50.9641], Avg: [-820.329 -820.329 -820.329] (1.000)
Step: 49499, Reward: [-468.596 -468.596 -468.596] [37.2331], Avg: [-820.012 -820.012 -820.012] (1.000)
Step: 49549, Reward: [-347.242 -347.242 -347.242] [41.0934], Avg: [-819.576 -819.576 -819.576] (1.000)
Step: 49599, Reward: [-405.414 -405.414 -405.414] [81.7976], Avg: [-819.241 -819.241 -819.241] (1.000)
Step: 49649, Reward: [-394.687 -394.687 -394.687] [17.1207], Avg: [-818.831 -818.831 -818.831] (1.000)
Step: 49699, Reward: [-488.715 -488.715 -488.715] [35.4174], Avg: [-818.534 -818.534 -818.534] (1.000)
Step: 49749, Reward: [-446.784 -446.784 -446.784] [48.6460], Avg: [-818.21 -818.21 -818.21] (1.000)
Step: 49799, Reward: [-465.678 -465.678 -465.678] [57.0669], Avg: [-817.913 -817.913 -817.913] (1.000)
Step: 49849, Reward: [-463.261 -463.261 -463.261] [42.9976], Avg: [-817.6 -817.6 -817.6] (1.000)
Step: 49899, Reward: [-421.184 -421.184 -421.184] [31.2021], Avg: [-817.234 -817.234 -817.234] (1.000)
Step: 49949, Reward: [-467.475 -467.475 -467.475] [51.3432], Avg: [-816.936 -816.936 -816.936] (1.000)
Step: 49999, Reward: [-479.291 -479.291 -479.291] [41.2548], Avg: [-816.639 -816.639 -816.639] (1.000)
Step: 50049, Reward: [-478.819 -478.819 -478.819] [60.1837], Avg: [-816.362 -816.362 -816.362] (1.000)
Step: 50099, Reward: [-507.974 -507.974 -507.974] [76.9104], Avg: [-816.131 -816.131 -816.131] (1.000)
Step: 50149, Reward: [-552.535 -552.535 -552.535] [30.7907], Avg: [-815.899 -815.899 -815.899] (1.000)
Step: 50199, Reward: [-524.134 -524.134 -524.134] [77.5403], Avg: [-815.685 -815.685 -815.685] (1.000)
Step: 50249, Reward: [-570.982 -570.982 -570.982] [66.7815], Avg: [-815.508 -815.508 -815.508] (1.000)
Step: 50299, Reward: [-545.326 -545.326 -545.326] [62.5942], Avg: [-815.302 -815.302 -815.302] (1.000)
Step: 50349, Reward: [-601.703 -601.703 -601.703] [79.7693], Avg: [-815.169 -815.169 -815.169] (1.000)
Step: 50399, Reward: [-694.923 -694.923 -694.923] [89.1117], Avg: [-815.138 -815.138 -815.138] (1.000)
Step: 50449, Reward: [-803.549 -803.549 -803.549] [102.0812], Avg: [-815.228 -815.228 -815.228] (1.000)
Step: 50499, Reward: [-801.637 -801.637 -801.637] [74.4325], Avg: [-815.288 -815.288 -815.288] (1.000)
Step: 50549, Reward: [-871.883 -871.883 -871.883] [140.2474], Avg: [-815.483 -815.483 -815.483] (1.000)
Step: 50599, Reward: [-809.382 -809.382 -809.382] [102.0488], Avg: [-815.578 -815.578 -815.578] (1.000)
Step: 50649, Reward: [-899.392 -899.392 -899.392] [173.1076], Avg: [-815.831 -815.831 -815.831] (1.000)
Step: 50699, Reward: [-883.825 -883.825 -883.825] [136.3492], Avg: [-816.033 -816.033 -816.033] (1.000)
Step: 50749, Reward: [-848.783 -848.783 -848.783] [198.7705], Avg: [-816.261 -816.261 -816.261] (1.000)
Step: 50799, Reward: [-930.819 -930.819 -930.819] [172.0382], Avg: [-816.543 -816.543 -816.543] (1.000)
Step: 50849, Reward: [-896.351 -896.351 -896.351] [209.0131], Avg: [-816.827 -816.827 -816.827] (1.000)
Step: 50899, Reward: [-849.471 -849.471 -849.471] [77.5572], Avg: [-816.935 -816.935 -816.935] (1.000)
Step: 50949, Reward: [-677.744 -677.744 -677.744] [64.3928], Avg: [-816.862 -816.862 -816.862] (1.000)
Step: 50999, Reward: [-809.116 -809.116 -809.116] [98.2497], Avg: [-816.951 -816.951 -816.951] (1.000)
Step: 51049, Reward: [-716.095 -716.095 -716.095] [41.8493], Avg: [-816.893 -816.893 -816.893] (1.000)
Step: 51099, Reward: [-672.685 -672.685 -672.685] [57.8631], Avg: [-816.808 -816.808 -816.808] (1.000)
Step: 51149, Reward: [-567.708 -567.708 -567.708] [29.1639], Avg: [-816.593 -816.593 -816.593] (1.000)
Step: 51199, Reward: [-535.515 -535.515 -535.515] [69.3983], Avg: [-816.387 -816.387 -816.387] (1.000)
Step: 51249, Reward: [-521.133 -521.133 -521.133] [59.5501], Avg: [-816.157 -816.157 -816.157] (1.000)
Step: 51299, Reward: [-526.751 -526.751 -526.751] [71.0144], Avg: [-815.944 -815.944 -815.944] (1.000)
Step: 51349, Reward: [-519.245 -519.245 -519.245] [58.2776], Avg: [-815.712 -815.712 -815.712] (1.000)
Step: 51399, Reward: [-486.652 -486.652 -486.652] [21.3487], Avg: [-815.412 -815.412 -815.412] (1.000)
Step: 51449, Reward: [-474.624 -474.624 -474.624] [46.1678], Avg: [-815.126 -815.126 -815.126] (1.000)
Step: 51499, Reward: [-475.857 -475.857 -475.857] [60.6126], Avg: [-814.855 -814.855 -814.855] (1.000)
Step: 51549, Reward: [-501.434 -501.434 -501.434] [81.2123], Avg: [-814.63 -814.63 -814.63] (1.000)
Step: 51599, Reward: [-499.607 -499.607 -499.607] [86.8304], Avg: [-814.409 -814.409 -814.409] (1.000)
Step: 51649, Reward: [-466.043 -466.043 -466.043] [58.2470], Avg: [-814.128 -814.128 -814.128] (1.000)
Step: 51699, Reward: [-477.686 -477.686 -477.686] [83.2659], Avg: [-813.883 -813.883 -813.883] (1.000)
Step: 51749, Reward: [-446.163 -446.163 -446.163] [50.4055], Avg: [-813.577 -813.577 -813.577] (1.000)
Step: 51799, Reward: [-407.647 -407.647 -407.647] [53.1942], Avg: [-813.236 -813.236 -813.236] (1.000)
Step: 51849, Reward: [-375.799 -375.799 -375.799] [55.9116], Avg: [-812.868 -812.868 -812.868] (1.000)
Step: 51899, Reward: [-384.67 -384.67 -384.67] [54.6520], Avg: [-812.509 -812.509 -812.509] (1.000)
Step: 51949, Reward: [-396.264 -396.264 -396.264] [21.2184], Avg: [-812.128 -812.128 -812.128] (1.000)
Step: 51999, Reward: [-396.295 -396.295 -396.295] [56.7860], Avg: [-811.783 -811.783 -811.783] (1.000)
Step: 52049, Reward: [-401.987 -401.987 -401.987] [49.0422], Avg: [-811.437 -811.437 -811.437] (1.000)
Step: 52099, Reward: [-456.527 -456.527 -456.527] [57.4169], Avg: [-811.151 -811.151 -811.151] (1.000)
Step: 52149, Reward: [-370.072 -370.072 -370.072] [39.7530], Avg: [-810.766 -810.766 -810.766] (1.000)
Step: 52199, Reward: [-392.705 -392.705 -392.705] [43.5645], Avg: [-810.408 -810.408 -810.408] (1.000)
Step: 52249, Reward: [-345.667 -345.667 -345.667] [28.9418], Avg: [-809.991 -809.991 -809.991] (1.000)
Step: 52299, Reward: [-429.242 -429.242 -429.242] [47.4433], Avg: [-809.672 -809.672 -809.672] (1.000)
Step: 52349, Reward: [-359.697 -359.697 -359.697] [31.0112], Avg: [-809.272 -809.272 -809.272] (1.000)
Step: 52399, Reward: [-403.155 -403.155 -403.155] [46.9783], Avg: [-808.929 -808.929 -808.929] (1.000)
Step: 52449, Reward: [-379.838 -379.838 -379.838] [25.5011], Avg: [-808.544 -808.544 -808.544] (1.000)
Step: 52499, Reward: [-379.162 -379.162 -379.162] [60.0147], Avg: [-808.193 -808.193 -808.193] (1.000)
Step: 52549, Reward: [-413.73 -413.73 -413.73] [70.0556], Avg: [-807.884 -807.884 -807.884] (1.000)
Step: 52599, Reward: [-373.007 -373.007 -373.007] [87.3940], Avg: [-807.554 -807.554 -807.554] (1.000)
Step: 52649, Reward: [-433.16 -433.16 -433.16] [59.9678], Avg: [-807.255 -807.255 -807.255] (1.000)
Step: 52699, Reward: [-387.236 -387.236 -387.236] [44.1521], Avg: [-806.898 -806.898 -806.898] (1.000)
Step: 52749, Reward: [-358.194 -358.194 -358.194] [47.6310], Avg: [-806.518 -806.518 -806.518] (1.000)
Step: 52799, Reward: [-420.338 -420.338 -420.338] [56.3201], Avg: [-806.206 -806.206 -806.206] (1.000)
Step: 52849, Reward: [-380.272 -380.272 -380.272] [31.3820], Avg: [-805.833 -805.833 -805.833] (1.000)
Step: 52899, Reward: [-386.815 -386.815 -386.815] [56.2014], Avg: [-805.49 -805.49 -805.49] (1.000)
Step: 52949, Reward: [-357.686 -357.686 -357.686] [89.5390], Avg: [-805.151 -805.151 -805.151] (1.000)
Step: 52999, Reward: [-378.836 -378.836 -378.836] [56.6032], Avg: [-804.803 -804.803 -804.803] (1.000)
Step: 53049, Reward: [-364.868 -364.868 -364.868] [42.2443], Avg: [-804.428 -804.428 -804.428] (1.000)
Step: 53099, Reward: [-403.773 -403.773 -403.773] [56.9290], Avg: [-804.104 -804.104 -804.104] (1.000)
Step: 53149, Reward: [-438.017 -438.017 -438.017] [31.7922], Avg: [-803.79 -803.79 -803.79] (1.000)
Step: 53199, Reward: [-418.716 -418.716 -418.716] [52.7276], Avg: [-803.477 -803.477 -803.477] (1.000)
Step: 53249, Reward: [-362.226 -362.226 -362.226] [68.6429], Avg: [-803.127 -803.127 -803.127] (1.000)
Step: 53299, Reward: [-408.208 -408.208 -408.208] [45.4867], Avg: [-802.8 -802.8 -802.8] (1.000)
Step: 53349, Reward: [-414.342 -414.342 -414.342] [33.7319], Avg: [-802.467 -802.467 -802.467] (1.000)
Step: 53399, Reward: [-349.905 -349.905 -349.905] [30.4396], Avg: [-802.072 -802.072 -802.072] (1.000)
Step: 53449, Reward: [-409.633 -409.633 -409.633] [27.7216], Avg: [-801.731 -801.731 -801.731] (1.000)
Step: 53499, Reward: [-416.776 -416.776 -416.776] [78.0112], Avg: [-801.444 -801.444 -801.444] (1.000)
Step: 53549, Reward: [-384.484 -384.484 -384.484] [92.1156], Avg: [-801.141 -801.141 -801.141] (1.000)
Step: 53599, Reward: [-377.841 -377.841 -377.841] [47.2068], Avg: [-800.79 -800.79 -800.79] (1.000)
Step: 53649, Reward: [-427.096 -427.096 -427.096] [69.3015], Avg: [-800.506 -800.506 -800.506] (1.000)
Step: 53699, Reward: [-469.054 -469.054 -469.054] [62.5440], Avg: [-800.256 -800.256 -800.256] (1.000)
Step: 53749, Reward: [-391.737 -391.737 -391.737] [79.7126], Avg: [-799.95 -799.95 -799.95] (1.000)
Step: 53799, Reward: [-405.194 -405.194 -405.194] [60.2458], Avg: [-799.639 -799.639 -799.639] (1.000)
Step: 53849, Reward: [-426.215 -426.215 -426.215] [70.2600], Avg: [-799.357 -799.357 -799.357] (1.000)
Step: 53899, Reward: [-414.989 -414.989 -414.989] [96.2494], Avg: [-799.09 -799.09 -799.09] (1.000)
Step: 53949, Reward: [-367.651 -367.651 -367.651] [62.2237], Avg: [-798.748 -798.748 -798.748] (1.000)
Step: 53999, Reward: [-456.157 -456.157 -456.157] [22.3215], Avg: [-798.451 -798.451 -798.451] (1.000)
Step: 54049, Reward: [-452.051 -452.051 -452.051] [65.0716], Avg: [-798.191 -798.191 -798.191] (1.000)
Step: 54099, Reward: [-385.265 -385.265 -385.265] [52.0072], Avg: [-797.858 -797.858 -797.858] (1.000)
Step: 54149, Reward: [-448.76 -448.76 -448.76] [59.0367], Avg: [-797.59 -797.59 -797.59] (1.000)
Step: 54199, Reward: [-445.881 -445.881 -445.881] [79.4747], Avg: [-797.339 -797.339 -797.339] (1.000)
Step: 54249, Reward: [-396.581 -396.581 -396.581] [73.0988], Avg: [-797.037 -797.037 -797.037] (1.000)
Step: 54299, Reward: [-440.435 -440.435 -440.435] [77.3450], Avg: [-796.779 -796.779 -796.779] (1.000)
Step: 54349, Reward: [-383.222 -383.222 -383.222] [63.6871], Avg: [-796.458 -796.458 -796.458] (1.000)
Step: 54399, Reward: [-376.379 -376.379 -376.379] [41.8929], Avg: [-796.11 -796.11 -796.11] (1.000)
Step: 54449, Reward: [-417.704 -417.704 -417.704] [80.0174], Avg: [-795.836 -795.836 -795.836] (1.000)
Step: 54499, Reward: [-373.415 -373.415 -373.415] [67.1732], Avg: [-795.51 -795.51 -795.51] (1.000)
Step: 54549, Reward: [-393.333 -393.333 -393.333] [90.3275], Avg: [-795.224 -795.224 -795.224] (1.000)
Step: 54599, Reward: [-478.22 -478.22 -478.22] [25.8635], Avg: [-794.958 -794.958 -794.958] (1.000)
Step: 54649, Reward: [-455.753 -455.753 -455.753] [54.5023], Avg: [-794.697 -794.697 -794.697] (1.000)
Step: 54699, Reward: [-392.671 -392.671 -392.671] [45.1655], Avg: [-794.371 -794.371 -794.371] (1.000)
Step: 54749, Reward: [-510.158 -510.158 -510.158] [63.0045], Avg: [-794.169 -794.169 -794.169] (1.000)
Step: 54799, Reward: [-504.403 -504.403 -504.403] [83.6956], Avg: [-793.981 -793.981 -793.981] (1.000)
Step: 54849, Reward: [-435.361 -435.361 -435.361] [32.1603], Avg: [-793.683 -793.683 -793.683] (1.000)
Step: 54899, Reward: [-414.997 -414.997 -414.997] [52.2146], Avg: [-793.386 -793.386 -793.386] (1.000)
Step: 54949, Reward: [-484.243 -484.243 -484.243] [48.2108], Avg: [-793.149 -793.149 -793.149] (1.000)
Step: 54999, Reward: [-435.407 -435.407 -435.407] [21.0001], Avg: [-792.842 -792.842 -792.842] (1.000)
Step: 55049, Reward: [-467.085 -467.085 -467.085] [72.5713], Avg: [-792.612 -792.612 -792.612] (1.000)
Step: 55099, Reward: [-483.839 -483.839 -483.839] [78.8067], Avg: [-792.404 -792.404 -792.404] (1.000)
Step: 55149, Reward: [-464.215 -464.215 -464.215] [51.9935], Avg: [-792.153 -792.153 -792.153] (1.000)
Step: 55199, Reward: [-457.892 -457.892 -457.892] [62.1055], Avg: [-791.907 -791.907 -791.907] (1.000)
Step: 55249, Reward: [-494.624 -494.624 -494.624] [59.6806], Avg: [-791.692 -791.692 -791.692] (1.000)
Step: 55299, Reward: [-455.985 -455.985 -455.985] [53.6615], Avg: [-791.437 -791.437 -791.437] (1.000)
Step: 55349, Reward: [-501.609 -501.609 -501.609] [46.5618], Avg: [-791.217 -791.217 -791.217] (1.000)
Step: 55399, Reward: [-522.591 -522.591 -522.591] [28.1159], Avg: [-791. -791. -791.] (1.000)
Step: 55449, Reward: [-455.712 -455.712 -455.712] [75.2208], Avg: [-790.766 -790.766 -790.766] (1.000)
Step: 55499, Reward: [-435.431 -435.431 -435.431] [52.3792], Avg: [-790.493 -790.493 -790.493] (1.000)
Step: 55549, Reward: [-447.127 -447.127 -447.127] [65.1934], Avg: [-790.242 -790.242 -790.242] (1.000)
Step: 55599, Reward: [-457.168 -457.168 -457.168] [10.4204], Avg: [-789.952 -789.952 -789.952] (1.000)
Step: 55649, Reward: [-452.454 -452.454 -452.454] [23.9062], Avg: [-789.67 -789.67 -789.67] (1.000)
Step: 55699, Reward: [-449.089 -449.089 -449.089] [40.1726], Avg: [-789.401 -789.401 -789.401] (1.000)
Step: 55749, Reward: [-436.056 -436.056 -436.056] [55.6896], Avg: [-789.134 -789.134 -789.134] (1.000)
Step: 55799, Reward: [-402.667 -402.667 -402.667] [53.1736], Avg: [-788.835 -788.835 -788.835] (1.000)
Step: 55849, Reward: [-422.567 -422.567 -422.567] [58.1797], Avg: [-788.559 -788.559 -788.559] (1.000)
Step: 55899, Reward: [-420.806 -420.806 -420.806] [86.8698], Avg: [-788.308 -788.308 -788.308] (1.000)
Step: 55949, Reward: [-383.491 -383.491 -383.491] [43.1595], Avg: [-787.985 -787.985 -787.985] (1.000)
Step: 55999, Reward: [-453.35 -453.35 -453.35] [121.1891], Avg: [-787.794 -787.794 -787.794] (1.000)
Step: 56049, Reward: [-417.238 -417.238 -417.238] [41.4140], Avg: [-787.501 -787.501 -787.501] (1.000)
Step: 56099, Reward: [-396.637 -396.637 -396.637] [45.0294], Avg: [-787.192 -787.192 -787.192] (1.000)
Step: 56149, Reward: [-424.697 -424.697 -424.697] [53.2858], Avg: [-786.917 -786.917 -786.917] (1.000)
Step: 56199, Reward: [-447.433 -447.433 -447.433] [52.7383], Avg: [-786.662 -786.662 -786.662] (1.000)
Step: 56249, Reward: [-408.794 -408.794 -408.794] [112.3239], Avg: [-786.426 -786.426 -786.426] (1.000)
Step: 56299, Reward: [-366.048 -366.048 -366.048] [79.6434], Avg: [-786.123 -786.123 -786.123] (1.000)
Step: 56349, Reward: [-401.321 -401.321 -401.321] [48.6260], Avg: [-785.825 -785.825 -785.825] (1.000)
Step: 56399, Reward: [-421.442 -421.442 -421.442] [104.5427], Avg: [-785.595 -785.595 -785.595] (1.000)
Step: 56449, Reward: [-341.708 -341.708 -341.708] [36.8416], Avg: [-785.234 -785.234 -785.234] (1.000)
Step: 56499, Reward: [-420.13 -420.13 -420.13] [58.5883], Avg: [-784.963 -784.963 -784.963] (1.000)
Step: 56549, Reward: [-393.974 -393.974 -393.974] [56.7672], Avg: [-784.667 -784.667 -784.667] (1.000)
Step: 56599, Reward: [-414.058 -414.058 -414.058] [47.3715], Avg: [-784.382 -784.382 -784.382] (1.000)
Step: 56649, Reward: [-440.634 -440.634 -440.634] [117.9994], Avg: [-784.182 -784.182 -784.182] (1.000)
Step: 56699, Reward: [-377.164 -377.164 -377.164] [64.5269], Avg: [-783.88 -783.88 -783.88] (1.000)
Step: 56749, Reward: [-473.02 -473.02 -473.02] [77.5930], Avg: [-783.675 -783.675 -783.675] (1.000)
Step: 56799, Reward: [-422.964 -422.964 -422.964] [52.4413], Avg: [-783.404 -783.404 -783.404] (1.000)
Step: 56849, Reward: [-463.48 -463.48 -463.48] [51.1595], Avg: [-783.167 -783.167 -783.167] (1.000)
Step: 56899, Reward: [-516.075 -516.075 -516.075] [63.9417], Avg: [-782.989 -782.989 -782.989] (1.000)
Step: 56949, Reward: [-543.87 -543.87 -543.87] [26.7273], Avg: [-782.802 -782.802 -782.802] (1.000)
Step: 56999, Reward: [-488.644 -488.644 -488.644] [80.9232], Avg: [-782.615 -782.615 -782.615] (1.000)
Step: 57049, Reward: [-536.877 -536.877 -536.877] [52.2439], Avg: [-782.446 -782.446 -782.446] (1.000)
Step: 57099, Reward: [-570.823 -570.823 -570.823] [63.4483], Avg: [-782.316 -782.316 -782.316] (1.000)
Step: 57149, Reward: [-570.682 -570.682 -570.682] [87.4989], Avg: [-782.207 -782.207 -782.207] (1.000)
Step: 57199, Reward: [-574.819 -574.819 -574.819] [92.2286], Avg: [-782.107 -782.107 -782.107] (1.000)
Step: 57249, Reward: [-458.939 -458.939 -458.939] [28.6315], Avg: [-781.849 -781.849 -781.849] (1.000)
Step: 57299, Reward: [-588.534 -588.534 -588.534] [60.2485], Avg: [-781.733 -781.733 -781.733] (1.000)
Step: 57349, Reward: [-588.857 -588.857 -588.857] [121.8704], Avg: [-781.671 -781.671 -781.671] (1.000)
Step: 57399, Reward: [-618.794 -618.794 -618.794] [100.9408], Avg: [-781.617 -781.617 -781.617] (1.000)
Step: 57449, Reward: [-777.572 -777.572 -777.572] [128.5885], Avg: [-781.726 -781.726 -781.726] (1.000)
Step: 57499, Reward: [-618.519 -618.519 -618.519] [64.8063], Avg: [-781.64 -781.64 -781.64] (1.000)
Step: 57549, Reward: [-596.206 -596.206 -596.206] [101.7017], Avg: [-781.567 -781.567 -781.567] (1.000)
Step: 57599, Reward: [-541.949 -541.949 -541.949] [56.5933], Avg: [-781.409 -781.409 -781.409] (1.000)
Step: 57649, Reward: [-498.137 -498.137 -498.137] [68.6705], Avg: [-781.222 -781.222 -781.222] (1.000)
Step: 57699, Reward: [-503.458 -503.458 -503.458] [73.7190], Avg: [-781.046 -781.046 -781.046] (1.000)
Step: 57749, Reward: [-518.136 -518.136 -518.136] [69.1026], Avg: [-780.878 -780.878 -780.878] (1.000)
Step: 57799, Reward: [-518.434 -518.434 -518.434] [31.3830], Avg: [-780.678 -780.678 -780.678] (1.000)
Step: 57849, Reward: [-388.499 -388.499 -388.499] [47.9506], Avg: [-780.38 -780.38 -780.38] (1.000)
Step: 57899, Reward: [-506.651 -506.651 -506.651] [145.3173], Avg: [-780.27 -780.27 -780.27] (1.000)
Step: 57949, Reward: [-518.019 -518.019 -518.019] [74.4390], Avg: [-780.107 -780.107 -780.107] (1.000)
Step: 57999, Reward: [-533.126 -533.126 -533.126] [106.7803], Avg: [-779.987 -779.987 -779.987] (1.000)
Step: 58049, Reward: [-515.976 -515.976 -515.976] [55.7832], Avg: [-779.807 -779.807 -779.807] (1.000)
Step: 58099, Reward: [-493.432 -493.432 -493.432] [72.3922], Avg: [-779.623 -779.623 -779.623] (1.000)
Step: 58149, Reward: [-573.513 -573.513 -573.513] [70.6306], Avg: [-779.507 -779.507 -779.507] (1.000)
Step: 58199, Reward: [-582.925 -582.925 -582.925] [118.7314], Avg: [-779.44 -779.44 -779.44] (1.000)
Step: 58249, Reward: [-648.262 -648.262 -648.262] [100.8434], Avg: [-779.414 -779.414 -779.414] (1.000)
Step: 58299, Reward: [-686.89 -686.89 -686.89] [95.0489], Avg: [-779.416 -779.416 -779.416] (1.000)
Step: 58349, Reward: [-621.624 -621.624 -621.624] [64.7276], Avg: [-779.336 -779.336 -779.336] (1.000)
Step: 58399, Reward: [-648.869 -648.869 -648.869] [88.5135], Avg: [-779.3 -779.3 -779.3] (1.000)
Step: 58449, Reward: [-562.038 -562.038 -562.038] [92.5033], Avg: [-779.193 -779.193 -779.193] (1.000)
Step: 58499, Reward: [-677.012 -677.012 -677.012] [67.5902], Avg: [-779.164 -779.164 -779.164] (1.000)
Step: 58549, Reward: [-735.739 -735.739 -735.739] [92.9228], Avg: [-779.206 -779.206 -779.206] (1.000)
Step: 58599, Reward: [-682.478 -682.478 -682.478] [24.6529], Avg: [-779.145 -779.145 -779.145] (1.000)
Step: 58649, Reward: [-782.015 -782.015 -782.015] [86.3231], Avg: [-779.221 -779.221 -779.221] (1.000)
Step: 58699, Reward: [-702.287 -702.287 -702.287] [60.7954], Avg: [-779.207 -779.207 -779.207] (1.000)
Step: 58749, Reward: [-843.471 -843.471 -843.471] [129.3032], Avg: [-779.372 -779.372 -779.372] (1.000)
Step: 58799, Reward: [-583.308 -583.308 -583.308] [46.4905], Avg: [-779.245 -779.245 -779.245] (1.000)
Step: 58849, Reward: [-676.929 -676.929 -676.929] [61.1582], Avg: [-779.21 -779.21 -779.21] (1.000)
Step: 58899, Reward: [-678.469 -678.469 -678.469] [124.1853], Avg: [-779.229 -779.229 -779.229] (1.000)
Step: 58949, Reward: [-833.427 -833.427 -833.427] [44.9897], Avg: [-779.314 -779.314 -779.314] (1.000)
Step: 58999, Reward: [-1059.826 -1059.826 -1059.826] [95.5043], Avg: [-779.632 -779.632 -779.632] (1.000)
Step: 59049, Reward: [-925.225 -925.225 -925.225] [82.4711], Avg: [-779.825 -779.825 -779.825] (1.000)
Step: 59099, Reward: [-1040.114 -1040.114 -1040.114] [88.1276], Avg: [-780.12 -780.12 -780.12] (1.000)
Step: 59149, Reward: [-909.534 -909.534 -909.534] [59.3288], Avg: [-780.28 -780.28 -780.28] (1.000)
Step: 59199, Reward: [-814.643 -814.643 -814.643] [81.3622], Avg: [-780.377 -780.377 -780.377] (1.000)
Step: 59249, Reward: [-676.185 -676.185 -676.185] [79.2617], Avg: [-780.356 -780.356 -780.356] (1.000)
Step: 59299, Reward: [-633.01 -633.01 -633.01] [46.0000], Avg: [-780.271 -780.271 -780.271] (1.000)
Step: 59349, Reward: [-664.234 -664.234 -664.234] [61.0503], Avg: [-780.225 -780.225 -780.225] (1.000)
Step: 59399, Reward: [-639.669 -639.669 -639.669] [56.8345], Avg: [-780.154 -780.154 -780.154] (1.000)
Step: 59449, Reward: [-648.481 -648.481 -648.481] [54.3804], Avg: [-780.089 -780.089 -780.089] (1.000)
Step: 59499, Reward: [-642.955 -642.955 -642.955] [88.7360], Avg: [-780.048 -780.048 -780.048] (1.000)
Step: 59549, Reward: [-571.531 -571.531 -571.531] [57.6586], Avg: [-779.922 -779.922 -779.922] (1.000)
Step: 59599, Reward: [-541.373 -541.373 -541.373] [80.2035], Avg: [-779.789 -779.789 -779.789] (1.000)
Step: 59649, Reward: [-532.106 -532.106 -532.106] [44.0206], Avg: [-779.618 -779.618 -779.618] (1.000)
Step: 59699, Reward: [-445.795 -445.795 -445.795] [60.8162], Avg: [-779.39 -779.39 -779.39] (1.000)
Step: 59749, Reward: [-556.927 -556.927 -556.927] [33.3527], Avg: [-779.231 -779.231 -779.231] (1.000)
Step: 59799, Reward: [-566.589 -566.589 -566.589] [81.6507], Avg: [-779.122 -779.122 -779.122] (1.000)
Step: 59849, Reward: [-465.692 -465.692 -465.692] [53.4358], Avg: [-778.905 -778.905 -778.905] (1.000)
Step: 59899, Reward: [-527.219 -527.219 -527.219] [36.0997], Avg: [-778.725 -778.725 -778.725] (1.000)
Step: 59949, Reward: [-500.221 -500.221 -500.221] [75.3347], Avg: [-778.555 -778.555 -778.555] (1.000)
Step: 59999, Reward: [-463.843 -463.843 -463.843] [86.5492], Avg: [-778.365 -778.365 -778.365] (1.000)
Step: 60049, Reward: [-537.012 -537.012 -537.012] [121.6322], Avg: [-778.265 -778.265 -778.265] (1.000)
Step: 60099, Reward: [-449.978 -449.978 -449.978] [25.4649], Avg: [-778.013 -778.013 -778.013] (1.000)
Step: 60149, Reward: [-482.848 -482.848 -482.848] [71.9032], Avg: [-777.828 -777.828 -777.828] (1.000)
Step: 60199, Reward: [-426.124 -426.124 -426.124] [58.5876], Avg: [-777.584 -777.584 -777.584] (1.000)
Step: 60249, Reward: [-468.645 -468.645 -468.645] [54.7349], Avg: [-777.373 -777.373 -777.373] (1.000)
Step: 60299, Reward: [-466.961 -466.961 -466.961] [43.5089], Avg: [-777.152 -777.152 -777.152] (1.000)
Step: 60349, Reward: [-485.726 -485.726 -485.726] [59.1578], Avg: [-776.96 -776.96 -776.96] (1.000)
Step: 60399, Reward: [-527.837 -527.837 -527.837] [17.7084], Avg: [-776.768 -776.768 -776.768] (1.000)
Step: 60449, Reward: [-503.572 -503.572 -503.572] [46.9268], Avg: [-776.581 -776.581 -776.581] (1.000)
Step: 60499, Reward: [-461.605 -461.605 -461.605] [69.1848], Avg: [-776.378 -776.378 -776.378] (1.000)
Step: 60549, Reward: [-421.999 -421.999 -421.999] [89.8382], Avg: [-776.159 -776.159 -776.159] (1.000)
Step: 60599, Reward: [-514.695 -514.695 -514.695] [98.1374], Avg: [-776.025 -776.025 -776.025] (1.000)
Step: 60649, Reward: [-496.734 -496.734 -496.734] [13.3889], Avg: [-775.805 -775.805 -775.805] (1.000)
Step: 60699, Reward: [-473.633 -473.633 -473.633] [37.9631], Avg: [-775.588 -775.588 -775.588] (1.000)
Step: 60749, Reward: [-459.093 -459.093 -459.093] [27.5081], Avg: [-775.35 -775.35 -775.35] (1.000)
Step: 60799, Reward: [-536.551 -536.551 -536.551] [71.8140], Avg: [-775.213 -775.213 -775.213] (1.000)
Step: 60849, Reward: [-670.32 -670.32 -670.32] [72.7582], Avg: [-775.186 -775.186 -775.186] (1.000)
Step: 60899, Reward: [-654.916 -654.916 -654.916] [98.0367], Avg: [-775.168 -775.168 -775.168] (1.000)
Step: 60949, Reward: [-621.79 -621.79 -621.79] [49.5722], Avg: [-775.083 -775.083 -775.083] (1.000)
Step: 60999, Reward: [-670.469 -670.469 -670.469] [127.9285], Avg: [-775.102 -775.102 -775.102] (1.000)
Step: 61049, Reward: [-577.288 -577.288 -577.288] [46.9610], Avg: [-774.978 -774.978 -774.978] (1.000)
Step: 61099, Reward: [-693.642 -693.642 -693.642] [67.6546], Avg: [-774.967 -774.967 -774.967] (1.000)
Step: 61149, Reward: [-854.87 -854.87 -854.87] [168.0103], Avg: [-775.17 -775.17 -775.17] (1.000)
Step: 61199, Reward: [-788.57 -788.57 -788.57] [89.8783], Avg: [-775.254 -775.254 -775.254] (1.000)
Step: 61249, Reward: [-878.587 -878.587 -878.587] [119.7713], Avg: [-775.436 -775.436 -775.436] (1.000)
Step: 61299, Reward: [-789.64 -789.64 -789.64] [83.4693], Avg: [-775.516 -775.516 -775.516] (1.000)
Step: 61349, Reward: [-793.2 -793.2 -793.2] [114.7913], Avg: [-775.624 -775.624 -775.624] (1.000)
Step: 61399, Reward: [-697.622 -697.622 -697.622] [41.2277], Avg: [-775.594 -775.594 -775.594] (1.000)
Step: 61449, Reward: [-786.238 -786.238 -786.238] [70.4276], Avg: [-775.66 -775.66 -775.66] (1.000)
Step: 61499, Reward: [-676.258 -676.258 -676.258] [63.9332], Avg: [-775.631 -775.631 -775.631] (1.000)
Step: 61549, Reward: [-613.083 -613.083 -613.083] [106.9159], Avg: [-775.586 -775.586 -775.586] (1.000)
Step: 61599, Reward: [-570.419 -570.419 -570.419] [58.5213], Avg: [-775.467 -775.467 -775.467] (1.000)
Step: 61649, Reward: [-581.687 -581.687 -581.687] [68.3435], Avg: [-775.365 -775.365 -775.365] (1.000)
Step: 61699, Reward: [-525.6 -525.6 -525.6] [73.5899], Avg: [-775.222 -775.222 -775.222] (1.000)
Step: 61749, Reward: [-523.796 -523.796 -523.796] [67.5516], Avg: [-775.074 -775.074 -775.074] (1.000)
Step: 61799, Reward: [-476.564 -476.564 -476.564] [64.3091], Avg: [-774.884 -774.884 -774.884] (1.000)
Step: 61849, Reward: [-494.913 -494.913 -494.913] [52.6147], Avg: [-774.7 -774.7 -774.7] (1.000)
Step: 61899, Reward: [-456.002 -456.002 -456.002] [31.5202], Avg: [-774.468 -774.468 -774.468] (1.000)
Step: 61949, Reward: [-451.609 -451.609 -451.609] [43.7315], Avg: [-774.243 -774.243 -774.243] (1.000)
Step: 61999, Reward: [-428.128 -428.128 -428.128] [86.8036], Avg: [-774.034 -774.034 -774.034] (1.000)
Step: 62049, Reward: [-484.223 -484.223 -484.223] [50.5245], Avg: [-773.841 -773.841 -773.841] (1.000)
Step: 62099, Reward: [-465.047 -465.047 -465.047] [77.0613], Avg: [-773.655 -773.655 -773.655] (1.000)
Step: 62149, Reward: [-415.823 -415.823 -415.823] [62.7395], Avg: [-773.417 -773.417 -773.417] (1.000)
Step: 62199, Reward: [-457.867 -457.867 -457.867] [69.5717], Avg: [-773.219 -773.219 -773.219] (1.000)
Step: 62249, Reward: [-493.854 -493.854 -493.854] [67.2391], Avg: [-773.049 -773.049 -773.049] (1.000)
Step: 62299, Reward: [-457.706 -457.706 -457.706] [44.8023], Avg: [-772.832 -772.832 -772.832] (1.000)
Step: 62349, Reward: [-397.383 -397.383 -397.383] [61.1540], Avg: [-772.58 -772.58 -772.58] (1.000)
Step: 62399, Reward: [-462.933 -462.933 -462.933] [88.2784], Avg: [-772.402 -772.402 -772.402] (1.000)
Step: 62449, Reward: [-431.562 -431.562 -431.562] [66.1453], Avg: [-772.183 -772.183 -772.183] (1.000)
Step: 62499, Reward: [-442.227 -442.227 -442.227] [18.7211], Avg: [-771.934 -771.934 -771.934] (1.000)
Step: 62549, Reward: [-474.283 -474.283 -474.283] [45.5081], Avg: [-771.732 -771.732 -771.732] (1.000)
Step: 62599, Reward: [-448.21 -448.21 -448.21] [36.2612], Avg: [-771.503 -771.503 -771.503] (1.000)
Step: 62649, Reward: [-447.104 -447.104 -447.104] [65.1200], Avg: [-771.296 -771.296 -771.296] (1.000)
Step: 62699, Reward: [-446.608 -446.608 -446.608] [74.9146], Avg: [-771.096 -771.096 -771.096] (1.000)
Step: 62749, Reward: [-431.53 -431.53 -431.53] [85.4031], Avg: [-770.894 -770.894 -770.894] (1.000)
Step: 62799, Reward: [-437.7 -437.7 -437.7] [67.8531], Avg: [-770.683 -770.683 -770.683] (1.000)
Step: 62849, Reward: [-369.94 -369.94 -369.94] [51.7274], Avg: [-770.405 -770.405 -770.405] (1.000)
Step: 62899, Reward: [-447.095 -447.095 -447.095] [45.5458], Avg: [-770.184 -770.184 -770.184] (1.000)
Step: 62949, Reward: [-473.751 -473.751 -473.751] [61.5332], Avg: [-769.998 -769.998 -769.998] (1.000)
Step: 62999, Reward: [-476.324 -476.324 -476.324] [39.9500], Avg: [-769.796 -769.796 -769.796] (1.000)
Step: 63049, Reward: [-432.044 -432.044 -432.044] [46.3386], Avg: [-769.565 -769.565 -769.565] (1.000)
Step: 63099, Reward: [-427.941 -427.941 -427.941] [116.5275], Avg: [-769.387 -769.387 -769.387] (1.000)
Step: 63149, Reward: [-401.42 -401.42 -401.42] [36.6963], Avg: [-769.125 -769.125 -769.125] (1.000)
Step: 63199, Reward: [-379.95 -379.95 -379.95] [37.0502], Avg: [-768.846 -768.846 -768.846] (1.000)
Step: 63249, Reward: [-430.951 -430.951 -430.951] [92.7502], Avg: [-768.652 -768.652 -768.652] (1.000)
Step: 63299, Reward: [-409.325 -409.325 -409.325] [35.9997], Avg: [-768.397 -768.397 -768.397] (1.000)
Step: 63349, Reward: [-387.457 -387.457 -387.457] [53.5138], Avg: [-768.138 -768.138 -768.138] (1.000)
Step: 63399, Reward: [-416.772 -416.772 -416.772] [72.1684], Avg: [-767.918 -767.918 -767.918] (1.000)
Step: 63449, Reward: [-470.722 -470.722 -470.722] [56.1240], Avg: [-767.728 -767.728 -767.728] (1.000)
Step: 63499, Reward: [-400.75 -400.75 -400.75] [45.1280], Avg: [-767.475 -767.475 -767.475] (1.000)
Step: 63549, Reward: [-438.431 -438.431 -438.431] [109.6499], Avg: [-767.302 -767.302 -767.302] (1.000)
Step: 63599, Reward: [-378.778 -378.778 -378.778] [22.1200], Avg: [-767.014 -767.014 -767.014] (1.000)
Step: 63649, Reward: [-461.897 -461.897 -461.897] [53.4696], Avg: [-766.816 -766.816 -766.816] (1.000)
Step: 63699, Reward: [-395.301 -395.301 -395.301] [27.7731], Avg: [-766.547 -766.547 -766.547] (1.000)
Step: 63749, Reward: [-383.193 -383.193 -383.193] [118.4706], Avg: [-766.339 -766.339 -766.339] (1.000)
Step: 63799, Reward: [-404.697 -404.697 -404.697] [51.0880], Avg: [-766.095 -766.095 -766.095] (1.000)
Step: 63849, Reward: [-404.203 -404.203 -404.203] [54.0779], Avg: [-765.854 -765.854 -765.854] (1.000)
Step: 63899, Reward: [-465.814 -465.814 -465.814] [49.6282], Avg: [-765.658 -765.658 -765.658] (1.000)
Step: 63949, Reward: [-359.433 -359.433 -359.433] [26.9192], Avg: [-765.362 -765.362 -765.362] (1.000)
Step: 63999, Reward: [-346.024 -346.024 -346.024] [45.9771], Avg: [-765.07 -765.07 -765.07] (1.000)
Step: 64049, Reward: [-449.7 -449.7 -449.7] [65.7022], Avg: [-764.875 -764.875 -764.875] (1.000)
Step: 64099, Reward: [-403.725 -403.725 -403.725] [43.8193], Avg: [-764.628 -764.628 -764.628] (1.000)
Step: 64149, Reward: [-415.675 -415.675 -415.675] [171.0617], Avg: [-764.489 -764.489 -764.489] (1.000)
Step: 64199, Reward: [-413.64 -413.64 -413.64] [51.3122], Avg: [-764.256 -764.256 -764.256] (1.000)
Step: 64249, Reward: [-361.677 -361.677 -361.677] [55.0229], Avg: [-763.985 -763.985 -763.985] (1.000)
Step: 64299, Reward: [-384.396 -384.396 -384.396] [17.6747], Avg: [-763.704 -763.704 -763.704] (1.000)
Step: 64349, Reward: [-475.605 -475.605 -475.605] [68.4447], Avg: [-763.533 -763.533 -763.533] (1.000)
Step: 64399, Reward: [-371.318 -371.318 -371.318] [34.1458], Avg: [-763.255 -763.255 -763.255] (1.000)
Step: 64449, Reward: [-474.034 -474.034 -474.034] [30.2168], Avg: [-763.054 -763.054 -763.054] (1.000)
Step: 64499, Reward: [-442.48 -442.48 -442.48] [83.2951], Avg: [-762.87 -762.87 -762.87] (1.000)
Step: 64549, Reward: [-462.998 -462.998 -462.998] [67.7618], Avg: [-762.691 -762.691 -762.691] (1.000)
Step: 64599, Reward: [-393.349 -393.349 -393.349] [77.5100], Avg: [-762.465 -762.465 -762.465] (1.000)
Step: 64649, Reward: [-396.404 -396.404 -396.404] [60.7389], Avg: [-762.229 -762.229 -762.229] (1.000)
Step: 64699, Reward: [-344.86 -344.86 -344.86] [45.8777], Avg: [-761.942 -761.942 -761.942] (1.000)
Step: 64749, Reward: [-350.004 -350.004 -350.004] [36.7607], Avg: [-761.652 -761.652 -761.652] (1.000)
Step: 64799, Reward: [-393.529 -393.529 -393.529] [52.3813], Avg: [-761.408 -761.408 -761.408] (1.000)
Step: 64849, Reward: [-444.472 -444.472 -444.472] [104.6000], Avg: [-761.244 -761.244 -761.244] (1.000)
Step: 64899, Reward: [-467.339 -467.339 -467.339] [46.8128], Avg: [-761.054 -761.054 -761.054] (1.000)
Step: 64949, Reward: [-463.57 -463.57 -463.57] [43.8681], Avg: [-760.859 -760.859 -760.859] (1.000)
Step: 64999, Reward: [-383.833 -383.833 -383.833] [46.3063], Avg: [-760.604 -760.604 -760.604] (1.000)
Step: 65049, Reward: [-452.332 -452.332 -452.332] [80.1105], Avg: [-760.429 -760.429 -760.429] (1.000)
Step: 65099, Reward: [-484.506 -484.506 -484.506] [111.0473], Avg: [-760.302 -760.302 -760.302] (1.000)
Step: 65149, Reward: [-516.234 -516.234 -516.234] [132.6407], Avg: [-760.217 -760.217 -760.217] (1.000)
Step: 65199, Reward: [-447.298 -447.298 -447.298] [69.8496], Avg: [-760.031 -760.031 -760.031] (1.000)
Step: 65249, Reward: [-463.453 -463.453 -463.453] [92.8932], Avg: [-759.874 -759.874 -759.874] (1.000)
Step: 65299, Reward: [-470.213 -470.213 -470.213] [64.9389], Avg: [-759.702 -759.702 -759.702] (1.000)
Step: 65349, Reward: [-528.988 -528.988 -528.988] [227.9700], Avg: [-759.7 -759.7 -759.7] (1.000)
Step: 65399, Reward: [-553.967 -553.967 -553.967] [92.5143], Avg: [-759.614 -759.614 -759.614] (1.000)
Step: 65449, Reward: [-616.436 -616.436 -616.436] [34.6019], Avg: [-759.531 -759.531 -759.531] (1.000)
Step: 65499, Reward: [-591.369 -591.369 -591.369] [210.9440], Avg: [-759.563 -759.563 -759.563] (1.000)
Step: 65549, Reward: [-570.296 -570.296 -570.296] [174.2436], Avg: [-759.552 -759.552 -759.552] (1.000)
Step: 65599, Reward: [-579.416 -579.416 -579.416] [167.7634], Avg: [-759.543 -759.543 -759.543] (1.000)
Step: 65649, Reward: [-577.151 -577.151 -577.151] [46.6839], Avg: [-759.439 -759.439 -759.439] (1.000)
Step: 65699, Reward: [-522.879 -522.879 -522.879] [92.9920], Avg: [-759.33 -759.33 -759.33] (1.000)
Step: 65749, Reward: [-554.144 -554.144 -554.144] [176.9759], Avg: [-759.308 -759.308 -759.308] (1.000)
Step: 65799, Reward: [-532.849 -532.849 -532.849] [135.8169], Avg: [-759.24 -759.24 -759.24] (1.000)
Step: 65849, Reward: [-518.425 -518.425 -518.425] [91.7219], Avg: [-759.126 -759.126 -759.126] (1.000)
Step: 65899, Reward: [-473.892 -473.892 -473.892] [39.4659], Avg: [-758.94 -758.94 -758.94] (1.000)
Step: 65949, Reward: [-560.38 -560.38 -560.38] [48.2490], Avg: [-758.826 -758.826 -758.826] (1.000)
Step: 65999, Reward: [-504.401 -504.401 -504.401] [96.2095], Avg: [-758.706 -758.706 -758.706] (1.000)
Step: 66049, Reward: [-585.31 -585.31 -585.31] [55.5316], Avg: [-758.617 -758.617 -758.617] (1.000)
Step: 66099, Reward: [-499.431 -499.431 -499.431] [117.7922], Avg: [-758.51 -758.51 -758.51] (1.000)
Step: 66149, Reward: [-544.206 -544.206 -544.206] [122.6547], Avg: [-758.441 -758.441 -758.441] (1.000)
Step: 66199, Reward: [-609.211 -609.211 -609.211] [184.6314], Avg: [-758.467 -758.467 -758.467] (1.000)
Step: 66249, Reward: [-569.467 -569.467 -569.467] [89.7003], Avg: [-758.392 -758.392 -758.392] (1.000)
Step: 66299, Reward: [-594.099 -594.099 -594.099] [101.0310], Avg: [-758.345 -758.345 -758.345] (1.000)
Step: 66349, Reward: [-512.493 -512.493 -512.493] [93.4914], Avg: [-758.23 -758.23 -758.23] (1.000)
Step: 66399, Reward: [-642.652 -642.652 -642.652] [197.5789], Avg: [-758.292 -758.292 -758.292] (1.000)
Step: 66449, Reward: [-639.163 -639.163 -639.163] [140.6460], Avg: [-758.308 -758.308 -758.308] (1.000)
Step: 66499, Reward: [-649.885 -649.885 -649.885] [234.2977], Avg: [-758.403 -758.403 -758.403] (1.000)
Step: 66549, Reward: [-707.165 -707.165 -707.165] [73.7464], Avg: [-758.419 -758.419 -758.419] (1.000)
Step: 66599, Reward: [-835.815 -835.815 -835.815] [73.0101], Avg: [-758.532 -758.532 -758.532] (1.000)
Step: 66649, Reward: [-649.374 -649.374 -649.374] [92.8455], Avg: [-758.52 -758.52 -758.52] (1.000)
Step: 66699, Reward: [-498.09 -498.09 -498.09] [149.8508], Avg: [-758.437 -758.437 -758.437] (1.000)
Step: 66749, Reward: [-622.898 -622.898 -622.898] [119.3754], Avg: [-758.425 -758.425 -758.425] (1.000)
Step: 66799, Reward: [-662.457 -662.457 -662.457] [115.0727], Avg: [-758.439 -758.439 -758.439] (1.000)
Step: 66849, Reward: [-772.877 -772.877 -772.877] [256.1384], Avg: [-758.642 -758.642 -758.642] (1.000)
Step: 66899, Reward: [-647.044 -647.044 -647.044] [124.5656], Avg: [-758.651 -758.651 -758.651] (1.000)
Step: 66949, Reward: [-588.756 -588.756 -588.756] [157.5900], Avg: [-758.642 -758.642 -758.642] (1.000)
Step: 66999, Reward: [-719.84 -719.84 -719.84] [140.1709], Avg: [-758.718 -758.718 -758.718] (1.000)
Step: 67049, Reward: [-669.598 -669.598 -669.598] [101.9214], Avg: [-758.727 -758.727 -758.727] (1.000)
Step: 67099, Reward: [-615.467 -615.467 -615.467] [130.0221], Avg: [-758.718 -758.718 -758.718] (1.000)
Step: 67149, Reward: [-553.977 -553.977 -553.977] [105.0008], Avg: [-758.643 -758.643 -758.643] (1.000)
Step: 67199, Reward: [-597.302 -597.302 -597.302] [100.0697], Avg: [-758.598 -758.598 -758.598] (1.000)
Step: 67249, Reward: [-602.426 -602.426 -602.426] [152.8176], Avg: [-758.595 -758.595 -758.595] (1.000)
Step: 67299, Reward: [-830.747 -830.747 -830.747] [154.5249], Avg: [-758.764 -758.764 -758.764] (1.000)
Step: 67349, Reward: [-582.948 -582.948 -582.948] [181.2722], Avg: [-758.768 -758.768 -758.768] (1.000)
Step: 67399, Reward: [-556.207 -556.207 -556.207] [165.5520], Avg: [-758.74 -758.74 -758.74] (1.000)
Step: 67449, Reward: [-807.812 -807.812 -807.812] [105.7355], Avg: [-758.855 -758.855 -758.855] (1.000)
Step: 67499, Reward: [-850.493 -850.493 -850.493] [162.7628], Avg: [-759.043 -759.043 -759.043] (1.000)
Step: 67549, Reward: [-565.957 -565.957 -565.957] [151.8086], Avg: [-759.013 -759.013 -759.013] (1.000)
Step: 67599, Reward: [-558.46 -558.46 -558.46] [193.2478], Avg: [-759.008 -759.008 -759.008] (1.000)
Step: 67649, Reward: [-680.013 -680.013 -680.013] [215.9046], Avg: [-759.109 -759.109 -759.109] (1.000)
Step: 67699, Reward: [-490.678 -490.678 -490.678] [39.9314], Avg: [-758.94 -758.94 -758.94] (1.000)
Step: 67749, Reward: [-578.881 -578.881 -578.881] [141.6853], Avg: [-758.912 -758.912 -758.912] (1.000)
Step: 67799, Reward: [-668.166 -668.166 -668.166] [94.0582], Avg: [-758.914 -758.914 -758.914] (1.000)
Step: 67849, Reward: [-680.811 -680.811 -680.811] [142.2220], Avg: [-758.961 -758.961 -758.961] (1.000)
Step: 67899, Reward: [-688.956 -688.956 -688.956] [100.5405], Avg: [-758.984 -758.984 -758.984] (1.000)
Step: 67949, Reward: [-592.357 -592.357 -592.357] [102.8091], Avg: [-758.937 -758.937 -758.937] (1.000)
Step: 67999, Reward: [-499.522 -499.522 -499.522] [101.6584], Avg: [-758.821 -758.821 -758.821] (1.000)
Step: 68049, Reward: [-540.823 -540.823 -540.823] [116.1328], Avg: [-758.746 -758.746 -758.746] (1.000)
Step: 68099, Reward: [-671.304 -671.304 -671.304] [186.9565], Avg: [-758.819 -758.819 -758.819] (1.000)
Step: 68149, Reward: [-653.457 -653.457 -653.457] [152.2339], Avg: [-758.853 -758.853 -758.853] (1.000)
Step: 68199, Reward: [-517.935 -517.935 -517.935] [143.3744], Avg: [-758.782 -758.782 -758.782] (1.000)
Step: 68249, Reward: [-637.55 -637.55 -637.55] [210.5276], Avg: [-758.847 -758.847 -758.847] (1.000)
Step: 68299, Reward: [-636.944 -636.944 -636.944] [182.9021], Avg: [-758.892 -758.892 -758.892] (1.000)
Step: 68349, Reward: [-644.325 -644.325 -644.325] [140.5904], Avg: [-758.911 -758.911 -758.911] (1.000)
Step: 68399, Reward: [-719.045 -719.045 -719.045] [194.9383], Avg: [-759.024 -759.024 -759.024] (1.000)
Step: 68449, Reward: [-726.334 -726.334 -726.334] [169.4258], Avg: [-759.124 -759.124 -759.124] (1.000)
Step: 68499, Reward: [-570.36 -570.36 -570.36] [107.0348], Avg: [-759.065 -759.065 -759.065] (1.000)
Step: 68549, Reward: [-487.248 -487.248 -487.248] [134.9021], Avg: [-758.965 -758.965 -758.965] (1.000)
Step: 68599, Reward: [-620.536 -620.536 -620.536] [147.7790], Avg: [-758.972 -758.972 -758.972] (1.000)
Step: 68649, Reward: [-677.598 -677.598 -677.598] [190.4418], Avg: [-759.051 -759.051 -759.051] (1.000)
Step: 68699, Reward: [-621.026 -621.026 -621.026] [159.6194], Avg: [-759.067 -759.067 -759.067] (1.000)
Step: 68749, Reward: [-564.062 -564.062 -564.062] [232.5470], Avg: [-759.094 -759.094 -759.094] (1.000)
Step: 68799, Reward: [-610.191 -610.191 -610.191] [163.4401], Avg: [-759.105 -759.105 -759.105] (1.000)
Step: 68849, Reward: [-603.004 -603.004 -603.004] [149.8309], Avg: [-759.1 -759.1 -759.1] (1.000)
Step: 68899, Reward: [-599.031 -599.031 -599.031] [148.6705], Avg: [-759.092 -759.092 -759.092] (1.000)
Step: 68949, Reward: [-444.208 -444.208 -444.208] [83.7365], Avg: [-758.924 -758.924 -758.924] (1.000)
Step: 68999, Reward: [-617.826 -617.826 -617.826] [157.7822], Avg: [-758.936 -758.936 -758.936] (1.000)
Step: 69049, Reward: [-661.195 -661.195 -661.195] [173.0907], Avg: [-758.991 -758.991 -758.991] (1.000)
Step: 69099, Reward: [-486.458 -486.458 -486.458] [67.6988], Avg: [-758.843 -758.843 -758.843] (1.000)
Step: 69149, Reward: [-655.423 -655.423 -655.423] [255.4258], Avg: [-758.953 -758.953 -758.953] (1.000)
Step: 69199, Reward: [-571.424 -571.424 -571.424] [68.6972], Avg: [-758.867 -758.867 -758.867] (1.000)
Step: 69249, Reward: [-576.187 -576.187 -576.187] [69.1052], Avg: [-758.785 -758.785 -758.785] (1.000)
Step: 69299, Reward: [-548.345 -548.345 -548.345] [175.1773], Avg: [-758.759 -758.759 -758.759] (1.000)
Step: 69349, Reward: [-529.856 -529.856 -529.856] [140.5626], Avg: [-758.696 -758.696 -758.696] (1.000)
Step: 69399, Reward: [-533.882 -533.882 -533.882] [65.9054], Avg: [-758.581 -758.581 -758.581] (1.000)
Step: 69449, Reward: [-535.487 -535.487 -535.487] [158.6481], Avg: [-758.535 -758.535 -758.535] (1.000)
Step: 69499, Reward: [-649.952 -649.952 -649.952] [305.3499], Avg: [-758.676 -758.676 -758.676] (1.000)
Step: 69549, Reward: [-555.371 -555.371 -555.371] [107.2302], Avg: [-758.607 -758.607 -758.607] (1.000)
Step: 69599, Reward: [-705.246 -705.246 -705.246] [230.0577], Avg: [-758.734 -758.734 -758.734] (1.000)
Step: 69649, Reward: [-838.123 -838.123 -838.123] [196.0973], Avg: [-758.932 -758.932 -758.932] (1.000)
Step: 69699, Reward: [-481.193 -481.193 -481.193] [57.9000], Avg: [-758.774 -758.774 -758.774] (1.000)
Step: 69749, Reward: [-595.771 -595.771 -595.771] [102.6209], Avg: [-758.731 -758.731 -758.731] (1.000)
Step: 69799, Reward: [-683.534 -683.534 -683.534] [76.4196], Avg: [-758.732 -758.732 -758.732] (1.000)
Step: 69849, Reward: [-579.557 -579.557 -579.557] [94.0262], Avg: [-758.671 -758.671 -758.671] (1.000)
Step: 69899, Reward: [-640.811 -640.811 -640.811] [141.6637], Avg: [-758.688 -758.688 -758.688] (1.000)
Step: 69949, Reward: [-575.586 -575.586 -575.586] [127.5033], Avg: [-758.648 -758.648 -758.648] (1.000)
Step: 69999, Reward: [-815.67 -815.67 -815.67] [283.6473], Avg: [-758.891 -758.891 -758.891] (1.000)
Step: 70049, Reward: [-720.025 -720.025 -720.025] [217.0538], Avg: [-759.019 -759.019 -759.019] (1.000)
Step: 70099, Reward: [-1038.269 -1038.269 -1038.269] [343.7093], Avg: [-759.463 -759.463 -759.463] (1.000)
Step: 70149, Reward: [-816.454 -816.454 -816.454] [375.3784], Avg: [-759.771 -759.771 -759.771] (1.000)
Step: 70199, Reward: [-758.727 -758.727 -758.727] [290.0753], Avg: [-759.977 -759.977 -759.977] (1.000)
Step: 70249, Reward: [-680.4 -680.4 -680.4] [210.9470], Avg: [-760.07 -760.07 -760.07] (1.000)
Step: 70299, Reward: [-719.716 -719.716 -719.716] [392.9760], Avg: [-760.321 -760.321 -760.321] (1.000)
Step: 70349, Reward: [-748.925 -748.925 -748.925] [446.1168], Avg: [-760.63 -760.63 -760.63] (1.000)
Step: 70399, Reward: [-581.912 -581.912 -581.912] [154.4455], Avg: [-760.613 -760.613 -760.613] (1.000)
Step: 70449, Reward: [-749.534 -749.534 -749.534] [168.1471], Avg: [-760.724 -760.724 -760.724] (1.000)
Step: 70499, Reward: [-546.031 -546.031 -546.031] [124.6235], Avg: [-760.661 -760.661 -760.661] (1.000)
Step: 70549, Reward: [-692.089 -692.089 -692.089] [183.3734], Avg: [-760.742 -760.742 -760.742] (1.000)
Step: 70599, Reward: [-558.028 -558.028 -558.028] [190.5397], Avg: [-760.733 -760.733 -760.733] (1.000)
Step: 70649, Reward: [-607.62 -607.62 -607.62] [185.1718], Avg: [-760.756 -760.756 -760.756] (1.000)
Step: 70699, Reward: [-516.099 -516.099 -516.099] [143.6331], Avg: [-760.685 -760.685 -760.685] (1.000)
Step: 70749, Reward: [-442.939 -442.939 -442.939] [78.4963], Avg: [-760.515 -760.515 -760.515] (1.000)
Step: 70799, Reward: [-417.766 -417.766 -417.766] [35.6930], Avg: [-760.299 -760.299 -760.299] (1.000)
Step: 70849, Reward: [-431.096 -431.096 -431.096] [68.8994], Avg: [-760.115 -760.115 -760.115] (1.000)
Step: 70899, Reward: [-448.907 -448.907 -448.907] [98.3234], Avg: [-759.965 -759.965 -759.965] (1.000)
Step: 70949, Reward: [-390.093 -390.093 -390.093] [93.1692], Avg: [-759.77 -759.77 -759.77] (1.000)
Step: 70999, Reward: [-438.103 -438.103 -438.103] [99.2946], Avg: [-759.613 -759.613 -759.613] (1.000)
Step: 71049, Reward: [-466.024 -466.024 -466.024] [43.8275], Avg: [-759.437 -759.437 -759.437] (1.000)
Step: 71099, Reward: [-422.187 -422.187 -422.187] [59.6730], Avg: [-759.242 -759.242 -759.242] (1.000)
Step: 71149, Reward: [-403.872 -403.872 -403.872] [58.0421], Avg: [-759.033 -759.033 -759.033] (1.000)
Step: 71199, Reward: [-364.844 -364.844 -364.844] [12.7957], Avg: [-758.765 -758.765 -758.765] (1.000)
Step: 71249, Reward: [-392.507 -392.507 -392.507] [109.4607], Avg: [-758.585 -758.585 -758.585] (1.000)
Step: 71299, Reward: [-506.012 -506.012 -506.012] [99.8896], Avg: [-758.478 -758.478 -758.478] (1.000)
Step: 71349, Reward: [-410.424 -410.424 -410.424] [109.1145], Avg: [-758.311 -758.311 -758.311] (1.000)
Step: 71399, Reward: [-456.638 -456.638 -456.638] [29.4408], Avg: [-758.12 -758.12 -758.12] (1.000)
Step: 71449, Reward: [-444.581 -444.581 -444.581] [66.3519], Avg: [-757.947 -757.947 -757.947] (1.000)
Step: 71499, Reward: [-465.282 -465.282 -465.282] [89.7805], Avg: [-757.805 -757.805 -757.805] (1.000)
Step: 71549, Reward: [-497.127 -497.127 -497.127] [109.6949], Avg: [-757.7 -757.7 -757.7] (1.000)
Step: 71599, Reward: [-490.587 -490.587 -490.587] [163.3251], Avg: [-757.627 -757.627 -757.627] (1.000)
Step: 71649, Reward: [-388.335 -388.335 -388.335] [22.3564], Avg: [-757.385 -757.385 -757.385] (1.000)
Step: 71699, Reward: [-472.554 -472.554 -472.554] [79.7817], Avg: [-757.242 -757.242 -757.242] (1.000)
Step: 71749, Reward: [-451.624 -451.624 -451.624] [30.2355], Avg: [-757.05 -757.05 -757.05] (1.000)
Step: 71799, Reward: [-402.064 -402.064 -402.064] [42.1226], Avg: [-756.832 -756.832 -756.832] (1.000)
Step: 71849, Reward: [-391.148 -391.148 -391.148] [88.6364], Avg: [-756.64 -756.64 -756.64] (1.000)
Step: 71899, Reward: [-454.733 -454.733 -454.733] [63.7059], Avg: [-756.474 -756.474 -756.474] (1.000)
Step: 71949, Reward: [-448.094 -448.094 -448.094] [41.4928], Avg: [-756.288 -756.288 -756.288] (1.000)
Step: 71999, Reward: [-447.427 -447.427 -447.427] [36.8347], Avg: [-756.1 -756.1 -756.1] (1.000)
Step: 72049, Reward: [-440.982 -440.982 -440.982] [58.2859], Avg: [-755.921 -755.921 -755.921] (1.000)
Step: 72099, Reward: [-405.312 -405.312 -405.312] [50.6630], Avg: [-755.713 -755.713 -755.713] (1.000)
Step: 72149, Reward: [-361.08 -361.08 -361.08] [52.8375], Avg: [-755.476 -755.476 -755.476] (1.000)
Step: 72199, Reward: [-404.332 -404.332 -404.332] [61.1179], Avg: [-755.276 -755.276 -755.276] (1.000)
Step: 72249, Reward: [-421.441 -421.441 -421.441] [91.1197], Avg: [-755.108 -755.108 -755.108] (1.000)
Step: 72299, Reward: [-396.332 -396.332 -396.332] [87.2823], Avg: [-754.92 -754.92 -754.92] (1.000)
Step: 72349, Reward: [-421.883 -421.883 -421.883] [79.1729], Avg: [-754.744 -754.744 -754.744] (1.000)
Step: 72399, Reward: [-473.644 -473.644 -473.644] [106.1912], Avg: [-754.624 -754.624 -754.624] (1.000)
Step: 72449, Reward: [-384.502 -384.502 -384.502] [79.1208], Avg: [-754.423 -754.423 -754.423] (1.000)
Step: 72499, Reward: [-356.815 -356.815 -356.815] [38.8879], Avg: [-754.175 -754.175 -754.175] (1.000)
Step: 72549, Reward: [-462.329 -462.329 -462.329] [99.1403], Avg: [-754.043 -754.043 -754.043] (1.000)
Step: 72599, Reward: [-443.234 -443.234 -443.234] [61.9372], Avg: [-753.871 -753.871 -753.871] (1.000)
Step: 72649, Reward: [-446.234 -446.234 -446.234] [51.8415], Avg: [-753.695 -753.695 -753.695] (1.000)
Step: 72699, Reward: [-477.843 -477.843 -477.843] [28.9509], Avg: [-753.525 -753.525 -753.525] (1.000)
Step: 72749, Reward: [-431.55 -431.55 -431.55] [64.1561], Avg: [-753.348 -753.348 -753.348] (1.000)
Step: 72799, Reward: [-382.083 -382.083 -382.083] [40.3589], Avg: [-753.121 -753.121 -753.121] (1.000)
Step: 72849, Reward: [-477.681 -477.681 -477.681] [17.7618], Avg: [-752.944 -752.944 -752.944] (1.000)
Step: 72899, Reward: [-377.597 -377.597 -377.597] [62.1329], Avg: [-752.729 -752.729 -752.729] (1.000)
Step: 72949, Reward: [-389.365 -389.365 -389.365] [58.5384], Avg: [-752.52 -752.52 -752.52] (1.000)
Step: 72999, Reward: [-380.093 -380.093 -380.093] [80.4750], Avg: [-752.32 -752.32 -752.32] (1.000)
Step: 73049, Reward: [-389.743 -389.743 -389.743] [75.2884], Avg: [-752.124 -752.124 -752.124] (1.000)
Step: 73099, Reward: [-393.009 -393.009 -393.009] [69.4272], Avg: [-751.926 -751.926 -751.926] (1.000)
Step: 73149, Reward: [-388.853 -388.853 -388.853] [72.1973], Avg: [-751.727 -751.727 -751.727] (1.000)
Step: 73199, Reward: [-391.822 -391.822 -391.822] [40.1370], Avg: [-751.508 -751.508 -751.508] (1.000)
Step: 73249, Reward: [-428.993 -428.993 -428.993] [103.2897], Avg: [-751.359 -751.359 -751.359] (1.000)
Step: 73299, Reward: [-404.222 -404.222 -404.222] [93.2570], Avg: [-751.185 -751.185 -751.185] (1.000)
Step: 73349, Reward: [-469.109 -469.109 -469.109] [96.8035], Avg: [-751.059 -751.059 -751.059] (1.000)
Step: 73399, Reward: [-409.512 -409.512 -409.512] [58.9613], Avg: [-750.867 -750.867 -750.867] (1.000)
Step: 73449, Reward: [-400.366 -400.366 -400.366] [65.2242], Avg: [-750.672 -750.672 -750.672] (1.000)
Step: 73499, Reward: [-428.121 -428.121 -428.121] [70.8182], Avg: [-750.501 -750.501 -750.501] (1.000)
Step: 73549, Reward: [-444.854 -444.854 -444.854] [66.5755], Avg: [-750.339 -750.339 -750.339] (1.000)
Step: 73599, Reward: [-514.418 -514.418 -514.418] [110.8808], Avg: [-750.254 -750.254 -750.254] (1.000)
Step: 73649, Reward: [-472.011 -472.011 -472.011] [127.3948], Avg: [-750.151 -750.151 -750.151] (1.000)
Step: 73699, Reward: [-418.874 -418.874 -418.874] [68.6130], Avg: [-749.973 -749.973 -749.973] (1.000)
Step: 73749, Reward: [-425.801 -425.801 -425.801] [77.0615], Avg: [-749.806 -749.806 -749.806] (1.000)
Step: 73799, Reward: [-413.367 -413.367 -413.367] [83.0256], Avg: [-749.634 -749.634 -749.634] (1.000)
Step: 73849, Reward: [-497.233 -497.233 -497.233] [90.9631], Avg: [-749.525 -749.525 -749.525] (1.000)
Step: 73899, Reward: [-454.48 -454.48 -454.48] [171.5782], Avg: [-749.441 -749.441 -749.441] (1.000)
Step: 73949, Reward: [-427.01 -427.01 -427.01] [81.9380], Avg: [-749.278 -749.278 -749.278] (1.000)
Step: 73999, Reward: [-489.905 -489.905 -489.905] [143.2541], Avg: [-749.2 -749.2 -749.2] (1.000)
Step: 74049, Reward: [-461.282 -461.282 -461.282] [86.7320], Avg: [-749.064 -749.064 -749.064] (1.000)
Step: 74099, Reward: [-497.597 -497.597 -497.597] [181.6560], Avg: [-749.017 -749.017 -749.017] (1.000)
Step: 74149, Reward: [-507.82 -507.82 -507.82] [58.3505], Avg: [-748.894 -748.894 -748.894] (1.000)
Step: 74199, Reward: [-470.602 -470.602 -470.602] [81.5550], Avg: [-748.761 -748.761 -748.761] (1.000)
Step: 74249, Reward: [-466.843 -466.843 -466.843] [123.6980], Avg: [-748.655 -748.655 -748.655] (1.000)
Step: 74299, Reward: [-456.92 -456.92 -456.92] [60.2381], Avg: [-748.499 -748.499 -748.499] (1.000)
Step: 74349, Reward: [-449.243 -449.243 -449.243] [68.4228], Avg: [-748.344 -748.344 -748.344] (1.000)
Step: 74399, Reward: [-438.429 -438.429 -438.429] [48.9341], Avg: [-748.168 -748.168 -748.168] (1.000)
Step: 74449, Reward: [-494.502 -494.502 -494.502] [81.8464], Avg: [-748.053 -748.053 -748.053] (1.000)
Step: 74499, Reward: [-554.396 -554.396 -554.396] [225.5883], Avg: [-748.074 -748.074 -748.074] (1.000)
Step: 74549, Reward: [-474.359 -474.359 -474.359] [72.8040], Avg: [-747.94 -747.94 -747.94] (1.000)
Step: 74599, Reward: [-502.424 -502.424 -502.424] [118.6185], Avg: [-747.855 -747.855 -747.855] (1.000)
Step: 74649, Reward: [-558.215 -558.215 -558.215] [77.3132], Avg: [-747.779 -747.779 -747.779] (1.000)
Step: 74699, Reward: [-521.004 -521.004 -521.004] [75.4168], Avg: [-747.678 -747.678 -747.678] (1.000)
Step: 74749, Reward: [-399.721 -399.721 -399.721] [46.8933], Avg: [-747.477 -747.477 -747.477] (1.000)
Step: 74799, Reward: [-433.302 -433.302 -433.302] [63.4530], Avg: [-747.309 -747.309 -747.309] (1.000)
Step: 74849, Reward: [-483.303 -483.303 -483.303] [68.9316], Avg: [-747.179 -747.179 -747.179] (1.000)
Step: 74899, Reward: [-462.362 -462.362 -462.362] [127.6207], Avg: [-747.074 -747.074 -747.074] (1.000)
Step: 74949, Reward: [-454.417 -454.417 -454.417] [65.3257], Avg: [-746.922 -746.922 -746.922] (1.000)
Step: 74999, Reward: [-459.703 -459.703 -459.703] [59.5126], Avg: [-746.77 -746.77 -746.77] (1.000)
Step: 75049, Reward: [-440.686 -440.686 -440.686] [48.3814], Avg: [-746.599 -746.599 -746.599] (1.000)
Step: 75099, Reward: [-471.953 -471.953 -471.953] [128.7786], Avg: [-746.501 -746.501 -746.501] (1.000)
Step: 75149, Reward: [-422.075 -422.075 -422.075] [50.3048], Avg: [-746.319 -746.319 -746.319] (1.000)
Step: 75199, Reward: [-413.35 -413.35 -413.35] [44.6133], Avg: [-746.127 -746.127 -746.127] (1.000)
Step: 75249, Reward: [-441.93 -441.93 -441.93] [76.8885], Avg: [-745.976 -745.976 -745.976] (1.000)
Step: 75299, Reward: [-449.735 -449.735 -449.735] [94.0567], Avg: [-745.842 -745.842 -745.842] (1.000)
Step: 75349, Reward: [-396.887 -396.887 -396.887] [90.5108], Avg: [-745.671 -745.671 -745.671] (1.000)
Step: 75399, Reward: [-413.079 -413.079 -413.079] [45.7650], Avg: [-745.48 -745.48 -745.48] (1.000)
Step: 75449, Reward: [-438.026 -438.026 -438.026] [82.8951], Avg: [-745.332 -745.332 -745.332] (1.000)
Step: 75499, Reward: [-489.71 -489.71 -489.71] [45.5761], Avg: [-745.192 -745.192 -745.192] (1.000)
Step: 75549, Reward: [-431.719 -431.719 -431.719] [91.4616], Avg: [-745.046 -745.046 -745.046] (1.000)
Step: 75599, Reward: [-419.033 -419.033 -419.033] [63.7633], Avg: [-744.872 -744.872 -744.872] (1.000)
Step: 75649, Reward: [-425.309 -425.309 -425.309] [42.5913], Avg: [-744.689 -744.689 -744.689] (1.000)
Step: 75699, Reward: [-389.345 -389.345 -389.345] [34.8158], Avg: [-744.477 -744.477 -744.477] (1.000)
Step: 75749, Reward: [-452.709 -452.709 -452.709] [56.8101], Avg: [-744.322 -744.322 -744.322] (1.000)
Step: 75799, Reward: [-417.775 -417.775 -417.775] [45.2848], Avg: [-744.137 -744.137 -744.137] (1.000)
Step: 75849, Reward: [-551.798 -551.798 -551.798] [138.3643], Avg: [-744.101 -744.101 -744.101] (1.000)
Step: 75899, Reward: [-470.214 -470.214 -470.214] [73.7786], Avg: [-743.969 -743.969 -743.969] (1.000)
Step: 75949, Reward: [-458.981 -458.981 -458.981] [17.6564], Avg: [-743.793 -743.793 -743.793] (1.000)
Step: 75999, Reward: [-492.865 -492.865 -492.865] [113.6793], Avg: [-743.703 -743.703 -743.703] (1.000)
Step: 76049, Reward: [-470.097 -470.097 -470.097] [82.2304], Avg: [-743.577 -743.577 -743.577] (1.000)
Step: 76099, Reward: [-486.653 -486.653 -486.653] [59.6482], Avg: [-743.448 -743.448 -743.448] (1.000)
Step: 76149, Reward: [-384.557 -384.557 -384.557] [51.5870], Avg: [-743.246 -743.246 -743.246] (1.000)
Step: 76199, Reward: [-447.537 -447.537 -447.537] [92.7986], Avg: [-743.113 -743.113 -743.113] (1.000)
Step: 76249, Reward: [-401.987 -401.987 -401.987] [39.2401], Avg: [-742.915 -742.915 -742.915] (1.000)
Step: 76299, Reward: [-490.053 -490.053 -490.053] [122.5164], Avg: [-742.829 -742.829 -742.829] (1.000)
Step: 76349, Reward: [-459.381 -459.381 -459.381] [71.0840], Avg: [-742.69 -742.69 -742.69] (1.000)
Step: 76399, Reward: [-416.358 -416.358 -416.358] [50.7594], Avg: [-742.51 -742.51 -742.51] (1.000)
Step: 76449, Reward: [-447.983 -447.983 -447.983] [76.8521], Avg: [-742.367 -742.367 -742.367] (1.000)
Step: 76499, Reward: [-431.514 -431.514 -431.514] [135.6800], Avg: [-742.253 -742.253 -742.253] (1.000)
Step: 76549, Reward: [-395.555 -395.555 -395.555] [72.7059], Avg: [-742.074 -742.074 -742.074] (1.000)
Step: 76599, Reward: [-413.668 -413.668 -413.668] [39.8364], Avg: [-741.886 -741.886 -741.886] (1.000)
Step: 76649, Reward: [-470.394 -470.394 -470.394] [52.8000], Avg: [-741.743 -741.743 -741.743] (1.000)
Step: 76699, Reward: [-455.308 -455.308 -455.308] [52.7438], Avg: [-741.591 -741.591 -741.591] (1.000)
Step: 76749, Reward: [-489.256 -489.256 -489.256] [89.6248], Avg: [-741.485 -741.485 -741.485] (1.000)
Step: 76799, Reward: [-397.374 -397.374 -397.374] [43.0442], Avg: [-741.289 -741.289 -741.289] (1.000)
Step: 76849, Reward: [-437.915 -437.915 -437.915] [44.6153], Avg: [-741.12 -741.12 -741.12] (1.000)
Step: 76899, Reward: [-446.683 -446.683 -446.683] [52.9184], Avg: [-740.963 -740.963 -740.963] (1.000)
Step: 76949, Reward: [-437.79 -437.79 -437.79] [64.2115], Avg: [-740.808 -740.808 -740.808] (1.000)
Step: 76999, Reward: [-477.084 -477.084 -477.084] [67.8146], Avg: [-740.681 -740.681 -740.681] (1.000)
Step: 77049, Reward: [-424.714 -424.714 -424.714] [94.5155], Avg: [-740.537 -740.537 -740.537] (1.000)
Step: 77099, Reward: [-507.629 -507.629 -507.629] [42.9769], Avg: [-740.414 -740.414 -740.414] (1.000)
Step: 77149, Reward: [-462.961 -462.961 -462.961] [62.5789], Avg: [-740.275 -740.275 -740.275] (1.000)
Step: 77199, Reward: [-498.444 -498.444 -498.444] [67.4121], Avg: [-740.162 -740.162 -740.162] (1.000)
Step: 77249, Reward: [-531.508 -531.508 -531.508] [175.2824], Avg: [-740.14 -740.14 -740.14] (1.000)
Step: 77299, Reward: [-501.603 -501.603 -501.603] [60.9475], Avg: [-740.025 -740.025 -740.025] (1.000)
Step: 77349, Reward: [-444.135 -444.135 -444.135] [47.5822], Avg: [-739.865 -739.865 -739.865] (1.000)
Step: 77399, Reward: [-476.853 -476.853 -476.853] [44.4037], Avg: [-739.723 -739.723 -739.723] (1.000)
Step: 77449, Reward: [-518.769 -518.769 -518.769] [87.6131], Avg: [-739.637 -739.637 -739.637] (1.000)
Step: 77499, Reward: [-497.065 -497.065 -497.065] [66.9103], Avg: [-739.524 -739.524 -739.524] (1.000)
Step: 77549, Reward: [-591.909 -591.909 -591.909] [181.5073], Avg: [-739.546 -739.546 -739.546] (1.000)
Step: 77599, Reward: [-575.814 -575.814 -575.814] [118.7257], Avg: [-739.517 -739.517 -739.517] (1.000)
Step: 77649, Reward: [-525.838 -525.838 -525.838] [78.2704], Avg: [-739.43 -739.43 -739.43] (1.000)
Step: 77699, Reward: [-628.791 -628.791 -628.791] [100.8658], Avg: [-739.423 -739.423 -739.423] (1.000)
Step: 77749, Reward: [-530.722 -530.722 -530.722] [74.3845], Avg: [-739.337 -739.337 -739.337] (1.000)
Step: 77799, Reward: [-578.215 -578.215 -578.215] [78.3880], Avg: [-739.284 -739.284 -739.284] (1.000)
Step: 77849, Reward: [-481.033 -481.033 -481.033] [85.3014], Avg: [-739.173 -739.173 -739.173] (1.000)
Step: 77899, Reward: [-525.925 -525.925 -525.925] [121.9317], Avg: [-739.114 -739.114 -739.114] (1.000)
Step: 77949, Reward: [-490.696 -490.696 -490.696] [96.1903], Avg: [-739.017 -739.017 -739.017] (1.000)
Step: 77999, Reward: [-478.358 -478.358 -478.358] [40.4845], Avg: [-738.875 -738.875 -738.875] (1.000)
Step: 78049, Reward: [-420.461 -420.461 -420.461] [47.7800], Avg: [-738.702 -738.702 -738.702] (1.000)
Step: 78099, Reward: [-528.391 -528.391 -528.391] [95.7233], Avg: [-738.629 -738.629 -738.629] (1.000)
Step: 78149, Reward: [-462.303 -462.303 -462.303] [49.3417], Avg: [-738.483 -738.483 -738.483] (1.000)
Step: 78199, Reward: [-390.647 -390.647 -390.647] [22.7434], Avg: [-738.276 -738.276 -738.276] (1.000)
Step: 78249, Reward: [-476.491 -476.491 -476.491] [81.7194], Avg: [-738.161 -738.161 -738.161] (1.000)
Step: 78299, Reward: [-453.113 -453.113 -453.113] [74.6422], Avg: [-738.026 -738.026 -738.026] (1.000)
Step: 78349, Reward: [-454.528 -454.528 -454.528] [89.9565], Avg: [-737.903 -737.903 -737.903] (1.000)
Step: 78399, Reward: [-417.834 -417.834 -417.834] [116.3275], Avg: [-737.773 -737.773 -737.773] (1.000)
Step: 78449, Reward: [-457.742 -457.742 -457.742] [21.8790], Avg: [-737.608 -737.608 -737.608] (1.000)
Step: 78499, Reward: [-487.578 -487.578 -487.578] [124.6081], Avg: [-737.528 -737.528 -737.528] (1.000)
Step: 78549, Reward: [-473.631 -473.631 -473.631] [54.1400], Avg: [-737.395 -737.395 -737.395] (1.000)
Step: 78599, Reward: [-460.117 -460.117 -460.117] [54.2547], Avg: [-737.253 -737.253 -737.253] (1.000)
Step: 78649, Reward: [-453.784 -453.784 -453.784] [77.2883], Avg: [-737.122 -737.122 -737.122] (1.000)
Step: 78699, Reward: [-416.341 -416.341 -416.341] [83.7296], Avg: [-736.971 -736.971 -736.971] (1.000)
Step: 78749, Reward: [-516.516 -516.516 -516.516] [133.9784], Avg: [-736.916 -736.916 -736.916] (1.000)
Step: 78799, Reward: [-515.624 -515.624 -515.624] [93.0144], Avg: [-736.835 -736.835 -736.835] (1.000)
Step: 78849, Reward: [-596.89 -596.89 -596.89] [150.2246], Avg: [-736.841 -736.841 -736.841] (1.000)
Step: 78899, Reward: [-493.946 -493.946 -493.946] [56.9230], Avg: [-736.724 -736.724 -736.724] (1.000)
Step: 78949, Reward: [-573.32 -573.32 -573.32] [163.6969], Avg: [-736.724 -736.724 -736.724] (1.000)
Step: 78999, Reward: [-490.412 -490.412 -490.412] [125.1671], Avg: [-736.647 -736.647 -736.647] (1.000)
Step: 79049, Reward: [-504.718 -504.718 -504.718] [176.6360], Avg: [-736.612 -736.612 -736.612] (1.000)
Step: 79099, Reward: [-423.231 -423.231 -423.231] [61.7553], Avg: [-736.453 -736.453 -736.453] (1.000)
Step: 79149, Reward: [-473.876 -473.876 -473.876] [70.4787], Avg: [-736.332 -736.332 -736.332] (1.000)
Step: 79199, Reward: [-403.121 -403.121 -403.121] [61.7565], Avg: [-736.16 -736.16 -736.16] (1.000)
Step: 79249, Reward: [-486.683 -486.683 -486.683] [58.0040], Avg: [-736.04 -736.04 -736.04] (1.000)
Step: 79299, Reward: [-480.745 -480.745 -480.745] [104.4112], Avg: [-735.944 -735.944 -735.944] (1.000)
Step: 79349, Reward: [-408.973 -408.973 -408.973] [92.8229], Avg: [-735.797 -735.797 -735.797] (1.000)
Step: 79399, Reward: [-416.452 -416.452 -416.452] [36.4294], Avg: [-735.619 -735.619 -735.619] (1.000)
Step: 79449, Reward: [-381.3 -381.3 -381.3] [45.2496], Avg: [-735.424 -735.424 -735.424] (1.000)
Step: 79499, Reward: [-416.366 -416.366 -416.366] [96.2168], Avg: [-735.284 -735.284 -735.284] (1.000)
Step: 79549, Reward: [-463.348 -463.348 -463.348] [98.7330], Avg: [-735.175 -735.175 -735.175] (1.000)
Step: 79599, Reward: [-422.001 -422.001 -422.001] [61.5778], Avg: [-735.017 -735.017 -735.017] (1.000)
Step: 79649, Reward: [-389.319 -389.319 -389.319] [52.2076], Avg: [-734.833 -734.833 -734.833] (1.000)
Step: 79699, Reward: [-492.617 -492.617 -492.617] [78.7377], Avg: [-734.73 -734.73 -734.73] (1.000)
Step: 79749, Reward: [-488.962 -488.962 -488.962] [120.8339], Avg: [-734.652 -734.652 -734.652] (1.000)
Step: 79799, Reward: [-440.058 -440.058 -440.058] [89.3242], Avg: [-734.523 -734.523 -734.523] (1.000)
Step: 79849, Reward: [-421.554 -421.554 -421.554] [81.9567], Avg: [-734.379 -734.379 -734.379] (1.000)
Step: 79899, Reward: [-477.89 -477.89 -477.89] [101.8271], Avg: [-734.282 -734.282 -734.282] (1.000)
Step: 79949, Reward: [-477.764 -477.764 -477.764] [70.2992], Avg: [-734.166 -734.166 -734.166] (1.000)
Step: 79999, Reward: [-422.618 -422.618 -422.618] [38.4829], Avg: [-733.995 -733.995 -733.995] (1.000)
Step: 80049, Reward: [-384.887 -384.887 -384.887] [58.5836], Avg: [-733.813 -733.813 -733.813] (1.000)
Step: 80099, Reward: [-508.675 -508.675 -508.675] [62.0763], Avg: [-733.712 -733.712 -733.712] (1.000)
Step: 80149, Reward: [-462.846 -462.846 -462.846] [35.8788], Avg: [-733.565 -733.565 -733.565] (1.000)
Step: 80199, Reward: [-395.379 -395.379 -395.379] [63.5949], Avg: [-733.394 -733.394 -733.394] (1.000)
Step: 80249, Reward: [-412.417 -412.417 -412.417] [91.9053], Avg: [-733.251 -733.251 -733.251] (1.000)
Step: 80299, Reward: [-510.321 -510.321 -510.321] [66.1365], Avg: [-733.153 -733.153 -733.153] (1.000)
Step: 80349, Reward: [-528.502 -528.502 -528.502] [84.5714], Avg: [-733.079 -733.079 -733.079] (1.000)
Step: 80399, Reward: [-360.513 -360.513 -360.513] [35.7188], Avg: [-732.869 -732.869 -732.869] (1.000)
Step: 80449, Reward: [-457.796 -457.796 -457.796] [45.5947], Avg: [-732.727 -732.727 -732.727] (1.000)
Step: 80499, Reward: [-453.235 -453.235 -453.235] [90.3710], Avg: [-732.609 -732.609 -732.609] (1.000)
Step: 80549, Reward: [-525.969 -525.969 -525.969] [110.2006], Avg: [-732.549 -732.549 -732.549] (1.000)
Step: 80599, Reward: [-473.862 -473.862 -473.862] [109.0011], Avg: [-732.456 -732.456 -732.456] (1.000)
Step: 80649, Reward: [-445.404 -445.404 -445.404] [82.9586], Avg: [-732.33 -732.33 -732.33] (1.000)
Step: 80699, Reward: [-436.76 -436.76 -436.76] [104.8551], Avg: [-732.212 -732.212 -732.212] (1.000)
Step: 80749, Reward: [-415.372 -415.372 -415.372] [66.2203], Avg: [-732.057 -732.057 -732.057] (1.000)
Step: 80799, Reward: [-410.517 -410.517 -410.517] [99.7125], Avg: [-731.919 -731.919 -731.919] (1.000)
Step: 80849, Reward: [-520.134 -520.134 -520.134] [70.6024], Avg: [-731.832 -731.832 -731.832] (1.000)
Step: 80899, Reward: [-448.138 -448.138 -448.138] [99.0793], Avg: [-731.718 -731.718 -731.718] (1.000)
Step: 80949, Reward: [-422.165 -422.165 -422.165] [42.3410], Avg: [-731.553 -731.553 -731.553] (1.000)
Step: 80999, Reward: [-457.386 -457.386 -457.386] [175.4375], Avg: [-731.492 -731.492 -731.492] (1.000)
Step: 81049, Reward: [-467.61 -467.61 -467.61] [114.2068], Avg: [-731.4 -731.4 -731.4] (1.000)
Step: 81099, Reward: [-468.343 -468.343 -468.343] [81.0456], Avg: [-731.287 -731.287 -731.287] (1.000)
Step: 81149, Reward: [-447.439 -447.439 -447.439] [89.0193], Avg: [-731.167 -731.167 -731.167] (1.000)
Step: 81199, Reward: [-518.163 -518.163 -518.163] [54.2131], Avg: [-731.07 -731.07 -731.07] (1.000)
Step: 81249, Reward: [-439.799 -439.799 -439.799] [98.8003], Avg: [-730.951 -730.951 -730.951] (1.000)
Step: 81299, Reward: [-484.736 -484.736 -484.736] [100.4771], Avg: [-730.861 -730.861 -730.861] (1.000)
Step: 81349, Reward: [-513.152 -513.152 -513.152] [112.9423], Avg: [-730.797 -730.797 -730.797] (1.000)
Step: 81399, Reward: [-439.843 -439.843 -439.843] [99.6850], Avg: [-730.68 -730.68 -730.68] (1.000)
Step: 81449, Reward: [-499.671 -499.671 -499.671] [90.3660], Avg: [-730.593 -730.593 -730.593] (1.000)
Step: 81499, Reward: [-394.412 -394.412 -394.412] [90.8197], Avg: [-730.443 -730.443 -730.443] (1.000)
Step: 81549, Reward: [-473.813 -473.813 -473.813] [63.3699], Avg: [-730.324 -730.324 -730.324] (1.000)
Step: 81599, Reward: [-456.657 -456.657 -456.657] [48.2212], Avg: [-730.186 -730.186 -730.186] (1.000)
Step: 81649, Reward: [-478.716 -478.716 -478.716] [51.1729], Avg: [-730.063 -730.063 -730.063] (1.000)
Step: 81699, Reward: [-462.675 -462.675 -462.675] [68.6452], Avg: [-729.942 -729.942 -729.942] (1.000)
Step: 81749, Reward: [-437.728 -437.728 -437.728] [44.5275], Avg: [-729.79 -729.79 -729.79] (1.000)
Step: 81799, Reward: [-427.228 -427.228 -427.228] [57.4039], Avg: [-729.64 -729.64 -729.64] (1.000)
Step: 81849, Reward: [-481.537 -481.537 -481.537] [69.5151], Avg: [-729.531 -729.531 -729.531] (1.000)
Step: 81899, Reward: [-461.557 -461.557 -461.557] [81.9184], Avg: [-729.418 -729.418 -729.418] (1.000)
Step: 81949, Reward: [-669.89 -669.89 -669.89] [154.7654], Avg: [-729.476 -729.476 -729.476] (1.000)
Step: 81999, Reward: [-469.157 -469.157 -469.157] [159.1338], Avg: [-729.414 -729.414 -729.414] (1.000)
Step: 82049, Reward: [-458.206 -458.206 -458.206] [52.3433], Avg: [-729.281 -729.281 -729.281] (1.000)
Step: 82099, Reward: [-496.308 -496.308 -496.308] [151.8776], Avg: [-729.231 -729.231 -729.231] (1.000)
Step: 82149, Reward: [-999.789 -999.789 -999.789] [304.0884], Avg: [-729.581 -729.581 -729.581] (1.000)
Step: 82199, Reward: [-502.135 -502.135 -502.135] [133.9046], Avg: [-729.524 -729.524 -729.524] (1.000)
Step: 82249, Reward: [-491.993 -491.993 -491.993] [144.5637], Avg: [-729.468 -729.468 -729.468] (1.000)
Step: 82299, Reward: [-470.52 -470.52 -470.52] [116.7385], Avg: [-729.381 -729.381 -729.381] (1.000)
Step: 82349, Reward: [-517.056 -517.056 -517.056] [92.7638], Avg: [-729.309 -729.309 -729.309] (1.000)
Step: 82399, Reward: [-475.15 -475.15 -475.15] [152.2671], Avg: [-729.247 -729.247 -729.247] (1.000)
Step: 82449, Reward: [-416.584 -416.584 -416.584] [32.3026], Avg: [-729.077 -729.077 -729.077] (1.000)
Step: 82499, Reward: [-442.687 -442.687 -442.687] [104.0191], Avg: [-728.966 -728.966 -728.966] (1.000)
Step: 82549, Reward: [-517.526 -517.526 -517.526] [70.7066], Avg: [-728.881 -728.881 -728.881] (1.000)
Step: 82599, Reward: [-428.172 -428.172 -428.172] [33.4870], Avg: [-728.719 -728.719 -728.719] (1.000)
Step: 82649, Reward: [-411.719 -411.719 -411.719] [103.5069], Avg: [-728.59 -728.59 -728.59] (1.000)
Step: 82699, Reward: [-422.896 -422.896 -422.896] [54.9502], Avg: [-728.439 -728.439 -728.439] (1.000)
Step: 82749, Reward: [-463.777 -463.777 -463.777] [68.2183], Avg: [-728.32 -728.32 -728.32] (1.000)
Step: 82799, Reward: [-411.343 -411.343 -411.343] [59.2480], Avg: [-728.164 -728.164 -728.164] (1.000)
Step: 82849, Reward: [-471.171 -471.171 -471.171] [46.2106], Avg: [-728.037 -728.037 -728.037] (1.000)
Step: 82899, Reward: [-513.904 -513.904 -513.904] [63.9608], Avg: [-727.947 -727.947 -727.947] (1.000)
Step: 82949, Reward: [-416.761 -416.761 -416.761] [68.9286], Avg: [-727.801 -727.801 -727.801] (1.000)
Step: 82999, Reward: [-458.329 -458.329 -458.329] [72.2098], Avg: [-727.682 -727.682 -727.682] (1.000)
Step: 83049, Reward: [-398.042 -398.042 -398.042] [66.2896], Avg: [-727.523 -727.523 -727.523] (1.000)
Step: 83099, Reward: [-406.462 -406.462 -406.462] [116.1516], Avg: [-727.4 -727.4 -727.4] (1.000)
Step: 83149, Reward: [-455.312 -455.312 -455.312] [89.6622], Avg: [-727.29 -727.29 -727.29] (1.000)
Step: 83199, Reward: [-353.357 -353.357 -353.357] [45.3633], Avg: [-727.093 -727.093 -727.093] (1.000)
Step: 83249, Reward: [-483.653 -483.653 -483.653] [113.1477], Avg: [-727.014 -727.014 -727.014] (1.000)
Step: 83299, Reward: [-475.72 -475.72 -475.72] [25.5602], Avg: [-726.879 -726.879 -726.879] (1.000)
Step: 83349, Reward: [-400.355 -400.355 -400.355] [50.7646], Avg: [-726.714 -726.714 -726.714] (1.000)
Step: 83399, Reward: [-385.472 -385.472 -385.472] [39.4519], Avg: [-726.533 -726.533 -726.533] (1.000)
Step: 83449, Reward: [-399.396 -399.396 -399.396] [48.3179], Avg: [-726.366 -726.366 -726.366] (1.000)
Step: 83499, Reward: [-469.005 -469.005 -469.005] [33.7969], Avg: [-726.232 -726.232 -726.232] (1.000)
Step: 83549, Reward: [-462.696 -462.696 -462.696] [48.1495], Avg: [-726.103 -726.103 -726.103] (1.000)
Step: 83599, Reward: [-416.177 -416.177 -416.177] [62.2226], Avg: [-725.955 -725.955 -725.955] (1.000)
Step: 83649, Reward: [-427.458 -427.458 -427.458] [38.2106], Avg: [-725.799 -725.799 -725.799] (1.000)
Step: 83699, Reward: [-466.15 -466.15 -466.15] [110.3242], Avg: [-725.71 -725.71 -725.71] (1.000)
Step: 83749, Reward: [-380.814 -380.814 -380.814] [77.0339], Avg: [-725.55 -725.55 -725.55] (1.000)
Step: 83799, Reward: [-413.706 -413.706 -413.706] [66.1394], Avg: [-725.403 -725.403 -725.403] (1.000)
Step: 83849, Reward: [-353.733 -353.733 -353.733] [52.1389], Avg: [-725.213 -725.213 -725.213] (1.000)
Step: 83899, Reward: [-389.335 -389.335 -389.335] [44.6036], Avg: [-725.039 -725.039 -725.039] (1.000)
Step: 83949, Reward: [-455.625 -455.625 -455.625] [48.9138], Avg: [-724.908 -724.908 -724.908] (1.000)
Step: 83999, Reward: [-382.882 -382.882 -382.882] [52.4457], Avg: [-724.736 -724.736 -724.736] (1.000)
Step: 84049, Reward: [-445.464 -445.464 -445.464] [24.6585], Avg: [-724.584 -724.584 -724.584] (1.000)
Step: 84099, Reward: [-386.633 -386.633 -386.633] [63.7180], Avg: [-724.421 -724.421 -724.421] (1.000)
Step: 84149, Reward: [-399.096 -399.096 -399.096] [50.6274], Avg: [-724.258 -724.258 -724.258] (1.000)
Step: 84199, Reward: [-455.934 -455.934 -455.934] [41.5071], Avg: [-724.123 -724.123 -724.123] (1.000)
Step: 84249, Reward: [-455.432 -455.432 -455.432] [53.9857], Avg: [-723.996 -723.996 -723.996] (1.000)
Step: 84299, Reward: [-471.87 -471.87 -471.87] [63.9581], Avg: [-723.884 -723.884 -723.884] (1.000)
Step: 84349, Reward: [-550.718 -550.718 -550.718] [48.7594], Avg: [-723.81 -723.81 -723.81] (1.000)
Step: 84399, Reward: [-439.774 -439.774 -439.774] [44.4037], Avg: [-723.668 -723.668 -723.668] (1.000)
Step: 84449, Reward: [-568.583 -568.583 -568.583] [97.5800], Avg: [-723.634 -723.634 -723.634] (1.000)
Step: 84499, Reward: [-537.322 -537.322 -537.322] [62.3614], Avg: [-723.561 -723.561 -723.561] (1.000)
Step: 84549, Reward: [-475.299 -475.299 -475.299] [56.9081], Avg: [-723.448 -723.448 -723.448] (1.000)
Step: 84599, Reward: [-420.093 -420.093 -420.093] [39.2967], Avg: [-723.292 -723.292 -723.292] (1.000)
Step: 84649, Reward: [-466.526 -466.526 -466.526] [97.0599], Avg: [-723.197 -723.197 -723.197] (1.000)
Step: 84699, Reward: [-445.576 -445.576 -445.576] [35.7831], Avg: [-723.055 -723.055 -723.055] (1.000)
Step: 84749, Reward: [-425.171 -425.171 -425.171] [109.8121], Avg: [-722.944 -722.944 -722.944] (1.000)
Step: 84799, Reward: [-420.675 -420.675 -420.675] [41.0074], Avg: [-722.79 -722.79 -722.79] (1.000)
Step: 84849, Reward: [-382.688 -382.688 -382.688] [45.6843], Avg: [-722.616 -722.616 -722.616] (1.000)
Step: 84899, Reward: [-375.492 -375.492 -375.492] [69.9291], Avg: [-722.453 -722.453 -722.453] (1.000)
Step: 84949, Reward: [-374.802 -374.802 -374.802] [58.3041], Avg: [-722.283 -722.283 -722.283] (1.000)
Step: 84999, Reward: [-451.007 -451.007 -451.007] [80.7106], Avg: [-722.171 -722.171 -722.171] (1.000)
Step: 85049, Reward: [-389.779 -389.779 -389.779] [60.1458], Avg: [-722.01 -722.01 -722.01] (1.000)
Step: 85099, Reward: [-415.572 -415.572 -415.572] [45.9736], Avg: [-721.857 -721.857 -721.857] (1.000)
Step: 85149, Reward: [-437.356 -437.356 -437.356] [44.6258], Avg: [-721.717 -721.717 -721.717] (1.000)
Step: 85199, Reward: [-380.551 -380.551 -380.551] [44.7157], Avg: [-721.543 -721.543 -721.543] (1.000)
Step: 85249, Reward: [-438.531 -438.531 -438.531] [55.3960], Avg: [-721.409 -721.409 -721.409] (1.000)
Step: 85299, Reward: [-401.806 -401.806 -401.806] [37.2030], Avg: [-721.244 -721.244 -721.244] (1.000)
Step: 85349, Reward: [-439.881 -439.881 -439.881] [75.4061], Avg: [-721.123 -721.123 -721.123] (1.000)
Step: 85399, Reward: [-402.729 -402.729 -402.729] [46.2008], Avg: [-720.964 -720.964 -720.964] (1.000)
Step: 85449, Reward: [-411.76 -411.76 -411.76] [45.7478], Avg: [-720.809 -720.809 -720.809] (1.000)
Step: 85499, Reward: [-381.765 -381.765 -381.765] [55.5028], Avg: [-720.644 -720.644 -720.644] (1.000)
Step: 85549, Reward: [-319.605 -319.605 -319.605] [58.6251], Avg: [-720.443 -720.443 -720.443] (1.000)
Step: 85599, Reward: [-364.334 -364.334 -364.334] [71.0885], Avg: [-720.277 -720.277 -720.277] (1.000)
Step: 85649, Reward: [-398.534 -398.534 -398.534] [68.1167], Avg: [-720.129 -720.129 -720.129] (1.000)
Step: 85699, Reward: [-420.168 -420.168 -420.168] [54.5371], Avg: [-719.986 -719.986 -719.986] (1.000)
Step: 85749, Reward: [-353.723 -353.723 -353.723] [30.3539], Avg: [-719.79 -719.79 -719.79] (1.000)
Step: 85799, Reward: [-379.582 -379.582 -379.582] [64.2507], Avg: [-719.629 -719.629 -719.629] (1.000)
Step: 85849, Reward: [-397.395 -397.395 -397.395] [20.9591], Avg: [-719.454 -719.454 -719.454] (1.000)
Step: 85899, Reward: [-422.785 -422.785 -422.785] [52.9100], Avg: [-719.312 -719.312 -719.312] (1.000)
Step: 85949, Reward: [-380.272 -380.272 -380.272] [80.9664], Avg: [-719.162 -719.162 -719.162] (1.000)
Step: 85999, Reward: [-360.727 -360.727 -360.727] [22.2855], Avg: [-718.966 -718.966 -718.966] (1.000)
Step: 86049, Reward: [-424.247 -424.247 -424.247] [67.2076], Avg: [-718.834 -718.834 -718.834] (1.000)
Step: 86099, Reward: [-382.076 -382.076 -382.076] [89.6277], Avg: [-718.69 -718.69 -718.69] (1.000)
Step: 86149, Reward: [-468.442 -468.442 -468.442] [37.6641], Avg: [-718.567 -718.567 -718.567] (1.000)
Step: 86199, Reward: [-495.809 -495.809 -495.809] [81.1017], Avg: [-718.485 -718.485 -718.485] (1.000)
Step: 86249, Reward: [-405.154 -405.154 -405.154] [31.3680], Avg: [-718.321 -718.321 -718.321] (1.000)
Step: 86299, Reward: [-445.722 -445.722 -445.722] [94.3732], Avg: [-718.218 -718.218 -718.218] (1.000)
Step: 86349, Reward: [-418.538 -418.538 -418.538] [60.1378], Avg: [-718.079 -718.079 -718.079] (1.000)
Step: 86399, Reward: [-412.926 -412.926 -412.926] [103.3502], Avg: [-717.963 -717.963 -717.963] (1.000)
Step: 86449, Reward: [-413.615 -413.615 -413.615] [107.1560], Avg: [-717.849 -717.849 -717.849] (1.000)
Step: 86499, Reward: [-458.339 -458.339 -458.339] [39.5019], Avg: [-717.721 -717.721 -717.721] (1.000)
Step: 86549, Reward: [-510.343 -510.343 -510.343] [98.5688], Avg: [-717.659 -717.659 -717.659] (1.000)
Step: 86599, Reward: [-556.303 -556.303 -556.303] [118.7736], Avg: [-717.634 -717.634 -717.634] (1.000)
Step: 86649, Reward: [-714.395 -714.395 -714.395] [262.4296], Avg: [-717.784 -717.784 -717.784] (1.000)
Step: 86699, Reward: [-991.064 -991.064 -991.064] [184.4709], Avg: [-718.048 -718.048 -718.048] (1.000)
Step: 86749, Reward: [-873.654 -873.654 -873.654] [162.9960], Avg: [-718.231 -718.231 -718.231] (1.000)
Step: 86799, Reward: [-626.839 -626.839 -626.839] [109.7306], Avg: [-718.242 -718.242 -718.242] (1.000)
Step: 86849, Reward: [-995.685 -995.685 -995.685] [139.7438], Avg: [-718.482 -718.482 -718.482] (1.000)
Step: 86899, Reward: [-738.573 -738.573 -738.573] [136.5507], Avg: [-718.572 -718.572 -718.572] (1.000)
Step: 86949, Reward: [-691.527 -691.527 -691.527] [159.3851], Avg: [-718.648 -718.648 -718.648] (1.000)
Step: 86999, Reward: [-593.108 -593.108 -593.108] [70.5262], Avg: [-718.617 -718.617 -718.617] (1.000)
Step: 87049, Reward: [-517.437 -517.437 -517.437] [66.5892], Avg: [-718.539 -718.539 -718.539] (1.000)
Step: 87099, Reward: [-504.859 -504.859 -504.859] [94.1661], Avg: [-718.471 -718.471 -718.471] (1.000)
Step: 87149, Reward: [-425.054 -425.054 -425.054] [68.7614], Avg: [-718.342 -718.342 -718.342] (1.000)
Step: 87199, Reward: [-429.533 -429.533 -429.533] [49.3021], Avg: [-718.204 -718.204 -718.204] (1.000)
Step: 87249, Reward: [-364.614 -364.614 -364.614] [42.5153], Avg: [-718.026 -718.026 -718.026] (1.000)
Step: 87299, Reward: [-381.698 -381.698 -381.698] [43.2986], Avg: [-717.858 -717.858 -717.858] (1.000)
Step: 87349, Reward: [-373.274 -373.274 -373.274] [78.5813], Avg: [-717.706 -717.706 -717.706] (1.000)
Step: 87399, Reward: [-492.908 -492.908 -492.908] [43.6190], Avg: [-717.602 -717.602 -717.602] (1.000)
Step: 87449, Reward: [-398.623 -398.623 -398.623] [36.6067], Avg: [-717.441 -717.441 -717.441] (1.000)
Step: 87499, Reward: [-358.063 -358.063 -358.063] [63.5070], Avg: [-717.272 -717.272 -717.272] (1.000)
Step: 87549, Reward: [-325.396 -325.396 -325.396] [12.6525], Avg: [-717.055 -717.055 -717.055] (1.000)
Step: 87599, Reward: [-373.78 -373.78 -373.78] [53.5342], Avg: [-716.89 -716.89 -716.89] (1.000)
Step: 87649, Reward: [-334.498 -334.498 -334.498] [44.1664], Avg: [-716.697 -716.697 -716.697] (1.000)
Step: 87699, Reward: [-368.364 -368.364 -368.364] [62.0010], Avg: [-716.534 -716.534 -716.534] (1.000)
Step: 87749, Reward: [-398.904 -398.904 -398.904] [40.1498], Avg: [-716.376 -716.376 -716.376] (1.000)
Step: 87799, Reward: [-420.633 -420.633 -420.633] [81.5041], Avg: [-716.254 -716.254 -716.254] (1.000)
Step: 87849, Reward: [-400.595 -400.595 -400.595] [44.4053], Avg: [-716.099 -716.099 -716.099] (1.000)
Step: 87899, Reward: [-412.957 -412.957 -412.957] [76.2264], Avg: [-715.97 -715.97 -715.97] (1.000)
Step: 87949, Reward: [-459.458 -459.458 -459.458] [75.2059], Avg: [-715.867 -715.867 -715.867] (1.000)
Step: 87999, Reward: [-423.089 -423.089 -423.089] [50.9508], Avg: [-715.73 -715.73 -715.73] (1.000)
Step: 88049, Reward: [-384.724 -384.724 -384.724] [81.8131], Avg: [-715.588 -715.588 -715.588] (1.000)
Step: 88099, Reward: [-386.376 -386.376 -386.376] [45.9108], Avg: [-715.427 -715.427 -715.427] (1.000)
Step: 88149, Reward: [-378.046 -378.046 -378.046] [48.0221], Avg: [-715.263 -715.263 -715.263] (1.000)
Step: 88199, Reward: [-447.478 -447.478 -447.478] [46.3902], Avg: [-715.138 -715.138 -715.138] (1.000)
Step: 88249, Reward: [-369.374 -369.374 -369.374] [68.0121], Avg: [-714.98 -714.98 -714.98] (1.000)
Step: 88299, Reward: [-428.086 -428.086 -428.086] [94.6573], Avg: [-714.872 -714.872 -714.872] (1.000)
Step: 88349, Reward: [-397.994 -397.994 -397.994] [46.6041], Avg: [-714.719 -714.719 -714.719] (1.000)
Step: 88399, Reward: [-425.239 -425.239 -425.239] [50.2035], Avg: [-714.583 -714.583 -714.583] (1.000)
Step: 88449, Reward: [-358.779 -358.779 -358.779] [30.9719], Avg: [-714.4 -714.4 -714.4] (1.000)
Step: 88499, Reward: [-392.7 -392.7 -392.7] [39.3093], Avg: [-714.24 -714.24 -714.24] (1.000)
Step: 88549, Reward: [-463.646 -463.646 -463.646] [136.1247], Avg: [-714.175 -714.175 -714.175] (1.000)
Step: 88599, Reward: [-411.035 -411.035 -411.035] [67.5023], Avg: [-714.042 -714.042 -714.042] (1.000)
Step: 88649, Reward: [-348.158 -348.158 -348.158] [48.9589], Avg: [-713.864 -713.864 -713.864] (1.000)
Step: 88699, Reward: [-395.659 -395.659 -395.659] [58.5442], Avg: [-713.717 -713.717 -713.717] (1.000)
Step: 88749, Reward: [-472.627 -472.627 -472.627] [144.1226], Avg: [-713.663 -713.663 -713.663] (1.000)
Step: 88799, Reward: [-440.819 -440.819 -440.819] [20.7834], Avg: [-713.521 -713.521 -713.521] (1.000)
Step: 88849, Reward: [-456.952 -456.952 -456.952] [85.0691], Avg: [-713.424 -713.424 -713.424] (1.000)
Step: 88899, Reward: [-376.683 -376.683 -376.683] [46.3498], Avg: [-713.261 -713.261 -713.261] (1.000)
Step: 88949, Reward: [-377.116 -377.116 -377.116] [70.9130], Avg: [-713.112 -713.112 -713.112] (1.000)
Step: 88999, Reward: [-380.414 -380.414 -380.414] [58.4188], Avg: [-712.958 -712.958 -712.958] (1.000)
Step: 89049, Reward: [-385.553 -385.553 -385.553] [102.4183], Avg: [-712.831 -712.831 -712.831] (1.000)
Step: 89099, Reward: [-451.167 -451.167 -451.167] [104.6828], Avg: [-712.743 -712.743 -712.743] (1.000)
Step: 89149, Reward: [-487.697 -487.697 -487.697] [87.9855], Avg: [-712.666 -712.666 -712.666] (1.000)
Step: 89199, Reward: [-433.017 -433.017 -433.017] [120.5143], Avg: [-712.577 -712.577 -712.577] (1.000)
Step: 89249, Reward: [-493.332 -493.332 -493.332] [54.3845], Avg: [-712.485 -712.485 -712.485] (1.000)
Step: 89299, Reward: [-497.944 -497.944 -497.944] [63.0680], Avg: [-712.4 -712.4 -712.4] (1.000)
Step: 89349, Reward: [-524.44 -524.44 -524.44] [65.4484], Avg: [-712.332 -712.332 -712.332] (1.000)
Step: 89399, Reward: [-676.582 -676.582 -676.582] [249.0784], Avg: [-712.451 -712.451 -712.451] (1.000)
Step: 89449, Reward: [-641.588 -641.588 -641.588] [187.2869], Avg: [-712.516 -712.516 -712.516] (1.000)
Step: 89499, Reward: [-499.709 -499.709 -499.709] [67.4368], Avg: [-712.435 -712.435 -712.435] (1.000)
Step: 89549, Reward: [-406.64 -406.64 -406.64] [63.1626], Avg: [-712.299 -712.299 -712.299] (1.000)
Step: 89599, Reward: [-450.052 -450.052 -450.052] [121.7002], Avg: [-712.221 -712.221 -712.221] (1.000)
Step: 89649, Reward: [-602.614 -602.614 -602.614] [197.5009], Avg: [-712.27 -712.27 -712.27] (1.000)
Step: 89699, Reward: [-630.913 -630.913 -630.913] [199.5269], Avg: [-712.336 -712.336 -712.336] (1.000)
Step: 89749, Reward: [-701.049 -701.049 -701.049] [151.6313], Avg: [-712.414 -712.414 -712.414] (1.000)
Step: 89799, Reward: [-608.569 -608.569 -608.569] [120.8678], Avg: [-712.423 -712.423 -712.423] (1.000)
Step: 89849, Reward: [-503.448 -503.448 -503.448] [174.1257], Avg: [-712.404 -712.404 -712.404] (1.000)
Step: 89899, Reward: [-535.663 -535.663 -535.663] [138.1888], Avg: [-712.383 -712.383 -712.383] (1.000)
Step: 89949, Reward: [-663.174 -663.174 -663.174] [67.6557], Avg: [-712.393 -712.393 -712.393] (1.000)
Step: 89999, Reward: [-531.225 -531.225 -531.225] [119.7021], Avg: [-712.359 -712.359 -712.359] (1.000)
Step: 90049, Reward: [-460.375 -460.375 -460.375] [117.4162], Avg: [-712.284 -712.284 -712.284] (1.000)
Step: 90099, Reward: [-1036.8 -1036.8 -1036.8] [182.1605], Avg: [-712.565 -712.565 -712.565] (1.000)
Step: 90149, Reward: [-821.459 -821.459 -821.459] [322.9107], Avg: [-712.805 -712.805 -712.805] (1.000)
Step: 90199, Reward: [-676.84 -676.84 -676.84] [135.6979], Avg: [-712.86 -712.86 -712.86] (1.000)
Step: 90249, Reward: [-544.955 -544.955 -544.955] [139.7055], Avg: [-712.844 -712.844 -712.844] (1.000)
Step: 90299, Reward: [-687.008 -687.008 -687.008] [210.9865], Avg: [-712.947 -712.947 -712.947] (1.000)
Step: 90349, Reward: [-637.173 -637.173 -637.173] [86.7730], Avg: [-712.953 -712.953 -712.953] (1.000)
Step: 90399, Reward: [-521.295 -521.295 -521.295] [172.7319], Avg: [-712.942 -712.942 -712.942] (1.000)
Step: 90449, Reward: [-588.672 -588.672 -588.672] [20.0654], Avg: [-712.885 -712.885 -712.885] (1.000)
Step: 90499, Reward: [-487.277 -487.277 -487.277] [108.2458], Avg: [-712.82 -712.82 -712.82] (1.000)
Step: 90549, Reward: [-524.093 -524.093 -524.093] [131.6701], Avg: [-712.788 -712.788 -712.788] (1.000)
Step: 90599, Reward: [-789.926 -789.926 -789.926] [262.1517], Avg: [-712.976 -712.976 -712.976] (1.000)
Step: 90649, Reward: [-733.572 -733.572 -733.572] [310.9918], Avg: [-713.159 -713.159 -713.159] (1.000)
Step: 90699, Reward: [-707.799 -707.799 -707.799] [220.5839], Avg: [-713.277 -713.277 -713.277] (1.000)
Step: 90749, Reward: [-905.334 -905.334 -905.334] [332.8723], Avg: [-713.566 -713.566 -713.566] (1.000)
Step: 90799, Reward: [-840.599 -840.599 -840.599] [319.7267], Avg: [-713.812 -713.812 -713.812] (1.000)
Step: 90849, Reward: [-764.757 -764.757 -764.757] [167.2987], Avg: [-713.933 -713.933 -713.933] (1.000)
Step: 90899, Reward: [-549.92 -549.92 -549.92] [39.4317], Avg: [-713.864 -713.864 -713.864] (1.000)
Step: 90949, Reward: [-785.039 -785.039 -785.039] [320.8373], Avg: [-714.08 -714.08 -714.08] (1.000)
Step: 90999, Reward: [-1024.72 -1024.72 -1024.72] [267.5191], Avg: [-714.397 -714.397 -714.397] (1.000)
Step: 91049, Reward: [-926.912 -926.912 -926.912] [277.3354], Avg: [-714.666 -714.666 -714.666] (1.000)
Step: 91099, Reward: [-757.956 -757.956 -757.956] [231.6434], Avg: [-714.817 -714.817 -714.817] (1.000)
Step: 91149, Reward: [-1083.111 -1083.111 -1083.111] [147.4719], Avg: [-715.1 -715.1 -715.1] (1.000)
Step: 91199, Reward: [-627.224 -627.224 -627.224] [161.9733], Avg: [-715.141 -715.141 -715.141] (1.000)
Step: 91249, Reward: [-732.317 -732.317 -732.317] [254.5177], Avg: [-715.29 -715.29 -715.29] (1.000)
Step: 91299, Reward: [-618.566 -618.566 -618.566] [160.2131], Avg: [-715.324 -715.324 -715.324] (1.000)
Step: 91349, Reward: [-801.929 -801.929 -801.929] [167.1778], Avg: [-715.463 -715.463 -715.463] (1.000)
Step: 91399, Reward: [-638.888 -638.888 -638.888] [189.9464], Avg: [-715.525 -715.525 -715.525] (1.000)
Step: 91449, Reward: [-598.511 -598.511 -598.511] [189.7729], Avg: [-715.565 -715.565 -715.565] (1.000)
Step: 91499, Reward: [-589.537 -589.537 -589.537] [167.6813], Avg: [-715.588 -715.588 -715.588] (1.000)
Step: 91549, Reward: [-808.18 -808.18 -808.18] [279.0161], Avg: [-715.791 -715.791 -715.791] (1.000)
Step: 91599, Reward: [-746.13 -746.13 -746.13] [376.5578], Avg: [-716.013 -716.013 -716.013] (1.000)
Step: 91649, Reward: [-679.783 -679.783 -679.783] [159.1079], Avg: [-716.08 -716.08 -716.08] (1.000)
Step: 91699, Reward: [-595.947 -595.947 -595.947] [30.0100], Avg: [-716.031 -716.031 -716.031] (1.000)
Step: 91749, Reward: [-574.269 -574.269 -574.269] [117.9314], Avg: [-716.018 -716.018 -716.018] (1.000)
Step: 91799, Reward: [-614.109 -614.109 -614.109] [159.0696], Avg: [-716.049 -716.049 -716.049] (1.000)
Step: 91849, Reward: [-469.855 -469.855 -469.855] [53.1067], Avg: [-715.944 -715.944 -715.944] (1.000)
Step: 91899, Reward: [-438.577 -438.577 -438.577] [50.4753], Avg: [-715.82 -715.82 -715.82] (1.000)
Step: 91949, Reward: [-518.092 -518.092 -518.092] [129.8629], Avg: [-715.783 -715.783 -715.783] (1.000)
Step: 91999, Reward: [-409.136 -409.136 -409.136] [61.2201], Avg: [-715.65 -715.65 -715.65] (1.000)
Step: 92049, Reward: [-552.614 -552.614 -552.614] [109.2960], Avg: [-715.621 -715.621 -715.621] (1.000)
Step: 92099, Reward: [-553.14 -553.14 -553.14] [158.2095], Avg: [-715.619 -715.619 -715.619] (1.000)
Step: 92149, Reward: [-402.014 -402.014 -402.014] [76.9752], Avg: [-715.49 -715.49 -715.49] (1.000)
Step: 92199, Reward: [-422.156 -422.156 -422.156] [37.1626], Avg: [-715.351 -715.351 -715.351] (1.000)
Step: 92249, Reward: [-404.089 -404.089 -404.089] [71.0863], Avg: [-715.221 -715.221 -715.221] (1.000)
Step: 92299, Reward: [-430.434 -430.434 -430.434] [64.6771], Avg: [-715.102 -715.102 -715.102] (1.000)
Step: 92349, Reward: [-435.184 -435.184 -435.184] [48.7768], Avg: [-714.977 -714.977 -714.977] (1.000)
Step: 92399, Reward: [-506.765 -506.765 -506.765] [131.2048], Avg: [-714.935 -714.935 -714.935] (1.000)
Step: 92449, Reward: [-420.72 -420.72 -420.72] [44.7095], Avg: [-714.8 -714.8 -714.8] (1.000)
Step: 92499, Reward: [-492.15 -492.15 -492.15] [60.6643], Avg: [-714.712 -714.712 -714.712] (1.000)
Step: 92549, Reward: [-548.563 -548.563 -548.563] [84.9236], Avg: [-714.669 -714.669 -714.669] (1.000)
Step: 92599, Reward: [-507.038 -507.038 -507.038] [134.9914], Avg: [-714.629 -714.629 -714.629] (1.000)
Step: 92649, Reward: [-447.447 -447.447 -447.447] [83.9106], Avg: [-714.53 -714.53 -714.53] (1.000)
Step: 92699, Reward: [-441.866 -441.866 -441.866] [54.8942], Avg: [-714.413 -714.413 -714.413] (1.000)
Step: 92749, Reward: [-456.26 -456.26 -456.26] [46.0737], Avg: [-714.299 -714.299 -714.299] (1.000)
Step: 92799, Reward: [-453.809 -453.809 -453.809] [67.2372], Avg: [-714.195 -714.195 -714.195] (1.000)
Step: 92849, Reward: [-469.789 -469.789 -469.789] [60.8147], Avg: [-714.096 -714.096 -714.096] (1.000)
Step: 92899, Reward: [-442.956 -442.956 -442.956] [45.5239], Avg: [-713.974 -713.974 -713.974] (1.000)
Step: 92949, Reward: [-388.858 -388.858 -388.858] [67.0655], Avg: [-713.835 -713.835 -713.835] (1.000)
Step: 92999, Reward: [-464.612 -464.612 -464.612] [60.6249], Avg: [-713.734 -713.734 -713.734] (1.000)
Step: 93049, Reward: [-384.703 -384.703 -384.703] [63.2949], Avg: [-713.591 -713.591 -713.591] (1.000)
Step: 93099, Reward: [-454.733 -454.733 -454.733] [23.8518], Avg: [-713.465 -713.465 -713.465] (1.000)
Step: 93149, Reward: [-480.556 -480.556 -480.556] [94.4155], Avg: [-713.391 -713.391 -713.391] (1.000)
Step: 93199, Reward: [-495.294 -495.294 -495.294] [77.0445], Avg: [-713.315 -713.315 -713.315] (1.000)
Step: 93249, Reward: [-449.073 -449.073 -449.073] [72.4361], Avg: [-713.212 -713.212 -713.212] (1.000)
Step: 93299, Reward: [-436.094 -436.094 -436.094] [41.0973], Avg: [-713.086 -713.086 -713.086] (1.000)
Step: 93349, Reward: [-474.643 -474.643 -474.643] [91.3415], Avg: [-713.007 -713.007 -713.007] (1.000)
Step: 93399, Reward: [-430.821 -430.821 -430.821] [71.5797], Avg: [-712.894 -712.894 -712.894] (1.000)
Step: 93449, Reward: [-421.119 -421.119 -421.119] [80.8184], Avg: [-712.781 -712.781 -712.781] (1.000)
Step: 93499, Reward: [-437.232 -437.232 -437.232] [48.0127], Avg: [-712.66 -712.66 -712.66] (1.000)
Step: 93549, Reward: [-449.695 -449.695 -449.695] [56.7105], Avg: [-712.549 -712.549 -712.549] (1.000)
Step: 93599, Reward: [-432.312 -432.312 -432.312] [70.2529], Avg: [-712.437 -712.437 -712.437] (1.000)
Step: 93649, Reward: [-458.505 -458.505 -458.505] [53.9543], Avg: [-712.33 -712.33 -712.33] (1.000)
Step: 93699, Reward: [-480.892 -480.892 -480.892] [150.1437], Avg: [-712.287 -712.287 -712.287] (1.000)
Step: 93749, Reward: [-469.945 -469.945 -469.945] [113.3669], Avg: [-712.218 -712.218 -712.218] (1.000)
Step: 93799, Reward: [-430.636 -430.636 -430.636] [81.3232], Avg: [-712.112 -712.112 -712.112] (1.000)
Step: 93849, Reward: [-499.317 -499.317 -499.317] [125.0061], Avg: [-712.065 -712.065 -712.065] (1.000)
Step: 93899, Reward: [-399.928 -399.928 -399.928] [54.9485], Avg: [-711.928 -711.928 -711.928] (1.000)
Step: 93949, Reward: [-457.904 -457.904 -457.904] [90.4960], Avg: [-711.841 -711.841 -711.841] (1.000)
Step: 93999, Reward: [-449.429 -449.429 -449.429] [87.3634], Avg: [-711.748 -711.748 -711.748] (1.000)
Step: 94049, Reward: [-473.868 -473.868 -473.868] [153.2908], Avg: [-711.703 -711.703 -711.703] (1.000)
Step: 94099, Reward: [-514.745 -514.745 -514.745] [113.2914], Avg: [-711.658 -711.658 -711.658] (1.000)
Step: 94149, Reward: [-457.982 -457.982 -457.982] [73.9827], Avg: [-711.563 -711.563 -711.563] (1.000)
Step: 94199, Reward: [-468.742 -468.742 -468.742] [68.5244], Avg: [-711.47 -711.47 -711.47] (1.000)
Step: 94249, Reward: [-427.953 -427.953 -427.953] [86.9085], Avg: [-711.366 -711.366 -711.366] (1.000)
Step: 94299, Reward: [-407.636 -407.636 -407.636] [75.5329], Avg: [-711.245 -711.245 -711.245] (1.000)
Step: 94349, Reward: [-437.396 -437.396 -437.396] [81.0727], Avg: [-711.143 -711.143 -711.143] (1.000)
Step: 94399, Reward: [-433.066 -433.066 -433.066] [77.4968], Avg: [-711.037 -711.037 -711.037] (1.000)
Step: 94449, Reward: [-389.043 -389.043 -389.043] [47.3719], Avg: [-710.891 -710.891 -710.891] (1.000)
Step: 94499, Reward: [-338.065 -338.065 -338.065] [68.6888], Avg: [-710.73 -710.73 -710.73] (1.000)
Step: 94549, Reward: [-423.028 -423.028 -423.028] [29.1255], Avg: [-710.594 -710.594 -710.594] (1.000)
Step: 94599, Reward: [-379.566 -379.566 -379.566] [57.3708], Avg: [-710.449 -710.449 -710.449] (1.000)
Step: 94649, Reward: [-477.044 -477.044 -477.044] [94.1730], Avg: [-710.375 -710.375 -710.375] (1.000)
Step: 94699, Reward: [-421.402 -421.402 -421.402] [41.8407], Avg: [-710.245 -710.245 -710.245] (1.000)
Step: 94749, Reward: [-425.028 -425.028 -425.028] [33.6272], Avg: [-710.112 -710.112 -710.112] (1.000)
Step: 94799, Reward: [-377.418 -377.418 -377.418] [45.8580], Avg: [-709.961 -709.961 -709.961] (1.000)
Step: 94849, Reward: [-404.004 -404.004 -404.004] [70.8628], Avg: [-709.837 -709.837 -709.837] (1.000)
Step: 94899, Reward: [-353.166 -353.166 -353.166] [72.2035], Avg: [-709.687 -709.687 -709.687] (1.000)
Step: 94949, Reward: [-455.182 -455.182 -455.182] [40.1944], Avg: [-709.574 -709.574 -709.574] (1.000)
Step: 94999, Reward: [-378.236 -378.236 -378.236] [49.0499], Avg: [-709.426 -709.426 -709.426] (1.000)
Step: 95049, Reward: [-412.337 -412.337 -412.337] [73.4127], Avg: [-709.308 -709.308 -709.308] (1.000)
Step: 95099, Reward: [-438.159 -438.159 -438.159] [81.6879], Avg: [-709.208 -709.208 -709.208] (1.000)
Step: 95149, Reward: [-398.886 -398.886 -398.886] [74.7779], Avg: [-709.085 -709.085 -709.085] (1.000)
Step: 95199, Reward: [-399.262 -399.262 -399.262] [28.7258], Avg: [-708.937 -708.937 -708.937] (1.000)
Step: 95249, Reward: [-401.126 -401.126 -401.126] [48.4004], Avg: [-708.801 -708.801 -708.801] (1.000)
Step: 95299, Reward: [-426.811 -426.811 -426.811] [94.2749], Avg: [-708.702 -708.702 -708.702] (1.000)
Step: 95349, Reward: [-351.258 -351.258 -351.258] [53.2385], Avg: [-708.543 -708.543 -708.543] (1.000)
Step: 95399, Reward: [-371.466 -371.466 -371.466] [75.3368], Avg: [-708.406 -708.406 -708.406] (1.000)
Step: 95449, Reward: [-391.321 -391.321 -391.321] [28.9091], Avg: [-708.255 -708.255 -708.255] (1.000)
Step: 95499, Reward: [-399.587 -399.587 -399.587] [62.2045], Avg: [-708.126 -708.126 -708.126] (1.000)
Step: 95549, Reward: [-488.83 -488.83 -488.83] [117.0155], Avg: [-708.072 -708.072 -708.072] (1.000)
Step: 95599, Reward: [-400.843 -400.843 -400.843] [49.9480], Avg: [-707.938 -707.938 -707.938] (1.000)
Step: 95649, Reward: [-409.112 -409.112 -409.112] [92.9652], Avg: [-707.83 -707.83 -707.83] (1.000)
Step: 95699, Reward: [-459.272 -459.272 -459.272] [48.3563], Avg: [-707.725 -707.725 -707.725] (1.000)
Step: 95749, Reward: [-439.525 -439.525 -439.525] [17.1488], Avg: [-707.594 -707.594 -707.594] (1.000)
Step: 95799, Reward: [-447.849 -447.849 -447.849] [85.2153], Avg: [-707.503 -707.503 -707.503] (1.000)
Step: 95849, Reward: [-451.664 -451.664 -451.664] [101.3932], Avg: [-707.423 -707.423 -707.423] (1.000)
Step: 95899, Reward: [-425.379 -425.379 -425.379] [17.3987], Avg: [-707.285 -707.285 -707.285] (1.000)
Step: 95949, Reward: [-401.128 -401.128 -401.128] [56.2008], Avg: [-707.154 -707.154 -707.154] (1.000)
Step: 95999, Reward: [-413.521 -413.521 -413.521] [81.6478], Avg: [-707.044 -707.044 -707.044] (1.000)
Step: 96049, Reward: [-358.585 -358.585 -358.585] [52.6670], Avg: [-706.89 -706.89 -706.89] (1.000)
Step: 96099, Reward: [-449.438 -449.438 -449.438] [26.0083], Avg: [-706.77 -706.77 -706.77] (1.000)
Step: 96149, Reward: [-445.357 -445.357 -445.357] [112.0276], Avg: [-706.692 -706.692 -706.692] (1.000)
Step: 96199, Reward: [-435.927 -435.927 -435.927] [105.8620], Avg: [-706.606 -706.606 -706.606] (1.000)
Step: 96249, Reward: [-434.622 -434.622 -434.622] [53.7476], Avg: [-706.493 -706.493 -706.493] (1.000)
Step: 96299, Reward: [-417.233 -417.233 -417.233] [75.5006], Avg: [-706.382 -706.382 -706.382] (1.000)
Step: 96349, Reward: [-440.847 -440.847 -440.847] [54.7539], Avg: [-706.272 -706.272 -706.272] (1.000)
Step: 96399, Reward: [-441.597 -441.597 -441.597] [68.1562], Avg: [-706.17 -706.17 -706.17] (1.000)
Step: 96449, Reward: [-389.479 -389.479 -389.479] [47.3181], Avg: [-706.031 -706.031 -706.031] (1.000)
Step: 96499, Reward: [-398.493 -398.493 -398.493] [68.5925], Avg: [-705.907 -705.907 -705.907] (1.000)
Step: 96549, Reward: [-476.016 -476.016 -476.016] [80.6254], Avg: [-705.83 -705.83 -705.83] (1.000)
Step: 96599, Reward: [-405.088 -405.088 -405.088] [67.4131], Avg: [-705.709 -705.709 -705.709] (1.000)
Step: 96649, Reward: [-419.94 -419.94 -419.94] [49.0875], Avg: [-705.586 -705.586 -705.586] (1.000)
Step: 96699, Reward: [-395.383 -395.383 -395.383] [91.5005], Avg: [-705.473 -705.473 -705.473] (1.000)
Step: 96749, Reward: [-378.376 -378.376 -378.376] [40.4091], Avg: [-705.325 -705.325 -705.325] (1.000)
Step: 96799, Reward: [-468.28 -468.28 -468.28] [130.0218], Avg: [-705.27 -705.27 -705.27] (1.000)
Step: 96849, Reward: [-505.497 -505.497 -505.497] [43.6632], Avg: [-705.189 -705.189 -705.189] (1.000)
Step: 96899, Reward: [-478.133 -478.133 -478.133] [126.5365], Avg: [-705.137 -705.137 -705.137] (1.000)
Step: 96949, Reward: [-409.554 -409.554 -409.554] [54.1021], Avg: [-705.013 -705.013 -705.013] (1.000)
Step: 96999, Reward: [-409.24 -409.24 -409.24] [78.7869], Avg: [-704.901 -704.901 -704.901] (1.000)
Step: 97049, Reward: [-421.702 -421.702 -421.702] [76.5640], Avg: [-704.795 -704.795 -704.795] (1.000)
Step: 97099, Reward: [-436.901 -436.901 -436.901] [57.2610], Avg: [-704.686 -704.686 -704.686] (1.000)
Step: 97149, Reward: [-461.591 -461.591 -461.591] [57.9657], Avg: [-704.591 -704.591 -704.591] (1.000)
Step: 97199, Reward: [-408.657 -408.657 -408.657] [19.2997], Avg: [-704.449 -704.449 -704.449] (1.000)
Step: 97249, Reward: [-426.51 -426.51 -426.51] [24.1940], Avg: [-704.318 -704.318 -704.318] (1.000)
Step: 97299, Reward: [-376.112 -376.112 -376.112] [71.2765], Avg: [-704.186 -704.186 -704.186] (1.000)
Step: 97349, Reward: [-436.421 -436.421 -436.421] [69.5771], Avg: [-704.084 -704.084 -704.084] (1.000)
Step: 97399, Reward: [-488.497 -488.497 -488.497] [165.4844], Avg: [-704.059 -704.059 -704.059] (1.000)
Step: 97449, Reward: [-414.09 -414.09 -414.09] [46.2792], Avg: [-703.934 -703.934 -703.934] (1.000)
Step: 97499, Reward: [-447.407 -447.407 -447.407] [87.8050], Avg: [-703.847 -703.847 -703.847] (1.000)
Step: 97549, Reward: [-453.782 -453.782 -453.782] [34.0145], Avg: [-703.736 -703.736 -703.736] (1.000)
Step: 97599, Reward: [-551.967 -551.967 -551.967] [146.0927], Avg: [-703.733 -703.733 -703.733] (1.000)
Step: 97649, Reward: [-421.57 -421.57 -421.57] [86.1076], Avg: [-703.633 -703.633 -703.633] (1.000)
Step: 97699, Reward: [-474.053 -474.053 -474.053] [107.8213], Avg: [-703.571 -703.571 -703.571] (1.000)
Step: 97749, Reward: [-491.052 -491.052 -491.052] [92.7445], Avg: [-703.509 -703.509 -703.509] (1.000)
Step: 97799, Reward: [-492.747 -492.747 -492.747] [36.2345], Avg: [-703.42 -703.42 -703.42] (1.000)
Step: 97849, Reward: [-474.511 -474.511 -474.511] [45.7035], Avg: [-703.327 -703.327 -703.327] (1.000)
Step: 97899, Reward: [-459.221 -459.221 -459.221] [102.4946], Avg: [-703.254 -703.254 -703.254] (1.000)
Step: 97949, Reward: [-499.386 -499.386 -499.386] [81.8827], Avg: [-703.192 -703.192 -703.192] (1.000)
Step: 97999, Reward: [-426.858 -426.858 -426.858] [109.0084], Avg: [-703.107 -703.107 -703.107] (1.000)
Step: 98049, Reward: [-543.698 -543.698 -543.698] [129.2675], Avg: [-703.091 -703.091 -703.091] (1.000)
Step: 98099, Reward: [-556.41 -556.41 -556.41] [110.0066], Avg: [-703.073 -703.073 -703.073] (1.000)
Step: 98149, Reward: [-607.334 -607.334 -607.334] [152.5169], Avg: [-703.101 -703.101 -703.101] (1.000)
Step: 98199, Reward: [-511.011 -511.011 -511.011] [108.1862], Avg: [-703.059 -703.059 -703.059] (1.000)
Step: 98249, Reward: [-730.241 -730.241 -730.241] [205.5742], Avg: [-703.177 -703.177 -703.177] (1.000)
Step: 98299, Reward: [-556.434 -556.434 -556.434] [173.8606], Avg: [-703.191 -703.191 -703.191] (1.000)
Step: 98349, Reward: [-517.814 -517.814 -517.814] [106.3436], Avg: [-703.151 -703.151 -703.151] (1.000)
Step: 98399, Reward: [-566.566 -566.566 -566.566] [152.6836], Avg: [-703.159 -703.159 -703.159] (1.000)
Step: 98449, Reward: [-537.788 -537.788 -537.788] [83.8503], Avg: [-703.118 -703.118 -703.118] (1.000)
Step: 98499, Reward: [-590.796 -590.796 -590.796] [94.1586], Avg: [-703.108 -703.108 -703.108] (1.000)
Step: 98549, Reward: [-552.635 -552.635 -552.635] [51.8064], Avg: [-703.058 -703.058 -703.058] (1.000)
Step: 98599, Reward: [-436.971 -436.971 -436.971] [80.5140], Avg: [-702.964 -702.964 -702.964] (1.000)
Step: 98649, Reward: [-524.947 -524.947 -524.947] [150.6098], Avg: [-702.95 -702.95 -702.95] (1.000)
Step: 98699, Reward: [-462.527 -462.527 -462.527] [96.1300], Avg: [-702.877 -702.877 -702.877] (1.000)
Step: 98749, Reward: [-416.496 -416.496 -416.496] [56.4862], Avg: [-702.761 -702.761 -702.761] (1.000)
Step: 98799, Reward: [-461.109 -461.109 -461.109] [116.6305], Avg: [-702.698 -702.698 -702.698] (1.000)
Step: 98849, Reward: [-459.257 -459.257 -459.257] [51.8298], Avg: [-702.601 -702.601 -702.601] (1.000)
Step: 98899, Reward: [-499.918 -499.918 -499.918] [173.0998], Avg: [-702.586 -702.586 -702.586] (1.000)
Step: 98949, Reward: [-489.63 -489.63 -489.63] [121.7501], Avg: [-702.54 -702.54 -702.54] (1.000)
Step: 98999, Reward: [-419.654 -419.654 -419.654] [95.8359], Avg: [-702.445 -702.445 -702.445] (1.000)
Step: 99049, Reward: [-480.107 -480.107 -480.107] [93.4738], Avg: [-702.38 -702.38 -702.38] (1.000)
Step: 99099, Reward: [-600.148 -600.148 -600.148] [146.8284], Avg: [-702.403 -702.403 -702.403] (1.000)
Step: 99149, Reward: [-476.726 -476.726 -476.726] [38.4939], Avg: [-702.308 -702.308 -702.308] (1.000)
Step: 99199, Reward: [-476.542 -476.542 -476.542] [106.8920], Avg: [-702.248 -702.248 -702.248] (1.000)
Step: 99249, Reward: [-434.765 -434.765 -434.765] [104.1899], Avg: [-702.166 -702.166 -702.166] (1.000)
Step: 99299, Reward: [-549.95 -549.95 -549.95] [102.3012], Avg: [-702.141 -702.141 -702.141] (1.000)
Step: 99349, Reward: [-422.153 -422.153 -422.153] [49.2745], Avg: [-702.025 -702.025 -702.025] (1.000)
Step: 99399, Reward: [-518.655 -518.655 -518.655] [111.2169], Avg: [-701.988 -701.988 -701.988] (1.000)
Step: 99449, Reward: [-577.056 -577.056 -577.056] [91.1702], Avg: [-701.972 -701.972 -701.972] (1.000)
Step: 99499, Reward: [-562.585 -562.585 -562.585] [117.7352], Avg: [-701.961 -701.961 -701.961] (1.000)
Step: 99549, Reward: [-488.076 -488.076 -488.076] [89.3909], Avg: [-701.898 -701.898 -701.898] (1.000)
Step: 99599, Reward: [-626.953 -626.953 -626.953] [200.0897], Avg: [-701.961 -701.961 -701.961] (1.000)
Step: 99649, Reward: [-524.841 -524.841 -524.841] [98.7154], Avg: [-701.922 -701.922 -701.922] (1.000)
Step: 99699, Reward: [-454.226 -454.226 -454.226] [64.6678], Avg: [-701.83 -701.83 -701.83] (1.000)
Step: 99749, Reward: [-560.584 -560.584 -560.584] [184.1611], Avg: [-701.851 -701.851 -701.851] (1.000)
Step: 99799, Reward: [-576.629 -576.629 -576.629] [150.4399], Avg: [-701.864 -701.864 -701.864] (1.000)
Step: 99849, Reward: [-609.179 -609.179 -609.179] [154.5509], Avg: [-701.895 -701.895 -701.895] (1.000)
Step: 99899, Reward: [-524.239 -524.239 -524.239] [105.8248], Avg: [-701.859 -701.859 -701.859] (1.000)
Step: 99949, Reward: [-629.653 -629.653 -629.653] [61.2024], Avg: [-701.853 -701.853 -701.853] (1.000)
Step: 99999, Reward: [-426.846 -426.846 -426.846] [12.1892], Avg: [-701.722 -701.722 -701.722] (1.000)
