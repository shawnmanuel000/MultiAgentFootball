Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		self.episode_batch.update({"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=len(state[0].shape)==len(self.state_size[0]))
		return list(zip(*one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()))

	def train(self, state, action, next_state, reward, done):
		# import pdb
		# pdb.set_trace()
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			self.episode_batch.update({"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
		self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

	def save_models(self, path):
		self.mac.save_models(path)
		torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
		torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
		torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

	def load_models(self, path):
		self.mac.load_models(path)
		self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
		self.target_critic.load_state_dict(self.critic.state_dict())
		self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
		self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, 128)
		self.fc2 = torch.nn.Linear(128, 128)
		self.fc3 = torch.nn.Linear(128, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def load_state(self, other_mac):
		self.agent.load_state_dict(other_mac.agent.state_dict())

	def cuda(self):
		self.agent.cuda()

	def save_models(self, path):
		torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

	def load_models(self, path):
		self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		h = self.rnn(x, h_in)
		q = self.fc2(h)
		return q, h

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
# env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-626.079 -626.079 -626.079] [54.5698], Avg: [-680.649 -680.649 -680.649] (1.000)
Step: 99, Reward: [-1729.944 -1729.944 -1729.944] [101.3631], Avg: [-1255.978 -1255.978 -1255.978] (1.000)
Step: 149, Reward: [-2013.137 -2013.137 -2013.137] [96.6629], Avg: [-1540.585 -1540.585 -1540.585] (1.000)
Step: 199, Reward: [-2142.368 -2142.368 -2142.368] [194.5297], Avg: [-1739.663 -1739.663 -1739.663] (1.000)
Step: 249, Reward: [-1763.016 -1763.016 -1763.016] [130.8693], Avg: [-1770.508 -1770.508 -1770.508] (1.000)
Step: 299, Reward: [-2047.035 -2047.035 -2047.035] [210.9887], Avg: [-1851.76 -1851.76 -1851.76] (1.000)
Step: 349, Reward: [-2021.541 -2021.541 -2021.541] [145.3145], Avg: [-1896.774 -1896.774 -1896.774] (1.000)
Step: 399, Reward: [-2202.098 -2202.098 -2202.098] [126.0523], Avg: [-1950.696 -1950.696 -1950.696] (1.000)
Step: 449, Reward: [-2027.332 -2027.332 -2027.332] [132.8159], Avg: [-1973.968 -1973.968 -1973.968] (1.000)
Step: 499, Reward: [-2004.726 -2004.726 -2004.726] [154.4868], Avg: [-1992.493 -1992.493 -1992.493] (1.000)
Step: 549, Reward: [-2139.347 -2139.347 -2139.347] [266.1921], Avg: [-2030.043 -2030.043 -2030.043] (1.000)
Step: 599, Reward: [-2139.924 -2139.924 -2139.924] [155.1201], Avg: [-2052.126 -2052.126 -2052.126] (1.000)
Step: 649, Reward: [-2043.827 -2043.827 -2043.827] [132.7133], Avg: [-2061.696 -2061.696 -2061.696] (1.000)
Step: 699, Reward: [-1945.893 -1945.893 -1945.893] [216.9340], Avg: [-2068.92 -2068.92 -2068.92] (1.000)
Step: 749, Reward: [-2007.329 -2007.329 -2007.329] [282.6089], Avg: [-2083.655 -2083.655 -2083.655] (1.000)
Step: 799, Reward: [-2163.035 -2163.035 -2163.035] [192.1772], Avg: [-2100.627 -2100.627 -2100.627] (1.000)
Step: 849, Reward: [-2009.616 -2009.616 -2009.616] [148.6878], Avg: [-2104.02 -2104.02 -2104.02] (1.000)
Step: 899, Reward: [-1565.85 -1565.85 -1565.85] [110.8551], Avg: [-2080.28 -2080.28 -2080.28] (1.000)
Step: 949, Reward: [-1685.603 -1685.603 -1685.603] [79.0042], Avg: [-2063.666 -2063.666 -2063.666] (1.000)
Step: 999, Reward: [-1671.65 -1671.65 -1671.65] [46.4001], Avg: [-2046.385 -2046.385 -2046.385] (1.000)
Step: 1049, Reward: [-1069.844 -1069.844 -1069.844] [348.8726], Avg: [-2016.496 -2016.496 -2016.496] (1.000)
Step: 1099, Reward: [-741.319 -741.319 -741.319] [179.9491], Avg: [-1966.713 -1966.713 -1966.713] (1.000)
Step: 1149, Reward: [-582.43 -582.43 -582.43] [122.4243], Avg: [-1911.849 -1911.849 -1911.849] (1.000)
Step: 1199, Reward: [-705.625 -705.625 -705.625] [170.1894], Avg: [-1868.681 -1868.681 -1868.681] (1.000)
Step: 1249, Reward: [-648.05 -648.05 -648.05] [209.0256], Avg: [-1828.217 -1828.217 -1828.217] (1.000)
Step: 1299, Reward: [-577.088 -577.088 -577.088] [147.4786], Avg: [-1785.769 -1785.769 -1785.769] (1.000)
Step: 1349, Reward: [-456.822 -456.822 -456.822] [67.8415], Avg: [-1739.061 -1739.061 -1739.061] (1.000)
Step: 1399, Reward: [-754.304 -754.304 -754.304] [349.8032], Avg: [-1716.384 -1716.384 -1716.384] (1.000)
Step: 1449, Reward: [-440.178 -440.178 -440.178] [96.1094], Avg: [-1675.691 -1675.691 -1675.691] (1.000)
Step: 1499, Reward: [-436.182 -436.182 -436.182] [65.6108], Avg: [-1636.561 -1636.561 -1636.561] (1.000)
Step: 1549, Reward: [-435.336 -435.336 -435.336] [28.4871], Avg: [-1598.731 -1598.731 -1598.731] (1.000)
Step: 1599, Reward: [-416.611 -416.611 -416.611] [47.3708], Avg: [-1563.27 -1563.27 -1563.27] (1.000)
Step: 1649, Reward: [-498.368 -498.368 -498.368] [41.8745], Avg: [-1532.269 -1532.269 -1532.269] (1.000)
Step: 1699, Reward: [-414.852 -414.852 -414.852] [61.9349], Avg: [-1501.226 -1501.226 -1501.226] (1.000)
Step: 1749, Reward: [-514.686 -514.686 -514.686] [137.7021], Avg: [-1476.973 -1476.973 -1476.973] (1.000)
Step: 1799, Reward: [-462.938 -462.938 -462.938] [97.9277], Avg: [-1451.526 -1451.526 -1451.526] (1.000)
Step: 1849, Reward: [-1882.984 -1882.984 -1882.984] [103.2036], Avg: [-1465.976 -1465.976 -1465.976] (1.000)
Step: 1899, Reward: [-2038.385 -2038.385 -2038.385] [308.2676], Avg: [-1489.152 -1489.152 -1489.152] (1.000)
Step: 1949, Reward: [-1934.039 -1934.039 -1934.039] [168.6080], Avg: [-1504.883 -1504.883 -1504.883] (1.000)
Step: 1999, Reward: [-2049.187 -2049.187 -2049.187] [180.0938], Avg: [-1522.993 -1522.993 -1522.993] (1.000)
Step: 2049, Reward: [-1965.95 -1965.95 -1965.95] [139.5492], Avg: [-1537.2 -1537.2 -1537.2] (1.000)
Step: 2099, Reward: [-500.118 -500.118 -500.118] [95.1876], Avg: [-1514.774 -1514.774 -1514.774] (1.000)
Step: 2149, Reward: [-379.308 -379.308 -379.308] [105.9596], Avg: [-1490.832 -1490.832 -1490.832] (1.000)
Step: 2199, Reward: [-529.459 -529.459 -529.459] [192.6635], Avg: [-1473.361 -1473.361 -1473.361] (1.000)
Step: 2249, Reward: [-1189.992 -1189.992 -1189.992] [488.9577], Avg: [-1477.93 -1477.93 -1477.93] (1.000)
Step: 2299, Reward: [-1066.061 -1066.061 -1066.061] [416.1402], Avg: [-1478.023 -1478.023 -1478.023] (1.000)
Step: 2349, Reward: [-527.168 -527.168 -527.168] [105.0349], Avg: [-1460.027 -1460.027 -1460.027] (1.000)
Step: 2399, Reward: [-441.57 -441.57 -441.57] [30.3459], Avg: [-1439.441 -1439.441 -1439.441] (1.000)
Step: 2449, Reward: [-503.736 -503.736 -503.736] [89.9832], Avg: [-1422.181 -1422.181 -1422.181] (1.000)
Step: 2499, Reward: [-365.621 -365.621 -365.621] [78.3157], Avg: [-1402.616 -1402.616 -1402.616] (1.000)
Step: 2549, Reward: [-394.529 -394.529 -394.529] [74.9131], Avg: [-1384.319 -1384.319 -1384.319] (1.000)
Step: 2599, Reward: [-430.44 -430.44 -430.44] [51.9088], Avg: [-1366.973 -1366.973 -1366.973] (1.000)
Step: 2649, Reward: [-385.612 -385.612 -385.612] [66.4971], Avg: [-1349.712 -1349.712 -1349.712] (1.000)
Step: 2699, Reward: [-423.565 -423.565 -423.565] [22.1701], Avg: [-1332.971 -1332.971 -1332.971] (1.000)
Step: 2749, Reward: [-419.506 -419.506 -419.506] [68.8979], Avg: [-1317.616 -1317.616 -1317.616] (1.000)
Step: 2799, Reward: [-459.763 -459.763 -459.763] [71.3594], Avg: [-1303.571 -1303.571 -1303.571] (1.000)
Step: 2849, Reward: [-442.612 -442.612 -442.612] [32.3488], Avg: [-1289.034 -1289.034 -1289.034] (1.000)
Step: 2899, Reward: [-393.81 -393.81 -393.81] [83.7234], Avg: [-1275.043 -1275.043 -1275.043] (1.000)
Step: 2949, Reward: [-409.722 -409.722 -409.722] [77.6339], Avg: [-1261.692 -1261.692 -1261.692] (1.000)
Step: 2999, Reward: [-410.912 -410.912 -410.912] [104.4724], Avg: [-1249.254 -1249.254 -1249.254] (1.000)
Step: 3049, Reward: [-472.07 -472.07 -472.07] [61.1436], Avg: [-1237.515 -1237.515 -1237.515] (1.000)
Step: 3099, Reward: [-464.015 -464.015 -464.015] [85.0928], Avg: [-1226.412 -1226.412 -1226.412] (1.000)
Step: 3149, Reward: [-459.778 -459.778 -459.778] [61.7313], Avg: [-1215.223 -1215.223 -1215.223] (1.000)
Step: 3199, Reward: [-461.463 -461.463 -461.463] [54.2279], Avg: [-1204.293 -1204.293 -1204.293] (1.000)
Step: 3249, Reward: [-604.601 -604.601 -604.601] [128.6965], Avg: [-1197.047 -1197.047 -1197.047] (1.000)
Step: 3299, Reward: [-513.271 -513.271 -513.271] [52.2027], Avg: [-1187.477 -1187.477 -1187.477] (1.000)
Step: 3349, Reward: [-542.986 -542.986 -542.986] [77.8747], Avg: [-1179.02 -1179.02 -1179.02] (1.000)
Step: 3399, Reward: [-455.967 -455.967 -455.967] [62.6774], Avg: [-1169.309 -1169.309 -1169.309] (1.000)
Step: 3449, Reward: [-411.65 -411.65 -411.65] [69.2430], Avg: [-1159.332 -1159.332 -1159.332] (1.000)
Step: 3499, Reward: [-621.021 -621.021 -621.021] [37.7505], Avg: [-1152.181 -1152.181 -1152.181] (1.000)
Step: 3549, Reward: [-624.264 -624.264 -624.264] [164.5857], Avg: [-1147.064 -1147.064 -1147.064] (1.000)
Step: 3599, Reward: [-494.2 -494.2 -494.2] [76.1120], Avg: [-1139.053 -1139.053 -1139.053] (1.000)
Step: 3649, Reward: [-650.563 -650.563 -650.563] [103.2479], Avg: [-1133.776 -1133.776 -1133.776] (1.000)
Step: 3699, Reward: [-489.388 -489.388 -489.388] [147.2005], Avg: [-1127.057 -1127.057 -1127.057] (1.000)
Step: 3749, Reward: [-965.535 -965.535 -965.535] [181.8970], Avg: [-1127.329 -1127.329 -1127.329] (1.000)
Step: 3799, Reward: [-801.82 -801.82 -801.82] [75.9265], Avg: [-1124.045 -1124.045 -1124.045] (1.000)
Step: 3849, Reward: [-470.904 -470.904 -470.904] [33.2403], Avg: [-1115.994 -1115.994 -1115.994] (1.000)
Step: 3899, Reward: [-508.499 -508.499 -508.499] [47.2717], Avg: [-1108.812 -1108.812 -1108.812] (1.000)
Step: 3949, Reward: [-409.649 -409.649 -409.649] [56.3002], Avg: [-1100.674 -1100.674 -1100.674] (1.000)
Step: 3999, Reward: [-421.973 -421.973 -421.973] [77.3014], Avg: [-1093.157 -1093.157 -1093.157] (1.000)
Step: 4049, Reward: [-338.78 -338.78 -338.78] [31.6427], Avg: [-1084.234 -1084.234 -1084.234] (1.000)
Step: 4099, Reward: [-508.451 -508.451 -508.451] [51.7311], Avg: [-1077.843 -1077.843 -1077.843] (1.000)
Step: 4149, Reward: [-436.191 -436.191 -436.191] [105.2138], Avg: [-1071.38 -1071.38 -1071.38] (1.000)
Step: 4199, Reward: [-478.827 -478.827 -478.827] [130.6276], Avg: [-1065.881 -1065.881 -1065.881] (1.000)
Step: 4249, Reward: [-390.813 -390.813 -390.813] [81.6523], Avg: [-1058.9 -1058.9 -1058.9] (1.000)
Step: 4299, Reward: [-435.252 -435.252 -435.252] [122.9415], Avg: [-1053.078 -1053.078 -1053.078] (1.000)
Step: 4349, Reward: [-375.528 -375.528 -375.528] [83.5526], Avg: [-1046.25 -1046.25 -1046.25] (1.000)
Step: 4399, Reward: [-391.724 -391.724 -391.724] [109.8209], Avg: [-1040.06 -1040.06 -1040.06] (1.000)
Step: 4449, Reward: [-463.469 -463.469 -463.469] [119.6014], Avg: [-1034.926 -1034.926 -1034.926] (1.000)
Step: 4499, Reward: [-442.032 -442.032 -442.032] [102.8971], Avg: [-1029.481 -1029.481 -1029.481] (1.000)
Step: 4549, Reward: [-388.576 -388.576 -388.576] [69.5941], Avg: [-1023.203 -1023.203 -1023.203] (1.000)
Step: 4599, Reward: [-407.664 -407.664 -407.664] [71.1120], Avg: [-1017.285 -1017.285 -1017.285] (1.000)
Step: 4649, Reward: [-395.166 -395.166 -395.166] [48.2656], Avg: [-1011.115 -1011.115 -1011.115] (1.000)
Step: 4699, Reward: [-450.744 -450.744 -450.744] [110.7588], Avg: [-1006.332 -1006.332 -1006.332] (1.000)
Step: 4749, Reward: [-393.972 -393.972 -393.972] [85.1890], Avg: [-1000.783 -1000.783 -1000.783] (1.000)
Step: 4799, Reward: [-461.756 -461.756 -461.756] [72.7841], Avg: [-995.926 -995.926 -995.926] (1.000)
Step: 4849, Reward: [-386.288 -386.288 -386.288] [85.1332], Avg: [-990.519 -990.519 -990.519] (1.000)
Step: 4899, Reward: [-414.836 -414.836 -414.836] [78.2811], Avg: [-985.443 -985.443 -985.443] (1.000)
Step: 4949, Reward: [-480.453 -480.453 -480.453] [57.5726], Avg: [-980.924 -980.924 -980.924] (1.000)
Step: 4999, Reward: [-418.503 -418.503 -418.503] [105.5111], Avg: [-976.355 -976.355 -976.355] (1.000)
Step: 5049, Reward: [-383.271 -383.271 -383.271] [46.3922], Avg: [-970.942 -970.942 -970.942] (1.000)
Step: 5099, Reward: [-450.962 -450.962 -450.962] [91.5048], Avg: [-966.741 -966.741 -966.741] (1.000)
Step: 5149, Reward: [-411.205 -411.205 -411.205] [84.5597], Avg: [-962.169 -962.169 -962.169] (1.000)
Step: 5199, Reward: [-409.597 -409.597 -409.597] [80.7562], Avg: [-957.632 -957.632 -957.632] (1.000)
Step: 5249, Reward: [-425.92 -425.92 -425.92] [45.5390], Avg: [-953.002 -953.002 -953.002] (1.000)
Step: 5299, Reward: [-483.075 -483.075 -483.075] [183.5546], Avg: [-950.3 -950.3 -950.3] (1.000)
Step: 5349, Reward: [-450.561 -450.561 -450.561] [79.7942], Avg: [-946.375 -946.375 -946.375] (1.000)
Step: 5399, Reward: [-426.244 -426.244 -426.244] [76.3050], Avg: [-942.266 -942.266 -942.266] (1.000)
Step: 5449, Reward: [-388.858 -388.858 -388.858] [28.9715], Avg: [-937.454 -937.454 -937.454] (1.000)
Step: 5499, Reward: [-407.938 -407.938 -407.938] [70.8923], Avg: [-933.285 -933.285 -933.285] (1.000)
Step: 5549, Reward: [-477.171 -477.171 -477.171] [101.8133], Avg: [-930.093 -930.093 -930.093] (1.000)
Step: 5599, Reward: [-508.734 -508.734 -508.734] [73.4501], Avg: [-926.987 -926.987 -926.987] (1.000)
Step: 5649, Reward: [-451.191 -451.191 -451.191] [90.1357], Avg: [-923.574 -923.574 -923.574] (1.000)
Step: 5699, Reward: [-397.142 -397.142 -397.142] [112.3420], Avg: [-919.942 -919.942 -919.942] (1.000)
Step: 5749, Reward: [-446.681 -446.681 -446.681] [67.2554], Avg: [-916.411 -916.411 -916.411] (1.000)
Step: 5799, Reward: [-426.684 -426.684 -426.684] [89.6953], Avg: [-912.963 -912.963 -912.963] (1.000)
Step: 5849, Reward: [-440.304 -440.304 -440.304] [111.2121], Avg: [-909.873 -909.873 -909.873] (1.000)
Step: 5899, Reward: [-470.853 -470.853 -470.853] [90.6016], Avg: [-906.921 -906.921 -906.921] (1.000)
Step: 5949, Reward: [-361.393 -361.393 -361.393] [93.1973], Avg: [-903.119 -903.119 -903.119] (1.000)
Step: 5999, Reward: [-442.203 -442.203 -442.203] [86.8710], Avg: [-900.002 -900.002 -900.002] (1.000)
Step: 6049, Reward: [-459.743 -459.743 -459.743] [140.3040], Avg: [-897.523 -897.523 -897.523] (1.000)
Step: 6099, Reward: [-361.57 -361.57 -361.57] [64.9391], Avg: [-893.663 -893.663 -893.663] (1.000)
Step: 6149, Reward: [-399.426 -399.426 -399.426] [71.8975], Avg: [-890.229 -890.229 -890.229] (1.000)
Step: 6199, Reward: [-373.073 -373.073 -373.073] [130.6293], Avg: [-887.112 -887.112 -887.112] (1.000)
Step: 6249, Reward: [-400.563 -400.563 -400.563] [65.6365], Avg: [-883.745 -883.745 -883.745] (1.000)
Step: 6299, Reward: [-609.39 -609.39 -609.39] [119.6489], Avg: [-882.517 -882.517 -882.517] (1.000)
Step: 6349, Reward: [-831.771 -831.771 -831.771] [102.7121], Avg: [-882.926 -882.926 -882.926] (1.000)
Step: 6399, Reward: [-521.938 -521.938 -521.938] [101.8132], Avg: [-880.901 -880.901 -880.901] (1.000)
Step: 6449, Reward: [-1168.538 -1168.538 -1168.538] [233.0732], Avg: [-884.938 -884.938 -884.938] (1.000)
Step: 6499, Reward: [-1230.01 -1230.01 -1230.01] [126.3108], Avg: [-888.564 -888.564 -888.564] (1.000)
Step: 6549, Reward: [-977.081 -977.081 -977.081] [218.2282], Avg: [-890.905 -890.905 -890.905] (1.000)
Step: 6599, Reward: [-712.775 -712.775 -712.775] [109.7012], Avg: [-890.387 -890.387 -890.387] (1.000)
Step: 6649, Reward: [-404.232 -404.232 -404.232] [52.3356], Avg: [-887.125 -887.125 -887.125] (1.000)
Step: 6699, Reward: [-400.441 -400.441 -400.441] [61.3916], Avg: [-883.951 -883.951 -883.951] (1.000)
Step: 6749, Reward: [-499.367 -499.367 -499.367] [51.1676], Avg: [-881.481 -881.481 -881.481] (1.000)
Step: 6799, Reward: [-476.107 -476.107 -476.107] [107.1019], Avg: [-879.288 -879.288 -879.288] (1.000)
Step: 6849, Reward: [-801.726 -801.726 -801.726] [139.6869], Avg: [-879.742 -879.742 -879.742] (1.000)
Step: 6899, Reward: [-2061.98 -2061.98 -2061.98] [265.5469], Avg: [-890.233 -890.233 -890.233] (1.000)
Step: 6949, Reward: [-2110.66 -2110.66 -2110.66] [157.2109], Avg: [-900.144 -900.144 -900.144] (1.000)
Step: 6999, Reward: [-1948.814 -1948.814 -1948.814] [166.7879], Avg: [-908.826 -908.826 -908.826] (1.000)
Step: 7049, Reward: [-2042.727 -2042.727 -2042.727] [205.5964], Avg: [-918.326 -918.326 -918.326] (1.000)
Step: 7099, Reward: [-2041.279 -2041.279 -2041.279] [213.7449], Avg: [-927.739 -927.739 -927.739] (1.000)
Step: 7149, Reward: [-739.047 -739.047 -739.047] [103.0359], Avg: [-927.14 -927.14 -927.14] (1.000)
Step: 7199, Reward: [-454.055 -454.055 -454.055] [70.0922], Avg: [-924.342 -924.342 -924.342] (1.000)
Step: 7249, Reward: [-438.352 -438.352 -438.352] [96.7197], Avg: [-921.657 -921.657 -921.657] (1.000)
Step: 7299, Reward: [-385.403 -385.403 -385.403] [63.1922], Avg: [-918.417 -918.417 -918.417] (1.000)
Step: 7349, Reward: [-606.673 -606.673 -606.673] [130.4019], Avg: [-917.183 -917.183 -917.183] (1.000)
Step: 7399, Reward: [-658.789 -658.789 -658.789] [76.2954], Avg: [-915.953 -915.953 -915.953] (1.000)
Step: 7449, Reward: [-1499.422 -1499.422 -1499.422] [260.7218], Avg: [-921.619 -921.619 -921.619] (1.000)
Step: 7499, Reward: [-1053.646 -1053.646 -1053.646] [220.9175], Avg: [-923.972 -923.972 -923.972] (1.000)
Step: 7549, Reward: [-1929.852 -1929.852 -1929.852] [236.0707], Avg: [-932.196 -932.196 -932.196] (1.000)
Step: 7599, Reward: [-2140.909 -2140.909 -2140.909] [170.1942], Avg: [-941.268 -941.268 -941.268] (1.000)
Step: 7649, Reward: [-2020.277 -2020.277 -2020.277] [255.4170], Avg: [-949.99 -949.99 -949.99] (1.000)
Step: 7699, Reward: [-1998.816 -1998.816 -1998.816] [123.6868], Avg: [-957.604 -957.604 -957.604] (1.000)
Step: 7749, Reward: [-1592.773 -1592.773 -1592.773] [81.4667], Avg: [-962.227 -962.227 -962.227] (1.000)
Step: 7799, Reward: [-1509.251 -1509.251 -1509.251] [275.8843], Avg: [-967.502 -967.502 -967.502] (1.000)
Step: 7849, Reward: [-1404.738 -1404.738 -1404.738] [132.6303], Avg: [-971.132 -971.132 -971.132] (1.000)
Step: 7899, Reward: [-1407.588 -1407.588 -1407.588] [260.2707], Avg: [-975.542 -975.542 -975.542] (1.000)
Step: 7949, Reward: [-935.31 -935.31 -935.31] [220.5323], Avg: [-976.675 -976.675 -976.675] (1.000)
Step: 7999, Reward: [-794.702 -794.702 -794.702] [48.4133], Avg: [-975.841 -975.841 -975.841] (1.000)
Step: 8049, Reward: [-749.31 -749.31 -749.31] [96.3144], Avg: [-975.032 -975.032 -975.032] (1.000)
Step: 8099, Reward: [-687.032 -687.032 -687.032] [123.0880], Avg: [-974.014 -974.014 -974.014] (1.000)
Step: 8149, Reward: [-649.327 -649.327 -649.327] [102.7266], Avg: [-972.652 -972.652 -972.652] (1.000)
Step: 8199, Reward: [-585.694 -585.694 -585.694] [126.4476], Avg: [-971.064 -971.064 -971.064] (1.000)
Step: 8249, Reward: [-534.452 -534.452 -534.452] [71.3146], Avg: [-968.85 -968.85 -968.85] (1.000)
Step: 8299, Reward: [-549.517 -549.517 -549.517] [75.8326], Avg: [-966.781 -966.781 -966.781] (1.000)
Step: 8349, Reward: [-522.781 -522.781 -522.781] [112.2719], Avg: [-964.794 -964.794 -964.794] (1.000)
Step: 8399, Reward: [-528.47 -528.47 -528.47] [91.0065], Avg: [-962.739 -962.739 -962.739] (1.000)
Step: 8449, Reward: [-840.053 -840.053 -840.053] [240.5913], Avg: [-963.436 -963.436 -963.436] (1.000)
Step: 8499, Reward: [-993.093 -993.093 -993.093] [246.7496], Avg: [-965.062 -965.062 -965.062] (1.000)
Step: 8549, Reward: [-1147.52 -1147.52 -1147.52] [108.1272], Avg: [-966.762 -966.762 -966.762] (1.000)
Step: 8599, Reward: [-738.929 -738.929 -738.929] [106.2346], Avg: [-966.055 -966.055 -966.055] (1.000)
Step: 8649, Reward: [-926.282 -926.282 -926.282] [209.2904], Avg: [-967.035 -967.035 -967.035] (1.000)
Step: 8699, Reward: [-908.104 -908.104 -908.104] [108.5449], Avg: [-967.32 -967.32 -967.32] (1.000)
Step: 8749, Reward: [-1312.381 -1312.381 -1312.381] [112.0776], Avg: [-969.932 -969.932 -969.932] (1.000)
Step: 8799, Reward: [-1285.802 -1285.802 -1285.802] [137.7330], Avg: [-972.509 -972.509 -972.509] (1.000)
Step: 8849, Reward: [-1181.375 -1181.375 -1181.375] [189.2644], Avg: [-974.759 -974.759 -974.759] (1.000)
Step: 8899, Reward: [-674.146 -674.146 -674.146] [61.2438], Avg: [-973.414 -973.414 -973.414] (1.000)
Step: 8949, Reward: [-626.37 -626.37 -626.37] [141.4484], Avg: [-972.265 -972.265 -972.265] (1.000)
Step: 8999, Reward: [-519.996 -519.996 -519.996] [52.3331], Avg: [-970.043 -970.043 -970.043] (1.000)
Step: 9049, Reward: [-548.716 -548.716 -548.716] [91.5346], Avg: [-968.221 -968.221 -968.221] (1.000)
Step: 9099, Reward: [-515.451 -515.451 -515.451] [55.5093], Avg: [-966.038 -966.038 -966.038] (1.000)
Step: 9149, Reward: [-475.815 -475.815 -475.815] [60.4825], Avg: [-963.69 -963.69 -963.69] (1.000)
Step: 9199, Reward: [-547.739 -547.739 -547.739] [83.1087], Avg: [-961.881 -961.881 -961.881] (1.000)
Step: 9249, Reward: [-526.151 -526.151 -526.151] [87.4498], Avg: [-959.999 -959.999 -959.999] (1.000)
Step: 9299, Reward: [-494.455 -494.455 -494.455] [139.2046], Avg: [-958.244 -958.244 -958.244] (1.000)
Step: 9349, Reward: [-513.077 -513.077 -513.077] [126.8347], Avg: [-956.542 -956.542 -956.542] (1.000)
Step: 9399, Reward: [-430.461 -430.461 -430.461] [70.2039], Avg: [-954.117 -954.117 -954.117] (1.000)
Step: 9449, Reward: [-384.523 -384.523 -384.523] [76.9890], Avg: [-951.511 -951.511 -951.511] (1.000)
Step: 9499, Reward: [-422.483 -422.483 -422.483] [73.0168], Avg: [-949.111 -949.111 -949.111] (1.000)
Step: 9549, Reward: [-411.996 -411.996 -411.996] [78.5339], Avg: [-946.71 -946.71 -946.71] (1.000)
Step: 9599, Reward: [-391.024 -391.024 -391.024] [38.5498], Avg: [-944.016 -944.016 -944.016] (1.000)
Step: 9649, Reward: [-431.759 -431.759 -431.759] [75.9620], Avg: [-941.756 -941.756 -941.756] (1.000)
Step: 9699, Reward: [-386.227 -386.227 -386.227] [70.7549], Avg: [-939.257 -939.257 -939.257] (1.000)
Step: 9749, Reward: [-373.479 -373.479 -373.479] [103.4754], Avg: [-936.886 -936.886 -936.886] (1.000)
Step: 9799, Reward: [-398.812 -398.812 -398.812] [57.3720], Avg: [-934.433 -934.433 -934.433] (1.000)
Step: 9849, Reward: [-484.854 -484.854 -484.854] [81.2158], Avg: [-932.564 -932.564 -932.564] (1.000)
Step: 9899, Reward: [-466.155 -466.155 -466.155] [62.9628], Avg: [-930.526 -930.526 -930.526] (1.000)
Step: 9949, Reward: [-364.304 -364.304 -364.304] [37.4994], Avg: [-927.869 -927.869 -927.869] (1.000)
Step: 9999, Reward: [-437.824 -437.824 -437.824] [75.6464], Avg: [-925.797 -925.797 -925.797] (1.000)
Step: 10049, Reward: [-446.441 -446.441 -446.441] [145.6553], Avg: [-924.137 -924.137 -924.137] (1.000)
Step: 10099, Reward: [-425.118 -425.118 -425.118] [109.4247], Avg: [-922.208 -922.208 -922.208] (1.000)
Step: 10149, Reward: [-467.97 -467.97 -467.97] [85.5375], Avg: [-920.392 -920.392 -920.392] (1.000)
Step: 10199, Reward: [-523.734 -523.734 -523.734] [77.6864], Avg: [-918.828 -918.828 -918.828] (1.000)
Step: 10249, Reward: [-460.413 -460.413 -460.413] [87.4069], Avg: [-917.018 -917.018 -917.018] (1.000)
Step: 10299, Reward: [-382.316 -382.316 -382.316] [66.0112], Avg: [-914.743 -914.743 -914.743] (1.000)
Step: 10349, Reward: [-400.781 -400.781 -400.781] [98.5826], Avg: [-912.737 -912.737 -912.737] (1.000)
Step: 10399, Reward: [-463.378 -463.378 -463.378] [119.1206], Avg: [-911.149 -911.149 -911.149] (1.000)
Step: 10449, Reward: [-440.747 -440.747 -440.747] [48.1432], Avg: [-909.129 -909.129 -909.129] (1.000)
Step: 10499, Reward: [-436.663 -436.663 -436.663] [98.7878], Avg: [-907.349 -907.349 -907.349] (1.000)
Step: 10549, Reward: [-389.752 -389.752 -389.752] [29.9472], Avg: [-905.038 -905.038 -905.038] (1.000)
Step: 10599, Reward: [-458.807 -458.807 -458.807] [74.4916], Avg: [-903.285 -903.285 -903.285] (1.000)
Step: 10649, Reward: [-403.492 -403.492 -403.492] [112.2409], Avg: [-901.465 -901.465 -901.465] (1.000)
Step: 10699, Reward: [-486. -486. -486.] [100.7669], Avg: [-899.994 -899.994 -899.994] (1.000)
Step: 10749, Reward: [-463.903 -463.903 -463.903] [70.8830], Avg: [-898.296 -898.296 -898.296] (1.000)
Step: 10799, Reward: [-423.143 -423.143 -423.143] [112.2542], Avg: [-896.616 -896.616 -896.616] (1.000)
Step: 10849, Reward: [-388.45 -388.45 -388.45] [45.1289], Avg: [-894.482 -894.482 -894.482] (1.000)
Step: 10899, Reward: [-485.778 -485.778 -485.778] [110.7070], Avg: [-893.115 -893.115 -893.115] (1.000)
Step: 10949, Reward: [-412.546 -412.546 -412.546] [31.9763], Avg: [-891.067 -891.067 -891.067] (1.000)
Step: 10999, Reward: [-532.632 -532.632 -532.632] [45.6638], Avg: [-889.645 -889.645 -889.645] (1.000)
Step: 11049, Reward: [-478.272 -478.272 -478.272] [95.3709], Avg: [-888.215 -888.215 -888.215] (1.000)
Step: 11099, Reward: [-426.543 -426.543 -426.543] [44.7502], Avg: [-886.337 -886.337 -886.337] (1.000)
Step: 11149, Reward: [-498.003 -498.003 -498.003] [109.7234], Avg: [-885.088 -885.088 -885.088] (1.000)
Step: 11199, Reward: [-433.331 -433.331 -433.331] [110.3907], Avg: [-883.564 -883.564 -883.564] (1.000)
Step: 11249, Reward: [-486.392 -486.392 -486.392] [181.7965], Avg: [-882.606 -882.606 -882.606] (1.000)
Step: 11299, Reward: [-419.443 -419.443 -419.443] [64.1931], Avg: [-880.841 -880.841 -880.841] (1.000)
Step: 11349, Reward: [-411.132 -411.132 -411.132] [88.3141], Avg: [-879.161 -879.161 -879.161] (1.000)
Step: 11399, Reward: [-445.789 -445.789 -445.789] [109.6852], Avg: [-877.741 -877.741 -877.741] (1.000)
Step: 11449, Reward: [-418.08 -418.08 -418.08] [92.6034], Avg: [-876.138 -876.138 -876.138] (1.000)
Step: 11499, Reward: [-426.975 -426.975 -426.975] [67.3585], Avg: [-874.478 -874.478 -874.478] (1.000)
Step: 11549, Reward: [-439.039 -439.039 -439.039] [40.0772], Avg: [-872.767 -872.767 -872.767] (1.000)
Step: 11599, Reward: [-380.264 -380.264 -380.264] [64.6713], Avg: [-870.923 -870.923 -870.923] (1.000)
Step: 11649, Reward: [-451.348 -451.348 -451.348] [91.5464], Avg: [-869.515 -869.515 -869.515] (1.000)
Step: 11699, Reward: [-419.146 -419.146 -419.146] [89.1998], Avg: [-867.971 -867.971 -867.971] (1.000)
Step: 11749, Reward: [-490.815 -490.815 -490.815] [82.5484], Avg: [-866.718 -866.718 -866.718] (1.000)
Step: 11799, Reward: [-373.456 -373.456 -373.456] [59.1105], Avg: [-864.878 -864.878 -864.878] (1.000)
Step: 11849, Reward: [-545.989 -545.989 -545.989] [132.1016], Avg: [-864.09 -864.09 -864.09] (1.000)
Step: 11899, Reward: [-445.695 -445.695 -445.695] [108.7344], Avg: [-862.789 -862.789 -862.789] (1.000)
Step: 11949, Reward: [-405.107 -405.107 -405.107] [53.5091], Avg: [-861.098 -861.098 -861.098] (1.000)
Step: 11999, Reward: [-381.147 -381.147 -381.147] [46.8827], Avg: [-859.293 -859.293 -859.293] (1.000)
Step: 12049, Reward: [-386.175 -386.175 -386.175] [71.3424], Avg: [-857.626 -857.626 -857.626] (1.000)
Step: 12099, Reward: [-469.045 -469.045 -469.045] [67.9330], Avg: [-856.301 -856.301 -856.301] (1.000)
Step: 12149, Reward: [-435.22 -435.22 -435.22] [70.3525], Avg: [-854.858 -854.858 -854.858] (1.000)
Step: 12199, Reward: [-428.42 -428.42 -428.42] [88.5844], Avg: [-853.473 -853.473 -853.473] (1.000)
Step: 12249, Reward: [-408.409 -408.409 -408.409] [86.3346], Avg: [-852.009 -852.009 -852.009] (1.000)
Step: 12299, Reward: [-419.943 -419.943 -419.943] [45.4277], Avg: [-850.437 -850.437 -850.437] (1.000)
Step: 12349, Reward: [-442.601 -442.601 -442.601] [81.2566], Avg: [-849.115 -849.115 -849.115] (1.000)
Step: 12399, Reward: [-422.446 -422.446 -422.446] [83.8400], Avg: [-847.733 -847.733 -847.733] (1.000)
Step: 12449, Reward: [-444.966 -444.966 -444.966] [65.5511], Avg: [-846.379 -846.379 -846.379] (1.000)
Step: 12499, Reward: [-408.424 -408.424 -408.424] [73.0207], Avg: [-844.919 -844.919 -844.919] (1.000)
Step: 12549, Reward: [-448.752 -448.752 -448.752] [139.9945], Avg: [-843.898 -843.898 -843.898] (1.000)
Step: 12599, Reward: [-426.176 -426.176 -426.176] [76.1165], Avg: [-842.543 -842.543 -842.543] (1.000)
Step: 12649, Reward: [-416.777 -416.777 -416.777] [60.3331], Avg: [-841.098 -841.098 -841.098] (1.000)
Step: 12699, Reward: [-421.73 -421.73 -421.73] [88.2095], Avg: [-839.794 -839.794 -839.794] (1.000)
Step: 12749, Reward: [-416.677 -416.677 -416.677] [57.5641], Avg: [-838.361 -838.361 -838.361] (1.000)
Step: 12799, Reward: [-492.887 -492.887 -492.887] [130.2577], Avg: [-837.52 -837.52 -837.52] (1.000)
Step: 12849, Reward: [-465.432 -465.432 -465.432] [64.2972], Avg: [-836.323 -836.323 -836.323] (1.000)
Step: 12899, Reward: [-410.673 -410.673 -410.673] [39.8028], Avg: [-834.827 -834.827 -834.827] (1.000)
Step: 12949, Reward: [-449.377 -449.377 -449.377] [75.2181], Avg: [-833.629 -833.629 -833.629] (1.000)
Step: 12999, Reward: [-351.969 -351.969 -351.969] [80.2130], Avg: [-832.085 -832.085 -832.085] (1.000)
Step: 13049, Reward: [-458.145 -458.145 -458.145] [89.5908], Avg: [-830.996 -830.996 -830.996] (1.000)
Step: 13099, Reward: [-489.149 -489.149 -489.149] [116.5716], Avg: [-830.136 -830.136 -830.136] (1.000)
Step: 13149, Reward: [-431.929 -431.929 -431.929] [79.9766], Avg: [-828.926 -828.926 -828.926] (1.000)
Step: 13199, Reward: [-363.956 -363.956 -363.956] [70.2446], Avg: [-827.431 -827.431 -827.431] (1.000)
Step: 13249, Reward: [-459.945 -459.945 -459.945] [84.1242], Avg: [-826.362 -826.362 -826.362] (1.000)
Step: 13299, Reward: [-445.366 -445.366 -445.366] [77.3058], Avg: [-825.22 -825.22 -825.22] (1.000)
Step: 13349, Reward: [-438.79 -438.79 -438.79] [103.8805], Avg: [-824.162 -824.162 -824.162] (1.000)
Step: 13399, Reward: [-394.547 -394.547 -394.547] [70.1287], Avg: [-822.82 -822.82 -822.82] (1.000)
Step: 13449, Reward: [-449.369 -449.369 -449.369] [36.0885], Avg: [-821.566 -821.566 -821.566] (1.000)
Step: 13499, Reward: [-456.841 -456.841 -456.841] [82.2771], Avg: [-820.52 -820.52 -820.52] (1.000)
Step: 13549, Reward: [-478.09 -478.09 -478.09] [135.4660], Avg: [-819.756 -819.756 -819.756] (1.000)
Step: 13599, Reward: [-468.493 -468.493 -468.493] [67.3577], Avg: [-818.713 -818.713 -818.713] (1.000)
Step: 13649, Reward: [-445.365 -445.365 -445.365] [111.6176], Avg: [-817.754 -817.754 -817.754] (1.000)
Step: 13699, Reward: [-409.074 -409.074 -409.074] [80.5745], Avg: [-816.556 -816.556 -816.556] (1.000)
Step: 13749, Reward: [-451.608 -451.608 -451.608] [65.4166], Avg: [-815.467 -815.467 -815.467] (1.000)
Step: 13799, Reward: [-473.896 -473.896 -473.896] [84.0531], Avg: [-814.534 -814.534 -814.534] (1.000)
Step: 13849, Reward: [-534.285 -534.285 -534.285] [113.5431], Avg: [-813.932 -813.932 -813.932] (1.000)
Step: 13899, Reward: [-424.278 -424.278 -424.278] [114.2166], Avg: [-812.941 -812.941 -812.941] (1.000)
Step: 13949, Reward: [-514.046 -514.046 -514.046] [88.2133], Avg: [-812.186 -812.186 -812.186] (1.000)
Step: 13999, Reward: [-480.912 -480.912 -480.912] [105.1092], Avg: [-811.379 -811.379 -811.379] (1.000)
Step: 14049, Reward: [-494.866 -494.866 -494.866] [87.2618], Avg: [-810.563 -810.563 -810.563] (1.000)
Step: 14099, Reward: [-492.181 -492.181 -492.181] [50.5628], Avg: [-809.613 -809.613 -809.613] (1.000)
Step: 14149, Reward: [-378.148 -378.148 -378.148] [71.9322], Avg: [-808.343 -808.343 -808.343] (1.000)
Step: 14199, Reward: [-394.976 -394.976 -394.976] [49.5832], Avg: [-807.062 -807.062 -807.062] (1.000)
Step: 14249, Reward: [-485.646 -485.646 -485.646] [87.8223], Avg: [-806.242 -806.242 -806.242] (1.000)
Step: 14299, Reward: [-417.349 -417.349 -417.349] [91.6656], Avg: [-805.203 -805.203 -805.203] (1.000)
Step: 14349, Reward: [-489.328 -489.328 -489.328] [67.1127], Avg: [-804.336 -804.336 -804.336] (1.000)
Step: 14399, Reward: [-393.789 -393.789 -393.789] [76.5228], Avg: [-803.176 -803.176 -803.176] (1.000)
Step: 14449, Reward: [-464.912 -464.912 -464.912] [63.4578], Avg: [-802.225 -802.225 -802.225] (1.000)
Step: 14499, Reward: [-455.619 -455.619 -455.619] [49.9064], Avg: [-801.202 -801.202 -801.202] (1.000)
Step: 14549, Reward: [-431.492 -431.492 -431.492] [62.9728], Avg: [-800.148 -800.148 -800.148] (1.000)
Step: 14599, Reward: [-405.936 -405.936 -405.936] [65.2242], Avg: [-799.021 -799.021 -799.021] (1.000)
Step: 14649, Reward: [-475.112 -475.112 -475.112] [105.0639], Avg: [-798.275 -798.275 -798.275] (1.000)
Step: 14699, Reward: [-365.769 -365.769 -365.769] [81.1064], Avg: [-797.079 -797.079 -797.079] (1.000)
Step: 14749, Reward: [-440.003 -440.003 -440.003] [81.7327], Avg: [-796.146 -796.146 -796.146] (1.000)
Step: 14799, Reward: [-396.467 -396.467 -396.467] [111.2981], Avg: [-795.172 -795.172 -795.172] (1.000)
Step: 14849, Reward: [-439.628 -439.628 -439.628] [70.4819], Avg: [-794.212 -794.212 -794.212] (1.000)
Step: 14899, Reward: [-404.579 -404.579 -404.579] [75.7427], Avg: [-793.159 -793.159 -793.159] (1.000)
Step: 14949, Reward: [-459.585 -459.585 -459.585] [59.3612], Avg: [-792.241 -792.241 -792.241] (1.000)
Step: 14999, Reward: [-402.458 -402.458 -402.458] [52.5431], Avg: [-791.117 -791.117 -791.117] (1.000)
Step: 15049, Reward: [-437.073 -437.073 -437.073] [134.9221], Avg: [-790.389 -790.389 -790.389] (1.000)
Step: 15099, Reward: [-380.088 -380.088 -380.088] [88.0484], Avg: [-789.322 -789.322 -789.322] (1.000)
Step: 15149, Reward: [-354.042 -354.042 -354.042] [39.5803], Avg: [-788.016 -788.016 -788.016] (1.000)
Step: 15199, Reward: [-501.573 -501.573 -501.573] [170.7090], Avg: [-787.636 -787.636 -787.636] (1.000)
Step: 15249, Reward: [-443.764 -443.764 -443.764] [128.4031], Avg: [-786.929 -786.929 -786.929] (1.000)
Step: 15299, Reward: [-409.551 -409.551 -409.551] [63.7684], Avg: [-785.904 -785.904 -785.904] (1.000)
Step: 15349, Reward: [-423.836 -423.836 -423.836] [88.6415], Avg: [-785.014 -785.014 -785.014] (1.000)
Step: 15399, Reward: [-457.219 -457.219 -457.219] [115.4028], Avg: [-784.324 -784.324 -784.324] (1.000)
Step: 15449, Reward: [-503.919 -503.919 -503.919] [127.2730], Avg: [-783.829 -783.829 -783.829] (1.000)
Step: 15499, Reward: [-475.282 -475.282 -475.282] [86.1400], Avg: [-783.111 -783.111 -783.111] (1.000)
Step: 15549, Reward: [-477.013 -477.013 -477.013] [114.9379], Avg: [-782.496 -782.496 -782.496] (1.000)
Step: 15599, Reward: [-374.643 -374.643 -374.643] [63.7277], Avg: [-781.393 -781.393 -781.393] (1.000)
Step: 15649, Reward: [-395.937 -395.937 -395.937] [74.9487], Avg: [-780.401 -780.401 -780.401] (1.000)
Step: 15699, Reward: [-369.927 -369.927 -369.927] [98.7477], Avg: [-779.409 -779.409 -779.409] (1.000)
Step: 15749, Reward: [-474.526 -474.526 -474.526] [49.4368], Avg: [-778.598 -778.598 -778.598] (1.000)
Step: 15799, Reward: [-478.983 -478.983 -478.983] [76.2608], Avg: [-777.891 -777.891 -777.891] (1.000)
Step: 15849, Reward: [-406.267 -406.267 -406.267] [55.2110], Avg: [-776.893 -776.893 -776.893] (1.000)
Step: 15899, Reward: [-465.905 -465.905 -465.905] [85.7864], Avg: [-776.185 -776.185 -776.185] (1.000)
Step: 15949, Reward: [-346.24 -346.24 -346.24] [62.6631], Avg: [-775.033 -775.033 -775.033] (1.000)
Step: 15999, Reward: [-562.113 -562.113 -562.113] [89.4064], Avg: [-774.647 -774.647 -774.647] (1.000)
Step: 16049, Reward: [-496.615 -496.615 -496.615] [82.7226], Avg: [-774.039 -774.039 -774.039] (1.000)
Step: 16099, Reward: [-402.518 -402.518 -402.518] [101.8204], Avg: [-773.201 -773.201 -773.201] (1.000)
Step: 16149, Reward: [-400.408 -400.408 -400.408] [81.4692], Avg: [-772.299 -772.299 -772.299] (1.000)
Step: 16199, Reward: [-461.595 -461.595 -461.595] [64.7354], Avg: [-771.54 -771.54 -771.54] (1.000)
Step: 16249, Reward: [-412.836 -412.836 -412.836] [119.6445], Avg: [-770.805 -770.805 -770.805] (1.000)
Step: 16299, Reward: [-412.197 -412.197 -412.197] [59.5714], Avg: [-769.887 -769.887 -769.887] (1.000)
Step: 16349, Reward: [-480.154 -480.154 -480.154] [104.9974], Avg: [-769.322 -769.322 -769.322] (1.000)
Step: 16399, Reward: [-413.114 -413.114 -413.114] [122.0600], Avg: [-768.608 -768.608 -768.608] (1.000)
Step: 16449, Reward: [-466.729 -466.729 -466.729] [79.3598], Avg: [-767.932 -767.932 -767.932] (1.000)
Step: 16499, Reward: [-410.834 -410.834 -410.834] [54.2887], Avg: [-767.015 -767.015 -767.015] (1.000)
Step: 16549, Reward: [-454.718 -454.718 -454.718] [72.1939], Avg: [-766.289 -766.289 -766.289] (1.000)
Step: 16599, Reward: [-428.386 -428.386 -428.386] [54.7251], Avg: [-765.436 -765.436 -765.436] (1.000)
Step: 16649, Reward: [-429.484 -429.484 -429.484] [104.3431], Avg: [-764.741 -764.741 -764.741] (1.000)
Step: 16699, Reward: [-387.139 -387.139 -387.139] [63.9937], Avg: [-763.802 -763.802 -763.802] (1.000)
Step: 16749, Reward: [-404.55 -404.55 -404.55] [143.7620], Avg: [-763.158 -763.158 -763.158] (1.000)
Step: 16799, Reward: [-450.592 -450.592 -450.592] [73.2634], Avg: [-762.446 -762.446 -762.446] (1.000)
Step: 16849, Reward: [-451.359 -451.359 -451.359] [65.6452], Avg: [-761.718 -761.718 -761.718] (1.000)
Step: 16899, Reward: [-453.368 -453.368 -453.368] [68.5666], Avg: [-761.009 -761.009 -761.009] (1.000)
Step: 16949, Reward: [-475.797 -475.797 -475.797] [74.4314], Avg: [-760.387 -760.387 -760.387] (1.000)
Step: 16999, Reward: [-419.168 -419.168 -419.168] [93.1730], Avg: [-759.657 -759.657 -759.657] (1.000)
Step: 17049, Reward: [-457.254 -457.254 -457.254] [94.1733], Avg: [-759.047 -759.047 -759.047] (1.000)
Step: 17099, Reward: [-419.427 -419.427 -419.427] [108.8071], Avg: [-758.372 -758.372 -758.372] (1.000)
Step: 17149, Reward: [-396.404 -396.404 -396.404] [84.5788], Avg: [-757.563 -757.563 -757.563] (1.000)
Step: 17199, Reward: [-404.824 -404.824 -404.824] [105.0823], Avg: [-756.843 -756.843 -756.843] (1.000)
Step: 17249, Reward: [-452.907 -452.907 -452.907] [65.4931], Avg: [-756.152 -756.152 -756.152] (1.000)
Step: 17299, Reward: [-457.765 -457.765 -457.765] [49.2952], Avg: [-755.432 -755.432 -755.432] (1.000)
Step: 17349, Reward: [-532.176 -532.176 -532.176] [85.7364], Avg: [-755.036 -755.036 -755.036] (1.000)
Step: 17399, Reward: [-404.75 -404.75 -404.75] [48.8193], Avg: [-754.169 -754.169 -754.169] (1.000)
Step: 17449, Reward: [-440.317 -440.317 -440.317] [113.7817], Avg: [-753.596 -753.596 -753.596] (1.000)
Step: 17499, Reward: [-386.819 -386.819 -386.819] [75.9998], Avg: [-752.765 -752.765 -752.765] (1.000)
Step: 17549, Reward: [-445.206 -445.206 -445.206] [53.8133], Avg: [-752.042 -752.042 -752.042] (1.000)
Step: 17599, Reward: [-479.929 -479.929 -479.929] [64.7408], Avg: [-751.453 -751.453 -751.453] (1.000)
Step: 17649, Reward: [-480.33 -480.33 -480.33] [101.0095], Avg: [-750.971 -750.971 -750.971] (1.000)
Step: 17699, Reward: [-533.182 -533.182 -533.182] [194.4364], Avg: [-750.905 -750.905 -750.905] (1.000)
Step: 17749, Reward: [-375.442 -375.442 -375.442] [41.2497], Avg: [-749.964 -749.964 -749.964] (1.000)
Step: 17799, Reward: [-401.92 -401.92 -401.92] [86.5312], Avg: [-749.229 -749.229 -749.229] (1.000)
Step: 17849, Reward: [-458.123 -458.123 -458.123] [113.4505], Avg: [-748.732 -748.732 -748.732] (1.000)
Step: 17899, Reward: [-477.494 -477.494 -477.494] [137.5816], Avg: [-748.358 -748.358 -748.358] (1.000)
Step: 17949, Reward: [-410.842 -410.842 -410.842] [20.1858], Avg: [-747.474 -747.474 -747.474] (1.000)
Step: 17999, Reward: [-484.16 -484.16 -484.16] [80.9846], Avg: [-746.968 -746.968 -746.968] (1.000)
Step: 18049, Reward: [-456.549 -456.549 -456.549] [59.0899], Avg: [-746.327 -746.327 -746.327] (1.000)
Step: 18099, Reward: [-405.416 -405.416 -405.416] [97.8183], Avg: [-745.656 -745.656 -745.656] (1.000)
Step: 18149, Reward: [-425.789 -425.789 -425.789] [57.0796], Avg: [-744.932 -744.932 -744.932] (1.000)
Step: 18199, Reward: [-444.088 -444.088 -444.088] [110.1058], Avg: [-744.408 -744.408 -744.408] (1.000)
Step: 18249, Reward: [-403.892 -403.892 -403.892] [108.5091], Avg: [-743.772 -743.772 -743.772] (1.000)
Step: 18299, Reward: [-395.582 -395.582 -395.582] [52.2924], Avg: [-742.964 -742.964 -742.964] (1.000)
Step: 18349, Reward: [-455.426 -455.426 -455.426] [75.3177], Avg: [-742.385 -742.385 -742.385] (1.000)
Step: 18399, Reward: [-338.199 -338.199 -338.199] [20.6121], Avg: [-741.343 -741.343 -741.343] (1.000)
Step: 18449, Reward: [-345.316 -345.316 -345.316] [90.2275], Avg: [-740.514 -740.514 -740.514] (1.000)
Step: 18499, Reward: [-419.194 -419.194 -419.194] [63.6164], Avg: [-739.818 -739.818 -739.818] (1.000)
Step: 18549, Reward: [-476.969 -476.969 -476.969] [97.3601], Avg: [-739.372 -739.372 -739.372] (1.000)
Step: 18599, Reward: [-414.535 -414.535 -414.535] [62.1935], Avg: [-738.666 -738.666 -738.666] (1.000)
Step: 18649, Reward: [-419.522 -419.522 -419.522] [75.5963], Avg: [-738.013 -738.013 -738.013] (1.000)
Step: 18699, Reward: [-414.761 -414.761 -414.761] [40.1706], Avg: [-737.256 -737.256 -737.256] (1.000)
Step: 18749, Reward: [-417.505 -417.505 -417.505] [67.3260], Avg: [-736.583 -736.583 -736.583] (1.000)
Step: 18799, Reward: [-388.353 -388.353 -388.353] [28.4966], Avg: [-735.732 -735.732 -735.732] (1.000)
Step: 18849, Reward: [-391.824 -391.824 -391.824] [51.5885], Avg: [-734.957 -734.957 -734.957] (1.000)
Step: 18899, Reward: [-424.542 -424.542 -424.542] [65.2405], Avg: [-734.308 -734.308 -734.308] (1.000)
Step: 18949, Reward: [-476.736 -476.736 -476.736] [123.4442], Avg: [-733.955 -733.955 -733.955] (1.000)
Step: 18999, Reward: [-434.55 -434.55 -434.55] [148.4975], Avg: [-733.557 -733.557 -733.557] (1.000)
Step: 19049, Reward: [-431.179 -431.179 -431.179] [90.6588], Avg: [-733.002 -733.002 -733.002] (1.000)
Step: 19099, Reward: [-383.247 -383.247 -383.247] [46.7755], Avg: [-732.209 -732.209 -732.209] (1.000)
Step: 19149, Reward: [-420.076 -420.076 -420.076] [93.3649], Avg: [-731.637 -731.637 -731.637] (1.000)
Step: 19199, Reward: [-445.78 -445.78 -445.78] [162.9581], Avg: [-731.317 -731.317 -731.317] (1.000)
Step: 19249, Reward: [-415.166 -415.166 -415.166] [83.8487], Avg: [-730.714 -730.714 -730.714] (1.000)
Step: 19299, Reward: [-453.47 -453.47 -453.47] [46.9340], Avg: [-730.117 -730.117 -730.117] (1.000)
Step: 19349, Reward: [-453.195 -453.195 -453.195] [66.2811], Avg: [-729.573 -729.573 -729.573] (1.000)
Step: 19399, Reward: [-419.727 -419.727 -419.727] [36.1619], Avg: [-728.868 -728.868 -728.868] (1.000)
Step: 19449, Reward: [-470.824 -470.824 -470.824] [100.1377], Avg: [-728.462 -728.462 -728.462] (1.000)
Step: 19499, Reward: [-433.397 -433.397 -433.397] [75.7680], Avg: [-727.899 -727.899 -727.899] (1.000)
Step: 19549, Reward: [-448.482 -448.482 -448.482] [87.7745], Avg: [-727.409 -727.409 -727.409] (1.000)
Step: 19599, Reward: [-395.625 -395.625 -395.625] [85.9594], Avg: [-726.782 -726.782 -726.782] (1.000)
Step: 19649, Reward: [-424.322 -424.322 -424.322] [28.2734], Avg: [-726.084 -726.084 -726.084] (1.000)
Step: 19699, Reward: [-474.093 -474.093 -474.093] [52.3814], Avg: [-725.578 -725.578 -725.578] (1.000)
Step: 19749, Reward: [-418.097 -418.097 -418.097] [72.3514], Avg: [-724.983 -724.983 -724.983] (1.000)
Step: 19799, Reward: [-452.483 -452.483 -452.483] [71.6402], Avg: [-724.475 -724.475 -724.475] (1.000)
Step: 19849, Reward: [-430.806 -430.806 -430.806] [81.8863], Avg: [-723.942 -723.942 -723.942] (1.000)
Step: 19899, Reward: [-420.579 -420.579 -420.579] [94.4876], Avg: [-723.417 -723.417 -723.417] (1.000)
Step: 19949, Reward: [-429.449 -429.449 -429.449] [100.4843], Avg: [-722.932 -722.932 -722.932] (1.000)
Step: 19999, Reward: [-400.323 -400.323 -400.323] [102.9828], Avg: [-722.383 -722.383 -722.383] (1.000)
Step: 20049, Reward: [-436.868 -436.868 -436.868] [73.6285], Avg: [-721.855 -721.855 -721.855] (1.000)
Step: 20099, Reward: [-518.055 -518.055 -518.055] [54.4058], Avg: [-721.483 -721.483 -721.483] (1.000)
Step: 20149, Reward: [-415.584 -415.584 -415.584] [87.8363], Avg: [-720.942 -720.942 -720.942] (1.000)
Step: 20199, Reward: [-479.286 -479.286 -479.286] [172.3289], Avg: [-720.77 -720.77 -720.77] (1.000)
Step: 20249, Reward: [-452.016 -452.016 -452.016] [117.7216], Avg: [-720.397 -720.397 -720.397] (1.000)
Step: 20299, Reward: [-371.505 -371.505 -371.505] [58.2035], Avg: [-719.681 -719.681 -719.681] (1.000)
Step: 20349, Reward: [-388.765 -388.765 -388.765] [63.3408], Avg: [-719.024 -719.024 -719.024] (1.000)
Step: 20399, Reward: [-432.78 -432.78 -432.78] [92.9254], Avg: [-718.55 -718.55 -718.55] (1.000)
Step: 20449, Reward: [-407.902 -407.902 -407.902] [75.4685], Avg: [-717.975 -717.975 -717.975] (1.000)
Step: 20499, Reward: [-416.881 -416.881 -416.881] [36.0860], Avg: [-717.329 -717.329 -717.329] (1.000)
Step: 20549, Reward: [-464.298 -464.298 -464.298] [123.9784], Avg: [-717.015 -717.015 -717.015] (1.000)
Step: 20599, Reward: [-455.214 -455.214 -455.214] [64.9258], Avg: [-716.537 -716.537 -716.537] (1.000)
Step: 20649, Reward: [-410.066 -410.066 -410.066] [65.9549], Avg: [-715.955 -715.955 -715.955] (1.000)
Step: 20699, Reward: [-405.927 -405.927 -405.927] [88.1642], Avg: [-715.419 -715.419 -715.419] (1.000)
Step: 20749, Reward: [-455.4 -455.4 -455.4] [164.5366], Avg: [-715.189 -715.189 -715.189] (1.000)
Step: 20799, Reward: [-489.541 -489.541 -489.541] [60.4295], Avg: [-714.791 -714.791 -714.791] (1.000)
Step: 20849, Reward: [-427.946 -427.946 -427.946] [45.7425], Avg: [-714.213 -714.213 -714.213] (1.000)
Step: 20899, Reward: [-375.305 -375.305 -375.305] [72.7294], Avg: [-713.577 -713.577 -713.577] (1.000)
Step: 20949, Reward: [-333.829 -333.829 -333.829] [46.3358], Avg: [-712.781 -712.781 -712.781] (1.000)
Step: 20999, Reward: [-356.405 -356.405 -356.405] [38.7248], Avg: [-712.024 -712.024 -712.024] (1.000)
Step: 21049, Reward: [-415.437 -415.437 -415.437] [85.4900], Avg: [-711.523 -711.523 -711.523] (1.000)
Step: 21099, Reward: [-409.305 -409.305 -409.305] [94.5417], Avg: [-711.031 -711.031 -711.031] (1.000)
Step: 21149, Reward: [-505.246 -505.246 -505.246] [73.1577], Avg: [-710.717 -710.717 -710.717] (1.000)
Step: 21199, Reward: [-456.921 -456.921 -456.921] [106.0497], Avg: [-710.369 -710.369 -710.369] (1.000)
Step: 21249, Reward: [-399.031 -399.031 -399.031] [80.1665], Avg: [-709.825 -709.825 -709.825] (1.000)
Step: 21299, Reward: [-439.853 -439.853 -439.853] [123.9384], Avg: [-709.482 -709.482 -709.482] (1.000)
Step: 21349, Reward: [-406.188 -406.188 -406.188] [41.8764], Avg: [-708.87 -708.87 -708.87] (1.000)
Step: 21399, Reward: [-430.936 -430.936 -430.936] [71.5939], Avg: [-708.388 -708.388 -708.388] (1.000)
Step: 21449, Reward: [-451.839 -451.839 -451.839] [78.4672], Avg: [-707.973 -707.973 -707.973] (1.000)
Step: 21499, Reward: [-426.361 -426.361 -426.361] [102.1757], Avg: [-707.555 -707.555 -707.555] (1.000)
Step: 21549, Reward: [-376.269 -376.269 -376.269] [64.0758], Avg: [-706.935 -706.935 -706.935] (1.000)
Step: 21599, Reward: [-488.509 -488.509 -488.509] [70.4038], Avg: [-706.593 -706.593 -706.593] (1.000)
Step: 21649, Reward: [-388.461 -388.461 -388.461] [71.2185], Avg: [-706.023 -706.023 -706.023] (1.000)
Step: 21699, Reward: [-391.755 -391.755 -391.755] [54.2274], Avg: [-705.423 -705.423 -705.423] (1.000)
Step: 21749, Reward: [-390.339 -390.339 -390.339] [91.3740], Avg: [-704.909 -704.909 -704.909] (1.000)
Step: 21799, Reward: [-410.989 -410.989 -410.989] [100.6447], Avg: [-704.466 -704.466 -704.466] (1.000)
Step: 21849, Reward: [-358.416 -358.416 -358.416] [31.2653], Avg: [-703.746 -703.746 -703.746] (1.000)
Step: 21899, Reward: [-347.577 -347.577 -347.577] [89.6829], Avg: [-703.137 -703.137 -703.137] (1.000)
Step: 21949, Reward: [-389.301 -389.301 -389.301] [47.3866], Avg: [-702.53 -702.53 -702.53] (1.000)
Step: 21999, Reward: [-474.776 -474.776 -474.776] [122.5351], Avg: [-702.291 -702.291 -702.291] (1.000)
Step: 22049, Reward: [-381.141 -381.141 -381.141] [82.1426], Avg: [-701.749 -701.749 -701.749] (1.000)
Step: 22099, Reward: [-399.467 -399.467 -399.467] [99.2577], Avg: [-701.29 -701.29 -701.29] (1.000)
Step: 22149, Reward: [-422.083 -422.083 -422.083] [68.8256], Avg: [-700.815 -700.815 -700.815] (1.000)
Step: 22199, Reward: [-414.198 -414.198 -414.198] [59.9622], Avg: [-700.304 -700.304 -700.304] (1.000)
Step: 22249, Reward: [-514.474 -514.474 -514.474] [114.7744], Avg: [-700.145 -700.145 -700.145] (1.000)
Step: 22299, Reward: [-459.612 -459.612 -459.612] [111.3327], Avg: [-699.855 -699.855 -699.855] (1.000)
Step: 22349, Reward: [-414.655 -414.655 -414.655] [67.5929], Avg: [-699.368 -699.368 -699.368] (1.000)
Step: 22399, Reward: [-464.902 -464.902 -464.902] [55.8267], Avg: [-698.969 -698.969 -698.969] (1.000)
Step: 22449, Reward: [-444.629 -444.629 -444.629] [70.0475], Avg: [-698.559 -698.559 -698.559] (1.000)
Step: 22499, Reward: [-360.242 -360.242 -360.242] [58.4312], Avg: [-697.937 -697.937 -697.937] (1.000)
Step: 22549, Reward: [-423.145 -423.145 -423.145] [51.9169], Avg: [-697.443 -697.443 -697.443] (1.000)
Step: 22599, Reward: [-499.067 -499.067 -499.067] [114.1343], Avg: [-697.256 -697.256 -697.256] (1.000)
Step: 22649, Reward: [-505.159 -505.159 -505.159] [129.8220], Avg: [-697.119 -697.119 -697.119] (1.000)
Step: 22699, Reward: [-439.044 -439.044 -439.044] [40.6173], Avg: [-696.64 -696.64 -696.64] (1.000)
Step: 22749, Reward: [-533.1 -533.1 -533.1] [100.6781], Avg: [-696.502 -696.502 -696.502] (1.000)
Step: 22799, Reward: [-420.506 -420.506 -420.506] [106.3372], Avg: [-696.13 -696.13 -696.13] (1.000)
Step: 22849, Reward: [-413.206 -413.206 -413.206] [97.3644], Avg: [-695.724 -695.724 -695.724] (1.000)
Step: 22899, Reward: [-386.345 -386.345 -386.345] [69.7613], Avg: [-695.201 -695.201 -695.201] (1.000)
Step: 22949, Reward: [-468.366 -468.366 -468.366] [66.5392], Avg: [-694.851 -694.851 -694.851] (1.000)
Step: 22999, Reward: [-389.757 -389.757 -389.757] [67.1788], Avg: [-694.334 -694.334 -694.334] (1.000)
Step: 23049, Reward: [-493.681 -493.681 -493.681] [56.0761], Avg: [-694.021 -694.021 -694.021] (1.000)
Step: 23099, Reward: [-435.257 -435.257 -435.257] [35.7102], Avg: [-693.538 -693.538 -693.538] (1.000)
Step: 23149, Reward: [-406.52 -406.52 -406.52] [67.7698], Avg: [-693.064 -693.064 -693.064] (1.000)
Step: 23199, Reward: [-443.809 -443.809 -443.809] [102.4052], Avg: [-692.748 -692.748 -692.748] (1.000)
Step: 23249, Reward: [-433.618 -433.618 -433.618] [76.1759], Avg: [-692.354 -692.354 -692.354] (1.000)
Step: 23299, Reward: [-468.952 -468.952 -468.952] [100.1050], Avg: [-692.09 -692.09 -692.09] (1.000)
Step: 23349, Reward: [-408.294 -408.294 -408.294] [53.9014], Avg: [-691.597 -691.597 -691.597] (1.000)
Step: 23399, Reward: [-397.695 -397.695 -397.695] [83.6969], Avg: [-691.148 -691.148 -691.148] (1.000)
Step: 23449, Reward: [-434.763 -434.763 -434.763] [64.5164], Avg: [-690.739 -690.739 -690.739] (1.000)
Step: 23499, Reward: [-383.663 -383.663 -383.663] [37.7521], Avg: [-690.166 -690.166 -690.166] (1.000)
Step: 23549, Reward: [-399.471 -399.471 -399.471] [70.2041], Avg: [-689.698 -689.698 -689.698] (1.000)
Step: 23599, Reward: [-403.691 -403.691 -403.691] [39.2324], Avg: [-689.175 -689.175 -689.175] (1.000)
Step: 23649, Reward: [-344.388 -344.388 -344.388] [50.4441], Avg: [-688.553 -688.553 -688.553] (1.000)
Step: 23699, Reward: [-374.8 -374.8 -374.8] [65.8708], Avg: [-688.03 -688.03 -688.03] (1.000)
Step: 23749, Reward: [-486.185 -486.185 -486.185] [90.6144], Avg: [-687.796 -687.796 -687.796] (1.000)
Step: 23799, Reward: [-368.717 -368.717 -368.717] [108.9120], Avg: [-687.354 -687.354 -687.354] (1.000)
Step: 23849, Reward: [-439.19 -439.19 -439.19] [84.2750], Avg: [-687.011 -687.011 -687.011] (1.000)
Step: 23899, Reward: [-420.851 -420.851 -420.851] [135.6609], Avg: [-686.738 -686.738 -686.738] (1.000)
Step: 23949, Reward: [-427.049 -427.049 -427.049] [67.6939], Avg: [-686.337 -686.337 -686.337] (1.000)
Step: 23999, Reward: [-461.847 -461.847 -461.847] [97.1401], Avg: [-686.071 -686.071 -686.071] (1.000)
Step: 24049, Reward: [-383.229 -383.229 -383.229] [38.8366], Avg: [-685.523 -685.523 -685.523] (1.000)
Step: 24099, Reward: [-454.997 -454.997 -454.997] [133.1140], Avg: [-685.32 -685.32 -685.32] (1.000)
Step: 24149, Reward: [-418.513 -418.513 -418.513] [63.5989], Avg: [-684.9 -684.9 -684.9] (1.000)
Step: 24199, Reward: [-471.317 -471.317 -471.317] [119.7401], Avg: [-684.706 -684.706 -684.706] (1.000)
Step: 24249, Reward: [-484.189 -484.189 -484.189] [43.5593], Avg: [-684.382 -684.382 -684.382] (1.000)
Step: 24299, Reward: [-440.196 -440.196 -440.196] [60.8499], Avg: [-684.005 -684.005 -684.005] (1.000)
Step: 24349, Reward: [-425.121 -425.121 -425.121] [58.0924], Avg: [-683.593 -683.593 -683.593] (1.000)
Step: 24399, Reward: [-443.768 -443.768 -443.768] [41.7218], Avg: [-683.187 -683.187 -683.187] (1.000)
Step: 24449, Reward: [-452.365 -452.365 -452.365] [28.6180], Avg: [-682.773 -682.773 -682.773] (1.000)
Step: 24499, Reward: [-417.163 -417.163 -417.163] [91.5514], Avg: [-682.418 -682.418 -682.418] (1.000)
Step: 24549, Reward: [-394.194 -394.194 -394.194] [36.7729], Avg: [-681.906 -681.906 -681.906] (1.000)
Step: 24599, Reward: [-540.302 -540.302 -540.302] [108.5913], Avg: [-681.839 -681.839 -681.839] (1.000)
Step: 24649, Reward: [-416.814 -416.814 -416.814] [128.6849], Avg: [-681.562 -681.562 -681.562] (1.000)
Step: 24699, Reward: [-425.423 -425.423 -425.423] [97.3464], Avg: [-681.241 -681.241 -681.241] (1.000)
Step: 24749, Reward: [-503.703 -503.703 -503.703] [54.7477], Avg: [-680.993 -680.993 -680.993] (1.000)
Step: 24799, Reward: [-403.804 -403.804 -403.804] [108.7299], Avg: [-680.653 -680.653 -680.653] (1.000)
Step: 24849, Reward: [-432.832 -432.832 -432.832] [66.4646], Avg: [-680.288 -680.288 -680.288] (1.000)
Step: 24899, Reward: [-405.832 -405.832 -405.832] [111.3296], Avg: [-679.961 -679.961 -679.961] (1.000)
Step: 24949, Reward: [-464.687 -464.687 -464.687] [107.3197], Avg: [-679.744 -679.744 -679.744] (1.000)
Step: 24999, Reward: [-452.043 -452.043 -452.043] [90.0854], Avg: [-679.469 -679.469 -679.469] (1.000)
Step: 25049, Reward: [-430.18 -430.18 -430.18] [96.1792], Avg: [-679.163 -679.163 -679.163] (1.000)
Step: 25099, Reward: [-366.979 -366.979 -366.979] [24.2747], Avg: [-678.59 -678.59 -678.59] (1.000)
Step: 25149, Reward: [-436.1 -436.1 -436.1] [42.6277], Avg: [-678.193 -678.193 -678.193] (1.000)
Step: 25199, Reward: [-392.799 -392.799 -392.799] [66.5958], Avg: [-677.758 -677.758 -677.758] (1.000)
Step: 25249, Reward: [-375.807 -375.807 -375.807] [81.5580], Avg: [-677.322 -677.322 -677.322] (1.000)
Step: 25299, Reward: [-456.853 -456.853 -456.853] [90.8000], Avg: [-677.066 -677.066 -677.066] (1.000)
Step: 25349, Reward: [-422.204 -422.204 -422.204] [76.8069], Avg: [-676.715 -676.715 -676.715] (1.000)
Step: 25399, Reward: [-519.129 -519.129 -519.129] [117.0597], Avg: [-676.635 -676.635 -676.635] (1.000)
Step: 25449, Reward: [-396.981 -396.981 -396.981] [96.7246], Avg: [-676.275 -676.275 -676.275] (1.000)
Step: 25499, Reward: [-459.076 -459.076 -459.076] [107.6142], Avg: [-676.061 -676.061 -676.061] (1.000)
Step: 25549, Reward: [-465.525 -465.525 -465.525] [72.8670], Avg: [-675.791 -675.791 -675.791] (1.000)
Step: 25599, Reward: [-381.222 -381.222 -381.222] [65.0949], Avg: [-675.343 -675.343 -675.343] (1.000)
Step: 25649, Reward: [-405.248 -405.248 -405.248] [65.7006], Avg: [-674.945 -674.945 -674.945] (1.000)
Step: 25699, Reward: [-459.317 -459.317 -459.317] [90.1360], Avg: [-674.7 -674.7 -674.7] (1.000)
Step: 25749, Reward: [-446.868 -446.868 -446.868] [30.2256], Avg: [-674.317 -674.317 -674.317] (1.000)
Step: 25799, Reward: [-415.973 -415.973 -415.973] [96.1336], Avg: [-674.002 -674.002 -674.002] (1.000)
Step: 25849, Reward: [-493.736 -493.736 -493.736] [107.1428], Avg: [-673.861 -673.861 -673.861] (1.000)
Step: 25899, Reward: [-372.321 -372.321 -372.321] [80.2115], Avg: [-673.434 -673.434 -673.434] (1.000)
Step: 25949, Reward: [-368.233 -368.233 -368.233] [68.0823], Avg: [-672.977 -672.977 -672.977] (1.000)
Step: 25999, Reward: [-360.064 -360.064 -360.064] [21.6659], Avg: [-672.417 -672.417 -672.417] (1.000)
Step: 26049, Reward: [-398.284 -398.284 -398.284] [61.6319], Avg: [-672.009 -672.009 -672.009] (1.000)
Step: 26099, Reward: [-426.1 -426.1 -426.1] [29.7849], Avg: [-671.595 -671.595 -671.595] (1.000)
Step: 26149, Reward: [-372.605 -372.605 -372.605] [29.3411], Avg: [-671.079 -671.079 -671.079] (1.000)
Step: 26199, Reward: [-381.668 -381.668 -381.668] [101.2979], Avg: [-670.72 -670.72 -670.72] (1.000)
Step: 26249, Reward: [-412.219 -412.219 -412.219] [69.8579], Avg: [-670.361 -670.361 -670.361] (1.000)
Step: 26299, Reward: [-400.427 -400.427 -400.427] [23.7374], Avg: [-669.893 -669.893 -669.893] (1.000)
Step: 26349, Reward: [-450.059 -450.059 -450.059] [86.0423], Avg: [-669.639 -669.639 -669.639] (1.000)
Step: 26399, Reward: [-487.004 -487.004 -487.004] [95.2522], Avg: [-669.473 -669.473 -669.473] (1.000)
Step: 26449, Reward: [-464.934 -464.934 -464.934] [80.9723], Avg: [-669.24 -669.24 -669.24] (1.000)
Step: 26499, Reward: [-432.449 -432.449 -432.449] [64.7358], Avg: [-668.915 -668.915 -668.915] (1.000)
Step: 26549, Reward: [-368.387 -368.387 -368.387] [43.3107], Avg: [-668.431 -668.431 -668.431] (1.000)
Step: 26599, Reward: [-476.683 -476.683 -476.683] [53.7617], Avg: [-668.171 -668.171 -668.171] (1.000)
Step: 26649, Reward: [-465.093 -465.093 -465.093] [56.2879], Avg: [-667.896 -667.896 -667.896] (1.000)
Step: 26699, Reward: [-456.976 -456.976 -456.976] [47.3406], Avg: [-667.59 -667.59 -667.59] (1.000)
Step: 26749, Reward: [-447.541 -447.541 -447.541] [55.8456], Avg: [-667.283 -667.283 -667.283] (1.000)
Step: 26799, Reward: [-392.022 -392.022 -392.022] [79.2553], Avg: [-666.917 -666.917 -666.917] (1.000)
Step: 26849, Reward: [-380.134 -380.134 -380.134] [61.9101], Avg: [-666.498 -666.498 -666.498] (1.000)
Step: 26899, Reward: [-527.663 -527.663 -527.663] [114.7232], Avg: [-666.454 -666.454 -666.454] (1.000)
Step: 26949, Reward: [-475.927 -475.927 -475.927] [60.1803], Avg: [-666.212 -666.212 -666.212] (1.000)
Step: 26999, Reward: [-444.232 -444.232 -444.232] [141.7905], Avg: [-666.063 -666.063 -666.063] (1.000)
Step: 27049, Reward: [-356.382 -356.382 -356.382] [24.1848], Avg: [-665.535 -665.535 -665.535] (1.000)
Step: 27099, Reward: [-371.17 -371.17 -371.17] [102.2958], Avg: [-665.181 -665.181 -665.181] (1.000)
Step: 27149, Reward: [-350.146 -350.146 -350.146] [90.5638], Avg: [-664.768 -664.768 -664.768] (1.000)
Step: 27199, Reward: [-407.684 -407.684 -407.684] [63.1281], Avg: [-664.411 -664.411 -664.411] (1.000)
Step: 27249, Reward: [-399.441 -399.441 -399.441] [45.0298], Avg: [-664.008 -664.008 -664.008] (1.000)
Step: 27299, Reward: [-461.698 -461.698 -461.698] [99.7499], Avg: [-663.82 -663.82 -663.82] (1.000)
Step: 27349, Reward: [-473.732 -473.732 -473.732] [75.3162], Avg: [-663.61 -663.61 -663.61] (1.000)
Step: 27399, Reward: [-466.317 -466.317 -466.317] [85.3339], Avg: [-663.406 -663.406 -663.406] (1.000)
Step: 27449, Reward: [-362.017 -362.017 -362.017] [63.2590], Avg: [-662.972 -662.972 -662.972] (1.000)
Step: 27499, Reward: [-431.406 -431.406 -431.406] [63.9019], Avg: [-662.667 -662.667 -662.667] (1.000)
Step: 27549, Reward: [-392.015 -392.015 -392.015] [60.6432], Avg: [-662.286 -662.286 -662.286] (1.000)
Step: 27599, Reward: [-417.733 -417.733 -417.733] [62.8105], Avg: [-661.957 -661.957 -661.957] (1.000)
Step: 27649, Reward: [-460.809 -460.809 -460.809] [54.3668], Avg: [-661.691 -661.691 -661.691] (1.000)
Step: 27699, Reward: [-441.348 -441.348 -441.348] [79.9743], Avg: [-661.438 -661.438 -661.438] (1.000)
Step: 27749, Reward: [-475.868 -475.868 -475.868] [38.6521], Avg: [-661.173 -661.173 -661.173] (1.000)
Step: 27799, Reward: [-386.921 -386.921 -386.921] [71.9198], Avg: [-660.809 -660.809 -660.809] (1.000)
Step: 27849, Reward: [-540.177 -540.177 -540.177] [128.5629], Avg: [-660.823 -660.823 -660.823] (1.000)
Step: 27899, Reward: [-370.303 -370.303 -370.303] [47.7686], Avg: [-660.388 -660.388 -660.388] (1.000)
Step: 27949, Reward: [-354.448 -354.448 -354.448] [40.2616], Avg: [-659.913 -659.913 -659.913] (1.000)
Step: 27999, Reward: [-454.532 -454.532 -454.532] [113.3865], Avg: [-659.749 -659.749 -659.749] (1.000)
Step: 28049, Reward: [-415.754 -415.754 -415.754] [99.6932], Avg: [-659.492 -659.492 -659.492] (1.000)
Step: 28099, Reward: [-503.967 -503.967 -503.967] [94.3415], Avg: [-659.383 -659.383 -659.383] (1.000)
Step: 28149, Reward: [-467.789 -467.789 -467.789] [67.9014], Avg: [-659.163 -659.163 -659.163] (1.000)
Step: 28199, Reward: [-441.606 -441.606 -441.606] [70.1035], Avg: [-658.902 -658.902 -658.902] (1.000)
Step: 28249, Reward: [-435.883 -435.883 -435.883] [49.5004], Avg: [-658.595 -658.595 -658.595] (1.000)
Step: 28299, Reward: [-495.64 -495.64 -495.64] [140.4807], Avg: [-658.555 -658.555 -658.555] (1.000)
Step: 28349, Reward: [-449.914 -449.914 -449.914] [115.6376], Avg: [-658.391 -658.391 -658.391] (1.000)
Step: 28399, Reward: [-515.189 -515.189 -515.189] [151.9652], Avg: [-658.406 -658.406 -658.406] (1.000)
Step: 28449, Reward: [-499.657 -499.657 -499.657] [43.6615], Avg: [-658.204 -658.204 -658.204] (1.000)
Step: 28499, Reward: [-442.488 -442.488 -442.488] [54.9211], Avg: [-657.922 -657.922 -657.922] (1.000)
Step: 28549, Reward: [-548.108 -548.108 -548.108] [155.8314], Avg: [-658.002 -658.002 -658.002] (1.000)
Step: 28599, Reward: [-484.498 -484.498 -484.498] [120.3928], Avg: [-657.91 -657.91 -657.91] (1.000)
Step: 28649, Reward: [-428.649 -428.649 -428.649] [82.2489], Avg: [-657.653 -657.653 -657.653] (1.000)
Step: 28699, Reward: [-442.648 -442.648 -442.648] [93.9955], Avg: [-657.442 -657.442 -657.442] (1.000)
Step: 28749, Reward: [-453.479 -453.479 -453.479] [68.5917], Avg: [-657.207 -657.207 -657.207] (1.000)
Step: 28799, Reward: [-463.586 -463.586 -463.586] [132.0487], Avg: [-657.1 -657.1 -657.1] (1.000)
Step: 28849, Reward: [-431.837 -431.837 -431.837] [95.0928], Avg: [-656.874 -656.874 -656.874] (1.000)
Step: 28899, Reward: [-478.083 -478.083 -478.083] [93.1133], Avg: [-656.726 -656.726 -656.726] (1.000)
Step: 28949, Reward: [-415.997 -415.997 -415.997] [85.4022], Avg: [-656.458 -656.458 -656.458] (1.000)
Step: 28999, Reward: [-400.691 -400.691 -400.691] [79.8502], Avg: [-656.155 -656.155 -656.155] (1.000)
Step: 29049, Reward: [-441.099 -441.099 -441.099] [83.7426], Avg: [-655.928 -655.928 -655.928] (1.000)
Step: 29099, Reward: [-483.462 -483.462 -483.462] [75.7423], Avg: [-655.762 -655.762 -655.762] (1.000)
Step: 29149, Reward: [-447.443 -447.443 -447.443] [58.1153], Avg: [-655.505 -655.505 -655.505] (1.000)
Step: 29199, Reward: [-441.458 -441.458 -441.458] [100.3477], Avg: [-655.31 -655.31 -655.31] (1.000)
Step: 29249, Reward: [-426.602 -426.602 -426.602] [57.4295], Avg: [-655.017 -655.017 -655.017] (1.000)
Step: 29299, Reward: [-413.657 -413.657 -413.657] [70.2648], Avg: [-654.725 -654.725 -654.725] (1.000)
Step: 29349, Reward: [-366.312 -366.312 -366.312] [48.2153], Avg: [-654.316 -654.316 -654.316] (1.000)
Step: 29399, Reward: [-469.021 -469.021 -469.021] [86.0113], Avg: [-654.147 -654.147 -654.147] (1.000)
Step: 29449, Reward: [-415.207 -415.207 -415.207] [207.1701], Avg: [-654.093 -654.093 -654.093] (1.000)
Step: 29499, Reward: [-454.114 -454.114 -454.114] [62.4232], Avg: [-653.86 -653.86 -653.86] (1.000)
Step: 29549, Reward: [-415.997 -415.997 -415.997] [44.3045], Avg: [-653.533 -653.533 -653.533] (1.000)
Step: 29599, Reward: [-448.789 -448.789 -448.789] [41.5206], Avg: [-653.257 -653.257 -653.257] (1.000)
Step: 29649, Reward: [-416.175 -416.175 -416.175] [49.1298], Avg: [-652.94 -652.94 -652.94] (1.000)
Step: 29699, Reward: [-395.056 -395.056 -395.056] [74.2179], Avg: [-652.631 -652.631 -652.631] (1.000)
Step: 29749, Reward: [-387.517 -387.517 -387.517] [42.0119], Avg: [-652.256 -652.256 -652.256] (1.000)
Step: 29799, Reward: [-448.499 -448.499 -448.499] [71.7318], Avg: [-652.034 -652.034 -652.034] (1.000)
Step: 29849, Reward: [-431.706 -431.706 -431.706] [71.7945], Avg: [-651.785 -651.785 -651.785] (1.000)
Step: 29899, Reward: [-484.041 -484.041 -484.041] [83.2338], Avg: [-651.644 -651.644 -651.644] (1.000)
Step: 29949, Reward: [-407.345 -407.345 -407.345] [75.0945], Avg: [-651.362 -651.362 -651.362] (1.000)
Step: 29999, Reward: [-446.665 -446.665 -446.665] [69.7044], Avg: [-651.137 -651.137 -651.137] (1.000)
Step: 30049, Reward: [-379.067 -379.067 -379.067] [73.6153], Avg: [-650.806 -650.806 -650.806] (1.000)
Step: 30099, Reward: [-456.642 -456.642 -456.642] [81.3634], Avg: [-650.619 -650.619 -650.619] (1.000)
Step: 30149, Reward: [-413.796 -413.796 -413.796] [28.3383], Avg: [-650.273 -650.273 -650.273] (1.000)
Step: 30199, Reward: [-558.511 -558.511 -558.511] [76.1096], Avg: [-650.247 -650.247 -650.247] (1.000)
Step: 30249, Reward: [-423.218 -423.218 -423.218] [124.9169], Avg: [-650.079 -650.079 -650.079] (1.000)
Step: 30299, Reward: [-442.503 -442.503 -442.503] [33.6849], Avg: [-649.792 -649.792 -649.792] (1.000)
Step: 30349, Reward: [-435.044 -435.044 -435.044] [89.0432], Avg: [-649.585 -649.585 -649.585] (1.000)
Step: 30399, Reward: [-418.882 -418.882 -418.882] [42.0815], Avg: [-649.274 -649.274 -649.274] (1.000)
Step: 30449, Reward: [-451.148 -451.148 -451.148] [121.7683], Avg: [-649.149 -649.149 -649.149] (1.000)
Step: 30499, Reward: [-387.686 -387.686 -387.686] [76.1135], Avg: [-648.845 -648.845 -648.845] (1.000)
Step: 30549, Reward: [-336.038 -336.038 -336.038] [68.9773], Avg: [-648.446 -648.446 -648.446] (1.000)
Step: 30599, Reward: [-463.068 -463.068 -463.068] [125.4162], Avg: [-648.348 -648.348 -648.348] (1.000)
Step: 30649, Reward: [-498.759 -498.759 -498.759] [99.9345], Avg: [-648.267 -648.267 -648.267] (1.000)
Step: 30699, Reward: [-393.016 -393.016 -393.016] [53.1838], Avg: [-647.938 -647.938 -647.938] (1.000)
Step: 30749, Reward: [-483.738 -483.738 -483.738] [88.6353], Avg: [-647.815 -647.815 -647.815] (1.000)
Step: 30799, Reward: [-420.327 -420.327 -420.327] [67.0748], Avg: [-647.555 -647.555 -647.555] (1.000)
Step: 30849, Reward: [-429.87 -429.87 -429.87] [131.5742], Avg: [-647.415 -647.415 -647.415] (1.000)
Step: 30899, Reward: [-460.576 -460.576 -460.576] [69.0461], Avg: [-647.225 -647.225 -647.225] (1.000)
Step: 30949, Reward: [-433.09 -433.09 -433.09] [62.9845], Avg: [-646.98 -646.98 -646.98] (1.000)
Step: 30999, Reward: [-444.785 -444.785 -444.785] [41.7013], Avg: [-646.721 -646.721 -646.721] (1.000)
Step: 31049, Reward: [-539.151 -539.151 -539.151] [115.0005], Avg: [-646.733 -646.733 -646.733] (1.000)
Step: 31099, Reward: [-509.395 -509.395 -509.395] [102.5363], Avg: [-646.677 -646.677 -646.677] (1.000)
Step: 31149, Reward: [-435.489 -435.489 -435.489] [91.8639], Avg: [-646.486 -646.486 -646.486] (1.000)
Step: 31199, Reward: [-373.007 -373.007 -373.007] [102.1004], Avg: [-646.211 -646.211 -646.211] (1.000)
Step: 31249, Reward: [-447.022 -447.022 -447.022] [55.2409], Avg: [-645.981 -645.981 -645.981] (1.000)
Step: 31299, Reward: [-454.75 -454.75 -454.75] [50.6694], Avg: [-645.756 -645.756 -645.756] (1.000)
Step: 31349, Reward: [-543.527 -543.527 -543.527] [138.6977], Avg: [-645.815 -645.815 -645.815] (1.000)
Step: 31399, Reward: [-506.231 -506.231 -506.231] [129.1258], Avg: [-645.798 -645.798 -645.798] (1.000)
Step: 31449, Reward: [-426.702 -426.702 -426.702] [33.7396], Avg: [-645.503 -645.503 -645.503] (1.000)
Step: 31499, Reward: [-447.367 -447.367 -447.367] [49.0712], Avg: [-645.267 -645.267 -645.267] (1.000)
Step: 31549, Reward: [-426.197 -426.197 -426.197] [110.2773], Avg: [-645.094 -645.094 -645.094] (1.000)
Step: 31599, Reward: [-362.606 -362.606 -362.606] [53.0770], Avg: [-644.731 -644.731 -644.731] (1.000)
Step: 31649, Reward: [-430.136 -430.136 -430.136] [23.5416], Avg: [-644.429 -644.429 -644.429] (1.000)
Step: 31699, Reward: [-495.947 -495.947 -495.947] [57.5846], Avg: [-644.286 -644.286 -644.286] (1.000)
Step: 31749, Reward: [-390.471 -390.471 -390.471] [100.9372], Avg: [-644.045 -644.045 -644.045] (1.000)
Step: 31799, Reward: [-452.262 -452.262 -452.262] [44.3817], Avg: [-643.814 -643.814 -643.814] (1.000)
Step: 31849, Reward: [-447.86 -447.86 -447.86] [79.9507], Avg: [-643.631 -643.631 -643.631] (1.000)
Step: 31899, Reward: [-432.066 -432.066 -432.066] [54.9120], Avg: [-643.386 -643.386 -643.386] (1.000)
Step: 31949, Reward: [-403.806 -403.806 -403.806] [36.1317], Avg: [-643.068 -643.068 -643.068] (1.000)
Step: 31999, Reward: [-444.784 -444.784 -444.784] [80.5016], Avg: [-642.883 -642.883 -642.883] (1.000)
Step: 32049, Reward: [-543.315 -543.315 -543.315] [98.3170], Avg: [-642.882 -642.882 -642.882] (1.000)
Step: 32099, Reward: [-377.158 -377.158 -377.158] [21.5108], Avg: [-642.501 -642.501 -642.501] (1.000)
Step: 32149, Reward: [-442.83 -442.83 -442.83] [109.9152], Avg: [-642.362 -642.362 -642.362] (1.000)
Step: 32199, Reward: [-431.34 -431.34 -431.34] [69.5529], Avg: [-642.142 -642.142 -642.142] (1.000)
Step: 32249, Reward: [-438.064 -438.064 -438.064] [77.9489], Avg: [-641.946 -641.946 -641.946] (1.000)
Step: 32299, Reward: [-417.891 -417.891 -417.891] [58.6803], Avg: [-641.69 -641.69 -641.69] (1.000)
Step: 32349, Reward: [-419.681 -419.681 -419.681] [111.7224], Avg: [-641.52 -641.52 -641.52] (1.000)
Step: 32399, Reward: [-333.096 -333.096 -333.096] [24.5087], Avg: [-641.082 -641.082 -641.082] (1.000)
Step: 32449, Reward: [-371.219 -371.219 -371.219] [84.3982], Avg: [-640.796 -640.796 -640.796] (1.000)
Step: 32499, Reward: [-503.803 -503.803 -503.803] [104.0633], Avg: [-640.745 -640.745 -640.745] (1.000)
Step: 32549, Reward: [-391.979 -391.979 -391.979] [47.6615], Avg: [-640.436 -640.436 -640.436] (1.000)
Step: 32599, Reward: [-403.431 -403.431 -403.431] [85.3230], Avg: [-640.204 -640.204 -640.204] (1.000)
Step: 32649, Reward: [-373.808 -373.808 -373.808] [86.9523], Avg: [-639.929 -639.929 -639.929] (1.000)
Step: 32699, Reward: [-448.063 -448.063 -448.063] [124.1217], Avg: [-639.825 -639.825 -639.825] (1.000)
Step: 32749, Reward: [-594.145 -594.145 -594.145] [110.9582], Avg: [-639.925 -639.925 -639.925] (1.000)
Step: 32799, Reward: [-420.971 -420.971 -420.971] [83.8437], Avg: [-639.719 -639.719 -639.719] (1.000)
Step: 32849, Reward: [-392.671 -392.671 -392.671] [115.6826], Avg: [-639.519 -639.519 -639.519] (1.000)
Step: 32899, Reward: [-432.891 -432.891 -432.891] [89.9011], Avg: [-639.342 -639.342 -639.342] (1.000)
Step: 32949, Reward: [-463.277 -463.277 -463.277] [42.1741], Avg: [-639.139 -639.139 -639.139] (1.000)
Step: 32999, Reward: [-471.032 -471.032 -471.032] [131.0858], Avg: [-639.082 -639.082 -639.082] (1.000)
Step: 33049, Reward: [-403.696 -403.696 -403.696] [40.4789], Avg: [-638.788 -638.788 -638.788] (1.000)
Step: 33099, Reward: [-410.322 -410.322 -410.322] [88.5142], Avg: [-638.576 -638.576 -638.576] (1.000)
Step: 33149, Reward: [-449.135 -449.135 -449.135] [54.8649], Avg: [-638.373 -638.373 -638.373] (1.000)
Step: 33199, Reward: [-436.674 -436.674 -436.674] [108.1339], Avg: [-638.232 -638.232 -638.232] (1.000)
Step: 33249, Reward: [-407.331 -407.331 -407.331] [146.3525], Avg: [-638.105 -638.105 -638.105] (1.000)
Step: 33299, Reward: [-422.712 -422.712 -422.712] [23.2709], Avg: [-637.817 -637.817 -637.817] (1.000)
Step: 33349, Reward: [-405. -405. -405.] [86.9704], Avg: [-637.598 -637.598 -637.598] (1.000)
Step: 33399, Reward: [-392.027 -392.027 -392.027] [107.5571], Avg: [-637.391 -637.391 -637.391] (1.000)
Step: 33449, Reward: [-422.028 -422.028 -422.028] [77.3633], Avg: [-637.185 -637.185 -637.185] (1.000)
Step: 33499, Reward: [-490.125 -490.125 -490.125] [107.9680], Avg: [-637.127 -637.127 -637.127] (1.000)
Step: 33549, Reward: [-360.826 -360.826 -360.826] [38.3370], Avg: [-636.772 -636.772 -636.772] (1.000)
Step: 33599, Reward: [-471.348 -471.348 -471.348] [96.4019], Avg: [-636.669 -636.669 -636.669] (1.000)
Step: 33649, Reward: [-463.536 -463.536 -463.536] [98.1300], Avg: [-636.558 -636.558 -636.558] (1.000)
Step: 33699, Reward: [-436.677 -436.677 -436.677] [38.0881], Avg: [-636.318 -636.318 -636.318] (1.000)
Step: 33749, Reward: [-420.584 -420.584 -420.584] [98.0667], Avg: [-636.144 -636.144 -636.144] (1.000)
Step: 33799, Reward: [-517.41 -517.41 -517.41] [108.2619], Avg: [-636.128 -636.128 -636.128] (1.000)
Step: 33849, Reward: [-369.702 -369.702 -369.702] [83.9235], Avg: [-635.859 -635.859 -635.859] (1.000)
Step: 33899, Reward: [-429.771 -429.771 -429.771] [63.4867], Avg: [-635.648 -635.648 -635.648] (1.000)
Step: 33949, Reward: [-407.246 -407.246 -407.246] [68.9641], Avg: [-635.413 -635.413 -635.413] (1.000)
Step: 33999, Reward: [-393.887 -393.887 -393.887] [43.1682], Avg: [-635.122 -635.122 -635.122] (1.000)
Step: 34049, Reward: [-375.3 -375.3 -375.3] [92.6599], Avg: [-634.876 -634.876 -634.876] (1.000)
Step: 34099, Reward: [-387.236 -387.236 -387.236] [37.1778], Avg: [-634.568 -634.568 -634.568] (1.000)
Step: 34149, Reward: [-398.284 -398.284 -398.284] [73.1714], Avg: [-634.329 -634.329 -634.329] (1.000)
Step: 34199, Reward: [-448.062 -448.062 -448.062] [110.1100], Avg: [-634.217 -634.217 -634.217] (1.000)
Step: 34249, Reward: [-478.829 -478.829 -478.829] [96.1653], Avg: [-634.131 -634.131 -634.131] (1.000)
Step: 34299, Reward: [-439.971 -439.971 -439.971] [82.1054], Avg: [-633.968 -633.968 -633.968] (1.000)
Step: 34349, Reward: [-443.39 -443.39 -443.39] [89.9226], Avg: [-633.821 -633.821 -633.821] (1.000)
Step: 34399, Reward: [-477.519 -477.519 -477.519] [55.1192], Avg: [-633.674 -633.674 -633.674] (1.000)
Step: 34449, Reward: [-506.482 -506.482 -506.482] [54.3100], Avg: [-633.568 -633.568 -633.568] (1.000)
Step: 34499, Reward: [-452.212 -452.212 -452.212] [97.4594], Avg: [-633.447 -633.447 -633.447] (1.000)
Step: 34549, Reward: [-390.039 -390.039 -390.039] [67.7970], Avg: [-633.193 -633.193 -633.193] (1.000)
Step: 34599, Reward: [-376.565 -376.565 -376.565] [32.1690], Avg: [-632.868 -632.868 -632.868] (1.000)
Step: 34649, Reward: [-382.894 -382.894 -382.894] [60.4498], Avg: [-632.595 -632.595 -632.595] (1.000)
Step: 34699, Reward: [-428.92 -428.92 -428.92] [59.8706], Avg: [-632.388 -632.388 -632.388] (1.000)
Step: 34749, Reward: [-479.634 -479.634 -479.634] [54.8918], Avg: [-632.247 -632.247 -632.247] (1.000)
Step: 34799, Reward: [-383.891 -383.891 -383.891] [30.3891], Avg: [-631.934 -631.934 -631.934] (1.000)
Step: 34849, Reward: [-415.292 -415.292 -415.292] [43.8971], Avg: [-631.686 -631.686 -631.686] (1.000)
Step: 34899, Reward: [-528.565 -528.565 -528.565] [86.5517], Avg: [-631.662 -631.662 -631.662] (1.000)
Step: 34949, Reward: [-390.119 -390.119 -390.119] [44.3920], Avg: [-631.38 -631.38 -631.38] (1.000)
Step: 34999, Reward: [-411.579 -411.579 -411.579] [51.7279], Avg: [-631.14 -631.14 -631.14] (1.000)
Step: 35049, Reward: [-433.703 -433.703 -433.703] [42.5453], Avg: [-630.919 -630.919 -630.919] (1.000)
Step: 35099, Reward: [-437.663 -437.663 -437.663] [34.9651], Avg: [-630.693 -630.693 -630.693] (1.000)
Step: 35149, Reward: [-462.529 -462.529 -462.529] [129.8998], Avg: [-630.639 -630.639 -630.639] (1.000)
Step: 35199, Reward: [-419.929 -419.929 -419.929] [54.1349], Avg: [-630.417 -630.417 -630.417] (1.000)
Step: 35249, Reward: [-479.929 -479.929 -479.929] [86.5158], Avg: [-630.326 -630.326 -630.326] (1.000)
Step: 35299, Reward: [-381.763 -381.763 -381.763] [99.9345], Avg: [-630.115 -630.115 -630.115] (1.000)
Step: 35349, Reward: [-437.17 -437.17 -437.17] [105.1419], Avg: [-629.991 -629.991 -629.991] (1.000)
Step: 35399, Reward: [-463.148 -463.148 -463.148] [88.5213], Avg: [-629.88 -629.88 -629.88] (1.000)
Step: 35449, Reward: [-427.341 -427.341 -427.341] [42.1479], Avg: [-629.654 -629.654 -629.654] (1.000)
Step: 35499, Reward: [-458.723 -458.723 -458.723] [105.6703], Avg: [-629.562 -629.562 -629.562] (1.000)
Step: 35549, Reward: [-440.619 -440.619 -440.619] [55.1968], Avg: [-629.374 -629.374 -629.374] (1.000)
Step: 35599, Reward: [-487.976 -487.976 -487.976] [125.6057], Avg: [-629.352 -629.352 -629.352] (1.000)
Step: 35649, Reward: [-435.73 -435.73 -435.73] [93.0733], Avg: [-629.211 -629.211 -629.211] (1.000)
Step: 35699, Reward: [-395.361 -395.361 -395.361] [47.8917], Avg: [-628.951 -628.951 -628.951] (1.000)
Step: 35749, Reward: [-354.151 -354.151 -354.151] [77.7338], Avg: [-628.675 -628.675 -628.675] (1.000)
Step: 35799, Reward: [-453.421 -453.421 -453.421] [57.9976], Avg: [-628.511 -628.511 -628.511] (1.000)
Step: 35849, Reward: [-479.373 -479.373 -479.373] [100.8064], Avg: [-628.444 -628.444 -628.444] (1.000)
Step: 35899, Reward: [-471.85 -471.85 -471.85] [99.8890], Avg: [-628.365 -628.365 -628.365] (1.000)
Step: 35949, Reward: [-407.872 -407.872 -407.872] [127.7470], Avg: [-628.236 -628.236 -628.236] (1.000)
Step: 35999, Reward: [-464.366 -464.366 -464.366] [101.8247], Avg: [-628.15 -628.15 -628.15] (1.000)
Step: 36049, Reward: [-431.55 -431.55 -431.55] [45.5041], Avg: [-627.94 -627.94 -627.94] (1.000)
Step: 36099, Reward: [-488.806 -488.806 -488.806] [171.1942], Avg: [-627.984 -627.984 -627.984] (1.000)
Step: 36149, Reward: [-476.031 -476.031 -476.031] [35.0016], Avg: [-627.823 -627.823 -627.823] (1.000)
Step: 36199, Reward: [-446.638 -446.638 -446.638] [78.9175], Avg: [-627.681 -627.681 -627.681] (1.000)
Step: 36249, Reward: [-440.03 -440.03 -440.03] [45.4139], Avg: [-627.485 -627.485 -627.485] (1.000)
Step: 36299, Reward: [-488.476 -488.476 -488.476] [124.2352], Avg: [-627.465 -627.465 -627.465] (1.000)
Step: 36349, Reward: [-441.68 -441.68 -441.68] [68.1024], Avg: [-627.303 -627.303 -627.303] (1.000)
Step: 36399, Reward: [-488.568 -488.568 -488.568] [40.8464], Avg: [-627.169 -627.169 -627.169] (1.000)
Step: 36449, Reward: [-454.699 -454.699 -454.699] [99.0645], Avg: [-627.068 -627.068 -627.068] (1.000)
Step: 36499, Reward: [-422.748 -422.748 -422.748] [88.0647], Avg: [-626.909 -626.909 -626.909] (1.000)
Step: 36549, Reward: [-461.405 -461.405 -461.405] [29.2531], Avg: [-626.722 -626.722 -626.722] (1.000)
Step: 36599, Reward: [-477.883 -477.883 -477.883] [130.3890], Avg: [-626.697 -626.697 -626.697] (1.000)
Step: 36649, Reward: [-387.253 -387.253 -387.253] [55.4759], Avg: [-626.446 -626.446 -626.446] (1.000)
Step: 36699, Reward: [-403.164 -403.164 -403.164] [99.7189], Avg: [-626.278 -626.278 -626.278] (1.000)
Step: 36749, Reward: [-543.198 -543.198 -543.198] [54.7745], Avg: [-626.239 -626.239 -626.239] (1.000)
Step: 36799, Reward: [-423.941 -423.941 -423.941] [89.5424], Avg: [-626.086 -626.086 -626.086] (1.000)
Step: 36849, Reward: [-430.507 -430.507 -430.507] [69.6203], Avg: [-625.915 -625.915 -625.915] (1.000)
Step: 36899, Reward: [-412.941 -412.941 -412.941] [77.1668], Avg: [-625.731 -625.731 -625.731] (1.000)
Step: 36949, Reward: [-392.889 -392.889 -392.889] [101.1682], Avg: [-625.553 -625.553 -625.553] (1.000)
Step: 36999, Reward: [-330.49 -330.49 -330.49] [46.2154], Avg: [-625.217 -625.217 -625.217] (1.000)
Step: 37049, Reward: [-558.816 -558.816 -558.816] [66.6443], Avg: [-625.217 -625.217 -625.217] (1.000)
Step: 37099, Reward: [-453.844 -453.844 -453.844] [130.9183], Avg: [-625.162 -625.162 -625.162] (1.000)
Step: 37149, Reward: [-460.999 -460.999 -460.999] [56.1644], Avg: [-625.017 -625.017 -625.017] (1.000)
Step: 37199, Reward: [-465.04 -465.04 -465.04] [98.4227], Avg: [-624.934 -624.934 -624.934] (1.000)
Step: 37249, Reward: [-436.125 -436.125 -436.125] [61.8372], Avg: [-624.764 -624.764 -624.764] (1.000)
Step: 37299, Reward: [-413.817 -413.817 -413.817] [88.0355], Avg: [-624.599 -624.599 -624.599] (1.000)
Step: 37349, Reward: [-418.134 -418.134 -418.134] [57.2635], Avg: [-624.399 -624.399 -624.399] (1.000)
Step: 37399, Reward: [-435.124 -435.124 -435.124] [47.2084], Avg: [-624.209 -624.209 -624.209] (1.000)
Step: 37449, Reward: [-401.429 -401.429 -401.429] [72.4800], Avg: [-624.009 -624.009 -624.009] (1.000)
Step: 37499, Reward: [-450.591 -450.591 -450.591] [144.5394], Avg: [-623.97 -623.97 -623.97] (1.000)
Step: 37549, Reward: [-434.679 -434.679 -434.679] [125.9652], Avg: [-623.886 -623.886 -623.886] (1.000)
Step: 37599, Reward: [-442.074 -442.074 -442.074] [122.7278], Avg: [-623.807 -623.807 -623.807] (1.000)
Step: 37649, Reward: [-396.615 -396.615 -396.615] [43.7491], Avg: [-623.564 -623.564 -623.564] (1.000)
Step: 37699, Reward: [-459.915 -459.915 -459.915] [105.9513], Avg: [-623.487 -623.487 -623.487] (1.000)
Step: 37749, Reward: [-447.549 -447.549 -447.549] [66.8531], Avg: [-623.343 -623.343 -623.343] (1.000)
Step: 37799, Reward: [-379.72 -379.72 -379.72] [122.6460], Avg: [-623.183 -623.183 -623.183] (1.000)
Step: 37849, Reward: [-378.994 -378.994 -378.994] [64.0585], Avg: [-622.945 -622.945 -622.945] (1.000)
Step: 37899, Reward: [-376.704 -376.704 -376.704] [41.8309], Avg: [-622.675 -622.675 -622.675] (1.000)
Step: 37949, Reward: [-470.553 -470.553 -470.553] [97.3261], Avg: [-622.603 -622.603 -622.603] (1.000)
Step: 37999, Reward: [-439.945 -439.945 -439.945] [79.5987], Avg: [-622.467 -622.467 -622.467] (1.000)
Step: 38049, Reward: [-488.684 -488.684 -488.684] [69.9143], Avg: [-622.383 -622.383 -622.383] (1.000)
Step: 38099, Reward: [-507.329 -507.329 -507.329] [169.0276], Avg: [-622.454 -622.454 -622.454] (1.000)
Step: 38149, Reward: [-474.459 -474.459 -474.459] [49.2235], Avg: [-622.325 -622.325 -622.325] (1.000)
Step: 38199, Reward: [-452.606 -452.606 -452.606] [53.8680], Avg: [-622.173 -622.173 -622.173] (1.000)
Step: 38249, Reward: [-412.939 -412.939 -412.939] [72.9555], Avg: [-621.995 -621.995 -621.995] (1.000)
Step: 38299, Reward: [-471.903 -471.903 -471.903] [111.8155], Avg: [-621.945 -621.945 -621.945] (1.000)
Step: 38349, Reward: [-424.466 -424.466 -424.466] [62.1835], Avg: [-621.769 -621.769 -621.769] (1.000)
Step: 38399, Reward: [-439.859 -439.859 -439.859] [88.9093], Avg: [-621.648 -621.648 -621.648] (1.000)
Step: 38449, Reward: [-409.308 -409.308 -409.308] [103.2425], Avg: [-621.506 -621.506 -621.506] (1.000)
Step: 38499, Reward: [-366.597 -366.597 -366.597] [60.0765], Avg: [-621.253 -621.253 -621.253] (1.000)
Step: 38549, Reward: [-426.227 -426.227 -426.227] [43.9271], Avg: [-621.057 -621.057 -621.057] (1.000)
Step: 38599, Reward: [-421.07 -421.07 -421.07] [43.5920], Avg: [-620.854 -620.854 -620.854] (1.000)
Step: 38649, Reward: [-427.51 -427.51 -427.51] [118.5180], Avg: [-620.757 -620.757 -620.757] (1.000)
Step: 38699, Reward: [-406.234 -406.234 -406.234] [94.5874], Avg: [-620.602 -620.602 -620.602] (1.000)
Step: 38749, Reward: [-521.316 -521.316 -521.316] [53.9963], Avg: [-620.544 -620.544 -620.544] (1.000)
Step: 38799, Reward: [-442.942 -442.942 -442.942] [95.9731], Avg: [-620.439 -620.439 -620.439] (1.000)
Step: 38849, Reward: [-413.044 -413.044 -413.044] [92.1441], Avg: [-620.29 -620.29 -620.29] (1.000)
Step: 38899, Reward: [-427.513 -427.513 -427.513] [80.2295], Avg: [-620.146 -620.146 -620.146] (1.000)
Step: 38949, Reward: [-432.269 -432.269 -432.269] [105.2298], Avg: [-620.04 -620.04 -620.04] (1.000)
Step: 38999, Reward: [-443.781 -443.781 -443.781] [79.8532], Avg: [-619.916 -619.916 -619.916] (1.000)
Step: 39049, Reward: [-451.178 -451.178 -451.178] [71.3638], Avg: [-619.791 -619.791 -619.791] (1.000)
Step: 39099, Reward: [-407.533 -407.533 -407.533] [64.5224], Avg: [-619.602 -619.602 -619.602] (1.000)
Step: 39149, Reward: [-377.965 -377.965 -377.965] [56.2785], Avg: [-619.366 -619.366 -619.366] (1.000)
Step: 39199, Reward: [-463.341 -463.341 -463.341] [131.7120], Avg: [-619.335 -619.335 -619.335] (1.000)
Step: 39249, Reward: [-457.449 -457.449 -457.449] [53.2210], Avg: [-619.196 -619.196 -619.196] (1.000)
Step: 39299, Reward: [-479.235 -479.235 -479.235] [84.8034], Avg: [-619.126 -619.126 -619.126] (1.000)
Step: 39349, Reward: [-396.26 -396.26 -396.26] [76.3335], Avg: [-618.94 -618.94 -618.94] (1.000)
Step: 39399, Reward: [-433.342 -433.342 -433.342] [80.1784], Avg: [-618.806 -618.806 -618.806] (1.000)
Step: 39449, Reward: [-489.661 -489.661 -489.661] [38.1832], Avg: [-618.691 -618.691 -618.691] (1.000)
Step: 39499, Reward: [-472.197 -472.197 -472.197] [91.4139], Avg: [-618.621 -618.621 -618.621] (1.000)
Step: 39549, Reward: [-397.588 -397.588 -397.588] [70.3086], Avg: [-618.431 -618.431 -618.431] (1.000)
Step: 39599, Reward: [-480.031 -480.031 -480.031] [98.0774], Avg: [-618.38 -618.38 -618.38] (1.000)
Step: 39649, Reward: [-420.115 -420.115 -420.115] [69.0763], Avg: [-618.217 -618.217 -618.217] (1.000)
Step: 39699, Reward: [-400.447 -400.447 -400.447] [66.8932], Avg: [-618.027 -618.027 -618.027] (1.000)
Step: 39749, Reward: [-387.457 -387.457 -387.457] [53.0262], Avg: [-617.803 -617.803 -617.803] (1.000)
Step: 39799, Reward: [-467.91 -467.91 -467.91] [68.3777], Avg: [-617.701 -617.701 -617.701] (1.000)
Step: 39849, Reward: [-404.09 -404.09 -404.09] [52.5412], Avg: [-617.499 -617.499 -617.499] (1.000)
Step: 39899, Reward: [-439.643 -439.643 -439.643] [76.3793], Avg: [-617.372 -617.372 -617.372] (1.000)
Step: 39949, Reward: [-427.647 -427.647 -427.647] [143.4239], Avg: [-617.314 -617.314 -617.314] (1.000)
Step: 39999, Reward: [-410.378 -410.378 -410.378] [73.6250], Avg: [-617.147 -617.147 -617.147] (1.000)
Step: 40049, Reward: [-405.657 -405.657 -405.657] [77.2006], Avg: [-616.979 -616.979 -616.979] (1.000)
Step: 40099, Reward: [-430.768 -430.768 -430.768] [32.0727], Avg: [-616.787 -616.787 -616.787] (1.000)
Step: 40149, Reward: [-461.217 -461.217 -461.217] [129.1709], Avg: [-616.754 -616.754 -616.754] (1.000)
Step: 40199, Reward: [-368.718 -368.718 -368.718] [57.4143], Avg: [-616.517 -616.517 -616.517] (1.000)
Step: 40249, Reward: [-408.623 -408.623 -408.623] [27.2147], Avg: [-616.293 -616.293 -616.293] (1.000)
Step: 40299, Reward: [-606.224 -606.224 -606.224] [140.0326], Avg: [-616.454 -616.454 -616.454] (1.000)
Step: 40349, Reward: [-374.842 -374.842 -374.842] [82.5169], Avg: [-616.257 -616.257 -616.257] (1.000)
Step: 40399, Reward: [-470.421 -470.421 -470.421] [62.8148], Avg: [-616.154 -616.154 -616.154] (1.000)
Step: 40449, Reward: [-406.808 -406.808 -406.808] [63.2156], Avg: [-615.974 -615.974 -615.974] (1.000)
Step: 40499, Reward: [-386.426 -386.426 -386.426] [51.8720], Avg: [-615.754 -615.754 -615.754] (1.000)
Step: 40549, Reward: [-416.891 -416.891 -416.891] [113.9537], Avg: [-615.65 -615.65 -615.65] (1.000)
Step: 40599, Reward: [-391.446 -391.446 -391.446] [62.1883], Avg: [-615.45 -615.45 -615.45] (1.000)
Step: 40649, Reward: [-451.26 -451.26 -451.26] [45.2150], Avg: [-615.304 -615.304 -615.304] (1.000)
Step: 40699, Reward: [-483.084 -483.084 -483.084] [108.8753], Avg: [-615.275 -615.275 -615.275] (1.000)
Step: 40749, Reward: [-421.47 -421.47 -421.47] [69.4856], Avg: [-615.122 -615.122 -615.122] (1.000)
Step: 40799, Reward: [-497.941 -497.941 -497.941] [88.3574], Avg: [-615.087 -615.087 -615.087] (1.000)
Step: 40849, Reward: [-442.431 -442.431 -442.431] [86.0617], Avg: [-614.981 -614.981 -614.981] (1.000)
Step: 40899, Reward: [-517.604 -517.604 -517.604] [185.5333], Avg: [-615.089 -615.089 -615.089] (1.000)
Step: 40949, Reward: [-533.399 -533.399 -533.399] [115.9455], Avg: [-615.131 -615.131 -615.131] (1.000)
Step: 40999, Reward: [-466.134 -466.134 -466.134] [39.1799], Avg: [-614.997 -614.997 -614.997] (1.000)
Step: 41049, Reward: [-592.897 -592.897 -592.897] [220.9970], Avg: [-615.239 -615.239 -615.239] (1.000)
Step: 41099, Reward: [-811.418 -811.418 -811.418] [726.6207], Avg: [-616.362 -616.362 -616.362] (1.000)
Step: 41149, Reward: [-785.722 -785.722 -785.722] [775.6633], Avg: [-617.51 -617.51 -617.51] (1.000)
Step: 41199, Reward: [-502.381 -502.381 -502.381] [97.4407], Avg: [-617.489 -617.489 -617.489] (1.000)
Step: 41249, Reward: [-620.17 -620.17 -620.17] [284.8377], Avg: [-617.837 -617.837 -617.837] (1.000)
Step: 41299, Reward: [-519.914 -519.914 -519.914] [148.3780], Avg: [-617.898 -617.898 -617.898] (1.000)
Step: 41349, Reward: [-549.196 -549.196 -549.196] [134.8661], Avg: [-617.978 -617.978 -617.978] (1.000)
Step: 41399, Reward: [-491.743 -491.743 -491.743] [68.3763], Avg: [-617.908 -617.908 -617.908] (1.000)
Step: 41449, Reward: [-814.644 -814.644 -814.644] [675.6710], Avg: [-618.961 -618.961 -618.961] (1.000)
Step: 41499, Reward: [-495.764 -495.764 -495.764] [77.5253], Avg: [-618.906 -618.906 -618.906] (1.000)
Step: 41549, Reward: [-808.826 -808.826 -808.826] [750.8661], Avg: [-620.038 -620.038 -620.038] (1.000)
Step: 41599, Reward: [-563.515 -563.515 -563.515] [142.4289], Avg: [-620.141 -620.141 -620.141] (1.000)
Step: 41649, Reward: [-763.28 -763.28 -763.28] [633.3393], Avg: [-621.073 -621.073 -621.073] (1.000)
Step: 41699, Reward: [-779.418 -779.418 -779.418] [718.4020], Avg: [-622.124 -622.124 -622.124] (1.000)
Step: 41749, Reward: [-1387.941 -1387.941 -1387.941] [800.6694], Avg: [-624. -624. -624.] (1.000)
Step: 41799, Reward: [-1785.162 -1785.162 -1785.162] [709.1101], Avg: [-626.238 -626.238 -626.238] (1.000)
Step: 41849, Reward: [-1650.275 -1650.275 -1650.275] [580.4977], Avg: [-628.155 -628.155 -628.155] (1.000)
Step: 41899, Reward: [-1479.775 -1479.775 -1479.775] [922.9820], Avg: [-630.272 -630.272 -630.272] (1.000)
Step: 41949, Reward: [-770.769 -770.769 -770.769] [602.9617], Avg: [-631.158 -631.158 -631.158] (1.000)
Step: 41999, Reward: [-1011.694 -1011.694 -1011.694] [699.6655], Avg: [-632.444 -632.444 -632.444] (1.000)
Step: 42049, Reward: [-382.576 -382.576 -382.576] [66.6434], Avg: [-632.226 -632.226 -632.226] (1.000)
Step: 42099, Reward: [-823.136 -823.136 -823.136] [525.3786], Avg: [-633.077 -633.077 -633.077] (1.000)
Step: 42149, Reward: [-506.55 -506.55 -506.55] [120.4129], Avg: [-633.07 -633.07 -633.07] (1.000)
Step: 42199, Reward: [-390.347 -390.347 -390.347] [75.5854], Avg: [-632.872 -632.872 -632.872] (1.000)
Step: 42249, Reward: [-822.841 -822.841 -822.841] [694.7536], Avg: [-633.919 -633.919 -633.919] (1.000)
Step: 42299, Reward: [-719.421 -719.421 -719.421] [675.8007], Avg: [-634.819 -634.819 -634.819] (1.000)
Step: 42349, Reward: [-544.682 -544.682 -544.682] [87.8234], Avg: [-634.816 -634.816 -634.816] (1.000)
Step: 42399, Reward: [-712.174 -712.174 -712.174] [621.6912], Avg: [-635.64 -635.64 -635.64] (1.000)
Step: 42449, Reward: [-629.991 -629.991 -629.991] [330.7784], Avg: [-636.023 -636.023 -636.023] (1.000)
Step: 42499, Reward: [-462.895 -462.895 -462.895] [116.4975], Avg: [-635.957 -635.957 -635.957] (1.000)
Step: 42549, Reward: [-366.505 -366.505 -366.505] [38.3694], Avg: [-635.685 -635.685 -635.685] (1.000)
Step: 42599, Reward: [-569.263 -569.263 -569.263] [306.0735], Avg: [-635.966 -635.966 -635.966] (1.000)
Step: 42649, Reward: [-416.738 -416.738 -416.738] [83.5713], Avg: [-635.807 -635.807 -635.807] (1.000)
Step: 42699, Reward: [-424.884 -424.884 -424.884] [35.7705], Avg: [-635.602 -635.602 -635.602] (1.000)
Step: 42749, Reward: [-370.651 -370.651 -370.651] [61.9590], Avg: [-635.365 -635.365 -635.365] (1.000)
Step: 42799, Reward: [-435.759 -435.759 -435.759] [117.6016], Avg: [-635.269 -635.269 -635.269] (1.000)
Step: 42849, Reward: [-458.412 -458.412 -458.412] [143.2545], Avg: [-635.23 -635.23 -635.23] (1.000)
Step: 42899, Reward: [-431.9 -431.9 -431.9] [99.4699], Avg: [-635.109 -635.109 -635.109] (1.000)
Step: 42949, Reward: [-402.396 -402.396 -402.396] [73.3500], Avg: [-634.923 -634.923 -634.923] (1.000)
Step: 42999, Reward: [-460.631 -460.631 -460.631] [110.6977], Avg: [-634.849 -634.849 -634.849] (1.000)
Step: 43049, Reward: [-570.436 -570.436 -570.436] [293.0917], Avg: [-635.115 -635.115 -635.115] (1.000)
Step: 43099, Reward: [-525.12 -525.12 -525.12] [98.4734], Avg: [-635.102 -635.102 -635.102] (1.000)
Step: 43149, Reward: [-674.178 -674.178 -674.178] [683.6072], Avg: [-635.939 -635.939 -635.939] (1.000)
Step: 43199, Reward: [-558.916 -558.916 -558.916] [394.3645], Avg: [-636.306 -636.306 -636.306] (1.000)
Step: 43249, Reward: [-488.275 -488.275 -488.275] [111.5111], Avg: [-636.264 -636.264 -636.264] (1.000)
Step: 43299, Reward: [-471.901 -471.901 -471.901] [96.8323], Avg: [-636.186 -636.186 -636.186] (1.000)
Step: 43349, Reward: [-593.895 -593.895 -593.895] [124.6729], Avg: [-636.281 -636.281 -636.281] (1.000)
Step: 43399, Reward: [-455.893 -455.893 -455.893] [67.0678], Avg: [-636.151 -636.151 -636.151] (1.000)
Step: 43449, Reward: [-451.796 -451.796 -451.796] [101.7128], Avg: [-636.055 -636.055 -636.055] (1.000)
Step: 43499, Reward: [-452.126 -452.126 -452.126] [83.1124], Avg: [-635.94 -635.94 -635.94] (1.000)
Step: 43549, Reward: [-446.102 -446.102 -446.102] [114.1165], Avg: [-635.853 -635.853 -635.853] (1.000)
Step: 43599, Reward: [-516.708 -516.708 -516.708] [103.9222], Avg: [-635.835 -635.835 -635.835] (1.000)
Step: 43649, Reward: [-915.814 -915.814 -915.814] [804.7591], Avg: [-637.078 -637.078 -637.078] (1.000)
Step: 43699, Reward: [-516.634 -516.634 -516.634] [211.7243], Avg: [-637.182 -637.182 -637.182] (1.000)
Step: 43749, Reward: [-432.134 -432.134 -432.134] [56.1028], Avg: [-637.012 -637.012 -637.012] (1.000)
Step: 43799, Reward: [-409.124 -409.124 -409.124] [56.1634], Avg: [-636.816 -636.816 -636.816] (1.000)
Step: 43849, Reward: [-517.942 -517.942 -517.942] [166.7765], Avg: [-636.871 -636.871 -636.871] (1.000)
Step: 43899, Reward: [-486.396 -486.396 -486.396] [155.7995], Avg: [-636.877 -636.877 -636.877] (1.000)
Step: 43949, Reward: [-441.523 -441.523 -441.523] [61.3632], Avg: [-636.724 -636.724 -636.724] (1.000)
Step: 43999, Reward: [-450.767 -450.767 -450.767] [19.2997], Avg: [-636.535 -636.535 -636.535] (1.000)
Step: 44049, Reward: [-650.449 -650.449 -650.449] [59.2281], Avg: [-636.618 -636.618 -636.618] (1.000)
Step: 44099, Reward: [-445.824 -445.824 -445.824] [109.6299], Avg: [-636.526 -636.526 -636.526] (1.000)
Step: 44149, Reward: [-463.334 -463.334 -463.334] [83.7589], Avg: [-636.424 -636.424 -636.424] (1.000)
Step: 44199, Reward: [-518.157 -518.157 -518.157] [45.6989], Avg: [-636.342 -636.342 -636.342] (1.000)
Step: 44249, Reward: [-409.76 -409.76 -409.76] [109.2570], Avg: [-636.21 -636.21 -636.21] (1.000)
Step: 44299, Reward: [-434.922 -434.922 -434.922] [122.1122], Avg: [-636.12 -636.12 -636.12] (1.000)
Step: 44349, Reward: [-423.18 -423.18 -423.18] [53.3634], Avg: [-635.941 -635.941 -635.941] (1.000)
Step: 44399, Reward: [-543.008 -543.008 -543.008] [202.1917], Avg: [-636.064 -636.064 -636.064] (1.000)
Step: 44449, Reward: [-415.471 -415.471 -415.471] [48.5295], Avg: [-635.87 -635.87 -635.87] (1.000)
Step: 44499, Reward: [-447.863 -447.863 -447.863] [67.8668], Avg: [-635.735 -635.735 -635.735] (1.000)
Step: 44549, Reward: [-448.338 -448.338 -448.338] [56.1077], Avg: [-635.588 -635.588 -635.588] (1.000)
Step: 44599, Reward: [-385.247 -385.247 -385.247] [41.3615], Avg: [-635.353 -635.353 -635.353] (1.000)
Step: 44649, Reward: [-350.104 -350.104 -350.104] [47.8388], Avg: [-635.088 -635.088 -635.088] (1.000)
Step: 44699, Reward: [-404.034 -404.034 -404.034] [87.0409], Avg: [-634.926 -634.926 -634.926] (1.000)
Step: 44749, Reward: [-414.209 -414.209 -414.209] [35.0104], Avg: [-634.719 -634.719 -634.719] (1.000)
Step: 44799, Reward: [-434.597 -434.597 -434.597] [47.9351], Avg: [-634.549 -634.549 -634.549] (1.000)
Step: 44849, Reward: [-622.95 -622.95 -622.95] [431.5903], Avg: [-635.017 -635.017 -635.017] (1.000)
Step: 44899, Reward: [-704.395 -704.395 -704.395] [435.0489], Avg: [-635.579 -635.579 -635.579] (1.000)
Step: 44949, Reward: [-458.499 -458.499 -458.499] [90.3873], Avg: [-635.483 -635.483 -635.483] (1.000)
Step: 44999, Reward: [-1689.171 -1689.171 -1689.171] [557.0823], Avg: [-637.272 -637.272 -637.272] (1.000)
Step: 45049, Reward: [-2085.768 -2085.768 -2085.768] [135.1226], Avg: [-639.03 -639.03 -639.03] (1.000)
Step: 45099, Reward: [-1935.352 -1935.352 -1935.352] [298.0149], Avg: [-640.798 -640.798 -640.798] (1.000)
Step: 45149, Reward: [-1993.715 -1993.715 -1993.715] [203.7659], Avg: [-642.521 -642.521 -642.521] (1.000)
Step: 45199, Reward: [-2172.604 -2172.604 -2172.604] [136.2129], Avg: [-644.365 -644.365 -644.365] (1.000)
Step: 45249, Reward: [-1108.762 -1108.762 -1108.762] [704.2551], Avg: [-645.656 -645.656 -645.656] (1.000)
Step: 45299, Reward: [-1950.848 -1950.848 -1950.848] [187.2071], Avg: [-647.303 -647.303 -647.303] (1.000)
Step: 45349, Reward: [-1898.731 -1898.731 -1898.731] [120.3118], Avg: [-648.816 -648.816 -648.816] (1.000)
Step: 45399, Reward: [-2086.24 -2086.24 -2086.24] [140.4647], Avg: [-650.553 -650.553 -650.553] (1.000)
Step: 45449, Reward: [-1906.503 -1906.503 -1906.503] [178.6751], Avg: [-652.132 -652.132 -652.132] (1.000)
Step: 45499, Reward: [-1987.225 -1987.225 -1987.225] [156.0724], Avg: [-653.77 -653.77 -653.77] (1.000)
Step: 45549, Reward: [-2087.264 -2087.264 -2087.264] [162.7592], Avg: [-655.523 -655.523 -655.523] (1.000)
Step: 45599, Reward: [-2090.064 -2090.064 -2090.064] [297.7092], Avg: [-657.422 -657.422 -657.422] (1.000)
Step: 45649, Reward: [-1859.37 -1859.37 -1859.37] [122.6469], Avg: [-658.873 -658.873 -658.873] (1.000)
Step: 45699, Reward: [-1157.006 -1157.006 -1157.006] [183.0472], Avg: [-659.618 -659.618 -659.618] (1.000)
Step: 45749, Reward: [-1240.272 -1240.272 -1240.272] [129.5948], Avg: [-660.394 -660.394 -660.394] (1.000)
Step: 45799, Reward: [-1789.651 -1789.651 -1789.651] [139.1277], Avg: [-661.779 -661.779 -661.779] (1.000)
Step: 45849, Reward: [-1542.81 -1542.81 -1542.81] [140.1302], Avg: [-662.893 -662.893 -662.893] (1.000)
Step: 45899, Reward: [-1577.458 -1577.458 -1577.458] [218.4745], Avg: [-664.127 -664.127 -664.127] (1.000)
Step: 45949, Reward: [-1548.962 -1548.962 -1548.962] [70.2985], Avg: [-665.166 -665.166 -665.166] (1.000)
Step: 45999, Reward: [-1707.137 -1707.137 -1707.137] [354.4862], Avg: [-666.684 -666.684 -666.684] (1.000)
Step: 46049, Reward: [-2124.526 -2124.526 -2124.526] [216.4642], Avg: [-668.502 -668.502 -668.502] (1.000)
Step: 46099, Reward: [-1980.547 -1980.547 -1980.547] [155.1737], Avg: [-670.093 -670.093 -670.093] (1.000)
Step: 46149, Reward: [-1906.192 -1906.192 -1906.192] [170.0445], Avg: [-671.617 -671.617 -671.617] (1.000)
Step: 46199, Reward: [-2006.671 -2006.671 -2006.671] [196.2226], Avg: [-673.274 -673.274 -673.274] (1.000)
Step: 46249, Reward: [-2120.961 -2120.961 -2120.961] [121.0729], Avg: [-674.97 -674.97 -674.97] (1.000)
Step: 46299, Reward: [-2122.942 -2122.942 -2122.942] [192.6619], Avg: [-676.742 -676.742 -676.742] (1.000)
Step: 46349, Reward: [-1944.964 -1944.964 -1944.964] [95.6360], Avg: [-678.213 -678.213 -678.213] (1.000)
Step: 46399, Reward: [-2121.413 -2121.413 -2121.413] [192.2227], Avg: [-679.975 -679.975 -679.975] (1.000)
Step: 46449, Reward: [-2114.301 -2114.301 -2114.301] [154.3909], Avg: [-681.685 -681.685 -681.685] (1.000)
Step: 46499, Reward: [-2046.417 -2046.417 -2046.417] [47.9313], Avg: [-683.204 -683.204 -683.204] (1.000)
Step: 46549, Reward: [-1955.266 -1955.266 -1955.266] [101.8285], Avg: [-684.68 -684.68 -684.68] (1.000)
Step: 46599, Reward: [-2090.143 -2090.143 -2090.143] [172.1818], Avg: [-686.373 -686.373 -686.373] (1.000)
Step: 46649, Reward: [-1891.337 -1891.337 -1891.337] [178.2365], Avg: [-687.855 -687.855 -687.855] (1.000)
Step: 46699, Reward: [-1659.28 -1659.28 -1659.28] [184.3935], Avg: [-689.093 -689.093 -689.093] (1.000)
Step: 46749, Reward: [-1825.02 -1825.02 -1825.02] [44.0308], Avg: [-690.355 -690.355 -690.355] (1.000)
Step: 46799, Reward: [-1626.626 -1626.626 -1626.626] [182.5541], Avg: [-691.55 -691.55 -691.55] (1.000)
Step: 46849, Reward: [-1723.128 -1723.128 -1723.128] [171.8599], Avg: [-692.834 -692.834 -692.834] (1.000)
Step: 46899, Reward: [-1357.099 -1357.099 -1357.099] [91.6291], Avg: [-693.64 -693.64 -693.64] (1.000)
Step: 46949, Reward: [-1399.275 -1399.275 -1399.275] [132.5555], Avg: [-694.533 -694.533 -694.533] (1.000)
Step: 46999, Reward: [-787.835 -787.835 -787.835] [80.1776], Avg: [-694.718 -694.718 -694.718] (1.000)
Step: 47049, Reward: [-504.8 -504.8 -504.8] [76.4527], Avg: [-694.597 -694.597 -694.597] (1.000)
Step: 47099, Reward: [-477.605 -477.605 -477.605] [95.6984], Avg: [-694.468 -694.468 -694.468] (1.000)
Step: 47149, Reward: [-641.318 -641.318 -641.318] [59.8100], Avg: [-694.475 -694.475 -694.475] (1.000)
Step: 47199, Reward: [-561.96 -561.96 -561.96] [126.1146], Avg: [-694.468 -694.468 -694.468] (1.000)
Step: 47249, Reward: [-599.218 -599.218 -599.218] [72.7995], Avg: [-694.445 -694.445 -694.445] (1.000)
Step: 47299, Reward: [-549.887 -549.887 -549.887] [167.4647], Avg: [-694.469 -694.469 -694.469] (1.000)
Step: 47349, Reward: [-584.758 -584.758 -584.758] [118.6181], Avg: [-694.478 -694.478 -694.478] (1.000)
Step: 47399, Reward: [-623.584 -623.584 -623.584] [173.4673], Avg: [-694.587 -694.587 -694.587] (1.000)
Step: 47449, Reward: [-756.183 -756.183 -756.183] [114.6841], Avg: [-694.772 -694.772 -694.772] (1.000)
Step: 47499, Reward: [-507.374 -507.374 -507.374] [153.2709], Avg: [-694.736 -694.736 -694.736] (1.000)
Step: 47549, Reward: [-451.289 -451.289 -451.289] [66.7139], Avg: [-694.551 -694.551 -694.551] (1.000)
Step: 47599, Reward: [-568.196 -568.196 -568.196] [112.5645], Avg: [-694.536 -694.536 -694.536] (1.000)
Step: 47649, Reward: [-451.766 -451.766 -451.766] [49.6249], Avg: [-694.333 -694.333 -694.333] (1.000)
Step: 47699, Reward: [-456.342 -456.342 -456.342] [108.7939], Avg: [-694.198 -694.198 -694.198] (1.000)
Step: 47749, Reward: [-481.93 -481.93 -481.93] [25.2772], Avg: [-694.002 -694.002 -694.002] (1.000)
Step: 47799, Reward: [-489.693 -489.693 -489.693] [140.2936], Avg: [-693.935 -693.935 -693.935] (1.000)
Step: 47849, Reward: [-521.846 -521.846 -521.846] [78.0207], Avg: [-693.837 -693.837 -693.837] (1.000)
Step: 47899, Reward: [-497.961 -497.961 -497.961] [77.6176], Avg: [-693.713 -693.713 -693.713] (1.000)
Step: 47949, Reward: [-500.215 -500.215 -500.215] [71.6020], Avg: [-693.586 -693.586 -693.586] (1.000)
Step: 47999, Reward: [-550.249 -550.249 -550.249] [219.0538], Avg: [-693.665 -693.665 -693.665] (1.000)
Step: 48049, Reward: [-501.38 -501.38 -501.38] [76.1221], Avg: [-693.544 -693.544 -693.544] (1.000)
Step: 48099, Reward: [-809.434 -809.434 -809.434] [308.2449], Avg: [-693.985 -693.985 -693.985] (1.000)
Step: 48149, Reward: [-506.572 -506.572 -506.572] [137.4530], Avg: [-693.933 -693.933 -693.933] (1.000)
Step: 48199, Reward: [-1224.084 -1224.084 -1224.084] [541.6347], Avg: [-695.045 -695.045 -695.045] (1.000)
Step: 48249, Reward: [-1354.155 -1354.155 -1354.155] [427.1004], Avg: [-696.171 -696.171 -696.171] (1.000)
Step: 48299, Reward: [-1700.286 -1700.286 -1700.286] [648.2858], Avg: [-697.881 -697.881 -697.881] (1.000)
Step: 48349, Reward: [-1779.21 -1779.21 -1779.21] [593.7458], Avg: [-699.614 -699.614 -699.614] (1.000)
Step: 48399, Reward: [-2058.6 -2058.6 -2058.6] [206.7721], Avg: [-701.231 -701.231 -701.231] (1.000)
Step: 48449, Reward: [-2061.54 -2061.54 -2061.54] [126.1807], Avg: [-702.765 -702.765 -702.765] (1.000)
Step: 48499, Reward: [-2112.204 -2112.204 -2112.204] [161.7931], Avg: [-704.385 -704.385 -704.385] (1.000)
Step: 48549, Reward: [-1887.668 -1887.668 -1887.668] [105.5898], Avg: [-705.712 -705.712 -705.712] (1.000)
Step: 48599, Reward: [-2109.49 -2109.49 -2109.49] [119.1684], Avg: [-707.279 -707.279 -707.279] (1.000)
Step: 48649, Reward: [-2027.792 -2027.792 -2027.792] [227.4591], Avg: [-708.87 -708.87 -708.87] (1.000)
Step: 48699, Reward: [-1876.951 -1876.951 -1876.951] [498.3396], Avg: [-710.581 -710.581 -710.581] (1.000)
Step: 48749, Reward: [-1423.971 -1423.971 -1423.971] [479.0519], Avg: [-711.804 -711.804 -711.804] (1.000)
Step: 48799, Reward: [-1451.26 -1451.26 -1451.26] [567.1725], Avg: [-713.143 -713.143 -713.143] (1.000)
Step: 48849, Reward: [-1589.214 -1589.214 -1589.214] [233.8240], Avg: [-714.279 -714.279 -714.279] (1.000)
Step: 48899, Reward: [-1701.497 -1701.497 -1701.497] [381.7085], Avg: [-715.678 -715.678 -715.678] (1.000)
Step: 48949, Reward: [-1813.54 -1813.54 -1813.54] [209.1211], Avg: [-717.013 -717.013 -717.013] (1.000)
Step: 48999, Reward: [-1754.592 -1754.592 -1754.592] [380.3496], Avg: [-718.46 -718.46 -718.46] (1.000)
Step: 49049, Reward: [-684.963 -684.963 -684.963] [289.8871], Avg: [-718.722 -718.722 -718.722] (1.000)
Step: 49099, Reward: [-946.393 -946.393 -946.393] [531.3613], Avg: [-719.495 -719.495 -719.495] (1.000)
Step: 49149, Reward: [-549.732 -549.732 -549.732] [160.4919], Avg: [-719.485 -719.485 -719.485] (1.000)
Step: 49199, Reward: [-889.921 -889.921 -889.921] [610.6974], Avg: [-720.279 -720.279 -720.279] (1.000)
Step: 49249, Reward: [-468.65 -468.65 -468.65] [114.7958], Avg: [-720.14 -720.14 -720.14] (1.000)
Step: 49299, Reward: [-595.473 -595.473 -595.473] [96.9123], Avg: [-720.112 -720.112 -720.112] (1.000)
Step: 49349, Reward: [-574.575 -574.575 -574.575] [186.3340], Avg: [-720.153 -720.153 -720.153] (1.000)
Step: 49399, Reward: [-358.829 -358.829 -358.829] [86.8485], Avg: [-719.876 -719.876 -719.876] (1.000)
Step: 49449, Reward: [-414.322 -414.322 -414.322] [43.9401], Avg: [-719.611 -719.611 -719.611] (1.000)
Step: 49499, Reward: [-505.43 -505.43 -505.43] [64.8580], Avg: [-719.46 -719.46 -719.46] (1.000)
Step: 49549, Reward: [-486.232 -486.232 -486.232] [56.3087], Avg: [-719.282 -719.282 -719.282] (1.000)
Step: 49599, Reward: [-481.176 -481.176 -481.176] [144.0198], Avg: [-719.187 -719.187 -719.187] (1.000)
Step: 49649, Reward: [-484.514 -484.514 -484.514] [111.3458], Avg: [-719.063 -719.063 -719.063] (1.000)
Step: 49699, Reward: [-476.154 -476.154 -476.154] [72.9569], Avg: [-718.892 -718.892 -718.892] (1.000)
Step: 49749, Reward: [-429.612 -429.612 -429.612] [65.8943], Avg: [-718.667 -718.667 -718.667] (1.000)
Step: 49799, Reward: [-382.728 -382.728 -382.728] [62.1666], Avg: [-718.392 -718.392 -718.392] (1.000)
Step: 49849, Reward: [-408.973 -408.973 -408.973] [84.0366], Avg: [-718.166 -718.166 -718.166] (1.000)
Step: 49899, Reward: [-408.886 -408.886 -408.886] [69.8773], Avg: [-717.926 -717.926 -717.926] (1.000)
Step: 49949, Reward: [-467.244 -467.244 -467.244] [76.8166], Avg: [-717.752 -717.752 -717.752] (1.000)
Step: 49999, Reward: [-466.308 -466.308 -466.308] [118.5823], Avg: [-717.619 -717.619 -717.619] (1.000)
Step: 50049, Reward: [-430.791 -430.791 -430.791] [83.5550], Avg: [-717.416 -717.416 -717.416] (1.000)
Step: 50099, Reward: [-413.159 -413.159 -413.159] [103.6052], Avg: [-717.216 -717.216 -717.216] (1.000)
Step: 50149, Reward: [-743.547 -743.547 -743.547] [700.1922], Avg: [-717.94 -717.94 -717.94] (1.000)
Step: 50199, Reward: [-449.572 -449.572 -449.572] [101.4618], Avg: [-717.774 -717.774 -717.774] (1.000)
Step: 50249, Reward: [-613.371 -613.371 -613.371] [101.5014], Avg: [-717.771 -717.771 -717.771] (1.000)
Step: 50299, Reward: [-456.869 -456.869 -456.869] [113.9395], Avg: [-717.625 -717.625 -717.625] (1.000)
Step: 50349, Reward: [-506.2 -506.2 -506.2] [92.3311], Avg: [-717.507 -717.507 -717.507] (1.000)
Step: 50399, Reward: [-439.406 -439.406 -439.406] [124.8680], Avg: [-717.355 -717.355 -717.355] (1.000)
Step: 50449, Reward: [-501.443 -501.443 -501.443] [86.3511], Avg: [-717.227 -717.227 -717.227] (1.000)
Step: 50499, Reward: [-1098.603 -1098.603 -1098.603] [761.1075], Avg: [-718.358 -718.358 -718.358] (1.000)
Step: 50549, Reward: [-545.347 -545.347 -545.347] [85.1465], Avg: [-718.271 -718.271 -718.271] (1.000)
Step: 50599, Reward: [-945.203 -945.203 -945.203] [518.2910], Avg: [-719.007 -719.007 -719.007] (1.000)
Step: 50649, Reward: [-961.574 -961.574 -961.574] [679.6216], Avg: [-719.918 -719.918 -719.918] (1.000)
Step: 50699, Reward: [-870.885 -870.885 -870.885] [747.2289], Avg: [-720.803 -720.803 -720.803] (1.000)
Step: 50749, Reward: [-1112.436 -1112.436 -1112.436] [855.4590], Avg: [-722.032 -722.032 -722.032] (1.000)
Step: 50799, Reward: [-533.525 -533.525 -533.525] [135.3514], Avg: [-721.98 -721.98 -721.98] (1.000)
Step: 50849, Reward: [-648.557 -648.557 -648.557] [66.8901], Avg: [-721.973 -721.973 -721.973] (1.000)
Step: 50899, Reward: [-590.812 -590.812 -590.812] [120.7260], Avg: [-721.963 -721.963 -721.963] (1.000)
Step: 50949, Reward: [-1276.914 -1276.914 -1276.914] [718.5338], Avg: [-723.213 -723.213 -723.213] (1.000)
Step: 50999, Reward: [-551.911 -551.911 -551.911] [153.8398], Avg: [-723.196 -723.196 -723.196] (1.000)
Step: 51049, Reward: [-2148.522 -2148.522 -2148.522] [185.6684], Avg: [-724.773 -724.773 -724.773] (1.000)
Step: 51099, Reward: [-619.18 -619.18 -619.18] [99.5466], Avg: [-724.768 -724.768 -724.768] (1.000)
Step: 51149, Reward: [-2265.022 -2265.022 -2265.022] [247.2916], Avg: [-726.515 -726.515 -726.515] (1.000)
Step: 51199, Reward: [-2081.776 -2081.776 -2081.776] [208.8587], Avg: [-728.042 -728.042 -728.042] (1.000)
Step: 51249, Reward: [-2092.313 -2092.313 -2092.313] [89.1770], Avg: [-729.46 -729.46 -729.46] (1.000)
Step: 51299, Reward: [-2043.707 -2043.707 -2043.707] [189.3590], Avg: [-730.926 -730.926 -730.926] (1.000)
Step: 51349, Reward: [-533.158 -533.158 -533.158] [161.2781], Avg: [-730.89 -730.89 -730.89] (1.000)
Step: 51399, Reward: [-1451.654 -1451.654 -1451.654] [741.2886], Avg: [-732.313 -732.313 -732.313] (1.000)
Step: 51449, Reward: [-861.637 -861.637 -861.637] [530.5953], Avg: [-732.954 -732.954 -732.954] (1.000)
Step: 51499, Reward: [-872.741 -872.741 -872.741] [587.9834], Avg: [-733.66 -733.66 -733.66] (1.000)
Step: 51549, Reward: [-875.783 -875.783 -875.783] [663.5570], Avg: [-734.442 -734.442 -734.442] (1.000)
Step: 51599, Reward: [-598.629 -598.629 -598.629] [177.3324], Avg: [-734.482 -734.482 -734.482] (1.000)
Step: 51649, Reward: [-484.394 -484.394 -484.394] [66.7828], Avg: [-734.305 -734.305 -734.305] (1.000)
Step: 51699, Reward: [-498.392 -498.392 -498.392] [81.8516], Avg: [-734.156 -734.156 -734.156] (1.000)
Step: 51749, Reward: [-508.138 -508.138 -508.138] [154.4332], Avg: [-734.087 -734.087 -734.087] (1.000)
Step: 51799, Reward: [-504.828 -504.828 -504.828] [133.5457], Avg: [-733.994 -733.994 -733.994] (1.000)
Step: 51849, Reward: [-477.818 -477.818 -477.818] [92.8053], Avg: [-733.837 -733.837 -733.837] (1.000)
Step: 51899, Reward: [-560.276 -560.276 -560.276] [160.6215], Avg: [-733.824 -733.824 -733.824] (1.000)
Step: 51949, Reward: [-547.04 -547.04 -547.04] [60.2164], Avg: [-733.702 -733.702 -733.702] (1.000)
Step: 51999, Reward: [-836.646 -836.646 -836.646] [569.3461], Avg: [-734.349 -734.349 -734.349] (1.000)
Step: 52049, Reward: [-570.887 -570.887 -570.887] [170.2740], Avg: [-734.355 -734.355 -734.355] (1.000)
Step: 52099, Reward: [-800.691 -800.691 -800.691] [693.6200], Avg: [-735.085 -735.085 -735.085] (1.000)
Step: 52149, Reward: [-460.218 -460.218 -460.218] [95.7745], Avg: [-734.913 -734.913 -734.913] (1.000)
Step: 52199, Reward: [-767.273 -767.273 -767.273] [479.8275], Avg: [-735.404 -735.404 -735.404] (1.000)
Step: 52249, Reward: [-456.058 -456.058 -456.058] [112.1648], Avg: [-735.244 -735.244 -735.244] (1.000)
Step: 52299, Reward: [-459.407 -459.407 -459.407] [27.4936], Avg: [-735.006 -735.006 -735.006] (1.000)
Step: 52349, Reward: [-484.301 -484.301 -484.301] [55.3654], Avg: [-734.82 -734.82 -734.82] (1.000)
Step: 52399, Reward: [-439.054 -439.054 -439.054] [58.7718], Avg: [-734.593 -734.593 -734.593] (1.000)
Step: 52449, Reward: [-446.586 -446.586 -446.586] [28.4399], Avg: [-734.346 -734.346 -734.346] (1.000)
Step: 52499, Reward: [-376.877 -376.877 -376.877] [61.0084], Avg: [-734.064 -734.064 -734.064] (1.000)
Step: 52549, Reward: [-508.348 -508.348 -508.348] [167.0186], Avg: [-734.008 -734.008 -734.008] (1.000)
Step: 52599, Reward: [-488.866 -488.866 -488.866] [87.1786], Avg: [-733.858 -733.858 -733.858] (1.000)
Step: 52649, Reward: [-524.484 -524.484 -524.484] [126.9661], Avg: [-733.779 -733.779 -733.779] (1.000)
Step: 52699, Reward: [-386.363 -386.363 -386.363] [108.7357], Avg: [-733.553 -733.553 -733.553] (1.000)
Step: 52749, Reward: [-456.136 -456.136 -456.136] [101.3715], Avg: [-733.386 -733.386 -733.386] (1.000)
Step: 52799, Reward: [-432.879 -432.879 -432.879] [50.3044], Avg: [-733.149 -733.149 -733.149] (1.000)
Step: 52849, Reward: [-461.744 -461.744 -461.744] [153.6624], Avg: [-733.038 -733.038 -733.038] (1.000)
Step: 52899, Reward: [-594.375 -594.375 -594.375] [302.9461], Avg: [-733.193 -733.193 -733.193] (1.000)
Step: 52949, Reward: [-449.153 -449.153 -449.153] [80.6411], Avg: [-733.001 -733.001 -733.001] (1.000)
Step: 52999, Reward: [-498.178 -498.178 -498.178] [185.1632], Avg: [-732.954 -732.954 -732.954] (1.000)
Step: 53049, Reward: [-549.026 -549.026 -549.026] [233.9721], Avg: [-733.001 -733.001 -733.001] (1.000)
Step: 53099, Reward: [-484.51 -484.51 -484.51] [84.5187], Avg: [-732.847 -732.847 -732.847] (1.000)
Step: 53149, Reward: [-482.213 -482.213 -482.213] [62.5363], Avg: [-732.67 -732.67 -732.67] (1.000)
Step: 53199, Reward: [-563.509 -563.509 -563.509] [119.9795], Avg: [-732.624 -732.624 -732.624] (1.000)
Step: 53249, Reward: [-357.908 -357.908 -357.908] [53.2568], Avg: [-732.322 -732.322 -732.322] (1.000)
Step: 53299, Reward: [-346.266 -346.266 -346.266] [79.0303], Avg: [-732.034 -732.034 -732.034] (1.000)
Step: 53349, Reward: [-427.92 -427.92 -427.92] [38.1641], Avg: [-731.785 -731.785 -731.785] (1.000)
Step: 53399, Reward: [-408.561 -408.561 -408.561] [72.8001], Avg: [-731.55 -731.55 -731.55] (1.000)
Step: 53449, Reward: [-479.23 -479.23 -479.23] [120.7446], Avg: [-731.427 -731.427 -731.427] (1.000)
Step: 53499, Reward: [-494.511 -494.511 -494.511] [98.9996], Avg: [-731.298 -731.298 -731.298] (1.000)
Step: 53549, Reward: [-503.056 -503.056 -503.056] [137.0999], Avg: [-731.213 -731.213 -731.213] (1.000)
Step: 53599, Reward: [-432.25 -432.25 -432.25] [125.3761], Avg: [-731.051 -731.051 -731.051] (1.000)
Step: 53649, Reward: [-457.343 -457.343 -457.343] [43.7785], Avg: [-730.837 -730.837 -730.837] (1.000)
Step: 53699, Reward: [-717.124 -717.124 -717.124] [563.4946], Avg: [-731.349 -731.349 -731.349] (1.000)
Step: 53749, Reward: [-501.247 -501.247 -501.247] [157.5005], Avg: [-731.281 -731.281 -731.281] (1.000)
Step: 53799, Reward: [-472.432 -472.432 -472.432] [137.8594], Avg: [-731.169 -731.169 -731.169] (1.000)
Step: 53849, Reward: [-428.685 -428.685 -428.685] [85.6782], Avg: [-730.967 -730.967 -730.967] (1.000)
Step: 53899, Reward: [-458.593 -458.593 -458.593] [59.2010], Avg: [-730.77 -730.77 -730.77] (1.000)
Step: 53949, Reward: [-546.349 -546.349 -546.349] [136.7028], Avg: [-730.725 -730.725 -730.725] (1.000)
Step: 53999, Reward: [-560.806 -560.806 -560.806] [178.7952], Avg: [-730.734 -730.734 -730.734] (1.000)
Step: 54049, Reward: [-396.825 -396.825 -396.825] [61.5264], Avg: [-730.482 -730.482 -730.482] (1.000)
Step: 54099, Reward: [-462.02 -462.02 -462.02] [236.5735], Avg: [-730.452 -730.452 -730.452] (1.000)
Step: 54149, Reward: [-433.887 -433.887 -433.887] [93.6507], Avg: [-730.265 -730.265 -730.265] (1.000)
Step: 54199, Reward: [-474.474 -474.474 -474.474] [218.6295], Avg: [-730.231 -730.231 -730.231] (1.000)
Step: 54249, Reward: [-728.156 -728.156 -728.156] [559.5035], Avg: [-730.744 -730.744 -730.744] (1.000)
Step: 54299, Reward: [-460.641 -460.641 -460.641] [210.8561], Avg: [-730.69 -730.69 -730.69] (1.000)
Step: 54349, Reward: [-580.03 -580.03 -580.03] [186.6852], Avg: [-730.723 -730.723 -730.723] (1.000)
Step: 54399, Reward: [-378.993 -378.993 -378.993] [54.7889], Avg: [-730.45 -730.45 -730.45] (1.000)
Step: 54449, Reward: [-783.084 -783.084 -783.084] [345.0138], Avg: [-730.815 -730.815 -730.815] (1.000)
Step: 54499, Reward: [-757.306 -757.306 -757.306] [657.3854], Avg: [-731.443 -731.443 -731.443] (1.000)
Step: 54549, Reward: [-439.441 -439.441 -439.441] [49.5157], Avg: [-731.22 -731.22 -731.22] (1.000)
Step: 54599, Reward: [-777.517 -777.517 -777.517] [605.4168], Avg: [-731.817 -731.817 -731.817] (1.000)
Step: 54649, Reward: [-812.046 -812.046 -812.046] [713.0659], Avg: [-732.543 -732.543 -732.543] (1.000)
Step: 54699, Reward: [-571.65 -571.65 -571.65] [317.0178], Avg: [-732.686 -732.686 -732.686] (1.000)
Step: 54749, Reward: [-531.84 -531.84 -531.84] [179.1457], Avg: [-732.666 -732.666 -732.666] (1.000)
Step: 54799, Reward: [-853.739 -853.739 -853.739] [689.7508], Avg: [-733.406 -733.406 -733.406] (1.000)
Step: 54849, Reward: [-376.661 -376.661 -376.661] [55.9439], Avg: [-733.131 -733.131 -733.131] (1.000)
Step: 54899, Reward: [-475.854 -475.854 -475.854] [96.4918], Avg: [-732.985 -732.985 -732.985] (1.000)
Step: 54949, Reward: [-472.336 -472.336 -472.336] [186.6083], Avg: [-732.918 -732.918 -732.918] (1.000)
Step: 54999, Reward: [-413.6 -413.6 -413.6] [50.1326], Avg: [-732.673 -732.673 -732.673] (1.000)
Step: 55049, Reward: [-474.053 -474.053 -474.053] [65.2071], Avg: [-732.497 -732.497 -732.497] (1.000)
Step: 55099, Reward: [-404.389 -404.389 -404.389] [96.8128], Avg: [-732.287 -732.287 -732.287] (1.000)
Step: 55149, Reward: [-497.772 -497.772 -497.772] [171.1578], Avg: [-732.23 -732.23 -732.23] (1.000)
Step: 55199, Reward: [-536.835 -536.835 -536.835] [330.6691], Avg: [-732.352 -732.352 -732.352] (1.000)
Step: 55249, Reward: [-779.051 -779.051 -779.051] [616.9671], Avg: [-732.953 -732.953 -732.953] (1.000)
Step: 55299, Reward: [-481.432 -481.432 -481.432] [84.0573], Avg: [-732.802 -732.802 -732.802] (1.000)
Step: 55349, Reward: [-473.686 -473.686 -473.686] [141.6686], Avg: [-732.695 -732.695 -732.695] (1.000)
Step: 55399, Reward: [-499.094 -499.094 -499.094] [56.8913], Avg: [-732.536 -732.536 -732.536] (1.000)
Step: 55449, Reward: [-547.225 -547.225 -547.225] [117.5096], Avg: [-732.475 -732.475 -732.475] (1.000)
Step: 55499, Reward: [-459.12 -459.12 -459.12] [94.6451], Avg: [-732.314 -732.314 -732.314] (1.000)
Step: 55549, Reward: [-516.552 -516.552 -516.552] [41.0868], Avg: [-732.157 -732.157 -732.157] (1.000)
Step: 55599, Reward: [-432.607 -432.607 -432.607] [123.2846], Avg: [-731.998 -731.998 -731.998] (1.000)
Step: 55649, Reward: [-773.315 -773.315 -773.315] [597.4208], Avg: [-732.572 -732.572 -732.572] (1.000)
Step: 55699, Reward: [-470.045 -470.045 -470.045] [116.8631], Avg: [-732.441 -732.441 -732.441] (1.000)
Step: 55749, Reward: [-397.563 -397.563 -397.563] [45.8801], Avg: [-732.182 -732.182 -732.182] (1.000)
Step: 55799, Reward: [-408.828 -408.828 -408.828] [78.4979], Avg: [-731.963 -731.963 -731.963] (1.000)
Step: 55849, Reward: [-546.098 -546.098 -546.098] [161.1090], Avg: [-731.94 -731.94 -731.94] (1.000)
Step: 55899, Reward: [-498.618 -498.618 -498.618] [68.3738], Avg: [-731.793 -731.793 -731.793] (1.000)
Step: 55949, Reward: [-555.7 -555.7 -555.7] [239.1802], Avg: [-731.849 -731.849 -731.849] (1.000)
Step: 55999, Reward: [-767.783 -767.783 -767.783] [735.3300], Avg: [-732.538 -732.538 -732.538] (1.000)
Step: 56049, Reward: [-664.783 -664.783 -664.783] [509.9167], Avg: [-732.932 -732.932 -732.932] (1.000)
Step: 56099, Reward: [-538.606 -538.606 -538.606] [212.4126], Avg: [-732.949 -732.949 -732.949] (1.000)
Step: 56149, Reward: [-717.617 -717.617 -717.617] [628.7448], Avg: [-733.495 -733.495 -733.495] (1.000)
Step: 56199, Reward: [-426.221 -426.221 -426.221] [97.0111], Avg: [-733.308 -733.308 -733.308] (1.000)
Step: 56249, Reward: [-477.302 -477.302 -477.302] [220.1074], Avg: [-733.276 -733.276 -733.276] (1.000)
Step: 56299, Reward: [-493.388 -493.388 -493.388] [32.8795], Avg: [-733.092 -733.092 -733.092] (1.000)
Step: 56349, Reward: [-796.162 -796.162 -796.162] [623.9739], Avg: [-733.702 -733.702 -733.702] (1.000)
Step: 56399, Reward: [-871.821 -871.821 -871.821] [703.2554], Avg: [-734.447 -734.447 -734.447] (1.000)
Step: 56449, Reward: [-853.411 -853.411 -853.411] [756.2324], Avg: [-735.223 -735.223 -735.223] (1.000)
Step: 56499, Reward: [-375.959 -375.959 -375.959] [88.5543], Avg: [-734.983 -734.983 -734.983] (1.000)
Step: 56549, Reward: [-477.053 -477.053 -477.053] [114.5881], Avg: [-734.856 -734.856 -734.856] (1.000)
Step: 56599, Reward: [-348.503 -348.503 -348.503] [41.2998], Avg: [-734.552 -734.552 -734.552] (1.000)
Step: 56649, Reward: [-415.802 -415.802 -415.802] [178.7134], Avg: [-734.428 -734.428 -734.428] (1.000)
Step: 56699, Reward: [-514.664 -514.664 -514.664] [208.3521], Avg: [-734.418 -734.418 -734.418] (1.000)
Step: 56749, Reward: [-450.921 -450.921 -450.921] [76.9453], Avg: [-734.236 -734.236 -734.236] (1.000)
Step: 56799, Reward: [-441.236 -441.236 -441.236] [38.0215], Avg: [-734.011 -734.011 -734.011] (1.000)
Step: 56849, Reward: [-501.35 -501.35 -501.35] [68.2353], Avg: [-733.867 -733.867 -733.867] (1.000)
Step: 56899, Reward: [-680.048 -680.048 -680.048] [709.1384], Avg: [-734.443 -734.443 -734.443] (1.000)
Step: 56949, Reward: [-706.896 -706.896 -706.896] [675.2345], Avg: [-735.011 -735.011 -735.011] (1.000)
Step: 56999, Reward: [-509.21 -509.21 -509.21] [221.5326], Avg: [-735.008 -735.008 -735.008] (1.000)
Step: 57049, Reward: [-721.723 -721.723 -721.723] [571.5625], Avg: [-735.497 -735.497 -735.497] (1.000)
Step: 57099, Reward: [-1101.407 -1101.407 -1101.407] [749.0549], Avg: [-736.473 -736.473 -736.473] (1.000)
Step: 57149, Reward: [-751.911 -751.911 -751.911] [709.3585], Avg: [-737.107 -737.107 -737.107] (1.000)
Step: 57199, Reward: [-859.012 -859.012 -859.012] [616.5825], Avg: [-737.753 -737.753 -737.753] (1.000)
Step: 57249, Reward: [-924.209 -924.209 -924.209] [753.6601], Avg: [-738.574 -738.574 -738.574] (1.000)
Step: 57299, Reward: [-463.442 -463.442 -463.442] [93.8850], Avg: [-738.416 -738.416 -738.416] (1.000)
Step: 57349, Reward: [-830.664 -830.664 -830.664] [663.1800], Avg: [-739.074 -739.074 -739.074] (1.000)
Step: 57399, Reward: [-823.604 -823.604 -823.604] [582.9795], Avg: [-739.656 -739.656 -739.656] (1.000)
Step: 57449, Reward: [-489.458 -489.458 -489.458] [155.1848], Avg: [-739.573 -739.573 -739.573] (1.000)
Step: 57499, Reward: [-776.367 -776.367 -776.367] [696.2157], Avg: [-740.211 -740.211 -740.211] (1.000)
Step: 57549, Reward: [-561.681 -561.681 -561.681] [191.0694], Avg: [-740.221 -740.221 -740.221] (1.000)
Step: 57599, Reward: [-492.138 -492.138 -492.138] [166.7340], Avg: [-740.151 -740.151 -740.151] (1.000)
Step: 57649, Reward: [-635.134 -635.134 -635.134] [287.7927], Avg: [-740.309 -740.309 -740.309] (1.000)
Step: 57699, Reward: [-537.83 -537.83 -537.83] [207.7739], Avg: [-740.314 -740.314 -740.314] (1.000)
Step: 57749, Reward: [-438.509 -438.509 -438.509] [56.2174], Avg: [-740.101 -740.101 -740.101] (1.000)
Step: 57799, Reward: [-435.665 -435.665 -435.665] [43.5941], Avg: [-739.876 -739.876 -739.876] (1.000)
Step: 57849, Reward: [-558.298 -558.298 -558.298] [267.7674], Avg: [-739.95 -739.95 -739.95] (1.000)
Step: 57899, Reward: [-450.281 -450.281 -450.281] [109.3386], Avg: [-739.794 -739.794 -739.794] (1.000)
Step: 57949, Reward: [-399.42 -399.42 -399.42] [49.8770], Avg: [-739.544 -739.544 -739.544] (1.000)
Step: 57999, Reward: [-802.172 -802.172 -802.172] [624.8103], Avg: [-740.136 -740.136 -740.136] (1.000)
Step: 58049, Reward: [-889.118 -889.118 -889.118] [816.2545], Avg: [-740.968 -740.968 -740.968] (1.000)
Step: 58099, Reward: [-508.752 -508.752 -508.752] [146.8830], Avg: [-740.894 -740.894 -740.894] (1.000)
Step: 58149, Reward: [-697.559 -697.559 -697.559] [521.1250], Avg: [-741.305 -741.305 -741.305] (1.000)
Step: 58199, Reward: [-486.794 -486.794 -486.794] [146.0381], Avg: [-741.212 -741.212 -741.212] (1.000)
Step: 58249, Reward: [-1090.869 -1090.869 -1090.869] [660.4187], Avg: [-742.079 -742.079 -742.079] (1.000)
Step: 58299, Reward: [-893.498 -893.498 -893.498] [596.0391], Avg: [-742.72 -742.72 -742.72] (1.000)
Step: 58349, Reward: [-1194.508 -1194.508 -1194.508] [718.0948], Avg: [-743.722 -743.722 -743.722] (1.000)
Step: 58399, Reward: [-1834.53 -1834.53 -1834.53] [577.8134], Avg: [-745.151 -745.151 -745.151] (1.000)
Step: 58449, Reward: [-532.212 -532.212 -532.212] [120.8364], Avg: [-745.072 -745.072 -745.072] (1.000)
Step: 58499, Reward: [-874.216 -874.216 -874.216] [473.9968], Avg: [-745.588 -745.588 -745.588] (1.000)
Step: 58549, Reward: [-758.382 -758.382 -758.382] [544.4034], Avg: [-746.064 -746.064 -746.064] (1.000)
Step: 58599, Reward: [-616.727 -616.727 -616.727] [293.7647], Avg: [-746.204 -746.204 -746.204] (1.000)
Step: 58649, Reward: [-728.108 -728.108 -728.108] [598.6650], Avg: [-746.699 -746.699 -746.699] (1.000)
Step: 58699, Reward: [-802.629 -802.629 -802.629] [635.8998], Avg: [-747.288 -747.288 -747.288] (1.000)
Step: 58749, Reward: [-494.653 -494.653 -494.653] [136.6356], Avg: [-747.189 -747.189 -747.189] (1.000)
Step: 58799, Reward: [-785.011 -785.011 -785.011] [728.0986], Avg: [-747.841 -747.841 -747.841] (1.000)
Step: 58849, Reward: [-497.664 -497.664 -497.664] [175.3097], Avg: [-747.777 -747.777 -747.777] (1.000)
Step: 58899, Reward: [-687.012 -687.012 -687.012] [452.5960], Avg: [-748.11 -748.11 -748.11] (1.000)
Step: 58949, Reward: [-454.535 -454.535 -454.535] [110.3147], Avg: [-747.954 -747.954 -747.954] (1.000)
Step: 58999, Reward: [-1163.795 -1163.795 -1163.795] [770.6357], Avg: [-748.96 -748.96 -748.96] (1.000)
Step: 59049, Reward: [-1090.221 -1090.221 -1090.221] [665.9204], Avg: [-749.813 -749.813 -749.813] (1.000)
Step: 59099, Reward: [-418.825 -418.825 -418.825] [64.7708], Avg: [-749.587 -749.587 -749.587] (1.000)
Step: 59149, Reward: [-731.814 -731.814 -731.814] [737.3421], Avg: [-750.196 -750.196 -750.196] (1.000)
Step: 59199, Reward: [-406.808 -406.808 -406.808] [77.2937], Avg: [-749.971 -749.971 -749.971] (1.000)
Step: 59249, Reward: [-452.545 -452.545 -452.545] [41.7450], Avg: [-749.755 -749.755 -749.755] (1.000)
Step: 59299, Reward: [-445.36 -445.36 -445.36] [57.2257], Avg: [-749.547 -749.547 -749.547] (1.000)
Step: 59349, Reward: [-502.712 -502.712 -502.712] [64.6841], Avg: [-749.393 -749.393 -749.393] (1.000)
Step: 59399, Reward: [-886.252 -886.252 -886.252] [605.2753], Avg: [-750.018 -750.018 -750.018] (1.000)
Step: 59449, Reward: [-709.864 -709.864 -709.864] [385.3333], Avg: [-750.308 -750.308 -750.308] (1.000)
Step: 59499, Reward: [-556.794 -556.794 -556.794] [153.9320], Avg: [-750.275 -750.275 -750.275] (1.000)
Step: 59549, Reward: [-599.337 -599.337 -599.337] [186.7764], Avg: [-750.305 -750.305 -750.305] (1.000)
Step: 59599, Reward: [-433.788 -433.788 -433.788] [70.3413], Avg: [-750.099 -750.099 -750.099] (1.000)
Step: 59649, Reward: [-553.765 -553.765 -553.765] [131.6082], Avg: [-750.044 -750.044 -750.044] (1.000)
Step: 59699, Reward: [-833.868 -833.868 -833.868] [517.8102], Avg: [-750.548 -750.548 -750.548] (1.000)
Step: 59749, Reward: [-1104.308 -1104.308 -1104.308] [687.6707], Avg: [-751.42 -751.42 -751.42] (1.000)
Step: 59799, Reward: [-519.78 -519.78 -519.78] [77.6865], Avg: [-751.291 -751.291 -751.291] (1.000)
Step: 59849, Reward: [-589.119 -589.119 -589.119] [55.5081], Avg: [-751.202 -751.202 -751.202] (1.000)
Step: 59899, Reward: [-1711.056 -1711.056 -1711.056] [571.5489], Avg: [-752.48 -752.48 -752.48] (1.000)
Step: 59949, Reward: [-1441.343 -1441.343 -1441.343] [781.0303], Avg: [-753.706 -753.706 -753.706] (1.000)
Step: 59999, Reward: [-1738.776 -1738.776 -1738.776] [516.9459], Avg: [-754.958 -754.958 -754.958] (1.000)
Step: 60049, Reward: [-2019.636 -2019.636 -2019.636] [200.6753], Avg: [-756.178 -756.178 -756.178] (1.000)
Step: 60099, Reward: [-2111.062 -2111.062 -2111.062] [142.0117], Avg: [-757.423 -757.423 -757.423] (1.000)
Step: 60149, Reward: [-2004.22 -2004.22 -2004.22] [290.4591], Avg: [-758.701 -758.701 -758.701] (1.000)
Step: 60199, Reward: [-1948.802 -1948.802 -1948.802] [134.2905], Avg: [-759.801 -759.801 -759.801] (1.000)
Step: 60249, Reward: [-1929.149 -1929.149 -1929.149] [232.9007], Avg: [-760.965 -760.965 -760.965] (1.000)
Step: 60299, Reward: [-2035.064 -2035.064 -2035.064] [190.4558], Avg: [-762.179 -762.179 -762.179] (1.000)
Step: 60349, Reward: [-1900.724 -1900.724 -1900.724] [185.1979], Avg: [-763.276 -763.276 -763.276] (1.000)
Step: 60399, Reward: [-2045.242 -2045.242 -2045.242] [155.6336], Avg: [-764.466 -764.466 -764.466] (1.000)
Step: 60449, Reward: [-1864.552 -1864.552 -1864.552] [662.5671], Avg: [-765.924 -765.924 -765.924] (1.000)
Step: 60499, Reward: [-926.396 -926.396 -926.396] [675.5301], Avg: [-766.615 -766.615 -766.615] (1.000)
Step: 60549, Reward: [-949.102 -949.102 -949.102] [582.7864], Avg: [-767.247 -767.247 -767.247] (1.000)
Step: 60599, Reward: [-500.127 -500.127 -500.127] [90.4219], Avg: [-767.101 -767.101 -767.101] (1.000)
Step: 60649, Reward: [-460.911 -460.911 -460.911] [92.6906], Avg: [-766.925 -766.925 -766.925] (1.000)
Step: 60699, Reward: [-530.103 -530.103 -530.103] [208.8797], Avg: [-766.902 -766.902 -766.902] (1.000)
Step: 60749, Reward: [-511.14 -511.14 -511.14] [55.4336], Avg: [-766.737 -766.737 -766.737] (1.000)
Step: 60799, Reward: [-792.641 -792.641 -792.641] [590.9052], Avg: [-767.244 -767.244 -767.244] (1.000)
Step: 60849, Reward: [-393.569 -393.569 -393.569] [44.3437], Avg: [-766.974 -766.974 -766.974] (1.000)
Step: 60899, Reward: [-487.638 -487.638 -487.638] [272.8647], Avg: [-766.968 -766.968 -766.968] (1.000)
Step: 60949, Reward: [-513.847 -513.847 -513.847] [178.5331], Avg: [-766.907 -766.907 -766.907] (1.000)
Step: 60999, Reward: [-397.09 -397.09 -397.09] [62.1603], Avg: [-766.655 -766.655 -766.655] (1.000)
Step: 61049, Reward: [-387.079 -387.079 -387.079] [99.7013], Avg: [-766.426 -766.426 -766.426] (1.000)
Step: 61099, Reward: [-407.883 -407.883 -407.883] [89.1585], Avg: [-766.205 -766.205 -766.205] (1.000)
Step: 61149, Reward: [-384.289 -384.289 -384.289] [37.5569], Avg: [-765.924 -765.924 -765.924] (1.000)
Step: 61199, Reward: [-457.323 -457.323 -457.323] [59.6420], Avg: [-765.72 -765.72 -765.72] (1.000)
Step: 61249, Reward: [-419.005 -419.005 -419.005] [63.5260], Avg: [-765.489 -765.489 -765.489] (1.000)
Step: 61299, Reward: [-412.476 -412.476 -412.476] [95.5017], Avg: [-765.279 -765.279 -765.279] (1.000)
Step: 61349, Reward: [-683.712 -683.712 -683.712] [269.2226], Avg: [-765.432 -765.432 -765.432] (1.000)
Step: 61399, Reward: [-442.871 -442.871 -442.871] [136.0955], Avg: [-765.28 -765.28 -765.28] (1.000)
Step: 61449, Reward: [-405.519 -405.519 -405.519] [88.9996], Avg: [-765.06 -765.06 -765.06] (1.000)
Step: 61499, Reward: [-735.472 -735.472 -735.472] [663.8105], Avg: [-765.576 -765.576 -765.576] (1.000)
Step: 61549, Reward: [-508.016 -508.016 -508.016] [85.4547], Avg: [-765.436 -765.436 -765.436] (1.000)
Step: 61599, Reward: [-879.908 -879.908 -879.908] [728.6108], Avg: [-766.12 -766.12 -766.12] (1.000)
Step: 61649, Reward: [-501.937 -501.937 -501.937] [126.2960], Avg: [-766.008 -766.008 -766.008] (1.000)
Step: 61699, Reward: [-1458.206 -1458.206 -1458.206] [908.7231], Avg: [-767.306 -767.306 -767.306] (1.000)
Step: 61749, Reward: [-526.92 -526.92 -526.92] [134.4090], Avg: [-767.22 -767.22 -767.22] (1.000)
Step: 61799, Reward: [-1046.211 -1046.211 -1046.211] [813.0004], Avg: [-768.103 -768.103 -768.103] (1.000)
Step: 61849, Reward: [-490.043 -490.043 -490.043] [133.3811], Avg: [-767.986 -767.986 -767.986] (1.000)
Step: 61899, Reward: [-512.008 -512.008 -512.008] [110.2739], Avg: [-767.869 -767.869 -767.869] (1.000)
Step: 61949, Reward: [-540.78 -540.78 -540.78] [56.4796], Avg: [-767.731 -767.731 -767.731] (1.000)
Step: 61999, Reward: [-496.992 -496.992 -496.992] [77.0184], Avg: [-767.575 -767.575 -767.575] (1.000)
Step: 62049, Reward: [-783.144 -783.144 -783.144] [588.6375], Avg: [-768.062 -768.062 -768.062] (1.000)
Step: 62099, Reward: [-607.168 -607.168 -607.168] [302.8159], Avg: [-768.176 -768.176 -768.176] (1.000)
Step: 62149, Reward: [-784.49 -784.49 -784.49] [699.9216], Avg: [-768.752 -768.752 -768.752] (1.000)
Step: 62199, Reward: [-428.985 -428.985 -428.985] [115.3069], Avg: [-768.572 -768.572 -768.572] (1.000)
Step: 62249, Reward: [-781.954 -781.954 -781.954] [547.6296], Avg: [-769.022 -769.022 -769.022] (1.000)
Step: 62299, Reward: [-1192.206 -1192.206 -1192.206] [713.3211], Avg: [-769.934 -769.934 -769.934] (1.000)
Step: 62349, Reward: [-1143.236 -1143.236 -1143.236] [831.8738], Avg: [-770.901 -770.901 -770.901] (1.000)
Step: 62399, Reward: [-1190.706 -1190.706 -1190.706] [695.1552], Avg: [-771.794 -771.794 -771.794] (1.000)
Step: 62449, Reward: [-490.051 -490.051 -490.051] [72.4914], Avg: [-771.627 -771.627 -771.627] (1.000)
Step: 62499, Reward: [-1075.806 -1075.806 -1075.806] [803.9059], Avg: [-772.513 -772.513 -772.513] (1.000)
Step: 62549, Reward: [-534.039 -534.039 -534.039] [96.0838], Avg: [-772.399 -772.399 -772.399] (1.000)
Step: 62599, Reward: [-1358.223 -1358.223 -1358.223] [849.5947], Avg: [-773.546 -773.546 -773.546] (1.000)
Step: 62649, Reward: [-583.205 -583.205 -583.205] [199.0960], Avg: [-773.553 -773.553 -773.553] (1.000)
Step: 62699, Reward: [-442.626 -442.626 -442.626] [110.1092], Avg: [-773.377 -773.377 -773.377] (1.000)
Step: 62749, Reward: [-1845.21 -1845.21 -1845.21] [479.3347], Avg: [-774.613 -774.613 -774.613] (1.000)
Step: 62799, Reward: [-911.178 -911.178 -911.178] [708.4021], Avg: [-775.285 -775.285 -775.285] (1.000)
Step: 62849, Reward: [-1598.802 -1598.802 -1598.802] [951.0540], Avg: [-776.697 -776.697 -776.697] (1.000)
Step: 62899, Reward: [-475.554 -475.554 -475.554] [122.3280], Avg: [-776.555 -776.555 -776.555] (1.000)
Step: 62949, Reward: [-1795.529 -1795.529 -1795.529] [179.7980], Avg: [-777.507 -777.507 -777.507] (1.000)
Step: 62999, Reward: [-1990.485 -1990.485 -1990.485] [132.8936], Avg: [-778.575 -778.575 -778.575] (1.000)
Step: 63049, Reward: [-1190.677 -1190.677 -1190.677] [583.1610], Avg: [-779.365 -779.365 -779.365] (1.000)
Step: 63099, Reward: [-571.316 -571.316 -571.316] [133.3027], Avg: [-779.305 -779.305 -779.305] (1.000)
Step: 63149, Reward: [-1118.774 -1118.774 -1118.774] [569.0175], Avg: [-780.025 -780.025 -780.025] (1.000)
Step: 63199, Reward: [-757.1 -757.1 -757.1] [448.1756], Avg: [-780.361 -780.361 -780.361] (1.000)
Step: 63249, Reward: [-662.754 -662.754 -662.754] [134.6667], Avg: [-780.375 -780.375 -780.375] (1.000)
Step: 63299, Reward: [-840.505 -840.505 -840.505] [667.3173], Avg: [-780.949 -780.949 -780.949] (1.000)
Step: 63349, Reward: [-1179.75 -1179.75 -1179.75] [716.4716], Avg: [-781.829 -781.829 -781.829] (1.000)
Step: 63399, Reward: [-2101.051 -2101.051 -2101.051] [379.4204], Avg: [-783.169 -783.169 -783.169] (1.000)
Step: 63449, Reward: [-2022.851 -2022.851 -2022.851] [180.1240], Avg: [-784.288 -784.288 -784.288] (1.000)
Step: 63499, Reward: [-1717.563 -1717.563 -1717.563] [265.0598], Avg: [-785.232 -785.232 -785.232] (1.000)
Step: 63549, Reward: [-2069.577 -2069.577 -2069.577] [146.1334], Avg: [-786.357 -786.357 -786.357] (1.000)
Step: 63599, Reward: [-2101.831 -2101.831 -2101.831] [371.2720], Avg: [-787.683 -787.683 -787.683] (1.000)
Step: 63649, Reward: [-2088.193 -2088.193 -2088.193] [165.2053], Avg: [-788.834 -788.834 -788.834] (1.000)
Step: 63699, Reward: [-1985.133 -1985.133 -1985.133] [189.3632], Avg: [-789.922 -789.922 -789.922] (1.000)
Step: 63749, Reward: [-1974.66 -1974.66 -1974.66] [227.1448], Avg: [-791.029 -791.029 -791.029] (1.000)
Step: 63799, Reward: [-1276.879 -1276.879 -1276.879] [599.6631], Avg: [-791.88 -791.88 -791.88] (1.000)
Step: 63849, Reward: [-535.598 -535.598 -535.598] [150.3309], Avg: [-791.797 -791.797 -791.797] (1.000)
Step: 63899, Reward: [-1467.098 -1467.098 -1467.098] [275.0553], Avg: [-792.541 -792.541 -792.541] (1.000)
Step: 63949, Reward: [-1207.731 -1207.731 -1207.731] [473.8931], Avg: [-793.236 -793.236 -793.236] (1.000)
Step: 63999, Reward: [-886.268 -886.268 -886.268] [582.8351], Avg: [-793.764 -793.764 -793.764] (1.000)
Step: 64049, Reward: [-975.849 -975.849 -975.849] [601.5683], Avg: [-794.376 -794.376 -794.376] (1.000)
Step: 64099, Reward: [-805.747 -805.747 -805.747] [601.3056], Avg: [-794.854 -794.854 -794.854] (1.000)
Step: 64149, Reward: [-800.114 -800.114 -800.114] [580.0383], Avg: [-795.31 -795.31 -795.31] (1.000)
Step: 64199, Reward: [-687.871 -687.871 -687.871] [365.9098], Avg: [-795.511 -795.511 -795.511] (1.000)
Step: 64249, Reward: [-478.26 -478.26 -478.26] [48.4345], Avg: [-795.302 -795.302 -795.302] (1.000)
Step: 64299, Reward: [-801.111 -801.111 -801.111] [692.7631], Avg: [-795.845 -795.845 -795.845] (1.000)
Step: 64349, Reward: [-435.738 -435.738 -435.738] [111.0093], Avg: [-795.652 -795.652 -795.652] (1.000)
Step: 64399, Reward: [-665.574 -665.574 -665.574] [365.1895], Avg: [-795.834 -795.834 -795.834] (1.000)
Step: 64449, Reward: [-506.001 -506.001 -506.001] [200.9900], Avg: [-795.765 -795.765 -795.765] (1.000)
Step: 64499, Reward: [-626.308 -626.308 -626.308] [320.4081], Avg: [-795.882 -795.882 -795.882] (1.000)
Step: 64549, Reward: [-802.299 -802.299 -802.299] [687.2927], Avg: [-796.42 -796.42 -796.42] (1.000)
Step: 64599, Reward: [-514.695 -514.695 -514.695] [24.3807], Avg: [-796.22 -796.22 -796.22] (1.000)
Step: 64649, Reward: [-731.789 -731.789 -731.789] [379.6651], Avg: [-796.464 -796.464 -796.464] (1.000)
Step: 64699, Reward: [-744.459 -744.459 -744.459] [225.7074], Avg: [-796.598 -796.598 -796.598] (1.000)
Step: 64749, Reward: [-602.836 -602.836 -602.836] [341.8531], Avg: [-796.713 -796.713 -796.713] (1.000)
Step: 64799, Reward: [-452.286 -452.286 -452.286] [139.5920], Avg: [-796.555 -796.555 -796.555] (1.000)
Step: 64849, Reward: [-451.76 -451.76 -451.76] [85.0791], Avg: [-796.354 -796.354 -796.354] (1.000)
Step: 64899, Reward: [-556.599 -556.599 -556.599] [242.2992], Avg: [-796.356 -796.356 -796.356] (1.000)
Step: 64949, Reward: [-572.218 -572.218 -572.218] [116.0915], Avg: [-796.273 -796.273 -796.273] (1.000)
Step: 64999, Reward: [-432.487 -432.487 -432.487] [30.6033], Avg: [-796.017 -796.017 -796.017] (1.000)
Step: 65049, Reward: [-445.086 -445.086 -445.086] [65.9563], Avg: [-795.798 -795.798 -795.798] (1.000)
Step: 65099, Reward: [-486.406 -486.406 -486.406] [93.5236], Avg: [-795.632 -795.632 -795.632] (1.000)
Step: 65149, Reward: [-483.763 -483.763 -483.763] [152.1291], Avg: [-795.51 -795.51 -795.51] (1.000)
Step: 65199, Reward: [-607.288 -607.288 -607.288] [201.9904], Avg: [-795.52 -795.52 -795.52] (1.000)
Step: 65249, Reward: [-452.783 -452.783 -452.783] [70.3691], Avg: [-795.311 -795.311 -795.311] (1.000)
Step: 65299, Reward: [-408.316 -408.316 -408.316] [86.3874], Avg: [-795.081 -795.081 -795.081] (1.000)
Step: 65349, Reward: [-406.24 -406.24 -406.24] [30.9139], Avg: [-794.807 -794.807 -794.807] (1.000)
Step: 65399, Reward: [-446.559 -446.559 -446.559] [49.0159], Avg: [-794.579 -794.579 -794.579] (1.000)
Step: 65449, Reward: [-394.457 -394.457 -394.457] [47.1848], Avg: [-794.309 -794.309 -794.309] (1.000)
Step: 65499, Reward: [-413.729 -413.729 -413.729] [73.6864], Avg: [-794.075 -794.075 -794.075] (1.000)
Step: 65549, Reward: [-503.792 -503.792 -503.792] [109.4630], Avg: [-793.937 -793.937 -793.937] (1.000)
Step: 65599, Reward: [-573.39 -573.39 -573.39] [294.9550], Avg: [-793.993 -793.993 -793.993] (1.000)
Step: 65649, Reward: [-479.746 -479.746 -479.746] [213.1044], Avg: [-793.916 -793.916 -793.916] (1.000)
Step: 65699, Reward: [-530.506 -530.506 -530.506] [165.3783], Avg: [-793.842 -793.842 -793.842] (1.000)
Step: 65749, Reward: [-486. -486. -486.] [138.7478], Avg: [-793.713 -793.713 -793.713] (1.000)
Step: 65799, Reward: [-412.377 -412.377 -412.377] [79.2699], Avg: [-793.484 -793.484 -793.484] (1.000)
Step: 65849, Reward: [-380.799 -380.799 -380.799] [55.2233], Avg: [-793.212 -793.212 -793.212] (1.000)
Step: 65899, Reward: [-521.897 -521.897 -521.897] [190.8735], Avg: [-793.151 -793.151 -793.151] (1.000)
Step: 65949, Reward: [-706.692 -706.692 -706.692] [646.2028], Avg: [-793.576 -793.576 -793.576] (1.000)
Step: 65999, Reward: [-570.848 -570.848 -570.848] [150.2770], Avg: [-793.521 -793.521 -793.521] (1.000)
Step: 66049, Reward: [-836.937 -836.937 -836.937] [686.6881], Avg: [-794.073 -794.073 -794.073] (1.000)
Step: 66099, Reward: [-532.164 -532.164 -532.164] [83.5302], Avg: [-793.939 -793.939 -793.939] (1.000)
Step: 66149, Reward: [-472.594 -472.594 -472.594] [173.3625], Avg: [-793.827 -793.827 -793.827] (1.000)
Step: 66199, Reward: [-490.68 -490.68 -490.68] [96.3465], Avg: [-793.67 -793.67 -793.67] (1.000)
Step: 66249, Reward: [-653.122 -653.122 -653.122] [192.8548], Avg: [-793.71 -793.71 -793.71] (1.000)
Step: 66299, Reward: [-726.51 -726.51 -726.51] [487.9285], Avg: [-794.027 -794.027 -794.027] (1.000)
Step: 66349, Reward: [-978.553 -978.553 -978.553] [515.1284], Avg: [-794.554 -794.554 -794.554] (1.000)
Step: 66399, Reward: [-1030.774 -1030.774 -1030.774] [752.8524], Avg: [-795.299 -795.299 -795.299] (1.000)
Step: 66449, Reward: [-804.027 -804.027 -804.027] [615.1432], Avg: [-795.769 -795.769 -795.769] (1.000)
Step: 66499, Reward: [-986.541 -986.541 -986.541] [715.8348], Avg: [-796.45 -796.45 -796.45] (1.000)
Step: 66549, Reward: [-533.693 -533.693 -533.693] [68.2366], Avg: [-796.304 -796.304 -796.304] (1.000)
Step: 66599, Reward: [-805.707 -805.707 -805.707] [651.2021], Avg: [-796.8 -796.8 -796.8] (1.000)
Step: 66649, Reward: [-636.489 -636.489 -636.489] [456.5857], Avg: [-797.022 -797.022 -797.022] (1.000)
Step: 66699, Reward: [-465.827 -465.827 -465.827] [70.6704], Avg: [-796.827 -796.827 -796.827] (1.000)
Step: 66749, Reward: [-613.075 -613.075 -613.075] [302.8975], Avg: [-796.916 -796.916 -796.916] (1.000)
Step: 66799, Reward: [-451.341 -451.341 -451.341] [71.2628], Avg: [-796.711 -796.711 -796.711] (1.000)
Step: 66849, Reward: [-417.845 -417.845 -417.845] [71.0261], Avg: [-796.481 -796.481 -796.481] (1.000)
Step: 66899, Reward: [-1204.307 -1204.307 -1204.307] [972.1331], Avg: [-797.512 -797.512 -797.512] (1.000)
Step: 66949, Reward: [-539.956 -539.956 -539.956] [121.5913], Avg: [-797.411 -797.411 -797.411] (1.000)
Step: 66999, Reward: [-463.286 -463.286 -463.286] [53.9135], Avg: [-797.202 -797.202 -797.202] (1.000)
Step: 67049, Reward: [-536.57 -536.57 -536.57] [86.2605], Avg: [-797.071 -797.071 -797.071] (1.000)
Step: 67099, Reward: [-344.106 -344.106 -344.106] [44.3545], Avg: [-796.767 -796.767 -796.767] (1.000)
Step: 67149, Reward: [-441.042 -441.042 -441.042] [31.9230], Avg: [-796.526 -796.526 -796.526] (1.000)
Step: 67199, Reward: [-389.516 -389.516 -389.516] [65.0800], Avg: [-796.271 -796.271 -796.271] (1.000)
Step: 67249, Reward: [-1039.59 -1039.59 -1039.59] [756.6251], Avg: [-797.015 -797.015 -797.015] (1.000)
Step: 67299, Reward: [-492.103 -492.103 -492.103] [95.1557], Avg: [-796.859 -796.859 -796.859] (1.000)
Step: 67349, Reward: [-481.796 -481.796 -481.796] [48.3514], Avg: [-796.661 -796.661 -796.661] (1.000)
Step: 67399, Reward: [-708.267 -708.267 -708.267] [115.9109], Avg: [-796.682 -796.682 -796.682] (1.000)
Step: 67449, Reward: [-495.365 -495.365 -495.365] [105.4506], Avg: [-796.536 -796.536 -796.536] (1.000)
Step: 67499, Reward: [-463.247 -463.247 -463.247] [128.8318], Avg: [-796.385 -796.385 -796.385] (1.000)
Step: 67549, Reward: [-619.904 -619.904 -619.904] [135.1752], Avg: [-796.354 -796.354 -796.354] (1.000)
Step: 67599, Reward: [-1339.177 -1339.177 -1339.177] [599.3668], Avg: [-797.199 -797.199 -797.199] (1.000)
Step: 67649, Reward: [-558.021 -558.021 -558.021] [113.5664], Avg: [-797.106 -797.106 -797.106] (1.000)
Step: 67699, Reward: [-560.647 -560.647 -560.647] [102.1929], Avg: [-797.007 -797.007 -797.007] (1.000)
Step: 67749, Reward: [-619.567 -619.567 -619.567] [103.0321], Avg: [-796.952 -796.952 -796.952] (1.000)
Step: 67799, Reward: [-585.076 -585.076 -585.076] [125.9571], Avg: [-796.889 -796.889 -796.889] (1.000)
Step: 67849, Reward: [-589.36 -589.36 -589.36] [187.7580], Avg: [-796.874 -796.874 -796.874] (1.000)
Step: 67899, Reward: [-486.612 -486.612 -486.612] [94.6875], Avg: [-796.716 -796.716 -796.716] (1.000)
Step: 67949, Reward: [-465.435 -465.435 -465.435] [88.4751], Avg: [-796.537 -796.537 -796.537] (1.000)
Step: 67999, Reward: [-601.235 -601.235 -601.235] [153.4710], Avg: [-796.506 -796.506 -796.506] (1.000)
Step: 68049, Reward: [-897.344 -897.344 -897.344] [510.7796], Avg: [-796.955 -796.955 -796.955] (1.000)
Step: 68099, Reward: [-2205.893 -2205.893 -2205.893] [91.5684], Avg: [-798.057 -798.057 -798.057] (1.000)
Step: 68149, Reward: [-556.257 -556.257 -556.257] [211.6117], Avg: [-798.035 -798.035 -798.035] (1.000)
Step: 68199, Reward: [-418.583 -418.583 -418.583] [54.0207], Avg: [-797.796 -797.796 -797.796] (1.000)
Step: 68249, Reward: [-522.064 -522.064 -522.064] [83.4856], Avg: [-797.656 -797.656 -797.656] (1.000)
Step: 68299, Reward: [-500.105 -500.105 -500.105] [100.7284], Avg: [-797.512 -797.512 -797.512] (1.000)
Step: 68349, Reward: [-520.202 -520.202 -520.202] [145.3845], Avg: [-797.415 -797.415 -797.415] (1.000)
Step: 68399, Reward: [-456.602 -456.602 -456.602] [127.3578], Avg: [-797.259 -797.259 -797.259] (1.000)
Step: 68449, Reward: [-467.739 -467.739 -467.739] [141.9238], Avg: [-797.122 -797.122 -797.122] (1.000)
Step: 68499, Reward: [-515.327 -515.327 -515.327] [79.7979], Avg: [-796.974 -796.974 -796.974] (1.000)
Step: 68549, Reward: [-732.288 -732.288 -732.288] [658.0018], Avg: [-797.407 -797.407 -797.407] (1.000)
Step: 68599, Reward: [-509.685 -509.685 -509.685] [145.1975], Avg: [-797.303 -797.303 -797.303] (1.000)
Step: 68649, Reward: [-440.749 -440.749 -440.749] [105.2242], Avg: [-797.12 -797.12 -797.12] (1.000)
Step: 68699, Reward: [-553.398 -553.398 -553.398] [153.9609], Avg: [-797.055 -797.055 -797.055] (1.000)
Step: 68749, Reward: [-524.999 -524.999 -524.999] [141.1801], Avg: [-796.96 -796.96 -796.96] (1.000)
Step: 68799, Reward: [-406.197 -406.197 -406.197] [48.6135], Avg: [-796.711 -796.711 -796.711] (1.000)
Step: 68849, Reward: [-380.953 -380.953 -380.953] [117.8548], Avg: [-796.495 -796.495 -796.495] (1.000)
Step: 68899, Reward: [-434.004 -434.004 -434.004] [76.9423], Avg: [-796.288 -796.288 -796.288] (1.000)
Step: 68949, Reward: [-818.33 -818.33 -818.33] [551.0344], Avg: [-796.703 -796.703 -796.703] (1.000)
Step: 68999, Reward: [-470.13 -470.13 -470.13] [125.3038], Avg: [-796.557 -796.557 -796.557] (1.000)
Step: 69049, Reward: [-526.205 -526.205 -526.205] [51.6317], Avg: [-796.399 -796.399 -796.399] (1.000)
Step: 69099, Reward: [-461.491 -461.491 -461.491] [130.6779], Avg: [-796.251 -796.251 -796.251] (1.000)
Step: 69149, Reward: [-499.939 -499.939 -499.939] [99.3499], Avg: [-796.109 -796.109 -796.109] (1.000)
Step: 69199, Reward: [-552.416 -552.416 -552.416] [311.5477], Avg: [-796.158 -796.158 -796.158] (1.000)
Step: 69249, Reward: [-624.768 -624.768 -624.768] [229.2964], Avg: [-796.2 -796.2 -796.2] (1.000)
Step: 69299, Reward: [-908.882 -908.882 -908.882] [482.6109], Avg: [-796.629 -796.629 -796.629] (1.000)
Step: 69349, Reward: [-432.605 -432.605 -432.605] [108.8967], Avg: [-796.445 -796.445 -796.445] (1.000)
Step: 69399, Reward: [-957.277 -957.277 -957.277] [696.8735], Avg: [-797.063 -797.063 -797.063] (1.000)
Step: 69449, Reward: [-579.405 -579.405 -579.405] [95.0220], Avg: [-796.975 -796.975 -796.975] (1.000)
Step: 69499, Reward: [-522.56 -522.56 -522.56] [134.4900], Avg: [-796.874 -796.874 -796.874] (1.000)
Step: 69549, Reward: [-506.659 -506.659 -506.659] [75.1797], Avg: [-796.72 -796.72 -796.72] (1.000)
Step: 69599, Reward: [-1258.791 -1258.791 -1258.791] [757.3586], Avg: [-797.596 -797.596 -797.596] (1.000)
Step: 69649, Reward: [-1297.441 -1297.441 -1297.441] [865.9997], Avg: [-798.576 -798.576 -798.576] (1.000)
Step: 69699, Reward: [-457.004 -457.004 -457.004] [71.9482], Avg: [-798.383 -798.383 -798.383] (1.000)
Step: 69749, Reward: [-780.547 -780.547 -780.547] [688.0060], Avg: [-798.863 -798.863 -798.863] (1.000)
Step: 69799, Reward: [-1156.507 -1156.507 -1156.507] [803.8361], Avg: [-799.695 -799.695 -799.695] (1.000)
Step: 69849, Reward: [-518.739 -518.739 -518.739] [97.4008], Avg: [-799.564 -799.564 -799.564] (1.000)
Step: 69899, Reward: [-489.485 -489.485 -489.485] [76.7394], Avg: [-799.397 -799.397 -799.397] (1.000)
Step: 69949, Reward: [-514.932 -514.932 -514.932] [122.3972], Avg: [-799.281 -799.281 -799.281] (1.000)
Step: 69999, Reward: [-740.862 -740.862 -740.862] [545.3913], Avg: [-799.629 -799.629 -799.629] (1.000)
Step: 70049, Reward: [-794.101 -794.101 -794.101] [626.0825], Avg: [-800.072 -800.072 -800.072] (1.000)
Step: 70099, Reward: [-807.865 -807.865 -807.865] [806.9101], Avg: [-800.653 -800.653 -800.653] (1.000)
Step: 70149, Reward: [-894.741 -894.741 -894.741] [516.6645], Avg: [-801.088 -801.088 -801.088] (1.000)
Step: 70199, Reward: [-490.028 -490.028 -490.028] [154.7015], Avg: [-800.977 -800.977 -800.977] (1.000)
Step: 70249, Reward: [-2091.028 -2091.028 -2091.028] [145.6414], Avg: [-801.999 -801.999 -801.999] (1.000)
Step: 70299, Reward: [-505.045 -505.045 -505.045] [199.9794], Avg: [-801.93 -801.93 -801.93] (1.000)
Step: 70349, Reward: [-985.773 -985.773 -985.773] [702.7931], Avg: [-802.56 -802.56 -802.56] (1.000)
Step: 70399, Reward: [-2049.265 -2049.265 -2049.265] [180.4089], Avg: [-803.573 -803.573 -803.573] (1.000)
Step: 70449, Reward: [-509.756 -509.756 -509.756] [139.1993], Avg: [-803.464 -803.464 -803.464] (1.000)
Step: 70499, Reward: [-475.992 -475.992 -475.992] [61.4874], Avg: [-803.275 -803.275 -803.275] (1.000)
Step: 70549, Reward: [-571.122 -571.122 -571.122] [224.2560], Avg: [-803.269 -803.269 -803.269] (1.000)
Step: 70599, Reward: [-2031.299 -2031.299 -2031.299] [208.3901], Avg: [-804.287 -804.287 -804.287] (1.000)
Step: 70649, Reward: [-1961.466 -1961.466 -1961.466] [113.4645], Avg: [-805.186 -805.186 -805.186] (1.000)
Step: 70699, Reward: [-2072.947 -2072.947 -2072.947] [136.2245], Avg: [-806.179 -806.179 -806.179] (1.000)
Step: 70749, Reward: [-1247.687 -1247.687 -1247.687] [732.9260], Avg: [-807.009 -807.009 -807.009] (1.000)
Step: 70799, Reward: [-2089.549 -2089.549 -2089.549] [156.5138], Avg: [-808.025 -808.025 -808.025] (1.000)
Step: 70849, Reward: [-2167.348 -2167.348 -2167.348] [196.5885], Avg: [-809.123 -809.123 -809.123] (1.000)
Step: 70899, Reward: [-833.165 -833.165 -833.165] [491.4776], Avg: [-809.487 -809.487 -809.487] (1.000)
Step: 70949, Reward: [-677.069 -677.069 -677.069] [549.7511], Avg: [-809.781 -809.781 -809.781] (1.000)
Step: 70999, Reward: [-789.44 -789.44 -789.44] [545.8022], Avg: [-810.151 -810.151 -810.151] (1.000)
Step: 71049, Reward: [-889.219 -889.219 -889.219] [740.2023], Avg: [-810.727 -810.727 -810.727] (1.000)
Step: 71099, Reward: [-833.781 -833.781 -833.781] [569.5054], Avg: [-811.144 -811.144 -811.144] (1.000)
Step: 71149, Reward: [-595.829 -595.829 -595.829] [176.2780], Avg: [-811.117 -811.117 -811.117] (1.000)
Step: 71199, Reward: [-627.499 -627.499 -627.499] [192.6175], Avg: [-811.123 -811.123 -811.123] (1.000)
Step: 71249, Reward: [-835.379 -835.379 -835.379] [706.6519], Avg: [-811.636 -811.636 -811.636] (1.000)
Step: 71299, Reward: [-462.251 -462.251 -462.251] [111.3324], Avg: [-811.469 -811.469 -811.469] (1.000)
Step: 71349, Reward: [-808.525 -808.525 -808.525] [746.7681], Avg: [-811.99 -811.99 -811.99] (1.000)
Step: 71399, Reward: [-459.851 -459.851 -459.851] [46.6887], Avg: [-811.776 -811.776 -811.776] (1.000)
Step: 71449, Reward: [-773.183 -773.183 -773.183] [672.1607], Avg: [-812.22 -812.22 -812.22] (1.000)
Step: 71499, Reward: [-793.274 -793.274 -793.274] [631.7505], Avg: [-812.648 -812.648 -812.648] (1.000)
Step: 71549, Reward: [-465.708 -465.708 -465.708] [43.0651], Avg: [-812.436 -812.436 -812.436] (1.000)
Step: 71599, Reward: [-491.934 -491.934 -491.934] [91.2734], Avg: [-812.276 -812.276 -812.276] (1.000)
Step: 71649, Reward: [-782.113 -782.113 -782.113] [778.1372], Avg: [-812.798 -812.798 -812.798] (1.000)
Step: 71699, Reward: [-546.339 -546.339 -546.339] [133.5185], Avg: [-812.705 -812.705 -812.705] (1.000)
Step: 71749, Reward: [-737.562 -737.562 -737.562] [558.7033], Avg: [-813.042 -813.042 -813.042] (1.000)
Step: 71799, Reward: [-900.142 -900.142 -900.142] [614.5668], Avg: [-813.531 -813.531 -813.531] (1.000)
Step: 71849, Reward: [-779.206 -779.206 -779.206] [654.3668], Avg: [-813.962 -813.962 -813.962] (1.000)
Step: 71899, Reward: [-803.511 -803.511 -803.511] [552.2453], Avg: [-814.339 -814.339 -814.339] (1.000)
Step: 71949, Reward: [-687.035 -687.035 -687.035] [560.2722], Avg: [-814.64 -814.64 -814.64] (1.000)
Step: 71999, Reward: [-1225.06 -1225.06 -1225.06] [688.6958], Avg: [-815.403 -815.403 -815.403] (1.000)
Step: 72049, Reward: [-854.484 -854.484 -854.484] [833.8172], Avg: [-816.009 -816.009 -816.009] (1.000)
Step: 72099, Reward: [-895.609 -895.609 -895.609] [695.8247], Avg: [-816.547 -816.547 -816.547] (1.000)
Step: 72149, Reward: [-472.053 -472.053 -472.053] [139.7026], Avg: [-816.405 -816.405 -816.405] (1.000)
Step: 72199, Reward: [-780.466 -780.466 -780.466] [493.6522], Avg: [-816.722 -816.722 -816.722] (1.000)
Step: 72249, Reward: [-454.662 -454.662 -454.662] [169.3832], Avg: [-816.588 -816.588 -816.588] (1.000)
Step: 72299, Reward: [-742.076 -742.076 -742.076] [708.4696], Avg: [-817.027 -817.027 -817.027] (1.000)
Step: 72349, Reward: [-713.622 -713.622 -713.622] [574.7040], Avg: [-817.352 -817.352 -817.352] (1.000)
Step: 72399, Reward: [-705.056 -705.056 -705.056] [418.5710], Avg: [-817.564 -817.564 -817.564] (1.000)
Step: 72449, Reward: [-579.245 -579.245 -579.245] [347.0783], Avg: [-817.639 -817.639 -817.639] (1.000)
Step: 72499, Reward: [-427.595 -427.595 -427.595] [61.4249], Avg: [-817.412 -817.412 -817.412] (1.000)
Step: 72549, Reward: [-1159.753 -1159.753 -1159.753] [625.1883], Avg: [-818.079 -818.079 -818.079] (1.000)
Step: 72599, Reward: [-1271.164 -1271.164 -1271.164] [884.8116], Avg: [-819.001 -819.001 -819.001] (1.000)
Step: 72649, Reward: [-489.525 -489.525 -489.525] [149.5212], Avg: [-818.877 -818.877 -818.877] (1.000)
Step: 72699, Reward: [-527.615 -527.615 -527.615] [184.0490], Avg: [-818.803 -818.803 -818.803] (1.000)
Step: 72749, Reward: [-539.916 -539.916 -539.916] [117.5972], Avg: [-818.692 -818.692 -818.692] (1.000)
Step: 72799, Reward: [-877.328 -877.328 -877.328] [590.7625], Avg: [-819.138 -819.138 -819.138] (1.000)
Step: 72849, Reward: [-722.72 -722.72 -722.72] [627.4281], Avg: [-819.503 -819.503 -819.503] (1.000)
Step: 72899, Reward: [-734.078 -734.078 -734.078] [564.3569], Avg: [-819.831 -819.831 -819.831] (1.000)
Step: 72949, Reward: [-1252.435 -1252.435 -1252.435] [897.5018], Avg: [-820.743 -820.743 -820.743] (1.000)
Step: 72999, Reward: [-474.187 -474.187 -474.187] [133.0061], Avg: [-820.596 -820.596 -820.596] (1.000)
Step: 73049, Reward: [-490.898 -490.898 -490.898] [122.3138], Avg: [-820.455 -820.455 -820.455] (1.000)
Step: 73099, Reward: [-488.108 -488.108 -488.108] [94.7933], Avg: [-820.292 -820.292 -820.292] (1.000)
Step: 73149, Reward: [-578.433 -578.433 -578.433] [213.3524], Avg: [-820.273 -820.273 -820.273] (1.000)
Step: 73199, Reward: [-709.573 -709.573 -709.573] [719.1013], Avg: [-820.688 -820.688 -820.688] (1.000)
Step: 73249, Reward: [-429.838 -429.838 -429.838] [58.2838], Avg: [-820.461 -820.461 -820.461] (1.000)
Step: 73299, Reward: [-798.259 -798.259 -798.259] [692.6931], Avg: [-820.918 -820.918 -820.918] (1.000)
Step: 73349, Reward: [-599.199 -599.199 -599.199] [314.5685], Avg: [-820.982 -820.982 -820.982] (1.000)
Step: 73399, Reward: [-831.843 -831.843 -831.843] [777.8452], Avg: [-821.519 -821.519 -821.519] (1.000)
Step: 73449, Reward: [-513.552 -513.552 -513.552] [127.9460], Avg: [-821.397 -821.397 -821.397] (1.000)
Step: 73499, Reward: [-1047.332 -1047.332 -1047.332] [783.0498], Avg: [-822.083 -822.083 -822.083] (1.000)
Step: 73549, Reward: [-441.723 -441.723 -441.723] [203.0347], Avg: [-821.962 -821.962 -821.962] (1.000)
Step: 73599, Reward: [-753.064 -753.064 -753.064] [555.1649], Avg: [-822.293 -822.293 -822.293] (1.000)
Step: 73649, Reward: [-766.654 -766.654 -766.654] [609.0889], Avg: [-822.668 -822.668 -822.668] (1.000)
Step: 73699, Reward: [-1034.162 -1034.162 -1034.162] [693.7787], Avg: [-823.283 -823.283 -823.283] (1.000)
Step: 73749, Reward: [-745.822 -745.822 -745.822] [667.3738], Avg: [-823.683 -823.683 -823.683] (1.000)
Step: 73799, Reward: [-641.356 -641.356 -641.356] [369.4332], Avg: [-823.809 -823.809 -823.809] (1.000)
Step: 73849, Reward: [-683.535 -683.535 -683.535] [609.0101], Avg: [-824.127 -824.127 -824.127] (1.000)
Step: 73899, Reward: [-454.244 -454.244 -454.244] [120.9670], Avg: [-823.958 -823.958 -823.958] (1.000)
Step: 73949, Reward: [-573.505 -573.505 -573.505] [125.2986], Avg: [-823.874 -823.874 -823.874] (1.000)
Step: 73999, Reward: [-427.122 -427.122 -427.122] [63.7426], Avg: [-823.649 -823.649 -823.649] (1.000)
Step: 74049, Reward: [-608.405 -608.405 -608.405] [415.0575], Avg: [-823.784 -823.784 -823.784] (1.000)
Step: 74099, Reward: [-417.4 -417.4 -417.4] [59.3089], Avg: [-823.549 -823.549 -823.549] (1.000)
Step: 74149, Reward: [-441.385 -441.385 -441.385] [122.6318], Avg: [-823.374 -823.374 -823.374] (1.000)
Step: 74199, Reward: [-627.306 -627.306 -627.306] [415.6451], Avg: [-823.522 -823.522 -823.522] (1.000)
Step: 74249, Reward: [-536.271 -536.271 -536.271] [281.3921], Avg: [-823.518 -823.518 -823.518] (1.000)
Step: 74299, Reward: [-751.615 -751.615 -751.615] [514.8319], Avg: [-823.816 -823.816 -823.816] (1.000)
Step: 74349, Reward: [-483.519 -483.519 -483.519] [259.2300], Avg: [-823.762 -823.762 -823.762] (1.000)
Step: 74399, Reward: [-318.536 -318.536 -318.536] [56.3833], Avg: [-823.46 -823.46 -823.46] (1.000)
Step: 74449, Reward: [-882.169 -882.169 -882.169] [609.6662], Avg: [-823.909 -823.909 -823.909] (1.000)
Step: 74499, Reward: [-818.338 -818.338 -818.338] [692.2876], Avg: [-824.37 -824.37 -824.37] (1.000)
Step: 74549, Reward: [-601.795 -601.795 -601.795] [221.1974], Avg: [-824.369 -824.369 -824.369] (1.000)
Step: 74599, Reward: [-880.401 -880.401 -880.401] [808.5470], Avg: [-824.949 -824.949 -824.949] (1.000)
Step: 74649, Reward: [-390.084 -390.084 -390.084] [68.3504], Avg: [-824.703 -824.703 -824.703] (1.000)
Step: 74699, Reward: [-762.492 -762.492 -762.492] [570.5443], Avg: [-825.043 -825.043 -825.043] (1.000)
Step: 74749, Reward: [-1006.156 -1006.156 -1006.156] [666.0067], Avg: [-825.61 -825.61 -825.61] (1.000)
Step: 74799, Reward: [-632.208 -632.208 -632.208] [447.8018], Avg: [-825.78 -825.78 -825.78] (1.000)
Step: 74849, Reward: [-802.948 -802.948 -802.948] [700.3874], Avg: [-826.233 -826.233 -826.233] (1.000)
Step: 74899, Reward: [-768.968 -768.968 -768.968] [622.4626], Avg: [-826.61 -826.61 -826.61] (1.000)
Step: 74949, Reward: [-541.191 -541.191 -541.191] [153.4424], Avg: [-826.522 -826.522 -826.522] (1.000)
Step: 74999, Reward: [-1181.77 -1181.77 -1181.77] [820.2986], Avg: [-827.306 -827.306 -827.306] (1.000)
Step: 75049, Reward: [-873.224 -873.224 -873.224] [663.2520], Avg: [-827.778 -827.778 -827.778] (1.000)
Step: 75099, Reward: [-742.817 -742.817 -742.817] [551.0355], Avg: [-828.088 -828.088 -828.088] (1.000)
Step: 75149, Reward: [-1137.948 -1137.948 -1137.948] [770.2214], Avg: [-828.807 -828.807 -828.807] (1.000)
Step: 75199, Reward: [-830.294 -830.294 -830.294] [688.8622], Avg: [-829.266 -829.266 -829.266] (1.000)
Step: 75249, Reward: [-1166.298 -1166.298 -1166.298] [799.8343], Avg: [-830.021 -830.021 -830.021] (1.000)
Step: 75299, Reward: [-1058.9 -1058.9 -1058.9] [824.8687], Avg: [-830.721 -830.721 -830.721] (1.000)
Step: 75349, Reward: [-814.827 -814.827 -814.827] [618.7767], Avg: [-831.121 -831.121 -831.121] (1.000)
Step: 75399, Reward: [-741.182 -741.182 -741.182] [680.7700], Avg: [-831.513 -831.513 -831.513] (1.000)
Step: 75449, Reward: [-770.134 -770.134 -770.134] [660.3343], Avg: [-831.91 -831.91 -831.91] (1.000)
Step: 75499, Reward: [-814.594 -814.594 -814.594] [620.3395], Avg: [-832.309 -832.309 -832.309] (1.000)
Step: 75549, Reward: [-842.022 -842.022 -842.022] [663.4453], Avg: [-832.755 -832.755 -832.755] (1.000)
Step: 75599, Reward: [-879.296 -879.296 -879.296] [602.4263], Avg: [-833.184 -833.184 -833.184] (1.000)
Step: 75649, Reward: [-841.518 -841.518 -841.518] [715.7461], Avg: [-833.662 -833.662 -833.662] (1.000)
Step: 75699, Reward: [-711.276 -711.276 -711.276] [573.4361], Avg: [-833.96 -833.96 -833.96] (1.000)
Step: 75749, Reward: [-389.979 -389.979 -389.979] [81.5501], Avg: [-833.721 -833.721 -833.721] (1.000)
Step: 75799, Reward: [-850.051 -850.051 -850.051] [711.1613], Avg: [-834.201 -834.201 -834.201] (1.000)
Step: 75849, Reward: [-777.812 -777.812 -777.812] [654.3450], Avg: [-834.595 -834.595 -834.595] (1.000)
Step: 75899, Reward: [-503.681 -503.681 -503.681] [46.3588], Avg: [-834.408 -834.408 -834.408] (1.000)
Step: 75949, Reward: [-499.499 -499.499 -499.499] [125.0477], Avg: [-834.27 -834.27 -834.27] (1.000)
Step: 75999, Reward: [-420.035 -420.035 -420.035] [82.3240], Avg: [-834.051 -834.051 -834.051] (1.000)
Step: 76049, Reward: [-575.775 -575.775 -575.775] [145.2487], Avg: [-833.977 -833.977 -833.977] (1.000)
Step: 76099, Reward: [-424.51 -424.51 -424.51] [70.3475], Avg: [-833.754 -833.754 -833.754] (1.000)
Step: 76149, Reward: [-514.66 -514.66 -514.66] [83.1756], Avg: [-833.599 -833.599 -833.599] (1.000)
Step: 76199, Reward: [-488.131 -488.131 -488.131] [111.2665], Avg: [-833.446 -833.446 -833.446] (1.000)
Step: 76249, Reward: [-466.221 -466.221 -466.221] [37.2888], Avg: [-833.229 -833.229 -833.229] (1.000)
Step: 76299, Reward: [-451.304 -451.304 -451.304] [143.3833], Avg: [-833.073 -833.073 -833.073] (1.000)
Step: 76349, Reward: [-715.394 -715.394 -715.394] [685.1788], Avg: [-833.445 -833.445 -833.445] (1.000)
Step: 76399, Reward: [-500.998 -500.998 -500.998] [49.4890], Avg: [-833.259 -833.259 -833.259] (1.000)
Step: 76449, Reward: [-420.389 -420.389 -420.389] [80.8449], Avg: [-833.042 -833.042 -833.042] (1.000)
Step: 76499, Reward: [-800.342 -800.342 -800.342] [780.4963], Avg: [-833.531 -833.531 -833.531] (1.000)
Step: 76549, Reward: [-521.482 -521.482 -521.482] [242.5934], Avg: [-833.486 -833.486 -833.486] (1.000)
Step: 76599, Reward: [-416.391 -416.391 -416.391] [71.0521], Avg: [-833.26 -833.26 -833.26] (1.000)
Step: 76649, Reward: [-480.654 -480.654 -480.654] [193.1626], Avg: [-833.156 -833.156 -833.156] (1.000)
Step: 76699, Reward: [-795.95 -795.95 -795.95] [666.8451], Avg: [-833.566 -833.566 -833.566] (1.000)
Step: 76749, Reward: [-488.423 -488.423 -488.423] [131.9575], Avg: [-833.427 -833.427 -833.427] (1.000)
Step: 76799, Reward: [-542.241 -542.241 -542.241] [129.6781], Avg: [-833.322 -833.322 -833.322] (1.000)
Step: 76849, Reward: [-588.12 -588.12 -588.12] [126.0286], Avg: [-833.245 -833.245 -833.245] (1.000)
Step: 76899, Reward: [-458.7 -458.7 -458.7] [217.7401], Avg: [-833.143 -833.143 -833.143] (1.000)
Step: 76949, Reward: [-460.984 -460.984 -460.984] [149.0309], Avg: [-832.998 -832.998 -832.998] (1.000)
Step: 76999, Reward: [-491.195 -491.195 -491.195] [224.0664], Avg: [-832.921 -832.921 -832.921] (1.000)
Step: 77049, Reward: [-719.561 -719.561 -719.561] [766.1045], Avg: [-833.345 -833.345 -833.345] (1.000)
Step: 77099, Reward: [-586.498 -586.498 -586.498] [127.6956], Avg: [-833.267 -833.267 -833.267] (1.000)
Step: 77149, Reward: [-410.024 -410.024 -410.024] [79.3202], Avg: [-833.045 -833.045 -833.045] (1.000)
Step: 77199, Reward: [-442.7 -442.7 -442.7] [118.3499], Avg: [-832.868 -832.868 -832.868] (1.000)
Step: 77249, Reward: [-387.372 -387.372 -387.372] [71.5146], Avg: [-832.626 -832.626 -832.626] (1.000)
Step: 77299, Reward: [-445.187 -445.187 -445.187] [63.9115], Avg: [-832.417 -832.417 -832.417] (1.000)
Step: 77349, Reward: [-455.257 -455.257 -455.257] [131.0515], Avg: [-832.258 -832.258 -832.258] (1.000)
Step: 77399, Reward: [-480.357 -480.357 -480.357] [107.8463], Avg: [-832.1 -832.1 -832.1] (1.000)
Step: 77449, Reward: [-497.32 -497.32 -497.32] [188.0064], Avg: [-832.006 -832.006 -832.006] (1.000)
Step: 77499, Reward: [-743.543 -743.543 -743.543] [817.9103], Avg: [-832.476 -832.476 -832.476] (1.000)
Step: 77549, Reward: [-496.283 -496.283 -496.283] [65.1210], Avg: [-832.301 -832.301 -832.301] (1.000)
Step: 77599, Reward: [-528.016 -528.016 -528.016] [203.0673], Avg: [-832.236 -832.236 -832.236] (1.000)
Step: 77649, Reward: [-426.91 -426.91 -426.91] [88.8734], Avg: [-832.032 -832.032 -832.032] (1.000)
Step: 77699, Reward: [-616.935 -616.935 -616.935] [298.8578], Avg: [-832.086 -832.086 -832.086] (1.000)
Step: 77749, Reward: [-455.545 -455.545 -455.545] [96.3349], Avg: [-831.906 -831.906 -831.906] (1.000)
Step: 77799, Reward: [-571.46 -571.46 -571.46] [167.3378], Avg: [-831.846 -831.846 -831.846] (1.000)
Step: 77849, Reward: [-430.551 -430.551 -430.551] [112.5460], Avg: [-831.661 -831.661 -831.661] (1.000)
Step: 77899, Reward: [-515.932 -515.932 -515.932] [173.5092], Avg: [-831.57 -831.57 -831.57] (1.000)
Step: 77949, Reward: [-422.736 -422.736 -422.736] [81.8720], Avg: [-831.36 -831.36 -831.36] (1.000)
Step: 77999, Reward: [-454.662 -454.662 -454.662] [58.2674], Avg: [-831.156 -831.156 -831.156] (1.000)
Step: 78049, Reward: [-475.366 -475.366 -475.366] [69.8504], Avg: [-830.973 -830.973 -830.973] (1.000)
Step: 78099, Reward: [-481.843 -481.843 -481.843] [54.2134], Avg: [-830.784 -830.784 -830.784] (1.000)
Step: 78149, Reward: [-575.418 -575.418 -575.418] [223.6452], Avg: [-830.763 -830.763 -830.763] (1.000)
Step: 78199, Reward: [-813.193 -813.193 -813.193] [675.7277], Avg: [-831.184 -831.184 -831.184] (1.000)
Step: 78249, Reward: [-574.524 -574.524 -574.524] [160.6974], Avg: [-831.123 -831.123 -831.123] (1.000)
Step: 78299, Reward: [-546.7 -546.7 -546.7] [166.5013], Avg: [-831.048 -831.048 -831.048] (1.000)
Step: 78349, Reward: [-621.195 -621.195 -621.195] [259.1127], Avg: [-831.079 -831.079 -831.079] (1.000)
Step: 78399, Reward: [-490.5 -490.5 -490.5] [176.1932], Avg: [-830.974 -830.974 -830.974] (1.000)
Step: 78449, Reward: [-775.699 -775.699 -775.699] [647.3447], Avg: [-831.352 -831.352 -831.352] (1.000)
Step: 78499, Reward: [-839.606 -839.606 -839.606] [783.2774], Avg: [-831.856 -831.856 -831.856] (1.000)
Step: 78549, Reward: [-722.81 -722.81 -722.81] [322.3146], Avg: [-831.992 -831.992 -831.992] (1.000)
Step: 78599, Reward: [-747.309 -747.309 -747.309] [638.7779], Avg: [-832.344 -832.344 -832.344] (1.000)
Step: 78649, Reward: [-525.931 -525.931 -525.931] [190.1262], Avg: [-832.27 -832.27 -832.27] (1.000)
Step: 78699, Reward: [-509.996 -509.996 -509.996] [191.9537], Avg: [-832.187 -832.187 -832.187] (1.000)
Step: 78749, Reward: [-365.364 -365.364 -365.364] [49.1795], Avg: [-831.922 -831.922 -831.922] (1.000)
Step: 78799, Reward: [-847.605 -847.605 -847.605] [711.0067], Avg: [-832.383 -832.383 -832.383] (1.000)
Step: 78849, Reward: [-702.838 -702.838 -702.838] [571.7673], Avg: [-832.664 -832.664 -832.664] (1.000)
Step: 78899, Reward: [-657.017 -657.017 -657.017] [403.4535], Avg: [-832.808 -832.808 -832.808] (1.000)
Step: 78949, Reward: [-511.625 -511.625 -511.625] [118.4758], Avg: [-832.68 -832.68 -832.68] (1.000)
Step: 78999, Reward: [-430.408 -430.408 -430.408] [146.0809], Avg: [-832.517 -832.517 -832.517] (1.000)
Step: 79049, Reward: [-799.725 -799.725 -799.725] [657.4477], Avg: [-832.913 -832.913 -832.913] (1.000)
Step: 79099, Reward: [-793.029 -793.029 -793.029] [669.2810], Avg: [-833.31 -833.31 -833.31] (1.000)
Step: 79149, Reward: [-759.383 -759.383 -759.383] [627.8592], Avg: [-833.66 -833.66 -833.66] (1.000)
Step: 79199, Reward: [-417.916 -417.916 -417.916] [60.6023], Avg: [-833.436 -833.436 -833.436] (1.000)
Step: 79249, Reward: [-384.469 -384.469 -384.469] [153.5127], Avg: [-833.25 -833.25 -833.25] (1.000)
Step: 79299, Reward: [-596.221 -596.221 -596.221] [153.3333], Avg: [-833.197 -833.197 -833.197] (1.000)
Step: 79349, Reward: [-789.967 -789.967 -789.967] [678.5852], Avg: [-833.597 -833.597 -833.597] (1.000)
Step: 79399, Reward: [-843.369 -843.369 -843.369] [804.5046], Avg: [-834.11 -834.11 -834.11] (1.000)
Step: 79449, Reward: [-1014.397 -1014.397 -1014.397] [786.1201], Avg: [-834.718 -834.718 -834.718] (1.000)
Step: 79499, Reward: [-723.806 -723.806 -723.806] [661.6698], Avg: [-835.065 -835.065 -835.065] (1.000)
Step: 79549, Reward: [-789.917 -789.917 -789.917] [662.2018], Avg: [-835.452 -835.452 -835.452] (1.000)
Step: 79599, Reward: [-845.994 -845.994 -845.994] [626.6351], Avg: [-835.853 -835.853 -835.853] (1.000)
Step: 79649, Reward: [-888.964 -888.964 -888.964] [630.7637], Avg: [-836.282 -836.282 -836.282] (1.000)
Step: 79699, Reward: [-830.742 -830.742 -830.742] [712.2426], Avg: [-836.725 -836.725 -836.725] (1.000)
Step: 79749, Reward: [-1026.23 -1026.23 -1026.23] [744.2298], Avg: [-837.311 -837.311 -837.311] (1.000)
Step: 79799, Reward: [-421.593 -421.593 -421.593] [39.2288], Avg: [-837.075 -837.075 -837.075] (1.000)
Step: 79849, Reward: [-451.233 -451.233 -451.233] [116.2837], Avg: [-836.906 -836.906 -836.906] (1.000)
Step: 79899, Reward: [-702.907 -702.907 -702.907] [617.3320], Avg: [-837.209 -837.209 -837.209] (1.000)
Step: 79949, Reward: [-1571.882 -1571.882 -1571.882] [736.1635], Avg: [-838.128 -838.128 -838.128] (1.000)
Step: 79999, Reward: [-647.046 -647.046 -647.046] [482.3951], Avg: [-838.31 -838.31 -838.31] (1.000)
Step: 80049, Reward: [-1280.88 -1280.88 -1280.88] [599.4821], Avg: [-838.961 -838.961 -838.961] (1.000)
Step: 80099, Reward: [-1390.098 -1390.098 -1390.098] [673.0972], Avg: [-839.726 -839.726 -839.726] (1.000)
Step: 80149, Reward: [-1069.041 -1069.041 -1069.041] [702.8135], Avg: [-840.307 -840.307 -840.307] (1.000)
Step: 80199, Reward: [-1090.223 -1090.223 -1090.223] [658.2046], Avg: [-840.873 -840.873 -840.873] (1.000)
Step: 80249, Reward: [-1954.696 -1954.696 -1954.696] [176.8460], Avg: [-841.677 -841.677 -841.677] (1.000)
Step: 80299, Reward: [-1995.948 -1995.948 -1995.948] [303.0971], Avg: [-842.585 -842.585 -842.585] (1.000)
Step: 80349, Reward: [-2041.027 -2041.027 -2041.027] [133.9724], Avg: [-843.414 -843.414 -843.414] (1.000)
Step: 80399, Reward: [-1502.779 -1502.779 -1502.779] [699.2848], Avg: [-844.259 -844.259 -844.259] (1.000)
Step: 80449, Reward: [-1996.372 -1996.372 -1996.372] [183.8389], Avg: [-845.089 -845.089 -845.089] (1.000)
Step: 80499, Reward: [-1630.034 -1630.034 -1630.034] [516.7664], Avg: [-845.898 -845.898 -845.898] (1.000)
Step: 80549, Reward: [-1363.365 -1363.365 -1363.365] [690.6234], Avg: [-846.648 -846.648 -846.648] (1.000)
Step: 80599, Reward: [-1626.954 -1626.954 -1626.954] [484.2250], Avg: [-847.432 -847.432 -847.432] (1.000)
Step: 80649, Reward: [-1401.811 -1401.811 -1401.811] [719.4332], Avg: [-848.222 -848.222 -848.222] (1.000)
Step: 80699, Reward: [-1004.464 -1004.464 -1004.464] [698.0699], Avg: [-848.751 -848.751 -848.751] (1.000)
Step: 80749, Reward: [-1492.696 -1492.696 -1492.696] [726.0098], Avg: [-849.599 -849.599 -849.599] (1.000)
Step: 80799, Reward: [-1127.331 -1127.331 -1127.331] [807.4238], Avg: [-850.271 -850.271 -850.271] (1.000)
Step: 80849, Reward: [-1192.754 -1192.754 -1192.754] [678.3018], Avg: [-850.902 -850.902 -850.902] (1.000)
Step: 80899, Reward: [-921.742 -921.742 -921.742] [597.4566], Avg: [-851.315 -851.315 -851.315] (1.000)
Step: 80949, Reward: [-1381.481 -1381.481 -1381.481] [684.5674], Avg: [-852.065 -852.065 -852.065] (1.000)
Step: 80999, Reward: [-1154.235 -1154.235 -1154.235] [839.8716], Avg: [-852.77 -852.77 -852.77] (1.000)
Step: 81049, Reward: [-783.474 -783.474 -783.474] [598.7363], Avg: [-853.097 -853.097 -853.097] (1.000)
Step: 81099, Reward: [-768.681 -768.681 -768.681] [672.4802], Avg: [-853.46 -853.46 -853.46] (1.000)
Step: 81149, Reward: [-440.397 -440.397 -440.397] [227.8897], Avg: [-853.346 -853.346 -853.346] (1.000)
Step: 81199, Reward: [-741.267 -741.267 -741.267] [524.1419], Avg: [-853.599 -853.599 -853.599] (1.000)
Step: 81249, Reward: [-749.689 -749.689 -749.689] [616.5515], Avg: [-853.915 -853.915 -853.915] (1.000)
Step: 81299, Reward: [-484.32 -484.32 -484.32] [50.5394], Avg: [-853.718 -853.718 -853.718] (1.000)
Step: 81349, Reward: [-776.305 -776.305 -776.305] [702.8077], Avg: [-854.103 -854.103 -854.103] (1.000)
Step: 81399, Reward: [-555.124 -555.124 -555.124] [107.2724], Avg: [-853.985 -853.985 -853.985] (1.000)
Step: 81449, Reward: [-439.634 -439.634 -439.634] [117.7805], Avg: [-853.803 -853.803 -853.803] (1.000)
Step: 81499, Reward: [-434.721 -434.721 -434.721] [184.6723], Avg: [-853.659 -853.659 -853.659] (1.000)
Step: 81549, Reward: [-479.098 -479.098 -479.098] [122.0245], Avg: [-853.504 -853.504 -853.504] (1.000)
Step: 81599, Reward: [-605.342 -605.342 -605.342] [160.2537], Avg: [-853.451 -853.451 -853.451] (1.000)
Step: 81649, Reward: [-460.787 -460.787 -460.787] [56.9496], Avg: [-853.245 -853.245 -853.245] (1.000)
Step: 81699, Reward: [-494.537 -494.537 -494.537] [95.5269], Avg: [-853.084 -853.084 -853.084] (1.000)
Step: 81749, Reward: [-419.002 -419.002 -419.002] [96.4614], Avg: [-852.877 -852.877 -852.877] (1.000)
Step: 81799, Reward: [-392.142 -392.142 -392.142] [55.5549], Avg: [-852.63 -852.63 -852.63] (1.000)
Step: 81849, Reward: [-602.41 -602.41 -602.41] [188.5336], Avg: [-852.592 -852.592 -852.592] (1.000)
Step: 81899, Reward: [-481.177 -481.177 -481.177] [64.0726], Avg: [-852.404 -852.404 -852.404] (1.000)
Step: 81949, Reward: [-442.048 -442.048 -442.048] [97.8426], Avg: [-852.214 -852.214 -852.214] (1.000)
Step: 81999, Reward: [-528.841 -528.841 -528.841] [192.5750], Avg: [-852.134 -852.134 -852.134] (1.000)
Step: 82049, Reward: [-469.587 -469.587 -469.587] [111.6366], Avg: [-851.969 -851.969 -851.969] (1.000)
Step: 82099, Reward: [-519.454 -519.454 -519.454] [255.4856], Avg: [-851.922 -851.922 -851.922] (1.000)
Step: 82149, Reward: [-614.736 -614.736 -614.736] [126.0212], Avg: [-851.854 -851.854 -851.854] (1.000)
Step: 82199, Reward: [-523.777 -523.777 -523.777] [199.3952], Avg: [-851.776 -851.776 -851.776] (1.000)
Step: 82249, Reward: [-435.328 -435.328 -435.328] [123.1848], Avg: [-851.598 -851.598 -851.598] (1.000)
Step: 82299, Reward: [-505.688 -505.688 -505.688] [172.4431], Avg: [-851.492 -851.492 -851.492] (1.000)
Step: 82349, Reward: [-321.285 -321.285 -321.285] [41.1034], Avg: [-851.195 -851.195 -851.195] (1.000)
Step: 82399, Reward: [-458.227 -458.227 -458.227] [173.3173], Avg: [-851.062 -851.062 -851.062] (1.000)
Step: 82449, Reward: [-508.065 -508.065 -508.065] [87.4401], Avg: [-850.907 -850.907 -850.907] (1.000)
Step: 82499, Reward: [-483.548 -483.548 -483.548] [60.8107], Avg: [-850.721 -850.721 -850.721] (1.000)
Step: 82549, Reward: [-500.169 -500.169 -500.169] [33.3059], Avg: [-850.529 -850.529 -850.529] (1.000)
Step: 82599, Reward: [-521.104 -521.104 -521.104] [209.9469], Avg: [-850.457 -850.457 -850.457] (1.000)
Step: 82649, Reward: [-716.682 -716.682 -716.682] [606.2954], Avg: [-850.743 -850.743 -850.743] (1.000)
Step: 82699, Reward: [-669.192 -669.192 -669.192] [447.2468], Avg: [-850.903 -850.903 -850.903] (1.000)
Step: 82749, Reward: [-375.254 -375.254 -375.254] [92.3724], Avg: [-850.672 -850.672 -850.672] (1.000)
Step: 82799, Reward: [-536.94 -536.94 -536.94] [161.9298], Avg: [-850.58 -850.58 -850.58] (1.000)
Step: 82849, Reward: [-464.952 -464.952 -464.952] [150.0431], Avg: [-850.438 -850.438 -850.438] (1.000)
Step: 82899, Reward: [-406.909 -406.909 -406.909] [90.9378], Avg: [-850.225 -850.225 -850.225] (1.000)
Step: 82949, Reward: [-968.093 -968.093 -968.093] [709.8959], Avg: [-850.724 -850.724 -850.724] (1.000)
Step: 82999, Reward: [-896.018 -896.018 -896.018] [674.5478], Avg: [-851.158 -851.158 -851.158] (1.000)
Step: 83049, Reward: [-875.04 -875.04 -875.04] [678.6611], Avg: [-851.581 -851.581 -851.581] (1.000)
Step: 83099, Reward: [-1107.994 -1107.994 -1107.994] [922.6905], Avg: [-852.29 -852.29 -852.29] (1.000)
Step: 83149, Reward: [-485.79 -485.79 -485.79] [86.9109], Avg: [-852.122 -852.122 -852.122] (1.000)
Step: 83199, Reward: [-782.235 -782.235 -782.235] [347.2969], Avg: [-852.289 -852.289 -852.289] (1.000)
Step: 83249, Reward: [-701.43 -701.43 -701.43] [469.3731], Avg: [-852.48 -852.48 -852.48] (1.000)
Step: 83299, Reward: [-493.218 -493.218 -493.218] [227.3107], Avg: [-852.401 -852.401 -852.401] (1.000)
Step: 83349, Reward: [-422.177 -422.177 -422.177] [115.9183], Avg: [-852.212 -852.212 -852.212] (1.000)
Step: 83399, Reward: [-800.158 -800.158 -800.158] [569.4756], Avg: [-852.523 -852.523 -852.523] (1.000)
Step: 83449, Reward: [-563.124 -563.124 -563.124] [120.2993], Avg: [-852.421 -852.421 -852.421] (1.000)
Step: 83499, Reward: [-386.107 -386.107 -386.107] [47.2788], Avg: [-852.17 -852.17 -852.17] (1.000)
Step: 83549, Reward: [-518.392 -518.392 -518.392] [137.4862], Avg: [-852.053 -852.053 -852.053] (1.000)
Step: 83599, Reward: [-428.996 -428.996 -428.996] [86.8107], Avg: [-851.852 -851.852 -851.852] (1.000)
Step: 83649, Reward: [-423.554 -423.554 -423.554] [87.4201], Avg: [-851.648 -851.648 -851.648] (1.000)
Step: 83699, Reward: [-813.774 -813.774 -813.774] [635.2086], Avg: [-852.005 -852.005 -852.005] (1.000)
Step: 83749, Reward: [-706.274 -706.274 -706.274] [585.3193], Avg: [-852.267 -852.267 -852.267] (1.000)
Step: 83799, Reward: [-752.52 -752.52 -752.52] [626.0769], Avg: [-852.581 -852.581 -852.581] (1.000)
Step: 83849, Reward: [-762.475 -762.475 -762.475] [570.3636], Avg: [-852.868 -852.868 -852.868] (1.000)
Step: 83899, Reward: [-502.98 -502.98 -502.98] [103.3216], Avg: [-852.721 -852.721 -852.721] (1.000)
Step: 83949, Reward: [-718.492 -718.492 -718.492] [661.5693], Avg: [-853.035 -853.035 -853.035] (1.000)
Step: 83999, Reward: [-764.038 -764.038 -764.038] [636.2573], Avg: [-853.361 -853.361 -853.361] (1.000)
Step: 84049, Reward: [-815.792 -815.792 -815.792] [732.5674], Avg: [-853.774 -853.774 -853.774] (1.000)
Step: 84099, Reward: [-725.226 -725.226 -725.226] [438.5424], Avg: [-853.958 -853.958 -853.958] (1.000)
Step: 84149, Reward: [-697.87 -697.87 -697.87] [537.3539], Avg: [-854.185 -854.185 -854.185] (1.000)
Step: 84199, Reward: [-533.053 -533.053 -533.053] [385.7772], Avg: [-854.223 -854.223 -854.223] (1.000)
Step: 84249, Reward: [-496.652 -496.652 -496.652] [112.0474], Avg: [-854.078 -854.078 -854.078] (1.000)
Step: 84299, Reward: [-824.564 -824.564 -824.564] [668.0398], Avg: [-854.456 -854.456 -854.456] (1.000)
Step: 84349, Reward: [-468.909 -468.909 -468.909] [78.7574], Avg: [-854.275 -854.275 -854.275] (1.000)
Step: 84399, Reward: [-533.528 -533.528 -533.528] [81.4408], Avg: [-854.133 -854.133 -854.133] (1.000)
Step: 84449, Reward: [-494.519 -494.519 -494.519] [212.4716], Avg: [-854.046 -854.046 -854.046] (1.000)
Step: 84499, Reward: [-590.9 -590.9 -590.9] [222.8316], Avg: [-854.022 -854.022 -854.022] (1.000)
Step: 84549, Reward: [-530.892 -530.892 -530.892] [111.6242], Avg: [-853.897 -853.897 -853.897] (1.000)
Step: 84599, Reward: [-562.35 -562.35 -562.35] [353.8133], Avg: [-853.933 -853.933 -853.933] (1.000)
Step: 84649, Reward: [-441.684 -441.684 -441.684] [64.7916], Avg: [-853.728 -853.728 -853.728] (1.000)
Step: 84699, Reward: [-559.856 -559.856 -559.856] [299.7994], Avg: [-853.732 -853.732 -853.732] (1.000)
Step: 84749, Reward: [-868.862 -868.862 -868.862] [797.1542], Avg: [-854.211 -854.211 -854.211] (1.000)
Step: 84799, Reward: [-423.681 -423.681 -423.681] [119.6657], Avg: [-854.028 -854.028 -854.028] (1.000)
Step: 84849, Reward: [-450.108 -450.108 -450.108] [167.8487], Avg: [-853.889 -853.889 -853.889] (1.000)
Step: 84899, Reward: [-427.245 -427.245 -427.245] [97.5344], Avg: [-853.695 -853.695 -853.695] (1.000)
Step: 84949, Reward: [-439.776 -439.776 -439.776] [91.9297], Avg: [-853.505 -853.505 -853.505] (1.000)
Step: 84999, Reward: [-356.707 -356.707 -356.707] [84.2347], Avg: [-853.263 -853.263 -853.263] (1.000)
Step: 85049, Reward: [-501.941 -501.941 -501.941] [211.9901], Avg: [-853.181 -853.181 -853.181] (1.000)
Step: 85099, Reward: [-514.15 -514.15 -514.15] [236.8025], Avg: [-853.121 -853.121 -853.121] (1.000)
Step: 85149, Reward: [-493.003 -493.003 -493.003] [40.9482], Avg: [-852.933 -852.933 -852.933] (1.000)
Step: 85199, Reward: [-470.726 -470.726 -470.726] [62.3673], Avg: [-852.745 -852.745 -852.745] (1.000)
Step: 85249, Reward: [-494.241 -494.241 -494.241] [90.2384], Avg: [-852.588 -852.588 -852.588] (1.000)
Step: 85299, Reward: [-523.116 -523.116 -523.116] [96.0481], Avg: [-852.451 -852.451 -852.451] (1.000)
Step: 85349, Reward: [-458.2 -458.2 -458.2] [98.0865], Avg: [-852.278 -852.278 -852.278] (1.000)
Step: 85399, Reward: [-455.419 -455.419 -455.419] [53.6359], Avg: [-852.077 -852.077 -852.077] (1.000)
Step: 85449, Reward: [-451.185 -451.185 -451.185] [206.7388], Avg: [-851.963 -851.963 -851.963] (1.000)
Step: 85499, Reward: [-476.157 -476.157 -476.157] [81.2668], Avg: [-851.791 -851.791 -851.791] (1.000)
Step: 85549, Reward: [-810.947 -810.947 -810.947] [736.7289], Avg: [-852.198 -852.198 -852.198] (1.000)
Step: 85599, Reward: [-453.986 -453.986 -453.986] [141.0714], Avg: [-852.048 -852.048 -852.048] (1.000)
Step: 85649, Reward: [-441.897 -441.897 -441.897] [78.4324], Avg: [-851.854 -851.854 -851.854] (1.000)
Step: 85699, Reward: [-522.648 -522.648 -522.648] [152.9714], Avg: [-851.751 -851.751 -851.751] (1.000)
Step: 85749, Reward: [-728.038 -728.038 -728.038] [628.5930], Avg: [-852.045 -852.045 -852.045] (1.000)
Step: 85799, Reward: [-748.512 -748.512 -748.512] [638.9787], Avg: [-852.357 -852.357 -852.357] (1.000)
Step: 85849, Reward: [-438.206 -438.206 -438.206] [161.2403], Avg: [-852.21 -852.21 -852.21] (1.000)
Step: 85899, Reward: [-734.349 -734.349 -734.349] [622.0659], Avg: [-852.504 -852.504 -852.504] (1.000)
Step: 85949, Reward: [-585.142 -585.142 -585.142] [174.3847], Avg: [-852.45 -852.45 -852.45] (1.000)
Step: 85999, Reward: [-425.904 -425.904 -425.904] [60.1238], Avg: [-852.237 -852.237 -852.237] (1.000)
Step: 86049, Reward: [-593.274 -593.274 -593.274] [236.1053], Avg: [-852.223 -852.223 -852.223] (1.000)
Step: 86099, Reward: [-727.9 -727.9 -727.9] [529.9050], Avg: [-852.459 -852.459 -852.459] (1.000)
Step: 86149, Reward: [-746.435 -746.435 -746.435] [657.7141], Avg: [-852.779 -852.779 -852.779] (1.000)
Step: 86199, Reward: [-447.281 -447.281 -447.281] [173.3097], Avg: [-852.644 -852.644 -852.644] (1.000)
Step: 86249, Reward: [-430.394 -430.394 -430.394] [89.8155], Avg: [-852.452 -852.452 -852.452] (1.000)
Step: 86299, Reward: [-523.237 -523.237 -523.237] [127.2193], Avg: [-852.335 -852.335 -852.335] (1.000)
Step: 86349, Reward: [-470.478 -470.478 -470.478] [189.7112], Avg: [-852.223 -852.223 -852.223] (1.000)
Step: 86399, Reward: [-519.719 -519.719 -519.719] [242.1193], Avg: [-852.171 -852.171 -852.171] (1.000)
Step: 86449, Reward: [-429.05 -429.05 -429.05] [55.0597], Avg: [-851.958 -851.958 -851.958] (1.000)
Step: 86499, Reward: [-419.699 -419.699 -419.699] [134.0107], Avg: [-851.786 -851.786 -851.786] (1.000)
Step: 86549, Reward: [-443.679 -443.679 -443.679] [127.3244], Avg: [-851.623 -851.623 -851.623] (1.000)
Step: 86599, Reward: [-494.105 -494.105 -494.105] [120.1215], Avg: [-851.486 -851.486 -851.486] (1.000)
Step: 86649, Reward: [-363.98 -363.98 -363.98] [69.3036], Avg: [-851.245 -851.245 -851.245] (1.000)
Step: 86699, Reward: [-459.318 -459.318 -459.318] [64.2340], Avg: [-851.056 -851.056 -851.056] (1.000)
Step: 86749, Reward: [-473.137 -473.137 -473.137] [94.4224], Avg: [-850.893 -850.893 -850.893] (1.000)
Step: 86799, Reward: [-394.845 -394.845 -394.845] [29.8516], Avg: [-850.647 -850.647 -850.647] (1.000)
Step: 86849, Reward: [-450.052 -450.052 -450.052] [52.8029], Avg: [-850.447 -850.447 -850.447] (1.000)
Step: 86899, Reward: [-507.06 -507.06 -507.06] [144.9563], Avg: [-850.333 -850.333 -850.333] (1.000)
Step: 86949, Reward: [-432.49 -432.49 -432.49] [48.7127], Avg: [-850.121 -850.121 -850.121] (1.000)
Step: 86999, Reward: [-465.327 -465.327 -465.327] [147.9801], Avg: [-849.984 -849.984 -849.984] (1.000)
Step: 87049, Reward: [-405.873 -405.873 -405.873] [92.9591], Avg: [-849.783 -849.783 -849.783] (1.000)
Step: 87099, Reward: [-467.502 -467.502 -467.502] [203.7142], Avg: [-849.68 -849.68 -849.68] (1.000)
Step: 87149, Reward: [-401.69 -401.69 -401.69] [91.9198], Avg: [-849.476 -849.476 -849.476] (1.000)
Step: 87199, Reward: [-532.038 -532.038 -532.038] [217.3481], Avg: [-849.419 -849.419 -849.419] (1.000)
Step: 87249, Reward: [-463.766 -463.766 -463.766] [117.4803], Avg: [-849.265 -849.265 -849.265] (1.000)
Step: 87299, Reward: [-498.469 -498.469 -498.469] [159.1960], Avg: [-849.155 -849.155 -849.155] (1.000)
Step: 87349, Reward: [-557.185 -557.185 -557.185] [239.3327], Avg: [-849.125 -849.125 -849.125] (1.000)
Step: 87399, Reward: [-429.092 -429.092 -429.092] [43.4337], Avg: [-848.91 -848.91 -848.91] (1.000)
Step: 87449, Reward: [-770.72 -770.72 -770.72] [686.8870], Avg: [-849.258 -849.258 -849.258] (1.000)
Step: 87499, Reward: [-497.777 -497.777 -497.777] [130.6877], Avg: [-849.131 -849.131 -849.131] (1.000)
Step: 87549, Reward: [-391.65 -391.65 -391.65] [57.3751], Avg: [-848.903 -848.903 -848.903] (1.000)
Step: 87599, Reward: [-449.114 -449.114 -449.114] [134.1277], Avg: [-848.751 -848.751 -848.751] (1.000)
Step: 87649, Reward: [-412.324 -412.324 -412.324] [57.6507], Avg: [-848.535 -848.535 -848.535] (1.000)
Step: 87699, Reward: [-515.949 -515.949 -515.949] [151.0244], Avg: [-848.432 -848.432 -848.432] (1.000)
Step: 87749, Reward: [-476.595 -476.595 -476.595] [163.9167], Avg: [-848.313 -848.313 -848.313] (1.000)
Step: 87799, Reward: [-431.491 -431.491 -431.491] [58.8379], Avg: [-848.109 -848.109 -848.109] (1.000)
Step: 87849, Reward: [-549.155 -549.155 -549.155] [157.4718], Avg: [-848.029 -848.029 -848.029] (1.000)
Step: 87899, Reward: [-417.181 -417.181 -417.181] [118.4262], Avg: [-847.851 -847.851 -847.851] (1.000)
Step: 87949, Reward: [-576.771 -576.771 -576.771] [204.3975], Avg: [-847.813 -847.813 -847.813] (1.000)
Step: 87999, Reward: [-657.609 -657.609 -657.609] [488.2128], Avg: [-847.983 -847.983 -847.983] (1.000)
Step: 88049, Reward: [-608.696 -608.696 -608.696] [217.7218], Avg: [-847.97 -847.97 -847.97] (1.000)
Step: 88099, Reward: [-449.596 -449.596 -449.596] [102.9555], Avg: [-847.803 -847.803 -847.803] (1.000)
Step: 88149, Reward: [-420.003 -420.003 -420.003] [39.1751], Avg: [-847.582 -847.582 -847.582] (1.000)
Step: 88199, Reward: [-488.084 -488.084 -488.084] [121.3325], Avg: [-847.447 -847.447 -847.447] (1.000)
Step: 88249, Reward: [-504.674 -504.674 -504.674] [358.0974], Avg: [-847.456 -847.456 -847.456] (1.000)
Step: 88299, Reward: [-424.526 -424.526 -424.526] [58.8557], Avg: [-847.25 -847.25 -847.25] (1.000)
Step: 88349, Reward: [-496.073 -496.073 -496.073] [30.5861], Avg: [-847.068 -847.068 -847.068] (1.000)
Step: 88399, Reward: [-483.47 -483.47 -483.47] [149.4702], Avg: [-846.947 -846.947 -846.947] (1.000)
Step: 88449, Reward: [-769.795 -769.795 -769.795] [621.7039], Avg: [-847.255 -847.255 -847.255] (1.000)
Step: 88499, Reward: [-443.82 -443.82 -443.82] [57.1591], Avg: [-847.059 -847.059 -847.059] (1.000)
Step: 88549, Reward: [-418.817 -418.817 -418.817] [60.6940], Avg: [-846.852 -846.852 -846.852] (1.000)
Step: 88599, Reward: [-439.36 -439.36 -439.36] [106.9325], Avg: [-846.682 -846.682 -846.682] (1.000)
Step: 88649, Reward: [-489.569 -489.569 -489.569] [134.8048], Avg: [-846.557 -846.557 -846.557] (1.000)
Step: 88699, Reward: [-475.886 -475.886 -475.886] [130.0363], Avg: [-846.421 -846.421 -846.421] (1.000)
Step: 88749, Reward: [-431.606 -431.606 -431.606] [28.7854], Avg: [-846.204 -846.204 -846.204] (1.000)
Step: 88799, Reward: [-539.063 -539.063 -539.063] [71.9473], Avg: [-846.071 -846.071 -846.071] (1.000)
Step: 88849, Reward: [-454.3 -454.3 -454.3] [64.9416], Avg: [-845.887 -845.887 -845.887] (1.000)
Step: 88899, Reward: [-458.781 -458.781 -458.781] [105.6135], Avg: [-845.729 -845.729 -845.729] (1.000)
Step: 88949, Reward: [-453.299 -453.299 -453.299] [86.6967], Avg: [-845.557 -845.557 -845.557] (1.000)
Step: 88999, Reward: [-512.131 -512.131 -512.131] [167.7692], Avg: [-845.464 -845.464 -845.464] (1.000)
Step: 89049, Reward: [-383.75 -383.75 -383.75] [40.5309], Avg: [-845.228 -845.228 -845.228] (1.000)
Step: 89099, Reward: [-406.197 -406.197 -406.197] [71.2340], Avg: [-845.021 -845.021 -845.021] (1.000)
Step: 89149, Reward: [-460.079 -460.079 -460.079] [93.8147], Avg: [-844.858 -844.858 -844.858] (1.000)
Step: 89199, Reward: [-434.578 -434.578 -434.578] [40.1152], Avg: [-844.65 -844.65 -844.65] (1.000)
Step: 89249, Reward: [-546.999 -546.999 -546.999] [193.8062], Avg: [-844.592 -844.592 -844.592] (1.000)
Step: 89299, Reward: [-477.296 -477.296 -477.296] [55.1578], Avg: [-844.418 -844.418 -844.418] (1.000)
Step: 89349, Reward: [-496.991 -496.991 -496.991] [111.8698], Avg: [-844.286 -844.286 -844.286] (1.000)
Step: 89399, Reward: [-501.685 -501.685 -501.685] [193.9072], Avg: [-844.203 -844.203 -844.203] (1.000)
Step: 89449, Reward: [-559.862 -559.862 -559.862] [251.7822], Avg: [-844.184 -844.184 -844.184] (1.000)
Step: 89499, Reward: [-682.503 -682.503 -682.503] [557.0287], Avg: [-844.405 -844.405 -844.405] (1.000)
Step: 89549, Reward: [-508.009 -508.009 -508.009] [111.6496], Avg: [-844.28 -844.28 -844.28] (1.000)
Step: 89599, Reward: [-465.243 -465.243 -465.243] [132.5841], Avg: [-844.142 -844.142 -844.142] (1.000)
Step: 89649, Reward: [-481.653 -481.653 -481.653] [96.3420], Avg: [-843.994 -843.994 -843.994] (1.000)
Step: 89699, Reward: [-507.807 -507.807 -507.807] [123.7855], Avg: [-843.875 -843.875 -843.875] (1.000)
Step: 89749, Reward: [-407.729 -407.729 -407.729] [51.3424], Avg: [-843.661 -843.661 -843.661] (1.000)
Step: 89799, Reward: [-445.459 -445.459 -445.459] [80.6999], Avg: [-843.484 -843.484 -843.484] (1.000)
Step: 89849, Reward: [-632.65 -632.65 -632.65] [494.7601], Avg: [-843.642 -843.642 -843.642] (1.000)
Step: 89899, Reward: [-586.193 -586.193 -586.193] [289.7645], Avg: [-843.66 -843.66 -843.66] (1.000)
Step: 89949, Reward: [-582.181 -582.181 -582.181] [424.5774], Avg: [-843.751 -843.751 -843.751] (1.000)
Step: 89999, Reward: [-478.235 -478.235 -478.235] [109.5095], Avg: [-843.609 -843.609 -843.609] (1.000)
Step: 90049, Reward: [-604.003 -604.003 -604.003] [130.5088], Avg: [-843.548 -843.548 -843.548] (1.000)
Step: 90099, Reward: [-429.079 -429.079 -429.079] [55.2193], Avg: [-843.349 -843.349 -843.349] (1.000)
Step: 90149, Reward: [-485.341 -485.341 -485.341] [117.6245], Avg: [-843.215 -843.215 -843.215] (1.000)
Step: 90199, Reward: [-469.549 -469.549 -469.549] [54.3530], Avg: [-843.038 -843.038 -843.038] (1.000)
Step: 90249, Reward: [-415.73 -415.73 -415.73] [92.0414], Avg: [-842.853 -842.853 -842.853] (1.000)
Step: 90299, Reward: [-469.315 -469.315 -469.315] [148.9188], Avg: [-842.728 -842.728 -842.728] (1.000)
Step: 90349, Reward: [-458.491 -458.491 -458.491] [46.4934], Avg: [-842.541 -842.541 -842.541] (1.000)
Step: 90399, Reward: [-364.929 -364.929 -364.929] [56.9575], Avg: [-842.309 -842.309 -842.309] (1.000)
Step: 90449, Reward: [-433.584 -433.584 -433.584] [105.1822], Avg: [-842.141 -842.141 -842.141] (1.000)
Step: 90499, Reward: [-474.034 -474.034 -474.034] [77.7275], Avg: [-841.98 -841.98 -841.98] (1.000)
Step: 90549, Reward: [-669.18 -669.18 -669.18] [598.4651], Avg: [-842.215 -842.215 -842.215] (1.000)
Step: 90599, Reward: [-489.42 -489.42 -489.42] [156.1952], Avg: [-842.107 -842.107 -842.107] (1.000)
Step: 90649, Reward: [-448.877 -448.877 -448.877] [138.0443], Avg: [-841.966 -841.966 -841.966] (1.000)
Step: 90699, Reward: [-392.524 -392.524 -392.524] [66.2183], Avg: [-841.755 -841.755 -841.755] (1.000)
Step: 90749, Reward: [-450.99 -450.99 -450.99] [49.6274], Avg: [-841.567 -841.567 -841.567] (1.000)
Step: 90799, Reward: [-503.84 -503.84 -503.84] [151.5569], Avg: [-841.464 -841.464 -841.464] (1.000)
Step: 90849, Reward: [-442.094 -442.094 -442.094] [85.9918], Avg: [-841.292 -841.292 -841.292] (1.000)
Step: 90899, Reward: [-430.941 -430.941 -430.941] [124.3629], Avg: [-841.135 -841.135 -841.135] (1.000)
Step: 90949, Reward: [-444.065 -444.065 -444.065] [84.8701], Avg: [-840.963 -840.963 -840.963] (1.000)
Step: 90999, Reward: [-502.95 -502.95 -502.95] [132.0919], Avg: [-840.85 -840.85 -840.85] (1.000)
Step: 91049, Reward: [-426.599 -426.599 -426.599] [58.6770], Avg: [-840.655 -840.655 -840.655] (1.000)
Step: 91099, Reward: [-396.51 -396.51 -396.51] [77.3531], Avg: [-840.453 -840.453 -840.453] (1.000)
Step: 91149, Reward: [-456.724 -456.724 -456.724] [89.3282], Avg: [-840.292 -840.292 -840.292] (1.000)
Step: 91199, Reward: [-433.243 -433.243 -433.243] [51.0565], Avg: [-840.097 -840.097 -840.097] (1.000)
Step: 91249, Reward: [-425.23 -425.23 -425.23] [46.2183], Avg: [-839.895 -839.895 -839.895] (1.000)
Step: 91299, Reward: [-370.501 -370.501 -370.501] [51.0044], Avg: [-839.666 -839.666 -839.666] (1.000)
Step: 91349, Reward: [-444.155 -444.155 -444.155] [97.0563], Avg: [-839.502 -839.502 -839.502] (1.000)
Step: 91399, Reward: [-451.435 -451.435 -451.435] [54.4943], Avg: [-839.32 -839.32 -839.32] (1.000)
Step: 91449, Reward: [-485.742 -485.742 -485.742] [13.0801], Avg: [-839.134 -839.134 -839.134] (1.000)
Step: 91499, Reward: [-390.159 -390.159 -390.159] [48.1138], Avg: [-838.915 -838.915 -838.915] (1.000)
Step: 91549, Reward: [-388.965 -388.965 -388.965] [80.4315], Avg: [-838.713 -838.713 -838.713] (1.000)
Step: 91599, Reward: [-534.502 -534.502 -534.502] [136.6326], Avg: [-838.621 -838.621 -838.621] (1.000)
Step: 91649, Reward: [-491.279 -491.279 -491.279] [135.6032], Avg: [-838.506 -838.506 -838.506] (1.000)
Step: 91699, Reward: [-358.962 -358.962 -358.962] [83.4376], Avg: [-838.29 -838.29 -838.29] (1.000)
Step: 91749, Reward: [-498.728 -498.728 -498.728] [106.9506], Avg: [-838.163 -838.163 -838.163] (1.000)
Step: 91799, Reward: [-549.564 -549.564 -549.564] [185.6164], Avg: [-838.107 -838.107 -838.107] (1.000)
Step: 91849, Reward: [-409.794 -409.794 -409.794] [57.6047], Avg: [-837.905 -837.905 -837.905] (1.000)
Step: 91899, Reward: [-463.772 -463.772 -463.772] [114.0129], Avg: [-837.764 -837.764 -837.764] (1.000)
Step: 91949, Reward: [-475.437 -475.437 -475.437] [68.9229], Avg: [-837.604 -837.604 -837.604] (1.000)
Step: 91999, Reward: [-511.705 -511.705 -511.705] [54.9247], Avg: [-837.457 -837.457 -837.457] (1.000)
Step: 92049, Reward: [-452.605 -452.605 -452.605] [74.2989], Avg: [-837.288 -837.288 -837.288] (1.000)
Step: 92099, Reward: [-887.879 -887.879 -887.879] [690.4934], Avg: [-837.69 -837.69 -837.69] (1.000)
Step: 92149, Reward: [-501.407 -501.407 -501.407] [198.8084], Avg: [-837.616 -837.616 -837.616] (1.000)
Step: 92199, Reward: [-528.643 -528.643 -528.643] [74.2075], Avg: [-837.488 -837.488 -837.488] (1.000)
Step: 92249, Reward: [-494.958 -494.958 -494.958] [55.6675], Avg: [-837.333 -837.333 -837.333] (1.000)
Step: 92299, Reward: [-438.306 -438.306 -438.306] [106.1233], Avg: [-837.174 -837.174 -837.174] (1.000)
Step: 92349, Reward: [-408.917 -408.917 -408.917] [47.0730], Avg: [-836.968 -836.968 -836.968] (1.000)
Step: 92399, Reward: [-454.641 -454.641 -454.641] [79.9480], Avg: [-836.804 -836.804 -836.804] (1.000)
Step: 92449, Reward: [-421.912 -421.912 -421.912] [34.3601], Avg: [-836.599 -836.599 -836.599] (1.000)
Step: 92499, Reward: [-483.223 -483.223 -483.223] [140.5404], Avg: [-836.483 -836.483 -836.483] (1.000)
Step: 92549, Reward: [-561.074 -561.074 -561.074] [71.8371], Avg: [-836.373 -836.373 -836.373] (1.000)
Step: 92599, Reward: [-435.257 -435.257 -435.257] [163.6966], Avg: [-836.245 -836.245 -836.245] (1.000)
Step: 92649, Reward: [-543.201 -543.201 -543.201] [81.5173], Avg: [-836.131 -836.131 -836.131] (1.000)
Step: 92699, Reward: [-486.344 -486.344 -486.344] [50.3214], Avg: [-835.97 -835.97 -835.97] (1.000)
Step: 92749, Reward: [-417.782 -417.782 -417.782] [88.7453], Avg: [-835.792 -835.792 -835.792] (1.000)
Step: 92799, Reward: [-451.658 -451.658 -451.658] [56.2002], Avg: [-835.615 -835.615 -835.615] (1.000)
Step: 92849, Reward: [-538.022 -538.022 -538.022] [155.8615], Avg: [-835.539 -835.539 -835.539] (1.000)
Step: 92899, Reward: [-547.053 -547.053 -547.053] [208.7259], Avg: [-835.496 -835.496 -835.496] (1.000)
Step: 92949, Reward: [-376.904 -376.904 -376.904] [58.1168], Avg: [-835.281 -835.281 -835.281] (1.000)
Step: 92999, Reward: [-504.661 -504.661 -504.661] [79.9587], Avg: [-835.146 -835.146 -835.146] (1.000)
Step: 93049, Reward: [-417.438 -417.438 -417.438] [81.9387], Avg: [-834.965 -834.965 -834.965] (1.000)
Step: 93099, Reward: [-401.47 -401.47 -401.47] [96.0289], Avg: [-834.784 -834.784 -834.784] (1.000)
Step: 93149, Reward: [-474.543 -474.543 -474.543] [88.9989], Avg: [-834.639 -834.639 -834.639] (1.000)
Step: 93199, Reward: [-495.01 -495.01 -495.01] [153.0690], Avg: [-834.539 -834.539 -834.539] (1.000)
Step: 93249, Reward: [-407.774 -407.774 -407.774] [78.5124], Avg: [-834.352 -834.352 -834.352] (1.000)
Step: 93299, Reward: [-420.979 -420.979 -420.979] [85.3415], Avg: [-834.176 -834.176 -834.176] (1.000)
Step: 93349, Reward: [-427.895 -427.895 -427.895] [77.8676], Avg: [-834. -834. -834.] (1.000)
Step: 93399, Reward: [-495.765 -495.765 -495.765] [132.5285], Avg: [-833.89 -833.89 -833.89] (1.000)
Step: 93449, Reward: [-390.644 -390.644 -390.644] [79.0429], Avg: [-833.695 -833.695 -833.695] (1.000)
Step: 93499, Reward: [-435.718 -435.718 -435.718] [68.6862], Avg: [-833.519 -833.519 -833.519] (1.000)
Step: 93549, Reward: [-453.012 -453.012 -453.012] [158.9287], Avg: [-833.401 -833.401 -833.401] (1.000)
Step: 93599, Reward: [-493.238 -493.238 -493.238] [130.0888], Avg: [-833.288 -833.288 -833.288] (1.000)
Step: 93649, Reward: [-480.415 -480.415 -480.415] [133.5475], Avg: [-833.171 -833.171 -833.171] (1.000)
Step: 93699, Reward: [-428.835 -428.835 -428.835] [63.5725], Avg: [-832.989 -832.989 -832.989] (1.000)
Step: 93749, Reward: [-448.135 -448.135 -448.135] [175.7361], Avg: [-832.878 -832.878 -832.878] (1.000)
Step: 93799, Reward: [-485.889 -485.889 -485.889] [124.1115], Avg: [-832.759 -832.759 -832.759] (1.000)
Step: 93849, Reward: [-789.829 -789.829 -789.829] [592.4673], Avg: [-833.052 -833.052 -833.052] (1.000)
Step: 93899, Reward: [-434.696 -434.696 -434.696] [102.0928], Avg: [-832.894 -832.894 -832.894] (1.000)
Step: 93949, Reward: [-412.023 -412.023 -412.023] [49.3111], Avg: [-832.696 -832.696 -832.696] (1.000)
Step: 93999, Reward: [-373.012 -373.012 -373.012] [34.0100], Avg: [-832.47 -832.47 -832.47] (1.000)
Step: 94049, Reward: [-380.986 -380.986 -380.986] [59.7287], Avg: [-832.262 -832.262 -832.262] (1.000)
Step: 94099, Reward: [-418.129 -418.129 -418.129] [43.1486], Avg: [-832.065 -832.065 -832.065] (1.000)
Step: 94149, Reward: [-524.83 -524.83 -524.83] [137.2471], Avg: [-831.974 -831.974 -831.974] (1.000)
Step: 94199, Reward: [-493.038 -493.038 -493.038] [78.1798], Avg: [-831.836 -831.836 -831.836] (1.000)
Step: 94249, Reward: [-623.734 -623.734 -623.734] [347.9105], Avg: [-831.91 -831.91 -831.91] (1.000)
Step: 94299, Reward: [-448.375 -448.375 -448.375] [104.2240], Avg: [-831.762 -831.762 -831.762] (1.000)
Step: 94349, Reward: [-485.862 -485.862 -485.862] [105.4787], Avg: [-831.635 -831.635 -831.635] (1.000)
Step: 94399, Reward: [-726.657 -726.657 -726.657] [495.1892], Avg: [-831.841 -831.841 -831.841] (1.000)
Step: 94449, Reward: [-645.621 -645.621 -645.621] [506.8540], Avg: [-832.011 -832.011 -832.011] (1.000)
Step: 94499, Reward: [-477.813 -477.813 -477.813] [141.4580], Avg: [-831.898 -831.898 -831.898] (1.000)
Step: 94549, Reward: [-438.364 -438.364 -438.364] [89.4890], Avg: [-831.738 -831.738 -831.738] (1.000)
Step: 94599, Reward: [-442.246 -442.246 -442.246] [81.4881], Avg: [-831.575 -831.575 -831.575] (1.000)
Step: 94649, Reward: [-469.408 -469.408 -469.408] [72.0502], Avg: [-831.422 -831.422 -831.422] (1.000)
Step: 94699, Reward: [-445.969 -445.969 -445.969] [113.4693], Avg: [-831.278 -831.278 -831.278] (1.000)
Step: 94749, Reward: [-813.823 -813.823 -813.823] [797.6442], Avg: [-831.69 -831.69 -831.69] (1.000)
Step: 94799, Reward: [-454.528 -454.528 -454.528] [130.0728], Avg: [-831.559 -831.559 -831.559] (1.000)
Step: 94849, Reward: [-427.45 -427.45 -427.45] [61.9339], Avg: [-831.379 -831.379 -831.379] (1.000)
Step: 94899, Reward: [-474.208 -474.208 -474.208] [96.6686], Avg: [-831.242 -831.242 -831.242] (1.000)
Step: 94949, Reward: [-540.698 -540.698 -540.698] [167.8423], Avg: [-831.177 -831.177 -831.177] (1.000)
Step: 94999, Reward: [-438.864 -438.864 -438.864] [97.1035], Avg: [-831.022 -831.022 -831.022] (1.000)
Step: 95049, Reward: [-384.548 -384.548 -384.548] [99.9009], Avg: [-830.839 -830.839 -830.839] (1.000)
Step: 95099, Reward: [-598.359 -598.359 -598.359] [187.2869], Avg: [-830.816 -830.816 -830.816] (1.000)
Step: 95149, Reward: [-466.168 -466.168 -466.168] [60.2936], Avg: [-830.656 -830.656 -830.656] (1.000)
Step: 95199, Reward: [-405.859 -405.859 -405.859] [54.1492], Avg: [-830.461 -830.461 -830.461] (1.000)
Step: 95249, Reward: [-463.42 -463.42 -463.42] [72.0906], Avg: [-830.306 -830.306 -830.306] (1.000)
Step: 95299, Reward: [-520.092 -520.092 -520.092] [93.0972], Avg: [-830.192 -830.192 -830.192] (1.000)
Step: 95349, Reward: [-593.116 -593.116 -593.116] [293.4873], Avg: [-830.222 -830.222 -830.222] (1.000)
Step: 95399, Reward: [-547.177 -547.177 -547.177] [114.3969], Avg: [-830.134 -830.134 -830.134] (1.000)
Step: 95449, Reward: [-447.192 -447.192 -447.192] [132.0902], Avg: [-830.002 -830.002 -830.002] (1.000)
Step: 95499, Reward: [-452.022 -452.022 -452.022] [58.1200], Avg: [-829.835 -829.835 -829.835] (1.000)
Step: 95549, Reward: [-566.979 -566.979 -566.979] [190.3274], Avg: [-829.797 -829.797 -829.797] (1.000)
Step: 95599, Reward: [-426.141 -426.141 -426.141] [118.3520], Avg: [-829.648 -829.648 -829.648] (1.000)
Step: 95649, Reward: [-548.361 -548.361 -548.361] [180.2324], Avg: [-829.595 -829.595 -829.595] (1.000)
Step: 95699, Reward: [-441.9 -441.9 -441.9] [77.9624], Avg: [-829.433 -829.433 -829.433] (1.000)
Step: 95749, Reward: [-425.613 -425.613 -425.613] [91.0196], Avg: [-829.27 -829.27 -829.27] (1.000)
Step: 95799, Reward: [-434.193 -434.193 -434.193] [60.2948], Avg: [-829.095 -829.095 -829.095] (1.000)
Step: 95849, Reward: [-406.05 -406.05 -406.05] [58.4150], Avg: [-828.905 -828.905 -828.905] (1.000)
Step: 95899, Reward: [-501.309 -501.309 -501.309] [83.6700], Avg: [-828.777 -828.777 -828.777] (1.000)
Step: 95949, Reward: [-464.124 -464.124 -464.124] [105.6679], Avg: [-828.642 -828.642 -828.642] (1.000)
Step: 95999, Reward: [-436.989 -436.989 -436.989] [56.1439], Avg: [-828.468 -828.468 -828.468] (1.000)
Step: 96049, Reward: [-465.547 -465.547 -465.547] [71.7868], Avg: [-828.316 -828.316 -828.316] (1.000)
Step: 96099, Reward: [-456.439 -456.439 -456.439] [151.2438], Avg: [-828.201 -828.201 -828.201] (1.000)
Step: 96149, Reward: [-497.261 -497.261 -497.261] [66.5266], Avg: [-828.064 -828.064 -828.064] (1.000)
Step: 96199, Reward: [-543.284 -543.284 -543.284] [61.1826], Avg: [-827.948 -827.948 -827.948] (1.000)
Step: 96249, Reward: [-517.904 -517.904 -517.904] [203.3243], Avg: [-827.892 -827.892 -827.892] (1.000)
Step: 96299, Reward: [-381.035 -381.035 -381.035] [93.4532], Avg: [-827.709 -827.709 -827.709] (1.000)
Step: 96349, Reward: [-460.091 -460.091 -460.091] [104.9023], Avg: [-827.572 -827.572 -827.572] (1.000)
Step: 96399, Reward: [-473.518 -473.518 -473.518] [115.1097], Avg: [-827.448 -827.448 -827.448] (1.000)
Step: 96449, Reward: [-397.584 -397.584 -397.584] [116.8062], Avg: [-827.286 -827.286 -827.286] (1.000)
Step: 96499, Reward: [-500.106 -500.106 -500.106] [92.2463], Avg: [-827.164 -827.164 -827.164] (1.000)
Step: 96549, Reward: [-338.681 -338.681 -338.681] [26.8739], Avg: [-826.925 -826.925 -826.925] (1.000)
Step: 96599, Reward: [-442.137 -442.137 -442.137] [116.7059], Avg: [-826.787 -826.787 -826.787] (1.000)
Step: 96649, Reward: [-505.383 -505.383 -505.383] [137.5085], Avg: [-826.691 -826.691 -826.691] (1.000)
Step: 96699, Reward: [-469.986 -469.986 -469.986] [110.9563], Avg: [-826.564 -826.564 -826.564] (1.000)
Step: 96749, Reward: [-466.912 -466.912 -466.912] [83.0742], Avg: [-826.421 -826.421 -826.421] (1.000)
Step: 96799, Reward: [-402.491 -402.491 -402.491] [82.8473], Avg: [-826.245 -826.245 -826.245] (1.000)
Step: 96849, Reward: [-447.501 -447.501 -447.501] [96.1987], Avg: [-826.099 -826.099 -826.099] (1.000)
Step: 96899, Reward: [-423.125 -423.125 -423.125] [84.7474], Avg: [-825.935 -825.935 -825.935] (1.000)
Step: 96949, Reward: [-417.921 -417.921 -417.921] [33.9396], Avg: [-825.742 -825.742 -825.742] (1.000)
Step: 96999, Reward: [-452.72 -452.72 -452.72] [54.4717], Avg: [-825.578 -825.578 -825.578] (1.000)
Step: 97049, Reward: [-397.255 -397.255 -397.255] [100.8490], Avg: [-825.409 -825.409 -825.409] (1.000)
Step: 97099, Reward: [-400.829 -400.829 -400.829] [51.5965], Avg: [-825.217 -825.217 -825.217] (1.000)
Step: 97149, Reward: [-395.053 -395.053 -395.053] [51.0605], Avg: [-825.022 -825.022 -825.022] (1.000)
Step: 97199, Reward: [-504.838 -504.838 -504.838] [93.5864], Avg: [-824.906 -824.906 -824.906] (1.000)
Step: 97249, Reward: [-487.856 -487.856 -487.856] [92.6511], Avg: [-824.78 -824.78 -824.78] (1.000)
Step: 97299, Reward: [-474.399 -474.399 -474.399] [49.5316], Avg: [-824.625 -824.625 -824.625] (1.000)
Step: 97349, Reward: [-386.356 -386.356 -386.356] [69.2772], Avg: [-824.436 -824.436 -824.436] (1.000)
Step: 97399, Reward: [-389.764 -389.764 -389.764] [49.1557], Avg: [-824.238 -824.238 -824.238] (1.000)
Step: 97449, Reward: [-456.132 -456.132 -456.132] [93.5539], Avg: [-824.097 -824.097 -824.097] (1.000)
Step: 97499, Reward: [-486.36 -486.36 -486.36] [68.2657], Avg: [-823.959 -823.959 -823.959] (1.000)
Step: 97549, Reward: [-361.239 -361.239 -361.239] [70.2561], Avg: [-823.758 -823.758 -823.758] (1.000)
Step: 97599, Reward: [-508.57 -508.57 -508.57] [185.8431], Avg: [-823.691 -823.691 -823.691] (1.000)
Step: 97649, Reward: [-394.564 -394.564 -394.564] [69.7292], Avg: [-823.507 -823.507 -823.507] (1.000)
Step: 97699, Reward: [-438.361 -438.361 -438.361] [136.5162], Avg: [-823.38 -823.38 -823.38] (1.000)
Step: 97749, Reward: [-406.562 -406.562 -406.562] [68.3104], Avg: [-823.202 -823.202 -823.202] (1.000)
Step: 97799, Reward: [-449.828 -449.828 -449.828] [76.0166], Avg: [-823.05 -823.05 -823.05] (1.000)
Step: 97849, Reward: [-446.586 -446.586 -446.586] [138.1386], Avg: [-822.928 -822.928 -822.928] (1.000)
Step: 97899, Reward: [-469.888 -469.888 -469.888] [84.6107], Avg: [-822.791 -822.791 -822.791] (1.000)
Step: 97949, Reward: [-461.053 -461.053 -461.053] [69.4926], Avg: [-822.642 -822.642 -822.642] (1.000)
Step: 97999, Reward: [-368.782 -368.782 -368.782] [83.7408], Avg: [-822.453 -822.453 -822.453] (1.000)
Step: 98049, Reward: [-438.115 -438.115 -438.115] [62.2600], Avg: [-822.289 -822.289 -822.289] (1.000)
Step: 98099, Reward: [-469.036 -469.036 -469.036] [153.7893], Avg: [-822.187 -822.187 -822.187] (1.000)
Step: 98149, Reward: [-707.93 -707.93 -707.93] [536.9221], Avg: [-822.402 -822.402 -822.402] (1.000)
Step: 98199, Reward: [-454.101 -454.101 -454.101] [101.7352], Avg: [-822.267 -822.267 -822.267] (1.000)
Step: 98249, Reward: [-562.566 -562.566 -562.566] [93.3731], Avg: [-822.182 -822.182 -822.182] (1.000)
Step: 98299, Reward: [-377.828 -377.828 -377.828] [48.9614], Avg: [-821.981 -821.981 -821.981] (1.000)
Step: 98349, Reward: [-458.498 -458.498 -458.498] [114.7841], Avg: [-821.855 -821.855 -821.855] (1.000)
Step: 98399, Reward: [-512.926 -512.926 -512.926] [97.5062], Avg: [-821.747 -821.747 -821.747] (1.000)
Step: 98449, Reward: [-387.135 -387.135 -387.135] [46.9192], Avg: [-821.55 -821.55 -821.55] (1.000)
Step: 98499, Reward: [-564.557 -564.557 -564.557] [444.6141], Avg: [-821.645 -821.645 -821.645] (1.000)
Step: 98549, Reward: [-456.918 -456.918 -456.918] [81.5537], Avg: [-821.502 -821.502 -821.502] (1.000)
Step: 98599, Reward: [-472.299 -472.299 -472.299] [69.9954], Avg: [-821.36 -821.36 -821.36] (1.000)
Step: 98649, Reward: [-590.758 -590.758 -590.758] [305.3061], Avg: [-821.398 -821.398 -821.398] (1.000)
Step: 98699, Reward: [-476.945 -476.945 -476.945] [102.4986], Avg: [-821.275 -821.275 -821.275] (1.000)
Step: 98749, Reward: [-465.364 -465.364 -465.364] [78.7747], Avg: [-821.135 -821.135 -821.135] (1.000)
Step: 98799, Reward: [-401.085 -401.085 -401.085] [70.5923], Avg: [-820.958 -820.958 -820.958] (1.000)
Step: 98849, Reward: [-414.286 -414.286 -414.286] [102.1695], Avg: [-820.804 -820.804 -820.804] (1.000)
Step: 98899, Reward: [-433.398 -433.398 -433.398] [50.1247], Avg: [-820.634 -820.634 -820.634] (1.000)
Step: 98949, Reward: [-427.984 -427.984 -427.984] [75.5846], Avg: [-820.474 -820.474 -820.474] (1.000)
Step: 98999, Reward: [-396.245 -396.245 -396.245] [68.4475], Avg: [-820.294 -820.294 -820.294] (1.000)
Step: 99049, Reward: [-484.364 -484.364 -484.364] [174.5587], Avg: [-820.212 -820.212 -820.212] (1.000)
Step: 99099, Reward: [-439.017 -439.017 -439.017] [96.4154], Avg: [-820.069 -820.069 -820.069] (1.000)
Step: 99149, Reward: [-449.862 -449.862 -449.862] [80.4943], Avg: [-819.923 -819.923 -819.923] (1.000)
Step: 99199, Reward: [-438.452 -438.452 -438.452] [55.7144], Avg: [-819.758 -819.758 -819.758] (1.000)
Step: 99249, Reward: [-473.874 -473.874 -473.874] [73.9163], Avg: [-819.621 -819.621 -819.621] (1.000)
Step: 99299, Reward: [-454.501 -454.501 -454.501] [109.8673], Avg: [-819.493 -819.493 -819.493] (1.000)
Step: 99349, Reward: [-458.217 -458.217 -458.217] [82.5618], Avg: [-819.353 -819.353 -819.353] (1.000)
Step: 99399, Reward: [-452.733 -452.733 -452.733] [134.0089], Avg: [-819.236 -819.236 -819.236] (1.000)
Step: 99449, Reward: [-527.006 -527.006 -527.006] [151.6855], Avg: [-819.165 -819.165 -819.165] (1.000)
Step: 99499, Reward: [-495.805 -495.805 -495.805] [77.3290], Avg: [-819.041 -819.041 -819.041] (1.000)
Step: 99549, Reward: [-462.18 -462.18 -462.18] [84.6606], Avg: [-818.905 -818.905 -818.905] (1.000)
Step: 99599, Reward: [-501.9 -501.9 -501.9] [118.0263], Avg: [-818.805 -818.805 -818.805] (1.000)
Step: 99649, Reward: [-394.601 -394.601 -394.601] [72.2825], Avg: [-818.628 -818.628 -818.628] (1.000)
Step: 99699, Reward: [-519.741 -519.741 -519.741] [89.6634], Avg: [-818.523 -818.523 -818.523] (1.000)
Step: 99749, Reward: [-441.679 -441.679 -441.679] [84.9431], Avg: [-818.377 -818.377 -818.377] (1.000)
Step: 99799, Reward: [-391.46 -391.46 -391.46] [64.6413], Avg: [-818.195 -818.195 -818.195] (1.000)
Step: 99849, Reward: [-418.829 -418.829 -418.829] [55.6597], Avg: [-818.023 -818.023 -818.023] (1.000)
Step: 99899, Reward: [-527.735 -527.735 -527.735] [130.1971], Avg: [-817.943 -817.943 -817.943] (1.000)
Step: 99949, Reward: [-418.201 -418.201 -418.201] [74.2443], Avg: [-817.78 -817.78 -817.78] (1.000)
Step: 99999, Reward: [-452.17 -452.17 -452.17] [124.1570], Avg: [-817.66 -817.66 -817.66] (1.000)
