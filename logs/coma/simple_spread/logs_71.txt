Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 4,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 4
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1000

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
# 		del self.network
# 		n_agents = len(action_size)
# 		n_actions = action_size[0][-1]
# 		n_obs = state_size[0][-1]
# 		state_len = int(np.sum([np.prod(space) for space in state_size]))
# 		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
# 		groups = {"agents": n_agents}
# 		scheme = {
# 			"state": {"vshape": state_len},
# 			"obs": {"vshape": n_obs, "group": "agents"},
# 			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
# 			"reward": {"vshape": (1,)},
# 			"done": {"vshape": (1,), "dtype": torch.uint8},
# 		}
		
# 		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
# 		self.replay_buffer = ReplayBuffer(scheme, groups, 2000, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
# 		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
# 		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.episode_batch = self.new_episode_batch(NUM_ENVS)
# 		self.mac.init_hidden(batch_size=NUM_ENVS)
# 		self.num_envs = NUM_ENVS
# 		self.time = 0
# 		self.replay_buffer2 = MultiagentReplayBuffer(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
# 		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
# 		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
# 		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
# 		state_joint = np.concatenate(state, -1)
# 		obs = np.concatenate(state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
# 		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		self.episode_batch.update({"state": [state_joint], "obs": [obs]}, ts=self.step)
# 		actions = self.mac.select_actions(self.episode_batch, inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
# 		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
# 		return np.split(self.action, actions.size(-1), axis=-2)

# 	def train(self, state, action, next_state, reward, done):
# 		actions, rewards, dones = [list(zip(*x)) for x in [action, reward, done]]
# 		actions_one_hot = [np.argmax(a, -1) for a in actions]
# 		rewards = [np.mean(rewards, -1)]
# 		dones = [np.any(dones, -1)]
# 		obs = np.concatenate(state, -2)
# 		next_obs = np.concatenate(next_state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		actor_inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		post_transition_data = {"actions": actions_one_hot, "reward": rewards, "done": dones}
# 		self.replay_buffer2.add(state, action, next_state, reward, done)
# 		self.episode_batch.update(post_transition_data, ts=self.step)
# 		if np.any(done[0]):
# 			state_joint = np.concatenate(state, -1)
# 			self.episode_batch.update({"state": [state_joint], "obs": [next_obs]}, ts=self.step)
# 			actions = self.mac.select_actions(self.episode_batch, actor_inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 			self.episode_batch.update({"actions": actions}, ts=self.step)
# 			self.replay_buffer.insert_episode_batch(self.episode_batch)
# 			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
# 				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
# 				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, self.device)
# 				max_ep_t = episode_sample.max_t_filled()
# 				episode_sample = episode_sample[:, :max_ep_t]
# 				if episode_sample.device != self.device: episode_sample.to(self.device)
# 				self.learner.train(episode_sample)
# 			self.episode_batch = self.new_episode_batch(state[0].shape[0])
# 			self.mac.init_hidden(self.num_envs)
# 			self.time += self.step
# 			self.step = 0

# class OneHot():
# 	def __init__(self, out_dim):
# 		self.out_dim = out_dim

# 	def transform(self, tensor):
# 		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
# 		y_onehot.scatter_(-1, tensor.long(), 1)
# 		return y_onehot.float()

# 	def infer_output_info(self, vshape_in, dtype_in):
# 		return (self.out_dim,), torch.float32

# class COMALearner():
# 	def __init__(self, mac, scheme, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.last_target_update_step = 0
# 		self.mac = mac
# 		self.critic_training_steps = 0
# 		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
# 		self.critic_params = list(self.critic.parameters())
# 		self.agent_params = list(mac.parameters())
# 		self.params = self.agent_params + self.critic_params
# 		self.target_critic = copy.deepcopy(self.critic)
# 		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
# 		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

# 	def train(self, batch):
# 		# Get the relevant quantities
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length
# 		rewards = batch["reward"][:, :-1]
# 		actions = batch["actions"][:, :]
# 		done = batch["done"][:, :-1].float()
# 		mask = batch["filled"][:, :-1].float()
# 		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
# 		critic_mask = mask.clone()
# 		mask = mask.repeat(1, 1, self.n_agents).view(-1)
# 		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
# 		actions = actions[:,:-1]
# 		mac_out = []
# 		self.mac.init_hidden(batch.batch_size)
# 		for t in range(batch.max_seq_length - 1):
# 			agent_outs = self.mac.forward(batch, t=t)
# 			mac_out.append(agent_outs)
# 		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
# 		# Mask out unavailable actions, renormalise (as in action selection)
# 		q_vals = q_vals.reshape(-1, self.n_actions)
# 		pi = mac_out.view(-1, self.n_actions)
# 		baseline = (pi * q_vals).sum(-1).detach()
# 		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken[mask == 0] = 1.0
# 		log_pi_taken = torch.log(pi_taken)
# 		advantages = (q_taken - baseline).detach()
# 		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
# 		self.agent_optimiser.zero_grad()
# 		coma_loss.backward()
# 		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
# 		self.agent_optimiser.step()
# 		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
# 			self._update_targets()
# 			self.last_target_update_step = self.critic_training_steps

# 	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
# 		target_q_vals = self.target_critic(batch)[:, :]
# 		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
# 		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
# 		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
# 		for t in reversed(range(rewards.size(1))):
# 			mask_t = mask[:, t].expand(-1, self.n_agents)
# 			if mask_t.sum() == 0:
# 				continue
# 			q_t = self.critic(batch, t)
# 			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
# 			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
# 			targets_t = targets[:, t]
# 			td_error = (q_taken - targets_t.detach())
# 			# 0-out the targets that came from padded data
# 			masked_td_error = td_error * mask_t
# 			loss = (masked_td_error ** 2).sum() / mask_t.sum()
# 			self.critic_optimiser.zero_grad()
# 			loss.backward()
# 			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
# 			self.critic_optimiser.step()
# 			self.critic_training_steps += 1
# 		return q_vals

# 	def _update_targets(self):
# 		self.target_critic.load_state_dict(self.critic.state_dict())

# 	def cuda(self):
# 		self.mac.cuda()
# 		self.critic.cuda()
# 		self.target_critic.cuda()

# def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
# 	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
# 	# Initialise  last  lambda -return  for  not  done  episodes
# 	ret = target_qs.new_zeros(*target_qs.shape)
# 	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
# 	# Backwards  recursive  update  of the "forward  view"
# 	for t in range(ret.shape[1] - 2, -1,  -1):
# 		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
# 	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
# 	return ret[:, 0:-1]

# class COMACritic(torch.nn.Module):
# 	def __init__(self, scheme, n_agents, n_actions):
# 		super(COMACritic, self).__init__()
# 		self.n_actions = n_actions
# 		self.n_agents = n_agents
# 		input_shape = self._get_input_shape(scheme)
# 		self.output_type = "q"
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

# 	def forward(self, batch, t=None):
# 		inputs = self._build_inputs(batch, t=t)
# 		x = torch.relu(self.fc1(inputs))
# 		x = torch.relu(self.fc2(x))
# 		q = self.fc3(x)
# 		return q

# 	def _build_inputs(self, batch, t=None):
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length if t is None else 1
# 		ts = slice(None) if t is None else slice(t, t+1)
# 		inputs = []
# 		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
# 		inputs.append(batch["obs"][:, ts])
# 		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
# 		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
# 		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
# 		# last actions
# 		if t == 0:
# 			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		elif isinstance(t, int):
# 			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		else:
# 			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
# 			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 			inputs.append(last_actions)

# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["state"]["vshape"]
# 		input_shape += scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
# 		input_shape += self.n_agents
# 		return input_shape

# class BasicMAC:
# 	def __init__(self, scheme, groups, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
# 		self.action_selector = MultinomialActionSelector()
# 		self.hidden_states = None

# 	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
# 		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
# 		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
# 		return chosen_actions

# 	def forward(self, ep_batch, inputs, t, test_mode=False):
# 		agent_inputs = self._build_inputs(ep_batch, t)
# 		agent_outs = self.agent(agent_inputs, self.hidden_states)
# 		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
# 		if not test_mode:
# 			epsilon_action_num = agent_outs.size(-1)
# 			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
# 		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

# 	def init_hidden(self, batch_size):
# 		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

# 	def parameters(self):
# 		return self.agent.parameters()

# 	def _build_inputs(self, batch, t):
# 		bs = batch.batch_size
# 		inputs = []
# 		inputs.append(batch["obs"][:, t])  # b1av
# 		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0]
# 		input_shape += self.n_agents
# 		return input_shape

# class RNNAgent(torch.nn.Module):
# 	def __init__(self, input_shape, output_shape):
# 		super(RNNAgent, self).__init__()
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		# self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

# 	def init_hidden(self):
# 		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

# 	def forward(self, inputs, hidden_state):
# 		x = torch.relu(self.fc1(inputs))
# 		# h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
# 		# h = self.rnn(x, h_in)
# 		x = self.fc3(x).relu()
# 		q = self.fc2(x)
# 		return q

# class MultinomialActionSelector():
# 	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
# 		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
# 		self.epsilon = self.schedule.eval(0)

# 	def select_action(self, agent_inputs, t_env, test_mode=False):
# 		self.epsilon = self.schedule.eval(t_env)
# 		masked_policies = agent_inputs.clone()
# 		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
# 		return picked_actions

# class DecayThenFlatSchedule():
# 	def __init__(self, start, finish, time_length, decay="exp"):
# 		self.start = start
# 		self.finish = finish
# 		self.time_length = time_length
# 		self.delta = (self.start - self.finish) / self.time_length
# 		self.decay = decay
# 		if self.decay in ["exp"]:
# 			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

# 	def eval(self, T):
# 		if self.decay in ["linear"]:
# 			return max(self.finish, self.start - self.delta * T)
# 		elif self.decay in ["exp"]:
# 			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

# from types import SimpleNamespace as SN

# class EpisodeBatch():
# 	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
# 		self.scheme = scheme.copy()
# 		self.groups = groups
# 		self.batch_size = batch_size
# 		self.max_seq_length = max_seq_length
# 		self.preprocess = {} if preprocess is None else preprocess
# 		self.device = device

# 		if data is not None:
# 			self.data = data
# 		else:
# 			self.data = SN()
# 			self.data.transition_data = {}
# 			self.data.episode_data = {}
# 			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

# 	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
# 		if preprocess is not None:
# 			for k in preprocess:
# 				assert k in scheme
# 				new_k = preprocess[k][0]
# 				transforms = preprocess[k][1]
# 				vshape = self.scheme[k]["vshape"]
# 				dtype = self.scheme[k]["dtype"]
# 				for transform in transforms:
# 					vshape, dtype = transform.infer_output_info(vshape, dtype)
# 				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
# 				if "group" in self.scheme[k]:
# 					self.scheme[new_k]["group"] = self.scheme[k]["group"]
# 				if "episode_const" in self.scheme[k]:
# 					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

# 		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
# 		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

# 		for field_key, field_info in scheme.items():
# 			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
# 			vshape = field_info["vshape"]
# 			episode_const = field_info.get("episode_const", False)
# 			group = field_info.get("group", None)
# 			dtype = field_info.get("dtype", torch.float32)

# 			if isinstance(vshape, int):
# 				vshape = (vshape,)
# 			if group:
# 				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
# 				shape = (groups[group], *vshape)
# 			else:
# 				shape = vshape
# 			if episode_const:
# 				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
# 			else:
# 				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

# 	def extend(self, scheme, groups=None):
# 		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

# 	def to(self, device):
# 		for k, v in self.data.transition_data.items():
# 			self.data.transition_data[k] = v.to(device)
# 		for k, v in self.data.episode_data.items():
# 			self.data.episode_data[k] = v.to(device)
# 		self.device = device

# 	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
# 		slices = self._parse_slices((bs, ts))
# 		for k, v in data.items():
# 			if k in self.data.transition_data:
# 				target = self.data.transition_data
# 				if mark_filled:
# 					target["filled"][slices] = 1
# 					mark_filled = False
# 				_slices = slices
# 			elif k in self.data.episode_data:
# 				target = self.data.episode_data
# 				_slices = slices[0]
# 			else:
# 				raise KeyError("{} not found in transition or episode data".format(k))

# 			dtype = self.scheme[k].get("dtype", torch.float32)
# 			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
# 			self._check_safe_view(v, target[k][_slices])
# 			target[k][_slices] = v.view_as(target[k][_slices])

# 			if k in self.preprocess:
# 				new_k = self.preprocess[k][0]
# 				v = target[k][_slices]
# 				for transform in self.preprocess[k][1]:
# 					v = transform.transform(v)
# 				target[new_k][_slices] = v.view_as(target[new_k][_slices])

# 	def _check_safe_view(self, v, dest):
# 		idx = len(v.shape) - 1
# 		for s in dest.shape[::-1]:
# 			if v.shape[idx] != s:
# 				if s != 1:
# 					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
# 			else:
# 				idx -= 1

# 	def __getitem__(self, item):
# 		if isinstance(item, str):
# 			if item in self.data.episode_data:
# 				return self.data.episode_data[item]
# 			elif item in self.data.transition_data:
# 				return self.data.transition_data[item]
# 			else:
# 				raise ValueError
# 		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
# 			new_data = self._new_data_sn()
# 			for key in item:
# 				if key in self.data.transition_data:
# 					new_data.transition_data[key] = self.data.transition_data[key]
# 				elif key in self.data.episode_data:
# 					new_data.episode_data[key] = self.data.episode_data[key]
# 				else:
# 					raise KeyError("Unrecognised key {}".format(key))

# 			# Update the scheme to only have the requested keys
# 			new_scheme = {key: self.scheme[key] for key in item}
# 			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
# 						for key in item if "group" in self.scheme[key]}
# 			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
# 			return ret
# 		else:
# 			item = self._parse_slices(item)
# 			new_data = self._new_data_sn()
# 			for k, v in self.data.transition_data.items():
# 				new_data.transition_data[k] = v[item]
# 			for k, v in self.data.episode_data.items():
# 				new_data.episode_data[k] = v[item[0]]

# 			ret_bs = self._get_num_items(item[0], self.batch_size)
# 			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

# 			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
# 			return ret

# 	def _get_num_items(self, indexing_item, max_size):
# 		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
# 			return len(indexing_item)
# 		elif isinstance(indexing_item, slice):
# 			_range = indexing_item.indices(max_size)
# 			return 1 + (_range[1] - _range[0] - 1)//_range[2]

# 	def _new_data_sn(self):
# 		new_data = SN()
# 		new_data.transition_data = {}
# 		new_data.episode_data = {}
# 		return new_data

# 	def _parse_slices(self, items):
# 		parsed = []
# 		# Only batch slice given, add full time slice
# 		if (isinstance(items, slice)  # slice a:b
# 			or isinstance(items, int)  # int i
# 			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
# 			):
# 			items = (items, slice(None))

# 		# Need the time indexing to be contiguous
# 		if isinstance(items[1], list):
# 			raise IndexError("Indexing across Time must be contiguous")

# 		for item in items:
# 			#TODO: stronger checks to ensure only supported options get through
# 			if isinstance(item, int):
# 				# Convert single indices to slices
# 				parsed.append(slice(item, item+1))
# 			else:
# 				# Leave slices and lists as is
# 				parsed.append(item)
# 		return parsed

# 	def max_t_filled(self):
# 		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

# class ReplayBuffer(EpisodeBatch):
# 	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
# 		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
# 		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
# 		self.buffer_index = 0
# 		self.episodes_in_buffer = 0

# 	def insert_episode_batch(self, ep_batch):
# 		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
# 			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
# 			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
# 			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
# 			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
# 			self.buffer_index = self.buffer_index % self.buffer_size
# 			assert self.buffer_index < self.buffer_size
# 		else:
# 			buffer_left = self.buffer_size - self.buffer_index
# 			self.insert_episode_batch(ep_batch[0:buffer_left, :])
# 			self.insert_episode_batch(ep_batch[buffer_left:, :])

# 	def can_sample(self, batch_size):
# 		return self.episodes_in_buffer >= batch_size

# 	def sample(self, batch_size):
# 		assert self.can_sample(batch_size)
# 		if self.episodes_in_buffer == batch_size:
# 			return self[:batch_size]
# 		else:
# 			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
# 			return self[ep_ids]


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_probs = self.action_probs(state).softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action_in = dist.sample()
		action = one_hot_from_indices(action_in, action_probs.size(-1))
		entropy = dist.entropy()
		return action, action_probs, entropy

	def init_hidden(self, batch_size=1):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		q_values = self.q_values(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + 0.001*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(2000, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[4], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-453.778 -453.778 -453.778] [31.121], Avg: [-453.778 -453.778 -453.778] (1.0000) ({r_i: None, r_t: [-8.468 -8.468 -8.468], eps: 1.0})
Step:     100, Reward: [-557.889 -557.889 -557.889] [105.944], Avg: [-505.834 -505.834 -505.834] (1.0000) ({r_i: None, r_t: [-994.045 -994.045 -994.045], eps: 1.0})
Step:     200, Reward: [-521.773 -521.773 -521.773] [57.931], Avg: [-511.147 -511.147 -511.147] (1.0000) ({r_i: None, r_t: [-1027.341 -1027.341 -1027.341], eps: 1.0})
Step:     300, Reward: [-693.600 -693.600 -693.600] [130.791], Avg: [-556.760 -556.760 -556.760] (1.0000) ({r_i: None, r_t: [-964.607 -964.607 -964.607], eps: 1.0})
Step:     400, Reward: [-532.021 -532.021 -532.021] [91.296], Avg: [-551.812 -551.812 -551.812] (1.0000) ({r_i: None, r_t: [-1102.508 -1102.508 -1102.508], eps: 1.0})
Step:     500, Reward: [-557.323 -557.323 -557.323] [101.564], Avg: [-552.731 -552.731 -552.731] (1.0000) ({r_i: None, r_t: [-1045.825 -1045.825 -1045.825], eps: 1.0})
Step:     600, Reward: [-505.257 -505.257 -505.257] [54.190], Avg: [-545.949 -545.949 -545.949] (1.0000) ({r_i: None, r_t: [-973.786 -973.786 -973.786], eps: 1.0})
Step:     700, Reward: [-582.211 -582.211 -582.211] [110.922], Avg: [-550.481 -550.481 -550.481] (1.0000) ({r_i: None, r_t: [-1022.122 -1022.122 -1022.122], eps: 1.0})
Step:     800, Reward: [-508.133 -508.133 -508.133] [55.390], Avg: [-545.776 -545.776 -545.776] (1.0000) ({r_i: None, r_t: [-1053.042 -1053.042 -1053.042], eps: 1.0})
Step:     900, Reward: [-492.725 -492.725 -492.725] [99.151], Avg: [-540.471 -540.471 -540.471] (1.0000) ({r_i: None, r_t: [-954.309 -954.309 -954.309], eps: 1.0})
Step:    1000, Reward: [-510.401 -510.401 -510.401] [35.757], Avg: [-537.737 -537.737 -537.737] (1.0000) ({r_i: None, r_t: [-1211.826 -1211.826 -1211.826], eps: 1.0})
Step:    1100, Reward: [-425.919 -425.919 -425.919] [51.265], Avg: [-528.419 -528.419 -528.419] (1.0000) ({r_i: None, r_t: [-1130.739 -1130.739 -1130.739], eps: 1.0})
Step:    1200, Reward: [-467.103 -467.103 -467.103] [66.432], Avg: [-523.702 -523.702 -523.702] (1.0000) ({r_i: None, r_t: [-1026.819 -1026.819 -1026.819], eps: 1.0})
Step:    1300, Reward: [-549.533 -549.533 -549.533] [121.575], Avg: [-525.548 -525.548 -525.548] (1.0000) ({r_i: None, r_t: [-1065.921 -1065.921 -1065.921], eps: 1.0})
Step:    1400, Reward: [-501.982 -501.982 -501.982] [107.069], Avg: [-523.976 -523.976 -523.976] (1.0000) ({r_i: None, r_t: [-1014.109 -1014.109 -1014.109], eps: 1.0})
Step:    1500, Reward: [-528.415 -528.415 -528.415] [91.678], Avg: [-524.254 -524.254 -524.254] (1.0000) ({r_i: None, r_t: [-1153.642 -1153.642 -1153.642], eps: 1.0})
Step:    1600, Reward: [-523.116 -523.116 -523.116] [85.361], Avg: [-524.187 -524.187 -524.187] (1.0000) ({r_i: None, r_t: [-880.084 -880.084 -880.084], eps: 1.0})
Step:    1700, Reward: [-513.477 -513.477 -513.477] [95.859], Avg: [-523.592 -523.592 -523.592] (1.0000) ({r_i: None, r_t: [-1068.137 -1068.137 -1068.137], eps: 1.0})
Step:    1800, Reward: [-529.790 -529.790 -529.790] [85.444], Avg: [-523.918 -523.918 -523.918] (1.0000) ({r_i: None, r_t: [-1111.083 -1111.083 -1111.083], eps: 1.0})
Step:    1900, Reward: [-487.826 -487.826 -487.826] [37.953], Avg: [-522.114 -522.114 -522.114] (1.0000) ({r_i: None, r_t: [-959.476 -959.476 -959.476], eps: 1.0})
Step:    2000, Reward: [-659.118 -659.118 -659.118] [103.771], Avg: [-528.638 -528.638 -528.638] (1.0000) ({r_i: None, r_t: [-1079.071 -1079.071 -1079.071], eps: 1.0})
Step:    2100, Reward: [-523.043 -523.043 -523.043] [91.976], Avg: [-528.383 -528.383 -528.383] (1.0000) ({r_i: None, r_t: [-1073.720 -1073.720 -1073.720], eps: 1.0})
Step:    2200, Reward: [-456.182 -456.182 -456.182] [81.462], Avg: [-525.244 -525.244 -525.244] (1.0000) ({r_i: None, r_t: [-957.095 -957.095 -957.095], eps: 1.0})
Step:    2300, Reward: [-436.665 -436.665 -436.665] [59.608], Avg: [-521.553 -521.553 -521.553] (1.0000) ({r_i: None, r_t: [-1125.142 -1125.142 -1125.142], eps: 1.0})
Step:    2400, Reward: [-535.389 -535.389 -535.389] [127.125], Avg: [-522.107 -522.107 -522.107] (1.0000) ({r_i: None, r_t: [-881.898 -881.898 -881.898], eps: 1.0})
Step:    2500, Reward: [-478.208 -478.208 -478.208] [149.960], Avg: [-520.418 -520.418 -520.418] (1.0000) ({r_i: None, r_t: [-988.950 -988.950 -988.950], eps: 1.0})
Step:    2600, Reward: [-433.319 -433.319 -433.319] [79.984], Avg: [-517.192 -517.192 -517.192] (1.0000) ({r_i: None, r_t: [-1023.141 -1023.141 -1023.141], eps: 1.0})
Step:    2700, Reward: [-480.478 -480.478 -480.478] [47.807], Avg: [-515.881 -515.881 -515.881] (1.0000) ({r_i: None, r_t: [-964.942 -964.942 -964.942], eps: 1.0})
Step:    2800, Reward: [-471.577 -471.577 -471.577] [34.444], Avg: [-514.353 -514.353 -514.353] (1.0000) ({r_i: None, r_t: [-1133.238 -1133.238 -1133.238], eps: 1.0})
Step:    2900, Reward: [-481.498 -481.498 -481.498] [29.472], Avg: [-513.258 -513.258 -513.258] (1.0000) ({r_i: None, r_t: [-970.084 -970.084 -970.084], eps: 1.0})
Step:    3000, Reward: [-527.698 -527.698 -527.698] [141.751], Avg: [-513.724 -513.724 -513.724] (1.0000) ({r_i: None, r_t: [-924.724 -924.724 -924.724], eps: 1.0})
Step:    3100, Reward: [-540.194 -540.194 -540.194] [128.946], Avg: [-514.551 -514.551 -514.551] (1.0000) ({r_i: None, r_t: [-1074.400 -1074.400 -1074.400], eps: 1.0})
Step:    3200, Reward: [-442.507 -442.507 -442.507] [46.010], Avg: [-512.368 -512.368 -512.368] (1.0000) ({r_i: None, r_t: [-1139.607 -1139.607 -1139.607], eps: 1.0})
Step:    3300, Reward: [-638.865 -638.865 -638.865] [132.304], Avg: [-516.089 -516.089 -516.089] (1.0000) ({r_i: None, r_t: [-1082.582 -1082.582 -1082.582], eps: 1.0})
Step:    3400, Reward: [-515.277 -515.277 -515.277] [80.721], Avg: [-516.065 -516.065 -516.065] (1.0000) ({r_i: None, r_t: [-897.449 -897.449 -897.449], eps: 1.0})
Step:    3500, Reward: [-577.845 -577.845 -577.845] [146.436], Avg: [-517.782 -517.782 -517.782] (1.0000) ({r_i: None, r_t: [-1091.140 -1091.140 -1091.140], eps: 1.0})
Step:    3600, Reward: [-504.352 -504.352 -504.352] [60.550], Avg: [-517.419 -517.419 -517.419] (1.0000) ({r_i: None, r_t: [-1043.990 -1043.990 -1043.990], eps: 1.0})
Step:    3700, Reward: [-530.518 -530.518 -530.518] [95.019], Avg: [-517.763 -517.763 -517.763] (1.0000) ({r_i: None, r_t: [-962.720 -962.720 -962.720], eps: 1.0})
Step:    3800, Reward: [-519.980 -519.980 -519.980] [44.974], Avg: [-517.820 -517.820 -517.820] (1.0000) ({r_i: None, r_t: [-941.498 -941.498 -941.498], eps: 1.0})
Step:    3900, Reward: [-519.061 -519.061 -519.061] [105.100], Avg: [-517.851 -517.851 -517.851] (1.0000) ({r_i: None, r_t: [-1154.568 -1154.568 -1154.568], eps: 1.0})
Step:    4000, Reward: [-402.208 -402.208 -402.208] [13.963], Avg: [-515.031 -515.031 -515.031] (1.0000) ({r_i: None, r_t: [-1100.404 -1100.404 -1100.404], eps: 1.0})
Step:    4100, Reward: [-550.950 -550.950 -550.950] [65.949], Avg: [-515.886 -515.886 -515.886] (1.0000) ({r_i: None, r_t: [-1127.442 -1127.442 -1127.442], eps: 1.0})
Step:    4200, Reward: [-536.069 -536.069 -536.069] [128.769], Avg: [-516.355 -516.355 -516.355] (1.0000) ({r_i: None, r_t: [-1093.687 -1093.687 -1093.687], eps: 1.0})
Step:    4300, Reward: [-519.878 -519.878 -519.878] [137.015], Avg: [-516.435 -516.435 -516.435] (1.0000) ({r_i: None, r_t: [-967.081 -967.081 -967.081], eps: 1.0})
Step:    4400, Reward: [-472.154 -472.154 -472.154] [104.571], Avg: [-515.451 -515.451 -515.451] (1.0000) ({r_i: None, r_t: [-1035.959 -1035.959 -1035.959], eps: 1.0})
Step:    4500, Reward: [-486.470 -486.470 -486.470] [71.899], Avg: [-514.821 -514.821 -514.821] (1.0000) ({r_i: None, r_t: [-1039.198 -1039.198 -1039.198], eps: 1.0})
Step:    4600, Reward: [-529.744 -529.744 -529.744] [28.466], Avg: [-515.139 -515.139 -515.139] (1.0000) ({r_i: None, r_t: [-1162.006 -1162.006 -1162.006], eps: 1.0})
Step:    4700, Reward: [-528.133 -528.133 -528.133] [48.248], Avg: [-515.409 -515.409 -515.409] (1.0000) ({r_i: None, r_t: [-1094.053 -1094.053 -1094.053], eps: 1.0})
Step:    4800, Reward: [-608.424 -608.424 -608.424] [184.175], Avg: [-517.308 -517.308 -517.308] (1.0000) ({r_i: None, r_t: [-945.348 -945.348 -945.348], eps: 1.0})
Step:    4900, Reward: [-561.614 -561.614 -561.614] [63.253], Avg: [-518.194 -518.194 -518.194] (1.0000) ({r_i: None, r_t: [-985.506 -985.506 -985.506], eps: 1.0})
Step:    5000, Reward: [-554.653 -554.653 -554.653] [175.757], Avg: [-518.909 -518.909 -518.909] (1.0000) ({r_i: None, r_t: [-1055.137 -1055.137 -1055.137], eps: 1.0})
Step:    5100, Reward: [-524.434 -524.434 -524.434] [95.083], Avg: [-519.015 -519.015 -519.015] (1.0000) ({r_i: None, r_t: [-1010.159 -1010.159 -1010.159], eps: 1.0})
Step:    5200, Reward: [-476.779 -476.779 -476.779] [130.049], Avg: [-518.218 -518.218 -518.218] (1.0000) ({r_i: None, r_t: [-1035.929 -1035.929 -1035.929], eps: 1.0})
Step:    5300, Reward: [-543.171 -543.171 -543.171] [74.474], Avg: [-518.680 -518.680 -518.680] (1.0000) ({r_i: None, r_t: [-990.362 -990.362 -990.362], eps: 1.0})
Step:    5400, Reward: [-528.781 -528.781 -528.781] [73.591], Avg: [-518.864 -518.864 -518.864] (1.0000) ({r_i: None, r_t: [-921.414 -921.414 -921.414], eps: 1.0})
Step:    5500, Reward: [-551.060 -551.060 -551.060] [86.626], Avg: [-519.439 -519.439 -519.439] (1.0000) ({r_i: None, r_t: [-1075.308 -1075.308 -1075.308], eps: 1.0})
Step:    5600, Reward: [-555.557 -555.557 -555.557] [53.785], Avg: [-520.072 -520.072 -520.072] (1.0000) ({r_i: None, r_t: [-1081.361 -1081.361 -1081.361], eps: 1.0})
Step:    5700, Reward: [-546.006 -546.006 -546.006] [112.417], Avg: [-520.519 -520.519 -520.519] (1.0000) ({r_i: None, r_t: [-1082.553 -1082.553 -1082.553], eps: 1.0})
Step:    5800, Reward: [-584.168 -584.168 -584.168] [37.096], Avg: [-521.598 -521.598 -521.598] (1.0000) ({r_i: None, r_t: [-970.173 -970.173 -970.173], eps: 1.0})
Step:    5900, Reward: [-521.469 -521.469 -521.469] [90.155], Avg: [-521.596 -521.596 -521.596] (1.0000) ({r_i: None, r_t: [-1024.887 -1024.887 -1024.887], eps: 1.0})
Step:    6000, Reward: [-518.430 -518.430 -518.430] [49.476], Avg: [-521.544 -521.544 -521.544] (1.0000) ({r_i: None, r_t: [-1084.849 -1084.849 -1084.849], eps: 1.0})
Step:    6100, Reward: [-431.113 -431.113 -431.113] [44.827], Avg: [-520.086 -520.086 -520.086] (1.0000) ({r_i: None, r_t: [-932.644 -932.644 -932.644], eps: 1.0})
Step:    6200, Reward: [-463.588 -463.588 -463.588] [56.776], Avg: [-519.189 -519.189 -519.189] (1.0000) ({r_i: None, r_t: [-1279.633 -1279.633 -1279.633], eps: 1.0})
Step:    6300, Reward: [-440.013 -440.013 -440.013] [54.772], Avg: [-517.952 -517.952 -517.952] (1.0000) ({r_i: None, r_t: [-1008.724 -1008.724 -1008.724], eps: 1.0})
Step:    6400, Reward: [-457.478 -457.478 -457.478] [26.256], Avg: [-517.021 -517.021 -517.021] (1.0000) ({r_i: None, r_t: [-1049.946 -1049.946 -1049.946], eps: 1.0})
Step:    6500, Reward: [-453.190 -453.190 -453.190] [65.814], Avg: [-516.054 -516.054 -516.054] (1.0000) ({r_i: None, r_t: [-1003.105 -1003.105 -1003.105], eps: 1.0})
Step:    6600, Reward: [-504.930 -504.930 -504.930] [72.996], Avg: [-515.888 -515.888 -515.888] (1.0000) ({r_i: None, r_t: [-1054.350 -1054.350 -1054.350], eps: 1.0})
Step:    6700, Reward: [-528.033 -528.033 -528.033] [132.604], Avg: [-516.067 -516.067 -516.067] (1.0000) ({r_i: None, r_t: [-1008.007 -1008.007 -1008.007], eps: 1.0})
Step:    6800, Reward: [-538.470 -538.470 -538.470] [142.412], Avg: [-516.391 -516.391 -516.391] (1.0000) ({r_i: None, r_t: [-972.954 -972.954 -972.954], eps: 1.0})
Step:    6900, Reward: [-601.160 -601.160 -601.160] [186.739], Avg: [-517.602 -517.602 -517.602] (1.0000) ({r_i: None, r_t: [-1147.204 -1147.204 -1147.204], eps: 1.0})
Step:    7000, Reward: [-502.435 -502.435 -502.435] [114.742], Avg: [-517.389 -517.389 -517.389] (1.0000) ({r_i: None, r_t: [-1109.421 -1109.421 -1109.421], eps: 1.0})
Step:    7100, Reward: [-592.911 -592.911 -592.911] [189.674], Avg: [-518.438 -518.438 -518.438] (1.0000) ({r_i: None, r_t: [-1049.064 -1049.064 -1049.064], eps: 1.0})
Step:    7200, Reward: [-436.676 -436.676 -436.676] [72.955], Avg: [-517.318 -517.318 -517.318] (1.0000) ({r_i: None, r_t: [-1080.802 -1080.802 -1080.802], eps: 1.0})
Step:    7300, Reward: [-548.313 -548.313 -548.313] [131.358], Avg: [-517.737 -517.737 -517.737] (1.0000) ({r_i: None, r_t: [-928.791 -928.791 -928.791], eps: 1.0})
Step:    7400, Reward: [-491.238 -491.238 -491.238] [48.488], Avg: [-517.383 -517.383 -517.383] (1.0000) ({r_i: None, r_t: [-1003.634 -1003.634 -1003.634], eps: 1.0})
Step:    7500, Reward: [-507.227 -507.227 -507.227] [101.823], Avg: [-517.250 -517.250 -517.250] (1.0000) ({r_i: None, r_t: [-1033.342 -1033.342 -1033.342], eps: 1.0})
Step:    7600, Reward: [-413.901 -413.901 -413.901] [73.076], Avg: [-515.907 -515.907 -515.907] (0.9950) ({r_i: None, r_t: [-1004.383 -1004.383 -1004.383], eps: 0.995})
Step:    7700, Reward: [-547.187 -547.187 -547.187] [129.625], Avg: [-516.308 -516.308 -516.308] (0.9950) ({r_i: None, r_t: [-1138.269 -1138.269 -1138.269], eps: 0.995})
Step:    7800, Reward: [-489.006 -489.006 -489.006] [13.346], Avg: [-515.963 -515.963 -515.963] (0.9950) ({r_i: None, r_t: [-929.574 -929.574 -929.574], eps: 0.995})
Step:    7900, Reward: [-480.791 -480.791 -480.791] [89.821], Avg: [-515.523 -515.523 -515.523] (0.9950) ({r_i: None, r_t: [-1038.555 -1038.555 -1038.555], eps: 0.995})
Step:    8000, Reward: [-533.438 -533.438 -533.438] [73.571], Avg: [-515.744 -515.744 -515.744] (0.9950) ({r_i: None, r_t: [-1069.323 -1069.323 -1069.323], eps: 0.995})
Step:    8100, Reward: [-504.299 -504.299 -504.299] [93.974], Avg: [-515.605 -515.605 -515.605] (0.9950) ({r_i: None, r_t: [-1007.934 -1007.934 -1007.934], eps: 0.995})
Step:    8200, Reward: [-585.574 -585.574 -585.574] [170.376], Avg: [-516.448 -516.448 -516.448] (0.9950) ({r_i: None, r_t: [-913.399 -913.399 -913.399], eps: 0.995})
Step:    8300, Reward: [-425.242 -425.242 -425.242] [76.545], Avg: [-515.362 -515.362 -515.362] (0.9950) ({r_i: None, r_t: [-1114.412 -1114.412 -1114.412], eps: 0.995})
Step:    8400, Reward: [-510.591 -510.591 -510.591] [45.364], Avg: [-515.306 -515.306 -515.306] (0.9950) ({r_i: None, r_t: [-933.078 -933.078 -933.078], eps: 0.995})
Step:    8500, Reward: [-482.104 -482.104 -482.104] [27.552], Avg: [-514.920 -514.920 -514.920] (0.9950) ({r_i: None, r_t: [-955.567 -955.567 -955.567], eps: 0.995})
Step:    8600, Reward: [-490.318 -490.318 -490.318] [78.496], Avg: [-514.637 -514.637 -514.637] (0.9950) ({r_i: None, r_t: [-1085.626 -1085.626 -1085.626], eps: 0.995})
Step:    8700, Reward: [-441.617 -441.617 -441.617] [74.418], Avg: [-513.807 -513.807 -513.807] (0.9950) ({r_i: None, r_t: [-996.220 -996.220 -996.220], eps: 0.995})
Step:    8800, Reward: [-477.915 -477.915 -477.915] [112.377], Avg: [-513.404 -513.404 -513.404] (0.9950) ({r_i: None, r_t: [-1103.778 -1103.778 -1103.778], eps: 0.995})
Step:    8900, Reward: [-434.645 -434.645 -434.645] [80.071], Avg: [-512.529 -512.529 -512.529] (0.9950) ({r_i: None, r_t: [-951.847 -951.847 -951.847], eps: 0.995})
Step:    9000, Reward: [-492.488 -492.488 -492.488] [75.612], Avg: [-512.309 -512.309 -512.309] (0.9950) ({r_i: None, r_t: [-937.747 -937.747 -937.747], eps: 0.995})
Step:    9100, Reward: [-507.582 -507.582 -507.582] [101.707], Avg: [-512.257 -512.257 -512.257] (0.9950) ({r_i: None, r_t: [-1053.406 -1053.406 -1053.406], eps: 0.995})
Step:    9200, Reward: [-552.831 -552.831 -552.831] [93.959], Avg: [-512.694 -512.694 -512.694] (0.9950) ({r_i: None, r_t: [-958.544 -958.544 -958.544], eps: 0.995})
Step:    9300, Reward: [-476.540 -476.540 -476.540] [70.406], Avg: [-512.309 -512.309 -512.309] (0.9950) ({r_i: None, r_t: [-1011.929 -1011.929 -1011.929], eps: 0.995})
Step:    9400, Reward: [-564.011 -564.011 -564.011] [43.691], Avg: [-512.853 -512.853 -512.853] (0.9950) ({r_i: None, r_t: [-1003.717 -1003.717 -1003.717], eps: 0.995})
Step:    9500, Reward: [-503.250 -503.250 -503.250] [82.406], Avg: [-512.753 -512.753 -512.753] (0.9950) ({r_i: None, r_t: [-1034.384 -1034.384 -1034.384], eps: 0.995})
Step:    9600, Reward: [-514.402 -514.402 -514.402] [49.596], Avg: [-512.770 -512.770 -512.770] (0.9950) ({r_i: None, r_t: [-1184.977 -1184.977 -1184.977], eps: 0.995})
Step:    9700, Reward: [-674.125 -674.125 -674.125] [227.062], Avg: [-514.417 -514.417 -514.417] (0.9950) ({r_i: None, r_t: [-1055.710 -1055.710 -1055.710], eps: 0.995})
Step:    9800, Reward: [-546.404 -546.404 -546.404] [66.228], Avg: [-514.740 -514.740 -514.740] (0.9950) ({r_i: None, r_t: [-998.956 -998.956 -998.956], eps: 0.995})
Step:    9900, Reward: [-508.229 -508.229 -508.229] [53.921], Avg: [-514.675 -514.675 -514.675] (0.9950) ({r_i: None, r_t: [-1011.207 -1011.207 -1011.207], eps: 0.995})
Step:   10000, Reward: [-533.441 -533.441 -533.441] [81.580], Avg: [-514.860 -514.860 -514.860] (0.9950) ({r_i: None, r_t: [-1089.332 -1089.332 -1089.332], eps: 0.995})
Step:   10100, Reward: [-516.676 -516.676 -516.676] [45.805], Avg: [-514.878 -514.878 -514.878] (0.9950) ({r_i: None, r_t: [-1152.359 -1152.359 -1152.359], eps: 0.995})
Step:   10200, Reward: [-380.347 -380.347 -380.347] [33.213], Avg: [-513.572 -513.572 -513.572] (0.9950) ({r_i: None, r_t: [-988.193 -988.193 -988.193], eps: 0.995})
Step:   10300, Reward: [-463.670 -463.670 -463.670] [84.390], Avg: [-513.092 -513.092 -513.092] (0.9950) ({r_i: None, r_t: [-959.702 -959.702 -959.702], eps: 0.995})
Step:   10400, Reward: [-475.997 -475.997 -475.997] [68.079], Avg: [-512.739 -512.739 -512.739] (0.9950) ({r_i: None, r_t: [-1007.716 -1007.716 -1007.716], eps: 0.995})
Step:   10500, Reward: [-560.852 -560.852 -560.852] [106.303], Avg: [-513.193 -513.193 -513.193] (0.9950) ({r_i: None, r_t: [-1025.851 -1025.851 -1025.851], eps: 0.995})
Step:   10600, Reward: [-455.885 -455.885 -455.885] [52.815], Avg: [-512.657 -512.657 -512.657] (0.9950) ({r_i: None, r_t: [-946.122 -946.122 -946.122], eps: 0.995})
Step:   10700, Reward: [-520.979 -520.979 -520.979] [127.919], Avg: [-512.734 -512.734 -512.734] (0.9950) ({r_i: None, r_t: [-1060.492 -1060.492 -1060.492], eps: 0.995})
Step:   10800, Reward: [-516.832 -516.832 -516.832] [54.612], Avg: [-512.772 -512.772 -512.772] (0.9950) ({r_i: None, r_t: [-1071.100 -1071.100 -1071.100], eps: 0.995})
Step:   10900, Reward: [-481.540 -481.540 -481.540] [73.864], Avg: [-512.488 -512.488 -512.488] (0.9950) ({r_i: None, r_t: [-1020.439 -1020.439 -1020.439], eps: 0.995})
Step:   11000, Reward: [-576.104 -576.104 -576.104] [105.341], Avg: [-513.061 -513.061 -513.061] (0.9950) ({r_i: None, r_t: [-932.623 -932.623 -932.623], eps: 0.995})
Step:   11100, Reward: [-539.987 -539.987 -539.987] [110.682], Avg: [-513.302 -513.302 -513.302] (0.9950) ({r_i: None, r_t: [-1070.636 -1070.636 -1070.636], eps: 0.995})
Step:   11200, Reward: [-462.090 -462.090 -462.090] [26.904], Avg: [-512.848 -512.848 -512.848] (0.9950) ({r_i: None, r_t: [-995.845 -995.845 -995.845], eps: 0.995})
Step:   11300, Reward: [-545.824 -545.824 -545.824] [149.360], Avg: [-513.138 -513.138 -513.138] (0.9950) ({r_i: None, r_t: [-1022.007 -1022.007 -1022.007], eps: 0.995})
Step:   11400, Reward: [-478.287 -478.287 -478.287] [56.508], Avg: [-512.835 -512.835 -512.835] (0.9950) ({r_i: None, r_t: [-1117.109 -1117.109 -1117.109], eps: 0.995})
Step:   11500, Reward: [-438.710 -438.710 -438.710] [105.032], Avg: [-512.196 -512.196 -512.196] (0.9950) ({r_i: None, r_t: [-1091.487 -1091.487 -1091.487], eps: 0.995})
Step:   11600, Reward: [-523.919 -523.919 -523.919] [117.764], Avg: [-512.296 -512.296 -512.296] (0.9950) ({r_i: None, r_t: [-1057.580 -1057.580 -1057.580], eps: 0.995})
Step:   11700, Reward: [-499.987 -499.987 -499.987] [39.483], Avg: [-512.191 -512.191 -512.191] (0.9950) ({r_i: None, r_t: [-1005.600 -1005.600 -1005.600], eps: 0.995})
Step:   11800, Reward: [-442.234 -442.234 -442.234] [47.814], Avg: [-511.604 -511.604 -511.604] (0.9950) ({r_i: None, r_t: [-948.897 -948.897 -948.897], eps: 0.995})
Step:   11900, Reward: [-480.809 -480.809 -480.809] [66.886], Avg: [-511.347 -511.347 -511.347] (0.9950) ({r_i: None, r_t: [-1027.601 -1027.601 -1027.601], eps: 0.995})
Step:   12000, Reward: [-510.402 -510.402 -510.402] [70.181], Avg: [-511.339 -511.339 -511.339] (0.9950) ({r_i: None, r_t: [-1062.771 -1062.771 -1062.771], eps: 0.995})
Step:   12100, Reward: [-485.702 -485.702 -485.702] [88.858], Avg: [-511.129 -511.129 -511.129] (0.9950) ({r_i: None, r_t: [-1075.087 -1075.087 -1075.087], eps: 0.995})
Step:   12200, Reward: [-584.409 -584.409 -584.409] [172.917], Avg: [-511.725 -511.725 -511.725] (0.9950) ({r_i: None, r_t: [-975.704 -975.704 -975.704], eps: 0.995})
Step:   12300, Reward: [-504.473 -504.473 -504.473] [70.827], Avg: [-511.666 -511.666 -511.666] (0.9950) ({r_i: None, r_t: [-959.287 -959.287 -959.287], eps: 0.995})
Step:   12400, Reward: [-475.535 -475.535 -475.535] [52.966], Avg: [-511.377 -511.377 -511.377] (0.9950) ({r_i: None, r_t: [-969.109 -969.109 -969.109], eps: 0.995})
Step:   12500, Reward: [-471.126 -471.126 -471.126] [24.793], Avg: [-511.058 -511.058 -511.058] (0.9950) ({r_i: None, r_t: [-832.058 -832.058 -832.058], eps: 0.995})
Step:   12600, Reward: [-492.586 -492.586 -492.586] [117.715], Avg: [-510.912 -510.912 -510.912] (0.9950) ({r_i: None, r_t: [-994.259 -994.259 -994.259], eps: 0.995})
Step:   12700, Reward: [-535.921 -535.921 -535.921] [149.246], Avg: [-511.108 -511.108 -511.108] (0.9950) ({r_i: None, r_t: [-1008.001 -1008.001 -1008.001], eps: 0.995})
Step:   12800, Reward: [-479.866 -479.866 -479.866] [55.928], Avg: [-510.866 -510.866 -510.866] (0.9950) ({r_i: None, r_t: [-978.916 -978.916 -978.916], eps: 0.995})
Step:   12900, Reward: [-557.835 -557.835 -557.835] [86.106], Avg: [-511.227 -511.227 -511.227] (0.9950) ({r_i: None, r_t: [-1018.409 -1018.409 -1018.409], eps: 0.995})
Step:   13000, Reward: [-445.395 -445.395 -445.395] [68.121], Avg: [-510.724 -510.724 -510.724] (0.9950) ({r_i: None, r_t: [-982.326 -982.326 -982.326], eps: 0.995})
Step:   13100, Reward: [-503.451 -503.451 -503.451] [80.657], Avg: [-510.669 -510.669 -510.669] (0.9950) ({r_i: None, r_t: [-1079.161 -1079.161 -1079.161], eps: 0.995})
Step:   13200, Reward: [-459.455 -459.455 -459.455] [51.153], Avg: [-510.284 -510.284 -510.284] (0.9950) ({r_i: None, r_t: [-981.757 -981.757 -981.757], eps: 0.995})
Step:   13300, Reward: [-477.779 -477.779 -477.779] [15.416], Avg: [-510.042 -510.042 -510.042] (0.9950) ({r_i: None, r_t: [-1005.298 -1005.298 -1005.298], eps: 0.995})
Step:   13400, Reward: [-485.974 -485.974 -485.974] [111.179], Avg: [-509.863 -509.863 -509.863] (0.9950) ({r_i: None, r_t: [-1039.470 -1039.470 -1039.470], eps: 0.995})
Step:   13500, Reward: [-545.192 -545.192 -545.192] [80.075], Avg: [-510.123 -510.123 -510.123] (0.9950) ({r_i: None, r_t: [-975.689 -975.689 -975.689], eps: 0.995})
Step:   13600, Reward: [-578.856 -578.856 -578.856] [114.697], Avg: [-510.625 -510.625 -510.625] (0.9950) ({r_i: None, r_t: [-1118.355 -1118.355 -1118.355], eps: 0.995})
Step:   13700, Reward: [-538.918 -538.918 -538.918] [79.461], Avg: [-510.830 -510.830 -510.830] (0.9950) ({r_i: None, r_t: [-1040.571 -1040.571 -1040.571], eps: 0.995})
Step:   13800, Reward: [-425.717 -425.717 -425.717] [73.251], Avg: [-510.217 -510.217 -510.217] (0.9950) ({r_i: None, r_t: [-1001.916 -1001.916 -1001.916], eps: 0.995})
Step:   13900, Reward: [-582.594 -582.594 -582.594] [130.355], Avg: [-510.734 -510.734 -510.734] (0.9950) ({r_i: None, r_t: [-1039.903 -1039.903 -1039.903], eps: 0.995})
Step:   14000, Reward: [-519.469 -519.469 -519.469] [86.535], Avg: [-510.796 -510.796 -510.796] (0.9950) ({r_i: None, r_t: [-1013.811 -1013.811 -1013.811], eps: 0.995})
Step:   14100, Reward: [-461.509 -461.509 -461.509] [40.752], Avg: [-510.449 -510.449 -510.449] (0.9950) ({r_i: None, r_t: [-981.750 -981.750 -981.750], eps: 0.995})
Step:   14200, Reward: [-525.715 -525.715 -525.715] [57.421], Avg: [-510.556 -510.556 -510.556] (0.9950) ({r_i: None, r_t: [-1025.047 -1025.047 -1025.047], eps: 0.995})
Step:   14300, Reward: [-520.379 -520.379 -520.379] [45.028], Avg: [-510.624 -510.624 -510.624] (0.9950) ({r_i: None, r_t: [-987.207 -987.207 -987.207], eps: 0.995})
Step:   14400, Reward: [-515.695 -515.695 -515.695] [90.577], Avg: [-510.659 -510.659 -510.659] (0.9950) ({r_i: None, r_t: [-1226.953 -1226.953 -1226.953], eps: 0.995})
Step:   14500, Reward: [-508.417 -508.417 -508.417] [94.932], Avg: [-510.644 -510.644 -510.644] (0.9950) ({r_i: None, r_t: [-1097.693 -1097.693 -1097.693], eps: 0.995})
Step:   14600, Reward: [-494.796 -494.796 -494.796] [66.739], Avg: [-510.536 -510.536 -510.536] (0.9950) ({r_i: None, r_t: [-881.768 -881.768 -881.768], eps: 0.995})
Step:   14700, Reward: [-471.954 -471.954 -471.954] [83.503], Avg: [-510.275 -510.275 -510.275] (0.9950) ({r_i: None, r_t: [-912.740 -912.740 -912.740], eps: 0.995})
Step:   14800, Reward: [-479.844 -479.844 -479.844] [48.525], Avg: [-510.071 -510.071 -510.071] (0.9950) ({r_i: None, r_t: [-1095.811 -1095.811 -1095.811], eps: 0.995})
Step:   14900, Reward: [-434.076 -434.076 -434.076] [59.597], Avg: [-509.564 -509.564 -509.564] (0.9950) ({r_i: None, r_t: [-1028.167 -1028.167 -1028.167], eps: 0.995})
Step:   15000, Reward: [-660.260 -660.260 -660.260] [73.560], Avg: [-510.562 -510.562 -510.562] (0.9950) ({r_i: None, r_t: [-1039.377 -1039.377 -1039.377], eps: 0.995})
Step:   15100, Reward: [-618.637 -618.637 -618.637] [91.354], Avg: [-511.273 -511.273 -511.273] (0.9950) ({r_i: None, r_t: [-1090.724 -1090.724 -1090.724], eps: 0.995})
Step:   15200, Reward: [-632.282 -632.282 -632.282] [105.633], Avg: [-512.064 -512.064 -512.064] (0.9950) ({r_i: None, r_t: [-969.208 -969.208 -969.208], eps: 0.995})
Step:   15300, Reward: [-425.224 -425.224 -425.224] [34.307], Avg: [-511.501 -511.501 -511.501] (0.9950) ({r_i: None, r_t: [-1053.740 -1053.740 -1053.740], eps: 0.995})
Step:   15400, Reward: [-554.329 -554.329 -554.329] [49.466], Avg: [-511.777 -511.777 -511.777] (0.9950) ({r_i: None, r_t: [-961.973 -961.973 -961.973], eps: 0.995})
Step:   15500, Reward: [-453.789 -453.789 -453.789] [31.868], Avg: [-511.405 -511.405 -511.405] (0.9950) ({r_i: None, r_t: [-963.785 -963.785 -963.785], eps: 0.995})
Step:   15600, Reward: [-406.116 -406.116 -406.116] [48.055], Avg: [-510.734 -510.734 -510.734] (0.9950) ({r_i: None, r_t: [-1013.183 -1013.183 -1013.183], eps: 0.995})
Step:   15700, Reward: [-562.553 -562.553 -562.553] [133.250], Avg: [-511.062 -511.062 -511.062] (0.9950) ({r_i: None, r_t: [-928.250 -928.250 -928.250], eps: 0.995})
Step:   15800, Reward: [-498.624 -498.624 -498.624] [45.437], Avg: [-510.984 -510.984 -510.984] (0.9950) ({r_i: None, r_t: [-1055.751 -1055.751 -1055.751], eps: 0.995})
Step:   15900, Reward: [-443.671 -443.671 -443.671] [72.155], Avg: [-510.563 -510.563 -510.563] (0.9950) ({r_i: None, r_t: [-1068.830 -1068.830 -1068.830], eps: 0.995})
Step:   16000, Reward: [-487.561 -487.561 -487.561] [58.856], Avg: [-510.421 -510.421 -510.421] (0.9950) ({r_i: None, r_t: [-1121.194 -1121.194 -1121.194], eps: 0.995})
Step:   16100, Reward: [-549.391 -549.391 -549.391] [174.880], Avg: [-510.661 -510.661 -510.661] (0.9950) ({r_i: None, r_t: [-1028.479 -1028.479 -1028.479], eps: 0.995})
Step:   16200, Reward: [-553.971 -553.971 -553.971] [118.614], Avg: [-510.927 -510.927 -510.927] (0.9950) ({r_i: None, r_t: [-953.778 -953.778 -953.778], eps: 0.995})
Step:   16300, Reward: [-482.577 -482.577 -482.577] [83.215], Avg: [-510.754 -510.754 -510.754] (0.9950) ({r_i: None, r_t: [-975.527 -975.527 -975.527], eps: 0.995})
Step:   16400, Reward: [-475.781 -475.781 -475.781] [56.314], Avg: [-510.542 -510.542 -510.542] (0.9950) ({r_i: None, r_t: [-1050.456 -1050.456 -1050.456], eps: 0.995})
Step:   16500, Reward: [-468.549 -468.549 -468.549] [48.657], Avg: [-510.289 -510.289 -510.289] (0.9950) ({r_i: None, r_t: [-961.877 -961.877 -961.877], eps: 0.995})
Step:   16600, Reward: [-547.953 -547.953 -547.953] [84.252], Avg: [-510.515 -510.515 -510.515] (0.9950) ({r_i: None, r_t: [-1058.962 -1058.962 -1058.962], eps: 0.995})
Step:   16700, Reward: [-516.377 -516.377 -516.377] [82.207], Avg: [-510.550 -510.550 -510.550] (0.9950) ({r_i: None, r_t: [-1057.986 -1057.986 -1057.986], eps: 0.995})
Step:   16800, Reward: [-437.831 -437.831 -437.831] [50.311], Avg: [-510.119 -510.119 -510.119] (0.9950) ({r_i: None, r_t: [-1039.380 -1039.380 -1039.380], eps: 0.995})
Step:   16900, Reward: [-571.462 -571.462 -571.462] [97.411], Avg: [-510.480 -510.480 -510.480] (0.9950) ({r_i: None, r_t: [-995.305 -995.305 -995.305], eps: 0.995})
Step:   17000, Reward: [-501.313 -501.313 -501.313] [74.092], Avg: [-510.426 -510.426 -510.426] (0.9950) ({r_i: None, r_t: [-938.425 -938.425 -938.425], eps: 0.995})
Step:   17100, Reward: [-557.448 -557.448 -557.448] [185.573], Avg: [-510.700 -510.700 -510.700] (0.9950) ({r_i: None, r_t: [-1064.097 -1064.097 -1064.097], eps: 0.995})
Step:   17200, Reward: [-518.970 -518.970 -518.970] [100.893], Avg: [-510.748 -510.748 -510.748] (0.9950) ({r_i: None, r_t: [-893.654 -893.654 -893.654], eps: 0.995})
Step:   17300, Reward: [-520.520 -520.520 -520.520] [110.349], Avg: [-510.804 -510.804 -510.804] (0.9950) ({r_i: None, r_t: [-1031.287 -1031.287 -1031.287], eps: 0.995})
Step:   17400, Reward: [-495.414 -495.414 -495.414] [82.410], Avg: [-510.716 -510.716 -510.716] (0.9950) ({r_i: None, r_t: [-974.567 -974.567 -974.567], eps: 0.995})
Step:   17500, Reward: [-533.034 -533.034 -533.034] [89.624], Avg: [-510.843 -510.843 -510.843] (0.9950) ({r_i: None, r_t: [-1012.240 -1012.240 -1012.240], eps: 0.995})
Step:   17600, Reward: [-442.178 -442.178 -442.178] [57.581], Avg: [-510.455 -510.455 -510.455] (0.9950) ({r_i: None, r_t: [-943.134 -943.134 -943.134], eps: 0.995})
Step:   17700, Reward: [-523.990 -523.990 -523.990] [17.162], Avg: [-510.531 -510.531 -510.531] (0.9950) ({r_i: None, r_t: [-899.643 -899.643 -899.643], eps: 0.995})
Step:   17800, Reward: [-515.241 -515.241 -515.241] [114.642], Avg: [-510.557 -510.557 -510.557] (0.9950) ({r_i: None, r_t: [-1015.774 -1015.774 -1015.774], eps: 0.995})
Step:   17900, Reward: [-464.687 -464.687 -464.687] [62.524], Avg: [-510.302 -510.302 -510.302] (0.9950) ({r_i: None, r_t: [-947.081 -947.081 -947.081], eps: 0.995})
Step:   18000, Reward: [-397.335 -397.335 -397.335] [55.098], Avg: [-509.678 -509.678 -509.678] (0.9950) ({r_i: None, r_t: [-967.094 -967.094 -967.094], eps: 0.995})
Step:   18100, Reward: [-530.689 -530.689 -530.689] [78.201], Avg: [-509.794 -509.794 -509.794] (0.9950) ({r_i: None, r_t: [-978.833 -978.833 -978.833], eps: 0.995})
Step:   18200, Reward: [-469.379 -469.379 -469.379] [83.786], Avg: [-509.573 -509.573 -509.573] (0.9950) ({r_i: None, r_t: [-983.840 -983.840 -983.840], eps: 0.995})
Step:   18300, Reward: [-508.215 -508.215 -508.215] [30.116], Avg: [-509.565 -509.565 -509.565] (0.9950) ({r_i: None, r_t: [-1028.702 -1028.702 -1028.702], eps: 0.995})
Step:   18400, Reward: [-517.005 -517.005 -517.005] [63.839], Avg: [-509.606 -509.606 -509.606] (0.9950) ({r_i: None, r_t: [-1037.263 -1037.263 -1037.263], eps: 0.995})
Step:   18500, Reward: [-500.092 -500.092 -500.092] [81.474], Avg: [-509.554 -509.554 -509.554] (0.9950) ({r_i: None, r_t: [-1042.291 -1042.291 -1042.291], eps: 0.995})
Step:   18600, Reward: [-458.199 -458.199 -458.199] [55.371], Avg: [-509.280 -509.280 -509.280] (0.9950) ({r_i: None, r_t: [-928.120 -928.120 -928.120], eps: 0.995})
Step:   18700, Reward: [-454.462 -454.462 -454.462] [28.036], Avg: [-508.988 -508.988 -508.988] (0.9950) ({r_i: None, r_t: [-961.448 -961.448 -961.448], eps: 0.995})
Step:   18800, Reward: [-490.464 -490.464 -490.464] [1.443], Avg: [-508.890 -508.890 -508.890] (0.9950) ({r_i: None, r_t: [-939.157 -939.157 -939.157], eps: 0.995})
Step:   18900, Reward: [-447.625 -447.625 -447.625] [113.801], Avg: [-508.568 -508.568 -508.568] (0.9950) ({r_i: None, r_t: [-973.372 -973.372 -973.372], eps: 0.995})
Step:   19000, Reward: [-436.718 -436.718 -436.718] [96.003], Avg: [-508.192 -508.192 -508.192] (0.9950) ({r_i: None, r_t: [-1056.981 -1056.981 -1056.981], eps: 0.995})
Step:   19100, Reward: [-510.926 -510.926 -510.926] [97.030], Avg: [-508.206 -508.206 -508.206] (0.9950) ({r_i: None, r_t: [-972.255 -972.255 -972.255], eps: 0.995})
Step:   19200, Reward: [-655.714 -655.714 -655.714] [118.105], Avg: [-508.970 -508.970 -508.970] (0.9950) ({r_i: None, r_t: [-975.701 -975.701 -975.701], eps: 0.995})
Step:   19300, Reward: [-475.745 -475.745 -475.745] [21.085], Avg: [-508.799 -508.799 -508.799] (0.9950) ({r_i: None, r_t: [-941.427 -941.427 -941.427], eps: 0.995})
Step:   19400, Reward: [-587.147 -587.147 -587.147] [161.634], Avg: [-509.201 -509.201 -509.201] (0.9950) ({r_i: None, r_t: [-894.595 -894.595 -894.595], eps: 0.995})
Step:   19500, Reward: [-512.171 -512.171 -512.171] [131.445], Avg: [-509.216 -509.216 -509.216] (0.9950) ({r_i: None, r_t: [-938.260 -938.260 -938.260], eps: 0.995})
Step:   19600, Reward: [-469.025 -469.025 -469.025] [64.717], Avg: [-509.012 -509.012 -509.012] (0.9950) ({r_i: None, r_t: [-1156.387 -1156.387 -1156.387], eps: 0.995})
Step:   19700, Reward: [-470.750 -470.750 -470.750] [92.422], Avg: [-508.819 -508.819 -508.819] (0.9950) ({r_i: None, r_t: [-896.792 -896.792 -896.792], eps: 0.995})
Step:   19800, Reward: [-530.638 -530.638 -530.638] [150.981], Avg: [-508.928 -508.928 -508.928] (0.9950) ({r_i: None, r_t: [-995.242 -995.242 -995.242], eps: 0.995})
Step:   19900, Reward: [-516.239 -516.239 -516.239] [55.509], Avg: [-508.965 -508.965 -508.965] (0.9950) ({r_i: None, r_t: [-903.275 -903.275 -903.275], eps: 0.995})
Step:   20000, Reward: [-464.615 -464.615 -464.615] [68.364], Avg: [-508.744 -508.744 -508.744] (0.9950) ({r_i: None, r_t: [-1004.844 -1004.844 -1004.844], eps: 0.995})
Step:   20100, Reward: [-492.544 -492.544 -492.544] [140.983], Avg: [-508.664 -508.664 -508.664] (0.9900) ({r_i: None, r_t: [-910.732 -910.732 -910.732], eps: 0.99})
Step:   20200, Reward: [-502.421 -502.421 -502.421] [98.799], Avg: [-508.633 -508.633 -508.633] (0.9900) ({r_i: None, r_t: [-1073.281 -1073.281 -1073.281], eps: 0.99})
Step:   20300, Reward: [-457.214 -457.214 -457.214] [75.909], Avg: [-508.381 -508.381 -508.381] (0.9900) ({r_i: None, r_t: [-1001.209 -1001.209 -1001.209], eps: 0.99})
Step:   20400, Reward: [-586.477 -586.477 -586.477] [125.876], Avg: [-508.762 -508.762 -508.762] (0.9900) ({r_i: None, r_t: [-923.049 -923.049 -923.049], eps: 0.99})
Step:   20500, Reward: [-538.323 -538.323 -538.323] [93.837], Avg: [-508.906 -508.906 -508.906] (0.9900) ({r_i: None, r_t: [-816.646 -816.646 -816.646], eps: 0.99})
Step:   20600, Reward: [-498.090 -498.090 -498.090] [57.111], Avg: [-508.853 -508.853 -508.853] (0.9900) ({r_i: None, r_t: [-992.953 -992.953 -992.953], eps: 0.99})
Step:   20700, Reward: [-450.779 -450.779 -450.779] [24.407], Avg: [-508.574 -508.574 -508.574] (0.9900) ({r_i: None, r_t: [-1010.338 -1010.338 -1010.338], eps: 0.99})
Step:   20800, Reward: [-527.197 -527.197 -527.197] [74.042], Avg: [-508.663 -508.663 -508.663] (0.9900) ({r_i: None, r_t: [-1047.346 -1047.346 -1047.346], eps: 0.99})
Step:   20900, Reward: [-475.417 -475.417 -475.417] [56.812], Avg: [-508.505 -508.505 -508.505] (0.9900) ({r_i: None, r_t: [-996.739 -996.739 -996.739], eps: 0.99})
Step:   21000, Reward: [-472.981 -472.981 -472.981] [83.105], Avg: [-508.337 -508.337 -508.337] (0.9900) ({r_i: None, r_t: [-845.757 -845.757 -845.757], eps: 0.99})
Step:   21100, Reward: [-460.674 -460.674 -460.674] [54.692], Avg: [-508.112 -508.112 -508.112] (0.9900) ({r_i: None, r_t: [-1052.006 -1052.006 -1052.006], eps: 0.99})
Step:   21200, Reward: [-446.639 -446.639 -446.639] [46.720], Avg: [-507.823 -507.823 -507.823] (0.9900) ({r_i: None, r_t: [-1047.707 -1047.707 -1047.707], eps: 0.99})
Step:   21300, Reward: [-449.113 -449.113 -449.113] [19.141], Avg: [-507.549 -507.549 -507.549] (0.9900) ({r_i: None, r_t: [-899.783 -899.783 -899.783], eps: 0.99})
Step:   21400, Reward: [-413.308 -413.308 -413.308] [77.370], Avg: [-507.110 -507.110 -507.110] (0.9900) ({r_i: None, r_t: [-1114.798 -1114.798 -1114.798], eps: 0.99})
Step:   21500, Reward: [-446.576 -446.576 -446.576] [59.782], Avg: [-506.830 -506.830 -506.830] (0.9900) ({r_i: None, r_t: [-1025.024 -1025.024 -1025.024], eps: 0.99})
Step:   21600, Reward: [-493.365 -493.365 -493.365] [61.267], Avg: [-506.768 -506.768 -506.768] (0.9900) ({r_i: None, r_t: [-1114.784 -1114.784 -1114.784], eps: 0.99})
Step:   21700, Reward: [-419.411 -419.411 -419.411] [17.805], Avg: [-506.367 -506.367 -506.367] (0.9900) ({r_i: None, r_t: [-895.900 -895.900 -895.900], eps: 0.99})
Step:   21800, Reward: [-440.263 -440.263 -440.263] [18.877], Avg: [-506.066 -506.066 -506.066] (0.9900) ({r_i: None, r_t: [-860.304 -860.304 -860.304], eps: 0.99})
Step:   21900, Reward: [-505.414 -505.414 -505.414] [64.087], Avg: [-506.063 -506.063 -506.063] (0.9900) ({r_i: None, r_t: [-985.041 -985.041 -985.041], eps: 0.99})
Step:   22000, Reward: [-431.359 -431.359 -431.359] [86.804], Avg: [-505.725 -505.725 -505.725] (0.9900) ({r_i: None, r_t: [-931.677 -931.677 -931.677], eps: 0.99})
Step:   22100, Reward: [-425.463 -425.463 -425.463] [48.541], Avg: [-505.363 -505.363 -505.363] (0.9900) ({r_i: None, r_t: [-981.220 -981.220 -981.220], eps: 0.99})
Step:   22200, Reward: [-489.278 -489.278 -489.278] [125.148], Avg: [-505.291 -505.291 -505.291] (0.9900) ({r_i: None, r_t: [-1018.683 -1018.683 -1018.683], eps: 0.99})
Step:   22300, Reward: [-497.413 -497.413 -497.413] [19.425], Avg: [-505.256 -505.256 -505.256] (0.9900) ({r_i: None, r_t: [-1046.233 -1046.233 -1046.233], eps: 0.99})
Step:   22400, Reward: [-590.233 -590.233 -590.233] [86.968], Avg: [-505.633 -505.633 -505.633] (0.9900) ({r_i: None, r_t: [-993.503 -993.503 -993.503], eps: 0.99})
Step:   22500, Reward: [-499.478 -499.478 -499.478] [45.145], Avg: [-505.606 -505.606 -505.606] (0.9900) ({r_i: None, r_t: [-903.516 -903.516 -903.516], eps: 0.99})
Step:   22600, Reward: [-476.501 -476.501 -476.501] [70.237], Avg: [-505.478 -505.478 -505.478] (0.9900) ({r_i: None, r_t: [-987.628 -987.628 -987.628], eps: 0.99})
Step:   22700, Reward: [-595.695 -595.695 -595.695] [220.709], Avg: [-505.874 -505.874 -505.874] (0.9900) ({r_i: None, r_t: [-1021.050 -1021.050 -1021.050], eps: 0.99})
Step:   22800, Reward: [-519.815 -519.815 -519.815] [75.613], Avg: [-505.935 -505.935 -505.935] (0.9900) ({r_i: None, r_t: [-970.954 -970.954 -970.954], eps: 0.99})
Step:   22900, Reward: [-501.340 -501.340 -501.340] [55.965], Avg: [-505.915 -505.915 -505.915] (0.9900) ({r_i: None, r_t: [-1042.688 -1042.688 -1042.688], eps: 0.99})
Step:   23000, Reward: [-423.786 -423.786 -423.786] [55.861], Avg: [-505.559 -505.559 -505.559] (0.9900) ({r_i: None, r_t: [-995.675 -995.675 -995.675], eps: 0.99})
Step:   23100, Reward: [-512.169 -512.169 -512.169] [80.274], Avg: [-505.587 -505.587 -505.587] (0.9900) ({r_i: None, r_t: [-971.050 -971.050 -971.050], eps: 0.99})
Step:   23200, Reward: [-496.286 -496.286 -496.286] [131.645], Avg: [-505.548 -505.548 -505.548] (0.9900) ({r_i: None, r_t: [-1085.712 -1085.712 -1085.712], eps: 0.99})
Step:   23300, Reward: [-648.982 -648.982 -648.982] [72.187], Avg: [-506.161 -506.161 -506.161] (0.9900) ({r_i: None, r_t: [-991.637 -991.637 -991.637], eps: 0.99})
Step:   23400, Reward: [-515.717 -515.717 -515.717] [59.369], Avg: [-506.201 -506.201 -506.201] (0.9900) ({r_i: None, r_t: [-991.725 -991.725 -991.725], eps: 0.99})
Step:   23500, Reward: [-555.998 -555.998 -555.998] [66.137], Avg: [-506.412 -506.412 -506.412] (0.9900) ({r_i: None, r_t: [-966.342 -966.342 -966.342], eps: 0.99})
Step:   23600, Reward: [-575.204 -575.204 -575.204] [78.608], Avg: [-506.702 -506.702 -506.702] (0.9900) ({r_i: None, r_t: [-1067.523 -1067.523 -1067.523], eps: 0.99})
Step:   23700, Reward: [-537.885 -537.885 -537.885] [34.303], Avg: [-506.833 -506.833 -506.833] (0.9900) ({r_i: None, r_t: [-1041.978 -1041.978 -1041.978], eps: 0.99})
Step:   23800, Reward: [-486.849 -486.849 -486.849] [62.216], Avg: [-506.750 -506.750 -506.750] (0.9900) ({r_i: None, r_t: [-988.662 -988.662 -988.662], eps: 0.99})
Step:   23900, Reward: [-487.163 -487.163 -487.163] [69.018], Avg: [-506.668 -506.668 -506.668] (0.9900) ({r_i: None, r_t: [-1027.047 -1027.047 -1027.047], eps: 0.99})
Step:   24000, Reward: [-443.793 -443.793 -443.793] [76.009], Avg: [-506.407 -506.407 -506.407] (0.9900) ({r_i: None, r_t: [-1099.705 -1099.705 -1099.705], eps: 0.99})
Step:   24100, Reward: [-519.193 -519.193 -519.193] [87.803], Avg: [-506.460 -506.460 -506.460] (0.9900) ({r_i: None, r_t: [-991.812 -991.812 -991.812], eps: 0.99})
Step:   24200, Reward: [-551.360 -551.360 -551.360] [165.493], Avg: [-506.645 -506.645 -506.645] (0.9900) ({r_i: None, r_t: [-1047.764 -1047.764 -1047.764], eps: 0.99})
Step:   24300, Reward: [-445.892 -445.892 -445.892] [91.373], Avg: [-506.396 -506.396 -506.396] (0.9900) ({r_i: None, r_t: [-972.544 -972.544 -972.544], eps: 0.99})
Step:   24400, Reward: [-591.605 -591.605 -591.605] [73.883], Avg: [-506.744 -506.744 -506.744] (0.9900) ({r_i: None, r_t: [-989.811 -989.811 -989.811], eps: 0.99})
Step:   24500, Reward: [-471.968 -471.968 -471.968] [100.401], Avg: [-506.602 -506.602 -506.602] (0.9900) ({r_i: None, r_t: [-813.981 -813.981 -813.981], eps: 0.99})
Step:   24600, Reward: [-672.195 -672.195 -672.195] [193.644], Avg: [-507.273 -507.273 -507.273] (0.9900) ({r_i: None, r_t: [-948.548 -948.548 -948.548], eps: 0.99})
Step:   24700, Reward: [-431.163 -431.163 -431.163] [51.813], Avg: [-506.966 -506.966 -506.966] (0.9900) ({r_i: None, r_t: [-968.916 -968.916 -968.916], eps: 0.99})
Step:   24800, Reward: [-514.213 -514.213 -514.213] [118.850], Avg: [-506.995 -506.995 -506.995] (0.9900) ({r_i: None, r_t: [-1017.730 -1017.730 -1017.730], eps: 0.99})
Step:   24900, Reward: [-508.628 -508.628 -508.628] [59.627], Avg: [-507.002 -507.002 -507.002] (0.9900) ({r_i: None, r_t: [-1120.310 -1120.310 -1120.310], eps: 0.99})
Step:   25000, Reward: [-399.421 -399.421 -399.421] [103.286], Avg: [-506.573 -506.573 -506.573] (0.9900) ({r_i: None, r_t: [-1031.705 -1031.705 -1031.705], eps: 0.99})
Step:   25100, Reward: [-459.215 -459.215 -459.215] [33.857], Avg: [-506.385 -506.385 -506.385] (0.9900) ({r_i: None, r_t: [-945.062 -945.062 -945.062], eps: 0.99})
Step:   25200, Reward: [-542.646 -542.646 -542.646] [61.147], Avg: [-506.528 -506.528 -506.528] (0.9900) ({r_i: None, r_t: [-1096.655 -1096.655 -1096.655], eps: 0.99})
Step:   25300, Reward: [-550.213 -550.213 -550.213] [72.028], Avg: [-506.700 -506.700 -506.700] (0.9900) ({r_i: None, r_t: [-1060.502 -1060.502 -1060.502], eps: 0.99})
Step:   25400, Reward: [-485.472 -485.472 -485.472] [82.962], Avg: [-506.617 -506.617 -506.617] (0.9900) ({r_i: None, r_t: [-978.770 -978.770 -978.770], eps: 0.99})
Step:   25500, Reward: [-536.100 -536.100 -536.100] [73.737], Avg: [-506.732 -506.732 -506.732] (0.9900) ({r_i: None, r_t: [-1113.051 -1113.051 -1113.051], eps: 0.99})
Step:   25600, Reward: [-470.713 -470.713 -470.713] [16.329], Avg: [-506.592 -506.592 -506.592] (0.9900) ({r_i: None, r_t: [-1046.667 -1046.667 -1046.667], eps: 0.99})
Step:   25700, Reward: [-585.856 -585.856 -585.856] [34.308], Avg: [-506.899 -506.899 -506.899] (0.9900) ({r_i: None, r_t: [-928.919 -928.919 -928.919], eps: 0.99})
Step:   25800, Reward: [-513.254 -513.254 -513.254] [69.157], Avg: [-506.924 -506.924 -506.924] (0.9900) ({r_i: None, r_t: [-1044.004 -1044.004 -1044.004], eps: 0.99})
Step:   25900, Reward: [-590.340 -590.340 -590.340] [151.200], Avg: [-507.245 -507.245 -507.245] (0.9900) ({r_i: None, r_t: [-1068.446 -1068.446 -1068.446], eps: 0.99})
Step:   26000, Reward: [-481.115 -481.115 -481.115] [84.370], Avg: [-507.145 -507.145 -507.145] (0.9900) ({r_i: None, r_t: [-983.197 -983.197 -983.197], eps: 0.99})
Step:   26100, Reward: [-468.941 -468.941 -468.941] [45.318], Avg: [-506.999 -506.999 -506.999] (0.9900) ({r_i: None, r_t: [-1013.128 -1013.128 -1013.128], eps: 0.99})
Step:   26200, Reward: [-506.641 -506.641 -506.641] [9.058], Avg: [-506.997 -506.997 -506.997] (0.9900) ({r_i: None, r_t: [-996.821 -996.821 -996.821], eps: 0.99})
Step:   26300, Reward: [-489.061 -489.061 -489.061] [90.953], Avg: [-506.929 -506.929 -506.929] (0.9900) ({r_i: None, r_t: [-1104.819 -1104.819 -1104.819], eps: 0.99})
Step:   26400, Reward: [-448.013 -448.013 -448.013] [160.526], Avg: [-506.707 -506.707 -506.707] (0.9900) ({r_i: None, r_t: [-898.015 -898.015 -898.015], eps: 0.99})
Step:   26500, Reward: [-496.972 -496.972 -496.972] [84.500], Avg: [-506.671 -506.671 -506.671] (0.9900) ({r_i: None, r_t: [-945.812 -945.812 -945.812], eps: 0.99})
Step:   26600, Reward: [-454.456 -454.456 -454.456] [45.444], Avg: [-506.475 -506.475 -506.475] (0.9900) ({r_i: None, r_t: [-1013.026 -1013.026 -1013.026], eps: 0.99})
Step:   26700, Reward: [-437.271 -437.271 -437.271] [73.046], Avg: [-506.217 -506.217 -506.217] (0.9900) ({r_i: None, r_t: [-942.900 -942.900 -942.900], eps: 0.99})
Step:   26800, Reward: [-539.070 -539.070 -539.070] [124.019], Avg: [-506.339 -506.339 -506.339] (0.9900) ({r_i: None, r_t: [-1035.147 -1035.147 -1035.147], eps: 0.99})
Step:   26900, Reward: [-452.790 -452.790 -452.790] [30.714], Avg: [-506.141 -506.141 -506.141] (0.9900) ({r_i: None, r_t: [-992.115 -992.115 -992.115], eps: 0.99})
Step:   27000, Reward: [-547.438 -547.438 -547.438] [46.149], Avg: [-506.293 -506.293 -506.293] (0.9900) ({r_i: None, r_t: [-1153.024 -1153.024 -1153.024], eps: 0.99})
Step:   27100, Reward: [-566.235 -566.235 -566.235] [123.709], Avg: [-506.513 -506.513 -506.513] (0.9900) ({r_i: None, r_t: [-993.117 -993.117 -993.117], eps: 0.99})
Step:   27200, Reward: [-554.143 -554.143 -554.143] [142.248], Avg: [-506.688 -506.688 -506.688] (0.9900) ({r_i: None, r_t: [-1012.426 -1012.426 -1012.426], eps: 0.99})
Step:   27300, Reward: [-485.421 -485.421 -485.421] [51.484], Avg: [-506.610 -506.610 -506.610] (0.9900) ({r_i: None, r_t: [-1080.627 -1080.627 -1080.627], eps: 0.99})
Step:   27400, Reward: [-551.555 -551.555 -551.555] [56.435], Avg: [-506.774 -506.774 -506.774] (0.9900) ({r_i: None, r_t: [-1232.552 -1232.552 -1232.552], eps: 0.99})
Step:   27500, Reward: [-571.824 -571.824 -571.824] [139.211], Avg: [-507.009 -507.009 -507.009] (0.9900) ({r_i: None, r_t: [-1061.019 -1061.019 -1061.019], eps: 0.99})
Step:   27600, Reward: [-487.027 -487.027 -487.027] [115.080], Avg: [-506.937 -506.937 -506.937] (0.9900) ({r_i: None, r_t: [-1013.317 -1013.317 -1013.317], eps: 0.99})
Step:   27700, Reward: [-455.281 -455.281 -455.281] [104.090], Avg: [-506.751 -506.751 -506.751] (0.9900) ({r_i: None, r_t: [-980.116 -980.116 -980.116], eps: 0.99})
Step:   27800, Reward: [-474.373 -474.373 -474.373] [49.260], Avg: [-506.635 -506.635 -506.635] (0.9900) ({r_i: None, r_t: [-989.209 -989.209 -989.209], eps: 0.99})
Step:   27900, Reward: [-448.915 -448.915 -448.915] [27.948], Avg: [-506.429 -506.429 -506.429] (0.9900) ({r_i: None, r_t: [-1047.590 -1047.590 -1047.590], eps: 0.99})
Step:   28000, Reward: [-516.823 -516.823 -516.823] [60.507], Avg: [-506.466 -506.466 -506.466] (0.9900) ({r_i: None, r_t: [-961.393 -961.393 -961.393], eps: 0.99})
Step:   28100, Reward: [-519.438 -519.438 -519.438] [30.320], Avg: [-506.512 -506.512 -506.512] (0.9900) ({r_i: None, r_t: [-1003.577 -1003.577 -1003.577], eps: 0.99})
Step:   28200, Reward: [-482.467 -482.467 -482.467] [54.815], Avg: [-506.427 -506.427 -506.427] (0.9900) ({r_i: None, r_t: [-1011.606 -1011.606 -1011.606], eps: 0.99})
Step:   28300, Reward: [-550.447 -550.447 -550.447] [87.431], Avg: [-506.582 -506.582 -506.582] (0.9900) ({r_i: None, r_t: [-931.352 -931.352 -931.352], eps: 0.99})
Step:   28400, Reward: [-454.612 -454.612 -454.612] [73.488], Avg: [-506.400 -506.400 -506.400] (0.9900) ({r_i: None, r_t: [-1077.745 -1077.745 -1077.745], eps: 0.99})
Step:   28500, Reward: [-482.574 -482.574 -482.574] [110.288], Avg: [-506.317 -506.317 -506.317] (0.9900) ({r_i: None, r_t: [-1021.688 -1021.688 -1021.688], eps: 0.99})
Step:   28600, Reward: [-531.898 -531.898 -531.898] [95.281], Avg: [-506.406 -506.406 -506.406] (0.9900) ({r_i: None, r_t: [-989.252 -989.252 -989.252], eps: 0.99})
Step:   28700, Reward: [-535.894 -535.894 -535.894] [51.575], Avg: [-506.508 -506.508 -506.508] (0.9900) ({r_i: None, r_t: [-1005.876 -1005.876 -1005.876], eps: 0.99})
Step:   28800, Reward: [-506.527 -506.527 -506.527] [139.151], Avg: [-506.508 -506.508 -506.508] (0.9900) ({r_i: None, r_t: [-1078.325 -1078.325 -1078.325], eps: 0.99})
Step:   28900, Reward: [-560.278 -560.278 -560.278] [96.992], Avg: [-506.694 -506.694 -506.694] (0.9900) ({r_i: None, r_t: [-1060.229 -1060.229 -1060.229], eps: 0.99})
Step:   29000, Reward: [-637.600 -637.600 -637.600] [132.606], Avg: [-507.143 -507.143 -507.143] (0.9900) ({r_i: None, r_t: [-1127.631 -1127.631 -1127.631], eps: 0.99})
Step:   29100, Reward: [-443.600 -443.600 -443.600] [47.372], Avg: [-506.926 -506.926 -506.926] (0.9900) ({r_i: None, r_t: [-969.673 -969.673 -969.673], eps: 0.99})
Step:   29200, Reward: [-564.978 -564.978 -564.978] [94.514], Avg: [-507.124 -507.124 -507.124] (0.9900) ({r_i: None, r_t: [-1031.693 -1031.693 -1031.693], eps: 0.99})
Step:   29300, Reward: [-416.997 -416.997 -416.997] [65.244], Avg: [-506.817 -506.817 -506.817] (0.9900) ({r_i: None, r_t: [-1109.581 -1109.581 -1109.581], eps: 0.99})
Step:   29400, Reward: [-490.595 -490.595 -490.595] [65.055], Avg: [-506.762 -506.762 -506.762] (0.9900) ({r_i: None, r_t: [-1031.620 -1031.620 -1031.620], eps: 0.99})
Step:   29500, Reward: [-542.887 -542.887 -542.887] [60.339], Avg: [-506.884 -506.884 -506.884] (0.9900) ({r_i: None, r_t: [-1125.206 -1125.206 -1125.206], eps: 0.99})
Step:   29600, Reward: [-442.264 -442.264 -442.264] [54.247], Avg: [-506.667 -506.667 -506.667] (0.9900) ({r_i: None, r_t: [-1083.868 -1083.868 -1083.868], eps: 0.99})
Step:   29700, Reward: [-469.550 -469.550 -469.550] [99.907], Avg: [-506.542 -506.542 -506.542] (0.9900) ({r_i: None, r_t: [-1068.236 -1068.236 -1068.236], eps: 0.99})
Step:   29800, Reward: [-520.587 -520.587 -520.587] [117.317], Avg: [-506.589 -506.589 -506.589] (0.9900) ({r_i: None, r_t: [-1153.515 -1153.515 -1153.515], eps: 0.99})
Step:   29900, Reward: [-503.771 -503.771 -503.771] [51.907], Avg: [-506.580 -506.580 -506.580] (0.9900) ({r_i: None, r_t: [-1095.036 -1095.036 -1095.036], eps: 0.99})
Step:   30000, Reward: [-520.907 -520.907 -520.907] [126.912], Avg: [-506.627 -506.627 -506.627] (0.9900) ({r_i: None, r_t: [-1146.174 -1146.174 -1146.174], eps: 0.99})
Step:   30100, Reward: [-550.901 -550.901 -550.901] [150.433], Avg: [-506.774 -506.774 -506.774] (0.9900) ({r_i: None, r_t: [-953.471 -953.471 -953.471], eps: 0.99})
Step:   30200, Reward: [-526.853 -526.853 -526.853] [48.626], Avg: [-506.840 -506.840 -506.840] (0.9900) ({r_i: None, r_t: [-1111.643 -1111.643 -1111.643], eps: 0.99})
Step:   30300, Reward: [-555.175 -555.175 -555.175] [96.844], Avg: [-506.999 -506.999 -506.999] (0.9900) ({r_i: None, r_t: [-1035.002 -1035.002 -1035.002], eps: 0.99})
Step:   30400, Reward: [-562.504 -562.504 -562.504] [98.142], Avg: [-507.181 -507.181 -507.181] (0.9900) ({r_i: None, r_t: [-1098.318 -1098.318 -1098.318], eps: 0.99})
Step:   30500, Reward: [-536.928 -536.928 -536.928] [73.945], Avg: [-507.279 -507.279 -507.279] (0.9900) ({r_i: None, r_t: [-992.894 -992.894 -992.894], eps: 0.99})
Step:   30600, Reward: [-435.312 -435.312 -435.312] [70.992], Avg: [-507.044 -507.044 -507.044] (0.9900) ({r_i: None, r_t: [-1059.145 -1059.145 -1059.145], eps: 0.99})
Step:   30700, Reward: [-562.946 -562.946 -562.946] [117.292], Avg: [-507.226 -507.226 -507.226] (0.9900) ({r_i: None, r_t: [-1024.151 -1024.151 -1024.151], eps: 0.99})
Step:   30800, Reward: [-534.528 -534.528 -534.528] [53.750], Avg: [-507.314 -507.314 -507.314] (0.9900) ({r_i: None, r_t: [-1062.653 -1062.653 -1062.653], eps: 0.99})
Step:   30900, Reward: [-654.267 -654.267 -654.267] [154.017], Avg: [-507.788 -507.788 -507.788] (0.9900) ({r_i: None, r_t: [-1009.757 -1009.757 -1009.757], eps: 0.99})
Step:   31000, Reward: [-530.699 -530.699 -530.699] [97.736], Avg: [-507.862 -507.862 -507.862] (0.9900) ({r_i: None, r_t: [-1186.608 -1186.608 -1186.608], eps: 0.99})
Step:   31100, Reward: [-461.227 -461.227 -461.227] [90.737], Avg: [-507.712 -507.712 -507.712] (0.9900) ({r_i: None, r_t: [-1136.038 -1136.038 -1136.038], eps: 0.99})
Step:   31200, Reward: [-579.392 -579.392 -579.392] [49.189], Avg: [-507.941 -507.941 -507.941] (0.9900) ({r_i: None, r_t: [-1197.058 -1197.058 -1197.058], eps: 0.99})
Step:   31300, Reward: [-505.392 -505.392 -505.392] [89.093], Avg: [-507.933 -507.933 -507.933] (0.9900) ({r_i: None, r_t: [-1050.415 -1050.415 -1050.415], eps: 0.99})
Step:   31400, Reward: [-498.656 -498.656 -498.656] [119.339], Avg: [-507.904 -507.904 -507.904] (0.9900) ({r_i: None, r_t: [-1071.588 -1071.588 -1071.588], eps: 0.99})
Step:   31500, Reward: [-451.790 -451.790 -451.790] [41.981], Avg: [-507.726 -507.726 -507.726] (0.9900) ({r_i: None, r_t: [-1045.002 -1045.002 -1045.002], eps: 0.99})
Step:   31600, Reward: [-564.735 -564.735 -564.735] [77.648], Avg: [-507.906 -507.906 -507.906] (0.9900) ({r_i: None, r_t: [-962.016 -962.016 -962.016], eps: 0.99})
Step:   31700, Reward: [-502.431 -502.431 -502.431] [121.594], Avg: [-507.889 -507.889 -507.889] (0.9900) ({r_i: None, r_t: [-964.603 -964.603 -964.603], eps: 0.99})
Step:   31800, Reward: [-401.985 -401.985 -401.985] [56.418], Avg: [-507.557 -507.557 -507.557] (0.9900) ({r_i: None, r_t: [-1072.109 -1072.109 -1072.109], eps: 0.99})
Step:   31900, Reward: [-647.644 -647.644 -647.644] [239.474], Avg: [-507.994 -507.994 -507.994] (0.9900) ({r_i: None, r_t: [-1190.357 -1190.357 -1190.357], eps: 0.99})
Step:   32000, Reward: [-490.340 -490.340 -490.340] [58.436], Avg: [-507.939 -507.939 -507.939] (0.9900) ({r_i: None, r_t: [-1021.443 -1021.443 -1021.443], eps: 0.99})
Step:   32100, Reward: [-516.060 -516.060 -516.060] [83.360], Avg: [-507.965 -507.965 -507.965] (0.9900) ({r_i: None, r_t: [-1115.149 -1115.149 -1115.149], eps: 0.99})
Step:   32200, Reward: [-528.175 -528.175 -528.175] [100.848], Avg: [-508.027 -508.027 -508.027] (0.9900) ({r_i: None, r_t: [-1033.165 -1033.165 -1033.165], eps: 0.99})
Step:   32300, Reward: [-578.787 -578.787 -578.787] [89.275], Avg: [-508.246 -508.246 -508.246] (0.9900) ({r_i: None, r_t: [-1007.479 -1007.479 -1007.479], eps: 0.99})
Step:   32400, Reward: [-497.293 -497.293 -497.293] [148.552], Avg: [-508.212 -508.212 -508.212] (0.9900) ({r_i: None, r_t: [-1122.657 -1122.657 -1122.657], eps: 0.99})
Step:   32500, Reward: [-541.715 -541.715 -541.715] [67.946], Avg: [-508.315 -508.315 -508.315] (0.9900) ({r_i: None, r_t: [-1053.471 -1053.471 -1053.471], eps: 0.99})
Step:   32600, Reward: [-534.279 -534.279 -534.279] [169.354], Avg: [-508.394 -508.394 -508.394] (0.9851) ({r_i: None, r_t: [-1082.136 -1082.136 -1082.136], eps: 0.985})
Step:   32700, Reward: [-510.597 -510.597 -510.597] [101.242], Avg: [-508.401 -508.401 -508.401] (0.9851) ({r_i: None, r_t: [-1003.261 -1003.261 -1003.261], eps: 0.985})
Step:   32800, Reward: [-492.615 -492.615 -492.615] [48.921], Avg: [-508.353 -508.353 -508.353] (0.9851) ({r_i: None, r_t: [-1061.756 -1061.756 -1061.756], eps: 0.985})
Step:   32900, Reward: [-450.548 -450.548 -450.548] [79.328], Avg: [-508.178 -508.178 -508.178] (0.9851) ({r_i: None, r_t: [-1031.502 -1031.502 -1031.502], eps: 0.985})
Step:   33000, Reward: [-508.940 -508.940 -508.940] [125.748], Avg: [-508.180 -508.180 -508.180] (0.9851) ({r_i: None, r_t: [-1087.306 -1087.306 -1087.306], eps: 0.985})
Step:   33100, Reward: [-527.009 -527.009 -527.009] [143.525], Avg: [-508.237 -508.237 -508.237] (0.9851) ({r_i: None, r_t: [-863.624 -863.624 -863.624], eps: 0.985})
Step:   33200, Reward: [-575.956 -575.956 -575.956] [103.584], Avg: [-508.440 -508.440 -508.440] (0.9851) ({r_i: None, r_t: [-1191.856 -1191.856 -1191.856], eps: 0.985})
Step:   33300, Reward: [-476.275 -476.275 -476.275] [35.063], Avg: [-508.344 -508.344 -508.344] (0.9851) ({r_i: None, r_t: [-1047.277 -1047.277 -1047.277], eps: 0.985})
Step:   33400, Reward: [-587.827 -587.827 -587.827] [156.123], Avg: [-508.581 -508.581 -508.581] (0.9851) ({r_i: None, r_t: [-1159.009 -1159.009 -1159.009], eps: 0.985})
Step:   33500, Reward: [-590.429 -590.429 -590.429] [145.832], Avg: [-508.825 -508.825 -508.825] (0.9851) ({r_i: None, r_t: [-987.671 -987.671 -987.671], eps: 0.985})
Step:   33600, Reward: [-672.665 -672.665 -672.665] [85.916], Avg: [-509.311 -509.311 -509.311] (0.9851) ({r_i: None, r_t: [-1041.297 -1041.297 -1041.297], eps: 0.985})
Step:   33700, Reward: [-502.107 -502.107 -502.107] [189.514], Avg: [-509.289 -509.289 -509.289] (0.9851) ({r_i: None, r_t: [-1045.866 -1045.866 -1045.866], eps: 0.985})
Step:   33800, Reward: [-532.670 -532.670 -532.670] [100.445], Avg: [-509.358 -509.358 -509.358] (0.9851) ({r_i: None, r_t: [-1129.504 -1129.504 -1129.504], eps: 0.985})
Step:   33900, Reward: [-606.236 -606.236 -606.236] [66.742], Avg: [-509.643 -509.643 -509.643] (0.9851) ({r_i: None, r_t: [-1117.543 -1117.543 -1117.543], eps: 0.985})
Step:   34000, Reward: [-491.921 -491.921 -491.921] [38.892], Avg: [-509.591 -509.591 -509.591] (0.9851) ({r_i: None, r_t: [-893.835 -893.835 -893.835], eps: 0.985})
Step:   34100, Reward: [-553.063 -553.063 -553.063] [49.238], Avg: [-509.719 -509.719 -509.719] (0.9851) ({r_i: None, r_t: [-949.237 -949.237 -949.237], eps: 0.985})
Step:   34200, Reward: [-502.125 -502.125 -502.125] [28.040], Avg: [-509.696 -509.696 -509.696] (0.9851) ({r_i: None, r_t: [-1213.757 -1213.757 -1213.757], eps: 0.985})
Step:   34300, Reward: [-555.756 -555.756 -555.756] [20.950], Avg: [-509.830 -509.830 -509.830] (0.9851) ({r_i: None, r_t: [-978.906 -978.906 -978.906], eps: 0.985})
Step:   34400, Reward: [-603.494 -603.494 -603.494] [92.990], Avg: [-510.102 -510.102 -510.102] (0.9851) ({r_i: None, r_t: [-1068.178 -1068.178 -1068.178], eps: 0.985})
Step:   34500, Reward: [-552.342 -552.342 -552.342] [40.210], Avg: [-510.224 -510.224 -510.224] (0.9851) ({r_i: None, r_t: [-1083.932 -1083.932 -1083.932], eps: 0.985})
Step:   34600, Reward: [-533.935 -533.935 -533.935] [9.057], Avg: [-510.292 -510.292 -510.292] (0.9851) ({r_i: None, r_t: [-1110.002 -1110.002 -1110.002], eps: 0.985})
Step:   34700, Reward: [-447.208 -447.208 -447.208] [52.491], Avg: [-510.111 -510.111 -510.111] (0.9851) ({r_i: None, r_t: [-1101.833 -1101.833 -1101.833], eps: 0.985})
Step:   34800, Reward: [-506.600 -506.600 -506.600] [92.863], Avg: [-510.101 -510.101 -510.101] (0.9851) ({r_i: None, r_t: [-1073.058 -1073.058 -1073.058], eps: 0.985})
Step:   34900, Reward: [-519.654 -519.654 -519.654] [72.062], Avg: [-510.128 -510.128 -510.128] (0.9851) ({r_i: None, r_t: [-1103.977 -1103.977 -1103.977], eps: 0.985})
Step:   35000, Reward: [-546.235 -546.235 -546.235] [91.627], Avg: [-510.231 -510.231 -510.231] (0.9851) ({r_i: None, r_t: [-1115.514 -1115.514 -1115.514], eps: 0.985})
Step:   35100, Reward: [-481.902 -481.902 -481.902] [80.779], Avg: [-510.151 -510.151 -510.151] (0.9851) ({r_i: None, r_t: [-1126.151 -1126.151 -1126.151], eps: 0.985})
Step:   35200, Reward: [-514.437 -514.437 -514.437] [97.430], Avg: [-510.163 -510.163 -510.163] (0.9851) ({r_i: None, r_t: [-959.976 -959.976 -959.976], eps: 0.985})
Step:   35300, Reward: [-513.813 -513.813 -513.813] [138.470], Avg: [-510.173 -510.173 -510.173] (0.9851) ({r_i: None, r_t: [-1115.695 -1115.695 -1115.695], eps: 0.985})
Step:   35400, Reward: [-471.774 -471.774 -471.774] [119.551], Avg: [-510.065 -510.065 -510.065] (0.9851) ({r_i: None, r_t: [-1071.695 -1071.695 -1071.695], eps: 0.985})
Step:   35500, Reward: [-549.711 -549.711 -549.711] [84.259], Avg: [-510.176 -510.176 -510.176] (0.9851) ({r_i: None, r_t: [-1024.158 -1024.158 -1024.158], eps: 0.985})
Step:   35600, Reward: [-534.050 -534.050 -534.050] [37.494], Avg: [-510.243 -510.243 -510.243] (0.9851) ({r_i: None, r_t: [-973.723 -973.723 -973.723], eps: 0.985})
Step:   35700, Reward: [-512.569 -512.569 -512.569] [43.705], Avg: [-510.250 -510.250 -510.250] (0.9851) ({r_i: None, r_t: [-1146.069 -1146.069 -1146.069], eps: 0.985})
Step:   35800, Reward: [-574.071 -574.071 -574.071] [36.969], Avg: [-510.427 -510.427 -510.427] (0.9851) ({r_i: None, r_t: [-948.926 -948.926 -948.926], eps: 0.985})
Step:   35900, Reward: [-598.763 -598.763 -598.763] [120.576], Avg: [-510.673 -510.673 -510.673] (0.9851) ({r_i: None, r_t: [-1006.593 -1006.593 -1006.593], eps: 0.985})
Step:   36000, Reward: [-579.631 -579.631 -579.631] [119.961], Avg: [-510.864 -510.864 -510.864] (0.9851) ({r_i: None, r_t: [-1150.116 -1150.116 -1150.116], eps: 0.985})
Step:   36100, Reward: [-622.821 -622.821 -622.821] [54.828], Avg: [-511.173 -511.173 -511.173] (0.9851) ({r_i: None, r_t: [-1164.959 -1164.959 -1164.959], eps: 0.985})
Step:   36200, Reward: [-503.246 -503.246 -503.246] [89.430], Avg: [-511.151 -511.151 -511.151] (0.9851) ({r_i: None, r_t: [-841.458 -841.458 -841.458], eps: 0.985})
Step:   36300, Reward: [-534.619 -534.619 -534.619] [70.729], Avg: [-511.216 -511.216 -511.216] (0.9851) ({r_i: None, r_t: [-1064.061 -1064.061 -1064.061], eps: 0.985})
Step:   36400, Reward: [-439.399 -439.399 -439.399] [26.025], Avg: [-511.019 -511.019 -511.019] (0.9851) ({r_i: None, r_t: [-1017.904 -1017.904 -1017.904], eps: 0.985})
Step:   36500, Reward: [-565.094 -565.094 -565.094] [66.521], Avg: [-511.167 -511.167 -511.167] (0.9851) ({r_i: None, r_t: [-885.570 -885.570 -885.570], eps: 0.985})
Step:   36600, Reward: [-504.090 -504.090 -504.090] [82.992], Avg: [-511.147 -511.147 -511.147] (0.9851) ({r_i: None, r_t: [-1157.651 -1157.651 -1157.651], eps: 0.985})
Step:   36700, Reward: [-535.294 -535.294 -535.294] [69.201], Avg: [-511.213 -511.213 -511.213] (0.9851) ({r_i: None, r_t: [-1025.137 -1025.137 -1025.137], eps: 0.985})
Step:   36800, Reward: [-559.933 -559.933 -559.933] [95.721], Avg: [-511.345 -511.345 -511.345] (0.9851) ({r_i: None, r_t: [-922.082 -922.082 -922.082], eps: 0.985})
Step:   36900, Reward: [-557.950 -557.950 -557.950] [19.391], Avg: [-511.471 -511.471 -511.471] (0.9851) ({r_i: None, r_t: [-975.595 -975.595 -975.595], eps: 0.985})
Step:   37000, Reward: [-591.797 -591.797 -591.797] [75.837], Avg: [-511.687 -511.687 -511.687] (0.9851) ({r_i: None, r_t: [-980.608 -980.608 -980.608], eps: 0.985})
Step:   37100, Reward: [-541.148 -541.148 -541.148] [76.922], Avg: [-511.767 -511.767 -511.767] (0.9851) ({r_i: None, r_t: [-945.827 -945.827 -945.827], eps: 0.985})
Step:   37200, Reward: [-447.362 -447.362 -447.362] [104.153], Avg: [-511.594 -511.594 -511.594] (0.9851) ({r_i: None, r_t: [-1011.892 -1011.892 -1011.892], eps: 0.985})
Step:   37300, Reward: [-627.996 -627.996 -627.996] [82.431], Avg: [-511.905 -511.905 -511.905] (0.9851) ({r_i: None, r_t: [-1030.052 -1030.052 -1030.052], eps: 0.985})
Step:   37400, Reward: [-483.511 -483.511 -483.511] [39.908], Avg: [-511.830 -511.830 -511.830] (0.9851) ({r_i: None, r_t: [-1026.001 -1026.001 -1026.001], eps: 0.985})
Step:   37500, Reward: [-639.913 -639.913 -639.913] [179.578], Avg: [-512.170 -512.170 -512.170] (0.9851) ({r_i: None, r_t: [-1069.847 -1069.847 -1069.847], eps: 0.985})
Step:   37600, Reward: [-540.047 -540.047 -540.047] [95.452], Avg: [-512.244 -512.244 -512.244] (0.9851) ({r_i: None, r_t: [-1037.156 -1037.156 -1037.156], eps: 0.985})
Step:   37700, Reward: [-526.309 -526.309 -526.309] [94.344], Avg: [-512.281 -512.281 -512.281] (0.9851) ({r_i: None, r_t: [-1118.526 -1118.526 -1118.526], eps: 0.985})
Step:   37800, Reward: [-541.477 -541.477 -541.477] [161.864], Avg: [-512.358 -512.358 -512.358] (0.9851) ({r_i: None, r_t: [-1094.460 -1094.460 -1094.460], eps: 0.985})
Step:   37900, Reward: [-569.915 -569.915 -569.915] [121.575], Avg: [-512.510 -512.510 -512.510] (0.9851) ({r_i: None, r_t: [-1051.876 -1051.876 -1051.876], eps: 0.985})
Step:   38000, Reward: [-463.527 -463.527 -463.527] [33.405], Avg: [-512.381 -512.381 -512.381] (0.9851) ({r_i: None, r_t: [-1133.581 -1133.581 -1133.581], eps: 0.985})
Step:   38100, Reward: [-585.525 -585.525 -585.525] [135.736], Avg: [-512.573 -512.573 -512.573] (0.9851) ({r_i: None, r_t: [-1091.457 -1091.457 -1091.457], eps: 0.985})
Step:   38200, Reward: [-549.184 -549.184 -549.184] [40.231], Avg: [-512.668 -512.668 -512.668] (0.9851) ({r_i: None, r_t: [-1084.448 -1084.448 -1084.448], eps: 0.985})
Step:   38300, Reward: [-527.198 -527.198 -527.198] [116.201], Avg: [-512.706 -512.706 -512.706] (0.9851) ({r_i: None, r_t: [-1000.055 -1000.055 -1000.055], eps: 0.985})
Step:   38400, Reward: [-553.976 -553.976 -553.976] [79.090], Avg: [-512.813 -512.813 -512.813] (0.9851) ({r_i: None, r_t: [-973.812 -973.812 -973.812], eps: 0.985})
Step:   38500, Reward: [-588.724 -588.724 -588.724] [74.677], Avg: [-513.010 -513.010 -513.010] (0.9851) ({r_i: None, r_t: [-1103.216 -1103.216 -1103.216], eps: 0.985})
Step:   38600, Reward: [-463.094 -463.094 -463.094] [104.332], Avg: [-512.881 -512.881 -512.881] (0.9851) ({r_i: None, r_t: [-1055.875 -1055.875 -1055.875], eps: 0.985})
Step:   38700, Reward: [-557.873 -557.873 -557.873] [119.195], Avg: [-512.997 -512.997 -512.997] (0.9851) ({r_i: None, r_t: [-986.755 -986.755 -986.755], eps: 0.985})
Step:   38800, Reward: [-499.611 -499.611 -499.611] [85.288], Avg: [-512.963 -512.963 -512.963] (0.9851) ({r_i: None, r_t: [-1060.739 -1060.739 -1060.739], eps: 0.985})
Step:   38900, Reward: [-526.360 -526.360 -526.360] [107.083], Avg: [-512.997 -512.997 -512.997] (0.9851) ({r_i: None, r_t: [-1083.549 -1083.549 -1083.549], eps: 0.985})
Step:   39000, Reward: [-523.899 -523.899 -523.899] [81.387], Avg: [-513.025 -513.025 -513.025] (0.9851) ({r_i: None, r_t: [-1074.265 -1074.265 -1074.265], eps: 0.985})
Step:   39100, Reward: [-545.384 -545.384 -545.384] [56.008], Avg: [-513.107 -513.107 -513.107] (0.9851) ({r_i: None, r_t: [-1096.821 -1096.821 -1096.821], eps: 0.985})
Step:   39200, Reward: [-458.414 -458.414 -458.414] [84.972], Avg: [-512.968 -512.968 -512.968] (0.9851) ({r_i: None, r_t: [-901.286 -901.286 -901.286], eps: 0.985})
Step:   39300, Reward: [-516.276 -516.276 -516.276] [114.817], Avg: [-512.977 -512.977 -512.977] (0.9851) ({r_i: None, r_t: [-1010.728 -1010.728 -1010.728], eps: 0.985})
Step:   39400, Reward: [-497.234 -497.234 -497.234] [83.246], Avg: [-512.937 -512.937 -512.937] (0.9851) ({r_i: None, r_t: [-1200.203 -1200.203 -1200.203], eps: 0.985})
Step:   39500, Reward: [-511.013 -511.013 -511.013] [138.263], Avg: [-512.932 -512.932 -512.932] (0.9851) ({r_i: None, r_t: [-1094.115 -1094.115 -1094.115], eps: 0.985})
Step:   39600, Reward: [-538.265 -538.265 -538.265] [61.163], Avg: [-512.996 -512.996 -512.996] (0.9851) ({r_i: None, r_t: [-1083.526 -1083.526 -1083.526], eps: 0.985})
Step:   39700, Reward: [-460.771 -460.771 -460.771] [39.451], Avg: [-512.864 -512.864 -512.864] (0.9851) ({r_i: None, r_t: [-1218.140 -1218.140 -1218.140], eps: 0.985})
Step:   39800, Reward: [-581.842 -581.842 -581.842] [196.262], Avg: [-513.037 -513.037 -513.037] (0.9851) ({r_i: None, r_t: [-1237.603 -1237.603 -1237.603], eps: 0.985})
Step:   39900, Reward: [-560.175 -560.175 -560.175] [64.185], Avg: [-513.155 -513.155 -513.155] (0.9851) ({r_i: None, r_t: [-1203.028 -1203.028 -1203.028], eps: 0.985})
Step:   40000, Reward: [-571.099 -571.099 -571.099] [125.783], Avg: [-513.300 -513.300 -513.300] (0.9851) ({r_i: None, r_t: [-1039.224 -1039.224 -1039.224], eps: 0.985})
Step:   40100, Reward: [-549.279 -549.279 -549.279] [140.457], Avg: [-513.389 -513.389 -513.389] (0.9851) ({r_i: None, r_t: [-1051.900 -1051.900 -1051.900], eps: 0.985})
Step:   40200, Reward: [-465.805 -465.805 -465.805] [70.231], Avg: [-513.271 -513.271 -513.271] (0.9851) ({r_i: None, r_t: [-1090.383 -1090.383 -1090.383], eps: 0.985})
Step:   40300, Reward: [-597.349 -597.349 -597.349] [78.294], Avg: [-513.479 -513.479 -513.479] (0.9851) ({r_i: None, r_t: [-1181.455 -1181.455 -1181.455], eps: 0.985})
Step:   40400, Reward: [-519.412 -519.412 -519.412] [107.779], Avg: [-513.494 -513.494 -513.494] (0.9851) ({r_i: None, r_t: [-1155.910 -1155.910 -1155.910], eps: 0.985})
Step:   40500, Reward: [-481.776 -481.776 -481.776] [126.281], Avg: [-513.416 -513.416 -513.416] (0.9851) ({r_i: None, r_t: [-1149.960 -1149.960 -1149.960], eps: 0.985})
Step:   40600, Reward: [-606.685 -606.685 -606.685] [136.269], Avg: [-513.645 -513.645 -513.645] (0.9851) ({r_i: None, r_t: [-946.378 -946.378 -946.378], eps: 0.985})
Step:   40700, Reward: [-508.996 -508.996 -508.996] [45.709], Avg: [-513.634 -513.634 -513.634] (0.9851) ({r_i: None, r_t: [-1062.828 -1062.828 -1062.828], eps: 0.985})
Step:   40800, Reward: [-462.036 -462.036 -462.036] [68.175], Avg: [-513.507 -513.507 -513.507] (0.9851) ({r_i: None, r_t: [-1068.400 -1068.400 -1068.400], eps: 0.985})
Step:   40900, Reward: [-555.194 -555.194 -555.194] [73.632], Avg: [-513.609 -513.609 -513.609] (0.9851) ({r_i: None, r_t: [-990.594 -990.594 -990.594], eps: 0.985})
Step:   41000, Reward: [-502.914 -502.914 -502.914] [38.353], Avg: [-513.583 -513.583 -513.583] (0.9851) ({r_i: None, r_t: [-1136.808 -1136.808 -1136.808], eps: 0.985})
Step:   41100, Reward: [-406.834 -406.834 -406.834] [40.421], Avg: [-513.324 -513.324 -513.324] (0.9851) ({r_i: None, r_t: [-1080.630 -1080.630 -1080.630], eps: 0.985})
Step:   41200, Reward: [-621.952 -621.952 -621.952] [68.055], Avg: [-513.587 -513.587 -513.587] (0.9851) ({r_i: None, r_t: [-1061.163 -1061.163 -1061.163], eps: 0.985})
Step:   41300, Reward: [-499.661 -499.661 -499.661] [79.054], Avg: [-513.553 -513.553 -513.553] (0.9851) ({r_i: None, r_t: [-1037.760 -1037.760 -1037.760], eps: 0.985})
Step:   41400, Reward: [-549.497 -549.497 -549.497] [143.456], Avg: [-513.640 -513.640 -513.640] (0.9851) ({r_i: None, r_t: [-1004.344 -1004.344 -1004.344], eps: 0.985})
Step:   41500, Reward: [-622.697 -622.697 -622.697] [58.699], Avg: [-513.902 -513.902 -513.902] (0.9851) ({r_i: None, r_t: [-968.009 -968.009 -968.009], eps: 0.985})
Step:   41600, Reward: [-519.153 -519.153 -519.153] [114.247], Avg: [-513.915 -513.915 -513.915] (0.9851) ({r_i: None, r_t: [-982.926 -982.926 -982.926], eps: 0.985})
Step:   41700, Reward: [-623.602 -623.602 -623.602] [121.596], Avg: [-514.177 -514.177 -514.177] (0.9851) ({r_i: None, r_t: [-1075.497 -1075.497 -1075.497], eps: 0.985})
Step:   41800, Reward: [-548.680 -548.680 -548.680] [16.245], Avg: [-514.259 -514.259 -514.259] (0.9851) ({r_i: None, r_t: [-916.402 -916.402 -916.402], eps: 0.985})
Step:   41900, Reward: [-522.263 -522.263 -522.263] [153.341], Avg: [-514.278 -514.278 -514.278] (0.9851) ({r_i: None, r_t: [-1161.965 -1161.965 -1161.965], eps: 0.985})
Step:   42000, Reward: [-530.286 -530.286 -530.286] [119.932], Avg: [-514.316 -514.316 -514.316] (0.9851) ({r_i: None, r_t: [-1184.652 -1184.652 -1184.652], eps: 0.985})
Step:   42100, Reward: [-535.636 -535.636 -535.636] [111.410], Avg: [-514.367 -514.367 -514.367] (0.9851) ({r_i: None, r_t: [-1180.247 -1180.247 -1180.247], eps: 0.985})
Step:   42200, Reward: [-489.374 -489.374 -489.374] [88.389], Avg: [-514.308 -514.308 -514.308] (0.9851) ({r_i: None, r_t: [-1046.104 -1046.104 -1046.104], eps: 0.985})
Step:   42300, Reward: [-567.635 -567.635 -567.635] [59.843], Avg: [-514.434 -514.434 -514.434] (0.9851) ({r_i: None, r_t: [-1191.915 -1191.915 -1191.915], eps: 0.985})
Step:   42400, Reward: [-511.237 -511.237 -511.237] [125.356], Avg: [-514.426 -514.426 -514.426] (0.9851) ({r_i: None, r_t: [-1000.011 -1000.011 -1000.011], eps: 0.985})
Step:   42500, Reward: [-560.788 -560.788 -560.788] [111.789], Avg: [-514.535 -514.535 -514.535] (0.9851) ({r_i: None, r_t: [-990.417 -990.417 -990.417], eps: 0.985})
Step:   42600, Reward: [-639.866 -639.866 -639.866] [92.266], Avg: [-514.829 -514.829 -514.829] (0.9851) ({r_i: None, r_t: [-1113.380 -1113.380 -1113.380], eps: 0.985})
Step:   42700, Reward: [-584.700 -584.700 -584.700] [75.182], Avg: [-514.992 -514.992 -514.992] (0.9851) ({r_i: None, r_t: [-1146.122 -1146.122 -1146.122], eps: 0.985})
Step:   42800, Reward: [-454.198 -454.198 -454.198] [38.913], Avg: [-514.850 -514.850 -514.850] (0.9851) ({r_i: None, r_t: [-1119.016 -1119.016 -1119.016], eps: 0.985})
Step:   42900, Reward: [-555.322 -555.322 -555.322] [73.616], Avg: [-514.944 -514.944 -514.944] (0.9851) ({r_i: None, r_t: [-1216.136 -1216.136 -1216.136], eps: 0.985})
Step:   43000, Reward: [-511.895 -511.895 -511.895] [40.519], Avg: [-514.937 -514.937 -514.937] (0.9851) ({r_i: None, r_t: [-1021.761 -1021.761 -1021.761], eps: 0.985})
Step:   43100, Reward: [-526.043 -526.043 -526.043] [188.312], Avg: [-514.963 -514.963 -514.963] (0.9851) ({r_i: None, r_t: [-1024.553 -1024.553 -1024.553], eps: 0.985})
Step:   43200, Reward: [-616.555 -616.555 -616.555] [136.704], Avg: [-515.197 -515.197 -515.197] (0.9851) ({r_i: None, r_t: [-1110.323 -1110.323 -1110.323], eps: 0.985})
Step:   43300, Reward: [-574.389 -574.389 -574.389] [95.642], Avg: [-515.334 -515.334 -515.334] (0.9851) ({r_i: None, r_t: [-1148.914 -1148.914 -1148.914], eps: 0.985})
Step:   43400, Reward: [-471.321 -471.321 -471.321] [63.758], Avg: [-515.233 -515.233 -515.233] (0.9851) ({r_i: None, r_t: [-988.133 -988.133 -988.133], eps: 0.985})
Step:   43500, Reward: [-496.403 -496.403 -496.403] [75.094], Avg: [-515.189 -515.189 -515.189] (0.9851) ({r_i: None, r_t: [-1126.934 -1126.934 -1126.934], eps: 0.985})
Step:   43600, Reward: [-518.313 -518.313 -518.313] [93.974], Avg: [-515.197 -515.197 -515.197] (0.9851) ({r_i: None, r_t: [-1131.331 -1131.331 -1131.331], eps: 0.985})
Step:   43700, Reward: [-491.047 -491.047 -491.047] [150.556], Avg: [-515.141 -515.141 -515.141] (0.9851) ({r_i: None, r_t: [-972.355 -972.355 -972.355], eps: 0.985})
Step:   43800, Reward: [-504.674 -504.674 -504.674] [50.256], Avg: [-515.118 -515.118 -515.118] (0.9851) ({r_i: None, r_t: [-1050.838 -1050.838 -1050.838], eps: 0.985})
Step:   43900, Reward: [-598.289 -598.289 -598.289] [144.233], Avg: [-515.307 -515.307 -515.307] (0.9851) ({r_i: None, r_t: [-1055.707 -1055.707 -1055.707], eps: 0.985})
Step:   44000, Reward: [-536.197 -536.197 -536.197] [156.557], Avg: [-515.354 -515.354 -515.354] (0.9851) ({r_i: None, r_t: [-1044.193 -1044.193 -1044.193], eps: 0.985})
Step:   44100, Reward: [-516.578 -516.578 -516.578] [80.941], Avg: [-515.357 -515.357 -515.357] (0.9851) ({r_i: None, r_t: [-1035.754 -1035.754 -1035.754], eps: 0.985})
Step:   44200, Reward: [-471.300 -471.300 -471.300] [50.971], Avg: [-515.257 -515.257 -515.257] (0.9851) ({r_i: None, r_t: [-1079.489 -1079.489 -1079.489], eps: 0.985})
Step:   44300, Reward: [-527.265 -527.265 -527.265] [115.101], Avg: [-515.284 -515.284 -515.284] (0.9851) ({r_i: None, r_t: [-987.274 -987.274 -987.274], eps: 0.985})
Step:   44400, Reward: [-539.311 -539.311 -539.311] [141.702], Avg: [-515.338 -515.338 -515.338] (0.9851) ({r_i: None, r_t: [-1111.389 -1111.389 -1111.389], eps: 0.985})
Step:   44500, Reward: [-538.011 -538.011 -538.011] [82.295], Avg: [-515.389 -515.389 -515.389] (0.9851) ({r_i: None, r_t: [-1182.772 -1182.772 -1182.772], eps: 0.985})
Step:   44600, Reward: [-534.669 -534.669 -534.669] [98.959], Avg: [-515.432 -515.432 -515.432] (0.9851) ({r_i: None, r_t: [-1019.070 -1019.070 -1019.070], eps: 0.985})
Step:   44700, Reward: [-693.881 -693.881 -693.881] [230.313], Avg: [-515.831 -515.831 -515.831] (0.9851) ({r_i: None, r_t: [-1143.056 -1143.056 -1143.056], eps: 0.985})
Step:   44800, Reward: [-550.277 -550.277 -550.277] [75.373], Avg: [-515.907 -515.907 -515.907] (0.9851) ({r_i: None, r_t: [-1191.100 -1191.100 -1191.100], eps: 0.985})
Step:   44900, Reward: [-532.320 -532.320 -532.320] [96.528], Avg: [-515.944 -515.944 -515.944] (0.9851) ({r_i: None, r_t: [-1158.715 -1158.715 -1158.715], eps: 0.985})
Step:   45000, Reward: [-500.823 -500.823 -500.823] [48.437], Avg: [-515.910 -515.910 -515.910] (0.9851) ({r_i: None, r_t: [-1094.092 -1094.092 -1094.092], eps: 0.985})
Step:   45100, Reward: [-652.453 -652.453 -652.453] [68.911], Avg: [-516.212 -516.212 -516.212] (0.9801) ({r_i: None, r_t: [-1142.272 -1142.272 -1142.272], eps: 0.98})
Step:   45200, Reward: [-529.414 -529.414 -529.414] [72.797], Avg: [-516.242 -516.242 -516.242] (0.9801) ({r_i: None, r_t: [-1149.889 -1149.889 -1149.889], eps: 0.98})
Step:   45300, Reward: [-543.905 -543.905 -543.905] [177.164], Avg: [-516.302 -516.302 -516.302] (0.9801) ({r_i: None, r_t: [-1058.822 -1058.822 -1058.822], eps: 0.98})
Step:   45400, Reward: [-582.245 -582.245 -582.245] [74.527], Avg: [-516.447 -516.447 -516.447] (0.9801) ({r_i: None, r_t: [-953.229 -953.229 -953.229], eps: 0.98})
Step:   45500, Reward: [-596.249 -596.249 -596.249] [243.338], Avg: [-516.622 -516.622 -516.622] (0.9801) ({r_i: None, r_t: [-1058.917 -1058.917 -1058.917], eps: 0.98})
Step:   45600, Reward: [-633.492 -633.492 -633.492] [185.832], Avg: [-516.878 -516.878 -516.878] (0.9801) ({r_i: None, r_t: [-1096.213 -1096.213 -1096.213], eps: 0.98})
Step:   45700, Reward: [-503.408 -503.408 -503.408] [120.323], Avg: [-516.849 -516.849 -516.849] (0.9801) ({r_i: None, r_t: [-1033.228 -1033.228 -1033.228], eps: 0.98})
Step:   45800, Reward: [-561.435 -561.435 -561.435] [80.875], Avg: [-516.946 -516.946 -516.946] (0.9801) ({r_i: None, r_t: [-1156.229 -1156.229 -1156.229], eps: 0.98})
Step:   45900, Reward: [-547.799 -547.799 -547.799] [142.377], Avg: [-517.013 -517.013 -517.013] (0.9801) ({r_i: None, r_t: [-998.992 -998.992 -998.992], eps: 0.98})
Step:   46000, Reward: [-500.624 -500.624 -500.624] [44.184], Avg: [-516.977 -516.977 -516.977] (0.9801) ({r_i: None, r_t: [-1157.595 -1157.595 -1157.595], eps: 0.98})
Step:   46100, Reward: [-539.901 -539.901 -539.901] [85.825], Avg: [-517.027 -517.027 -517.027] (0.9801) ({r_i: None, r_t: [-990.098 -990.098 -990.098], eps: 0.98})
Step:   46200, Reward: [-580.502 -580.502 -580.502] [147.175], Avg: [-517.164 -517.164 -517.164] (0.9801) ({r_i: None, r_t: [-1050.516 -1050.516 -1050.516], eps: 0.98})
Step:   46300, Reward: [-674.899 -674.899 -674.899] [47.902], Avg: [-517.504 -517.504 -517.504] (0.9801) ({r_i: None, r_t: [-1041.660 -1041.660 -1041.660], eps: 0.98})
Step:   46400, Reward: [-570.397 -570.397 -570.397] [137.614], Avg: [-517.618 -517.618 -517.618] (0.9801) ({r_i: None, r_t: [-1143.771 -1143.771 -1143.771], eps: 0.98})
Step:   46500, Reward: [-461.894 -461.894 -461.894] [145.818], Avg: [-517.498 -517.498 -517.498] (0.9801) ({r_i: None, r_t: [-1063.415 -1063.415 -1063.415], eps: 0.98})
Step:   46600, Reward: [-580.150 -580.150 -580.150] [86.207], Avg: [-517.632 -517.632 -517.632] (0.9801) ({r_i: None, r_t: [-1190.700 -1190.700 -1190.700], eps: 0.98})
Step:   46700, Reward: [-518.332 -518.332 -518.332] [121.052], Avg: [-517.634 -517.634 -517.634] (0.9801) ({r_i: None, r_t: [-1073.498 -1073.498 -1073.498], eps: 0.98})
Step:   46800, Reward: [-519.377 -519.377 -519.377] [68.858], Avg: [-517.638 -517.638 -517.638] (0.9801) ({r_i: None, r_t: [-1175.056 -1175.056 -1175.056], eps: 0.98})
Step:   46900, Reward: [-560.866 -560.866 -560.866] [86.896], Avg: [-517.730 -517.730 -517.730] (0.9801) ({r_i: None, r_t: [-1207.019 -1207.019 -1207.019], eps: 0.98})
Step:   47000, Reward: [-571.974 -571.974 -571.974] [88.383], Avg: [-517.845 -517.845 -517.845] (0.9801) ({r_i: None, r_t: [-1074.232 -1074.232 -1074.232], eps: 0.98})
Step:   47100, Reward: [-541.876 -541.876 -541.876] [47.847], Avg: [-517.896 -517.896 -517.896] (0.9801) ({r_i: None, r_t: [-1147.228 -1147.228 -1147.228], eps: 0.98})
Step:   47200, Reward: [-495.666 -495.666 -495.666] [71.567], Avg: [-517.849 -517.849 -517.849] (0.9801) ({r_i: None, r_t: [-1193.859 -1193.859 -1193.859], eps: 0.98})
Step:   47300, Reward: [-669.594 -669.594 -669.594] [166.483], Avg: [-518.169 -518.169 -518.169] (0.9801) ({r_i: None, r_t: [-1167.217 -1167.217 -1167.217], eps: 0.98})
Step:   47400, Reward: [-575.407 -575.407 -575.407] [102.355], Avg: [-518.289 -518.289 -518.289] (0.9801) ({r_i: None, r_t: [-1260.547 -1260.547 -1260.547], eps: 0.98})
Step:   47500, Reward: [-536.663 -536.663 -536.663] [79.302], Avg: [-518.328 -518.328 -518.328] (0.9801) ({r_i: None, r_t: [-1100.037 -1100.037 -1100.037], eps: 0.98})
