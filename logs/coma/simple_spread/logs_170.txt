Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread, Date: 13/03/2020 17:41:40
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, PTCritic, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, TARGET_UPDATE_RATE, one_hot_from_indices

EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
TIME_BATCHES = 100				# The number of batches of time steps to train critic in reverse time sequence
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.action_probs(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + len(state_size)], action_size[0])
		self.critic = lambda s,a: PTCritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + len(state_size)], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, lr=lr, gpu=gpu, load=load, name="coma")

	def get_action_probs(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		critic_losses = []
		q_next_value = self.critic_target(critic_inputs)
		q_next_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_next_taken = torch.cat([q_next_taken, torch.zeros_like(q_next_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = PTACAgent.compute_ma_gae(rewards.unsqueeze(-1), dones.unsqueeze(-1), q_next_taken)
		q_value = torch.zeros_like(q_next_value)
		t_batch = max(rewards.size(1)//TIME_BATCHES, 1)
		for t in reversed(range(0,min(rewards.size(1), t_batch*TIME_BATCHES),t_batch)):
			q_value[:,t:t+t_batch] = self.critic_local(critic_inputs[:,t:t+t_batch])
			q_taken = torch.gather(q_value[:,t:t+t_batch], dim=-1, index=actions[:,t:t+t_batch].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - q_target[:,t:t+t_batch].detach())
			critic_loss = critic_error.pow(2).mean()
			critic_losses.append(critic_loss.detach().cpu().numpy())
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		self.soft_copy(self.critic_local, self.critic_target)

		action_probs = self.get_action_probs(actor_inputs, eps, grad=True)[1]
		q_value = q_value.reshape(-1, action_probs.shape[-1])
		pi = action_probs.view(-1, action_probs.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())
		return [np.mean(critic_losses), np.mean(actor_loss.detach().cpu().numpy())]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)
		self.stats = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action_probs(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	# def train(self, state, action, next_state, reward, done):
	# 	self.step = 0 if not hasattr(self, "step") else self.step + 1
	# 	self.buffer.append((state, action, reward, done))
	# 	if np.any(done[0]):
	# 		sample = list(map(lambda x: self.to_tensor(x), zip(*self.buffer)))
	# 		states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
	# 		obs, actions = [x.squeeze(-2) for x in [states, actions]]
	# 		state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
	# 		actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
	# 		agent_mask = (1-torch.eye(self.n_agents, device=self.network.device))
	# 		agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
	# 		last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
	# 		last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
	# 		agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
	# 		critic_inputs = torch.cat([state, obs, actions_joint * agent_mask, last_actions_joint, agent_inds], dim=-1)
	# 		actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
	# 		self.replay_buffer.add([self.to_numpy([x.transpose(0,1)]) for x in (actions, critic_inputs, actor_inputs, rewards, dones)])
	# 		self.buffer.clear()
	# 	if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
	# 		actions, critic_inputs, actor_inputs, rewards, dones = [x[0] for x in self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))]
	# 		self.stats.append(self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1), dones.mean(-1), self.eps))
	# 	if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1), dones.mean(-1), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

	def get_stats(self):
		stats = {k:v for k,v in zip(["critic_loss", "actor_loss"], np.mean(self.stats, axis=0))} if len(self.stats)>0 else {}
		self.stats = []
		return {**stats, **super().get_stats()}

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 512				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
# env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward,eps: [0.1*(ballr(o[0,88], o[0,89])) + r for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def train(model, steps=10000, ports=16, env_name=env_name, trial_at=500, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			save_dir = env_name + "/" +  "_".join(["rs"]*int(reward_shape) + ["icm"]*int(icm))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(save_dir, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.selfport is not None or MPI_RANK>0:
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	elif args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	else:
		train(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-506.861 -506.861 -506.861] [85.693], Avg: [-506.861 -506.861 -506.861] (1.0000) <00:00:00> ({r_i: None, r_t: [-8.673 -8.673 -8.673], eps: 1.0})
Step:     500, Reward: [-468.283 -468.283 -468.283] [92.128], Avg: [-487.572 -487.572 -487.572] (0.9044) <00:00:06> ({r_i: None, r_t: [-4944.258 -4944.258 -4944.258], eps: 0.904})
Step:    1000, Reward: [-467.344 -467.344 -467.344] [81.022], Avg: [-480.830 -480.830 -480.830] (0.8179) <00:00:12> ({r_i: None, r_t: [-4841.273 -4841.273 -4841.273], eps: 0.818})
Step:    1500, Reward: [-494.927 -494.927 -494.927] [86.946], Avg: [-484.354 -484.354 -484.354] (0.7397) <00:00:19> ({r_i: None, r_t: [-4781.507 -4781.507 -4781.507], eps: 0.74})
Step:    2000, Reward: [-475.489 -475.489 -475.489] [101.000], Avg: [-482.581 -482.581 -482.581] (0.6690) <00:00:25> ({r_i: None, r_t: [-4837.901 -4837.901 -4837.901], eps: 0.669})
Step:    2500, Reward: [-467.911 -467.911 -467.911] [74.768], Avg: [-480.136 -480.136 -480.136] (0.6050) <00:00:32> ({r_i: None, r_t: [-4758.279 -4758.279 -4758.279], eps: 0.605})
Step:    3000, Reward: [-507.993 -507.993 -507.993] [138.002], Avg: [-484.115 -484.115 -484.115] (0.5472) <00:00:38> ({r_i: None, r_t: [-4893.437 -4893.437 -4893.437], eps: 0.547})
Step:    3500, Reward: [-506.257 -506.257 -506.257] [76.423], Avg: [-486.883 -486.883 -486.883] (0.4948) <00:00:45> ({r_i: None, r_t: [-4746.545 -4746.545 -4746.545], eps: 0.495})
Step:    4000, Reward: [-462.009 -462.009 -462.009] [95.998], Avg: [-484.119 -484.119 -484.119] (0.4475) <00:00:51> ({r_i: None, r_t: [-4933.889 -4933.889 -4933.889], eps: 0.448})
Step:    4500, Reward: [-475.651 -475.651 -475.651] [79.357], Avg: [-483.272 -483.272 -483.272] (0.4047) <00:00:58> ({r_i: None, r_t: [-4843.170 -4843.170 -4843.170], eps: 0.405})
Step:    5000, Reward: [-524.920 -524.920 -524.920] [118.775], Avg: [-487.059 -487.059 -487.059] (0.3660) <00:01:04> ({r_i: None, r_t: [-4862.428 -4862.428 -4862.428], eps: 0.366})
Step:    5500, Reward: [-465.521 -465.521 -465.521] [66.873], Avg: [-485.264 -485.264 -485.264] (0.3310) <00:01:10> ({r_i: None, r_t: [-4838.536 -4838.536 -4838.536], eps: 0.331})
Step:    6000, Reward: [-447.305 -447.305 -447.305] [59.799], Avg: [-482.344 -482.344 -482.344] (0.2994) <00:01:17> ({r_i: None, r_t: [-4749.189 -4749.189 -4749.189], eps: 0.299})
Step:    6500, Reward: [-480.747 -480.747 -480.747] [81.478], Avg: [-482.230 -482.230 -482.230] (0.2708) <00:01:23> ({r_i: None, r_t: [-4875.754 -4875.754 -4875.754], eps: 0.271})
Step:    7000, Reward: [-502.947 -502.947 -502.947] [118.162], Avg: [-483.611 -483.611 -483.611] (0.2449) <00:01:29> ({r_i: None, r_t: [-4763.667 -4763.667 -4763.667], eps: 0.245})
Step:    7500, Reward: [-468.654 -468.654 -468.654] [88.614], Avg: [-482.676 -482.676 -482.676] (0.2215) <00:01:36> ({r_i: None, r_t: [-4675.622 -4675.622 -4675.622], eps: 0.221})
Step:    8000, Reward: [-429.563 -429.563 -429.563] [82.401], Avg: [-479.552 -479.552 -479.552] (0.2003) <00:01:42> ({r_i: None, r_t: [-4784.475 -4784.475 -4784.475], eps: 0.2})
Step:    8500, Reward: [-493.296 -493.296 -493.296] [78.705], Avg: [-480.315 -480.315 -480.315] (0.1811) <00:01:49> ({r_i: None, r_t: [-4699.762 -4699.762 -4699.762], eps: 0.181})
Step:    9000, Reward: [-494.932 -494.932 -494.932] [92.905], Avg: [-481.085 -481.085 -481.085] (0.1638) <00:01:55> ({r_i: None, r_t: [-4578.326 -4578.326 -4578.326], eps: 0.164})
Step:    9500, Reward: [-433.692 -433.692 -433.692] [47.655], Avg: [-478.715 -478.715 -478.715] (0.1481) <00:02:01> ({r_i: None, r_t: [-4762.959 -4762.959 -4762.959], eps: 0.148})
Step:   10000, Reward: [-429.314 -429.314 -429.314] [71.464], Avg: [-476.363 -476.363 -476.363] (0.1340) <00:02:08> ({r_i: None, r_t: [-4735.543 -4735.543 -4735.543], eps: 0.134})
Step:   10500, Reward: [-452.445 -452.445 -452.445] [100.584], Avg: [-475.275 -475.275 -475.275] (0.1212) <00:02:14> ({r_i: None, r_t: [-4561.523 -4561.523 -4561.523], eps: 0.121})
Step:   11000, Reward: [-449.418 -449.418 -449.418] [86.311], Avg: [-474.151 -474.151 -474.151] (0.1096) <00:02:21> ({r_i: None, r_t: [-4477.019 -4477.019 -4477.019], eps: 0.11})
Step:   11500, Reward: [-501.357 -501.357 -501.357] [106.669], Avg: [-475.285 -475.285 -475.285] (0.1000) <00:02:27> ({r_i: None, r_t: [-4575.185 -4575.185 -4575.185], eps: 0.1})
Step:   12000, Reward: [-517.275 -517.275 -517.275] [97.228], Avg: [-476.964 -476.964 -476.964] (0.1000) <00:02:34> ({r_i: None, r_t: [-4463.181 -4463.181 -4463.181], eps: 0.1})
Step:   12500, Reward: [-455.576 -455.576 -455.576] [101.579], Avg: [-476.142 -476.142 -476.142] (0.1000) <00:02:40> ({r_i: None, r_t: [-4637.561 -4637.561 -4637.561], eps: 0.1})
Step:   13000, Reward: [-451.233 -451.233 -451.233] [90.167], Avg: [-475.219 -475.219 -475.219] (0.1000) <00:02:46> ({r_i: None, r_t: [-4518.316 -4518.316 -4518.316], eps: 0.1})
Step:   13500, Reward: [-456.175 -456.175 -456.175] [82.135], Avg: [-474.539 -474.539 -474.539] (0.1000) <00:02:53> ({r_i: None, r_t: [-4630.020 -4630.020 -4630.020], eps: 0.1})
Step:   14000, Reward: [-474.868 -474.868 -474.868] [81.804], Avg: [-474.550 -474.550 -474.550] (0.1000) <00:02:59> ({r_i: None, r_t: [-4408.708 -4408.708 -4408.708], eps: 0.1})
Step:   14500, Reward: [-442.124 -442.124 -442.124] [84.010], Avg: [-473.470 -473.470 -473.470] (0.1000) <00:03:05> ({r_i: None, r_t: [-4586.418 -4586.418 -4586.418], eps: 0.1})
Step:   15000, Reward: [-427.188 -427.188 -427.188] [56.840], Avg: [-471.977 -471.977 -471.977] (0.1000) <00:03:12> ({r_i: None, r_t: [-4574.659 -4574.659 -4574.659], eps: 0.1})
Step:   15500, Reward: [-416.294 -416.294 -416.294] [59.881], Avg: [-470.236 -470.236 -470.236] (0.1000) <00:03:18> ({r_i: None, r_t: [-4544.424 -4544.424 -4544.424], eps: 0.1})
Step:   16000, Reward: [-480.404 -480.404 -480.404] [88.856], Avg: [-470.545 -470.545 -470.545] (0.1000) <00:03:24> ({r_i: None, r_t: [-4547.140 -4547.140 -4547.140], eps: 0.1})
Step:   16500, Reward: [-451.443 -451.443 -451.443] [78.086], Avg: [-469.983 -469.983 -469.983] (0.1000) <00:03:31> ({r_i: None, r_t: [-4488.566 -4488.566 -4488.566], eps: 0.1})
Step:   17000, Reward: [-492.294 -492.294 -492.294] [119.212], Avg: [-470.620 -470.620 -470.620] (0.1000) <00:03:37> ({r_i: None, r_t: [-4361.604 -4361.604 -4361.604], eps: 0.1})
Step:   17500, Reward: [-445.731 -445.731 -445.731] [80.652], Avg: [-469.929 -469.929 -469.929] (0.1000) <00:03:44> ({r_i: None, r_t: [-4369.203 -4369.203 -4369.203], eps: 0.1})
Step:   18000, Reward: [-429.024 -429.024 -429.024] [77.050], Avg: [-468.823 -468.823 -468.823] (0.1000) <00:03:50> ({r_i: None, r_t: [-4513.453 -4513.453 -4513.453], eps: 0.1})
Step:   18500, Reward: [-415.134 -415.134 -415.134] [71.605], Avg: [-467.410 -467.410 -467.410] (0.1000) <00:03:57> ({r_i: None, r_t: [-4472.538 -4472.538 -4472.538], eps: 0.1})
Step:   19000, Reward: [-435.981 -435.981 -435.981] [80.109], Avg: [-466.605 -466.605 -466.605] (0.1000) <00:04:03> ({r_i: None, r_t: [-4604.490 -4604.490 -4604.490], eps: 0.1})
Step:   19500, Reward: [-458.609 -458.609 -458.609] [74.186], Avg: [-466.405 -466.405 -466.405] (0.1000) <00:04:09> ({r_i: None, r_t: [-4447.567 -4447.567 -4447.567], eps: 0.1})
Step:   20000, Reward: [-458.569 -458.569 -458.569] [75.748], Avg: [-466.214 -466.214 -466.214] (0.1000) <00:04:16> ({r_i: None, r_t: [-4436.320 -4436.320 -4436.320], eps: 0.1})
Step:   20500, Reward: [-419.514 -419.514 -419.514] [70.623], Avg: [-465.102 -465.102 -465.102] (0.1000) <00:04:22> ({r_i: None, r_t: [-4478.533 -4478.533 -4478.533], eps: 0.1})
Step:   21000, Reward: [-469.290 -469.290 -469.290] [111.810], Avg: [-465.199 -465.199 -465.199] (0.1000) <00:04:29> ({r_i: None, r_t: [-4558.364 -4558.364 -4558.364], eps: 0.1})
Step:   21500, Reward: [-469.483 -469.483 -469.483] [113.300], Avg: [-465.296 -465.296 -465.296] (0.1000) <00:04:35> ({r_i: None, r_t: [-4513.805 -4513.805 -4513.805], eps: 0.1})
Step:   22000, Reward: [-470.886 -470.886 -470.886] [98.787], Avg: [-465.421 -465.421 -465.421] (0.1000) <00:04:41> ({r_i: None, r_t: [-4405.274 -4405.274 -4405.274], eps: 0.1})
Step:   22500, Reward: [-438.827 -438.827 -438.827] [90.657], Avg: [-464.843 -464.843 -464.843] (0.1000) <00:04:48> ({r_i: None, r_t: [-4562.218 -4562.218 -4562.218], eps: 0.1})
Step:   23000, Reward: [-434.514 -434.514 -434.514] [64.783], Avg: [-464.197 -464.197 -464.197] (0.1000) <00:04:54> ({r_i: None, r_t: [-4477.235 -4477.235 -4477.235], eps: 0.1})
Step:   23500, Reward: [-446.312 -446.312 -446.312] [95.457], Avg: [-463.825 -463.825 -463.825] (0.1000) <00:05:01> ({r_i: None, r_t: [-4474.447 -4474.447 -4474.447], eps: 0.1})
Step:   24000, Reward: [-458.202 -458.202 -458.202] [115.933], Avg: [-463.710 -463.710 -463.710] (0.1000) <00:05:07> ({r_i: None, r_t: [-4484.761 -4484.761 -4484.761], eps: 0.1})
Step:   24500, Reward: [-461.687 -461.687 -461.687] [91.627], Avg: [-463.669 -463.669 -463.669] (0.1000) <00:05:13> ({r_i: None, r_t: [-4461.877 -4461.877 -4461.877], eps: 0.1})
Step:   25000, Reward: [-438.462 -438.462 -438.462] [100.737], Avg: [-463.175 -463.175 -463.175] (0.1000) <00:05:19> ({r_i: None, r_t: [-4542.817 -4542.817 -4542.817], eps: 0.1})
Step:   25500, Reward: [-455.314 -455.314 -455.314] [80.041], Avg: [-463.024 -463.024 -463.024] (0.1000) <00:05:26> ({r_i: None, r_t: [-4565.713 -4565.713 -4565.713], eps: 0.1})
Step:   26000, Reward: [-419.136 -419.136 -419.136] [59.429], Avg: [-462.196 -462.196 -462.196] (0.1000) <00:05:32> ({r_i: None, r_t: [-4448.349 -4448.349 -4448.349], eps: 0.1})
Step:   26500, Reward: [-462.037 -462.037 -462.037] [132.741], Avg: [-462.193 -462.193 -462.193] (0.1000) <00:05:39> ({r_i: None, r_t: [-4489.394 -4489.394 -4489.394], eps: 0.1})
Step:   27000, Reward: [-416.773 -416.773 -416.773] [65.820], Avg: [-461.367 -461.367 -461.367] (0.1000) <00:05:45> ({r_i: None, r_t: [-4455.457 -4455.457 -4455.457], eps: 0.1})
Step:   27500, Reward: [-456.193 -456.193 -456.193] [85.229], Avg: [-461.275 -461.275 -461.275] (0.1000) <00:05:52> ({r_i: None, r_t: [-4405.313 -4405.313 -4405.313], eps: 0.1})
Step:   28000, Reward: [-426.696 -426.696 -426.696] [64.787], Avg: [-460.668 -460.668 -460.668] (0.1000) <00:05:58> ({r_i: None, r_t: [-4507.210 -4507.210 -4507.210], eps: 0.1})
Step:   28500, Reward: [-466.682 -466.682 -466.682] [105.550], Avg: [-460.772 -460.772 -460.772] (0.1000) <00:06:05> ({r_i: None, r_t: [-4392.429 -4392.429 -4392.429], eps: 0.1})
Step:   29000, Reward: [-410.834 -410.834 -410.834] [74.144], Avg: [-459.925 -459.925 -459.925] (0.1000) <00:06:11> ({r_i: None, r_t: [-4436.161 -4436.161 -4436.161], eps: 0.1})
Step:   29500, Reward: [-434.226 -434.226 -434.226] [82.139], Avg: [-459.497 -459.497 -459.497] (0.1000) <00:06:18> ({r_i: None, r_t: [-4276.523 -4276.523 -4276.523], eps: 0.1})
Step:   30000, Reward: [-450.622 -450.622 -450.622] [89.462], Avg: [-459.352 -459.352 -459.352] (0.1000) <00:06:24> ({r_i: None, r_t: [-4427.463 -4427.463 -4427.463], eps: 0.1})
Step:   30500, Reward: [-443.832 -443.832 -443.832] [87.502], Avg: [-459.101 -459.101 -459.101] (0.1000) <00:06:30> ({r_i: None, r_t: [-4542.968 -4542.968 -4542.968], eps: 0.1})
Step:   31000, Reward: [-427.761 -427.761 -427.761] [103.391], Avg: [-458.604 -458.604 -458.604] (0.1000) <00:06:37> ({r_i: None, r_t: [-4598.297 -4598.297 -4598.297], eps: 0.1})
Step:   31500, Reward: [-421.929 -421.929 -421.929] [88.262], Avg: [-458.031 -458.031 -458.031] (0.1000) <00:06:43> ({r_i: None, r_t: [-4428.937 -4428.937 -4428.937], eps: 0.1})
Step:   32000, Reward: [-427.024 -427.024 -427.024] [89.152], Avg: [-457.554 -457.554 -457.554] (0.1000) <00:06:50> ({r_i: None, r_t: [-4529.788 -4529.788 -4529.788], eps: 0.1})
Step:   32500, Reward: [-459.273 -459.273 -459.273] [62.697], Avg: [-457.580 -457.580 -457.580] (0.1000) <00:06:56> ({r_i: None, r_t: [-4484.058 -4484.058 -4484.058], eps: 0.1})
Step:   33000, Reward: [-482.575 -482.575 -482.575] [91.020], Avg: [-457.953 -457.953 -457.953] (0.1000) <00:07:03> ({r_i: None, r_t: [-4476.058 -4476.058 -4476.058], eps: 0.1})
Step:   33500, Reward: [-417.640 -417.640 -417.640] [75.090], Avg: [-457.360 -457.360 -457.360] (0.1000) <00:07:09> ({r_i: None, r_t: [-4454.156 -4454.156 -4454.156], eps: 0.1})
Step:   34000, Reward: [-442.806 -442.806 -442.806] [115.818], Avg: [-457.149 -457.149 -457.149] (0.1000) <00:07:15> ({r_i: None, r_t: [-4544.600 -4544.600 -4544.600], eps: 0.1})
Step:   34500, Reward: [-446.648 -446.648 -446.648] [146.796], Avg: [-456.999 -456.999 -456.999] (0.1000) <00:07:21> ({r_i: None, r_t: [-4466.301 -4466.301 -4466.301], eps: 0.1})
Step:   35000, Reward: [-395.869 -395.869 -395.869] [80.325], Avg: [-456.138 -456.138 -456.138] (0.1000) <00:07:28> ({r_i: None, r_t: [-4353.981 -4353.981 -4353.981], eps: 0.1})
Step:   35500, Reward: [-437.871 -437.871 -437.871] [74.373], Avg: [-455.884 -455.884 -455.884] (0.1000) <00:07:34> ({r_i: None, r_t: [-4351.850 -4351.850 -4351.850], eps: 0.1})
Step:   36000, Reward: [-433.330 -433.330 -433.330] [88.884], Avg: [-455.575 -455.575 -455.575] (0.1000) <00:07:41> ({r_i: None, r_t: [-4472.080 -4472.080 -4472.080], eps: 0.1})
Step:   36500, Reward: [-439.224 -439.224 -439.224] [64.352], Avg: [-455.354 -455.354 -455.354] (0.1000) <00:07:47> ({r_i: None, r_t: [-4554.601 -4554.601 -4554.601], eps: 0.1})
Step:   37000, Reward: [-489.038 -489.038 -489.038] [150.768], Avg: [-455.804 -455.804 -455.804] (0.1000) <00:07:54> ({r_i: None, r_t: [-4518.143 -4518.143 -4518.143], eps: 0.1})
Step:   37500, Reward: [-464.033 -464.033 -464.033] [80.123], Avg: [-455.912 -455.912 -455.912] (0.1000) <00:08:00> ({r_i: None, r_t: [-4447.516 -4447.516 -4447.516], eps: 0.1})
Step:   38000, Reward: [-486.517 -486.517 -486.517] [134.462], Avg: [-456.309 -456.309 -456.309] (0.1000) <00:08:06> ({r_i: None, r_t: [-4420.924 -4420.924 -4420.924], eps: 0.1})
Step:   38500, Reward: [-416.633 -416.633 -416.633] [93.198], Avg: [-455.801 -455.801 -455.801] (0.1000) <00:08:12> ({r_i: None, r_t: [-4286.352 -4286.352 -4286.352], eps: 0.1})
Step:   39000, Reward: [-441.623 -441.623 -441.623] [100.702], Avg: [-455.621 -455.621 -455.621] (0.1000) <00:08:19> ({r_i: None, r_t: [-4407.520 -4407.520 -4407.520], eps: 0.1})
Step:   39500, Reward: [-453.861 -453.861 -453.861] [114.985], Avg: [-455.599 -455.599 -455.599] (0.1000) <00:08:25> ({r_i: None, r_t: [-4404.230 -4404.230 -4404.230], eps: 0.1})
Step:   40000, Reward: [-414.330 -414.330 -414.330] [73.941], Avg: [-455.090 -455.090 -455.090] (0.1000) <00:08:32> ({r_i: None, r_t: [-4498.979 -4498.979 -4498.979], eps: 0.1})
Step:   40500, Reward: [-453.017 -453.017 -453.017] [67.210], Avg: [-455.064 -455.064 -455.064] (0.1000) <00:08:38> ({r_i: None, r_t: [-4303.890 -4303.890 -4303.890], eps: 0.1})
Step:   41000, Reward: [-476.630 -476.630 -476.630] [96.829], Avg: [-455.324 -455.324 -455.324] (0.1000) <00:08:44> ({r_i: None, r_t: [-4364.249 -4364.249 -4364.249], eps: 0.1})
Step:   41500, Reward: [-429.219 -429.219 -429.219] [80.166], Avg: [-455.013 -455.013 -455.013] (0.1000) <00:08:51> ({r_i: None, r_t: [-4327.769 -4327.769 -4327.769], eps: 0.1})
Step:   42000, Reward: [-417.301 -417.301 -417.301] [82.761], Avg: [-454.570 -454.570 -454.570] (0.1000) <00:08:57> ({r_i: None, r_t: [-4411.865 -4411.865 -4411.865], eps: 0.1})
Step:   42500, Reward: [-456.879 -456.879 -456.879] [77.918], Avg: [-454.597 -454.597 -454.597] (0.1000) <00:09:04> ({r_i: None, r_t: [-4460.264 -4460.264 -4460.264], eps: 0.1})
Step:   43000, Reward: [-434.429 -434.429 -434.429] [82.710], Avg: [-454.365 -454.365 -454.365] (0.1000) <00:09:10> ({r_i: None, r_t: [-4401.549 -4401.549 -4401.549], eps: 0.1})
Step:   43500, Reward: [-426.514 -426.514 -426.514] [71.086], Avg: [-454.048 -454.048 -454.048] (0.1000) <00:09:16> ({r_i: None, r_t: [-4433.688 -4433.688 -4433.688], eps: 0.1})
Step:   44000, Reward: [-468.840 -468.840 -468.840] [81.159], Avg: [-454.215 -454.215 -454.215] (0.1000) <00:09:23> ({r_i: None, r_t: [-4446.566 -4446.566 -4446.566], eps: 0.1})
Step:   44500, Reward: [-435.702 -435.702 -435.702] [73.306], Avg: [-454.009 -454.009 -454.009] (0.1000) <00:09:29> ({r_i: None, r_t: [-4375.836 -4375.836 -4375.836], eps: 0.1})
Step:   45000, Reward: [-495.525 -495.525 -495.525] [104.133], Avg: [-454.465 -454.465 -454.465] (0.1000) <00:09:36> ({r_i: None, r_t: [-4517.315 -4517.315 -4517.315], eps: 0.1})
Step:   45500, Reward: [-428.502 -428.502 -428.502] [62.432], Avg: [-454.183 -454.183 -454.183] (0.1000) <00:09:42> ({r_i: None, r_t: [-4431.192 -4431.192 -4431.192], eps: 0.1})
Step:   46000, Reward: [-408.128 -408.128 -408.128] [81.043], Avg: [-453.688 -453.688 -453.688] (0.1000) <00:09:48> ({r_i: None, r_t: [-4473.382 -4473.382 -4473.382], eps: 0.1})
Step:   46500, Reward: [-491.209 -491.209 -491.209] [106.536], Avg: [-454.087 -454.087 -454.087] (0.1000) <00:09:55> ({r_i: None, r_t: [-4484.681 -4484.681 -4484.681], eps: 0.1})
Step:   47000, Reward: [-442.688 -442.688 -442.688] [118.043], Avg: [-453.967 -453.967 -453.967] (0.1000) <00:10:01> ({r_i: None, r_t: [-4472.625 -4472.625 -4472.625], eps: 0.1})
Step:   47500, Reward: [-443.485 -443.485 -443.485] [70.851], Avg: [-453.858 -453.858 -453.858] (0.1000) <00:10:08> ({r_i: None, r_t: [-4350.129 -4350.129 -4350.129], eps: 0.1})
Step:   48000, Reward: [-471.701 -471.701 -471.701] [125.889], Avg: [-454.042 -454.042 -454.042] (0.1000) <00:10:14> ({r_i: None, r_t: [-4380.105 -4380.105 -4380.105], eps: 0.1})
Step:   48500, Reward: [-418.104 -418.104 -418.104] [74.535], Avg: [-453.675 -453.675 -453.675] (0.1000) <00:10:20> ({r_i: None, r_t: [-4271.298 -4271.298 -4271.298], eps: 0.1})
Step:   49000, Reward: [-448.188 -448.188 -448.188] [100.843], Avg: [-453.619 -453.619 -453.619] (0.1000) <00:10:27> ({r_i: None, r_t: [-4454.629 -4454.629 -4454.629], eps: 0.1})
Step:   49500, Reward: [-428.811 -428.811 -428.811] [66.681], Avg: [-453.371 -453.371 -453.371] (0.1000) <00:10:34> ({r_i: None, r_t: [-4407.287 -4407.287 -4407.287], eps: 0.1})
Step:   50000, Reward: [-428.041 -428.041 -428.041] [95.021], Avg: [-453.121 -453.121 -453.121] (0.1000) <00:10:40> ({r_i: None, r_t: [-4233.849 -4233.849 -4233.849], eps: 0.1})
Step:   50500, Reward: [-419.608 -419.608 -419.608] [84.616], Avg: [-452.792 -452.792 -452.792] (0.1000) <00:10:47> ({r_i: None, r_t: [-4400.348 -4400.348 -4400.348], eps: 0.1})
Step:   51000, Reward: [-473.587 -473.587 -473.587] [104.310], Avg: [-452.994 -452.994 -452.994] (0.1000) <00:10:53> ({r_i: None, r_t: [-4407.853 -4407.853 -4407.853], eps: 0.1})
Step:   51500, Reward: [-441.932 -441.932 -441.932] [101.809], Avg: [-452.888 -452.888 -452.888] (0.1000) <00:11:00> ({r_i: None, r_t: [-4403.960 -4403.960 -4403.960], eps: 0.1})
Step:   52000, Reward: [-470.266 -470.266 -470.266] [91.427], Avg: [-453.053 -453.053 -453.053] (0.1000) <00:11:06> ({r_i: None, r_t: [-4356.999 -4356.999 -4356.999], eps: 0.1})
Step:   52500, Reward: [-452.070 -452.070 -452.070] [82.764], Avg: [-453.044 -453.044 -453.044] (0.1000) <00:11:12> ({r_i: None, r_t: [-4429.694 -4429.694 -4429.694], eps: 0.1})
Step:   53000, Reward: [-423.606 -423.606 -423.606] [87.811], Avg: [-452.769 -452.769 -452.769] (0.1000) <00:11:19> ({r_i: None, r_t: [-4650.788 -4650.788 -4650.788], eps: 0.1})
Step:   53500, Reward: [-469.517 -469.517 -469.517] [76.546], Avg: [-452.924 -452.924 -452.924] (0.1000) <00:11:25> ({r_i: None, r_t: [-4338.525 -4338.525 -4338.525], eps: 0.1})
Step:   54000, Reward: [-462.644 -462.644 -462.644] [103.420], Avg: [-453.013 -453.013 -453.013] (0.1000) <00:11:32> ({r_i: None, r_t: [-4378.573 -4378.573 -4378.573], eps: 0.1})
Step:   54500, Reward: [-429.581 -429.581 -429.581] [94.718], Avg: [-452.800 -452.800 -452.800] (0.1000) <00:11:38> ({r_i: None, r_t: [-4364.661 -4364.661 -4364.661], eps: 0.1})
Step:   55000, Reward: [-439.520 -439.520 -439.520] [94.749], Avg: [-452.680 -452.680 -452.680] (0.1000) <00:11:45> ({r_i: None, r_t: [-4367.012 -4367.012 -4367.012], eps: 0.1})
Step:   55500, Reward: [-462.964 -462.964 -462.964] [93.705], Avg: [-452.772 -452.772 -452.772] (0.1000) <00:11:51> ({r_i: None, r_t: [-4346.964 -4346.964 -4346.964], eps: 0.1})
Step:   56000, Reward: [-416.174 -416.174 -416.174] [72.733], Avg: [-452.448 -452.448 -452.448] (0.1000) <00:11:58> ({r_i: None, r_t: [-4336.263 -4336.263 -4336.263], eps: 0.1})
Step:   56500, Reward: [-433.255 -433.255 -433.255] [98.293], Avg: [-452.280 -452.280 -452.280] (0.1000) <00:12:04> ({r_i: None, r_t: [-4463.721 -4463.721 -4463.721], eps: 0.1})
Step:   57000, Reward: [-449.958 -449.958 -449.958] [108.952], Avg: [-452.260 -452.260 -452.260] (0.1000) <00:12:11> ({r_i: None, r_t: [-4492.430 -4492.430 -4492.430], eps: 0.1})
Step:   57500, Reward: [-446.069 -446.069 -446.069] [65.237], Avg: [-452.206 -452.206 -452.206] (0.1000) <00:12:17> ({r_i: None, r_t: [-4318.128 -4318.128 -4318.128], eps: 0.1})
Step:   58000, Reward: [-446.128 -446.128 -446.128] [115.834], Avg: [-452.154 -452.154 -452.154] (0.1000) <00:12:23> ({r_i: None, r_t: [-4447.138 -4447.138 -4447.138], eps: 0.1})
Step:   58500, Reward: [-418.507 -418.507 -418.507] [60.873], Avg: [-451.869 -451.869 -451.869] (0.1000) <00:12:30> ({r_i: None, r_t: [-4419.157 -4419.157 -4419.157], eps: 0.1})
Step:   59000, Reward: [-401.613 -401.613 -401.613] [90.962], Avg: [-451.447 -451.447 -451.447] (0.1000) <00:12:36> ({r_i: None, r_t: [-4391.170 -4391.170 -4391.170], eps: 0.1})
Step:   59500, Reward: [-470.143 -470.143 -470.143] [107.469], Avg: [-451.603 -451.603 -451.603] (0.1000) <00:12:43> ({r_i: None, r_t: [-4371.109 -4371.109 -4371.109], eps: 0.1})
Step:   60000, Reward: [-401.375 -401.375 -401.375] [85.320], Avg: [-451.188 -451.188 -451.188] (0.1000) <00:12:49> ({r_i: None, r_t: [-4422.952 -4422.952 -4422.952], eps: 0.1})
Step:   60500, Reward: [-467.236 -467.236 -467.236] [61.398], Avg: [-451.319 -451.319 -451.319] (0.1000) <00:12:56> ({r_i: None, r_t: [-4395.624 -4395.624 -4395.624], eps: 0.1})
Step:   61000, Reward: [-434.536 -434.536 -434.536] [76.497], Avg: [-451.183 -451.183 -451.183] (0.1000) <00:13:02> ({r_i: None, r_t: [-4366.281 -4366.281 -4366.281], eps: 0.1})
Step:   61500, Reward: [-491.521 -491.521 -491.521] [85.243], Avg: [-451.508 -451.508 -451.508] (0.1000) <00:13:09> ({r_i: None, r_t: [-4424.479 -4424.479 -4424.479], eps: 0.1})
Step:   62000, Reward: [-449.220 -449.220 -449.220] [86.477], Avg: [-451.490 -451.490 -451.490] (0.1000) <00:13:15> ({r_i: None, r_t: [-4412.010 -4412.010 -4412.010], eps: 0.1})
Step:   62500, Reward: [-477.351 -477.351 -477.351] [93.404], Avg: [-451.695 -451.695 -451.695] (0.1000) <00:13:21> ({r_i: None, r_t: [-4437.270 -4437.270 -4437.270], eps: 0.1})
Step:   63000, Reward: [-444.912 -444.912 -444.912] [94.333], Avg: [-451.641 -451.641 -451.641] (0.1000) <00:13:28> ({r_i: None, r_t: [-4455.484 -4455.484 -4455.484], eps: 0.1})
Step:   63500, Reward: [-425.364 -425.364 -425.364] [76.085], Avg: [-451.436 -451.436 -451.436] (0.1000) <00:13:34> ({r_i: None, r_t: [-4417.273 -4417.273 -4417.273], eps: 0.1})
Step:   64000, Reward: [-410.683 -410.683 -410.683] [94.042], Avg: [-451.120 -451.120 -451.120] (0.1000) <00:13:41> ({r_i: None, r_t: [-4398.167 -4398.167 -4398.167], eps: 0.1})
Step:   64500, Reward: [-400.661 -400.661 -400.661] [51.705], Avg: [-450.732 -450.732 -450.732] (0.1000) <00:13:47> ({r_i: None, r_t: [-4444.136 -4444.136 -4444.136], eps: 0.1})
Step:   65000, Reward: [-451.995 -451.995 -451.995] [90.831], Avg: [-450.742 -450.742 -450.742] (0.1000) <00:13:54> ({r_i: None, r_t: [-4367.110 -4367.110 -4367.110], eps: 0.1})
Step:   65500, Reward: [-472.168 -472.168 -472.168] [128.525], Avg: [-450.904 -450.904 -450.904] (0.1000) <00:14:00> ({r_i: None, r_t: [-4348.126 -4348.126 -4348.126], eps: 0.1})
Step:   66000, Reward: [-448.988 -448.988 -448.988] [81.325], Avg: [-450.890 -450.890 -450.890] (0.1000) <00:14:07> ({r_i: None, r_t: [-4381.296 -4381.296 -4381.296], eps: 0.1})
Step:   66500, Reward: [-406.369 -406.369 -406.369] [65.276], Avg: [-450.557 -450.557 -450.557] (0.1000) <00:14:13> ({r_i: None, r_t: [-4377.143 -4377.143 -4377.143], eps: 0.1})
Step:   67000, Reward: [-417.144 -417.144 -417.144] [76.623], Avg: [-450.310 -450.310 -450.310] (0.1000) <00:14:20> ({r_i: None, r_t: [-4400.843 -4400.843 -4400.843], eps: 0.1})
Step:   67500, Reward: [-470.415 -470.415 -470.415] [97.793], Avg: [-450.458 -450.458 -450.458] (0.1000) <00:14:26> ({r_i: None, r_t: [-4388.811 -4388.811 -4388.811], eps: 0.1})
Step:   68000, Reward: [-416.566 -416.566 -416.566] [100.848], Avg: [-450.210 -450.210 -450.210] (0.1000) <00:14:32> ({r_i: None, r_t: [-4406.877 -4406.877 -4406.877], eps: 0.1})
Step:   68500, Reward: [-453.546 -453.546 -453.546] [83.367], Avg: [-450.235 -450.235 -450.235] (0.1000) <00:14:39> ({r_i: None, r_t: [-4480.080 -4480.080 -4480.080], eps: 0.1})
Step:   69000, Reward: [-438.430 -438.430 -438.430] [73.525], Avg: [-450.150 -450.150 -450.150] (0.1000) <00:14:45> ({r_i: None, r_t: [-4319.385 -4319.385 -4319.385], eps: 0.1})
Step:   69500, Reward: [-448.280 -448.280 -448.280] [96.100], Avg: [-450.136 -450.136 -450.136] (0.1000) <00:14:52> ({r_i: None, r_t: [-4370.168 -4370.168 -4370.168], eps: 0.1})
Step:   70000, Reward: [-503.802 -503.802 -503.802] [121.442], Avg: [-450.517 -450.517 -450.517] (0.1000) <00:14:58> ({r_i: None, r_t: [-4475.359 -4475.359 -4475.359], eps: 0.1})
Step:   70500, Reward: [-437.440 -437.440 -437.440] [111.253], Avg: [-450.425 -450.425 -450.425] (0.1000) <00:15:05> ({r_i: None, r_t: [-4452.211 -4452.211 -4452.211], eps: 0.1})
Step:   71000, Reward: [-392.522 -392.522 -392.522] [81.531], Avg: [-450.020 -450.020 -450.020] (0.1000) <00:15:11> ({r_i: None, r_t: [-4403.497 -4403.497 -4403.497], eps: 0.1})
Step:   71500, Reward: [-478.754 -478.754 -478.754] [87.970], Avg: [-450.219 -450.219 -450.219] (0.1000) <00:15:18> ({r_i: None, r_t: [-4313.080 -4313.080 -4313.080], eps: 0.1})
Step:   72000, Reward: [-427.386 -427.386 -427.386] [90.032], Avg: [-450.062 -450.062 -450.062] (0.1000) <00:15:24> ({r_i: None, r_t: [-4258.979 -4258.979 -4258.979], eps: 0.1})
Step:   72500, Reward: [-429.094 -429.094 -429.094] [97.802], Avg: [-449.918 -449.918 -449.918] (0.1000) <00:15:31> ({r_i: None, r_t: [-4424.022 -4424.022 -4424.022], eps: 0.1})
Step:   73000, Reward: [-424.269 -424.269 -424.269] [100.525], Avg: [-449.744 -449.744 -449.744] (0.1000) <00:15:37> ({r_i: None, r_t: [-4422.114 -4422.114 -4422.114], eps: 0.1})
Step:   73500, Reward: [-445.909 -445.909 -445.909] [98.529], Avg: [-449.718 -449.718 -449.718] (0.1000) <00:15:44> ({r_i: None, r_t: [-4532.764 -4532.764 -4532.764], eps: 0.1})
Step:   74000, Reward: [-427.584 -427.584 -427.584] [63.850], Avg: [-449.569 -449.569 -449.569] (0.1000) <00:15:50> ({r_i: None, r_t: [-4415.567 -4415.567 -4415.567], eps: 0.1})
Step:   74500, Reward: [-468.509 -468.509 -468.509] [85.946], Avg: [-449.696 -449.696 -449.696] (0.1000) <00:15:57> ({r_i: None, r_t: [-4504.041 -4504.041 -4504.041], eps: 0.1})
Step:   75000, Reward: [-462.661 -462.661 -462.661] [133.318], Avg: [-449.782 -449.782 -449.782] (0.1000) <00:16:04> ({r_i: None, r_t: [-4439.071 -4439.071 -4439.071], eps: 0.1})
Step:   75500, Reward: [-454.874 -454.874 -454.874] [88.012], Avg: [-449.815 -449.815 -449.815] (0.1000) <00:16:10> ({r_i: None, r_t: [-4450.275 -4450.275 -4450.275], eps: 0.1})
Step:   76000, Reward: [-469.147 -469.147 -469.147] [117.029], Avg: [-449.941 -449.941 -449.941] (0.1000) <00:16:16> ({r_i: None, r_t: [-4435.513 -4435.513 -4435.513], eps: 0.1})
Step:   76500, Reward: [-414.314 -414.314 -414.314] [98.572], Avg: [-449.710 -449.710 -449.710] (0.1000) <00:16:22> ({r_i: None, r_t: [-4344.016 -4344.016 -4344.016], eps: 0.1})
Step:   77000, Reward: [-441.084 -441.084 -441.084] [75.564], Avg: [-449.654 -449.654 -449.654] (0.1000) <00:16:29> ({r_i: None, r_t: [-4338.350 -4338.350 -4338.350], eps: 0.1})
Step:   77500, Reward: [-407.459 -407.459 -407.459] [82.792], Avg: [-449.384 -449.384 -449.384] (0.1000) <00:16:35> ({r_i: None, r_t: [-4480.823 -4480.823 -4480.823], eps: 0.1})
Step:   78000, Reward: [-427.966 -427.966 -427.966] [94.962], Avg: [-449.247 -449.247 -449.247] (0.1000) <00:16:42> ({r_i: None, r_t: [-4420.308 -4420.308 -4420.308], eps: 0.1})
Step:   78500, Reward: [-474.701 -474.701 -474.701] [122.355], Avg: [-449.409 -449.409 -449.409] (0.1000) <00:16:48> ({r_i: None, r_t: [-4261.014 -4261.014 -4261.014], eps: 0.1})
Step:   79000, Reward: [-424.456 -424.456 -424.456] [85.123], Avg: [-449.252 -449.252 -449.252] (0.1000) <00:16:55> ({r_i: None, r_t: [-4367.190 -4367.190 -4367.190], eps: 0.1})
Step:   79500, Reward: [-470.906 -470.906 -470.906] [68.706], Avg: [-449.387 -449.387 -449.387] (0.1000) <00:17:01> ({r_i: None, r_t: [-4452.168 -4452.168 -4452.168], eps: 0.1})
Step:   80000, Reward: [-462.966 -462.966 -462.966] [138.139], Avg: [-449.471 -449.471 -449.471] (0.1000) <00:17:08> ({r_i: None, r_t: [-4417.440 -4417.440 -4417.440], eps: 0.1})
Step:   80500, Reward: [-447.577 -447.577 -447.577] [76.425], Avg: [-449.460 -449.460 -449.460] (0.1000) <00:17:14> ({r_i: None, r_t: [-4444.969 -4444.969 -4444.969], eps: 0.1})
Step:   81000, Reward: [-423.243 -423.243 -423.243] [74.757], Avg: [-449.299 -449.299 -449.299] (0.1000) <00:17:21> ({r_i: None, r_t: [-4394.041 -4394.041 -4394.041], eps: 0.1})
Step:   81500, Reward: [-461.596 -461.596 -461.596] [98.579], Avg: [-449.374 -449.374 -449.374] (0.1000) <00:17:27> ({r_i: None, r_t: [-4459.930 -4459.930 -4459.930], eps: 0.1})
Step:   82000, Reward: [-468.064 -468.064 -468.064] [97.131], Avg: [-449.487 -449.487 -449.487] (0.1000) <00:17:33> ({r_i: None, r_t: [-4291.740 -4291.740 -4291.740], eps: 0.1})
Step:   82500, Reward: [-415.458 -415.458 -415.458] [84.121], Avg: [-449.282 -449.282 -449.282] (0.1000) <00:17:40> ({r_i: None, r_t: [-4320.493 -4320.493 -4320.493], eps: 0.1})
Step:   83000, Reward: [-503.472 -503.472 -503.472] [99.710], Avg: [-449.607 -449.607 -449.607] (0.1000) <00:17:46> ({r_i: None, r_t: [-4410.775 -4410.775 -4410.775], eps: 0.1})
Step:   83500, Reward: [-443.249 -443.249 -443.249] [87.757], Avg: [-449.569 -449.569 -449.569] (0.1000) <00:17:53> ({r_i: None, r_t: [-4474.345 -4474.345 -4474.345], eps: 0.1})
Step:   84000, Reward: [-426.477 -426.477 -426.477] [107.961], Avg: [-449.432 -449.432 -449.432] (0.1000) <00:17:59> ({r_i: None, r_t: [-4431.887 -4431.887 -4431.887], eps: 0.1})
Step:   84500, Reward: [-414.675 -414.675 -414.675] [92.682], Avg: [-449.228 -449.228 -449.228] (0.1000) <00:18:06> ({r_i: None, r_t: [-4418.201 -4418.201 -4418.201], eps: 0.1})
Step:   85000, Reward: [-459.510 -459.510 -459.510] [135.171], Avg: [-449.288 -449.288 -449.288] (0.1000) <00:18:12> ({r_i: None, r_t: [-4351.497 -4351.497 -4351.497], eps: 0.1})
Step:   85500, Reward: [-471.264 -471.264 -471.264] [105.979], Avg: [-449.416 -449.416 -449.416] (0.1000) <00:18:19> ({r_i: None, r_t: [-4316.388 -4316.388 -4316.388], eps: 0.1})
Step:   86000, Reward: [-487.138 -487.138 -487.138] [100.980], Avg: [-449.634 -449.634 -449.634] (0.1000) <00:18:25> ({r_i: None, r_t: [-4446.384 -4446.384 -4446.384], eps: 0.1})
Step:   86500, Reward: [-428.215 -428.215 -428.215] [58.508], Avg: [-449.510 -449.510 -449.510] (0.1000) <00:18:31> ({r_i: None, r_t: [-4426.962 -4426.962 -4426.962], eps: 0.1})
Step:   87000, Reward: [-450.270 -450.270 -450.270] [67.478], Avg: [-449.515 -449.515 -449.515] (0.1000) <00:18:37> ({r_i: None, r_t: [-4701.063 -4701.063 -4701.063], eps: 0.1})
Step:   87500, Reward: [-412.154 -412.154 -412.154] [95.500], Avg: [-449.303 -449.303 -449.303] (0.1000) <00:18:44> ({r_i: None, r_t: [-4481.898 -4481.898 -4481.898], eps: 0.1})
Step:   88000, Reward: [-440.533 -440.533 -440.533] [66.993], Avg: [-449.253 -449.253 -449.253] (0.1000) <00:18:50> ({r_i: None, r_t: [-4318.447 -4318.447 -4318.447], eps: 0.1})
Step:   88500, Reward: [-445.675 -445.675 -445.675] [77.812], Avg: [-449.233 -449.233 -449.233] (0.1000) <00:18:57> ({r_i: None, r_t: [-4346.474 -4346.474 -4346.474], eps: 0.1})
Step:   89000, Reward: [-435.939 -435.939 -435.939] [54.656], Avg: [-449.159 -449.159 -449.159] (0.1000) <00:19:03> ({r_i: None, r_t: [-4426.534 -4426.534 -4426.534], eps: 0.1})
Step:   89500, Reward: [-430.987 -430.987 -430.987] [61.702], Avg: [-449.058 -449.058 -449.058] (0.1000) <00:19:10> ({r_i: None, r_t: [-4354.437 -4354.437 -4354.437], eps: 0.1})
Step:   90000, Reward: [-470.259 -470.259 -470.259] [70.688], Avg: [-449.175 -449.175 -449.175] (0.1000) <00:19:16> ({r_i: None, r_t: [-4500.588 -4500.588 -4500.588], eps: 0.1})
Step:   90500, Reward: [-487.366 -487.366 -487.366] [129.263], Avg: [-449.385 -449.385 -449.385] (0.1000) <00:19:23> ({r_i: None, r_t: [-4471.140 -4471.140 -4471.140], eps: 0.1})
Step:   91000, Reward: [-437.299 -437.299 -437.299] [65.521], Avg: [-449.319 -449.319 -449.319] (0.1000) <00:19:29> ({r_i: None, r_t: [-4493.127 -4493.127 -4493.127], eps: 0.1})
Step:   91500, Reward: [-409.891 -409.891 -409.891] [53.717], Avg: [-449.104 -449.104 -449.104] (0.1000) <00:19:36> ({r_i: None, r_t: [-4353.681 -4353.681 -4353.681], eps: 0.1})
Step:   92000, Reward: [-398.149 -398.149 -398.149] [90.434], Avg: [-448.829 -448.829 -448.829] (0.1000) <00:19:42> ({r_i: None, r_t: [-4443.292 -4443.292 -4443.292], eps: 0.1})
Step:   92500, Reward: [-474.337 -474.337 -474.337] [84.591], Avg: [-448.966 -448.966 -448.966] (0.1000) <00:19:49> ({r_i: None, r_t: [-4439.078 -4439.078 -4439.078], eps: 0.1})
Step:   93000, Reward: [-445.334 -445.334 -445.334] [87.751], Avg: [-448.947 -448.947 -448.947] (0.1000) <00:19:56> ({r_i: None, r_t: [-4525.858 -4525.858 -4525.858], eps: 0.1})
Step:   93500, Reward: [-440.271 -440.271 -440.271] [91.663], Avg: [-448.900 -448.900 -448.900] (0.1000) <00:20:02> ({r_i: None, r_t: [-4258.852 -4258.852 -4258.852], eps: 0.1})
Step:   94000, Reward: [-457.381 -457.381 -457.381] [92.960], Avg: [-448.945 -448.945 -448.945] (0.1000) <00:20:09> ({r_i: None, r_t: [-4367.046 -4367.046 -4367.046], eps: 0.1})
Step:   94500, Reward: [-413.119 -413.119 -413.119] [69.432], Avg: [-448.757 -448.757 -448.757] (0.1000) <00:20:15> ({r_i: None, r_t: [-4404.927 -4404.927 -4404.927], eps: 0.1})
Step:   95000, Reward: [-427.954 -427.954 -427.954] [80.228], Avg: [-448.648 -448.648 -448.648] (0.1000) <00:20:21> ({r_i: None, r_t: [-4550.038 -4550.038 -4550.038], eps: 0.1})
Step:   95500, Reward: [-420.675 -420.675 -420.675] [88.074], Avg: [-448.502 -448.502 -448.502] (0.1000) <00:20:28> ({r_i: None, r_t: [-4436.475 -4436.475 -4436.475], eps: 0.1})
Step:   96000, Reward: [-431.477 -431.477 -431.477] [87.081], Avg: [-448.414 -448.414 -448.414] (0.1000) <00:20:34> ({r_i: None, r_t: [-4402.563 -4402.563 -4402.563], eps: 0.1})
Step:   96500, Reward: [-461.265 -461.265 -461.265] [98.355], Avg: [-448.480 -448.480 -448.480] (0.1000) <00:20:41> ({r_i: None, r_t: [-4320.041 -4320.041 -4320.041], eps: 0.1})
Step:   97000, Reward: [-400.927 -400.927 -400.927] [74.554], Avg: [-448.236 -448.236 -448.236] (0.1000) <00:20:47> ({r_i: None, r_t: [-4404.953 -4404.953 -4404.953], eps: 0.1})
Step:   97500, Reward: [-432.544 -432.544 -432.544] [77.939], Avg: [-448.156 -448.156 -448.156] (0.1000) <00:20:53> ({r_i: None, r_t: [-4316.252 -4316.252 -4316.252], eps: 0.1})
Step:   98000, Reward: [-433.286 -433.286 -433.286] [79.327], Avg: [-448.081 -448.081 -448.081] (0.1000) <00:21:00> ({r_i: None, r_t: [-4467.121 -4467.121 -4467.121], eps: 0.1})
Step:   98500, Reward: [-414.007 -414.007 -414.007] [77.207], Avg: [-447.909 -447.909 -447.909] (0.1000) <00:21:06> ({r_i: None, r_t: [-4362.871 -4362.871 -4362.871], eps: 0.1})
Step:   99000, Reward: [-422.288 -422.288 -422.288] [69.263], Avg: [-447.780 -447.780 -447.780] (0.1000) <00:21:13> ({r_i: None, r_t: [-4456.458 -4456.458 -4456.458], eps: 0.1})
Step:   99500, Reward: [-415.397 -415.397 -415.397] [74.100], Avg: [-447.618 -447.618 -447.618] (0.1000) <00:21:20> ({r_i: None, r_t: [-4540.959 -4540.959 -4540.959], eps: 0.1})
Step:  100000, Reward: [-413.293 -413.293 -413.293] [72.630], Avg: [-447.447 -447.447 -447.447] (0.1000) <00:21:26> ({r_i: None, r_t: [-4527.650 -4527.650 -4527.650], eps: 0.1})
Step:  100500, Reward: [-432.799 -432.799 -432.799] [117.473], Avg: [-447.375 -447.375 -447.375] (0.1000) <00:21:32> ({r_i: None, r_t: [-4293.841 -4293.841 -4293.841], eps: 0.1})
Step:  101000, Reward: [-402.556 -402.556 -402.556] [97.117], Avg: [-447.154 -447.154 -447.154] (0.1000) <00:21:39> ({r_i: None, r_t: [-4397.190 -4397.190 -4397.190], eps: 0.1})
Step:  101500, Reward: [-441.514 -441.514 -441.514] [85.690], Avg: [-447.126 -447.126 -447.126] (0.1000) <00:21:46> ({r_i: None, r_t: [-4250.128 -4250.128 -4250.128], eps: 0.1})
Step:  102000, Reward: [-471.666 -471.666 -471.666] [88.295], Avg: [-447.246 -447.246 -447.246] (0.1000) <00:21:53> ({r_i: None, r_t: [-4349.912 -4349.912 -4349.912], eps: 0.1})
Step:  102500, Reward: [-423.332 -423.332 -423.332] [48.457], Avg: [-447.130 -447.130 -447.130] (0.1000) <00:21:59> ({r_i: None, r_t: [-4441.157 -4441.157 -4441.157], eps: 0.1})
Step:  103000, Reward: [-451.213 -451.213 -451.213] [105.549], Avg: [-447.150 -447.150 -447.150] (0.1000) <00:22:06> ({r_i: None, r_t: [-4433.745 -4433.745 -4433.745], eps: 0.1})
Step:  103500, Reward: [-432.368 -432.368 -432.368] [75.269], Avg: [-447.079 -447.079 -447.079] (0.1000) <00:22:13> ({r_i: None, r_t: [-4441.632 -4441.632 -4441.632], eps: 0.1})
Step:  104000, Reward: [-429.113 -429.113 -429.113] [64.487], Avg: [-446.993 -446.993 -446.993] (0.1000) <00:22:19> ({r_i: None, r_t: [-4329.456 -4329.456 -4329.456], eps: 0.1})
Step:  104500, Reward: [-424.025 -424.025 -424.025] [88.118], Avg: [-446.883 -446.883 -446.883] (0.1000) <00:22:26> ({r_i: None, r_t: [-4344.825 -4344.825 -4344.825], eps: 0.1})
Step:  105000, Reward: [-449.394 -449.394 -449.394] [74.541], Avg: [-446.895 -446.895 -446.895] (0.1000) <00:22:32> ({r_i: None, r_t: [-4314.274 -4314.274 -4314.274], eps: 0.1})
Step:  105500, Reward: [-448.117 -448.117 -448.117] [89.346], Avg: [-446.901 -446.901 -446.901] (0.1000) <00:22:39> ({r_i: None, r_t: [-4551.498 -4551.498 -4551.498], eps: 0.1})
Step:  106000, Reward: [-402.659 -402.659 -402.659] [52.590], Avg: [-446.693 -446.693 -446.693] (0.1000) <00:22:45> ({r_i: None, r_t: [-4365.215 -4365.215 -4365.215], eps: 0.1})
Step:  106500, Reward: [-432.535 -432.535 -432.535] [59.970], Avg: [-446.627 -446.627 -446.627] (0.1000) <00:22:52> ({r_i: None, r_t: [-4337.755 -4337.755 -4337.755], eps: 0.1})
Step:  107000, Reward: [-477.967 -477.967 -477.967] [114.633], Avg: [-446.773 -446.773 -446.773] (0.1000) <00:22:59> ({r_i: None, r_t: [-4455.002 -4455.002 -4455.002], eps: 0.1})
Step:  107500, Reward: [-419.554 -419.554 -419.554] [79.788], Avg: [-446.647 -446.647 -446.647] (0.1000) <00:23:05> ({r_i: None, r_t: [-4432.333 -4432.333 -4432.333], eps: 0.1})
Step:  108000, Reward: [-429.854 -429.854 -429.854] [73.394], Avg: [-446.569 -446.569 -446.569] (0.1000) <00:23:12> ({r_i: None, r_t: [-4467.881 -4467.881 -4467.881], eps: 0.1})
Step:  108500, Reward: [-416.583 -416.583 -416.583] [90.594], Avg: [-446.432 -446.432 -446.432] (0.1000) <00:23:18> ({r_i: None, r_t: [-4419.820 -4419.820 -4419.820], eps: 0.1})
Step:  109000, Reward: [-392.720 -392.720 -392.720] [74.142], Avg: [-446.187 -446.187 -446.187] (0.1000) <00:23:25> ({r_i: None, r_t: [-4347.332 -4347.332 -4347.332], eps: 0.1})
Step:  109500, Reward: [-418.068 -418.068 -418.068] [89.442], Avg: [-446.059 -446.059 -446.059] (0.1000) <00:23:32> ({r_i: None, r_t: [-4369.639 -4369.639 -4369.639], eps: 0.1})
Step:  110000, Reward: [-445.325 -445.325 -445.325] [95.241], Avg: [-446.055 -446.055 -446.055] (0.1000) <00:23:38> ({r_i: None, r_t: [-4335.324 -4335.324 -4335.324], eps: 0.1})
Step:  110500, Reward: [-431.320 -431.320 -431.320] [85.017], Avg: [-445.989 -445.989 -445.989] (0.1000) <00:23:45> ({r_i: None, r_t: [-4409.039 -4409.039 -4409.039], eps: 0.1})
Step:  111000, Reward: [-448.529 -448.529 -448.529] [75.912], Avg: [-446.000 -446.000 -446.000] (0.1000) <00:23:51> ({r_i: None, r_t: [-4316.667 -4316.667 -4316.667], eps: 0.1})
Step:  111500, Reward: [-411.032 -411.032 -411.032] [55.403], Avg: [-445.844 -445.844 -445.844] (0.1000) <00:23:58> ({r_i: None, r_t: [-4466.275 -4466.275 -4466.275], eps: 0.1})
Step:  112000, Reward: [-436.016 -436.016 -436.016] [76.632], Avg: [-445.801 -445.801 -445.801] (0.1000) <00:24:05> ({r_i: None, r_t: [-4283.958 -4283.958 -4283.958], eps: 0.1})
Step:  112500, Reward: [-444.570 -444.570 -444.570] [108.471], Avg: [-445.795 -445.795 -445.795] (0.1000) <00:24:11> ({r_i: None, r_t: [-4402.072 -4402.072 -4402.072], eps: 0.1})
Step:  113000, Reward: [-476.271 -476.271 -476.271] [79.919], Avg: [-445.930 -445.930 -445.930] (0.1000) <00:24:18> ({r_i: None, r_t: [-4414.255 -4414.255 -4414.255], eps: 0.1})
Step:  113500, Reward: [-420.444 -420.444 -420.444] [83.263], Avg: [-445.818 -445.818 -445.818] (0.1000) <00:24:24> ({r_i: None, r_t: [-4355.006 -4355.006 -4355.006], eps: 0.1})
Step:  114000, Reward: [-441.730 -441.730 -441.730] [101.991], Avg: [-445.800 -445.800 -445.800] (0.1000) <00:24:31> ({r_i: None, r_t: [-4397.489 -4397.489 -4397.489], eps: 0.1})
Step:  114500, Reward: [-430.532 -430.532 -430.532] [122.282], Avg: [-445.733 -445.733 -445.733] (0.1000) <00:24:38> ({r_i: None, r_t: [-4363.629 -4363.629 -4363.629], eps: 0.1})
Step:  115000, Reward: [-406.926 -406.926 -406.926] [82.911], Avg: [-445.566 -445.566 -445.566] (0.1000) <00:24:44> ({r_i: None, r_t: [-4544.730 -4544.730 -4544.730], eps: 0.1})
Step:  115500, Reward: [-442.044 -442.044 -442.044] [74.387], Avg: [-445.550 -445.550 -445.550] (0.1000) <00:24:51> ({r_i: None, r_t: [-4431.283 -4431.283 -4431.283], eps: 0.1})
Step:  116000, Reward: [-425.042 -425.042 -425.042] [82.625], Avg: [-445.462 -445.462 -445.462] (0.1000) <00:24:58> ({r_i: None, r_t: [-4320.023 -4320.023 -4320.023], eps: 0.1})
Step:  116500, Reward: [-438.307 -438.307 -438.307] [58.470], Avg: [-445.432 -445.432 -445.432] (0.1000) <00:25:04> ({r_i: None, r_t: [-4294.767 -4294.767 -4294.767], eps: 0.1})
Step:  117000, Reward: [-468.950 -468.950 -468.950] [138.583], Avg: [-445.532 -445.532 -445.532] (0.1000) <00:25:11> ({r_i: None, r_t: [-4321.743 -4321.743 -4321.743], eps: 0.1})
Step:  117500, Reward: [-408.846 -408.846 -408.846] [88.129], Avg: [-445.376 -445.376 -445.376] (0.1000) <00:25:17> ({r_i: None, r_t: [-4342.774 -4342.774 -4342.774], eps: 0.1})
Step:  118000, Reward: [-420.247 -420.247 -420.247] [86.651], Avg: [-445.270 -445.270 -445.270] (0.1000) <00:25:24> ({r_i: None, r_t: [-4449.355 -4449.355 -4449.355], eps: 0.1})
Step:  118500, Reward: [-461.667 -461.667 -461.667] [123.901], Avg: [-445.339 -445.339 -445.339] (0.1000) <00:25:31> ({r_i: None, r_t: [-4343.047 -4343.047 -4343.047], eps: 0.1})
Step:  119000, Reward: [-427.084 -427.084 -427.084] [78.413], Avg: [-445.263 -445.263 -445.263] (0.1000) <00:25:38> ({r_i: None, r_t: [-4445.101 -4445.101 -4445.101], eps: 0.1})
Step:  119500, Reward: [-428.095 -428.095 -428.095] [93.718], Avg: [-445.191 -445.191 -445.191] (0.1000) <00:25:44> ({r_i: None, r_t: [-4341.802 -4341.802 -4341.802], eps: 0.1})
Step:  120000, Reward: [-438.534 -438.534 -438.534] [60.161], Avg: [-445.164 -445.164 -445.164] (0.1000) <00:25:50> ({r_i: None, r_t: [-4375.704 -4375.704 -4375.704], eps: 0.1})
Step:  120500, Reward: [-406.193 -406.193 -406.193] [76.477], Avg: [-445.003 -445.003 -445.003] (0.1000) <00:25:57> ({r_i: None, r_t: [-4425.562 -4425.562 -4425.562], eps: 0.1})
Step:  121000, Reward: [-407.354 -407.354 -407.354] [88.582], Avg: [-444.848 -444.848 -444.848] (0.1000) <00:26:04> ({r_i: None, r_t: [-4364.611 -4364.611 -4364.611], eps: 0.1})
Step:  121500, Reward: [-449.445 -449.445 -449.445] [99.637], Avg: [-444.867 -444.867 -444.867] (0.1000) <00:26:10> ({r_i: None, r_t: [-4268.405 -4268.405 -4268.405], eps: 0.1})
Step:  122000, Reward: [-428.653 -428.653 -428.653] [124.035], Avg: [-444.800 -444.800 -444.800] (0.1000) <00:26:17> ({r_i: None, r_t: [-4430.334 -4430.334 -4430.334], eps: 0.1})
Step:  122500, Reward: [-473.616 -473.616 -473.616] [100.916], Avg: [-444.918 -444.918 -444.918] (0.1000) <00:26:24> ({r_i: None, r_t: [-4240.163 -4240.163 -4240.163], eps: 0.1})
Step:  123000, Reward: [-400.141 -400.141 -400.141] [61.796], Avg: [-444.736 -444.736 -444.736] (0.1000) <00:26:31> ({r_i: None, r_t: [-4457.919 -4457.919 -4457.919], eps: 0.1})
Step:  123500, Reward: [-409.601 -409.601 -409.601] [58.442], Avg: [-444.595 -444.595 -444.595] (0.1000) <00:26:37> ({r_i: None, r_t: [-4354.557 -4354.557 -4354.557], eps: 0.1})
Step:  124000, Reward: [-446.154 -446.154 -446.154] [124.414], Avg: [-444.601 -444.601 -444.601] (0.1000) <00:26:44> ({r_i: None, r_t: [-4379.427 -4379.427 -4379.427], eps: 0.1})
Step:  124500, Reward: [-476.792 -476.792 -476.792] [83.903], Avg: [-444.730 -444.730 -444.730] (0.1000) <00:26:50> ({r_i: None, r_t: [-4362.648 -4362.648 -4362.648], eps: 0.1})
Step:  125000, Reward: [-443.432 -443.432 -443.432] [87.075], Avg: [-444.724 -444.724 -444.724] (0.1000) <00:26:57> ({r_i: None, r_t: [-4432.083 -4432.083 -4432.083], eps: 0.1})
Step:  125500, Reward: [-417.526 -417.526 -417.526] [71.570], Avg: [-444.616 -444.616 -444.616] (0.1000) <00:27:03> ({r_i: None, r_t: [-4354.429 -4354.429 -4354.429], eps: 0.1})
Step:  126000, Reward: [-437.931 -437.931 -437.931] [99.349], Avg: [-444.590 -444.590 -444.590] (0.1000) <00:27:10> ({r_i: None, r_t: [-4374.948 -4374.948 -4374.948], eps: 0.1})
Step:  126500, Reward: [-429.020 -429.020 -429.020] [99.427], Avg: [-444.529 -444.529 -444.529] (0.1000) <00:27:16> ({r_i: None, r_t: [-4511.344 -4511.344 -4511.344], eps: 0.1})
Step:  127000, Reward: [-464.483 -464.483 -464.483] [109.182], Avg: [-444.607 -444.607 -444.607] (0.1000) <00:27:23> ({r_i: None, r_t: [-4415.563 -4415.563 -4415.563], eps: 0.1})
Step:  127500, Reward: [-445.566 -445.566 -445.566] [139.250], Avg: [-444.611 -444.611 -444.611] (0.1000) <00:27:30> ({r_i: None, r_t: [-4346.266 -4346.266 -4346.266], eps: 0.1})
Step:  128000, Reward: [-491.343 -491.343 -491.343] [94.315], Avg: [-444.793 -444.793 -444.793] (0.1000) <00:27:36> ({r_i: None, r_t: [-4388.960 -4388.960 -4388.960], eps: 0.1})
Step:  128500, Reward: [-406.007 -406.007 -406.007] [88.006], Avg: [-444.642 -444.642 -444.642] (0.1000) <00:27:43> ({r_i: None, r_t: [-4387.904 -4387.904 -4387.904], eps: 0.1})
Step:  129000, Reward: [-399.580 -399.580 -399.580] [86.241], Avg: [-444.468 -444.468 -444.468] (0.1000) <00:27:49> ({r_i: None, r_t: [-4284.060 -4284.060 -4284.060], eps: 0.1})
Step:  129500, Reward: [-450.329 -450.329 -450.329] [101.748], Avg: [-444.491 -444.491 -444.491] (0.1000) <00:27:56> ({r_i: None, r_t: [-4358.831 -4358.831 -4358.831], eps: 0.1})
Step:  130000, Reward: [-453.255 -453.255 -453.255] [84.728], Avg: [-444.524 -444.524 -444.524] (0.1000) <00:28:02> ({r_i: None, r_t: [-4467.807 -4467.807 -4467.807], eps: 0.1})
Step:  130500, Reward: [-479.152 -479.152 -479.152] [89.898], Avg: [-444.657 -444.657 -444.657] (0.1000) <00:28:09> ({r_i: None, r_t: [-4451.313 -4451.313 -4451.313], eps: 0.1})
Step:  131000, Reward: [-475.185 -475.185 -475.185] [96.179], Avg: [-444.773 -444.773 -444.773] (0.1000) <00:28:15> ({r_i: None, r_t: [-4451.844 -4451.844 -4451.844], eps: 0.1})
Step:  131500, Reward: [-427.377 -427.377 -427.377] [57.762], Avg: [-444.707 -444.707 -444.707] (0.1000) <00:28:22> ({r_i: None, r_t: [-4614.062 -4614.062 -4614.062], eps: 0.1})
Step:  132000, Reward: [-466.317 -466.317 -466.317] [79.141], Avg: [-444.788 -444.788 -444.788] (0.1000) <00:28:29> ({r_i: None, r_t: [-4509.302 -4509.302 -4509.302], eps: 0.1})
Step:  132500, Reward: [-434.114 -434.114 -434.114] [55.580], Avg: [-444.748 -444.748 -444.748] (0.1000) <00:28:35> ({r_i: None, r_t: [-4409.766 -4409.766 -4409.766], eps: 0.1})
Step:  133000, Reward: [-413.327 -413.327 -413.327] [67.222], Avg: [-444.630 -444.630 -444.630] (0.1000) <00:28:42> ({r_i: None, r_t: [-4302.712 -4302.712 -4302.712], eps: 0.1})
Step:  133500, Reward: [-461.284 -461.284 -461.284] [94.617], Avg: [-444.693 -444.693 -444.693] (0.1000) <00:28:48> ({r_i: None, r_t: [-4460.112 -4460.112 -4460.112], eps: 0.1})
Step:  134000, Reward: [-442.403 -442.403 -442.403] [87.095], Avg: [-444.684 -444.684 -444.684] (0.1000) <00:28:55> ({r_i: None, r_t: [-4339.656 -4339.656 -4339.656], eps: 0.1})
Step:  134500, Reward: [-430.476 -430.476 -430.476] [91.472], Avg: [-444.631 -444.631 -444.631] (0.1000) <00:29:02> ({r_i: None, r_t: [-4528.796 -4528.796 -4528.796], eps: 0.1})
Step:  135000, Reward: [-442.968 -442.968 -442.968] [82.948], Avg: [-444.625 -444.625 -444.625] (0.1000) <00:29:09> ({r_i: None, r_t: [-4391.688 -4391.688 -4391.688], eps: 0.1})
Step:  135500, Reward: [-437.491 -437.491 -437.491] [101.387], Avg: [-444.599 -444.599 -444.599] (0.1000) <00:29:15> ({r_i: None, r_t: [-4523.103 -4523.103 -4523.103], eps: 0.1})
Step:  136000, Reward: [-451.871 -451.871 -451.871] [85.452], Avg: [-444.626 -444.626 -444.626] (0.1000) <00:29:22> ({r_i: None, r_t: [-4443.627 -4443.627 -4443.627], eps: 0.1})
Step:  136500, Reward: [-447.930 -447.930 -447.930] [83.908], Avg: [-444.638 -444.638 -444.638] (0.1000) <00:29:28> ({r_i: None, r_t: [-4332.106 -4332.106 -4332.106], eps: 0.1})
Step:  137000, Reward: [-423.958 -423.958 -423.958] [70.298], Avg: [-444.563 -444.563 -444.563] (0.1000) <00:29:35> ({r_i: None, r_t: [-4377.006 -4377.006 -4377.006], eps: 0.1})
Step:  137500, Reward: [-472.743 -472.743 -472.743] [94.481], Avg: [-444.665 -444.665 -444.665] (0.1000) <00:29:42> ({r_i: None, r_t: [-4474.148 -4474.148 -4474.148], eps: 0.1})
Step:  138000, Reward: [-467.954 -467.954 -467.954] [86.592], Avg: [-444.749 -444.749 -444.749] (0.1000) <00:29:48> ({r_i: None, r_t: [-4335.945 -4335.945 -4335.945], eps: 0.1})
Step:  138500, Reward: [-425.251 -425.251 -425.251] [107.863], Avg: [-444.679 -444.679 -444.679] (0.1000) <00:29:55> ({r_i: None, r_t: [-4322.900 -4322.900 -4322.900], eps: 0.1})
Step:  139000, Reward: [-437.423 -437.423 -437.423] [84.456], Avg: [-444.653 -444.653 -444.653] (0.1000) <00:30:01> ({r_i: None, r_t: [-4300.253 -4300.253 -4300.253], eps: 0.1})
Step:  139500, Reward: [-441.635 -441.635 -441.635] [57.428], Avg: [-444.642 -444.642 -444.642] (0.1000) <00:30:08> ({r_i: None, r_t: [-4303.130 -4303.130 -4303.130], eps: 0.1})
Step:  140000, Reward: [-428.412 -428.412 -428.412] [51.490], Avg: [-444.584 -444.584 -444.584] (0.1000) <00:30:14> ({r_i: None, r_t: [-4379.526 -4379.526 -4379.526], eps: 0.1})
Step:  140500, Reward: [-432.579 -432.579 -432.579] [115.099], Avg: [-444.542 -444.542 -444.542] (0.1000) <00:30:21> ({r_i: None, r_t: [-4382.911 -4382.911 -4382.911], eps: 0.1})
Step:  141000, Reward: [-483.408 -483.408 -483.408] [131.618], Avg: [-444.679 -444.679 -444.679] (0.1000) <00:30:28> ({r_i: None, r_t: [-4475.606 -4475.606 -4475.606], eps: 0.1})
Step:  141500, Reward: [-445.885 -445.885 -445.885] [92.976], Avg: [-444.683 -444.683 -444.683] (0.1000) <00:30:34> ({r_i: None, r_t: [-4303.960 -4303.960 -4303.960], eps: 0.1})
Step:  142000, Reward: [-427.502 -427.502 -427.502] [81.344], Avg: [-444.623 -444.623 -444.623] (0.1000) <00:30:41> ({r_i: None, r_t: [-4353.340 -4353.340 -4353.340], eps: 0.1})
Step:  142500, Reward: [-414.624 -414.624 -414.624] [71.967], Avg: [-444.518 -444.518 -444.518] (0.1000) <00:30:48> ({r_i: None, r_t: [-4410.882 -4410.882 -4410.882], eps: 0.1})
Step:  143000, Reward: [-430.172 -430.172 -430.172] [91.977], Avg: [-444.468 -444.468 -444.468] (0.1000) <00:30:54> ({r_i: None, r_t: [-4527.599 -4527.599 -4527.599], eps: 0.1})
Step:  143500, Reward: [-425.658 -425.658 -425.658] [66.041], Avg: [-444.403 -444.403 -444.403] (0.1000) <00:31:01> ({r_i: None, r_t: [-4299.763 -4299.763 -4299.763], eps: 0.1})
Step:  144000, Reward: [-468.799 -468.799 -468.799] [116.478], Avg: [-444.487 -444.487 -444.487] (0.1000) <00:31:07> ({r_i: None, r_t: [-4520.613 -4520.613 -4520.613], eps: 0.1})
Step:  144500, Reward: [-434.286 -434.286 -434.286] [104.845], Avg: [-444.452 -444.452 -444.452] (0.1000) <00:31:13> ({r_i: None, r_t: [-4342.223 -4342.223 -4342.223], eps: 0.1})
Step:  145000, Reward: [-440.592 -440.592 -440.592] [85.035], Avg: [-444.439 -444.439 -444.439] (0.1000) <00:31:20> ({r_i: None, r_t: [-4426.854 -4426.854 -4426.854], eps: 0.1})
Step:  145500, Reward: [-401.083 -401.083 -401.083] [69.794], Avg: [-444.290 -444.290 -444.290] (0.1000) <00:31:26> ({r_i: None, r_t: [-4352.067 -4352.067 -4352.067], eps: 0.1})
Step:  146000, Reward: [-426.653 -426.653 -426.653] [57.335], Avg: [-444.230 -444.230 -444.230] (0.1000) <00:31:33> ({r_i: None, r_t: [-4527.605 -4527.605 -4527.605], eps: 0.1})
Step:  146500, Reward: [-438.686 -438.686 -438.686] [75.282], Avg: [-444.211 -444.211 -444.211] (0.1000) <00:31:40> ({r_i: None, r_t: [-4365.416 -4365.416 -4365.416], eps: 0.1})
Step:  147000, Reward: [-430.717 -430.717 -430.717] [83.877], Avg: [-444.165 -444.165 -444.165] (0.1000) <00:31:46> ({r_i: None, r_t: [-4452.253 -4452.253 -4452.253], eps: 0.1})
Step:  147500, Reward: [-423.621 -423.621 -423.621] [97.621], Avg: [-444.096 -444.096 -444.096] (0.1000) <00:31:53> ({r_i: None, r_t: [-4506.012 -4506.012 -4506.012], eps: 0.1})
Step:  148000, Reward: [-481.037 -481.037 -481.037] [80.230], Avg: [-444.220 -444.220 -444.220] (0.1000) <00:32:00> ({r_i: None, r_t: [-4388.692 -4388.692 -4388.692], eps: 0.1})
Step:  148500, Reward: [-431.434 -431.434 -431.434] [51.696], Avg: [-444.177 -444.177 -444.177] (0.1000) <00:32:06> ({r_i: None, r_t: [-4466.372 -4466.372 -4466.372], eps: 0.1})
Step:  149000, Reward: [-450.147 -450.147 -450.147] [118.332], Avg: [-444.197 -444.197 -444.197] (0.1000) <00:32:12> ({r_i: None, r_t: [-4445.271 -4445.271 -4445.271], eps: 0.1})
Step:  149500, Reward: [-441.039 -441.039 -441.039] [95.728], Avg: [-444.187 -444.187 -444.187] (0.1000) <00:32:19> ({r_i: None, r_t: [-4357.386 -4357.386 -4357.386], eps: 0.1})
Step:  150000, Reward: [-490.843 -490.843 -490.843] [104.898], Avg: [-444.342 -444.342 -444.342] (0.1000) <00:32:26> ({r_i: None, r_t: [-4319.782 -4319.782 -4319.782], eps: 0.1})
Step:  150500, Reward: [-450.206 -450.206 -450.206] [80.863], Avg: [-444.361 -444.361 -444.361] (0.1000) <00:32:33> ({r_i: None, r_t: [-4451.320 -4451.320 -4451.320], eps: 0.1})
Step:  151000, Reward: [-473.760 -473.760 -473.760] [89.837], Avg: [-444.458 -444.458 -444.458] (0.1000) <00:32:39> ({r_i: None, r_t: [-4401.678 -4401.678 -4401.678], eps: 0.1})
Step:  151500, Reward: [-455.420 -455.420 -455.420] [64.265], Avg: [-444.494 -444.494 -444.494] (0.1000) <00:32:46> ({r_i: None, r_t: [-4469.656 -4469.656 -4469.656], eps: 0.1})
Step:  152000, Reward: [-433.257 -433.257 -433.257] [95.619], Avg: [-444.458 -444.458 -444.458] (0.1000) <00:32:53> ({r_i: None, r_t: [-4496.153 -4496.153 -4496.153], eps: 0.1})
Step:  152500, Reward: [-462.081 -462.081 -462.081] [110.447], Avg: [-444.515 -444.515 -444.515] (0.1000) <00:32:59> ({r_i: None, r_t: [-4334.696 -4334.696 -4334.696], eps: 0.1})
Step:  153000, Reward: [-401.588 -401.588 -401.588] [69.549], Avg: [-444.375 -444.375 -444.375] (0.1000) <00:33:06> ({r_i: None, r_t: [-4449.265 -4449.265 -4449.265], eps: 0.1})
Step:  153500, Reward: [-413.115 -413.115 -413.115] [74.070], Avg: [-444.274 -444.274 -444.274] (0.1000) <00:33:13> ({r_i: None, r_t: [-4455.753 -4455.753 -4455.753], eps: 0.1})
Step:  154000, Reward: [-472.103 -472.103 -472.103] [85.136], Avg: [-444.364 -444.364 -444.364] (0.1000) <00:33:19> ({r_i: None, r_t: [-4532.309 -4532.309 -4532.309], eps: 0.1})
Step:  154500, Reward: [-432.694 -432.694 -432.694] [72.892], Avg: [-444.326 -444.326 -444.326] (0.1000) <00:33:26> ({r_i: None, r_t: [-4323.457 -4323.457 -4323.457], eps: 0.1})
Step:  155000, Reward: [-412.162 -412.162 -412.162] [76.713], Avg: [-444.223 -444.223 -444.223] (0.1000) <00:33:33> ({r_i: None, r_t: [-4307.669 -4307.669 -4307.669], eps: 0.1})
Step:  155500, Reward: [-432.716 -432.716 -432.716] [81.513], Avg: [-444.186 -444.186 -444.186] (0.1000) <00:33:39> ({r_i: None, r_t: [-4420.539 -4420.539 -4420.539], eps: 0.1})
Step:  156000, Reward: [-458.018 -458.018 -458.018] [106.897], Avg: [-444.230 -444.230 -444.230] (0.1000) <00:33:46> ({r_i: None, r_t: [-4378.356 -4378.356 -4378.356], eps: 0.1})
Step:  156500, Reward: [-411.547 -411.547 -411.547] [93.377], Avg: [-444.126 -444.126 -444.126] (0.1000) <00:33:53> ({r_i: None, r_t: [-4305.346 -4305.346 -4305.346], eps: 0.1})
Step:  157000, Reward: [-479.537 -479.537 -479.537] [122.661], Avg: [-444.238 -444.238 -444.238] (0.1000) <00:33:59> ({r_i: None, r_t: [-4410.780 -4410.780 -4410.780], eps: 0.1})
Step:  157500, Reward: [-435.213 -435.213 -435.213] [78.377], Avg: [-444.210 -444.210 -444.210] (0.1000) <00:34:06> ({r_i: None, r_t: [-4350.964 -4350.964 -4350.964], eps: 0.1})
Step:  158000, Reward: [-436.300 -436.300 -436.300] [88.462], Avg: [-444.185 -444.185 -444.185] (0.1000) <00:34:13> ({r_i: None, r_t: [-4385.258 -4385.258 -4385.258], eps: 0.1})
Step:  158500, Reward: [-424.063 -424.063 -424.063] [58.433], Avg: [-444.122 -444.122 -444.122] (0.1000) <00:34:19> ({r_i: None, r_t: [-4396.282 -4396.282 -4396.282], eps: 0.1})
Step:  159000, Reward: [-412.130 -412.130 -412.130] [98.648], Avg: [-444.021 -444.021 -444.021] (0.1000) <00:34:25> ({r_i: None, r_t: [-4335.326 -4335.326 -4335.326], eps: 0.1})
Step:  159500, Reward: [-438.552 -438.552 -438.552] [97.525], Avg: [-444.004 -444.004 -444.004] (0.1000) <00:34:32> ({r_i: None, r_t: [-4361.676 -4361.676 -4361.676], eps: 0.1})
Step:  160000, Reward: [-429.452 -429.452 -429.452] [70.774], Avg: [-443.959 -443.959 -443.959] (0.1000) <00:34:38> ({r_i: None, r_t: [-4414.364 -4414.364 -4414.364], eps: 0.1})
Step:  160500, Reward: [-439.066 -439.066 -439.066] [80.883], Avg: [-443.944 -443.944 -443.944] (0.1000) <00:34:45> ({r_i: None, r_t: [-4496.446 -4496.446 -4496.446], eps: 0.1})
Step:  161000, Reward: [-447.878 -447.878 -447.878] [88.855], Avg: [-443.956 -443.956 -443.956] (0.1000) <00:34:52> ({r_i: None, r_t: [-4349.501 -4349.501 -4349.501], eps: 0.1})
Step:  161500, Reward: [-436.121 -436.121 -436.121] [102.947], Avg: [-443.932 -443.932 -443.932] (0.1000) <00:34:59> ({r_i: None, r_t: [-4332.045 -4332.045 -4332.045], eps: 0.1})
Step:  162000, Reward: [-436.430 -436.430 -436.430] [74.562], Avg: [-443.909 -443.909 -443.909] (0.1000) <00:35:05> ({r_i: None, r_t: [-4404.535 -4404.535 -4404.535], eps: 0.1})
Step:  162500, Reward: [-424.504 -424.504 -424.504] [122.199], Avg: [-443.849 -443.849 -443.849] (0.1000) <00:35:12> ({r_i: None, r_t: [-4381.094 -4381.094 -4381.094], eps: 0.1})
Step:  163000, Reward: [-438.952 -438.952 -438.952] [106.499], Avg: [-443.834 -443.834 -443.834] (0.1000) <00:35:19> ({r_i: None, r_t: [-4403.798 -4403.798 -4403.798], eps: 0.1})
Step:  163500, Reward: [-478.914 -478.914 -478.914] [109.407], Avg: [-443.941 -443.941 -443.941] (0.1000) <00:35:25> ({r_i: None, r_t: [-4417.026 -4417.026 -4417.026], eps: 0.1})
Step:  164000, Reward: [-407.555 -407.555 -407.555] [92.834], Avg: [-443.831 -443.831 -443.831] (0.1000) <00:35:31> ({r_i: None, r_t: [-4340.279 -4340.279 -4340.279], eps: 0.1})
Step:  164500, Reward: [-445.494 -445.494 -445.494] [85.589], Avg: [-443.836 -443.836 -443.836] (0.1000) <00:35:38> ({r_i: None, r_t: [-4355.541 -4355.541 -4355.541], eps: 0.1})
Step:  165000, Reward: [-398.578 -398.578 -398.578] [52.405], Avg: [-443.699 -443.699 -443.699] (0.1000) <00:35:44> ({r_i: None, r_t: [-4359.666 -4359.666 -4359.666], eps: 0.1})
Step:  165500, Reward: [-456.836 -456.836 -456.836] [142.525], Avg: [-443.738 -443.738 -443.738] (0.1000) <00:35:51> ({r_i: None, r_t: [-4437.422 -4437.422 -4437.422], eps: 0.1})
Step:  166000, Reward: [-436.201 -436.201 -436.201] [93.866], Avg: [-443.716 -443.716 -443.716] (0.1000) <00:35:58> ({r_i: None, r_t: [-4322.298 -4322.298 -4322.298], eps: 0.1})
Step:  166500, Reward: [-390.829 -390.829 -390.829] [85.544], Avg: [-443.557 -443.557 -443.557] (0.1000) <00:36:04> ({r_i: None, r_t: [-4491.155 -4491.155 -4491.155], eps: 0.1})
Step:  167000, Reward: [-479.950 -479.950 -479.950] [107.873], Avg: [-443.666 -443.666 -443.666] (0.1000) <00:36:11> ({r_i: None, r_t: [-4215.148 -4215.148 -4215.148], eps: 0.1})
Step:  167500, Reward: [-435.969 -435.969 -435.969] [91.123], Avg: [-443.643 -443.643 -443.643] (0.1000) <00:36:18> ({r_i: None, r_t: [-4399.053 -4399.053 -4399.053], eps: 0.1})
Step:  168000, Reward: [-415.304 -415.304 -415.304] [56.063], Avg: [-443.559 -443.559 -443.559] (0.1000) <00:36:24> ({r_i: None, r_t: [-4486.291 -4486.291 -4486.291], eps: 0.1})
Step:  168500, Reward: [-435.989 -435.989 -435.989] [99.038], Avg: [-443.537 -443.537 -443.537] (0.1000) <00:36:31> ({r_i: None, r_t: [-4391.923 -4391.923 -4391.923], eps: 0.1})
Step:  169000, Reward: [-477.222 -477.222 -477.222] [115.210], Avg: [-443.636 -443.636 -443.636] (0.1000) <00:36:37> ({r_i: None, r_t: [-4341.579 -4341.579 -4341.579], eps: 0.1})
Step:  169500, Reward: [-439.195 -439.195 -439.195] [72.577], Avg: [-443.623 -443.623 -443.623] (0.1000) <00:36:44> ({r_i: None, r_t: [-4491.423 -4491.423 -4491.423], eps: 0.1})
Step:  170000, Reward: [-420.828 -420.828 -420.828] [62.203], Avg: [-443.556 -443.556 -443.556] (0.1000) <00:36:51> ({r_i: None, r_t: [-4436.398 -4436.398 -4436.398], eps: 0.1})
Step:  170500, Reward: [-445.221 -445.221 -445.221] [85.590], Avg: [-443.561 -443.561 -443.561] (0.1000) <00:36:57> ({r_i: None, r_t: [-4385.964 -4385.964 -4385.964], eps: 0.1})
Step:  171000, Reward: [-436.573 -436.573 -436.573] [91.857], Avg: [-443.541 -443.541 -443.541] (0.1000) <00:37:04> ({r_i: None, r_t: [-4478.484 -4478.484 -4478.484], eps: 0.1})
Step:  171500, Reward: [-447.307 -447.307 -447.307] [89.809], Avg: [-443.552 -443.552 -443.552] (0.1000) <00:37:11> ({r_i: None, r_t: [-4577.704 -4577.704 -4577.704], eps: 0.1})
Step:  172000, Reward: [-476.683 -476.683 -476.683] [112.568], Avg: [-443.648 -443.648 -443.648] (0.1000) <00:37:17> ({r_i: None, r_t: [-4564.771 -4564.771 -4564.771], eps: 0.1})
Step:  172500, Reward: [-426.542 -426.542 -426.542] [76.154], Avg: [-443.598 -443.598 -443.598] (0.1000) <00:37:24> ({r_i: None, r_t: [-4604.660 -4604.660 -4604.660], eps: 0.1})
Step:  173000, Reward: [-499.343 -499.343 -499.343] [92.743], Avg: [-443.759 -443.759 -443.759] (0.1000) <00:37:30> ({r_i: None, r_t: [-4534.052 -4534.052 -4534.052], eps: 0.1})
Step:  173500, Reward: [-435.112 -435.112 -435.112] [61.022], Avg: [-443.734 -443.734 -443.734] (0.1000) <00:37:36> ({r_i: None, r_t: [-4479.690 -4479.690 -4479.690], eps: 0.1})
Step:  174000, Reward: [-443.692 -443.692 -443.692] [106.595], Avg: [-443.734 -443.734 -443.734] (0.1000) <00:37:43> ({r_i: None, r_t: [-4360.900 -4360.900 -4360.900], eps: 0.1})
Step:  174500, Reward: [-464.096 -464.096 -464.096] [73.570], Avg: [-443.792 -443.792 -443.792] (0.1000) <00:37:50> ({r_i: None, r_t: [-4437.264 -4437.264 -4437.264], eps: 0.1})
Step:  175000, Reward: [-445.726 -445.726 -445.726] [68.137], Avg: [-443.798 -443.798 -443.798] (0.1000) <00:37:56> ({r_i: None, r_t: [-4469.718 -4469.718 -4469.718], eps: 0.1})
Step:  175500, Reward: [-439.920 -439.920 -439.920] [72.569], Avg: [-443.787 -443.787 -443.787] (0.1000) <00:38:03> ({r_i: None, r_t: [-4395.952 -4395.952 -4395.952], eps: 0.1})
Step:  176000, Reward: [-427.272 -427.272 -427.272] [71.748], Avg: [-443.740 -443.740 -443.740] (0.1000) <00:38:10> ({r_i: None, r_t: [-4547.735 -4547.735 -4547.735], eps: 0.1})
Step:  176500, Reward: [-468.259 -468.259 -468.259] [103.324], Avg: [-443.809 -443.809 -443.809] (0.1000) <00:38:16> ({r_i: None, r_t: [-4397.917 -4397.917 -4397.917], eps: 0.1})
Step:  177000, Reward: [-420.748 -420.748 -420.748] [91.308], Avg: [-443.744 -443.744 -443.744] (0.1000) <00:38:23> ({r_i: None, r_t: [-4420.928 -4420.928 -4420.928], eps: 0.1})
Step:  177500, Reward: [-415.617 -415.617 -415.617] [56.701], Avg: [-443.665 -443.665 -443.665] (0.1000) <00:38:30> ({r_i: None, r_t: [-4350.504 -4350.504 -4350.504], eps: 0.1})
Step:  178000, Reward: [-447.605 -447.605 -447.605] [106.417], Avg: [-443.676 -443.676 -443.676] (0.1000) <00:38:36> ({r_i: None, r_t: [-4481.297 -4481.297 -4481.297], eps: 0.1})
Step:  178500, Reward: [-447.360 -447.360 -447.360] [98.174], Avg: [-443.686 -443.686 -443.686] (0.1000) <00:38:43> ({r_i: None, r_t: [-4433.821 -4433.821 -4433.821], eps: 0.1})
Step:  179000, Reward: [-477.418 -477.418 -477.418] [92.616], Avg: [-443.780 -443.780 -443.780] (0.1000) <00:38:49> ({r_i: None, r_t: [-4400.068 -4400.068 -4400.068], eps: 0.1})
Step:  179500, Reward: [-437.574 -437.574 -437.574] [72.963], Avg: [-443.763 -443.763 -443.763] (0.1000) <00:38:56> ({r_i: None, r_t: [-4478.525 -4478.525 -4478.525], eps: 0.1})
Step:  180000, Reward: [-452.562 -452.562 -452.562] [101.626], Avg: [-443.787 -443.787 -443.787] (0.1000) <00:39:02> ({r_i: None, r_t: [-4425.285 -4425.285 -4425.285], eps: 0.1})
Step:  180500, Reward: [-416.671 -416.671 -416.671] [111.344], Avg: [-443.713 -443.713 -443.713] (0.1000) <00:39:09> ({r_i: None, r_t: [-4399.780 -4399.780 -4399.780], eps: 0.1})
Step:  181000, Reward: [-444.869 -444.869 -444.869] [86.918], Avg: [-443.716 -443.716 -443.716] (0.1000) <00:39:15> ({r_i: None, r_t: [-4362.848 -4362.848 -4362.848], eps: 0.1})
Step:  181500, Reward: [-442.918 -442.918 -442.918] [78.057], Avg: [-443.714 -443.714 -443.714] (0.1000) <00:39:22> ({r_i: None, r_t: [-4415.340 -4415.340 -4415.340], eps: 0.1})
Step:  182000, Reward: [-462.365 -462.365 -462.365] [105.023], Avg: [-443.765 -443.765 -443.765] (0.1000) <00:39:29> ({r_i: None, r_t: [-4467.810 -4467.810 -4467.810], eps: 0.1})
Step:  182500, Reward: [-439.784 -439.784 -439.784] [95.444], Avg: [-443.754 -443.754 -443.754] (0.1000) <00:39:35> ({r_i: None, r_t: [-4421.044 -4421.044 -4421.044], eps: 0.1})
Step:  183000, Reward: [-443.434 -443.434 -443.434] [89.298], Avg: [-443.753 -443.753 -443.753] (0.1000) <00:39:41> ({r_i: None, r_t: [-4429.160 -4429.160 -4429.160], eps: 0.1})
Step:  183500, Reward: [-404.225 -404.225 -404.225] [54.861], Avg: [-443.645 -443.645 -443.645] (0.1000) <00:39:48> ({r_i: None, r_t: [-4392.852 -4392.852 -4392.852], eps: 0.1})
Step:  184000, Reward: [-434.143 -434.143 -434.143] [86.109], Avg: [-443.620 -443.620 -443.620] (0.1000) <00:39:55> ({r_i: None, r_t: [-4497.741 -4497.741 -4497.741], eps: 0.1})
Step:  184500, Reward: [-484.800 -484.800 -484.800] [89.606], Avg: [-443.731 -443.731 -443.731] (0.1000) <00:40:01> ({r_i: None, r_t: [-4429.812 -4429.812 -4429.812], eps: 0.1})
Step:  185000, Reward: [-465.769 -465.769 -465.769] [108.997], Avg: [-443.790 -443.790 -443.790] (0.1000) <00:40:08> ({r_i: None, r_t: [-4415.091 -4415.091 -4415.091], eps: 0.1})
Step:  185500, Reward: [-447.451 -447.451 -447.451] [101.421], Avg: [-443.800 -443.800 -443.800] (0.1000) <00:40:15> ({r_i: None, r_t: [-4359.515 -4359.515 -4359.515], eps: 0.1})
Step:  186000, Reward: [-416.133 -416.133 -416.133] [88.481], Avg: [-443.726 -443.726 -443.726] (0.1000) <00:40:21> ({r_i: None, r_t: [-4359.560 -4359.560 -4359.560], eps: 0.1})
Step:  186500, Reward: [-425.376 -425.376 -425.376] [71.107], Avg: [-443.677 -443.677 -443.677] (0.1000) <00:40:28> ({r_i: None, r_t: [-4454.338 -4454.338 -4454.338], eps: 0.1})
Step:  187000, Reward: [-441.663 -441.663 -441.663] [91.984], Avg: [-443.672 -443.672 -443.672] (0.1000) <00:40:34> ({r_i: None, r_t: [-4416.011 -4416.011 -4416.011], eps: 0.1})
Step:  187500, Reward: [-445.438 -445.438 -445.438] [82.463], Avg: [-443.676 -443.676 -443.676] (0.1000) <00:40:41> ({r_i: None, r_t: [-4483.317 -4483.317 -4483.317], eps: 0.1})
Step:  188000, Reward: [-441.908 -441.908 -441.908] [85.207], Avg: [-443.672 -443.672 -443.672] (0.1000) <00:40:47> ({r_i: None, r_t: [-4465.700 -4465.700 -4465.700], eps: 0.1})
Step:  188500, Reward: [-456.565 -456.565 -456.565] [89.404], Avg: [-443.706 -443.706 -443.706] (0.1000) <00:40:53> ({r_i: None, r_t: [-4365.170 -4365.170 -4365.170], eps: 0.1})
Step:  189000, Reward: [-431.844 -431.844 -431.844] [59.693], Avg: [-443.674 -443.674 -443.674] (0.1000) <00:41:00> ({r_i: None, r_t: [-4389.762 -4389.762 -4389.762], eps: 0.1})
Step:  189500, Reward: [-410.851 -410.851 -410.851] [60.841], Avg: [-443.588 -443.588 -443.588] (0.1000) <00:41:07> ({r_i: None, r_t: [-4389.302 -4389.302 -4389.302], eps: 0.1})
Step:  190000, Reward: [-414.850 -414.850 -414.850] [77.360], Avg: [-443.513 -443.513 -443.513] (0.1000) <00:41:13> ({r_i: None, r_t: [-4422.766 -4422.766 -4422.766], eps: 0.1})
Step:  190500, Reward: [-433.140 -433.140 -433.140] [74.136], Avg: [-443.485 -443.485 -443.485] (0.1000) <00:41:20> ({r_i: None, r_t: [-4522.567 -4522.567 -4522.567], eps: 0.1})
Step:  191000, Reward: [-471.018 -471.018 -471.018] [108.117], Avg: [-443.557 -443.557 -443.557] (0.1000) <00:41:27> ({r_i: None, r_t: [-4415.435 -4415.435 -4415.435], eps: 0.1})
Step:  191500, Reward: [-418.385 -418.385 -418.385] [114.256], Avg: [-443.492 -443.492 -443.492] (0.1000) <00:41:34> ({r_i: None, r_t: [-4374.486 -4374.486 -4374.486], eps: 0.1})
Step:  192000, Reward: [-450.624 -450.624 -450.624] [74.364], Avg: [-443.510 -443.510 -443.510] (0.1000) <00:41:40> ({r_i: None, r_t: [-4417.683 -4417.683 -4417.683], eps: 0.1})
Step:  192500, Reward: [-453.063 -453.063 -453.063] [108.951], Avg: [-443.535 -443.535 -443.535] (0.1000) <00:41:47> ({r_i: None, r_t: [-4421.216 -4421.216 -4421.216], eps: 0.1})
Step:  193000, Reward: [-403.239 -403.239 -403.239] [71.082], Avg: [-443.431 -443.431 -443.431] (0.1000) <00:41:53> ({r_i: None, r_t: [-4362.857 -4362.857 -4362.857], eps: 0.1})
Step:  193500, Reward: [-436.686 -436.686 -436.686] [85.386], Avg: [-443.414 -443.414 -443.414] (0.1000) <00:42:00> ({r_i: None, r_t: [-4432.804 -4432.804 -4432.804], eps: 0.1})
Step:  194000, Reward: [-437.430 -437.430 -437.430] [103.128], Avg: [-443.398 -443.398 -443.398] (0.1000) <00:42:07> ({r_i: None, r_t: [-4446.139 -4446.139 -4446.139], eps: 0.1})
Step:  194500, Reward: [-409.207 -409.207 -409.207] [70.050], Avg: [-443.311 -443.311 -443.311] (0.1000) <00:42:13> ({r_i: None, r_t: [-4542.153 -4542.153 -4542.153], eps: 0.1})
Step:  195000, Reward: [-439.685 -439.685 -439.685] [64.370], Avg: [-443.301 -443.301 -443.301] (0.1000) <00:42:20> ({r_i: None, r_t: [-4344.001 -4344.001 -4344.001], eps: 0.1})
Step:  195500, Reward: [-419.681 -419.681 -419.681] [100.585], Avg: [-443.241 -443.241 -443.241] (0.1000) <00:42:26> ({r_i: None, r_t: [-4448.344 -4448.344 -4448.344], eps: 0.1})
Step:  196000, Reward: [-475.032 -475.032 -475.032] [114.416], Avg: [-443.322 -443.322 -443.322] (0.1000) <00:42:33> ({r_i: None, r_t: [-4449.466 -4449.466 -4449.466], eps: 0.1})
Step:  196500, Reward: [-422.238 -422.238 -422.238] [94.567], Avg: [-443.268 -443.268 -443.268] (0.1000) <00:42:40> ({r_i: None, r_t: [-4421.501 -4421.501 -4421.501], eps: 0.1})
Step:  197000, Reward: [-416.925 -416.925 -416.925] [70.896], Avg: [-443.202 -443.202 -443.202] (0.1000) <00:42:46> ({r_i: None, r_t: [-4430.448 -4430.448 -4430.448], eps: 0.1})
Step:  197500, Reward: [-434.233 -434.233 -434.233] [38.907], Avg: [-443.179 -443.179 -443.179] (0.1000) <00:42:53> ({r_i: None, r_t: [-4485.122 -4485.122 -4485.122], eps: 0.1})
Step:  198000, Reward: [-442.464 -442.464 -442.464] [98.002], Avg: [-443.177 -443.177 -443.177] (0.1000) <00:43:00> ({r_i: None, r_t: [-4282.737 -4282.737 -4282.737], eps: 0.1})
Step:  198500, Reward: [-448.535 -448.535 -448.535] [91.976], Avg: [-443.191 -443.191 -443.191] (0.1000) <00:43:06> ({r_i: None, r_t: [-4507.116 -4507.116 -4507.116], eps: 0.1})
Step:  199000, Reward: [-439.127 -439.127 -439.127] [81.353], Avg: [-443.181 -443.181 -443.181] (0.1000) <00:43:13> ({r_i: None, r_t: [-4369.037 -4369.037 -4369.037], eps: 0.1})
Step:  199500, Reward: [-406.032 -406.032 -406.032] [87.085], Avg: [-443.088 -443.088 -443.088] (0.1000) <00:43:19> ({r_i: None, r_t: [-4512.708 -4512.708 -4512.708], eps: 0.1})
Step:  200000, Reward: [-489.227 -489.227 -489.227] [86.103], Avg: [-443.203 -443.203 -443.203] (0.1000) <00:43:26> ({r_i: None, r_t: [-4342.326 -4342.326 -4342.326], eps: 0.1})
