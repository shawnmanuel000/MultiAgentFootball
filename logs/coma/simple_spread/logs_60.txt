Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		# n_obs = state_size[0][-1]
		# state_len = int(np.sum([np.prod(space) for space in state_size]))
		# preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		# groups = {"agents": self.n_agents}
		# scheme = {
		# 	"state": {"vshape": state_len},
		# 	"obs": {"vshape": n_obs, "group": "agents"},
		# 	"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
		# 	"reward": {"vshape": (1,)},
		# 	"done": {"vshape": (1,), "dtype": torch.uint8},
		# }
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		# self.replay_buffer = ReplayBuffer(scheme, groups, 2000, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.mac, self.n_agents, n_actions, device=self.device)
		# self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		# self.episode_batch = self.new_episode_batch(NUM_ENVS)
		# self.mac.init_hidden(batch_size=NUM_ENVS)
		# self.num_envs = NUM_ENVS
		# self.time = 0
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		# self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		# if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		# if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		# self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		# state_joint = np.concatenate(state, -1)
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		# self.episode_batch.update({"state": [state_joint], "obs": [obs]}, ts=self.step)
		actions = self.mac.select_actions(None, inputs, t_ep=None, t_env=None, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		# actions, rewards, dones = [list(zip(*x)) for x in [action, reward, done]]
		# actions_one_hot = [np.argmax(a, -1) for a in actions]
		# rewards = [np.mean(rewards, -1)]
		# dones = [np.any(dones, -1)]
		# obs = np.concatenate(state, -2)
		# post_transition_data = {"actions": actions_one_hot, "reward": rewards, "done": dones}
		# self.episode_batch.update(post_transition_data, ts=self.step)
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			# state_joint = np.concatenate(state, -1)
			# self.episode_batch.update({"state": [state_joint], "obs": [next_obs]}, ts=self.step)
			# next_obs = np.concatenate(next_state, -2)
			# agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
			# actor_inputs = torch.from_numpy(np.concatenate([next_obs, self.action, agent_ids], -1))
			# actions = self.mac.select_actions(self.episode_batch, actor_inputs, t_ep=self.step, t_env=self.time, test_mode=False)
			# self.episode_batch.update({"actions": actions}, ts=self.step)
			# self.replay_buffer.insert_episode_batch(self.episode_batch)
			# if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				# episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True))
			# self.episode_batch = self.new_episode_batch(state[0].shape[0])
			# self.mac.init_hidden(self.num_envs)
			# self.time += self.step
			# self.step = 0

# class OneHot():
# 	def __init__(self, out_dim):
# 		self.out_dim = out_dim

# 	def transform(self, tensor):
# 		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
# 		y_onehot.scatter_(-1, tensor.long(), 1)
# 		return y_onehot.float()

# 	def infer_output_info(self, vshape_in, dtype_in):
# 		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, state_size, action_size, mac, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_):
		# bs = batch.batch_size
		# max_t = batch.max_seq_length
		# rewards = batch["reward"][:, :-1]
		# actions = batch["actions"][:, :]
		# done = batch["done"][:, :-1].float()
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		# actions = actions[:,:-1]
		# mac_out = torch.stack([self.mac.forward(batch, actor_inputs[:,t], t) for t in range(actions.shape[1])], dim=1)
		mac_out = torch.stack([self.mac.forward(batch, actor_inputs[:,t], t) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		# q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		# pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		# targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		# targets = build_td_lambda_targets(rewards, done, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t#.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			# q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	# def cuda(self):
	# 	self.mac.cuda()
	# 	self.critic.cuda()
	# 	self.target_critic.cuda()

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		# input_shape = self._get_input_shape(scheme)
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		# inputs = self._build_inputs(batch, t=t)
		# x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	# def _build_inputs(self, batch, t=None):
	# 	bs = batch.batch_size
	# 	max_t = batch.max_seq_length if t is None else 1
	# 	ts = slice(None) if t is None else slice(t, t+1)
	# 	inputs = []
	# 	inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
	# 	inputs.append(batch["obs"][:, ts])
	# 	actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
	# 	agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
	# 	agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
	# 	inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
	# 	# last actions
	# 	if t == 0:
	# 		inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
	# 	elif isinstance(t, int):
	# 		inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
	# 	else:
	# 		last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
	# 		last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
	# 		inputs.append(last_actions)

	# 	inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
	# 	inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
	# 	return inputs

	# def _get_input_shape(self, scheme):
	# 	input_shape = scheme["state"]["vshape"]
	# 	input_shape += scheme["obs"]["vshape"]
	# 	input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
	# 	input_shape += self.n_agents
	# 	return input_shape

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		# agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs = self.agent(actor_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs#.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	# def _build_inputs(self, batch, t):
	# 	bs = batch.batch_size
	# 	inputs = []
	# 	inputs.append(batch["obs"][:, t])  # b1av
	# 	inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
	# 	inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
	# 	inputs = torch.cat([x.reshape(bs, self.n_agents, -1) for x in inputs], dim=-1)
	# 	return inputs

	# def _get_input_shape(self, scheme):
	# 	input_shape = scheme["obs"]["vshape"]
	# 	input_shape += scheme["actions_onehot"]["vshape"][0]
	# 	input_shape += self.n_agents
	# 	return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

# class EpisodeBatch():
# 	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
# 		self.scheme = scheme.copy()
# 		self.groups = groups
# 		self.batch_size = batch_size
# 		self.max_seq_length = max_seq_length
# 		self.preprocess = {} if preprocess is None else preprocess
# 		self.device = device

# 		if data is not None:
# 			self.data = data
# 		else:
# 			self.data = SN()
# 			self.data.transition_data = {}
# 			self.data.episode_data = {}
# 			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

# 	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
# 		if preprocess is not None:
# 			for k in preprocess:
# 				assert k in scheme
# 				new_k = preprocess[k][0]
# 				transforms = preprocess[k][1]
# 				vshape = self.scheme[k]["vshape"]
# 				dtype = self.scheme[k]["dtype"]
# 				for transform in transforms:
# 					vshape, dtype = transform.infer_output_info(vshape, dtype)
# 				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
# 				if "group" in self.scheme[k]:
# 					self.scheme[new_k]["group"] = self.scheme[k]["group"]
# 				if "episode_const" in self.scheme[k]:
# 					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

# 		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
# 		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

# 		for field_key, field_info in scheme.items():
# 			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
# 			vshape = field_info["vshape"]
# 			episode_const = field_info.get("episode_const", False)
# 			group = field_info.get("group", None)
# 			dtype = field_info.get("dtype", torch.float32)

# 			if isinstance(vshape, int):
# 				vshape = (vshape,)
# 			if group:
# 				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
# 				shape = (groups[group], *vshape)
# 			else:
# 				shape = vshape
# 			if episode_const:
# 				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
# 			else:
# 				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

# 	def extend(self, scheme, groups=None):
# 		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

# 	def to(self, device):
# 		for k, v in self.data.transition_data.items():
# 			self.data.transition_data[k] = v.to(device)
# 		for k, v in self.data.episode_data.items():
# 			self.data.episode_data[k] = v.to(device)
# 		self.device = device

# 	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
# 		slices = self._parse_slices((bs, ts))
# 		for k, v in data.items():
# 			if k in self.data.transition_data:
# 				target = self.data.transition_data
# 				if mark_filled:
# 					target["filled"][slices] = 1
# 					mark_filled = False
# 				_slices = slices
# 			elif k in self.data.episode_data:
# 				target = self.data.episode_data
# 				_slices = slices[0]
# 			else:
# 				raise KeyError("{} not found in transition or episode data".format(k))

# 			dtype = self.scheme[k].get("dtype", torch.float32)
# 			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
# 			self._check_safe_view(v, target[k][_slices])
# 			target[k][_slices] = v.view_as(target[k][_slices])

# 			if k in self.preprocess:
# 				new_k = self.preprocess[k][0]
# 				v = target[k][_slices]
# 				for transform in self.preprocess[k][1]:
# 					v = transform.transform(v)
# 				target[new_k][_slices] = v.view_as(target[new_k][_slices])

# 	def _check_safe_view(self, v, dest):
# 		idx = len(v.shape) - 1
# 		for s in dest.shape[::-1]:
# 			if v.shape[idx] != s:
# 				if s != 1:
# 					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
# 			else:
# 				idx -= 1

# 	def __getitem__(self, item):
# 		if isinstance(item, str):
# 			if item in self.data.episode_data:
# 				return self.data.episode_data[item]
# 			elif item in self.data.transition_data:
# 				return self.data.transition_data[item]
# 			else:
# 				raise ValueError
# 		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
# 			new_data = self._new_data_sn()
# 			for key in item:
# 				if key in self.data.transition_data:
# 					new_data.transition_data[key] = self.data.transition_data[key]
# 				elif key in self.data.episode_data:
# 					new_data.episode_data[key] = self.data.episode_data[key]
# 				else:
# 					raise KeyError("Unrecognised key {}".format(key))

# 			# Update the scheme to only have the requested keys
# 			new_scheme = {key: self.scheme[key] for key in item}
# 			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
# 						for key in item if "group" in self.scheme[key]}
# 			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
# 			return ret
# 		else:
# 			item = self._parse_slices(item)
# 			new_data = self._new_data_sn()
# 			for k, v in self.data.transition_data.items():
# 				new_data.transition_data[k] = v[item]
# 			for k, v in self.data.episode_data.items():
# 				new_data.episode_data[k] = v[item[0]]

# 			ret_bs = self._get_num_items(item[0], self.batch_size)
# 			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

# 			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
# 			return ret

# 	def _get_num_items(self, indexing_item, max_size):
# 		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
# 			return len(indexing_item)
# 		elif isinstance(indexing_item, slice):
# 			_range = indexing_item.indices(max_size)
# 			return 1 + (_range[1] - _range[0] - 1)//_range[2]

# 	def _new_data_sn(self):
# 		new_data = SN()
# 		new_data.transition_data = {}
# 		new_data.episode_data = {}
# 		return new_data

# 	def _parse_slices(self, items):
# 		parsed = []
# 		# Only batch slice given, add full time slice
# 		if (isinstance(items, slice)  # slice a:b
# 			or isinstance(items, int)  # int i
# 			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
# 			):
# 			items = (items, slice(None))

# 		# Need the time indexing to be contiguous
# 		if isinstance(items[1], list):
# 			raise IndexError("Indexing across Time must be contiguous")

# 		for item in items:
# 			#TODO: stronger checks to ensure only supported options get through
# 			if isinstance(item, int):
# 				# Convert single indices to slices
# 				parsed.append(slice(item, item+1))
# 			else:
# 				# Leave slices and lists as is
# 				parsed.append(item)
# 		return parsed

# 	def max_t_filled(self):
# 		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

# class ReplayBuffer(EpisodeBatch):
# 	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
# 		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
# 		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
# 		self.buffer_index = 0
# 		self.episodes_in_buffer = 0

# 	def insert_episode_batch(self, ep_batch):
# 		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
# 			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
# 			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
# 			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
# 			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
# 			self.buffer_index = self.buffer_index % self.buffer_size
# 			assert self.buffer_index < self.buffer_size
# 		else:
# 			buffer_left = self.buffer_size - self.buffer_index
# 			self.insert_episode_batch(ep_batch[0:buffer_left, :])
# 			self.insert_episode_batch(ep_batch[buffer_left:, :])

# 	def can_sample(self, batch_size):
# 		return self.episodes_in_buffer >= batch_size

# 	def sample(self, batch_size):
# 		assert self.can_sample(batch_size)
# 		if self.episodes_in_buffer == batch_size:
# 			return self[:batch_size]
# 		else:
# 			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
# 			return self[ep_ids]


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-443.602 -443.602 -443.602] [67.334], Avg: [-443.602 -443.602 -443.602] (1.0000) ({r_i: None, r_t: [-9.300 -9.300 -9.300], eps: 1.0})
Step:     100, Reward: [-435.852 -435.852 -435.852] [73.300], Avg: [-439.727 -439.727 -439.727] (1.0000) ({r_i: None, r_t: [-975.658 -975.658 -975.658], eps: 1.0})
Step:     200, Reward: [-515.151 -515.151 -515.151] [85.830], Avg: [-464.868 -464.868 -464.868] (1.0000) ({r_i: None, r_t: [-982.465 -982.465 -982.465], eps: 1.0})
Step:     300, Reward: [-492.683 -492.683 -492.683] [90.124], Avg: [-471.822 -471.822 -471.822] (1.0000) ({r_i: None, r_t: [-1022.124 -1022.124 -1022.124], eps: 1.0})
Step:     400, Reward: [-492.919 -492.919 -492.919] [104.443], Avg: [-476.041 -476.041 -476.041] (1.0000) ({r_i: None, r_t: [-961.592 -961.592 -961.592], eps: 1.0})
Step:     500, Reward: [-452.528 -452.528 -452.528] [86.270], Avg: [-472.123 -472.123 -472.123] (1.0000) ({r_i: None, r_t: [-1004.712 -1004.712 -1004.712], eps: 1.0})
Step:     600, Reward: [-491.344 -491.344 -491.344] [72.136], Avg: [-474.868 -474.868 -474.868] (1.0000) ({r_i: None, r_t: [-1010.399 -1010.399 -1010.399], eps: 1.0})
Step:     700, Reward: [-470.297 -470.297 -470.297] [83.854], Avg: [-474.297 -474.297 -474.297] (1.0000) ({r_i: None, r_t: [-986.214 -986.214 -986.214], eps: 1.0})
Step:     800, Reward: [-515.866 -515.866 -515.866] [92.035], Avg: [-478.916 -478.916 -478.916] (1.0000) ({r_i: None, r_t: [-1023.309 -1023.309 -1023.309], eps: 1.0})
Step:     900, Reward: [-465.486 -465.486 -465.486] [84.326], Avg: [-477.573 -477.573 -477.573] (1.0000) ({r_i: None, r_t: [-927.985 -927.985 -927.985], eps: 1.0})
Step:    1000, Reward: [-470.448 -470.448 -470.448] [104.743], Avg: [-476.925 -476.925 -476.925] (1.0000) ({r_i: None, r_t: [-997.247 -997.247 -997.247], eps: 1.0})
Step:    1100, Reward: [-466.720 -466.720 -466.720] [96.611], Avg: [-476.075 -476.075 -476.075] (1.0000) ({r_i: None, r_t: [-1001.233 -1001.233 -1001.233], eps: 1.0})
Step:    1200, Reward: [-477.079 -477.079 -477.079] [77.077], Avg: [-476.152 -476.152 -476.152] (1.0000) ({r_i: None, r_t: [-995.007 -995.007 -995.007], eps: 1.0})
Step:    1300, Reward: [-482.030 -482.030 -482.030] [84.367], Avg: [-476.572 -476.572 -476.572] (1.0000) ({r_i: None, r_t: [-987.313 -987.313 -987.313], eps: 1.0})
Step:    1400, Reward: [-497.925 -497.925 -497.925] [87.794], Avg: [-477.995 -477.995 -477.995] (1.0000) ({r_i: None, r_t: [-955.515 -955.515 -955.515], eps: 1.0})
Step:    1500, Reward: [-494.778 -494.778 -494.778] [99.687], Avg: [-479.044 -479.044 -479.044] (1.0000) ({r_i: None, r_t: [-943.744 -943.744 -943.744], eps: 1.0})
Step:    1600, Reward: [-466.388 -466.388 -466.388] [76.705], Avg: [-478.300 -478.300 -478.300] (1.0000) ({r_i: None, r_t: [-964.975 -964.975 -964.975], eps: 1.0})
Step:    1700, Reward: [-448.096 -448.096 -448.096] [87.970], Avg: [-476.622 -476.622 -476.622] (1.0000) ({r_i: None, r_t: [-912.333 -912.333 -912.333], eps: 1.0})
Step:    1800, Reward: [-494.408 -494.408 -494.408] [85.790], Avg: [-477.558 -477.558 -477.558] (1.0000) ({r_i: None, r_t: [-1018.007 -1018.007 -1018.007], eps: 1.0})
Step:    1900, Reward: [-492.532 -492.532 -492.532] [93.018], Avg: [-478.307 -478.307 -478.307] (1.0000) ({r_i: None, r_t: [-937.511 -937.511 -937.511], eps: 1.0})
Step:    2000, Reward: [-483.536 -483.536 -483.536] [73.248], Avg: [-478.556 -478.556 -478.556] (1.0000) ({r_i: None, r_t: [-974.373 -974.373 -974.373], eps: 1.0})
Step:    2100, Reward: [-528.238 -528.238 -528.238] [117.511], Avg: [-480.814 -480.814 -480.814] (1.0000) ({r_i: None, r_t: [-935.243 -935.243 -935.243], eps: 1.0})
Step:    2200, Reward: [-506.471 -506.471 -506.471] [79.834], Avg: [-481.929 -481.929 -481.929] (1.0000) ({r_i: None, r_t: [-933.173 -933.173 -933.173], eps: 1.0})
Step:    2300, Reward: [-524.926 -524.926 -524.926] [101.743], Avg: [-483.721 -483.721 -483.721] (1.0000) ({r_i: None, r_t: [-993.680 -993.680 -993.680], eps: 1.0})
Step:    2400, Reward: [-480.971 -480.971 -480.971] [94.404], Avg: [-483.611 -483.611 -483.611] (1.0000) ({r_i: None, r_t: [-1010.034 -1010.034 -1010.034], eps: 1.0})
Step:    2500, Reward: [-457.636 -457.636 -457.636] [77.512], Avg: [-482.612 -482.612 -482.612] (1.0000) ({r_i: None, r_t: [-1033.588 -1033.588 -1033.588], eps: 1.0})
Step:    2600, Reward: [-510.551 -510.551 -510.551] [97.940], Avg: [-483.647 -483.647 -483.647] (1.0000) ({r_i: None, r_t: [-988.772 -988.772 -988.772], eps: 1.0})
Step:    2700, Reward: [-512.457 -512.457 -512.457] [112.622], Avg: [-484.676 -484.676 -484.676] (1.0000) ({r_i: None, r_t: [-957.451 -957.451 -957.451], eps: 1.0})
Step:    2800, Reward: [-513.876 -513.876 -513.876] [98.949], Avg: [-485.683 -485.683 -485.683] (1.0000) ({r_i: None, r_t: [-940.559 -940.559 -940.559], eps: 1.0})
Step:    2900, Reward: [-489.371 -489.371 -489.371] [96.417], Avg: [-485.805 -485.805 -485.805] (1.0000) ({r_i: None, r_t: [-925.908 -925.908 -925.908], eps: 1.0})
Step:    3000, Reward: [-465.332 -465.332 -465.332] [69.668], Avg: [-485.145 -485.145 -485.145] (1.0000) ({r_i: None, r_t: [-927.986 -927.986 -927.986], eps: 1.0})
Step:    3100, Reward: [-471.388 -471.388 -471.388] [79.608], Avg: [-484.715 -484.715 -484.715] (1.0000) ({r_i: None, r_t: [-961.142 -961.142 -961.142], eps: 1.0})
Step:    3200, Reward: [-465.026 -465.026 -465.026] [88.353], Avg: [-484.119 -484.119 -484.119] (1.0000) ({r_i: None, r_t: [-952.798 -952.798 -952.798], eps: 1.0})
Step:    3300, Reward: [-479.102 -479.102 -479.102] [98.908], Avg: [-483.971 -483.971 -483.971] (1.0000) ({r_i: None, r_t: [-936.064 -936.064 -936.064], eps: 1.0})
Step:    3400, Reward: [-488.302 -488.302 -488.302] [96.842], Avg: [-484.095 -484.095 -484.095] (1.0000) ({r_i: None, r_t: [-956.369 -956.369 -956.369], eps: 1.0})
Step:    3500, Reward: [-487.414 -487.414 -487.414] [83.835], Avg: [-484.187 -484.187 -484.187] (1.0000) ({r_i: None, r_t: [-991.260 -991.260 -991.260], eps: 1.0})
Step:    3600, Reward: [-460.731 -460.731 -460.731] [94.774], Avg: [-483.553 -483.553 -483.553] (1.0000) ({r_i: None, r_t: [-896.175 -896.175 -896.175], eps: 1.0})
Step:    3700, Reward: [-501.028 -501.028 -501.028] [103.536], Avg: [-484.013 -484.013 -484.013] (1.0000) ({r_i: None, r_t: [-979.938 -979.938 -979.938], eps: 1.0})
Step:    3800, Reward: [-474.849 -474.849 -474.849] [108.545], Avg: [-483.778 -483.778 -483.778] (1.0000) ({r_i: None, r_t: [-969.418 -969.418 -969.418], eps: 1.0})
Step:    3900, Reward: [-481.610 -481.610 -481.610] [99.796], Avg: [-483.724 -483.724 -483.724] (1.0000) ({r_i: None, r_t: [-945.110 -945.110 -945.110], eps: 1.0})
Step:    4000, Reward: [-510.852 -510.852 -510.852] [82.546], Avg: [-484.385 -484.385 -484.385] (1.0000) ({r_i: None, r_t: [-962.347 -962.347 -962.347], eps: 1.0})
Step:    4100, Reward: [-440.691 -440.691 -440.691] [72.994], Avg: [-483.345 -483.345 -483.345] (1.0000) ({r_i: None, r_t: [-930.063 -930.063 -930.063], eps: 1.0})
Step:    4200, Reward: [-483.132 -483.132 -483.132] [92.955], Avg: [-483.340 -483.340 -483.340] (1.0000) ({r_i: None, r_t: [-1004.784 -1004.784 -1004.784], eps: 1.0})
Step:    4300, Reward: [-472.478 -472.478 -472.478] [93.681], Avg: [-483.093 -483.093 -483.093] (1.0000) ({r_i: None, r_t: [-912.336 -912.336 -912.336], eps: 1.0})
Step:    4400, Reward: [-458.904 -458.904 -458.904] [80.215], Avg: [-482.556 -482.556 -482.556] (1.0000) ({r_i: None, r_t: [-1025.011 -1025.011 -1025.011], eps: 1.0})
Step:    4500, Reward: [-472.616 -472.616 -472.616] [94.432], Avg: [-482.340 -482.340 -482.340] (1.0000) ({r_i: None, r_t: [-918.299 -918.299 -918.299], eps: 1.0})
Step:    4600, Reward: [-495.724 -495.724 -495.724] [127.210], Avg: [-482.624 -482.624 -482.624] (1.0000) ({r_i: None, r_t: [-953.643 -953.643 -953.643], eps: 1.0})
Step:    4700, Reward: [-485.550 -485.550 -485.550] [90.190], Avg: [-482.685 -482.685 -482.685] (1.0000) ({r_i: None, r_t: [-938.000 -938.000 -938.000], eps: 1.0})
Step:    4800, Reward: [-476.397 -476.397 -476.397] [57.393], Avg: [-482.557 -482.557 -482.557] (1.0000) ({r_i: None, r_t: [-978.194 -978.194 -978.194], eps: 1.0})
Step:    4900, Reward: [-476.779 -476.779 -476.779] [105.108], Avg: [-482.441 -482.441 -482.441] (1.0000) ({r_i: None, r_t: [-933.297 -933.297 -933.297], eps: 1.0})
Step:    5000, Reward: [-479.208 -479.208 -479.208] [87.540], Avg: [-482.378 -482.378 -482.378] (1.0000) ({r_i: None, r_t: [-1047.700 -1047.700 -1047.700], eps: 1.0})
Step:    5100, Reward: [-491.439 -491.439 -491.439] [100.320], Avg: [-482.552 -482.552 -482.552] (1.0000) ({r_i: None, r_t: [-1010.470 -1010.470 -1010.470], eps: 1.0})
Step:    5200, Reward: [-475.303 -475.303 -475.303] [93.010], Avg: [-482.415 -482.415 -482.415] (1.0000) ({r_i: None, r_t: [-961.026 -961.026 -961.026], eps: 1.0})
Step:    5300, Reward: [-465.264 -465.264 -465.264] [95.461], Avg: [-482.098 -482.098 -482.098] (1.0000) ({r_i: None, r_t: [-966.699 -966.699 -966.699], eps: 1.0})
Step:    5400, Reward: [-467.030 -467.030 -467.030] [78.373], Avg: [-481.824 -481.824 -481.824] (1.0000) ({r_i: None, r_t: [-926.119 -926.119 -926.119], eps: 1.0})
Step:    5500, Reward: [-463.378 -463.378 -463.378] [87.246], Avg: [-481.495 -481.495 -481.495] (1.0000) ({r_i: None, r_t: [-1010.517 -1010.517 -1010.517], eps: 1.0})
Step:    5600, Reward: [-521.710 -521.710 -521.710] [134.243], Avg: [-482.200 -482.200 -482.200] (1.0000) ({r_i: None, r_t: [-957.427 -957.427 -957.427], eps: 1.0})
Step:    5700, Reward: [-430.740 -430.740 -430.740] [69.916], Avg: [-481.313 -481.313 -481.313] (1.0000) ({r_i: None, r_t: [-1012.639 -1012.639 -1012.639], eps: 1.0})
Step:    5800, Reward: [-463.420 -463.420 -463.420] [76.472], Avg: [-481.010 -481.010 -481.010] (1.0000) ({r_i: None, r_t: [-963.665 -963.665 -963.665], eps: 1.0})
Step:    5900, Reward: [-497.078 -497.078 -497.078] [115.029], Avg: [-481.277 -481.277 -481.277] (1.0000) ({r_i: None, r_t: [-937.853 -937.853 -937.853], eps: 1.0})
Step:    6000, Reward: [-503.282 -503.282 -503.282] [101.571], Avg: [-481.638 -481.638 -481.638] (1.0000) ({r_i: None, r_t: [-932.486 -932.486 -932.486], eps: 1.0})
Step:    6100, Reward: [-507.151 -507.151 -507.151] [70.111], Avg: [-482.050 -482.050 -482.050] (1.0000) ({r_i: None, r_t: [-985.980 -985.980 -985.980], eps: 1.0})
Step:    6200, Reward: [-462.353 -462.353 -462.353] [69.330], Avg: [-481.737 -481.737 -481.737] (1.0000) ({r_i: None, r_t: [-939.466 -939.466 -939.466], eps: 1.0})
Step:    6300, Reward: [-456.793 -456.793 -456.793] [56.715], Avg: [-481.347 -481.347 -481.347] (1.0000) ({r_i: None, r_t: [-967.050 -967.050 -967.050], eps: 1.0})
Step:    6400, Reward: [-494.316 -494.316 -494.316] [71.873], Avg: [-481.547 -481.547 -481.547] (1.0000) ({r_i: None, r_t: [-926.036 -926.036 -926.036], eps: 1.0})
Step:    6500, Reward: [-459.548 -459.548 -459.548] [51.529], Avg: [-481.213 -481.213 -481.213] (1.0000) ({r_i: None, r_t: [-948.427 -948.427 -948.427], eps: 1.0})
Step:    6600, Reward: [-481.713 -481.713 -481.713] [69.324], Avg: [-481.221 -481.221 -481.221] (1.0000) ({r_i: None, r_t: [-937.928 -937.928 -937.928], eps: 1.0})
Step:    6700, Reward: [-474.798 -474.798 -474.798] [80.133], Avg: [-481.126 -481.126 -481.126] (1.0000) ({r_i: None, r_t: [-929.047 -929.047 -929.047], eps: 1.0})
Step:    6800, Reward: [-432.856 -432.856 -432.856] [61.692], Avg: [-480.427 -480.427 -480.427] (1.0000) ({r_i: None, r_t: [-898.987 -898.987 -898.987], eps: 1.0})
Step:    6900, Reward: [-440.991 -440.991 -440.991] [57.342], Avg: [-479.863 -479.863 -479.863] (1.0000) ({r_i: None, r_t: [-916.837 -916.837 -916.837], eps: 1.0})
Step:    7000, Reward: [-448.173 -448.173 -448.173] [77.197], Avg: [-479.417 -479.417 -479.417] (1.0000) ({r_i: None, r_t: [-914.946 -914.946 -914.946], eps: 1.0})
Step:    7100, Reward: [-462.962 -462.962 -462.962] [58.208], Avg: [-479.189 -479.189 -479.189] (1.0000) ({r_i: None, r_t: [-927.827 -927.827 -927.827], eps: 1.0})
Step:    7200, Reward: [-464.896 -464.896 -464.896] [80.132], Avg: [-478.993 -478.993 -478.993] (1.0000) ({r_i: None, r_t: [-849.523 -849.523 -849.523], eps: 1.0})
Step:    7300, Reward: [-423.160 -423.160 -423.160] [84.140], Avg: [-478.238 -478.238 -478.238] (1.0000) ({r_i: None, r_t: [-934.366 -934.366 -934.366], eps: 1.0})
Step:    7400, Reward: [-434.213 -434.213 -434.213] [82.558], Avg: [-477.651 -477.651 -477.651] (1.0000) ({r_i: None, r_t: [-956.071 -956.071 -956.071], eps: 1.0})
Step:    7500, Reward: [-439.106 -439.106 -439.106] [70.585], Avg: [-477.144 -477.144 -477.144] (1.0000) ({r_i: None, r_t: [-896.450 -896.450 -896.450], eps: 1.0})
Step:    7600, Reward: [-461.516 -461.516 -461.516] [70.668], Avg: [-476.941 -476.941 -476.941] (1.0000) ({r_i: None, r_t: [-916.860 -916.860 -916.860], eps: 1.0})
Step:    7700, Reward: [-470.114 -470.114 -470.114] [73.054], Avg: [-476.854 -476.854 -476.854] (1.0000) ({r_i: None, r_t: [-937.815 -937.815 -937.815], eps: 1.0})
Step:    7800, Reward: [-459.619 -459.619 -459.619] [58.470], Avg: [-476.635 -476.635 -476.635] (1.0000) ({r_i: None, r_t: [-934.243 -934.243 -934.243], eps: 1.0})
Step:    7900, Reward: [-462.158 -462.158 -462.158] [119.477], Avg: [-476.454 -476.454 -476.454] (1.0000) ({r_i: None, r_t: [-918.290 -918.290 -918.290], eps: 1.0})
Step:    8000, Reward: [-450.655 -450.655 -450.655] [82.384], Avg: [-476.136 -476.136 -476.136] (1.0000) ({r_i: None, r_t: [-904.324 -904.324 -904.324], eps: 1.0})
Step:    8100, Reward: [-475.771 -475.771 -475.771] [104.933], Avg: [-476.132 -476.132 -476.132] (1.0000) ({r_i: None, r_t: [-921.098 -921.098 -921.098], eps: 1.0})
Step:    8200, Reward: [-482.390 -482.390 -482.390] [105.357], Avg: [-476.207 -476.207 -476.207] (1.0000) ({r_i: None, r_t: [-893.729 -893.729 -893.729], eps: 1.0})
Step:    8300, Reward: [-446.806 -446.806 -446.806] [70.160], Avg: [-475.857 -475.857 -475.857] (1.0000) ({r_i: None, r_t: [-889.118 -889.118 -889.118], eps: 1.0})
Step:    8400, Reward: [-462.142 -462.142 -462.142] [66.208], Avg: [-475.696 -475.696 -475.696] (1.0000) ({r_i: None, r_t: [-949.312 -949.312 -949.312], eps: 1.0})
Step:    8500, Reward: [-423.902 -423.902 -423.902] [72.754], Avg: [-475.093 -475.093 -475.093] (1.0000) ({r_i: None, r_t: [-913.580 -913.580 -913.580], eps: 1.0})
Step:    8600, Reward: [-443.495 -443.495 -443.495] [60.678], Avg: [-474.730 -474.730 -474.730] (1.0000) ({r_i: None, r_t: [-926.141 -926.141 -926.141], eps: 1.0})
Step:    8700, Reward: [-436.251 -436.251 -436.251] [76.530], Avg: [-474.293 -474.293 -474.293] (1.0000) ({r_i: None, r_t: [-867.225 -867.225 -867.225], eps: 1.0})
Step:    8800, Reward: [-449.423 -449.423 -449.423] [47.687], Avg: [-474.013 -474.013 -474.013] (1.0000) ({r_i: None, r_t: [-927.964 -927.964 -927.964], eps: 1.0})
Step:    8900, Reward: [-462.104 -462.104 -462.104] [105.131], Avg: [-473.881 -473.881 -473.881] (1.0000) ({r_i: None, r_t: [-874.199 -874.199 -874.199], eps: 1.0})
Step:    9000, Reward: [-449.418 -449.418 -449.418] [65.008], Avg: [-473.612 -473.612 -473.612] (1.0000) ({r_i: None, r_t: [-879.960 -879.960 -879.960], eps: 1.0})
Step:    9100, Reward: [-449.950 -449.950 -449.950] [68.054], Avg: [-473.355 -473.355 -473.355] (1.0000) ({r_i: None, r_t: [-932.095 -932.095 -932.095], eps: 1.0})
Step:    9200, Reward: [-452.044 -452.044 -452.044] [76.424], Avg: [-473.126 -473.126 -473.126] (1.0000) ({r_i: None, r_t: [-917.623 -917.623 -917.623], eps: 1.0})
Step:    9300, Reward: [-469.209 -469.209 -469.209] [68.087], Avg: [-473.084 -473.084 -473.084] (1.0000) ({r_i: None, r_t: [-868.909 -868.909 -868.909], eps: 1.0})
Step:    9400, Reward: [-440.634 -440.634 -440.634] [74.192], Avg: [-472.743 -472.743 -472.743] (1.0000) ({r_i: None, r_t: [-927.120 -927.120 -927.120], eps: 1.0})
Step:    9500, Reward: [-399.510 -399.510 -399.510] [48.660], Avg: [-471.980 -471.980 -471.980] (1.0000) ({r_i: None, r_t: [-865.722 -865.722 -865.722], eps: 1.0})
Step:    9600, Reward: [-432.653 -432.653 -432.653] [63.036], Avg: [-471.574 -471.574 -471.574] (1.0000) ({r_i: None, r_t: [-858.202 -858.202 -858.202], eps: 1.0})
Step:    9700, Reward: [-435.705 -435.705 -435.705] [50.629], Avg: [-471.208 -471.208 -471.208] (1.0000) ({r_i: None, r_t: [-821.861 -821.861 -821.861], eps: 1.0})
Step:    9800, Reward: [-440.513 -440.513 -440.513] [90.676], Avg: [-470.898 -470.898 -470.898] (1.0000) ({r_i: None, r_t: [-880.148 -880.148 -880.148], eps: 1.0})
Step:    9900, Reward: [-439.936 -439.936 -439.936] [68.939], Avg: [-470.589 -470.589 -470.589] (1.0000) ({r_i: None, r_t: [-843.642 -843.642 -843.642], eps: 1.0})
Step:   10000, Reward: [-426.849 -426.849 -426.849] [63.625], Avg: [-470.156 -470.156 -470.156] (1.0000) ({r_i: None, r_t: [-901.400 -901.400 -901.400], eps: 1.0})
Step:   10100, Reward: [-424.409 -424.409 -424.409] [48.064], Avg: [-469.707 -469.707 -469.707] (1.0000) ({r_i: None, r_t: [-805.858 -805.858 -805.858], eps: 1.0})
Step:   10200, Reward: [-428.719 -428.719 -428.719] [71.557], Avg: [-469.309 -469.309 -469.309] (1.0000) ({r_i: None, r_t: [-864.303 -864.303 -864.303], eps: 1.0})
Step:   10300, Reward: [-417.355 -417.355 -417.355] [98.470], Avg: [-468.810 -468.810 -468.810] (1.0000) ({r_i: None, r_t: [-893.601 -893.601 -893.601], eps: 1.0})
Step:   10400, Reward: [-404.902 -404.902 -404.902] [42.485], Avg: [-468.201 -468.201 -468.201] (1.0000) ({r_i: None, r_t: [-869.711 -869.711 -869.711], eps: 1.0})
Step:   10500, Reward: [-449.531 -449.531 -449.531] [53.236], Avg: [-468.025 -468.025 -468.025] (1.0000) ({r_i: None, r_t: [-865.573 -865.573 -865.573], eps: 1.0})
Step:   10600, Reward: [-451.579 -451.579 -451.579] [90.810], Avg: [-467.871 -467.871 -467.871] (1.0000) ({r_i: None, r_t: [-864.329 -864.329 -864.329], eps: 1.0})
Step:   10700, Reward: [-451.965 -451.965 -451.965] [74.816], Avg: [-467.724 -467.724 -467.724] (1.0000) ({r_i: None, r_t: [-861.174 -861.174 -861.174], eps: 1.0})
Step:   10800, Reward: [-417.039 -417.039 -417.039] [67.218], Avg: [-467.259 -467.259 -467.259] (1.0000) ({r_i: None, r_t: [-857.651 -857.651 -857.651], eps: 1.0})
Step:   10900, Reward: [-452.177 -452.177 -452.177] [59.154], Avg: [-467.122 -467.122 -467.122] (1.0000) ({r_i: None, r_t: [-880.945 -880.945 -880.945], eps: 1.0})
Step:   11000, Reward: [-417.269 -417.269 -417.269] [73.916], Avg: [-466.673 -466.673 -466.673] (1.0000) ({r_i: None, r_t: [-890.065 -890.065 -890.065], eps: 1.0})
Step:   11100, Reward: [-479.443 -479.443 -479.443] [78.477], Avg: [-466.787 -466.787 -466.787] (1.0000) ({r_i: None, r_t: [-862.462 -862.462 -862.462], eps: 1.0})
Step:   11200, Reward: [-440.436 -440.436 -440.436] [69.325], Avg: [-466.553 -466.553 -466.553] (1.0000) ({r_i: None, r_t: [-926.218 -926.218 -926.218], eps: 1.0})
Step:   11300, Reward: [-420.134 -420.134 -420.134] [39.008], Avg: [-466.146 -466.146 -466.146] (1.0000) ({r_i: None, r_t: [-881.637 -881.637 -881.637], eps: 1.0})
Step:   11400, Reward: [-432.550 -432.550 -432.550] [78.831], Avg: [-465.854 -465.854 -465.854] (1.0000) ({r_i: None, r_t: [-868.033 -868.033 -868.033], eps: 1.0})
Step:   11500, Reward: [-451.318 -451.318 -451.318] [82.070], Avg: [-465.729 -465.729 -465.729] (1.0000) ({r_i: None, r_t: [-894.030 -894.030 -894.030], eps: 1.0})
Step:   11600, Reward: [-464.958 -464.958 -464.958] [79.895], Avg: [-465.722 -465.722 -465.722] (1.0000) ({r_i: None, r_t: [-898.545 -898.545 -898.545], eps: 1.0})
Step:   11700, Reward: [-427.836 -427.836 -427.836] [73.934], Avg: [-465.401 -465.401 -465.401] (1.0000) ({r_i: None, r_t: [-888.858 -888.858 -888.858], eps: 1.0})
Step:   11800, Reward: [-470.655 -470.655 -470.655] [57.207], Avg: [-465.445 -465.445 -465.445] (1.0000) ({r_i: None, r_t: [-884.391 -884.391 -884.391], eps: 1.0})
Step:   11900, Reward: [-452.971 -452.971 -452.971] [70.532], Avg: [-465.341 -465.341 -465.341] (1.0000) ({r_i: None, r_t: [-878.427 -878.427 -878.427], eps: 1.0})
Step:   12000, Reward: [-444.497 -444.497 -444.497] [50.328], Avg: [-465.169 -465.169 -465.169] (1.0000) ({r_i: None, r_t: [-904.738 -904.738 -904.738], eps: 1.0})
Step:   12100, Reward: [-488.324 -488.324 -488.324] [103.144], Avg: [-465.359 -465.359 -465.359] (1.0000) ({r_i: None, r_t: [-886.345 -886.345 -886.345], eps: 1.0})
Step:   12200, Reward: [-436.275 -436.275 -436.275] [83.954], Avg: [-465.122 -465.122 -465.122] (1.0000) ({r_i: None, r_t: [-896.854 -896.854 -896.854], eps: 1.0})
Step:   12300, Reward: [-458.011 -458.011 -458.011] [48.669], Avg: [-465.065 -465.065 -465.065] (1.0000) ({r_i: None, r_t: [-888.470 -888.470 -888.470], eps: 1.0})
Step:   12400, Reward: [-419.542 -419.542 -419.542] [58.241], Avg: [-464.701 -464.701 -464.701] (1.0000) ({r_i: None, r_t: [-894.029 -894.029 -894.029], eps: 1.0})
Step:   12500, Reward: [-430.451 -430.451 -430.451] [69.914], Avg: [-464.429 -464.429 -464.429] (1.0000) ({r_i: None, r_t: [-956.216 -956.216 -956.216], eps: 1.0})
Step:   12600, Reward: [-484.129 -484.129 -484.129] [162.812], Avg: [-464.584 -464.584 -464.584] (1.0000) ({r_i: None, r_t: [-901.865 -901.865 -901.865], eps: 1.0})
Step:   12700, Reward: [-483.261 -483.261 -483.261] [89.439], Avg: [-464.730 -464.730 -464.730] (1.0000) ({r_i: None, r_t: [-935.132 -935.132 -935.132], eps: 1.0})
Step:   12800, Reward: [-413.123 -413.123 -413.123] [70.276], Avg: [-464.330 -464.330 -464.330] (1.0000) ({r_i: None, r_t: [-894.280 -894.280 -894.280], eps: 1.0})
Step:   12900, Reward: [-461.214 -461.214 -461.214] [130.127], Avg: [-464.306 -464.306 -464.306] (1.0000) ({r_i: None, r_t: [-868.144 -868.144 -868.144], eps: 1.0})
Step:   13000, Reward: [-457.021 -457.021 -457.021] [82.118], Avg: [-464.250 -464.250 -464.250] (1.0000) ({r_i: None, r_t: [-896.907 -896.907 -896.907], eps: 1.0})
Step:   13100, Reward: [-435.625 -435.625 -435.625] [66.255], Avg: [-464.034 -464.034 -464.034] (1.0000) ({r_i: None, r_t: [-911.474 -911.474 -911.474], eps: 1.0})
Step:   13200, Reward: [-414.344 -414.344 -414.344] [77.925], Avg: [-463.660 -463.660 -463.660] (1.0000) ({r_i: None, r_t: [-865.238 -865.238 -865.238], eps: 1.0})
Step:   13300, Reward: [-428.059 -428.059 -428.059] [51.987], Avg: [-463.394 -463.394 -463.394] (1.0000) ({r_i: None, r_t: [-886.235 -886.235 -886.235], eps: 1.0})
Step:   13400, Reward: [-398.174 -398.174 -398.174] [51.316], Avg: [-462.911 -462.911 -462.911] (1.0000) ({r_i: None, r_t: [-835.975 -835.975 -835.975], eps: 1.0})
Step:   13500, Reward: [-440.245 -440.245 -440.245] [76.391], Avg: [-462.745 -462.745 -462.745] (1.0000) ({r_i: None, r_t: [-806.383 -806.383 -806.383], eps: 1.0})
Step:   13600, Reward: [-424.937 -424.937 -424.937] [54.758], Avg: [-462.469 -462.469 -462.469] (1.0000) ({r_i: None, r_t: [-854.706 -854.706 -854.706], eps: 1.0})
Step:   13700, Reward: [-407.041 -407.041 -407.041] [59.938], Avg: [-462.067 -462.067 -462.067] (1.0000) ({r_i: None, r_t: [-870.113 -870.113 -870.113], eps: 1.0})
Step:   13800, Reward: [-416.142 -416.142 -416.142] [40.000], Avg: [-461.737 -461.737 -461.737] (1.0000) ({r_i: None, r_t: [-817.912 -817.912 -817.912], eps: 1.0})
Step:   13900, Reward: [-439.170 -439.170 -439.170] [71.354], Avg: [-461.575 -461.575 -461.575] (1.0000) ({r_i: None, r_t: [-815.902 -815.902 -815.902], eps: 1.0})
Step:   14000, Reward: [-444.331 -444.331 -444.331] [88.331], Avg: [-461.453 -461.453 -461.453] (1.0000) ({r_i: None, r_t: [-801.025 -801.025 -801.025], eps: 1.0})
Step:   14100, Reward: [-405.690 -405.690 -405.690] [49.755], Avg: [-461.060 -461.060 -461.060] (1.0000) ({r_i: None, r_t: [-843.451 -843.451 -843.451], eps: 1.0})
Step:   14200, Reward: [-407.110 -407.110 -407.110] [64.584], Avg: [-460.683 -460.683 -460.683] (1.0000) ({r_i: None, r_t: [-762.958 -762.958 -762.958], eps: 1.0})
Step:   14300, Reward: [-406.328 -406.328 -406.328] [78.549], Avg: [-460.306 -460.306 -460.306] (1.0000) ({r_i: None, r_t: [-841.817 -841.817 -841.817], eps: 1.0})
Step:   14400, Reward: [-394.293 -394.293 -394.293] [60.401], Avg: [-459.850 -459.850 -459.850] (1.0000) ({r_i: None, r_t: [-845.886 -845.886 -845.886], eps: 1.0})
Step:   14500, Reward: [-421.371 -421.371 -421.371] [53.682], Avg: [-459.587 -459.587 -459.587] (1.0000) ({r_i: None, r_t: [-799.976 -799.976 -799.976], eps: 1.0})
Step:   14600, Reward: [-427.032 -427.032 -427.032] [57.662], Avg: [-459.365 -459.365 -459.365] (1.0000) ({r_i: None, r_t: [-793.267 -793.267 -793.267], eps: 1.0})
Step:   14700, Reward: [-432.237 -432.237 -432.237] [70.013], Avg: [-459.182 -459.182 -459.182] (1.0000) ({r_i: None, r_t: [-839.839 -839.839 -839.839], eps: 1.0})
Step:   14800, Reward: [-429.491 -429.491 -429.491] [52.795], Avg: [-458.983 -458.983 -458.983] (1.0000) ({r_i: None, r_t: [-867.363 -867.363 -867.363], eps: 1.0})
Step:   14900, Reward: [-441.589 -441.589 -441.589] [98.792], Avg: [-458.867 -458.867 -458.867] (1.0000) ({r_i: None, r_t: [-860.909 -860.909 -860.909], eps: 1.0})
Step:   15000, Reward: [-425.580 -425.580 -425.580] [53.051], Avg: [-458.646 -458.646 -458.646] (1.0000) ({r_i: None, r_t: [-792.597 -792.597 -792.597], eps: 1.0})
Step:   15100, Reward: [-437.462 -437.462 -437.462] [53.041], Avg: [-458.507 -458.507 -458.507] (1.0000) ({r_i: None, r_t: [-853.983 -853.983 -853.983], eps: 1.0})
Step:   15200, Reward: [-413.488 -413.488 -413.488] [57.181], Avg: [-458.213 -458.213 -458.213] (1.0000) ({r_i: None, r_t: [-830.671 -830.671 -830.671], eps: 1.0})
Step:   15300, Reward: [-431.095 -431.095 -431.095] [69.888], Avg: [-458.037 -458.037 -458.037] (1.0000) ({r_i: None, r_t: [-832.023 -832.023 -832.023], eps: 1.0})
Step:   15400, Reward: [-451.210 -451.210 -451.210] [91.183], Avg: [-457.993 -457.993 -457.993] (1.0000) ({r_i: None, r_t: [-831.261 -831.261 -831.261], eps: 1.0})
Step:   15500, Reward: [-432.400 -432.400 -432.400] [102.952], Avg: [-457.829 -457.829 -457.829] (1.0000) ({r_i: None, r_t: [-843.996 -843.996 -843.996], eps: 1.0})
Step:   15600, Reward: [-458.916 -458.916 -458.916] [96.955], Avg: [-457.836 -457.836 -457.836] (1.0000) ({r_i: None, r_t: [-850.229 -850.229 -850.229], eps: 1.0})
Step:   15700, Reward: [-445.609 -445.609 -445.609] [78.516], Avg: [-457.758 -457.758 -457.758] (1.0000) ({r_i: None, r_t: [-854.850 -854.850 -854.850], eps: 1.0})
Step:   15800, Reward: [-442.209 -442.209 -442.209] [61.593], Avg: [-457.660 -457.660 -457.660] (1.0000) ({r_i: None, r_t: [-848.566 -848.566 -848.566], eps: 1.0})
Step:   15900, Reward: [-422.503 -422.503 -422.503] [67.990], Avg: [-457.441 -457.441 -457.441] (1.0000) ({r_i: None, r_t: [-852.941 -852.941 -852.941], eps: 1.0})
Step:   16000, Reward: [-414.196 -414.196 -414.196] [64.290], Avg: [-457.172 -457.172 -457.172] (1.0000) ({r_i: None, r_t: [-863.246 -863.246 -863.246], eps: 1.0})
Step:   16100, Reward: [-443.303 -443.303 -443.303] [69.803], Avg: [-457.086 -457.086 -457.086] (1.0000) ({r_i: None, r_t: [-797.349 -797.349 -797.349], eps: 1.0})
Step:   16200, Reward: [-440.091 -440.091 -440.091] [61.037], Avg: [-456.982 -456.982 -456.982] (1.0000) ({r_i: None, r_t: [-833.870 -833.870 -833.870], eps: 1.0})
Step:   16300, Reward: [-442.691 -442.691 -442.691] [86.769], Avg: [-456.895 -456.895 -456.895] (1.0000) ({r_i: None, r_t: [-863.836 -863.836 -863.836], eps: 1.0})
Step:   16400, Reward: [-429.593 -429.593 -429.593] [60.507], Avg: [-456.730 -456.730 -456.730] (1.0000) ({r_i: None, r_t: [-833.391 -833.391 -833.391], eps: 1.0})
Step:   16500, Reward: [-446.118 -446.118 -446.118] [59.075], Avg: [-456.666 -456.666 -456.666] (1.0000) ({r_i: None, r_t: [-811.571 -811.571 -811.571], eps: 1.0})
Step:   16600, Reward: [-431.084 -431.084 -431.084] [59.755], Avg: [-456.512 -456.512 -456.512] (1.0000) ({r_i: None, r_t: [-847.597 -847.597 -847.597], eps: 1.0})
Step:   16700, Reward: [-402.734 -402.734 -402.734] [59.318], Avg: [-456.192 -456.192 -456.192] (1.0000) ({r_i: None, r_t: [-853.036 -853.036 -853.036], eps: 1.0})
Step:   16800, Reward: [-413.693 -413.693 -413.693] [63.632], Avg: [-455.941 -455.941 -455.941] (1.0000) ({r_i: None, r_t: [-806.689 -806.689 -806.689], eps: 1.0})
Step:   16900, Reward: [-429.589 -429.589 -429.589] [53.649], Avg: [-455.786 -455.786 -455.786] (1.0000) ({r_i: None, r_t: [-812.324 -812.324 -812.324], eps: 1.0})
Step:   17000, Reward: [-428.430 -428.430 -428.430] [54.029], Avg: [-455.626 -455.626 -455.626] (1.0000) ({r_i: None, r_t: [-833.965 -833.965 -833.965], eps: 1.0})
Step:   17100, Reward: [-424.457 -424.457 -424.457] [51.353], Avg: [-455.445 -455.445 -455.445] (1.0000) ({r_i: None, r_t: [-854.467 -854.467 -854.467], eps: 1.0})
Step:   17200, Reward: [-439.474 -439.474 -439.474] [56.185], Avg: [-455.352 -455.352 -455.352] (1.0000) ({r_i: None, r_t: [-831.151 -831.151 -831.151], eps: 1.0})
Step:   17300, Reward: [-406.537 -406.537 -406.537] [64.150], Avg: [-455.072 -455.072 -455.072] (1.0000) ({r_i: None, r_t: [-829.304 -829.304 -829.304], eps: 1.0})
Step:   17400, Reward: [-376.382 -376.382 -376.382] [52.649], Avg: [-454.622 -454.622 -454.622] (1.0000) ({r_i: None, r_t: [-808.255 -808.255 -808.255], eps: 1.0})
Step:   17500, Reward: [-400.997 -400.997 -400.997] [61.234], Avg: [-454.317 -454.317 -454.317] (1.0000) ({r_i: None, r_t: [-801.452 -801.452 -801.452], eps: 1.0})
Step:   17600, Reward: [-400.983 -400.983 -400.983] [53.629], Avg: [-454.016 -454.016 -454.016] (1.0000) ({r_i: None, r_t: [-787.750 -787.750 -787.750], eps: 1.0})
Step:   17700, Reward: [-390.812 -390.812 -390.812] [42.515], Avg: [-453.661 -453.661 -453.661] (1.0000) ({r_i: None, r_t: [-844.988 -844.988 -844.988], eps: 1.0})
Step:   17800, Reward: [-403.805 -403.805 -403.805] [66.812], Avg: [-453.382 -453.382 -453.382] (1.0000) ({r_i: None, r_t: [-820.999 -820.999 -820.999], eps: 1.0})
Step:   17900, Reward: [-428.044 -428.044 -428.044] [54.767], Avg: [-453.242 -453.242 -453.242] (1.0000) ({r_i: None, r_t: [-778.990 -778.990 -778.990], eps: 1.0})
Step:   18000, Reward: [-422.767 -422.767 -422.767] [57.886], Avg: [-453.073 -453.073 -453.073] (1.0000) ({r_i: None, r_t: [-815.578 -815.578 -815.578], eps: 1.0})
Step:   18100, Reward: [-439.207 -439.207 -439.207] [57.394], Avg: [-452.997 -452.997 -452.997] (1.0000) ({r_i: None, r_t: [-858.032 -858.032 -858.032], eps: 1.0})
Step:   18200, Reward: [-429.911 -429.911 -429.911] [61.808], Avg: [-452.871 -452.871 -452.871] (1.0000) ({r_i: None, r_t: [-845.162 -845.162 -845.162], eps: 1.0})
Step:   18300, Reward: [-411.570 -411.570 -411.570] [56.769], Avg: [-452.647 -452.647 -452.647] (1.0000) ({r_i: None, r_t: [-841.529 -841.529 -841.529], eps: 1.0})
Step:   18400, Reward: [-435.959 -435.959 -435.959] [67.691], Avg: [-452.556 -452.556 -452.556] (1.0000) ({r_i: None, r_t: [-866.343 -866.343 -866.343], eps: 1.0})
Step:   18500, Reward: [-475.748 -475.748 -475.748] [57.472], Avg: [-452.681 -452.681 -452.681] (1.0000) ({r_i: None, r_t: [-856.829 -856.829 -856.829], eps: 1.0})
Step:   18600, Reward: [-451.782 -451.782 -451.782] [63.902], Avg: [-452.676 -452.676 -452.676] (1.0000) ({r_i: None, r_t: [-851.724 -851.724 -851.724], eps: 1.0})
Step:   18700, Reward: [-426.331 -426.331 -426.331] [47.442], Avg: [-452.536 -452.536 -452.536] (1.0000) ({r_i: None, r_t: [-857.741 -857.741 -857.741], eps: 1.0})
Step:   18800, Reward: [-425.085 -425.085 -425.085] [69.203], Avg: [-452.391 -452.391 -452.391] (1.0000) ({r_i: None, r_t: [-872.863 -872.863 -872.863], eps: 1.0})
Step:   18900, Reward: [-438.077 -438.077 -438.077] [74.254], Avg: [-452.316 -452.316 -452.316] (1.0000) ({r_i: None, r_t: [-837.727 -837.727 -837.727], eps: 1.0})
Step:   19000, Reward: [-426.692 -426.692 -426.692] [47.894], Avg: [-452.181 -452.181 -452.181] (1.0000) ({r_i: None, r_t: [-915.387 -915.387 -915.387], eps: 1.0})
Step:   19100, Reward: [-414.438 -414.438 -414.438] [70.161], Avg: [-451.985 -451.985 -451.985] (1.0000) ({r_i: None, r_t: [-888.729 -888.729 -888.729], eps: 1.0})
Step:   19200, Reward: [-423.850 -423.850 -423.850] [52.915], Avg: [-451.839 -451.839 -451.839] (1.0000) ({r_i: None, r_t: [-853.943 -853.943 -853.943], eps: 1.0})
Step:   19300, Reward: [-415.623 -415.623 -415.623] [51.664], Avg: [-451.652 -451.652 -451.652] (1.0000) ({r_i: None, r_t: [-874.399 -874.399 -874.399], eps: 1.0})
Step:   19400, Reward: [-415.649 -415.649 -415.649] [58.724], Avg: [-451.468 -451.468 -451.468] (1.0000) ({r_i: None, r_t: [-838.788 -838.788 -838.788], eps: 1.0})
Step:   19500, Reward: [-454.738 -454.738 -454.738] [57.564], Avg: [-451.484 -451.484 -451.484] (1.0000) ({r_i: None, r_t: [-848.417 -848.417 -848.417], eps: 1.0})
Step:   19600, Reward: [-408.565 -408.565 -408.565] [53.560], Avg: [-451.267 -451.267 -451.267] (1.0000) ({r_i: None, r_t: [-848.231 -848.231 -848.231], eps: 1.0})
Step:   19700, Reward: [-415.031 -415.031 -415.031] [61.562], Avg: [-451.083 -451.083 -451.083] (1.0000) ({r_i: None, r_t: [-844.083 -844.083 -844.083], eps: 1.0})
Step:   19800, Reward: [-437.926 -437.926 -437.926] [71.795], Avg: [-451.017 -451.017 -451.017] (1.0000) ({r_i: None, r_t: [-828.491 -828.491 -828.491], eps: 1.0})
Step:   19900, Reward: [-425.491 -425.491 -425.491] [69.487], Avg: [-450.890 -450.890 -450.890] (1.0000) ({r_i: None, r_t: [-821.541 -821.541 -821.541], eps: 1.0})
Step:   20000, Reward: [-392.751 -392.751 -392.751] [43.862], Avg: [-450.600 -450.600 -450.600] (1.0000) ({r_i: None, r_t: [-780.117 -780.117 -780.117], eps: 1.0})
Step:   20100, Reward: [-407.602 -407.602 -407.602] [77.244], Avg: [-450.388 -450.388 -450.388] (1.0000) ({r_i: None, r_t: [-835.827 -835.827 -835.827], eps: 1.0})
Step:   20200, Reward: [-402.121 -402.121 -402.121] [62.644], Avg: [-450.150 -450.150 -450.150] (1.0000) ({r_i: None, r_t: [-822.441 -822.441 -822.441], eps: 1.0})
Step:   20300, Reward: [-423.826 -423.826 -423.826] [52.223], Avg: [-450.021 -450.021 -450.021] (1.0000) ({r_i: None, r_t: [-834.327 -834.327 -834.327], eps: 1.0})
Step:   20400, Reward: [-434.085 -434.085 -434.085] [66.390], Avg: [-449.943 -449.943 -449.943] (1.0000) ({r_i: None, r_t: [-770.259 -770.259 -770.259], eps: 1.0})
Step:   20500, Reward: [-397.819 -397.819 -397.819] [74.583], Avg: [-449.690 -449.690 -449.690] (1.0000) ({r_i: None, r_t: [-809.971 -809.971 -809.971], eps: 1.0})
Step:   20600, Reward: [-370.493 -370.493 -370.493] [54.097], Avg: [-449.307 -449.307 -449.307] (1.0000) ({r_i: None, r_t: [-799.286 -799.286 -799.286], eps: 1.0})
Step:   20700, Reward: [-398.422 -398.422 -398.422] [68.646], Avg: [-449.063 -449.063 -449.063] (1.0000) ({r_i: None, r_t: [-796.370 -796.370 -796.370], eps: 1.0})
Step:   20800, Reward: [-421.615 -421.615 -421.615] [56.659], Avg: [-448.931 -448.931 -448.931] (1.0000) ({r_i: None, r_t: [-795.894 -795.894 -795.894], eps: 1.0})
Step:   20900, Reward: [-421.942 -421.942 -421.942] [47.849], Avg: [-448.803 -448.803 -448.803] (1.0000) ({r_i: None, r_t: [-816.684 -816.684 -816.684], eps: 1.0})
Step:   21000, Reward: [-418.298 -418.298 -418.298] [51.037], Avg: [-448.658 -448.658 -448.658] (1.0000) ({r_i: None, r_t: [-838.199 -838.199 -838.199], eps: 1.0})
Step:   21100, Reward: [-413.682 -413.682 -413.682] [70.466], Avg: [-448.493 -448.493 -448.493] (1.0000) ({r_i: None, r_t: [-786.958 -786.958 -786.958], eps: 1.0})
Step:   21200, Reward: [-399.742 -399.742 -399.742] [66.087], Avg: [-448.265 -448.265 -448.265] (1.0000) ({r_i: None, r_t: [-842.944 -842.944 -842.944], eps: 1.0})
Step:   21300, Reward: [-406.570 -406.570 -406.570] [63.843], Avg: [-448.070 -448.070 -448.070] (1.0000) ({r_i: None, r_t: [-835.172 -835.172 -835.172], eps: 1.0})
Step:   21400, Reward: [-409.439 -409.439 -409.439] [56.190], Avg: [-447.890 -447.890 -447.890] (1.0000) ({r_i: None, r_t: [-830.842 -830.842 -830.842], eps: 1.0})
Step:   21500, Reward: [-421.708 -421.708 -421.708] [59.729], Avg: [-447.769 -447.769 -447.769] (1.0000) ({r_i: None, r_t: [-850.744 -850.744 -850.744], eps: 1.0})
Step:   21600, Reward: [-387.223 -387.223 -387.223] [58.011], Avg: [-447.490 -447.490 -447.490] (1.0000) ({r_i: None, r_t: [-846.669 -846.669 -846.669], eps: 1.0})
Step:   21700, Reward: [-399.362 -399.362 -399.362] [74.803], Avg: [-447.269 -447.269 -447.269] (1.0000) ({r_i: None, r_t: [-829.231 -829.231 -829.231], eps: 1.0})
Step:   21800, Reward: [-432.407 -432.407 -432.407] [75.586], Avg: [-447.201 -447.201 -447.201] (1.0000) ({r_i: None, r_t: [-812.948 -812.948 -812.948], eps: 1.0})
Step:   21900, Reward: [-402.191 -402.191 -402.191] [74.727], Avg: [-446.997 -446.997 -446.997] (1.0000) ({r_i: None, r_t: [-857.643 -857.643 -857.643], eps: 1.0})
Step:   22000, Reward: [-408.382 -408.382 -408.382] [64.093], Avg: [-446.822 -446.822 -446.822] (1.0000) ({r_i: None, r_t: [-817.297 -817.297 -817.297], eps: 1.0})
Step:   22100, Reward: [-443.319 -443.319 -443.319] [59.576], Avg: [-446.806 -446.806 -446.806] (1.0000) ({r_i: None, r_t: [-867.337 -867.337 -867.337], eps: 1.0})
Step:   22200, Reward: [-424.136 -424.136 -424.136] [66.164], Avg: [-446.704 -446.704 -446.704] (1.0000) ({r_i: None, r_t: [-828.646 -828.646 -828.646], eps: 1.0})
Step:   22300, Reward: [-422.811 -422.811 -422.811] [66.803], Avg: [-446.598 -446.598 -446.598] (1.0000) ({r_i: None, r_t: [-821.195 -821.195 -821.195], eps: 1.0})
Step:   22400, Reward: [-407.956 -407.956 -407.956] [63.315], Avg: [-446.426 -446.426 -446.426] (1.0000) ({r_i: None, r_t: [-870.874 -870.874 -870.874], eps: 1.0})
Step:   22500, Reward: [-411.710 -411.710 -411.710] [58.812], Avg: [-446.272 -446.272 -446.272] (1.0000) ({r_i: None, r_t: [-858.949 -858.949 -858.949], eps: 1.0})
Step:   22600, Reward: [-414.145 -414.145 -414.145] [56.344], Avg: [-446.131 -446.131 -446.131] (1.0000) ({r_i: None, r_t: [-809.268 -809.268 -809.268], eps: 1.0})
Step:   22700, Reward: [-407.664 -407.664 -407.664] [53.253], Avg: [-445.962 -445.962 -445.962] (1.0000) ({r_i: None, r_t: [-782.971 -782.971 -782.971], eps: 1.0})
Step:   22800, Reward: [-403.758 -403.758 -403.758] [57.135], Avg: [-445.778 -445.778 -445.778] (1.0000) ({r_i: None, r_t: [-885.871 -885.871 -885.871], eps: 1.0})
Step:   22900, Reward: [-422.980 -422.980 -422.980] [64.426], Avg: [-445.679 -445.679 -445.679] (1.0000) ({r_i: None, r_t: [-839.966 -839.966 -839.966], eps: 1.0})
Step:   23000, Reward: [-398.122 -398.122 -398.122] [36.378], Avg: [-445.473 -445.473 -445.473] (1.0000) ({r_i: None, r_t: [-790.008 -790.008 -790.008], eps: 1.0})
Step:   23100, Reward: [-421.678 -421.678 -421.678] [43.379], Avg: [-445.370 -445.370 -445.370] (1.0000) ({r_i: None, r_t: [-800.683 -800.683 -800.683], eps: 1.0})
Step:   23200, Reward: [-381.772 -381.772 -381.772] [40.526], Avg: [-445.097 -445.097 -445.097] (1.0000) ({r_i: None, r_t: [-804.351 -804.351 -804.351], eps: 1.0})
Step:   23300, Reward: [-405.439 -405.439 -405.439] [85.905], Avg: [-444.928 -444.928 -444.928] (1.0000) ({r_i: None, r_t: [-792.166 -792.166 -792.166], eps: 1.0})
Step:   23400, Reward: [-408.713 -408.713 -408.713] [54.161], Avg: [-444.774 -444.774 -444.774] (1.0000) ({r_i: None, r_t: [-792.312 -792.312 -792.312], eps: 1.0})
Step:   23500, Reward: [-415.960 -415.960 -415.960] [87.481], Avg: [-444.652 -444.652 -444.652] (1.0000) ({r_i: None, r_t: [-827.194 -827.194 -827.194], eps: 1.0})
Step:   23600, Reward: [-412.907 -412.907 -412.907] [48.801], Avg: [-444.518 -444.518 -444.518] (1.0000) ({r_i: None, r_t: [-797.390 -797.390 -797.390], eps: 1.0})
Step:   23700, Reward: [-392.769 -392.769 -392.769] [54.816], Avg: [-444.300 -444.300 -444.300] (1.0000) ({r_i: None, r_t: [-819.526 -819.526 -819.526], eps: 1.0})
Step:   23800, Reward: [-405.162 -405.162 -405.162] [42.843], Avg: [-444.137 -444.137 -444.137] (1.0000) ({r_i: None, r_t: [-815.043 -815.043 -815.043], eps: 1.0})
Step:   23900, Reward: [-433.139 -433.139 -433.139] [60.511], Avg: [-444.091 -444.091 -444.091] (1.0000) ({r_i: None, r_t: [-866.658 -866.658 -866.658], eps: 1.0})
Step:   24000, Reward: [-407.921 -407.921 -407.921] [55.696], Avg: [-443.941 -443.941 -443.941] (1.0000) ({r_i: None, r_t: [-830.345 -830.345 -830.345], eps: 1.0})
Step:   24100, Reward: [-400.951 -400.951 -400.951] [47.389], Avg: [-443.763 -443.763 -443.763] (1.0000) ({r_i: None, r_t: [-826.900 -826.900 -826.900], eps: 1.0})
Step:   24200, Reward: [-425.370 -425.370 -425.370] [76.382], Avg: [-443.687 -443.687 -443.687] (1.0000) ({r_i: None, r_t: [-846.185 -846.185 -846.185], eps: 1.0})
Step:   24300, Reward: [-406.358 -406.358 -406.358] [67.165], Avg: [-443.534 -443.534 -443.534] (1.0000) ({r_i: None, r_t: [-885.915 -885.915 -885.915], eps: 1.0})
Step:   24400, Reward: [-426.634 -426.634 -426.634] [77.731], Avg: [-443.465 -443.465 -443.465] (1.0000) ({r_i: None, r_t: [-820.330 -820.330 -820.330], eps: 1.0})
Step:   24500, Reward: [-419.976 -419.976 -419.976] [55.036], Avg: [-443.370 -443.370 -443.370] (1.0000) ({r_i: None, r_t: [-798.208 -798.208 -798.208], eps: 1.0})
Step:   24600, Reward: [-421.858 -421.858 -421.858] [61.438], Avg: [-443.283 -443.283 -443.283] (1.0000) ({r_i: None, r_t: [-839.902 -839.902 -839.902], eps: 1.0})
Step:   24700, Reward: [-419.741 -419.741 -419.741] [68.967], Avg: [-443.188 -443.188 -443.188] (1.0000) ({r_i: None, r_t: [-865.689 -865.689 -865.689], eps: 1.0})
Step:   24800, Reward: [-435.684 -435.684 -435.684] [78.716], Avg: [-443.158 -443.158 -443.158] (1.0000) ({r_i: None, r_t: [-852.181 -852.181 -852.181], eps: 1.0})
Step:   24900, Reward: [-432.521 -432.521 -432.521] [76.284], Avg: [-443.115 -443.115 -443.115] (1.0000) ({r_i: None, r_t: [-849.525 -849.525 -849.525], eps: 1.0})
Step:   25000, Reward: [-430.349 -430.349 -430.349] [50.183], Avg: [-443.064 -443.064 -443.064] (1.0000) ({r_i: None, r_t: [-844.082 -844.082 -844.082], eps: 1.0})
Step:   25100, Reward: [-431.994 -431.994 -431.994] [76.067], Avg: [-443.020 -443.020 -443.020] (1.0000) ({r_i: None, r_t: [-832.534 -832.534 -832.534], eps: 1.0})
Step:   25200, Reward: [-419.736 -419.736 -419.736] [60.821], Avg: [-442.928 -442.928 -442.928] (1.0000) ({r_i: None, r_t: [-843.742 -843.742 -843.742], eps: 1.0})
Step:   25300, Reward: [-412.559 -412.559 -412.559] [67.883], Avg: [-442.809 -442.809 -442.809] (1.0000) ({r_i: None, r_t: [-801.834 -801.834 -801.834], eps: 1.0})
Step:   25400, Reward: [-414.200 -414.200 -414.200] [55.787], Avg: [-442.697 -442.697 -442.697] (1.0000) ({r_i: None, r_t: [-890.539 -890.539 -890.539], eps: 1.0})
Step:   25500, Reward: [-436.213 -436.213 -436.213] [67.418], Avg: [-442.671 -442.671 -442.671] (1.0000) ({r_i: None, r_t: [-875.172 -875.172 -875.172], eps: 1.0})
Step:   25600, Reward: [-435.523 -435.523 -435.523] [68.937], Avg: [-442.643 -442.643 -442.643] (1.0000) ({r_i: None, r_t: [-850.084 -850.084 -850.084], eps: 1.0})
Step:   25700, Reward: [-428.197 -428.197 -428.197] [68.250], Avg: [-442.587 -442.587 -442.587] (1.0000) ({r_i: None, r_t: [-847.855 -847.855 -847.855], eps: 1.0})
Step:   25800, Reward: [-416.203 -416.203 -416.203] [65.622], Avg: [-442.486 -442.486 -442.486] (1.0000) ({r_i: None, r_t: [-816.881 -816.881 -816.881], eps: 1.0})
Step:   25900, Reward: [-395.358 -395.358 -395.358] [55.685], Avg: [-442.304 -442.304 -442.304] (1.0000) ({r_i: None, r_t: [-811.374 -811.374 -811.374], eps: 1.0})
Step:   26000, Reward: [-423.087 -423.087 -423.087] [83.207], Avg: [-442.231 -442.231 -442.231] (1.0000) ({r_i: None, r_t: [-826.128 -826.128 -826.128], eps: 1.0})
Step:   26100, Reward: [-416.072 -416.072 -416.072] [78.937], Avg: [-442.131 -442.131 -442.131] (1.0000) ({r_i: None, r_t: [-819.313 -819.313 -819.313], eps: 1.0})
Step:   26200, Reward: [-408.403 -408.403 -408.403] [62.680], Avg: [-442.003 -442.003 -442.003] (1.0000) ({r_i: None, r_t: [-832.452 -832.452 -832.452], eps: 1.0})
Step:   26300, Reward: [-373.105 -373.105 -373.105] [53.040], Avg: [-441.742 -441.742 -441.742] (1.0000) ({r_i: None, r_t: [-845.018 -845.018 -845.018], eps: 1.0})
Step:   26400, Reward: [-425.145 -425.145 -425.145] [53.820], Avg: [-441.679 -441.679 -441.679] (1.0000) ({r_i: None, r_t: [-831.122 -831.122 -831.122], eps: 1.0})
Step:   26500, Reward: [-421.724 -421.724 -421.724] [65.291], Avg: [-441.604 -441.604 -441.604] (1.0000) ({r_i: None, r_t: [-810.631 -810.631 -810.631], eps: 1.0})
Step:   26600, Reward: [-457.050 -457.050 -457.050] [58.029], Avg: [-441.662 -441.662 -441.662] (1.0000) ({r_i: None, r_t: [-842.947 -842.947 -842.947], eps: 1.0})
Step:   26700, Reward: [-423.411 -423.411 -423.411] [48.431], Avg: [-441.594 -441.594 -441.594] (1.0000) ({r_i: None, r_t: [-858.639 -858.639 -858.639], eps: 1.0})
Step:   26800, Reward: [-397.723 -397.723 -397.723] [56.279], Avg: [-441.431 -441.431 -441.431] (1.0000) ({r_i: None, r_t: [-841.337 -841.337 -841.337], eps: 1.0})
Step:   26900, Reward: [-421.339 -421.339 -421.339] [61.900], Avg: [-441.356 -441.356 -441.356] (1.0000) ({r_i: None, r_t: [-842.123 -842.123 -842.123], eps: 1.0})
Step:   27000, Reward: [-397.931 -397.931 -397.931] [56.743], Avg: [-441.196 -441.196 -441.196] (1.0000) ({r_i: None, r_t: [-871.529 -871.529 -871.529], eps: 1.0})
Step:   27100, Reward: [-410.235 -410.235 -410.235] [66.660], Avg: [-441.082 -441.082 -441.082] (1.0000) ({r_i: None, r_t: [-845.983 -845.983 -845.983], eps: 1.0})
Step:   27200, Reward: [-422.535 -422.535 -422.535] [58.237], Avg: [-441.014 -441.014 -441.014] (1.0000) ({r_i: None, r_t: [-829.722 -829.722 -829.722], eps: 1.0})
Step:   27300, Reward: [-444.756 -444.756 -444.756] [62.665], Avg: [-441.028 -441.028 -441.028] (1.0000) ({r_i: None, r_t: [-852.465 -852.465 -852.465], eps: 1.0})
Step:   27400, Reward: [-410.724 -410.724 -410.724] [65.210], Avg: [-440.918 -440.918 -440.918] (1.0000) ({r_i: None, r_t: [-857.152 -857.152 -857.152], eps: 1.0})
Step:   27500, Reward: [-405.498 -405.498 -405.498] [69.574], Avg: [-440.789 -440.789 -440.789] (1.0000) ({r_i: None, r_t: [-848.260 -848.260 -848.260], eps: 1.0})
Step:   27600, Reward: [-433.519 -433.519 -433.519] [56.459], Avg: [-440.763 -440.763 -440.763] (1.0000) ({r_i: None, r_t: [-836.155 -836.155 -836.155], eps: 1.0})
Step:   27700, Reward: [-420.535 -420.535 -420.535] [66.002], Avg: [-440.690 -440.690 -440.690] (1.0000) ({r_i: None, r_t: [-860.543 -860.543 -860.543], eps: 1.0})
Step:   27800, Reward: [-417.503 -417.503 -417.503] [69.927], Avg: [-440.607 -440.607 -440.607] (1.0000) ({r_i: None, r_t: [-819.053 -819.053 -819.053], eps: 1.0})
Step:   27900, Reward: [-405.613 -405.613 -405.613] [43.451], Avg: [-440.482 -440.482 -440.482] (1.0000) ({r_i: None, r_t: [-817.623 -817.623 -817.623], eps: 1.0})
Step:   28000, Reward: [-451.844 -451.844 -451.844] [71.156], Avg: [-440.523 -440.523 -440.523] (1.0000) ({r_i: None, r_t: [-908.820 -908.820 -908.820], eps: 1.0})
Step:   28100, Reward: [-416.103 -416.103 -416.103] [64.897], Avg: [-440.436 -440.436 -440.436] (1.0000) ({r_i: None, r_t: [-864.273 -864.273 -864.273], eps: 1.0})
Step:   28200, Reward: [-457.342 -457.342 -457.342] [75.396], Avg: [-440.496 -440.496 -440.496] (1.0000) ({r_i: None, r_t: [-877.368 -877.368 -877.368], eps: 1.0})
Step:   28300, Reward: [-427.774 -427.774 -427.774] [54.421], Avg: [-440.451 -440.451 -440.451] (1.0000) ({r_i: None, r_t: [-874.732 -874.732 -874.732], eps: 1.0})
Step:   28400, Reward: [-419.648 -419.648 -419.648] [53.702], Avg: [-440.378 -440.378 -440.378] (1.0000) ({r_i: None, r_t: [-825.220 -825.220 -825.220], eps: 1.0})
Step:   28500, Reward: [-428.194 -428.194 -428.194] [62.782], Avg: [-440.335 -440.335 -440.335] (1.0000) ({r_i: None, r_t: [-882.072 -882.072 -882.072], eps: 1.0})
Step:   28600, Reward: [-424.003 -424.003 -424.003] [52.307], Avg: [-440.279 -440.279 -440.279] (1.0000) ({r_i: None, r_t: [-884.138 -884.138 -884.138], eps: 1.0})
Step:   28700, Reward: [-436.400 -436.400 -436.400] [57.077], Avg: [-440.265 -440.265 -440.265] (1.0000) ({r_i: None, r_t: [-894.397 -894.397 -894.397], eps: 1.0})
Step:   28800, Reward: [-441.217 -441.217 -441.217] [69.086], Avg: [-440.268 -440.268 -440.268] (1.0000) ({r_i: None, r_t: [-849.580 -849.580 -849.580], eps: 1.0})
Step:   28900, Reward: [-420.668 -420.668 -420.668] [70.322], Avg: [-440.201 -440.201 -440.201] (1.0000) ({r_i: None, r_t: [-829.741 -829.741 -829.741], eps: 1.0})
Step:   29000, Reward: [-444.108 -444.108 -444.108] [66.111], Avg: [-440.214 -440.214 -440.214] (1.0000) ({r_i: None, r_t: [-852.045 -852.045 -852.045], eps: 1.0})
Step:   29100, Reward: [-432.369 -432.369 -432.369] [70.201], Avg: [-440.187 -440.187 -440.187] (1.0000) ({r_i: None, r_t: [-859.902 -859.902 -859.902], eps: 1.0})
Step:   29200, Reward: [-423.854 -423.854 -423.854] [50.138], Avg: [-440.132 -440.132 -440.132] (1.0000) ({r_i: None, r_t: [-851.904 -851.904 -851.904], eps: 1.0})
Step:   29300, Reward: [-425.943 -425.943 -425.943] [57.482], Avg: [-440.083 -440.083 -440.083] (1.0000) ({r_i: None, r_t: [-831.509 -831.509 -831.509], eps: 1.0})
Step:   29400, Reward: [-403.864 -403.864 -403.864] [59.130], Avg: [-439.961 -439.961 -439.961] (1.0000) ({r_i: None, r_t: [-888.248 -888.248 -888.248], eps: 1.0})
Step:   29500, Reward: [-442.465 -442.465 -442.465] [32.564], Avg: [-439.969 -439.969 -439.969] (1.0000) ({r_i: None, r_t: [-818.748 -818.748 -818.748], eps: 1.0})
Step:   29600, Reward: [-414.043 -414.043 -414.043] [72.525], Avg: [-439.882 -439.882 -439.882] (1.0000) ({r_i: None, r_t: [-863.252 -863.252 -863.252], eps: 1.0})
Step:   29700, Reward: [-437.973 -437.973 -437.973] [51.664], Avg: [-439.875 -439.875 -439.875] (1.0000) ({r_i: None, r_t: [-871.357 -871.357 -871.357], eps: 1.0})
Step:   29800, Reward: [-429.144 -429.144 -429.144] [51.824], Avg: [-439.839 -439.839 -439.839] (1.0000) ({r_i: None, r_t: [-845.170 -845.170 -845.170], eps: 1.0})
Step:   29900, Reward: [-396.055 -396.055 -396.055] [62.444], Avg: [-439.693 -439.693 -439.693] (1.0000) ({r_i: None, r_t: [-898.246 -898.246 -898.246], eps: 1.0})
Step:   30000, Reward: [-432.730 -432.730 -432.730] [71.500], Avg: [-439.670 -439.670 -439.670] (1.0000) ({r_i: None, r_t: [-893.068 -893.068 -893.068], eps: 1.0})
Step:   30100, Reward: [-443.007 -443.007 -443.007] [67.404], Avg: [-439.681 -439.681 -439.681] (1.0000) ({r_i: None, r_t: [-852.196 -852.196 -852.196], eps: 1.0})
Step:   30200, Reward: [-443.202 -443.202 -443.202] [55.004], Avg: [-439.693 -439.693 -439.693] (1.0000) ({r_i: None, r_t: [-853.322 -853.322 -853.322], eps: 1.0})
Step:   30300, Reward: [-455.378 -455.378 -455.378] [68.657], Avg: [-439.745 -439.745 -439.745] (1.0000) ({r_i: None, r_t: [-868.994 -868.994 -868.994], eps: 1.0})
Step:   30400, Reward: [-469.702 -469.702 -469.702] [46.604], Avg: [-439.843 -439.843 -439.843] (1.0000) ({r_i: None, r_t: [-831.630 -831.630 -831.630], eps: 1.0})
Step:   30500, Reward: [-406.503 -406.503 -406.503] [71.452], Avg: [-439.734 -439.734 -439.734] (1.0000) ({r_i: None, r_t: [-861.754 -861.754 -861.754], eps: 1.0})
Step:   30600, Reward: [-420.957 -420.957 -420.957] [64.877], Avg: [-439.673 -439.673 -439.673] (1.0000) ({r_i: None, r_t: [-886.596 -886.596 -886.596], eps: 1.0})
Step:   30700, Reward: [-455.028 -455.028 -455.028] [75.142], Avg: [-439.723 -439.723 -439.723] (1.0000) ({r_i: None, r_t: [-882.910 -882.910 -882.910], eps: 1.0})
Step:   30800, Reward: [-449.317 -449.317 -449.317] [78.716], Avg: [-439.754 -439.754 -439.754] (1.0000) ({r_i: None, r_t: [-855.218 -855.218 -855.218], eps: 1.0})
Step:   30900, Reward: [-407.307 -407.307 -407.307] [46.040], Avg: [-439.649 -439.649 -439.649] (1.0000) ({r_i: None, r_t: [-894.174 -894.174 -894.174], eps: 1.0})
Step:   31000, Reward: [-442.029 -442.029 -442.029] [65.665], Avg: [-439.657 -439.657 -439.657] (1.0000) ({r_i: None, r_t: [-844.514 -844.514 -844.514], eps: 1.0})
Step:   31100, Reward: [-452.526 -452.526 -452.526] [55.504], Avg: [-439.698 -439.698 -439.698] (1.0000) ({r_i: None, r_t: [-881.903 -881.903 -881.903], eps: 1.0})
Step:   31200, Reward: [-471.438 -471.438 -471.438] [69.174], Avg: [-439.799 -439.799 -439.799] (1.0000) ({r_i: None, r_t: [-892.714 -892.714 -892.714], eps: 1.0})
Step:   31300, Reward: [-463.461 -463.461 -463.461] [64.823], Avg: [-439.875 -439.875 -439.875] (1.0000) ({r_i: None, r_t: [-936.382 -936.382 -936.382], eps: 1.0})
Step:   31400, Reward: [-439.010 -439.010 -439.010] [74.937], Avg: [-439.872 -439.872 -439.872] (1.0000) ({r_i: None, r_t: [-908.708 -908.708 -908.708], eps: 1.0})
Step:   31500, Reward: [-437.054 -437.054 -437.054] [44.758], Avg: [-439.863 -439.863 -439.863] (1.0000) ({r_i: None, r_t: [-923.617 -923.617 -923.617], eps: 1.0})
Step:   31600, Reward: [-482.215 -482.215 -482.215] [70.829], Avg: [-439.997 -439.997 -439.997] (1.0000) ({r_i: None, r_t: [-865.414 -865.414 -865.414], eps: 1.0})
Step:   31700, Reward: [-464.564 -464.564 -464.564] [47.840], Avg: [-440.074 -440.074 -440.074] (1.0000) ({r_i: None, r_t: [-874.255 -874.255 -874.255], eps: 1.0})
Step:   31800, Reward: [-446.715 -446.715 -446.715] [56.606], Avg: [-440.095 -440.095 -440.095] (1.0000) ({r_i: None, r_t: [-874.073 -874.073 -874.073], eps: 1.0})
Step:   31900, Reward: [-479.486 -479.486 -479.486] [72.709], Avg: [-440.218 -440.218 -440.218] (1.0000) ({r_i: None, r_t: [-950.661 -950.661 -950.661], eps: 1.0})
Step:   32000, Reward: [-417.668 -417.668 -417.668] [54.660], Avg: [-440.147 -440.147 -440.147] (1.0000) ({r_i: None, r_t: [-869.625 -869.625 -869.625], eps: 1.0})
Step:   32100, Reward: [-427.597 -427.597 -427.597] [55.321], Avg: [-440.108 -440.108 -440.108] (1.0000) ({r_i: None, r_t: [-872.449 -872.449 -872.449], eps: 1.0})
Step:   32200, Reward: [-435.697 -435.697 -435.697] [46.889], Avg: [-440.095 -440.095 -440.095] (1.0000) ({r_i: None, r_t: [-817.023 -817.023 -817.023], eps: 1.0})
Step:   32300, Reward: [-427.783 -427.783 -427.783] [55.065], Avg: [-440.057 -440.057 -440.057] (1.0000) ({r_i: None, r_t: [-878.958 -878.958 -878.958], eps: 1.0})
Step:   32400, Reward: [-438.046 -438.046 -438.046] [73.991], Avg: [-440.051 -440.051 -440.051] (1.0000) ({r_i: None, r_t: [-869.954 -869.954 -869.954], eps: 1.0})
Step:   32500, Reward: [-427.691 -427.691 -427.691] [42.439], Avg: [-440.013 -440.013 -440.013] (1.0000) ({r_i: None, r_t: [-871.083 -871.083 -871.083], eps: 1.0})
Step:   32600, Reward: [-435.333 -435.333 -435.333] [56.470], Avg: [-439.998 -439.998 -439.998] (1.0000) ({r_i: None, r_t: [-901.364 -901.364 -901.364], eps: 1.0})
Step:   32700, Reward: [-458.980 -458.980 -458.980] [79.673], Avg: [-440.056 -440.056 -440.056] (1.0000) ({r_i: None, r_t: [-895.757 -895.757 -895.757], eps: 1.0})
Step:   32800, Reward: [-421.591 -421.591 -421.591] [73.688], Avg: [-440.000 -440.000 -440.000] (1.0000) ({r_i: None, r_t: [-851.238 -851.238 -851.238], eps: 1.0})
Step:   32900, Reward: [-418.837 -418.837 -418.837] [47.979], Avg: [-439.936 -439.936 -439.936] (1.0000) ({r_i: None, r_t: [-907.449 -907.449 -907.449], eps: 1.0})
Step:   33000, Reward: [-426.338 -426.338 -426.338] [72.197], Avg: [-439.895 -439.895 -439.895] (1.0000) ({r_i: None, r_t: [-890.913 -890.913 -890.913], eps: 1.0})
Step:   33100, Reward: [-447.446 -447.446 -447.446] [56.665], Avg: [-439.918 -439.918 -439.918] (1.0000) ({r_i: None, r_t: [-870.941 -870.941 -870.941], eps: 1.0})
Step:   33200, Reward: [-445.714 -445.714 -445.714] [68.247], Avg: [-439.935 -439.935 -439.935] (1.0000) ({r_i: None, r_t: [-878.204 -878.204 -878.204], eps: 1.0})
Step:   33300, Reward: [-410.466 -410.466 -410.466] [78.545], Avg: [-439.847 -439.847 -439.847] (1.0000) ({r_i: None, r_t: [-861.865 -861.865 -861.865], eps: 1.0})
Step:   33400, Reward: [-446.871 -446.871 -446.871] [67.340], Avg: [-439.868 -439.868 -439.868] (1.0000) ({r_i: None, r_t: [-893.208 -893.208 -893.208], eps: 1.0})
Step:   33500, Reward: [-459.699 -459.699 -459.699] [71.567], Avg: [-439.927 -439.927 -439.927] (1.0000) ({r_i: None, r_t: [-925.621 -925.621 -925.621], eps: 1.0})
Step:   33600, Reward: [-427.983 -427.983 -427.983] [73.194], Avg: [-439.891 -439.891 -439.891] (1.0000) ({r_i: None, r_t: [-887.665 -887.665 -887.665], eps: 1.0})
Step:   33700, Reward: [-471.433 -471.433 -471.433] [73.931], Avg: [-439.985 -439.985 -439.985] (1.0000) ({r_i: None, r_t: [-913.400 -913.400 -913.400], eps: 1.0})
Step:   33800, Reward: [-443.962 -443.962 -443.962] [66.663], Avg: [-439.996 -439.996 -439.996] (1.0000) ({r_i: None, r_t: [-918.512 -918.512 -918.512], eps: 1.0})
Step:   33900, Reward: [-466.523 -466.523 -466.523] [50.955], Avg: [-440.074 -440.074 -440.074] (1.0000) ({r_i: None, r_t: [-903.670 -903.670 -903.670], eps: 1.0})
Step:   34000, Reward: [-441.102 -441.102 -441.102] [71.832], Avg: [-440.078 -440.078 -440.078] (1.0000) ({r_i: None, r_t: [-897.399 -897.399 -897.399], eps: 1.0})
Step:   34100, Reward: [-425.409 -425.409 -425.409] [64.304], Avg: [-440.035 -440.035 -440.035] (1.0000) ({r_i: None, r_t: [-887.794 -887.794 -887.794], eps: 1.0})
Step:   34200, Reward: [-470.278 -470.278 -470.278] [57.995], Avg: [-440.123 -440.123 -440.123] (1.0000) ({r_i: None, r_t: [-904.757 -904.757 -904.757], eps: 1.0})
Step:   34300, Reward: [-443.581 -443.581 -443.581] [55.801], Avg: [-440.133 -440.133 -440.133] (1.0000) ({r_i: None, r_t: [-868.924 -868.924 -868.924], eps: 1.0})
Step:   34400, Reward: [-440.651 -440.651 -440.651] [58.051], Avg: [-440.134 -440.134 -440.134] (1.0000) ({r_i: None, r_t: [-912.854 -912.854 -912.854], eps: 1.0})
Step:   34500, Reward: [-454.624 -454.624 -454.624] [66.013], Avg: [-440.176 -440.176 -440.176] (1.0000) ({r_i: None, r_t: [-901.389 -901.389 -901.389], eps: 1.0})
Step:   34600, Reward: [-446.528 -446.528 -446.528] [68.689], Avg: [-440.195 -440.195 -440.195] (1.0000) ({r_i: None, r_t: [-940.015 -940.015 -940.015], eps: 1.0})
Step:   34700, Reward: [-431.056 -431.056 -431.056] [51.451], Avg: [-440.168 -440.168 -440.168] (1.0000) ({r_i: None, r_t: [-900.856 -900.856 -900.856], eps: 1.0})
Step:   34800, Reward: [-464.775 -464.775 -464.775] [76.617], Avg: [-440.239 -440.239 -440.239] (1.0000) ({r_i: None, r_t: [-907.054 -907.054 -907.054], eps: 1.0})
Step:   34900, Reward: [-445.088 -445.088 -445.088] [51.534], Avg: [-440.253 -440.253 -440.253] (1.0000) ({r_i: None, r_t: [-899.961 -899.961 -899.961], eps: 1.0})
Step:   35000, Reward: [-407.374 -407.374 -407.374] [48.748], Avg: [-440.159 -440.159 -440.159] (1.0000) ({r_i: None, r_t: [-891.664 -891.664 -891.664], eps: 1.0})
Step:   35100, Reward: [-441.497 -441.497 -441.497] [64.132], Avg: [-440.163 -440.163 -440.163] (1.0000) ({r_i: None, r_t: [-900.105 -900.105 -900.105], eps: 1.0})
Step:   35200, Reward: [-478.457 -478.457 -478.457] [57.645], Avg: [-440.271 -440.271 -440.271] (1.0000) ({r_i: None, r_t: [-890.326 -890.326 -890.326], eps: 1.0})
Step:   35300, Reward: [-449.483 -449.483 -449.483] [70.742], Avg: [-440.297 -440.297 -440.297] (1.0000) ({r_i: None, r_t: [-916.686 -916.686 -916.686], eps: 1.0})
Step:   35400, Reward: [-449.803 -449.803 -449.803] [67.743], Avg: [-440.324 -440.324 -440.324] (1.0000) ({r_i: None, r_t: [-901.635 -901.635 -901.635], eps: 1.0})
Step:   35500, Reward: [-450.685 -450.685 -450.685] [45.541], Avg: [-440.353 -440.353 -440.353] (1.0000) ({r_i: None, r_t: [-921.031 -921.031 -921.031], eps: 1.0})
Step:   35600, Reward: [-478.011 -478.011 -478.011] [67.931], Avg: [-440.459 -440.459 -440.459] (1.0000) ({r_i: None, r_t: [-868.076 -868.076 -868.076], eps: 1.0})
Step:   35700, Reward: [-431.746 -431.746 -431.746] [54.869], Avg: [-440.434 -440.434 -440.434] (1.0000) ({r_i: None, r_t: [-935.117 -935.117 -935.117], eps: 1.0})
Step:   35800, Reward: [-471.795 -471.795 -471.795] [52.473], Avg: [-440.522 -440.522 -440.522] (1.0000) ({r_i: None, r_t: [-893.761 -893.761 -893.761], eps: 1.0})
Step:   35900, Reward: [-431.474 -431.474 -431.474] [53.115], Avg: [-440.497 -440.497 -440.497] (1.0000) ({r_i: None, r_t: [-932.498 -932.498 -932.498], eps: 1.0})
Step:   36000, Reward: [-453.134 -453.134 -453.134] [71.649], Avg: [-440.532 -440.532 -440.532] (1.0000) ({r_i: None, r_t: [-955.530 -955.530 -955.530], eps: 1.0})
Step:   36100, Reward: [-479.211 -479.211 -479.211] [69.268], Avg: [-440.638 -440.638 -440.638] (1.0000) ({r_i: None, r_t: [-910.453 -910.453 -910.453], eps: 1.0})
Step:   36200, Reward: [-455.339 -455.339 -455.339] [82.705], Avg: [-440.679 -440.679 -440.679] (1.0000) ({r_i: None, r_t: [-911.099 -911.099 -911.099], eps: 1.0})
Step:   36300, Reward: [-452.805 -452.805 -452.805] [51.553], Avg: [-440.712 -440.712 -440.712] (1.0000) ({r_i: None, r_t: [-911.458 -911.458 -911.458], eps: 1.0})
Step:   36400, Reward: [-449.232 -449.232 -449.232] [57.220], Avg: [-440.736 -440.736 -440.736] (1.0000) ({r_i: None, r_t: [-885.786 -885.786 -885.786], eps: 1.0})
Step:   36500, Reward: [-462.536 -462.536 -462.536] [86.081], Avg: [-440.795 -440.795 -440.795] (1.0000) ({r_i: None, r_t: [-941.873 -941.873 -941.873], eps: 1.0})
Step:   36600, Reward: [-460.366 -460.366 -460.366] [52.712], Avg: [-440.848 -440.848 -440.848] (1.0000) ({r_i: None, r_t: [-909.167 -909.167 -909.167], eps: 1.0})
Step:   36700, Reward: [-444.559 -444.559 -444.559] [83.146], Avg: [-440.858 -440.858 -440.858] (1.0000) ({r_i: None, r_t: [-905.424 -905.424 -905.424], eps: 1.0})
Step:   36800, Reward: [-439.994 -439.994 -439.994] [67.588], Avg: [-440.856 -440.856 -440.856] (1.0000) ({r_i: None, r_t: [-928.972 -928.972 -928.972], eps: 1.0})
Step:   36900, Reward: [-469.148 -469.148 -469.148] [73.802], Avg: [-440.933 -440.933 -440.933] (1.0000) ({r_i: None, r_t: [-916.168 -916.168 -916.168], eps: 1.0})
Step:   37000, Reward: [-438.995 -438.995 -438.995] [74.805], Avg: [-440.927 -440.927 -440.927] (1.0000) ({r_i: None, r_t: [-889.171 -889.171 -889.171], eps: 1.0})
Step:   37100, Reward: [-455.068 -455.068 -455.068] [74.839], Avg: [-440.965 -440.965 -440.965] (1.0000) ({r_i: None, r_t: [-919.218 -919.218 -919.218], eps: 1.0})
Step:   37200, Reward: [-445.270 -445.270 -445.270] [54.363], Avg: [-440.977 -440.977 -440.977] (1.0000) ({r_i: None, r_t: [-913.692 -913.692 -913.692], eps: 1.0})
Step:   37300, Reward: [-480.777 -480.777 -480.777] [63.367], Avg: [-441.083 -441.083 -441.083] (1.0000) ({r_i: None, r_t: [-849.261 -849.261 -849.261], eps: 1.0})
Step:   37400, Reward: [-476.111 -476.111 -476.111] [65.434], Avg: [-441.177 -441.177 -441.177] (1.0000) ({r_i: None, r_t: [-922.484 -922.484 -922.484], eps: 1.0})
Step:   37500, Reward: [-406.689 -406.689 -406.689] [49.073], Avg: [-441.085 -441.085 -441.085] (1.0000) ({r_i: None, r_t: [-907.346 -907.346 -907.346], eps: 1.0})
Step:   37600, Reward: [-442.627 -442.627 -442.627] [58.539], Avg: [-441.089 -441.089 -441.089] (1.0000) ({r_i: None, r_t: [-876.072 -876.072 -876.072], eps: 1.0})
Step:   37700, Reward: [-486.223 -486.223 -486.223] [62.213], Avg: [-441.209 -441.209 -441.209] (1.0000) ({r_i: None, r_t: [-874.309 -874.309 -874.309], eps: 1.0})
Step:   37800, Reward: [-475.216 -475.216 -475.216] [75.981], Avg: [-441.298 -441.298 -441.298] (1.0000) ({r_i: None, r_t: [-875.457 -875.457 -875.457], eps: 1.0})
Step:   37900, Reward: [-468.153 -468.153 -468.153] [55.126], Avg: [-441.369 -441.369 -441.369] (1.0000) ({r_i: None, r_t: [-865.286 -865.286 -865.286], eps: 1.0})
Step:   38000, Reward: [-427.303 -427.303 -427.303] [66.301], Avg: [-441.332 -441.332 -441.332] (1.0000) ({r_i: None, r_t: [-940.473 -940.473 -940.473], eps: 1.0})
Step:   38100, Reward: [-487.524 -487.524 -487.524] [72.255], Avg: [-441.453 -441.453 -441.453] (1.0000) ({r_i: None, r_t: [-915.065 -915.065 -915.065], eps: 1.0})
Step:   38200, Reward: [-485.511 -485.511 -485.511] [69.348], Avg: [-441.568 -441.568 -441.568] (1.0000) ({r_i: None, r_t: [-928.311 -928.311 -928.311], eps: 1.0})
Step:   38300, Reward: [-444.167 -444.167 -444.167] [68.835], Avg: [-441.575 -441.575 -441.575] (1.0000) ({r_i: None, r_t: [-934.202 -934.202 -934.202], eps: 1.0})
Step:   38400, Reward: [-437.248 -437.248 -437.248] [62.437], Avg: [-441.564 -441.564 -441.564] (1.0000) ({r_i: None, r_t: [-914.832 -914.832 -914.832], eps: 1.0})
Step:   38500, Reward: [-443.792 -443.792 -443.792] [81.024], Avg: [-441.569 -441.569 -441.569] (1.0000) ({r_i: None, r_t: [-929.891 -929.891 -929.891], eps: 1.0})
Step:   38600, Reward: [-442.541 -442.541 -442.541] [61.986], Avg: [-441.572 -441.572 -441.572] (1.0000) ({r_i: None, r_t: [-931.918 -931.918 -931.918], eps: 1.0})
Step:   38700, Reward: [-444.127 -444.127 -444.127] [69.046], Avg: [-441.578 -441.578 -441.578] (1.0000) ({r_i: None, r_t: [-944.646 -944.646 -944.646], eps: 1.0})
Step:   38800, Reward: [-475.334 -475.334 -475.334] [58.260], Avg: [-441.665 -441.665 -441.665] (1.0000) ({r_i: None, r_t: [-898.871 -898.871 -898.871], eps: 1.0})
Step:   38900, Reward: [-462.567 -462.567 -462.567] [71.960], Avg: [-441.719 -441.719 -441.719] (1.0000) ({r_i: None, r_t: [-929.315 -929.315 -929.315], eps: 1.0})
Step:   39000, Reward: [-449.962 -449.962 -449.962] [80.413], Avg: [-441.740 -441.740 -441.740] (1.0000) ({r_i: None, r_t: [-920.404 -920.404 -920.404], eps: 1.0})
Step:   39100, Reward: [-480.249 -480.249 -480.249] [82.549], Avg: [-441.838 -441.838 -441.838] (1.0000) ({r_i: None, r_t: [-888.177 -888.177 -888.177], eps: 1.0})
Step:   39200, Reward: [-438.552 -438.552 -438.552] [51.390], Avg: [-441.830 -441.830 -441.830] (1.0000) ({r_i: None, r_t: [-931.683 -931.683 -931.683], eps: 1.0})
Step:   39300, Reward: [-448.698 -448.698 -448.698] [71.168], Avg: [-441.847 -441.847 -441.847] (1.0000) ({r_i: None, r_t: [-926.768 -926.768 -926.768], eps: 1.0})
Step:   39400, Reward: [-465.596 -465.596 -465.596] [67.275], Avg: [-441.907 -441.907 -441.907] (1.0000) ({r_i: None, r_t: [-921.830 -921.830 -921.830], eps: 1.0})
Step:   39500, Reward: [-454.954 -454.954 -454.954] [70.485], Avg: [-441.940 -441.940 -441.940] (1.0000) ({r_i: None, r_t: [-942.890 -942.890 -942.890], eps: 1.0})
Step:   39600, Reward: [-452.927 -452.927 -452.927] [53.869], Avg: [-441.968 -441.968 -441.968] (1.0000) ({r_i: None, r_t: [-884.071 -884.071 -884.071], eps: 1.0})
Step:   39700, Reward: [-479.139 -479.139 -479.139] [74.019], Avg: [-442.061 -442.061 -442.061] (1.0000) ({r_i: None, r_t: [-950.593 -950.593 -950.593], eps: 1.0})
Step:   39800, Reward: [-476.428 -476.428 -476.428] [59.408], Avg: [-442.147 -442.147 -442.147] (1.0000) ({r_i: None, r_t: [-928.722 -928.722 -928.722], eps: 1.0})
Step:   39900, Reward: [-471.736 -471.736 -471.736] [59.109], Avg: [-442.221 -442.221 -442.221] (1.0000) ({r_i: None, r_t: [-894.818 -894.818 -894.818], eps: 1.0})
Step:   40000, Reward: [-448.652 -448.652 -448.652] [57.597], Avg: [-442.237 -442.237 -442.237] (1.0000) ({r_i: None, r_t: [-892.113 -892.113 -892.113], eps: 1.0})
Step:   40100, Reward: [-493.895 -493.895 -493.895] [62.612], Avg: [-442.366 -442.366 -442.366] (1.0000) ({r_i: None, r_t: [-899.850 -899.850 -899.850], eps: 1.0})
Step:   40200, Reward: [-452.280 -452.280 -452.280] [72.434], Avg: [-442.391 -442.391 -442.391] (1.0000) ({r_i: None, r_t: [-862.695 -862.695 -862.695], eps: 1.0})
Step:   40300, Reward: [-464.283 -464.283 -464.283] [78.924], Avg: [-442.445 -442.445 -442.445] (1.0000) ({r_i: None, r_t: [-901.575 -901.575 -901.575], eps: 1.0})
Step:   40400, Reward: [-469.924 -469.924 -469.924] [62.116], Avg: [-442.513 -442.513 -442.513] (1.0000) ({r_i: None, r_t: [-954.107 -954.107 -954.107], eps: 1.0})
Step:   40500, Reward: [-428.853 -428.853 -428.853] [52.237], Avg: [-442.479 -442.479 -442.479] (1.0000) ({r_i: None, r_t: [-970.255 -970.255 -970.255], eps: 1.0})
Step:   40600, Reward: [-486.172 -486.172 -486.172] [60.017], Avg: [-442.586 -442.586 -442.586] (1.0000) ({r_i: None, r_t: [-927.752 -927.752 -927.752], eps: 1.0})
Step:   40700, Reward: [-449.410 -449.410 -449.410] [57.706], Avg: [-442.603 -442.603 -442.603] (1.0000) ({r_i: None, r_t: [-933.041 -933.041 -933.041], eps: 1.0})
Step:   40800, Reward: [-453.299 -453.299 -453.299] [62.706], Avg: [-442.629 -442.629 -442.629] (1.0000) ({r_i: None, r_t: [-925.708 -925.708 -925.708], eps: 1.0})
Step:   40900, Reward: [-472.390 -472.390 -472.390] [60.355], Avg: [-442.702 -442.702 -442.702] (1.0000) ({r_i: None, r_t: [-933.963 -933.963 -933.963], eps: 1.0})
Step:   41000, Reward: [-451.963 -451.963 -451.963] [69.190], Avg: [-442.724 -442.724 -442.724] (1.0000) ({r_i: None, r_t: [-943.937 -943.937 -943.937], eps: 1.0})
Step:   41100, Reward: [-460.812 -460.812 -460.812] [66.883], Avg: [-442.768 -442.768 -442.768] (1.0000) ({r_i: None, r_t: [-943.850 -943.850 -943.850], eps: 1.0})
Step:   41200, Reward: [-436.901 -436.901 -436.901] [43.380], Avg: [-442.754 -442.754 -442.754] (1.0000) ({r_i: None, r_t: [-921.799 -921.799 -921.799], eps: 1.0})
Step:   41300, Reward: [-486.537 -486.537 -486.537] [48.478], Avg: [-442.860 -442.860 -442.860] (1.0000) ({r_i: None, r_t: [-909.542 -909.542 -909.542], eps: 1.0})
Step:   41400, Reward: [-476.261 -476.261 -476.261] [72.075], Avg: [-442.940 -442.940 -442.940] (1.0000) ({r_i: None, r_t: [-954.616 -954.616 -954.616], eps: 1.0})
Step:   41500, Reward: [-453.506 -453.506 -453.506] [80.187], Avg: [-442.966 -442.966 -442.966] (1.0000) ({r_i: None, r_t: [-923.648 -923.648 -923.648], eps: 1.0})
Step:   41600, Reward: [-449.124 -449.124 -449.124] [95.215], Avg: [-442.980 -442.980 -442.980] (1.0000) ({r_i: None, r_t: [-904.765 -904.765 -904.765], eps: 1.0})
Step:   41700, Reward: [-461.465 -461.465 -461.465] [74.363], Avg: [-443.025 -443.025 -443.025] (1.0000) ({r_i: None, r_t: [-933.230 -933.230 -933.230], eps: 1.0})
Step:   41800, Reward: [-444.651 -444.651 -444.651] [55.103], Avg: [-443.028 -443.028 -443.028] (1.0000) ({r_i: None, r_t: [-897.276 -897.276 -897.276], eps: 1.0})
Step:   41900, Reward: [-490.786 -490.786 -490.786] [59.589], Avg: [-443.142 -443.142 -443.142] (1.0000) ({r_i: None, r_t: [-910.890 -910.890 -910.890], eps: 1.0})
Step:   42000, Reward: [-483.708 -483.708 -483.708] [68.113], Avg: [-443.239 -443.239 -443.239] (1.0000) ({r_i: None, r_t: [-901.347 -901.347 -901.347], eps: 1.0})
Step:   42100, Reward: [-431.871 -431.871 -431.871] [59.761], Avg: [-443.212 -443.212 -443.212] (1.0000) ({r_i: None, r_t: [-879.841 -879.841 -879.841], eps: 1.0})
Step:   42200, Reward: [-454.871 -454.871 -454.871] [58.582], Avg: [-443.239 -443.239 -443.239] (1.0000) ({r_i: None, r_t: [-872.178 -872.178 -872.178], eps: 1.0})
Step:   42300, Reward: [-479.577 -479.577 -479.577] [63.502], Avg: [-443.325 -443.325 -443.325] (1.0000) ({r_i: None, r_t: [-878.853 -878.853 -878.853], eps: 1.0})
Step:   42400, Reward: [-458.435 -458.435 -458.435] [69.356], Avg: [-443.360 -443.360 -443.360] (1.0000) ({r_i: None, r_t: [-887.323 -887.323 -887.323], eps: 1.0})
Step:   42500, Reward: [-491.039 -491.039 -491.039] [83.385], Avg: [-443.472 -443.472 -443.472] (1.0000) ({r_i: None, r_t: [-862.316 -862.316 -862.316], eps: 1.0})
Step:   42600, Reward: [-459.968 -459.968 -459.968] [72.800], Avg: [-443.511 -443.511 -443.511] (1.0000) ({r_i: None, r_t: [-925.996 -925.996 -925.996], eps: 1.0})
Step:   42700, Reward: [-456.127 -456.127 -456.127] [49.159], Avg: [-443.540 -443.540 -443.540] (1.0000) ({r_i: None, r_t: [-880.228 -880.228 -880.228], eps: 1.0})
Step:   42800, Reward: [-466.492 -466.492 -466.492] [63.404], Avg: [-443.594 -443.594 -443.594] (1.0000) ({r_i: None, r_t: [-866.986 -866.986 -866.986], eps: 1.0})
Step:   42900, Reward: [-465.774 -465.774 -465.774] [48.049], Avg: [-443.646 -443.646 -443.646] (1.0000) ({r_i: None, r_t: [-897.521 -897.521 -897.521], eps: 1.0})
Step:   43000, Reward: [-455.359 -455.359 -455.359] [66.529], Avg: [-443.673 -443.673 -443.673] (1.0000) ({r_i: None, r_t: [-892.844 -892.844 -892.844], eps: 1.0})
Step:   43100, Reward: [-490.817 -490.817 -490.817] [92.322], Avg: [-443.782 -443.782 -443.782] (1.0000) ({r_i: None, r_t: [-910.858 -910.858 -910.858], eps: 1.0})
Step:   43200, Reward: [-454.823 -454.823 -454.823] [66.284], Avg: [-443.807 -443.807 -443.807] (1.0000) ({r_i: None, r_t: [-893.137 -893.137 -893.137], eps: 1.0})
Step:   43300, Reward: [-442.758 -442.758 -442.758] [76.453], Avg: [-443.805 -443.805 -443.805] (1.0000) ({r_i: None, r_t: [-923.563 -923.563 -923.563], eps: 1.0})
Step:   43400, Reward: [-490.644 -490.644 -490.644] [67.468], Avg: [-443.913 -443.913 -443.913] (1.0000) ({r_i: None, r_t: [-868.871 -868.871 -868.871], eps: 1.0})
Step:   43500, Reward: [-465.148 -465.148 -465.148] [70.557], Avg: [-443.961 -443.961 -443.961] (1.0000) ({r_i: None, r_t: [-898.213 -898.213 -898.213], eps: 1.0})
Step:   43600, Reward: [-459.248 -459.248 -459.248] [53.352], Avg: [-443.996 -443.996 -443.996] (1.0000) ({r_i: None, r_t: [-846.107 -846.107 -846.107], eps: 1.0})
Step:   43700, Reward: [-481.099 -481.099 -481.099] [56.707], Avg: [-444.081 -444.081 -444.081] (1.0000) ({r_i: None, r_t: [-885.856 -885.856 -885.856], eps: 1.0})
Step:   43800, Reward: [-437.976 -437.976 -437.976] [69.679], Avg: [-444.067 -444.067 -444.067] (1.0000) ({r_i: None, r_t: [-841.113 -841.113 -841.113], eps: 1.0})
Step:   43900, Reward: [-460.374 -460.374 -460.374] [67.675], Avg: [-444.104 -444.104 -444.104] (1.0000) ({r_i: None, r_t: [-905.294 -905.294 -905.294], eps: 1.0})
Step:   44000, Reward: [-481.645 -481.645 -481.645] [67.627], Avg: [-444.189 -444.189 -444.189] (1.0000) ({r_i: None, r_t: [-930.948 -930.948 -930.948], eps: 1.0})
Step:   44100, Reward: [-438.315 -438.315 -438.315] [72.056], Avg: [-444.176 -444.176 -444.176] (1.0000) ({r_i: None, r_t: [-906.967 -906.967 -906.967], eps: 1.0})
Step:   44200, Reward: [-462.747 -462.747 -462.747] [85.012], Avg: [-444.218 -444.218 -444.218] (1.0000) ({r_i: None, r_t: [-883.226 -883.226 -883.226], eps: 1.0})
Step:   44300, Reward: [-452.538 -452.538 -452.538] [73.685], Avg: [-444.237 -444.237 -444.237] (1.0000) ({r_i: None, r_t: [-887.405 -887.405 -887.405], eps: 1.0})
Step:   44400, Reward: [-466.951 -466.951 -466.951] [66.245], Avg: [-444.288 -444.288 -444.288] (1.0000) ({r_i: None, r_t: [-894.313 -894.313 -894.313], eps: 1.0})
Step:   44500, Reward: [-462.473 -462.473 -462.473] [81.142], Avg: [-444.328 -444.328 -444.328] (1.0000) ({r_i: None, r_t: [-856.313 -856.313 -856.313], eps: 1.0})
Step:   44600, Reward: [-449.304 -449.304 -449.304] [57.791], Avg: [-444.340 -444.340 -444.340] (1.0000) ({r_i: None, r_t: [-894.013 -894.013 -894.013], eps: 1.0})
Step:   44700, Reward: [-438.434 -438.434 -438.434] [81.053], Avg: [-444.326 -444.326 -444.326] (1.0000) ({r_i: None, r_t: [-899.788 -899.788 -899.788], eps: 1.0})
Step:   44800, Reward: [-465.183 -465.183 -465.183] [85.484], Avg: [-444.373 -444.373 -444.373] (1.0000) ({r_i: None, r_t: [-920.075 -920.075 -920.075], eps: 1.0})
Step:   44900, Reward: [-436.128 -436.128 -436.128] [53.453], Avg: [-444.355 -444.355 -444.355] (1.0000) ({r_i: None, r_t: [-971.114 -971.114 -971.114], eps: 1.0})
Step:   45000, Reward: [-483.124 -483.124 -483.124] [56.630], Avg: [-444.440 -444.440 -444.440] (1.0000) ({r_i: None, r_t: [-901.513 -901.513 -901.513], eps: 1.0})
Step:   45100, Reward: [-436.344 -436.344 -436.344] [54.059], Avg: [-444.423 -444.423 -444.423] (1.0000) ({r_i: None, r_t: [-912.062 -912.062 -912.062], eps: 1.0})
Step:   45200, Reward: [-446.376 -446.376 -446.376] [68.214], Avg: [-444.427 -444.427 -444.427] (1.0000) ({r_i: None, r_t: [-900.447 -900.447 -900.447], eps: 1.0})
Step:   45300, Reward: [-449.555 -449.555 -449.555] [42.543], Avg: [-444.438 -444.438 -444.438] (1.0000) ({r_i: None, r_t: [-874.388 -874.388 -874.388], eps: 1.0})
Step:   45400, Reward: [-466.928 -466.928 -466.928] [55.929], Avg: [-444.488 -444.488 -444.488] (1.0000) ({r_i: None, r_t: [-887.030 -887.030 -887.030], eps: 1.0})
Step:   45500, Reward: [-457.940 -457.940 -457.940] [70.280], Avg: [-444.517 -444.517 -444.517] (1.0000) ({r_i: None, r_t: [-922.948 -922.948 -922.948], eps: 1.0})
Step:   45600, Reward: [-467.055 -467.055 -467.055] [85.155], Avg: [-444.566 -444.566 -444.566] (1.0000) ({r_i: None, r_t: [-888.626 -888.626 -888.626], eps: 1.0})
Step:   45700, Reward: [-450.845 -450.845 -450.845] [53.902], Avg: [-444.580 -444.580 -444.580] (1.0000) ({r_i: None, r_t: [-893.829 -893.829 -893.829], eps: 1.0})
Step:   45800, Reward: [-429.717 -429.717 -429.717] [64.868], Avg: [-444.548 -444.548 -444.548] (1.0000) ({r_i: None, r_t: [-867.147 -867.147 -867.147], eps: 1.0})
Step:   45900, Reward: [-448.511 -448.511 -448.511] [63.774], Avg: [-444.556 -444.556 -444.556] (1.0000) ({r_i: None, r_t: [-892.344 -892.344 -892.344], eps: 1.0})
Step:   46000, Reward: [-446.404 -446.404 -446.404] [56.004], Avg: [-444.560 -444.560 -444.560] (1.0000) ({r_i: None, r_t: [-915.165 -915.165 -915.165], eps: 1.0})
Step:   46100, Reward: [-456.811 -456.811 -456.811] [49.473], Avg: [-444.587 -444.587 -444.587] (1.0000) ({r_i: None, r_t: [-932.176 -932.176 -932.176], eps: 1.0})
Step:   46200, Reward: [-448.482 -448.482 -448.482] [65.915], Avg: [-444.595 -444.595 -444.595] (1.0000) ({r_i: None, r_t: [-897.744 -897.744 -897.744], eps: 1.0})
Step:   46300, Reward: [-431.690 -431.690 -431.690] [81.679], Avg: [-444.568 -444.568 -444.568] (1.0000) ({r_i: None, r_t: [-877.415 -877.415 -877.415], eps: 1.0})
Step:   46400, Reward: [-451.869 -451.869 -451.869] [73.823], Avg: [-444.583 -444.583 -444.583] (1.0000) ({r_i: None, r_t: [-913.183 -913.183 -913.183], eps: 1.0})
Step:   46500, Reward: [-446.944 -446.944 -446.944] [73.973], Avg: [-444.588 -444.588 -444.588] (1.0000) ({r_i: None, r_t: [-953.443 -953.443 -953.443], eps: 1.0})
Step:   46600, Reward: [-469.472 -469.472 -469.472] [56.060], Avg: [-444.642 -444.642 -444.642] (1.0000) ({r_i: None, r_t: [-902.119 -902.119 -902.119], eps: 1.0})
Step:   46700, Reward: [-503.996 -503.996 -503.996] [72.760], Avg: [-444.768 -444.768 -444.768] (1.0000) ({r_i: None, r_t: [-937.260 -937.260 -937.260], eps: 1.0})
Step:   46800, Reward: [-457.317 -457.317 -457.317] [55.596], Avg: [-444.795 -444.795 -444.795] (1.0000) ({r_i: None, r_t: [-887.005 -887.005 -887.005], eps: 1.0})
Step:   46900, Reward: [-464.891 -464.891 -464.891] [82.147], Avg: [-444.838 -444.838 -444.838] (1.0000) ({r_i: None, r_t: [-909.167 -909.167 -909.167], eps: 1.0})
Step:   47000, Reward: [-471.474 -471.474 -471.474] [51.203], Avg: [-444.894 -444.894 -444.894] (1.0000) ({r_i: None, r_t: [-912.434 -912.434 -912.434], eps: 1.0})
Step:   47100, Reward: [-456.815 -456.815 -456.815] [52.428], Avg: [-444.920 -444.920 -444.920] (1.0000) ({r_i: None, r_t: [-872.156 -872.156 -872.156], eps: 1.0})
Step:   47200, Reward: [-479.248 -479.248 -479.248] [78.260], Avg: [-444.992 -444.992 -444.992] (1.0000) ({r_i: None, r_t: [-863.352 -863.352 -863.352], eps: 1.0})
Step:   47300, Reward: [-448.348 -448.348 -448.348] [63.916], Avg: [-444.999 -444.999 -444.999] (1.0000) ({r_i: None, r_t: [-903.229 -903.229 -903.229], eps: 1.0})
Step:   47400, Reward: [-450.093 -450.093 -450.093] [61.358], Avg: [-445.010 -445.010 -445.010] (1.0000) ({r_i: None, r_t: [-884.566 -884.566 -884.566], eps: 1.0})
Step:   47500, Reward: [-454.779 -454.779 -454.779] [72.995], Avg: [-445.031 -445.031 -445.031] (1.0000) ({r_i: None, r_t: [-878.029 -878.029 -878.029], eps: 1.0})
Step:   47600, Reward: [-419.491 -419.491 -419.491] [53.492], Avg: [-444.977 -444.977 -444.977] (1.0000) ({r_i: None, r_t: [-893.899 -893.899 -893.899], eps: 1.0})
Step:   47700, Reward: [-477.946 -477.946 -477.946] [88.580], Avg: [-445.046 -445.046 -445.046] (1.0000) ({r_i: None, r_t: [-901.268 -901.268 -901.268], eps: 1.0})
Step:   47800, Reward: [-467.603 -467.603 -467.603] [60.772], Avg: [-445.093 -445.093 -445.093] (1.0000) ({r_i: None, r_t: [-897.294 -897.294 -897.294], eps: 1.0})
Step:   47900, Reward: [-462.310 -462.310 -462.310] [48.666], Avg: [-445.129 -445.129 -445.129] (1.0000) ({r_i: None, r_t: [-907.955 -907.955 -907.955], eps: 1.0})
Step:   48000, Reward: [-432.021 -432.021 -432.021] [66.315], Avg: [-445.102 -445.102 -445.102] (1.0000) ({r_i: None, r_t: [-958.216 -958.216 -958.216], eps: 1.0})
Step:   48100, Reward: [-445.023 -445.023 -445.023] [58.818], Avg: [-445.102 -445.102 -445.102] (1.0000) ({r_i: None, r_t: [-904.522 -904.522 -904.522], eps: 1.0})
Step:   48200, Reward: [-435.628 -435.628 -435.628] [59.684], Avg: [-445.082 -445.082 -445.082] (1.0000) ({r_i: None, r_t: [-905.137 -905.137 -905.137], eps: 1.0})
Step:   48300, Reward: [-482.773 -482.773 -482.773] [78.836], Avg: [-445.160 -445.160 -445.160] (1.0000) ({r_i: None, r_t: [-883.897 -883.897 -883.897], eps: 1.0})
Step:   48400, Reward: [-447.732 -447.732 -447.732] [68.321], Avg: [-445.165 -445.165 -445.165] (1.0000) ({r_i: None, r_t: [-926.699 -926.699 -926.699], eps: 1.0})
Step:   48500, Reward: [-462.189 -462.189 -462.189] [78.190], Avg: [-445.200 -445.200 -445.200] (1.0000) ({r_i: None, r_t: [-947.188 -947.188 -947.188], eps: 1.0})
Step:   48600, Reward: [-464.865 -464.865 -464.865] [51.377], Avg: [-445.241 -445.241 -445.241] (1.0000) ({r_i: None, r_t: [-935.361 -935.361 -935.361], eps: 1.0})
Step:   48700, Reward: [-466.454 -466.454 -466.454] [66.042], Avg: [-445.284 -445.284 -445.284] (1.0000) ({r_i: None, r_t: [-919.014 -919.014 -919.014], eps: 1.0})
Step:   48800, Reward: [-444.009 -444.009 -444.009] [74.946], Avg: [-445.281 -445.281 -445.281] (1.0000) ({r_i: None, r_t: [-916.347 -916.347 -916.347], eps: 1.0})
Step:   48900, Reward: [-469.599 -469.599 -469.599] [58.656], Avg: [-445.331 -445.331 -445.331] (1.0000) ({r_i: None, r_t: [-896.499 -896.499 -896.499], eps: 1.0})
Step:   49000, Reward: [-455.881 -455.881 -455.881] [76.838], Avg: [-445.353 -445.353 -445.353] (1.0000) ({r_i: None, r_t: [-933.381 -933.381 -933.381], eps: 1.0})
Step:   49100, Reward: [-447.372 -447.372 -447.372] [68.773], Avg: [-445.357 -445.357 -445.357] (1.0000) ({r_i: None, r_t: [-924.201 -924.201 -924.201], eps: 1.0})
Step:   49200, Reward: [-465.928 -465.928 -465.928] [63.585], Avg: [-445.398 -445.398 -445.398] (1.0000) ({r_i: None, r_t: [-945.958 -945.958 -945.958], eps: 1.0})
Step:   49300, Reward: [-438.814 -438.814 -438.814] [56.994], Avg: [-445.385 -445.385 -445.385] (1.0000) ({r_i: None, r_t: [-962.192 -962.192 -962.192], eps: 1.0})
Step:   49400, Reward: [-471.873 -471.873 -471.873] [64.488], Avg: [-445.439 -445.439 -445.439] (1.0000) ({r_i: None, r_t: [-979.272 -979.272 -979.272], eps: 1.0})
Step:   49500, Reward: [-454.291 -454.291 -454.291] [66.405], Avg: [-445.456 -445.456 -445.456] (1.0000) ({r_i: None, r_t: [-886.798 -886.798 -886.798], eps: 1.0})
Step:   49600, Reward: [-458.106 -458.106 -458.106] [61.933], Avg: [-445.482 -445.482 -445.482] (1.0000) ({r_i: None, r_t: [-915.645 -915.645 -915.645], eps: 1.0})
Step:   49700, Reward: [-451.422 -451.422 -451.422] [60.988], Avg: [-445.494 -445.494 -445.494] (1.0000) ({r_i: None, r_t: [-941.668 -941.668 -941.668], eps: 1.0})
Step:   49800, Reward: [-444.548 -444.548 -444.548] [61.450], Avg: [-445.492 -445.492 -445.492] (1.0000) ({r_i: None, r_t: [-941.487 -941.487 -941.487], eps: 1.0})
Step:   49900, Reward: [-473.792 -473.792 -473.792] [66.904], Avg: [-445.548 -445.548 -445.548] (1.0000) ({r_i: None, r_t: [-909.048 -909.048 -909.048], eps: 1.0})
Step:   50000, Reward: [-455.383 -455.383 -455.383] [73.629], Avg: [-445.568 -445.568 -445.568] (1.0000) ({r_i: None, r_t: [-965.218 -965.218 -965.218], eps: 1.0})
Step:   50100, Reward: [-464.287 -464.287 -464.287] [69.056], Avg: [-445.605 -445.605 -445.605] (1.0000) ({r_i: None, r_t: [-906.204 -906.204 -906.204], eps: 1.0})
Step:   50200, Reward: [-450.757 -450.757 -450.757] [72.287], Avg: [-445.616 -445.616 -445.616] (1.0000) ({r_i: None, r_t: [-949.716 -949.716 -949.716], eps: 1.0})
Step:   50300, Reward: [-469.467 -469.467 -469.467] [75.782], Avg: [-445.663 -445.663 -445.663] (1.0000) ({r_i: None, r_t: [-895.520 -895.520 -895.520], eps: 1.0})
Step:   50400, Reward: [-467.172 -467.172 -467.172] [56.941], Avg: [-445.706 -445.706 -445.706] (1.0000) ({r_i: None, r_t: [-901.599 -901.599 -901.599], eps: 1.0})
Step:   50500, Reward: [-465.545 -465.545 -465.545] [77.991], Avg: [-445.745 -445.745 -445.745] (1.0000) ({r_i: None, r_t: [-896.956 -896.956 -896.956], eps: 1.0})
Step:   50600, Reward: [-474.449 -474.449 -474.449] [56.344], Avg: [-445.801 -445.801 -445.801] (1.0000) ({r_i: None, r_t: [-907.260 -907.260 -907.260], eps: 1.0})
Step:   50700, Reward: [-479.505 -479.505 -479.505] [91.351], Avg: [-445.868 -445.868 -445.868] (1.0000) ({r_i: None, r_t: [-908.000 -908.000 -908.000], eps: 1.0})
Step:   50800, Reward: [-429.166 -429.166 -429.166] [66.518], Avg: [-445.835 -445.835 -445.835] (1.0000) ({r_i: None, r_t: [-909.298 -909.298 -909.298], eps: 1.0})
Step:   50900, Reward: [-476.329 -476.329 -476.329] [56.688], Avg: [-445.895 -445.895 -445.895] (1.0000) ({r_i: None, r_t: [-892.675 -892.675 -892.675], eps: 1.0})
Step:   51000, Reward: [-447.873 -447.873 -447.873] [62.173], Avg: [-445.899 -445.899 -445.899] (1.0000) ({r_i: None, r_t: [-949.421 -949.421 -949.421], eps: 1.0})
Step:   51100, Reward: [-445.683 -445.683 -445.683] [79.585], Avg: [-445.898 -445.898 -445.898] (1.0000) ({r_i: None, r_t: [-904.744 -904.744 -904.744], eps: 1.0})
Step:   51200, Reward: [-438.118 -438.118 -438.118] [39.039], Avg: [-445.883 -445.883 -445.883] (1.0000) ({r_i: None, r_t: [-911.844 -911.844 -911.844], eps: 1.0})
Step:   51300, Reward: [-430.807 -430.807 -430.807] [65.447], Avg: [-445.854 -445.854 -445.854] (1.0000) ({r_i: None, r_t: [-880.946 -880.946 -880.946], eps: 1.0})
Step:   51400, Reward: [-445.287 -445.287 -445.287] [69.694], Avg: [-445.853 -445.853 -445.853] (1.0000) ({r_i: None, r_t: [-924.453 -924.453 -924.453], eps: 1.0})
Step:   51500, Reward: [-473.031 -473.031 -473.031] [88.134], Avg: [-445.905 -445.905 -445.905] (1.0000) ({r_i: None, r_t: [-903.293 -903.293 -903.293], eps: 1.0})
Step:   51600, Reward: [-461.257 -461.257 -461.257] [79.982], Avg: [-445.935 -445.935 -445.935] (1.0000) ({r_i: None, r_t: [-861.849 -861.849 -861.849], eps: 1.0})
Step:   51700, Reward: [-479.235 -479.235 -479.235] [62.301], Avg: [-445.999 -445.999 -445.999] (1.0000) ({r_i: None, r_t: [-863.855 -863.855 -863.855], eps: 1.0})
Step:   51800, Reward: [-456.069 -456.069 -456.069] [68.237], Avg: [-446.019 -446.019 -446.019] (1.0000) ({r_i: None, r_t: [-894.366 -894.366 -894.366], eps: 1.0})
Step:   51900, Reward: [-460.048 -460.048 -460.048] [48.121], Avg: [-446.046 -446.046 -446.046] (1.0000) ({r_i: None, r_t: [-892.656 -892.656 -892.656], eps: 1.0})
Step:   52000, Reward: [-448.958 -448.958 -448.958] [40.759], Avg: [-446.051 -446.051 -446.051] (1.0000) ({r_i: None, r_t: [-923.703 -923.703 -923.703], eps: 1.0})
Step:   52100, Reward: [-463.705 -463.705 -463.705] [76.225], Avg: [-446.085 -446.085 -446.085] (1.0000) ({r_i: None, r_t: [-944.715 -944.715 -944.715], eps: 1.0})
Step:   52200, Reward: [-448.151 -448.151 -448.151] [55.114], Avg: [-446.089 -446.089 -446.089] (1.0000) ({r_i: None, r_t: [-902.277 -902.277 -902.277], eps: 1.0})
Step:   52300, Reward: [-517.192 -517.192 -517.192] [56.889], Avg: [-446.225 -446.225 -446.225] (1.0000) ({r_i: None, r_t: [-916.793 -916.793 -916.793], eps: 1.0})
Step:   52400, Reward: [-457.230 -457.230 -457.230] [50.507], Avg: [-446.246 -446.246 -446.246] (1.0000) ({r_i: None, r_t: [-934.845 -934.845 -934.845], eps: 1.0})
Step:   52500, Reward: [-457.773 -457.773 -457.773] [96.071], Avg: [-446.268 -446.268 -446.268] (1.0000) ({r_i: None, r_t: [-957.324 -957.324 -957.324], eps: 1.0})
Step:   52600, Reward: [-445.852 -445.852 -445.852] [62.413], Avg: [-446.267 -446.267 -446.267] (1.0000) ({r_i: None, r_t: [-928.051 -928.051 -928.051], eps: 1.0})
Step:   52700, Reward: [-467.103 -467.103 -467.103] [78.610], Avg: [-446.306 -446.306 -446.306] (1.0000) ({r_i: None, r_t: [-952.626 -952.626 -952.626], eps: 1.0})
Step:   52800, Reward: [-462.213 -462.213 -462.213] [62.539], Avg: [-446.336 -446.336 -446.336] (1.0000) ({r_i: None, r_t: [-937.591 -937.591 -937.591], eps: 1.0})
Step:   52900, Reward: [-464.264 -464.264 -464.264] [67.225], Avg: [-446.370 -446.370 -446.370] (1.0000) ({r_i: None, r_t: [-883.214 -883.214 -883.214], eps: 1.0})
Step:   53000, Reward: [-459.059 -459.059 -459.059] [79.883], Avg: [-446.394 -446.394 -446.394] (1.0000) ({r_i: None, r_t: [-907.395 -907.395 -907.395], eps: 1.0})
Step:   53100, Reward: [-486.484 -486.484 -486.484] [77.121], Avg: [-446.469 -446.469 -446.469] (1.0000) ({r_i: None, r_t: [-933.556 -933.556 -933.556], eps: 1.0})
Step:   53200, Reward: [-468.659 -468.659 -468.659] [74.112], Avg: [-446.511 -446.511 -446.511] (1.0000) ({r_i: None, r_t: [-917.535 -917.535 -917.535], eps: 1.0})
Step:   53300, Reward: [-478.113 -478.113 -478.113] [84.210], Avg: [-446.570 -446.570 -446.570] (1.0000) ({r_i: None, r_t: [-957.428 -957.428 -957.428], eps: 1.0})
Step:   53400, Reward: [-468.750 -468.750 -468.750] [61.493], Avg: [-446.612 -446.612 -446.612] (1.0000) ({r_i: None, r_t: [-903.703 -903.703 -903.703], eps: 1.0})
Step:   53500, Reward: [-460.490 -460.490 -460.490] [67.139], Avg: [-446.638 -446.638 -446.638] (1.0000) ({r_i: None, r_t: [-915.755 -915.755 -915.755], eps: 1.0})
Step:   53600, Reward: [-432.223 -432.223 -432.223] [66.511], Avg: [-446.611 -446.611 -446.611] (1.0000) ({r_i: None, r_t: [-911.458 -911.458 -911.458], eps: 1.0})
Step:   53700, Reward: [-479.136 -479.136 -479.136] [68.924], Avg: [-446.671 -446.671 -446.671] (1.0000) ({r_i: None, r_t: [-965.621 -965.621 -965.621], eps: 1.0})
Step:   53800, Reward: [-479.383 -479.383 -479.383] [48.135], Avg: [-446.732 -446.732 -446.732] (1.0000) ({r_i: None, r_t: [-956.566 -956.566 -956.566], eps: 1.0})
Step:   53900, Reward: [-428.693 -428.693 -428.693] [80.853], Avg: [-446.698 -446.698 -446.698] (1.0000) ({r_i: None, r_t: [-974.581 -974.581 -974.581], eps: 1.0})
Step:   54000, Reward: [-468.819 -468.819 -468.819] [57.143], Avg: [-446.739 -446.739 -446.739] (1.0000) ({r_i: None, r_t: [-907.470 -907.470 -907.470], eps: 1.0})
Step:   54100, Reward: [-468.934 -468.934 -468.934] [67.061], Avg: [-446.780 -446.780 -446.780] (1.0000) ({r_i: None, r_t: [-888.543 -888.543 -888.543], eps: 1.0})
Step:   54200, Reward: [-465.840 -465.840 -465.840] [98.108], Avg: [-446.815 -446.815 -446.815] (1.0000) ({r_i: None, r_t: [-921.005 -921.005 -921.005], eps: 1.0})
Step:   54300, Reward: [-443.171 -443.171 -443.171] [78.894], Avg: [-446.809 -446.809 -446.809] (1.0000) ({r_i: None, r_t: [-941.244 -941.244 -941.244], eps: 1.0})
Step:   54400, Reward: [-480.275 -480.275 -480.275] [69.799], Avg: [-446.870 -446.870 -446.870] (1.0000) ({r_i: None, r_t: [-937.103 -937.103 -937.103], eps: 1.0})
Step:   54500, Reward: [-445.751 -445.751 -445.751] [67.166], Avg: [-446.868 -446.868 -446.868] (1.0000) ({r_i: None, r_t: [-857.155 -857.155 -857.155], eps: 1.0})
Step:   54600, Reward: [-497.308 -497.308 -497.308] [60.908], Avg: [-446.960 -446.960 -446.960] (1.0000) ({r_i: None, r_t: [-917.270 -917.270 -917.270], eps: 1.0})
Step:   54700, Reward: [-467.941 -467.941 -467.941] [66.525], Avg: [-446.998 -446.998 -446.998] (1.0000) ({r_i: None, r_t: [-883.526 -883.526 -883.526], eps: 1.0})
Step:   54800, Reward: [-478.466 -478.466 -478.466] [79.118], Avg: [-447.056 -447.056 -447.056] (1.0000) ({r_i: None, r_t: [-943.940 -943.940 -943.940], eps: 1.0})
Step:   54900, Reward: [-453.227 -453.227 -453.227] [72.752], Avg: [-447.067 -447.067 -447.067] (1.0000) ({r_i: None, r_t: [-910.869 -910.869 -910.869], eps: 1.0})
Step:   55000, Reward: [-466.092 -466.092 -466.092] [95.881], Avg: [-447.102 -447.102 -447.102] (1.0000) ({r_i: None, r_t: [-875.181 -875.181 -875.181], eps: 1.0})
Step:   55100, Reward: [-466.200 -466.200 -466.200] [73.679], Avg: [-447.136 -447.136 -447.136] (1.0000) ({r_i: None, r_t: [-872.240 -872.240 -872.240], eps: 1.0})
Step:   55200, Reward: [-451.838 -451.838 -451.838] [71.019], Avg: [-447.145 -447.145 -447.145] (1.0000) ({r_i: None, r_t: [-919.932 -919.932 -919.932], eps: 1.0})
Step:   55300, Reward: [-469.145 -469.145 -469.145] [70.359], Avg: [-447.184 -447.184 -447.184] (1.0000) ({r_i: None, r_t: [-886.943 -886.943 -886.943], eps: 1.0})
Step:   55400, Reward: [-486.788 -486.788 -486.788] [56.822], Avg: [-447.256 -447.256 -447.256] (1.0000) ({r_i: None, r_t: [-873.526 -873.526 -873.526], eps: 1.0})
Step:   55500, Reward: [-443.412 -443.412 -443.412] [50.026], Avg: [-447.249 -447.249 -447.249] (1.0000) ({r_i: None, r_t: [-981.514 -981.514 -981.514], eps: 1.0})
Step:   55600, Reward: [-427.763 -427.763 -427.763] [73.539], Avg: [-447.214 -447.214 -447.214] (1.0000) ({r_i: None, r_t: [-940.556 -940.556 -940.556], eps: 1.0})
Step:   55700, Reward: [-452.239 -452.239 -452.239] [53.029], Avg: [-447.223 -447.223 -447.223] (1.0000) ({r_i: None, r_t: [-930.807 -930.807 -930.807], eps: 1.0})
Step:   55800, Reward: [-451.262 -451.262 -451.262] [68.107], Avg: [-447.230 -447.230 -447.230] (1.0000) ({r_i: None, r_t: [-928.496 -928.496 -928.496], eps: 1.0})
Step:   55900, Reward: [-501.553 -501.553 -501.553] [72.373], Avg: [-447.327 -447.327 -447.327] (1.0000) ({r_i: None, r_t: [-931.003 -931.003 -931.003], eps: 1.0})
Step:   56000, Reward: [-489.246 -489.246 -489.246] [65.500], Avg: [-447.402 -447.402 -447.402] (1.0000) ({r_i: None, r_t: [-893.020 -893.020 -893.020], eps: 1.0})
Step:   56100, Reward: [-472.118 -472.118 -472.118] [76.385], Avg: [-447.446 -447.446 -447.446] (1.0000) ({r_i: None, r_t: [-908.107 -908.107 -908.107], eps: 1.0})
Step:   56200, Reward: [-475.967 -475.967 -475.967] [65.653], Avg: [-447.496 -447.496 -447.496] (1.0000) ({r_i: None, r_t: [-908.695 -908.695 -908.695], eps: 1.0})
Step:   56300, Reward: [-464.304 -464.304 -464.304] [40.351], Avg: [-447.526 -447.526 -447.526] (1.0000) ({r_i: None, r_t: [-883.054 -883.054 -883.054], eps: 1.0})
Step:   56400, Reward: [-500.953 -500.953 -500.953] [56.330], Avg: [-447.621 -447.621 -447.621] (1.0000) ({r_i: None, r_t: [-885.040 -885.040 -885.040], eps: 1.0})
Step:   56500, Reward: [-457.772 -457.772 -457.772] [66.466], Avg: [-447.639 -447.639 -447.639] (1.0000) ({r_i: None, r_t: [-908.711 -908.711 -908.711], eps: 1.0})
Step:   56600, Reward: [-447.869 -447.869 -447.869] [57.408], Avg: [-447.639 -447.639 -447.639] (1.0000) ({r_i: None, r_t: [-885.307 -885.307 -885.307], eps: 1.0})
Step:   56700, Reward: [-416.804 -416.804 -416.804] [65.576], Avg: [-447.585 -447.585 -447.585] (1.0000) ({r_i: None, r_t: [-924.244 -924.244 -924.244], eps: 1.0})
Step:   56800, Reward: [-448.301 -448.301 -448.301] [74.958], Avg: [-447.586 -447.586 -447.586] (1.0000) ({r_i: None, r_t: [-939.812 -939.812 -939.812], eps: 1.0})
Step:   56900, Reward: [-447.299 -447.299 -447.299] [54.085], Avg: [-447.586 -447.586 -447.586] (1.0000) ({r_i: None, r_t: [-915.857 -915.857 -915.857], eps: 1.0})
Step:   57000, Reward: [-442.668 -442.668 -442.668] [67.955], Avg: [-447.577 -447.577 -447.577] (1.0000) ({r_i: None, r_t: [-902.822 -902.822 -902.822], eps: 1.0})
Step:   57100, Reward: [-450.250 -450.250 -450.250] [48.344], Avg: [-447.582 -447.582 -447.582] (1.0000) ({r_i: None, r_t: [-950.875 -950.875 -950.875], eps: 1.0})
Step:   57200, Reward: [-449.315 -449.315 -449.315] [87.879], Avg: [-447.585 -447.585 -447.585] (1.0000) ({r_i: None, r_t: [-887.881 -887.881 -887.881], eps: 1.0})
Step:   57300, Reward: [-467.351 -467.351 -467.351] [57.227], Avg: [-447.619 -447.619 -447.619] (1.0000) ({r_i: None, r_t: [-914.233 -914.233 -914.233], eps: 1.0})
Step:   57400, Reward: [-456.958 -456.958 -456.958] [67.336], Avg: [-447.635 -447.635 -447.635] (1.0000) ({r_i: None, r_t: [-904.665 -904.665 -904.665], eps: 1.0})
Step:   57500, Reward: [-453.279 -453.279 -453.279] [45.798], Avg: [-447.645 -447.645 -447.645] (1.0000) ({r_i: None, r_t: [-904.761 -904.761 -904.761], eps: 1.0})
Step:   57600, Reward: [-471.960 -471.960 -471.960] [78.703], Avg: [-447.687 -447.687 -447.687] (1.0000) ({r_i: None, r_t: [-867.337 -867.337 -867.337], eps: 1.0})
Step:   57700, Reward: [-438.130 -438.130 -438.130] [59.863], Avg: [-447.671 -447.671 -447.671] (1.0000) ({r_i: None, r_t: [-884.009 -884.009 -884.009], eps: 1.0})
Step:   57800, Reward: [-454.729 -454.729 -454.729] [67.277], Avg: [-447.683 -447.683 -447.683] (1.0000) ({r_i: None, r_t: [-925.801 -925.801 -925.801], eps: 1.0})
Step:   57900, Reward: [-470.816 -470.816 -470.816] [75.007], Avg: [-447.723 -447.723 -447.723] (1.0000) ({r_i: None, r_t: [-925.178 -925.178 -925.178], eps: 1.0})
Step:   58000, Reward: [-445.058 -445.058 -445.058] [57.125], Avg: [-447.718 -447.718 -447.718] (1.0000) ({r_i: None, r_t: [-925.210 -925.210 -925.210], eps: 1.0})
Step:   58100, Reward: [-472.746 -472.746 -472.746] [61.945], Avg: [-447.761 -447.761 -447.761] (1.0000) ({r_i: None, r_t: [-901.443 -901.443 -901.443], eps: 1.0})
Step:   58200, Reward: [-440.931 -440.931 -440.931] [65.390], Avg: [-447.750 -447.750 -447.750] (1.0000) ({r_i: None, r_t: [-953.030 -953.030 -953.030], eps: 1.0})
Step:   58300, Reward: [-481.326 -481.326 -481.326] [86.573], Avg: [-447.807 -447.807 -447.807] (1.0000) ({r_i: None, r_t: [-916.751 -916.751 -916.751], eps: 1.0})
Step:   58400, Reward: [-462.225 -462.225 -462.225] [81.199], Avg: [-447.832 -447.832 -447.832] (1.0000) ({r_i: None, r_t: [-966.369 -966.369 -966.369], eps: 1.0})
Step:   58500, Reward: [-447.909 -447.909 -447.909] [54.487], Avg: [-447.832 -447.832 -447.832] (1.0000) ({r_i: None, r_t: [-893.353 -893.353 -893.353], eps: 1.0})
Step:   58600, Reward: [-462.996 -462.996 -462.996] [51.113], Avg: [-447.858 -447.858 -447.858] (1.0000) ({r_i: None, r_t: [-943.934 -943.934 -943.934], eps: 1.0})
Step:   58700, Reward: [-486.469 -486.469 -486.469] [83.162], Avg: [-447.923 -447.923 -447.923] (1.0000) ({r_i: None, r_t: [-876.367 -876.367 -876.367], eps: 1.0})
Step:   58800, Reward: [-465.096 -465.096 -465.096] [71.246], Avg: [-447.952 -447.952 -447.952] (1.0000) ({r_i: None, r_t: [-893.980 -893.980 -893.980], eps: 1.0})
Step:   58900, Reward: [-458.012 -458.012 -458.012] [72.081], Avg: [-447.970 -447.970 -447.970] (1.0000) ({r_i: None, r_t: [-907.450 -907.450 -907.450], eps: 1.0})
Step:   59000, Reward: [-476.229 -476.229 -476.229] [73.082], Avg: [-448.017 -448.017 -448.017] (1.0000) ({r_i: None, r_t: [-895.505 -895.505 -895.505], eps: 1.0})
Step:   59100, Reward: [-470.522 -470.522 -470.522] [56.383], Avg: [-448.055 -448.055 -448.055] (1.0000) ({r_i: None, r_t: [-918.630 -918.630 -918.630], eps: 1.0})
Step:   59200, Reward: [-436.061 -436.061 -436.061] [58.515], Avg: [-448.035 -448.035 -448.035] (1.0000) ({r_i: None, r_t: [-920.458 -920.458 -920.458], eps: 1.0})
Step:   59300, Reward: [-469.921 -469.921 -469.921] [82.224], Avg: [-448.072 -448.072 -448.072] (1.0000) ({r_i: None, r_t: [-934.603 -934.603 -934.603], eps: 1.0})
Step:   59400, Reward: [-429.976 -429.976 -429.976] [59.778], Avg: [-448.042 -448.042 -448.042] (1.0000) ({r_i: None, r_t: [-910.294 -910.294 -910.294], eps: 1.0})
Step:   59500, Reward: [-440.992 -440.992 -440.992] [80.619], Avg: [-448.030 -448.030 -448.030] (1.0000) ({r_i: None, r_t: [-854.300 -854.300 -854.300], eps: 1.0})
Step:   59600, Reward: [-440.076 -440.076 -440.076] [89.345], Avg: [-448.016 -448.016 -448.016] (1.0000) ({r_i: None, r_t: [-930.484 -930.484 -930.484], eps: 1.0})
Step:   59700, Reward: [-430.954 -430.954 -430.954] [56.637], Avg: [-447.988 -447.988 -447.988] (1.0000) ({r_i: None, r_t: [-961.959 -961.959 -961.959], eps: 1.0})
Step:   59800, Reward: [-432.167 -432.167 -432.167] [58.781], Avg: [-447.961 -447.961 -447.961] (1.0000) ({r_i: None, r_t: [-897.548 -897.548 -897.548], eps: 1.0})
Step:   59900, Reward: [-456.320 -456.320 -456.320] [69.727], Avg: [-447.975 -447.975 -447.975] (1.0000) ({r_i: None, r_t: [-894.334 -894.334 -894.334], eps: 1.0})
Step:   60000, Reward: [-446.015 -446.015 -446.015] [55.895], Avg: [-447.972 -447.972 -447.972] (1.0000) ({r_i: None, r_t: [-876.962 -876.962 -876.962], eps: 1.0})
Step:   60100, Reward: [-454.069 -454.069 -454.069] [63.380], Avg: [-447.982 -447.982 -447.982] (1.0000) ({r_i: None, r_t: [-909.383 -909.383 -909.383], eps: 1.0})
Step:   60200, Reward: [-420.806 -420.806 -420.806] [65.799], Avg: [-447.937 -447.937 -447.937] (1.0000) ({r_i: None, r_t: [-879.648 -879.648 -879.648], eps: 1.0})
Step:   60300, Reward: [-435.102 -435.102 -435.102] [76.396], Avg: [-447.916 -447.916 -447.916] (1.0000) ({r_i: None, r_t: [-922.996 -922.996 -922.996], eps: 1.0})
Step:   60400, Reward: [-429.341 -429.341 -429.341] [55.685], Avg: [-447.885 -447.885 -447.885] (1.0000) ({r_i: None, r_t: [-919.497 -919.497 -919.497], eps: 1.0})
Step:   60500, Reward: [-444.705 -444.705 -444.705] [81.560], Avg: [-447.880 -447.880 -447.880] (1.0000) ({r_i: None, r_t: [-910.327 -910.327 -910.327], eps: 1.0})
Step:   60600, Reward: [-457.143 -457.143 -457.143] [49.970], Avg: [-447.895 -447.895 -447.895] (1.0000) ({r_i: None, r_t: [-920.029 -920.029 -920.029], eps: 1.0})
Step:   60700, Reward: [-447.222 -447.222 -447.222] [78.940], Avg: [-447.894 -447.894 -447.894] (1.0000) ({r_i: None, r_t: [-883.042 -883.042 -883.042], eps: 1.0})
Step:   60800, Reward: [-445.444 -445.444 -445.444] [81.802], Avg: [-447.890 -447.890 -447.890] (1.0000) ({r_i: None, r_t: [-889.198 -889.198 -889.198], eps: 1.0})
Step:   60900, Reward: [-414.846 -414.846 -414.846] [65.801], Avg: [-447.836 -447.836 -447.836] (1.0000) ({r_i: None, r_t: [-908.574 -908.574 -908.574], eps: 1.0})
Step:   61000, Reward: [-450.879 -450.879 -450.879] [68.631], Avg: [-447.841 -447.841 -447.841] (1.0000) ({r_i: None, r_t: [-847.554 -847.554 -847.554], eps: 1.0})
Step:   61100, Reward: [-413.125 -413.125 -413.125] [68.960], Avg: [-447.784 -447.784 -447.784] (1.0000) ({r_i: None, r_t: [-886.920 -886.920 -886.920], eps: 1.0})
Step:   61200, Reward: [-432.444 -432.444 -432.444] [58.859], Avg: [-447.759 -447.759 -447.759] (1.0000) ({r_i: None, r_t: [-853.842 -853.842 -853.842], eps: 1.0})
Step:   61300, Reward: [-437.933 -437.933 -437.933] [69.198], Avg: [-447.743 -447.743 -447.743] (1.0000) ({r_i: None, r_t: [-916.996 -916.996 -916.996], eps: 1.0})
Step:   61400, Reward: [-420.407 -420.407 -420.407] [63.228], Avg: [-447.699 -447.699 -447.699] (1.0000) ({r_i: None, r_t: [-852.712 -852.712 -852.712], eps: 1.0})
Step:   61500, Reward: [-465.747 -465.747 -465.747] [76.846], Avg: [-447.728 -447.728 -447.728] (1.0000) ({r_i: None, r_t: [-870.425 -870.425 -870.425], eps: 1.0})
Step:   61600, Reward: [-444.496 -444.496 -444.496] [64.335], Avg: [-447.723 -447.723 -447.723] (1.0000) ({r_i: None, r_t: [-878.303 -878.303 -878.303], eps: 1.0})
Step:   61700, Reward: [-461.894 -461.894 -461.894] [76.413], Avg: [-447.746 -447.746 -447.746] (1.0000) ({r_i: None, r_t: [-870.545 -870.545 -870.545], eps: 1.0})
Step:   61800, Reward: [-427.740 -427.740 -427.740] [73.186], Avg: [-447.713 -447.713 -447.713] (1.0000) ({r_i: None, r_t: [-913.024 -913.024 -913.024], eps: 1.0})
Step:   61900, Reward: [-459.312 -459.312 -459.312] [68.173], Avg: [-447.732 -447.732 -447.732] (1.0000) ({r_i: None, r_t: [-845.889 -845.889 -845.889], eps: 1.0})
Step:   62000, Reward: [-465.666 -465.666 -465.666] [69.956], Avg: [-447.761 -447.761 -447.761] (1.0000) ({r_i: None, r_t: [-903.940 -903.940 -903.940], eps: 1.0})
Step:   62100, Reward: [-445.298 -445.298 -445.298] [63.482], Avg: [-447.757 -447.757 -447.757] (1.0000) ({r_i: None, r_t: [-910.331 -910.331 -910.331], eps: 1.0})
Step:   62200, Reward: [-431.568 -431.568 -431.568] [69.152], Avg: [-447.731 -447.731 -447.731] (1.0000) ({r_i: None, r_t: [-819.475 -819.475 -819.475], eps: 1.0})
Step:   62300, Reward: [-421.844 -421.844 -421.844] [69.623], Avg: [-447.690 -447.690 -447.690] (1.0000) ({r_i: None, r_t: [-869.122 -869.122 -869.122], eps: 1.0})
Step:   62400, Reward: [-433.522 -433.522 -433.522] [73.374], Avg: [-447.667 -447.667 -447.667] (1.0000) ({r_i: None, r_t: [-875.093 -875.093 -875.093], eps: 1.0})
Step:   62500, Reward: [-432.248 -432.248 -432.248] [53.271], Avg: [-447.642 -447.642 -447.642] (1.0000) ({r_i: None, r_t: [-896.955 -896.955 -896.955], eps: 1.0})
Step:   62600, Reward: [-439.138 -439.138 -439.138] [66.596], Avg: [-447.629 -447.629 -447.629] (1.0000) ({r_i: None, r_t: [-856.158 -856.158 -856.158], eps: 1.0})
Step:   62700, Reward: [-420.965 -420.965 -420.965] [40.838], Avg: [-447.586 -447.586 -447.586] (1.0000) ({r_i: None, r_t: [-846.249 -846.249 -846.249], eps: 1.0})
Step:   62800, Reward: [-431.370 -431.370 -431.370] [51.830], Avg: [-447.560 -447.560 -447.560] (1.0000) ({r_i: None, r_t: [-883.565 -883.565 -883.565], eps: 1.0})
Step:   62900, Reward: [-475.581 -475.581 -475.581] [81.469], Avg: [-447.605 -447.605 -447.605] (1.0000) ({r_i: None, r_t: [-819.627 -819.627 -819.627], eps: 1.0})
Step:   63000, Reward: [-417.995 -417.995 -417.995] [55.475], Avg: [-447.558 -447.558 -447.558] (1.0000) ({r_i: None, r_t: [-845.347 -845.347 -845.347], eps: 1.0})
Step:   63100, Reward: [-443.316 -443.316 -443.316] [66.676], Avg: [-447.551 -447.551 -447.551] (1.0000) ({r_i: None, r_t: [-858.497 -858.497 -858.497], eps: 1.0})
Step:   63200, Reward: [-428.447 -428.447 -428.447] [63.287], Avg: [-447.521 -447.521 -447.521] (1.0000) ({r_i: None, r_t: [-891.546 -891.546 -891.546], eps: 1.0})
Step:   63300, Reward: [-431.638 -431.638 -431.638] [46.206], Avg: [-447.496 -447.496 -447.496] (1.0000) ({r_i: None, r_t: [-830.936 -830.936 -830.936], eps: 1.0})
Step:   63400, Reward: [-457.569 -457.569 -457.569] [91.096], Avg: [-447.512 -447.512 -447.512] (1.0000) ({r_i: None, r_t: [-841.803 -841.803 -841.803], eps: 1.0})
Step:   63500, Reward: [-430.890 -430.890 -430.890] [54.610], Avg: [-447.486 -447.486 -447.486] (1.0000) ({r_i: None, r_t: [-904.818 -904.818 -904.818], eps: 1.0})
Step:   63600, Reward: [-409.888 -409.888 -409.888] [70.347], Avg: [-447.427 -447.427 -447.427] (1.0000) ({r_i: None, r_t: [-858.416 -858.416 -858.416], eps: 1.0})
Step:   63700, Reward: [-396.051 -396.051 -396.051] [57.167], Avg: [-447.346 -447.346 -447.346] (1.0000) ({r_i: None, r_t: [-855.345 -855.345 -855.345], eps: 1.0})
Step:   63800, Reward: [-415.457 -415.457 -415.457] [58.090], Avg: [-447.296 -447.296 -447.296] (1.0000) ({r_i: None, r_t: [-847.101 -847.101 -847.101], eps: 1.0})
Step:   63900, Reward: [-420.093 -420.093 -420.093] [66.144], Avg: [-447.254 -447.254 -447.254] (1.0000) ({r_i: None, r_t: [-868.040 -868.040 -868.040], eps: 1.0})
Step:   64000, Reward: [-409.563 -409.563 -409.563] [90.898], Avg: [-447.195 -447.195 -447.195] (1.0000) ({r_i: None, r_t: [-833.810 -833.810 -833.810], eps: 1.0})
Step:   64100, Reward: [-435.219 -435.219 -435.219] [52.666], Avg: [-447.176 -447.176 -447.176] (1.0000) ({r_i: None, r_t: [-821.476 -821.476 -821.476], eps: 1.0})
Step:   64200, Reward: [-421.116 -421.116 -421.116] [80.320], Avg: [-447.136 -447.136 -447.136] (1.0000) ({r_i: None, r_t: [-802.069 -802.069 -802.069], eps: 1.0})
Step:   64300, Reward: [-436.863 -436.863 -436.863] [74.989], Avg: [-447.120 -447.120 -447.120] (1.0000) ({r_i: None, r_t: [-829.643 -829.643 -829.643], eps: 1.0})
Step:   64400, Reward: [-406.430 -406.430 -406.430] [66.441], Avg: [-447.057 -447.057 -447.057] (1.0000) ({r_i: None, r_t: [-810.795 -810.795 -810.795], eps: 1.0})
Step:   64500, Reward: [-438.598 -438.598 -438.598] [64.802], Avg: [-447.044 -447.044 -447.044] (1.0000) ({r_i: None, r_t: [-798.048 -798.048 -798.048], eps: 1.0})
Step:   64600, Reward: [-430.596 -430.596 -430.596] [77.865], Avg: [-447.018 -447.018 -447.018] (1.0000) ({r_i: None, r_t: [-823.409 -823.409 -823.409], eps: 1.0})
Step:   64700, Reward: [-390.036 -390.036 -390.036] [72.137], Avg: [-446.930 -446.930 -446.930] (1.0000) ({r_i: None, r_t: [-835.832 -835.832 -835.832], eps: 1.0})
Step:   64800, Reward: [-410.428 -410.428 -410.428] [65.920], Avg: [-446.874 -446.874 -446.874] (1.0000) ({r_i: None, r_t: [-833.996 -833.996 -833.996], eps: 1.0})
Step:   64900, Reward: [-397.978 -397.978 -397.978] [52.913], Avg: [-446.799 -446.799 -446.799] (1.0000) ({r_i: None, r_t: [-801.789 -801.789 -801.789], eps: 1.0})
Step:   65000, Reward: [-410.679 -410.679 -410.679] [58.335], Avg: [-446.743 -446.743 -446.743] (1.0000) ({r_i: None, r_t: [-817.820 -817.820 -817.820], eps: 1.0})
Step:   65100, Reward: [-420.103 -420.103 -420.103] [73.709], Avg: [-446.703 -446.703 -446.703] (1.0000) ({r_i: None, r_t: [-771.444 -771.444 -771.444], eps: 1.0})
Step:   65200, Reward: [-387.932 -387.932 -387.932] [62.455], Avg: [-446.613 -446.613 -446.613] (1.0000) ({r_i: None, r_t: [-836.049 -836.049 -836.049], eps: 1.0})
Step:   65300, Reward: [-409.002 -409.002 -409.002] [69.579], Avg: [-446.555 -446.555 -446.555] (1.0000) ({r_i: None, r_t: [-827.961 -827.961 -827.961], eps: 1.0})
Step:   65400, Reward: [-393.592 -393.592 -393.592] [71.435], Avg: [-446.474 -446.474 -446.474] (1.0000) ({r_i: None, r_t: [-798.698 -798.698 -798.698], eps: 1.0})
Step:   65500, Reward: [-388.825 -388.825 -388.825] [63.493], Avg: [-446.386 -446.386 -446.386] (1.0000) ({r_i: None, r_t: [-791.039 -791.039 -791.039], eps: 1.0})
Step:   65600, Reward: [-389.772 -389.772 -389.772] [60.341], Avg: [-446.300 -446.300 -446.300] (1.0000) ({r_i: None, r_t: [-810.465 -810.465 -810.465], eps: 1.0})
Step:   65700, Reward: [-416.268 -416.268 -416.268] [64.228], Avg: [-446.254 -446.254 -446.254] (1.0000) ({r_i: None, r_t: [-811.807 -811.807 -811.807], eps: 1.0})
Step:   65800, Reward: [-396.792 -396.792 -396.792] [78.499], Avg: [-446.179 -446.179 -446.179] (1.0000) ({r_i: None, r_t: [-787.447 -787.447 -787.447], eps: 1.0})
Step:   65900, Reward: [-369.677 -369.677 -369.677] [76.452], Avg: [-446.064 -446.064 -446.064] (1.0000) ({r_i: None, r_t: [-846.333 -846.333 -846.333], eps: 1.0})
Step:   66000, Reward: [-372.576 -372.576 -372.576] [52.429], Avg: [-445.952 -445.952 -445.952] (1.0000) ({r_i: None, r_t: [-782.564 -782.564 -782.564], eps: 1.0})
Step:   66100, Reward: [-400.664 -400.664 -400.664] [56.809], Avg: [-445.884 -445.884 -445.884] (1.0000) ({r_i: None, r_t: [-835.438 -835.438 -835.438], eps: 1.0})
Step:   66200, Reward: [-427.569 -427.569 -427.569] [94.551], Avg: [-445.856 -445.856 -445.856] (1.0000) ({r_i: None, r_t: [-866.159 -866.159 -866.159], eps: 1.0})
Step:   66300, Reward: [-383.360 -383.360 -383.360] [67.367], Avg: [-445.762 -445.762 -445.762] (1.0000) ({r_i: None, r_t: [-789.685 -789.685 -789.685], eps: 1.0})
Step:   66400, Reward: [-398.025 -398.025 -398.025] [56.283], Avg: [-445.690 -445.690 -445.690] (1.0000) ({r_i: None, r_t: [-821.186 -821.186 -821.186], eps: 1.0})
Step:   66500, Reward: [-394.745 -394.745 -394.745] [82.779], Avg: [-445.614 -445.614 -445.614] (1.0000) ({r_i: None, r_t: [-841.384 -841.384 -841.384], eps: 1.0})
Step:   66600, Reward: [-409.053 -409.053 -409.053] [95.910], Avg: [-445.559 -445.559 -445.559] (1.0000) ({r_i: None, r_t: [-802.060 -802.060 -802.060], eps: 1.0})
Step:   66700, Reward: [-385.739 -385.739 -385.739] [62.368], Avg: [-445.470 -445.470 -445.470] (1.0000) ({r_i: None, r_t: [-848.795 -848.795 -848.795], eps: 1.0})
Step:   66800, Reward: [-423.650 -423.650 -423.650] [62.016], Avg: [-445.437 -445.437 -445.437] (1.0000) ({r_i: None, r_t: [-813.411 -813.411 -813.411], eps: 1.0})
Step:   66900, Reward: [-401.519 -401.519 -401.519] [58.013], Avg: [-445.371 -445.371 -445.371] (1.0000) ({r_i: None, r_t: [-804.127 -804.127 -804.127], eps: 1.0})
Step:   67000, Reward: [-430.344 -430.344 -430.344] [78.976], Avg: [-445.349 -445.349 -445.349] (1.0000) ({r_i: None, r_t: [-847.872 -847.872 -847.872], eps: 1.0})
Step:   67100, Reward: [-406.258 -406.258 -406.258] [96.718], Avg: [-445.291 -445.291 -445.291] (1.0000) ({r_i: None, r_t: [-848.421 -848.421 -848.421], eps: 1.0})
Step:   67200, Reward: [-401.180 -401.180 -401.180] [74.098], Avg: [-445.225 -445.225 -445.225] (1.0000) ({r_i: None, r_t: [-773.162 -773.162 -773.162], eps: 1.0})
Step:   67300, Reward: [-388.729 -388.729 -388.729] [53.921], Avg: [-445.141 -445.141 -445.141] (1.0000) ({r_i: None, r_t: [-766.923 -766.923 -766.923], eps: 1.0})
Step:   67400, Reward: [-409.187 -409.187 -409.187] [56.211], Avg: [-445.088 -445.088 -445.088] (1.0000) ({r_i: None, r_t: [-824.130 -824.130 -824.130], eps: 1.0})
Step:   67500, Reward: [-431.733 -431.733 -431.733] [110.872], Avg: [-445.068 -445.068 -445.068] (1.0000) ({r_i: None, r_t: [-793.057 -793.057 -793.057], eps: 1.0})
Step:   67600, Reward: [-434.119 -434.119 -434.119] [122.437], Avg: [-445.052 -445.052 -445.052] (1.0000) ({r_i: None, r_t: [-838.128 -838.128 -838.128], eps: 1.0})
Step:   67700, Reward: [-423.824 -423.824 -423.824] [85.560], Avg: [-445.021 -445.021 -445.021] (1.0000) ({r_i: None, r_t: [-862.679 -862.679 -862.679], eps: 1.0})
Step:   67800, Reward: [-411.125 -411.125 -411.125] [66.750], Avg: [-444.971 -444.971 -444.971] (1.0000) ({r_i: None, r_t: [-818.800 -818.800 -818.800], eps: 1.0})
Step:   67900, Reward: [-372.174 -372.174 -372.174] [74.196], Avg: [-444.864 -444.864 -444.864] (1.0000) ({r_i: None, r_t: [-809.466 -809.466 -809.466], eps: 1.0})
Step:   68000, Reward: [-404.575 -404.575 -404.575] [82.544], Avg: [-444.805 -444.805 -444.805] (1.0000) ({r_i: None, r_t: [-835.745 -835.745 -835.745], eps: 1.0})
Step:   68100, Reward: [-416.409 -416.409 -416.409] [61.399], Avg: [-444.763 -444.763 -444.763] (1.0000) ({r_i: None, r_t: [-819.665 -819.665 -819.665], eps: 1.0})
Step:   68200, Reward: [-447.501 -447.501 -447.501] [82.174], Avg: [-444.767 -444.767 -444.767] (1.0000) ({r_i: None, r_t: [-841.957 -841.957 -841.957], eps: 1.0})
Step:   68300, Reward: [-438.003 -438.003 -438.003] [78.141], Avg: [-444.757 -444.757 -444.757] (1.0000) ({r_i: None, r_t: [-873.529 -873.529 -873.529], eps: 1.0})
Step:   68400, Reward: [-447.345 -447.345 -447.345] [96.506], Avg: [-444.761 -444.761 -444.761] (1.0000) ({r_i: None, r_t: [-777.609 -777.609 -777.609], eps: 1.0})
Step:   68500, Reward: [-439.841 -439.841 -439.841] [114.857], Avg: [-444.754 -444.754 -444.754] (1.0000) ({r_i: None, r_t: [-864.096 -864.096 -864.096], eps: 1.0})
Step:   68600, Reward: [-472.165 -472.165 -472.165] [116.623], Avg: [-444.794 -444.794 -444.794] (1.0000) ({r_i: None, r_t: [-839.353 -839.353 -839.353], eps: 1.0})
Step:   68700, Reward: [-455.662 -455.662 -455.662] [89.228], Avg: [-444.810 -444.810 -444.810] (1.0000) ({r_i: None, r_t: [-841.379 -841.379 -841.379], eps: 1.0})
Step:   68800, Reward: [-397.921 -397.921 -397.921] [68.494], Avg: [-444.742 -444.742 -444.742] (1.0000) ({r_i: None, r_t: [-814.549 -814.549 -814.549], eps: 1.0})
Step:   68900, Reward: [-452.866 -452.866 -452.866] [71.376], Avg: [-444.753 -444.753 -444.753] (1.0000) ({r_i: None, r_t: [-833.907 -833.907 -833.907], eps: 1.0})
Step:   69000, Reward: [-430.034 -430.034 -430.034] [57.606], Avg: [-444.732 -444.732 -444.732] (1.0000) ({r_i: None, r_t: [-804.215 -804.215 -804.215], eps: 1.0})
Step:   69100, Reward: [-436.761 -436.761 -436.761] [107.473], Avg: [-444.720 -444.720 -444.720] (1.0000) ({r_i: None, r_t: [-752.431 -752.431 -752.431], eps: 1.0})
Step:   69200, Reward: [-436.353 -436.353 -436.353] [82.977], Avg: [-444.708 -444.708 -444.708] (1.0000) ({r_i: None, r_t: [-877.415 -877.415 -877.415], eps: 1.0})
Step:   69300, Reward: [-390.146 -390.146 -390.146] [90.566], Avg: [-444.630 -444.630 -444.630] (1.0000) ({r_i: None, r_t: [-811.312 -811.312 -811.312], eps: 1.0})
Step:   69400, Reward: [-399.720 -399.720 -399.720] [78.828], Avg: [-444.565 -444.565 -444.565] (1.0000) ({r_i: None, r_t: [-852.189 -852.189 -852.189], eps: 1.0})
Step:   69500, Reward: [-425.419 -425.419 -425.419] [99.312], Avg: [-444.538 -444.538 -444.538] (1.0000) ({r_i: None, r_t: [-857.743 -857.743 -857.743], eps: 1.0})
Step:   69600, Reward: [-419.898 -419.898 -419.898] [74.347], Avg: [-444.502 -444.502 -444.502] (1.0000) ({r_i: None, r_t: [-837.541 -837.541 -837.541], eps: 1.0})
Step:   69700, Reward: [-421.146 -421.146 -421.146] [81.209], Avg: [-444.469 -444.469 -444.469] (1.0000) ({r_i: None, r_t: [-815.778 -815.778 -815.778], eps: 1.0})
Step:   69800, Reward: [-392.322 -392.322 -392.322] [74.724], Avg: [-444.394 -444.394 -444.394] (1.0000) ({r_i: None, r_t: [-937.939 -937.939 -937.939], eps: 1.0})
Step:   69900, Reward: [-440.104 -440.104 -440.104] [61.673], Avg: [-444.388 -444.388 -444.388] (1.0000) ({r_i: None, r_t: [-832.894 -832.894 -832.894], eps: 1.0})
Step:   70000, Reward: [-388.552 -388.552 -388.552] [75.103], Avg: [-444.308 -444.308 -444.308] (1.0000) ({r_i: None, r_t: [-816.183 -816.183 -816.183], eps: 1.0})
Step:   70100, Reward: [-463.679 -463.679 -463.679] [107.032], Avg: [-444.336 -444.336 -444.336] (1.0000) ({r_i: None, r_t: [-884.715 -884.715 -884.715], eps: 1.0})
Step:   70200, Reward: [-440.926 -440.926 -440.926] [69.660], Avg: [-444.331 -444.331 -444.331] (1.0000) ({r_i: None, r_t: [-831.130 -831.130 -831.130], eps: 1.0})
Step:   70300, Reward: [-452.724 -452.724 -452.724] [77.419], Avg: [-444.343 -444.343 -444.343] (1.0000) ({r_i: None, r_t: [-892.825 -892.825 -892.825], eps: 1.0})
Step:   70400, Reward: [-419.934 -419.934 -419.934] [81.688], Avg: [-444.309 -444.309 -444.309] (1.0000) ({r_i: None, r_t: [-812.955 -812.955 -812.955], eps: 1.0})
Step:   70500, Reward: [-445.183 -445.183 -445.183] [99.574], Avg: [-444.310 -444.310 -444.310] (1.0000) ({r_i: None, r_t: [-855.263 -855.263 -855.263], eps: 1.0})
Step:   70600, Reward: [-463.795 -463.795 -463.795] [111.133], Avg: [-444.337 -444.337 -444.337] (1.0000) ({r_i: None, r_t: [-821.471 -821.471 -821.471], eps: 1.0})
Step:   70700, Reward: [-389.469 -389.469 -389.469] [76.215], Avg: [-444.260 -444.260 -444.260] (1.0000) ({r_i: None, r_t: [-863.351 -863.351 -863.351], eps: 1.0})
Step:   70800, Reward: [-437.474 -437.474 -437.474] [90.961], Avg: [-444.250 -444.250 -444.250] (1.0000) ({r_i: None, r_t: [-881.727 -881.727 -881.727], eps: 1.0})
Step:   70900, Reward: [-422.625 -422.625 -422.625] [68.755], Avg: [-444.220 -444.220 -444.220] (1.0000) ({r_i: None, r_t: [-875.400 -875.400 -875.400], eps: 1.0})
Step:   71000, Reward: [-446.373 -446.373 -446.373] [80.150], Avg: [-444.223 -444.223 -444.223] (1.0000) ({r_i: None, r_t: [-813.546 -813.546 -813.546], eps: 1.0})
Step:   71100, Reward: [-385.178 -385.178 -385.178] [87.103], Avg: [-444.140 -444.140 -444.140] (1.0000) ({r_i: None, r_t: [-877.428 -877.428 -877.428], eps: 1.0})
Step:   71200, Reward: [-425.887 -425.887 -425.887] [68.487], Avg: [-444.114 -444.114 -444.114] (1.0000) ({r_i: None, r_t: [-850.832 -850.832 -850.832], eps: 1.0})
Step:   71300, Reward: [-412.509 -412.509 -412.509] [79.706], Avg: [-444.070 -444.070 -444.070] (1.0000) ({r_i: None, r_t: [-872.056 -872.056 -872.056], eps: 1.0})
Step:   71400, Reward: [-448.647 -448.647 -448.647] [88.005], Avg: [-444.076 -444.076 -444.076] (1.0000) ({r_i: None, r_t: [-835.597 -835.597 -835.597], eps: 1.0})
Step:   71500, Reward: [-413.715 -413.715 -413.715] [77.513], Avg: [-444.034 -444.034 -444.034] (1.0000) ({r_i: None, r_t: [-808.371 -808.371 -808.371], eps: 1.0})
Step:   71600, Reward: [-421.689 -421.689 -421.689] [76.425], Avg: [-444.003 -444.003 -444.003] (1.0000) ({r_i: None, r_t: [-888.967 -888.967 -888.967], eps: 1.0})
Step:   71700, Reward: [-440.055 -440.055 -440.055] [65.874], Avg: [-443.997 -443.997 -443.997] (1.0000) ({r_i: None, r_t: [-805.896 -805.896 -805.896], eps: 1.0})
Step:   71800, Reward: [-406.408 -406.408 -406.408] [86.514], Avg: [-443.945 -443.945 -443.945] (1.0000) ({r_i: None, r_t: [-811.188 -811.188 -811.188], eps: 1.0})
Step:   71900, Reward: [-435.821 -435.821 -435.821] [62.454], Avg: [-443.934 -443.934 -443.934] (1.0000) ({r_i: None, r_t: [-854.403 -854.403 -854.403], eps: 1.0})
Step:   72000, Reward: [-389.571 -389.571 -389.571] [105.771], Avg: [-443.858 -443.858 -443.858] (1.0000) ({r_i: None, r_t: [-802.978 -802.978 -802.978], eps: 1.0})
Step:   72100, Reward: [-398.761 -398.761 -398.761] [55.477], Avg: [-443.796 -443.796 -443.796] (1.0000) ({r_i: None, r_t: [-847.315 -847.315 -847.315], eps: 1.0})
Step:   72200, Reward: [-410.680 -410.680 -410.680] [49.046], Avg: [-443.750 -443.750 -443.750] (1.0000) ({r_i: None, r_t: [-843.365 -843.365 -843.365], eps: 1.0})
Step:   72300, Reward: [-395.288 -395.288 -395.288] [75.568], Avg: [-443.683 -443.683 -443.683] (1.0000) ({r_i: None, r_t: [-752.808 -752.808 -752.808], eps: 1.0})
Step:   72400, Reward: [-408.253 -408.253 -408.253] [62.343], Avg: [-443.634 -443.634 -443.634] (1.0000) ({r_i: None, r_t: [-801.242 -801.242 -801.242], eps: 1.0})
Step:   72500, Reward: [-420.695 -420.695 -420.695] [83.681], Avg: [-443.603 -443.603 -443.603] (1.0000) ({r_i: None, r_t: [-809.943 -809.943 -809.943], eps: 1.0})
Step:   72600, Reward: [-386.837 -386.837 -386.837] [73.313], Avg: [-443.525 -443.525 -443.525] (1.0000) ({r_i: None, r_t: [-835.745 -835.745 -835.745], eps: 1.0})
Step:   72700, Reward: [-421.366 -421.366 -421.366] [57.918], Avg: [-443.494 -443.494 -443.494] (1.0000) ({r_i: None, r_t: [-791.277 -791.277 -791.277], eps: 1.0})
Step:   72800, Reward: [-405.350 -405.350 -405.350] [106.305], Avg: [-443.442 -443.442 -443.442] (1.0000) ({r_i: None, r_t: [-850.525 -850.525 -850.525], eps: 1.0})
Step:   72900, Reward: [-375.754 -375.754 -375.754] [82.376], Avg: [-443.349 -443.349 -443.349] (1.0000) ({r_i: None, r_t: [-821.645 -821.645 -821.645], eps: 1.0})
Step:   73000, Reward: [-385.362 -385.362 -385.362] [52.552], Avg: [-443.270 -443.270 -443.270] (1.0000) ({r_i: None, r_t: [-817.081 -817.081 -817.081], eps: 1.0})
Step:   73100, Reward: [-387.597 -387.597 -387.597] [75.216], Avg: [-443.194 -443.194 -443.194] (1.0000) ({r_i: None, r_t: [-810.835 -810.835 -810.835], eps: 1.0})
Step:   73200, Reward: [-430.925 -430.925 -430.925] [61.588], Avg: [-443.177 -443.177 -443.177] (1.0000) ({r_i: None, r_t: [-833.424 -833.424 -833.424], eps: 1.0})
Step:   73300, Reward: [-404.596 -404.596 -404.596] [58.918], Avg: [-443.124 -443.124 -443.124] (1.0000) ({r_i: None, r_t: [-831.721 -831.721 -831.721], eps: 1.0})
Step:   73400, Reward: [-415.108 -415.108 -415.108] [81.779], Avg: [-443.086 -443.086 -443.086] (1.0000) ({r_i: None, r_t: [-789.177 -789.177 -789.177], eps: 1.0})
Step:   73500, Reward: [-430.347 -430.347 -430.347] [125.555], Avg: [-443.069 -443.069 -443.069] (1.0000) ({r_i: None, r_t: [-829.202 -829.202 -829.202], eps: 1.0})
Step:   73600, Reward: [-402.360 -402.360 -402.360] [50.394], Avg: [-443.014 -443.014 -443.014] (1.0000) ({r_i: None, r_t: [-757.441 -757.441 -757.441], eps: 1.0})
Step:   73700, Reward: [-418.687 -418.687 -418.687] [99.399], Avg: [-442.981 -442.981 -442.981] (1.0000) ({r_i: None, r_t: [-824.298 -824.298 -824.298], eps: 1.0})
Step:   73800, Reward: [-400.431 -400.431 -400.431] [80.002], Avg: [-442.923 -442.923 -442.923] (1.0000) ({r_i: None, r_t: [-845.851 -845.851 -845.851], eps: 1.0})
Step:   73900, Reward: [-439.065 -439.065 -439.065] [82.322], Avg: [-442.918 -442.918 -442.918] (1.0000) ({r_i: None, r_t: [-812.450 -812.450 -812.450], eps: 1.0})
Step:   74000, Reward: [-401.670 -401.670 -401.670] [78.436], Avg: [-442.862 -442.862 -442.862] (1.0000) ({r_i: None, r_t: [-796.208 -796.208 -796.208], eps: 1.0})
Step:   74100, Reward: [-433.162 -433.162 -433.162] [99.520], Avg: [-442.849 -442.849 -442.849] (1.0000) ({r_i: None, r_t: [-814.572 -814.572 -814.572], eps: 1.0})
Step:   74200, Reward: [-416.614 -416.614 -416.614] [59.953], Avg: [-442.814 -442.814 -442.814] (1.0000) ({r_i: None, r_t: [-797.814 -797.814 -797.814], eps: 1.0})
Step:   74300, Reward: [-433.291 -433.291 -433.291] [99.387], Avg: [-442.801 -442.801 -442.801] (1.0000) ({r_i: None, r_t: [-809.850 -809.850 -809.850], eps: 1.0})
Step:   74400, Reward: [-398.644 -398.644 -398.644] [77.303], Avg: [-442.742 -442.742 -442.742] (1.0000) ({r_i: None, r_t: [-782.742 -782.742 -782.742], eps: 1.0})
Step:   74500, Reward: [-409.076 -409.076 -409.076] [81.955], Avg: [-442.697 -442.697 -442.697] (1.0000) ({r_i: None, r_t: [-829.516 -829.516 -829.516], eps: 1.0})
Step:   74600, Reward: [-407.978 -407.978 -407.978] [69.513], Avg: [-442.650 -442.650 -442.650] (1.0000) ({r_i: None, r_t: [-813.455 -813.455 -813.455], eps: 1.0})
Step:   74700, Reward: [-425.751 -425.751 -425.751] [53.416], Avg: [-442.628 -442.628 -442.628] (1.0000) ({r_i: None, r_t: [-850.673 -850.673 -850.673], eps: 1.0})
Step:   74800, Reward: [-380.203 -380.203 -380.203] [59.604], Avg: [-442.544 -442.544 -442.544] (1.0000) ({r_i: None, r_t: [-795.219 -795.219 -795.219], eps: 1.0})
Step:   74900, Reward: [-423.640 -423.640 -423.640] [61.914], Avg: [-442.519 -442.519 -442.519] (1.0000) ({r_i: None, r_t: [-807.896 -807.896 -807.896], eps: 1.0})
Step:   75000, Reward: [-420.046 -420.046 -420.046] [76.405], Avg: [-442.489 -442.489 -442.489] (1.0000) ({r_i: None, r_t: [-791.403 -791.403 -791.403], eps: 1.0})
Step:   75100, Reward: [-373.120 -373.120 -373.120] [59.269], Avg: [-442.397 -442.397 -442.397] (1.0000) ({r_i: None, r_t: [-830.637 -830.637 -830.637], eps: 1.0})
Step:   75200, Reward: [-383.844 -383.844 -383.844] [73.549], Avg: [-442.319 -442.319 -442.319] (1.0000) ({r_i: None, r_t: [-837.342 -837.342 -837.342], eps: 1.0})
Step:   75300, Reward: [-400.972 -400.972 -400.972] [82.507], Avg: [-442.264 -442.264 -442.264] (1.0000) ({r_i: None, r_t: [-786.791 -786.791 -786.791], eps: 1.0})
Step:   75400, Reward: [-402.217 -402.217 -402.217] [77.425], Avg: [-442.211 -442.211 -442.211] (1.0000) ({r_i: None, r_t: [-870.070 -870.070 -870.070], eps: 1.0})
Step:   75500, Reward: [-408.366 -408.366 -408.366] [66.437], Avg: [-442.167 -442.167 -442.167] (1.0000) ({r_i: None, r_t: [-772.793 -772.793 -772.793], eps: 1.0})
Step:   75600, Reward: [-374.638 -374.638 -374.638] [55.155], Avg: [-442.077 -442.077 -442.077] (1.0000) ({r_i: None, r_t: [-806.051 -806.051 -806.051], eps: 1.0})
Step:   75700, Reward: [-397.756 -397.756 -397.756] [64.563], Avg: [-442.019 -442.019 -442.019] (1.0000) ({r_i: None, r_t: [-806.919 -806.919 -806.919], eps: 1.0})
Step:   75800, Reward: [-377.726 -377.726 -377.726] [54.319], Avg: [-441.934 -441.934 -441.934] (1.0000) ({r_i: None, r_t: [-805.063 -805.063 -805.063], eps: 1.0})
Step:   75900, Reward: [-427.876 -427.876 -427.876] [75.603], Avg: [-441.916 -441.916 -441.916] (1.0000) ({r_i: None, r_t: [-785.180 -785.180 -785.180], eps: 1.0})
Step:   76000, Reward: [-413.591 -413.591 -413.591] [57.487], Avg: [-441.878 -441.878 -441.878] (1.0000) ({r_i: None, r_t: [-792.768 -792.768 -792.768], eps: 1.0})
Step:   76100, Reward: [-406.356 -406.356 -406.356] [95.894], Avg: [-441.832 -441.832 -441.832] (1.0000) ({r_i: None, r_t: [-828.190 -828.190 -828.190], eps: 1.0})
Step:   76200, Reward: [-435.975 -435.975 -435.975] [81.294], Avg: [-441.824 -441.824 -441.824] (1.0000) ({r_i: None, r_t: [-808.945 -808.945 -808.945], eps: 1.0})
Step:   76300, Reward: [-415.916 -415.916 -415.916] [79.817], Avg: [-441.790 -441.790 -441.790] (1.0000) ({r_i: None, r_t: [-825.103 -825.103 -825.103], eps: 1.0})
Step:   76400, Reward: [-436.049 -436.049 -436.049] [54.884], Avg: [-441.783 -441.783 -441.783] (1.0000) ({r_i: None, r_t: [-788.729 -788.729 -788.729], eps: 1.0})
Step:   76500, Reward: [-412.994 -412.994 -412.994] [63.024], Avg: [-441.745 -441.745 -441.745] (1.0000) ({r_i: None, r_t: [-787.309 -787.309 -787.309], eps: 1.0})
Step:   76600, Reward: [-388.485 -388.485 -388.485] [46.537], Avg: [-441.676 -441.676 -441.676] (1.0000) ({r_i: None, r_t: [-791.953 -791.953 -791.953], eps: 1.0})
Step:   76700, Reward: [-435.464 -435.464 -435.464] [125.603], Avg: [-441.668 -441.668 -441.668] (1.0000) ({r_i: None, r_t: [-810.681 -810.681 -810.681], eps: 1.0})
Step:   76800, Reward: [-374.712 -374.712 -374.712] [58.844], Avg: [-441.581 -441.581 -441.581] (1.0000) ({r_i: None, r_t: [-789.468 -789.468 -789.468], eps: 1.0})
Step:   76900, Reward: [-393.061 -393.061 -393.061] [60.047], Avg: [-441.518 -441.518 -441.518] (1.0000) ({r_i: None, r_t: [-791.553 -791.553 -791.553], eps: 1.0})

Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.mac, self.n_agents, n_actions, device=self.device)
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.mac.select_actions(None, inputs, t_ep=None, t_env=None, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True))

class COMALearner():
	def __init__(self, state_size, action_size, mac, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac

		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())

		# self.agent_params = list(mac.parameters())
		self.agent_params = list(self.agent.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def select_actions(self, inputs, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(inputs, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, actor_inputs, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_):
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		mac_out = torch.stack([self.mac.forward(batch, actor_inputs[:,t], t) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def parameters(self):
		return self.agent.parameters()

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def forward(self, inputs):
		x = torch.relu(self.fc1(inputs))
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-504.806 -504.806 -504.806] [97.616], Avg: [-504.806 -504.806 -504.806] (1.0000) ({r_i: None, r_t: [-8.907 -8.907 -8.907], eps: 1.0})
Step:   77000, Reward: [-437.636 -437.636 -437.636] [104.514], Avg: [-441.513 -441.513 -441.513] (1.0000) ({r_i: None, r_t: [-835.060 -835.060 -835.060], eps: 1.0})
Step:   77100, Reward: [-392.912 -392.912 -392.912] [57.855], Avg: [-441.450 -441.450 -441.450] (1.0000) ({r_i: None, r_t: [-777.658 -777.658 -777.658], eps: 1.0})
Step:   77200, Reward: [-454.088 -454.088 -454.088] [99.119], Avg: [-441.466 -441.466 -441.466] (1.0000) ({r_i: None, r_t: [-776.239 -776.239 -776.239], eps: 1.0})
Step:     100, Reward: [-534.100 -534.100 -534.100] [115.509], Avg: [-519.453 -519.453 -519.453] (1.0000) ({r_i: None, r_t: [-952.402 -952.402 -952.402], eps: 1.0})
Step:   77300, Reward: [-423.265 -423.265 -423.265] [93.141], Avg: [-441.442 -441.442 -441.442] (1.0000) ({r_i: None, r_t: [-792.672 -792.672 -792.672], eps: 1.0})
Step:   77400, Reward: [-376.019 -376.019 -376.019] [50.841], Avg: [-441.358 -441.358 -441.358] (1.0000) ({r_i: None, r_t: [-758.395 -758.395 -758.395], eps: 1.0})
Step:   77500, Reward: [-416.406 -416.406 -416.406] [85.658], Avg: [-441.326 -441.326 -441.326] (1.0000) ({r_i: None, r_t: [-809.477 -809.477 -809.477], eps: 1.0})
Step:   77600, Reward: [-414.203 -414.203 -414.203] [111.593], Avg: [-441.291 -441.291 -441.291] (1.0000) ({r_i: None, r_t: [-816.035 -816.035 -816.035], eps: 1.0})
Step:   77700, Reward: [-397.051 -397.051 -397.051] [64.271], Avg: [-441.234 -441.234 -441.234] (1.0000) ({r_i: None, r_t: [-831.190 -831.190 -831.190], eps: 1.0})
Step:   77800, Reward: [-420.113 -420.113 -420.113] [71.676], Avg: [-441.207 -441.207 -441.207] (1.0000) ({r_i: None, r_t: [-812.269 -812.269 -812.269], eps: 1.0})
Step:   77900, Reward: [-400.087 -400.087 -400.087] [106.805], Avg: [-441.154 -441.154 -441.154] (1.0000) ({r_i: None, r_t: [-834.579 -834.579 -834.579], eps: 1.0})
Step:   78000, Reward: [-400.649 -400.649 -400.649] [94.256], Avg: [-441.102 -441.102 -441.102] (1.0000) ({r_i: None, r_t: [-848.875 -848.875 -848.875], eps: 1.0})
Step:   78100, Reward: [-411.842 -411.842 -411.842] [87.873], Avg: [-441.065 -441.065 -441.065] (1.0000) ({r_i: None, r_t: [-777.338 -777.338 -777.338], eps: 1.0})
Step:   78200, Reward: [-431.230 -431.230 -431.230] [82.901], Avg: [-441.052 -441.052 -441.052] (1.0000) ({r_i: None, r_t: [-838.139 -838.139 -838.139], eps: 1.0})
Step:   78300, Reward: [-438.526 -438.526 -438.526] [87.335], Avg: [-441.049 -441.049 -441.049] (1.0000) ({r_i: None, r_t: [-737.532 -737.532 -737.532], eps: 1.0})
Step:   78400, Reward: [-410.590 -410.590 -410.590] [89.836], Avg: [-441.010 -441.010 -441.010] (1.0000) ({r_i: None, r_t: [-825.984 -825.984 -825.984], eps: 1.0})
Step:   78500, Reward: [-408.562 -408.562 -408.562] [68.878], Avg: [-440.969 -440.969 -440.969] (1.0000) ({r_i: None, r_t: [-850.611 -850.611 -850.611], eps: 1.0})
Step:   78600, Reward: [-392.110 -392.110 -392.110] [93.602], Avg: [-440.907 -440.907 -440.907] (1.0000) ({r_i: None, r_t: [-810.758 -810.758 -810.758], eps: 1.0})
Step:   78700, Reward: [-422.634 -422.634 -422.634] [81.730], Avg: [-440.884 -440.884 -440.884] (1.0000) ({r_i: None, r_t: [-777.186 -777.186 -777.186], eps: 1.0})
Step:   78800, Reward: [-448.714 -448.714 -448.714] [79.398], Avg: [-440.894 -440.894 -440.894] (1.0000) ({r_i: None, r_t: [-840.767 -840.767 -840.767], eps: 1.0})
Step:   78900, Reward: [-404.513 -404.513 -404.513] [62.571], Avg: [-440.848 -440.848 -440.848] (1.0000) ({r_i: None, r_t: [-775.948 -775.948 -775.948], eps: 1.0})
Step:   79000, Reward: [-380.413 -380.413 -380.413] [64.575], Avg: [-440.771 -440.771 -440.771] (1.0000) ({r_i: None, r_t: [-795.357 -795.357 -795.357], eps: 1.0})
Step:   79100, Reward: [-430.461 -430.461 -430.461] [86.848], Avg: [-440.758 -440.758 -440.758] (1.0000) ({r_i: None, r_t: [-806.089 -806.089 -806.089], eps: 1.0})
Step:   79200, Reward: [-419.821 -419.821 -419.821] [80.067], Avg: [-440.732 -440.732 -440.732] (1.0000) ({r_i: None, r_t: [-769.311 -769.311 -769.311], eps: 1.0})
Step:   79300, Reward: [-389.193 -389.193 -389.193] [60.113], Avg: [-440.667 -440.667 -440.667] (1.0000) ({r_i: None, r_t: [-825.562 -825.562 -825.562], eps: 1.0})
Step:   79400, Reward: [-406.570 -406.570 -406.570] [82.886], Avg: [-440.624 -440.624 -440.624] (1.0000) ({r_i: None, r_t: [-798.618 -798.618 -798.618], eps: 1.0})
Step:   79500, Reward: [-386.073 -386.073 -386.073] [64.238], Avg: [-440.556 -440.556 -440.556] (1.0000) ({r_i: None, r_t: [-838.803 -838.803 -838.803], eps: 1.0})
Step:   79600, Reward: [-407.385 -407.385 -407.385] [103.996], Avg: [-440.514 -440.514 -440.514] (1.0000) ({r_i: None, r_t: [-800.419 -800.419 -800.419], eps: 1.0})
Step:   79700, Reward: [-390.170 -390.170 -390.170] [85.776], Avg: [-440.451 -440.451 -440.451] (1.0000) ({r_i: None, r_t: [-786.182 -786.182 -786.182], eps: 1.0})
Step:   79800, Reward: [-402.787 -402.787 -402.787] [63.967], Avg: [-440.404 -440.404 -440.404] (1.0000) ({r_i: None, r_t: [-811.970 -811.970 -811.970], eps: 1.0})
Step:   79900, Reward: [-379.804 -379.804 -379.804] [87.313], Avg: [-440.328 -440.328 -440.328] (1.0000) ({r_i: None, r_t: [-840.335 -840.335 -840.335], eps: 1.0})
Step:   80000, Reward: [-398.442 -398.442 -398.442] [74.548], Avg: [-440.276 -440.276 -440.276] (1.0000) ({r_i: None, r_t: [-805.321 -805.321 -805.321], eps: 1.0})
Step:   80100, Reward: [-399.211 -399.211 -399.211] [54.947], Avg: [-440.224 -440.224 -440.224] (1.0000) ({r_i: None, r_t: [-773.870 -773.870 -773.870], eps: 1.0})
Step:   80200, Reward: [-416.583 -416.583 -416.583] [42.074], Avg: [-440.195 -440.195 -440.195] (1.0000) ({r_i: None, r_t: [-818.041 -818.041 -818.041], eps: 1.0})
Step:   80300, Reward: [-412.108 -412.108 -412.108] [60.206], Avg: [-440.160 -440.160 -440.160] (1.0000) ({r_i: None, r_t: [-867.955 -867.955 -867.955], eps: 1.0})
Step:   80400, Reward: [-429.149 -429.149 -429.149] [111.985], Avg: [-440.146 -440.146 -440.146] (1.0000) ({r_i: None, r_t: [-798.359 -798.359 -798.359], eps: 1.0})
Step:   80500, Reward: [-410.925 -410.925 -410.925] [69.886], Avg: [-440.110 -440.110 -440.110] (1.0000) ({r_i: None, r_t: [-805.550 -805.550 -805.550], eps: 1.0})
Step:   80600, Reward: [-407.218 -407.218 -407.218] [85.057], Avg: [-440.069 -440.069 -440.069] (1.0000) ({r_i: None, r_t: [-838.191 -838.191 -838.191], eps: 1.0})
Step:   80700, Reward: [-453.596 -453.596 -453.596] [91.634], Avg: [-440.086 -440.086 -440.086] (1.0000) ({r_i: None, r_t: [-791.111 -791.111 -791.111], eps: 1.0})
Step:   80800, Reward: [-375.500 -375.500 -375.500] [59.410], Avg: [-440.006 -440.006 -440.006] (1.0000) ({r_i: None, r_t: [-885.344 -885.344 -885.344], eps: 1.0})
Step:   80900, Reward: [-384.513 -384.513 -384.513] [73.505], Avg: [-439.938 -439.938 -439.938] (1.0000) ({r_i: None, r_t: [-827.875 -827.875 -827.875], eps: 1.0})
Step:   81000, Reward: [-439.201 -439.201 -439.201] [101.105], Avg: [-439.937 -439.937 -439.937] (1.0000) ({r_i: None, r_t: [-848.179 -848.179 -848.179], eps: 1.0})
Step:   81100, Reward: [-450.993 -450.993 -450.993] [87.837], Avg: [-439.951 -439.951 -439.951] (1.0000) ({r_i: None, r_t: [-868.564 -868.564 -868.564], eps: 1.0})
Step:   81200, Reward: [-372.502 -372.502 -372.502] [39.430], Avg: [-439.868 -439.868 -439.868] (1.0000) ({r_i: None, r_t: [-787.129 -787.129 -787.129], eps: 1.0})
Step:   81300, Reward: [-418.066 -418.066 -418.066] [106.394], Avg: [-439.841 -439.841 -439.841] (1.0000) ({r_i: None, r_t: [-830.693 -830.693 -830.693], eps: 1.0})
Step:   81400, Reward: [-382.525 -382.525 -382.525] [78.541], Avg: [-439.770 -439.770 -439.770] (1.0000) ({r_i: None, r_t: [-788.557 -788.557 -788.557], eps: 1.0})
Step:   81500, Reward: [-415.221 -415.221 -415.221] [97.412], Avg: [-439.740 -439.740 -439.740] (1.0000) ({r_i: None, r_t: [-798.646 -798.646 -798.646], eps: 1.0})
Step:   81600, Reward: [-407.766 -407.766 -407.766] [95.291], Avg: [-439.701 -439.701 -439.701] (1.0000) ({r_i: None, r_t: [-866.979 -866.979 -866.979], eps: 1.0})
Step:   81700, Reward: [-395.164 -395.164 -395.164] [69.945], Avg: [-439.647 -439.647 -439.647] (1.0000) ({r_i: None, r_t: [-866.983 -866.983 -866.983], eps: 1.0})
Step:   81800, Reward: [-386.547 -386.547 -386.547] [70.800], Avg: [-439.582 -439.582 -439.582] (1.0000) ({r_i: None, r_t: [-833.609 -833.609 -833.609], eps: 1.0})
Step:   81900, Reward: [-456.270 -456.270 -456.270] [90.900], Avg: [-439.602 -439.602 -439.602] (1.0000) ({r_i: None, r_t: [-832.079 -832.079 -832.079], eps: 1.0})
Step:   82000, Reward: [-434.028 -434.028 -434.028] [86.384], Avg: [-439.595 -439.595 -439.595] (1.0000) ({r_i: None, r_t: [-820.937 -820.937 -820.937], eps: 1.0})
Step:   82100, Reward: [-382.567 -382.567 -382.567] [75.599], Avg: [-439.526 -439.526 -439.526] (1.0000) ({r_i: None, r_t: [-782.462 -782.462 -782.462], eps: 1.0})
Step:   82200, Reward: [-433.409 -433.409 -433.409] [76.779], Avg: [-439.519 -439.519 -439.519] (1.0000) ({r_i: None, r_t: [-851.719 -851.719 -851.719], eps: 1.0})
Step:   82300, Reward: [-397.282 -397.282 -397.282] [62.503], Avg: [-439.467 -439.467 -439.467] (1.0000) ({r_i: None, r_t: [-888.439 -888.439 -888.439], eps: 1.0})
Step:   82400, Reward: [-396.099 -396.099 -396.099] [44.732], Avg: [-439.415 -439.415 -439.415] (1.0000) ({r_i: None, r_t: [-743.759 -743.759 -743.759], eps: 1.0})
Step:   82500, Reward: [-404.318 -404.318 -404.318] [63.240], Avg: [-439.372 -439.372 -439.372] (1.0000) ({r_i: None, r_t: [-825.546 -825.546 -825.546], eps: 1.0})
Step:   82600, Reward: [-382.825 -382.825 -382.825] [56.990], Avg: [-439.304 -439.304 -439.304] (1.0000) ({r_i: None, r_t: [-810.098 -810.098 -810.098], eps: 1.0})
Step:   82700, Reward: [-388.500 -388.500 -388.500] [73.103], Avg: [-439.243 -439.243 -439.243] (1.0000) ({r_i: None, r_t: [-775.158 -775.158 -775.158], eps: 1.0})
Step:   82800, Reward: [-416.546 -416.546 -416.546] [70.971], Avg: [-439.215 -439.215 -439.215] (1.0000) ({r_i: None, r_t: [-836.442 -836.442 -836.442], eps: 1.0})
Step:   82900, Reward: [-382.002 -382.002 -382.002] [52.602], Avg: [-439.146 -439.146 -439.146] (1.0000) ({r_i: None, r_t: [-837.864 -837.864 -837.864], eps: 1.0})
Step:   83000, Reward: [-383.130 -383.130 -383.130] [55.632], Avg: [-439.079 -439.079 -439.079] (1.0000) ({r_i: None, r_t: [-778.834 -778.834 -778.834], eps: 1.0})
Step:   83100, Reward: [-408.430 -408.430 -408.430] [52.680], Avg: [-439.042 -439.042 -439.042] (1.0000) ({r_i: None, r_t: [-822.385 -822.385 -822.385], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		# self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.learner.select_actions(inputs, t_env=None, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True))

class COMALearner():
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0

		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())

		# self.agent_params = list(mac.parameters())
		self.agent_params = list(self.agent.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def select_actions(self, inputs, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(inputs, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, actor_inputs, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_):
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		mac_out = torch.stack([self.forward(batch, actor_inputs[:,t], t) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def parameters(self):
		return self.agent.parameters()

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def forward(self, inputs):
		x = self.fc1(inputs).relu()
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-508.764 -508.764 -508.764] [132.013], Avg: [-508.764 -508.764 -508.764] (1.0000) ({r_i: None, r_t: [-8.529 -8.529 -8.529], eps: 1.0})
Step:   83200, Reward: [-438.546 -438.546 -438.546] [65.048], Avg: [-439.041 -439.041 -439.041] (1.0000) ({r_i: None, r_t: [-815.531 -815.531 -815.531], eps: 1.0})
Step:   83300, Reward: [-413.504 -413.504 -413.504] [74.118], Avg: [-439.011 -439.011 -439.011] (1.0000) ({r_i: None, r_t: [-791.287 -791.287 -791.287], eps: 1.0})
Step:   83400, Reward: [-386.606 -386.606 -386.606] [80.783], Avg: [-438.948 -438.948 -438.948] (1.0000) ({r_i: None, r_t: [-832.352 -832.352 -832.352], eps: 1.0})
Step:   83500, Reward: [-431.843 -431.843 -431.843] [78.298], Avg: [-438.940 -438.940 -438.940] (1.0000) ({r_i: None, r_t: [-795.150 -795.150 -795.150], eps: 1.0})
Step:   83600, Reward: [-443.240 -443.240 -443.240] [51.612], Avg: [-438.945 -438.945 -438.945] (1.0000) ({r_i: None, r_t: [-845.072 -845.072 -845.072], eps: 1.0})
Step:   83700, Reward: [-405.454 -405.454 -405.454] [52.019], Avg: [-438.905 -438.905 -438.905] (1.0000) ({r_i: None, r_t: [-823.892 -823.892 -823.892], eps: 1.0})
Step:   83800, Reward: [-407.098 -407.098 -407.098] [62.470], Avg: [-438.867 -438.867 -438.867] (1.0000) ({r_i: None, r_t: [-823.196 -823.196 -823.196], eps: 1.0})
Step:   83900, Reward: [-380.692 -380.692 -380.692] [55.813], Avg: [-438.798 -438.798 -438.798] (1.0000) ({r_i: None, r_t: [-829.727 -829.727 -829.727], eps: 1.0})
Step:   84000, Reward: [-414.500 -414.500 -414.500] [48.848], Avg: [-438.769 -438.769 -438.769] (1.0000) ({r_i: None, r_t: [-807.656 -807.656 -807.656], eps: 1.0})
Step:   84100, Reward: [-414.460 -414.460 -414.460] [47.938], Avg: [-438.740 -438.740 -438.740] (1.0000) ({r_i: None, r_t: [-806.231 -806.231 -806.231], eps: 1.0})
Step:   84200, Reward: [-398.418 -398.418 -398.418] [65.040], Avg: [-438.692 -438.692 -438.692] (1.0000) ({r_i: None, r_t: [-809.577 -809.577 -809.577], eps: 1.0})
Step:   84300, Reward: [-384.604 -384.604 -384.604] [69.270], Avg: [-438.628 -438.628 -438.628] (1.0000) ({r_i: None, r_t: [-843.544 -843.544 -843.544], eps: 1.0})
Step:   84400, Reward: [-407.017 -407.017 -407.017] [75.975], Avg: [-438.591 -438.591 -438.591] (1.0000) ({r_i: None, r_t: [-861.311 -861.311 -861.311], eps: 1.0})
Step:   84500, Reward: [-399.001 -399.001 -399.001] [64.634], Avg: [-438.544 -438.544 -438.544] (1.0000) ({r_i: None, r_t: [-829.431 -829.431 -829.431], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		# self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.learner.select_actions(inputs, t_env=None, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True))

class COMALearner():
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0

		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())

		# self.agent_params = list(mac.parameters())
		self.agent_params = list(self.agent.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def select_actions(self, inputs, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(inputs, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, actor_inputs, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_):
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		mac_out = torch.stack([self.forward(actor_inputs[:,t]) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def parameters(self):
		return self.agent.parameters()

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def forward(self, inputs):
		x = self.fc1(inputs).relu()
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-470.368 -470.368 -470.368] [77.525], Avg: [-470.368 -470.368 -470.368] (1.0000) ({r_i: None, r_t: [-9.161 -9.161 -9.161], eps: 1.0})
Step:   84600, Reward: [-433.467 -433.467 -433.467] [66.075], Avg: [-438.538 -438.538 -438.538] (1.0000) ({r_i: None, r_t: [-804.340 -804.340 -804.340], eps: 1.0})
Step:   84700, Reward: [-400.689 -400.689 -400.689] [65.945], Avg: [-438.493 -438.493 -438.493] (1.0000) ({r_i: None, r_t: [-814.997 -814.997 -814.997], eps: 1.0})
Step:     100, Reward: [-514.242 -514.242 -514.242] [117.657], Avg: [-492.305 -492.305 -492.305] (1.0000) ({r_i: None, r_t: [-960.543 -960.543 -960.543], eps: 1.0})
Step:   84800, Reward: [-408.399 -408.399 -408.399] [65.940], Avg: [-438.458 -438.458 -438.458] (1.0000) ({r_i: None, r_t: [-813.705 -813.705 -813.705], eps: 1.0})
Step:   84900, Reward: [-411.863 -411.863 -411.863] [78.517], Avg: [-438.426 -438.426 -438.426] (1.0000) ({r_i: None, r_t: [-793.966 -793.966 -793.966], eps: 1.0})
Step:   85000, Reward: [-398.002 -398.002 -398.002] [74.941], Avg: [-438.379 -438.379 -438.379] (1.0000) ({r_i: None, r_t: [-782.368 -782.368 -782.368], eps: 1.0})
Step:   85100, Reward: [-391.485 -391.485 -391.485] [59.471], Avg: [-438.324 -438.324 -438.324] (1.0000) ({r_i: None, r_t: [-798.009 -798.009 -798.009], eps: 1.0})
Step:   85200, Reward: [-400.074 -400.074 -400.074] [58.731], Avg: [-438.279 -438.279 -438.279] (1.0000) ({r_i: None, r_t: [-783.885 -783.885 -783.885], eps: 1.0})
Step:   85300, Reward: [-425.320 -425.320 -425.320] [57.632], Avg: [-438.264 -438.264 -438.264] (1.0000) ({r_i: None, r_t: [-781.034 -781.034 -781.034], eps: 1.0})
Step:   85400, Reward: [-406.089 -406.089 -406.089] [73.996], Avg: [-438.226 -438.226 -438.226] (1.0000) ({r_i: None, r_t: [-772.640 -772.640 -772.640], eps: 1.0})
Step:   85500, Reward: [-400.977 -400.977 -400.977] [70.471], Avg: [-438.183 -438.183 -438.183] (1.0000) ({r_i: None, r_t: [-795.726 -795.726 -795.726], eps: 1.0})
Step:   85600, Reward: [-423.706 -423.706 -423.706] [52.995], Avg: [-438.166 -438.166 -438.166] (1.0000) ({r_i: None, r_t: [-769.863 -769.863 -769.863], eps: 1.0})
Step:   85700, Reward: [-442.853 -442.853 -442.853] [69.657], Avg: [-438.171 -438.171 -438.171] (1.0000) ({r_i: None, r_t: [-803.781 -803.781 -803.781], eps: 1.0})
Step:   85800, Reward: [-400.586 -400.586 -400.586] [72.316], Avg: [-438.127 -438.127 -438.127] (1.0000) ({r_i: None, r_t: [-811.619 -811.619 -811.619], eps: 1.0})
Step:   85900, Reward: [-435.594 -435.594 -435.594] [56.207], Avg: [-438.125 -438.125 -438.125] (1.0000) ({r_i: None, r_t: [-764.341 -764.341 -764.341], eps: 1.0})
Step:   86000, Reward: [-413.830 -413.830 -413.830] [74.799], Avg: [-438.096 -438.096 -438.096] (1.0000) ({r_i: None, r_t: [-801.458 -801.458 -801.458], eps: 1.0})
Step:   86100, Reward: [-405.403 -405.403 -405.403] [46.960], Avg: [-438.058 -438.058 -438.058] (1.0000) ({r_i: None, r_t: [-870.509 -870.509 -870.509], eps: 1.0})
Step:   86200, Reward: [-384.998 -384.998 -384.998] [64.873], Avg: [-437.997 -437.997 -437.997] (1.0000) ({r_i: None, r_t: [-841.663 -841.663 -841.663], eps: 1.0})
Step:   86300, Reward: [-390.264 -390.264 -390.264] [66.073], Avg: [-437.942 -437.942 -437.942] (1.0000) ({r_i: None, r_t: [-816.642 -816.642 -816.642], eps: 1.0})
Step:   86400, Reward: [-444.613 -444.613 -444.613] [67.780], Avg: [-437.949 -437.949 -437.949] (1.0000) ({r_i: None, r_t: [-769.662 -769.662 -769.662], eps: 1.0})
Step:   86500, Reward: [-398.810 -398.810 -398.810] [59.534], Avg: [-437.904 -437.904 -437.904] (1.0000) ({r_i: None, r_t: [-783.128 -783.128 -783.128], eps: 1.0})
Step:   86600, Reward: [-398.041 -398.041 -398.041] [65.561], Avg: [-437.858 -437.858 -437.858] (1.0000) ({r_i: None, r_t: [-789.856 -789.856 -789.856], eps: 1.0})
Step:   86700, Reward: [-383.369 -383.369 -383.369] [63.181], Avg: [-437.795 -437.795 -437.795] (1.0000) ({r_i: None, r_t: [-803.522 -803.522 -803.522], eps: 1.0})
Step:   86800, Reward: [-403.780 -403.780 -403.780] [64.143], Avg: [-437.756 -437.756 -437.756] (1.0000) ({r_i: None, r_t: [-777.990 -777.990 -777.990], eps: 1.0})
Step:   86900, Reward: [-408.744 -408.744 -408.744] [44.548], Avg: [-437.723 -437.723 -437.723] (1.0000) ({r_i: None, r_t: [-805.436 -805.436 -805.436], eps: 1.0})
Step:   87000, Reward: [-374.391 -374.391 -374.391] [59.769], Avg: [-437.650 -437.650 -437.650] (1.0000) ({r_i: None, r_t: [-836.504 -836.504 -836.504], eps: 1.0})
Step:   87100, Reward: [-416.325 -416.325 -416.325] [57.580], Avg: [-437.626 -437.626 -437.626] (1.0000) ({r_i: None, r_t: [-857.008 -857.008 -857.008], eps: 1.0})
Step:   87200, Reward: [-440.157 -440.157 -440.157] [51.250], Avg: [-437.629 -437.629 -437.629] (1.0000) ({r_i: None, r_t: [-827.903 -827.903 -827.903], eps: 1.0})
Step:   87300, Reward: [-417.052 -417.052 -417.052] [80.914], Avg: [-437.605 -437.605 -437.605] (1.0000) ({r_i: None, r_t: [-827.602 -827.602 -827.602], eps: 1.0})
Step:   87400, Reward: [-432.670 -432.670 -432.670] [52.531], Avg: [-437.599 -437.599 -437.599] (1.0000) ({r_i: None, r_t: [-795.680 -795.680 -795.680], eps: 1.0})
Step:   87500, Reward: [-433.291 -433.291 -433.291] [59.672], Avg: [-437.595 -437.595 -437.595] (1.0000) ({r_i: None, r_t: [-809.338 -809.338 -809.338], eps: 1.0})
Step:   87600, Reward: [-382.235 -382.235 -382.235] [51.891], Avg: [-437.531 -437.531 -437.531] (1.0000) ({r_i: None, r_t: [-845.840 -845.840 -845.840], eps: 1.0})
Step:   87700, Reward: [-421.220 -421.220 -421.220] [57.521], Avg: [-437.513 -437.513 -437.513] (1.0000) ({r_i: None, r_t: [-776.887 -776.887 -776.887], eps: 1.0})
Step:   87800, Reward: [-392.892 -392.892 -392.892] [79.498], Avg: [-437.462 -437.462 -437.462] (1.0000) ({r_i: None, r_t: [-828.080 -828.080 -828.080], eps: 1.0})
Step:   87900, Reward: [-431.512 -431.512 -431.512] [62.716], Avg: [-437.455 -437.455 -437.455] (1.0000) ({r_i: None, r_t: [-836.903 -836.903 -836.903], eps: 1.0})
Step:   88000, Reward: [-415.193 -415.193 -415.193] [59.476], Avg: [-437.430 -437.430 -437.430] (1.0000) ({r_i: None, r_t: [-836.696 -836.696 -836.696], eps: 1.0})
Step:   88100, Reward: [-387.884 -387.884 -387.884] [58.481], Avg: [-437.374 -437.374 -437.374] (1.0000) ({r_i: None, r_t: [-827.016 -827.016 -827.016], eps: 1.0})
Step:   88200, Reward: [-438.253 -438.253 -438.253] [77.839], Avg: [-437.375 -437.375 -437.375] (1.0000) ({r_i: None, r_t: [-861.597 -861.597 -861.597], eps: 1.0})
Step:   88300, Reward: [-417.770 -417.770 -417.770] [72.745], Avg: [-437.353 -437.353 -437.353] (1.0000) ({r_i: None, r_t: [-808.995 -808.995 -808.995], eps: 1.0})
Step:   88400, Reward: [-418.500 -418.500 -418.500] [65.481], Avg: [-437.331 -437.331 -437.331] (1.0000) ({r_i: None, r_t: [-859.808 -859.808 -859.808], eps: 1.0})
Step:   88500, Reward: [-421.073 -421.073 -421.073] [49.985], Avg: [-437.313 -437.313 -437.313] (1.0000) ({r_i: None, r_t: [-788.156 -788.156 -788.156], eps: 1.0})
Step:   88600, Reward: [-412.758 -412.758 -412.758] [54.705], Avg: [-437.285 -437.285 -437.285] (1.0000) ({r_i: None, r_t: [-840.265 -840.265 -840.265], eps: 1.0})
Step:   88700, Reward: [-384.294 -384.294 -384.294] [64.478], Avg: [-437.226 -437.226 -437.226] (1.0000) ({r_i: None, r_t: [-851.276 -851.276 -851.276], eps: 1.0})
Step:   88800, Reward: [-384.362 -384.362 -384.362] [73.097], Avg: [-437.166 -437.166 -437.166] (1.0000) ({r_i: None, r_t: [-847.411 -847.411 -847.411], eps: 1.0})
Step:   88900, Reward: [-390.559 -390.559 -390.559] [61.151], Avg: [-437.114 -437.114 -437.114] (1.0000) ({r_i: None, r_t: [-788.097 -788.097 -788.097], eps: 1.0})
Step:   89000, Reward: [-407.659 -407.659 -407.659] [51.756], Avg: [-437.081 -437.081 -437.081] (1.0000) ({r_i: None, r_t: [-840.686 -840.686 -840.686], eps: 1.0})
Step:   89100, Reward: [-443.790 -443.790 -443.790] [76.138], Avg: [-437.088 -437.088 -437.088] (1.0000) ({r_i: None, r_t: [-813.889 -813.889 -813.889], eps: 1.0})
Step:   89200, Reward: [-405.275 -405.275 -405.275] [59.767], Avg: [-437.053 -437.053 -437.053] (1.0000) ({r_i: None, r_t: [-825.459 -825.459 -825.459], eps: 1.0})
Step:   89300, Reward: [-406.532 -406.532 -406.532] [54.757], Avg: [-437.019 -437.019 -437.019] (1.0000) ({r_i: None, r_t: [-835.859 -835.859 -835.859], eps: 1.0})
Step:   89400, Reward: [-412.200 -412.200 -412.200] [73.575], Avg: [-436.991 -436.991 -436.991] (1.0000) ({r_i: None, r_t: [-782.480 -782.480 -782.480], eps: 1.0})
Step:   89500, Reward: [-408.257 -408.257 -408.257] [73.317], Avg: [-436.959 -436.959 -436.959] (1.0000) ({r_i: None, r_t: [-801.214 -801.214 -801.214], eps: 1.0})
Step:   89600, Reward: [-405.841 -405.841 -405.841] [57.271], Avg: [-436.924 -436.924 -436.924] (1.0000) ({r_i: None, r_t: [-779.523 -779.523 -779.523], eps: 1.0})
Step:   89700, Reward: [-386.654 -386.654 -386.654] [61.077], Avg: [-436.868 -436.868 -436.868] (1.0000) ({r_i: None, r_t: [-820.736 -820.736 -820.736], eps: 1.0})
Step:   89800, Reward: [-421.616 -421.616 -421.616] [67.516], Avg: [-436.851 -436.851 -436.851] (1.0000) ({r_i: None, r_t: [-835.486 -835.486 -835.486], eps: 1.0})
Step:   89900, Reward: [-403.449 -403.449 -403.449] [78.611], Avg: [-436.814 -436.814 -436.814] (1.0000) ({r_i: None, r_t: [-814.527 -814.527 -814.527], eps: 1.0})
Step:   90000, Reward: [-428.970 -428.970 -428.970] [80.862], Avg: [-436.805 -436.805 -436.805] (1.0000) ({r_i: None, r_t: [-862.987 -862.987 -862.987], eps: 1.0})
Step:   90100, Reward: [-419.322 -419.322 -419.322] [52.797], Avg: [-436.786 -436.786 -436.786] (1.0000) ({r_i: None, r_t: [-842.812 -842.812 -842.812], eps: 1.0})
Step:   90200, Reward: [-417.401 -417.401 -417.401] [78.469], Avg: [-436.764 -436.764 -436.764] (1.0000) ({r_i: None, r_t: [-836.647 -836.647 -836.647], eps: 1.0})
Step:   90300, Reward: [-416.659 -416.659 -416.659] [66.870], Avg: [-436.742 -436.742 -436.742] (1.0000) ({r_i: None, r_t: [-828.034 -828.034 -828.034], eps: 1.0})
Step:   90400, Reward: [-387.892 -387.892 -387.892] [56.663], Avg: [-436.688 -436.688 -436.688] (1.0000) ({r_i: None, r_t: [-807.412 -807.412 -807.412], eps: 1.0})
Step:   90500, Reward: [-410.325 -410.325 -410.325] [71.189], Avg: [-436.659 -436.659 -436.659] (1.0000) ({r_i: None, r_t: [-820.923 -820.923 -820.923], eps: 1.0})
Step:   90600, Reward: [-432.351 -432.351 -432.351] [80.840], Avg: [-436.654 -436.654 -436.654] (1.0000) ({r_i: None, r_t: [-822.584 -822.584 -822.584], eps: 1.0})
Step:   90700, Reward: [-387.341 -387.341 -387.341] [82.331], Avg: [-436.600 -436.600 -436.600] (1.0000) ({r_i: None, r_t: [-799.826 -799.826 -799.826], eps: 1.0})
Step:   90800, Reward: [-411.474 -411.474 -411.474] [74.418], Avg: [-436.572 -436.572 -436.572] (1.0000) ({r_i: None, r_t: [-841.184 -841.184 -841.184], eps: 1.0})
Step:   90900, Reward: [-391.659 -391.659 -391.659] [64.816], Avg: [-436.523 -436.523 -436.523] (1.0000) ({r_i: None, r_t: [-860.729 -860.729 -860.729], eps: 1.0})
Step:   91000, Reward: [-397.129 -397.129 -397.129] [52.115], Avg: [-436.480 -436.480 -436.480] (1.0000) ({r_i: None, r_t: [-820.253 -820.253 -820.253], eps: 1.0})
Step:   91100, Reward: [-394.954 -394.954 -394.954] [42.247], Avg: [-436.434 -436.434 -436.434] (1.0000) ({r_i: None, r_t: [-802.134 -802.134 -802.134], eps: 1.0})
Step:   91200, Reward: [-417.257 -417.257 -417.257] [74.156], Avg: [-436.413 -436.413 -436.413] (1.0000) ({r_i: None, r_t: [-837.618 -837.618 -837.618], eps: 1.0})
Step:   91300, Reward: [-409.272 -409.272 -409.272] [72.376], Avg: [-436.384 -436.384 -436.384] (1.0000) ({r_i: None, r_t: [-790.761 -790.761 -790.761], eps: 1.0})
Step:   91400, Reward: [-425.281 -425.281 -425.281] [64.733], Avg: [-436.371 -436.371 -436.371] (1.0000) ({r_i: None, r_t: [-803.799 -803.799 -803.799], eps: 1.0})
Step:   91500, Reward: [-417.812 -417.812 -417.812] [48.071], Avg: [-436.351 -436.351 -436.351] (1.0000) ({r_i: None, r_t: [-792.479 -792.479 -792.479], eps: 1.0})
Step:   91600, Reward: [-384.036 -384.036 -384.036] [78.079], Avg: [-436.294 -436.294 -436.294] (1.0000) ({r_i: None, r_t: [-760.487 -760.487 -760.487], eps: 1.0})
Step:   91700, Reward: [-410.302 -410.302 -410.302] [61.530], Avg: [-436.266 -436.266 -436.266] (1.0000) ({r_i: None, r_t: [-807.393 -807.393 -807.393], eps: 1.0})
Step:   91800, Reward: [-405.939 -405.939 -405.939] [85.407], Avg: [-436.233 -436.233 -436.233] (1.0000) ({r_i: None, r_t: [-808.278 -808.278 -808.278], eps: 1.0})
Step:   91900, Reward: [-401.175 -401.175 -401.175] [61.360], Avg: [-436.195 -436.195 -436.195] (1.0000) ({r_i: None, r_t: [-816.315 -816.315 -816.315], eps: 1.0})
Step:   92000, Reward: [-381.412 -381.412 -381.412] [62.545], Avg: [-436.135 -436.135 -436.135] (1.0000) ({r_i: None, r_t: [-803.360 -803.360 -803.360], eps: 1.0})
Step:   92100, Reward: [-418.904 -418.904 -418.904] [90.258], Avg: [-436.117 -436.117 -436.117] (1.0000) ({r_i: None, r_t: [-793.347 -793.347 -793.347], eps: 1.0})
Step:   92200, Reward: [-418.437 -418.437 -418.437] [49.893], Avg: [-436.097 -436.097 -436.097] (1.0000) ({r_i: None, r_t: [-794.918 -794.918 -794.918], eps: 1.0})
Step:   92300, Reward: [-376.941 -376.941 -376.941] [41.141], Avg: [-436.033 -436.033 -436.033] (1.0000) ({r_i: None, r_t: [-815.627 -815.627 -815.627], eps: 1.0})
Step:   92400, Reward: [-414.666 -414.666 -414.666] [52.754], Avg: [-436.010 -436.010 -436.010] (1.0000) ({r_i: None, r_t: [-786.559 -786.559 -786.559], eps: 1.0})
Step:   92500, Reward: [-399.717 -399.717 -399.717] [70.671], Avg: [-435.971 -435.971 -435.971] (1.0000) ({r_i: None, r_t: [-788.369 -788.369 -788.369], eps: 1.0})
Step:   92600, Reward: [-382.017 -382.017 -382.017] [46.525], Avg: [-435.913 -435.913 -435.913] (1.0000) ({r_i: None, r_t: [-796.761 -796.761 -796.761], eps: 1.0})
Step:   92700, Reward: [-408.498 -408.498 -408.498] [64.822], Avg: [-435.883 -435.883 -435.883] (1.0000) ({r_i: None, r_t: [-811.023 -811.023 -811.023], eps: 1.0})
Step:   92800, Reward: [-379.673 -379.673 -379.673] [44.552], Avg: [-435.823 -435.823 -435.823] (1.0000) ({r_i: None, r_t: [-842.948 -842.948 -842.948], eps: 1.0})
Step:   92900, Reward: [-412.669 -412.669 -412.669] [66.479], Avg: [-435.798 -435.798 -435.798] (1.0000) ({r_i: None, r_t: [-787.734 -787.734 -787.734], eps: 1.0})
Step:   93000, Reward: [-412.140 -412.140 -412.140] [65.016], Avg: [-435.773 -435.773 -435.773] (1.0000) ({r_i: None, r_t: [-775.396 -775.396 -775.396], eps: 1.0})
Step:   93100, Reward: [-388.511 -388.511 -388.511] [63.123], Avg: [-435.722 -435.722 -435.722] (1.0000) ({r_i: None, r_t: [-810.407 -810.407 -810.407], eps: 1.0})
Step:   93200, Reward: [-403.002 -403.002 -403.002] [83.187], Avg: [-435.687 -435.687 -435.687] (1.0000) ({r_i: None, r_t: [-838.740 -838.740 -838.740], eps: 1.0})
Step:   93300, Reward: [-392.960 -392.960 -392.960] [58.879], Avg: [-435.641 -435.641 -435.641] (1.0000) ({r_i: None, r_t: [-768.530 -768.530 -768.530], eps: 1.0})
Step:   93400, Reward: [-399.292 -399.292 -399.292] [71.406], Avg: [-435.602 -435.602 -435.602] (1.0000) ({r_i: None, r_t: [-730.590 -730.590 -730.590], eps: 1.0})
Step:   93500, Reward: [-373.850 -373.850 -373.850] [79.531], Avg: [-435.536 -435.536 -435.536] (1.0000) ({r_i: None, r_t: [-781.209 -781.209 -781.209], eps: 1.0})
Step:   93600, Reward: [-364.013 -364.013 -364.013] [67.258], Avg: [-435.460 -435.460 -435.460] (1.0000) ({r_i: None, r_t: [-755.025 -755.025 -755.025], eps: 1.0})
Step:   93700, Reward: [-412.660 -412.660 -412.660] [78.868], Avg: [-435.436 -435.436 -435.436] (1.0000) ({r_i: None, r_t: [-793.690 -793.690 -793.690], eps: 1.0})
Step:   93800, Reward: [-367.919 -367.919 -367.919] [55.384], Avg: [-435.364 -435.364 -435.364] (1.0000) ({r_i: None, r_t: [-776.300 -776.300 -776.300], eps: 1.0})
Step:   93900, Reward: [-369.566 -369.566 -369.566] [63.261], Avg: [-435.294 -435.294 -435.294] (1.0000) ({r_i: None, r_t: [-776.853 -776.853 -776.853], eps: 1.0})
Step:   94000, Reward: [-404.145 -404.145 -404.145] [48.387], Avg: [-435.261 -435.261 -435.261] (1.0000) ({r_i: None, r_t: [-765.514 -765.514 -765.514], eps: 1.0})
Step:   94100, Reward: [-399.069 -399.069 -399.069] [45.819], Avg: [-435.222 -435.222 -435.222] (1.0000) ({r_i: None, r_t: [-784.230 -784.230 -784.230], eps: 1.0})
Step:   94200, Reward: [-407.458 -407.458 -407.458] [78.212], Avg: [-435.193 -435.193 -435.193] (1.0000) ({r_i: None, r_t: [-789.215 -789.215 -789.215], eps: 1.0})
Step:   94300, Reward: [-406.006 -406.006 -406.006] [73.468], Avg: [-435.162 -435.162 -435.162] (1.0000) ({r_i: None, r_t: [-798.009 -798.009 -798.009], eps: 1.0})
Step:   94400, Reward: [-365.042 -365.042 -365.042] [69.538], Avg: [-435.088 -435.088 -435.088] (1.0000) ({r_i: None, r_t: [-781.519 -781.519 -781.519], eps: 1.0})
Step:   94500, Reward: [-411.136 -411.136 -411.136] [72.887], Avg: [-435.062 -435.062 -435.062] (1.0000) ({r_i: None, r_t: [-794.344 -794.344 -794.344], eps: 1.0})
Step:   94600, Reward: [-377.450 -377.450 -377.450] [61.879], Avg: [-435.001 -435.001 -435.001] (1.0000) ({r_i: None, r_t: [-831.759 -831.759 -831.759], eps: 1.0})
Step:   94700, Reward: [-407.522 -407.522 -407.522] [56.485], Avg: [-434.972 -434.972 -434.972] (1.0000) ({r_i: None, r_t: [-770.806 -770.806 -770.806], eps: 1.0})
Step:   94800, Reward: [-376.709 -376.709 -376.709] [69.278], Avg: [-434.911 -434.911 -434.911] (1.0000) ({r_i: None, r_t: [-765.403 -765.403 -765.403], eps: 1.0})
Step:   94900, Reward: [-356.201 -356.201 -356.201] [69.417], Avg: [-434.828 -434.828 -434.828] (1.0000) ({r_i: None, r_t: [-757.973 -757.973 -757.973], eps: 1.0})
Step:   95000, Reward: [-401.713 -401.713 -401.713] [71.787], Avg: [-434.793 -434.793 -434.793] (1.0000) ({r_i: None, r_t: [-740.320 -740.320 -740.320], eps: 1.0})
Step:   95100, Reward: [-399.176 -399.176 -399.176] [70.768], Avg: [-434.756 -434.756 -434.756] (1.0000) ({r_i: None, r_t: [-783.731 -783.731 -783.731], eps: 1.0})
Step:   95200, Reward: [-390.331 -390.331 -390.331] [61.175], Avg: [-434.709 -434.709 -434.709] (1.0000) ({r_i: None, r_t: [-789.969 -789.969 -789.969], eps: 1.0})
Step:   95300, Reward: [-387.989 -387.989 -387.989] [69.891], Avg: [-434.660 -434.660 -434.660] (1.0000) ({r_i: None, r_t: [-779.532 -779.532 -779.532], eps: 1.0})
Step:   95400, Reward: [-385.970 -385.970 -385.970] [85.804], Avg: [-434.609 -434.609 -434.609] (1.0000) ({r_i: None, r_t: [-829.106 -829.106 -829.106], eps: 1.0})
Step:   95500, Reward: [-421.980 -421.980 -421.980] [66.557], Avg: [-434.596 -434.596 -434.596] (1.0000) ({r_i: None, r_t: [-825.851 -825.851 -825.851], eps: 1.0})
Step:   95600, Reward: [-400.515 -400.515 -400.515] [68.814], Avg: [-434.561 -434.561 -434.561] (1.0000) ({r_i: None, r_t: [-819.646 -819.646 -819.646], eps: 1.0})
Step:   95700, Reward: [-374.043 -374.043 -374.043] [65.657], Avg: [-434.497 -434.497 -434.497] (1.0000) ({r_i: None, r_t: [-773.748 -773.748 -773.748], eps: 1.0})
Step:   95800, Reward: [-395.099 -395.099 -395.099] [69.079], Avg: [-434.456 -434.456 -434.456] (1.0000) ({r_i: None, r_t: [-760.344 -760.344 -760.344], eps: 1.0})
Step:   95900, Reward: [-385.022 -385.022 -385.022] [71.356], Avg: [-434.405 -434.405 -434.405] (1.0000) ({r_i: None, r_t: [-815.317 -815.317 -815.317], eps: 1.0})
Step:   96000, Reward: [-391.965 -391.965 -391.965] [79.998], Avg: [-434.361 -434.361 -434.361] (1.0000) ({r_i: None, r_t: [-815.928 -815.928 -815.928], eps: 1.0})
Step:   96100, Reward: [-390.540 -390.540 -390.540] [74.913], Avg: [-434.315 -434.315 -434.315] (1.0000) ({r_i: None, r_t: [-778.345 -778.345 -778.345], eps: 1.0})
Step:   96200, Reward: [-424.819 -424.819 -424.819] [53.538], Avg: [-434.305 -434.305 -434.305] (1.0000) ({r_i: None, r_t: [-816.216 -816.216 -816.216], eps: 1.0})
Step:   96300, Reward: [-398.410 -398.410 -398.410] [79.901], Avg: [-434.268 -434.268 -434.268] (1.0000) ({r_i: None, r_t: [-837.543 -837.543 -837.543], eps: 1.0})
Step:   96400, Reward: [-385.830 -385.830 -385.830] [62.362], Avg: [-434.218 -434.218 -434.218] (1.0000) ({r_i: None, r_t: [-832.509 -832.509 -832.509], eps: 1.0})
Step:   96500, Reward: [-404.269 -404.269 -404.269] [62.802], Avg: [-434.187 -434.187 -434.187] (1.0000) ({r_i: None, r_t: [-872.469 -872.469 -872.469], eps: 1.0})
Step:   96600, Reward: [-381.766 -381.766 -381.766] [66.862], Avg: [-434.133 -434.133 -434.133] (1.0000) ({r_i: None, r_t: [-736.758 -736.758 -736.758], eps: 1.0})
Step:   96700, Reward: [-397.128 -397.128 -397.128] [27.832], Avg: [-434.094 -434.094 -434.094] (1.0000) ({r_i: None, r_t: [-849.936 -849.936 -849.936], eps: 1.0})
Step:   96800, Reward: [-412.199 -412.199 -412.199] [80.489], Avg: [-434.072 -434.072 -434.072] (1.0000) ({r_i: None, r_t: [-750.784 -750.784 -750.784], eps: 1.0})
Step:   96900, Reward: [-357.617 -357.617 -357.617] [70.857], Avg: [-433.993 -433.993 -433.993] (1.0000) ({r_i: None, r_t: [-766.480 -766.480 -766.480], eps: 1.0})
Step:   97000, Reward: [-422.418 -422.418 -422.418] [61.360], Avg: [-433.981 -433.981 -433.981] (1.0000) ({r_i: None, r_t: [-762.199 -762.199 -762.199], eps: 1.0})
Step:   97100, Reward: [-363.705 -363.705 -363.705] [47.906], Avg: [-433.909 -433.909 -433.909] (1.0000) ({r_i: None, r_t: [-763.393 -763.393 -763.393], eps: 1.0})
Step:   97200, Reward: [-402.449 -402.449 -402.449] [49.371], Avg: [-433.876 -433.876 -433.876] (1.0000) ({r_i: None, r_t: [-803.940 -803.940 -803.940], eps: 1.0})
Step:   97300, Reward: [-375.557 -375.557 -375.557] [77.420], Avg: [-433.816 -433.816 -433.816] (1.0000) ({r_i: None, r_t: [-773.238 -773.238 -773.238], eps: 1.0})
Step:   97400, Reward: [-383.837 -383.837 -383.837] [62.290], Avg: [-433.765 -433.765 -433.765] (1.0000) ({r_i: None, r_t: [-779.421 -779.421 -779.421], eps: 1.0})
Step:   97500, Reward: [-403.198 -403.198 -403.198] [85.186], Avg: [-433.734 -433.734 -433.734] (1.0000) ({r_i: None, r_t: [-822.360 -822.360 -822.360], eps: 1.0})
Step:   97600, Reward: [-395.522 -395.522 -395.522] [59.684], Avg: [-433.695 -433.695 -433.695] (1.0000) ({r_i: None, r_t: [-775.725 -775.725 -775.725], eps: 1.0})
Step:   97700, Reward: [-410.697 -410.697 -410.697] [74.239], Avg: [-433.671 -433.671 -433.671] (1.0000) ({r_i: None, r_t: [-768.800 -768.800 -768.800], eps: 1.0})
Step:   97800, Reward: [-408.376 -408.376 -408.376] [86.989], Avg: [-433.645 -433.645 -433.645] (1.0000) ({r_i: None, r_t: [-811.215 -811.215 -811.215], eps: 1.0})
Step:   97900, Reward: [-390.935 -390.935 -390.935] [48.830], Avg: [-433.602 -433.602 -433.602] (1.0000) ({r_i: None, r_t: [-806.419 -806.419 -806.419], eps: 1.0})
Step:   98000, Reward: [-395.748 -395.748 -395.748] [69.353], Avg: [-433.563 -433.563 -433.563] (1.0000) ({r_i: None, r_t: [-788.427 -788.427 -788.427], eps: 1.0})
Step:   98100, Reward: [-390.564 -390.564 -390.564] [47.900], Avg: [-433.519 -433.519 -433.519] (1.0000) ({r_i: None, r_t: [-769.584 -769.584 -769.584], eps: 1.0})
Step:   98200, Reward: [-387.774 -387.774 -387.774] [40.847], Avg: [-433.473 -433.473 -433.473] (1.0000) ({r_i: None, r_t: [-833.223 -833.223 -833.223], eps: 1.0})
Step:   98300, Reward: [-404.760 -404.760 -404.760] [51.359], Avg: [-433.444 -433.444 -433.444] (1.0000) ({r_i: None, r_t: [-766.402 -766.402 -766.402], eps: 1.0})
Step:   98400, Reward: [-412.245 -412.245 -412.245] [62.436], Avg: [-433.422 -433.422 -433.422] (1.0000) ({r_i: None, r_t: [-806.696 -806.696 -806.696], eps: 1.0})
Step:   98500, Reward: [-353.636 -353.636 -353.636] [67.339], Avg: [-433.341 -433.341 -433.341] (1.0000) ({r_i: None, r_t: [-819.799 -819.799 -819.799], eps: 1.0})
Step:   98600, Reward: [-380.302 -380.302 -380.302] [61.198], Avg: [-433.288 -433.288 -433.288] (1.0000) ({r_i: None, r_t: [-843.662 -843.662 -843.662], eps: 1.0})
Step:   98700, Reward: [-392.280 -392.280 -392.280] [58.800], Avg: [-433.246 -433.246 -433.246] (1.0000) ({r_i: None, r_t: [-786.961 -786.961 -786.961], eps: 1.0})
Step:   98800, Reward: [-400.321 -400.321 -400.321] [74.092], Avg: [-433.213 -433.213 -433.213] (1.0000) ({r_i: None, r_t: [-811.443 -811.443 -811.443], eps: 1.0})
Step:   98900, Reward: [-413.357 -413.357 -413.357] [62.271], Avg: [-433.193 -433.193 -433.193] (1.0000) ({r_i: None, r_t: [-770.114 -770.114 -770.114], eps: 1.0})
Step:   99000, Reward: [-389.958 -389.958 -389.958] [54.833], Avg: [-433.149 -433.149 -433.149] (1.0000) ({r_i: None, r_t: [-759.885 -759.885 -759.885], eps: 1.0})
Step:   99100, Reward: [-381.062 -381.062 -381.062] [47.192], Avg: [-433.097 -433.097 -433.097] (1.0000) ({r_i: None, r_t: [-850.579 -850.579 -850.579], eps: 1.0})
Step:   99200, Reward: [-414.458 -414.458 -414.458] [60.937], Avg: [-433.078 -433.078 -433.078] (1.0000) ({r_i: None, r_t: [-799.577 -799.577 -799.577], eps: 1.0})
Step:   99300, Reward: [-396.748 -396.748 -396.748] [62.785], Avg: [-433.041 -433.041 -433.041] (1.0000) ({r_i: None, r_t: [-793.101 -793.101 -793.101], eps: 1.0})
Step:   99400, Reward: [-426.849 -426.849 -426.849] [77.902], Avg: [-433.035 -433.035 -433.035] (1.0000) ({r_i: None, r_t: [-818.229 -818.229 -818.229], eps: 1.0})
Step:   99500, Reward: [-377.505 -377.505 -377.505] [56.824], Avg: [-432.979 -432.979 -432.979] (1.0000) ({r_i: None, r_t: [-794.795 -794.795 -794.795], eps: 1.0})
Step:   99600, Reward: [-404.876 -404.876 -404.876] [58.061], Avg: [-432.951 -432.951 -432.951] (1.0000) ({r_i: None, r_t: [-796.152 -796.152 -796.152], eps: 1.0})
Step:   99700, Reward: [-397.568 -397.568 -397.568] [49.870], Avg: [-432.916 -432.916 -432.916] (1.0000) ({r_i: None, r_t: [-767.022 -767.022 -767.022], eps: 1.0})
Step:   99800, Reward: [-419.522 -419.522 -419.522] [52.604], Avg: [-432.902 -432.902 -432.902] (1.0000) ({r_i: None, r_t: [-809.639 -809.639 -809.639], eps: 1.0})
Step:   99900, Reward: [-403.266 -403.266 -403.266] [53.694], Avg: [-432.873 -432.873 -432.873] (1.0000) ({r_i: None, r_t: [-788.932 -788.932 -788.932], eps: 1.0})
Step:  100000, Reward: [-397.860 -397.860 -397.860] [58.694], Avg: [-432.838 -432.838 -432.838] (1.0000) ({r_i: None, r_t: [-782.168 -782.168 -782.168], eps: 1.0})
Step:  100100, Reward: [-421.248 -421.248 -421.248] [71.344], Avg: [-432.826 -432.826 -432.826] (1.0000) ({r_i: None, r_t: [-813.227 -813.227 -813.227], eps: 1.0})
Step:  100200, Reward: [-386.448 -386.448 -386.448] [58.579], Avg: [-432.780 -432.780 -432.780] (1.0000) ({r_i: None, r_t: [-850.599 -850.599 -850.599], eps: 1.0})
Step:  100300, Reward: [-421.623 -421.623 -421.623] [75.907], Avg: [-432.769 -432.769 -432.769] (1.0000) ({r_i: None, r_t: [-842.159 -842.159 -842.159], eps: 1.0})
Step:  100400, Reward: [-400.811 -400.811 -400.811] [55.366], Avg: [-432.737 -432.737 -432.737] (1.0000) ({r_i: None, r_t: [-794.649 -794.649 -794.649], eps: 1.0})
Step:  100500, Reward: [-405.076 -405.076 -405.076] [64.283], Avg: [-432.709 -432.709 -432.709] (1.0000) ({r_i: None, r_t: [-813.132 -813.132 -813.132], eps: 1.0})
Step:  100600, Reward: [-382.147 -382.147 -382.147] [47.738], Avg: [-432.659 -432.659 -432.659] (1.0000) ({r_i: None, r_t: [-752.015 -752.015 -752.015], eps: 1.0})
Step:  100700, Reward: [-410.571 -410.571 -410.571] [65.726], Avg: [-432.637 -432.637 -432.637] (1.0000) ({r_i: None, r_t: [-780.606 -780.606 -780.606], eps: 1.0})
Step:  100800, Reward: [-416.036 -416.036 -416.036] [49.815], Avg: [-432.621 -432.621 -432.621] (1.0000) ({r_i: None, r_t: [-773.214 -773.214 -773.214], eps: 1.0})
Step:  100900, Reward: [-408.086 -408.086 -408.086] [42.964], Avg: [-432.597 -432.597 -432.597] (1.0000) ({r_i: None, r_t: [-816.725 -816.725 -816.725], eps: 1.0})
Step:  101000, Reward: [-405.260 -405.260 -405.260] [68.070], Avg: [-432.570 -432.570 -432.570] (1.0000) ({r_i: None, r_t: [-829.274 -829.274 -829.274], eps: 1.0})
Step:  101100, Reward: [-410.311 -410.311 -410.311] [64.844], Avg: [-432.548 -432.548 -432.548] (1.0000) ({r_i: None, r_t: [-800.956 -800.956 -800.956], eps: 1.0})
Step:  101200, Reward: [-404.037 -404.037 -404.037] [71.228], Avg: [-432.519 -432.519 -432.519] (1.0000) ({r_i: None, r_t: [-734.578 -734.578 -734.578], eps: 1.0})
Step:  101300, Reward: [-389.719 -389.719 -389.719] [60.253], Avg: [-432.477 -432.477 -432.477] (1.0000) ({r_i: None, r_t: [-804.883 -804.883 -804.883], eps: 1.0})
Step:  101400, Reward: [-401.330 -401.330 -401.330] [77.902], Avg: [-432.446 -432.446 -432.446] (1.0000) ({r_i: None, r_t: [-809.105 -809.105 -809.105], eps: 1.0})
Step:  101500, Reward: [-391.778 -391.778 -391.778] [72.947], Avg: [-432.406 -432.406 -432.406] (1.0000) ({r_i: None, r_t: [-801.662 -801.662 -801.662], eps: 1.0})
Step:  101600, Reward: [-385.462 -385.462 -385.462] [55.827], Avg: [-432.360 -432.360 -432.360] (1.0000) ({r_i: None, r_t: [-761.163 -761.163 -761.163], eps: 1.0})
Step:  101700, Reward: [-384.919 -384.919 -384.919] [70.230], Avg: [-432.314 -432.314 -432.314] (1.0000) ({r_i: None, r_t: [-760.139 -760.139 -760.139], eps: 1.0})
Step:  101800, Reward: [-403.067 -403.067 -403.067] [60.559], Avg: [-432.285 -432.285 -432.285] (1.0000) ({r_i: None, r_t: [-853.768 -853.768 -853.768], eps: 1.0})
Step:  101900, Reward: [-381.637 -381.637 -381.637] [58.413], Avg: [-432.235 -432.235 -432.235] (1.0000) ({r_i: None, r_t: [-837.989 -837.989 -837.989], eps: 1.0})
Step:  102000, Reward: [-417.344 -417.344 -417.344] [73.944], Avg: [-432.221 -432.221 -432.221] (1.0000) ({r_i: None, r_t: [-825.180 -825.180 -825.180], eps: 1.0})
Step:  102100, Reward: [-346.463 -346.463 -346.463] [60.848], Avg: [-432.137 -432.137 -432.137] (1.0000) ({r_i: None, r_t: [-780.523 -780.523 -780.523], eps: 1.0})
Step:  102200, Reward: [-424.260 -424.260 -424.260] [60.676], Avg: [-432.129 -432.129 -432.129] (1.0000) ({r_i: None, r_t: [-805.474 -805.474 -805.474], eps: 1.0})
Step:  102300, Reward: [-419.908 -419.908 -419.908] [86.961], Avg: [-432.117 -432.117 -432.117] (1.0000) ({r_i: None, r_t: [-802.758 -802.758 -802.758], eps: 1.0})
Step:  102400, Reward: [-422.901 -422.901 -422.901] [102.885], Avg: [-432.108 -432.108 -432.108] (1.0000) ({r_i: None, r_t: [-797.824 -797.824 -797.824], eps: 1.0})
Step:  102500, Reward: [-354.602 -354.602 -354.602] [66.752], Avg: [-432.033 -432.033 -432.033] (1.0000) ({r_i: None, r_t: [-757.373 -757.373 -757.373], eps: 1.0})
Step:  102600, Reward: [-356.441 -356.441 -356.441] [44.851], Avg: [-431.959 -431.959 -431.959] (1.0000) ({r_i: None, r_t: [-737.553 -737.553 -737.553], eps: 1.0})
Step:  102700, Reward: [-382.844 -382.844 -382.844] [69.210], Avg: [-431.911 -431.911 -431.911] (1.0000) ({r_i: None, r_t: [-778.248 -778.248 -778.248], eps: 1.0})
Step:  102800, Reward: [-388.505 -388.505 -388.505] [72.273], Avg: [-431.869 -431.869 -431.869] (1.0000) ({r_i: None, r_t: [-769.303 -769.303 -769.303], eps: 1.0})
Step:  102900, Reward: [-357.287 -357.287 -357.287] [54.028], Avg: [-431.797 -431.797 -431.797] (1.0000) ({r_i: None, r_t: [-726.979 -726.979 -726.979], eps: 1.0})
Step:  103000, Reward: [-374.926 -374.926 -374.926] [64.975], Avg: [-431.742 -431.742 -431.742] (1.0000) ({r_i: None, r_t: [-741.467 -741.467 -741.467], eps: 1.0})
Step:  103100, Reward: [-379.937 -379.937 -379.937] [71.667], Avg: [-431.691 -431.691 -431.691] (1.0000) ({r_i: None, r_t: [-761.078 -761.078 -761.078], eps: 1.0})
Step:  103200, Reward: [-406.112 -406.112 -406.112] [76.864], Avg: [-431.667 -431.667 -431.667] (1.0000) ({r_i: None, r_t: [-779.695 -779.695 -779.695], eps: 1.0})
Step:  103300, Reward: [-361.237 -361.237 -361.237] [41.775], Avg: [-431.598 -431.598 -431.598] (1.0000) ({r_i: None, r_t: [-854.468 -854.468 -854.468], eps: 1.0})
Step:  103400, Reward: [-376.955 -376.955 -376.955] [60.586], Avg: [-431.546 -431.546 -431.546] (1.0000) ({r_i: None, r_t: [-777.584 -777.584 -777.584], eps: 1.0})
Step:  103500, Reward: [-406.217 -406.217 -406.217] [73.562], Avg: [-431.521 -431.521 -431.521] (1.0000) ({r_i: None, r_t: [-759.609 -759.609 -759.609], eps: 1.0})
Step:  103600, Reward: [-361.301 -361.301 -361.301] [74.226], Avg: [-431.453 -431.453 -431.453] (1.0000) ({r_i: None, r_t: [-792.156 -792.156 -792.156], eps: 1.0})
Step:  103700, Reward: [-392.899 -392.899 -392.899] [52.935], Avg: [-431.416 -431.416 -431.416] (1.0000) ({r_i: None, r_t: [-779.812 -779.812 -779.812], eps: 1.0})
Step:  103800, Reward: [-392.185 -392.185 -392.185] [62.787], Avg: [-431.379 -431.379 -431.379] (1.0000) ({r_i: None, r_t: [-758.750 -758.750 -758.750], eps: 1.0})
Step:  103900, Reward: [-370.939 -370.939 -370.939] [54.032], Avg: [-431.320 -431.320 -431.320] (1.0000) ({r_i: None, r_t: [-785.665 -785.665 -785.665], eps: 1.0})
Step:  104000, Reward: [-403.858 -403.858 -403.858] [88.289], Avg: [-431.294 -431.294 -431.294] (1.0000) ({r_i: None, r_t: [-790.383 -790.383 -790.383], eps: 1.0})
Step:  104100, Reward: [-376.851 -376.851 -376.851] [58.630], Avg: [-431.242 -431.242 -431.242] (1.0000) ({r_i: None, r_t: [-754.499 -754.499 -754.499], eps: 1.0})
Step:  104200, Reward: [-424.772 -424.772 -424.772] [76.237], Avg: [-431.236 -431.236 -431.236] (1.0000) ({r_i: None, r_t: [-753.599 -753.599 -753.599], eps: 1.0})
Step:  104300, Reward: [-413.826 -413.826 -413.826] [64.641], Avg: [-431.219 -431.219 -431.219] (1.0000) ({r_i: None, r_t: [-725.404 -725.404 -725.404], eps: 1.0})
Step:  104400, Reward: [-383.775 -383.775 -383.775] [76.252], Avg: [-431.174 -431.174 -431.174] (1.0000) ({r_i: None, r_t: [-764.840 -764.840 -764.840], eps: 1.0})
Step:  104500, Reward: [-370.151 -370.151 -370.151] [72.331], Avg: [-431.115 -431.115 -431.115] (1.0000) ({r_i: None, r_t: [-774.131 -774.131 -774.131], eps: 1.0})
Step:  104600, Reward: [-406.382 -406.382 -406.382] [74.235], Avg: [-431.092 -431.092 -431.092] (1.0000) ({r_i: None, r_t: [-764.322 -764.322 -764.322], eps: 1.0})
Step:  104700, Reward: [-405.957 -405.957 -405.957] [62.916], Avg: [-431.068 -431.068 -431.068] (1.0000) ({r_i: None, r_t: [-750.685 -750.685 -750.685], eps: 1.0})
Step:  104800, Reward: [-365.171 -365.171 -365.171] [78.087], Avg: [-431.005 -431.005 -431.005] (1.0000) ({r_i: None, r_t: [-755.495 -755.495 -755.495], eps: 1.0})
Step:  104900, Reward: [-382.042 -382.042 -382.042] [40.211], Avg: [-430.958 -430.958 -430.958] (1.0000) ({r_i: None, r_t: [-834.333 -834.333 -834.333], eps: 1.0})
Step:  105000, Reward: [-408.655 -408.655 -408.655] [73.875], Avg: [-430.937 -430.937 -430.937] (1.0000) ({r_i: None, r_t: [-722.534 -722.534 -722.534], eps: 1.0})
Step:  105100, Reward: [-401.292 -401.292 -401.292] [73.199], Avg: [-430.909 -430.909 -430.909] (1.0000) ({r_i: None, r_t: [-795.279 -795.279 -795.279], eps: 1.0})
Step:  105200, Reward: [-357.105 -357.105 -357.105] [74.242], Avg: [-430.839 -430.839 -430.839] (1.0000) ({r_i: None, r_t: [-787.067 -787.067 -787.067], eps: 1.0})
Step:  105300, Reward: [-374.993 -374.993 -374.993] [81.900], Avg: [-430.786 -430.786 -430.786] (1.0000) ({r_i: None, r_t: [-794.877 -794.877 -794.877], eps: 1.0})
Step:  105400, Reward: [-395.869 -395.869 -395.869] [86.543], Avg: [-430.753 -430.753 -430.753] (1.0000) ({r_i: None, r_t: [-829.528 -829.528 -829.528], eps: 1.0})
Step:  105500, Reward: [-404.656 -404.656 -404.656] [79.778], Avg: [-430.728 -430.728 -430.728] (1.0000) ({r_i: None, r_t: [-779.265 -779.265 -779.265], eps: 1.0})
Step:  105600, Reward: [-415.530 -415.530 -415.530] [79.660], Avg: [-430.714 -430.714 -430.714] (1.0000) ({r_i: None, r_t: [-770.648 -770.648 -770.648], eps: 1.0})
Step:  105700, Reward: [-399.948 -399.948 -399.948] [69.427], Avg: [-430.684 -430.684 -430.684] (1.0000) ({r_i: None, r_t: [-787.045 -787.045 -787.045], eps: 1.0})
Step:  105800, Reward: [-380.774 -380.774 -380.774] [82.247], Avg: [-430.637 -430.637 -430.637] (1.0000) ({r_i: None, r_t: [-739.739 -739.739 -739.739], eps: 1.0})
Step:  105900, Reward: [-388.634 -388.634 -388.634] [61.840], Avg: [-430.598 -430.598 -430.598] (1.0000) ({r_i: None, r_t: [-775.548 -775.548 -775.548], eps: 1.0})
Step:  106000, Reward: [-423.389 -423.389 -423.389] [69.434], Avg: [-430.591 -430.591 -430.591] (1.0000) ({r_i: None, r_t: [-811.358 -811.358 -811.358], eps: 1.0})
Step:  106100, Reward: [-382.658 -382.658 -382.658] [79.427], Avg: [-430.546 -430.546 -430.546] (1.0000) ({r_i: None, r_t: [-815.254 -815.254 -815.254], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		# self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.learner.select_actions(inputs, t_env=None, eps=self.eps, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True), self.eps)
			
			self.eps = max(self.eps * self.decay, EPS_MIN)

class COMALearner():
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0

		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		# self.action_selector = MultinomialActionSelector()

		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())

		# self.agent_params = list(mac.parameters())
		self.agent_params = list(self.agent.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def select_actions(self, inputs, t_env, eps, test_mode=False):
		agent_outputs = self.forward(inputs, eps, test_mode=test_mode)
		# chosen_actions = self.action_selector.select_action(agent_outputs, t_env, test_mode=test_mode)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/epsilon_action_num)
		return agent_outs

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_, eps):
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def parameters(self):
		return self.agent.parameters()

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def forward(self, inputs):
		x = self.fc1(inputs).relu()
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-489.951 -489.951 -489.951] [70.124], Avg: [-489.951 -489.951 -489.951] (1.0000) ({r_i: None, r_t: [-9.426 -9.426 -9.426], eps: 1.0})
Step:  106200, Reward: [-391.790 -391.790 -391.790] [58.677], Avg: [-430.509 -430.509 -430.509] (1.0000) ({r_i: None, r_t: [-831.330 -831.330 -831.330], eps: 1.0})
Step:  106300, Reward: [-421.298 -421.298 -421.298] [56.254], Avg: [-430.501 -430.501 -430.501] (1.0000) ({r_i: None, r_t: [-773.584 -773.584 -773.584], eps: 1.0})
Step:  106400, Reward: [-392.372 -392.372 -392.372] [83.484], Avg: [-430.465 -430.465 -430.465] (1.0000) ({r_i: None, r_t: [-799.180 -799.180 -799.180], eps: 1.0})
Step:  106500, Reward: [-402.823 -402.823 -402.823] [77.662], Avg: [-430.439 -430.439 -430.439] (1.0000) ({r_i: None, r_t: [-748.717 -748.717 -748.717], eps: 1.0})
Step:  106600, Reward: [-399.132 -399.132 -399.132] [79.006], Avg: [-430.410 -430.410 -430.410] (1.0000) ({r_i: None, r_t: [-806.868 -806.868 -806.868], eps: 1.0})
Step:  106700, Reward: [-416.353 -416.353 -416.353] [68.530], Avg: [-430.396 -430.396 -430.396] (1.0000) ({r_i: None, r_t: [-813.727 -813.727 -813.727], eps: 1.0})
Step:  106800, Reward: [-408.246 -408.246 -408.246] [88.887], Avg: [-430.376 -430.376 -430.376] (1.0000) ({r_i: None, r_t: [-791.016 -791.016 -791.016], eps: 1.0})
Step:  106900, Reward: [-439.849 -439.849 -439.849] [70.685], Avg: [-430.385 -430.385 -430.385] (1.0000) ({r_i: None, r_t: [-785.950 -785.950 -785.950], eps: 1.0})
Step:  107000, Reward: [-400.249 -400.249 -400.249] [83.855], Avg: [-430.356 -430.356 -430.356] (1.0000) ({r_i: None, r_t: [-755.398 -755.398 -755.398], eps: 1.0})
Step:  107100, Reward: [-466.180 -466.180 -466.180] [83.205], Avg: [-430.390 -430.390 -430.390] (1.0000) ({r_i: None, r_t: [-840.913 -840.913 -840.913], eps: 1.0})
Step:  107200, Reward: [-397.995 -397.995 -397.995] [75.347], Avg: [-430.360 -430.360 -430.360] (1.0000) ({r_i: None, r_t: [-850.257 -850.257 -850.257], eps: 1.0})
Step:  107300, Reward: [-396.206 -396.206 -396.206] [87.551], Avg: [-430.328 -430.328 -430.328] (1.0000) ({r_i: None, r_t: [-775.628 -775.628 -775.628], eps: 1.0})
Step:  107400, Reward: [-403.777 -403.777 -403.777] [90.067], Avg: [-430.303 -430.303 -430.303] (1.0000) ({r_i: None, r_t: [-870.365 -870.365 -870.365], eps: 1.0})
Step:  107500, Reward: [-402.234 -402.234 -402.234] [72.560], Avg: [-430.277 -430.277 -430.277] (1.0000) ({r_i: None, r_t: [-829.755 -829.755 -829.755], eps: 1.0})
Step:  107600, Reward: [-403.330 -403.330 -403.330] [83.223], Avg: [-430.252 -430.252 -430.252] (1.0000) ({r_i: None, r_t: [-831.691 -831.691 -831.691], eps: 1.0})
Step:  107700, Reward: [-431.668 -431.668 -431.668] [72.105], Avg: [-430.253 -430.253 -430.253] (1.0000) ({r_i: None, r_t: [-801.764 -801.764 -801.764], eps: 1.0})
Step:  107800, Reward: [-396.625 -396.625 -396.625] [67.513], Avg: [-430.222 -430.222 -430.222] (1.0000) ({r_i: None, r_t: [-833.446 -833.446 -833.446], eps: 1.0})
Step:  107900, Reward: [-428.129 -428.129 -428.129] [69.771], Avg: [-430.220 -430.220 -430.220] (1.0000) ({r_i: None, r_t: [-917.270 -917.270 -917.270], eps: 1.0})
Step:  108000, Reward: [-414.017 -414.017 -414.017] [71.565], Avg: [-430.205 -430.205 -430.205] (1.0000) ({r_i: None, r_t: [-857.426 -857.426 -857.426], eps: 1.0})
Step:  108100, Reward: [-431.516 -431.516 -431.516] [76.076], Avg: [-430.206 -430.206 -430.206] (1.0000) ({r_i: None, r_t: [-792.790 -792.790 -792.790], eps: 1.0})
Step:  108200, Reward: [-447.104 -447.104 -447.104] [120.440], Avg: [-430.222 -430.222 -430.222] (1.0000) ({r_i: None, r_t: [-880.524 -880.524 -880.524], eps: 1.0})
Step:  108300, Reward: [-447.159 -447.159 -447.159] [100.328], Avg: [-430.238 -430.238 -430.238] (1.0000) ({r_i: None, r_t: [-866.645 -866.645 -866.645], eps: 1.0})
Step:  108400, Reward: [-430.614 -430.614 -430.614] [69.517], Avg: [-430.238 -430.238 -430.238] (1.0000) ({r_i: None, r_t: [-879.293 -879.293 -879.293], eps: 1.0})
Step:  108500, Reward: [-441.755 -441.755 -441.755] [105.702], Avg: [-430.249 -430.249 -430.249] (1.0000) ({r_i: None, r_t: [-790.448 -790.448 -790.448], eps: 1.0})
Step:  108600, Reward: [-418.028 -418.028 -418.028] [74.873], Avg: [-430.237 -430.237 -430.237] (1.0000) ({r_i: None, r_t: [-822.601 -822.601 -822.601], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, PTCNetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.n_agents = len(action_size)
		n_actions = action_size[0][-1]
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		# self.mac = BasicMAC(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.learner = COMALearner(state_size, action_size, self.n_agents, n_actions, device=self.device)
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.buffer = []

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.learner.select_actions(inputs, t_env=None, eps=self.eps, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions_, rewards_, dones_ = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer2.add((states_, actions_, rewards_, dones_))
			self.buffer.clear()

			if len(self.replay_buffer2) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				
				states_, actions_, rewards_, dones_ = map(lambda x:torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions_ = actions_.squeeze(-2)
				actions_joint = actions_.view(*actions_.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions_[:, 0:1]), actions_[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)

				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)

				self.learner.train(None, actions_, critic_inputs, actor_inputs, rewards_.mean(-1, keepdims=True), dones_.mean(-1, keepdims=True), self.eps)
			
			self.eps = max(self.eps * self.decay, EPS_MIN)

class COMALearner():
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0

		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		# self.action_selector = MultinomialActionSelector()

		self.critic_training_steps = 0
		self.critic = COMACritic(state_size, action_size, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())

		# self.agent_params = list(mac.parameters())
		self.agent_params = list(self.agent.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def select_actions(self, inputs, t_env, eps, test_mode=False):
		agent_outputs = self.forward(inputs, eps, test_mode=test_mode)
		# chosen_actions = self.action_selector.select_action(agent_outputs, t_env, test_mode=test_mode)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/epsilon_action_num)
		return agent_outs

	def train(self, batch, actions_, critic_inputs, actor_inputs, rewards_, dones_, eps):
		q_vals = self._train_critic(batch, (actions_, critic_inputs, actor_inputs, rewards_, dones_))
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions_.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - (advantages * log_pi_taken).sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, sample):
		actions_, critic_inputs, actor_inputs, rewards_, dones_ = sample
		target_q_vals = self.target_critic(batch, critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions_.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = build_td_lambda_targets(rewards_, dones_, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards_.size(1))):
			q_t = self.critic(batch, critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions_[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			loss = (td_error ** 2).sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + n_agents
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, critic_inputs, t=None):
		x = torch.relu(self.fc1(critic_inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

class BasicMAC:
	def __init__(self, state_size, action_size, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(state_size[0][-1] + action_size[0][-1] + n_agents, self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, actor_inputs, t, test_mode=False):
		agent_outs = self.agent(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs

	def parameters(self):
		return self.agent.parameters()

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def forward(self, inputs):
		x = self.fc1(inputs).relu()
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		self.T = 0
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T=None):
		T = self.T if T is None else T
		self.T += 1
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-524.526 -524.526 -524.526] [68.681], Avg: [-524.526 -524.526 -524.526] (1.0000) ({r_i: None, r_t: [-10.033 -10.033 -10.033], eps: 1.0})
Step:  108700, Reward: [-377.393 -377.393 -377.393] [64.757], Avg: [-430.189 -430.189 -430.189] (1.0000) ({r_i: None, r_t: [-872.722 -872.722 -872.722], eps: 1.0})
Step:     100, Reward: [-511.334 -511.334 -511.334] [70.999], Avg: [-517.930 -517.930 -517.930] (0.9900) ({r_i: None, r_t: [-962.094 -962.094 -962.094], eps: 0.99})
Step:  108800, Reward: [-410.803 -410.803 -410.803] [80.746], Avg: [-430.171 -430.171 -430.171] (1.0000) ({r_i: None, r_t: [-838.340 -838.340 -838.340], eps: 1.0})
Step:     200, Reward: [-508.632 -508.632 -508.632] [126.274], Avg: [-514.831 -514.831 -514.831] (0.9801) ({r_i: None, r_t: [-971.053 -971.053 -971.053], eps: 0.98})
Step:  108900, Reward: [-437.560 -437.560 -437.560] [73.506], Avg: [-430.178 -430.178 -430.178] (1.0000) ({r_i: None, r_t: [-860.172 -860.172 -860.172], eps: 1.0})
Step:     300, Reward: [-515.131 -515.131 -515.131] [109.369], Avg: [-514.906 -514.906 -514.906] (0.9704) ({r_i: None, r_t: [-1013.457 -1013.457 -1013.457], eps: 0.97})
Step:  109000, Reward: [-412.468 -412.468 -412.468] [85.370], Avg: [-430.162 -430.162 -430.162] (1.0000) ({r_i: None, r_t: [-809.863 -809.863 -809.863], eps: 1.0})
Step:     400, Reward: [-479.463 -479.463 -479.463] [109.206], Avg: [-507.817 -507.817 -507.817] (0.9607) ({r_i: None, r_t: [-985.172 -985.172 -985.172], eps: 0.961})
Step:  109100, Reward: [-451.942 -451.942 -451.942] [58.676], Avg: [-430.182 -430.182 -430.182] (1.0000) ({r_i: None, r_t: [-828.072 -828.072 -828.072], eps: 1.0})
Step:     500, Reward: [-501.872 -501.872 -501.872] [103.019], Avg: [-506.826 -506.826 -506.826] (0.9511) ({r_i: None, r_t: [-1021.449 -1021.449 -1021.449], eps: 0.951})
Step:  109200, Reward: [-455.705 -455.705 -455.705] [99.947], Avg: [-430.205 -430.205 -430.205] (1.0000) ({r_i: None, r_t: [-852.935 -852.935 -852.935], eps: 1.0})
Step:     600, Reward: [-555.825 -555.825 -555.825] [129.997], Avg: [-513.826 -513.826 -513.826] (0.9416) ({r_i: None, r_t: [-896.231 -896.231 -896.231], eps: 0.942})
Step:  109300, Reward: [-387.867 -387.867 -387.867] [89.790], Avg: [-430.166 -430.166 -430.166] (1.0000) ({r_i: None, r_t: [-865.146 -865.146 -865.146], eps: 1.0})
Step:     700, Reward: [-499.237 -499.237 -499.237] [90.674], Avg: [-512.002 -512.002 -512.002] (0.9322) ({r_i: None, r_t: [-1009.246 -1009.246 -1009.246], eps: 0.932})
Step:  109400, Reward: [-397.757 -397.757 -397.757] [100.450], Avg: [-430.137 -430.137 -430.137] (1.0000) ({r_i: None, r_t: [-789.954 -789.954 -789.954], eps: 1.0})
Step:     800, Reward: [-505.425 -505.425 -505.425] [109.259], Avg: [-511.272 -511.272 -511.272] (0.9229) ({r_i: None, r_t: [-961.369 -961.369 -961.369], eps: 0.923})
Step:  109500, Reward: [-461.391 -461.391 -461.391] [103.485], Avg: [-430.165 -430.165 -430.165] (1.0000) ({r_i: None, r_t: [-843.254 -843.254 -843.254], eps: 1.0})
Step:     900, Reward: [-525.818 -525.818 -525.818] [90.439], Avg: [-512.726 -512.726 -512.726] (0.9137) ({r_i: None, r_t: [-1010.943 -1010.943 -1010.943], eps: 0.914})
Step:  109600, Reward: [-414.245 -414.245 -414.245] [96.846], Avg: [-430.151 -430.151 -430.151] (1.0000) ({r_i: None, r_t: [-854.808 -854.808 -854.808], eps: 1.0})
Step:    1000, Reward: [-439.501 -439.501 -439.501] [60.002], Avg: [-506.069 -506.069 -506.069] (0.9046) ({r_i: None, r_t: [-977.778 -977.778 -977.778], eps: 0.905})
Step:  109700, Reward: [-400.254 -400.254 -400.254] [73.000], Avg: [-430.123 -430.123 -430.123] (1.0000) ({r_i: None, r_t: [-861.744 -861.744 -861.744], eps: 1.0})
Step:    1100, Reward: [-487.582 -487.582 -487.582] [90.442], Avg: [-504.529 -504.529 -504.529] (0.8956) ({r_i: None, r_t: [-963.067 -963.067 -963.067], eps: 0.896})
Step:  109800, Reward: [-404.342 -404.342 -404.342] [94.397], Avg: [-430.100 -430.100 -430.100] (1.0000) ({r_i: None, r_t: [-790.448 -790.448 -790.448], eps: 1.0})
Step:    1200, Reward: [-451.870 -451.870 -451.870] [106.653], Avg: [-500.478 -500.478 -500.478] (0.8867) ({r_i: None, r_t: [-1014.805 -1014.805 -1014.805], eps: 0.887})
Step:  109900, Reward: [-448.655 -448.655 -448.655] [119.807], Avg: [-430.117 -430.117 -430.117] (1.0000) ({r_i: None, r_t: [-930.071 -930.071 -930.071], eps: 1.0})
Step:    1300, Reward: [-508.235 -508.235 -508.235] [106.586], Avg: [-501.032 -501.032 -501.032] (0.8778) ({r_i: None, r_t: [-997.664 -997.664 -997.664], eps: 0.878})
Step:  110000, Reward: [-399.559 -399.559 -399.559] [92.307], Avg: [-430.089 -430.089 -430.089] (1.0000) ({r_i: None, r_t: [-870.067 -870.067 -870.067], eps: 1.0})
Step:    1400, Reward: [-501.779 -501.779 -501.779] [111.166], Avg: [-501.082 -501.082 -501.082] (0.8691) ({r_i: None, r_t: [-955.613 -955.613 -955.613], eps: 0.869})
Step:  110100, Reward: [-428.528 -428.528 -428.528] [100.402], Avg: [-430.088 -430.088 -430.088] (1.0000) ({r_i: None, r_t: [-866.126 -866.126 -866.126], eps: 1.0})
Step:    1500, Reward: [-526.191 -526.191 -526.191] [98.865], Avg: [-502.651 -502.651 -502.651] (0.8604) ({r_i: None, r_t: [-1003.831 -1003.831 -1003.831], eps: 0.86})
Step:  110200, Reward: [-447.842 -447.842 -447.842] [86.962], Avg: [-430.104 -430.104 -430.104] (1.0000) ({r_i: None, r_t: [-904.096 -904.096 -904.096], eps: 1.0})
Step:    1600, Reward: [-514.663 -514.663 -514.663] [77.969], Avg: [-503.358 -503.358 -503.358] (0.8518) ({r_i: None, r_t: [-989.585 -989.585 -989.585], eps: 0.852})
Step:  110300, Reward: [-435.686 -435.686 -435.686] [60.320], Avg: [-430.109 -430.109 -430.109] (1.0000) ({r_i: None, r_t: [-879.871 -879.871 -879.871], eps: 1.0})
Step:    1700, Reward: [-464.228 -464.228 -464.228] [91.446], Avg: [-501.184 -501.184 -501.184] (0.8433) ({r_i: None, r_t: [-1003.128 -1003.128 -1003.128], eps: 0.843})
Step:  110400, Reward: [-467.934 -467.934 -467.934] [62.214], Avg: [-430.143 -430.143 -430.143] (1.0000) ({r_i: None, r_t: [-782.372 -782.372 -782.372], eps: 1.0})
Step:    1800, Reward: [-493.948 -493.948 -493.948] [116.474], Avg: [-500.803 -500.803 -500.803] (0.8349) ({r_i: None, r_t: [-1002.438 -1002.438 -1002.438], eps: 0.835})
Step:  110500, Reward: [-432.989 -432.989 -432.989] [70.060], Avg: [-430.146 -430.146 -430.146] (1.0000) ({r_i: None, r_t: [-860.458 -860.458 -860.458], eps: 1.0})
Step:    1900, Reward: [-540.790 -540.790 -540.790] [124.379], Avg: [-502.803 -502.803 -502.803] (0.8266) ({r_i: None, r_t: [-981.951 -981.951 -981.951], eps: 0.827})
Step:  110600, Reward: [-418.373 -418.373 -418.373] [96.223], Avg: [-430.135 -430.135 -430.135] (1.0000) ({r_i: None, r_t: [-813.187 -813.187 -813.187], eps: 1.0})
Step:    2000, Reward: [-535.982 -535.982 -535.982] [124.873], Avg: [-504.382 -504.382 -504.382] (0.8183) ({r_i: None, r_t: [-1018.587 -1018.587 -1018.587], eps: 0.818})
Step:  110700, Reward: [-415.314 -415.314 -415.314] [78.719], Avg: [-430.122 -430.122 -430.122] (1.0000) ({r_i: None, r_t: [-892.951 -892.951 -892.951], eps: 1.0})
Step:    2100, Reward: [-512.840 -512.840 -512.840] [110.743], Avg: [-504.767 -504.767 -504.767] (0.8102) ({r_i: None, r_t: [-1036.034 -1036.034 -1036.034], eps: 0.81})
Step:  110800, Reward: [-410.937 -410.937 -410.937] [98.422], Avg: [-430.104 -430.104 -430.104] (1.0000) ({r_i: None, r_t: [-947.313 -947.313 -947.313], eps: 1.0})
Step:    2200, Reward: [-540.076 -540.076 -540.076] [118.644], Avg: [-506.302 -506.302 -506.302] (0.8021) ({r_i: None, r_t: [-1051.104 -1051.104 -1051.104], eps: 0.802})
Step:  110900, Reward: [-425.006 -425.006 -425.006] [61.269], Avg: [-430.100 -430.100 -430.100] (1.0000) ({r_i: None, r_t: [-800.482 -800.482 -800.482], eps: 1.0})
Step:    2300, Reward: [-510.964 -510.964 -510.964] [95.878], Avg: [-506.496 -506.496 -506.496] (0.7941) ({r_i: None, r_t: [-991.699 -991.699 -991.699], eps: 0.794})
Step:  111000, Reward: [-421.564 -421.564 -421.564] [121.940], Avg: [-430.092 -430.092 -430.092] (1.0000) ({r_i: None, r_t: [-829.531 -829.531 -829.531], eps: 1.0})
Step:    2400, Reward: [-476.698 -476.698 -476.698] [85.936], Avg: [-505.304 -505.304 -505.304] (0.7862) ({r_i: None, r_t: [-953.436 -953.436 -953.436], eps: 0.786})
Step:  111100, Reward: [-412.011 -412.011 -412.011] [99.097], Avg: [-430.076 -430.076 -430.076] (1.0000) ({r_i: None, r_t: [-795.897 -795.897 -795.897], eps: 1.0})
Step:    2500, Reward: [-468.177 -468.177 -468.177] [83.494], Avg: [-503.876 -503.876 -503.876] (0.7783) ({r_i: None, r_t: [-986.075 -986.075 -986.075], eps: 0.778})
Step:  111200, Reward: [-420.698 -420.698 -420.698] [81.074], Avg: [-430.067 -430.067 -430.067] (1.0000) ({r_i: None, r_t: [-861.242 -861.242 -861.242], eps: 1.0})
Step:    2600, Reward: [-499.002 -499.002 -499.002] [126.346], Avg: [-503.696 -503.696 -503.696] (0.7705) ({r_i: None, r_t: [-895.201 -895.201 -895.201], eps: 0.771})
Step:  111300, Reward: [-454.532 -454.532 -454.532] [90.747], Avg: [-430.089 -430.089 -430.089] (1.0000) ({r_i: None, r_t: [-879.121 -879.121 -879.121], eps: 1.0})
Step:    2700, Reward: [-492.861 -492.861 -492.861] [57.454], Avg: [-503.309 -503.309 -503.309] (0.7629) ({r_i: None, r_t: [-1018.152 -1018.152 -1018.152], eps: 0.763})
Step:  111400, Reward: [-453.085 -453.085 -453.085] [66.842], Avg: [-430.110 -430.110 -430.110] (1.0000) ({r_i: None, r_t: [-878.401 -878.401 -878.401], eps: 1.0})
Step:    2800, Reward: [-465.063 -465.063 -465.063] [66.048], Avg: [-501.990 -501.990 -501.990] (0.7553) ({r_i: None, r_t: [-1065.243 -1065.243 -1065.243], eps: 0.755})
Step:  111500, Reward: [-441.279 -441.279 -441.279] [86.695], Avg: [-430.120 -430.120 -430.120] (1.0000) ({r_i: None, r_t: [-807.889 -807.889 -807.889], eps: 1.0})
Step:    2900, Reward: [-517.869 -517.869 -517.869] [134.667], Avg: [-502.519 -502.519 -502.519] (0.7477) ({r_i: None, r_t: [-1010.865 -1010.865 -1010.865], eps: 0.748})
Step:  111600, Reward: [-434.668 -434.668 -434.668] [90.278], Avg: [-430.124 -430.124 -430.124] (1.0000) ({r_i: None, r_t: [-843.864 -843.864 -843.864], eps: 1.0})
Step:    3000, Reward: [-459.275 -459.275 -459.275] [89.776], Avg: [-501.124 -501.124 -501.124] (0.7403) ({r_i: None, r_t: [-952.846 -952.846 -952.846], eps: 0.74})
Step:  111700, Reward: [-427.480 -427.480 -427.480] [86.955], Avg: [-430.122 -430.122 -430.122] (1.0000) ({r_i: None, r_t: [-871.797 -871.797 -871.797], eps: 1.0})
Step:    3100, Reward: [-470.102 -470.102 -470.102] [84.872], Avg: [-500.155 -500.155 -500.155] (0.7329) ({r_i: None, r_t: [-980.777 -980.777 -980.777], eps: 0.733})
Step:  111800, Reward: [-363.609 -363.609 -363.609] [81.768], Avg: [-430.062 -430.062 -430.062] (1.0000) ({r_i: None, r_t: [-874.593 -874.593 -874.593], eps: 1.0})
Step:    3200, Reward: [-473.890 -473.890 -473.890] [105.856], Avg: [-499.359 -499.359 -499.359] (0.7256) ({r_i: None, r_t: [-923.531 -923.531 -923.531], eps: 0.726})
Step:  111900, Reward: [-416.251 -416.251 -416.251] [100.442], Avg: [-430.050 -430.050 -430.050] (1.0000) ({r_i: None, r_t: [-815.776 -815.776 -815.776], eps: 1.0})
Step:    3300, Reward: [-521.409 -521.409 -521.409] [110.143], Avg: [-500.008 -500.008 -500.008] (0.7183) ({r_i: None, r_t: [-939.379 -939.379 -939.379], eps: 0.718})
Step:  112000, Reward: [-426.928 -426.928 -426.928] [117.484], Avg: [-430.047 -430.047 -430.047] (1.0000) ({r_i: None, r_t: [-813.294 -813.294 -813.294], eps: 1.0})
Step:    3400, Reward: [-457.632 -457.632 -457.632] [93.598], Avg: [-498.797 -498.797 -498.797] (0.7112) ({r_i: None, r_t: [-1031.814 -1031.814 -1031.814], eps: 0.711})
Step:  112100, Reward: [-464.001 -464.001 -464.001] [101.231], Avg: [-430.077 -430.077 -430.077] (1.0000) ({r_i: None, r_t: [-823.163 -823.163 -823.163], eps: 1.0})
Step:    3500, Reward: [-529.331 -529.331 -529.331] [81.453], Avg: [-499.645 -499.645 -499.645] (0.7041) ({r_i: None, r_t: [-953.118 -953.118 -953.118], eps: 0.704})
Step:  112200, Reward: [-388.285 -388.285 -388.285] [83.191], Avg: [-430.040 -430.040 -430.040] (1.0000) ({r_i: None, r_t: [-838.362 -838.362 -838.362], eps: 1.0})
Step:    3600, Reward: [-473.131 -473.131 -473.131] [80.669], Avg: [-498.928 -498.928 -498.928] (0.6970) ({r_i: None, r_t: [-1016.101 -1016.101 -1016.101], eps: 0.697})
Step:  112300, Reward: [-402.661 -402.661 -402.661] [61.860], Avg: [-430.016 -430.016 -430.016] (1.0000) ({r_i: None, r_t: [-876.327 -876.327 -876.327], eps: 1.0})
Step:    3700, Reward: [-511.655 -511.655 -511.655] [104.879], Avg: [-499.263 -499.263 -499.263] (0.6901) ({r_i: None, r_t: [-960.130 -960.130 -960.130], eps: 0.69})
Step:  112400, Reward: [-389.409 -389.409 -389.409] [59.758], Avg: [-429.980 -429.980 -429.980] (1.0000) ({r_i: None, r_t: [-843.694 -843.694 -843.694], eps: 1.0})
Step:    3800, Reward: [-453.187 -453.187 -453.187] [89.403], Avg: [-498.082 -498.082 -498.082] (0.6832) ({r_i: None, r_t: [-996.419 -996.419 -996.419], eps: 0.683})
Step:  112500, Reward: [-400.914 -400.914 -400.914] [79.052], Avg: [-429.954 -429.954 -429.954] (1.0000) ({r_i: None, r_t: [-813.649 -813.649 -813.649], eps: 1.0})
Step:    3900, Reward: [-502.377 -502.377 -502.377] [93.604], Avg: [-498.189 -498.189 -498.189] (0.6764) ({r_i: None, r_t: [-944.693 -944.693 -944.693], eps: 0.676})
Step:  112600, Reward: [-388.484 -388.484 -388.484] [87.674], Avg: [-429.917 -429.917 -429.917] (1.0000) ({r_i: None, r_t: [-842.955 -842.955 -842.955], eps: 1.0})
Step:    4000, Reward: [-435.092 -435.092 -435.092] [53.065], Avg: [-496.650 -496.650 -496.650] (0.6696) ({r_i: None, r_t: [-974.566 -974.566 -974.566], eps: 0.67})
Step:  112700, Reward: [-409.903 -409.903 -409.903] [61.841], Avg: [-429.899 -429.899 -429.899] (1.0000) ({r_i: None, r_t: [-790.155 -790.155 -790.155], eps: 1.0})
Step:    4100, Reward: [-497.646 -497.646 -497.646] [115.145], Avg: [-496.674 -496.674 -496.674] (0.6630) ({r_i: None, r_t: [-983.494 -983.494 -983.494], eps: 0.663})
Step:  112800, Reward: [-385.264 -385.264 -385.264] [75.271], Avg: [-429.860 -429.860 -429.860] (1.0000) ({r_i: None, r_t: [-791.378 -791.378 -791.378], eps: 1.0})
Step:    4200, Reward: [-478.937 -478.937 -478.937] [92.786], Avg: [-496.261 -496.261 -496.261] (0.6564) ({r_i: None, r_t: [-1004.297 -1004.297 -1004.297], eps: 0.656})
Step:  112900, Reward: [-431.938 -431.938 -431.938] [87.360], Avg: [-429.862 -429.862 -429.862] (1.0000) ({r_i: None, r_t: [-814.370 -814.370 -814.370], eps: 1.0})
Step:    4300, Reward: [-458.593 -458.593 -458.593] [79.645], Avg: [-495.405 -495.405 -495.405] (0.6498) ({r_i: None, r_t: [-939.207 -939.207 -939.207], eps: 0.65})
Step:  113000, Reward: [-373.914 -373.914 -373.914] [51.386], Avg: [-429.812 -429.812 -429.812] (1.0000) ({r_i: None, r_t: [-859.134 -859.134 -859.134], eps: 1.0})
Step:    4400, Reward: [-487.218 -487.218 -487.218] [69.299], Avg: [-495.223 -495.223 -495.223] (0.6433) ({r_i: None, r_t: [-948.420 -948.420 -948.420], eps: 0.643})
Step:  113100, Reward: [-414.449 -414.449 -414.449] [61.123], Avg: [-429.799 -429.799 -429.799] (1.0000) ({r_i: None, r_t: [-820.221 -820.221 -820.221], eps: 1.0})
Step:    4500, Reward: [-442.829 -442.829 -442.829] [60.853], Avg: [-494.084 -494.084 -494.084] (0.6369) ({r_i: None, r_t: [-932.796 -932.796 -932.796], eps: 0.637})
Step:  113200, Reward: [-438.068 -438.068 -438.068] [78.019], Avg: [-429.806 -429.806 -429.806] (1.0000) ({r_i: None, r_t: [-838.080 -838.080 -838.080], eps: 1.0})
Step:    4600, Reward: [-481.972 -481.972 -481.972] [100.329], Avg: [-493.827 -493.827 -493.827] (0.6306) ({r_i: None, r_t: [-932.851 -932.851 -932.851], eps: 0.631})
Step:  113300, Reward: [-414.863 -414.863 -414.863] [66.359], Avg: [-429.793 -429.793 -429.793] (1.0000) ({r_i: None, r_t: [-822.280 -822.280 -822.280], eps: 1.0})
Step:    4700, Reward: [-442.069 -442.069 -442.069] [55.585], Avg: [-492.748 -492.748 -492.748] (0.6243) ({r_i: None, r_t: [-946.985 -946.985 -946.985], eps: 0.624})
Step:  113400, Reward: [-368.485 -368.485 -368.485] [52.950], Avg: [-429.739 -429.739 -429.739] (1.0000) ({r_i: None, r_t: [-796.579 -796.579 -796.579], eps: 1.0})
Step:    4800, Reward: [-485.044 -485.044 -485.044] [105.103], Avg: [-492.591 -492.591 -492.591] (0.6180) ({r_i: None, r_t: [-1003.621 -1003.621 -1003.621], eps: 0.618})
Step:  113500, Reward: [-394.012 -394.012 -394.012] [71.252], Avg: [-429.707 -429.707 -429.707] (1.0000) ({r_i: None, r_t: [-787.053 -787.053 -787.053], eps: 1.0})
Step:    4900, Reward: [-539.013 -539.013 -539.013] [88.877], Avg: [-493.520 -493.520 -493.520] (0.6119) ({r_i: None, r_t: [-977.112 -977.112 -977.112], eps: 0.612})
Step:  113600, Reward: [-397.903 -397.903 -397.903] [109.550], Avg: [-429.679 -429.679 -429.679] (1.0000) ({r_i: None, r_t: [-811.110 -811.110 -811.110], eps: 1.0})
Step:    5000, Reward: [-528.104 -528.104 -528.104] [118.767], Avg: [-494.198 -494.198 -494.198] (0.6058) ({r_i: None, r_t: [-1024.209 -1024.209 -1024.209], eps: 0.606})
Step:  113700, Reward: [-466.825 -466.825 -466.825] [107.184], Avg: [-429.712 -429.712 -429.712] (1.0000) ({r_i: None, r_t: [-820.260 -820.260 -820.260], eps: 1.0})
Step:    5100, Reward: [-466.482 -466.482 -466.482] [78.152], Avg: [-493.665 -493.665 -493.665] (0.5997) ({r_i: None, r_t: [-921.010 -921.010 -921.010], eps: 0.6})
Step:  113800, Reward: [-407.759 -407.759 -407.759] [79.073], Avg: [-429.693 -429.693 -429.693] (1.0000) ({r_i: None, r_t: [-771.497 -771.497 -771.497], eps: 1.0})
Step:    5200, Reward: [-488.476 -488.476 -488.476] [73.609], Avg: [-493.567 -493.567 -493.567] (0.5937) ({r_i: None, r_t: [-945.939 -945.939 -945.939], eps: 0.594})
Step:  113900, Reward: [-434.234 -434.234 -434.234] [98.817], Avg: [-429.697 -429.697 -429.697] (1.0000) ({r_i: None, r_t: [-789.998 -789.998 -789.998], eps: 1.0})
Step:    5300, Reward: [-436.694 -436.694 -436.694] [44.792], Avg: [-492.514 -492.514 -492.514] (0.5878) ({r_i: None, r_t: [-982.273 -982.273 -982.273], eps: 0.588})
Step:  114000, Reward: [-410.391 -410.391 -410.391] [83.684], Avg: [-429.680 -429.680 -429.680] (1.0000) ({r_i: None, r_t: [-780.273 -780.273 -780.273], eps: 1.0})
Step:    5400, Reward: [-438.595 -438.595 -438.595] [60.844], Avg: [-491.533 -491.533 -491.533] (0.5820) ({r_i: None, r_t: [-929.854 -929.854 -929.854], eps: 0.582})
Step:  114100, Reward: [-361.387 -361.387 -361.387] [66.730], Avg: [-429.620 -429.620 -429.620] (1.0000) ({r_i: None, r_t: [-842.189 -842.189 -842.189], eps: 1.0})
Step:    5500, Reward: [-478.486 -478.486 -478.486] [76.620], Avg: [-491.300 -491.300 -491.300] (0.5762) ({r_i: None, r_t: [-965.685 -965.685 -965.685], eps: 0.576})
Step:  114200, Reward: [-410.345 -410.345 -410.345] [82.240], Avg: [-429.603 -429.603 -429.603] (1.0000) ({r_i: None, r_t: [-840.756 -840.756 -840.756], eps: 1.0})
Step:    5600, Reward: [-455.609 -455.609 -455.609] [63.057], Avg: [-490.674 -490.674 -490.674] (0.5704) ({r_i: None, r_t: [-961.989 -961.989 -961.989], eps: 0.57})
Step:  114300, Reward: [-452.433 -452.433 -452.433] [68.308], Avg: [-429.623 -429.623 -429.623] (1.0000) ({r_i: None, r_t: [-812.363 -812.363 -812.363], eps: 1.0})
Step:  114400, Reward: [-422.387 -422.387 -422.387] [115.981], Avg: [-429.617 -429.617 -429.617] (1.0000) ({r_i: None, r_t: [-774.249 -774.249 -774.249], eps: 1.0})
Step:    5700, Reward: [-518.652 -518.652 -518.652] [84.479], Avg: [-491.157 -491.157 -491.157] (0.5647) ({r_i: None, r_t: [-949.688 -949.688 -949.688], eps: 0.565})
Step:  114500, Reward: [-393.354 -393.354 -393.354] [52.593], Avg: [-429.585 -429.585 -429.585] (1.0000) ({r_i: None, r_t: [-817.443 -817.443 -817.443], eps: 1.0})
Step:    5800, Reward: [-482.505 -482.505 -482.505] [67.630], Avg: [-491.010 -491.010 -491.010] (0.5591) ({r_i: None, r_t: [-1009.759 -1009.759 -1009.759], eps: 0.559})
Step:  114600, Reward: [-395.352 -395.352 -395.352] [59.410], Avg: [-429.555 -429.555 -429.555] (1.0000) ({r_i: None, r_t: [-863.502 -863.502 -863.502], eps: 1.0})
Step:    5900, Reward: [-484.706 -484.706 -484.706] [101.964], Avg: [-490.905 -490.905 -490.905] (0.5535) ({r_i: None, r_t: [-881.826 -881.826 -881.826], eps: 0.554})
Step:  114700, Reward: [-402.036 -402.036 -402.036] [83.981], Avg: [-429.531 -429.531 -429.531] (1.0000) ({r_i: None, r_t: [-826.581 -826.581 -826.581], eps: 1.0})
Step:    6000, Reward: [-466.718 -466.718 -466.718] [74.015], Avg: [-490.508 -490.508 -490.508] (0.5480) ({r_i: None, r_t: [-933.438 -933.438 -933.438], eps: 0.548})
Step:  114800, Reward: [-396.256 -396.256 -396.256] [81.594], Avg: [-429.502 -429.502 -429.502] (1.0000) ({r_i: None, r_t: [-808.348 -808.348 -808.348], eps: 1.0})
Step:    6100, Reward: [-474.952 -474.952 -474.952] [79.552], Avg: [-490.257 -490.257 -490.257] (0.5425) ({r_i: None, r_t: [-974.305 -974.305 -974.305], eps: 0.543})
Step:  114900, Reward: [-413.518 -413.518 -413.518] [66.854], Avg: [-429.488 -429.488 -429.488] (1.0000) ({r_i: None, r_t: [-836.019 -836.019 -836.019], eps: 1.0})
Step:    6200, Reward: [-461.192 -461.192 -461.192] [86.921], Avg: [-489.796 -489.796 -489.796] (0.5371) ({r_i: None, r_t: [-923.033 -923.033 -923.033], eps: 0.537})
Step:  115000, Reward: [-429.613 -429.613 -429.613] [90.675], Avg: [-429.488 -429.488 -429.488] (1.0000) ({r_i: None, r_t: [-785.919 -785.919 -785.919], eps: 1.0})
Step:    6300, Reward: [-509.876 -509.876 -509.876] [99.762], Avg: [-490.110 -490.110 -490.110] (0.5318) ({r_i: None, r_t: [-872.795 -872.795 -872.795], eps: 0.532})
Step:  115100, Reward: [-377.305 -377.305 -377.305] [64.249], Avg: [-429.443 -429.443 -429.443] (1.0000) ({r_i: None, r_t: [-800.896 -800.896 -800.896], eps: 1.0})
Step:    6400, Reward: [-446.594 -446.594 -446.594] [64.114], Avg: [-489.440 -489.440 -489.440] (0.5264) ({r_i: None, r_t: [-882.250 -882.250 -882.250], eps: 0.526})
Step:  115200, Reward: [-371.548 -371.548 -371.548] [54.946], Avg: [-429.393 -429.393 -429.393] (1.0000) ({r_i: None, r_t: [-825.470 -825.470 -825.470], eps: 1.0})
Step:    6500, Reward: [-492.396 -492.396 -492.396] [111.623], Avg: [-489.485 -489.485 -489.485] (0.5212) ({r_i: None, r_t: [-924.423 -924.423 -924.423], eps: 0.521})
Step:  115300, Reward: [-425.096 -425.096 -425.096] [73.539], Avg: [-429.389 -429.389 -429.389] (1.0000) ({r_i: None, r_t: [-782.597 -782.597 -782.597], eps: 1.0})
Step:    6600, Reward: [-507.649 -507.649 -507.649] [88.474], Avg: [-489.756 -489.756 -489.756] (0.5160) ({r_i: None, r_t: [-930.597 -930.597 -930.597], eps: 0.516})
Step:  115400, Reward: [-362.842 -362.842 -362.842] [64.807], Avg: [-429.332 -429.332 -429.332] (1.0000) ({r_i: None, r_t: [-787.722 -787.722 -787.722], eps: 1.0})
Step:    6700, Reward: [-495.959 -495.959 -495.959] [92.407], Avg: [-489.847 -489.847 -489.847] (0.5108) ({r_i: None, r_t: [-945.373 -945.373 -945.373], eps: 0.511})
Step:  115500, Reward: [-411.293 -411.293 -411.293] [67.885], Avg: [-429.316 -429.316 -429.316] (1.0000) ({r_i: None, r_t: [-850.596 -850.596 -850.596], eps: 1.0})
Step:    6800, Reward: [-495.555 -495.555 -495.555] [104.746], Avg: [-489.930 -489.930 -489.930] (0.5058) ({r_i: None, r_t: [-918.620 -918.620 -918.620], eps: 0.506})
Step:  115600, Reward: [-394.389 -394.389 -394.389] [70.990], Avg: [-429.286 -429.286 -429.286] (1.0000) ({r_i: None, r_t: [-819.315 -819.315 -819.315], eps: 1.0})
Step:    6900, Reward: [-471.250 -471.250 -471.250] [62.910], Avg: [-489.663 -489.663 -489.663] (0.5007) ({r_i: None, r_t: [-887.483 -887.483 -887.483], eps: 0.501})
Step:  115700, Reward: [-362.716 -362.716 -362.716] [57.493], Avg: [-429.228 -429.228 -429.228] (1.0000) ({r_i: None, r_t: [-823.026 -823.026 -823.026], eps: 1.0})
Step:    7000, Reward: [-477.274 -477.274 -477.274] [81.105], Avg: [-489.489 -489.489 -489.489] (0.4957) ({r_i: None, r_t: [-970.451 -970.451 -970.451], eps: 0.496})
Step:  115800, Reward: [-377.489 -377.489 -377.489] [66.901], Avg: [-429.184 -429.184 -429.184] (1.0000) ({r_i: None, r_t: [-805.965 -805.965 -805.965], eps: 1.0})
Step:    7100, Reward: [-499.410 -499.410 -499.410] [105.986], Avg: [-489.627 -489.627 -489.627] (0.4908) ({r_i: None, r_t: [-951.010 -951.010 -951.010], eps: 0.491})
Step:  115900, Reward: [-395.804 -395.804 -395.804] [67.044], Avg: [-429.155 -429.155 -429.155] (1.0000) ({r_i: None, r_t: [-790.114 -790.114 -790.114], eps: 1.0})
Step:    7200, Reward: [-451.370 -451.370 -451.370] [77.996], Avg: [-489.103 -489.103 -489.103] (0.4859) ({r_i: None, r_t: [-981.720 -981.720 -981.720], eps: 0.486})
Step:  116000, Reward: [-396.486 -396.486 -396.486] [72.102], Avg: [-429.127 -429.127 -429.127] (1.0000) ({r_i: None, r_t: [-860.537 -860.537 -860.537], eps: 1.0})
Step:    7300, Reward: [-468.545 -468.545 -468.545] [42.326], Avg: [-488.825 -488.825 -488.825] (0.4810) ({r_i: None, r_t: [-948.284 -948.284 -948.284], eps: 0.481})
Step:  116100, Reward: [-399.488 -399.488 -399.488] [67.033], Avg: [-429.101 -429.101 -429.101] (1.0000) ({r_i: None, r_t: [-814.743 -814.743 -814.743], eps: 1.0})
Step:    7400, Reward: [-466.901 -466.901 -466.901] [93.108], Avg: [-488.532 -488.532 -488.532] (0.4762) ({r_i: None, r_t: [-926.556 -926.556 -926.556], eps: 0.476})
Step:  116200, Reward: [-349.356 -349.356 -349.356] [55.196], Avg: [-429.033 -429.033 -429.033] (1.0000) ({r_i: None, r_t: [-735.673 -735.673 -735.673], eps: 1.0})
Step:    7500, Reward: [-460.804 -460.804 -460.804] [86.076], Avg: [-488.168 -488.168 -488.168] (0.4715) ({r_i: None, r_t: [-914.986 -914.986 -914.986], eps: 0.471})
Step:  116300, Reward: [-373.287 -373.287 -373.287] [56.392], Avg: [-428.985 -428.985 -428.985] (1.0000) ({r_i: None, r_t: [-818.930 -818.930 -818.930], eps: 1.0})
Step:    7600, Reward: [-462.599 -462.599 -462.599] [83.647], Avg: [-487.836 -487.836 -487.836] (0.4668) ({r_i: None, r_t: [-943.476 -943.476 -943.476], eps: 0.467})
Step:  116400, Reward: [-374.899 -374.899 -374.899] [102.750], Avg: [-428.938 -428.938 -428.938] (1.0000) ({r_i: None, r_t: [-795.745 -795.745 -795.745], eps: 1.0})
Step:    7700, Reward: [-466.017 -466.017 -466.017] [89.129], Avg: [-487.556 -487.556 -487.556] (0.4621) ({r_i: None, r_t: [-939.202 -939.202 -939.202], eps: 0.462})
Step:  116500, Reward: [-368.464 -368.464 -368.464] [70.207], Avg: [-428.886 -428.886 -428.886] (1.0000) ({r_i: None, r_t: [-813.034 -813.034 -813.034], eps: 1.0})
Step:    7800, Reward: [-476.031 -476.031 -476.031] [84.275], Avg: [-487.410 -487.410 -487.410] (0.4575) ({r_i: None, r_t: [-964.232 -964.232 -964.232], eps: 0.458})
Step:  116600, Reward: [-452.345 -452.345 -452.345] [101.648], Avg: [-428.907 -428.907 -428.907] (1.0000) ({r_i: None, r_t: [-796.834 -796.834 -796.834], eps: 1.0})
Step:    7900, Reward: [-465.595 -465.595 -465.595] [144.640], Avg: [-487.137 -487.137 -487.137] (0.4529) ({r_i: None, r_t: [-927.416 -927.416 -927.416], eps: 0.453})
Step:  116700, Reward: [-381.926 -381.926 -381.926] [73.893], Avg: [-428.866 -428.866 -428.866] (1.0000) ({r_i: None, r_t: [-747.260 -747.260 -747.260], eps: 1.0})
Step:    8000, Reward: [-455.112 -455.112 -455.112] [60.050], Avg: [-486.742 -486.742 -486.742] (0.4484) ({r_i: None, r_t: [-941.431 -941.431 -941.431], eps: 0.448})
Step:  116800, Reward: [-384.900 -384.900 -384.900] [49.111], Avg: [-428.829 -428.829 -428.829] (1.0000) ({r_i: None, r_t: [-819.176 -819.176 -819.176], eps: 1.0})
Step:    8100, Reward: [-440.404 -440.404 -440.404] [64.009], Avg: [-486.177 -486.177 -486.177] (0.4440) ({r_i: None, r_t: [-939.898 -939.898 -939.898], eps: 0.444})
Step:  116900, Reward: [-416.740 -416.740 -416.740] [94.486], Avg: [-428.818 -428.818 -428.818] (1.0000) ({r_i: None, r_t: [-815.193 -815.193 -815.193], eps: 1.0})
Step:    8200, Reward: [-457.144 -457.144 -457.144] [66.802], Avg: [-485.827 -485.827 -485.827] (0.4395) ({r_i: None, r_t: [-956.842 -956.842 -956.842], eps: 0.44})
Step:  117000, Reward: [-435.413 -435.413 -435.413] [73.787], Avg: [-428.824 -428.824 -428.824] (1.0000) ({r_i: None, r_t: [-782.789 -782.789 -782.789], eps: 1.0})
Step:    8300, Reward: [-452.541 -452.541 -452.541] [109.269], Avg: [-485.431 -485.431 -485.431] (0.4351) ({r_i: None, r_t: [-981.616 -981.616 -981.616], eps: 0.435})
Step:  117100, Reward: [-391.887 -391.887 -391.887] [87.817], Avg: [-428.792 -428.792 -428.792] (1.0000) ({r_i: None, r_t: [-769.337 -769.337 -769.337], eps: 1.0})
Step:    8400, Reward: [-471.123 -471.123 -471.123] [58.248], Avg: [-485.262 -485.262 -485.262] (0.4308) ({r_i: None, r_t: [-939.932 -939.932 -939.932], eps: 0.431})
Step:  117200, Reward: [-364.805 -364.805 -364.805] [53.023], Avg: [-428.738 -428.738 -428.738] (1.0000) ({r_i: None, r_t: [-829.136 -829.136 -829.136], eps: 1.0})
Step:    8500, Reward: [-446.413 -446.413 -446.413] [73.262], Avg: [-484.811 -484.811 -484.811] (0.4265) ({r_i: None, r_t: [-930.566 -930.566 -930.566], eps: 0.427})
Step:  117300, Reward: [-404.937 -404.937 -404.937] [92.591], Avg: [-428.718 -428.718 -428.718] (1.0000) ({r_i: None, r_t: [-816.390 -816.390 -816.390], eps: 1.0})
Step:    8600, Reward: [-442.168 -442.168 -442.168] [55.141], Avg: [-484.320 -484.320 -484.320] (0.4223) ({r_i: None, r_t: [-897.191 -897.191 -897.191], eps: 0.422})
Step:  117400, Reward: [-397.073 -397.073 -397.073] [82.883], Avg: [-428.691 -428.691 -428.691] (1.0000) ({r_i: None, r_t: [-772.277 -772.277 -772.277], eps: 1.0})
Step:    8700, Reward: [-461.994 -461.994 -461.994] [75.561], Avg: [-484.067 -484.067 -484.067] (0.4180) ({r_i: None, r_t: [-935.476 -935.476 -935.476], eps: 0.418})
Step:  117500, Reward: [-388.026 -388.026 -388.026] [77.265], Avg: [-428.656 -428.656 -428.656] (1.0000) ({r_i: None, r_t: [-810.913 -810.913 -810.913], eps: 1.0})
Step:    8800, Reward: [-467.094 -467.094 -467.094] [48.931], Avg: [-483.876 -483.876 -483.876] (0.4139) ({r_i: None, r_t: [-878.550 -878.550 -878.550], eps: 0.414})
Step:  117600, Reward: [-397.744 -397.744 -397.744] [61.260], Avg: [-428.630 -428.630 -428.630] (1.0000) ({r_i: None, r_t: [-811.339 -811.339 -811.339], eps: 1.0})
Step:    8900, Reward: [-438.038 -438.038 -438.038] [69.935], Avg: [-483.367 -483.367 -483.367] (0.4097) ({r_i: None, r_t: [-922.293 -922.293 -922.293], eps: 0.41})
Step:  117700, Reward: [-404.303 -404.303 -404.303] [78.150], Avg: [-428.609 -428.609 -428.609] (1.0000) ({r_i: None, r_t: [-802.936 -802.936 -802.936], eps: 1.0})
Step:    9000, Reward: [-395.523 -395.523 -395.523] [46.150], Avg: [-482.401 -482.401 -482.401] (0.4057) ({r_i: None, r_t: [-891.063 -891.063 -891.063], eps: 0.406})
Step:  117800, Reward: [-370.960 -370.960 -370.960] [88.178], Avg: [-428.560 -428.560 -428.560] (1.0000) ({r_i: None, r_t: [-767.071 -767.071 -767.071], eps: 1.0})
Step:    9100, Reward: [-431.990 -431.990 -431.990] [73.773], Avg: [-481.853 -481.853 -481.853] (0.4016) ({r_i: None, r_t: [-942.513 -942.513 -942.513], eps: 0.402})
Step:  117900, Reward: [-398.029 -398.029 -398.029] [88.508], Avg: [-428.534 -428.534 -428.534] (1.0000) ({r_i: None, r_t: [-810.966 -810.966 -810.966], eps: 1.0})
Step:    9200, Reward: [-424.397 -424.397 -424.397] [62.531], Avg: [-481.236 -481.236 -481.236] (0.3976) ({r_i: None, r_t: [-895.462 -895.462 -895.462], eps: 0.398})
Step:  118000, Reward: [-394.457 -394.457 -394.457] [82.052], Avg: [-428.506 -428.506 -428.506] (1.0000) ({r_i: None, r_t: [-751.162 -751.162 -751.162], eps: 1.0})
Step:    9300, Reward: [-439.499 -439.499 -439.499] [92.714], Avg: [-480.792 -480.792 -480.792] (0.3936) ({r_i: None, r_t: [-934.111 -934.111 -934.111], eps: 0.394})
Step:  118100, Reward: [-402.472 -402.472 -402.472] [77.576], Avg: [-428.484 -428.484 -428.484] (1.0000) ({r_i: None, r_t: [-817.199 -817.199 -817.199], eps: 1.0})
Step:    9400, Reward: [-441.798 -441.798 -441.798] [75.521], Avg: [-480.381 -480.381 -480.381] (0.3897) ({r_i: None, r_t: [-897.572 -897.572 -897.572], eps: 0.39})
Step:  118200, Reward: [-385.221 -385.221 -385.221] [67.539], Avg: [-428.447 -428.447 -428.447] (1.0000) ({r_i: None, r_t: [-771.106 -771.106 -771.106], eps: 1.0})
Step:    9500, Reward: [-437.624 -437.624 -437.624] [84.977], Avg: [-479.936 -479.936 -479.936] (0.3858) ({r_i: None, r_t: [-880.345 -880.345 -880.345], eps: 0.386})
Step:  118300, Reward: [-392.736 -392.736 -392.736] [52.426], Avg: [-428.417 -428.417 -428.417] (1.0000) ({r_i: None, r_t: [-801.671 -801.671 -801.671], eps: 1.0})
Step:    9600, Reward: [-426.253 -426.253 -426.253] [69.617], Avg: [-479.382 -479.382 -479.382] (0.3820) ({r_i: None, r_t: [-895.523 -895.523 -895.523], eps: 0.382})
Step:  118400, Reward: [-381.445 -381.445 -381.445] [58.666], Avg: [-428.377 -428.377 -428.377] (1.0000) ({r_i: None, r_t: [-781.576 -781.576 -781.576], eps: 1.0})
Step:    9700, Reward: [-440.746 -440.746 -440.746] [67.555], Avg: [-478.988 -478.988 -478.988] (0.3782) ({r_i: None, r_t: [-897.621 -897.621 -897.621], eps: 0.378})
Step:  118500, Reward: [-413.985 -413.985 -413.985] [100.514], Avg: [-428.365 -428.365 -428.365] (1.0000) ({r_i: None, r_t: [-814.110 -814.110 -814.110], eps: 1.0})
Step:    9800, Reward: [-455.202 -455.202 -455.202] [70.406], Avg: [-478.748 -478.748 -478.748] (0.3744) ({r_i: None, r_t: [-876.123 -876.123 -876.123], eps: 0.374})
Step:  118600, Reward: [-412.933 -412.933 -412.933] [59.201], Avg: [-428.352 -428.352 -428.352] (1.0000) ({r_i: None, r_t: [-769.235 -769.235 -769.235], eps: 1.0})
Step:    9900, Reward: [-426.714 -426.714 -426.714] [65.737], Avg: [-478.228 -478.228 -478.228] (0.3707) ({r_i: None, r_t: [-869.495 -869.495 -869.495], eps: 0.371})
Step:  118700, Reward: [-368.675 -368.675 -368.675] [63.544], Avg: [-428.302 -428.302 -428.302] (1.0000) ({r_i: None, r_t: [-781.640 -781.640 -781.640], eps: 1.0})
Step:   10000, Reward: [-456.117 -456.117 -456.117] [80.308], Avg: [-478.009 -478.009 -478.009] (0.3670) ({r_i: None, r_t: [-906.158 -906.158 -906.158], eps: 0.367})
Step:  118800, Reward: [-373.438 -373.438 -373.438] [62.821], Avg: [-428.256 -428.256 -428.256] (1.0000) ({r_i: None, r_t: [-761.187 -761.187 -761.187], eps: 1.0})
Step:   10100, Reward: [-482.219 -482.219 -482.219] [101.696], Avg: [-478.050 -478.050 -478.050] (0.3633) ({r_i: None, r_t: [-920.820 -920.820 -920.820], eps: 0.363})
Step:  118900, Reward: [-400.377 -400.377 -400.377] [52.165], Avg: [-428.232 -428.232 -428.232] (1.0000) ({r_i: None, r_t: [-800.058 -800.058 -800.058], eps: 1.0})
Step:   10200, Reward: [-414.836 -414.836 -414.836] [55.923], Avg: [-477.436 -477.436 -477.436] (0.3597) ({r_i: None, r_t: [-876.544 -876.544 -876.544], eps: 0.36})
Step:  119000, Reward: [-407.921 -407.921 -407.921] [80.970], Avg: [-428.215 -428.215 -428.215] (1.0000) ({r_i: None, r_t: [-811.252 -811.252 -811.252], eps: 1.0})
Step:   10300, Reward: [-407.520 -407.520 -407.520] [54.728], Avg: [-476.764 -476.764 -476.764] (0.3561) ({r_i: None, r_t: [-958.610 -958.610 -958.610], eps: 0.356})
Step:  119100, Reward: [-393.247 -393.247 -393.247] [83.298], Avg: [-428.186 -428.186 -428.186] (1.0000) ({r_i: None, r_t: [-804.794 -804.794 -804.794], eps: 1.0})
Step:   10400, Reward: [-481.872 -481.872 -481.872] [99.742], Avg: [-476.813 -476.813 -476.813] (0.3525) ({r_i: None, r_t: [-923.893 -923.893 -923.893], eps: 0.353})
Step:  119200, Reward: [-395.281 -395.281 -395.281] [76.236], Avg: [-428.158 -428.158 -428.158] (1.0000) ({r_i: None, r_t: [-835.841 -835.841 -835.841], eps: 1.0})
Step:   10500, Reward: [-464.489 -464.489 -464.489] [76.046], Avg: [-476.696 -476.696 -476.696] (0.3490) ({r_i: None, r_t: [-919.931 -919.931 -919.931], eps: 0.349})
Step:  119300, Reward: [-403.908 -403.908 -403.908] [69.901], Avg: [-428.138 -428.138 -428.138] (1.0000) ({r_i: None, r_t: [-801.258 -801.258 -801.258], eps: 1.0})
Step:   10600, Reward: [-459.032 -459.032 -459.032] [117.687], Avg: [-476.531 -476.531 -476.531] (0.3455) ({r_i: None, r_t: [-922.332 -922.332 -922.332], eps: 0.346})
Step:  119400, Reward: [-402.284 -402.284 -402.284] [98.643], Avg: [-428.116 -428.116 -428.116] (1.0000) ({r_i: None, r_t: [-798.917 -798.917 -798.917], eps: 1.0})
Step:   10700, Reward: [-477.751 -477.751 -477.751] [73.101], Avg: [-476.542 -476.542 -476.542] (0.3421) ({r_i: None, r_t: [-864.821 -864.821 -864.821], eps: 0.342})
Step:  119500, Reward: [-367.087 -367.087 -367.087] [82.731], Avg: [-428.065 -428.065 -428.065] (1.0000) ({r_i: None, r_t: [-811.047 -811.047 -811.047], eps: 1.0})
Step:   10800, Reward: [-435.880 -435.880 -435.880] [78.068], Avg: [-476.169 -476.169 -476.169] (0.3387) ({r_i: None, r_t: [-903.843 -903.843 -903.843], eps: 0.339})
Step:  119600, Reward: [-426.463 -426.463 -426.463] [73.701], Avg: [-428.064 -428.064 -428.064] (1.0000) ({r_i: None, r_t: [-785.942 -785.942 -785.942], eps: 1.0})
Step:   10900, Reward: [-431.931 -431.931 -431.931] [72.093], Avg: [-475.767 -475.767 -475.767] (0.3353) ({r_i: None, r_t: [-886.218 -886.218 -886.218], eps: 0.335})
Step:  119700, Reward: [-420.071 -420.071 -420.071] [77.843], Avg: [-428.057 -428.057 -428.057] (1.0000) ({r_i: None, r_t: [-802.019 -802.019 -802.019], eps: 1.0})
Step:   11000, Reward: [-449.324 -449.324 -449.324] [63.991], Avg: [-475.529 -475.529 -475.529] (0.3320) ({r_i: None, r_t: [-891.120 -891.120 -891.120], eps: 0.332})
Step:  119800, Reward: [-397.020 -397.020 -397.020] [51.237], Avg: [-428.031 -428.031 -428.031] (1.0000) ({r_i: None, r_t: [-777.377 -777.377 -777.377], eps: 1.0})
Step:   11100, Reward: [-477.586 -477.586 -477.586] [82.866], Avg: [-475.547 -475.547 -475.547] (0.3286) ({r_i: None, r_t: [-904.821 -904.821 -904.821], eps: 0.329})
Step:  119900, Reward: [-385.755 -385.755 -385.755] [70.814], Avg: [-427.996 -427.996 -427.996] (1.0000) ({r_i: None, r_t: [-799.707 -799.707 -799.707], eps: 1.0})
Step:   11200, Reward: [-448.093 -448.093 -448.093] [90.423], Avg: [-475.304 -475.304 -475.304] (0.3254) ({r_i: None, r_t: [-897.928 -897.928 -897.928], eps: 0.325})
Step:  120000, Reward: [-392.008 -392.008 -392.008] [64.528], Avg: [-427.966 -427.966 -427.966] (1.0000) ({r_i: None, r_t: [-769.240 -769.240 -769.240], eps: 1.0})
Step:   11300, Reward: [-459.631 -459.631 -459.631] [106.162], Avg: [-475.167 -475.167 -475.167] (0.3221) ({r_i: None, r_t: [-871.698 -871.698 -871.698], eps: 0.322})
Step:  120100, Reward: [-371.248 -371.248 -371.248] [65.892], Avg: [-427.919 -427.919 -427.919] (1.0000) ({r_i: None, r_t: [-812.504 -812.504 -812.504], eps: 1.0})
Step:   11400, Reward: [-434.159 -434.159 -434.159] [54.583], Avg: [-474.810 -474.810 -474.810] (0.3189) ({r_i: None, r_t: [-917.450 -917.450 -917.450], eps: 0.319})
Step:  120200, Reward: [-413.845 -413.845 -413.845] [42.401], Avg: [-427.907 -427.907 -427.907] (1.0000) ({r_i: None, r_t: [-791.740 -791.740 -791.740], eps: 1.0})
Step:   11500, Reward: [-452.553 -452.553 -452.553] [50.943], Avg: [-474.618 -474.618 -474.618] (0.3157) ({r_i: None, r_t: [-912.074 -912.074 -912.074], eps: 0.316})
Step:  120300, Reward: [-387.635 -387.635 -387.635] [54.882], Avg: [-427.874 -427.874 -427.874] (1.0000) ({r_i: None, r_t: [-786.150 -786.150 -786.150], eps: 1.0})
Step:   11600, Reward: [-421.502 -421.502 -421.502] [72.961], Avg: [-474.164 -474.164 -474.164] (0.3126) ({r_i: None, r_t: [-889.822 -889.822 -889.822], eps: 0.313})
Step:  120400, Reward: [-423.872 -423.872 -423.872] [76.308], Avg: [-427.871 -427.871 -427.871] (1.0000) ({r_i: None, r_t: [-828.774 -828.774 -828.774], eps: 1.0})
Step:   11700, Reward: [-409.859 -409.859 -409.859] [61.754], Avg: [-473.620 -473.620 -473.620] (0.3095) ({r_i: None, r_t: [-858.465 -858.465 -858.465], eps: 0.309})
Step:  120500, Reward: [-420.374 -420.374 -420.374] [65.204], Avg: [-427.864 -427.864 -427.864] (1.0000) ({r_i: None, r_t: [-773.971 -773.971 -773.971], eps: 1.0})
Step:   11800, Reward: [-422.006 -422.006 -422.006] [45.647], Avg: [-473.186 -473.186 -473.186] (0.3064) ({r_i: None, r_t: [-857.703 -857.703 -857.703], eps: 0.306})
Step:  120600, Reward: [-413.978 -413.978 -413.978] [65.467], Avg: [-427.853 -427.853 -427.853] (1.0000) ({r_i: None, r_t: [-805.221 -805.221 -805.221], eps: 1.0})
Step:   11900, Reward: [-424.884 -424.884 -424.884] [68.679], Avg: [-472.783 -472.783 -472.783] (0.3033) ({r_i: None, r_t: [-841.546 -841.546 -841.546], eps: 0.303})
Step:  120700, Reward: [-395.317 -395.317 -395.317] [65.849], Avg: [-427.826 -427.826 -427.826] (1.0000) ({r_i: None, r_t: [-817.157 -817.157 -817.157], eps: 1.0})
Step:   12000, Reward: [-433.537 -433.537 -433.537] [93.985], Avg: [-472.459 -472.459 -472.459] (0.3003) ({r_i: None, r_t: [-862.469 -862.469 -862.469], eps: 0.3})
Step:  120800, Reward: [-409.418 -409.418 -409.418] [72.558], Avg: [-427.811 -427.811 -427.811] (1.0000) ({r_i: None, r_t: [-851.559 -851.559 -851.559], eps: 1.0})
Step:   12100, Reward: [-410.042 -410.042 -410.042] [66.628], Avg: [-471.947 -471.947 -471.947] (0.2973) ({r_i: None, r_t: [-862.992 -862.992 -862.992], eps: 0.297})
Step:  120900, Reward: [-420.416 -420.416 -420.416] [58.758], Avg: [-427.805 -427.805 -427.805] (1.0000) ({r_i: None, r_t: [-802.939 -802.939 -802.939], eps: 1.0})
Step:   12200, Reward: [-407.287 -407.287 -407.287] [47.401], Avg: [-471.422 -471.422 -471.422] (0.2943) ({r_i: None, r_t: [-884.087 -884.087 -884.087], eps: 0.294})
Step:  121000, Reward: [-390.698 -390.698 -390.698] [67.891], Avg: [-427.774 -427.774 -427.774] (1.0000) ({r_i: None, r_t: [-825.138 -825.138 -825.138], eps: 1.0})
Step:   12300, Reward: [-460.919 -460.919 -460.919] [76.429], Avg: [-471.337 -471.337 -471.337] (0.2914) ({r_i: None, r_t: [-855.542 -855.542 -855.542], eps: 0.291})
Step:  121100, Reward: [-366.763 -366.763 -366.763] [43.861], Avg: [-427.724 -427.724 -427.724] (1.0000) ({r_i: None, r_t: [-843.619 -843.619 -843.619], eps: 1.0})
Step:   12400, Reward: [-439.422 -439.422 -439.422] [84.834], Avg: [-471.082 -471.082 -471.082] (0.2885) ({r_i: None, r_t: [-842.878 -842.878 -842.878], eps: 0.288})
Step:  121200, Reward: [-376.630 -376.630 -376.630] [60.068], Avg: [-427.682 -427.682 -427.682] (1.0000) ({r_i: None, r_t: [-772.332 -772.332 -772.332], eps: 1.0})
Step:   12500, Reward: [-440.633 -440.633 -440.633] [96.310], Avg: [-470.840 -470.840 -470.840] (0.2856) ({r_i: None, r_t: [-844.191 -844.191 -844.191], eps: 0.286})
Step:  121300, Reward: [-416.461 -416.461 -416.461] [72.414], Avg: [-427.672 -427.672 -427.672] (1.0000) ({r_i: None, r_t: [-780.581 -780.581 -780.581], eps: 1.0})
Step:   12600, Reward: [-419.154 -419.154 -419.154] [85.978], Avg: [-470.433 -470.433 -470.433] (0.2828) ({r_i: None, r_t: [-859.863 -859.863 -859.863], eps: 0.283})
Step:  121400, Reward: [-403.183 -403.183 -403.183] [70.733], Avg: [-427.652 -427.652 -427.652] (1.0000) ({r_i: None, r_t: [-842.814 -842.814 -842.814], eps: 1.0})
Step:   12700, Reward: [-419.259 -419.259 -419.259] [72.951], Avg: [-470.033 -470.033 -470.033] (0.2799) ({r_i: None, r_t: [-806.378 -806.378 -806.378], eps: 0.28})
Step:  121500, Reward: [-409.381 -409.381 -409.381] [54.878], Avg: [-427.637 -427.637 -427.637] (1.0000) ({r_i: None, r_t: [-818.064 -818.064 -818.064], eps: 1.0})
Step:   12800, Reward: [-414.882 -414.882 -414.882] [48.751], Avg: [-469.606 -469.606 -469.606] (0.2771) ({r_i: None, r_t: [-839.413 -839.413 -839.413], eps: 0.277})
Step:  121600, Reward: [-424.221 -424.221 -424.221] [63.257], Avg: [-427.634 -427.634 -427.634] (1.0000) ({r_i: None, r_t: [-795.203 -795.203 -795.203], eps: 1.0})
Step:   12900, Reward: [-420.967 -420.967 -420.967] [66.031], Avg: [-469.232 -469.232 -469.232] (0.2744) ({r_i: None, r_t: [-883.222 -883.222 -883.222], eps: 0.274})
Step:  121700, Reward: [-405.168 -405.168 -405.168] [82.307], Avg: [-427.616 -427.616 -427.616] (1.0000) ({r_i: None, r_t: [-823.193 -823.193 -823.193], eps: 1.0})
Step:   13000, Reward: [-397.924 -397.924 -397.924] [71.472], Avg: [-468.687 -468.687 -468.687] (0.2716) ({r_i: None, r_t: [-873.586 -873.586 -873.586], eps: 0.272})
Step:  121800, Reward: [-405.109 -405.109 -405.109] [61.457], Avg: [-427.597 -427.597 -427.597] (1.0000) ({r_i: None, r_t: [-823.576 -823.576 -823.576], eps: 1.0})
Step:   13100, Reward: [-413.390 -413.390 -413.390] [56.796], Avg: [-468.268 -468.268 -468.268] (0.2689) ({r_i: None, r_t: [-846.434 -846.434 -846.434], eps: 0.269})
Step:  121900, Reward: [-398.641 -398.641 -398.641] [64.531], Avg: [-427.574 -427.574 -427.574] (1.0000) ({r_i: None, r_t: [-772.342 -772.342 -772.342], eps: 1.0})
Step:   13200, Reward: [-412.770 -412.770 -412.770] [49.931], Avg: [-467.851 -467.851 -467.851] (0.2663) ({r_i: None, r_t: [-884.199 -884.199 -884.199], eps: 0.266})
Step:  122000, Reward: [-364.370 -364.370 -364.370] [54.077], Avg: [-427.522 -427.522 -427.522] (1.0000) ({r_i: None, r_t: [-801.689 -801.689 -801.689], eps: 1.0})
Step:   13300, Reward: [-409.880 -409.880 -409.880] [62.511], Avg: [-467.418 -467.418 -467.418] (0.2636) ({r_i: None, r_t: [-872.004 -872.004 -872.004], eps: 0.264})
Step:  122100, Reward: [-382.147 -382.147 -382.147] [61.446], Avg: [-427.485 -427.485 -427.485] (1.0000) ({r_i: None, r_t: [-786.444 -786.444 -786.444], eps: 1.0})
Step:   13400, Reward: [-428.326 -428.326 -428.326] [79.372], Avg: [-467.129 -467.129 -467.129] (0.2610) ({r_i: None, r_t: [-837.573 -837.573 -837.573], eps: 0.261})
Step:  122200, Reward: [-381.099 -381.099 -381.099] [71.737], Avg: [-427.447 -427.447 -427.447] (1.0000) ({r_i: None, r_t: [-810.225 -810.225 -810.225], eps: 1.0})
Step:   13500, Reward: [-442.059 -442.059 -442.059] [66.986], Avg: [-466.944 -466.944 -466.944] (0.2584) ({r_i: None, r_t: [-851.650 -851.650 -851.650], eps: 0.258})
Step:  122300, Reward: [-381.436 -381.436 -381.436] [59.307], Avg: [-427.409 -427.409 -427.409] (1.0000) ({r_i: None, r_t: [-839.653 -839.653 -839.653], eps: 1.0})
Step:   13600, Reward: [-423.680 -423.680 -423.680] [51.996], Avg: [-466.629 -466.629 -466.629] (0.2558) ({r_i: None, r_t: [-827.724 -827.724 -827.724], eps: 0.256})
Step:  122400, Reward: [-372.392 -372.392 -372.392] [87.928], Avg: [-427.364 -427.364 -427.364] (1.0000) ({r_i: None, r_t: [-818.429 -818.429 -818.429], eps: 1.0})
Step:   13700, Reward: [-446.833 -446.833 -446.833] [60.384], Avg: [-466.485 -466.485 -466.485] (0.2532) ({r_i: None, r_t: [-837.746 -837.746 -837.746], eps: 0.253})
Step:  122500, Reward: [-388.349 -388.349 -388.349] [57.554], Avg: [-427.332 -427.332 -427.332] (1.0000) ({r_i: None, r_t: [-778.359 -778.359 -778.359], eps: 1.0})
Step:   13800, Reward: [-443.905 -443.905 -443.905] [53.123], Avg: [-466.323 -466.323 -466.323] (0.2507) ({r_i: None, r_t: [-857.381 -857.381 -857.381], eps: 0.251})
Step:  122600, Reward: [-408.220 -408.220 -408.220] [74.706], Avg: [-427.317 -427.317 -427.317] (1.0000) ({r_i: None, r_t: [-835.292 -835.292 -835.292], eps: 1.0})
Step:   13900, Reward: [-431.376 -431.376 -431.376] [79.197], Avg: [-466.073 -466.073 -466.073] (0.2482) ({r_i: None, r_t: [-834.590 -834.590 -834.590], eps: 0.248})
Step:  122700, Reward: [-388.851 -388.851 -388.851] [73.002], Avg: [-427.286 -427.286 -427.286] (1.0000) ({r_i: None, r_t: [-797.303 -797.303 -797.303], eps: 1.0})
Step:   14000, Reward: [-408.463 -408.463 -408.463] [70.818], Avg: [-465.665 -465.665 -465.665] (0.2457) ({r_i: None, r_t: [-864.474 -864.474 -864.474], eps: 0.246})
Step:  122800, Reward: [-392.587 -392.587 -392.587] [94.173], Avg: [-427.257 -427.257 -427.257] (1.0000) ({r_i: None, r_t: [-833.488 -833.488 -833.488], eps: 1.0})
Step:   14100, Reward: [-431.171 -431.171 -431.171] [57.489], Avg: [-465.422 -465.422 -465.422] (0.2433) ({r_i: None, r_t: [-859.358 -859.358 -859.358], eps: 0.243})
Step:  122900, Reward: [-388.177 -388.177 -388.177] [63.447], Avg: [-427.226 -427.226 -427.226] (1.0000) ({r_i: None, r_t: [-743.130 -743.130 -743.130], eps: 1.0})
Step:   14200, Reward: [-406.908 -406.908 -406.908] [69.196], Avg: [-465.012 -465.012 -465.012] (0.2409) ({r_i: None, r_t: [-822.630 -822.630 -822.630], eps: 0.241})
Step:  123000, Reward: [-384.783 -384.783 -384.783] [80.112], Avg: [-427.191 -427.191 -427.191] (1.0000) ({r_i: None, r_t: [-741.172 -741.172 -741.172], eps: 1.0})
Step:   14300, Reward: [-412.766 -412.766 -412.766] [84.153], Avg: [-464.650 -464.650 -464.650] (0.2385) ({r_i: None, r_t: [-850.075 -850.075 -850.075], eps: 0.238})
Step:  123100, Reward: [-369.882 -369.882 -369.882] [65.708], Avg: [-427.145 -427.145 -427.145] (1.0000) ({r_i: None, r_t: [-776.001 -776.001 -776.001], eps: 1.0})
Step:   14400, Reward: [-445.614 -445.614 -445.614] [63.997], Avg: [-464.518 -464.518 -464.518] (0.2361) ({r_i: None, r_t: [-857.199 -857.199 -857.199], eps: 0.236})
Step:  123200, Reward: [-427.617 -427.617 -427.617] [64.906], Avg: [-427.145 -427.145 -427.145] (1.0000) ({r_i: None, r_t: [-734.624 -734.624 -734.624], eps: 1.0})
Step:   14500, Reward: [-452.362 -452.362 -452.362] [57.280], Avg: [-464.435 -464.435 -464.435] (0.2337) ({r_i: None, r_t: [-837.542 -837.542 -837.542], eps: 0.234})
Step:  123300, Reward: [-342.434 -342.434 -342.434] [49.379], Avg: [-427.076 -427.076 -427.076] (1.0000) ({r_i: None, r_t: [-799.387 -799.387 -799.387], eps: 1.0})
Step:   14600, Reward: [-410.940 -410.940 -410.940] [50.242], Avg: [-464.071 -464.071 -464.071] (0.2314) ({r_i: None, r_t: [-891.341 -891.341 -891.341], eps: 0.231})
Step:  123400, Reward: [-393.806 -393.806 -393.806] [83.201], Avg: [-427.049 -427.049 -427.049] (1.0000) ({r_i: None, r_t: [-786.706 -786.706 -786.706], eps: 1.0})
Step:   14700, Reward: [-394.036 -394.036 -394.036] [68.442], Avg: [-463.598 -463.598 -463.598] (0.2291) ({r_i: None, r_t: [-863.613 -863.613 -863.613], eps: 0.229})
Step:  123500, Reward: [-398.039 -398.039 -398.039] [59.625], Avg: [-427.026 -427.026 -427.026] (1.0000) ({r_i: None, r_t: [-779.999 -779.999 -779.999], eps: 1.0})
Step:   14800, Reward: [-404.924 -404.924 -404.924] [67.060], Avg: [-463.204 -463.204 -463.204] (0.2268) ({r_i: None, r_t: [-872.662 -872.662 -872.662], eps: 0.227})
Step:  123600, Reward: [-392.093 -392.093 -392.093] [81.239], Avg: [-426.998 -426.998 -426.998] (1.0000) ({r_i: None, r_t: [-730.539 -730.539 -730.539], eps: 1.0})
Step:   14900, Reward: [-404.931 -404.931 -404.931] [56.399], Avg: [-462.816 -462.816 -462.816] (0.2245) ({r_i: None, r_t: [-828.335 -828.335 -828.335], eps: 0.225})
Step:  123700, Reward: [-385.606 -385.606 -385.606] [66.543], Avg: [-426.964 -426.964 -426.964] (1.0000) ({r_i: None, r_t: [-787.298 -787.298 -787.298], eps: 1.0})
Step:   15000, Reward: [-385.519 -385.519 -385.519] [44.648], Avg: [-462.304 -462.304 -462.304] (0.2223) ({r_i: None, r_t: [-826.293 -826.293 -826.293], eps: 0.222})
Step:  123800, Reward: [-381.927 -381.927 -381.927] [48.383], Avg: [-426.928 -426.928 -426.928] (1.0000) ({r_i: None, r_t: [-798.557 -798.557 -798.557], eps: 1.0})
Step:   15100, Reward: [-408.272 -408.272 -408.272] [64.997], Avg: [-461.948 -461.948 -461.948] (0.2201) ({r_i: None, r_t: [-856.653 -856.653 -856.653], eps: 0.22})
Step:  123900, Reward: [-369.078 -369.078 -369.078] [63.122], Avg: [-426.881 -426.881 -426.881] (1.0000) ({r_i: None, r_t: [-749.880 -749.880 -749.880], eps: 1.0})
Step:   15200, Reward: [-380.135 -380.135 -380.135] [55.938], Avg: [-461.414 -461.414 -461.414] (0.2179) ({r_i: None, r_t: [-814.285 -814.285 -814.285], eps: 0.218})
Step:  124000, Reward: [-370.714 -370.714 -370.714] [54.425], Avg: [-426.836 -426.836 -426.836] (1.0000) ({r_i: None, r_t: [-759.875 -759.875 -759.875], eps: 1.0})
Step:   15300, Reward: [-395.796 -395.796 -395.796] [56.888], Avg: [-460.988 -460.988 -460.988] (0.2157) ({r_i: None, r_t: [-876.345 -876.345 -876.345], eps: 0.216})
Step:  124100, Reward: [-371.308 -371.308 -371.308] [95.099], Avg: [-426.791 -426.791 -426.791] (1.0000) ({r_i: None, r_t: [-756.804 -756.804 -756.804], eps: 1.0})
Step:   15400, Reward: [-395.166 -395.166 -395.166] [42.740], Avg: [-460.563 -460.563 -460.563] (0.2136) ({r_i: None, r_t: [-808.009 -808.009 -808.009], eps: 0.214})
Step:  124200, Reward: [-349.763 -349.763 -349.763] [54.669], Avg: [-426.729 -426.729 -426.729] (1.0000) ({r_i: None, r_t: [-785.741 -785.741 -785.741], eps: 1.0})
Step:   15500, Reward: [-400.860 -400.860 -400.860] [57.198], Avg: [-460.180 -460.180 -460.180] (0.2114) ({r_i: None, r_t: [-805.975 -805.975 -805.975], eps: 0.211})
Step:  124300, Reward: [-385.229 -385.229 -385.229] [80.774], Avg: [-426.696 -426.696 -426.696] (1.0000) ({r_i: None, r_t: [-793.211 -793.211 -793.211], eps: 1.0})
Step:   15600, Reward: [-410.235 -410.235 -410.235] [63.251], Avg: [-459.862 -459.862 -459.862] (0.2093) ({r_i: None, r_t: [-834.975 -834.975 -834.975], eps: 0.209})
Step:  124400, Reward: [-412.345 -412.345 -412.345] [94.126], Avg: [-426.684 -426.684 -426.684] (1.0000) ({r_i: None, r_t: [-757.118 -757.118 -757.118], eps: 1.0})
Step:   15700, Reward: [-425.141 -425.141 -425.141] [79.421], Avg: [-459.642 -459.642 -459.642] (0.2072) ({r_i: None, r_t: [-840.125 -840.125 -840.125], eps: 0.207})
Step:  124500, Reward: [-399.934 -399.934 -399.934] [77.687], Avg: [-426.663 -426.663 -426.663] (1.0000) ({r_i: None, r_t: [-776.618 -776.618 -776.618], eps: 1.0})
Step:   15800, Reward: [-384.182 -384.182 -384.182] [54.378], Avg: [-459.168 -459.168 -459.168] (0.2052) ({r_i: None, r_t: [-825.741 -825.741 -825.741], eps: 0.205})
Step:  124600, Reward: [-384.122 -384.122 -384.122] [70.374], Avg: [-426.629 -426.629 -426.629] (1.0000) ({r_i: None, r_t: [-743.307 -743.307 -743.307], eps: 1.0})
Step:   15900, Reward: [-431.948 -431.948 -431.948] [74.238], Avg: [-458.998 -458.998 -458.998] (0.2031) ({r_i: None, r_t: [-791.526 -791.526 -791.526], eps: 0.203})
Step:  124700, Reward: [-385.921 -385.921 -385.921] [77.872], Avg: [-426.596 -426.596 -426.596] (1.0000) ({r_i: None, r_t: [-766.654 -766.654 -766.654], eps: 1.0})
Step:   16000, Reward: [-391.972 -391.972 -391.972] [54.083], Avg: [-458.581 -458.581 -458.581] (0.2011) ({r_i: None, r_t: [-813.159 -813.159 -813.159], eps: 0.201})
Step:  124800, Reward: [-384.664 -384.664 -384.664] [73.059], Avg: [-426.563 -426.563 -426.563] (1.0000) ({r_i: None, r_t: [-801.471 -801.471 -801.471], eps: 1.0})
Step:   16100, Reward: [-404.818 -404.818 -404.818] [70.533], Avg: [-458.249 -458.249 -458.249] (0.1991) ({r_i: None, r_t: [-821.637 -821.637 -821.637], eps: 0.199})
Step:  124900, Reward: [-375.367 -375.367 -375.367] [82.009], Avg: [-426.522 -426.522 -426.522] (1.0000) ({r_i: None, r_t: [-751.410 -751.410 -751.410], eps: 1.0})
Step:   16200, Reward: [-433.581 -433.581 -433.581] [55.216], Avg: [-458.098 -458.098 -458.098] (0.1971) ({r_i: None, r_t: [-831.724 -831.724 -831.724], eps: 0.197})
Step:  125000, Reward: [-384.777 -384.777 -384.777] [67.874], Avg: [-426.488 -426.488 -426.488] (1.0000) ({r_i: None, r_t: [-776.397 -776.397 -776.397], eps: 1.0})
Step:   16300, Reward: [-387.035 -387.035 -387.035] [53.896], Avg: [-457.665 -457.665 -457.665] (0.1951) ({r_i: None, r_t: [-816.599 -816.599 -816.599], eps: 0.195})
Step:  125100, Reward: [-365.689 -365.689 -365.689] [62.736], Avg: [-426.440 -426.440 -426.440] (1.0000) ({r_i: None, r_t: [-750.970 -750.970 -750.970], eps: 1.0})
Step:   16400, Reward: [-395.066 -395.066 -395.066] [59.675], Avg: [-457.285 -457.285 -457.285] (0.1932) ({r_i: None, r_t: [-829.931 -829.931 -829.931], eps: 0.193})
Step:  125200, Reward: [-387.036 -387.036 -387.036] [68.848], Avg: [-426.408 -426.408 -426.408] (1.0000) ({r_i: None, r_t: [-771.390 -771.390 -771.390], eps: 1.0})
Step:   16500, Reward: [-432.484 -432.484 -432.484] [66.926], Avg: [-457.136 -457.136 -457.136] (0.1913) ({r_i: None, r_t: [-852.800 -852.800 -852.800], eps: 0.191})
Step:  125300, Reward: [-400.958 -400.958 -400.958] [60.037], Avg: [-426.388 -426.388 -426.388] (1.0000) ({r_i: None, r_t: [-791.898 -791.898 -791.898], eps: 1.0})
Step:   16600, Reward: [-415.432 -415.432 -415.432] [70.974], Avg: [-456.886 -456.886 -456.886] (0.1893) ({r_i: None, r_t: [-800.376 -800.376 -800.376], eps: 0.189})
Step:  125400, Reward: [-403.872 -403.872 -403.872] [52.273], Avg: [-426.370 -426.370 -426.370] (1.0000) ({r_i: None, r_t: [-765.401 -765.401 -765.401], eps: 1.0})
Step:   16700, Reward: [-411.137 -411.137 -411.137] [66.951], Avg: [-456.614 -456.614 -456.614] (0.1875) ({r_i: None, r_t: [-813.455 -813.455 -813.455], eps: 0.187})
Step:  125500, Reward: [-423.909 -423.909 -423.909] [87.931], Avg: [-426.368 -426.368 -426.368] (1.0000) ({r_i: None, r_t: [-749.187 -749.187 -749.187], eps: 1.0})
Step:   16800, Reward: [-425.140 -425.140 -425.140] [77.201], Avg: [-456.428 -456.428 -456.428] (0.1856) ({r_i: None, r_t: [-807.832 -807.832 -807.832], eps: 0.186})
Step:  125600, Reward: [-439.529 -439.529 -439.529] [102.048], Avg: [-426.379 -426.379 -426.379] (1.0000) ({r_i: None, r_t: [-787.083 -787.083 -787.083], eps: 1.0})
Step:   16900, Reward: [-410.655 -410.655 -410.655] [48.464], Avg: [-456.158 -456.158 -456.158] (0.1837) ({r_i: None, r_t: [-819.499 -819.499 -819.499], eps: 0.184})
Step:  125700, Reward: [-386.610 -386.610 -386.610] [60.973], Avg: [-426.347 -426.347 -426.347] (1.0000) ({r_i: None, r_t: [-810.670 -810.670 -810.670], eps: 1.0})
Step:   17000, Reward: [-452.059 -452.059 -452.059] [60.778], Avg: [-456.134 -456.134 -456.134] (0.1819) ({r_i: None, r_t: [-840.083 -840.083 -840.083], eps: 0.182})
Step:  125800, Reward: [-361.767 -361.767 -361.767] [67.493], Avg: [-426.296 -426.296 -426.296] (1.0000) ({r_i: None, r_t: [-772.520 -772.520 -772.520], eps: 1.0})
Step:   17100, Reward: [-384.603 -384.603 -384.603] [48.263], Avg: [-455.719 -455.719 -455.719] (0.1801) ({r_i: None, r_t: [-823.999 -823.999 -823.999], eps: 0.18})
Step:  125900, Reward: [-367.603 -367.603 -367.603] [59.882], Avg: [-426.249 -426.249 -426.249] (1.0000) ({r_i: None, r_t: [-776.913 -776.913 -776.913], eps: 1.0})
Step:   17200, Reward: [-432.374 -432.374 -432.374] [66.818], Avg: [-455.584 -455.584 -455.584] (0.1783) ({r_i: None, r_t: [-788.653 -788.653 -788.653], eps: 0.178})
Step:  126000, Reward: [-412.736 -412.736 -412.736] [61.820], Avg: [-426.238 -426.238 -426.238] (1.0000) ({r_i: None, r_t: [-765.863 -765.863 -765.863], eps: 1.0})
Step:   17300, Reward: [-437.065 -437.065 -437.065] [74.861], Avg: [-455.477 -455.477 -455.477] (0.1765) ({r_i: None, r_t: [-832.904 -832.904 -832.904], eps: 0.177})
Step:  126100, Reward: [-358.055 -358.055 -358.055] [67.612], Avg: [-426.184 -426.184 -426.184] (1.0000) ({r_i: None, r_t: [-802.839 -802.839 -802.839], eps: 1.0})
Step:   17400, Reward: [-400.566 -400.566 -400.566] [55.771], Avg: [-455.163 -455.163 -455.163] (0.1748) ({r_i: None, r_t: [-842.809 -842.809 -842.809], eps: 0.175})
Step:  126200, Reward: [-437.671 -437.671 -437.671] [85.231], Avg: [-426.193 -426.193 -426.193] (1.0000) ({r_i: None, r_t: [-748.583 -748.583 -748.583], eps: 1.0})
Step:   17500, Reward: [-445.387 -445.387 -445.387] [59.721], Avg: [-455.108 -455.108 -455.108] (0.1730) ({r_i: None, r_t: [-836.051 -836.051 -836.051], eps: 0.173})
Step:  126300, Reward: [-393.537 -393.537 -393.537] [75.158], Avg: [-426.168 -426.168 -426.168] (1.0000) ({r_i: None, r_t: [-780.748 -780.748 -780.748], eps: 1.0})
Step:   17600, Reward: [-408.048 -408.048 -408.048] [54.753], Avg: [-454.842 -454.842 -454.842] (0.1713) ({r_i: None, r_t: [-829.197 -829.197 -829.197], eps: 0.171})
Step:  126400, Reward: [-411.576 -411.576 -411.576] [73.978], Avg: [-426.156 -426.156 -426.156] (1.0000) ({r_i: None, r_t: [-752.680 -752.680 -752.680], eps: 1.0})
Step:   17700, Reward: [-441.944 -441.944 -441.944] [59.657], Avg: [-454.770 -454.770 -454.770] (0.1696) ({r_i: None, r_t: [-814.446 -814.446 -814.446], eps: 0.17})
Step:  126500, Reward: [-369.841 -369.841 -369.841] [79.842], Avg: [-426.112 -426.112 -426.112] (1.0000) ({r_i: None, r_t: [-728.960 -728.960 -728.960], eps: 1.0})
Step:   17800, Reward: [-424.007 -424.007 -424.007] [74.462], Avg: [-454.598 -454.598 -454.598] (0.1679) ({r_i: None, r_t: [-838.885 -838.885 -838.885], eps: 0.168})
Step:  126600, Reward: [-389.679 -389.679 -389.679] [62.128], Avg: [-426.083 -426.083 -426.083] (1.0000) ({r_i: None, r_t: [-763.450 -763.450 -763.450], eps: 1.0})
Step:   17900, Reward: [-413.212 -413.212 -413.212] [57.265], Avg: [-454.368 -454.368 -454.368] (0.1662) ({r_i: None, r_t: [-852.845 -852.845 -852.845], eps: 0.166})
Step:  126700, Reward: [-376.131 -376.131 -376.131] [63.005], Avg: [-426.043 -426.043 -426.043] (1.0000) ({r_i: None, r_t: [-734.583 -734.583 -734.583], eps: 1.0})
Step:   18000, Reward: [-389.123 -389.123 -389.123] [42.193], Avg: [-454.007 -454.007 -454.007] (0.1646) ({r_i: None, r_t: [-832.181 -832.181 -832.181], eps: 0.165})
Step:  126800, Reward: [-375.553 -375.553 -375.553] [64.288], Avg: [-426.004 -426.004 -426.004] (1.0000) ({r_i: None, r_t: [-714.279 -714.279 -714.279], eps: 1.0})
Step:   18100, Reward: [-421.212 -421.212 -421.212] [57.573], Avg: [-453.827 -453.827 -453.827] (0.1629) ({r_i: None, r_t: [-833.916 -833.916 -833.916], eps: 0.163})
Step:  126900, Reward: [-389.819 -389.819 -389.819] [53.020], Avg: [-425.975 -425.975 -425.975] (1.0000) ({r_i: None, r_t: [-761.342 -761.342 -761.342], eps: 1.0})
Step:   18200, Reward: [-413.227 -413.227 -413.227] [51.393], Avg: [-453.605 -453.605 -453.605] (0.1613) ({r_i: None, r_t: [-829.843 -829.843 -829.843], eps: 0.161})
Step:  127000, Reward: [-401.678 -401.678 -401.678] [64.674], Avg: [-425.956 -425.956 -425.956] (1.0000) ({r_i: None, r_t: [-781.236 -781.236 -781.236], eps: 1.0})
Step:   18300, Reward: [-413.043 -413.043 -413.043] [69.638], Avg: [-453.385 -453.385 -453.385] (0.1597) ({r_i: None, r_t: [-833.744 -833.744 -833.744], eps: 0.16})
Step:  127100, Reward: [-399.454 -399.454 -399.454] [76.088], Avg: [-425.935 -425.935 -425.935] (1.0000) ({r_i: None, r_t: [-797.801 -797.801 -797.801], eps: 1.0})
Step:   18400, Reward: [-420.059 -420.059 -420.059] [69.222], Avg: [-453.205 -453.205 -453.205] (0.1581) ({r_i: None, r_t: [-808.618 -808.618 -808.618], eps: 0.158})
Step:  127200, Reward: [-381.333 -381.333 -381.333] [61.129], Avg: [-425.900 -425.900 -425.900] (1.0000) ({r_i: None, r_t: [-772.123 -772.123 -772.123], eps: 1.0})
Step:   18500, Reward: [-393.926 -393.926 -393.926] [66.026], Avg: [-452.886 -452.886 -452.886] (0.1565) ({r_i: None, r_t: [-845.040 -845.040 -845.040], eps: 0.157})
Step:  127300, Reward: [-388.767 -388.767 -388.767] [75.790], Avg: [-425.871 -425.871 -425.871] (1.0000) ({r_i: None, r_t: [-749.757 -749.757 -749.757], eps: 1.0})
Step:   18600, Reward: [-411.452 -411.452 -411.452] [80.284], Avg: [-452.664 -452.664 -452.664] (0.1549) ({r_i: None, r_t: [-807.466 -807.466 -807.466], eps: 0.155})
Step:  127400, Reward: [-358.758 -358.758 -358.758] [62.416], Avg: [-425.818 -425.818 -425.818] (1.0000) ({r_i: None, r_t: [-754.444 -754.444 -754.444], eps: 1.0})
Step:   18700, Reward: [-422.833 -422.833 -422.833] [46.801], Avg: [-452.506 -452.506 -452.506] (0.1534) ({r_i: None, r_t: [-818.641 -818.641 -818.641], eps: 0.153})
Step:  127500, Reward: [-372.820 -372.820 -372.820] [80.619], Avg: [-425.777 -425.777 -425.777] (1.0000) ({r_i: None, r_t: [-790.401 -790.401 -790.401], eps: 1.0})
Step:   18800, Reward: [-428.446 -428.446 -428.446] [62.274], Avg: [-452.378 -452.378 -452.378] (0.1519) ({r_i: None, r_t: [-799.566 -799.566 -799.566], eps: 0.152})
Step:  127600, Reward: [-405.494 -405.494 -405.494] [58.408], Avg: [-425.761 -425.761 -425.761] (1.0000) ({r_i: None, r_t: [-768.035 -768.035 -768.035], eps: 1.0})
Step:   18900, Reward: [-392.353 -392.353 -392.353] [73.581], Avg: [-452.062 -452.062 -452.062] (0.1504) ({r_i: None, r_t: [-777.924 -777.924 -777.924], eps: 0.15})
Step:  127700, Reward: [-380.406 -380.406 -380.406] [42.431], Avg: [-425.725 -425.725 -425.725] (1.0000) ({r_i: None, r_t: [-762.217 -762.217 -762.217], eps: 1.0})
Step:   19000, Reward: [-425.690 -425.690 -425.690] [74.587], Avg: [-451.924 -451.924 -451.924] (0.1489) ({r_i: None, r_t: [-860.600 -860.600 -860.600], eps: 0.149})
Step:  127800, Reward: [-351.027 -351.027 -351.027] [63.038], Avg: [-425.667 -425.667 -425.667] (1.0000) ({r_i: None, r_t: [-748.301 -748.301 -748.301], eps: 1.0})
Step:   19100, Reward: [-433.660 -433.660 -433.660] [81.126], Avg: [-451.829 -451.829 -451.829] (0.1474) ({r_i: None, r_t: [-798.124 -798.124 -798.124], eps: 0.147})
Step:  127900, Reward: [-352.560 -352.560 -352.560] [46.412], Avg: [-425.610 -425.610 -425.610] (1.0000) ({r_i: None, r_t: [-728.183 -728.183 -728.183], eps: 1.0})
Step:   19200, Reward: [-412.578 -412.578 -412.578] [43.809], Avg: [-451.626 -451.626 -451.626] (0.1459) ({r_i: None, r_t: [-805.000 -805.000 -805.000], eps: 0.146})
Step:  128000, Reward: [-365.996 -365.996 -365.996] [70.885], Avg: [-425.563 -425.563 -425.563] (1.0000) ({r_i: None, r_t: [-747.341 -747.341 -747.341], eps: 1.0})
Step:   19300, Reward: [-409.295 -409.295 -409.295] [64.374], Avg: [-451.408 -451.408 -451.408] (0.1444) ({r_i: None, r_t: [-828.532 -828.532 -828.532], eps: 0.144})
Step:  128100, Reward: [-403.842 -403.842 -403.842] [58.376], Avg: [-425.546 -425.546 -425.546] (1.0000) ({r_i: None, r_t: [-728.411 -728.411 -728.411], eps: 1.0})
Step:   19400, Reward: [-420.576 -420.576 -420.576] [66.305], Avg: [-451.250 -451.250 -451.250] (0.1430) ({r_i: None, r_t: [-858.398 -858.398 -858.398], eps: 0.143})
Step:  128200, Reward: [-358.770 -358.770 -358.770] [65.188], Avg: [-425.494 -425.494 -425.494] (1.0000) ({r_i: None, r_t: [-756.687 -756.687 -756.687], eps: 1.0})
Step:   19500, Reward: [-414.191 -414.191 -414.191] [51.896], Avg: [-451.060 -451.060 -451.060] (0.1416) ({r_i: None, r_t: [-792.769 -792.769 -792.769], eps: 0.142})
Step:  128300, Reward: [-341.823 -341.823 -341.823] [67.296], Avg: [-425.429 -425.429 -425.429] (1.0000) ({r_i: None, r_t: [-775.498 -775.498 -775.498], eps: 1.0})
Step:   19600, Reward: [-397.536 -397.536 -397.536] [76.735], Avg: [-450.789 -450.789 -450.789] (0.1402) ({r_i: None, r_t: [-841.315 -841.315 -841.315], eps: 0.14})
Step:  128400, Reward: [-373.382 -373.382 -373.382] [82.565], Avg: [-425.389 -425.389 -425.389] (1.0000) ({r_i: None, r_t: [-831.325 -831.325 -831.325], eps: 1.0})
Step:   19700, Reward: [-389.517 -389.517 -389.517] [47.790], Avg: [-450.479 -450.479 -450.479] (0.1388) ({r_i: None, r_t: [-819.956 -819.956 -819.956], eps: 0.139})
Step:  128500, Reward: [-378.728 -378.728 -378.728] [86.587], Avg: [-425.352 -425.352 -425.352] (1.0000) ({r_i: None, r_t: [-777.275 -777.275 -777.275], eps: 1.0})
Step:   19800, Reward: [-414.313 -414.313 -414.313] [65.310], Avg: [-450.298 -450.298 -450.298] (0.1374) ({r_i: None, r_t: [-796.764 -796.764 -796.764], eps: 0.137})
Step:  128600, Reward: [-353.305 -353.305 -353.305] [55.771], Avg: [-425.296 -425.296 -425.296] (1.0000) ({r_i: None, r_t: [-761.569 -761.569 -761.569], eps: 1.0})
Step:   19900, Reward: [-430.338 -430.338 -430.338] [86.020], Avg: [-450.198 -450.198 -450.198] (0.1360) ({r_i: None, r_t: [-775.849 -775.849 -775.849], eps: 0.136})
Step:  128700, Reward: [-375.719 -375.719 -375.719] [76.475], Avg: [-425.258 -425.258 -425.258] (1.0000) ({r_i: None, r_t: [-789.283 -789.283 -789.283], eps: 1.0})
Step:   20000, Reward: [-415.888 -415.888 -415.888] [68.956], Avg: [-450.027 -450.027 -450.027] (0.1347) ({r_i: None, r_t: [-796.716 -796.716 -796.716], eps: 0.135})
Step:  128800, Reward: [-374.959 -374.959 -374.959] [55.601], Avg: [-425.219 -425.219 -425.219] (1.0000) ({r_i: None, r_t: [-762.128 -762.128 -762.128], eps: 1.0})
Step:   20100, Reward: [-399.785 -399.785 -399.785] [64.860], Avg: [-449.778 -449.778 -449.778] (0.1333) ({r_i: None, r_t: [-796.988 -796.988 -796.988], eps: 0.133})
Step:  128900, Reward: [-370.190 -370.190 -370.190] [56.491], Avg: [-425.176 -425.176 -425.176] (1.0000) ({r_i: None, r_t: [-754.928 -754.928 -754.928], eps: 1.0})
Step:   20200, Reward: [-388.967 -388.967 -388.967] [58.294], Avg: [-449.479 -449.479 -449.479] (0.1320) ({r_i: None, r_t: [-801.571 -801.571 -801.571], eps: 0.132})
Step:  129000, Reward: [-384.636 -384.636 -384.636] [82.298], Avg: [-425.145 -425.145 -425.145] (1.0000) ({r_i: None, r_t: [-763.224 -763.224 -763.224], eps: 1.0})
Step:   20300, Reward: [-419.599 -419.599 -419.599] [48.420], Avg: [-449.332 -449.332 -449.332] (0.1307) ({r_i: None, r_t: [-826.162 -826.162 -826.162], eps: 0.131})
Step:  129100, Reward: [-364.167 -364.167 -364.167] [57.932], Avg: [-425.098 -425.098 -425.098] (1.0000) ({r_i: None, r_t: [-738.843 -738.843 -738.843], eps: 1.0})
Step:   20400, Reward: [-410.690 -410.690 -410.690] [61.468], Avg: [-449.144 -449.144 -449.144] (0.1294) ({r_i: None, r_t: [-815.013 -815.013 -815.013], eps: 0.129})
Step:  129200, Reward: [-393.501 -393.501 -393.501] [74.000], Avg: [-425.073 -425.073 -425.073] (1.0000) ({r_i: None, r_t: [-754.956 -754.956 -754.956], eps: 1.0})
Step:   20500, Reward: [-383.666 -383.666 -383.666] [61.807], Avg: [-448.826 -448.826 -448.826] (0.1281) ({r_i: None, r_t: [-825.180 -825.180 -825.180], eps: 0.128})
Step:  129300, Reward: [-341.653 -341.653 -341.653] [60.401], Avg: [-425.009 -425.009 -425.009] (1.0000) ({r_i: None, r_t: [-743.670 -743.670 -743.670], eps: 1.0})
Step:   20600, Reward: [-383.167 -383.167 -383.167] [62.788], Avg: [-448.509 -448.509 -448.509] (0.1268) ({r_i: None, r_t: [-853.430 -853.430 -853.430], eps: 0.127})
Step:  129400, Reward: [-350.953 -350.953 -350.953] [61.095], Avg: [-424.952 -424.952 -424.952] (1.0000) ({r_i: None, r_t: [-745.012 -745.012 -745.012], eps: 1.0})
Step:   20700, Reward: [-422.662 -422.662 -422.662] [81.926], Avg: [-448.385 -448.385 -448.385] (0.1255) ({r_i: None, r_t: [-853.867 -853.867 -853.867], eps: 0.126})
Step:  129500, Reward: [-361.160 -361.160 -361.160] [46.336], Avg: [-424.902 -424.902 -424.902] (1.0000) ({r_i: None, r_t: [-759.132 -759.132 -759.132], eps: 1.0})
Step:   20800, Reward: [-412.540 -412.540 -412.540] [54.634], Avg: [-448.213 -448.213 -448.213] (0.1243) ({r_i: None, r_t: [-849.367 -849.367 -849.367], eps: 0.124})
Step:  129600, Reward: [-379.761 -379.761 -379.761] [65.120], Avg: [-424.868 -424.868 -424.868] (1.0000) ({r_i: None, r_t: [-808.717 -808.717 -808.717], eps: 1.0})
Step:   20900, Reward: [-379.339 -379.339 -379.339] [63.811], Avg: [-447.885 -447.885 -447.885] (0.1230) ({r_i: None, r_t: [-776.760 -776.760 -776.760], eps: 0.123})
Step:  129700, Reward: [-391.192 -391.192 -391.192] [77.469], Avg: [-424.842 -424.842 -424.842] (1.0000) ({r_i: None, r_t: [-752.205 -752.205 -752.205], eps: 1.0})
Step:   21000, Reward: [-426.030 -426.030 -426.030] [67.126], Avg: [-447.781 -447.781 -447.781] (0.1218) ({r_i: None, r_t: [-793.746 -793.746 -793.746], eps: 0.122})
Step:  129800, Reward: [-339.597 -339.597 -339.597] [45.554], Avg: [-424.776 -424.776 -424.776] (1.0000) ({r_i: None, r_t: [-779.563 -779.563 -779.563], eps: 1.0})
Step:   21100, Reward: [-401.600 -401.600 -401.600] [46.600], Avg: [-447.564 -447.564 -447.564] (0.1206) ({r_i: None, r_t: [-808.654 -808.654 -808.654], eps: 0.121})
Step:  129900, Reward: [-357.376 -357.376 -357.376] [52.200], Avg: [-424.724 -424.724 -424.724] (1.0000) ({r_i: None, r_t: [-796.348 -796.348 -796.348], eps: 1.0})
Step:   21200, Reward: [-399.532 -399.532 -399.532] [61.507], Avg: [-447.338 -447.338 -447.338] (0.1194) ({r_i: None, r_t: [-794.214 -794.214 -794.214], eps: 0.119})
Step:  130000, Reward: [-384.846 -384.846 -384.846] [62.445], Avg: [-424.694 -424.694 -424.694] (1.0000) ({r_i: None, r_t: [-726.416 -726.416 -726.416], eps: 1.0})
Step:   21300, Reward: [-385.712 -385.712 -385.712] [57.830], Avg: [-447.050 -447.050 -447.050] (0.1182) ({r_i: None, r_t: [-786.970 -786.970 -786.970], eps: 0.118})
Step:  130100, Reward: [-357.519 -357.519 -357.519] [60.306], Avg: [-424.642 -424.642 -424.642] (1.0000) ({r_i: None, r_t: [-751.044 -751.044 -751.044], eps: 1.0})
Step:   21400, Reward: [-390.705 -390.705 -390.705] [63.227], Avg: [-446.788 -446.788 -446.788] (0.1170) ({r_i: None, r_t: [-807.729 -807.729 -807.729], eps: 0.117})
Step:  130200, Reward: [-376.025 -376.025 -376.025] [79.233], Avg: [-424.605 -424.605 -424.605] (1.0000) ({r_i: None, r_t: [-809.055 -809.055 -809.055], eps: 1.0})
Step:   21500, Reward: [-427.410 -427.410 -427.410] [69.387], Avg: [-446.698 -446.698 -446.698] (0.1159) ({r_i: None, r_t: [-829.691 -829.691 -829.691], eps: 0.116})
Step:  130300, Reward: [-372.442 -372.442 -372.442] [73.457], Avg: [-424.565 -424.565 -424.565] (1.0000) ({r_i: None, r_t: [-760.838 -760.838 -760.838], eps: 1.0})
Step:   21600, Reward: [-423.373 -423.373 -423.373] [62.862], Avg: [-446.591 -446.591 -446.591] (0.1147) ({r_i: None, r_t: [-851.878 -851.878 -851.878], eps: 0.115})
Step:  130400, Reward: [-362.825 -362.825 -362.825] [77.576], Avg: [-424.517 -424.517 -424.517] (1.0000) ({r_i: None, r_t: [-752.075 -752.075 -752.075], eps: 1.0})
Step:   21700, Reward: [-392.125 -392.125 -392.125] [69.207], Avg: [-446.341 -446.341 -446.341] (0.1136) ({r_i: None, r_t: [-818.442 -818.442 -818.442], eps: 0.114})
Step:  130500, Reward: [-405.408 -405.408 -405.408] [69.328], Avg: [-424.503 -424.503 -424.503] (1.0000) ({r_i: None, r_t: [-759.558 -759.558 -759.558], eps: 1.0})
Step:   21800, Reward: [-395.611 -395.611 -395.611] [61.701], Avg: [-446.109 -446.109 -446.109] (0.1124) ({r_i: None, r_t: [-829.981 -829.981 -829.981], eps: 0.112})
Step:  130600, Reward: [-405.679 -405.679 -405.679] [62.682], Avg: [-424.488 -424.488 -424.488] (1.0000) ({r_i: None, r_t: [-755.660 -755.660 -755.660], eps: 1.0})
Step:   21900, Reward: [-400.178 -400.178 -400.178] [53.568], Avg: [-445.901 -445.901 -445.901] (0.1113) ({r_i: None, r_t: [-831.511 -831.511 -831.511], eps: 0.111})
Step:  130700, Reward: [-341.805 -341.805 -341.805] [44.957], Avg: [-424.425 -424.425 -424.425] (1.0000) ({r_i: None, r_t: [-770.432 -770.432 -770.432], eps: 1.0})
Step:   22000, Reward: [-388.395 -388.395 -388.395] [59.837], Avg: [-445.640 -445.640 -445.640] (0.1102) ({r_i: None, r_t: [-837.987 -837.987 -837.987], eps: 0.11})
Step:  130800, Reward: [-377.324 -377.324 -377.324] [86.465], Avg: [-424.389 -424.389 -424.389] (1.0000) ({r_i: None, r_t: [-771.330 -771.330 -771.330], eps: 1.0})
Step:   22100, Reward: [-398.968 -398.968 -398.968] [51.391], Avg: [-445.430 -445.430 -445.430] (0.1091) ({r_i: None, r_t: [-828.261 -828.261 -828.261], eps: 0.109})
Step:  130900, Reward: [-386.851 -386.851 -386.851] [59.297], Avg: [-424.360 -424.360 -424.360] (1.0000) ({r_i: None, r_t: [-767.262 -767.262 -767.262], eps: 1.0})
Step:   22200, Reward: [-415.523 -415.523 -415.523] [58.269], Avg: [-445.296 -445.296 -445.296] (0.1080) ({r_i: None, r_t: [-813.094 -813.094 -813.094], eps: 0.108})
Step:  131000, Reward: [-391.265 -391.265 -391.265] [79.138], Avg: [-424.335 -424.335 -424.335] (1.0000) ({r_i: None, r_t: [-752.959 -752.959 -752.959], eps: 1.0})
Step:   22300, Reward: [-396.614 -396.614 -396.614] [80.160], Avg: [-445.079 -445.079 -445.079] (0.1069) ({r_i: None, r_t: [-831.401 -831.401 -831.401], eps: 0.107})
Step:  131100, Reward: [-369.704 -369.704 -369.704] [61.467], Avg: [-424.294 -424.294 -424.294] (1.0000) ({r_i: None, r_t: [-732.603 -732.603 -732.603], eps: 1.0})
Step:   22400, Reward: [-408.361 -408.361 -408.361] [60.900], Avg: [-444.916 -444.916 -444.916] (0.1059) ({r_i: None, r_t: [-819.712 -819.712 -819.712], eps: 0.106})
Step:  131200, Reward: [-383.602 -383.602 -383.602] [64.214], Avg: [-424.263 -424.263 -424.263] (1.0000) ({r_i: None, r_t: [-778.455 -778.455 -778.455], eps: 1.0})
Step:   22500, Reward: [-410.467 -410.467 -410.467] [54.127], Avg: [-444.763 -444.763 -444.763] (0.1048) ({r_i: None, r_t: [-807.920 -807.920 -807.920], eps: 0.105})
Step:  131300, Reward: [-370.358 -370.358 -370.358] [40.876], Avg: [-424.222 -424.222 -424.222] (1.0000) ({r_i: None, r_t: [-758.837 -758.837 -758.837], eps: 1.0})
Step:   22600, Reward: [-415.629 -415.629 -415.629] [49.019], Avg: [-444.635 -444.635 -444.635] (0.1038) ({r_i: None, r_t: [-814.350 -814.350 -814.350], eps: 0.104})
Step:  131400, Reward: [-384.631 -384.631 -384.631] [61.034], Avg: [-424.191 -424.191 -424.191] (1.0000) ({r_i: None, r_t: [-758.472 -758.472 -758.472], eps: 1.0})
Step:   22700, Reward: [-406.094 -406.094 -406.094] [64.444], Avg: [-444.466 -444.466 -444.466] (0.1027) ({r_i: None, r_t: [-858.116 -858.116 -858.116], eps: 0.103})
Step:  131500, Reward: [-382.279 -382.279 -382.279] [61.923], Avg: [-424.160 -424.160 -424.160] (1.0000) ({r_i: None, r_t: [-760.662 -760.662 -760.662], eps: 1.0})
Step:   22800, Reward: [-420.637 -420.637 -420.637] [66.015], Avg: [-444.362 -444.362 -444.362] (0.1017) ({r_i: None, r_t: [-857.145 -857.145 -857.145], eps: 0.102})
Step:  131600, Reward: [-383.514 -383.514 -383.514] [70.401], Avg: [-424.129 -424.129 -424.129] (1.0000) ({r_i: None, r_t: [-755.378 -755.378 -755.378], eps: 1.0})
Step:   22900, Reward: [-428.474 -428.474 -428.474] [48.799], Avg: [-444.293 -444.293 -444.293] (0.1007) ({r_i: None, r_t: [-860.079 -860.079 -860.079], eps: 0.101})
Step:  131700, Reward: [-393.749 -393.749 -393.749] [80.250], Avg: [-424.106 -424.106 -424.106] (1.0000) ({r_i: None, r_t: [-790.759 -790.759 -790.759], eps: 1.0})
Step:   23000, Reward: [-417.883 -417.883 -417.883] [71.557], Avg: [-444.178 -444.178 -444.178] (0.0997) ({r_i: None, r_t: [-821.671 -821.671 -821.671], eps: 0.1})
Step:  131800, Reward: [-370.059 -370.059 -370.059] [64.109], Avg: [-424.065 -424.065 -424.065] (1.0000) ({r_i: None, r_t: [-751.628 -751.628 -751.628], eps: 1.0})
Step:   23100, Reward: [-448.443 -448.443 -448.443] [56.165], Avg: [-444.197 -444.197 -444.197] (0.0987) ({r_i: None, r_t: [-846.007 -846.007 -846.007], eps: 0.099})
Step:  131900, Reward: [-378.030 -378.030 -378.030] [71.800], Avg: [-424.030 -424.030 -424.030] (1.0000) ({r_i: None, r_t: [-740.413 -740.413 -740.413], eps: 1.0})
Step:   23200, Reward: [-423.520 -423.520 -423.520] [70.979], Avg: [-444.108 -444.108 -444.108] (0.0977) ({r_i: None, r_t: [-846.478 -846.478 -846.478], eps: 0.098})
Step:  132000, Reward: [-352.599 -352.599 -352.599] [69.484], Avg: [-423.976 -423.976 -423.976] (1.0000) ({r_i: None, r_t: [-741.303 -741.303 -741.303], eps: 1.0})
Step:   23300, Reward: [-428.371 -428.371 -428.371] [49.762], Avg: [-444.041 -444.041 -444.041] (0.0967) ({r_i: None, r_t: [-853.401 -853.401 -853.401], eps: 0.097})
Step:  132100, Reward: [-355.741 -355.741 -355.741] [72.572], Avg: [-423.924 -423.924 -423.924] (1.0000) ({r_i: None, r_t: [-718.829 -718.829 -718.829], eps: 1.0})
Step:   23400, Reward: [-426.930 -426.930 -426.930] [74.678], Avg: [-443.968 -443.968 -443.968] (0.0958) ({r_i: None, r_t: [-830.703 -830.703 -830.703], eps: 0.096})
Step:  132200, Reward: [-335.436 -335.436 -335.436] [39.740], Avg: [-423.857 -423.857 -423.857] (1.0000) ({r_i: None, r_t: [-711.896 -711.896 -711.896], eps: 1.0})
Step:   23500, Reward: [-424.230 -424.230 -424.230] [67.900], Avg: [-443.884 -443.884 -443.884] (0.0948) ({r_i: None, r_t: [-902.111 -902.111 -902.111], eps: 0.095})
Step:  132300, Reward: [-399.705 -399.705 -399.705] [73.048], Avg: [-423.839 -423.839 -423.839] (1.0000) ({r_i: None, r_t: [-733.252 -733.252 -733.252], eps: 1.0})
Step:   23600, Reward: [-438.646 -438.646 -438.646] [70.680], Avg: [-443.862 -443.862 -443.862] (0.0939) ({r_i: None, r_t: [-832.414 -832.414 -832.414], eps: 0.094})
Step:  132400, Reward: [-352.812 -352.812 -352.812] [69.032], Avg: [-423.785 -423.785 -423.785] (1.0000) ({r_i: None, r_t: [-788.914 -788.914 -788.914], eps: 1.0})
Step:   23700, Reward: [-428.056 -428.056 -428.056] [73.550], Avg: [-443.796 -443.796 -443.796] (0.0929) ({r_i: None, r_t: [-869.563 -869.563 -869.563], eps: 0.093})
Step:  132500, Reward: [-376.137 -376.137 -376.137] [53.012], Avg: [-423.749 -423.749 -423.749] (1.0000) ({r_i: None, r_t: [-759.871 -759.871 -759.871], eps: 1.0})
Step:   23800, Reward: [-427.694 -427.694 -427.694] [61.617], Avg: [-443.728 -443.728 -443.728] (0.0920) ({r_i: None, r_t: [-825.485 -825.485 -825.485], eps: 0.092})
Step:  132600, Reward: [-381.358 -381.358 -381.358] [72.807], Avg: [-423.718 -423.718 -423.718] (1.0000) ({r_i: None, r_t: [-762.639 -762.639 -762.639], eps: 1.0})
Step:   23900, Reward: [-422.104 -422.104 -422.104] [58.217], Avg: [-443.638 -443.638 -443.638] (0.0911) ({r_i: None, r_t: [-830.171 -830.171 -830.171], eps: 0.091})
Step:  132700, Reward: [-409.436 -409.436 -409.436] [63.493], Avg: [-423.707 -423.707 -423.707] (1.0000) ({r_i: None, r_t: [-755.026 -755.026 -755.026], eps: 1.0})
Step:   24000, Reward: [-400.406 -400.406 -400.406] [58.767], Avg: [-443.459 -443.459 -443.459] (0.0902) ({r_i: None, r_t: [-891.536 -891.536 -891.536], eps: 0.09})
Step:  132800, Reward: [-370.509 -370.509 -370.509] [54.512], Avg: [-423.667 -423.667 -423.667] (1.0000) ({r_i: None, r_t: [-787.884 -787.884 -787.884], eps: 1.0})
Step:   24100, Reward: [-439.826 -439.826 -439.826] [48.267], Avg: [-443.444 -443.444 -443.444] (0.0893) ({r_i: None, r_t: [-856.859 -856.859 -856.859], eps: 0.089})
Step:  132900, Reward: [-392.781 -392.781 -392.781] [69.333], Avg: [-423.644 -423.644 -423.644] (1.0000) ({r_i: None, r_t: [-803.373 -803.373 -803.373], eps: 1.0})
Step:   24200, Reward: [-454.482 -454.482 -454.482] [80.129], Avg: [-443.489 -443.489 -443.489] (0.0884) ({r_i: None, r_t: [-855.150 -855.150 -855.150], eps: 0.088})
Step:  133000, Reward: [-345.775 -345.775 -345.775] [60.487], Avg: [-423.585 -423.585 -423.585] (1.0000) ({r_i: None, r_t: [-752.213 -752.213 -752.213], eps: 1.0})
Step:   24300, Reward: [-430.884 -430.884 -430.884] [48.428], Avg: [-443.438 -443.438 -443.438] (0.0875) ({r_i: None, r_t: [-847.274 -847.274 -847.274], eps: 0.088})
Step:  133100, Reward: [-372.747 -372.747 -372.747] [49.602], Avg: [-423.547 -423.547 -423.547] (1.0000) ({r_i: None, r_t: [-747.935 -747.935 -747.935], eps: 1.0})
Step:   24400, Reward: [-422.643 -422.643 -422.643] [64.315], Avg: [-443.353 -443.353 -443.353] (0.0866) ({r_i: None, r_t: [-890.437 -890.437 -890.437], eps: 0.087})
Step:  133200, Reward: [-396.917 -396.917 -396.917] [69.782], Avg: [-423.527 -423.527 -423.527] (1.0000) ({r_i: None, r_t: [-732.576 -732.576 -732.576], eps: 1.0})
Step:   24500, Reward: [-408.992 -408.992 -408.992] [64.381], Avg: [-443.213 -443.213 -443.213] (0.0858) ({r_i: None, r_t: [-859.415 -859.415 -859.415], eps: 0.086})
Step:  133300, Reward: [-355.611 -355.611 -355.611] [74.728], Avg: [-423.476 -423.476 -423.476] (1.0000) ({r_i: None, r_t: [-741.712 -741.712 -741.712], eps: 1.0})
Step:   24600, Reward: [-445.249 -445.249 -445.249] [58.525], Avg: [-443.221 -443.221 -443.221] (0.0849) ({r_i: None, r_t: [-817.898 -817.898 -817.898], eps: 0.085})
Step:  133400, Reward: [-380.546 -380.546 -380.546] [53.845], Avg: [-423.444 -423.444 -423.444] (1.0000) ({r_i: None, r_t: [-711.046 -711.046 -711.046], eps: 1.0})
Step:   24700, Reward: [-453.203 -453.203 -453.203] [71.820], Avg: [-443.262 -443.262 -443.262] (0.0841) ({r_i: None, r_t: [-932.225 -932.225 -932.225], eps: 0.084})
Step:  133500, Reward: [-357.922 -357.922 -357.922] [46.659], Avg: [-423.395 -423.395 -423.395] (1.0000) ({r_i: None, r_t: [-721.137 -721.137 -721.137], eps: 1.0})
Step:   24800, Reward: [-426.718 -426.718 -426.718] [69.965], Avg: [-443.195 -443.195 -443.195] (0.0832) ({r_i: None, r_t: [-876.205 -876.205 -876.205], eps: 0.083})
Step:  133600, Reward: [-392.481 -392.481 -392.481] [74.442], Avg: [-423.372 -423.372 -423.372] (1.0000) ({r_i: None, r_t: [-767.472 -767.472 -767.472], eps: 1.0})
Step:   24900, Reward: [-423.128 -423.128 -423.128] [57.407], Avg: [-443.115 -443.115 -443.115] (0.0824) ({r_i: None, r_t: [-913.995 -913.995 -913.995], eps: 0.082})
Step:  133700, Reward: [-379.355 -379.355 -379.355] [71.055], Avg: [-423.339 -423.339 -423.339] (1.0000) ({r_i: None, r_t: [-728.311 -728.311 -728.311], eps: 1.0})
Step:   25000, Reward: [-422.058 -422.058 -422.058] [53.520], Avg: [-443.031 -443.031 -443.031] (0.0816) ({r_i: None, r_t: [-849.651 -849.651 -849.651], eps: 0.082})
Step:  133800, Reward: [-383.315 -383.315 -383.315] [88.007], Avg: [-423.309 -423.309 -423.309] (1.0000) ({r_i: None, r_t: [-775.311 -775.311 -775.311], eps: 1.0})
Step:  133900, Reward: [-363.629 -363.629 -363.629] [68.570], Avg: [-423.264 -423.264 -423.264] (1.0000) ({r_i: None, r_t: [-740.485 -740.485 -740.485], eps: 1.0})
Step:   25100, Reward: [-419.500 -419.500 -419.500] [76.768], Avg: [-442.938 -442.938 -442.938] (0.0808) ({r_i: None, r_t: [-779.410 -779.410 -779.410], eps: 0.081})
Step:  134000, Reward: [-357.770 -357.770 -357.770] [63.929], Avg: [-423.215 -423.215 -423.215] (1.0000) ({r_i: None, r_t: [-787.863 -787.863 -787.863], eps: 1.0})
Step:   25200, Reward: [-414.293 -414.293 -414.293] [78.929], Avg: [-442.824 -442.824 -442.824] (0.0800) ({r_i: None, r_t: [-857.304 -857.304 -857.304], eps: 0.08})
Step:   25300, Reward: [-429.052 -429.052 -429.052] [77.900], Avg: [-442.770 -442.770 -442.770] (0.0792) ({r_i: None, r_t: [-898.683 -898.683 -898.683], eps: 0.079})
Step:  134100, Reward: [-384.840 -384.840 -384.840] [75.541], Avg: [-423.187 -423.187 -423.187] (1.0000) ({r_i: None, r_t: [-800.755 -800.755 -800.755], eps: 1.0})
Step:   25400, Reward: [-428.252 -428.252 -428.252] [75.476], Avg: [-442.713 -442.713 -442.713] (0.0784) ({r_i: None, r_t: [-837.040 -837.040 -837.040], eps: 0.078})
Step:  134200, Reward: [-403.999 -403.999 -403.999] [75.570], Avg: [-423.173 -423.173 -423.173] (1.0000) ({r_i: None, r_t: [-785.015 -785.015 -785.015], eps: 1.0})
Step:   25500, Reward: [-419.283 -419.283 -419.283] [54.109], Avg: [-442.622 -442.622 -442.622] (0.0776) ({r_i: None, r_t: [-851.745 -851.745 -851.745], eps: 0.078})
Step:  134300, Reward: [-344.502 -344.502 -344.502] [44.794], Avg: [-423.114 -423.114 -423.114] (1.0000) ({r_i: None, r_t: [-721.404 -721.404 -721.404], eps: 1.0})
Step:   25600, Reward: [-440.949 -440.949 -440.949] [79.329], Avg: [-442.615 -442.615 -442.615] (0.0768) ({r_i: None, r_t: [-824.820 -824.820 -824.820], eps: 0.077})
Step:  134400, Reward: [-390.790 -390.790 -390.790] [52.553], Avg: [-423.090 -423.090 -423.090] (1.0000) ({r_i: None, r_t: [-743.990 -743.990 -743.990], eps: 1.0})
Step:   25700, Reward: [-374.239 -374.239 -374.239] [44.694], Avg: [-442.350 -442.350 -442.350] (0.0760) ({r_i: None, r_t: [-841.396 -841.396 -841.396], eps: 0.076})
Step:  134500, Reward: [-364.396 -364.396 -364.396] [88.509], Avg: [-423.046 -423.046 -423.046] (1.0000) ({r_i: None, r_t: [-752.032 -752.032 -752.032], eps: 1.0})
Step:   25800, Reward: [-432.599 -432.599 -432.599] [50.406], Avg: [-442.313 -442.313 -442.313] (0.0753) ({r_i: None, r_t: [-856.932 -856.932 -856.932], eps: 0.075})
Step:  134600, Reward: [-394.026 -394.026 -394.026] [97.260], Avg: [-423.025 -423.025 -423.025] (1.0000) ({r_i: None, r_t: [-782.149 -782.149 -782.149], eps: 1.0})
Step:  134700, Reward: [-380.227 -380.227 -380.227] [76.485], Avg: [-422.993 -422.993 -422.993] (1.0000) ({r_i: None, r_t: [-734.026 -734.026 -734.026], eps: 1.0})
Step:   25900, Reward: [-412.873 -412.873 -412.873] [90.291], Avg: [-442.199 -442.199 -442.199] (0.0745) ({r_i: None, r_t: [-868.099 -868.099 -868.099], eps: 0.075})
Step:  134800, Reward: [-389.370 -389.370 -389.370] [60.333], Avg: [-422.968 -422.968 -422.968] (1.0000) ({r_i: None, r_t: [-740.820 -740.820 -740.820], eps: 1.0})
Step:   26000, Reward: [-402.961 -402.961 -402.961] [57.031], Avg: [-442.049 -442.049 -442.049] (0.0738) ({r_i: None, r_t: [-869.028 -869.028 -869.028], eps: 0.074})
Step:  134900, Reward: [-363.909 -363.909 -363.909] [62.262], Avg: [-422.924 -422.924 -422.924] (1.0000) ({r_i: None, r_t: [-744.862 -744.862 -744.862], eps: 1.0})
Step:   26100, Reward: [-417.620 -417.620 -417.620] [82.624], Avg: [-441.956 -441.956 -441.956] (0.0731) ({r_i: None, r_t: [-813.502 -813.502 -813.502], eps: 0.073})
Step:   26200, Reward: [-436.211 -436.211 -436.211] [45.039], Avg: [-441.934 -441.934 -441.934] (0.0723) ({r_i: None, r_t: [-892.049 -892.049 -892.049], eps: 0.072})
Step:  135000, Reward: [-354.525 -354.525 -354.525] [50.515], Avg: [-422.874 -422.874 -422.874] (1.0000) ({r_i: None, r_t: [-742.736 -742.736 -742.736], eps: 1.0})
Step:   26300, Reward: [-405.345 -405.345 -405.345] [64.494], Avg: [-441.795 -441.795 -441.795] (0.0716) ({r_i: None, r_t: [-892.032 -892.032 -892.032], eps: 0.072})
Step:  135100, Reward: [-379.458 -379.458 -379.458] [80.762], Avg: [-422.842 -422.842 -422.842] (1.0000) ({r_i: None, r_t: [-772.560 -772.560 -772.560], eps: 1.0})
Step:   26400, Reward: [-420.467 -420.467 -420.467] [76.535], Avg: [-441.715 -441.715 -441.715] (0.0709) ({r_i: None, r_t: [-856.239 -856.239 -856.239], eps: 0.071})
Step:  135200, Reward: [-362.275 -362.275 -362.275] [39.869], Avg: [-422.797 -422.797 -422.797] (1.0000) ({r_i: None, r_t: [-756.051 -756.051 -756.051], eps: 1.0})
Step:  135300, Reward: [-378.168 -378.168 -378.168] [70.910], Avg: [-422.764 -422.764 -422.764] (1.0000) ({r_i: None, r_t: [-730.167 -730.167 -730.167], eps: 1.0})
Step:   26500, Reward: [-433.749 -433.749 -433.749] [72.750], Avg: [-441.685 -441.685 -441.685] (0.0702) ({r_i: None, r_t: [-857.960 -857.960 -857.960], eps: 0.07})
Step:  135400, Reward: [-355.391 -355.391 -355.391] [76.953], Avg: [-422.714 -422.714 -422.714] (1.0000) ({r_i: None, r_t: [-709.582 -709.582 -709.582], eps: 1.0})
Step:   26600, Reward: [-453.065 -453.065 -453.065] [62.431], Avg: [-441.727 -441.727 -441.727] (0.0695) ({r_i: None, r_t: [-836.123 -836.123 -836.123], eps: 0.069})
Step:  135500, Reward: [-387.708 -387.708 -387.708] [78.723], Avg: [-422.688 -422.688 -422.688] (1.0000) ({r_i: None, r_t: [-748.998 -748.998 -748.998], eps: 1.0})
Step:   26700, Reward: [-419.880 -419.880 -419.880] [78.508], Avg: [-441.646 -441.646 -441.646] (0.0688) ({r_i: None, r_t: [-808.392 -808.392 -808.392], eps: 0.069})
Step:  135600, Reward: [-396.830 -396.830 -396.830] [85.154], Avg: [-422.669 -422.669 -422.669] (1.0000) ({r_i: None, r_t: [-758.068 -758.068 -758.068], eps: 1.0})
Step:   26800, Reward: [-439.345 -439.345 -439.345] [63.931], Avg: [-441.637 -441.637 -441.637] (0.0681) ({r_i: None, r_t: [-841.903 -841.903 -841.903], eps: 0.068})
Step:  135700, Reward: [-378.185 -378.185 -378.185] [65.155], Avg: [-422.637 -422.637 -422.637] (1.0000) ({r_i: None, r_t: [-786.954 -786.954 -786.954], eps: 1.0})
Step:   26900, Reward: [-429.834 -429.834 -429.834] [80.984], Avg: [-441.594 -441.594 -441.594] (0.0674) ({r_i: None, r_t: [-860.045 -860.045 -860.045], eps: 0.067})
Step:  135800, Reward: [-380.836 -380.836 -380.836] [76.959], Avg: [-422.606 -422.606 -422.606] (1.0000) ({r_i: None, r_t: [-772.427 -772.427 -772.427], eps: 1.0})
Step:   27000, Reward: [-403.419 -403.419 -403.419] [68.197], Avg: [-441.453 -441.453 -441.453] (0.0668) ({r_i: None, r_t: [-864.964 -864.964 -864.964], eps: 0.067})
Step:  135900, Reward: [-372.752 -372.752 -372.752] [49.090], Avg: [-422.569 -422.569 -422.569] (1.0000) ({r_i: None, r_t: [-764.389 -764.389 -764.389], eps: 1.0})
Step:   27100, Reward: [-400.255 -400.255 -400.255] [57.950], Avg: [-441.301 -441.301 -441.301] (0.0661) ({r_i: None, r_t: [-857.978 -857.978 -857.978], eps: 0.066})
Step:  136000, Reward: [-364.119 -364.119 -364.119] [65.756], Avg: [-422.526 -422.526 -422.526] (1.0000) ({r_i: None, r_t: [-761.269 -761.269 -761.269], eps: 1.0})
Step:   27200, Reward: [-421.836 -421.836 -421.836] [63.903], Avg: [-441.230 -441.230 -441.230] (0.0654) ({r_i: None, r_t: [-911.237 -911.237 -911.237], eps: 0.065})
Step:  136100, Reward: [-393.023 -393.023 -393.023] [80.558], Avg: [-422.505 -422.505 -422.505] (1.0000) ({r_i: None, r_t: [-717.765 -717.765 -717.765], eps: 1.0})
Step:   27300, Reward: [-428.549 -428.549 -428.549] [51.863], Avg: [-441.184 -441.184 -441.184] (0.0648) ({r_i: None, r_t: [-853.729 -853.729 -853.729], eps: 0.065})
Step:   27400, Reward: [-424.442 -424.442 -424.442] [77.284], Avg: [-441.123 -441.123 -441.123] (0.0641) ({r_i: None, r_t: [-871.867 -871.867 -871.867], eps: 0.064})
Step:  136200, Reward: [-368.088 -368.088 -368.088] [71.620], Avg: [-422.465 -422.465 -422.465] (1.0000) ({r_i: None, r_t: [-781.684 -781.684 -781.684], eps: 1.0})
Step:   27500, Reward: [-420.681 -420.681 -420.681] [54.141], Avg: [-441.049 -441.049 -441.049] (0.0635) ({r_i: None, r_t: [-805.953 -805.953 -805.953], eps: 0.063})
Step:  136300, Reward: [-383.834 -383.834 -383.834] [64.809], Avg: [-422.436 -422.436 -422.436] (1.0000) ({r_i: None, r_t: [-790.580 -790.580 -790.580], eps: 1.0})
Step:   27600, Reward: [-422.223 -422.223 -422.223] [74.824], Avg: [-440.981 -440.981 -440.981] (0.0629) ({r_i: None, r_t: [-848.043 -848.043 -848.043], eps: 0.063})
Step:  136400, Reward: [-353.462 -353.462 -353.462] [76.477], Avg: [-422.386 -422.386 -422.386] (1.0000) ({r_i: None, r_t: [-768.568 -768.568 -768.568], eps: 1.0})
Step:   27700, Reward: [-412.332 -412.332 -412.332] [68.718], Avg: [-440.878 -440.878 -440.878] (0.0622) ({r_i: None, r_t: [-854.977 -854.977 -854.977], eps: 0.062})
Step:  136500, Reward: [-349.163 -349.163 -349.163] [45.813], Avg: [-422.332 -422.332 -422.332] (1.0000) ({r_i: None, r_t: [-771.363 -771.363 -771.363], eps: 1.0})
Step:   27800, Reward: [-425.332 -425.332 -425.332] [71.391], Avg: [-440.822 -440.822 -440.822] (0.0616) ({r_i: None, r_t: [-815.208 -815.208 -815.208], eps: 0.062})
Step:  136600, Reward: [-381.807 -381.807 -381.807] [65.673], Avg: [-422.303 -422.303 -422.303] (1.0000) ({r_i: None, r_t: [-718.591 -718.591 -718.591], eps: 1.0})
Step:   27900, Reward: [-403.567 -403.567 -403.567] [65.888], Avg: [-440.689 -440.689 -440.689] (0.0610) ({r_i: None, r_t: [-849.473 -849.473 -849.473], eps: 0.061})
Step:  136700, Reward: [-378.608 -378.608 -378.608] [80.539], Avg: [-422.271 -422.271 -422.271] (1.0000) ({r_i: None, r_t: [-726.768 -726.768 -726.768], eps: 1.0})
Step:   28000, Reward: [-460.557 -460.557 -460.557] [66.051], Avg: [-440.760 -440.760 -440.760] (0.0604) ({r_i: None, r_t: [-832.772 -832.772 -832.772], eps: 0.06})
Step:  136800, Reward: [-372.595 -372.595 -372.595] [55.632], Avg: [-422.234 -422.234 -422.234] (1.0000) ({r_i: None, r_t: [-746.266 -746.266 -746.266], eps: 1.0})
Step:   28100, Reward: [-413.121 -413.121 -413.121] [58.592], Avg: [-440.662 -440.662 -440.662] (0.0598) ({r_i: None, r_t: [-844.135 -844.135 -844.135], eps: 0.06})
Step:  136900, Reward: [-370.392 -370.392 -370.392] [72.969], Avg: [-422.197 -422.197 -422.197] (1.0000) ({r_i: None, r_t: [-756.564 -756.564 -756.564], eps: 1.0})
Step:   28200, Reward: [-397.141 -397.141 -397.141] [64.210], Avg: [-440.508 -440.508 -440.508] (0.0592) ({r_i: None, r_t: [-872.120 -872.120 -872.120], eps: 0.059})
Step:  137000, Reward: [-355.509 -355.509 -355.509] [68.999], Avg: [-422.148 -422.148 -422.148] (1.0000) ({r_i: None, r_t: [-768.237 -768.237 -768.237], eps: 1.0})
Step:   28300, Reward: [-425.452 -425.452 -425.452] [59.790], Avg: [-440.455 -440.455 -440.455] (0.0586) ({r_i: None, r_t: [-831.298 -831.298 -831.298], eps: 0.059})
Step:  137100, Reward: [-404.092 -404.092 -404.092] [75.940], Avg: [-422.135 -422.135 -422.135] (1.0000) ({r_i: None, r_t: [-737.068 -737.068 -737.068], eps: 1.0})
Step:   28400, Reward: [-438.081 -438.081 -438.081] [72.759], Avg: [-440.447 -440.447 -440.447] (0.0580) ({r_i: None, r_t: [-857.738 -857.738 -857.738], eps: 0.058})
Step:  137200, Reward: [-385.857 -385.857 -385.857] [66.849], Avg: [-422.108 -422.108 -422.108] (1.0000) ({r_i: None, r_t: [-774.146 -774.146 -774.146], eps: 1.0})
Step:   28500, Reward: [-417.363 -417.363 -417.363] [53.643], Avg: [-440.366 -440.366 -440.366] (0.0574) ({r_i: None, r_t: [-867.318 -867.318 -867.318], eps: 0.057})
Step:  137300, Reward: [-370.511 -370.511 -370.511] [54.514], Avg: [-422.071 -422.071 -422.071] (1.0000) ({r_i: None, r_t: [-769.128 -769.128 -769.128], eps: 1.0})
Step:   28600, Reward: [-431.697 -431.697 -431.697] [63.670], Avg: [-440.336 -440.336 -440.336] (0.0569) ({r_i: None, r_t: [-845.999 -845.999 -845.999], eps: 0.057})
Step:  137400, Reward: [-409.177 -409.177 -409.177] [77.555], Avg: [-422.061 -422.061 -422.061] (1.0000) ({r_i: None, r_t: [-734.892 -734.892 -734.892], eps: 1.0})
Step:   28700, Reward: [-454.799 -454.799 -454.799] [60.850], Avg: [-440.386 -440.386 -440.386] (0.0563) ({r_i: None, r_t: [-873.305 -873.305 -873.305], eps: 0.056})
Step:  137500, Reward: [-377.488 -377.488 -377.488] [63.866], Avg: [-422.029 -422.029 -422.029] (1.0000) ({r_i: None, r_t: [-757.681 -757.681 -757.681], eps: 1.0})
Step:   28800, Reward: [-398.840 -398.840 -398.840] [48.350], Avg: [-440.242 -440.242 -440.242] (0.0557) ({r_i: None, r_t: [-830.264 -830.264 -830.264], eps: 0.056})
Step:  137600, Reward: [-373.513 -373.513 -373.513] [76.496], Avg: [-421.994 -421.994 -421.994] (1.0000) ({r_i: None, r_t: [-737.591 -737.591 -737.591], eps: 1.0})
Step:   28900, Reward: [-457.531 -457.531 -457.531] [67.500], Avg: [-440.302 -440.302 -440.302] (0.0552) ({r_i: None, r_t: [-824.481 -824.481 -824.481], eps: 0.055})
Step:  137700, Reward: [-368.809 -368.809 -368.809] [61.952], Avg: [-421.955 -421.955 -421.955] (1.0000) ({r_i: None, r_t: [-759.426 -759.426 -759.426], eps: 1.0})
Step:   29000, Reward: [-417.256 -417.256 -417.256] [69.302], Avg: [-440.223 -440.223 -440.223] (0.0546) ({r_i: None, r_t: [-840.603 -840.603 -840.603], eps: 0.055})
Step:  137800, Reward: [-359.535 -359.535 -359.535] [56.922], Avg: [-421.910 -421.910 -421.910] (1.0000) ({r_i: None, r_t: [-776.734 -776.734 -776.734], eps: 1.0})
Step:   29100, Reward: [-447.439 -447.439 -447.439] [57.374], Avg: [-440.247 -440.247 -440.247] (0.0541) ({r_i: None, r_t: [-885.445 -885.445 -885.445], eps: 0.054})
Step:  137900, Reward: [-342.027 -342.027 -342.027] [61.660], Avg: [-421.852 -421.852 -421.852] (1.0000) ({r_i: None, r_t: [-761.635 -761.635 -761.635], eps: 1.0})
Step:   29200, Reward: [-443.104 -443.104 -443.104] [72.198], Avg: [-440.257 -440.257 -440.257] (0.0535) ({r_i: None, r_t: [-864.348 -864.348 -864.348], eps: 0.054})
Step:  138000, Reward: [-392.382 -392.382 -392.382] [54.844], Avg: [-421.831 -421.831 -421.831] (1.0000) ({r_i: None, r_t: [-725.179 -725.179 -725.179], eps: 1.0})
Step:   29300, Reward: [-434.799 -434.799 -434.799] [67.930], Avg: [-440.238 -440.238 -440.238] (0.0530) ({r_i: None, r_t: [-849.496 -849.496 -849.496], eps: 0.053})
Step:  138100, Reward: [-379.752 -379.752 -379.752] [79.149], Avg: [-421.800 -421.800 -421.800] (1.0000) ({r_i: None, r_t: [-760.302 -760.302 -760.302], eps: 1.0})
Step:   29400, Reward: [-435.793 -435.793 -435.793] [58.698], Avg: [-440.223 -440.223 -440.223] (0.0525) ({r_i: None, r_t: [-831.248 -831.248 -831.248], eps: 0.052})
Step:  138200, Reward: [-367.976 -367.976 -367.976] [85.857], Avg: [-421.761 -421.761 -421.761] (1.0000) ({r_i: None, r_t: [-742.559 -742.559 -742.559], eps: 1.0})
Step:  138300, Reward: [-359.289 -359.289 -359.289] [85.042], Avg: [-421.716 -421.716 -421.716] (1.0000) ({r_i: None, r_t: [-719.916 -719.916 -719.916], eps: 1.0})
Step:   29500, Reward: [-450.760 -450.760 -450.760] [48.025], Avg: [-440.259 -440.259 -440.259] (0.0520) ({r_i: None, r_t: [-857.837 -857.837 -857.837], eps: 0.052})
Step:  138400, Reward: [-355.526 -355.526 -355.526] [34.534], Avg: [-421.668 -421.668 -421.668] (1.0000) ({r_i: None, r_t: [-752.454 -752.454 -752.454], eps: 1.0})
Step:   29600, Reward: [-400.104 -400.104 -400.104] [69.091], Avg: [-440.124 -440.124 -440.124] (0.0514) ({r_i: None, r_t: [-835.555 -835.555 -835.555], eps: 0.051})
Step:  138500, Reward: [-361.854 -361.854 -361.854] [52.348], Avg: [-421.625 -421.625 -421.625] (1.0000) ({r_i: None, r_t: [-781.057 -781.057 -781.057], eps: 1.0})
Step:   29700, Reward: [-428.563 -428.563 -428.563] [73.600], Avg: [-440.085 -440.085 -440.085] (0.0509) ({r_i: None, r_t: [-845.683 -845.683 -845.683], eps: 0.051})
Step:  138600, Reward: [-357.035 -357.035 -357.035] [62.768], Avg: [-421.579 -421.579 -421.579] (1.0000) ({r_i: None, r_t: [-762.276 -762.276 -762.276], eps: 1.0})
Step:   29800, Reward: [-453.383 -453.383 -453.383] [55.391], Avg: [-440.129 -440.129 -440.129] (0.0504) ({r_i: None, r_t: [-882.569 -882.569 -882.569], eps: 0.05})
Step:  138700, Reward: [-359.263 -359.263 -359.263] [76.808], Avg: [-421.534 -421.534 -421.534] (1.0000) ({r_i: None, r_t: [-735.524 -735.524 -735.524], eps: 1.0})
Step:   29900, Reward: [-433.359 -433.359 -433.359] [65.367], Avg: [-440.107 -440.107 -440.107] (0.0499) ({r_i: None, r_t: [-834.701 -834.701 -834.701], eps: 0.05})
Step:  138800, Reward: [-352.920 -352.920 -352.920] [60.446], Avg: [-421.484 -421.484 -421.484] (1.0000) ({r_i: None, r_t: [-754.048 -754.048 -754.048], eps: 1.0})
Step:   30000, Reward: [-431.449 -431.449 -431.449] [74.122], Avg: [-440.078 -440.078 -440.078] (0.0494) ({r_i: None, r_t: [-849.507 -849.507 -849.507], eps: 0.049})
Step:  138900, Reward: [-375.417 -375.417 -375.417] [52.808], Avg: [-421.451 -421.451 -421.451] (1.0000) ({r_i: None, r_t: [-707.461 -707.461 -707.461], eps: 1.0})
Step:   30100, Reward: [-431.473 -431.473 -431.473] [67.249], Avg: [-440.050 -440.050 -440.050] (0.0489) ({r_i: None, r_t: [-872.810 -872.810 -872.810], eps: 0.049})
Step:  139000, Reward: [-363.012 -363.012 -363.012] [72.065], Avg: [-421.409 -421.409 -421.409] (1.0000) ({r_i: None, r_t: [-747.498 -747.498 -747.498], eps: 1.0})
Step:   30200, Reward: [-437.983 -437.983 -437.983] [59.829], Avg: [-440.043 -440.043 -440.043] (0.0484) ({r_i: None, r_t: [-837.553 -837.553 -837.553], eps: 0.048})
Step:  139100, Reward: [-359.840 -359.840 -359.840] [69.836], Avg: [-421.365 -421.365 -421.365] (1.0000) ({r_i: None, r_t: [-732.968 -732.968 -732.968], eps: 1.0})
Step:   30300, Reward: [-424.983 -424.983 -424.983] [47.240], Avg: [-439.993 -439.993 -439.993] (0.0479) ({r_i: None, r_t: [-840.064 -840.064 -840.064], eps: 0.048})
Step:  139200, Reward: [-360.605 -360.605 -360.605] [62.893], Avg: [-421.321 -421.321 -421.321] (1.0000) ({r_i: None, r_t: [-745.204 -745.204 -745.204], eps: 1.0})
Step:   30400, Reward: [-448.828 -448.828 -448.828] [53.252], Avg: [-440.022 -440.022 -440.022] (0.0475) ({r_i: None, r_t: [-860.941 -860.941 -860.941], eps: 0.047})
Step:  139300, Reward: [-411.844 -411.844 -411.844] [91.450], Avg: [-421.315 -421.315 -421.315] (1.0000) ({r_i: None, r_t: [-765.704 -765.704 -765.704], eps: 1.0})
Step:   30500, Reward: [-426.998 -426.998 -426.998] [35.086], Avg: [-439.980 -439.980 -439.980] (0.0470) ({r_i: None, r_t: [-847.913 -847.913 -847.913], eps: 0.047})
Step:  139400, Reward: [-369.461 -369.461 -369.461] [65.237], Avg: [-421.277 -421.277 -421.277] (1.0000) ({r_i: None, r_t: [-787.401 -787.401 -787.401], eps: 1.0})
Step:   30600, Reward: [-449.167 -449.167 -449.167] [56.379], Avg: [-440.010 -440.010 -440.010] (0.0465) ({r_i: None, r_t: [-832.244 -832.244 -832.244], eps: 0.047})
Step:  139500, Reward: [-372.830 -372.830 -372.830] [73.292], Avg: [-421.243 -421.243 -421.243] (1.0000) ({r_i: None, r_t: [-745.706 -745.706 -745.706], eps: 1.0})
Step:   30700, Reward: [-424.907 -424.907 -424.907] [72.015], Avg: [-439.961 -439.961 -439.961] (0.0461) ({r_i: None, r_t: [-846.853 -846.853 -846.853], eps: 0.046})
Step:  139600, Reward: [-346.524 -346.524 -346.524] [50.364], Avg: [-421.189 -421.189 -421.189] (1.0000) ({r_i: None, r_t: [-733.435 -733.435 -733.435], eps: 1.0})
Step:   30800, Reward: [-432.560 -432.560 -432.560] [61.743], Avg: [-439.937 -439.937 -439.937] (0.0456) ({r_i: None, r_t: [-811.572 -811.572 -811.572], eps: 0.046})
Step:  139700, Reward: [-361.942 -361.942 -361.942] [59.406], Avg: [-421.147 -421.147 -421.147] (1.0000) ({r_i: None, r_t: [-773.254 -773.254 -773.254], eps: 1.0})
Step:   30900, Reward: [-428.305 -428.305 -428.305] [67.791], Avg: [-439.899 -439.899 -439.899] (0.0452) ({r_i: None, r_t: [-797.447 -797.447 -797.447], eps: 0.045})
Step:  139800, Reward: [-349.376 -349.376 -349.376] [45.984], Avg: [-421.095 -421.095 -421.095] (1.0000) ({r_i: None, r_t: [-770.150 -770.150 -770.150], eps: 1.0})
Step:   31000, Reward: [-418.052 -418.052 -418.052] [62.727], Avg: [-439.829 -439.829 -439.829] (0.0447) ({r_i: None, r_t: [-835.805 -835.805 -835.805], eps: 0.045})
Step:  139900, Reward: [-379.166 -379.166 -379.166] [86.048], Avg: [-421.066 -421.066 -421.066] (1.0000) ({r_i: None, r_t: [-735.059 -735.059 -735.059], eps: 1.0})
Step:   31100, Reward: [-444.853 -444.853 -444.853] [51.974], Avg: [-439.845 -439.845 -439.845] (0.0443) ({r_i: None, r_t: [-850.190 -850.190 -850.190], eps: 0.044})
Step:  140000, Reward: [-388.012 -388.012 -388.012] [78.042], Avg: [-421.042 -421.042 -421.042] (1.0000) ({r_i: None, r_t: [-751.768 -751.768 -751.768], eps: 1.0})
Step:   31200, Reward: [-415.560 -415.560 -415.560] [46.837], Avg: [-439.767 -439.767 -439.767] (0.0438) ({r_i: None, r_t: [-851.569 -851.569 -851.569], eps: 0.044})
Step:  140100, Reward: [-393.303 -393.303 -393.303] [59.889], Avg: [-421.022 -421.022 -421.022] (1.0000) ({r_i: None, r_t: [-757.611 -757.611 -757.611], eps: 1.0})
Step:   31300, Reward: [-410.774 -410.774 -410.774] [65.434], Avg: [-439.675 -439.675 -439.675] (0.0434) ({r_i: None, r_t: [-837.365 -837.365 -837.365], eps: 0.043})
Step:  140200, Reward: [-377.514 -377.514 -377.514] [59.804], Avg: [-420.991 -420.991 -420.991] (1.0000) ({r_i: None, r_t: [-703.735 -703.735 -703.735], eps: 1.0})
Step:   31400, Reward: [-423.637 -423.637 -423.637] [71.669], Avg: [-439.624 -439.624 -439.624] (0.0429) ({r_i: None, r_t: [-874.079 -874.079 -874.079], eps: 0.043})
Step:  140300, Reward: [-349.520 -349.520 -349.520] [61.299], Avg: [-420.940 -420.940 -420.940] (1.0000) ({r_i: None, r_t: [-804.171 -804.171 -804.171], eps: 1.0})
Step:   31500, Reward: [-427.067 -427.067 -427.067] [56.050], Avg: [-439.584 -439.584 -439.584] (0.0425) ({r_i: None, r_t: [-866.218 -866.218 -866.218], eps: 0.043})
Step:  140400, Reward: [-411.362 -411.362 -411.362] [75.992], Avg: [-420.933 -420.933 -420.933] (1.0000) ({r_i: None, r_t: [-747.309 -747.309 -747.309], eps: 1.0})
Step:   31600, Reward: [-407.194 -407.194 -407.194] [50.787], Avg: [-439.482 -439.482 -439.482] (0.0421) ({r_i: None, r_t: [-862.040 -862.040 -862.040], eps: 0.042})
Step:  140500, Reward: [-365.833 -365.833 -365.833] [84.362], Avg: [-420.894 -420.894 -420.894] (1.0000) ({r_i: None, r_t: [-783.378 -783.378 -783.378], eps: 1.0})
Step:   31700, Reward: [-427.127 -427.127 -427.127] [70.190], Avg: [-439.443 -439.443 -439.443] (0.0417) ({r_i: None, r_t: [-833.864 -833.864 -833.864], eps: 0.042})
Step:  140600, Reward: [-359.670 -359.670 -359.670] [46.723], Avg: [-420.851 -420.851 -420.851] (1.0000) ({r_i: None, r_t: [-719.075 -719.075 -719.075], eps: 1.0})
Step:   31800, Reward: [-442.574 -442.574 -442.574] [73.061], Avg: [-439.453 -439.453 -439.453] (0.0413) ({r_i: None, r_t: [-907.275 -907.275 -907.275], eps: 0.041})
Step:  140700, Reward: [-376.170 -376.170 -376.170] [51.912], Avg: [-420.819 -420.819 -420.819] (1.0000) ({r_i: None, r_t: [-721.861 -721.861 -721.861], eps: 1.0})
Step:   31900, Reward: [-408.431 -408.431 -408.431] [65.461], Avg: [-439.356 -439.356 -439.356] (0.0408) ({r_i: None, r_t: [-829.986 -829.986 -829.986], eps: 0.041})
Step:  140800, Reward: [-388.427 -388.427 -388.427] [41.752], Avg: [-420.796 -420.796 -420.796] (1.0000) ({r_i: None, r_t: [-737.153 -737.153 -737.153], eps: 1.0})
Step:   32000, Reward: [-421.424 -421.424 -421.424] [75.147], Avg: [-439.300 -439.300 -439.300] (0.0404) ({r_i: None, r_t: [-862.050 -862.050 -862.050], eps: 0.04})
Step:  140900, Reward: [-367.421 -367.421 -367.421] [51.752], Avg: [-420.758 -420.758 -420.758] (1.0000) ({r_i: None, r_t: [-740.555 -740.555 -740.555], eps: 1.0})
Step:   32100, Reward: [-424.755 -424.755 -424.755] [51.836], Avg: [-439.255 -439.255 -439.255] (0.0400) ({r_i: None, r_t: [-849.228 -849.228 -849.228], eps: 0.04})
Step:  141000, Reward: [-346.021 -346.021 -346.021] [54.493], Avg: [-420.705 -420.705 -420.705] (1.0000) ({r_i: None, r_t: [-771.318 -771.318 -771.318], eps: 1.0})
Step:   32200, Reward: [-427.311 -427.311 -427.311] [74.546], Avg: [-439.218 -439.218 -439.218] (0.0396) ({r_i: None, r_t: [-867.524 -867.524 -867.524], eps: 0.04})
Step:  141100, Reward: [-379.913 -379.913 -379.913] [83.602], Avg: [-420.676 -420.676 -420.676] (1.0000) ({r_i: None, r_t: [-728.577 -728.577 -728.577], eps: 1.0})
Step:   32300, Reward: [-395.857 -395.857 -395.857] [73.587], Avg: [-439.084 -439.084 -439.084] (0.0392) ({r_i: None, r_t: [-865.227 -865.227 -865.227], eps: 0.039})
Step:  141200, Reward: [-385.692 -385.692 -385.692] [76.996], Avg: [-420.652 -420.652 -420.652] (1.0000) ({r_i: None, r_t: [-738.038 -738.038 -738.038], eps: 1.0})
Step:   32400, Reward: [-412.685 -412.685 -412.685] [68.009], Avg: [-439.003 -439.003 -439.003] (0.0388) ({r_i: None, r_t: [-848.265 -848.265 -848.265], eps: 0.039})
Step:  141300, Reward: [-404.740 -404.740 -404.740] [66.972], Avg: [-420.640 -420.640 -420.640] (1.0000) ({r_i: None, r_t: [-735.131 -735.131 -735.131], eps: 1.0})
Step:   32500, Reward: [-428.564 -428.564 -428.564] [63.908], Avg: [-438.971 -438.971 -438.971] (0.0385) ({r_i: None, r_t: [-810.968 -810.968 -810.968], eps: 0.038})
Step:  141400, Reward: [-393.614 -393.614 -393.614] [80.723], Avg: [-420.621 -420.621 -420.621] (1.0000) ({r_i: None, r_t: [-734.590 -734.590 -734.590], eps: 1.0})
Step:   32600, Reward: [-402.073 -402.073 -402.073] [65.680], Avg: [-438.858 -438.858 -438.858] (0.0381) ({r_i: None, r_t: [-852.869 -852.869 -852.869], eps: 0.038})
Step:  141500, Reward: [-365.213 -365.213 -365.213] [63.439], Avg: [-420.582 -420.582 -420.582] (1.0000) ({r_i: None, r_t: [-743.539 -743.539 -743.539], eps: 1.0})
Step:   32700, Reward: [-403.426 -403.426 -403.426] [62.270], Avg: [-438.750 -438.750 -438.750] (0.0377) ({r_i: None, r_t: [-883.394 -883.394 -883.394], eps: 0.038})
Step:  141600, Reward: [-371.486 -371.486 -371.486] [74.800], Avg: [-420.547 -420.547 -420.547] (1.0000) ({r_i: None, r_t: [-744.024 -744.024 -744.024], eps: 1.0})
Step:   32800, Reward: [-411.081 -411.081 -411.081] [79.324], Avg: [-438.666 -438.666 -438.666] (0.0373) ({r_i: None, r_t: [-847.014 -847.014 -847.014], eps: 0.037})
Step:  141700, Reward: [-358.968 -358.968 -358.968] [68.561], Avg: [-420.504 -420.504 -420.504] (1.0000) ({r_i: None, r_t: [-737.037 -737.037 -737.037], eps: 1.0})
Step:   32900, Reward: [-399.263 -399.263 -399.263] [60.769], Avg: [-438.547 -438.547 -438.547] (0.0369) ({r_i: None, r_t: [-842.461 -842.461 -842.461], eps: 0.037})
Step:  141800, Reward: [-355.154 -355.154 -355.154] [43.411], Avg: [-420.458 -420.458 -420.458] (1.0000) ({r_i: None, r_t: [-778.977 -778.977 -778.977], eps: 1.0})
Step:   33000, Reward: [-413.878 -413.878 -413.878] [57.656], Avg: [-438.472 -438.472 -438.472] (0.0366) ({r_i: None, r_t: [-884.179 -884.179 -884.179], eps: 0.037})
Step:  141900, Reward: [-401.432 -401.432 -401.432] [84.630], Avg: [-420.445 -420.445 -420.445] (1.0000) ({r_i: None, r_t: [-753.123 -753.123 -753.123], eps: 1.0})
Step:   33100, Reward: [-420.293 -420.293 -420.293] [70.735], Avg: [-438.417 -438.417 -438.417] (0.0362) ({r_i: None, r_t: [-887.593 -887.593 -887.593], eps: 0.036})
Step:  142000, Reward: [-372.749 -372.749 -372.749] [77.766], Avg: [-420.411 -420.411 -420.411] (1.0000) ({r_i: None, r_t: [-761.936 -761.936 -761.936], eps: 1.0})
Step:   33200, Reward: [-403.588 -403.588 -403.588] [66.223], Avg: [-438.313 -438.313 -438.313] (0.0359) ({r_i: None, r_t: [-862.268 -862.268 -862.268], eps: 0.036})
Step:  142100, Reward: [-362.182 -362.182 -362.182] [55.314], Avg: [-420.370 -420.370 -420.370] (1.0000) ({r_i: None, r_t: [-738.305 -738.305 -738.305], eps: 1.0})
Step:   33300, Reward: [-406.661 -406.661 -406.661] [69.983], Avg: [-438.218 -438.218 -438.218] (0.0355) ({r_i: None, r_t: [-820.670 -820.670 -820.670], eps: 0.035})
Step:  142200, Reward: [-393.796 -393.796 -393.796] [58.073], Avg: [-420.351 -420.351 -420.351] (1.0000) ({r_i: None, r_t: [-766.879 -766.879 -766.879], eps: 1.0})
Step:   33400, Reward: [-440.308 -440.308 -440.308] [76.061], Avg: [-438.224 -438.224 -438.224] (0.0351) ({r_i: None, r_t: [-816.113 -816.113 -816.113], eps: 0.035})
Step:  142300, Reward: [-360.216 -360.216 -360.216] [89.158], Avg: [-420.309 -420.309 -420.309] (1.0000) ({r_i: None, r_t: [-759.605 -759.605 -759.605], eps: 1.0})
Step:   33500, Reward: [-436.115 -436.115 -436.115] [77.945], Avg: [-438.218 -438.218 -438.218] (0.0348) ({r_i: None, r_t: [-836.852 -836.852 -836.852], eps: 0.035})
Step:  142400, Reward: [-341.931 -341.931 -341.931] [47.430], Avg: [-420.254 -420.254 -420.254] (1.0000) ({r_i: None, r_t: [-732.644 -732.644 -732.644], eps: 1.0})
Step:   33600, Reward: [-404.904 -404.904 -404.904] [70.707], Avg: [-438.119 -438.119 -438.119] (0.0344) ({r_i: None, r_t: [-849.094 -849.094 -849.094], eps: 0.034})
Step:  142500, Reward: [-394.779 -394.779 -394.779] [80.449], Avg: [-420.236 -420.236 -420.236] (1.0000) ({r_i: None, r_t: [-722.946 -722.946 -722.946], eps: 1.0})
Step:   33700, Reward: [-432.590 -432.590 -432.590] [65.474], Avg: [-438.103 -438.103 -438.103] (0.0341) ({r_i: None, r_t: [-815.743 -815.743 -815.743], eps: 0.034})
Step:  142600, Reward: [-368.386 -368.386 -368.386] [72.260], Avg: [-420.200 -420.200 -420.200] (1.0000) ({r_i: None, r_t: [-727.825 -727.825 -727.825], eps: 1.0})
Step:   33800, Reward: [-435.314 -435.314 -435.314] [61.683], Avg: [-438.095 -438.095 -438.095] (0.0338) ({r_i: None, r_t: [-825.856 -825.856 -825.856], eps: 0.034})
Step:  142700, Reward: [-383.442 -383.442 -383.442] [74.426], Avg: [-420.174 -420.174 -420.174] (1.0000) ({r_i: None, r_t: [-721.270 -721.270 -721.270], eps: 1.0})
Step:   33900, Reward: [-405.077 -405.077 -405.077] [64.661], Avg: [-437.998 -437.998 -437.998] (0.0334) ({r_i: None, r_t: [-779.692 -779.692 -779.692], eps: 0.033})
Step:  142800, Reward: [-375.321 -375.321 -375.321] [72.166], Avg: [-420.143 -420.143 -420.143] (1.0000) ({r_i: None, r_t: [-750.453 -750.453 -750.453], eps: 1.0})
Step:   34000, Reward: [-410.908 -410.908 -410.908] [55.665], Avg: [-437.918 -437.918 -437.918] (0.0331) ({r_i: None, r_t: [-839.328 -839.328 -839.328], eps: 0.033})
Step:  142900, Reward: [-371.736 -371.736 -371.736] [79.222], Avg: [-420.109 -420.109 -420.109] (1.0000) ({r_i: None, r_t: [-713.235 -713.235 -713.235], eps: 1.0})
Step:   34100, Reward: [-414.260 -414.260 -414.260] [57.249], Avg: [-437.849 -437.849 -437.849] (0.0328) ({r_i: None, r_t: [-852.813 -852.813 -852.813], eps: 0.033})
Step:  143000, Reward: [-361.709 -361.709 -361.709] [69.090], Avg: [-420.068 -420.068 -420.068] (1.0000) ({r_i: None, r_t: [-759.724 -759.724 -759.724], eps: 1.0})
Step:   34200, Reward: [-413.157 -413.157 -413.157] [65.980], Avg: [-437.777 -437.777 -437.777] (0.0324) ({r_i: None, r_t: [-816.196 -816.196 -816.196], eps: 0.032})
Step:  143100, Reward: [-366.421 -366.421 -366.421] [68.909], Avg: [-420.031 -420.031 -420.031] (1.0000) ({r_i: None, r_t: [-723.845 -723.845 -723.845], eps: 1.0})
Step:   34300, Reward: [-413.790 -413.790 -413.790] [68.390], Avg: [-437.707 -437.707 -437.707] (0.0321) ({r_i: None, r_t: [-761.732 -761.732 -761.732], eps: 0.032})
Step:  143200, Reward: [-356.672 -356.672 -356.672] [54.249], Avg: [-419.986 -419.986 -419.986] (1.0000) ({r_i: None, r_t: [-736.209 -736.209 -736.209], eps: 1.0})
Step:   34400, Reward: [-433.482 -433.482 -433.482] [62.737], Avg: [-437.695 -437.695 -437.695] (0.0318) ({r_i: None, r_t: [-830.568 -830.568 -830.568], eps: 0.032})
Step:  143300, Reward: [-362.194 -362.194 -362.194] [66.244], Avg: [-419.946 -419.946 -419.946] (1.0000) ({r_i: None, r_t: [-739.237 -739.237 -739.237], eps: 1.0})
Step:   34500, Reward: [-402.807 -402.807 -402.807] [82.655], Avg: [-437.594 -437.594 -437.594] (0.0315) ({r_i: None, r_t: [-829.290 -829.290 -829.290], eps: 0.031})
Step:  143400, Reward: [-362.662 -362.662 -362.662] [52.940], Avg: [-419.906 -419.906 -419.906] (1.0000) ({r_i: None, r_t: [-732.264 -732.264 -732.264], eps: 1.0})
Step:   34600, Reward: [-425.639 -425.639 -425.639] [90.518], Avg: [-437.560 -437.560 -437.560] (0.0312) ({r_i: None, r_t: [-854.228 -854.228 -854.228], eps: 0.031})
Step:  143500, Reward: [-367.495 -367.495 -367.495] [62.080], Avg: [-419.870 -419.870 -419.870] (1.0000) ({r_i: None, r_t: [-716.994 -716.994 -716.994], eps: 1.0})
Step:   34700, Reward: [-427.254 -427.254 -427.254] [66.685], Avg: [-437.530 -437.530 -437.530] (0.0308) ({r_i: None, r_t: [-838.977 -838.977 -838.977], eps: 0.031})
Step:  143600, Reward: [-369.823 -369.823 -369.823] [61.450], Avg: [-419.835 -419.835 -419.835] (1.0000) ({r_i: None, r_t: [-748.762 -748.762 -748.762], eps: 1.0})
Step:   34800, Reward: [-383.495 -383.495 -383.495] [62.227], Avg: [-437.375 -437.375 -437.375] (0.0305) ({r_i: None, r_t: [-827.343 -827.343 -827.343], eps: 0.031})
Step:  143700, Reward: [-383.015 -383.015 -383.015] [74.318], Avg: [-419.809 -419.809 -419.809] (1.0000) ({r_i: None, r_t: [-753.713 -753.713 -753.713], eps: 1.0})
Step:   34900, Reward: [-417.968 -417.968 -417.968] [63.132], Avg: [-437.320 -437.320 -437.320] (0.0302) ({r_i: None, r_t: [-843.406 -843.406 -843.406], eps: 0.03})
Step:  143800, Reward: [-394.818 -394.818 -394.818] [65.249], Avg: [-419.792 -419.792 -419.792] (1.0000) ({r_i: None, r_t: [-676.307 -676.307 -676.307], eps: 1.0})
Step:   35000, Reward: [-413.871 -413.871 -413.871] [71.020], Avg: [-437.253 -437.253 -437.253] (0.0299) ({r_i: None, r_t: [-811.720 -811.720 -811.720], eps: 0.03})
Step:  143900, Reward: [-364.615 -364.615 -364.615] [76.503], Avg: [-419.754 -419.754 -419.754] (1.0000) ({r_i: None, r_t: [-749.994 -749.994 -749.994], eps: 1.0})
Step:   35100, Reward: [-407.566 -407.566 -407.566] [54.468], Avg: [-437.169 -437.169 -437.169] (0.0296) ({r_i: None, r_t: [-777.914 -777.914 -777.914], eps: 0.03})
Step:  144000, Reward: [-367.757 -367.757 -367.757] [80.789], Avg: [-419.718 -419.718 -419.718] (1.0000) ({r_i: None, r_t: [-723.218 -723.218 -723.218], eps: 1.0})
Step:   35200, Reward: [-362.791 -362.791 -362.791] [59.926], Avg: [-436.958 -436.958 -436.958] (0.0293) ({r_i: None, r_t: [-814.697 -814.697 -814.697], eps: 0.029})
Step:  144100, Reward: [-391.621 -391.621 -391.621] [70.016], Avg: [-419.698 -419.698 -419.698] (1.0000) ({r_i: None, r_t: [-745.703 -745.703 -745.703], eps: 1.0})
Step:   35300, Reward: [-426.070 -426.070 -426.070] [67.476], Avg: [-436.927 -436.927 -436.927] (0.0290) ({r_i: None, r_t: [-852.245 -852.245 -852.245], eps: 0.029})
Step:  144200, Reward: [-378.756 -378.756 -378.756] [71.549], Avg: [-419.670 -419.670 -419.670] (1.0000) ({r_i: None, r_t: [-718.275 -718.275 -718.275], eps: 1.0})
Step:   35400, Reward: [-405.775 -405.775 -405.775] [78.987], Avg: [-436.839 -436.839 -436.839] (0.0288) ({r_i: None, r_t: [-789.953 -789.953 -789.953], eps: 0.029})
Step:  144300, Reward: [-377.021 -377.021 -377.021] [67.591], Avg: [-419.640 -419.640 -419.640] (1.0000) ({r_i: None, r_t: [-684.898 -684.898 -684.898], eps: 1.0})
Step:   35500, Reward: [-362.557 -362.557 -362.557] [57.518], Avg: [-436.631 -436.631 -436.631] (0.0285) ({r_i: None, r_t: [-804.779 -804.779 -804.779], eps: 0.028})
Step:  144400, Reward: [-364.608 -364.608 -364.608] [50.759], Avg: [-419.602 -419.602 -419.602] (1.0000) ({r_i: None, r_t: [-742.047 -742.047 -742.047], eps: 1.0})
Step:   35600, Reward: [-421.928 -421.928 -421.928] [72.893], Avg: [-436.590 -436.590 -436.590] (0.0282) ({r_i: None, r_t: [-821.131 -821.131 -821.131], eps: 0.028})
Step:  144500, Reward: [-363.615 -363.615 -363.615] [50.367], Avg: [-419.563 -419.563 -419.563] (1.0000) ({r_i: None, r_t: [-771.762 -771.762 -771.762], eps: 1.0})
Step:   35700, Reward: [-394.030 -394.030 -394.030] [48.858], Avg: [-436.471 -436.471 -436.471] (0.0279) ({r_i: None, r_t: [-799.623 -799.623 -799.623], eps: 0.028})
Step:  144600, Reward: [-399.306 -399.306 -399.306] [67.696], Avg: [-419.549 -419.549 -419.549] (1.0000) ({r_i: None, r_t: [-744.796 -744.796 -744.796], eps: 1.0})
Step:   35800, Reward: [-396.830 -396.830 -396.830] [68.299], Avg: [-436.360 -436.360 -436.360] (0.0276) ({r_i: None, r_t: [-773.161 -773.161 -773.161], eps: 0.028})
Step:  144700, Reward: [-380.927 -380.927 -380.927] [68.362], Avg: [-419.523 -419.523 -419.523] (1.0000) ({r_i: None, r_t: [-749.125 -749.125 -749.125], eps: 1.0})
Step:   35900, Reward: [-407.068 -407.068 -407.068] [67.559], Avg: [-436.279 -436.279 -436.279] (0.0274) ({r_i: None, r_t: [-793.738 -793.738 -793.738], eps: 0.027})
Step:  144800, Reward: [-358.837 -358.837 -358.837] [69.604], Avg: [-419.481 -419.481 -419.481] (1.0000) ({r_i: None, r_t: [-752.360 -752.360 -752.360], eps: 1.0})
Step:   36000, Reward: [-407.428 -407.428 -407.428] [57.680], Avg: [-436.199 -436.199 -436.199] (0.0271) ({r_i: None, r_t: [-836.967 -836.967 -836.967], eps: 0.027})
Step:  144900, Reward: [-376.675 -376.675 -376.675] [61.438], Avg: [-419.451 -419.451 -419.451] (1.0000) ({r_i: None, r_t: [-725.324 -725.324 -725.324], eps: 1.0})
Step:   36100, Reward: [-414.832 -414.832 -414.832] [61.245], Avg: [-436.140 -436.140 -436.140] (0.0268) ({r_i: None, r_t: [-855.816 -855.816 -855.816], eps: 0.027})
Step:  145000, Reward: [-368.560 -368.560 -368.560] [65.523], Avg: [-419.416 -419.416 -419.416] (1.0000) ({r_i: None, r_t: [-735.524 -735.524 -735.524], eps: 1.0})
Step:   36200, Reward: [-395.941 -395.941 -395.941] [41.489], Avg: [-436.029 -436.029 -436.029] (0.0265) ({r_i: None, r_t: [-811.076 -811.076 -811.076], eps: 0.027})
Step:  145100, Reward: [-368.635 -368.635 -368.635] [63.175], Avg: [-419.381 -419.381 -419.381] (1.0000) ({r_i: None, r_t: [-729.288 -729.288 -729.288], eps: 1.0})
Step:   36300, Reward: [-385.922 -385.922 -385.922] [72.856], Avg: [-435.892 -435.892 -435.892] (0.0263) ({r_i: None, r_t: [-809.385 -809.385 -809.385], eps: 0.026})
Step:  145200, Reward: [-344.712 -344.712 -344.712] [62.618], Avg: [-419.330 -419.330 -419.330] (1.0000) ({r_i: None, r_t: [-721.626 -721.626 -721.626], eps: 1.0})
Step:   36400, Reward: [-395.884 -395.884 -395.884] [68.989], Avg: [-435.782 -435.782 -435.782] (0.0260) ({r_i: None, r_t: [-813.196 -813.196 -813.196], eps: 0.026})
Step:  145300, Reward: [-387.388 -387.388 -387.388] [65.551], Avg: [-419.308 -419.308 -419.308] (1.0000) ({r_i: None, r_t: [-733.877 -733.877 -733.877], eps: 1.0})
Step:   36500, Reward: [-416.538 -416.538 -416.538] [59.613], Avg: [-435.729 -435.729 -435.729] (0.0258) ({r_i: None, r_t: [-829.355 -829.355 -829.355], eps: 0.026})
Step:  145400, Reward: [-374.686 -374.686 -374.686] [62.440], Avg: [-419.277 -419.277 -419.277] (1.0000) ({r_i: None, r_t: [-752.555 -752.555 -752.555], eps: 1.0})
Step:   36600, Reward: [-405.649 -405.649 -405.649] [55.064], Avg: [-435.647 -435.647 -435.647] (0.0255) ({r_i: None, r_t: [-803.514 -803.514 -803.514], eps: 0.025})
Step:  145500, Reward: [-341.321 -341.321 -341.321] [70.470], Avg: [-419.224 -419.224 -419.224] (1.0000) ({r_i: None, r_t: [-754.263 -754.263 -754.263], eps: 1.0})
Step:   36700, Reward: [-413.585 -413.585 -413.585] [60.137], Avg: [-435.587 -435.587 -435.587] (0.0252) ({r_i: None, r_t: [-834.265 -834.265 -834.265], eps: 0.025})
Step:  145600, Reward: [-354.075 -354.075 -354.075] [67.022], Avg: [-419.179 -419.179 -419.179] (1.0000) ({r_i: None, r_t: [-782.797 -782.797 -782.797], eps: 1.0})
Step:   36800, Reward: [-420.012 -420.012 -420.012] [66.993], Avg: [-435.545 -435.545 -435.545] (0.0250) ({r_i: None, r_t: [-796.821 -796.821 -796.821], eps: 0.025})
Step:  145700, Reward: [-343.514 -343.514 -343.514] [53.693], Avg: [-419.127 -419.127 -419.127] (1.0000) ({r_i: None, r_t: [-730.126 -730.126 -730.126], eps: 1.0})
Step:   36900, Reward: [-391.698 -391.698 -391.698] [85.547], Avg: [-435.427 -435.427 -435.427] (0.0247) ({r_i: None, r_t: [-827.754 -827.754 -827.754], eps: 0.025})
Step:  145800, Reward: [-331.055 -331.055 -331.055] [59.323], Avg: [-419.067 -419.067 -419.067] (1.0000) ({r_i: None, r_t: [-771.187 -771.187 -771.187], eps: 1.0})
Step:   37000, Reward: [-399.665 -399.665 -399.665] [63.223], Avg: [-435.330 -435.330 -435.330] (0.0245) ({r_i: None, r_t: [-819.587 -819.587 -819.587], eps: 0.024})
Step:  145900, Reward: [-399.001 -399.001 -399.001] [76.555], Avg: [-419.053 -419.053 -419.053] (1.0000) ({r_i: None, r_t: [-743.029 -743.029 -743.029], eps: 1.0})
Step:   37100, Reward: [-408.433 -408.433 -408.433] [57.258], Avg: [-435.258 -435.258 -435.258] (0.0243) ({r_i: None, r_t: [-758.938 -758.938 -758.938], eps: 0.024})
Step:  146000, Reward: [-346.886 -346.886 -346.886] [62.491], Avg: [-419.004 -419.004 -419.004] (1.0000) ({r_i: None, r_t: [-764.994 -764.994 -764.994], eps: 1.0})
Step:   37200, Reward: [-396.395 -396.395 -396.395] [65.915], Avg: [-435.154 -435.154 -435.154] (0.0240) ({r_i: None, r_t: [-828.780 -828.780 -828.780], eps: 0.024})
Step:  146100, Reward: [-378.481 -378.481 -378.481] [84.206], Avg: [-418.976 -418.976 -418.976] (1.0000) ({r_i: None, r_t: [-751.248 -751.248 -751.248], eps: 1.0})
Step:   37300, Reward: [-410.345 -410.345 -410.345] [70.566], Avg: [-435.088 -435.088 -435.088] (0.0238) ({r_i: None, r_t: [-780.115 -780.115 -780.115], eps: 0.024})
Step:  146200, Reward: [-354.485 -354.485 -354.485] [68.632], Avg: [-418.932 -418.932 -418.932] (1.0000) ({r_i: None, r_t: [-718.310 -718.310 -718.310], eps: 1.0})
Step:   37400, Reward: [-416.072 -416.072 -416.072] [67.082], Avg: [-435.037 -435.037 -435.037] (0.0235) ({r_i: None, r_t: [-832.317 -832.317 -832.317], eps: 0.024})
Step:  146300, Reward: [-367.289 -367.289 -367.289] [81.369], Avg: [-418.896 -418.896 -418.896] (1.0000) ({r_i: None, r_t: [-749.678 -749.678 -749.678], eps: 1.0})
Step:   37500, Reward: [-395.107 -395.107 -395.107] [72.827], Avg: [-434.931 -434.931 -434.931] (0.0233) ({r_i: None, r_t: [-862.031 -862.031 -862.031], eps: 0.023})
Step:  146400, Reward: [-386.119 -386.119 -386.119] [54.576], Avg: [-418.874 -418.874 -418.874] (1.0000) ({r_i: None, r_t: [-706.231 -706.231 -706.231], eps: 1.0})
Step:   37600, Reward: [-381.043 -381.043 -381.043] [61.019], Avg: [-434.788 -434.788 -434.788] (0.0231) ({r_i: None, r_t: [-799.079 -799.079 -799.079], eps: 0.023})
Step:  146500, Reward: [-362.558 -362.558 -362.558] [58.979], Avg: [-418.836 -418.836 -418.836] (1.0000) ({r_i: None, r_t: [-746.311 -746.311 -746.311], eps: 1.0})
Step:   37700, Reward: [-421.649 -421.649 -421.649] [56.572], Avg: [-434.753 -434.753 -434.753] (0.0228) ({r_i: None, r_t: [-803.792 -803.792 -803.792], eps: 0.023})
Step:  146600, Reward: [-394.290 -394.290 -394.290] [70.259], Avg: [-418.819 -418.819 -418.819] (1.0000) ({r_i: None, r_t: [-750.314 -750.314 -750.314], eps: 1.0})
Step:   37800, Reward: [-382.507 -382.507 -382.507] [75.071], Avg: [-434.615 -434.615 -434.615] (0.0226) ({r_i: None, r_t: [-829.726 -829.726 -829.726], eps: 0.023})
Step:  146700, Reward: [-392.463 -392.463 -392.463] [91.445], Avg: [-418.801 -418.801 -418.801] (1.0000) ({r_i: None, r_t: [-772.882 -772.882 -772.882], eps: 1.0})
Step:   37900, Reward: [-425.648 -425.648 -425.648] [65.739], Avg: [-434.591 -434.591 -434.591] (0.0224) ({r_i: None, r_t: [-822.098 -822.098 -822.098], eps: 0.022})
Step:  146800, Reward: [-376.825 -376.825 -376.825] [47.653], Avg: [-418.772 -418.772 -418.772] (1.0000) ({r_i: None, r_t: [-768.485 -768.485 -768.485], eps: 1.0})
Step:   38000, Reward: [-399.265 -399.265 -399.265] [85.525], Avg: [-434.499 -434.499 -434.499] (0.0222) ({r_i: None, r_t: [-811.961 -811.961 -811.961], eps: 0.022})
Step:  146900, Reward: [-382.919 -382.919 -382.919] [68.994], Avg: [-418.748 -418.748 -418.748] (1.0000) ({r_i: None, r_t: [-736.051 -736.051 -736.051], eps: 1.0})
Step:   38100, Reward: [-408.870 -408.870 -408.870] [82.010], Avg: [-434.432 -434.432 -434.432] (0.0219) ({r_i: None, r_t: [-800.360 -800.360 -800.360], eps: 0.022})
Step:  147000, Reward: [-364.941 -364.941 -364.941] [61.820], Avg: [-418.711 -418.711 -418.711] (1.0000) ({r_i: None, r_t: [-779.755 -779.755 -779.755], eps: 1.0})
Step:   38200, Reward: [-367.568 -367.568 -367.568] [65.726], Avg: [-434.257 -434.257 -434.257] (0.0217) ({r_i: None, r_t: [-819.424 -819.424 -819.424], eps: 0.022})
Step:  147100, Reward: [-390.260 -390.260 -390.260] [77.543], Avg: [-418.692 -418.692 -418.692] (1.0000) ({r_i: None, r_t: [-767.753 -767.753 -767.753], eps: 1.0})
Step:   38300, Reward: [-393.765 -393.765 -393.765] [69.787], Avg: [-434.152 -434.152 -434.152] (0.0215) ({r_i: None, r_t: [-820.334 -820.334 -820.334], eps: 0.022})
Step:  147200, Reward: [-380.705 -380.705 -380.705] [67.961], Avg: [-418.666 -418.666 -418.666] (1.0000) ({r_i: None, r_t: [-772.854 -772.854 -772.854], eps: 1.0})
Step:   38400, Reward: [-428.656 -428.656 -428.656] [77.619], Avg: [-434.137 -434.137 -434.137] (0.0213) ({r_i: None, r_t: [-796.058 -796.058 -796.058], eps: 0.021})
Step:  147300, Reward: [-386.598 -386.598 -386.598] [70.145], Avg: [-418.645 -418.645 -418.645] (1.0000) ({r_i: None, r_t: [-715.143 -715.143 -715.143], eps: 1.0})
Step:   38500, Reward: [-394.840 -394.840 -394.840] [67.100], Avg: [-434.036 -434.036 -434.036] (0.0211) ({r_i: None, r_t: [-822.746 -822.746 -822.746], eps: 0.021})
Step:  147400, Reward: [-374.250 -374.250 -374.250] [46.811], Avg: [-418.614 -418.614 -418.614] (1.0000) ({r_i: None, r_t: [-760.742 -760.742 -760.742], eps: 1.0})
Step:   38600, Reward: [-436.865 -436.865 -436.865] [68.296], Avg: [-434.043 -434.043 -434.043] (0.0209) ({r_i: None, r_t: [-813.501 -813.501 -813.501], eps: 0.021})
Step:  147500, Reward: [-330.230 -330.230 -330.230] [49.251], Avg: [-418.555 -418.555 -418.555] (1.0000) ({r_i: None, r_t: [-723.248 -723.248 -723.248], eps: 1.0})
Step:   38700, Reward: [-443.500 -443.500 -443.500] [65.222], Avg: [-434.067 -434.067 -434.067] (0.0207) ({r_i: None, r_t: [-779.852 -779.852 -779.852], eps: 0.021})
Step:  147600, Reward: [-372.484 -372.484 -372.484] [56.576], Avg: [-418.523 -418.523 -418.523] (1.0000) ({r_i: None, r_t: [-769.243 -769.243 -769.243], eps: 1.0})
Step:   38800, Reward: [-406.993 -406.993 -406.993] [78.882], Avg: [-433.998 -433.998 -433.998] (0.0205) ({r_i: None, r_t: [-863.881 -863.881 -863.881], eps: 0.02})
Step:  147700, Reward: [-381.114 -381.114 -381.114] [68.668], Avg: [-418.498 -418.498 -418.498] (1.0000) ({r_i: None, r_t: [-733.304 -733.304 -733.304], eps: 1.0})
Step:   38900, Reward: [-403.570 -403.570 -403.570] [81.864], Avg: [-433.920 -433.920 -433.920] (0.0202) ({r_i: None, r_t: [-785.102 -785.102 -785.102], eps: 0.02})
Step:  147800, Reward: [-341.506 -341.506 -341.506] [51.733], Avg: [-418.446 -418.446 -418.446] (1.0000) ({r_i: None, r_t: [-731.692 -731.692 -731.692], eps: 1.0})
Step:   39000, Reward: [-386.260 -386.260 -386.260] [58.899], Avg: [-433.798 -433.798 -433.798] (0.0200) ({r_i: None, r_t: [-774.908 -774.908 -774.908], eps: 0.02})
Step:  147900, Reward: [-359.399 -359.399 -359.399] [53.658], Avg: [-418.406 -418.406 -418.406] (1.0000) ({r_i: None, r_t: [-707.655 -707.655 -707.655], eps: 1.0})
Step:   39100, Reward: [-434.140 -434.140 -434.140] [68.844], Avg: [-433.799 -433.799 -433.799] (0.0198) ({r_i: None, r_t: [-840.891 -840.891 -840.891], eps: 0.02})
Step:  148000, Reward: [-359.441 -359.441 -359.441] [56.811], Avg: [-418.366 -418.366 -418.366] (1.0000) ({r_i: None, r_t: [-754.891 -754.891 -754.891], eps: 1.0})
Step:   39200, Reward: [-431.616 -431.616 -431.616] [57.296], Avg: [-433.793 -433.793 -433.793] (0.0196) ({r_i: None, r_t: [-794.278 -794.278 -794.278], eps: 0.02})
Step:  148100, Reward: [-365.940 -365.940 -365.940] [61.887], Avg: [-418.331 -418.331 -418.331] (1.0000) ({r_i: None, r_t: [-745.654 -745.654 -745.654], eps: 1.0})
Step:   39300, Reward: [-414.651 -414.651 -414.651] [45.638], Avg: [-433.744 -433.744 -433.744] (0.0195) ({r_i: None, r_t: [-823.274 -823.274 -823.274], eps: 0.019})
Step:  148200, Reward: [-351.738 -351.738 -351.738] [50.222], Avg: [-418.286 -418.286 -418.286] (1.0000) ({r_i: None, r_t: [-721.204 -721.204 -721.204], eps: 1.0})
Step:   39400, Reward: [-410.836 -410.836 -410.836] [45.529], Avg: [-433.686 -433.686 -433.686] (0.0193) ({r_i: None, r_t: [-800.941 -800.941 -800.941], eps: 0.019})
Step:  148300, Reward: [-381.577 -381.577 -381.577] [78.847], Avg: [-418.261 -418.261 -418.261] (1.0000) ({r_i: None, r_t: [-719.920 -719.920 -719.920], eps: 1.0})
Step:   39500, Reward: [-385.830 -385.830 -385.830] [41.411], Avg: [-433.566 -433.566 -433.566] (0.0191) ({r_i: None, r_t: [-839.886 -839.886 -839.886], eps: 0.019})
Step:  148400, Reward: [-397.535 -397.535 -397.535] [61.781], Avg: [-418.247 -418.247 -418.247] (1.0000) ({r_i: None, r_t: [-748.648 -748.648 -748.648], eps: 1.0})
Step:   39600, Reward: [-421.164 -421.164 -421.164] [59.768], Avg: [-433.534 -433.534 -433.534] (0.0189) ({r_i: None, r_t: [-842.933 -842.933 -842.933], eps: 0.019})
Step:  148500, Reward: [-359.985 -359.985 -359.985] [74.778], Avg: [-418.208 -418.208 -418.208] (1.0000) ({r_i: None, r_t: [-754.194 -754.194 -754.194], eps: 1.0})
Step:   39700, Reward: [-438.605 -438.605 -438.605] [68.333], Avg: [-433.547 -433.547 -433.547] (0.0187) ({r_i: None, r_t: [-879.177 -879.177 -879.177], eps: 0.019})
Step:  148600, Reward: [-358.323 -358.323 -358.323] [81.788], Avg: [-418.168 -418.168 -418.168] (1.0000) ({r_i: None, r_t: [-752.983 -752.983 -752.983], eps: 1.0})
Step:   39800, Reward: [-424.309 -424.309 -424.309] [66.006], Avg: [-433.524 -433.524 -433.524] (0.0185) ({r_i: None, r_t: [-812.096 -812.096 -812.096], eps: 0.019})
Step:  148700, Reward: [-356.126 -356.126 -356.126] [73.217], Avg: [-418.126 -418.126 -418.126] (1.0000) ({r_i: None, r_t: [-746.901 -746.901 -746.901], eps: 1.0})
Step:   39900, Reward: [-365.817 -365.817 -365.817] [50.070], Avg: [-433.355 -433.355 -433.355] (0.0183) ({r_i: None, r_t: [-811.558 -811.558 -811.558], eps: 0.018})
Step:  148800, Reward: [-375.259 -375.259 -375.259] [52.962], Avg: [-418.097 -418.097 -418.097] (1.0000) ({r_i: None, r_t: [-715.575 -715.575 -715.575], eps: 1.0})
Step:   40000, Reward: [-411.007 -411.007 -411.007] [66.198], Avg: [-433.299 -433.299 -433.299] (0.0181) ({r_i: None, r_t: [-853.140 -853.140 -853.140], eps: 0.018})
Step:  148900, Reward: [-375.760 -375.760 -375.760] [68.803], Avg: [-418.069 -418.069 -418.069] (1.0000) ({r_i: None, r_t: [-756.288 -756.288 -756.288], eps: 1.0})
Step:   40100, Reward: [-408.012 -408.012 -408.012] [48.358], Avg: [-433.236 -433.236 -433.236] (0.0180) ({r_i: None, r_t: [-830.322 -830.322 -830.322], eps: 0.018})
Step:  149000, Reward: [-404.580 -404.580 -404.580] [59.078], Avg: [-418.060 -418.060 -418.060] (1.0000) ({r_i: None, r_t: [-770.856 -770.856 -770.856], eps: 1.0})
Step:   40200, Reward: [-416.550 -416.550 -416.550] [82.285], Avg: [-433.195 -433.195 -433.195] (0.0178) ({r_i: None, r_t: [-847.141 -847.141 -847.141], eps: 0.018})
Step:  149100, Reward: [-373.054 -373.054 -373.054] [65.883], Avg: [-418.030 -418.030 -418.030] (1.0000) ({r_i: None, r_t: [-738.343 -738.343 -738.343], eps: 1.0})
Step:   40300, Reward: [-404.407 -404.407 -404.407] [81.721], Avg: [-433.123 -433.123 -433.123] (0.0176) ({r_i: None, r_t: [-837.653 -837.653 -837.653], eps: 0.018})
Step:  149200, Reward: [-375.845 -375.845 -375.845] [58.807], Avg: [-418.001 -418.001 -418.001] (1.0000) ({r_i: None, r_t: [-770.227 -770.227 -770.227], eps: 1.0})
Step:   40400, Reward: [-422.581 -422.581 -422.581] [59.863], Avg: [-433.097 -433.097 -433.097] (0.0174) ({r_i: None, r_t: [-784.563 -784.563 -784.563], eps: 0.017})
Step:  149300, Reward: [-340.872 -340.872 -340.872] [65.912], Avg: [-417.950 -417.950 -417.950] (1.0000) ({r_i: None, r_t: [-754.832 -754.832 -754.832], eps: 1.0})
Step:   40500, Reward: [-423.954 -423.954 -423.954] [72.111], Avg: [-433.075 -433.075 -433.075] (0.0172) ({r_i: None, r_t: [-797.568 -797.568 -797.568], eps: 0.017})
Step:  149400, Reward: [-393.503 -393.503 -393.503] [81.811], Avg: [-417.934 -417.934 -417.934] (1.0000) ({r_i: None, r_t: [-732.354 -732.354 -732.354], eps: 1.0})
Step:   40600, Reward: [-408.234 -408.234 -408.234] [43.584], Avg: [-433.014 -433.014 -433.014] (0.0171) ({r_i: None, r_t: [-819.936 -819.936 -819.936], eps: 0.017})
Step:  149500, Reward: [-343.689 -343.689 -343.689] [43.347], Avg: [-417.884 -417.884 -417.884] (1.0000) ({r_i: None, r_t: [-697.634 -697.634 -697.634], eps: 1.0})
Step:   40700, Reward: [-456.097 -456.097 -456.097] [56.406], Avg: [-433.070 -433.070 -433.070] (0.0169) ({r_i: None, r_t: [-853.722 -853.722 -853.722], eps: 0.017})
Step:  149600, Reward: [-337.701 -337.701 -337.701] [53.308], Avg: [-417.830 -417.830 -417.830] (1.0000) ({r_i: None, r_t: [-720.153 -720.153 -720.153], eps: 1.0})
Step:   40800, Reward: [-420.875 -420.875 -420.875] [66.798], Avg: [-433.041 -433.041 -433.041] (0.0167) ({r_i: None, r_t: [-841.472 -841.472 -841.472], eps: 0.017})
Step:  149700, Reward: [-363.237 -363.237 -363.237] [66.852], Avg: [-417.794 -417.794 -417.794] (1.0000) ({r_i: None, r_t: [-725.533 -725.533 -725.533], eps: 1.0})
Step:   40900, Reward: [-419.567 -419.567 -419.567] [69.076], Avg: [-433.008 -433.008 -433.008] (0.0166) ({r_i: None, r_t: [-795.474 -795.474 -795.474], eps: 0.017})
Step:  149800, Reward: [-338.522 -338.522 -338.522] [55.225], Avg: [-417.741 -417.741 -417.741] (1.0000) ({r_i: None, r_t: [-752.716 -752.716 -752.716], eps: 1.0})
Step:   41000, Reward: [-398.440 -398.440 -398.440] [71.818], Avg: [-432.924 -432.924 -432.924] (0.0164) ({r_i: None, r_t: [-802.890 -802.890 -802.890], eps: 0.016})
Step:  149900, Reward: [-388.150 -388.150 -388.150] [64.878], Avg: [-417.721 -417.721 -417.721] (1.0000) ({r_i: None, r_t: [-683.416 -683.416 -683.416], eps: 1.0})
Step:   41100, Reward: [-450.697 -450.697 -450.697] [54.692], Avg: [-432.967 -432.967 -432.967] (0.0162) ({r_i: None, r_t: [-811.481 -811.481 -811.481], eps: 0.016})
Step:  150000, Reward: [-362.318 -362.318 -362.318] [63.276], Avg: [-417.684 -417.684 -417.684] (1.0000) ({r_i: None, r_t: [-751.760 -751.760 -751.760], eps: 1.0})
Step:   41200, Reward: [-418.160 -418.160 -418.160] [71.768], Avg: [-432.931 -432.931 -432.931] (0.0161) ({r_i: None, r_t: [-794.481 -794.481 -794.481], eps: 0.016})
Step:  150100, Reward: [-391.526 -391.526 -391.526] [62.604], Avg: [-417.667 -417.667 -417.667] (1.0000) ({r_i: None, r_t: [-739.457 -739.457 -739.457], eps: 1.0})
Step:   41300, Reward: [-418.461 -418.461 -418.461] [60.417], Avg: [-432.896 -432.896 -432.896] (0.0159) ({r_i: None, r_t: [-840.278 -840.278 -840.278], eps: 0.016})
Step:  150200, Reward: [-361.665 -361.665 -361.665] [52.702], Avg: [-417.630 -417.630 -417.630] (1.0000) ({r_i: None, r_t: [-719.943 -719.943 -719.943], eps: 1.0})
Step:   41400, Reward: [-365.246 -365.246 -365.246] [57.500], Avg: [-432.733 -432.733 -432.733] (0.0158) ({r_i: None, r_t: [-809.544 -809.544 -809.544], eps: 0.016})
Step:  150300, Reward: [-358.608 -358.608 -358.608] [74.636], Avg: [-417.590 -417.590 -417.590] (1.0000) ({r_i: None, r_t: [-713.644 -713.644 -713.644], eps: 1.0})
Step:   41500, Reward: [-411.638 -411.638 -411.638] [73.686], Avg: [-432.682 -432.682 -432.682] (0.0156) ({r_i: None, r_t: [-829.174 -829.174 -829.174], eps: 0.016})
Step:  150400, Reward: [-364.338 -364.338 -364.338] [47.998], Avg: [-417.555 -417.555 -417.555] (1.0000) ({r_i: None, r_t: [-710.522 -710.522 -710.522], eps: 1.0})
Step:   41600, Reward: [-407.851 -407.851 -407.851] [66.379], Avg: [-432.623 -432.623 -432.623] (0.0154) ({r_i: None, r_t: [-778.498 -778.498 -778.498], eps: 0.015})
Step:  150500, Reward: [-352.383 -352.383 -352.383] [76.510], Avg: [-417.512 -417.512 -417.512] (1.0000) ({r_i: None, r_t: [-771.264 -771.264 -771.264], eps: 1.0})
Step:   41700, Reward: [-432.067 -432.067 -432.067] [62.547], Avg: [-432.621 -432.621 -432.621] (0.0153) ({r_i: None, r_t: [-798.250 -798.250 -798.250], eps: 0.015})
Step:  150600, Reward: [-383.607 -383.607 -383.607] [70.559], Avg: [-417.489 -417.489 -417.489] (1.0000) ({r_i: None, r_t: [-704.280 -704.280 -704.280], eps: 1.0})
Step:   41800, Reward: [-411.164 -411.164 -411.164] [61.149], Avg: [-432.570 -432.570 -432.570] (0.0151) ({r_i: None, r_t: [-814.729 -814.729 -814.729], eps: 0.015})
Step:  150700, Reward: [-380.642 -380.642 -380.642] [78.020], Avg: [-417.465 -417.465 -417.465] (1.0000) ({r_i: None, r_t: [-771.542 -771.542 -771.542], eps: 1.0})
Step:   41900, Reward: [-429.399 -429.399 -429.399] [55.838], Avg: [-432.563 -432.563 -432.563] (0.0150) ({r_i: None, r_t: [-801.499 -801.499 -801.499], eps: 0.015})
Step:  150800, Reward: [-378.885 -378.885 -378.885] [52.618], Avg: [-417.439 -417.439 -417.439] (1.0000) ({r_i: None, r_t: [-761.748 -761.748 -761.748], eps: 1.0})
Step:   42000, Reward: [-412.696 -412.696 -412.696] [56.977], Avg: [-432.515 -432.515 -432.515] (0.0148) ({r_i: None, r_t: [-802.402 -802.402 -802.402], eps: 0.015})
Step:  150900, Reward: [-369.063 -369.063 -369.063] [65.027], Avg: [-417.407 -417.407 -417.407] (1.0000) ({r_i: None, r_t: [-778.679 -778.679 -778.679], eps: 1.0})
Step:   42100, Reward: [-437.827 -437.827 -437.827] [77.196], Avg: [-432.528 -432.528 -432.528] (0.0147) ({r_i: None, r_t: [-850.531 -850.531 -850.531], eps: 0.015})
Step:  151000, Reward: [-362.480 -362.480 -362.480] [78.686], Avg: [-417.371 -417.371 -417.371] (1.0000) ({r_i: None, r_t: [-775.644 -775.644 -775.644], eps: 1.0})
Step:   42200, Reward: [-440.564 -440.564 -440.564] [68.900], Avg: [-432.547 -432.547 -432.547] (0.0145) ({r_i: None, r_t: [-830.006 -830.006 -830.006], eps: 0.015})
Step:  151100, Reward: [-355.442 -355.442 -355.442] [67.649], Avg: [-417.330 -417.330 -417.330] (1.0000) ({r_i: None, r_t: [-723.160 -723.160 -723.160], eps: 1.0})
Step:   42300, Reward: [-432.022 -432.022 -432.022] [68.957], Avg: [-432.546 -432.546 -432.546] (0.0144) ({r_i: None, r_t: [-836.433 -836.433 -836.433], eps: 0.014})
Step:  151200, Reward: [-360.957 -360.957 -360.957] [71.826], Avg: [-417.293 -417.293 -417.293] (1.0000) ({r_i: None, r_t: [-743.801 -743.801 -743.801], eps: 1.0})
Step:   42400, Reward: [-413.145 -413.145 -413.145] [53.903], Avg: [-432.500 -432.500 -432.500] (0.0143) ({r_i: None, r_t: [-832.321 -832.321 -832.321], eps: 0.014})
Step:  151300, Reward: [-367.661 -367.661 -367.661] [43.187], Avg: [-417.260 -417.260 -417.260] (1.0000) ({r_i: None, r_t: [-724.082 -724.082 -724.082], eps: 1.0})
Step:   42500, Reward: [-409.156 -409.156 -409.156] [62.142], Avg: [-432.445 -432.445 -432.445] (0.0141) ({r_i: None, r_t: [-874.921 -874.921 -874.921], eps: 0.014})
Step:  151400, Reward: [-354.027 -354.027 -354.027] [66.810], Avg: [-417.218 -417.218 -417.218] (1.0000) ({r_i: None, r_t: [-718.655 -718.655 -718.655], eps: 1.0})
Step:   42600, Reward: [-424.869 -424.869 -424.869] [76.237], Avg: [-432.428 -432.428 -432.428] (0.0140) ({r_i: None, r_t: [-851.447 -851.447 -851.447], eps: 0.014})
Step:  151500, Reward: [-368.514 -368.514 -368.514] [61.069], Avg: [-417.186 -417.186 -417.186] (1.0000) ({r_i: None, r_t: [-724.513 -724.513 -724.513], eps: 1.0})
Step:   42700, Reward: [-426.672 -426.672 -426.672] [65.453], Avg: [-432.414 -432.414 -432.414] (0.0138) ({r_i: None, r_t: [-838.094 -838.094 -838.094], eps: 0.014})
Step:  151600, Reward: [-341.656 -341.656 -341.656] [66.563], Avg: [-417.136 -417.136 -417.136] (1.0000) ({r_i: None, r_t: [-722.380 -722.380 -722.380], eps: 1.0})
Step:   42800, Reward: [-420.304 -420.304 -420.304] [65.695], Avg: [-432.386 -432.386 -432.386] (0.0137) ({r_i: None, r_t: [-784.861 -784.861 -784.861], eps: 0.014})
Step:  151700, Reward: [-373.365 -373.365 -373.365] [60.205], Avg: [-417.107 -417.107 -417.107] (1.0000) ({r_i: None, r_t: [-708.911 -708.911 -708.911], eps: 1.0})
Step:   42900, Reward: [-420.169 -420.169 -420.169] [57.217], Avg: [-432.357 -432.357 -432.357] (0.0136) ({r_i: None, r_t: [-795.654 -795.654 -795.654], eps: 0.014})
Step:  151800, Reward: [-369.224 -369.224 -369.224] [51.490], Avg: [-417.076 -417.076 -417.076] (1.0000) ({r_i: None, r_t: [-744.617 -744.617 -744.617], eps: 1.0})
Step:   43000, Reward: [-466.190 -466.190 -466.190] [72.247], Avg: [-432.436 -432.436 -432.436] (0.0134) ({r_i: None, r_t: [-842.886 -842.886 -842.886], eps: 0.013})
Step:  151900, Reward: [-388.613 -388.613 -388.613] [85.515], Avg: [-417.057 -417.057 -417.057] (1.0000) ({r_i: None, r_t: [-729.187 -729.187 -729.187], eps: 1.0})
Step:   43100, Reward: [-414.470 -414.470 -414.470] [47.458], Avg: [-432.394 -432.394 -432.394] (0.0133) ({r_i: None, r_t: [-853.081 -853.081 -853.081], eps: 0.013})
Step:  152000, Reward: [-356.607 -356.607 -356.607] [66.040], Avg: [-417.017 -417.017 -417.017] (1.0000) ({r_i: None, r_t: [-756.838 -756.838 -756.838], eps: 1.0})
Step:   43200, Reward: [-436.152 -436.152 -436.152] [59.785], Avg: [-432.403 -432.403 -432.403] (0.0132) ({r_i: None, r_t: [-859.592 -859.592 -859.592], eps: 0.013})
Step:  152100, Reward: [-339.141 -339.141 -339.141] [55.583], Avg: [-416.966 -416.966 -416.966] (1.0000) ({r_i: None, r_t: [-743.951 -743.951 -743.951], eps: 1.0})
Step:   43300, Reward: [-427.063 -427.063 -427.063] [55.874], Avg: [-432.391 -432.391 -432.391] (0.0130) ({r_i: None, r_t: [-873.414 -873.414 -873.414], eps: 0.013})
Step:  152200, Reward: [-360.369 -360.369 -360.369] [66.397], Avg: [-416.929 -416.929 -416.929] (1.0000) ({r_i: None, r_t: [-745.742 -745.742 -745.742], eps: 1.0})
Step:   43400, Reward: [-421.136 -421.136 -421.136] [68.562], Avg: [-432.365 -432.365 -432.365] (0.0129) ({r_i: None, r_t: [-805.117 -805.117 -805.117], eps: 0.013})
Step:  152300, Reward: [-374.553 -374.553 -374.553] [76.970], Avg: [-416.901 -416.901 -416.901] (1.0000) ({r_i: None, r_t: [-746.265 -746.265 -746.265], eps: 1.0})
Step:   43500, Reward: [-415.696 -415.696 -415.696] [78.764], Avg: [-432.327 -432.327 -432.327] (0.0128) ({r_i: None, r_t: [-830.493 -830.493 -830.493], eps: 0.013})
Step:  152400, Reward: [-367.103 -367.103 -367.103] [63.484], Avg: [-416.869 -416.869 -416.869] (1.0000) ({r_i: None, r_t: [-754.318 -754.318 -754.318], eps: 1.0})
Step:   43600, Reward: [-383.921 -383.921 -383.921] [71.632], Avg: [-432.216 -432.216 -432.216] (0.0126) ({r_i: None, r_t: [-794.285 -794.285 -794.285], eps: 0.013})
Step:  152500, Reward: [-396.541 -396.541 -396.541] [70.342], Avg: [-416.855 -416.855 -416.855] (1.0000) ({r_i: None, r_t: [-720.338 -720.338 -720.338], eps: 1.0})
Step:   43700, Reward: [-407.214 -407.214 -407.214] [63.797], Avg: [-432.159 -432.159 -432.159] (0.0125) ({r_i: None, r_t: [-803.250 -803.250 -803.250], eps: 0.013})
Step:  152600, Reward: [-350.880 -350.880 -350.880] [60.798], Avg: [-416.812 -416.812 -416.812] (1.0000) ({r_i: None, r_t: [-728.544 -728.544 -728.544], eps: 1.0})
Step:  152700, Reward: [-366.951 -366.951 -366.951] [68.364], Avg: [-416.779 -416.779 -416.779] (1.0000) ({r_i: None, r_t: [-772.465 -772.465 -772.465], eps: 1.0})
Step:   43800, Reward: [-410.134 -410.134 -410.134] [72.077], Avg: [-432.109 -432.109 -432.109] (0.0124) ({r_i: None, r_t: [-847.124 -847.124 -847.124], eps: 0.012})
Step:  152800, Reward: [-348.213 -348.213 -348.213] [70.400], Avg: [-416.735 -416.735 -416.735] (1.0000) ({r_i: None, r_t: [-740.438 -740.438 -740.438], eps: 1.0})
Step:   43900, Reward: [-422.413 -422.413 -422.413] [71.777], Avg: [-432.087 -432.087 -432.087] (0.0123) ({r_i: None, r_t: [-808.182 -808.182 -808.182], eps: 0.012})
Step:  152900, Reward: [-385.815 -385.815 -385.815] [77.197], Avg: [-416.714 -416.714 -416.714] (1.0000) ({r_i: None, r_t: [-728.013 -728.013 -728.013], eps: 1.0})
Step:   44000, Reward: [-413.990 -413.990 -413.990] [41.000], Avg: [-432.046 -432.046 -432.046] (0.0121) ({r_i: None, r_t: [-802.734 -802.734 -802.734], eps: 0.012})
Step:  153000, Reward: [-387.158 -387.158 -387.158] [77.002], Avg: [-416.695 -416.695 -416.695] (1.0000) ({r_i: None, r_t: [-695.386 -695.386 -695.386], eps: 1.0})
Step:   44100, Reward: [-427.194 -427.194 -427.194] [82.275], Avg: [-432.035 -432.035 -432.035] (0.0120) ({r_i: None, r_t: [-824.924 -824.924 -824.924], eps: 0.012})
Step:  153100, Reward: [-387.330 -387.330 -387.330] [84.102], Avg: [-416.676 -416.676 -416.676] (1.0000) ({r_i: None, r_t: [-666.231 -666.231 -666.231], eps: 1.0})
Step:   44200, Reward: [-439.863 -439.863 -439.863] [82.879], Avg: [-432.052 -432.052 -432.052] (0.0119) ({r_i: None, r_t: [-820.203 -820.203 -820.203], eps: 0.012})
Step:  153200, Reward: [-372.138 -372.138 -372.138] [75.024], Avg: [-416.647 -416.647 -416.647] (1.0000) ({r_i: None, r_t: [-728.212 -728.212 -728.212], eps: 1.0})
Step:   44300, Reward: [-433.376 -433.376 -433.376] [78.668], Avg: [-432.055 -432.055 -432.055] (0.0118) ({r_i: None, r_t: [-819.545 -819.545 -819.545], eps: 0.012})
Step:  153300, Reward: [-382.693 -382.693 -382.693] [69.846], Avg: [-416.625 -416.625 -416.625] (1.0000) ({r_i: None, r_t: [-780.928 -780.928 -780.928], eps: 1.0})
Step:   44400, Reward: [-386.529 -386.529 -386.529] [61.934], Avg: [-431.953 -431.953 -431.953] (0.0117) ({r_i: None, r_t: [-837.888 -837.888 -837.888], eps: 0.012})
Step:  153400, Reward: [-391.546 -391.546 -391.546] [93.472], Avg: [-416.608 -416.608 -416.608] (1.0000) ({r_i: None, r_t: [-747.182 -747.182 -747.182], eps: 1.0})
Step:   44500, Reward: [-402.474 -402.474 -402.474] [76.486], Avg: [-431.887 -431.887 -431.887] (0.0115) ({r_i: None, r_t: [-845.999 -845.999 -845.999], eps: 0.012})
Step:  153500, Reward: [-399.724 -399.724 -399.724] [72.744], Avg: [-416.597 -416.597 -416.597] (1.0000) ({r_i: None, r_t: [-807.697 -807.697 -807.697], eps: 1.0})
Step:   44600, Reward: [-430.159 -430.159 -430.159] [57.053], Avg: [-431.883 -431.883 -431.883] (0.0114) ({r_i: None, r_t: [-825.678 -825.678 -825.678], eps: 0.011})
Step:  153600, Reward: [-369.672 -369.672 -369.672] [80.854], Avg: [-416.567 -416.567 -416.567] (1.0000) ({r_i: None, r_t: [-740.412 -740.412 -740.412], eps: 1.0})
Step:   44700, Reward: [-404.360 -404.360 -404.360] [66.073], Avg: [-431.822 -431.822 -431.822] (0.0113) ({r_i: None, r_t: [-825.591 -825.591 -825.591], eps: 0.011})
Step:  153700, Reward: [-391.875 -391.875 -391.875] [88.998], Avg: [-416.551 -416.551 -416.551] (1.0000) ({r_i: None, r_t: [-727.979 -727.979 -727.979], eps: 1.0})
Step:   44800, Reward: [-405.477 -405.477 -405.477] [45.352], Avg: [-431.763 -431.763 -431.763] (0.0112) ({r_i: None, r_t: [-833.659 -833.659 -833.659], eps: 0.011})
Step:  153800, Reward: [-349.535 -349.535 -349.535] [52.267], Avg: [-416.507 -416.507 -416.507] (1.0000) ({r_i: None, r_t: [-715.072 -715.072 -715.072], eps: 1.0})
Step:   44900, Reward: [-435.471 -435.471 -435.471] [88.674], Avg: [-431.771 -431.771 -431.771] (0.0111) ({r_i: None, r_t: [-837.233 -837.233 -837.233], eps: 0.011})
Step:  153900, Reward: [-366.107 -366.107 -366.107] [45.226], Avg: [-416.475 -416.475 -416.475] (1.0000) ({r_i: None, r_t: [-691.190 -691.190 -691.190], eps: 1.0})
Step:   45000, Reward: [-407.741 -407.741 -407.741] [75.878], Avg: [-431.718 -431.718 -431.718] (0.0110) ({r_i: None, r_t: [-804.621 -804.621 -804.621], eps: 0.011})
Step:  154000, Reward: [-345.851 -345.851 -345.851] [63.455], Avg: [-416.429 -416.429 -416.429] (1.0000) ({r_i: None, r_t: [-734.293 -734.293 -734.293], eps: 1.0})
Step:   45100, Reward: [-397.116 -397.116 -397.116] [59.734], Avg: [-431.641 -431.641 -431.641] (0.0109) ({r_i: None, r_t: [-804.506 -804.506 -804.506], eps: 0.011})
Step:  154100, Reward: [-348.171 -348.171 -348.171] [38.689], Avg: [-416.384 -416.384 -416.384] (1.0000) ({r_i: None, r_t: [-724.520 -724.520 -724.520], eps: 1.0})
Step:   45200, Reward: [-397.198 -397.198 -397.198] [47.337], Avg: [-431.565 -431.565 -431.565] (0.0108) ({r_i: None, r_t: [-814.213 -814.213 -814.213], eps: 0.011})
Step:  154200, Reward: [-375.906 -375.906 -375.906] [83.239], Avg: [-416.358 -416.358 -416.358] (1.0000) ({r_i: None, r_t: [-762.139 -762.139 -762.139], eps: 1.0})
Step:   45300, Reward: [-428.069 -428.069 -428.069] [83.440], Avg: [-431.558 -431.558 -431.558] (0.0107) ({r_i: None, r_t: [-797.788 -797.788 -797.788], eps: 0.011})
Step:  154300, Reward: [-362.465 -362.465 -362.465] [68.943], Avg: [-416.323 -416.323 -416.323] (1.0000) ({r_i: None, r_t: [-740.687 -740.687 -740.687], eps: 1.0})
Step:   45400, Reward: [-417.704 -417.704 -417.704] [80.108], Avg: [-431.527 -431.527 -431.527] (0.0106) ({r_i: None, r_t: [-745.772 -745.772 -745.772], eps: 0.011})
Step:  154400, Reward: [-393.585 -393.585 -393.585] [64.366], Avg: [-416.309 -416.309 -416.309] (1.0000) ({r_i: None, r_t: [-738.063 -738.063 -738.063], eps: 1.0})
Step:   45500, Reward: [-446.208 -446.208 -446.208] [50.094], Avg: [-431.559 -431.559 -431.559] (0.0104) ({r_i: None, r_t: [-863.352 -863.352 -863.352], eps: 0.01})
Step:  154500, Reward: [-362.731 -362.731 -362.731] [54.688], Avg: [-416.274 -416.274 -416.274] (1.0000) ({r_i: None, r_t: [-726.656 -726.656 -726.656], eps: 1.0})
Step:   45600, Reward: [-405.698 -405.698 -405.698] [76.219], Avg: [-431.503 -431.503 -431.503] (0.0103) ({r_i: None, r_t: [-842.219 -842.219 -842.219], eps: 0.01})
Step:  154600, Reward: [-388.576 -388.576 -388.576] [53.175], Avg: [-416.256 -416.256 -416.256] (1.0000) ({r_i: None, r_t: [-742.109 -742.109 -742.109], eps: 1.0})
Step:   45700, Reward: [-419.948 -419.948 -419.948] [60.330], Avg: [-431.477 -431.477 -431.477] (0.0102) ({r_i: None, r_t: [-852.315 -852.315 -852.315], eps: 0.01})
Step:  154700, Reward: [-396.604 -396.604 -396.604] [61.583], Avg: [-416.243 -416.243 -416.243] (1.0000) ({r_i: None, r_t: [-722.496 -722.496 -722.496], eps: 1.0})
Step:   45800, Reward: [-401.957 -401.957 -401.957] [69.805], Avg: [-431.413 -431.413 -431.413] (0.0101) ({r_i: None, r_t: [-824.976 -824.976 -824.976], eps: 0.01})
Step:  154800, Reward: [-349.619 -349.619 -349.619] [60.631], Avg: [-416.200 -416.200 -416.200] (1.0000) ({r_i: None, r_t: [-748.038 -748.038 -748.038], eps: 1.0})
Step:   45900, Reward: [-408.612 -408.612 -408.612] [60.195], Avg: [-431.364 -431.364 -431.364] (0.0100) ({r_i: None, r_t: [-836.430 -836.430 -836.430], eps: 0.01})
Step:  154900, Reward: [-384.901 -384.901 -384.901] [93.338], Avg: [-416.180 -416.180 -416.180] (1.0000) ({r_i: None, r_t: [-738.735 -738.735 -738.735], eps: 1.0})
Step:   46000, Reward: [-443.124 -443.124 -443.124] [86.402], Avg: [-431.389 -431.389 -431.389] (0.0099) ({r_i: None, r_t: [-818.975 -818.975 -818.975], eps: 0.01})
Step:  155000, Reward: [-370.747 -370.747 -370.747] [55.020], Avg: [-416.151 -416.151 -416.151] (1.0000) ({r_i: None, r_t: [-750.351 -750.351 -750.351], eps: 1.0})
Step:   46100, Reward: [-430.359 -430.359 -430.359] [59.562], Avg: [-431.387 -431.387 -431.387] (0.0098) ({r_i: None, r_t: [-832.604 -832.604 -832.604], eps: 0.01})
Step:  155100, Reward: [-376.263 -376.263 -376.263] [49.069], Avg: [-416.125 -416.125 -416.125] (1.0000) ({r_i: None, r_t: [-716.633 -716.633 -716.633], eps: 1.0})
Step:   46200, Reward: [-397.320 -397.320 -397.320] [67.876], Avg: [-431.313 -431.313 -431.313] (0.0097) ({r_i: None, r_t: [-787.615 -787.615 -787.615], eps: 0.01})
Step:  155200, Reward: [-383.116 -383.116 -383.116] [56.860], Avg: [-416.104 -416.104 -416.104] (1.0000) ({r_i: None, r_t: [-743.980 -743.980 -743.980], eps: 1.0})
Step:   46300, Reward: [-408.379 -408.379 -408.379] [54.351], Avg: [-431.264 -431.264 -431.264] (0.0096) ({r_i: None, r_t: [-815.029 -815.029 -815.029], eps: 0.01})
Step:  155300, Reward: [-361.924 -361.924 -361.924] [48.154], Avg: [-416.069 -416.069 -416.069] (1.0000) ({r_i: None, r_t: [-776.600 -776.600 -776.600], eps: 1.0})
Step:   46400, Reward: [-413.124 -413.124 -413.124] [61.454], Avg: [-431.225 -431.225 -431.225] (0.0095) ({r_i: None, r_t: [-789.436 -789.436 -789.436], eps: 0.01})
Step:  155400, Reward: [-392.559 -392.559 -392.559] [76.281], Avg: [-416.054 -416.054 -416.054] (1.0000) ({r_i: None, r_t: [-679.875 -679.875 -679.875], eps: 1.0})
Step:   46500, Reward: [-442.601 -442.601 -442.601] [69.472], Avg: [-431.249 -431.249 -431.249] (0.0095) ({r_i: None, r_t: [-844.452 -844.452 -844.452], eps: 0.009})
Step:  155500, Reward: [-365.156 -365.156 -365.156] [63.548], Avg: [-416.021 -416.021 -416.021] (1.0000) ({r_i: None, r_t: [-711.317 -711.317 -711.317], eps: 1.0})
Step:   46600, Reward: [-400.357 -400.357 -400.357] [69.344], Avg: [-431.183 -431.183 -431.183] (0.0094) ({r_i: None, r_t: [-825.245 -825.245 -825.245], eps: 0.009})
Step:  155600, Reward: [-368.597 -368.597 -368.597] [48.857], Avg: [-415.991 -415.991 -415.991] (1.0000) ({r_i: None, r_t: [-716.301 -716.301 -716.301], eps: 1.0})
Step:   46700, Reward: [-376.938 -376.938 -376.938] [66.545], Avg: [-431.067 -431.067 -431.067] (0.0093) ({r_i: None, r_t: [-772.231 -772.231 -772.231], eps: 0.009})
Step:  155700, Reward: [-360.819 -360.819 -360.819] [58.386], Avg: [-415.955 -415.955 -415.955] (1.0000) ({r_i: None, r_t: [-762.039 -762.039 -762.039], eps: 1.0})
Step:   46800, Reward: [-458.986 -458.986 -458.986] [46.705], Avg: [-431.127 -431.127 -431.127] (0.0092) ({r_i: None, r_t: [-765.599 -765.599 -765.599], eps: 0.009})
Step:  155800, Reward: [-354.018 -354.018 -354.018] [64.763], Avg: [-415.916 -415.916 -415.916] (1.0000) ({r_i: None, r_t: [-748.035 -748.035 -748.035], eps: 1.0})
Step:   46900, Reward: [-429.386 -429.386 -429.386] [70.541], Avg: [-431.123 -431.123 -431.123] (0.0091) ({r_i: None, r_t: [-782.184 -782.184 -782.184], eps: 0.009})
Step:  155900, Reward: [-383.791 -383.791 -383.791] [65.713], Avg: [-415.895 -415.895 -415.895] (1.0000) ({r_i: None, r_t: [-729.279 -729.279 -729.279], eps: 1.0})
Step:   47000, Reward: [-396.421 -396.421 -396.421] [56.061], Avg: [-431.049 -431.049 -431.049] (0.0090) ({r_i: None, r_t: [-806.933 -806.933 -806.933], eps: 0.009})
Step:  156000, Reward: [-341.230 -341.230 -341.230] [57.825], Avg: [-415.847 -415.847 -415.847] (1.0000) ({r_i: None, r_t: [-741.296 -741.296 -741.296], eps: 1.0})
Step:   47100, Reward: [-416.858 -416.858 -416.858] [78.750], Avg: [-431.019 -431.019 -431.019] (0.0089) ({r_i: None, r_t: [-849.600 -849.600 -849.600], eps: 0.009})
Step:  156100, Reward: [-368.676 -368.676 -368.676] [71.389], Avg: [-415.817 -415.817 -415.817] (1.0000) ({r_i: None, r_t: [-757.014 -757.014 -757.014], eps: 1.0})
Step:   47200, Reward: [-421.666 -421.666 -421.666] [54.188], Avg: [-430.999 -430.999 -430.999] (0.0088) ({r_i: None, r_t: [-813.478 -813.478 -813.478], eps: 0.009})
Step:  156200, Reward: [-334.546 -334.546 -334.546] [63.734], Avg: [-415.765 -415.765 -415.765] (1.0000) ({r_i: None, r_t: [-739.313 -739.313 -739.313], eps: 1.0})
Step:   47300, Reward: [-414.943 -414.943 -414.943] [48.295], Avg: [-430.966 -430.966 -430.966] (0.0087) ({r_i: None, r_t: [-857.661 -857.661 -857.661], eps: 0.009})
Step:  156300, Reward: [-351.033 -351.033 -351.033] [66.415], Avg: [-415.724 -415.724 -415.724] (1.0000) ({r_i: None, r_t: [-750.955 -750.955 -750.955], eps: 1.0})
Step:   47400, Reward: [-426.203 -426.203 -426.203] [75.392], Avg: [-430.956 -430.956 -430.956] (0.0086) ({r_i: None, r_t: [-855.305 -855.305 -855.305], eps: 0.009})
Step:  156400, Reward: [-386.475 -386.475 -386.475] [50.061], Avg: [-415.705 -415.705 -415.705] (1.0000) ({r_i: None, r_t: [-750.131 -750.131 -750.131], eps: 1.0})
Step:   47500, Reward: [-444.653 -444.653 -444.653] [76.701], Avg: [-430.984 -430.984 -430.984] (0.0085) ({r_i: None, r_t: [-810.330 -810.330 -810.330], eps: 0.009})
Step:  156500, Reward: [-370.862 -370.862 -370.862] [89.556], Avg: [-415.676 -415.676 -415.676] (1.0000) ({r_i: None, r_t: [-801.568 -801.568 -801.568], eps: 1.0})
Step:   47600, Reward: [-397.986 -397.986 -397.986] [56.764], Avg: [-430.915 -430.915 -430.915] (0.0085) ({r_i: None, r_t: [-812.605 -812.605 -812.605], eps: 0.008})
Step:  156600, Reward: [-369.494 -369.494 -369.494] [53.422], Avg: [-415.647 -415.647 -415.647] (1.0000) ({r_i: None, r_t: [-749.616 -749.616 -749.616], eps: 1.0})
Step:   47700, Reward: [-419.434 -419.434 -419.434] [81.788], Avg: [-430.891 -430.891 -430.891] (0.0084) ({r_i: None, r_t: [-802.620 -802.620 -802.620], eps: 0.008})
Step:  156700, Reward: [-358.112 -358.112 -358.112] [63.198], Avg: [-415.610 -415.610 -415.610] (1.0000) ({r_i: None, r_t: [-679.267 -679.267 -679.267], eps: 1.0})
Step:   47800, Reward: [-395.767 -395.767 -395.767] [75.796], Avg: [-430.818 -430.818 -430.818] (0.0083) ({r_i: None, r_t: [-824.633 -824.633 -824.633], eps: 0.008})
Step:  156800, Reward: [-372.582 -372.582 -372.582] [83.197], Avg: [-415.583 -415.583 -415.583] (1.0000) ({r_i: None, r_t: [-770.685 -770.685 -770.685], eps: 1.0})
Step:   47900, Reward: [-423.273 -423.273 -423.273] [58.067], Avg: [-430.802 -430.802 -430.802] (0.0082) ({r_i: None, r_t: [-821.025 -821.025 -821.025], eps: 0.008})
Step:  156900, Reward: [-374.122 -374.122 -374.122] [77.401], Avg: [-415.556 -415.556 -415.556] (1.0000) ({r_i: None, r_t: [-781.926 -781.926 -781.926], eps: 1.0})
Step:   48000, Reward: [-414.856 -414.856 -414.856] [66.449], Avg: [-430.769 -430.769 -430.769] (0.0081) ({r_i: None, r_t: [-861.081 -861.081 -861.081], eps: 0.008})
Step:  157000, Reward: [-361.164 -361.164 -361.164] [69.641], Avg: [-415.522 -415.522 -415.522] (1.0000) ({r_i: None, r_t: [-697.163 -697.163 -697.163], eps: 1.0})
Step:   48100, Reward: [-414.263 -414.263 -414.263] [62.774], Avg: [-430.735 -430.735 -430.735] (0.0081) ({r_i: None, r_t: [-804.098 -804.098 -804.098], eps: 0.008})
Step:  157100, Reward: [-395.115 -395.115 -395.115] [66.634], Avg: [-415.509 -415.509 -415.509] (1.0000) ({r_i: None, r_t: [-757.672 -757.672 -757.672], eps: 1.0})
Step:   48200, Reward: [-392.284 -392.284 -392.284] [69.645], Avg: [-430.655 -430.655 -430.655] (0.0080) ({r_i: None, r_t: [-832.414 -832.414 -832.414], eps: 0.008})
Step:  157200, Reward: [-357.405 -357.405 -357.405] [39.683], Avg: [-415.472 -415.472 -415.472] (1.0000) ({r_i: None, r_t: [-783.109 -783.109 -783.109], eps: 1.0})
Step:   48300, Reward: [-420.724 -420.724 -420.724] [68.794], Avg: [-430.635 -430.635 -430.635] (0.0079) ({r_i: None, r_t: [-808.068 -808.068 -808.068], eps: 0.008})
Step:  157300, Reward: [-377.776 -377.776 -377.776] [104.359], Avg: [-415.448 -415.448 -415.448] (1.0000) ({r_i: None, r_t: [-726.667 -726.667 -726.667], eps: 1.0})
Step:   48400, Reward: [-405.295 -405.295 -405.295] [79.280], Avg: [-430.582 -430.582 -430.582] (0.0078) ({r_i: None, r_t: [-848.917 -848.917 -848.917], eps: 0.008})
Step:  157400, Reward: [-340.377 -340.377 -340.377] [86.401], Avg: [-415.400 -415.400 -415.400] (1.0000) ({r_i: None, r_t: [-730.954 -730.954 -730.954], eps: 1.0})
Step:   48500, Reward: [-423.984 -423.984 -423.984] [61.176], Avg: [-430.569 -430.569 -430.569] (0.0077) ({r_i: None, r_t: [-804.661 -804.661 -804.661], eps: 0.008})
Step:  157500, Reward: [-371.813 -371.813 -371.813] [79.750], Avg: [-415.372 -415.372 -415.372] (1.0000) ({r_i: None, r_t: [-717.688 -717.688 -717.688], eps: 1.0})
Step:   48600, Reward: [-414.839 -414.839 -414.839] [83.843], Avg: [-430.536 -430.536 -430.536] (0.0077) ({r_i: None, r_t: [-842.378 -842.378 -842.378], eps: 0.008})
Step:  157600, Reward: [-409.743 -409.743 -409.743] [65.206], Avg: [-415.369 -415.369 -415.369] (1.0000) ({r_i: None, r_t: [-731.110 -731.110 -731.110], eps: 1.0})
Step:   48700, Reward: [-427.748 -427.748 -427.748] [55.182], Avg: [-430.531 -430.531 -430.531] (0.0076) ({r_i: None, r_t: [-815.781 -815.781 -815.781], eps: 0.008})
Step:  157700, Reward: [-351.657 -351.657 -351.657] [58.496], Avg: [-415.329 -415.329 -415.329] (1.0000) ({r_i: None, r_t: [-732.472 -732.472 -732.472], eps: 1.0})
Step:   48800, Reward: [-395.829 -395.829 -395.829] [52.030], Avg: [-430.460 -430.460 -430.460] (0.0075) ({r_i: None, r_t: [-841.096 -841.096 -841.096], eps: 0.008})
Step:  157800, Reward: [-366.516 -366.516 -366.516] [72.303], Avg: [-415.298 -415.298 -415.298] (1.0000) ({r_i: None, r_t: [-701.339 -701.339 -701.339], eps: 1.0})
Step:   48900, Reward: [-434.020 -434.020 -434.020] [74.548], Avg: [-430.467 -430.467 -430.467] (0.0074) ({r_i: None, r_t: [-841.153 -841.153 -841.153], eps: 0.007})
Step:  157900, Reward: [-342.677 -342.677 -342.677] [51.977], Avg: [-415.252 -415.252 -415.252] (1.0000) ({r_i: None, r_t: [-729.703 -729.703 -729.703], eps: 1.0})
Step:   49000, Reward: [-406.148 -406.148 -406.148] [67.220], Avg: [-430.418 -430.418 -430.418] (0.0074) ({r_i: None, r_t: [-789.101 -789.101 -789.101], eps: 0.007})
Step:  158000, Reward: [-378.671 -378.671 -378.671] [63.615], Avg: [-415.228 -415.228 -415.228] (1.0000) ({r_i: None, r_t: [-757.682 -757.682 -757.682], eps: 1.0})
Step:   49100, Reward: [-458.896 -458.896 -458.896] [56.096], Avg: [-430.475 -430.475 -430.475] (0.0073) ({r_i: None, r_t: [-859.402 -859.402 -859.402], eps: 0.007})
Step:  158100, Reward: [-356.388 -356.388 -356.388] [59.864], Avg: [-415.191 -415.191 -415.191] (1.0000) ({r_i: None, r_t: [-739.023 -739.023 -739.023], eps: 1.0})
Step:   49200, Reward: [-386.003 -386.003 -386.003] [67.260], Avg: [-430.385 -430.385 -430.385] (0.0072) ({r_i: None, r_t: [-807.304 -807.304 -807.304], eps: 0.007})
Step:  158200, Reward: [-314.608 -314.608 -314.608] [36.683], Avg: [-415.128 -415.128 -415.128] (1.0000) ({r_i: None, r_t: [-711.264 -711.264 -711.264], eps: 1.0})
Step:   49300, Reward: [-393.347 -393.347 -393.347] [74.570], Avg: [-430.310 -430.310 -430.310] (0.0071) ({r_i: None, r_t: [-828.748 -828.748 -828.748], eps: 0.007})
Step:  158300, Reward: [-363.429 -363.429 -363.429] [55.212], Avg: [-415.095 -415.095 -415.095] (1.0000) ({r_i: None, r_t: [-713.435 -713.435 -713.435], eps: 1.0})
Step:   49400, Reward: [-402.926 -402.926 -402.926] [69.465], Avg: [-430.255 -430.255 -430.255] (0.0071) ({r_i: None, r_t: [-860.034 -860.034 -860.034], eps: 0.007})
Step:  158400, Reward: [-407.602 -407.602 -407.602] [60.663], Avg: [-415.090 -415.090 -415.090] (1.0000) ({r_i: None, r_t: [-800.097 -800.097 -800.097], eps: 1.0})
Step:   49500, Reward: [-423.418 -423.418 -423.418] [62.090], Avg: [-430.241 -430.241 -430.241] (0.0070) ({r_i: None, r_t: [-837.062 -837.062 -837.062], eps: 0.007})
Step:  158500, Reward: [-418.750 -418.750 -418.750] [75.242], Avg: [-415.093 -415.093 -415.093] (1.0000) ({r_i: None, r_t: [-672.326 -672.326 -672.326], eps: 1.0})
Step:   49600, Reward: [-432.826 -432.826 -432.826] [64.127], Avg: [-430.246 -430.246 -430.246] (0.0069) ({r_i: None, r_t: [-838.808 -838.808 -838.808], eps: 0.007})
Step:  158600, Reward: [-362.572 -362.572 -362.572] [42.895], Avg: [-415.060 -415.060 -415.060] (1.0000) ({r_i: None, r_t: [-679.317 -679.317 -679.317], eps: 1.0})
Step:   49700, Reward: [-413.101 -413.101 -413.101] [53.639], Avg: [-430.212 -430.212 -430.212] (0.0069) ({r_i: None, r_t: [-833.691 -833.691 -833.691], eps: 0.007})
Step:  158700, Reward: [-382.404 -382.404 -382.404] [60.938], Avg: [-415.039 -415.039 -415.039] (1.0000) ({r_i: None, r_t: [-728.098 -728.098 -728.098], eps: 1.0})
Step:   49800, Reward: [-409.198 -409.198 -409.198] [66.484], Avg: [-430.170 -430.170 -430.170] (0.0068) ({r_i: None, r_t: [-815.652 -815.652 -815.652], eps: 0.007})
Step:  158800, Reward: [-362.472 -362.472 -362.472] [59.986], Avg: [-415.006 -415.006 -415.006] (1.0000) ({r_i: None, r_t: [-763.308 -763.308 -763.308], eps: 1.0})
Step:   49900, Reward: [-426.512 -426.512 -426.512] [70.221], Avg: [-430.162 -430.162 -430.162] (0.0067) ({r_i: None, r_t: [-819.070 -819.070 -819.070], eps: 0.007})
Step:  158900, Reward: [-356.115 -356.115 -356.115] [53.961], Avg: [-414.969 -414.969 -414.969] (1.0000) ({r_i: None, r_t: [-733.559 -733.559 -733.559], eps: 1.0})
Step:  159000, Reward: [-369.445 -369.445 -369.445] [59.761], Avg: [-414.940 -414.940 -414.940] (1.0000) ({r_i: None, r_t: [-741.778 -741.778 -741.778], eps: 1.0})
Step:   50000, Reward: [-412.580 -412.580 -412.580] [49.734], Avg: [-430.127 -430.127 -430.127] (0.0067) ({r_i: None, r_t: [-813.599 -813.599 -813.599], eps: 0.007})
Step:  159100, Reward: [-355.158 -355.158 -355.158] [75.821], Avg: [-414.903 -414.903 -414.903] (1.0000) ({r_i: None, r_t: [-712.565 -712.565 -712.565], eps: 1.0})
Step:   50100, Reward: [-412.507 -412.507 -412.507] [65.667], Avg: [-430.092 -430.092 -430.092] (0.0066) ({r_i: None, r_t: [-845.495 -845.495 -845.495], eps: 0.007})
Step:  159200, Reward: [-353.480 -353.480 -353.480] [70.076], Avg: [-414.864 -414.864 -414.864] (1.0000) ({r_i: None, r_t: [-732.828 -732.828 -732.828], eps: 1.0})
Step:   50200, Reward: [-402.133 -402.133 -402.133] [74.811], Avg: [-430.037 -430.037 -430.037] (0.0065) ({r_i: None, r_t: [-821.828 -821.828 -821.828], eps: 0.007})
Step:  159300, Reward: [-368.257 -368.257 -368.257] [49.393], Avg: [-414.835 -414.835 -414.835] (1.0000) ({r_i: None, r_t: [-702.529 -702.529 -702.529], eps: 1.0})
Step:   50300, Reward: [-418.482 -418.482 -418.482] [67.485], Avg: [-430.014 -430.014 -430.014] (0.0065) ({r_i: None, r_t: [-839.084 -839.084 -839.084], eps: 0.006})
Step:  159400, Reward: [-344.834 -344.834 -344.834] [57.598], Avg: [-414.791 -414.791 -414.791] (1.0000) ({r_i: None, r_t: [-789.772 -789.772 -789.772], eps: 1.0})
Step:   50400, Reward: [-408.752 -408.752 -408.752] [50.460], Avg: [-429.972 -429.972 -429.972] (0.0064) ({r_i: None, r_t: [-845.314 -845.314 -845.314], eps: 0.006})
Step:  159500, Reward: [-371.794 -371.794 -371.794] [71.961], Avg: [-414.764 -414.764 -414.764] (1.0000) ({r_i: None, r_t: [-784.883 -784.883 -784.883], eps: 1.0})
Step:   50500, Reward: [-429.604 -429.604 -429.604] [69.026], Avg: [-429.971 -429.971 -429.971] (0.0063) ({r_i: None, r_t: [-892.125 -892.125 -892.125], eps: 0.006})
Step:  159600, Reward: [-370.146 -370.146 -370.146] [73.750], Avg: [-414.736 -414.736 -414.736] (1.0000) ({r_i: None, r_t: [-759.645 -759.645 -759.645], eps: 1.0})
Step:   50600, Reward: [-398.791 -398.791 -398.791] [51.729], Avg: [-429.909 -429.909 -429.909] (0.0063) ({r_i: None, r_t: [-833.229 -833.229 -833.229], eps: 0.006})
Step:  159700, Reward: [-395.076 -395.076 -395.076] [85.671], Avg: [-414.724 -414.724 -414.724] (1.0000) ({r_i: None, r_t: [-712.133 -712.133 -712.133], eps: 1.0})
Step:   50700, Reward: [-417.942 -417.942 -417.942] [64.498], Avg: [-429.886 -429.886 -429.886] (0.0062) ({r_i: None, r_t: [-827.535 -827.535 -827.535], eps: 0.006})
Step:  159800, Reward: [-363.845 -363.845 -363.845] [72.855], Avg: [-414.692 -414.692 -414.692] (1.0000) ({r_i: None, r_t: [-730.355 -730.355 -730.355], eps: 1.0})
Step:   50800, Reward: [-417.815 -417.815 -417.815] [52.847], Avg: [-429.862 -429.862 -429.862] (0.0061) ({r_i: None, r_t: [-807.326 -807.326 -807.326], eps: 0.006})
Step:  159900, Reward: [-369.712 -369.712 -369.712] [67.733], Avg: [-414.664 -414.664 -414.664] (1.0000) ({r_i: None, r_t: [-776.334 -776.334 -776.334], eps: 1.0})
Step:   50900, Reward: [-382.701 -382.701 -382.701] [49.435], Avg: [-429.770 -429.770 -429.770] (0.0061) ({r_i: None, r_t: [-829.984 -829.984 -829.984], eps: 0.006})
Step:  160000, Reward: [-379.483 -379.483 -379.483] [74.510], Avg: [-414.642 -414.642 -414.642] (1.0000) ({r_i: None, r_t: [-747.478 -747.478 -747.478], eps: 1.0})
Step:   51000, Reward: [-417.723 -417.723 -417.723] [47.217], Avg: [-429.746 -429.746 -429.746] (0.0060) ({r_i: None, r_t: [-786.607 -786.607 -786.607], eps: 0.006})
Step:  160100, Reward: [-342.518 -342.518 -342.518] [62.743], Avg: [-414.597 -414.597 -414.597] (1.0000) ({r_i: None, r_t: [-735.287 -735.287 -735.287], eps: 1.0})
Step:   51100, Reward: [-390.814 -390.814 -390.814] [66.496], Avg: [-429.670 -429.670 -429.670] (0.0060) ({r_i: None, r_t: [-819.727 -819.727 -819.727], eps: 0.006})
Step:  160200, Reward: [-378.656 -378.656 -378.656] [62.294], Avg: [-414.575 -414.575 -414.575] (1.0000) ({r_i: None, r_t: [-728.557 -728.557 -728.557], eps: 1.0})
Step:   51200, Reward: [-396.280 -396.280 -396.280] [59.404], Avg: [-429.605 -429.605 -429.605] (0.0059) ({r_i: None, r_t: [-866.077 -866.077 -866.077], eps: 0.006})
Step:  160300, Reward: [-349.039 -349.039 -349.039] [56.143], Avg: [-414.534 -414.534 -414.534] (1.0000) ({r_i: None, r_t: [-716.230 -716.230 -716.230], eps: 1.0})
Step:   51300, Reward: [-410.700 -410.700 -410.700] [64.524], Avg: [-429.568 -429.568 -429.568] (0.0058) ({r_i: None, r_t: [-833.773 -833.773 -833.773], eps: 0.006})
Step:  160400, Reward: [-370.330 -370.330 -370.330] [83.241], Avg: [-414.506 -414.506 -414.506] (1.0000) ({r_i: None, r_t: [-720.819 -720.819 -720.819], eps: 1.0})
Step:   51400, Reward: [-403.015 -403.015 -403.015] [76.908], Avg: [-429.517 -429.517 -429.517] (0.0058) ({r_i: None, r_t: [-781.054 -781.054 -781.054], eps: 0.006})
Step:  160500, Reward: [-351.068 -351.068 -351.068] [68.035], Avg: [-414.467 -414.467 -414.467] (1.0000) ({r_i: None, r_t: [-726.516 -726.516 -726.516], eps: 1.0})
Step:   51500, Reward: [-427.701 -427.701 -427.701] [56.491], Avg: [-429.513 -429.513 -429.513] (0.0057) ({r_i: None, r_t: [-841.150 -841.150 -841.150], eps: 0.006})
Step:  160600, Reward: [-387.131 -387.131 -387.131] [52.874], Avg: [-414.450 -414.450 -414.450] (1.0000) ({r_i: None, r_t: [-727.252 -727.252 -727.252], eps: 1.0})
Step:   51600, Reward: [-449.744 -449.744 -449.744] [65.051], Avg: [-429.552 -429.552 -429.552] (0.0057) ({r_i: None, r_t: [-830.024 -830.024 -830.024], eps: 0.006})
Step:  160700, Reward: [-347.839 -347.839 -347.839] [74.621], Avg: [-414.408 -414.408 -414.408] (1.0000) ({r_i: None, r_t: [-719.395 -719.395 -719.395], eps: 1.0})
Step:   51700, Reward: [-417.314 -417.314 -417.314] [60.878], Avg: [-429.529 -429.529 -429.529] (0.0056) ({r_i: None, r_t: [-847.229 -847.229 -847.229], eps: 0.006})
Step:  160800, Reward: [-388.732 -388.732 -388.732] [52.114], Avg: [-414.392 -414.392 -414.392] (1.0000) ({r_i: None, r_t: [-739.302 -739.302 -739.302], eps: 1.0})
Step:   51800, Reward: [-392.924 -392.924 -392.924] [69.526], Avg: [-429.458 -429.458 -429.458] (0.0056) ({r_i: None, r_t: [-800.670 -800.670 -800.670], eps: 0.006})
Step:  160900, Reward: [-380.454 -380.454 -380.454] [48.048], Avg: [-414.371 -414.371 -414.371] (1.0000) ({r_i: None, r_t: [-739.683 -739.683 -739.683], eps: 1.0})
Step:   51900, Reward: [-430.714 -430.714 -430.714] [53.890], Avg: [-429.461 -429.461 -429.461] (0.0055) ({r_i: None, r_t: [-823.673 -823.673 -823.673], eps: 0.005})
Step:  161000, Reward: [-370.538 -370.538 -370.538] [59.466], Avg: [-414.344 -414.344 -414.344] (1.0000) ({r_i: None, r_t: [-741.773 -741.773 -741.773], eps: 1.0})
Step:   52000, Reward: [-410.049 -410.049 -410.049] [72.970], Avg: [-429.423 -429.423 -429.423] (0.0054) ({r_i: None, r_t: [-824.908 -824.908 -824.908], eps: 0.005})
Step:  161100, Reward: [-384.226 -384.226 -384.226] [60.532], Avg: [-414.325 -414.325 -414.325] (1.0000) ({r_i: None, r_t: [-716.978 -716.978 -716.978], eps: 1.0})
Step:   52100, Reward: [-436.575 -436.575 -436.575] [53.600], Avg: [-429.437 -429.437 -429.437] (0.0054) ({r_i: None, r_t: [-799.276 -799.276 -799.276], eps: 0.005})
Step:  161200, Reward: [-367.788 -367.788 -367.788] [63.771], Avg: [-414.296 -414.296 -414.296] (1.0000) ({r_i: None, r_t: [-717.220 -717.220 -717.220], eps: 1.0})
Step:   52200, Reward: [-421.678 -421.678 -421.678] [67.024], Avg: [-429.422 -429.422 -429.422] (0.0053) ({r_i: None, r_t: [-824.972 -824.972 -824.972], eps: 0.005})
Step:  161300, Reward: [-350.488 -350.488 -350.488] [53.538], Avg: [-414.257 -414.257 -414.257] (1.0000) ({r_i: None, r_t: [-725.275 -725.275 -725.275], eps: 1.0})
Step:   52300, Reward: [-388.711 -388.711 -388.711] [50.730], Avg: [-429.344 -429.344 -429.344] (0.0053) ({r_i: None, r_t: [-833.462 -833.462 -833.462], eps: 0.005})
Step:  161400, Reward: [-396.225 -396.225 -396.225] [70.786], Avg: [-414.246 -414.246 -414.246] (1.0000) ({r_i: None, r_t: [-733.185 -733.185 -733.185], eps: 1.0})
Step:   52400, Reward: [-407.886 -407.886 -407.886] [68.300], Avg: [-429.304 -429.304 -429.304] (0.0052) ({r_i: None, r_t: [-878.329 -878.329 -878.329], eps: 0.005})
Step:  161500, Reward: [-384.970 -384.970 -384.970] [54.856], Avg: [-414.228 -414.228 -414.228] (1.0000) ({r_i: None, r_t: [-743.198 -743.198 -743.198], eps: 1.0})
Step:   52500, Reward: [-446.752 -446.752 -446.752] [76.264], Avg: [-429.337 -429.337 -429.337] (0.0052) ({r_i: None, r_t: [-824.658 -824.658 -824.658], eps: 0.005})
Step:  161600, Reward: [-371.183 -371.183 -371.183] [78.848], Avg: [-414.201 -414.201 -414.201] (1.0000) ({r_i: None, r_t: [-712.147 -712.147 -712.147], eps: 1.0})
Step:   52600, Reward: [-422.659 -422.659 -422.659] [55.106], Avg: [-429.324 -429.324 -429.324] (0.0051) ({r_i: None, r_t: [-828.735 -828.735 -828.735], eps: 0.005})
Step:  161700, Reward: [-353.575 -353.575 -353.575] [60.513], Avg: [-414.164 -414.164 -414.164] (1.0000) ({r_i: None, r_t: [-732.046 -732.046 -732.046], eps: 1.0})
Step:   52700, Reward: [-401.059 -401.059 -401.059] [77.320], Avg: [-429.271 -429.271 -429.271] (0.0051) ({r_i: None, r_t: [-865.989 -865.989 -865.989], eps: 0.005})
Step:  161800, Reward: [-348.724 -348.724 -348.724] [59.273], Avg: [-414.123 -414.123 -414.123] (1.0000) ({r_i: None, r_t: [-721.834 -721.834 -721.834], eps: 1.0})
Step:   52800, Reward: [-412.559 -412.559 -412.559] [58.301], Avg: [-429.239 -429.239 -429.239] (0.0050) ({r_i: None, r_t: [-857.213 -857.213 -857.213], eps: 0.005})
Step:  161900, Reward: [-359.137 -359.137 -359.137] [59.107], Avg: [-414.089 -414.089 -414.089] (1.0000) ({r_i: None, r_t: [-737.920 -737.920 -737.920], eps: 1.0})
Step:   52900, Reward: [-413.188 -413.188 -413.188] [87.385], Avg: [-429.209 -429.209 -429.209] (0.0050) ({r_i: None, r_t: [-820.804 -820.804 -820.804], eps: 0.005})
Step:  162000, Reward: [-333.215 -333.215 -333.215] [64.991], Avg: [-414.039 -414.039 -414.039] (1.0000) ({r_i: None, r_t: [-755.196 -755.196 -755.196], eps: 1.0})
Step:   53000, Reward: [-429.959 -429.959 -429.959] [60.717], Avg: [-429.210 -429.210 -429.210] (0.0049) ({r_i: None, r_t: [-837.260 -837.260 -837.260], eps: 0.005})
Step:  162100, Reward: [-390.371 -390.371 -390.371] [48.835], Avg: [-414.025 -414.025 -414.025] (1.0000) ({r_i: None, r_t: [-725.096 -725.096 -725.096], eps: 1.0})
Step:   53100, Reward: [-430.586 -430.586 -430.586] [59.702], Avg: [-429.213 -429.213 -429.213] (0.0049) ({r_i: None, r_t: [-866.417 -866.417 -866.417], eps: 0.005})
Step:  162200, Reward: [-377.723 -377.723 -377.723] [65.747], Avg: [-414.002 -414.002 -414.002] (1.0000) ({r_i: None, r_t: [-724.000 -724.000 -724.000], eps: 1.0})
Step:   53200, Reward: [-440.783 -440.783 -440.783] [55.426], Avg: [-429.234 -429.234 -429.234] (0.0048) ({r_i: None, r_t: [-860.972 -860.972 -860.972], eps: 0.005})
Step:  162300, Reward: [-382.317 -382.317 -382.317] [41.932], Avg: [-413.983 -413.983 -413.983] (1.0000) ({r_i: None, r_t: [-707.188 -707.188 -707.188], eps: 1.0})
Step:   53300, Reward: [-402.622 -402.622 -402.622] [68.756], Avg: [-429.185 -429.185 -429.185] (0.0048) ({r_i: None, r_t: [-862.872 -862.872 -862.872], eps: 0.005})
Step:  162400, Reward: [-362.017 -362.017 -362.017] [52.328], Avg: [-413.951 -413.951 -413.951] (1.0000) ({r_i: None, r_t: [-720.823 -720.823 -720.823], eps: 1.0})
Step:   53400, Reward: [-389.585 -389.585 -389.585] [54.424], Avg: [-429.110 -429.110 -429.110] (0.0047) ({r_i: None, r_t: [-855.268 -855.268 -855.268], eps: 0.005})
Step:  162500, Reward: [-355.323 -355.323 -355.323] [60.829], Avg: [-413.915 -413.915 -413.915] (1.0000) ({r_i: None, r_t: [-756.537 -756.537 -756.537], eps: 1.0})
Step:   53500, Reward: [-405.059 -405.059 -405.059] [74.160], Avg: [-429.066 -429.066 -429.066] (0.0047) ({r_i: None, r_t: [-870.560 -870.560 -870.560], eps: 0.005})
Step:  162600, Reward: [-378.598 -378.598 -378.598] [65.618], Avg: [-413.893 -413.893 -413.893] (1.0000) ({r_i: None, r_t: [-738.555 -738.555 -738.555], eps: 1.0})
Step:   53600, Reward: [-415.675 -415.675 -415.675] [58.787], Avg: [-429.041 -429.041 -429.041] (0.0046) ({r_i: None, r_t: [-806.562 -806.562 -806.562], eps: 0.005})
Step:  162700, Reward: [-391.779 -391.779 -391.779] [68.536], Avg: [-413.879 -413.879 -413.879] (1.0000) ({r_i: None, r_t: [-740.345 -740.345 -740.345], eps: 1.0})
Step:   53700, Reward: [-421.240 -421.240 -421.240] [77.700], Avg: [-429.026 -429.026 -429.026] (0.0046) ({r_i: None, r_t: [-843.285 -843.285 -843.285], eps: 0.005})
Step:  162800, Reward: [-375.091 -375.091 -375.091] [44.839], Avg: [-413.856 -413.856 -413.856] (1.0000) ({r_i: None, r_t: [-709.355 -709.355 -709.355], eps: 1.0})
Step:   53800, Reward: [-413.482 -413.482 -413.482] [89.529], Avg: [-428.997 -428.997 -428.997] (0.0045) ({r_i: None, r_t: [-848.976 -848.976 -848.976], eps: 0.005})
Step:  162900, Reward: [-356.627 -356.627 -356.627] [54.464], Avg: [-413.821 -413.821 -413.821] (1.0000) ({r_i: None, r_t: [-740.457 -740.457 -740.457], eps: 1.0})
Step:   53900, Reward: [-372.171 -372.171 -372.171] [40.892], Avg: [-428.892 -428.892 -428.892] (0.0045) ({r_i: None, r_t: [-793.953 -793.953 -793.953], eps: 0.005})
Step:  163000, Reward: [-341.264 -341.264 -341.264] [53.326], Avg: [-413.776 -413.776 -413.776] (1.0000) ({r_i: None, r_t: [-787.752 -787.752 -787.752], eps: 1.0})
Step:   54000, Reward: [-377.411 -377.411 -377.411] [65.438], Avg: [-428.797 -428.797 -428.797] (0.0045) ({r_i: None, r_t: [-819.156 -819.156 -819.156], eps: 0.004})
Step:  163100, Reward: [-379.756 -379.756 -379.756] [80.808], Avg: [-413.755 -413.755 -413.755] (1.0000) ({r_i: None, r_t: [-714.745 -714.745 -714.745], eps: 1.0})
Step:   54100, Reward: [-402.402 -402.402 -402.402] [43.013], Avg: [-428.748 -428.748 -428.748] (0.0044) ({r_i: None, r_t: [-826.371 -826.371 -826.371], eps: 0.004})
Step:  163200, Reward: [-356.521 -356.521 -356.521] [68.020], Avg: [-413.720 -413.720 -413.720] (1.0000) ({r_i: None, r_t: [-739.788 -739.788 -739.788], eps: 1.0})
Step:   54200, Reward: [-412.583 -412.583 -412.583] [74.933], Avg: [-428.718 -428.718 -428.718] (0.0044) ({r_i: None, r_t: [-847.275 -847.275 -847.275], eps: 0.004})
Step:  163300, Reward: [-365.240 -365.240 -365.240] [53.496], Avg: [-413.691 -413.691 -413.691] (1.0000) ({r_i: None, r_t: [-730.659 -730.659 -730.659], eps: 1.0})
Step:   54300, Reward: [-375.285 -375.285 -375.285] [52.415], Avg: [-428.620 -428.620 -428.620] (0.0043) ({r_i: None, r_t: [-835.829 -835.829 -835.829], eps: 0.004})
Step:  163400, Reward: [-367.757 -367.757 -367.757] [57.921], Avg: [-413.662 -413.662 -413.662] (1.0000) ({r_i: None, r_t: [-734.448 -734.448 -734.448], eps: 1.0})
Step:   54400, Reward: [-379.033 -379.033 -379.033] [58.591], Avg: [-428.529 -428.529 -428.529] (0.0043) ({r_i: None, r_t: [-798.807 -798.807 -798.807], eps: 0.004})
Step:  163500, Reward: [-376.689 -376.689 -376.689] [48.440], Avg: [-413.640 -413.640 -413.640] (1.0000) ({r_i: None, r_t: [-748.343 -748.343 -748.343], eps: 1.0})
Step:   54500, Reward: [-402.591 -402.591 -402.591] [41.112], Avg: [-428.482 -428.482 -428.482] (0.0042) ({r_i: None, r_t: [-807.260 -807.260 -807.260], eps: 0.004})
Step:  163600, Reward: [-360.875 -360.875 -360.875] [55.040], Avg: [-413.608 -413.608 -413.608] (1.0000) ({r_i: None, r_t: [-729.651 -729.651 -729.651], eps: 1.0})
Step:   54600, Reward: [-393.532 -393.532 -393.532] [64.862], Avg: [-428.418 -428.418 -428.418] (0.0042) ({r_i: None, r_t: [-817.491 -817.491 -817.491], eps: 0.004})
Step:  163700, Reward: [-373.986 -373.986 -373.986] [69.910], Avg: [-413.583 -413.583 -413.583] (1.0000) ({r_i: None, r_t: [-738.125 -738.125 -738.125], eps: 1.0})
Step:   54700, Reward: [-408.986 -408.986 -408.986] [61.581], Avg: [-428.382 -428.382 -428.382] (0.0042) ({r_i: None, r_t: [-800.619 -800.619 -800.619], eps: 0.004})
Step:  163800, Reward: [-373.676 -373.676 -373.676] [76.489], Avg: [-413.559 -413.559 -413.559] (1.0000) ({r_i: None, r_t: [-708.173 -708.173 -708.173], eps: 1.0})
Step:   54800, Reward: [-414.112 -414.112 -414.112] [35.668], Avg: [-428.356 -428.356 -428.356] (0.0041) ({r_i: None, r_t: [-826.225 -826.225 -826.225], eps: 0.004})
Step:  163900, Reward: [-349.511 -349.511 -349.511] [86.804], Avg: [-413.520 -413.520 -413.520] (1.0000) ({r_i: None, r_t: [-729.312 -729.312 -729.312], eps: 1.0})
Step:   54900, Reward: [-411.597 -411.597 -411.597] [43.276], Avg: [-428.326 -428.326 -428.326] (0.0041) ({r_i: None, r_t: [-814.244 -814.244 -814.244], eps: 0.004})
Step:  164000, Reward: [-364.212 -364.212 -364.212] [62.095], Avg: [-413.490 -413.490 -413.490] (1.0000) ({r_i: None, r_t: [-728.298 -728.298 -728.298], eps: 1.0})
Step:   55000, Reward: [-407.309 -407.309 -407.309] [46.340], Avg: [-428.288 -428.288 -428.288] (0.0040) ({r_i: None, r_t: [-807.473 -807.473 -807.473], eps: 0.004})
Step:  164100, Reward: [-365.733 -365.733 -365.733] [59.777], Avg: [-413.461 -413.461 -413.461] (1.0000) ({r_i: None, r_t: [-792.738 -792.738 -792.738], eps: 1.0})
Step:   55100, Reward: [-385.629 -385.629 -385.629] [64.992], Avg: [-428.211 -428.211 -428.211] (0.0040) ({r_i: None, r_t: [-794.754 -794.754 -794.754], eps: 0.004})
Step:  164200, Reward: [-369.185 -369.185 -369.185] [62.575], Avg: [-413.434 -413.434 -413.434] (1.0000) ({r_i: None, r_t: [-739.449 -739.449 -739.449], eps: 1.0})
Step:   55200, Reward: [-423.974 -423.974 -423.974] [70.784], Avg: [-428.203 -428.203 -428.203] (0.0040) ({r_i: None, r_t: [-818.105 -818.105 -818.105], eps: 0.004})
Step:  164300, Reward: [-356.855 -356.855 -356.855] [62.258], Avg: [-413.399 -413.399 -413.399] (1.0000) ({r_i: None, r_t: [-714.704 -714.704 -714.704], eps: 1.0})
Step:   55300, Reward: [-403.958 -403.958 -403.958] [57.155], Avg: [-428.159 -428.159 -428.159] (0.0039) ({r_i: None, r_t: [-784.461 -784.461 -784.461], eps: 0.004})
Step:  164400, Reward: [-360.065 -360.065 -360.065] [69.453], Avg: [-413.367 -413.367 -413.367] (1.0000) ({r_i: None, r_t: [-759.695 -759.695 -759.695], eps: 1.0})
Step:   55400, Reward: [-403.382 -403.382 -403.382] [61.785], Avg: [-428.114 -428.114 -428.114] (0.0039) ({r_i: None, r_t: [-780.316 -780.316 -780.316], eps: 0.004})
Step:  164500, Reward: [-366.879 -366.879 -366.879] [56.087], Avg: [-413.339 -413.339 -413.339] (1.0000) ({r_i: None, r_t: [-749.795 -749.795 -749.795], eps: 1.0})
Step:   55500, Reward: [-399.511 -399.511 -399.511] [72.651], Avg: [-428.063 -428.063 -428.063] (0.0038) ({r_i: None, r_t: [-806.808 -806.808 -806.808], eps: 0.004})
Step:  164600, Reward: [-385.610 -385.610 -385.610] [48.174], Avg: [-413.322 -413.322 -413.322] (1.0000) ({r_i: None, r_t: [-708.498 -708.498 -708.498], eps: 1.0})
Step:   55600, Reward: [-402.958 -402.958 -402.958] [55.940], Avg: [-428.018 -428.018 -428.018] (0.0038) ({r_i: None, r_t: [-755.900 -755.900 -755.900], eps: 0.004})
Step:  164700, Reward: [-397.486 -397.486 -397.486] [72.787], Avg: [-413.312 -413.312 -413.312] (1.0000) ({r_i: None, r_t: [-721.803 -721.803 -721.803], eps: 1.0})
Step:   55700, Reward: [-421.874 -421.874 -421.874] [75.514], Avg: [-428.007 -428.007 -428.007] (0.0038) ({r_i: None, r_t: [-814.478 -814.478 -814.478], eps: 0.004})
Step:  164800, Reward: [-402.536 -402.536 -402.536] [89.268], Avg: [-413.306 -413.306 -413.306] (1.0000) ({r_i: None, r_t: [-697.839 -697.839 -697.839], eps: 1.0})
Step:   55800, Reward: [-391.247 -391.247 -391.247] [77.972], Avg: [-427.941 -427.941 -427.941] (0.0037) ({r_i: None, r_t: [-779.487 -779.487 -779.487], eps: 0.004})
Step:  164900, Reward: [-359.604 -359.604 -359.604] [69.835], Avg: [-413.273 -413.273 -413.273] (1.0000) ({r_i: None, r_t: [-768.907 -768.907 -768.907], eps: 1.0})
Step:  165000, Reward: [-382.713 -382.713 -382.713] [70.088], Avg: [-413.255 -413.255 -413.255] (1.0000) ({r_i: None, r_t: [-727.809 -727.809 -727.809], eps: 1.0})
Step:   55900, Reward: [-377.288 -377.288 -377.288] [78.273], Avg: [-427.851 -427.851 -427.851] (0.0037) ({r_i: None, r_t: [-775.310 -775.310 -775.310], eps: 0.004})
Step:  165100, Reward: [-371.422 -371.422 -371.422] [54.153], Avg: [-413.229 -413.229 -413.229] (1.0000) ({r_i: None, r_t: [-713.021 -713.021 -713.021], eps: 1.0})
Step:   56000, Reward: [-390.728 -390.728 -390.728] [84.240], Avg: [-427.785 -427.785 -427.785] (0.0036) ({r_i: None, r_t: [-836.067 -836.067 -836.067], eps: 0.004})
Step:  165200, Reward: [-398.125 -398.125 -398.125] [65.828], Avg: [-413.220 -413.220 -413.220] (1.0000) ({r_i: None, r_t: [-736.358 -736.358 -736.358], eps: 1.0})
Step:   56100, Reward: [-397.400 -397.400 -397.400] [61.522], Avg: [-427.730 -427.730 -427.730] (0.0036) ({r_i: None, r_t: [-807.128 -807.128 -807.128], eps: 0.004})
Step:  165300, Reward: [-377.995 -377.995 -377.995] [61.881], Avg: [-413.199 -413.199 -413.199] (1.0000) ({r_i: None, r_t: [-756.400 -756.400 -756.400], eps: 1.0})
Step:   56200, Reward: [-404.199 -404.199 -404.199] [74.445], Avg: [-427.689 -427.689 -427.689] (0.0036) ({r_i: None, r_t: [-769.193 -769.193 -769.193], eps: 0.004})
Step:  165400, Reward: [-365.439 -365.439 -365.439] [59.647], Avg: [-413.170 -413.170 -413.170] (1.0000) ({r_i: None, r_t: [-694.884 -694.884 -694.884], eps: 1.0})
Step:   56300, Reward: [-424.346 -424.346 -424.346] [63.338], Avg: [-427.683 -427.683 -427.683] (0.0035) ({r_i: None, r_t: [-799.776 -799.776 -799.776], eps: 0.004})
Step:  165500, Reward: [-337.885 -337.885 -337.885] [52.710], Avg: [-413.125 -413.125 -413.125] (1.0000) ({r_i: None, r_t: [-736.248 -736.248 -736.248], eps: 1.0})
Step:   56400, Reward: [-379.683 -379.683 -379.683] [80.306], Avg: [-427.598 -427.598 -427.598] (0.0035) ({r_i: None, r_t: [-791.001 -791.001 -791.001], eps: 0.004})
Step:  165600, Reward: [-382.614 -382.614 -382.614] [58.630], Avg: [-413.106 -413.106 -413.106] (1.0000) ({r_i: None, r_t: [-743.780 -743.780 -743.780], eps: 1.0})
Step:   56500, Reward: [-399.238 -399.238 -399.238] [73.078], Avg: [-427.548 -427.548 -427.548] (0.0035) ({r_i: None, r_t: [-848.137 -848.137 -848.137], eps: 0.003})
Step:  165700, Reward: [-349.736 -349.736 -349.736] [35.862], Avg: [-413.068 -413.068 -413.068] (1.0000) ({r_i: None, r_t: [-741.862 -741.862 -741.862], eps: 1.0})
Step:   56600, Reward: [-377.648 -377.648 -377.648] [73.733], Avg: [-427.460 -427.460 -427.460] (0.0034) ({r_i: None, r_t: [-759.446 -759.446 -759.446], eps: 0.003})
Step:  165800, Reward: [-372.431 -372.431 -372.431] [49.017], Avg: [-413.044 -413.044 -413.044] (1.0000) ({r_i: None, r_t: [-756.289 -756.289 -756.289], eps: 1.0})
Step:   56700, Reward: [-392.955 -392.955 -392.955] [73.242], Avg: [-427.399 -427.399 -427.399] (0.0034) ({r_i: None, r_t: [-790.434 -790.434 -790.434], eps: 0.003})
Step:  165900, Reward: [-373.011 -373.011 -373.011] [55.328], Avg: [-413.019 -413.019 -413.019] (1.0000) ({r_i: None, r_t: [-764.648 -764.648 -764.648], eps: 1.0})
Step:   56800, Reward: [-394.001 -394.001 -394.001] [85.277], Avg: [-427.340 -427.340 -427.340] (0.0034) ({r_i: None, r_t: [-768.531 -768.531 -768.531], eps: 0.003})
Step:  166000, Reward: [-348.058 -348.058 -348.058] [49.361], Avg: [-412.980 -412.980 -412.980] (1.0000) ({r_i: None, r_t: [-750.161 -750.161 -750.161], eps: 1.0})
Step:   56900, Reward: [-372.655 -372.655 -372.655] [59.336], Avg: [-427.244 -427.244 -427.244] (0.0033) ({r_i: None, r_t: [-759.619 -759.619 -759.619], eps: 0.003})
Step:  166100, Reward: [-382.175 -382.175 -382.175] [38.431], Avg: [-412.962 -412.962 -412.962] (1.0000) ({r_i: None, r_t: [-721.617 -721.617 -721.617], eps: 1.0})
Step:   57000, Reward: [-403.174 -403.174 -403.174] [65.461], Avg: [-427.202 -427.202 -427.202] (0.0033) ({r_i: None, r_t: [-782.062 -782.062 -782.062], eps: 0.003})
Step:  166200, Reward: [-355.836 -355.836 -355.836] [81.883], Avg: [-412.927 -412.927 -412.927] (1.0000) ({r_i: None, r_t: [-710.287 -710.287 -710.287], eps: 1.0})
Step:   57100, Reward: [-382.960 -382.960 -382.960] [80.554], Avg: [-427.125 -427.125 -427.125] (0.0033) ({r_i: None, r_t: [-795.090 -795.090 -795.090], eps: 0.003})
Step:  166300, Reward: [-347.260 -347.260 -347.260] [38.378], Avg: [-412.888 -412.888 -412.888] (1.0000) ({r_i: None, r_t: [-737.747 -737.747 -737.747], eps: 1.0})
Step:   57200, Reward: [-382.861 -382.861 -382.861] [58.524], Avg: [-427.048 -427.048 -427.048] (0.0032) ({r_i: None, r_t: [-795.902 -795.902 -795.902], eps: 0.003})
Step:  166400, Reward: [-380.237 -380.237 -380.237] [75.773], Avg: [-412.868 -412.868 -412.868] (1.0000) ({r_i: None, r_t: [-785.719 -785.719 -785.719], eps: 1.0})
Step:   57300, Reward: [-398.219 -398.219 -398.219] [84.533], Avg: [-426.997 -426.997 -426.997] (0.0032) ({r_i: None, r_t: [-755.697 -755.697 -755.697], eps: 0.003})
Step:  166500, Reward: [-401.737 -401.737 -401.737] [64.095], Avg: [-412.862 -412.862 -412.862] (1.0000) ({r_i: None, r_t: [-747.436 -747.436 -747.436], eps: 1.0})
Step:   57400, Reward: [-385.445 -385.445 -385.445] [57.852], Avg: [-426.925 -426.925 -426.925] (0.0032) ({r_i: None, r_t: [-789.955 -789.955 -789.955], eps: 0.003})
Step:  166600, Reward: [-358.987 -358.987 -358.987] [44.929], Avg: [-412.829 -412.829 -412.829] (1.0000) ({r_i: None, r_t: [-715.184 -715.184 -715.184], eps: 1.0})
Step:   57500, Reward: [-385.774 -385.774 -385.774] [80.684], Avg: [-426.854 -426.854 -426.854] (0.0031) ({r_i: None, r_t: [-813.750 -813.750 -813.750], eps: 0.003})
Step:  166700, Reward: [-398.720 -398.720 -398.720] [59.794], Avg: [-412.821 -412.821 -412.821] (1.0000) ({r_i: None, r_t: [-751.777 -751.777 -751.777], eps: 1.0})
Step:   57600, Reward: [-406.819 -406.819 -406.819] [64.635], Avg: [-426.819 -426.819 -426.819] (0.0031) ({r_i: None, r_t: [-744.380 -744.380 -744.380], eps: 0.003})
Step:  166800, Reward: [-366.412 -366.412 -366.412] [57.325], Avg: [-412.793 -412.793 -412.793] (1.0000) ({r_i: None, r_t: [-784.698 -784.698 -784.698], eps: 1.0})
Step:   57700, Reward: [-412.480 -412.480 -412.480] [70.043], Avg: [-426.794 -426.794 -426.794] (0.0031) ({r_i: None, r_t: [-786.679 -786.679 -786.679], eps: 0.003})
Step:  166900, Reward: [-387.566 -387.566 -387.566] [34.076], Avg: [-412.778 -412.778 -412.778] (1.0000) ({r_i: None, r_t: [-761.097 -761.097 -761.097], eps: 1.0})
Step:   57800, Reward: [-373.539 -373.539 -373.539] [59.236], Avg: [-426.702 -426.702 -426.702] (0.0030) ({r_i: None, r_t: [-774.775 -774.775 -774.775], eps: 0.003})
Step:  167000, Reward: [-366.662 -366.662 -366.662] [60.116], Avg: [-412.750 -412.750 -412.750] (1.0000) ({r_i: None, r_t: [-733.290 -733.290 -733.290], eps: 1.0})
Step:   57900, Reward: [-351.692 -351.692 -351.692] [49.086], Avg: [-426.573 -426.573 -426.573] (0.0030) ({r_i: None, r_t: [-744.971 -744.971 -744.971], eps: 0.003})
Step:  167100, Reward: [-388.127 -388.127 -388.127] [61.044], Avg: [-412.736 -412.736 -412.736] (1.0000) ({r_i: None, r_t: [-715.160 -715.160 -715.160], eps: 1.0})
Step:   58000, Reward: [-361.641 -361.641 -361.641] [54.781], Avg: [-426.461 -426.461 -426.461] (0.0030) ({r_i: None, r_t: [-747.536 -747.536 -747.536], eps: 0.003})
Step:  167200, Reward: [-363.504 -363.504 -363.504] [72.022], Avg: [-412.706 -412.706 -412.706] (1.0000) ({r_i: None, r_t: [-737.287 -737.287 -737.287], eps: 1.0})
Step:   58100, Reward: [-379.343 -379.343 -379.343] [56.991], Avg: [-426.380 -426.380 -426.380] (0.0030) ({r_i: None, r_t: [-763.006 -763.006 -763.006], eps: 0.003})
Step:  167300, Reward: [-380.424 -380.424 -380.424] [81.759], Avg: [-412.687 -412.687 -412.687] (1.0000) ({r_i: None, r_t: [-723.703 -723.703 -723.703], eps: 1.0})
Step:   58200, Reward: [-384.312 -384.312 -384.312] [71.270], Avg: [-426.308 -426.308 -426.308] (0.0029) ({r_i: None, r_t: [-790.442 -790.442 -790.442], eps: 0.003})
Step:  167400, Reward: [-373.965 -373.965 -373.965] [50.552], Avg: [-412.664 -412.664 -412.664] (1.0000) ({r_i: None, r_t: [-775.544 -775.544 -775.544], eps: 1.0})
Step:   58300, Reward: [-416.765 -416.765 -416.765] [66.772], Avg: [-426.292 -426.292 -426.292] (0.0029) ({r_i: None, r_t: [-723.069 -723.069 -723.069], eps: 0.003})
Step:  167500, Reward: [-375.998 -375.998 -375.998] [52.808], Avg: [-412.642 -412.642 -412.642] (1.0000) ({r_i: None, r_t: [-744.811 -744.811 -744.811], eps: 1.0})
Step:   58400, Reward: [-419.512 -419.512 -419.512] [70.160], Avg: [-426.280 -426.280 -426.280] (0.0029) ({r_i: None, r_t: [-745.340 -745.340 -745.340], eps: 0.003})
Step:  167600, Reward: [-365.256 -365.256 -365.256] [61.745], Avg: [-412.614 -412.614 -412.614] (1.0000) ({r_i: None, r_t: [-751.300 -751.300 -751.300], eps: 1.0})
Step:   58500, Reward: [-362.611 -362.611 -362.611] [65.135], Avg: [-426.171 -426.171 -426.171] (0.0028) ({r_i: None, r_t: [-749.026 -749.026 -749.026], eps: 0.003})
Step:  167700, Reward: [-389.375 -389.375 -389.375] [74.005], Avg: [-412.600 -412.600 -412.600] (1.0000) ({r_i: None, r_t: [-757.136 -757.136 -757.136], eps: 1.0})
Step:   58600, Reward: [-357.353 -357.353 -357.353] [59.524], Avg: [-426.054 -426.054 -426.054] (0.0028) ({r_i: None, r_t: [-756.085 -756.085 -756.085], eps: 0.003})
Step:  167800, Reward: [-356.724 -356.724 -356.724] [62.885], Avg: [-412.567 -412.567 -412.567] (1.0000) ({r_i: None, r_t: [-752.260 -752.260 -752.260], eps: 1.0})
Step:   58700, Reward: [-394.374 -394.374 -394.374] [67.745], Avg: [-426.000 -426.000 -426.000] (0.0028) ({r_i: None, r_t: [-768.814 -768.814 -768.814], eps: 0.003})
Step:  167900, Reward: [-367.211 -367.211 -367.211] [65.977], Avg: [-412.540 -412.540 -412.540] (1.0000) ({r_i: None, r_t: [-719.133 -719.133 -719.133], eps: 1.0})
Step:   58800, Reward: [-388.808 -388.808 -388.808] [66.512], Avg: [-425.937 -425.937 -425.937] (0.0028) ({r_i: None, r_t: [-790.312 -790.312 -790.312], eps: 0.003})
Step:  168000, Reward: [-362.766 -362.766 -362.766] [66.994], Avg: [-412.510 -412.510 -412.510] (1.0000) ({r_i: None, r_t: [-778.811 -778.811 -778.811], eps: 1.0})
Step:   58900, Reward: [-378.893 -378.893 -378.893] [81.796], Avg: [-425.857 -425.857 -425.857] (0.0027) ({r_i: None, r_t: [-786.780 -786.780 -786.780], eps: 0.003})
Step:  168100, Reward: [-359.076 -359.076 -359.076] [58.979], Avg: [-412.478 -412.478 -412.478] (1.0000) ({r_i: None, r_t: [-760.195 -760.195 -760.195], eps: 1.0})
Step:   59000, Reward: [-352.996 -352.996 -352.996] [51.998], Avg: [-425.734 -425.734 -425.734] (0.0027) ({r_i: None, r_t: [-781.698 -781.698 -781.698], eps: 0.003})
Step:  168200, Reward: [-353.535 -353.535 -353.535] [51.281], Avg: [-412.443 -412.443 -412.443] (1.0000) ({r_i: None, r_t: [-792.270 -792.270 -792.270], eps: 1.0})
Step:   59100, Reward: [-383.195 -383.195 -383.195] [49.496], Avg: [-425.662 -425.662 -425.662] (0.0027) ({r_i: None, r_t: [-736.770 -736.770 -736.770], eps: 0.003})
Step:  168300, Reward: [-403.738 -403.738 -403.738] [69.027], Avg: [-412.438 -412.438 -412.438] (1.0000) ({r_i: None, r_t: [-751.325 -751.325 -751.325], eps: 1.0})
Step:   59200, Reward: [-359.052 -359.052 -359.052] [67.061], Avg: [-425.550 -425.550 -425.550] (0.0026) ({r_i: None, r_t: [-827.540 -827.540 -827.540], eps: 0.003})
Step:  168400, Reward: [-338.398 -338.398 -338.398] [69.336], Avg: [-412.394 -412.394 -412.394] (1.0000) ({r_i: None, r_t: [-757.473 -757.473 -757.473], eps: 1.0})
Step:   59300, Reward: [-351.672 -351.672 -351.672] [73.213], Avg: [-425.426 -425.426 -425.426] (0.0026) ({r_i: None, r_t: [-778.347 -778.347 -778.347], eps: 0.003})
Step:  168500, Reward: [-368.990 -368.990 -368.990] [51.944], Avg: [-412.368 -412.368 -412.368] (1.0000) ({r_i: None, r_t: [-729.036 -729.036 -729.036], eps: 1.0})
Step:   59400, Reward: [-393.616 -393.616 -393.616] [70.970], Avg: [-425.372 -425.372 -425.372] (0.0026) ({r_i: None, r_t: [-790.761 -790.761 -790.761], eps: 0.003})
Step:  168600, Reward: [-368.044 -368.044 -368.044] [46.883], Avg: [-412.342 -412.342 -412.342] (1.0000) ({r_i: None, r_t: [-749.449 -749.449 -749.449], eps: 1.0})
Step:   59500, Reward: [-368.626 -368.626 -368.626] [74.115], Avg: [-425.277 -425.277 -425.277] (0.0026) ({r_i: None, r_t: [-767.798 -767.798 -767.798], eps: 0.003})
Step:  168700, Reward: [-390.872 -390.872 -390.872] [56.708], Avg: [-412.329 -412.329 -412.329] (1.0000) ({r_i: None, r_t: [-736.269 -736.269 -736.269], eps: 1.0})
Step:   59600, Reward: [-389.661 -389.661 -389.661] [72.461], Avg: [-425.217 -425.217 -425.217] (0.0025) ({r_i: None, r_t: [-771.834 -771.834 -771.834], eps: 0.003})
Step:  168800, Reward: [-391.078 -391.078 -391.078] [73.856], Avg: [-412.317 -412.317 -412.317] (1.0000) ({r_i: None, r_t: [-745.080 -745.080 -745.080], eps: 1.0})
Step:   59700, Reward: [-391.217 -391.217 -391.217] [80.620], Avg: [-425.160 -425.160 -425.160] (0.0025) ({r_i: None, r_t: [-762.629 -762.629 -762.629], eps: 0.003})
Step:  168900, Reward: [-380.876 -380.876 -380.876] [62.740], Avg: [-412.298 -412.298 -412.298] (1.0000) ({r_i: None, r_t: [-758.354 -758.354 -758.354], eps: 1.0})
Step:   59800, Reward: [-397.386 -397.386 -397.386] [61.348], Avg: [-425.114 -425.114 -425.114] (0.0025) ({r_i: None, r_t: [-758.248 -758.248 -758.248], eps: 0.002})
Step:  169000, Reward: [-372.269 -372.269 -372.269] [55.507], Avg: [-412.275 -412.275 -412.275] (1.0000) ({r_i: None, r_t: [-811.756 -811.756 -811.756], eps: 1.0})
Step:   59900, Reward: [-382.715 -382.715 -382.715] [69.444], Avg: [-425.043 -425.043 -425.043] (0.0025) ({r_i: None, r_t: [-789.284 -789.284 -789.284], eps: 0.002})
Step:  169100, Reward: [-378.200 -378.200 -378.200] [44.864], Avg: [-412.254 -412.254 -412.254] (1.0000) ({r_i: None, r_t: [-758.724 -758.724 -758.724], eps: 1.0})
Step:   60000, Reward: [-393.841 -393.841 -393.841] [65.911], Avg: [-424.991 -424.991 -424.991] (0.0024) ({r_i: None, r_t: [-803.317 -803.317 -803.317], eps: 0.002})
Step:  169200, Reward: [-360.829 -360.829 -360.829] [68.135], Avg: [-412.224 -412.224 -412.224] (1.0000) ({r_i: None, r_t: [-745.053 -745.053 -745.053], eps: 1.0})
Step:   60100, Reward: [-386.467 -386.467 -386.467] [74.383], Avg: [-424.927 -424.927 -424.927] (0.0024) ({r_i: None, r_t: [-722.891 -722.891 -722.891], eps: 0.002})
Step:  169300, Reward: [-377.868 -377.868 -377.868] [44.391], Avg: [-412.204 -412.204 -412.204] (1.0000) ({r_i: None, r_t: [-760.841 -760.841 -760.841], eps: 1.0})
Step:   60200, Reward: [-375.034 -375.034 -375.034] [80.080], Avg: [-424.845 -424.845 -424.845] (0.0024) ({r_i: None, r_t: [-793.021 -793.021 -793.021], eps: 0.002})
Step:  169400, Reward: [-381.792 -381.792 -381.792] [73.351], Avg: [-412.186 -412.186 -412.186] (1.0000) ({r_i: None, r_t: [-772.023 -772.023 -772.023], eps: 1.0})
Step:   60300, Reward: [-372.935 -372.935 -372.935] [88.183], Avg: [-424.759 -424.759 -424.759] (0.0024) ({r_i: None, r_t: [-774.730 -774.730 -774.730], eps: 0.002})
Step:  169500, Reward: [-391.733 -391.733 -391.733] [81.155], Avg: [-412.174 -412.174 -412.174] (1.0000) ({r_i: None, r_t: [-763.834 -763.834 -763.834], eps: 1.0})
Step:   60400, Reward: [-397.303 -397.303 -397.303] [80.572], Avg: [-424.713 -424.713 -424.713] (0.0023) ({r_i: None, r_t: [-829.433 -829.433 -829.433], eps: 0.002})
Step:  169600, Reward: [-348.646 -348.646 -348.646] [35.679], Avg: [-412.136 -412.136 -412.136] (1.0000) ({r_i: None, r_t: [-735.700 -735.700 -735.700], eps: 1.0})
Step:   60500, Reward: [-402.851 -402.851 -402.851] [54.508], Avg: [-424.677 -424.677 -424.677] (0.0023) ({r_i: None, r_t: [-810.720 -810.720 -810.720], eps: 0.002})
Step:  169700, Reward: [-376.436 -376.436 -376.436] [49.556], Avg: [-412.115 -412.115 -412.115] (1.0000) ({r_i: None, r_t: [-783.162 -783.162 -783.162], eps: 1.0})
Step:   60600, Reward: [-388.815 -388.815 -388.815] [72.186], Avg: [-424.618 -424.618 -424.618] (0.0023) ({r_i: None, r_t: [-784.669 -784.669 -784.669], eps: 0.002})
Step:  169800, Reward: [-398.599 -398.599 -398.599] [54.654], Avg: [-412.107 -412.107 -412.107] (1.0000) ({r_i: None, r_t: [-728.348 -728.348 -728.348], eps: 1.0})
Step:   60700, Reward: [-422.967 -422.967 -422.967] [81.192], Avg: [-424.615 -424.615 -424.615] (0.0023) ({r_i: None, r_t: [-781.822 -781.822 -781.822], eps: 0.002})
Step:  169900, Reward: [-396.875 -396.875 -396.875] [60.232], Avg: [-412.098 -412.098 -412.098] (1.0000) ({r_i: None, r_t: [-734.237 -734.237 -734.237], eps: 1.0})
Step:   60800, Reward: [-378.959 -378.959 -378.959] [75.348], Avg: [-424.540 -424.540 -424.540] (0.0023) ({r_i: None, r_t: [-802.371 -802.371 -802.371], eps: 0.002})
Step:  170000, Reward: [-375.410 -375.410 -375.410] [53.965], Avg: [-412.077 -412.077 -412.077] (1.0000) ({r_i: None, r_t: [-740.684 -740.684 -740.684], eps: 1.0})
Step:   60900, Reward: [-355.291 -355.291 -355.291] [71.412], Avg: [-424.427 -424.427 -424.427] (0.0022) ({r_i: None, r_t: [-760.561 -760.561 -760.561], eps: 0.002})
Step:  170100, Reward: [-377.361 -377.361 -377.361] [58.203], Avg: [-412.056 -412.056 -412.056] (1.0000) ({r_i: None, r_t: [-723.992 -723.992 -723.992], eps: 1.0})
Step:   61000, Reward: [-383.156 -383.156 -383.156] [70.752], Avg: [-424.359 -424.359 -424.359] (0.0022) ({r_i: None, r_t: [-802.809 -802.809 -802.809], eps: 0.002})
Step:  170200, Reward: [-420.712 -420.712 -420.712] [83.819], Avg: [-412.061 -412.061 -412.061] (1.0000) ({r_i: None, r_t: [-765.124 -765.124 -765.124], eps: 1.0})
Step:   61100, Reward: [-375.210 -375.210 -375.210] [57.532], Avg: [-424.279 -424.279 -424.279] (0.0022) ({r_i: None, r_t: [-767.301 -767.301 -767.301], eps: 0.002})
Step:  170300, Reward: [-373.173 -373.173 -373.173] [73.352], Avg: [-412.039 -412.039 -412.039] (1.0000) ({r_i: None, r_t: [-756.782 -756.782 -756.782], eps: 1.0})
Step:   61200, Reward: [-378.443 -378.443 -378.443] [54.547], Avg: [-424.204 -424.204 -424.204] (0.0022) ({r_i: None, r_t: [-797.006 -797.006 -797.006], eps: 0.002})
Step:  170400, Reward: [-354.328 -354.328 -354.328] [52.813], Avg: [-412.005 -412.005 -412.005] (1.0000) ({r_i: None, r_t: [-806.938 -806.938 -806.938], eps: 1.0})
Step:   61300, Reward: [-382.922 -382.922 -382.922] [68.396], Avg: [-424.137 -424.137 -424.137] (0.0021) ({r_i: None, r_t: [-769.348 -769.348 -769.348], eps: 0.002})
Step:  170500, Reward: [-397.742 -397.742 -397.742] [70.010], Avg: [-411.996 -411.996 -411.996] (1.0000) ({r_i: None, r_t: [-739.767 -739.767 -739.767], eps: 1.0})
Step:   61400, Reward: [-393.240 -393.240 -393.240] [71.557], Avg: [-424.087 -424.087 -424.087] (0.0021) ({r_i: None, r_t: [-777.731 -777.731 -777.731], eps: 0.002})
Step:  170600, Reward: [-393.725 -393.725 -393.725] [59.014], Avg: [-411.986 -411.986 -411.986] (1.0000) ({r_i: None, r_t: [-771.161 -771.161 -771.161], eps: 1.0})
Step:   61500, Reward: [-387.932 -387.932 -387.932] [68.790], Avg: [-424.028 -424.028 -424.028] (0.0021) ({r_i: None, r_t: [-731.612 -731.612 -731.612], eps: 0.002})
Step:  170700, Reward: [-398.460 -398.460 -398.460] [105.752], Avg: [-411.978 -411.978 -411.978] (1.0000) ({r_i: None, r_t: [-733.009 -733.009 -733.009], eps: 1.0})
Step:   61600, Reward: [-377.485 -377.485 -377.485] [57.522], Avg: [-423.953 -423.953 -423.953] (0.0021) ({r_i: None, r_t: [-790.799 -790.799 -790.799], eps: 0.002})
Step:  170800, Reward: [-391.433 -391.433 -391.433] [69.725], Avg: [-411.966 -411.966 -411.966] (1.0000) ({r_i: None, r_t: [-742.691 -742.691 -742.691], eps: 1.0})
Step:   61700, Reward: [-441.621 -441.621 -441.621] [79.244], Avg: [-423.981 -423.981 -423.981] (0.0021) ({r_i: None, r_t: [-742.711 -742.711 -742.711], eps: 0.002})
Step:  170900, Reward: [-358.729 -358.729 -358.729] [60.410], Avg: [-411.935 -411.935 -411.935] (1.0000) ({r_i: None, r_t: [-753.531 -753.531 -753.531], eps: 1.0})
Step:   61800, Reward: [-401.474 -401.474 -401.474] [80.830], Avg: [-423.945 -423.945 -423.945] (0.0020) ({r_i: None, r_t: [-804.958 -804.958 -804.958], eps: 0.002})
Step:  171000, Reward: [-387.428 -387.428 -387.428] [52.790], Avg: [-411.920 -411.920 -411.920] (1.0000) ({r_i: None, r_t: [-765.044 -765.044 -765.044], eps: 1.0})
Step:   61900, Reward: [-409.506 -409.506 -409.506] [76.831], Avg: [-423.922 -423.922 -423.922] (0.0020) ({r_i: None, r_t: [-786.952 -786.952 -786.952], eps: 0.002})
Step:  171100, Reward: [-355.406 -355.406 -355.406] [34.922], Avg: [-411.887 -411.887 -411.887] (1.0000) ({r_i: None, r_t: [-757.127 -757.127 -757.127], eps: 1.0})
Step:   62000, Reward: [-398.140 -398.140 -398.140] [68.726], Avg: [-423.880 -423.880 -423.880] (0.0020) ({r_i: None, r_t: [-766.842 -766.842 -766.842], eps: 0.002})
Step:  171200, Reward: [-381.865 -381.865 -381.865] [58.356], Avg: [-411.870 -411.870 -411.870] (1.0000) ({r_i: None, r_t: [-766.083 -766.083 -766.083], eps: 1.0})
Step:   62100, Reward: [-367.230 -367.230 -367.230] [70.188], Avg: [-423.789 -423.789 -423.789] (0.0020) ({r_i: None, r_t: [-815.245 -815.245 -815.245], eps: 0.002})
Step:  171300, Reward: [-386.258 -386.258 -386.258] [66.609], Avg: [-411.855 -411.855 -411.855] (1.0000) ({r_i: None, r_t: [-776.693 -776.693 -776.693], eps: 1.0})
Step:   62200, Reward: [-417.955 -417.955 -417.955] [63.573], Avg: [-423.780 -423.780 -423.780] (0.0020) ({r_i: None, r_t: [-788.695 -788.695 -788.695], eps: 0.002})
Step:  171400, Reward: [-370.036 -370.036 -370.036] [59.895], Avg: [-411.830 -411.830 -411.830] (1.0000) ({r_i: None, r_t: [-764.429 -764.429 -764.429], eps: 1.0})
Step:   62300, Reward: [-384.856 -384.856 -384.856] [68.243], Avg: [-423.717 -423.717 -423.717] (0.0019) ({r_i: None, r_t: [-800.478 -800.478 -800.478], eps: 0.002})
Step:  171500, Reward: [-387.295 -387.295 -387.295] [56.371], Avg: [-411.816 -411.816 -411.816] (1.0000) ({r_i: None, r_t: [-729.645 -729.645 -729.645], eps: 1.0})
Step:   62400, Reward: [-395.045 -395.045 -395.045] [62.213], Avg: [-423.671 -423.671 -423.671] (0.0019) ({r_i: None, r_t: [-775.498 -775.498 -775.498], eps: 0.002})
Step:  171600, Reward: [-386.251 -386.251 -386.251] [72.895], Avg: [-411.801 -411.801 -411.801] (1.0000) ({r_i: None, r_t: [-764.133 -764.133 -764.133], eps: 1.0})
Step:   62500, Reward: [-408.345 -408.345 -408.345] [73.050], Avg: [-423.647 -423.647 -423.647] (0.0019) ({r_i: None, r_t: [-759.251 -759.251 -759.251], eps: 0.002})
Step:  171700, Reward: [-388.973 -388.973 -388.973] [72.135], Avg: [-411.788 -411.788 -411.788] (1.0000) ({r_i: None, r_t: [-759.745 -759.745 -759.745], eps: 1.0})
Step:   62600, Reward: [-405.172 -405.172 -405.172] [90.196], Avg: [-423.617 -423.617 -423.617] (0.0019) ({r_i: None, r_t: [-779.575 -779.575 -779.575], eps: 0.002})
Step:  171800, Reward: [-408.069 -408.069 -408.069] [69.178], Avg: [-411.786 -411.786 -411.786] (1.0000) ({r_i: None, r_t: [-724.941 -724.941 -724.941], eps: 1.0})
Step:   62700, Reward: [-393.314 -393.314 -393.314] [77.623], Avg: [-423.569 -423.569 -423.569] (0.0019) ({r_i: None, r_t: [-823.696 -823.696 -823.696], eps: 0.002})
Step:  171900, Reward: [-377.278 -377.278 -377.278] [71.513], Avg: [-411.766 -411.766 -411.766] (1.0000) ({r_i: None, r_t: [-763.714 -763.714 -763.714], eps: 1.0})
Step:   62800, Reward: [-393.124 -393.124 -393.124] [62.339], Avg: [-423.521 -423.521 -423.521] (0.0018) ({r_i: None, r_t: [-827.862 -827.862 -827.862], eps: 0.002})
Step:  172000, Reward: [-373.504 -373.504 -373.504] [68.781], Avg: [-411.744 -411.744 -411.744] (1.0000) ({r_i: None, r_t: [-776.981 -776.981 -776.981], eps: 1.0})
Step:   62900, Reward: [-420.181 -420.181 -420.181] [103.269], Avg: [-423.516 -423.516 -423.516] (0.0018) ({r_i: None, r_t: [-761.793 -761.793 -761.793], eps: 0.002})
Step:  172100, Reward: [-376.960 -376.960 -376.960] [45.843], Avg: [-411.723 -411.723 -411.723] (1.0000) ({r_i: None, r_t: [-767.460 -767.460 -767.460], eps: 1.0})
Step:   63000, Reward: [-394.161 -394.161 -394.161] [81.700], Avg: [-423.469 -423.469 -423.469] (0.0018) ({r_i: None, r_t: [-799.887 -799.887 -799.887], eps: 0.002})
Step:  172200, Reward: [-418.526 -418.526 -418.526] [74.391], Avg: [-411.727 -411.727 -411.727] (1.0000) ({r_i: None, r_t: [-784.715 -784.715 -784.715], eps: 1.0})
Step:   63100, Reward: [-400.974 -400.974 -400.974] [43.208], Avg: [-423.433 -423.433 -423.433] (0.0018) ({r_i: None, r_t: [-806.448 -806.448 -806.448], eps: 0.002})
Step:  172300, Reward: [-388.118 -388.118 -388.118] [61.638], Avg: [-411.714 -411.714 -411.714] (1.0000) ({r_i: None, r_t: [-792.799 -792.799 -792.799], eps: 1.0})
Step:   63200, Reward: [-425.498 -425.498 -425.498] [83.161], Avg: [-423.437 -423.437 -423.437] (0.0018) ({r_i: None, r_t: [-792.494 -792.494 -792.494], eps: 0.002})
Step:  172400, Reward: [-386.205 -386.205 -386.205] [54.352], Avg: [-411.699 -411.699 -411.699] (1.0000) ({r_i: None, r_t: [-778.493 -778.493 -778.493], eps: 1.0})
Step:   63300, Reward: [-390.421 -390.421 -390.421] [82.381], Avg: [-423.385 -423.385 -423.385] (0.0018) ({r_i: None, r_t: [-787.062 -787.062 -787.062], eps: 0.002})
Step:  172500, Reward: [-394.680 -394.680 -394.680] [71.149], Avg: [-411.689 -411.689 -411.689] (1.0000) ({r_i: None, r_t: [-773.350 -773.350 -773.350], eps: 1.0})
Step:   63400, Reward: [-426.430 -426.430 -426.430] [115.140], Avg: [-423.389 -423.389 -423.389] (0.0017) ({r_i: None, r_t: [-764.967 -764.967 -764.967], eps: 0.002})
Step:  172600, Reward: [-361.582 -361.582 -361.582] [60.173], Avg: [-411.660 -411.660 -411.660] (1.0000) ({r_i: None, r_t: [-786.710 -786.710 -786.710], eps: 1.0})
Step:   63500, Reward: [-425.661 -425.661 -425.661] [58.630], Avg: [-423.393 -423.393 -423.393] (0.0017) ({r_i: None, r_t: [-773.623 -773.623 -773.623], eps: 0.002})
Step:  172700, Reward: [-379.285 -379.285 -379.285] [71.415], Avg: [-411.641 -411.641 -411.641] (1.0000) ({r_i: None, r_t: [-784.273 -784.273 -784.273], eps: 1.0})
Step:   63600, Reward: [-386.214 -386.214 -386.214] [63.100], Avg: [-423.335 -423.335 -423.335] (0.0017) ({r_i: None, r_t: [-808.904 -808.904 -808.904], eps: 0.002})
Step:  172800, Reward: [-387.138 -387.138 -387.138] [71.776], Avg: [-411.627 -411.627 -411.627] (1.0000) ({r_i: None, r_t: [-765.925 -765.925 -765.925], eps: 1.0})
Step:   63700, Reward: [-404.645 -404.645 -404.645] [65.288], Avg: [-423.305 -423.305 -423.305] (0.0017) ({r_i: None, r_t: [-810.934 -810.934 -810.934], eps: 0.002})
Step:  172900, Reward: [-377.013 -377.013 -377.013] [63.498], Avg: [-411.607 -411.607 -411.607] (1.0000) ({r_i: None, r_t: [-802.992 -802.992 -802.992], eps: 1.0})
Step:   63800, Reward: [-391.992 -391.992 -391.992] [87.385], Avg: [-423.256 -423.256 -423.256] (0.0017) ({r_i: None, r_t: [-799.989 -799.989 -799.989], eps: 0.002})
Step:  173000, Reward: [-379.997 -379.997 -379.997] [57.145], Avg: [-411.589 -411.589 -411.589] (1.0000) ({r_i: None, r_t: [-789.574 -789.574 -789.574], eps: 1.0})
Step:   63900, Reward: [-411.246 -411.246 -411.246] [91.958], Avg: [-423.238 -423.238 -423.238] (0.0017) ({r_i: None, r_t: [-811.327 -811.327 -811.327], eps: 0.002})
Step:  173100, Reward: [-413.130 -413.130 -413.130] [51.509], Avg: [-411.590 -411.590 -411.590] (1.0000) ({r_i: None, r_t: [-782.941 -782.941 -782.941], eps: 1.0})
Step:   64000, Reward: [-413.042 -413.042 -413.042] [92.897], Avg: [-423.222 -423.222 -423.222] (0.0016) ({r_i: None, r_t: [-789.857 -789.857 -789.857], eps: 0.002})
Step:  173200, Reward: [-382.044 -382.044 -382.044] [60.074], Avg: [-411.573 -411.573 -411.573] (1.0000) ({r_i: None, r_t: [-807.421 -807.421 -807.421], eps: 1.0})
Step:  173300, Reward: [-390.741 -390.741 -390.741] [64.809], Avg: [-411.561 -411.561 -411.561] (1.0000) ({r_i: None, r_t: [-736.028 -736.028 -736.028], eps: 1.0})
Step:   64100, Reward: [-404.960 -404.960 -404.960] [61.568], Avg: [-423.193 -423.193 -423.193] (0.0016) ({r_i: None, r_t: [-791.730 -791.730 -791.730], eps: 0.002})
Step:   64200, Reward: [-373.319 -373.319 -373.319] [76.413], Avg: [-423.116 -423.116 -423.116] (0.0016) ({r_i: None, r_t: [-820.479 -820.479 -820.479], eps: 0.002})
Step:  173400, Reward: [-403.725 -403.725 -403.725] [69.636], Avg: [-411.556 -411.556 -411.556] (1.0000) ({r_i: None, r_t: [-785.247 -785.247 -785.247], eps: 1.0})
Step:   64300, Reward: [-390.433 -390.433 -390.433] [55.409], Avg: [-423.065 -423.065 -423.065] (0.0016) ({r_i: None, r_t: [-845.756 -845.756 -845.756], eps: 0.002})
Step:  173500, Reward: [-387.716 -387.716 -387.716] [64.529], Avg: [-411.542 -411.542 -411.542] (1.0000) ({r_i: None, r_t: [-791.527 -791.527 -791.527], eps: 1.0})
Step:  173600, Reward: [-374.173 -374.173 -374.173] [72.194], Avg: [-411.521 -411.521 -411.521] (1.0000) ({r_i: None, r_t: [-748.584 -748.584 -748.584], eps: 1.0})
Step:   64400, Reward: [-368.694 -368.694 -368.694] [86.624], Avg: [-422.981 -422.981 -422.981] (0.0016) ({r_i: None, r_t: [-877.090 -877.090 -877.090], eps: 0.002})
Step:  173700, Reward: [-375.421 -375.421 -375.421] [69.074], Avg: [-411.500 -411.500 -411.500] (1.0000) ({r_i: None, r_t: [-746.211 -746.211 -746.211], eps: 1.0})
Step:   64500, Reward: [-435.397 -435.397 -435.397] [81.089], Avg: [-423.000 -423.000 -423.000] (0.0016) ({r_i: None, r_t: [-773.445 -773.445 -773.445], eps: 0.002})
Step:  173800, Reward: [-399.613 -399.613 -399.613] [80.711], Avg: [-411.493 -411.493 -411.493] (1.0000) ({r_i: None, r_t: [-773.274 -773.274 -773.274], eps: 1.0})
Step:   64600, Reward: [-390.359 -390.359 -390.359] [82.167], Avg: [-422.949 -422.949 -422.949] (0.0015) ({r_i: None, r_t: [-830.911 -830.911 -830.911], eps: 0.002})
Step:  173900, Reward: [-387.708 -387.708 -387.708] [69.412], Avg: [-411.480 -411.480 -411.480] (1.0000) ({r_i: None, r_t: [-773.011 -773.011 -773.011], eps: 1.0})
Step:   64700, Reward: [-441.757 -441.757 -441.757] [62.305], Avg: [-422.978 -422.978 -422.978] (0.0015) ({r_i: None, r_t: [-797.815 -797.815 -797.815], eps: 0.002})
Step:  174000, Reward: [-393.290 -393.290 -393.290] [85.127], Avg: [-411.469 -411.469 -411.469] (1.0000) ({r_i: None, r_t: [-769.707 -769.707 -769.707], eps: 1.0})
Step:   64800, Reward: [-416.185 -416.185 -416.185] [66.491], Avg: [-422.968 -422.968 -422.968] (0.0015) ({r_i: None, r_t: [-748.126 -748.126 -748.126], eps: 0.002})
Step:  174100, Reward: [-375.618 -375.618 -375.618] [74.055], Avg: [-411.448 -411.448 -411.448] (1.0000) ({r_i: None, r_t: [-796.349 -796.349 -796.349], eps: 1.0})
Step:   64900, Reward: [-400.204 -400.204 -400.204] [63.134], Avg: [-422.933 -422.933 -422.933] (0.0015) ({r_i: None, r_t: [-823.121 -823.121 -823.121], eps: 0.001})
Step:  174200, Reward: [-396.419 -396.419 -396.419] [71.262], Avg: [-411.440 -411.440 -411.440] (1.0000) ({r_i: None, r_t: [-775.847 -775.847 -775.847], eps: 1.0})
Step:   65000, Reward: [-407.915 -407.915 -407.915] [93.823], Avg: [-422.910 -422.910 -422.910] (0.0015) ({r_i: None, r_t: [-829.669 -829.669 -829.669], eps: 0.001})
Step:  174300, Reward: [-385.481 -385.481 -385.481] [72.622], Avg: [-411.425 -411.425 -411.425] (1.0000) ({r_i: None, r_t: [-786.973 -786.973 -786.973], eps: 1.0})
Step:   65100, Reward: [-405.092 -405.092 -405.092] [72.475], Avg: [-422.882 -422.882 -422.882] (0.0015) ({r_i: None, r_t: [-808.875 -808.875 -808.875], eps: 0.001})
Step:  174400, Reward: [-419.263 -419.263 -419.263] [57.843], Avg: [-411.429 -411.429 -411.429] (1.0000) ({r_i: None, r_t: [-757.612 -757.612 -757.612], eps: 1.0})
Step:   65200, Reward: [-407.379 -407.379 -407.379] [62.409], Avg: [-422.859 -422.859 -422.859] (0.0014) ({r_i: None, r_t: [-876.833 -876.833 -876.833], eps: 0.001})
Step:  174500, Reward: [-385.084 -385.084 -385.084] [74.099], Avg: [-411.414 -411.414 -411.414] (1.0000) ({r_i: None, r_t: [-800.465 -800.465 -800.465], eps: 1.0})
Step:   65300, Reward: [-445.848 -445.848 -445.848] [115.498], Avg: [-422.894 -422.894 -422.894] (0.0014) ({r_i: None, r_t: [-784.710 -784.710 -784.710], eps: 0.001})
Step:  174600, Reward: [-391.637 -391.637 -391.637] [44.718], Avg: [-411.403 -411.403 -411.403] (1.0000) ({r_i: None, r_t: [-815.554 -815.554 -815.554], eps: 1.0})
Step:   65400, Reward: [-437.091 -437.091 -437.091] [57.931], Avg: [-422.916 -422.916 -422.916] (0.0014) ({r_i: None, r_t: [-812.018 -812.018 -812.018], eps: 0.001})
Step:  174700, Reward: [-350.563 -350.563 -350.563] [45.268], Avg: [-411.368 -411.368 -411.368] (1.0000) ({r_i: None, r_t: [-809.330 -809.330 -809.330], eps: 1.0})
Step:   65500, Reward: [-387.189 -387.189 -387.189] [62.272], Avg: [-422.861 -422.861 -422.861] (0.0014) ({r_i: None, r_t: [-858.990 -858.990 -858.990], eps: 0.001})
Step:  174800, Reward: [-378.282 -378.282 -378.282] [74.593], Avg: [-411.349 -411.349 -411.349] (1.0000) ({r_i: None, r_t: [-795.743 -795.743 -795.743], eps: 1.0})
Step:   65600, Reward: [-407.590 -407.590 -407.590] [71.439], Avg: [-422.838 -422.838 -422.838] (0.0014) ({r_i: None, r_t: [-810.588 -810.588 -810.588], eps: 0.001})
Step:  174900, Reward: [-376.162 -376.162 -376.162] [92.651], Avg: [-411.329 -411.329 -411.329] (1.0000) ({r_i: None, r_t: [-745.821 -745.821 -745.821], eps: 1.0})
Step:   65700, Reward: [-397.344 -397.344 -397.344] [78.554], Avg: [-422.799 -422.799 -422.799] (0.0014) ({r_i: None, r_t: [-822.478 -822.478 -822.478], eps: 0.001})
Step:  175000, Reward: [-385.321 -385.321 -385.321] [75.746], Avg: [-411.314 -411.314 -411.314] (1.0000) ({r_i: None, r_t: [-792.125 -792.125 -792.125], eps: 1.0})
Step:   65800, Reward: [-415.219 -415.219 -415.219] [85.379], Avg: [-422.788 -422.788 -422.788] (0.0014) ({r_i: None, r_t: [-848.749 -848.749 -848.749], eps: 0.001})
Step:  175100, Reward: [-401.307 -401.307 -401.307] [74.310], Avg: [-411.309 -411.309 -411.309] (1.0000) ({r_i: None, r_t: [-773.545 -773.545 -773.545], eps: 1.0})
Step:   65900, Reward: [-386.859 -386.859 -386.859] [91.388], Avg: [-422.733 -422.733 -422.733] (0.0014) ({r_i: None, r_t: [-840.045 -840.045 -840.045], eps: 0.001})
Step:  175200, Reward: [-384.104 -384.104 -384.104] [69.459], Avg: [-411.293 -411.293 -411.293] (1.0000) ({r_i: None, r_t: [-769.127 -769.127 -769.127], eps: 1.0})
Step:   66000, Reward: [-383.289 -383.289 -383.289] [58.680], Avg: [-422.674 -422.674 -422.674] (0.0013) ({r_i: None, r_t: [-828.692 -828.692 -828.692], eps: 0.001})
Step:  175300, Reward: [-396.223 -396.223 -396.223] [71.235], Avg: [-411.285 -411.285 -411.285] (1.0000) ({r_i: None, r_t: [-780.285 -780.285 -780.285], eps: 1.0})
Step:   66100, Reward: [-405.625 -405.625 -405.625] [59.068], Avg: [-422.648 -422.648 -422.648] (0.0013) ({r_i: None, r_t: [-790.711 -790.711 -790.711], eps: 0.001})
Step:  175400, Reward: [-369.242 -369.242 -369.242] [93.072], Avg: [-411.261 -411.261 -411.261] (1.0000) ({r_i: None, r_t: [-764.217 -764.217 -764.217], eps: 1.0})
Step:   66200, Reward: [-412.784 -412.784 -412.784] [50.584], Avg: [-422.633 -422.633 -422.633] (0.0013) ({r_i: None, r_t: [-811.539 -811.539 -811.539], eps: 0.001})
Step:  175500, Reward: [-387.130 -387.130 -387.130] [78.171], Avg: [-411.247 -411.247 -411.247] (1.0000) ({r_i: None, r_t: [-820.021 -820.021 -820.021], eps: 1.0})
Step:   66300, Reward: [-420.292 -420.292 -420.292] [76.819], Avg: [-422.629 -422.629 -422.629] (0.0013) ({r_i: None, r_t: [-817.032 -817.032 -817.032], eps: 0.001})
Step:  175600, Reward: [-393.910 -393.910 -393.910] [60.890], Avg: [-411.237 -411.237 -411.237] (1.0000) ({r_i: None, r_t: [-795.634 -795.634 -795.634], eps: 1.0})
Step:   66400, Reward: [-399.427 -399.427 -399.427] [57.411], Avg: [-422.594 -422.594 -422.594] (0.0013) ({r_i: None, r_t: [-845.874 -845.874 -845.874], eps: 0.001})
Step:  175700, Reward: [-391.007 -391.007 -391.007] [75.242], Avg: [-411.225 -411.225 -411.225] (1.0000) ({r_i: None, r_t: [-815.294 -815.294 -815.294], eps: 1.0})
Step:   66500, Reward: [-358.524 -358.524 -358.524] [57.155], Avg: [-422.498 -422.498 -422.498] (0.0013) ({r_i: None, r_t: [-810.873 -810.873 -810.873], eps: 0.001})
Step:  175800, Reward: [-417.200 -417.200 -417.200] [72.555], Avg: [-411.229 -411.229 -411.229] (1.0000) ({r_i: None, r_t: [-830.458 -830.458 -830.458], eps: 1.0})
Step:   66600, Reward: [-401.609 -401.609 -401.609] [89.962], Avg: [-422.467 -422.467 -422.467] (0.0013) ({r_i: None, r_t: [-782.212 -782.212 -782.212], eps: 0.001})
Step:  175900, Reward: [-394.170 -394.170 -394.170] [55.518], Avg: [-411.219 -411.219 -411.219] (1.0000) ({r_i: None, r_t: [-784.206 -784.206 -784.206], eps: 1.0})
Step:   66700, Reward: [-402.604 -402.604 -402.604] [59.197], Avg: [-422.437 -422.437 -422.437] (0.0012) ({r_i: None, r_t: [-812.108 -812.108 -812.108], eps: 0.001})
Step:  176000, Reward: [-382.401 -382.401 -382.401] [63.501], Avg: [-411.203 -411.203 -411.203] (1.0000) ({r_i: None, r_t: [-767.611 -767.611 -767.611], eps: 1.0})
Step:   66800, Reward: [-420.212 -420.212 -420.212] [76.872], Avg: [-422.434 -422.434 -422.434] (0.0012) ({r_i: None, r_t: [-830.993 -830.993 -830.993], eps: 0.001})
Step:  176100, Reward: [-374.331 -374.331 -374.331] [55.690], Avg: [-411.182 -411.182 -411.182] (1.0000) ({r_i: None, r_t: [-786.491 -786.491 -786.491], eps: 1.0})
Step:   66900, Reward: [-405.061 -405.061 -405.061] [69.850], Avg: [-422.408 -422.408 -422.408] (0.0012) ({r_i: None, r_t: [-873.056 -873.056 -873.056], eps: 0.001})
Step:  176200, Reward: [-384.160 -384.160 -384.160] [61.303], Avg: [-411.167 -411.167 -411.167] (1.0000) ({r_i: None, r_t: [-768.617 -768.617 -768.617], eps: 1.0})
Step:   67000, Reward: [-424.926 -424.926 -424.926] [64.039], Avg: [-422.412 -422.412 -422.412] (0.0012) ({r_i: None, r_t: [-792.834 -792.834 -792.834], eps: 0.001})
Step:  176300, Reward: [-392.777 -392.777 -392.777] [65.191], Avg: [-411.156 -411.156 -411.156] (1.0000) ({r_i: None, r_t: [-780.237 -780.237 -780.237], eps: 1.0})
Step:   67100, Reward: [-385.205 -385.205 -385.205] [55.155], Avg: [-422.356 -422.356 -422.356] (0.0012) ({r_i: None, r_t: [-809.211 -809.211 -809.211], eps: 0.001})
Step:  176400, Reward: [-378.431 -378.431 -378.431] [56.286], Avg: [-411.138 -411.138 -411.138] (1.0000) ({r_i: None, r_t: [-804.738 -804.738 -804.738], eps: 1.0})
Step:   67200, Reward: [-374.872 -374.872 -374.872] [56.472], Avg: [-422.286 -422.286 -422.286] (0.0012) ({r_i: None, r_t: [-824.276 -824.276 -824.276], eps: 0.001})
Step:  176500, Reward: [-379.007 -379.007 -379.007] [37.454], Avg: [-411.119 -411.119 -411.119] (1.0000) ({r_i: None, r_t: [-772.943 -772.943 -772.943], eps: 1.0})
Step:   67300, Reward: [-406.890 -406.890 -406.890] [70.133], Avg: [-422.263 -422.263 -422.263] (0.0012) ({r_i: None, r_t: [-794.789 -794.789 -794.789], eps: 0.001})
Step:  176600, Reward: [-424.509 -424.509 -424.509] [91.635], Avg: [-411.127 -411.127 -411.127] (1.0000) ({r_i: None, r_t: [-792.153 -792.153 -792.153], eps: 1.0})
Step:   67400, Reward: [-395.748 -395.748 -395.748] [72.201], Avg: [-422.224 -422.224 -422.224] (0.0012) ({r_i: None, r_t: [-823.035 -823.035 -823.035], eps: 0.001})
Step:  176700, Reward: [-403.346 -403.346 -403.346] [45.138], Avg: [-411.123 -411.123 -411.123] (1.0000) ({r_i: None, r_t: [-798.766 -798.766 -798.766], eps: 1.0})
Step:   67500, Reward: [-409.655 -409.655 -409.655] [64.657], Avg: [-422.205 -422.205 -422.205] (0.0012) ({r_i: None, r_t: [-820.568 -820.568 -820.568], eps: 0.001})
Step:  176800, Reward: [-390.689 -390.689 -390.689] [60.983], Avg: [-411.111 -411.111 -411.111] (1.0000) ({r_i: None, r_t: [-788.570 -788.570 -788.570], eps: 1.0})
Step:   67600, Reward: [-435.072 -435.072 -435.072] [88.234], Avg: [-422.224 -422.224 -422.224] (0.0011) ({r_i: None, r_t: [-845.330 -845.330 -845.330], eps: 0.001})
Step:  176900, Reward: [-401.307 -401.307 -401.307] [105.813], Avg: [-411.106 -411.106 -411.106] (1.0000) ({r_i: None, r_t: [-783.512 -783.512 -783.512], eps: 1.0})
Step:   67700, Reward: [-445.574 -445.574 -445.574] [108.044], Avg: [-422.259 -422.259 -422.259] (0.0011) ({r_i: None, r_t: [-829.384 -829.384 -829.384], eps: 0.001})
Step:  177000, Reward: [-401.877 -401.877 -401.877] [61.285], Avg: [-411.100 -411.100 -411.100] (1.0000) ({r_i: None, r_t: [-758.031 -758.031 -758.031], eps: 1.0})
Step:   67800, Reward: [-375.222 -375.222 -375.222] [66.459], Avg: [-422.189 -422.189 -422.189] (0.0011) ({r_i: None, r_t: [-820.509 -820.509 -820.509], eps: 0.001})
Step:  177100, Reward: [-354.226 -354.226 -354.226] [52.136], Avg: [-411.068 -411.068 -411.068] (1.0000) ({r_i: None, r_t: [-760.390 -760.390 -760.390], eps: 1.0})
Step:   67900, Reward: [-378.924 -378.924 -378.924] [77.907], Avg: [-422.126 -422.126 -422.126] (0.0011) ({r_i: None, r_t: [-793.300 -793.300 -793.300], eps: 0.001})
Step:  177200, Reward: [-364.610 -364.610 -364.610] [78.501], Avg: [-411.042 -411.042 -411.042] (1.0000) ({r_i: None, r_t: [-802.796 -802.796 -802.796], eps: 1.0})
Step:   68000, Reward: [-404.312 -404.312 -404.312] [59.086], Avg: [-422.099 -422.099 -422.099] (0.0011) ({r_i: None, r_t: [-821.903 -821.903 -821.903], eps: 0.001})
Step:  177300, Reward: [-389.233 -389.233 -389.233] [53.240], Avg: [-411.030 -411.030 -411.030] (1.0000) ({r_i: None, r_t: [-778.290 -778.290 -778.290], eps: 1.0})
Step:   68100, Reward: [-403.149 -403.149 -403.149] [76.740], Avg: [-422.072 -422.072 -422.072] (0.0011) ({r_i: None, r_t: [-808.580 -808.580 -808.580], eps: 0.001})
Step:  177400, Reward: [-378.307 -378.307 -378.307] [82.597], Avg: [-411.011 -411.011 -411.011] (1.0000) ({r_i: None, r_t: [-725.018 -725.018 -725.018], eps: 1.0})
Step:   68200, Reward: [-416.014 -416.014 -416.014] [71.143], Avg: [-422.063 -422.063 -422.063] (0.0011) ({r_i: None, r_t: [-821.161 -821.161 -821.161], eps: 0.001})
Step:  177500, Reward: [-351.183 -351.183 -351.183] [64.921], Avg: [-410.978 -410.978 -410.978] (1.0000) ({r_i: None, r_t: [-781.155 -781.155 -781.155], eps: 1.0})
Step:   68300, Reward: [-400.347 -400.347 -400.347] [65.135], Avg: [-422.031 -422.031 -422.031] (0.0011) ({r_i: None, r_t: [-834.201 -834.201 -834.201], eps: 0.001})
Step:  177600, Reward: [-402.700 -402.700 -402.700] [60.703], Avg: [-410.973 -410.973 -410.973] (1.0000) ({r_i: None, r_t: [-835.232 -835.232 -835.232], eps: 1.0})
Step:   68400, Reward: [-424.827 -424.827 -424.827] [110.791], Avg: [-422.035 -422.035 -422.035] (0.0011) ({r_i: None, r_t: [-808.061 -808.061 -808.061], eps: 0.001})
Step:  177700, Reward: [-408.109 -408.109 -408.109] [72.992], Avg: [-410.971 -410.971 -410.971] (1.0000) ({r_i: None, r_t: [-795.813 -795.813 -795.813], eps: 1.0})
Step:   68500, Reward: [-396.686 -396.686 -396.686] [62.348], Avg: [-421.998 -421.998 -421.998] (0.0010) ({r_i: None, r_t: [-785.604 -785.604 -785.604], eps: 0.001})
Step:  177800, Reward: [-374.914 -374.914 -374.914] [90.386], Avg: [-410.951 -410.951 -410.951] (1.0000) ({r_i: None, r_t: [-770.881 -770.881 -770.881], eps: 1.0})
Step:   68600, Reward: [-394.032 -394.032 -394.032] [63.114], Avg: [-421.957 -421.957 -421.957] (0.0010) ({r_i: None, r_t: [-817.300 -817.300 -817.300], eps: 0.001})
Step:  177900, Reward: [-390.618 -390.618 -390.618] [69.214], Avg: [-410.940 -410.940 -410.940] (1.0000) ({r_i: None, r_t: [-810.228 -810.228 -810.228], eps: 1.0})
Step:   68700, Reward: [-378.584 -378.584 -378.584] [59.799], Avg: [-421.894 -421.894 -421.894] (0.0010) ({r_i: None, r_t: [-826.750 -826.750 -826.750], eps: 0.001})
Step:  178000, Reward: [-392.459 -392.459 -392.459] [57.041], Avg: [-410.929 -410.929 -410.929] (1.0000) ({r_i: None, r_t: [-840.897 -840.897 -840.897], eps: 1.0})
Step:   68800, Reward: [-391.995 -391.995 -391.995] [69.756], Avg: [-421.851 -421.851 -421.851] (0.0010) ({r_i: None, r_t: [-800.962 -800.962 -800.962], eps: 0.001})
Step:  178100, Reward: [-401.791 -401.791 -401.791] [67.533], Avg: [-410.924 -410.924 -410.924] (1.0000) ({r_i: None, r_t: [-778.982 -778.982 -778.982], eps: 1.0})
Step:   68900, Reward: [-426.390 -426.390 -426.390] [58.912], Avg: [-421.858 -421.858 -421.858] (0.0010) ({r_i: None, r_t: [-820.091 -820.091 -820.091], eps: 0.001})
Step:  178200, Reward: [-392.063 -392.063 -392.063] [53.203], Avg: [-410.914 -410.914 -410.914] (1.0000) ({r_i: None, r_t: [-835.182 -835.182 -835.182], eps: 1.0})
Step:   69000, Reward: [-418.673 -418.673 -418.673] [50.578], Avg: [-421.853 -421.853 -421.853] (0.0010) ({r_i: None, r_t: [-809.159 -809.159 -809.159], eps: 0.001})
Step:  178300, Reward: [-390.185 -390.185 -390.185] [41.734], Avg: [-410.902 -410.902 -410.902] (1.0000) ({r_i: None, r_t: [-804.040 -804.040 -804.040], eps: 1.0})
Step:   69100, Reward: [-415.913 -415.913 -415.913] [90.449], Avg: [-421.844 -421.844 -421.844] (0.0010) ({r_i: None, r_t: [-821.418 -821.418 -821.418], eps: 0.001})
Step:  178400, Reward: [-404.147 -404.147 -404.147] [86.436], Avg: [-410.898 -410.898 -410.898] (1.0000) ({r_i: None, r_t: [-776.767 -776.767 -776.767], eps: 1.0})
Step:   69200, Reward: [-399.103 -399.103 -399.103] [95.301], Avg: [-421.812 -421.812 -421.812] (0.0010) ({r_i: None, r_t: [-859.712 -859.712 -859.712], eps: 0.001})
Step:  178500, Reward: [-382.304 -382.304 -382.304] [63.812], Avg: [-410.882 -410.882 -410.882] (1.0000) ({r_i: None, r_t: [-769.178 -769.178 -769.178], eps: 1.0})
Step:   69300, Reward: [-404.807 -404.807 -404.807] [66.062], Avg: [-421.787 -421.787 -421.787] (0.0010) ({r_i: None, r_t: [-822.962 -822.962 -822.962], eps: 0.001})
Step:  178600, Reward: [-350.817 -350.817 -350.817] [55.474], Avg: [-410.849 -410.849 -410.849] (1.0000) ({r_i: None, r_t: [-783.030 -783.030 -783.030], eps: 1.0})
Step:   69400, Reward: [-416.185 -416.185 -416.185] [76.582], Avg: [-421.779 -421.779 -421.779] (0.0010) ({r_i: None, r_t: [-802.103 -802.103 -802.103], eps: 0.001})
Step:  178700, Reward: [-405.047 -405.047 -405.047] [65.816], Avg: [-410.845 -410.845 -410.845] (1.0000) ({r_i: None, r_t: [-801.428 -801.428 -801.428], eps: 1.0})
Step:   69500, Reward: [-396.169 -396.169 -396.169] [92.730], Avg: [-421.742 -421.742 -421.742] (0.0010) ({r_i: None, r_t: [-796.533 -796.533 -796.533], eps: 0.001})
Step:  178800, Reward: [-414.477 -414.477 -414.477] [68.163], Avg: [-410.847 -410.847 -410.847] (1.0000) ({r_i: None, r_t: [-817.624 -817.624 -817.624], eps: 1.0})
Step:   69600, Reward: [-391.803 -391.803 -391.803] [64.600], Avg: [-421.699 -421.699 -421.699] (0.0010) ({r_i: None, r_t: [-829.711 -829.711 -829.711], eps: 0.001})
Step:  178900, Reward: [-415.427 -415.427 -415.427] [77.018], Avg: [-410.850 -410.850 -410.850] (1.0000) ({r_i: None, r_t: [-796.759 -796.759 -796.759], eps: 1.0})
Step:   69700, Reward: [-405.436 -405.436 -405.436] [78.873], Avg: [-421.676 -421.676 -421.676] (0.0010) ({r_i: None, r_t: [-834.182 -834.182 -834.182], eps: 0.001})
Step:  179000, Reward: [-367.793 -367.793 -367.793] [53.698], Avg: [-410.826 -410.826 -410.826] (1.0000) ({r_i: None, r_t: [-799.776 -799.776 -799.776], eps: 1.0})
Step:   69800, Reward: [-403.818 -403.818 -403.818] [78.928], Avg: [-421.650 -421.650 -421.650] (0.0010) ({r_i: None, r_t: [-863.780 -863.780 -863.780], eps: 0.001})
Step:  179100, Reward: [-385.943 -385.943 -385.943] [57.937], Avg: [-410.812 -410.812 -410.812] (1.0000) ({r_i: None, r_t: [-767.885 -767.885 -767.885], eps: 1.0})
Step:   69900, Reward: [-413.995 -413.995 -413.995] [67.807], Avg: [-421.640 -421.640 -421.640] (0.0010) ({r_i: None, r_t: [-824.888 -824.888 -824.888], eps: 0.001})
Step:  179200, Reward: [-409.319 -409.319 -409.319] [68.411], Avg: [-410.811 -410.811 -410.811] (1.0000) ({r_i: None, r_t: [-788.151 -788.151 -788.151], eps: 1.0})
Step:   70000, Reward: [-412.731 -412.731 -412.731] [99.541], Avg: [-421.627 -421.627 -421.627] (0.0010) ({r_i: None, r_t: [-792.872 -792.872 -792.872], eps: 0.001})
Step:  179300, Reward: [-406.684 -406.684 -406.684] [60.985], Avg: [-410.809 -410.809 -410.809] (1.0000) ({r_i: None, r_t: [-761.577 -761.577 -761.577], eps: 1.0})
Step:   70100, Reward: [-409.155 -409.155 -409.155] [79.574], Avg: [-421.609 -421.609 -421.609] (0.0010) ({r_i: None, r_t: [-839.730 -839.730 -839.730], eps: 0.001})
Step:  179400, Reward: [-384.631 -384.631 -384.631] [91.419], Avg: [-410.794 -410.794 -410.794] (1.0000) ({r_i: None, r_t: [-801.973 -801.973 -801.973], eps: 1.0})
Step:   70200, Reward: [-383.267 -383.267 -383.267] [59.612], Avg: [-421.554 -421.554 -421.554] (0.0010) ({r_i: None, r_t: [-785.300 -785.300 -785.300], eps: 0.001})
Step:  179500, Reward: [-402.944 -402.944 -402.944] [60.698], Avg: [-410.790 -410.790 -410.790] (1.0000) ({r_i: None, r_t: [-780.315 -780.315 -780.315], eps: 1.0})
Step:   70300, Reward: [-414.872 -414.872 -414.872] [67.126], Avg: [-421.545 -421.545 -421.545] (0.0010) ({r_i: None, r_t: [-824.193 -824.193 -824.193], eps: 0.001})
Step:  179600, Reward: [-379.853 -379.853 -379.853] [67.536], Avg: [-410.773 -410.773 -410.773] (1.0000) ({r_i: None, r_t: [-791.108 -791.108 -791.108], eps: 1.0})
Step:   70400, Reward: [-390.841 -390.841 -390.841] [98.947], Avg: [-421.501 -421.501 -421.501] (0.0010) ({r_i: None, r_t: [-810.574 -810.574 -810.574], eps: 0.001})
Step:  179700, Reward: [-379.229 -379.229 -379.229] [62.135], Avg: [-410.755 -410.755 -410.755] (1.0000) ({r_i: None, r_t: [-847.558 -847.558 -847.558], eps: 1.0})
Step:   70500, Reward: [-394.283 -394.283 -394.283] [89.959], Avg: [-421.463 -421.463 -421.463] (0.0010) ({r_i: None, r_t: [-791.549 -791.549 -791.549], eps: 0.001})
Step:  179800, Reward: [-423.751 -423.751 -423.751] [66.797], Avg: [-410.762 -410.762 -410.762] (1.0000) ({r_i: None, r_t: [-819.970 -819.970 -819.970], eps: 1.0})
Step:   70600, Reward: [-414.542 -414.542 -414.542] [89.644], Avg: [-421.453 -421.453 -421.453] (0.0010) ({r_i: None, r_t: [-763.767 -763.767 -763.767], eps: 0.001})
Step:  179900, Reward: [-403.083 -403.083 -403.083] [70.065], Avg: [-410.758 -410.758 -410.758] (1.0000) ({r_i: None, r_t: [-826.907 -826.907 -826.907], eps: 1.0})
Step:   70700, Reward: [-401.063 -401.063 -401.063] [70.504], Avg: [-421.424 -421.424 -421.424] (0.0010) ({r_i: None, r_t: [-770.930 -770.930 -770.930], eps: 0.001})
Step:  180000, Reward: [-386.910 -386.910 -386.910] [61.590], Avg: [-410.745 -410.745 -410.745] (1.0000) ({r_i: None, r_t: [-838.834 -838.834 -838.834], eps: 1.0})
Step:   70800, Reward: [-390.995 -390.995 -390.995] [73.350], Avg: [-421.381 -421.381 -421.381] (0.0010) ({r_i: None, r_t: [-783.535 -783.535 -783.535], eps: 0.001})
Step:  180100, Reward: [-425.011 -425.011 -425.011] [89.429], Avg: [-410.753 -410.753 -410.753] (1.0000) ({r_i: None, r_t: [-753.147 -753.147 -753.147], eps: 1.0})
Step:   70900, Reward: [-406.467 -406.467 -406.467] [70.109], Avg: [-421.360 -421.360 -421.360] (0.0010) ({r_i: None, r_t: [-793.907 -793.907 -793.907], eps: 0.001})
Step:  180200, Reward: [-360.342 -360.342 -360.342] [78.586], Avg: [-410.725 -410.725 -410.725] (1.0000) ({r_i: None, r_t: [-757.610 -757.610 -757.610], eps: 1.0})
Step:   71000, Reward: [-377.795 -377.795 -377.795] [64.295], Avg: [-421.299 -421.299 -421.299] (0.0010) ({r_i: None, r_t: [-840.055 -840.055 -840.055], eps: 0.001})
Step:  180300, Reward: [-392.935 -392.935 -392.935] [46.555], Avg: [-410.715 -410.715 -410.715] (1.0000) ({r_i: None, r_t: [-765.559 -765.559 -765.559], eps: 1.0})
Step:   71100, Reward: [-383.692 -383.692 -383.692] [70.702], Avg: [-421.246 -421.246 -421.246] (0.0010) ({r_i: None, r_t: [-824.069 -824.069 -824.069], eps: 0.001})
Step:  180400, Reward: [-391.871 -391.871 -391.871] [72.614], Avg: [-410.704 -410.704 -410.704] (1.0000) ({r_i: None, r_t: [-767.022 -767.022 -767.022], eps: 1.0})
Step:   71200, Reward: [-431.695 -431.695 -431.695] [80.527], Avg: [-421.261 -421.261 -421.261] (0.0010) ({r_i: None, r_t: [-760.901 -760.901 -760.901], eps: 0.001})
Step:  180500, Reward: [-400.336 -400.336 -400.336] [58.113], Avg: [-410.699 -410.699 -410.699] (1.0000) ({r_i: None, r_t: [-776.841 -776.841 -776.841], eps: 1.0})
Step:   71300, Reward: [-414.453 -414.453 -414.453] [61.527], Avg: [-421.251 -421.251 -421.251] (0.0010) ({r_i: None, r_t: [-815.104 -815.104 -815.104], eps: 0.001})
Step:  180600, Reward: [-371.338 -371.338 -371.338] [71.678], Avg: [-410.677 -410.677 -410.677] (1.0000) ({r_i: None, r_t: [-795.728 -795.728 -795.728], eps: 1.0})
Step:   71400, Reward: [-377.599 -377.599 -377.599] [59.157], Avg: [-421.190 -421.190 -421.190] (0.0010) ({r_i: None, r_t: [-763.307 -763.307 -763.307], eps: 0.001})
Step:  180700, Reward: [-378.957 -378.957 -378.957] [62.494], Avg: [-410.659 -410.659 -410.659] (1.0000) ({r_i: None, r_t: [-778.317 -778.317 -778.317], eps: 1.0})
Step:   71500, Reward: [-397.764 -397.764 -397.764] [63.445], Avg: [-421.158 -421.158 -421.158] (0.0010) ({r_i: None, r_t: [-791.807 -791.807 -791.807], eps: 0.001})
Step:  180800, Reward: [-409.361 -409.361 -409.361] [47.985], Avg: [-410.659 -410.659 -410.659] (1.0000) ({r_i: None, r_t: [-747.871 -747.871 -747.871], eps: 1.0})
Step:   71600, Reward: [-409.267 -409.267 -409.267] [69.160], Avg: [-421.141 -421.141 -421.141] (0.0010) ({r_i: None, r_t: [-779.965 -779.965 -779.965], eps: 0.001})
Step:  180900, Reward: [-443.357 -443.357 -443.357] [91.237], Avg: [-410.677 -410.677 -410.677] (1.0000) ({r_i: None, r_t: [-769.096 -769.096 -769.096], eps: 1.0})
Step:   71700, Reward: [-384.105 -384.105 -384.105] [56.773], Avg: [-421.089 -421.089 -421.089] (0.0010) ({r_i: None, r_t: [-828.497 -828.497 -828.497], eps: 0.001})
Step:  181000, Reward: [-382.640 -382.640 -382.640] [57.961], Avg: [-410.661 -410.661 -410.661] (1.0000) ({r_i: None, r_t: [-772.034 -772.034 -772.034], eps: 1.0})
Step:   71800, Reward: [-342.590 -342.590 -342.590] [54.646], Avg: [-420.980 -420.980 -420.980] (0.0010) ({r_i: None, r_t: [-803.030 -803.030 -803.030], eps: 0.001})
Step:  181100, Reward: [-394.086 -394.086 -394.086] [48.496], Avg: [-410.652 -410.652 -410.652] (1.0000) ({r_i: None, r_t: [-758.577 -758.577 -758.577], eps: 1.0})
Step:   71900, Reward: [-353.402 -353.402 -353.402] [67.145], Avg: [-420.886 -420.886 -420.886] (0.0010) ({r_i: None, r_t: [-782.116 -782.116 -782.116], eps: 0.001})
Step:  181200, Reward: [-408.890 -408.890 -408.890] [84.798], Avg: [-410.651 -410.651 -410.651] (1.0000) ({r_i: None, r_t: [-744.052 -744.052 -744.052], eps: 1.0})
Step:   72000, Reward: [-381.249 -381.249 -381.249] [49.943], Avg: [-420.831 -420.831 -420.831] (0.0010) ({r_i: None, r_t: [-773.232 -773.232 -773.232], eps: 0.001})
Step:  181300, Reward: [-378.650 -378.650 -378.650] [64.142], Avg: [-410.633 -410.633 -410.633] (1.0000) ({r_i: None, r_t: [-756.665 -756.665 -756.665], eps: 1.0})
Step:   72100, Reward: [-376.203 -376.203 -376.203] [46.593], Avg: [-420.770 -420.770 -420.770] (0.0010) ({r_i: None, r_t: [-734.129 -734.129 -734.129], eps: 0.001})
Step:  181400, Reward: [-392.776 -392.776 -392.776] [53.873], Avg: [-410.624 -410.624 -410.624] (1.0000) ({r_i: None, r_t: [-801.870 -801.870 -801.870], eps: 1.0})
Step:   72200, Reward: [-382.897 -382.897 -382.897] [66.152], Avg: [-420.717 -420.717 -420.717] (0.0010) ({r_i: None, r_t: [-797.669 -797.669 -797.669], eps: 0.001})
Step:  181500, Reward: [-385.945 -385.945 -385.945] [43.614], Avg: [-410.610 -410.610 -410.610] (1.0000) ({r_i: None, r_t: [-783.779 -783.779 -783.779], eps: 1.0})
Step:   72300, Reward: [-391.398 -391.398 -391.398] [74.172], Avg: [-420.677 -420.677 -420.677] (0.0010) ({r_i: None, r_t: [-817.263 -817.263 -817.263], eps: 0.001})
Step:  181600, Reward: [-371.033 -371.033 -371.033] [70.764], Avg: [-410.588 -410.588 -410.588] (1.0000) ({r_i: None, r_t: [-788.244 -788.244 -788.244], eps: 1.0})
Step:   72400, Reward: [-345.945 -345.945 -345.945] [61.255], Avg: [-420.574 -420.574 -420.574] (0.0010) ({r_i: None, r_t: [-815.872 -815.872 -815.872], eps: 0.001})
Step:  181700, Reward: [-382.889 -382.889 -382.889] [84.205], Avg: [-410.573 -410.573 -410.573] (1.0000) ({r_i: None, r_t: [-780.615 -780.615 -780.615], eps: 1.0})
Step:   72500, Reward: [-413.171 -413.171 -413.171] [86.548], Avg: [-420.564 -420.564 -420.564] (0.0010) ({r_i: None, r_t: [-825.682 -825.682 -825.682], eps: 0.001})
Step:  181800, Reward: [-379.272 -379.272 -379.272] [69.466], Avg: [-410.556 -410.556 -410.556] (1.0000) ({r_i: None, r_t: [-817.985 -817.985 -817.985], eps: 1.0})
Step:   72600, Reward: [-420.280 -420.280 -420.280] [77.355], Avg: [-420.563 -420.563 -420.563] (0.0010) ({r_i: None, r_t: [-740.237 -740.237 -740.237], eps: 0.001})
Step:  181900, Reward: [-397.807 -397.807 -397.807] [72.587], Avg: [-410.549 -410.549 -410.549] (1.0000) ({r_i: None, r_t: [-746.193 -746.193 -746.193], eps: 1.0})
Step:   72700, Reward: [-411.922 -411.922 -411.922] [68.477], Avg: [-420.551 -420.551 -420.551] (0.0010) ({r_i: None, r_t: [-784.929 -784.929 -784.929], eps: 0.001})
Step:  182000, Reward: [-392.422 -392.422 -392.422] [63.801], Avg: [-410.539 -410.539 -410.539] (1.0000) ({r_i: None, r_t: [-845.330 -845.330 -845.330], eps: 1.0})
Step:   72800, Reward: [-393.949 -393.949 -393.949] [70.080], Avg: [-420.515 -420.515 -420.515] (0.0010) ({r_i: None, r_t: [-761.615 -761.615 -761.615], eps: 0.001})
Step:  182100, Reward: [-381.463 -381.463 -381.463] [58.621], Avg: [-410.523 -410.523 -410.523] (1.0000) ({r_i: None, r_t: [-814.928 -814.928 -814.928], eps: 1.0})
Step:   72900, Reward: [-410.161 -410.161 -410.161] [64.460], Avg: [-420.501 -420.501 -420.501] (0.0010) ({r_i: None, r_t: [-812.838 -812.838 -812.838], eps: 0.001})
Step:  182200, Reward: [-425.123 -425.123 -425.123] [55.374], Avg: [-410.531 -410.531 -410.531] (1.0000) ({r_i: None, r_t: [-777.811 -777.811 -777.811], eps: 1.0})
Step:   73000, Reward: [-374.310 -374.310 -374.310] [60.923], Avg: [-420.437 -420.437 -420.437] (0.0010) ({r_i: None, r_t: [-780.912 -780.912 -780.912], eps: 0.001})
Step:  182300, Reward: [-401.897 -401.897 -401.897] [72.504], Avg: [-410.526 -410.526 -410.526] (1.0000) ({r_i: None, r_t: [-769.297 -769.297 -769.297], eps: 1.0})
Step:   73100, Reward: [-383.013 -383.013 -383.013] [46.128], Avg: [-420.386 -420.386 -420.386] (0.0010) ({r_i: None, r_t: [-793.321 -793.321 -793.321], eps: 0.001})
Step:  182400, Reward: [-415.064 -415.064 -415.064] [80.666], Avg: [-410.529 -410.529 -410.529] (1.0000) ({r_i: None, r_t: [-812.378 -812.378 -812.378], eps: 1.0})
Step:   73200, Reward: [-395.119 -395.119 -395.119] [69.695], Avg: [-420.352 -420.352 -420.352] (0.0010) ({r_i: None, r_t: [-783.071 -783.071 -783.071], eps: 0.001})
Step:  182500, Reward: [-372.750 -372.750 -372.750] [58.180], Avg: [-410.508 -410.508 -410.508] (1.0000) ({r_i: None, r_t: [-820.668 -820.668 -820.668], eps: 1.0})
Step:   73300, Reward: [-364.650 -364.650 -364.650] [67.911], Avg: [-420.276 -420.276 -420.276] (0.0010) ({r_i: None, r_t: [-755.634 -755.634 -755.634], eps: 0.001})
Step:  182600, Reward: [-390.608 -390.608 -390.608] [77.642], Avg: [-410.497 -410.497 -410.497] (1.0000) ({r_i: None, r_t: [-761.757 -761.757 -761.757], eps: 1.0})
Step:   73400, Reward: [-375.333 -375.333 -375.333] [60.699], Avg: [-420.215 -420.215 -420.215] (0.0010) ({r_i: None, r_t: [-780.624 -780.624 -780.624], eps: 0.001})
Step:  182700, Reward: [-396.231 -396.231 -396.231] [72.027], Avg: [-410.489 -410.489 -410.489] (1.0000) ({r_i: None, r_t: [-807.131 -807.131 -807.131], eps: 1.0})
Step:   73500, Reward: [-354.869 -354.869 -354.869] [61.160], Avg: [-420.126 -420.126 -420.126] (0.0010) ({r_i: None, r_t: [-826.833 -826.833 -826.833], eps: 0.001})
Step:  182800, Reward: [-360.918 -360.918 -360.918] [54.862], Avg: [-410.462 -410.462 -410.462] (1.0000) ({r_i: None, r_t: [-795.972 -795.972 -795.972], eps: 1.0})
Step:   73600, Reward: [-389.429 -389.429 -389.429] [64.475], Avg: [-420.084 -420.084 -420.084] (0.0010) ({r_i: None, r_t: [-804.769 -804.769 -804.769], eps: 0.001})
Step:  182900, Reward: [-360.077 -360.077 -360.077] [68.875], Avg: [-410.435 -410.435 -410.435] (1.0000) ({r_i: None, r_t: [-749.144 -749.144 -749.144], eps: 1.0})
Step:   73700, Reward: [-411.579 -411.579 -411.579] [57.652], Avg: [-420.073 -420.073 -420.073] (0.0010) ({r_i: None, r_t: [-802.582 -802.582 -802.582], eps: 0.001})
Step:  183000, Reward: [-406.109 -406.109 -406.109] [68.490], Avg: [-410.432 -410.432 -410.432] (1.0000) ({r_i: None, r_t: [-818.461 -818.461 -818.461], eps: 1.0})
Step:   73800, Reward: [-399.146 -399.146 -399.146] [76.500], Avg: [-420.044 -420.044 -420.044] (0.0010) ({r_i: None, r_t: [-747.671 -747.671 -747.671], eps: 0.001})
Step:  183100, Reward: [-423.047 -423.047 -423.047] [53.195], Avg: [-410.439 -410.439 -410.439] (1.0000) ({r_i: None, r_t: [-772.866 -772.866 -772.866], eps: 1.0})
Step:   73900, Reward: [-367.964 -367.964 -367.964] [68.589], Avg: [-419.974 -419.974 -419.974] (0.0010) ({r_i: None, r_t: [-788.456 -788.456 -788.456], eps: 0.001})
Step:  183200, Reward: [-361.904 -361.904 -361.904] [56.603], Avg: [-410.413 -410.413 -410.413] (1.0000) ({r_i: None, r_t: [-786.377 -786.377 -786.377], eps: 1.0})
Step:   74000, Reward: [-399.196 -399.196 -399.196] [73.136], Avg: [-419.946 -419.946 -419.946] (0.0010) ({r_i: None, r_t: [-804.882 -804.882 -804.882], eps: 0.001})
Step:  183300, Reward: [-383.496 -383.496 -383.496] [86.439], Avg: [-410.398 -410.398 -410.398] (1.0000) ({r_i: None, r_t: [-809.415 -809.415 -809.415], eps: 1.0})
Step:   74100, Reward: [-364.648 -364.648 -364.648] [75.021], Avg: [-419.872 -419.872 -419.872] (0.0010) ({r_i: None, r_t: [-809.190 -809.190 -809.190], eps: 0.001})
Step:  183400, Reward: [-401.583 -401.583 -401.583] [103.139], Avg: [-410.393 -410.393 -410.393] (1.0000) ({r_i: None, r_t: [-822.938 -822.938 -822.938], eps: 1.0})
Step:   74200, Reward: [-377.377 -377.377 -377.377] [69.424], Avg: [-419.814 -419.814 -419.814] (0.0010) ({r_i: None, r_t: [-781.238 -781.238 -781.238], eps: 0.001})
Step:  183500, Reward: [-405.466 -405.466 -405.466] [71.512], Avg: [-410.391 -410.391 -410.391] (1.0000) ({r_i: None, r_t: [-809.764 -809.764 -809.764], eps: 1.0})
Step:   74300, Reward: [-377.575 -377.575 -377.575] [58.133], Avg: [-419.758 -419.758 -419.758] (0.0010) ({r_i: None, r_t: [-817.575 -817.575 -817.575], eps: 0.001})
Step:  183600, Reward: [-353.443 -353.443 -353.443] [55.302], Avg: [-410.360 -410.360 -410.360] (1.0000) ({r_i: None, r_t: [-753.716 -753.716 -753.716], eps: 1.0})
Step:  183700, Reward: [-392.184 -392.184 -392.184] [46.247], Avg: [-410.350 -410.350 -410.350] (1.0000) ({r_i: None, r_t: [-744.747 -744.747 -744.747], eps: 1.0})
Step:   74400, Reward: [-391.448 -391.448 -391.448] [42.287], Avg: [-419.720 -419.720 -419.720] (0.0010) ({r_i: None, r_t: [-796.341 -796.341 -796.341], eps: 0.001})
Step:  183800, Reward: [-386.138 -386.138 -386.138] [76.860], Avg: [-410.336 -410.336 -410.336] (1.0000) ({r_i: None, r_t: [-815.797 -815.797 -815.797], eps: 1.0})
Step:   74500, Reward: [-364.116 -364.116 -364.116] [63.343], Avg: [-419.645 -419.645 -419.645] (0.0010) ({r_i: None, r_t: [-792.963 -792.963 -792.963], eps: 0.001})
Step:  183900, Reward: [-408.462 -408.462 -408.462] [74.317], Avg: [-410.335 -410.335 -410.335] (1.0000) ({r_i: None, r_t: [-794.145 -794.145 -794.145], eps: 1.0})
Step:   74600, Reward: [-367.575 -367.575 -367.575] [70.720], Avg: [-419.575 -419.575 -419.575] (0.0010) ({r_i: None, r_t: [-830.712 -830.712 -830.712], eps: 0.001})
Step:  184000, Reward: [-376.147 -376.147 -376.147] [73.197], Avg: [-410.317 -410.317 -410.317] (1.0000) ({r_i: None, r_t: [-765.297 -765.297 -765.297], eps: 1.0})
Step:   74700, Reward: [-410.766 -410.766 -410.766] [88.697], Avg: [-419.564 -419.564 -419.564] (0.0010) ({r_i: None, r_t: [-747.120 -747.120 -747.120], eps: 0.001})
Step:  184100, Reward: [-393.412 -393.412 -393.412] [68.888], Avg: [-410.308 -410.308 -410.308] (1.0000) ({r_i: None, r_t: [-820.316 -820.316 -820.316], eps: 1.0})
Step:   74800, Reward: [-387.094 -387.094 -387.094] [71.998], Avg: [-419.520 -419.520 -419.520] (0.0010) ({r_i: None, r_t: [-785.189 -785.189 -785.189], eps: 0.001})
Step:  184200, Reward: [-385.123 -385.123 -385.123] [71.108], Avg: [-410.294 -410.294 -410.294] (1.0000) ({r_i: None, r_t: [-788.176 -788.176 -788.176], eps: 1.0})
Step:   74900, Reward: [-371.540 -371.540 -371.540] [58.737], Avg: [-419.456 -419.456 -419.456] (0.0010) ({r_i: None, r_t: [-790.700 -790.700 -790.700], eps: 0.001})
Step:  184300, Reward: [-394.672 -394.672 -394.672] [82.145], Avg: [-410.286 -410.286 -410.286] (1.0000) ({r_i: None, r_t: [-810.517 -810.517 -810.517], eps: 1.0})
Step:   75000, Reward: [-408.385 -408.385 -408.385] [78.397], Avg: [-419.441 -419.441 -419.441] (0.0010) ({r_i: None, r_t: [-778.117 -778.117 -778.117], eps: 0.001})
Step:  184400, Reward: [-394.398 -394.398 -394.398] [53.708], Avg: [-410.277 -410.277 -410.277] (1.0000) ({r_i: None, r_t: [-759.866 -759.866 -759.866], eps: 1.0})
Step:   75100, Reward: [-375.367 -375.367 -375.367] [67.865], Avg: [-419.383 -419.383 -419.383] (0.0010) ({r_i: None, r_t: [-818.802 -818.802 -818.802], eps: 0.001})
Step:  184500, Reward: [-382.813 -382.813 -382.813] [71.864], Avg: [-410.262 -410.262 -410.262] (1.0000) ({r_i: None, r_t: [-773.667 -773.667 -773.667], eps: 1.0})
Step:   75200, Reward: [-437.036 -437.036 -437.036] [83.195], Avg: [-419.406 -419.406 -419.406] (0.0010) ({r_i: None, r_t: [-776.187 -776.187 -776.187], eps: 0.001})
Step:  184600, Reward: [-399.806 -399.806 -399.806] [76.404], Avg: [-410.256 -410.256 -410.256] (1.0000) ({r_i: None, r_t: [-756.531 -756.531 -756.531], eps: 1.0})
Step:   75300, Reward: [-452.148 -452.148 -452.148] [125.238], Avg: [-419.450 -419.450 -419.450] (0.0010) ({r_i: None, r_t: [-753.323 -753.323 -753.323], eps: 0.001})
Step:  184700, Reward: [-378.504 -378.504 -378.504] [75.482], Avg: [-410.239 -410.239 -410.239] (1.0000) ({r_i: None, r_t: [-782.131 -782.131 -782.131], eps: 1.0})
Step:   75400, Reward: [-376.510 -376.510 -376.510] [72.415], Avg: [-419.393 -419.393 -419.393] (0.0010) ({r_i: None, r_t: [-742.662 -742.662 -742.662], eps: 0.001})
Step:  184800, Reward: [-407.268 -407.268 -407.268] [75.774], Avg: [-410.238 -410.238 -410.238] (1.0000) ({r_i: None, r_t: [-787.016 -787.016 -787.016], eps: 1.0})
Step:   75500, Reward: [-380.410 -380.410 -380.410] [106.537], Avg: [-419.341 -419.341 -419.341] (0.0010) ({r_i: None, r_t: [-758.312 -758.312 -758.312], eps: 0.001})
Step:  184900, Reward: [-387.430 -387.430 -387.430] [57.662], Avg: [-410.225 -410.225 -410.225] (1.0000) ({r_i: None, r_t: [-779.014 -779.014 -779.014], eps: 1.0})
Step:   75600, Reward: [-423.919 -423.919 -423.919] [79.885], Avg: [-419.347 -419.347 -419.347] (0.0010) ({r_i: None, r_t: [-787.038 -787.038 -787.038], eps: 0.001})
Step:  185000, Reward: [-398.568 -398.568 -398.568] [34.891], Avg: [-410.219 -410.219 -410.219] (1.0000) ({r_i: None, r_t: [-798.999 -798.999 -798.999], eps: 1.0})
Step:   75700, Reward: [-387.640 -387.640 -387.640] [61.219], Avg: [-419.306 -419.306 -419.306] (0.0010) ({r_i: None, r_t: [-755.417 -755.417 -755.417], eps: 0.001})
Step:  185100, Reward: [-414.185 -414.185 -414.185] [72.120], Avg: [-410.221 -410.221 -410.221] (1.0000) ({r_i: None, r_t: [-780.540 -780.540 -780.540], eps: 1.0})
Step:   75800, Reward: [-398.558 -398.558 -398.558] [60.283], Avg: [-419.278 -419.278 -419.278] (0.0010) ({r_i: None, r_t: [-795.910 -795.910 -795.910], eps: 0.001})
Step:  185200, Reward: [-356.865 -356.865 -356.865] [57.124], Avg: [-410.192 -410.192 -410.192] (1.0000) ({r_i: None, r_t: [-791.858 -791.858 -791.858], eps: 1.0})
Step:   75900, Reward: [-415.283 -415.283 -415.283] [111.095], Avg: [-419.273 -419.273 -419.273] (0.0010) ({r_i: None, r_t: [-728.728 -728.728 -728.728], eps: 0.001})
Step:  185300, Reward: [-377.198 -377.198 -377.198] [92.693], Avg: [-410.175 -410.175 -410.175] (1.0000) ({r_i: None, r_t: [-794.763 -794.763 -794.763], eps: 1.0})
Step:   76000, Reward: [-388.714 -388.714 -388.714] [69.701], Avg: [-419.233 -419.233 -419.233] (0.0010) ({r_i: None, r_t: [-821.089 -821.089 -821.089], eps: 0.001})
Step:  185400, Reward: [-357.621 -357.621 -357.621] [62.011], Avg: [-410.146 -410.146 -410.146] (1.0000) ({r_i: None, r_t: [-823.902 -823.902 -823.902], eps: 1.0})
Step:   76100, Reward: [-381.927 -381.927 -381.927] [73.845], Avg: [-419.184 -419.184 -419.184] (0.0010) ({r_i: None, r_t: [-781.008 -781.008 -781.008], eps: 0.001})
Step:  185500, Reward: [-403.196 -403.196 -403.196] [73.851], Avg: [-410.142 -410.142 -410.142] (1.0000) ({r_i: None, r_t: [-868.022 -868.022 -868.022], eps: 1.0})
Step:   76200, Reward: [-420.273 -420.273 -420.273] [72.905], Avg: [-419.185 -419.185 -419.185] (0.0010) ({r_i: None, r_t: [-800.476 -800.476 -800.476], eps: 0.001})
Step:  185600, Reward: [-403.740 -403.740 -403.740] [58.768], Avg: [-410.139 -410.139 -410.139] (1.0000) ({r_i: None, r_t: [-822.624 -822.624 -822.624], eps: 1.0})
Step:   76300, Reward: [-362.705 -362.705 -362.705] [78.073], Avg: [-419.111 -419.111 -419.111] (0.0010) ({r_i: None, r_t: [-799.585 -799.585 -799.585], eps: 0.001})
Step:  185700, Reward: [-400.313 -400.313 -400.313] [71.304], Avg: [-410.134 -410.134 -410.134] (1.0000) ({r_i: None, r_t: [-810.596 -810.596 -810.596], eps: 1.0})
Step:   76400, Reward: [-428.878 -428.878 -428.878] [66.972], Avg: [-419.124 -419.124 -419.124] (0.0010) ({r_i: None, r_t: [-736.394 -736.394 -736.394], eps: 0.001})
Step:  185800, Reward: [-395.936 -395.936 -395.936] [63.288], Avg: [-410.126 -410.126 -410.126] (1.0000) ({r_i: None, r_t: [-818.164 -818.164 -818.164], eps: 1.0})
Step:   76500, Reward: [-376.868 -376.868 -376.868] [67.305], Avg: [-419.069 -419.069 -419.069] (0.0010) ({r_i: None, r_t: [-868.178 -868.178 -868.178], eps: 0.001})
Step:  185900, Reward: [-435.418 -435.418 -435.418] [67.318], Avg: [-410.140 -410.140 -410.140] (1.0000) ({r_i: None, r_t: [-790.430 -790.430 -790.430], eps: 1.0})
Step:   76600, Reward: [-400.927 -400.927 -400.927] [62.273], Avg: [-419.045 -419.045 -419.045] (0.0010) ({r_i: None, r_t: [-759.830 -759.830 -759.830], eps: 0.001})
Step:  186000, Reward: [-404.438 -404.438 -404.438] [64.540], Avg: [-410.137 -410.137 -410.137] (1.0000) ({r_i: None, r_t: [-813.497 -813.497 -813.497], eps: 1.0})
Step:   76700, Reward: [-400.486 -400.486 -400.486] [81.772], Avg: [-419.021 -419.021 -419.021] (0.0010) ({r_i: None, r_t: [-790.865 -790.865 -790.865], eps: 0.001})
Step:  186100, Reward: [-414.868 -414.868 -414.868] [86.890], Avg: [-410.139 -410.139 -410.139] (1.0000) ({r_i: None, r_t: [-805.321 -805.321 -805.321], eps: 1.0})
Step:   76800, Reward: [-382.398 -382.398 -382.398] [60.165], Avg: [-418.973 -418.973 -418.973] (0.0010) ({r_i: None, r_t: [-790.290 -790.290 -790.290], eps: 0.001})
Step:  186200, Reward: [-434.612 -434.612 -434.612] [89.308], Avg: [-410.152 -410.152 -410.152] (1.0000) ({r_i: None, r_t: [-833.081 -833.081 -833.081], eps: 1.0})
Step:   76900, Reward: [-385.131 -385.131 -385.131] [71.275], Avg: [-418.930 -418.930 -418.930] (0.0010) ({r_i: None, r_t: [-768.804 -768.804 -768.804], eps: 0.001})
Step:  186300, Reward: [-401.225 -401.225 -401.225] [71.641], Avg: [-410.148 -410.148 -410.148] (1.0000) ({r_i: None, r_t: [-798.471 -798.471 -798.471], eps: 1.0})
Step:   77000, Reward: [-403.758 -403.758 -403.758] [68.589], Avg: [-418.910 -418.910 -418.910] (0.0010) ({r_i: None, r_t: [-771.582 -771.582 -771.582], eps: 0.001})
Step:  186400, Reward: [-388.615 -388.615 -388.615] [85.280], Avg: [-410.136 -410.136 -410.136] (1.0000) ({r_i: None, r_t: [-827.769 -827.769 -827.769], eps: 1.0})
Step:   77100, Reward: [-395.227 -395.227 -395.227] [66.584], Avg: [-418.879 -418.879 -418.879] (0.0010) ({r_i: None, r_t: [-814.283 -814.283 -814.283], eps: 0.001})
Step:  186500, Reward: [-354.505 -354.505 -354.505] [53.870], Avg: [-410.106 -410.106 -410.106] (1.0000) ({r_i: None, r_t: [-836.591 -836.591 -836.591], eps: 1.0})
Step:   77200, Reward: [-403.627 -403.627 -403.627] [88.096], Avg: [-418.859 -418.859 -418.859] (0.0010) ({r_i: None, r_t: [-787.503 -787.503 -787.503], eps: 0.001})
Step:  186600, Reward: [-354.445 -354.445 -354.445] [72.849], Avg: [-410.076 -410.076 -410.076] (1.0000) ({r_i: None, r_t: [-811.756 -811.756 -811.756], eps: 1.0})
Step:   77300, Reward: [-408.763 -408.763 -408.763] [77.027], Avg: [-418.846 -418.846 -418.846] (0.0010) ({r_i: None, r_t: [-764.431 -764.431 -764.431], eps: 0.001})
Step:  186700, Reward: [-429.164 -429.164 -429.164] [79.724], Avg: [-410.087 -410.087 -410.087] (1.0000) ({r_i: None, r_t: [-780.707 -780.707 -780.707], eps: 1.0})
Step:   77400, Reward: [-394.463 -394.463 -394.463] [58.594], Avg: [-418.815 -418.815 -418.815] (0.0010) ({r_i: None, r_t: [-780.607 -780.607 -780.607], eps: 0.001})
Step:  186800, Reward: [-383.745 -383.745 -383.745] [70.256], Avg: [-410.072 -410.072 -410.072] (1.0000) ({r_i: None, r_t: [-840.502 -840.502 -840.502], eps: 1.0})
Step:   77500, Reward: [-389.089 -389.089 -389.089] [71.381], Avg: [-418.777 -418.777 -418.777] (0.0010) ({r_i: None, r_t: [-774.784 -774.784 -774.784], eps: 0.001})
Step:  186900, Reward: [-423.372 -423.372 -423.372] [75.779], Avg: [-410.080 -410.080 -410.080] (1.0000) ({r_i: None, r_t: [-842.747 -842.747 -842.747], eps: 1.0})
Step:   77600, Reward: [-362.030 -362.030 -362.030] [59.049], Avg: [-418.704 -418.704 -418.704] (0.0010) ({r_i: None, r_t: [-765.436 -765.436 -765.436], eps: 0.001})
Step:  187000, Reward: [-425.262 -425.262 -425.262] [84.948], Avg: [-410.088 -410.088 -410.088] (1.0000) ({r_i: None, r_t: [-785.151 -785.151 -785.151], eps: 1.0})
Step:   77700, Reward: [-390.323 -390.323 -390.323] [72.914], Avg: [-418.667 -418.667 -418.667] (0.0010) ({r_i: None, r_t: [-763.463 -763.463 -763.463], eps: 0.001})
Step:  187100, Reward: [-361.389 -361.389 -361.389] [75.661], Avg: [-410.062 -410.062 -410.062] (1.0000) ({r_i: None, r_t: [-788.248 -788.248 -788.248], eps: 1.0})
Step:   77800, Reward: [-401.010 -401.010 -401.010] [56.169], Avg: [-418.644 -418.644 -418.644] (0.0010) ({r_i: None, r_t: [-767.144 -767.144 -767.144], eps: 0.001})
Step:  187200, Reward: [-405.928 -405.928 -405.928] [61.730], Avg: [-410.059 -410.059 -410.059] (1.0000) ({r_i: None, r_t: [-793.098 -793.098 -793.098], eps: 1.0})
Step:   77900, Reward: [-424.401 -424.401 -424.401] [84.914], Avg: [-418.652 -418.652 -418.652] (0.0010) ({r_i: None, r_t: [-765.688 -765.688 -765.688], eps: 0.001})
Step:  187300, Reward: [-387.952 -387.952 -387.952] [67.687], Avg: [-410.048 -410.048 -410.048] (1.0000) ({r_i: None, r_t: [-759.655 -759.655 -759.655], eps: 1.0})
Step:   78000, Reward: [-396.987 -396.987 -396.987] [95.939], Avg: [-418.624 -418.624 -418.624] (0.0010) ({r_i: None, r_t: [-821.043 -821.043 -821.043], eps: 0.001})
Step:  187400, Reward: [-401.125 -401.125 -401.125] [62.225], Avg: [-410.043 -410.043 -410.043] (1.0000) ({r_i: None, r_t: [-776.552 -776.552 -776.552], eps: 1.0})
Step:   78100, Reward: [-382.321 -382.321 -382.321] [61.043], Avg: [-418.578 -418.578 -418.578] (0.0010) ({r_i: None, r_t: [-777.672 -777.672 -777.672], eps: 0.001})
Step:  187500, Reward: [-392.835 -392.835 -392.835] [45.463], Avg: [-410.034 -410.034 -410.034] (1.0000) ({r_i: None, r_t: [-747.836 -747.836 -747.836], eps: 1.0})
Step:   78200, Reward: [-393.762 -393.762 -393.762] [80.531], Avg: [-418.546 -418.546 -418.546] (0.0010) ({r_i: None, r_t: [-792.432 -792.432 -792.432], eps: 0.001})
Step:  187600, Reward: [-384.677 -384.677 -384.677] [75.482], Avg: [-410.020 -410.020 -410.020] (1.0000) ({r_i: None, r_t: [-795.511 -795.511 -795.511], eps: 1.0})
Step:   78300, Reward: [-393.117 -393.117 -393.117] [71.120], Avg: [-418.514 -418.514 -418.514] (0.0010) ({r_i: None, r_t: [-807.488 -807.488 -807.488], eps: 0.001})
Step:  187700, Reward: [-367.228 -367.228 -367.228] [56.631], Avg: [-409.997 -409.997 -409.997] (1.0000) ({r_i: None, r_t: [-818.146 -818.146 -818.146], eps: 1.0})
Step:   78400, Reward: [-407.416 -407.416 -407.416] [65.119], Avg: [-418.499 -418.499 -418.499] (0.0010) ({r_i: None, r_t: [-778.095 -778.095 -778.095], eps: 0.001})
Step:  187800, Reward: [-403.436 -403.436 -403.436] [65.918], Avg: [-409.994 -409.994 -409.994] (1.0000) ({r_i: None, r_t: [-802.245 -802.245 -802.245], eps: 1.0})
Step:   78500, Reward: [-413.060 -413.060 -413.060] [72.133], Avg: [-418.492 -418.492 -418.492] (0.0010) ({r_i: None, r_t: [-795.986 -795.986 -795.986], eps: 0.001})
Step:  187900, Reward: [-390.594 -390.594 -390.594] [76.151], Avg: [-409.984 -409.984 -409.984] (1.0000) ({r_i: None, r_t: [-800.284 -800.284 -800.284], eps: 1.0})
Step:   78600, Reward: [-371.331 -371.331 -371.331] [76.612], Avg: [-418.433 -418.433 -418.433] (0.0010) ({r_i: None, r_t: [-760.043 -760.043 -760.043], eps: 0.001})
Step:  188000, Reward: [-427.821 -427.821 -427.821] [80.737], Avg: [-409.993 -409.993 -409.993] (1.0000) ({r_i: None, r_t: [-816.216 -816.216 -816.216], eps: 1.0})
Step:   78700, Reward: [-411.768 -411.768 -411.768] [109.040], Avg: [-418.424 -418.424 -418.424] (0.0010) ({r_i: None, r_t: [-779.437 -779.437 -779.437], eps: 0.001})
Step:  188100, Reward: [-363.528 -363.528 -363.528] [66.429], Avg: [-409.968 -409.968 -409.968] (1.0000) ({r_i: None, r_t: [-782.010 -782.010 -782.010], eps: 1.0})
Step:   78800, Reward: [-428.508 -428.508 -428.508] [77.919], Avg: [-418.437 -418.437 -418.437] (0.0010) ({r_i: None, r_t: [-794.006 -794.006 -794.006], eps: 0.001})
Step:  188200, Reward: [-410.151 -410.151 -410.151] [83.640], Avg: [-409.969 -409.969 -409.969] (1.0000) ({r_i: None, r_t: [-772.085 -772.085 -772.085], eps: 1.0})
Step:   78900, Reward: [-424.825 -424.825 -424.825] [87.848], Avg: [-418.445 -418.445 -418.445] (0.0010) ({r_i: None, r_t: [-800.794 -800.794 -800.794], eps: 0.001})
Step:  188300, Reward: [-416.889 -416.889 -416.889] [73.174], Avg: [-409.972 -409.972 -409.972] (1.0000) ({r_i: None, r_t: [-860.916 -860.916 -860.916], eps: 1.0})
Step:   79000, Reward: [-417.168 -417.168 -417.168] [77.299], Avg: [-418.443 -418.443 -418.443] (0.0010) ({r_i: None, r_t: [-788.113 -788.113 -788.113], eps: 0.001})
Step:  188400, Reward: [-419.490 -419.490 -419.490] [69.903], Avg: [-409.977 -409.977 -409.977] (1.0000) ({r_i: None, r_t: [-753.072 -753.072 -753.072], eps: 1.0})
Step:   79100, Reward: [-398.492 -398.492 -398.492] [84.600], Avg: [-418.418 -418.418 -418.418] (0.0010) ({r_i: None, r_t: [-770.153 -770.153 -770.153], eps: 0.001})
Step:  188500, Reward: [-413.253 -413.253 -413.253] [85.611], Avg: [-409.979 -409.979 -409.979] (1.0000) ({r_i: None, r_t: [-805.452 -805.452 -805.452], eps: 1.0})
Step:   79200, Reward: [-391.102 -391.102 -391.102] [76.289], Avg: [-418.384 -418.384 -418.384] (0.0010) ({r_i: None, r_t: [-750.429 -750.429 -750.429], eps: 0.001})
Step:  188600, Reward: [-377.490 -377.490 -377.490] [84.335], Avg: [-409.962 -409.962 -409.962] (1.0000) ({r_i: None, r_t: [-783.923 -783.923 -783.923], eps: 1.0})
Step:   79300, Reward: [-381.663 -381.663 -381.663] [68.713], Avg: [-418.337 -418.337 -418.337] (0.0010) ({r_i: None, r_t: [-809.783 -809.783 -809.783], eps: 0.001})
Step:  188700, Reward: [-431.358 -431.358 -431.358] [80.531], Avg: [-409.973 -409.973 -409.973] (1.0000) ({r_i: None, r_t: [-824.916 -824.916 -824.916], eps: 1.0})
Step:   79400, Reward: [-412.837 -412.837 -412.837] [89.420], Avg: [-418.331 -418.331 -418.331] (0.0010) ({r_i: None, r_t: [-827.038 -827.038 -827.038], eps: 0.001})
Step:  188800, Reward: [-409.083 -409.083 -409.083] [61.198], Avg: [-409.973 -409.973 -409.973] (1.0000) ({r_i: None, r_t: [-809.282 -809.282 -809.282], eps: 1.0})
Step:   79500, Reward: [-373.936 -373.936 -373.936] [72.075], Avg: [-418.275 -418.275 -418.275] (0.0010) ({r_i: None, r_t: [-781.211 -781.211 -781.211], eps: 0.001})
Step:  188900, Reward: [-411.816 -411.816 -411.816] [86.396], Avg: [-409.974 -409.974 -409.974] (1.0000) ({r_i: None, r_t: [-794.366 -794.366 -794.366], eps: 1.0})
Step:   79600, Reward: [-396.447 -396.447 -396.447] [69.750], Avg: [-418.247 -418.247 -418.247] (0.0010) ({r_i: None, r_t: [-795.192 -795.192 -795.192], eps: 0.001})
Step:  189000, Reward: [-394.101 -394.101 -394.101] [83.189], Avg: [-409.965 -409.965 -409.965] (1.0000) ({r_i: None, r_t: [-863.929 -863.929 -863.929], eps: 1.0})
Step:  189100, Reward: [-434.707 -434.707 -434.707] [72.864], Avg: [-409.978 -409.978 -409.978] (1.0000) ({r_i: None, r_t: [-772.852 -772.852 -772.852], eps: 1.0})
Step:   79700, Reward: [-385.410 -385.410 -385.410] [52.886], Avg: [-418.206 -418.206 -418.206] (0.0010) ({r_i: None, r_t: [-770.953 -770.953 -770.953], eps: 0.001})
Step:  189200, Reward: [-383.887 -383.887 -383.887] [63.062], Avg: [-409.965 -409.965 -409.965] (1.0000) ({r_i: None, r_t: [-822.676 -822.676 -822.676], eps: 1.0})
Step:   79800, Reward: [-388.477 -388.477 -388.477] [97.482], Avg: [-418.169 -418.169 -418.169] (0.0010) ({r_i: None, r_t: [-727.872 -727.872 -727.872], eps: 0.001})
Step:  189300, Reward: [-385.342 -385.342 -385.342] [61.297], Avg: [-409.952 -409.952 -409.952] (1.0000) ({r_i: None, r_t: [-800.011 -800.011 -800.011], eps: 1.0})
Step:   79900, Reward: [-366.522 -366.522 -366.522] [76.484], Avg: [-418.104 -418.104 -418.104] (0.0010) ({r_i: None, r_t: [-776.695 -776.695 -776.695], eps: 0.001})
Step:  189400, Reward: [-404.368 -404.368 -404.368] [78.719], Avg: [-409.949 -409.949 -409.949] (1.0000) ({r_i: None, r_t: [-804.305 -804.305 -804.305], eps: 1.0})
Step:   80000, Reward: [-391.098 -391.098 -391.098] [73.052], Avg: [-418.071 -418.071 -418.071] (0.0010) ({r_i: None, r_t: [-766.934 -766.934 -766.934], eps: 0.001})
Step:  189500, Reward: [-395.819 -395.819 -395.819] [47.771], Avg: [-409.941 -409.941 -409.941] (1.0000) ({r_i: None, r_t: [-796.420 -796.420 -796.420], eps: 1.0})
Step:   80100, Reward: [-372.349 -372.349 -372.349] [84.854], Avg: [-418.014 -418.014 -418.014] (0.0010) ({r_i: None, r_t: [-763.920 -763.920 -763.920], eps: 0.001})
Step:  189600, Reward: [-417.502 -417.502 -417.502] [89.492], Avg: [-409.945 -409.945 -409.945] (1.0000) ({r_i: None, r_t: [-803.192 -803.192 -803.192], eps: 1.0})
Step:   80200, Reward: [-378.496 -378.496 -378.496] [63.571], Avg: [-417.965 -417.965 -417.965] (0.0010) ({r_i: None, r_t: [-783.077 -783.077 -783.077], eps: 0.001})
Step:  189700, Reward: [-400.449 -400.449 -400.449] [82.684], Avg: [-409.940 -409.940 -409.940] (1.0000) ({r_i: None, r_t: [-795.691 -795.691 -795.691], eps: 1.0})
Step:   80300, Reward: [-415.230 -415.230 -415.230] [62.197], Avg: [-417.961 -417.961 -417.961] (0.0010) ({r_i: None, r_t: [-785.921 -785.921 -785.921], eps: 0.001})
Step:  189800, Reward: [-372.013 -372.013 -372.013] [53.939], Avg: [-409.920 -409.920 -409.920] (1.0000) ({r_i: None, r_t: [-802.756 -802.756 -802.756], eps: 1.0})
Step:   80400, Reward: [-398.858 -398.858 -398.858] [88.771], Avg: [-417.937 -417.937 -417.937] (0.0010) ({r_i: None, r_t: [-742.854 -742.854 -742.854], eps: 0.001})
Step:  189900, Reward: [-418.616 -418.616 -418.616] [86.431], Avg: [-409.925 -409.925 -409.925] (1.0000) ({r_i: None, r_t: [-799.654 -799.654 -799.654], eps: 1.0})
Step:   80500, Reward: [-344.101 -344.101 -344.101] [59.628], Avg: [-417.846 -417.846 -417.846] (0.0010) ({r_i: None, r_t: [-786.869 -786.869 -786.869], eps: 0.001})
Step:  190000, Reward: [-432.600 -432.600 -432.600] [83.671], Avg: [-409.937 -409.937 -409.937] (1.0000) ({r_i: None, r_t: [-832.047 -832.047 -832.047], eps: 1.0})
Step:   80600, Reward: [-370.088 -370.088 -370.088] [76.945], Avg: [-417.787 -417.787 -417.787] (0.0010) ({r_i: None, r_t: [-764.871 -764.871 -764.871], eps: 0.001})
Step:  190100, Reward: [-389.123 -389.123 -389.123] [56.457], Avg: [-409.926 -409.926 -409.926] (1.0000) ({r_i: None, r_t: [-811.927 -811.927 -811.927], eps: 1.0})
Step:   80700, Reward: [-338.144 -338.144 -338.144] [43.800], Avg: [-417.688 -417.688 -417.688] (0.0010) ({r_i: None, r_t: [-805.669 -805.669 -805.669], eps: 0.001})
Step:  190200, Reward: [-406.046 -406.046 -406.046] [85.544], Avg: [-409.924 -409.924 -409.924] (1.0000) ({r_i: None, r_t: [-816.432 -816.432 -816.432], eps: 1.0})
Step:   80800, Reward: [-431.585 -431.585 -431.585] [69.096], Avg: [-417.705 -417.705 -417.705] (0.0010) ({r_i: None, r_t: [-769.134 -769.134 -769.134], eps: 0.001})
Step:  190300, Reward: [-393.306 -393.306 -393.306] [54.985], Avg: [-409.915 -409.915 -409.915] (1.0000) ({r_i: None, r_t: [-795.849 -795.849 -795.849], eps: 1.0})
Step:   80900, Reward: [-381.612 -381.612 -381.612] [64.258], Avg: [-417.661 -417.661 -417.661] (0.0010) ({r_i: None, r_t: [-815.618 -815.618 -815.618], eps: 0.001})
Step:  190400, Reward: [-396.910 -396.910 -396.910] [65.772], Avg: [-409.908 -409.908 -409.908] (1.0000) ({r_i: None, r_t: [-800.444 -800.444 -800.444], eps: 1.0})
Step:   81000, Reward: [-419.354 -419.354 -419.354] [72.373], Avg: [-417.663 -417.663 -417.663] (0.0010) ({r_i: None, r_t: [-783.506 -783.506 -783.506], eps: 0.001})
Step:  190500, Reward: [-407.389 -407.389 -407.389] [87.022], Avg: [-409.907 -409.907 -409.907] (1.0000) ({r_i: None, r_t: [-771.974 -771.974 -771.974], eps: 1.0})
Step:   81100, Reward: [-398.725 -398.725 -398.725] [78.964], Avg: [-417.639 -417.639 -417.639] (0.0010) ({r_i: None, r_t: [-756.335 -756.335 -756.335], eps: 0.001})
Step:  190600, Reward: [-391.682 -391.682 -391.682] [74.408], Avg: [-409.897 -409.897 -409.897] (1.0000) ({r_i: None, r_t: [-844.423 -844.423 -844.423], eps: 1.0})
Step:   81200, Reward: [-398.922 -398.922 -398.922] [61.719], Avg: [-417.616 -417.616 -417.616] (0.0010) ({r_i: None, r_t: [-778.401 -778.401 -778.401], eps: 0.001})
Step:  190700, Reward: [-389.714 -389.714 -389.714] [73.802], Avg: [-409.887 -409.887 -409.887] (1.0000) ({r_i: None, r_t: [-822.629 -822.629 -822.629], eps: 1.0})
Step:   81300, Reward: [-383.588 -383.588 -383.588] [74.422], Avg: [-417.575 -417.575 -417.575] (0.0010) ({r_i: None, r_t: [-733.811 -733.811 -733.811], eps: 0.001})
Step:  190800, Reward: [-411.555 -411.555 -411.555] [77.390], Avg: [-409.888 -409.888 -409.888] (1.0000) ({r_i: None, r_t: [-785.208 -785.208 -785.208], eps: 1.0})
Step:   81400, Reward: [-384.171 -384.171 -384.171] [65.701], Avg: [-417.534 -417.534 -417.534] (0.0010) ({r_i: None, r_t: [-767.750 -767.750 -767.750], eps: 0.001})
Step:  190900, Reward: [-379.605 -379.605 -379.605] [91.817], Avg: [-409.872 -409.872 -409.872] (1.0000) ({r_i: None, r_t: [-786.380 -786.380 -786.380], eps: 1.0})
Step:   81500, Reward: [-405.792 -405.792 -405.792] [82.269], Avg: [-417.519 -417.519 -417.519] (0.0010) ({r_i: None, r_t: [-783.103 -783.103 -783.103], eps: 0.001})
Step:  191000, Reward: [-415.256 -415.256 -415.256] [80.117], Avg: [-409.874 -409.874 -409.874] (1.0000) ({r_i: None, r_t: [-837.642 -837.642 -837.642], eps: 1.0})
Step:   81600, Reward: [-421.562 -421.562 -421.562] [61.877], Avg: [-417.524 -417.524 -417.524] (0.0010) ({r_i: None, r_t: [-792.636 -792.636 -792.636], eps: 0.001})
Step:  191100, Reward: [-403.537 -403.537 -403.537] [61.722], Avg: [-409.871 -409.871 -409.871] (1.0000) ({r_i: None, r_t: [-843.417 -843.417 -843.417], eps: 1.0})
Step:   81700, Reward: [-384.270 -384.270 -384.270] [60.895], Avg: [-417.484 -417.484 -417.484] (0.0010) ({r_i: None, r_t: [-741.532 -741.532 -741.532], eps: 0.001})
Step:  191200, Reward: [-408.504 -408.504 -408.504] [80.340], Avg: [-409.870 -409.870 -409.870] (1.0000) ({r_i: None, r_t: [-816.567 -816.567 -816.567], eps: 1.0})
Step:   81800, Reward: [-406.155 -406.155 -406.155] [72.813], Avg: [-417.470 -417.470 -417.470] (0.0010) ({r_i: None, r_t: [-772.044 -772.044 -772.044], eps: 0.001})
Step:  191300, Reward: [-397.583 -397.583 -397.583] [77.874], Avg: [-409.864 -409.864 -409.864] (1.0000) ({r_i: None, r_t: [-777.157 -777.157 -777.157], eps: 1.0})
Step:   81900, Reward: [-405.623 -405.623 -405.623] [71.446], Avg: [-417.455 -417.455 -417.455] (0.0010) ({r_i: None, r_t: [-778.235 -778.235 -778.235], eps: 0.001})
Step:  191400, Reward: [-411.735 -411.735 -411.735] [68.111], Avg: [-409.865 -409.865 -409.865] (1.0000) ({r_i: None, r_t: [-802.783 -802.783 -802.783], eps: 1.0})
Step:   82000, Reward: [-394.759 -394.759 -394.759] [71.652], Avg: [-417.428 -417.428 -417.428] (0.0010) ({r_i: None, r_t: [-801.409 -801.409 -801.409], eps: 0.001})
Step:  191500, Reward: [-412.881 -412.881 -412.881] [85.311], Avg: [-409.867 -409.867 -409.867] (1.0000) ({r_i: None, r_t: [-794.614 -794.614 -794.614], eps: 1.0})
Step:   82100, Reward: [-393.756 -393.756 -393.756] [60.273], Avg: [-417.399 -417.399 -417.399] (0.0010) ({r_i: None, r_t: [-777.468 -777.468 -777.468], eps: 0.001})
Step:  191600, Reward: [-372.308 -372.308 -372.308] [65.038], Avg: [-409.847 -409.847 -409.847] (1.0000) ({r_i: None, r_t: [-809.952 -809.952 -809.952], eps: 1.0})
Step:   82200, Reward: [-393.214 -393.214 -393.214] [73.962], Avg: [-417.369 -417.369 -417.369] (0.0010) ({r_i: None, r_t: [-772.108 -772.108 -772.108], eps: 0.001})
Step:  191700, Reward: [-369.615 -369.615 -369.615] [50.186], Avg: [-409.826 -409.826 -409.826] (1.0000) ({r_i: None, r_t: [-788.818 -788.818 -788.818], eps: 1.0})
Step:   82300, Reward: [-373.087 -373.087 -373.087] [68.769], Avg: [-417.316 -417.316 -417.316] (0.0010) ({r_i: None, r_t: [-782.926 -782.926 -782.926], eps: 0.001})
Step:  191800, Reward: [-387.200 -387.200 -387.200] [80.643], Avg: [-409.814 -409.814 -409.814] (1.0000) ({r_i: None, r_t: [-820.290 -820.290 -820.290], eps: 1.0})
Step:   82400, Reward: [-396.583 -396.583 -396.583] [79.100], Avg: [-417.291 -417.291 -417.291] (0.0010) ({r_i: None, r_t: [-794.881 -794.881 -794.881], eps: 0.001})
Step:  191900, Reward: [-407.935 -407.935 -407.935] [55.727], Avg: [-409.813 -409.813 -409.813] (1.0000) ({r_i: None, r_t: [-801.733 -801.733 -801.733], eps: 1.0})
Step:   82500, Reward: [-401.085 -401.085 -401.085] [66.470], Avg: [-417.271 -417.271 -417.271] (0.0010) ({r_i: None, r_t: [-819.556 -819.556 -819.556], eps: 0.001})
Step:  192000, Reward: [-381.229 -381.229 -381.229] [53.800], Avg: [-409.798 -409.798 -409.798] (1.0000) ({r_i: None, r_t: [-862.192 -862.192 -862.192], eps: 1.0})
Step:   82600, Reward: [-378.996 -378.996 -378.996] [58.006], Avg: [-417.225 -417.225 -417.225] (0.0010) ({r_i: None, r_t: [-800.833 -800.833 -800.833], eps: 0.001})
Step:  192100, Reward: [-423.735 -423.735 -423.735] [67.104], Avg: [-409.806 -409.806 -409.806] (1.0000) ({r_i: None, r_t: [-840.244 -840.244 -840.244], eps: 1.0})
Step:   82700, Reward: [-389.162 -389.162 -389.162] [77.926], Avg: [-417.191 -417.191 -417.191] (0.0010) ({r_i: None, r_t: [-792.608 -792.608 -792.608], eps: 0.001})
Step:  192200, Reward: [-404.203 -404.203 -404.203] [95.535], Avg: [-409.803 -409.803 -409.803] (1.0000) ({r_i: None, r_t: [-834.191 -834.191 -834.191], eps: 1.0})
Step:   82800, Reward: [-368.297 -368.297 -368.297] [82.765], Avg: [-417.132 -417.132 -417.132] (0.0010) ({r_i: None, r_t: [-810.020 -810.020 -810.020], eps: 0.001})
Step:  192300, Reward: [-407.883 -407.883 -407.883] [101.757], Avg: [-409.802 -409.802 -409.802] (1.0000) ({r_i: None, r_t: [-849.203 -849.203 -849.203], eps: 1.0})
Step:   82900, Reward: [-346.044 -346.044 -346.044] [54.671], Avg: [-417.046 -417.046 -417.046] (0.0010) ({r_i: None, r_t: [-802.234 -802.234 -802.234], eps: 0.001})
Step:  192400, Reward: [-410.343 -410.343 -410.343] [89.182], Avg: [-409.802 -409.802 -409.802] (1.0000) ({r_i: None, r_t: [-823.647 -823.647 -823.647], eps: 1.0})
Step:   83000, Reward: [-384.167 -384.167 -384.167] [72.396], Avg: [-417.007 -417.007 -417.007] (0.0010) ({r_i: None, r_t: [-739.233 -739.233 -739.233], eps: 0.001})
Step:  192500, Reward: [-395.713 -395.713 -395.713] [84.887], Avg: [-409.795 -409.795 -409.795] (1.0000) ({r_i: None, r_t: [-835.988 -835.988 -835.988], eps: 1.0})
Step:   83100, Reward: [-405.350 -405.350 -405.350] [94.802], Avg: [-416.993 -416.993 -416.993] (0.0010) ({r_i: None, r_t: [-797.575 -797.575 -797.575], eps: 0.001})
Step:  192600, Reward: [-420.206 -420.206 -420.206] [102.388], Avg: [-409.800 -409.800 -409.800] (1.0000) ({r_i: None, r_t: [-801.345 -801.345 -801.345], eps: 1.0})
Step:   83200, Reward: [-426.219 -426.219 -426.219] [86.219], Avg: [-417.004 -417.004 -417.004] (0.0010) ({r_i: None, r_t: [-763.529 -763.529 -763.529], eps: 0.001})
Step:  192700, Reward: [-394.028 -394.028 -394.028] [82.155], Avg: [-409.792 -409.792 -409.792] (1.0000) ({r_i: None, r_t: [-835.243 -835.243 -835.243], eps: 1.0})
Step:   83300, Reward: [-411.986 -411.986 -411.986] [70.894], Avg: [-416.998 -416.998 -416.998] (0.0010) ({r_i: None, r_t: [-779.371 -779.371 -779.371], eps: 0.001})
Step:  192800, Reward: [-416.684 -416.684 -416.684] [86.936], Avg: [-409.795 -409.795 -409.795] (1.0000) ({r_i: None, r_t: [-816.900 -816.900 -816.900], eps: 1.0})
Step:   83400, Reward: [-395.760 -395.760 -395.760] [106.768], Avg: [-416.972 -416.972 -416.972] (0.0010) ({r_i: None, r_t: [-808.850 -808.850 -808.850], eps: 0.001})
Step:  192900, Reward: [-439.627 -439.627 -439.627] [65.515], Avg: [-409.811 -409.811 -409.811] (1.0000) ({r_i: None, r_t: [-808.096 -808.096 -808.096], eps: 1.0})
Step:   83500, Reward: [-404.087 -404.087 -404.087] [75.552], Avg: [-416.957 -416.957 -416.957] (0.0010) ({r_i: None, r_t: [-763.283 -763.283 -763.283], eps: 0.001})
Step:  193000, Reward: [-398.623 -398.623 -398.623] [63.170], Avg: [-409.805 -409.805 -409.805] (1.0000) ({r_i: None, r_t: [-772.230 -772.230 -772.230], eps: 1.0})
Step:   83600, Reward: [-366.885 -366.885 -366.885] [90.504], Avg: [-416.897 -416.897 -416.897] (0.0010) ({r_i: None, r_t: [-759.026 -759.026 -759.026], eps: 0.001})
Step:  193100, Reward: [-398.267 -398.267 -398.267] [86.121], Avg: [-409.799 -409.799 -409.799] (1.0000) ({r_i: None, r_t: [-815.579 -815.579 -815.579], eps: 1.0})
Step:   83700, Reward: [-358.874 -358.874 -358.874] [35.469], Avg: [-416.828 -416.828 -416.828] (0.0010) ({r_i: None, r_t: [-781.742 -781.742 -781.742], eps: 0.001})
Step:  193200, Reward: [-396.558 -396.558 -396.558] [61.056], Avg: [-409.792 -409.792 -409.792] (1.0000) ({r_i: None, r_t: [-775.188 -775.188 -775.188], eps: 1.0})
Step:   83800, Reward: [-364.273 -364.273 -364.273] [82.842], Avg: [-416.765 -416.765 -416.765] (0.0010) ({r_i: None, r_t: [-796.292 -796.292 -796.292], eps: 0.001})
Step:  193300, Reward: [-410.786 -410.786 -410.786] [72.381], Avg: [-409.793 -409.793 -409.793] (1.0000) ({r_i: None, r_t: [-834.086 -834.086 -834.086], eps: 1.0})
Step:   83900, Reward: [-408.279 -408.279 -408.279] [90.885], Avg: [-416.755 -416.755 -416.755] (0.0010) ({r_i: None, r_t: [-741.495 -741.495 -741.495], eps: 0.001})
Step:  193400, Reward: [-372.552 -372.552 -372.552] [48.397], Avg: [-409.774 -409.774 -409.774] (1.0000) ({r_i: None, r_t: [-843.338 -843.338 -843.338], eps: 1.0})
Step:   84000, Reward: [-381.016 -381.016 -381.016] [97.941], Avg: [-416.712 -416.712 -416.712] (0.0010) ({r_i: None, r_t: [-736.022 -736.022 -736.022], eps: 0.001})
Step:  193500, Reward: [-412.757 -412.757 -412.757] [109.509], Avg: [-409.775 -409.775 -409.775] (1.0000) ({r_i: None, r_t: [-821.584 -821.584 -821.584], eps: 1.0})
Step:   84100, Reward: [-396.199 -396.199 -396.199] [93.687], Avg: [-416.688 -416.688 -416.688] (0.0010) ({r_i: None, r_t: [-773.697 -773.697 -773.697], eps: 0.001})
Step:  193600, Reward: [-373.649 -373.649 -373.649] [89.115], Avg: [-409.756 -409.756 -409.756] (1.0000) ({r_i: None, r_t: [-903.970 -903.970 -903.970], eps: 1.0})
Step:   84200, Reward: [-380.643 -380.643 -380.643] [44.226], Avg: [-416.645 -416.645 -416.645] (0.0010) ({r_i: None, r_t: [-774.480 -774.480 -774.480], eps: 0.001})
Step:  193700, Reward: [-395.674 -395.674 -395.674] [78.173], Avg: [-409.749 -409.749 -409.749] (1.0000) ({r_i: None, r_t: [-778.753 -778.753 -778.753], eps: 1.0})
Step:   84300, Reward: [-414.288 -414.288 -414.288] [83.119], Avg: [-416.643 -416.643 -416.643] (0.0010) ({r_i: None, r_t: [-746.686 -746.686 -746.686], eps: 0.001})
Step:  193800, Reward: [-426.616 -426.616 -426.616] [90.169], Avg: [-409.758 -409.758 -409.758] (1.0000) ({r_i: None, r_t: [-790.835 -790.835 -790.835], eps: 1.0})
Step:   84400, Reward: [-394.068 -394.068 -394.068] [70.211], Avg: [-416.616 -416.616 -416.616] (0.0010) ({r_i: None, r_t: [-760.093 -760.093 -760.093], eps: 0.001})
Step:  193900, Reward: [-403.485 -403.485 -403.485] [80.344], Avg: [-409.755 -409.755 -409.755] (1.0000) ({r_i: None, r_t: [-772.823 -772.823 -772.823], eps: 1.0})
Step:   84500, Reward: [-390.895 -390.895 -390.895] [51.869], Avg: [-416.585 -416.585 -416.585] (0.0010) ({r_i: None, r_t: [-758.581 -758.581 -758.581], eps: 0.001})
Step:  194000, Reward: [-394.170 -394.170 -394.170] [63.155], Avg: [-409.747 -409.747 -409.747] (1.0000) ({r_i: None, r_t: [-757.525 -757.525 -757.525], eps: 1.0})
Step:   84600, Reward: [-361.708 -361.708 -361.708] [46.617], Avg: [-416.521 -416.521 -416.521] (0.0010) ({r_i: None, r_t: [-807.047 -807.047 -807.047], eps: 0.001})
Step:  194100, Reward: [-397.981 -397.981 -397.981] [68.726], Avg: [-409.741 -409.741 -409.741] (1.0000) ({r_i: None, r_t: [-850.405 -850.405 -850.405], eps: 1.0})
Step:   84700, Reward: [-406.274 -406.274 -406.274] [90.433], Avg: [-416.509 -416.509 -416.509] (0.0010) ({r_i: None, r_t: [-772.206 -772.206 -772.206], eps: 0.001})
Step:  194200, Reward: [-409.663 -409.663 -409.663] [97.373], Avg: [-409.741 -409.741 -409.741] (1.0000) ({r_i: None, r_t: [-806.539 -806.539 -806.539], eps: 1.0})
Step:   84800, Reward: [-395.580 -395.580 -395.580] [79.830], Avg: [-416.484 -416.484 -416.484] (0.0010) ({r_i: None, r_t: [-803.454 -803.454 -803.454], eps: 0.001})
Step:  194300, Reward: [-398.341 -398.341 -398.341] [92.600], Avg: [-409.735 -409.735 -409.735] (1.0000) ({r_i: None, r_t: [-814.109 -814.109 -814.109], eps: 1.0})
Step:   84900, Reward: [-410.631 -410.631 -410.631] [71.515], Avg: [-416.477 -416.477 -416.477] (0.0010) ({r_i: None, r_t: [-823.549 -823.549 -823.549], eps: 0.001})
Step:  194400, Reward: [-394.137 -394.137 -394.137] [61.352], Avg: [-409.727 -409.727 -409.727] (1.0000) ({r_i: None, r_t: [-784.525 -784.525 -784.525], eps: 1.0})
Step:   85000, Reward: [-401.462 -401.462 -401.462] [69.185], Avg: [-416.459 -416.459 -416.459] (0.0010) ({r_i: None, r_t: [-795.943 -795.943 -795.943], eps: 0.001})
Step:  194500, Reward: [-395.158 -395.158 -395.158] [60.910], Avg: [-409.719 -409.719 -409.719] (1.0000) ({r_i: None, r_t: [-826.208 -826.208 -826.208], eps: 1.0})
Step:   85100, Reward: [-407.479 -407.479 -407.479] [88.809], Avg: [-416.449 -416.449 -416.449] (0.0010) ({r_i: None, r_t: [-799.749 -799.749 -799.749], eps: 0.001})
Step:  194600, Reward: [-429.868 -429.868 -429.868] [84.976], Avg: [-409.730 -409.730 -409.730] (1.0000) ({r_i: None, r_t: [-769.211 -769.211 -769.211], eps: 1.0})
Step:   85200, Reward: [-386.126 -386.126 -386.126] [94.071], Avg: [-416.413 -416.413 -416.413] (0.0010) ({r_i: None, r_t: [-812.349 -812.349 -812.349], eps: 0.001})
Step:  194700, Reward: [-384.270 -384.270 -384.270] [67.800], Avg: [-409.716 -409.716 -409.716] (1.0000) ({r_i: None, r_t: [-855.620 -855.620 -855.620], eps: 1.0})
Step:   85300, Reward: [-386.082 -386.082 -386.082] [65.157], Avg: [-416.378 -416.378 -416.378] (0.0010) ({r_i: None, r_t: [-762.361 -762.361 -762.361], eps: 0.001})
Step:  194800, Reward: [-395.671 -395.671 -395.671] [71.276], Avg: [-409.709 -409.709 -409.709] (1.0000) ({r_i: None, r_t: [-864.506 -864.506 -864.506], eps: 1.0})
Step:   85400, Reward: [-439.804 -439.804 -439.804] [123.371], Avg: [-416.405 -416.405 -416.405] (0.0010) ({r_i: None, r_t: [-780.603 -780.603 -780.603], eps: 0.001})
Step:  194900, Reward: [-401.242 -401.242 -401.242] [80.975], Avg: [-409.705 -409.705 -409.705] (1.0000) ({r_i: None, r_t: [-792.283 -792.283 -792.283], eps: 1.0})
Step:   85500, Reward: [-386.487 -386.487 -386.487] [66.375], Avg: [-416.370 -416.370 -416.370] (0.0010) ({r_i: None, r_t: [-786.563 -786.563 -786.563], eps: 0.001})
Step:  195000, Reward: [-401.516 -401.516 -401.516] [76.713], Avg: [-409.701 -409.701 -409.701] (1.0000) ({r_i: None, r_t: [-872.865 -872.865 -872.865], eps: 1.0})
Step:   85600, Reward: [-399.633 -399.633 -399.633] [62.968], Avg: [-416.351 -416.351 -416.351] (0.0010) ({r_i: None, r_t: [-791.880 -791.880 -791.880], eps: 0.001})
Step:  195100, Reward: [-410.068 -410.068 -410.068] [49.913], Avg: [-409.701 -409.701 -409.701] (1.0000) ({r_i: None, r_t: [-842.645 -842.645 -842.645], eps: 1.0})
Step:   85700, Reward: [-405.634 -405.634 -405.634] [68.938], Avg: [-416.338 -416.338 -416.338] (0.0010) ({r_i: None, r_t: [-820.222 -820.222 -820.222], eps: 0.001})
Step:  195200, Reward: [-390.744 -390.744 -390.744] [86.230], Avg: [-409.691 -409.691 -409.691] (1.0000) ({r_i: None, r_t: [-813.330 -813.330 -813.330], eps: 1.0})
Step:   85800, Reward: [-422.939 -422.939 -422.939] [92.743], Avg: [-416.346 -416.346 -416.346] (0.0010) ({r_i: None, r_t: [-801.860 -801.860 -801.860], eps: 0.001})
Step:  195300, Reward: [-395.689 -395.689 -395.689] [71.352], Avg: [-409.684 -409.684 -409.684] (1.0000) ({r_i: None, r_t: [-784.216 -784.216 -784.216], eps: 1.0})
Step:   85900, Reward: [-418.878 -418.878 -418.878] [73.657], Avg: [-416.349 -416.349 -416.349] (0.0010) ({r_i: None, r_t: [-808.657 -808.657 -808.657], eps: 0.001})
Step:  195400, Reward: [-407.416 -407.416 -407.416] [76.249], Avg: [-409.683 -409.683 -409.683] (1.0000) ({r_i: None, r_t: [-798.200 -798.200 -798.200], eps: 1.0})
Step:   86000, Reward: [-367.928 -367.928 -367.928] [46.394], Avg: [-416.293 -416.293 -416.293] (0.0010) ({r_i: None, r_t: [-787.499 -787.499 -787.499], eps: 0.001})
Step:  195500, Reward: [-392.007 -392.007 -392.007] [107.213], Avg: [-409.674 -409.674 -409.674] (1.0000) ({r_i: None, r_t: [-827.839 -827.839 -827.839], eps: 1.0})
Step:   86100, Reward: [-374.877 -374.877 -374.877] [45.715], Avg: [-416.245 -416.245 -416.245] (0.0010) ({r_i: None, r_t: [-790.825 -790.825 -790.825], eps: 0.001})
Step:  195600, Reward: [-438.437 -438.437 -438.437] [92.015], Avg: [-409.689 -409.689 -409.689] (1.0000) ({r_i: None, r_t: [-813.760 -813.760 -813.760], eps: 1.0})
Step:   86200, Reward: [-397.515 -397.515 -397.515] [57.951], Avg: [-416.223 -416.223 -416.223] (0.0010) ({r_i: None, r_t: [-816.736 -816.736 -816.736], eps: 0.001})
Step:  195700, Reward: [-366.394 -366.394 -366.394] [45.879], Avg: [-409.666 -409.666 -409.666] (1.0000) ({r_i: None, r_t: [-802.868 -802.868 -802.868], eps: 1.0})
Step:   86300, Reward: [-410.169 -410.169 -410.169] [73.890], Avg: [-416.216 -416.216 -416.216] (0.0010) ({r_i: None, r_t: [-773.025 -773.025 -773.025], eps: 0.001})
Step:  195800, Reward: [-410.561 -410.561 -410.561] [54.466], Avg: [-409.667 -409.667 -409.667] (1.0000) ({r_i: None, r_t: [-886.501 -886.501 -886.501], eps: 1.0})
Step:   86400, Reward: [-453.332 -453.332 -453.332] [106.410], Avg: [-416.259 -416.259 -416.259] (0.0010) ({r_i: None, r_t: [-818.908 -818.908 -818.908], eps: 0.001})
Step:  195900, Reward: [-392.564 -392.564 -392.564] [61.947], Avg: [-409.658 -409.658 -409.658] (1.0000) ({r_i: None, r_t: [-797.284 -797.284 -797.284], eps: 1.0})
Step:   86500, Reward: [-394.037 -394.037 -394.037] [55.115], Avg: [-416.233 -416.233 -416.233] (0.0010) ({r_i: None, r_t: [-843.071 -843.071 -843.071], eps: 0.001})
Step:  196000, Reward: [-415.037 -415.037 -415.037] [74.834], Avg: [-409.661 -409.661 -409.661] (1.0000) ({r_i: None, r_t: [-783.232 -783.232 -783.232], eps: 1.0})
Step:   86600, Reward: [-384.283 -384.283 -384.283] [95.823], Avg: [-416.196 -416.196 -416.196] (0.0010) ({r_i: None, r_t: [-803.081 -803.081 -803.081], eps: 0.001})
Step:  196100, Reward: [-415.588 -415.588 -415.588] [75.794], Avg: [-409.664 -409.664 -409.664] (1.0000) ({r_i: None, r_t: [-740.652 -740.652 -740.652], eps: 1.0})
Step:   86700, Reward: [-404.257 -404.257 -404.257] [92.967], Avg: [-416.182 -416.182 -416.182] (0.0010) ({r_i: None, r_t: [-838.346 -838.346 -838.346], eps: 0.001})
Step:  196200, Reward: [-437.441 -437.441 -437.441] [76.129], Avg: [-409.678 -409.678 -409.678] (1.0000) ({r_i: None, r_t: [-787.038 -787.038 -787.038], eps: 1.0})
Step:   86800, Reward: [-397.391 -397.391 -397.391] [50.952], Avg: [-416.161 -416.161 -416.161] (0.0010) ({r_i: None, r_t: [-794.210 -794.210 -794.210], eps: 0.001})
Step:  196300, Reward: [-406.973 -406.973 -406.973] [114.801], Avg: [-409.677 -409.677 -409.677] (1.0000) ({r_i: None, r_t: [-823.374 -823.374 -823.374], eps: 1.0})
Step:   86900, Reward: [-390.713 -390.713 -390.713] [67.169], Avg: [-416.132 -416.132 -416.132] (0.0010) ({r_i: None, r_t: [-796.460 -796.460 -796.460], eps: 0.001})
Step:  196400, Reward: [-393.476 -393.476 -393.476] [82.172], Avg: [-409.668 -409.668 -409.668] (1.0000) ({r_i: None, r_t: [-825.815 -825.815 -825.815], eps: 1.0})
Step:   87000, Reward: [-386.115 -386.115 -386.115] [46.339], Avg: [-416.097 -416.097 -416.097] (0.0010) ({r_i: None, r_t: [-805.934 -805.934 -805.934], eps: 0.001})
Step:  196500, Reward: [-425.660 -425.660 -425.660] [57.933], Avg: [-409.677 -409.677 -409.677] (1.0000) ({r_i: None, r_t: [-773.741 -773.741 -773.741], eps: 1.0})
Step:   87100, Reward: [-425.930 -425.930 -425.930] [82.182], Avg: [-416.108 -416.108 -416.108] (0.0010) ({r_i: None, r_t: [-805.646 -805.646 -805.646], eps: 0.001})
Step:  196600, Reward: [-380.010 -380.010 -380.010] [77.854], Avg: [-409.661 -409.661 -409.661] (1.0000) ({r_i: None, r_t: [-783.884 -783.884 -783.884], eps: 1.0})
Step:   87200, Reward: [-413.971 -413.971 -413.971] [84.326], Avg: [-416.106 -416.106 -416.106] (0.0010) ({r_i: None, r_t: [-815.815 -815.815 -815.815], eps: 0.001})
Step:  196700, Reward: [-428.460 -428.460 -428.460] [86.206], Avg: [-409.671 -409.671 -409.671] (1.0000) ({r_i: None, r_t: [-752.900 -752.900 -752.900], eps: 1.0})
Step:   87300, Reward: [-413.305 -413.305 -413.305] [68.509], Avg: [-416.103 -416.103 -416.103] (0.0010) ({r_i: None, r_t: [-843.542 -843.542 -843.542], eps: 0.001})
Step:  196800, Reward: [-413.495 -413.495 -413.495] [100.081], Avg: [-409.673 -409.673 -409.673] (1.0000) ({r_i: None, r_t: [-830.769 -830.769 -830.769], eps: 1.0})
Step:   87400, Reward: [-394.576 -394.576 -394.576] [42.747], Avg: [-416.078 -416.078 -416.078] (0.0010) ({r_i: None, r_t: [-782.127 -782.127 -782.127], eps: 0.001})
Step:  196900, Reward: [-394.721 -394.721 -394.721] [82.496], Avg: [-409.665 -409.665 -409.665] (1.0000) ({r_i: None, r_t: [-784.244 -784.244 -784.244], eps: 1.0})
Step:   87500, Reward: [-395.137 -395.137 -395.137] [91.009], Avg: [-416.054 -416.054 -416.054] (0.0010) ({r_i: None, r_t: [-799.891 -799.891 -799.891], eps: 0.001})
Step:  197000, Reward: [-436.050 -436.050 -436.050] [98.958], Avg: [-409.679 -409.679 -409.679] (1.0000) ({r_i: None, r_t: [-814.562 -814.562 -814.562], eps: 1.0})
Step:   87600, Reward: [-385.893 -385.893 -385.893] [56.080], Avg: [-416.020 -416.020 -416.020] (0.0010) ({r_i: None, r_t: [-805.375 -805.375 -805.375], eps: 0.001})
Step:  197100, Reward: [-372.283 -372.283 -372.283] [56.027], Avg: [-409.660 -409.660 -409.660] (1.0000) ({r_i: None, r_t: [-814.513 -814.513 -814.513], eps: 1.0})
Step:   87700, Reward: [-388.980 -388.980 -388.980] [46.030], Avg: [-415.989 -415.989 -415.989] (0.0010) ({r_i: None, r_t: [-793.702 -793.702 -793.702], eps: 0.001})
Step:  197200, Reward: [-410.837 -410.837 -410.837] [84.872], Avg: [-409.660 -409.660 -409.660] (1.0000) ({r_i: None, r_t: [-793.045 -793.045 -793.045], eps: 1.0})
Step:   87800, Reward: [-409.146 -409.146 -409.146] [70.158], Avg: [-415.981 -415.981 -415.981] (0.0010) ({r_i: None, r_t: [-805.131 -805.131 -805.131], eps: 0.001})
Step:  197300, Reward: [-420.146 -420.146 -420.146] [99.768], Avg: [-409.666 -409.666 -409.666] (1.0000) ({r_i: None, r_t: [-790.929 -790.929 -790.929], eps: 1.0})
Step:   87900, Reward: [-410.497 -410.497 -410.497] [54.827], Avg: [-415.975 -415.975 -415.975] (0.0010) ({r_i: None, r_t: [-804.901 -804.901 -804.901], eps: 0.001})
Step:  197400, Reward: [-459.156 -459.156 -459.156] [104.059], Avg: [-409.691 -409.691 -409.691] (1.0000) ({r_i: None, r_t: [-800.997 -800.997 -800.997], eps: 1.0})
Step:   88000, Reward: [-403.157 -403.157 -403.157] [56.494], Avg: [-415.960 -415.960 -415.960] (0.0010) ({r_i: None, r_t: [-795.825 -795.825 -795.825], eps: 0.001})
Step:  197500, Reward: [-418.658 -418.658 -418.658] [63.752], Avg: [-409.695 -409.695 -409.695] (1.0000) ({r_i: None, r_t: [-840.986 -840.986 -840.986], eps: 1.0})
Step:   88100, Reward: [-398.792 -398.792 -398.792] [69.379], Avg: [-415.941 -415.941 -415.941] (0.0010) ({r_i: None, r_t: [-829.042 -829.042 -829.042], eps: 0.001})
Step:  197600, Reward: [-409.918 -409.918 -409.918] [70.471], Avg: [-409.695 -409.695 -409.695] (1.0000) ({r_i: None, r_t: [-809.670 -809.670 -809.670], eps: 1.0})
Step:   88200, Reward: [-400.062 -400.062 -400.062] [76.040], Avg: [-415.923 -415.923 -415.923] (0.0010) ({r_i: None, r_t: [-751.177 -751.177 -751.177], eps: 0.001})
Step:  197700, Reward: [-359.413 -359.413 -359.413] [58.761], Avg: [-409.670 -409.670 -409.670] (1.0000) ({r_i: None, r_t: [-787.189 -787.189 -787.189], eps: 1.0})
Step:  197800, Reward: [-433.789 -433.789 -433.789] [61.037], Avg: [-409.682 -409.682 -409.682] (1.0000) ({r_i: None, r_t: [-796.237 -796.237 -796.237], eps: 1.0})
Step:   88300, Reward: [-379.654 -379.654 -379.654] [63.128], Avg: [-415.882 -415.882 -415.882] (0.0010) ({r_i: None, r_t: [-815.219 -815.219 -815.219], eps: 0.001})
Step:  197900, Reward: [-413.237 -413.237 -413.237] [103.574], Avg: [-409.684 -409.684 -409.684] (1.0000) ({r_i: None, r_t: [-821.109 -821.109 -821.109], eps: 1.0})
Step:   88400, Reward: [-371.339 -371.339 -371.339] [60.918], Avg: [-415.832 -415.832 -415.832] (0.0010) ({r_i: None, r_t: [-778.235 -778.235 -778.235], eps: 0.001})
Step:  198000, Reward: [-439.840 -439.840 -439.840] [65.877], Avg: [-409.699 -409.699 -409.699] (1.0000) ({r_i: None, r_t: [-779.861 -779.861 -779.861], eps: 1.0})
Step:   88500, Reward: [-400.457 -400.457 -400.457] [89.090], Avg: [-415.814 -415.814 -415.814] (0.0010) ({r_i: None, r_t: [-826.510 -826.510 -826.510], eps: 0.001})
Step:  198100, Reward: [-394.678 -394.678 -394.678] [80.306], Avg: [-409.692 -409.692 -409.692] (1.0000) ({r_i: None, r_t: [-783.920 -783.920 -783.920], eps: 1.0})
Step:   88600, Reward: [-430.113 -430.113 -430.113] [93.906], Avg: [-415.830 -415.830 -415.830] (0.0010) ({r_i: None, r_t: [-790.287 -790.287 -790.287], eps: 0.001})
Step:  198200, Reward: [-397.407 -397.407 -397.407] [99.477], Avg: [-409.685 -409.685 -409.685] (1.0000) ({r_i: None, r_t: [-810.847 -810.847 -810.847], eps: 1.0})
Step:   88700, Reward: [-399.089 -399.089 -399.089] [61.946], Avg: [-415.812 -415.812 -415.812] (0.0010) ({r_i: None, r_t: [-820.537 -820.537 -820.537], eps: 0.001})
Step:  198300, Reward: [-379.927 -379.927 -379.927] [58.990], Avg: [-409.670 -409.670 -409.670] (1.0000) ({r_i: None, r_t: [-802.466 -802.466 -802.466], eps: 1.0})
Step:   88800, Reward: [-388.900 -388.900 -388.900] [60.667], Avg: [-415.781 -415.781 -415.781] (0.0010) ({r_i: None, r_t: [-782.108 -782.108 -782.108], eps: 0.001})
Step:  198400, Reward: [-408.461 -408.461 -408.461] [58.989], Avg: [-409.670 -409.670 -409.670] (1.0000) ({r_i: None, r_t: [-810.971 -810.971 -810.971], eps: 1.0})
Step:   88900, Reward: [-389.011 -389.011 -389.011] [72.530], Avg: [-415.751 -415.751 -415.751] (0.0010) ({r_i: None, r_t: [-803.856 -803.856 -803.856], eps: 0.001})
Step:  198500, Reward: [-378.762 -378.762 -378.762] [60.220], Avg: [-409.654 -409.654 -409.654] (1.0000) ({r_i: None, r_t: [-774.220 -774.220 -774.220], eps: 1.0})
Step:   89000, Reward: [-415.260 -415.260 -415.260] [105.253], Avg: [-415.751 -415.751 -415.751] (0.0010) ({r_i: None, r_t: [-816.305 -816.305 -816.305], eps: 0.001})
Step:  198600, Reward: [-417.969 -417.969 -417.969] [78.636], Avg: [-409.658 -409.658 -409.658] (1.0000) ({r_i: None, r_t: [-788.014 -788.014 -788.014], eps: 1.0})
Step:   89100, Reward: [-414.435 -414.435 -414.435] [71.640], Avg: [-415.749 -415.749 -415.749] (0.0010) ({r_i: None, r_t: [-790.560 -790.560 -790.560], eps: 0.001})
Step:  198700, Reward: [-408.046 -408.046 -408.046] [79.255], Avg: [-409.658 -409.658 -409.658] (1.0000) ({r_i: None, r_t: [-763.699 -763.699 -763.699], eps: 1.0})
Step:   89200, Reward: [-424.785 -424.785 -424.785] [77.209], Avg: [-415.759 -415.759 -415.759] (0.0010) ({r_i: None, r_t: [-761.683 -761.683 -761.683], eps: 0.001})
Step:  198800, Reward: [-414.042 -414.042 -414.042] [75.941], Avg: [-409.660 -409.660 -409.660] (1.0000) ({r_i: None, r_t: [-813.525 -813.525 -813.525], eps: 1.0})
Step:   89300, Reward: [-383.562 -383.562 -383.562] [60.761], Avg: [-415.723 -415.723 -415.723] (0.0010) ({r_i: None, r_t: [-768.170 -768.170 -768.170], eps: 0.001})
Step:  198900, Reward: [-403.339 -403.339 -403.339] [65.681], Avg: [-409.657 -409.657 -409.657] (1.0000) ({r_i: None, r_t: [-780.566 -780.566 -780.566], eps: 1.0})
Step:   89400, Reward: [-386.632 -386.632 -386.632] [74.492], Avg: [-415.691 -415.691 -415.691] (0.0010) ({r_i: None, r_t: [-760.463 -760.463 -760.463], eps: 0.001})
Step:  199000, Reward: [-403.937 -403.937 -403.937] [63.633], Avg: [-409.654 -409.654 -409.654] (1.0000) ({r_i: None, r_t: [-830.497 -830.497 -830.497], eps: 1.0})
Step:   89500, Reward: [-413.113 -413.113 -413.113] [77.714], Avg: [-415.688 -415.688 -415.688] (0.0010) ({r_i: None, r_t: [-745.084 -745.084 -745.084], eps: 0.001})
Step:  199100, Reward: [-440.417 -440.417 -440.417] [79.727], Avg: [-409.669 -409.669 -409.669] (1.0000) ({r_i: None, r_t: [-862.117 -862.117 -862.117], eps: 1.0})
Step:   89600, Reward: [-355.812 -355.812 -355.812] [61.826], Avg: [-415.621 -415.621 -415.621] (0.0010) ({r_i: None, r_t: [-803.724 -803.724 -803.724], eps: 0.001})
Step:  199200, Reward: [-401.054 -401.054 -401.054] [72.580], Avg: [-409.665 -409.665 -409.665] (1.0000) ({r_i: None, r_t: [-829.788 -829.788 -829.788], eps: 1.0})
Step:   89700, Reward: [-407.116 -407.116 -407.116] [68.521], Avg: [-415.612 -415.612 -415.612] (0.0010) ({r_i: None, r_t: [-755.829 -755.829 -755.829], eps: 0.001})
Step:  199300, Reward: [-429.292 -429.292 -429.292] [75.379], Avg: [-409.675 -409.675 -409.675] (1.0000) ({r_i: None, r_t: [-840.636 -840.636 -840.636], eps: 1.0})
Step:   89800, Reward: [-382.293 -382.293 -382.293] [75.686], Avg: [-415.575 -415.575 -415.575] (0.0010) ({r_i: None, r_t: [-776.482 -776.482 -776.482], eps: 0.001})
Step:  199400, Reward: [-401.869 -401.869 -401.869] [79.968], Avg: [-409.671 -409.671 -409.671] (1.0000) ({r_i: None, r_t: [-804.674 -804.674 -804.674], eps: 1.0})
Step:   89900, Reward: [-337.888 -337.888 -337.888] [56.448], Avg: [-415.488 -415.488 -415.488] (0.0010) ({r_i: None, r_t: [-792.493 -792.493 -792.493], eps: 0.001})
Step:  199500, Reward: [-347.057 -347.057 -347.057] [53.961], Avg: [-409.639 -409.639 -409.639] (1.0000) ({r_i: None, r_t: [-744.812 -744.812 -744.812], eps: 1.0})
Step:   90000, Reward: [-377.273 -377.273 -377.273] [57.348], Avg: [-415.446 -415.446 -415.446] (0.0010) ({r_i: None, r_t: [-798.309 -798.309 -798.309], eps: 0.001})
Step:  199600, Reward: [-383.341 -383.341 -383.341] [67.323], Avg: [-409.626 -409.626 -409.626] (1.0000) ({r_i: None, r_t: [-847.090 -847.090 -847.090], eps: 1.0})
Step:   90100, Reward: [-403.386 -403.386 -403.386] [66.361], Avg: [-415.433 -415.433 -415.433] (0.0010) ({r_i: None, r_t: [-743.397 -743.397 -743.397], eps: 0.001})
Step:  199700, Reward: [-403.571 -403.571 -403.571] [68.434], Avg: [-409.623 -409.623 -409.623] (1.0000) ({r_i: None, r_t: [-777.125 -777.125 -777.125], eps: 1.0})
Step:   90200, Reward: [-431.106 -431.106 -431.106] [101.529], Avg: [-415.450 -415.450 -415.450] (0.0010) ({r_i: None, r_t: [-815.065 -815.065 -815.065], eps: 0.001})
Step:  199800, Reward: [-426.787 -426.787 -426.787] [96.026], Avg: [-409.632 -409.632 -409.632] (1.0000) ({r_i: None, r_t: [-808.049 -808.049 -808.049], eps: 1.0})
Step:   90300, Reward: [-407.631 -407.631 -407.631] [69.094], Avg: [-415.441 -415.441 -415.441] (0.0010) ({r_i: None, r_t: [-776.422 -776.422 -776.422], eps: 0.001})
Step:  199900, Reward: [-422.242 -422.242 -422.242] [89.351], Avg: [-409.638 -409.638 -409.638] (1.0000) ({r_i: None, r_t: [-824.367 -824.367 -824.367], eps: 1.0})
Step:   90400, Reward: [-399.345 -399.345 -399.345] [62.937], Avg: [-415.423 -415.423 -415.423] (0.0010) ({r_i: None, r_t: [-805.901 -805.901 -805.901], eps: 0.001})
Step:  200000, Reward: [-414.821 -414.821 -414.821] [88.399], Avg: [-409.641 -409.641 -409.641] (1.0000) ({r_i: None, r_t: [-842.509 -842.509 -842.509], eps: 1.0})
Step:   90500, Reward: [-420.076 -420.076 -420.076] [80.868], Avg: [-415.429 -415.429 -415.429] (0.0010) ({r_i: None, r_t: [-849.046 -849.046 -849.046], eps: 0.001})
Step:   90600, Reward: [-374.564 -374.564 -374.564] [55.002], Avg: [-415.384 -415.384 -415.384] (0.0010) ({r_i: None, r_t: [-778.182 -778.182 -778.182], eps: 0.001})
Step:   90700, Reward: [-406.136 -406.136 -406.136] [73.840], Avg: [-415.373 -415.373 -415.373] (0.0010) ({r_i: None, r_t: [-820.630 -820.630 -820.630], eps: 0.001})
Step:   90800, Reward: [-387.268 -387.268 -387.268] [56.348], Avg: [-415.342 -415.342 -415.342] (0.0010) ({r_i: None, r_t: [-773.978 -773.978 -773.978], eps: 0.001})
Step:   90900, Reward: [-354.822 -354.822 -354.822] [64.148], Avg: [-415.276 -415.276 -415.276] (0.0010) ({r_i: None, r_t: [-842.605 -842.605 -842.605], eps: 0.001})
Step:   91000, Reward: [-383.270 -383.270 -383.270] [76.314], Avg: [-415.241 -415.241 -415.241] (0.0010) ({r_i: None, r_t: [-754.382 -754.382 -754.382], eps: 0.001})
Step:   91100, Reward: [-417.698 -417.698 -417.698] [84.627], Avg: [-415.244 -415.244 -415.244] (0.0010) ({r_i: None, r_t: [-825.544 -825.544 -825.544], eps: 0.001})
Step:   91200, Reward: [-383.741 -383.741 -383.741] [57.042], Avg: [-415.209 -415.209 -415.209] (0.0010) ({r_i: None, r_t: [-771.677 -771.677 -771.677], eps: 0.001})
Step:   91300, Reward: [-380.686 -380.686 -380.686] [68.984], Avg: [-415.171 -415.171 -415.171] (0.0010) ({r_i: None, r_t: [-764.873 -764.873 -764.873], eps: 0.001})
Step:   91400, Reward: [-420.955 -420.955 -420.955] [83.759], Avg: [-415.178 -415.178 -415.178] (0.0010) ({r_i: None, r_t: [-786.540 -786.540 -786.540], eps: 0.001})
Step:   91500, Reward: [-390.105 -390.105 -390.105] [71.907], Avg: [-415.150 -415.150 -415.150] (0.0010) ({r_i: None, r_t: [-794.221 -794.221 -794.221], eps: 0.001})
Step:   91600, Reward: [-374.619 -374.619 -374.619] [88.345], Avg: [-415.106 -415.106 -415.106] (0.0010) ({r_i: None, r_t: [-801.004 -801.004 -801.004], eps: 0.001})
Step:   91700, Reward: [-397.620 -397.620 -397.620] [81.887], Avg: [-415.087 -415.087 -415.087] (0.0010) ({r_i: None, r_t: [-757.365 -757.365 -757.365], eps: 0.001})
Step:   91800, Reward: [-382.974 -382.974 -382.974] [86.649], Avg: [-415.052 -415.052 -415.052] (0.0010) ({r_i: None, r_t: [-785.870 -785.870 -785.870], eps: 0.001})
Step:   91900, Reward: [-381.968 -381.968 -381.968] [52.558], Avg: [-415.016 -415.016 -415.016] (0.0010) ({r_i: None, r_t: [-777.637 -777.637 -777.637], eps: 0.001})
Step:   92000, Reward: [-404.716 -404.716 -404.716] [84.111], Avg: [-415.005 -415.005 -415.005] (0.0010) ({r_i: None, r_t: [-751.470 -751.470 -751.470], eps: 0.001})
Step:   92100, Reward: [-399.894 -399.894 -399.894] [64.294], Avg: [-414.988 -414.988 -414.988] (0.0010) ({r_i: None, r_t: [-767.060 -767.060 -767.060], eps: 0.001})
Step:   92200, Reward: [-360.064 -360.064 -360.064] [70.980], Avg: [-414.929 -414.929 -414.929] (0.0010) ({r_i: None, r_t: [-772.692 -772.692 -772.692], eps: 0.001})
Step:   92300, Reward: [-364.377 -364.377 -364.377] [44.055], Avg: [-414.874 -414.874 -414.874] (0.0010) ({r_i: None, r_t: [-784.531 -784.531 -784.531], eps: 0.001})
Step:   92400, Reward: [-392.717 -392.717 -392.717] [53.232], Avg: [-414.850 -414.850 -414.850] (0.0010) ({r_i: None, r_t: [-743.424 -743.424 -743.424], eps: 0.001})
Step:   92500, Reward: [-386.751 -386.751 -386.751] [45.252], Avg: [-414.820 -414.820 -414.820] (0.0010) ({r_i: None, r_t: [-795.848 -795.848 -795.848], eps: 0.001})
Step:   92600, Reward: [-396.442 -396.442 -396.442] [54.344], Avg: [-414.800 -414.800 -414.800] (0.0010) ({r_i: None, r_t: [-737.789 -737.789 -737.789], eps: 0.001})
Step:   92700, Reward: [-405.199 -405.199 -405.199] [62.276], Avg: [-414.790 -414.790 -414.790] (0.0010) ({r_i: None, r_t: [-838.707 -838.707 -838.707], eps: 0.001})
Step:   92800, Reward: [-360.363 -360.363 -360.363] [72.283], Avg: [-414.731 -414.731 -414.731] (0.0010) ({r_i: None, r_t: [-768.527 -768.527 -768.527], eps: 0.001})
Step:   92900, Reward: [-378.606 -378.606 -378.606] [48.563], Avg: [-414.692 -414.692 -414.692] (0.0010) ({r_i: None, r_t: [-747.994 -747.994 -747.994], eps: 0.001})
Step:   93000, Reward: [-429.224 -429.224 -429.224] [120.872], Avg: [-414.708 -414.708 -414.708] (0.0010) ({r_i: None, r_t: [-804.145 -804.145 -804.145], eps: 0.001})
Step:   93100, Reward: [-389.321 -389.321 -389.321] [69.909], Avg: [-414.681 -414.681 -414.681] (0.0010) ({r_i: None, r_t: [-840.746 -840.746 -840.746], eps: 0.001})
Step:   93200, Reward: [-378.756 -378.756 -378.756] [63.967], Avg: [-414.642 -414.642 -414.642] (0.0010) ({r_i: None, r_t: [-805.059 -805.059 -805.059], eps: 0.001})
Step:   93300, Reward: [-393.880 -393.880 -393.880] [54.950], Avg: [-414.620 -414.620 -414.620] (0.0010) ({r_i: None, r_t: [-753.153 -753.153 -753.153], eps: 0.001})
Step:   93400, Reward: [-381.091 -381.091 -381.091] [54.108], Avg: [-414.584 -414.584 -414.584] (0.0010) ({r_i: None, r_t: [-754.948 -754.948 -754.948], eps: 0.001})
Step:   93500, Reward: [-408.558 -408.558 -408.558] [80.832], Avg: [-414.578 -414.578 -414.578] (0.0010) ({r_i: None, r_t: [-791.037 -791.037 -791.037], eps: 0.001})
Step:   93600, Reward: [-402.868 -402.868 -402.868] [91.585], Avg: [-414.565 -414.565 -414.565] (0.0010) ({r_i: None, r_t: [-756.907 -756.907 -756.907], eps: 0.001})
Step:   93700, Reward: [-395.707 -395.707 -395.707] [60.347], Avg: [-414.545 -414.545 -414.545] (0.0010) ({r_i: None, r_t: [-789.392 -789.392 -789.392], eps: 0.001})
Step:   93800, Reward: [-403.090 -403.090 -403.090] [67.901], Avg: [-414.533 -414.533 -414.533] (0.0010) ({r_i: None, r_t: [-779.670 -779.670 -779.670], eps: 0.001})
Step:   93900, Reward: [-394.814 -394.814 -394.814] [83.470], Avg: [-414.512 -414.512 -414.512] (0.0010) ({r_i: None, r_t: [-833.769 -833.769 -833.769], eps: 0.001})
Step:   94000, Reward: [-392.839 -392.839 -392.839] [48.921], Avg: [-414.489 -414.489 -414.489] (0.0010) ({r_i: None, r_t: [-773.936 -773.936 -773.936], eps: 0.001})
Step:   94100, Reward: [-413.095 -413.095 -413.095] [91.765], Avg: [-414.487 -414.487 -414.487] (0.0010) ({r_i: None, r_t: [-789.855 -789.855 -789.855], eps: 0.001})
Step:   94200, Reward: [-370.222 -370.222 -370.222] [61.958], Avg: [-414.440 -414.440 -414.440] (0.0010) ({r_i: None, r_t: [-807.025 -807.025 -807.025], eps: 0.001})
Step:   94300, Reward: [-414.097 -414.097 -414.097] [92.290], Avg: [-414.440 -414.440 -414.440] (0.0010) ({r_i: None, r_t: [-783.392 -783.392 -783.392], eps: 0.001})
Step:   94400, Reward: [-380.025 -380.025 -380.025] [68.082], Avg: [-414.404 -414.404 -414.404] (0.0010) ({r_i: None, r_t: [-783.157 -783.157 -783.157], eps: 0.001})
Step:   94500, Reward: [-378.640 -378.640 -378.640] [85.219], Avg: [-414.366 -414.366 -414.366] (0.0010) ({r_i: None, r_t: [-747.134 -747.134 -747.134], eps: 0.001})
Step:   94600, Reward: [-402.167 -402.167 -402.167] [59.771], Avg: [-414.353 -414.353 -414.353] (0.0010) ({r_i: None, r_t: [-782.160 -782.160 -782.160], eps: 0.001})
Step:   94700, Reward: [-396.929 -396.929 -396.929] [67.589], Avg: [-414.335 -414.335 -414.335] (0.0010) ({r_i: None, r_t: [-771.442 -771.442 -771.442], eps: 0.001})
Step:   94800, Reward: [-406.672 -406.672 -406.672] [107.285], Avg: [-414.327 -414.327 -414.327] (0.0010) ({r_i: None, r_t: [-768.676 -768.676 -768.676], eps: 0.001})
Step:   94900, Reward: [-390.828 -390.828 -390.828] [65.235], Avg: [-414.302 -414.302 -414.302] (0.0010) ({r_i: None, r_t: [-792.955 -792.955 -792.955], eps: 0.001})
Step:   95000, Reward: [-359.779 -359.779 -359.779] [55.068], Avg: [-414.244 -414.244 -414.244] (0.0010) ({r_i: None, r_t: [-763.151 -763.151 -763.151], eps: 0.001})
Step:   95100, Reward: [-371.676 -371.676 -371.676] [59.752], Avg: [-414.200 -414.200 -414.200] (0.0010) ({r_i: None, r_t: [-796.938 -796.938 -796.938], eps: 0.001})
Step:   95200, Reward: [-389.381 -389.381 -389.381] [59.511], Avg: [-414.174 -414.174 -414.174] (0.0010) ({r_i: None, r_t: [-745.558 -745.558 -745.558], eps: 0.001})
Step:   95300, Reward: [-435.534 -435.534 -435.534] [94.782], Avg: [-414.196 -414.196 -414.196] (0.0010) ({r_i: None, r_t: [-782.670 -782.670 -782.670], eps: 0.001})
Step:   95400, Reward: [-375.579 -375.579 -375.579] [83.378], Avg: [-414.156 -414.156 -414.156] (0.0010) ({r_i: None, r_t: [-772.273 -772.273 -772.273], eps: 0.001})
Step:   95500, Reward: [-406.292 -406.292 -406.292] [55.539], Avg: [-414.147 -414.147 -414.147] (0.0010) ({r_i: None, r_t: [-799.242 -799.242 -799.242], eps: 0.001})
Step:   95600, Reward: [-364.063 -364.063 -364.063] [65.950], Avg: [-414.095 -414.095 -414.095] (0.0010) ({r_i: None, r_t: [-767.523 -767.523 -767.523], eps: 0.001})
Step:   95700, Reward: [-376.155 -376.155 -376.155] [80.895], Avg: [-414.055 -414.055 -414.055] (0.0010) ({r_i: None, r_t: [-797.986 -797.986 -797.986], eps: 0.001})
Step:   95800, Reward: [-368.519 -368.519 -368.519] [64.206], Avg: [-414.008 -414.008 -414.008] (0.0010) ({r_i: None, r_t: [-715.854 -715.854 -715.854], eps: 0.001})
Step:   95900, Reward: [-375.533 -375.533 -375.533] [77.487], Avg: [-413.968 -413.968 -413.968] (0.0010) ({r_i: None, r_t: [-764.475 -764.475 -764.475], eps: 0.001})
Step:   96000, Reward: [-376.564 -376.564 -376.564] [50.269], Avg: [-413.929 -413.929 -413.929] (0.0010) ({r_i: None, r_t: [-738.594 -738.594 -738.594], eps: 0.001})
Step:   96100, Reward: [-405.761 -405.761 -405.761] [100.213], Avg: [-413.921 -413.921 -413.921] (0.0010) ({r_i: None, r_t: [-823.182 -823.182 -823.182], eps: 0.001})
Step:   96200, Reward: [-404.439 -404.439 -404.439] [99.424], Avg: [-413.911 -413.911 -413.911] (0.0010) ({r_i: None, r_t: [-754.176 -754.176 -754.176], eps: 0.001})
Step:   96300, Reward: [-390.339 -390.339 -390.339] [74.184], Avg: [-413.886 -413.886 -413.886] (0.0010) ({r_i: None, r_t: [-815.450 -815.450 -815.450], eps: 0.001})
Step:   96400, Reward: [-392.030 -392.030 -392.030] [69.936], Avg: [-413.864 -413.864 -413.864] (0.0010) ({r_i: None, r_t: [-784.296 -784.296 -784.296], eps: 0.001})
Step:   96500, Reward: [-366.601 -366.601 -366.601] [76.194], Avg: [-413.815 -413.815 -413.815] (0.0010) ({r_i: None, r_t: [-791.046 -791.046 -791.046], eps: 0.001})
Step:   96600, Reward: [-395.539 -395.539 -395.539] [66.773], Avg: [-413.796 -413.796 -413.796] (0.0010) ({r_i: None, r_t: [-781.194 -781.194 -781.194], eps: 0.001})
Step:   96700, Reward: [-383.973 -383.973 -383.973] [84.028], Avg: [-413.765 -413.765 -413.765] (0.0010) ({r_i: None, r_t: [-782.634 -782.634 -782.634], eps: 0.001})
Step:   96800, Reward: [-408.269 -408.269 -408.269] [51.566], Avg: [-413.759 -413.759 -413.759] (0.0010) ({r_i: None, r_t: [-741.282 -741.282 -741.282], eps: 0.001})
Step:   96900, Reward: [-396.268 -396.268 -396.268] [65.608], Avg: [-413.741 -413.741 -413.741] (0.0010) ({r_i: None, r_t: [-768.987 -768.987 -768.987], eps: 0.001})
Step:   97000, Reward: [-372.624 -372.624 -372.624] [83.548], Avg: [-413.699 -413.699 -413.699] (0.0010) ({r_i: None, r_t: [-797.913 -797.913 -797.913], eps: 0.001})
Step:   97100, Reward: [-349.202 -349.202 -349.202] [44.451], Avg: [-413.633 -413.633 -413.633] (0.0010) ({r_i: None, r_t: [-830.177 -830.177 -830.177], eps: 0.001})
Step:   97200, Reward: [-389.786 -389.786 -389.786] [76.449], Avg: [-413.608 -413.608 -413.608] (0.0010) ({r_i: None, r_t: [-783.668 -783.668 -783.668], eps: 0.001})
Step:   97300, Reward: [-387.217 -387.217 -387.217] [62.398], Avg: [-413.581 -413.581 -413.581] (0.0010) ({r_i: None, r_t: [-770.506 -770.506 -770.506], eps: 0.001})
Step:   97400, Reward: [-411.696 -411.696 -411.696] [85.862], Avg: [-413.579 -413.579 -413.579] (0.0010) ({r_i: None, r_t: [-757.751 -757.751 -757.751], eps: 0.001})
Step:   97500, Reward: [-366.197 -366.197 -366.197] [52.216], Avg: [-413.530 -413.530 -413.530] (0.0010) ({r_i: None, r_t: [-778.844 -778.844 -778.844], eps: 0.001})
Step:   97600, Reward: [-390.755 -390.755 -390.755] [53.919], Avg: [-413.507 -413.507 -413.507] (0.0010) ({r_i: None, r_t: [-778.020 -778.020 -778.020], eps: 0.001})
Step:   97700, Reward: [-395.470 -395.470 -395.470] [93.034], Avg: [-413.489 -413.489 -413.489] (0.0010) ({r_i: None, r_t: [-792.966 -792.966 -792.966], eps: 0.001})
Step:   97800, Reward: [-383.708 -383.708 -383.708] [70.565], Avg: [-413.458 -413.458 -413.458] (0.0010) ({r_i: None, r_t: [-783.307 -783.307 -783.307], eps: 0.001})
Step:   97900, Reward: [-391.343 -391.343 -391.343] [113.606], Avg: [-413.436 -413.436 -413.436] (0.0010) ({r_i: None, r_t: [-798.436 -798.436 -798.436], eps: 0.001})
Step:   98000, Reward: [-379.654 -379.654 -379.654] [62.026], Avg: [-413.401 -413.401 -413.401] (0.0010) ({r_i: None, r_t: [-786.766 -786.766 -786.766], eps: 0.001})
Step:   98100, Reward: [-412.209 -412.209 -412.209] [78.923], Avg: [-413.400 -413.400 -413.400] (0.0010) ({r_i: None, r_t: [-772.047 -772.047 -772.047], eps: 0.001})
Step:   98200, Reward: [-396.205 -396.205 -396.205] [75.212], Avg: [-413.383 -413.383 -413.383] (0.0010) ({r_i: None, r_t: [-785.559 -785.559 -785.559], eps: 0.001})
Step:   98300, Reward: [-378.795 -378.795 -378.795] [65.674], Avg: [-413.347 -413.347 -413.347] (0.0010) ({r_i: None, r_t: [-747.114 -747.114 -747.114], eps: 0.001})
Step:   98400, Reward: [-385.323 -385.323 -385.323] [64.328], Avg: [-413.319 -413.319 -413.319] (0.0010) ({r_i: None, r_t: [-793.097 -793.097 -793.097], eps: 0.001})
Step:   98500, Reward: [-402.798 -402.798 -402.798] [125.799], Avg: [-413.308 -413.308 -413.308] (0.0010) ({r_i: None, r_t: [-733.015 -733.015 -733.015], eps: 0.001})
Step:   98600, Reward: [-370.395 -370.395 -370.395] [60.838], Avg: [-413.265 -413.265 -413.265] (0.0010) ({r_i: None, r_t: [-714.311 -714.311 -714.311], eps: 0.001})
Step:   98700, Reward: [-381.917 -381.917 -381.917] [63.160], Avg: [-413.233 -413.233 -413.233] (0.0010) ({r_i: None, r_t: [-758.959 -758.959 -758.959], eps: 0.001})
Step:   98800, Reward: [-362.611 -362.611 -362.611] [70.743], Avg: [-413.182 -413.182 -413.182] (0.0010) ({r_i: None, r_t: [-739.448 -739.448 -739.448], eps: 0.001})
Step:   98900, Reward: [-405.167 -405.167 -405.167] [117.462], Avg: [-413.174 -413.174 -413.174] (0.0010) ({r_i: None, r_t: [-752.290 -752.290 -752.290], eps: 0.001})
Step:   99000, Reward: [-432.071 -432.071 -432.071] [101.450], Avg: [-413.193 -413.193 -413.193] (0.0010) ({r_i: None, r_t: [-746.074 -746.074 -746.074], eps: 0.001})
Step:   99100, Reward: [-414.992 -414.992 -414.992] [74.815], Avg: [-413.195 -413.195 -413.195] (0.0010) ({r_i: None, r_t: [-794.831 -794.831 -794.831], eps: 0.001})
Step:   99200, Reward: [-402.542 -402.542 -402.542] [86.888], Avg: [-413.184 -413.184 -413.184] (0.0010) ({r_i: None, r_t: [-767.960 -767.960 -767.960], eps: 0.001})
Step:   99300, Reward: [-386.278 -386.278 -386.278] [55.419], Avg: [-413.157 -413.157 -413.157] (0.0010) ({r_i: None, r_t: [-729.950 -729.950 -729.950], eps: 0.001})
Step:   99400, Reward: [-423.907 -423.907 -423.907] [86.587], Avg: [-413.168 -413.168 -413.168] (0.0010) ({r_i: None, r_t: [-783.968 -783.968 -783.968], eps: 0.001})
Step:   99500, Reward: [-403.558 -403.558 -403.558] [90.384], Avg: [-413.158 -413.158 -413.158] (0.0010) ({r_i: None, r_t: [-778.879 -778.879 -778.879], eps: 0.001})
Step:   99600, Reward: [-410.704 -410.704 -410.704] [108.300], Avg: [-413.156 -413.156 -413.156] (0.0010) ({r_i: None, r_t: [-771.098 -771.098 -771.098], eps: 0.001})
Step:   99700, Reward: [-390.199 -390.199 -390.199] [64.293], Avg: [-413.133 -413.133 -413.133] (0.0010) ({r_i: None, r_t: [-791.294 -791.294 -791.294], eps: 0.001})
Step:   99800, Reward: [-382.924 -382.924 -382.924] [62.075], Avg: [-413.102 -413.102 -413.102] (0.0010) ({r_i: None, r_t: [-799.116 -799.116 -799.116], eps: 0.001})
Step:   99900, Reward: [-394.923 -394.923 -394.923] [63.042], Avg: [-413.084 -413.084 -413.084] (0.0010) ({r_i: None, r_t: [-749.179 -749.179 -749.179], eps: 0.001})
Step:  100000, Reward: [-396.461 -396.461 -396.461] [124.337], Avg: [-413.068 -413.068 -413.068] (0.0010) ({r_i: None, r_t: [-810.748 -810.748 -810.748], eps: 0.001})
Step:  100100, Reward: [-388.651 -388.651 -388.651] [93.800], Avg: [-413.043 -413.043 -413.043] (0.0010) ({r_i: None, r_t: [-760.952 -760.952 -760.952], eps: 0.001})
Step:  100200, Reward: [-387.674 -387.674 -387.674] [82.559], Avg: [-413.018 -413.018 -413.018] (0.0010) ({r_i: None, r_t: [-712.698 -712.698 -712.698], eps: 0.001})
Step:  100300, Reward: [-359.038 -359.038 -359.038] [47.468], Avg: [-412.964 -412.964 -412.964] (0.0010) ({r_i: None, r_t: [-805.288 -805.288 -805.288], eps: 0.001})
Step:  100400, Reward: [-389.140 -389.140 -389.140] [77.920], Avg: [-412.940 -412.940 -412.940] (0.0010) ({r_i: None, r_t: [-758.069 -758.069 -758.069], eps: 0.001})
Step:  100500, Reward: [-401.526 -401.526 -401.526] [104.671], Avg: [-412.929 -412.929 -412.929] (0.0010) ({r_i: None, r_t: [-743.593 -743.593 -743.593], eps: 0.001})
Step:  100600, Reward: [-380.297 -380.297 -380.297] [59.972], Avg: [-412.897 -412.897 -412.897] (0.0010) ({r_i: None, r_t: [-776.768 -776.768 -776.768], eps: 0.001})
Step:  100700, Reward: [-371.653 -371.653 -371.653] [62.388], Avg: [-412.856 -412.856 -412.856] (0.0010) ({r_i: None, r_t: [-824.171 -824.171 -824.171], eps: 0.001})
Step:  100800, Reward: [-364.307 -364.307 -364.307] [83.229], Avg: [-412.808 -412.808 -412.808] (0.0010) ({r_i: None, r_t: [-815.195 -815.195 -815.195], eps: 0.001})
Step:  100900, Reward: [-380.348 -380.348 -380.348] [61.336], Avg: [-412.776 -412.776 -412.776] (0.0010) ({r_i: None, r_t: [-786.726 -786.726 -786.726], eps: 0.001})
Step:  101000, Reward: [-361.816 -361.816 -361.816] [57.849], Avg: [-412.725 -412.725 -412.725] (0.0010) ({r_i: None, r_t: [-765.168 -765.168 -765.168], eps: 0.001})
Step:  101100, Reward: [-400.420 -400.420 -400.420] [74.087], Avg: [-412.713 -412.713 -412.713] (0.0010) ({r_i: None, r_t: [-763.477 -763.477 -763.477], eps: 0.001})
Step:  101200, Reward: [-388.120 -388.120 -388.120] [89.581], Avg: [-412.689 -412.689 -412.689] (0.0010) ({r_i: None, r_t: [-768.078 -768.078 -768.078], eps: 0.001})
Step:  101300, Reward: [-392.431 -392.431 -392.431] [61.687], Avg: [-412.669 -412.669 -412.669] (0.0010) ({r_i: None, r_t: [-755.671 -755.671 -755.671], eps: 0.001})
Step:  101400, Reward: [-401.316 -401.316 -401.316] [86.101], Avg: [-412.658 -412.658 -412.658] (0.0010) ({r_i: None, r_t: [-759.913 -759.913 -759.913], eps: 0.001})
Step:  101500, Reward: [-372.898 -372.898 -372.898] [71.922], Avg: [-412.618 -412.618 -412.618] (0.0010) ({r_i: None, r_t: [-749.405 -749.405 -749.405], eps: 0.001})
Step:  101600, Reward: [-399.313 -399.313 -399.313] [54.578], Avg: [-412.605 -412.605 -412.605] (0.0010) ({r_i: None, r_t: [-759.858 -759.858 -759.858], eps: 0.001})
Step:  101700, Reward: [-406.787 -406.787 -406.787] [61.766], Avg: [-412.600 -412.600 -412.600] (0.0010) ({r_i: None, r_t: [-786.599 -786.599 -786.599], eps: 0.001})
Step:  101800, Reward: [-373.782 -373.782 -373.782] [78.157], Avg: [-412.561 -412.561 -412.561] (0.0010) ({r_i: None, r_t: [-756.724 -756.724 -756.724], eps: 0.001})
Step:  101900, Reward: [-386.864 -386.864 -386.864] [49.967], Avg: [-412.536 -412.536 -412.536] (0.0010) ({r_i: None, r_t: [-760.345 -760.345 -760.345], eps: 0.001})
Step:  102000, Reward: [-368.317 -368.317 -368.317] [51.987], Avg: [-412.493 -412.493 -412.493] (0.0010) ({r_i: None, r_t: [-734.482 -734.482 -734.482], eps: 0.001})
Step:  102100, Reward: [-403.523 -403.523 -403.523] [63.546], Avg: [-412.484 -412.484 -412.484] (0.0010) ({r_i: None, r_t: [-756.343 -756.343 -756.343], eps: 0.001})
Step:  102200, Reward: [-368.298 -368.298 -368.298] [67.994], Avg: [-412.441 -412.441 -412.441] (0.0010) ({r_i: None, r_t: [-764.402 -764.402 -764.402], eps: 0.001})
Step:  102300, Reward: [-402.814 -402.814 -402.814] [79.078], Avg: [-412.432 -412.432 -412.432] (0.0010) ({r_i: None, r_t: [-822.594 -822.594 -822.594], eps: 0.001})
Step:  102400, Reward: [-392.100 -392.100 -392.100] [45.812], Avg: [-412.412 -412.412 -412.412] (0.0010) ({r_i: None, r_t: [-736.714 -736.714 -736.714], eps: 0.001})
Step:  102500, Reward: [-400.631 -400.631 -400.631] [69.116], Avg: [-412.400 -412.400 -412.400] (0.0010) ({r_i: None, r_t: [-737.163 -737.163 -737.163], eps: 0.001})
Step:  102600, Reward: [-380.529 -380.529 -380.529] [54.033], Avg: [-412.369 -412.369 -412.369] (0.0010) ({r_i: None, r_t: [-797.500 -797.500 -797.500], eps: 0.001})
Step:  102700, Reward: [-404.201 -404.201 -404.201] [114.427], Avg: [-412.361 -412.361 -412.361] (0.0010) ({r_i: None, r_t: [-769.982 -769.982 -769.982], eps: 0.001})
Step:  102800, Reward: [-393.629 -393.629 -393.629] [50.684], Avg: [-412.343 -412.343 -412.343] (0.0010) ({r_i: None, r_t: [-763.449 -763.449 -763.449], eps: 0.001})
Step:  102900, Reward: [-374.936 -374.936 -374.936] [59.147], Avg: [-412.307 -412.307 -412.307] (0.0010) ({r_i: None, r_t: [-746.484 -746.484 -746.484], eps: 0.001})
Step:  103000, Reward: [-430.074 -430.074 -430.074] [85.094], Avg: [-412.324 -412.324 -412.324] (0.0010) ({r_i: None, r_t: [-772.893 -772.893 -772.893], eps: 0.001})
Step:  103100, Reward: [-381.715 -381.715 -381.715] [55.902], Avg: [-412.294 -412.294 -412.294] (0.0010) ({r_i: None, r_t: [-787.418 -787.418 -787.418], eps: 0.001})
Step:  103200, Reward: [-412.271 -412.271 -412.271] [94.605], Avg: [-412.294 -412.294 -412.294] (0.0010) ({r_i: None, r_t: [-748.147 -748.147 -748.147], eps: 0.001})
Step:  103300, Reward: [-389.977 -389.977 -389.977] [72.175], Avg: [-412.273 -412.273 -412.273] (0.0010) ({r_i: None, r_t: [-778.272 -778.272 -778.272], eps: 0.001})
Step:  103400, Reward: [-388.738 -388.738 -388.738] [78.875], Avg: [-412.250 -412.250 -412.250] (0.0010) ({r_i: None, r_t: [-764.649 -764.649 -764.649], eps: 0.001})
Step:  103500, Reward: [-378.266 -378.266 -378.266] [49.860], Avg: [-412.217 -412.217 -412.217] (0.0010) ({r_i: None, r_t: [-791.305 -791.305 -791.305], eps: 0.001})
Step:  103600, Reward: [-401.630 -401.630 -401.630] [63.039], Avg: [-412.207 -412.207 -412.207] (0.0010) ({r_i: None, r_t: [-700.241 -700.241 -700.241], eps: 0.001})
Step:  103700, Reward: [-372.899 -372.899 -372.899] [56.070], Avg: [-412.169 -412.169 -412.169] (0.0010) ({r_i: None, r_t: [-769.332 -769.332 -769.332], eps: 0.001})
Step:  103800, Reward: [-377.140 -377.140 -377.140] [70.393], Avg: [-412.135 -412.135 -412.135] (0.0010) ({r_i: None, r_t: [-765.835 -765.835 -765.835], eps: 0.001})
Step:  103900, Reward: [-358.609 -358.609 -358.609] [71.173], Avg: [-412.084 -412.084 -412.084] (0.0010) ({r_i: None, r_t: [-750.652 -750.652 -750.652], eps: 0.001})
Step:  104000, Reward: [-439.070 -439.070 -439.070] [91.470], Avg: [-412.110 -412.110 -412.110] (0.0010) ({r_i: None, r_t: [-735.596 -735.596 -735.596], eps: 0.001})
Step:  104100, Reward: [-375.831 -375.831 -375.831] [62.824], Avg: [-412.075 -412.075 -412.075] (0.0010) ({r_i: None, r_t: [-749.180 -749.180 -749.180], eps: 0.001})
Step:  104200, Reward: [-384.424 -384.424 -384.424] [58.111], Avg: [-412.049 -412.049 -412.049] (0.0010) ({r_i: None, r_t: [-738.983 -738.983 -738.983], eps: 0.001})
Step:  104300, Reward: [-420.071 -420.071 -420.071] [80.493], Avg: [-412.056 -412.056 -412.056] (0.0010) ({r_i: None, r_t: [-788.328 -788.328 -788.328], eps: 0.001})
Step:  104400, Reward: [-373.773 -373.773 -373.773] [65.700], Avg: [-412.020 -412.020 -412.020] (0.0010) ({r_i: None, r_t: [-767.421 -767.421 -767.421], eps: 0.001})
Step:  104500, Reward: [-395.431 -395.431 -395.431] [88.636], Avg: [-412.004 -412.004 -412.004] (0.0010) ({r_i: None, r_t: [-749.323 -749.323 -749.323], eps: 0.001})
Step:  104600, Reward: [-384.258 -384.258 -384.258] [65.542], Avg: [-411.977 -411.977 -411.977] (0.0010) ({r_i: None, r_t: [-738.366 -738.366 -738.366], eps: 0.001})
Step:  104700, Reward: [-388.861 -388.861 -388.861] [57.496], Avg: [-411.955 -411.955 -411.955] (0.0010) ({r_i: None, r_t: [-767.303 -767.303 -767.303], eps: 0.001})
Step:  104800, Reward: [-398.620 -398.620 -398.620] [64.496], Avg: [-411.942 -411.942 -411.942] (0.0010) ({r_i: None, r_t: [-746.857 -746.857 -746.857], eps: 0.001})
Step:  104900, Reward: [-398.070 -398.070 -398.070] [45.601], Avg: [-411.929 -411.929 -411.929] (0.0010) ({r_i: None, r_t: [-787.204 -787.204 -787.204], eps: 0.001})
Step:  105000, Reward: [-368.600 -368.600 -368.600] [61.465], Avg: [-411.888 -411.888 -411.888] (0.0010) ({r_i: None, r_t: [-799.374 -799.374 -799.374], eps: 0.001})
Step:  105100, Reward: [-397.409 -397.409 -397.409] [71.877], Avg: [-411.874 -411.874 -411.874] (0.0010) ({r_i: None, r_t: [-776.391 -776.391 -776.391], eps: 0.001})
Step:  105200, Reward: [-394.910 -394.910 -394.910] [49.039], Avg: [-411.858 -411.858 -411.858] (0.0010) ({r_i: None, r_t: [-758.044 -758.044 -758.044], eps: 0.001})
Step:  105300, Reward: [-348.374 -348.374 -348.374] [44.187], Avg: [-411.798 -411.798 -411.798] (0.0010) ({r_i: None, r_t: [-748.497 -748.497 -748.497], eps: 0.001})
Step:  105400, Reward: [-429.339 -429.339 -429.339] [90.286], Avg: [-411.815 -411.815 -411.815] (0.0010) ({r_i: None, r_t: [-759.232 -759.232 -759.232], eps: 0.001})
Step:  105500, Reward: [-390.536 -390.536 -390.536] [36.546], Avg: [-411.794 -411.794 -411.794] (0.0010) ({r_i: None, r_t: [-804.851 -804.851 -804.851], eps: 0.001})
Step:  105600, Reward: [-373.876 -373.876 -373.876] [72.614], Avg: [-411.759 -411.759 -411.759] (0.0010) ({r_i: None, r_t: [-775.366 -775.366 -775.366], eps: 0.001})
Step:  105700, Reward: [-401.230 -401.230 -401.230] [71.737], Avg: [-411.749 -411.749 -411.749] (0.0010) ({r_i: None, r_t: [-733.810 -733.810 -733.810], eps: 0.001})
Step:  105800, Reward: [-390.646 -390.646 -390.646] [100.764], Avg: [-411.729 -411.729 -411.729] (0.0010) ({r_i: None, r_t: [-787.195 -787.195 -787.195], eps: 0.001})
Step:  105900, Reward: [-387.596 -387.596 -387.596] [61.349], Avg: [-411.706 -411.706 -411.706] (0.0010) ({r_i: None, r_t: [-771.615 -771.615 -771.615], eps: 0.001})
Step:  106000, Reward: [-356.020 -356.020 -356.020] [58.320], Avg: [-411.653 -411.653 -411.653] (0.0010) ({r_i: None, r_t: [-786.478 -786.478 -786.478], eps: 0.001})
Step:  106100, Reward: [-389.244 -389.244 -389.244] [54.786], Avg: [-411.632 -411.632 -411.632] (0.0010) ({r_i: None, r_t: [-786.193 -786.193 -786.193], eps: 0.001})
Step:  106200, Reward: [-386.420 -386.420 -386.420] [77.016], Avg: [-411.609 -411.609 -411.609] (0.0010) ({r_i: None, r_t: [-777.149 -777.149 -777.149], eps: 0.001})
Step:  106300, Reward: [-379.274 -379.274 -379.274] [47.102], Avg: [-411.578 -411.578 -411.578] (0.0010) ({r_i: None, r_t: [-745.885 -745.885 -745.885], eps: 0.001})
Step:  106400, Reward: [-387.749 -387.749 -387.749] [74.109], Avg: [-411.556 -411.556 -411.556] (0.0010) ({r_i: None, r_t: [-834.489 -834.489 -834.489], eps: 0.001})
Step:  106500, Reward: [-383.570 -383.570 -383.570] [45.442], Avg: [-411.530 -411.530 -411.530] (0.0010) ({r_i: None, r_t: [-783.452 -783.452 -783.452], eps: 0.001})
Step:  106600, Reward: [-395.535 -395.535 -395.535] [48.474], Avg: [-411.515 -411.515 -411.515] (0.0010) ({r_i: None, r_t: [-837.010 -837.010 -837.010], eps: 0.001})
Step:  106700, Reward: [-397.624 -397.624 -397.624] [55.985], Avg: [-411.502 -411.502 -411.502] (0.0010) ({r_i: None, r_t: [-751.883 -751.883 -751.883], eps: 0.001})
Step:  106800, Reward: [-396.662 -396.662 -396.662] [49.491], Avg: [-411.488 -411.488 -411.488] (0.0010) ({r_i: None, r_t: [-796.405 -796.405 -796.405], eps: 0.001})
Step:  106900, Reward: [-404.423 -404.423 -404.423] [64.409], Avg: [-411.481 -411.481 -411.481] (0.0010) ({r_i: None, r_t: [-786.257 -786.257 -786.257], eps: 0.001})
Step:  107000, Reward: [-417.896 -417.896 -417.896] [93.437], Avg: [-411.487 -411.487 -411.487] (0.0010) ({r_i: None, r_t: [-738.981 -738.981 -738.981], eps: 0.001})
Step:  107100, Reward: [-377.271 -377.271 -377.271] [61.195], Avg: [-411.455 -411.455 -411.455] (0.0010) ({r_i: None, r_t: [-785.547 -785.547 -785.547], eps: 0.001})
Step:  107200, Reward: [-362.101 -362.101 -362.101] [73.617], Avg: [-411.409 -411.409 -411.409] (0.0010) ({r_i: None, r_t: [-823.825 -823.825 -823.825], eps: 0.001})
Step:  107300, Reward: [-356.383 -356.383 -356.383] [68.289], Avg: [-411.358 -411.358 -411.358] (0.0010) ({r_i: None, r_t: [-775.269 -775.269 -775.269], eps: 0.001})
Step:  107400, Reward: [-405.414 -405.414 -405.414] [65.385], Avg: [-411.352 -411.352 -411.352] (0.0010) ({r_i: None, r_t: [-760.354 -760.354 -760.354], eps: 0.001})
Step:  107500, Reward: [-376.044 -376.044 -376.044] [77.806], Avg: [-411.320 -411.320 -411.320] (0.0010) ({r_i: None, r_t: [-783.367 -783.367 -783.367], eps: 0.001})
Step:  107600, Reward: [-410.115 -410.115 -410.115] [61.937], Avg: [-411.318 -411.318 -411.318] (0.0010) ({r_i: None, r_t: [-773.039 -773.039 -773.039], eps: 0.001})
Step:  107700, Reward: [-427.885 -427.885 -427.885] [95.402], Avg: [-411.334 -411.334 -411.334] (0.0010) ({r_i: None, r_t: [-794.529 -794.529 -794.529], eps: 0.001})
Step:  107800, Reward: [-382.629 -382.629 -382.629] [48.255], Avg: [-411.307 -411.307 -411.307] (0.0010) ({r_i: None, r_t: [-809.877 -809.877 -809.877], eps: 0.001})
Step:  107900, Reward: [-413.803 -413.803 -413.803] [85.527], Avg: [-411.310 -411.310 -411.310] (0.0010) ({r_i: None, r_t: [-727.995 -727.995 -727.995], eps: 0.001})
Step:  108000, Reward: [-432.830 -432.830 -432.830] [71.705], Avg: [-411.329 -411.329 -411.329] (0.0010) ({r_i: None, r_t: [-770.643 -770.643 -770.643], eps: 0.001})
Step:  108100, Reward: [-408.691 -408.691 -408.691] [68.977], Avg: [-411.327 -411.327 -411.327] (0.0010) ({r_i: None, r_t: [-774.428 -774.428 -774.428], eps: 0.001})
Step:  108200, Reward: [-369.650 -369.650 -369.650] [57.710], Avg: [-411.289 -411.289 -411.289] (0.0010) ({r_i: None, r_t: [-755.806 -755.806 -755.806], eps: 0.001})
Step:  108300, Reward: [-407.484 -407.484 -407.484] [90.279], Avg: [-411.285 -411.285 -411.285] (0.0010) ({r_i: None, r_t: [-767.371 -767.371 -767.371], eps: 0.001})
Step:  108400, Reward: [-406.436 -406.436 -406.436] [65.363], Avg: [-411.281 -411.281 -411.281] (0.0010) ({r_i: None, r_t: [-792.334 -792.334 -792.334], eps: 0.001})
Step:  108500, Reward: [-383.079 -383.079 -383.079] [62.935], Avg: [-411.255 -411.255 -411.255] (0.0010) ({r_i: None, r_t: [-758.385 -758.385 -758.385], eps: 0.001})
Step:  108600, Reward: [-361.374 -361.374 -361.374] [59.238], Avg: [-411.209 -411.209 -411.209] (0.0010) ({r_i: None, r_t: [-853.164 -853.164 -853.164], eps: 0.001})
Step:  108700, Reward: [-403.449 -403.449 -403.449] [84.201], Avg: [-411.202 -411.202 -411.202] (0.0010) ({r_i: None, r_t: [-766.686 -766.686 -766.686], eps: 0.001})
Step:  108800, Reward: [-412.120 -412.120 -412.120] [82.313], Avg: [-411.202 -411.202 -411.202] (0.0010) ({r_i: None, r_t: [-774.946 -774.946 -774.946], eps: 0.001})
Step:  108900, Reward: [-411.560 -411.560 -411.560] [75.504], Avg: [-411.203 -411.203 -411.203] (0.0010) ({r_i: None, r_t: [-753.483 -753.483 -753.483], eps: 0.001})
Step:  109000, Reward: [-361.331 -361.331 -361.331] [73.848], Avg: [-411.157 -411.157 -411.157] (0.0010) ({r_i: None, r_t: [-755.397 -755.397 -755.397], eps: 0.001})
Step:  109100, Reward: [-377.470 -377.470 -377.470] [73.304], Avg: [-411.126 -411.126 -411.126] (0.0010) ({r_i: None, r_t: [-775.548 -775.548 -775.548], eps: 0.001})
Step:  109200, Reward: [-399.749 -399.749 -399.749] [63.238], Avg: [-411.116 -411.116 -411.116] (0.0010) ({r_i: None, r_t: [-813.899 -813.899 -813.899], eps: 0.001})
Step:  109300, Reward: [-381.384 -381.384 -381.384] [90.548], Avg: [-411.089 -411.089 -411.089] (0.0010) ({r_i: None, r_t: [-797.358 -797.358 -797.358], eps: 0.001})
Step:  109400, Reward: [-408.732 -408.732 -408.732] [64.484], Avg: [-411.086 -411.086 -411.086] (0.0010) ({r_i: None, r_t: [-799.151 -799.151 -799.151], eps: 0.001})
Step:  109500, Reward: [-383.982 -383.982 -383.982] [73.981], Avg: [-411.062 -411.062 -411.062] (0.0010) ({r_i: None, r_t: [-794.661 -794.661 -794.661], eps: 0.001})
Step:  109600, Reward: [-417.906 -417.906 -417.906] [50.224], Avg: [-411.068 -411.068 -411.068] (0.0010) ({r_i: None, r_t: [-795.549 -795.549 -795.549], eps: 0.001})
Step:  109700, Reward: [-389.692 -389.692 -389.692] [69.515], Avg: [-411.048 -411.048 -411.048] (0.0010) ({r_i: None, r_t: [-776.857 -776.857 -776.857], eps: 0.001})
Step:  109800, Reward: [-381.894 -381.894 -381.894] [60.700], Avg: [-411.022 -411.022 -411.022] (0.0010) ({r_i: None, r_t: [-742.896 -742.896 -742.896], eps: 0.001})
Step:  109900, Reward: [-393.438 -393.438 -393.438] [48.560], Avg: [-411.006 -411.006 -411.006] (0.0010) ({r_i: None, r_t: [-800.880 -800.880 -800.880], eps: 0.001})
Step:  110000, Reward: [-346.495 -346.495 -346.495] [54.979], Avg: [-410.947 -410.947 -410.947] (0.0010) ({r_i: None, r_t: [-745.822 -745.822 -745.822], eps: 0.001})
Step:  110100, Reward: [-400.594 -400.594 -400.594] [72.587], Avg: [-410.938 -410.938 -410.938] (0.0010) ({r_i: None, r_t: [-756.554 -756.554 -756.554], eps: 0.001})
Step:  110200, Reward: [-407.117 -407.117 -407.117] [115.918], Avg: [-410.934 -410.934 -410.934] (0.0010) ({r_i: None, r_t: [-743.196 -743.196 -743.196], eps: 0.001})
Step:  110300, Reward: [-391.554 -391.554 -391.554] [56.858], Avg: [-410.917 -410.917 -410.917] (0.0010) ({r_i: None, r_t: [-810.599 -810.599 -810.599], eps: 0.001})
Step:  110400, Reward: [-392.778 -392.778 -392.778] [66.160], Avg: [-410.901 -410.901 -410.901] (0.0010) ({r_i: None, r_t: [-762.975 -762.975 -762.975], eps: 0.001})
Step:  110500, Reward: [-402.687 -402.687 -402.687] [104.577], Avg: [-410.893 -410.893 -410.893] (0.0010) ({r_i: None, r_t: [-800.818 -800.818 -800.818], eps: 0.001})
Step:  110600, Reward: [-377.621 -377.621 -377.621] [55.233], Avg: [-410.863 -410.863 -410.863] (0.0010) ({r_i: None, r_t: [-719.944 -719.944 -719.944], eps: 0.001})
Step:  110700, Reward: [-370.553 -370.553 -370.553] [68.278], Avg: [-410.827 -410.827 -410.827] (0.0010) ({r_i: None, r_t: [-822.025 -822.025 -822.025], eps: 0.001})
Step:  110800, Reward: [-385.783 -385.783 -385.783] [58.350], Avg: [-410.804 -410.804 -410.804] (0.0010) ({r_i: None, r_t: [-757.798 -757.798 -757.798], eps: 0.001})
Step:  110900, Reward: [-429.640 -429.640 -429.640] [116.805], Avg: [-410.821 -410.821 -410.821] (0.0010) ({r_i: None, r_t: [-767.110 -767.110 -767.110], eps: 0.001})
Step:  111000, Reward: [-372.568 -372.568 -372.568] [58.130], Avg: [-410.787 -410.787 -410.787] (0.0010) ({r_i: None, r_t: [-744.854 -744.854 -744.854], eps: 0.001})
Step:  111100, Reward: [-355.863 -355.863 -355.863] [68.111], Avg: [-410.737 -410.737 -410.737] (0.0010) ({r_i: None, r_t: [-768.813 -768.813 -768.813], eps: 0.001})
Step:  111200, Reward: [-364.035 -364.035 -364.035] [54.827], Avg: [-410.695 -410.695 -410.695] (0.0010) ({r_i: None, r_t: [-769.233 -769.233 -769.233], eps: 0.001})
Step:  111300, Reward: [-366.253 -366.253 -366.253] [71.423], Avg: [-410.655 -410.655 -410.655] (0.0010) ({r_i: None, r_t: [-773.678 -773.678 -773.678], eps: 0.001})
Step:  111400, Reward: [-408.301 -408.301 -408.301] [58.694], Avg: [-410.653 -410.653 -410.653] (0.0010) ({r_i: None, r_t: [-755.042 -755.042 -755.042], eps: 0.001})
Step:  111500, Reward: [-397.372 -397.372 -397.372] [71.790], Avg: [-410.641 -410.641 -410.641] (0.0010) ({r_i: None, r_t: [-754.386 -754.386 -754.386], eps: 0.001})
Step:  111600, Reward: [-422.751 -422.751 -422.751] [74.212], Avg: [-410.652 -410.652 -410.652] (0.0010) ({r_i: None, r_t: [-843.150 -843.150 -843.150], eps: 0.001})
Step:  111700, Reward: [-365.685 -365.685 -365.685] [69.203], Avg: [-410.612 -410.612 -410.612] (0.0010) ({r_i: None, r_t: [-747.864 -747.864 -747.864], eps: 0.001})
Step:  111800, Reward: [-382.199 -382.199 -382.199] [66.232], Avg: [-410.587 -410.587 -410.587] (0.0010) ({r_i: None, r_t: [-757.530 -757.530 -757.530], eps: 0.001})
Step:  111900, Reward: [-374.262 -374.262 -374.262] [60.594], Avg: [-410.554 -410.554 -410.554] (0.0010) ({r_i: None, r_t: [-770.244 -770.244 -770.244], eps: 0.001})
Step:  112000, Reward: [-410.571 -410.571 -410.571] [68.715], Avg: [-410.554 -410.554 -410.554] (0.0010) ({r_i: None, r_t: [-768.211 -768.211 -768.211], eps: 0.001})
Step:  112100, Reward: [-402.170 -402.170 -402.170] [69.357], Avg: [-410.547 -410.547 -410.547] (0.0010) ({r_i: None, r_t: [-805.212 -805.212 -805.212], eps: 0.001})
Step:  112200, Reward: [-423.610 -423.610 -423.610] [51.717], Avg: [-410.558 -410.558 -410.558] (0.0010) ({r_i: None, r_t: [-817.885 -817.885 -817.885], eps: 0.001})
Step:  112300, Reward: [-391.737 -391.737 -391.737] [86.839], Avg: [-410.542 -410.542 -410.542] (0.0010) ({r_i: None, r_t: [-793.002 -793.002 -793.002], eps: 0.001})
Step:  112400, Reward: [-357.083 -357.083 -357.083] [48.893], Avg: [-410.494 -410.494 -410.494] (0.0010) ({r_i: None, r_t: [-782.542 -782.542 -782.542], eps: 0.001})
Step:  112500, Reward: [-380.671 -380.671 -380.671] [78.844], Avg: [-410.468 -410.468 -410.468] (0.0010) ({r_i: None, r_t: [-777.161 -777.161 -777.161], eps: 0.001})
Step:  112600, Reward: [-393.313 -393.313 -393.313] [71.646], Avg: [-410.452 -410.452 -410.452] (0.0010) ({r_i: None, r_t: [-792.585 -792.585 -792.585], eps: 0.001})
Step:  112700, Reward: [-353.972 -353.972 -353.972] [70.897], Avg: [-410.402 -410.402 -410.402] (0.0010) ({r_i: None, r_t: [-795.814 -795.814 -795.814], eps: 0.001})
Step:  112800, Reward: [-398.899 -398.899 -398.899] [71.244], Avg: [-410.392 -410.392 -410.392] (0.0010) ({r_i: None, r_t: [-762.816 -762.816 -762.816], eps: 0.001})
Step:  112900, Reward: [-374.535 -374.535 -374.535] [87.946], Avg: [-410.360 -410.360 -410.360] (0.0010) ({r_i: None, r_t: [-762.027 -762.027 -762.027], eps: 0.001})
Step:  113000, Reward: [-369.546 -369.546 -369.546] [65.916], Avg: [-410.324 -410.324 -410.324] (0.0010) ({r_i: None, r_t: [-740.518 -740.518 -740.518], eps: 0.001})
Step:  113100, Reward: [-401.433 -401.433 -401.433] [65.340], Avg: [-410.316 -410.316 -410.316] (0.0010) ({r_i: None, r_t: [-786.332 -786.332 -786.332], eps: 0.001})
Step:  113200, Reward: [-389.145 -389.145 -389.145] [60.481], Avg: [-410.298 -410.298 -410.298] (0.0010) ({r_i: None, r_t: [-737.038 -737.038 -737.038], eps: 0.001})
Step:  113300, Reward: [-371.478 -371.478 -371.478] [85.722], Avg: [-410.264 -410.264 -410.264] (0.0010) ({r_i: None, r_t: [-761.570 -761.570 -761.570], eps: 0.001})
Step:  113400, Reward: [-391.417 -391.417 -391.417] [53.469], Avg: [-410.247 -410.247 -410.247] (0.0010) ({r_i: None, r_t: [-753.134 -753.134 -753.134], eps: 0.001})
Step:  113500, Reward: [-383.123 -383.123 -383.123] [56.713], Avg: [-410.223 -410.223 -410.223] (0.0010) ({r_i: None, r_t: [-786.233 -786.233 -786.233], eps: 0.001})
Step:  113600, Reward: [-378.844 -378.844 -378.844] [68.281], Avg: [-410.195 -410.195 -410.195] (0.0010) ({r_i: None, r_t: [-809.536 -809.536 -809.536], eps: 0.001})
Step:  113700, Reward: [-402.163 -402.163 -402.163] [55.005], Avg: [-410.188 -410.188 -410.188] (0.0010) ({r_i: None, r_t: [-765.189 -765.189 -765.189], eps: 0.001})
Step:  113800, Reward: [-393.008 -393.008 -393.008] [84.128], Avg: [-410.173 -410.173 -410.173] (0.0010) ({r_i: None, r_t: [-774.373 -774.373 -774.373], eps: 0.001})
Step:  113900, Reward: [-385.285 -385.285 -385.285] [66.659], Avg: [-410.151 -410.151 -410.151] (0.0010) ({r_i: None, r_t: [-787.944 -787.944 -787.944], eps: 0.001})
Step:  114000, Reward: [-432.572 -432.572 -432.572] [102.617], Avg: [-410.171 -410.171 -410.171] (0.0010) ({r_i: None, r_t: [-766.658 -766.658 -766.658], eps: 0.001})
Step:  114100, Reward: [-393.792 -393.792 -393.792] [72.117], Avg: [-410.157 -410.157 -410.157] (0.0010) ({r_i: None, r_t: [-767.584 -767.584 -767.584], eps: 0.001})
Step:  114200, Reward: [-380.063 -380.063 -380.063] [72.965], Avg: [-410.130 -410.130 -410.130] (0.0010) ({r_i: None, r_t: [-769.070 -769.070 -769.070], eps: 0.001})
Step:  114300, Reward: [-379.044 -379.044 -379.044] [66.872], Avg: [-410.103 -410.103 -410.103] (0.0010) ({r_i: None, r_t: [-753.418 -753.418 -753.418], eps: 0.001})
Step:  114400, Reward: [-377.141 -377.141 -377.141] [87.596], Avg: [-410.074 -410.074 -410.074] (0.0010) ({r_i: None, r_t: [-723.001 -723.001 -723.001], eps: 0.001})
Step:  114500, Reward: [-380.399 -380.399 -380.399] [57.701], Avg: [-410.049 -410.049 -410.049] (0.0010) ({r_i: None, r_t: [-766.374 -766.374 -766.374], eps: 0.001})
Step:  114600, Reward: [-375.522 -375.522 -375.522] [85.252], Avg: [-410.018 -410.018 -410.018] (0.0010) ({r_i: None, r_t: [-735.186 -735.186 -735.186], eps: 0.001})
Step:  114700, Reward: [-404.354 -404.354 -404.354] [66.717], Avg: [-410.014 -410.014 -410.014] (0.0010) ({r_i: None, r_t: [-767.142 -767.142 -767.142], eps: 0.001})
Step:  114800, Reward: [-354.817 -354.817 -354.817] [67.318], Avg: [-409.966 -409.966 -409.966] (0.0010) ({r_i: None, r_t: [-726.161 -726.161 -726.161], eps: 0.001})
Step:  114900, Reward: [-392.908 -392.908 -392.908] [98.890], Avg: [-409.951 -409.951 -409.951] (0.0010) ({r_i: None, r_t: [-738.032 -738.032 -738.032], eps: 0.001})
Step:  115000, Reward: [-359.645 -359.645 -359.645] [59.322], Avg: [-409.907 -409.907 -409.907] (0.0010) ({r_i: None, r_t: [-711.902 -711.902 -711.902], eps: 0.001})
Step:  115100, Reward: [-407.010 -407.010 -407.010] [96.036], Avg: [-409.904 -409.904 -409.904] (0.0010) ({r_i: None, r_t: [-757.917 -757.917 -757.917], eps: 0.001})
Step:  115200, Reward: [-368.402 -368.402 -368.402] [62.274], Avg: [-409.868 -409.868 -409.868] (0.0010) ({r_i: None, r_t: [-764.874 -764.874 -764.874], eps: 0.001})
Step:  115300, Reward: [-389.651 -389.651 -389.651] [63.777], Avg: [-409.851 -409.851 -409.851] (0.0010) ({r_i: None, r_t: [-735.377 -735.377 -735.377], eps: 0.001})
Step:  115400, Reward: [-371.469 -371.469 -371.469] [76.378], Avg: [-409.818 -409.818 -409.818] (0.0010) ({r_i: None, r_t: [-756.630 -756.630 -756.630], eps: 0.001})
Step:  115500, Reward: [-380.641 -380.641 -380.641] [53.465], Avg: [-409.792 -409.792 -409.792] (0.0010) ({r_i: None, r_t: [-751.848 -751.848 -751.848], eps: 0.001})
Step:  115600, Reward: [-370.564 -370.564 -370.564] [83.684], Avg: [-409.759 -409.759 -409.759] (0.0010) ({r_i: None, r_t: [-807.668 -807.668 -807.668], eps: 0.001})
Step:  115700, Reward: [-368.793 -368.793 -368.793] [76.686], Avg: [-409.723 -409.723 -409.723] (0.0010) ({r_i: None, r_t: [-774.938 -774.938 -774.938], eps: 0.001})
Step:  115800, Reward: [-377.702 -377.702 -377.702] [71.236], Avg: [-409.696 -409.696 -409.696] (0.0010) ({r_i: None, r_t: [-783.475 -783.475 -783.475], eps: 0.001})
Step:  115900, Reward: [-408.559 -408.559 -408.559] [100.810], Avg: [-409.695 -409.695 -409.695] (0.0010) ({r_i: None, r_t: [-745.322 -745.322 -745.322], eps: 0.001})
Step:  116000, Reward: [-374.543 -374.543 -374.543] [62.519], Avg: [-409.664 -409.664 -409.664] (0.0010) ({r_i: None, r_t: [-763.654 -763.654 -763.654], eps: 0.001})
Step:  116100, Reward: [-380.874 -380.874 -380.874] [63.521], Avg: [-409.640 -409.640 -409.640] (0.0010) ({r_i: None, r_t: [-740.069 -740.069 -740.069], eps: 0.001})
Step:  116200, Reward: [-378.834 -378.834 -378.834] [76.490], Avg: [-409.613 -409.613 -409.613] (0.0010) ({r_i: None, r_t: [-752.834 -752.834 -752.834], eps: 0.001})
Step:  116300, Reward: [-343.441 -343.441 -343.441] [49.391], Avg: [-409.556 -409.556 -409.556] (0.0010) ({r_i: None, r_t: [-797.100 -797.100 -797.100], eps: 0.001})
Step:  116400, Reward: [-361.937 -361.937 -361.937] [61.754], Avg: [-409.515 -409.515 -409.515] (0.0010) ({r_i: None, r_t: [-815.215 -815.215 -815.215], eps: 0.001})
Step:  116500, Reward: [-373.064 -373.064 -373.064] [89.513], Avg: [-409.484 -409.484 -409.484] (0.0010) ({r_i: None, r_t: [-721.761 -721.761 -721.761], eps: 0.001})
Step:  116600, Reward: [-388.739 -388.739 -388.739] [64.686], Avg: [-409.466 -409.466 -409.466] (0.0010) ({r_i: None, r_t: [-776.941 -776.941 -776.941], eps: 0.001})
Step:  116700, Reward: [-386.604 -386.604 -386.604] [70.158], Avg: [-409.447 -409.447 -409.447] (0.0010) ({r_i: None, r_t: [-785.171 -785.171 -785.171], eps: 0.001})
Step:  116800, Reward: [-366.153 -366.153 -366.153] [67.484], Avg: [-409.410 -409.410 -409.410] (0.0010) ({r_i: None, r_t: [-730.069 -730.069 -730.069], eps: 0.001})
Step:  116900, Reward: [-407.489 -407.489 -407.489] [77.836], Avg: [-409.408 -409.408 -409.408] (0.0010) ({r_i: None, r_t: [-767.239 -767.239 -767.239], eps: 0.001})
Step:  117000, Reward: [-422.302 -422.302 -422.302] [78.878], Avg: [-409.419 -409.419 -409.419] (0.0010) ({r_i: None, r_t: [-732.530 -732.530 -732.530], eps: 0.001})
Step:  117100, Reward: [-388.644 -388.644 -388.644] [79.713], Avg: [-409.401 -409.401 -409.401] (0.0010) ({r_i: None, r_t: [-744.973 -744.973 -744.973], eps: 0.001})
Step:  117200, Reward: [-376.948 -376.948 -376.948] [75.413], Avg: [-409.374 -409.374 -409.374] (0.0010) ({r_i: None, r_t: [-803.721 -803.721 -803.721], eps: 0.001})
Step:  117300, Reward: [-398.122 -398.122 -398.122] [81.954], Avg: [-409.364 -409.364 -409.364] (0.0010) ({r_i: None, r_t: [-775.947 -775.947 -775.947], eps: 0.001})
Step:  117400, Reward: [-377.418 -377.418 -377.418] [79.815], Avg: [-409.337 -409.337 -409.337] (0.0010) ({r_i: None, r_t: [-771.642 -771.642 -771.642], eps: 0.001})
Step:  117500, Reward: [-386.785 -386.785 -386.785] [56.501], Avg: [-409.318 -409.318 -409.318] (0.0010) ({r_i: None, r_t: [-792.833 -792.833 -792.833], eps: 0.001})
Step:  117600, Reward: [-363.204 -363.204 -363.204] [56.935], Avg: [-409.279 -409.279 -409.279] (0.0010) ({r_i: None, r_t: [-756.127 -756.127 -756.127], eps: 0.001})
Step:  117700, Reward: [-382.035 -382.035 -382.035] [78.293], Avg: [-409.255 -409.255 -409.255] (0.0010) ({r_i: None, r_t: [-756.083 -756.083 -756.083], eps: 0.001})
Step:  117800, Reward: [-387.254 -387.254 -387.254] [78.763], Avg: [-409.237 -409.237 -409.237] (0.0010) ({r_i: None, r_t: [-770.446 -770.446 -770.446], eps: 0.001})
Step:  117900, Reward: [-383.719 -383.719 -383.719] [70.263], Avg: [-409.215 -409.215 -409.215] (0.0010) ({r_i: None, r_t: [-772.797 -772.797 -772.797], eps: 0.001})
Step:  118000, Reward: [-355.378 -355.378 -355.378] [70.357], Avg: [-409.170 -409.170 -409.170] (0.0010) ({r_i: None, r_t: [-742.127 -742.127 -742.127], eps: 0.001})
Step:  118100, Reward: [-400.825 -400.825 -400.825] [73.856], Avg: [-409.162 -409.162 -409.162] (0.0010) ({r_i: None, r_t: [-813.787 -813.787 -813.787], eps: 0.001})
Step:  118200, Reward: [-384.156 -384.156 -384.156] [78.932], Avg: [-409.141 -409.141 -409.141] (0.0010) ({r_i: None, r_t: [-778.255 -778.255 -778.255], eps: 0.001})
Step:  118300, Reward: [-375.022 -375.022 -375.022] [50.785], Avg: [-409.112 -409.112 -409.112] (0.0010) ({r_i: None, r_t: [-772.364 -772.364 -772.364], eps: 0.001})
Step:  118400, Reward: [-400.030 -400.030 -400.030] [66.882], Avg: [-409.105 -409.105 -409.105] (0.0010) ({r_i: None, r_t: [-740.481 -740.481 -740.481], eps: 0.001})
Step:  118500, Reward: [-370.555 -370.555 -370.555] [64.767], Avg: [-409.072 -409.072 -409.072] (0.0010) ({r_i: None, r_t: [-801.864 -801.864 -801.864], eps: 0.001})
Step:  118600, Reward: [-403.301 -403.301 -403.301] [59.909], Avg: [-409.067 -409.067 -409.067] (0.0010) ({r_i: None, r_t: [-757.301 -757.301 -757.301], eps: 0.001})
Step:  118700, Reward: [-369.798 -369.798 -369.798] [45.385], Avg: [-409.034 -409.034 -409.034] (0.0010) ({r_i: None, r_t: [-790.639 -790.639 -790.639], eps: 0.001})
Step:  118800, Reward: [-401.130 -401.130 -401.130] [64.253], Avg: [-409.028 -409.028 -409.028] (0.0010) ({r_i: None, r_t: [-796.115 -796.115 -796.115], eps: 0.001})
Step:  118900, Reward: [-379.645 -379.645 -379.645] [70.840], Avg: [-409.003 -409.003 -409.003] (0.0010) ({r_i: None, r_t: [-810.864 -810.864 -810.864], eps: 0.001})
Step:  119000, Reward: [-368.017 -368.017 -368.017] [50.462], Avg: [-408.969 -408.969 -408.969] (0.0010) ({r_i: None, r_t: [-816.430 -816.430 -816.430], eps: 0.001})
Step:  119100, Reward: [-400.784 -400.784 -400.784] [71.076], Avg: [-408.962 -408.962 -408.962] (0.0010) ({r_i: None, r_t: [-754.155 -754.155 -754.155], eps: 0.001})
Step:  119200, Reward: [-349.963 -349.963 -349.963] [49.690], Avg: [-408.912 -408.912 -408.912] (0.0010) ({r_i: None, r_t: [-769.535 -769.535 -769.535], eps: 0.001})
Step:  119300, Reward: [-378.727 -378.727 -378.727] [47.822], Avg: [-408.887 -408.887 -408.887] (0.0010) ({r_i: None, r_t: [-801.066 -801.066 -801.066], eps: 0.001})
Step:  119400, Reward: [-380.648 -380.648 -380.648] [40.302], Avg: [-408.863 -408.863 -408.863] (0.0010) ({r_i: None, r_t: [-775.701 -775.701 -775.701], eps: 0.001})
Step:  119500, Reward: [-394.802 -394.802 -394.802] [63.327], Avg: [-408.852 -408.852 -408.852] (0.0010) ({r_i: None, r_t: [-792.134 -792.134 -792.134], eps: 0.001})
Step:  119600, Reward: [-393.290 -393.290 -393.290] [61.541], Avg: [-408.839 -408.839 -408.839] (0.0010) ({r_i: None, r_t: [-759.200 -759.200 -759.200], eps: 0.001})
Step:  119700, Reward: [-386.506 -386.506 -386.506] [55.997], Avg: [-408.820 -408.820 -408.820] (0.0010) ({r_i: None, r_t: [-784.143 -784.143 -784.143], eps: 0.001})
Step:  119800, Reward: [-399.641 -399.641 -399.641] [77.584], Avg: [-408.812 -408.812 -408.812] (0.0010) ({r_i: None, r_t: [-774.660 -774.660 -774.660], eps: 0.001})
Step:  119900, Reward: [-416.827 -416.827 -416.827] [57.039], Avg: [-408.819 -408.819 -408.819] (0.0010) ({r_i: None, r_t: [-801.470 -801.470 -801.470], eps: 0.001})
Step:  120000, Reward: [-361.936 -361.936 -361.936] [41.445], Avg: [-408.780 -408.780 -408.780] (0.0010) ({r_i: None, r_t: [-782.273 -782.273 -782.273], eps: 0.001})
Step:  120100, Reward: [-400.020 -400.020 -400.020] [72.322], Avg: [-408.773 -408.773 -408.773] (0.0010) ({r_i: None, r_t: [-755.772 -755.772 -755.772], eps: 0.001})
Step:  120200, Reward: [-380.439 -380.439 -380.439] [53.459], Avg: [-408.749 -408.749 -408.749] (0.0010) ({r_i: None, r_t: [-736.502 -736.502 -736.502], eps: 0.001})
Step:  120300, Reward: [-401.352 -401.352 -401.352] [66.507], Avg: [-408.743 -408.743 -408.743] (0.0010) ({r_i: None, r_t: [-735.834 -735.834 -735.834], eps: 0.001})
Step:  120400, Reward: [-362.966 -362.966 -362.966] [63.762], Avg: [-408.705 -408.705 -408.705] (0.0010) ({r_i: None, r_t: [-763.114 -763.114 -763.114], eps: 0.001})
Step:  120500, Reward: [-367.941 -367.941 -367.941] [61.101], Avg: [-408.671 -408.671 -408.671] (0.0010) ({r_i: None, r_t: [-802.119 -802.119 -802.119], eps: 0.001})
Step:  120600, Reward: [-359.127 -359.127 -359.127] [56.214], Avg: [-408.630 -408.630 -408.630] (0.0010) ({r_i: None, r_t: [-769.338 -769.338 -769.338], eps: 0.001})
Step:  120700, Reward: [-376.289 -376.289 -376.289] [49.620], Avg: [-408.603 -408.603 -408.603] (0.0010) ({r_i: None, r_t: [-763.714 -763.714 -763.714], eps: 0.001})
Step:  120800, Reward: [-376.957 -376.957 -376.957] [57.964], Avg: [-408.577 -408.577 -408.577] (0.0010) ({r_i: None, r_t: [-778.885 -778.885 -778.885], eps: 0.001})
Step:  120900, Reward: [-398.050 -398.050 -398.050] [75.576], Avg: [-408.569 -408.569 -408.569] (0.0010) ({r_i: None, r_t: [-818.678 -818.678 -818.678], eps: 0.001})
Step:  121000, Reward: [-406.250 -406.250 -406.250] [47.396], Avg: [-408.567 -408.567 -408.567] (0.0010) ({r_i: None, r_t: [-781.821 -781.821 -781.821], eps: 0.001})
Step:  121100, Reward: [-371.316 -371.316 -371.316] [58.055], Avg: [-408.536 -408.536 -408.536] (0.0010) ({r_i: None, r_t: [-738.698 -738.698 -738.698], eps: 0.001})
Step:  121200, Reward: [-392.121 -392.121 -392.121] [55.836], Avg: [-408.522 -408.522 -408.522] (0.0010) ({r_i: None, r_t: [-761.530 -761.530 -761.530], eps: 0.001})
Step:  121300, Reward: [-394.575 -394.575 -394.575] [60.700], Avg: [-408.511 -408.511 -408.511] (0.0010) ({r_i: None, r_t: [-771.580 -771.580 -771.580], eps: 0.001})
Step:  121400, Reward: [-393.672 -393.672 -393.672] [69.419], Avg: [-408.499 -408.499 -408.499] (0.0010) ({r_i: None, r_t: [-788.974 -788.974 -788.974], eps: 0.001})
Step:  121500, Reward: [-375.128 -375.128 -375.128] [63.917], Avg: [-408.471 -408.471 -408.471] (0.0010) ({r_i: None, r_t: [-808.066 -808.066 -808.066], eps: 0.001})
Step:  121600, Reward: [-423.093 -423.093 -423.093] [90.667], Avg: [-408.483 -408.483 -408.483] (0.0010) ({r_i: None, r_t: [-797.928 -797.928 -797.928], eps: 0.001})
Step:  121700, Reward: [-387.587 -387.587 -387.587] [74.145], Avg: [-408.466 -408.466 -408.466] (0.0010) ({r_i: None, r_t: [-774.832 -774.832 -774.832], eps: 0.001})
Step:  121800, Reward: [-382.196 -382.196 -382.196] [54.326], Avg: [-408.445 -408.445 -408.445] (0.0010) ({r_i: None, r_t: [-803.152 -803.152 -803.152], eps: 0.001})
Step:  121900, Reward: [-371.442 -371.442 -371.442] [49.391], Avg: [-408.414 -408.414 -408.414] (0.0010) ({r_i: None, r_t: [-817.357 -817.357 -817.357], eps: 0.001})
Step:  122000, Reward: [-365.023 -365.023 -365.023] [54.838], Avg: [-408.379 -408.379 -408.379] (0.0010) ({r_i: None, r_t: [-762.812 -762.812 -762.812], eps: 0.001})
Step:  122100, Reward: [-382.656 -382.656 -382.656] [33.383], Avg: [-408.358 -408.358 -408.358] (0.0010) ({r_i: None, r_t: [-749.835 -749.835 -749.835], eps: 0.001})
Step:  122200, Reward: [-384.972 -384.972 -384.972] [75.090], Avg: [-408.338 -408.338 -408.338] (0.0010) ({r_i: None, r_t: [-769.153 -769.153 -769.153], eps: 0.001})
Step:  122300, Reward: [-418.965 -418.965 -418.965] [66.445], Avg: [-408.347 -408.347 -408.347] (0.0010) ({r_i: None, r_t: [-767.111 -767.111 -767.111], eps: 0.001})
Step:  122400, Reward: [-381.283 -381.283 -381.283] [73.688], Avg: [-408.325 -408.325 -408.325] (0.0010) ({r_i: None, r_t: [-775.980 -775.980 -775.980], eps: 0.001})
Step:  122500, Reward: [-388.387 -388.387 -388.387] [52.102], Avg: [-408.309 -408.309 -408.309] (0.0010) ({r_i: None, r_t: [-760.383 -760.383 -760.383], eps: 0.001})
Step:  122600, Reward: [-348.027 -348.027 -348.027] [56.376], Avg: [-408.260 -408.260 -408.260] (0.0010) ({r_i: None, r_t: [-773.231 -773.231 -773.231], eps: 0.001})
Step:  122700, Reward: [-385.798 -385.798 -385.798] [51.355], Avg: [-408.241 -408.241 -408.241] (0.0010) ({r_i: None, r_t: [-741.876 -741.876 -741.876], eps: 0.001})
Step:  122800, Reward: [-379.118 -379.118 -379.118] [64.863], Avg: [-408.218 -408.218 -408.218] (0.0010) ({r_i: None, r_t: [-739.633 -739.633 -739.633], eps: 0.001})
Step:  122900, Reward: [-356.327 -356.327 -356.327] [52.483], Avg: [-408.176 -408.176 -408.176] (0.0010) ({r_i: None, r_t: [-764.000 -764.000 -764.000], eps: 0.001})
Step:  123000, Reward: [-404.978 -404.978 -404.978] [54.470], Avg: [-408.173 -408.173 -408.173] (0.0010) ({r_i: None, r_t: [-793.429 -793.429 -793.429], eps: 0.001})
Step:  123100, Reward: [-390.598 -390.598 -390.598] [44.880], Avg: [-408.159 -408.159 -408.159] (0.0010) ({r_i: None, r_t: [-754.337 -754.337 -754.337], eps: 0.001})
Step:  123200, Reward: [-383.073 -383.073 -383.073] [61.640], Avg: [-408.138 -408.138 -408.138] (0.0010) ({r_i: None, r_t: [-742.338 -742.338 -742.338], eps: 0.001})
Step:  123300, Reward: [-359.338 -359.338 -359.338] [50.202], Avg: [-408.099 -408.099 -408.099] (0.0010) ({r_i: None, r_t: [-771.084 -771.084 -771.084], eps: 0.001})
Step:  123400, Reward: [-368.819 -368.819 -368.819] [56.489], Avg: [-408.067 -408.067 -408.067] (0.0010) ({r_i: None, r_t: [-769.086 -769.086 -769.086], eps: 0.001})
Step:  123500, Reward: [-364.549 -364.549 -364.549] [45.321], Avg: [-408.032 -408.032 -408.032] (0.0010) ({r_i: None, r_t: [-788.713 -788.713 -788.713], eps: 0.001})
Step:  123600, Reward: [-372.481 -372.481 -372.481] [46.893], Avg: [-408.003 -408.003 -408.003] (0.0010) ({r_i: None, r_t: [-792.399 -792.399 -792.399], eps: 0.001})
Step:  123700, Reward: [-386.808 -386.808 -386.808] [63.239], Avg: [-407.986 -407.986 -407.986] (0.0010) ({r_i: None, r_t: [-740.409 -740.409 -740.409], eps: 0.001})
Step:  123800, Reward: [-414.945 -414.945 -414.945] [67.360], Avg: [-407.991 -407.991 -407.991] (0.0010) ({r_i: None, r_t: [-804.395 -804.395 -804.395], eps: 0.001})
Step:  123900, Reward: [-389.761 -389.761 -389.761] [71.469], Avg: [-407.977 -407.977 -407.977] (0.0010) ({r_i: None, r_t: [-735.606 -735.606 -735.606], eps: 0.001})
Step:  124000, Reward: [-392.959 -392.959 -392.959] [55.992], Avg: [-407.965 -407.965 -407.965] (0.0010) ({r_i: None, r_t: [-761.165 -761.165 -761.165], eps: 0.001})
Step:  124100, Reward: [-399.460 -399.460 -399.460] [78.654], Avg: [-407.958 -407.958 -407.958] (0.0010) ({r_i: None, r_t: [-723.961 -723.961 -723.961], eps: 0.001})
Step:  124200, Reward: [-396.839 -396.839 -396.839] [63.133], Avg: [-407.949 -407.949 -407.949] (0.0010) ({r_i: None, r_t: [-754.995 -754.995 -754.995], eps: 0.001})
Step:  124300, Reward: [-404.374 -404.374 -404.374] [58.543], Avg: [-407.946 -407.946 -407.946] (0.0010) ({r_i: None, r_t: [-767.647 -767.647 -767.647], eps: 0.001})
Step:  124400, Reward: [-404.592 -404.592 -404.592] [79.925], Avg: [-407.943 -407.943 -407.943] (0.0010) ({r_i: None, r_t: [-797.896 -797.896 -797.896], eps: 0.001})
Step:  124500, Reward: [-435.957 -435.957 -435.957] [65.321], Avg: [-407.966 -407.966 -407.966] (0.0010) ({r_i: None, r_t: [-804.447 -804.447 -804.447], eps: 0.001})
Step:  124600, Reward: [-382.722 -382.722 -382.722] [64.018], Avg: [-407.946 -407.946 -407.946] (0.0010) ({r_i: None, r_t: [-757.680 -757.680 -757.680], eps: 0.001})
Step:  124700, Reward: [-372.379 -372.379 -372.379] [68.489], Avg: [-407.917 -407.917 -407.917] (0.0010) ({r_i: None, r_t: [-829.255 -829.255 -829.255], eps: 0.001})
Step:  124800, Reward: [-391.368 -391.368 -391.368] [61.577], Avg: [-407.904 -407.904 -407.904] (0.0010) ({r_i: None, r_t: [-771.295 -771.295 -771.295], eps: 0.001})
Step:  124900, Reward: [-367.071 -367.071 -367.071] [46.049], Avg: [-407.871 -407.871 -407.871] (0.0010) ({r_i: None, r_t: [-815.190 -815.190 -815.190], eps: 0.001})
Step:  125000, Reward: [-408.091 -408.091 -408.091] [74.937], Avg: [-407.871 -407.871 -407.871] (0.0010) ({r_i: None, r_t: [-779.804 -779.804 -779.804], eps: 0.001})
Step:  125100, Reward: [-407.677 -407.677 -407.677] [64.919], Avg: [-407.871 -407.871 -407.871] (0.0010) ({r_i: None, r_t: [-808.076 -808.076 -808.076], eps: 0.001})
Step:  125200, Reward: [-438.692 -438.692 -438.692] [80.246], Avg: [-407.896 -407.896 -407.896] (0.0010) ({r_i: None, r_t: [-783.520 -783.520 -783.520], eps: 0.001})
Step:  125300, Reward: [-372.923 -372.923 -372.923] [64.340], Avg: [-407.868 -407.868 -407.868] (0.0010) ({r_i: None, r_t: [-829.664 -829.664 -829.664], eps: 0.001})
Step:  125400, Reward: [-398.388 -398.388 -398.388] [65.651], Avg: [-407.860 -407.860 -407.860] (0.0010) ({r_i: None, r_t: [-822.035 -822.035 -822.035], eps: 0.001})
Step:  125500, Reward: [-417.113 -417.113 -417.113] [95.463], Avg: [-407.868 -407.868 -407.868] (0.0010) ({r_i: None, r_t: [-792.026 -792.026 -792.026], eps: 0.001})
Step:  125600, Reward: [-418.339 -418.339 -418.339] [80.449], Avg: [-407.876 -407.876 -407.876] (0.0010) ({r_i: None, r_t: [-871.165 -871.165 -871.165], eps: 0.001})
Step:  125700, Reward: [-433.242 -433.242 -433.242] [90.607], Avg: [-407.896 -407.896 -407.896] (0.0010) ({r_i: None, r_t: [-837.173 -837.173 -837.173], eps: 0.001})
Step:  125800, Reward: [-426.093 -426.093 -426.093] [99.351], Avg: [-407.911 -407.911 -407.911] (0.0010) ({r_i: None, r_t: [-804.480 -804.480 -804.480], eps: 0.001})
Step:  125900, Reward: [-416.037 -416.037 -416.037] [98.911], Avg: [-407.917 -407.917 -407.917] (0.0010) ({r_i: None, r_t: [-782.812 -782.812 -782.812], eps: 0.001})
Step:  126000, Reward: [-416.181 -416.181 -416.181] [51.438], Avg: [-407.924 -407.924 -407.924] (0.0010) ({r_i: None, r_t: [-835.921 -835.921 -835.921], eps: 0.001})
Step:  126100, Reward: [-399.970 -399.970 -399.970] [58.097], Avg: [-407.917 -407.917 -407.917] (0.0010) ({r_i: None, r_t: [-787.435 -787.435 -787.435], eps: 0.001})
Step:  126200, Reward: [-404.051 -404.051 -404.051] [49.378], Avg: [-407.914 -407.914 -407.914] (0.0010) ({r_i: None, r_t: [-858.880 -858.880 -858.880], eps: 0.001})
Step:  126300, Reward: [-409.541 -409.541 -409.541] [65.142], Avg: [-407.916 -407.916 -407.916] (0.0010) ({r_i: None, r_t: [-839.051 -839.051 -839.051], eps: 0.001})
Step:  126400, Reward: [-432.253 -432.253 -432.253] [64.511], Avg: [-407.935 -407.935 -407.935] (0.0010) ({r_i: None, r_t: [-803.033 -803.033 -803.033], eps: 0.001})
Step:  126500, Reward: [-415.036 -415.036 -415.036] [80.469], Avg: [-407.940 -407.940 -407.940] (0.0010) ({r_i: None, r_t: [-781.315 -781.315 -781.315], eps: 0.001})
Step:  126600, Reward: [-377.129 -377.129 -377.129] [89.144], Avg: [-407.916 -407.916 -407.916] (0.0010) ({r_i: None, r_t: [-773.051 -773.051 -773.051], eps: 0.001})
Step:  126700, Reward: [-390.686 -390.686 -390.686] [66.336], Avg: [-407.903 -407.903 -407.903] (0.0010) ({r_i: None, r_t: [-782.152 -782.152 -782.152], eps: 0.001})
Step:  126800, Reward: [-417.986 -417.986 -417.986] [76.843], Avg: [-407.910 -407.910 -407.910] (0.0010) ({r_i: None, r_t: [-776.443 -776.443 -776.443], eps: 0.001})
Step:  126900, Reward: [-398.829 -398.829 -398.829] [80.212], Avg: [-407.903 -407.903 -407.903] (0.0010) ({r_i: None, r_t: [-812.733 -812.733 -812.733], eps: 0.001})
Step:  127000, Reward: [-395.167 -395.167 -395.167] [89.520], Avg: [-407.893 -407.893 -407.893] (0.0010) ({r_i: None, r_t: [-795.896 -795.896 -795.896], eps: 0.001})
Step:  127100, Reward: [-396.539 -396.539 -396.539] [74.851], Avg: [-407.884 -407.884 -407.884] (0.0010) ({r_i: None, r_t: [-818.520 -818.520 -818.520], eps: 0.001})
Step:  127200, Reward: [-440.752 -440.752 -440.752] [64.587], Avg: [-407.910 -407.910 -407.910] (0.0010) ({r_i: None, r_t: [-774.426 -774.426 -774.426], eps: 0.001})
Step:  127300, Reward: [-411.986 -411.986 -411.986] [85.893], Avg: [-407.913 -407.913 -407.913] (0.0010) ({r_i: None, r_t: [-749.238 -749.238 -749.238], eps: 0.001})
Step:  127400, Reward: [-404.669 -404.669 -404.669] [75.236], Avg: [-407.911 -407.911 -407.911] (0.0010) ({r_i: None, r_t: [-770.788 -770.788 -770.788], eps: 0.001})
Step:  127500, Reward: [-377.064 -377.064 -377.064] [71.216], Avg: [-407.887 -407.887 -407.887] (0.0010) ({r_i: None, r_t: [-768.060 -768.060 -768.060], eps: 0.001})
Step:  127600, Reward: [-442.044 -442.044 -442.044] [78.311], Avg: [-407.913 -407.913 -407.913] (0.0010) ({r_i: None, r_t: [-785.871 -785.871 -785.871], eps: 0.001})
Step:  127700, Reward: [-385.379 -385.379 -385.379] [67.519], Avg: [-407.896 -407.896 -407.896] (0.0010) ({r_i: None, r_t: [-820.029 -820.029 -820.029], eps: 0.001})
Step:  127800, Reward: [-425.128 -425.128 -425.128] [65.051], Avg: [-407.909 -407.909 -407.909] (0.0010) ({r_i: None, r_t: [-853.792 -853.792 -853.792], eps: 0.001})
Step:  127900, Reward: [-417.624 -417.624 -417.624] [65.936], Avg: [-407.917 -407.917 -407.917] (0.0010) ({r_i: None, r_t: [-773.359 -773.359 -773.359], eps: 0.001})
Step:  128000, Reward: [-422.143 -422.143 -422.143] [76.478], Avg: [-407.928 -407.928 -407.928] (0.0010) ({r_i: None, r_t: [-837.129 -837.129 -837.129], eps: 0.001})
Step:  128100, Reward: [-408.073 -408.073 -408.073] [81.648], Avg: [-407.928 -407.928 -407.928] (0.0010) ({r_i: None, r_t: [-788.067 -788.067 -788.067], eps: 0.001})
Step:  128200, Reward: [-394.087 -394.087 -394.087] [77.379], Avg: [-407.917 -407.917 -407.917] (0.0010) ({r_i: None, r_t: [-814.166 -814.166 -814.166], eps: 0.001})
Step:  128300, Reward: [-387.356 -387.356 -387.356] [57.039], Avg: [-407.901 -407.901 -407.901] (0.0010) ({r_i: None, r_t: [-784.382 -784.382 -784.382], eps: 0.001})
Step:  128400, Reward: [-388.907 -388.907 -388.907] [61.289], Avg: [-407.886 -407.886 -407.886] (0.0010) ({r_i: None, r_t: [-822.596 -822.596 -822.596], eps: 0.001})
Step:  128500, Reward: [-395.888 -395.888 -395.888] [77.765], Avg: [-407.877 -407.877 -407.877] (0.0010) ({r_i: None, r_t: [-790.259 -790.259 -790.259], eps: 0.001})
Step:  128600, Reward: [-378.457 -378.457 -378.457] [102.309], Avg: [-407.854 -407.854 -407.854] (0.0010) ({r_i: None, r_t: [-757.203 -757.203 -757.203], eps: 0.001})
Step:  128700, Reward: [-387.050 -387.050 -387.050] [47.923], Avg: [-407.838 -407.838 -407.838] (0.0010) ({r_i: None, r_t: [-823.962 -823.962 -823.962], eps: 0.001})
Step:  128800, Reward: [-402.794 -402.794 -402.794] [58.904], Avg: [-407.834 -407.834 -407.834] (0.0010) ({r_i: None, r_t: [-792.799 -792.799 -792.799], eps: 0.001})
Step:  128900, Reward: [-393.331 -393.331 -393.331] [63.504], Avg: [-407.823 -407.823 -407.823] (0.0010) ({r_i: None, r_t: [-760.493 -760.493 -760.493], eps: 0.001})
Step:  129000, Reward: [-399.165 -399.165 -399.165] [54.973], Avg: [-407.816 -407.816 -407.816] (0.0010) ({r_i: None, r_t: [-787.682 -787.682 -787.682], eps: 0.001})
Step:  129100, Reward: [-409.262 -409.262 -409.262] [57.021], Avg: [-407.817 -407.817 -407.817] (0.0010) ({r_i: None, r_t: [-768.605 -768.605 -768.605], eps: 0.001})
Step:  129200, Reward: [-433.012 -433.012 -433.012] [81.460], Avg: [-407.837 -407.837 -407.837] (0.0010) ({r_i: None, r_t: [-806.439 -806.439 -806.439], eps: 0.001})
Step:  129300, Reward: [-381.821 -381.821 -381.821] [71.963], Avg: [-407.817 -407.817 -407.817] (0.0010) ({r_i: None, r_t: [-776.269 -776.269 -776.269], eps: 0.001})
Step:  129400, Reward: [-402.133 -402.133 -402.133] [75.771], Avg: [-407.812 -407.812 -407.812] (0.0010) ({r_i: None, r_t: [-811.175 -811.175 -811.175], eps: 0.001})
Step:  129500, Reward: [-389.902 -389.902 -389.902] [72.683], Avg: [-407.799 -407.799 -407.799] (0.0010) ({r_i: None, r_t: [-795.648 -795.648 -795.648], eps: 0.001})
Step:  129600, Reward: [-404.452 -404.452 -404.452] [82.693], Avg: [-407.796 -407.796 -407.796] (0.0010) ({r_i: None, r_t: [-813.771 -813.771 -813.771], eps: 0.001})
Step:  129700, Reward: [-427.243 -427.243 -427.243] [50.548], Avg: [-407.811 -407.811 -407.811] (0.0010) ({r_i: None, r_t: [-817.003 -817.003 -817.003], eps: 0.001})
Step:  129800, Reward: [-398.557 -398.557 -398.557] [82.628], Avg: [-407.804 -407.804 -407.804] (0.0010) ({r_i: None, r_t: [-797.991 -797.991 -797.991], eps: 0.001})
Step:  129900, Reward: [-400.543 -400.543 -400.543] [70.549], Avg: [-407.798 -407.798 -407.798] (0.0010) ({r_i: None, r_t: [-797.090 -797.090 -797.090], eps: 0.001})
Step:  130000, Reward: [-421.565 -421.565 -421.565] [61.668], Avg: [-407.809 -407.809 -407.809] (0.0010) ({r_i: None, r_t: [-779.036 -779.036 -779.036], eps: 0.001})
Step:  130100, Reward: [-390.748 -390.748 -390.748] [90.411], Avg: [-407.796 -407.796 -407.796] (0.0010) ({r_i: None, r_t: [-782.921 -782.921 -782.921], eps: 0.001})
Step:  130200, Reward: [-423.433 -423.433 -423.433] [93.646], Avg: [-407.808 -407.808 -407.808] (0.0010) ({r_i: None, r_t: [-816.883 -816.883 -816.883], eps: 0.001})
Step:  130300, Reward: [-393.357 -393.357 -393.357] [82.565], Avg: [-407.797 -407.797 -407.797] (0.0010) ({r_i: None, r_t: [-796.057 -796.057 -796.057], eps: 0.001})
Step:  130400, Reward: [-398.020 -398.020 -398.020] [93.036], Avg: [-407.789 -407.789 -407.789] (0.0010) ({r_i: None, r_t: [-859.312 -859.312 -859.312], eps: 0.001})
Step:  130500, Reward: [-422.026 -422.026 -422.026] [79.668], Avg: [-407.800 -407.800 -407.800] (0.0010) ({r_i: None, r_t: [-861.874 -861.874 -861.874], eps: 0.001})
Step:  130600, Reward: [-435.431 -435.431 -435.431] [72.227], Avg: [-407.821 -407.821 -407.821] (0.0010) ({r_i: None, r_t: [-742.105 -742.105 -742.105], eps: 0.001})
Step:  130700, Reward: [-420.543 -420.543 -420.543] [52.147], Avg: [-407.831 -407.831 -407.831] (0.0010) ({r_i: None, r_t: [-830.254 -830.254 -830.254], eps: 0.001})
Step:  130800, Reward: [-433.692 -433.692 -433.692] [71.489], Avg: [-407.851 -407.851 -407.851] (0.0010) ({r_i: None, r_t: [-807.537 -807.537 -807.537], eps: 0.001})
Step:  130900, Reward: [-401.081 -401.081 -401.081] [83.453], Avg: [-407.846 -407.846 -407.846] (0.0010) ({r_i: None, r_t: [-819.406 -819.406 -819.406], eps: 0.001})
Step:  131000, Reward: [-401.058 -401.058 -401.058] [72.574], Avg: [-407.840 -407.840 -407.840] (0.0010) ({r_i: None, r_t: [-810.927 -810.927 -810.927], eps: 0.001})
Step:  131100, Reward: [-410.108 -410.108 -410.108] [62.674], Avg: [-407.842 -407.842 -407.842] (0.0010) ({r_i: None, r_t: [-833.525 -833.525 -833.525], eps: 0.001})
Step:  131200, Reward: [-399.147 -399.147 -399.147] [70.958], Avg: [-407.835 -407.835 -407.835] (0.0010) ({r_i: None, r_t: [-833.222 -833.222 -833.222], eps: 0.001})
Step:  131300, Reward: [-419.216 -419.216 -419.216] [54.077], Avg: [-407.844 -407.844 -407.844] (0.0010) ({r_i: None, r_t: [-788.699 -788.699 -788.699], eps: 0.001})
Step:  131400, Reward: [-411.731 -411.731 -411.731] [79.306], Avg: [-407.847 -407.847 -407.847] (0.0010) ({r_i: None, r_t: [-837.001 -837.001 -837.001], eps: 0.001})
Step:  131500, Reward: [-412.863 -412.863 -412.863] [75.183], Avg: [-407.851 -407.851 -407.851] (0.0010) ({r_i: None, r_t: [-809.448 -809.448 -809.448], eps: 0.001})
Step:  131600, Reward: [-418.810 -418.810 -418.810] [57.158], Avg: [-407.859 -407.859 -407.859] (0.0010) ({r_i: None, r_t: [-791.154 -791.154 -791.154], eps: 0.001})
Step:  131700, Reward: [-409.265 -409.265 -409.265] [79.649], Avg: [-407.860 -407.860 -407.860] (0.0010) ({r_i: None, r_t: [-858.659 -858.659 -858.659], eps: 0.001})
Step:  131800, Reward: [-392.616 -392.616 -392.616] [69.070], Avg: [-407.849 -407.849 -407.849] (0.0010) ({r_i: None, r_t: [-833.635 -833.635 -833.635], eps: 0.001})
Step:  131900, Reward: [-402.550 -402.550 -402.550] [79.285], Avg: [-407.845 -407.845 -407.845] (0.0010) ({r_i: None, r_t: [-858.542 -858.542 -858.542], eps: 0.001})
Step:  132000, Reward: [-382.210 -382.210 -382.210] [58.443], Avg: [-407.825 -407.825 -407.825] (0.0010) ({r_i: None, r_t: [-828.208 -828.208 -828.208], eps: 0.001})
Step:  132100, Reward: [-388.715 -388.715 -388.715] [60.503], Avg: [-407.811 -407.811 -407.811] (0.0010) ({r_i: None, r_t: [-766.404 -766.404 -766.404], eps: 0.001})
Step:  132200, Reward: [-411.963 -411.963 -411.963] [86.987], Avg: [-407.814 -407.814 -407.814] (0.0010) ({r_i: None, r_t: [-806.631 -806.631 -806.631], eps: 0.001})
Step:  132300, Reward: [-422.719 -422.719 -422.719] [75.977], Avg: [-407.825 -407.825 -407.825] (0.0010) ({r_i: None, r_t: [-816.811 -816.811 -816.811], eps: 0.001})
Step:  132400, Reward: [-430.011 -430.011 -430.011] [76.180], Avg: [-407.842 -407.842 -407.842] (0.0010) ({r_i: None, r_t: [-846.158 -846.158 -846.158], eps: 0.001})
Step:  132500, Reward: [-390.845 -390.845 -390.845] [74.389], Avg: [-407.829 -407.829 -407.829] (0.0010) ({r_i: None, r_t: [-836.605 -836.605 -836.605], eps: 0.001})
Step:  132600, Reward: [-394.775 -394.775 -394.775] [82.644], Avg: [-407.819 -407.819 -407.819] (0.0010) ({r_i: None, r_t: [-799.694 -799.694 -799.694], eps: 0.001})
Step:  132700, Reward: [-440.998 -440.998 -440.998] [125.350], Avg: [-407.844 -407.844 -407.844] (0.0010) ({r_i: None, r_t: [-857.694 -857.694 -857.694], eps: 0.001})
Step:  132800, Reward: [-402.079 -402.079 -402.079] [70.740], Avg: [-407.840 -407.840 -407.840] (0.0010) ({r_i: None, r_t: [-829.849 -829.849 -829.849], eps: 0.001})
Step:  132900, Reward: [-397.160 -397.160 -397.160] [72.372], Avg: [-407.832 -407.832 -407.832] (0.0010) ({r_i: None, r_t: [-823.391 -823.391 -823.391], eps: 0.001})
Step:  133000, Reward: [-369.473 -369.473 -369.473] [73.557], Avg: [-407.803 -407.803 -407.803] (0.0010) ({r_i: None, r_t: [-781.632 -781.632 -781.632], eps: 0.001})
Step:  133100, Reward: [-415.373 -415.373 -415.373] [63.977], Avg: [-407.809 -407.809 -407.809] (0.0010) ({r_i: None, r_t: [-845.091 -845.091 -845.091], eps: 0.001})
Step:  133200, Reward: [-398.098 -398.098 -398.098] [78.914], Avg: [-407.802 -407.802 -407.802] (0.0010) ({r_i: None, r_t: [-790.235 -790.235 -790.235], eps: 0.001})
Step:  133300, Reward: [-376.104 -376.104 -376.104] [40.372], Avg: [-407.778 -407.778 -407.778] (0.0010) ({r_i: None, r_t: [-817.635 -817.635 -817.635], eps: 0.001})
Step:  133400, Reward: [-419.510 -419.510 -419.510] [73.739], Avg: [-407.787 -407.787 -407.787] (0.0010) ({r_i: None, r_t: [-769.265 -769.265 -769.265], eps: 0.001})
Step:  133500, Reward: [-395.024 -395.024 -395.024] [71.287], Avg: [-407.777 -407.777 -407.777] (0.0010) ({r_i: None, r_t: [-781.841 -781.841 -781.841], eps: 0.001})
Step:  133600, Reward: [-399.675 -399.675 -399.675] [83.303], Avg: [-407.771 -407.771 -407.771] (0.0010) ({r_i: None, r_t: [-814.404 -814.404 -814.404], eps: 0.001})
Step:  133700, Reward: [-415.924 -415.924 -415.924] [80.338], Avg: [-407.777 -407.777 -407.777] (0.0010) ({r_i: None, r_t: [-758.230 -758.230 -758.230], eps: 0.001})
Step:  133800, Reward: [-431.749 -431.749 -431.749] [114.200], Avg: [-407.795 -407.795 -407.795] (0.0010) ({r_i: None, r_t: [-793.952 -793.952 -793.952], eps: 0.001})
Step:  133900, Reward: [-413.220 -413.220 -413.220] [80.942], Avg: [-407.799 -407.799 -407.799] (0.0010) ({r_i: None, r_t: [-828.998 -828.998 -828.998], eps: 0.001})
Step:  134000, Reward: [-480.057 -480.057 -480.057] [93.555], Avg: [-407.853 -407.853 -407.853] (0.0010) ({r_i: None, r_t: [-837.850 -837.850 -837.850], eps: 0.001})
Step:  134100, Reward: [-381.802 -381.802 -381.802] [50.413], Avg: [-407.833 -407.833 -407.833] (0.0010) ({r_i: None, r_t: [-847.402 -847.402 -847.402], eps: 0.001})
Step:  134200, Reward: [-388.100 -388.100 -388.100] [57.954], Avg: [-407.819 -407.819 -407.819] (0.0010) ({r_i: None, r_t: [-845.625 -845.625 -845.625], eps: 0.001})
Step:  134300, Reward: [-402.958 -402.958 -402.958] [48.118], Avg: [-407.815 -407.815 -407.815] (0.0010) ({r_i: None, r_t: [-786.284 -786.284 -786.284], eps: 0.001})
Step:  134400, Reward: [-445.675 -445.675 -445.675] [86.549], Avg: [-407.843 -407.843 -407.843] (0.0010) ({r_i: None, r_t: [-785.202 -785.202 -785.202], eps: 0.001})
Step:  134500, Reward: [-402.515 -402.515 -402.515] [59.644], Avg: [-407.839 -407.839 -407.839] (0.0010) ({r_i: None, r_t: [-826.460 -826.460 -826.460], eps: 0.001})
Step:  134600, Reward: [-401.309 -401.309 -401.309] [61.797], Avg: [-407.834 -407.834 -407.834] (0.0010) ({r_i: None, r_t: [-772.333 -772.333 -772.333], eps: 0.001})
Step:  134700, Reward: [-431.424 -431.424 -431.424] [111.965], Avg: [-407.852 -407.852 -407.852] (0.0010) ({r_i: None, r_t: [-772.020 -772.020 -772.020], eps: 0.001})
Step:  134800, Reward: [-407.381 -407.381 -407.381] [65.251], Avg: [-407.852 -407.852 -407.852] (0.0010) ({r_i: None, r_t: [-843.695 -843.695 -843.695], eps: 0.001})
Step:  134900, Reward: [-433.967 -433.967 -433.967] [79.044], Avg: [-407.871 -407.871 -407.871] (0.0010) ({r_i: None, r_t: [-855.310 -855.310 -855.310], eps: 0.001})
Step:  135000, Reward: [-403.034 -403.034 -403.034] [58.407], Avg: [-407.867 -407.867 -407.867] (0.0010) ({r_i: None, r_t: [-795.308 -795.308 -795.308], eps: 0.001})
Step:  135100, Reward: [-402.411 -402.411 -402.411] [56.028], Avg: [-407.863 -407.863 -407.863] (0.0010) ({r_i: None, r_t: [-879.760 -879.760 -879.760], eps: 0.001})
Step:  135200, Reward: [-389.144 -389.144 -389.144] [55.361], Avg: [-407.850 -407.850 -407.850] (0.0010) ({r_i: None, r_t: [-838.296 -838.296 -838.296], eps: 0.001})
Step:  135300, Reward: [-438.592 -438.592 -438.592] [66.993], Avg: [-407.872 -407.872 -407.872] (0.0010) ({r_i: None, r_t: [-824.481 -824.481 -824.481], eps: 0.001})
Step:  135400, Reward: [-421.071 -421.071 -421.071] [86.865], Avg: [-407.882 -407.882 -407.882] (0.0010) ({r_i: None, r_t: [-779.795 -779.795 -779.795], eps: 0.001})
Step:  135500, Reward: [-377.769 -377.769 -377.769] [50.305], Avg: [-407.860 -407.860 -407.860] (0.0010) ({r_i: None, r_t: [-841.336 -841.336 -841.336], eps: 0.001})
Step:  135600, Reward: [-419.028 -419.028 -419.028] [74.910], Avg: [-407.868 -407.868 -407.868] (0.0010) ({r_i: None, r_t: [-838.070 -838.070 -838.070], eps: 0.001})
Step:  135700, Reward: [-409.289 -409.289 -409.289] [89.705], Avg: [-407.869 -407.869 -407.869] (0.0010) ({r_i: None, r_t: [-794.102 -794.102 -794.102], eps: 0.001})
Step:  135800, Reward: [-390.644 -390.644 -390.644] [93.255], Avg: [-407.856 -407.856 -407.856] (0.0010) ({r_i: None, r_t: [-842.354 -842.354 -842.354], eps: 0.001})
Step:  135900, Reward: [-415.584 -415.584 -415.584] [99.514], Avg: [-407.862 -407.862 -407.862] (0.0010) ({r_i: None, r_t: [-835.924 -835.924 -835.924], eps: 0.001})
Step:  136000, Reward: [-408.892 -408.892 -408.892] [66.349], Avg: [-407.863 -407.863 -407.863] (0.0010) ({r_i: None, r_t: [-826.558 -826.558 -826.558], eps: 0.001})
Step:  136100, Reward: [-414.897 -414.897 -414.897] [85.273], Avg: [-407.868 -407.868 -407.868] (0.0010) ({r_i: None, r_t: [-815.282 -815.282 -815.282], eps: 0.001})
Step:  136200, Reward: [-390.017 -390.017 -390.017] [69.839], Avg: [-407.855 -407.855 -407.855] (0.0010) ({r_i: None, r_t: [-817.097 -817.097 -817.097], eps: 0.001})
Step:  136300, Reward: [-384.519 -384.519 -384.519] [80.451], Avg: [-407.838 -407.838 -407.838] (0.0010) ({r_i: None, r_t: [-810.809 -810.809 -810.809], eps: 0.001})
Step:  136400, Reward: [-429.529 -429.529 -429.529] [57.962], Avg: [-407.854 -407.854 -407.854] (0.0010) ({r_i: None, r_t: [-782.857 -782.857 -782.857], eps: 0.001})
Step:  136500, Reward: [-416.214 -416.214 -416.214] [76.363], Avg: [-407.860 -407.860 -407.860] (0.0010) ({r_i: None, r_t: [-834.776 -834.776 -834.776], eps: 0.001})
Step:  136600, Reward: [-423.078 -423.078 -423.078] [68.917], Avg: [-407.871 -407.871 -407.871] (0.0010) ({r_i: None, r_t: [-797.797 -797.797 -797.797], eps: 0.001})
Step:  136700, Reward: [-388.784 -388.784 -388.784] [63.555], Avg: [-407.857 -407.857 -407.857] (0.0010) ({r_i: None, r_t: [-793.060 -793.060 -793.060], eps: 0.001})
Step:  136800, Reward: [-385.898 -385.898 -385.898] [73.128], Avg: [-407.841 -407.841 -407.841] (0.0010) ({r_i: None, r_t: [-757.724 -757.724 -757.724], eps: 0.001})
Step:  136900, Reward: [-362.573 -362.573 -362.573] [85.151], Avg: [-407.808 -407.808 -407.808] (0.0010) ({r_i: None, r_t: [-784.594 -784.594 -784.594], eps: 0.001})
Step:  137000, Reward: [-408.332 -408.332 -408.332] [75.235], Avg: [-407.808 -407.808 -407.808] (0.0010) ({r_i: None, r_t: [-806.183 -806.183 -806.183], eps: 0.001})
Step:  137100, Reward: [-362.110 -362.110 -362.110] [45.021], Avg: [-407.775 -407.775 -407.775] (0.0010) ({r_i: None, r_t: [-839.205 -839.205 -839.205], eps: 0.001})
Step:  137200, Reward: [-354.843 -354.843 -354.843] [61.610], Avg: [-407.736 -407.736 -407.736] (0.0010) ({r_i: None, r_t: [-742.516 -742.516 -742.516], eps: 0.001})
Step:  137300, Reward: [-413.823 -413.823 -413.823] [68.723], Avg: [-407.741 -407.741 -407.741] (0.0010) ({r_i: None, r_t: [-828.313 -828.313 -828.313], eps: 0.001})
Step:  137400, Reward: [-374.191 -374.191 -374.191] [58.861], Avg: [-407.716 -407.716 -407.716] (0.0010) ({r_i: None, r_t: [-773.393 -773.393 -773.393], eps: 0.001})
Step:  137500, Reward: [-395.217 -395.217 -395.217] [85.245], Avg: [-407.707 -407.707 -407.707] (0.0010) ({r_i: None, r_t: [-823.514 -823.514 -823.514], eps: 0.001})
Step:  137600, Reward: [-403.099 -403.099 -403.099] [59.546], Avg: [-407.704 -407.704 -407.704] (0.0010) ({r_i: None, r_t: [-802.815 -802.815 -802.815], eps: 0.001})
Step:  137700, Reward: [-424.376 -424.376 -424.376] [77.604], Avg: [-407.716 -407.716 -407.716] (0.0010) ({r_i: None, r_t: [-761.730 -761.730 -761.730], eps: 0.001})
Step:  137800, Reward: [-375.935 -375.935 -375.935] [47.444], Avg: [-407.693 -407.693 -407.693] (0.0010) ({r_i: None, r_t: [-789.323 -789.323 -789.323], eps: 0.001})
Step:  137900, Reward: [-397.754 -397.754 -397.754] [89.307], Avg: [-407.686 -407.686 -407.686] (0.0010) ({r_i: None, r_t: [-791.642 -791.642 -791.642], eps: 0.001})
Step:  138000, Reward: [-389.594 -389.594 -389.594] [73.327], Avg: [-407.673 -407.673 -407.673] (0.0010) ({r_i: None, r_t: [-773.489 -773.489 -773.489], eps: 0.001})
Step:  138100, Reward: [-377.372 -377.372 -377.372] [47.707], Avg: [-407.651 -407.651 -407.651] (0.0010) ({r_i: None, r_t: [-790.399 -790.399 -790.399], eps: 0.001})
Step:  138200, Reward: [-355.835 -355.835 -355.835] [78.664], Avg: [-407.613 -407.613 -407.613] (0.0010) ({r_i: None, r_t: [-729.710 -729.710 -729.710], eps: 0.001})
Step:  138300, Reward: [-397.306 -397.306 -397.306] [70.601], Avg: [-407.606 -407.606 -407.606] (0.0010) ({r_i: None, r_t: [-752.346 -752.346 -752.346], eps: 0.001})
Step:  138400, Reward: [-402.548 -402.548 -402.548] [80.193], Avg: [-407.602 -407.602 -407.602] (0.0010) ({r_i: None, r_t: [-762.628 -762.628 -762.628], eps: 0.001})
Step:  138500, Reward: [-416.783 -416.783 -416.783] [60.834], Avg: [-407.609 -407.609 -407.609] (0.0010) ({r_i: None, r_t: [-744.451 -744.451 -744.451], eps: 0.001})
Step:  138600, Reward: [-357.331 -357.331 -357.331] [48.422], Avg: [-407.573 -407.573 -407.573] (0.0010) ({r_i: None, r_t: [-731.633 -731.633 -731.633], eps: 0.001})
Step:  138700, Reward: [-398.141 -398.141 -398.141] [58.251], Avg: [-407.566 -407.566 -407.566] (0.0010) ({r_i: None, r_t: [-741.945 -741.945 -741.945], eps: 0.001})
Step:  138800, Reward: [-386.107 -386.107 -386.107] [59.375], Avg: [-407.550 -407.550 -407.550] (0.0010) ({r_i: None, r_t: [-782.285 -782.285 -782.285], eps: 0.001})
Step:  138900, Reward: [-385.553 -385.553 -385.553] [64.570], Avg: [-407.535 -407.535 -407.535] (0.0010) ({r_i: None, r_t: [-733.384 -733.384 -733.384], eps: 0.001})
Step:  139000, Reward: [-359.359 -359.359 -359.359] [80.354], Avg: [-407.500 -407.500 -407.500] (0.0010) ({r_i: None, r_t: [-738.714 -738.714 -738.714], eps: 0.001})
Step:  139100, Reward: [-368.595 -368.595 -368.595] [53.599], Avg: [-407.472 -407.472 -407.472] (0.0010) ({r_i: None, r_t: [-763.620 -763.620 -763.620], eps: 0.001})
Step:  139200, Reward: [-406.988 -406.988 -406.988] [78.491], Avg: [-407.472 -407.472 -407.472] (0.0010) ({r_i: None, r_t: [-742.325 -742.325 -742.325], eps: 0.001})
Step:  139300, Reward: [-366.749 -366.749 -366.749] [54.916], Avg: [-407.442 -407.442 -407.442] (0.0010) ({r_i: None, r_t: [-719.496 -719.496 -719.496], eps: 0.001})
Step:  139400, Reward: [-387.392 -387.392 -387.392] [54.126], Avg: [-407.428 -407.428 -407.428] (0.0010) ({r_i: None, r_t: [-784.709 -784.709 -784.709], eps: 0.001})
Step:  139500, Reward: [-375.907 -375.907 -375.907] [55.974], Avg: [-407.405 -407.405 -407.405] (0.0010) ({r_i: None, r_t: [-781.183 -781.183 -781.183], eps: 0.001})
Step:  139600, Reward: [-372.245 -372.245 -372.245] [84.889], Avg: [-407.380 -407.380 -407.380] (0.0010) ({r_i: None, r_t: [-753.609 -753.609 -753.609], eps: 0.001})
Step:  139700, Reward: [-371.211 -371.211 -371.211] [60.954], Avg: [-407.354 -407.354 -407.354] (0.0010) ({r_i: None, r_t: [-769.467 -769.467 -769.467], eps: 0.001})
Step:  139800, Reward: [-394.514 -394.514 -394.514] [72.850], Avg: [-407.345 -407.345 -407.345] (0.0010) ({r_i: None, r_t: [-727.986 -727.986 -727.986], eps: 0.001})
Step:  139900, Reward: [-383.483 -383.483 -383.483] [69.574], Avg: [-407.328 -407.328 -407.328] (0.0010) ({r_i: None, r_t: [-728.209 -728.209 -728.209], eps: 0.001})
Step:  140000, Reward: [-390.639 -390.639 -390.639] [66.564], Avg: [-407.316 -407.316 -407.316] (0.0010) ({r_i: None, r_t: [-797.773 -797.773 -797.773], eps: 0.001})
Step:  140100, Reward: [-349.718 -349.718 -349.718] [80.635], Avg: [-407.275 -407.275 -407.275] (0.0010) ({r_i: None, r_t: [-736.165 -736.165 -736.165], eps: 0.001})
Step:  140200, Reward: [-395.780 -395.780 -395.780] [70.688], Avg: [-407.267 -407.267 -407.267] (0.0010) ({r_i: None, r_t: [-728.506 -728.506 -728.506], eps: 0.001})
Step:  140300, Reward: [-383.756 -383.756 -383.756] [56.512], Avg: [-407.250 -407.250 -407.250] (0.0010) ({r_i: None, r_t: [-749.992 -749.992 -749.992], eps: 0.001})
Step:  140400, Reward: [-386.164 -386.164 -386.164] [81.535], Avg: [-407.235 -407.235 -407.235] (0.0010) ({r_i: None, r_t: [-739.655 -739.655 -739.655], eps: 0.001})
Step:  140500, Reward: [-363.237 -363.237 -363.237] [51.511], Avg: [-407.204 -407.204 -407.204] (0.0010) ({r_i: None, r_t: [-793.084 -793.084 -793.084], eps: 0.001})
Step:  140600, Reward: [-377.070 -377.070 -377.070] [57.448], Avg: [-407.183 -407.183 -407.183] (0.0010) ({r_i: None, r_t: [-749.915 -749.915 -749.915], eps: 0.001})
Step:  140700, Reward: [-370.383 -370.383 -370.383] [67.204], Avg: [-407.156 -407.156 -407.156] (0.0010) ({r_i: None, r_t: [-751.043 -751.043 -751.043], eps: 0.001})
Step:  140800, Reward: [-370.636 -370.636 -370.636] [79.721], Avg: [-407.130 -407.130 -407.130] (0.0010) ({r_i: None, r_t: [-767.703 -767.703 -767.703], eps: 0.001})
Step:  140900, Reward: [-348.895 -348.895 -348.895] [66.018], Avg: [-407.089 -407.089 -407.089] (0.0010) ({r_i: None, r_t: [-739.550 -739.550 -739.550], eps: 0.001})
Step:  141000, Reward: [-385.117 -385.117 -385.117] [71.441], Avg: [-407.074 -407.074 -407.074] (0.0010) ({r_i: None, r_t: [-743.232 -743.232 -743.232], eps: 0.001})
Step:  141100, Reward: [-358.416 -358.416 -358.416] [61.747], Avg: [-407.039 -407.039 -407.039] (0.0010) ({r_i: None, r_t: [-731.758 -731.758 -731.758], eps: 0.001})
Step:  141200, Reward: [-355.648 -355.648 -355.648] [74.819], Avg: [-407.003 -407.003 -407.003] (0.0010) ({r_i: None, r_t: [-717.034 -717.034 -717.034], eps: 0.001})
Step:  141300, Reward: [-387.915 -387.915 -387.915] [75.481], Avg: [-406.989 -406.989 -406.989] (0.0010) ({r_i: None, r_t: [-752.855 -752.855 -752.855], eps: 0.001})
Step:  141400, Reward: [-380.021 -380.021 -380.021] [81.306], Avg: [-406.970 -406.970 -406.970] (0.0010) ({r_i: None, r_t: [-744.969 -744.969 -744.969], eps: 0.001})
Step:  141500, Reward: [-395.148 -395.148 -395.148] [66.412], Avg: [-406.962 -406.962 -406.962] (0.0010) ({r_i: None, r_t: [-729.466 -729.466 -729.466], eps: 0.001})
Step:  141600, Reward: [-390.870 -390.870 -390.870] [59.748], Avg: [-406.951 -406.951 -406.951] (0.0010) ({r_i: None, r_t: [-715.584 -715.584 -715.584], eps: 0.001})
Step:  141700, Reward: [-371.539 -371.539 -371.539] [67.674], Avg: [-406.926 -406.926 -406.926] (0.0010) ({r_i: None, r_t: [-757.197 -757.197 -757.197], eps: 0.001})
Step:  141800, Reward: [-347.730 -347.730 -347.730] [57.745], Avg: [-406.884 -406.884 -406.884] (0.0010) ({r_i: None, r_t: [-776.713 -776.713 -776.713], eps: 0.001})
Step:  141900, Reward: [-410.840 -410.840 -410.840] [62.140], Avg: [-406.887 -406.887 -406.887] (0.0010) ({r_i: None, r_t: [-767.562 -767.562 -767.562], eps: 0.001})
Step:  142000, Reward: [-344.376 -344.376 -344.376] [46.266], Avg: [-406.843 -406.843 -406.843] (0.0010) ({r_i: None, r_t: [-753.942 -753.942 -753.942], eps: 0.001})
Step:  142100, Reward: [-360.680 -360.680 -360.680] [69.669], Avg: [-406.810 -406.810 -406.810] (0.0010) ({r_i: None, r_t: [-727.290 -727.290 -727.290], eps: 0.001})
Step:  142200, Reward: [-363.276 -363.276 -363.276] [63.615], Avg: [-406.780 -406.780 -406.780] (0.0010) ({r_i: None, r_t: [-759.278 -759.278 -759.278], eps: 0.001})
Step:  142300, Reward: [-386.565 -386.565 -386.565] [76.357], Avg: [-406.765 -406.765 -406.765] (0.0010) ({r_i: None, r_t: [-770.063 -770.063 -770.063], eps: 0.001})
Step:  142400, Reward: [-364.513 -364.513 -364.513] [59.495], Avg: [-406.736 -406.736 -406.736] (0.0010) ({r_i: None, r_t: [-714.515 -714.515 -714.515], eps: 0.001})
Step:  142500, Reward: [-317.448 -317.448 -317.448] [38.831], Avg: [-406.673 -406.673 -406.673] (0.0010) ({r_i: None, r_t: [-741.997 -741.997 -741.997], eps: 0.001})
Step:  142600, Reward: [-365.224 -365.224 -365.224] [66.544], Avg: [-406.644 -406.644 -406.644] (0.0010) ({r_i: None, r_t: [-747.504 -747.504 -747.504], eps: 0.001})
Step:  142700, Reward: [-364.201 -364.201 -364.201] [80.357], Avg: [-406.614 -406.614 -406.614] (0.0010) ({r_i: None, r_t: [-778.771 -778.771 -778.771], eps: 0.001})
Step:  142800, Reward: [-363.728 -363.728 -363.728] [53.706], Avg: [-406.584 -406.584 -406.584] (0.0010) ({r_i: None, r_t: [-768.447 -768.447 -768.447], eps: 0.001})
Step:  142900, Reward: [-364.891 -364.891 -364.891] [61.895], Avg: [-406.555 -406.555 -406.555] (0.0010) ({r_i: None, r_t: [-707.942 -707.942 -707.942], eps: 0.001})
Step:  143000, Reward: [-370.264 -370.264 -370.264] [69.669], Avg: [-406.530 -406.530 -406.530] (0.0010) ({r_i: None, r_t: [-733.646 -733.646 -733.646], eps: 0.001})
Step:  143100, Reward: [-405.084 -405.084 -405.084] [75.545], Avg: [-406.529 -406.529 -406.529] (0.0010) ({r_i: None, r_t: [-764.162 -764.162 -764.162], eps: 0.001})
Step:  143200, Reward: [-389.228 -389.228 -389.228] [69.358], Avg: [-406.517 -406.517 -406.517] (0.0010) ({r_i: None, r_t: [-739.611 -739.611 -739.611], eps: 0.001})
Step:  143300, Reward: [-373.649 -373.649 -373.649] [47.009], Avg: [-406.494 -406.494 -406.494] (0.0010) ({r_i: None, r_t: [-749.930 -749.930 -749.930], eps: 0.001})
Step:  143400, Reward: [-400.160 -400.160 -400.160] [61.511], Avg: [-406.489 -406.489 -406.489] (0.0010) ({r_i: None, r_t: [-737.350 -737.350 -737.350], eps: 0.001})
Step:  143500, Reward: [-360.537 -360.537 -360.537] [60.158], Avg: [-406.457 -406.457 -406.457] (0.0010) ({r_i: None, r_t: [-784.704 -784.704 -784.704], eps: 0.001})
Step:  143600, Reward: [-374.322 -374.322 -374.322] [60.017], Avg: [-406.435 -406.435 -406.435] (0.0010) ({r_i: None, r_t: [-739.766 -739.766 -739.766], eps: 0.001})
Step:  143700, Reward: [-401.086 -401.086 -401.086] [71.821], Avg: [-406.431 -406.431 -406.431] (0.0010) ({r_i: None, r_t: [-738.423 -738.423 -738.423], eps: 0.001})
Step:  143800, Reward: [-348.246 -348.246 -348.246] [55.921], Avg: [-406.391 -406.391 -406.391] (0.0010) ({r_i: None, r_t: [-779.233 -779.233 -779.233], eps: 0.001})
Step:  143900, Reward: [-388.650 -388.650 -388.650] [69.554], Avg: [-406.379 -406.379 -406.379] (0.0010) ({r_i: None, r_t: [-767.239 -767.239 -767.239], eps: 0.001})
Step:  144000, Reward: [-377.942 -377.942 -377.942] [64.361], Avg: [-406.359 -406.359 -406.359] (0.0010) ({r_i: None, r_t: [-738.792 -738.792 -738.792], eps: 0.001})
Step:  144100, Reward: [-378.806 -378.806 -378.806] [75.646], Avg: [-406.340 -406.340 -406.340] (0.0010) ({r_i: None, r_t: [-727.306 -727.306 -727.306], eps: 0.001})
Step:  144200, Reward: [-354.916 -354.916 -354.916] [58.353], Avg: [-406.304 -406.304 -406.304] (0.0010) ({r_i: None, r_t: [-730.528 -730.528 -730.528], eps: 0.001})
Step:  144300, Reward: [-417.335 -417.335 -417.335] [88.218], Avg: [-406.312 -406.312 -406.312] (0.0010) ({r_i: None, r_t: [-778.701 -778.701 -778.701], eps: 0.001})
Step:  144400, Reward: [-381.389 -381.389 -381.389] [62.790], Avg: [-406.294 -406.294 -406.294] (0.0010) ({r_i: None, r_t: [-756.283 -756.283 -756.283], eps: 0.001})
Step:  144500, Reward: [-388.059 -388.059 -388.059] [60.061], Avg: [-406.282 -406.282 -406.282] (0.0010) ({r_i: None, r_t: [-763.701 -763.701 -763.701], eps: 0.001})
Step:  144600, Reward: [-379.325 -379.325 -379.325] [53.314], Avg: [-406.263 -406.263 -406.263] (0.0010) ({r_i: None, r_t: [-768.363 -768.363 -768.363], eps: 0.001})
Step:  144700, Reward: [-426.390 -426.390 -426.390] [71.972], Avg: [-406.277 -406.277 -406.277] (0.0010) ({r_i: None, r_t: [-753.948 -753.948 -753.948], eps: 0.001})
Step:  144800, Reward: [-379.789 -379.789 -379.789] [63.254], Avg: [-406.259 -406.259 -406.259] (0.0010) ({r_i: None, r_t: [-765.643 -765.643 -765.643], eps: 0.001})
Step:  144900, Reward: [-359.545 -359.545 -359.545] [47.100], Avg: [-406.227 -406.227 -406.227] (0.0010) ({r_i: None, r_t: [-781.815 -781.815 -781.815], eps: 0.001})
Step:  145000, Reward: [-361.897 -361.897 -361.897] [73.351], Avg: [-406.196 -406.196 -406.196] (0.0010) ({r_i: None, r_t: [-758.973 -758.973 -758.973], eps: 0.001})
Step:  145100, Reward: [-373.744 -373.744 -373.744] [48.710], Avg: [-406.174 -406.174 -406.174] (0.0010) ({r_i: None, r_t: [-764.390 -764.390 -764.390], eps: 0.001})
Step:  145200, Reward: [-367.651 -367.651 -367.651] [58.697], Avg: [-406.147 -406.147 -406.147] (0.0010) ({r_i: None, r_t: [-744.765 -744.765 -744.765], eps: 0.001})
Step:  145300, Reward: [-397.978 -397.978 -397.978] [64.085], Avg: [-406.142 -406.142 -406.142] (0.0010) ({r_i: None, r_t: [-747.451 -747.451 -747.451], eps: 0.001})
Step:  145400, Reward: [-361.466 -361.466 -361.466] [72.054], Avg: [-406.111 -406.111 -406.111] (0.0010) ({r_i: None, r_t: [-759.356 -759.356 -759.356], eps: 0.001})
Step:  145500, Reward: [-368.925 -368.925 -368.925] [68.434], Avg: [-406.085 -406.085 -406.085] (0.0010) ({r_i: None, r_t: [-776.655 -776.655 -776.655], eps: 0.001})
Step:  145600, Reward: [-378.096 -378.096 -378.096] [75.006], Avg: [-406.066 -406.066 -406.066] (0.0010) ({r_i: None, r_t: [-784.595 -784.595 -784.595], eps: 0.001})
Step:  145700, Reward: [-383.032 -383.032 -383.032] [54.518], Avg: [-406.050 -406.050 -406.050] (0.0010) ({r_i: None, r_t: [-744.901 -744.901 -744.901], eps: 0.001})
Step:  145800, Reward: [-385.134 -385.134 -385.134] [73.375], Avg: [-406.036 -406.036 -406.036] (0.0010) ({r_i: None, r_t: [-769.876 -769.876 -769.876], eps: 0.001})
Step:  145900, Reward: [-375.558 -375.558 -375.558] [79.260], Avg: [-406.015 -406.015 -406.015] (0.0010) ({r_i: None, r_t: [-747.552 -747.552 -747.552], eps: 0.001})
Step:  146000, Reward: [-378.457 -378.457 -378.457] [92.269], Avg: [-405.996 -405.996 -405.996] (0.0010) ({r_i: None, r_t: [-752.700 -752.700 -752.700], eps: 0.001})
Step:  146100, Reward: [-350.395 -350.395 -350.395] [59.591], Avg: [-405.958 -405.958 -405.958] (0.0010) ({r_i: None, r_t: [-772.434 -772.434 -772.434], eps: 0.001})
Step:  146200, Reward: [-373.437 -373.437 -373.437] [52.070], Avg: [-405.936 -405.936 -405.936] (0.0010) ({r_i: None, r_t: [-738.092 -738.092 -738.092], eps: 0.001})
Step:  146300, Reward: [-399.848 -399.848 -399.848] [63.806], Avg: [-405.932 -405.932 -405.932] (0.0010) ({r_i: None, r_t: [-761.262 -761.262 -761.262], eps: 0.001})
Step:  146400, Reward: [-402.196 -402.196 -402.196] [70.481], Avg: [-405.929 -405.929 -405.929] (0.0010) ({r_i: None, r_t: [-753.637 -753.637 -753.637], eps: 0.001})
Step:  146500, Reward: [-392.783 -392.783 -392.783] [78.202], Avg: [-405.920 -405.920 -405.920] (0.0010) ({r_i: None, r_t: [-800.309 -800.309 -800.309], eps: 0.001})
Step:  146600, Reward: [-366.576 -366.576 -366.576] [70.412], Avg: [-405.894 -405.894 -405.894] (0.0010) ({r_i: None, r_t: [-747.325 -747.325 -747.325], eps: 0.001})
Step:  146700, Reward: [-390.788 -390.788 -390.788] [74.024], Avg: [-405.883 -405.883 -405.883] (0.0010) ({r_i: None, r_t: [-745.275 -745.275 -745.275], eps: 0.001})
Step:  146800, Reward: [-376.637 -376.637 -376.637] [63.458], Avg: [-405.863 -405.863 -405.863] (0.0010) ({r_i: None, r_t: [-729.026 -729.026 -729.026], eps: 0.001})
Step:  146900, Reward: [-423.215 -423.215 -423.215] [86.443], Avg: [-405.875 -405.875 -405.875] (0.0010) ({r_i: None, r_t: [-714.369 -714.369 -714.369], eps: 0.001})
Step:  147000, Reward: [-355.407 -355.407 -355.407] [60.817], Avg: [-405.841 -405.841 -405.841] (0.0010) ({r_i: None, r_t: [-758.266 -758.266 -758.266], eps: 0.001})
Step:  147100, Reward: [-364.350 -364.350 -364.350] [62.490], Avg: [-405.813 -405.813 -405.813] (0.0010) ({r_i: None, r_t: [-772.889 -772.889 -772.889], eps: 0.001})
Step:  147200, Reward: [-396.410 -396.410 -396.410] [37.045], Avg: [-405.806 -405.806 -405.806] (0.0010) ({r_i: None, r_t: [-778.470 -778.470 -778.470], eps: 0.001})
Step:  147300, Reward: [-393.541 -393.541 -393.541] [50.679], Avg: [-405.798 -405.798 -405.798] (0.0010) ({r_i: None, r_t: [-752.474 -752.474 -752.474], eps: 0.001})
Step:  147400, Reward: [-386.675 -386.675 -386.675] [72.361], Avg: [-405.785 -405.785 -405.785] (0.0010) ({r_i: None, r_t: [-721.731 -721.731 -721.731], eps: 0.001})
Step:  147500, Reward: [-375.423 -375.423 -375.423] [51.511], Avg: [-405.764 -405.764 -405.764] (0.0010) ({r_i: None, r_t: [-780.986 -780.986 -780.986], eps: 0.001})
Step:  147600, Reward: [-378.573 -378.573 -378.573] [55.935], Avg: [-405.746 -405.746 -405.746] (0.0010) ({r_i: None, r_t: [-784.307 -784.307 -784.307], eps: 0.001})
Step:  147700, Reward: [-413.829 -413.829 -413.829] [61.014], Avg: [-405.751 -405.751 -405.751] (0.0010) ({r_i: None, r_t: [-763.207 -763.207 -763.207], eps: 0.001})
Step:  147800, Reward: [-387.113 -387.113 -387.113] [78.011], Avg: [-405.739 -405.739 -405.739] (0.0010) ({r_i: None, r_t: [-723.946 -723.946 -723.946], eps: 0.001})
Step:  147900, Reward: [-360.812 -360.812 -360.812] [63.299], Avg: [-405.708 -405.708 -405.708] (0.0010) ({r_i: None, r_t: [-725.737 -725.737 -725.737], eps: 0.001})
Step:  148000, Reward: [-362.269 -362.269 -362.269] [70.759], Avg: [-405.679 -405.679 -405.679] (0.0010) ({r_i: None, r_t: [-750.305 -750.305 -750.305], eps: 0.001})
Step:  148100, Reward: [-390.979 -390.979 -390.979] [65.329], Avg: [-405.669 -405.669 -405.669] (0.0010) ({r_i: None, r_t: [-785.601 -785.601 -785.601], eps: 0.001})
Step:  148200, Reward: [-369.470 -369.470 -369.470] [56.303], Avg: [-405.645 -405.645 -405.645] (0.0010) ({r_i: None, r_t: [-813.384 -813.384 -813.384], eps: 0.001})
Step:  148300, Reward: [-336.560 -336.560 -336.560] [47.301], Avg: [-405.598 -405.598 -405.598] (0.0010) ({r_i: None, r_t: [-750.468 -750.468 -750.468], eps: 0.001})
Step:  148400, Reward: [-383.389 -383.389 -383.389] [60.397], Avg: [-405.583 -405.583 -405.583] (0.0010) ({r_i: None, r_t: [-774.508 -774.508 -774.508], eps: 0.001})
Step:  148500, Reward: [-378.212 -378.212 -378.212] [64.114], Avg: [-405.565 -405.565 -405.565] (0.0010) ({r_i: None, r_t: [-766.266 -766.266 -766.266], eps: 0.001})
Step:  148600, Reward: [-390.257 -390.257 -390.257] [59.428], Avg: [-405.555 -405.555 -405.555] (0.0010) ({r_i: None, r_t: [-762.094 -762.094 -762.094], eps: 0.001})
Step:  148700, Reward: [-379.745 -379.745 -379.745] [59.554], Avg: [-405.537 -405.537 -405.537] (0.0010) ({r_i: None, r_t: [-742.710 -742.710 -742.710], eps: 0.001})
Step:  148800, Reward: [-366.207 -366.207 -366.207] [65.453], Avg: [-405.511 -405.511 -405.511] (0.0010) ({r_i: None, r_t: [-787.074 -787.074 -787.074], eps: 0.001})
Step:  148900, Reward: [-385.338 -385.338 -385.338] [55.797], Avg: [-405.497 -405.497 -405.497] (0.0010) ({r_i: None, r_t: [-754.697 -754.697 -754.697], eps: 0.001})
Step:  149000, Reward: [-361.272 -361.272 -361.272] [52.278], Avg: [-405.468 -405.468 -405.468] (0.0010) ({r_i: None, r_t: [-738.694 -738.694 -738.694], eps: 0.001})
Step:  149100, Reward: [-389.488 -389.488 -389.488] [52.512], Avg: [-405.457 -405.457 -405.457] (0.0010) ({r_i: None, r_t: [-785.246 -785.246 -785.246], eps: 0.001})
Step:  149200, Reward: [-379.806 -379.806 -379.806] [84.549], Avg: [-405.440 -405.440 -405.440] (0.0010) ({r_i: None, r_t: [-723.664 -723.664 -723.664], eps: 0.001})
Step:  149300, Reward: [-349.358 -349.358 -349.358] [63.565], Avg: [-405.402 -405.402 -405.402] (0.0010) ({r_i: None, r_t: [-753.234 -753.234 -753.234], eps: 0.001})
Step:  149400, Reward: [-384.648 -384.648 -384.648] [63.176], Avg: [-405.388 -405.388 -405.388] (0.0010) ({r_i: None, r_t: [-728.982 -728.982 -728.982], eps: 0.001})
Step:  149500, Reward: [-388.116 -388.116 -388.116] [75.733], Avg: [-405.377 -405.377 -405.377] (0.0010) ({r_i: None, r_t: [-733.127 -733.127 -733.127], eps: 0.001})
Step:  149600, Reward: [-364.036 -364.036 -364.036] [66.350], Avg: [-405.349 -405.349 -405.349] (0.0010) ({r_i: None, r_t: [-772.790 -772.790 -772.790], eps: 0.001})
Step:  149700, Reward: [-356.930 -356.930 -356.930] [70.824], Avg: [-405.317 -405.317 -405.317] (0.0010) ({r_i: None, r_t: [-752.740 -752.740 -752.740], eps: 0.001})
Step:  149800, Reward: [-382.343 -382.343 -382.343] [70.675], Avg: [-405.302 -405.302 -405.302] (0.0010) ({r_i: None, r_t: [-711.654 -711.654 -711.654], eps: 0.001})
Step:  149900, Reward: [-363.667 -363.667 -363.667] [63.245], Avg: [-405.274 -405.274 -405.274] (0.0010) ({r_i: None, r_t: [-755.183 -755.183 -755.183], eps: 0.001})
Step:  150000, Reward: [-374.435 -374.435 -374.435] [62.955], Avg: [-405.253 -405.253 -405.253] (0.0010) ({r_i: None, r_t: [-758.690 -758.690 -758.690], eps: 0.001})
Step:  150100, Reward: [-373.680 -373.680 -373.680] [72.638], Avg: [-405.232 -405.232 -405.232] (0.0010) ({r_i: None, r_t: [-714.503 -714.503 -714.503], eps: 0.001})
Step:  150200, Reward: [-380.977 -380.977 -380.977] [52.186], Avg: [-405.216 -405.216 -405.216] (0.0010) ({r_i: None, r_t: [-747.668 -747.668 -747.668], eps: 0.001})
Step:  150300, Reward: [-356.410 -356.410 -356.410] [59.373], Avg: [-405.184 -405.184 -405.184] (0.0010) ({r_i: None, r_t: [-743.967 -743.967 -743.967], eps: 0.001})
Step:  150400, Reward: [-368.815 -368.815 -368.815] [65.154], Avg: [-405.159 -405.159 -405.159] (0.0010) ({r_i: None, r_t: [-743.907 -743.907 -743.907], eps: 0.001})
Step:  150500, Reward: [-350.286 -350.286 -350.286] [70.200], Avg: [-405.123 -405.123 -405.123] (0.0010) ({r_i: None, r_t: [-717.310 -717.310 -717.310], eps: 0.001})
Step:  150600, Reward: [-363.829 -363.829 -363.829] [55.241], Avg: [-405.096 -405.096 -405.096] (0.0010) ({r_i: None, r_t: [-745.969 -745.969 -745.969], eps: 0.001})
Step:  150700, Reward: [-385.169 -385.169 -385.169] [47.372], Avg: [-405.082 -405.082 -405.082] (0.0010) ({r_i: None, r_t: [-767.604 -767.604 -767.604], eps: 0.001})
Step:  150800, Reward: [-359.036 -359.036 -359.036] [70.020], Avg: [-405.052 -405.052 -405.052] (0.0010) ({r_i: None, r_t: [-780.861 -780.861 -780.861], eps: 0.001})
Step:  150900, Reward: [-374.977 -374.977 -374.977] [71.519], Avg: [-405.032 -405.032 -405.032] (0.0010) ({r_i: None, r_t: [-710.706 -710.706 -710.706], eps: 0.001})
Step:  151000, Reward: [-378.989 -378.989 -378.989] [54.169], Avg: [-405.015 -405.015 -405.015] (0.0010) ({r_i: None, r_t: [-747.695 -747.695 -747.695], eps: 0.001})
Step:  151100, Reward: [-390.317 -390.317 -390.317] [56.665], Avg: [-405.005 -405.005 -405.005] (0.0010) ({r_i: None, r_t: [-761.281 -761.281 -761.281], eps: 0.001})
Step:  151200, Reward: [-377.195 -377.195 -377.195] [71.605], Avg: [-404.987 -404.987 -404.987] (0.0010) ({r_i: None, r_t: [-754.246 -754.246 -754.246], eps: 0.001})
Step:  151300, Reward: [-363.197 -363.197 -363.197] [53.517], Avg: [-404.959 -404.959 -404.959] (0.0010) ({r_i: None, r_t: [-723.240 -723.240 -723.240], eps: 0.001})
Step:  151400, Reward: [-372.607 -372.607 -372.607] [68.364], Avg: [-404.938 -404.938 -404.938] (0.0010) ({r_i: None, r_t: [-743.696 -743.696 -743.696], eps: 0.001})
Step:  151500, Reward: [-384.931 -384.931 -384.931] [55.524], Avg: [-404.924 -404.924 -404.924] (0.0010) ({r_i: None, r_t: [-714.641 -714.641 -714.641], eps: 0.001})
Step:  151600, Reward: [-400.508 -400.508 -400.508] [87.321], Avg: [-404.922 -404.922 -404.922] (0.0010) ({r_i: None, r_t: [-770.391 -770.391 -770.391], eps: 0.001})
Step:  151700, Reward: [-365.539 -365.539 -365.539] [53.888], Avg: [-404.896 -404.896 -404.896] (0.0010) ({r_i: None, r_t: [-746.821 -746.821 -746.821], eps: 0.001})
Step:  151800, Reward: [-383.481 -383.481 -383.481] [69.595], Avg: [-404.882 -404.882 -404.882] (0.0010) ({r_i: None, r_t: [-756.250 -756.250 -756.250], eps: 0.001})
Step:  151900, Reward: [-386.886 -386.886 -386.886] [81.253], Avg: [-404.870 -404.870 -404.870] (0.0010) ({r_i: None, r_t: [-758.416 -758.416 -758.416], eps: 0.001})
Step:  152000, Reward: [-380.007 -380.007 -380.007] [85.319], Avg: [-404.853 -404.853 -404.853] (0.0010) ({r_i: None, r_t: [-742.617 -742.617 -742.617], eps: 0.001})
Step:  152100, Reward: [-363.660 -363.660 -363.660] [53.657], Avg: [-404.826 -404.826 -404.826] (0.0010) ({r_i: None, r_t: [-716.903 -716.903 -716.903], eps: 0.001})
Step:  152200, Reward: [-358.464 -358.464 -358.464] [63.125], Avg: [-404.796 -404.796 -404.796] (0.0010) ({r_i: None, r_t: [-775.860 -775.860 -775.860], eps: 0.001})
Step:  152300, Reward: [-361.395 -361.395 -361.395] [54.731], Avg: [-404.767 -404.767 -404.767] (0.0010) ({r_i: None, r_t: [-767.456 -767.456 -767.456], eps: 0.001})
Step:  152400, Reward: [-367.563 -367.563 -367.563] [50.315], Avg: [-404.743 -404.743 -404.743] (0.0010) ({r_i: None, r_t: [-742.683 -742.683 -742.683], eps: 0.001})
Step:  152500, Reward: [-395.216 -395.216 -395.216] [69.118], Avg: [-404.737 -404.737 -404.737] (0.0010) ({r_i: None, r_t: [-759.161 -759.161 -759.161], eps: 0.001})
Step:  152600, Reward: [-387.219 -387.219 -387.219] [59.612], Avg: [-404.725 -404.725 -404.725] (0.0010) ({r_i: None, r_t: [-752.017 -752.017 -752.017], eps: 0.001})
Step:  152700, Reward: [-371.173 -371.173 -371.173] [53.016], Avg: [-404.703 -404.703 -404.703] (0.0010) ({r_i: None, r_t: [-750.005 -750.005 -750.005], eps: 0.001})
Step:  152800, Reward: [-373.913 -373.913 -373.913] [58.969], Avg: [-404.683 -404.683 -404.683] (0.0010) ({r_i: None, r_t: [-703.031 -703.031 -703.031], eps: 0.001})
Step:  152900, Reward: [-380.730 -380.730 -380.730] [52.812], Avg: [-404.667 -404.667 -404.667] (0.0010) ({r_i: None, r_t: [-754.686 -754.686 -754.686], eps: 0.001})
Step:  153000, Reward: [-355.481 -355.481 -355.481] [78.797], Avg: [-404.635 -404.635 -404.635] (0.0010) ({r_i: None, r_t: [-754.793 -754.793 -754.793], eps: 0.001})
Step:  153100, Reward: [-391.439 -391.439 -391.439] [76.656], Avg: [-404.627 -404.627 -404.627] (0.0010) ({r_i: None, r_t: [-730.978 -730.978 -730.978], eps: 0.001})
Step:  153200, Reward: [-369.180 -369.180 -369.180] [53.177], Avg: [-404.604 -404.604 -404.604] (0.0010) ({r_i: None, r_t: [-760.837 -760.837 -760.837], eps: 0.001})
Step:  153300, Reward: [-389.300 -389.300 -389.300] [40.837], Avg: [-404.594 -404.594 -404.594] (0.0010) ({r_i: None, r_t: [-758.482 -758.482 -758.482], eps: 0.001})
Step:  153400, Reward: [-341.925 -341.925 -341.925] [40.094], Avg: [-404.553 -404.553 -404.553] (0.0010) ({r_i: None, r_t: [-746.417 -746.417 -746.417], eps: 0.001})
Step:  153500, Reward: [-401.040 -401.040 -401.040] [61.349], Avg: [-404.551 -404.551 -404.551] (0.0010) ({r_i: None, r_t: [-787.983 -787.983 -787.983], eps: 0.001})
Step:  153600, Reward: [-363.976 -363.976 -363.976] [69.327], Avg: [-404.524 -404.524 -404.524] (0.0010) ({r_i: None, r_t: [-727.346 -727.346 -727.346], eps: 0.001})
Step:  153700, Reward: [-410.603 -410.603 -410.603] [56.019], Avg: [-404.528 -404.528 -404.528] (0.0010) ({r_i: None, r_t: [-765.270 -765.270 -765.270], eps: 0.001})
Step:  153800, Reward: [-383.258 -383.258 -383.258] [76.876], Avg: [-404.514 -404.514 -404.514] (0.0010) ({r_i: None, r_t: [-753.341 -753.341 -753.341], eps: 0.001})
Step:  153900, Reward: [-348.052 -348.052 -348.052] [40.585], Avg: [-404.478 -404.478 -404.478] (0.0010) ({r_i: None, r_t: [-744.218 -744.218 -744.218], eps: 0.001})
Step:  154000, Reward: [-376.831 -376.831 -376.831] [59.142], Avg: [-404.460 -404.460 -404.460] (0.0010) ({r_i: None, r_t: [-757.096 -757.096 -757.096], eps: 0.001})
Step:  154100, Reward: [-404.003 -404.003 -404.003] [73.965], Avg: [-404.459 -404.459 -404.459] (0.0010) ({r_i: None, r_t: [-702.929 -702.929 -702.929], eps: 0.001})
Step:  154200, Reward: [-362.297 -362.297 -362.297] [68.609], Avg: [-404.432 -404.432 -404.432] (0.0010) ({r_i: None, r_t: [-752.327 -752.327 -752.327], eps: 0.001})
Step:  154300, Reward: [-415.952 -415.952 -415.952] [57.585], Avg: [-404.439 -404.439 -404.439] (0.0010) ({r_i: None, r_t: [-758.320 -758.320 -758.320], eps: 0.001})
Step:  154400, Reward: [-377.687 -377.687 -377.687] [78.395], Avg: [-404.422 -404.422 -404.422] (0.0010) ({r_i: None, r_t: [-760.200 -760.200 -760.200], eps: 0.001})
Step:  154500, Reward: [-357.239 -357.239 -357.239] [60.886], Avg: [-404.392 -404.392 -404.392] (0.0010) ({r_i: None, r_t: [-741.381 -741.381 -741.381], eps: 0.001})
Step:  154600, Reward: [-376.125 -376.125 -376.125] [65.067], Avg: [-404.373 -404.373 -404.373] (0.0010) ({r_i: None, r_t: [-773.817 -773.817 -773.817], eps: 0.001})
Step:  154700, Reward: [-359.802 -359.802 -359.802] [52.824], Avg: [-404.345 -404.345 -404.345] (0.0010) ({r_i: None, r_t: [-763.584 -763.584 -763.584], eps: 0.001})
Step:  154800, Reward: [-380.175 -380.175 -380.175] [63.537], Avg: [-404.329 -404.329 -404.329] (0.0010) ({r_i: None, r_t: [-720.161 -720.161 -720.161], eps: 0.001})
Step:  154900, Reward: [-363.719 -363.719 -363.719] [60.808], Avg: [-404.303 -404.303 -404.303] (0.0010) ({r_i: None, r_t: [-735.170 -735.170 -735.170], eps: 0.001})
Step:  155000, Reward: [-385.862 -385.862 -385.862] [60.422], Avg: [-404.291 -404.291 -404.291] (0.0010) ({r_i: None, r_t: [-757.250 -757.250 -757.250], eps: 0.001})
Step:  155100, Reward: [-375.776 -375.776 -375.776] [59.758], Avg: [-404.273 -404.273 -404.273] (0.0010) ({r_i: None, r_t: [-741.180 -741.180 -741.180], eps: 0.001})
Step:  155200, Reward: [-339.937 -339.937 -339.937] [63.962], Avg: [-404.231 -404.231 -404.231] (0.0010) ({r_i: None, r_t: [-730.394 -730.394 -730.394], eps: 0.001})
Step:  155300, Reward: [-353.964 -353.964 -353.964] [91.374], Avg: [-404.199 -404.199 -404.199] (0.0010) ({r_i: None, r_t: [-772.820 -772.820 -772.820], eps: 0.001})
Step:  155400, Reward: [-327.869 -327.869 -327.869] [55.069], Avg: [-404.150 -404.150 -404.150] (0.0010) ({r_i: None, r_t: [-780.791 -780.791 -780.791], eps: 0.001})
Step:  155500, Reward: [-384.760 -384.760 -384.760] [61.912], Avg: [-404.137 -404.137 -404.137] (0.0010) ({r_i: None, r_t: [-745.463 -745.463 -745.463], eps: 0.001})
Step:  155600, Reward: [-368.032 -368.032 -368.032] [69.951], Avg: [-404.114 -404.114 -404.114] (0.0010) ({r_i: None, r_t: [-757.443 -757.443 -757.443], eps: 0.001})
Step:  155700, Reward: [-356.953 -356.953 -356.953] [40.321], Avg: [-404.084 -404.084 -404.084] (0.0010) ({r_i: None, r_t: [-709.312 -709.312 -709.312], eps: 0.001})
Step:  155800, Reward: [-377.543 -377.543 -377.543] [57.816], Avg: [-404.067 -404.067 -404.067] (0.0010) ({r_i: None, r_t: [-778.465 -778.465 -778.465], eps: 0.001})
Step:  155900, Reward: [-364.205 -364.205 -364.205] [63.004], Avg: [-404.041 -404.041 -404.041] (0.0010) ({r_i: None, r_t: [-752.124 -752.124 -752.124], eps: 0.001})
Step:  156000, Reward: [-353.156 -353.156 -353.156] [65.337], Avg: [-404.009 -404.009 -404.009] (0.0010) ({r_i: None, r_t: [-734.324 -734.324 -734.324], eps: 0.001})
Step:  156100, Reward: [-392.138 -392.138 -392.138] [85.869], Avg: [-404.001 -404.001 -404.001] (0.0010) ({r_i: None, r_t: [-723.585 -723.585 -723.585], eps: 0.001})
Step:  156200, Reward: [-357.869 -357.869 -357.869] [47.829], Avg: [-403.971 -403.971 -403.971] (0.0010) ({r_i: None, r_t: [-724.786 -724.786 -724.786], eps: 0.001})
Step:  156300, Reward: [-391.784 -391.784 -391.784] [71.153], Avg: [-403.964 -403.964 -403.964] (0.0010) ({r_i: None, r_t: [-756.501 -756.501 -756.501], eps: 0.001})
Step:  156400, Reward: [-377.900 -377.900 -377.900] [54.290], Avg: [-403.947 -403.947 -403.947] (0.0010) ({r_i: None, r_t: [-735.053 -735.053 -735.053], eps: 0.001})
Step:  156500, Reward: [-378.914 -378.914 -378.914] [74.568], Avg: [-403.931 -403.931 -403.931] (0.0010) ({r_i: None, r_t: [-732.657 -732.657 -732.657], eps: 0.001})
Step:  156600, Reward: [-367.148 -367.148 -367.148] [59.414], Avg: [-403.908 -403.908 -403.908] (0.0010) ({r_i: None, r_t: [-728.448 -728.448 -728.448], eps: 0.001})
Step:  156700, Reward: [-379.392 -379.392 -379.392] [54.268], Avg: [-403.892 -403.892 -403.892] (0.0010) ({r_i: None, r_t: [-703.759 -703.759 -703.759], eps: 0.001})
Step:  156800, Reward: [-386.879 -386.879 -386.879] [62.304], Avg: [-403.881 -403.881 -403.881] (0.0010) ({r_i: None, r_t: [-698.995 -698.995 -698.995], eps: 0.001})
Step:  156900, Reward: [-353.216 -353.216 -353.216] [60.574], Avg: [-403.849 -403.849 -403.849] (0.0010) ({r_i: None, r_t: [-801.761 -801.761 -801.761], eps: 0.001})
Step:  157000, Reward: [-376.007 -376.007 -376.007] [57.792], Avg: [-403.831 -403.831 -403.831] (0.0010) ({r_i: None, r_t: [-683.286 -683.286 -683.286], eps: 0.001})
Step:  157100, Reward: [-376.786 -376.786 -376.786] [65.039], Avg: [-403.814 -403.814 -403.814] (0.0010) ({r_i: None, r_t: [-798.349 -798.349 -798.349], eps: 0.001})
Step:  157200, Reward: [-372.083 -372.083 -372.083] [59.682], Avg: [-403.794 -403.794 -403.794] (0.0010) ({r_i: None, r_t: [-735.322 -735.322 -735.322], eps: 0.001})
Step:  157300, Reward: [-343.381 -343.381 -343.381] [46.680], Avg: [-403.755 -403.755 -403.755] (0.0010) ({r_i: None, r_t: [-751.833 -751.833 -751.833], eps: 0.001})
Step:  157400, Reward: [-408.184 -408.184 -408.184] [74.406], Avg: [-403.758 -403.758 -403.758] (0.0010) ({r_i: None, r_t: [-749.661 -749.661 -749.661], eps: 0.001})
Step:  157500, Reward: [-358.784 -358.784 -358.784] [66.918], Avg: [-403.730 -403.730 -403.730] (0.0010) ({r_i: None, r_t: [-742.486 -742.486 -742.486], eps: 0.001})
Step:  157600, Reward: [-372.214 -372.214 -372.214] [65.382], Avg: [-403.710 -403.710 -403.710] (0.0010) ({r_i: None, r_t: [-762.307 -762.307 -762.307], eps: 0.001})
Step:  157700, Reward: [-360.880 -360.880 -360.880] [60.270], Avg: [-403.682 -403.682 -403.682] (0.0010) ({r_i: None, r_t: [-727.258 -727.258 -727.258], eps: 0.001})
Step:  157800, Reward: [-353.181 -353.181 -353.181] [54.653], Avg: [-403.650 -403.650 -403.650] (0.0010) ({r_i: None, r_t: [-765.233 -765.233 -765.233], eps: 0.001})
Step:  157900, Reward: [-402.532 -402.532 -402.532] [54.839], Avg: [-403.650 -403.650 -403.650] (0.0010) ({r_i: None, r_t: [-739.089 -739.089 -739.089], eps: 0.001})
Step:  158000, Reward: [-343.008 -343.008 -343.008] [56.441], Avg: [-403.611 -403.611 -403.611] (0.0010) ({r_i: None, r_t: [-755.692 -755.692 -755.692], eps: 0.001})
Step:  158100, Reward: [-359.532 -359.532 -359.532] [52.216], Avg: [-403.584 -403.584 -403.584] (0.0010) ({r_i: None, r_t: [-749.376 -749.376 -749.376], eps: 0.001})
Step:  158200, Reward: [-369.620 -369.620 -369.620] [62.060], Avg: [-403.562 -403.562 -403.562] (0.0010) ({r_i: None, r_t: [-743.246 -743.246 -743.246], eps: 0.001})
Step:  158300, Reward: [-375.983 -375.983 -375.983] [61.115], Avg: [-403.545 -403.545 -403.545] (0.0010) ({r_i: None, r_t: [-699.546 -699.546 -699.546], eps: 0.001})
Step:  158400, Reward: [-405.691 -405.691 -405.691] [61.542], Avg: [-403.546 -403.546 -403.546] (0.0010) ({r_i: None, r_t: [-730.524 -730.524 -730.524], eps: 0.001})
Step:  158500, Reward: [-367.184 -367.184 -367.184] [71.005], Avg: [-403.523 -403.523 -403.523] (0.0010) ({r_i: None, r_t: [-765.497 -765.497 -765.497], eps: 0.001})
Step:  158600, Reward: [-358.068 -358.068 -358.068] [53.345], Avg: [-403.494 -403.494 -403.494] (0.0010) ({r_i: None, r_t: [-736.743 -736.743 -736.743], eps: 0.001})
Step:  158700, Reward: [-393.930 -393.930 -393.930] [58.319], Avg: [-403.488 -403.488 -403.488] (0.0010) ({r_i: None, r_t: [-708.289 -708.289 -708.289], eps: 0.001})
Step:  158800, Reward: [-391.127 -391.127 -391.127] [54.181], Avg: [-403.481 -403.481 -403.481] (0.0010) ({r_i: None, r_t: [-732.426 -732.426 -732.426], eps: 0.001})
Step:  158900, Reward: [-359.062 -359.062 -359.062] [84.076], Avg: [-403.453 -403.453 -403.453] (0.0010) ({r_i: None, r_t: [-726.767 -726.767 -726.767], eps: 0.001})
Step:  159000, Reward: [-385.300 -385.300 -385.300] [69.977], Avg: [-403.441 -403.441 -403.441] (0.0010) ({r_i: None, r_t: [-696.764 -696.764 -696.764], eps: 0.001})
Step:  159100, Reward: [-364.316 -364.316 -364.316] [51.971], Avg: [-403.417 -403.417 -403.417] (0.0010) ({r_i: None, r_t: [-716.478 -716.478 -716.478], eps: 0.001})
Step:  159200, Reward: [-406.033 -406.033 -406.033] [89.648], Avg: [-403.418 -403.418 -403.418] (0.0010) ({r_i: None, r_t: [-736.697 -736.697 -736.697], eps: 0.001})
Step:  159300, Reward: [-355.616 -355.616 -355.616] [69.817], Avg: [-403.388 -403.388 -403.388] (0.0010) ({r_i: None, r_t: [-724.195 -724.195 -724.195], eps: 0.001})
Step:  159400, Reward: [-342.713 -342.713 -342.713] [63.138], Avg: [-403.350 -403.350 -403.350] (0.0010) ({r_i: None, r_t: [-744.728 -744.728 -744.728], eps: 0.001})
Step:  159500, Reward: [-395.618 -395.618 -395.618] [62.446], Avg: [-403.346 -403.346 -403.346] (0.0010) ({r_i: None, r_t: [-757.139 -757.139 -757.139], eps: 0.001})
Step:  159600, Reward: [-375.200 -375.200 -375.200] [38.361], Avg: [-403.328 -403.328 -403.328] (0.0010) ({r_i: None, r_t: [-726.250 -726.250 -726.250], eps: 0.001})
Step:  159700, Reward: [-353.832 -353.832 -353.832] [76.927], Avg: [-403.297 -403.297 -403.297] (0.0010) ({r_i: None, r_t: [-762.883 -762.883 -762.883], eps: 0.001})
Step:  159800, Reward: [-391.641 -391.641 -391.641] [52.011], Avg: [-403.290 -403.290 -403.290] (0.0010) ({r_i: None, r_t: [-709.438 -709.438 -709.438], eps: 0.001})
Step:  159900, Reward: [-368.863 -368.863 -368.863] [66.712], Avg: [-403.268 -403.268 -403.268] (0.0010) ({r_i: None, r_t: [-769.202 -769.202 -769.202], eps: 0.001})
Step:  160000, Reward: [-366.483 -366.483 -366.483] [41.482], Avg: [-403.245 -403.245 -403.245] (0.0010) ({r_i: None, r_t: [-811.121 -811.121 -811.121], eps: 0.001})
Step:  160100, Reward: [-366.947 -366.947 -366.947] [61.864], Avg: [-403.222 -403.222 -403.222] (0.0010) ({r_i: None, r_t: [-719.641 -719.641 -719.641], eps: 0.001})
Step:  160200, Reward: [-365.680 -365.680 -365.680] [67.112], Avg: [-403.199 -403.199 -403.199] (0.0010) ({r_i: None, r_t: [-754.070 -754.070 -754.070], eps: 0.001})
Step:  160300, Reward: [-387.328 -387.328 -387.328] [58.650], Avg: [-403.189 -403.189 -403.189] (0.0010) ({r_i: None, r_t: [-737.546 -737.546 -737.546], eps: 0.001})
Step:  160400, Reward: [-363.942 -363.942 -363.942] [60.236], Avg: [-403.165 -403.165 -403.165] (0.0010) ({r_i: None, r_t: [-711.552 -711.552 -711.552], eps: 0.001})
Step:  160500, Reward: [-382.205 -382.205 -382.205] [65.025], Avg: [-403.152 -403.152 -403.152] (0.0010) ({r_i: None, r_t: [-754.136 -754.136 -754.136], eps: 0.001})
Step:  160600, Reward: [-408.047 -408.047 -408.047] [55.148], Avg: [-403.155 -403.155 -403.155] (0.0010) ({r_i: None, r_t: [-763.444 -763.444 -763.444], eps: 0.001})
Step:  160700, Reward: [-392.184 -392.184 -392.184] [65.168], Avg: [-403.148 -403.148 -403.148] (0.0010) ({r_i: None, r_t: [-786.605 -786.605 -786.605], eps: 0.001})
Step:  160800, Reward: [-373.818 -373.818 -373.818] [74.134], Avg: [-403.130 -403.130 -403.130] (0.0010) ({r_i: None, r_t: [-745.872 -745.872 -745.872], eps: 0.001})
Step:  160900, Reward: [-377.119 -377.119 -377.119] [43.593], Avg: [-403.114 -403.114 -403.114] (0.0010) ({r_i: None, r_t: [-775.162 -775.162 -775.162], eps: 0.001})
Step:  161000, Reward: [-377.278 -377.278 -377.278] [65.014], Avg: [-403.097 -403.097 -403.097] (0.0010) ({r_i: None, r_t: [-764.933 -764.933 -764.933], eps: 0.001})
Step:  161100, Reward: [-369.589 -369.589 -369.589] [73.322], Avg: [-403.077 -403.077 -403.077] (0.0010) ({r_i: None, r_t: [-752.660 -752.660 -752.660], eps: 0.001})
Step:  161200, Reward: [-361.155 -361.155 -361.155] [63.866], Avg: [-403.051 -403.051 -403.051] (0.0010) ({r_i: None, r_t: [-767.079 -767.079 -767.079], eps: 0.001})
Step:  161300, Reward: [-382.404 -382.404 -382.404] [60.261], Avg: [-403.038 -403.038 -403.038] (0.0010) ({r_i: None, r_t: [-737.790 -737.790 -737.790], eps: 0.001})
Step:  161400, Reward: [-350.097 -350.097 -350.097] [64.603], Avg: [-403.005 -403.005 -403.005] (0.0010) ({r_i: None, r_t: [-754.703 -754.703 -754.703], eps: 0.001})
Step:  161500, Reward: [-340.156 -340.156 -340.156] [49.568], Avg: [-402.966 -402.966 -402.966] (0.0010) ({r_i: None, r_t: [-730.834 -730.834 -730.834], eps: 0.001})
Step:  161600, Reward: [-351.710 -351.710 -351.710] [75.304], Avg: [-402.935 -402.935 -402.935] (0.0010) ({r_i: None, r_t: [-759.714 -759.714 -759.714], eps: 0.001})
Step:  161700, Reward: [-372.697 -372.697 -372.697] [103.097], Avg: [-402.916 -402.916 -402.916] (0.0010) ({r_i: None, r_t: [-751.903 -751.903 -751.903], eps: 0.001})
Step:  161800, Reward: [-348.855 -348.855 -348.855] [60.938], Avg: [-402.882 -402.882 -402.882] (0.0010) ({r_i: None, r_t: [-738.036 -738.036 -738.036], eps: 0.001})
Step:  161900, Reward: [-362.053 -362.053 -362.053] [84.464], Avg: [-402.857 -402.857 -402.857] (0.0010) ({r_i: None, r_t: [-744.574 -744.574 -744.574], eps: 0.001})
Step:  162000, Reward: [-394.429 -394.429 -394.429] [64.001], Avg: [-402.852 -402.852 -402.852] (0.0010) ({r_i: None, r_t: [-688.468 -688.468 -688.468], eps: 0.001})
Step:  162100, Reward: [-347.566 -347.566 -347.566] [60.922], Avg: [-402.818 -402.818 -402.818] (0.0010) ({r_i: None, r_t: [-726.176 -726.176 -726.176], eps: 0.001})
Step:  162200, Reward: [-372.995 -372.995 -372.995] [65.151], Avg: [-402.800 -402.800 -402.800] (0.0010) ({r_i: None, r_t: [-791.483 -791.483 -791.483], eps: 0.001})
Step:  162300, Reward: [-400.149 -400.149 -400.149] [72.991], Avg: [-402.798 -402.798 -402.798] (0.0010) ({r_i: None, r_t: [-741.553 -741.553 -741.553], eps: 0.001})
Step:  162400, Reward: [-389.061 -389.061 -389.061] [56.595], Avg: [-402.790 -402.790 -402.790] (0.0010) ({r_i: None, r_t: [-750.548 -750.548 -750.548], eps: 0.001})
Step:  162500, Reward: [-364.766 -364.766 -364.766] [85.432], Avg: [-402.766 -402.766 -402.766] (0.0010) ({r_i: None, r_t: [-742.678 -742.678 -742.678], eps: 0.001})
Step:  162600, Reward: [-365.083 -365.083 -365.083] [48.252], Avg: [-402.743 -402.743 -402.743] (0.0010) ({r_i: None, r_t: [-734.609 -734.609 -734.609], eps: 0.001})
Step:  162700, Reward: [-376.658 -376.658 -376.658] [71.177], Avg: [-402.727 -402.727 -402.727] (0.0010) ({r_i: None, r_t: [-795.108 -795.108 -795.108], eps: 0.001})
Step:  162800, Reward: [-374.119 -374.119 -374.119] [63.913], Avg: [-402.709 -402.709 -402.709] (0.0010) ({r_i: None, r_t: [-746.601 -746.601 -746.601], eps: 0.001})
Step:  162900, Reward: [-371.740 -371.740 -371.740] [60.351], Avg: [-402.690 -402.690 -402.690] (0.0010) ({r_i: None, r_t: [-732.164 -732.164 -732.164], eps: 0.001})
Step:  163000, Reward: [-346.700 -346.700 -346.700] [62.755], Avg: [-402.656 -402.656 -402.656] (0.0010) ({r_i: None, r_t: [-743.553 -743.553 -743.553], eps: 0.001})
Step:  163100, Reward: [-336.644 -336.644 -336.644] [74.259], Avg: [-402.616 -402.616 -402.616] (0.0010) ({r_i: None, r_t: [-742.101 -742.101 -742.101], eps: 0.001})
Step:  163200, Reward: [-394.432 -394.432 -394.432] [80.753], Avg: [-402.611 -402.611 -402.611] (0.0010) ({r_i: None, r_t: [-711.391 -711.391 -711.391], eps: 0.001})
Step:  163300, Reward: [-353.668 -353.668 -353.668] [45.177], Avg: [-402.581 -402.581 -402.581] (0.0010) ({r_i: None, r_t: [-721.944 -721.944 -721.944], eps: 0.001})
Step:  163400, Reward: [-350.618 -350.618 -350.618] [49.639], Avg: [-402.549 -402.549 -402.549] (0.0010) ({r_i: None, r_t: [-761.592 -761.592 -761.592], eps: 0.001})
Step:  163500, Reward: [-367.736 -367.736 -367.736] [53.813], Avg: [-402.528 -402.528 -402.528] (0.0010) ({r_i: None, r_t: [-748.604 -748.604 -748.604], eps: 0.001})
Step:  163600, Reward: [-367.300 -367.300 -367.300] [72.750], Avg: [-402.506 -402.506 -402.506] (0.0010) ({r_i: None, r_t: [-742.750 -742.750 -742.750], eps: 0.001})
Step:  163700, Reward: [-361.237 -361.237 -361.237] [50.114], Avg: [-402.481 -402.481 -402.481] (0.0010) ({r_i: None, r_t: [-713.913 -713.913 -713.913], eps: 0.001})
Step:  163800, Reward: [-384.656 -384.656 -384.656] [77.942], Avg: [-402.470 -402.470 -402.470] (0.0010) ({r_i: None, r_t: [-727.504 -727.504 -727.504], eps: 0.001})
Step:  163900, Reward: [-352.994 -352.994 -352.994] [50.992], Avg: [-402.440 -402.440 -402.440] (0.0010) ({r_i: None, r_t: [-682.989 -682.989 -682.989], eps: 0.001})
Step:  164000, Reward: [-364.741 -364.741 -364.741] [70.690], Avg: [-402.417 -402.417 -402.417] (0.0010) ({r_i: None, r_t: [-754.855 -754.855 -754.855], eps: 0.001})
Step:  164100, Reward: [-355.103 -355.103 -355.103] [58.428], Avg: [-402.388 -402.388 -402.388] (0.0010) ({r_i: None, r_t: [-725.902 -725.902 -725.902], eps: 0.001})
Step:  164200, Reward: [-353.541 -353.541 -353.541] [66.978], Avg: [-402.358 -402.358 -402.358] (0.0010) ({r_i: None, r_t: [-755.007 -755.007 -755.007], eps: 0.001})
Step:  164300, Reward: [-379.283 -379.283 -379.283] [87.234], Avg: [-402.344 -402.344 -402.344] (0.0010) ({r_i: None, r_t: [-762.218 -762.218 -762.218], eps: 0.001})
Step:  164400, Reward: [-396.574 -396.574 -396.574] [73.343], Avg: [-402.341 -402.341 -402.341] (0.0010) ({r_i: None, r_t: [-750.499 -750.499 -750.499], eps: 0.001})
Step:  164500, Reward: [-381.403 -381.403 -381.403] [57.889], Avg: [-402.328 -402.328 -402.328] (0.0010) ({r_i: None, r_t: [-770.149 -770.149 -770.149], eps: 0.001})
Step:  164600, Reward: [-368.724 -368.724 -368.724] [83.392], Avg: [-402.308 -402.308 -402.308] (0.0010) ({r_i: None, r_t: [-772.817 -772.817 -772.817], eps: 0.001})
Step:  164700, Reward: [-389.836 -389.836 -389.836] [74.604], Avg: [-402.300 -402.300 -402.300] (0.0010) ({r_i: None, r_t: [-725.787 -725.787 -725.787], eps: 0.001})
Step:  164800, Reward: [-380.481 -380.481 -380.481] [54.097], Avg: [-402.287 -402.287 -402.287] (0.0010) ({r_i: None, r_t: [-741.409 -741.409 -741.409], eps: 0.001})
Step:  164900, Reward: [-377.752 -377.752 -377.752] [72.159], Avg: [-402.272 -402.272 -402.272] (0.0010) ({r_i: None, r_t: [-707.715 -707.715 -707.715], eps: 0.001})
Step:  165000, Reward: [-395.859 -395.859 -395.859] [76.631], Avg: [-402.268 -402.268 -402.268] (0.0010) ({r_i: None, r_t: [-726.980 -726.980 -726.980], eps: 0.001})
Step:  165100, Reward: [-362.397 -362.397 -362.397] [60.237], Avg: [-402.244 -402.244 -402.244] (0.0010) ({r_i: None, r_t: [-761.022 -761.022 -761.022], eps: 0.001})
Step:  165200, Reward: [-372.425 -372.425 -372.425] [68.214], Avg: [-402.226 -402.226 -402.226] (0.0010) ({r_i: None, r_t: [-776.299 -776.299 -776.299], eps: 0.001})
Step:  165300, Reward: [-390.197 -390.197 -390.197] [80.283], Avg: [-402.219 -402.219 -402.219] (0.0010) ({r_i: None, r_t: [-764.096 -764.096 -764.096], eps: 0.001})
Step:  165400, Reward: [-361.936 -361.936 -361.936] [62.360], Avg: [-402.194 -402.194 -402.194] (0.0010) ({r_i: None, r_t: [-738.303 -738.303 -738.303], eps: 0.001})
Step:  165500, Reward: [-366.757 -366.757 -366.757] [57.144], Avg: [-402.173 -402.173 -402.173] (0.0010) ({r_i: None, r_t: [-726.065 -726.065 -726.065], eps: 0.001})
Step:  165600, Reward: [-372.957 -372.957 -372.957] [59.295], Avg: [-402.155 -402.155 -402.155] (0.0010) ({r_i: None, r_t: [-750.497 -750.497 -750.497], eps: 0.001})
Step:  165700, Reward: [-373.670 -373.670 -373.670] [67.205], Avg: [-402.138 -402.138 -402.138] (0.0010) ({r_i: None, r_t: [-758.549 -758.549 -758.549], eps: 0.001})
Step:  165800, Reward: [-388.119 -388.119 -388.119] [79.634], Avg: [-402.130 -402.130 -402.130] (0.0010) ({r_i: None, r_t: [-779.819 -779.819 -779.819], eps: 0.001})
Step:  165900, Reward: [-370.647 -370.647 -370.647] [67.274], Avg: [-402.111 -402.111 -402.111] (0.0010) ({r_i: None, r_t: [-766.000 -766.000 -766.000], eps: 0.001})
Step:  166000, Reward: [-371.520 -371.520 -371.520] [82.937], Avg: [-402.092 -402.092 -402.092] (0.0010) ({r_i: None, r_t: [-776.895 -776.895 -776.895], eps: 0.001})
Step:  166100, Reward: [-384.970 -384.970 -384.970] [82.997], Avg: [-402.082 -402.082 -402.082] (0.0010) ({r_i: None, r_t: [-768.655 -768.655 -768.655], eps: 0.001})
Step:  166200, Reward: [-375.954 -375.954 -375.954] [55.496], Avg: [-402.066 -402.066 -402.066] (0.0010) ({r_i: None, r_t: [-775.760 -775.760 -775.760], eps: 0.001})
Step:  166300, Reward: [-393.182 -393.182 -393.182] [57.109], Avg: [-402.061 -402.061 -402.061] (0.0010) ({r_i: None, r_t: [-766.587 -766.587 -766.587], eps: 0.001})
Step:  166400, Reward: [-374.817 -374.817 -374.817] [66.394], Avg: [-402.045 -402.045 -402.045] (0.0010) ({r_i: None, r_t: [-685.344 -685.344 -685.344], eps: 0.001})
Step:  166500, Reward: [-361.635 -361.635 -361.635] [72.642], Avg: [-402.020 -402.020 -402.020] (0.0010) ({r_i: None, r_t: [-757.929 -757.929 -757.929], eps: 0.001})
Step:  166600, Reward: [-384.091 -384.091 -384.091] [59.594], Avg: [-402.010 -402.010 -402.010] (0.0010) ({r_i: None, r_t: [-763.768 -763.768 -763.768], eps: 0.001})
Step:  166700, Reward: [-335.474 -335.474 -335.474] [57.112], Avg: [-401.970 -401.970 -401.970] (0.0010) ({r_i: None, r_t: [-741.384 -741.384 -741.384], eps: 0.001})
Step:  166800, Reward: [-381.307 -381.307 -381.307] [67.149], Avg: [-401.957 -401.957 -401.957] (0.0010) ({r_i: None, r_t: [-772.342 -772.342 -772.342], eps: 0.001})
Step:  166900, Reward: [-389.440 -389.440 -389.440] [67.042], Avg: [-401.950 -401.950 -401.950] (0.0010) ({r_i: None, r_t: [-777.516 -777.516 -777.516], eps: 0.001})
Step:  167000, Reward: [-363.223 -363.223 -363.223] [66.718], Avg: [-401.927 -401.927 -401.927] (0.0010) ({r_i: None, r_t: [-750.296 -750.296 -750.296], eps: 0.001})
Step:  167100, Reward: [-364.065 -364.065 -364.065] [46.315], Avg: [-401.904 -401.904 -401.904] (0.0010) ({r_i: None, r_t: [-735.100 -735.100 -735.100], eps: 0.001})
Step:  167200, Reward: [-352.995 -352.995 -352.995] [68.676], Avg: [-401.875 -401.875 -401.875] (0.0010) ({r_i: None, r_t: [-741.915 -741.915 -741.915], eps: 0.001})
Step:  167300, Reward: [-401.870 -401.870 -401.870] [65.451], Avg: [-401.875 -401.875 -401.875] (0.0010) ({r_i: None, r_t: [-748.804 -748.804 -748.804], eps: 0.001})
Step:  167400, Reward: [-360.023 -360.023 -360.023] [44.888], Avg: [-401.850 -401.850 -401.850] (0.0010) ({r_i: None, r_t: [-718.237 -718.237 -718.237], eps: 0.001})
Step:  167500, Reward: [-396.893 -396.893 -396.893] [92.458], Avg: [-401.847 -401.847 -401.847] (0.0010) ({r_i: None, r_t: [-764.188 -764.188 -764.188], eps: 0.001})
Step:  167600, Reward: [-366.496 -366.496 -366.496] [68.944], Avg: [-401.826 -401.826 -401.826] (0.0010) ({r_i: None, r_t: [-745.190 -745.190 -745.190], eps: 0.001})
Step:  167700, Reward: [-366.316 -366.316 -366.316] [74.821], Avg: [-401.805 -401.805 -401.805] (0.0010) ({r_i: None, r_t: [-830.759 -830.759 -830.759], eps: 0.001})
Step:  167800, Reward: [-356.644 -356.644 -356.644] [60.105], Avg: [-401.778 -401.778 -401.778] (0.0010) ({r_i: None, r_t: [-746.712 -746.712 -746.712], eps: 0.001})
Step:  167900, Reward: [-363.926 -363.926 -363.926] [62.767], Avg: [-401.755 -401.755 -401.755] (0.0010) ({r_i: None, r_t: [-721.589 -721.589 -721.589], eps: 0.001})
Step:  168000, Reward: [-349.590 -349.590 -349.590] [52.585], Avg: [-401.724 -401.724 -401.724] (0.0010) ({r_i: None, r_t: [-764.227 -764.227 -764.227], eps: 0.001})
Step:  168100, Reward: [-380.836 -380.836 -380.836] [60.669], Avg: [-401.712 -401.712 -401.712] (0.0010) ({r_i: None, r_t: [-759.019 -759.019 -759.019], eps: 0.001})
Step:  168200, Reward: [-396.772 -396.772 -396.772] [55.942], Avg: [-401.709 -401.709 -401.709] (0.0010) ({r_i: None, r_t: [-724.745 -724.745 -724.745], eps: 0.001})
Step:  168300, Reward: [-384.482 -384.482 -384.482] [64.112], Avg: [-401.698 -401.698 -401.698] (0.0010) ({r_i: None, r_t: [-737.461 -737.461 -737.461], eps: 0.001})
Step:  168400, Reward: [-351.641 -351.641 -351.641] [67.595], Avg: [-401.669 -401.669 -401.669] (0.0010) ({r_i: None, r_t: [-747.410 -747.410 -747.410], eps: 0.001})
Step:  168500, Reward: [-380.594 -380.594 -380.594] [69.414], Avg: [-401.656 -401.656 -401.656] (0.0010) ({r_i: None, r_t: [-788.081 -788.081 -788.081], eps: 0.001})
Step:  168600, Reward: [-400.612 -400.612 -400.612] [87.157], Avg: [-401.656 -401.656 -401.656] (0.0010) ({r_i: None, r_t: [-724.006 -724.006 -724.006], eps: 0.001})
Step:  168700, Reward: [-413.824 -413.824 -413.824] [53.169], Avg: [-401.663 -401.663 -401.663] (0.0010) ({r_i: None, r_t: [-725.883 -725.883 -725.883], eps: 0.001})
Step:  168800, Reward: [-386.289 -386.289 -386.289] [67.503], Avg: [-401.654 -401.654 -401.654] (0.0010) ({r_i: None, r_t: [-728.918 -728.918 -728.918], eps: 0.001})
Step:  168900, Reward: [-348.776 -348.776 -348.776] [61.144], Avg: [-401.622 -401.622 -401.622] (0.0010) ({r_i: None, r_t: [-718.980 -718.980 -718.980], eps: 0.001})
Step:  169000, Reward: [-353.513 -353.513 -353.513] [63.928], Avg: [-401.594 -401.594 -401.594] (0.0010) ({r_i: None, r_t: [-732.622 -732.622 -732.622], eps: 0.001})
Step:  169100, Reward: [-363.483 -363.483 -363.483] [46.441], Avg: [-401.571 -401.571 -401.571] (0.0010) ({r_i: None, r_t: [-766.669 -766.669 -766.669], eps: 0.001})
Step:  169200, Reward: [-390.592 -390.592 -390.592] [46.836], Avg: [-401.565 -401.565 -401.565] (0.0010) ({r_i: None, r_t: [-774.541 -774.541 -774.541], eps: 0.001})
Step:  169300, Reward: [-366.765 -366.765 -366.765] [62.760], Avg: [-401.544 -401.544 -401.544] (0.0010) ({r_i: None, r_t: [-752.222 -752.222 -752.222], eps: 0.001})
Step:  169400, Reward: [-372.777 -372.777 -372.777] [60.643], Avg: [-401.527 -401.527 -401.527] (0.0010) ({r_i: None, r_t: [-723.488 -723.488 -723.488], eps: 0.001})
Step:  169500, Reward: [-364.397 -364.397 -364.397] [64.702], Avg: [-401.506 -401.506 -401.506] (0.0010) ({r_i: None, r_t: [-710.345 -710.345 -710.345], eps: 0.001})
Step:  169600, Reward: [-376.966 -376.966 -376.966] [69.579], Avg: [-401.491 -401.491 -401.491] (0.0010) ({r_i: None, r_t: [-732.026 -732.026 -732.026], eps: 0.001})
Step:  169700, Reward: [-364.871 -364.871 -364.871] [56.156], Avg: [-401.470 -401.470 -401.470] (0.0010) ({r_i: None, r_t: [-764.498 -764.498 -764.498], eps: 0.001})
Step:  169800, Reward: [-390.949 -390.949 -390.949] [58.282], Avg: [-401.463 -401.463 -401.463] (0.0010) ({r_i: None, r_t: [-718.655 -718.655 -718.655], eps: 0.001})
Step:  169900, Reward: [-338.926 -338.926 -338.926] [63.423], Avg: [-401.427 -401.427 -401.427] (0.0010) ({r_i: None, r_t: [-743.064 -743.064 -743.064], eps: 0.001})
Step:  170000, Reward: [-343.798 -343.798 -343.798] [44.978], Avg: [-401.393 -401.393 -401.393] (0.0010) ({r_i: None, r_t: [-747.835 -747.835 -747.835], eps: 0.001})
Step:  170100, Reward: [-335.933 -335.933 -335.933] [49.209], Avg: [-401.354 -401.354 -401.354] (0.0010) ({r_i: None, r_t: [-751.653 -751.653 -751.653], eps: 0.001})
Step:  170200, Reward: [-364.418 -364.418 -364.418] [60.332], Avg: [-401.333 -401.333 -401.333] (0.0010) ({r_i: None, r_t: [-742.260 -742.260 -742.260], eps: 0.001})
Step:  170300, Reward: [-381.604 -381.604 -381.604] [50.998], Avg: [-401.321 -401.321 -401.321] (0.0010) ({r_i: None, r_t: [-778.414 -778.414 -778.414], eps: 0.001})
Step:  170400, Reward: [-369.836 -369.836 -369.836] [67.348], Avg: [-401.303 -401.303 -401.303] (0.0010) ({r_i: None, r_t: [-742.464 -742.464 -742.464], eps: 0.001})
Step:  170500, Reward: [-372.390 -372.390 -372.390] [77.703], Avg: [-401.286 -401.286 -401.286] (0.0010) ({r_i: None, r_t: [-715.472 -715.472 -715.472], eps: 0.001})
Step:  170600, Reward: [-371.747 -371.747 -371.747] [60.378], Avg: [-401.268 -401.268 -401.268] (0.0010) ({r_i: None, r_t: [-713.578 -713.578 -713.578], eps: 0.001})
Step:  170700, Reward: [-390.771 -390.771 -390.771] [65.127], Avg: [-401.262 -401.262 -401.262] (0.0010) ({r_i: None, r_t: [-729.354 -729.354 -729.354], eps: 0.001})
Step:  170800, Reward: [-371.271 -371.271 -371.271] [73.201], Avg: [-401.245 -401.245 -401.245] (0.0010) ({r_i: None, r_t: [-750.522 -750.522 -750.522], eps: 0.001})
Step:  170900, Reward: [-383.804 -383.804 -383.804] [76.353], Avg: [-401.234 -401.234 -401.234] (0.0010) ({r_i: None, r_t: [-769.474 -769.474 -769.474], eps: 0.001})
Step:  171000, Reward: [-351.202 -351.202 -351.202] [77.301], Avg: [-401.205 -401.205 -401.205] (0.0010) ({r_i: None, r_t: [-799.440 -799.440 -799.440], eps: 0.001})
Step:  171100, Reward: [-392.468 -392.468 -392.468] [65.664], Avg: [-401.200 -401.200 -401.200] (0.0010) ({r_i: None, r_t: [-696.433 -696.433 -696.433], eps: 0.001})
Step:  171200, Reward: [-367.807 -367.807 -367.807] [76.634], Avg: [-401.181 -401.181 -401.181] (0.0010) ({r_i: None, r_t: [-776.745 -776.745 -776.745], eps: 0.001})
Step:  171300, Reward: [-359.111 -359.111 -359.111] [64.587], Avg: [-401.156 -401.156 -401.156] (0.0010) ({r_i: None, r_t: [-746.255 -746.255 -746.255], eps: 0.001})
Step:  171400, Reward: [-366.876 -366.876 -366.876] [56.570], Avg: [-401.136 -401.136 -401.136] (0.0010) ({r_i: None, r_t: [-724.139 -724.139 -724.139], eps: 0.001})
Step:  171500, Reward: [-362.832 -362.832 -362.832] [80.962], Avg: [-401.114 -401.114 -401.114] (0.0010) ({r_i: None, r_t: [-721.644 -721.644 -721.644], eps: 0.001})
Step:  171600, Reward: [-372.009 -372.009 -372.009] [78.479], Avg: [-401.097 -401.097 -401.097] (0.0010) ({r_i: None, r_t: [-722.922 -722.922 -722.922], eps: 0.001})
Step:  171700, Reward: [-387.430 -387.430 -387.430] [71.200], Avg: [-401.089 -401.089 -401.089] (0.0010) ({r_i: None, r_t: [-730.883 -730.883 -730.883], eps: 0.001})
Step:  171800, Reward: [-362.728 -362.728 -362.728] [67.994], Avg: [-401.066 -401.066 -401.066] (0.0010) ({r_i: None, r_t: [-757.470 -757.470 -757.470], eps: 0.001})
Step:  171900, Reward: [-366.949 -366.949 -366.949] [60.326], Avg: [-401.047 -401.047 -401.047] (0.0010) ({r_i: None, r_t: [-773.743 -773.743 -773.743], eps: 0.001})
Step:  172000, Reward: [-380.731 -380.731 -380.731] [63.319], Avg: [-401.035 -401.035 -401.035] (0.0010) ({r_i: None, r_t: [-747.834 -747.834 -747.834], eps: 0.001})
Step:  172100, Reward: [-350.158 -350.158 -350.158] [68.715], Avg: [-401.005 -401.005 -401.005] (0.0010) ({r_i: None, r_t: [-772.563 -772.563 -772.563], eps: 0.001})
Step:  172200, Reward: [-378.186 -378.186 -378.186] [78.744], Avg: [-400.992 -400.992 -400.992] (0.0010) ({r_i: None, r_t: [-755.585 -755.585 -755.585], eps: 0.001})
Step:  172300, Reward: [-369.582 -369.582 -369.582] [49.682], Avg: [-400.974 -400.974 -400.974] (0.0010) ({r_i: None, r_t: [-719.956 -719.956 -719.956], eps: 0.001})
Step:  172400, Reward: [-408.756 -408.756 -408.756] [69.341], Avg: [-400.978 -400.978 -400.978] (0.0010) ({r_i: None, r_t: [-743.407 -743.407 -743.407], eps: 0.001})
Step:  172500, Reward: [-364.711 -364.711 -364.711] [80.066], Avg: [-400.957 -400.957 -400.957] (0.0010) ({r_i: None, r_t: [-748.746 -748.746 -748.746], eps: 0.001})
Step:  172600, Reward: [-384.345 -384.345 -384.345] [53.531], Avg: [-400.948 -400.948 -400.948] (0.0010) ({r_i: None, r_t: [-732.916 -732.916 -732.916], eps: 0.001})
Step:  172700, Reward: [-398.319 -398.319 -398.319] [72.221], Avg: [-400.946 -400.946 -400.946] (0.0010) ({r_i: None, r_t: [-721.161 -721.161 -721.161], eps: 0.001})
Step:  172800, Reward: [-347.588 -347.588 -347.588] [86.547], Avg: [-400.915 -400.915 -400.915] (0.0010) ({r_i: None, r_t: [-739.886 -739.886 -739.886], eps: 0.001})
Step:  172900, Reward: [-377.851 -377.851 -377.851] [101.565], Avg: [-400.902 -400.902 -400.902] (0.0010) ({r_i: None, r_t: [-709.843 -709.843 -709.843], eps: 0.001})
Step:  173000, Reward: [-382.994 -382.994 -382.994] [58.540], Avg: [-400.892 -400.892 -400.892] (0.0010) ({r_i: None, r_t: [-752.818 -752.818 -752.818], eps: 0.001})
Step:  173100, Reward: [-358.959 -358.959 -358.959] [72.475], Avg: [-400.867 -400.867 -400.867] (0.0010) ({r_i: None, r_t: [-794.611 -794.611 -794.611], eps: 0.001})
Step:  173200, Reward: [-363.689 -363.689 -363.689] [68.272], Avg: [-400.846 -400.846 -400.846] (0.0010) ({r_i: None, r_t: [-754.947 -754.947 -754.947], eps: 0.001})
Step:  173300, Reward: [-382.140 -382.140 -382.140] [62.732], Avg: [-400.835 -400.835 -400.835] (0.0010) ({r_i: None, r_t: [-751.563 -751.563 -751.563], eps: 0.001})
Step:  173400, Reward: [-361.789 -361.789 -361.789] [76.338], Avg: [-400.813 -400.813 -400.813] (0.0010) ({r_i: None, r_t: [-735.923 -735.923 -735.923], eps: 0.001})
Step:  173500, Reward: [-349.584 -349.584 -349.584] [52.173], Avg: [-400.783 -400.783 -400.783] (0.0010) ({r_i: None, r_t: [-753.081 -753.081 -753.081], eps: 0.001})
Step:  173600, Reward: [-355.465 -355.465 -355.465] [60.378], Avg: [-400.757 -400.757 -400.757] (0.0010) ({r_i: None, r_t: [-732.905 -732.905 -732.905], eps: 0.001})
Step:  173700, Reward: [-380.375 -380.375 -380.375] [58.374], Avg: [-400.745 -400.745 -400.745] (0.0010) ({r_i: None, r_t: [-711.540 -711.540 -711.540], eps: 0.001})
Step:  173800, Reward: [-375.766 -375.766 -375.766] [55.981], Avg: [-400.731 -400.731 -400.731] (0.0010) ({r_i: None, r_t: [-749.370 -749.370 -749.370], eps: 0.001})
Step:  173900, Reward: [-388.931 -388.931 -388.931] [68.148], Avg: [-400.724 -400.724 -400.724] (0.0010) ({r_i: None, r_t: [-764.657 -764.657 -764.657], eps: 0.001})
Step:  174000, Reward: [-378.951 -378.951 -378.951] [61.590], Avg: [-400.712 -400.712 -400.712] (0.0010) ({r_i: None, r_t: [-734.551 -734.551 -734.551], eps: 0.001})
Step:  174100, Reward: [-373.518 -373.518 -373.518] [50.347], Avg: [-400.696 -400.696 -400.696] (0.0010) ({r_i: None, r_t: [-715.390 -715.390 -715.390], eps: 0.001})
Step:  174200, Reward: [-384.339 -384.339 -384.339] [49.384], Avg: [-400.687 -400.687 -400.687] (0.0010) ({r_i: None, r_t: [-749.184 -749.184 -749.184], eps: 0.001})
Step:  174300, Reward: [-383.399 -383.399 -383.399] [85.946], Avg: [-400.677 -400.677 -400.677] (0.0010) ({r_i: None, r_t: [-762.306 -762.306 -762.306], eps: 0.001})
Step:  174400, Reward: [-377.514 -377.514 -377.514] [57.878], Avg: [-400.664 -400.664 -400.664] (0.0010) ({r_i: None, r_t: [-758.131 -758.131 -758.131], eps: 0.001})
Step:  174500, Reward: [-365.786 -365.786 -365.786] [64.476], Avg: [-400.644 -400.644 -400.644] (0.0010) ({r_i: None, r_t: [-734.498 -734.498 -734.498], eps: 0.001})
Step:  174600, Reward: [-358.891 -358.891 -358.891] [59.913], Avg: [-400.620 -400.620 -400.620] (0.0010) ({r_i: None, r_t: [-764.023 -764.023 -764.023], eps: 0.001})
Step:  174700, Reward: [-394.919 -394.919 -394.919] [72.079], Avg: [-400.616 -400.616 -400.616] (0.0010) ({r_i: None, r_t: [-689.478 -689.478 -689.478], eps: 0.001})
Step:  174800, Reward: [-372.562 -372.562 -372.562] [38.957], Avg: [-400.600 -400.600 -400.600] (0.0010) ({r_i: None, r_t: [-737.033 -737.033 -737.033], eps: 0.001})
Step:  174900, Reward: [-360.708 -360.708 -360.708] [53.478], Avg: [-400.578 -400.578 -400.578] (0.0010) ({r_i: None, r_t: [-745.256 -745.256 -745.256], eps: 0.001})
Step:  175000, Reward: [-361.464 -361.464 -361.464] [50.951], Avg: [-400.555 -400.555 -400.555] (0.0010) ({r_i: None, r_t: [-781.365 -781.365 -781.365], eps: 0.001})
Step:  175100, Reward: [-372.908 -372.908 -372.908] [60.619], Avg: [-400.539 -400.539 -400.539] (0.0010) ({r_i: None, r_t: [-739.922 -739.922 -739.922], eps: 0.001})
Step:  175200, Reward: [-338.057 -338.057 -338.057] [49.950], Avg: [-400.504 -400.504 -400.504] (0.0010) ({r_i: None, r_t: [-744.106 -744.106 -744.106], eps: 0.001})
Step:  175300, Reward: [-358.571 -358.571 -358.571] [58.492], Avg: [-400.480 -400.480 -400.480] (0.0010) ({r_i: None, r_t: [-769.362 -769.362 -769.362], eps: 0.001})
Step:  175400, Reward: [-362.370 -362.370 -362.370] [35.743], Avg: [-400.458 -400.458 -400.458] (0.0010) ({r_i: None, r_t: [-772.419 -772.419 -772.419], eps: 0.001})
Step:  175500, Reward: [-396.545 -396.545 -396.545] [62.029], Avg: [-400.456 -400.456 -400.456] (0.0010) ({r_i: None, r_t: [-710.421 -710.421 -710.421], eps: 0.001})
Step:  175600, Reward: [-376.296 -376.296 -376.296] [69.899], Avg: [-400.442 -400.442 -400.442] (0.0010) ({r_i: None, r_t: [-774.331 -774.331 -774.331], eps: 0.001})
Step:  175700, Reward: [-357.118 -357.118 -357.118] [96.747], Avg: [-400.418 -400.418 -400.418] (0.0010) ({r_i: None, r_t: [-743.204 -743.204 -743.204], eps: 0.001})
Step:  175800, Reward: [-384.101 -384.101 -384.101] [89.786], Avg: [-400.408 -400.408 -400.408] (0.0010) ({r_i: None, r_t: [-713.886 -713.886 -713.886], eps: 0.001})
Step:  175900, Reward: [-372.996 -372.996 -372.996] [65.290], Avg: [-400.393 -400.393 -400.393] (0.0010) ({r_i: None, r_t: [-734.397 -734.397 -734.397], eps: 0.001})
Step:  176000, Reward: [-358.811 -358.811 -358.811] [61.235], Avg: [-400.369 -400.369 -400.369] (0.0010) ({r_i: None, r_t: [-755.076 -755.076 -755.076], eps: 0.001})
Step:  176100, Reward: [-361.004 -361.004 -361.004] [55.334], Avg: [-400.347 -400.347 -400.347] (0.0010) ({r_i: None, r_t: [-757.572 -757.572 -757.572], eps: 0.001})
Step:  176200, Reward: [-379.997 -379.997 -379.997] [85.153], Avg: [-400.335 -400.335 -400.335] (0.0010) ({r_i: None, r_t: [-744.524 -744.524 -744.524], eps: 0.001})
Step:  176300, Reward: [-379.750 -379.750 -379.750] [66.043], Avg: [-400.324 -400.324 -400.324] (0.0010) ({r_i: None, r_t: [-784.798 -784.798 -784.798], eps: 0.001})
Step:  176400, Reward: [-384.285 -384.285 -384.285] [74.529], Avg: [-400.314 -400.314 -400.314] (0.0010) ({r_i: None, r_t: [-770.278 -770.278 -770.278], eps: 0.001})
Step:  176500, Reward: [-353.377 -353.377 -353.377] [61.503], Avg: [-400.288 -400.288 -400.288] (0.0010) ({r_i: None, r_t: [-777.691 -777.691 -777.691], eps: 0.001})
Step:  176600, Reward: [-369.663 -369.663 -369.663] [53.644], Avg: [-400.271 -400.271 -400.271] (0.0010) ({r_i: None, r_t: [-765.064 -765.064 -765.064], eps: 0.001})
Step:  176700, Reward: [-379.374 -379.374 -379.374] [57.194], Avg: [-400.259 -400.259 -400.259] (0.0010) ({r_i: None, r_t: [-752.589 -752.589 -752.589], eps: 0.001})
Step:  176800, Reward: [-401.489 -401.489 -401.489] [69.773], Avg: [-400.259 -400.259 -400.259] (0.0010) ({r_i: None, r_t: [-751.221 -751.221 -751.221], eps: 0.001})
Step:  176900, Reward: [-381.058 -381.058 -381.058] [75.392], Avg: [-400.249 -400.249 -400.249] (0.0010) ({r_i: None, r_t: [-754.332 -754.332 -754.332], eps: 0.001})
Step:  177000, Reward: [-342.515 -342.515 -342.515] [57.205], Avg: [-400.216 -400.216 -400.216] (0.0010) ({r_i: None, r_t: [-726.539 -726.539 -726.539], eps: 0.001})
Step:  177100, Reward: [-365.332 -365.332 -365.332] [39.165], Avg: [-400.196 -400.196 -400.196] (0.0010) ({r_i: None, r_t: [-748.579 -748.579 -748.579], eps: 0.001})
Step:  177200, Reward: [-363.208 -363.208 -363.208] [95.317], Avg: [-400.175 -400.175 -400.175] (0.0010) ({r_i: None, r_t: [-740.286 -740.286 -740.286], eps: 0.001})
Step:  177300, Reward: [-408.963 -408.963 -408.963] [52.408], Avg: [-400.180 -400.180 -400.180] (0.0010) ({r_i: None, r_t: [-749.662 -749.662 -749.662], eps: 0.001})
Step:  177400, Reward: [-390.381 -390.381 -390.381] [83.302], Avg: [-400.175 -400.175 -400.175] (0.0010) ({r_i: None, r_t: [-718.713 -718.713 -718.713], eps: 0.001})
Step:  177500, Reward: [-354.405 -354.405 -354.405] [66.054], Avg: [-400.149 -400.149 -400.149] (0.0010) ({r_i: None, r_t: [-726.952 -726.952 -726.952], eps: 0.001})
Step:  177600, Reward: [-365.159 -365.159 -365.159] [53.660], Avg: [-400.129 -400.129 -400.129] (0.0010) ({r_i: None, r_t: [-792.218 -792.218 -792.218], eps: 0.001})
Step:  177700, Reward: [-375.408 -375.408 -375.408] [76.920], Avg: [-400.115 -400.115 -400.115] (0.0010) ({r_i: None, r_t: [-782.693 -782.693 -782.693], eps: 0.001})
Step:  177800, Reward: [-364.578 -364.578 -364.578] [59.637], Avg: [-400.095 -400.095 -400.095] (0.0010) ({r_i: None, r_t: [-766.013 -766.013 -766.013], eps: 0.001})
Step:  177900, Reward: [-425.952 -425.952 -425.952] [67.244], Avg: [-400.110 -400.110 -400.110] (0.0010) ({r_i: None, r_t: [-765.576 -765.576 -765.576], eps: 0.001})
Step:  178000, Reward: [-393.951 -393.951 -393.951] [48.678], Avg: [-400.107 -400.107 -400.107] (0.0010) ({r_i: None, r_t: [-733.781 -733.781 -733.781], eps: 0.001})
Step:  178100, Reward: [-400.595 -400.595 -400.595] [75.586], Avg: [-400.107 -400.107 -400.107] (0.0010) ({r_i: None, r_t: [-757.240 -757.240 -757.240], eps: 0.001})
Step:  178200, Reward: [-369.250 -369.250 -369.250] [61.101], Avg: [-400.090 -400.090 -400.090] (0.0010) ({r_i: None, r_t: [-761.167 -761.167 -761.167], eps: 0.001})
Step:  178300, Reward: [-350.954 -350.954 -350.954] [69.692], Avg: [-400.062 -400.062 -400.062] (0.0010) ({r_i: None, r_t: [-768.663 -768.663 -768.663], eps: 0.001})
Step:  178400, Reward: [-351.775 -351.775 -351.775] [41.469], Avg: [-400.035 -400.035 -400.035] (0.0010) ({r_i: None, r_t: [-782.350 -782.350 -782.350], eps: 0.001})
Step:  178500, Reward: [-373.306 -373.306 -373.306] [59.784], Avg: [-400.020 -400.020 -400.020] (0.0010) ({r_i: None, r_t: [-775.012 -775.012 -775.012], eps: 0.001})
Step:  178600, Reward: [-404.290 -404.290 -404.290] [76.826], Avg: [-400.022 -400.022 -400.022] (0.0010) ({r_i: None, r_t: [-748.063 -748.063 -748.063], eps: 0.001})
Step:  178700, Reward: [-340.199 -340.199 -340.199] [63.408], Avg: [-399.989 -399.989 -399.989] (0.0010) ({r_i: None, r_t: [-794.566 -794.566 -794.566], eps: 0.001})
Step:  178800, Reward: [-389.557 -389.557 -389.557] [62.928], Avg: [-399.983 -399.983 -399.983] (0.0010) ({r_i: None, r_t: [-729.444 -729.444 -729.444], eps: 0.001})
Step:  178900, Reward: [-361.484 -361.484 -361.484] [62.369], Avg: [-399.962 -399.962 -399.962] (0.0010) ({r_i: None, r_t: [-760.073 -760.073 -760.073], eps: 0.001})
Step:  179000, Reward: [-380.771 -380.771 -380.771] [75.922], Avg: [-399.951 -399.951 -399.951] (0.0010) ({r_i: None, r_t: [-757.325 -757.325 -757.325], eps: 0.001})
Step:  179100, Reward: [-368.761 -368.761 -368.761] [72.636], Avg: [-399.933 -399.933 -399.933] (0.0010) ({r_i: None, r_t: [-744.355 -744.355 -744.355], eps: 0.001})
Step:  179200, Reward: [-397.713 -397.713 -397.713] [47.015], Avg: [-399.932 -399.932 -399.932] (0.0010) ({r_i: None, r_t: [-763.424 -763.424 -763.424], eps: 0.001})
Step:  179300, Reward: [-350.294 -350.294 -350.294] [57.186], Avg: [-399.905 -399.905 -399.905] (0.0010) ({r_i: None, r_t: [-753.380 -753.380 -753.380], eps: 0.001})
Step:  179400, Reward: [-360.802 -360.802 -360.802] [55.848], Avg: [-399.883 -399.883 -399.883] (0.0010) ({r_i: None, r_t: [-747.525 -747.525 -747.525], eps: 0.001})
Step:  179500, Reward: [-348.500 -348.500 -348.500] [64.674], Avg: [-399.854 -399.854 -399.854] (0.0010) ({r_i: None, r_t: [-735.634 -735.634 -735.634], eps: 0.001})
Step:  179600, Reward: [-362.617 -362.617 -362.617] [53.019], Avg: [-399.833 -399.833 -399.833] (0.0010) ({r_i: None, r_t: [-738.020 -738.020 -738.020], eps: 0.001})
Step:  179700, Reward: [-372.219 -372.219 -372.219] [82.264], Avg: [-399.818 -399.818 -399.818] (0.0010) ({r_i: None, r_t: [-779.127 -779.127 -779.127], eps: 0.001})
Step:  179800, Reward: [-360.247 -360.247 -360.247] [59.788], Avg: [-399.796 -399.796 -399.796] (0.0010) ({r_i: None, r_t: [-753.159 -753.159 -753.159], eps: 0.001})
Step:  179900, Reward: [-359.947 -359.947 -359.947] [84.440], Avg: [-399.774 -399.774 -399.774] (0.0010) ({r_i: None, r_t: [-763.918 -763.918 -763.918], eps: 0.001})
Step:  180000, Reward: [-369.467 -369.467 -369.467] [55.104], Avg: [-399.757 -399.757 -399.757] (0.0010) ({r_i: None, r_t: [-751.259 -751.259 -751.259], eps: 0.001})
Step:  180100, Reward: [-384.278 -384.278 -384.278] [85.957], Avg: [-399.749 -399.749 -399.749] (0.0010) ({r_i: None, r_t: [-747.300 -747.300 -747.300], eps: 0.001})
Step:  180200, Reward: [-389.333 -389.333 -389.333] [73.569], Avg: [-399.743 -399.743 -399.743] (0.0010) ({r_i: None, r_t: [-751.982 -751.982 -751.982], eps: 0.001})
Step:  180300, Reward: [-393.767 -393.767 -393.767] [75.637], Avg: [-399.739 -399.739 -399.739] (0.0010) ({r_i: None, r_t: [-786.858 -786.858 -786.858], eps: 0.001})
Step:  180400, Reward: [-390.059 -390.059 -390.059] [63.017], Avg: [-399.734 -399.734 -399.734] (0.0010) ({r_i: None, r_t: [-788.496 -788.496 -788.496], eps: 0.001})
Step:  180500, Reward: [-366.876 -366.876 -366.876] [57.660], Avg: [-399.716 -399.716 -399.716] (0.0010) ({r_i: None, r_t: [-771.416 -771.416 -771.416], eps: 0.001})
Step:  180600, Reward: [-369.600 -369.600 -369.600] [64.193], Avg: [-399.699 -399.699 -399.699] (0.0010) ({r_i: None, r_t: [-745.352 -745.352 -745.352], eps: 0.001})
Step:  180700, Reward: [-374.774 -374.774 -374.774] [61.432], Avg: [-399.685 -399.685 -399.685] (0.0010) ({r_i: None, r_t: [-768.437 -768.437 -768.437], eps: 0.001})
Step:  180800, Reward: [-379.950 -379.950 -379.950] [60.100], Avg: [-399.674 -399.674 -399.674] (0.0010) ({r_i: None, r_t: [-749.274 -749.274 -749.274], eps: 0.001})
Step:  180900, Reward: [-379.496 -379.496 -379.496] [78.742], Avg: [-399.663 -399.663 -399.663] (0.0010) ({r_i: None, r_t: [-752.353 -752.353 -752.353], eps: 0.001})
Step:  181000, Reward: [-356.092 -356.092 -356.092] [65.082], Avg: [-399.639 -399.639 -399.639] (0.0010) ({r_i: None, r_t: [-788.310 -788.310 -788.310], eps: 0.001})
Step:  181100, Reward: [-404.457 -404.457 -404.457] [63.008], Avg: [-399.642 -399.642 -399.642] (0.0010) ({r_i: None, r_t: [-755.344 -755.344 -755.344], eps: 0.001})
Step:  181200, Reward: [-382.992 -382.992 -382.992] [39.878], Avg: [-399.633 -399.633 -399.633] (0.0010) ({r_i: None, r_t: [-725.329 -725.329 -725.329], eps: 0.001})
Step:  181300, Reward: [-413.879 -413.879 -413.879] [62.983], Avg: [-399.641 -399.641 -399.641] (0.0010) ({r_i: None, r_t: [-802.747 -802.747 -802.747], eps: 0.001})
Step:  181400, Reward: [-371.397 -371.397 -371.397] [66.415], Avg: [-399.625 -399.625 -399.625] (0.0010) ({r_i: None, r_t: [-759.000 -759.000 -759.000], eps: 0.001})
Step:  181500, Reward: [-380.250 -380.250 -380.250] [73.704], Avg: [-399.614 -399.614 -399.614] (0.0010) ({r_i: None, r_t: [-807.982 -807.982 -807.982], eps: 0.001})
Step:  181600, Reward: [-373.969 -373.969 -373.969] [67.472], Avg: [-399.600 -399.600 -399.600] (0.0010) ({r_i: None, r_t: [-757.260 -757.260 -757.260], eps: 0.001})
Step:  181700, Reward: [-386.523 -386.523 -386.523] [75.608], Avg: [-399.593 -399.593 -399.593] (0.0010) ({r_i: None, r_t: [-758.624 -758.624 -758.624], eps: 0.001})
Step:  181800, Reward: [-387.897 -387.897 -387.897] [57.930], Avg: [-399.587 -399.587 -399.587] (0.0010) ({r_i: None, r_t: [-752.185 -752.185 -752.185], eps: 0.001})
Step:  181900, Reward: [-375.007 -375.007 -375.007] [62.734], Avg: [-399.573 -399.573 -399.573] (0.0010) ({r_i: None, r_t: [-787.482 -787.482 -787.482], eps: 0.001})
Step:  182000, Reward: [-370.627 -370.627 -370.627] [60.531], Avg: [-399.557 -399.557 -399.557] (0.0010) ({r_i: None, r_t: [-766.767 -766.767 -766.767], eps: 0.001})
Step:  182100, Reward: [-378.723 -378.723 -378.723] [85.161], Avg: [-399.546 -399.546 -399.546] (0.0010) ({r_i: None, r_t: [-744.515 -744.515 -744.515], eps: 0.001})
Step:  182200, Reward: [-393.838 -393.838 -393.838] [58.715], Avg: [-399.543 -399.543 -399.543] (0.0010) ({r_i: None, r_t: [-821.808 -821.808 -821.808], eps: 0.001})
Step:  182300, Reward: [-381.505 -381.505 -381.505] [63.548], Avg: [-399.533 -399.533 -399.533] (0.0010) ({r_i: None, r_t: [-732.561 -732.561 -732.561], eps: 0.001})
Step:  182400, Reward: [-388.564 -388.564 -388.564] [53.665], Avg: [-399.527 -399.527 -399.527] (0.0010) ({r_i: None, r_t: [-763.266 -763.266 -763.266], eps: 0.001})
Step:  182500, Reward: [-396.314 -396.314 -396.314] [84.654], Avg: [-399.525 -399.525 -399.525] (0.0010) ({r_i: None, r_t: [-716.716 -716.716 -716.716], eps: 0.001})
Step:  182600, Reward: [-399.620 -399.620 -399.620] [64.626], Avg: [-399.525 -399.525 -399.525] (0.0010) ({r_i: None, r_t: [-753.947 -753.947 -753.947], eps: 0.001})
Step:  182700, Reward: [-370.732 -370.732 -370.732] [55.334], Avg: [-399.509 -399.509 -399.509] (0.0010) ({r_i: None, r_t: [-775.783 -775.783 -775.783], eps: 0.001})
Step:  182800, Reward: [-394.639 -394.639 -394.639] [66.754], Avg: [-399.507 -399.507 -399.507] (0.0010) ({r_i: None, r_t: [-802.063 -802.063 -802.063], eps: 0.001})
Step:  182900, Reward: [-359.515 -359.515 -359.515] [56.418], Avg: [-399.485 -399.485 -399.485] (0.0010) ({r_i: None, r_t: [-804.594 -804.594 -804.594], eps: 0.001})
Step:  183000, Reward: [-370.971 -370.971 -370.971] [48.920], Avg: [-399.469 -399.469 -399.469] (0.0010) ({r_i: None, r_t: [-822.006 -822.006 -822.006], eps: 0.001})
Step:  183100, Reward: [-363.336 -363.336 -363.336] [61.905], Avg: [-399.450 -399.450 -399.450] (0.0010) ({r_i: None, r_t: [-747.297 -747.297 -747.297], eps: 0.001})
Step:  183200, Reward: [-368.706 -368.706 -368.706] [69.341], Avg: [-399.433 -399.433 -399.433] (0.0010) ({r_i: None, r_t: [-748.371 -748.371 -748.371], eps: 0.001})
Step:  183300, Reward: [-376.578 -376.578 -376.578] [64.317], Avg: [-399.420 -399.420 -399.420] (0.0010) ({r_i: None, r_t: [-742.408 -742.408 -742.408], eps: 0.001})
Step:  183400, Reward: [-379.700 -379.700 -379.700] [68.361], Avg: [-399.410 -399.410 -399.410] (0.0010) ({r_i: None, r_t: [-738.026 -738.026 -738.026], eps: 0.001})
Step:  183500, Reward: [-363.316 -363.316 -363.316] [79.077], Avg: [-399.390 -399.390 -399.390] (0.0010) ({r_i: None, r_t: [-734.094 -734.094 -734.094], eps: 0.001})
Step:  183600, Reward: [-394.933 -394.933 -394.933] [57.419], Avg: [-399.387 -399.387 -399.387] (0.0010) ({r_i: None, r_t: [-795.300 -795.300 -795.300], eps: 0.001})
Step:  183700, Reward: [-380.593 -380.593 -380.593] [60.637], Avg: [-399.377 -399.377 -399.377] (0.0010) ({r_i: None, r_t: [-734.425 -734.425 -734.425], eps: 0.001})
Step:  183800, Reward: [-388.631 -388.631 -388.631] [57.036], Avg: [-399.371 -399.371 -399.371] (0.0010) ({r_i: None, r_t: [-797.480 -797.480 -797.480], eps: 0.001})
Step:  183900, Reward: [-355.782 -355.782 -355.782] [63.401], Avg: [-399.348 -399.348 -399.348] (0.0010) ({r_i: None, r_t: [-719.616 -719.616 -719.616], eps: 0.001})
Step:  184000, Reward: [-349.803 -349.803 -349.803] [60.653], Avg: [-399.321 -399.321 -399.321] (0.0010) ({r_i: None, r_t: [-739.207 -739.207 -739.207], eps: 0.001})
Step:  184100, Reward: [-405.748 -405.748 -405.748] [53.672], Avg: [-399.324 -399.324 -399.324] (0.0010) ({r_i: None, r_t: [-768.557 -768.557 -768.557], eps: 0.001})
Step:  184200, Reward: [-393.966 -393.966 -393.966] [73.060], Avg: [-399.321 -399.321 -399.321] (0.0010) ({r_i: None, r_t: [-708.439 -708.439 -708.439], eps: 0.001})
Step:  184300, Reward: [-375.906 -375.906 -375.906] [54.246], Avg: [-399.309 -399.309 -399.309] (0.0010) ({r_i: None, r_t: [-745.208 -745.208 -745.208], eps: 0.001})
Step:  184400, Reward: [-367.259 -367.259 -367.259] [61.781], Avg: [-399.291 -399.291 -399.291] (0.0010) ({r_i: None, r_t: [-753.671 -753.671 -753.671], eps: 0.001})
Step:  184500, Reward: [-385.037 -385.037 -385.037] [80.839], Avg: [-399.284 -399.284 -399.284] (0.0010) ({r_i: None, r_t: [-751.698 -751.698 -751.698], eps: 0.001})
Step:  184600, Reward: [-399.795 -399.795 -399.795] [78.081], Avg: [-399.284 -399.284 -399.284] (0.0010) ({r_i: None, r_t: [-727.407 -727.407 -727.407], eps: 0.001})
Step:  184700, Reward: [-371.163 -371.163 -371.163] [65.695], Avg: [-399.269 -399.269 -399.269] (0.0010) ({r_i: None, r_t: [-758.542 -758.542 -758.542], eps: 0.001})
Step:  184800, Reward: [-401.490 -401.490 -401.490] [67.168], Avg: [-399.270 -399.270 -399.270] (0.0010) ({r_i: None, r_t: [-771.647 -771.647 -771.647], eps: 0.001})
Step:  184900, Reward: [-362.210 -362.210 -362.210] [69.132], Avg: [-399.250 -399.250 -399.250] (0.0010) ({r_i: None, r_t: [-764.745 -764.745 -764.745], eps: 0.001})
Step:  185000, Reward: [-410.376 -410.376 -410.376] [69.983], Avg: [-399.256 -399.256 -399.256] (0.0010) ({r_i: None, r_t: [-829.202 -829.202 -829.202], eps: 0.001})
Step:  185100, Reward: [-360.106 -360.106 -360.106] [68.203], Avg: [-399.235 -399.235 -399.235] (0.0010) ({r_i: None, r_t: [-809.691 -809.691 -809.691], eps: 0.001})
Step:  185200, Reward: [-385.185 -385.185 -385.185] [70.613], Avg: [-399.227 -399.227 -399.227] (0.0010) ({r_i: None, r_t: [-773.382 -773.382 -773.382], eps: 0.001})
Step:  185300, Reward: [-396.550 -396.550 -396.550] [63.730], Avg: [-399.226 -399.226 -399.226] (0.0010) ({r_i: None, r_t: [-750.114 -750.114 -750.114], eps: 0.001})
Step:  185400, Reward: [-381.338 -381.338 -381.338] [53.778], Avg: [-399.216 -399.216 -399.216] (0.0010) ({r_i: None, r_t: [-766.984 -766.984 -766.984], eps: 0.001})
Step:  185500, Reward: [-403.697 -403.697 -403.697] [81.354], Avg: [-399.218 -399.218 -399.218] (0.0010) ({r_i: None, r_t: [-760.326 -760.326 -760.326], eps: 0.001})
Step:  185600, Reward: [-388.547 -388.547 -388.547] [87.586], Avg: [-399.213 -399.213 -399.213] (0.0010) ({r_i: None, r_t: [-785.503 -785.503 -785.503], eps: 0.001})
Step:  185700, Reward: [-385.506 -385.506 -385.506] [65.069], Avg: [-399.205 -399.205 -399.205] (0.0010) ({r_i: None, r_t: [-764.733 -764.733 -764.733], eps: 0.001})
Step:  185800, Reward: [-371.755 -371.755 -371.755] [70.321], Avg: [-399.191 -399.191 -399.191] (0.0010) ({r_i: None, r_t: [-775.109 -775.109 -775.109], eps: 0.001})
Step:  185900, Reward: [-345.545 -345.545 -345.545] [67.678], Avg: [-399.162 -399.162 -399.162] (0.0010) ({r_i: None, r_t: [-794.006 -794.006 -794.006], eps: 0.001})
Step:  186000, Reward: [-380.278 -380.278 -380.278] [78.906], Avg: [-399.152 -399.152 -399.152] (0.0010) ({r_i: None, r_t: [-703.670 -703.670 -703.670], eps: 0.001})
Step:  186100, Reward: [-390.799 -390.799 -390.799] [64.838], Avg: [-399.147 -399.147 -399.147] (0.0010) ({r_i: None, r_t: [-735.812 -735.812 -735.812], eps: 0.001})
Step:  186200, Reward: [-409.323 -409.323 -409.323] [75.607], Avg: [-399.153 -399.153 -399.153] (0.0010) ({r_i: None, r_t: [-767.127 -767.127 -767.127], eps: 0.001})
Step:  186300, Reward: [-389.071 -389.071 -389.071] [67.857], Avg: [-399.147 -399.147 -399.147] (0.0010) ({r_i: None, r_t: [-780.491 -780.491 -780.491], eps: 0.001})
Step:  186400, Reward: [-415.108 -415.108 -415.108] [78.893], Avg: [-399.156 -399.156 -399.156] (0.0010) ({r_i: None, r_t: [-766.111 -766.111 -766.111], eps: 0.001})
Step:  186500, Reward: [-371.198 -371.198 -371.198] [69.201], Avg: [-399.141 -399.141 -399.141] (0.0010) ({r_i: None, r_t: [-758.180 -758.180 -758.180], eps: 0.001})
Step:  186600, Reward: [-381.655 -381.655 -381.655] [67.098], Avg: [-399.131 -399.131 -399.131] (0.0010) ({r_i: None, r_t: [-839.016 -839.016 -839.016], eps: 0.001})
Step:  186700, Reward: [-393.745 -393.745 -393.745] [64.831], Avg: [-399.128 -399.128 -399.128] (0.0010) ({r_i: None, r_t: [-740.759 -740.759 -740.759], eps: 0.001})
Step:  186800, Reward: [-379.177 -379.177 -379.177] [52.020], Avg: [-399.118 -399.118 -399.118] (0.0010) ({r_i: None, r_t: [-775.320 -775.320 -775.320], eps: 0.001})
Step:  186900, Reward: [-359.581 -359.581 -359.581] [67.583], Avg: [-399.097 -399.097 -399.097] (0.0010) ({r_i: None, r_t: [-789.474 -789.474 -789.474], eps: 0.001})
Step:  187000, Reward: [-346.084 -346.084 -346.084] [52.880], Avg: [-399.068 -399.068 -399.068] (0.0010) ({r_i: None, r_t: [-763.433 -763.433 -763.433], eps: 0.001})
Step:  187100, Reward: [-386.798 -386.798 -386.798] [93.147], Avg: [-399.062 -399.062 -399.062] (0.0010) ({r_i: None, r_t: [-696.668 -696.668 -696.668], eps: 0.001})
Step:  187200, Reward: [-419.383 -419.383 -419.383] [84.413], Avg: [-399.073 -399.073 -399.073] (0.0010) ({r_i: None, r_t: [-776.051 -776.051 -776.051], eps: 0.001})
Step:  187300, Reward: [-403.937 -403.937 -403.937] [82.059], Avg: [-399.075 -399.075 -399.075] (0.0010) ({r_i: None, r_t: [-768.201 -768.201 -768.201], eps: 0.001})
Step:  187400, Reward: [-368.939 -368.939 -368.939] [52.123], Avg: [-399.059 -399.059 -399.059] (0.0010) ({r_i: None, r_t: [-781.561 -781.561 -781.561], eps: 0.001})
Step:  187500, Reward: [-403.425 -403.425 -403.425] [66.066], Avg: [-399.061 -399.061 -399.061] (0.0010) ({r_i: None, r_t: [-788.784 -788.784 -788.784], eps: 0.001})
Step:  187600, Reward: [-375.440 -375.440 -375.440] [66.168], Avg: [-399.049 -399.049 -399.049] (0.0010) ({r_i: None, r_t: [-756.977 -756.977 -756.977], eps: 0.001})
Step:  187700, Reward: [-411.992 -411.992 -411.992] [52.736], Avg: [-399.056 -399.056 -399.056] (0.0010) ({r_i: None, r_t: [-773.241 -773.241 -773.241], eps: 0.001})
Step:  187800, Reward: [-367.630 -367.630 -367.630] [55.433], Avg: [-399.039 -399.039 -399.039] (0.0010) ({r_i: None, r_t: [-751.380 -751.380 -751.380], eps: 0.001})
Step:  187900, Reward: [-388.777 -388.777 -388.777] [78.958], Avg: [-399.034 -399.034 -399.034] (0.0010) ({r_i: None, r_t: [-732.408 -732.408 -732.408], eps: 0.001})
Step:  188000, Reward: [-397.939 -397.939 -397.939] [61.563], Avg: [-399.033 -399.033 -399.033] (0.0010) ({r_i: None, r_t: [-744.136 -744.136 -744.136], eps: 0.001})
Step:  188100, Reward: [-350.574 -350.574 -350.574] [59.517], Avg: [-399.007 -399.007 -399.007] (0.0010) ({r_i: None, r_t: [-810.192 -810.192 -810.192], eps: 0.001})
Step:  188200, Reward: [-392.297 -392.297 -392.297] [51.421], Avg: [-399.004 -399.004 -399.004] (0.0010) ({r_i: None, r_t: [-835.249 -835.249 -835.249], eps: 0.001})
Step:  188300, Reward: [-394.240 -394.240 -394.240] [87.635], Avg: [-399.001 -399.001 -399.001] (0.0010) ({r_i: None, r_t: [-765.460 -765.460 -765.460], eps: 0.001})
Step:  188400, Reward: [-384.021 -384.021 -384.021] [57.285], Avg: [-398.993 -398.993 -398.993] (0.0010) ({r_i: None, r_t: [-776.164 -776.164 -776.164], eps: 0.001})
Step:  188500, Reward: [-392.713 -392.713 -392.713] [69.478], Avg: [-398.990 -398.990 -398.990] (0.0010) ({r_i: None, r_t: [-792.972 -792.972 -792.972], eps: 0.001})
Step:  188600, Reward: [-373.437 -373.437 -373.437] [58.672], Avg: [-398.976 -398.976 -398.976] (0.0010) ({r_i: None, r_t: [-801.406 -801.406 -801.406], eps: 0.001})
Step:  188700, Reward: [-400.699 -400.699 -400.699] [65.633], Avg: [-398.977 -398.977 -398.977] (0.0010) ({r_i: None, r_t: [-786.765 -786.765 -786.765], eps: 0.001})
Step:  188800, Reward: [-349.750 -349.750 -349.750] [47.853], Avg: [-398.951 -398.951 -398.951] (0.0010) ({r_i: None, r_t: [-790.032 -790.032 -790.032], eps: 0.001})
Step:  188900, Reward: [-397.719 -397.719 -397.719] [83.516], Avg: [-398.951 -398.951 -398.951] (0.0010) ({r_i: None, r_t: [-785.776 -785.776 -785.776], eps: 0.001})
Step:  189000, Reward: [-383.702 -383.702 -383.702] [65.064], Avg: [-398.942 -398.942 -398.942] (0.0010) ({r_i: None, r_t: [-771.451 -771.451 -771.451], eps: 0.001})
Step:  189100, Reward: [-364.138 -364.138 -364.138] [64.822], Avg: [-398.924 -398.924 -398.924] (0.0010) ({r_i: None, r_t: [-767.257 -767.257 -767.257], eps: 0.001})
Step:  189200, Reward: [-373.907 -373.907 -373.907] [80.667], Avg: [-398.911 -398.911 -398.911] (0.0010) ({r_i: None, r_t: [-781.161 -781.161 -781.161], eps: 0.001})
Step:  189300, Reward: [-411.147 -411.147 -411.147] [77.230], Avg: [-398.917 -398.917 -398.917] (0.0010) ({r_i: None, r_t: [-713.499 -713.499 -713.499], eps: 0.001})
Step:  189400, Reward: [-420.680 -420.680 -420.680] [72.166], Avg: [-398.929 -398.929 -398.929] (0.0010) ({r_i: None, r_t: [-744.440 -744.440 -744.440], eps: 0.001})
Step:  189500, Reward: [-374.330 -374.330 -374.330] [55.652], Avg: [-398.916 -398.916 -398.916] (0.0010) ({r_i: None, r_t: [-756.612 -756.612 -756.612], eps: 0.001})
Step:  189600, Reward: [-378.266 -378.266 -378.266] [64.630], Avg: [-398.905 -398.905 -398.905] (0.0010) ({r_i: None, r_t: [-746.099 -746.099 -746.099], eps: 0.001})
Step:  189700, Reward: [-391.308 -391.308 -391.308] [71.293], Avg: [-398.901 -398.901 -398.901] (0.0010) ({r_i: None, r_t: [-722.756 -722.756 -722.756], eps: 0.001})
Step:  189800, Reward: [-411.613 -411.613 -411.613] [67.425], Avg: [-398.908 -398.908 -398.908] (0.0010) ({r_i: None, r_t: [-763.368 -763.368 -763.368], eps: 0.001})
Step:  189900, Reward: [-372.787 -372.787 -372.787] [77.108], Avg: [-398.894 -398.894 -398.894] (0.0010) ({r_i: None, r_t: [-752.320 -752.320 -752.320], eps: 0.001})
Step:  190000, Reward: [-409.151 -409.151 -409.151] [85.243], Avg: [-398.899 -398.899 -398.899] (0.0010) ({r_i: None, r_t: [-808.602 -808.602 -808.602], eps: 0.001})
Step:  190100, Reward: [-377.705 -377.705 -377.705] [57.252], Avg: [-398.888 -398.888 -398.888] (0.0010) ({r_i: None, r_t: [-827.637 -827.637 -827.637], eps: 0.001})
Step:  190200, Reward: [-367.730 -367.730 -367.730] [103.735], Avg: [-398.872 -398.872 -398.872] (0.0010) ({r_i: None, r_t: [-722.859 -722.859 -722.859], eps: 0.001})
Step:  190300, Reward: [-380.603 -380.603 -380.603] [71.350], Avg: [-398.862 -398.862 -398.862] (0.0010) ({r_i: None, r_t: [-797.345 -797.345 -797.345], eps: 0.001})
Step:  190400, Reward: [-357.382 -357.382 -357.382] [64.165], Avg: [-398.840 -398.840 -398.840] (0.0010) ({r_i: None, r_t: [-740.742 -740.742 -740.742], eps: 0.001})
Step:  190500, Reward: [-388.537 -388.537 -388.537] [75.736], Avg: [-398.835 -398.835 -398.835] (0.0010) ({r_i: None, r_t: [-752.772 -752.772 -752.772], eps: 0.001})
Step:  190600, Reward: [-367.299 -367.299 -367.299] [84.399], Avg: [-398.818 -398.818 -398.818] (0.0010) ({r_i: None, r_t: [-730.800 -730.800 -730.800], eps: 0.001})
Step:  190700, Reward: [-394.319 -394.319 -394.319] [79.267], Avg: [-398.816 -398.816 -398.816] (0.0010) ({r_i: None, r_t: [-728.410 -728.410 -728.410], eps: 0.001})
Step:  190800, Reward: [-414.871 -414.871 -414.871] [82.935], Avg: [-398.824 -398.824 -398.824] (0.0010) ({r_i: None, r_t: [-771.707 -771.707 -771.707], eps: 0.001})
Step:  190900, Reward: [-382.992 -382.992 -382.992] [72.535], Avg: [-398.816 -398.816 -398.816] (0.0010) ({r_i: None, r_t: [-766.484 -766.484 -766.484], eps: 0.001})
Step:  191000, Reward: [-368.186 -368.186 -368.186] [65.223], Avg: [-398.800 -398.800 -398.800] (0.0010) ({r_i: None, r_t: [-752.105 -752.105 -752.105], eps: 0.001})
Step:  191100, Reward: [-372.637 -372.637 -372.637] [54.769], Avg: [-398.786 -398.786 -398.786] (0.0010) ({r_i: None, r_t: [-764.440 -764.440 -764.440], eps: 0.001})
Step:  191200, Reward: [-409.526 -409.526 -409.526] [79.665], Avg: [-398.792 -398.792 -398.792] (0.0010) ({r_i: None, r_t: [-725.042 -725.042 -725.042], eps: 0.001})
Step:  191300, Reward: [-361.196 -361.196 -361.196] [50.909], Avg: [-398.772 -398.772 -398.772] (0.0010) ({r_i: None, r_t: [-812.473 -812.473 -812.473], eps: 0.001})
Step:  191400, Reward: [-393.615 -393.615 -393.615] [64.629], Avg: [-398.770 -398.770 -398.770] (0.0010) ({r_i: None, r_t: [-756.731 -756.731 -756.731], eps: 0.001})
Step:  191500, Reward: [-371.129 -371.129 -371.129] [73.145], Avg: [-398.755 -398.755 -398.755] (0.0010) ({r_i: None, r_t: [-705.809 -705.809 -705.809], eps: 0.001})
Step:  191600, Reward: [-373.363 -373.363 -373.363] [75.235], Avg: [-398.742 -398.742 -398.742] (0.0010) ({r_i: None, r_t: [-745.916 -745.916 -745.916], eps: 0.001})
Step:  191700, Reward: [-398.226 -398.226 -398.226] [75.686], Avg: [-398.742 -398.742 -398.742] (0.0010) ({r_i: None, r_t: [-743.916 -743.916 -743.916], eps: 0.001})
Step:  191800, Reward: [-364.606 -364.606 -364.606] [52.427], Avg: [-398.724 -398.724 -398.724] (0.0010) ({r_i: None, r_t: [-737.357 -737.357 -737.357], eps: 0.001})
Step:  191900, Reward: [-340.634 -340.634 -340.634] [65.884], Avg: [-398.694 -398.694 -398.694] (0.0010) ({r_i: None, r_t: [-733.888 -733.888 -733.888], eps: 0.001})
Step:  192000, Reward: [-377.486 -377.486 -377.486] [69.353], Avg: [-398.683 -398.683 -398.683] (0.0010) ({r_i: None, r_t: [-787.098 -787.098 -787.098], eps: 0.001})
Step:  192100, Reward: [-357.903 -357.903 -357.903] [60.829], Avg: [-398.662 -398.662 -398.662] (0.0010) ({r_i: None, r_t: [-764.290 -764.290 -764.290], eps: 0.001})
Step:  192200, Reward: [-380.711 -380.711 -380.711] [57.823], Avg: [-398.652 -398.652 -398.652] (0.0010) ({r_i: None, r_t: [-758.022 -758.022 -758.022], eps: 0.001})
Step:  192300, Reward: [-372.514 -372.514 -372.514] [63.678], Avg: [-398.639 -398.639 -398.639] (0.0010) ({r_i: None, r_t: [-791.826 -791.826 -791.826], eps: 0.001})
Step:  192400, Reward: [-412.688 -412.688 -412.688] [76.345], Avg: [-398.646 -398.646 -398.646] (0.0010) ({r_i: None, r_t: [-765.172 -765.172 -765.172], eps: 0.001})
Step:  192500, Reward: [-387.416 -387.416 -387.416] [63.443], Avg: [-398.640 -398.640 -398.640] (0.0010) ({r_i: None, r_t: [-779.677 -779.677 -779.677], eps: 0.001})
Step:  192600, Reward: [-424.633 -424.633 -424.633] [64.704], Avg: [-398.654 -398.654 -398.654] (0.0010) ({r_i: None, r_t: [-743.381 -743.381 -743.381], eps: 0.001})
Step:  192700, Reward: [-417.555 -417.555 -417.555] [63.158], Avg: [-398.663 -398.663 -398.663] (0.0010) ({r_i: None, r_t: [-773.023 -773.023 -773.023], eps: 0.001})
Step:  192800, Reward: [-401.236 -401.236 -401.236] [83.567], Avg: [-398.665 -398.665 -398.665] (0.0010) ({r_i: None, r_t: [-788.369 -788.369 -788.369], eps: 0.001})
Step:  192900, Reward: [-389.856 -389.856 -389.856] [63.571], Avg: [-398.660 -398.660 -398.660] (0.0010) ({r_i: None, r_t: [-775.304 -775.304 -775.304], eps: 0.001})
Step:  193000, Reward: [-390.361 -390.361 -390.361] [82.608], Avg: [-398.656 -398.656 -398.656] (0.0010) ({r_i: None, r_t: [-769.422 -769.422 -769.422], eps: 0.001})
Step:  193100, Reward: [-394.403 -394.403 -394.403] [89.983], Avg: [-398.654 -398.654 -398.654] (0.0010) ({r_i: None, r_t: [-763.898 -763.898 -763.898], eps: 0.001})
Step:  193200, Reward: [-422.000 -422.000 -422.000] [78.089], Avg: [-398.666 -398.666 -398.666] (0.0010) ({r_i: None, r_t: [-728.348 -728.348 -728.348], eps: 0.001})
Step:  193300, Reward: [-355.452 -355.452 -355.452] [39.088], Avg: [-398.643 -398.643 -398.643] (0.0010) ({r_i: None, r_t: [-759.076 -759.076 -759.076], eps: 0.001})
Step:  193400, Reward: [-365.646 -365.646 -365.646] [66.680], Avg: [-398.626 -398.626 -398.626] (0.0010) ({r_i: None, r_t: [-807.050 -807.050 -807.050], eps: 0.001})
Step:  193500, Reward: [-387.285 -387.285 -387.285] [57.064], Avg: [-398.620 -398.620 -398.620] (0.0010) ({r_i: None, r_t: [-796.503 -796.503 -796.503], eps: 0.001})
Step:  193600, Reward: [-372.841 -372.841 -372.841] [72.955], Avg: [-398.607 -398.607 -398.607] (0.0010) ({r_i: None, r_t: [-763.231 -763.231 -763.231], eps: 0.001})
Step:  193700, Reward: [-381.202 -381.202 -381.202] [103.069], Avg: [-398.598 -398.598 -398.598] (0.0010) ({r_i: None, r_t: [-768.722 -768.722 -768.722], eps: 0.001})
Step:  193800, Reward: [-353.154 -353.154 -353.154] [59.423], Avg: [-398.575 -398.575 -398.575] (0.0010) ({r_i: None, r_t: [-764.013 -764.013 -764.013], eps: 0.001})
Step:  193900, Reward: [-380.514 -380.514 -380.514] [77.322], Avg: [-398.565 -398.565 -398.565] (0.0010) ({r_i: None, r_t: [-793.433 -793.433 -793.433], eps: 0.001})
Step:  194000, Reward: [-364.936 -364.936 -364.936] [75.417], Avg: [-398.548 -398.548 -398.548] (0.0010) ({r_i: None, r_t: [-718.308 -718.308 -718.308], eps: 0.001})
Step:  194100, Reward: [-350.332 -350.332 -350.332] [56.225], Avg: [-398.523 -398.523 -398.523] (0.0010) ({r_i: None, r_t: [-759.072 -759.072 -759.072], eps: 0.001})
Step:  194200, Reward: [-385.433 -385.433 -385.433] [59.178], Avg: [-398.517 -398.517 -398.517] (0.0010) ({r_i: None, r_t: [-779.791 -779.791 -779.791], eps: 0.001})
Step:  194300, Reward: [-402.456 -402.456 -402.456] [56.192], Avg: [-398.519 -398.519 -398.519] (0.0010) ({r_i: None, r_t: [-759.257 -759.257 -759.257], eps: 0.001})
Step:  194400, Reward: [-361.001 -361.001 -361.001] [42.503], Avg: [-398.499 -398.499 -398.499] (0.0010) ({r_i: None, r_t: [-723.444 -723.444 -723.444], eps: 0.001})
Step:  194500, Reward: [-370.883 -370.883 -370.883] [68.850], Avg: [-398.485 -398.485 -398.485] (0.0010) ({r_i: None, r_t: [-795.307 -795.307 -795.307], eps: 0.001})
Step:  194600, Reward: [-409.258 -409.258 -409.258] [74.592], Avg: [-398.491 -398.491 -398.491] (0.0010) ({r_i: None, r_t: [-778.915 -778.915 -778.915], eps: 0.001})
Step:  194700, Reward: [-365.787 -365.787 -365.787] [75.089], Avg: [-398.474 -398.474 -398.474] (0.0010) ({r_i: None, r_t: [-834.058 -834.058 -834.058], eps: 0.001})
Step:  194800, Reward: [-399.517 -399.517 -399.517] [68.093], Avg: [-398.474 -398.474 -398.474] (0.0010) ({r_i: None, r_t: [-754.037 -754.037 -754.037], eps: 0.001})
Step:  194900, Reward: [-376.345 -376.345 -376.345] [82.568], Avg: [-398.463 -398.463 -398.463] (0.0010) ({r_i: None, r_t: [-739.327 -739.327 -739.327], eps: 0.001})
Step:  195000, Reward: [-367.246 -367.246 -367.246] [55.758], Avg: [-398.447 -398.447 -398.447] (0.0010) ({r_i: None, r_t: [-693.624 -693.624 -693.624], eps: 0.001})
Step:  195100, Reward: [-360.807 -360.807 -360.807] [50.012], Avg: [-398.428 -398.428 -398.428] (0.0010) ({r_i: None, r_t: [-717.807 -717.807 -717.807], eps: 0.001})
Step:  195200, Reward: [-369.555 -369.555 -369.555] [72.315], Avg: [-398.413 -398.413 -398.413] (0.0010) ({r_i: None, r_t: [-769.937 -769.937 -769.937], eps: 0.001})
Step:  195300, Reward: [-399.057 -399.057 -399.057] [56.578], Avg: [-398.413 -398.413 -398.413] (0.0010) ({r_i: None, r_t: [-818.247 -818.247 -818.247], eps: 0.001})
Step:  195400, Reward: [-382.732 -382.732 -382.732] [50.390], Avg: [-398.405 -398.405 -398.405] (0.0010) ({r_i: None, r_t: [-747.286 -747.286 -747.286], eps: 0.001})
Step:  195500, Reward: [-361.799 -361.799 -361.799] [74.335], Avg: [-398.387 -398.387 -398.387] (0.0010) ({r_i: None, r_t: [-736.000 -736.000 -736.000], eps: 0.001})
Step:  195600, Reward: [-397.634 -397.634 -397.634] [90.573], Avg: [-398.386 -398.386 -398.386] (0.0010) ({r_i: None, r_t: [-760.425 -760.425 -760.425], eps: 0.001})
Step:  195700, Reward: [-361.758 -361.758 -361.758] [59.032], Avg: [-398.367 -398.367 -398.367] (0.0010) ({r_i: None, r_t: [-749.149 -749.149 -749.149], eps: 0.001})
Step:  195800, Reward: [-362.617 -362.617 -362.617] [70.143], Avg: [-398.349 -398.349 -398.349] (0.0010) ({r_i: None, r_t: [-788.835 -788.835 -788.835], eps: 0.001})
Step:  195900, Reward: [-383.303 -383.303 -383.303] [59.803], Avg: [-398.342 -398.342 -398.342] (0.0010) ({r_i: None, r_t: [-769.343 -769.343 -769.343], eps: 0.001})
Step:  196000, Reward: [-362.802 -362.802 -362.802] [69.564], Avg: [-398.323 -398.323 -398.323] (0.0010) ({r_i: None, r_t: [-735.476 -735.476 -735.476], eps: 0.001})
Step:  196100, Reward: [-384.552 -384.552 -384.552] [80.338], Avg: [-398.316 -398.316 -398.316] (0.0010) ({r_i: None, r_t: [-789.835 -789.835 -789.835], eps: 0.001})
Step:  196200, Reward: [-396.230 -396.230 -396.230] [75.618], Avg: [-398.315 -398.315 -398.315] (0.0010) ({r_i: None, r_t: [-737.031 -737.031 -737.031], eps: 0.001})
Step:  196300, Reward: [-374.750 -374.750 -374.750] [84.290], Avg: [-398.303 -398.303 -398.303] (0.0010) ({r_i: None, r_t: [-755.524 -755.524 -755.524], eps: 0.001})
Step:  196400, Reward: [-349.741 -349.741 -349.741] [56.159], Avg: [-398.279 -398.279 -398.279] (0.0010) ({r_i: None, r_t: [-745.956 -745.956 -745.956], eps: 0.001})
Step:  196500, Reward: [-386.084 -386.084 -386.084] [72.555], Avg: [-398.272 -398.272 -398.272] (0.0010) ({r_i: None, r_t: [-790.667 -790.667 -790.667], eps: 0.001})
Step:  196600, Reward: [-413.155 -413.155 -413.155] [85.826], Avg: [-398.280 -398.280 -398.280] (0.0010) ({r_i: None, r_t: [-793.877 -793.877 -793.877], eps: 0.001})
Step:  196700, Reward: [-374.772 -374.772 -374.772] [66.406], Avg: [-398.268 -398.268 -398.268] (0.0010) ({r_i: None, r_t: [-792.137 -792.137 -792.137], eps: 0.001})
Step:  196800, Reward: [-410.592 -410.592 -410.592] [63.259], Avg: [-398.274 -398.274 -398.274] (0.0010) ({r_i: None, r_t: [-772.654 -772.654 -772.654], eps: 0.001})
Step:  196900, Reward: [-381.199 -381.199 -381.199] [79.178], Avg: [-398.266 -398.266 -398.266] (0.0010) ({r_i: None, r_t: [-814.225 -814.225 -814.225], eps: 0.001})
Step:  197000, Reward: [-404.244 -404.244 -404.244] [77.350], Avg: [-398.269 -398.269 -398.269] (0.0010) ({r_i: None, r_t: [-763.885 -763.885 -763.885], eps: 0.001})
Step:  197100, Reward: [-380.946 -380.946 -380.946] [43.298], Avg: [-398.260 -398.260 -398.260] (0.0010) ({r_i: None, r_t: [-743.028 -743.028 -743.028], eps: 0.001})
Step:  197200, Reward: [-382.976 -382.976 -382.976] [50.355], Avg: [-398.252 -398.252 -398.252] (0.0010) ({r_i: None, r_t: [-752.477 -752.477 -752.477], eps: 0.001})
Step:  197300, Reward: [-397.454 -397.454 -397.454] [59.128], Avg: [-398.252 -398.252 -398.252] (0.0010) ({r_i: None, r_t: [-765.048 -765.048 -765.048], eps: 0.001})
Step:  197400, Reward: [-389.734 -389.734 -389.734] [60.529], Avg: [-398.247 -398.247 -398.247] (0.0010) ({r_i: None, r_t: [-747.113 -747.113 -747.113], eps: 0.001})
Step:  197500, Reward: [-376.573 -376.573 -376.573] [46.892], Avg: [-398.236 -398.236 -398.236] (0.0010) ({r_i: None, r_t: [-810.773 -810.773 -810.773], eps: 0.001})
Step:  197600, Reward: [-359.801 -359.801 -359.801] [81.364], Avg: [-398.217 -398.217 -398.217] (0.0010) ({r_i: None, r_t: [-790.749 -790.749 -790.749], eps: 0.001})
Step:  197700, Reward: [-374.787 -374.787 -374.787] [74.544], Avg: [-398.205 -398.205 -398.205] (0.0010) ({r_i: None, r_t: [-766.966 -766.966 -766.966], eps: 0.001})
Step:  197800, Reward: [-365.667 -365.667 -365.667] [88.596], Avg: [-398.189 -398.189 -398.189] (0.0010) ({r_i: None, r_t: [-790.590 -790.590 -790.590], eps: 0.001})
Step:  197900, Reward: [-388.080 -388.080 -388.080] [76.047], Avg: [-398.184 -398.184 -398.184] (0.0010) ({r_i: None, r_t: [-797.884 -797.884 -797.884], eps: 0.001})
Step:  198000, Reward: [-418.223 -418.223 -418.223] [81.637], Avg: [-398.194 -398.194 -398.194] (0.0010) ({r_i: None, r_t: [-761.310 -761.310 -761.310], eps: 0.001})
Step:  198100, Reward: [-411.515 -411.515 -411.515] [65.146], Avg: [-398.200 -398.200 -398.200] (0.0010) ({r_i: None, r_t: [-766.432 -766.432 -766.432], eps: 0.001})
Step:  198200, Reward: [-422.677 -422.677 -422.677] [75.572], Avg: [-398.213 -398.213 -398.213] (0.0010) ({r_i: None, r_t: [-786.992 -786.992 -786.992], eps: 0.001})
Step:  198300, Reward: [-380.312 -380.312 -380.312] [60.789], Avg: [-398.204 -398.204 -398.204] (0.0010) ({r_i: None, r_t: [-803.388 -803.388 -803.388], eps: 0.001})
Step:  198400, Reward: [-371.746 -371.746 -371.746] [95.029], Avg: [-398.190 -398.190 -398.190] (0.0010) ({r_i: None, r_t: [-718.345 -718.345 -718.345], eps: 0.001})
Step:  198500, Reward: [-423.980 -423.980 -423.980] [73.368], Avg: [-398.203 -398.203 -398.203] (0.0010) ({r_i: None, r_t: [-813.078 -813.078 -813.078], eps: 0.001})
Step:  198600, Reward: [-379.835 -379.835 -379.835] [63.605], Avg: [-398.194 -398.194 -398.194] (0.0010) ({r_i: None, r_t: [-771.832 -771.832 -771.832], eps: 0.001})
Step:  198700, Reward: [-405.672 -405.672 -405.672] [62.677], Avg: [-398.198 -398.198 -398.198] (0.0010) ({r_i: None, r_t: [-789.958 -789.958 -789.958], eps: 0.001})
Step:  198800, Reward: [-402.860 -402.860 -402.860] [71.705], Avg: [-398.200 -398.200 -398.200] (0.0010) ({r_i: None, r_t: [-819.286 -819.286 -819.286], eps: 0.001})
Step:  198900, Reward: [-376.077 -376.077 -376.077] [53.664], Avg: [-398.189 -398.189 -398.189] (0.0010) ({r_i: None, r_t: [-789.609 -789.609 -789.609], eps: 0.001})
Step:  199000, Reward: [-377.350 -377.350 -377.350] [74.071], Avg: [-398.179 -398.179 -398.179] (0.0010) ({r_i: None, r_t: [-800.409 -800.409 -800.409], eps: 0.001})
Step:  199100, Reward: [-388.817 -388.817 -388.817] [45.904], Avg: [-398.174 -398.174 -398.174] (0.0010) ({r_i: None, r_t: [-785.680 -785.680 -785.680], eps: 0.001})
Step:  199200, Reward: [-415.227 -415.227 -415.227] [83.053], Avg: [-398.183 -398.183 -398.183] (0.0010) ({r_i: None, r_t: [-802.549 -802.549 -802.549], eps: 0.001})
Step:  199300, Reward: [-384.444 -384.444 -384.444] [48.216], Avg: [-398.176 -398.176 -398.176] (0.0010) ({r_i: None, r_t: [-775.240 -775.240 -775.240], eps: 0.001})
Step:  199400, Reward: [-388.214 -388.214 -388.214] [73.303], Avg: [-398.171 -398.171 -398.171] (0.0010) ({r_i: None, r_t: [-757.002 -757.002 -757.002], eps: 0.001})
Step:  199500, Reward: [-409.843 -409.843 -409.843] [64.141], Avg: [-398.176 -398.176 -398.176] (0.0010) ({r_i: None, r_t: [-796.711 -796.711 -796.711], eps: 0.001})
Step:  199600, Reward: [-416.782 -416.782 -416.782] [83.114], Avg: [-398.186 -398.186 -398.186] (0.0010) ({r_i: None, r_t: [-787.857 -787.857 -787.857], eps: 0.001})
Step:  199700, Reward: [-411.927 -411.927 -411.927] [79.005], Avg: [-398.193 -398.193 -398.193] (0.0010) ({r_i: None, r_t: [-829.905 -829.905 -829.905], eps: 0.001})
Step:  199800, Reward: [-386.540 -386.540 -386.540] [66.640], Avg: [-398.187 -398.187 -398.187] (0.0010) ({r_i: None, r_t: [-811.279 -811.279 -811.279], eps: 0.001})
Step:  199900, Reward: [-385.339 -385.339 -385.339] [66.137], Avg: [-398.180 -398.180 -398.180] (0.0010) ({r_i: None, r_t: [-766.624 -766.624 -766.624], eps: 0.001})
Step:  200000, Reward: [-418.337 -418.337 -418.337] [67.550], Avg: [-398.191 -398.191 -398.191] (0.0010) ({r_i: None, r_t: [-764.263 -764.263 -764.263], eps: 0.001})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action = self.layer3(state)
		return action

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.last_target_update_step = 0
		self.critic_training_steps = 0

		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)

		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def select_actions(self, inputs, eps):
		agent_outputs = self.forward(inputs, eps)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def train(self, batch, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_t = self.critic_local(critic_inputs[:,t], t)
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			critic_loss = (td_error ** 2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters())
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * log_pi_taken).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		# self.network = COMANetwork(state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.network.select_actions(inputs, eps=self.eps)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.train(None, actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)



import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork2(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-447.646 -447.646 -447.646] [74.622], Avg: [-447.646 -447.646 -447.646] (1.0000) ({r_i: None, r_t: [-8.433 -8.433 -8.433], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class PTCNetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE, gpu=True, load="", name="ptac"): 
		super().__init__(tau, gpu, name)

	def save_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass
		
	def load_model(self, net="qlearning", dirname="pytorch", name="checkpoint"):
		pass

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action = self.layer3(state)
		return action

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.last_target_update_step = 0
		self.critic_training_steps = 0

		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)

		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def select_actions(self, inputs, eps):
		agent_outputs = self.forward(inputs, eps)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def train(self, batch, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_t = self.critic_local(critic_inputs[:,t])
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			critic_loss = (td_error ** 2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters())
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * log_pi_taken).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		# self.network = COMANetwork(state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.network.select_actions(inputs, eps=self.eps)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.train(None, actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)



import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

class COMANetwork2(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, use_target=False, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
			model.soft_copy(model.critic_local, model.critic_target)

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent2(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-484.715 -484.715 -484.715] [105.745], Avg: [-484.715 -484.715 -484.715] (1.0000) ({r_i: None, r_t: [-8.648 -8.648 -8.648], eps: 1.0})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action = self.layer3(state)
		return action

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.critic_training_steps = 0
		self.last_target_update_step = 0
		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)
		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def select_actions(self, inputs, eps):
		agent_outputs = self.forward(inputs, eps)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def train(self, batch, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_t = self.critic_local(critic_inputs[:,t])
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			critic_loss = (td_error ** 2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters())
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * log_pi_taken).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.network.select_actions(inputs, eps=self.eps)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.train(None, actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-483.626 -483.626 -483.626] [89.404], Avg: [-483.626 -483.626 -483.626] (1.0000) ({r_i: None, r_t: [-8.824 -8.824 -8.824], eps: 1.0})
Step:     100, Reward: [-517.944 -517.944 -517.944] [91.244], Avg: [-500.785 -500.785 -500.785] (0.9900) ({r_i: None, r_t: [-1007.412 -1007.412 -1007.412], eps: 0.99})
Step:     200, Reward: [-500.508 -500.508 -500.508] [113.541], Avg: [-500.692 -500.692 -500.692] (0.9801) ({r_i: None, r_t: [-1010.759 -1010.759 -1010.759], eps: 0.98})
Step:     300, Reward: [-453.249 -453.249 -453.249] [82.185], Avg: [-488.832 -488.832 -488.832] (0.9704) ({r_i: None, r_t: [-1018.567 -1018.567 -1018.567], eps: 0.97})
Step:     400, Reward: [-482.532 -482.532 -482.532] [120.666], Avg: [-487.572 -487.572 -487.572] (0.9607) ({r_i: None, r_t: [-1008.386 -1008.386 -1008.386], eps: 0.961})
Step:     500, Reward: [-469.663 -469.663 -469.663] [117.396], Avg: [-484.587 -484.587 -484.587] (0.9511) ({r_i: None, r_t: [-1004.580 -1004.580 -1004.580], eps: 0.951})
Step:     600, Reward: [-517.772 -517.772 -517.772] [102.129], Avg: [-489.328 -489.328 -489.328] (0.9416) ({r_i: None, r_t: [-1039.596 -1039.596 -1039.596], eps: 0.942})
Step:     700, Reward: [-510.647 -510.647 -510.647] [87.098], Avg: [-491.993 -491.993 -491.993] (0.9322) ({r_i: None, r_t: [-948.159 -948.159 -948.159], eps: 0.932})
Step:     800, Reward: [-488.720 -488.720 -488.720] [105.720], Avg: [-491.629 -491.629 -491.629] (0.9229) ({r_i: None, r_t: [-957.003 -957.003 -957.003], eps: 0.923})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action = self.layer3(state)
		return action

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.critic_training_steps = 0
		self.last_target_update_step = 0
		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)
		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def select_actions(self, inputs, eps):
		agent_outputs = self.forward(inputs, eps)
		chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
		return chosen_actions

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def train(self, batch, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_t = self.critic_local(critic_inputs[:,t])
			q_vals[:, t] = q_t
			q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			critic_loss = (td_error ** 2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters())
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.forward(actor_inputs[:,t], eps) for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * log_pi_taken).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		actions = self.network.select_actions(inputs, eps=self.eps)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.train(None, actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-488.096 -488.096 -488.096] [81.667], Avg: [-488.096 -488.096 -488.096] (1.0000) ({r_i: None, r_t: [-8.574 -8.574 -8.574], eps: 1.0})
Step:     100, Reward: [-460.275 -460.275 -460.275] [63.576], Avg: [-474.185 -474.185 -474.185] (0.9900) ({r_i: None, r_t: [-1018.778 -1018.778 -1018.778], eps: 0.99})
Step:     200, Reward: [-511.206 -511.206 -511.206] [71.817], Avg: [-486.526 -486.526 -486.526] (0.9801) ({r_i: None, r_t: [-992.297 -992.297 -992.297], eps: 0.98})
Step:     300, Reward: [-506.434 -506.434 -506.434] [128.775], Avg: [-491.503 -491.503 -491.503] (0.9704) ({r_i: None, r_t: [-971.457 -971.457 -971.457], eps: 0.97})
Step:     400, Reward: [-476.837 -476.837 -476.837] [110.576], Avg: [-488.570 -488.570 -488.570] (0.9607) ({r_i: None, r_t: [-1054.687 -1054.687 -1054.687], eps: 0.961})
Step:     500, Reward: [-496.091 -496.091 -496.091] [82.807], Avg: [-489.823 -489.823 -489.823] (0.9511) ({r_i: None, r_t: [-924.741 -924.741 -924.741], eps: 0.951})
Step:     600, Reward: [-465.358 -465.358 -465.358] [69.913], Avg: [-486.328 -486.328 -486.328] (0.9416) ({r_i: None, r_t: [-971.322 -971.322 -971.322], eps: 0.942})
Step:     700, Reward: [-533.669 -533.669 -533.669] [81.184], Avg: [-492.246 -492.246 -492.246] (0.9322) ({r_i: None, r_t: [-1004.474 -1004.474 -1004.474], eps: 0.932})
Step:     800, Reward: [-436.112 -436.112 -436.112] [55.036], Avg: [-486.009 -486.009 -486.009] (0.9229) ({r_i: None, r_t: [-1023.209 -1023.209 -1023.209], eps: 0.923})
Step:     900, Reward: [-503.092 -503.092 -503.092] [72.210], Avg: [-487.717 -487.717 -487.717] (0.9137) ({r_i: None, r_t: [-889.784 -889.784 -889.784], eps: 0.914})
Step:    1000, Reward: [-514.445 -514.445 -514.445] [103.127], Avg: [-490.147 -490.147 -490.147] (0.9046) ({r_i: None, r_t: [-1031.325 -1031.325 -1031.325], eps: 0.905})
Step:    1100, Reward: [-470.229 -470.229 -470.229] [99.107], Avg: [-488.487 -488.487 -488.487] (0.8956) ({r_i: None, r_t: [-966.345 -966.345 -966.345], eps: 0.896})
Step:    1200, Reward: [-510.803 -510.803 -510.803] [74.748], Avg: [-490.204 -490.204 -490.204] (0.8867) ({r_i: None, r_t: [-990.515 -990.515 -990.515], eps: 0.887})
Step:    1300, Reward: [-473.590 -473.590 -473.590] [57.571], Avg: [-489.017 -489.017 -489.017] (0.8778) ({r_i: None, r_t: [-1041.518 -1041.518 -1041.518], eps: 0.878})
Step:    1400, Reward: [-502.309 -502.309 -502.309] [96.862], Avg: [-489.903 -489.903 -489.903] (0.8691) ({r_i: None, r_t: [-1007.876 -1007.876 -1007.876], eps: 0.869})
Step:    1500, Reward: [-514.635 -514.635 -514.635] [83.919], Avg: [-491.449 -491.449 -491.449] (0.8604) ({r_i: None, r_t: [-905.103 -905.103 -905.103], eps: 0.86})
Step:    1600, Reward: [-471.128 -471.128 -471.128] [70.134], Avg: [-490.254 -490.254 -490.254] (0.8518) ({r_i: None, r_t: [-1017.236 -1017.236 -1017.236], eps: 0.852})
Step:    1700, Reward: [-491.611 -491.611 -491.611] [76.531], Avg: [-490.329 -490.329 -490.329] (0.8433) ({r_i: None, r_t: [-990.282 -990.282 -990.282], eps: 0.843})
Step:    1800, Reward: [-466.781 -466.781 -466.781] [84.114], Avg: [-489.090 -489.090 -489.090] (0.8349) ({r_i: None, r_t: [-1023.579 -1023.579 -1023.579], eps: 0.835})
Step:    1900, Reward: [-477.920 -477.920 -477.920] [82.296], Avg: [-488.531 -488.531 -488.531] (0.8266) ({r_i: None, r_t: [-966.592 -966.592 -966.592], eps: 0.827})
Step:    2000, Reward: [-479.787 -479.787 -479.787] [55.232], Avg: [-488.115 -488.115 -488.115] (0.8183) ({r_i: None, r_t: [-951.574 -951.574 -951.574], eps: 0.818})
Step:    2100, Reward: [-480.067 -480.067 -480.067] [105.415], Avg: [-487.749 -487.749 -487.749] (0.8102) ({r_i: None, r_t: [-967.612 -967.612 -967.612], eps: 0.81})
Step:    2200, Reward: [-505.166 -505.166 -505.166] [104.807], Avg: [-488.506 -488.506 -488.506] (0.8021) ({r_i: None, r_t: [-970.645 -970.645 -970.645], eps: 0.802})
Step:    2300, Reward: [-474.440 -474.440 -474.440] [88.510], Avg: [-487.920 -487.920 -487.920] (0.7941) ({r_i: None, r_t: [-971.343 -971.343 -971.343], eps: 0.794})
Step:    2400, Reward: [-469.920 -469.920 -469.920] [75.887], Avg: [-487.200 -487.200 -487.200] (0.7862) ({r_i: None, r_t: [-982.105 -982.105 -982.105], eps: 0.786})
Step:    2500, Reward: [-484.078 -484.078 -484.078] [81.956], Avg: [-487.080 -487.080 -487.080] (0.7783) ({r_i: None, r_t: [-965.800 -965.800 -965.800], eps: 0.778})
Step:    2600, Reward: [-482.920 -482.920 -482.920] [81.336], Avg: [-486.926 -486.926 -486.926] (0.7705) ({r_i: None, r_t: [-990.438 -990.438 -990.438], eps: 0.771})
Step:    2700, Reward: [-479.285 -479.285 -479.285] [86.473], Avg: [-486.653 -486.653 -486.653] (0.7629) ({r_i: None, r_t: [-974.303 -974.303 -974.303], eps: 0.763})
Step:    2800, Reward: [-494.920 -494.920 -494.920] [111.699], Avg: [-486.938 -486.938 -486.938] (0.7553) ({r_i: None, r_t: [-1008.420 -1008.420 -1008.420], eps: 0.755})
Step:    2900, Reward: [-445.434 -445.434 -445.434] [65.405], Avg: [-485.555 -485.555 -485.555] (0.7477) ({r_i: None, r_t: [-958.021 -958.021 -958.021], eps: 0.748})
Step:    3000, Reward: [-481.871 -481.871 -481.871] [105.449], Avg: [-485.436 -485.436 -485.436] (0.7403) ({r_i: None, r_t: [-941.072 -941.072 -941.072], eps: 0.74})
Step:    3100, Reward: [-499.808 -499.808 -499.808] [91.975], Avg: [-485.885 -485.885 -485.885] (0.7329) ({r_i: None, r_t: [-947.710 -947.710 -947.710], eps: 0.733})
Step:    3200, Reward: [-502.937 -502.937 -502.937] [103.193], Avg: [-486.402 -486.402 -486.402] (0.7256) ({r_i: None, r_t: [-913.636 -913.636 -913.636], eps: 0.726})
Step:    3300, Reward: [-450.698 -450.698 -450.698] [69.192], Avg: [-485.352 -485.352 -485.352] (0.7183) ({r_i: None, r_t: [-940.751 -940.751 -940.751], eps: 0.718})
Step:    3400, Reward: [-459.920 -459.920 -459.920] [61.340], Avg: [-484.625 -484.625 -484.625] (0.7112) ({r_i: None, r_t: [-984.616 -984.616 -984.616], eps: 0.711})
Step:    3500, Reward: [-476.209 -476.209 -476.209] [106.330], Avg: [-484.391 -484.391 -484.391] (0.7041) ({r_i: None, r_t: [-950.825 -950.825 -950.825], eps: 0.704})
Step:    3600, Reward: [-481.606 -481.606 -481.606] [82.074], Avg: [-484.316 -484.316 -484.316] (0.6970) ({r_i: None, r_t: [-936.136 -936.136 -936.136], eps: 0.697})
Step:    3700, Reward: [-498.275 -498.275 -498.275] [83.789], Avg: [-484.683 -484.683 -484.683] (0.6901) ({r_i: None, r_t: [-934.339 -934.339 -934.339], eps: 0.69})
Step:    3800, Reward: [-485.625 -485.625 -485.625] [98.634], Avg: [-484.707 -484.707 -484.707] (0.6832) ({r_i: None, r_t: [-1012.143 -1012.143 -1012.143], eps: 0.683})
Step:    3900, Reward: [-523.209 -523.209 -523.209] [114.159], Avg: [-485.670 -485.670 -485.670] (0.6764) ({r_i: None, r_t: [-1003.895 -1003.895 -1003.895], eps: 0.676})
Step:    4000, Reward: [-509.864 -509.864 -509.864] [70.623], Avg: [-486.260 -486.260 -486.260] (0.6696) ({r_i: None, r_t: [-956.759 -956.759 -956.759], eps: 0.67})
Step:    4100, Reward: [-441.809 -441.809 -441.809] [67.782], Avg: [-485.202 -485.202 -485.202] (0.6630) ({r_i: None, r_t: [-932.481 -932.481 -932.481], eps: 0.663})
Step:    4200, Reward: [-484.271 -484.271 -484.271] [92.717], Avg: [-485.180 -485.180 -485.180] (0.6564) ({r_i: None, r_t: [-933.086 -933.086 -933.086], eps: 0.656})
Step:    4300, Reward: [-506.546 -506.546 -506.546] [106.210], Avg: [-485.666 -485.666 -485.666] (0.6498) ({r_i: None, r_t: [-942.223 -942.223 -942.223], eps: 0.65})
Step:    4400, Reward: [-466.153 -466.153 -466.153] [58.769], Avg: [-485.232 -485.232 -485.232] (0.6433) ({r_i: None, r_t: [-927.640 -927.640 -927.640], eps: 0.643})
Step:    4500, Reward: [-461.177 -461.177 -461.177] [64.500], Avg: [-484.709 -484.709 -484.709] (0.6369) ({r_i: None, r_t: [-980.448 -980.448 -980.448], eps: 0.637})
Step:    4600, Reward: [-467.861 -467.861 -467.861] [90.973], Avg: [-484.351 -484.351 -484.351] (0.6306) ({r_i: None, r_t: [-955.345 -955.345 -955.345], eps: 0.631})
Step:    4700, Reward: [-463.635 -463.635 -463.635] [66.224], Avg: [-483.919 -483.919 -483.919] (0.6243) ({r_i: None, r_t: [-937.081 -937.081 -937.081], eps: 0.624})
Step:    4800, Reward: [-456.142 -456.142 -456.142] [51.518], Avg: [-483.352 -483.352 -483.352] (0.6180) ({r_i: None, r_t: [-934.976 -934.976 -934.976], eps: 0.618})
Step:    4900, Reward: [-480.189 -480.189 -480.189] [65.677], Avg: [-483.289 -483.289 -483.289] (0.6119) ({r_i: None, r_t: [-912.816 -912.816 -912.816], eps: 0.612})
Step:    5000, Reward: [-433.434 -433.434 -433.434] [54.063], Avg: [-482.311 -482.311 -482.311] (0.6058) ({r_i: None, r_t: [-901.868 -901.868 -901.868], eps: 0.606})
Step:    5100, Reward: [-423.923 -423.923 -423.923] [54.034], Avg: [-481.188 -481.188 -481.188] (0.5997) ({r_i: None, r_t: [-912.583 -912.583 -912.583], eps: 0.6})
Step:    5200, Reward: [-470.124 -470.124 -470.124] [69.716], Avg: [-480.980 -480.980 -480.980] (0.5937) ({r_i: None, r_t: [-924.777 -924.777 -924.777], eps: 0.594})
Step:    5300, Reward: [-467.587 -467.587 -467.587] [94.008], Avg: [-480.732 -480.732 -480.732] (0.5878) ({r_i: None, r_t: [-988.178 -988.178 -988.178], eps: 0.588})
Step:    5400, Reward: [-433.632 -433.632 -433.632] [75.400], Avg: [-479.875 -479.875 -479.875] (0.5820) ({r_i: None, r_t: [-915.187 -915.187 -915.187], eps: 0.582})
Step:    5500, Reward: [-440.380 -440.380 -440.380] [59.172], Avg: [-479.170 -479.170 -479.170] (0.5762) ({r_i: None, r_t: [-919.849 -919.849 -919.849], eps: 0.576})
Step:    5600, Reward: [-466.876 -466.876 -466.876] [67.992], Avg: [-478.954 -478.954 -478.954] (0.5704) ({r_i: None, r_t: [-922.011 -922.011 -922.011], eps: 0.57})
Step:    5700, Reward: [-495.242 -495.242 -495.242] [121.309], Avg: [-479.235 -479.235 -479.235] (0.5647) ({r_i: None, r_t: [-905.597 -905.597 -905.597], eps: 0.565})
Step:    5800, Reward: [-449.831 -449.831 -449.831] [103.790], Avg: [-478.737 -478.737 -478.737] (0.5591) ({r_i: None, r_t: [-883.824 -883.824 -883.824], eps: 0.559})
Step:    5900, Reward: [-464.527 -464.527 -464.527] [113.344], Avg: [-478.500 -478.500 -478.500] (0.5535) ({r_i: None, r_t: [-931.554 -931.554 -931.554], eps: 0.554})
Step:    6000, Reward: [-469.628 -469.628 -469.628] [78.427], Avg: [-478.355 -478.355 -478.355] (0.5480) ({r_i: None, r_t: [-897.369 -897.369 -897.369], eps: 0.548})
Step:    6100, Reward: [-485.122 -485.122 -485.122] [86.770], Avg: [-478.464 -478.464 -478.464] (0.5425) ({r_i: None, r_t: [-914.075 -914.075 -914.075], eps: 0.543})
Step:    6200, Reward: [-482.628 -482.628 -482.628] [100.616], Avg: [-478.530 -478.530 -478.530] (0.5371) ({r_i: None, r_t: [-973.069 -973.069 -973.069], eps: 0.537})
Step:    6300, Reward: [-439.914 -439.914 -439.914] [56.375], Avg: [-477.926 -477.926 -477.926] (0.5318) ({r_i: None, r_t: [-911.222 -911.222 -911.222], eps: 0.532})
Step:    6400, Reward: [-447.061 -447.061 -447.061] [67.749], Avg: [-477.452 -477.452 -477.452] (0.5264) ({r_i: None, r_t: [-904.467 -904.467 -904.467], eps: 0.526})
Step:    6500, Reward: [-488.668 -488.668 -488.668] [68.120], Avg: [-477.622 -477.622 -477.622] (0.5212) ({r_i: None, r_t: [-922.449 -922.449 -922.449], eps: 0.521})
Step:    6600, Reward: [-470.340 -470.340 -470.340] [64.064], Avg: [-477.513 -477.513 -477.513] (0.5160) ({r_i: None, r_t: [-947.924 -947.924 -947.924], eps: 0.516})
Step:    6700, Reward: [-470.611 -470.611 -470.611] [76.734], Avg: [-477.411 -477.411 -477.411] (0.5108) ({r_i: None, r_t: [-951.068 -951.068 -951.068], eps: 0.511})
Step:    6800, Reward: [-465.065 -465.065 -465.065] [67.640], Avg: [-477.232 -477.232 -477.232] (0.5058) ({r_i: None, r_t: [-922.029 -922.029 -922.029], eps: 0.506})
Step:    6900, Reward: [-431.169 -431.169 -431.169] [64.404], Avg: [-476.574 -476.574 -476.574] (0.5007) ({r_i: None, r_t: [-910.046 -910.046 -910.046], eps: 0.501})
Step:    7000, Reward: [-416.616 -416.616 -416.616] [62.941], Avg: [-475.730 -475.730 -475.730] (0.4957) ({r_i: None, r_t: [-883.391 -883.391 -883.391], eps: 0.496})
Step:    7100, Reward: [-461.361 -461.361 -461.361] [94.584], Avg: [-475.530 -475.530 -475.530] (0.4908) ({r_i: None, r_t: [-896.190 -896.190 -896.190], eps: 0.491})
Step:    7200, Reward: [-443.801 -443.801 -443.801] [60.705], Avg: [-475.096 -475.096 -475.096] (0.4859) ({r_i: None, r_t: [-903.251 -903.251 -903.251], eps: 0.486})
Step:    7300, Reward: [-451.470 -451.470 -451.470] [72.451], Avg: [-474.776 -474.776 -474.776] (0.4810) ({r_i: None, r_t: [-894.386 -894.386 -894.386], eps: 0.481})
Step:    7400, Reward: [-421.176 -421.176 -421.176] [86.709], Avg: [-474.062 -474.062 -474.062] (0.4762) ({r_i: None, r_t: [-885.687 -885.687 -885.687], eps: 0.476})
Step:    7500, Reward: [-472.260 -472.260 -472.260] [73.294], Avg: [-474.038 -474.038 -474.038] (0.4715) ({r_i: None, r_t: [-937.038 -937.038 -937.038], eps: 0.471})
Step:    7600, Reward: [-463.635 -463.635 -463.635] [80.561], Avg: [-473.903 -473.903 -473.903] (0.4668) ({r_i: None, r_t: [-914.330 -914.330 -914.330], eps: 0.467})
Step:    7700, Reward: [-448.967 -448.967 -448.967] [79.128], Avg: [-473.583 -473.583 -473.583] (0.4621) ({r_i: None, r_t: [-927.919 -927.919 -927.919], eps: 0.462})
Step:    7800, Reward: [-438.728 -438.728 -438.728] [77.500], Avg: [-473.142 -473.142 -473.142] (0.4575) ({r_i: None, r_t: [-903.207 -903.207 -903.207], eps: 0.458})
Step:    7900, Reward: [-460.844 -460.844 -460.844] [103.152], Avg: [-472.988 -472.988 -472.988] (0.4529) ({r_i: None, r_t: [-872.827 -872.827 -872.827], eps: 0.453})
Step:    8000, Reward: [-505.458 -505.458 -505.458] [90.329], Avg: [-473.389 -473.389 -473.389] (0.4484) ({r_i: None, r_t: [-889.150 -889.150 -889.150], eps: 0.448})
Step:    8100, Reward: [-435.941 -435.941 -435.941] [83.650], Avg: [-472.932 -472.932 -472.932] (0.4440) ({r_i: None, r_t: [-854.620 -854.620 -854.620], eps: 0.444})
Step:    8200, Reward: [-434.449 -434.449 -434.449] [72.755], Avg: [-472.469 -472.469 -472.469] (0.4395) ({r_i: None, r_t: [-946.642 -946.642 -946.642], eps: 0.44})
Step:    8300, Reward: [-459.010 -459.010 -459.010] [80.952], Avg: [-472.309 -472.309 -472.309] (0.4351) ({r_i: None, r_t: [-937.378 -937.378 -937.378], eps: 0.435})
Step:    8400, Reward: [-474.503 -474.503 -474.503] [102.371], Avg: [-472.334 -472.334 -472.334] (0.4308) ({r_i: None, r_t: [-895.517 -895.517 -895.517], eps: 0.431})
Step:    8500, Reward: [-435.677 -435.677 -435.677] [73.751], Avg: [-471.908 -471.908 -471.908] (0.4265) ({r_i: None, r_t: [-953.359 -953.359 -953.359], eps: 0.427})
Step:    8600, Reward: [-408.803 -408.803 -408.803] [50.077], Avg: [-471.183 -471.183 -471.183] (0.4223) ({r_i: None, r_t: [-892.839 -892.839 -892.839], eps: 0.422})
Step:    8700, Reward: [-433.124 -433.124 -433.124] [72.282], Avg: [-470.750 -470.750 -470.750] (0.4180) ({r_i: None, r_t: [-870.369 -870.369 -870.369], eps: 0.418})
Step:    8800, Reward: [-461.710 -461.710 -461.710] [93.235], Avg: [-470.649 -470.649 -470.649] (0.4139) ({r_i: None, r_t: [-921.950 -921.950 -921.950], eps: 0.414})
Step:    8900, Reward: [-473.630 -473.630 -473.630] [94.629], Avg: [-470.682 -470.682 -470.682] (0.4097) ({r_i: None, r_t: [-953.684 -953.684 -953.684], eps: 0.41})
Step:    9000, Reward: [-435.646 -435.646 -435.646] [62.205], Avg: [-470.297 -470.297 -470.297] (0.4057) ({r_i: None, r_t: [-870.903 -870.903 -870.903], eps: 0.406})
Step:    9100, Reward: [-450.929 -450.929 -450.929] [70.529], Avg: [-470.086 -470.086 -470.086] (0.4016) ({r_i: None, r_t: [-852.868 -852.868 -852.868], eps: 0.402})
Step:    9200, Reward: [-405.868 -405.868 -405.868] [49.014], Avg: [-469.396 -469.396 -469.396] (0.3976) ({r_i: None, r_t: [-932.692 -932.692 -932.692], eps: 0.398})
Step:    9300, Reward: [-452.118 -452.118 -452.118] [53.126], Avg: [-469.212 -469.212 -469.212] (0.3936) ({r_i: None, r_t: [-894.351 -894.351 -894.351], eps: 0.394})
Step:    9400, Reward: [-473.635 -473.635 -473.635] [123.920], Avg: [-469.259 -469.259 -469.259] (0.3897) ({r_i: None, r_t: [-913.784 -913.784 -913.784], eps: 0.39})
Step:    9500, Reward: [-441.771 -441.771 -441.771] [82.608], Avg: [-468.972 -468.972 -468.972] (0.3858) ({r_i: None, r_t: [-892.660 -892.660 -892.660], eps: 0.386})
Step:    9600, Reward: [-471.098 -471.098 -471.098] [70.968], Avg: [-468.994 -468.994 -468.994] (0.3820) ({r_i: None, r_t: [-930.196 -930.196 -930.196], eps: 0.382})
Step:    9700, Reward: [-435.722 -435.722 -435.722] [86.128], Avg: [-468.655 -468.655 -468.655] (0.3782) ({r_i: None, r_t: [-886.312 -886.312 -886.312], eps: 0.378})
Step:    9800, Reward: [-472.479 -472.479 -472.479] [94.135], Avg: [-468.693 -468.693 -468.693] (0.3744) ({r_i: None, r_t: [-876.703 -876.703 -876.703], eps: 0.374})
Step:    9900, Reward: [-472.786 -472.786 -472.786] [60.154], Avg: [-468.734 -468.734 -468.734] (0.3707) ({r_i: None, r_t: [-854.202 -854.202 -854.202], eps: 0.371})
Step:   10000, Reward: [-466.621 -466.621 -466.621] [81.915], Avg: [-468.713 -468.713 -468.713] (0.3670) ({r_i: None, r_t: [-867.375 -867.375 -867.375], eps: 0.367})
Step:   10100, Reward: [-444.498 -444.498 -444.498] [86.094], Avg: [-468.476 -468.476 -468.476] (0.3633) ({r_i: None, r_t: [-884.749 -884.749 -884.749], eps: 0.363})
Step:   10200, Reward: [-437.968 -437.968 -437.968] [68.675], Avg: [-468.180 -468.180 -468.180] (0.3597) ({r_i: None, r_t: [-862.089 -862.089 -862.089], eps: 0.36})
Step:   10300, Reward: [-420.943 -420.943 -420.943] [69.278], Avg: [-467.726 -467.726 -467.726] (0.3561) ({r_i: None, r_t: [-903.424 -903.424 -903.424], eps: 0.356})
Step:   10400, Reward: [-414.926 -414.926 -414.926] [74.792], Avg: [-467.223 -467.223 -467.223] (0.3525) ({r_i: None, r_t: [-880.362 -880.362 -880.362], eps: 0.353})
Step:   10500, Reward: [-422.437 -422.437 -422.437] [47.559], Avg: [-466.800 -466.800 -466.800] (0.3490) ({r_i: None, r_t: [-844.925 -844.925 -844.925], eps: 0.349})
Step:   10600, Reward: [-434.324 -434.324 -434.324] [72.140], Avg: [-466.497 -466.497 -466.497] (0.3455) ({r_i: None, r_t: [-874.717 -874.717 -874.717], eps: 0.346})
Step:   10700, Reward: [-453.584 -453.584 -453.584] [86.123], Avg: [-466.377 -466.377 -466.377] (0.3421) ({r_i: None, r_t: [-900.055 -900.055 -900.055], eps: 0.342})
Step:   10800, Reward: [-438.591 -438.591 -438.591] [65.979], Avg: [-466.122 -466.122 -466.122] (0.3387) ({r_i: None, r_t: [-888.955 -888.955 -888.955], eps: 0.339})
Step:   10900, Reward: [-403.503 -403.503 -403.503] [76.327], Avg: [-465.553 -465.553 -465.553] (0.3353) ({r_i: None, r_t: [-899.176 -899.176 -899.176], eps: 0.335})
Step:   11000, Reward: [-449.444 -449.444 -449.444] [86.083], Avg: [-465.408 -465.408 -465.408] (0.3320) ({r_i: None, r_t: [-824.251 -824.251 -824.251], eps: 0.332})
Step:   11100, Reward: [-412.519 -412.519 -412.519] [76.329], Avg: [-464.936 -464.936 -464.936] (0.3286) ({r_i: None, r_t: [-832.883 -832.883 -832.883], eps: 0.329})
Step:   11200, Reward: [-449.445 -449.445 -449.445] [94.903], Avg: [-464.798 -464.798 -464.798] (0.3254) ({r_i: None, r_t: [-865.339 -865.339 -865.339], eps: 0.325})
Step:   11300, Reward: [-415.044 -415.044 -415.044] [74.975], Avg: [-464.362 -464.362 -464.362] (0.3221) ({r_i: None, r_t: [-900.843 -900.843 -900.843], eps: 0.322})
Step:   11400, Reward: [-420.260 -420.260 -420.260] [42.421], Avg: [-463.979 -463.979 -463.979] (0.3189) ({r_i: None, r_t: [-870.964 -870.964 -870.964], eps: 0.319})
Step:   11500, Reward: [-447.680 -447.680 -447.680] [71.219], Avg: [-463.838 -463.838 -463.838] (0.3157) ({r_i: None, r_t: [-819.854 -819.854 -819.854], eps: 0.316})
Step:   11600, Reward: [-429.980 -429.980 -429.980] [55.508], Avg: [-463.549 -463.549 -463.549] (0.3126) ({r_i: None, r_t: [-839.446 -839.446 -839.446], eps: 0.313})
Step:   11700, Reward: [-410.250 -410.250 -410.250] [60.622], Avg: [-463.097 -463.097 -463.097] (0.3095) ({r_i: None, r_t: [-858.459 -858.459 -858.459], eps: 0.309})
Step:   11800, Reward: [-427.412 -427.412 -427.412] [80.353], Avg: [-462.797 -462.797 -462.797] (0.3064) ({r_i: None, r_t: [-848.661 -848.661 -848.661], eps: 0.306})
Step:   11900, Reward: [-429.306 -429.306 -429.306] [61.567], Avg: [-462.518 -462.518 -462.518] (0.3033) ({r_i: None, r_t: [-835.263 -835.263 -835.263], eps: 0.303})
Step:   12000, Reward: [-420.016 -420.016 -420.016] [49.687], Avg: [-462.167 -462.167 -462.167] (0.3003) ({r_i: None, r_t: [-885.467 -885.467 -885.467], eps: 0.3})
Step:   12100, Reward: [-432.621 -432.621 -432.621] [66.195], Avg: [-461.925 -461.925 -461.925] (0.2973) ({r_i: None, r_t: [-845.119 -845.119 -845.119], eps: 0.297})
Step:   12200, Reward: [-443.454 -443.454 -443.454] [102.665], Avg: [-461.774 -461.774 -461.774] (0.2943) ({r_i: None, r_t: [-896.056 -896.056 -896.056], eps: 0.294})
Step:   12300, Reward: [-425.212 -425.212 -425.212] [48.527], Avg: [-461.480 -461.480 -461.480] (0.2914) ({r_i: None, r_t: [-904.315 -904.315 -904.315], eps: 0.291})
Step:   12400, Reward: [-449.944 -449.944 -449.944] [60.205], Avg: [-461.387 -461.387 -461.387] (0.2885) ({r_i: None, r_t: [-854.675 -854.675 -854.675], eps: 0.288})
Step:   12500, Reward: [-404.198 -404.198 -404.198] [37.871], Avg: [-460.933 -460.933 -460.933] (0.2856) ({r_i: None, r_t: [-879.346 -879.346 -879.346], eps: 0.286})
Step:   12600, Reward: [-425.004 -425.004 -425.004] [46.799], Avg: [-460.650 -460.650 -460.650] (0.2828) ({r_i: None, r_t: [-851.178 -851.178 -851.178], eps: 0.283})
Step:   12700, Reward: [-435.410 -435.410 -435.410] [59.157], Avg: [-460.453 -460.453 -460.453] (0.2799) ({r_i: None, r_t: [-855.539 -855.539 -855.539], eps: 0.28})
Step:   12800, Reward: [-411.263 -411.263 -411.263] [57.543], Avg: [-460.072 -460.072 -460.072] (0.2771) ({r_i: None, r_t: [-856.737 -856.737 -856.737], eps: 0.277})
Step:   12900, Reward: [-461.459 -461.459 -461.459] [87.058], Avg: [-460.083 -460.083 -460.083] (0.2744) ({r_i: None, r_t: [-828.955 -828.955 -828.955], eps: 0.274})
Step:   13000, Reward: [-433.377 -433.377 -433.377] [60.333], Avg: [-459.879 -459.879 -459.879] (0.2716) ({r_i: None, r_t: [-808.520 -808.520 -808.520], eps: 0.272})
Step:   13100, Reward: [-393.906 -393.906 -393.906] [59.605], Avg: [-459.379 -459.379 -459.379] (0.2689) ({r_i: None, r_t: [-794.350 -794.350 -794.350], eps: 0.269})
Step:   13200, Reward: [-410.482 -410.482 -410.482] [83.871], Avg: [-459.011 -459.011 -459.011] (0.2663) ({r_i: None, r_t: [-817.964 -817.964 -817.964], eps: 0.266})
Step:   13300, Reward: [-420.484 -420.484 -420.484] [66.334], Avg: [-458.724 -458.724 -458.724] (0.2636) ({r_i: None, r_t: [-862.905 -862.905 -862.905], eps: 0.264})
Step:   13400, Reward: [-440.700 -440.700 -440.700] [56.654], Avg: [-458.590 -458.590 -458.590] (0.2610) ({r_i: None, r_t: [-883.275 -883.275 -883.275], eps: 0.261})
Step:   13500, Reward: [-403.338 -403.338 -403.338] [53.714], Avg: [-458.184 -458.184 -458.184] (0.2584) ({r_i: None, r_t: [-832.839 -832.839 -832.839], eps: 0.258})
Step:   13600, Reward: [-433.844 -433.844 -433.844] [62.345], Avg: [-458.006 -458.006 -458.006] (0.2558) ({r_i: None, r_t: [-826.755 -826.755 -826.755], eps: 0.256})
Step:   13700, Reward: [-414.809 -414.809 -414.809] [66.227], Avg: [-457.693 -457.693 -457.693] (0.2532) ({r_i: None, r_t: [-877.305 -877.305 -877.305], eps: 0.253})
Step:   13800, Reward: [-425.484 -425.484 -425.484] [79.464], Avg: [-457.462 -457.462 -457.462] (0.2507) ({r_i: None, r_t: [-869.599 -869.599 -869.599], eps: 0.251})
Step:   13900, Reward: [-421.053 -421.053 -421.053] [54.069], Avg: [-457.202 -457.202 -457.202] (0.2482) ({r_i: None, r_t: [-848.180 -848.180 -848.180], eps: 0.248})
Step:   14000, Reward: [-415.856 -415.856 -415.856] [63.097], Avg: [-456.908 -456.908 -456.908] (0.2457) ({r_i: None, r_t: [-861.861 -861.861 -861.861], eps: 0.246})
Step:   14100, Reward: [-433.273 -433.273 -433.273] [52.606], Avg: [-456.742 -456.742 -456.742] (0.2433) ({r_i: None, r_t: [-895.109 -895.109 -895.109], eps: 0.243})
Step:   14200, Reward: [-422.906 -422.906 -422.906] [76.382], Avg: [-456.505 -456.505 -456.505] (0.2409) ({r_i: None, r_t: [-902.589 -902.589 -902.589], eps: 0.241})
Step:   14300, Reward: [-427.882 -427.882 -427.882] [73.992], Avg: [-456.306 -456.306 -456.306] (0.2385) ({r_i: None, r_t: [-849.886 -849.886 -849.886], eps: 0.238})
Step:   14400, Reward: [-432.039 -432.039 -432.039] [60.516], Avg: [-456.139 -456.139 -456.139] (0.2361) ({r_i: None, r_t: [-843.932 -843.932 -843.932], eps: 0.236})
Step:   14500, Reward: [-436.812 -436.812 -436.812] [74.160], Avg: [-456.007 -456.007 -456.007] (0.2337) ({r_i: None, r_t: [-867.702 -867.702 -867.702], eps: 0.234})
Step:   14600, Reward: [-444.933 -444.933 -444.933] [73.214], Avg: [-455.931 -455.931 -455.931] (0.2314) ({r_i: None, r_t: [-937.523 -937.523 -937.523], eps: 0.231})
Step:   14700, Reward: [-414.883 -414.883 -414.883] [51.382], Avg: [-455.654 -455.654 -455.654] (0.2291) ({r_i: None, r_t: [-837.747 -837.747 -837.747], eps: 0.229})
Step:   14800, Reward: [-425.637 -425.637 -425.637] [80.779], Avg: [-455.453 -455.453 -455.453] (0.2268) ({r_i: None, r_t: [-845.332 -845.332 -845.332], eps: 0.227})
Step:   14900, Reward: [-441.399 -441.399 -441.399] [77.308], Avg: [-455.359 -455.359 -455.359] (0.2245) ({r_i: None, r_t: [-836.154 -836.154 -836.154], eps: 0.225})
Step:   15000, Reward: [-431.493 -431.493 -431.493] [58.637], Avg: [-455.201 -455.201 -455.201] (0.2223) ({r_i: None, r_t: [-820.941 -820.941 -820.941], eps: 0.222})
Step:   15100, Reward: [-436.359 -436.359 -436.359] [83.164], Avg: [-455.077 -455.077 -455.077] (0.2201) ({r_i: None, r_t: [-885.483 -885.483 -885.483], eps: 0.22})
Step:   15200, Reward: [-438.108 -438.108 -438.108] [55.291], Avg: [-454.966 -454.966 -454.966] (0.2179) ({r_i: None, r_t: [-863.210 -863.210 -863.210], eps: 0.218})
Step:   15300, Reward: [-415.833 -415.833 -415.833] [62.268], Avg: [-454.712 -454.712 -454.712] (0.2157) ({r_i: None, r_t: [-819.777 -819.777 -819.777], eps: 0.216})
Step:   15400, Reward: [-385.900 -385.900 -385.900] [52.444], Avg: [-454.268 -454.268 -454.268] (0.2136) ({r_i: None, r_t: [-797.905 -797.905 -797.905], eps: 0.214})
Step:   15500, Reward: [-411.804 -411.804 -411.804] [60.088], Avg: [-453.996 -453.996 -453.996] (0.2114) ({r_i: None, r_t: [-835.757 -835.757 -835.757], eps: 0.211})
Step:   15600, Reward: [-412.344 -412.344 -412.344] [51.786], Avg: [-453.730 -453.730 -453.730] (0.2093) ({r_i: None, r_t: [-850.825 -850.825 -850.825], eps: 0.209})
Step:   15700, Reward: [-403.191 -403.191 -403.191] [68.156], Avg: [-453.411 -453.411 -453.411] (0.2072) ({r_i: None, r_t: [-817.958 -817.958 -817.958], eps: 0.207})
Step:   15800, Reward: [-439.002 -439.002 -439.002] [64.861], Avg: [-453.320 -453.320 -453.320] (0.2052) ({r_i: None, r_t: [-823.934 -823.934 -823.934], eps: 0.205})
Step:   15900, Reward: [-433.443 -433.443 -433.443] [47.939], Avg: [-453.196 -453.196 -453.196] (0.2031) ({r_i: None, r_t: [-823.474 -823.474 -823.474], eps: 0.203})
Step:   16000, Reward: [-418.548 -418.548 -418.548] [57.906], Avg: [-452.981 -452.981 -452.981] (0.2011) ({r_i: None, r_t: [-817.472 -817.472 -817.472], eps: 0.201})
Step:   16100, Reward: [-391.599 -391.599 -391.599] [49.882], Avg: [-452.602 -452.602 -452.602] (0.1991) ({r_i: None, r_t: [-824.646 -824.646 -824.646], eps: 0.199})
Step:   16200, Reward: [-408.006 -408.006 -408.006] [44.260], Avg: [-452.328 -452.328 -452.328] (0.1971) ({r_i: None, r_t: [-808.566 -808.566 -808.566], eps: 0.197})
Step:   16300, Reward: [-410.053 -410.053 -410.053] [51.530], Avg: [-452.070 -452.070 -452.070] (0.1951) ({r_i: None, r_t: [-793.473 -793.473 -793.473], eps: 0.195})
Step:   16400, Reward: [-386.237 -386.237 -386.237] [56.830], Avg: [-451.671 -451.671 -451.671] (0.1932) ({r_i: None, r_t: [-797.223 -797.223 -797.223], eps: 0.193})
Step:   16500, Reward: [-406.620 -406.620 -406.620] [73.441], Avg: [-451.400 -451.400 -451.400] (0.1913) ({r_i: None, r_t: [-812.104 -812.104 -812.104], eps: 0.191})
Step:   16600, Reward: [-385.624 -385.624 -385.624] [52.467], Avg: [-451.006 -451.006 -451.006] (0.1893) ({r_i: None, r_t: [-810.513 -810.513 -810.513], eps: 0.189})
Step:   16700, Reward: [-413.729 -413.729 -413.729] [49.956], Avg: [-450.784 -450.784 -450.784] (0.1875) ({r_i: None, r_t: [-797.969 -797.969 -797.969], eps: 0.187})
Step:   16800, Reward: [-398.570 -398.570 -398.570] [35.145], Avg: [-450.475 -450.475 -450.475] (0.1856) ({r_i: None, r_t: [-793.624 -793.624 -793.624], eps: 0.186})
Step:   16900, Reward: [-383.941 -383.941 -383.941] [45.311], Avg: [-450.084 -450.084 -450.084] (0.1837) ({r_i: None, r_t: [-790.720 -790.720 -790.720], eps: 0.184})
Step:   17000, Reward: [-431.659 -431.659 -431.659] [71.434], Avg: [-449.976 -449.976 -449.976] (0.1819) ({r_i: None, r_t: [-798.994 -798.994 -798.994], eps: 0.182})
Step:   17100, Reward: [-382.712 -382.712 -382.712] [53.656], Avg: [-449.585 -449.585 -449.585] (0.1801) ({r_i: None, r_t: [-814.447 -814.447 -814.447], eps: 0.18})
Step:   17200, Reward: [-410.222 -410.222 -410.222] [53.417], Avg: [-449.357 -449.357 -449.357] (0.1783) ({r_i: None, r_t: [-870.531 -870.531 -870.531], eps: 0.178})
Step:   17300, Reward: [-394.590 -394.590 -394.590] [56.494], Avg: [-449.043 -449.043 -449.043] (0.1765) ({r_i: None, r_t: [-834.227 -834.227 -834.227], eps: 0.177})
Step:   17400, Reward: [-431.602 -431.602 -431.602] [54.053], Avg: [-448.943 -448.943 -448.943] (0.1748) ({r_i: None, r_t: [-804.031 -804.031 -804.031], eps: 0.175})
Step:   17500, Reward: [-432.328 -432.328 -432.328] [64.496], Avg: [-448.849 -448.849 -448.849] (0.1730) ({r_i: None, r_t: [-825.610 -825.610 -825.610], eps: 0.173})
Step:   17600, Reward: [-424.761 -424.761 -424.761] [67.046], Avg: [-448.713 -448.713 -448.713] (0.1713) ({r_i: None, r_t: [-784.493 -784.493 -784.493], eps: 0.171})
Step:   17700, Reward: [-388.780 -388.780 -388.780] [58.039], Avg: [-448.376 -448.376 -448.376] (0.1696) ({r_i: None, r_t: [-832.681 -832.681 -832.681], eps: 0.17})
Step:   17800, Reward: [-437.463 -437.463 -437.463] [68.699], Avg: [-448.315 -448.315 -448.315] (0.1679) ({r_i: None, r_t: [-842.300 -842.300 -842.300], eps: 0.168})
Step:   17900, Reward: [-412.775 -412.775 -412.775] [46.830], Avg: [-448.117 -448.117 -448.117] (0.1662) ({r_i: None, r_t: [-840.181 -840.181 -840.181], eps: 0.166})
Step:   18000, Reward: [-412.067 -412.067 -412.067] [46.736], Avg: [-447.918 -447.918 -447.918] (0.1646) ({r_i: None, r_t: [-835.767 -835.767 -835.767], eps: 0.165})
Step:   18100, Reward: [-414.843 -414.843 -414.843] [42.225], Avg: [-447.737 -447.737 -447.737] (0.1629) ({r_i: None, r_t: [-829.353 -829.353 -829.353], eps: 0.163})
Step:   18200, Reward: [-416.842 -416.842 -416.842] [86.871], Avg: [-447.568 -447.568 -447.568] (0.1613) ({r_i: None, r_t: [-818.512 -818.512 -818.512], eps: 0.161})
Step:   18300, Reward: [-394.318 -394.318 -394.318] [47.864], Avg: [-447.278 -447.278 -447.278] (0.1597) ({r_i: None, r_t: [-813.695 -813.695 -813.695], eps: 0.16})
Step:   18400, Reward: [-415.673 -415.673 -415.673] [66.928], Avg: [-447.107 -447.107 -447.107] (0.1581) ({r_i: None, r_t: [-861.641 -861.641 -861.641], eps: 0.158})
Step:   18500, Reward: [-408.120 -408.120 -408.120] [64.422], Avg: [-446.898 -446.898 -446.898] (0.1565) ({r_i: None, r_t: [-801.606 -801.606 -801.606], eps: 0.157})
Step:   18600, Reward: [-411.232 -411.232 -411.232] [64.949], Avg: [-446.707 -446.707 -446.707] (0.1549) ({r_i: None, r_t: [-810.796 -810.796 -810.796], eps: 0.155})
Step:   18700, Reward: [-402.692 -402.692 -402.692] [63.993], Avg: [-446.473 -446.473 -446.473] (0.1534) ({r_i: None, r_t: [-811.986 -811.986 -811.986], eps: 0.153})
Step:   18800, Reward: [-429.326 -429.326 -429.326] [63.690], Avg: [-446.382 -446.382 -446.382] (0.1519) ({r_i: None, r_t: [-860.883 -860.883 -860.883], eps: 0.152})
Step:   18900, Reward: [-442.924 -442.924 -442.924] [62.630], Avg: [-446.364 -446.364 -446.364] (0.1504) ({r_i: None, r_t: [-823.734 -823.734 -823.734], eps: 0.15})
Step:   19000, Reward: [-402.390 -402.390 -402.390] [53.418], Avg: [-446.134 -446.134 -446.134] (0.1489) ({r_i: None, r_t: [-827.619 -827.619 -827.619], eps: 0.149})
Step:   19100, Reward: [-417.545 -417.545 -417.545] [53.816], Avg: [-445.985 -445.985 -445.985] (0.1474) ({r_i: None, r_t: [-799.976 -799.976 -799.976], eps: 0.147})
Step:   19200, Reward: [-416.254 -416.254 -416.254] [44.803], Avg: [-445.831 -445.831 -445.831] (0.1459) ({r_i: None, r_t: [-883.456 -883.456 -883.456], eps: 0.146})
Step:   19300, Reward: [-409.171 -409.171 -409.171] [51.277], Avg: [-445.642 -445.642 -445.642] (0.1444) ({r_i: None, r_t: [-831.479 -831.479 -831.479], eps: 0.144})
Step:   19400, Reward: [-411.801 -411.801 -411.801] [71.493], Avg: [-445.468 -445.468 -445.468] (0.1430) ({r_i: None, r_t: [-844.799 -844.799 -844.799], eps: 0.143})
Step:   19500, Reward: [-436.736 -436.736 -436.736] [51.870], Avg: [-445.424 -445.424 -445.424] (0.1416) ({r_i: None, r_t: [-865.787 -865.787 -865.787], eps: 0.142})
Step:   19600, Reward: [-403.090 -403.090 -403.090] [69.235], Avg: [-445.209 -445.209 -445.209] (0.1402) ({r_i: None, r_t: [-810.729 -810.729 -810.729], eps: 0.14})
Step:   19700, Reward: [-427.254 -427.254 -427.254] [66.667], Avg: [-445.118 -445.118 -445.118] (0.1388) ({r_i: None, r_t: [-817.673 -817.673 -817.673], eps: 0.139})
Step:   19800, Reward: [-431.128 -431.128 -431.128] [53.823], Avg: [-445.048 -445.048 -445.048] (0.1374) ({r_i: None, r_t: [-844.990 -844.990 -844.990], eps: 0.137})
Step:   19900, Reward: [-393.958 -393.958 -393.958] [70.141], Avg: [-444.792 -444.792 -444.792] (0.1360) ({r_i: None, r_t: [-844.230 -844.230 -844.230], eps: 0.136})
Step:   20000, Reward: [-427.186 -427.186 -427.186] [55.105], Avg: [-444.705 -444.705 -444.705] (0.1347) ({r_i: None, r_t: [-850.650 -850.650 -850.650], eps: 0.135})
Step:   20100, Reward: [-410.135 -410.135 -410.135] [46.034], Avg: [-444.534 -444.534 -444.534] (0.1333) ({r_i: None, r_t: [-796.994 -796.994 -796.994], eps: 0.133})
Step:   20200, Reward: [-421.296 -421.296 -421.296] [51.730], Avg: [-444.419 -444.419 -444.419] (0.1320) ({r_i: None, r_t: [-840.384 -840.384 -840.384], eps: 0.132})
Step:   20300, Reward: [-439.758 -439.758 -439.758] [77.971], Avg: [-444.396 -444.396 -444.396] (0.1307) ({r_i: None, r_t: [-826.757 -826.757 -826.757], eps: 0.131})
Step:   20400, Reward: [-399.340 -399.340 -399.340] [59.598], Avg: [-444.177 -444.177 -444.177] (0.1294) ({r_i: None, r_t: [-799.838 -799.838 -799.838], eps: 0.129})
Step:   20500, Reward: [-388.398 -388.398 -388.398] [54.943], Avg: [-443.906 -443.906 -443.906] (0.1281) ({r_i: None, r_t: [-833.678 -833.678 -833.678], eps: 0.128})
Step:   20600, Reward: [-419.155 -419.155 -419.155] [71.702], Avg: [-443.786 -443.786 -443.786] (0.1268) ({r_i: None, r_t: [-850.397 -850.397 -850.397], eps: 0.127})
Step:   20700, Reward: [-443.607 -443.607 -443.607] [62.516], Avg: [-443.785 -443.785 -443.785] (0.1255) ({r_i: None, r_t: [-828.449 -828.449 -828.449], eps: 0.126})
Step:   20800, Reward: [-417.826 -417.826 -417.826] [46.705], Avg: [-443.661 -443.661 -443.661] (0.1243) ({r_i: None, r_t: [-854.566 -854.566 -854.566], eps: 0.124})
Step:   20900, Reward: [-434.662 -434.662 -434.662] [63.465], Avg: [-443.618 -443.618 -443.618] (0.1230) ({r_i: None, r_t: [-836.029 -836.029 -836.029], eps: 0.123})
Step:   21000, Reward: [-424.408 -424.408 -424.408] [86.289], Avg: [-443.527 -443.527 -443.527] (0.1218) ({r_i: None, r_t: [-877.841 -877.841 -877.841], eps: 0.122})
Step:   21100, Reward: [-408.492 -408.492 -408.492] [57.534], Avg: [-443.362 -443.362 -443.362] (0.1206) ({r_i: None, r_t: [-811.354 -811.354 -811.354], eps: 0.121})
Step:   21200, Reward: [-416.508 -416.508 -416.508] [55.150], Avg: [-443.236 -443.236 -443.236] (0.1194) ({r_i: None, r_t: [-870.315 -870.315 -870.315], eps: 0.119})
Step:   21300, Reward: [-402.052 -402.052 -402.052] [62.192], Avg: [-443.044 -443.044 -443.044] (0.1182) ({r_i: None, r_t: [-841.113 -841.113 -841.113], eps: 0.118})
Step:   21400, Reward: [-417.372 -417.372 -417.372] [66.959], Avg: [-442.924 -442.924 -442.924] (0.1170) ({r_i: None, r_t: [-857.238 -857.238 -857.238], eps: 0.117})
Step:   21500, Reward: [-413.219 -413.219 -413.219] [69.148], Avg: [-442.787 -442.787 -442.787] (0.1159) ({r_i: None, r_t: [-849.140 -849.140 -849.140], eps: 0.116})
Step:   21600, Reward: [-427.445 -427.445 -427.445] [65.636], Avg: [-442.716 -442.716 -442.716] (0.1147) ({r_i: None, r_t: [-848.831 -848.831 -848.831], eps: 0.115})
Step:   21700, Reward: [-400.401 -400.401 -400.401] [68.861], Avg: [-442.522 -442.522 -442.522] (0.1136) ({r_i: None, r_t: [-871.532 -871.532 -871.532], eps: 0.114})
Step:   21800, Reward: [-441.030 -441.030 -441.030] [65.940], Avg: [-442.515 -442.515 -442.515] (0.1124) ({r_i: None, r_t: [-872.585 -872.585 -872.585], eps: 0.112})
Step:   21900, Reward: [-423.794 -423.794 -423.794] [43.027], Avg: [-442.430 -442.430 -442.430] (0.1113) ({r_i: None, r_t: [-808.574 -808.574 -808.574], eps: 0.111})
Step:   22000, Reward: [-429.354 -429.354 -429.354] [69.537], Avg: [-442.371 -442.371 -442.371] (0.1102) ({r_i: None, r_t: [-839.735 -839.735 -839.735], eps: 0.11})
Step:   22100, Reward: [-426.504 -426.504 -426.504] [43.731], Avg: [-442.299 -442.299 -442.299] (0.1091) ({r_i: None, r_t: [-843.860 -843.860 -843.860], eps: 0.109})
Step:   22200, Reward: [-436.062 -436.062 -436.062] [67.043], Avg: [-442.271 -442.271 -442.271] (0.1080) ({r_i: None, r_t: [-862.882 -862.882 -862.882], eps: 0.108})
Step:   22300, Reward: [-423.347 -423.347 -423.347] [69.222], Avg: [-442.187 -442.187 -442.187] (0.1069) ({r_i: None, r_t: [-809.387 -809.387 -809.387], eps: 0.107})
Step:   22400, Reward: [-419.242 -419.242 -419.242] [76.534], Avg: [-442.085 -442.085 -442.085] (0.1059) ({r_i: None, r_t: [-850.462 -850.462 -850.462], eps: 0.106})
Step:   22500, Reward: [-430.148 -430.148 -430.148] [74.963], Avg: [-442.032 -442.032 -442.032] (0.1048) ({r_i: None, r_t: [-853.778 -853.778 -853.778], eps: 0.105})
Step:   22600, Reward: [-412.306 -412.306 -412.306] [65.544], Avg: [-441.901 -441.901 -441.901] (0.1038) ({r_i: None, r_t: [-800.735 -800.735 -800.735], eps: 0.104})
Step:   22700, Reward: [-415.530 -415.530 -415.530] [62.966], Avg: [-441.785 -441.785 -441.785] (0.1027) ({r_i: None, r_t: [-841.509 -841.509 -841.509], eps: 0.103})
Step:   22800, Reward: [-418.586 -418.586 -418.586] [56.946], Avg: [-441.684 -441.684 -441.684] (0.1017) ({r_i: None, r_t: [-859.694 -859.694 -859.694], eps: 0.102})
Step:   22900, Reward: [-393.968 -393.968 -393.968] [46.847], Avg: [-441.477 -441.477 -441.477] (0.1007) ({r_i: None, r_t: [-878.384 -878.384 -878.384], eps: 0.101})
Step:   23000, Reward: [-436.968 -436.968 -436.968] [53.464], Avg: [-441.457 -441.457 -441.457] (0.0997) ({r_i: None, r_t: [-796.827 -796.827 -796.827], eps: 0.1})
Step:   23100, Reward: [-435.120 -435.120 -435.120] [70.684], Avg: [-441.430 -441.430 -441.430] (0.0987) ({r_i: None, r_t: [-840.604 -840.604 -840.604], eps: 0.099})
Step:   23200, Reward: [-443.679 -443.679 -443.679] [77.344], Avg: [-441.439 -441.439 -441.439] (0.0977) ({r_i: None, r_t: [-838.704 -838.704 -838.704], eps: 0.098})
Step:   23300, Reward: [-483.195 -483.195 -483.195] [58.171], Avg: [-441.618 -441.618 -441.618] (0.0967) ({r_i: None, r_t: [-836.216 -836.216 -836.216], eps: 0.097})
Step:   23400, Reward: [-407.169 -407.169 -407.169] [65.930], Avg: [-441.471 -441.471 -441.471] (0.0958) ({r_i: None, r_t: [-878.432 -878.432 -878.432], eps: 0.096})
Step:   23500, Reward: [-417.137 -417.137 -417.137] [68.625], Avg: [-441.368 -441.368 -441.368] (0.0948) ({r_i: None, r_t: [-848.072 -848.072 -848.072], eps: 0.095})
Step:   23600, Reward: [-426.246 -426.246 -426.246] [64.480], Avg: [-441.304 -441.304 -441.304] (0.0939) ({r_i: None, r_t: [-847.859 -847.859 -847.859], eps: 0.094})
Step:   23700, Reward: [-417.569 -417.569 -417.569] [63.997], Avg: [-441.205 -441.205 -441.205] (0.0929) ({r_i: None, r_t: [-857.233 -857.233 -857.233], eps: 0.093})
Step:   23800, Reward: [-428.487 -428.487 -428.487] [67.644], Avg: [-441.151 -441.151 -441.151] (0.0920) ({r_i: None, r_t: [-884.595 -884.595 -884.595], eps: 0.092})
Step:   23900, Reward: [-419.672 -419.672 -419.672] [54.344], Avg: [-441.062 -441.062 -441.062] (0.0911) ({r_i: None, r_t: [-854.588 -854.588 -854.588], eps: 0.091})
Step:   24000, Reward: [-423.621 -423.621 -423.621] [71.227], Avg: [-440.990 -440.990 -440.990] (0.0902) ({r_i: None, r_t: [-867.500 -867.500 -867.500], eps: 0.09})
Step:   24100, Reward: [-466.850 -466.850 -466.850] [44.921], Avg: [-441.096 -441.096 -441.096] (0.0893) ({r_i: None, r_t: [-898.689 -898.689 -898.689], eps: 0.089})
Step:   24200, Reward: [-431.971 -431.971 -431.971] [74.878], Avg: [-441.059 -441.059 -441.059] (0.0884) ({r_i: None, r_t: [-907.480 -907.480 -907.480], eps: 0.088})
Step:   24300, Reward: [-434.988 -434.988 -434.988] [60.494], Avg: [-441.034 -441.034 -441.034] (0.0875) ({r_i: None, r_t: [-885.214 -885.214 -885.214], eps: 0.088})
Step:   24400, Reward: [-431.832 -431.832 -431.832] [72.908], Avg: [-440.996 -440.996 -440.996] (0.0866) ({r_i: None, r_t: [-873.086 -873.086 -873.086], eps: 0.087})
Step:   24500, Reward: [-458.642 -458.642 -458.642] [61.387], Avg: [-441.068 -441.068 -441.068] (0.0858) ({r_i: None, r_t: [-876.468 -876.468 -876.468], eps: 0.086})
Step:   24600, Reward: [-422.092 -422.092 -422.092] [51.949], Avg: [-440.991 -440.991 -440.991] (0.0849) ({r_i: None, r_t: [-867.142 -867.142 -867.142], eps: 0.085})
Step:   24700, Reward: [-428.487 -428.487 -428.487] [46.908], Avg: [-440.941 -440.941 -440.941] (0.0841) ({r_i: None, r_t: [-884.346 -884.346 -884.346], eps: 0.084})
Step:   24800, Reward: [-448.866 -448.866 -448.866] [50.339], Avg: [-440.973 -440.973 -440.973] (0.0832) ({r_i: None, r_t: [-883.237 -883.237 -883.237], eps: 0.083})
Step:   24900, Reward: [-434.346 -434.346 -434.346] [53.105], Avg: [-440.946 -440.946 -440.946] (0.0824) ({r_i: None, r_t: [-892.164 -892.164 -892.164], eps: 0.082})
Step:   25000, Reward: [-444.286 -444.286 -444.286] [51.436], Avg: [-440.960 -440.960 -440.960] (0.0816) ({r_i: None, r_t: [-904.674 -904.674 -904.674], eps: 0.082})
Step:   25100, Reward: [-442.963 -442.963 -442.963] [62.539], Avg: [-440.968 -440.968 -440.968] (0.0808) ({r_i: None, r_t: [-858.194 -858.194 -858.194], eps: 0.081})
Step:   25200, Reward: [-449.528 -449.528 -449.528] [76.519], Avg: [-441.001 -441.001 -441.001] (0.0800) ({r_i: None, r_t: [-901.303 -901.303 -901.303], eps: 0.08})
Step:   25300, Reward: [-412.209 -412.209 -412.209] [61.528], Avg: [-440.888 -440.888 -440.888] (0.0792) ({r_i: None, r_t: [-882.935 -882.935 -882.935], eps: 0.079})
Step:   25400, Reward: [-438.343 -438.343 -438.343] [68.619], Avg: [-440.878 -440.878 -440.878] (0.0784) ({r_i: None, r_t: [-860.031 -860.031 -860.031], eps: 0.078})
Step:   25500, Reward: [-407.280 -407.280 -407.280] [78.871], Avg: [-440.747 -440.747 -440.747] (0.0776) ({r_i: None, r_t: [-878.832 -878.832 -878.832], eps: 0.078})
Step:   25600, Reward: [-441.977 -441.977 -441.977] [61.738], Avg: [-440.752 -440.752 -440.752] (0.0768) ({r_i: None, r_t: [-846.822 -846.822 -846.822], eps: 0.077})
Step:   25700, Reward: [-405.948 -405.948 -405.948] [76.784], Avg: [-440.617 -440.617 -440.617] (0.0760) ({r_i: None, r_t: [-838.648 -838.648 -838.648], eps: 0.076})
Step:   25800, Reward: [-423.833 -423.833 -423.833] [77.828], Avg: [-440.552 -440.552 -440.552] (0.0753) ({r_i: None, r_t: [-873.627 -873.627 -873.627], eps: 0.075})
Step:   25900, Reward: [-412.758 -412.758 -412.758] [69.945], Avg: [-440.445 -440.445 -440.445] (0.0745) ({r_i: None, r_t: [-884.401 -884.401 -884.401], eps: 0.075})
Step:   26000, Reward: [-458.030 -458.030 -458.030] [70.921], Avg: [-440.512 -440.512 -440.512] (0.0738) ({r_i: None, r_t: [-881.577 -881.577 -881.577], eps: 0.074})
Step:   26100, Reward: [-433.945 -433.945 -433.945] [66.752], Avg: [-440.487 -440.487 -440.487] (0.0731) ({r_i: None, r_t: [-868.512 -868.512 -868.512], eps: 0.073})
Step:   26200, Reward: [-462.381 -462.381 -462.381] [69.652], Avg: [-440.571 -440.571 -440.571] (0.0723) ({r_i: None, r_t: [-886.400 -886.400 -886.400], eps: 0.072})
Step:   26300, Reward: [-459.900 -459.900 -459.900] [66.620], Avg: [-440.644 -440.644 -440.644] (0.0716) ({r_i: None, r_t: [-861.515 -861.515 -861.515], eps: 0.072})
Step:   26400, Reward: [-443.047 -443.047 -443.047] [71.295], Avg: [-440.653 -440.653 -440.653] (0.0709) ({r_i: None, r_t: [-863.538 -863.538 -863.538], eps: 0.071})
Step:   26500, Reward: [-416.329 -416.329 -416.329] [62.907], Avg: [-440.561 -440.561 -440.561] (0.0702) ({r_i: None, r_t: [-904.559 -904.559 -904.559], eps: 0.07})
Step:   26600, Reward: [-426.199 -426.199 -426.199] [53.971], Avg: [-440.508 -440.508 -440.508] (0.0695) ({r_i: None, r_t: [-882.081 -882.081 -882.081], eps: 0.069})
Step:   26700, Reward: [-465.052 -465.052 -465.052] [50.260], Avg: [-440.599 -440.599 -440.599] (0.0688) ({r_i: None, r_t: [-877.295 -877.295 -877.295], eps: 0.069})
Step:   26800, Reward: [-422.775 -422.775 -422.775] [37.400], Avg: [-440.533 -440.533 -440.533] (0.0681) ({r_i: None, r_t: [-873.166 -873.166 -873.166], eps: 0.068})
Step:   26900, Reward: [-448.621 -448.621 -448.621] [34.159], Avg: [-440.563 -440.563 -440.563] (0.0674) ({r_i: None, r_t: [-950.552 -950.552 -950.552], eps: 0.067})
Step:   27000, Reward: [-453.783 -453.783 -453.783] [39.866], Avg: [-440.612 -440.612 -440.612] (0.0668) ({r_i: None, r_t: [-880.874 -880.874 -880.874], eps: 0.067})
Step:   27100, Reward: [-440.882 -440.882 -440.882] [63.001], Avg: [-440.613 -440.613 -440.613] (0.0661) ({r_i: None, r_t: [-904.195 -904.195 -904.195], eps: 0.066})
Step:   27200, Reward: [-467.129 -467.129 -467.129] [53.474], Avg: [-440.710 -440.710 -440.710] (0.0654) ({r_i: None, r_t: [-903.726 -903.726 -903.726], eps: 0.065})
Step:   27300, Reward: [-457.730 -457.730 -457.730] [70.370], Avg: [-440.772 -440.772 -440.772] (0.0648) ({r_i: None, r_t: [-860.812 -860.812 -860.812], eps: 0.065})
Step:   27400, Reward: [-444.937 -444.937 -444.937] [69.094], Avg: [-440.787 -440.787 -440.787] (0.0641) ({r_i: None, r_t: [-915.942 -915.942 -915.942], eps: 0.064})
Step:   27500, Reward: [-422.053 -422.053 -422.053] [63.297], Avg: [-440.719 -440.719 -440.719] (0.0635) ({r_i: None, r_t: [-903.147 -903.147 -903.147], eps: 0.063})
Step:   27600, Reward: [-440.890 -440.890 -440.890] [74.805], Avg: [-440.720 -440.720 -440.720] (0.0629) ({r_i: None, r_t: [-899.086 -899.086 -899.086], eps: 0.063})
Step:   27700, Reward: [-467.028 -467.028 -467.028] [52.904], Avg: [-440.814 -440.814 -440.814] (0.0622) ({r_i: None, r_t: [-872.575 -872.575 -872.575], eps: 0.062})
Step:   27800, Reward: [-446.050 -446.050 -446.050] [59.361], Avg: [-440.833 -440.833 -440.833] (0.0616) ({r_i: None, r_t: [-930.315 -930.315 -930.315], eps: 0.062})
Step:   27900, Reward: [-439.804 -439.804 -439.804] [71.910], Avg: [-440.829 -440.829 -440.829] (0.0610) ({r_i: None, r_t: [-876.233 -876.233 -876.233], eps: 0.061})
Step:   28000, Reward: [-462.120 -462.120 -462.120] [65.755], Avg: [-440.905 -440.905 -440.905] (0.0604) ({r_i: None, r_t: [-870.196 -870.196 -870.196], eps: 0.06})
Step:   28100, Reward: [-469.307 -469.307 -469.307] [71.805], Avg: [-441.006 -441.006 -441.006] (0.0598) ({r_i: None, r_t: [-903.749 -903.749 -903.749], eps: 0.06})
Step:   28200, Reward: [-435.709 -435.709 -435.709] [57.198], Avg: [-440.987 -440.987 -440.987] (0.0592) ({r_i: None, r_t: [-902.478 -902.478 -902.478], eps: 0.059})
Step:   28300, Reward: [-460.159 -460.159 -460.159] [67.202], Avg: [-441.055 -441.055 -441.055] (0.0586) ({r_i: None, r_t: [-914.993 -914.993 -914.993], eps: 0.059})
Step:   28400, Reward: [-431.041 -431.041 -431.041] [66.425], Avg: [-441.020 -441.020 -441.020] (0.0580) ({r_i: None, r_t: [-869.151 -869.151 -869.151], eps: 0.058})
Step:   28500, Reward: [-448.375 -448.375 -448.375] [68.401], Avg: [-441.045 -441.045 -441.045] (0.0574) ({r_i: None, r_t: [-866.374 -866.374 -866.374], eps: 0.057})
Step:   28600, Reward: [-479.765 -479.765 -479.765] [72.687], Avg: [-441.180 -441.180 -441.180] (0.0569) ({r_i: None, r_t: [-868.875 -868.875 -868.875], eps: 0.057})
Step:   28700, Reward: [-466.378 -466.378 -466.378] [70.383], Avg: [-441.268 -441.268 -441.268] (0.0563) ({r_i: None, r_t: [-925.200 -925.200 -925.200], eps: 0.056})
Step:   28800, Reward: [-440.852 -440.852 -440.852] [78.536], Avg: [-441.266 -441.266 -441.266] (0.0557) ({r_i: None, r_t: [-895.502 -895.502 -895.502], eps: 0.056})
Step:   28900, Reward: [-460.210 -460.210 -460.210] [71.215], Avg: [-441.332 -441.332 -441.332] (0.0552) ({r_i: None, r_t: [-882.214 -882.214 -882.214], eps: 0.055})
Step:   29000, Reward: [-471.658 -471.658 -471.658] [70.642], Avg: [-441.436 -441.436 -441.436] (0.0546) ({r_i: None, r_t: [-911.077 -911.077 -911.077], eps: 0.055})
Step:   29100, Reward: [-473.901 -473.901 -473.901] [82.872], Avg: [-441.547 -441.547 -441.547] (0.0541) ({r_i: None, r_t: [-921.734 -921.734 -921.734], eps: 0.054})
Step:   29200, Reward: [-423.644 -423.644 -423.644] [79.607], Avg: [-441.486 -441.486 -441.486] (0.0535) ({r_i: None, r_t: [-878.923 -878.923 -878.923], eps: 0.054})
Step:   29300, Reward: [-475.647 -475.647 -475.647] [67.908], Avg: [-441.602 -441.602 -441.602] (0.0530) ({r_i: None, r_t: [-928.564 -928.564 -928.564], eps: 0.053})
Step:   29400, Reward: [-455.669 -455.669 -455.669] [74.194], Avg: [-441.650 -441.650 -441.650] (0.0525) ({r_i: None, r_t: [-932.404 -932.404 -932.404], eps: 0.052})
Step:   29500, Reward: [-420.736 -420.736 -420.736] [53.635], Avg: [-441.579 -441.579 -441.579] (0.0520) ({r_i: None, r_t: [-892.439 -892.439 -892.439], eps: 0.052})
Step:   29600, Reward: [-443.910 -443.910 -443.910] [76.482], Avg: [-441.587 -441.587 -441.587] (0.0514) ({r_i: None, r_t: [-886.953 -886.953 -886.953], eps: 0.051})
Step:   29700, Reward: [-484.510 -484.510 -484.510] [53.127], Avg: [-441.731 -441.731 -441.731] (0.0509) ({r_i: None, r_t: [-934.593 -934.593 -934.593], eps: 0.051})
Step:   29800, Reward: [-455.816 -455.816 -455.816] [69.264], Avg: [-441.778 -441.778 -441.778] (0.0504) ({r_i: None, r_t: [-910.676 -910.676 -910.676], eps: 0.05})
Step:   29900, Reward: [-437.274 -437.274 -437.274] [49.250], Avg: [-441.763 -441.763 -441.763] (0.0499) ({r_i: None, r_t: [-883.834 -883.834 -883.834], eps: 0.05})
Step:   30000, Reward: [-444.786 -444.786 -444.786] [59.159], Avg: [-441.773 -441.773 -441.773] (0.0494) ({r_i: None, r_t: [-917.047 -917.047 -917.047], eps: 0.049})
Step:   30100, Reward: [-456.964 -456.964 -456.964] [60.343], Avg: [-441.823 -441.823 -441.823] (0.0489) ({r_i: None, r_t: [-906.636 -906.636 -906.636], eps: 0.049})
Step:   30200, Reward: [-434.911 -434.911 -434.911] [56.532], Avg: [-441.801 -441.801 -441.801] (0.0484) ({r_i: None, r_t: [-930.507 -930.507 -930.507], eps: 0.048})
Step:   30300, Reward: [-446.924 -446.924 -446.924] [56.795], Avg: [-441.818 -441.818 -441.818] (0.0479) ({r_i: None, r_t: [-883.835 -883.835 -883.835], eps: 0.048})
Step:   30400, Reward: [-438.856 -438.856 -438.856] [78.511], Avg: [-441.808 -441.808 -441.808] (0.0475) ({r_i: None, r_t: [-922.517 -922.517 -922.517], eps: 0.047})
Step:   30500, Reward: [-476.935 -476.935 -476.935] [57.466], Avg: [-441.923 -441.923 -441.923] (0.0470) ({r_i: None, r_t: [-911.782 -911.782 -911.782], eps: 0.047})
Step:   30600, Reward: [-433.425 -433.425 -433.425] [73.588], Avg: [-441.895 -441.895 -441.895] (0.0465) ({r_i: None, r_t: [-885.272 -885.272 -885.272], eps: 0.047})
Step:   30700, Reward: [-473.026 -473.026 -473.026] [73.997], Avg: [-441.996 -441.996 -441.996] (0.0461) ({r_i: None, r_t: [-899.388 -899.388 -899.388], eps: 0.046})
Step:   30800, Reward: [-433.098 -433.098 -433.098] [51.322], Avg: [-441.967 -441.967 -441.967] (0.0456) ({r_i: None, r_t: [-872.103 -872.103 -872.103], eps: 0.046})
Step:   30900, Reward: [-431.013 -431.013 -431.013] [77.682], Avg: [-441.932 -441.932 -441.932] (0.0452) ({r_i: None, r_t: [-889.692 -889.692 -889.692], eps: 0.045})
Step:   31000, Reward: [-467.498 -467.498 -467.498] [71.258], Avg: [-442.014 -442.014 -442.014] (0.0447) ({r_i: None, r_t: [-911.106 -911.106 -911.106], eps: 0.045})
Step:   31100, Reward: [-430.033 -430.033 -430.033] [49.149], Avg: [-441.976 -441.976 -441.976] (0.0443) ({r_i: None, r_t: [-882.777 -882.777 -882.777], eps: 0.044})
Step:   31200, Reward: [-439.803 -439.803 -439.803] [60.053], Avg: [-441.969 -441.969 -441.969] (0.0438) ({r_i: None, r_t: [-871.909 -871.909 -871.909], eps: 0.044})
Step:   31300, Reward: [-431.295 -431.295 -431.295] [58.134], Avg: [-441.935 -441.935 -441.935] (0.0434) ({r_i: None, r_t: [-903.207 -903.207 -903.207], eps: 0.043})
Step:   31400, Reward: [-451.400 -451.400 -451.400] [63.888], Avg: [-441.965 -441.965 -441.965] (0.0429) ({r_i: None, r_t: [-878.195 -878.195 -878.195], eps: 0.043})
Step:   31500, Reward: [-432.054 -432.054 -432.054] [60.202], Avg: [-441.933 -441.933 -441.933] (0.0425) ({r_i: None, r_t: [-925.132 -925.132 -925.132], eps: 0.043})
Step:   31600, Reward: [-446.297 -446.297 -446.297] [83.508], Avg: [-441.947 -441.947 -441.947] (0.0421) ({r_i: None, r_t: [-875.087 -875.087 -875.087], eps: 0.042})
Step:   31700, Reward: [-427.132 -427.132 -427.132] [79.937], Avg: [-441.901 -441.901 -441.901] (0.0417) ({r_i: None, r_t: [-860.357 -860.357 -860.357], eps: 0.042})
Step:   31800, Reward: [-471.903 -471.903 -471.903] [63.912], Avg: [-441.995 -441.995 -441.995] (0.0413) ({r_i: None, r_t: [-920.301 -920.301 -920.301], eps: 0.041})
Step:   31900, Reward: [-475.421 -475.421 -475.421] [54.687], Avg: [-442.099 -442.099 -442.099] (0.0408) ({r_i: None, r_t: [-914.076 -914.076 -914.076], eps: 0.041})
Step:   32000, Reward: [-416.953 -416.953 -416.953] [69.786], Avg: [-442.021 -442.021 -442.021] (0.0404) ({r_i: None, r_t: [-948.052 -948.052 -948.052], eps: 0.04})
Step:   32100, Reward: [-437.364 -437.364 -437.364] [71.234], Avg: [-442.006 -442.006 -442.006] (0.0400) ({r_i: None, r_t: [-937.536 -937.536 -937.536], eps: 0.04})
Step:   32200, Reward: [-474.592 -474.592 -474.592] [51.057], Avg: [-442.107 -442.107 -442.107] (0.0396) ({r_i: None, r_t: [-919.093 -919.093 -919.093], eps: 0.04})
Step:   32300, Reward: [-473.382 -473.382 -473.382] [72.223], Avg: [-442.204 -442.204 -442.204] (0.0392) ({r_i: None, r_t: [-924.292 -924.292 -924.292], eps: 0.039})
Step:   32400, Reward: [-468.804 -468.804 -468.804] [52.942], Avg: [-442.286 -442.286 -442.286] (0.0388) ({r_i: None, r_t: [-927.726 -927.726 -927.726], eps: 0.039})
Step:   32500, Reward: [-482.347 -482.347 -482.347] [87.853], Avg: [-442.408 -442.408 -442.408] (0.0385) ({r_i: None, r_t: [-885.341 -885.341 -885.341], eps: 0.038})
Step:   32600, Reward: [-470.832 -470.832 -470.832] [61.421], Avg: [-442.495 -442.495 -442.495] (0.0381) ({r_i: None, r_t: [-873.149 -873.149 -873.149], eps: 0.038})
Step:   32700, Reward: [-433.894 -433.894 -433.894] [57.020], Avg: [-442.469 -442.469 -442.469] (0.0377) ({r_i: None, r_t: [-891.700 -891.700 -891.700], eps: 0.038})
Step:   32800, Reward: [-458.285 -458.285 -458.285] [79.371], Avg: [-442.517 -442.517 -442.517] (0.0373) ({r_i: None, r_t: [-962.158 -962.158 -962.158], eps: 0.037})
Step:   32900, Reward: [-475.907 -475.907 -475.907] [59.746], Avg: [-442.618 -442.618 -442.618] (0.0369) ({r_i: None, r_t: [-896.985 -896.985 -896.985], eps: 0.037})
Step:   33000, Reward: [-482.321 -482.321 -482.321] [54.727], Avg: [-442.738 -442.738 -442.738] (0.0366) ({r_i: None, r_t: [-911.579 -911.579 -911.579], eps: 0.037})
Step:   33100, Reward: [-466.702 -466.702 -466.702] [81.469], Avg: [-442.811 -442.811 -442.811] (0.0362) ({r_i: None, r_t: [-959.147 -959.147 -959.147], eps: 0.036})
Step:   33200, Reward: [-421.777 -421.777 -421.777] [71.725], Avg: [-442.747 -442.747 -442.747] (0.0359) ({r_i: None, r_t: [-942.154 -942.154 -942.154], eps: 0.036})
Step:   33300, Reward: [-455.667 -455.667 -455.667] [74.899], Avg: [-442.786 -442.786 -442.786] (0.0355) ({r_i: None, r_t: [-915.930 -915.930 -915.930], eps: 0.035})
Step:   33400, Reward: [-444.784 -444.784 -444.784] [63.644], Avg: [-442.792 -442.792 -442.792] (0.0351) ({r_i: None, r_t: [-884.828 -884.828 -884.828], eps: 0.035})
Step:   33500, Reward: [-437.123 -437.123 -437.123] [44.969], Avg: [-442.775 -442.775 -442.775] (0.0348) ({r_i: None, r_t: [-955.001 -955.001 -955.001], eps: 0.035})
Step:   33600, Reward: [-443.205 -443.205 -443.205] [60.346], Avg: [-442.776 -442.776 -442.776] (0.0344) ({r_i: None, r_t: [-915.155 -915.155 -915.155], eps: 0.034})
Step:   33700, Reward: [-440.990 -440.990 -440.990] [63.812], Avg: [-442.771 -442.771 -442.771] (0.0341) ({r_i: None, r_t: [-915.122 -915.122 -915.122], eps: 0.034})
Step:   33800, Reward: [-477.291 -477.291 -477.291] [51.932], Avg: [-442.873 -442.873 -442.873] (0.0338) ({r_i: None, r_t: [-927.777 -927.777 -927.777], eps: 0.034})
Step:   33900, Reward: [-450.273 -450.273 -450.273] [51.404], Avg: [-442.895 -442.895 -442.895] (0.0334) ({r_i: None, r_t: [-902.776 -902.776 -902.776], eps: 0.033})
Step:   34000, Reward: [-484.140 -484.140 -484.140] [60.205], Avg: [-443.016 -443.016 -443.016] (0.0331) ({r_i: None, r_t: [-937.328 -937.328 -937.328], eps: 0.033})
Step:   34100, Reward: [-464.797 -464.797 -464.797] [69.509], Avg: [-443.079 -443.079 -443.079] (0.0328) ({r_i: None, r_t: [-919.060 -919.060 -919.060], eps: 0.033})
Step:   34200, Reward: [-434.128 -434.128 -434.128] [63.300], Avg: [-443.053 -443.053 -443.053] (0.0324) ({r_i: None, r_t: [-890.737 -890.737 -890.737], eps: 0.032})
Step:   34300, Reward: [-474.309 -474.309 -474.309] [72.001], Avg: [-443.144 -443.144 -443.144] (0.0321) ({r_i: None, r_t: [-870.138 -870.138 -870.138], eps: 0.032})
Step:   34400, Reward: [-429.047 -429.047 -429.047] [75.669], Avg: [-443.103 -443.103 -443.103] (0.0318) ({r_i: None, r_t: [-863.180 -863.180 -863.180], eps: 0.032})
Step:   34500, Reward: [-455.105 -455.105 -455.105] [51.830], Avg: [-443.138 -443.138 -443.138] (0.0315) ({r_i: None, r_t: [-937.137 -937.137 -937.137], eps: 0.031})
Step:   34600, Reward: [-426.553 -426.553 -426.553] [75.142], Avg: [-443.090 -443.090 -443.090] (0.0312) ({r_i: None, r_t: [-932.920 -932.920 -932.920], eps: 0.031})
Step:   34700, Reward: [-465.730 -465.730 -465.730] [49.272], Avg: [-443.155 -443.155 -443.155] (0.0308) ({r_i: None, r_t: [-933.868 -933.868 -933.868], eps: 0.031})
Step:   34800, Reward: [-460.074 -460.074 -460.074] [58.820], Avg: [-443.204 -443.204 -443.204] (0.0305) ({r_i: None, r_t: [-887.547 -887.547 -887.547], eps: 0.031})
Step:   34900, Reward: [-430.428 -430.428 -430.428] [43.785], Avg: [-443.167 -443.167 -443.167] (0.0302) ({r_i: None, r_t: [-862.274 -862.274 -862.274], eps: 0.03})
Step:   35000, Reward: [-456.081 -456.081 -456.081] [66.222], Avg: [-443.204 -443.204 -443.204] (0.0299) ({r_i: None, r_t: [-928.201 -928.201 -928.201], eps: 0.03})
Step:   35100, Reward: [-415.377 -415.377 -415.377] [55.535], Avg: [-443.125 -443.125 -443.125] (0.0296) ({r_i: None, r_t: [-931.061 -931.061 -931.061], eps: 0.03})
Step:   35200, Reward: [-479.272 -479.272 -479.272] [56.766], Avg: [-443.227 -443.227 -443.227] (0.0293) ({r_i: None, r_t: [-873.010 -873.010 -873.010], eps: 0.029})
Step:   35300, Reward: [-452.489 -452.489 -452.489] [62.665], Avg: [-443.253 -443.253 -443.253] (0.0290) ({r_i: None, r_t: [-908.497 -908.497 -908.497], eps: 0.029})
Step:   35400, Reward: [-492.328 -492.328 -492.328] [47.613], Avg: [-443.392 -443.392 -443.392] (0.0288) ({r_i: None, r_t: [-900.999 -900.999 -900.999], eps: 0.029})
Step:   35500, Reward: [-441.128 -441.128 -441.128] [76.603], Avg: [-443.385 -443.385 -443.385] (0.0285) ({r_i: None, r_t: [-925.002 -925.002 -925.002], eps: 0.028})
Step:   35600, Reward: [-469.487 -469.487 -469.487] [64.876], Avg: [-443.458 -443.458 -443.458] (0.0282) ({r_i: None, r_t: [-920.173 -920.173 -920.173], eps: 0.028})
Step:   35700, Reward: [-476.735 -476.735 -476.735] [63.231], Avg: [-443.551 -443.551 -443.551] (0.0279) ({r_i: None, r_t: [-924.802 -924.802 -924.802], eps: 0.028})
Step:   35800, Reward: [-474.731 -474.731 -474.731] [65.629], Avg: [-443.638 -443.638 -443.638] (0.0276) ({r_i: None, r_t: [-940.770 -940.770 -940.770], eps: 0.028})
Step:   35900, Reward: [-451.548 -451.548 -451.548] [41.578], Avg: [-443.660 -443.660 -443.660] (0.0274) ({r_i: None, r_t: [-903.830 -903.830 -903.830], eps: 0.027})
Step:   36000, Reward: [-477.934 -477.934 -477.934] [63.609], Avg: [-443.755 -443.755 -443.755] (0.0271) ({r_i: None, r_t: [-950.914 -950.914 -950.914], eps: 0.027})
Step:   36100, Reward: [-469.032 -469.032 -469.032] [78.647], Avg: [-443.825 -443.825 -443.825] (0.0268) ({r_i: None, r_t: [-984.561 -984.561 -984.561], eps: 0.027})
Step:   36200, Reward: [-468.727 -468.727 -468.727] [53.112], Avg: [-443.894 -443.894 -443.894] (0.0265) ({r_i: None, r_t: [-947.497 -947.497 -947.497], eps: 0.027})
Step:   36300, Reward: [-474.628 -474.628 -474.628] [38.918], Avg: [-443.978 -443.978 -443.978] (0.0263) ({r_i: None, r_t: [-922.958 -922.958 -922.958], eps: 0.026})
Step:   36400, Reward: [-467.095 -467.095 -467.095] [55.389], Avg: [-444.041 -444.041 -444.041] (0.0260) ({r_i: None, r_t: [-893.026 -893.026 -893.026], eps: 0.026})
Step:   36500, Reward: [-468.631 -468.631 -468.631] [57.152], Avg: [-444.109 -444.109 -444.109] (0.0258) ({r_i: None, r_t: [-932.558 -932.558 -932.558], eps: 0.026})
Step:   36600, Reward: [-458.729 -458.729 -458.729] [62.547], Avg: [-444.148 -444.148 -444.148] (0.0255) ({r_i: None, r_t: [-923.316 -923.316 -923.316], eps: 0.025})
Step:   36700, Reward: [-436.172 -436.172 -436.172] [56.709], Avg: [-444.127 -444.127 -444.127] (0.0252) ({r_i: None, r_t: [-919.757 -919.757 -919.757], eps: 0.025})
Step:   36800, Reward: [-437.496 -437.496 -437.496] [80.265], Avg: [-444.109 -444.109 -444.109] (0.0250) ({r_i: None, r_t: [-939.188 -939.188 -939.188], eps: 0.025})
Step:   36900, Reward: [-466.714 -466.714 -466.714] [77.733], Avg: [-444.170 -444.170 -444.170] (0.0247) ({r_i: None, r_t: [-947.279 -947.279 -947.279], eps: 0.025})
Step:   37000, Reward: [-448.318 -448.318 -448.318] [68.352], Avg: [-444.181 -444.181 -444.181] (0.0245) ({r_i: None, r_t: [-883.140 -883.140 -883.140], eps: 0.024})
Step:   37100, Reward: [-470.923 -470.923 -470.923] [74.596], Avg: [-444.253 -444.253 -444.253] (0.0243) ({r_i: None, r_t: [-909.801 -909.801 -909.801], eps: 0.024})
Step:   37200, Reward: [-436.551 -436.551 -436.551] [63.960], Avg: [-444.232 -444.232 -444.232] (0.0240) ({r_i: None, r_t: [-932.960 -932.960 -932.960], eps: 0.024})
Step:   37300, Reward: [-458.205 -458.205 -458.205] [65.491], Avg: [-444.270 -444.270 -444.270] (0.0238) ({r_i: None, r_t: [-873.596 -873.596 -873.596], eps: 0.024})
Step:   37400, Reward: [-429.254 -429.254 -429.254] [57.306], Avg: [-444.230 -444.230 -444.230] (0.0235) ({r_i: None, r_t: [-917.664 -917.664 -917.664], eps: 0.024})
Step:   37500, Reward: [-472.456 -472.456 -472.456] [56.027], Avg: [-444.305 -444.305 -444.305] (0.0233) ({r_i: None, r_t: [-943.227 -943.227 -943.227], eps: 0.023})
Step:   37600, Reward: [-462.526 -462.526 -462.526] [77.011], Avg: [-444.353 -444.353 -444.353] (0.0231) ({r_i: None, r_t: [-862.607 -862.607 -862.607], eps: 0.023})
Step:   37700, Reward: [-468.882 -468.882 -468.882] [68.367], Avg: [-444.418 -444.418 -444.418] (0.0228) ({r_i: None, r_t: [-890.708 -890.708 -890.708], eps: 0.023})
Step:   37800, Reward: [-448.029 -448.029 -448.029] [65.804], Avg: [-444.427 -444.427 -444.427] (0.0226) ({r_i: None, r_t: [-892.723 -892.723 -892.723], eps: 0.023})
Step:   37900, Reward: [-462.241 -462.241 -462.241] [79.275], Avg: [-444.474 -444.474 -444.474] (0.0224) ({r_i: None, r_t: [-920.587 -920.587 -920.587], eps: 0.022})
Step:   38000, Reward: [-460.091 -460.091 -460.091] [37.651], Avg: [-444.515 -444.515 -444.515] (0.0222) ({r_i: None, r_t: [-874.912 -874.912 -874.912], eps: 0.022})
Step:   38100, Reward: [-423.856 -423.856 -423.856] [59.889], Avg: [-444.461 -444.461 -444.461] (0.0219) ({r_i: None, r_t: [-910.647 -910.647 -910.647], eps: 0.022})
Step:   38200, Reward: [-464.540 -464.540 -464.540] [73.831], Avg: [-444.514 -444.514 -444.514] (0.0217) ({r_i: None, r_t: [-913.983 -913.983 -913.983], eps: 0.022})
Step:   38300, Reward: [-461.980 -461.980 -461.980] [73.538], Avg: [-444.559 -444.559 -444.559] (0.0215) ({r_i: None, r_t: [-910.881 -910.881 -910.881], eps: 0.022})
Step:   38400, Reward: [-443.619 -443.619 -443.619] [71.787], Avg: [-444.557 -444.557 -444.557] (0.0213) ({r_i: None, r_t: [-879.817 -879.817 -879.817], eps: 0.021})
Step:   38500, Reward: [-448.555 -448.555 -448.555] [57.723], Avg: [-444.567 -444.567 -444.567] (0.0211) ({r_i: None, r_t: [-931.488 -931.488 -931.488], eps: 0.021})
Step:   38600, Reward: [-450.038 -450.038 -450.038] [60.119], Avg: [-444.581 -444.581 -444.581] (0.0209) ({r_i: None, r_t: [-986.454 -986.454 -986.454], eps: 0.021})
Step:   38700, Reward: [-439.483 -439.483 -439.483] [69.933], Avg: [-444.568 -444.568 -444.568] (0.0207) ({r_i: None, r_t: [-925.601 -925.601 -925.601], eps: 0.021})
Step:   38800, Reward: [-467.841 -467.841 -467.841] [83.722], Avg: [-444.628 -444.628 -444.628] (0.0205) ({r_i: None, r_t: [-949.479 -949.479 -949.479], eps: 0.02})
Step:   38900, Reward: [-426.622 -426.622 -426.622] [67.668], Avg: [-444.582 -444.582 -444.582] (0.0202) ({r_i: None, r_t: [-888.344 -888.344 -888.344], eps: 0.02})
Step:   39000, Reward: [-448.679 -448.679 -448.679] [48.876], Avg: [-444.592 -444.592 -444.592] (0.0200) ({r_i: None, r_t: [-908.231 -908.231 -908.231], eps: 0.02})
Step:   39100, Reward: [-433.174 -433.174 -433.174] [68.586], Avg: [-444.563 -444.563 -444.563] (0.0198) ({r_i: None, r_t: [-871.109 -871.109 -871.109], eps: 0.02})
Step:   39200, Reward: [-450.991 -450.991 -450.991] [64.724], Avg: [-444.579 -444.579 -444.579] (0.0196) ({r_i: None, r_t: [-923.420 -923.420 -923.420], eps: 0.02})
Step:   39300, Reward: [-438.732 -438.732 -438.732] [54.597], Avg: [-444.565 -444.565 -444.565] (0.0195) ({r_i: None, r_t: [-916.636 -916.636 -916.636], eps: 0.019})
Step:   39400, Reward: [-446.803 -446.803 -446.803] [65.122], Avg: [-444.570 -444.570 -444.570] (0.0193) ({r_i: None, r_t: [-916.778 -916.778 -916.778], eps: 0.019})
Step:   39500, Reward: [-455.724 -455.724 -455.724] [58.140], Avg: [-444.598 -444.598 -444.598] (0.0191) ({r_i: None, r_t: [-935.190 -935.190 -935.190], eps: 0.019})
Step:   39600, Reward: [-471.790 -471.790 -471.790] [68.258], Avg: [-444.667 -444.667 -444.667] (0.0189) ({r_i: None, r_t: [-936.242 -936.242 -936.242], eps: 0.019})
Step:   39700, Reward: [-463.446 -463.446 -463.446] [79.466], Avg: [-444.714 -444.714 -444.714] (0.0187) ({r_i: None, r_t: [-866.608 -866.608 -866.608], eps: 0.019})
Step:   39800, Reward: [-466.839 -466.839 -466.839] [63.256], Avg: [-444.770 -444.770 -444.770] (0.0185) ({r_i: None, r_t: [-917.376 -917.376 -917.376], eps: 0.019})
Step:   39900, Reward: [-451.676 -451.676 -451.676] [67.130], Avg: [-444.787 -444.787 -444.787] (0.0183) ({r_i: None, r_t: [-948.659 -948.659 -948.659], eps: 0.018})
Step:   40000, Reward: [-473.707 -473.707 -473.707] [53.589], Avg: [-444.859 -444.859 -444.859] (0.0181) ({r_i: None, r_t: [-924.487 -924.487 -924.487], eps: 0.018})
Step:   40100, Reward: [-463.362 -463.362 -463.362] [69.021], Avg: [-444.905 -444.905 -444.905] (0.0180) ({r_i: None, r_t: [-924.494 -924.494 -924.494], eps: 0.018})
Step:   40200, Reward: [-487.119 -487.119 -487.119] [66.520], Avg: [-445.010 -445.010 -445.010] (0.0178) ({r_i: None, r_t: [-973.136 -973.136 -973.136], eps: 0.018})
Step:   40300, Reward: [-436.033 -436.033 -436.033] [60.149], Avg: [-444.987 -444.987 -444.987] (0.0176) ({r_i: None, r_t: [-898.663 -898.663 -898.663], eps: 0.018})
Step:   40400, Reward: [-425.328 -425.328 -425.328] [48.086], Avg: [-444.939 -444.939 -444.939] (0.0174) ({r_i: None, r_t: [-910.885 -910.885 -910.885], eps: 0.017})
Step:   40500, Reward: [-457.780 -457.780 -457.780] [71.054], Avg: [-444.971 -444.971 -444.971] (0.0172) ({r_i: None, r_t: [-935.880 -935.880 -935.880], eps: 0.017})
Step:   40600, Reward: [-453.012 -453.012 -453.012] [67.279], Avg: [-444.990 -444.990 -444.990] (0.0171) ({r_i: None, r_t: [-927.822 -927.822 -927.822], eps: 0.017})
Step:   40700, Reward: [-452.897 -452.897 -452.897] [67.650], Avg: [-445.010 -445.010 -445.010] (0.0169) ({r_i: None, r_t: [-899.034 -899.034 -899.034], eps: 0.017})
Step:   40800, Reward: [-484.999 -484.999 -484.999] [74.332], Avg: [-445.107 -445.107 -445.107] (0.0167) ({r_i: None, r_t: [-933.308 -933.308 -933.308], eps: 0.017})
Step:   40900, Reward: [-416.742 -416.742 -416.742] [69.602], Avg: [-445.038 -445.038 -445.038] (0.0166) ({r_i: None, r_t: [-934.216 -934.216 -934.216], eps: 0.017})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.critic_training_steps = 0
		self.last_target_update_step = 0
		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)
		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			# chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters())
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-520.761 -520.761 -520.761] [72.731], Avg: [-520.761 -520.761 -520.761] (1.0000) ({r_i: None, r_t: [-7.814 -7.814 -7.814], eps: 1.0})
Step:   41000, Reward: [-455.038 -455.038 -455.038] [77.062], Avg: [-445.063 -445.063 -445.063] (0.0164) ({r_i: None, r_t: [-946.353 -946.353 -946.353], eps: 0.016})
Step:   41100, Reward: [-475.086 -475.086 -475.086] [51.615], Avg: [-445.135 -445.135 -445.135] (0.0162) ({r_i: None, r_t: [-915.835 -915.835 -915.835], eps: 0.016})
Step:   41200, Reward: [-451.056 -451.056 -451.056] [75.996], Avg: [-445.150 -445.150 -445.150] (0.0161) ({r_i: None, r_t: [-921.798 -921.798 -921.798], eps: 0.016})
Step:   41300, Reward: [-470.088 -470.088 -470.088] [59.528], Avg: [-445.210 -445.210 -445.210] (0.0159) ({r_i: None, r_t: [-924.946 -924.946 -924.946], eps: 0.016})
Step:   41400, Reward: [-436.972 -436.972 -436.972] [67.154], Avg: [-445.190 -445.190 -445.190] (0.0158) ({r_i: None, r_t: [-948.118 -948.118 -948.118], eps: 0.016})
Step:   41500, Reward: [-464.475 -464.475 -464.475] [68.490], Avg: [-445.237 -445.237 -445.237] (0.0156) ({r_i: None, r_t: [-951.580 -951.580 -951.580], eps: 0.016})
Step:   41600, Reward: [-434.907 -434.907 -434.907] [55.886], Avg: [-445.212 -445.212 -445.212] (0.0154) ({r_i: None, r_t: [-935.418 -935.418 -935.418], eps: 0.015})
Step:   41700, Reward: [-461.162 -461.162 -461.162] [62.028], Avg: [-445.250 -445.250 -445.250] (0.0153) ({r_i: None, r_t: [-908.539 -908.539 -908.539], eps: 0.015})
Step:   41800, Reward: [-483.127 -483.127 -483.127] [78.475], Avg: [-445.340 -445.340 -445.340] (0.0151) ({r_i: None, r_t: [-917.311 -917.311 -917.311], eps: 0.015})
Step:   41900, Reward: [-471.217 -471.217 -471.217] [63.358], Avg: [-445.402 -445.402 -445.402] (0.0150) ({r_i: None, r_t: [-973.062 -973.062 -973.062], eps: 0.015})
Step:   42000, Reward: [-445.504 -445.504 -445.504] [49.644], Avg: [-445.402 -445.402 -445.402] (0.0148) ({r_i: None, r_t: [-913.033 -913.033 -913.033], eps: 0.015})
Step:   42100, Reward: [-474.872 -474.872 -474.872] [56.455], Avg: [-445.472 -445.472 -445.472] (0.0147) ({r_i: None, r_t: [-899.626 -899.626 -899.626], eps: 0.015})
Step:   42200, Reward: [-442.383 -442.383 -442.383] [80.679], Avg: [-445.465 -445.465 -445.465] (0.0145) ({r_i: None, r_t: [-855.708 -855.708 -855.708], eps: 0.015})
Step:   42300, Reward: [-448.983 -448.983 -448.983] [60.408], Avg: [-445.473 -445.473 -445.473] (0.0144) ({r_i: None, r_t: [-935.612 -935.612 -935.612], eps: 0.014})
Step:   42400, Reward: [-470.488 -470.488 -470.488] [49.737], Avg: [-445.532 -445.532 -445.532] (0.0143) ({r_i: None, r_t: [-926.113 -926.113 -926.113], eps: 0.014})
Step:   42500, Reward: [-433.341 -433.341 -433.341] [47.709], Avg: [-445.503 -445.503 -445.503] (0.0141) ({r_i: None, r_t: [-932.044 -932.044 -932.044], eps: 0.014})
Step:   42600, Reward: [-473.155 -473.155 -473.155] [58.399], Avg: [-445.568 -445.568 -445.568] (0.0140) ({r_i: None, r_t: [-925.093 -925.093 -925.093], eps: 0.014})
Step:   42700, Reward: [-462.645 -462.645 -462.645] [79.604], Avg: [-445.608 -445.608 -445.608] (0.0138) ({r_i: None, r_t: [-927.374 -927.374 -927.374], eps: 0.014})
Step:   42800, Reward: [-473.970 -473.970 -473.970] [76.448], Avg: [-445.674 -445.674 -445.674] (0.0137) ({r_i: None, r_t: [-916.796 -916.796 -916.796], eps: 0.014})
Step:   42900, Reward: [-473.910 -473.910 -473.910] [64.753], Avg: [-445.740 -445.740 -445.740] (0.0136) ({r_i: None, r_t: [-926.100 -926.100 -926.100], eps: 0.014})
Step:   43000, Reward: [-443.431 -443.431 -443.431] [60.289], Avg: [-445.734 -445.734 -445.734] (0.0134) ({r_i: None, r_t: [-970.733 -970.733 -970.733], eps: 0.013})
Step:   43100, Reward: [-472.072 -472.072 -472.072] [71.852], Avg: [-445.795 -445.795 -445.795] (0.0133) ({r_i: None, r_t: [-924.015 -924.015 -924.015], eps: 0.013})
Step:   43200, Reward: [-465.133 -465.133 -465.133] [61.757], Avg: [-445.840 -445.840 -445.840] (0.0132) ({r_i: None, r_t: [-977.373 -977.373 -977.373], eps: 0.013})
Step:   43300, Reward: [-492.446 -492.446 -492.446] [70.558], Avg: [-445.947 -445.947 -445.947] (0.0130) ({r_i: None, r_t: [-953.729 -953.729 -953.729], eps: 0.013})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.critic_training_steps = 0
		self.last_target_update_step = 0
		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)
		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			# chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters(), retain=t>0)
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-515.307 -515.307 -515.307] [131.572], Avg: [-515.307 -515.307 -515.307] (1.0000) ({r_i: None, r_t: [-8.511 -8.511 -8.511], eps: 1.0})
Step:   43400, Reward: [-465.289 -465.289 -465.289] [60.307], Avg: [-445.992 -445.992 -445.992] (0.0129) ({r_i: None, r_t: [-908.588 -908.588 -908.588], eps: 0.013})
Step:   43500, Reward: [-459.301 -459.301 -459.301] [81.998], Avg: [-446.022 -446.022 -446.022] (0.0128) ({r_i: None, r_t: [-939.985 -939.985 -939.985], eps: 0.013})
Step:     100, Reward: [-492.662 -492.662 -492.662] [65.323], Avg: [-503.984 -503.984 -503.984] (0.9900) ({r_i: None, r_t: [-975.952 -975.952 -975.952], eps: 0.99})
Step:   43600, Reward: [-493.030 -493.030 -493.030] [75.782], Avg: [-446.130 -446.130 -446.130] (0.0126) ({r_i: None, r_t: [-926.287 -926.287 -926.287], eps: 0.013})
Step:   43700, Reward: [-480.994 -480.994 -480.994] [68.360], Avg: [-446.210 -446.210 -446.210] (0.0125) ({r_i: None, r_t: [-927.765 -927.765 -927.765], eps: 0.013})
Step:     200, Reward: [-502.697 -502.697 -502.697] [73.371], Avg: [-503.555 -503.555 -503.555] (0.9801) ({r_i: None, r_t: [-963.501 -963.501 -963.501], eps: 0.98})
Step:   43800, Reward: [-462.358 -462.358 -462.358] [78.024], Avg: [-446.246 -446.246 -446.246] (0.0124) ({r_i: None, r_t: [-949.478 -949.478 -949.478], eps: 0.012})
Step:     300, Reward: [-507.731 -507.731 -507.731] [106.755], Avg: [-504.599 -504.599 -504.599] (0.9704) ({r_i: None, r_t: [-959.143 -959.143 -959.143], eps: 0.97})
Step:   43900, Reward: [-449.237 -449.237 -449.237] [46.821], Avg: [-446.253 -446.253 -446.253] (0.0123) ({r_i: None, r_t: [-961.973 -961.973 -961.973], eps: 0.012})
Step:   44000, Reward: [-461.298 -461.298 -461.298] [88.102], Avg: [-446.287 -446.287 -446.287] (0.0121) ({r_i: None, r_t: [-935.343 -935.343 -935.343], eps: 0.012})
Step:   44100, Reward: [-469.625 -469.625 -469.625] [75.064], Avg: [-446.340 -446.340 -446.340] (0.0120) ({r_i: None, r_t: [-944.821 -944.821 -944.821], eps: 0.012})
Step:     400, Reward: [-532.412 -532.412 -532.412] [95.560], Avg: [-510.162 -510.162 -510.162] (0.9607) ({r_i: None, r_t: [-996.774 -996.774 -996.774], eps: 0.961})
Step:   44200, Reward: [-453.586 -453.586 -453.586] [74.188], Avg: [-446.356 -446.356 -446.356] (0.0119) ({r_i: None, r_t: [-971.362 -971.362 -971.362], eps: 0.012})
Step:   44300, Reward: [-472.738 -472.738 -472.738] [85.257], Avg: [-446.416 -446.416 -446.416] (0.0118) ({r_i: None, r_t: [-930.600 -930.600 -930.600], eps: 0.012})
Step:   44400, Reward: [-481.071 -481.071 -481.071] [75.575], Avg: [-446.494 -446.494 -446.494] (0.0117) ({r_i: None, r_t: [-945.056 -945.056 -945.056], eps: 0.012})
Step:   44500, Reward: [-512.521 -512.521 -512.521] [74.012], Avg: [-446.642 -446.642 -446.642] (0.0115) ({r_i: None, r_t: [-949.671 -949.671 -949.671], eps: 0.012})
Step:   44600, Reward: [-480.276 -480.276 -480.276] [58.290], Avg: [-446.717 -446.717 -446.717] (0.0114) ({r_i: None, r_t: [-954.045 -954.045 -954.045], eps: 0.011})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		super().__init__(gpu=gpu, name="coma")
		self.n_agents = len(state_size)
		self.critic_training_steps = 0
		self.last_target_update_step = 0
		self.actor_local = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_local = COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0]).to(self.device)
		self.critic_target = copy.deepcopy(self.critic_local)
		self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=LEARNING_RATE)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			# chosen_actions = torch.distributions.Categorical(agent_outputs).sample().long()
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def forward(self, actor_inputs, eps):
		agent_outs = self.actor_local(actor_inputs)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		agent_outs = ((1 - eps) * agent_outs + torch.ones_like(agent_outs).to(self.device) * eps/agent_outs.size(-1))
		return agent_outs

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).sum()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters(), retain=t>0)
			# self.critic_optimiser.zero_grad()
			# loss.backward()
			# torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
			# self.critic_optimiser.step()
			self.critic_training_steps += 1
		
		# q_vals = self._train_critic(actions, critic_inputs, actor_inputs, rewards, dones)
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).sum()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		# self.actor_optimiser.zero_grad()
		# coma_loss.backward()
		# torch.nn.utils.clip_grad_norm_(self.actor.parameters(), GRAD_NORM)
		# self.actor_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self.critic_target.load_state_dict(self.critic_local.state_dict())
			self.last_target_update_step = self.critic_training_steps

	# def _train_critic(self, actions, critic_inputs, actor_inputs, rewards, dones):
	# 	target_q_vals = self.critic_target(critic_inputs)[:, :]
	# 	targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
	# 	targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
	# 	targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
	# 	q_vals = torch.zeros_like(target_q_vals)
	# 	for t in reversed(range(rewards.size(1))):
	# 		q_t = self.critic_local(critic_inputs[:,t], t)
	# 		q_vals[:, t] = q_t
	# 		q_taken = torch.gather(q_t, dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
	# 		targets_t = targets[:, t]
	# 		td_error = (q_taken - targets_t.detach())
	# 		loss = (td_error ** 2).sum()
	# 		self.critic_optimiser.zero_grad()
	# 		loss.backward()
	# 		torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), GRAD_NORM)
	# 		self.critic_optimiser.step()
	# 		self.critic_training_steps += 1
	# 	return q_vals

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
		for t in range(ret.shape[1] - 2, -1,  -1):
			ret[:, t] = td_lambda * gamma * ret[:, t + 1] + (rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
		return ret[:, 0:-1]

	def save_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="checkpoint"):
		pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1, 1, 1, self.n_agents, 1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2], 1, -1).repeat(1, 1, self.n_agents, 1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2], -1, -1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-471.335 -471.335 -471.335] [64.265], Avg: [-471.335 -471.335 -471.335] (1.0000) ({r_i: None, r_t: [-8.340 -8.340 -8.340], eps: 1.0})
Step:   44700, Reward: [-512.212 -512.212 -512.212] [83.435], Avg: [-446.863 -446.863 -446.863] (0.0113) ({r_i: None, r_t: [-953.348 -953.348 -953.348], eps: 0.011})
Step:     100, Reward: [-510.479 -510.479 -510.479] [71.314], Avg: [-490.907 -490.907 -490.907] (0.9900) ({r_i: None, r_t: [-912.747 -912.747 -912.747], eps: 0.99})
Step:   44800, Reward: [-484.998 -484.998 -484.998] [61.600], Avg: [-446.948 -446.948 -446.948] (0.0112) ({r_i: None, r_t: [-928.756 -928.756 -928.756], eps: 0.011})
Step:     200, Reward: [-482.985 -482.985 -482.985] [91.474], Avg: [-488.266 -488.266 -488.266] (0.9801) ({r_i: None, r_t: [-955.217 -955.217 -955.217], eps: 0.98})
Step:   44900, Reward: [-478.882 -478.882 -478.882] [83.260], Avg: [-447.019 -447.019 -447.019] (0.0111) ({r_i: None, r_t: [-977.344 -977.344 -977.344], eps: 0.011})
Step:   45000, Reward: [-478.838 -478.838 -478.838] [51.322], Avg: [-447.090 -447.090 -447.090] (0.0110) ({r_i: None, r_t: [-932.943 -932.943 -932.943], eps: 0.011})
Step:     300, Reward: [-483.294 -483.294 -483.294] [100.503], Avg: [-487.023 -487.023 -487.023] (0.9704) ({r_i: None, r_t: [-976.224 -976.224 -976.224], eps: 0.97})
Step:   45100, Reward: [-484.828 -484.828 -484.828] [82.217], Avg: [-447.173 -447.173 -447.173] (0.0109) ({r_i: None, r_t: [-971.731 -971.731 -971.731], eps: 0.011})
Step:     400, Reward: [-464.894 -464.894 -464.894] [81.631], Avg: [-482.597 -482.597 -482.597] (0.9607) ({r_i: None, r_t: [-992.848 -992.848 -992.848], eps: 0.961})
Step:   45200, Reward: [-488.779 -488.779 -488.779] [81.716], Avg: [-447.265 -447.265 -447.265] (0.0108) ({r_i: None, r_t: [-961.048 -961.048 -961.048], eps: 0.011})
Step:     500, Reward: [-462.452 -462.452 -462.452] [108.327], Avg: [-479.240 -479.240 -479.240] (0.9511) ({r_i: None, r_t: [-965.949 -965.949 -965.949], eps: 0.951})
Step:   45300, Reward: [-474.323 -474.323 -474.323] [70.162], Avg: [-447.325 -447.325 -447.325] (0.0107) ({r_i: None, r_t: [-950.000 -950.000 -950.000], eps: 0.011})
Step:   45400, Reward: [-473.949 -473.949 -473.949] [52.241], Avg: [-447.383 -447.383 -447.383] (0.0106) ({r_i: None, r_t: [-915.856 -915.856 -915.856], eps: 0.011})
Step:     600, Reward: [-498.574 -498.574 -498.574] [106.511], Avg: [-482.002 -482.002 -482.002] (0.9416) ({r_i: None, r_t: [-930.010 -930.010 -930.010], eps: 0.942})
Step:   45500, Reward: [-452.890 -452.890 -452.890] [54.020], Avg: [-447.395 -447.395 -447.395] (0.0104) ({r_i: None, r_t: [-935.643 -935.643 -935.643], eps: 0.01})
Step:     700, Reward: [-456.446 -456.446 -456.446] [79.327], Avg: [-478.807 -478.807 -478.807] (0.9322) ({r_i: None, r_t: [-1002.762 -1002.762 -1002.762], eps: 0.932})
Step:   45600, Reward: [-482.098 -482.098 -482.098] [78.162], Avg: [-447.471 -447.471 -447.471] (0.0103) ({r_i: None, r_t: [-941.704 -941.704 -941.704], eps: 0.01})
Step:   45700, Reward: [-478.788 -478.788 -478.788] [65.192], Avg: [-447.539 -447.539 -447.539] (0.0102) ({r_i: None, r_t: [-948.462 -948.462 -948.462], eps: 0.01})
Step:     800, Reward: [-503.150 -503.150 -503.150] [105.944], Avg: [-481.512 -481.512 -481.512] (0.9229) ({r_i: None, r_t: [-995.529 -995.529 -995.529], eps: 0.923})
Step:   45800, Reward: [-491.826 -491.826 -491.826] [78.899], Avg: [-447.636 -447.636 -447.636] (0.0101) ({r_i: None, r_t: [-997.162 -997.162 -997.162], eps: 0.01})
Step:     900, Reward: [-464.038 -464.038 -464.038] [64.896], Avg: [-479.765 -479.765 -479.765] (0.9137) ({r_i: None, r_t: [-954.720 -954.720 -954.720], eps: 0.914})
Step:   45900, Reward: [-519.479 -519.479 -519.479] [73.644], Avg: [-447.792 -447.792 -447.792] (0.0100) ({r_i: None, r_t: [-990.243 -990.243 -990.243], eps: 0.01})
Step:    1000, Reward: [-518.900 -518.900 -518.900] [119.620], Avg: [-483.322 -483.322 -483.322] (0.9046) ({r_i: None, r_t: [-1019.567 -1019.567 -1019.567], eps: 0.905})
Step:   46000, Reward: [-483.634 -483.634 -483.634] [70.683], Avg: [-447.870 -447.870 -447.870] (0.0100) ({r_i: None, r_t: [-944.591 -944.591 -944.591], eps: 0.01})
Step:   46100, Reward: [-451.590 -451.590 -451.590] [70.752], Avg: [-447.878 -447.878 -447.878] (0.0100) ({r_i: None, r_t: [-954.702 -954.702 -954.702], eps: 0.01})
Step:    1100, Reward: [-486.291 -486.291 -486.291] [83.255], Avg: [-483.570 -483.570 -483.570] (0.8956) ({r_i: None, r_t: [-964.432 -964.432 -964.432], eps: 0.896})
Step:   46200, Reward: [-467.976 -467.976 -467.976] [54.774], Avg: [-447.921 -447.921 -447.921] (0.0100) ({r_i: None, r_t: [-917.077 -917.077 -917.077], eps: 0.01})
Step:    1200, Reward: [-471.645 -471.645 -471.645] [85.125], Avg: [-482.652 -482.652 -482.652] (0.8867) ({r_i: None, r_t: [-996.608 -996.608 -996.608], eps: 0.887})
Step:   46300, Reward: [-452.531 -452.531 -452.531] [56.141], Avg: [-447.931 -447.931 -447.931] (0.0100) ({r_i: None, r_t: [-983.119 -983.119 -983.119], eps: 0.01})
Step:    1300, Reward: [-512.346 -512.346 -512.346] [57.056], Avg: [-484.773 -484.773 -484.773] (0.8778) ({r_i: None, r_t: [-990.328 -990.328 -990.328], eps: 0.878})
Step:   46400, Reward: [-496.141 -496.141 -496.141] [55.179], Avg: [-448.035 -448.035 -448.035] (0.0100) ({r_i: None, r_t: [-929.540 -929.540 -929.540], eps: 0.01})
Step:   46500, Reward: [-478.890 -478.890 -478.890] [84.356], Avg: [-448.101 -448.101 -448.101] (0.0100) ({r_i: None, r_t: [-920.701 -920.701 -920.701], eps: 0.01})
Step:    1400, Reward: [-475.066 -475.066 -475.066] [91.912], Avg: [-484.126 -484.126 -484.126] (0.8691) ({r_i: None, r_t: [-957.641 -957.641 -957.641], eps: 0.869})
Step:   46600, Reward: [-471.897 -471.897 -471.897] [78.085], Avg: [-448.152 -448.152 -448.152] (0.0100) ({r_i: None, r_t: [-938.657 -938.657 -938.657], eps: 0.01})
Step:    1500, Reward: [-492.474 -492.474 -492.474] [100.434], Avg: [-484.648 -484.648 -484.648] (0.8604) ({r_i: None, r_t: [-919.529 -919.529 -919.529], eps: 0.86})
Step:   46700, Reward: [-505.682 -505.682 -505.682] [64.936], Avg: [-448.275 -448.275 -448.275] (0.0100) ({r_i: None, r_t: [-928.962 -928.962 -928.962], eps: 0.01})
Step:    1600, Reward: [-446.926 -446.926 -446.926] [75.137], Avg: [-482.429 -482.429 -482.429] (0.8518) ({r_i: None, r_t: [-984.551 -984.551 -984.551], eps: 0.852})
Step:   46800, Reward: [-473.110 -473.110 -473.110] [57.619], Avg: [-448.328 -448.328 -448.328] (0.0100) ({r_i: None, r_t: [-974.288 -974.288 -974.288], eps: 0.01})
Step:   46900, Reward: [-491.962 -491.962 -491.962] [78.809], Avg: [-448.421 -448.421 -448.421] (0.0100) ({r_i: None, r_t: [-979.915 -979.915 -979.915], eps: 0.01})
Step:    1700, Reward: [-498.849 -498.849 -498.849] [109.095], Avg: [-483.341 -483.341 -483.341] (0.8433) ({r_i: None, r_t: [-967.831 -967.831 -967.831], eps: 0.843})
Step:   47000, Reward: [-458.363 -458.363 -458.363] [75.552], Avg: [-448.442 -448.442 -448.442] (0.0100) ({r_i: None, r_t: [-938.611 -938.611 -938.611], eps: 0.01})
Step:    1800, Reward: [-496.503 -496.503 -496.503] [65.723], Avg: [-484.034 -484.034 -484.034] (0.8349) ({r_i: None, r_t: [-1021.408 -1021.408 -1021.408], eps: 0.835})
Step:   47100, Reward: [-475.545 -475.545 -475.545] [77.653], Avg: [-448.499 -448.499 -448.499] (0.0100) ({r_i: None, r_t: [-953.584 -953.584 -953.584], eps: 0.01})
Step:    1900, Reward: [-494.131 -494.131 -494.131] [76.161], Avg: [-484.539 -484.539 -484.539] (0.8266) ({r_i: None, r_t: [-1035.782 -1035.782 -1035.782], eps: 0.827})
Step:   47200, Reward: [-481.762 -481.762 -481.762] [92.093], Avg: [-448.570 -448.570 -448.570] (0.0100) ({r_i: None, r_t: [-947.178 -947.178 -947.178], eps: 0.01})
Step:   47300, Reward: [-493.879 -493.879 -493.879] [64.379], Avg: [-448.665 -448.665 -448.665] (0.0100) ({r_i: None, r_t: [-923.750 -923.750 -923.750], eps: 0.01})
Step:    2000, Reward: [-479.917 -479.917 -479.917] [94.961], Avg: [-484.319 -484.319 -484.319] (0.8183) ({r_i: None, r_t: [-961.312 -961.312 -961.312], eps: 0.818})
Step:   47400, Reward: [-487.127 -487.127 -487.127] [59.796], Avg: [-448.746 -448.746 -448.746] (0.0100) ({r_i: None, r_t: [-955.936 -955.936 -955.936], eps: 0.01})
Step:    2100, Reward: [-495.174 -495.174 -495.174] [101.962], Avg: [-484.812 -484.812 -484.812] (0.8102) ({r_i: None, r_t: [-949.114 -949.114 -949.114], eps: 0.81})
Step:   47500, Reward: [-461.897 -461.897 -461.897] [57.841], Avg: [-448.774 -448.774 -448.774] (0.0100) ({r_i: None, r_t: [-928.316 -928.316 -928.316], eps: 0.01})
Step:    2200, Reward: [-536.932 -536.932 -536.932] [63.074], Avg: [-487.078 -487.078 -487.078] (0.8021) ({r_i: None, r_t: [-899.909 -899.909 -899.909], eps: 0.802})
Step:   47600, Reward: [-471.578 -471.578 -471.578] [69.455], Avg: [-448.822 -448.822 -448.822] (0.0100) ({r_i: None, r_t: [-921.231 -921.231 -921.231], eps: 0.01})
Step:   47700, Reward: [-463.457 -463.457 -463.457] [83.932], Avg: [-448.852 -448.852 -448.852] (0.0100) ({r_i: None, r_t: [-920.286 -920.286 -920.286], eps: 0.01})
Step:    2300, Reward: [-513.234 -513.234 -513.234] [77.985], Avg: [-488.168 -488.168 -488.168] (0.7941) ({r_i: None, r_t: [-934.094 -934.094 -934.094], eps: 0.794})
Step:   47800, Reward: [-446.549 -446.549 -446.549] [55.442], Avg: [-448.847 -448.847 -448.847] (0.0100) ({r_i: None, r_t: [-985.991 -985.991 -985.991], eps: 0.01})
Step:    2400, Reward: [-513.064 -513.064 -513.064] [104.777], Avg: [-489.164 -489.164 -489.164] (0.7862) ({r_i: None, r_t: [-942.338 -942.338 -942.338], eps: 0.786})
Step:   47900, Reward: [-434.964 -434.964 -434.964] [79.297], Avg: [-448.819 -448.819 -448.819] (0.0100) ({r_i: None, r_t: [-943.724 -943.724 -943.724], eps: 0.01})
Step:   48000, Reward: [-449.054 -449.054 -449.054] [52.019], Avg: [-448.819 -448.819 -448.819] (0.0100) ({r_i: None, r_t: [-965.746 -965.746 -965.746], eps: 0.01})
Step:    2500, Reward: [-502.154 -502.154 -502.154] [74.947], Avg: [-489.663 -489.663 -489.663] (0.7783) ({r_i: None, r_t: [-1010.288 -1010.288 -1010.288], eps: 0.778})
Step:   48100, Reward: [-463.947 -463.947 -463.947] [63.734], Avg: [-448.850 -448.850 -448.850] (0.0100) ({r_i: None, r_t: [-891.681 -891.681 -891.681], eps: 0.01})
Step:    2600, Reward: [-484.032 -484.032 -484.032] [74.936], Avg: [-489.455 -489.455 -489.455] (0.7705) ({r_i: None, r_t: [-906.248 -906.248 -906.248], eps: 0.771})
Step:   48200, Reward: [-480.163 -480.163 -480.163] [77.226], Avg: [-448.915 -448.915 -448.915] (0.0100) ({r_i: None, r_t: [-939.602 -939.602 -939.602], eps: 0.01})
Step:    2700, Reward: [-458.675 -458.675 -458.675] [92.145], Avg: [-488.356 -488.356 -488.356] (0.7629) ({r_i: None, r_t: [-976.753 -976.753 -976.753], eps: 0.763})
Step:   48300, Reward: [-487.693 -487.693 -487.693] [87.282], Avg: [-448.995 -448.995 -448.995] (0.0100) ({r_i: None, r_t: [-948.945 -948.945 -948.945], eps: 0.01})
Step:   48400, Reward: [-471.355 -471.355 -471.355] [73.147], Avg: [-449.041 -449.041 -449.041] (0.0100) ({r_i: None, r_t: [-954.708 -954.708 -954.708], eps: 0.01})
Step:    2800, Reward: [-510.954 -510.954 -510.954] [99.880], Avg: [-489.135 -489.135 -489.135] (0.7553) ({r_i: None, r_t: [-1017.810 -1017.810 -1017.810], eps: 0.755})
Step:   48500, Reward: [-491.174 -491.174 -491.174] [82.063], Avg: [-449.128 -449.128 -449.128] (0.0100) ({r_i: None, r_t: [-950.952 -950.952 -950.952], eps: 0.01})
Step:    2900, Reward: [-475.070 -475.070 -475.070] [59.656], Avg: [-488.666 -488.666 -488.666] (0.7477) ({r_i: None, r_t: [-1009.505 -1009.505 -1009.505], eps: 0.748})
Step:   48600, Reward: [-448.460 -448.460 -448.460] [81.703], Avg: [-449.127 -449.127 -449.127] (0.0100) ({r_i: None, r_t: [-920.697 -920.697 -920.697], eps: 0.01})
Step:    3000, Reward: [-478.609 -478.609 -478.609] [87.765], Avg: [-488.342 -488.342 -488.342] (0.7403) ({r_i: None, r_t: [-1007.270 -1007.270 -1007.270], eps: 0.74})
Step:   48700, Reward: [-487.772 -487.772 -487.772] [77.870], Avg: [-449.206 -449.206 -449.206] (0.0100) ({r_i: None, r_t: [-935.612 -935.612 -935.612], eps: 0.01})
Step:   48800, Reward: [-473.305 -473.305 -473.305] [52.510], Avg: [-449.255 -449.255 -449.255] (0.0100) ({r_i: None, r_t: [-985.336 -985.336 -985.336], eps: 0.01})
Step:    3100, Reward: [-485.548 -485.548 -485.548] [94.915], Avg: [-488.254 -488.254 -488.254] (0.7329) ({r_i: None, r_t: [-905.037 -905.037 -905.037], eps: 0.733})
Step:   48900, Reward: [-504.627 -504.627 -504.627] [74.684], Avg: [-449.368 -449.368 -449.368] (0.0100) ({r_i: None, r_t: [-935.775 -935.775 -935.775], eps: 0.01})
Step:    3200, Reward: [-499.508 -499.508 -499.508] [91.639], Avg: [-488.595 -488.595 -488.595] (0.7256) ({r_i: None, r_t: [-884.132 -884.132 -884.132], eps: 0.726})
Step:   49000, Reward: [-494.471 -494.471 -494.471] [64.572], Avg: [-449.460 -449.460 -449.460] (0.0100) ({r_i: None, r_t: [-957.748 -957.748 -957.748], eps: 0.01})
Step:    3300, Reward: [-457.962 -457.962 -457.962] [72.722], Avg: [-487.694 -487.694 -487.694] (0.7183) ({r_i: None, r_t: [-974.920 -974.920 -974.920], eps: 0.718})
Step:   49100, Reward: [-480.703 -480.703 -480.703] [64.175], Avg: [-449.524 -449.524 -449.524] (0.0100) ({r_i: None, r_t: [-939.935 -939.935 -939.935], eps: 0.01})
Step:   49200, Reward: [-486.226 -486.226 -486.226] [73.320], Avg: [-449.598 -449.598 -449.598] (0.0100) ({r_i: None, r_t: [-970.698 -970.698 -970.698], eps: 0.01})
Step:    3400, Reward: [-472.895 -472.895 -472.895] [80.872], Avg: [-487.271 -487.271 -487.271] (0.7112) ({r_i: None, r_t: [-976.562 -976.562 -976.562], eps: 0.711})
Step:   49300, Reward: [-496.592 -496.592 -496.592] [56.962], Avg: [-449.693 -449.693 -449.693] (0.0100) ({r_i: None, r_t: [-990.495 -990.495 -990.495], eps: 0.01})
Step:    3500, Reward: [-516.286 -516.286 -516.286] [156.206], Avg: [-488.077 -488.077 -488.077] (0.7041) ({r_i: None, r_t: [-960.013 -960.013 -960.013], eps: 0.704})
Step:   49400, Reward: [-440.847 -440.847 -440.847] [51.795], Avg: [-449.675 -449.675 -449.675] (0.0100) ({r_i: None, r_t: [-963.862 -963.862 -963.862], eps: 0.01})
Step:    3600, Reward: [-459.082 -459.082 -459.082] [46.580], Avg: [-487.294 -487.294 -487.294] (0.6970) ({r_i: None, r_t: [-947.867 -947.867 -947.867], eps: 0.697})
Step:   49500, Reward: [-470.523 -470.523 -470.523] [58.446], Avg: [-449.717 -449.717 -449.717] (0.0100) ({r_i: None, r_t: [-929.133 -929.133 -929.133], eps: 0.01})
Step:   49600, Reward: [-479.254 -479.254 -479.254] [66.033], Avg: [-449.777 -449.777 -449.777] (0.0100) ({r_i: None, r_t: [-1000.379 -1000.379 -1000.379], eps: 0.01})
Step:    3700, Reward: [-478.462 -478.462 -478.462] [54.983], Avg: [-487.061 -487.061 -487.061] (0.6901) ({r_i: None, r_t: [-979.863 -979.863 -979.863], eps: 0.69})
Step:   49700, Reward: [-481.038 -481.038 -481.038] [75.687], Avg: [-449.840 -449.840 -449.840] (0.0100) ({r_i: None, r_t: [-944.745 -944.745 -944.745], eps: 0.01})
Step:    3800, Reward: [-467.156 -467.156 -467.156] [87.783], Avg: [-486.551 -486.551 -486.551] (0.6832) ({r_i: None, r_t: [-924.639 -924.639 -924.639], eps: 0.683})
Step:   49800, Reward: [-487.063 -487.063 -487.063] [72.841], Avg: [-449.914 -449.914 -449.914] (0.0100) ({r_i: None, r_t: [-943.174 -943.174 -943.174], eps: 0.01})
Step:    3900, Reward: [-483.780 -483.780 -483.780] [92.707], Avg: [-486.482 -486.482 -486.482] (0.6764) ({r_i: None, r_t: [-965.580 -965.580 -965.580], eps: 0.676})
Step:   49900, Reward: [-465.610 -465.610 -465.610] [71.526], Avg: [-449.946 -449.946 -449.946] (0.0100) ({r_i: None, r_t: [-993.176 -993.176 -993.176], eps: 0.01})
Step:   50000, Reward: [-478.540 -478.540 -478.540] [71.093], Avg: [-450.003 -450.003 -450.003] (0.0100) ({r_i: None, r_t: [-958.356 -958.356 -958.356], eps: 0.01})
Step:    4000, Reward: [-493.710 -493.710 -493.710] [161.443], Avg: [-486.658 -486.658 -486.658] (0.6696) ({r_i: None, r_t: [-935.931 -935.931 -935.931], eps: 0.67})
Step:   50100, Reward: [-475.362 -475.362 -475.362] [64.539], Avg: [-450.053 -450.053 -450.053] (0.0100) ({r_i: None, r_t: [-968.627 -968.627 -968.627], eps: 0.01})
Step:    4100, Reward: [-454.588 -454.588 -454.588] [88.309], Avg: [-485.894 -485.894 -485.894] (0.6630) ({r_i: None, r_t: [-955.888 -955.888 -955.888], eps: 0.663})
Step:   50200, Reward: [-480.998 -480.998 -480.998] [56.114], Avg: [-450.115 -450.115 -450.115] (0.0100) ({r_i: None, r_t: [-974.920 -974.920 -974.920], eps: 0.01})
Step:    4200, Reward: [-476.658 -476.658 -476.658] [46.418], Avg: [-485.680 -485.680 -485.680] (0.6564) ({r_i: None, r_t: [-971.321 -971.321 -971.321], eps: 0.656})
Step:   50300, Reward: [-480.306 -480.306 -480.306] [58.556], Avg: [-450.175 -450.175 -450.175] (0.0100) ({r_i: None, r_t: [-1013.903 -1013.903 -1013.903], eps: 0.01})
Step:   50400, Reward: [-480.761 -480.761 -480.761] [61.381], Avg: [-450.235 -450.235 -450.235] (0.0100) ({r_i: None, r_t: [-963.454 -963.454 -963.454], eps: 0.01})
Step:    4300, Reward: [-505.312 -505.312 -505.312] [113.834], Avg: [-486.126 -486.126 -486.126] (0.6498) ({r_i: None, r_t: [-1056.638 -1056.638 -1056.638], eps: 0.65})
Step:   50500, Reward: [-488.052 -488.052 -488.052] [70.036], Avg: [-450.310 -450.310 -450.310] (0.0100) ({r_i: None, r_t: [-977.711 -977.711 -977.711], eps: 0.01})
Step:    4400, Reward: [-449.730 -449.730 -449.730] [62.869], Avg: [-485.317 -485.317 -485.317] (0.6433) ({r_i: None, r_t: [-975.630 -975.630 -975.630], eps: 0.643})
Step:   50600, Reward: [-497.746 -497.746 -497.746] [61.457], Avg: [-450.403 -450.403 -450.403] (0.0100) ({r_i: None, r_t: [-993.600 -993.600 -993.600], eps: 0.01})
Step:    4500, Reward: [-475.813 -475.813 -475.813] [94.250], Avg: [-485.110 -485.110 -485.110] (0.6369) ({r_i: None, r_t: [-924.486 -924.486 -924.486], eps: 0.637})
Step:   50700, Reward: [-492.874 -492.874 -492.874] [54.162], Avg: [-450.487 -450.487 -450.487] (0.0100) ({r_i: None, r_t: [-945.559 -945.559 -945.559], eps: 0.01})
Step:   50800, Reward: [-493.385 -493.385 -493.385] [48.668], Avg: [-450.571 -450.571 -450.571] (0.0100) ({r_i: None, r_t: [-987.289 -987.289 -987.289], eps: 0.01})
Step:    4600, Reward: [-460.299 -460.299 -460.299] [85.790], Avg: [-484.582 -484.582 -484.582] (0.6306) ({r_i: None, r_t: [-1000.857 -1000.857 -1000.857], eps: 0.631})
Step:   50900, Reward: [-473.213 -473.213 -473.213] [66.022], Avg: [-450.616 -450.616 -450.616] (0.0100) ({r_i: None, r_t: [-975.675 -975.675 -975.675], eps: 0.01})
Step:    4700, Reward: [-466.063 -466.063 -466.063] [66.866], Avg: [-484.197 -484.197 -484.197] (0.6243) ({r_i: None, r_t: [-933.602 -933.602 -933.602], eps: 0.624})
Step:   51000, Reward: [-474.804 -474.804 -474.804] [70.093], Avg: [-450.663 -450.663 -450.663] (0.0100) ({r_i: None, r_t: [-947.554 -947.554 -947.554], eps: 0.01})
Step:   51100, Reward: [-484.427 -484.427 -484.427] [87.229], Avg: [-450.729 -450.729 -450.729] (0.0100) ({r_i: None, r_t: [-1032.694 -1032.694 -1032.694], eps: 0.01})
Step:    4800, Reward: [-492.850 -492.850 -492.850] [91.688], Avg: [-484.373 -484.373 -484.373] (0.6180) ({r_i: None, r_t: [-976.891 -976.891 -976.891], eps: 0.618})
Step:   51200, Reward: [-477.167 -477.167 -477.167] [80.634], Avg: [-450.781 -450.781 -450.781] (0.0100) ({r_i: None, r_t: [-991.522 -991.522 -991.522], eps: 0.01})
Step:    4900, Reward: [-452.668 -452.668 -452.668] [91.807], Avg: [-483.739 -483.739 -483.739] (0.6119) ({r_i: None, r_t: [-940.236 -940.236 -940.236], eps: 0.612})
Step:   51300, Reward: [-457.745 -457.745 -457.745] [56.914], Avg: [-450.794 -450.794 -450.794] (0.0100) ({r_i: None, r_t: [-996.549 -996.549 -996.549], eps: 0.01})
Step:    5000, Reward: [-494.144 -494.144 -494.144] [59.056], Avg: [-483.943 -483.943 -483.943] (0.6058) ({r_i: None, r_t: [-912.389 -912.389 -912.389], eps: 0.606})
Step:   51400, Reward: [-486.016 -486.016 -486.016] [82.017], Avg: [-450.862 -450.862 -450.862] (0.0100) ({r_i: None, r_t: [-1019.368 -1019.368 -1019.368], eps: 0.01})
Step:   51500, Reward: [-468.985 -468.985 -468.985] [59.469], Avg: [-450.898 -450.898 -450.898] (0.0100) ({r_i: None, r_t: [-969.579 -969.579 -969.579], eps: 0.01})
Step:    5100, Reward: [-458.889 -458.889 -458.889] [101.837], Avg: [-483.461 -483.461 -483.461] (0.5997) ({r_i: None, r_t: [-967.214 -967.214 -967.214], eps: 0.6})
Step:   51600, Reward: [-494.079 -494.079 -494.079] [62.511], Avg: [-450.981 -450.981 -450.981] (0.0100) ({r_i: None, r_t: [-993.587 -993.587 -993.587], eps: 0.01})
Step:    5200, Reward: [-458.070 -458.070 -458.070] [67.315], Avg: [-482.982 -482.982 -482.982] (0.5937) ({r_i: None, r_t: [-987.128 -987.128 -987.128], eps: 0.594})
Step:   51700, Reward: [-468.279 -468.279 -468.279] [82.951], Avg: [-451.015 -451.015 -451.015] (0.0100) ({r_i: None, r_t: [-948.262 -948.262 -948.262], eps: 0.01})
Step:    5300, Reward: [-471.227 -471.227 -471.227] [70.984], Avg: [-482.765 -482.765 -482.765] (0.5878) ({r_i: None, r_t: [-885.649 -885.649 -885.649], eps: 0.588})
Step:   51800, Reward: [-489.157 -489.157 -489.157] [59.385], Avg: [-451.088 -451.088 -451.088] (0.0100) ({r_i: None, r_t: [-971.644 -971.644 -971.644], eps: 0.01})
Step:   51900, Reward: [-493.047 -493.047 -493.047] [81.797], Avg: [-451.169 -451.169 -451.169] (0.0100) ({r_i: None, r_t: [-973.544 -973.544 -973.544], eps: 0.01})
Step:    5400, Reward: [-431.880 -431.880 -431.880] [65.917], Avg: [-481.839 -481.839 -481.839] (0.5820) ({r_i: None, r_t: [-934.491 -934.491 -934.491], eps: 0.582})
Step:   52000, Reward: [-492.276 -492.276 -492.276] [77.530], Avg: [-451.248 -451.248 -451.248] (0.0100) ({r_i: None, r_t: [-990.570 -990.570 -990.570], eps: 0.01})
Step:    5500, Reward: [-465.226 -465.226 -465.226] [70.622], Avg: [-481.543 -481.543 -481.543] (0.5762) ({r_i: None, r_t: [-902.584 -902.584 -902.584], eps: 0.576})
Step:   52100, Reward: [-469.715 -469.715 -469.715] [74.466], Avg: [-451.283 -451.283 -451.283] (0.0100) ({r_i: None, r_t: [-936.913 -936.913 -936.913], eps: 0.01})
Step:    5600, Reward: [-426.605 -426.605 -426.605] [55.384], Avg: [-480.579 -480.579 -480.579] (0.5704) ({r_i: None, r_t: [-945.618 -945.618 -945.618], eps: 0.57})
Step:   52200, Reward: [-490.103 -490.103 -490.103] [71.735], Avg: [-451.357 -451.357 -451.357] (0.0100) ({r_i: None, r_t: [-959.560 -959.560 -959.560], eps: 0.01})
Step:   52300, Reward: [-503.326 -503.326 -503.326] [49.491], Avg: [-451.456 -451.456 -451.456] (0.0100) ({r_i: None, r_t: [-991.507 -991.507 -991.507], eps: 0.01})
Step:    5700, Reward: [-448.944 -448.944 -448.944] [90.269], Avg: [-480.033 -480.033 -480.033] (0.5647) ({r_i: None, r_t: [-904.054 -904.054 -904.054], eps: 0.565})
Step:   52400, Reward: [-469.089 -469.089 -469.089] [69.230], Avg: [-451.490 -451.490 -451.490] (0.0100) ({r_i: None, r_t: [-1035.161 -1035.161 -1035.161], eps: 0.01})
Step:    5800, Reward: [-448.523 -448.523 -448.523] [78.151], Avg: [-479.499 -479.499 -479.499] (0.5591) ({r_i: None, r_t: [-874.873 -874.873 -874.873], eps: 0.559})
Step:   52500, Reward: [-488.391 -488.391 -488.391] [63.970], Avg: [-451.560 -451.560 -451.560] (0.0100) ({r_i: None, r_t: [-931.439 -931.439 -931.439], eps: 0.01})
Step:    5900, Reward: [-449.754 -449.754 -449.754] [71.348], Avg: [-479.004 -479.004 -479.004] (0.5535) ({r_i: None, r_t: [-899.426 -899.426 -899.426], eps: 0.554})
Step:   52600, Reward: [-493.504 -493.504 -493.504] [74.495], Avg: [-451.640 -451.640 -451.640] (0.0100) ({r_i: None, r_t: [-962.803 -962.803 -962.803], eps: 0.01})
Step:   52700, Reward: [-465.473 -465.473 -465.473] [64.977], Avg: [-451.666 -451.666 -451.666] (0.0100) ({r_i: None, r_t: [-940.598 -940.598 -940.598], eps: 0.01})
Step:    6000, Reward: [-453.844 -453.844 -453.844] [53.102], Avg: [-478.591 -478.591 -478.591] (0.5480) ({r_i: None, r_t: [-849.940 -849.940 -849.940], eps: 0.548})
Step:   52800, Reward: [-471.546 -471.546 -471.546] [81.224], Avg: [-451.703 -451.703 -451.703] (0.0100) ({r_i: None, r_t: [-930.853 -930.853 -930.853], eps: 0.01})
Step:    6100, Reward: [-441.811 -441.811 -441.811] [59.450], Avg: [-477.998 -477.998 -477.998] (0.5425) ({r_i: None, r_t: [-901.712 -901.712 -901.712], eps: 0.543})
Step:   52900, Reward: [-469.783 -469.783 -469.783] [78.579], Avg: [-451.738 -451.738 -451.738] (0.0100) ({r_i: None, r_t: [-1000.720 -1000.720 -1000.720], eps: 0.01})
Step:   53000, Reward: [-514.640 -514.640 -514.640] [90.116], Avg: [-451.856 -451.856 -451.856] (0.0100) ({r_i: None, r_t: [-961.605 -961.605 -961.605], eps: 0.01})
Step:    6200, Reward: [-433.036 -433.036 -433.036] [58.712], Avg: [-477.284 -477.284 -477.284] (0.5371) ({r_i: None, r_t: [-899.381 -899.381 -899.381], eps: 0.537})
Step:   53100, Reward: [-479.688 -479.688 -479.688] [74.402], Avg: [-451.908 -451.908 -451.908] (0.0100) ({r_i: None, r_t: [-961.783 -961.783 -961.783], eps: 0.01})
Step:    6300, Reward: [-429.117 -429.117 -429.117] [64.777], Avg: [-476.532 -476.532 -476.532] (0.5318) ({r_i: None, r_t: [-945.101 -945.101 -945.101], eps: 0.532})
Step:   53200, Reward: [-496.831 -496.831 -496.831] [74.425], Avg: [-451.993 -451.993 -451.993] (0.0100) ({r_i: None, r_t: [-972.186 -972.186 -972.186], eps: 0.01})
Step:    6400, Reward: [-449.001 -449.001 -449.001] [76.294], Avg: [-476.108 -476.108 -476.108] (0.5264) ({r_i: None, r_t: [-902.979 -902.979 -902.979], eps: 0.526})
Step:   53300, Reward: [-474.252 -474.252 -474.252] [55.824], Avg: [-452.034 -452.034 -452.034] (0.0100) ({r_i: None, r_t: [-998.439 -998.439 -998.439], eps: 0.01})
Step:   53400, Reward: [-464.463 -464.463 -464.463] [44.606], Avg: [-452.058 -452.058 -452.058] (0.0100) ({r_i: None, r_t: [-939.020 -939.020 -939.020], eps: 0.01})
Step:    6500, Reward: [-478.268 -478.268 -478.268] [61.194], Avg: [-476.141 -476.141 -476.141] (0.5212) ({r_i: None, r_t: [-927.327 -927.327 -927.327], eps: 0.521})
Step:   53500, Reward: [-500.820 -500.820 -500.820] [66.796], Avg: [-452.149 -452.149 -452.149] (0.0100) ({r_i: None, r_t: [-940.805 -940.805 -940.805], eps: 0.01})
Step:    6600, Reward: [-426.423 -426.423 -426.423] [54.531], Avg: [-475.399 -475.399 -475.399] (0.5160) ({r_i: None, r_t: [-904.121 -904.121 -904.121], eps: 0.516})
Step:   53600, Reward: [-472.722 -472.722 -472.722] [62.911], Avg: [-452.187 -452.187 -452.187] (0.0100) ({r_i: None, r_t: [-937.536 -937.536 -937.536], eps: 0.01})
Step:   53700, Reward: [-482.774 -482.774 -482.774] [69.390], Avg: [-452.244 -452.244 -452.244] (0.0100) ({r_i: None, r_t: [-997.636 -997.636 -997.636], eps: 0.01})
Step:    6700, Reward: [-456.055 -456.055 -456.055] [95.465], Avg: [-475.114 -475.114 -475.114] (0.5108) ({r_i: None, r_t: [-939.558 -939.558 -939.558], eps: 0.511})
Step:   53800, Reward: [-504.701 -504.701 -504.701] [77.279], Avg: [-452.341 -452.341 -452.341] (0.0100) ({r_i: None, r_t: [-899.757 -899.757 -899.757], eps: 0.01})
Step:    6800, Reward: [-466.215 -466.215 -466.215] [67.235], Avg: [-474.985 -474.985 -474.985] (0.5058) ({r_i: None, r_t: [-874.954 -874.954 -874.954], eps: 0.506})
Step:   53900, Reward: [-473.922 -473.922 -473.922] [62.761], Avg: [-452.381 -452.381 -452.381] (0.0100) ({r_i: None, r_t: [-959.923 -959.923 -959.923], eps: 0.01})
Step:    6900, Reward: [-467.954 -467.954 -467.954] [74.562], Avg: [-474.885 -474.885 -474.885] (0.5007) ({r_i: None, r_t: [-902.470 -902.470 -902.470], eps: 0.501})
Step:   54000, Reward: [-476.488 -476.488 -476.488] [86.067], Avg: [-452.426 -452.426 -452.426] (0.0100) ({r_i: None, r_t: [-959.396 -959.396 -959.396], eps: 0.01})
Step:   54100, Reward: [-440.208 -440.208 -440.208] [55.012], Avg: [-452.403 -452.403 -452.403] (0.0100) ({r_i: None, r_t: [-916.865 -916.865 -916.865], eps: 0.01})
Step:    7000, Reward: [-428.354 -428.354 -428.354] [45.374], Avg: [-474.230 -474.230 -474.230] (0.4957) ({r_i: None, r_t: [-960.395 -960.395 -960.395], eps: 0.496})
Step:   54200, Reward: [-456.858 -456.858 -456.858] [83.056], Avg: [-452.411 -452.411 -452.411] (0.0100) ({r_i: None, r_t: [-915.569 -915.569 -915.569], eps: 0.01})
Step:    7100, Reward: [-451.776 -451.776 -451.776] [64.412], Avg: [-473.918 -473.918 -473.918] (0.4908) ({r_i: None, r_t: [-917.584 -917.584 -917.584], eps: 0.491})
Step:   54300, Reward: [-453.228 -453.228 -453.228] [68.583], Avg: [-452.413 -452.413 -452.413] (0.0100) ({r_i: None, r_t: [-953.389 -953.389 -953.389], eps: 0.01})
Step:    7200, Reward: [-468.489 -468.489 -468.489] [69.235], Avg: [-473.843 -473.843 -473.843] (0.4859) ({r_i: None, r_t: [-973.218 -973.218 -973.218], eps: 0.486})
Step:   54400, Reward: [-440.719 -440.719 -440.719] [57.029], Avg: [-452.391 -452.391 -452.391] (0.0100) ({r_i: None, r_t: [-916.675 -916.675 -916.675], eps: 0.01})
Step:   54500, Reward: [-481.444 -481.444 -481.444] [61.535], Avg: [-452.444 -452.444 -452.444] (0.0100) ({r_i: None, r_t: [-959.227 -959.227 -959.227], eps: 0.01})
Step:    7300, Reward: [-447.830 -447.830 -447.830] [57.628], Avg: [-473.492 -473.492 -473.492] (0.4810) ({r_i: None, r_t: [-911.209 -911.209 -911.209], eps: 0.481})
Step:   54600, Reward: [-489.429 -489.429 -489.429] [64.342], Avg: [-452.512 -452.512 -452.512] (0.0100) ({r_i: None, r_t: [-880.800 -880.800 -880.800], eps: 0.01})
Step:    7400, Reward: [-473.080 -473.080 -473.080] [64.148], Avg: [-473.486 -473.486 -473.486] (0.4762) ({r_i: None, r_t: [-922.991 -922.991 -922.991], eps: 0.476})
Step:   54700, Reward: [-478.432 -478.432 -478.432] [61.441], Avg: [-452.559 -452.559 -452.559] (0.0100) ({r_i: None, r_t: [-938.571 -938.571 -938.571], eps: 0.01})
Step:    7500, Reward: [-466.148 -466.148 -466.148] [93.997], Avg: [-473.390 -473.390 -473.390] (0.4715) ({r_i: None, r_t: [-883.991 -883.991 -883.991], eps: 0.471})
Step:   54800, Reward: [-469.224 -469.224 -469.224] [54.341], Avg: [-452.590 -452.590 -452.590] (0.0100) ({r_i: None, r_t: [-927.147 -927.147 -927.147], eps: 0.01})
Step:   54900, Reward: [-448.954 -448.954 -448.954] [76.706], Avg: [-452.583 -452.583 -452.583] (0.0100) ({r_i: None, r_t: [-946.024 -946.024 -946.024], eps: 0.01})
Step:    7600, Reward: [-472.841 -472.841 -472.841] [76.584], Avg: [-473.383 -473.383 -473.383] (0.4668) ({r_i: None, r_t: [-868.825 -868.825 -868.825], eps: 0.467})
Step:   55000, Reward: [-471.048 -471.048 -471.048] [65.829], Avg: [-452.617 -452.617 -452.617] (0.0100) ({r_i: None, r_t: [-874.831 -874.831 -874.831], eps: 0.01})
Step:    7700, Reward: [-421.319 -421.319 -421.319] [48.817], Avg: [-472.715 -472.715 -472.715] (0.4621) ({r_i: None, r_t: [-924.783 -924.783 -924.783], eps: 0.462})
Step:   55100, Reward: [-454.013 -454.013 -454.013] [69.410], Avg: [-452.619 -452.619 -452.619] (0.0100) ({r_i: None, r_t: [-908.602 -908.602 -908.602], eps: 0.01})
Step:    7800, Reward: [-451.239 -451.239 -451.239] [91.198], Avg: [-472.443 -472.443 -472.443] (0.4575) ({r_i: None, r_t: [-941.312 -941.312 -941.312], eps: 0.458})
Step:   55200, Reward: [-433.535 -433.535 -433.535] [67.103], Avg: [-452.585 -452.585 -452.585] (0.0100) ({r_i: None, r_t: [-880.568 -880.568 -880.568], eps: 0.01})
Step:   55300, Reward: [-472.542 -472.542 -472.542] [49.150], Avg: [-452.621 -452.621 -452.621] (0.0100) ({r_i: None, r_t: [-950.889 -950.889 -950.889], eps: 0.01})
Step:    7900, Reward: [-504.470 -504.470 -504.470] [86.407], Avg: [-472.844 -472.844 -472.844] (0.4529) ({r_i: None, r_t: [-916.011 -916.011 -916.011], eps: 0.453})
Step:   55400, Reward: [-486.407 -486.407 -486.407] [68.219], Avg: [-452.682 -452.682 -452.682] (0.0100) ({r_i: None, r_t: [-906.393 -906.393 -906.393], eps: 0.01})
Step:    8000, Reward: [-409.930 -409.930 -409.930] [36.551], Avg: [-472.067 -472.067 -472.067] (0.4484) ({r_i: None, r_t: [-891.299 -891.299 -891.299], eps: 0.448})
Step:   55500, Reward: [-471.383 -471.383 -471.383] [81.305], Avg: [-452.715 -452.715 -452.715] (0.0100) ({r_i: None, r_t: [-862.911 -862.911 -862.911], eps: 0.01})
Step:    8100, Reward: [-468.377 -468.377 -468.377] [91.583], Avg: [-472.022 -472.022 -472.022] (0.4440) ({r_i: None, r_t: [-880.742 -880.742 -880.742], eps: 0.444})
Step:   55600, Reward: [-478.156 -478.156 -478.156] [73.464], Avg: [-452.761 -452.761 -452.761] (0.0100) ({r_i: None, r_t: [-860.420 -860.420 -860.420], eps: 0.01})
Step:   55700, Reward: [-472.372 -472.372 -472.372] [51.780], Avg: [-452.796 -452.796 -452.796] (0.0100) ({r_i: None, r_t: [-897.782 -897.782 -897.782], eps: 0.01})
Step:    8200, Reward: [-446.991 -446.991 -446.991] [84.993], Avg: [-471.720 -471.720 -471.720] (0.4395) ({r_i: None, r_t: [-917.335 -917.335 -917.335], eps: 0.44})
Step:   55800, Reward: [-429.114 -429.114 -429.114] [75.563], Avg: [-452.754 -452.754 -452.754] (0.0100) ({r_i: None, r_t: [-946.690 -946.690 -946.690], eps: 0.01})
Step:    8300, Reward: [-433.447 -433.447 -433.447] [84.344], Avg: [-471.265 -471.265 -471.265] (0.4351) ({r_i: None, r_t: [-865.037 -865.037 -865.037], eps: 0.435})
Step:   55900, Reward: [-484.961 -484.961 -484.961] [73.511], Avg: [-452.811 -452.811 -452.811] (0.0100) ({r_i: None, r_t: [-898.958 -898.958 -898.958], eps: 0.01})
Step:    8400, Reward: [-461.034 -461.034 -461.034] [89.701], Avg: [-471.144 -471.144 -471.144] (0.4308) ({r_i: None, r_t: [-872.109 -872.109 -872.109], eps: 0.431})
Step:   56000, Reward: [-470.210 -470.210 -470.210] [79.931], Avg: [-452.842 -452.842 -452.842] (0.0100) ({r_i: None, r_t: [-928.464 -928.464 -928.464], eps: 0.01})
Step:   56100, Reward: [-446.562 -446.562 -446.562] [61.114], Avg: [-452.831 -452.831 -452.831] (0.0100) ({r_i: None, r_t: [-909.963 -909.963 -909.963], eps: 0.01})
Step:    8500, Reward: [-449.179 -449.179 -449.179] [75.395], Avg: [-470.889 -470.889 -470.889] (0.4265) ({r_i: None, r_t: [-865.895 -865.895 -865.895], eps: 0.427})
Step:   56200, Reward: [-429.428 -429.428 -429.428] [69.312], Avg: [-452.789 -452.789 -452.789] (0.0100) ({r_i: None, r_t: [-876.752 -876.752 -876.752], eps: 0.01})
Step:    8600, Reward: [-454.230 -454.230 -454.230] [83.705], Avg: [-470.697 -470.697 -470.697] (0.4223) ({r_i: None, r_t: [-877.296 -877.296 -877.296], eps: 0.422})
Step:   56300, Reward: [-418.872 -418.872 -418.872] [66.360], Avg: [-452.729 -452.729 -452.729] (0.0100) ({r_i: None, r_t: [-886.985 -886.985 -886.985], eps: 0.01})
Step:   56400, Reward: [-445.612 -445.612 -445.612] [72.315], Avg: [-452.717 -452.717 -452.717] (0.0100) ({r_i: None, r_t: [-865.385 -865.385 -865.385], eps: 0.01})
Step:    8700, Reward: [-459.139 -459.139 -459.139] [75.648], Avg: [-470.566 -470.566 -470.566] (0.4180) ({r_i: None, r_t: [-858.683 -858.683 -858.683], eps: 0.418})
Step:   56500, Reward: [-471.892 -471.892 -471.892] [77.640], Avg: [-452.751 -452.751 -452.751] (0.0100) ({r_i: None, r_t: [-914.141 -914.141 -914.141], eps: 0.01})
Step:    8800, Reward: [-439.053 -439.053 -439.053] [85.222], Avg: [-470.212 -470.212 -470.212] (0.4139) ({r_i: None, r_t: [-866.746 -866.746 -866.746], eps: 0.414})
Step:   56600, Reward: [-405.677 -405.677 -405.677] [83.157], Avg: [-452.668 -452.668 -452.668] (0.0100) ({r_i: None, r_t: [-917.173 -917.173 -917.173], eps: 0.01})
Step:    8900, Reward: [-428.475 -428.475 -428.475] [64.035], Avg: [-469.748 -469.748 -469.748] (0.4097) ({r_i: None, r_t: [-857.674 -857.674 -857.674], eps: 0.41})
Step:   56700, Reward: [-424.487 -424.487 -424.487] [79.943], Avg: [-452.618 -452.618 -452.618] (0.0100) ({r_i: None, r_t: [-897.656 -897.656 -897.656], eps: 0.01})
Step:   56800, Reward: [-431.622 -431.622 -431.622] [75.355], Avg: [-452.581 -452.581 -452.581] (0.0100) ({r_i: None, r_t: [-900.259 -900.259 -900.259], eps: 0.01})
Step:    9000, Reward: [-458.139 -458.139 -458.139] [87.281], Avg: [-469.621 -469.621 -469.621] (0.4057) ({r_i: None, r_t: [-864.498 -864.498 -864.498], eps: 0.406})
Step:   56900, Reward: [-410.368 -410.368 -410.368] [57.116], Avg: [-452.507 -452.507 -452.507] (0.0100) ({r_i: None, r_t: [-861.082 -861.082 -861.082], eps: 0.01})
Step:    9100, Reward: [-458.864 -458.864 -458.864] [69.310], Avg: [-469.504 -469.504 -469.504] (0.4016) ({r_i: None, r_t: [-897.734 -897.734 -897.734], eps: 0.402})
Step:   57000, Reward: [-408.079 -408.079 -408.079] [67.235], Avg: [-452.429 -452.429 -452.429] (0.0100) ({r_i: None, r_t: [-812.530 -812.530 -812.530], eps: 0.01})
Step:    9200, Reward: [-433.280 -433.280 -433.280] [61.453], Avg: [-469.114 -469.114 -469.114] (0.3976) ({r_i: None, r_t: [-830.000 -830.000 -830.000], eps: 0.398})
Step:   57100, Reward: [-448.991 -448.991 -448.991] [50.368], Avg: [-452.423 -452.423 -452.423] (0.0100) ({r_i: None, r_t: [-908.700 -908.700 -908.700], eps: 0.01})
Step:   57200, Reward: [-417.537 -417.537 -417.537] [73.681], Avg: [-452.362 -452.362 -452.362] (0.0100) ({r_i: None, r_t: [-830.160 -830.160 -830.160], eps: 0.01})
Step:    9300, Reward: [-415.624 -415.624 -415.624] [71.528], Avg: [-468.545 -468.545 -468.545] (0.3936) ({r_i: None, r_t: [-864.810 -864.810 -864.810], eps: 0.394})
Step:   57300, Reward: [-412.598 -412.598 -412.598] [48.321], Avg: [-452.293 -452.293 -452.293] (0.0100) ({r_i: None, r_t: [-814.543 -814.543 -814.543], eps: 0.01})
Step:    9400, Reward: [-442.365 -442.365 -442.365] [43.795], Avg: [-468.270 -468.270 -468.270] (0.3897) ({r_i: None, r_t: [-822.378 -822.378 -822.378], eps: 0.39})
Step:   57400, Reward: [-451.909 -451.909 -451.909] [72.306], Avg: [-452.292 -452.292 -452.292] (0.0100) ({r_i: None, r_t: [-885.487 -885.487 -885.487], eps: 0.01})
Step:    9500, Reward: [-425.933 -425.933 -425.933] [49.533], Avg: [-467.829 -467.829 -467.829] (0.3858) ({r_i: None, r_t: [-855.286 -855.286 -855.286], eps: 0.386})
Step:   57500, Reward: [-405.761 -405.761 -405.761] [79.469], Avg: [-452.212 -452.212 -452.212] (0.0100) ({r_i: None, r_t: [-870.171 -870.171 -870.171], eps: 0.01})
Step:   57600, Reward: [-388.444 -388.444 -388.444] [68.497], Avg: [-452.101 -452.101 -452.101] (0.0100) ({r_i: None, r_t: [-865.313 -865.313 -865.313], eps: 0.01})
Step:    9600, Reward: [-413.133 -413.133 -413.133] [38.607], Avg: [-467.265 -467.265 -467.265] (0.3820) ({r_i: None, r_t: [-800.447 -800.447 -800.447], eps: 0.382})
Step:   57700, Reward: [-411.769 -411.769 -411.769] [70.707], Avg: [-452.031 -452.031 -452.031] (0.0100) ({r_i: None, r_t: [-819.120 -819.120 -819.120], eps: 0.01})
Step:    9700, Reward: [-419.806 -419.806 -419.806] [42.519], Avg: [-466.780 -466.780 -466.780] (0.3782) ({r_i: None, r_t: [-817.909 -817.909 -817.909], eps: 0.378})
Step:   57800, Reward: [-431.132 -431.132 -431.132] [57.963], Avg: [-451.995 -451.995 -451.995] (0.0100) ({r_i: None, r_t: [-840.978 -840.978 -840.978], eps: 0.01})
Step:    9800, Reward: [-417.115 -417.115 -417.115] [72.557], Avg: [-466.279 -466.279 -466.279] (0.3744) ({r_i: None, r_t: [-807.990 -807.990 -807.990], eps: 0.374})
Step:   57900, Reward: [-408.113 -408.113 -408.113] [66.556], Avg: [-451.920 -451.920 -451.920] (0.0100) ({r_i: None, r_t: [-853.471 -853.471 -853.471], eps: 0.01})
Step:   58000, Reward: [-409.088 -409.088 -409.088] [59.238], Avg: [-451.846 -451.846 -451.846] (0.0100) ({r_i: None, r_t: [-801.749 -801.749 -801.749], eps: 0.01})
Step:    9900, Reward: [-453.546 -453.546 -453.546] [62.483], Avg: [-466.151 -466.151 -466.151] (0.3707) ({r_i: None, r_t: [-883.108 -883.108 -883.108], eps: 0.371})
Step:   58100, Reward: [-432.914 -432.914 -432.914] [59.482], Avg: [-451.813 -451.813 -451.813] (0.0100) ({r_i: None, r_t: [-860.483 -860.483 -860.483], eps: 0.01})
Step:   10000, Reward: [-425.920 -425.920 -425.920] [67.078], Avg: [-465.753 -465.753 -465.753] (0.3670) ({r_i: None, r_t: [-798.040 -798.040 -798.040], eps: 0.367})
Step:   58200, Reward: [-446.243 -446.243 -446.243] [66.361], Avg: [-451.804 -451.804 -451.804] (0.0100) ({r_i: None, r_t: [-832.425 -832.425 -832.425], eps: 0.01})
Step:   10100, Reward: [-418.023 -418.023 -418.023] [59.606], Avg: [-465.285 -465.285 -465.285] (0.3633) ({r_i: None, r_t: [-869.671 -869.671 -869.671], eps: 0.363})
Step:   58300, Reward: [-436.697 -436.697 -436.697] [110.068], Avg: [-451.778 -451.778 -451.778] (0.0100) ({r_i: None, r_t: [-816.352 -816.352 -816.352], eps: 0.01})
Step:   58400, Reward: [-404.897 -404.897 -404.897] [62.674], Avg: [-451.698 -451.698 -451.698] (0.0100) ({r_i: None, r_t: [-847.952 -847.952 -847.952], eps: 0.01})
Step:   10200, Reward: [-427.863 -427.863 -427.863] [56.646], Avg: [-464.922 -464.922 -464.922] (0.3597) ({r_i: None, r_t: [-845.681 -845.681 -845.681], eps: 0.36})
Step:   58500, Reward: [-431.099 -431.099 -431.099] [81.850], Avg: [-451.663 -451.663 -451.663] (0.0100) ({r_i: None, r_t: [-869.843 -869.843 -869.843], eps: 0.01})
Step:   10300, Reward: [-409.020 -409.020 -409.020] [57.292], Avg: [-464.384 -464.384 -464.384] (0.3561) ({r_i: None, r_t: [-831.020 -831.020 -831.020], eps: 0.356})
Step:   58600, Reward: [-410.604 -410.604 -410.604] [56.937], Avg: [-451.593 -451.593 -451.593] (0.0100) ({r_i: None, r_t: [-810.464 -810.464 -810.464], eps: 0.01})
Step:   10400, Reward: [-402.253 -402.253 -402.253] [74.299], Avg: [-463.793 -463.793 -463.793] (0.3525) ({r_i: None, r_t: [-787.777 -787.777 -787.777], eps: 0.353})
Step:   58700, Reward: [-425.766 -425.766 -425.766] [60.156], Avg: [-451.549 -451.549 -451.549] (0.0100) ({r_i: None, r_t: [-776.991 -776.991 -776.991], eps: 0.01})
Step:   58800, Reward: [-453.048 -453.048 -453.048] [78.768], Avg: [-451.551 -451.551 -451.551] (0.0100) ({r_i: None, r_t: [-801.923 -801.923 -801.923], eps: 0.01})
Step:   10500, Reward: [-420.941 -420.941 -420.941] [58.356], Avg: [-463.388 -463.388 -463.388] (0.3490) ({r_i: None, r_t: [-839.069 -839.069 -839.069], eps: 0.349})
Step:   58900, Reward: [-428.241 -428.241 -428.241] [65.145], Avg: [-451.512 -451.512 -451.512] (0.0100) ({r_i: None, r_t: [-868.387 -868.387 -868.387], eps: 0.01})
Step:   10600, Reward: [-415.928 -415.928 -415.928] [54.551], Avg: [-462.945 -462.945 -462.945] (0.3455) ({r_i: None, r_t: [-847.718 -847.718 -847.718], eps: 0.346})
Step:   59000, Reward: [-430.381 -430.381 -430.381] [58.733], Avg: [-451.476 -451.476 -451.476] (0.0100) ({r_i: None, r_t: [-848.625 -848.625 -848.625], eps: 0.01})
Step:   10700, Reward: [-430.125 -430.125 -430.125] [76.956], Avg: [-462.641 -462.641 -462.641] (0.3421) ({r_i: None, r_t: [-845.011 -845.011 -845.011], eps: 0.342})
Step:   59100, Reward: [-403.902 -403.902 -403.902] [72.106], Avg: [-451.396 -451.396 -451.396] (0.0100) ({r_i: None, r_t: [-822.397 -822.397 -822.397], eps: 0.01})
Step:   59200, Reward: [-418.087 -418.087 -418.087] [72.240], Avg: [-451.339 -451.339 -451.339] (0.0100) ({r_i: None, r_t: [-814.270 -814.270 -814.270], eps: 0.01})
Step:   10800, Reward: [-404.706 -404.706 -404.706] [62.144], Avg: [-462.109 -462.109 -462.109] (0.3387) ({r_i: None, r_t: [-831.996 -831.996 -831.996], eps: 0.339})
Step:   59300, Reward: [-427.434 -427.434 -427.434] [98.651], Avg: [-451.299 -451.299 -451.299] (0.0100) ({r_i: None, r_t: [-850.610 -850.610 -850.610], eps: 0.01})
Step:   10900, Reward: [-427.564 -427.564 -427.564] [53.630], Avg: [-461.795 -461.795 -461.795] (0.3353) ({r_i: None, r_t: [-836.489 -836.489 -836.489], eps: 0.335})
Step:   59400, Reward: [-393.213 -393.213 -393.213] [98.112], Avg: [-451.202 -451.202 -451.202] (0.0100) ({r_i: None, r_t: [-825.102 -825.102 -825.102], eps: 0.01})
Step:   59500, Reward: [-454.768 -454.768 -454.768] [74.171], Avg: [-451.208 -451.208 -451.208] (0.0100) ({r_i: None, r_t: [-771.802 -771.802 -771.802], eps: 0.01})
Step:   11000, Reward: [-455.586 -455.586 -455.586] [82.959], Avg: [-461.739 -461.739 -461.739] (0.3320) ({r_i: None, r_t: [-852.427 -852.427 -852.427], eps: 0.332})
Step:   59600, Reward: [-422.306 -422.306 -422.306] [58.155], Avg: [-451.159 -451.159 -451.159] (0.0100) ({r_i: None, r_t: [-784.507 -784.507 -784.507], eps: 0.01})
Step:   11100, Reward: [-426.632 -426.632 -426.632] [64.864], Avg: [-461.426 -461.426 -461.426] (0.3286) ({r_i: None, r_t: [-848.110 -848.110 -848.110], eps: 0.329})
Step:   59700, Reward: [-354.598 -354.598 -354.598] [49.006], Avg: [-450.998 -450.998 -450.998] (0.0100) ({r_i: None, r_t: [-780.217 -780.217 -780.217], eps: 0.01})
Step:   11200, Reward: [-440.033 -440.033 -440.033] [51.105], Avg: [-461.237 -461.237 -461.237] (0.3254) ({r_i: None, r_t: [-829.968 -829.968 -829.968], eps: 0.325})
Step:   59800, Reward: [-404.614 -404.614 -404.614] [76.179], Avg: [-450.920 -450.920 -450.920] (0.0100) ({r_i: None, r_t: [-822.229 -822.229 -822.229], eps: 0.01})
Step:   59900, Reward: [-377.918 -377.918 -377.918] [60.758], Avg: [-450.799 -450.799 -450.799] (0.0100) ({r_i: None, r_t: [-825.898 -825.898 -825.898], eps: 0.01})
Step:   11300, Reward: [-417.860 -417.860 -417.860] [60.802], Avg: [-460.856 -460.856 -460.856] (0.3221) ({r_i: None, r_t: [-839.340 -839.340 -839.340], eps: 0.322})
Step:   60000, Reward: [-427.000 -427.000 -427.000] [70.886], Avg: [-450.759 -450.759 -450.759] (0.0100) ({r_i: None, r_t: [-774.000 -774.000 -774.000], eps: 0.01})
Step:   11400, Reward: [-464.238 -464.238 -464.238] [71.816], Avg: [-460.886 -460.886 -460.886] (0.3189) ({r_i: None, r_t: [-896.358 -896.358 -896.358], eps: 0.319})
Step:   60100, Reward: [-395.658 -395.658 -395.658] [59.832], Avg: [-450.667 -450.667 -450.667] (0.0100) ({r_i: None, r_t: [-824.964 -824.964 -824.964], eps: 0.01})
Step:   11500, Reward: [-437.493 -437.493 -437.493] [71.573], Avg: [-460.684 -460.684 -460.684] (0.3157) ({r_i: None, r_t: [-894.887 -894.887 -894.887], eps: 0.316})
Step:   60200, Reward: [-384.868 -384.868 -384.868] [68.841], Avg: [-450.558 -450.558 -450.558] (0.0100) ({r_i: None, r_t: [-823.868 -823.868 -823.868], eps: 0.01})
Step:   60300, Reward: [-381.143 -381.143 -381.143] [68.488], Avg: [-450.443 -450.443 -450.443] (0.0100) ({r_i: None, r_t: [-787.053 -787.053 -787.053], eps: 0.01})
Step:   11600, Reward: [-454.178 -454.178 -454.178] [72.881], Avg: [-460.628 -460.628 -460.628] (0.3126) ({r_i: None, r_t: [-870.045 -870.045 -870.045], eps: 0.313})
Step:   60400, Reward: [-387.543 -387.543 -387.543] [52.637], Avg: [-450.339 -450.339 -450.339] (0.0100) ({r_i: None, r_t: [-805.970 -805.970 -805.970], eps: 0.01})
Step:   11700, Reward: [-431.083 -431.083 -431.083] [70.216], Avg: [-460.378 -460.378 -460.378] (0.3095) ({r_i: None, r_t: [-836.238 -836.238 -836.238], eps: 0.309})
Step:   60500, Reward: [-414.561 -414.561 -414.561] [77.490], Avg: [-450.280 -450.280 -450.280] (0.0100) ({r_i: None, r_t: [-784.397 -784.397 -784.397], eps: 0.01})
Step:   60600, Reward: [-385.450 -385.450 -385.450] [43.957], Avg: [-450.174 -450.174 -450.174] (0.0100) ({r_i: None, r_t: [-796.403 -796.403 -796.403], eps: 0.01})
Step:   11800, Reward: [-414.042 -414.042 -414.042] [66.769], Avg: [-459.989 -459.989 -459.989] (0.3064) ({r_i: None, r_t: [-860.349 -860.349 -860.349], eps: 0.306})
Step:   60700, Reward: [-409.292 -409.292 -409.292] [47.312], Avg: [-450.106 -450.106 -450.106] (0.0100) ({r_i: None, r_t: [-825.195 -825.195 -825.195], eps: 0.01})
Step:   11900, Reward: [-435.091 -435.091 -435.091] [77.868], Avg: [-459.781 -459.781 -459.781] (0.3033) ({r_i: None, r_t: [-858.184 -858.184 -858.184], eps: 0.303})
Step:   60800, Reward: [-418.155 -418.155 -418.155] [60.340], Avg: [-450.054 -450.054 -450.054] (0.0100) ({r_i: None, r_t: [-775.034 -775.034 -775.034], eps: 0.01})
Step:   12000, Reward: [-403.387 -403.387 -403.387] [38.679], Avg: [-459.315 -459.315 -459.315] (0.3003) ({r_i: None, r_t: [-809.196 -809.196 -809.196], eps: 0.3})
Step:   60900, Reward: [-395.010 -395.010 -395.010] [64.407], Avg: [-449.964 -449.964 -449.964] (0.0100) ({r_i: None, r_t: [-791.619 -791.619 -791.619], eps: 0.01})
Step:   61000, Reward: [-424.408 -424.408 -424.408] [83.355], Avg: [-449.922 -449.922 -449.922] (0.0100) ({r_i: None, r_t: [-786.809 -786.809 -786.809], eps: 0.01})
Step:   12100, Reward: [-442.811 -442.811 -442.811] [59.586], Avg: [-459.180 -459.180 -459.180] (0.2973) ({r_i: None, r_t: [-851.207 -851.207 -851.207], eps: 0.297})
Step:   61100, Reward: [-375.574 -375.574 -375.574] [92.248], Avg: [-449.800 -449.800 -449.800] (0.0100) ({r_i: None, r_t: [-760.224 -760.224 -760.224], eps: 0.01})
Step:   12200, Reward: [-442.142 -442.142 -442.142] [71.106], Avg: [-459.041 -459.041 -459.041] (0.2943) ({r_i: None, r_t: [-826.921 -826.921 -826.921], eps: 0.294})
Step:   61200, Reward: [-384.508 -384.508 -384.508] [65.668], Avg: [-449.694 -449.694 -449.694] (0.0100) ({r_i: None, r_t: [-785.576 -785.576 -785.576], eps: 0.01})
Step:   12300, Reward: [-431.693 -431.693 -431.693] [76.223], Avg: [-458.821 -458.821 -458.821] (0.2914) ({r_i: None, r_t: [-836.624 -836.624 -836.624], eps: 0.291})
Step:   61300, Reward: [-423.089 -423.089 -423.089] [80.154], Avg: [-449.650 -449.650 -449.650] (0.0100) ({r_i: None, r_t: [-823.885 -823.885 -823.885], eps: 0.01})
Step:   61400, Reward: [-389.584 -389.584 -389.584] [69.605], Avg: [-449.553 -449.553 -449.553] (0.0100) ({r_i: None, r_t: [-788.153 -788.153 -788.153], eps: 0.01})
Step:   12400, Reward: [-421.451 -421.451 -421.451] [53.138], Avg: [-458.522 -458.522 -458.522] (0.2885) ({r_i: None, r_t: [-841.060 -841.060 -841.060], eps: 0.288})
Step:   61500, Reward: [-389.029 -389.029 -389.029] [59.930], Avg: [-449.455 -449.455 -449.455] (0.0100) ({r_i: None, r_t: [-796.368 -796.368 -796.368], eps: 0.01})
Step:   12500, Reward: [-425.620 -425.620 -425.620] [77.227], Avg: [-458.261 -458.261 -458.261] (0.2856) ({r_i: None, r_t: [-815.194 -815.194 -815.194], eps: 0.286})
Step:   61600, Reward: [-373.537 -373.537 -373.537] [80.548], Avg: [-449.332 -449.332 -449.332] (0.0100) ({r_i: None, r_t: [-787.208 -787.208 -787.208], eps: 0.01})
Step:   12600, Reward: [-418.022 -418.022 -418.022] [51.888], Avg: [-457.944 -457.944 -457.944] (0.2828) ({r_i: None, r_t: [-793.257 -793.257 -793.257], eps: 0.283})
Step:   61700, Reward: [-432.284 -432.284 -432.284] [73.576], Avg: [-449.304 -449.304 -449.304] (0.0100) ({r_i: None, r_t: [-807.089 -807.089 -807.089], eps: 0.01})
Step:   61800, Reward: [-422.635 -422.635 -422.635] [81.250], Avg: [-449.261 -449.261 -449.261] (0.0100) ({r_i: None, r_t: [-763.796 -763.796 -763.796], eps: 0.01})
Step:   12700, Reward: [-434.386 -434.386 -434.386] [60.598], Avg: [-457.760 -457.760 -457.760] (0.2799) ({r_i: None, r_t: [-845.006 -845.006 -845.006], eps: 0.28})
Step:   61900, Reward: [-424.971 -424.971 -424.971] [101.001], Avg: [-449.222 -449.222 -449.222] (0.0100) ({r_i: None, r_t: [-818.276 -818.276 -818.276], eps: 0.01})
Step:   12800, Reward: [-395.910 -395.910 -395.910] [46.642], Avg: [-457.280 -457.280 -457.280] (0.2771) ({r_i: None, r_t: [-819.521 -819.521 -819.521], eps: 0.277})
Step:   62000, Reward: [-380.255 -380.255 -380.255] [52.849], Avg: [-449.111 -449.111 -449.111] (0.0100) ({r_i: None, r_t: [-828.031 -828.031 -828.031], eps: 0.01})
Step:   62100, Reward: [-406.115 -406.115 -406.115] [77.816], Avg: [-449.041 -449.041 -449.041] (0.0100) ({r_i: None, r_t: [-791.122 -791.122 -791.122], eps: 0.01})
Step:   12900, Reward: [-388.661 -388.661 -388.661] [53.104], Avg: [-456.752 -456.752 -456.752] (0.2744) ({r_i: None, r_t: [-862.449 -862.449 -862.449], eps: 0.274})
Step:   62200, Reward: [-400.805 -400.805 -400.805] [96.930], Avg: [-448.964 -448.964 -448.964] (0.0100) ({r_i: None, r_t: [-869.114 -869.114 -869.114], eps: 0.01})
Step:   13000, Reward: [-436.068 -436.068 -436.068] [76.918], Avg: [-456.594 -456.594 -456.594] (0.2716) ({r_i: None, r_t: [-816.441 -816.441 -816.441], eps: 0.272})
Step:   62300, Reward: [-436.876 -436.876 -436.876] [58.169], Avg: [-448.945 -448.945 -448.945] (0.0100) ({r_i: None, r_t: [-770.951 -770.951 -770.951], eps: 0.01})
Step:   13100, Reward: [-408.833 -408.833 -408.833] [73.293], Avg: [-456.233 -456.233 -456.233] (0.2689) ({r_i: None, r_t: [-820.231 -820.231 -820.231], eps: 0.269})
Step:   62400, Reward: [-427.883 -427.883 -427.883] [57.280], Avg: [-448.911 -448.911 -448.911] (0.0100) ({r_i: None, r_t: [-771.435 -771.435 -771.435], eps: 0.01})
Step:   62500, Reward: [-433.390 -433.390 -433.390] [65.448], Avg: [-448.886 -448.886 -448.886] (0.0100) ({r_i: None, r_t: [-812.144 -812.144 -812.144], eps: 0.01})
Step:   13200, Reward: [-406.549 -406.549 -406.549] [64.651], Avg: [-455.859 -455.859 -455.859] (0.2663) ({r_i: None, r_t: [-826.048 -826.048 -826.048], eps: 0.266})
Step:   62600, Reward: [-413.929 -413.929 -413.929] [90.865], Avg: [-448.830 -448.830 -448.830] (0.0100) ({r_i: None, r_t: [-785.828 -785.828 -785.828], eps: 0.01})
Step:   13300, Reward: [-405.441 -405.441 -405.441] [61.326], Avg: [-455.483 -455.483 -455.483] (0.2636) ({r_i: None, r_t: [-856.097 -856.097 -856.097], eps: 0.264})
Step:   62700, Reward: [-384.102 -384.102 -384.102] [82.659], Avg: [-448.727 -448.727 -448.727] (0.0100) ({r_i: None, r_t: [-806.245 -806.245 -806.245], eps: 0.01})
Step:   13400, Reward: [-408.601 -408.601 -408.601] [50.700], Avg: [-455.136 -455.136 -455.136] (0.2610) ({r_i: None, r_t: [-829.766 -829.766 -829.766], eps: 0.261})
Step:   62800, Reward: [-415.694 -415.694 -415.694] [89.740], Avg: [-448.675 -448.675 -448.675] (0.0100) ({r_i: None, r_t: [-788.095 -788.095 -788.095], eps: 0.01})
Step:   62900, Reward: [-392.860 -392.860 -392.860] [68.796], Avg: [-448.586 -448.586 -448.586] (0.0100) ({r_i: None, r_t: [-763.515 -763.515 -763.515], eps: 0.01})
Step:   13500, Reward: [-426.203 -426.203 -426.203] [49.001], Avg: [-454.923 -454.923 -454.923] (0.2584) ({r_i: None, r_t: [-851.267 -851.267 -851.267], eps: 0.258})
Step:   63000, Reward: [-396.572 -396.572 -396.572] [72.022], Avg: [-448.504 -448.504 -448.504] (0.0100) ({r_i: None, r_t: [-845.024 -845.024 -845.024], eps: 0.01})
Step:   13600, Reward: [-422.931 -422.931 -422.931] [61.002], Avg: [-454.689 -454.689 -454.689] (0.2558) ({r_i: None, r_t: [-856.018 -856.018 -856.018], eps: 0.256})
Step:   63100, Reward: [-380.034 -380.034 -380.034] [58.023], Avg: [-448.395 -448.395 -448.395] (0.0100) ({r_i: None, r_t: [-830.224 -830.224 -830.224], eps: 0.01})
Step:   63200, Reward: [-387.663 -387.663 -387.663] [68.088], Avg: [-448.300 -448.300 -448.300] (0.0100) ({r_i: None, r_t: [-812.307 -812.307 -812.307], eps: 0.01})
Step:   13700, Reward: [-421.968 -421.968 -421.968] [69.186], Avg: [-454.452 -454.452 -454.452] (0.2532) ({r_i: None, r_t: [-826.034 -826.034 -826.034], eps: 0.253})
Step:   63300, Reward: [-437.928 -437.928 -437.928] [82.763], Avg: [-448.283 -448.283 -448.283] (0.0100) ({r_i: None, r_t: [-809.960 -809.960 -809.960], eps: 0.01})
Step:   13800, Reward: [-425.754 -425.754 -425.754] [60.835], Avg: [-454.246 -454.246 -454.246] (0.2507) ({r_i: None, r_t: [-857.504 -857.504 -857.504], eps: 0.251})
Step:   63400, Reward: [-360.657 -360.657 -360.657] [74.444], Avg: [-448.145 -448.145 -448.145] (0.0100) ({r_i: None, r_t: [-792.537 -792.537 -792.537], eps: 0.01})
Step:   13900, Reward: [-427.131 -427.131 -427.131] [58.305], Avg: [-454.052 -454.052 -454.052] (0.2482) ({r_i: None, r_t: [-873.015 -873.015 -873.015], eps: 0.248})
Step:   63500, Reward: [-405.203 -405.203 -405.203] [55.614], Avg: [-448.078 -448.078 -448.078] (0.0100) ({r_i: None, r_t: [-790.049 -790.049 -790.049], eps: 0.01})
Step:   63600, Reward: [-393.455 -393.455 -393.455] [65.692], Avg: [-447.992 -447.992 -447.992] (0.0100) ({r_i: None, r_t: [-820.962 -820.962 -820.962], eps: 0.01})
Step:   14000, Reward: [-426.473 -426.473 -426.473] [45.962], Avg: [-453.856 -453.856 -453.856] (0.2457) ({r_i: None, r_t: [-841.485 -841.485 -841.485], eps: 0.246})
Step:   63700, Reward: [-426.840 -426.840 -426.840] [69.481], Avg: [-447.959 -447.959 -447.959] (0.0100) ({r_i: None, r_t: [-788.665 -788.665 -788.665], eps: 0.01})
Step:   14100, Reward: [-406.812 -406.812 -406.812] [68.403], Avg: [-453.525 -453.525 -453.525] (0.2433) ({r_i: None, r_t: [-816.972 -816.972 -816.972], eps: 0.243})
Step:   63800, Reward: [-452.501 -452.501 -452.501] [118.022], Avg: [-447.966 -447.966 -447.966] (0.0100) ({r_i: None, r_t: [-797.075 -797.075 -797.075], eps: 0.01})
Step:   14200, Reward: [-410.606 -410.606 -410.606] [74.835], Avg: [-453.225 -453.225 -453.225] (0.2409) ({r_i: None, r_t: [-842.109 -842.109 -842.109], eps: 0.241})
Step:   63900, Reward: [-391.452 -391.452 -391.452] [81.679], Avg: [-447.878 -447.878 -447.878] (0.0100) ({r_i: None, r_t: [-820.495 -820.495 -820.495], eps: 0.01})
Step:   64000, Reward: [-402.831 -402.831 -402.831] [63.515], Avg: [-447.807 -447.807 -447.807] (0.0100) ({r_i: None, r_t: [-828.970 -828.970 -828.970], eps: 0.01})
Step:   14300, Reward: [-415.420 -415.420 -415.420] [64.003], Avg: [-452.963 -452.963 -452.963] (0.2385) ({r_i: None, r_t: [-847.618 -847.618 -847.618], eps: 0.238})
Step:   64100, Reward: [-406.150 -406.150 -406.150] [105.011], Avg: [-447.742 -447.742 -447.742] (0.0100) ({r_i: None, r_t: [-827.780 -827.780 -827.780], eps: 0.01})
Step:   14400, Reward: [-408.251 -408.251 -408.251] [51.778], Avg: [-452.654 -452.654 -452.654] (0.2361) ({r_i: None, r_t: [-859.151 -859.151 -859.151], eps: 0.236})
Step:   64200, Reward: [-382.256 -382.256 -382.256] [95.310], Avg: [-447.641 -447.641 -447.641] (0.0100) ({r_i: None, r_t: [-844.675 -844.675 -844.675], eps: 0.01})
Step:   14500, Reward: [-370.412 -370.412 -370.412] [67.969], Avg: [-452.091 -452.091 -452.091] (0.2337) ({r_i: None, r_t: [-846.006 -846.006 -846.006], eps: 0.234})
Step:   64300, Reward: [-443.585 -443.585 -443.585] [65.258], Avg: [-447.634 -447.634 -447.634] (0.0100) ({r_i: None, r_t: [-887.659 -887.659 -887.659], eps: 0.01})
Step:   64400, Reward: [-401.294 -401.294 -401.294] [72.283], Avg: [-447.562 -447.562 -447.562] (0.0100) ({r_i: None, r_t: [-794.105 -794.105 -794.105], eps: 0.01})
Step:   14600, Reward: [-424.454 -424.454 -424.454] [58.968], Avg: [-451.903 -451.903 -451.903] (0.2314) ({r_i: None, r_t: [-876.484 -876.484 -876.484], eps: 0.231})
Step:   64500, Reward: [-403.539 -403.539 -403.539] [59.704], Avg: [-447.494 -447.494 -447.494] (0.0100) ({r_i: None, r_t: [-819.098 -819.098 -819.098], eps: 0.01})
Step:   14700, Reward: [-417.136 -417.136 -417.136] [85.099], Avg: [-451.668 -451.668 -451.668] (0.2291) ({r_i: None, r_t: [-848.406 -848.406 -848.406], eps: 0.229})
Step:   64600, Reward: [-400.666 -400.666 -400.666] [69.724], Avg: [-447.422 -447.422 -447.422] (0.0100) ({r_i: None, r_t: [-824.058 -824.058 -824.058], eps: 0.01})
Step:   14800, Reward: [-416.084 -416.084 -416.084] [100.665], Avg: [-451.429 -451.429 -451.429] (0.2268) ({r_i: None, r_t: [-864.675 -864.675 -864.675], eps: 0.227})
Step:   64700, Reward: [-416.910 -416.910 -416.910] [78.104], Avg: [-447.375 -447.375 -447.375] (0.0100) ({r_i: None, r_t: [-834.192 -834.192 -834.192], eps: 0.01})
Step:   64800, Reward: [-404.563 -404.563 -404.563] [66.791], Avg: [-447.309 -447.309 -447.309] (0.0100) ({r_i: None, r_t: [-834.270 -834.270 -834.270], eps: 0.01})
Step:   14900, Reward: [-456.086 -456.086 -456.086] [69.484], Avg: [-451.460 -451.460 -451.460] (0.2245) ({r_i: None, r_t: [-834.385 -834.385 -834.385], eps: 0.225})
Step:   64900, Reward: [-448.400 -448.400 -448.400] [80.767], Avg: [-447.311 -447.311 -447.311] (0.0100) ({r_i: None, r_t: [-819.733 -819.733 -819.733], eps: 0.01})
Step:   15000, Reward: [-382.470 -382.470 -382.470] [44.605], Avg: [-451.003 -451.003 -451.003] (0.2223) ({r_i: None, r_t: [-819.237 -819.237 -819.237], eps: 0.222})
Step:   65000, Reward: [-395.123 -395.123 -395.123] [97.307], Avg: [-447.230 -447.230 -447.230] (0.0100) ({r_i: None, r_t: [-850.758 -850.758 -850.758], eps: 0.01})
Step:   15100, Reward: [-429.305 -429.305 -429.305] [75.417], Avg: [-450.861 -450.861 -450.861] (0.2201) ({r_i: None, r_t: [-830.364 -830.364 -830.364], eps: 0.22})
Step:   65100, Reward: [-400.342 -400.342 -400.342] [48.356], Avg: [-447.158 -447.158 -447.158] (0.0100) ({r_i: None, r_t: [-842.400 -842.400 -842.400], eps: 0.01})
Step:   65200, Reward: [-389.226 -389.226 -389.226] [65.479], Avg: [-447.070 -447.070 -447.070] (0.0100) ({r_i: None, r_t: [-794.298 -794.298 -794.298], eps: 0.01})
Step:   15200, Reward: [-426.557 -426.557 -426.557] [50.961], Avg: [-450.702 -450.702 -450.702] (0.2179) ({r_i: None, r_t: [-820.006 -820.006 -820.006], eps: 0.218})
Step:   65300, Reward: [-404.734 -404.734 -404.734] [108.221], Avg: [-447.005 -447.005 -447.005] (0.0100) ({r_i: None, r_t: [-816.399 -816.399 -816.399], eps: 0.01})
Step:   15300, Reward: [-420.200 -420.200 -420.200] [66.397], Avg: [-450.504 -450.504 -450.504] (0.2157) ({r_i: None, r_t: [-820.106 -820.106 -820.106], eps: 0.216})
Step:   65400, Reward: [-409.680 -409.680 -409.680] [74.469], Avg: [-446.948 -446.948 -446.948] (0.0100) ({r_i: None, r_t: [-798.880 -798.880 -798.880], eps: 0.01})
Step:   65500, Reward: [-399.472 -399.472 -399.472] [69.729], Avg: [-446.876 -446.876 -446.876] (0.0100) ({r_i: None, r_t: [-822.998 -822.998 -822.998], eps: 0.01})
Step:   15400, Reward: [-416.219 -416.219 -416.219] [56.336], Avg: [-450.282 -450.282 -450.282] (0.2136) ({r_i: None, r_t: [-818.892 -818.892 -818.892], eps: 0.214})
Step:   65600, Reward: [-426.810 -426.810 -426.810] [97.514], Avg: [-446.845 -446.845 -446.845] (0.0100) ({r_i: None, r_t: [-813.315 -813.315 -813.315], eps: 0.01})
Step:   15500, Reward: [-391.561 -391.561 -391.561] [53.084], Avg: [-449.906 -449.906 -449.906] (0.2114) ({r_i: None, r_t: [-807.405 -807.405 -807.405], eps: 0.211})
Step:   65700, Reward: [-463.957 -463.957 -463.957] [91.519], Avg: [-446.871 -446.871 -446.871] (0.0100) ({r_i: None, r_t: [-773.256 -773.256 -773.256], eps: 0.01})
Step:   15600, Reward: [-401.560 -401.560 -401.560] [37.366], Avg: [-449.598 -449.598 -449.598] (0.2093) ({r_i: None, r_t: [-795.525 -795.525 -795.525], eps: 0.209})
Step:   65800, Reward: [-402.011 -402.011 -402.011] [61.214], Avg: [-446.803 -446.803 -446.803] (0.0100) ({r_i: None, r_t: [-856.518 -856.518 -856.518], eps: 0.01})
Step:   65900, Reward: [-414.834 -414.834 -414.834] [77.410], Avg: [-446.755 -446.755 -446.755] (0.0100) ({r_i: None, r_t: [-800.960 -800.960 -800.960], eps: 0.01})
Step:   15700, Reward: [-385.461 -385.461 -385.461] [61.293], Avg: [-449.192 -449.192 -449.192] (0.2072) ({r_i: None, r_t: [-797.066 -797.066 -797.066], eps: 0.207})
Step:   66000, Reward: [-425.393 -425.393 -425.393] [70.815], Avg: [-446.722 -446.722 -446.722] (0.0100) ({r_i: None, r_t: [-787.369 -787.369 -787.369], eps: 0.01})
Step:   15800, Reward: [-408.212 -408.212 -408.212] [54.194], Avg: [-448.934 -448.934 -448.934] (0.2052) ({r_i: None, r_t: [-801.165 -801.165 -801.165], eps: 0.205})
Step:   66100, Reward: [-401.009 -401.009 -401.009] [73.651], Avg: [-446.653 -446.653 -446.653] (0.0100) ({r_i: None, r_t: [-802.521 -802.521 -802.521], eps: 0.01})
Step:   15900, Reward: [-372.742 -372.742 -372.742] [41.257], Avg: [-448.458 -448.458 -448.458] (0.2031) ({r_i: None, r_t: [-812.575 -812.575 -812.575], eps: 0.203})
Step:   66200, Reward: [-421.848 -421.848 -421.848] [70.770], Avg: [-446.616 -446.616 -446.616] (0.0100) ({r_i: None, r_t: [-774.198 -774.198 -774.198], eps: 0.01})
Step:   66300, Reward: [-384.625 -384.625 -384.625] [97.728], Avg: [-446.522 -446.522 -446.522] (0.0100) ({r_i: None, r_t: [-801.548 -801.548 -801.548], eps: 0.01})
Step:   16000, Reward: [-398.757 -398.757 -398.757] [39.837], Avg: [-448.149 -448.149 -448.149] (0.2011) ({r_i: None, r_t: [-800.906 -800.906 -800.906], eps: 0.201})
Step:   66400, Reward: [-391.735 -391.735 -391.735] [32.603], Avg: [-446.440 -446.440 -446.440] (0.0100) ({r_i: None, r_t: [-791.366 -791.366 -791.366], eps: 0.01})
Step:   16100, Reward: [-425.185 -425.185 -425.185] [52.784], Avg: [-448.008 -448.008 -448.008] (0.1991) ({r_i: None, r_t: [-826.021 -826.021 -826.021], eps: 0.199})
Step:   66500, Reward: [-390.577 -390.577 -390.577] [67.193], Avg: [-446.356 -446.356 -446.356] (0.0100) ({r_i: None, r_t: [-787.942 -787.942 -787.942], eps: 0.01})
Step:   16200, Reward: [-425.643 -425.643 -425.643] [56.981], Avg: [-447.871 -447.871 -447.871] (0.1971) ({r_i: None, r_t: [-828.667 -828.667 -828.667], eps: 0.197})
Step:   66600, Reward: [-406.180 -406.180 -406.180] [66.401], Avg: [-446.296 -446.296 -446.296] (0.0100) ({r_i: None, r_t: [-789.197 -789.197 -789.197], eps: 0.01})
Step:   66700, Reward: [-391.563 -391.563 -391.563] [76.500], Avg: [-446.214 -446.214 -446.214] (0.0100) ({r_i: None, r_t: [-814.092 -814.092 -814.092], eps: 0.01})
Step:   16300, Reward: [-433.913 -433.913 -433.913] [65.276], Avg: [-447.785 -447.785 -447.785] (0.1951) ({r_i: None, r_t: [-818.409 -818.409 -818.409], eps: 0.195})
Step:   66800, Reward: [-400.183 -400.183 -400.183] [60.343], Avg: [-446.145 -446.145 -446.145] (0.0100) ({r_i: None, r_t: [-773.785 -773.785 -773.785], eps: 0.01})
Step:   16400, Reward: [-417.799 -417.799 -417.799] [31.801], Avg: [-447.604 -447.604 -447.604] (0.1932) ({r_i: None, r_t: [-845.353 -845.353 -845.353], eps: 0.193})
Step:   66900, Reward: [-412.486 -412.486 -412.486] [98.064], Avg: [-446.095 -446.095 -446.095] (0.0100) ({r_i: None, r_t: [-799.599 -799.599 -799.599], eps: 0.01})
Step:   16500, Reward: [-409.890 -409.890 -409.890] [61.718], Avg: [-447.376 -447.376 -447.376] (0.1913) ({r_i: None, r_t: [-794.753 -794.753 -794.753], eps: 0.191})
Step:   67000, Reward: [-398.476 -398.476 -398.476] [74.006], Avg: [-446.024 -446.024 -446.024] (0.0100) ({r_i: None, r_t: [-781.644 -781.644 -781.644], eps: 0.01})
Step:   67100, Reward: [-398.431 -398.431 -398.431] [70.089], Avg: [-445.953 -445.953 -445.953] (0.0100) ({r_i: None, r_t: [-795.301 -795.301 -795.301], eps: 0.01})
Step:   16600, Reward: [-404.974 -404.974 -404.974] [41.048], Avg: [-447.123 -447.123 -447.123] (0.1893) ({r_i: None, r_t: [-825.215 -825.215 -825.215], eps: 0.189})
Step:   67200, Reward: [-420.909 -420.909 -420.909] [70.621], Avg: [-445.916 -445.916 -445.916] (0.0100) ({r_i: None, r_t: [-856.211 -856.211 -856.211], eps: 0.01})
Step:   16700, Reward: [-429.493 -429.493 -429.493] [63.220], Avg: [-447.018 -447.018 -447.018] (0.1875) ({r_i: None, r_t: [-800.421 -800.421 -800.421], eps: 0.187})
Step:   67300, Reward: [-400.162 -400.162 -400.162] [54.409], Avg: [-445.848 -445.848 -445.848] (0.0100) ({r_i: None, r_t: [-817.177 -817.177 -817.177], eps: 0.01})
Step:   16800, Reward: [-406.763 -406.763 -406.763] [69.836], Avg: [-446.779 -446.779 -446.779] (0.1856) ({r_i: None, r_t: [-833.001 -833.001 -833.001], eps: 0.186})
Step:   67400, Reward: [-425.740 -425.740 -425.740] [51.733], Avg: [-445.818 -445.818 -445.818] (0.0100) ({r_i: None, r_t: [-782.481 -782.481 -782.481], eps: 0.01})
Step:   67500, Reward: [-453.621 -453.621 -453.621] [124.279], Avg: [-445.830 -445.830 -445.830] (0.0100) ({r_i: None, r_t: [-755.803 -755.803 -755.803], eps: 0.01})
Step:   16900, Reward: [-401.368 -401.368 -401.368] [46.940], Avg: [-446.512 -446.512 -446.512] (0.1837) ({r_i: None, r_t: [-771.240 -771.240 -771.240], eps: 0.184})
Step:   67600, Reward: [-399.482 -399.482 -399.482] [85.986], Avg: [-445.761 -445.761 -445.761] (0.0100) ({r_i: None, r_t: [-809.522 -809.522 -809.522], eps: 0.01})
Step:   17000, Reward: [-435.988 -435.988 -435.988] [61.640], Avg: [-446.451 -446.451 -446.451] (0.1819) ({r_i: None, r_t: [-768.484 -768.484 -768.484], eps: 0.182})
Step:   67700, Reward: [-384.266 -384.266 -384.266] [38.239], Avg: [-445.671 -445.671 -445.671] (0.0100) ({r_i: None, r_t: [-829.454 -829.454 -829.454], eps: 0.01})
Step:   17100, Reward: [-396.031 -396.031 -396.031] [59.129], Avg: [-446.158 -446.158 -446.158] (0.1801) ({r_i: None, r_t: [-799.700 -799.700 -799.700], eps: 0.18})
Step:   67800, Reward: [-392.117 -392.117 -392.117] [71.641], Avg: [-445.592 -445.592 -445.592] (0.0100) ({r_i: None, r_t: [-763.919 -763.919 -763.919], eps: 0.01})
Step:   67900, Reward: [-423.709 -423.709 -423.709] [88.758], Avg: [-445.560 -445.560 -445.560] (0.0100) ({r_i: None, r_t: [-854.662 -854.662 -854.662], eps: 0.01})
Step:   17200, Reward: [-418.539 -418.539 -418.539] [58.917], Avg: [-445.998 -445.998 -445.998] (0.1783) ({r_i: None, r_t: [-824.377 -824.377 -824.377], eps: 0.178})
Step:   68000, Reward: [-403.541 -403.541 -403.541] [66.665], Avg: [-445.498 -445.498 -445.498] (0.0100) ({r_i: None, r_t: [-795.143 -795.143 -795.143], eps: 0.01})
Step:   17300, Reward: [-426.609 -426.609 -426.609] [53.886], Avg: [-445.887 -445.887 -445.887] (0.1765) ({r_i: None, r_t: [-808.962 -808.962 -808.962], eps: 0.177})
Step:   68100, Reward: [-417.465 -417.465 -417.465] [90.818], Avg: [-445.457 -445.457 -445.457] (0.0100) ({r_i: None, r_t: [-810.489 -810.489 -810.489], eps: 0.01})
Step:   68200, Reward: [-392.106 -392.106 -392.106] [78.036], Avg: [-445.379 -445.379 -445.379] (0.0100) ({r_i: None, r_t: [-832.483 -832.483 -832.483], eps: 0.01})
Step:   17400, Reward: [-412.873 -412.873 -412.873] [55.639], Avg: [-445.698 -445.698 -445.698] (0.1748) ({r_i: None, r_t: [-831.624 -831.624 -831.624], eps: 0.175})
Step:   68300, Reward: [-389.234 -389.234 -389.234] [81.671], Avg: [-445.297 -445.297 -445.297] (0.0100) ({r_i: None, r_t: [-829.229 -829.229 -829.229], eps: 0.01})
Step:   17500, Reward: [-440.566 -440.566 -440.566] [57.120], Avg: [-445.669 -445.669 -445.669] (0.1730) ({r_i: None, r_t: [-845.767 -845.767 -845.767], eps: 0.173})
Step:   68400, Reward: [-412.831 -412.831 -412.831] [64.466], Avg: [-445.249 -445.249 -445.249] (0.0100) ({r_i: None, r_t: [-806.582 -806.582 -806.582], eps: 0.01})
Step:   17600, Reward: [-429.046 -429.046 -429.046] [67.029], Avg: [-445.575 -445.575 -445.575] (0.1713) ({r_i: None, r_t: [-829.213 -829.213 -829.213], eps: 0.171})
Step:   68500, Reward: [-413.662 -413.662 -413.662] [70.843], Avg: [-445.203 -445.203 -445.203] (0.0100) ({r_i: None, r_t: [-809.864 -809.864 -809.864], eps: 0.01})
Step:   68600, Reward: [-419.566 -419.566 -419.566] [84.662], Avg: [-445.166 -445.166 -445.166] (0.0100) ({r_i: None, r_t: [-786.991 -786.991 -786.991], eps: 0.01})
Step:   17700, Reward: [-443.254 -443.254 -443.254] [76.046], Avg: [-445.562 -445.562 -445.562] (0.1696) ({r_i: None, r_t: [-836.866 -836.866 -836.866], eps: 0.17})
Step:   68700, Reward: [-392.477 -392.477 -392.477] [96.038], Avg: [-445.089 -445.089 -445.089] (0.0100) ({r_i: None, r_t: [-845.737 -845.737 -845.737], eps: 0.01})
Step:   17800, Reward: [-395.684 -395.684 -395.684] [59.270], Avg: [-445.283 -445.283 -445.283] (0.1679) ({r_i: None, r_t: [-838.266 -838.266 -838.266], eps: 0.168})
Step:   68800, Reward: [-457.415 -457.415 -457.415] [81.925], Avg: [-445.107 -445.107 -445.107] (0.0100) ({r_i: None, r_t: [-825.182 -825.182 -825.182], eps: 0.01})
Step:   17900, Reward: [-400.340 -400.340 -400.340] [63.373], Avg: [-445.033 -445.033 -445.033] (0.1662) ({r_i: None, r_t: [-841.770 -841.770 -841.770], eps: 0.166})
Step:   68900, Reward: [-419.997 -419.997 -419.997] [79.065], Avg: [-445.071 -445.071 -445.071] (0.0100) ({r_i: None, r_t: [-827.296 -827.296 -827.296], eps: 0.01})
Step:   69000, Reward: [-442.869 -442.869 -442.869] [86.977], Avg: [-445.068 -445.068 -445.068] (0.0100) ({r_i: None, r_t: [-815.160 -815.160 -815.160], eps: 0.01})
Step:   18000, Reward: [-407.150 -407.150 -407.150] [59.755], Avg: [-444.824 -444.824 -444.824] (0.1646) ({r_i: None, r_t: [-828.290 -828.290 -828.290], eps: 0.165})
Step:   69100, Reward: [-416.868 -416.868 -416.868] [64.613], Avg: [-445.027 -445.027 -445.027] (0.0100) ({r_i: None, r_t: [-893.535 -893.535 -893.535], eps: 0.01})
Step:   18100, Reward: [-428.962 -428.962 -428.962] [51.890], Avg: [-444.737 -444.737 -444.737] (0.1629) ({r_i: None, r_t: [-856.637 -856.637 -856.637], eps: 0.163})
Step:   69200, Reward: [-459.616 -459.616 -459.616] [98.949], Avg: [-445.048 -445.048 -445.048] (0.0100) ({r_i: None, r_t: [-835.246 -835.246 -835.246], eps: 0.01})
Step:   69300, Reward: [-414.049 -414.049 -414.049] [82.819], Avg: [-445.003 -445.003 -445.003] (0.0100) ({r_i: None, r_t: [-824.405 -824.405 -824.405], eps: 0.01})
Step:   18200, Reward: [-403.113 -403.113 -403.113] [49.197], Avg: [-444.510 -444.510 -444.510] (0.1613) ({r_i: None, r_t: [-840.764 -840.764 -840.764], eps: 0.161})
Step:   69400, Reward: [-416.185 -416.185 -416.185] [97.683], Avg: [-444.962 -444.962 -444.962] (0.0100) ({r_i: None, r_t: [-828.328 -828.328 -828.328], eps: 0.01})
Step:   18300, Reward: [-436.827 -436.827 -436.827] [51.631], Avg: [-444.468 -444.468 -444.468] (0.1597) ({r_i: None, r_t: [-809.969 -809.969 -809.969], eps: 0.16})
Step:   69500, Reward: [-446.702 -446.702 -446.702] [93.429], Avg: [-444.964 -444.964 -444.964] (0.0100) ({r_i: None, r_t: [-824.627 -824.627 -824.627], eps: 0.01})
Step:   18400, Reward: [-451.744 -451.744 -451.744] [77.205], Avg: [-444.507 -444.507 -444.507] (0.1581) ({r_i: None, r_t: [-815.304 -815.304 -815.304], eps: 0.158})
Step:   69600, Reward: [-426.214 -426.214 -426.214] [88.090], Avg: [-444.937 -444.937 -444.937] (0.0100) ({r_i: None, r_t: [-836.634 -836.634 -836.634], eps: 0.01})
Step:   69700, Reward: [-455.603 -455.603 -455.603] [97.487], Avg: [-444.953 -444.953 -444.953] (0.0100) ({r_i: None, r_t: [-847.810 -847.810 -847.810], eps: 0.01})
Step:   18500, Reward: [-422.995 -422.995 -422.995] [66.303], Avg: [-444.391 -444.391 -444.391] (0.1565) ({r_i: None, r_t: [-818.195 -818.195 -818.195], eps: 0.157})
Step:   69800, Reward: [-421.987 -421.987 -421.987] [95.384], Avg: [-444.920 -444.920 -444.920] (0.0100) ({r_i: None, r_t: [-900.523 -900.523 -900.523], eps: 0.01})
Step:   18600, Reward: [-449.450 -449.450 -449.450] [58.307], Avg: [-444.419 -444.419 -444.419] (0.1549) ({r_i: None, r_t: [-827.469 -827.469 -827.469], eps: 0.155})
Step:   69900, Reward: [-445.042 -445.042 -445.042] [96.697], Avg: [-444.920 -444.920 -444.920] (0.0100) ({r_i: None, r_t: [-873.107 -873.107 -873.107], eps: 0.01})
Step:   18700, Reward: [-408.205 -408.205 -408.205] [65.326], Avg: [-444.226 -444.226 -444.226] (0.1534) ({r_i: None, r_t: [-876.502 -876.502 -876.502], eps: 0.153})
Step:   70000, Reward: [-441.639 -441.639 -441.639] [63.021], Avg: [-444.915 -444.915 -444.915] (0.0100) ({r_i: None, r_t: [-836.859 -836.859 -836.859], eps: 0.01})
Step:   70100, Reward: [-390.740 -390.740 -390.740] [53.334], Avg: [-444.838 -444.838 -444.838] (0.0100) ({r_i: None, r_t: [-861.279 -861.279 -861.279], eps: 0.01})
Step:   18800, Reward: [-423.839 -423.839 -423.839] [55.743], Avg: [-444.118 -444.118 -444.118] (0.1519) ({r_i: None, r_t: [-874.512 -874.512 -874.512], eps: 0.152})
Step:   70200, Reward: [-471.887 -471.887 -471.887] [84.131], Avg: [-444.877 -444.877 -444.877] (0.0100) ({r_i: None, r_t: [-848.008 -848.008 -848.008], eps: 0.01})
Step:   18900, Reward: [-401.718 -401.718 -401.718] [56.837], Avg: [-443.895 -443.895 -443.895] (0.1504) ({r_i: None, r_t: [-823.035 -823.035 -823.035], eps: 0.15})
Step:   70300, Reward: [-438.642 -438.642 -438.642] [80.123], Avg: [-444.868 -444.868 -444.868] (0.0100) ({r_i: None, r_t: [-889.020 -889.020 -889.020], eps: 0.01})
Step:   19000, Reward: [-405.199 -405.199 -405.199] [61.232], Avg: [-443.692 -443.692 -443.692] (0.1489) ({r_i: None, r_t: [-781.988 -781.988 -781.988], eps: 0.149})
Step:   70400, Reward: [-416.497 -416.497 -416.497] [83.019], Avg: [-444.827 -444.827 -444.827] (0.0100) ({r_i: None, r_t: [-864.855 -864.855 -864.855], eps: 0.01})
Step:   70500, Reward: [-459.844 -459.844 -459.844] [93.533], Avg: [-444.849 -444.849 -444.849] (0.0100) ({r_i: None, r_t: [-872.243 -872.243 -872.243], eps: 0.01})
Step:   19100, Reward: [-410.202 -410.202 -410.202] [54.663], Avg: [-443.518 -443.518 -443.518] (0.1474) ({r_i: None, r_t: [-820.806 -820.806 -820.806], eps: 0.147})
Step:   70600, Reward: [-437.185 -437.185 -437.185] [131.553], Avg: [-444.838 -444.838 -444.838] (0.0100) ({r_i: None, r_t: [-882.198 -882.198 -882.198], eps: 0.01})
Step:   19200, Reward: [-409.217 -409.217 -409.217] [50.738], Avg: [-443.340 -443.340 -443.340] (0.1459) ({r_i: None, r_t: [-817.091 -817.091 -817.091], eps: 0.146})
Step:   70700, Reward: [-421.157 -421.157 -421.157] [99.901], Avg: [-444.804 -444.804 -444.804] (0.0100) ({r_i: None, r_t: [-861.877 -861.877 -861.877], eps: 0.01})
Step:   19300, Reward: [-434.364 -434.364 -434.364] [53.893], Avg: [-443.294 -443.294 -443.294] (0.1444) ({r_i: None, r_t: [-844.201 -844.201 -844.201], eps: 0.144})
Step:   70800, Reward: [-382.312 -382.312 -382.312] [80.187], Avg: [-444.716 -444.716 -444.716] (0.0100) ({r_i: None, r_t: [-860.031 -860.031 -860.031], eps: 0.01})
Step:   70900, Reward: [-415.636 -415.636 -415.636] [90.409], Avg: [-444.675 -444.675 -444.675] (0.0100) ({r_i: None, r_t: [-894.485 -894.485 -894.485], eps: 0.01})
Step:   19400, Reward: [-412.250 -412.250 -412.250] [65.597], Avg: [-443.135 -443.135 -443.135] (0.1430) ({r_i: None, r_t: [-822.868 -822.868 -822.868], eps: 0.143})
Step:   71000, Reward: [-444.183 -444.183 -444.183] [66.579], Avg: [-444.675 -444.675 -444.675] (0.0100) ({r_i: None, r_t: [-864.309 -864.309 -864.309], eps: 0.01})
Step:   19500, Reward: [-433.950 -433.950 -433.950] [48.757], Avg: [-443.088 -443.088 -443.088] (0.1416) ({r_i: None, r_t: [-809.700 -809.700 -809.700], eps: 0.142})
Step:   71100, Reward: [-419.950 -419.950 -419.950] [71.455], Avg: [-444.640 -444.640 -444.640] (0.0100) ({r_i: None, r_t: [-858.498 -858.498 -858.498], eps: 0.01})
Step:   19600, Reward: [-415.398 -415.398 -415.398] [44.285], Avg: [-442.947 -442.947 -442.947] (0.1402) ({r_i: None, r_t: [-820.009 -820.009 -820.009], eps: 0.14})
Step:   71200, Reward: [-376.000 -376.000 -376.000] [75.365], Avg: [-444.544 -444.544 -444.544] (0.0100) ({r_i: None, r_t: [-935.272 -935.272 -935.272], eps: 0.01})
Step:   71300, Reward: [-418.369 -418.369 -418.369] [82.602], Avg: [-444.507 -444.507 -444.507] (0.0100) ({r_i: None, r_t: [-857.858 -857.858 -857.858], eps: 0.01})
Step:   19700, Reward: [-420.040 -420.040 -420.040] [49.697], Avg: [-442.832 -442.832 -442.832] (0.1388) ({r_i: None, r_t: [-847.390 -847.390 -847.390], eps: 0.139})
Step:   71400, Reward: [-421.109 -421.109 -421.109] [84.281], Avg: [-444.474 -444.474 -444.474] (0.0100) ({r_i: None, r_t: [-859.268 -859.268 -859.268], eps: 0.01})
Step:   19800, Reward: [-425.731 -425.731 -425.731] [67.309], Avg: [-442.746 -442.746 -442.746] (0.1374) ({r_i: None, r_t: [-840.611 -840.611 -840.611], eps: 0.137})
Step:   71500, Reward: [-414.001 -414.001 -414.001] [110.085], Avg: [-444.432 -444.432 -444.432] (0.0100) ({r_i: None, r_t: [-851.408 -851.408 -851.408], eps: 0.01})
Step:   71600, Reward: [-449.006 -449.006 -449.006] [109.580], Avg: [-444.438 -444.438 -444.438] (0.0100) ({r_i: None, r_t: [-875.249 -875.249 -875.249], eps: 0.01})
Step:   19900, Reward: [-448.890 -448.890 -448.890] [76.877], Avg: [-442.776 -442.776 -442.776] (0.1360) ({r_i: None, r_t: [-802.832 -802.832 -802.832], eps: 0.136})
Step:   71700, Reward: [-451.657 -451.657 -451.657] [97.228], Avg: [-444.448 -444.448 -444.448] (0.0100) ({r_i: None, r_t: [-875.991 -875.991 -875.991], eps: 0.01})
Step:   20000, Reward: [-431.862 -431.862 -431.862] [63.984], Avg: [-442.722 -442.722 -442.722] (0.1347) ({r_i: None, r_t: [-785.121 -785.121 -785.121], eps: 0.135})
Step:   71800, Reward: [-422.787 -422.787 -422.787] [75.593], Avg: [-444.418 -444.418 -444.418] (0.0100) ({r_i: None, r_t: [-897.390 -897.390 -897.390], eps: 0.01})
Step:   20100, Reward: [-429.266 -429.266 -429.266] [58.108], Avg: [-442.655 -442.655 -442.655] (0.1333) ({r_i: None, r_t: [-829.376 -829.376 -829.376], eps: 0.133})
Step:   71900, Reward: [-423.204 -423.204 -423.204] [105.227], Avg: [-444.389 -444.389 -444.389] (0.0100) ({r_i: None, r_t: [-845.898 -845.898 -845.898], eps: 0.01})
Step:   72000, Reward: [-486.016 -486.016 -486.016] [120.494], Avg: [-444.446 -444.446 -444.446] (0.0100) ({r_i: None, r_t: [-841.519 -841.519 -841.519], eps: 0.01})
Step:   20200, Reward: [-401.848 -401.848 -401.848] [50.132], Avg: [-442.454 -442.454 -442.454] (0.1320) ({r_i: None, r_t: [-821.891 -821.891 -821.891], eps: 0.132})
Step:   72100, Reward: [-450.893 -450.893 -450.893] [120.604], Avg: [-444.455 -444.455 -444.455] (0.0100) ({r_i: None, r_t: [-871.521 -871.521 -871.521], eps: 0.01})
Step:   20300, Reward: [-389.808 -389.808 -389.808] [54.325], Avg: [-442.196 -442.196 -442.196] (0.1307) ({r_i: None, r_t: [-886.222 -886.222 -886.222], eps: 0.131})
Step:   72200, Reward: [-487.389 -487.389 -487.389] [103.061], Avg: [-444.515 -444.515 -444.515] (0.0100) ({r_i: None, r_t: [-890.758 -890.758 -890.758], eps: 0.01})
Step:   20400, Reward: [-442.481 -442.481 -442.481] [49.901], Avg: [-442.198 -442.198 -442.198] (0.1294) ({r_i: None, r_t: [-761.514 -761.514 -761.514], eps: 0.129})
Step:   72300, Reward: [-479.430 -479.430 -479.430] [99.847], Avg: [-444.563 -444.563 -444.563] (0.0100) ({r_i: None, r_t: [-950.175 -950.175 -950.175], eps: 0.01})
Step:   72400, Reward: [-433.103 -433.103 -433.103] [85.228], Avg: [-444.547 -444.547 -444.547] (0.0100) ({r_i: None, r_t: [-961.595 -961.595 -961.595], eps: 0.01})
Step:   20500, Reward: [-432.228 -432.228 -432.228] [57.100], Avg: [-442.149 -442.149 -442.149] (0.1281) ({r_i: None, r_t: [-795.591 -795.591 -795.591], eps: 0.128})
Step:   72500, Reward: [-436.951 -436.951 -436.951] [77.130], Avg: [-444.537 -444.537 -444.537] (0.0100) ({r_i: None, r_t: [-852.839 -852.839 -852.839], eps: 0.01})
Step:   20600, Reward: [-429.259 -429.259 -429.259] [28.504], Avg: [-442.087 -442.087 -442.087] (0.1268) ({r_i: None, r_t: [-853.078 -853.078 -853.078], eps: 0.127})
Step:   72600, Reward: [-466.136 -466.136 -466.136] [117.699], Avg: [-444.566 -444.566 -444.566] (0.0100) ({r_i: None, r_t: [-901.921 -901.921 -901.921], eps: 0.01})
Step:   20700, Reward: [-450.188 -450.188 -450.188] [71.266], Avg: [-442.126 -442.126 -442.126] (0.1255) ({r_i: None, r_t: [-821.937 -821.937 -821.937], eps: 0.126})
Step:   72700, Reward: [-488.303 -488.303 -488.303] [129.956], Avg: [-444.626 -444.626 -444.626] (0.0100) ({r_i: None, r_t: [-913.540 -913.540 -913.540], eps: 0.01})
Step:   72800, Reward: [-476.074 -476.074 -476.074] [97.632], Avg: [-444.670 -444.670 -444.670] (0.0100) ({r_i: None, r_t: [-893.474 -893.474 -893.474], eps: 0.01})
Step:   20800, Reward: [-443.705 -443.705 -443.705] [53.496], Avg: [-442.134 -442.134 -442.134] (0.1243) ({r_i: None, r_t: [-852.447 -852.447 -852.447], eps: 0.124})
Step:   72900, Reward: [-438.681 -438.681 -438.681] [90.936], Avg: [-444.661 -444.661 -444.661] (0.0100) ({r_i: None, r_t: [-958.637 -958.637 -958.637], eps: 0.01})
Step:   20900, Reward: [-418.305 -418.305 -418.305] [70.323], Avg: [-442.020 -442.020 -442.020] (0.1230) ({r_i: None, r_t: [-832.986 -832.986 -832.986], eps: 0.123})
Step:   73000, Reward: [-522.849 -522.849 -522.849] [178.781], Avg: [-444.768 -444.768 -444.768] (0.0100) ({r_i: None, r_t: [-920.674 -920.674 -920.674], eps: 0.01})
Step:   21000, Reward: [-438.776 -438.776 -438.776] [56.990], Avg: [-442.005 -442.005 -442.005] (0.1218) ({r_i: None, r_t: [-821.938 -821.938 -821.938], eps: 0.122})
Step:   73100, Reward: [-547.321 -547.321 -547.321] [168.940], Avg: [-444.908 -444.908 -444.908] (0.0100) ({r_i: None, r_t: [-951.568 -951.568 -951.568], eps: 0.01})
Step:   73200, Reward: [-504.835 -504.835 -504.835] [135.345], Avg: [-444.990 -444.990 -444.990] (0.0100) ({r_i: None, r_t: [-1040.498 -1040.498 -1040.498], eps: 0.01})
Step:   21100, Reward: [-397.150 -397.150 -397.150] [54.994], Avg: [-441.793 -441.793 -441.793] (0.1206) ({r_i: None, r_t: [-856.543 -856.543 -856.543], eps: 0.121})
Step:   73300, Reward: [-472.232 -472.232 -472.232] [189.394], Avg: [-445.027 -445.027 -445.027] (0.0100) ({r_i: None, r_t: [-1057.911 -1057.911 -1057.911], eps: 0.01})
Step:   21200, Reward: [-410.890 -410.890 -410.890] [80.814], Avg: [-441.648 -441.648 -441.648] (0.1194) ({r_i: None, r_t: [-860.310 -860.310 -860.310], eps: 0.119})
Step:   73400, Reward: [-454.817 -454.817 -454.817] [82.087], Avg: [-445.041 -445.041 -445.041] (0.0100) ({r_i: None, r_t: [-916.621 -916.621 -916.621], eps: 0.01})
Step:   21300, Reward: [-397.135 -397.135 -397.135] [71.511], Avg: [-441.440 -441.440 -441.440] (0.1182) ({r_i: None, r_t: [-833.501 -833.501 -833.501], eps: 0.118})
Step:   73500, Reward: [-514.337 -514.337 -514.337] [152.559], Avg: [-445.135 -445.135 -445.135] (0.0100) ({r_i: None, r_t: [-1131.588 -1131.588 -1131.588], eps: 0.01})
Step:   73600, Reward: [-487.073 -487.073 -487.073] [111.616], Avg: [-445.192 -445.192 -445.192] (0.0100) ({r_i: None, r_t: [-1061.510 -1061.510 -1061.510], eps: 0.01})
Step:   21400, Reward: [-414.780 -414.780 -414.780] [50.191], Avg: [-441.316 -441.316 -441.316] (0.1170) ({r_i: None, r_t: [-827.610 -827.610 -827.610], eps: 0.117})
Step:   73700, Reward: [-524.349 -524.349 -524.349] [166.266], Avg: [-445.299 -445.299 -445.299] (0.0100) ({r_i: None, r_t: [-1129.556 -1129.556 -1129.556], eps: 0.01})
Step:   21500, Reward: [-436.334 -436.334 -436.334] [38.112], Avg: [-441.293 -441.293 -441.293] (0.1159) ({r_i: None, r_t: [-825.954 -825.954 -825.954], eps: 0.116})
Step:   73800, Reward: [-496.554 -496.554 -496.554] [163.887], Avg: [-445.368 -445.368 -445.368] (0.0100) ({r_i: None, r_t: [-1070.424 -1070.424 -1070.424], eps: 0.01})
Step:   21600, Reward: [-409.885 -409.885 -409.885] [82.177], Avg: [-441.148 -441.148 -441.148] (0.1147) ({r_i: None, r_t: [-865.034 -865.034 -865.034], eps: 0.115})
Step:   73900, Reward: [-707.432 -707.432 -707.432] [348.476], Avg: [-445.722 -445.722 -445.722] (0.0100) ({r_i: None, r_t: [-1165.249 -1165.249 -1165.249], eps: 0.01})
Step:   74000, Reward: [-583.285 -583.285 -583.285] [245.163], Avg: [-445.908 -445.908 -445.908] (0.0100) ({r_i: None, r_t: [-1099.398 -1099.398 -1099.398], eps: 0.01})
Step:   21700, Reward: [-410.283 -410.283 -410.283] [81.765], Avg: [-441.007 -441.007 -441.007] (0.1136) ({r_i: None, r_t: [-908.614 -908.614 -908.614], eps: 0.114})
Step:   74100, Reward: [-627.893 -627.893 -627.893] [272.986], Avg: [-446.153 -446.153 -446.153] (0.0100) ({r_i: None, r_t: [-1177.023 -1177.023 -1177.023], eps: 0.01})
Step:   21800, Reward: [-419.526 -419.526 -419.526] [68.437], Avg: [-440.909 -440.909 -440.909] (0.1124) ({r_i: None, r_t: [-828.735 -828.735 -828.735], eps: 0.112})
Step:   74200, Reward: [-740.460 -740.460 -740.460] [402.499], Avg: [-446.549 -446.549 -446.549] (0.0100) ({r_i: None, r_t: [-1581.453 -1581.453 -1581.453], eps: 0.01})
Step:   21900, Reward: [-398.853 -398.853 -398.853] [71.990], Avg: [-440.717 -440.717 -440.717] (0.1113) ({r_i: None, r_t: [-855.154 -855.154 -855.154], eps: 0.111})
Step:   74300, Reward: [-848.457 -848.457 -848.457] [435.786], Avg: [-447.090 -447.090 -447.090] (0.0100) ({r_i: None, r_t: [-1261.485 -1261.485 -1261.485], eps: 0.01})
Step:   74400, Reward: [-893.607 -893.607 -893.607] [504.353], Avg: [-447.689 -447.689 -447.689] (0.0100) ({r_i: None, r_t: [-1519.164 -1519.164 -1519.164], eps: 0.01})
Step:   22000, Reward: [-435.516 -435.516 -435.516] [75.761], Avg: [-440.694 -440.694 -440.694] (0.1102) ({r_i: None, r_t: [-840.866 -840.866 -840.866], eps: 0.11})
Step:   74500, Reward: [-557.792 -557.792 -557.792] [147.505], Avg: [-447.837 -447.837 -447.837] (0.0100) ({r_i: None, r_t: [-1231.579 -1231.579 -1231.579], eps: 0.01})
Step:   22100, Reward: [-435.027 -435.027 -435.027] [52.161], Avg: [-440.668 -440.668 -440.668] (0.1091) ({r_i: None, r_t: [-816.631 -816.631 -816.631], eps: 0.109})
Step:   74600, Reward: [-928.147 -928.147 -928.147] [592.820], Avg: [-448.480 -448.480 -448.480] (0.0100) ({r_i: None, r_t: [-1441.261 -1441.261 -1441.261], eps: 0.01})
Step:   22200, Reward: [-393.927 -393.927 -393.927] [59.744], Avg: [-440.459 -440.459 -440.459] (0.1080) ({r_i: None, r_t: [-837.714 -837.714 -837.714], eps: 0.108})
Step:   74700, Reward: [-741.353 -741.353 -741.353] [353.336], Avg: [-448.871 -448.871 -448.871] (0.0100) ({r_i: None, r_t: [-1650.781 -1650.781 -1650.781], eps: 0.01})
Step:   74800, Reward: [-761.617 -761.617 -761.617] [406.498], Avg: [-449.289 -449.289 -449.289] (0.0100) ({r_i: None, r_t: [-1798.542 -1798.542 -1798.542], eps: 0.01})
Step:   22300, Reward: [-411.975 -411.975 -411.975] [58.606], Avg: [-440.332 -440.332 -440.332] (0.1069) ({r_i: None, r_t: [-831.793 -831.793 -831.793], eps: 0.107})
Step:   74900, Reward: [-797.860 -797.860 -797.860] [466.078], Avg: [-449.753 -449.753 -449.753] (0.0100) ({r_i: None, r_t: [-1630.303 -1630.303 -1630.303], eps: 0.01})
Step:   22400, Reward: [-393.925 -393.925 -393.925] [54.473], Avg: [-440.125 -440.125 -440.125] (0.1059) ({r_i: None, r_t: [-842.382 -842.382 -842.382], eps: 0.106})
Step:   75000, Reward: [-910.795 -910.795 -910.795] [552.209], Avg: [-450.367 -450.367 -450.367] (0.0100) ({r_i: None, r_t: [-1428.734 -1428.734 -1428.734], eps: 0.01})
Step:   22500, Reward: [-410.259 -410.259 -410.259] [47.672], Avg: [-439.993 -439.993 -439.993] (0.1048) ({r_i: None, r_t: [-817.565 -817.565 -817.565], eps: 0.105})
Step:   75100, Reward: [-907.904 -907.904 -907.904] [427.836], Avg: [-450.976 -450.976 -450.976] (0.0100) ({r_i: None, r_t: [-1863.940 -1863.940 -1863.940], eps: 0.01})
Step:   75200, Reward: [-751.392 -751.392 -751.392] [445.484], Avg: [-451.375 -451.375 -451.375] (0.0100) ({r_i: None, r_t: [-1549.651 -1549.651 -1549.651], eps: 0.01})
Step:   22600, Reward: [-417.677 -417.677 -417.677] [79.816], Avg: [-439.895 -439.895 -439.895] (0.1038) ({r_i: None, r_t: [-851.590 -851.590 -851.590], eps: 0.104})
Step:   75300, Reward: [-1121.820 -1121.820 -1121.820] [526.707], Avg: [-452.264 -452.264 -452.264] (0.0100) ({r_i: None, r_t: [-1600.303 -1600.303 -1600.303], eps: 0.01})
Step:   22700, Reward: [-434.418 -434.418 -434.418] [91.154], Avg: [-439.871 -439.871 -439.871] (0.1027) ({r_i: None, r_t: [-852.832 -852.832 -852.832], eps: 0.103})
Step:   75400, Reward: [-738.985 -738.985 -738.985] [341.954], Avg: [-452.644 -452.644 -452.644] (0.0100) ({r_i: None, r_t: [-1507.267 -1507.267 -1507.267], eps: 0.01})
Step:   75500, Reward: [-662.611 -662.611 -662.611] [219.098], Avg: [-452.921 -452.921 -452.921] (0.0100) ({r_i: None, r_t: [-1579.911 -1579.911 -1579.911], eps: 0.01})
Step:   22800, Reward: [-413.062 -413.062 -413.062] [59.007], Avg: [-439.754 -439.754 -439.754] (0.1017) ({r_i: None, r_t: [-794.516 -794.516 -794.516], eps: 0.102})
Step:   75600, Reward: [-783.558 -783.558 -783.558] [527.297], Avg: [-453.358 -453.358 -453.358] (0.0100) ({r_i: None, r_t: [-1520.029 -1520.029 -1520.029], eps: 0.01})
Step:   22900, Reward: [-414.020 -414.020 -414.020] [60.478], Avg: [-439.642 -439.642 -439.642] (0.1007) ({r_i: None, r_t: [-823.050 -823.050 -823.050], eps: 0.101})
Step:   75700, Reward: [-706.776 -706.776 -706.776] [394.493], Avg: [-453.692 -453.692 -453.692] (0.0100) ({r_i: None, r_t: [-1393.818 -1393.818 -1393.818], eps: 0.01})
Step:   23000, Reward: [-393.829 -393.829 -393.829] [84.348], Avg: [-439.444 -439.444 -439.444] (0.0997) ({r_i: None, r_t: [-860.563 -860.563 -860.563], eps: 0.1})
Step:   75800, Reward: [-844.937 -844.937 -844.937] [451.493], Avg: [-454.208 -454.208 -454.208] (0.0100) ({r_i: None, r_t: [-1436.246 -1436.246 -1436.246], eps: 0.01})
Step:   75900, Reward: [-659.499 -659.499 -659.499] [351.245], Avg: [-454.478 -454.478 -454.478] (0.0100) ({r_i: None, r_t: [-1292.387 -1292.387 -1292.387], eps: 0.01})
Step:   23100, Reward: [-424.222 -424.222 -424.222] [50.346], Avg: [-439.378 -439.378 -439.378] (0.0987) ({r_i: None, r_t: [-837.450 -837.450 -837.450], eps: 0.099})
Step:   76000, Reward: [-781.192 -781.192 -781.192] [437.858], Avg: [-454.907 -454.907 -454.907] (0.0100) ({r_i: None, r_t: [-1447.075 -1447.075 -1447.075], eps: 0.01})
Step:   23200, Reward: [-414.349 -414.349 -414.349] [61.085], Avg: [-439.271 -439.271 -439.271] (0.0977) ({r_i: None, r_t: [-851.011 -851.011 -851.011], eps: 0.098})
Step:   76100, Reward: [-731.893 -731.893 -731.893] [354.616], Avg: [-455.271 -455.271 -455.271] (0.0100) ({r_i: None, r_t: [-1529.526 -1529.526 -1529.526], eps: 0.01})
Step:   23300, Reward: [-423.301 -423.301 -423.301] [68.216], Avg: [-439.202 -439.202 -439.202] (0.0967) ({r_i: None, r_t: [-820.319 -820.319 -820.319], eps: 0.097})
Step:   76200, Reward: [-686.855 -686.855 -686.855] [327.046], Avg: [-455.574 -455.574 -455.574] (0.0100) ({r_i: None, r_t: [-1427.571 -1427.571 -1427.571], eps: 0.01})
Step:   76300, Reward: [-853.529 -853.529 -853.529] [363.360], Avg: [-456.095 -456.095 -456.095] (0.0100) ({r_i: None, r_t: [-1284.618 -1284.618 -1284.618], eps: 0.01})
Step:   23400, Reward: [-443.686 -443.686 -443.686] [46.873], Avg: [-439.221 -439.221 -439.221] (0.0958) ({r_i: None, r_t: [-789.846 -789.846 -789.846], eps: 0.096})
Step:   76400, Reward: [-631.177 -631.177 -631.177] [320.741], Avg: [-456.324 -456.324 -456.324] (0.0100) ({r_i: None, r_t: [-1604.265 -1604.265 -1604.265], eps: 0.01})
Step:   23500, Reward: [-418.357 -418.357 -418.357] [61.669], Avg: [-439.133 -439.133 -439.133] (0.0948) ({r_i: None, r_t: [-856.163 -856.163 -856.163], eps: 0.095})
Step:   76500, Reward: [-722.132 -722.132 -722.132] [224.107], Avg: [-456.671 -456.671 -456.671] (0.0100) ({r_i: None, r_t: [-1656.575 -1656.575 -1656.575], eps: 0.01})
Step:   76600, Reward: [-699.420 -699.420 -699.420] [400.655], Avg: [-456.988 -456.988 -456.988] (0.0100) ({r_i: None, r_t: [-1329.936 -1329.936 -1329.936], eps: 0.01})
Step:   23600, Reward: [-425.089 -425.089 -425.089] [75.834], Avg: [-439.074 -439.074 -439.074] (0.0939) ({r_i: None, r_t: [-848.978 -848.978 -848.978], eps: 0.094})
Step:   76700, Reward: [-686.169 -686.169 -686.169] [261.701], Avg: [-457.286 -457.286 -457.286] (0.0100) ({r_i: None, r_t: [-1303.797 -1303.797 -1303.797], eps: 0.01})
Step:   23700, Reward: [-447.212 -447.212 -447.212] [74.815], Avg: [-439.108 -439.108 -439.108] (0.0929) ({r_i: None, r_t: [-850.030 -850.030 -850.030], eps: 0.093})
Step:   76800, Reward: [-604.499 -604.499 -604.499] [194.937], Avg: [-457.477 -457.477 -457.477] (0.0100) ({r_i: None, r_t: [-1318.207 -1318.207 -1318.207], eps: 0.01})
Step:   23800, Reward: [-387.632 -387.632 -387.632] [51.554], Avg: [-438.893 -438.893 -438.893] (0.0920) ({r_i: None, r_t: [-833.518 -833.518 -833.518], eps: 0.092})
Step:   76900, Reward: [-753.765 -753.765 -753.765] [418.977], Avg: [-457.862 -457.862 -457.862] (0.0100) ({r_i: None, r_t: [-1544.063 -1544.063 -1544.063], eps: 0.01})
Step:   77000, Reward: [-576.955 -576.955 -576.955] [122.058], Avg: [-458.017 -458.017 -458.017] (0.0100) ({r_i: None, r_t: [-1359.328 -1359.328 -1359.328], eps: 0.01})
Step:   23900, Reward: [-412.747 -412.747 -412.747] [66.948], Avg: [-438.784 -438.784 -438.784] (0.0911) ({r_i: None, r_t: [-834.828 -834.828 -834.828], eps: 0.091})
Step:   77100, Reward: [-698.453 -698.453 -698.453] [509.702], Avg: [-458.328 -458.328 -458.328] (0.0100) ({r_i: None, r_t: [-1236.710 -1236.710 -1236.710], eps: 0.01})
Step:   24000, Reward: [-415.255 -415.255 -415.255] [68.759], Avg: [-438.686 -438.686 -438.686] (0.0902) ({r_i: None, r_t: [-866.696 -866.696 -866.696], eps: 0.09})
Step:   77200, Reward: [-626.494 -626.494 -626.494] [288.973], Avg: [-458.546 -458.546 -458.546] (0.0100) ({r_i: None, r_t: [-1176.761 -1176.761 -1176.761], eps: 0.01})
Step:   24100, Reward: [-414.349 -414.349 -414.349] [71.159], Avg: [-438.585 -438.585 -438.585] (0.0893) ({r_i: None, r_t: [-866.489 -866.489 -866.489], eps: 0.089})
Step:   77300, Reward: [-601.169 -601.169 -601.169] [298.621], Avg: [-458.730 -458.730 -458.730] (0.0100) ({r_i: None, r_t: [-1180.548 -1180.548 -1180.548], eps: 0.01})
Step:   77400, Reward: [-657.644 -657.644 -657.644] [404.030], Avg: [-458.987 -458.987 -458.987] (0.0100) ({r_i: None, r_t: [-1300.867 -1300.867 -1300.867], eps: 0.01})
Step:   24200, Reward: [-461.096 -461.096 -461.096] [70.832], Avg: [-438.678 -438.678 -438.678] (0.0884) ({r_i: None, r_t: [-842.952 -842.952 -842.952], eps: 0.088})
Step:   77500, Reward: [-717.533 -717.533 -717.533] [270.682], Avg: [-459.320 -459.320 -459.320] (0.0100) ({r_i: None, r_t: [-1139.512 -1139.512 -1139.512], eps: 0.01})
Step:   24300, Reward: [-407.524 -407.524 -407.524] [65.121], Avg: [-438.550 -438.550 -438.550] (0.0875) ({r_i: None, r_t: [-863.526 -863.526 -863.526], eps: 0.088})
Step:   77600, Reward: [-523.465 -523.465 -523.465] [194.702], Avg: [-459.402 -459.402 -459.402] (0.0100) ({r_i: None, r_t: [-1386.645 -1386.645 -1386.645], eps: 0.01})
Step:   24400, Reward: [-407.257 -407.257 -407.257] [65.899], Avg: [-438.423 -438.423 -438.423] (0.0866) ({r_i: None, r_t: [-850.840 -850.840 -850.840], eps: 0.087})
Step:   77700, Reward: [-584.127 -584.127 -584.127] [119.730], Avg: [-459.563 -459.563 -459.563] (0.0100) ({r_i: None, r_t: [-1291.129 -1291.129 -1291.129], eps: 0.01})
Step:   77800, Reward: [-601.153 -601.153 -601.153] [223.438], Avg: [-459.744 -459.744 -459.744] (0.0100) ({r_i: None, r_t: [-1147.688 -1147.688 -1147.688], eps: 0.01})
Step:   24500, Reward: [-453.088 -453.088 -453.088] [68.185], Avg: [-438.482 -438.482 -438.482] (0.0858) ({r_i: None, r_t: [-831.784 -831.784 -831.784], eps: 0.086})
Step:   77900, Reward: [-784.897 -784.897 -784.897] [428.320], Avg: [-460.161 -460.161 -460.161] (0.0100) ({r_i: None, r_t: [-1374.879 -1374.879 -1374.879], eps: 0.01})
Step:   24600, Reward: [-441.798 -441.798 -441.798] [62.090], Avg: [-438.496 -438.496 -438.496] (0.0849) ({r_i: None, r_t: [-865.171 -865.171 -865.171], eps: 0.085})
Step:   78000, Reward: [-708.167 -708.167 -708.167] [400.219], Avg: [-460.479 -460.479 -460.479] (0.0100) ({r_i: None, r_t: [-1393.411 -1393.411 -1393.411], eps: 0.01})
Step:   24700, Reward: [-432.403 -432.403 -432.403] [54.570], Avg: [-438.471 -438.471 -438.471] (0.0841) ({r_i: None, r_t: [-843.987 -843.987 -843.987], eps: 0.084})
Step:   78100, Reward: [-636.961 -636.961 -636.961] [241.038], Avg: [-460.705 -460.705 -460.705] (0.0100) ({r_i: None, r_t: [-1333.223 -1333.223 -1333.223], eps: 0.01})
Step:   78200, Reward: [-701.175 -701.175 -701.175] [338.642], Avg: [-461.012 -461.012 -461.012] (0.0100) ({r_i: None, r_t: [-1319.999 -1319.999 -1319.999], eps: 0.01})
Step:   24800, Reward: [-399.954 -399.954 -399.954] [61.048], Avg: [-438.316 -438.316 -438.316] (0.0832) ({r_i: None, r_t: [-856.075 -856.075 -856.075], eps: 0.083})
Step:   78300, Reward: [-654.283 -654.283 -654.283] [367.706], Avg: [-461.258 -461.258 -461.258] (0.0100) ({r_i: None, r_t: [-1188.642 -1188.642 -1188.642], eps: 0.01})
Step:   24900, Reward: [-438.453 -438.453 -438.453] [73.306], Avg: [-438.317 -438.317 -438.317] (0.0824) ({r_i: None, r_t: [-845.648 -845.648 -845.648], eps: 0.082})
Step:   78400, Reward: [-725.809 -725.809 -725.809] [289.085], Avg: [-461.595 -461.595 -461.595] (0.0100) ({r_i: None, r_t: [-1480.197 -1480.197 -1480.197], eps: 0.01})
Step:   78500, Reward: [-672.446 -672.446 -672.446] [307.568], Avg: [-461.863 -461.863 -461.863] (0.0100) ({r_i: None, r_t: [-1428.194 -1428.194 -1428.194], eps: 0.01})
Step:   25000, Reward: [-454.760 -454.760 -454.760] [55.879], Avg: [-438.382 -438.382 -438.382] (0.0816) ({r_i: None, r_t: [-859.826 -859.826 -859.826], eps: 0.082})
Step:   78600, Reward: [-758.056 -758.056 -758.056] [306.457], Avg: [-462.240 -462.240 -462.240] (0.0100) ({r_i: None, r_t: [-1674.300 -1674.300 -1674.300], eps: 0.01})
Step:   25100, Reward: [-433.436 -433.436 -433.436] [69.096], Avg: [-438.363 -438.363 -438.363] (0.0808) ({r_i: None, r_t: [-854.160 -854.160 -854.160], eps: 0.081})
Step:   78700, Reward: [-690.913 -690.913 -690.913] [280.614], Avg: [-462.530 -462.530 -462.530] (0.0100) ({r_i: None, r_t: [-1523.185 -1523.185 -1523.185], eps: 0.01})
Step:   25200, Reward: [-440.642 -440.642 -440.642] [81.346], Avg: [-438.372 -438.372 -438.372] (0.0800) ({r_i: None, r_t: [-854.889 -854.889 -854.889], eps: 0.08})
Step:   78800, Reward: [-823.052 -823.052 -823.052] [342.489], Avg: [-462.987 -462.987 -462.987] (0.0100) ({r_i: None, r_t: [-1285.624 -1285.624 -1285.624], eps: 0.01})
Step:   78900, Reward: [-874.928 -874.928 -874.928] [399.632], Avg: [-463.508 -463.508 -463.508] (0.0100) ({r_i: None, r_t: [-1361.175 -1361.175 -1361.175], eps: 0.01})
Step:   25300, Reward: [-445.856 -445.856 -445.856] [49.074], Avg: [-438.401 -438.401 -438.401] (0.0792) ({r_i: None, r_t: [-853.899 -853.899 -853.899], eps: 0.079})
Step:   79000, Reward: [-601.941 -601.941 -601.941] [210.485], Avg: [-463.683 -463.683 -463.683] (0.0100) ({r_i: None, r_t: [-1666.820 -1666.820 -1666.820], eps: 0.01})
Step:   25400, Reward: [-451.575 -451.575 -451.575] [57.685], Avg: [-438.453 -438.453 -438.453] (0.0784) ({r_i: None, r_t: [-843.871 -843.871 -843.871], eps: 0.078})
Step:   79100, Reward: [-885.685 -885.685 -885.685] [427.591], Avg: [-464.216 -464.216 -464.216] (0.0100) ({r_i: None, r_t: [-1822.825 -1822.825 -1822.825], eps: 0.01})
Step:   25500, Reward: [-443.612 -443.612 -443.612] [72.968], Avg: [-438.473 -438.473 -438.473] (0.0776) ({r_i: None, r_t: [-868.361 -868.361 -868.361], eps: 0.078})
Step:   79200, Reward: [-763.253 -763.253 -763.253] [419.247], Avg: [-464.593 -464.593 -464.593] (0.0100) ({r_i: None, r_t: [-1586.686 -1586.686 -1586.686], eps: 0.01})
Step:   25600, Reward: [-451.697 -451.697 -451.697] [77.604], Avg: [-438.525 -438.525 -438.525] (0.0768) ({r_i: None, r_t: [-869.610 -869.610 -869.610], eps: 0.077})
Step:   79300, Reward: [-752.127 -752.127 -752.127] [432.868], Avg: [-464.955 -464.955 -464.955] (0.0100) ({r_i: None, r_t: [-1475.626 -1475.626 -1475.626], eps: 0.01})
Step:   79400, Reward: [-778.624 -778.624 -778.624] [399.866], Avg: [-465.350 -465.350 -465.350] (0.0100) ({r_i: None, r_t: [-1517.502 -1517.502 -1517.502], eps: 0.01})
Step:   25700, Reward: [-440.618 -440.618 -440.618] [74.899], Avg: [-438.533 -438.533 -438.533] (0.0760) ({r_i: None, r_t: [-867.100 -867.100 -867.100], eps: 0.076})
Step:   79500, Reward: [-758.946 -758.946 -758.946] [466.513], Avg: [-465.719 -465.719 -465.719] (0.0100) ({r_i: None, r_t: [-1611.827 -1611.827 -1611.827], eps: 0.01})
Step:   25800, Reward: [-444.307 -444.307 -444.307] [60.844], Avg: [-438.555 -438.555 -438.555] (0.0753) ({r_i: None, r_t: [-863.728 -863.728 -863.728], eps: 0.075})
Step:   79600, Reward: [-902.097 -902.097 -902.097] [427.577], Avg: [-466.266 -466.266 -466.266] (0.0100) ({r_i: None, r_t: [-1696.697 -1696.697 -1696.697], eps: 0.01})
Step:   25900, Reward: [-466.959 -466.959 -466.959] [64.407], Avg: [-438.664 -438.664 -438.664] (0.0745) ({r_i: None, r_t: [-820.177 -820.177 -820.177], eps: 0.075})
Step:   79700, Reward: [-886.296 -886.296 -886.296] [566.610], Avg: [-466.793 -466.793 -466.793] (0.0100) ({r_i: None, r_t: [-1505.376 -1505.376 -1505.376], eps: 0.01})
Step:   79800, Reward: [-755.004 -755.004 -755.004] [402.576], Avg: [-467.153 -467.153 -467.153] (0.0100) ({r_i: None, r_t: [-1677.775 -1677.775 -1677.775], eps: 0.01})
Step:   26000, Reward: [-430.177 -430.177 -430.177] [50.261], Avg: [-438.632 -438.632 -438.632] (0.0738) ({r_i: None, r_t: [-868.691 -868.691 -868.691], eps: 0.074})
Step:   79900, Reward: [-1029.085 -1029.085 -1029.085] [453.039], Avg: [-467.856 -467.856 -467.856] (0.0100) ({r_i: None, r_t: [-1581.484 -1581.484 -1581.484], eps: 0.01})
Step:   26100, Reward: [-428.319 -428.319 -428.319] [90.669], Avg: [-438.592 -438.592 -438.592] (0.0731) ({r_i: None, r_t: [-916.784 -916.784 -916.784], eps: 0.073})
Step:   80000, Reward: [-805.461 -805.461 -805.461] [536.425], Avg: [-468.277 -468.277 -468.277] (0.0100) ({r_i: None, r_t: [-1386.854 -1386.854 -1386.854], eps: 0.01})
Step:   80100, Reward: [-727.380 -727.380 -727.380] [438.035], Avg: [-468.600 -468.600 -468.600] (0.0100) ({r_i: None, r_t: [-1432.093 -1432.093 -1432.093], eps: 0.01})
Step:   26200, Reward: [-447.926 -447.926 -447.926] [70.710], Avg: [-438.628 -438.628 -438.628] (0.0723) ({r_i: None, r_t: [-843.826 -843.826 -843.826], eps: 0.072})
Step:   80200, Reward: [-605.210 -605.210 -605.210] [211.440], Avg: [-468.771 -468.771 -468.771] (0.0100) ({r_i: None, r_t: [-1525.382 -1525.382 -1525.382], eps: 0.01})
Step:   26300, Reward: [-467.115 -467.115 -467.115] [69.138], Avg: [-438.736 -438.736 -438.736] (0.0716) ({r_i: None, r_t: [-852.520 -852.520 -852.520], eps: 0.072})
Step:   80300, Reward: [-861.018 -861.018 -861.018] [513.585], Avg: [-469.258 -469.258 -469.258] (0.0100) ({r_i: None, r_t: [-1445.801 -1445.801 -1445.801], eps: 0.01})
Step:   26400, Reward: [-432.233 -432.233 -432.233] [72.741], Avg: [-438.711 -438.711 -438.711] (0.0709) ({r_i: None, r_t: [-843.796 -843.796 -843.796], eps: 0.071})
Step:   80400, Reward: [-620.847 -620.847 -620.847] [243.825], Avg: [-469.447 -469.447 -469.447] (0.0100) ({r_i: None, r_t: [-1763.966 -1763.966 -1763.966], eps: 0.01})
Step:   80500, Reward: [-736.480 -736.480 -736.480] [410.327], Avg: [-469.778 -469.778 -469.778] (0.0100) ({r_i: None, r_t: [-1578.418 -1578.418 -1578.418], eps: 0.01})
Step:   26500, Reward: [-425.459 -425.459 -425.459] [62.104], Avg: [-438.661 -438.661 -438.661] (0.0702) ({r_i: None, r_t: [-868.830 -868.830 -868.830], eps: 0.07})
Step:   80600, Reward: [-572.777 -572.777 -572.777] [197.771], Avg: [-469.906 -469.906 -469.906] (0.0100) ({r_i: None, r_t: [-1447.979 -1447.979 -1447.979], eps: 0.01})
Step:   26600, Reward: [-467.641 -467.641 -467.641] [61.024], Avg: [-438.770 -438.770 -438.770] (0.0695) ({r_i: None, r_t: [-854.326 -854.326 -854.326], eps: 0.069})
Step:   80700, Reward: [-798.321 -798.321 -798.321] [366.274], Avg: [-470.312 -470.312 -470.312] (0.0100) ({r_i: None, r_t: [-1484.050 -1484.050 -1484.050], eps: 0.01})
Step:   26700, Reward: [-438.244 -438.244 -438.244] [77.518], Avg: [-438.768 -438.768 -438.768] (0.0688) ({r_i: None, r_t: [-895.908 -895.908 -895.908], eps: 0.069})
Step:   80800, Reward: [-680.164 -680.164 -680.164] [371.573], Avg: [-470.571 -470.571 -470.571] (0.0100) ({r_i: None, r_t: [-1485.783 -1485.783 -1485.783], eps: 0.01})
Step:   80900, Reward: [-627.425 -627.425 -627.425] [187.649], Avg: [-470.765 -470.765 -470.765] (0.0100) ({r_i: None, r_t: [-1261.968 -1261.968 -1261.968], eps: 0.01})
Step:   26800, Reward: [-471.198 -471.198 -471.198] [66.807], Avg: [-438.889 -438.889 -438.889] (0.0681) ({r_i: None, r_t: [-863.461 -863.461 -863.461], eps: 0.068})
Step:   81000, Reward: [-866.667 -866.667 -866.667] [477.136], Avg: [-471.253 -471.253 -471.253] (0.0100) ({r_i: None, r_t: [-1424.107 -1424.107 -1424.107], eps: 0.01})
Step:   26900, Reward: [-435.850 -435.850 -435.850] [81.101], Avg: [-438.877 -438.877 -438.877] (0.0674) ({r_i: None, r_t: [-867.883 -867.883 -867.883], eps: 0.067})
Step:   81100, Reward: [-620.270 -620.270 -620.270] [276.627], Avg: [-471.437 -471.437 -471.437] (0.0100) ({r_i: None, r_t: [-1432.138 -1432.138 -1432.138], eps: 0.01})
Step:   81200, Reward: [-589.616 -589.616 -589.616] [193.420], Avg: [-471.582 -471.582 -471.582] (0.0100) ({r_i: None, r_t: [-1348.447 -1348.447 -1348.447], eps: 0.01})
Step:   27000, Reward: [-432.169 -432.169 -432.169] [56.356], Avg: [-438.853 -438.853 -438.853] (0.0668) ({r_i: None, r_t: [-906.835 -906.835 -906.835], eps: 0.067})
Step:   81300, Reward: [-813.740 -813.740 -813.740] [354.664], Avg: [-472.003 -472.003 -472.003] (0.0100) ({r_i: None, r_t: [-1354.891 -1354.891 -1354.891], eps: 0.01})
Step:   27100, Reward: [-445.547 -445.547 -445.547] [52.297], Avg: [-438.877 -438.877 -438.877] (0.0661) ({r_i: None, r_t: [-894.750 -894.750 -894.750], eps: 0.066})
Step:   81400, Reward: [-606.654 -606.654 -606.654] [153.040], Avg: [-472.168 -472.168 -472.168] (0.0100) ({r_i: None, r_t: [-1182.040 -1182.040 -1182.040], eps: 0.01})
Step:   27200, Reward: [-440.885 -440.885 -440.885] [71.214], Avg: [-438.884 -438.884 -438.884] (0.0654) ({r_i: None, r_t: [-877.871 -877.871 -877.871], eps: 0.065})
Step:   81500, Reward: [-634.919 -634.919 -634.919] [228.395], Avg: [-472.367 -472.367 -472.367] (0.0100) ({r_i: None, r_t: [-1305.849 -1305.849 -1305.849], eps: 0.01})
Step:   27300, Reward: [-450.314 -450.314 -450.314] [64.731], Avg: [-438.926 -438.926 -438.926] (0.0648) ({r_i: None, r_t: [-861.914 -861.914 -861.914], eps: 0.065})
Step:   81600, Reward: [-489.411 -489.411 -489.411] [99.179], Avg: [-472.388 -472.388 -472.388] (0.0100) ({r_i: None, r_t: [-1137.869 -1137.869 -1137.869], eps: 0.01})
Step:   81700, Reward: [-570.412 -570.412 -570.412] [156.167], Avg: [-472.508 -472.508 -472.508] (0.0100) ({r_i: None, r_t: [-1199.272 -1199.272 -1199.272], eps: 0.01})
Step:   27400, Reward: [-428.779 -428.779 -428.779] [61.530], Avg: [-438.889 -438.889 -438.889] (0.0641) ({r_i: None, r_t: [-877.176 -877.176 -877.176], eps: 0.064})
Step:   81800, Reward: [-525.570 -525.570 -525.570] [161.540], Avg: [-472.573 -472.573 -472.573] (0.0100) ({r_i: None, r_t: [-1266.958 -1266.958 -1266.958], eps: 0.01})
Step:   27500, Reward: [-425.824 -425.824 -425.824] [82.458], Avg: [-438.842 -438.842 -438.842] (0.0635) ({r_i: None, r_t: [-886.901 -886.901 -886.901], eps: 0.063})
Step:   81900, Reward: [-552.787 -552.787 -552.787] [123.320], Avg: [-472.670 -472.670 -472.670] (0.0100) ({r_i: None, r_t: [-1112.894 -1112.894 -1112.894], eps: 0.01})
Step:   82000, Reward: [-490.245 -490.245 -490.245] [68.610], Avg: [-472.692 -472.692 -472.692] (0.0100) ({r_i: None, r_t: [-1330.528 -1330.528 -1330.528], eps: 0.01})
Step:   27600, Reward: [-455.600 -455.600 -455.600] [74.897], Avg: [-438.902 -438.902 -438.902] (0.0629) ({r_i: None, r_t: [-890.497 -890.497 -890.497], eps: 0.063})
Step:   82100, Reward: [-485.314 -485.314 -485.314] [146.341], Avg: [-472.707 -472.707 -472.707] (0.0100) ({r_i: None, r_t: [-1178.109 -1178.109 -1178.109], eps: 0.01})
Step:   27700, Reward: [-417.762 -417.762 -417.762] [59.931], Avg: [-438.826 -438.826 -438.826] (0.0622) ({r_i: None, r_t: [-905.978 -905.978 -905.978], eps: 0.062})
Step:   82200, Reward: [-528.686 -528.686 -528.686] [103.720], Avg: [-472.775 -472.775 -472.775] (0.0100) ({r_i: None, r_t: [-1150.579 -1150.579 -1150.579], eps: 0.01})
Step:   27800, Reward: [-410.968 -410.968 -410.968] [50.567], Avg: [-438.727 -438.727 -438.727] (0.0616) ({r_i: None, r_t: [-853.510 -853.510 -853.510], eps: 0.062})
Step:   82300, Reward: [-553.723 -553.723 -553.723] [169.598], Avg: [-472.873 -472.873 -472.873] (0.0100) ({r_i: None, r_t: [-1150.382 -1150.382 -1150.382], eps: 0.01})
Step:   82400, Reward: [-494.942 -494.942 -494.942] [138.613], Avg: [-472.900 -472.900 -472.900] (0.0100) ({r_i: None, r_t: [-1091.595 -1091.595 -1091.595], eps: 0.01})
Step:   27900, Reward: [-423.545 -423.545 -423.545] [58.343], Avg: [-438.672 -438.672 -438.672] (0.0610) ({r_i: None, r_t: [-898.991 -898.991 -898.991], eps: 0.061})
Step:   82500, Reward: [-503.486 -503.486 -503.486] [124.438], Avg: [-472.937 -472.937 -472.937] (0.0100) ({r_i: None, r_t: [-1104.538 -1104.538 -1104.538], eps: 0.01})
Step:   28000, Reward: [-447.508 -447.508 -447.508] [77.544], Avg: [-438.704 -438.704 -438.704] (0.0604) ({r_i: None, r_t: [-880.150 -880.150 -880.150], eps: 0.06})
Step:   82600, Reward: [-555.502 -555.502 -555.502] [143.796], Avg: [-473.037 -473.037 -473.037] (0.0100) ({r_i: None, r_t: [-1043.438 -1043.438 -1043.438], eps: 0.01})
Step:   28100, Reward: [-463.785 -463.785 -463.785] [73.762], Avg: [-438.793 -438.793 -438.793] (0.0598) ({r_i: None, r_t: [-922.245 -922.245 -922.245], eps: 0.06})
Step:   82700, Reward: [-488.097 -488.097 -488.097] [122.831], Avg: [-473.055 -473.055 -473.055] (0.0100) ({r_i: None, r_t: [-1016.808 -1016.808 -1016.808], eps: 0.01})
Step:   82800, Reward: [-495.474 -495.474 -495.474] [122.687], Avg: [-473.082 -473.082 -473.082] (0.0100) ({r_i: None, r_t: [-995.601 -995.601 -995.601], eps: 0.01})
Step:   28200, Reward: [-450.327 -450.327 -450.327] [55.682], Avg: [-438.833 -438.833 -438.833] (0.0592) ({r_i: None, r_t: [-907.031 -907.031 -907.031], eps: 0.059})
Step:   82900, Reward: [-513.712 -513.712 -513.712] [122.662], Avg: [-473.131 -473.131 -473.131] (0.0100) ({r_i: None, r_t: [-1043.012 -1043.012 -1043.012], eps: 0.01})
Step:   28300, Reward: [-449.314 -449.314 -449.314] [58.824], Avg: [-438.870 -438.870 -438.870] (0.0586) ({r_i: None, r_t: [-871.597 -871.597 -871.597], eps: 0.059})
Step:   83000, Reward: [-443.254 -443.254 -443.254] [104.953], Avg: [-473.095 -473.095 -473.095] (0.0100) ({r_i: None, r_t: [-1001.774 -1001.774 -1001.774], eps: 0.01})
Step:   28400, Reward: [-434.726 -434.726 -434.726] [68.860], Avg: [-438.856 -438.856 -438.856] (0.0580) ({r_i: None, r_t: [-882.230 -882.230 -882.230], eps: 0.058})
Step:   83100, Reward: [-520.337 -520.337 -520.337] [204.366], Avg: [-473.152 -473.152 -473.152] (0.0100) ({r_i: None, r_t: [-1086.584 -1086.584 -1086.584], eps: 0.01})
Step:   83200, Reward: [-488.808 -488.808 -488.808] [90.097], Avg: [-473.171 -473.171 -473.171] (0.0100) ({r_i: None, r_t: [-960.390 -960.390 -960.390], eps: 0.01})
Step:   28500, Reward: [-448.874 -448.874 -448.874] [74.785], Avg: [-438.891 -438.891 -438.891] (0.0574) ({r_i: None, r_t: [-899.396 -899.396 -899.396], eps: 0.057})
Step:   83300, Reward: [-497.467 -497.467 -497.467] [130.092], Avg: [-473.200 -473.200 -473.200] (0.0100) ({r_i: None, r_t: [-989.373 -989.373 -989.373], eps: 0.01})
Step:   28600, Reward: [-460.533 -460.533 -460.533] [50.882], Avg: [-438.966 -438.966 -438.966] (0.0569) ({r_i: None, r_t: [-886.882 -886.882 -886.882], eps: 0.057})
Step:   83400, Reward: [-486.795 -486.795 -486.795] [81.077], Avg: [-473.216 -473.216 -473.216] (0.0100) ({r_i: None, r_t: [-1037.808 -1037.808 -1037.808], eps: 0.01})
Step:   28700, Reward: [-468.293 -468.293 -468.293] [73.046], Avg: [-439.068 -439.068 -439.068] (0.0563) ({r_i: None, r_t: [-906.565 -906.565 -906.565], eps: 0.056})
Step:   83500, Reward: [-453.028 -453.028 -453.028] [70.935], Avg: [-473.192 -473.192 -473.192] (0.0100) ({r_i: None, r_t: [-960.565 -960.565 -960.565], eps: 0.01})
Step:   83600, Reward: [-503.223 -503.223 -503.223] [98.159], Avg: [-473.228 -473.228 -473.228] (0.0100) ({r_i: None, r_t: [-949.719 -949.719 -949.719], eps: 0.01})
Step:   28800, Reward: [-426.261 -426.261 -426.261] [63.060], Avg: [-439.024 -439.024 -439.024] (0.0557) ({r_i: None, r_t: [-909.153 -909.153 -909.153], eps: 0.056})
Step:   83700, Reward: [-464.528 -464.528 -464.528] [118.681], Avg: [-473.218 -473.218 -473.218] (0.0100) ({r_i: None, r_t: [-955.095 -955.095 -955.095], eps: 0.01})
Step:   28900, Reward: [-474.121 -474.121 -474.121] [54.230], Avg: [-439.145 -439.145 -439.145] (0.0552) ({r_i: None, r_t: [-921.002 -921.002 -921.002], eps: 0.055})
Step:   83800, Reward: [-471.155 -471.155 -471.155] [82.379], Avg: [-473.215 -473.215 -473.215] (0.0100) ({r_i: None, r_t: [-977.795 -977.795 -977.795], eps: 0.01})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

HIDDEN_SIZE = 64
LEARN_RATE = 0.0003
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

EPS_MAX = 1.0
EPS_MIN = 0.1
EPS_DECAY = 0.995
EPISODE_BUFFER = 320				# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16				# Number of samples to train on for each train step

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		# self.critic_training_steps = 0
		# self.last_target_update_step = 0
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, name="coma")
		# self.critic_target = copy.deepcopy(self.critic_local)
		# self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=lr)
		# self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=lr)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimiser, critic_loss, self.critic_local.parameters(), retain=t>0)
			# self.critic_training_steps += 1
		
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimiser, actor_loss, self.actor_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)
		# if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
		# 	self.critic_target.load_state_dict(self.critic_local.state_dict())
		# 	self.last_target_update_step = self.critic_training_steps

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	# def save_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
	# 	# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	# def load_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1,1,1,self.n_agents,1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-502.426 -502.426 -502.426] [88.147], Avg: [-502.426 -502.426 -502.426] (1.0000) ({r_i: None, r_t: [-9.796 -9.796 -9.796], eps: 1.0})
Step:   29000, Reward: [-470.775 -470.775 -470.775] [60.664], Avg: [-439.254 -439.254 -439.254] (0.0546) ({r_i: None, r_t: [-931.051 -931.051 -931.051], eps: 0.055})
Step:   83900, Reward: [-465.588 -465.588 -465.588] [105.604], Avg: [-473.206 -473.206 -473.206] (0.0100) ({r_i: None, r_t: [-953.835 -953.835 -953.835], eps: 0.01})
Step:   84000, Reward: [-581.573 -581.573 -581.573] [201.447], Avg: [-473.335 -473.335 -473.335] (0.0100) ({r_i: None, r_t: [-929.931 -929.931 -929.931], eps: 0.01})
Step:   29100, Reward: [-428.244 -428.244 -428.244] [50.916], Avg: [-439.216 -439.216 -439.216] (0.0541) ({r_i: None, r_t: [-927.700 -927.700 -927.700], eps: 0.054})
Step:   84100, Reward: [-498.009 -498.009 -498.009] [116.582], Avg: [-473.364 -473.364 -473.364] (0.0100) ({r_i: None, r_t: [-962.957 -962.957 -962.957], eps: 0.01})
Step:   29200, Reward: [-476.586 -476.586 -476.586] [67.908], Avg: [-439.343 -439.343 -439.343] (0.0535) ({r_i: None, r_t: [-923.564 -923.564 -923.564], eps: 0.054})
Step:   84200, Reward: [-477.059 -477.059 -477.059] [92.621], Avg: [-473.369 -473.369 -473.369] (0.0100) ({r_i: None, r_t: [-969.040 -969.040 -969.040], eps: 0.01})
Step:   84300, Reward: [-464.123 -464.123 -464.123] [100.323], Avg: [-473.358 -473.358 -473.358] (0.0100) ({r_i: None, r_t: [-970.680 -970.680 -970.680], eps: 0.01})
Step:   29300, Reward: [-464.616 -464.616 -464.616] [64.521], Avg: [-439.429 -439.429 -439.429] (0.0530) ({r_i: None, r_t: [-912.235 -912.235 -912.235], eps: 0.053})
Step:   84400, Reward: [-566.169 -566.169 -566.169] [145.349], Avg: [-473.468 -473.468 -473.468] (0.0100) ({r_i: None, r_t: [-940.602 -940.602 -940.602], eps: 0.01})
Step:   29400, Reward: [-437.198 -437.198 -437.198] [64.799], Avg: [-439.422 -439.422 -439.422] (0.0525) ({r_i: None, r_t: [-905.812 -905.812 -905.812], eps: 0.052})
Step:   84500, Reward: [-485.218 -485.218 -485.218] [104.905], Avg: [-473.481 -473.481 -473.481] (0.0100) ({r_i: None, r_t: [-1010.525 -1010.525 -1010.525], eps: 0.01})
Step:   29500, Reward: [-488.034 -488.034 -488.034] [58.742], Avg: [-439.586 -439.586 -439.586] (0.0520) ({r_i: None, r_t: [-908.770 -908.770 -908.770], eps: 0.052})
Step:   84600, Reward: [-546.915 -546.915 -546.915] [161.535], Avg: [-473.568 -473.568 -473.568] (0.0100) ({r_i: None, r_t: [-902.308 -902.308 -902.308], eps: 0.01})
Step:   29600, Reward: [-452.474 -452.474 -452.474] [74.306], Avg: [-439.629 -439.629 -439.629] (0.0514) ({r_i: None, r_t: [-943.658 -943.658 -943.658], eps: 0.051})
Step:   84700, Reward: [-497.907 -497.907 -497.907] [116.936], Avg: [-473.597 -473.597 -473.597] (0.0100) ({r_i: None, r_t: [-975.533 -975.533 -975.533], eps: 0.01})
Step:   84800, Reward: [-556.699 -556.699 -556.699] [147.152], Avg: [-473.695 -473.695 -473.695] (0.0100) ({r_i: None, r_t: [-1005.624 -1005.624 -1005.624], eps: 0.01})
Step:   29700, Reward: [-446.797 -446.797 -446.797] [62.754], Avg: [-439.653 -439.653 -439.653] (0.0509) ({r_i: None, r_t: [-880.104 -880.104 -880.104], eps: 0.051})
Step:   84900, Reward: [-475.773 -475.773 -475.773] [146.217], Avg: [-473.697 -473.697 -473.697] (0.0100) ({r_i: None, r_t: [-1010.773 -1010.773 -1010.773], eps: 0.01})
Step:   29800, Reward: [-452.602 -452.602 -452.602] [55.771], Avg: [-439.697 -439.697 -439.697] (0.0504) ({r_i: None, r_t: [-935.629 -935.629 -935.629], eps: 0.05})
Step:   85000, Reward: [-496.091 -496.091 -496.091] [102.559], Avg: [-473.723 -473.723 -473.723] (0.0100) ({r_i: None, r_t: [-993.964 -993.964 -993.964], eps: 0.01})
Step:   85100, Reward: [-576.983 -576.983 -576.983] [188.614], Avg: [-473.845 -473.845 -473.845] (0.0100) ({r_i: None, r_t: [-1090.600 -1090.600 -1090.600], eps: 0.01})
Step:   29900, Reward: [-454.129 -454.129 -454.129] [72.998], Avg: [-439.745 -439.745 -439.745] (0.0499) ({r_i: None, r_t: [-942.427 -942.427 -942.427], eps: 0.05})
Step:   85200, Reward: [-532.566 -532.566 -532.566] [164.118], Avg: [-473.914 -473.914 -473.914] (0.0100) ({r_i: None, r_t: [-980.830 -980.830 -980.830], eps: 0.01})
Step:   30000, Reward: [-438.337 -438.337 -438.337] [88.779], Avg: [-439.740 -439.740 -439.740] (0.0494) ({r_i: None, r_t: [-888.633 -888.633 -888.633], eps: 0.049})
Step:   85300, Reward: [-632.805 -632.805 -632.805] [260.827], Avg: [-474.100 -474.100 -474.100] (0.0100) ({r_i: None, r_t: [-1109.351 -1109.351 -1109.351], eps: 0.01})
Step:   30100, Reward: [-442.616 -442.616 -442.616] [62.571], Avg: [-439.750 -439.750 -439.750] (0.0489) ({r_i: None, r_t: [-915.976 -915.976 -915.976], eps: 0.049})
Step:   85400, Reward: [-518.883 -518.883 -518.883] [176.867], Avg: [-474.152 -474.152 -474.152] (0.0100) ({r_i: None, r_t: [-1137.101 -1137.101 -1137.101], eps: 0.01})
Step:   85500, Reward: [-591.001 -591.001 -591.001] [150.971], Avg: [-474.288 -474.288 -474.288] (0.0100) ({r_i: None, r_t: [-1072.084 -1072.084 -1072.084], eps: 0.01})
Step:   30200, Reward: [-467.070 -467.070 -467.070] [62.983], Avg: [-439.840 -439.840 -439.840] (0.0484) ({r_i: None, r_t: [-942.627 -942.627 -942.627], eps: 0.048})
Step:   85600, Reward: [-543.370 -543.370 -543.370] [209.366], Avg: [-474.369 -474.369 -474.369] (0.0100) ({r_i: None, r_t: [-1184.406 -1184.406 -1184.406], eps: 0.01})
Step:   30300, Reward: [-448.822 -448.822 -448.822] [65.103], Avg: [-439.869 -439.869 -439.869] (0.0479) ({r_i: None, r_t: [-928.412 -928.412 -928.412], eps: 0.048})
Step:   85700, Reward: [-476.208 -476.208 -476.208] [111.435], Avg: [-474.371 -474.371 -474.371] (0.0100) ({r_i: None, r_t: [-1125.775 -1125.775 -1125.775], eps: 0.01})
Step:   30400, Reward: [-452.282 -452.282 -452.282] [43.186], Avg: [-439.910 -439.910 -439.910] (0.0475) ({r_i: None, r_t: [-935.879 -935.879 -935.879], eps: 0.047})
Step:   85800, Reward: [-502.817 -502.817 -502.817] [100.092], Avg: [-474.404 -474.404 -474.404] (0.0100) ({r_i: None, r_t: [-1145.596 -1145.596 -1145.596], eps: 0.01})
Step:   85900, Reward: [-498.725 -498.725 -498.725] [105.023], Avg: [-474.433 -474.433 -474.433] (0.0100) ({r_i: None, r_t: [-1005.911 -1005.911 -1005.911], eps: 0.01})
Step:   30500, Reward: [-474.434 -474.434 -474.434] [77.516], Avg: [-440.023 -440.023 -440.023] (0.0470) ({r_i: None, r_t: [-927.199 -927.199 -927.199], eps: 0.047})
Step:   86000, Reward: [-577.063 -577.063 -577.063] [226.782], Avg: [-474.552 -474.552 -474.552] (0.0100) ({r_i: None, r_t: [-1052.064 -1052.064 -1052.064], eps: 0.01})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

HIDDEN_SIZE = 64
LEARN_RATE = 0.0003
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

EPS_MAX = 1.0
EPS_MIN = 0.1
EPS_DECAY = 0.995
EPISODE_BUFFER = 320				# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16				# Number of samples to train on for each train step

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		# self.critic_training_steps = 0
		# self.last_target_update_step = 0
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, name="coma")
		# self.critic_target = copy.deepcopy(self.critic_local)
		# self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=lr)
		# self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=lr)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
			# self.critic_training_steps += 1
		
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)
		# if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
		# 	self.critic_target.load_state_dict(self.critic_local.state_dict())
		# 	self.last_target_update_step = self.critic_training_steps

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	# def save_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
	# 	# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	# def load_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1,1,1,self.n_agents,1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-513.709 -513.709 -513.709] [100.792], Avg: [-513.709 -513.709 -513.709] (1.0000) ({r_i: None, r_t: [-9.452 -9.452 -9.452], eps: 1.0})
Step:   30600, Reward: [-474.693 -474.693 -474.693] [50.625], Avg: [-440.136 -440.136 -440.136] (0.0465) ({r_i: None, r_t: [-903.668 -903.668 -903.668], eps: 0.047})
Step:   86100, Reward: [-545.206 -545.206 -545.206] [189.829], Avg: [-474.634 -474.634 -474.634] (0.0100) ({r_i: None, r_t: [-1065.593 -1065.593 -1065.593], eps: 0.01})
Step:   30700, Reward: [-454.765 -454.765 -454.765] [56.677], Avg: [-440.183 -440.183 -440.183] (0.0461) ({r_i: None, r_t: [-880.176 -880.176 -880.176], eps: 0.046})
Step:   86200, Reward: [-494.660 -494.660 -494.660] [124.494], Avg: [-474.657 -474.657 -474.657] (0.0100) ({r_i: None, r_t: [-942.572 -942.572 -942.572], eps: 0.01})
Step:     100, Reward: [-472.156 -472.156 -472.156] [97.433], Avg: [-492.932 -492.932 -492.932] (0.9900) ({r_i: None, r_t: [-959.556 -959.556 -959.556], eps: 0.99})
Step:   86300, Reward: [-533.132 -533.132 -533.132] [178.707], Avg: [-474.725 -474.725 -474.725] (0.0100) ({r_i: None, r_t: [-1131.103 -1131.103 -1131.103], eps: 0.01})
Step:   30800, Reward: [-458.648 -458.648 -458.648] [75.947], Avg: [-440.243 -440.243 -440.243] (0.0456) ({r_i: None, r_t: [-940.474 -940.474 -940.474], eps: 0.046})
Step:   86400, Reward: [-528.084 -528.084 -528.084] [224.473], Avg: [-474.786 -474.786 -474.786] (0.0100) ({r_i: None, r_t: [-1076.349 -1076.349 -1076.349], eps: 0.01})
Step:     200, Reward: [-504.009 -504.009 -504.009] [122.949], Avg: [-496.625 -496.625 -496.625] (0.9801) ({r_i: None, r_t: [-920.132 -920.132 -920.132], eps: 0.98})
Step:   30900, Reward: [-416.071 -416.071 -416.071] [65.038], Avg: [-440.165 -440.165 -440.165] (0.0452) ({r_i: None, r_t: [-897.950 -897.950 -897.950], eps: 0.045})
Step:   86500, Reward: [-508.573 -508.573 -508.573] [261.994], Avg: [-474.825 -474.825 -474.825] (0.0100) ({r_i: None, r_t: [-984.735 -984.735 -984.735], eps: 0.01})
Step:   31000, Reward: [-448.492 -448.492 -448.492] [68.187], Avg: [-440.192 -440.192 -440.192] (0.0447) ({r_i: None, r_t: [-887.330 -887.330 -887.330], eps: 0.045})
Step:   86600, Reward: [-434.668 -434.668 -434.668] [80.761], Avg: [-474.779 -474.779 -474.779] (0.0100) ({r_i: None, r_t: [-999.371 -999.371 -999.371], eps: 0.01})
Step:     300, Reward: [-474.851 -474.851 -474.851] [98.746], Avg: [-491.181 -491.181 -491.181] (0.9704) ({r_i: None, r_t: [-919.775 -919.775 -919.775], eps: 0.97})
Step:   86700, Reward: [-463.816 -463.816 -463.816] [90.664], Avg: [-474.766 -474.766 -474.766] (0.0100) ({r_i: None, r_t: [-1011.907 -1011.907 -1011.907], eps: 0.01})
Step:   31100, Reward: [-488.898 -488.898 -488.898] [67.267], Avg: [-440.348 -440.348 -440.348] (0.0443) ({r_i: None, r_t: [-906.487 -906.487 -906.487], eps: 0.044})
Step:   86800, Reward: [-474.636 -474.636 -474.636] [122.893], Avg: [-474.766 -474.766 -474.766] (0.0100) ({r_i: None, r_t: [-924.471 -924.471 -924.471], eps: 0.01})
Step:     400, Reward: [-481.556 -481.556 -481.556] [90.963], Avg: [-489.256 -489.256 -489.256] (0.9607) ({r_i: None, r_t: [-1004.467 -1004.467 -1004.467], eps: 0.961})
Step:   31200, Reward: [-479.094 -479.094 -479.094] [50.730], Avg: [-440.472 -440.472 -440.472] (0.0438) ({r_i: None, r_t: [-907.025 -907.025 -907.025], eps: 0.044})
Step:   86900, Reward: [-464.756 -464.756 -464.756] [82.574], Avg: [-474.755 -474.755 -474.755] (0.0100) ({r_i: None, r_t: [-998.546 -998.546 -998.546], eps: 0.01})
Step:   87000, Reward: [-459.145 -459.145 -459.145] [70.537], Avg: [-474.737 -474.737 -474.737] (0.0100) ({r_i: None, r_t: [-1004.832 -1004.832 -1004.832], eps: 0.01})
Step:   31300, Reward: [-466.273 -466.273 -466.273] [64.378], Avg: [-440.554 -440.554 -440.554] (0.0434) ({r_i: None, r_t: [-885.573 -885.573 -885.573], eps: 0.043})
Step:     500, Reward: [-471.720 -471.720 -471.720] [115.972], Avg: [-486.333 -486.333 -486.333] (0.9511) ({r_i: None, r_t: [-953.218 -953.218 -953.218], eps: 0.951})
Step:   87100, Reward: [-476.696 -476.696 -476.696] [95.855], Avg: [-474.739 -474.739 -474.739] (0.0100) ({r_i: None, r_t: [-896.017 -896.017 -896.017], eps: 0.01})
Step:   31400, Reward: [-467.118 -467.118 -467.118] [69.539], Avg: [-440.638 -440.638 -440.638] (0.0429) ({r_i: None, r_t: [-924.512 -924.512 -924.512], eps: 0.043})
Step:   87200, Reward: [-473.216 -473.216 -473.216] [110.836], Avg: [-474.737 -474.737 -474.737] (0.0100) ({r_i: None, r_t: [-1051.624 -1051.624 -1051.624], eps: 0.01})
Step:     600, Reward: [-506.638 -506.638 -506.638] [117.325], Avg: [-489.234 -489.234 -489.234] (0.9416) ({r_i: None, r_t: [-1022.646 -1022.646 -1022.646], eps: 0.942})
Step:   31500, Reward: [-458.116 -458.116 -458.116] [82.417], Avg: [-440.694 -440.694 -440.694] (0.0425) ({r_i: None, r_t: [-945.339 -945.339 -945.339], eps: 0.043})
Step:   87300, Reward: [-463.434 -463.434 -463.434] [91.050], Avg: [-474.724 -474.724 -474.724] (0.0100) ({r_i: None, r_t: [-880.782 -880.782 -880.782], eps: 0.01})
Step:   87400, Reward: [-473.438 -473.438 -473.438] [92.475], Avg: [-474.723 -474.723 -474.723] (0.0100) ({r_i: None, r_t: [-925.210 -925.210 -925.210], eps: 0.01})
Step:   31600, Reward: [-464.679 -464.679 -464.679] [67.114], Avg: [-440.769 -440.769 -440.769] (0.0421) ({r_i: None, r_t: [-900.856 -900.856 -900.856], eps: 0.042})
Step:   87500, Reward: [-515.454 -515.454 -515.454] [125.632], Avg: [-474.769 -474.769 -474.769] (0.0100) ({r_i: None, r_t: [-937.232 -937.232 -937.232], eps: 0.01})
Step:   31700, Reward: [-430.370 -430.370 -430.370] [78.220], Avg: [-440.737 -440.737 -440.737] (0.0417) ({r_i: None, r_t: [-932.079 -932.079 -932.079], eps: 0.042})
Step:   87600, Reward: [-460.107 -460.107 -460.107] [73.594], Avg: [-474.753 -474.753 -474.753] (0.0100) ({r_i: None, r_t: [-944.257 -944.257 -944.257], eps: 0.01})
Step:   31800, Reward: [-484.185 -484.185 -484.185] [78.598], Avg: [-440.873 -440.873 -440.873] (0.0413) ({r_i: None, r_t: [-967.651 -967.651 -967.651], eps: 0.041})
Step:   87700, Reward: [-467.652 -467.652 -467.652] [87.756], Avg: [-474.745 -474.745 -474.745] (0.0100) ({r_i: None, r_t: [-922.684 -922.684 -922.684], eps: 0.01})
Step:   87800, Reward: [-448.614 -448.614 -448.614] [57.004], Avg: [-474.715 -474.715 -474.715] (0.0100) ({r_i: None, r_t: [-944.357 -944.357 -944.357], eps: 0.01})
Step:   31900, Reward: [-462.172 -462.172 -462.172] [71.274], Avg: [-440.939 -440.939 -440.939] (0.0408) ({r_i: None, r_t: [-955.279 -955.279 -955.279], eps: 0.041})
Step:   87900, Reward: [-457.267 -457.267 -457.267] [103.355], Avg: [-474.695 -474.695 -474.695] (0.0100) ({r_i: None, r_t: [-974.176 -974.176 -974.176], eps: 0.01})
Step:   32000, Reward: [-434.686 -434.686 -434.686] [85.881], Avg: [-440.920 -440.920 -440.920] (0.0404) ({r_i: None, r_t: [-934.781 -934.781 -934.781], eps: 0.04})
Step:   88000, Reward: [-506.159 -506.159 -506.159] [138.589], Avg: [-474.731 -474.731 -474.731] (0.0100) ({r_i: None, r_t: [-975.412 -975.412 -975.412], eps: 0.01})
Step:   32100, Reward: [-484.087 -484.087 -484.087] [57.860], Avg: [-441.054 -441.054 -441.054] (0.0400) ({r_i: None, r_t: [-903.421 -903.421 -903.421], eps: 0.04})
Step:   88100, Reward: [-490.883 -490.883 -490.883] [81.354], Avg: [-474.749 -474.749 -474.749] (0.0100) ({r_i: None, r_t: [-903.604 -903.604 -903.604], eps: 0.01})
Step:   88200, Reward: [-422.901 -422.901 -422.901] [92.220], Avg: [-474.690 -474.690 -474.690] (0.0100) ({r_i: None, r_t: [-951.070 -951.070 -951.070], eps: 0.01})
Step:   32200, Reward: [-434.068 -434.068 -434.068] [82.077], Avg: [-441.032 -441.032 -441.032] (0.0396) ({r_i: None, r_t: [-938.034 -938.034 -938.034], eps: 0.04})
Step:   88300, Reward: [-467.843 -467.843 -467.843] [114.458], Avg: [-474.683 -474.683 -474.683] (0.0100) ({r_i: None, r_t: [-939.921 -939.921 -939.921], eps: 0.01})
Step:   32300, Reward: [-460.988 -460.988 -460.988] [67.272], Avg: [-441.094 -441.094 -441.094] (0.0392) ({r_i: None, r_t: [-947.246 -947.246 -947.246], eps: 0.039})
Step:   88400, Reward: [-542.475 -542.475 -542.475] [141.780], Avg: [-474.759 -474.759 -474.759] (0.0100) ({r_i: None, r_t: [-935.693 -935.693 -935.693], eps: 0.01})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

HIDDEN_SIZE = 64
LEARN_RATE = 0.0003
TARGET_UPDATE = 200
TARGET_UPDATE_RATE = 0.005

EPS_MAX = 1.0
EPS_MIN = 0.1
EPS_DECAY = 0.995
EPISODE_BUFFER = 320				# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 16				# Number of samples to train on for each train step

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], HIDDEN_SIZE)
		self.layer2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.layer3 = torch.nn.Linear(HIDDEN_SIZE, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		# self.critic_training_steps = 0
		# self.last_target_update_step = 0
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, name="coma")
		# self.critic_target = copy.deepcopy(self.critic_local)
		# self.actor_optimiser = torch.optim.Adam(self.actor_local.parameters(), lr=lr)
		# self.critic_optimiser = torch.optim.Adam(self.critic_local.parameters(), lr=lr)

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		target_q_vals = self.critic_target(critic_inputs)
		targets_taken = torch.gather(target_q_vals, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		targets_taken = torch.cat([targets_taken, torch.zeros_like(targets_taken[:,-1]).unsqueeze(1)], dim=1)
		targets = self.build_td_lambda_targets(rewards, dones, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)
		for t in reversed(range(rewards.size(1))):
			q_vals[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_vals[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - targets[:, t].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
			# self.critic_training_steps += 1
		
		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_vals = q_vals.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)
		# if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
		# 	self.critic_target.load_state_dict(self.critic_local.state_dict())
		# 	self.last_target_update_step = self.critic_training_steps

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	# def save_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
	# 	# [PTACNetwork.save_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	# def load_model(self, dirname="pytorch", name="checkpoint"):
	# 	pass
		# [PTACNetwork.load_model(model, self.name, dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states_, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.replay_buffer.add((states_, actions, rewards, dones))
			self.buffer.clear()
			if len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
				sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
				states_, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
				state = states_.repeat(1,1,1,self.n_agents,1).view(*states_.shape[:3],-1)
				obs = states_.squeeze(-2)
				actions = actions.squeeze(-2)
				actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
				agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
				action_masked = actions_joint * agent_mask
				last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
				last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
				agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
				critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
				actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
				self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
			self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-485.845 -485.845 -485.845] [93.525], Avg: [-485.845 -485.845 -485.845] (1.0000) ({r_i: None, r_t: [-8.498 -8.498 -8.498], eps: 1.0})
Step:   32400, Reward: [-430.526 -430.526 -430.526] [62.620], Avg: [-441.061 -441.061 -441.061] (0.0388) ({r_i: None, r_t: [-925.311 -925.311 -925.311], eps: 0.039})
Step:   88500, Reward: [-473.064 -473.064 -473.064] [159.051], Avg: [-474.757 -474.757 -474.757] (0.0100) ({r_i: None, r_t: [-915.556 -915.556 -915.556], eps: 0.01})
Step:     100, Reward: [-506.342 -506.342 -506.342] [113.018], Avg: [-496.094 -496.094 -496.094] (0.9900) ({r_i: None, r_t: [-955.313 -955.313 -955.313], eps: 0.99})
Step:   88600, Reward: [-563.985 -563.985 -563.985] [187.102], Avg: [-474.858 -474.858 -474.858] (0.0100) ({r_i: None, r_t: [-964.598 -964.598 -964.598], eps: 0.01})
Step:   32500, Reward: [-459.162 -459.162 -459.162] [71.269], Avg: [-441.117 -441.117 -441.117] (0.0385) ({r_i: None, r_t: [-909.216 -909.216 -909.216], eps: 0.038})
Step:   88700, Reward: [-488.466 -488.466 -488.466] [119.383], Avg: [-474.873 -474.873 -474.873] (0.0100) ({r_i: None, r_t: [-891.142 -891.142 -891.142], eps: 0.01})
Step:     200, Reward: [-510.365 -510.365 -510.365] [103.196], Avg: [-500.851 -500.851 -500.851] (0.9801) ({r_i: None, r_t: [-1004.547 -1004.547 -1004.547], eps: 0.98})
Step:   32600, Reward: [-463.352 -463.352 -463.352] [68.813], Avg: [-441.185 -441.185 -441.185] (0.0381) ({r_i: None, r_t: [-924.870 -924.870 -924.870], eps: 0.038})
Step:   88800, Reward: [-485.662 -485.662 -485.662] [115.941], Avg: [-474.885 -474.885 -474.885] (0.0100) ({r_i: None, r_t: [-1045.412 -1045.412 -1045.412], eps: 0.01})
Step:     300, Reward: [-489.877 -489.877 -489.877] [86.720], Avg: [-498.107 -498.107 -498.107] (0.9704) ({r_i: None, r_t: [-983.081 -983.081 -983.081], eps: 0.97})
Step:   32700, Reward: [-487.123 -487.123 -487.123] [63.991], Avg: [-441.325 -441.325 -441.325] (0.0377) ({r_i: None, r_t: [-865.941 -865.941 -865.941], eps: 0.038})
Step:   88900, Reward: [-489.841 -489.841 -489.841] [133.616], Avg: [-474.902 -474.902 -474.902] (0.0100) ({r_i: None, r_t: [-1050.480 -1050.480 -1050.480], eps: 0.01})
Step:     400, Reward: [-516.407 -516.407 -516.407] [86.606], Avg: [-501.767 -501.767 -501.767] (0.9607) ({r_i: None, r_t: [-984.959 -984.959 -984.959], eps: 0.961})
Step:   89000, Reward: [-557.522 -557.522 -557.522] [154.991], Avg: [-474.995 -474.995 -474.995] (0.0100) ({r_i: None, r_t: [-992.077 -992.077 -992.077], eps: 0.01})
Step:   32800, Reward: [-429.055 -429.055 -429.055] [70.604], Avg: [-441.288 -441.288 -441.288] (0.0373) ({r_i: None, r_t: [-882.857 -882.857 -882.857], eps: 0.037})
Step:   89100, Reward: [-526.490 -526.490 -526.490] [179.722], Avg: [-475.053 -475.053 -475.053] (0.0100) ({r_i: None, r_t: [-1000.766 -1000.766 -1000.766], eps: 0.01})
Step:     500, Reward: [-531.797 -531.797 -531.797] [132.464], Avg: [-506.772 -506.772 -506.772] (0.9511) ({r_i: None, r_t: [-961.249 -961.249 -961.249], eps: 0.951})
Step:   32900, Reward: [-465.807 -465.807 -465.807] [78.076], Avg: [-441.362 -441.362 -441.362] (0.0369) ({r_i: None, r_t: [-924.352 -924.352 -924.352], eps: 0.037})
Step:   89200, Reward: [-450.588 -450.588 -450.588] [107.343], Avg: [-475.025 -475.025 -475.025] (0.0100) ({r_i: None, r_t: [-952.499 -952.499 -952.499], eps: 0.01})
Step:     600, Reward: [-512.695 -512.695 -512.695] [99.039], Avg: [-507.618 -507.618 -507.618] (0.9416) ({r_i: None, r_t: [-986.697 -986.697 -986.697], eps: 0.942})
Step:   33000, Reward: [-462.780 -462.780 -462.780] [98.934], Avg: [-441.427 -441.427 -441.427] (0.0366) ({r_i: None, r_t: [-938.294 -938.294 -938.294], eps: 0.037})
Step:   89300, Reward: [-521.452 -521.452 -521.452] [159.817], Avg: [-475.077 -475.077 -475.077] (0.0100) ({r_i: None, r_t: [-1146.590 -1146.590 -1146.590], eps: 0.01})
Step:     700, Reward: [-466.732 -466.732 -466.732] [72.803], Avg: [-502.508 -502.508 -502.508] (0.9322) ({r_i: None, r_t: [-995.925 -995.925 -995.925], eps: 0.932})
Step:   89400, Reward: [-560.412 -560.412 -560.412] [178.367], Avg: [-475.173 -475.173 -475.173] (0.0100) ({r_i: None, r_t: [-1050.733 -1050.733 -1050.733], eps: 0.01})
Step:   33100, Reward: [-477.568 -477.568 -477.568] [75.946], Avg: [-441.536 -441.536 -441.536] (0.0362) ({r_i: None, r_t: [-922.849 -922.849 -922.849], eps: 0.036})
Step:   89500, Reward: [-560.809 -560.809 -560.809] [211.200], Avg: [-475.268 -475.268 -475.268] (0.0100) ({r_i: None, r_t: [-1253.941 -1253.941 -1253.941], eps: 0.01})
Step:     800, Reward: [-506.662 -506.662 -506.662] [101.672], Avg: [-502.969 -502.969 -502.969] (0.9229) ({r_i: None, r_t: [-1008.826 -1008.826 -1008.826], eps: 0.923})
Step:   33200, Reward: [-456.607 -456.607 -456.607] [65.284], Avg: [-441.581 -441.581 -441.581] (0.0359) ({r_i: None, r_t: [-948.201 -948.201 -948.201], eps: 0.036})
Step:   89600, Reward: [-621.692 -621.692 -621.692] [270.304], Avg: [-475.431 -475.431 -475.431] (0.0100) ({r_i: None, r_t: [-1163.815 -1163.815 -1163.815], eps: 0.01})
Step:     900, Reward: [-462.989 -462.989 -462.989] [95.925], Avg: [-498.971 -498.971 -498.971] (0.9137) ({r_i: None, r_t: [-1026.071 -1026.071 -1026.071], eps: 0.914})
Step:   33300, Reward: [-468.611 -468.611 -468.611] [46.455], Avg: [-441.662 -441.662 -441.662] (0.0355) ({r_i: None, r_t: [-957.411 -957.411 -957.411], eps: 0.035})
Step:   89700, Reward: [-597.194 -597.194 -597.194] [255.019], Avg: [-475.567 -475.567 -475.567] (0.0100) ({r_i: None, r_t: [-1010.793 -1010.793 -1010.793], eps: 0.01})
Step:    1000, Reward: [-519.047 -519.047 -519.047] [104.831], Avg: [-500.796 -500.796 -500.796] (0.9046) ({r_i: None, r_t: [-1028.075 -1028.075 -1028.075], eps: 0.905})
Step:   89800, Reward: [-719.635 -719.635 -719.635] [366.896], Avg: [-475.838 -475.838 -475.838] (0.0100) ({r_i: None, r_t: [-1049.184 -1049.184 -1049.184], eps: 0.01})
Step:   33400, Reward: [-436.471 -436.471 -436.471] [61.150], Avg: [-441.646 -441.646 -441.646] (0.0351) ({r_i: None, r_t: [-931.404 -931.404 -931.404], eps: 0.035})
Step:   89900, Reward: [-579.005 -579.005 -579.005] [163.283], Avg: [-475.953 -475.953 -475.953] (0.0100) ({r_i: None, r_t: [-1180.983 -1180.983 -1180.983], eps: 0.01})
Step:    1100, Reward: [-478.830 -478.830 -478.830] [89.588], Avg: [-498.966 -498.966 -498.966] (0.8956) ({r_i: None, r_t: [-996.710 -996.710 -996.710], eps: 0.896})
Step:   33500, Reward: [-445.972 -445.972 -445.972] [57.648], Avg: [-441.659 -441.659 -441.659] (0.0348) ({r_i: None, r_t: [-905.169 -905.169 -905.169], eps: 0.035})
Step:   90000, Reward: [-495.444 -495.444 -495.444] [174.659], Avg: [-475.975 -475.975 -475.975] (0.0100) ({r_i: None, r_t: [-1113.206 -1113.206 -1113.206], eps: 0.01})
Step:    1200, Reward: [-549.130 -549.130 -549.130] [132.343], Avg: [-502.825 -502.825 -502.825] (0.8867) ({r_i: None, r_t: [-998.386 -998.386 -998.386], eps: 0.887})
Step:   33600, Reward: [-482.034 -482.034 -482.034] [86.520], Avg: [-441.779 -441.779 -441.779] (0.0344) ({r_i: None, r_t: [-931.317 -931.317 -931.317], eps: 0.034})
Step:   90100, Reward: [-528.052 -528.052 -528.052] [167.746], Avg: [-476.032 -476.032 -476.032] (0.0100) ({r_i: None, r_t: [-1142.168 -1142.168 -1142.168], eps: 0.01})
Step:    1300, Reward: [-521.919 -521.919 -521.919] [103.740], Avg: [-504.188 -504.188 -504.188] (0.8778) ({r_i: None, r_t: [-1018.320 -1018.320 -1018.320], eps: 0.878})
Step:   90200, Reward: [-466.830 -466.830 -466.830] [103.968], Avg: [-476.022 -476.022 -476.022] (0.0100) ({r_i: None, r_t: [-1012.885 -1012.885 -1012.885], eps: 0.01})
Step:   33700, Reward: [-480.614 -480.614 -480.614] [58.767], Avg: [-441.894 -441.894 -441.894] (0.0341) ({r_i: None, r_t: [-948.617 -948.617 -948.617], eps: 0.034})
Step:   90300, Reward: [-516.497 -516.497 -516.497] [230.652], Avg: [-476.067 -476.067 -476.067] (0.0100) ({r_i: None, r_t: [-1200.647 -1200.647 -1200.647], eps: 0.01})
Step:    1400, Reward: [-483.463 -483.463 -483.463] [82.188], Avg: [-502.807 -502.807 -502.807] (0.8691) ({r_i: None, r_t: [-976.763 -976.763 -976.763], eps: 0.869})
Step:   33800, Reward: [-488.335 -488.335 -488.335] [65.886], Avg: [-442.031 -442.031 -442.031] (0.0338) ({r_i: None, r_t: [-945.568 -945.568 -945.568], eps: 0.034})
Step:   90400, Reward: [-446.725 -446.725 -446.725] [93.682], Avg: [-476.035 -476.035 -476.035] (0.0100) ({r_i: None, r_t: [-1025.521 -1025.521 -1025.521], eps: 0.01})
Step:    1500, Reward: [-507.943 -507.943 -507.943] [142.520], Avg: [-503.128 -503.128 -503.128] (0.8604) ({r_i: None, r_t: [-995.416 -995.416 -995.416], eps: 0.86})
Step:   33900, Reward: [-465.922 -465.922 -465.922] [60.205], Avg: [-442.101 -442.101 -442.101] (0.0334) ({r_i: None, r_t: [-908.470 -908.470 -908.470], eps: 0.033})
Step:   90500, Reward: [-490.921 -490.921 -490.921] [83.561], Avg: [-476.051 -476.051 -476.051] (0.0100) ({r_i: None, r_t: [-1116.576 -1116.576 -1116.576], eps: 0.01})
Step:    1600, Reward: [-524.441 -524.441 -524.441] [105.190], Avg: [-504.381 -504.381 -504.381] (0.8518) ({r_i: None, r_t: [-1001.465 -1001.465 -1001.465], eps: 0.852})
Step:   90600, Reward: [-560.300 -560.300 -560.300] [200.204], Avg: [-476.144 -476.144 -476.144] (0.0100) ({r_i: None, r_t: [-1124.155 -1124.155 -1124.155], eps: 0.01})
Step:   34000, Reward: [-483.040 -483.040 -483.040] [68.874], Avg: [-442.221 -442.221 -442.221] (0.0331) ({r_i: None, r_t: [-936.950 -936.950 -936.950], eps: 0.033})
Step:   90700, Reward: [-473.209 -473.209 -473.209] [149.314], Avg: [-476.141 -476.141 -476.141] (0.0100) ({r_i: None, r_t: [-1018.956 -1018.956 -1018.956], eps: 0.01})
Step:    1700, Reward: [-439.136 -439.136 -439.136] [40.313], Avg: [-500.757 -500.757 -500.757] (0.8433) ({r_i: None, r_t: [-1007.070 -1007.070 -1007.070], eps: 0.843})
Step:   34100, Reward: [-479.698 -479.698 -479.698] [50.966], Avg: [-442.331 -442.331 -442.331] (0.0328) ({r_i: None, r_t: [-882.321 -882.321 -882.321], eps: 0.033})
Step:   90800, Reward: [-441.964 -441.964 -441.964] [91.504], Avg: [-476.103 -476.103 -476.103] (0.0100) ({r_i: None, r_t: [-963.947 -963.947 -963.947], eps: 0.01})
Step:    1800, Reward: [-504.827 -504.827 -504.827] [82.284], Avg: [-500.971 -500.971 -500.971] (0.8349) ({r_i: None, r_t: [-969.790 -969.790 -969.790], eps: 0.835})
Step:   34200, Reward: [-445.672 -445.672 -445.672] [67.328], Avg: [-442.340 -442.340 -442.340] (0.0324) ({r_i: None, r_t: [-946.803 -946.803 -946.803], eps: 0.032})
Step:   90900, Reward: [-467.508 -467.508 -467.508] [123.183], Avg: [-476.094 -476.094 -476.094] (0.0100) ({r_i: None, r_t: [-962.554 -962.554 -962.554], eps: 0.01})
Step:    1900, Reward: [-496.793 -496.793 -496.793] [100.282], Avg: [-500.762 -500.762 -500.762] (0.8266) ({r_i: None, r_t: [-992.031 -992.031 -992.031], eps: 0.827})
Step:   91000, Reward: [-512.924 -512.924 -512.924] [106.710], Avg: [-476.134 -476.134 -476.134] (0.0100) ({r_i: None, r_t: [-1040.360 -1040.360 -1040.360], eps: 0.01})
Step:   34300, Reward: [-434.949 -434.949 -434.949] [64.001], Avg: [-442.319 -442.319 -442.319] (0.0321) ({r_i: None, r_t: [-903.758 -903.758 -903.758], eps: 0.032})
Step:   91100, Reward: [-443.864 -443.864 -443.864] [81.445], Avg: [-476.099 -476.099 -476.099] (0.0100) ({r_i: None, r_t: [-957.563 -957.563 -957.563], eps: 0.01})
Step:    2000, Reward: [-472.336 -472.336 -472.336] [96.378], Avg: [-499.408 -499.408 -499.408] (0.8183) ({r_i: None, r_t: [-902.923 -902.923 -902.923], eps: 0.818})
Step:   34400, Reward: [-485.132 -485.132 -485.132] [66.091], Avg: [-442.443 -442.443 -442.443] (0.0318) ({r_i: None, r_t: [-906.665 -906.665 -906.665], eps: 0.032})
Step:   91200, Reward: [-574.491 -574.491 -574.491] [236.366], Avg: [-476.206 -476.206 -476.206] (0.0100) ({r_i: None, r_t: [-1021.711 -1021.711 -1021.711], eps: 0.01})
Step:    2100, Reward: [-511.359 -511.359 -511.359] [87.018], Avg: [-499.952 -499.952 -499.952] (0.8102) ({r_i: None, r_t: [-983.333 -983.333 -983.333], eps: 0.81})
Step:   34500, Reward: [-480.630 -480.630 -480.630] [52.790], Avg: [-442.553 -442.553 -442.553] (0.0315) ({r_i: None, r_t: [-886.282 -886.282 -886.282], eps: 0.031})
Step:   91300, Reward: [-415.556 -415.556 -415.556] [82.547], Avg: [-476.140 -476.140 -476.140] (0.0100) ({r_i: None, r_t: [-936.692 -936.692 -936.692], eps: 0.01})
Step:    2200, Reward: [-505.111 -505.111 -505.111] [104.569], Avg: [-500.176 -500.176 -500.176] (0.8021) ({r_i: None, r_t: [-992.950 -992.950 -992.950], eps: 0.802})
Step:   91400, Reward: [-438.517 -438.517 -438.517] [74.182], Avg: [-476.099 -476.099 -476.099] (0.0100) ({r_i: None, r_t: [-905.538 -905.538 -905.538], eps: 0.01})
Step:   34600, Reward: [-445.352 -445.352 -445.352] [75.467], Avg: [-442.561 -442.561 -442.561] (0.0312) ({r_i: None, r_t: [-937.870 -937.870 -937.870], eps: 0.031})
Step:   91500, Reward: [-460.852 -460.852 -460.852] [119.171], Avg: [-476.082 -476.082 -476.082] (0.0100) ({r_i: None, r_t: [-936.275 -936.275 -936.275], eps: 0.01})
Step:    2300, Reward: [-487.337 -487.337 -487.337] [66.278], Avg: [-499.641 -499.641 -499.641] (0.7941) ({r_i: None, r_t: [-1010.195 -1010.195 -1010.195], eps: 0.794})
Step:   34700, Reward: [-463.234 -463.234 -463.234] [79.301], Avg: [-442.621 -442.621 -442.621] (0.0308) ({r_i: None, r_t: [-928.699 -928.699 -928.699], eps: 0.031})
Step:   91600, Reward: [-475.310 -475.310 -475.310] [110.410], Avg: [-476.081 -476.081 -476.081] (0.0100) ({r_i: None, r_t: [-907.502 -907.502 -907.502], eps: 0.01})
Step:    2400, Reward: [-469.270 -469.270 -469.270] [81.035], Avg: [-498.426 -498.426 -498.426] (0.7862) ({r_i: None, r_t: [-969.751 -969.751 -969.751], eps: 0.786})
Step:   34800, Reward: [-446.667 -446.667 -446.667] [72.557], Avg: [-442.632 -442.632 -442.632] (0.0305) ({r_i: None, r_t: [-936.303 -936.303 -936.303], eps: 0.031})
Step:   91700, Reward: [-430.584 -430.584 -430.584] [99.609], Avg: [-476.032 -476.032 -476.032] (0.0100) ({r_i: None, r_t: [-908.174 -908.174 -908.174], eps: 0.01})
Step:    2500, Reward: [-492.390 -492.390 -492.390] [92.798], Avg: [-498.194 -498.194 -498.194] (0.7783) ({r_i: None, r_t: [-1004.822 -1004.822 -1004.822], eps: 0.778})
Step:   91800, Reward: [-430.621 -430.621 -430.621] [92.965], Avg: [-475.982 -475.982 -475.982] (0.0100) ({r_i: None, r_t: [-941.952 -941.952 -941.952], eps: 0.01})
Step:   34900, Reward: [-455.317 -455.317 -455.317] [54.890], Avg: [-442.669 -442.669 -442.669] (0.0302) ({r_i: None, r_t: [-947.901 -947.901 -947.901], eps: 0.03})
Step:   91900, Reward: [-503.093 -503.093 -503.093] [97.187], Avg: [-476.012 -476.012 -476.012] (0.0100) ({r_i: None, r_t: [-1003.463 -1003.463 -1003.463], eps: 0.01})
Step:    2600, Reward: [-491.074 -491.074 -491.074] [99.884], Avg: [-497.930 -497.930 -497.930] (0.7705) ({r_i: None, r_t: [-977.543 -977.543 -977.543], eps: 0.771})
Step:   35000, Reward: [-478.955 -478.955 -478.955] [68.894], Avg: [-442.772 -442.772 -442.772] (0.0299) ({r_i: None, r_t: [-889.962 -889.962 -889.962], eps: 0.03})
Step:   92000, Reward: [-435.586 -435.586 -435.586] [84.033], Avg: [-475.968 -475.968 -475.968] (0.0100) ({r_i: None, r_t: [-943.024 -943.024 -943.024], eps: 0.01})
Step:    2700, Reward: [-479.981 -479.981 -479.981] [57.178], Avg: [-497.289 -497.289 -497.289] (0.7629) ({r_i: None, r_t: [-995.201 -995.201 -995.201], eps: 0.763})
Step:   35100, Reward: [-449.749 -449.749 -449.749] [70.674], Avg: [-442.792 -442.792 -442.792] (0.0296) ({r_i: None, r_t: [-938.848 -938.848 -938.848], eps: 0.03})
Step:   92100, Reward: [-434.055 -434.055 -434.055] [104.950], Avg: [-475.923 -475.923 -475.923] (0.0100) ({r_i: None, r_t: [-896.665 -896.665 -896.665], eps: 0.01})
Step:   92200, Reward: [-456.202 -456.202 -456.202] [81.591], Avg: [-475.901 -475.901 -475.901] (0.0100) ({r_i: None, r_t: [-876.379 -876.379 -876.379], eps: 0.01})
Step:    2800, Reward: [-515.469 -515.469 -515.469] [67.682], Avg: [-497.916 -497.916 -497.916] (0.7553) ({r_i: None, r_t: [-989.147 -989.147 -989.147], eps: 0.755})
Step:   35200, Reward: [-474.954 -474.954 -474.954] [52.507], Avg: [-442.883 -442.883 -442.883] (0.0293) ({r_i: None, r_t: [-963.642 -963.642 -963.642], eps: 0.029})
Step:   92300, Reward: [-437.129 -437.129 -437.129] [120.538], Avg: [-475.859 -475.859 -475.859] (0.0100) ({r_i: None, r_t: [-950.475 -950.475 -950.475], eps: 0.01})
Step:    2900, Reward: [-494.728 -494.728 -494.728] [103.445], Avg: [-497.810 -497.810 -497.810] (0.7477) ({r_i: None, r_t: [-981.276 -981.276 -981.276], eps: 0.748})
Step:   35300, Reward: [-482.040 -482.040 -482.040] [89.869], Avg: [-442.994 -442.994 -442.994] (0.0290) ({r_i: None, r_t: [-976.977 -976.977 -976.977], eps: 0.029})
Step:   92400, Reward: [-494.303 -494.303 -494.303] [125.626], Avg: [-475.879 -475.879 -475.879] (0.0100) ({r_i: None, r_t: [-967.816 -967.816 -967.816], eps: 0.01})
Step:    3000, Reward: [-466.357 -466.357 -466.357] [61.656], Avg: [-496.795 -496.795 -496.795] (0.7403) ({r_i: None, r_t: [-997.030 -997.030 -997.030], eps: 0.74})
Step:   35400, Reward: [-468.491 -468.491 -468.491] [43.741], Avg: [-443.065 -443.065 -443.065] (0.0288) ({r_i: None, r_t: [-937.835 -937.835 -937.835], eps: 0.029})
Step:   92500, Reward: [-437.648 -437.648 -437.648] [86.952], Avg: [-475.838 -475.838 -475.838] (0.0100) ({r_i: None, r_t: [-896.440 -896.440 -896.440], eps: 0.01})
Step:   92600, Reward: [-446.892 -446.892 -446.892] [84.051], Avg: [-475.807 -475.807 -475.807] (0.0100) ({r_i: None, r_t: [-897.600 -897.600 -897.600], eps: 0.01})
Step:    3100, Reward: [-495.009 -495.009 -495.009] [85.575], Avg: [-496.739 -496.739 -496.739] (0.7329) ({r_i: None, r_t: [-1051.016 -1051.016 -1051.016], eps: 0.733})
Step:   35500, Reward: [-467.831 -467.831 -467.831] [64.424], Avg: [-443.135 -443.135 -443.135] (0.0285) ({r_i: None, r_t: [-951.945 -951.945 -951.945], eps: 0.028})
Step:   92700, Reward: [-397.873 -397.873 -397.873] [88.452], Avg: [-475.723 -475.723 -475.723] (0.0100) ({r_i: None, r_t: [-898.065 -898.065 -898.065], eps: 0.01})
Step:    3200, Reward: [-425.543 -425.543 -425.543] [36.239], Avg: [-494.582 -494.582 -494.582] (0.7256) ({r_i: None, r_t: [-975.436 -975.436 -975.436], eps: 0.726})
Step:   35600, Reward: [-435.101 -435.101 -435.101] [65.105], Avg: [-443.113 -443.113 -443.113] (0.0282) ({r_i: None, r_t: [-900.339 -900.339 -900.339], eps: 0.028})
Step:   92800, Reward: [-447.269 -447.269 -447.269] [82.840], Avg: [-475.692 -475.692 -475.692] (0.0100) ({r_i: None, r_t: [-839.587 -839.587 -839.587], eps: 0.01})
Step:    3300, Reward: [-472.784 -472.784 -472.784] [58.506], Avg: [-493.941 -493.941 -493.941] (0.7183) ({r_i: None, r_t: [-955.171 -955.171 -955.171], eps: 0.718})
Step:   35700, Reward: [-470.476 -470.476 -470.476] [55.483], Avg: [-443.189 -443.189 -443.189] (0.0279) ({r_i: None, r_t: [-952.272 -952.272 -952.272], eps: 0.028})
Step:   92900, Reward: [-435.039 -435.039 -435.039] [119.368], Avg: [-475.648 -475.648 -475.648] (0.0100) ({r_i: None, r_t: [-843.147 -843.147 -843.147], eps: 0.01})
Step:   93000, Reward: [-436.256 -436.256 -436.256] [90.678], Avg: [-475.606 -475.606 -475.606] (0.0100) ({r_i: None, r_t: [-865.216 -865.216 -865.216], eps: 0.01})
Step:    3400, Reward: [-498.180 -498.180 -498.180] [53.648], Avg: [-494.062 -494.062 -494.062] (0.7112) ({r_i: None, r_t: [-963.085 -963.085 -963.085], eps: 0.711})
Step:   35800, Reward: [-488.359 -488.359 -488.359] [110.106], Avg: [-443.315 -443.315 -443.315] (0.0276) ({r_i: None, r_t: [-882.534 -882.534 -882.534], eps: 0.028})
Step:   93100, Reward: [-471.134 -471.134 -471.134] [87.673], Avg: [-475.601 -475.601 -475.601] (0.0100) ({r_i: None, r_t: [-913.319 -913.319 -913.319], eps: 0.01})
Step:    3500, Reward: [-492.719 -492.719 -492.719] [73.683], Avg: [-494.025 -494.025 -494.025] (0.7041) ({r_i: None, r_t: [-1026.418 -1026.418 -1026.418], eps: 0.704})
Step:   35900, Reward: [-482.080 -482.080 -482.080] [81.880], Avg: [-443.422 -443.422 -443.422] (0.0274) ({r_i: None, r_t: [-975.222 -975.222 -975.222], eps: 0.027})
Step:   93200, Reward: [-442.991 -442.991 -442.991] [96.303], Avg: [-475.566 -475.566 -475.566] (0.0100) ({r_i: None, r_t: [-916.735 -916.735 -916.735], eps: 0.01})
Step:    3600, Reward: [-489.766 -489.766 -489.766] [90.130], Avg: [-493.910 -493.910 -493.910] (0.6970) ({r_i: None, r_t: [-978.391 -978.391 -978.391], eps: 0.697})
Step:   93300, Reward: [-428.172 -428.172 -428.172] [93.890], Avg: [-475.516 -475.516 -475.516] (0.0100) ({r_i: None, r_t: [-893.384 -893.384 -893.384], eps: 0.01})
Step:   36000, Reward: [-496.967 -496.967 -496.967] [46.546], Avg: [-443.571 -443.571 -443.571] (0.0271) ({r_i: None, r_t: [-939.651 -939.651 -939.651], eps: 0.027})
Step:   93400, Reward: [-463.082 -463.082 -463.082] [103.566], Avg: [-475.502 -475.502 -475.502] (0.0100) ({r_i: None, r_t: [-886.754 -886.754 -886.754], eps: 0.01})
Step:    3700, Reward: [-459.786 -459.786 -459.786] [77.197], Avg: [-493.012 -493.012 -493.012] (0.6901) ({r_i: None, r_t: [-962.345 -962.345 -962.345], eps: 0.69})
Step:   36100, Reward: [-481.934 -481.934 -481.934] [68.929], Avg: [-443.677 -443.677 -443.677] (0.0268) ({r_i: None, r_t: [-996.077 -996.077 -996.077], eps: 0.027})
Step:   93500, Reward: [-450.114 -450.114 -450.114] [80.290], Avg: [-475.475 -475.475 -475.475] (0.0100) ({r_i: None, r_t: [-811.847 -811.847 -811.847], eps: 0.01})
Step:    3800, Reward: [-436.292 -436.292 -436.292] [53.729], Avg: [-491.557 -491.557 -491.557] (0.6832) ({r_i: None, r_t: [-898.654 -898.654 -898.654], eps: 0.683})
Step:   36200, Reward: [-477.333 -477.333 -477.333] [56.435], Avg: [-443.769 -443.769 -443.769] (0.0265) ({r_i: None, r_t: [-969.154 -969.154 -969.154], eps: 0.027})
Step:   93600, Reward: [-425.490 -425.490 -425.490] [72.668], Avg: [-475.422 -475.422 -475.422] (0.0100) ({r_i: None, r_t: [-898.267 -898.267 -898.267], eps: 0.01})
Step:    3900, Reward: [-473.610 -473.610 -473.610] [64.264], Avg: [-491.109 -491.109 -491.109] (0.6764) ({r_i: None, r_t: [-923.875 -923.875 -923.875], eps: 0.676})
Step:   93700, Reward: [-434.276 -434.276 -434.276] [118.746], Avg: [-475.378 -475.378 -475.378] (0.0100) ({r_i: None, r_t: [-852.375 -852.375 -852.375], eps: 0.01})
Step:   36300, Reward: [-474.909 -474.909 -474.909] [82.025], Avg: [-443.855 -443.855 -443.855] (0.0263) ({r_i: None, r_t: [-928.739 -928.739 -928.739], eps: 0.026})
Step:   93800, Reward: [-444.994 -444.994 -444.994] [116.246], Avg: [-475.346 -475.346 -475.346] (0.0100) ({r_i: None, r_t: [-853.962 -853.962 -853.962], eps: 0.01})
Step:    4000, Reward: [-497.730 -497.730 -497.730] [95.199], Avg: [-491.270 -491.270 -491.270] (0.6696) ({r_i: None, r_t: [-982.813 -982.813 -982.813], eps: 0.67})
Step:   36400, Reward: [-500.950 -500.950 -500.950] [81.955], Avg: [-444.011 -444.011 -444.011] (0.0260) ({r_i: None, r_t: [-972.781 -972.781 -972.781], eps: 0.026})
Step:   93900, Reward: [-435.874 -435.874 -435.874] [93.796], Avg: [-475.304 -475.304 -475.304] (0.0100) ({r_i: None, r_t: [-974.836 -974.836 -974.836], eps: 0.01})
Step:    4100, Reward: [-534.498 -534.498 -534.498] [82.034], Avg: [-492.299 -492.299 -492.299] (0.6630) ({r_i: None, r_t: [-984.219 -984.219 -984.219], eps: 0.663})
Step:   36500, Reward: [-469.076 -469.076 -469.076] [67.925], Avg: [-444.080 -444.080 -444.080] (0.0258) ({r_i: None, r_t: [-929.186 -929.186 -929.186], eps: 0.026})
Step:   94000, Reward: [-454.987 -454.987 -454.987] [143.212], Avg: [-475.282 -475.282 -475.282] (0.0100) ({r_i: None, r_t: [-873.768 -873.768 -873.768], eps: 0.01})
Step:    4200, Reward: [-510.142 -510.142 -510.142] [93.839], Avg: [-492.714 -492.714 -492.714] (0.6564) ({r_i: None, r_t: [-974.383 -974.383 -974.383], eps: 0.656})
Step:   36600, Reward: [-493.528 -493.528 -493.528] [65.656], Avg: [-444.215 -444.215 -444.215] (0.0255) ({r_i: None, r_t: [-918.221 -918.221 -918.221], eps: 0.025})
Step:   94100, Reward: [-426.454 -426.454 -426.454] [74.578], Avg: [-475.230 -475.230 -475.230] (0.0100) ({r_i: None, r_t: [-851.341 -851.341 -851.341], eps: 0.01})
Step:   94200, Reward: [-444.056 -444.056 -444.056] [99.236], Avg: [-475.197 -475.197 -475.197] (0.0100) ({r_i: None, r_t: [-884.747 -884.747 -884.747], eps: 0.01})
Step:    4300, Reward: [-490.596 -490.596 -490.596] [82.442], Avg: [-492.666 -492.666 -492.666] (0.6498) ({r_i: None, r_t: [-953.542 -953.542 -953.542], eps: 0.65})
Step:   36700, Reward: [-506.930 -506.930 -506.930] [84.310], Avg: [-444.385 -444.385 -444.385] (0.0252) ({r_i: None, r_t: [-949.433 -949.433 -949.433], eps: 0.025})
Step:   94300, Reward: [-452.141 -452.141 -452.141] [101.082], Avg: [-475.173 -475.173 -475.173] (0.0100) ({r_i: None, r_t: [-843.791 -843.791 -843.791], eps: 0.01})
Step:    4400, Reward: [-497.159 -497.159 -497.159] [108.518], Avg: [-492.766 -492.766 -492.766] (0.6433) ({r_i: None, r_t: [-938.028 -938.028 -938.028], eps: 0.643})
Step:   36800, Reward: [-489.085 -489.085 -489.085] [59.851], Avg: [-444.506 -444.506 -444.506] (0.0250) ({r_i: None, r_t: [-964.526 -964.526 -964.526], eps: 0.025})
Step:   94400, Reward: [-432.765 -432.765 -432.765] [95.033], Avg: [-475.128 -475.128 -475.128] (0.0100) ({r_i: None, r_t: [-873.829 -873.829 -873.829], eps: 0.01})
Step:    4500, Reward: [-450.963 -450.963 -450.963] [74.235], Avg: [-491.857 -491.857 -491.857] (0.6369) ({r_i: None, r_t: [-969.290 -969.290 -969.290], eps: 0.637})
Step:   36900, Reward: [-498.462 -498.462 -498.462] [71.023], Avg: [-444.652 -444.652 -444.652] (0.0247) ({r_i: None, r_t: [-926.737 -926.737 -926.737], eps: 0.025})
Step:   94500, Reward: [-430.069 -430.069 -430.069] [61.582], Avg: [-475.080 -475.080 -475.080] (0.0100) ({r_i: None, r_t: [-865.808 -865.808 -865.808], eps: 0.01})
Step:   94600, Reward: [-435.238 -435.238 -435.238] [91.971], Avg: [-475.038 -475.038 -475.038] (0.0100) ({r_i: None, r_t: [-868.588 -868.588 -868.588], eps: 0.01})
Step:    4600, Reward: [-456.473 -456.473 -456.473] [91.616], Avg: [-491.104 -491.104 -491.104] (0.6306) ({r_i: None, r_t: [-1013.084 -1013.084 -1013.084], eps: 0.631})
Step:   37000, Reward: [-475.391 -475.391 -475.391] [59.381], Avg: [-444.735 -444.735 -444.735] (0.0245) ({r_i: None, r_t: [-913.020 -913.020 -913.020], eps: 0.024})
Step:   94700, Reward: [-416.564 -416.564 -416.564] [78.203], Avg: [-474.976 -474.976 -474.976] (0.0100) ({r_i: None, r_t: [-854.262 -854.262 -854.262], eps: 0.01})
Step:    4700, Reward: [-518.914 -518.914 -518.914] [134.639], Avg: [-491.684 -491.684 -491.684] (0.6243) ({r_i: None, r_t: [-1004.381 -1004.381 -1004.381], eps: 0.624})
Step:   37100, Reward: [-446.930 -446.930 -446.930] [68.938], Avg: [-444.741 -444.741 -444.741] (0.0243) ({r_i: None, r_t: [-925.128 -925.128 -925.128], eps: 0.024})
Step:   94800, Reward: [-419.404 -419.404 -419.404] [77.135], Avg: [-474.918 -474.918 -474.918] (0.0100) ({r_i: None, r_t: [-857.311 -857.311 -857.311], eps: 0.01})
Step:    4800, Reward: [-499.441 -499.441 -499.441] [92.919], Avg: [-491.842 -491.842 -491.842] (0.6180) ({r_i: None, r_t: [-1001.058 -1001.058 -1001.058], eps: 0.618})
Step:   37200, Reward: [-473.547 -473.547 -473.547] [53.686], Avg: [-444.818 -444.818 -444.818] (0.0240) ({r_i: None, r_t: [-953.891 -953.891 -953.891], eps: 0.024})
Step:   94900, Reward: [-408.572 -408.572 -408.572] [104.166], Avg: [-474.848 -474.848 -474.848] (0.0100) ({r_i: None, r_t: [-894.594 -894.594 -894.594], eps: 0.01})
Step:   95000, Reward: [-431.725 -431.725 -431.725] [85.465], Avg: [-474.803 -474.803 -474.803] (0.0100) ({r_i: None, r_t: [-870.035 -870.035 -870.035], eps: 0.01})
Step:    4900, Reward: [-481.553 -481.553 -481.553] [109.070], Avg: [-491.636 -491.636 -491.636] (0.6119) ({r_i: None, r_t: [-908.706 -908.706 -908.706], eps: 0.612})
Step:   37300, Reward: [-490.461 -490.461 -490.461] [79.376], Avg: [-444.940 -444.940 -444.940] (0.0238) ({r_i: None, r_t: [-968.238 -968.238 -968.238], eps: 0.024})
Step:   95100, Reward: [-442.131 -442.131 -442.131] [93.313], Avg: [-474.768 -474.768 -474.768] (0.0100) ({r_i: None, r_t: [-859.448 -859.448 -859.448], eps: 0.01})
Step:    5000, Reward: [-504.895 -504.895 -504.895] [125.618], Avg: [-491.896 -491.896 -491.896] (0.6058) ({r_i: None, r_t: [-999.312 -999.312 -999.312], eps: 0.606})
Step:   37400, Reward: [-468.000 -468.000 -468.000] [64.915], Avg: [-445.002 -445.002 -445.002] (0.0235) ({r_i: None, r_t: [-971.841 -971.841 -971.841], eps: 0.024})
Step:   95200, Reward: [-436.179 -436.179 -436.179] [124.422], Avg: [-474.728 -474.728 -474.728] (0.0100) ({r_i: None, r_t: [-893.998 -893.998 -893.998], eps: 0.01})
Step:    5100, Reward: [-521.521 -521.521 -521.521] [88.754], Avg: [-492.466 -492.466 -492.466] (0.5997) ({r_i: None, r_t: [-976.833 -976.833 -976.833], eps: 0.6})
Step:   95300, Reward: [-429.181 -429.181 -429.181] [98.000], Avg: [-474.680 -474.680 -474.680] (0.0100) ({r_i: None, r_t: [-919.482 -919.482 -919.482], eps: 0.01})
Step:   37500, Reward: [-475.294 -475.294 -475.294] [64.141], Avg: [-445.082 -445.082 -445.082] (0.0233) ({r_i: None, r_t: [-974.350 -974.350 -974.350], eps: 0.023})
Step:   95400, Reward: [-426.974 -426.974 -426.974] [85.669], Avg: [-474.630 -474.630 -474.630] (0.0100) ({r_i: None, r_t: [-876.121 -876.121 -876.121], eps: 0.01})
Step:    5200, Reward: [-478.164 -478.164 -478.164] [77.522], Avg: [-492.196 -492.196 -492.196] (0.5937) ({r_i: None, r_t: [-962.888 -962.888 -962.888], eps: 0.594})
Step:   37600, Reward: [-464.758 -464.758 -464.758] [79.410], Avg: [-445.134 -445.134 -445.134] (0.0231) ({r_i: None, r_t: [-920.383 -920.383 -920.383], eps: 0.023})
Step:   95500, Reward: [-431.003 -431.003 -431.003] [73.339], Avg: [-474.585 -474.585 -474.585] (0.0100) ({r_i: None, r_t: [-903.513 -903.513 -903.513], eps: 0.01})
Step:    5300, Reward: [-497.951 -497.951 -497.951] [103.065], Avg: [-492.303 -492.303 -492.303] (0.5878) ({r_i: None, r_t: [-921.986 -921.986 -921.986], eps: 0.588})
Step:   37700, Reward: [-477.180 -477.180 -477.180] [86.274], Avg: [-445.219 -445.219 -445.219] (0.0228) ({r_i: None, r_t: [-977.384 -977.384 -977.384], eps: 0.023})
Step:   95600, Reward: [-453.542 -453.542 -453.542] [113.493], Avg: [-474.563 -474.563 -474.563] (0.0100) ({r_i: None, r_t: [-893.866 -893.866 -893.866], eps: 0.01})
Step:    5400, Reward: [-511.104 -511.104 -511.104] [126.815], Avg: [-492.645 -492.645 -492.645] (0.5820) ({r_i: None, r_t: [-994.030 -994.030 -994.030], eps: 0.582})
Step:   95700, Reward: [-468.936 -468.936 -468.936] [94.677], Avg: [-474.557 -474.557 -474.557] (0.0100) ({r_i: None, r_t: [-826.656 -826.656 -826.656], eps: 0.01})
Step:   37800, Reward: [-489.112 -489.112 -489.112] [71.798], Avg: [-445.335 -445.335 -445.335] (0.0226) ({r_i: None, r_t: [-962.159 -962.159 -962.159], eps: 0.023})
Step:   95800, Reward: [-473.778 -473.778 -473.778] [74.933], Avg: [-474.556 -474.556 -474.556] (0.0100) ({r_i: None, r_t: [-909.079 -909.079 -909.079], eps: 0.01})
Step:    5500, Reward: [-495.244 -495.244 -495.244] [104.854], Avg: [-492.691 -492.691 -492.691] (0.5762) ({r_i: None, r_t: [-947.464 -947.464 -947.464], eps: 0.576})
Step:   37900, Reward: [-448.290 -448.290 -448.290] [60.956], Avg: [-445.343 -445.343 -445.343] (0.0224) ({r_i: None, r_t: [-951.226 -951.226 -951.226], eps: 0.022})
Step:   95900, Reward: [-440.000 -440.000 -440.000] [90.782], Avg: [-474.520 -474.520 -474.520] (0.0100) ({r_i: None, r_t: [-841.771 -841.771 -841.771], eps: 0.01})
Step:    5600, Reward: [-454.304 -454.304 -454.304] [81.409], Avg: [-492.017 -492.017 -492.017] (0.5704) ({r_i: None, r_t: [-977.814 -977.814 -977.814], eps: 0.57})
Step:   38000, Reward: [-452.660 -452.660 -452.660] [66.132], Avg: [-445.362 -445.362 -445.362] (0.0222) ({r_i: None, r_t: [-950.684 -950.684 -950.684], eps: 0.022})
Step:   96000, Reward: [-429.312 -429.312 -429.312] [52.020], Avg: [-474.473 -474.473 -474.473] (0.0100) ({r_i: None, r_t: [-913.030 -913.030 -913.030], eps: 0.01})
Step:    5700, Reward: [-471.978 -471.978 -471.978] [65.008], Avg: [-491.672 -491.672 -491.672] (0.5647) ({r_i: None, r_t: [-904.017 -904.017 -904.017], eps: 0.565})
Step:   96100, Reward: [-501.875 -501.875 -501.875] [119.967], Avg: [-474.501 -474.501 -474.501] (0.0100) ({r_i: None, r_t: [-806.396 -806.396 -806.396], eps: 0.01})
Step:   38100, Reward: [-500.215 -500.215 -500.215] [59.151], Avg: [-445.505 -445.505 -445.505] (0.0219) ({r_i: None, r_t: [-978.684 -978.684 -978.684], eps: 0.022})
Step:   96200, Reward: [-431.370 -431.370 -431.370] [88.851], Avg: [-474.457 -474.457 -474.457] (0.0100) ({r_i: None, r_t: [-867.934 -867.934 -867.934], eps: 0.01})
Step:    5800, Reward: [-444.878 -444.878 -444.878] [64.353], Avg: [-490.879 -490.879 -490.879] (0.5591) ({r_i: None, r_t: [-961.463 -961.463 -961.463], eps: 0.559})
Step:   38200, Reward: [-484.747 -484.747 -484.747] [71.620], Avg: [-445.608 -445.608 -445.608] (0.0217) ({r_i: None, r_t: [-943.133 -943.133 -943.133], eps: 0.022})
Step:   96300, Reward: [-448.160 -448.160 -448.160] [105.302], Avg: [-474.429 -474.429 -474.429] (0.0100) ({r_i: None, r_t: [-865.891 -865.891 -865.891], eps: 0.01})
Step:    5900, Reward: [-459.115 -459.115 -459.115] [113.869], Avg: [-490.349 -490.349 -490.349] (0.5535) ({r_i: None, r_t: [-975.522 -975.522 -975.522], eps: 0.554})
Step:   38300, Reward: [-483.851 -483.851 -483.851] [50.269], Avg: [-445.708 -445.708 -445.708] (0.0215) ({r_i: None, r_t: [-975.206 -975.206 -975.206], eps: 0.022})
Step:   96400, Reward: [-400.040 -400.040 -400.040] [56.183], Avg: [-474.352 -474.352 -474.352] (0.0100) ({r_i: None, r_t: [-892.954 -892.954 -892.954], eps: 0.01})
Step:    6000, Reward: [-475.543 -475.543 -475.543] [75.306], Avg: [-490.107 -490.107 -490.107] (0.5480) ({r_i: None, r_t: [-920.489 -920.489 -920.489], eps: 0.548})
Step:   96500, Reward: [-408.565 -408.565 -408.565] [65.213], Avg: [-474.284 -474.284 -474.284] (0.0100) ({r_i: None, r_t: [-870.949 -870.949 -870.949], eps: 0.01})
Step:   38400, Reward: [-514.224 -514.224 -514.224] [88.449], Avg: [-445.886 -445.886 -445.886] (0.0213) ({r_i: None, r_t: [-935.238 -935.238 -935.238], eps: 0.021})
Step:   96600, Reward: [-434.731 -434.731 -434.731] [68.765], Avg: [-474.243 -474.243 -474.243] (0.0100) ({r_i: None, r_t: [-857.096 -857.096 -857.096], eps: 0.01})
Step:    6100, Reward: [-434.145 -434.145 -434.145] [70.423], Avg: [-489.204 -489.204 -489.204] (0.5425) ({r_i: None, r_t: [-906.476 -906.476 -906.476], eps: 0.543})
Step:   38500, Reward: [-507.327 -507.327 -507.327] [76.416], Avg: [-446.045 -446.045 -446.045] (0.0211) ({r_i: None, r_t: [-939.230 -939.230 -939.230], eps: 0.021})
Step:   96700, Reward: [-433.624 -433.624 -433.624] [82.691], Avg: [-474.201 -474.201 -474.201] (0.0100) ({r_i: None, r_t: [-869.998 -869.998 -869.998], eps: 0.01})
Step:    6200, Reward: [-457.307 -457.307 -457.307] [50.197], Avg: [-488.698 -488.698 -488.698] (0.5371) ({r_i: None, r_t: [-910.897 -910.897 -910.897], eps: 0.537})
Step:   38600, Reward: [-491.175 -491.175 -491.175] [72.771], Avg: [-446.161 -446.161 -446.161] (0.0209) ({r_i: None, r_t: [-937.563 -937.563 -937.563], eps: 0.021})
Step:   96800, Reward: [-467.591 -467.591 -467.591] [101.153], Avg: [-474.194 -474.194 -474.194] (0.0100) ({r_i: None, r_t: [-897.466 -897.466 -897.466], eps: 0.01})
Step:    6300, Reward: [-464.128 -464.128 -464.128] [76.968], Avg: [-488.314 -488.314 -488.314] (0.5318) ({r_i: None, r_t: [-914.666 -914.666 -914.666], eps: 0.532})
Step:   96900, Reward: [-452.543 -452.543 -452.543] [91.558], Avg: [-474.172 -474.172 -474.172] (0.0100) ({r_i: None, r_t: [-865.865 -865.865 -865.865], eps: 0.01})
Step:   38700, Reward: [-464.017 -464.017 -464.017] [65.615], Avg: [-446.207 -446.207 -446.207] (0.0207) ({r_i: None, r_t: [-959.435 -959.435 -959.435], eps: 0.021})
Step:   97000, Reward: [-460.634 -460.634 -460.634] [83.683], Avg: [-474.158 -474.158 -474.158] (0.0100) ({r_i: None, r_t: [-864.906 -864.906 -864.906], eps: 0.01})
Step:    6400, Reward: [-461.999 -461.999 -461.999] [110.064], Avg: [-487.909 -487.909 -487.909] (0.5264) ({r_i: None, r_t: [-959.877 -959.877 -959.877], eps: 0.526})
Step:   38800, Reward: [-479.265 -479.265 -479.265] [53.330], Avg: [-446.292 -446.292 -446.292] (0.0205) ({r_i: None, r_t: [-976.668 -976.668 -976.668], eps: 0.02})
Step:   97100, Reward: [-446.344 -446.344 -446.344] [78.838], Avg: [-474.129 -474.129 -474.129] (0.0100) ({r_i: None, r_t: [-840.563 -840.563 -840.563], eps: 0.01})
Step:    6500, Reward: [-492.490 -492.490 -492.490] [84.177], Avg: [-487.978 -487.978 -487.978] (0.5212) ({r_i: None, r_t: [-908.391 -908.391 -908.391], eps: 0.521})
Step:   38900, Reward: [-469.714 -469.714 -469.714] [64.920], Avg: [-446.352 -446.352 -446.352] (0.0202) ({r_i: None, r_t: [-917.393 -917.393 -917.393], eps: 0.02})
Step:   97200, Reward: [-443.716 -443.716 -443.716] [57.561], Avg: [-474.098 -474.098 -474.098] (0.0100) ({r_i: None, r_t: [-863.244 -863.244 -863.244], eps: 0.01})
Step:    6600, Reward: [-517.504 -517.504 -517.504] [112.730], Avg: [-488.419 -488.419 -488.419] (0.5160) ({r_i: None, r_t: [-948.652 -948.652 -948.652], eps: 0.516})
Step:   97300, Reward: [-433.020 -433.020 -433.020] [72.575], Avg: [-474.056 -474.056 -474.056] (0.0100) ({r_i: None, r_t: [-838.456 -838.456 -838.456], eps: 0.01})
Step:   39000, Reward: [-465.057 -465.057 -465.057] [55.548], Avg: [-446.400 -446.400 -446.400] (0.0200) ({r_i: None, r_t: [-1018.959 -1018.959 -1018.959], eps: 0.02})
Step:   97400, Reward: [-416.238 -416.238 -416.238] [97.514], Avg: [-473.997 -473.997 -473.997] (0.0100) ({r_i: None, r_t: [-864.496 -864.496 -864.496], eps: 0.01})
Step:    6700, Reward: [-504.925 -504.925 -504.925] [97.628], Avg: [-488.662 -488.662 -488.662] (0.5108) ({r_i: None, r_t: [-877.904 -877.904 -877.904], eps: 0.511})
Step:   39100, Reward: [-488.640 -488.640 -488.640] [62.510], Avg: [-446.508 -446.508 -446.508] (0.0198) ({r_i: None, r_t: [-966.409 -966.409 -966.409], eps: 0.02})
Step:   97500, Reward: [-448.135 -448.135 -448.135] [96.475], Avg: [-473.970 -473.970 -473.970] (0.0100) ({r_i: None, r_t: [-843.693 -843.693 -843.693], eps: 0.01})
Step:    6800, Reward: [-439.990 -439.990 -439.990] [81.010], Avg: [-487.957 -487.957 -487.957] (0.5058) ({r_i: None, r_t: [-927.799 -927.799 -927.799], eps: 0.506})
Step:   39200, Reward: [-490.920 -490.920 -490.920] [42.878], Avg: [-446.621 -446.621 -446.621] (0.0196) ({r_i: None, r_t: [-970.861 -970.861 -970.861], eps: 0.02})
Step:   97600, Reward: [-407.589 -407.589 -407.589] [69.802], Avg: [-473.902 -473.902 -473.902] (0.0100) ({r_i: None, r_t: [-865.700 -865.700 -865.700], eps: 0.01})
Step:   97700, Reward: [-419.609 -419.609 -419.609] [88.333], Avg: [-473.847 -473.847 -473.847] (0.0100) ({r_i: None, r_t: [-835.625 -835.625 -835.625], eps: 0.01})
Step:    6900, Reward: [-469.140 -469.140 -469.140] [78.705], Avg: [-487.688 -487.688 -487.688] (0.5007) ({r_i: None, r_t: [-965.755 -965.755 -965.755], eps: 0.501})
Step:   39300, Reward: [-501.531 -501.531 -501.531] [81.679], Avg: [-446.760 -446.760 -446.760] (0.0195) ({r_i: None, r_t: [-945.475 -945.475 -945.475], eps: 0.019})
Step:   97800, Reward: [-401.600 -401.600 -401.600] [77.069], Avg: [-473.773 -473.773 -473.773] (0.0100) ({r_i: None, r_t: [-827.757 -827.757 -827.757], eps: 0.01})
Step:    7000, Reward: [-494.338 -494.338 -494.338] [70.919], Avg: [-487.781 -487.781 -487.781] (0.4957) ({r_i: None, r_t: [-882.925 -882.925 -882.925], eps: 0.496})
Step:   39400, Reward: [-477.272 -477.272 -477.272] [50.287], Avg: [-446.838 -446.838 -446.838] (0.0193) ({r_i: None, r_t: [-1001.666 -1001.666 -1001.666], eps: 0.019})
Step:   97900, Reward: [-412.913 -412.913 -412.913] [84.223], Avg: [-473.711 -473.711 -473.711] (0.0100) ({r_i: None, r_t: [-864.058 -864.058 -864.058], eps: 0.01})
Step:    7100, Reward: [-492.293 -492.293 -492.293] [89.419], Avg: [-487.844 -487.844 -487.844] (0.4908) ({r_i: None, r_t: [-963.594 -963.594 -963.594], eps: 0.491})
Step:   39500, Reward: [-476.991 -476.991 -476.991] [61.348], Avg: [-446.914 -446.914 -446.914] (0.0191) ({r_i: None, r_t: [-909.482 -909.482 -909.482], eps: 0.019})
Step:   98000, Reward: [-422.326 -422.326 -422.326] [85.251], Avg: [-473.659 -473.659 -473.659] (0.0100) ({r_i: None, r_t: [-845.325 -845.325 -845.325], eps: 0.01})
Step:   98100, Reward: [-412.319 -412.319 -412.319] [102.284], Avg: [-473.596 -473.596 -473.596] (0.0100) ({r_i: None, r_t: [-736.063 -736.063 -736.063], eps: 0.01})
Step:    7200, Reward: [-476.816 -476.816 -476.816] [66.854], Avg: [-487.693 -487.693 -487.693] (0.4859) ({r_i: None, r_t: [-878.000 -878.000 -878.000], eps: 0.486})
Step:   39600, Reward: [-475.053 -475.053 -475.053] [59.811], Avg: [-446.985 -446.985 -446.985] (0.0189) ({r_i: None, r_t: [-986.500 -986.500 -986.500], eps: 0.019})
Step:   98200, Reward: [-466.386 -466.386 -466.386] [113.290], Avg: [-473.589 -473.589 -473.589] (0.0100) ({r_i: None, r_t: [-854.795 -854.795 -854.795], eps: 0.01})
Step:    7300, Reward: [-453.373 -453.373 -453.373] [70.685], Avg: [-487.229 -487.229 -487.229] (0.4810) ({r_i: None, r_t: [-909.717 -909.717 -909.717], eps: 0.481})
Step:   39700, Reward: [-491.163 -491.163 -491.163] [79.180], Avg: [-447.096 -447.096 -447.096] (0.0187) ({r_i: None, r_t: [-958.016 -958.016 -958.016], eps: 0.019})
Step:   98300, Reward: [-442.255 -442.255 -442.255] [97.924], Avg: [-473.557 -473.557 -473.557] (0.0100) ({r_i: None, r_t: [-888.027 -888.027 -888.027], eps: 0.01})
Step:    7400, Reward: [-454.270 -454.270 -454.270] [56.991], Avg: [-486.790 -486.790 -486.790] (0.4762) ({r_i: None, r_t: [-920.888 -920.888 -920.888], eps: 0.476})
Step:   98400, Reward: [-413.491 -413.491 -413.491] [68.374], Avg: [-473.496 -473.496 -473.496] (0.0100) ({r_i: None, r_t: [-851.907 -851.907 -851.907], eps: 0.01})
Step:   39800, Reward: [-446.584 -446.584 -446.584] [57.079], Avg: [-447.094 -447.094 -447.094] (0.0185) ({r_i: None, r_t: [-969.850 -969.850 -969.850], eps: 0.019})
Step:   98500, Reward: [-383.418 -383.418 -383.418] [92.863], Avg: [-473.405 -473.405 -473.405] (0.0100) ({r_i: None, r_t: [-835.373 -835.373 -835.373], eps: 0.01})
Step:    7500, Reward: [-474.586 -474.586 -474.586] [73.676], Avg: [-486.629 -486.629 -486.629] (0.4715) ({r_i: None, r_t: [-896.477 -896.477 -896.477], eps: 0.471})
Step:   39900, Reward: [-502.301 -502.301 -502.301] [86.519], Avg: [-447.232 -447.232 -447.232] (0.0183) ({r_i: None, r_t: [-1013.929 -1013.929 -1013.929], eps: 0.018})
Step:   98600, Reward: [-427.117 -427.117 -427.117] [86.020], Avg: [-473.358 -473.358 -473.358] (0.0100) ({r_i: None, r_t: [-892.910 -892.910 -892.910], eps: 0.01})
Step:    7600, Reward: [-412.578 -412.578 -412.578] [72.286], Avg: [-485.667 -485.667 -485.667] (0.4668) ({r_i: None, r_t: [-935.429 -935.429 -935.429], eps: 0.467})
Step:   40000, Reward: [-499.195 -499.195 -499.195] [72.922], Avg: [-447.362 -447.362 -447.362] (0.0181) ({r_i: None, r_t: [-980.091 -980.091 -980.091], eps: 0.018})
Step:   98700, Reward: [-411.614 -411.614 -411.614] [57.907], Avg: [-473.295 -473.295 -473.295] (0.0100) ({r_i: None, r_t: [-810.623 -810.623 -810.623], eps: 0.01})
Step:    7700, Reward: [-441.763 -441.763 -441.763] [57.096], Avg: [-485.105 -485.105 -485.105] (0.4621) ({r_i: None, r_t: [-894.798 -894.798 -894.798], eps: 0.462})
Step:   98800, Reward: [-410.636 -410.636 -410.636] [73.009], Avg: [-473.232 -473.232 -473.232] (0.0100) ({r_i: None, r_t: [-844.221 -844.221 -844.221], eps: 0.01})
Step:   40100, Reward: [-462.781 -462.781 -462.781] [45.202], Avg: [-447.400 -447.400 -447.400] (0.0180) ({r_i: None, r_t: [-961.336 -961.336 -961.336], eps: 0.018})
Step:   98900, Reward: [-479.723 -479.723 -479.723] [79.232], Avg: [-473.238 -473.238 -473.238] (0.0100) ({r_i: None, r_t: [-833.708 -833.708 -833.708], eps: 0.01})
Step:    7800, Reward: [-486.002 -486.002 -486.002] [79.741], Avg: [-485.116 -485.116 -485.116] (0.4575) ({r_i: None, r_t: [-961.735 -961.735 -961.735], eps: 0.458})
Step:   40200, Reward: [-453.839 -453.839 -453.839] [49.931], Avg: [-447.416 -447.416 -447.416] (0.0178) ({r_i: None, r_t: [-918.944 -918.944 -918.944], eps: 0.018})
Step:   99000, Reward: [-393.576 -393.576 -393.576] [90.721], Avg: [-473.158 -473.158 -473.158] (0.0100) ({r_i: None, r_t: [-871.980 -871.980 -871.980], eps: 0.01})
Step:    7900, Reward: [-459.146 -459.146 -459.146] [81.562], Avg: [-484.791 -484.791 -484.791] (0.4529) ({r_i: None, r_t: [-911.330 -911.330 -911.330], eps: 0.453})
Step:   40300, Reward: [-459.689 -459.689 -459.689] [76.912], Avg: [-447.447 -447.447 -447.447] (0.0176) ({r_i: None, r_t: [-950.908 -950.908 -950.908], eps: 0.018})
Step:   99100, Reward: [-458.167 -458.167 -458.167] [90.728], Avg: [-473.143 -473.143 -473.143] (0.0100) ({r_i: None, r_t: [-825.237 -825.237 -825.237], eps: 0.01})
Step:    8000, Reward: [-430.574 -430.574 -430.574] [96.845], Avg: [-484.122 -484.122 -484.122] (0.4484) ({r_i: None, r_t: [-955.839 -955.839 -955.839], eps: 0.448})
Step:   99200, Reward: [-389.261 -389.261 -389.261] [63.615], Avg: [-473.058 -473.058 -473.058] (0.0100) ({r_i: None, r_t: [-823.500 -823.500 -823.500], eps: 0.01})
Step:   40400, Reward: [-476.785 -476.785 -476.785] [68.661], Avg: [-447.519 -447.519 -447.519] (0.0174) ({r_i: None, r_t: [-979.842 -979.842 -979.842], eps: 0.017})
Step:   99300, Reward: [-396.045 -396.045 -396.045] [69.946], Avg: [-472.981 -472.981 -472.981] (0.0100) ({r_i: None, r_t: [-754.744 -754.744 -754.744], eps: 0.01})
Step:    8100, Reward: [-484.616 -484.616 -484.616] [102.251], Avg: [-484.128 -484.128 -484.128] (0.4440) ({r_i: None, r_t: [-913.632 -913.632 -913.632], eps: 0.444})
Step:   40500, Reward: [-522.396 -522.396 -522.396] [57.762], Avg: [-447.703 -447.703 -447.703] (0.0172) ({r_i: None, r_t: [-891.002 -891.002 -891.002], eps: 0.017})
Step:   99400, Reward: [-407.210 -407.210 -407.210] [99.601], Avg: [-472.915 -472.915 -472.915] (0.0100) ({r_i: None, r_t: [-806.226 -806.226 -806.226], eps: 0.01})
Step:    8200, Reward: [-442.458 -442.458 -442.458] [86.568], Avg: [-483.626 -483.626 -483.626] (0.4395) ({r_i: None, r_t: [-877.075 -877.075 -877.075], eps: 0.44})
Step:   40600, Reward: [-464.888 -464.888 -464.888] [65.148], Avg: [-447.746 -447.746 -447.746] (0.0171) ({r_i: None, r_t: [-909.707 -909.707 -909.707], eps: 0.017})
Step:   99500, Reward: [-388.311 -388.311 -388.311] [88.599], Avg: [-472.830 -472.830 -472.830] (0.0100) ({r_i: None, r_t: [-791.617 -791.617 -791.617], eps: 0.01})
Step:   99600, Reward: [-386.234 -386.234 -386.234] [82.644], Avg: [-472.743 -472.743 -472.743] (0.0100) ({r_i: None, r_t: [-813.049 -813.049 -813.049], eps: 0.01})
Step:    8300, Reward: [-454.031 -454.031 -454.031] [72.722], Avg: [-483.274 -483.274 -483.274] (0.4351) ({r_i: None, r_t: [-927.823 -927.823 -927.823], eps: 0.435})
Step:   40700, Reward: [-485.340 -485.340 -485.340] [57.237], Avg: [-447.838 -447.838 -447.838] (0.0169) ({r_i: None, r_t: [-949.013 -949.013 -949.013], eps: 0.017})
Step:   99700, Reward: [-384.172 -384.172 -384.172] [66.090], Avg: [-472.654 -472.654 -472.654] (0.0100) ({r_i: None, r_t: [-770.648 -770.648 -770.648], eps: 0.01})
Step:    8400, Reward: [-463.071 -463.071 -463.071] [96.207], Avg: [-483.036 -483.036 -483.036] (0.4308) ({r_i: None, r_t: [-914.969 -914.969 -914.969], eps: 0.431})
Step:   40800, Reward: [-502.287 -502.287 -502.287] [91.978], Avg: [-447.971 -447.971 -447.971] (0.0167) ({r_i: None, r_t: [-981.806 -981.806 -981.806], eps: 0.017})
Step:   99800, Reward: [-396.832 -396.832 -396.832] [74.292], Avg: [-472.578 -472.578 -472.578] (0.0100) ({r_i: None, r_t: [-806.445 -806.445 -806.445], eps: 0.01})
Step:    8500, Reward: [-458.196 -458.196 -458.196] [69.386], Avg: [-482.747 -482.747 -482.747] (0.4265) ({r_i: None, r_t: [-943.860 -943.860 -943.860], eps: 0.427})
Step:   99900, Reward: [-410.049 -410.049 -410.049] [93.823], Avg: [-472.516 -472.516 -472.516] (0.0100) ({r_i: None, r_t: [-795.536 -795.536 -795.536], eps: 0.01})
Step:   40900, Reward: [-485.462 -485.462 -485.462] [69.002], Avg: [-448.062 -448.062 -448.062] (0.0166) ({r_i: None, r_t: [-957.040 -957.040 -957.040], eps: 0.017})
Step:  100000, Reward: [-400.349 -400.349 -400.349] [75.309], Avg: [-472.444 -472.444 -472.444] (0.0100) ({r_i: None, r_t: [-834.752 -834.752 -834.752], eps: 0.01})
Step:    8600, Reward: [-450.395 -450.395 -450.395] [99.966], Avg: [-482.375 -482.375 -482.375] (0.4223) ({r_i: None, r_t: [-860.063 -860.063 -860.063], eps: 0.422})
Step:   41000, Reward: [-477.846 -477.846 -477.846] [55.900], Avg: [-448.135 -448.135 -448.135] (0.0164) ({r_i: None, r_t: [-967.856 -967.856 -967.856], eps: 0.016})
Step:  100100, Reward: [-410.312 -410.312 -410.312] [76.976], Avg: [-472.382 -472.382 -472.382] (0.0100) ({r_i: None, r_t: [-889.001 -889.001 -889.001], eps: 0.01})
Step:    8700, Reward: [-436.947 -436.947 -436.947] [39.543], Avg: [-481.859 -481.859 -481.859] (0.4180) ({r_i: None, r_t: [-923.040 -923.040 -923.040], eps: 0.418})
Step:   41100, Reward: [-493.156 -493.156 -493.156] [68.171], Avg: [-448.244 -448.244 -448.244] (0.0162) ({r_i: None, r_t: [-932.952 -932.952 -932.952], eps: 0.016})
Step:  100200, Reward: [-418.633 -418.633 -418.633] [83.942], Avg: [-472.328 -472.328 -472.328] (0.0100) ({r_i: None, r_t: [-812.811 -812.811 -812.811], eps: 0.01})
Step:    8800, Reward: [-473.192 -473.192 -473.192] [85.450], Avg: [-481.762 -481.762 -481.762] (0.4139) ({r_i: None, r_t: [-916.111 -916.111 -916.111], eps: 0.414})
Step:  100300, Reward: [-456.848 -456.848 -456.848] [102.748], Avg: [-472.313 -472.313 -472.313] (0.0100) ({r_i: None, r_t: [-836.128 -836.128 -836.128], eps: 0.01})
Step:   41200, Reward: [-443.812 -443.812 -443.812] [68.054], Avg: [-448.233 -448.233 -448.233] (0.0161) ({r_i: None, r_t: [-975.479 -975.479 -975.479], eps: 0.016})
Step:  100400, Reward: [-395.856 -395.856 -395.856] [86.090], Avg: [-472.237 -472.237 -472.237] (0.0100) ({r_i: None, r_t: [-838.361 -838.361 -838.361], eps: 0.01})
Step:    8900, Reward: [-469.202 -469.202 -469.202] [85.460], Avg: [-481.622 -481.622 -481.622] (0.4097) ({r_i: None, r_t: [-936.109 -936.109 -936.109], eps: 0.41})
Step:   41300, Reward: [-454.636 -454.636 -454.636] [63.736], Avg: [-448.249 -448.249 -448.249] (0.0159) ({r_i: None, r_t: [-969.410 -969.410 -969.410], eps: 0.016})
Step:  100500, Reward: [-400.014 -400.014 -400.014] [78.479], Avg: [-472.165 -472.165 -472.165] (0.0100) ({r_i: None, r_t: [-842.790 -842.790 -842.790], eps: 0.01})
Step:    9000, Reward: [-469.952 -469.952 -469.952] [71.351], Avg: [-481.494 -481.494 -481.494] (0.4057) ({r_i: None, r_t: [-856.733 -856.733 -856.733], eps: 0.406})
Step:   41400, Reward: [-480.376 -480.376 -480.376] [61.163], Avg: [-448.326 -448.326 -448.326] (0.0158) ({r_i: None, r_t: [-967.119 -967.119 -967.119], eps: 0.016})
Step:  100600, Reward: [-427.730 -427.730 -427.730] [62.284], Avg: [-472.121 -472.121 -472.121] (0.0100) ({r_i: None, r_t: [-830.109 -830.109 -830.109], eps: 0.01})
Step:    9100, Reward: [-459.573 -459.573 -459.573] [100.022], Avg: [-481.256 -481.256 -481.256] (0.4016) ({r_i: None, r_t: [-874.353 -874.353 -874.353], eps: 0.402})
Step:  100700, Reward: [-432.308 -432.308 -432.308] [56.377], Avg: [-472.081 -472.081 -472.081] (0.0100) ({r_i: None, r_t: [-865.233 -865.233 -865.233], eps: 0.01})
Step:   41500, Reward: [-484.627 -484.627 -484.627] [59.290], Avg: [-448.414 -448.414 -448.414] (0.0156) ({r_i: None, r_t: [-936.187 -936.187 -936.187], eps: 0.016})
Step:  100800, Reward: [-403.157 -403.157 -403.157] [68.092], Avg: [-472.013 -472.013 -472.013] (0.0100) ({r_i: None, r_t: [-840.339 -840.339 -840.339], eps: 0.01})
Step:    9200, Reward: [-456.645 -456.645 -456.645] [65.468], Avg: [-480.991 -480.991 -480.991] (0.3976) ({r_i: None, r_t: [-933.482 -933.482 -933.482], eps: 0.398})
Step:   41600, Reward: [-485.391 -485.391 -485.391] [78.254], Avg: [-448.502 -448.502 -448.502] (0.0154) ({r_i: None, r_t: [-940.753 -940.753 -940.753], eps: 0.015})
Step:  100900, Reward: [-441.356 -441.356 -441.356] [86.290], Avg: [-471.983 -471.983 -471.983] (0.0100) ({r_i: None, r_t: [-891.228 -891.228 -891.228], eps: 0.01})
Step:    9300, Reward: [-458.208 -458.208 -458.208] [75.778], Avg: [-480.749 -480.749 -480.749] (0.3936) ({r_i: None, r_t: [-914.642 -914.642 -914.642], eps: 0.394})
Step:   41700, Reward: [-478.557 -478.557 -478.557] [65.556], Avg: [-448.574 -448.574 -448.574] (0.0153) ({r_i: None, r_t: [-948.082 -948.082 -948.082], eps: 0.015})
Step:  101000, Reward: [-387.543 -387.543 -387.543] [78.513], Avg: [-471.899 -471.899 -471.899] (0.0100) ({r_i: None, r_t: [-863.699 -863.699 -863.699], eps: 0.01})
Step:    9400, Reward: [-441.947 -441.947 -441.947] [73.262], Avg: [-480.340 -480.340 -480.340] (0.3897) ({r_i: None, r_t: [-831.938 -831.938 -831.938], eps: 0.39})
Step:  101100, Reward: [-412.941 -412.941 -412.941] [78.721], Avg: [-471.841 -471.841 -471.841] (0.0100) ({r_i: None, r_t: [-809.564 -809.564 -809.564], eps: 0.01})
Step:   41800, Reward: [-470.042 -470.042 -470.042] [74.806], Avg: [-448.625 -448.625 -448.625] (0.0151) ({r_i: None, r_t: [-946.244 -946.244 -946.244], eps: 0.015})
Step:  101200, Reward: [-438.347 -438.347 -438.347] [108.279], Avg: [-471.808 -471.808 -471.808] (0.0100) ({r_i: None, r_t: [-817.897 -817.897 -817.897], eps: 0.01})
Step:    9500, Reward: [-448.441 -448.441 -448.441] [79.328], Avg: [-480.008 -480.008 -480.008] (0.3858) ({r_i: None, r_t: [-905.891 -905.891 -905.891], eps: 0.386})
Step:   41900, Reward: [-481.330 -481.330 -481.330] [61.803], Avg: [-448.703 -448.703 -448.703] (0.0150) ({r_i: None, r_t: [-933.683 -933.683 -933.683], eps: 0.015})
Step:  101300, Reward: [-384.359 -384.359 -384.359] [68.883], Avg: [-471.721 -471.721 -471.721] (0.0100) ({r_i: None, r_t: [-805.607 -805.607 -805.607], eps: 0.01})
Step:    9600, Reward: [-423.052 -423.052 -423.052] [66.165], Avg: [-479.421 -479.421 -479.421] (0.3820) ({r_i: None, r_t: [-929.001 -929.001 -929.001], eps: 0.382})
Step:   42000, Reward: [-469.859 -469.859 -469.859] [80.936], Avg: [-448.754 -448.754 -448.754] (0.0148) ({r_i: None, r_t: [-989.423 -989.423 -989.423], eps: 0.015})
Step:  101400, Reward: [-411.617 -411.617 -411.617] [120.025], Avg: [-471.662 -471.662 -471.662] (0.0100) ({r_i: None, r_t: [-822.270 -822.270 -822.270], eps: 0.01})
Step:  101500, Reward: [-423.563 -423.563 -423.563] [80.844], Avg: [-471.615 -471.615 -471.615] (0.0100) ({r_i: None, r_t: [-823.555 -823.555 -823.555], eps: 0.01})
Step:    9700, Reward: [-458.605 -458.605 -458.605] [125.524], Avg: [-479.208 -479.208 -479.208] (0.3782) ({r_i: None, r_t: [-894.793 -894.793 -894.793], eps: 0.378})
Step:   42100, Reward: [-480.711 -480.711 -480.711] [49.051], Avg: [-448.829 -448.829 -448.829] (0.0147) ({r_i: None, r_t: [-936.771 -936.771 -936.771], eps: 0.015})
Step:  101600, Reward: [-435.735 -435.735 -435.735] [63.748], Avg: [-471.580 -471.580 -471.580] (0.0100) ({r_i: None, r_t: [-862.035 -862.035 -862.035], eps: 0.01})
Step:    9800, Reward: [-454.075 -454.075 -454.075] [107.194], Avg: [-478.954 -478.954 -478.954] (0.3744) ({r_i: None, r_t: [-914.761 -914.761 -914.761], eps: 0.374})
Step:   42200, Reward: [-485.923 -485.923 -485.923] [86.650], Avg: [-448.917 -448.917 -448.917] (0.0145) ({r_i: None, r_t: [-978.276 -978.276 -978.276], eps: 0.015})
Step:  101700, Reward: [-435.822 -435.822 -435.822] [63.124], Avg: [-471.545 -471.545 -471.545] (0.0100) ({r_i: None, r_t: [-788.422 -788.422 -788.422], eps: 0.01})
Step:    9900, Reward: [-451.750 -451.750 -451.750] [69.566], Avg: [-478.682 -478.682 -478.682] (0.3707) ({r_i: None, r_t: [-875.800 -875.800 -875.800], eps: 0.371})
Step:   42300, Reward: [-482.536 -482.536 -482.536] [69.845], Avg: [-448.996 -448.996 -448.996] (0.0144) ({r_i: None, r_t: [-983.111 -983.111 -983.111], eps: 0.014})
Step:  101800, Reward: [-378.866 -378.866 -378.866] [57.335], Avg: [-471.454 -471.454 -471.454] (0.0100) ({r_i: None, r_t: [-807.148 -807.148 -807.148], eps: 0.01})
Step:  101900, Reward: [-386.842 -386.842 -386.842] [65.558], Avg: [-471.371 -471.371 -471.371] (0.0100) ({r_i: None, r_t: [-786.231 -786.231 -786.231], eps: 0.01})
Step:   10000, Reward: [-428.154 -428.154 -428.154] [61.946], Avg: [-478.182 -478.182 -478.182] (0.3670) ({r_i: None, r_t: [-921.391 -921.391 -921.391], eps: 0.367})
Step:   42400, Reward: [-456.439 -456.439 -456.439] [72.150], Avg: [-449.014 -449.014 -449.014] (0.0143) ({r_i: None, r_t: [-954.910 -954.910 -954.910], eps: 0.014})
Step:  102000, Reward: [-404.653 -404.653 -404.653] [97.594], Avg: [-471.305 -471.305 -471.305] (0.0100) ({r_i: None, r_t: [-819.625 -819.625 -819.625], eps: 0.01})
Step:   10100, Reward: [-422.360 -422.360 -422.360] [60.287], Avg: [-477.635 -477.635 -477.635] (0.3633) ({r_i: None, r_t: [-920.293 -920.293 -920.293], eps: 0.363})
Step:   42500, Reward: [-469.857 -469.857 -469.857] [83.903], Avg: [-449.063 -449.063 -449.063] (0.0141) ({r_i: None, r_t: [-972.813 -972.813 -972.813], eps: 0.014})
Step:  102100, Reward: [-402.027 -402.027 -402.027] [65.995], Avg: [-471.237 -471.237 -471.237] (0.0100) ({r_i: None, r_t: [-850.418 -850.418 -850.418], eps: 0.01})
Step:   10200, Reward: [-400.988 -400.988 -400.988] [66.800], Avg: [-476.891 -476.891 -476.891] (0.3597) ({r_i: None, r_t: [-913.496 -913.496 -913.496], eps: 0.36})
Step:  102200, Reward: [-422.837 -422.837 -422.837] [71.564], Avg: [-471.190 -471.190 -471.190] (0.0100) ({r_i: None, r_t: [-798.945 -798.945 -798.945], eps: 0.01})
Step:   42600, Reward: [-455.959 -455.959 -455.959] [71.878], Avg: [-449.079 -449.079 -449.079] (0.0140) ({r_i: None, r_t: [-953.865 -953.865 -953.865], eps: 0.014})
Step:  102300, Reward: [-400.025 -400.025 -400.025] [66.960], Avg: [-471.121 -471.121 -471.121] (0.0100) ({r_i: None, r_t: [-850.201 -850.201 -850.201], eps: 0.01})
Step:   10300, Reward: [-436.590 -436.590 -436.590] [75.305], Avg: [-476.503 -476.503 -476.503] (0.3561) ({r_i: None, r_t: [-929.734 -929.734 -929.734], eps: 0.356})
Step:   42700, Reward: [-456.689 -456.689 -456.689] [67.947], Avg: [-449.097 -449.097 -449.097] (0.0138) ({r_i: None, r_t: [-946.568 -946.568 -946.568], eps: 0.014})
Step:  102400, Reward: [-389.474 -389.474 -389.474] [80.187], Avg: [-471.041 -471.041 -471.041] (0.0100) ({r_i: None, r_t: [-840.744 -840.744 -840.744], eps: 0.01})
Step:   10400, Reward: [-409.350 -409.350 -409.350] [46.793], Avg: [-475.864 -475.864 -475.864] (0.3525) ({r_i: None, r_t: [-879.759 -879.759 -879.759], eps: 0.353})
Step:   42800, Reward: [-506.829 -506.829 -506.829] [62.346], Avg: [-449.231 -449.231 -449.231] (0.0137) ({r_i: None, r_t: [-954.017 -954.017 -954.017], eps: 0.014})
Step:  102500, Reward: [-428.826 -428.826 -428.826] [95.308], Avg: [-471.000 -471.000 -471.000] (0.0100) ({r_i: None, r_t: [-878.030 -878.030 -878.030], eps: 0.01})
Step:   10500, Reward: [-455.314 -455.314 -455.314] [90.450], Avg: [-475.670 -475.670 -475.670] (0.3490) ({r_i: None, r_t: [-880.232 -880.232 -880.232], eps: 0.349})
Step:  102600, Reward: [-390.793 -390.793 -390.793] [82.376], Avg: [-470.922 -470.922 -470.922] (0.0100) ({r_i: None, r_t: [-834.142 -834.142 -834.142], eps: 0.01})
Step:   42900, Reward: [-529.295 -529.295 -529.295] [52.851], Avg: [-449.417 -449.417 -449.417] (0.0136) ({r_i: None, r_t: [-911.244 -911.244 -911.244], eps: 0.014})
Step:  102700, Reward: [-409.639 -409.639 -409.639] [74.126], Avg: [-470.862 -470.862 -470.862] (0.0100) ({r_i: None, r_t: [-768.209 -768.209 -768.209], eps: 0.01})
Step:   10600, Reward: [-449.291 -449.291 -449.291] [72.537], Avg: [-475.423 -475.423 -475.423] (0.3455) ({r_i: None, r_t: [-880.872 -880.872 -880.872], eps: 0.346})
Step:   43000, Reward: [-480.936 -480.936 -480.936] [60.561], Avg: [-449.490 -449.490 -449.490] (0.0134) ({r_i: None, r_t: [-1011.499 -1011.499 -1011.499], eps: 0.013})
Step:  102800, Reward: [-386.747 -386.747 -386.747] [116.768], Avg: [-470.780 -470.780 -470.780] (0.0100) ({r_i: None, r_t: [-885.814 -885.814 -885.814], eps: 0.01})
Step:   10700, Reward: [-454.589 -454.589 -454.589] [76.595], Avg: [-475.230 -475.230 -475.230] (0.3421) ({r_i: None, r_t: [-865.038 -865.038 -865.038], eps: 0.342})
Step:   43100, Reward: [-471.450 -471.450 -471.450] [63.326], Avg: [-449.541 -449.541 -449.541] (0.0133) ({r_i: None, r_t: [-930.527 -930.527 -930.527], eps: 0.013})
Step:  102900, Reward: [-372.860 -372.860 -372.860] [66.845], Avg: [-470.685 -470.685 -470.685] (0.0100) ({r_i: None, r_t: [-772.938 -772.938 -772.938], eps: 0.01})
Step:   10800, Reward: [-463.290 -463.290 -463.290] [94.045], Avg: [-475.121 -475.121 -475.121] (0.3387) ({r_i: None, r_t: [-884.590 -884.590 -884.590], eps: 0.339})
Step:  103000, Reward: [-442.609 -442.609 -442.609] [89.220], Avg: [-470.658 -470.658 -470.658] (0.0100) ({r_i: None, r_t: [-845.262 -845.262 -845.262], eps: 0.01})
Step:   43200, Reward: [-462.138 -462.138 -462.138] [58.934], Avg: [-449.570 -449.570 -449.570] (0.0132) ({r_i: None, r_t: [-915.219 -915.219 -915.219], eps: 0.013})
Step:  103100, Reward: [-412.222 -412.222 -412.222] [77.303], Avg: [-470.601 -470.601 -470.601] (0.0100) ({r_i: None, r_t: [-789.089 -789.089 -789.089], eps: 0.01})
Step:   10900, Reward: [-443.453 -443.453 -443.453] [59.468], Avg: [-474.833 -474.833 -474.833] (0.3353) ({r_i: None, r_t: [-865.525 -865.525 -865.525], eps: 0.335})
Step:   43300, Reward: [-458.462 -458.462 -458.462] [57.104], Avg: [-449.591 -449.591 -449.591] (0.0130) ({r_i: None, r_t: [-958.050 -958.050 -958.050], eps: 0.013})
Step:  103200, Reward: [-400.105 -400.105 -400.105] [85.333], Avg: [-470.533 -470.533 -470.533] (0.0100) ({r_i: None, r_t: [-813.849 -813.849 -813.849], eps: 0.01})
Step:   11000, Reward: [-419.586 -419.586 -419.586] [54.598], Avg: [-474.335 -474.335 -474.335] (0.3320) ({r_i: None, r_t: [-916.542 -916.542 -916.542], eps: 0.332})
Step:   43400, Reward: [-463.740 -463.740 -463.740] [56.210], Avg: [-449.623 -449.623 -449.623] (0.0129) ({r_i: None, r_t: [-966.035 -966.035 -966.035], eps: 0.013})
Step:  103300, Reward: [-420.085 -420.085 -420.085] [60.957], Avg: [-470.484 -470.484 -470.484] (0.0100) ({r_i: None, r_t: [-865.649 -865.649 -865.649], eps: 0.01})
Step:   11100, Reward: [-443.398 -443.398 -443.398] [49.157], Avg: [-474.059 -474.059 -474.059] (0.3286) ({r_i: None, r_t: [-886.846 -886.846 -886.846], eps: 0.329})
Step:  103400, Reward: [-373.173 -373.173 -373.173] [103.341], Avg: [-470.390 -470.390 -470.390] (0.0100) ({r_i: None, r_t: [-823.543 -823.543 -823.543], eps: 0.01})
Step:   43500, Reward: [-468.590 -468.590 -468.590] [79.483], Avg: [-449.667 -449.667 -449.667] (0.0128) ({r_i: None, r_t: [-959.151 -959.151 -959.151], eps: 0.013})
Step:  103500, Reward: [-411.395 -411.395 -411.395] [97.647], Avg: [-470.333 -470.333 -470.333] (0.0100) ({r_i: None, r_t: [-816.549 -816.549 -816.549], eps: 0.01})
Step:   11200, Reward: [-419.102 -419.102 -419.102] [76.120], Avg: [-473.573 -473.573 -473.573] (0.3254) ({r_i: None, r_t: [-840.261 -840.261 -840.261], eps: 0.325})
Step:   43600, Reward: [-472.548 -472.548 -472.548] [70.540], Avg: [-449.719 -449.719 -449.719] (0.0126) ({r_i: None, r_t: [-961.883 -961.883 -961.883], eps: 0.013})
Step:  103600, Reward: [-425.866 -425.866 -425.866] [65.420], Avg: [-470.291 -470.291 -470.291] (0.0100) ({r_i: None, r_t: [-766.767 -766.767 -766.767], eps: 0.01})
Step:   11300, Reward: [-443.551 -443.551 -443.551] [84.171], Avg: [-473.309 -473.309 -473.309] (0.3221) ({r_i: None, r_t: [-937.828 -937.828 -937.828], eps: 0.322})
Step:   43700, Reward: [-468.322 -468.322 -468.322] [63.034], Avg: [-449.762 -449.762 -449.762] (0.0125) ({r_i: None, r_t: [-941.759 -941.759 -941.759], eps: 0.013})
Step:  103700, Reward: [-394.613 -394.613 -394.613] [70.437], Avg: [-470.218 -470.218 -470.218] (0.0100) ({r_i: None, r_t: [-746.807 -746.807 -746.807], eps: 0.01})
Step:   11400, Reward: [-418.174 -418.174 -418.174] [46.409], Avg: [-472.830 -472.830 -472.830] (0.3189) ({r_i: None, r_t: [-922.191 -922.191 -922.191], eps: 0.319})
Step:  103800, Reward: [-458.473 -458.473 -458.473] [95.738], Avg: [-470.206 -470.206 -470.206] (0.0100) ({r_i: None, r_t: [-765.618 -765.618 -765.618], eps: 0.01})
Step:   43800, Reward: [-488.520 -488.520 -488.520] [52.158], Avg: [-449.850 -449.850 -449.850] (0.0124) ({r_i: None, r_t: [-971.279 -971.279 -971.279], eps: 0.012})
Step:  103900, Reward: [-413.203 -413.203 -413.203] [93.803], Avg: [-470.152 -470.152 -470.152] (0.0100) ({r_i: None, r_t: [-810.949 -810.949 -810.949], eps: 0.01})
Step:   11500, Reward: [-429.941 -429.941 -429.941] [81.168], Avg: [-472.460 -472.460 -472.460] (0.3157) ({r_i: None, r_t: [-887.418 -887.418 -887.418], eps: 0.316})
Step:   43900, Reward: [-478.155 -478.155 -478.155] [57.097], Avg: [-449.914 -449.914 -449.914] (0.0123) ({r_i: None, r_t: [-939.351 -939.351 -939.351], eps: 0.012})
Step:  104000, Reward: [-379.184 -379.184 -379.184] [89.090], Avg: [-470.064 -470.064 -470.064] (0.0100) ({r_i: None, r_t: [-815.163 -815.163 -815.163], eps: 0.01})
Step:   11600, Reward: [-438.590 -438.590 -438.590] [100.059], Avg: [-472.171 -472.171 -472.171] (0.3126) ({r_i: None, r_t: [-871.296 -871.296 -871.296], eps: 0.313})
Step:   44000, Reward: [-483.896 -483.896 -483.896] [74.996], Avg: [-449.991 -449.991 -449.991] (0.0121) ({r_i: None, r_t: [-956.719 -956.719 -956.719], eps: 0.012})
Step:  104100, Reward: [-432.924 -432.924 -432.924] [56.337], Avg: [-470.029 -470.029 -470.029] (0.0100) ({r_i: None, r_t: [-818.542 -818.542 -818.542], eps: 0.01})
Step:   11700, Reward: [-399.393 -399.393 -399.393] [61.892], Avg: [-471.554 -471.554 -471.554] (0.3095) ({r_i: None, r_t: [-845.135 -845.135 -845.135], eps: 0.309})
Step:  104200, Reward: [-423.762 -423.762 -423.762] [81.648], Avg: [-469.984 -469.984 -469.984] (0.0100) ({r_i: None, r_t: [-798.980 -798.980 -798.980], eps: 0.01})
Step:   44100, Reward: [-512.634 -512.634 -512.634] [73.036], Avg: [-450.133 -450.133 -450.133] (0.0120) ({r_i: None, r_t: [-1006.662 -1006.662 -1006.662], eps: 0.012})
Step:  104300, Reward: [-436.041 -436.041 -436.041] [98.785], Avg: [-469.952 -469.952 -469.952] (0.0100) ({r_i: None, r_t: [-864.070 -864.070 -864.070], eps: 0.01})
Step:   11800, Reward: [-428.650 -428.650 -428.650] [85.269], Avg: [-471.193 -471.193 -471.193] (0.3064) ({r_i: None, r_t: [-916.303 -916.303 -916.303], eps: 0.306})
Step:   44200, Reward: [-470.270 -470.270 -470.270] [52.852], Avg: [-450.179 -450.179 -450.179] (0.0119) ({r_i: None, r_t: [-957.239 -957.239 -957.239], eps: 0.012})
Step:  104400, Reward: [-394.225 -394.225 -394.225] [64.920], Avg: [-469.879 -469.879 -469.879] (0.0100) ({r_i: None, r_t: [-798.462 -798.462 -798.462], eps: 0.01})
Step:   11900, Reward: [-440.007 -440.007 -440.007] [67.071], Avg: [-470.933 -470.933 -470.933] (0.3033) ({r_i: None, r_t: [-832.204 -832.204 -832.204], eps: 0.303})
Step:   44300, Reward: [-505.175 -505.175 -505.175] [66.273], Avg: [-450.302 -450.302 -450.302] (0.0118) ({r_i: None, r_t: [-1017.436 -1017.436 -1017.436], eps: 0.012})
Step:  104500, Reward: [-440.560 -440.560 -440.560] [120.638], Avg: [-469.851 -469.851 -469.851] (0.0100) ({r_i: None, r_t: [-849.260 -849.260 -849.260], eps: 0.01})
Step:  104600, Reward: [-411.636 -411.636 -411.636] [88.702], Avg: [-469.796 -469.796 -469.796] (0.0100) ({r_i: None, r_t: [-886.494 -886.494 -886.494], eps: 0.01})
Step:   12000, Reward: [-414.730 -414.730 -414.730] [73.630], Avg: [-470.469 -470.469 -470.469] (0.3003) ({r_i: None, r_t: [-906.656 -906.656 -906.656], eps: 0.3})
Step:   44400, Reward: [-499.489 -499.489 -499.489] [69.493], Avg: [-450.413 -450.413 -450.413] (0.0117) ({r_i: None, r_t: [-976.704 -976.704 -976.704], eps: 0.012})
Step:  104700, Reward: [-386.127 -386.127 -386.127] [89.755], Avg: [-469.716 -469.716 -469.716] (0.0100) ({r_i: None, r_t: [-826.102 -826.102 -826.102], eps: 0.01})
Step:   12100, Reward: [-427.501 -427.501 -427.501] [82.170], Avg: [-470.117 -470.117 -470.117] (0.2973) ({r_i: None, r_t: [-882.172 -882.172 -882.172], eps: 0.297})
Step:   44500, Reward: [-467.803 -467.803 -467.803] [64.674], Avg: [-450.452 -450.452 -450.452] (0.0115) ({r_i: None, r_t: [-995.026 -995.026 -995.026], eps: 0.012})
Step:  104800, Reward: [-454.463 -454.463 -454.463] [126.851], Avg: [-469.701 -469.701 -469.701] (0.0100) ({r_i: None, r_t: [-835.357 -835.357 -835.357], eps: 0.01})
Step:   12200, Reward: [-443.310 -443.310 -443.310] [96.918], Avg: [-469.899 -469.899 -469.899] (0.2943) ({r_i: None, r_t: [-849.150 -849.150 -849.150], eps: 0.294})
Step:   44600, Reward: [-474.502 -474.502 -474.502] [51.368], Avg: [-450.506 -450.506 -450.506] (0.0114) ({r_i: None, r_t: [-998.826 -998.826 -998.826], eps: 0.011})
Step:  104900, Reward: [-420.850 -420.850 -420.850] [63.480], Avg: [-469.655 -469.655 -469.655] (0.0100) ({r_i: None, r_t: [-777.653 -777.653 -777.653], eps: 0.01})
Step:  105000, Reward: [-423.363 -423.363 -423.363] [85.993], Avg: [-469.611 -469.611 -469.611] (0.0100) ({r_i: None, r_t: [-825.402 -825.402 -825.402], eps: 0.01})
Step:   12300, Reward: [-406.871 -406.871 -406.871] [46.239], Avg: [-469.390 -469.390 -469.390] (0.2914) ({r_i: None, r_t: [-884.533 -884.533 -884.533], eps: 0.291})
Step:   44700, Reward: [-496.392 -496.392 -496.392] [72.870], Avg: [-450.608 -450.608 -450.608] (0.0113) ({r_i: None, r_t: [-965.440 -965.440 -965.440], eps: 0.011})
Step:  105100, Reward: [-403.297 -403.297 -403.297] [79.773], Avg: [-469.548 -469.548 -469.548] (0.0100) ({r_i: None, r_t: [-844.060 -844.060 -844.060], eps: 0.01})
Step:   12400, Reward: [-385.486 -385.486 -385.486] [64.282], Avg: [-468.719 -468.719 -468.719] (0.2885) ({r_i: None, r_t: [-828.507 -828.507 -828.507], eps: 0.288})
Step:   44800, Reward: [-475.097 -475.097 -475.097] [64.131], Avg: [-450.663 -450.663 -450.663] (0.0112) ({r_i: None, r_t: [-957.637 -957.637 -957.637], eps: 0.011})
Step:  105200, Reward: [-400.642 -400.642 -400.642] [80.984], Avg: [-469.482 -469.482 -469.482] (0.0100) ({r_i: None, r_t: [-805.348 -805.348 -805.348], eps: 0.01})
Step:   12500, Reward: [-442.188 -442.188 -442.188] [45.184], Avg: [-468.509 -468.509 -468.509] (0.2856) ({r_i: None, r_t: [-849.184 -849.184 -849.184], eps: 0.286})
Step:   44900, Reward: [-476.082 -476.082 -476.082] [80.147], Avg: [-450.719 -450.719 -450.719] (0.0111) ({r_i: None, r_t: [-1002.619 -1002.619 -1002.619], eps: 0.011})
Step:  105300, Reward: [-376.922 -376.922 -376.922] [81.691], Avg: [-469.394 -469.394 -469.394] (0.0100) ({r_i: None, r_t: [-805.044 -805.044 -805.044], eps: 0.01})
Step:  105400, Reward: [-419.739 -419.739 -419.739] [71.388], Avg: [-469.347 -469.347 -469.347] (0.0100) ({r_i: None, r_t: [-823.699 -823.699 -823.699], eps: 0.01})
Step:   12600, Reward: [-421.031 -421.031 -421.031] [79.934], Avg: [-468.135 -468.135 -468.135] (0.2828) ({r_i: None, r_t: [-897.954 -897.954 -897.954], eps: 0.283})
Step:   45000, Reward: [-465.210 -465.210 -465.210] [66.025], Avg: [-450.751 -450.751 -450.751] (0.0110) ({r_i: None, r_t: [-963.934 -963.934 -963.934], eps: 0.011})
Step:  105500, Reward: [-393.768 -393.768 -393.768] [62.147], Avg: [-469.276 -469.276 -469.276] (0.0100) ({r_i: None, r_t: [-797.958 -797.958 -797.958], eps: 0.01})
Step:   12700, Reward: [-465.384 -465.384 -465.384] [112.940], Avg: [-468.113 -468.113 -468.113] (0.2799) ({r_i: None, r_t: [-904.962 -904.962 -904.962], eps: 0.28})
Step:   45100, Reward: [-524.984 -524.984 -524.984] [70.241], Avg: [-450.916 -450.916 -450.916] (0.0109) ({r_i: None, r_t: [-981.772 -981.772 -981.772], eps: 0.011})
Step:  105600, Reward: [-390.076 -390.076 -390.076] [68.756], Avg: [-469.201 -469.201 -469.201] (0.0100) ({r_i: None, r_t: [-828.423 -828.423 -828.423], eps: 0.01})
Step:   12800, Reward: [-427.997 -427.997 -427.997] [55.143], Avg: [-467.802 -467.802 -467.802] (0.2771) ({r_i: None, r_t: [-833.765 -833.765 -833.765], eps: 0.277})
Step:  105700, Reward: [-399.597 -399.597 -399.597] [62.559], Avg: [-469.135 -469.135 -469.135] (0.0100) ({r_i: None, r_t: [-824.950 -824.950 -824.950], eps: 0.01})
Step:   45200, Reward: [-503.360 -503.360 -503.360] [55.799], Avg: [-451.031 -451.031 -451.031] (0.0108) ({r_i: None, r_t: [-975.357 -975.357 -975.357], eps: 0.011})
Step:  105800, Reward: [-401.317 -401.317 -401.317] [94.915], Avg: [-469.071 -469.071 -469.071] (0.0100) ({r_i: None, r_t: [-858.890 -858.890 -858.890], eps: 0.01})
Step:   12900, Reward: [-405.264 -405.264 -405.264] [49.419], Avg: [-467.321 -467.321 -467.321] (0.2744) ({r_i: None, r_t: [-818.831 -818.831 -818.831], eps: 0.274})
Step:   45300, Reward: [-466.552 -466.552 -466.552] [82.538], Avg: [-451.066 -451.066 -451.066] (0.0107) ({r_i: None, r_t: [-932.593 -932.593 -932.593], eps: 0.011})
Step:  105900, Reward: [-435.278 -435.278 -435.278] [79.783], Avg: [-469.039 -469.039 -469.039] (0.0100) ({r_i: None, r_t: [-859.944 -859.944 -859.944], eps: 0.01})
Step:   13000, Reward: [-400.702 -400.702 -400.702] [49.671], Avg: [-466.813 -466.813 -466.813] (0.2716) ({r_i: None, r_t: [-841.699 -841.699 -841.699], eps: 0.272})
Step:   45400, Reward: [-489.142 -489.142 -489.142] [87.165], Avg: [-451.149 -451.149 -451.149] (0.0106) ({r_i: None, r_t: [-958.020 -958.020 -958.020], eps: 0.011})
Step:  106000, Reward: [-392.481 -392.481 -392.481] [63.410], Avg: [-468.967 -468.967 -468.967] (0.0100) ({r_i: None, r_t: [-781.326 -781.326 -781.326], eps: 0.01})
Step:   13100, Reward: [-420.792 -420.792 -420.792] [79.110], Avg: [-466.464 -466.464 -466.464] (0.2689) ({r_i: None, r_t: [-856.734 -856.734 -856.734], eps: 0.269})
Step:   45500, Reward: [-454.381 -454.381 -454.381] [61.169], Avg: [-451.156 -451.156 -451.156] (0.0104) ({r_i: None, r_t: [-929.105 -929.105 -929.105], eps: 0.01})
Step:  106100, Reward: [-456.348 -456.348 -456.348] [96.752], Avg: [-468.955 -468.955 -468.955] (0.0100) ({r_i: None, r_t: [-822.544 -822.544 -822.544], eps: 0.01})
Step:  106200, Reward: [-477.595 -477.595 -477.595] [98.460], Avg: [-468.963 -468.963 -468.963] (0.0100) ({r_i: None, r_t: [-806.958 -806.958 -806.958], eps: 0.01})
Step:   13200, Reward: [-421.803 -421.803 -421.803] [68.831], Avg: [-466.128 -466.128 -466.128] (0.2663) ({r_i: None, r_t: [-836.045 -836.045 -836.045], eps: 0.266})
Step:   45600, Reward: [-462.649 -462.649 -462.649] [66.314], Avg: [-451.181 -451.181 -451.181] (0.0103) ({r_i: None, r_t: [-946.532 -946.532 -946.532], eps: 0.01})
Step:  106300, Reward: [-407.101 -407.101 -407.101] [82.415], Avg: [-468.905 -468.905 -468.905] (0.0100) ({r_i: None, r_t: [-826.494 -826.494 -826.494], eps: 0.01})
Step:   13300, Reward: [-427.758 -427.758 -427.758] [49.248], Avg: [-465.842 -465.842 -465.842] (0.2636) ({r_i: None, r_t: [-816.811 -816.811 -816.811], eps: 0.264})
Step:   45700, Reward: [-465.782 -465.782 -465.782] [70.592], Avg: [-451.213 -451.213 -451.213] (0.0102) ({r_i: None, r_t: [-984.850 -984.850 -984.850], eps: 0.01})
Step:  106400, Reward: [-400.467 -400.467 -400.467] [78.263], Avg: [-468.841 -468.841 -468.841] (0.0100) ({r_i: None, r_t: [-846.387 -846.387 -846.387], eps: 0.01})
Step:   13400, Reward: [-424.478 -424.478 -424.478] [50.816], Avg: [-465.536 -465.536 -465.536] (0.2610) ({r_i: None, r_t: [-853.691 -853.691 -853.691], eps: 0.261})
Step:   45800, Reward: [-492.678 -492.678 -492.678] [59.104], Avg: [-451.304 -451.304 -451.304] (0.0101) ({r_i: None, r_t: [-962.337 -962.337 -962.337], eps: 0.01})
Step:  106500, Reward: [-382.114 -382.114 -382.114] [67.069], Avg: [-468.759 -468.759 -468.759] (0.0100) ({r_i: None, r_t: [-787.683 -787.683 -787.683], eps: 0.01})
Step:  106600, Reward: [-388.986 -388.986 -388.986] [64.578], Avg: [-468.685 -468.685 -468.685] (0.0100) ({r_i: None, r_t: [-785.347 -785.347 -785.347], eps: 0.01})
Step:   13500, Reward: [-420.605 -420.605 -420.605] [72.044], Avg: [-465.205 -465.205 -465.205] (0.2584) ({r_i: None, r_t: [-839.433 -839.433 -839.433], eps: 0.258})
Step:   45900, Reward: [-493.052 -493.052 -493.052] [88.906], Avg: [-451.394 -451.394 -451.394] (0.0100) ({r_i: None, r_t: [-965.656 -965.656 -965.656], eps: 0.01})
Step:  106700, Reward: [-418.425 -418.425 -418.425] [89.940], Avg: [-468.638 -468.638 -468.638] (0.0100) ({r_i: None, r_t: [-835.911 -835.911 -835.911], eps: 0.01})
Step:   13600, Reward: [-432.994 -432.994 -432.994] [89.176], Avg: [-464.970 -464.970 -464.970] (0.2558) ({r_i: None, r_t: [-852.810 -852.810 -852.810], eps: 0.256})
Step:   46000, Reward: [-463.909 -463.909 -463.909] [70.198], Avg: [-451.422 -451.422 -451.422] (0.0100) ({r_i: None, r_t: [-972.136 -972.136 -972.136], eps: 0.01})
Step:  106800, Reward: [-413.698 -413.698 -413.698] [50.640], Avg: [-468.586 -468.586 -468.586] (0.0100) ({r_i: None, r_t: [-856.742 -856.742 -856.742], eps: 0.01})
Step:   13700, Reward: [-429.782 -429.782 -429.782] [76.003], Avg: [-464.715 -464.715 -464.715] (0.2532) ({r_i: None, r_t: [-845.932 -845.932 -845.932], eps: 0.253})
Step:  106900, Reward: [-434.136 -434.136 -434.136] [74.991], Avg: [-468.554 -468.554 -468.554] (0.0100) ({r_i: None, r_t: [-831.924 -831.924 -831.924], eps: 0.01})
Step:   46100, Reward: [-488.376 -488.376 -488.376] [54.822], Avg: [-451.502 -451.502 -451.502] (0.0100) ({r_i: None, r_t: [-988.131 -988.131 -988.131], eps: 0.01})
Step:  107000, Reward: [-429.989 -429.989 -429.989] [127.766], Avg: [-468.518 -468.518 -468.518] (0.0100) ({r_i: None, r_t: [-815.408 -815.408 -815.408], eps: 0.01})
Step:   13800, Reward: [-429.748 -429.748 -429.748] [67.780], Avg: [-464.464 -464.464 -464.464] (0.2507) ({r_i: None, r_t: [-822.814 -822.814 -822.814], eps: 0.251})
Step:   46200, Reward: [-507.300 -507.300 -507.300] [80.970], Avg: [-451.622 -451.622 -451.622] (0.0100) ({r_i: None, r_t: [-961.811 -961.811 -961.811], eps: 0.01})
Step:  107100, Reward: [-387.377 -387.377 -387.377] [62.237], Avg: [-468.442 -468.442 -468.442] (0.0100) ({r_i: None, r_t: [-865.650 -865.650 -865.650], eps: 0.01})
Step:   13900, Reward: [-443.423 -443.423 -443.423] [89.927], Avg: [-464.313 -464.313 -464.313] (0.2482) ({r_i: None, r_t: [-804.464 -804.464 -804.464], eps: 0.248})
Step:   46300, Reward: [-476.812 -476.812 -476.812] [40.144], Avg: [-451.676 -451.676 -451.676] (0.0100) ({r_i: None, r_t: [-975.253 -975.253 -975.253], eps: 0.01})
Step:  107200, Reward: [-414.693 -414.693 -414.693] [84.552], Avg: [-468.392 -468.392 -468.392] (0.0100) ({r_i: None, r_t: [-821.831 -821.831 -821.831], eps: 0.01})
Step:   14000, Reward: [-396.020 -396.020 -396.020] [59.387], Avg: [-463.829 -463.829 -463.829] (0.2457) ({r_i: None, r_t: [-867.335 -867.335 -867.335], eps: 0.246})
Step:   46400, Reward: [-488.979 -488.979 -488.979] [66.557], Avg: [-451.757 -451.757 -451.757] (0.0100) ({r_i: None, r_t: [-913.036 -913.036 -913.036], eps: 0.01})
Step:  107300, Reward: [-419.093 -419.093 -419.093] [70.089], Avg: [-468.346 -468.346 -468.346] (0.0100) ({r_i: None, r_t: [-865.098 -865.098 -865.098], eps: 0.01})
Step:  107400, Reward: [-419.834 -419.834 -419.834] [92.427], Avg: [-468.301 -468.301 -468.301] (0.0100) ({r_i: None, r_t: [-818.823 -818.823 -818.823], eps: 0.01})
Step:   14100, Reward: [-435.601 -435.601 -435.601] [61.850], Avg: [-463.630 -463.630 -463.630] (0.2433) ({r_i: None, r_t: [-827.728 -827.728 -827.728], eps: 0.243})
Step:   46500, Reward: [-516.839 -516.839 -516.839] [62.394], Avg: [-451.896 -451.896 -451.896] (0.0100) ({r_i: None, r_t: [-970.581 -970.581 -970.581], eps: 0.01})
Step:  107500, Reward: [-461.127 -461.127 -461.127] [76.959], Avg: [-468.295 -468.295 -468.295] (0.0100) ({r_i: None, r_t: [-824.450 -824.450 -824.450], eps: 0.01})
Step:   14200, Reward: [-465.080 -465.080 -465.080] [93.876], Avg: [-463.640 -463.640 -463.640] (0.2409) ({r_i: None, r_t: [-834.561 -834.561 -834.561], eps: 0.241})
Step:   46600, Reward: [-483.572 -483.572 -483.572] [91.101], Avg: [-451.964 -451.964 -451.964] (0.0100) ({r_i: None, r_t: [-982.461 -982.461 -982.461], eps: 0.01})
Step:  107600, Reward: [-434.145 -434.145 -434.145] [99.975], Avg: [-468.263 -468.263 -468.263] (0.0100) ({r_i: None, r_t: [-794.625 -794.625 -794.625], eps: 0.01})
Step:   14300, Reward: [-388.829 -388.829 -388.829] [38.130], Avg: [-463.121 -463.121 -463.121] (0.2385) ({r_i: None, r_t: [-852.734 -852.734 -852.734], eps: 0.238})
Step:  107700, Reward: [-397.376 -397.376 -397.376] [39.873], Avg: [-468.197 -468.197 -468.197] (0.0100) ({r_i: None, r_t: [-826.333 -826.333 -826.333], eps: 0.01})
Step:   46700, Reward: [-523.939 -523.939 -523.939] [70.162], Avg: [-452.118 -452.118 -452.118] (0.0100) ({r_i: None, r_t: [-946.956 -946.956 -946.956], eps: 0.01})
Step:  107800, Reward: [-418.305 -418.305 -418.305] [76.243], Avg: [-468.151 -468.151 -468.151] (0.0100) ({r_i: None, r_t: [-806.838 -806.838 -806.838], eps: 0.01})
Step:   14400, Reward: [-400.660 -400.660 -400.660] [74.825], Avg: [-462.690 -462.690 -462.690] (0.2361) ({r_i: None, r_t: [-836.596 -836.596 -836.596], eps: 0.236})
Step:   46800, Reward: [-507.041 -507.041 -507.041] [65.504], Avg: [-452.235 -452.235 -452.235] (0.0100) ({r_i: None, r_t: [-963.908 -963.908 -963.908], eps: 0.01})
Step:  107900, Reward: [-403.397 -403.397 -403.397] [74.510], Avg: [-468.091 -468.091 -468.091] (0.0100) ({r_i: None, r_t: [-830.750 -830.750 -830.750], eps: 0.01})
Step:   14500, Reward: [-403.050 -403.050 -403.050] [59.917], Avg: [-462.281 -462.281 -462.281] (0.2337) ({r_i: None, r_t: [-840.048 -840.048 -840.048], eps: 0.234})
Step:   46900, Reward: [-472.452 -472.452 -472.452] [54.519], Avg: [-452.278 -452.278 -452.278] (0.0100) ({r_i: None, r_t: [-968.364 -968.364 -968.364], eps: 0.01})
Step:  108000, Reward: [-396.527 -396.527 -396.527] [58.433], Avg: [-468.025 -468.025 -468.025] (0.0100) ({r_i: None, r_t: [-822.576 -822.576 -822.576], eps: 0.01})
Step:   14600, Reward: [-432.386 -432.386 -432.386] [92.224], Avg: [-462.078 -462.078 -462.078] (0.2314) ({r_i: None, r_t: [-852.264 -852.264 -852.264], eps: 0.231})
Step:  108100, Reward: [-385.426 -385.426 -385.426] [78.937], Avg: [-467.948 -467.948 -467.948] (0.0100) ({r_i: None, r_t: [-845.699 -845.699 -845.699], eps: 0.01})
Step:   47000, Reward: [-512.460 -512.460 -512.460] [87.884], Avg: [-452.406 -452.406 -452.406] (0.0100) ({r_i: None, r_t: [-968.326 -968.326 -968.326], eps: 0.01})
Step:  108200, Reward: [-396.269 -396.269 -396.269] [70.820], Avg: [-467.882 -467.882 -467.882] (0.0100) ({r_i: None, r_t: [-844.549 -844.549 -844.549], eps: 0.01})
Step:   14700, Reward: [-401.442 -401.442 -401.442] [48.591], Avg: [-461.668 -461.668 -461.668] (0.2291) ({r_i: None, r_t: [-814.489 -814.489 -814.489], eps: 0.229})
Step:   47100, Reward: [-455.299 -455.299 -455.299] [44.746], Avg: [-452.412 -452.412 -452.412] (0.0100) ({r_i: None, r_t: [-990.027 -990.027 -990.027], eps: 0.01})
Step:  108300, Reward: [-434.747 -434.747 -434.747] [76.346], Avg: [-467.852 -467.852 -467.852] (0.0100) ({r_i: None, r_t: [-836.356 -836.356 -836.356], eps: 0.01})
Step:   14800, Reward: [-441.058 -441.058 -441.058] [73.174], Avg: [-461.530 -461.530 -461.530] (0.2268) ({r_i: None, r_t: [-853.471 -853.471 -853.471], eps: 0.227})
Step:   47200, Reward: [-468.418 -468.418 -468.418] [57.170], Avg: [-452.446 -452.446 -452.446] (0.0100) ({r_i: None, r_t: [-950.300 -950.300 -950.300], eps: 0.01})
Step:  108400, Reward: [-416.854 -416.854 -416.854] [86.350], Avg: [-467.805 -467.805 -467.805] (0.0100) ({r_i: None, r_t: [-867.260 -867.260 -867.260], eps: 0.01})
Step:   14900, Reward: [-417.874 -417.874 -417.874] [57.996], Avg: [-461.239 -461.239 -461.239] (0.2245) ({r_i: None, r_t: [-845.837 -845.837 -845.837], eps: 0.225})
Step:  108500, Reward: [-417.646 -417.646 -417.646] [126.219], Avg: [-467.758 -467.758 -467.758] (0.0100) ({r_i: None, r_t: [-791.817 -791.817 -791.817], eps: 0.01})
Step:   47300, Reward: [-482.927 -482.927 -482.927] [51.841], Avg: [-452.510 -452.510 -452.510] (0.0100) ({r_i: None, r_t: [-952.652 -952.652 -952.652], eps: 0.01})
Step:  108600, Reward: [-409.350 -409.350 -409.350] [91.790], Avg: [-467.705 -467.705 -467.705] (0.0100) ({r_i: None, r_t: [-817.992 -817.992 -817.992], eps: 0.01})
Step:   15000, Reward: [-418.979 -418.979 -418.979] [51.418], Avg: [-460.959 -460.959 -460.959] (0.2223) ({r_i: None, r_t: [-819.662 -819.662 -819.662], eps: 0.222})
Step:   47400, Reward: [-499.468 -499.468 -499.468] [69.555], Avg: [-452.609 -452.609 -452.609] (0.0100) ({r_i: None, r_t: [-994.863 -994.863 -994.863], eps: 0.01})
Step:  108700, Reward: [-412.641 -412.641 -412.641] [65.063], Avg: [-467.654 -467.654 -467.654] (0.0100) ({r_i: None, r_t: [-817.250 -817.250 -817.250], eps: 0.01})
Step:   15100, Reward: [-410.655 -410.655 -410.655] [55.072], Avg: [-460.628 -460.628 -460.628] (0.2201) ({r_i: None, r_t: [-914.681 -914.681 -914.681], eps: 0.22})
Step:   47500, Reward: [-519.877 -519.877 -519.877] [63.739], Avg: [-452.750 -452.750 -452.750] (0.0100) ({r_i: None, r_t: [-953.814 -953.814 -953.814], eps: 0.01})
Step:  108800, Reward: [-383.053 -383.053 -383.053] [60.933], Avg: [-467.576 -467.576 -467.576] (0.0100) ({r_i: None, r_t: [-823.953 -823.953 -823.953], eps: 0.01})
Step:  108900, Reward: [-391.436 -391.436 -391.436] [89.314], Avg: [-467.506 -467.506 -467.506] (0.0100) ({r_i: None, r_t: [-794.686 -794.686 -794.686], eps: 0.01})
Step:   15200, Reward: [-411.129 -411.129 -411.129] [67.920], Avg: [-460.305 -460.305 -460.305] (0.2179) ({r_i: None, r_t: [-834.775 -834.775 -834.775], eps: 0.218})
Step:   47600, Reward: [-501.413 -501.413 -501.413] [69.770], Avg: [-452.852 -452.852 -452.852] (0.0100) ({r_i: None, r_t: [-1014.681 -1014.681 -1014.681], eps: 0.01})
Step:  109000, Reward: [-418.319 -418.319 -418.319] [79.442], Avg: [-467.461 -467.461 -467.461] (0.0100) ({r_i: None, r_t: [-816.373 -816.373 -816.373], eps: 0.01})
Step:   15300, Reward: [-412.434 -412.434 -412.434] [73.086], Avg: [-459.994 -459.994 -459.994] (0.2157) ({r_i: None, r_t: [-812.569 -812.569 -812.569], eps: 0.216})
Step:   47700, Reward: [-476.009 -476.009 -476.009] [93.752], Avg: [-452.901 -452.901 -452.901] (0.0100) ({r_i: None, r_t: [-938.126 -938.126 -938.126], eps: 0.01})
Step:  109100, Reward: [-422.970 -422.970 -422.970] [97.893], Avg: [-467.421 -467.421 -467.421] (0.0100) ({r_i: None, r_t: [-831.411 -831.411 -831.411], eps: 0.01})
Step:   15400, Reward: [-434.852 -434.852 -434.852] [59.351], Avg: [-459.832 -459.832 -459.832] (0.2136) ({r_i: None, r_t: [-882.021 -882.021 -882.021], eps: 0.214})
Step:   47800, Reward: [-475.002 -475.002 -475.002] [69.977], Avg: [-452.947 -452.947 -452.947] (0.0100) ({r_i: None, r_t: [-1000.520 -1000.520 -1000.520], eps: 0.01})
Step:  109200, Reward: [-378.426 -378.426 -378.426] [75.920], Avg: [-467.339 -467.339 -467.339] (0.0100) ({r_i: None, r_t: [-843.738 -843.738 -843.738], eps: 0.01})
Step:   15500, Reward: [-414.974 -414.974 -414.974] [75.243], Avg: [-459.544 -459.544 -459.544] (0.2114) ({r_i: None, r_t: [-808.790 -808.790 -808.790], eps: 0.211})
Step:  109300, Reward: [-411.740 -411.740 -411.740] [55.637], Avg: [-467.288 -467.288 -467.288] (0.0100) ({r_i: None, r_t: [-815.401 -815.401 -815.401], eps: 0.01})
Step:   47900, Reward: [-471.931 -471.931 -471.931] [52.969], Avg: [-452.986 -452.986 -452.986] (0.0100) ({r_i: None, r_t: [-943.470 -943.470 -943.470], eps: 0.01})
Step:  109400, Reward: [-421.006 -421.006 -421.006] [101.380], Avg: [-467.246 -467.246 -467.246] (0.0100) ({r_i: None, r_t: [-814.218 -814.218 -814.218], eps: 0.01})
Step:   15600, Reward: [-380.881 -380.881 -380.881] [51.675], Avg: [-459.043 -459.043 -459.043] (0.2093) ({r_i: None, r_t: [-838.200 -838.200 -838.200], eps: 0.209})
Step:   48000, Reward: [-488.189 -488.189 -488.189] [64.899], Avg: [-453.060 -453.060 -453.060] (0.0100) ({r_i: None, r_t: [-1022.218 -1022.218 -1022.218], eps: 0.01})
Step:  109500, Reward: [-407.988 -407.988 -407.988] [71.577], Avg: [-467.192 -467.192 -467.192] (0.0100) ({r_i: None, r_t: [-800.064 -800.064 -800.064], eps: 0.01})
Step:   15700, Reward: [-442.060 -442.060 -442.060] [88.677], Avg: [-458.936 -458.936 -458.936] (0.2072) ({r_i: None, r_t: [-831.459 -831.459 -831.459], eps: 0.207})
Step:   48100, Reward: [-490.812 -490.812 -490.812] [73.109], Avg: [-453.138 -453.138 -453.138] (0.0100) ({r_i: None, r_t: [-959.692 -959.692 -959.692], eps: 0.01})
Step:  109600, Reward: [-370.788 -370.788 -370.788] [78.405], Avg: [-467.104 -467.104 -467.104] (0.0100) ({r_i: None, r_t: [-771.277 -771.277 -771.277], eps: 0.01})
Step:   15800, Reward: [-431.861 -431.861 -431.861] [41.670], Avg: [-458.765 -458.765 -458.765] (0.2052) ({r_i: None, r_t: [-853.914 -853.914 -853.914], eps: 0.205})
Step:  109700, Reward: [-414.131 -414.131 -414.131] [81.143], Avg: [-467.056 -467.056 -467.056] (0.0100) ({r_i: None, r_t: [-832.371 -832.371 -832.371], eps: 0.01})
Step:   48200, Reward: [-498.711 -498.711 -498.711] [68.103], Avg: [-453.232 -453.232 -453.232] (0.0100) ({r_i: None, r_t: [-998.689 -998.689 -998.689], eps: 0.01})
Step:  109800, Reward: [-449.649 -449.649 -449.649] [104.487], Avg: [-467.040 -467.040 -467.040] (0.0100) ({r_i: None, r_t: [-833.809 -833.809 -833.809], eps: 0.01})
Step:   15900, Reward: [-377.472 -377.472 -377.472] [66.374], Avg: [-458.257 -458.257 -458.257] (0.2031) ({r_i: None, r_t: [-832.949 -832.949 -832.949], eps: 0.203})
Step:   48300, Reward: [-513.498 -513.498 -513.498] [50.960], Avg: [-453.357 -453.357 -453.357] (0.0100) ({r_i: None, r_t: [-967.000 -967.000 -967.000], eps: 0.01})
Step:  109900, Reward: [-387.143 -387.143 -387.143] [84.525], Avg: [-466.967 -466.967 -466.967] (0.0100) ({r_i: None, r_t: [-823.449 -823.449 -823.449], eps: 0.01})
Step:   16000, Reward: [-445.035 -445.035 -445.035] [66.001], Avg: [-458.175 -458.175 -458.175] (0.2011) ({r_i: None, r_t: [-787.388 -787.388 -787.388], eps: 0.201})
Step:   48400, Reward: [-490.072 -490.072 -490.072] [65.427], Avg: [-453.432 -453.432 -453.432] (0.0100) ({r_i: None, r_t: [-990.499 -990.499 -990.499], eps: 0.01})
Step:  110000, Reward: [-437.042 -437.042 -437.042] [129.811], Avg: [-466.940 -466.940 -466.940] (0.0100) ({r_i: None, r_t: [-801.076 -801.076 -801.076], eps: 0.01})
Step:   16100, Reward: [-401.465 -401.465 -401.465] [66.821], Avg: [-457.825 -457.825 -457.825] (0.1991) ({r_i: None, r_t: [-823.742 -823.742 -823.742], eps: 0.199})
Step:  110100, Reward: [-464.323 -464.323 -464.323] [113.944], Avg: [-466.938 -466.938 -466.938] (0.0100) ({r_i: None, r_t: [-874.480 -874.480 -874.480], eps: 0.01})
Step:   48500, Reward: [-479.286 -479.286 -479.286] [56.881], Avg: [-453.486 -453.486 -453.486] (0.0100) ({r_i: None, r_t: [-986.431 -986.431 -986.431], eps: 0.01})
Step:  110200, Reward: [-383.633 -383.633 -383.633] [55.550], Avg: [-466.862 -466.862 -466.862] (0.0100) ({r_i: None, r_t: [-798.123 -798.123 -798.123], eps: 0.01})
Step:   16200, Reward: [-443.846 -443.846 -443.846] [76.973], Avg: [-457.739 -457.739 -457.739] (0.1971) ({r_i: None, r_t: [-843.454 -843.454 -843.454], eps: 0.197})
Step:   48600, Reward: [-493.508 -493.508 -493.508] [75.398], Avg: [-453.568 -453.568 -453.568] (0.0100) ({r_i: None, r_t: [-1015.658 -1015.658 -1015.658], eps: 0.01})
Step:  110300, Reward: [-431.862 -431.862 -431.862] [130.823], Avg: [-466.831 -466.831 -466.831] (0.0100) ({r_i: None, r_t: [-841.265 -841.265 -841.265], eps: 0.01})
Step:   16300, Reward: [-428.827 -428.827 -428.827] [102.716], Avg: [-457.563 -457.563 -457.563] (0.1951) ({r_i: None, r_t: [-848.025 -848.025 -848.025], eps: 0.195})
Step:   48700, Reward: [-459.200 -459.200 -459.200] [48.113], Avg: [-453.579 -453.579 -453.579] (0.0100) ({r_i: None, r_t: [-1018.159 -1018.159 -1018.159], eps: 0.01})
Step:  110400, Reward: [-385.532 -385.532 -385.532] [92.414], Avg: [-466.757 -466.757 -466.757] (0.0100) ({r_i: None, r_t: [-818.249 -818.249 -818.249], eps: 0.01})
Step:  110500, Reward: [-406.392 -406.392 -406.392] [64.975], Avg: [-466.703 -466.703 -466.703] (0.0100) ({r_i: None, r_t: [-793.309 -793.309 -793.309], eps: 0.01})
Step:   16400, Reward: [-429.247 -429.247 -429.247] [96.455], Avg: [-457.391 -457.391 -457.391] (0.1932) ({r_i: None, r_t: [-866.836 -866.836 -866.836], eps: 0.193})
Step:   48800, Reward: [-473.532 -473.532 -473.532] [60.281], Avg: [-453.620 -453.620 -453.620] (0.0100) ({r_i: None, r_t: [-1051.122 -1051.122 -1051.122], eps: 0.01})
Step:  110600, Reward: [-452.802 -452.802 -452.802] [103.450], Avg: [-466.690 -466.690 -466.690] (0.0100) ({r_i: None, r_t: [-818.636 -818.636 -818.636], eps: 0.01})
Step:   16500, Reward: [-383.140 -383.140 -383.140] [59.278], Avg: [-456.944 -456.944 -456.944] (0.1913) ({r_i: None, r_t: [-835.423 -835.423 -835.423], eps: 0.191})
Step:   48900, Reward: [-516.977 -516.977 -516.977] [56.901], Avg: [-453.750 -453.750 -453.750] (0.0100) ({r_i: None, r_t: [-1031.831 -1031.831 -1031.831], eps: 0.01})
Step:  110700, Reward: [-403.295 -403.295 -403.295] [77.577], Avg: [-466.633 -466.633 -466.633] (0.0100) ({r_i: None, r_t: [-830.032 -830.032 -830.032], eps: 0.01})
Step:   16600, Reward: [-402.568 -402.568 -402.568] [67.923], Avg: [-456.618 -456.618 -456.618] (0.1893) ({r_i: None, r_t: [-865.097 -865.097 -865.097], eps: 0.189})
Step:  110800, Reward: [-405.703 -405.703 -405.703] [70.816], Avg: [-466.578 -466.578 -466.578] (0.0100) ({r_i: None, r_t: [-800.683 -800.683 -800.683], eps: 0.01})
Step:   49000, Reward: [-513.231 -513.231 -513.231] [66.935], Avg: [-453.871 -453.871 -453.871] (0.0100) ({r_i: None, r_t: [-1015.534 -1015.534 -1015.534], eps: 0.01})
Step:  110900, Reward: [-426.972 -426.972 -426.972] [80.769], Avg: [-466.542 -466.542 -466.542] (0.0100) ({r_i: None, r_t: [-832.547 -832.547 -832.547], eps: 0.01})
Step:   16700, Reward: [-420.537 -420.537 -420.537] [75.064], Avg: [-456.404 -456.404 -456.404] (0.1875) ({r_i: None, r_t: [-863.393 -863.393 -863.393], eps: 0.187})
Step:   49100, Reward: [-505.578 -505.578 -505.578] [63.321], Avg: [-453.976 -453.976 -453.976] (0.0100) ({r_i: None, r_t: [-1006.700 -1006.700 -1006.700], eps: 0.01})
Step:  111000, Reward: [-409.410 -409.410 -409.410] [67.803], Avg: [-466.491 -466.491 -466.491] (0.0100) ({r_i: None, r_t: [-829.261 -829.261 -829.261], eps: 0.01})
Step:   16800, Reward: [-445.146 -445.146 -445.146] [102.214], Avg: [-456.337 -456.337 -456.337] (0.1856) ({r_i: None, r_t: [-885.675 -885.675 -885.675], eps: 0.186})
Step:   49200, Reward: [-492.618 -492.618 -492.618] [59.947], Avg: [-454.054 -454.054 -454.054] (0.0100) ({r_i: None, r_t: [-949.610 -949.610 -949.610], eps: 0.01})
Step:  111100, Reward: [-413.247 -413.247 -413.247] [86.926], Avg: [-466.443 -466.443 -466.443] (0.0100) ({r_i: None, r_t: [-836.223 -836.223 -836.223], eps: 0.01})
Step:   16900, Reward: [-429.190 -429.190 -429.190] [90.040], Avg: [-456.177 -456.177 -456.177] (0.1837) ({r_i: None, r_t: [-845.001 -845.001 -845.001], eps: 0.184})
Step:  111200, Reward: [-417.988 -417.988 -417.988] [52.455], Avg: [-466.399 -466.399 -466.399] (0.0100) ({r_i: None, r_t: [-851.325 -851.325 -851.325], eps: 0.01})
Step:   49300, Reward: [-505.885 -505.885 -505.885] [67.227], Avg: [-454.159 -454.159 -454.159] (0.0100) ({r_i: None, r_t: [-1000.553 -1000.553 -1000.553], eps: 0.01})
Step:  111300, Reward: [-406.727 -406.727 -406.727] [60.469], Avg: [-466.346 -466.346 -466.346] (0.0100) ({r_i: None, r_t: [-873.533 -873.533 -873.533], eps: 0.01})
Step:   17000, Reward: [-411.784 -411.784 -411.784] [58.027], Avg: [-455.918 -455.918 -455.918] (0.1819) ({r_i: None, r_t: [-854.767 -854.767 -854.767], eps: 0.182})
Step:   49400, Reward: [-477.905 -477.905 -477.905] [88.957], Avg: [-454.207 -454.207 -454.207] (0.0100) ({r_i: None, r_t: [-956.977 -956.977 -956.977], eps: 0.01})
Step:  111400, Reward: [-434.257 -434.257 -434.257] [83.811], Avg: [-466.317 -466.317 -466.317] (0.0100) ({r_i: None, r_t: [-802.474 -802.474 -802.474], eps: 0.01})
Step:   17100, Reward: [-423.742 -423.742 -423.742] [84.779], Avg: [-455.731 -455.731 -455.731] (0.1801) ({r_i: None, r_t: [-797.946 -797.946 -797.946], eps: 0.18})
Step:   49500, Reward: [-486.334 -486.334 -486.334] [63.154], Avg: [-454.272 -454.272 -454.272] (0.0100) ({r_i: None, r_t: [-1002.184 -1002.184 -1002.184], eps: 0.01})
Step:  111500, Reward: [-430.680 -430.680 -430.680] [96.383], Avg: [-466.285 -466.285 -466.285] (0.0100) ({r_i: None, r_t: [-846.573 -846.573 -846.573], eps: 0.01})
Step:  111600, Reward: [-432.802 -432.802 -432.802] [75.348], Avg: [-466.255 -466.255 -466.255] (0.0100) ({r_i: None, r_t: [-828.796 -828.796 -828.796], eps: 0.01})
Step:   17200, Reward: [-454.620 -454.620 -454.620] [77.825], Avg: [-455.724 -455.724 -455.724] (0.1783) ({r_i: None, r_t: [-871.627 -871.627 -871.627], eps: 0.178})
Step:   49600, Reward: [-506.421 -506.421 -506.421] [57.910], Avg: [-454.377 -454.377 -454.377] (0.0100) ({r_i: None, r_t: [-969.386 -969.386 -969.386], eps: 0.01})
Step:  111700, Reward: [-413.204 -413.204 -413.204] [67.119], Avg: [-466.208 -466.208 -466.208] (0.0100) ({r_i: None, r_t: [-814.132 -814.132 -814.132], eps: 0.01})
Step:   17300, Reward: [-400.737 -400.737 -400.737] [51.544], Avg: [-455.408 -455.408 -455.408] (0.1765) ({r_i: None, r_t: [-899.908 -899.908 -899.908], eps: 0.177})
Step:   49700, Reward: [-511.872 -511.872 -511.872] [73.620], Avg: [-454.492 -454.492 -454.492] (0.0100) ({r_i: None, r_t: [-978.994 -978.994 -978.994], eps: 0.01})
Step:  111800, Reward: [-414.449 -414.449 -414.449] [58.507], Avg: [-466.161 -466.161 -466.161] (0.0100) ({r_i: None, r_t: [-878.255 -878.255 -878.255], eps: 0.01})
Step:   17400, Reward: [-390.441 -390.441 -390.441] [55.147], Avg: [-455.037 -455.037 -455.037] (0.1748) ({r_i: None, r_t: [-899.084 -899.084 -899.084], eps: 0.175})
Step:   49800, Reward: [-509.206 -509.206 -509.206] [62.498], Avg: [-454.602 -454.602 -454.602] (0.0100) ({r_i: None, r_t: [-994.306 -994.306 -994.306], eps: 0.01})
Step:  111900, Reward: [-399.042 -399.042 -399.042] [78.746], Avg: [-466.101 -466.101 -466.101] (0.0100) ({r_i: None, r_t: [-835.495 -835.495 -835.495], eps: 0.01})
Step:  112000, Reward: [-443.000 -443.000 -443.000] [92.748], Avg: [-466.081 -466.081 -466.081] (0.0100) ({r_i: None, r_t: [-836.863 -836.863 -836.863], eps: 0.01})
Step:   17500, Reward: [-403.869 -403.869 -403.869] [37.509], Avg: [-454.746 -454.746 -454.746] (0.1730) ({r_i: None, r_t: [-826.333 -826.333 -826.333], eps: 0.173})
Step:   49900, Reward: [-483.833 -483.833 -483.833] [70.805], Avg: [-454.660 -454.660 -454.660] (0.0100) ({r_i: None, r_t: [-1011.114 -1011.114 -1011.114], eps: 0.01})
Step:  112100, Reward: [-385.302 -385.302 -385.302] [90.518], Avg: [-466.009 -466.009 -466.009] (0.0100) ({r_i: None, r_t: [-825.708 -825.708 -825.708], eps: 0.01})
Step:   17600, Reward: [-440.633 -440.633 -440.633] [95.019], Avg: [-454.667 -454.667 -454.667] (0.1713) ({r_i: None, r_t: [-821.620 -821.620 -821.620], eps: 0.171})
Step:   50000, Reward: [-466.895 -466.895 -466.895] [58.800], Avg: [-454.685 -454.685 -454.685] (0.0100) ({r_i: None, r_t: [-986.569 -986.569 -986.569], eps: 0.01})
Step:  112200, Reward: [-405.694 -405.694 -405.694] [63.148], Avg: [-465.955 -465.955 -465.955] (0.0100) ({r_i: None, r_t: [-822.130 -822.130 -822.130], eps: 0.01})
Step:   17700, Reward: [-431.572 -431.572 -431.572] [49.133], Avg: [-454.537 -454.537 -454.537] (0.1696) ({r_i: None, r_t: [-847.767 -847.767 -847.767], eps: 0.17})
Step:   50100, Reward: [-504.820 -504.820 -504.820] [78.917], Avg: [-454.785 -454.785 -454.785] (0.0100) ({r_i: None, r_t: [-967.843 -967.843 -967.843], eps: 0.01})
Step:  112300, Reward: [-443.384 -443.384 -443.384] [64.499], Avg: [-465.935 -465.935 -465.935] (0.0100) ({r_i: None, r_t: [-859.106 -859.106 -859.106], eps: 0.01})
Step:   17800, Reward: [-411.553 -411.553 -411.553] [61.897], Avg: [-454.297 -454.297 -454.297] (0.1679) ({r_i: None, r_t: [-877.605 -877.605 -877.605], eps: 0.168})
Step:  112400, Reward: [-401.541 -401.541 -401.541] [66.377], Avg: [-465.878 -465.878 -465.878] (0.0100) ({r_i: None, r_t: [-826.744 -826.744 -826.744], eps: 0.01})
Step:   50200, Reward: [-506.383 -506.383 -506.383] [61.524], Avg: [-454.887 -454.887 -454.887] (0.0100) ({r_i: None, r_t: [-1026.930 -1026.930 -1026.930], eps: 0.01})
Step:  112500, Reward: [-440.015 -440.015 -440.015] [105.137], Avg: [-465.855 -465.855 -465.855] (0.0100) ({r_i: None, r_t: [-847.676 -847.676 -847.676], eps: 0.01})
Step:   17900, Reward: [-424.827 -424.827 -424.827] [64.099], Avg: [-454.133 -454.133 -454.133] (0.1662) ({r_i: None, r_t: [-819.427 -819.427 -819.427], eps: 0.166})
Step:   50300, Reward: [-514.872 -514.872 -514.872] [77.564], Avg: [-455.006 -455.006 -455.006] (0.0100) ({r_i: None, r_t: [-956.471 -956.471 -956.471], eps: 0.01})
Step:  112600, Reward: [-427.416 -427.416 -427.416] [74.213], Avg: [-465.821 -465.821 -465.821] (0.0100) ({r_i: None, r_t: [-860.033 -860.033 -860.033], eps: 0.01})
Step:   18000, Reward: [-441.907 -441.907 -441.907] [77.472], Avg: [-454.065 -454.065 -454.065] (0.1646) ({r_i: None, r_t: [-855.987 -855.987 -855.987], eps: 0.165})
Step:   50400, Reward: [-520.040 -520.040 -520.040] [76.213], Avg: [-455.135 -455.135 -455.135] (0.0100) ({r_i: None, r_t: [-999.271 -999.271 -999.271], eps: 0.01})
Step:  112700, Reward: [-422.485 -422.485 -422.485] [57.178], Avg: [-465.782 -465.782 -465.782] (0.0100) ({r_i: None, r_t: [-840.132 -840.132 -840.132], eps: 0.01})
Step:   18100, Reward: [-419.264 -419.264 -419.264] [58.203], Avg: [-453.874 -453.874 -453.874] (0.1629) ({r_i: None, r_t: [-864.260 -864.260 -864.260], eps: 0.163})
Step:  112800, Reward: [-373.417 -373.417 -373.417] [80.388], Avg: [-465.700 -465.700 -465.700] (0.0100) ({r_i: None, r_t: [-822.767 -822.767 -822.767], eps: 0.01})
Step:   50500, Reward: [-475.843 -475.843 -475.843] [80.921], Avg: [-455.176 -455.176 -455.176] (0.0100) ({r_i: None, r_t: [-1009.568 -1009.568 -1009.568], eps: 0.01})
Step:  112900, Reward: [-400.006 -400.006 -400.006] [62.478], Avg: [-465.642 -465.642 -465.642] (0.0100) ({r_i: None, r_t: [-806.286 -806.286 -806.286], eps: 0.01})
Step:   18200, Reward: [-398.055 -398.055 -398.055] [58.101], Avg: [-453.569 -453.569 -453.569] (0.1613) ({r_i: None, r_t: [-855.064 -855.064 -855.064], eps: 0.161})
Step:   50600, Reward: [-490.535 -490.535 -490.535] [62.402], Avg: [-455.246 -455.246 -455.246] (0.0100) ({r_i: None, r_t: [-982.930 -982.930 -982.930], eps: 0.01})
Step:  113000, Reward: [-436.092 -436.092 -436.092] [91.366], Avg: [-465.616 -465.616 -465.616] (0.0100) ({r_i: None, r_t: [-786.854 -786.854 -786.854], eps: 0.01})
Step:   18300, Reward: [-425.427 -425.427 -425.427] [98.879], Avg: [-453.416 -453.416 -453.416] (0.1597) ({r_i: None, r_t: [-825.841 -825.841 -825.841], eps: 0.16})
Step:   50700, Reward: [-475.922 -475.922 -475.922] [75.280], Avg: [-455.286 -455.286 -455.286] (0.0100) ({r_i: None, r_t: [-987.698 -987.698 -987.698], eps: 0.01})
Step:  113100, Reward: [-387.790 -387.790 -387.790] [68.649], Avg: [-465.547 -465.547 -465.547] (0.0100) ({r_i: None, r_t: [-801.047 -801.047 -801.047], eps: 0.01})
Step:  113200, Reward: [-424.284 -424.284 -424.284] [100.840], Avg: [-465.511 -465.511 -465.511] (0.0100) ({r_i: None, r_t: [-788.261 -788.261 -788.261], eps: 0.01})
Step:   18400, Reward: [-399.267 -399.267 -399.267] [60.808], Avg: [-453.124 -453.124 -453.124] (0.1581) ({r_i: None, r_t: [-853.030 -853.030 -853.030], eps: 0.158})
Step:   50800, Reward: [-515.775 -515.775 -515.775] [72.273], Avg: [-455.405 -455.405 -455.405] (0.0100) ({r_i: None, r_t: [-995.970 -995.970 -995.970], eps: 0.01})
Step:  113300, Reward: [-405.798 -405.798 -405.798] [70.318], Avg: [-465.458 -465.458 -465.458] (0.0100) ({r_i: None, r_t: [-794.547 -794.547 -794.547], eps: 0.01})
Step:   18500, Reward: [-406.974 -406.974 -406.974] [52.673], Avg: [-452.875 -452.875 -452.875] (0.1565) ({r_i: None, r_t: [-799.690 -799.690 -799.690], eps: 0.157})
Step:   50900, Reward: [-486.426 -486.426 -486.426] [63.895], Avg: [-455.466 -455.466 -455.466] (0.0100) ({r_i: None, r_t: [-1005.693 -1005.693 -1005.693], eps: 0.01})
Step:  113400, Reward: [-400.949 -400.949 -400.949] [92.032], Avg: [-465.402 -465.402 -465.402] (0.0100) ({r_i: None, r_t: [-835.319 -835.319 -835.319], eps: 0.01})
Step:   18600, Reward: [-396.135 -396.135 -396.135] [81.492], Avg: [-452.572 -452.572 -452.572] (0.1549) ({r_i: None, r_t: [-852.302 -852.302 -852.302], eps: 0.155})
Step:   51000, Reward: [-469.852 -469.852 -469.852] [54.596], Avg: [-455.494 -455.494 -455.494] (0.0100) ({r_i: None, r_t: [-966.277 -966.277 -966.277], eps: 0.01})
Step:  113500, Reward: [-414.341 -414.341 -414.341] [85.028], Avg: [-465.357 -465.357 -465.357] (0.0100) ({r_i: None, r_t: [-865.247 -865.247 -865.247], eps: 0.01})
Step:  113600, Reward: [-402.313 -402.313 -402.313] [52.528], Avg: [-465.301 -465.301 -465.301] (0.0100) ({r_i: None, r_t: [-793.169 -793.169 -793.169], eps: 0.01})
Step:   18700, Reward: [-428.466 -428.466 -428.466] [67.518], Avg: [-452.444 -452.444 -452.444] (0.1534) ({r_i: None, r_t: [-845.236 -845.236 -845.236], eps: 0.153})
Step:   51100, Reward: [-514.037 -514.037 -514.037] [59.835], Avg: [-455.608 -455.608 -455.608] (0.0100) ({r_i: None, r_t: [-1010.319 -1010.319 -1010.319], eps: 0.01})
Step:  113700, Reward: [-393.355 -393.355 -393.355] [82.202], Avg: [-465.238 -465.238 -465.238] (0.0100) ({r_i: None, r_t: [-829.598 -829.598 -829.598], eps: 0.01})
Step:   18800, Reward: [-388.142 -388.142 -388.142] [67.297], Avg: [-452.104 -452.104 -452.104] (0.1519) ({r_i: None, r_t: [-852.979 -852.979 -852.979], eps: 0.152})
Step:   51200, Reward: [-533.205 -533.205 -533.205] [53.599], Avg: [-455.760 -455.760 -455.760] (0.0100) ({r_i: None, r_t: [-969.729 -969.729 -969.729], eps: 0.01})
Step:  113800, Reward: [-425.484 -425.484 -425.484] [88.610], Avg: [-465.203 -465.203 -465.203] (0.0100) ({r_i: None, r_t: [-830.307 -830.307 -830.307], eps: 0.01})
Step:   18900, Reward: [-388.159 -388.159 -388.159] [49.990], Avg: [-451.767 -451.767 -451.767] (0.1504) ({r_i: None, r_t: [-811.097 -811.097 -811.097], eps: 0.15})
Step:   51300, Reward: [-477.245 -477.245 -477.245] [74.335], Avg: [-455.802 -455.802 -455.802] (0.0100) ({r_i: None, r_t: [-995.576 -995.576 -995.576], eps: 0.01})
Step:  113900, Reward: [-409.953 -409.953 -409.953] [63.771], Avg: [-465.155 -465.155 -465.155] (0.0100) ({r_i: None, r_t: [-826.888 -826.888 -826.888], eps: 0.01})
Step:  114000, Reward: [-411.704 -411.704 -411.704] [99.249], Avg: [-465.108 -465.108 -465.108] (0.0100) ({r_i: None, r_t: [-837.450 -837.450 -837.450], eps: 0.01})
Step:   19000, Reward: [-431.867 -431.867 -431.867] [73.526], Avg: [-451.663 -451.663 -451.663] (0.1489) ({r_i: None, r_t: [-830.034 -830.034 -830.034], eps: 0.149})
Step:   51400, Reward: [-494.479 -494.479 -494.479] [39.383], Avg: [-455.877 -455.877 -455.877] (0.0100) ({r_i: None, r_t: [-1014.984 -1014.984 -1014.984], eps: 0.01})
Step:  114100, Reward: [-421.217 -421.217 -421.217] [49.301], Avg: [-465.069 -465.069 -465.069] (0.0100) ({r_i: None, r_t: [-872.050 -872.050 -872.050], eps: 0.01})
Step:   19100, Reward: [-426.770 -426.770 -426.770] [75.785], Avg: [-451.533 -451.533 -451.533] (0.1474) ({r_i: None, r_t: [-855.818 -855.818 -855.818], eps: 0.147})
Step:   51500, Reward: [-515.303 -515.303 -515.303] [52.761], Avg: [-455.992 -455.992 -455.992] (0.0100) ({r_i: None, r_t: [-987.821 -987.821 -987.821], eps: 0.01})
Step:  114200, Reward: [-385.021 -385.021 -385.021] [74.038], Avg: [-464.999 -464.999 -464.999] (0.0100) ({r_i: None, r_t: [-827.377 -827.377 -827.377], eps: 0.01})
Step:   19200, Reward: [-409.715 -409.715 -409.715] [72.903], Avg: [-451.316 -451.316 -451.316] (0.1459) ({r_i: None, r_t: [-826.467 -826.467 -826.467], eps: 0.146})
Step:  114300, Reward: [-408.921 -408.921 -408.921] [60.345], Avg: [-464.950 -464.950 -464.950] (0.0100) ({r_i: None, r_t: [-801.705 -801.705 -801.705], eps: 0.01})
Step:   51600, Reward: [-521.585 -521.585 -521.585] [75.756], Avg: [-456.119 -456.119 -456.119] (0.0100) ({r_i: None, r_t: [-957.022 -957.022 -957.022], eps: 0.01})
Step:  114400, Reward: [-423.639 -423.639 -423.639] [88.315], Avg: [-464.914 -464.914 -464.914] (0.0100) ({r_i: None, r_t: [-846.122 -846.122 -846.122], eps: 0.01})
Step:   19300, Reward: [-411.837 -411.837 -411.837] [49.734], Avg: [-451.113 -451.113 -451.113] (0.1444) ({r_i: None, r_t: [-856.007 -856.007 -856.007], eps: 0.144})
Step:   51700, Reward: [-506.005 -506.005 -506.005] [83.140], Avg: [-456.215 -456.215 -456.215] (0.0100) ({r_i: None, r_t: [-941.559 -941.559 -941.559], eps: 0.01})
Step:  114500, Reward: [-374.557 -374.557 -374.557] [64.425], Avg: [-464.835 -464.835 -464.835] (0.0100) ({r_i: None, r_t: [-826.799 -826.799 -826.799], eps: 0.01})
Step:   19400, Reward: [-404.500 -404.500 -404.500] [57.767], Avg: [-450.874 -450.874 -450.874] (0.1430) ({r_i: None, r_t: [-830.856 -830.856 -830.856], eps: 0.143})
Step:   51800, Reward: [-491.261 -491.261 -491.261] [76.897], Avg: [-456.283 -456.283 -456.283] (0.0100) ({r_i: None, r_t: [-1018.560 -1018.560 -1018.560], eps: 0.01})
Step:  114600, Reward: [-424.847 -424.847 -424.847] [134.048], Avg: [-464.800 -464.800 -464.800] (0.0100) ({r_i: None, r_t: [-798.691 -798.691 -798.691], eps: 0.01})
Step:   19500, Reward: [-428.616 -428.616 -428.616] [57.366], Avg: [-450.760 -450.760 -450.760] (0.1416) ({r_i: None, r_t: [-776.560 -776.560 -776.560], eps: 0.142})
Step:  114700, Reward: [-415.952 -415.952 -415.952] [67.803], Avg: [-464.758 -464.758 -464.758] (0.0100) ({r_i: None, r_t: [-776.700 -776.700 -776.700], eps: 0.01})
Step:   51900, Reward: [-474.183 -474.183 -474.183] [73.381], Avg: [-456.317 -456.317 -456.317] (0.0100) ({r_i: None, r_t: [-985.696 -985.696 -985.696], eps: 0.01})
Step:  114800, Reward: [-384.165 -384.165 -384.165] [78.292], Avg: [-464.688 -464.688 -464.688] (0.0100) ({r_i: None, r_t: [-845.199 -845.199 -845.199], eps: 0.01})
Step:   19600, Reward: [-399.534 -399.534 -399.534] [74.046], Avg: [-450.500 -450.500 -450.500] (0.1402) ({r_i: None, r_t: [-846.354 -846.354 -846.354], eps: 0.14})
Step:   52000, Reward: [-516.817 -516.817 -516.817] [77.602], Avg: [-456.433 -456.433 -456.433] (0.0100) ({r_i: None, r_t: [-1010.096 -1010.096 -1010.096], eps: 0.01})
Step:  114900, Reward: [-426.059 -426.059 -426.059] [54.041], Avg: [-464.654 -464.654 -464.654] (0.0100) ({r_i: None, r_t: [-829.416 -829.416 -829.416], eps: 0.01})
Step:   19700, Reward: [-457.374 -457.374 -457.374] [86.747], Avg: [-450.535 -450.535 -450.535] (0.1388) ({r_i: None, r_t: [-831.609 -831.609 -831.609], eps: 0.139})
Step:   52100, Reward: [-498.290 -498.290 -498.290] [74.017], Avg: [-456.513 -456.513 -456.513] (0.0100) ({r_i: None, r_t: [-1006.250 -1006.250 -1006.250], eps: 0.01})
Step:  115000, Reward: [-404.909 -404.909 -404.909] [77.824], Avg: [-464.602 -464.602 -464.602] (0.0100) ({r_i: None, r_t: [-823.275 -823.275 -823.275], eps: 0.01})
Step:   19800, Reward: [-394.439 -394.439 -394.439] [46.195], Avg: [-450.253 -450.253 -450.253] (0.1374) ({r_i: None, r_t: [-824.257 -824.257 -824.257], eps: 0.137})
Step:  115100, Reward: [-428.306 -428.306 -428.306] [97.459], Avg: [-464.571 -464.571 -464.571] (0.0100) ({r_i: None, r_t: [-799.918 -799.918 -799.918], eps: 0.01})
Step:   52200, Reward: [-503.721 -503.721 -503.721] [80.964], Avg: [-456.604 -456.604 -456.604] (0.0100) ({r_i: None, r_t: [-979.290 -979.290 -979.290], eps: 0.01})
Step:  115200, Reward: [-399.969 -399.969 -399.969] [65.786], Avg: [-464.515 -464.515 -464.515] (0.0100) ({r_i: None, r_t: [-802.915 -802.915 -802.915], eps: 0.01})
Step:   19900, Reward: [-408.635 -408.635 -408.635] [69.639], Avg: [-450.045 -450.045 -450.045] (0.1360) ({r_i: None, r_t: [-848.859 -848.859 -848.859], eps: 0.136})
Step:   52300, Reward: [-488.128 -488.128 -488.128] [71.041], Avg: [-456.664 -456.664 -456.664] (0.0100) ({r_i: None, r_t: [-963.011 -963.011 -963.011], eps: 0.01})
Step:  115300, Reward: [-401.922 -401.922 -401.922] [69.339], Avg: [-464.460 -464.460 -464.460] (0.0100) ({r_i: None, r_t: [-816.601 -816.601 -816.601], eps: 0.01})
Step:   20000, Reward: [-466.152 -466.152 -466.152] [77.924], Avg: [-450.125 -450.125 -450.125] (0.1347) ({r_i: None, r_t: [-803.426 -803.426 -803.426], eps: 0.135})
Step:   52400, Reward: [-488.299 -488.299 -488.299] [54.006], Avg: [-456.724 -456.724 -456.724] (0.0100) ({r_i: None, r_t: [-1010.686 -1010.686 -1010.686], eps: 0.01})
Step:  115400, Reward: [-420.691 -420.691 -420.691] [91.607], Avg: [-464.423 -464.423 -464.423] (0.0100) ({r_i: None, r_t: [-809.242 -809.242 -809.242], eps: 0.01})
Step:   20100, Reward: [-425.652 -425.652 -425.652] [80.667], Avg: [-450.004 -450.004 -450.004] (0.1333) ({r_i: None, r_t: [-805.319 -805.319 -805.319], eps: 0.133})
Step:  115500, Reward: [-397.622 -397.622 -397.622] [99.609], Avg: [-464.365 -464.365 -464.365] (0.0100) ({r_i: None, r_t: [-778.362 -778.362 -778.362], eps: 0.01})
Step:   52500, Reward: [-505.727 -505.727 -505.727] [93.556], Avg: [-456.817 -456.817 -456.817] (0.0100) ({r_i: None, r_t: [-983.674 -983.674 -983.674], eps: 0.01})
Step:  115600, Reward: [-424.664 -424.664 -424.664] [82.113], Avg: [-464.330 -464.330 -464.330] (0.0100) ({r_i: None, r_t: [-843.295 -843.295 -843.295], eps: 0.01})
Step:   20200, Reward: [-395.416 -395.416 -395.416] [62.831], Avg: [-449.735 -449.735 -449.735] (0.1320) ({r_i: None, r_t: [-836.313 -836.313 -836.313], eps: 0.132})
Step:   52600, Reward: [-514.893 -514.893 -514.893] [61.933], Avg: [-456.927 -456.927 -456.927] (0.0100) ({r_i: None, r_t: [-952.367 -952.367 -952.367], eps: 0.01})
Step:  115700, Reward: [-388.183 -388.183 -388.183] [91.179], Avg: [-464.265 -464.265 -464.265] (0.0100) ({r_i: None, r_t: [-762.461 -762.461 -762.461], eps: 0.01})
Step:   20300, Reward: [-406.769 -406.769 -406.769] [62.348], Avg: [-449.525 -449.525 -449.525] (0.1307) ({r_i: None, r_t: [-805.538 -805.538 -805.538], eps: 0.131})
Step:   52700, Reward: [-508.189 -508.189 -508.189] [69.381], Avg: [-457.024 -457.024 -457.024] (0.0100) ({r_i: None, r_t: [-956.722 -956.722 -956.722], eps: 0.01})
Step:  115800, Reward: [-415.649 -415.649 -415.649] [80.866], Avg: [-464.223 -464.223 -464.223] (0.0100) ({r_i: None, r_t: [-792.948 -792.948 -792.948], eps: 0.01})
Step:   20400, Reward: [-382.658 -382.658 -382.658] [36.950], Avg: [-449.198 -449.198 -449.198] (0.1294) ({r_i: None, r_t: [-835.007 -835.007 -835.007], eps: 0.129})
Step:  115900, Reward: [-408.965 -408.965 -408.965] [76.270], Avg: [-464.175 -464.175 -464.175] (0.0100) ({r_i: None, r_t: [-792.700 -792.700 -792.700], eps: 0.01})
Step:   52800, Reward: [-507.976 -507.976 -507.976] [53.325], Avg: [-457.121 -457.121 -457.121] (0.0100) ({r_i: None, r_t: [-1014.209 -1014.209 -1014.209], eps: 0.01})
Step:  116000, Reward: [-384.469 -384.469 -384.469] [61.911], Avg: [-464.106 -464.106 -464.106] (0.0100) ({r_i: None, r_t: [-810.964 -810.964 -810.964], eps: 0.01})
Step:   20500, Reward: [-426.441 -426.441 -426.441] [74.311], Avg: [-449.088 -449.088 -449.088] (0.1281) ({r_i: None, r_t: [-843.521 -843.521 -843.521], eps: 0.128})
Step:   52900, Reward: [-474.504 -474.504 -474.504] [57.346], Avg: [-457.154 -457.154 -457.154] (0.0100) ({r_i: None, r_t: [-995.733 -995.733 -995.733], eps: 0.01})
Step:  116100, Reward: [-427.419 -427.419 -427.419] [119.557], Avg: [-464.075 -464.075 -464.075] (0.0100) ({r_i: None, r_t: [-767.216 -767.216 -767.216], eps: 0.01})
Step:   20600, Reward: [-415.520 -415.520 -415.520] [61.019], Avg: [-448.926 -448.926 -448.926] (0.1268) ({r_i: None, r_t: [-848.246 -848.246 -848.246], eps: 0.127})
Step:  116200, Reward: [-436.112 -436.112 -436.112] [91.551], Avg: [-464.051 -464.051 -464.051] (0.0100) ({r_i: None, r_t: [-856.801 -856.801 -856.801], eps: 0.01})
Step:   53000, Reward: [-471.001 -471.001 -471.001] [48.364], Avg: [-457.180 -457.180 -457.180] (0.0100) ({r_i: None, r_t: [-1013.871 -1013.871 -1013.871], eps: 0.01})
Step:   20700, Reward: [-433.159 -433.159 -433.159] [81.408], Avg: [-448.850 -448.850 -448.850] (0.1255) ({r_i: None, r_t: [-843.966 -843.966 -843.966], eps: 0.126})
Step:  116300, Reward: [-400.793 -400.793 -400.793] [54.651], Avg: [-463.997 -463.997 -463.997] (0.0100) ({r_i: None, r_t: [-769.420 -769.420 -769.420], eps: 0.01})
Step:   53100, Reward: [-487.113 -487.113 -487.113] [93.211], Avg: [-457.236 -457.236 -457.236] (0.0100) ({r_i: None, r_t: [-1007.357 -1007.357 -1007.357], eps: 0.01})
Step:  116400, Reward: [-368.543 -368.543 -368.543] [57.070], Avg: [-463.915 -463.915 -463.915] (0.0100) ({r_i: None, r_t: [-753.885 -753.885 -753.885], eps: 0.01})
Step:   20800, Reward: [-415.965 -415.965 -415.965] [80.054], Avg: [-448.693 -448.693 -448.693] (0.1243) ({r_i: None, r_t: [-818.431 -818.431 -818.431], eps: 0.124})
Step:   53200, Reward: [-494.509 -494.509 -494.509] [61.967], Avg: [-457.306 -457.306 -457.306] (0.0100) ({r_i: None, r_t: [-1051.189 -1051.189 -1051.189], eps: 0.01})
Step:  116500, Reward: [-414.480 -414.480 -414.480] [68.730], Avg: [-463.872 -463.872 -463.872] (0.0100) ({r_i: None, r_t: [-799.454 -799.454 -799.454], eps: 0.01})
Step:   20900, Reward: [-450.125 -450.125 -450.125] [87.711], Avg: [-448.699 -448.699 -448.699] (0.1230) ({r_i: None, r_t: [-866.348 -866.348 -866.348], eps: 0.123})
Step:   53300, Reward: [-469.971 -469.971 -469.971] [68.141], Avg: [-457.330 -457.330 -457.330] (0.0100) ({r_i: None, r_t: [-965.277 -965.277 -965.277], eps: 0.01})
Step:  116600, Reward: [-428.127 -428.127 -428.127] [87.279], Avg: [-463.842 -463.842 -463.842] (0.0100) ({r_i: None, r_t: [-830.021 -830.021 -830.021], eps: 0.01})
Step:   21000, Reward: [-404.226 -404.226 -404.226] [70.724], Avg: [-448.489 -448.489 -448.489] (0.1218) ({r_i: None, r_t: [-844.122 -844.122 -844.122], eps: 0.122})
Step:  116700, Reward: [-418.439 -418.439 -418.439] [110.411], Avg: [-463.803 -463.803 -463.803] (0.0100) ({r_i: None, r_t: [-833.065 -833.065 -833.065], eps: 0.01})
Step:   53400, Reward: [-532.451 -532.451 -532.451] [71.372], Avg: [-457.470 -457.470 -457.470] (0.0100) ({r_i: None, r_t: [-967.188 -967.188 -967.188], eps: 0.01})
Step:  116800, Reward: [-412.479 -412.479 -412.479] [102.667], Avg: [-463.759 -463.759 -463.759] (0.0100) ({r_i: None, r_t: [-781.758 -781.758 -781.758], eps: 0.01})
Step:   21100, Reward: [-420.937 -420.937 -420.937] [95.115], Avg: [-448.359 -448.359 -448.359] (0.1206) ({r_i: None, r_t: [-851.134 -851.134 -851.134], eps: 0.121})
Step:   53500, Reward: [-503.346 -503.346 -503.346] [75.000], Avg: [-457.556 -457.556 -457.556] (0.0100) ({r_i: None, r_t: [-1002.530 -1002.530 -1002.530], eps: 0.01})
Step:  116900, Reward: [-391.531 -391.531 -391.531] [65.483], Avg: [-463.697 -463.697 -463.697] (0.0100) ({r_i: None, r_t: [-758.719 -758.719 -758.719], eps: 0.01})
Step:   21200, Reward: [-396.503 -396.503 -396.503] [67.467], Avg: [-448.115 -448.115 -448.115] (0.1194) ({r_i: None, r_t: [-842.874 -842.874 -842.874], eps: 0.119})
Step:  117000, Reward: [-361.750 -361.750 -361.750] [54.094], Avg: [-463.610 -463.610 -463.610] (0.0100) ({r_i: None, r_t: [-760.762 -760.762 -760.762], eps: 0.01})
Step:   53600, Reward: [-497.117 -497.117 -497.117] [45.820], Avg: [-457.629 -457.629 -457.629] (0.0100) ({r_i: None, r_t: [-965.175 -965.175 -965.175], eps: 0.01})
Step:  117100, Reward: [-415.080 -415.080 -415.080] [96.867], Avg: [-463.569 -463.569 -463.569] (0.0100) ({r_i: None, r_t: [-767.488 -767.488 -767.488], eps: 0.01})
Step:   21300, Reward: [-442.869 -442.869 -442.869] [65.671], Avg: [-448.091 -448.091 -448.091] (0.1182) ({r_i: None, r_t: [-897.529 -897.529 -897.529], eps: 0.118})
Step:   53700, Reward: [-521.563 -521.563 -521.563] [84.824], Avg: [-457.748 -457.748 -457.748] (0.0100) ({r_i: None, r_t: [-999.942 -999.942 -999.942], eps: 0.01})
Step:  117200, Reward: [-381.525 -381.525 -381.525] [60.662], Avg: [-463.499 -463.499 -463.499] (0.0100) ({r_i: None, r_t: [-808.590 -808.590 -808.590], eps: 0.01})
Step:   21400, Reward: [-391.114 -391.114 -391.114] [64.759], Avg: [-447.826 -447.826 -447.826] (0.1170) ({r_i: None, r_t: [-787.150 -787.150 -787.150], eps: 0.117})
Step:   53800, Reward: [-491.945 -491.945 -491.945] [71.125], Avg: [-457.811 -457.811 -457.811] (0.0100) ({r_i: None, r_t: [-937.809 -937.809 -937.809], eps: 0.01})
Step:  117300, Reward: [-393.420 -393.420 -393.420] [71.194], Avg: [-463.439 -463.439 -463.439] (0.0100) ({r_i: None, r_t: [-873.841 -873.841 -873.841], eps: 0.01})
Step:   21500, Reward: [-397.933 -397.933 -397.933] [43.432], Avg: [-447.595 -447.595 -447.595] (0.1159) ({r_i: None, r_t: [-836.254 -836.254 -836.254], eps: 0.116})
Step:  117400, Reward: [-369.075 -369.075 -369.075] [67.251], Avg: [-463.359 -463.359 -463.359] (0.0100) ({r_i: None, r_t: [-832.340 -832.340 -832.340], eps: 0.01})
Step:   53900, Reward: [-504.141 -504.141 -504.141] [66.684], Avg: [-457.897 -457.897 -457.897] (0.0100) ({r_i: None, r_t: [-989.624 -989.624 -989.624], eps: 0.01})
Step:  117500, Reward: [-381.986 -381.986 -381.986] [91.321], Avg: [-463.289 -463.289 -463.289] (0.0100) ({r_i: None, r_t: [-795.441 -795.441 -795.441], eps: 0.01})
Step:   21600, Reward: [-411.371 -411.371 -411.371] [74.664], Avg: [-447.428 -447.428 -447.428] (0.1147) ({r_i: None, r_t: [-863.801 -863.801 -863.801], eps: 0.115})
Step:   54000, Reward: [-476.817 -476.817 -476.817] [58.662], Avg: [-457.932 -457.932 -457.932] (0.0100) ({r_i: None, r_t: [-953.449 -953.449 -953.449], eps: 0.01})
Step:  117600, Reward: [-409.517 -409.517 -409.517] [76.126], Avg: [-463.244 -463.244 -463.244] (0.0100) ({r_i: None, r_t: [-864.241 -864.241 -864.241], eps: 0.01})
Step:   21700, Reward: [-431.249 -431.249 -431.249] [63.504], Avg: [-447.354 -447.354 -447.354] (0.1136) ({r_i: None, r_t: [-829.008 -829.008 -829.008], eps: 0.114})
Step:   54100, Reward: [-505.319 -505.319 -505.319] [78.553], Avg: [-458.020 -458.020 -458.020] (0.0100) ({r_i: None, r_t: [-982.682 -982.682 -982.682], eps: 0.01})
Step:  117700, Reward: [-443.272 -443.272 -443.272] [94.616], Avg: [-463.227 -463.227 -463.227] (0.0100) ({r_i: None, r_t: [-769.166 -769.166 -769.166], eps: 0.01})
Step:   21800, Reward: [-424.402 -424.402 -424.402] [78.246], Avg: [-447.249 -447.249 -447.249] (0.1124) ({r_i: None, r_t: [-900.462 -900.462 -900.462], eps: 0.112})
Step:  117800, Reward: [-399.122 -399.122 -399.122] [91.580], Avg: [-463.172 -463.172 -463.172] (0.0100) ({r_i: None, r_t: [-804.265 -804.265 -804.265], eps: 0.01})
Step:   54200, Reward: [-471.309 -471.309 -471.309] [44.845], Avg: [-458.044 -458.044 -458.044] (0.0100) ({r_i: None, r_t: [-974.210 -974.210 -974.210], eps: 0.01})
Step:  117900, Reward: [-406.473 -406.473 -406.473] [82.014], Avg: [-463.124 -463.124 -463.124] (0.0100) ({r_i: None, r_t: [-850.830 -850.830 -850.830], eps: 0.01})
Step:   21900, Reward: [-402.229 -402.229 -402.229] [61.054], Avg: [-447.044 -447.044 -447.044] (0.1113) ({r_i: None, r_t: [-821.442 -821.442 -821.442], eps: 0.111})
Step:   54300, Reward: [-471.312 -471.312 -471.312] [64.774], Avg: [-458.069 -458.069 -458.069] (0.0100) ({r_i: None, r_t: [-991.735 -991.735 -991.735], eps: 0.01})
Step:  118000, Reward: [-428.920 -428.920 -428.920] [74.743], Avg: [-463.095 -463.095 -463.095] (0.0100) ({r_i: None, r_t: [-838.544 -838.544 -838.544], eps: 0.01})
Step:   22000, Reward: [-419.993 -419.993 -419.993] [93.473], Avg: [-446.922 -446.922 -446.922] (0.1102) ({r_i: None, r_t: [-805.839 -805.839 -805.839], eps: 0.11})
Step:   54400, Reward: [-498.724 -498.724 -498.724] [75.603], Avg: [-458.143 -458.143 -458.143] (0.0100) ({r_i: None, r_t: [-951.097 -951.097 -951.097], eps: 0.01})
Step:  118100, Reward: [-393.435 -393.435 -393.435] [82.732], Avg: [-463.036 -463.036 -463.036] (0.0100) ({r_i: None, r_t: [-783.145 -783.145 -783.145], eps: 0.01})
Step:   22100, Reward: [-396.332 -396.332 -396.332] [53.669], Avg: [-446.694 -446.694 -446.694] (0.1091) ({r_i: None, r_t: [-824.729 -824.729 -824.729], eps: 0.109})
Step:  118200, Reward: [-423.631 -423.631 -423.631] [61.807], Avg: [-463.003 -463.003 -463.003] (0.0100) ({r_i: None, r_t: [-785.701 -785.701 -785.701], eps: 0.01})
Step:   54500, Reward: [-474.209 -474.209 -474.209] [65.529], Avg: [-458.173 -458.173 -458.173] (0.0100) ({r_i: None, r_t: [-954.112 -954.112 -954.112], eps: 0.01})
Step:  118300, Reward: [-433.672 -433.672 -433.672] [117.162], Avg: [-462.978 -462.978 -462.978] (0.0100) ({r_i: None, r_t: [-805.273 -805.273 -805.273], eps: 0.01})
Step:   22200, Reward: [-400.584 -400.584 -400.584] [74.380], Avg: [-446.487 -446.487 -446.487] (0.1080) ({r_i: None, r_t: [-836.769 -836.769 -836.769], eps: 0.108})
Step:   54600, Reward: [-523.031 -523.031 -523.031] [59.890], Avg: [-458.291 -458.291 -458.291] (0.0100) ({r_i: None, r_t: [-945.677 -945.677 -945.677], eps: 0.01})
Step:  118400, Reward: [-375.666 -375.666 -375.666] [50.848], Avg: [-462.905 -462.905 -462.905] (0.0100) ({r_i: None, r_t: [-831.426 -831.426 -831.426], eps: 0.01})
Step:   22300, Reward: [-423.833 -423.833 -423.833] [74.930], Avg: [-446.386 -446.386 -446.386] (0.1069) ({r_i: None, r_t: [-864.379 -864.379 -864.379], eps: 0.107})
Step:   54700, Reward: [-500.014 -500.014 -500.014] [81.967], Avg: [-458.367 -458.367 -458.367] (0.0100) ({r_i: None, r_t: [-938.666 -938.666 -938.666], eps: 0.01})
Step:  118500, Reward: [-407.562 -407.562 -407.562] [80.636], Avg: [-462.858 -462.858 -462.858] (0.0100) ({r_i: None, r_t: [-791.176 -791.176 -791.176], eps: 0.01})
Step:   22400, Reward: [-412.693 -412.693 -412.693] [61.506], Avg: [-446.236 -446.236 -446.236] (0.1059) ({r_i: None, r_t: [-820.180 -820.180 -820.180], eps: 0.106})
Step:  118600, Reward: [-422.012 -422.012 -422.012] [103.310], Avg: [-462.824 -462.824 -462.824] (0.0100) ({r_i: None, r_t: [-848.025 -848.025 -848.025], eps: 0.01})
Step:   54800, Reward: [-506.741 -506.741 -506.741] [94.769], Avg: [-458.455 -458.455 -458.455] (0.0100) ({r_i: None, r_t: [-961.678 -961.678 -961.678], eps: 0.01})
Step:  118700, Reward: [-402.592 -402.592 -402.592] [58.338], Avg: [-462.773 -462.773 -462.773] (0.0100) ({r_i: None, r_t: [-781.515 -781.515 -781.515], eps: 0.01})
Step:   22500, Reward: [-417.927 -417.927 -417.927] [67.416], Avg: [-446.111 -446.111 -446.111] (0.1048) ({r_i: None, r_t: [-868.163 -868.163 -868.163], eps: 0.105})
Step:   54900, Reward: [-480.966 -480.966 -480.966] [73.054], Avg: [-458.496 -458.496 -458.496] (0.0100) ({r_i: None, r_t: [-934.157 -934.157 -934.157], eps: 0.01})
Step:  118800, Reward: [-424.142 -424.142 -424.142] [76.129], Avg: [-462.740 -462.740 -462.740] (0.0100) ({r_i: None, r_t: [-832.389 -832.389 -832.389], eps: 0.01})
Step:   22600, Reward: [-451.738 -451.738 -451.738] [66.093], Avg: [-446.136 -446.136 -446.136] (0.1038) ({r_i: None, r_t: [-844.767 -844.767 -844.767], eps: 0.104})
Step:   55000, Reward: [-473.655 -473.655 -473.655] [83.323], Avg: [-458.524 -458.524 -458.524] (0.0100) ({r_i: None, r_t: [-976.763 -976.763 -976.763], eps: 0.01})
Step:  118900, Reward: [-357.717 -357.717 -357.717] [64.742], Avg: [-462.652 -462.652 -462.652] (0.0100) ({r_i: None, r_t: [-872.652 -872.652 -872.652], eps: 0.01})
Step:   22700, Reward: [-440.337 -440.337 -440.337] [109.386], Avg: [-446.110 -446.110 -446.110] (0.1027) ({r_i: None, r_t: [-827.168 -827.168 -827.168], eps: 0.103})
Step:  119000, Reward: [-399.096 -399.096 -399.096] [96.412], Avg: [-462.599 -462.599 -462.599] (0.0100) ({r_i: None, r_t: [-816.922 -816.922 -816.922], eps: 0.01})
Step:   55100, Reward: [-485.116 -485.116 -485.116] [68.155], Avg: [-458.572 -458.572 -458.572] (0.0100) ({r_i: None, r_t: [-963.350 -963.350 -963.350], eps: 0.01})
Step:  119100, Reward: [-398.715 -398.715 -398.715] [105.617], Avg: [-462.545 -462.545 -462.545] (0.0100) ({r_i: None, r_t: [-864.867 -864.867 -864.867], eps: 0.01})
Step:   22800, Reward: [-401.219 -401.219 -401.219] [59.075], Avg: [-445.914 -445.914 -445.914] (0.1017) ({r_i: None, r_t: [-802.856 -802.856 -802.856], eps: 0.102})
Step:   55200, Reward: [-506.349 -506.349 -506.349] [71.060], Avg: [-458.658 -458.658 -458.658] (0.0100) ({r_i: None, r_t: [-940.650 -940.650 -940.650], eps: 0.01})
Step:  119200, Reward: [-428.214 -428.214 -428.214] [104.406], Avg: [-462.516 -462.516 -462.516] (0.0100) ({r_i: None, r_t: [-793.381 -793.381 -793.381], eps: 0.01})
Step:   22900, Reward: [-424.565 -424.565 -424.565] [64.895], Avg: [-445.821 -445.821 -445.821] (0.1007) ({r_i: None, r_t: [-828.827 -828.827 -828.827], eps: 0.101})
Step:   55300, Reward: [-512.470 -512.470 -512.470] [75.102], Avg: [-458.756 -458.756 -458.756] (0.0100) ({r_i: None, r_t: [-971.234 -971.234 -971.234], eps: 0.01})
Step:  119300, Reward: [-420.799 -420.799 -420.799] [109.305], Avg: [-462.482 -462.482 -462.482] (0.0100) ({r_i: None, r_t: [-844.069 -844.069 -844.069], eps: 0.01})
Step:   23000, Reward: [-373.356 -373.356 -373.356] [56.763], Avg: [-445.508 -445.508 -445.508] (0.1000) ({r_i: None, r_t: [-859.982 -859.982 -859.982], eps: 0.1})
Step:  119400, Reward: [-370.971 -370.971 -370.971] [70.083], Avg: [-462.405 -462.405 -462.405] (0.0100) ({r_i: None, r_t: [-841.705 -841.705 -841.705], eps: 0.01})
Step:   55400, Reward: [-493.457 -493.457 -493.457] [65.841], Avg: [-458.818 -458.818 -458.818] (0.0100) ({r_i: None, r_t: [-967.051 -967.051 -967.051], eps: 0.01})
Step:  119500, Reward: [-400.980 -400.980 -400.980] [61.593], Avg: [-462.354 -462.354 -462.354] (0.0100) ({r_i: None, r_t: [-849.090 -849.090 -849.090], eps: 0.01})
Step:   23100, Reward: [-402.945 -402.945 -402.945] [84.020], Avg: [-445.324 -445.324 -445.324] (0.1000) ({r_i: None, r_t: [-811.812 -811.812 -811.812], eps: 0.1})
Step:   55500, Reward: [-496.872 -496.872 -496.872] [88.745], Avg: [-458.886 -458.886 -458.886] (0.0100) ({r_i: None, r_t: [-961.362 -961.362 -961.362], eps: 0.01})
Step:  119600, Reward: [-441.895 -441.895 -441.895] [124.835], Avg: [-462.337 -462.337 -462.337] (0.0100) ({r_i: None, r_t: [-757.033 -757.033 -757.033], eps: 0.01})
Step:   23200, Reward: [-409.904 -409.904 -409.904] [52.826], Avg: [-445.172 -445.172 -445.172] (0.1000) ({r_i: None, r_t: [-822.423 -822.423 -822.423], eps: 0.1})
Step:   55600, Reward: [-487.571 -487.571 -487.571] [88.609], Avg: [-458.938 -458.938 -458.938] (0.0100) ({r_i: None, r_t: [-975.428 -975.428 -975.428], eps: 0.01})
Step:  119700, Reward: [-424.886 -424.886 -424.886] [75.015], Avg: [-462.305 -462.305 -462.305] (0.0100) ({r_i: None, r_t: [-783.473 -783.473 -783.473], eps: 0.01})
Step:   23300, Reward: [-404.860 -404.860 -404.860] [55.700], Avg: [-445.000 -445.000 -445.000] (0.1000) ({r_i: None, r_t: [-851.015 -851.015 -851.015], eps: 0.1})
Step:  119800, Reward: [-393.132 -393.132 -393.132] [74.065], Avg: [-462.248 -462.248 -462.248] (0.0100) ({r_i: None, r_t: [-824.749 -824.749 -824.749], eps: 0.01})
Step:   55700, Reward: [-496.818 -496.818 -496.818] [74.270], Avg: [-459.006 -459.006 -459.006] (0.0100) ({r_i: None, r_t: [-931.022 -931.022 -931.022], eps: 0.01})
Step:  119900, Reward: [-396.151 -396.151 -396.151] [83.300], Avg: [-462.192 -462.192 -462.192] (0.0100) ({r_i: None, r_t: [-799.094 -799.094 -799.094], eps: 0.01})
Step:   23400, Reward: [-403.458 -403.458 -403.458] [72.382], Avg: [-444.823 -444.823 -444.823] (0.1000) ({r_i: None, r_t: [-835.094 -835.094 -835.094], eps: 0.1})
Step:   55800, Reward: [-459.797 -459.797 -459.797] [77.902], Avg: [-459.007 -459.007 -459.007] (0.0100) ({r_i: None, r_t: [-961.115 -961.115 -961.115], eps: 0.01})
Step:  120000, Reward: [-413.658 -413.658 -413.658] [84.053], Avg: [-462.152 -462.152 -462.152] (0.0100) ({r_i: None, r_t: [-779.398 -779.398 -779.398], eps: 0.01})
Step:   23500, Reward: [-417.645 -417.645 -417.645] [96.035], Avg: [-444.708 -444.708 -444.708] (0.1000) ({r_i: None, r_t: [-826.600 -826.600 -826.600], eps: 0.1})
Step:   55900, Reward: [-504.767 -504.767 -504.767] [93.098], Avg: [-459.089 -459.089 -459.089] (0.0100) ({r_i: None, r_t: [-915.979 -915.979 -915.979], eps: 0.01})
Step:  120100, Reward: [-387.671 -387.671 -387.671] [82.193], Avg: [-462.090 -462.090 -462.090] (0.0100) ({r_i: None, r_t: [-853.368 -853.368 -853.368], eps: 0.01})
Step:   23600, Reward: [-384.921 -384.921 -384.921] [51.422], Avg: [-444.456 -444.456 -444.456] (0.1000) ({r_i: None, r_t: [-845.859 -845.859 -845.859], eps: 0.1})
Step:  120200, Reward: [-399.537 -399.537 -399.537] [76.327], Avg: [-462.038 -462.038 -462.038] (0.0100) ({r_i: None, r_t: [-836.030 -836.030 -836.030], eps: 0.01})
Step:   56000, Reward: [-454.425 -454.425 -454.425] [50.366], Avg: [-459.081 -459.081 -459.081] (0.0100) ({r_i: None, r_t: [-940.829 -940.829 -940.829], eps: 0.01})
Step:  120300, Reward: [-426.594 -426.594 -426.594] [74.894], Avg: [-462.009 -462.009 -462.009] (0.0100) ({r_i: None, r_t: [-777.930 -777.930 -777.930], eps: 0.01})
Step:   23700, Reward: [-411.740 -411.740 -411.740] [66.692], Avg: [-444.318 -444.318 -444.318] (0.1000) ({r_i: None, r_t: [-806.495 -806.495 -806.495], eps: 0.1})
Step:   56100, Reward: [-461.243 -461.243 -461.243] [78.370], Avg: [-459.085 -459.085 -459.085] (0.0100) ({r_i: None, r_t: [-982.580 -982.580 -982.580], eps: 0.01})
Step:  120400, Reward: [-362.441 -362.441 -362.441] [72.033], Avg: [-461.926 -461.926 -461.926] (0.0100) ({r_i: None, r_t: [-864.288 -864.288 -864.288], eps: 0.01})
Step:   23800, Reward: [-418.912 -418.912 -418.912] [104.862], Avg: [-444.212 -444.212 -444.212] (0.1000) ({r_i: None, r_t: [-840.754 -840.754 -840.754], eps: 0.1})
Step:   56200, Reward: [-475.038 -475.038 -475.038] [69.218], Avg: [-459.113 -459.113 -459.113] (0.0100) ({r_i: None, r_t: [-953.799 -953.799 -953.799], eps: 0.01})
Step:  120500, Reward: [-437.484 -437.484 -437.484] [80.073], Avg: [-461.906 -461.906 -461.906] (0.0100) ({r_i: None, r_t: [-870.736 -870.736 -870.736], eps: 0.01})
Step:  120600, Reward: [-420.155 -420.155 -420.155] [81.849], Avg: [-461.871 -461.871 -461.871] (0.0100) ({r_i: None, r_t: [-831.980 -831.980 -831.980], eps: 0.01})
Step:   23900, Reward: [-412.082 -412.082 -412.082] [66.724], Avg: [-444.078 -444.078 -444.078] (0.1000) ({r_i: None, r_t: [-817.791 -817.791 -817.791], eps: 0.1})
Step:   56300, Reward: [-511.753 -511.753 -511.753] [77.840], Avg: [-459.206 -459.206 -459.206] (0.0100) ({r_i: None, r_t: [-951.581 -951.581 -951.581], eps: 0.01})
Step:  120700, Reward: [-400.036 -400.036 -400.036] [88.871], Avg: [-461.820 -461.820 -461.820] (0.0100) ({r_i: None, r_t: [-926.611 -926.611 -926.611], eps: 0.01})
Step:   24000, Reward: [-404.873 -404.873 -404.873] [88.873], Avg: [-443.915 -443.915 -443.915] (0.1000) ({r_i: None, r_t: [-819.842 -819.842 -819.842], eps: 0.1})
Step:   56400, Reward: [-497.075 -497.075 -497.075] [75.401], Avg: [-459.273 -459.273 -459.273] (0.0100) ({r_i: None, r_t: [-953.119 -953.119 -953.119], eps: 0.01})
Step:  120800, Reward: [-408.831 -408.831 -408.831] [74.777], Avg: [-461.776 -461.776 -461.776] (0.0100) ({r_i: None, r_t: [-793.747 -793.747 -793.747], eps: 0.01})
Step:   24100, Reward: [-421.551 -421.551 -421.551] [62.149], Avg: [-443.823 -443.823 -443.823] (0.1000) ({r_i: None, r_t: [-777.758 -777.758 -777.758], eps: 0.1})
Step:   56500, Reward: [-477.125 -477.125 -477.125] [55.845], Avg: [-459.305 -459.305 -459.305] (0.0100) ({r_i: None, r_t: [-969.514 -969.514 -969.514], eps: 0.01})
Step:  120900, Reward: [-408.726 -408.726 -408.726] [118.590], Avg: [-461.732 -461.732 -461.732] (0.0100) ({r_i: None, r_t: [-805.291 -805.291 -805.291], eps: 0.01})
Step:  121000, Reward: [-457.923 -457.923 -457.923] [111.701], Avg: [-461.729 -461.729 -461.729] (0.0100) ({r_i: None, r_t: [-888.715 -888.715 -888.715], eps: 0.01})
Step:   24200, Reward: [-411.596 -411.596 -411.596] [57.854], Avg: [-443.690 -443.690 -443.690] (0.1000) ({r_i: None, r_t: [-807.506 -807.506 -807.506], eps: 0.1})
Step:   56600, Reward: [-459.434 -459.434 -459.434] [74.170], Avg: [-459.305 -459.305 -459.305] (0.0100) ({r_i: None, r_t: [-933.568 -933.568 -933.568], eps: 0.01})
Step:  121100, Reward: [-392.302 -392.302 -392.302] [87.029], Avg: [-461.672 -461.672 -461.672] (0.0100) ({r_i: None, r_t: [-871.355 -871.355 -871.355], eps: 0.01})
Step:   24300, Reward: [-395.902 -395.902 -395.902] [78.208], Avg: [-443.495 -443.495 -443.495] (0.1000) ({r_i: None, r_t: [-815.450 -815.450 -815.450], eps: 0.1})
Step:   56700, Reward: [-499.543 -499.543 -499.543] [65.455], Avg: [-459.376 -459.376 -459.376] (0.0100) ({r_i: None, r_t: [-965.902 -965.902 -965.902], eps: 0.01})
Step:  121200, Reward: [-413.130 -413.130 -413.130] [87.501], Avg: [-461.632 -461.632 -461.632] (0.0100) ({r_i: None, r_t: [-770.090 -770.090 -770.090], eps: 0.01})
Step:   24400, Reward: [-415.451 -415.451 -415.451] [87.424], Avg: [-443.380 -443.380 -443.380] (0.1000) ({r_i: None, r_t: [-829.459 -829.459 -829.459], eps: 0.1})
Step:   56800, Reward: [-506.391 -506.391 -506.391] [77.409], Avg: [-459.458 -459.458 -459.458] (0.0100) ({r_i: None, r_t: [-971.890 -971.890 -971.890], eps: 0.01})
Step:  121300, Reward: [-391.253 -391.253 -391.253] [105.250], Avg: [-461.574 -461.574 -461.574] (0.0100) ({r_i: None, r_t: [-769.777 -769.777 -769.777], eps: 0.01})
Step:  121400, Reward: [-410.004 -410.004 -410.004] [87.791], Avg: [-461.531 -461.531 -461.531] (0.0100) ({r_i: None, r_t: [-787.713 -787.713 -787.713], eps: 0.01})
Step:   24500, Reward: [-397.583 -397.583 -397.583] [62.202], Avg: [-443.194 -443.194 -443.194] (0.1000) ({r_i: None, r_t: [-810.063 -810.063 -810.063], eps: 0.1})
Step:   56900, Reward: [-487.905 -487.905 -487.905] [71.257], Avg: [-459.508 -459.508 -459.508] (0.0100) ({r_i: None, r_t: [-992.285 -992.285 -992.285], eps: 0.01})
Step:  121500, Reward: [-433.586 -433.586 -433.586] [58.478], Avg: [-461.508 -461.508 -461.508] (0.0100) ({r_i: None, r_t: [-812.196 -812.196 -812.196], eps: 0.01})
Step:   24600, Reward: [-381.839 -381.839 -381.839] [65.315], Avg: [-442.946 -442.946 -442.946] (0.1000) ({r_i: None, r_t: [-801.406 -801.406 -801.406], eps: 0.1})
Step:   57000, Reward: [-518.737 -518.737 -518.737] [83.512], Avg: [-459.612 -459.612 -459.612] (0.0100) ({r_i: None, r_t: [-987.701 -987.701 -987.701], eps: 0.01})
Step:  121600, Reward: [-424.162 -424.162 -424.162] [65.288], Avg: [-461.478 -461.478 -461.478] (0.0100) ({r_i: None, r_t: [-848.571 -848.571 -848.571], eps: 0.01})
Step:   24700, Reward: [-421.543 -421.543 -421.543] [101.438], Avg: [-442.859 -442.859 -442.859] (0.1000) ({r_i: None, r_t: [-792.268 -792.268 -792.268], eps: 0.1})
Step:   57100, Reward: [-495.354 -495.354 -495.354] [79.695], Avg: [-459.675 -459.675 -459.675] (0.0100) ({r_i: None, r_t: [-955.051 -955.051 -955.051], eps: 0.01})
Step:  121700, Reward: [-450.880 -450.880 -450.880] [80.362], Avg: [-461.469 -461.469 -461.469] (0.0100) ({r_i: None, r_t: [-804.537 -804.537 -804.537], eps: 0.01})
Step:  121800, Reward: [-387.722 -387.722 -387.722] [89.549], Avg: [-461.409 -461.409 -461.409] (0.0100) ({r_i: None, r_t: [-786.574 -786.574 -786.574], eps: 0.01})
Step:   24800, Reward: [-376.143 -376.143 -376.143] [59.592], Avg: [-442.591 -442.591 -442.591] (0.1000) ({r_i: None, r_t: [-782.770 -782.770 -782.770], eps: 0.1})
Step:   57200, Reward: [-483.745 -483.745 -483.745] [72.344], Avg: [-459.717 -459.717 -459.717] (0.0100) ({r_i: None, r_t: [-956.477 -956.477 -956.477], eps: 0.01})
Step:  121900, Reward: [-436.831 -436.831 -436.831] [71.416], Avg: [-461.388 -461.388 -461.388] (0.0100) ({r_i: None, r_t: [-933.053 -933.053 -933.053], eps: 0.01})
Step:   24900, Reward: [-408.026 -408.026 -408.026] [67.387], Avg: [-442.453 -442.453 -442.453] (0.1000) ({r_i: None, r_t: [-830.303 -830.303 -830.303], eps: 0.1})
Step:   57300, Reward: [-475.748 -475.748 -475.748] [69.545], Avg: [-459.745 -459.745 -459.745] (0.0100) ({r_i: None, r_t: [-942.256 -942.256 -942.256], eps: 0.01})
Step:  122000, Reward: [-431.212 -431.212 -431.212] [92.217], Avg: [-461.364 -461.364 -461.364] (0.0100) ({r_i: None, r_t: [-797.037 -797.037 -797.037], eps: 0.01})
Step:   25000, Reward: [-434.258 -434.258 -434.258] [68.622], Avg: [-442.420 -442.420 -442.420] (0.1000) ({r_i: None, r_t: [-848.491 -848.491 -848.491], eps: 0.1})
Step:   57400, Reward: [-472.760 -472.760 -472.760] [75.177], Avg: [-459.767 -459.767 -459.767] (0.0100) ({r_i: None, r_t: [-953.307 -953.307 -953.307], eps: 0.01})
Step:  122100, Reward: [-406.379 -406.379 -406.379] [84.160], Avg: [-461.319 -461.319 -461.319] (0.0100) ({r_i: None, r_t: [-841.220 -841.220 -841.220], eps: 0.01})
Step:  122200, Reward: [-438.121 -438.121 -438.121] [88.820], Avg: [-461.300 -461.300 -461.300] (0.0100) ({r_i: None, r_t: [-848.440 -848.440 -848.440], eps: 0.01})
Step:   25100, Reward: [-415.195 -415.195 -415.195] [66.392], Avg: [-442.312 -442.312 -442.312] (0.1000) ({r_i: None, r_t: [-829.512 -829.512 -829.512], eps: 0.1})
Step:   57500, Reward: [-506.471 -506.471 -506.471] [60.603], Avg: [-459.848 -459.848 -459.848] (0.0100) ({r_i: None, r_t: [-996.370 -996.370 -996.370], eps: 0.01})
Step:  122300, Reward: [-382.464 -382.464 -382.464] [76.633], Avg: [-461.235 -461.235 -461.235] (0.0100) ({r_i: None, r_t: [-809.035 -809.035 -809.035], eps: 0.01})
Step:   25200, Reward: [-387.000 -387.000 -387.000] [68.409], Avg: [-442.094 -442.094 -442.094] (0.1000) ({r_i: None, r_t: [-815.313 -815.313 -815.313], eps: 0.1})
Step:   57600, Reward: [-451.322 -451.322 -451.322] [64.639], Avg: [-459.833 -459.833 -459.833] (0.0100) ({r_i: None, r_t: [-963.037 -963.037 -963.037], eps: 0.01})
Step:  122400, Reward: [-453.163 -453.163 -453.163] [118.619], Avg: [-461.229 -461.229 -461.229] (0.0100) ({r_i: None, r_t: [-786.431 -786.431 -786.431], eps: 0.01})
Step:   25300, Reward: [-398.638 -398.638 -398.638] [63.221], Avg: [-441.923 -441.923 -441.923] (0.1000) ({r_i: None, r_t: [-782.865 -782.865 -782.865], eps: 0.1})
Step:  122500, Reward: [-400.407 -400.407 -400.407] [89.976], Avg: [-461.179 -461.179 -461.179] (0.0100) ({r_i: None, r_t: [-878.139 -878.139 -878.139], eps: 0.01})
Step:   57700, Reward: [-462.957 -462.957 -462.957] [57.760], Avg: [-459.839 -459.839 -459.839] (0.0100) ({r_i: None, r_t: [-983.767 -983.767 -983.767], eps: 0.01})
Step:  122600, Reward: [-416.252 -416.252 -416.252] [94.807], Avg: [-461.143 -461.143 -461.143] (0.0100) ({r_i: None, r_t: [-853.526 -853.526 -853.526], eps: 0.01})
Step:   25400, Reward: [-417.967 -417.967 -417.967] [104.906], Avg: [-441.829 -441.829 -441.829] (0.1000) ({r_i: None, r_t: [-830.429 -830.429 -830.429], eps: 0.1})
Step:   57800, Reward: [-481.388 -481.388 -481.388] [67.101], Avg: [-459.876 -459.876 -459.876] (0.0100) ({r_i: None, r_t: [-957.378 -957.378 -957.378], eps: 0.01})
Step:  122700, Reward: [-419.344 -419.344 -419.344] [84.314], Avg: [-461.109 -461.109 -461.109] (0.0100) ({r_i: None, r_t: [-783.086 -783.086 -783.086], eps: 0.01})
Step:   25500, Reward: [-427.692 -427.692 -427.692] [82.351], Avg: [-441.773 -441.773 -441.773] (0.1000) ({r_i: None, r_t: [-851.192 -851.192 -851.192], eps: 0.1})
Step:   57900, Reward: [-478.408 -478.408 -478.408] [83.661], Avg: [-459.908 -459.908 -459.908] (0.0100) ({r_i: None, r_t: [-965.610 -965.610 -965.610], eps: 0.01})
Step:  122800, Reward: [-440.068 -440.068 -440.068] [92.732], Avg: [-461.091 -461.091 -461.091] (0.0100) ({r_i: None, r_t: [-847.555 -847.555 -847.555], eps: 0.01})
Step:   25600, Reward: [-439.543 -439.543 -439.543] [72.018], Avg: [-441.765 -441.765 -441.765] (0.1000) ({r_i: None, r_t: [-829.165 -829.165 -829.165], eps: 0.1})
Step:  122900, Reward: [-409.015 -409.015 -409.015] [79.702], Avg: [-461.049 -461.049 -461.049] (0.0100) ({r_i: None, r_t: [-878.364 -878.364 -878.364], eps: 0.01})
Step:   58000, Reward: [-512.216 -512.216 -512.216] [91.444], Avg: [-459.998 -459.998 -459.998] (0.0100) ({r_i: None, r_t: [-977.209 -977.209 -977.209], eps: 0.01})
Step:  123000, Reward: [-406.657 -406.657 -406.657] [63.540], Avg: [-461.005 -461.005 -461.005] (0.0100) ({r_i: None, r_t: [-825.051 -825.051 -825.051], eps: 0.01})
Step:   25700, Reward: [-413.153 -413.153 -413.153] [84.326], Avg: [-441.654 -441.654 -441.654] (0.1000) ({r_i: None, r_t: [-821.380 -821.380 -821.380], eps: 0.1})
Step:   58100, Reward: [-505.167 -505.167 -505.167] [68.615], Avg: [-460.076 -460.076 -460.076] (0.0100) ({r_i: None, r_t: [-1011.656 -1011.656 -1011.656], eps: 0.01})
Step:  123100, Reward: [-420.738 -420.738 -420.738] [98.937], Avg: [-460.972 -460.972 -460.972] (0.0100) ({r_i: None, r_t: [-814.626 -814.626 -814.626], eps: 0.01})
Step:   25800, Reward: [-383.249 -383.249 -383.249] [85.048], Avg: [-441.428 -441.428 -441.428] (0.1000) ({r_i: None, r_t: [-799.998 -799.998 -799.998], eps: 0.1})
Step:   58200, Reward: [-488.019 -488.019 -488.019] [99.158], Avg: [-460.124 -460.124 -460.124] (0.0100) ({r_i: None, r_t: [-952.099 -952.099 -952.099], eps: 0.01})
Step:  123200, Reward: [-455.188 -455.188 -455.188] [82.670], Avg: [-460.967 -460.967 -460.967] (0.0100) ({r_i: None, r_t: [-785.824 -785.824 -785.824], eps: 0.01})
Step:   25900, Reward: [-403.694 -403.694 -403.694] [97.117], Avg: [-441.283 -441.283 -441.283] (0.1000) ({r_i: None, r_t: [-789.498 -789.498 -789.498], eps: 0.1})
Step:  123300, Reward: [-399.174 -399.174 -399.174] [57.883], Avg: [-460.917 -460.917 -460.917] (0.0100) ({r_i: None, r_t: [-852.705 -852.705 -852.705], eps: 0.01})
Step:   58300, Reward: [-470.247 -470.247 -470.247] [69.733], Avg: [-460.141 -460.141 -460.141] (0.0100) ({r_i: None, r_t: [-974.122 -974.122 -974.122], eps: 0.01})
Step:  123400, Reward: [-412.699 -412.699 -412.699] [70.730], Avg: [-460.878 -460.878 -460.878] (0.0100) ({r_i: None, r_t: [-840.268 -840.268 -840.268], eps: 0.01})
Step:   26000, Reward: [-410.600 -410.600 -410.600] [67.289], Avg: [-441.166 -441.166 -441.166] (0.1000) ({r_i: None, r_t: [-824.446 -824.446 -824.446], eps: 0.1})
Step:   58400, Reward: [-486.850 -486.850 -486.850] [81.391], Avg: [-460.187 -460.187 -460.187] (0.0100) ({r_i: None, r_t: [-975.598 -975.598 -975.598], eps: 0.01})
Step:  123500, Reward: [-404.142 -404.142 -404.142] [69.904], Avg: [-460.832 -460.832 -460.832] (0.0100) ({r_i: None, r_t: [-828.900 -828.900 -828.900], eps: 0.01})
Step:   26100, Reward: [-411.773 -411.773 -411.773] [80.554], Avg: [-441.053 -441.053 -441.053] (0.1000) ({r_i: None, r_t: [-800.820 -800.820 -800.820], eps: 0.1})
Step:   58500, Reward: [-464.553 -464.553 -464.553] [69.351], Avg: [-460.194 -460.194 -460.194] (0.0100) ({r_i: None, r_t: [-984.921 -984.921 -984.921], eps: 0.01})
Step:  123600, Reward: [-371.939 -371.939 -371.939] [55.715], Avg: [-460.761 -460.761 -460.761] (0.0100) ({r_i: None, r_t: [-879.349 -879.349 -879.349], eps: 0.01})
Step:   26200, Reward: [-396.411 -396.411 -396.411] [76.103], Avg: [-440.884 -440.884 -440.884] (0.1000) ({r_i: None, r_t: [-865.441 -865.441 -865.441], eps: 0.1})
Step:  123700, Reward: [-393.250 -393.250 -393.250] [54.160], Avg: [-460.706 -460.706 -460.706] (0.0100) ({r_i: None, r_t: [-802.864 -802.864 -802.864], eps: 0.01})
Step:   58600, Reward: [-496.657 -496.657 -496.657] [59.658], Avg: [-460.256 -460.256 -460.256] (0.0100) ({r_i: None, r_t: [-986.587 -986.587 -986.587], eps: 0.01})
Step:  123800, Reward: [-400.638 -400.638 -400.638] [70.526], Avg: [-460.658 -460.658 -460.658] (0.0100) ({r_i: None, r_t: [-770.421 -770.421 -770.421], eps: 0.01})
Step:   26300, Reward: [-413.530 -413.530 -413.530] [58.602], Avg: [-440.780 -440.780 -440.780] (0.1000) ({r_i: None, r_t: [-815.695 -815.695 -815.695], eps: 0.1})
Step:   58700, Reward: [-486.750 -486.750 -486.750] [84.156], Avg: [-460.301 -460.301 -460.301] (0.0100) ({r_i: None, r_t: [-959.203 -959.203 -959.203], eps: 0.01})
Step:  123900, Reward: [-421.333 -421.333 -421.333] [120.389], Avg: [-460.626 -460.626 -460.626] (0.0100) ({r_i: None, r_t: [-819.632 -819.632 -819.632], eps: 0.01})
Step:   26400, Reward: [-424.849 -424.849 -424.849] [70.846], Avg: [-440.720 -440.720 -440.720] (0.1000) ({r_i: None, r_t: [-836.973 -836.973 -836.973], eps: 0.1})
Step:   58800, Reward: [-457.706 -457.706 -457.706] [71.787], Avg: [-460.297 -460.297 -460.297] (0.0100) ({r_i: None, r_t: [-1037.083 -1037.083 -1037.083], eps: 0.01})
Step:  124000, Reward: [-408.558 -408.558 -408.558] [77.472], Avg: [-460.584 -460.584 -460.584] (0.0100) ({r_i: None, r_t: [-816.737 -816.737 -816.737], eps: 0.01})
Step:   26500, Reward: [-404.671 -404.671 -404.671] [67.772], Avg: [-440.584 -440.584 -440.584] (0.1000) ({r_i: None, r_t: [-833.840 -833.840 -833.840], eps: 0.1})
Step:  124100, Reward: [-404.529 -404.529 -404.529] [87.911], Avg: [-460.539 -460.539 -460.539] (0.0100) ({r_i: None, r_t: [-813.422 -813.422 -813.422], eps: 0.01})
Step:   58900, Reward: [-476.464 -476.464 -476.464] [63.460], Avg: [-460.324 -460.324 -460.324] (0.0100) ({r_i: None, r_t: [-1030.565 -1030.565 -1030.565], eps: 0.01})
Step:  124200, Reward: [-430.966 -430.966 -430.966] [63.560], Avg: [-460.515 -460.515 -460.515] (0.0100) ({r_i: None, r_t: [-839.196 -839.196 -839.196], eps: 0.01})
Step:   26600, Reward: [-445.186 -445.186 -445.186] [65.695], Avg: [-440.602 -440.602 -440.602] (0.1000) ({r_i: None, r_t: [-801.493 -801.493 -801.493], eps: 0.1})
Step:   59000, Reward: [-489.188 -489.188 -489.188] [73.934], Avg: [-460.373 -460.373 -460.373] (0.0100) ({r_i: None, r_t: [-996.075 -996.075 -996.075], eps: 0.01})
Step:  124300, Reward: [-419.442 -419.442 -419.442] [95.173], Avg: [-460.482 -460.482 -460.482] (0.0100) ({r_i: None, r_t: [-887.109 -887.109 -887.109], eps: 0.01})
Step:   26700, Reward: [-394.956 -394.956 -394.956] [65.703], Avg: [-440.431 -440.431 -440.431] (0.1000) ({r_i: None, r_t: [-846.278 -846.278 -846.278], eps: 0.1})
Step:   59100, Reward: [-468.273 -468.273 -468.273] [72.754], Avg: [-460.386 -460.386 -460.386] (0.0100) ({r_i: None, r_t: [-998.839 -998.839 -998.839], eps: 0.01})
Step:  124400, Reward: [-427.913 -427.913 -427.913] [58.670], Avg: [-460.456 -460.456 -460.456] (0.0100) ({r_i: None, r_t: [-823.867 -823.867 -823.867], eps: 0.01})
Step:   26800, Reward: [-429.886 -429.886 -429.886] [74.369], Avg: [-440.392 -440.392 -440.392] (0.1000) ({r_i: None, r_t: [-788.996 -788.996 -788.996], eps: 0.1})
Step:  124500, Reward: [-396.579 -396.579 -396.579] [71.692], Avg: [-460.405 -460.405 -460.405] (0.0100) ({r_i: None, r_t: [-885.417 -885.417 -885.417], eps: 0.01})
Step:   59200, Reward: [-487.123 -487.123 -487.123] [73.809], Avg: [-460.431 -460.431 -460.431] (0.0100) ({r_i: None, r_t: [-947.618 -947.618 -947.618], eps: 0.01})
Step:  124600, Reward: [-401.795 -401.795 -401.795] [73.351], Avg: [-460.358 -460.358 -460.358] (0.0100) ({r_i: None, r_t: [-869.873 -869.873 -869.873], eps: 0.01})
Step:   26900, Reward: [-403.344 -403.344 -403.344] [84.567], Avg: [-440.255 -440.255 -440.255] (0.1000) ({r_i: None, r_t: [-820.328 -820.328 -820.328], eps: 0.1})
Step:   59300, Reward: [-520.556 -520.556 -520.556] [78.500], Avg: [-460.533 -460.533 -460.533] (0.0100) ({r_i: None, r_t: [-1021.984 -1021.984 -1021.984], eps: 0.01})
Step:  124700, Reward: [-407.456 -407.456 -407.456] [64.575], Avg: [-460.315 -460.315 -460.315] (0.0100) ({r_i: None, r_t: [-845.798 -845.798 -845.798], eps: 0.01})
Step:   27000, Reward: [-397.296 -397.296 -397.296] [59.182], Avg: [-440.096 -440.096 -440.096] (0.1000) ({r_i: None, r_t: [-817.518 -817.518 -817.518], eps: 0.1})
Step:   59400, Reward: [-480.095 -480.095 -480.095] [52.333], Avg: [-460.566 -460.566 -460.566] (0.0100) ({r_i: None, r_t: [-1005.805 -1005.805 -1005.805], eps: 0.01})
Step:  124800, Reward: [-378.971 -378.971 -378.971] [93.435], Avg: [-460.250 -460.250 -460.250] (0.0100) ({r_i: None, r_t: [-830.443 -830.443 -830.443], eps: 0.01})
Step:   27100, Reward: [-369.791 -369.791 -369.791] [73.294], Avg: [-439.838 -439.838 -439.838] (0.1000) ({r_i: None, r_t: [-859.633 -859.633 -859.633], eps: 0.1})
Step:  124900, Reward: [-385.575 -385.575 -385.575] [59.493], Avg: [-460.190 -460.190 -460.190] (0.0100) ({r_i: None, r_t: [-851.926 -851.926 -851.926], eps: 0.01})
Step:   59500, Reward: [-464.053 -464.053 -464.053] [66.650], Avg: [-460.571 -460.571 -460.571] (0.0100) ({r_i: None, r_t: [-979.527 -979.527 -979.527], eps: 0.01})
Step:  125000, Reward: [-423.385 -423.385 -423.385] [75.268], Avg: [-460.161 -460.161 -460.161] (0.0100) ({r_i: None, r_t: [-812.893 -812.893 -812.893], eps: 0.01})
Step:   27200, Reward: [-428.865 -428.865 -428.865] [76.194], Avg: [-439.798 -439.798 -439.798] (0.1000) ({r_i: None, r_t: [-834.968 -834.968 -834.968], eps: 0.1})
Step:   59600, Reward: [-507.063 -507.063 -507.063] [73.798], Avg: [-460.649 -460.649 -460.649] (0.0100) ({r_i: None, r_t: [-957.759 -957.759 -957.759], eps: 0.01})
Step:  125100, Reward: [-411.372 -411.372 -411.372] [119.095], Avg: [-460.122 -460.122 -460.122] (0.0100) ({r_i: None, r_t: [-831.199 -831.199 -831.199], eps: 0.01})
Step:   27300, Reward: [-402.506 -402.506 -402.506] [69.278], Avg: [-439.662 -439.662 -439.662] (0.1000) ({r_i: None, r_t: [-796.482 -796.482 -796.482], eps: 0.1})
Step:   59700, Reward: [-479.412 -479.412 -479.412] [62.315], Avg: [-460.681 -460.681 -460.681] (0.0100) ({r_i: None, r_t: [-993.328 -993.328 -993.328], eps: 0.01})
Step:  125200, Reward: [-421.325 -421.325 -421.325] [68.772], Avg: [-460.091 -460.091 -460.091] (0.0100) ({r_i: None, r_t: [-763.339 -763.339 -763.339], eps: 0.01})
Step:   27400, Reward: [-418.913 -418.913 -418.913] [69.029], Avg: [-439.586 -439.586 -439.586] (0.1000) ({r_i: None, r_t: [-818.058 -818.058 -818.058], eps: 0.1})
Step:  125300, Reward: [-400.518 -400.518 -400.518] [80.662], Avg: [-460.043 -460.043 -460.043] (0.0100) ({r_i: None, r_t: [-815.502 -815.502 -815.502], eps: 0.01})
Step:   59800, Reward: [-499.826 -499.826 -499.826] [50.481], Avg: [-460.746 -460.746 -460.746] (0.0100) ({r_i: None, r_t: [-964.681 -964.681 -964.681], eps: 0.01})
Step:  125400, Reward: [-390.082 -390.082 -390.082] [86.821], Avg: [-459.988 -459.988 -459.988] (0.0100) ({r_i: None, r_t: [-875.283 -875.283 -875.283], eps: 0.01})
Step:   27500, Reward: [-407.814 -407.814 -407.814] [65.277], Avg: [-439.471 -439.471 -439.471] (0.1000) ({r_i: None, r_t: [-825.559 -825.559 -825.559], eps: 0.1})
Step:   59900, Reward: [-505.548 -505.548 -505.548] [56.841], Avg: [-460.821 -460.821 -460.821] (0.0100) ({r_i: None, r_t: [-988.916 -988.916 -988.916], eps: 0.01})
Step:  125500, Reward: [-432.851 -432.851 -432.851] [71.431], Avg: [-459.966 -459.966 -459.966] (0.0100) ({r_i: None, r_t: [-816.927 -816.927 -816.927], eps: 0.01})
Step:   27600, Reward: [-401.993 -401.993 -401.993] [49.661], Avg: [-439.336 -439.336 -439.336] (0.1000) ({r_i: None, r_t: [-770.028 -770.028 -770.028], eps: 0.1})
Step:   60000, Reward: [-476.418 -476.418 -476.418] [61.045], Avg: [-460.847 -460.847 -460.847] (0.0100) ({r_i: None, r_t: [-978.467 -978.467 -978.467], eps: 0.01})
Step:  125600, Reward: [-413.265 -413.265 -413.265] [57.072], Avg: [-459.929 -459.929 -459.929] (0.0100) ({r_i: None, r_t: [-784.739 -784.739 -784.739], eps: 0.01})
Step:   27700, Reward: [-423.534 -423.534 -423.534] [83.300], Avg: [-439.279 -439.279 -439.279] (0.1000) ({r_i: None, r_t: [-850.375 -850.375 -850.375], eps: 0.1})
Step:  125700, Reward: [-404.974 -404.974 -404.974] [69.342], Avg: [-459.885 -459.885 -459.885] (0.0100) ({r_i: None, r_t: [-820.236 -820.236 -820.236], eps: 0.01})
Step:   60100, Reward: [-468.662 -468.662 -468.662] [53.374], Avg: [-460.860 -460.860 -460.860] (0.0100) ({r_i: None, r_t: [-941.194 -941.194 -941.194], eps: 0.01})
Step:  125800, Reward: [-404.065 -404.065 -404.065] [75.715], Avg: [-459.841 -459.841 -459.841] (0.0100) ({r_i: None, r_t: [-827.074 -827.074 -827.074], eps: 0.01})
Step:   27800, Reward: [-407.836 -407.836 -407.836] [82.876], Avg: [-439.166 -439.166 -439.166] (0.1000) ({r_i: None, r_t: [-784.663 -784.663 -784.663], eps: 0.1})
Step:   60200, Reward: [-525.730 -525.730 -525.730] [56.118], Avg: [-460.967 -460.967 -460.967] (0.0100) ({r_i: None, r_t: [-962.268 -962.268 -962.268], eps: 0.01})
Step:  125900, Reward: [-402.289 -402.289 -402.289] [67.669], Avg: [-459.795 -459.795 -459.795] (0.0100) ({r_i: None, r_t: [-888.830 -888.830 -888.830], eps: 0.01})
Step:   27900, Reward: [-416.632 -416.632 -416.632] [86.248], Avg: [-439.086 -439.086 -439.086] (0.1000) ({r_i: None, r_t: [-847.418 -847.418 -847.418], eps: 0.1})
Step:   60300, Reward: [-512.458 -512.458 -512.458] [81.334], Avg: [-461.052 -461.052 -461.052] (0.0100) ({r_i: None, r_t: [-978.491 -978.491 -978.491], eps: 0.01})
Step:  126000, Reward: [-411.972 -411.972 -411.972] [50.323], Avg: [-459.757 -459.757 -459.757] (0.0100) ({r_i: None, r_t: [-838.073 -838.073 -838.073], eps: 0.01})
Step:   28000, Reward: [-428.361 -428.361 -428.361] [81.398], Avg: [-439.048 -439.048 -439.048] (0.1000) ({r_i: None, r_t: [-842.627 -842.627 -842.627], eps: 0.1})
Step:  126100, Reward: [-453.319 -453.319 -453.319] [69.440], Avg: [-459.752 -459.752 -459.752] (0.0100) ({r_i: None, r_t: [-802.014 -802.014 -802.014], eps: 0.01})
Step:   60400, Reward: [-473.584 -473.584 -473.584] [74.228], Avg: [-461.073 -461.073 -461.073] (0.0100) ({r_i: None, r_t: [-974.783 -974.783 -974.783], eps: 0.01})
Step:  126200, Reward: [-416.255 -416.255 -416.255] [86.244], Avg: [-459.718 -459.718 -459.718] (0.0100) ({r_i: None, r_t: [-824.048 -824.048 -824.048], eps: 0.01})
Step:   28100, Reward: [-457.117 -457.117 -457.117] [112.439], Avg: [-439.112 -439.112 -439.112] (0.1000) ({r_i: None, r_t: [-850.065 -850.065 -850.065], eps: 0.1})
Step:   60500, Reward: [-487.643 -487.643 -487.643] [65.479], Avg: [-461.117 -461.117 -461.117] (0.0100) ({r_i: None, r_t: [-1010.327 -1010.327 -1010.327], eps: 0.01})
Step:  126300, Reward: [-474.554 -474.554 -474.554] [115.247], Avg: [-459.730 -459.730 -459.730] (0.0100) ({r_i: None, r_t: [-825.194 -825.194 -825.194], eps: 0.01})
Step:   28200, Reward: [-424.187 -424.187 -424.187] [94.730], Avg: [-439.059 -439.059 -439.059] (0.1000) ({r_i: None, r_t: [-813.561 -813.561 -813.561], eps: 0.1})
Step:  126400, Reward: [-419.513 -419.513 -419.513] [89.746], Avg: [-459.698 -459.698 -459.698] (0.0100) ({r_i: None, r_t: [-793.986 -793.986 -793.986], eps: 0.01})
Step:   60600, Reward: [-479.728 -479.728 -479.728] [63.638], Avg: [-461.148 -461.148 -461.148] (0.0100) ({r_i: None, r_t: [-943.902 -943.902 -943.902], eps: 0.01})
Step:  126500, Reward: [-437.178 -437.178 -437.178] [85.663], Avg: [-459.680 -459.680 -459.680] (0.0100) ({r_i: None, r_t: [-802.487 -802.487 -802.487], eps: 0.01})
Step:   28300, Reward: [-436.238 -436.238 -436.238] [121.750], Avg: [-439.049 -439.049 -439.049] (0.1000) ({r_i: None, r_t: [-802.654 -802.654 -802.654], eps: 0.1})
Step:   60700, Reward: [-490.590 -490.590 -490.590] [60.111], Avg: [-461.196 -461.196 -461.196] (0.0100) ({r_i: None, r_t: [-941.059 -941.059 -941.059], eps: 0.01})
Step:  126600, Reward: [-384.319 -384.319 -384.319] [66.233], Avg: [-459.620 -459.620 -459.620] (0.0100) ({r_i: None, r_t: [-840.737 -840.737 -840.737], eps: 0.01})
Step:   28400, Reward: [-402.932 -402.932 -402.932] [86.310], Avg: [-438.922 -438.922 -438.922] (0.1000) ({r_i: None, r_t: [-784.872 -784.872 -784.872], eps: 0.1})
Step:   60800, Reward: [-463.469 -463.469 -463.469] [79.392], Avg: [-461.200 -461.200 -461.200] (0.0100) ({r_i: None, r_t: [-930.552 -930.552 -930.552], eps: 0.01})
Step:  126700, Reward: [-431.154 -431.154 -431.154] [73.889], Avg: [-459.598 -459.598 -459.598] (0.0100) ({r_i: None, r_t: [-837.317 -837.317 -837.317], eps: 0.01})
Step:   28500, Reward: [-414.985 -414.985 -414.985] [99.957], Avg: [-438.839 -438.839 -438.839] (0.1000) ({r_i: None, r_t: [-826.620 -826.620 -826.620], eps: 0.1})
Step:  126800, Reward: [-431.251 -431.251 -431.251] [90.799], Avg: [-459.576 -459.576 -459.576] (0.0100) ({r_i: None, r_t: [-841.990 -841.990 -841.990], eps: 0.01})
Step:   60900, Reward: [-486.156 -486.156 -486.156] [89.031], Avg: [-461.241 -461.241 -461.241] (0.0100) ({r_i: None, r_t: [-953.112 -953.112 -953.112], eps: 0.01})
Step:  126900, Reward: [-388.603 -388.603 -388.603] [64.919], Avg: [-459.520 -459.520 -459.520] (0.0100) ({r_i: None, r_t: [-844.838 -844.838 -844.838], eps: 0.01})
Step:   28600, Reward: [-406.199 -406.199 -406.199] [63.599], Avg: [-438.725 -438.725 -438.725] (0.1000) ({r_i: None, r_t: [-852.759 -852.759 -852.759], eps: 0.1})
Step:   61000, Reward: [-489.397 -489.397 -489.397] [78.384], Avg: [-461.287 -461.287 -461.287] (0.0100) ({r_i: None, r_t: [-976.202 -976.202 -976.202], eps: 0.01})
Step:  127000, Reward: [-455.915 -455.915 -455.915] [116.575], Avg: [-459.517 -459.517 -459.517] (0.0100) ({r_i: None, r_t: [-805.413 -805.413 -805.413], eps: 0.01})
Step:   28700, Reward: [-423.006 -423.006 -423.006] [80.802], Avg: [-438.670 -438.670 -438.670] (0.1000) ({r_i: None, r_t: [-816.253 -816.253 -816.253], eps: 0.1})
Step:   61100, Reward: [-462.475 -462.475 -462.475] [60.574], Avg: [-461.289 -461.289 -461.289] (0.0100) ({r_i: None, r_t: [-917.154 -917.154 -917.154], eps: 0.01})
Step:  127100, Reward: [-371.201 -371.201 -371.201] [91.975], Avg: [-459.448 -459.448 -459.448] (0.0100) ({r_i: None, r_t: [-848.948 -848.948 -848.948], eps: 0.01})
Step:   28800, Reward: [-412.259 -412.259 -412.259] [63.764], Avg: [-438.579 -438.579 -438.579] (0.1000) ({r_i: None, r_t: [-800.270 -800.270 -800.270], eps: 0.1})
Step:  127200, Reward: [-404.922 -404.922 -404.922] [90.822], Avg: [-459.405 -459.405 -459.405] (0.0100) ({r_i: None, r_t: [-855.596 -855.596 -855.596], eps: 0.01})
Step:   61200, Reward: [-498.916 -498.916 -498.916] [73.917], Avg: [-461.350 -461.350 -461.350] (0.0100) ({r_i: None, r_t: [-992.941 -992.941 -992.941], eps: 0.01})
Step:  127300, Reward: [-410.623 -410.623 -410.623] [115.344], Avg: [-459.366 -459.366 -459.366] (0.0100) ({r_i: None, r_t: [-847.841 -847.841 -847.841], eps: 0.01})
Step:   28900, Reward: [-454.056 -454.056 -454.056] [98.991], Avg: [-438.632 -438.632 -438.632] (0.1000) ({r_i: None, r_t: [-894.720 -894.720 -894.720], eps: 0.1})
Step:   61300, Reward: [-464.589 -464.589 -464.589] [57.175], Avg: [-461.355 -461.355 -461.355] (0.0100) ({r_i: None, r_t: [-955.772 -955.772 -955.772], eps: 0.01})
Step:  127400, Reward: [-423.822 -423.822 -423.822] [107.937], Avg: [-459.339 -459.339 -459.339] (0.0100) ({r_i: None, r_t: [-808.537 -808.537 -808.537], eps: 0.01})
Step:   29000, Reward: [-430.290 -430.290 -430.290] [65.787], Avg: [-438.604 -438.604 -438.604] (0.1000) ({r_i: None, r_t: [-860.228 -860.228 -860.228], eps: 0.1})
Step:   61400, Reward: [-461.896 -461.896 -461.896] [65.401], Avg: [-461.356 -461.356 -461.356] (0.0100) ({r_i: None, r_t: [-935.695 -935.695 -935.695], eps: 0.01})
Step:  127500, Reward: [-404.201 -404.201 -404.201] [83.652], Avg: [-459.295 -459.295 -459.295] (0.0100) ({r_i: None, r_t: [-826.782 -826.782 -826.782], eps: 0.01})
Step:   29100, Reward: [-460.890 -460.890 -460.890] [128.698], Avg: [-438.680 -438.680 -438.680] (0.1000) ({r_i: None, r_t: [-872.109 -872.109 -872.109], eps: 0.1})
Step:  127600, Reward: [-411.506 -411.506 -411.506] [100.333], Avg: [-459.258 -459.258 -459.258] (0.0100) ({r_i: None, r_t: [-783.481 -783.481 -783.481], eps: 0.01})
Step:   61500, Reward: [-500.973 -500.973 -500.973] [67.080], Avg: [-461.421 -461.421 -461.421] (0.0100) ({r_i: None, r_t: [-997.306 -997.306 -997.306], eps: 0.01})
Step:  127700, Reward: [-402.620 -402.620 -402.620] [88.957], Avg: [-459.214 -459.214 -459.214] (0.0100) ({r_i: None, r_t: [-834.730 -834.730 -834.730], eps: 0.01})
Step:   29200, Reward: [-406.027 -406.027 -406.027] [61.816], Avg: [-438.569 -438.569 -438.569] (0.1000) ({r_i: None, r_t: [-890.757 -890.757 -890.757], eps: 0.1})
Step:   61600, Reward: [-477.007 -477.007 -477.007] [70.877], Avg: [-461.446 -461.446 -461.446] (0.0100) ({r_i: None, r_t: [-934.591 -934.591 -934.591], eps: 0.01})
Step:  127800, Reward: [-406.238 -406.238 -406.238] [76.499], Avg: [-459.172 -459.172 -459.172] (0.0100) ({r_i: None, r_t: [-847.240 -847.240 -847.240], eps: 0.01})
Step:   29300, Reward: [-426.153 -426.153 -426.153] [72.179], Avg: [-438.526 -438.526 -438.526] (0.1000) ({r_i: None, r_t: [-859.387 -859.387 -859.387], eps: 0.1})
Step:   61700, Reward: [-461.279 -461.279 -461.279] [62.104], Avg: [-461.446 -461.446 -461.446] (0.0100) ({r_i: None, r_t: [-926.067 -926.067 -926.067], eps: 0.01})
Step:  127900, Reward: [-386.505 -386.505 -386.505] [89.418], Avg: [-459.115 -459.115 -459.115] (0.0100) ({r_i: None, r_t: [-822.469 -822.469 -822.469], eps: 0.01})
Step:   29400, Reward: [-423.583 -423.583 -423.583] [74.167], Avg: [-438.476 -438.476 -438.476] (0.1000) ({r_i: None, r_t: [-837.661 -837.661 -837.661], eps: 0.1})
Step:  128000, Reward: [-400.609 -400.609 -400.609] [74.607], Avg: [-459.070 -459.070 -459.070] (0.0100) ({r_i: None, r_t: [-814.042 -814.042 -814.042], eps: 0.01})
Step:   61800, Reward: [-448.527 -448.527 -448.527] [51.510], Avg: [-461.425 -461.425 -461.425] (0.0100) ({r_i: None, r_t: [-966.606 -966.606 -966.606], eps: 0.01})
Step:  128100, Reward: [-377.737 -377.737 -377.737] [64.477], Avg: [-459.006 -459.006 -459.006] (0.0100) ({r_i: None, r_t: [-824.569 -824.569 -824.569], eps: 0.01})
Step:   29500, Reward: [-437.059 -437.059 -437.059] [73.279], Avg: [-438.471 -438.471 -438.471] (0.1000) ({r_i: None, r_t: [-864.237 -864.237 -864.237], eps: 0.1})
Step:   61900, Reward: [-448.814 -448.814 -448.814] [81.663], Avg: [-461.404 -461.404 -461.404] (0.0100) ({r_i: None, r_t: [-974.986 -974.986 -974.986], eps: 0.01})
Step:  128200, Reward: [-423.847 -423.847 -423.847] [135.024], Avg: [-458.979 -458.979 -458.979] (0.0100) ({r_i: None, r_t: [-837.429 -837.429 -837.429], eps: 0.01})
Step:   29600, Reward: [-418.842 -418.842 -418.842] [80.749], Avg: [-438.405 -438.405 -438.405] (0.1000) ({r_i: None, r_t: [-889.719 -889.719 -889.719], eps: 0.1})
Step:   62000, Reward: [-443.856 -443.856 -443.856] [59.671], Avg: [-461.376 -461.376 -461.376] (0.0100) ({r_i: None, r_t: [-934.464 -934.464 -934.464], eps: 0.01})
Step:  128300, Reward: [-393.782 -393.782 -393.782] [74.995], Avg: [-458.928 -458.928 -458.928] (0.0100) ({r_i: None, r_t: [-829.061 -829.061 -829.061], eps: 0.01})
Step:   29700, Reward: [-397.158 -397.158 -397.158] [85.953], Avg: [-438.266 -438.266 -438.266] (0.1000) ({r_i: None, r_t: [-856.082 -856.082 -856.082], eps: 0.1})
Step:  128400, Reward: [-412.630 -412.630 -412.630] [110.695], Avg: [-458.892 -458.892 -458.892] (0.0100) ({r_i: None, r_t: [-790.271 -790.271 -790.271], eps: 0.01})
Step:   62100, Reward: [-472.003 -472.003 -472.003] [87.671], Avg: [-461.393 -461.393 -461.393] (0.0100) ({r_i: None, r_t: [-946.212 -946.212 -946.212], eps: 0.01})
Step:  128500, Reward: [-435.596 -435.596 -435.596] [115.223], Avg: [-458.874 -458.874 -458.874] (0.0100) ({r_i: None, r_t: [-866.620 -866.620 -866.620], eps: 0.01})
Step:   29800, Reward: [-411.469 -411.469 -411.469] [85.689], Avg: [-438.177 -438.177 -438.177] (0.1000) ({r_i: None, r_t: [-831.022 -831.022 -831.022], eps: 0.1})
Step:   62200, Reward: [-492.832 -492.832 -492.832] [91.731], Avg: [-461.444 -461.444 -461.444] (0.0100) ({r_i: None, r_t: [-927.038 -927.038 -927.038], eps: 0.01})
Step:  128600, Reward: [-383.068 -383.068 -383.068] [88.955], Avg: [-458.815 -458.815 -458.815] (0.0100) ({r_i: None, r_t: [-862.114 -862.114 -862.114], eps: 0.01})
Step:   29900, Reward: [-456.470 -456.470 -456.470] [90.208], Avg: [-438.238 -438.238 -438.238] (0.1000) ({r_i: None, r_t: [-839.887 -839.887 -839.887], eps: 0.1})
Step:   62300, Reward: [-462.180 -462.180 -462.180] [82.115], Avg: [-461.445 -461.445 -461.445] (0.0100) ({r_i: None, r_t: [-961.793 -961.793 -961.793], eps: 0.01})
Step:  128700, Reward: [-437.158 -437.158 -437.158] [71.973], Avg: [-458.798 -458.798 -458.798] (0.0100) ({r_i: None, r_t: [-834.706 -834.706 -834.706], eps: 0.01})
Step:  128800, Reward: [-410.255 -410.255 -410.255] [113.246], Avg: [-458.761 -458.761 -458.761] (0.0100) ({r_i: None, r_t: [-869.219 -869.219 -869.219], eps: 0.01})
Step:   30000, Reward: [-438.065 -438.065 -438.065] [72.579], Avg: [-438.237 -438.237 -438.237] (0.1000) ({r_i: None, r_t: [-848.701 -848.701 -848.701], eps: 0.1})
Step:   62400, Reward: [-468.598 -468.598 -468.598] [91.166], Avg: [-461.456 -461.456 -461.456] (0.0100) ({r_i: None, r_t: [-955.496 -955.496 -955.496], eps: 0.01})
Step:  128900, Reward: [-393.249 -393.249 -393.249] [63.517], Avg: [-458.710 -458.710 -458.710] (0.0100) ({r_i: None, r_t: [-845.769 -845.769 -845.769], eps: 0.01})
Step:   30100, Reward: [-436.572 -436.572 -436.572] [92.661], Avg: [-438.232 -438.232 -438.232] (0.1000) ({r_i: None, r_t: [-853.204 -853.204 -853.204], eps: 0.1})
Step:   62500, Reward: [-479.000 -479.000 -479.000] [61.659], Avg: [-461.484 -461.484 -461.484] (0.0100) ({r_i: None, r_t: [-916.795 -916.795 -916.795], eps: 0.01})
Step:  129000, Reward: [-406.171 -406.171 -406.171] [93.363], Avg: [-458.669 -458.669 -458.669] (0.0100) ({r_i: None, r_t: [-861.739 -861.739 -861.739], eps: 0.01})
Step:   30200, Reward: [-453.580 -453.580 -453.580] [94.684], Avg: [-438.282 -438.282 -438.282] (0.1000) ({r_i: None, r_t: [-827.092 -827.092 -827.092], eps: 0.1})
Step:   62600, Reward: [-447.682 -447.682 -447.682] [53.412], Avg: [-461.462 -461.462 -461.462] (0.0100) ({r_i: None, r_t: [-938.289 -938.289 -938.289], eps: 0.01})
Step:  129100, Reward: [-409.039 -409.039 -409.039] [76.377], Avg: [-458.631 -458.631 -458.631] (0.0100) ({r_i: None, r_t: [-811.438 -811.438 -811.438], eps: 0.01})
Step:  129200, Reward: [-402.984 -402.984 -402.984] [89.626], Avg: [-458.588 -458.588 -458.588] (0.0100) ({r_i: None, r_t: [-793.299 -793.299 -793.299], eps: 0.01})
Step:   30300, Reward: [-393.333 -393.333 -393.333] [90.562], Avg: [-438.134 -438.134 -438.134] (0.1000) ({r_i: None, r_t: [-838.706 -838.706 -838.706], eps: 0.1})
Step:   62700, Reward: [-485.639 -485.639 -485.639] [66.630], Avg: [-461.501 -461.501 -461.501] (0.0100) ({r_i: None, r_t: [-901.071 -901.071 -901.071], eps: 0.01})
Step:  129300, Reward: [-398.600 -398.600 -398.600] [100.038], Avg: [-458.541 -458.541 -458.541] (0.0100) ({r_i: None, r_t: [-797.771 -797.771 -797.771], eps: 0.01})
Step:   30400, Reward: [-421.391 -421.391 -421.391] [78.188], Avg: [-438.080 -438.080 -438.080] (0.1000) ({r_i: None, r_t: [-800.390 -800.390 -800.390], eps: 0.1})
Step:   62800, Reward: [-460.372 -460.372 -460.372] [50.085], Avg: [-461.499 -461.499 -461.499] (0.0100) ({r_i: None, r_t: [-896.713 -896.713 -896.713], eps: 0.01})
Step:  129400, Reward: [-449.274 -449.274 -449.274] [165.788], Avg: [-458.534 -458.534 -458.534] (0.0100) ({r_i: None, r_t: [-845.567 -845.567 -845.567], eps: 0.01})
Step:   30500, Reward: [-413.691 -413.691 -413.691] [91.660], Avg: [-438.000 -438.000 -438.000] (0.1000) ({r_i: None, r_t: [-804.744 -804.744 -804.744], eps: 0.1})
Step:   62900, Reward: [-450.304 -450.304 -450.304] [58.424], Avg: [-461.481 -461.481 -461.481] (0.0100) ({r_i: None, r_t: [-890.942 -890.942 -890.942], eps: 0.01})
Step:  129500, Reward: [-426.603 -426.603 -426.603] [77.001], Avg: [-458.509 -458.509 -458.509] (0.0100) ({r_i: None, r_t: [-809.866 -809.866 -809.866], eps: 0.01})
Step:  129600, Reward: [-452.292 -452.292 -452.292] [96.239], Avg: [-458.505 -458.505 -458.505] (0.0100) ({r_i: None, r_t: [-811.335 -811.335 -811.335], eps: 0.01})
Step:   30600, Reward: [-443.058 -443.058 -443.058] [104.308], Avg: [-438.016 -438.016 -438.016] (0.1000) ({r_i: None, r_t: [-849.712 -849.712 -849.712], eps: 0.1})
Step:   63000, Reward: [-484.054 -484.054 -484.054] [66.936], Avg: [-461.517 -461.517 -461.517] (0.0100) ({r_i: None, r_t: [-900.520 -900.520 -900.520], eps: 0.01})
Step:  129700, Reward: [-449.012 -449.012 -449.012] [105.313], Avg: [-458.497 -458.497 -458.497] (0.0100) ({r_i: None, r_t: [-810.463 -810.463 -810.463], eps: 0.01})
Step:   30700, Reward: [-389.854 -389.854 -389.854] [73.789], Avg: [-437.860 -437.860 -437.860] (0.1000) ({r_i: None, r_t: [-852.635 -852.635 -852.635], eps: 0.1})
Step:   63100, Reward: [-475.894 -475.894 -475.894] [73.193], Avg: [-461.540 -461.540 -461.540] (0.0100) ({r_i: None, r_t: [-865.791 -865.791 -865.791], eps: 0.01})
Step:  129800, Reward: [-419.145 -419.145 -419.145] [96.513], Avg: [-458.467 -458.467 -458.467] (0.0100) ({r_i: None, r_t: [-847.493 -847.493 -847.493], eps: 0.01})
Step:   30800, Reward: [-403.343 -403.343 -403.343] [65.832], Avg: [-437.748 -437.748 -437.748] (0.1000) ({r_i: None, r_t: [-827.283 -827.283 -827.283], eps: 0.1})
Step:   63200, Reward: [-448.740 -448.740 -448.740] [70.774], Avg: [-461.520 -461.520 -461.520] (0.0100) ({r_i: None, r_t: [-1003.791 -1003.791 -1003.791], eps: 0.01})
Step:  129900, Reward: [-416.057 -416.057 -416.057] [76.508], Avg: [-458.434 -458.434 -458.434] (0.0100) ({r_i: None, r_t: [-822.995 -822.995 -822.995], eps: 0.01})
Step:   30900, Reward: [-407.733 -407.733 -407.733] [69.616], Avg: [-437.651 -437.651 -437.651] (0.1000) ({r_i: None, r_t: [-800.933 -800.933 -800.933], eps: 0.1})
Step:  130000, Reward: [-416.912 -416.912 -416.912] [71.975], Avg: [-458.403 -458.403 -458.403] (0.0100) ({r_i: None, r_t: [-803.935 -803.935 -803.935], eps: 0.01})
Step:   63300, Reward: [-464.308 -464.308 -464.308] [73.865], Avg: [-461.524 -461.524 -461.524] (0.0100) ({r_i: None, r_t: [-926.348 -926.348 -926.348], eps: 0.01})
Step:  130100, Reward: [-394.707 -394.707 -394.707] [72.777], Avg: [-458.354 -458.354 -458.354] (0.0100) ({r_i: None, r_t: [-839.126 -839.126 -839.126], eps: 0.01})
Step:   31000, Reward: [-420.386 -420.386 -420.386] [91.867], Avg: [-437.596 -437.596 -437.596] (0.1000) ({r_i: None, r_t: [-788.636 -788.636 -788.636], eps: 0.1})
Step:   63400, Reward: [-439.541 -439.541 -439.541] [68.330], Avg: [-461.489 -461.489 -461.489] (0.0100) ({r_i: None, r_t: [-932.359 -932.359 -932.359], eps: 0.01})
Step:  130200, Reward: [-416.385 -416.385 -416.385] [96.795], Avg: [-458.321 -458.321 -458.321] (0.0100) ({r_i: None, r_t: [-893.671 -893.671 -893.671], eps: 0.01})
Step:   31100, Reward: [-442.330 -442.330 -442.330] [95.222], Avg: [-437.611 -437.611 -437.611] (0.1000) ({r_i: None, r_t: [-862.755 -862.755 -862.755], eps: 0.1})
Step:   63500, Reward: [-480.529 -480.529 -480.529] [76.444], Avg: [-461.519 -461.519 -461.519] (0.0100) ({r_i: None, r_t: [-926.169 -926.169 -926.169], eps: 0.01})
Step:  130300, Reward: [-404.657 -404.657 -404.657] [83.690], Avg: [-458.280 -458.280 -458.280] (0.0100) ({r_i: None, r_t: [-873.259 -873.259 -873.259], eps: 0.01})
Step:  130400, Reward: [-405.055 -405.055 -405.055] [82.484], Avg: [-458.239 -458.239 -458.239] (0.0100) ({r_i: None, r_t: [-817.021 -817.021 -817.021], eps: 0.01})
Step:   31200, Reward: [-444.484 -444.484 -444.484] [84.205], Avg: [-437.633 -437.633 -437.633] (0.1000) ({r_i: None, r_t: [-867.080 -867.080 -867.080], eps: 0.1})
Step:   63600, Reward: [-423.483 -423.483 -423.483] [61.704], Avg: [-461.460 -461.460 -461.460] (0.0100) ({r_i: None, r_t: [-969.071 -969.071 -969.071], eps: 0.01})
Step:  130500, Reward: [-455.523 -455.523 -455.523] [90.424], Avg: [-458.237 -458.237 -458.237] (0.0100) ({r_i: None, r_t: [-843.124 -843.124 -843.124], eps: 0.01})
Step:   31300, Reward: [-432.729 -432.729 -432.729] [108.635], Avg: [-437.617 -437.617 -437.617] (0.1000) ({r_i: None, r_t: [-867.724 -867.724 -867.724], eps: 0.1})
Step:   63700, Reward: [-463.464 -463.464 -463.464] [55.233], Avg: [-461.463 -461.463 -461.463] (0.0100) ({r_i: None, r_t: [-911.800 -911.800 -911.800], eps: 0.01})
Step:  130600, Reward: [-417.156 -417.156 -417.156] [112.026], Avg: [-458.206 -458.206 -458.206] (0.0100) ({r_i: None, r_t: [-802.266 -802.266 -802.266], eps: 0.01})
Step:   31400, Reward: [-437.436 -437.436 -437.436] [38.341], Avg: [-437.617 -437.617 -437.617] (0.1000) ({r_i: None, r_t: [-794.821 -794.821 -794.821], eps: 0.1})
Step:   63800, Reward: [-467.526 -467.526 -467.526] [51.678], Avg: [-461.472 -461.472 -461.472] (0.0100) ({r_i: None, r_t: [-901.570 -901.570 -901.570], eps: 0.01})
Step:  130700, Reward: [-399.299 -399.299 -399.299] [69.329], Avg: [-458.161 -458.161 -458.161] (0.0100) ({r_i: None, r_t: [-860.339 -860.339 -860.339], eps: 0.01})
Step:   31500, Reward: [-449.475 -449.475 -449.475] [105.490], Avg: [-437.654 -437.654 -437.654] (0.1000) ({r_i: None, r_t: [-831.341 -831.341 -831.341], eps: 0.1})
Step:  130800, Reward: [-396.893 -396.893 -396.893] [68.573], Avg: [-458.114 -458.114 -458.114] (0.0100) ({r_i: None, r_t: [-773.803 -773.803 -773.803], eps: 0.01})
Step:   63900, Reward: [-445.449 -445.449 -445.449] [64.066], Avg: [-461.447 -461.447 -461.447] (0.0100) ({r_i: None, r_t: [-918.282 -918.282 -918.282], eps: 0.01})
Step:  130900, Reward: [-385.170 -385.170 -385.170] [65.718], Avg: [-458.058 -458.058 -458.058] (0.0100) ({r_i: None, r_t: [-789.754 -789.754 -789.754], eps: 0.01})
Step:   31600, Reward: [-412.483 -412.483 -412.483] [80.638], Avg: [-437.575 -437.575 -437.575] (0.1000) ({r_i: None, r_t: [-786.363 -786.363 -786.363], eps: 0.1})
Step:   64000, Reward: [-480.015 -480.015 -480.015] [62.708], Avg: [-461.476 -461.476 -461.476] (0.0100) ({r_i: None, r_t: [-958.862 -958.862 -958.862], eps: 0.01})
Step:  131000, Reward: [-389.616 -389.616 -389.616] [61.258], Avg: [-458.006 -458.006 -458.006] (0.0100) ({r_i: None, r_t: [-801.647 -801.647 -801.647], eps: 0.01})
Step:   31700, Reward: [-405.455 -405.455 -405.455] [62.710], Avg: [-437.474 -437.474 -437.474] (0.1000) ({r_i: None, r_t: [-811.834 -811.834 -811.834], eps: 0.1})
Step:   64100, Reward: [-435.210 -435.210 -435.210] [58.581], Avg: [-461.435 -461.435 -461.435] (0.0100) ({r_i: None, r_t: [-958.549 -958.549 -958.549], eps: 0.01})
Step:  131100, Reward: [-393.915 -393.915 -393.915] [70.348], Avg: [-457.957 -457.957 -457.957] (0.0100) ({r_i: None, r_t: [-830.346 -830.346 -830.346], eps: 0.01})
Step:  131200, Reward: [-412.065 -412.065 -412.065] [95.726], Avg: [-457.922 -457.922 -457.922] (0.0100) ({r_i: None, r_t: [-869.740 -869.740 -869.740], eps: 0.01})
Step:   31800, Reward: [-407.357 -407.357 -407.357] [58.869], Avg: [-437.380 -437.380 -437.380] (0.1000) ({r_i: None, r_t: [-825.362 -825.362 -825.362], eps: 0.1})
Step:   64200, Reward: [-486.813 -486.813 -486.813] [74.354], Avg: [-461.475 -461.475 -461.475] (0.0100) ({r_i: None, r_t: [-893.270 -893.270 -893.270], eps: 0.01})
Step:  131300, Reward: [-392.151 -392.151 -392.151] [88.126], Avg: [-457.872 -457.872 -457.872] (0.0100) ({r_i: None, r_t: [-845.726 -845.726 -845.726], eps: 0.01})
Step:   31900, Reward: [-401.477 -401.477 -401.477] [58.610], Avg: [-437.267 -437.267 -437.267] (0.1000) ({r_i: None, r_t: [-821.292 -821.292 -821.292], eps: 0.1})
Step:   64300, Reward: [-450.554 -450.554 -450.554] [61.034], Avg: [-461.458 -461.458 -461.458] (0.0100) ({r_i: None, r_t: [-892.351 -892.351 -892.351], eps: 0.01})
Step:  131400, Reward: [-414.758 -414.758 -414.758] [77.463], Avg: [-457.840 -457.840 -457.840] (0.0100) ({r_i: None, r_t: [-881.442 -881.442 -881.442], eps: 0.01})
Step:   32000, Reward: [-407.640 -407.640 -407.640] [100.467], Avg: [-437.175 -437.175 -437.175] (0.1000) ({r_i: None, r_t: [-836.366 -836.366 -836.366], eps: 0.1})
Step:   64400, Reward: [-445.012 -445.012 -445.012] [54.498], Avg: [-461.432 -461.432 -461.432] (0.0100) ({r_i: None, r_t: [-885.169 -885.169 -885.169], eps: 0.01})
Step:  131500, Reward: [-423.601 -423.601 -423.601] [112.705], Avg: [-457.814 -457.814 -457.814] (0.0100) ({r_i: None, r_t: [-783.010 -783.010 -783.010], eps: 0.01})
Step:  131600, Reward: [-417.441 -417.441 -417.441] [74.591], Avg: [-457.783 -457.783 -457.783] (0.0100) ({r_i: None, r_t: [-890.510 -890.510 -890.510], eps: 0.01})
Step:   32100, Reward: [-415.036 -415.036 -415.036] [66.618], Avg: [-437.106 -437.106 -437.106] (0.1000) ({r_i: None, r_t: [-818.522 -818.522 -818.522], eps: 0.1})
Step:   64500, Reward: [-443.068 -443.068 -443.068] [65.521], Avg: [-461.404 -461.404 -461.404] (0.0100) ({r_i: None, r_t: [-938.922 -938.922 -938.922], eps: 0.01})
Step:  131700, Reward: [-439.861 -439.861 -439.861] [149.454], Avg: [-457.769 -457.769 -457.769] (0.0100) ({r_i: None, r_t: [-812.812 -812.812 -812.812], eps: 0.01})
Step:   32200, Reward: [-408.127 -408.127 -408.127] [123.476], Avg: [-437.017 -437.017 -437.017] (0.1000) ({r_i: None, r_t: [-816.958 -816.958 -816.958], eps: 0.1})
Step:   64600, Reward: [-458.299 -458.299 -458.299] [68.383], Avg: [-461.399 -461.399 -461.399] (0.0100) ({r_i: None, r_t: [-871.392 -871.392 -871.392], eps: 0.01})
Step:  131800, Reward: [-386.041 -386.041 -386.041] [63.303], Avg: [-457.715 -457.715 -457.715] (0.0100) ({r_i: None, r_t: [-870.855 -870.855 -870.855], eps: 0.01})
Step:   32300, Reward: [-439.165 -439.165 -439.165] [82.508], Avg: [-437.023 -437.023 -437.023] (0.1000) ({r_i: None, r_t: [-854.987 -854.987 -854.987], eps: 0.1})
Step:  131900, Reward: [-391.198 -391.198 -391.198] [97.659], Avg: [-457.665 -457.665 -457.665] (0.0100) ({r_i: None, r_t: [-775.157 -775.157 -775.157], eps: 0.01})
Step:   64700, Reward: [-441.522 -441.522 -441.522] [69.011], Avg: [-461.368 -461.368 -461.368] (0.0100) ({r_i: None, r_t: [-923.399 -923.399 -923.399], eps: 0.01})
Step:  132000, Reward: [-413.181 -413.181 -413.181] [86.198], Avg: [-457.631 -457.631 -457.631] (0.0100) ({r_i: None, r_t: [-808.209 -808.209 -808.209], eps: 0.01})
Step:   32400, Reward: [-394.260 -394.260 -394.260] [66.366], Avg: [-436.892 -436.892 -436.892] (0.1000) ({r_i: None, r_t: [-828.189 -828.189 -828.189], eps: 0.1})
Step:   64800, Reward: [-471.984 -471.984 -471.984] [76.890], Avg: [-461.385 -461.385 -461.385] (0.0100) ({r_i: None, r_t: [-916.559 -916.559 -916.559], eps: 0.01})
Step:  132100, Reward: [-358.965 -358.965 -358.965] [73.669], Avg: [-457.556 -457.556 -457.556] (0.0100) ({r_i: None, r_t: [-816.416 -816.416 -816.416], eps: 0.01})
Step:   32500, Reward: [-383.971 -383.971 -383.971] [67.024], Avg: [-436.729 -436.729 -436.729] (0.1000) ({r_i: None, r_t: [-839.716 -839.716 -839.716], eps: 0.1})
Step:   64900, Reward: [-422.841 -422.841 -422.841] [69.312], Avg: [-461.325 -461.325 -461.325] (0.0100) ({r_i: None, r_t: [-920.428 -920.428 -920.428], eps: 0.01})
Step:  132200, Reward: [-392.296 -392.296 -392.296] [98.617], Avg: [-457.507 -457.507 -457.507] (0.0100) ({r_i: None, r_t: [-781.052 -781.052 -781.052], eps: 0.01})
Step:   32600, Reward: [-406.936 -406.936 -406.936] [68.292], Avg: [-436.638 -436.638 -436.638] (0.1000) ({r_i: None, r_t: [-803.751 -803.751 -803.751], eps: 0.1})
Step:  132300, Reward: [-431.588 -431.588 -431.588] [91.740], Avg: [-457.487 -457.487 -457.487] (0.0100) ({r_i: None, r_t: [-792.431 -792.431 -792.431], eps: 0.01})
Step:   65000, Reward: [-422.008 -422.008 -422.008] [50.708], Avg: [-461.265 -461.265 -461.265] (0.0100) ({r_i: None, r_t: [-877.917 -877.917 -877.917], eps: 0.01})
Step:  132400, Reward: [-397.095 -397.095 -397.095] [88.832], Avg: [-457.442 -457.442 -457.442] (0.0100) ({r_i: None, r_t: [-843.035 -843.035 -843.035], eps: 0.01})
Step:   32700, Reward: [-416.994 -416.994 -416.994] [79.369], Avg: [-436.578 -436.578 -436.578] (0.1000) ({r_i: None, r_t: [-842.811 -842.811 -842.811], eps: 0.1})
Step:   65100, Reward: [-444.240 -444.240 -444.240] [68.091], Avg: [-461.239 -461.239 -461.239] (0.0100) ({r_i: None, r_t: [-925.988 -925.988 -925.988], eps: 0.01})
Step:  132500, Reward: [-408.178 -408.178 -408.178] [95.047], Avg: [-457.405 -457.405 -457.405] (0.0100) ({r_i: None, r_t: [-817.427 -817.427 -817.427], eps: 0.01})
Step:   32800, Reward: [-445.282 -445.282 -445.282] [82.553], Avg: [-436.605 -436.605 -436.605] (0.1000) ({r_i: None, r_t: [-823.762 -823.762 -823.762], eps: 0.1})
Step:   65200, Reward: [-435.650 -435.650 -435.650] [72.220], Avg: [-461.200 -461.200 -461.200] (0.0100) ({r_i: None, r_t: [-875.216 -875.216 -875.216], eps: 0.01})
Step:  132600, Reward: [-424.280 -424.280 -424.280] [105.010], Avg: [-457.380 -457.380 -457.380] (0.0100) ({r_i: None, r_t: [-810.317 -810.317 -810.317], eps: 0.01})
Step:   32900, Reward: [-428.260 -428.260 -428.260] [91.733], Avg: [-436.579 -436.579 -436.579] (0.1000) ({r_i: None, r_t: [-829.347 -829.347 -829.347], eps: 0.1})
Step:  132700, Reward: [-402.739 -402.739 -402.739] [71.993], Avg: [-457.338 -457.338 -457.338] (0.0100) ({r_i: None, r_t: [-809.046 -809.046 -809.046], eps: 0.01})
Step:   65300, Reward: [-464.080 -464.080 -464.080] [71.633], Avg: [-461.204 -461.204 -461.204] (0.0100) ({r_i: None, r_t: [-853.029 -853.029 -853.029], eps: 0.01})
Step:  132800, Reward: [-445.841 -445.841 -445.841] [106.981], Avg: [-457.330 -457.330 -457.330] (0.0100) ({r_i: None, r_t: [-833.945 -833.945 -833.945], eps: 0.01})
Step:   33000, Reward: [-426.210 -426.210 -426.210] [119.856], Avg: [-436.548 -436.548 -436.548] (0.1000) ({r_i: None, r_t: [-806.402 -806.402 -806.402], eps: 0.1})
Step:   65400, Reward: [-461.110 -461.110 -461.110] [72.106], Avg: [-461.204 -461.204 -461.204] (0.0100) ({r_i: None, r_t: [-936.205 -936.205 -936.205], eps: 0.01})
Step:  132900, Reward: [-458.776 -458.776 -458.776] [74.749], Avg: [-457.331 -457.331 -457.331] (0.0100) ({r_i: None, r_t: [-813.191 -813.191 -813.191], eps: 0.01})
Step:   33100, Reward: [-434.071 -434.071 -434.071] [90.225], Avg: [-436.541 -436.541 -436.541] (0.1000) ({r_i: None, r_t: [-833.644 -833.644 -833.644], eps: 0.1})
Step:   65500, Reward: [-466.377 -466.377 -466.377] [78.001], Avg: [-461.212 -461.212 -461.212] (0.0100) ({r_i: None, r_t: [-923.477 -923.477 -923.477], eps: 0.01})
Step:  133000, Reward: [-448.071 -448.071 -448.071] [92.287], Avg: [-457.324 -457.324 -457.324] (0.0100) ({r_i: None, r_t: [-861.496 -861.496 -861.496], eps: 0.01})
Step:   33200, Reward: [-410.300 -410.300 -410.300] [78.188], Avg: [-436.462 -436.462 -436.462] (0.1000) ({r_i: None, r_t: [-862.222 -862.222 -862.222], eps: 0.1})
Step:  133100, Reward: [-430.365 -430.365 -430.365] [106.955], Avg: [-457.304 -457.304 -457.304] (0.0100) ({r_i: None, r_t: [-847.542 -847.542 -847.542], eps: 0.01})
Step:   65600, Reward: [-501.961 -501.961 -501.961] [81.586], Avg: [-461.274 -461.274 -461.274] (0.0100) ({r_i: None, r_t: [-880.567 -880.567 -880.567], eps: 0.01})
Step:  133200, Reward: [-389.245 -389.245 -389.245] [108.932], Avg: [-457.253 -457.253 -457.253] (0.0100) ({r_i: None, r_t: [-881.707 -881.707 -881.707], eps: 0.01})
Step:   33300, Reward: [-415.145 -415.145 -415.145] [51.111], Avg: [-436.398 -436.398 -436.398] (0.1000) ({r_i: None, r_t: [-809.469 -809.469 -809.469], eps: 0.1})
Step:   65700, Reward: [-449.359 -449.359 -449.359] [64.703], Avg: [-461.256 -461.256 -461.256] (0.0100) ({r_i: None, r_t: [-919.681 -919.681 -919.681], eps: 0.01})
Step:  133300, Reward: [-414.572 -414.572 -414.572] [73.305], Avg: [-457.221 -457.221 -457.221] (0.0100) ({r_i: None, r_t: [-802.034 -802.034 -802.034], eps: 0.01})
Step:   33400, Reward: [-425.969 -425.969 -425.969] [78.276], Avg: [-436.367 -436.367 -436.367] (0.1000) ({r_i: None, r_t: [-841.400 -841.400 -841.400], eps: 0.1})
Step:   65800, Reward: [-505.488 -505.488 -505.488] [98.224], Avg: [-461.323 -461.323 -461.323] (0.0100) ({r_i: None, r_t: [-856.153 -856.153 -856.153], eps: 0.01})
Step:  133400, Reward: [-396.678 -396.678 -396.678] [66.122], Avg: [-457.175 -457.175 -457.175] (0.0100) ({r_i: None, r_t: [-811.132 -811.132 -811.132], eps: 0.01})
Step:   33500, Reward: [-429.987 -429.987 -429.987] [51.028], Avg: [-436.348 -436.348 -436.348] (0.1000) ({r_i: None, r_t: [-819.799 -819.799 -819.799], eps: 0.1})
Step:  133500, Reward: [-411.564 -411.564 -411.564] [82.468], Avg: [-457.141 -457.141 -457.141] (0.0100) ({r_i: None, r_t: [-867.601 -867.601 -867.601], eps: 0.01})
Step:   65900, Reward: [-465.951 -465.951 -465.951] [80.929], Avg: [-461.330 -461.330 -461.330] (0.0100) ({r_i: None, r_t: [-913.581 -913.581 -913.581], eps: 0.01})
Step:  133600, Reward: [-430.804 -430.804 -430.804] [123.162], Avg: [-457.121 -457.121 -457.121] (0.0100) ({r_i: None, r_t: [-846.133 -846.133 -846.133], eps: 0.01})
Step:   33600, Reward: [-427.301 -427.301 -427.301] [115.237], Avg: [-436.321 -436.321 -436.321] (0.1000) ({r_i: None, r_t: [-815.387 -815.387 -815.387], eps: 0.1})
Step:   66000, Reward: [-455.422 -455.422 -455.422] [54.890], Avg: [-461.321 -461.321 -461.321] (0.0100) ({r_i: None, r_t: [-924.047 -924.047 -924.047], eps: 0.01})
Step:  133700, Reward: [-447.265 -447.265 -447.265] [81.614], Avg: [-457.114 -457.114 -457.114] (0.0100) ({r_i: None, r_t: [-808.151 -808.151 -808.151], eps: 0.01})
Step:   33700, Reward: [-410.758 -410.758 -410.758] [91.768], Avg: [-436.245 -436.245 -436.245] (0.1000) ({r_i: None, r_t: [-825.209 -825.209 -825.209], eps: 0.1})
Step:   66100, Reward: [-445.340 -445.340 -445.340] [75.690], Avg: [-461.297 -461.297 -461.297] (0.0100) ({r_i: None, r_t: [-879.938 -879.938 -879.938], eps: 0.01})
Step:  133800, Reward: [-397.721 -397.721 -397.721] [74.360], Avg: [-457.070 -457.070 -457.070] (0.0100) ({r_i: None, r_t: [-817.399 -817.399 -817.399], eps: 0.01})
Step:   33800, Reward: [-397.656 -397.656 -397.656] [62.923], Avg: [-436.132 -436.132 -436.132] (0.1000) ({r_i: None, r_t: [-872.216 -872.216 -872.216], eps: 0.1})
Step:  133900, Reward: [-416.792 -416.792 -416.792] [93.438], Avg: [-457.040 -457.040 -457.040] (0.0100) ({r_i: None, r_t: [-801.344 -801.344 -801.344], eps: 0.01})
Step:   66200, Reward: [-470.233 -470.233 -470.233] [81.087], Avg: [-461.310 -461.310 -461.310] (0.0100) ({r_i: None, r_t: [-886.579 -886.579 -886.579], eps: 0.01})
Step:  134000, Reward: [-405.691 -405.691 -405.691] [68.049], Avg: [-457.001 -457.001 -457.001] (0.0100) ({r_i: None, r_t: [-844.966 -844.966 -844.966], eps: 0.01})
Step:   33900, Reward: [-423.383 -423.383 -423.383] [65.833], Avg: [-436.094 -436.094 -436.094] (0.1000) ({r_i: None, r_t: [-778.608 -778.608 -778.608], eps: 0.1})
Step:   66300, Reward: [-446.305 -446.305 -446.305] [71.359], Avg: [-461.288 -461.288 -461.288] (0.0100) ({r_i: None, r_t: [-908.354 -908.354 -908.354], eps: 0.01})
Step:  134100, Reward: [-403.279 -403.279 -403.279] [102.432], Avg: [-456.961 -456.961 -456.961] (0.0100) ({r_i: None, r_t: [-752.515 -752.515 -752.515], eps: 0.01})
Step:   34000, Reward: [-433.795 -433.795 -433.795] [85.012], Avg: [-436.087 -436.087 -436.087] (0.1000) ({r_i: None, r_t: [-774.869 -774.869 -774.869], eps: 0.1})
Step:   66400, Reward: [-465.216 -465.216 -465.216] [90.388], Avg: [-461.294 -461.294 -461.294] (0.0100) ({r_i: None, r_t: [-921.711 -921.711 -921.711], eps: 0.01})
Step:  134200, Reward: [-431.958 -431.958 -431.958] [79.889], Avg: [-456.943 -456.943 -456.943] (0.0100) ({r_i: None, r_t: [-829.220 -829.220 -829.220], eps: 0.01})
Step:  134300, Reward: [-398.310 -398.310 -398.310] [65.832], Avg: [-456.899 -456.899 -456.899] (0.0100) ({r_i: None, r_t: [-831.983 -831.983 -831.983], eps: 0.01})
Step:   34100, Reward: [-391.928 -391.928 -391.928] [72.266], Avg: [-435.958 -435.958 -435.958] (0.1000) ({r_i: None, r_t: [-846.482 -846.482 -846.482], eps: 0.1})
Step:   66500, Reward: [-477.479 -477.479 -477.479] [66.671], Avg: [-461.318 -461.318 -461.318] (0.0100) ({r_i: None, r_t: [-931.029 -931.029 -931.029], eps: 0.01})
Step:  134400, Reward: [-394.069 -394.069 -394.069] [59.335], Avg: [-456.852 -456.852 -456.852] (0.0100) ({r_i: None, r_t: [-884.107 -884.107 -884.107], eps: 0.01})
Step:   34200, Reward: [-420.248 -420.248 -420.248] [56.910], Avg: [-435.912 -435.912 -435.912] (0.1000) ({r_i: None, r_t: [-806.878 -806.878 -806.878], eps: 0.1})
Step:   66600, Reward: [-433.995 -433.995 -433.995] [75.250], Avg: [-461.277 -461.277 -461.277] (0.0100) ({r_i: None, r_t: [-904.315 -904.315 -904.315], eps: 0.01})
Step:  134500, Reward: [-423.629 -423.629 -423.629] [59.933], Avg: [-456.828 -456.828 -456.828] (0.0100) ({r_i: None, r_t: [-868.675 -868.675 -868.675], eps: 0.01})
Step:   34300, Reward: [-414.959 -414.959 -414.959] [126.167], Avg: [-435.852 -435.852 -435.852] (0.1000) ({r_i: None, r_t: [-769.261 -769.261 -769.261], eps: 0.1})
Step:   66700, Reward: [-434.491 -434.491 -434.491] [62.794], Avg: [-461.237 -461.237 -461.237] (0.0100) ({r_i: None, r_t: [-932.454 -932.454 -932.454], eps: 0.01})
Step:  134600, Reward: [-431.541 -431.541 -431.541] [69.299], Avg: [-456.809 -456.809 -456.809] (0.0100) ({r_i: None, r_t: [-800.084 -800.084 -800.084], eps: 0.01})
Step:  134700, Reward: [-434.929 -434.929 -434.929] [72.244], Avg: [-456.793 -456.793 -456.793] (0.0100) ({r_i: None, r_t: [-862.085 -862.085 -862.085], eps: 0.01})
Step:   34400, Reward: [-413.494 -413.494 -413.494] [89.826], Avg: [-435.787 -435.787 -435.787] (0.1000) ({r_i: None, r_t: [-889.763 -889.763 -889.763], eps: 0.1})
Step:   66800, Reward: [-437.884 -437.884 -437.884] [65.612], Avg: [-461.202 -461.202 -461.202] (0.0100) ({r_i: None, r_t: [-929.159 -929.159 -929.159], eps: 0.01})
Step:  134800, Reward: [-430.756 -430.756 -430.756] [79.379], Avg: [-456.773 -456.773 -456.773] (0.0100) ({r_i: None, r_t: [-806.231 -806.231 -806.231], eps: 0.01})
Step:   34500, Reward: [-392.739 -392.739 -392.739] [48.801], Avg: [-435.662 -435.662 -435.662] (0.1000) ({r_i: None, r_t: [-791.144 -791.144 -791.144], eps: 0.1})
Step:   66900, Reward: [-480.606 -480.606 -480.606] [65.147], Avg: [-461.231 -461.231 -461.231] (0.0100) ({r_i: None, r_t: [-904.904 -904.904 -904.904], eps: 0.01})
Step:  134900, Reward: [-392.526 -392.526 -392.526] [82.671], Avg: [-456.726 -456.726 -456.726] (0.0100) ({r_i: None, r_t: [-833.254 -833.254 -833.254], eps: 0.01})
Step:   34600, Reward: [-398.966 -398.966 -398.966] [62.731], Avg: [-435.557 -435.557 -435.557] (0.1000) ({r_i: None, r_t: [-825.239 -825.239 -825.239], eps: 0.1})
Step:   67000, Reward: [-453.904 -453.904 -453.904] [89.136], Avg: [-461.220 -461.220 -461.220] (0.0100) ({r_i: None, r_t: [-945.357 -945.357 -945.357], eps: 0.01})
Step:  135000, Reward: [-386.034 -386.034 -386.034] [56.859], Avg: [-456.674 -456.674 -456.674] (0.0100) ({r_i: None, r_t: [-875.147 -875.147 -875.147], eps: 0.01})
Step:  135100, Reward: [-415.222 -415.222 -415.222] [72.536], Avg: [-456.643 -456.643 -456.643] (0.0100) ({r_i: None, r_t: [-851.823 -851.823 -851.823], eps: 0.01})
Step:   34700, Reward: [-436.044 -436.044 -436.044] [84.200], Avg: [-435.558 -435.558 -435.558] (0.1000) ({r_i: None, r_t: [-847.901 -847.901 -847.901], eps: 0.1})
Step:   67100, Reward: [-455.176 -455.176 -455.176] [65.148], Avg: [-461.211 -461.211 -461.211] (0.0100) ({r_i: None, r_t: [-908.320 -908.320 -908.320], eps: 0.01})
Step:  135200, Reward: [-384.515 -384.515 -384.515] [66.231], Avg: [-456.590 -456.590 -456.590] (0.0100) ({r_i: None, r_t: [-790.415 -790.415 -790.415], eps: 0.01})
Step:   34800, Reward: [-408.348 -408.348 -408.348] [88.228], Avg: [-435.480 -435.480 -435.480] (0.1000) ({r_i: None, r_t: [-786.728 -786.728 -786.728], eps: 0.1})
Step:   67200, Reward: [-423.482 -423.482 -423.482] [91.766], Avg: [-461.155 -461.155 -461.155] (0.0100) ({r_i: None, r_t: [-895.279 -895.279 -895.279], eps: 0.01})
Step:  135300, Reward: [-406.300 -406.300 -406.300] [81.092], Avg: [-456.552 -456.552 -456.552] (0.0100) ({r_i: None, r_t: [-843.351 -843.351 -843.351], eps: 0.01})
Step:   34900, Reward: [-428.247 -428.247 -428.247] [89.383], Avg: [-435.459 -435.459 -435.459] (0.1000) ({r_i: None, r_t: [-844.238 -844.238 -844.238], eps: 0.1})
Step:  135400, Reward: [-414.755 -414.755 -414.755] [81.251], Avg: [-456.522 -456.522 -456.522] (0.0100) ({r_i: None, r_t: [-843.214 -843.214 -843.214], eps: 0.01})
Step:   67300, Reward: [-458.690 -458.690 -458.690] [72.866], Avg: [-461.151 -461.151 -461.151] (0.0100) ({r_i: None, r_t: [-864.277 -864.277 -864.277], eps: 0.01})
Step:  135500, Reward: [-406.812 -406.812 -406.812] [73.094], Avg: [-456.485 -456.485 -456.485] (0.0100) ({r_i: None, r_t: [-860.951 -860.951 -860.951], eps: 0.01})
Step:   35000, Reward: [-413.329 -413.329 -413.329] [72.284], Avg: [-435.396 -435.396 -435.396] (0.1000) ({r_i: None, r_t: [-846.314 -846.314 -846.314], eps: 0.1})
Step:   67400, Reward: [-438.575 -438.575 -438.575] [77.383], Avg: [-461.118 -461.118 -461.118] (0.0100) ({r_i: None, r_t: [-882.062 -882.062 -882.062], eps: 0.01})
Step:  135600, Reward: [-409.635 -409.635 -409.635] [75.457], Avg: [-456.450 -456.450 -456.450] (0.0100) ({r_i: None, r_t: [-857.840 -857.840 -857.840], eps: 0.01})
Step:   35100, Reward: [-427.087 -427.087 -427.087] [63.252], Avg: [-435.373 -435.373 -435.373] (0.1000) ({r_i: None, r_t: [-820.670 -820.670 -820.670], eps: 0.1})
Step:   67500, Reward: [-442.802 -442.802 -442.802] [69.981], Avg: [-461.091 -461.091 -461.091] (0.0100) ({r_i: None, r_t: [-899.672 -899.672 -899.672], eps: 0.01})
Step:  135700, Reward: [-408.213 -408.213 -408.213] [81.806], Avg: [-456.415 -456.415 -456.415] (0.0100) ({r_i: None, r_t: [-827.408 -827.408 -827.408], eps: 0.01})
Step:   35200, Reward: [-431.855 -431.855 -431.855] [96.671], Avg: [-435.363 -435.363 -435.363] (0.1000) ({r_i: None, r_t: [-869.968 -869.968 -869.968], eps: 0.1})
Step:  135800, Reward: [-391.323 -391.323 -391.323] [62.677], Avg: [-456.367 -456.367 -456.367] (0.0100) ({r_i: None, r_t: [-830.541 -830.541 -830.541], eps: 0.01})
Step:   67600, Reward: [-457.442 -457.442 -457.442] [69.321], Avg: [-461.085 -461.085 -461.085] (0.0100) ({r_i: None, r_t: [-922.143 -922.143 -922.143], eps: 0.01})
Step:  135900, Reward: [-445.784 -445.784 -445.784] [82.222], Avg: [-456.359 -456.359 -456.359] (0.0100) ({r_i: None, r_t: [-809.050 -809.050 -809.050], eps: 0.01})
Step:   35300, Reward: [-391.390 -391.390 -391.390] [66.611], Avg: [-435.238 -435.238 -435.238] (0.1000) ({r_i: None, r_t: [-843.471 -843.471 -843.471], eps: 0.1})
Step:   67700, Reward: [-476.219 -476.219 -476.219] [78.380], Avg: [-461.108 -461.108 -461.108] (0.0100) ({r_i: None, r_t: [-901.564 -901.564 -901.564], eps: 0.01})
Step:  136000, Reward: [-407.625 -407.625 -407.625] [76.075], Avg: [-456.323 -456.323 -456.323] (0.0100) ({r_i: None, r_t: [-865.369 -865.369 -865.369], eps: 0.01})
Step:   35400, Reward: [-427.651 -427.651 -427.651] [85.894], Avg: [-435.217 -435.217 -435.217] (0.1000) ({r_i: None, r_t: [-831.345 -831.345 -831.345], eps: 0.1})
Step:   67800, Reward: [-446.517 -446.517 -446.517] [78.999], Avg: [-461.086 -461.086 -461.086] (0.0100) ({r_i: None, r_t: [-871.593 -871.593 -871.593], eps: 0.01})
Step:  136100, Reward: [-444.704 -444.704 -444.704] [110.810], Avg: [-456.315 -456.315 -456.315] (0.0100) ({r_i: None, r_t: [-854.459 -854.459 -854.459], eps: 0.01})
Step:   35500, Reward: [-457.631 -457.631 -457.631] [114.386], Avg: [-435.280 -435.280 -435.280] (0.1000) ({r_i: None, r_t: [-839.993 -839.993 -839.993], eps: 0.1})
Step:  136200, Reward: [-370.064 -370.064 -370.064] [71.751], Avg: [-456.252 -456.252 -456.252] (0.0100) ({r_i: None, r_t: [-801.510 -801.510 -801.510], eps: 0.01})
Step:   67900, Reward: [-444.033 -444.033 -444.033] [69.677], Avg: [-461.061 -461.061 -461.061] (0.0100) ({r_i: None, r_t: [-934.383 -934.383 -934.383], eps: 0.01})
Step:  136300, Reward: [-383.870 -383.870 -383.870] [81.381], Avg: [-456.198 -456.198 -456.198] (0.0100) ({r_i: None, r_t: [-820.352 -820.352 -820.352], eps: 0.01})
Step:   35600, Reward: [-406.802 -406.802 -406.802] [67.061], Avg: [-435.200 -435.200 -435.200] (0.1000) ({r_i: None, r_t: [-859.810 -859.810 -859.810], eps: 0.1})
Step:   68000, Reward: [-448.555 -448.555 -448.555] [83.248], Avg: [-461.043 -461.043 -461.043] (0.0100) ({r_i: None, r_t: [-921.219 -921.219 -921.219], eps: 0.01})
Step:  136400, Reward: [-396.772 -396.772 -396.772] [77.191], Avg: [-456.155 -456.155 -456.155] (0.0100) ({r_i: None, r_t: [-820.944 -820.944 -820.944], eps: 0.01})
Step:   35700, Reward: [-453.599 -453.599 -453.599] [63.074], Avg: [-435.252 -435.252 -435.252] (0.1000) ({r_i: None, r_t: [-837.344 -837.344 -837.344], eps: 0.1})
Step:   68100, Reward: [-479.044 -479.044 -479.044] [64.314], Avg: [-461.069 -461.069 -461.069] (0.0100) ({r_i: None, r_t: [-908.898 -908.898 -908.898], eps: 0.01})
Step:  136500, Reward: [-395.536 -395.536 -395.536] [81.587], Avg: [-456.111 -456.111 -456.111] (0.0100) ({r_i: None, r_t: [-828.173 -828.173 -828.173], eps: 0.01})
Step:   35800, Reward: [-416.839 -416.839 -416.839] [72.435], Avg: [-435.200 -435.200 -435.200] (0.1000) ({r_i: None, r_t: [-787.818 -787.818 -787.818], eps: 0.1})
Step:  136600, Reward: [-425.306 -425.306 -425.306] [69.020], Avg: [-456.088 -456.088 -456.088] (0.0100) ({r_i: None, r_t: [-838.275 -838.275 -838.275], eps: 0.01})
Step:   68200, Reward: [-432.208 -432.208 -432.208] [68.060], Avg: [-461.027 -461.027 -461.027] (0.0100) ({r_i: None, r_t: [-912.018 -912.018 -912.018], eps: 0.01})
Step:  136700, Reward: [-386.323 -386.323 -386.323] [53.311], Avg: [-456.037 -456.037 -456.037] (0.0100) ({r_i: None, r_t: [-852.402 -852.402 -852.402], eps: 0.01})
Step:   35900, Reward: [-413.693 -413.693 -413.693] [93.037], Avg: [-435.141 -435.141 -435.141] (0.1000) ({r_i: None, r_t: [-860.556 -860.556 -860.556], eps: 0.1})
Step:   68300, Reward: [-421.568 -421.568 -421.568] [60.542], Avg: [-460.969 -460.969 -460.969] (0.0100) ({r_i: None, r_t: [-873.660 -873.660 -873.660], eps: 0.01})
Step:  136800, Reward: [-413.670 -413.670 -413.670] [66.985], Avg: [-456.006 -456.006 -456.006] (0.0100) ({r_i: None, r_t: [-841.163 -841.163 -841.163], eps: 0.01})
Step:   36000, Reward: [-423.424 -423.424 -423.424] [70.524], Avg: [-435.108 -435.108 -435.108] (0.1000) ({r_i: None, r_t: [-789.828 -789.828 -789.828], eps: 0.1})
Step:   68400, Reward: [-440.922 -440.922 -440.922] [62.859], Avg: [-460.940 -460.940 -460.940] (0.0100) ({r_i: None, r_t: [-902.556 -902.556 -902.556], eps: 0.01})
Step:  136900, Reward: [-402.812 -402.812 -402.812] [73.737], Avg: [-455.967 -455.967 -455.967] (0.0100) ({r_i: None, r_t: [-795.914 -795.914 -795.914], eps: 0.01})
Step:   36100, Reward: [-433.201 -433.201 -433.201] [78.258], Avg: [-435.103 -435.103 -435.103] (0.1000) ({r_i: None, r_t: [-845.598 -845.598 -845.598], eps: 0.1})
Step:   68500, Reward: [-444.244 -444.244 -444.244] [70.176], Avg: [-460.916 -460.916 -460.916] (0.0100) ({r_i: None, r_t: [-909.545 -909.545 -909.545], eps: 0.01})
Step:  137000, Reward: [-393.565 -393.565 -393.565] [84.139], Avg: [-455.922 -455.922 -455.922] (0.0100) ({r_i: None, r_t: [-803.655 -803.655 -803.655], eps: 0.01})
Step:  137100, Reward: [-469.487 -469.487 -469.487] [143.149], Avg: [-455.932 -455.932 -455.932] (0.0100) ({r_i: None, r_t: [-815.320 -815.320 -815.320], eps: 0.01})
Step:   36200, Reward: [-431.535 -431.535 -431.535] [44.796], Avg: [-435.093 -435.093 -435.093] (0.1000) ({r_i: None, r_t: [-810.312 -810.312 -810.312], eps: 0.1})
Step:   68600, Reward: [-463.982 -463.982 -463.982] [87.779], Avg: [-460.920 -460.920 -460.920] (0.0100) ({r_i: None, r_t: [-900.952 -900.952 -900.952], eps: 0.01})
Step:  137200, Reward: [-418.978 -418.978 -418.978] [65.625], Avg: [-455.905 -455.905 -455.905] (0.0100) ({r_i: None, r_t: [-908.497 -908.497 -908.497], eps: 0.01})
Step:   36300, Reward: [-427.745 -427.745 -427.745] [58.464], Avg: [-435.073 -435.073 -435.073] (0.1000) ({r_i: None, r_t: [-844.039 -844.039 -844.039], eps: 0.1})
Step:   68700, Reward: [-433.005 -433.005 -433.005] [68.719], Avg: [-460.879 -460.879 -460.879] (0.0100) ({r_i: None, r_t: [-891.078 -891.078 -891.078], eps: 0.01})
Step:  137300, Reward: [-398.347 -398.347 -398.347] [56.959], Avg: [-455.863 -455.863 -455.863] (0.0100) ({r_i: None, r_t: [-828.979 -828.979 -828.979], eps: 0.01})
Step:   36400, Reward: [-410.648 -410.648 -410.648] [74.539], Avg: [-435.006 -435.006 -435.006] (0.1000) ({r_i: None, r_t: [-852.372 -852.372 -852.372], eps: 0.1})
Step:  137400, Reward: [-446.254 -446.254 -446.254] [88.805], Avg: [-455.856 -455.856 -455.856] (0.0100) ({r_i: None, r_t: [-860.998 -860.998 -860.998], eps: 0.01})
Step:   68800, Reward: [-437.717 -437.717 -437.717] [75.717], Avg: [-460.846 -460.846 -460.846] (0.0100) ({r_i: None, r_t: [-911.412 -911.412 -911.412], eps: 0.01})
Step:  137500, Reward: [-444.402 -444.402 -444.402] [85.066], Avg: [-455.848 -455.848 -455.848] (0.0100) ({r_i: None, r_t: [-842.145 -842.145 -842.145], eps: 0.01})
Step:   36500, Reward: [-402.633 -402.633 -402.633] [81.536], Avg: [-434.918 -434.918 -434.918] (0.1000) ({r_i: None, r_t: [-883.094 -883.094 -883.094], eps: 0.1})
Step:   68900, Reward: [-456.009 -456.009 -456.009] [72.481], Avg: [-460.839 -460.839 -460.839] (0.0100) ({r_i: None, r_t: [-885.225 -885.225 -885.225], eps: 0.01})
Step:  137600, Reward: [-421.601 -421.601 -421.601] [89.106], Avg: [-455.823 -455.823 -455.823] (0.0100) ({r_i: None, r_t: [-851.879 -851.879 -851.879], eps: 0.01})
Step:   36600, Reward: [-433.969 -433.969 -433.969] [59.105], Avg: [-434.915 -434.915 -434.915] (0.1000) ({r_i: None, r_t: [-830.761 -830.761 -830.761], eps: 0.1})
Step:   69000, Reward: [-432.804 -432.804 -432.804] [72.470], Avg: [-460.798 -460.798 -460.798] (0.0100) ({r_i: None, r_t: [-949.304 -949.304 -949.304], eps: 0.01})
Step:  137700, Reward: [-389.845 -389.845 -389.845] [77.062], Avg: [-455.775 -455.775 -455.775] (0.0100) ({r_i: None, r_t: [-834.958 -834.958 -834.958], eps: 0.01})
Step:   36700, Reward: [-381.395 -381.395 -381.395] [48.446], Avg: [-434.770 -434.770 -434.770] (0.1000) ({r_i: None, r_t: [-807.690 -807.690 -807.690], eps: 0.1})
Step:  137800, Reward: [-435.423 -435.423 -435.423] [126.689], Avg: [-455.760 -455.760 -455.760] (0.0100) ({r_i: None, r_t: [-842.214 -842.214 -842.214], eps: 0.01})
Step:   69100, Reward: [-462.112 -462.112 -462.112] [72.899], Avg: [-460.800 -460.800 -460.800] (0.0100) ({r_i: None, r_t: [-912.575 -912.575 -912.575], eps: 0.01})
Step:  137900, Reward: [-379.456 -379.456 -379.456] [75.801], Avg: [-455.705 -455.705 -455.705] (0.0100) ({r_i: None, r_t: [-812.553 -812.553 -812.553], eps: 0.01})
Step:   36800, Reward: [-413.903 -413.903 -413.903] [70.711], Avg: [-434.713 -434.713 -434.713] (0.1000) ({r_i: None, r_t: [-822.698 -822.698 -822.698], eps: 0.1})
Step:   69200, Reward: [-436.727 -436.727 -436.727] [67.619], Avg: [-460.765 -460.765 -460.765] (0.0100) ({r_i: None, r_t: [-907.753 -907.753 -907.753], eps: 0.01})
Step:  138000, Reward: [-386.465 -386.465 -386.465] [62.738], Avg: [-455.655 -455.655 -455.655] (0.0100) ({r_i: None, r_t: [-856.039 -856.039 -856.039], eps: 0.01})
Step:   36900, Reward: [-436.790 -436.790 -436.790] [82.827], Avg: [-434.719 -434.719 -434.719] (0.1000) ({r_i: None, r_t: [-838.361 -838.361 -838.361], eps: 0.1})
Step:   69300, Reward: [-425.670 -425.670 -425.670] [54.800], Avg: [-460.715 -460.715 -460.715] (0.0100) ({r_i: None, r_t: [-900.595 -900.595 -900.595], eps: 0.01})
Step:  138100, Reward: [-380.588 -380.588 -380.588] [92.566], Avg: [-455.600 -455.600 -455.600] (0.0100) ({r_i: None, r_t: [-822.645 -822.645 -822.645], eps: 0.01})
Step:   37000, Reward: [-403.411 -403.411 -403.411] [71.151], Avg: [-434.634 -434.634 -434.634] (0.1000) ({r_i: None, r_t: [-816.505 -816.505 -816.505], eps: 0.1})
Step:  138200, Reward: [-369.713 -369.713 -369.713] [54.801], Avg: [-455.538 -455.538 -455.538] (0.0100) ({r_i: None, r_t: [-845.235 -845.235 -845.235], eps: 0.01})
Step:   69400, Reward: [-474.563 -474.563 -474.563] [55.158], Avg: [-460.735 -460.735 -460.735] (0.0100) ({r_i: None, r_t: [-912.801 -912.801 -912.801], eps: 0.01})
Step:  138300, Reward: [-410.386 -410.386 -410.386] [106.528], Avg: [-455.506 -455.506 -455.506] (0.0100) ({r_i: None, r_t: [-829.694 -829.694 -829.694], eps: 0.01})
Step:   37100, Reward: [-446.714 -446.714 -446.714] [112.821], Avg: [-434.667 -434.667 -434.667] (0.1000) ({r_i: None, r_t: [-829.620 -829.620 -829.620], eps: 0.1})
Step:   69500, Reward: [-458.031 -458.031 -458.031] [53.327], Avg: [-460.731 -460.731 -460.731] (0.0100) ({r_i: None, r_t: [-902.937 -902.937 -902.937], eps: 0.01})
Step:  138400, Reward: [-414.980 -414.980 -414.980] [92.888], Avg: [-455.476 -455.476 -455.476] (0.0100) ({r_i: None, r_t: [-820.761 -820.761 -820.761], eps: 0.01})
Step:   37200, Reward: [-399.504 -399.504 -399.504] [62.228], Avg: [-434.572 -434.572 -434.572] (0.1000) ({r_i: None, r_t: [-840.422 -840.422 -840.422], eps: 0.1})
Step:   69600, Reward: [-449.527 -449.527 -449.527] [70.510], Avg: [-460.715 -460.715 -460.715] (0.0100) ({r_i: None, r_t: [-874.798 -874.798 -874.798], eps: 0.01})
Step:  138500, Reward: [-386.662 -386.662 -386.662] [67.982], Avg: [-455.427 -455.427 -455.427] (0.0100) ({r_i: None, r_t: [-858.270 -858.270 -858.270], eps: 0.01})
Step:   37300, Reward: [-384.522 -384.522 -384.522] [72.831], Avg: [-434.439 -434.439 -434.439] (0.1000) ({r_i: None, r_t: [-852.053 -852.053 -852.053], eps: 0.1})
Step:  138600, Reward: [-412.902 -412.902 -412.902] [77.872], Avg: [-455.396 -455.396 -455.396] (0.0100) ({r_i: None, r_t: [-830.560 -830.560 -830.560], eps: 0.01})
Step:   69700, Reward: [-423.461 -423.461 -423.461] [73.875], Avg: [-460.661 -460.661 -460.661] (0.0100) ({r_i: None, r_t: [-835.209 -835.209 -835.209], eps: 0.01})
Step:  138700, Reward: [-424.610 -424.610 -424.610] [70.716], Avg: [-455.374 -455.374 -455.374] (0.0100) ({r_i: None, r_t: [-822.123 -822.123 -822.123], eps: 0.01})
Step:   37400, Reward: [-441.230 -441.230 -441.230] [55.336], Avg: [-434.457 -434.457 -434.457] (0.1000) ({r_i: None, r_t: [-786.056 -786.056 -786.056], eps: 0.1})
Step:   69800, Reward: [-430.634 -430.634 -430.634] [73.619], Avg: [-460.618 -460.618 -460.618] (0.0100) ({r_i: None, r_t: [-874.241 -874.241 -874.241], eps: 0.01})
Step:  138800, Reward: [-432.916 -432.916 -432.916] [80.519], Avg: [-455.358 -455.358 -455.358] (0.0100) ({r_i: None, r_t: [-872.322 -872.322 -872.322], eps: 0.01})
Step:   37500, Reward: [-416.582 -416.582 -416.582] [64.834], Avg: [-434.409 -434.409 -434.409] (0.1000) ({r_i: None, r_t: [-804.856 -804.856 -804.856], eps: 0.1})
Step:   69900, Reward: [-429.471 -429.471 -429.471] [65.729], Avg: [-460.574 -460.574 -460.574] (0.0100) ({r_i: None, r_t: [-887.740 -887.740 -887.740], eps: 0.01})
Step:  138900, Reward: [-402.862 -402.862 -402.862] [64.908], Avg: [-455.320 -455.320 -455.320] (0.0100) ({r_i: None, r_t: [-851.264 -851.264 -851.264], eps: 0.01})
Step:  139000, Reward: [-399.535 -399.535 -399.535] [71.921], Avg: [-455.280 -455.280 -455.280] (0.0100) ({r_i: None, r_t: [-835.129 -835.129 -835.129], eps: 0.01})
Step:   37600, Reward: [-394.164 -394.164 -394.164] [63.249], Avg: [-434.302 -434.302 -434.302] (0.1000) ({r_i: None, r_t: [-768.865 -768.865 -768.865], eps: 0.1})
Step:   70000, Reward: [-442.869 -442.869 -442.869] [70.281], Avg: [-460.549 -460.549 -460.549] (0.0100) ({r_i: None, r_t: [-908.893 -908.893 -908.893], eps: 0.01})
Step:  139100, Reward: [-438.750 -438.750 -438.750] [98.065], Avg: [-455.268 -455.268 -455.268] (0.0100) ({r_i: None, r_t: [-877.458 -877.458 -877.458], eps: 0.01})
Step:   37700, Reward: [-405.335 -405.335 -405.335] [57.759], Avg: [-434.226 -434.226 -434.226] (0.1000) ({r_i: None, r_t: [-797.311 -797.311 -797.311], eps: 0.1})
Step:   70100, Reward: [-422.251 -422.251 -422.251] [72.450], Avg: [-460.494 -460.494 -460.494] (0.0100) ({r_i: None, r_t: [-886.598 -886.598 -886.598], eps: 0.01})
Step:  139200, Reward: [-476.554 -476.554 -476.554] [120.601], Avg: [-455.283 -455.283 -455.283] (0.0100) ({r_i: None, r_t: [-862.667 -862.667 -862.667], eps: 0.01})
Step:   37800, Reward: [-387.932 -387.932 -387.932] [64.234], Avg: [-434.104 -434.104 -434.104] (0.1000) ({r_i: None, r_t: [-800.067 -800.067 -800.067], eps: 0.1})
Step:  139300, Reward: [-398.230 -398.230 -398.230] [84.320], Avg: [-455.242 -455.242 -455.242] (0.0100) ({r_i: None, r_t: [-857.178 -857.178 -857.178], eps: 0.01})
Step:   70200, Reward: [-449.434 -449.434 -449.434] [62.996], Avg: [-460.478 -460.478 -460.478] (0.0100) ({r_i: None, r_t: [-898.786 -898.786 -898.786], eps: 0.01})
Step:  139400, Reward: [-410.549 -410.549 -410.549] [71.279], Avg: [-455.210 -455.210 -455.210] (0.0100) ({r_i: None, r_t: [-873.634 -873.634 -873.634], eps: 0.01})
Step:   37900, Reward: [-387.725 -387.725 -387.725] [55.695], Avg: [-433.982 -433.982 -433.982] (0.1000) ({r_i: None, r_t: [-789.494 -789.494 -789.494], eps: 0.1})
Step:   70300, Reward: [-437.953 -437.953 -437.953] [80.188], Avg: [-460.446 -460.446 -460.446] (0.0100) ({r_i: None, r_t: [-888.075 -888.075 -888.075], eps: 0.01})
Step:  139500, Reward: [-382.021 -382.021 -382.021] [80.632], Avg: [-455.158 -455.158 -455.158] (0.0100) ({r_i: None, r_t: [-877.281 -877.281 -877.281], eps: 0.01})
Step:   38000, Reward: [-389.032 -389.032 -389.032] [59.662], Avg: [-433.864 -433.864 -433.864] (0.1000) ({r_i: None, r_t: [-838.601 -838.601 -838.601], eps: 0.1})
Step:   70400, Reward: [-423.198 -423.198 -423.198] [71.897], Avg: [-460.394 -460.394 -460.394] (0.0100) ({r_i: None, r_t: [-852.555 -852.555 -852.555], eps: 0.01})
Step:  139600, Reward: [-396.104 -396.104 -396.104] [44.593], Avg: [-455.115 -455.115 -455.115] (0.0100) ({r_i: None, r_t: [-888.759 -888.759 -888.759], eps: 0.01})
Step:   38100, Reward: [-375.031 -375.031 -375.031] [71.238], Avg: [-433.710 -433.710 -433.710] (0.1000) ({r_i: None, r_t: [-806.450 -806.450 -806.450], eps: 0.1})
Step:  139700, Reward: [-452.280 -452.280 -452.280] [119.795], Avg: [-455.113 -455.113 -455.113] (0.0100) ({r_i: None, r_t: [-908.964 -908.964 -908.964], eps: 0.01})
Step:   70500, Reward: [-478.647 -478.647 -478.647] [57.734], Avg: [-460.419 -460.419 -460.419] (0.0100) ({r_i: None, r_t: [-908.730 -908.730 -908.730], eps: 0.01})
Step:  139800, Reward: [-421.677 -421.677 -421.677] [71.062], Avg: [-455.090 -455.090 -455.090] (0.0100) ({r_i: None, r_t: [-865.522 -865.522 -865.522], eps: 0.01})
Step:   38200, Reward: [-424.699 -424.699 -424.699] [54.229], Avg: [-433.686 -433.686 -433.686] (0.1000) ({r_i: None, r_t: [-781.091 -781.091 -781.091], eps: 0.1})
Step:   70600, Reward: [-459.740 -459.740 -459.740] [88.440], Avg: [-460.419 -460.419 -460.419] (0.0100) ({r_i: None, r_t: [-861.470 -861.470 -861.470], eps: 0.01})
Step:  139900, Reward: [-379.323 -379.323 -379.323] [61.470], Avg: [-455.035 -455.035 -455.035] (0.0100) ({r_i: None, r_t: [-880.579 -880.579 -880.579], eps: 0.01})
Step:   38300, Reward: [-438.699 -438.699 -438.699] [87.965], Avg: [-433.699 -433.699 -433.699] (0.1000) ({r_i: None, r_t: [-789.706 -789.706 -789.706], eps: 0.1})
Step:   70700, Reward: [-444.957 -444.957 -444.957] [77.348], Avg: [-460.397 -460.397 -460.397] (0.0100) ({r_i: None, r_t: [-898.076 -898.076 -898.076], eps: 0.01})
Step:  140000, Reward: [-439.836 -439.836 -439.836] [86.709], Avg: [-455.025 -455.025 -455.025] (0.0100) ({r_i: None, r_t: [-870.293 -870.293 -870.293], eps: 0.01})
Step:   38400, Reward: [-388.533 -388.533 -388.533] [80.701], Avg: [-433.582 -433.582 -433.582] (0.1000) ({r_i: None, r_t: [-837.156 -837.156 -837.156], eps: 0.1})
Step:  140100, Reward: [-474.841 -474.841 -474.841] [83.115], Avg: [-455.039 -455.039 -455.039] (0.0100) ({r_i: None, r_t: [-850.776 -850.776 -850.776], eps: 0.01})
Step:   70800, Reward: [-419.833 -419.833 -419.833] [65.244], Avg: [-460.339 -460.339 -460.339] (0.0100) ({r_i: None, r_t: [-898.491 -898.491 -898.491], eps: 0.01})
Step:  140200, Reward: [-440.046 -440.046 -440.046] [79.300], Avg: [-455.028 -455.028 -455.028] (0.0100) ({r_i: None, r_t: [-892.841 -892.841 -892.841], eps: 0.01})
Step:   38500, Reward: [-410.452 -410.452 -410.452] [52.243], Avg: [-433.522 -433.522 -433.522] (0.1000) ({r_i: None, r_t: [-818.488 -818.488 -818.488], eps: 0.1})
Step:   70900, Reward: [-437.191 -437.191 -437.191] [69.575], Avg: [-460.307 -460.307 -460.307] (0.0100) ({r_i: None, r_t: [-905.660 -905.660 -905.660], eps: 0.01})
Step:  140300, Reward: [-428.799 -428.799 -428.799] [75.074], Avg: [-455.009 -455.009 -455.009] (0.0100) ({r_i: None, r_t: [-871.560 -871.560 -871.560], eps: 0.01})
Step:   38600, Reward: [-414.443 -414.443 -414.443] [69.246], Avg: [-433.473 -433.473 -433.473] (0.1000) ({r_i: None, r_t: [-862.391 -862.391 -862.391], eps: 0.1})
Step:   71000, Reward: [-467.230 -467.230 -467.230] [76.779], Avg: [-460.317 -460.317 -460.317] (0.0100) ({r_i: None, r_t: [-871.614 -871.614 -871.614], eps: 0.01})
Step:  140400, Reward: [-380.312 -380.312 -380.312] [74.220], Avg: [-454.956 -454.956 -454.956] (0.0100) ({r_i: None, r_t: [-821.650 -821.650 -821.650], eps: 0.01})
Step:  140500, Reward: [-417.292 -417.292 -417.292] [64.401], Avg: [-454.929 -454.929 -454.929] (0.0100) ({r_i: None, r_t: [-862.261 -862.261 -862.261], eps: 0.01})
Step:   38700, Reward: [-433.698 -433.698 -433.698] [56.780], Avg: [-433.473 -433.473 -433.473] (0.1000) ({r_i: None, r_t: [-844.159 -844.159 -844.159], eps: 0.1})
Step:   71100, Reward: [-457.353 -457.353 -457.353] [50.611], Avg: [-460.312 -460.312 -460.312] (0.0100) ({r_i: None, r_t: [-865.824 -865.824 -865.824], eps: 0.01})
Step:  140600, Reward: [-470.690 -470.690 -470.690] [118.758], Avg: [-454.941 -454.941 -454.941] (0.0100) ({r_i: None, r_t: [-891.979 -891.979 -891.979], eps: 0.01})
Step:   38800, Reward: [-432.682 -432.682 -432.682] [90.405], Avg: [-433.471 -433.471 -433.471] (0.1000) ({r_i: None, r_t: [-814.231 -814.231 -814.231], eps: 0.1})
Step:   71200, Reward: [-424.506 -424.506 -424.506] [54.415], Avg: [-460.262 -460.262 -460.262] (0.0100) ({r_i: None, r_t: [-873.270 -873.270 -873.270], eps: 0.01})
Step:  140700, Reward: [-412.376 -412.376 -412.376] [56.100], Avg: [-454.910 -454.910 -454.910] (0.0100) ({r_i: None, r_t: [-859.614 -859.614 -859.614], eps: 0.01})
Step:   38900, Reward: [-391.540 -391.540 -391.540] [51.389], Avg: [-433.364 -433.364 -433.364] (0.1000) ({r_i: None, r_t: [-813.075 -813.075 -813.075], eps: 0.1})
Step:   71300, Reward: [-427.549 -427.549 -427.549] [56.256], Avg: [-460.216 -460.216 -460.216] (0.0100) ({r_i: None, r_t: [-921.260 -921.260 -921.260], eps: 0.01})
Step:  140800, Reward: [-418.502 -418.502 -418.502] [77.276], Avg: [-454.885 -454.885 -454.885] (0.0100) ({r_i: None, r_t: [-855.468 -855.468 -855.468], eps: 0.01})
Step:  140900, Reward: [-420.540 -420.540 -420.540] [79.681], Avg: [-454.860 -454.860 -454.860] (0.0100) ({r_i: None, r_t: [-862.745 -862.745 -862.745], eps: 0.01})
Step:   39000, Reward: [-415.935 -415.935 -415.935] [61.419], Avg: [-433.319 -433.319 -433.319] (0.1000) ({r_i: None, r_t: [-800.254 -800.254 -800.254], eps: 0.1})
Step:   71400, Reward: [-440.191 -440.191 -440.191] [65.020], Avg: [-460.188 -460.188 -460.188] (0.0100) ({r_i: None, r_t: [-892.597 -892.597 -892.597], eps: 0.01})
Step:  141000, Reward: [-418.909 -418.909 -418.909] [125.067], Avg: [-454.835 -454.835 -454.835] (0.0100) ({r_i: None, r_t: [-825.337 -825.337 -825.337], eps: 0.01})
Step:   39100, Reward: [-382.128 -382.128 -382.128] [76.383], Avg: [-433.188 -433.188 -433.188] (0.1000) ({r_i: None, r_t: [-838.929 -838.929 -838.929], eps: 0.1})
Step:   71500, Reward: [-455.602 -455.602 -455.602] [84.757], Avg: [-460.182 -460.182 -460.182] (0.0100) ({r_i: None, r_t: [-888.560 -888.560 -888.560], eps: 0.01})
Step:  141100, Reward: [-421.524 -421.524 -421.524] [87.196], Avg: [-454.811 -454.811 -454.811] (0.0100) ({r_i: None, r_t: [-863.918 -863.918 -863.918], eps: 0.01})
Step:   39200, Reward: [-415.993 -415.993 -415.993] [103.085], Avg: [-433.145 -433.145 -433.145] (0.1000) ({r_i: None, r_t: [-828.119 -828.119 -828.119], eps: 0.1})
Step:  141200, Reward: [-378.568 -378.568 -378.568] [59.234], Avg: [-454.757 -454.757 -454.757] (0.0100) ({r_i: None, r_t: [-820.524 -820.524 -820.524], eps: 0.01})
Step:   71600, Reward: [-425.290 -425.290 -425.290] [62.481], Avg: [-460.133 -460.133 -460.133] (0.0100) ({r_i: None, r_t: [-856.226 -856.226 -856.226], eps: 0.01})
Step:  141300, Reward: [-449.261 -449.261 -449.261] [108.950], Avg: [-454.753 -454.753 -454.753] (0.0100) ({r_i: None, r_t: [-826.216 -826.216 -826.216], eps: 0.01})
Step:   39300, Reward: [-425.474 -425.474 -425.474] [99.324], Avg: [-433.125 -433.125 -433.125] (0.1000) ({r_i: None, r_t: [-780.742 -780.742 -780.742], eps: 0.1})
Step:   71700, Reward: [-434.541 -434.541 -434.541] [65.731], Avg: [-460.098 -460.098 -460.098] (0.0100) ({r_i: None, r_t: [-916.098 -916.098 -916.098], eps: 0.01})
Step:  141400, Reward: [-434.733 -434.733 -434.733] [95.237], Avg: [-454.739 -454.739 -454.739] (0.0100) ({r_i: None, r_t: [-845.242 -845.242 -845.242], eps: 0.01})
Step:   39400, Reward: [-383.660 -383.660 -383.660] [85.603], Avg: [-433.000 -433.000 -433.000] (0.1000) ({r_i: None, r_t: [-837.465 -837.465 -837.465], eps: 0.1})
Step:   71800, Reward: [-449.146 -449.146 -449.146] [60.719], Avg: [-460.082 -460.082 -460.082] (0.0100) ({r_i: None, r_t: [-896.694 -896.694 -896.694], eps: 0.01})
Step:  141500, Reward: [-391.009 -391.009 -391.009] [67.783], Avg: [-454.694 -454.694 -454.694] (0.0100) ({r_i: None, r_t: [-824.340 -824.340 -824.340], eps: 0.01})
Step:   39500, Reward: [-421.971 -421.971 -421.971] [59.330], Avg: [-432.972 -432.972 -432.972] (0.1000) ({r_i: None, r_t: [-848.397 -848.397 -848.397], eps: 0.1})
Step:  141600, Reward: [-420.176 -420.176 -420.176] [98.616], Avg: [-454.670 -454.670 -454.670] (0.0100) ({r_i: None, r_t: [-835.112 -835.112 -835.112], eps: 0.01})
Step:   71900, Reward: [-444.335 -444.335 -444.335] [73.937], Avg: [-460.061 -460.061 -460.061] (0.0100) ({r_i: None, r_t: [-858.135 -858.135 -858.135], eps: 0.01})
Step:  141700, Reward: [-428.757 -428.757 -428.757] [71.502], Avg: [-454.651 -454.651 -454.651] (0.0100) ({r_i: None, r_t: [-869.521 -869.521 -869.521], eps: 0.01})
Step:   39600, Reward: [-438.384 -438.384 -438.384] [44.083], Avg: [-432.986 -432.986 -432.986] (0.1000) ({r_i: None, r_t: [-832.890 -832.890 -832.890], eps: 0.1})
Step:   72000, Reward: [-435.330 -435.330 -435.330] [77.621], Avg: [-460.026 -460.026 -460.026] (0.0100) ({r_i: None, r_t: [-868.250 -868.250 -868.250], eps: 0.01})
Step:  141800, Reward: [-399.060 -399.060 -399.060] [103.000], Avg: [-454.612 -454.612 -454.612] (0.0100) ({r_i: None, r_t: [-822.749 -822.749 -822.749], eps: 0.01})
Step:   39700, Reward: [-400.076 -400.076 -400.076] [81.943], Avg: [-432.903 -432.903 -432.903] (0.1000) ({r_i: None, r_t: [-826.748 -826.748 -826.748], eps: 0.1})
Step:   72100, Reward: [-464.890 -464.890 -464.890] [74.450], Avg: [-460.033 -460.033 -460.033] (0.0100) ({r_i: None, r_t: [-830.373 -830.373 -830.373], eps: 0.01})
Step:  141900, Reward: [-412.860 -412.860 -412.860] [68.101], Avg: [-454.583 -454.583 -454.583] (0.0100) ({r_i: None, r_t: [-833.994 -833.994 -833.994], eps: 0.01})
Step:   39800, Reward: [-378.352 -378.352 -378.352] [42.832], Avg: [-432.766 -432.766 -432.766] (0.1000) ({r_i: None, r_t: [-792.483 -792.483 -792.483], eps: 0.1})
Step:  142000, Reward: [-431.510 -431.510 -431.510] [94.651], Avg: [-454.567 -454.567 -454.567] (0.0100) ({r_i: None, r_t: [-835.500 -835.500 -835.500], eps: 0.01})
Step:   72200, Reward: [-411.654 -411.654 -411.654] [66.581], Avg: [-459.966 -459.966 -459.966] (0.0100) ({r_i: None, r_t: [-899.508 -899.508 -899.508], eps: 0.01})
Step:  142100, Reward: [-397.268 -397.268 -397.268] [68.387], Avg: [-454.526 -454.526 -454.526] (0.0100) ({r_i: None, r_t: [-833.810 -833.810 -833.810], eps: 0.01})
Step:   39900, Reward: [-407.488 -407.488 -407.488] [66.914], Avg: [-432.703 -432.703 -432.703] (0.1000) ({r_i: None, r_t: [-865.772 -865.772 -865.772], eps: 0.1})
Step:   72300, Reward: [-457.082 -457.082 -457.082] [53.916], Avg: [-459.962 -459.962 -459.962] (0.0100) ({r_i: None, r_t: [-872.030 -872.030 -872.030], eps: 0.01})
Step:  142200, Reward: [-452.429 -452.429 -452.429] [95.246], Avg: [-454.525 -454.525 -454.525] (0.0100) ({r_i: None, r_t: [-826.147 -826.147 -826.147], eps: 0.01})
Step:   40000, Reward: [-401.175 -401.175 -401.175] [67.380], Avg: [-432.625 -432.625 -432.625] (0.1000) ({r_i: None, r_t: [-796.382 -796.382 -796.382], eps: 0.1})
Step:   72400, Reward: [-432.241 -432.241 -432.241] [65.074], Avg: [-459.924 -459.924 -459.924] (0.0100) ({r_i: None, r_t: [-876.443 -876.443 -876.443], eps: 0.01})
Step:  142300, Reward: [-439.798 -439.798 -439.798] [109.534], Avg: [-454.515 -454.515 -454.515] (0.0100) ({r_i: None, r_t: [-862.770 -862.770 -862.770], eps: 0.01})
Step:  142400, Reward: [-402.756 -402.756 -402.756] [79.225], Avg: [-454.478 -454.478 -454.478] (0.0100) ({r_i: None, r_t: [-878.711 -878.711 -878.711], eps: 0.01})
Step:   40100, Reward: [-430.556 -430.556 -430.556] [94.696], Avg: [-432.619 -432.619 -432.619] (0.1000) ({r_i: None, r_t: [-801.079 -801.079 -801.079], eps: 0.1})
Step:   72500, Reward: [-426.568 -426.568 -426.568] [72.340], Avg: [-459.878 -459.878 -459.878] (0.0100) ({r_i: None, r_t: [-870.236 -870.236 -870.236], eps: 0.01})
Step:  142500, Reward: [-427.555 -427.555 -427.555] [87.595], Avg: [-454.459 -454.459 -454.459] (0.0100) ({r_i: None, r_t: [-807.229 -807.229 -807.229], eps: 0.01})
Step:   40200, Reward: [-411.464 -411.464 -411.464] [100.076], Avg: [-432.567 -432.567 -432.567] (0.1000) ({r_i: None, r_t: [-803.319 -803.319 -803.319], eps: 0.1})
Step:   72600, Reward: [-419.186 -419.186 -419.186] [65.074], Avg: [-459.822 -459.822 -459.822] (0.0100) ({r_i: None, r_t: [-846.321 -846.321 -846.321], eps: 0.01})
Step:  142600, Reward: [-437.283 -437.283 -437.283] [80.372], Avg: [-454.447 -454.447 -454.447] (0.0100) ({r_i: None, r_t: [-821.082 -821.082 -821.082], eps: 0.01})
Step:   40300, Reward: [-376.267 -376.267 -376.267] [77.738], Avg: [-432.428 -432.428 -432.428] (0.1000) ({r_i: None, r_t: [-836.851 -836.851 -836.851], eps: 0.1})
Step:   72700, Reward: [-431.716 -431.716 -431.716] [64.075], Avg: [-459.783 -459.783 -459.783] (0.0100) ({r_i: None, r_t: [-872.622 -872.622 -872.622], eps: 0.01})
Step:  142700, Reward: [-413.998 -413.998 -413.998] [66.450], Avg: [-454.419 -454.419 -454.419] (0.0100) ({r_i: None, r_t: [-845.943 -845.943 -845.943], eps: 0.01})
Step:  142800, Reward: [-370.665 -370.665 -370.665] [82.624], Avg: [-454.360 -454.360 -454.360] (0.0100) ({r_i: None, r_t: [-870.776 -870.776 -870.776], eps: 0.01})
Step:   40400, Reward: [-396.019 -396.019 -396.019] [54.266], Avg: [-432.338 -432.338 -432.338] (0.1000) ({r_i: None, r_t: [-802.154 -802.154 -802.154], eps: 0.1})
Step:   72800, Reward: [-449.522 -449.522 -449.522] [55.338], Avg: [-459.769 -459.769 -459.769] (0.0100) ({r_i: None, r_t: [-909.066 -909.066 -909.066], eps: 0.01})
Step:  142900, Reward: [-441.727 -441.727 -441.727] [117.852], Avg: [-454.352 -454.352 -454.352] (0.0100) ({r_i: None, r_t: [-788.913 -788.913 -788.913], eps: 0.01})
Step:   40500, Reward: [-400.566 -400.566 -400.566] [81.765], Avg: [-432.259 -432.259 -432.259] (0.1000) ({r_i: None, r_t: [-856.568 -856.568 -856.568], eps: 0.1})
Step:   72900, Reward: [-444.227 -444.227 -444.227] [65.958], Avg: [-459.748 -459.748 -459.748] (0.0100) ({r_i: None, r_t: [-853.243 -853.243 -853.243], eps: 0.01})
Step:  143000, Reward: [-438.803 -438.803 -438.803] [65.980], Avg: [-454.341 -454.341 -454.341] (0.0100) ({r_i: None, r_t: [-788.408 -788.408 -788.408], eps: 0.01})
Step:   40600, Reward: [-424.106 -424.106 -424.106] [66.372], Avg: [-432.239 -432.239 -432.239] (0.1000) ({r_i: None, r_t: [-807.067 -807.067 -807.067], eps: 0.1})
Step:   73000, Reward: [-454.661 -454.661 -454.661] [68.950], Avg: [-459.741 -459.741 -459.741] (0.0100) ({r_i: None, r_t: [-862.495 -862.495 -862.495], eps: 0.01})
Step:  143100, Reward: [-425.518 -425.518 -425.518] [77.877], Avg: [-454.321 -454.321 -454.321] (0.0100) ({r_i: None, r_t: [-872.968 -872.968 -872.968], eps: 0.01})
Step:  143200, Reward: [-412.146 -412.146 -412.146] [90.404], Avg: [-454.291 -454.291 -454.291] (0.0100) ({r_i: None, r_t: [-840.745 -840.745 -840.745], eps: 0.01})
Step:   40700, Reward: [-417.436 -417.436 -417.436] [84.747], Avg: [-432.203 -432.203 -432.203] (0.1000) ({r_i: None, r_t: [-847.706 -847.706 -847.706], eps: 0.1})
Step:   73100, Reward: [-439.524 -439.524 -439.524] [82.355], Avg: [-459.713 -459.713 -459.713] (0.0100) ({r_i: None, r_t: [-894.470 -894.470 -894.470], eps: 0.01})
Step:  143300, Reward: [-380.893 -380.893 -380.893] [81.674], Avg: [-454.240 -454.240 -454.240] (0.0100) ({r_i: None, r_t: [-851.711 -851.711 -851.711], eps: 0.01})
Step:   40800, Reward: [-382.294 -382.294 -382.294] [57.345], Avg: [-432.081 -432.081 -432.081] (0.1000) ({r_i: None, r_t: [-787.915 -787.915 -787.915], eps: 0.1})
Step:   73200, Reward: [-455.357 -455.357 -455.357] [59.375], Avg: [-459.707 -459.707 -459.707] (0.0100) ({r_i: None, r_t: [-901.298 -901.298 -901.298], eps: 0.01})
Step:  143400, Reward: [-419.026 -419.026 -419.026] [113.430], Avg: [-454.215 -454.215 -454.215] (0.0100) ({r_i: None, r_t: [-779.325 -779.325 -779.325], eps: 0.01})
Step:   40900, Reward: [-416.980 -416.980 -416.980] [69.380], Avg: [-432.044 -432.044 -432.044] (0.1000) ({r_i: None, r_t: [-823.582 -823.582 -823.582], eps: 0.1})
Step:   73300, Reward: [-440.250 -440.250 -440.250] [72.410], Avg: [-459.681 -459.681 -459.681] (0.0100) ({r_i: None, r_t: [-884.826 -884.826 -884.826], eps: 0.01})
Step:  143500, Reward: [-422.816 -422.816 -422.816] [70.884], Avg: [-454.194 -454.194 -454.194] (0.0100) ({r_i: None, r_t: [-810.401 -810.401 -810.401], eps: 0.01})
Step:  143600, Reward: [-412.820 -412.820 -412.820] [88.841], Avg: [-454.165 -454.165 -454.165] (0.0100) ({r_i: None, r_t: [-829.323 -829.323 -829.323], eps: 0.01})
Step:   41000, Reward: [-444.264 -444.264 -444.264] [68.522], Avg: [-432.074 -432.074 -432.074] (0.1000) ({r_i: None, r_t: [-860.245 -860.245 -860.245], eps: 0.1})
Step:   73400, Reward: [-399.920 -399.920 -399.920] [53.845], Avg: [-459.600 -459.600 -459.600] (0.0100) ({r_i: None, r_t: [-890.323 -890.323 -890.323], eps: 0.01})
Step:  143700, Reward: [-421.550 -421.550 -421.550] [73.247], Avg: [-454.142 -454.142 -454.142] (0.0100) ({r_i: None, r_t: [-801.352 -801.352 -801.352], eps: 0.01})
Step:   41100, Reward: [-435.194 -435.194 -435.194] [67.014], Avg: [-432.082 -432.082 -432.082] (0.1000) ({r_i: None, r_t: [-749.151 -749.151 -749.151], eps: 0.1})
Step:   73500, Reward: [-448.200 -448.200 -448.200] [82.594], Avg: [-459.584 -459.584 -459.584] (0.0100) ({r_i: None, r_t: [-852.839 -852.839 -852.839], eps: 0.01})
Step:  143800, Reward: [-404.293 -404.293 -404.293] [68.914], Avg: [-454.107 -454.107 -454.107] (0.0100) ({r_i: None, r_t: [-888.307 -888.307 -888.307], eps: 0.01})
Step:   41200, Reward: [-428.371 -428.371 -428.371] [67.027], Avg: [-432.073 -432.073 -432.073] (0.1000) ({r_i: None, r_t: [-865.740 -865.740 -865.740], eps: 0.1})
Step:  143900, Reward: [-417.351 -417.351 -417.351] [81.505], Avg: [-454.082 -454.082 -454.082] (0.0100) ({r_i: None, r_t: [-829.632 -829.632 -829.632], eps: 0.01})
Step:   73600, Reward: [-438.516 -438.516 -438.516] [57.832], Avg: [-459.556 -459.556 -459.556] (0.0100) ({r_i: None, r_t: [-874.446 -874.446 -874.446], eps: 0.01})
Step:  144000, Reward: [-414.200 -414.200 -414.200] [64.153], Avg: [-454.054 -454.054 -454.054] (0.0100) ({r_i: None, r_t: [-866.936 -866.936 -866.936], eps: 0.01})
Step:   41300, Reward: [-445.618 -445.618 -445.618] [97.999], Avg: [-432.105 -432.105 -432.105] (0.1000) ({r_i: None, r_t: [-792.546 -792.546 -792.546], eps: 0.1})
Step:   73700, Reward: [-441.138 -441.138 -441.138] [72.306], Avg: [-459.531 -459.531 -459.531] (0.0100) ({r_i: None, r_t: [-864.455 -864.455 -864.455], eps: 0.01})
Step:  144100, Reward: [-396.601 -396.601 -396.601] [57.292], Avg: [-454.014 -454.014 -454.014] (0.0100) ({r_i: None, r_t: [-810.009 -810.009 -810.009], eps: 0.01})
Step:   41400, Reward: [-423.992 -423.992 -423.992] [91.173], Avg: [-432.086 -432.086 -432.086] (0.1000) ({r_i: None, r_t: [-773.714 -773.714 -773.714], eps: 0.1})
Step:   73800, Reward: [-457.085 -457.085 -457.085] [84.572], Avg: [-459.527 -459.527 -459.527] (0.0100) ({r_i: None, r_t: [-869.834 -869.834 -869.834], eps: 0.01})
Step:  144200, Reward: [-428.432 -428.432 -428.432] [86.146], Avg: [-453.997 -453.997 -453.997] (0.0100) ({r_i: None, r_t: [-799.644 -799.644 -799.644], eps: 0.01})
Step:   41500, Reward: [-433.716 -433.716 -433.716] [114.236], Avg: [-432.090 -432.090 -432.090] (0.1000) ({r_i: None, r_t: [-832.246 -832.246 -832.246], eps: 0.1})
Step:  144300, Reward: [-452.484 -452.484 -452.484] [95.980], Avg: [-453.996 -453.996 -453.996] (0.0100) ({r_i: None, r_t: [-808.194 -808.194 -808.194], eps: 0.01})
Step:   73900, Reward: [-470.174 -470.174 -470.174] [84.077], Avg: [-459.542 -459.542 -459.542] (0.0100) ({r_i: None, r_t: [-891.920 -891.920 -891.920], eps: 0.01})
Step:  144400, Reward: [-418.963 -418.963 -418.963] [96.884], Avg: [-453.971 -453.971 -453.971] (0.0100) ({r_i: None, r_t: [-844.958 -844.958 -844.958], eps: 0.01})
Step:   41600, Reward: [-400.155 -400.155 -400.155] [73.884], Avg: [-432.013 -432.013 -432.013] (0.1000) ({r_i: None, r_t: [-801.946 -801.946 -801.946], eps: 0.1})
Step:   74000, Reward: [-443.665 -443.665 -443.665] [58.207], Avg: [-459.520 -459.520 -459.520] (0.0100) ({r_i: None, r_t: [-846.096 -846.096 -846.096], eps: 0.01})
Step:  144500, Reward: [-398.110 -398.110 -398.110] [61.010], Avg: [-453.933 -453.933 -453.933] (0.0100) ({r_i: None, r_t: [-821.575 -821.575 -821.575], eps: 0.01})
Step:   41700, Reward: [-390.956 -390.956 -390.956] [60.761], Avg: [-431.915 -431.915 -431.915] (0.1000) ({r_i: None, r_t: [-824.231 -824.231 -824.231], eps: 0.1})
Step:   74100, Reward: [-441.874 -441.874 -441.874] [75.374], Avg: [-459.496 -459.496 -459.496] (0.0100) ({r_i: None, r_t: [-851.201 -851.201 -851.201], eps: 0.01})
Step:  144600, Reward: [-423.024 -423.024 -423.024] [80.342], Avg: [-453.911 -453.911 -453.911] (0.0100) ({r_i: None, r_t: [-927.716 -927.716 -927.716], eps: 0.01})
Step:   41800, Reward: [-404.218 -404.218 -404.218] [73.333], Avg: [-431.849 -431.849 -431.849] (0.1000) ({r_i: None, r_t: [-780.992 -780.992 -780.992], eps: 0.1})
Step:  144700, Reward: [-390.355 -390.355 -390.355] [79.075], Avg: [-453.867 -453.867 -453.867] (0.0100) ({r_i: None, r_t: [-853.876 -853.876 -853.876], eps: 0.01})
Step:   74200, Reward: [-435.268 -435.268 -435.268] [62.897], Avg: [-459.464 -459.464 -459.464] (0.0100) ({r_i: None, r_t: [-876.452 -876.452 -876.452], eps: 0.01})
Step:  144800, Reward: [-424.661 -424.661 -424.661] [90.853], Avg: [-453.847 -453.847 -453.847] (0.0100) ({r_i: None, r_t: [-851.376 -851.376 -851.376], eps: 0.01})
Step:   41900, Reward: [-448.370 -448.370 -448.370] [116.861], Avg: [-431.888 -431.888 -431.888] (0.1000) ({r_i: None, r_t: [-888.359 -888.359 -888.359], eps: 0.1})
Step:   74300, Reward: [-442.696 -442.696 -442.696] [88.289], Avg: [-459.441 -459.441 -459.441] (0.0100) ({r_i: None, r_t: [-904.020 -904.020 -904.020], eps: 0.01})
Step:  144900, Reward: [-431.755 -431.755 -431.755] [96.137], Avg: [-453.832 -453.832 -453.832] (0.0100) ({r_i: None, r_t: [-825.437 -825.437 -825.437], eps: 0.01})
Step:   42000, Reward: [-409.567 -409.567 -409.567] [54.569], Avg: [-431.835 -431.835 -431.835] (0.1000) ({r_i: None, r_t: [-866.850 -866.850 -866.850], eps: 0.1})
Step:   74400, Reward: [-440.381 -440.381 -440.381] [80.263], Avg: [-459.416 -459.416 -459.416] (0.0100) ({r_i: None, r_t: [-863.586 -863.586 -863.586], eps: 0.01})
Step:  145000, Reward: [-432.924 -432.924 -432.924] [88.250], Avg: [-453.818 -453.818 -453.818] (0.0100) ({r_i: None, r_t: [-822.781 -822.781 -822.781], eps: 0.01})
Step:   42100, Reward: [-487.295 -487.295 -487.295] [111.470], Avg: [-431.966 -431.966 -431.966] (0.1000) ({r_i: None, r_t: [-862.522 -862.522 -862.522], eps: 0.1})
Step:  145100, Reward: [-403.264 -403.264 -403.264] [63.311], Avg: [-453.783 -453.783 -453.783] (0.0100) ({r_i: None, r_t: [-803.771 -803.771 -803.771], eps: 0.01})
Step:   74500, Reward: [-427.981 -427.981 -427.981] [69.261], Avg: [-459.374 -459.374 -459.374] (0.0100) ({r_i: None, r_t: [-846.618 -846.618 -846.618], eps: 0.01})
Step:  145200, Reward: [-457.850 -457.850 -457.850] [97.457], Avg: [-453.786 -453.786 -453.786] (0.0100) ({r_i: None, r_t: [-847.330 -847.330 -847.330], eps: 0.01})
Step:   42200, Reward: [-430.415 -430.415 -430.415] [70.241], Avg: [-431.963 -431.963 -431.963] (0.1000) ({r_i: None, r_t: [-813.669 -813.669 -813.669], eps: 0.1})
Step:   74600, Reward: [-434.692 -434.692 -434.692] [77.462], Avg: [-459.341 -459.341 -459.341] (0.0100) ({r_i: None, r_t: [-861.168 -861.168 -861.168], eps: 0.01})
Step:  145300, Reward: [-393.076 -393.076 -393.076] [86.236], Avg: [-453.744 -453.744 -453.744] (0.0100) ({r_i: None, r_t: [-842.090 -842.090 -842.090], eps: 0.01})
Step:   42300, Reward: [-434.400 -434.400 -434.400] [88.625], Avg: [-431.969 -431.969 -431.969] (0.1000) ({r_i: None, r_t: [-834.110 -834.110 -834.110], eps: 0.1})
Step:   74700, Reward: [-447.381 -447.381 -447.381] [53.164], Avg: [-459.325 -459.325 -459.325] (0.0100) ({r_i: None, r_t: [-868.202 -868.202 -868.202], eps: 0.01})
Step:  145400, Reward: [-433.747 -433.747 -433.747] [107.720], Avg: [-453.730 -453.730 -453.730] (0.0100) ({r_i: None, r_t: [-868.306 -868.306 -868.306], eps: 0.01})
Step:   42400, Reward: [-394.212 -394.212 -394.212] [57.636], Avg: [-431.880 -431.880 -431.880] (0.1000) ({r_i: None, r_t: [-823.463 -823.463 -823.463], eps: 0.1})
Step:  145500, Reward: [-376.381 -376.381 -376.381] [93.505], Avg: [-453.677 -453.677 -453.677] (0.0100) ({r_i: None, r_t: [-875.461 -875.461 -875.461], eps: 0.01})
Step:   74800, Reward: [-423.362 -423.362 -423.362] [75.415], Avg: [-459.277 -459.277 -459.277] (0.0100) ({r_i: None, r_t: [-859.415 -859.415 -859.415], eps: 0.01})
Step:  145600, Reward: [-372.210 -372.210 -372.210] [59.605], Avg: [-453.621 -453.621 -453.621] (0.0100) ({r_i: None, r_t: [-783.331 -783.331 -783.331], eps: 0.01})
Step:   42500, Reward: [-421.106 -421.106 -421.106] [84.959], Avg: [-431.854 -431.854 -431.854] (0.1000) ({r_i: None, r_t: [-831.822 -831.822 -831.822], eps: 0.1})
Step:   74900, Reward: [-430.382 -430.382 -430.382] [52.508], Avg: [-459.238 -459.238 -459.238] (0.0100) ({r_i: None, r_t: [-856.084 -856.084 -856.084], eps: 0.01})
Step:  145700, Reward: [-409.564 -409.564 -409.564] [96.456], Avg: [-453.591 -453.591 -453.591] (0.0100) ({r_i: None, r_t: [-852.318 -852.318 -852.318], eps: 0.01})
Step:   42600, Reward: [-442.363 -442.363 -442.363] [86.972], Avg: [-431.879 -431.879 -431.879] (0.1000) ({r_i: None, r_t: [-782.726 -782.726 -782.726], eps: 0.1})
Step:   75000, Reward: [-417.989 -417.989 -417.989] [71.215], Avg: [-459.183 -459.183 -459.183] (0.0100) ({r_i: None, r_t: [-872.161 -872.161 -872.161], eps: 0.01})
Step:  145800, Reward: [-435.140 -435.140 -435.140] [117.054], Avg: [-453.578 -453.578 -453.578] (0.0100) ({r_i: None, r_t: [-849.014 -849.014 -849.014], eps: 0.01})
Step:   42700, Reward: [-395.876 -395.876 -395.876] [69.313], Avg: [-431.795 -431.795 -431.795] (0.1000) ({r_i: None, r_t: [-786.572 -786.572 -786.572], eps: 0.1})
Step:  145900, Reward: [-404.325 -404.325 -404.325] [82.842], Avg: [-453.545 -453.545 -453.545] (0.0100) ({r_i: None, r_t: [-883.767 -883.767 -883.767], eps: 0.01})
Step:   75100, Reward: [-422.279 -422.279 -422.279] [71.187], Avg: [-459.134 -459.134 -459.134] (0.0100) ({r_i: None, r_t: [-849.541 -849.541 -849.541], eps: 0.01})
Step:  146000, Reward: [-442.433 -442.433 -442.433] [104.551], Avg: [-453.537 -453.537 -453.537] (0.0100) ({r_i: None, r_t: [-818.138 -818.138 -818.138], eps: 0.01})
Step:   42800, Reward: [-417.877 -417.877 -417.877] [71.534], Avg: [-431.762 -431.762 -431.762] (0.1000) ({r_i: None, r_t: [-800.250 -800.250 -800.250], eps: 0.1})
Step:   75200, Reward: [-421.858 -421.858 -421.858] [77.033], Avg: [-459.085 -459.085 -459.085] (0.0100) ({r_i: None, r_t: [-827.976 -827.976 -827.976], eps: 0.01})
Step:  146100, Reward: [-370.588 -370.588 -370.588] [80.309], Avg: [-453.480 -453.480 -453.480] (0.0100) ({r_i: None, r_t: [-890.543 -890.543 -890.543], eps: 0.01})
Step:   42900, Reward: [-453.863 -453.863 -453.863] [58.809], Avg: [-431.814 -431.814 -431.814] (0.1000) ({r_i: None, r_t: [-845.738 -845.738 -845.738], eps: 0.1})
Step:   75300, Reward: [-420.806 -420.806 -420.806] [69.351], Avg: [-459.034 -459.034 -459.034] (0.0100) ({r_i: None, r_t: [-846.848 -846.848 -846.848], eps: 0.01})
Step:  146200, Reward: [-417.511 -417.511 -417.511] [89.557], Avg: [-453.456 -453.456 -453.456] (0.0100) ({r_i: None, r_t: [-829.510 -829.510 -829.510], eps: 0.01})
Step:   43000, Reward: [-426.738 -426.738 -426.738] [70.189], Avg: [-431.802 -431.802 -431.802] (0.1000) ({r_i: None, r_t: [-848.761 -848.761 -848.761], eps: 0.1})
Step:  146300, Reward: [-403.930 -403.930 -403.930] [99.691], Avg: [-453.422 -453.422 -453.422] (0.0100) ({r_i: None, r_t: [-859.066 -859.066 -859.066], eps: 0.01})
Step:   75400, Reward: [-434.694 -434.694 -434.694] [69.962], Avg: [-459.002 -459.002 -459.002] (0.0100) ({r_i: None, r_t: [-864.191 -864.191 -864.191], eps: 0.01})
Step:  146400, Reward: [-408.646 -408.646 -408.646] [96.753], Avg: [-453.391 -453.391 -453.391] (0.0100) ({r_i: None, r_t: [-796.106 -796.106 -796.106], eps: 0.01})
Step:   43100, Reward: [-406.306 -406.306 -406.306] [86.098], Avg: [-431.743 -431.743 -431.743] (0.1000) ({r_i: None, r_t: [-844.641 -844.641 -844.641], eps: 0.1})
Step:   75500, Reward: [-450.532 -450.532 -450.532] [64.431], Avg: [-458.990 -458.990 -458.990] (0.0100) ({r_i: None, r_t: [-852.814 -852.814 -852.814], eps: 0.01})
Step:  146500, Reward: [-383.616 -383.616 -383.616] [54.970], Avg: [-453.344 -453.344 -453.344] (0.0100) ({r_i: None, r_t: [-832.214 -832.214 -832.214], eps: 0.01})
Step:   43200, Reward: [-461.293 -461.293 -461.293] [71.425], Avg: [-431.811 -431.811 -431.811] (0.1000) ({r_i: None, r_t: [-846.970 -846.970 -846.970], eps: 0.1})
Step:   75600, Reward: [-431.038 -431.038 -431.038] [68.865], Avg: [-458.953 -458.953 -458.953] (0.0100) ({r_i: None, r_t: [-812.861 -812.861 -812.861], eps: 0.01})
Step:  146600, Reward: [-400.642 -400.642 -400.642] [70.704], Avg: [-453.308 -453.308 -453.308] (0.0100) ({r_i: None, r_t: [-840.767 -840.767 -840.767], eps: 0.01})
Step:   43300, Reward: [-422.413 -422.413 -422.413] [102.513], Avg: [-431.790 -431.790 -431.790] (0.1000) ({r_i: None, r_t: [-834.653 -834.653 -834.653], eps: 0.1})
Step:   75700, Reward: [-402.484 -402.484 -402.484] [56.745], Avg: [-458.879 -458.879 -458.879] (0.0100) ({r_i: None, r_t: [-900.774 -900.774 -900.774], eps: 0.01})
Step:  146700, Reward: [-450.854 -450.854 -450.854] [117.324], Avg: [-453.306 -453.306 -453.306] (0.0100) ({r_i: None, r_t: [-843.458 -843.458 -843.458], eps: 0.01})
Step:  146800, Reward: [-438.253 -438.253 -438.253] [90.589], Avg: [-453.296 -453.296 -453.296] (0.0100) ({r_i: None, r_t: [-810.110 -810.110 -810.110], eps: 0.01})
Step:   43400, Reward: [-415.319 -415.319 -415.319] [70.942], Avg: [-431.752 -431.752 -431.752] (0.1000) ({r_i: None, r_t: [-841.383 -841.383 -841.383], eps: 0.1})
Step:   75800, Reward: [-406.198 -406.198 -406.198] [88.063], Avg: [-458.810 -458.810 -458.810] (0.0100) ({r_i: None, r_t: [-845.914 -845.914 -845.914], eps: 0.01})
Step:  146900, Reward: [-421.779 -421.779 -421.779] [73.829], Avg: [-453.274 -453.274 -453.274] (0.0100) ({r_i: None, r_t: [-850.580 -850.580 -850.580], eps: 0.01})
Step:   43500, Reward: [-423.087 -423.087 -423.087] [70.979], Avg: [-431.732 -431.732 -431.732] (0.1000) ({r_i: None, r_t: [-859.891 -859.891 -859.891], eps: 0.1})
Step:   75900, Reward: [-449.013 -449.013 -449.013] [77.206], Avg: [-458.797 -458.797 -458.797] (0.0100) ({r_i: None, r_t: [-863.760 -863.760 -863.760], eps: 0.01})
Step:  147000, Reward: [-419.010 -419.010 -419.010] [82.991], Avg: [-453.251 -453.251 -453.251] (0.0100) ({r_i: None, r_t: [-817.288 -817.288 -817.288], eps: 0.01})
Step:   43600, Reward: [-462.048 -462.048 -462.048] [57.733], Avg: [-431.801 -431.801 -431.801] (0.1000) ({r_i: None, r_t: [-858.782 -858.782 -858.782], eps: 0.1})
Step:  147100, Reward: [-411.031 -411.031 -411.031] [100.725], Avg: [-453.222 -453.222 -453.222] (0.0100) ({r_i: None, r_t: [-836.175 -836.175 -836.175], eps: 0.01})
Step:   76000, Reward: [-387.556 -387.556 -387.556] [67.569], Avg: [-458.703 -458.703 -458.703] (0.0100) ({r_i: None, r_t: [-874.649 -874.649 -874.649], eps: 0.01})
Step:  147200, Reward: [-412.192 -412.192 -412.192] [104.660], Avg: [-453.194 -453.194 -453.194] (0.0100) ({r_i: None, r_t: [-851.981 -851.981 -851.981], eps: 0.01})
Step:   43700, Reward: [-435.423 -435.423 -435.423] [82.362], Avg: [-431.810 -431.810 -431.810] (0.1000) ({r_i: None, r_t: [-806.673 -806.673 -806.673], eps: 0.1})
Step:   76100, Reward: [-420.097 -420.097 -420.097] [80.189], Avg: [-458.652 -458.652 -458.652] (0.0100) ({r_i: None, r_t: [-838.742 -838.742 -838.742], eps: 0.01})
Step:  147300, Reward: [-406.714 -406.714 -406.714] [73.163], Avg: [-453.163 -453.163 -453.163] (0.0100) ({r_i: None, r_t: [-866.886 -866.886 -866.886], eps: 0.01})
Step:   43800, Reward: [-481.389 -481.389 -481.389] [72.884], Avg: [-431.922 -431.922 -431.922] (0.1000) ({r_i: None, r_t: [-819.140 -819.140 -819.140], eps: 0.1})
Step:   76200, Reward: [-393.339 -393.339 -393.339] [66.150], Avg: [-458.567 -458.567 -458.567] (0.0100) ({r_i: None, r_t: [-842.046 -842.046 -842.046], eps: 0.01})
Step:  147400, Reward: [-425.951 -425.951 -425.951] [104.691], Avg: [-453.144 -453.144 -453.144] (0.0100) ({r_i: None, r_t: [-890.763 -890.763 -890.763], eps: 0.01})
Step:   43900, Reward: [-424.605 -424.605 -424.605] [63.291], Avg: [-431.906 -431.906 -431.906] (0.1000) ({r_i: None, r_t: [-896.009 -896.009 -896.009], eps: 0.1})
Step:  147500, Reward: [-398.469 -398.469 -398.469] [63.531], Avg: [-453.107 -453.107 -453.107] (0.0100) ({r_i: None, r_t: [-749.132 -749.132 -749.132], eps: 0.01})
Step:   76300, Reward: [-421.154 -421.154 -421.154] [64.265], Avg: [-458.518 -458.518 -458.518] (0.0100) ({r_i: None, r_t: [-904.252 -904.252 -904.252], eps: 0.01})
Step:  147600, Reward: [-415.735 -415.735 -415.735] [82.538], Avg: [-453.082 -453.082 -453.082] (0.0100) ({r_i: None, r_t: [-792.296 -792.296 -792.296], eps: 0.01})
Step:   44000, Reward: [-489.913 -489.913 -489.913] [126.712], Avg: [-432.037 -432.037 -432.037] (0.1000) ({r_i: None, r_t: [-868.124 -868.124 -868.124], eps: 0.1})
Step:   76400, Reward: [-424.838 -424.838 -424.838] [66.129], Avg: [-458.474 -458.474 -458.474] (0.0100) ({r_i: None, r_t: [-867.445 -867.445 -867.445], eps: 0.01})
Step:  147700, Reward: [-432.964 -432.964 -432.964] [91.828], Avg: [-453.069 -453.069 -453.069] (0.0100) ({r_i: None, r_t: [-866.480 -866.480 -866.480], eps: 0.01})
Step:   44100, Reward: [-411.480 -411.480 -411.480] [62.849], Avg: [-431.991 -431.991 -431.991] (0.1000) ({r_i: None, r_t: [-831.473 -831.473 -831.473], eps: 0.1})
Step:   76500, Reward: [-466.874 -466.874 -466.874] [60.088], Avg: [-458.485 -458.485 -458.485] (0.0100) ({r_i: None, r_t: [-866.124 -866.124 -866.124], eps: 0.01})
Step:  147800, Reward: [-408.907 -408.907 -408.907] [80.621], Avg: [-453.039 -453.039 -453.039] (0.0100) ({r_i: None, r_t: [-764.397 -764.397 -764.397], eps: 0.01})
Step:  147900, Reward: [-428.711 -428.711 -428.711] [67.537], Avg: [-453.022 -453.022 -453.022] (0.0100) ({r_i: None, r_t: [-849.988 -849.988 -849.988], eps: 0.01})
Step:   44200, Reward: [-440.056 -440.056 -440.056] [97.632], Avg: [-432.009 -432.009 -432.009] (0.1000) ({r_i: None, r_t: [-867.583 -867.583 -867.583], eps: 0.1})
Step:   76600, Reward: [-429.361 -429.361 -429.361] [59.003], Avg: [-458.447 -458.447 -458.447] (0.0100) ({r_i: None, r_t: [-841.735 -841.735 -841.735], eps: 0.01})
Step:  148000, Reward: [-443.726 -443.726 -443.726] [95.102], Avg: [-453.016 -453.016 -453.016] (0.0100) ({r_i: None, r_t: [-847.935 -847.935 -847.935], eps: 0.01})
Step:   44300, Reward: [-416.220 -416.220 -416.220] [67.544], Avg: [-431.974 -431.974 -431.974] (0.1000) ({r_i: None, r_t: [-864.085 -864.085 -864.085], eps: 0.1})
Step:   76700, Reward: [-437.110 -437.110 -437.110] [67.358], Avg: [-458.419 -458.419 -458.419] (0.0100) ({r_i: None, r_t: [-879.510 -879.510 -879.510], eps: 0.01})
Step:  148100, Reward: [-415.770 -415.770 -415.770] [76.956], Avg: [-452.991 -452.991 -452.991] (0.0100) ({r_i: None, r_t: [-855.872 -855.872 -855.872], eps: 0.01})
Step:   44400, Reward: [-421.899 -421.899 -421.899] [107.786], Avg: [-431.951 -431.951 -431.951] (0.1000) ({r_i: None, r_t: [-870.622 -870.622 -870.622], eps: 0.1})
Step:   76800, Reward: [-392.558 -392.558 -392.558] [49.921], Avg: [-458.333 -458.333 -458.333] (0.0100) ({r_i: None, r_t: [-843.952 -843.952 -843.952], eps: 0.01})
Step:  148200, Reward: [-393.464 -393.464 -393.464] [62.741], Avg: [-452.951 -452.951 -452.951] (0.0100) ({r_i: None, r_t: [-811.434 -811.434 -811.434], eps: 0.01})
Step:  148300, Reward: [-454.680 -454.680 -454.680] [113.797], Avg: [-452.952 -452.952 -452.952] (0.0100) ({r_i: None, r_t: [-833.242 -833.242 -833.242], eps: 0.01})
Step:   44500, Reward: [-420.208 -420.208 -420.208] [83.536], Avg: [-431.925 -431.925 -431.925] (0.1000) ({r_i: None, r_t: [-837.179 -837.179 -837.179], eps: 0.1})
Step:   76900, Reward: [-431.289 -431.289 -431.289] [94.576], Avg: [-458.298 -458.298 -458.298] (0.0100) ({r_i: None, r_t: [-863.874 -863.874 -863.874], eps: 0.01})
Step:  148400, Reward: [-427.246 -427.246 -427.246] [99.519], Avg: [-452.935 -452.935 -452.935] (0.0100) ({r_i: None, r_t: [-874.082 -874.082 -874.082], eps: 0.01})
Step:   44600, Reward: [-421.635 -421.635 -421.635] [77.251], Avg: [-431.902 -431.902 -431.902] (0.1000) ({r_i: None, r_t: [-873.692 -873.692 -873.692], eps: 0.1})
Step:   77000, Reward: [-428.030 -428.030 -428.030] [59.978], Avg: [-458.259 -458.259 -458.259] (0.0100) ({r_i: None, r_t: [-866.474 -866.474 -866.474], eps: 0.01})
Step:  148500, Reward: [-439.250 -439.250 -439.250] [69.577], Avg: [-452.925 -452.925 -452.925] (0.0100) ({r_i: None, r_t: [-806.641 -806.641 -806.641], eps: 0.01})
Step:   44700, Reward: [-405.454 -405.454 -405.454] [97.628], Avg: [-431.843 -431.843 -431.843] (0.1000) ({r_i: None, r_t: [-843.533 -843.533 -843.533], eps: 0.1})
Step:   77100, Reward: [-413.722 -413.722 -413.722] [56.413], Avg: [-458.201 -458.201 -458.201] (0.0100) ({r_i: None, r_t: [-871.370 -871.370 -871.370], eps: 0.01})
Step:  148600, Reward: [-445.361 -445.361 -445.361] [104.758], Avg: [-452.920 -452.920 -452.920] (0.0100) ({r_i: None, r_t: [-870.871 -870.871 -870.871], eps: 0.01})
Step:  148700, Reward: [-405.499 -405.499 -405.499] [66.301], Avg: [-452.888 -452.888 -452.888] (0.0100) ({r_i: None, r_t: [-855.319 -855.319 -855.319], eps: 0.01})
Step:   44800, Reward: [-409.645 -409.645 -409.645] [69.123], Avg: [-431.793 -431.793 -431.793] (0.1000) ({r_i: None, r_t: [-857.170 -857.170 -857.170], eps: 0.1})
Step:   77200, Reward: [-446.840 -446.840 -446.840] [51.318], Avg: [-458.187 -458.187 -458.187] (0.0100) ({r_i: None, r_t: [-876.433 -876.433 -876.433], eps: 0.01})
Step:  148800, Reward: [-393.469 -393.469 -393.469] [74.792], Avg: [-452.848 -452.848 -452.848] (0.0100) ({r_i: None, r_t: [-867.379 -867.379 -867.379], eps: 0.01})
Step:   44900, Reward: [-404.077 -404.077 -404.077] [77.080], Avg: [-431.731 -431.731 -431.731] (0.1000) ({r_i: None, r_t: [-886.876 -886.876 -886.876], eps: 0.1})
Step:   77300, Reward: [-425.662 -425.662 -425.662] [57.484], Avg: [-458.145 -458.145 -458.145] (0.0100) ({r_i: None, r_t: [-833.181 -833.181 -833.181], eps: 0.01})
Step:  148900, Reward: [-445.462 -445.462 -445.462] [106.007], Avg: [-452.844 -452.844 -452.844] (0.0100) ({r_i: None, r_t: [-864.734 -864.734 -864.734], eps: 0.01})
Step:   45000, Reward: [-418.112 -418.112 -418.112] [103.705], Avg: [-431.701 -431.701 -431.701] (0.1000) ({r_i: None, r_t: [-890.079 -890.079 -890.079], eps: 0.1})
Step:  149000, Reward: [-424.825 -424.825 -424.825] [124.397], Avg: [-452.825 -452.825 -452.825] (0.0100) ({r_i: None, r_t: [-840.531 -840.531 -840.531], eps: 0.01})
Step:   77400, Reward: [-418.916 -418.916 -418.916] [70.098], Avg: [-458.094 -458.094 -458.094] (0.0100) ({r_i: None, r_t: [-837.212 -837.212 -837.212], eps: 0.01})
Step:  149100, Reward: [-406.837 -406.837 -406.837] [83.412], Avg: [-452.794 -452.794 -452.794] (0.0100) ({r_i: None, r_t: [-848.880 -848.880 -848.880], eps: 0.01})
Step:   45100, Reward: [-405.970 -405.970 -405.970] [89.046], Avg: [-431.644 -431.644 -431.644] (0.1000) ({r_i: None, r_t: [-884.317 -884.317 -884.317], eps: 0.1})
Step:   77500, Reward: [-410.698 -410.698 -410.698] [70.484], Avg: [-458.033 -458.033 -458.033] (0.0100) ({r_i: None, r_t: [-822.044 -822.044 -822.044], eps: 0.01})
Step:  149200, Reward: [-430.611 -430.611 -430.611] [94.616], Avg: [-452.779 -452.779 -452.779] (0.0100) ({r_i: None, r_t: [-840.782 -840.782 -840.782], eps: 0.01})
Step:   45200, Reward: [-483.374 -483.374 -483.374] [113.917], Avg: [-431.759 -431.759 -431.759] (0.1000) ({r_i: None, r_t: [-845.289 -845.289 -845.289], eps: 0.1})
Step:   77600, Reward: [-430.789 -430.789 -430.789] [68.881], Avg: [-457.998 -457.998 -457.998] (0.0100) ({r_i: None, r_t: [-843.194 -843.194 -843.194], eps: 0.01})
Step:  149300, Reward: [-473.270 -473.270 -473.270] [112.501], Avg: [-452.793 -452.793 -452.793] (0.0100) ({r_i: None, r_t: [-903.691 -903.691 -903.691], eps: 0.01})
Step:   45300, Reward: [-419.057 -419.057 -419.057] [64.890], Avg: [-431.731 -431.731 -431.731] (0.1000) ({r_i: None, r_t: [-846.893 -846.893 -846.893], eps: 0.1})
Step:   77700, Reward: [-390.526 -390.526 -390.526] [62.853], Avg: [-457.911 -457.911 -457.911] (0.0100) ({r_i: None, r_t: [-829.586 -829.586 -829.586], eps: 0.01})
Step:  149400, Reward: [-434.224 -434.224 -434.224] [118.155], Avg: [-452.780 -452.780 -452.780] (0.0100) ({r_i: None, r_t: [-809.665 -809.665 -809.665], eps: 0.01})
Step:  149500, Reward: [-427.839 -427.839 -427.839] [90.859], Avg: [-452.764 -452.764 -452.764] (0.0100) ({r_i: None, r_t: [-878.862 -878.862 -878.862], eps: 0.01})
Step:   45400, Reward: [-414.619 -414.619 -414.619] [54.515], Avg: [-431.693 -431.693 -431.693] (0.1000) ({r_i: None, r_t: [-835.561 -835.561 -835.561], eps: 0.1})
Step:   77800, Reward: [-398.010 -398.010 -398.010] [59.828], Avg: [-457.834 -457.834 -457.834] (0.0100) ({r_i: None, r_t: [-816.848 -816.848 -816.848], eps: 0.01})
Step:  149600, Reward: [-435.914 -435.914 -435.914] [88.603], Avg: [-452.752 -452.752 -452.752] (0.0100) ({r_i: None, r_t: [-838.832 -838.832 -838.832], eps: 0.01})
Step:   45500, Reward: [-408.568 -408.568 -408.568] [106.207], Avg: [-431.642 -431.642 -431.642] (0.1000) ({r_i: None, r_t: [-846.075 -846.075 -846.075], eps: 0.1})
Step:   77900, Reward: [-445.227 -445.227 -445.227] [69.030], Avg: [-457.818 -457.818 -457.818] (0.0100) ({r_i: None, r_t: [-835.907 -835.907 -835.907], eps: 0.01})
Step:  149700, Reward: [-413.601 -413.601 -413.601] [129.975], Avg: [-452.726 -452.726 -452.726] (0.0100) ({r_i: None, r_t: [-866.054 -866.054 -866.054], eps: 0.01})
Step:   45600, Reward: [-401.500 -401.500 -401.500] [91.139], Avg: [-431.576 -431.576 -431.576] (0.1000) ({r_i: None, r_t: [-834.119 -834.119 -834.119], eps: 0.1})
Step:  149800, Reward: [-421.760 -421.760 -421.760] [67.958], Avg: [-452.706 -452.706 -452.706] (0.0100) ({r_i: None, r_t: [-841.767 -841.767 -841.767], eps: 0.01})
Step:   78000, Reward: [-402.664 -402.664 -402.664] [67.341], Avg: [-457.747 -457.747 -457.747] (0.0100) ({r_i: None, r_t: [-799.325 -799.325 -799.325], eps: 0.01})
Step:  149900, Reward: [-420.527 -420.527 -420.527] [86.004], Avg: [-452.684 -452.684 -452.684] (0.0100) ({r_i: None, r_t: [-865.038 -865.038 -865.038], eps: 0.01})
Step:   45700, Reward: [-444.469 -444.469 -444.469] [61.820], Avg: [-431.604 -431.604 -431.604] (0.1000) ({r_i: None, r_t: [-820.584 -820.584 -820.584], eps: 0.1})
Step:   78100, Reward: [-404.193 -404.193 -404.193] [82.813], Avg: [-457.679 -457.679 -457.679] (0.0100) ({r_i: None, r_t: [-844.081 -844.081 -844.081], eps: 0.01})
Step:  150000, Reward: [-407.324 -407.324 -407.324] [81.127], Avg: [-452.654 -452.654 -452.654] (0.0100) ({r_i: None, r_t: [-830.148 -830.148 -830.148], eps: 0.01})
Step:   45800, Reward: [-426.821 -426.821 -426.821] [105.446], Avg: [-431.594 -431.594 -431.594] (0.1000) ({r_i: None, r_t: [-823.778 -823.778 -823.778], eps: 0.1})
Step:   78200, Reward: [-392.221 -392.221 -392.221] [52.959], Avg: [-457.595 -457.595 -457.595] (0.0100) ({r_i: None, r_t: [-809.276 -809.276 -809.276], eps: 0.01})
Step:  150100, Reward: [-391.967 -391.967 -391.967] [72.833], Avg: [-452.614 -452.614 -452.614] (0.0100) ({r_i: None, r_t: [-825.073 -825.073 -825.073], eps: 0.01})
Step:   45900, Reward: [-426.919 -426.919 -426.919] [127.427], Avg: [-431.584 -431.584 -431.584] (0.1000) ({r_i: None, r_t: [-884.380 -884.380 -884.380], eps: 0.1})
Step:  150200, Reward: [-399.186 -399.186 -399.186] [58.105], Avg: [-452.578 -452.578 -452.578] (0.0100) ({r_i: None, r_t: [-860.150 -860.150 -860.150], eps: 0.01})
Step:   78300, Reward: [-399.900 -399.900 -399.900] [69.821], Avg: [-457.522 -457.522 -457.522] (0.0100) ({r_i: None, r_t: [-834.717 -834.717 -834.717], eps: 0.01})
Step:  150300, Reward: [-393.243 -393.243 -393.243] [86.303], Avg: [-452.539 -452.539 -452.539] (0.0100) ({r_i: None, r_t: [-804.008 -804.008 -804.008], eps: 0.01})
Step:   46000, Reward: [-410.251 -410.251 -410.251] [107.972], Avg: [-431.538 -431.538 -431.538] (0.1000) ({r_i: None, r_t: [-898.269 -898.269 -898.269], eps: 0.1})
Step:   78400, Reward: [-371.237 -371.237 -371.237] [48.153], Avg: [-457.412 -457.412 -457.412] (0.0100) ({r_i: None, r_t: [-777.797 -777.797 -777.797], eps: 0.01})
Step:  150400, Reward: [-417.732 -417.732 -417.732] [69.204], Avg: [-452.515 -452.515 -452.515] (0.0100) ({r_i: None, r_t: [-797.451 -797.451 -797.451], eps: 0.01})
Step:   46100, Reward: [-405.296 -405.296 -405.296] [79.071], Avg: [-431.481 -431.481 -431.481] (0.1000) ({r_i: None, r_t: [-879.523 -879.523 -879.523], eps: 0.1})
Step:   78500, Reward: [-381.281 -381.281 -381.281] [72.213], Avg: [-457.315 -457.315 -457.315] (0.0100) ({r_i: None, r_t: [-737.650 -737.650 -737.650], eps: 0.01})
Step:  150500, Reward: [-430.179 -430.179 -430.179] [112.312], Avg: [-452.501 -452.501 -452.501] (0.0100) ({r_i: None, r_t: [-895.466 -895.466 -895.466], eps: 0.01})
Step:   46200, Reward: [-420.603 -420.603 -420.603] [91.817], Avg: [-431.457 -431.457 -431.457] (0.1000) ({r_i: None, r_t: [-772.412 -772.412 -772.412], eps: 0.1})
Step:  150600, Reward: [-431.407 -431.407 -431.407] [86.170], Avg: [-452.487 -452.487 -452.487] (0.0100) ({r_i: None, r_t: [-859.885 -859.885 -859.885], eps: 0.01})
Step:   78600, Reward: [-370.954 -370.954 -370.954] [55.839], Avg: [-457.205 -457.205 -457.205] (0.0100) ({r_i: None, r_t: [-777.398 -777.398 -777.398], eps: 0.01})
Step:  150700, Reward: [-401.190 -401.190 -401.190] [74.524], Avg: [-452.453 -452.453 -452.453] (0.0100) ({r_i: None, r_t: [-856.062 -856.062 -856.062], eps: 0.01})
Step:   46300, Reward: [-408.893 -408.893 -408.893] [52.170], Avg: [-431.409 -431.409 -431.409] (0.1000) ({r_i: None, r_t: [-866.777 -866.777 -866.777], eps: 0.1})
Step:   78700, Reward: [-398.391 -398.391 -398.391] [77.207], Avg: [-457.131 -457.131 -457.131] (0.0100) ({r_i: None, r_t: [-794.218 -794.218 -794.218], eps: 0.01})
Step:  150800, Reward: [-438.087 -438.087 -438.087] [63.939], Avg: [-452.443 -452.443 -452.443] (0.0100) ({r_i: None, r_t: [-912.905 -912.905 -912.905], eps: 0.01})
Step:   46400, Reward: [-444.415 -444.415 -444.415] [84.836], Avg: [-431.437 -431.437 -431.437] (0.1000) ({r_i: None, r_t: [-875.924 -875.924 -875.924], eps: 0.1})
Step:   78800, Reward: [-395.935 -395.935 -395.935] [79.708], Avg: [-457.053 -457.053 -457.053] (0.0100) ({r_i: None, r_t: [-772.022 -772.022 -772.022], eps: 0.01})
Step:  150900, Reward: [-375.693 -375.693 -375.693] [57.962], Avg: [-452.392 -452.392 -452.392] (0.0100) ({r_i: None, r_t: [-888.574 -888.574 -888.574], eps: 0.01})
Step:   46500, Reward: [-410.629 -410.629 -410.629] [81.182], Avg: [-431.392 -431.392 -431.392] (0.1000) ({r_i: None, r_t: [-882.048 -882.048 -882.048], eps: 0.1})
Step:  151000, Reward: [-397.289 -397.289 -397.289] [70.040], Avg: [-452.356 -452.356 -452.356] (0.0100) ({r_i: None, r_t: [-856.012 -856.012 -856.012], eps: 0.01})
Step:   78900, Reward: [-354.523 -354.523 -354.523] [76.094], Avg: [-456.923 -456.923 -456.923] (0.0100) ({r_i: None, r_t: [-780.071 -780.071 -780.071], eps: 0.01})
Step:  151100, Reward: [-419.564 -419.564 -419.564] [80.415], Avg: [-452.334 -452.334 -452.334] (0.0100) ({r_i: None, r_t: [-792.350 -792.350 -792.350], eps: 0.01})
Step:   46600, Reward: [-397.440 -397.440 -397.440] [61.578], Avg: [-431.319 -431.319 -431.319] (0.1000) ({r_i: None, r_t: [-820.936 -820.936 -820.936], eps: 0.1})
Step:   79000, Reward: [-388.349 -388.349 -388.349] [55.328], Avg: [-456.836 -456.836 -456.836] (0.0100) ({r_i: None, r_t: [-791.942 -791.942 -791.942], eps: 0.01})
Step:  151200, Reward: [-497.408 -497.408 -497.408] [77.702], Avg: [-452.364 -452.364 -452.364] (0.0100) ({r_i: None, r_t: [-837.336 -837.336 -837.336], eps: 0.01})
Step:   46700, Reward: [-407.971 -407.971 -407.971] [64.623], Avg: [-431.269 -431.269 -431.269] (0.1000) ({r_i: None, r_t: [-851.705 -851.705 -851.705], eps: 0.1})
Step:   79100, Reward: [-397.026 -397.026 -397.026] [73.513], Avg: [-456.761 -456.761 -456.761] (0.0100) ({r_i: None, r_t: [-784.058 -784.058 -784.058], eps: 0.01})
Step:  151300, Reward: [-418.756 -418.756 -418.756] [75.170], Avg: [-452.342 -452.342 -452.342] (0.0100) ({r_i: None, r_t: [-805.720 -805.720 -805.720], eps: 0.01})
Step:   46800, Reward: [-442.955 -442.955 -442.955] [67.622], Avg: [-431.294 -431.294 -431.294] (0.1000) ({r_i: None, r_t: [-838.002 -838.002 -838.002], eps: 0.1})
Step:  151400, Reward: [-446.922 -446.922 -446.922] [102.550], Avg: [-452.338 -452.338 -452.338] (0.0100) ({r_i: None, r_t: [-885.805 -885.805 -885.805], eps: 0.01})
Step:   79200, Reward: [-383.489 -383.489 -383.489] [68.719], Avg: [-456.669 -456.669 -456.669] (0.0100) ({r_i: None, r_t: [-770.909 -770.909 -770.909], eps: 0.01})
Step:  151500, Reward: [-410.126 -410.126 -410.126] [90.400], Avg: [-452.310 -452.310 -452.310] (0.0100) ({r_i: None, r_t: [-840.145 -840.145 -840.145], eps: 0.01})
Step:   46900, Reward: [-433.845 -433.845 -433.845] [58.438], Avg: [-431.300 -431.300 -431.300] (0.1000) ({r_i: None, r_t: [-834.659 -834.659 -834.659], eps: 0.1})
Step:   79300, Reward: [-380.564 -380.564 -380.564] [61.045], Avg: [-456.573 -456.573 -456.573] (0.0100) ({r_i: None, r_t: [-765.949 -765.949 -765.949], eps: 0.01})
Step:  151600, Reward: [-393.604 -393.604 -393.604] [70.841], Avg: [-452.272 -452.272 -452.272] (0.0100) ({r_i: None, r_t: [-819.311 -819.311 -819.311], eps: 0.01})
Step:   47000, Reward: [-420.549 -420.549 -420.549] [77.195], Avg: [-431.277 -431.277 -431.277] (0.1000) ({r_i: None, r_t: [-890.843 -890.843 -890.843], eps: 0.1})
Step:  151700, Reward: [-420.912 -420.912 -420.912] [86.106], Avg: [-452.251 -452.251 -452.251] (0.0100) ({r_i: None, r_t: [-862.645 -862.645 -862.645], eps: 0.01})
Step:   79400, Reward: [-406.492 -406.492 -406.492] [63.098], Avg: [-456.510 -456.510 -456.510] (0.0100) ({r_i: None, r_t: [-778.822 -778.822 -778.822], eps: 0.01})
Step:  151800, Reward: [-447.346 -447.346 -447.346] [96.593], Avg: [-452.248 -452.248 -452.248] (0.0100) ({r_i: None, r_t: [-791.316 -791.316 -791.316], eps: 0.01})
Step:   47100, Reward: [-453.358 -453.358 -453.358] [122.227], Avg: [-431.324 -431.324 -431.324] (0.1000) ({r_i: None, r_t: [-847.208 -847.208 -847.208], eps: 0.1})
Step:   79500, Reward: [-391.745 -391.745 -391.745] [66.062], Avg: [-456.428 -456.428 -456.428] (0.0100) ({r_i: None, r_t: [-732.409 -732.409 -732.409], eps: 0.01})
Step:  151900, Reward: [-508.502 -508.502 -508.502] [99.116], Avg: [-452.285 -452.285 -452.285] (0.0100) ({r_i: None, r_t: [-809.585 -809.585 -809.585], eps: 0.01})
Step:   47200, Reward: [-428.387 -428.387 -428.387] [93.548], Avg: [-431.317 -431.317 -431.317] (0.1000) ({r_i: None, r_t: [-792.185 -792.185 -792.185], eps: 0.1})
Step:   79600, Reward: [-389.220 -389.220 -389.220] [72.406], Avg: [-456.344 -456.344 -456.344] (0.0100) ({r_i: None, r_t: [-774.226 -774.226 -774.226], eps: 0.01})
Step:  152000, Reward: [-409.098 -409.098 -409.098] [85.405], Avg: [-452.256 -452.256 -452.256] (0.0100) ({r_i: None, r_t: [-886.224 -886.224 -886.224], eps: 0.01})
Step:   47300, Reward: [-433.848 -433.848 -433.848] [73.442], Avg: [-431.323 -431.323 -431.323] (0.1000) ({r_i: None, r_t: [-870.777 -870.777 -870.777], eps: 0.1})
Step:  152100, Reward: [-444.867 -444.867 -444.867] [71.434], Avg: [-452.251 -452.251 -452.251] (0.0100) ({r_i: None, r_t: [-817.522 -817.522 -817.522], eps: 0.01})
Step:   79700, Reward: [-403.966 -403.966 -403.966] [97.476], Avg: [-456.278 -456.278 -456.278] (0.0100) ({r_i: None, r_t: [-821.363 -821.363 -821.363], eps: 0.01})
Step:   47400, Reward: [-408.098 -408.098 -408.098] [66.499], Avg: [-431.274 -431.274 -431.274] (0.1000) ({r_i: None, r_t: [-851.077 -851.077 -851.077], eps: 0.1})
Step:  152200, Reward: [-377.669 -377.669 -377.669] [61.486], Avg: [-452.202 -452.202 -452.202] (0.0100) ({r_i: None, r_t: [-849.637 -849.637 -849.637], eps: 0.01})
Step:   79800, Reward: [-372.403 -372.403 -372.403] [75.592], Avg: [-456.173 -456.173 -456.173] (0.0100) ({r_i: None, r_t: [-778.235 -778.235 -778.235], eps: 0.01})
Step:  152300, Reward: [-434.754 -434.754 -434.754] [82.003], Avg: [-452.191 -452.191 -452.191] (0.0100) ({r_i: None, r_t: [-868.483 -868.483 -868.483], eps: 0.01})
Step:   47500, Reward: [-431.206 -431.206 -431.206] [121.508], Avg: [-431.274 -431.274 -431.274] (0.1000) ({r_i: None, r_t: [-797.593 -797.593 -797.593], eps: 0.1})
Step:   79900, Reward: [-390.344 -390.344 -390.344] [57.792], Avg: [-456.091 -456.091 -456.091] (0.0100) ({r_i: None, r_t: [-763.373 -763.373 -763.373], eps: 0.01})
Step:  152400, Reward: [-425.105 -425.105 -425.105] [78.849], Avg: [-452.173 -452.173 -452.173] (0.0100) ({r_i: None, r_t: [-839.116 -839.116 -839.116], eps: 0.01})
Step:   47600, Reward: [-419.533 -419.533 -419.533] [79.680], Avg: [-431.249 -431.249 -431.249] (0.1000) ({r_i: None, r_t: [-794.859 -794.859 -794.859], eps: 0.1})
Step:   80000, Reward: [-375.749 -375.749 -375.749] [70.412], Avg: [-455.991 -455.991 -455.991] (0.0100) ({r_i: None, r_t: [-780.883 -780.883 -780.883], eps: 0.01})
Step:  152500, Reward: [-412.816 -412.816 -412.816] [90.952], Avg: [-452.147 -452.147 -452.147] (0.0100) ({r_i: None, r_t: [-815.597 -815.597 -815.597], eps: 0.01})
Step:   47700, Reward: [-408.263 -408.263 -408.263] [82.933], Avg: [-431.201 -431.201 -431.201] (0.1000) ({r_i: None, r_t: [-830.413 -830.413 -830.413], eps: 0.1})
Step:  152600, Reward: [-401.121 -401.121 -401.121] [63.190], Avg: [-452.114 -452.114 -452.114] (0.0100) ({r_i: None, r_t: [-823.626 -823.626 -823.626], eps: 0.01})
Step:   80100, Reward: [-352.848 -352.848 -352.848] [56.591], Avg: [-455.862 -455.862 -455.862] (0.0100) ({r_i: None, r_t: [-751.017 -751.017 -751.017], eps: 0.01})
Step:  152700, Reward: [-404.242 -404.242 -404.242] [103.088], Avg: [-452.083 -452.083 -452.083] (0.0100) ({r_i: None, r_t: [-792.380 -792.380 -792.380], eps: 0.01})
Step:   47800, Reward: [-390.659 -390.659 -390.659] [62.599], Avg: [-431.116 -431.116 -431.116] (0.1000) ({r_i: None, r_t: [-852.141 -852.141 -852.141], eps: 0.1})
Step:   80200, Reward: [-381.291 -381.291 -381.291] [67.558], Avg: [-455.769 -455.769 -455.769] (0.0100) ({r_i: None, r_t: [-740.268 -740.268 -740.268], eps: 0.01})
Step:  152800, Reward: [-431.480 -431.480 -431.480] [75.179], Avg: [-452.069 -452.069 -452.069] (0.0100) ({r_i: None, r_t: [-866.778 -866.778 -866.778], eps: 0.01})
Step:   47900, Reward: [-438.538 -438.538 -438.538] [99.444], Avg: [-431.132 -431.132 -431.132] (0.1000) ({r_i: None, r_t: [-815.376 -815.376 -815.376], eps: 0.1})
Step:   80300, Reward: [-355.235 -355.235 -355.235] [47.844], Avg: [-455.644 -455.644 -455.644] (0.0100) ({r_i: None, r_t: [-797.026 -797.026 -797.026], eps: 0.01})
Step:  152900, Reward: [-433.231 -433.231 -433.231] [101.108], Avg: [-452.057 -452.057 -452.057] (0.0100) ({r_i: None, r_t: [-875.536 -875.536 -875.536], eps: 0.01})
Step:   48000, Reward: [-430.208 -430.208 -430.208] [66.044], Avg: [-431.130 -431.130 -431.130] (0.1000) ({r_i: None, r_t: [-809.777 -809.777 -809.777], eps: 0.1})
Step:  153000, Reward: [-438.629 -438.629 -438.629] [90.187], Avg: [-452.048 -452.048 -452.048] (0.0100) ({r_i: None, r_t: [-840.682 -840.682 -840.682], eps: 0.01})
Step:   80400, Reward: [-384.103 -384.103 -384.103] [74.413], Avg: [-455.555 -455.555 -455.555] (0.0100) ({r_i: None, r_t: [-750.365 -750.365 -750.365], eps: 0.01})
Step:  153100, Reward: [-423.923 -423.923 -423.923] [92.027], Avg: [-452.030 -452.030 -452.030] (0.0100) ({r_i: None, r_t: [-799.646 -799.646 -799.646], eps: 0.01})
Step:   48100, Reward: [-434.559 -434.559 -434.559] [88.965], Avg: [-431.137 -431.137 -431.137] (0.1000) ({r_i: None, r_t: [-825.754 -825.754 -825.754], eps: 0.1})
Step:   80500, Reward: [-394.871 -394.871 -394.871] [67.522], Avg: [-455.480 -455.480 -455.480] (0.0100) ({r_i: None, r_t: [-749.288 -749.288 -749.288], eps: 0.01})
Step:  153200, Reward: [-423.752 -423.752 -423.752] [103.088], Avg: [-452.011 -452.011 -452.011] (0.0100) ({r_i: None, r_t: [-861.647 -861.647 -861.647], eps: 0.01})
Step:   48200, Reward: [-424.850 -424.850 -424.850] [52.956], Avg: [-431.124 -431.124 -431.124] (0.1000) ({r_i: None, r_t: [-853.890 -853.890 -853.890], eps: 0.1})
Step:   80600, Reward: [-419.950 -419.950 -419.950] [43.374], Avg: [-455.436 -455.436 -455.436] (0.0100) ({r_i: None, r_t: [-777.043 -777.043 -777.043], eps: 0.01})
Step:  153300, Reward: [-468.018 -468.018 -468.018] [100.778], Avg: [-452.022 -452.022 -452.022] (0.0100) ({r_i: None, r_t: [-887.222 -887.222 -887.222], eps: 0.01})
Step:   48300, Reward: [-356.411 -356.411 -356.411] [59.324], Avg: [-430.970 -430.970 -430.970] (0.1000) ({r_i: None, r_t: [-874.836 -874.836 -874.836], eps: 0.1})
Step:  153400, Reward: [-385.525 -385.525 -385.525] [98.261], Avg: [-451.978 -451.978 -451.978] (0.0100) ({r_i: None, r_t: [-817.276 -817.276 -817.276], eps: 0.01})
Step:   80700, Reward: [-373.913 -373.913 -373.913] [74.906], Avg: [-455.335 -455.335 -455.335] (0.0100) ({r_i: None, r_t: [-772.863 -772.863 -772.863], eps: 0.01})
Step:  153500, Reward: [-419.578 -419.578 -419.578] [81.535], Avg: [-451.957 -451.957 -451.957] (0.0100) ({r_i: None, r_t: [-834.184 -834.184 -834.184], eps: 0.01})
Step:   48400, Reward: [-413.725 -413.725 -413.725] [80.544], Avg: [-430.934 -430.934 -430.934] (0.1000) ({r_i: None, r_t: [-861.671 -861.671 -861.671], eps: 0.1})
Step:   80800, Reward: [-376.372 -376.372 -376.372] [70.248], Avg: [-455.238 -455.238 -455.238] (0.0100) ({r_i: None, r_t: [-750.901 -750.901 -750.901], eps: 0.01})
Step:  153600, Reward: [-456.800 -456.800 -456.800] [104.567], Avg: [-451.961 -451.961 -451.961] (0.0100) ({r_i: None, r_t: [-824.977 -824.977 -824.977], eps: 0.01})
Step:   48500, Reward: [-440.230 -440.230 -440.230] [76.346], Avg: [-430.953 -430.953 -430.953] (0.1000) ({r_i: None, r_t: [-905.305 -905.305 -905.305], eps: 0.1})
Step:   80900, Reward: [-382.756 -382.756 -382.756] [61.064], Avg: [-455.148 -455.148 -455.148] (0.0100) ({r_i: None, r_t: [-734.476 -734.476 -734.476], eps: 0.01})
Step:  153700, Reward: [-405.521 -405.521 -405.521] [88.670], Avg: [-451.930 -451.930 -451.930] (0.0100) ({r_i: None, r_t: [-833.191 -833.191 -833.191], eps: 0.01})
Step:   48600, Reward: [-437.686 -437.686 -437.686] [112.476], Avg: [-430.967 -430.967 -430.967] (0.1000) ({r_i: None, r_t: [-872.689 -872.689 -872.689], eps: 0.1})
Step:  153800, Reward: [-439.851 -439.851 -439.851] [99.231], Avg: [-451.922 -451.922 -451.922] (0.0100) ({r_i: None, r_t: [-875.600 -875.600 -875.600], eps: 0.01})
Step:   81000, Reward: [-372.060 -372.060 -372.060] [62.823], Avg: [-455.046 -455.046 -455.046] (0.0100) ({r_i: None, r_t: [-761.204 -761.204 -761.204], eps: 0.01})
Step:  153900, Reward: [-419.095 -419.095 -419.095] [96.816], Avg: [-451.901 -451.901 -451.901] (0.0100) ({r_i: None, r_t: [-840.868 -840.868 -840.868], eps: 0.01})
Step:   48700, Reward: [-433.093 -433.093 -433.093] [67.871], Avg: [-430.971 -430.971 -430.971] (0.1000) ({r_i: None, r_t: [-856.409 -856.409 -856.409], eps: 0.1})
Step:   81100, Reward: [-392.980 -392.980 -392.980] [63.021], Avg: [-454.969 -454.969 -454.969] (0.0100) ({r_i: None, r_t: [-753.924 -753.924 -753.924], eps: 0.01})
Step:  154000, Reward: [-416.708 -416.708 -416.708] [69.771], Avg: [-451.878 -451.878 -451.878] (0.0100) ({r_i: None, r_t: [-860.025 -860.025 -860.025], eps: 0.01})
Step:   48800, Reward: [-413.958 -413.958 -413.958] [67.807], Avg: [-430.937 -430.937 -430.937] (0.1000) ({r_i: None, r_t: [-834.844 -834.844 -834.844], eps: 0.1})
Step:   81200, Reward: [-381.617 -381.617 -381.617] [75.496], Avg: [-454.879 -454.879 -454.879] (0.0100) ({r_i: None, r_t: [-765.067 -765.067 -765.067], eps: 0.01})
Step:  154100, Reward: [-437.209 -437.209 -437.209] [100.895], Avg: [-451.869 -451.869 -451.869] (0.0100) ({r_i: None, r_t: [-807.419 -807.419 -807.419], eps: 0.01})
Step:   48900, Reward: [-445.772 -445.772 -445.772] [64.648], Avg: [-430.967 -430.967 -430.967] (0.1000) ({r_i: None, r_t: [-833.165 -833.165 -833.165], eps: 0.1})
Step:  154200, Reward: [-432.841 -432.841 -432.841] [77.151], Avg: [-451.856 -451.856 -451.856] (0.0100) ({r_i: None, r_t: [-875.213 -875.213 -875.213], eps: 0.01})
Step:   81300, Reward: [-391.735 -391.735 -391.735] [60.539], Avg: [-454.801 -454.801 -454.801] (0.0100) ({r_i: None, r_t: [-783.600 -783.600 -783.600], eps: 0.01})
Step:  154300, Reward: [-441.976 -441.976 -441.976] [75.330], Avg: [-451.850 -451.850 -451.850] (0.0100) ({r_i: None, r_t: [-811.510 -811.510 -811.510], eps: 0.01})
Step:   49000, Reward: [-406.133 -406.133 -406.133] [79.262], Avg: [-430.916 -430.916 -430.916] (0.1000) ({r_i: None, r_t: [-838.348 -838.348 -838.348], eps: 0.1})
Step:   81400, Reward: [-388.974 -388.974 -388.974] [55.952], Avg: [-454.721 -454.721 -454.721] (0.0100) ({r_i: None, r_t: [-788.433 -788.433 -788.433], eps: 0.01})
Step:  154400, Reward: [-427.772 -427.772 -427.772] [80.669], Avg: [-451.834 -451.834 -451.834] (0.0100) ({r_i: None, r_t: [-827.418 -827.418 -827.418], eps: 0.01})
Step:   49100, Reward: [-426.952 -426.952 -426.952] [75.452], Avg: [-430.908 -430.908 -430.908] (0.1000) ({r_i: None, r_t: [-829.431 -829.431 -829.431], eps: 0.1})
Step:   81500, Reward: [-380.497 -380.497 -380.497] [70.789], Avg: [-454.630 -454.630 -454.630] (0.0100) ({r_i: None, r_t: [-791.160 -791.160 -791.160], eps: 0.01})
Step:  154500, Reward: [-377.871 -377.871 -377.871] [70.143], Avg: [-451.787 -451.787 -451.787] (0.0100) ({r_i: None, r_t: [-845.853 -845.853 -845.853], eps: 0.01})
Step:   49200, Reward: [-405.857 -405.857 -405.857] [62.804], Avg: [-430.857 -430.857 -430.857] (0.1000) ({r_i: None, r_t: [-870.225 -870.225 -870.225], eps: 0.1})
Step:  154600, Reward: [-430.634 -430.634 -430.634] [73.846], Avg: [-451.773 -451.773 -451.773] (0.0100) ({r_i: None, r_t: [-836.665 -836.665 -836.665], eps: 0.01})
Step:   81600, Reward: [-395.470 -395.470 -395.470] [56.284], Avg: [-454.557 -454.557 -454.557] (0.0100) ({r_i: None, r_t: [-760.660 -760.660 -760.660], eps: 0.01})
Step:  154700, Reward: [-414.930 -414.930 -414.930] [61.235], Avg: [-451.749 -451.749 -451.749] (0.0100) ({r_i: None, r_t: [-869.396 -869.396 -869.396], eps: 0.01})
Step:   49300, Reward: [-431.723 -431.723 -431.723] [78.642], Avg: [-430.859 -430.859 -430.859] (0.1000) ({r_i: None, r_t: [-815.478 -815.478 -815.478], eps: 0.1})
Step:   81700, Reward: [-386.683 -386.683 -386.683] [63.255], Avg: [-454.474 -454.474 -454.474] (0.0100) ({r_i: None, r_t: [-780.585 -780.585 -780.585], eps: 0.01})
Step:  154800, Reward: [-428.349 -428.349 -428.349] [93.914], Avg: [-451.734 -451.734 -451.734] (0.0100) ({r_i: None, r_t: [-757.013 -757.013 -757.013], eps: 0.01})
Step:   49400, Reward: [-419.199 -419.199 -419.199] [73.445], Avg: [-430.836 -430.836 -430.836] (0.1000) ({r_i: None, r_t: [-873.479 -873.479 -873.479], eps: 0.1})
Step:   81800, Reward: [-390.168 -390.168 -390.168] [64.321], Avg: [-454.396 -454.396 -454.396] (0.0100) ({r_i: None, r_t: [-758.107 -758.107 -758.107], eps: 0.01})
Step:  154900, Reward: [-406.550 -406.550 -406.550] [93.980], Avg: [-451.705 -451.705 -451.705] (0.0100) ({r_i: None, r_t: [-787.508 -787.508 -787.508], eps: 0.01})
Step:   49500, Reward: [-413.634 -413.634 -413.634] [75.766], Avg: [-430.801 -430.801 -430.801] (0.1000) ({r_i: None, r_t: [-861.152 -861.152 -861.152], eps: 0.1})
Step:  155000, Reward: [-441.775 -441.775 -441.775] [87.547], Avg: [-451.699 -451.699 -451.699] (0.0100) ({r_i: None, r_t: [-837.963 -837.963 -837.963], eps: 0.01})
Step:   81900, Reward: [-373.440 -373.440 -373.440] [69.158], Avg: [-454.297 -454.297 -454.297] (0.0100) ({r_i: None, r_t: [-755.514 -755.514 -755.514], eps: 0.01})
Step:  155100, Reward: [-435.257 -435.257 -435.257] [81.721], Avg: [-451.688 -451.688 -451.688] (0.0100) ({r_i: None, r_t: [-875.562 -875.562 -875.562], eps: 0.01})
Step:   49600, Reward: [-432.274 -432.274 -432.274] [61.963], Avg: [-430.804 -430.804 -430.804] (0.1000) ({r_i: None, r_t: [-831.659 -831.659 -831.659], eps: 0.1})
Step:   82000, Reward: [-386.520 -386.520 -386.520] [60.842], Avg: [-454.215 -454.215 -454.215] (0.0100) ({r_i: None, r_t: [-805.525 -805.525 -805.525], eps: 0.01})
Step:  155200, Reward: [-414.869 -414.869 -414.869] [79.192], Avg: [-451.664 -451.664 -451.664] (0.0100) ({r_i: None, r_t: [-831.994 -831.994 -831.994], eps: 0.01})
Step:   49700, Reward: [-440.626 -440.626 -440.626] [74.603], Avg: [-430.824 -430.824 -430.824] (0.1000) ({r_i: None, r_t: [-820.222 -820.222 -820.222], eps: 0.1})
Step:   82100, Reward: [-421.513 -421.513 -421.513] [78.843], Avg: [-454.175 -454.175 -454.175] (0.0100) ({r_i: None, r_t: [-741.918 -741.918 -741.918], eps: 0.01})
Step:  155300, Reward: [-413.105 -413.105 -413.105] [93.388], Avg: [-451.639 -451.639 -451.639] (0.0100) ({r_i: None, r_t: [-851.127 -851.127 -851.127], eps: 0.01})
Step:  155400, Reward: [-410.388 -410.388 -410.388] [89.900], Avg: [-451.613 -451.613 -451.613] (0.0100) ({r_i: None, r_t: [-845.187 -845.187 -845.187], eps: 0.01})
Step:   49800, Reward: [-456.383 -456.383 -456.383] [94.400], Avg: [-430.875 -430.875 -430.875] (0.1000) ({r_i: None, r_t: [-804.823 -804.823 -804.823], eps: 0.1})
Step:   82200, Reward: [-392.248 -392.248 -392.248] [45.175], Avg: [-454.100 -454.100 -454.100] (0.0100) ({r_i: None, r_t: [-808.265 -808.265 -808.265], eps: 0.01})
Step:  155500, Reward: [-387.351 -387.351 -387.351] [81.087], Avg: [-451.572 -451.572 -451.572] (0.0100) ({r_i: None, r_t: [-851.913 -851.913 -851.913], eps: 0.01})
Step:   49900, Reward: [-455.877 -455.877 -455.877] [110.398], Avg: [-430.925 -430.925 -430.925] (0.1000) ({r_i: None, r_t: [-817.853 -817.853 -817.853], eps: 0.1})
Step:   82300, Reward: [-379.864 -379.864 -379.864] [76.348], Avg: [-454.009 -454.009 -454.009] (0.0100) ({r_i: None, r_t: [-776.267 -776.267 -776.267], eps: 0.01})
Step:  155600, Reward: [-395.848 -395.848 -395.848] [57.011], Avg: [-451.536 -451.536 -451.536] (0.0100) ({r_i: None, r_t: [-868.030 -868.030 -868.030], eps: 0.01})
Step:   50000, Reward: [-448.071 -448.071 -448.071] [77.542], Avg: [-430.959 -430.959 -430.959] (0.1000) ({r_i: None, r_t: [-848.464 -848.464 -848.464], eps: 0.1})
Step:   82400, Reward: [-439.326 -439.326 -439.326] [82.165], Avg: [-453.992 -453.992 -453.992] (0.0100) ({r_i: None, r_t: [-799.566 -799.566 -799.566], eps: 0.01})
Step:  155700, Reward: [-423.438 -423.438 -423.438] [41.437], Avg: [-451.518 -451.518 -451.518] (0.0100) ({r_i: None, r_t: [-831.047 -831.047 -831.047], eps: 0.01})
Step:  155800, Reward: [-424.244 -424.244 -424.244] [79.156], Avg: [-451.500 -451.500 -451.500] (0.0100) ({r_i: None, r_t: [-824.454 -824.454 -824.454], eps: 0.01})
Step:   50100, Reward: [-441.822 -441.822 -441.822] [81.028], Avg: [-430.981 -430.981 -430.981] (0.1000) ({r_i: None, r_t: [-807.183 -807.183 -807.183], eps: 0.1})
Step:   82500, Reward: [-381.670 -381.670 -381.670] [56.865], Avg: [-453.904 -453.904 -453.904] (0.0100) ({r_i: None, r_t: [-784.683 -784.683 -784.683], eps: 0.01})
Step:  155900, Reward: [-416.646 -416.646 -416.646] [52.418], Avg: [-451.478 -451.478 -451.478] (0.0100) ({r_i: None, r_t: [-801.120 -801.120 -801.120], eps: 0.01})
Step:   50200, Reward: [-429.289 -429.289 -429.289] [116.697], Avg: [-430.977 -430.977 -430.977] (0.1000) ({r_i: None, r_t: [-808.906 -808.906 -808.906], eps: 0.1})
Step:   82600, Reward: [-412.856 -412.856 -412.856] [72.053], Avg: [-453.854 -453.854 -453.854] (0.0100) ({r_i: None, r_t: [-787.662 -787.662 -787.662], eps: 0.01})
Step:  156000, Reward: [-374.455 -374.455 -374.455] [63.236], Avg: [-451.429 -451.429 -451.429] (0.0100) ({r_i: None, r_t: [-798.346 -798.346 -798.346], eps: 0.01})
Step:   50300, Reward: [-430.070 -430.070 -430.070] [89.924], Avg: [-430.976 -430.976 -430.976] (0.1000) ({r_i: None, r_t: [-850.147 -850.147 -850.147], eps: 0.1})
Step:  156100, Reward: [-409.300 -409.300 -409.300] [83.747], Avg: [-451.402 -451.402 -451.402] (0.0100) ({r_i: None, r_t: [-783.213 -783.213 -783.213], eps: 0.01})
Step:   82700, Reward: [-423.166 -423.166 -423.166] [65.868], Avg: [-453.817 -453.817 -453.817] (0.0100) ({r_i: None, r_t: [-853.127 -853.127 -853.127], eps: 0.01})
Step:  156200, Reward: [-408.431 -408.431 -408.431] [77.560], Avg: [-451.374 -451.374 -451.374] (0.0100) ({r_i: None, r_t: [-880.488 -880.488 -880.488], eps: 0.01})
Step:   50400, Reward: [-405.250 -405.250 -405.250] [86.735], Avg: [-430.925 -430.925 -430.925] (0.1000) ({r_i: None, r_t: [-845.867 -845.867 -845.867], eps: 0.1})
Step:   82800, Reward: [-380.619 -380.619 -380.619] [74.530], Avg: [-453.729 -453.729 -453.729] (0.0100) ({r_i: None, r_t: [-809.159 -809.159 -809.159], eps: 0.01})
Step:  156300, Reward: [-396.585 -396.585 -396.585] [79.816], Avg: [-451.339 -451.339 -451.339] (0.0100) ({r_i: None, r_t: [-801.618 -801.618 -801.618], eps: 0.01})
Step:   50500, Reward: [-384.810 -384.810 -384.810] [81.028], Avg: [-430.834 -430.834 -430.834] (0.1000) ({r_i: None, r_t: [-843.443 -843.443 -843.443], eps: 0.1})
Step:   82900, Reward: [-402.046 -402.046 -402.046] [70.671], Avg: [-453.667 -453.667 -453.667] (0.0100) ({r_i: None, r_t: [-842.016 -842.016 -842.016], eps: 0.01})
Step:  156400, Reward: [-412.580 -412.580 -412.580] [66.955], Avg: [-451.314 -451.314 -451.314] (0.0100) ({r_i: None, r_t: [-807.637 -807.637 -807.637], eps: 0.01})
Step:   50600, Reward: [-400.679 -400.679 -400.679] [101.446], Avg: [-430.774 -430.774 -430.774] (0.1000) ({r_i: None, r_t: [-845.490 -845.490 -845.490], eps: 0.1})
Step:  156500, Reward: [-414.997 -414.997 -414.997] [101.913], Avg: [-451.291 -451.291 -451.291] (0.0100) ({r_i: None, r_t: [-865.067 -865.067 -865.067], eps: 0.01})
Step:   83000, Reward: [-433.275 -433.275 -433.275] [76.331], Avg: [-453.642 -453.642 -453.642] (0.0100) ({r_i: None, r_t: [-835.699 -835.699 -835.699], eps: 0.01})
Step:  156600, Reward: [-389.699 -389.699 -389.699] [88.588], Avg: [-451.252 -451.252 -451.252] (0.0100) ({r_i: None, r_t: [-887.181 -887.181 -887.181], eps: 0.01})
Step:   50700, Reward: [-396.231 -396.231 -396.231] [73.968], Avg: [-430.706 -430.706 -430.706] (0.1000) ({r_i: None, r_t: [-802.388 -802.388 -802.388], eps: 0.1})
Step:   83100, Reward: [-429.088 -429.088 -429.088] [87.130], Avg: [-453.613 -453.613 -453.613] (0.0100) ({r_i: None, r_t: [-830.369 -830.369 -830.369], eps: 0.01})
Step:  156700, Reward: [-390.444 -390.444 -390.444] [64.305], Avg: [-451.213 -451.213 -451.213] (0.0100) ({r_i: None, r_t: [-833.878 -833.878 -833.878], eps: 0.01})
Step:   50800, Reward: [-452.257 -452.257 -452.257] [83.004], Avg: [-430.748 -430.748 -430.748] (0.1000) ({r_i: None, r_t: [-834.776 -834.776 -834.776], eps: 0.1})
Step:   83200, Reward: [-418.022 -418.022 -418.022] [104.972], Avg: [-453.570 -453.570 -453.570] (0.0100) ({r_i: None, r_t: [-902.331 -902.331 -902.331], eps: 0.01})
Step:  156800, Reward: [-450.067 -450.067 -450.067] [77.662], Avg: [-451.212 -451.212 -451.212] (0.0100) ({r_i: None, r_t: [-864.833 -864.833 -864.833], eps: 0.01})
Step:   50900, Reward: [-380.193 -380.193 -380.193] [47.853], Avg: [-430.649 -430.649 -430.649] (0.1000) ({r_i: None, r_t: [-819.695 -819.695 -819.695], eps: 0.1})
Step:  156900, Reward: [-404.897 -404.897 -404.897] [71.185], Avg: [-451.183 -451.183 -451.183] (0.0100) ({r_i: None, r_t: [-872.516 -872.516 -872.516], eps: 0.01})
Step:   83300, Reward: [-414.015 -414.015 -414.015] [71.938], Avg: [-453.523 -453.523 -453.523] (0.0100) ({r_i: None, r_t: [-863.474 -863.474 -863.474], eps: 0.01})
Step:  157000, Reward: [-425.218 -425.218 -425.218] [79.406], Avg: [-451.166 -451.166 -451.166] (0.0100) ({r_i: None, r_t: [-892.596 -892.596 -892.596], eps: 0.01})
Step:   51000, Reward: [-389.313 -389.313 -389.313] [54.934], Avg: [-430.568 -430.568 -430.568] (0.1000) ({r_i: None, r_t: [-836.159 -836.159 -836.159], eps: 0.1})
Step:   83400, Reward: [-383.644 -383.644 -383.644] [70.579], Avg: [-453.439 -453.439 -453.439] (0.0100) ({r_i: None, r_t: [-846.151 -846.151 -846.151], eps: 0.01})
Step:  157100, Reward: [-422.014 -422.014 -422.014] [83.231], Avg: [-451.148 -451.148 -451.148] (0.0100) ({r_i: None, r_t: [-844.128 -844.128 -844.128], eps: 0.01})
Step:   51100, Reward: [-435.006 -435.006 -435.006] [114.226], Avg: [-430.577 -430.577 -430.577] (0.1000) ({r_i: None, r_t: [-849.780 -849.780 -849.780], eps: 0.1})
Step:   83500, Reward: [-400.349 -400.349 -400.349] [95.326], Avg: [-453.375 -453.375 -453.375] (0.0100) ({r_i: None, r_t: [-822.407 -822.407 -822.407], eps: 0.01})
Step:  157200, Reward: [-438.117 -438.117 -438.117] [77.985], Avg: [-451.139 -451.139 -451.139] (0.0100) ({r_i: None, r_t: [-861.735 -861.735 -861.735], eps: 0.01})
Step:  157300, Reward: [-395.726 -395.726 -395.726] [84.729], Avg: [-451.104 -451.104 -451.104] (0.0100) ({r_i: None, r_t: [-831.320 -831.320 -831.320], eps: 0.01})
Step:   51200, Reward: [-417.458 -417.458 -417.458] [84.524], Avg: [-430.551 -430.551 -430.551] (0.1000) ({r_i: None, r_t: [-834.858 -834.858 -834.858], eps: 0.1})
Step:   83600, Reward: [-382.709 -382.709 -382.709] [66.739], Avg: [-453.291 -453.291 -453.291] (0.0100) ({r_i: None, r_t: [-831.384 -831.384 -831.384], eps: 0.01})
Step:  157400, Reward: [-421.601 -421.601 -421.601] [80.326], Avg: [-451.085 -451.085 -451.085] (0.0100) ({r_i: None, r_t: [-879.698 -879.698 -879.698], eps: 0.01})
Step:   51300, Reward: [-378.421 -378.421 -378.421] [48.942], Avg: [-430.450 -430.450 -430.450] (0.1000) ({r_i: None, r_t: [-852.629 -852.629 -852.629], eps: 0.1})
Step:   83700, Reward: [-395.974 -395.974 -395.974] [127.017], Avg: [-453.223 -453.223 -453.223] (0.0100) ({r_i: None, r_t: [-860.956 -860.956 -860.956], eps: 0.01})
Step:  157500, Reward: [-414.714 -414.714 -414.714] [60.730], Avg: [-451.062 -451.062 -451.062] (0.0100) ({r_i: None, r_t: [-862.184 -862.184 -862.184], eps: 0.01})
Step:   51400, Reward: [-433.655 -433.655 -433.655] [82.772], Avg: [-430.456 -430.456 -430.456] (0.1000) ({r_i: None, r_t: [-828.378 -828.378 -828.378], eps: 0.1})
Step:   83800, Reward: [-465.083 -465.083 -465.083] [94.225], Avg: [-453.237 -453.237 -453.237] (0.0100) ({r_i: None, r_t: [-815.420 -815.420 -815.420], eps: 0.01})
Step:  157600, Reward: [-457.446 -457.446 -457.446] [104.416], Avg: [-451.066 -451.066 -451.066] (0.0100) ({r_i: None, r_t: [-860.892 -860.892 -860.892], eps: 0.01})
Step:  157700, Reward: [-406.721 -406.721 -406.721] [88.076], Avg: [-451.038 -451.038 -451.038] (0.0100) ({r_i: None, r_t: [-827.200 -827.200 -827.200], eps: 0.01})
Step:   51500, Reward: [-408.665 -408.665 -408.665] [78.094], Avg: [-430.414 -430.414 -430.414] (0.1000) ({r_i: None, r_t: [-777.867 -777.867 -777.867], eps: 0.1})
Step:   83900, Reward: [-418.938 -418.938 -418.938] [95.053], Avg: [-453.196 -453.196 -453.196] (0.0100) ({r_i: None, r_t: [-826.978 -826.978 -826.978], eps: 0.01})
Step:  157800, Reward: [-425.136 -425.136 -425.136] [87.982], Avg: [-451.022 -451.022 -451.022] (0.0100) ({r_i: None, r_t: [-835.719 -835.719 -835.719], eps: 0.01})
Step:   51600, Reward: [-429.624 -429.624 -429.624] [122.647], Avg: [-430.413 -430.413 -430.413] (0.1000) ({r_i: None, r_t: [-820.782 -820.782 -820.782], eps: 0.1})
Step:   84000, Reward: [-459.216 -459.216 -459.216] [88.470], Avg: [-453.203 -453.203 -453.203] (0.0100) ({r_i: None, r_t: [-891.116 -891.116 -891.116], eps: 0.01})
Step:  157900, Reward: [-400.825 -400.825 -400.825] [103.697], Avg: [-450.990 -450.990 -450.990] (0.0100) ({r_i: None, r_t: [-820.433 -820.433 -820.433], eps: 0.01})
Step:   51700, Reward: [-405.063 -405.063 -405.063] [70.973], Avg: [-430.364 -430.364 -430.364] (0.1000) ({r_i: None, r_t: [-825.812 -825.812 -825.812], eps: 0.1})
Step:  158000, Reward: [-402.854 -402.854 -402.854] [93.970], Avg: [-450.960 -450.960 -450.960] (0.0100) ({r_i: None, r_t: [-815.269 -815.269 -815.269], eps: 0.01})
Step:   84100, Reward: [-437.267 -437.267 -437.267] [82.619], Avg: [-453.184 -453.184 -453.184] (0.0100) ({r_i: None, r_t: [-848.702 -848.702 -848.702], eps: 0.01})
Step:  158100, Reward: [-426.547 -426.547 -426.547] [80.724], Avg: [-450.944 -450.944 -450.944] (0.0100) ({r_i: None, r_t: [-848.478 -848.478 -848.478], eps: 0.01})
Step:   51800, Reward: [-392.128 -392.128 -392.128] [93.732], Avg: [-430.290 -430.290 -430.290] (0.1000) ({r_i: None, r_t: [-802.865 -802.865 -802.865], eps: 0.1})
Step:   84200, Reward: [-433.530 -433.530 -433.530] [108.654], Avg: [-453.161 -453.161 -453.161] (0.0100) ({r_i: None, r_t: [-903.700 -903.700 -903.700], eps: 0.01})
Step:  158200, Reward: [-426.000 -426.000 -426.000] [84.520], Avg: [-450.929 -450.929 -450.929] (0.0100) ({r_i: None, r_t: [-837.075 -837.075 -837.075], eps: 0.01})
Step:   51900, Reward: [-422.920 -422.920 -422.920] [98.296], Avg: [-430.276 -430.276 -430.276] (0.1000) ({r_i: None, r_t: [-832.150 -832.150 -832.150], eps: 0.1})
Step:   84300, Reward: [-417.138 -417.138 -417.138] [79.440], Avg: [-453.118 -453.118 -453.118] (0.0100) ({r_i: None, r_t: [-878.834 -878.834 -878.834], eps: 0.01})
Step:  158300, Reward: [-427.927 -427.927 -427.927] [112.120], Avg: [-450.914 -450.914 -450.914] (0.0100) ({r_i: None, r_t: [-839.087 -839.087 -839.087], eps: 0.01})
Step:   52000, Reward: [-429.555 -429.555 -429.555] [75.743], Avg: [-430.274 -430.274 -430.274] (0.1000) ({r_i: None, r_t: [-868.012 -868.012 -868.012], eps: 0.1})
Step:   84400, Reward: [-424.978 -424.978 -424.978] [115.211], Avg: [-453.085 -453.085 -453.085] (0.0100) ({r_i: None, r_t: [-890.554 -890.554 -890.554], eps: 0.01})
Step:  158400, Reward: [-393.020 -393.020 -393.020] [75.231], Avg: [-450.878 -450.878 -450.878] (0.0100) ({r_i: None, r_t: [-778.824 -778.824 -778.824], eps: 0.01})
Step:  158500, Reward: [-406.557 -406.557 -406.557] [64.136], Avg: [-450.850 -450.850 -450.850] (0.0100) ({r_i: None, r_t: [-843.915 -843.915 -843.915], eps: 0.01})
Step:   52100, Reward: [-425.874 -425.874 -425.874] [92.261], Avg: [-430.266 -430.266 -430.266] (0.1000) ({r_i: None, r_t: [-847.905 -847.905 -847.905], eps: 0.1})
Step:   84500, Reward: [-453.472 -453.472 -453.472] [150.832], Avg: [-453.085 -453.085 -453.085] (0.0100) ({r_i: None, r_t: [-888.441 -888.441 -888.441], eps: 0.01})
Step:  158600, Reward: [-415.268 -415.268 -415.268] [89.492], Avg: [-450.827 -450.827 -450.827] (0.0100) ({r_i: None, r_t: [-803.486 -803.486 -803.486], eps: 0.01})
Step:   52200, Reward: [-433.058 -433.058 -433.058] [85.414], Avg: [-430.271 -430.271 -430.271] (0.1000) ({r_i: None, r_t: [-795.120 -795.120 -795.120], eps: 0.1})
Step:   84600, Reward: [-408.493 -408.493 -408.493] [81.447], Avg: [-453.033 -453.033 -453.033] (0.0100) ({r_i: None, r_t: [-928.252 -928.252 -928.252], eps: 0.01})
Step:  158700, Reward: [-387.076 -387.076 -387.076] [60.675], Avg: [-450.787 -450.787 -450.787] (0.0100) ({r_i: None, r_t: [-839.632 -839.632 -839.632], eps: 0.01})
Step:   52300, Reward: [-370.911 -370.911 -370.911] [55.023], Avg: [-430.158 -430.158 -430.158] (0.1000) ({r_i: None, r_t: [-819.942 -819.942 -819.942], eps: 0.1})
Step:   84700, Reward: [-519.700 -519.700 -519.700] [108.542], Avg: [-453.111 -453.111 -453.111] (0.0100) ({r_i: None, r_t: [-852.028 -852.028 -852.028], eps: 0.01})
Step:  158800, Reward: [-392.109 -392.109 -392.109] [47.716], Avg: [-450.750 -450.750 -450.750] (0.0100) ({r_i: None, r_t: [-847.615 -847.615 -847.615], eps: 0.01})
Step:  158900, Reward: [-365.937 -365.937 -365.937] [63.645], Avg: [-450.697 -450.697 -450.697] (0.0100) ({r_i: None, r_t: [-898.829 -898.829 -898.829], eps: 0.01})
Step:   52400, Reward: [-406.202 -406.202 -406.202] [75.013], Avg: [-430.112 -430.112 -430.112] (0.1000) ({r_i: None, r_t: [-836.924 -836.924 -836.924], eps: 0.1})
Step:   84800, Reward: [-509.543 -509.543 -509.543] [154.561], Avg: [-453.178 -453.178 -453.178] (0.0100) ({r_i: None, r_t: [-1027.341 -1027.341 -1027.341], eps: 0.01})
Step:  159000, Reward: [-435.579 -435.579 -435.579] [70.660], Avg: [-450.687 -450.687 -450.687] (0.0100) ({r_i: None, r_t: [-834.930 -834.930 -834.930], eps: 0.01})
Step:   52500, Reward: [-427.620 -427.620 -427.620] [75.728], Avg: [-430.108 -430.108 -430.108] (0.1000) ({r_i: None, r_t: [-824.042 -824.042 -824.042], eps: 0.1})
Step:   84900, Reward: [-526.040 -526.040 -526.040] [155.495], Avg: [-453.263 -453.263 -453.263] (0.0100) ({r_i: None, r_t: [-987.031 -987.031 -987.031], eps: 0.01})
Step:  159100, Reward: [-415.746 -415.746 -415.746] [99.551], Avg: [-450.665 -450.665 -450.665] (0.0100) ({r_i: None, r_t: [-821.674 -821.674 -821.674], eps: 0.01})
Step:   52600, Reward: [-416.148 -416.148 -416.148] [51.103], Avg: [-430.081 -430.081 -430.081] (0.1000) ({r_i: None, r_t: [-809.328 -809.328 -809.328], eps: 0.1})
Step:   85000, Reward: [-390.554 -390.554 -390.554] [77.821], Avg: [-453.190 -453.190 -453.190] (0.0100) ({r_i: None, r_t: [-1116.694 -1116.694 -1116.694], eps: 0.01})
Step:  159200, Reward: [-403.318 -403.318 -403.318] [87.986], Avg: [-450.636 -450.636 -450.636] (0.0100) ({r_i: None, r_t: [-863.208 -863.208 -863.208], eps: 0.01})
Step:  159300, Reward: [-446.284 -446.284 -446.284] [98.170], Avg: [-450.633 -450.633 -450.633] (0.0100) ({r_i: None, r_t: [-818.859 -818.859 -818.859], eps: 0.01})
Step:   52700, Reward: [-436.597 -436.597 -436.597] [98.440], Avg: [-430.093 -430.093 -430.093] (0.1000) ({r_i: None, r_t: [-857.964 -857.964 -857.964], eps: 0.1})
Step:   85100, Reward: [-587.488 -587.488 -587.488] [194.934], Avg: [-453.347 -453.347 -453.347] (0.0100) ({r_i: None, r_t: [-1049.205 -1049.205 -1049.205], eps: 0.01})
Step:  159400, Reward: [-401.578 -401.578 -401.578] [76.913], Avg: [-450.602 -450.602 -450.602] (0.0100) ({r_i: None, r_t: [-823.125 -823.125 -823.125], eps: 0.01})
Step:   52800, Reward: [-436.166 -436.166 -436.166] [69.327], Avg: [-430.105 -430.105 -430.105] (0.1000) ({r_i: None, r_t: [-855.521 -855.521 -855.521], eps: 0.1})
Step:   85200, Reward: [-454.162 -454.162 -454.162] [125.623], Avg: [-453.348 -453.348 -453.348] (0.0100) ({r_i: None, r_t: [-1214.135 -1214.135 -1214.135], eps: 0.01})
Step:  159500, Reward: [-421.031 -421.031 -421.031] [66.309], Avg: [-450.584 -450.584 -450.584] (0.0100) ({r_i: None, r_t: [-878.080 -878.080 -878.080], eps: 0.01})
Step:   52900, Reward: [-396.952 -396.952 -396.952] [84.320], Avg: [-430.042 -430.042 -430.042] (0.1000) ({r_i: None, r_t: [-840.410 -840.410 -840.410], eps: 0.1})
Step:   85300, Reward: [-560.008 -560.008 -560.008] [134.723], Avg: [-453.473 -453.473 -453.473] (0.0100) ({r_i: None, r_t: [-1116.387 -1116.387 -1116.387], eps: 0.01})
Step:  159600, Reward: [-406.868 -406.868 -406.868] [90.365], Avg: [-450.556 -450.556 -450.556] (0.0100) ({r_i: None, r_t: [-822.659 -822.659 -822.659], eps: 0.01})
Step:  159700, Reward: [-441.541 -441.541 -441.541] [96.238], Avg: [-450.551 -450.551 -450.551] (0.0100) ({r_i: None, r_t: [-797.423 -797.423 -797.423], eps: 0.01})
Step:   53000, Reward: [-438.894 -438.894 -438.894] [93.117], Avg: [-430.059 -430.059 -430.059] (0.1000) ({r_i: None, r_t: [-810.546 -810.546 -810.546], eps: 0.1})
Step:   85400, Reward: [-510.259 -510.259 -510.259] [90.234], Avg: [-453.540 -453.540 -453.540] (0.0100) ({r_i: None, r_t: [-981.198 -981.198 -981.198], eps: 0.01})
Step:  159800, Reward: [-414.571 -414.571 -414.571] [74.604], Avg: [-450.528 -450.528 -450.528] (0.0100) ({r_i: None, r_t: [-826.146 -826.146 -826.146], eps: 0.01})
Step:   53100, Reward: [-432.835 -432.835 -432.835] [95.784], Avg: [-430.064 -430.064 -430.064] (0.1000) ({r_i: None, r_t: [-858.385 -858.385 -858.385], eps: 0.1})
Step:   85500, Reward: [-579.028 -579.028 -579.028] [202.242], Avg: [-453.686 -453.686 -453.686] (0.0100) ({r_i: None, r_t: [-1140.395 -1140.395 -1140.395], eps: 0.01})
Step:  159900, Reward: [-398.007 -398.007 -398.007] [93.548], Avg: [-450.495 -450.495 -450.495] (0.0100) ({r_i: None, r_t: [-815.498 -815.498 -815.498], eps: 0.01})
Step:   53200, Reward: [-418.618 -418.618 -418.618] [81.812], Avg: [-430.043 -430.043 -430.043] (0.1000) ({r_i: None, r_t: [-849.747 -849.747 -849.747], eps: 0.1})
Step:   85600, Reward: [-690.563 -690.563 -690.563] [284.774], Avg: [-453.963 -453.963 -453.963] (0.0100) ({r_i: None, r_t: [-1053.399 -1053.399 -1053.399], eps: 0.01})
Step:  160000, Reward: [-426.090 -426.090 -426.090] [56.220], Avg: [-450.480 -450.480 -450.480] (0.0100) ({r_i: None, r_t: [-840.358 -840.358 -840.358], eps: 0.01})
Step:  160100, Reward: [-412.576 -412.576 -412.576] [76.907], Avg: [-450.456 -450.456 -450.456] (0.0100) ({r_i: None, r_t: [-827.109 -827.109 -827.109], eps: 0.01})
Step:   53300, Reward: [-452.772 -452.772 -452.772] [56.309], Avg: [-430.085 -430.085 -430.085] (0.1000) ({r_i: None, r_t: [-826.599 -826.599 -826.599], eps: 0.1})
Step:   85700, Reward: [-498.798 -498.798 -498.798] [227.073], Avg: [-454.015 -454.015 -454.015] (0.0100) ({r_i: None, r_t: [-1310.583 -1310.583 -1310.583], eps: 0.01})
Step:  160200, Reward: [-425.431 -425.431 -425.431] [82.896], Avg: [-450.441 -450.441 -450.441] (0.0100) ({r_i: None, r_t: [-832.313 -832.313 -832.313], eps: 0.01})
Step:   53400, Reward: [-381.941 -381.941 -381.941] [54.912], Avg: [-429.995 -429.995 -429.995] (0.1000) ({r_i: None, r_t: [-793.719 -793.719 -793.719], eps: 0.1})
Step:   85800, Reward: [-572.113 -572.113 -572.113] [284.346], Avg: [-454.152 -454.152 -454.152] (0.0100) ({r_i: None, r_t: [-1191.891 -1191.891 -1191.891], eps: 0.01})
Step:  160300, Reward: [-425.946 -425.946 -425.946] [91.267], Avg: [-450.425 -450.425 -450.425] (0.0100) ({r_i: None, r_t: [-776.770 -776.770 -776.770], eps: 0.01})
Step:   53500, Reward: [-426.384 -426.384 -426.384] [69.841], Avg: [-429.989 -429.989 -429.989] (0.1000) ({r_i: None, r_t: [-829.572 -829.572 -829.572], eps: 0.1})
Step:   85900, Reward: [-475.584 -475.584 -475.584] [153.659], Avg: [-454.177 -454.177 -454.177] (0.0100) ({r_i: None, r_t: [-1353.599 -1353.599 -1353.599], eps: 0.01})
Step:  160400, Reward: [-447.258 -447.258 -447.258] [111.102], Avg: [-450.423 -450.423 -450.423] (0.0100) ({r_i: None, r_t: [-798.749 -798.749 -798.749], eps: 0.01})
Step:  160500, Reward: [-388.080 -388.080 -388.080] [88.098], Avg: [-450.385 -450.385 -450.385] (0.0100) ({r_i: None, r_t: [-825.048 -825.048 -825.048], eps: 0.01})
Step:   53600, Reward: [-412.056 -412.056 -412.056] [85.849], Avg: [-429.955 -429.955 -429.955] (0.1000) ({r_i: None, r_t: [-869.013 -869.013 -869.013], eps: 0.1})
Step:   86000, Reward: [-589.475 -589.475 -589.475] [294.037], Avg: [-454.334 -454.334 -454.334] (0.0100) ({r_i: None, r_t: [-1185.943 -1185.943 -1185.943], eps: 0.01})
Step:  160600, Reward: [-421.250 -421.250 -421.250] [123.120], Avg: [-450.366 -450.366 -450.366] (0.0100) ({r_i: None, r_t: [-832.981 -832.981 -832.981], eps: 0.01})
Step:   53700, Reward: [-407.362 -407.362 -407.362] [64.274], Avg: [-429.913 -429.913 -429.913] (0.1000) ({r_i: None, r_t: [-813.369 -813.369 -813.369], eps: 0.1})
Step:   86100, Reward: [-595.010 -595.010 -595.010] [203.601], Avg: [-454.498 -454.498 -454.498] (0.0100) ({r_i: None, r_t: [-1205.168 -1205.168 -1205.168], eps: 0.01})
Step:  160700, Reward: [-408.280 -408.280 -408.280] [95.937], Avg: [-450.340 -450.340 -450.340] (0.0100) ({r_i: None, r_t: [-880.292 -880.292 -880.292], eps: 0.01})
Step:   53800, Reward: [-446.973 -446.973 -446.973] [61.127], Avg: [-429.945 -429.945 -429.945] (0.1000) ({r_i: None, r_t: [-836.550 -836.550 -836.550], eps: 0.1})
Step:  160800, Reward: [-398.332 -398.332 -398.332] [90.325], Avg: [-450.308 -450.308 -450.308] (0.0100) ({r_i: None, r_t: [-822.459 -822.459 -822.459], eps: 0.01})
Step:   86200, Reward: [-539.587 -539.587 -539.587] [187.031], Avg: [-454.596 -454.596 -454.596] (0.0100) ({r_i: None, r_t: [-1077.624 -1077.624 -1077.624], eps: 0.01})
Step:  160900, Reward: [-420.067 -420.067 -420.067] [108.545], Avg: [-450.289 -450.289 -450.289] (0.0100) ({r_i: None, r_t: [-811.775 -811.775 -811.775], eps: 0.01})
Step:   53900, Reward: [-422.949 -422.949 -422.949] [65.213], Avg: [-429.932 -429.932 -429.932] (0.1000) ({r_i: None, r_t: [-839.894 -839.894 -839.894], eps: 0.1})
Step:   86300, Reward: [-722.984 -722.984 -722.984] [384.041], Avg: [-454.907 -454.907 -454.907] (0.0100) ({r_i: None, r_t: [-1037.898 -1037.898 -1037.898], eps: 0.01})
Step:  161000, Reward: [-418.985 -418.985 -418.985] [62.535], Avg: [-450.270 -450.270 -450.270] (0.0100) ({r_i: None, r_t: [-812.098 -812.098 -812.098], eps: 0.01})
Step:   54000, Reward: [-405.676 -405.676 -405.676] [60.976], Avg: [-429.887 -429.887 -429.887] (0.1000) ({r_i: None, r_t: [-849.055 -849.055 -849.055], eps: 0.1})
Step:   86400, Reward: [-756.231 -756.231 -756.231] [336.190], Avg: [-455.255 -455.255 -455.255] (0.0100) ({r_i: None, r_t: [-1280.612 -1280.612 -1280.612], eps: 0.01})
Step:  161100, Reward: [-411.979 -411.979 -411.979] [97.715], Avg: [-450.246 -450.246 -450.246] (0.0100) ({r_i: None, r_t: [-863.141 -863.141 -863.141], eps: 0.01})
Step:   54100, Reward: [-402.371 -402.371 -402.371] [89.239], Avg: [-429.836 -429.836 -429.836] (0.1000) ({r_i: None, r_t: [-829.890 -829.890 -829.890], eps: 0.1})
Step:  161200, Reward: [-419.960 -419.960 -419.960] [90.093], Avg: [-450.227 -450.227 -450.227] (0.0100) ({r_i: None, r_t: [-837.338 -837.338 -837.338], eps: 0.01})
Step:   86500, Reward: [-610.385 -610.385 -610.385] [291.192], Avg: [-455.434 -455.434 -455.434] (0.0100) ({r_i: None, r_t: [-1299.190 -1299.190 -1299.190], eps: 0.01})
Step:  161300, Reward: [-393.235 -393.235 -393.235] [60.770], Avg: [-450.192 -450.192 -450.192] (0.0100) ({r_i: None, r_t: [-798.753 -798.753 -798.753], eps: 0.01})
Step:   54200, Reward: [-364.394 -364.394 -364.394] [57.737], Avg: [-429.716 -429.716 -429.716] (0.1000) ({r_i: None, r_t: [-838.790 -838.790 -838.790], eps: 0.1})
Step:   86600, Reward: [-594.784 -594.784 -594.784] [233.617], Avg: [-455.595 -455.595 -455.595] (0.0100) ({r_i: None, r_t: [-1217.724 -1217.724 -1217.724], eps: 0.01})
Step:  161400, Reward: [-415.541 -415.541 -415.541] [65.754], Avg: [-450.170 -450.170 -450.170] (0.0100) ({r_i: None, r_t: [-861.899 -861.899 -861.899], eps: 0.01})
Step:   54300, Reward: [-416.053 -416.053 -416.053] [79.522], Avg: [-429.691 -429.691 -429.691] (0.1000) ({r_i: None, r_t: [-814.474 -814.474 -814.474], eps: 0.1})
Step:   86700, Reward: [-525.856 -525.856 -525.856] [151.756], Avg: [-455.676 -455.676 -455.676] (0.0100) ({r_i: None, r_t: [-1211.886 -1211.886 -1211.886], eps: 0.01})
Step:  161500, Reward: [-434.268 -434.268 -434.268] [113.115], Avg: [-450.161 -450.161 -450.161] (0.0100) ({r_i: None, r_t: [-794.831 -794.831 -794.831], eps: 0.01})
Step:   54400, Reward: [-403.421 -403.421 -403.421] [48.106], Avg: [-429.643 -429.643 -429.643] (0.1000) ({r_i: None, r_t: [-799.623 -799.623 -799.623], eps: 0.1})
Step:  161600, Reward: [-432.324 -432.324 -432.324] [114.272], Avg: [-450.150 -450.150 -450.150] (0.0100) ({r_i: None, r_t: [-843.732 -843.732 -843.732], eps: 0.01})
Step:   86800, Reward: [-639.665 -639.665 -639.665] [347.783], Avg: [-455.888 -455.888 -455.888] (0.0100) ({r_i: None, r_t: [-1264.690 -1264.690 -1264.690], eps: 0.01})
Step:   54500, Reward: [-409.450 -409.450 -409.450] [91.100], Avg: [-429.606 -429.606 -429.606] (0.1000) ({r_i: None, r_t: [-785.929 -785.929 -785.929], eps: 0.1})
Step:  161700, Reward: [-415.908 -415.908 -415.908] [85.516], Avg: [-450.128 -450.128 -450.128] (0.0100) ({r_i: None, r_t: [-821.618 -821.618 -821.618], eps: 0.01})
Step:   86900, Reward: [-694.196 -694.196 -694.196] [290.971], Avg: [-456.162 -456.162 -456.162] (0.0100) ({r_i: None, r_t: [-1151.102 -1151.102 -1151.102], eps: 0.01})
Step:  161800, Reward: [-426.077 -426.077 -426.077] [112.190], Avg: [-450.114 -450.114 -450.114] (0.0100) ({r_i: None, r_t: [-861.199 -861.199 -861.199], eps: 0.01})
Step:   54600, Reward: [-381.773 -381.773 -381.773] [59.784], Avg: [-429.518 -429.518 -429.518] (0.1000) ({r_i: None, r_t: [-820.839 -820.839 -820.839], eps: 0.1})
Step:   87000, Reward: [-527.888 -527.888 -527.888] [191.165], Avg: [-456.244 -456.244 -456.244] (0.0100) ({r_i: None, r_t: [-1193.795 -1193.795 -1193.795], eps: 0.01})
Step:  161900, Reward: [-379.091 -379.091 -379.091] [73.763], Avg: [-450.070 -450.070 -450.070] (0.0100) ({r_i: None, r_t: [-826.454 -826.454 -826.454], eps: 0.01})
Step:   54700, Reward: [-425.080 -425.080 -425.080] [75.209], Avg: [-429.510 -429.510 -429.510] (0.1000) ({r_i: None, r_t: [-816.025 -816.025 -816.025], eps: 0.1})
Step:  162000, Reward: [-421.301 -421.301 -421.301] [88.234], Avg: [-450.052 -450.052 -450.052] (0.0100) ({r_i: None, r_t: [-806.605 -806.605 -806.605], eps: 0.01})
Step:   87100, Reward: [-444.152 -444.152 -444.152] [119.220], Avg: [-456.230 -456.230 -456.230] (0.0100) ({r_i: None, r_t: [-992.092 -992.092 -992.092], eps: 0.01})
Step:   54800, Reward: [-394.361 -394.361 -394.361] [56.260], Avg: [-429.446 -429.446 -429.446] (0.1000) ({r_i: None, r_t: [-780.393 -780.393 -780.393], eps: 0.1})
Step:  162100, Reward: [-437.677 -437.677 -437.677] [103.669], Avg: [-450.044 -450.044 -450.044] (0.0100) ({r_i: None, r_t: [-814.334 -814.334 -814.334], eps: 0.01})
Step:   87200, Reward: [-781.940 -781.940 -781.940] [356.743], Avg: [-456.603 -456.603 -456.603] (0.0100) ({r_i: None, r_t: [-1208.606 -1208.606 -1208.606], eps: 0.01})
Step:  162200, Reward: [-423.819 -423.819 -423.819] [109.366], Avg: [-450.028 -450.028 -450.028] (0.0100) ({r_i: None, r_t: [-840.568 -840.568 -840.568], eps: 0.01})
Step:   54900, Reward: [-413.646 -413.646 -413.646] [78.674], Avg: [-429.417 -429.417 -429.417] (0.1000) ({r_i: None, r_t: [-831.262 -831.262 -831.262], eps: 0.1})
Step:   87300, Reward: [-494.746 -494.746 -494.746] [99.402], Avg: [-456.647 -456.647 -456.647] (0.0100) ({r_i: None, r_t: [-1099.661 -1099.661 -1099.661], eps: 0.01})
Step:  162300, Reward: [-423.034 -423.034 -423.034] [71.004], Avg: [-450.012 -450.012 -450.012] (0.0100) ({r_i: None, r_t: [-837.524 -837.524 -837.524], eps: 0.01})
Step:   55000, Reward: [-423.054 -423.054 -423.054] [89.922], Avg: [-429.406 -429.406 -429.406] (0.1000) ({r_i: None, r_t: [-809.354 -809.354 -809.354], eps: 0.1})
Step:  162400, Reward: [-389.441 -389.441 -389.441] [69.768], Avg: [-449.974 -449.974 -449.974] (0.0100) ({r_i: None, r_t: [-848.079 -848.079 -848.079], eps: 0.01})
Step:   87400, Reward: [-528.993 -528.993 -528.993] [146.548], Avg: [-456.730 -456.730 -456.730] (0.0100) ({r_i: None, r_t: [-1113.546 -1113.546 -1113.546], eps: 0.01})
Step:   55100, Reward: [-396.044 -396.044 -396.044] [62.999], Avg: [-429.345 -429.345 -429.345] (0.1000) ({r_i: None, r_t: [-805.813 -805.813 -805.813], eps: 0.1})
Step:  162500, Reward: [-428.862 -428.862 -428.862] [74.596], Avg: [-449.961 -449.961 -449.961] (0.0100) ({r_i: None, r_t: [-813.997 -813.997 -813.997], eps: 0.01})
Step:   87500, Reward: [-501.107 -501.107 -501.107] [156.712], Avg: [-456.780 -456.780 -456.780] (0.0100) ({r_i: None, r_t: [-1135.791 -1135.791 -1135.791], eps: 0.01})
Step:  162600, Reward: [-394.572 -394.572 -394.572] [87.950], Avg: [-449.927 -449.927 -449.927] (0.0100) ({r_i: None, r_t: [-851.441 -851.441 -851.441], eps: 0.01})
Step:   55200, Reward: [-395.952 -395.952 -395.952] [76.181], Avg: [-429.285 -429.285 -429.285] (0.1000) ({r_i: None, r_t: [-830.299 -830.299 -830.299], eps: 0.1})
Step:   87600, Reward: [-568.004 -568.004 -568.004] [212.389], Avg: [-456.907 -456.907 -456.907] (0.0100) ({r_i: None, r_t: [-977.348 -977.348 -977.348], eps: 0.01})
Step:  162700, Reward: [-376.705 -376.705 -376.705] [85.606], Avg: [-449.882 -449.882 -449.882] (0.0100) ({r_i: None, r_t: [-852.952 -852.952 -852.952], eps: 0.01})
Step:   55300, Reward: [-374.351 -374.351 -374.351] [62.084], Avg: [-429.186 -429.186 -429.186] (0.1000) ({r_i: None, r_t: [-835.408 -835.408 -835.408], eps: 0.1})
Step:  162800, Reward: [-402.406 -402.406 -402.406] [83.160], Avg: [-449.853 -449.853 -449.853] (0.0100) ({r_i: None, r_t: [-821.888 -821.888 -821.888], eps: 0.01})
Step:   87700, Reward: [-532.618 -532.618 -532.618] [162.685], Avg: [-456.993 -456.993 -456.993] (0.0100) ({r_i: None, r_t: [-1061.239 -1061.239 -1061.239], eps: 0.01})
Step:  162900, Reward: [-428.186 -428.186 -428.186] [89.401], Avg: [-449.840 -449.840 -449.840] (0.0100) ({r_i: None, r_t: [-841.307 -841.307 -841.307], eps: 0.01})
Step:   55400, Reward: [-372.192 -372.192 -372.192] [63.433], Avg: [-429.083 -429.083 -429.083] (0.1000) ({r_i: None, r_t: [-846.352 -846.352 -846.352], eps: 0.1})
Step:   87800, Reward: [-557.216 -557.216 -557.216] [239.061], Avg: [-457.107 -457.107 -457.107] (0.0100) ({r_i: None, r_t: [-1062.054 -1062.054 -1062.054], eps: 0.01})
Step:  163000, Reward: [-405.500 -405.500 -405.500] [71.570], Avg: [-449.813 -449.813 -449.813] (0.0100) ({r_i: None, r_t: [-843.824 -843.824 -843.824], eps: 0.01})
Step:   55500, Reward: [-417.743 -417.743 -417.743] [76.892], Avg: [-429.063 -429.063 -429.063] (0.1000) ({r_i: None, r_t: [-832.923 -832.923 -832.923], eps: 0.1})
Step:   87900, Reward: [-484.553 -484.553 -484.553] [205.979], Avg: [-457.138 -457.138 -457.138] (0.0100) ({r_i: None, r_t: [-1028.301 -1028.301 -1028.301], eps: 0.01})
Step:  163100, Reward: [-412.538 -412.538 -412.538] [79.713], Avg: [-449.790 -449.790 -449.790] (0.0100) ({r_i: None, r_t: [-826.233 -826.233 -826.233], eps: 0.01})
Step:   55600, Reward: [-383.454 -383.454 -383.454] [56.519], Avg: [-428.981 -428.981 -428.981] (0.1000) ({r_i: None, r_t: [-873.180 -873.180 -873.180], eps: 0.1})
Step:  163200, Reward: [-465.073 -465.073 -465.073] [90.216], Avg: [-449.799 -449.799 -449.799] (0.0100) ({r_i: None, r_t: [-818.189 -818.189 -818.189], eps: 0.01})
Step:   88000, Reward: [-462.079 -462.079 -462.079] [122.530], Avg: [-457.144 -457.144 -457.144] (0.0100) ({r_i: None, r_t: [-1009.539 -1009.539 -1009.539], eps: 0.01})
Step:  163300, Reward: [-368.530 -368.530 -368.530] [83.032], Avg: [-449.749 -449.749 -449.749] (0.0100) ({r_i: None, r_t: [-803.416 -803.416 -803.416], eps: 0.01})
Step:   55700, Reward: [-388.582 -388.582 -388.582] [78.313], Avg: [-428.908 -428.908 -428.908] (0.1000) ({r_i: None, r_t: [-815.366 -815.366 -815.366], eps: 0.1})
Step:   88100, Reward: [-569.698 -569.698 -569.698] [189.211], Avg: [-457.272 -457.272 -457.272] (0.0100) ({r_i: None, r_t: [-1087.268 -1087.268 -1087.268], eps: 0.01})
Step:  163400, Reward: [-402.955 -402.955 -402.955] [79.821], Avg: [-449.721 -449.721 -449.721] (0.0100) ({r_i: None, r_t: [-783.084 -783.084 -783.084], eps: 0.01})
Step:   55800, Reward: [-370.090 -370.090 -370.090] [42.128], Avg: [-428.803 -428.803 -428.803] (0.1000) ({r_i: None, r_t: [-820.031 -820.031 -820.031], eps: 0.1})
Step:   88200, Reward: [-504.451 -504.451 -504.451] [167.923], Avg: [-457.325 -457.325 -457.325] (0.0100) ({r_i: None, r_t: [-974.846 -974.846 -974.846], eps: 0.01})
Step:  163500, Reward: [-403.810 -403.810 -403.810] [84.160], Avg: [-449.693 -449.693 -449.693] (0.0100) ({r_i: None, r_t: [-891.836 -891.836 -891.836], eps: 0.01})
Step:   55900, Reward: [-394.107 -394.107 -394.107] [64.017], Avg: [-428.741 -428.741 -428.741] (0.1000) ({r_i: None, r_t: [-820.832 -820.832 -820.832], eps: 0.1})
Step:  163600, Reward: [-363.486 -363.486 -363.486] [67.909], Avg: [-449.640 -449.640 -449.640] (0.0100) ({r_i: None, r_t: [-805.782 -805.782 -805.782], eps: 0.01})
Step:   88300, Reward: [-638.367 -638.367 -638.367] [209.221], Avg: [-457.530 -457.530 -457.530] (0.0100) ({r_i: None, r_t: [-1168.140 -1168.140 -1168.140], eps: 0.01})
Step:  163700, Reward: [-399.303 -399.303 -399.303] [56.902], Avg: [-449.609 -449.609 -449.609] (0.0100) ({r_i: None, r_t: [-813.651 -813.651 -813.651], eps: 0.01})
Step:   56000, Reward: [-383.568 -383.568 -383.568] [51.681], Avg: [-428.661 -428.661 -428.661] (0.1000) ({r_i: None, r_t: [-784.877 -784.877 -784.877], eps: 0.1})
Step:   88400, Reward: [-512.637 -512.637 -512.637] [136.642], Avg: [-457.592 -457.592 -457.592] (0.0100) ({r_i: None, r_t: [-1053.619 -1053.619 -1053.619], eps: 0.01})
Step:  163800, Reward: [-406.116 -406.116 -406.116] [82.283], Avg: [-449.583 -449.583 -449.583] (0.0100) ({r_i: None, r_t: [-853.986 -853.986 -853.986], eps: 0.01})
Step:   56100, Reward: [-433.903 -433.903 -433.903] [83.900], Avg: [-428.670 -428.670 -428.670] (0.1000) ({r_i: None, r_t: [-828.752 -828.752 -828.752], eps: 0.1})
Step:   88500, Reward: [-662.212 -662.212 -662.212] [280.446], Avg: [-457.823 -457.823 -457.823] (0.0100) ({r_i: None, r_t: [-973.923 -973.923 -973.923], eps: 0.01})
Step:  163900, Reward: [-412.062 -412.062 -412.062] [88.978], Avg: [-449.560 -449.560 -449.560] (0.0100) ({r_i: None, r_t: [-809.976 -809.976 -809.976], eps: 0.01})
Step:   56200, Reward: [-418.398 -418.398 -418.398] [74.267], Avg: [-428.652 -428.652 -428.652] (0.1000) ({r_i: None, r_t: [-757.579 -757.579 -757.579], eps: 0.1})
Step:  164000, Reward: [-369.016 -369.016 -369.016] [66.638], Avg: [-449.511 -449.511 -449.511] (0.0100) ({r_i: None, r_t: [-800.200 -800.200 -800.200], eps: 0.01})
Step:   88600, Reward: [-530.872 -530.872 -530.872] [146.053], Avg: [-457.906 -457.906 -457.906] (0.0100) ({r_i: None, r_t: [-1132.978 -1132.978 -1132.978], eps: 0.01})
Step:  164100, Reward: [-447.263 -447.263 -447.263] [95.312], Avg: [-449.510 -449.510 -449.510] (0.0100) ({r_i: None, r_t: [-859.225 -859.225 -859.225], eps: 0.01})
Step:   56300, Reward: [-416.188 -416.188 -416.188] [82.720], Avg: [-428.630 -428.630 -428.630] (0.1000) ({r_i: None, r_t: [-829.058 -829.058 -829.058], eps: 0.1})
Step:   88700, Reward: [-556.473 -556.473 -556.473] [202.907], Avg: [-458.017 -458.017 -458.017] (0.0100) ({r_i: None, r_t: [-1079.596 -1079.596 -1079.596], eps: 0.01})
Step:  164200, Reward: [-418.215 -418.215 -418.215] [94.860], Avg: [-449.490 -449.490 -449.490] (0.0100) ({r_i: None, r_t: [-875.678 -875.678 -875.678], eps: 0.01})
Step:   56400, Reward: [-422.434 -422.434 -422.434] [115.470], Avg: [-428.619 -428.619 -428.619] (0.1000) ({r_i: None, r_t: [-828.984 -828.984 -828.984], eps: 0.1})
Step:   88800, Reward: [-587.942 -587.942 -587.942] [254.362], Avg: [-458.163 -458.163 -458.163] (0.0100) ({r_i: None, r_t: [-1091.614 -1091.614 -1091.614], eps: 0.01})
Step:  164300, Reward: [-433.156 -433.156 -433.156] [121.079], Avg: [-449.481 -449.481 -449.481] (0.0100) ({r_i: None, r_t: [-827.134 -827.134 -827.134], eps: 0.01})
Step:  164400, Reward: [-418.883 -418.883 -418.883] [86.804], Avg: [-449.462 -449.462 -449.462] (0.0100) ({r_i: None, r_t: [-852.233 -852.233 -852.233], eps: 0.01})
Step:   56500, Reward: [-411.068 -411.068 -411.068] [66.392], Avg: [-428.588 -428.588 -428.588] (0.1000) ({r_i: None, r_t: [-835.742 -835.742 -835.742], eps: 0.1})
Step:   88900, Reward: [-511.815 -511.815 -511.815] [179.348], Avg: [-458.223 -458.223 -458.223] (0.0100) ({r_i: None, r_t: [-1183.605 -1183.605 -1183.605], eps: 0.01})
Step:  164500, Reward: [-420.960 -420.960 -420.960] [81.968], Avg: [-449.445 -449.445 -449.445] (0.0100) ({r_i: None, r_t: [-859.460 -859.460 -859.460], eps: 0.01})
Step:   56600, Reward: [-439.680 -439.680 -439.680] [110.240], Avg: [-428.607 -428.607 -428.607] (0.1000) ({r_i: None, r_t: [-844.860 -844.860 -844.860], eps: 0.1})
Step:   89000, Reward: [-758.882 -758.882 -758.882] [442.126], Avg: [-458.560 -458.560 -458.560] (0.0100) ({r_i: None, r_t: [-1289.814 -1289.814 -1289.814], eps: 0.01})
Step:  164600, Reward: [-458.792 -458.792 -458.792] [103.481], Avg: [-449.450 -449.450 -449.450] (0.0100) ({r_i: None, r_t: [-785.411 -785.411 -785.411], eps: 0.01})
Step:   56700, Reward: [-399.414 -399.414 -399.414] [105.229], Avg: [-428.556 -428.556 -428.556] (0.1000) ({r_i: None, r_t: [-761.707 -761.707 -761.707], eps: 0.1})
Step:   89100, Reward: [-640.017 -640.017 -640.017] [380.157], Avg: [-458.764 -458.764 -458.764] (0.0100) ({r_i: None, r_t: [-1464.447 -1464.447 -1464.447], eps: 0.01})
Step:  164700, Reward: [-397.248 -397.248 -397.248] [73.702], Avg: [-449.419 -449.419 -449.419] (0.0100) ({r_i: None, r_t: [-847.964 -847.964 -847.964], eps: 0.01})
Step:   56800, Reward: [-372.838 -372.838 -372.838] [52.577], Avg: [-428.458 -428.458 -428.458] (0.1000) ({r_i: None, r_t: [-793.832 -793.832 -793.832], eps: 0.1})
Step:  164800, Reward: [-449.203 -449.203 -449.203] [92.118], Avg: [-449.418 -449.418 -449.418] (0.0100) ({r_i: None, r_t: [-847.477 -847.477 -847.477], eps: 0.01})
Step:   89200, Reward: [-810.064 -810.064 -810.064] [411.894], Avg: [-459.157 -459.157 -459.157] (0.0100) ({r_i: None, r_t: [-1157.033 -1157.033 -1157.033], eps: 0.01})
Step:  164900, Reward: [-403.764 -403.764 -403.764] [74.818], Avg: [-449.391 -449.391 -449.391] (0.0100) ({r_i: None, r_t: [-848.488 -848.488 -848.488], eps: 0.01})
Step:   56900, Reward: [-398.167 -398.167 -398.167] [88.041], Avg: [-428.405 -428.405 -428.405] (0.1000) ({r_i: None, r_t: [-828.021 -828.021 -828.021], eps: 0.1})
Step:   89300, Reward: [-569.158 -569.158 -569.158] [256.649], Avg: [-459.280 -459.280 -459.280] (0.0100) ({r_i: None, r_t: [-1265.811 -1265.811 -1265.811], eps: 0.01})
Step:  165000, Reward: [-421.264 -421.264 -421.264] [58.482], Avg: [-449.374 -449.374 -449.374] (0.0100) ({r_i: None, r_t: [-833.758 -833.758 -833.758], eps: 0.01})
Step:   57000, Reward: [-445.925 -445.925 -445.925] [55.122], Avg: [-428.435 -428.435 -428.435] (0.1000) ({r_i: None, r_t: [-849.659 -849.659 -849.659], eps: 0.1})
Step:   89400, Reward: [-617.129 -617.129 -617.129] [204.836], Avg: [-459.457 -459.457 -459.457] (0.0100) ({r_i: None, r_t: [-1152.338 -1152.338 -1152.338], eps: 0.01})
Step:  165100, Reward: [-384.415 -384.415 -384.415] [91.642], Avg: [-449.334 -449.334 -449.334] (0.0100) ({r_i: None, r_t: [-856.462 -856.462 -856.462], eps: 0.01})
Step:   57100, Reward: [-418.929 -418.929 -418.929] [79.205], Avg: [-428.419 -428.419 -428.419] (0.1000) ({r_i: None, r_t: [-783.418 -783.418 -783.418], eps: 0.1})
Step:  165200, Reward: [-417.880 -417.880 -417.880] [89.932], Avg: [-449.315 -449.315 -449.315] (0.0100) ({r_i: None, r_t: [-810.989 -810.989 -810.989], eps: 0.01})
Step:   89500, Reward: [-745.487 -745.487 -745.487] [415.193], Avg: [-459.776 -459.776 -459.776] (0.0100) ({r_i: None, r_t: [-1391.383 -1391.383 -1391.383], eps: 0.01})
Step:  165300, Reward: [-418.082 -418.082 -418.082] [78.030], Avg: [-449.297 -449.297 -449.297] (0.0100) ({r_i: None, r_t: [-833.948 -833.948 -833.948], eps: 0.01})
Step:   57200, Reward: [-419.625 -419.625 -419.625] [65.226], Avg: [-428.403 -428.403 -428.403] (0.1000) ({r_i: None, r_t: [-772.296 -772.296 -772.296], eps: 0.1})
Step:   89600, Reward: [-635.875 -635.875 -635.875] [331.019], Avg: [-459.972 -459.972 -459.972] (0.0100) ({r_i: None, r_t: [-1531.605 -1531.605 -1531.605], eps: 0.01})
Step:  165400, Reward: [-430.461 -430.461 -430.461] [84.882], Avg: [-449.285 -449.285 -449.285] (0.0100) ({r_i: None, r_t: [-848.183 -848.183 -848.183], eps: 0.01})
Step:   57300, Reward: [-428.539 -428.539 -428.539] [81.139], Avg: [-428.404 -428.404 -428.404] (0.1000) ({r_i: None, r_t: [-867.735 -867.735 -867.735], eps: 0.1})
Step:   89700, Reward: [-851.518 -851.518 -851.518] [405.954], Avg: [-460.408 -460.408 -460.408] (0.0100) ({r_i: None, r_t: [-1446.586 -1446.586 -1446.586], eps: 0.01})
Step:  165500, Reward: [-425.986 -425.986 -425.986] [62.620], Avg: [-449.271 -449.271 -449.271] (0.0100) ({r_i: None, r_t: [-881.195 -881.195 -881.195], eps: 0.01})
Step:   57400, Reward: [-431.688 -431.688 -431.688] [77.017], Avg: [-428.409 -428.409 -428.409] (0.1000) ({r_i: None, r_t: [-828.453 -828.453 -828.453], eps: 0.1})
Step:  165600, Reward: [-400.060 -400.060 -400.060] [74.278], Avg: [-449.241 -449.241 -449.241] (0.0100) ({r_i: None, r_t: [-804.917 -804.917 -804.917], eps: 0.01})
Step:   89800, Reward: [-901.096 -901.096 -901.096] [500.540], Avg: [-460.898 -460.898 -460.898] (0.0100) ({r_i: None, r_t: [-1211.384 -1211.384 -1211.384], eps: 0.01})
Step:  165700, Reward: [-377.662 -377.662 -377.662] [81.703], Avg: [-449.198 -449.198 -449.198] (0.0100) ({r_i: None, r_t: [-841.245 -841.245 -841.245], eps: 0.01})
Step:   57500, Reward: [-444.877 -444.877 -444.877] [71.632], Avg: [-428.438 -428.438 -428.438] (0.1000) ({r_i: None, r_t: [-848.370 -848.370 -848.370], eps: 0.1})
Step:   89900, Reward: [-751.180 -751.180 -751.180] [463.760], Avg: [-461.221 -461.221 -461.221] (0.0100) ({r_i: None, r_t: [-1421.688 -1421.688 -1421.688], eps: 0.01})
Step:  165800, Reward: [-440.347 -440.347 -440.347] [77.837], Avg: [-449.193 -449.193 -449.193] (0.0100) ({r_i: None, r_t: [-810.844 -810.844 -810.844], eps: 0.01})
Step:   57600, Reward: [-388.254 -388.254 -388.254] [70.806], Avg: [-428.368 -428.368 -428.368] (0.1000) ({r_i: None, r_t: [-846.481 -846.481 -846.481], eps: 0.1})
Step:   90000, Reward: [-795.188 -795.188 -795.188] [483.284], Avg: [-461.592 -461.592 -461.592] (0.0100) ({r_i: None, r_t: [-1464.478 -1464.478 -1464.478], eps: 0.01})
Step:  165900, Reward: [-427.148 -427.148 -427.148] [88.151], Avg: [-449.180 -449.180 -449.180] (0.0100) ({r_i: None, r_t: [-832.563 -832.563 -832.563], eps: 0.01})
Step:   57700, Reward: [-397.917 -397.917 -397.917] [65.731], Avg: [-428.316 -428.316 -428.316] (0.1000) ({r_i: None, r_t: [-826.837 -826.837 -826.837], eps: 0.1})
Step:  166000, Reward: [-426.919 -426.919 -426.919] [109.494], Avg: [-449.166 -449.166 -449.166] (0.0100) ({r_i: None, r_t: [-861.067 -861.067 -861.067], eps: 0.01})
Step:   90100, Reward: [-1027.568 -1027.568 -1027.568] [519.549], Avg: [-462.219 -462.219 -462.219] (0.0100) ({r_i: None, r_t: [-1484.860 -1484.860 -1484.860], eps: 0.01})
Step:  166100, Reward: [-394.901 -394.901 -394.901] [65.894], Avg: [-449.134 -449.134 -449.134] (0.0100) ({r_i: None, r_t: [-825.385 -825.385 -825.385], eps: 0.01})
Step:   57800, Reward: [-411.174 -411.174 -411.174] [41.877], Avg: [-428.286 -428.286 -428.286] (0.1000) ({r_i: None, r_t: [-833.187 -833.187 -833.187], eps: 0.1})
Step:   90200, Reward: [-780.160 -780.160 -780.160] [448.843], Avg: [-462.571 -462.571 -462.571] (0.0100) ({r_i: None, r_t: [-1927.041 -1927.041 -1927.041], eps: 0.01})
Step:  166200, Reward: [-417.252 -417.252 -417.252] [66.163], Avg: [-449.114 -449.114 -449.114] (0.0100) ({r_i: None, r_t: [-839.775 -839.775 -839.775], eps: 0.01})
Step:   57900, Reward: [-399.352 -399.352 -399.352] [57.531], Avg: [-428.236 -428.236 -428.236] (0.1000) ({r_i: None, r_t: [-802.651 -802.651 -802.651], eps: 0.1})
Step:   90300, Reward: [-940.543 -940.543 -940.543] [512.498], Avg: [-463.100 -463.100 -463.100] (0.0100) ({r_i: None, r_t: [-1792.259 -1792.259 -1792.259], eps: 0.01})
Step:  166300, Reward: [-413.615 -413.615 -413.615] [109.997], Avg: [-449.093 -449.093 -449.093] (0.0100) ({r_i: None, r_t: [-811.460 -811.460 -811.460], eps: 0.01})
Step:   58000, Reward: [-418.390 -418.390 -418.390] [66.299], Avg: [-428.219 -428.219 -428.219] (0.1000) ({r_i: None, r_t: [-892.648 -892.648 -892.648], eps: 0.1})
Step:  166400, Reward: [-381.244 -381.244 -381.244] [82.830], Avg: [-449.052 -449.052 -449.052] (0.0100) ({r_i: None, r_t: [-798.150 -798.150 -798.150], eps: 0.01})
Step:   90400, Reward: [-939.997 -939.997 -939.997] [600.721], Avg: [-463.627 -463.627 -463.627] (0.0100) ({r_i: None, r_t: [-1829.054 -1829.054 -1829.054], eps: 0.01})
Step:  166500, Reward: [-433.936 -433.936 -433.936] [138.706], Avg: [-449.043 -449.043 -449.043] (0.0100) ({r_i: None, r_t: [-879.489 -879.489 -879.489], eps: 0.01})
Step:   58100, Reward: [-428.069 -428.069 -428.069] [87.656], Avg: [-428.219 -428.219 -428.219] (0.1000) ({r_i: None, r_t: [-824.453 -824.453 -824.453], eps: 0.1})
Step:   90500, Reward: [-746.331 -746.331 -746.331] [328.849], Avg: [-463.939 -463.939 -463.939] (0.0100) ({r_i: None, r_t: [-1595.872 -1595.872 -1595.872], eps: 0.01})
Step:  166600, Reward: [-432.610 -432.610 -432.610] [69.092], Avg: [-449.033 -449.033 -449.033] (0.0100) ({r_i: None, r_t: [-865.785 -865.785 -865.785], eps: 0.01})
Step:   58200, Reward: [-389.432 -389.432 -389.432] [63.434], Avg: [-428.152 -428.152 -428.152] (0.1000) ({r_i: None, r_t: [-761.151 -761.151 -761.151], eps: 0.1})
Step:   90600, Reward: [-756.977 -756.977 -756.977] [350.990], Avg: [-464.262 -464.262 -464.262] (0.0100) ({r_i: None, r_t: [-1951.609 -1951.609 -1951.609], eps: 0.01})
Step:  166700, Reward: [-410.718 -410.718 -410.718] [81.078], Avg: [-449.010 -449.010 -449.010] (0.0100) ({r_i: None, r_t: [-841.301 -841.301 -841.301], eps: 0.01})
Step:   58300, Reward: [-414.205 -414.205 -414.205] [81.968], Avg: [-428.129 -428.129 -428.129] (0.1000) ({r_i: None, r_t: [-829.601 -829.601 -829.601], eps: 0.1})
Step:  166800, Reward: [-425.678 -425.678 -425.678] [105.405], Avg: [-448.996 -448.996 -448.996] (0.0100) ({r_i: None, r_t: [-867.977 -867.977 -867.977], eps: 0.01})
Step:   90700, Reward: [-1015.492 -1015.492 -1015.492] [439.814], Avg: [-464.869 -464.869 -464.869] (0.0100) ({r_i: None, r_t: [-2015.808 -2015.808 -2015.808], eps: 0.01})
Step:  166900, Reward: [-439.107 -439.107 -439.107] [93.993], Avg: [-448.991 -448.991 -448.991] (0.0100) ({r_i: None, r_t: [-833.723 -833.723 -833.723], eps: 0.01})
Step:   58400, Reward: [-463.604 -463.604 -463.604] [104.314], Avg: [-428.189 -428.189 -428.189] (0.1000) ({r_i: None, r_t: [-827.034 -827.034 -827.034], eps: 0.1})
Step:   90800, Reward: [-775.993 -775.993 -775.993] [370.013], Avg: [-465.211 -465.211 -465.211] (0.0100) ({r_i: None, r_t: [-1578.826 -1578.826 -1578.826], eps: 0.01})
Step:  167000, Reward: [-382.272 -382.272 -382.272] [58.780], Avg: [-448.951 -448.951 -448.951] (0.0100) ({r_i: None, r_t: [-841.981 -841.981 -841.981], eps: 0.01})
Step:   58500, Reward: [-408.834 -408.834 -408.834] [60.523], Avg: [-428.156 -428.156 -428.156] (0.1000) ({r_i: None, r_t: [-823.281 -823.281 -823.281], eps: 0.1})
Step:   90900, Reward: [-756.683 -756.683 -756.683] [394.675], Avg: [-465.532 -465.532 -465.532] (0.0100) ({r_i: None, r_t: [-2041.877 -2041.877 -2041.877], eps: 0.01})
Step:  167100, Reward: [-401.997 -401.997 -401.997] [71.806], Avg: [-448.923 -448.923 -448.923] (0.0100) ({r_i: None, r_t: [-839.005 -839.005 -839.005], eps: 0.01})
Step:   58600, Reward: [-396.234 -396.234 -396.234] [48.966], Avg: [-428.102 -428.102 -428.102] (0.1000) ({r_i: None, r_t: [-796.452 -796.452 -796.452], eps: 0.1})
Step:  167200, Reward: [-443.370 -443.370 -443.370] [91.279], Avg: [-448.919 -448.919 -448.919] (0.0100) ({r_i: None, r_t: [-780.877 -780.877 -780.877], eps: 0.01})
Step:   91000, Reward: [-796.993 -796.993 -796.993] [468.981], Avg: [-465.895 -465.895 -465.895] (0.0100) ({r_i: None, r_t: [-1881.008 -1881.008 -1881.008], eps: 0.01})
Step:  167300, Reward: [-416.345 -416.345 -416.345] [95.517], Avg: [-448.900 -448.900 -448.900] (0.0100) ({r_i: None, r_t: [-840.132 -840.132 -840.132], eps: 0.01})
Step:   58700, Reward: [-363.613 -363.613 -363.613] [63.979], Avg: [-427.992 -427.992 -427.992] (0.1000) ({r_i: None, r_t: [-777.535 -777.535 -777.535], eps: 0.1})
Step:   91100, Reward: [-1207.647 -1207.647 -1207.647] [492.149], Avg: [-466.709 -466.709 -466.709] (0.0100) ({r_i: None, r_t: [-2102.860 -2102.860 -2102.860], eps: 0.01})
Step:  167400, Reward: [-406.485 -406.485 -406.485] [109.176], Avg: [-448.874 -448.874 -448.874] (0.0100) ({r_i: None, r_t: [-869.118 -869.118 -869.118], eps: 0.01})
Step:   58800, Reward: [-402.838 -402.838 -402.838] [84.159], Avg: [-427.949 -427.949 -427.949] (0.1000) ({r_i: None, r_t: [-832.027 -832.027 -832.027], eps: 0.1})
Step:   91200, Reward: [-1009.840 -1009.840 -1009.840] [552.070], Avg: [-467.304 -467.304 -467.304] (0.0100) ({r_i: None, r_t: [-1634.932 -1634.932 -1634.932], eps: 0.01})
Step:  167500, Reward: [-397.303 -397.303 -397.303] [96.924], Avg: [-448.844 -448.844 -448.844] (0.0100) ({r_i: None, r_t: [-833.509 -833.509 -833.509], eps: 0.01})
Step:  167600, Reward: [-386.249 -386.249 -386.249] [78.557], Avg: [-448.806 -448.806 -448.806] (0.0100) ({r_i: None, r_t: [-827.760 -827.760 -827.760], eps: 0.01})
Step:   58900, Reward: [-417.666 -417.666 -417.666] [96.393], Avg: [-427.932 -427.932 -427.932] (0.1000) ({r_i: None, r_t: [-827.990 -827.990 -827.990], eps: 0.1})
Step:   91300, Reward: [-933.290 -933.290 -933.290] [506.439], Avg: [-467.813 -467.813 -467.813] (0.0100) ({r_i: None, r_t: [-1797.113 -1797.113 -1797.113], eps: 0.01})
Step:  167700, Reward: [-444.656 -444.656 -444.656] [110.266], Avg: [-448.804 -448.804 -448.804] (0.0100) ({r_i: None, r_t: [-816.430 -816.430 -816.430], eps: 0.01})
Step:   59000, Reward: [-392.921 -392.921 -392.921] [43.351], Avg: [-427.873 -427.873 -427.873] (0.1000) ({r_i: None, r_t: [-848.603 -848.603 -848.603], eps: 0.1})
Step:   91400, Reward: [-1010.174 -1010.174 -1010.174] [487.773], Avg: [-468.406 -468.406 -468.406] (0.0100) ({r_i: None, r_t: [-2145.256 -2145.256 -2145.256], eps: 0.01})
Step:  167800, Reward: [-450.852 -450.852 -450.852] [84.049], Avg: [-448.805 -448.805 -448.805] (0.0100) ({r_i: None, r_t: [-816.906 -816.906 -816.906], eps: 0.01})
Step:   59100, Reward: [-394.830 -394.830 -394.830] [63.051], Avg: [-427.817 -427.817 -427.817] (0.1000) ({r_i: None, r_t: [-842.114 -842.114 -842.114], eps: 0.1})
Step:   91500, Reward: [-890.274 -890.274 -890.274] [401.017], Avg: [-468.867 -468.867 -468.867] (0.0100) ({r_i: None, r_t: [-1819.503 -1819.503 -1819.503], eps: 0.01})
Step:  167900, Reward: [-399.857 -399.857 -399.857] [67.442], Avg: [-448.776 -448.776 -448.776] (0.0100) ({r_i: None, r_t: [-868.784 -868.784 -868.784], eps: 0.01})
Step:  168000, Reward: [-432.428 -432.428 -432.428] [107.348], Avg: [-448.766 -448.766 -448.766] (0.0100) ({r_i: None, r_t: [-879.546 -879.546 -879.546], eps: 0.01})
Step:   59200, Reward: [-456.804 -456.804 -456.804] [56.629], Avg: [-427.866 -427.866 -427.866] (0.1000) ({r_i: None, r_t: [-856.714 -856.714 -856.714], eps: 0.1})
Step:   91600, Reward: [-886.813 -886.813 -886.813] [501.699], Avg: [-469.323 -469.323 -469.323] (0.0100) ({r_i: None, r_t: [-1740.952 -1740.952 -1740.952], eps: 0.01})
Step:  168100, Reward: [-411.612 -411.612 -411.612] [91.511], Avg: [-448.744 -448.744 -448.744] (0.0100) ({r_i: None, r_t: [-887.167 -887.167 -887.167], eps: 0.01})
Step:   59300, Reward: [-434.665 -434.665 -434.665] [69.328], Avg: [-427.877 -427.877 -427.877] (0.1000) ({r_i: None, r_t: [-869.975 -869.975 -869.975], eps: 0.1})
Step:   91700, Reward: [-1000.495 -1000.495 -1000.495] [643.518], Avg: [-469.901 -469.901 -469.901] (0.0100) ({r_i: None, r_t: [-2011.495 -2011.495 -2011.495], eps: 0.01})
Step:  168200, Reward: [-416.771 -416.771 -416.771] [62.351], Avg: [-448.725 -448.725 -448.725] (0.0100) ({r_i: None, r_t: [-810.591 -810.591 -810.591], eps: 0.01})
Step:   59400, Reward: [-455.759 -455.759 -455.759] [65.103], Avg: [-427.924 -427.924 -427.924] (0.1000) ({r_i: None, r_t: [-839.202 -839.202 -839.202], eps: 0.1})
Step:   91800, Reward: [-1058.680 -1058.680 -1058.680] [424.676], Avg: [-470.542 -470.542 -470.542] (0.0100) ({r_i: None, r_t: [-2452.967 -2452.967 -2452.967], eps: 0.01})
Step:  168300, Reward: [-404.499 -404.499 -404.499] [73.381], Avg: [-448.699 -448.699 -448.699] (0.0100) ({r_i: None, r_t: [-833.927 -833.927 -833.927], eps: 0.01})
Step:  168400, Reward: [-413.395 -413.395 -413.395] [61.782], Avg: [-448.678 -448.678 -448.678] (0.0100) ({r_i: None, r_t: [-874.375 -874.375 -874.375], eps: 0.01})
Step:   59500, Reward: [-428.671 -428.671 -428.671] [83.826], Avg: [-427.925 -427.925 -427.925] (0.1000) ({r_i: None, r_t: [-871.159 -871.159 -871.159], eps: 0.1})
Step:   91900, Reward: [-1209.008 -1209.008 -1209.008] [494.212], Avg: [-471.345 -471.345 -471.345] (0.0100) ({r_i: None, r_t: [-2212.193 -2212.193 -2212.193], eps: 0.01})
Step:  168500, Reward: [-433.769 -433.769 -433.769] [92.108], Avg: [-448.669 -448.669 -448.669] (0.0100) ({r_i: None, r_t: [-822.080 -822.080 -822.080], eps: 0.01})
Step:   59600, Reward: [-430.663 -430.663 -430.663] [101.971], Avg: [-427.930 -427.930 -427.930] (0.1000) ({r_i: None, r_t: [-825.049 -825.049 -825.049], eps: 0.1})
Step:   92000, Reward: [-1038.249 -1038.249 -1038.249] [440.935], Avg: [-471.960 -471.960 -471.960] (0.0100) ({r_i: None, r_t: [-1690.043 -1690.043 -1690.043], eps: 0.01})
Step:  168600, Reward: [-440.863 -440.863 -440.863] [84.141], Avg: [-448.664 -448.664 -448.664] (0.0100) ({r_i: None, r_t: [-871.450 -871.450 -871.450], eps: 0.01})
Step:   59700, Reward: [-440.919 -440.919 -440.919] [90.423], Avg: [-427.952 -427.952 -427.952] (0.1000) ({r_i: None, r_t: [-807.232 -807.232 -807.232], eps: 0.1})
Step:  168700, Reward: [-395.719 -395.719 -395.719] [61.513], Avg: [-448.633 -448.633 -448.633] (0.0100) ({r_i: None, r_t: [-845.380 -845.380 -845.380], eps: 0.01})
Step:   92100, Reward: [-1069.409 -1069.409 -1069.409] [495.501], Avg: [-472.608 -472.608 -472.608] (0.0100) ({r_i: None, r_t: [-1748.478 -1748.478 -1748.478], eps: 0.01})
Step:  168800, Reward: [-399.853 -399.853 -399.853] [81.948], Avg: [-448.604 -448.604 -448.604] (0.0100) ({r_i: None, r_t: [-844.957 -844.957 -844.957], eps: 0.01})
Step:   59800, Reward: [-450.645 -450.645 -450.645] [127.869], Avg: [-427.990 -427.990 -427.990] (0.1000) ({r_i: None, r_t: [-794.256 -794.256 -794.256], eps: 0.1})
Step:   92200, Reward: [-1140.416 -1140.416 -1140.416] [489.264], Avg: [-473.332 -473.332 -473.332] (0.0100) ({r_i: None, r_t: [-2351.312 -2351.312 -2351.312], eps: 0.01})
Step:  168900, Reward: [-442.514 -442.514 -442.514] [94.335], Avg: [-448.601 -448.601 -448.601] (0.0100) ({r_i: None, r_t: [-829.870 -829.870 -829.870], eps: 0.01})
Step:   59900, Reward: [-424.635 -424.635 -424.635] [103.435], Avg: [-427.984 -427.984 -427.984] (0.1000) ({r_i: None, r_t: [-872.107 -872.107 -872.107], eps: 0.1})
Step:   92300, Reward: [-1175.250 -1175.250 -1175.250] [583.388], Avg: [-474.091 -474.091 -474.091] (0.0100) ({r_i: None, r_t: [-1807.494 -1807.494 -1807.494], eps: 0.01})
Step:  169000, Reward: [-407.972 -407.972 -407.972] [84.990], Avg: [-448.577 -448.577 -448.577] (0.0100) ({r_i: None, r_t: [-847.904 -847.904 -847.904], eps: 0.01})
Step:   60000, Reward: [-393.334 -393.334 -393.334] [51.364], Avg: [-427.926 -427.926 -427.926] (0.1000) ({r_i: None, r_t: [-866.274 -866.274 -866.274], eps: 0.1})
Step:  169100, Reward: [-435.136 -435.136 -435.136] [128.444], Avg: [-448.569 -448.569 -448.569] (0.0100) ({r_i: None, r_t: [-871.266 -871.266 -871.266], eps: 0.01})
Step:   92400, Reward: [-993.090 -993.090 -993.090] [535.051], Avg: [-474.652 -474.652 -474.652] (0.0100) ({r_i: None, r_t: [-2400.024 -2400.024 -2400.024], eps: 0.01})
Step:  169200, Reward: [-460.445 -460.445 -460.445] [103.655], Avg: [-448.576 -448.576 -448.576] (0.0100) ({r_i: None, r_t: [-840.700 -840.700 -840.700], eps: 0.01})
Step:   60100, Reward: [-408.728 -408.728 -408.728] [73.459], Avg: [-427.894 -427.894 -427.894] (0.1000) ({r_i: None, r_t: [-845.661 -845.661 -845.661], eps: 0.1})
Step:   92500, Reward: [-1045.782 -1045.782 -1045.782] [500.098], Avg: [-475.269 -475.269 -475.269] (0.0100) ({r_i: None, r_t: [-2015.964 -2015.964 -2015.964], eps: 0.01})
Step:  169300, Reward: [-433.909 -433.909 -433.909] [82.746], Avg: [-448.567 -448.567 -448.567] (0.0100) ({r_i: None, r_t: [-793.350 -793.350 -793.350], eps: 0.01})
Step:   60200, Reward: [-421.739 -421.739 -421.739] [87.845], Avg: [-427.884 -427.884 -427.884] (0.1000) ({r_i: None, r_t: [-808.543 -808.543 -808.543], eps: 0.1})
Step:   92600, Reward: [-1296.482 -1296.482 -1296.482] [565.328], Avg: [-476.155 -476.155 -476.155] (0.0100) ({r_i: None, r_t: [-1989.112 -1989.112 -1989.112], eps: 0.01})
Step:  169400, Reward: [-427.861 -427.861 -427.861] [72.638], Avg: [-448.555 -448.555 -448.555] (0.0100) ({r_i: None, r_t: [-889.015 -889.015 -889.015], eps: 0.01})
Step:   60300, Reward: [-427.138 -427.138 -427.138] [73.618], Avg: [-427.883 -427.883 -427.883] (0.1000) ({r_i: None, r_t: [-850.880 -850.880 -850.880], eps: 0.1})
Step:  169500, Reward: [-407.220 -407.220 -407.220] [66.585], Avg: [-448.530 -448.530 -448.530] (0.0100) ({r_i: None, r_t: [-907.095 -907.095 -907.095], eps: 0.01})
Step:   92700, Reward: [-640.751 -640.751 -640.751] [290.194], Avg: [-476.332 -476.332 -476.332] (0.0100) ({r_i: None, r_t: [-2217.083 -2217.083 -2217.083], eps: 0.01})
Step:  169600, Reward: [-402.754 -402.754 -402.754] [80.307], Avg: [-448.503 -448.503 -448.503] (0.0100) ({r_i: None, r_t: [-886.338 -886.338 -886.338], eps: 0.01})
Step:   60400, Reward: [-422.319 -422.319 -422.319] [66.830], Avg: [-427.874 -427.874 -427.874] (0.1000) ({r_i: None, r_t: [-837.392 -837.392 -837.392], eps: 0.1})
Step:   92800, Reward: [-1032.841 -1032.841 -1032.841] [461.254], Avg: [-476.931 -476.931 -476.931] (0.0100) ({r_i: None, r_t: [-2141.080 -2141.080 -2141.080], eps: 0.01})
Step:  169700, Reward: [-403.023 -403.023 -403.023] [101.129], Avg: [-448.477 -448.477 -448.477] (0.0100) ({r_i: None, r_t: [-848.989 -848.989 -848.989], eps: 0.01})
Step:   60500, Reward: [-446.394 -446.394 -446.394] [83.693], Avg: [-427.904 -427.904 -427.904] (0.1000) ({r_i: None, r_t: [-846.402 -846.402 -846.402], eps: 0.1})
Step:   92900, Reward: [-876.376 -876.376 -876.376] [496.869], Avg: [-477.361 -477.361 -477.361] (0.0100) ({r_i: None, r_t: [-2009.893 -2009.893 -2009.893], eps: 0.01})
Step:  169800, Reward: [-378.842 -378.842 -378.842] [93.905], Avg: [-448.436 -448.436 -448.436] (0.0100) ({r_i: None, r_t: [-809.747 -809.747 -809.747], eps: 0.01})
Step:  169900, Reward: [-423.027 -423.027 -423.027] [66.768], Avg: [-448.421 -448.421 -448.421] (0.0100) ({r_i: None, r_t: [-793.972 -793.972 -793.972], eps: 0.01})
Step:   60600, Reward: [-478.662 -478.662 -478.662] [121.844], Avg: [-427.988 -427.988 -427.988] (0.1000) ({r_i: None, r_t: [-838.511 -838.511 -838.511], eps: 0.1})
Step:   93000, Reward: [-1254.813 -1254.813 -1254.813] [526.579], Avg: [-478.196 -478.196 -478.196] (0.0100) ({r_i: None, r_t: [-2273.096 -2273.096 -2273.096], eps: 0.01})
Step:  170000, Reward: [-379.234 -379.234 -379.234] [77.633], Avg: [-448.380 -448.380 -448.380] (0.0100) ({r_i: None, r_t: [-842.530 -842.530 -842.530], eps: 0.01})
Step:   60700, Reward: [-425.568 -425.568 -425.568] [64.687], Avg: [-427.984 -427.984 -427.984] (0.1000) ({r_i: None, r_t: [-855.378 -855.378 -855.378], eps: 0.1})
Step:   93100, Reward: [-1356.877 -1356.877 -1356.877] [644.096], Avg: [-479.139 -479.139 -479.139] (0.0100) ({r_i: None, r_t: [-2366.557 -2366.557 -2366.557], eps: 0.01})
Step:  170100, Reward: [-436.196 -436.196 -436.196] [68.735], Avg: [-448.373 -448.373 -448.373] (0.0100) ({r_i: None, r_t: [-837.035 -837.035 -837.035], eps: 0.01})
Step:   60800, Reward: [-431.281 -431.281 -431.281] [98.275], Avg: [-427.989 -427.989 -427.989] (0.1000) ({r_i: None, r_t: [-794.927 -794.927 -794.927], eps: 0.1})
Step:   93200, Reward: [-1000.522 -1000.522 -1000.522] [466.280], Avg: [-479.698 -479.698 -479.698] (0.0100) ({r_i: None, r_t: [-1816.385 -1816.385 -1816.385], eps: 0.01})
Step:  170200, Reward: [-376.197 -376.197 -376.197] [54.604], Avg: [-448.330 -448.330 -448.330] (0.0100) ({r_i: None, r_t: [-793.358 -793.358 -793.358], eps: 0.01})
Step:  170300, Reward: [-471.773 -471.773 -471.773] [116.826], Avg: [-448.344 -448.344 -448.344] (0.0100) ({r_i: None, r_t: [-863.188 -863.188 -863.188], eps: 0.01})
Step:   60900, Reward: [-438.236 -438.236 -438.236] [82.636], Avg: [-428.006 -428.006 -428.006] (0.1000) ({r_i: None, r_t: [-929.785 -929.785 -929.785], eps: 0.1})
Step:   93300, Reward: [-1251.464 -1251.464 -1251.464] [402.313], Avg: [-480.524 -480.524 -480.524] (0.0100) ({r_i: None, r_t: [-2120.042 -2120.042 -2120.042], eps: 0.01})
Step:  170400, Reward: [-398.531 -398.531 -398.531] [58.861], Avg: [-448.315 -448.315 -448.315] (0.0100) ({r_i: None, r_t: [-886.718 -886.718 -886.718], eps: 0.01})
Step:   61000, Reward: [-427.454 -427.454 -427.454] [88.210], Avg: [-428.005 -428.005 -428.005] (0.1000) ({r_i: None, r_t: [-857.900 -857.900 -857.900], eps: 0.1})
Step:   93400, Reward: [-1031.864 -1031.864 -1031.864] [498.628], Avg: [-481.114 -481.114 -481.114] (0.0100) ({r_i: None, r_t: [-2035.873 -2035.873 -2035.873], eps: 0.01})
Step:  170500, Reward: [-387.588 -387.588 -387.588] [70.149], Avg: [-448.279 -448.279 -448.279] (0.0100) ({r_i: None, r_t: [-844.458 -844.458 -844.458], eps: 0.01})
Step:   61100, Reward: [-397.637 -397.637 -397.637] [56.201], Avg: [-427.956 -427.956 -427.956] (0.1000) ({r_i: None, r_t: [-817.104 -817.104 -817.104], eps: 0.1})
Step:   93500, Reward: [-932.887 -932.887 -932.887] [435.786], Avg: [-481.596 -481.596 -481.596] (0.0100) ({r_i: None, r_t: [-2173.852 -2173.852 -2173.852], eps: 0.01})
Step:  170600, Reward: [-419.171 -419.171 -419.171] [67.085], Avg: [-448.262 -448.262 -448.262] (0.0100) ({r_i: None, r_t: [-852.315 -852.315 -852.315], eps: 0.01})
Step:   61200, Reward: [-410.701 -410.701 -410.701] [94.069], Avg: [-427.928 -427.928 -427.928] (0.1000) ({r_i: None, r_t: [-829.383 -829.383 -829.383], eps: 0.1})
Step:  170700, Reward: [-425.273 -425.273 -425.273] [83.440], Avg: [-448.249 -448.249 -448.249] (0.0100) ({r_i: None, r_t: [-839.337 -839.337 -839.337], eps: 0.01})
Step:   93600, Reward: [-934.841 -934.841 -934.841] [344.495], Avg: [-482.080 -482.080 -482.080] (0.0100) ({r_i: None, r_t: [-2288.585 -2288.585 -2288.585], eps: 0.01})
Step:  170800, Reward: [-400.332 -400.332 -400.332] [87.627], Avg: [-448.221 -448.221 -448.221] (0.0100) ({r_i: None, r_t: [-814.490 -814.490 -814.490], eps: 0.01})
Step:   61300, Reward: [-440.842 -440.842 -440.842] [96.551], Avg: [-427.949 -427.949 -427.949] (0.1000) ({r_i: None, r_t: [-880.573 -880.573 -880.573], eps: 0.1})
Step:   93700, Reward: [-1056.770 -1056.770 -1056.770] [519.874], Avg: [-482.693 -482.693 -482.693] (0.0100) ({r_i: None, r_t: [-2127.149 -2127.149 -2127.149], eps: 0.01})
Step:  170900, Reward: [-488.001 -488.001 -488.001] [87.268], Avg: [-448.244 -448.244 -448.244] (0.0100) ({r_i: None, r_t: [-849.078 -849.078 -849.078], eps: 0.01})
Step:   61400, Reward: [-426.314 -426.314 -426.314] [74.497], Avg: [-427.946 -427.946 -427.946] (0.1000) ({r_i: None, r_t: [-855.917 -855.917 -855.917], eps: 0.1})
Step:   93800, Reward: [-962.954 -962.954 -962.954] [550.422], Avg: [-483.204 -483.204 -483.204] (0.0100) ({r_i: None, r_t: [-1748.796 -1748.796 -1748.796], eps: 0.01})
Step:  171000, Reward: [-415.343 -415.343 -415.343] [70.775], Avg: [-448.225 -448.225 -448.225] (0.0100) ({r_i: None, r_t: [-831.081 -831.081 -831.081], eps: 0.01})
Step:   61500, Reward: [-446.265 -446.265 -446.265] [98.791], Avg: [-427.976 -427.976 -427.976] (0.1000) ({r_i: None, r_t: [-849.497 -849.497 -849.497], eps: 0.1})
Step:  171100, Reward: [-439.495 -439.495 -439.495] [142.557], Avg: [-448.220 -448.220 -448.220] (0.0100) ({r_i: None, r_t: [-883.626 -883.626 -883.626], eps: 0.01})
Step:   93900, Reward: [-743.031 -743.031 -743.031] [336.287], Avg: [-483.480 -483.480 -483.480] (0.0100) ({r_i: None, r_t: [-2094.504 -2094.504 -2094.504], eps: 0.01})
Step:  171200, Reward: [-404.616 -404.616 -404.616] [49.550], Avg: [-448.194 -448.194 -448.194] (0.0100) ({r_i: None, r_t: [-806.948 -806.948 -806.948], eps: 0.01})
Step:   61600, Reward: [-406.461 -406.461 -406.461] [61.948], Avg: [-427.941 -427.941 -427.941] (0.1000) ({r_i: None, r_t: [-815.092 -815.092 -815.092], eps: 0.1})
Step:   94000, Reward: [-1178.199 -1178.199 -1178.199] [497.190], Avg: [-484.219 -484.219 -484.219] (0.0100) ({r_i: None, r_t: [-1912.044 -1912.044 -1912.044], eps: 0.01})
Step:  171300, Reward: [-393.242 -393.242 -393.242] [88.468], Avg: [-448.162 -448.162 -448.162] (0.0100) ({r_i: None, r_t: [-884.512 -884.512 -884.512], eps: 0.01})
Step:   61700, Reward: [-481.786 -481.786 -481.786] [111.748], Avg: [-428.028 -428.028 -428.028] (0.1000) ({r_i: None, r_t: [-869.840 -869.840 -869.840], eps: 0.1})
Step:   94100, Reward: [-1062.301 -1062.301 -1062.301] [381.985], Avg: [-484.832 -484.832 -484.832] (0.0100) ({r_i: None, r_t: [-2269.467 -2269.467 -2269.467], eps: 0.01})
Step:  171400, Reward: [-448.697 -448.697 -448.697] [101.984], Avg: [-448.163 -448.163 -448.163] (0.0100) ({r_i: None, r_t: [-838.350 -838.350 -838.350], eps: 0.01})
Step:  171500, Reward: [-400.564 -400.564 -400.564] [85.373], Avg: [-448.135 -448.135 -448.135] (0.0100) ({r_i: None, r_t: [-838.951 -838.951 -838.951], eps: 0.01})
Step:   61800, Reward: [-432.766 -432.766 -432.766] [99.304], Avg: [-428.036 -428.036 -428.036] (0.1000) ({r_i: None, r_t: [-828.764 -828.764 -828.764], eps: 0.1})
Step:   94200, Reward: [-892.746 -892.746 -892.746] [510.951], Avg: [-485.265 -485.265 -485.265] (0.0100) ({r_i: None, r_t: [-1962.672 -1962.672 -1962.672], eps: 0.01})
Step:  171600, Reward: [-411.273 -411.273 -411.273] [88.462], Avg: [-448.113 -448.113 -448.113] (0.0100) ({r_i: None, r_t: [-774.410 -774.410 -774.410], eps: 0.01})
Step:   61900, Reward: [-436.152 -436.152 -436.152] [97.736], Avg: [-428.049 -428.049 -428.049] (0.1000) ({r_i: None, r_t: [-892.332 -892.332 -892.332], eps: 0.1})
Step:   94300, Reward: [-849.057 -849.057 -849.057] [510.148], Avg: [-485.650 -485.650 -485.650] (0.0100) ({r_i: None, r_t: [-1976.793 -1976.793 -1976.793], eps: 0.01})
Step:  171700, Reward: [-441.064 -441.064 -441.064] [89.426], Avg: [-448.109 -448.109 -448.109] (0.0100) ({r_i: None, r_t: [-893.707 -893.707 -893.707], eps: 0.01})
Step:   62000, Reward: [-434.265 -434.265 -434.265] [80.291], Avg: [-428.059 -428.059 -428.059] (0.1000) ({r_i: None, r_t: [-903.429 -903.429 -903.429], eps: 0.1})
Step:   94400, Reward: [-858.270 -858.270 -858.270] [470.941], Avg: [-486.045 -486.045 -486.045] (0.0100) ({r_i: None, r_t: [-1810.676 -1810.676 -1810.676], eps: 0.01})
Step:  171800, Reward: [-397.432 -397.432 -397.432] [79.976], Avg: [-448.080 -448.080 -448.080] (0.0100) ({r_i: None, r_t: [-785.768 -785.768 -785.768], eps: 0.01})
Step:  171900, Reward: [-421.329 -421.329 -421.329] [86.859], Avg: [-448.064 -448.064 -448.064] (0.0100) ({r_i: None, r_t: [-829.937 -829.937 -829.937], eps: 0.01})
Step:   62100, Reward: [-413.779 -413.779 -413.779] [55.043], Avg: [-428.036 -428.036 -428.036] (0.1000) ({r_i: None, r_t: [-881.537 -881.537 -881.537], eps: 0.1})
Step:   94500, Reward: [-1021.270 -1021.270 -1021.270] [571.169], Avg: [-486.610 -486.610 -486.610] (0.0100) ({r_i: None, r_t: [-1733.221 -1733.221 -1733.221], eps: 0.01})
Step:  172000, Reward: [-402.337 -402.337 -402.337] [63.549], Avg: [-448.038 -448.038 -448.038] (0.0100) ({r_i: None, r_t: [-861.350 -861.350 -861.350], eps: 0.01})
Step:   62200, Reward: [-403.537 -403.537 -403.537] [49.858], Avg: [-427.996 -427.996 -427.996] (0.1000) ({r_i: None, r_t: [-865.064 -865.064 -865.064], eps: 0.1})
Step:   94600, Reward: [-767.182 -767.182 -767.182] [407.682], Avg: [-486.907 -486.907 -486.907] (0.0100) ({r_i: None, r_t: [-1911.321 -1911.321 -1911.321], eps: 0.01})
Step:  172100, Reward: [-439.795 -439.795 -439.795] [78.850], Avg: [-448.033 -448.033 -448.033] (0.0100) ({r_i: None, r_t: [-904.057 -904.057 -904.057], eps: 0.01})
Step:   62300, Reward: [-409.835 -409.835 -409.835] [71.663], Avg: [-427.967 -427.967 -427.967] (0.1000) ({r_i: None, r_t: [-898.602 -898.602 -898.602], eps: 0.1})
Step:  172200, Reward: [-411.443 -411.443 -411.443] [91.230], Avg: [-448.012 -448.012 -448.012] (0.0100) ({r_i: None, r_t: [-846.491 -846.491 -846.491], eps: 0.01})
Step:   94700, Reward: [-945.294 -945.294 -945.294] [536.606], Avg: [-487.390 -487.390 -487.390] (0.0100) ({r_i: None, r_t: [-1622.197 -1622.197 -1622.197], eps: 0.01})
Step:  172300, Reward: [-413.673 -413.673 -413.673] [79.382], Avg: [-447.992 -447.992 -447.992] (0.0100) ({r_i: None, r_t: [-829.965 -829.965 -829.965], eps: 0.01})
Step:   62400, Reward: [-451.524 -451.524 -451.524] [85.938], Avg: [-428.005 -428.005 -428.005] (0.1000) ({r_i: None, r_t: [-846.606 -846.606 -846.606], eps: 0.1})
Step:   94800, Reward: [-799.324 -799.324 -799.324] [395.983], Avg: [-487.719 -487.719 -487.719] (0.0100) ({r_i: None, r_t: [-1760.738 -1760.738 -1760.738], eps: 0.01})
Step:  172400, Reward: [-461.465 -461.465 -461.465] [93.298], Avg: [-448.000 -448.000 -448.000] (0.0100) ({r_i: None, r_t: [-854.814 -854.814 -854.814], eps: 0.01})
Step:   62500, Reward: [-411.060 -411.060 -411.060] [101.905], Avg: [-427.978 -427.978 -427.978] (0.1000) ({r_i: None, r_t: [-831.498 -831.498 -831.498], eps: 0.1})
Step:   94900, Reward: [-891.364 -891.364 -891.364] [497.288], Avg: [-488.144 -488.144 -488.144] (0.0100) ({r_i: None, r_t: [-1813.599 -1813.599 -1813.599], eps: 0.01})
Step:  172500, Reward: [-428.852 -428.852 -428.852] [91.517], Avg: [-447.988 -447.988 -447.988] (0.0100) ({r_i: None, r_t: [-842.555 -842.555 -842.555], eps: 0.01})
Step:   62600, Reward: [-428.789 -428.789 -428.789] [75.961], Avg: [-427.979 -427.979 -427.979] (0.1000) ({r_i: None, r_t: [-834.634 -834.634 -834.634], eps: 0.1})
Step:  172600, Reward: [-379.519 -379.519 -379.519] [60.882], Avg: [-447.949 -447.949 -447.949] (0.0100) ({r_i: None, r_t: [-899.590 -899.590 -899.590], eps: 0.01})
Step:   95000, Reward: [-720.770 -720.770 -720.770] [316.585], Avg: [-488.388 -488.388 -488.388] (0.0100) ({r_i: None, r_t: [-1641.811 -1641.811 -1641.811], eps: 0.01})
Step:  172700, Reward: [-425.986 -425.986 -425.986] [91.214], Avg: [-447.936 -447.936 -447.936] (0.0100) ({r_i: None, r_t: [-833.522 -833.522 -833.522], eps: 0.01})
Step:   62700, Reward: [-462.730 -462.730 -462.730] [122.186], Avg: [-428.035 -428.035 -428.035] (0.1000) ({r_i: None, r_t: [-861.814 -861.814 -861.814], eps: 0.1})
Step:   95100, Reward: [-851.633 -851.633 -851.633] [377.440], Avg: [-488.770 -488.770 -488.770] (0.0100) ({r_i: None, r_t: [-1739.846 -1739.846 -1739.846], eps: 0.01})
Step:  172800, Reward: [-365.502 -365.502 -365.502] [73.693], Avg: [-447.888 -447.888 -447.888] (0.0100) ({r_i: None, r_t: [-846.297 -846.297 -846.297], eps: 0.01})
Step:   62800, Reward: [-425.805 -425.805 -425.805] [87.278], Avg: [-428.031 -428.031 -428.031] (0.1000) ({r_i: None, r_t: [-860.347 -860.347 -860.347], eps: 0.1})
Step:   95200, Reward: [-929.353 -929.353 -929.353] [497.312], Avg: [-489.232 -489.232 -489.232] (0.0100) ({r_i: None, r_t: [-1816.524 -1816.524 -1816.524], eps: 0.01})
Step:  172900, Reward: [-429.810 -429.810 -429.810] [104.502], Avg: [-447.878 -447.878 -447.878] (0.0100) ({r_i: None, r_t: [-850.884 -850.884 -850.884], eps: 0.01})
Step:  173000, Reward: [-434.572 -434.572 -434.572] [84.436], Avg: [-447.870 -447.870 -447.870] (0.0100) ({r_i: None, r_t: [-863.444 -863.444 -863.444], eps: 0.01})
Step:   62900, Reward: [-460.956 -460.956 -460.956] [62.936], Avg: [-428.083 -428.083 -428.083] (0.1000) ({r_i: None, r_t: [-828.192 -828.192 -828.192], eps: 0.1})
Step:   95300, Reward: [-828.663 -828.663 -828.663] [431.865], Avg: [-489.588 -489.588 -489.588] (0.0100) ({r_i: None, r_t: [-1440.766 -1440.766 -1440.766], eps: 0.01})
Step:  173100, Reward: [-425.226 -425.226 -425.226] [55.730], Avg: [-447.857 -447.857 -447.857] (0.0100) ({r_i: None, r_t: [-818.764 -818.764 -818.764], eps: 0.01})
Step:   63000, Reward: [-435.136 -435.136 -435.136] [75.175], Avg: [-428.094 -428.094 -428.094] (0.1000) ({r_i: None, r_t: [-847.177 -847.177 -847.177], eps: 0.1})
Step:   95400, Reward: [-878.981 -878.981 -878.981] [414.359], Avg: [-489.996 -489.996 -489.996] (0.0100) ({r_i: None, r_t: [-1948.089 -1948.089 -1948.089], eps: 0.01})
Step:  173200, Reward: [-433.002 -433.002 -433.002] [63.831], Avg: [-447.849 -447.849 -447.849] (0.0100) ({r_i: None, r_t: [-797.112 -797.112 -797.112], eps: 0.01})
Step:   63100, Reward: [-415.275 -415.275 -415.275] [83.698], Avg: [-428.074 -428.074 -428.074] (0.1000) ({r_i: None, r_t: [-867.035 -867.035 -867.035], eps: 0.1})
Step:   95500, Reward: [-898.352 -898.352 -898.352] [540.781], Avg: [-490.423 -490.423 -490.423] (0.0100) ({r_i: None, r_t: [-1534.089 -1534.089 -1534.089], eps: 0.01})
Step:  173300, Reward: [-400.963 -400.963 -400.963] [61.920], Avg: [-447.822 -447.822 -447.822] (0.0100) ({r_i: None, r_t: [-785.949 -785.949 -785.949], eps: 0.01})
Step:  173400, Reward: [-422.991 -422.991 -422.991] [67.633], Avg: [-447.807 -447.807 -447.807] (0.0100) ({r_i: None, r_t: [-844.178 -844.178 -844.178], eps: 0.01})
Step:   63200, Reward: [-419.022 -419.022 -419.022] [61.144], Avg: [-428.060 -428.060 -428.060] (0.1000) ({r_i: None, r_t: [-826.311 -826.311 -826.311], eps: 0.1})
Step:   95600, Reward: [-818.918 -818.918 -818.918] [439.750], Avg: [-490.766 -490.766 -490.766] (0.0100) ({r_i: None, r_t: [-1451.544 -1451.544 -1451.544], eps: 0.01})
Step:  173500, Reward: [-400.597 -400.597 -400.597] [68.611], Avg: [-447.780 -447.780 -447.780] (0.0100) ({r_i: None, r_t: [-848.907 -848.907 -848.907], eps: 0.01})
Step:   63300, Reward: [-464.731 -464.731 -464.731] [92.181], Avg: [-428.118 -428.118 -428.118] (0.1000) ({r_i: None, r_t: [-815.744 -815.744 -815.744], eps: 0.1})
Step:   95700, Reward: [-808.014 -808.014 -808.014] [551.317], Avg: [-491.097 -491.097 -491.097] (0.0100) ({r_i: None, r_t: [-1719.658 -1719.658 -1719.658], eps: 0.01})
Step:  173600, Reward: [-363.402 -363.402 -363.402] [54.351], Avg: [-447.732 -447.732 -447.732] (0.0100) ({r_i: None, r_t: [-847.969 -847.969 -847.969], eps: 0.01})
Step:   63400, Reward: [-396.713 -396.713 -396.713] [78.347], Avg: [-428.068 -428.068 -428.068] (0.1000) ({r_i: None, r_t: [-831.686 -831.686 -831.686], eps: 0.1})
Step:  173700, Reward: [-410.924 -410.924 -410.924] [72.826], Avg: [-447.710 -447.710 -447.710] (0.0100) ({r_i: None, r_t: [-865.723 -865.723 -865.723], eps: 0.01})
Step:   95800, Reward: [-665.212 -665.212 -665.212] [310.023], Avg: [-491.279 -491.279 -491.279] (0.0100) ({r_i: None, r_t: [-1631.173 -1631.173 -1631.173], eps: 0.01})
Step:  173800, Reward: [-416.808 -416.808 -416.808] [75.735], Avg: [-447.693 -447.693 -447.693] (0.0100) ({r_i: None, r_t: [-901.311 -901.311 -901.311], eps: 0.01})
Step:   63500, Reward: [-426.609 -426.609 -426.609] [70.127], Avg: [-428.066 -428.066 -428.066] (0.1000) ({r_i: None, r_t: [-873.294 -873.294 -873.294], eps: 0.1})
Step:   95900, Reward: [-825.613 -825.613 -825.613] [421.598], Avg: [-491.627 -491.627 -491.627] (0.0100) ({r_i: None, r_t: [-1616.003 -1616.003 -1616.003], eps: 0.01})
Step:  173900, Reward: [-429.556 -429.556 -429.556] [110.878], Avg: [-447.682 -447.682 -447.682] (0.0100) ({r_i: None, r_t: [-826.749 -826.749 -826.749], eps: 0.01})
Step:   63600, Reward: [-428.968 -428.968 -428.968] [76.666], Avg: [-428.067 -428.067 -428.067] (0.1000) ({r_i: None, r_t: [-858.928 -858.928 -858.928], eps: 0.1})
Step:   96000, Reward: [-902.419 -902.419 -902.419] [482.161], Avg: [-492.055 -492.055 -492.055] (0.0100) ({r_i: None, r_t: [-1404.031 -1404.031 -1404.031], eps: 0.01})
Step:  174000, Reward: [-361.538 -361.538 -361.538] [71.455], Avg: [-447.633 -447.633 -447.633] (0.0100) ({r_i: None, r_t: [-853.103 -853.103 -853.103], eps: 0.01})
Step:   63700, Reward: [-420.859 -420.859 -420.859] [91.646], Avg: [-428.056 -428.056 -428.056] (0.1000) ({r_i: None, r_t: [-832.244 -832.244 -832.244], eps: 0.1})
Step:   96100, Reward: [-590.456 -590.456 -590.456] [199.216], Avg: [-492.157 -492.157 -492.157] (0.0100) ({r_i: None, r_t: [-1597.338 -1597.338 -1597.338], eps: 0.01})
Step:  174100, Reward: [-421.707 -421.707 -421.707] [85.713], Avg: [-447.618 -447.618 -447.618] (0.0100) ({r_i: None, r_t: [-847.832 -847.832 -847.832], eps: 0.01})
Step:  174200, Reward: [-422.979 -422.979 -422.979] [91.315], Avg: [-447.604 -447.604 -447.604] (0.0100) ({r_i: None, r_t: [-849.623 -849.623 -849.623], eps: 0.01})
Step:   63800, Reward: [-427.339 -427.339 -427.339] [54.049], Avg: [-428.055 -428.055 -428.055] (0.1000) ({r_i: None, r_t: [-833.764 -833.764 -833.764], eps: 0.1})
Step:   96200, Reward: [-649.737 -649.737 -649.737] [319.929], Avg: [-492.321 -492.321 -492.321] (0.0100) ({r_i: None, r_t: [-1326.500 -1326.500 -1326.500], eps: 0.01})
Step:  174300, Reward: [-375.479 -375.479 -375.479] [55.903], Avg: [-447.562 -447.562 -447.562] (0.0100) ({r_i: None, r_t: [-908.622 -908.622 -908.622], eps: 0.01})
Step:   63900, Reward: [-468.008 -468.008 -468.008] [92.372], Avg: [-428.117 -428.117 -428.117] (0.1000) ({r_i: None, r_t: [-839.422 -839.422 -839.422], eps: 0.1})
Step:   96300, Reward: [-601.155 -601.155 -601.155] [257.927], Avg: [-492.434 -492.434 -492.434] (0.0100) ({r_i: None, r_t: [-1622.446 -1622.446 -1622.446], eps: 0.01})
Step:  174400, Reward: [-461.275 -461.275 -461.275] [77.783], Avg: [-447.570 -447.570 -447.570] (0.0100) ({r_i: None, r_t: [-843.936 -843.936 -843.936], eps: 0.01})
Step:   64000, Reward: [-421.769 -421.769 -421.769] [70.288], Avg: [-428.107 -428.107 -428.107] (0.1000) ({r_i: None, r_t: [-794.422 -794.422 -794.422], eps: 0.1})
Step:   96400, Reward: [-729.649 -729.649 -729.649] [328.097], Avg: [-492.679 -492.679 -492.679] (0.0100) ({r_i: None, r_t: [-1534.437 -1534.437 -1534.437], eps: 0.01})
Step:  174500, Reward: [-415.474 -415.474 -415.474] [106.525], Avg: [-447.552 -447.552 -447.552] (0.0100) ({r_i: None, r_t: [-869.671 -869.671 -869.671], eps: 0.01})
Step:  174600, Reward: [-398.149 -398.149 -398.149] [63.731], Avg: [-447.523 -447.523 -447.523] (0.0100) ({r_i: None, r_t: [-844.920 -844.920 -844.920], eps: 0.01})
Step:   64100, Reward: [-422.131 -422.131 -422.131] [87.250], Avg: [-428.098 -428.098 -428.098] (0.1000) ({r_i: None, r_t: [-851.775 -851.775 -851.775], eps: 0.1})
Step:   96500, Reward: [-712.881 -712.881 -712.881] [415.986], Avg: [-492.907 -492.907 -492.907] (0.0100) ({r_i: None, r_t: [-1259.290 -1259.290 -1259.290], eps: 0.01})
Step:  174700, Reward: [-379.376 -379.376 -379.376] [46.818], Avg: [-447.484 -447.484 -447.484] (0.0100) ({r_i: None, r_t: [-875.690 -875.690 -875.690], eps: 0.01})
Step:   64200, Reward: [-395.639 -395.639 -395.639] [66.515], Avg: [-428.048 -428.048 -428.048] (0.1000) ({r_i: None, r_t: [-815.041 -815.041 -815.041], eps: 0.1})
Step:   96600, Reward: [-635.323 -635.323 -635.323] [372.910], Avg: [-493.055 -493.055 -493.055] (0.0100) ({r_i: None, r_t: [-1280.102 -1280.102 -1280.102], eps: 0.01})
Step:  174800, Reward: [-389.637 -389.637 -389.637] [60.843], Avg: [-447.451 -447.451 -447.451] (0.0100) ({r_i: None, r_t: [-836.025 -836.025 -836.025], eps: 0.01})
Step:   64300, Reward: [-430.290 -430.290 -430.290] [61.467], Avg: [-428.051 -428.051 -428.051] (0.1000) ({r_i: None, r_t: [-813.863 -813.863 -813.863], eps: 0.1})
Step:   96700, Reward: [-787.763 -787.763 -787.763] [373.413], Avg: [-493.359 -493.359 -493.359] (0.0100) ({r_i: None, r_t: [-1366.261 -1366.261 -1366.261], eps: 0.01})
Step:  174900, Reward: [-414.711 -414.711 -414.711] [108.042], Avg: [-447.433 -447.433 -447.433] (0.0100) ({r_i: None, r_t: [-875.519 -875.519 -875.519], eps: 0.01})
Step:  175000, Reward: [-414.714 -414.714 -414.714] [96.377], Avg: [-447.414 -447.414 -447.414] (0.0100) ({r_i: None, r_t: [-836.868 -836.868 -836.868], eps: 0.01})
Step:   64400, Reward: [-419.991 -419.991 -419.991] [68.533], Avg: [-428.039 -428.039 -428.039] (0.1000) ({r_i: None, r_t: [-831.829 -831.829 -831.829], eps: 0.1})
Step:   96800, Reward: [-602.996 -602.996 -602.996] [303.960], Avg: [-493.472 -493.472 -493.472] (0.0100) ({r_i: None, r_t: [-1241.604 -1241.604 -1241.604], eps: 0.01})
Step:  175100, Reward: [-405.992 -405.992 -405.992] [85.035], Avg: [-447.390 -447.390 -447.390] (0.0100) ({r_i: None, r_t: [-826.576 -826.576 -826.576], eps: 0.01})
Step:   64500, Reward: [-425.343 -425.343 -425.343] [111.824], Avg: [-428.034 -428.034 -428.034] (0.1000) ({r_i: None, r_t: [-841.305 -841.305 -841.305], eps: 0.1})
Step:   96900, Reward: [-636.027 -636.027 -636.027] [277.209], Avg: [-493.619 -493.619 -493.619] (0.0100) ({r_i: None, r_t: [-1194.769 -1194.769 -1194.769], eps: 0.01})
Step:  175200, Reward: [-443.870 -443.870 -443.870] [119.069], Avg: [-447.388 -447.388 -447.388] (0.0100) ({r_i: None, r_t: [-832.159 -832.159 -832.159], eps: 0.01})
Step:   64600, Reward: [-441.154 -441.154 -441.154] [86.275], Avg: [-428.055 -428.055 -428.055] (0.1000) ({r_i: None, r_t: [-847.191 -847.191 -847.191], eps: 0.1})
Step:  175300, Reward: [-432.633 -432.633 -432.633] [92.180], Avg: [-447.380 -447.380 -447.380] (0.0100) ({r_i: None, r_t: [-825.989 -825.989 -825.989], eps: 0.01})
Step:   97000, Reward: [-625.035 -625.035 -625.035] [251.316], Avg: [-493.754 -493.754 -493.754] (0.0100) ({r_i: None, r_t: [-1342.536 -1342.536 -1342.536], eps: 0.01})
Step:  175400, Reward: [-404.601 -404.601 -404.601] [85.727], Avg: [-447.356 -447.356 -447.356] (0.0100) ({r_i: None, r_t: [-837.309 -837.309 -837.309], eps: 0.01})
Step:   64700, Reward: [-424.044 -424.044 -424.044] [102.632], Avg: [-428.049 -428.049 -428.049] (0.1000) ({r_i: None, r_t: [-821.761 -821.761 -821.761], eps: 0.1})
Step:   97100, Reward: [-658.132 -658.132 -658.132] [247.945], Avg: [-493.924 -493.924 -493.924] (0.0100) ({r_i: None, r_t: [-1446.126 -1446.126 -1446.126], eps: 0.01})
Step:  175500, Reward: [-377.827 -377.827 -377.827] [71.259], Avg: [-447.316 -447.316 -447.316] (0.0100) ({r_i: None, r_t: [-777.017 -777.017 -777.017], eps: 0.01})
Step:   64800, Reward: [-425.650 -425.650 -425.650] [75.419], Avg: [-428.045 -428.045 -428.045] (0.1000) ({r_i: None, r_t: [-820.288 -820.288 -820.288], eps: 0.1})
Step:   97200, Reward: [-554.851 -554.851 -554.851] [285.573], Avg: [-493.986 -493.986 -493.986] (0.0100) ({r_i: None, r_t: [-1254.399 -1254.399 -1254.399], eps: 0.01})
Step:  175600, Reward: [-427.312 -427.312 -427.312] [72.163], Avg: [-447.305 -447.305 -447.305] (0.0100) ({r_i: None, r_t: [-837.793 -837.793 -837.793], eps: 0.01})
Step:   64900, Reward: [-373.070 -373.070 -373.070] [61.757], Avg: [-427.960 -427.960 -427.960] (0.1000) ({r_i: None, r_t: [-887.735 -887.735 -887.735], eps: 0.1})
Step:  175700, Reward: [-390.898 -390.898 -390.898] [69.683], Avg: [-447.273 -447.273 -447.273] (0.0100) ({r_i: None, r_t: [-864.012 -864.012 -864.012], eps: 0.01})
Step:   97300, Reward: [-617.284 -617.284 -617.284] [271.765], Avg: [-494.113 -494.113 -494.113] (0.0100) ({r_i: None, r_t: [-1196.417 -1196.417 -1196.417], eps: 0.01})
Step:  175800, Reward: [-434.858 -434.858 -434.858] [68.981], Avg: [-447.265 -447.265 -447.265] (0.0100) ({r_i: None, r_t: [-860.918 -860.918 -860.918], eps: 0.01})
Step:   65000, Reward: [-402.815 -402.815 -402.815] [45.361], Avg: [-427.922 -427.922 -427.922] (0.1000) ({r_i: None, r_t: [-802.778 -802.778 -802.778], eps: 0.1})
Step:   97400, Reward: [-582.884 -582.884 -582.884] [288.265], Avg: [-494.204 -494.204 -494.204] (0.0100) ({r_i: None, r_t: [-1135.798 -1135.798 -1135.798], eps: 0.01})
Step:  175900, Reward: [-406.209 -406.209 -406.209] [59.998], Avg: [-447.242 -447.242 -447.242] (0.0100) ({r_i: None, r_t: [-838.736 -838.736 -838.736], eps: 0.01})
Step:   65100, Reward: [-392.379 -392.379 -392.379] [84.252], Avg: [-427.867 -427.867 -427.867] (0.1000) ({r_i: None, r_t: [-812.984 -812.984 -812.984], eps: 0.1})
Step:   97500, Reward: [-619.934 -619.934 -619.934] [218.438], Avg: [-494.333 -494.333 -494.333] (0.0100) ({r_i: None, r_t: [-1179.331 -1179.331 -1179.331], eps: 0.01})
Step:  176000, Reward: [-415.939 -415.939 -415.939] [93.587], Avg: [-447.224 -447.224 -447.224] (0.0100) ({r_i: None, r_t: [-859.765 -859.765 -859.765], eps: 0.01})
Step:   65200, Reward: [-412.529 -412.529 -412.529] [60.947], Avg: [-427.844 -427.844 -427.844] (0.1000) ({r_i: None, r_t: [-817.400 -817.400 -817.400], eps: 0.1})
Step:  176100, Reward: [-454.289 -454.289 -454.289] [93.040], Avg: [-447.228 -447.228 -447.228] (0.0100) ({r_i: None, r_t: [-885.630 -885.630 -885.630], eps: 0.01})
Step:   97600, Reward: [-494.381 -494.381 -494.381] [245.386], Avg: [-494.333 -494.333 -494.333] (0.0100) ({r_i: None, r_t: [-1054.871 -1054.871 -1054.871], eps: 0.01})
Step:   65300, Reward: [-421.633 -421.633 -421.633] [92.126], Avg: [-427.834 -427.834 -427.834] (0.1000) ({r_i: None, r_t: [-829.119 -829.119 -829.119], eps: 0.1})
Step:  176200, Reward: [-406.258 -406.258 -406.258] [74.002], Avg: [-447.205 -447.205 -447.205] (0.0100) ({r_i: None, r_t: [-857.486 -857.486 -857.486], eps: 0.01})
Step:   97700, Reward: [-533.295 -533.295 -533.295] [256.807], Avg: [-494.373 -494.373 -494.373] (0.0100) ({r_i: None, r_t: [-1066.160 -1066.160 -1066.160], eps: 0.01})
Step:  176300, Reward: [-463.175 -463.175 -463.175] [105.932], Avg: [-447.214 -447.214 -447.214] (0.0100) ({r_i: None, r_t: [-838.563 -838.563 -838.563], eps: 0.01})
Step:   65400, Reward: [-426.171 -426.171 -426.171] [108.199], Avg: [-427.832 -427.832 -427.832] (0.1000) ({r_i: None, r_t: [-870.028 -870.028 -870.028], eps: 0.1})
Step:   97800, Reward: [-567.744 -567.744 -567.744] [213.977], Avg: [-494.447 -494.447 -494.447] (0.0100) ({r_i: None, r_t: [-980.203 -980.203 -980.203], eps: 0.01})
Step:  176400, Reward: [-399.124 -399.124 -399.124] [54.488], Avg: [-447.187 -447.187 -447.187] (0.0100) ({r_i: None, r_t: [-826.329 -826.329 -826.329], eps: 0.01})
Step:   65500, Reward: [-403.141 -403.141 -403.141] [72.739], Avg: [-427.794 -427.794 -427.794] (0.1000) ({r_i: None, r_t: [-795.999 -795.999 -795.999], eps: 0.1})
Step:  176500, Reward: [-416.489 -416.489 -416.489] [90.347], Avg: [-447.170 -447.170 -447.170] (0.0100) ({r_i: None, r_t: [-801.406 -801.406 -801.406], eps: 0.01})
Step:   97900, Reward: [-452.968 -452.968 -452.968] [61.332], Avg: [-494.405 -494.405 -494.405] (0.0100) ({r_i: None, r_t: [-1089.318 -1089.318 -1089.318], eps: 0.01})
Step:  176600, Reward: [-462.130 -462.130 -462.130] [87.435], Avg: [-447.178 -447.178 -447.178] (0.0100) ({r_i: None, r_t: [-803.521 -803.521 -803.521], eps: 0.01})
Step:   65600, Reward: [-391.275 -391.275 -391.275] [60.496], Avg: [-427.738 -427.738 -427.738] (0.1000) ({r_i: None, r_t: [-871.383 -871.383 -871.383], eps: 0.1})
Step:   98000, Reward: [-450.088 -450.088 -450.088] [107.889], Avg: [-494.360 -494.360 -494.360] (0.0100) ({r_i: None, r_t: [-904.368 -904.368 -904.368], eps: 0.01})
Step:  176700, Reward: [-389.620 -389.620 -389.620] [91.956], Avg: [-447.145 -447.145 -447.145] (0.0100) ({r_i: None, r_t: [-890.457 -890.457 -890.457], eps: 0.01})
Step:   65700, Reward: [-368.749 -368.749 -368.749] [75.280], Avg: [-427.649 -427.649 -427.649] (0.1000) ({r_i: None, r_t: [-828.319 -828.319 -828.319], eps: 0.1})
Step:   98100, Reward: [-592.860 -592.860 -592.860] [252.507], Avg: [-494.460 -494.460 -494.460] (0.0100) ({r_i: None, r_t: [-1128.124 -1128.124 -1128.124], eps: 0.01})
Step:  176800, Reward: [-485.869 -485.869 -485.869] [94.785], Avg: [-447.167 -447.167 -447.167] (0.0100) ({r_i: None, r_t: [-838.107 -838.107 -838.107], eps: 0.01})
Step:   65800, Reward: [-387.202 -387.202 -387.202] [66.412], Avg: [-427.587 -427.587 -427.587] (0.1000) ({r_i: None, r_t: [-861.452 -861.452 -861.452], eps: 0.1})
Step:  176900, Reward: [-392.177 -392.177 -392.177] [98.540], Avg: [-447.136 -447.136 -447.136] (0.0100) ({r_i: None, r_t: [-838.881 -838.881 -838.881], eps: 0.01})
Step:   98200, Reward: [-525.627 -525.627 -525.627] [221.958], Avg: [-494.492 -494.492 -494.492] (0.0100) ({r_i: None, r_t: [-1021.886 -1021.886 -1021.886], eps: 0.01})
Step:   65900, Reward: [-434.871 -434.871 -434.871] [95.914], Avg: [-427.598 -427.598 -427.598] (0.1000) ({r_i: None, r_t: [-822.942 -822.942 -822.942], eps: 0.1})
Step:  177000, Reward: [-414.556 -414.556 -414.556] [90.338], Avg: [-447.118 -447.118 -447.118] (0.0100) ({r_i: None, r_t: [-855.902 -855.902 -855.902], eps: 0.01})
Step:   98300, Reward: [-480.084 -480.084 -480.084] [135.914], Avg: [-494.477 -494.477 -494.477] (0.0100) ({r_i: None, r_t: [-963.966 -963.966 -963.966], eps: 0.01})
Step:  177100, Reward: [-433.222 -433.222 -433.222] [88.102], Avg: [-447.110 -447.110 -447.110] (0.0100) ({r_i: None, r_t: [-847.110 -847.110 -847.110], eps: 0.01})
Step:   66000, Reward: [-411.185 -411.185 -411.185] [72.062], Avg: [-427.574 -427.574 -427.574] (0.1000) ({r_i: None, r_t: [-813.064 -813.064 -813.064], eps: 0.1})
Step:   98400, Reward: [-485.165 -485.165 -485.165] [151.912], Avg: [-494.468 -494.468 -494.468] (0.0100) ({r_i: None, r_t: [-949.845 -949.845 -949.845], eps: 0.01})
Step:  177200, Reward: [-364.384 -364.384 -364.384] [82.290], Avg: [-447.063 -447.063 -447.063] (0.0100) ({r_i: None, r_t: [-862.217 -862.217 -862.217], eps: 0.01})
Step:   66100, Reward: [-428.256 -428.256 -428.256] [77.885], Avg: [-427.575 -427.575 -427.575] (0.1000) ({r_i: None, r_t: [-847.534 -847.534 -847.534], eps: 0.1})
Step:  177300, Reward: [-396.506 -396.506 -396.506] [76.966], Avg: [-447.035 -447.035 -447.035] (0.0100) ({r_i: None, r_t: [-855.029 -855.029 -855.029], eps: 0.01})
Step:   98500, Reward: [-532.280 -532.280 -532.280] [234.755], Avg: [-494.506 -494.506 -494.506] (0.0100) ({r_i: None, r_t: [-970.287 -970.287 -970.287], eps: 0.01})
Step:   66200, Reward: [-420.858 -420.858 -420.858] [55.178], Avg: [-427.564 -427.564 -427.564] (0.1000) ({r_i: None, r_t: [-785.565 -785.565 -785.565], eps: 0.1})
Step:  177400, Reward: [-439.833 -439.833 -439.833] [115.538], Avg: [-447.031 -447.031 -447.031] (0.0100) ({r_i: None, r_t: [-868.005 -868.005 -868.005], eps: 0.01})
Step:   98600, Reward: [-445.921 -445.921 -445.921] [95.641], Avg: [-494.457 -494.457 -494.457] (0.0100) ({r_i: None, r_t: [-956.150 -956.150 -956.150], eps: 0.01})
Step:  177500, Reward: [-432.107 -432.107 -432.107] [79.414], Avg: [-447.022 -447.022 -447.022] (0.0100) ({r_i: None, r_t: [-864.342 -864.342 -864.342], eps: 0.01})
Step:   66300, Reward: [-399.886 -399.886 -399.886] [70.566], Avg: [-427.523 -427.523 -427.523] (0.1000) ({r_i: None, r_t: [-777.397 -777.397 -777.397], eps: 0.1})
Step:   98700, Reward: [-414.587 -414.587 -414.587] [59.254], Avg: [-494.376 -494.376 -494.376] (0.0100) ({r_i: None, r_t: [-1031.573 -1031.573 -1031.573], eps: 0.01})
Step:  177600, Reward: [-442.136 -442.136 -442.136] [87.233], Avg: [-447.020 -447.020 -447.020] (0.0100) ({r_i: None, r_t: [-822.885 -822.885 -822.885], eps: 0.01})
Step:   66400, Reward: [-382.120 -382.120 -382.120] [69.168], Avg: [-427.455 -427.455 -427.455] (0.1000) ({r_i: None, r_t: [-820.262 -820.262 -820.262], eps: 0.1})
Step:  177700, Reward: [-406.927 -406.927 -406.927] [70.719], Avg: [-446.997 -446.997 -446.997] (0.0100) ({r_i: None, r_t: [-814.104 -814.104 -814.104], eps: 0.01})
Step:   98800, Reward: [-450.693 -450.693 -450.693] [86.355], Avg: [-494.332 -494.332 -494.332] (0.0100) ({r_i: None, r_t: [-907.910 -907.910 -907.910], eps: 0.01})
Step:   66500, Reward: [-425.986 -425.986 -425.986] [61.354], Avg: [-427.452 -427.452 -427.452] (0.1000) ({r_i: None, r_t: [-833.182 -833.182 -833.182], eps: 0.1})
Step:  177800, Reward: [-398.772 -398.772 -398.772] [87.535], Avg: [-446.970 -446.970 -446.970] (0.0100) ({r_i: None, r_t: [-778.826 -778.826 -778.826], eps: 0.01})
Step:   98900, Reward: [-439.837 -439.837 -439.837] [144.787], Avg: [-494.277 -494.277 -494.277] (0.0100) ({r_i: None, r_t: [-963.061 -963.061 -963.061], eps: 0.01})
Step:  177900, Reward: [-426.598 -426.598 -426.598] [80.786], Avg: [-446.959 -446.959 -446.959] (0.0100) ({r_i: None, r_t: [-851.984 -851.984 -851.984], eps: 0.01})
Step:   66600, Reward: [-409.440 -409.440 -409.440] [79.203], Avg: [-427.425 -427.425 -427.425] (0.1000) ({r_i: None, r_t: [-813.456 -813.456 -813.456], eps: 0.1})
Step:   99000, Reward: [-465.640 -465.640 -465.640] [102.119], Avg: [-494.248 -494.248 -494.248] (0.0100) ({r_i: None, r_t: [-945.261 -945.261 -945.261], eps: 0.01})
Step:  178000, Reward: [-408.290 -408.290 -408.290] [80.345], Avg: [-446.937 -446.937 -446.937] (0.0100) ({r_i: None, r_t: [-790.694 -790.694 -790.694], eps: 0.01})
Step:   66700, Reward: [-415.502 -415.502 -415.502] [88.878], Avg: [-427.407 -427.407 -427.407] (0.1000) ({r_i: None, r_t: [-807.456 -807.456 -807.456], eps: 0.1})
Step:   99100, Reward: [-461.219 -461.219 -461.219] [89.103], Avg: [-494.215 -494.215 -494.215] (0.0100) ({r_i: None, r_t: [-932.687 -932.687 -932.687], eps: 0.01})
Step:  178100, Reward: [-466.382 -466.382 -466.382] [76.454], Avg: [-446.948 -446.948 -446.948] (0.0100) ({r_i: None, r_t: [-901.253 -901.253 -901.253], eps: 0.01})
Step:   66800, Reward: [-440.915 -440.915 -440.915] [84.780], Avg: [-427.428 -427.428 -427.428] (0.1000) ({r_i: None, r_t: [-854.070 -854.070 -854.070], eps: 0.1})
Step:  178200, Reward: [-380.939 -380.939 -380.939] [68.915], Avg: [-446.911 -446.911 -446.911] (0.0100) ({r_i: None, r_t: [-815.766 -815.766 -815.766], eps: 0.01})
Step:   99200, Reward: [-465.540 -465.540 -465.540] [86.742], Avg: [-494.186 -494.186 -494.186] (0.0100) ({r_i: None, r_t: [-854.367 -854.367 -854.367], eps: 0.01})
Step:  178300, Reward: [-449.169 -449.169 -449.169] [85.256], Avg: [-446.912 -446.912 -446.912] (0.0100) ({r_i: None, r_t: [-844.349 -844.349 -844.349], eps: 0.01})
Step:   66900, Reward: [-408.457 -408.457 -408.457] [89.029], Avg: [-427.399 -427.399 -427.399] (0.1000) ({r_i: None, r_t: [-855.706 -855.706 -855.706], eps: 0.1})
Step:   99300, Reward: [-441.433 -441.433 -441.433] [68.086], Avg: [-494.133 -494.133 -494.133] (0.0100) ({r_i: None, r_t: [-849.360 -849.360 -849.360], eps: 0.01})
Step:  178400, Reward: [-455.286 -455.286 -455.286] [88.826], Avg: [-446.917 -446.917 -446.917] (0.0100) ({r_i: None, r_t: [-807.014 -807.014 -807.014], eps: 0.01})
Step:   67000, Reward: [-374.851 -374.851 -374.851] [68.119], Avg: [-427.321 -427.321 -427.321] (0.1000) ({r_i: None, r_t: [-832.431 -832.431 -832.431], eps: 0.1})
Step:  178500, Reward: [-458.179 -458.179 -458.179] [117.824], Avg: [-446.923 -446.923 -446.923] (0.0100) ({r_i: None, r_t: [-856.244 -856.244 -856.244], eps: 0.01})
Step:   99400, Reward: [-472.962 -472.962 -472.962] [110.856], Avg: [-494.112 -494.112 -494.112] (0.0100) ({r_i: None, r_t: [-900.869 -900.869 -900.869], eps: 0.01})
Step:   67100, Reward: [-386.452 -386.452 -386.452] [59.759], Avg: [-427.260 -427.260 -427.260] (0.1000) ({r_i: None, r_t: [-783.561 -783.561 -783.561], eps: 0.1})
Step:  178600, Reward: [-389.932 -389.932 -389.932] [53.330], Avg: [-446.891 -446.891 -446.891] (0.0100) ({r_i: None, r_t: [-780.123 -780.123 -780.123], eps: 0.01})
Step:   99500, Reward: [-438.017 -438.017 -438.017] [79.553], Avg: [-494.055 -494.055 -494.055] (0.0100) ({r_i: None, r_t: [-864.665 -864.665 -864.665], eps: 0.01})
Step:  178700, Reward: [-403.520 -403.520 -403.520] [54.024], Avg: [-446.867 -446.867 -446.867] (0.0100) ({r_i: None, r_t: [-812.929 -812.929 -812.929], eps: 0.01})
Step:   67200, Reward: [-389.252 -389.252 -389.252] [53.987], Avg: [-427.204 -427.204 -427.204] (0.1000) ({r_i: None, r_t: [-809.062 -809.062 -809.062], eps: 0.1})
Step:   99600, Reward: [-396.617 -396.617 -396.617] [56.259], Avg: [-493.958 -493.958 -493.958] (0.0100) ({r_i: None, r_t: [-905.398 -905.398 -905.398], eps: 0.01})
Step:  178800, Reward: [-400.112 -400.112 -400.112] [84.697], Avg: [-446.841 -446.841 -446.841] (0.0100) ({r_i: None, r_t: [-792.210 -792.210 -792.210], eps: 0.01})
Step:   67300, Reward: [-429.340 -429.340 -429.340] [61.588], Avg: [-427.207 -427.207 -427.207] (0.1000) ({r_i: None, r_t: [-819.454 -819.454 -819.454], eps: 0.1})
Step:  178900, Reward: [-459.827 -459.827 -459.827] [122.782], Avg: [-446.848 -446.848 -446.848] (0.0100) ({r_i: None, r_t: [-873.507 -873.507 -873.507], eps: 0.01})
Step:   99700, Reward: [-443.943 -443.943 -443.943] [60.645], Avg: [-493.907 -493.907 -493.907] (0.0100) ({r_i: None, r_t: [-895.836 -895.836 -895.836], eps: 0.01})
Step:   67400, Reward: [-387.454 -387.454 -387.454] [60.752], Avg: [-427.148 -427.148 -427.148] (0.1000) ({r_i: None, r_t: [-843.512 -843.512 -843.512], eps: 0.1})
Step:  179000, Reward: [-412.091 -412.091 -412.091] [47.291], Avg: [-446.829 -446.829 -446.829] (0.0100) ({r_i: None, r_t: [-793.022 -793.022 -793.022], eps: 0.01})
Step:   99800, Reward: [-419.494 -419.494 -419.494] [75.507], Avg: [-493.833 -493.833 -493.833] (0.0100) ({r_i: None, r_t: [-855.411 -855.411 -855.411], eps: 0.01})
Step:  179100, Reward: [-405.946 -405.946 -405.946] [87.352], Avg: [-446.806 -446.806 -446.806] (0.0100) ({r_i: None, r_t: [-833.414 -833.414 -833.414], eps: 0.01})
Step:   67500, Reward: [-415.489 -415.489 -415.489] [95.412], Avg: [-427.131 -427.131 -427.131] (0.1000) ({r_i: None, r_t: [-825.142 -825.142 -825.142], eps: 0.1})
Step:   99900, Reward: [-461.342 -461.342 -461.342] [96.177], Avg: [-493.800 -493.800 -493.800] (0.0100) ({r_i: None, r_t: [-893.657 -893.657 -893.657], eps: 0.01})
Step:  179200, Reward: [-418.386 -418.386 -418.386] [105.668], Avg: [-446.790 -446.790 -446.790] (0.0100) ({r_i: None, r_t: [-833.874 -833.874 -833.874], eps: 0.01})
Step:   67600, Reward: [-434.200 -434.200 -434.200] [57.677], Avg: [-427.141 -427.141 -427.141] (0.1000) ({r_i: None, r_t: [-846.553 -846.553 -846.553], eps: 0.1})
Step:  179300, Reward: [-399.237 -399.237 -399.237] [79.758], Avg: [-446.763 -446.763 -446.763] (0.0100) ({r_i: None, r_t: [-800.200 -800.200 -800.200], eps: 0.01})
Step:  100000, Reward: [-389.140 -389.140 -389.140] [48.740], Avg: [-493.696 -493.696 -493.696] (0.0100) ({r_i: None, r_t: [-840.963 -840.963 -840.963], eps: 0.01})
Step:   67700, Reward: [-384.827 -384.827 -384.827] [56.114], Avg: [-427.079 -427.079 -427.079] (0.1000) ({r_i: None, r_t: [-789.482 -789.482 -789.482], eps: 0.1})
Step:  179400, Reward: [-394.497 -394.497 -394.497] [53.070], Avg: [-446.734 -446.734 -446.734] (0.0100) ({r_i: None, r_t: [-888.401 -888.401 -888.401], eps: 0.01})
Step:  100100, Reward: [-417.160 -417.160 -417.160] [102.593], Avg: [-493.619 -493.619 -493.619] (0.0100) ({r_i: None, r_t: [-846.044 -846.044 -846.044], eps: 0.01})
Step:  179500, Reward: [-450.451 -450.451 -450.451] [82.438], Avg: [-446.736 -446.736 -446.736] (0.0100) ({r_i: None, r_t: [-810.939 -810.939 -810.939], eps: 0.01})
Step:   67800, Reward: [-390.169 -390.169 -390.169] [67.491], Avg: [-427.024 -427.024 -427.024] (0.1000) ({r_i: None, r_t: [-816.473 -816.473 -816.473], eps: 0.1})
Step:  100200, Reward: [-422.076 -422.076 -422.076] [81.319], Avg: [-493.548 -493.548 -493.548] (0.0100) ({r_i: None, r_t: [-857.144 -857.144 -857.144], eps: 0.01})
Step:  179600, Reward: [-454.854 -454.854 -454.854] [104.747], Avg: [-446.741 -446.741 -446.741] (0.0100) ({r_i: None, r_t: [-791.915 -791.915 -791.915], eps: 0.01})
Step:   67900, Reward: [-399.071 -399.071 -399.071] [81.228], Avg: [-426.983 -426.983 -426.983] (0.1000) ({r_i: None, r_t: [-846.463 -846.463 -846.463], eps: 0.1})
Step:  179700, Reward: [-420.683 -420.683 -420.683] [65.726], Avg: [-446.726 -446.726 -446.726] (0.0100) ({r_i: None, r_t: [-813.965 -813.965 -813.965], eps: 0.01})
Step:  100300, Reward: [-390.620 -390.620 -390.620] [65.828], Avg: [-493.446 -493.446 -493.446] (0.0100) ({r_i: None, r_t: [-856.591 -856.591 -856.591], eps: 0.01})
Step:  179800, Reward: [-423.678 -423.678 -423.678] [69.700], Avg: [-446.714 -446.714 -446.714] (0.0100) ({r_i: None, r_t: [-861.201 -861.201 -861.201], eps: 0.01})
Step:   68000, Reward: [-375.235 -375.235 -375.235] [58.674], Avg: [-426.907 -426.907 -426.907] (0.1000) ({r_i: None, r_t: [-806.911 -806.911 -806.911], eps: 0.1})
Step:  100400, Reward: [-381.636 -381.636 -381.636] [77.935], Avg: [-493.334 -493.334 -493.334] (0.0100) ({r_i: None, r_t: [-872.965 -872.965 -872.965], eps: 0.01})
Step:  179900, Reward: [-448.352 -448.352 -448.352] [82.449], Avg: [-446.714 -446.714 -446.714] (0.0100) ({r_i: None, r_t: [-813.214 -813.214 -813.214], eps: 0.01})
Step:   68100, Reward: [-398.628 -398.628 -398.628] [55.591], Avg: [-426.866 -426.866 -426.866] (0.1000) ({r_i: None, r_t: [-821.619 -821.619 -821.619], eps: 0.1})
Step:  100500, Reward: [-445.638 -445.638 -445.638] [78.771], Avg: [-493.287 -493.287 -493.287] (0.0100) ({r_i: None, r_t: [-774.516 -774.516 -774.516], eps: 0.01})
Step:  180000, Reward: [-415.881 -415.881 -415.881] [78.713], Avg: [-446.697 -446.697 -446.697] (0.0100) ({r_i: None, r_t: [-825.932 -825.932 -825.932], eps: 0.01})
Step:   68200, Reward: [-381.042 -381.042 -381.042] [43.264], Avg: [-426.799 -426.799 -426.799] (0.1000) ({r_i: None, r_t: [-802.273 -802.273 -802.273], eps: 0.1})
Step:  180100, Reward: [-384.593 -384.593 -384.593] [63.579], Avg: [-446.663 -446.663 -446.663] (0.0100) ({r_i: None, r_t: [-807.347 -807.347 -807.347], eps: 0.01})
Step:  100600, Reward: [-399.149 -399.149 -399.149] [61.395], Avg: [-493.193 -493.193 -493.193] (0.0100) ({r_i: None, r_t: [-861.417 -861.417 -861.417], eps: 0.01})
Step:  180200, Reward: [-440.185 -440.185 -440.185] [101.073], Avg: [-446.659 -446.659 -446.659] (0.0100) ({r_i: None, r_t: [-802.156 -802.156 -802.156], eps: 0.01})
Step:   68300, Reward: [-421.723 -421.723 -421.723] [79.006], Avg: [-426.791 -426.791 -426.791] (0.1000) ({r_i: None, r_t: [-830.188 -830.188 -830.188], eps: 0.1})
Step:  100700, Reward: [-430.784 -430.784 -430.784] [67.594], Avg: [-493.132 -493.132 -493.132] (0.0100) ({r_i: None, r_t: [-830.287 -830.287 -830.287], eps: 0.01})
Step:  180300, Reward: [-411.990 -411.990 -411.990] [75.990], Avg: [-446.640 -446.640 -446.640] (0.0100) ({r_i: None, r_t: [-839.397 -839.397 -839.397], eps: 0.01})
Step:   68400, Reward: [-439.095 -439.095 -439.095] [87.471], Avg: [-426.809 -426.809 -426.809] (0.1000) ({r_i: None, r_t: [-815.008 -815.008 -815.008], eps: 0.1})
Step:  100800, Reward: [-389.371 -389.371 -389.371] [80.083], Avg: [-493.029 -493.029 -493.029] (0.0100) ({r_i: None, r_t: [-886.465 -886.465 -886.465], eps: 0.01})
Step:  180400, Reward: [-442.293 -442.293 -442.293] [121.661], Avg: [-446.638 -446.638 -446.638] (0.0100) ({r_i: None, r_t: [-778.439 -778.439 -778.439], eps: 0.01})
Step:   68500, Reward: [-391.938 -391.938 -391.938] [79.466], Avg: [-426.758 -426.758 -426.758] (0.1000) ({r_i: None, r_t: [-825.958 -825.958 -825.958], eps: 0.1})
Step:  180500, Reward: [-381.135 -381.135 -381.135] [79.554], Avg: [-446.601 -446.601 -446.601] (0.0100) ({r_i: None, r_t: [-814.142 -814.142 -814.142], eps: 0.01})
Step:  100900, Reward: [-416.060 -416.060 -416.060] [85.478], Avg: [-492.953 -492.953 -492.953] (0.0100) ({r_i: None, r_t: [-936.723 -936.723 -936.723], eps: 0.01})
Step:  180600, Reward: [-442.429 -442.429 -442.429] [111.689], Avg: [-446.599 -446.599 -446.599] (0.0100) ({r_i: None, r_t: [-814.616 -814.616 -814.616], eps: 0.01})
Step:   68600, Reward: [-381.529 -381.529 -381.529] [67.752], Avg: [-426.693 -426.693 -426.693] (0.1000) ({r_i: None, r_t: [-803.499 -803.499 -803.499], eps: 0.1})
Step:  101000, Reward: [-414.946 -414.946 -414.946] [83.075], Avg: [-492.875 -492.875 -492.875] (0.0100) ({r_i: None, r_t: [-894.664 -894.664 -894.664], eps: 0.01})
Step:  180700, Reward: [-437.052 -437.052 -437.052] [91.166], Avg: [-446.594 -446.594 -446.594] (0.0100) ({r_i: None, r_t: [-810.209 -810.209 -810.209], eps: 0.01})
Step:   68700, Reward: [-391.821 -391.821 -391.821] [63.199], Avg: [-426.642 -426.642 -426.642] (0.1000) ({r_i: None, r_t: [-784.220 -784.220 -784.220], eps: 0.1})
Step:  101100, Reward: [-448.126 -448.126 -448.126] [82.076], Avg: [-492.831 -492.831 -492.831] (0.0100) ({r_i: None, r_t: [-860.373 -860.373 -860.373], eps: 0.01})
Step:  180800, Reward: [-403.193 -403.193 -403.193] [61.072], Avg: [-446.570 -446.570 -446.570] (0.0100) ({r_i: None, r_t: [-883.938 -883.938 -883.938], eps: 0.01})
Step:   68800, Reward: [-420.706 -420.706 -420.706] [98.258], Avg: [-426.633 -426.633 -426.633] (0.1000) ({r_i: None, r_t: [-806.824 -806.824 -806.824], eps: 0.1})
Step:  180900, Reward: [-405.760 -405.760 -405.760] [72.125], Avg: [-446.547 -446.547 -446.547] (0.0100) ({r_i: None, r_t: [-864.359 -864.359 -864.359], eps: 0.01})
Step:  101200, Reward: [-496.057 -496.057 -496.057] [108.139], Avg: [-492.834 -492.834 -492.834] (0.0100) ({r_i: None, r_t: [-916.587 -916.587 -916.587], eps: 0.01})
Step:  181000, Reward: [-397.175 -397.175 -397.175] [78.653], Avg: [-446.520 -446.520 -446.520] (0.0100) ({r_i: None, r_t: [-839.156 -839.156 -839.156], eps: 0.01})
Step:   68900, Reward: [-405.041 -405.041 -405.041] [59.528], Avg: [-426.602 -426.602 -426.602] (0.1000) ({r_i: None, r_t: [-776.021 -776.021 -776.021], eps: 0.1})
Step:  101300, Reward: [-461.020 -461.020 -461.020] [146.440], Avg: [-492.803 -492.803 -492.803] (0.0100) ({r_i: None, r_t: [-869.118 -869.118 -869.118], eps: 0.01})
Step:  181100, Reward: [-423.703 -423.703 -423.703] [80.820], Avg: [-446.507 -446.507 -446.507] (0.0100) ({r_i: None, r_t: [-828.615 -828.615 -828.615], eps: 0.01})
Step:   69000, Reward: [-438.491 -438.491 -438.491] [71.957], Avg: [-426.619 -426.619 -426.619] (0.1000) ({r_i: None, r_t: [-811.003 -811.003 -811.003], eps: 0.1})
Step:  101400, Reward: [-420.635 -420.635 -420.635] [100.436], Avg: [-492.732 -492.732 -492.732] (0.0100) ({r_i: None, r_t: [-871.524 -871.524 -871.524], eps: 0.01})
Step:  181200, Reward: [-383.098 -383.098 -383.098] [81.831], Avg: [-446.472 -446.472 -446.472] (0.0100) ({r_i: None, r_t: [-836.056 -836.056 -836.056], eps: 0.01})
Step:   69100, Reward: [-408.380 -408.380 -408.380] [55.555], Avg: [-426.593 -426.593 -426.593] (0.1000) ({r_i: None, r_t: [-823.826 -823.826 -823.826], eps: 0.1})
Step:  181300, Reward: [-421.560 -421.560 -421.560] [119.310], Avg: [-446.459 -446.459 -446.459] (0.0100) ({r_i: None, r_t: [-853.387 -853.387 -853.387], eps: 0.01})
Step:  101500, Reward: [-421.680 -421.680 -421.680] [96.062], Avg: [-492.662 -492.662 -492.662] (0.0100) ({r_i: None, r_t: [-883.360 -883.360 -883.360], eps: 0.01})
Step:  181400, Reward: [-398.193 -398.193 -398.193] [60.157], Avg: [-446.432 -446.432 -446.432] (0.0100) ({r_i: None, r_t: [-859.816 -859.816 -859.816], eps: 0.01})
Step:   69200, Reward: [-396.261 -396.261 -396.261] [73.215], Avg: [-426.549 -426.549 -426.549] (0.1000) ({r_i: None, r_t: [-811.169 -811.169 -811.169], eps: 0.1})
Step:  101600, Reward: [-436.099 -436.099 -436.099] [121.279], Avg: [-492.606 -492.606 -492.606] (0.0100) ({r_i: None, r_t: [-835.949 -835.949 -835.949], eps: 0.01})
Step:  181500, Reward: [-409.097 -409.097 -409.097] [76.599], Avg: [-446.412 -446.412 -446.412] (0.0100) ({r_i: None, r_t: [-826.938 -826.938 -826.938], eps: 0.01})
Step:   69300, Reward: [-394.729 -394.729 -394.729] [82.949], Avg: [-426.503 -426.503 -426.503] (0.1000) ({r_i: None, r_t: [-883.186 -883.186 -883.186], eps: 0.1})
Step:  101700, Reward: [-467.676 -467.676 -467.676] [87.319], Avg: [-492.582 -492.582 -492.582] (0.0100) ({r_i: None, r_t: [-844.828 -844.828 -844.828], eps: 0.01})
Step:  181600, Reward: [-454.502 -454.502 -454.502] [96.407], Avg: [-446.416 -446.416 -446.416] (0.0100) ({r_i: None, r_t: [-829.044 -829.044 -829.044], eps: 0.01})
Step:   69400, Reward: [-394.118 -394.118 -394.118] [103.299], Avg: [-426.457 -426.457 -426.457] (0.1000) ({r_i: None, r_t: [-886.925 -886.925 -886.925], eps: 0.1})
Step:  181700, Reward: [-460.239 -460.239 -460.239] [80.418], Avg: [-446.424 -446.424 -446.424] (0.0100) ({r_i: None, r_t: [-843.940 -843.940 -843.940], eps: 0.01})
Step:  101800, Reward: [-460.895 -460.895 -460.895] [69.117], Avg: [-492.551 -492.551 -492.551] (0.0100) ({r_i: None, r_t: [-885.744 -885.744 -885.744], eps: 0.01})
Step:  181800, Reward: [-489.865 -489.865 -489.865] [129.406], Avg: [-446.448 -446.448 -446.448] (0.0100) ({r_i: None, r_t: [-810.207 -810.207 -810.207], eps: 0.01})
Step:   69500, Reward: [-398.134 -398.134 -398.134] [61.247], Avg: [-426.416 -426.416 -426.416] (0.1000) ({r_i: None, r_t: [-808.766 -808.766 -808.766], eps: 0.1})
Step:  101900, Reward: [-472.565 -472.565 -472.565] [109.605], Avg: [-492.531 -492.531 -492.531] (0.0100) ({r_i: None, r_t: [-889.697 -889.697 -889.697], eps: 0.01})
Step:  181900, Reward: [-449.035 -449.035 -449.035] [78.383], Avg: [-446.449 -446.449 -446.449] (0.0100) ({r_i: None, r_t: [-872.901 -872.901 -872.901], eps: 0.01})
Step:   69600, Reward: [-413.003 -413.003 -413.003] [63.932], Avg: [-426.397 -426.397 -426.397] (0.1000) ({r_i: None, r_t: [-815.320 -815.320 -815.320], eps: 0.1})
Step:  102000, Reward: [-443.187 -443.187 -443.187] [103.232], Avg: [-492.483 -492.483 -492.483] (0.0100) ({r_i: None, r_t: [-908.808 -908.808 -908.808], eps: 0.01})
Step:  182000, Reward: [-478.166 -478.166 -478.166] [95.509], Avg: [-446.466 -446.466 -446.466] (0.0100) ({r_i: None, r_t: [-857.345 -857.345 -857.345], eps: 0.01})
Step:   69700, Reward: [-434.088 -434.088 -434.088] [80.549], Avg: [-426.408 -426.408 -426.408] (0.1000) ({r_i: None, r_t: [-812.466 -812.466 -812.466], eps: 0.1})
Step:  182100, Reward: [-417.222 -417.222 -417.222] [52.953], Avg: [-446.450 -446.450 -446.450] (0.0100) ({r_i: None, r_t: [-778.271 -778.271 -778.271], eps: 0.01})
Step:  102100, Reward: [-444.385 -444.385 -444.385] [73.626], Avg: [-492.436 -492.436 -492.436] (0.0100) ({r_i: None, r_t: [-961.303 -961.303 -961.303], eps: 0.01})
Step:  182200, Reward: [-432.888 -432.888 -432.888] [69.778], Avg: [-446.443 -446.443 -446.443] (0.0100) ({r_i: None, r_t: [-846.152 -846.152 -846.152], eps: 0.01})
Step:   69800, Reward: [-410.696 -410.696 -410.696] [85.657], Avg: [-426.385 -426.385 -426.385] (0.1000) ({r_i: None, r_t: [-797.117 -797.117 -797.117], eps: 0.1})
Step:  102200, Reward: [-451.207 -451.207 -451.207] [101.548], Avg: [-492.395 -492.395 -492.395] (0.0100) ({r_i: None, r_t: [-930.642 -930.642 -930.642], eps: 0.01})
Step:  182300, Reward: [-402.914 -402.914 -402.914] [70.752], Avg: [-446.419 -446.419 -446.419] (0.0100) ({r_i: None, r_t: [-863.714 -863.714 -863.714], eps: 0.01})
Step:   69900, Reward: [-366.667 -366.667 -366.667] [48.220], Avg: [-426.300 -426.300 -426.300] (0.1000) ({r_i: None, r_t: [-815.527 -815.527 -815.527], eps: 0.1})
Step:  102300, Reward: [-436.405 -436.405 -436.405] [74.519], Avg: [-492.341 -492.341 -492.341] (0.0100) ({r_i: None, r_t: [-896.644 -896.644 -896.644], eps: 0.01})
Step:  182400, Reward: [-410.075 -410.075 -410.075] [91.908], Avg: [-446.399 -446.399 -446.399] (0.0100) ({r_i: None, r_t: [-805.269 -805.269 -805.269], eps: 0.01})
Step:   70000, Reward: [-389.332 -389.332 -389.332] [59.863], Avg: [-426.247 -426.247 -426.247] (0.1000) ({r_i: None, r_t: [-775.948 -775.948 -775.948], eps: 0.1})
Step:  182500, Reward: [-412.249 -412.249 -412.249] [68.922], Avg: [-446.380 -446.380 -446.380] (0.0100) ({r_i: None, r_t: [-816.569 -816.569 -816.569], eps: 0.01})
Step:  102400, Reward: [-434.642 -434.642 -434.642] [92.488], Avg: [-492.284 -492.284 -492.284] (0.0100) ({r_i: None, r_t: [-855.386 -855.386 -855.386], eps: 0.01})
Step:  182600, Reward: [-406.681 -406.681 -406.681] [75.591], Avg: [-446.359 -446.359 -446.359] (0.0100) ({r_i: None, r_t: [-768.666 -768.666 -768.666], eps: 0.01})
Step:   70100, Reward: [-409.998 -409.998 -409.998] [65.237], Avg: [-426.224 -426.224 -426.224] (0.1000) ({r_i: None, r_t: [-874.835 -874.835 -874.835], eps: 0.1})
Step:  102500, Reward: [-410.633 -410.633 -410.633] [78.082], Avg: [-492.205 -492.205 -492.205] (0.0100) ({r_i: None, r_t: [-916.064 -916.064 -916.064], eps: 0.01})
Step:  182700, Reward: [-450.899 -450.899 -450.899] [75.554], Avg: [-446.361 -446.361 -446.361] (0.0100) ({r_i: None, r_t: [-808.211 -808.211 -808.211], eps: 0.01})
Step:   70200, Reward: [-428.130 -428.130 -428.130] [58.454], Avg: [-426.227 -426.227 -426.227] (0.1000) ({r_i: None, r_t: [-851.545 -851.545 -851.545], eps: 0.1})
Step:  102600, Reward: [-489.058 -489.058 -489.058] [87.823], Avg: [-492.202 -492.202 -492.202] (0.0100) ({r_i: None, r_t: [-958.672 -958.672 -958.672], eps: 0.01})
Step:  182800, Reward: [-380.456 -380.456 -380.456] [59.297], Avg: [-446.325 -446.325 -446.325] (0.0100) ({r_i: None, r_t: [-793.613 -793.613 -793.613], eps: 0.01})
Step:   70300, Reward: [-442.905 -442.905 -442.905] [88.207], Avg: [-426.250 -426.250 -426.250] (0.1000) ({r_i: None, r_t: [-792.187 -792.187 -792.187], eps: 0.1})
Step:  182900, Reward: [-382.489 -382.489 -382.489] [83.673], Avg: [-446.290 -446.290 -446.290] (0.0100) ({r_i: None, r_t: [-913.029 -913.029 -913.029], eps: 0.01})
Step:  102700, Reward: [-469.223 -469.223 -469.223] [128.969], Avg: [-492.179 -492.179 -492.179] (0.0100) ({r_i: None, r_t: [-1020.472 -1020.472 -1020.472], eps: 0.01})
Step:  183000, Reward: [-433.936 -433.936 -433.936] [74.619], Avg: [-446.283 -446.283 -446.283] (0.0100) ({r_i: None, r_t: [-828.408 -828.408 -828.408], eps: 0.01})
Step:   70400, Reward: [-416.179 -416.179 -416.179] [67.367], Avg: [-426.236 -426.236 -426.236] (0.1000) ({r_i: None, r_t: [-833.898 -833.898 -833.898], eps: 0.1})
Step:  102800, Reward: [-462.707 -462.707 -462.707] [92.485], Avg: [-492.151 -492.151 -492.151] (0.0100) ({r_i: None, r_t: [-911.582 -911.582 -911.582], eps: 0.01})
Step:  183100, Reward: [-413.961 -413.961 -413.961] [49.278], Avg: [-446.266 -446.266 -446.266] (0.0100) ({r_i: None, r_t: [-877.230 -877.230 -877.230], eps: 0.01})
Step:   70500, Reward: [-441.699 -441.699 -441.699] [58.840], Avg: [-426.258 -426.258 -426.258] (0.1000) ({r_i: None, r_t: [-802.486 -802.486 -802.486], eps: 0.1})
Step:  102900, Reward: [-477.664 -477.664 -477.664] [134.234], Avg: [-492.137 -492.137 -492.137] (0.0100) ({r_i: None, r_t: [-1008.842 -1008.842 -1008.842], eps: 0.01})
Step:  183200, Reward: [-416.241 -416.241 -416.241] [61.848], Avg: [-446.249 -446.249 -446.249] (0.0100) ({r_i: None, r_t: [-851.659 -851.659 -851.659], eps: 0.01})
Step:   70600, Reward: [-398.142 -398.142 -398.142] [69.520], Avg: [-426.218 -426.218 -426.218] (0.1000) ({r_i: None, r_t: [-812.174 -812.174 -812.174], eps: 0.1})
Step:  183300, Reward: [-363.263 -363.263 -363.263] [88.545], Avg: [-446.204 -446.204 -446.204] (0.0100) ({r_i: None, r_t: [-844.281 -844.281 -844.281], eps: 0.01})
Step:  103000, Reward: [-428.741 -428.741 -428.741] [63.792], Avg: [-492.075 -492.075 -492.075] (0.0100) ({r_i: None, r_t: [-1049.100 -1049.100 -1049.100], eps: 0.01})
Step:  183400, Reward: [-419.076 -419.076 -419.076] [114.594], Avg: [-446.189 -446.189 -446.189] (0.0100) ({r_i: None, r_t: [-831.504 -831.504 -831.504], eps: 0.01})
Step:   70700, Reward: [-402.664 -402.664 -402.664] [86.567], Avg: [-426.185 -426.185 -426.185] (0.1000) ({r_i: None, r_t: [-837.972 -837.972 -837.972], eps: 0.1})
Step:  103100, Reward: [-487.405 -487.405 -487.405] [169.618], Avg: [-492.071 -492.071 -492.071] (0.0100) ({r_i: None, r_t: [-1003.079 -1003.079 -1003.079], eps: 0.01})
Step:  183500, Reward: [-441.158 -441.158 -441.158] [112.630], Avg: [-446.187 -446.187 -446.187] (0.0100) ({r_i: None, r_t: [-794.498 -794.498 -794.498], eps: 0.01})
Step:   70800, Reward: [-420.801 -420.801 -420.801] [74.152], Avg: [-426.177 -426.177 -426.177] (0.1000) ({r_i: None, r_t: [-808.469 -808.469 -808.469], eps: 0.1})
Step:  103200, Reward: [-517.667 -517.667 -517.667] [173.647], Avg: [-492.096 -492.096 -492.096] (0.0100) ({r_i: None, r_t: [-966.343 -966.343 -966.343], eps: 0.01})
Step:  183600, Reward: [-419.836 -419.836 -419.836] [61.244], Avg: [-446.172 -446.172 -446.172] (0.0100) ({r_i: None, r_t: [-799.483 -799.483 -799.483], eps: 0.01})
Step:   70900, Reward: [-370.335 -370.335 -370.335] [66.050], Avg: [-426.099 -426.099 -426.099] (0.1000) ({r_i: None, r_t: [-828.886 -828.886 -828.886], eps: 0.1})
Step:  183700, Reward: [-423.150 -423.150 -423.150] [75.723], Avg: [-446.160 -446.160 -446.160] (0.0100) ({r_i: None, r_t: [-830.682 -830.682 -830.682], eps: 0.01})
Step:  103300, Reward: [-541.618 -541.618 -541.618] [199.661], Avg: [-492.143 -492.143 -492.143] (0.0100) ({r_i: None, r_t: [-944.817 -944.817 -944.817], eps: 0.01})
Step:  183800, Reward: [-404.773 -404.773 -404.773] [116.078], Avg: [-446.137 -446.137 -446.137] (0.0100) ({r_i: None, r_t: [-849.686 -849.686 -849.686], eps: 0.01})
Step:   71000, Reward: [-373.121 -373.121 -373.121] [86.127], Avg: [-426.024 -426.024 -426.024] (0.1000) ({r_i: None, r_t: [-772.288 -772.288 -772.288], eps: 0.1})
Step:  103400, Reward: [-501.067 -501.067 -501.067] [172.914], Avg: [-492.152 -492.152 -492.152] (0.0100) ({r_i: None, r_t: [-963.277 -963.277 -963.277], eps: 0.01})
Step:  183900, Reward: [-411.411 -411.411 -411.411] [58.904], Avg: [-446.118 -446.118 -446.118] (0.0100) ({r_i: None, r_t: [-863.264 -863.264 -863.264], eps: 0.01})
Step:   71100, Reward: [-393.372 -393.372 -393.372] [44.921], Avg: [-425.978 -425.978 -425.978] (0.1000) ({r_i: None, r_t: [-870.173 -870.173 -870.173], eps: 0.1})
Step:  103500, Reward: [-455.258 -455.258 -455.258] [93.694], Avg: [-492.116 -492.116 -492.116] (0.0100) ({r_i: None, r_t: [-987.396 -987.396 -987.396], eps: 0.01})
Step:  184000, Reward: [-436.312 -436.312 -436.312] [126.718], Avg: [-446.113 -446.113 -446.113] (0.0100) ({r_i: None, r_t: [-881.119 -881.119 -881.119], eps: 0.01})
Step:   71200, Reward: [-395.180 -395.180 -395.180] [50.337], Avg: [-425.935 -425.935 -425.935] (0.1000) ({r_i: None, r_t: [-808.358 -808.358 -808.358], eps: 0.1})
Step:  184100, Reward: [-419.815 -419.815 -419.815] [89.265], Avg: [-446.099 -446.099 -446.099] (0.0100) ({r_i: None, r_t: [-799.775 -799.775 -799.775], eps: 0.01})
Step:  103600, Reward: [-473.672 -473.672 -473.672] [112.946], Avg: [-492.099 -492.099 -492.099] (0.0100) ({r_i: None, r_t: [-1000.271 -1000.271 -1000.271], eps: 0.01})
Step:  184200, Reward: [-417.795 -417.795 -417.795] [69.610], Avg: [-446.083 -446.083 -446.083] (0.0100) ({r_i: None, r_t: [-816.833 -816.833 -816.833], eps: 0.01})
Step:   71300, Reward: [-410.331 -410.331 -410.331] [79.221], Avg: [-425.913 -425.913 -425.913] (0.1000) ({r_i: None, r_t: [-804.281 -804.281 -804.281], eps: 0.1})
Step:  103700, Reward: [-468.747 -468.747 -468.747] [150.128], Avg: [-492.076 -492.076 -492.076] (0.0100) ({r_i: None, r_t: [-1040.222 -1040.222 -1040.222], eps: 0.01})
Step:  184300, Reward: [-378.814 -378.814 -378.814] [68.124], Avg: [-446.047 -446.047 -446.047] (0.0100) ({r_i: None, r_t: [-838.883 -838.883 -838.883], eps: 0.01})
Step:   71400, Reward: [-432.392 -432.392 -432.392] [88.367], Avg: [-425.922 -425.922 -425.922] (0.1000) ({r_i: None, r_t: [-801.839 -801.839 -801.839], eps: 0.1})
Step:  103800, Reward: [-499.395 -499.395 -499.395] [143.667], Avg: [-492.083 -492.083 -492.083] (0.0100) ({r_i: None, r_t: [-994.635 -994.635 -994.635], eps: 0.01})
Step:  184400, Reward: [-396.865 -396.865 -396.865] [56.229], Avg: [-446.020 -446.020 -446.020] (0.0100) ({r_i: None, r_t: [-892.419 -892.419 -892.419], eps: 0.01})
Step:   71500, Reward: [-377.948 -377.948 -377.948] [45.651], Avg: [-425.855 -425.855 -425.855] (0.1000) ({r_i: None, r_t: [-846.524 -846.524 -846.524], eps: 0.1})
Step:  184500, Reward: [-425.904 -425.904 -425.904] [97.477], Avg: [-446.009 -446.009 -446.009] (0.0100) ({r_i: None, r_t: [-854.039 -854.039 -854.039], eps: 0.01})
Step:  103900, Reward: [-465.575 -465.575 -465.575] [154.812], Avg: [-492.058 -492.058 -492.058] (0.0100) ({r_i: None, r_t: [-1152.209 -1152.209 -1152.209], eps: 0.01})
Step:  184600, Reward: [-407.226 -407.226 -407.226] [82.042], Avg: [-445.988 -445.988 -445.988] (0.0100) ({r_i: None, r_t: [-864.108 -864.108 -864.108], eps: 0.01})
Step:   71600, Reward: [-389.535 -389.535 -389.535] [49.315], Avg: [-425.805 -425.805 -425.805] (0.1000) ({r_i: None, r_t: [-818.557 -818.557 -818.557], eps: 0.1})
Step:  104000, Reward: [-504.948 -504.948 -504.948] [114.285], Avg: [-492.070 -492.070 -492.070] (0.0100) ({r_i: None, r_t: [-1072.469 -1072.469 -1072.469], eps: 0.01})
Step:  184700, Reward: [-408.957 -408.957 -408.957] [73.153], Avg: [-445.968 -445.968 -445.968] (0.0100) ({r_i: None, r_t: [-836.387 -836.387 -836.387], eps: 0.01})
Step:   71700, Reward: [-428.761 -428.761 -428.761] [55.598], Avg: [-425.809 -425.809 -425.809] (0.1000) ({r_i: None, r_t: [-828.460 -828.460 -828.460], eps: 0.1})
Step:  104100, Reward: [-551.670 -551.670 -551.670] [141.721], Avg: [-492.127 -492.127 -492.127] (0.0100) ({r_i: None, r_t: [-1131.303 -1131.303 -1131.303], eps: 0.01})
Step:  184800, Reward: [-415.702 -415.702 -415.702] [96.519], Avg: [-445.952 -445.952 -445.952] (0.0100) ({r_i: None, r_t: [-824.415 -824.415 -824.415], eps: 0.01})
Step:  184900, Reward: [-437.516 -437.516 -437.516] [81.066], Avg: [-445.947 -445.947 -445.947] (0.0100) ({r_i: None, r_t: [-787.625 -787.625 -787.625], eps: 0.01})
Step:   71800, Reward: [-399.723 -399.723 -399.723] [75.245], Avg: [-425.773 -425.773 -425.773] (0.1000) ({r_i: None, r_t: [-775.786 -775.786 -775.786], eps: 0.1})
Step:  104200, Reward: [-527.216 -527.216 -527.216] [180.286], Avg: [-492.161 -492.161 -492.161] (0.0100) ({r_i: None, r_t: [-1135.691 -1135.691 -1135.691], eps: 0.01})
Step:  185000, Reward: [-416.602 -416.602 -416.602] [85.762], Avg: [-445.932 -445.932 -445.932] (0.0100) ({r_i: None, r_t: [-791.282 -791.282 -791.282], eps: 0.01})
Step:   71900, Reward: [-370.839 -370.839 -370.839] [34.524], Avg: [-425.696 -425.696 -425.696] (0.1000) ({r_i: None, r_t: [-823.957 -823.957 -823.957], eps: 0.1})
Step:  104300, Reward: [-657.833 -657.833 -657.833] [289.368], Avg: [-492.320 -492.320 -492.320] (0.0100) ({r_i: None, r_t: [-1128.553 -1128.553 -1128.553], eps: 0.01})
Step:  185100, Reward: [-436.826 -436.826 -436.826] [98.443], Avg: [-445.927 -445.927 -445.927] (0.0100) ({r_i: None, r_t: [-820.662 -820.662 -820.662], eps: 0.01})
Step:   72000, Reward: [-395.214 -395.214 -395.214] [72.199], Avg: [-425.654 -425.654 -425.654] (0.1000) ({r_i: None, r_t: [-784.684 -784.684 -784.684], eps: 0.1})
Step:  185200, Reward: [-375.590 -375.590 -375.590] [50.019], Avg: [-445.889 -445.889 -445.889] (0.0100) ({r_i: None, r_t: [-826.283 -826.283 -826.283], eps: 0.01})
Step:  104400, Reward: [-483.988 -483.988 -483.988] [147.897], Avg: [-492.312 -492.312 -492.312] (0.0100) ({r_i: None, r_t: [-1094.448 -1094.448 -1094.448], eps: 0.01})
Step:  185300, Reward: [-409.901 -409.901 -409.901] [93.141], Avg: [-445.869 -445.869 -445.869] (0.0100) ({r_i: None, r_t: [-800.931 -800.931 -800.931], eps: 0.01})
Step:   72100, Reward: [-402.304 -402.304 -402.304] [72.716], Avg: [-425.622 -425.622 -425.622] (0.1000) ({r_i: None, r_t: [-810.978 -810.978 -810.978], eps: 0.1})
Step:  104500, Reward: [-445.237 -445.237 -445.237] [127.896], Avg: [-492.267 -492.267 -492.267] (0.0100) ({r_i: None, r_t: [-949.270 -949.270 -949.270], eps: 0.01})
Step:  185400, Reward: [-373.480 -373.480 -373.480] [65.086], Avg: [-445.830 -445.830 -445.830] (0.0100) ({r_i: None, r_t: [-845.356 -845.356 -845.356], eps: 0.01})
Step:   72200, Reward: [-435.554 -435.554 -435.554] [119.170], Avg: [-425.635 -425.635 -425.635] (0.1000) ({r_i: None, r_t: [-824.332 -824.332 -824.332], eps: 0.1})
Step:  104600, Reward: [-501.583 -501.583 -501.583] [219.445], Avg: [-492.276 -492.276 -492.276] (0.0100) ({r_i: None, r_t: [-957.673 -957.673 -957.673], eps: 0.01})
Step:  185500, Reward: [-404.213 -404.213 -404.213] [87.144], Avg: [-445.808 -445.808 -445.808] (0.0100) ({r_i: None, r_t: [-783.927 -783.927 -783.927], eps: 0.01})
Step:   72300, Reward: [-404.683 -404.683 -404.683] [91.450], Avg: [-425.607 -425.607 -425.607] (0.1000) ({r_i: None, r_t: [-787.070 -787.070 -787.070], eps: 0.1})
Step:  185600, Reward: [-423.301 -423.301 -423.301] [61.258], Avg: [-445.796 -445.796 -445.796] (0.0100) ({r_i: None, r_t: [-870.647 -870.647 -870.647], eps: 0.01})
Step:  104700, Reward: [-486.667 -486.667 -486.667] [122.194], Avg: [-492.270 -492.270 -492.270] (0.0100) ({r_i: None, r_t: [-943.726 -943.726 -943.726], eps: 0.01})
Step:  185700, Reward: [-439.458 -439.458 -439.458] [77.775], Avg: [-445.792 -445.792 -445.792] (0.0100) ({r_i: None, r_t: [-817.667 -817.667 -817.667], eps: 0.01})
Step:   72400, Reward: [-409.217 -409.217 -409.217] [77.911], Avg: [-425.584 -425.584 -425.584] (0.1000) ({r_i: None, r_t: [-814.837 -814.837 -814.837], eps: 0.1})
Step:  104800, Reward: [-509.105 -509.105 -509.105] [166.160], Avg: [-492.286 -492.286 -492.286] (0.0100) ({r_i: None, r_t: [-1102.149 -1102.149 -1102.149], eps: 0.01})
Step:  185800, Reward: [-377.460 -377.460 -377.460] [72.651], Avg: [-445.756 -445.756 -445.756] (0.0100) ({r_i: None, r_t: [-805.707 -805.707 -805.707], eps: 0.01})
Step:   72500, Reward: [-394.941 -394.941 -394.941] [71.396], Avg: [-425.542 -425.542 -425.542] (0.1000) ({r_i: None, r_t: [-816.197 -816.197 -816.197], eps: 0.1})
Step:  104900, Reward: [-491.333 -491.333 -491.333] [88.020], Avg: [-492.285 -492.285 -492.285] (0.0100) ({r_i: None, r_t: [-1101.997 -1101.997 -1101.997], eps: 0.01})
Step:  185900, Reward: [-405.425 -405.425 -405.425] [57.422], Avg: [-445.734 -445.734 -445.734] (0.0100) ({r_i: None, r_t: [-858.699 -858.699 -858.699], eps: 0.01})
Step:   72600, Reward: [-406.795 -406.795 -406.795] [50.742], Avg: [-425.516 -425.516 -425.516] (0.1000) ({r_i: None, r_t: [-811.284 -811.284 -811.284], eps: 0.1})
Step:  105000, Reward: [-454.293 -454.293 -454.293] [167.747], Avg: [-492.249 -492.249 -492.249] (0.0100) ({r_i: None, r_t: [-1063.409 -1063.409 -1063.409], eps: 0.01})
Step:  186000, Reward: [-442.025 -442.025 -442.025] [95.892], Avg: [-445.732 -445.732 -445.732] (0.0100) ({r_i: None, r_t: [-802.999 -802.999 -802.999], eps: 0.01})
Step:   72700, Reward: [-398.912 -398.912 -398.912] [63.069], Avg: [-425.479 -425.479 -425.479] (0.1000) ({r_i: None, r_t: [-775.260 -775.260 -775.260], eps: 0.1})
Step:  186100, Reward: [-424.585 -424.585 -424.585] [118.785], Avg: [-445.721 -445.721 -445.721] (0.0100) ({r_i: None, r_t: [-812.127 -812.127 -812.127], eps: 0.01})
Step:  105100, Reward: [-442.135 -442.135 -442.135] [105.930], Avg: [-492.202 -492.202 -492.202] (0.0100) ({r_i: None, r_t: [-931.221 -931.221 -931.221], eps: 0.01})
Step:  186200, Reward: [-431.298 -431.298 -431.298] [68.799], Avg: [-445.713 -445.713 -445.713] (0.0100) ({r_i: None, r_t: [-854.285 -854.285 -854.285], eps: 0.01})
Step:   72800, Reward: [-403.837 -403.837 -403.837] [72.957], Avg: [-425.450 -425.450 -425.450] (0.1000) ({r_i: None, r_t: [-828.963 -828.963 -828.963], eps: 0.1})
Step:  105200, Reward: [-428.965 -428.965 -428.965] [77.829], Avg: [-492.141 -492.141 -492.141] (0.0100) ({r_i: None, r_t: [-964.303 -964.303 -964.303], eps: 0.01})
Step:  186300, Reward: [-414.519 -414.519 -414.519] [66.969], Avg: [-445.696 -445.696 -445.696] (0.0100) ({r_i: None, r_t: [-853.961 -853.961 -853.961], eps: 0.01})
Step:   72900, Reward: [-454.029 -454.029 -454.029] [104.520], Avg: [-425.489 -425.489 -425.489] (0.1000) ({r_i: None, r_t: [-854.283 -854.283 -854.283], eps: 0.1})
Step:  105300, Reward: [-477.886 -477.886 -477.886] [181.511], Avg: [-492.128 -492.128 -492.128] (0.0100) ({r_i: None, r_t: [-1016.623 -1016.623 -1016.623], eps: 0.01})
Step:  186400, Reward: [-435.182 -435.182 -435.182] [90.163], Avg: [-445.690 -445.690 -445.690] (0.0100) ({r_i: None, r_t: [-904.356 -904.356 -904.356], eps: 0.01})
Step:  186500, Reward: [-465.403 -465.403 -465.403] [114.979], Avg: [-445.701 -445.701 -445.701] (0.0100) ({r_i: None, r_t: [-839.162 -839.162 -839.162], eps: 0.01})
Step:   73000, Reward: [-386.768 -386.768 -386.768] [62.907], Avg: [-425.436 -425.436 -425.436] (0.1000) ({r_i: None, r_t: [-821.320 -821.320 -821.320], eps: 0.1})
Step:  105400, Reward: [-424.298 -424.298 -424.298] [82.475], Avg: [-492.064 -492.064 -492.064] (0.0100) ({r_i: None, r_t: [-946.144 -946.144 -946.144], eps: 0.01})
Step:  186600, Reward: [-432.129 -432.129 -432.129] [94.525], Avg: [-445.694 -445.694 -445.694] (0.0100) ({r_i: None, r_t: [-809.360 -809.360 -809.360], eps: 0.01})
Step:   73100, Reward: [-379.768 -379.768 -379.768] [53.211], Avg: [-425.373 -425.373 -425.373] (0.1000) ({r_i: None, r_t: [-824.167 -824.167 -824.167], eps: 0.1})
Step:  105500, Reward: [-449.730 -449.730 -449.730] [85.957], Avg: [-492.024 -492.024 -492.024] (0.0100) ({r_i: None, r_t: [-891.347 -891.347 -891.347], eps: 0.01})
Step:  186700, Reward: [-435.238 -435.238 -435.238] [65.223], Avg: [-445.688 -445.688 -445.688] (0.0100) ({r_i: None, r_t: [-862.099 -862.099 -862.099], eps: 0.01})
Step:   73200, Reward: [-424.554 -424.554 -424.554] [55.046], Avg: [-425.372 -425.372 -425.372] (0.1000) ({r_i: None, r_t: [-807.455 -807.455 -807.455], eps: 0.1})
Step:  105600, Reward: [-436.182 -436.182 -436.182] [73.367], Avg: [-491.971 -491.971 -491.971] (0.0100) ({r_i: None, r_t: [-773.299 -773.299 -773.299], eps: 0.01})
Step:  186800, Reward: [-446.497 -446.497 -446.497] [100.033], Avg: [-445.689 -445.689 -445.689] (0.0100) ({r_i: None, r_t: [-864.672 -864.672 -864.672], eps: 0.01})
Step:  186900, Reward: [-413.146 -413.146 -413.146] [70.797], Avg: [-445.671 -445.671 -445.671] (0.0100) ({r_i: None, r_t: [-801.933 -801.933 -801.933], eps: 0.01})
Step:   73300, Reward: [-426.299 -426.299 -426.299] [113.617], Avg: [-425.374 -425.374 -425.374] (0.1000) ({r_i: None, r_t: [-833.593 -833.593 -833.593], eps: 0.1})
Step:  105700, Reward: [-444.534 -444.534 -444.534] [111.690], Avg: [-491.926 -491.926 -491.926] (0.0100) ({r_i: None, r_t: [-812.780 -812.780 -812.780], eps: 0.01})
Step:  187000, Reward: [-435.086 -435.086 -435.086] [76.668], Avg: [-445.666 -445.666 -445.666] (0.0100) ({r_i: None, r_t: [-823.003 -823.003 -823.003], eps: 0.01})
Step:   73400, Reward: [-402.100 -402.100 -402.100] [78.435], Avg: [-425.342 -425.342 -425.342] (0.1000) ({r_i: None, r_t: [-856.274 -856.274 -856.274], eps: 0.1})
Step:  105800, Reward: [-454.823 -454.823 -454.823] [107.215], Avg: [-491.891 -491.891 -491.891] (0.0100) ({r_i: None, r_t: [-955.323 -955.323 -955.323], eps: 0.01})
Step:  187100, Reward: [-401.696 -401.696 -401.696] [67.923], Avg: [-445.642 -445.642 -445.642] (0.0100) ({r_i: None, r_t: [-863.994 -863.994 -863.994], eps: 0.01})
Step:   73500, Reward: [-432.739 -432.739 -432.739] [95.761], Avg: [-425.352 -425.352 -425.352] (0.1000) ({r_i: None, r_t: [-820.107 -820.107 -820.107], eps: 0.1})
Step:  105900, Reward: [-440.434 -440.434 -440.434] [101.266], Avg: [-491.842 -491.842 -491.842] (0.0100) ({r_i: None, r_t: [-854.247 -854.247 -854.247], eps: 0.01})
Step:  187200, Reward: [-445.631 -445.631 -445.631] [104.196], Avg: [-445.642 -445.642 -445.642] (0.0100) ({r_i: None, r_t: [-837.022 -837.022 -837.022], eps: 0.01})
Step:  187300, Reward: [-414.293 -414.293 -414.293] [73.203], Avg: [-445.625 -445.625 -445.625] (0.0100) ({r_i: None, r_t: [-870.810 -870.810 -870.810], eps: 0.01})
Step:   73600, Reward: [-394.573 -394.573 -394.573] [78.395], Avg: [-425.310 -425.310 -425.310] (0.1000) ({r_i: None, r_t: [-779.118 -779.118 -779.118], eps: 0.1})
Step:  106000, Reward: [-482.474 -482.474 -482.474] [122.351], Avg: [-491.834 -491.834 -491.834] (0.0100) ({r_i: None, r_t: [-885.960 -885.960 -885.960], eps: 0.01})
Step:  187400, Reward: [-425.053 -425.053 -425.053] [82.867], Avg: [-445.614 -445.614 -445.614] (0.0100) ({r_i: None, r_t: [-789.170 -789.170 -789.170], eps: 0.01})
Step:   73700, Reward: [-395.538 -395.538 -395.538] [66.487], Avg: [-425.270 -425.270 -425.270] (0.1000) ({r_i: None, r_t: [-808.182 -808.182 -808.182], eps: 0.1})
Step:  106100, Reward: [-451.322 -451.322 -451.322] [80.029], Avg: [-491.795 -491.795 -491.795] (0.0100) ({r_i: None, r_t: [-941.283 -941.283 -941.283], eps: 0.01})
Step:  187500, Reward: [-425.377 -425.377 -425.377] [74.642], Avg: [-445.604 -445.604 -445.604] (0.0100) ({r_i: None, r_t: [-818.326 -818.326 -818.326], eps: 0.01})
Step:   73800, Reward: [-407.307 -407.307 -407.307] [57.853], Avg: [-425.246 -425.246 -425.246] (0.1000) ({r_i: None, r_t: [-863.974 -863.974 -863.974], eps: 0.1})
Step:  106200, Reward: [-421.599 -421.599 -421.599] [71.778], Avg: [-491.729 -491.729 -491.729] (0.0100) ({r_i: None, r_t: [-882.715 -882.715 -882.715], eps: 0.01})
Step:  187600, Reward: [-463.593 -463.593 -463.593] [78.009], Avg: [-445.613 -445.613 -445.613] (0.0100) ({r_i: None, r_t: [-842.333 -842.333 -842.333], eps: 0.01})
Step:  187700, Reward: [-417.572 -417.572 -417.572] [62.945], Avg: [-445.598 -445.598 -445.598] (0.0100) ({r_i: None, r_t: [-793.624 -793.624 -793.624], eps: 0.01})
Step:   73900, Reward: [-412.051 -412.051 -412.051] [81.705], Avg: [-425.228 -425.228 -425.228] (0.1000) ({r_i: None, r_t: [-845.681 -845.681 -845.681], eps: 0.1})
Step:  106300, Reward: [-419.510 -419.510 -419.510] [77.433], Avg: [-491.661 -491.661 -491.661] (0.0100) ({r_i: None, r_t: [-814.991 -814.991 -814.991], eps: 0.01})
Step:  187800, Reward: [-383.872 -383.872 -383.872] [66.926], Avg: [-445.565 -445.565 -445.565] (0.0100) ({r_i: None, r_t: [-797.781 -797.781 -797.781], eps: 0.01})
Step:   74000, Reward: [-421.830 -421.830 -421.830] [61.755], Avg: [-425.223 -425.223 -425.223] (0.1000) ({r_i: None, r_t: [-879.624 -879.624 -879.624], eps: 0.1})
Step:  106400, Reward: [-422.827 -422.827 -422.827] [90.582], Avg: [-491.597 -491.597 -491.597] (0.0100) ({r_i: None, r_t: [-879.085 -879.085 -879.085], eps: 0.01})
Step:  187900, Reward: [-411.803 -411.803 -411.803] [113.111], Avg: [-445.547 -445.547 -445.547] (0.0100) ({r_i: None, r_t: [-851.386 -851.386 -851.386], eps: 0.01})
Step:   74100, Reward: [-449.381 -449.381 -449.381] [101.222], Avg: [-425.256 -425.256 -425.256] (0.1000) ({r_i: None, r_t: [-841.152 -841.152 -841.152], eps: 0.1})
Step:  106500, Reward: [-444.300 -444.300 -444.300] [108.264], Avg: [-491.552 -491.552 -491.552] (0.0100) ({r_i: None, r_t: [-831.854 -831.854 -831.854], eps: 0.01})
Step:  188000, Reward: [-419.398 -419.398 -419.398] [115.147], Avg: [-445.533 -445.533 -445.533] (0.0100) ({r_i: None, r_t: [-873.566 -873.566 -873.566], eps: 0.01})
Step:  188100, Reward: [-431.769 -431.769 -431.769] [91.244], Avg: [-445.526 -445.526 -445.526] (0.0100) ({r_i: None, r_t: [-869.826 -869.826 -869.826], eps: 0.01})
Step:   74200, Reward: [-421.271 -421.271 -421.271] [90.632], Avg: [-425.250 -425.250 -425.250] (0.1000) ({r_i: None, r_t: [-826.810 -826.810 -826.810], eps: 0.1})
Step:  106600, Reward: [-392.953 -392.953 -392.953] [86.160], Avg: [-491.460 -491.460 -491.460] (0.0100) ({r_i: None, r_t: [-812.960 -812.960 -812.960], eps: 0.01})
Step:  188200, Reward: [-379.503 -379.503 -379.503] [66.760], Avg: [-445.491 -445.491 -445.491] (0.0100) ({r_i: None, r_t: [-867.996 -867.996 -867.996], eps: 0.01})
Step:   74300, Reward: [-421.311 -421.311 -421.311] [85.628], Avg: [-425.245 -425.245 -425.245] (0.1000) ({r_i: None, r_t: [-818.669 -818.669 -818.669], eps: 0.1})
Step:  106700, Reward: [-462.830 -462.830 -462.830] [97.317], Avg: [-491.433 -491.433 -491.433] (0.0100) ({r_i: None, r_t: [-840.840 -840.840 -840.840], eps: 0.01})
Step:  188300, Reward: [-466.414 -466.414 -466.414] [84.199], Avg: [-445.502 -445.502 -445.502] (0.0100) ({r_i: None, r_t: [-901.572 -901.572 -901.572], eps: 0.01})
Step:   74400, Reward: [-404.355 -404.355 -404.355] [77.069], Avg: [-425.217 -425.217 -425.217] (0.1000) ({r_i: None, r_t: [-849.590 -849.590 -849.590], eps: 0.1})
Step:  188400, Reward: [-409.873 -409.873 -409.873] [107.602], Avg: [-445.483 -445.483 -445.483] (0.0100) ({r_i: None, r_t: [-784.734 -784.734 -784.734], eps: 0.01})
Step:  106800, Reward: [-390.588 -390.588 -390.588] [71.088], Avg: [-491.339 -491.339 -491.339] (0.0100) ({r_i: None, r_t: [-819.740 -819.740 -819.740], eps: 0.01})
Step:  188500, Reward: [-423.433 -423.433 -423.433] [72.845], Avg: [-445.472 -445.472 -445.472] (0.0100) ({r_i: None, r_t: [-768.893 -768.893 -768.893], eps: 0.01})
Step:   74500, Reward: [-382.488 -382.488 -382.488] [52.363], Avg: [-425.160 -425.160 -425.160] (0.1000) ({r_i: None, r_t: [-817.575 -817.575 -817.575], eps: 0.1})
Step:  106900, Reward: [-389.430 -389.430 -389.430] [59.078], Avg: [-491.244 -491.244 -491.244] (0.0100) ({r_i: None, r_t: [-803.113 -803.113 -803.113], eps: 0.01})
Step:  188600, Reward: [-442.127 -442.127 -442.127] [91.851], Avg: [-445.470 -445.470 -445.470] (0.0100) ({r_i: None, r_t: [-870.780 -870.780 -870.780], eps: 0.01})
Step:   74600, Reward: [-426.615 -426.615 -426.615] [76.345], Avg: [-425.162 -425.162 -425.162] (0.1000) ({r_i: None, r_t: [-813.140 -813.140 -813.140], eps: 0.1})
Step:  107000, Reward: [-406.749 -406.749 -406.749] [61.071], Avg: [-491.165 -491.165 -491.165] (0.0100) ({r_i: None, r_t: [-831.117 -831.117 -831.117], eps: 0.01})
Step:  188700, Reward: [-437.072 -437.072 -437.072] [96.407], Avg: [-445.465 -445.465 -445.465] (0.0100) ({r_i: None, r_t: [-801.436 -801.436 -801.436], eps: 0.01})
Step:   74700, Reward: [-399.934 -399.934 -399.934] [87.294], Avg: [-425.128 -425.128 -425.128] (0.1000) ({r_i: None, r_t: [-826.120 -826.120 -826.120], eps: 0.1})
Step:  188800, Reward: [-394.935 -394.935 -394.935] [94.592], Avg: [-445.439 -445.439 -445.439] (0.0100) ({r_i: None, r_t: [-885.082 -885.082 -885.082], eps: 0.01})
Step:  107100, Reward: [-413.837 -413.837 -413.837] [46.274], Avg: [-491.093 -491.093 -491.093] (0.0100) ({r_i: None, r_t: [-850.349 -850.349 -850.349], eps: 0.01})
Step:  188900, Reward: [-382.067 -382.067 -382.067] [77.572], Avg: [-445.405 -445.405 -445.405] (0.0100) ({r_i: None, r_t: [-879.184 -879.184 -879.184], eps: 0.01})
Step:   74800, Reward: [-416.148 -416.148 -416.148] [109.415], Avg: [-425.116 -425.116 -425.116] (0.1000) ({r_i: None, r_t: [-847.817 -847.817 -847.817], eps: 0.1})
Step:  107200, Reward: [-418.193 -418.193 -418.193] [57.945], Avg: [-491.025 -491.025 -491.025] (0.0100) ({r_i: None, r_t: [-791.996 -791.996 -791.996], eps: 0.01})
Step:  189000, Reward: [-418.490 -418.490 -418.490] [103.452], Avg: [-445.391 -445.391 -445.391] (0.0100) ({r_i: None, r_t: [-886.397 -886.397 -886.397], eps: 0.01})
Step:   74900, Reward: [-406.901 -406.901 -406.901] [43.428], Avg: [-425.092 -425.092 -425.092] (0.1000) ({r_i: None, r_t: [-830.131 -830.131 -830.131], eps: 0.1})
Step:  107300, Reward: [-424.967 -424.967 -424.967] [71.140], Avg: [-490.963 -490.963 -490.963] (0.0100) ({r_i: None, r_t: [-783.861 -783.861 -783.861], eps: 0.01})
Step:  189100, Reward: [-433.080 -433.080 -433.080] [91.210], Avg: [-445.384 -445.384 -445.384] (0.0100) ({r_i: None, r_t: [-918.528 -918.528 -918.528], eps: 0.01})
Step:   75000, Reward: [-410.100 -410.100 -410.100] [80.174], Avg: [-425.072 -425.072 -425.072] (0.1000) ({r_i: None, r_t: [-845.926 -845.926 -845.926], eps: 0.1})
Step:  189200, Reward: [-402.557 -402.557 -402.557] [85.023], Avg: [-445.362 -445.362 -445.362] (0.0100) ({r_i: None, r_t: [-850.461 -850.461 -850.461], eps: 0.01})
Step:  107400, Reward: [-433.360 -433.360 -433.360] [99.283], Avg: [-490.910 -490.910 -490.910] (0.0100) ({r_i: None, r_t: [-769.938 -769.938 -769.938], eps: 0.01})
Step:  189300, Reward: [-426.871 -426.871 -426.871] [67.627], Avg: [-445.352 -445.352 -445.352] (0.0100) ({r_i: None, r_t: [-877.926 -877.926 -877.926], eps: 0.01})
Step:   75100, Reward: [-423.283 -423.283 -423.283] [73.949], Avg: [-425.069 -425.069 -425.069] (0.1000) ({r_i: None, r_t: [-845.482 -845.482 -845.482], eps: 0.1})
Step:  107500, Reward: [-422.655 -422.655 -422.655] [95.726], Avg: [-490.846 -490.846 -490.846] (0.0100) ({r_i: None, r_t: [-731.294 -731.294 -731.294], eps: 0.01})
Step:  189400, Reward: [-457.019 -457.019 -457.019] [84.047], Avg: [-445.358 -445.358 -445.358] (0.0100) ({r_i: None, r_t: [-840.447 -840.447 -840.447], eps: 0.01})
Step:   75200, Reward: [-428.225 -428.225 -428.225] [68.357], Avg: [-425.074 -425.074 -425.074] (0.1000) ({r_i: None, r_t: [-875.747 -875.747 -875.747], eps: 0.1})
Step:  107600, Reward: [-401.088 -401.088 -401.088] [67.534], Avg: [-490.763 -490.763 -490.763] (0.0100) ({r_i: None, r_t: [-786.593 -786.593 -786.593], eps: 0.01})
Step:  189500, Reward: [-402.110 -402.110 -402.110] [69.810], Avg: [-445.335 -445.335 -445.335] (0.0100) ({r_i: None, r_t: [-858.390 -858.390 -858.390], eps: 0.01})
Step:   75300, Reward: [-408.330 -408.330 -408.330] [62.751], Avg: [-425.051 -425.051 -425.051] (0.1000) ({r_i: None, r_t: [-808.566 -808.566 -808.566], eps: 0.1})
Step:  189600, Reward: [-440.833 -440.833 -440.833] [93.134], Avg: [-445.333 -445.333 -445.333] (0.0100) ({r_i: None, r_t: [-942.862 -942.862 -942.862], eps: 0.01})
Step:  107700, Reward: [-377.181 -377.181 -377.181] [52.288], Avg: [-490.657 -490.657 -490.657] (0.0100) ({r_i: None, r_t: [-785.974 -785.974 -785.974], eps: 0.01})
Step:  189700, Reward: [-411.025 -411.025 -411.025] [75.790], Avg: [-445.315 -445.315 -445.315] (0.0100) ({r_i: None, r_t: [-852.729 -852.729 -852.729], eps: 0.01})
Step:   75400, Reward: [-422.515 -422.515 -422.515] [88.504], Avg: [-425.048 -425.048 -425.048] (0.1000) ({r_i: None, r_t: [-876.958 -876.958 -876.958], eps: 0.1})
Step:  107800, Reward: [-379.284 -379.284 -379.284] [70.537], Avg: [-490.554 -490.554 -490.554] (0.0100) ({r_i: None, r_t: [-792.322 -792.322 -792.322], eps: 0.01})
Step:  189800, Reward: [-423.933 -423.933 -423.933] [75.895], Avg: [-445.304 -445.304 -445.304] (0.0100) ({r_i: None, r_t: [-894.356 -894.356 -894.356], eps: 0.01})
Step:   75500, Reward: [-430.895 -430.895 -430.895] [66.357], Avg: [-425.056 -425.056 -425.056] (0.1000) ({r_i: None, r_t: [-804.468 -804.468 -804.468], eps: 0.1})
Step:  107900, Reward: [-396.239 -396.239 -396.239] [75.028], Avg: [-490.467 -490.467 -490.467] (0.0100) ({r_i: None, r_t: [-777.323 -777.323 -777.323], eps: 0.01})
Step:  189900, Reward: [-412.671 -412.671 -412.671] [85.863], Avg: [-445.286 -445.286 -445.286] (0.0100) ({r_i: None, r_t: [-854.751 -854.751 -854.751], eps: 0.01})
Step:   75600, Reward: [-399.812 -399.812 -399.812] [61.975], Avg: [-425.022 -425.022 -425.022] (0.1000) ({r_i: None, r_t: [-839.866 -839.866 -839.866], eps: 0.1})
Step:  190000, Reward: [-373.713 -373.713 -373.713] [82.599], Avg: [-445.249 -445.249 -445.249] (0.0100) ({r_i: None, r_t: [-906.405 -906.405 -906.405], eps: 0.01})
Step:  108000, Reward: [-386.052 -386.052 -386.052] [70.790], Avg: [-490.370 -490.370 -490.370] (0.0100) ({r_i: None, r_t: [-810.994 -810.994 -810.994], eps: 0.01})
Step:  190100, Reward: [-418.332 -418.332 -418.332] [78.118], Avg: [-445.235 -445.235 -445.235] (0.0100) ({r_i: None, r_t: [-839.445 -839.445 -839.445], eps: 0.01})
Step:   75700, Reward: [-406.657 -406.657 -406.657] [61.367], Avg: [-424.998 -424.998 -424.998] (0.1000) ({r_i: None, r_t: [-824.450 -824.450 -824.450], eps: 0.1})
Step:  108100, Reward: [-413.111 -413.111 -413.111] [89.074], Avg: [-490.299 -490.299 -490.299] (0.0100) ({r_i: None, r_t: [-786.537 -786.537 -786.537], eps: 0.01})
Step:  190200, Reward: [-423.603 -423.603 -423.603] [120.432], Avg: [-445.223 -445.223 -445.223] (0.0100) ({r_i: None, r_t: [-818.685 -818.685 -818.685], eps: 0.01})
Step:   75800, Reward: [-437.877 -437.877 -437.877] [94.832], Avg: [-425.015 -425.015 -425.015] (0.1000) ({r_i: None, r_t: [-828.365 -828.365 -828.365], eps: 0.1})
Step:  108200, Reward: [-379.476 -379.476 -379.476] [54.376], Avg: [-490.197 -490.197 -490.197] (0.0100) ({r_i: None, r_t: [-774.678 -774.678 -774.678], eps: 0.01})
Step:  190300, Reward: [-425.664 -425.664 -425.664] [110.226], Avg: [-445.213 -445.213 -445.213] (0.0100) ({r_i: None, r_t: [-847.516 -847.516 -847.516], eps: 0.01})
Step:   75900, Reward: [-409.650 -409.650 -409.650] [93.073], Avg: [-424.995 -424.995 -424.995] (0.1000) ({r_i: None, r_t: [-833.541 -833.541 -833.541], eps: 0.1})
Step:  190400, Reward: [-407.772 -407.772 -407.772] [113.125], Avg: [-445.193 -445.193 -445.193] (0.0100) ({r_i: None, r_t: [-887.505 -887.505 -887.505], eps: 0.01})
Step:  108300, Reward: [-373.994 -373.994 -373.994] [59.202], Avg: [-490.089 -490.089 -490.089] (0.0100) ({r_i: None, r_t: [-791.776 -791.776 -791.776], eps: 0.01})
Step:  190500, Reward: [-429.085 -429.085 -429.085] [83.896], Avg: [-445.185 -445.185 -445.185] (0.0100) ({r_i: None, r_t: [-855.517 -855.517 -855.517], eps: 0.01})
Step:   76000, Reward: [-436.525 -436.525 -436.525] [73.956], Avg: [-425.010 -425.010 -425.010] (0.1000) ({r_i: None, r_t: [-839.678 -839.678 -839.678], eps: 0.1})
Step:  108400, Reward: [-418.163 -418.163 -418.163] [53.328], Avg: [-490.023 -490.023 -490.023] (0.0100) ({r_i: None, r_t: [-775.000 -775.000 -775.000], eps: 0.01})
Step:  190600, Reward: [-441.312 -441.312 -441.312] [94.074], Avg: [-445.183 -445.183 -445.183] (0.0100) ({r_i: None, r_t: [-805.684 -805.684 -805.684], eps: 0.01})
Step:   76100, Reward: [-404.520 -404.520 -404.520] [77.768], Avg: [-424.983 -424.983 -424.983] (0.1000) ({r_i: None, r_t: [-878.567 -878.567 -878.567], eps: 0.1})
Step:  108500, Reward: [-380.548 -380.548 -380.548] [82.023], Avg: [-489.922 -489.922 -489.922] (0.0100) ({r_i: None, r_t: [-817.298 -817.298 -817.298], eps: 0.01})
Step:  190700, Reward: [-428.707 -428.707 -428.707] [70.840], Avg: [-445.174 -445.174 -445.174] (0.0100) ({r_i: None, r_t: [-929.713 -929.713 -929.713], eps: 0.01})
Step:  190800, Reward: [-434.335 -434.335 -434.335] [87.481], Avg: [-445.169 -445.169 -445.169] (0.0100) ({r_i: None, r_t: [-799.782 -799.782 -799.782], eps: 0.01})
Step:   76200, Reward: [-413.369 -413.369 -413.369] [81.468], Avg: [-424.968 -424.968 -424.968] (0.1000) ({r_i: None, r_t: [-852.095 -852.095 -852.095], eps: 0.1})
Step:  108600, Reward: [-387.009 -387.009 -387.009] [67.016], Avg: [-489.828 -489.828 -489.828] (0.0100) ({r_i: None, r_t: [-778.498 -778.498 -778.498], eps: 0.01})
Step:  190900, Reward: [-429.759 -429.759 -429.759] [59.672], Avg: [-445.160 -445.160 -445.160] (0.0100) ({r_i: None, r_t: [-815.432 -815.432 -815.432], eps: 0.01})
Step:   76300, Reward: [-410.336 -410.336 -410.336] [62.325], Avg: [-424.949 -424.949 -424.949] (0.1000) ({r_i: None, r_t: [-834.677 -834.677 -834.677], eps: 0.1})
Step:  108700, Reward: [-379.920 -379.920 -379.920] [62.757], Avg: [-489.727 -489.727 -489.727] (0.0100) ({r_i: None, r_t: [-737.792 -737.792 -737.792], eps: 0.01})
Step:  191000, Reward: [-424.945 -424.945 -424.945] [62.149], Avg: [-445.150 -445.150 -445.150] (0.0100) ({r_i: None, r_t: [-823.432 -823.432 -823.432], eps: 0.01})
Step:   76400, Reward: [-431.072 -431.072 -431.072] [69.023], Avg: [-424.957 -424.957 -424.957] (0.1000) ({r_i: None, r_t: [-795.880 -795.880 -795.880], eps: 0.1})
Step:  191100, Reward: [-437.971 -437.971 -437.971] [114.257], Avg: [-445.146 -445.146 -445.146] (0.0100) ({r_i: None, r_t: [-830.261 -830.261 -830.261], eps: 0.01})
Step:  108800, Reward: [-385.995 -385.995 -385.995] [67.938], Avg: [-489.631 -489.631 -489.631] (0.0100) ({r_i: None, r_t: [-775.368 -775.368 -775.368], eps: 0.01})
Step:  191200, Reward: [-447.579 -447.579 -447.579] [101.070], Avg: [-445.147 -445.147 -445.147] (0.0100) ({r_i: None, r_t: [-890.192 -890.192 -890.192], eps: 0.01})
Step:   76500, Reward: [-394.763 -394.763 -394.763] [48.369], Avg: [-424.917 -424.917 -424.917] (0.1000) ({r_i: None, r_t: [-801.669 -801.669 -801.669], eps: 0.1})
Step:  108900, Reward: [-396.883 -396.883 -396.883] [78.584], Avg: [-489.546 -489.546 -489.546] (0.0100) ({r_i: None, r_t: [-765.361 -765.361 -765.361], eps: 0.01})
Step:  191300, Reward: [-442.508 -442.508 -442.508] [91.893], Avg: [-445.146 -445.146 -445.146] (0.0100) ({r_i: None, r_t: [-828.850 -828.850 -828.850], eps: 0.01})
Step:   76600, Reward: [-410.117 -410.117 -410.117] [78.475], Avg: [-424.898 -424.898 -424.898] (0.1000) ({r_i: None, r_t: [-825.901 -825.901 -825.901], eps: 0.1})
Step:  109000, Reward: [-373.558 -373.558 -373.558] [83.505], Avg: [-489.440 -489.440 -489.440] (0.0100) ({r_i: None, r_t: [-789.260 -789.260 -789.260], eps: 0.01})
Step:  191400, Reward: [-424.623 -424.623 -424.623] [89.393], Avg: [-445.135 -445.135 -445.135] (0.0100) ({r_i: None, r_t: [-848.951 -848.951 -848.951], eps: 0.01})
Step:   76700, Reward: [-428.675 -428.675 -428.675] [95.375], Avg: [-424.903 -424.903 -424.903] (0.1000) ({r_i: None, r_t: [-854.438 -854.438 -854.438], eps: 0.1})
Step:  191500, Reward: [-377.835 -377.835 -377.835] [73.503], Avg: [-445.100 -445.100 -445.100] (0.0100) ({r_i: None, r_t: [-863.058 -863.058 -863.058], eps: 0.01})
Step:  109100, Reward: [-396.631 -396.631 -396.631] [74.029], Avg: [-489.355 -489.355 -489.355] (0.0100) ({r_i: None, r_t: [-773.836 -773.836 -773.836], eps: 0.01})
Step:  191600, Reward: [-434.308 -434.308 -434.308] [89.367], Avg: [-445.095 -445.095 -445.095] (0.0100) ({r_i: None, r_t: [-837.833 -837.833 -837.833], eps: 0.01})
Step:   76800, Reward: [-417.793 -417.793 -417.793] [44.577], Avg: [-424.894 -424.894 -424.894] (0.1000) ({r_i: None, r_t: [-821.629 -821.629 -821.629], eps: 0.1})
Step:  109200, Reward: [-415.803 -415.803 -415.803] [94.891], Avg: [-489.288 -489.288 -489.288] (0.0100) ({r_i: None, r_t: [-784.197 -784.197 -784.197], eps: 0.01})
Step:  191700, Reward: [-436.284 -436.284 -436.284] [107.156], Avg: [-445.090 -445.090 -445.090] (0.0100) ({r_i: None, r_t: [-895.021 -895.021 -895.021], eps: 0.01})
Step:   76900, Reward: [-416.826 -416.826 -416.826] [75.891], Avg: [-424.883 -424.883 -424.883] (0.1000) ({r_i: None, r_t: [-817.442 -817.442 -817.442], eps: 0.1})
Step:  109300, Reward: [-376.211 -376.211 -376.211] [67.533], Avg: [-489.184 -489.184 -489.184] (0.0100) ({r_i: None, r_t: [-801.993 -801.993 -801.993], eps: 0.01})
Step:  191800, Reward: [-421.582 -421.582 -421.582] [71.413], Avg: [-445.078 -445.078 -445.078] (0.0100) ({r_i: None, r_t: [-768.993 -768.993 -768.993], eps: 0.01})
Step:   77000, Reward: [-408.978 -408.978 -408.978] [52.024], Avg: [-424.863 -424.863 -424.863] (0.1000) ({r_i: None, r_t: [-832.384 -832.384 -832.384], eps: 0.1})
Step:  191900, Reward: [-419.085 -419.085 -419.085] [116.494], Avg: [-445.064 -445.064 -445.064] (0.0100) ({r_i: None, r_t: [-874.186 -874.186 -874.186], eps: 0.01})
Step:  109400, Reward: [-383.580 -383.580 -383.580] [50.302], Avg: [-489.088 -489.088 -489.088] (0.0100) ({r_i: None, r_t: [-773.443 -773.443 -773.443], eps: 0.01})
Step:  192000, Reward: [-414.430 -414.430 -414.430] [115.323], Avg: [-445.048 -445.048 -445.048] (0.0100) ({r_i: None, r_t: [-847.395 -847.395 -847.395], eps: 0.01})
Step:   77100, Reward: [-422.958 -422.958 -422.958] [58.971], Avg: [-424.860 -424.860 -424.860] (0.1000) ({r_i: None, r_t: [-811.774 -811.774 -811.774], eps: 0.1})
Step:  109500, Reward: [-399.837 -399.837 -399.837] [54.516], Avg: [-489.006 -489.006 -489.006] (0.0100) ({r_i: None, r_t: [-763.793 -763.793 -763.793], eps: 0.01})
Step:  192100, Reward: [-446.888 -446.888 -446.888] [63.043], Avg: [-445.049 -445.049 -445.049] (0.0100) ({r_i: None, r_t: [-864.691 -864.691 -864.691], eps: 0.01})
Step:   77200, Reward: [-368.998 -368.998 -368.998] [52.208], Avg: [-424.788 -424.788 -424.788] (0.1000) ({r_i: None, r_t: [-799.467 -799.467 -799.467], eps: 0.1})
Step:  109600, Reward: [-393.588 -393.588 -393.588] [73.717], Avg: [-488.919 -488.919 -488.919] (0.0100) ({r_i: None, r_t: [-745.715 -745.715 -745.715], eps: 0.01})
Step:  192200, Reward: [-416.204 -416.204 -416.204] [106.736], Avg: [-445.034 -445.034 -445.034] (0.0100) ({r_i: None, r_t: [-832.881 -832.881 -832.881], eps: 0.01})
Step:   77300, Reward: [-417.508 -417.508 -417.508] [65.692], Avg: [-424.779 -424.779 -424.779] (0.1000) ({r_i: None, r_t: [-797.324 -797.324 -797.324], eps: 0.1})
Step:  192300, Reward: [-406.656 -406.656 -406.656] [54.573], Avg: [-445.014 -445.014 -445.014] (0.0100) ({r_i: None, r_t: [-850.233 -850.233 -850.233], eps: 0.01})
Step:  109700, Reward: [-368.643 -368.643 -368.643] [52.785], Avg: [-488.810 -488.810 -488.810] (0.0100) ({r_i: None, r_t: [-784.807 -784.807 -784.807], eps: 0.01})
Step:  192400, Reward: [-402.688 -402.688 -402.688] [93.657], Avg: [-444.992 -444.992 -444.992] (0.0100) ({r_i: None, r_t: [-853.850 -853.850 -853.850], eps: 0.01})
Step:   77400, Reward: [-382.280 -382.280 -382.280] [69.839], Avg: [-424.724 -424.724 -424.724] (0.1000) ({r_i: None, r_t: [-771.865 -771.865 -771.865], eps: 0.1})
Step:  109800, Reward: [-409.354 -409.354 -409.354] [53.845], Avg: [-488.738 -488.738 -488.738] (0.0100) ({r_i: None, r_t: [-777.119 -777.119 -777.119], eps: 0.01})
Step:  192500, Reward: [-400.145 -400.145 -400.145] [68.747], Avg: [-444.969 -444.969 -444.969] (0.0100) ({r_i: None, r_t: [-812.644 -812.644 -812.644], eps: 0.01})
Step:   77500, Reward: [-408.600 -408.600 -408.600] [92.650], Avg: [-424.703 -424.703 -424.703] (0.1000) ({r_i: None, r_t: [-792.323 -792.323 -792.323], eps: 0.1})
Step:  109900, Reward: [-385.750 -385.750 -385.750] [70.652], Avg: [-488.644 -488.644 -488.644] (0.0100) ({r_i: None, r_t: [-737.769 -737.769 -737.769], eps: 0.01})
Step:  192600, Reward: [-422.958 -422.958 -422.958] [76.334], Avg: [-444.958 -444.958 -444.958] (0.0100) ({r_i: None, r_t: [-874.573 -874.573 -874.573], eps: 0.01})
Step:   77600, Reward: [-381.486 -381.486 -381.486] [58.569], Avg: [-424.647 -424.647 -424.647] (0.1000) ({r_i: None, r_t: [-803.087 -803.087 -803.087], eps: 0.1})
Step:  192700, Reward: [-428.484 -428.484 -428.484] [76.223], Avg: [-444.949 -444.949 -444.949] (0.0100) ({r_i: None, r_t: [-837.296 -837.296 -837.296], eps: 0.01})
Step:  110000, Reward: [-404.982 -404.982 -404.982] [77.834], Avg: [-488.568 -488.568 -488.568] (0.0100) ({r_i: None, r_t: [-738.405 -738.405 -738.405], eps: 0.01})
Step:  192800, Reward: [-404.479 -404.479 -404.479] [62.816], Avg: [-444.928 -444.928 -444.928] (0.0100) ({r_i: None, r_t: [-879.076 -879.076 -879.076], eps: 0.01})
Step:   77700, Reward: [-385.347 -385.347 -385.347] [57.690], Avg: [-424.597 -424.597 -424.597] (0.1000) ({r_i: None, r_t: [-784.957 -784.957 -784.957], eps: 0.1})
Step:  110100, Reward: [-363.231 -363.231 -363.231] [47.047], Avg: [-488.454 -488.454 -488.454] (0.0100) ({r_i: None, r_t: [-739.825 -739.825 -739.825], eps: 0.01})
Step:  192900, Reward: [-440.397 -440.397 -440.397] [87.132], Avg: [-444.926 -444.926 -444.926] (0.0100) ({r_i: None, r_t: [-926.243 -926.243 -926.243], eps: 0.01})
Step:   77800, Reward: [-378.548 -378.548 -378.548] [53.856], Avg: [-424.538 -424.538 -424.538] (0.1000) ({r_i: None, r_t: [-831.311 -831.311 -831.311], eps: 0.1})
Step:  110200, Reward: [-365.896 -365.896 -365.896] [67.449], Avg: [-488.343 -488.343 -488.343] (0.0100) ({r_i: None, r_t: [-782.212 -782.212 -782.212], eps: 0.01})
Step:  193000, Reward: [-438.505 -438.505 -438.505] [101.130], Avg: [-444.922 -444.922 -444.922] (0.0100) ({r_i: None, r_t: [-827.838 -827.838 -827.838], eps: 0.01})
Step:   77900, Reward: [-431.502 -431.502 -431.502] [62.095], Avg: [-424.547 -424.547 -424.547] (0.1000) ({r_i: None, r_t: [-799.601 -799.601 -799.601], eps: 0.1})
Step:  193100, Reward: [-437.883 -437.883 -437.883] [71.633], Avg: [-444.919 -444.919 -444.919] (0.0100) ({r_i: None, r_t: [-861.450 -861.450 -861.450], eps: 0.01})
Step:  110300, Reward: [-383.881 -383.881 -383.881] [74.998], Avg: [-488.249 -488.249 -488.249] (0.0100) ({r_i: None, r_t: [-760.712 -760.712 -760.712], eps: 0.01})
Step:  193200, Reward: [-418.019 -418.019 -418.019] [91.473], Avg: [-444.905 -444.905 -444.905] (0.0100) ({r_i: None, r_t: [-796.642 -796.642 -796.642], eps: 0.01})
Step:   78000, Reward: [-400.703 -400.703 -400.703] [62.798], Avg: [-424.516 -424.516 -424.516] (0.1000) ({r_i: None, r_t: [-806.941 -806.941 -806.941], eps: 0.1})
Step:  110400, Reward: [-392.707 -392.707 -392.707] [62.856], Avg: [-488.162 -488.162 -488.162] (0.0100) ({r_i: None, r_t: [-750.065 -750.065 -750.065], eps: 0.01})
Step:  193300, Reward: [-425.707 -425.707 -425.707] [98.637], Avg: [-444.895 -444.895 -444.895] (0.0100) ({r_i: None, r_t: [-805.157 -805.157 -805.157], eps: 0.01})
Step:   78100, Reward: [-397.579 -397.579 -397.579] [74.409], Avg: [-424.482 -424.482 -424.482] (0.1000) ({r_i: None, r_t: [-807.984 -807.984 -807.984], eps: 0.1})
Step:  110500, Reward: [-392.124 -392.124 -392.124] [74.551], Avg: [-488.075 -488.075 -488.075] (0.0100) ({r_i: None, r_t: [-773.514 -773.514 -773.514], eps: 0.01})
Step:  193400, Reward: [-415.219 -415.219 -415.219] [53.985], Avg: [-444.880 -444.880 -444.880] (0.0100) ({r_i: None, r_t: [-850.048 -850.048 -850.048], eps: 0.01})
Step:   78200, Reward: [-419.038 -419.038 -419.038] [81.079], Avg: [-424.475 -424.475 -424.475] (0.1000) ({r_i: None, r_t: [-824.581 -824.581 -824.581], eps: 0.1})
Step:  193500, Reward: [-411.317 -411.317 -411.317] [79.696], Avg: [-444.862 -444.862 -444.862] (0.0100) ({r_i: None, r_t: [-870.822 -870.822 -870.822], eps: 0.01})
Step:  110600, Reward: [-395.430 -395.430 -395.430] [73.425], Avg: [-487.992 -487.992 -487.992] (0.0100) ({r_i: None, r_t: [-797.854 -797.854 -797.854], eps: 0.01})
Step:  193600, Reward: [-422.442 -422.442 -422.442] [79.195], Avg: [-444.851 -444.851 -444.851] (0.0100) ({r_i: None, r_t: [-785.047 -785.047 -785.047], eps: 0.01})
Step:   78300, Reward: [-408.817 -408.817 -408.817] [49.746], Avg: [-424.455 -424.455 -424.455] (0.1000) ({r_i: None, r_t: [-849.514 -849.514 -849.514], eps: 0.1})
Step:  110700, Reward: [-391.496 -391.496 -391.496] [80.174], Avg: [-487.904 -487.904 -487.904] (0.0100) ({r_i: None, r_t: [-784.327 -784.327 -784.327], eps: 0.01})
Step:  193700, Reward: [-412.179 -412.179 -412.179] [98.388], Avg: [-444.834 -444.834 -444.834] (0.0100) ({r_i: None, r_t: [-838.354 -838.354 -838.354], eps: 0.01})
Step:   78400, Reward: [-402.275 -402.275 -402.275] [60.984], Avg: [-424.426 -424.426 -424.426] (0.1000) ({r_i: None, r_t: [-850.490 -850.490 -850.490], eps: 0.1})
Step:  110800, Reward: [-398.572 -398.572 -398.572] [65.253], Avg: [-487.824 -487.824 -487.824] (0.0100) ({r_i: None, r_t: [-760.495 -760.495 -760.495], eps: 0.01})
Step:  193800, Reward: [-396.448 -396.448 -396.448] [100.430], Avg: [-444.809 -444.809 -444.809] (0.0100) ({r_i: None, r_t: [-801.572 -801.572 -801.572], eps: 0.01})
Step:   78500, Reward: [-442.847 -442.847 -442.847] [82.454], Avg: [-424.450 -424.450 -424.450] (0.1000) ({r_i: None, r_t: [-767.501 -767.501 -767.501], eps: 0.1})
Step:  193900, Reward: [-403.832 -403.832 -403.832] [72.511], Avg: [-444.788 -444.788 -444.788] (0.0100) ({r_i: None, r_t: [-862.904 -862.904 -862.904], eps: 0.01})
Step:  110900, Reward: [-407.184 -407.184 -407.184] [63.657], Avg: [-487.751 -487.751 -487.751] (0.0100) ({r_i: None, r_t: [-841.030 -841.030 -841.030], eps: 0.01})
Step:  194000, Reward: [-393.496 -393.496 -393.496] [104.975], Avg: [-444.761 -444.761 -444.761] (0.0100) ({r_i: None, r_t: [-833.502 -833.502 -833.502], eps: 0.01})
Step:   78600, Reward: [-380.703 -380.703 -380.703] [58.657], Avg: [-424.394 -424.394 -424.394] (0.1000) ({r_i: None, r_t: [-800.784 -800.784 -800.784], eps: 0.1})
Step:  111000, Reward: [-372.009 -372.009 -372.009] [67.773], Avg: [-487.647 -487.647 -487.647] (0.0100) ({r_i: None, r_t: [-778.437 -778.437 -778.437], eps: 0.01})
Step:  194100, Reward: [-409.311 -409.311 -409.311] [84.803], Avg: [-444.743 -444.743 -444.743] (0.0100) ({r_i: None, r_t: [-833.912 -833.912 -833.912], eps: 0.01})
Step:   78700, Reward: [-394.628 -394.628 -394.628] [41.103], Avg: [-424.356 -424.356 -424.356] (0.1000) ({r_i: None, r_t: [-790.870 -790.870 -790.870], eps: 0.1})
Step:  111100, Reward: [-374.109 -374.109 -374.109] [63.890], Avg: [-487.545 -487.545 -487.545] (0.0100) ({r_i: None, r_t: [-739.870 -739.870 -739.870], eps: 0.01})
Step:  194200, Reward: [-428.277 -428.277 -428.277] [78.588], Avg: [-444.735 -444.735 -444.735] (0.0100) ({r_i: None, r_t: [-884.971 -884.971 -884.971], eps: 0.01})
Step:   78800, Reward: [-409.425 -409.425 -409.425] [84.138], Avg: [-424.338 -424.338 -424.338] (0.1000) ({r_i: None, r_t: [-777.688 -777.688 -777.688], eps: 0.1})
Step:  194300, Reward: [-417.376 -417.376 -417.376] [93.429], Avg: [-444.720 -444.720 -444.720] (0.0100) ({r_i: None, r_t: [-871.976 -871.976 -871.976], eps: 0.01})
Step:  111200, Reward: [-389.821 -389.821 -389.821] [70.243], Avg: [-487.457 -487.457 -487.457] (0.0100) ({r_i: None, r_t: [-799.922 -799.922 -799.922], eps: 0.01})
Step:  194400, Reward: [-404.715 -404.715 -404.715] [83.654], Avg: [-444.700 -444.700 -444.700] (0.0100) ({r_i: None, r_t: [-857.258 -857.258 -857.258], eps: 0.01})
Step:   78900, Reward: [-387.704 -387.704 -387.704] [59.021], Avg: [-424.291 -424.291 -424.291] (0.1000) ({r_i: None, r_t: [-789.576 -789.576 -789.576], eps: 0.1})
Step:  111300, Reward: [-422.812 -422.812 -422.812] [70.843], Avg: [-487.399 -487.399 -487.399] (0.0100) ({r_i: None, r_t: [-791.617 -791.617 -791.617], eps: 0.01})
Step:  194500, Reward: [-422.888 -422.888 -422.888] [70.173], Avg: [-444.689 -444.689 -444.689] (0.0100) ({r_i: None, r_t: [-849.792 -849.792 -849.792], eps: 0.01})
Step:   79000, Reward: [-417.696 -417.696 -417.696] [56.920], Avg: [-424.283 -424.283 -424.283] (0.1000) ({r_i: None, r_t: [-767.367 -767.367 -767.367], eps: 0.1})
Step:  111400, Reward: [-389.643 -389.643 -389.643] [73.170], Avg: [-487.311 -487.311 -487.311] (0.0100) ({r_i: None, r_t: [-781.179 -781.179 -781.179], eps: 0.01})
Step:  194600, Reward: [-429.555 -429.555 -429.555] [97.157], Avg: [-444.681 -444.681 -444.681] (0.0100) ({r_i: None, r_t: [-833.378 -833.378 -833.378], eps: 0.01})
Step:  194700, Reward: [-409.975 -409.975 -409.975] [110.332], Avg: [-444.663 -444.663 -444.663] (0.0100) ({r_i: None, r_t: [-873.138 -873.138 -873.138], eps: 0.01})
Step:   79100, Reward: [-369.019 -369.019 -369.019] [72.254], Avg: [-424.213 -424.213 -424.213] (0.1000) ({r_i: None, r_t: [-796.077 -796.077 -796.077], eps: 0.1})
Step:  111500, Reward: [-388.901 -388.901 -388.901] [50.963], Avg: [-487.223 -487.223 -487.223] (0.0100) ({r_i: None, r_t: [-807.799 -807.799 -807.799], eps: 0.01})
Step:  194800, Reward: [-415.137 -415.137 -415.137] [77.232], Avg: [-444.648 -444.648 -444.648] (0.0100) ({r_i: None, r_t: [-886.504 -886.504 -886.504], eps: 0.01})
Step:   79200, Reward: [-403.992 -403.992 -403.992] [60.516], Avg: [-424.188 -424.188 -424.188] (0.1000) ({r_i: None, r_t: [-790.877 -790.877 -790.877], eps: 0.1})
Step:  111600, Reward: [-371.150 -371.150 -371.150] [64.396], Avg: [-487.119 -487.119 -487.119] (0.0100) ({r_i: None, r_t: [-764.173 -764.173 -764.173], eps: 0.01})
Step:  194900, Reward: [-415.133 -415.133 -415.133] [98.755], Avg: [-444.633 -444.633 -444.633] (0.0100) ({r_i: None, r_t: [-889.820 -889.820 -889.820], eps: 0.01})
Step:   79300, Reward: [-375.347 -375.347 -375.347] [56.657], Avg: [-424.126 -424.126 -424.126] (0.1000) ({r_i: None, r_t: [-806.662 -806.662 -806.662], eps: 0.1})
Step:  195000, Reward: [-401.091 -401.091 -401.091] [93.873], Avg: [-444.610 -444.610 -444.610] (0.0100) ({r_i: None, r_t: [-877.302 -877.302 -877.302], eps: 0.01})
Step:  111700, Reward: [-416.463 -416.463 -416.463] [62.648], Avg: [-487.056 -487.056 -487.056] (0.0100) ({r_i: None, r_t: [-749.168 -749.168 -749.168], eps: 0.01})
Step:  195100, Reward: [-407.455 -407.455 -407.455] [76.662], Avg: [-444.591 -444.591 -444.591] (0.0100) ({r_i: None, r_t: [-860.111 -860.111 -860.111], eps: 0.01})
Step:   79400, Reward: [-412.220 -412.220 -412.220] [68.697], Avg: [-424.111 -424.111 -424.111] (0.1000) ({r_i: None, r_t: [-776.515 -776.515 -776.515], eps: 0.1})
Step:  111800, Reward: [-375.914 -375.914 -375.914] [50.502], Avg: [-486.957 -486.957 -486.957] (0.0100) ({r_i: None, r_t: [-764.851 -764.851 -764.851], eps: 0.01})
Step:  195200, Reward: [-450.123 -450.123 -450.123] [89.895], Avg: [-444.594 -444.594 -444.594] (0.0100) ({r_i: None, r_t: [-836.107 -836.107 -836.107], eps: 0.01})
Step:   79500, Reward: [-402.207 -402.207 -402.207] [77.518], Avg: [-424.084 -424.084 -424.084] (0.1000) ({r_i: None, r_t: [-774.903 -774.903 -774.903], eps: 0.1})
Step:  111900, Reward: [-358.926 -358.926 -358.926] [55.894], Avg: [-486.843 -486.843 -486.843] (0.0100) ({r_i: None, r_t: [-787.047 -787.047 -787.047], eps: 0.01})
Step:  195300, Reward: [-405.883 -405.883 -405.883] [107.223], Avg: [-444.574 -444.574 -444.574] (0.0100) ({r_i: None, r_t: [-867.983 -867.983 -867.983], eps: 0.01})
Step:   79600, Reward: [-395.517 -395.517 -395.517] [52.201], Avg: [-424.048 -424.048 -424.048] (0.1000) ({r_i: None, r_t: [-745.350 -745.350 -745.350], eps: 0.1})
Step:  195400, Reward: [-401.751 -401.751 -401.751] [75.002], Avg: [-444.553 -444.553 -444.553] (0.0100) ({r_i: None, r_t: [-866.175 -866.175 -866.175], eps: 0.01})
Step:  112000, Reward: [-398.068 -398.068 -398.068] [84.076], Avg: [-486.763 -486.763 -486.763] (0.0100) ({r_i: None, r_t: [-761.592 -761.592 -761.592], eps: 0.01})
Step:  195500, Reward: [-430.172 -430.172 -430.172] [76.035], Avg: [-444.545 -444.545 -444.545] (0.0100) ({r_i: None, r_t: [-802.413 -802.413 -802.413], eps: 0.01})
Step:   79700, Reward: [-405.188 -405.188 -405.188] [61.740], Avg: [-424.024 -424.024 -424.024] (0.1000) ({r_i: None, r_t: [-765.892 -765.892 -765.892], eps: 0.1})
Step:  112100, Reward: [-363.736 -363.736 -363.736] [80.797], Avg: [-486.654 -486.654 -486.654] (0.0100) ({r_i: None, r_t: [-759.304 -759.304 -759.304], eps: 0.01})
Step:  195600, Reward: [-410.393 -410.393 -410.393] [116.830], Avg: [-444.528 -444.528 -444.528] (0.0100) ({r_i: None, r_t: [-805.166 -805.166 -805.166], eps: 0.01})
Step:   79800, Reward: [-384.709 -384.709 -384.709] [57.737], Avg: [-423.975 -423.975 -423.975] (0.1000) ({r_i: None, r_t: [-792.379 -792.379 -792.379], eps: 0.1})
Step:  112200, Reward: [-365.286 -365.286 -365.286] [45.009], Avg: [-486.546 -486.546 -486.546] (0.0100) ({r_i: None, r_t: [-799.640 -799.640 -799.640], eps: 0.01})
Step:  195700, Reward: [-393.164 -393.164 -393.164] [98.355], Avg: [-444.502 -444.502 -444.502] (0.0100) ({r_i: None, r_t: [-801.244 -801.244 -801.244], eps: 0.01})
Step:   79900, Reward: [-382.119 -382.119 -382.119] [50.294], Avg: [-423.923 -423.923 -423.923] (0.1000) ({r_i: None, r_t: [-799.100 -799.100 -799.100], eps: 0.1})
Step:  195800, Reward: [-428.991 -428.991 -428.991] [112.554], Avg: [-444.494 -444.494 -444.494] (0.0100) ({r_i: None, r_t: [-875.757 -875.757 -875.757], eps: 0.01})
Step:  112300, Reward: [-380.306 -380.306 -380.306] [52.485], Avg: [-486.451 -486.451 -486.451] (0.0100) ({r_i: None, r_t: [-767.286 -767.286 -767.286], eps: 0.01})
Step:  195900, Reward: [-428.285 -428.285 -428.285] [94.577], Avg: [-444.485 -444.485 -444.485] (0.0100) ({r_i: None, r_t: [-812.863 -812.863 -812.863], eps: 0.01})
Step:   80000, Reward: [-396.482 -396.482 -396.482] [44.990], Avg: [-423.888 -423.888 -423.888] (0.1000) ({r_i: None, r_t: [-780.651 -780.651 -780.651], eps: 0.1})
Step:  112400, Reward: [-369.336 -369.336 -369.336] [76.331], Avg: [-486.347 -486.347 -486.347] (0.0100) ({r_i: None, r_t: [-782.664 -782.664 -782.664], eps: 0.01})
Step:  196000, Reward: [-433.449 -433.449 -433.449] [63.393], Avg: [-444.480 -444.480 -444.480] (0.0100) ({r_i: None, r_t: [-838.146 -838.146 -838.146], eps: 0.01})
Step:   80100, Reward: [-407.645 -407.645 -407.645] [55.636], Avg: [-423.868 -423.868 -423.868] (0.1000) ({r_i: None, r_t: [-779.150 -779.150 -779.150], eps: 0.1})
Step:  112500, Reward: [-370.202 -370.202 -370.202] [72.769], Avg: [-486.244 -486.244 -486.244] (0.0100) ({r_i: None, r_t: [-766.321 -766.321 -766.321], eps: 0.01})
Step:  196100, Reward: [-381.179 -381.179 -381.179] [60.171], Avg: [-444.447 -444.447 -444.447] (0.0100) ({r_i: None, r_t: [-796.514 -796.514 -796.514], eps: 0.01})
Step:   80200, Reward: [-396.828 -396.828 -396.828] [52.014], Avg: [-423.834 -423.834 -423.834] (0.1000) ({r_i: None, r_t: [-788.943 -788.943 -788.943], eps: 0.1})
Step:  196200, Reward: [-428.454 -428.454 -428.454] [111.784], Avg: [-444.439 -444.439 -444.439] (0.0100) ({r_i: None, r_t: [-849.102 -849.102 -849.102], eps: 0.01})
Step:  112600, Reward: [-401.114 -401.114 -401.114] [75.982], Avg: [-486.168 -486.168 -486.168] (0.0100) ({r_i: None, r_t: [-761.845 -761.845 -761.845], eps: 0.01})
Step:  196300, Reward: [-412.744 -412.744 -412.744] [65.470], Avg: [-444.423 -444.423 -444.423] (0.0100) ({r_i: None, r_t: [-837.572 -837.572 -837.572], eps: 0.01})
Step:   80300, Reward: [-352.833 -352.833 -352.833] [23.844], Avg: [-423.746 -423.746 -423.746] (0.1000) ({r_i: None, r_t: [-772.885 -772.885 -772.885], eps: 0.1})
Step:  112700, Reward: [-382.244 -382.244 -382.244] [84.070], Avg: [-486.076 -486.076 -486.076] (0.0100) ({r_i: None, r_t: [-734.050 -734.050 -734.050], eps: 0.01})
Step:  196400, Reward: [-409.360 -409.360 -409.360] [72.763], Avg: [-444.405 -444.405 -444.405] (0.0100) ({r_i: None, r_t: [-874.418 -874.418 -874.418], eps: 0.01})
Step:   80400, Reward: [-373.994 -373.994 -373.994] [49.643], Avg: [-423.684 -423.684 -423.684] (0.1000) ({r_i: None, r_t: [-768.775 -768.775 -768.775], eps: 0.1})
Step:  112800, Reward: [-366.923 -366.923 -366.923] [59.195], Avg: [-485.971 -485.971 -485.971] (0.0100) ({r_i: None, r_t: [-770.273 -770.273 -770.273], eps: 0.01})
Step:  196500, Reward: [-418.295 -418.295 -418.295] [64.800], Avg: [-444.392 -444.392 -444.392] (0.0100) ({r_i: None, r_t: [-803.634 -803.634 -803.634], eps: 0.01})
Step:   80500, Reward: [-384.614 -384.614 -384.614] [62.983], Avg: [-423.636 -423.636 -423.636] (0.1000) ({r_i: None, r_t: [-760.599 -760.599 -760.599], eps: 0.1})
Step:  196600, Reward: [-394.281 -394.281 -394.281] [65.521], Avg: [-444.367 -444.367 -444.367] (0.0100) ({r_i: None, r_t: [-862.329 -862.329 -862.329], eps: 0.01})
Step:  112900, Reward: [-377.049 -377.049 -377.049] [61.111], Avg: [-485.874 -485.874 -485.874] (0.0100) ({r_i: None, r_t: [-734.664 -734.664 -734.664], eps: 0.01})
Step:  196700, Reward: [-405.003 -405.003 -405.003] [65.300], Avg: [-444.347 -444.347 -444.347] (0.0100) ({r_i: None, r_t: [-825.448 -825.448 -825.448], eps: 0.01})
Step:   80600, Reward: [-393.093 -393.093 -393.093] [60.809], Avg: [-423.598 -423.598 -423.598] (0.1000) ({r_i: None, r_t: [-763.689 -763.689 -763.689], eps: 0.1})
Step:  113000, Reward: [-387.704 -387.704 -387.704] [66.734], Avg: [-485.787 -485.787 -485.787] (0.0100) ({r_i: None, r_t: [-748.326 -748.326 -748.326], eps: 0.01})
Step:  196800, Reward: [-401.580 -401.580 -401.580] [58.930], Avg: [-444.325 -444.325 -444.325] (0.0100) ({r_i: None, r_t: [-868.837 -868.837 -868.837], eps: 0.01})
Step:   80700, Reward: [-373.544 -373.544 -373.544] [40.597], Avg: [-423.536 -423.536 -423.536] (0.1000) ({r_i: None, r_t: [-754.032 -754.032 -754.032], eps: 0.1})
Step:  113100, Reward: [-388.589 -388.589 -388.589] [62.370], Avg: [-485.702 -485.702 -485.702] (0.0100) ({r_i: None, r_t: [-796.146 -796.146 -796.146], eps: 0.01})
Step:  196900, Reward: [-421.044 -421.044 -421.044] [88.776], Avg: [-444.313 -444.313 -444.313] (0.0100) ({r_i: None, r_t: [-829.291 -829.291 -829.291], eps: 0.01})
Step:  197000, Reward: [-396.086 -396.086 -396.086] [72.590], Avg: [-444.289 -444.289 -444.289] (0.0100) ({r_i: None, r_t: [-850.589 -850.589 -850.589], eps: 0.01})
Step:   80800, Reward: [-383.832 -383.832 -383.832] [58.818], Avg: [-423.487 -423.487 -423.487] (0.1000) ({r_i: None, r_t: [-789.305 -789.305 -789.305], eps: 0.1})
Step:  113200, Reward: [-387.930 -387.930 -387.930] [76.017], Avg: [-485.615 -485.615 -485.615] (0.0100) ({r_i: None, r_t: [-768.521 -768.521 -768.521], eps: 0.01})
Step:  197100, Reward: [-472.548 -472.548 -472.548] [94.569], Avg: [-444.303 -444.303 -444.303] (0.0100) ({r_i: None, r_t: [-909.649 -909.649 -909.649], eps: 0.01})
Step:   80900, Reward: [-398.484 -398.484 -398.484] [51.414], Avg: [-423.456 -423.456 -423.456] (0.1000) ({r_i: None, r_t: [-784.886 -784.886 -784.886], eps: 0.1})
Step:  113300, Reward: [-374.332 -374.332 -374.332] [58.682], Avg: [-485.517 -485.517 -485.517] (0.0100) ({r_i: None, r_t: [-735.428 -735.428 -735.428], eps: 0.01})
Step:  197200, Reward: [-401.189 -401.189 -401.189] [80.794], Avg: [-444.281 -444.281 -444.281] (0.0100) ({r_i: None, r_t: [-861.887 -861.887 -861.887], eps: 0.01})
Step:   81000, Reward: [-405.336 -405.336 -405.336] [63.977], Avg: [-423.434 -423.434 -423.434] (0.1000) ({r_i: None, r_t: [-717.147 -717.147 -717.147], eps: 0.1})
Step:  113400, Reward: [-406.060 -406.060 -406.060] [83.452], Avg: [-485.447 -485.447 -485.447] (0.0100) ({r_i: None, r_t: [-746.166 -746.166 -746.166], eps: 0.01})
Step:  197300, Reward: [-409.521 -409.521 -409.521] [88.080], Avg: [-444.263 -444.263 -444.263] (0.0100) ({r_i: None, r_t: [-879.960 -879.960 -879.960], eps: 0.01})
Step:  197400, Reward: [-437.356 -437.356 -437.356] [86.709], Avg: [-444.260 -444.260 -444.260] (0.0100) ({r_i: None, r_t: [-865.469 -865.469 -865.469], eps: 0.01})
Step:   81100, Reward: [-390.113 -390.113 -390.113] [46.932], Avg: [-423.393 -423.393 -423.393] (0.1000) ({r_i: None, r_t: [-792.394 -792.394 -792.394], eps: 0.1})
Step:  113500, Reward: [-373.843 -373.843 -373.843] [57.948], Avg: [-485.349 -485.349 -485.349] (0.0100) ({r_i: None, r_t: [-731.759 -731.759 -731.759], eps: 0.01})
Step:  197500, Reward: [-452.686 -452.686 -452.686] [84.173], Avg: [-444.264 -444.264 -444.264] (0.0100) ({r_i: None, r_t: [-895.589 -895.589 -895.589], eps: 0.01})
Step:   81200, Reward: [-376.728 -376.728 -376.728] [47.055], Avg: [-423.335 -423.335 -423.335] (0.1000) ({r_i: None, r_t: [-756.645 -756.645 -756.645], eps: 0.1})
Step:  113600, Reward: [-379.728 -379.728 -379.728] [64.429], Avg: [-485.256 -485.256 -485.256] (0.0100) ({r_i: None, r_t: [-728.212 -728.212 -728.212], eps: 0.01})
Step:  197600, Reward: [-408.568 -408.568 -408.568] [78.225], Avg: [-444.246 -444.246 -444.246] (0.0100) ({r_i: None, r_t: [-818.748 -818.748 -818.748], eps: 0.01})
Step:   81300, Reward: [-361.205 -361.205 -361.205] [53.224], Avg: [-423.259 -423.259 -423.259] (0.1000) ({r_i: None, r_t: [-725.056 -725.056 -725.056], eps: 0.1})
Step:  113700, Reward: [-395.036 -395.036 -395.036] [61.741], Avg: [-485.177 -485.177 -485.177] (0.0100) ({r_i: None, r_t: [-724.156 -724.156 -724.156], eps: 0.01})
Step:  197700, Reward: [-432.702 -432.702 -432.702] [69.800], Avg: [-444.240 -444.240 -444.240] (0.0100) ({r_i: None, r_t: [-819.605 -819.605 -819.605], eps: 0.01})
Step:  197800, Reward: [-428.769 -428.769 -428.769] [97.630], Avg: [-444.232 -444.232 -444.232] (0.0100) ({r_i: None, r_t: [-801.704 -801.704 -801.704], eps: 0.01})
Step:   81400, Reward: [-393.914 -393.914 -393.914] [52.074], Avg: [-423.223 -423.223 -423.223] (0.1000) ({r_i: None, r_t: [-744.880 -744.880 -744.880], eps: 0.1})
Step:  113800, Reward: [-388.290 -388.290 -388.290] [68.775], Avg: [-485.092 -485.092 -485.092] (0.0100) ({r_i: None, r_t: [-747.647 -747.647 -747.647], eps: 0.01})
Step:  197900, Reward: [-422.987 -422.987 -422.987] [94.664], Avg: [-444.222 -444.222 -444.222] (0.0100) ({r_i: None, r_t: [-886.780 -886.780 -886.780], eps: 0.01})
Step:   81500, Reward: [-392.042 -392.042 -392.042] [51.683], Avg: [-423.185 -423.185 -423.185] (0.1000) ({r_i: None, r_t: [-791.013 -791.013 -791.013], eps: 0.1})
Step:  113900, Reward: [-390.090 -390.090 -390.090] [62.905], Avg: [-485.008 -485.008 -485.008] (0.0100) ({r_i: None, r_t: [-754.042 -754.042 -754.042], eps: 0.01})
Step:  198000, Reward: [-463.397 -463.397 -463.397] [82.405], Avg: [-444.231 -444.231 -444.231] (0.0100) ({r_i: None, r_t: [-854.756 -854.756 -854.756], eps: 0.01})
Step:   81600, Reward: [-365.933 -365.933 -365.933] [52.144], Avg: [-423.115 -423.115 -423.115] (0.1000) ({r_i: None, r_t: [-768.495 -768.495 -768.495], eps: 0.1})
Step:  198100, Reward: [-441.086 -441.086 -441.086] [73.199], Avg: [-444.230 -444.230 -444.230] (0.0100) ({r_i: None, r_t: [-846.666 -846.666 -846.666], eps: 0.01})
Step:  114000, Reward: [-353.236 -353.236 -353.236] [56.124], Avg: [-484.893 -484.893 -484.893] (0.0100) ({r_i: None, r_t: [-799.747 -799.747 -799.747], eps: 0.01})
Step:  198200, Reward: [-397.401 -397.401 -397.401] [84.723], Avg: [-444.206 -444.206 -444.206] (0.0100) ({r_i: None, r_t: [-840.364 -840.364 -840.364], eps: 0.01})
Step:   81700, Reward: [-366.792 -366.792 -366.792] [52.341], Avg: [-423.046 -423.046 -423.046] (0.1000) ({r_i: None, r_t: [-769.821 -769.821 -769.821], eps: 0.1})
Step:  114100, Reward: [-398.518 -398.518 -398.518] [70.700], Avg: [-484.817 -484.817 -484.817] (0.0100) ({r_i: None, r_t: [-765.060 -765.060 -765.060], eps: 0.01})
Step:  198300, Reward: [-413.437 -413.437 -413.437] [61.725], Avg: [-444.191 -444.191 -444.191] (0.0100) ({r_i: None, r_t: [-856.369 -856.369 -856.369], eps: 0.01})
Step:   81800, Reward: [-382.560 -382.560 -382.560] [61.859], Avg: [-422.996 -422.996 -422.996] (0.1000) ({r_i: None, r_t: [-753.984 -753.984 -753.984], eps: 0.1})
Step:  114200, Reward: [-361.025 -361.025 -361.025] [60.373], Avg: [-484.709 -484.709 -484.709] (0.0100) ({r_i: None, r_t: [-776.478 -776.478 -776.478], eps: 0.01})
Step:  198400, Reward: [-417.153 -417.153 -417.153] [87.889], Avg: [-444.177 -444.177 -444.177] (0.0100) ({r_i: None, r_t: [-889.460 -889.460 -889.460], eps: 0.01})
Step:   81900, Reward: [-377.363 -377.363 -377.363] [62.437], Avg: [-422.941 -422.941 -422.941] (0.1000) ({r_i: None, r_t: [-729.014 -729.014 -729.014], eps: 0.1})
Step:  114300, Reward: [-377.740 -377.740 -377.740] [68.733], Avg: [-484.615 -484.615 -484.615] (0.0100) ({r_i: None, r_t: [-761.453 -761.453 -761.453], eps: 0.01})
Step:  198500, Reward: [-409.022 -409.022 -409.022] [74.575], Avg: [-444.159 -444.159 -444.159] (0.0100) ({r_i: None, r_t: [-842.250 -842.250 -842.250], eps: 0.01})
Step:  198600, Reward: [-416.679 -416.679 -416.679] [90.568], Avg: [-444.146 -444.146 -444.146] (0.0100) ({r_i: None, r_t: [-816.950 -816.950 -816.950], eps: 0.01})
Step:   82000, Reward: [-394.257 -394.257 -394.257] [49.090], Avg: [-422.906 -422.906 -422.906] (0.1000) ({r_i: None, r_t: [-745.332 -745.332 -745.332], eps: 0.1})
Step:  114400, Reward: [-390.272 -390.272 -390.272] [55.998], Avg: [-484.533 -484.533 -484.533] (0.0100) ({r_i: None, r_t: [-760.247 -760.247 -760.247], eps: 0.01})
Step:  198700, Reward: [-458.986 -458.986 -458.986] [85.013], Avg: [-444.153 -444.153 -444.153] (0.0100) ({r_i: None, r_t: [-867.148 -867.148 -867.148], eps: 0.01})
Step:   82100, Reward: [-384.003 -384.003 -384.003] [55.530], Avg: [-422.858 -422.858 -422.858] (0.1000) ({r_i: None, r_t: [-738.852 -738.852 -738.852], eps: 0.1})
Step:  114500, Reward: [-375.225 -375.225 -375.225] [69.273], Avg: [-484.438 -484.438 -484.438] (0.0100) ({r_i: None, r_t: [-773.315 -773.315 -773.315], eps: 0.01})
Step:  198800, Reward: [-410.445 -410.445 -410.445] [95.951], Avg: [-444.136 -444.136 -444.136] (0.0100) ({r_i: None, r_t: [-805.954 -805.954 -805.954], eps: 0.01})
Step:   82200, Reward: [-398.660 -398.660 -398.660] [61.630], Avg: [-422.829 -422.829 -422.829] (0.1000) ({r_i: None, r_t: [-773.089 -773.089 -773.089], eps: 0.1})
Step:  198900, Reward: [-399.056 -399.056 -399.056] [65.662], Avg: [-444.113 -444.113 -444.113] (0.0100) ({r_i: None, r_t: [-916.032 -916.032 -916.032], eps: 0.01})
Step:  114600, Reward: [-381.827 -381.827 -381.827] [43.142], Avg: [-484.348 -484.348 -484.348] (0.0100) ({r_i: None, r_t: [-766.885 -766.885 -766.885], eps: 0.01})
Step:  199000, Reward: [-417.900 -417.900 -417.900] [64.586], Avg: [-444.100 -444.100 -444.100] (0.0100) ({r_i: None, r_t: [-891.288 -891.288 -891.288], eps: 0.01})
Step:   82300, Reward: [-410.967 -410.967 -410.967] [55.266], Avg: [-422.815 -422.815 -422.815] (0.1000) ({r_i: None, r_t: [-776.959 -776.959 -776.959], eps: 0.1})
Step:  114700, Reward: [-366.535 -366.535 -366.535] [63.630], Avg: [-484.246 -484.246 -484.246] (0.0100) ({r_i: None, r_t: [-760.275 -760.275 -760.275], eps: 0.01})
Step:  199100, Reward: [-404.725 -404.725 -404.725] [115.352], Avg: [-444.081 -444.081 -444.081] (0.0100) ({r_i: None, r_t: [-829.417 -829.417 -829.417], eps: 0.01})
Step:   82400, Reward: [-389.537 -389.537 -389.537] [53.809], Avg: [-422.774 -422.774 -422.774] (0.1000) ({r_i: None, r_t: [-777.534 -777.534 -777.534], eps: 0.1})
Step:  114800, Reward: [-366.844 -366.844 -366.844] [52.622], Avg: [-484.143 -484.143 -484.143] (0.0100) ({r_i: None, r_t: [-749.339 -749.339 -749.339], eps: 0.01})
Step:  199200, Reward: [-427.581 -427.581 -427.581] [86.450], Avg: [-444.072 -444.072 -444.072] (0.0100) ({r_i: None, r_t: [-831.761 -831.761 -831.761], eps: 0.01})
Step:   82500, Reward: [-384.598 -384.598 -384.598] [48.050], Avg: [-422.728 -422.728 -422.728] (0.1000) ({r_i: None, r_t: [-791.698 -791.698 -791.698], eps: 0.1})
Step:  199300, Reward: [-406.905 -406.905 -406.905] [87.757], Avg: [-444.054 -444.054 -444.054] (0.0100) ({r_i: None, r_t: [-826.947 -826.947 -826.947], eps: 0.01})
Step:  114900, Reward: [-403.204 -403.204 -403.204] [68.158], Avg: [-484.073 -484.073 -484.073] (0.0100) ({r_i: None, r_t: [-768.773 -768.773 -768.773], eps: 0.01})
Step:  199400, Reward: [-392.847 -392.847 -392.847] [91.638], Avg: [-444.028 -444.028 -444.028] (0.0100) ({r_i: None, r_t: [-801.358 -801.358 -801.358], eps: 0.01})
Step:   82600, Reward: [-377.219 -377.219 -377.219] [52.322], Avg: [-422.673 -422.673 -422.673] (0.1000) ({r_i: None, r_t: [-748.529 -748.529 -748.529], eps: 0.1})
Step:  115000, Reward: [-378.930 -378.930 -378.930] [53.736], Avg: [-483.982 -483.982 -483.982] (0.0100) ({r_i: None, r_t: [-725.517 -725.517 -725.517], eps: 0.01})
Step:  199500, Reward: [-423.869 -423.869 -423.869] [97.518], Avg: [-444.018 -444.018 -444.018] (0.0100) ({r_i: None, r_t: [-889.321 -889.321 -889.321], eps: 0.01})
Step:   82700, Reward: [-394.309 -394.309 -394.309] [59.366], Avg: [-422.639 -422.639 -422.639] (0.1000) ({r_i: None, r_t: [-768.956 -768.956 -768.956], eps: 0.1})
Step:  115100, Reward: [-373.852 -373.852 -373.852] [56.939], Avg: [-483.886 -483.886 -483.886] (0.0100) ({r_i: None, r_t: [-789.973 -789.973 -789.973], eps: 0.01})
Step:  199600, Reward: [-412.301 -412.301 -412.301] [83.441], Avg: [-444.002 -444.002 -444.002] (0.0100) ({r_i: None, r_t: [-843.246 -843.246 -843.246], eps: 0.01})
Step:   82800, Reward: [-389.737 -389.737 -389.737] [59.857], Avg: [-422.599 -422.599 -422.599] (0.1000) ({r_i: None, r_t: [-792.250 -792.250 -792.250], eps: 0.1})
Step:  199700, Reward: [-417.267 -417.267 -417.267] [101.405], Avg: [-443.989 -443.989 -443.989] (0.0100) ({r_i: None, r_t: [-882.259 -882.259 -882.259], eps: 0.01})
Step:  115200, Reward: [-372.768 -372.768 -372.768] [60.608], Avg: [-483.790 -483.790 -483.790] (0.0100) ({r_i: None, r_t: [-753.120 -753.120 -753.120], eps: 0.01})
Step:  199800, Reward: [-416.215 -416.215 -416.215] [75.666], Avg: [-443.975 -443.975 -443.975] (0.0100) ({r_i: None, r_t: [-843.573 -843.573 -843.573], eps: 0.01})
Step:   82900, Reward: [-367.543 -367.543 -367.543] [35.341], Avg: [-422.533 -422.533 -422.533] (0.1000) ({r_i: None, r_t: [-763.146 -763.146 -763.146], eps: 0.1})
Step:  115300, Reward: [-396.505 -396.505 -396.505] [62.308], Avg: [-483.714 -483.714 -483.714] (0.0100) ({r_i: None, r_t: [-771.499 -771.499 -771.499], eps: 0.01})
Step:  199900, Reward: [-393.817 -393.817 -393.817] [80.836], Avg: [-443.950 -443.950 -443.950] (0.0100) ({r_i: None, r_t: [-832.140 -832.140 -832.140], eps: 0.01})
Step:   83000, Reward: [-374.678 -374.678 -374.678] [49.515], Avg: [-422.475 -422.475 -422.475] (0.1000) ({r_i: None, r_t: [-782.404 -782.404 -782.404], eps: 0.1})
Step:  115400, Reward: [-391.179 -391.179 -391.179] [76.362], Avg: [-483.634 -483.634 -483.634] (0.0100) ({r_i: None, r_t: [-774.684 -774.684 -774.684], eps: 0.01})
Step:  200000, Reward: [-389.044 -389.044 -389.044] [101.619], Avg: [-443.922 -443.922 -443.922] (0.0100) ({r_i: None, r_t: [-922.475 -922.475 -922.475], eps: 0.01})
Step:   83100, Reward: [-381.808 -381.808 -381.808] [61.738], Avg: [-422.426 -422.426 -422.426] (0.1000) ({r_i: None, r_t: [-741.439 -741.439 -741.439], eps: 0.1})
Step:  115500, Reward: [-376.355 -376.355 -376.355] [66.958], Avg: [-483.541 -483.541 -483.541] (0.0100) ({r_i: None, r_t: [-763.615 -763.615 -763.615], eps: 0.01})
Step:   83200, Reward: [-361.947 -361.947 -361.947] [49.989], Avg: [-422.354 -422.354 -422.354] (0.1000) ({r_i: None, r_t: [-773.194 -773.194 -773.194], eps: 0.1})
Step:  115600, Reward: [-381.920 -381.920 -381.920] [85.080], Avg: [-483.453 -483.453 -483.453] (0.0100) ({r_i: None, r_t: [-756.041 -756.041 -756.041], eps: 0.01})
Step:   83300, Reward: [-365.421 -365.421 -365.421] [55.919], Avg: [-422.285 -422.285 -422.285] (0.1000) ({r_i: None, r_t: [-762.570 -762.570 -762.570], eps: 0.1})
Step:  115700, Reward: [-368.888 -368.888 -368.888] [64.053], Avg: [-483.354 -483.354 -483.354] (0.0100) ({r_i: None, r_t: [-778.612 -778.612 -778.612], eps: 0.01})
Step:   83400, Reward: [-345.822 -345.822 -345.822] [35.489], Avg: [-422.194 -422.194 -422.194] (0.1000) ({r_i: None, r_t: [-739.293 -739.293 -739.293], eps: 0.1})
Step:  115800, Reward: [-379.777 -379.777 -379.777] [75.643], Avg: [-483.265 -483.265 -483.265] (0.0100) ({r_i: None, r_t: [-757.901 -757.901 -757.901], eps: 0.01})
Step:   83500, Reward: [-383.609 -383.609 -383.609] [51.048], Avg: [-422.148 -422.148 -422.148] (0.1000) ({r_i: None, r_t: [-746.754 -746.754 -746.754], eps: 0.1})
Step:  115900, Reward: [-369.670 -369.670 -369.670] [62.822], Avg: [-483.167 -483.167 -483.167] (0.0100) ({r_i: None, r_t: [-756.330 -756.330 -756.330], eps: 0.01})
Step:   83600, Reward: [-391.478 -391.478 -391.478] [70.980], Avg: [-422.111 -422.111 -422.111] (0.1000) ({r_i: None, r_t: [-782.979 -782.979 -782.979], eps: 0.1})
Step:  116000, Reward: [-396.586 -396.586 -396.586] [74.301], Avg: [-483.092 -483.092 -483.092] (0.0100) ({r_i: None, r_t: [-719.829 -719.829 -719.829], eps: 0.01})
Step:   83700, Reward: [-400.997 -400.997 -400.997] [69.980], Avg: [-422.086 -422.086 -422.086] (0.1000) ({r_i: None, r_t: [-796.224 -796.224 -796.224], eps: 0.1})
Step:  116100, Reward: [-366.068 -366.068 -366.068] [67.076], Avg: [-482.992 -482.992 -482.992] (0.0100) ({r_i: None, r_t: [-764.404 -764.404 -764.404], eps: 0.01})
Step:   83800, Reward: [-389.974 -389.974 -389.974] [56.276], Avg: [-422.048 -422.048 -422.048] (0.1000) ({r_i: None, r_t: [-770.002 -770.002 -770.002], eps: 0.1})
Step:  116200, Reward: [-397.346 -397.346 -397.346] [69.537], Avg: [-482.918 -482.918 -482.918] (0.0100) ({r_i: None, r_t: [-738.688 -738.688 -738.688], eps: 0.01})
Step:   83900, Reward: [-385.108 -385.108 -385.108] [47.121], Avg: [-422.004 -422.004 -422.004] (0.1000) ({r_i: None, r_t: [-777.305 -777.305 -777.305], eps: 0.1})
Step:  116300, Reward: [-379.625 -379.625 -379.625] [63.107], Avg: [-482.829 -482.829 -482.829] (0.0100) ({r_i: None, r_t: [-793.719 -793.719 -793.719], eps: 0.01})
Step:   84000, Reward: [-389.770 -389.770 -389.770] [69.014], Avg: [-421.965 -421.965 -421.965] (0.1000) ({r_i: None, r_t: [-780.811 -780.811 -780.811], eps: 0.1})
Step:  116400, Reward: [-427.671 -427.671 -427.671] [58.166], Avg: [-482.782 -482.782 -482.782] (0.0100) ({r_i: None, r_t: [-735.662 -735.662 -735.662], eps: 0.01})
Step:   84100, Reward: [-365.932 -365.932 -365.932] [64.911], Avg: [-421.899 -421.899 -421.899] (0.1000) ({r_i: None, r_t: [-747.953 -747.953 -747.953], eps: 0.1})
Step:  116500, Reward: [-375.333 -375.333 -375.333] [63.392], Avg: [-482.690 -482.690 -482.690] (0.0100) ({r_i: None, r_t: [-769.325 -769.325 -769.325], eps: 0.01})
Step:   84200, Reward: [-375.181 -375.181 -375.181] [54.001], Avg: [-421.843 -421.843 -421.843] (0.1000) ({r_i: None, r_t: [-782.690 -782.690 -782.690], eps: 0.1})
Step:  116600, Reward: [-417.992 -417.992 -417.992] [77.163], Avg: [-482.634 -482.634 -482.634] (0.0100) ({r_i: None, r_t: [-775.405 -775.405 -775.405], eps: 0.01})
Step:   84300, Reward: [-395.351 -395.351 -395.351] [50.298], Avg: [-421.812 -421.812 -421.812] (0.1000) ({r_i: None, r_t: [-783.128 -783.128 -783.128], eps: 0.1})
Step:  116700, Reward: [-392.738 -392.738 -392.738] [73.400], Avg: [-482.557 -482.557 -482.557] (0.0100) ({r_i: None, r_t: [-740.534 -740.534 -740.534], eps: 0.01})
Step:   84400, Reward: [-383.193 -383.193 -383.193] [58.485], Avg: [-421.766 -421.766 -421.766] (0.1000) ({r_i: None, r_t: [-783.007 -783.007 -783.007], eps: 0.1})
Step:  116800, Reward: [-373.683 -373.683 -373.683] [60.245], Avg: [-482.464 -482.464 -482.464] (0.0100) ({r_i: None, r_t: [-779.919 -779.919 -779.919], eps: 0.01})
Step:   84500, Reward: [-385.616 -385.616 -385.616] [62.992], Avg: [-421.724 -421.724 -421.724] (0.1000) ({r_i: None, r_t: [-763.138 -763.138 -763.138], eps: 0.1})
Step:  116900, Reward: [-411.974 -411.974 -411.974] [78.420], Avg: [-482.404 -482.404 -482.404] (0.0100) ({r_i: None, r_t: [-764.911 -764.911 -764.911], eps: 0.01})
Step:   84600, Reward: [-391.424 -391.424 -391.424] [58.119], Avg: [-421.688 -421.688 -421.688] (0.1000) ({r_i: None, r_t: [-785.657 -785.657 -785.657], eps: 0.1})
Step:  117000, Reward: [-379.097 -379.097 -379.097] [76.678], Avg: [-482.316 -482.316 -482.316] (0.0100) ({r_i: None, r_t: [-834.837 -834.837 -834.837], eps: 0.01})
Step:   84700, Reward: [-378.267 -378.267 -378.267] [51.545], Avg: [-421.637 -421.637 -421.637] (0.1000) ({r_i: None, r_t: [-753.204 -753.204 -753.204], eps: 0.1})
Step:  117100, Reward: [-408.718 -408.718 -408.718] [80.986], Avg: [-482.253 -482.253 -482.253] (0.0100) ({r_i: None, r_t: [-817.757 -817.757 -817.757], eps: 0.01})
Step:   84800, Reward: [-385.452 -385.452 -385.452] [63.645], Avg: [-421.594 -421.594 -421.594] (0.1000) ({r_i: None, r_t: [-814.288 -814.288 -814.288], eps: 0.1})
Step:  117200, Reward: [-399.546 -399.546 -399.546] [55.592], Avg: [-482.183 -482.183 -482.183] (0.0100) ({r_i: None, r_t: [-755.077 -755.077 -755.077], eps: 0.01})
Step:   84900, Reward: [-377.695 -377.695 -377.695] [50.185], Avg: [-421.542 -421.542 -421.542] (0.1000) ({r_i: None, r_t: [-728.776 -728.776 -728.776], eps: 0.1})
Step:  117300, Reward: [-410.492 -410.492 -410.492] [60.038], Avg: [-482.121 -482.121 -482.121] (0.0100) ({r_i: None, r_t: [-740.187 -740.187 -740.187], eps: 0.01})
Step:   85000, Reward: [-382.269 -382.269 -382.269] [56.197], Avg: [-421.496 -421.496 -421.496] (0.1000) ({r_i: None, r_t: [-735.836 -735.836 -735.836], eps: 0.1})
Step:  117400, Reward: [-362.560 -362.560 -362.560] [65.485], Avg: [-482.020 -482.020 -482.020] (0.0100) ({r_i: None, r_t: [-754.262 -754.262 -754.262], eps: 0.01})
Step:   85100, Reward: [-407.759 -407.759 -407.759] [62.400], Avg: [-421.480 -421.480 -421.480] (0.1000) ({r_i: None, r_t: [-772.086 -772.086 -772.086], eps: 0.1})
Step:  117500, Reward: [-389.223 -389.223 -389.223] [70.157], Avg: [-481.941 -481.941 -481.941] (0.0100) ({r_i: None, r_t: [-757.310 -757.310 -757.310], eps: 0.01})
Step:   85200, Reward: [-406.344 -406.344 -406.344] [72.373], Avg: [-421.462 -421.462 -421.462] (0.1000) ({r_i: None, r_t: [-743.960 -743.960 -743.960], eps: 0.1})
Step:  117600, Reward: [-397.984 -397.984 -397.984] [57.210], Avg: [-481.869 -481.869 -481.869] (0.0100) ({r_i: None, r_t: [-756.343 -756.343 -756.343], eps: 0.01})
Step:   85300, Reward: [-394.953 -394.953 -394.953] [58.076], Avg: [-421.431 -421.431 -421.431] (0.1000) ({r_i: None, r_t: [-736.862 -736.862 -736.862], eps: 0.1})
Step:  117700, Reward: [-367.530 -367.530 -367.530] [78.602], Avg: [-481.772 -481.772 -481.772] (0.0100) ({r_i: None, r_t: [-784.725 -784.725 -784.725], eps: 0.01})
Step:   85400, Reward: [-398.859 -398.859 -398.859] [63.960], Avg: [-421.405 -421.405 -421.405] (0.1000) ({r_i: None, r_t: [-781.585 -781.585 -781.585], eps: 0.1})
Step:  117800, Reward: [-370.661 -370.661 -370.661] [65.963], Avg: [-481.678 -481.678 -481.678] (0.0100) ({r_i: None, r_t: [-736.925 -736.925 -736.925], eps: 0.01})
Step:   85500, Reward: [-379.787 -379.787 -379.787] [77.351], Avg: [-421.356 -421.356 -421.356] (0.1000) ({r_i: None, r_t: [-773.772 -773.772 -773.772], eps: 0.1})
Step:  117900, Reward: [-382.477 -382.477 -382.477] [66.037], Avg: [-481.594 -481.594 -481.594] (0.0100) ({r_i: None, r_t: [-760.735 -760.735 -760.735], eps: 0.01})
Step:   85600, Reward: [-427.801 -427.801 -427.801] [41.539], Avg: [-421.364 -421.364 -421.364] (0.1000) ({r_i: None, r_t: [-781.445 -781.445 -781.445], eps: 0.1})
Step:  118000, Reward: [-389.863 -389.863 -389.863] [64.621], Avg: [-481.516 -481.516 -481.516] (0.0100) ({r_i: None, r_t: [-785.339 -785.339 -785.339], eps: 0.01})
Step:   85700, Reward: [-396.689 -396.689 -396.689] [70.581], Avg: [-421.335 -421.335 -421.335] (0.1000) ({r_i: None, r_t: [-805.722 -805.722 -805.722], eps: 0.1})
Step:  118100, Reward: [-390.411 -390.411 -390.411] [58.615], Avg: [-481.439 -481.439 -481.439] (0.0100) ({r_i: None, r_t: [-741.891 -741.891 -741.891], eps: 0.01})
Step:   85800, Reward: [-396.473 -396.473 -396.473] [78.591], Avg: [-421.306 -421.306 -421.306] (0.1000) ({r_i: None, r_t: [-739.867 -739.867 -739.867], eps: 0.1})
Step:  118200, Reward: [-407.710 -407.710 -407.710] [53.543], Avg: [-481.377 -481.377 -481.377] (0.0100) ({r_i: None, r_t: [-760.523 -760.523 -760.523], eps: 0.01})
Step:   85900, Reward: [-395.379 -395.379 -395.379] [67.158], Avg: [-421.276 -421.276 -421.276] (0.1000) ({r_i: None, r_t: [-777.750 -777.750 -777.750], eps: 0.1})
Step:  118300, Reward: [-398.256 -398.256 -398.256] [72.321], Avg: [-481.307 -481.307 -481.307] (0.0100) ({r_i: None, r_t: [-785.552 -785.552 -785.552], eps: 0.01})
Step:   86000, Reward: [-402.620 -402.620 -402.620] [58.936], Avg: [-421.254 -421.254 -421.254] (0.1000) ({r_i: None, r_t: [-776.236 -776.236 -776.236], eps: 0.1})
Step:  118400, Reward: [-366.538 -366.538 -366.538] [58.920], Avg: [-481.210 -481.210 -481.210] (0.0100) ({r_i: None, r_t: [-762.837 -762.837 -762.837], eps: 0.01})
Step:   86100, Reward: [-382.884 -382.884 -382.884] [63.771], Avg: [-421.210 -421.210 -421.210] (0.1000) ({r_i: None, r_t: [-754.196 -754.196 -754.196], eps: 0.1})
Step:  118500, Reward: [-380.961 -380.961 -380.961] [71.320], Avg: [-481.125 -481.125 -481.125] (0.0100) ({r_i: None, r_t: [-780.993 -780.993 -780.993], eps: 0.01})
Step:   86200, Reward: [-377.637 -377.637 -377.637] [68.925], Avg: [-421.159 -421.159 -421.159] (0.1000) ({r_i: None, r_t: [-762.202 -762.202 -762.202], eps: 0.1})
Step:  118600, Reward: [-375.739 -375.739 -375.739] [41.044], Avg: [-481.037 -481.037 -481.037] (0.0100) ({r_i: None, r_t: [-775.071 -775.071 -775.071], eps: 0.01})
Step:   86300, Reward: [-374.593 -374.593 -374.593] [60.630], Avg: [-421.105 -421.105 -421.105] (0.1000) ({r_i: None, r_t: [-762.167 -762.167 -762.167], eps: 0.1})
Step:  118700, Reward: [-387.141 -387.141 -387.141] [80.627], Avg: [-480.958 -480.958 -480.958] (0.0100) ({r_i: None, r_t: [-802.565 -802.565 -802.565], eps: 0.01})
Step:   86400, Reward: [-384.374 -384.374 -384.374] [63.933], Avg: [-421.063 -421.063 -421.063] (0.1000) ({r_i: None, r_t: [-777.161 -777.161 -777.161], eps: 0.1})
Step:  118800, Reward: [-384.290 -384.290 -384.290] [63.787], Avg: [-480.876 -480.876 -480.876] (0.0100) ({r_i: None, r_t: [-777.590 -777.590 -777.590], eps: 0.01})
Step:   86500, Reward: [-375.321 -375.321 -375.321] [74.004], Avg: [-421.010 -421.010 -421.010] (0.1000) ({r_i: None, r_t: [-763.772 -763.772 -763.772], eps: 0.1})
Step:  118900, Reward: [-400.735 -400.735 -400.735] [58.633], Avg: [-480.809 -480.809 -480.809] (0.0100) ({r_i: None, r_t: [-797.022 -797.022 -797.022], eps: 0.01})
Step:   86600, Reward: [-408.150 -408.150 -408.150] [51.782], Avg: [-420.995 -420.995 -420.995] (0.1000) ({r_i: None, r_t: [-764.421 -764.421 -764.421], eps: 0.1})
Step:  119000, Reward: [-398.412 -398.412 -398.412] [57.131], Avg: [-480.740 -480.740 -480.740] (0.0100) ({r_i: None, r_t: [-752.162 -752.162 -752.162], eps: 0.01})
Step:   86700, Reward: [-391.498 -391.498 -391.498] [63.248], Avg: [-420.961 -420.961 -420.961] (0.1000) ({r_i: None, r_t: [-758.035 -758.035 -758.035], eps: 0.1})
Step:  119100, Reward: [-379.119 -379.119 -379.119] [72.391], Avg: [-480.655 -480.655 -480.655] (0.0100) ({r_i: None, r_t: [-790.528 -790.528 -790.528], eps: 0.01})
Step:   86800, Reward: [-368.649 -368.649 -368.649] [51.754], Avg: [-420.901 -420.901 -420.901] (0.1000) ({r_i: None, r_t: [-756.229 -756.229 -756.229], eps: 0.1})
Step:  119200, Reward: [-414.112 -414.112 -414.112] [58.561], Avg: [-480.599 -480.599 -480.599] (0.0100) ({r_i: None, r_t: [-807.374 -807.374 -807.374], eps: 0.01})
Step:   86900, Reward: [-379.468 -379.468 -379.468] [56.547], Avg: [-420.853 -420.853 -420.853] (0.1000) ({r_i: None, r_t: [-767.067 -767.067 -767.067], eps: 0.1})
Step:  119300, Reward: [-423.287 -423.287 -423.287] [89.290], Avg: [-480.551 -480.551 -480.551] (0.0100) ({r_i: None, r_t: [-787.190 -787.190 -787.190], eps: 0.01})
Step:   87000, Reward: [-375.194 -375.194 -375.194] [38.291], Avg: [-420.801 -420.801 -420.801] (0.1000) ({r_i: None, r_t: [-764.372 -764.372 -764.372], eps: 0.1})
Step:  119400, Reward: [-388.616 -388.616 -388.616] [57.487], Avg: [-480.474 -480.474 -480.474] (0.0100) ({r_i: None, r_t: [-758.637 -758.637 -758.637], eps: 0.01})
Step:   87100, Reward: [-389.591 -389.591 -389.591] [69.012], Avg: [-420.765 -420.765 -420.765] (0.1000) ({r_i: None, r_t: [-721.181 -721.181 -721.181], eps: 0.1})
Step:  119500, Reward: [-418.937 -418.937 -418.937] [89.673], Avg: [-480.422 -480.422 -480.422] (0.0100) ({r_i: None, r_t: [-798.761 -798.761 -798.761], eps: 0.01})
Step:   87200, Reward: [-378.612 -378.612 -378.612] [41.607], Avg: [-420.717 -420.717 -420.717] (0.1000) ({r_i: None, r_t: [-787.522 -787.522 -787.522], eps: 0.1})
Step:  119600, Reward: [-391.386 -391.386 -391.386] [72.695], Avg: [-480.348 -480.348 -480.348] (0.0100) ({r_i: None, r_t: [-778.773 -778.773 -778.773], eps: 0.01})
Step:   87300, Reward: [-363.961 -363.961 -363.961] [66.978], Avg: [-420.652 -420.652 -420.652] (0.1000) ({r_i: None, r_t: [-808.826 -808.826 -808.826], eps: 0.1})
Step:  119700, Reward: [-378.321 -378.321 -378.321] [56.426], Avg: [-480.263 -480.263 -480.263] (0.0100) ({r_i: None, r_t: [-784.645 -784.645 -784.645], eps: 0.01})
Step:   87400, Reward: [-359.381 -359.381 -359.381] [44.089], Avg: [-420.582 -420.582 -420.582] (0.1000) ({r_i: None, r_t: [-724.172 -724.172 -724.172], eps: 0.1})
Step:  119800, Reward: [-407.016 -407.016 -407.016] [55.856], Avg: [-480.202 -480.202 -480.202] (0.0100) ({r_i: None, r_t: [-807.959 -807.959 -807.959], eps: 0.01})
Step:   87500, Reward: [-388.793 -388.793 -388.793] [55.301], Avg: [-420.546 -420.546 -420.546] (0.1000) ({r_i: None, r_t: [-755.504 -755.504 -755.504], eps: 0.1})
Step:  119900, Reward: [-416.001 -416.001 -416.001] [62.982], Avg: [-480.148 -480.148 -480.148] (0.0100) ({r_i: None, r_t: [-830.536 -830.536 -830.536], eps: 0.01})
Step:   87600, Reward: [-408.960 -408.960 -408.960] [61.891], Avg: [-420.532 -420.532 -420.532] (0.1000) ({r_i: None, r_t: [-779.840 -779.840 -779.840], eps: 0.1})
Step:  120000, Reward: [-396.888 -396.888 -396.888] [67.079], Avg: [-480.079 -480.079 -480.079] (0.0100) ({r_i: None, r_t: [-807.583 -807.583 -807.583], eps: 0.01})
Step:   87700, Reward: [-374.772 -374.772 -374.772] [52.229], Avg: [-420.480 -420.480 -420.480] (0.1000) ({r_i: None, r_t: [-781.574 -781.574 -781.574], eps: 0.1})
Step:  120100, Reward: [-381.704 -381.704 -381.704] [49.868], Avg: [-479.997 -479.997 -479.997] (0.0100) ({r_i: None, r_t: [-853.307 -853.307 -853.307], eps: 0.01})
Step:   87800, Reward: [-383.173 -383.173 -383.173] [66.869], Avg: [-420.438 -420.438 -420.438] (0.1000) ({r_i: None, r_t: [-775.946 -775.946 -775.946], eps: 0.1})
Step:  120200, Reward: [-406.773 -406.773 -406.773] [87.581], Avg: [-479.936 -479.936 -479.936] (0.0100) ({r_i: None, r_t: [-774.827 -774.827 -774.827], eps: 0.01})
Step:   87900, Reward: [-417.325 -417.325 -417.325] [53.332], Avg: [-420.434 -420.434 -420.434] (0.1000) ({r_i: None, r_t: [-741.979 -741.979 -741.979], eps: 0.1})
Step:  120300, Reward: [-388.010 -388.010 -388.010] [54.431], Avg: [-479.860 -479.860 -479.860] (0.0100) ({r_i: None, r_t: [-819.223 -819.223 -819.223], eps: 0.01})
Step:   88000, Reward: [-360.491 -360.491 -360.491] [56.859], Avg: [-420.366 -420.366 -420.366] (0.1000) ({r_i: None, r_t: [-759.223 -759.223 -759.223], eps: 0.1})
Step:  120400, Reward: [-389.346 -389.346 -389.346] [55.591], Avg: [-479.785 -479.785 -479.785] (0.0100) ({r_i: None, r_t: [-802.002 -802.002 -802.002], eps: 0.01})
Step:   88100, Reward: [-357.274 -357.274 -357.274] [64.540], Avg: [-420.295 -420.295 -420.295] (0.1000) ({r_i: None, r_t: [-769.630 -769.630 -769.630], eps: 0.1})
Step:  120500, Reward: [-372.986 -372.986 -372.986] [64.026], Avg: [-479.696 -479.696 -479.696] (0.0100) ({r_i: None, r_t: [-810.723 -810.723 -810.723], eps: 0.01})
Step:   88200, Reward: [-394.096 -394.096 -394.096] [56.475], Avg: [-420.265 -420.265 -420.265] (0.1000) ({r_i: None, r_t: [-773.364 -773.364 -773.364], eps: 0.1})
Step:  120600, Reward: [-412.562 -412.562 -412.562] [79.298], Avg: [-479.641 -479.641 -479.641] (0.0100) ({r_i: None, r_t: [-785.329 -785.329 -785.329], eps: 0.01})
Step:   88300, Reward: [-385.120 -385.120 -385.120] [56.982], Avg: [-420.225 -420.225 -420.225] (0.1000) ({r_i: None, r_t: [-737.388 -737.388 -737.388], eps: 0.1})
Step:  120700, Reward: [-414.387 -414.387 -414.387] [40.492], Avg: [-479.587 -479.587 -479.587] (0.0100) ({r_i: None, r_t: [-777.681 -777.681 -777.681], eps: 0.01})
Step:   88400, Reward: [-405.659 -405.659 -405.659] [77.002], Avg: [-420.209 -420.209 -420.209] (0.1000) ({r_i: None, r_t: [-749.773 -749.773 -749.773], eps: 0.1})
Step:  120800, Reward: [-418.458 -418.458 -418.458] [77.214], Avg: [-479.536 -479.536 -479.536] (0.0100) ({r_i: None, r_t: [-793.485 -793.485 -793.485], eps: 0.01})
Step:   88500, Reward: [-380.378 -380.378 -380.378] [55.431], Avg: [-420.164 -420.164 -420.164] (0.1000) ({r_i: None, r_t: [-780.648 -780.648 -780.648], eps: 0.1})
Step:  120900, Reward: [-373.876 -373.876 -373.876] [53.731], Avg: [-479.449 -479.449 -479.449] (0.0100) ({r_i: None, r_t: [-826.450 -826.450 -826.450], eps: 0.01})
Step:   88600, Reward: [-357.722 -357.722 -357.722] [52.836], Avg: [-420.094 -420.094 -420.094] (0.1000) ({r_i: None, r_t: [-788.505 -788.505 -788.505], eps: 0.1})
Step:  121000, Reward: [-418.925 -418.925 -418.925] [104.977], Avg: [-479.399 -479.399 -479.399] (0.0100) ({r_i: None, r_t: [-783.013 -783.013 -783.013], eps: 0.01})
Step:   88700, Reward: [-381.305 -381.305 -381.305] [72.180], Avg: [-420.050 -420.050 -420.050] (0.1000) ({r_i: None, r_t: [-814.464 -814.464 -814.464], eps: 0.1})
Step:  121100, Reward: [-402.488 -402.488 -402.488] [37.759], Avg: [-479.335 -479.335 -479.335] (0.0100) ({r_i: None, r_t: [-847.035 -847.035 -847.035], eps: 0.01})
Step:   88800, Reward: [-378.702 -378.702 -378.702] [71.773], Avg: [-420.003 -420.003 -420.003] (0.1000) ({r_i: None, r_t: [-757.217 -757.217 -757.217], eps: 0.1})
Step:  121200, Reward: [-394.516 -394.516 -394.516] [94.492], Avg: [-479.265 -479.265 -479.265] (0.0100) ({r_i: None, r_t: [-782.419 -782.419 -782.419], eps: 0.01})
Step:   88900, Reward: [-377.611 -377.611 -377.611] [63.952], Avg: [-419.956 -419.956 -419.956] (0.1000) ({r_i: None, r_t: [-752.324 -752.324 -752.324], eps: 0.1})
Step:  121300, Reward: [-382.631 -382.631 -382.631] [55.873], Avg: [-479.186 -479.186 -479.186] (0.0100) ({r_i: None, r_t: [-818.146 -818.146 -818.146], eps: 0.01})
Step:   89000, Reward: [-398.303 -398.303 -398.303] [93.901], Avg: [-419.931 -419.931 -419.931] (0.1000) ({r_i: None, r_t: [-751.182 -751.182 -751.182], eps: 0.1})
Step:  121400, Reward: [-389.562 -389.562 -389.562] [70.036], Avg: [-479.112 -479.112 -479.112] (0.0100) ({r_i: None, r_t: [-821.583 -821.583 -821.583], eps: 0.01})
Step:   89100, Reward: [-394.853 -394.853 -394.853] [80.143], Avg: [-419.903 -419.903 -419.903] (0.1000) ({r_i: None, r_t: [-776.293 -776.293 -776.293], eps: 0.1})
Step:  121500, Reward: [-403.744 -403.744 -403.744] [64.391], Avg: [-479.050 -479.050 -479.050] (0.0100) ({r_i: None, r_t: [-787.889 -787.889 -787.889], eps: 0.01})
Step:   89200, Reward: [-383.993 -383.993 -383.993] [52.810], Avg: [-419.863 -419.863 -419.863] (0.1000) ({r_i: None, r_t: [-766.164 -766.164 -766.164], eps: 0.1})
Step:  121600, Reward: [-419.697 -419.697 -419.697] [71.110], Avg: [-479.001 -479.001 -479.001] (0.0100) ({r_i: None, r_t: [-787.148 -787.148 -787.148], eps: 0.01})
Step:   89300, Reward: [-375.910 -375.910 -375.910] [59.349], Avg: [-419.814 -419.814 -419.814] (0.1000) ({r_i: None, r_t: [-758.630 -758.630 -758.630], eps: 0.1})
Step:  121700, Reward: [-396.219 -396.219 -396.219] [72.744], Avg: [-478.933 -478.933 -478.933] (0.0100) ({r_i: None, r_t: [-803.659 -803.659 -803.659], eps: 0.01})
Step:   89400, Reward: [-375.683 -375.683 -375.683] [73.071], Avg: [-419.765 -419.765 -419.765] (0.1000) ({r_i: None, r_t: [-769.631 -769.631 -769.631], eps: 0.1})
Step:  121800, Reward: [-415.739 -415.739 -415.739] [67.605], Avg: [-478.881 -478.881 -478.881] (0.0100) ({r_i: None, r_t: [-820.845 -820.845 -820.845], eps: 0.01})
Step:   89500, Reward: [-365.780 -365.780 -365.780] [32.009], Avg: [-419.704 -419.704 -419.704] (0.1000) ({r_i: None, r_t: [-788.330 -788.330 -788.330], eps: 0.1})
Step:  121900, Reward: [-377.191 -377.191 -377.191] [58.031], Avg: [-478.798 -478.798 -478.798] (0.0100) ({r_i: None, r_t: [-746.283 -746.283 -746.283], eps: 0.01})
Step:   89600, Reward: [-426.502 -426.502 -426.502] [65.734], Avg: [-419.712 -419.712 -419.712] (0.1000) ({r_i: None, r_t: [-765.037 -765.037 -765.037], eps: 0.1})
Step:  122000, Reward: [-429.999 -429.999 -429.999] [55.262], Avg: [-478.758 -478.758 -478.758] (0.0100) ({r_i: None, r_t: [-835.974 -835.974 -835.974], eps: 0.01})
Step:   89700, Reward: [-399.642 -399.642 -399.642] [56.202], Avg: [-419.690 -419.690 -419.690] (0.1000) ({r_i: None, r_t: [-765.913 -765.913 -765.913], eps: 0.1})
Step:  122100, Reward: [-373.436 -373.436 -373.436] [54.329], Avg: [-478.672 -478.672 -478.672] (0.0100) ({r_i: None, r_t: [-772.854 -772.854 -772.854], eps: 0.01})
Step:   89800, Reward: [-392.660 -392.660 -392.660] [46.483], Avg: [-419.660 -419.660 -419.660] (0.1000) ({r_i: None, r_t: [-816.300 -816.300 -816.300], eps: 0.1})
Step:  122200, Reward: [-363.073 -363.073 -363.073] [81.186], Avg: [-478.577 -478.577 -478.577] (0.0100) ({r_i: None, r_t: [-772.776 -772.776 -772.776], eps: 0.01})
Step:   89900, Reward: [-384.528 -384.528 -384.528] [58.249], Avg: [-419.620 -419.620 -419.620] (0.1000) ({r_i: None, r_t: [-774.551 -774.551 -774.551], eps: 0.1})
Step:  122300, Reward: [-394.680 -394.680 -394.680] [68.996], Avg: [-478.509 -478.509 -478.509] (0.0100) ({r_i: None, r_t: [-771.175 -771.175 -771.175], eps: 0.01})
Step:   90000, Reward: [-389.908 -389.908 -389.908] [85.880], Avg: [-419.587 -419.587 -419.587] (0.1000) ({r_i: None, r_t: [-727.783 -727.783 -727.783], eps: 0.1})
Step:  122400, Reward: [-402.148 -402.148 -402.148] [61.889], Avg: [-478.446 -478.446 -478.446] (0.0100) ({r_i: None, r_t: [-728.557 -728.557 -728.557], eps: 0.01})
Step:   90100, Reward: [-390.942 -390.942 -390.942] [51.597], Avg: [-419.556 -419.556 -419.556] (0.1000) ({r_i: None, r_t: [-768.506 -768.506 -768.506], eps: 0.1})
Step:  122500, Reward: [-419.644 -419.644 -419.644] [77.974], Avg: [-478.399 -478.399 -478.399] (0.0100) ({r_i: None, r_t: [-748.433 -748.433 -748.433], eps: 0.01})
Step:   90200, Reward: [-392.457 -392.457 -392.457] [68.232], Avg: [-419.526 -419.526 -419.526] (0.1000) ({r_i: None, r_t: [-778.129 -778.129 -778.129], eps: 0.1})
Step:  122600, Reward: [-393.027 -393.027 -393.027] [92.143], Avg: [-478.329 -478.329 -478.329] (0.0100) ({r_i: None, r_t: [-772.392 -772.392 -772.392], eps: 0.01})
Step:   90300, Reward: [-372.212 -372.212 -372.212] [54.479], Avg: [-419.473 -419.473 -419.473] (0.1000) ({r_i: None, r_t: [-755.566 -755.566 -755.566], eps: 0.1})
Step:  122700, Reward: [-403.941 -403.941 -403.941] [83.761], Avg: [-478.268 -478.268 -478.268] (0.0100) ({r_i: None, r_t: [-767.938 -767.938 -767.938], eps: 0.01})
Step:   90400, Reward: [-391.894 -391.894 -391.894] [49.179], Avg: [-419.443 -419.443 -419.443] (0.1000) ({r_i: None, r_t: [-787.295 -787.295 -787.295], eps: 0.1})
Step:  122800, Reward: [-397.135 -397.135 -397.135] [71.001], Avg: [-478.202 -478.202 -478.202] (0.0100) ({r_i: None, r_t: [-762.285 -762.285 -762.285], eps: 0.01})
Step:   90500, Reward: [-398.097 -398.097 -398.097] [58.663], Avg: [-419.419 -419.419 -419.419] (0.1000) ({r_i: None, r_t: [-803.601 -803.601 -803.601], eps: 0.1})
Step:  122900, Reward: [-407.959 -407.959 -407.959] [80.445], Avg: [-478.145 -478.145 -478.145] (0.0100) ({r_i: None, r_t: [-761.598 -761.598 -761.598], eps: 0.01})
Step:   90600, Reward: [-420.218 -420.218 -420.218] [47.945], Avg: [-419.420 -419.420 -419.420] (0.1000) ({r_i: None, r_t: [-777.895 -777.895 -777.895], eps: 0.1})
Step:  123000, Reward: [-375.109 -375.109 -375.109] [74.740], Avg: [-478.062 -478.062 -478.062] (0.0100) ({r_i: None, r_t: [-817.703 -817.703 -817.703], eps: 0.01})
Step:   90700, Reward: [-392.424 -392.424 -392.424] [58.925], Avg: [-419.391 -419.391 -419.391] (0.1000) ({r_i: None, r_t: [-767.318 -767.318 -767.318], eps: 0.1})
Step:  123100, Reward: [-369.900 -369.900 -369.900] [68.018], Avg: [-477.974 -477.974 -477.974] (0.0100) ({r_i: None, r_t: [-772.835 -772.835 -772.835], eps: 0.01})
Step:   90800, Reward: [-405.770 -405.770 -405.770] [84.984], Avg: [-419.376 -419.376 -419.376] (0.1000) ({r_i: None, r_t: [-814.361 -814.361 -814.361], eps: 0.1})
Step:  123200, Reward: [-378.409 -378.409 -378.409] [56.698], Avg: [-477.893 -477.893 -477.893] (0.0100) ({r_i: None, r_t: [-800.292 -800.292 -800.292], eps: 0.01})
Step:   90900, Reward: [-406.945 -406.945 -406.945] [65.794], Avg: [-419.362 -419.362 -419.362] (0.1000) ({r_i: None, r_t: [-827.437 -827.437 -827.437], eps: 0.1})
Step:  123300, Reward: [-394.924 -394.924 -394.924] [70.914], Avg: [-477.826 -477.826 -477.826] (0.0100) ({r_i: None, r_t: [-738.354 -738.354 -738.354], eps: 0.01})
Step:   91000, Reward: [-390.965 -390.965 -390.965] [71.984], Avg: [-419.331 -419.331 -419.331] (0.1000) ({r_i: None, r_t: [-746.786 -746.786 -746.786], eps: 0.1})
Step:  123400, Reward: [-368.143 -368.143 -368.143] [73.171], Avg: [-477.737 -477.737 -477.737] (0.0100) ({r_i: None, r_t: [-771.561 -771.561 -771.561], eps: 0.01})
Step:   91100, Reward: [-389.366 -389.366 -389.366] [61.419], Avg: [-419.298 -419.298 -419.298] (0.1000) ({r_i: None, r_t: [-788.463 -788.463 -788.463], eps: 0.1})
Step:  123500, Reward: [-406.481 -406.481 -406.481] [61.959], Avg: [-477.679 -477.679 -477.679] (0.0100) ({r_i: None, r_t: [-803.166 -803.166 -803.166], eps: 0.01})
Step:   91200, Reward: [-408.552 -408.552 -408.552] [91.599], Avg: [-419.286 -419.286 -419.286] (0.1000) ({r_i: None, r_t: [-779.565 -779.565 -779.565], eps: 0.1})
Step:  123600, Reward: [-395.601 -395.601 -395.601] [79.642], Avg: [-477.613 -477.613 -477.613] (0.0100) ({r_i: None, r_t: [-811.238 -811.238 -811.238], eps: 0.01})
Step:   91300, Reward: [-441.709 -441.709 -441.709] [75.921], Avg: [-419.311 -419.311 -419.311] (0.1000) ({r_i: None, r_t: [-782.105 -782.105 -782.105], eps: 0.1})
Step:  123700, Reward: [-413.933 -413.933 -413.933] [80.453], Avg: [-477.562 -477.562 -477.562] (0.0100) ({r_i: None, r_t: [-739.524 -739.524 -739.524], eps: 0.01})
Step:   91400, Reward: [-402.672 -402.672 -402.672] [56.392], Avg: [-419.292 -419.292 -419.292] (0.1000) ({r_i: None, r_t: [-777.214 -777.214 -777.214], eps: 0.1})
Step:  123800, Reward: [-411.449 -411.449 -411.449] [75.103], Avg: [-477.508 -477.508 -477.508] (0.0100) ({r_i: None, r_t: [-776.724 -776.724 -776.724], eps: 0.01})
Step:   91500, Reward: [-424.512 -424.512 -424.512] [78.597], Avg: [-419.298 -419.298 -419.298] (0.1000) ({r_i: None, r_t: [-820.822 -820.822 -820.822], eps: 0.1})
Step:  123900, Reward: [-348.600 -348.600 -348.600] [55.501], Avg: [-477.404 -477.404 -477.404] (0.0100) ({r_i: None, r_t: [-768.402 -768.402 -768.402], eps: 0.01})
Step:   91600, Reward: [-432.406 -432.406 -432.406] [49.774], Avg: [-419.312 -419.312 -419.312] (0.1000) ({r_i: None, r_t: [-814.476 -814.476 -814.476], eps: 0.1})
Step:  124000, Reward: [-375.414 -375.414 -375.414] [62.697], Avg: [-477.322 -477.322 -477.322] (0.0100) ({r_i: None, r_t: [-786.427 -786.427 -786.427], eps: 0.01})
Step:   91700, Reward: [-389.986 -389.986 -389.986] [51.032], Avg: [-419.280 -419.280 -419.280] (0.1000) ({r_i: None, r_t: [-837.318 -837.318 -837.318], eps: 0.1})
Step:  124100, Reward: [-396.253 -396.253 -396.253] [60.602], Avg: [-477.257 -477.257 -477.257] (0.0100) ({r_i: None, r_t: [-789.560 -789.560 -789.560], eps: 0.01})
Step:   91800, Reward: [-404.562 -404.562 -404.562] [72.738], Avg: [-419.264 -419.264 -419.264] (0.1000) ({r_i: None, r_t: [-781.802 -781.802 -781.802], eps: 0.1})
Step:  124200, Reward: [-357.583 -357.583 -357.583] [61.448], Avg: [-477.160 -477.160 -477.160] (0.0100) ({r_i: None, r_t: [-771.525 -771.525 -771.525], eps: 0.01})
Step:   91900, Reward: [-433.399 -433.399 -433.399] [97.212], Avg: [-419.280 -419.280 -419.280] (0.1000) ({r_i: None, r_t: [-797.372 -797.372 -797.372], eps: 0.1})
Step:  124300, Reward: [-393.910 -393.910 -393.910] [68.012], Avg: [-477.094 -477.094 -477.094] (0.0100) ({r_i: None, r_t: [-791.425 -791.425 -791.425], eps: 0.01})
Step:   92000, Reward: [-413.828 -413.828 -413.828] [100.868], Avg: [-419.274 -419.274 -419.274] (0.1000) ({r_i: None, r_t: [-781.384 -781.384 -781.384], eps: 0.1})
Step:  124400, Reward: [-368.528 -368.528 -368.528] [58.060], Avg: [-477.006 -477.006 -477.006] (0.0100) ({r_i: None, r_t: [-759.008 -759.008 -759.008], eps: 0.01})
Step:   92100, Reward: [-383.732 -383.732 -383.732] [69.447], Avg: [-419.235 -419.235 -419.235] (0.1000) ({r_i: None, r_t: [-811.149 -811.149 -811.149], eps: 0.1})
Step:  124500, Reward: [-395.727 -395.727 -395.727] [77.305], Avg: [-476.941 -476.941 -476.941] (0.0100) ({r_i: None, r_t: [-772.765 -772.765 -772.765], eps: 0.01})
Step:   92200, Reward: [-406.860 -406.860 -406.860] [60.523], Avg: [-419.222 -419.222 -419.222] (0.1000) ({r_i: None, r_t: [-828.391 -828.391 -828.391], eps: 0.1})
Step:  124600, Reward: [-397.697 -397.697 -397.697] [64.404], Avg: [-476.878 -476.878 -476.878] (0.0100) ({r_i: None, r_t: [-752.319 -752.319 -752.319], eps: 0.01})
Step:   92300, Reward: [-397.192 -397.192 -397.192] [87.700], Avg: [-419.198 -419.198 -419.198] (0.1000) ({r_i: None, r_t: [-780.859 -780.859 -780.859], eps: 0.1})
Step:  124700, Reward: [-382.371 -382.371 -382.371] [56.299], Avg: [-476.802 -476.802 -476.802] (0.0100) ({r_i: None, r_t: [-783.827 -783.827 -783.827], eps: 0.01})
Step:   92400, Reward: [-425.053 -425.053 -425.053] [117.648], Avg: [-419.204 -419.204 -419.204] (0.1000) ({r_i: None, r_t: [-852.189 -852.189 -852.189], eps: 0.1})
Step:  124800, Reward: [-419.923 -419.923 -419.923] [65.922], Avg: [-476.756 -476.756 -476.756] (0.0100) ({r_i: None, r_t: [-802.599 -802.599 -802.599], eps: 0.01})
Step:   92500, Reward: [-385.199 -385.199 -385.199] [63.327], Avg: [-419.168 -419.168 -419.168] (0.1000) ({r_i: None, r_t: [-790.478 -790.478 -790.478], eps: 0.1})
Step:  124900, Reward: [-393.748 -393.748 -393.748] [64.251], Avg: [-476.690 -476.690 -476.690] (0.0100) ({r_i: None, r_t: [-762.946 -762.946 -762.946], eps: 0.01})
Step:   92600, Reward: [-421.093 -421.093 -421.093] [48.487], Avg: [-419.170 -419.170 -419.170] (0.1000) ({r_i: None, r_t: [-777.177 -777.177 -777.177], eps: 0.1})
Step:  125000, Reward: [-358.139 -358.139 -358.139] [65.973], Avg: [-476.595 -476.595 -476.595] (0.0100) ({r_i: None, r_t: [-789.532 -789.532 -789.532], eps: 0.01})
Step:   92700, Reward: [-387.605 -387.605 -387.605] [73.943], Avg: [-419.136 -419.136 -419.136] (0.1000) ({r_i: None, r_t: [-782.847 -782.847 -782.847], eps: 0.1})
Step:  125100, Reward: [-396.828 -396.828 -396.828] [72.646], Avg: [-476.531 -476.531 -476.531] (0.0100) ({r_i: None, r_t: [-779.587 -779.587 -779.587], eps: 0.01})
Step:   92800, Reward: [-405.006 -405.006 -405.006] [57.914], Avg: [-419.121 -419.121 -419.121] (0.1000) ({r_i: None, r_t: [-773.830 -773.830 -773.830], eps: 0.1})
Step:  125200, Reward: [-379.087 -379.087 -379.087] [62.777], Avg: [-476.454 -476.454 -476.454] (0.0100) ({r_i: None, r_t: [-801.649 -801.649 -801.649], eps: 0.01})
Step:   92900, Reward: [-396.851 -396.851 -396.851] [60.132], Avg: [-419.097 -419.097 -419.097] (0.1000) ({r_i: None, r_t: [-838.273 -838.273 -838.273], eps: 0.1})
Step:  125300, Reward: [-391.717 -391.717 -391.717] [71.237], Avg: [-476.386 -476.386 -476.386] (0.0100) ({r_i: None, r_t: [-777.874 -777.874 -777.874], eps: 0.01})
Step:   93000, Reward: [-381.994 -381.994 -381.994] [61.520], Avg: [-419.057 -419.057 -419.057] (0.1000) ({r_i: None, r_t: [-775.626 -775.626 -775.626], eps: 0.1})
Step:  125400, Reward: [-363.511 -363.511 -363.511] [50.826], Avg: [-476.296 -476.296 -476.296] (0.0100) ({r_i: None, r_t: [-832.014 -832.014 -832.014], eps: 0.01})
Step:   93100, Reward: [-424.694 -424.694 -424.694] [78.229], Avg: [-419.063 -419.063 -419.063] (0.1000) ({r_i: None, r_t: [-807.701 -807.701 -807.701], eps: 0.1})
Step:  125500, Reward: [-410.165 -410.165 -410.165] [76.332], Avg: [-476.243 -476.243 -476.243] (0.0100) ({r_i: None, r_t: [-824.481 -824.481 -824.481], eps: 0.01})
Step:   93200, Reward: [-406.981 -406.981 -406.981] [68.463], Avg: [-419.050 -419.050 -419.050] (0.1000) ({r_i: None, r_t: [-772.943 -772.943 -772.943], eps: 0.1})
Step:  125600, Reward: [-397.694 -397.694 -397.694] [46.979], Avg: [-476.181 -476.181 -476.181] (0.0100) ({r_i: None, r_t: [-746.201 -746.201 -746.201], eps: 0.01})
Step:   93300, Reward: [-412.942 -412.942 -412.942] [72.106], Avg: [-419.043 -419.043 -419.043] (0.1000) ({r_i: None, r_t: [-759.880 -759.880 -759.880], eps: 0.1})
Step:  125700, Reward: [-376.577 -376.577 -376.577] [64.343], Avg: [-476.102 -476.102 -476.102] (0.0100) ({r_i: None, r_t: [-773.681 -773.681 -773.681], eps: 0.01})
Step:   93400, Reward: [-388.795 -388.795 -388.795] [42.806], Avg: [-419.011 -419.011 -419.011] (0.1000) ({r_i: None, r_t: [-760.803 -760.803 -760.803], eps: 0.1})
Step:  125800, Reward: [-397.157 -397.157 -397.157] [81.815], Avg: [-476.039 -476.039 -476.039] (0.0100) ({r_i: None, r_t: [-779.604 -779.604 -779.604], eps: 0.01})
Step:   93500, Reward: [-401.473 -401.473 -401.473] [71.946], Avg: [-418.992 -418.992 -418.992] (0.1000) ({r_i: None, r_t: [-767.188 -767.188 -767.188], eps: 0.1})
Step:  125900, Reward: [-367.965 -367.965 -367.965] [56.833], Avg: [-475.953 -475.953 -475.953] (0.0100) ({r_i: None, r_t: [-843.807 -843.807 -843.807], eps: 0.01})
Step:   93600, Reward: [-392.823 -392.823 -392.823] [66.597], Avg: [-418.964 -418.964 -418.964] (0.1000) ({r_i: None, r_t: [-770.816 -770.816 -770.816], eps: 0.1})
Step:  126000, Reward: [-409.279 -409.279 -409.279] [79.621], Avg: [-475.900 -475.900 -475.900] (0.0100) ({r_i: None, r_t: [-772.863 -772.863 -772.863], eps: 0.01})
Step:   93700, Reward: [-416.989 -416.989 -416.989] [84.858], Avg: [-418.962 -418.962 -418.962] (0.1000) ({r_i: None, r_t: [-761.243 -761.243 -761.243], eps: 0.1})
Step:  126100, Reward: [-374.948 -374.948 -374.948] [77.800], Avg: [-475.820 -475.820 -475.820] (0.0100) ({r_i: None, r_t: [-814.044 -814.044 -814.044], eps: 0.01})
Step:   93800, Reward: [-384.515 -384.515 -384.515] [63.666], Avg: [-418.926 -418.926 -418.926] (0.1000) ({r_i: None, r_t: [-756.742 -756.742 -756.742], eps: 0.1})
Step:  126200, Reward: [-362.679 -362.679 -362.679] [66.106], Avg: [-475.731 -475.731 -475.731] (0.0100) ({r_i: None, r_t: [-812.680 -812.680 -812.680], eps: 0.01})
Step:   93900, Reward: [-370.220 -370.220 -370.220] [56.341], Avg: [-418.874 -418.874 -418.874] (0.1000) ({r_i: None, r_t: [-769.969 -769.969 -769.969], eps: 0.1})
Step:  126300, Reward: [-358.256 -358.256 -358.256] [86.163], Avg: [-475.638 -475.638 -475.638] (0.0100) ({r_i: None, r_t: [-812.969 -812.969 -812.969], eps: 0.01})
Step:   94000, Reward: [-385.789 -385.789 -385.789] [64.409], Avg: [-418.839 -418.839 -418.839] (0.1000) ({r_i: None, r_t: [-722.711 -722.711 -722.711], eps: 0.1})
Step:  126400, Reward: [-437.911 -437.911 -437.911] [87.182], Avg: [-475.608 -475.608 -475.608] (0.0100) ({r_i: None, r_t: [-819.643 -819.643 -819.643], eps: 0.01})
Step:   94100, Reward: [-365.383 -365.383 -365.383] [55.517], Avg: [-418.782 -418.782 -418.782] (0.1000) ({r_i: None, r_t: [-833.778 -833.778 -833.778], eps: 0.1})
Step:  126500, Reward: [-410.704 -410.704 -410.704] [49.918], Avg: [-475.557 -475.557 -475.557] (0.0100) ({r_i: None, r_t: [-820.706 -820.706 -820.706], eps: 0.01})
Step:   94200, Reward: [-379.825 -379.825 -379.825] [58.366], Avg: [-418.740 -418.740 -418.740] (0.1000) ({r_i: None, r_t: [-799.760 -799.760 -799.760], eps: 0.1})
Step:  126600, Reward: [-414.385 -414.385 -414.385] [80.476], Avg: [-475.509 -475.509 -475.509] (0.0100) ({r_i: None, r_t: [-750.863 -750.863 -750.863], eps: 0.01})
Step:   94300, Reward: [-382.623 -382.623 -382.623] [72.333], Avg: [-418.702 -418.702 -418.702] (0.1000) ({r_i: None, r_t: [-751.836 -751.836 -751.836], eps: 0.1})
Step:  126700, Reward: [-403.172 -403.172 -403.172] [52.594], Avg: [-475.452 -475.452 -475.452] (0.0100) ({r_i: None, r_t: [-759.429 -759.429 -759.429], eps: 0.01})
Step:   94400, Reward: [-398.960 -398.960 -398.960] [77.325], Avg: [-418.681 -418.681 -418.681] (0.1000) ({r_i: None, r_t: [-783.966 -783.966 -783.966], eps: 0.1})
Step:  126800, Reward: [-424.076 -424.076 -424.076] [58.985], Avg: [-475.411 -475.411 -475.411] (0.0100) ({r_i: None, r_t: [-769.826 -769.826 -769.826], eps: 0.01})
Step:   94500, Reward: [-381.692 -381.692 -381.692] [57.343], Avg: [-418.642 -418.642 -418.642] (0.1000) ({r_i: None, r_t: [-805.974 -805.974 -805.974], eps: 0.1})
Step:  126900, Reward: [-405.549 -405.549 -405.549] [65.489], Avg: [-475.356 -475.356 -475.356] (0.0100) ({r_i: None, r_t: [-810.805 -810.805 -810.805], eps: 0.01})
Step:   94600, Reward: [-379.606 -379.606 -379.606] [63.558], Avg: [-418.601 -418.601 -418.601] (0.1000) ({r_i: None, r_t: [-735.719 -735.719 -735.719], eps: 0.1})
Step:  127000, Reward: [-393.204 -393.204 -393.204] [74.302], Avg: [-475.291 -475.291 -475.291] (0.0100) ({r_i: None, r_t: [-781.561 -781.561 -781.561], eps: 0.01})
Step:   94700, Reward: [-345.106 -345.106 -345.106] [58.904], Avg: [-418.523 -418.523 -418.523] (0.1000) ({r_i: None, r_t: [-748.596 -748.596 -748.596], eps: 0.1})
Step:  127100, Reward: [-375.557 -375.557 -375.557] [92.948], Avg: [-475.213 -475.213 -475.213] (0.0100) ({r_i: None, r_t: [-815.529 -815.529 -815.529], eps: 0.01})
Step:   94800, Reward: [-391.317 -391.317 -391.317] [51.158], Avg: [-418.495 -418.495 -418.495] (0.1000) ({r_i: None, r_t: [-786.435 -786.435 -786.435], eps: 0.1})
Step:  127200, Reward: [-425.543 -425.543 -425.543] [65.724], Avg: [-475.174 -475.174 -475.174] (0.0100) ({r_i: None, r_t: [-823.838 -823.838 -823.838], eps: 0.01})
Step:   94900, Reward: [-363.861 -363.861 -363.861] [42.154], Avg: [-418.437 -418.437 -418.437] (0.1000) ({r_i: None, r_t: [-795.626 -795.626 -795.626], eps: 0.1})
Step:  127300, Reward: [-367.811 -367.811 -367.811] [66.553], Avg: [-475.090 -475.090 -475.090] (0.0100) ({r_i: None, r_t: [-787.376 -787.376 -787.376], eps: 0.01})
Step:   95000, Reward: [-382.391 -382.391 -382.391] [40.884], Avg: [-418.399 -418.399 -418.399] (0.1000) ({r_i: None, r_t: [-762.468 -762.468 -762.468], eps: 0.1})
Step:  127400, Reward: [-374.691 -374.691 -374.691] [62.129], Avg: [-475.011 -475.011 -475.011] (0.0100) ({r_i: None, r_t: [-829.571 -829.571 -829.571], eps: 0.01})
Step:   95100, Reward: [-330.633 -330.633 -330.633] [56.932], Avg: [-418.307 -418.307 -418.307] (0.1000) ({r_i: None, r_t: [-745.358 -745.358 -745.358], eps: 0.1})
Step:  127500, Reward: [-403.011 -403.011 -403.011] [48.046], Avg: [-474.955 -474.955 -474.955] (0.0100) ({r_i: None, r_t: [-824.894 -824.894 -824.894], eps: 0.01})
Step:   95200, Reward: [-404.095 -404.095 -404.095] [55.165], Avg: [-418.292 -418.292 -418.292] (0.1000) ({r_i: None, r_t: [-758.295 -758.295 -758.295], eps: 0.1})
Step:  127600, Reward: [-396.152 -396.152 -396.152] [89.051], Avg: [-474.893 -474.893 -474.893] (0.0100) ({r_i: None, r_t: [-773.848 -773.848 -773.848], eps: 0.01})
Step:   95300, Reward: [-386.437 -386.437 -386.437] [54.081], Avg: [-418.259 -418.259 -418.259] (0.1000) ({r_i: None, r_t: [-752.087 -752.087 -752.087], eps: 0.1})
Step:  127700, Reward: [-411.163 -411.163 -411.163] [101.546], Avg: [-474.843 -474.843 -474.843] (0.0100) ({r_i: None, r_t: [-812.783 -812.783 -812.783], eps: 0.01})
Step:   95400, Reward: [-384.861 -384.861 -384.861] [48.357], Avg: [-418.224 -418.224 -418.224] (0.1000) ({r_i: None, r_t: [-804.394 -804.394 -804.394], eps: 0.1})
Step:  127800, Reward: [-421.246 -421.246 -421.246] [60.367], Avg: [-474.801 -474.801 -474.801] (0.0100) ({r_i: None, r_t: [-805.828 -805.828 -805.828], eps: 0.01})
Step:   95500, Reward: [-377.156 -377.156 -377.156] [54.431], Avg: [-418.181 -418.181 -418.181] (0.1000) ({r_i: None, r_t: [-727.187 -727.187 -727.187], eps: 0.1})
Step:  127900, Reward: [-382.018 -382.018 -382.018] [65.482], Avg: [-474.729 -474.729 -474.729] (0.0100) ({r_i: None, r_t: [-760.279 -760.279 -760.279], eps: 0.01})
Step:   95600, Reward: [-378.975 -378.975 -378.975] [56.663], Avg: [-418.140 -418.140 -418.140] (0.1000) ({r_i: None, r_t: [-750.665 -750.665 -750.665], eps: 0.1})
Step:  128000, Reward: [-399.653 -399.653 -399.653] [59.448], Avg: [-474.670 -474.670 -474.670] (0.0100) ({r_i: None, r_t: [-763.994 -763.994 -763.994], eps: 0.01})
Step:   95700, Reward: [-382.784 -382.784 -382.784] [63.882], Avg: [-418.103 -418.103 -418.103] (0.1000) ({r_i: None, r_t: [-731.794 -731.794 -731.794], eps: 0.1})
Step:  128100, Reward: [-368.957 -368.957 -368.957] [65.458], Avg: [-474.587 -474.587 -474.587] (0.0100) ({r_i: None, r_t: [-823.084 -823.084 -823.084], eps: 0.01})
Step:   95800, Reward: [-390.915 -390.915 -390.915] [63.519], Avg: [-418.075 -418.075 -418.075] (0.1000) ({r_i: None, r_t: [-698.890 -698.890 -698.890], eps: 0.1})
Step:  128200, Reward: [-406.485 -406.485 -406.485] [77.426], Avg: [-474.534 -474.534 -474.534] (0.0100) ({r_i: None, r_t: [-792.536 -792.536 -792.536], eps: 0.01})
Step:   95900, Reward: [-367.516 -367.516 -367.516] [65.712], Avg: [-418.022 -418.022 -418.022] (0.1000) ({r_i: None, r_t: [-761.324 -761.324 -761.324], eps: 0.1})
Step:  128300, Reward: [-424.161 -424.161 -424.161] [61.934], Avg: [-474.495 -474.495 -474.495] (0.0100) ({r_i: None, r_t: [-780.174 -780.174 -780.174], eps: 0.01})
Step:   96000, Reward: [-387.841 -387.841 -387.841] [60.834], Avg: [-417.991 -417.991 -417.991] (0.1000) ({r_i: None, r_t: [-765.429 -765.429 -765.429], eps: 0.1})
Step:  128400, Reward: [-402.990 -402.990 -402.990] [50.767], Avg: [-474.440 -474.440 -474.440] (0.0100) ({r_i: None, r_t: [-782.611 -782.611 -782.611], eps: 0.01})
Step:   96100, Reward: [-373.385 -373.385 -373.385] [41.194], Avg: [-417.944 -417.944 -417.944] (0.1000) ({r_i: None, r_t: [-749.955 -749.955 -749.955], eps: 0.1})
Step:  128500, Reward: [-392.419 -392.419 -392.419] [80.474], Avg: [-474.376 -474.376 -474.376] (0.0100) ({r_i: None, r_t: [-802.497 -802.497 -802.497], eps: 0.01})
Step:   96200, Reward: [-372.926 -372.926 -372.926] [62.094], Avg: [-417.898 -417.898 -417.898] (0.1000) ({r_i: None, r_t: [-733.700 -733.700 -733.700], eps: 0.1})
Step:  128600, Reward: [-409.123 -409.123 -409.123] [66.495], Avg: [-474.325 -474.325 -474.325] (0.0100) ({r_i: None, r_t: [-794.908 -794.908 -794.908], eps: 0.01})
Step:   96300, Reward: [-388.119 -388.119 -388.119] [70.963], Avg: [-417.867 -417.867 -417.867] (0.1000) ({r_i: None, r_t: [-798.581 -798.581 -798.581], eps: 0.1})
Step:  128700, Reward: [-377.595 -377.595 -377.595] [61.498], Avg: [-474.250 -474.250 -474.250] (0.0100) ({r_i: None, r_t: [-792.747 -792.747 -792.747], eps: 0.01})
Step:   96400, Reward: [-348.745 -348.745 -348.745] [46.077], Avg: [-417.795 -417.795 -417.795] (0.1000) ({r_i: None, r_t: [-769.456 -769.456 -769.456], eps: 0.1})
Step:  128800, Reward: [-384.431 -384.431 -384.431] [73.682], Avg: [-474.180 -474.180 -474.180] (0.0100) ({r_i: None, r_t: [-814.010 -814.010 -814.010], eps: 0.01})
Step:   96500, Reward: [-370.525 -370.525 -370.525] [40.798], Avg: [-417.746 -417.746 -417.746] (0.1000) ({r_i: None, r_t: [-766.332 -766.332 -766.332], eps: 0.1})
Step:  128900, Reward: [-425.937 -425.937 -425.937] [96.430], Avg: [-474.143 -474.143 -474.143] (0.0100) ({r_i: None, r_t: [-814.705 -814.705 -814.705], eps: 0.01})
Step:   96600, Reward: [-377.978 -377.978 -377.978] [51.474], Avg: [-417.705 -417.705 -417.705] (0.1000) ({r_i: None, r_t: [-728.891 -728.891 -728.891], eps: 0.1})
Step:  129000, Reward: [-388.559 -388.559 -388.559] [79.547], Avg: [-474.077 -474.077 -474.077] (0.0100) ({r_i: None, r_t: [-806.779 -806.779 -806.779], eps: 0.01})
Step:   96700, Reward: [-394.838 -394.838 -394.838] [59.434], Avg: [-417.681 -417.681 -417.681] (0.1000) ({r_i: None, r_t: [-769.106 -769.106 -769.106], eps: 0.1})
Step:  129100, Reward: [-374.020 -374.020 -374.020] [73.260], Avg: [-473.999 -473.999 -473.999] (0.0100) ({r_i: None, r_t: [-787.949 -787.949 -787.949], eps: 0.01})
Step:   96800, Reward: [-369.831 -369.831 -369.831] [73.642], Avg: [-417.632 -417.632 -417.632] (0.1000) ({r_i: None, r_t: [-756.422 -756.422 -756.422], eps: 0.1})
Step:  129200, Reward: [-415.936 -415.936 -415.936] [55.265], Avg: [-473.954 -473.954 -473.954] (0.0100) ({r_i: None, r_t: [-789.402 -789.402 -789.402], eps: 0.01})
Step:   96900, Reward: [-390.672 -390.672 -390.672] [76.813], Avg: [-417.604 -417.604 -417.604] (0.1000) ({r_i: None, r_t: [-763.495 -763.495 -763.495], eps: 0.1})
Step:  129300, Reward: [-373.782 -373.782 -373.782] [68.153], Avg: [-473.877 -473.877 -473.877] (0.0100) ({r_i: None, r_t: [-789.900 -789.900 -789.900], eps: 0.01})
Step:   97000, Reward: [-374.303 -374.303 -374.303] [56.449], Avg: [-417.560 -417.560 -417.560] (0.1000) ({r_i: None, r_t: [-740.228 -740.228 -740.228], eps: 0.1})
Step:  129400, Reward: [-389.059 -389.059 -389.059] [66.289], Avg: [-473.811 -473.811 -473.811] (0.0100) ({r_i: None, r_t: [-790.540 -790.540 -790.540], eps: 0.01})
Step:   97100, Reward: [-380.619 -380.619 -380.619] [68.997], Avg: [-417.522 -417.522 -417.522] (0.1000) ({r_i: None, r_t: [-769.259 -769.259 -769.259], eps: 0.1})
Step:  129500, Reward: [-409.954 -409.954 -409.954] [57.923], Avg: [-473.762 -473.762 -473.762] (0.0100) ({r_i: None, r_t: [-819.892 -819.892 -819.892], eps: 0.01})
Step:   97200, Reward: [-359.351 -359.351 -359.351] [57.491], Avg: [-417.462 -417.462 -417.462] (0.1000) ({r_i: None, r_t: [-758.428 -758.428 -758.428], eps: 0.1})
Step:  129600, Reward: [-396.937 -396.937 -396.937] [71.457], Avg: [-473.703 -473.703 -473.703] (0.0100) ({r_i: None, r_t: [-785.418 -785.418 -785.418], eps: 0.01})
Step:   97300, Reward: [-381.997 -381.997 -381.997] [52.348], Avg: [-417.425 -417.425 -417.425] (0.1000) ({r_i: None, r_t: [-761.605 -761.605 -761.605], eps: 0.1})
Step:  129700, Reward: [-379.926 -379.926 -379.926] [62.746], Avg: [-473.631 -473.631 -473.631] (0.0100) ({r_i: None, r_t: [-817.230 -817.230 -817.230], eps: 0.01})
Step:   97400, Reward: [-372.756 -372.756 -372.756] [70.094], Avg: [-417.380 -417.380 -417.380] (0.1000) ({r_i: None, r_t: [-712.018 -712.018 -712.018], eps: 0.1})
Step:  129800, Reward: [-410.038 -410.038 -410.038] [55.811], Avg: [-473.582 -473.582 -473.582] (0.0100) ({r_i: None, r_t: [-797.346 -797.346 -797.346], eps: 0.01})
Step:   97500, Reward: [-386.886 -386.886 -386.886] [70.576], Avg: [-417.348 -417.348 -417.348] (0.1000) ({r_i: None, r_t: [-744.668 -744.668 -744.668], eps: 0.1})
Step:  129900, Reward: [-373.075 -373.075 -373.075] [71.365], Avg: [-473.504 -473.504 -473.504] (0.0100) ({r_i: None, r_t: [-854.424 -854.424 -854.424], eps: 0.01})
Step:   97600, Reward: [-380.464 -380.464 -380.464] [55.065], Avg: [-417.311 -417.311 -417.311] (0.1000) ({r_i: None, r_t: [-757.425 -757.425 -757.425], eps: 0.1})
Step:  130000, Reward: [-453.887 -453.887 -453.887] [89.846], Avg: [-473.489 -473.489 -473.489] (0.0100) ({r_i: None, r_t: [-789.105 -789.105 -789.105], eps: 0.01})
Step:   97700, Reward: [-391.757 -391.757 -391.757] [66.572], Avg: [-417.284 -417.284 -417.284] (0.1000) ({r_i: None, r_t: [-770.517 -770.517 -770.517], eps: 0.1})
Step:  130100, Reward: [-359.543 -359.543 -359.543] [97.945], Avg: [-473.402 -473.402 -473.402] (0.0100) ({r_i: None, r_t: [-814.557 -814.557 -814.557], eps: 0.01})
Step:   97800, Reward: [-374.836 -374.836 -374.836] [65.481], Avg: [-417.241 -417.241 -417.241] (0.1000) ({r_i: None, r_t: [-734.538 -734.538 -734.538], eps: 0.1})
Step:  130200, Reward: [-388.560 -388.560 -388.560] [77.266], Avg: [-473.337 -473.337 -473.337] (0.0100) ({r_i: None, r_t: [-819.610 -819.610 -819.610], eps: 0.01})
Step:   97900, Reward: [-355.141 -355.141 -355.141] [44.476], Avg: [-417.178 -417.178 -417.178] (0.1000) ({r_i: None, r_t: [-768.796 -768.796 -768.796], eps: 0.1})
Step:  130300, Reward: [-426.966 -426.966 -426.966] [76.327], Avg: [-473.301 -473.301 -473.301] (0.0100) ({r_i: None, r_t: [-822.453 -822.453 -822.453], eps: 0.01})
Step:   98000, Reward: [-385.264 -385.264 -385.264] [51.733], Avg: [-417.145 -417.145 -417.145] (0.1000) ({r_i: None, r_t: [-733.885 -733.885 -733.885], eps: 0.1})
Step:  130400, Reward: [-418.033 -418.033 -418.033] [63.004], Avg: [-473.259 -473.259 -473.259] (0.0100) ({r_i: None, r_t: [-824.802 -824.802 -824.802], eps: 0.01})
Step:   98100, Reward: [-377.877 -377.877 -377.877] [52.614], Avg: [-417.105 -417.105 -417.105] (0.1000) ({r_i: None, r_t: [-714.263 -714.263 -714.263], eps: 0.1})
Step:  130500, Reward: [-408.541 -408.541 -408.541] [88.329], Avg: [-473.209 -473.209 -473.209] (0.0100) ({r_i: None, r_t: [-774.217 -774.217 -774.217], eps: 0.01})
Step:   98200, Reward: [-378.229 -378.229 -378.229] [52.565], Avg: [-417.066 -417.066 -417.066] (0.1000) ({r_i: None, r_t: [-777.039 -777.039 -777.039], eps: 0.1})
Step:  130600, Reward: [-383.211 -383.211 -383.211] [56.369], Avg: [-473.140 -473.140 -473.140] (0.0100) ({r_i: None, r_t: [-789.564 -789.564 -789.564], eps: 0.01})
Step:   98300, Reward: [-386.502 -386.502 -386.502] [62.734], Avg: [-417.035 -417.035 -417.035] (0.1000) ({r_i: None, r_t: [-786.846 -786.846 -786.846], eps: 0.1})
Step:  130700, Reward: [-396.580 -396.580 -396.580] [83.151], Avg: [-473.082 -473.082 -473.082] (0.0100) ({r_i: None, r_t: [-817.115 -817.115 -817.115], eps: 0.01})
Step:   98400, Reward: [-374.255 -374.255 -374.255] [59.725], Avg: [-416.991 -416.991 -416.991] (0.1000) ({r_i: None, r_t: [-796.461 -796.461 -796.461], eps: 0.1})
Step:  130800, Reward: [-438.341 -438.341 -438.341] [78.258], Avg: [-473.055 -473.055 -473.055] (0.0100) ({r_i: None, r_t: [-806.627 -806.627 -806.627], eps: 0.01})
Step:   98500, Reward: [-382.210 -382.210 -382.210] [60.764], Avg: [-416.956 -416.956 -416.956] (0.1000) ({r_i: None, r_t: [-737.664 -737.664 -737.664], eps: 0.1})
Step:  130900, Reward: [-394.900 -394.900 -394.900] [77.121], Avg: [-472.996 -472.996 -472.996] (0.0100) ({r_i: None, r_t: [-795.946 -795.946 -795.946], eps: 0.01})
Step:   98600, Reward: [-397.925 -397.925 -397.925] [62.289], Avg: [-416.937 -416.937 -416.937] (0.1000) ({r_i: None, r_t: [-776.339 -776.339 -776.339], eps: 0.1})
Step:  131000, Reward: [-384.531 -384.531 -384.531] [58.498], Avg: [-472.928 -472.928 -472.928] (0.0100) ({r_i: None, r_t: [-816.091 -816.091 -816.091], eps: 0.01})
Step:   98700, Reward: [-369.974 -369.974 -369.974] [57.638], Avg: [-416.889 -416.889 -416.889] (0.1000) ({r_i: None, r_t: [-739.687 -739.687 -739.687], eps: 0.1})
Step:  131100, Reward: [-382.336 -382.336 -382.336] [79.112], Avg: [-472.859 -472.859 -472.859] (0.0100) ({r_i: None, r_t: [-801.216 -801.216 -801.216], eps: 0.01})
Step:   98800, Reward: [-355.394 -355.394 -355.394] [55.227], Avg: [-416.827 -416.827 -416.827] (0.1000) ({r_i: None, r_t: [-724.771 -724.771 -724.771], eps: 0.1})
Step:  131200, Reward: [-411.828 -411.828 -411.828] [93.585], Avg: [-472.813 -472.813 -472.813] (0.0100) ({r_i: None, r_t: [-830.288 -830.288 -830.288], eps: 0.01})
Step:   98900, Reward: [-370.601 -370.601 -370.601] [60.489], Avg: [-416.780 -416.780 -416.780] (0.1000) ({r_i: None, r_t: [-749.080 -749.080 -749.080], eps: 0.1})
Step:  131300, Reward: [-417.056 -417.056 -417.056] [89.180], Avg: [-472.770 -472.770 -472.770] (0.0100) ({r_i: None, r_t: [-792.866 -792.866 -792.866], eps: 0.01})
Step:   99000, Reward: [-375.501 -375.501 -375.501] [50.723], Avg: [-416.739 -416.739 -416.739] (0.1000) ({r_i: None, r_t: [-770.465 -770.465 -770.465], eps: 0.1})
Step:  131400, Reward: [-446.055 -446.055 -446.055] [77.804], Avg: [-472.750 -472.750 -472.750] (0.0100) ({r_i: None, r_t: [-804.995 -804.995 -804.995], eps: 0.01})
Step:   99100, Reward: [-393.723 -393.723 -393.723] [45.596], Avg: [-416.715 -416.715 -416.715] (0.1000) ({r_i: None, r_t: [-749.124 -749.124 -749.124], eps: 0.1})
Step:  131500, Reward: [-419.021 -419.021 -419.021] [59.911], Avg: [-472.709 -472.709 -472.709] (0.0100) ({r_i: None, r_t: [-819.525 -819.525 -819.525], eps: 0.01})
Step:   99200, Reward: [-378.046 -378.046 -378.046] [47.793], Avg: [-416.676 -416.676 -416.676] (0.1000) ({r_i: None, r_t: [-779.752 -779.752 -779.752], eps: 0.1})
Step:  131600, Reward: [-386.039 -386.039 -386.039] [58.951], Avg: [-472.643 -472.643 -472.643] (0.0100) ({r_i: None, r_t: [-790.245 -790.245 -790.245], eps: 0.01})
Step:   99300, Reward: [-386.200 -386.200 -386.200] [68.635], Avg: [-416.646 -416.646 -416.646] (0.1000) ({r_i: None, r_t: [-768.888 -768.888 -768.888], eps: 0.1})
Step:  131700, Reward: [-411.831 -411.831 -411.831] [71.317], Avg: [-472.597 -472.597 -472.597] (0.0100) ({r_i: None, r_t: [-787.817 -787.817 -787.817], eps: 0.01})
Step:   99400, Reward: [-385.238 -385.238 -385.238] [56.896], Avg: [-416.614 -416.614 -416.614] (0.1000) ({r_i: None, r_t: [-735.762 -735.762 -735.762], eps: 0.1})
Step:  131800, Reward: [-408.760 -408.760 -408.760] [87.220], Avg: [-472.549 -472.549 -472.549] (0.0100) ({r_i: None, r_t: [-791.322 -791.322 -791.322], eps: 0.01})
Step:   99500, Reward: [-362.882 -362.882 -362.882] [47.910], Avg: [-416.560 -416.560 -416.560] (0.1000) ({r_i: None, r_t: [-746.690 -746.690 -746.690], eps: 0.1})
Step:  131900, Reward: [-398.621 -398.621 -398.621] [51.421], Avg: [-472.493 -472.493 -472.493] (0.0100) ({r_i: None, r_t: [-819.335 -819.335 -819.335], eps: 0.01})
Step:   99600, Reward: [-372.055 -372.055 -372.055] [64.144], Avg: [-416.516 -416.516 -416.516] (0.1000) ({r_i: None, r_t: [-728.346 -728.346 -728.346], eps: 0.1})
Step:  132000, Reward: [-369.013 -369.013 -369.013] [76.712], Avg: [-472.414 -472.414 -472.414] (0.0100) ({r_i: None, r_t: [-839.256 -839.256 -839.256], eps: 0.01})
Step:   99700, Reward: [-377.911 -377.911 -377.911] [47.084], Avg: [-416.477 -416.477 -416.477] (0.1000) ({r_i: None, r_t: [-746.873 -746.873 -746.873], eps: 0.1})
Step:  132100, Reward: [-392.848 -392.848 -392.848] [68.749], Avg: [-472.354 -472.354 -472.354] (0.0100) ({r_i: None, r_t: [-831.234 -831.234 -831.234], eps: 0.01})
Step:   99800, Reward: [-388.382 -388.382 -388.382] [72.751], Avg: [-416.449 -416.449 -416.449] (0.1000) ({r_i: None, r_t: [-767.981 -767.981 -767.981], eps: 0.1})
Step:  132200, Reward: [-388.938 -388.938 -388.938] [56.553], Avg: [-472.291 -472.291 -472.291] (0.0100) ({r_i: None, r_t: [-780.660 -780.660 -780.660], eps: 0.01})
Step:   99900, Reward: [-352.741 -352.741 -352.741] [62.021], Avg: [-416.385 -416.385 -416.385] (0.1000) ({r_i: None, r_t: [-709.809 -709.809 -709.809], eps: 0.1})
Step:  132300, Reward: [-390.832 -390.832 -390.832] [91.467], Avg: [-472.229 -472.229 -472.229] (0.0100) ({r_i: None, r_t: [-832.093 -832.093 -832.093], eps: 0.01})
Step:  100000, Reward: [-336.193 -336.193 -336.193] [62.065], Avg: [-416.305 -416.305 -416.305] (0.1000) ({r_i: None, r_t: [-764.065 -764.065 -764.065], eps: 0.1})
Step:  132400, Reward: [-407.190 -407.190 -407.190] [84.153], Avg: [-472.180 -472.180 -472.180] (0.0100) ({r_i: None, r_t: [-784.172 -784.172 -784.172], eps: 0.01})
Step:  100100, Reward: [-371.790 -371.790 -371.790] [58.759], Avg: [-416.261 -416.261 -416.261] (0.1000) ({r_i: None, r_t: [-758.759 -758.759 -758.759], eps: 0.1})
Step:  132500, Reward: [-416.639 -416.639 -416.639] [72.418], Avg: [-472.139 -472.139 -472.139] (0.0100) ({r_i: None, r_t: [-790.558 -790.558 -790.558], eps: 0.01})
Step:  100200, Reward: [-371.415 -371.415 -371.415] [65.791], Avg: [-416.216 -416.216 -416.216] (0.1000) ({r_i: None, r_t: [-755.895 -755.895 -755.895], eps: 0.1})
Step:  132600, Reward: [-425.559 -425.559 -425.559] [96.113], Avg: [-472.103 -472.103 -472.103] (0.0100) ({r_i: None, r_t: [-808.199 -808.199 -808.199], eps: 0.01})
Step:  100300, Reward: [-399.857 -399.857 -399.857] [74.553], Avg: [-416.200 -416.200 -416.200] (0.1000) ({r_i: None, r_t: [-743.032 -743.032 -743.032], eps: 0.1})
Step:  132700, Reward: [-374.590 -374.590 -374.590] [48.155], Avg: [-472.030 -472.030 -472.030] (0.0100) ({r_i: None, r_t: [-812.606 -812.606 -812.606], eps: 0.01})
Step:  100400, Reward: [-394.517 -394.517 -394.517] [50.006], Avg: [-416.178 -416.178 -416.178] (0.1000) ({r_i: None, r_t: [-765.090 -765.090 -765.090], eps: 0.1})
Step:  132800, Reward: [-415.145 -415.145 -415.145] [88.635], Avg: [-471.987 -471.987 -471.987] (0.0100) ({r_i: None, r_t: [-799.545 -799.545 -799.545], eps: 0.01})
Step:  100500, Reward: [-355.709 -355.709 -355.709] [61.944], Avg: [-416.118 -416.118 -416.118] (0.1000) ({r_i: None, r_t: [-776.010 -776.010 -776.010], eps: 0.1})
Step:  132900, Reward: [-408.285 -408.285 -408.285] [87.622], Avg: [-471.939 -471.939 -471.939] (0.0100) ({r_i: None, r_t: [-808.633 -808.633 -808.633], eps: 0.01})
Step:  100600, Reward: [-394.725 -394.725 -394.725] [57.018], Avg: [-416.097 -416.097 -416.097] (0.1000) ({r_i: None, r_t: [-771.265 -771.265 -771.265], eps: 0.1})
Step:  133000, Reward: [-400.259 -400.259 -400.259] [85.725], Avg: [-471.885 -471.885 -471.885] (0.0100) ({r_i: None, r_t: [-848.208 -848.208 -848.208], eps: 0.01})
Step:  100700, Reward: [-386.983 -386.983 -386.983] [67.171], Avg: [-416.068 -416.068 -416.068] (0.1000) ({r_i: None, r_t: [-765.944 -765.944 -765.944], eps: 0.1})
Step:  133100, Reward: [-415.201 -415.201 -415.201] [64.549], Avg: [-471.843 -471.843 -471.843] (0.0100) ({r_i: None, r_t: [-827.642 -827.642 -827.642], eps: 0.01})
Step:  100800, Reward: [-377.561 -377.561 -377.561] [50.169], Avg: [-416.030 -416.030 -416.030] (0.1000) ({r_i: None, r_t: [-769.275 -769.275 -769.275], eps: 0.1})
Step:  133200, Reward: [-421.091 -421.091 -421.091] [80.126], Avg: [-471.805 -471.805 -471.805] (0.0100) ({r_i: None, r_t: [-792.992 -792.992 -792.992], eps: 0.01})
Step:  100900, Reward: [-352.579 -352.579 -352.579] [57.488], Avg: [-415.967 -415.967 -415.967] (0.1000) ({r_i: None, r_t: [-775.448 -775.448 -775.448], eps: 0.1})
Step:  133300, Reward: [-398.537 -398.537 -398.537] [79.116], Avg: [-471.750 -471.750 -471.750] (0.0100) ({r_i: None, r_t: [-794.405 -794.405 -794.405], eps: 0.01})
Step:  101000, Reward: [-390.884 -390.884 -390.884] [75.295], Avg: [-415.942 -415.942 -415.942] (0.1000) ({r_i: None, r_t: [-743.229 -743.229 -743.229], eps: 0.1})
Step:  133400, Reward: [-428.236 -428.236 -428.236] [72.147], Avg: [-471.717 -471.717 -471.717] (0.0100) ({r_i: None, r_t: [-779.929 -779.929 -779.929], eps: 0.01})
Step:  101100, Reward: [-393.035 -393.035 -393.035] [76.063], Avg: [-415.919 -415.919 -415.919] (0.1000) ({r_i: None, r_t: [-736.167 -736.167 -736.167], eps: 0.1})
Step:  133500, Reward: [-453.866 -453.866 -453.866] [109.327], Avg: [-471.704 -471.704 -471.704] (0.0100) ({r_i: None, r_t: [-773.151 -773.151 -773.151], eps: 0.01})
Step:  101200, Reward: [-396.037 -396.037 -396.037] [63.844], Avg: [-415.900 -415.900 -415.900] (0.1000) ({r_i: None, r_t: [-739.035 -739.035 -739.035], eps: 0.1})
Step:  133600, Reward: [-383.870 -383.870 -383.870] [74.964], Avg: [-471.638 -471.638 -471.638] (0.0100) ({r_i: None, r_t: [-792.552 -792.552 -792.552], eps: 0.01})
Step:  101300, Reward: [-371.536 -371.536 -371.536] [60.054], Avg: [-415.856 -415.856 -415.856] (0.1000) ({r_i: None, r_t: [-760.972 -760.972 -760.972], eps: 0.1})
Step:  133700, Reward: [-447.609 -447.609 -447.609] [94.663], Avg: [-471.620 -471.620 -471.620] (0.0100) ({r_i: None, r_t: [-853.884 -853.884 -853.884], eps: 0.01})
Step:  101400, Reward: [-400.844 -400.844 -400.844] [66.317], Avg: [-415.841 -415.841 -415.841] (0.1000) ({r_i: None, r_t: [-790.219 -790.219 -790.219], eps: 0.1})
Step:  133800, Reward: [-440.404 -440.404 -440.404] [62.497], Avg: [-471.597 -471.597 -471.597] (0.0100) ({r_i: None, r_t: [-796.018 -796.018 -796.018], eps: 0.01})
Step:  101500, Reward: [-370.928 -370.928 -370.928] [59.725], Avg: [-415.797 -415.797 -415.797] (0.1000) ({r_i: None, r_t: [-791.131 -791.131 -791.131], eps: 0.1})
Step:  133900, Reward: [-404.061 -404.061 -404.061] [83.163], Avg: [-471.547 -471.547 -471.547] (0.0100) ({r_i: None, r_t: [-837.715 -837.715 -837.715], eps: 0.01})
Step:  101600, Reward: [-376.912 -376.912 -376.912] [69.447], Avg: [-415.759 -415.759 -415.759] (0.1000) ({r_i: None, r_t: [-754.888 -754.888 -754.888], eps: 0.1})
Step:  134000, Reward: [-426.429 -426.429 -426.429] [66.507], Avg: [-471.513 -471.513 -471.513] (0.0100) ({r_i: None, r_t: [-834.035 -834.035 -834.035], eps: 0.01})
Step:  101700, Reward: [-400.786 -400.786 -400.786] [63.925], Avg: [-415.744 -415.744 -415.744] (0.1000) ({r_i: None, r_t: [-771.312 -771.312 -771.312], eps: 0.1})
Step:  134100, Reward: [-387.029 -387.029 -387.029] [61.000], Avg: [-471.450 -471.450 -471.450] (0.0100) ({r_i: None, r_t: [-796.219 -796.219 -796.219], eps: 0.01})
Step:  101800, Reward: [-396.470 -396.470 -396.470] [56.979], Avg: [-415.725 -415.725 -415.725] (0.1000) ({r_i: None, r_t: [-787.181 -787.181 -787.181], eps: 0.1})
Step:  134200, Reward: [-366.870 -366.870 -366.870] [81.859], Avg: [-471.372 -471.372 -471.372] (0.0100) ({r_i: None, r_t: [-825.770 -825.770 -825.770], eps: 0.01})
Step:  101900, Reward: [-383.512 -383.512 -383.512] [50.507], Avg: [-415.693 -415.693 -415.693] (0.1000) ({r_i: None, r_t: [-742.064 -742.064 -742.064], eps: 0.1})
Step:  134300, Reward: [-391.288 -391.288 -391.288] [69.567], Avg: [-471.313 -471.313 -471.313] (0.0100) ({r_i: None, r_t: [-763.175 -763.175 -763.175], eps: 0.01})
Step:  102000, Reward: [-401.287 -401.287 -401.287] [76.632], Avg: [-415.679 -415.679 -415.679] (0.1000) ({r_i: None, r_t: [-737.744 -737.744 -737.744], eps: 0.1})
Step:  134400, Reward: [-418.716 -418.716 -418.716] [60.991], Avg: [-471.273 -471.273 -471.273] (0.0100) ({r_i: None, r_t: [-769.305 -769.305 -769.305], eps: 0.01})
Step:  102100, Reward: [-376.984 -376.984 -376.984] [46.028], Avg: [-415.641 -415.641 -415.641] (0.1000) ({r_i: None, r_t: [-823.850 -823.850 -823.850], eps: 0.1})
Step:  134500, Reward: [-374.750 -374.750 -374.750] [67.017], Avg: [-471.202 -471.202 -471.202] (0.0100) ({r_i: None, r_t: [-837.591 -837.591 -837.591], eps: 0.01})
Step:  102200, Reward: [-387.778 -387.778 -387.778] [55.968], Avg: [-415.614 -415.614 -415.614] (0.1000) ({r_i: None, r_t: [-750.804 -750.804 -750.804], eps: 0.1})
Step:  134600, Reward: [-381.184 -381.184 -381.184] [52.087], Avg: [-471.135 -471.135 -471.135] (0.0100) ({r_i: None, r_t: [-789.664 -789.664 -789.664], eps: 0.01})
Step:  102300, Reward: [-397.459 -397.459 -397.459] [47.977], Avg: [-415.597 -415.597 -415.597] (0.1000) ({r_i: None, r_t: [-777.323 -777.323 -777.323], eps: 0.1})
Step:  134700, Reward: [-405.299 -405.299 -405.299] [65.832], Avg: [-471.086 -471.086 -471.086] (0.0100) ({r_i: None, r_t: [-788.550 -788.550 -788.550], eps: 0.01})
Step:  102400, Reward: [-382.445 -382.445 -382.445] [60.650], Avg: [-415.564 -415.564 -415.564] (0.1000) ({r_i: None, r_t: [-772.179 -772.179 -772.179], eps: 0.1})
Step:  134800, Reward: [-419.670 -419.670 -419.670] [113.904], Avg: [-471.048 -471.048 -471.048] (0.0100) ({r_i: None, r_t: [-826.892 -826.892 -826.892], eps: 0.01})
Step:  102500, Reward: [-382.455 -382.455 -382.455] [33.689], Avg: [-415.532 -415.532 -415.532] (0.1000) ({r_i: None, r_t: [-754.455 -754.455 -754.455], eps: 0.1})
Step:  134900, Reward: [-419.301 -419.301 -419.301] [54.861], Avg: [-471.010 -471.010 -471.010] (0.0100) ({r_i: None, r_t: [-829.671 -829.671 -829.671], eps: 0.01})
Step:  102600, Reward: [-390.264 -390.264 -390.264] [63.120], Avg: [-415.507 -415.507 -415.507] (0.1000) ({r_i: None, r_t: [-768.849 -768.849 -768.849], eps: 0.1})
Step:  135000, Reward: [-384.294 -384.294 -384.294] [45.328], Avg: [-470.945 -470.945 -470.945] (0.0100) ({r_i: None, r_t: [-796.357 -796.357 -796.357], eps: 0.01})
Step:  102700, Reward: [-408.798 -408.798 -408.798] [46.306], Avg: [-415.501 -415.501 -415.501] (0.1000) ({r_i: None, r_t: [-767.682 -767.682 -767.682], eps: 0.1})
Step:  135100, Reward: [-398.449 -398.449 -398.449] [72.124], Avg: [-470.892 -470.892 -470.892] (0.0100) ({r_i: None, r_t: [-780.521 -780.521 -780.521], eps: 0.01})
Step:  102800, Reward: [-387.672 -387.672 -387.672] [44.242], Avg: [-415.474 -415.474 -415.474] (0.1000) ({r_i: None, r_t: [-774.082 -774.082 -774.082], eps: 0.1})
Step:  135200, Reward: [-412.407 -412.407 -412.407] [73.246], Avg: [-470.849 -470.849 -470.849] (0.0100) ({r_i: None, r_t: [-835.901 -835.901 -835.901], eps: 0.01})
Step:  102900, Reward: [-408.387 -408.387 -408.387] [70.556], Avg: [-415.467 -415.467 -415.467] (0.1000) ({r_i: None, r_t: [-787.344 -787.344 -787.344], eps: 0.1})
Step:  135300, Reward: [-415.471 -415.471 -415.471] [96.073], Avg: [-470.808 -470.808 -470.808] (0.0100) ({r_i: None, r_t: [-826.555 -826.555 -826.555], eps: 0.01})
Step:  103000, Reward: [-399.192 -399.192 -399.192] [60.233], Avg: [-415.451 -415.451 -415.451] (0.1000) ({r_i: None, r_t: [-769.464 -769.464 -769.464], eps: 0.1})
Step:  135400, Reward: [-410.410 -410.410 -410.410] [59.285], Avg: [-470.763 -470.763 -470.763] (0.0100) ({r_i: None, r_t: [-779.108 -779.108 -779.108], eps: 0.01})
Step:  103100, Reward: [-404.710 -404.710 -404.710] [64.287], Avg: [-415.441 -415.441 -415.441] (0.1000) ({r_i: None, r_t: [-816.262 -816.262 -816.262], eps: 0.1})
Step:  135500, Reward: [-452.258 -452.258 -452.258] [126.567], Avg: [-470.749 -470.749 -470.749] (0.0100) ({r_i: None, r_t: [-801.807 -801.807 -801.807], eps: 0.01})
Step:  103200, Reward: [-387.608 -387.608 -387.608] [65.656], Avg: [-415.414 -415.414 -415.414] (0.1000) ({r_i: None, r_t: [-762.950 -762.950 -762.950], eps: 0.1})
Step:  135600, Reward: [-401.517 -401.517 -401.517] [53.656], Avg: [-470.698 -470.698 -470.698] (0.0100) ({r_i: None, r_t: [-786.354 -786.354 -786.354], eps: 0.01})
Step:  103300, Reward: [-384.056 -384.056 -384.056] [55.628], Avg: [-415.383 -415.383 -415.383] (0.1000) ({r_i: None, r_t: [-760.838 -760.838 -760.838], eps: 0.1})
Step:  135700, Reward: [-420.729 -420.729 -420.729] [88.545], Avg: [-470.662 -470.662 -470.662] (0.0100) ({r_i: None, r_t: [-788.455 -788.455 -788.455], eps: 0.01})
Step:  103400, Reward: [-404.179 -404.179 -404.179] [58.583], Avg: [-415.373 -415.373 -415.373] (0.1000) ({r_i: None, r_t: [-788.204 -788.204 -788.204], eps: 0.1})
Step:  135800, Reward: [-403.758 -403.758 -403.758] [67.713], Avg: [-470.612 -470.612 -470.612] (0.0100) ({r_i: None, r_t: [-785.389 -785.389 -785.389], eps: 0.01})
Step:  103500, Reward: [-370.246 -370.246 -370.246] [63.091], Avg: [-415.329 -415.329 -415.329] (0.1000) ({r_i: None, r_t: [-796.530 -796.530 -796.530], eps: 0.1})
Step:  135900, Reward: [-393.910 -393.910 -393.910] [92.486], Avg: [-470.556 -470.556 -470.556] (0.0100) ({r_i: None, r_t: [-841.641 -841.641 -841.641], eps: 0.01})
Step:  103600, Reward: [-388.348 -388.348 -388.348] [64.072], Avg: [-415.303 -415.303 -415.303] (0.1000) ({r_i: None, r_t: [-791.749 -791.749 -791.749], eps: 0.1})
Step:  136000, Reward: [-406.960 -406.960 -406.960] [86.726], Avg: [-470.509 -470.509 -470.509] (0.0100) ({r_i: None, r_t: [-766.373 -766.373 -766.373], eps: 0.01})
Step:  103700, Reward: [-405.755 -405.755 -405.755] [67.466], Avg: [-415.294 -415.294 -415.294] (0.1000) ({r_i: None, r_t: [-775.902 -775.902 -775.902], eps: 0.1})
Step:  136100, Reward: [-406.635 -406.635 -406.635] [55.527], Avg: [-470.462 -470.462 -470.462] (0.0100) ({r_i: None, r_t: [-813.118 -813.118 -813.118], eps: 0.01})
Step:  103800, Reward: [-393.794 -393.794 -393.794] [57.415], Avg: [-415.273 -415.273 -415.273] (0.1000) ({r_i: None, r_t: [-804.102 -804.102 -804.102], eps: 0.1})
Step:  136200, Reward: [-409.443 -409.443 -409.443] [69.291], Avg: [-470.418 -470.418 -470.418] (0.0100) ({r_i: None, r_t: [-828.815 -828.815 -828.815], eps: 0.01})
Step:  103900, Reward: [-369.090 -369.090 -369.090] [61.646], Avg: [-415.229 -415.229 -415.229] (0.1000) ({r_i: None, r_t: [-792.860 -792.860 -792.860], eps: 0.1})
Step:  136300, Reward: [-400.713 -400.713 -400.713] [80.665], Avg: [-470.366 -470.366 -470.366] (0.0100) ({r_i: None, r_t: [-832.339 -832.339 -832.339], eps: 0.01})
Step:  104000, Reward: [-377.325 -377.325 -377.325] [39.365], Avg: [-415.192 -415.192 -415.192] (0.1000) ({r_i: None, r_t: [-792.517 -792.517 -792.517], eps: 0.1})
Step:  136400, Reward: [-415.594 -415.594 -415.594] [77.848], Avg: [-470.326 -470.326 -470.326] (0.0100) ({r_i: None, r_t: [-801.786 -801.786 -801.786], eps: 0.01})
Step:  104100, Reward: [-368.456 -368.456 -368.456] [32.419], Avg: [-415.147 -415.147 -415.147] (0.1000) ({r_i: None, r_t: [-757.688 -757.688 -757.688], eps: 0.1})
Step:  136500, Reward: [-396.189 -396.189 -396.189] [66.051], Avg: [-470.272 -470.272 -470.272] (0.0100) ({r_i: None, r_t: [-784.535 -784.535 -784.535], eps: 0.01})
Step:  104200, Reward: [-412.354 -412.354 -412.354] [70.450], Avg: [-415.145 -415.145 -415.145] (0.1000) ({r_i: None, r_t: [-779.323 -779.323 -779.323], eps: 0.1})
Step:  136600, Reward: [-399.584 -399.584 -399.584] [66.527], Avg: [-470.220 -470.220 -470.220] (0.0100) ({r_i: None, r_t: [-773.036 -773.036 -773.036], eps: 0.01})
Step:  104300, Reward: [-372.714 -372.714 -372.714] [55.113], Avg: [-415.104 -415.104 -415.104] (0.1000) ({r_i: None, r_t: [-800.471 -800.471 -800.471], eps: 0.1})
Step:  136700, Reward: [-387.822 -387.822 -387.822] [70.044], Avg: [-470.160 -470.160 -470.160] (0.0100) ({r_i: None, r_t: [-814.119 -814.119 -814.119], eps: 0.01})
Step:  104400, Reward: [-405.276 -405.276 -405.276] [95.663], Avg: [-415.095 -415.095 -415.095] (0.1000) ({r_i: None, r_t: [-755.631 -755.631 -755.631], eps: 0.1})
Step:  136800, Reward: [-394.288 -394.288 -394.288] [90.625], Avg: [-470.105 -470.105 -470.105] (0.0100) ({r_i: None, r_t: [-793.187 -793.187 -793.187], eps: 0.01})
Step:  104500, Reward: [-370.253 -370.253 -370.253] [61.097], Avg: [-415.052 -415.052 -415.052] (0.1000) ({r_i: None, r_t: [-764.417 -764.417 -764.417], eps: 0.1})
Step:  136900, Reward: [-421.751 -421.751 -421.751] [73.118], Avg: [-470.069 -470.069 -470.069] (0.0100) ({r_i: None, r_t: [-787.224 -787.224 -787.224], eps: 0.01})
Step:  104600, Reward: [-382.796 -382.796 -382.796] [51.985], Avg: [-415.021 -415.021 -415.021] (0.1000) ({r_i: None, r_t: [-779.276 -779.276 -779.276], eps: 0.1})
Step:  137000, Reward: [-380.444 -380.444 -380.444] [70.107], Avg: [-470.004 -470.004 -470.004] (0.0100) ({r_i: None, r_t: [-813.536 -813.536 -813.536], eps: 0.01})
Step:  104700, Reward: [-383.263 -383.263 -383.263] [54.154], Avg: [-414.991 -414.991 -414.991] (0.1000) ({r_i: None, r_t: [-758.816 -758.816 -758.816], eps: 0.1})
Step:  137100, Reward: [-413.102 -413.102 -413.102] [69.348], Avg: [-469.963 -469.963 -469.963] (0.0100) ({r_i: None, r_t: [-764.172 -764.172 -764.172], eps: 0.01})
Step:  104800, Reward: [-400.905 -400.905 -400.905] [61.002], Avg: [-414.977 -414.977 -414.977] (0.1000) ({r_i: None, r_t: [-791.797 -791.797 -791.797], eps: 0.1})
Step:  137200, Reward: [-408.626 -408.626 -408.626] [77.418], Avg: [-469.918 -469.918 -469.918] (0.0100) ({r_i: None, r_t: [-771.135 -771.135 -771.135], eps: 0.01})
Step:  104900, Reward: [-378.173 -378.173 -378.173] [56.904], Avg: [-414.942 -414.942 -414.942] (0.1000) ({r_i: None, r_t: [-768.993 -768.993 -768.993], eps: 0.1})
Step:  137300, Reward: [-392.413 -392.413 -392.413] [47.608], Avg: [-469.861 -469.861 -469.861] (0.0100) ({r_i: None, r_t: [-824.335 -824.335 -824.335], eps: 0.01})
Step:  105000, Reward: [-351.716 -351.716 -351.716] [40.835], Avg: [-414.882 -414.882 -414.882] (0.1000) ({r_i: None, r_t: [-791.171 -791.171 -791.171], eps: 0.1})
Step:  137400, Reward: [-394.507 -394.507 -394.507] [71.741], Avg: [-469.807 -469.807 -469.807] (0.0100) ({r_i: None, r_t: [-849.345 -849.345 -849.345], eps: 0.01})
Step:  105100, Reward: [-403.552 -403.552 -403.552] [70.232], Avg: [-414.871 -414.871 -414.871] (0.1000) ({r_i: None, r_t: [-808.791 -808.791 -808.791], eps: 0.1})
Step:  137500, Reward: [-390.444 -390.444 -390.444] [55.054], Avg: [-469.749 -469.749 -469.749] (0.0100) ({r_i: None, r_t: [-848.853 -848.853 -848.853], eps: 0.01})
Step:  105200, Reward: [-371.849 -371.849 -371.849] [50.913], Avg: [-414.830 -414.830 -414.830] (0.1000) ({r_i: None, r_t: [-785.373 -785.373 -785.373], eps: 0.1})
Step:  137600, Reward: [-406.302 -406.302 -406.302] [81.982], Avg: [-469.703 -469.703 -469.703] (0.0100) ({r_i: None, r_t: [-804.682 -804.682 -804.682], eps: 0.01})
Step:  105300, Reward: [-393.280 -393.280 -393.280] [53.713], Avg: [-414.810 -414.810 -414.810] (0.1000) ({r_i: None, r_t: [-798.905 -798.905 -798.905], eps: 0.1})
Step:  137700, Reward: [-375.934 -375.934 -375.934] [59.933], Avg: [-469.635 -469.635 -469.635] (0.0100) ({r_i: None, r_t: [-769.310 -769.310 -769.310], eps: 0.01})
Step:  105400, Reward: [-366.152 -366.152 -366.152] [68.535], Avg: [-414.764 -414.764 -414.764] (0.1000) ({r_i: None, r_t: [-775.388 -775.388 -775.388], eps: 0.1})
Step:  137800, Reward: [-392.060 -392.060 -392.060] [75.829], Avg: [-469.579 -469.579 -469.579] (0.0100) ({r_i: None, r_t: [-816.436 -816.436 -816.436], eps: 0.01})
Step:  105500, Reward: [-394.454 -394.454 -394.454] [70.629], Avg: [-414.745 -414.745 -414.745] (0.1000) ({r_i: None, r_t: [-802.021 -802.021 -802.021], eps: 0.1})
Step:  137900, Reward: [-423.142 -423.142 -423.142] [68.876], Avg: [-469.545 -469.545 -469.545] (0.0100) ({r_i: None, r_t: [-800.794 -800.794 -800.794], eps: 0.01})
Step:  105600, Reward: [-360.221 -360.221 -360.221] [45.096], Avg: [-414.693 -414.693 -414.693] (0.1000) ({r_i: None, r_t: [-829.810 -829.810 -829.810], eps: 0.1})
Step:  138000, Reward: [-419.979 -419.979 -419.979] [72.722], Avg: [-469.509 -469.509 -469.509] (0.0100) ({r_i: None, r_t: [-806.565 -806.565 -806.565], eps: 0.01})
Step:  105700, Reward: [-403.432 -403.432 -403.432] [69.665], Avg: [-414.682 -414.682 -414.682] (0.1000) ({r_i: None, r_t: [-774.914 -774.914 -774.914], eps: 0.1})
Step:  138100, Reward: [-422.564 -422.564 -422.564] [97.598], Avg: [-469.475 -469.475 -469.475] (0.0100) ({r_i: None, r_t: [-814.050 -814.050 -814.050], eps: 0.01})
Step:  105800, Reward: [-395.660 -395.660 -395.660] [65.533], Avg: [-414.664 -414.664 -414.664] (0.1000) ({r_i: None, r_t: [-767.570 -767.570 -767.570], eps: 0.1})
Step:  138200, Reward: [-407.239 -407.239 -407.239] [79.592], Avg: [-469.430 -469.430 -469.430] (0.0100) ({r_i: None, r_t: [-745.084 -745.084 -745.084], eps: 0.01})
Step:  105900, Reward: [-396.996 -396.996 -396.996] [55.598], Avg: [-414.648 -414.648 -414.648] (0.1000) ({r_i: None, r_t: [-759.185 -759.185 -759.185], eps: 0.1})
Step:  138300, Reward: [-398.748 -398.748 -398.748] [82.036], Avg: [-469.379 -469.379 -469.379] (0.0100) ({r_i: None, r_t: [-838.529 -838.529 -838.529], eps: 0.01})
Step:  106000, Reward: [-433.136 -433.136 -433.136] [60.815], Avg: [-414.665 -414.665 -414.665] (0.1000) ({r_i: None, r_t: [-785.486 -785.486 -785.486], eps: 0.1})
Step:  138400, Reward: [-339.580 -339.580 -339.580] [56.837], Avg: [-469.285 -469.285 -469.285] (0.0100) ({r_i: None, r_t: [-774.211 -774.211 -774.211], eps: 0.01})
Step:  106100, Reward: [-379.292 -379.292 -379.292] [68.357], Avg: [-414.632 -414.632 -414.632] (0.1000) ({r_i: None, r_t: [-801.648 -801.648 -801.648], eps: 0.1})
Step:  138500, Reward: [-401.337 -401.337 -401.337] [57.477], Avg: [-469.236 -469.236 -469.236] (0.0100) ({r_i: None, r_t: [-779.966 -779.966 -779.966], eps: 0.01})
Step:  106200, Reward: [-392.094 -392.094 -392.094] [77.535], Avg: [-414.611 -414.611 -414.611] (0.1000) ({r_i: None, r_t: [-810.716 -810.716 -810.716], eps: 0.1})
Step:  138600, Reward: [-381.507 -381.507 -381.507] [84.849], Avg: [-469.173 -469.173 -469.173] (0.0100) ({r_i: None, r_t: [-772.781 -772.781 -772.781], eps: 0.01})
Step:  106300, Reward: [-383.908 -383.908 -383.908] [49.427], Avg: [-414.582 -414.582 -414.582] (0.1000) ({r_i: None, r_t: [-766.547 -766.547 -766.547], eps: 0.1})
Step:  138700, Reward: [-426.527 -426.527 -426.527] [85.123], Avg: [-469.142 -469.142 -469.142] (0.0100) ({r_i: None, r_t: [-771.697 -771.697 -771.697], eps: 0.01})
Step:  106400, Reward: [-402.074 -402.074 -402.074] [57.747], Avg: [-414.570 -414.570 -414.570] (0.1000) ({r_i: None, r_t: [-791.959 -791.959 -791.959], eps: 0.1})
Step:  138800, Reward: [-421.883 -421.883 -421.883] [91.187], Avg: [-469.108 -469.108 -469.108] (0.0100) ({r_i: None, r_t: [-801.509 -801.509 -801.509], eps: 0.01})
Step:  106500, Reward: [-368.498 -368.498 -368.498] [51.491], Avg: [-414.527 -414.527 -414.527] (0.1000) ({r_i: None, r_t: [-803.057 -803.057 -803.057], eps: 0.1})
Step:  138900, Reward: [-420.115 -420.115 -420.115] [59.156], Avg: [-469.073 -469.073 -469.073] (0.0100) ({r_i: None, r_t: [-782.264 -782.264 -782.264], eps: 0.01})
Step:  106600, Reward: [-365.344 -365.344 -365.344] [45.946], Avg: [-414.481 -414.481 -414.481] (0.1000) ({r_i: None, r_t: [-772.346 -772.346 -772.346], eps: 0.1})
Step:  139000, Reward: [-405.682 -405.682 -405.682] [63.545], Avg: [-469.027 -469.027 -469.027] (0.0100) ({r_i: None, r_t: [-802.080 -802.080 -802.080], eps: 0.01})
Step:  106700, Reward: [-378.978 -378.978 -378.978] [80.037], Avg: [-414.448 -414.448 -414.448] (0.1000) ({r_i: None, r_t: [-780.132 -780.132 -780.132], eps: 0.1})
Step:  139100, Reward: [-370.991 -370.991 -370.991] [77.152], Avg: [-468.957 -468.957 -468.957] (0.0100) ({r_i: None, r_t: [-843.047 -843.047 -843.047], eps: 0.01})
Step:  106800, Reward: [-409.513 -409.513 -409.513] [65.620], Avg: [-414.443 -414.443 -414.443] (0.1000) ({r_i: None, r_t: [-779.790 -779.790 -779.790], eps: 0.1})
Step:  139200, Reward: [-426.657 -426.657 -426.657] [102.722], Avg: [-468.927 -468.927 -468.927] (0.0100) ({r_i: None, r_t: [-845.305 -845.305 -845.305], eps: 0.01})
Step:  106900, Reward: [-402.127 -402.127 -402.127] [48.959], Avg: [-414.431 -414.431 -414.431] (0.1000) ({r_i: None, r_t: [-758.197 -758.197 -758.197], eps: 0.1})
Step:  139300, Reward: [-383.088 -383.088 -383.088] [83.646], Avg: [-468.865 -468.865 -468.865] (0.0100) ({r_i: None, r_t: [-783.553 -783.553 -783.553], eps: 0.01})
Step:  107000, Reward: [-357.259 -357.259 -357.259] [55.659], Avg: [-414.378 -414.378 -414.378] (0.1000) ({r_i: None, r_t: [-765.020 -765.020 -765.020], eps: 0.1})
Step:  139400, Reward: [-378.657 -378.657 -378.657] [60.648], Avg: [-468.800 -468.800 -468.800] (0.0100) ({r_i: None, r_t: [-777.862 -777.862 -777.862], eps: 0.01})
Step:  107100, Reward: [-383.344 -383.344 -383.344] [59.439], Avg: [-414.349 -414.349 -414.349] (0.1000) ({r_i: None, r_t: [-752.411 -752.411 -752.411], eps: 0.1})
Step:  139500, Reward: [-376.989 -376.989 -376.989] [55.265], Avg: [-468.735 -468.735 -468.735] (0.0100) ({r_i: None, r_t: [-831.205 -831.205 -831.205], eps: 0.01})
Step:  107200, Reward: [-380.358 -380.358 -380.358] [42.850], Avg: [-414.317 -414.317 -414.317] (0.1000) ({r_i: None, r_t: [-774.458 -774.458 -774.458], eps: 0.1})
Step:  139600, Reward: [-393.781 -393.781 -393.781] [60.512], Avg: [-468.681 -468.681 -468.681] (0.0100) ({r_i: None, r_t: [-774.315 -774.315 -774.315], eps: 0.01})
Step:  107300, Reward: [-368.738 -368.738 -368.738] [50.889], Avg: [-414.275 -414.275 -414.275] (0.1000) ({r_i: None, r_t: [-712.362 -712.362 -712.362], eps: 0.1})
Step:  139700, Reward: [-405.720 -405.720 -405.720] [56.728], Avg: [-468.636 -468.636 -468.636] (0.0100) ({r_i: None, r_t: [-805.125 -805.125 -805.125], eps: 0.01})
Step:  107400, Reward: [-375.423 -375.423 -375.423] [64.091], Avg: [-414.239 -414.239 -414.239] (0.1000) ({r_i: None, r_t: [-779.398 -779.398 -779.398], eps: 0.1})
Step:  139800, Reward: [-401.610 -401.610 -401.610] [65.404], Avg: [-468.588 -468.588 -468.588] (0.0100) ({r_i: None, r_t: [-820.653 -820.653 -820.653], eps: 0.01})
Step:  107500, Reward: [-387.468 -387.468 -387.468] [59.302], Avg: [-414.214 -414.214 -414.214] (0.1000) ({r_i: None, r_t: [-744.155 -744.155 -744.155], eps: 0.1})
Step:  139900, Reward: [-376.406 -376.406 -376.406] [68.817], Avg: [-468.522 -468.522 -468.522] (0.0100) ({r_i: None, r_t: [-770.494 -770.494 -770.494], eps: 0.01})
Step:  107600, Reward: [-382.015 -382.015 -382.015] [75.258], Avg: [-414.184 -414.184 -414.184] (0.1000) ({r_i: None, r_t: [-745.603 -745.603 -745.603], eps: 0.1})
Step:  140000, Reward: [-382.799 -382.799 -382.799] [49.310], Avg: [-468.461 -468.461 -468.461] (0.0100) ({r_i: None, r_t: [-811.125 -811.125 -811.125], eps: 0.01})
Step:  107700, Reward: [-371.877 -371.877 -371.877] [58.556], Avg: [-414.145 -414.145 -414.145] (0.1000) ({r_i: None, r_t: [-735.940 -735.940 -735.940], eps: 0.1})
Step:  140100, Reward: [-404.645 -404.645 -404.645] [80.587], Avg: [-468.416 -468.416 -468.416] (0.0100) ({r_i: None, r_t: [-782.484 -782.484 -782.484], eps: 0.01})
Step:  107800, Reward: [-382.797 -382.797 -382.797] [57.807], Avg: [-414.116 -414.116 -414.116] (0.1000) ({r_i: None, r_t: [-709.036 -709.036 -709.036], eps: 0.1})
Step:  140200, Reward: [-375.374 -375.374 -375.374] [65.200], Avg: [-468.349 -468.349 -468.349] (0.0100) ({r_i: None, r_t: [-775.523 -775.523 -775.523], eps: 0.01})
Step:  107900, Reward: [-391.173 -391.173 -391.173] [62.144], Avg: [-414.095 -414.095 -414.095] (0.1000) ({r_i: None, r_t: [-740.220 -740.220 -740.220], eps: 0.1})
Step:  140300, Reward: [-360.378 -360.378 -360.378] [61.857], Avg: [-468.272 -468.272 -468.272] (0.0100) ({r_i: None, r_t: [-753.040 -753.040 -753.040], eps: 0.01})
Step:  108000, Reward: [-372.888 -372.888 -372.888] [44.788], Avg: [-414.056 -414.056 -414.056] (0.1000) ({r_i: None, r_t: [-715.325 -715.325 -715.325], eps: 0.1})
Step:  140400, Reward: [-412.083 -412.083 -412.083] [68.938], Avg: [-468.232 -468.232 -468.232] (0.0100) ({r_i: None, r_t: [-860.196 -860.196 -860.196], eps: 0.01})
Step:  108100, Reward: [-385.714 -385.714 -385.714] [62.488], Avg: [-414.030 -414.030 -414.030] (0.1000) ({r_i: None, r_t: [-735.503 -735.503 -735.503], eps: 0.1})
Step:  140500, Reward: [-406.434 -406.434 -406.434] [86.918], Avg: [-468.188 -468.188 -468.188] (0.0100) ({r_i: None, r_t: [-773.417 -773.417 -773.417], eps: 0.01})
Step:  108200, Reward: [-369.606 -369.606 -369.606] [56.377], Avg: [-413.989 -413.989 -413.989] (0.1000) ({r_i: None, r_t: [-769.473 -769.473 -769.473], eps: 0.1})
Step:  140600, Reward: [-369.420 -369.420 -369.420] [54.476], Avg: [-468.118 -468.118 -468.118] (0.0100) ({r_i: None, r_t: [-790.069 -790.069 -790.069], eps: 0.01})
Step:  108300, Reward: [-382.559 -382.559 -382.559] [65.337], Avg: [-413.960 -413.960 -413.960] (0.1000) ({r_i: None, r_t: [-763.430 -763.430 -763.430], eps: 0.1})
Step:  140700, Reward: [-381.036 -381.036 -381.036] [84.656], Avg: [-468.056 -468.056 -468.056] (0.0100) ({r_i: None, r_t: [-753.584 -753.584 -753.584], eps: 0.01})
Step:  108400, Reward: [-368.774 -368.774 -368.774] [65.804], Avg: [-413.919 -413.919 -413.919] (0.1000) ({r_i: None, r_t: [-790.573 -790.573 -790.573], eps: 0.1})
Step:  140800, Reward: [-425.882 -425.882 -425.882] [61.711], Avg: [-468.026 -468.026 -468.026] (0.0100) ({r_i: None, r_t: [-804.239 -804.239 -804.239], eps: 0.01})
Step:  108500, Reward: [-389.718 -389.718 -389.718] [66.533], Avg: [-413.896 -413.896 -413.896] (0.1000) ({r_i: None, r_t: [-741.506 -741.506 -741.506], eps: 0.1})
Step:  140900, Reward: [-391.082 -391.082 -391.082] [75.107], Avg: [-467.972 -467.972 -467.972] (0.0100) ({r_i: None, r_t: [-760.476 -760.476 -760.476], eps: 0.01})
Step:  108600, Reward: [-344.745 -344.745 -344.745] [40.961], Avg: [-413.833 -413.833 -413.833] (0.1000) ({r_i: None, r_t: [-763.863 -763.863 -763.863], eps: 0.1})
Step:  141000, Reward: [-384.461 -384.461 -384.461] [68.073], Avg: [-467.913 -467.913 -467.913] (0.0100) ({r_i: None, r_t: [-760.019 -760.019 -760.019], eps: 0.01})
Step:  108700, Reward: [-396.657 -396.657 -396.657] [75.301], Avg: [-413.817 -413.817 -413.817] (0.1000) ({r_i: None, r_t: [-730.673 -730.673 -730.673], eps: 0.1})
Step:  141100, Reward: [-378.960 -378.960 -378.960] [53.960], Avg: [-467.850 -467.850 -467.850] (0.0100) ({r_i: None, r_t: [-840.285 -840.285 -840.285], eps: 0.01})
Step:  108800, Reward: [-364.221 -364.221 -364.221] [65.180], Avg: [-413.771 -413.771 -413.771] (0.1000) ({r_i: None, r_t: [-739.993 -739.993 -739.993], eps: 0.1})
Step:  141200, Reward: [-360.271 -360.271 -360.271] [49.693], Avg: [-467.774 -467.774 -467.774] (0.0100) ({r_i: None, r_t: [-788.134 -788.134 -788.134], eps: 0.01})
Step:  108900, Reward: [-398.252 -398.252 -398.252] [72.907], Avg: [-413.757 -413.757 -413.757] (0.1000) ({r_i: None, r_t: [-764.406 -764.406 -764.406], eps: 0.1})
Step:  141300, Reward: [-375.159 -375.159 -375.159] [77.577], Avg: [-467.708 -467.708 -467.708] (0.0100) ({r_i: None, r_t: [-766.742 -766.742 -766.742], eps: 0.01})
Step:  109000, Reward: [-369.274 -369.274 -369.274] [43.184], Avg: [-413.716 -413.716 -413.716] (0.1000) ({r_i: None, r_t: [-777.349 -777.349 -777.349], eps: 0.1})
Step:  141400, Reward: [-397.873 -397.873 -397.873] [78.210], Avg: [-467.659 -467.659 -467.659] (0.0100) ({r_i: None, r_t: [-817.038 -817.038 -817.038], eps: 0.01})
Step:  109100, Reward: [-373.586 -373.586 -373.586] [60.881], Avg: [-413.680 -413.680 -413.680] (0.1000) ({r_i: None, r_t: [-722.938 -722.938 -722.938], eps: 0.1})
Step:  141500, Reward: [-396.911 -396.911 -396.911] [73.013], Avg: [-467.609 -467.609 -467.609] (0.0100) ({r_i: None, r_t: [-787.730 -787.730 -787.730], eps: 0.01})
Step:  109200, Reward: [-384.272 -384.272 -384.272] [53.985], Avg: [-413.653 -413.653 -413.653] (0.1000) ({r_i: None, r_t: [-756.355 -756.355 -756.355], eps: 0.1})
Step:  141600, Reward: [-393.240 -393.240 -393.240] [63.893], Avg: [-467.556 -467.556 -467.556] (0.0100) ({r_i: None, r_t: [-740.580 -740.580 -740.580], eps: 0.01})
Step:  109300, Reward: [-368.500 -368.500 -368.500] [61.919], Avg: [-413.611 -413.611 -413.611] (0.1000) ({r_i: None, r_t: [-740.811 -740.811 -740.811], eps: 0.1})
Step:  141700, Reward: [-406.136 -406.136 -406.136] [70.320], Avg: [-467.513 -467.513 -467.513] (0.0100) ({r_i: None, r_t: [-803.049 -803.049 -803.049], eps: 0.01})
Step:  109400, Reward: [-359.886 -359.886 -359.886] [48.147], Avg: [-413.562 -413.562 -413.562] (0.1000) ({r_i: None, r_t: [-790.167 -790.167 -790.167], eps: 0.1})
Step:  141800, Reward: [-362.304 -362.304 -362.304] [62.342], Avg: [-467.439 -467.439 -467.439] (0.0100) ({r_i: None, r_t: [-800.885 -800.885 -800.885], eps: 0.01})
Step:  109500, Reward: [-369.400 -369.400 -369.400] [44.825], Avg: [-413.522 -413.522 -413.522] (0.1000) ({r_i: None, r_t: [-768.904 -768.904 -768.904], eps: 0.1})
Step:  141900, Reward: [-377.796 -377.796 -377.796] [74.076], Avg: [-467.376 -467.376 -467.376] (0.0100) ({r_i: None, r_t: [-835.770 -835.770 -835.770], eps: 0.01})
Step:  109600, Reward: [-393.750 -393.750 -393.750] [72.157], Avg: [-413.504 -413.504 -413.504] (0.1000) ({r_i: None, r_t: [-735.676 -735.676 -735.676], eps: 0.1})
Step:  142000, Reward: [-386.618 -386.618 -386.618] [64.558], Avg: [-467.319 -467.319 -467.319] (0.0100) ({r_i: None, r_t: [-779.054 -779.054 -779.054], eps: 0.01})
Step:  109700, Reward: [-395.933 -395.933 -395.933] [85.372], Avg: [-413.488 -413.488 -413.488] (0.1000) ({r_i: None, r_t: [-753.574 -753.574 -753.574], eps: 0.1})
Step:  142100, Reward: [-370.297 -370.297 -370.297] [49.134], Avg: [-467.251 -467.251 -467.251] (0.0100) ({r_i: None, r_t: [-784.165 -784.165 -784.165], eps: 0.01})
Step:  109800, Reward: [-393.272 -393.272 -393.272] [74.368], Avg: [-413.470 -413.470 -413.470] (0.1000) ({r_i: None, r_t: [-764.254 -764.254 -764.254], eps: 0.1})
Step:  142200, Reward: [-394.819 -394.819 -394.819] [71.606], Avg: [-467.200 -467.200 -467.200] (0.0100) ({r_i: None, r_t: [-746.982 -746.982 -746.982], eps: 0.01})
Step:  109900, Reward: [-365.314 -365.314 -365.314] [74.230], Avg: [-413.426 -413.426 -413.426] (0.1000) ({r_i: None, r_t: [-758.166 -758.166 -758.166], eps: 0.1})
Step:  142300, Reward: [-393.935 -393.935 -393.935] [55.574], Avg: [-467.148 -467.148 -467.148] (0.0100) ({r_i: None, r_t: [-810.553 -810.553 -810.553], eps: 0.01})
Step:  110000, Reward: [-361.306 -361.306 -361.306] [52.007], Avg: [-413.378 -413.378 -413.378] (0.1000) ({r_i: None, r_t: [-778.663 -778.663 -778.663], eps: 0.1})
Step:  142400, Reward: [-416.671 -416.671 -416.671] [72.974], Avg: [-467.113 -467.113 -467.113] (0.0100) ({r_i: None, r_t: [-764.492 -764.492 -764.492], eps: 0.01})
Step:  110100, Reward: [-365.595 -365.595 -365.595] [62.517], Avg: [-413.335 -413.335 -413.335] (0.1000) ({r_i: None, r_t: [-767.499 -767.499 -767.499], eps: 0.1})
Step:  142500, Reward: [-398.009 -398.009 -398.009] [60.139], Avg: [-467.064 -467.064 -467.064] (0.0100) ({r_i: None, r_t: [-766.949 -766.949 -766.949], eps: 0.01})
Step:  110200, Reward: [-383.015 -383.015 -383.015] [45.104], Avg: [-413.308 -413.308 -413.308] (0.1000) ({r_i: None, r_t: [-767.708 -767.708 -767.708], eps: 0.1})
Step:  142600, Reward: [-388.866 -388.866 -388.866] [80.050], Avg: [-467.010 -467.010 -467.010] (0.0100) ({r_i: None, r_t: [-753.741 -753.741 -753.741], eps: 0.01})
Step:  110300, Reward: [-362.725 -362.725 -362.725] [54.186], Avg: [-413.262 -413.262 -413.262] (0.1000) ({r_i: None, r_t: [-712.546 -712.546 -712.546], eps: 0.1})
Step:  142700, Reward: [-373.292 -373.292 -373.292] [64.980], Avg: [-466.944 -466.944 -466.944] (0.0100) ({r_i: None, r_t: [-775.167 -775.167 -775.167], eps: 0.01})
Step:  110400, Reward: [-371.720 -371.720 -371.720] [53.526], Avg: [-413.224 -413.224 -413.224] (0.1000) ({r_i: None, r_t: [-692.734 -692.734 -692.734], eps: 0.1})
Step:  142800, Reward: [-413.616 -413.616 -413.616] [80.807], Avg: [-466.907 -466.907 -466.907] (0.0100) ({r_i: None, r_t: [-794.756 -794.756 -794.756], eps: 0.01})
Step:  110500, Reward: [-361.262 -361.262 -361.262] [38.524], Avg: [-413.177 -413.177 -413.177] (0.1000) ({r_i: None, r_t: [-788.745 -788.745 -788.745], eps: 0.1})
Step:  142900, Reward: [-410.293 -410.293 -410.293] [57.415], Avg: [-466.867 -466.867 -466.867] (0.0100) ({r_i: None, r_t: [-772.637 -772.637 -772.637], eps: 0.01})
Step:  110600, Reward: [-376.553 -376.553 -376.553] [78.108], Avg: [-413.144 -413.144 -413.144] (0.1000) ({r_i: None, r_t: [-711.780 -711.780 -711.780], eps: 0.1})
Step:  143000, Reward: [-403.456 -403.456 -403.456] [61.265], Avg: [-466.823 -466.823 -466.823] (0.0100) ({r_i: None, r_t: [-761.930 -761.930 -761.930], eps: 0.01})
Step:  110700, Reward: [-386.916 -386.916 -386.916] [59.871], Avg: [-413.120 -413.120 -413.120] (0.1000) ({r_i: None, r_t: [-758.506 -758.506 -758.506], eps: 0.1})
Step:  143100, Reward: [-404.555 -404.555 -404.555] [76.798], Avg: [-466.779 -466.779 -466.779] (0.0100) ({r_i: None, r_t: [-823.601 -823.601 -823.601], eps: 0.01})
Step:  110800, Reward: [-387.072 -387.072 -387.072] [67.456], Avg: [-413.097 -413.097 -413.097] (0.1000) ({r_i: None, r_t: [-727.523 -727.523 -727.523], eps: 0.1})
Step:  143200, Reward: [-389.289 -389.289 -389.289] [58.991], Avg: [-466.725 -466.725 -466.725] (0.0100) ({r_i: None, r_t: [-773.348 -773.348 -773.348], eps: 0.01})
Step:  110900, Reward: [-377.705 -377.705 -377.705] [54.755], Avg: [-413.065 -413.065 -413.065] (0.1000) ({r_i: None, r_t: [-735.156 -735.156 -735.156], eps: 0.1})
Step:  143300, Reward: [-402.596 -402.596 -402.596] [79.595], Avg: [-466.680 -466.680 -466.680] (0.0100) ({r_i: None, r_t: [-715.336 -715.336 -715.336], eps: 0.01})
Step:  111000, Reward: [-372.792 -372.792 -372.792] [54.787], Avg: [-413.029 -413.029 -413.029] (0.1000) ({r_i: None, r_t: [-757.407 -757.407 -757.407], eps: 0.1})
Step:  143400, Reward: [-386.050 -386.050 -386.050] [62.659], Avg: [-466.624 -466.624 -466.624] (0.0100) ({r_i: None, r_t: [-843.647 -843.647 -843.647], eps: 0.01})
Step:  111100, Reward: [-361.021 -361.021 -361.021] [52.180], Avg: [-412.982 -412.982 -412.982] (0.1000) ({r_i: None, r_t: [-764.248 -764.248 -764.248], eps: 0.1})
Step:  143500, Reward: [-381.191 -381.191 -381.191] [62.165], Avg: [-466.565 -466.565 -466.565] (0.0100) ({r_i: None, r_t: [-775.308 -775.308 -775.308], eps: 0.01})
Step:  111200, Reward: [-369.661 -369.661 -369.661] [53.834], Avg: [-412.943 -412.943 -412.943] (0.1000) ({r_i: None, r_t: [-738.368 -738.368 -738.368], eps: 0.1})
Step:  143600, Reward: [-397.936 -397.936 -397.936] [68.800], Avg: [-466.517 -466.517 -466.517] (0.0100) ({r_i: None, r_t: [-816.073 -816.073 -816.073], eps: 0.01})
Step:  111300, Reward: [-371.700 -371.700 -371.700] [57.423], Avg: [-412.906 -412.906 -412.906] (0.1000) ({r_i: None, r_t: [-708.310 -708.310 -708.310], eps: 0.1})
Step:  143700, Reward: [-378.328 -378.328 -378.328] [72.206], Avg: [-466.456 -466.456 -466.456] (0.0100) ({r_i: None, r_t: [-777.675 -777.675 -777.675], eps: 0.01})
Step:  111400, Reward: [-362.710 -362.710 -362.710] [52.505], Avg: [-412.861 -412.861 -412.861] (0.1000) ({r_i: None, r_t: [-725.377 -725.377 -725.377], eps: 0.1})
Step:  143800, Reward: [-410.562 -410.562 -410.562] [78.699], Avg: [-466.417 -466.417 -466.417] (0.0100) ({r_i: None, r_t: [-784.389 -784.389 -784.389], eps: 0.01})
Step:  111500, Reward: [-376.193 -376.193 -376.193] [34.961], Avg: [-412.828 -412.828 -412.828] (0.1000) ({r_i: None, r_t: [-736.090 -736.090 -736.090], eps: 0.1})
Step:  143900, Reward: [-393.273 -393.273 -393.273] [72.907], Avg: [-466.366 -466.366 -466.366] (0.0100) ({r_i: None, r_t: [-764.734 -764.734 -764.734], eps: 0.01})
Step:  111600, Reward: [-404.849 -404.849 -404.849] [71.983], Avg: [-412.821 -412.821 -412.821] (0.1000) ({r_i: None, r_t: [-734.830 -734.830 -734.830], eps: 0.1})
Step:  144000, Reward: [-370.055 -370.055 -370.055] [57.941], Avg: [-466.299 -466.299 -466.299] (0.0100) ({r_i: None, r_t: [-828.215 -828.215 -828.215], eps: 0.01})
Step:  111700, Reward: [-375.898 -375.898 -375.898] [60.313], Avg: [-412.788 -412.788 -412.788] (0.1000) ({r_i: None, r_t: [-746.846 -746.846 -746.846], eps: 0.1})
Step:  144100, Reward: [-449.213 -449.213 -449.213] [79.386], Avg: [-466.287 -466.287 -466.287] (0.0100) ({r_i: None, r_t: [-797.500 -797.500 -797.500], eps: 0.01})
Step:  111800, Reward: [-361.261 -361.261 -361.261] [50.040], Avg: [-412.742 -412.742 -412.742] (0.1000) ({r_i: None, r_t: [-733.832 -733.832 -733.832], eps: 0.1})
Step:  144200, Reward: [-395.217 -395.217 -395.217] [64.043], Avg: [-466.238 -466.238 -466.238] (0.0100) ({r_i: None, r_t: [-781.227 -781.227 -781.227], eps: 0.01})
Step:  111900, Reward: [-389.853 -389.853 -389.853] [53.585], Avg: [-412.722 -412.722 -412.722] (0.1000) ({r_i: None, r_t: [-770.269 -770.269 -770.269], eps: 0.1})
Step:  144300, Reward: [-396.661 -396.661 -396.661] [75.558], Avg: [-466.190 -466.190 -466.190] (0.0100) ({r_i: None, r_t: [-757.211 -757.211 -757.211], eps: 0.01})
Step:  112000, Reward: [-351.150 -351.150 -351.150] [51.230], Avg: [-412.667 -412.667 -412.667] (0.1000) ({r_i: None, r_t: [-745.548 -745.548 -745.548], eps: 0.1})
Step:  144400, Reward: [-394.115 -394.115 -394.115] [67.029], Avg: [-466.140 -466.140 -466.140] (0.0100) ({r_i: None, r_t: [-705.845 -705.845 -705.845], eps: 0.01})
Step:  112100, Reward: [-368.715 -368.715 -368.715] [59.447], Avg: [-412.628 -412.628 -412.628] (0.1000) ({r_i: None, r_t: [-757.077 -757.077 -757.077], eps: 0.1})
Step:  144500, Reward: [-367.094 -367.094 -367.094] [73.268], Avg: [-466.071 -466.071 -466.071] (0.0100) ({r_i: None, r_t: [-815.428 -815.428 -815.428], eps: 0.01})
Step:  112200, Reward: [-369.856 -369.856 -369.856] [59.878], Avg: [-412.589 -412.589 -412.589] (0.1000) ({r_i: None, r_t: [-756.162 -756.162 -756.162], eps: 0.1})
Step:  144600, Reward: [-364.131 -364.131 -364.131] [45.505], Avg: [-466.001 -466.001 -466.001] (0.0100) ({r_i: None, r_t: [-790.033 -790.033 -790.033], eps: 0.01})
Step:  112300, Reward: [-402.516 -402.516 -402.516] [74.122], Avg: [-412.580 -412.580 -412.580] (0.1000) ({r_i: None, r_t: [-740.998 -740.998 -740.998], eps: 0.1})
Step:  144700, Reward: [-400.505 -400.505 -400.505] [71.757], Avg: [-465.956 -465.956 -465.956] (0.0100) ({r_i: None, r_t: [-773.166 -773.166 -773.166], eps: 0.01})
Step:  112400, Reward: [-410.561 -410.561 -410.561] [51.531], Avg: [-412.579 -412.579 -412.579] (0.1000) ({r_i: None, r_t: [-803.330 -803.330 -803.330], eps: 0.1})
Step:  144800, Reward: [-399.673 -399.673 -399.673] [86.308], Avg: [-465.910 -465.910 -465.910] (0.0100) ({r_i: None, r_t: [-817.796 -817.796 -817.796], eps: 0.01})
Step:  112500, Reward: [-357.659 -357.659 -357.659] [59.669], Avg: [-412.530 -412.530 -412.530] (0.1000) ({r_i: None, r_t: [-772.211 -772.211 -772.211], eps: 0.1})
Step:  144900, Reward: [-370.500 -370.500 -370.500] [72.100], Avg: [-465.844 -465.844 -465.844] (0.0100) ({r_i: None, r_t: [-761.414 -761.414 -761.414], eps: 0.01})
Step:  112600, Reward: [-388.174 -388.174 -388.174] [74.078], Avg: [-412.508 -412.508 -412.508] (0.1000) ({r_i: None, r_t: [-799.084 -799.084 -799.084], eps: 0.1})
Step:  145000, Reward: [-385.157 -385.157 -385.157] [76.717], Avg: [-465.789 -465.789 -465.789] (0.0100) ({r_i: None, r_t: [-784.884 -784.884 -784.884], eps: 0.01})
Step:  112700, Reward: [-371.054 -371.054 -371.054] [57.302], Avg: [-412.472 -412.472 -412.472] (0.1000) ({r_i: None, r_t: [-764.271 -764.271 -764.271], eps: 0.1})
Step:  145100, Reward: [-366.086 -366.086 -366.086] [66.011], Avg: [-465.720 -465.720 -465.720] (0.0100) ({r_i: None, r_t: [-802.735 -802.735 -802.735], eps: 0.01})
Step:  112800, Reward: [-378.239 -378.239 -378.239] [44.442], Avg: [-412.441 -412.441 -412.441] (0.1000) ({r_i: None, r_t: [-795.880 -795.880 -795.880], eps: 0.1})
Step:  145200, Reward: [-411.388 -411.388 -411.388] [71.503], Avg: [-465.683 -465.683 -465.683] (0.0100) ({r_i: None, r_t: [-805.753 -805.753 -805.753], eps: 0.01})
Step:  112900, Reward: [-395.319 -395.319 -395.319] [71.943], Avg: [-412.426 -412.426 -412.426] (0.1000) ({r_i: None, r_t: [-757.588 -757.588 -757.588], eps: 0.1})
Step:  145300, Reward: [-403.529 -403.529 -403.529] [74.385], Avg: [-465.640 -465.640 -465.640] (0.0100) ({r_i: None, r_t: [-790.245 -790.245 -790.245], eps: 0.01})
Step:  113000, Reward: [-403.490 -403.490 -403.490] [66.579], Avg: [-412.418 -412.418 -412.418] (0.1000) ({r_i: None, r_t: [-757.356 -757.356 -757.356], eps: 0.1})
Step:  145400, Reward: [-380.429 -380.429 -380.429] [96.671], Avg: [-465.581 -465.581 -465.581] (0.0100) ({r_i: None, r_t: [-783.413 -783.413 -783.413], eps: 0.01})
Step:  113100, Reward: [-397.608 -397.608 -397.608] [85.871], Avg: [-412.405 -412.405 -412.405] (0.1000) ({r_i: None, r_t: [-767.688 -767.688 -767.688], eps: 0.1})
Step:  145500, Reward: [-417.519 -417.519 -417.519] [100.077], Avg: [-465.548 -465.548 -465.548] (0.0100) ({r_i: None, r_t: [-800.940 -800.940 -800.940], eps: 0.01})
Step:  113200, Reward: [-374.393 -374.393 -374.393] [55.182], Avg: [-412.372 -412.372 -412.372] (0.1000) ({r_i: None, r_t: [-781.740 -781.740 -781.740], eps: 0.1})
Step:  145600, Reward: [-392.375 -392.375 -392.375] [88.144], Avg: [-465.498 -465.498 -465.498] (0.0100) ({r_i: None, r_t: [-797.335 -797.335 -797.335], eps: 0.01})
Step:  113300, Reward: [-392.026 -392.026 -392.026] [70.901], Avg: [-412.354 -412.354 -412.354] (0.1000) ({r_i: None, r_t: [-790.003 -790.003 -790.003], eps: 0.1})
Step:  145700, Reward: [-418.301 -418.301 -418.301] [77.015], Avg: [-465.466 -465.466 -465.466] (0.0100) ({r_i: None, r_t: [-791.121 -791.121 -791.121], eps: 0.01})
Step:  113400, Reward: [-393.861 -393.861 -393.861] [69.215], Avg: [-412.337 -412.337 -412.337] (0.1000) ({r_i: None, r_t: [-800.887 -800.887 -800.887], eps: 0.1})
Step:  145800, Reward: [-397.438 -397.438 -397.438] [100.058], Avg: [-465.419 -465.419 -465.419] (0.0100) ({r_i: None, r_t: [-833.214 -833.214 -833.214], eps: 0.01})
Step:  113500, Reward: [-336.628 -336.628 -336.628] [57.835], Avg: [-412.271 -412.271 -412.271] (0.1000) ({r_i: None, r_t: [-773.711 -773.711 -773.711], eps: 0.1})
Step:  145900, Reward: [-403.905 -403.905 -403.905] [98.145], Avg: [-465.377 -465.377 -465.377] (0.0100) ({r_i: None, r_t: [-786.498 -786.498 -786.498], eps: 0.01})
Step:  113600, Reward: [-377.696 -377.696 -377.696] [65.162], Avg: [-412.240 -412.240 -412.240] (0.1000) ({r_i: None, r_t: [-762.537 -762.537 -762.537], eps: 0.1})
Step:  146000, Reward: [-403.702 -403.702 -403.702] [64.071], Avg: [-465.335 -465.335 -465.335] (0.0100) ({r_i: None, r_t: [-778.651 -778.651 -778.651], eps: 0.01})
Step:  113700, Reward: [-381.802 -381.802 -381.802] [39.442], Avg: [-412.214 -412.214 -412.214] (0.1000) ({r_i: None, r_t: [-767.480 -767.480 -767.480], eps: 0.1})
Step:  146100, Reward: [-400.011 -400.011 -400.011] [85.571], Avg: [-465.290 -465.290 -465.290] (0.0100) ({r_i: None, r_t: [-818.369 -818.369 -818.369], eps: 0.01})
Step:  113800, Reward: [-394.195 -394.195 -394.195] [62.317], Avg: [-412.198 -412.198 -412.198] (0.1000) ({r_i: None, r_t: [-732.540 -732.540 -732.540], eps: 0.1})
Step:  146200, Reward: [-389.096 -389.096 -389.096] [83.480], Avg: [-465.238 -465.238 -465.238] (0.0100) ({r_i: None, r_t: [-744.046 -744.046 -744.046], eps: 0.01})
Step:  113900, Reward: [-372.297 -372.297 -372.297] [57.112], Avg: [-412.163 -412.163 -412.163] (0.1000) ({r_i: None, r_t: [-746.278 -746.278 -746.278], eps: 0.1})
Step:  146300, Reward: [-405.645 -405.645 -405.645] [85.859], Avg: [-465.197 -465.197 -465.197] (0.0100) ({r_i: None, r_t: [-802.282 -802.282 -802.282], eps: 0.01})
Step:  114000, Reward: [-372.895 -372.895 -372.895] [77.264], Avg: [-412.128 -412.128 -412.128] (0.1000) ({r_i: None, r_t: [-737.059 -737.059 -737.059], eps: 0.1})
Step:  146400, Reward: [-396.337 -396.337 -396.337] [55.117], Avg: [-465.150 -465.150 -465.150] (0.0100) ({r_i: None, r_t: [-793.726 -793.726 -793.726], eps: 0.01})
Step:  114100, Reward: [-381.872 -381.872 -381.872] [69.960], Avg: [-412.102 -412.102 -412.102] (0.1000) ({r_i: None, r_t: [-740.225 -740.225 -740.225], eps: 0.1})
Step:  146500, Reward: [-379.043 -379.043 -379.043] [71.801], Avg: [-465.092 -465.092 -465.092] (0.0100) ({r_i: None, r_t: [-839.384 -839.384 -839.384], eps: 0.01})
Step:  114200, Reward: [-388.759 -388.759 -388.759] [63.909], Avg: [-412.081 -412.081 -412.081] (0.1000) ({r_i: None, r_t: [-741.633 -741.633 -741.633], eps: 0.1})
Step:  146600, Reward: [-391.210 -391.210 -391.210] [84.259], Avg: [-465.041 -465.041 -465.041] (0.0100) ({r_i: None, r_t: [-811.972 -811.972 -811.972], eps: 0.01})
Step:  114300, Reward: [-358.585 -358.585 -358.585] [56.104], Avg: [-412.035 -412.035 -412.035] (0.1000) ({r_i: None, r_t: [-763.645 -763.645 -763.645], eps: 0.1})
Step:  146700, Reward: [-371.875 -371.875 -371.875] [68.176], Avg: [-464.978 -464.978 -464.978] (0.0100) ({r_i: None, r_t: [-794.429 -794.429 -794.429], eps: 0.01})
Step:  114400, Reward: [-356.281 -356.281 -356.281] [69.245], Avg: [-411.986 -411.986 -411.986] (0.1000) ({r_i: None, r_t: [-760.207 -760.207 -760.207], eps: 0.1})
Step:  146800, Reward: [-383.402 -383.402 -383.402] [86.561], Avg: [-464.922 -464.922 -464.922] (0.0100) ({r_i: None, r_t: [-791.553 -791.553 -791.553], eps: 0.01})
Step:  114500, Reward: [-365.733 -365.733 -365.733] [52.381], Avg: [-411.946 -411.946 -411.946] (0.1000) ({r_i: None, r_t: [-743.311 -743.311 -743.311], eps: 0.1})
Step:  146900, Reward: [-385.746 -385.746 -385.746] [70.472], Avg: [-464.868 -464.868 -464.868] (0.0100) ({r_i: None, r_t: [-812.665 -812.665 -812.665], eps: 0.01})
Step:  114600, Reward: [-381.744 -381.744 -381.744] [73.375], Avg: [-411.919 -411.919 -411.919] (0.1000) ({r_i: None, r_t: [-709.961 -709.961 -709.961], eps: 0.1})
Step:  147000, Reward: [-385.451 -385.451 -385.451] [71.381], Avg: [-464.814 -464.814 -464.814] (0.0100) ({r_i: None, r_t: [-746.353 -746.353 -746.353], eps: 0.01})
Step:  114700, Reward: [-359.323 -359.323 -359.323] [50.510], Avg: [-411.873 -411.873 -411.873] (0.1000) ({r_i: None, r_t: [-734.562 -734.562 -734.562], eps: 0.1})
Step:  147100, Reward: [-405.140 -405.140 -405.140] [89.925], Avg: [-464.774 -464.774 -464.774] (0.0100) ({r_i: None, r_t: [-815.049 -815.049 -815.049], eps: 0.01})
Step:  114800, Reward: [-377.877 -377.877 -377.877] [64.209], Avg: [-411.844 -411.844 -411.844] (0.1000) ({r_i: None, r_t: [-732.086 -732.086 -732.086], eps: 0.1})
Step:  147200, Reward: [-373.157 -373.157 -373.157] [51.582], Avg: [-464.712 -464.712 -464.712] (0.0100) ({r_i: None, r_t: [-809.698 -809.698 -809.698], eps: 0.01})
Step:  114900, Reward: [-364.314 -364.314 -364.314] [57.476], Avg: [-411.802 -411.802 -411.802] (0.1000) ({r_i: None, r_t: [-745.794 -745.794 -745.794], eps: 0.1})
Step:  147300, Reward: [-390.627 -390.627 -390.627] [67.510], Avg: [-464.661 -464.661 -464.661] (0.0100) ({r_i: None, r_t: [-802.487 -802.487 -802.487], eps: 0.01})
Step:  115000, Reward: [-377.425 -377.425 -377.425] [60.645], Avg: [-411.773 -411.773 -411.773] (0.1000) ({r_i: None, r_t: [-770.037 -770.037 -770.037], eps: 0.1})
Step:  147400, Reward: [-416.770 -416.770 -416.770] [75.647], Avg: [-464.629 -464.629 -464.629] (0.0100) ({r_i: None, r_t: [-823.851 -823.851 -823.851], eps: 0.01})
Step:  115100, Reward: [-367.318 -367.318 -367.318] [65.129], Avg: [-411.734 -411.734 -411.734] (0.1000) ({r_i: None, r_t: [-766.550 -766.550 -766.550], eps: 0.1})
Step:  147500, Reward: [-380.352 -380.352 -380.352] [72.467], Avg: [-464.572 -464.572 -464.572] (0.0100) ({r_i: None, r_t: [-810.321 -810.321 -810.321], eps: 0.01})
Step:  115200, Reward: [-362.315 -362.315 -362.315] [43.618], Avg: [-411.691 -411.691 -411.691] (0.1000) ({r_i: None, r_t: [-735.930 -735.930 -735.930], eps: 0.1})
Step:  147600, Reward: [-384.844 -384.844 -384.844] [73.696], Avg: [-464.518 -464.518 -464.518] (0.0100) ({r_i: None, r_t: [-794.485 -794.485 -794.485], eps: 0.01})
Step:  115300, Reward: [-395.405 -395.405 -395.405] [44.922], Avg: [-411.677 -411.677 -411.677] (0.1000) ({r_i: None, r_t: [-741.373 -741.373 -741.373], eps: 0.1})
Step:  147700, Reward: [-409.555 -409.555 -409.555] [94.461], Avg: [-464.481 -464.481 -464.481] (0.0100) ({r_i: None, r_t: [-844.576 -844.576 -844.576], eps: 0.01})
Step:  115400, Reward: [-377.479 -377.479 -377.479] [45.751], Avg: [-411.647 -411.647 -411.647] (0.1000) ({r_i: None, r_t: [-760.212 -760.212 -760.212], eps: 0.1})
Step:  147800, Reward: [-398.646 -398.646 -398.646] [78.371], Avg: [-464.436 -464.436 -464.436] (0.0100) ({r_i: None, r_t: [-763.913 -763.913 -763.913], eps: 0.01})
Step:  115500, Reward: [-381.452 -381.452 -381.452] [67.263], Avg: [-411.621 -411.621 -411.621] (0.1000) ({r_i: None, r_t: [-740.713 -740.713 -740.713], eps: 0.1})
Step:  147900, Reward: [-401.587 -401.587 -401.587] [56.417], Avg: [-464.394 -464.394 -464.394] (0.0100) ({r_i: None, r_t: [-832.195 -832.195 -832.195], eps: 0.01})
Step:  115600, Reward: [-370.729 -370.729 -370.729] [63.084], Avg: [-411.586 -411.586 -411.586] (0.1000) ({r_i: None, r_t: [-780.766 -780.766 -780.766], eps: 0.1})
Step:  148000, Reward: [-404.695 -404.695 -404.695] [81.635], Avg: [-464.353 -464.353 -464.353] (0.0100) ({r_i: None, r_t: [-882.228 -882.228 -882.228], eps: 0.01})
Step:  115700, Reward: [-386.865 -386.865 -386.865] [57.285], Avg: [-411.565 -411.565 -411.565] (0.1000) ({r_i: None, r_t: [-775.892 -775.892 -775.892], eps: 0.1})
Step:  148100, Reward: [-359.678 -359.678 -359.678] [75.191], Avg: [-464.283 -464.283 -464.283] (0.0100) ({r_i: None, r_t: [-797.664 -797.664 -797.664], eps: 0.01})
Step:  115800, Reward: [-380.019 -380.019 -380.019] [53.498], Avg: [-411.537 -411.537 -411.537] (0.1000) ({r_i: None, r_t: [-750.635 -750.635 -750.635], eps: 0.1})
Step:  148200, Reward: [-430.137 -430.137 -430.137] [64.453], Avg: [-464.260 -464.260 -464.260] (0.0100) ({r_i: None, r_t: [-807.436 -807.436 -807.436], eps: 0.01})
Step:  115900, Reward: [-376.720 -376.720 -376.720] [70.383], Avg: [-411.507 -411.507 -411.507] (0.1000) ({r_i: None, r_t: [-795.354 -795.354 -795.354], eps: 0.1})
Step:  148300, Reward: [-407.741 -407.741 -407.741] [85.172], Avg: [-464.222 -464.222 -464.222] (0.0100) ({r_i: None, r_t: [-800.073 -800.073 -800.073], eps: 0.01})
Step:  116000, Reward: [-375.327 -375.327 -375.327] [54.944], Avg: [-411.476 -411.476 -411.476] (0.1000) ({r_i: None, r_t: [-742.887 -742.887 -742.887], eps: 0.1})
Step:  148400, Reward: [-380.515 -380.515 -380.515] [109.290], Avg: [-464.165 -464.165 -464.165] (0.0100) ({r_i: None, r_t: [-783.642 -783.642 -783.642], eps: 0.01})
Step:  116100, Reward: [-371.770 -371.770 -371.770] [54.536], Avg: [-411.442 -411.442 -411.442] (0.1000) ({r_i: None, r_t: [-750.808 -750.808 -750.808], eps: 0.1})
Step:  148500, Reward: [-412.181 -412.181 -412.181] [60.482], Avg: [-464.130 -464.130 -464.130] (0.0100) ({r_i: None, r_t: [-773.856 -773.856 -773.856], eps: 0.01})
Step:  116200, Reward: [-386.032 -386.032 -386.032] [47.602], Avg: [-411.420 -411.420 -411.420] (0.1000) ({r_i: None, r_t: [-740.243 -740.243 -740.243], eps: 0.1})
Step:  148600, Reward: [-410.125 -410.125 -410.125] [51.409], Avg: [-464.094 -464.094 -464.094] (0.0100) ({r_i: None, r_t: [-734.854 -734.854 -734.854], eps: 0.01})
Step:  116300, Reward: [-367.599 -367.599 -367.599] [54.497], Avg: [-411.383 -411.383 -411.383] (0.1000) ({r_i: None, r_t: [-777.175 -777.175 -777.175], eps: 0.1})
Step:  148700, Reward: [-384.394 -384.394 -384.394] [81.701], Avg: [-464.040 -464.040 -464.040] (0.0100) ({r_i: None, r_t: [-821.680 -821.680 -821.680], eps: 0.01})
Step:  116400, Reward: [-400.537 -400.537 -400.537] [66.428], Avg: [-411.373 -411.373 -411.373] (0.1000) ({r_i: None, r_t: [-783.252 -783.252 -783.252], eps: 0.1})
Step:  148800, Reward: [-391.256 -391.256 -391.256] [59.815], Avg: [-463.991 -463.991 -463.991] (0.0100) ({r_i: None, r_t: [-786.809 -786.809 -786.809], eps: 0.01})
Step:  116500, Reward: [-387.064 -387.064 -387.064] [65.461], Avg: [-411.352 -411.352 -411.352] (0.1000) ({r_i: None, r_t: [-754.169 -754.169 -754.169], eps: 0.1})
Step:  148900, Reward: [-418.203 -418.203 -418.203] [75.239], Avg: [-463.961 -463.961 -463.961] (0.0100) ({r_i: None, r_t: [-779.310 -779.310 -779.310], eps: 0.01})
Step:  116600, Reward: [-374.915 -374.915 -374.915] [45.985], Avg: [-411.321 -411.321 -411.321] (0.1000) ({r_i: None, r_t: [-758.913 -758.913 -758.913], eps: 0.1})
Step:  149000, Reward: [-392.021 -392.021 -392.021] [70.830], Avg: [-463.912 -463.912 -463.912] (0.0100) ({r_i: None, r_t: [-812.044 -812.044 -812.044], eps: 0.01})
Step:  116700, Reward: [-375.747 -375.747 -375.747] [68.373], Avg: [-411.291 -411.291 -411.291] (0.1000) ({r_i: None, r_t: [-807.247 -807.247 -807.247], eps: 0.1})
Step:  149100, Reward: [-393.805 -393.805 -393.805] [68.056], Avg: [-463.865 -463.865 -463.865] (0.0100) ({r_i: None, r_t: [-847.380 -847.380 -847.380], eps: 0.01})
Step:  116800, Reward: [-366.212 -366.212 -366.212] [66.712], Avg: [-411.252 -411.252 -411.252] (0.1000) ({r_i: None, r_t: [-762.270 -762.270 -762.270], eps: 0.1})
Step:  149200, Reward: [-393.165 -393.165 -393.165] [77.800], Avg: [-463.818 -463.818 -463.818] (0.0100) ({r_i: None, r_t: [-788.085 -788.085 -788.085], eps: 0.01})
Step:  116900, Reward: [-375.299 -375.299 -375.299] [53.088], Avg: [-411.221 -411.221 -411.221] (0.1000) ({r_i: None, r_t: [-787.263 -787.263 -787.263], eps: 0.1})
Step:  149300, Reward: [-374.890 -374.890 -374.890] [66.765], Avg: [-463.759 -463.759 -463.759] (0.0100) ({r_i: None, r_t: [-742.738 -742.738 -742.738], eps: 0.01})
Step:  117000, Reward: [-366.333 -366.333 -366.333] [69.600], Avg: [-411.183 -411.183 -411.183] (0.1000) ({r_i: None, r_t: [-774.600 -774.600 -774.600], eps: 0.1})
Step:  149400, Reward: [-388.412 -388.412 -388.412] [52.607], Avg: [-463.708 -463.708 -463.708] (0.0100) ({r_i: None, r_t: [-790.868 -790.868 -790.868], eps: 0.01})
Step:  117100, Reward: [-386.473 -386.473 -386.473] [65.915], Avg: [-411.162 -411.162 -411.162] (0.1000) ({r_i: None, r_t: [-767.807 -767.807 -767.807], eps: 0.1})
Step:  149500, Reward: [-416.496 -416.496 -416.496] [67.075], Avg: [-463.677 -463.677 -463.677] (0.0100) ({r_i: None, r_t: [-790.532 -790.532 -790.532], eps: 0.01})
Step:  117200, Reward: [-396.111 -396.111 -396.111] [60.376], Avg: [-411.149 -411.149 -411.149] (0.1000) ({r_i: None, r_t: [-819.566 -819.566 -819.566], eps: 0.1})
Step:  149600, Reward: [-409.460 -409.460 -409.460] [82.226], Avg: [-463.640 -463.640 -463.640] (0.0100) ({r_i: None, r_t: [-807.330 -807.330 -807.330], eps: 0.01})
Step:  117300, Reward: [-373.482 -373.482 -373.482] [57.713], Avg: [-411.117 -411.117 -411.117] (0.1000) ({r_i: None, r_t: [-775.557 -775.557 -775.557], eps: 0.1})
Step:  149700, Reward: [-376.484 -376.484 -376.484] [63.834], Avg: [-463.582 -463.582 -463.582] (0.0100) ({r_i: None, r_t: [-840.112 -840.112 -840.112], eps: 0.01})
Step:  117400, Reward: [-393.179 -393.179 -393.179] [77.218], Avg: [-411.102 -411.102 -411.102] (0.1000) ({r_i: None, r_t: [-773.264 -773.264 -773.264], eps: 0.1})
Step:  149800, Reward: [-401.120 -401.120 -401.120] [53.313], Avg: [-463.541 -463.541 -463.541] (0.0100) ({r_i: None, r_t: [-779.065 -779.065 -779.065], eps: 0.01})
Step:  117500, Reward: [-334.271 -334.271 -334.271] [90.390], Avg: [-411.036 -411.036 -411.036] (0.1000) ({r_i: None, r_t: [-801.560 -801.560 -801.560], eps: 0.1})
Step:  149900, Reward: [-390.022 -390.022 -390.022] [88.201], Avg: [-463.492 -463.492 -463.492] (0.0100) ({r_i: None, r_t: [-772.948 -772.948 -772.948], eps: 0.01})
Step:  117600, Reward: [-360.607 -360.607 -360.607] [50.343], Avg: [-410.994 -410.994 -410.994] (0.1000) ({r_i: None, r_t: [-782.404 -782.404 -782.404], eps: 0.1})
Step:  150000, Reward: [-398.163 -398.163 -398.163] [47.822], Avg: [-463.448 -463.448 -463.448] (0.0100) ({r_i: None, r_t: [-833.056 -833.056 -833.056], eps: 0.01})
Step:  117700, Reward: [-407.676 -407.676 -407.676] [66.960], Avg: [-410.991 -410.991 -410.991] (0.1000) ({r_i: None, r_t: [-759.019 -759.019 -759.019], eps: 0.1})
Step:  150100, Reward: [-387.182 -387.182 -387.182] [57.500], Avg: [-463.397 -463.397 -463.397] (0.0100) ({r_i: None, r_t: [-820.800 -820.800 -820.800], eps: 0.01})
Step:  117800, Reward: [-389.929 -389.929 -389.929] [83.462], Avg: [-410.973 -410.973 -410.973] (0.1000) ({r_i: None, r_t: [-787.473 -787.473 -787.473], eps: 0.1})
Step:  150200, Reward: [-398.315 -398.315 -398.315] [67.139], Avg: [-463.354 -463.354 -463.354] (0.0100) ({r_i: None, r_t: [-827.062 -827.062 -827.062], eps: 0.01})
Step:  117900, Reward: [-374.822 -374.822 -374.822] [63.683], Avg: [-410.942 -410.942 -410.942] (0.1000) ({r_i: None, r_t: [-774.620 -774.620 -774.620], eps: 0.1})
Step:  150300, Reward: [-354.450 -354.450 -354.450] [63.351], Avg: [-463.282 -463.282 -463.282] (0.0100) ({r_i: None, r_t: [-778.721 -778.721 -778.721], eps: 0.01})
Step:  118000, Reward: [-380.596 -380.596 -380.596] [63.216], Avg: [-410.917 -410.917 -410.917] (0.1000) ({r_i: None, r_t: [-754.264 -754.264 -754.264], eps: 0.1})
Step:  150400, Reward: [-436.751 -436.751 -436.751] [90.280], Avg: [-463.264 -463.264 -463.264] (0.0100) ({r_i: None, r_t: [-773.235 -773.235 -773.235], eps: 0.01})
Step:  118100, Reward: [-379.133 -379.133 -379.133] [60.195], Avg: [-410.890 -410.890 -410.890] (0.1000) ({r_i: None, r_t: [-751.902 -751.902 -751.902], eps: 0.1})
Step:  150500, Reward: [-393.082 -393.082 -393.082] [66.394], Avg: [-463.217 -463.217 -463.217] (0.0100) ({r_i: None, r_t: [-806.652 -806.652 -806.652], eps: 0.01})
Step:  118200, Reward: [-370.658 -370.658 -370.658] [59.304], Avg: [-410.856 -410.856 -410.856] (0.1000) ({r_i: None, r_t: [-777.099 -777.099 -777.099], eps: 0.1})
Step:  150600, Reward: [-363.549 -363.549 -363.549] [66.111], Avg: [-463.151 -463.151 -463.151] (0.0100) ({r_i: None, r_t: [-801.708 -801.708 -801.708], eps: 0.01})
Step:  118300, Reward: [-396.078 -396.078 -396.078] [49.466], Avg: [-410.843 -410.843 -410.843] (0.1000) ({r_i: None, r_t: [-764.506 -764.506 -764.506], eps: 0.1})
Step:  150700, Reward: [-400.719 -400.719 -400.719] [73.679], Avg: [-463.110 -463.110 -463.110] (0.0100) ({r_i: None, r_t: [-794.038 -794.038 -794.038], eps: 0.01})
Step:  118400, Reward: [-352.123 -352.123 -352.123] [40.300], Avg: [-410.794 -410.794 -410.794] (0.1000) ({r_i: None, r_t: [-736.082 -736.082 -736.082], eps: 0.1})
Step:  150800, Reward: [-416.467 -416.467 -416.467] [85.053], Avg: [-463.079 -463.079 -463.079] (0.0100) ({r_i: None, r_t: [-796.653 -796.653 -796.653], eps: 0.01})
Step:  118500, Reward: [-384.765 -384.765 -384.765] [48.665], Avg: [-410.772 -410.772 -410.772] (0.1000) ({r_i: None, r_t: [-763.676 -763.676 -763.676], eps: 0.1})
Step:  150900, Reward: [-397.441 -397.441 -397.441] [71.947], Avg: [-463.035 -463.035 -463.035] (0.0100) ({r_i: None, r_t: [-803.009 -803.009 -803.009], eps: 0.01})
Step:  118600, Reward: [-389.182 -389.182 -389.182] [79.111], Avg: [-410.754 -410.754 -410.754] (0.1000) ({r_i: None, r_t: [-790.428 -790.428 -790.428], eps: 0.1})
Step:  151000, Reward: [-412.693 -412.693 -412.693] [97.394], Avg: [-463.002 -463.002 -463.002] (0.0100) ({r_i: None, r_t: [-779.330 -779.330 -779.330], eps: 0.01})
Step:  118700, Reward: [-372.322 -372.322 -372.322] [64.707], Avg: [-410.721 -410.721 -410.721] (0.1000) ({r_i: None, r_t: [-777.685 -777.685 -777.685], eps: 0.1})
Step:  151100, Reward: [-380.501 -380.501 -380.501] [49.188], Avg: [-462.948 -462.948 -462.948] (0.0100) ({r_i: None, r_t: [-823.042 -823.042 -823.042], eps: 0.01})
Step:  118800, Reward: [-385.966 -385.966 -385.966] [74.332], Avg: [-410.700 -410.700 -410.700] (0.1000) ({r_i: None, r_t: [-769.774 -769.774 -769.774], eps: 0.1})
Step:  151200, Reward: [-409.412 -409.412 -409.412] [86.332], Avg: [-462.912 -462.912 -462.912] (0.0100) ({r_i: None, r_t: [-807.691 -807.691 -807.691], eps: 0.01})
Step:  118900, Reward: [-406.535 -406.535 -406.535] [73.558], Avg: [-410.697 -410.697 -410.697] (0.1000) ({r_i: None, r_t: [-762.125 -762.125 -762.125], eps: 0.1})
Step:  151300, Reward: [-397.886 -397.886 -397.886] [79.850], Avg: [-462.869 -462.869 -462.869] (0.0100) ({r_i: None, r_t: [-816.211 -816.211 -816.211], eps: 0.01})
Step:  119000, Reward: [-385.100 -385.100 -385.100] [78.915], Avg: [-410.675 -410.675 -410.675] (0.1000) ({r_i: None, r_t: [-782.041 -782.041 -782.041], eps: 0.1})
Step:  151400, Reward: [-390.817 -390.817 -390.817] [77.354], Avg: [-462.822 -462.822 -462.822] (0.0100) ({r_i: None, r_t: [-831.612 -831.612 -831.612], eps: 0.01})
Step:  119100, Reward: [-371.903 -371.903 -371.903] [51.098], Avg: [-410.643 -410.643 -410.643] (0.1000) ({r_i: None, r_t: [-733.425 -733.425 -733.425], eps: 0.1})
Step:  151500, Reward: [-402.104 -402.104 -402.104] [71.493], Avg: [-462.782 -462.782 -462.782] (0.0100) ({r_i: None, r_t: [-800.755 -800.755 -800.755], eps: 0.01})
Step:  119200, Reward: [-372.808 -372.808 -372.808] [47.995], Avg: [-410.611 -410.611 -410.611] (0.1000) ({r_i: None, r_t: [-766.764 -766.764 -766.764], eps: 0.1})
Step:  151600, Reward: [-383.969 -383.969 -383.969] [64.018], Avg: [-462.730 -462.730 -462.730] (0.0100) ({r_i: None, r_t: [-771.250 -771.250 -771.250], eps: 0.01})
Step:  119300, Reward: [-395.418 -395.418 -395.418] [84.331], Avg: [-410.598 -410.598 -410.598] (0.1000) ({r_i: None, r_t: [-806.090 -806.090 -806.090], eps: 0.1})
Step:  151700, Reward: [-388.815 -388.815 -388.815] [75.871], Avg: [-462.681 -462.681 -462.681] (0.0100) ({r_i: None, r_t: [-793.857 -793.857 -793.857], eps: 0.01})
Step:  119400, Reward: [-360.288 -360.288 -360.288] [50.819], Avg: [-410.556 -410.556 -410.556] (0.1000) ({r_i: None, r_t: [-788.244 -788.244 -788.244], eps: 0.1})
Step:  151800, Reward: [-361.239 -361.239 -361.239] [53.491], Avg: [-462.614 -462.614 -462.614] (0.0100) ({r_i: None, r_t: [-797.996 -797.996 -797.996], eps: 0.01})
Step:  119500, Reward: [-377.189 -377.189 -377.189] [74.660], Avg: [-410.528 -410.528 -410.528] (0.1000) ({r_i: None, r_t: [-747.139 -747.139 -747.139], eps: 0.1})
Step:  151900, Reward: [-391.785 -391.785 -391.785] [66.214], Avg: [-462.568 -462.568 -462.568] (0.0100) ({r_i: None, r_t: [-784.329 -784.329 -784.329], eps: 0.01})
Step:  119600, Reward: [-392.132 -392.132 -392.132] [56.436], Avg: [-410.513 -410.513 -410.513] (0.1000) ({r_i: None, r_t: [-741.741 -741.741 -741.741], eps: 0.1})
Step:  152000, Reward: [-404.552 -404.552 -404.552] [83.367], Avg: [-462.529 -462.529 -462.529] (0.0100) ({r_i: None, r_t: [-829.052 -829.052 -829.052], eps: 0.01})
Step:  119700, Reward: [-367.885 -367.885 -367.885] [56.223], Avg: [-410.477 -410.477 -410.477] (0.1000) ({r_i: None, r_t: [-777.853 -777.853 -777.853], eps: 0.1})
Step:  152100, Reward: [-393.013 -393.013 -393.013] [74.813], Avg: [-462.484 -462.484 -462.484] (0.0100) ({r_i: None, r_t: [-828.644 -828.644 -828.644], eps: 0.01})
Step:  119800, Reward: [-362.372 -362.372 -362.372] [53.898], Avg: [-410.437 -410.437 -410.437] (0.1000) ({r_i: None, r_t: [-768.460 -768.460 -768.460], eps: 0.1})
Step:  152200, Reward: [-370.732 -370.732 -370.732] [61.510], Avg: [-462.423 -462.423 -462.423] (0.0100) ({r_i: None, r_t: [-839.527 -839.527 -839.527], eps: 0.01})
Step:  119900, Reward: [-409.666 -409.666 -409.666] [57.056], Avg: [-410.437 -410.437 -410.437] (0.1000) ({r_i: None, r_t: [-787.465 -787.465 -787.465], eps: 0.1})
Step:  152300, Reward: [-396.309 -396.309 -396.309] [70.291], Avg: [-462.380 -462.380 -462.380] (0.0100) ({r_i: None, r_t: [-811.552 -811.552 -811.552], eps: 0.01})
Step:  120000, Reward: [-365.706 -365.706 -365.706] [80.753], Avg: [-410.399 -410.399 -410.399] (0.1000) ({r_i: None, r_t: [-760.198 -760.198 -760.198], eps: 0.1})
Step:  152400, Reward: [-382.337 -382.337 -382.337] [65.959], Avg: [-462.328 -462.328 -462.328] (0.0100) ({r_i: None, r_t: [-795.326 -795.326 -795.326], eps: 0.01})
Step:  120100, Reward: [-354.839 -354.839 -354.839] [60.230], Avg: [-410.353 -410.353 -410.353] (0.1000) ({r_i: None, r_t: [-791.158 -791.158 -791.158], eps: 0.1})
Step:  152500, Reward: [-386.148 -386.148 -386.148] [50.098], Avg: [-462.278 -462.278 -462.278] (0.0100) ({r_i: None, r_t: [-803.287 -803.287 -803.287], eps: 0.01})
Step:  120200, Reward: [-396.521 -396.521 -396.521] [97.459], Avg: [-410.342 -410.342 -410.342] (0.1000) ({r_i: None, r_t: [-742.795 -742.795 -742.795], eps: 0.1})
Step:  152600, Reward: [-383.745 -383.745 -383.745] [58.225], Avg: [-462.226 -462.226 -462.226] (0.0100) ({r_i: None, r_t: [-777.981 -777.981 -777.981], eps: 0.01})
Step:  120300, Reward: [-404.733 -404.733 -404.733] [59.655], Avg: [-410.337 -410.337 -410.337] (0.1000) ({r_i: None, r_t: [-772.882 -772.882 -772.882], eps: 0.1})
Step:  152700, Reward: [-413.037 -413.037 -413.037] [60.565], Avg: [-462.194 -462.194 -462.194] (0.0100) ({r_i: None, r_t: [-782.217 -782.217 -782.217], eps: 0.01})
Step:  120400, Reward: [-405.844 -405.844 -405.844] [75.312], Avg: [-410.333 -410.333 -410.333] (0.1000) ({r_i: None, r_t: [-780.012 -780.012 -780.012], eps: 0.1})
Step:  152800, Reward: [-404.642 -404.642 -404.642] [88.682], Avg: [-462.156 -462.156 -462.156] (0.0100) ({r_i: None, r_t: [-781.538 -781.538 -781.538], eps: 0.01})
Step:  120500, Reward: [-393.974 -393.974 -393.974] [59.922], Avg: [-410.320 -410.320 -410.320] (0.1000) ({r_i: None, r_t: [-729.309 -729.309 -729.309], eps: 0.1})
Step:  152900, Reward: [-415.795 -415.795 -415.795] [56.086], Avg: [-462.126 -462.126 -462.126] (0.0100) ({r_i: None, r_t: [-788.632 -788.632 -788.632], eps: 0.01})
Step:  120600, Reward: [-374.230 -374.230 -374.230] [53.669], Avg: [-410.290 -410.290 -410.290] (0.1000) ({r_i: None, r_t: [-766.009 -766.009 -766.009], eps: 0.1})
Step:  153000, Reward: [-409.073 -409.073 -409.073] [75.479], Avg: [-462.091 -462.091 -462.091] (0.0100) ({r_i: None, r_t: [-818.723 -818.723 -818.723], eps: 0.01})
Step:  120700, Reward: [-381.116 -381.116 -381.116] [51.832], Avg: [-410.266 -410.266 -410.266] (0.1000) ({r_i: None, r_t: [-800.204 -800.204 -800.204], eps: 0.1})
Step:  153100, Reward: [-373.906 -373.906 -373.906] [67.661], Avg: [-462.034 -462.034 -462.034] (0.0100) ({r_i: None, r_t: [-771.904 -771.904 -771.904], eps: 0.01})
Step:  120800, Reward: [-395.169 -395.169 -395.169] [76.235], Avg: [-410.253 -410.253 -410.253] (0.1000) ({r_i: None, r_t: [-770.599 -770.599 -770.599], eps: 0.1})
Step:  153200, Reward: [-435.016 -435.016 -435.016] [92.621], Avg: [-462.016 -462.016 -462.016] (0.0100) ({r_i: None, r_t: [-774.551 -774.551 -774.551], eps: 0.01})
Step:  120900, Reward: [-380.344 -380.344 -380.344] [61.269], Avg: [-410.229 -410.229 -410.229] (0.1000) ({r_i: None, r_t: [-782.878 -782.878 -782.878], eps: 0.1})
Step:  153300, Reward: [-393.763 -393.763 -393.763] [67.525], Avg: [-461.972 -461.972 -461.972] (0.0100) ({r_i: None, r_t: [-839.387 -839.387 -839.387], eps: 0.01})
Step:  121000, Reward: [-404.677 -404.677 -404.677] [57.986], Avg: [-410.224 -410.224 -410.224] (0.1000) ({r_i: None, r_t: [-785.014 -785.014 -785.014], eps: 0.1})
Step:  153400, Reward: [-419.226 -419.226 -419.226] [80.106], Avg: [-461.944 -461.944 -461.944] (0.0100) ({r_i: None, r_t: [-739.299 -739.299 -739.299], eps: 0.01})
Step:  121100, Reward: [-393.049 -393.049 -393.049] [94.903], Avg: [-410.210 -410.210 -410.210] (0.1000) ({r_i: None, r_t: [-746.747 -746.747 -746.747], eps: 0.1})
Step:  153500, Reward: [-424.008 -424.008 -424.008] [101.052], Avg: [-461.919 -461.919 -461.919] (0.0100) ({r_i: None, r_t: [-807.931 -807.931 -807.931], eps: 0.01})
Step:  121200, Reward: [-391.229 -391.229 -391.229] [60.514], Avg: [-410.194 -410.194 -410.194] (0.1000) ({r_i: None, r_t: [-769.288 -769.288 -769.288], eps: 0.1})
Step:  153600, Reward: [-353.877 -353.877 -353.877] [71.808], Avg: [-461.849 -461.849 -461.849] (0.0100) ({r_i: None, r_t: [-755.954 -755.954 -755.954], eps: 0.01})
Step:  121300, Reward: [-387.266 -387.266 -387.266] [70.459], Avg: [-410.175 -410.175 -410.175] (0.1000) ({r_i: None, r_t: [-815.220 -815.220 -815.220], eps: 0.1})
Step:  153700, Reward: [-427.927 -427.927 -427.927] [72.967], Avg: [-461.827 -461.827 -461.827] (0.0100) ({r_i: None, r_t: [-798.125 -798.125 -798.125], eps: 0.01})
Step:  121400, Reward: [-366.071 -366.071 -366.071] [63.281], Avg: [-410.139 -410.139 -410.139] (0.1000) ({r_i: None, r_t: [-786.996 -786.996 -786.996], eps: 0.1})
Step:  153800, Reward: [-391.268 -391.268 -391.268] [81.155], Avg: [-461.781 -461.781 -461.781] (0.0100) ({r_i: None, r_t: [-785.746 -785.746 -785.746], eps: 0.01})
Step:  121500, Reward: [-396.197 -396.197 -396.197] [69.215], Avg: [-410.127 -410.127 -410.127] (0.1000) ({r_i: None, r_t: [-739.344 -739.344 -739.344], eps: 0.1})
Step:  153900, Reward: [-394.919 -394.919 -394.919] [61.973], Avg: [-461.738 -461.738 -461.738] (0.0100) ({r_i: None, r_t: [-777.122 -777.122 -777.122], eps: 0.01})
Step:  121600, Reward: [-399.322 -399.322 -399.322] [66.699], Avg: [-410.119 -410.119 -410.119] (0.1000) ({r_i: None, r_t: [-748.552 -748.552 -748.552], eps: 0.1})
Step:  154000, Reward: [-400.982 -400.982 -400.982] [73.094], Avg: [-461.698 -461.698 -461.698] (0.0100) ({r_i: None, r_t: [-816.106 -816.106 -816.106], eps: 0.01})
Step:  121700, Reward: [-396.456 -396.456 -396.456] [49.493], Avg: [-410.107 -410.107 -410.107] (0.1000) ({r_i: None, r_t: [-797.968 -797.968 -797.968], eps: 0.1})
Step:  154100, Reward: [-389.393 -389.393 -389.393] [40.715], Avg: [-461.651 -461.651 -461.651] (0.0100) ({r_i: None, r_t: [-793.842 -793.842 -793.842], eps: 0.01})
Step:  121800, Reward: [-374.921 -374.921 -374.921] [50.670], Avg: [-410.079 -410.079 -410.079] (0.1000) ({r_i: None, r_t: [-815.923 -815.923 -815.923], eps: 0.1})
Step:  154200, Reward: [-426.329 -426.329 -426.329] [72.629], Avg: [-461.628 -461.628 -461.628] (0.0100) ({r_i: None, r_t: [-844.232 -844.232 -844.232], eps: 0.01})
Step:  121900, Reward: [-403.990 -403.990 -403.990] [84.240], Avg: [-410.074 -410.074 -410.074] (0.1000) ({r_i: None, r_t: [-784.793 -784.793 -784.793], eps: 0.1})
Step:  154300, Reward: [-375.843 -375.843 -375.843] [67.715], Avg: [-461.573 -461.573 -461.573] (0.0100) ({r_i: None, r_t: [-811.174 -811.174 -811.174], eps: 0.01})
Step:  122000, Reward: [-397.394 -397.394 -397.394] [65.738], Avg: [-410.063 -410.063 -410.063] (0.1000) ({r_i: None, r_t: [-740.439 -740.439 -740.439], eps: 0.1})
Step:  154400, Reward: [-423.285 -423.285 -423.285] [87.810], Avg: [-461.548 -461.548 -461.548] (0.0100) ({r_i: None, r_t: [-770.221 -770.221 -770.221], eps: 0.01})
Step:  122100, Reward: [-388.024 -388.024 -388.024] [77.339], Avg: [-410.045 -410.045 -410.045] (0.1000) ({r_i: None, r_t: [-785.444 -785.444 -785.444], eps: 0.1})
Step:  154500, Reward: [-429.738 -429.738 -429.738] [75.576], Avg: [-461.527 -461.527 -461.527] (0.0100) ({r_i: None, r_t: [-791.360 -791.360 -791.360], eps: 0.01})
Step:  122200, Reward: [-414.757 -414.757 -414.757] [89.874], Avg: [-410.049 -410.049 -410.049] (0.1000) ({r_i: None, r_t: [-777.427 -777.427 -777.427], eps: 0.1})
Step:  154600, Reward: [-396.849 -396.849 -396.849] [57.583], Avg: [-461.486 -461.486 -461.486] (0.0100) ({r_i: None, r_t: [-832.113 -832.113 -832.113], eps: 0.01})
Step:  122300, Reward: [-385.092 -385.092 -385.092] [64.929], Avg: [-410.029 -410.029 -410.029] (0.1000) ({r_i: None, r_t: [-785.865 -785.865 -785.865], eps: 0.1})
Step:  154700, Reward: [-392.386 -392.386 -392.386] [82.517], Avg: [-461.441 -461.441 -461.441] (0.0100) ({r_i: None, r_t: [-787.013 -787.013 -787.013], eps: 0.01})
Step:  122400, Reward: [-398.685 -398.685 -398.685] [61.085], Avg: [-410.019 -410.019 -410.019] (0.1000) ({r_i: None, r_t: [-771.730 -771.730 -771.730], eps: 0.1})
Step:  154800, Reward: [-391.004 -391.004 -391.004] [91.334], Avg: [-461.396 -461.396 -461.396] (0.0100) ({r_i: None, r_t: [-739.154 -739.154 -739.154], eps: 0.01})
Step:  122500, Reward: [-382.988 -382.988 -382.988] [71.560], Avg: [-409.997 -409.997 -409.997] (0.1000) ({r_i: None, r_t: [-765.847 -765.847 -765.847], eps: 0.1})
Step:  154900, Reward: [-398.705 -398.705 -398.705] [99.291], Avg: [-461.355 -461.355 -461.355] (0.0100) ({r_i: None, r_t: [-806.287 -806.287 -806.287], eps: 0.01})
Step:  122600, Reward: [-392.071 -392.071 -392.071] [61.360], Avg: [-409.983 -409.983 -409.983] (0.1000) ({r_i: None, r_t: [-806.004 -806.004 -806.004], eps: 0.1})
Step:  155000, Reward: [-369.320 -369.320 -369.320] [63.084], Avg: [-461.296 -461.296 -461.296] (0.0100) ({r_i: None, r_t: [-801.099 -801.099 -801.099], eps: 0.01})
Step:  122700, Reward: [-398.373 -398.373 -398.373] [56.708], Avg: [-409.973 -409.973 -409.973] (0.1000) ({r_i: None, r_t: [-783.782 -783.782 -783.782], eps: 0.1})
Step:  155100, Reward: [-384.594 -384.594 -384.594] [72.616], Avg: [-461.246 -461.246 -461.246] (0.0100) ({r_i: None, r_t: [-808.502 -808.502 -808.502], eps: 0.01})
Step:  122800, Reward: [-446.056 -446.056 -446.056] [70.539], Avg: [-410.003 -410.003 -410.003] (0.1000) ({r_i: None, r_t: [-787.289 -787.289 -787.289], eps: 0.1})
Step:  155200, Reward: [-412.772 -412.772 -412.772] [60.446], Avg: [-461.215 -461.215 -461.215] (0.0100) ({r_i: None, r_t: [-786.266 -786.266 -786.266], eps: 0.01})
Step:  122900, Reward: [-352.422 -352.422 -352.422] [48.963], Avg: [-409.956 -409.956 -409.956] (0.1000) ({r_i: None, r_t: [-784.080 -784.080 -784.080], eps: 0.1})
Step:  155300, Reward: [-406.321 -406.321 -406.321] [69.159], Avg: [-461.180 -461.180 -461.180] (0.0100) ({r_i: None, r_t: [-778.871 -778.871 -778.871], eps: 0.01})
Step:  123000, Reward: [-400.788 -400.788 -400.788] [67.024], Avg: [-409.948 -409.948 -409.948] (0.1000) ({r_i: None, r_t: [-763.658 -763.658 -763.658], eps: 0.1})
Step:  155400, Reward: [-390.541 -390.541 -390.541] [77.583], Avg: [-461.134 -461.134 -461.134] (0.0100) ({r_i: None, r_t: [-821.790 -821.790 -821.790], eps: 0.01})
Step:  123100, Reward: [-375.396 -375.396 -375.396] [48.036], Avg: [-409.920 -409.920 -409.920] (0.1000) ({r_i: None, r_t: [-780.571 -780.571 -780.571], eps: 0.1})
Step:  155500, Reward: [-393.278 -393.278 -393.278] [76.009], Avg: [-461.091 -461.091 -461.091] (0.0100) ({r_i: None, r_t: [-750.874 -750.874 -750.874], eps: 0.01})
Step:  123200, Reward: [-369.625 -369.625 -369.625] [37.178], Avg: [-409.888 -409.888 -409.888] (0.1000) ({r_i: None, r_t: [-786.245 -786.245 -786.245], eps: 0.1})
Step:  155600, Reward: [-398.755 -398.755 -398.755] [99.349], Avg: [-461.051 -461.051 -461.051] (0.0100) ({r_i: None, r_t: [-814.678 -814.678 -814.678], eps: 0.01})
Step:  123300, Reward: [-385.949 -385.949 -385.949] [44.155], Avg: [-409.868 -409.868 -409.868] (0.1000) ({r_i: None, r_t: [-806.682 -806.682 -806.682], eps: 0.1})
Step:  155700, Reward: [-428.797 -428.797 -428.797] [73.732], Avg: [-461.030 -461.030 -461.030] (0.0100) ({r_i: None, r_t: [-777.585 -777.585 -777.585], eps: 0.01})
Step:  123400, Reward: [-384.599 -384.599 -384.599] [68.506], Avg: [-409.848 -409.848 -409.848] (0.1000) ({r_i: None, r_t: [-783.245 -783.245 -783.245], eps: 0.1})
Step:  155800, Reward: [-393.347 -393.347 -393.347] [72.263], Avg: [-460.987 -460.987 -460.987] (0.0100) ({r_i: None, r_t: [-805.573 -805.573 -805.573], eps: 0.01})
Step:  123500, Reward: [-404.064 -404.064 -404.064] [57.451], Avg: [-409.843 -409.843 -409.843] (0.1000) ({r_i: None, r_t: [-743.415 -743.415 -743.415], eps: 0.1})
Step:  155900, Reward: [-377.144 -377.144 -377.144] [68.268], Avg: [-460.933 -460.933 -460.933] (0.0100) ({r_i: None, r_t: [-828.375 -828.375 -828.375], eps: 0.01})
Step:  123600, Reward: [-412.883 -412.883 -412.883] [62.353], Avg: [-409.846 -409.846 -409.846] (0.1000) ({r_i: None, r_t: [-776.460 -776.460 -776.460], eps: 0.1})
Step:  156000, Reward: [-392.850 -392.850 -392.850] [72.193], Avg: [-460.889 -460.889 -460.889] (0.0100) ({r_i: None, r_t: [-801.501 -801.501 -801.501], eps: 0.01})
Step:  123700, Reward: [-371.979 -371.979 -371.979] [66.354], Avg: [-409.815 -409.815 -409.815] (0.1000) ({r_i: None, r_t: [-818.781 -818.781 -818.781], eps: 0.1})
Step:  156100, Reward: [-381.844 -381.844 -381.844] [82.052], Avg: [-460.839 -460.839 -460.839] (0.0100) ({r_i: None, r_t: [-875.244 -875.244 -875.244], eps: 0.01})
Step:  123800, Reward: [-381.152 -381.152 -381.152] [61.240], Avg: [-409.792 -409.792 -409.792] (0.1000) ({r_i: None, r_t: [-791.198 -791.198 -791.198], eps: 0.1})
Step:  156200, Reward: [-395.733 -395.733 -395.733] [79.774], Avg: [-460.797 -460.797 -460.797] (0.0100) ({r_i: None, r_t: [-805.484 -805.484 -805.484], eps: 0.01})
Step:  123900, Reward: [-411.263 -411.263 -411.263] [58.685], Avg: [-409.793 -409.793 -409.793] (0.1000) ({r_i: None, r_t: [-816.102 -816.102 -816.102], eps: 0.1})
Step:  156300, Reward: [-404.161 -404.161 -404.161] [79.818], Avg: [-460.761 -460.761 -460.761] (0.0100) ({r_i: None, r_t: [-790.471 -790.471 -790.471], eps: 0.01})
Step:  124000, Reward: [-374.188 -374.188 -374.188] [51.744], Avg: [-409.764 -409.764 -409.764] (0.1000) ({r_i: None, r_t: [-795.216 -795.216 -795.216], eps: 0.1})
Step:  156400, Reward: [-365.082 -365.082 -365.082] [80.113], Avg: [-460.700 -460.700 -460.700] (0.0100) ({r_i: None, r_t: [-783.986 -783.986 -783.986], eps: 0.01})
Step:  124100, Reward: [-354.954 -354.954 -354.954] [53.430], Avg: [-409.720 -409.720 -409.720] (0.1000) ({r_i: None, r_t: [-773.868 -773.868 -773.868], eps: 0.1})
Step:  156500, Reward: [-362.103 -362.103 -362.103] [40.430], Avg: [-460.637 -460.637 -460.637] (0.0100) ({r_i: None, r_t: [-801.853 -801.853 -801.853], eps: 0.01})
Step:  124200, Reward: [-409.750 -409.750 -409.750] [72.049], Avg: [-409.720 -409.720 -409.720] (0.1000) ({r_i: None, r_t: [-805.562 -805.562 -805.562], eps: 0.1})
Step:  156600, Reward: [-415.730 -415.730 -415.730] [53.502], Avg: [-460.608 -460.608 -460.608] (0.0100) ({r_i: None, r_t: [-813.199 -813.199 -813.199], eps: 0.01})
Step:  124300, Reward: [-396.466 -396.466 -396.466] [52.455], Avg: [-409.710 -409.710 -409.710] (0.1000) ({r_i: None, r_t: [-825.151 -825.151 -825.151], eps: 0.1})
Step:  156700, Reward: [-390.692 -390.692 -390.692] [76.189], Avg: [-460.563 -460.563 -460.563] (0.0100) ({r_i: None, r_t: [-795.703 -795.703 -795.703], eps: 0.01})
Step:  124400, Reward: [-383.783 -383.783 -383.783] [62.245], Avg: [-409.689 -409.689 -409.689] (0.1000) ({r_i: None, r_t: [-773.324 -773.324 -773.324], eps: 0.1})
Step:  156800, Reward: [-388.688 -388.688 -388.688] [67.245], Avg: [-460.518 -460.518 -460.518] (0.0100) ({r_i: None, r_t: [-800.598 -800.598 -800.598], eps: 0.01})
Step:  124500, Reward: [-400.211 -400.211 -400.211] [57.426], Avg: [-409.681 -409.681 -409.681] (0.1000) ({r_i: None, r_t: [-796.706 -796.706 -796.706], eps: 0.1})
Step:  156900, Reward: [-348.700 -348.700 -348.700] [74.736], Avg: [-460.446 -460.446 -460.446] (0.0100) ({r_i: None, r_t: [-800.096 -800.096 -800.096], eps: 0.01})
Step:  124600, Reward: [-397.502 -397.502 -397.502] [61.613], Avg: [-409.671 -409.671 -409.671] (0.1000) ({r_i: None, r_t: [-784.544 -784.544 -784.544], eps: 0.1})
Step:  157000, Reward: [-427.137 -427.137 -427.137] [65.536], Avg: [-460.425 -460.425 -460.425] (0.0100) ({r_i: None, r_t: [-768.826 -768.826 -768.826], eps: 0.01})
Step:  124700, Reward: [-395.498 -395.498 -395.498] [68.418], Avg: [-409.660 -409.660 -409.660] (0.1000) ({r_i: None, r_t: [-788.787 -788.787 -788.787], eps: 0.1})
Step:  157100, Reward: [-428.563 -428.563 -428.563] [92.893], Avg: [-460.405 -460.405 -460.405] (0.0100) ({r_i: None, r_t: [-847.833 -847.833 -847.833], eps: 0.01})
Step:  124800, Reward: [-428.027 -428.027 -428.027] [53.973], Avg: [-409.675 -409.675 -409.675] (0.1000) ({r_i: None, r_t: [-790.815 -790.815 -790.815], eps: 0.1})
Step:  157200, Reward: [-399.516 -399.516 -399.516] [56.663], Avg: [-460.366 -460.366 -460.366] (0.0100) ({r_i: None, r_t: [-789.848 -789.848 -789.848], eps: 0.01})
Step:  124900, Reward: [-400.888 -400.888 -400.888] [99.676], Avg: [-409.668 -409.668 -409.668] (0.1000) ({r_i: None, r_t: [-762.440 -762.440 -762.440], eps: 0.1})
Step:  157300, Reward: [-423.942 -423.942 -423.942] [79.314], Avg: [-460.343 -460.343 -460.343] (0.0100) ({r_i: None, r_t: [-767.775 -767.775 -767.775], eps: 0.01})
Step:  125000, Reward: [-384.476 -384.476 -384.476] [66.127], Avg: [-409.648 -409.648 -409.648] (0.1000) ({r_i: None, r_t: [-756.179 -756.179 -756.179], eps: 0.1})
Step:  157400, Reward: [-423.480 -423.480 -423.480] [98.319], Avg: [-460.320 -460.320 -460.320] (0.0100) ({r_i: None, r_t: [-823.231 -823.231 -823.231], eps: 0.01})
Step:  125100, Reward: [-409.315 -409.315 -409.315] [59.143], Avg: [-409.647 -409.647 -409.647] (0.1000) ({r_i: None, r_t: [-811.455 -811.455 -811.455], eps: 0.1})
Step:  157500, Reward: [-358.792 -358.792 -358.792] [63.091], Avg: [-460.255 -460.255 -460.255] (0.0100) ({r_i: None, r_t: [-774.045 -774.045 -774.045], eps: 0.01})
Step:  125200, Reward: [-389.441 -389.441 -389.441] [51.868], Avg: [-409.631 -409.631 -409.631] (0.1000) ({r_i: None, r_t: [-788.358 -788.358 -788.358], eps: 0.1})
Step:  157600, Reward: [-434.541 -434.541 -434.541] [99.719], Avg: [-460.239 -460.239 -460.239] (0.0100) ({r_i: None, r_t: [-743.696 -743.696 -743.696], eps: 0.01})
Step:  125300, Reward: [-383.786 -383.786 -383.786] [77.157], Avg: [-409.611 -409.611 -409.611] (0.1000) ({r_i: None, r_t: [-808.672 -808.672 -808.672], eps: 0.1})
Step:  157700, Reward: [-363.354 -363.354 -363.354] [67.008], Avg: [-460.178 -460.178 -460.178] (0.0100) ({r_i: None, r_t: [-784.174 -784.174 -784.174], eps: 0.01})
Step:  125400, Reward: [-396.910 -396.910 -396.910] [52.867], Avg: [-409.600 -409.600 -409.600] (0.1000) ({r_i: None, r_t: [-750.884 -750.884 -750.884], eps: 0.1})
Step:  157800, Reward: [-417.124 -417.124 -417.124] [86.876], Avg: [-460.150 -460.150 -460.150] (0.0100) ({r_i: None, r_t: [-778.196 -778.196 -778.196], eps: 0.01})
Step:  125500, Reward: [-385.867 -385.867 -385.867] [43.410], Avg: [-409.581 -409.581 -409.581] (0.1000) ({r_i: None, r_t: [-783.219 -783.219 -783.219], eps: 0.1})
Step:  157900, Reward: [-409.953 -409.953 -409.953] [69.683], Avg: [-460.119 -460.119 -460.119] (0.0100) ({r_i: None, r_t: [-827.938 -827.938 -827.938], eps: 0.01})
Step:  125600, Reward: [-374.045 -374.045 -374.045] [62.908], Avg: [-409.553 -409.553 -409.553] (0.1000) ({r_i: None, r_t: [-740.718 -740.718 -740.718], eps: 0.1})
Step:  158000, Reward: [-397.632 -397.632 -397.632] [79.425], Avg: [-460.079 -460.079 -460.079] (0.0100) ({r_i: None, r_t: [-802.991 -802.991 -802.991], eps: 0.01})
Step:  125700, Reward: [-400.328 -400.328 -400.328] [53.487], Avg: [-409.546 -409.546 -409.546] (0.1000) ({r_i: None, r_t: [-810.836 -810.836 -810.836], eps: 0.1})
Step:  158100, Reward: [-385.897 -385.897 -385.897] [88.758], Avg: [-460.032 -460.032 -460.032] (0.0100) ({r_i: None, r_t: [-814.866 -814.866 -814.866], eps: 0.01})
Step:  125800, Reward: [-381.653 -381.653 -381.653] [75.956], Avg: [-409.524 -409.524 -409.524] (0.1000) ({r_i: None, r_t: [-821.979 -821.979 -821.979], eps: 0.1})
Step:  158200, Reward: [-397.178 -397.178 -397.178] [76.762], Avg: [-459.992 -459.992 -459.992] (0.0100) ({r_i: None, r_t: [-876.822 -876.822 -876.822], eps: 0.01})
Step:  125900, Reward: [-385.873 -385.873 -385.873] [74.015], Avg: [-409.505 -409.505 -409.505] (0.1000) ({r_i: None, r_t: [-733.843 -733.843 -733.843], eps: 0.1})
Step:  158300, Reward: [-423.963 -423.963 -423.963] [55.193], Avg: [-459.970 -459.970 -459.970] (0.0100) ({r_i: None, r_t: [-815.847 -815.847 -815.847], eps: 0.01})
Step:  126000, Reward: [-388.986 -388.986 -388.986] [71.755], Avg: [-409.489 -409.489 -409.489] (0.1000) ({r_i: None, r_t: [-808.021 -808.021 -808.021], eps: 0.1})
Step:  158400, Reward: [-380.643 -380.643 -380.643] [60.366], Avg: [-459.920 -459.920 -459.920] (0.0100) ({r_i: None, r_t: [-759.624 -759.624 -759.624], eps: 0.01})
Step:  126100, Reward: [-370.982 -370.982 -370.982] [54.549], Avg: [-409.458 -409.458 -409.458] (0.1000) ({r_i: None, r_t: [-805.062 -805.062 -805.062], eps: 0.1})
Step:  158500, Reward: [-428.357 -428.357 -428.357] [67.638], Avg: [-459.900 -459.900 -459.900] (0.0100) ({r_i: None, r_t: [-830.272 -830.272 -830.272], eps: 0.01})
Step:  126200, Reward: [-395.061 -395.061 -395.061] [51.659], Avg: [-409.447 -409.447 -409.447] (0.1000) ({r_i: None, r_t: [-748.008 -748.008 -748.008], eps: 0.1})
Step:  158600, Reward: [-404.128 -404.128 -404.128] [68.327], Avg: [-459.865 -459.865 -459.865] (0.0100) ({r_i: None, r_t: [-812.751 -812.751 -812.751], eps: 0.01})
Step:  126300, Reward: [-399.276 -399.276 -399.276] [54.506], Avg: [-409.439 -409.439 -409.439] (0.1000) ({r_i: None, r_t: [-763.827 -763.827 -763.827], eps: 0.1})
Step:  158700, Reward: [-378.609 -378.609 -378.609] [59.679], Avg: [-459.813 -459.813 -459.813] (0.0100) ({r_i: None, r_t: [-768.421 -768.421 -768.421], eps: 0.01})
Step:  126400, Reward: [-389.868 -389.868 -389.868] [70.335], Avg: [-409.423 -409.423 -409.423] (0.1000) ({r_i: None, r_t: [-812.666 -812.666 -812.666], eps: 0.1})
Step:  158800, Reward: [-382.837 -382.837 -382.837] [89.913], Avg: [-459.765 -459.765 -459.765] (0.0100) ({r_i: None, r_t: [-861.862 -861.862 -861.862], eps: 0.01})
Step:  126500, Reward: [-381.435 -381.435 -381.435] [64.116], Avg: [-409.401 -409.401 -409.401] (0.1000) ({r_i: None, r_t: [-806.791 -806.791 -806.791], eps: 0.1})
Step:  158900, Reward: [-384.323 -384.323 -384.323] [68.283], Avg: [-459.718 -459.718 -459.718] (0.0100) ({r_i: None, r_t: [-792.567 -792.567 -792.567], eps: 0.01})
Step:  126600, Reward: [-366.775 -366.775 -366.775] [61.854], Avg: [-409.368 -409.368 -409.368] (0.1000) ({r_i: None, r_t: [-806.240 -806.240 -806.240], eps: 0.1})
Step:  159000, Reward: [-414.549 -414.549 -414.549] [93.689], Avg: [-459.689 -459.689 -459.689] (0.0100) ({r_i: None, r_t: [-791.089 -791.089 -791.089], eps: 0.01})
Step:  126700, Reward: [-420.325 -420.325 -420.325] [62.945], Avg: [-409.376 -409.376 -409.376] (0.1000) ({r_i: None, r_t: [-778.695 -778.695 -778.695], eps: 0.1})
Step:  159100, Reward: [-383.056 -383.056 -383.056] [53.550], Avg: [-459.641 -459.641 -459.641] (0.0100) ({r_i: None, r_t: [-780.278 -780.278 -780.278], eps: 0.01})
Step:  126800, Reward: [-378.117 -378.117 -378.117] [64.562], Avg: [-409.352 -409.352 -409.352] (0.1000) ({r_i: None, r_t: [-776.325 -776.325 -776.325], eps: 0.1})
Step:  159200, Reward: [-385.821 -385.821 -385.821] [83.799], Avg: [-459.595 -459.595 -459.595] (0.0100) ({r_i: None, r_t: [-809.358 -809.358 -809.358], eps: 0.01})
Step:  126900, Reward: [-386.062 -386.062 -386.062] [53.803], Avg: [-409.333 -409.333 -409.333] (0.1000) ({r_i: None, r_t: [-794.924 -794.924 -794.924], eps: 0.1})
Step:  159300, Reward: [-379.995 -379.995 -379.995] [65.895], Avg: [-459.545 -459.545 -459.545] (0.0100) ({r_i: None, r_t: [-814.736 -814.736 -814.736], eps: 0.01})
Step:  127000, Reward: [-406.461 -406.461 -406.461] [72.885], Avg: [-409.331 -409.331 -409.331] (0.1000) ({r_i: None, r_t: [-783.614 -783.614 -783.614], eps: 0.1})
Step:  159400, Reward: [-403.136 -403.136 -403.136] [77.131], Avg: [-459.509 -459.509 -459.509] (0.0100) ({r_i: None, r_t: [-829.805 -829.805 -829.805], eps: 0.01})
Step:  127100, Reward: [-359.532 -359.532 -359.532] [69.535], Avg: [-409.292 -409.292 -409.292] (0.1000) ({r_i: None, r_t: [-774.068 -774.068 -774.068], eps: 0.1})
Step:  159500, Reward: [-385.771 -385.771 -385.771] [47.252], Avg: [-459.463 -459.463 -459.463] (0.0100) ({r_i: None, r_t: [-815.897 -815.897 -815.897], eps: 0.01})
Step:  127200, Reward: [-384.933 -384.933 -384.933] [66.300], Avg: [-409.273 -409.273 -409.273] (0.1000) ({r_i: None, r_t: [-769.811 -769.811 -769.811], eps: 0.1})
Step:  159600, Reward: [-381.515 -381.515 -381.515] [68.103], Avg: [-459.414 -459.414 -459.414] (0.0100) ({r_i: None, r_t: [-804.899 -804.899 -804.899], eps: 0.01})
Step:  127300, Reward: [-401.029 -401.029 -401.029] [82.896], Avg: [-409.266 -409.266 -409.266] (0.1000) ({r_i: None, r_t: [-774.279 -774.279 -774.279], eps: 0.1})
Step:  159700, Reward: [-402.956 -402.956 -402.956] [65.540], Avg: [-459.379 -459.379 -459.379] (0.0100) ({r_i: None, r_t: [-819.984 -819.984 -819.984], eps: 0.01})
Step:  127400, Reward: [-405.016 -405.016 -405.016] [58.889], Avg: [-409.263 -409.263 -409.263] (0.1000) ({r_i: None, r_t: [-764.859 -764.859 -764.859], eps: 0.1})
Step:  159800, Reward: [-424.172 -424.172 -424.172] [81.808], Avg: [-459.357 -459.357 -459.357] (0.0100) ({r_i: None, r_t: [-764.136 -764.136 -764.136], eps: 0.01})
Step:  127500, Reward: [-400.482 -400.482 -400.482] [68.895], Avg: [-409.256 -409.256 -409.256] (0.1000) ({r_i: None, r_t: [-761.871 -761.871 -761.871], eps: 0.1})
Step:  159900, Reward: [-405.551 -405.551 -405.551] [63.935], Avg: [-459.323 -459.323 -459.323] (0.0100) ({r_i: None, r_t: [-802.397 -802.397 -802.397], eps: 0.01})
Step:  127600, Reward: [-369.131 -369.131 -369.131] [63.391], Avg: [-409.225 -409.225 -409.225] (0.1000) ({r_i: None, r_t: [-766.030 -766.030 -766.030], eps: 0.1})
Step:  160000, Reward: [-376.204 -376.204 -376.204] [74.695], Avg: [-459.271 -459.271 -459.271] (0.0100) ({r_i: None, r_t: [-751.726 -751.726 -751.726], eps: 0.01})
Step:  127700, Reward: [-408.662 -408.662 -408.662] [62.198], Avg: [-409.224 -409.224 -409.224] (0.1000) ({r_i: None, r_t: [-819.004 -819.004 -819.004], eps: 0.1})
Step:  160100, Reward: [-415.937 -415.937 -415.937] [86.467], Avg: [-459.244 -459.244 -459.244] (0.0100) ({r_i: None, r_t: [-757.354 -757.354 -757.354], eps: 0.01})
Step:  127800, Reward: [-370.275 -370.275 -370.275] [74.722], Avg: [-409.194 -409.194 -409.194] (0.1000) ({r_i: None, r_t: [-810.355 -810.355 -810.355], eps: 0.1})
Step:  160200, Reward: [-366.210 -366.210 -366.210] [47.597], Avg: [-459.186 -459.186 -459.186] (0.0100) ({r_i: None, r_t: [-758.657 -758.657 -758.657], eps: 0.01})
Step:  127900, Reward: [-381.007 -381.007 -381.007] [43.339], Avg: [-409.172 -409.172 -409.172] (0.1000) ({r_i: None, r_t: [-765.300 -765.300 -765.300], eps: 0.1})
Step:  160300, Reward: [-360.894 -360.894 -360.894] [65.761], Avg: [-459.125 -459.125 -459.125] (0.0100) ({r_i: None, r_t: [-785.639 -785.639 -785.639], eps: 0.01})
Step:  128000, Reward: [-405.826 -405.826 -405.826] [83.114], Avg: [-409.169 -409.169 -409.169] (0.1000) ({r_i: None, r_t: [-798.017 -798.017 -798.017], eps: 0.1})
Step:  160400, Reward: [-366.534 -366.534 -366.534] [44.345], Avg: [-459.067 -459.067 -459.067] (0.0100) ({r_i: None, r_t: [-868.227 -868.227 -868.227], eps: 0.01})
Step:  128100, Reward: [-407.829 -407.829 -407.829] [61.414], Avg: [-409.168 -409.168 -409.168] (0.1000) ({r_i: None, r_t: [-822.375 -822.375 -822.375], eps: 0.1})
Step:  160500, Reward: [-417.358 -417.358 -417.358] [69.063], Avg: [-459.041 -459.041 -459.041] (0.0100) ({r_i: None, r_t: [-800.588 -800.588 -800.588], eps: 0.01})
Step:  128200, Reward: [-424.859 -424.859 -424.859] [64.801], Avg: [-409.180 -409.180 -409.180] (0.1000) ({r_i: None, r_t: [-794.297 -794.297 -794.297], eps: 0.1})
Step:  160600, Reward: [-407.834 -407.834 -407.834] [54.920], Avg: [-459.010 -459.010 -459.010] (0.0100) ({r_i: None, r_t: [-793.883 -793.883 -793.883], eps: 0.01})
Step:  128300, Reward: [-351.696 -351.696 -351.696] [60.032], Avg: [-409.135 -409.135 -409.135] (0.1000) ({r_i: None, r_t: [-759.496 -759.496 -759.496], eps: 0.1})
Step:  160700, Reward: [-411.477 -411.477 -411.477] [82.901], Avg: [-458.980 -458.980 -458.980] (0.0100) ({r_i: None, r_t: [-829.041 -829.041 -829.041], eps: 0.01})
Step:  128400, Reward: [-400.270 -400.270 -400.270] [88.081], Avg: [-409.129 -409.129 -409.129] (0.1000) ({r_i: None, r_t: [-778.273 -778.273 -778.273], eps: 0.1})
Step:  160800, Reward: [-398.790 -398.790 -398.790] [57.303], Avg: [-458.943 -458.943 -458.943] (0.0100) ({r_i: None, r_t: [-806.356 -806.356 -806.356], eps: 0.01})
Step:  128500, Reward: [-390.846 -390.846 -390.846] [62.598], Avg: [-409.114 -409.114 -409.114] (0.1000) ({r_i: None, r_t: [-755.700 -755.700 -755.700], eps: 0.1})
Step:  160900, Reward: [-397.909 -397.909 -397.909] [73.711], Avg: [-458.905 -458.905 -458.905] (0.0100) ({r_i: None, r_t: [-764.480 -764.480 -764.480], eps: 0.01})
Step:  128600, Reward: [-372.390 -372.390 -372.390] [40.324], Avg: [-409.086 -409.086 -409.086] (0.1000) ({r_i: None, r_t: [-804.718 -804.718 -804.718], eps: 0.1})
Step:  161000, Reward: [-412.503 -412.503 -412.503] [59.652], Avg: [-458.876 -458.876 -458.876] (0.0100) ({r_i: None, r_t: [-860.298 -860.298 -860.298], eps: 0.01})
Step:  128700, Reward: [-378.492 -378.492 -378.492] [41.825], Avg: [-409.062 -409.062 -409.062] (0.1000) ({r_i: None, r_t: [-749.351 -749.351 -749.351], eps: 0.1})
Step:  161100, Reward: [-402.114 -402.114 -402.114] [62.832], Avg: [-458.841 -458.841 -458.841] (0.0100) ({r_i: None, r_t: [-799.886 -799.886 -799.886], eps: 0.01})
Step:  128800, Reward: [-405.120 -405.120 -405.120] [79.816], Avg: [-409.059 -409.059 -409.059] (0.1000) ({r_i: None, r_t: [-795.964 -795.964 -795.964], eps: 0.1})
Step:  161200, Reward: [-428.999 -428.999 -428.999] [97.270], Avg: [-458.822 -458.822 -458.822] (0.0100) ({r_i: None, r_t: [-840.304 -840.304 -840.304], eps: 0.01})
Step:  128900, Reward: [-405.091 -405.091 -405.091] [73.269], Avg: [-409.056 -409.056 -409.056] (0.1000) ({r_i: None, r_t: [-793.331 -793.331 -793.331], eps: 0.1})
Step:  161300, Reward: [-380.653 -380.653 -380.653] [74.132], Avg: [-458.774 -458.774 -458.774] (0.0100) ({r_i: None, r_t: [-801.356 -801.356 -801.356], eps: 0.01})
Step:  129000, Reward: [-406.748 -406.748 -406.748] [88.767], Avg: [-409.054 -409.054 -409.054] (0.1000) ({r_i: None, r_t: [-778.018 -778.018 -778.018], eps: 0.1})
Step:  161400, Reward: [-406.514 -406.514 -406.514] [88.390], Avg: [-458.741 -458.741 -458.741] (0.0100) ({r_i: None, r_t: [-814.796 -814.796 -814.796], eps: 0.01})
Step:  129100, Reward: [-390.845 -390.845 -390.845] [39.976], Avg: [-409.040 -409.040 -409.040] (0.1000) ({r_i: None, r_t: [-748.681 -748.681 -748.681], eps: 0.1})
Step:  161500, Reward: [-392.962 -392.962 -392.962] [88.611], Avg: [-458.701 -458.701 -458.701] (0.0100) ({r_i: None, r_t: [-795.253 -795.253 -795.253], eps: 0.01})
Step:  129200, Reward: [-405.215 -405.215 -405.215] [62.073], Avg: [-409.037 -409.037 -409.037] (0.1000) ({r_i: None, r_t: [-766.930 -766.930 -766.930], eps: 0.1})
Step:  161600, Reward: [-384.531 -384.531 -384.531] [71.148], Avg: [-458.655 -458.655 -458.655] (0.0100) ({r_i: None, r_t: [-784.343 -784.343 -784.343], eps: 0.01})
Step:  129300, Reward: [-389.976 -389.976 -389.976] [51.661], Avg: [-409.022 -409.022 -409.022] (0.1000) ({r_i: None, r_t: [-771.395 -771.395 -771.395], eps: 0.1})
Step:  161700, Reward: [-403.187 -403.187 -403.187] [60.514], Avg: [-458.621 -458.621 -458.621] (0.0100) ({r_i: None, r_t: [-785.904 -785.904 -785.904], eps: 0.01})
Step:  129400, Reward: [-373.172 -373.172 -373.172] [38.394], Avg: [-408.995 -408.995 -408.995] (0.1000) ({r_i: None, r_t: [-813.883 -813.883 -813.883], eps: 0.1})
Step:  161800, Reward: [-416.669 -416.669 -416.669] [89.598], Avg: [-458.595 -458.595 -458.595] (0.0100) ({r_i: None, r_t: [-795.081 -795.081 -795.081], eps: 0.01})
Step:  129500, Reward: [-389.168 -389.168 -389.168] [71.390], Avg: [-408.979 -408.979 -408.979] (0.1000) ({r_i: None, r_t: [-775.395 -775.395 -775.395], eps: 0.1})
Step:  161900, Reward: [-403.878 -403.878 -403.878] [92.388], Avg: [-458.561 -458.561 -458.561] (0.0100) ({r_i: None, r_t: [-794.200 -794.200 -794.200], eps: 0.01})
Step:  129600, Reward: [-368.905 -368.905 -368.905] [61.693], Avg: [-408.948 -408.948 -408.948] (0.1000) ({r_i: None, r_t: [-769.590 -769.590 -769.590], eps: 0.1})
Step:  162000, Reward: [-397.167 -397.167 -397.167] [54.498], Avg: [-458.523 -458.523 -458.523] (0.0100) ({r_i: None, r_t: [-811.015 -811.015 -811.015], eps: 0.01})
Step:  129700, Reward: [-401.846 -401.846 -401.846] [78.760], Avg: [-408.943 -408.943 -408.943] (0.1000) ({r_i: None, r_t: [-809.396 -809.396 -809.396], eps: 0.1})
Step:  162100, Reward: [-393.325 -393.325 -393.325] [59.973], Avg: [-458.483 -458.483 -458.483] (0.0100) ({r_i: None, r_t: [-792.305 -792.305 -792.305], eps: 0.01})
Step:  129800, Reward: [-372.228 -372.228 -372.228] [68.305], Avg: [-408.915 -408.915 -408.915] (0.1000) ({r_i: None, r_t: [-805.029 -805.029 -805.029], eps: 0.1})
Step:  162200, Reward: [-381.101 -381.101 -381.101] [54.698], Avg: [-458.435 -458.435 -458.435] (0.0100) ({r_i: None, r_t: [-802.133 -802.133 -802.133], eps: 0.01})
Step:  129900, Reward: [-387.853 -387.853 -387.853] [72.288], Avg: [-408.898 -408.898 -408.898] (0.1000) ({r_i: None, r_t: [-766.231 -766.231 -766.231], eps: 0.1})
Step:  162300, Reward: [-380.395 -380.395 -380.395] [88.911], Avg: [-458.387 -458.387 -458.387] (0.0100) ({r_i: None, r_t: [-780.292 -780.292 -780.292], eps: 0.01})
Step:  130000, Reward: [-396.820 -396.820 -396.820] [76.697], Avg: [-408.889 -408.889 -408.889] (0.1000) ({r_i: None, r_t: [-798.230 -798.230 -798.230], eps: 0.1})
Step:  162400, Reward: [-380.313 -380.313 -380.313] [92.176], Avg: [-458.339 -458.339 -458.339] (0.0100) ({r_i: None, r_t: [-790.629 -790.629 -790.629], eps: 0.01})
Step:  130100, Reward: [-425.196 -425.196 -425.196] [75.133], Avg: [-408.902 -408.902 -408.902] (0.1000) ({r_i: None, r_t: [-805.747 -805.747 -805.747], eps: 0.1})
Step:  162500, Reward: [-366.264 -366.264 -366.264] [48.955], Avg: [-458.282 -458.282 -458.282] (0.0100) ({r_i: None, r_t: [-811.971 -811.971 -811.971], eps: 0.01})
Step:  130200, Reward: [-382.600 -382.600 -382.600] [72.412], Avg: [-408.882 -408.882 -408.882] (0.1000) ({r_i: None, r_t: [-797.554 -797.554 -797.554], eps: 0.1})
Step:  162600, Reward: [-372.746 -372.746 -372.746] [79.610], Avg: [-458.230 -458.230 -458.230] (0.0100) ({r_i: None, r_t: [-809.760 -809.760 -809.760], eps: 0.01})
Step:  130300, Reward: [-401.748 -401.748 -401.748] [80.529], Avg: [-408.876 -408.876 -408.876] (0.1000) ({r_i: None, r_t: [-789.908 -789.908 -789.908], eps: 0.1})
Step:  162700, Reward: [-391.320 -391.320 -391.320] [71.796], Avg: [-458.189 -458.189 -458.189] (0.0100) ({r_i: None, r_t: [-745.018 -745.018 -745.018], eps: 0.01})
Step:  130400, Reward: [-398.226 -398.226 -398.226] [35.824], Avg: [-408.868 -408.868 -408.868] (0.1000) ({r_i: None, r_t: [-791.724 -791.724 -791.724], eps: 0.1})
Step:  162800, Reward: [-381.719 -381.719 -381.719] [84.647], Avg: [-458.142 -458.142 -458.142] (0.0100) ({r_i: None, r_t: [-773.722 -773.722 -773.722], eps: 0.01})
Step:  130500, Reward: [-373.310 -373.310 -373.310] [44.819], Avg: [-408.841 -408.841 -408.841] (0.1000) ({r_i: None, r_t: [-781.306 -781.306 -781.306], eps: 0.1})
Step:  162900, Reward: [-395.995 -395.995 -395.995] [73.786], Avg: [-458.104 -458.104 -458.104] (0.0100) ({r_i: None, r_t: [-776.321 -776.321 -776.321], eps: 0.01})
Step:  130600, Reward: [-373.165 -373.165 -373.165] [71.113], Avg: [-408.813 -408.813 -408.813] (0.1000) ({r_i: None, r_t: [-824.659 -824.659 -824.659], eps: 0.1})
Step:  163000, Reward: [-371.811 -371.811 -371.811] [60.357], Avg: [-458.051 -458.051 -458.051] (0.0100) ({r_i: None, r_t: [-795.715 -795.715 -795.715], eps: 0.01})
Step:  130700, Reward: [-429.266 -429.266 -429.266] [83.849], Avg: [-408.829 -408.829 -408.829] (0.1000) ({r_i: None, r_t: [-765.070 -765.070 -765.070], eps: 0.1})
Step:  163100, Reward: [-387.727 -387.727 -387.727] [51.770], Avg: [-458.008 -458.008 -458.008] (0.0100) ({r_i: None, r_t: [-763.769 -763.769 -763.769], eps: 0.01})
Step:  130800, Reward: [-409.061 -409.061 -409.061] [72.887], Avg: [-408.829 -408.829 -408.829] (0.1000) ({r_i: None, r_t: [-807.876 -807.876 -807.876], eps: 0.1})
Step:  163200, Reward: [-395.221 -395.221 -395.221] [88.362], Avg: [-457.969 -457.969 -457.969] (0.0100) ({r_i: None, r_t: [-800.292 -800.292 -800.292], eps: 0.01})
Step:  130900, Reward: [-424.860 -424.860 -424.860] [85.811], Avg: [-408.841 -408.841 -408.841] (0.1000) ({r_i: None, r_t: [-790.413 -790.413 -790.413], eps: 0.1})
Step:  163300, Reward: [-381.265 -381.265 -381.265] [93.249], Avg: [-457.922 -457.922 -457.922] (0.0100) ({r_i: None, r_t: [-711.270 -711.270 -711.270], eps: 0.01})
Step:  131000, Reward: [-397.168 -397.168 -397.168] [64.086], Avg: [-408.833 -408.833 -408.833] (0.1000) ({r_i: None, r_t: [-773.719 -773.719 -773.719], eps: 0.1})
Step:  163400, Reward: [-409.323 -409.323 -409.323] [83.533], Avg: [-457.893 -457.893 -457.893] (0.0100) ({r_i: None, r_t: [-797.916 -797.916 -797.916], eps: 0.01})
Step:  131100, Reward: [-385.687 -385.687 -385.687] [47.703], Avg: [-408.815 -408.815 -408.815] (0.1000) ({r_i: None, r_t: [-810.875 -810.875 -810.875], eps: 0.1})
Step:  163500, Reward: [-395.836 -395.836 -395.836] [92.360], Avg: [-457.855 -457.855 -457.855] (0.0100) ({r_i: None, r_t: [-796.073 -796.073 -796.073], eps: 0.01})
Step:  131200, Reward: [-398.972 -398.972 -398.972] [47.892], Avg: [-408.807 -408.807 -408.807] (0.1000) ({r_i: None, r_t: [-814.219 -814.219 -814.219], eps: 0.1})
Step:  163600, Reward: [-393.225 -393.225 -393.225] [92.534], Avg: [-457.815 -457.815 -457.815] (0.0100) ({r_i: None, r_t: [-780.439 -780.439 -780.439], eps: 0.01})
Step:  131300, Reward: [-412.509 -412.509 -412.509] [54.198], Avg: [-408.810 -408.810 -408.810] (0.1000) ({r_i: None, r_t: [-803.788 -803.788 -803.788], eps: 0.1})
Step:  163700, Reward: [-390.845 -390.845 -390.845] [52.056], Avg: [-457.774 -457.774 -457.774] (0.0100) ({r_i: None, r_t: [-775.133 -775.133 -775.133], eps: 0.01})
Step:  131400, Reward: [-419.952 -419.952 -419.952] [89.711], Avg: [-408.819 -408.819 -408.819] (0.1000) ({r_i: None, r_t: [-799.267 -799.267 -799.267], eps: 0.1})
Step:  163800, Reward: [-411.236 -411.236 -411.236] [78.200], Avg: [-457.746 -457.746 -457.746] (0.0100) ({r_i: None, r_t: [-758.909 -758.909 -758.909], eps: 0.01})
Step:  131500, Reward: [-378.378 -378.378 -378.378] [68.537], Avg: [-408.796 -408.796 -408.796] (0.1000) ({r_i: None, r_t: [-800.338 -800.338 -800.338], eps: 0.1})
Step:  163900, Reward: [-372.396 -372.396 -372.396] [65.953], Avg: [-457.694 -457.694 -457.694] (0.0100) ({r_i: None, r_t: [-764.251 -764.251 -764.251], eps: 0.01})
Step:  131600, Reward: [-402.759 -402.759 -402.759] [75.677], Avg: [-408.791 -408.791 -408.791] (0.1000) ({r_i: None, r_t: [-801.592 -801.592 -801.592], eps: 0.1})
Step:  164000, Reward: [-394.562 -394.562 -394.562] [50.162], Avg: [-457.655 -457.655 -457.655] (0.0100) ({r_i: None, r_t: [-810.230 -810.230 -810.230], eps: 0.01})
Step:  131700, Reward: [-411.886 -411.886 -411.886] [59.368], Avg: [-408.793 -408.793 -408.793] (0.1000) ({r_i: None, r_t: [-772.613 -772.613 -772.613], eps: 0.1})
Step:  164100, Reward: [-371.826 -371.826 -371.826] [49.255], Avg: [-457.603 -457.603 -457.603] (0.0100) ({r_i: None, r_t: [-799.259 -799.259 -799.259], eps: 0.01})
Step:  131800, Reward: [-413.600 -413.600 -413.600] [73.752], Avg: [-408.797 -408.797 -408.797] (0.1000) ({r_i: None, r_t: [-831.866 -831.866 -831.866], eps: 0.1})
Step:  164200, Reward: [-367.022 -367.022 -367.022] [53.042], Avg: [-457.548 -457.548 -457.548] (0.0100) ({r_i: None, r_t: [-803.247 -803.247 -803.247], eps: 0.01})
Step:  131900, Reward: [-414.171 -414.171 -414.171] [82.787], Avg: [-408.801 -408.801 -408.801] (0.1000) ({r_i: None, r_t: [-767.396 -767.396 -767.396], eps: 0.1})
Step:  164300, Reward: [-396.885 -396.885 -396.885] [58.747], Avg: [-457.511 -457.511 -457.511] (0.0100) ({r_i: None, r_t: [-758.134 -758.134 -758.134], eps: 0.01})
Step:  132000, Reward: [-387.671 -387.671 -387.671] [46.431], Avg: [-408.785 -408.785 -408.785] (0.1000) ({r_i: None, r_t: [-818.212 -818.212 -818.212], eps: 0.1})
Step:  164400, Reward: [-399.602 -399.602 -399.602] [79.557], Avg: [-457.476 -457.476 -457.476] (0.0100) ({r_i: None, r_t: [-780.101 -780.101 -780.101], eps: 0.01})
Step:  132100, Reward: [-396.426 -396.426 -396.426] [63.399], Avg: [-408.776 -408.776 -408.776] (0.1000) ({r_i: None, r_t: [-774.430 -774.430 -774.430], eps: 0.1})
Step:  164500, Reward: [-397.242 -397.242 -397.242] [83.564], Avg: [-457.439 -457.439 -457.439] (0.0100) ({r_i: None, r_t: [-790.938 -790.938 -790.938], eps: 0.01})
Step:  132200, Reward: [-388.921 -388.921 -388.921] [81.941], Avg: [-408.761 -408.761 -408.761] (0.1000) ({r_i: None, r_t: [-845.558 -845.558 -845.558], eps: 0.1})
Step:  164600, Reward: [-391.949 -391.949 -391.949] [66.995], Avg: [-457.399 -457.399 -457.399] (0.0100) ({r_i: None, r_t: [-753.516 -753.516 -753.516], eps: 0.01})
Step:  132300, Reward: [-416.758 -416.758 -416.758] [90.406], Avg: [-408.767 -408.767 -408.767] (0.1000) ({r_i: None, r_t: [-807.894 -807.894 -807.894], eps: 0.1})
Step:  164700, Reward: [-417.823 -417.823 -417.823] [72.715], Avg: [-457.375 -457.375 -457.375] (0.0100) ({r_i: None, r_t: [-788.681 -788.681 -788.681], eps: 0.01})
Step:  132400, Reward: [-390.077 -390.077 -390.077] [58.270], Avg: [-408.753 -408.753 -408.753] (0.1000) ({r_i: None, r_t: [-811.809 -811.809 -811.809], eps: 0.1})
Step:  164800, Reward: [-412.025 -412.025 -412.025] [93.022], Avg: [-457.348 -457.348 -457.348] (0.0100) ({r_i: None, r_t: [-777.912 -777.912 -777.912], eps: 0.01})
Step:  132500, Reward: [-401.758 -401.758 -401.758] [70.160], Avg: [-408.747 -408.747 -408.747] (0.1000) ({r_i: None, r_t: [-774.535 -774.535 -774.535], eps: 0.1})
Step:  164900, Reward: [-363.644 -363.644 -363.644] [73.805], Avg: [-457.291 -457.291 -457.291] (0.0100) ({r_i: None, r_t: [-750.898 -750.898 -750.898], eps: 0.01})
Step:  132600, Reward: [-390.813 -390.813 -390.813] [84.334], Avg: [-408.734 -408.734 -408.734] (0.1000) ({r_i: None, r_t: [-830.650 -830.650 -830.650], eps: 0.1})
Step:  165000, Reward: [-371.432 -371.432 -371.432] [97.572], Avg: [-457.239 -457.239 -457.239] (0.0100) ({r_i: None, r_t: [-782.086 -782.086 -782.086], eps: 0.01})
Step:  132700, Reward: [-411.638 -411.638 -411.638] [62.674], Avg: [-408.736 -408.736 -408.736] (0.1000) ({r_i: None, r_t: [-813.649 -813.649 -813.649], eps: 0.1})
Step:  165100, Reward: [-398.291 -398.291 -398.291] [76.916], Avg: [-457.203 -457.203 -457.203] (0.0100) ({r_i: None, r_t: [-718.201 -718.201 -718.201], eps: 0.01})
Step:  132800, Reward: [-419.710 -419.710 -419.710] [68.694], Avg: [-408.744 -408.744 -408.744] (0.1000) ({r_i: None, r_t: [-814.045 -814.045 -814.045], eps: 0.1})
Step:  165200, Reward: [-386.244 -386.244 -386.244] [65.459], Avg: [-457.161 -457.161 -457.161] (0.0100) ({r_i: None, r_t: [-747.478 -747.478 -747.478], eps: 0.01})
Step:  132900, Reward: [-457.823 -457.823 -457.823] [82.302], Avg: [-408.781 -408.781 -408.781] (0.1000) ({r_i: None, r_t: [-795.709 -795.709 -795.709], eps: 0.1})
Step:  165300, Reward: [-406.804 -406.804 -406.804] [70.943], Avg: [-457.130 -457.130 -457.130] (0.0100) ({r_i: None, r_t: [-812.164 -812.164 -812.164], eps: 0.01})
Step:  133000, Reward: [-383.347 -383.347 -383.347] [77.689], Avg: [-408.762 -408.762 -408.762] (0.1000) ({r_i: None, r_t: [-816.488 -816.488 -816.488], eps: 0.1})
Step:  165400, Reward: [-358.312 -358.312 -358.312] [45.604], Avg: [-457.070 -457.070 -457.070] (0.0100) ({r_i: None, r_t: [-758.730 -758.730 -758.730], eps: 0.01})
Step:  133100, Reward: [-412.931 -412.931 -412.931] [68.252], Avg: [-408.765 -408.765 -408.765] (0.1000) ({r_i: None, r_t: [-803.099 -803.099 -803.099], eps: 0.1})
Step:  165500, Reward: [-440.532 -440.532 -440.532] [90.892], Avg: [-457.060 -457.060 -457.060] (0.0100) ({r_i: None, r_t: [-803.324 -803.324 -803.324], eps: 0.01})
Step:  133200, Reward: [-448.994 -448.994 -448.994] [71.601], Avg: [-408.795 -408.795 -408.795] (0.1000) ({r_i: None, r_t: [-837.699 -837.699 -837.699], eps: 0.1})
Step:  165600, Reward: [-419.401 -419.401 -419.401] [52.305], Avg: [-457.038 -457.038 -457.038] (0.0100) ({r_i: None, r_t: [-762.438 -762.438 -762.438], eps: 0.01})
Step:  133300, Reward: [-408.491 -408.491 -408.491] [69.604], Avg: [-408.795 -408.795 -408.795] (0.1000) ({r_i: None, r_t: [-796.609 -796.609 -796.609], eps: 0.1})
Step:  165700, Reward: [-377.764 -377.764 -377.764] [82.437], Avg: [-456.990 -456.990 -456.990] (0.0100) ({r_i: None, r_t: [-779.391 -779.391 -779.391], eps: 0.01})
Step:  133400, Reward: [-398.804 -398.804 -398.804] [66.507], Avg: [-408.788 -408.788 -408.788] (0.1000) ({r_i: None, r_t: [-850.694 -850.694 -850.694], eps: 0.1})
Step:  165800, Reward: [-384.832 -384.832 -384.832] [64.026], Avg: [-456.946 -456.946 -456.946] (0.0100) ({r_i: None, r_t: [-789.086 -789.086 -789.086], eps: 0.01})
Step:  133500, Reward: [-386.719 -386.719 -386.719] [59.837], Avg: [-408.771 -408.771 -408.771] (0.1000) ({r_i: None, r_t: [-803.823 -803.823 -803.823], eps: 0.1})
Step:  165900, Reward: [-398.409 -398.409 -398.409] [67.719], Avg: [-456.911 -456.911 -456.911] (0.0100) ({r_i: None, r_t: [-795.914 -795.914 -795.914], eps: 0.01})
Step:  133600, Reward: [-407.733 -407.733 -407.733] [80.465], Avg: [-408.770 -408.770 -408.770] (0.1000) ({r_i: None, r_t: [-799.843 -799.843 -799.843], eps: 0.1})
Step:  166000, Reward: [-411.994 -411.994 -411.994] [87.430], Avg: [-456.884 -456.884 -456.884] (0.0100) ({r_i: None, r_t: [-782.503 -782.503 -782.503], eps: 0.01})
Step:  133700, Reward: [-411.622 -411.622 -411.622] [118.854], Avg: [-408.773 -408.773 -408.773] (0.1000) ({r_i: None, r_t: [-778.145 -778.145 -778.145], eps: 0.1})
Step:  166100, Reward: [-376.361 -376.361 -376.361] [76.827], Avg: [-456.836 -456.836 -456.836] (0.0100) ({r_i: None, r_t: [-802.880 -802.880 -802.880], eps: 0.01})
Step:  133800, Reward: [-410.754 -410.754 -410.754] [80.987], Avg: [-408.774 -408.774 -408.774] (0.1000) ({r_i: None, r_t: [-827.978 -827.978 -827.978], eps: 0.1})
Step:  166200, Reward: [-386.081 -386.081 -386.081] [88.468], Avg: [-456.793 -456.793 -456.793] (0.0100) ({r_i: None, r_t: [-801.368 -801.368 -801.368], eps: 0.01})
Step:  133900, Reward: [-433.137 -433.137 -433.137] [99.628], Avg: [-408.792 -408.792 -408.792] (0.1000) ({r_i: None, r_t: [-811.864 -811.864 -811.864], eps: 0.1})
Step:  166300, Reward: [-393.604 -393.604 -393.604] [84.878], Avg: [-456.755 -456.755 -456.755] (0.0100) ({r_i: None, r_t: [-809.110 -809.110 -809.110], eps: 0.01})
Step:  134000, Reward: [-385.260 -385.260 -385.260] [38.666], Avg: [-408.775 -408.775 -408.775] (0.1000) ({r_i: None, r_t: [-815.241 -815.241 -815.241], eps: 0.1})
Step:  166400, Reward: [-380.008 -380.008 -380.008] [69.019], Avg: [-456.709 -456.709 -456.709] (0.0100) ({r_i: None, r_t: [-772.049 -772.049 -772.049], eps: 0.01})
Step:  134100, Reward: [-444.468 -444.468 -444.468] [78.876], Avg: [-408.801 -408.801 -408.801] (0.1000) ({r_i: None, r_t: [-815.235 -815.235 -815.235], eps: 0.1})
Step:  166500, Reward: [-388.562 -388.562 -388.562] [72.524], Avg: [-456.668 -456.668 -456.668] (0.0100) ({r_i: None, r_t: [-789.706 -789.706 -789.706], eps: 0.01})
Step:  134200, Reward: [-412.552 -412.552 -412.552] [63.692], Avg: [-408.804 -408.804 -408.804] (0.1000) ({r_i: None, r_t: [-843.204 -843.204 -843.204], eps: 0.1})
Step:  166600, Reward: [-368.685 -368.685 -368.685] [67.959], Avg: [-456.615 -456.615 -456.615] (0.0100) ({r_i: None, r_t: [-777.077 -777.077 -777.077], eps: 0.01})
Step:  134300, Reward: [-422.348 -422.348 -422.348] [64.801], Avg: [-408.814 -408.814 -408.814] (0.1000) ({r_i: None, r_t: [-806.041 -806.041 -806.041], eps: 0.1})
Step:  166700, Reward: [-361.080 -361.080 -361.080] [69.566], Avg: [-456.558 -456.558 -456.558] (0.0100) ({r_i: None, r_t: [-830.449 -830.449 -830.449], eps: 0.01})
Step:  134400, Reward: [-393.453 -393.453 -393.453] [63.328], Avg: [-408.803 -408.803 -408.803] (0.1000) ({r_i: None, r_t: [-796.928 -796.928 -796.928], eps: 0.1})
Step:  166800, Reward: [-407.166 -407.166 -407.166] [93.648], Avg: [-456.528 -456.528 -456.528] (0.0100) ({r_i: None, r_t: [-783.441 -783.441 -783.441], eps: 0.01})
Step:  134500, Reward: [-403.747 -403.747 -403.747] [64.771], Avg: [-408.799 -408.799 -408.799] (0.1000) ({r_i: None, r_t: [-818.075 -818.075 -818.075], eps: 0.1})
Step:  166900, Reward: [-372.852 -372.852 -372.852] [70.499], Avg: [-456.478 -456.478 -456.478] (0.0100) ({r_i: None, r_t: [-770.609 -770.609 -770.609], eps: 0.01})
Step:  134600, Reward: [-429.821 -429.821 -429.821] [115.547], Avg: [-408.815 -408.815 -408.815] (0.1000) ({r_i: None, r_t: [-814.633 -814.633 -814.633], eps: 0.1})
Step:  167000, Reward: [-374.090 -374.090 -374.090] [71.149], Avg: [-456.429 -456.429 -456.429] (0.0100) ({r_i: None, r_t: [-818.421 -818.421 -818.421], eps: 0.01})
Step:  134700, Reward: [-412.690 -412.690 -412.690] [71.420], Avg: [-408.817 -408.817 -408.817] (0.1000) ({r_i: None, r_t: [-768.394 -768.394 -768.394], eps: 0.1})
Step:  167100, Reward: [-379.449 -379.449 -379.449] [58.762], Avg: [-456.383 -456.383 -456.383] (0.0100) ({r_i: None, r_t: [-791.059 -791.059 -791.059], eps: 0.01})
Step:  134800, Reward: [-399.153 -399.153 -399.153] [95.370], Avg: [-408.810 -408.810 -408.810] (0.1000) ({r_i: None, r_t: [-834.918 -834.918 -834.918], eps: 0.1})
Step:  167200, Reward: [-428.784 -428.784 -428.784] [89.381], Avg: [-456.366 -456.366 -456.366] (0.0100) ({r_i: None, r_t: [-768.057 -768.057 -768.057], eps: 0.01})
Step:  134900, Reward: [-463.388 -463.388 -463.388] [101.290], Avg: [-408.851 -408.851 -408.851] (0.1000) ({r_i: None, r_t: [-832.196 -832.196 -832.196], eps: 0.1})
Step:  167300, Reward: [-407.829 -407.829 -407.829] [69.404], Avg: [-456.337 -456.337 -456.337] (0.0100) ({r_i: None, r_t: [-820.893 -820.893 -820.893], eps: 0.01})
Step:  135000, Reward: [-396.857 -396.857 -396.857] [70.526], Avg: [-408.842 -408.842 -408.842] (0.1000) ({r_i: None, r_t: [-861.608 -861.608 -861.608], eps: 0.1})
Step:  167400, Reward: [-372.787 -372.787 -372.787] [79.558], Avg: [-456.288 -456.288 -456.288] (0.0100) ({r_i: None, r_t: [-779.190 -779.190 -779.190], eps: 0.01})
Step:  135100, Reward: [-402.556 -402.556 -402.556] [71.270], Avg: [-408.837 -408.837 -408.837] (0.1000) ({r_i: None, r_t: [-820.571 -820.571 -820.571], eps: 0.1})
Step:  167500, Reward: [-393.569 -393.569 -393.569] [58.699], Avg: [-456.250 -456.250 -456.250] (0.0100) ({r_i: None, r_t: [-803.349 -803.349 -803.349], eps: 0.01})
Step:  135200, Reward: [-408.889 -408.889 -408.889] [99.929], Avg: [-408.837 -408.837 -408.837] (0.1000) ({r_i: None, r_t: [-833.808 -833.808 -833.808], eps: 0.1})
Step:  167600, Reward: [-374.736 -374.736 -374.736] [60.670], Avg: [-456.202 -456.202 -456.202] (0.0100) ({r_i: None, r_t: [-810.859 -810.859 -810.859], eps: 0.01})
Step:  135300, Reward: [-405.094 -405.094 -405.094] [95.871], Avg: [-408.834 -408.834 -408.834] (0.1000) ({r_i: None, r_t: [-833.242 -833.242 -833.242], eps: 0.1})
Step:  167700, Reward: [-380.953 -380.953 -380.953] [71.658], Avg: [-456.157 -456.157 -456.157] (0.0100) ({r_i: None, r_t: [-772.323 -772.323 -772.323], eps: 0.01})
Step:  135400, Reward: [-452.824 -452.824 -452.824] [93.394], Avg: [-408.867 -408.867 -408.867] (0.1000) ({r_i: None, r_t: [-829.542 -829.542 -829.542], eps: 0.1})
Step:  167800, Reward: [-392.301 -392.301 -392.301] [85.561], Avg: [-456.119 -456.119 -456.119] (0.0100) ({r_i: None, r_t: [-772.397 -772.397 -772.397], eps: 0.01})
Step:  135500, Reward: [-394.823 -394.823 -394.823] [55.741], Avg: [-408.857 -408.857 -408.857] (0.1000) ({r_i: None, r_t: [-791.590 -791.590 -791.590], eps: 0.1})
Step:  167900, Reward: [-380.778 -380.778 -380.778] [79.078], Avg: [-456.074 -456.074 -456.074] (0.0100) ({r_i: None, r_t: [-780.878 -780.878 -780.878], eps: 0.01})
Step:  135600, Reward: [-424.256 -424.256 -424.256] [104.732], Avg: [-408.868 -408.868 -408.868] (0.1000) ({r_i: None, r_t: [-822.822 -822.822 -822.822], eps: 0.1})
Step:  168000, Reward: [-378.022 -378.022 -378.022] [62.234], Avg: [-456.027 -456.027 -456.027] (0.0100) ({r_i: None, r_t: [-764.866 -764.866 -764.866], eps: 0.01})
Step:  135700, Reward: [-401.553 -401.553 -401.553] [61.989], Avg: [-408.862 -408.862 -408.862] (0.1000) ({r_i: None, r_t: [-838.400 -838.400 -838.400], eps: 0.1})
Step:  168100, Reward: [-424.930 -424.930 -424.930] [74.333], Avg: [-456.009 -456.009 -456.009] (0.0100) ({r_i: None, r_t: [-770.373 -770.373 -770.373], eps: 0.01})
Step:  135800, Reward: [-409.109 -409.109 -409.109] [91.070], Avg: [-408.863 -408.863 -408.863] (0.1000) ({r_i: None, r_t: [-793.244 -793.244 -793.244], eps: 0.1})
Step:  168200, Reward: [-388.733 -388.733 -388.733] [77.322], Avg: [-455.969 -455.969 -455.969] (0.0100) ({r_i: None, r_t: [-762.394 -762.394 -762.394], eps: 0.01})
Step:  135900, Reward: [-387.931 -387.931 -387.931] [78.864], Avg: [-408.847 -408.847 -408.847] (0.1000) ({r_i: None, r_t: [-778.725 -778.725 -778.725], eps: 0.1})
Step:  168300, Reward: [-371.911 -371.911 -371.911] [84.300], Avg: [-455.919 -455.919 -455.919] (0.0100) ({r_i: None, r_t: [-760.794 -760.794 -760.794], eps: 0.01})
Step:  136000, Reward: [-383.320 -383.320 -383.320] [57.308], Avg: [-408.829 -408.829 -408.829] (0.1000) ({r_i: None, r_t: [-836.461 -836.461 -836.461], eps: 0.1})
Step:  168400, Reward: [-364.047 -364.047 -364.047] [72.034], Avg: [-455.865 -455.865 -455.865] (0.0100) ({r_i: None, r_t: [-802.302 -802.302 -802.302], eps: 0.01})
Step:  136100, Reward: [-412.184 -412.184 -412.184] [78.554], Avg: [-408.831 -408.831 -408.831] (0.1000) ({r_i: None, r_t: [-785.495 -785.495 -785.495], eps: 0.1})
Step:  168500, Reward: [-369.430 -369.430 -369.430] [67.370], Avg: [-455.813 -455.813 -455.813] (0.0100) ({r_i: None, r_t: [-762.990 -762.990 -762.990], eps: 0.01})
Step:  136200, Reward: [-421.733 -421.733 -421.733] [89.537], Avg: [-408.840 -408.840 -408.840] (0.1000) ({r_i: None, r_t: [-776.507 -776.507 -776.507], eps: 0.1})
Step:  168600, Reward: [-399.374 -399.374 -399.374] [46.770], Avg: [-455.780 -455.780 -455.780] (0.0100) ({r_i: None, r_t: [-772.712 -772.712 -772.712], eps: 0.01})
Step:  136300, Reward: [-405.684 -405.684 -405.684] [67.802], Avg: [-408.838 -408.838 -408.838] (0.1000) ({r_i: None, r_t: [-804.886 -804.886 -804.886], eps: 0.1})
Step:  168700, Reward: [-372.597 -372.597 -372.597] [66.276], Avg: [-455.731 -455.731 -455.731] (0.0100) ({r_i: None, r_t: [-742.196 -742.196 -742.196], eps: 0.01})
Step:  136400, Reward: [-420.061 -420.061 -420.061] [55.685], Avg: [-408.846 -408.846 -408.846] (0.1000) ({r_i: None, r_t: [-782.129 -782.129 -782.129], eps: 0.1})
Step:  168800, Reward: [-364.323 -364.323 -364.323] [71.691], Avg: [-455.676 -455.676 -455.676] (0.0100) ({r_i: None, r_t: [-772.874 -772.874 -772.874], eps: 0.01})
Step:  136500, Reward: [-392.435 -392.435 -392.435] [81.881], Avg: [-408.834 -408.834 -408.834] (0.1000) ({r_i: None, r_t: [-755.678 -755.678 -755.678], eps: 0.1})
Step:  168900, Reward: [-381.938 -381.938 -381.938] [75.154], Avg: [-455.633 -455.633 -455.633] (0.0100) ({r_i: None, r_t: [-788.959 -788.959 -788.959], eps: 0.01})
Step:  136600, Reward: [-384.901 -384.901 -384.901] [75.412], Avg: [-408.817 -408.817 -408.817] (0.1000) ({r_i: None, r_t: [-844.601 -844.601 -844.601], eps: 0.1})
Step:  169000, Reward: [-407.357 -407.357 -407.357] [84.515], Avg: [-455.604 -455.604 -455.604] (0.0100) ({r_i: None, r_t: [-780.871 -780.871 -780.871], eps: 0.01})
Step:  136700, Reward: [-426.503 -426.503 -426.503] [109.524], Avg: [-408.830 -408.830 -408.830] (0.1000) ({r_i: None, r_t: [-780.778 -780.778 -780.778], eps: 0.1})
Step:  169100, Reward: [-370.257 -370.257 -370.257] [78.194], Avg: [-455.554 -455.554 -455.554] (0.0100) ({r_i: None, r_t: [-790.501 -790.501 -790.501], eps: 0.01})
Step:  136800, Reward: [-395.850 -395.850 -395.850] [58.913], Avg: [-408.820 -408.820 -408.820] (0.1000) ({r_i: None, r_t: [-786.641 -786.641 -786.641], eps: 0.1})
Step:  169200, Reward: [-417.659 -417.659 -417.659] [66.375], Avg: [-455.531 -455.531 -455.531] (0.0100) ({r_i: None, r_t: [-764.973 -764.973 -764.973], eps: 0.01})
Step:  136900, Reward: [-416.530 -416.530 -416.530] [81.240], Avg: [-408.826 -408.826 -408.826] (0.1000) ({r_i: None, r_t: [-837.697 -837.697 -837.697], eps: 0.1})
Step:  169300, Reward: [-352.821 -352.821 -352.821] [59.601], Avg: [-455.471 -455.471 -455.471] (0.0100) ({r_i: None, r_t: [-767.718 -767.718 -767.718], eps: 0.01})
Step:  137000, Reward: [-386.443 -386.443 -386.443] [59.367], Avg: [-408.810 -408.810 -408.810] (0.1000) ({r_i: None, r_t: [-775.419 -775.419 -775.419], eps: 0.1})
Step:  169400, Reward: [-406.053 -406.053 -406.053] [70.898], Avg: [-455.442 -455.442 -455.442] (0.0100) ({r_i: None, r_t: [-793.057 -793.057 -793.057], eps: 0.01})
Step:  137100, Reward: [-386.360 -386.360 -386.360] [84.261], Avg: [-408.793 -408.793 -408.793] (0.1000) ({r_i: None, r_t: [-764.749 -764.749 -764.749], eps: 0.1})
Step:  169500, Reward: [-369.879 -369.879 -369.879] [65.355], Avg: [-455.391 -455.391 -455.391] (0.0100) ({r_i: None, r_t: [-791.461 -791.461 -791.461], eps: 0.01})
Step:  137200, Reward: [-434.607 -434.607 -434.607] [73.215], Avg: [-408.812 -408.812 -408.812] (0.1000) ({r_i: None, r_t: [-814.225 -814.225 -814.225], eps: 0.1})
Step:  169600, Reward: [-364.326 -364.326 -364.326] [63.676], Avg: [-455.337 -455.337 -455.337] (0.0100) ({r_i: None, r_t: [-802.590 -802.590 -802.590], eps: 0.01})
Step:  137300, Reward: [-406.699 -406.699 -406.699] [82.781], Avg: [-408.810 -408.810 -408.810] (0.1000) ({r_i: None, r_t: [-828.313 -828.313 -828.313], eps: 0.1})
Step:  169700, Reward: [-418.670 -418.670 -418.670] [59.613], Avg: [-455.316 -455.316 -455.316] (0.0100) ({r_i: None, r_t: [-838.372 -838.372 -838.372], eps: 0.01})
Step:  137400, Reward: [-402.549 -402.549 -402.549] [47.021], Avg: [-408.806 -408.806 -408.806] (0.1000) ({r_i: None, r_t: [-754.699 -754.699 -754.699], eps: 0.1})
Step:  169800, Reward: [-344.178 -344.178 -344.178] [80.768], Avg: [-455.250 -455.250 -455.250] (0.0100) ({r_i: None, r_t: [-779.437 -779.437 -779.437], eps: 0.01})
Step:  137500, Reward: [-407.016 -407.016 -407.016] [108.183], Avg: [-408.805 -408.805 -408.805] (0.1000) ({r_i: None, r_t: [-794.997 -794.997 -794.997], eps: 0.1})
Step:  169900, Reward: [-406.495 -406.495 -406.495] [79.248], Avg: [-455.222 -455.222 -455.222] (0.0100) ({r_i: None, r_t: [-811.973 -811.973 -811.973], eps: 0.01})
Step:  137600, Reward: [-389.253 -389.253 -389.253] [70.370], Avg: [-408.790 -408.790 -408.790] (0.1000) ({r_i: None, r_t: [-792.087 -792.087 -792.087], eps: 0.1})
Step:  170000, Reward: [-377.097 -377.097 -377.097] [50.985], Avg: [-455.176 -455.176 -455.176] (0.0100) ({r_i: None, r_t: [-802.185 -802.185 -802.185], eps: 0.01})
Step:  137700, Reward: [-369.662 -369.662 -369.662] [69.109], Avg: [-408.762 -408.762 -408.762] (0.1000) ({r_i: None, r_t: [-796.811 -796.811 -796.811], eps: 0.1})
Step:  170100, Reward: [-376.801 -376.801 -376.801] [102.683], Avg: [-455.130 -455.130 -455.130] (0.0100) ({r_i: None, r_t: [-789.373 -789.373 -789.373], eps: 0.01})
Step:  137800, Reward: [-375.066 -375.066 -375.066] [54.208], Avg: [-408.738 -408.738 -408.738] (0.1000) ({r_i: None, r_t: [-766.754 -766.754 -766.754], eps: 0.1})
Step:  170200, Reward: [-390.533 -390.533 -390.533] [82.272], Avg: [-455.092 -455.092 -455.092] (0.0100) ({r_i: None, r_t: [-809.480 -809.480 -809.480], eps: 0.01})
Step:  137900, Reward: [-390.645 -390.645 -390.645] [61.404], Avg: [-408.724 -408.724 -408.724] (0.1000) ({r_i: None, r_t: [-807.429 -807.429 -807.429], eps: 0.1})
Step:  170300, Reward: [-373.120 -373.120 -373.120] [81.477], Avg: [-455.044 -455.044 -455.044] (0.0100) ({r_i: None, r_t: [-771.645 -771.645 -771.645], eps: 0.01})
Step:  138000, Reward: [-407.597 -407.597 -407.597] [70.560], Avg: [-408.724 -408.724 -408.724] (0.1000) ({r_i: None, r_t: [-809.392 -809.392 -809.392], eps: 0.1})
Step:  170400, Reward: [-385.322 -385.322 -385.322] [83.090], Avg: [-455.003 -455.003 -455.003] (0.0100) ({r_i: None, r_t: [-792.622 -792.622 -792.622], eps: 0.01})
Step:  138100, Reward: [-388.299 -388.299 -388.299] [57.667], Avg: [-408.709 -408.709 -408.709] (0.1000) ({r_i: None, r_t: [-781.025 -781.025 -781.025], eps: 0.1})
Step:  170500, Reward: [-356.757 -356.757 -356.757] [63.837], Avg: [-454.945 -454.945 -454.945] (0.0100) ({r_i: None, r_t: [-811.530 -811.530 -811.530], eps: 0.01})
Step:  138200, Reward: [-386.357 -386.357 -386.357] [54.273], Avg: [-408.693 -408.693 -408.693] (0.1000) ({r_i: None, r_t: [-793.719 -793.719 -793.719], eps: 0.1})
Step:  170600, Reward: [-353.064 -353.064 -353.064] [63.448], Avg: [-454.886 -454.886 -454.886] (0.0100) ({r_i: None, r_t: [-811.766 -811.766 -811.766], eps: 0.01})
Step:  138300, Reward: [-418.257 -418.257 -418.257] [87.877], Avg: [-408.700 -408.700 -408.700] (0.1000) ({r_i: None, r_t: [-762.589 -762.589 -762.589], eps: 0.1})
Step:  170700, Reward: [-382.725 -382.725 -382.725] [70.801], Avg: [-454.843 -454.843 -454.843] (0.0100) ({r_i: None, r_t: [-692.026 -692.026 -692.026], eps: 0.01})
Step:  138400, Reward: [-375.431 -375.431 -375.431] [70.524], Avg: [-408.676 -408.676 -408.676] (0.1000) ({r_i: None, r_t: [-782.391 -782.391 -782.391], eps: 0.1})
Step:  170800, Reward: [-386.827 -386.827 -386.827] [56.483], Avg: [-454.804 -454.804 -454.804] (0.0100) ({r_i: None, r_t: [-762.696 -762.696 -762.696], eps: 0.01})
Step:  138500, Reward: [-383.258 -383.258 -383.258] [70.850], Avg: [-408.657 -408.657 -408.657] (0.1000) ({r_i: None, r_t: [-789.548 -789.548 -789.548], eps: 0.1})
Step:  170900, Reward: [-407.885 -407.885 -407.885] [77.807], Avg: [-454.776 -454.776 -454.776] (0.0100) ({r_i: None, r_t: [-777.045 -777.045 -777.045], eps: 0.01})
Step:  138600, Reward: [-387.783 -387.783 -387.783] [87.441], Avg: [-408.642 -408.642 -408.642] (0.1000) ({r_i: None, r_t: [-774.257 -774.257 -774.257], eps: 0.1})
Step:  171000, Reward: [-364.247 -364.247 -364.247] [55.944], Avg: [-454.723 -454.723 -454.723] (0.0100) ({r_i: None, r_t: [-797.051 -797.051 -797.051], eps: 0.01})
Step:  138700, Reward: [-403.358 -403.358 -403.358] [73.298], Avg: [-408.638 -408.638 -408.638] (0.1000) ({r_i: None, r_t: [-756.041 -756.041 -756.041], eps: 0.1})
Step:  171100, Reward: [-397.903 -397.903 -397.903] [75.346], Avg: [-454.690 -454.690 -454.690] (0.0100) ({r_i: None, r_t: [-741.705 -741.705 -741.705], eps: 0.01})
Step:  138800, Reward: [-403.130 -403.130 -403.130] [52.891], Avg: [-408.634 -408.634 -408.634] (0.1000) ({r_i: None, r_t: [-792.653 -792.653 -792.653], eps: 0.1})
Step:  171200, Reward: [-389.490 -389.490 -389.490] [93.911], Avg: [-454.652 -454.652 -454.652] (0.0100) ({r_i: None, r_t: [-700.475 -700.475 -700.475], eps: 0.01})
Step:  138900, Reward: [-402.886 -402.886 -402.886] [62.247], Avg: [-408.630 -408.630 -408.630] (0.1000) ({r_i: None, r_t: [-823.386 -823.386 -823.386], eps: 0.1})
Step:  171300, Reward: [-380.491 -380.491 -380.491] [74.033], Avg: [-454.609 -454.609 -454.609] (0.0100) ({r_i: None, r_t: [-798.294 -798.294 -798.294], eps: 0.01})
Step:  139000, Reward: [-448.382 -448.382 -448.382] [81.201], Avg: [-408.659 -408.659 -408.659] (0.1000) ({r_i: None, r_t: [-811.136 -811.136 -811.136], eps: 0.1})
Step:  171400, Reward: [-401.064 -401.064 -401.064] [74.496], Avg: [-454.577 -454.577 -454.577] (0.0100) ({r_i: None, r_t: [-792.855 -792.855 -792.855], eps: 0.01})
Step:  139100, Reward: [-393.165 -393.165 -393.165] [52.343], Avg: [-408.648 -408.648 -408.648] (0.1000) ({r_i: None, r_t: [-784.753 -784.753 -784.753], eps: 0.1})
Step:  171500, Reward: [-370.008 -370.008 -370.008] [67.647], Avg: [-454.528 -454.528 -454.528] (0.0100) ({r_i: None, r_t: [-789.409 -789.409 -789.409], eps: 0.01})
Step:  139200, Reward: [-406.418 -406.418 -406.418] [77.221], Avg: [-408.646 -408.646 -408.646] (0.1000) ({r_i: None, r_t: [-820.865 -820.865 -820.865], eps: 0.1})
Step:  171600, Reward: [-375.595 -375.595 -375.595] [68.648], Avg: [-454.482 -454.482 -454.482] (0.0100) ({r_i: None, r_t: [-820.942 -820.942 -820.942], eps: 0.01})
Step:  139300, Reward: [-424.003 -424.003 -424.003] [60.999], Avg: [-408.657 -408.657 -408.657] (0.1000) ({r_i: None, r_t: [-790.044 -790.044 -790.044], eps: 0.1})
Step:  171700, Reward: [-380.325 -380.325 -380.325] [87.079], Avg: [-454.439 -454.439 -454.439] (0.0100) ({r_i: None, r_t: [-808.858 -808.858 -808.858], eps: 0.01})
Step:  139400, Reward: [-375.758 -375.758 -375.758] [76.556], Avg: [-408.634 -408.634 -408.634] (0.1000) ({r_i: None, r_t: [-785.613 -785.613 -785.613], eps: 0.1})
Step:  171800, Reward: [-382.643 -382.643 -382.643] [82.743], Avg: [-454.397 -454.397 -454.397] (0.0100) ({r_i: None, r_t: [-802.923 -802.923 -802.923], eps: 0.01})
Step:  139500, Reward: [-387.396 -387.396 -387.396] [56.492], Avg: [-408.618 -408.618 -408.618] (0.1000) ({r_i: None, r_t: [-793.182 -793.182 -793.182], eps: 0.1})
Step:  171900, Reward: [-428.490 -428.490 -428.490] [79.516], Avg: [-454.382 -454.382 -454.382] (0.0100) ({r_i: None, r_t: [-785.285 -785.285 -785.285], eps: 0.01})
Step:  139600, Reward: [-391.258 -391.258 -391.258] [67.525], Avg: [-408.606 -408.606 -408.606] (0.1000) ({r_i: None, r_t: [-787.240 -787.240 -787.240], eps: 0.1})
Step:  172000, Reward: [-385.673 -385.673 -385.673] [56.785], Avg: [-454.342 -454.342 -454.342] (0.0100) ({r_i: None, r_t: [-827.443 -827.443 -827.443], eps: 0.01})
Step:  139700, Reward: [-371.235 -371.235 -371.235] [66.811], Avg: [-408.579 -408.579 -408.579] (0.1000) ({r_i: None, r_t: [-794.955 -794.955 -794.955], eps: 0.1})
Step:  172100, Reward: [-396.201 -396.201 -396.201] [80.588], Avg: [-454.309 -454.309 -454.309] (0.0100) ({r_i: None, r_t: [-743.851 -743.851 -743.851], eps: 0.01})
Step:  139800, Reward: [-391.021 -391.021 -391.021] [65.663], Avg: [-408.567 -408.567 -408.567] (0.1000) ({r_i: None, r_t: [-789.468 -789.468 -789.468], eps: 0.1})
Step:  172200, Reward: [-401.810 -401.810 -401.810] [74.565], Avg: [-454.278 -454.278 -454.278] (0.0100) ({r_i: None, r_t: [-799.338 -799.338 -799.338], eps: 0.01})
Step:  139900, Reward: [-396.005 -396.005 -396.005] [59.879], Avg: [-408.558 -408.558 -408.558] (0.1000) ({r_i: None, r_t: [-807.402 -807.402 -807.402], eps: 0.1})
Step:  172300, Reward: [-397.029 -397.029 -397.029] [57.984], Avg: [-454.245 -454.245 -454.245] (0.0100) ({r_i: None, r_t: [-766.693 -766.693 -766.693], eps: 0.01})
Step:  140000, Reward: [-410.063 -410.063 -410.063] [57.854], Avg: [-408.559 -408.559 -408.559] (0.1000) ({r_i: None, r_t: [-776.431 -776.431 -776.431], eps: 0.1})
Step:  172400, Reward: [-387.597 -387.597 -387.597] [56.491], Avg: [-454.206 -454.206 -454.206] (0.0100) ({r_i: None, r_t: [-835.112 -835.112 -835.112], eps: 0.01})
Step:  140100, Reward: [-413.354 -413.354 -413.354] [74.060], Avg: [-408.562 -408.562 -408.562] (0.1000) ({r_i: None, r_t: [-793.462 -793.462 -793.462], eps: 0.1})
Step:  172500, Reward: [-364.977 -364.977 -364.977] [53.013], Avg: [-454.155 -454.155 -454.155] (0.0100) ({r_i: None, r_t: [-758.844 -758.844 -758.844], eps: 0.01})
Step:  140200, Reward: [-372.351 -372.351 -372.351] [66.834], Avg: [-408.536 -408.536 -408.536] (0.1000) ({r_i: None, r_t: [-793.297 -793.297 -793.297], eps: 0.1})
Step:  172600, Reward: [-368.513 -368.513 -368.513] [60.256], Avg: [-454.105 -454.105 -454.105] (0.0100) ({r_i: None, r_t: [-767.091 -767.091 -767.091], eps: 0.01})
Step:  140300, Reward: [-423.766 -423.766 -423.766] [68.986], Avg: [-408.547 -408.547 -408.547] (0.1000) ({r_i: None, r_t: [-767.025 -767.025 -767.025], eps: 0.1})
Step:  172700, Reward: [-423.017 -423.017 -423.017] [96.459], Avg: [-454.087 -454.087 -454.087] (0.0100) ({r_i: None, r_t: [-815.964 -815.964 -815.964], eps: 0.01})
Step:  140400, Reward: [-371.211 -371.211 -371.211] [62.107], Avg: [-408.521 -408.521 -408.521] (0.1000) ({r_i: None, r_t: [-797.474 -797.474 -797.474], eps: 0.1})
Step:  172800, Reward: [-367.292 -367.292 -367.292] [58.496], Avg: [-454.037 -454.037 -454.037] (0.0100) ({r_i: None, r_t: [-761.952 -761.952 -761.952], eps: 0.01})
Step:  140500, Reward: [-405.098 -405.098 -405.098] [55.169], Avg: [-408.518 -408.518 -408.518] (0.1000) ({r_i: None, r_t: [-821.798 -821.798 -821.798], eps: 0.1})
Step:  172900, Reward: [-381.612 -381.612 -381.612] [57.698], Avg: [-453.995 -453.995 -453.995] (0.0100) ({r_i: None, r_t: [-763.916 -763.916 -763.916], eps: 0.01})
Step:  140600, Reward: [-410.200 -410.200 -410.200] [76.214], Avg: [-408.519 -408.519 -408.519] (0.1000) ({r_i: None, r_t: [-809.732 -809.732 -809.732], eps: 0.1})
Step:  173000, Reward: [-385.232 -385.232 -385.232] [77.598], Avg: [-453.955 -453.955 -453.955] (0.0100) ({r_i: None, r_t: [-766.530 -766.530 -766.530], eps: 0.01})
Step:  140700, Reward: [-396.169 -396.169 -396.169] [81.288], Avg: [-408.511 -408.511 -408.511] (0.1000) ({r_i: None, r_t: [-756.373 -756.373 -756.373], eps: 0.1})
Step:  173100, Reward: [-372.801 -372.801 -372.801] [89.585], Avg: [-453.908 -453.908 -453.908] (0.0100) ({r_i: None, r_t: [-786.592 -786.592 -786.592], eps: 0.01})
Step:  140800, Reward: [-399.375 -399.375 -399.375] [77.602], Avg: [-408.504 -408.504 -408.504] (0.1000) ({r_i: None, r_t: [-782.209 -782.209 -782.209], eps: 0.1})
Step:  173200, Reward: [-368.608 -368.608 -368.608] [37.518], Avg: [-453.859 -453.859 -453.859] (0.0100) ({r_i: None, r_t: [-759.890 -759.890 -759.890], eps: 0.01})
Step:  140900, Reward: [-410.963 -410.963 -410.963] [61.947], Avg: [-408.506 -408.506 -408.506] (0.1000) ({r_i: None, r_t: [-824.288 -824.288 -824.288], eps: 0.1})
Step:  173300, Reward: [-402.278 -402.278 -402.278] [71.790], Avg: [-453.829 -453.829 -453.829] (0.0100) ({r_i: None, r_t: [-782.325 -782.325 -782.325], eps: 0.01})
Step:  141000, Reward: [-431.278 -431.278 -431.278] [68.204], Avg: [-408.522 -408.522 -408.522] (0.1000) ({r_i: None, r_t: [-811.189 -811.189 -811.189], eps: 0.1})
Step:  173400, Reward: [-403.924 -403.924 -403.924] [60.920], Avg: [-453.801 -453.801 -453.801] (0.0100) ({r_i: None, r_t: [-798.009 -798.009 -798.009], eps: 0.01})
Step:  141100, Reward: [-375.521 -375.521 -375.521] [59.850], Avg: [-408.499 -408.499 -408.499] (0.1000) ({r_i: None, r_t: [-759.478 -759.478 -759.478], eps: 0.1})
Step:  173500, Reward: [-380.777 -380.777 -380.777] [58.242], Avg: [-453.759 -453.759 -453.759] (0.0100) ({r_i: None, r_t: [-777.266 -777.266 -777.266], eps: 0.01})
Step:  141200, Reward: [-412.472 -412.472 -412.472] [57.045], Avg: [-408.501 -408.501 -408.501] (0.1000) ({r_i: None, r_t: [-811.683 -811.683 -811.683], eps: 0.1})
Step:  173600, Reward: [-381.750 -381.750 -381.750] [63.917], Avg: [-453.717 -453.717 -453.717] (0.0100) ({r_i: None, r_t: [-767.689 -767.689 -767.689], eps: 0.01})
Step:  141300, Reward: [-456.596 -456.596 -456.596] [98.874], Avg: [-408.535 -408.535 -408.535] (0.1000) ({r_i: None, r_t: [-774.362 -774.362 -774.362], eps: 0.1})
Step:  173700, Reward: [-401.928 -401.928 -401.928] [72.333], Avg: [-453.687 -453.687 -453.687] (0.0100) ({r_i: None, r_t: [-781.074 -781.074 -781.074], eps: 0.01})
Step:  141400, Reward: [-385.652 -385.652 -385.652] [90.358], Avg: [-408.519 -408.519 -408.519] (0.1000) ({r_i: None, r_t: [-822.651 -822.651 -822.651], eps: 0.1})
Step:  173800, Reward: [-379.015 -379.015 -379.015] [53.715], Avg: [-453.644 -453.644 -453.644] (0.0100) ({r_i: None, r_t: [-750.128 -750.128 -750.128], eps: 0.01})
Step:  141500, Reward: [-398.766 -398.766 -398.766] [83.156], Avg: [-408.512 -408.512 -408.512] (0.1000) ({r_i: None, r_t: [-842.741 -842.741 -842.741], eps: 0.1})
Step:  173900, Reward: [-402.360 -402.360 -402.360] [63.244], Avg: [-453.615 -453.615 -453.615] (0.0100) ({r_i: None, r_t: [-735.372 -735.372 -735.372], eps: 0.01})
Step:  141600, Reward: [-386.288 -386.288 -386.288] [87.355], Avg: [-408.497 -408.497 -408.497] (0.1000) ({r_i: None, r_t: [-811.224 -811.224 -811.224], eps: 0.1})
Step:  174000, Reward: [-365.359 -365.359 -365.359] [55.336], Avg: [-453.564 -453.564 -453.564] (0.0100) ({r_i: None, r_t: [-794.000 -794.000 -794.000], eps: 0.01})
Step:  141700, Reward: [-412.917 -412.917 -412.917] [53.942], Avg: [-408.500 -408.500 -408.500] (0.1000) ({r_i: None, r_t: [-737.241 -737.241 -737.241], eps: 0.1})
Step:  174100, Reward: [-411.279 -411.279 -411.279] [80.786], Avg: [-453.540 -453.540 -453.540] (0.0100) ({r_i: None, r_t: [-770.116 -770.116 -770.116], eps: 0.01})
Step:  141800, Reward: [-401.682 -401.682 -401.682] [58.091], Avg: [-408.495 -408.495 -408.495] (0.1000) ({r_i: None, r_t: [-783.504 -783.504 -783.504], eps: 0.1})
Step:  174200, Reward: [-356.765 -356.765 -356.765] [76.133], Avg: [-453.484 -453.484 -453.484] (0.0100) ({r_i: None, r_t: [-794.263 -794.263 -794.263], eps: 0.01})
Step:  141900, Reward: [-401.137 -401.137 -401.137] [76.990], Avg: [-408.490 -408.490 -408.490] (0.1000) ({r_i: None, r_t: [-744.672 -744.672 -744.672], eps: 0.1})
Step:  174300, Reward: [-422.884 -422.884 -422.884] [67.879], Avg: [-453.467 -453.467 -453.467] (0.0100) ({r_i: None, r_t: [-792.960 -792.960 -792.960], eps: 0.01})
Step:  142000, Reward: [-371.693 -371.693 -371.693] [50.499], Avg: [-408.464 -408.464 -408.464] (0.1000) ({r_i: None, r_t: [-837.362 -837.362 -837.362], eps: 0.1})
Step:  174400, Reward: [-370.503 -370.503 -370.503] [65.826], Avg: [-453.419 -453.419 -453.419] (0.0100) ({r_i: None, r_t: [-790.609 -790.609 -790.609], eps: 0.01})
Step:  142100, Reward: [-398.503 -398.503 -398.503] [82.004], Avg: [-408.457 -408.457 -408.457] (0.1000) ({r_i: None, r_t: [-768.657 -768.657 -768.657], eps: 0.1})
Step:  174500, Reward: [-412.809 -412.809 -412.809] [72.908], Avg: [-453.396 -453.396 -453.396] (0.0100) ({r_i: None, r_t: [-764.949 -764.949 -764.949], eps: 0.01})
Step:  142200, Reward: [-398.211 -398.211 -398.211] [74.826], Avg: [-408.450 -408.450 -408.450] (0.1000) ({r_i: None, r_t: [-786.047 -786.047 -786.047], eps: 0.1})
Step:  174600, Reward: [-396.373 -396.373 -396.373] [70.986], Avg: [-453.363 -453.363 -453.363] (0.0100) ({r_i: None, r_t: [-790.831 -790.831 -790.831], eps: 0.01})
Step:  142300, Reward: [-376.606 -376.606 -376.606] [61.505], Avg: [-408.427 -408.427 -408.427] (0.1000) ({r_i: None, r_t: [-838.738 -838.738 -838.738], eps: 0.1})
Step:  174700, Reward: [-410.915 -410.915 -410.915] [58.550], Avg: [-453.339 -453.339 -453.339] (0.0100) ({r_i: None, r_t: [-762.183 -762.183 -762.183], eps: 0.01})
Step:  142400, Reward: [-386.187 -386.187 -386.187] [57.898], Avg: [-408.412 -408.412 -408.412] (0.1000) ({r_i: None, r_t: [-794.689 -794.689 -794.689], eps: 0.1})
Step:  174800, Reward: [-391.212 -391.212 -391.212] [90.936], Avg: [-453.304 -453.304 -453.304] (0.0100) ({r_i: None, r_t: [-821.662 -821.662 -821.662], eps: 0.01})
Step:  142500, Reward: [-415.217 -415.217 -415.217] [58.023], Avg: [-408.417 -408.417 -408.417] (0.1000) ({r_i: None, r_t: [-777.559 -777.559 -777.559], eps: 0.1})
Step:  174900, Reward: [-359.311 -359.311 -359.311] [99.111], Avg: [-453.250 -453.250 -453.250] (0.0100) ({r_i: None, r_t: [-842.889 -842.889 -842.889], eps: 0.01})
Step:  142600, Reward: [-420.985 -420.985 -420.985] [85.003], Avg: [-408.425 -408.425 -408.425] (0.1000) ({r_i: None, r_t: [-852.517 -852.517 -852.517], eps: 0.1})
Step:  175000, Reward: [-387.759 -387.759 -387.759] [73.473], Avg: [-453.212 -453.212 -453.212] (0.0100) ({r_i: None, r_t: [-839.670 -839.670 -839.670], eps: 0.01})
Step:  142700, Reward: [-388.021 -388.021 -388.021] [76.250], Avg: [-408.411 -408.411 -408.411] (0.1000) ({r_i: None, r_t: [-844.087 -844.087 -844.087], eps: 0.1})
Step:  175100, Reward: [-371.740 -371.740 -371.740] [56.290], Avg: [-453.166 -453.166 -453.166] (0.0100) ({r_i: None, r_t: [-809.196 -809.196 -809.196], eps: 0.01})
Step:  142800, Reward: [-421.959 -421.959 -421.959] [73.616], Avg: [-408.421 -408.421 -408.421] (0.1000) ({r_i: None, r_t: [-778.902 -778.902 -778.902], eps: 0.1})
Step:  175200, Reward: [-432.137 -432.137 -432.137] [88.640], Avg: [-453.154 -453.154 -453.154] (0.0100) ({r_i: None, r_t: [-820.667 -820.667 -820.667], eps: 0.01})
Step:  142900, Reward: [-403.790 -403.790 -403.790] [62.047], Avg: [-408.417 -408.417 -408.417] (0.1000) ({r_i: None, r_t: [-833.695 -833.695 -833.695], eps: 0.1})
Step:  175300, Reward: [-406.097 -406.097 -406.097] [85.106], Avg: [-453.127 -453.127 -453.127] (0.0100) ({r_i: None, r_t: [-830.728 -830.728 -830.728], eps: 0.01})
Step:  143000, Reward: [-414.858 -414.858 -414.858] [66.617], Avg: [-408.422 -408.422 -408.422] (0.1000) ({r_i: None, r_t: [-809.439 -809.439 -809.439], eps: 0.1})
Step:  175400, Reward: [-410.126 -410.126 -410.126] [95.873], Avg: [-453.103 -453.103 -453.103] (0.0100) ({r_i: None, r_t: [-827.100 -827.100 -827.100], eps: 0.01})
Step:  143100, Reward: [-382.575 -382.575 -382.575] [66.235], Avg: [-408.404 -408.404 -408.404] (0.1000) ({r_i: None, r_t: [-783.449 -783.449 -783.449], eps: 0.1})
Step:  175500, Reward: [-366.457 -366.457 -366.457] [44.641], Avg: [-453.053 -453.053 -453.053] (0.0100) ({r_i: None, r_t: [-778.616 -778.616 -778.616], eps: 0.01})
Step:  143200, Reward: [-404.410 -404.410 -404.410] [65.944], Avg: [-408.401 -408.401 -408.401] (0.1000) ({r_i: None, r_t: [-786.484 -786.484 -786.484], eps: 0.1})
Step:  175600, Reward: [-377.202 -377.202 -377.202] [63.105], Avg: [-453.010 -453.010 -453.010] (0.0100) ({r_i: None, r_t: [-755.499 -755.499 -755.499], eps: 0.01})
Step:  143300, Reward: [-369.016 -369.016 -369.016] [56.074], Avg: [-408.374 -408.374 -408.374] (0.1000) ({r_i: None, r_t: [-792.236 -792.236 -792.236], eps: 0.1})
Step:  175700, Reward: [-432.762 -432.762 -432.762] [57.421], Avg: [-452.999 -452.999 -452.999] (0.0100) ({r_i: None, r_t: [-765.077 -765.077 -765.077], eps: 0.01})
Step:  143400, Reward: [-385.473 -385.473 -385.473] [74.475], Avg: [-408.358 -408.358 -408.358] (0.1000) ({r_i: None, r_t: [-785.821 -785.821 -785.821], eps: 0.1})
Step:  175800, Reward: [-412.203 -412.203 -412.203] [99.563], Avg: [-452.975 -452.975 -452.975] (0.0100) ({r_i: None, r_t: [-735.735 -735.735 -735.735], eps: 0.01})
Step:  143500, Reward: [-374.841 -374.841 -374.841] [46.643], Avg: [-408.334 -408.334 -408.334] (0.1000) ({r_i: None, r_t: [-747.332 -747.332 -747.332], eps: 0.1})
Step:  175900, Reward: [-425.089 -425.089 -425.089] [97.039], Avg: [-452.960 -452.960 -452.960] (0.0100) ({r_i: None, r_t: [-793.819 -793.819 -793.819], eps: 0.01})
Step:  143600, Reward: [-362.799 -362.799 -362.799] [39.844], Avg: [-408.303 -408.303 -408.303] (0.1000) ({r_i: None, r_t: [-778.306 -778.306 -778.306], eps: 0.1})
Step:  176000, Reward: [-400.243 -400.243 -400.243] [66.494], Avg: [-452.930 -452.930 -452.930] (0.0100) ({r_i: None, r_t: [-748.584 -748.584 -748.584], eps: 0.01})
Step:  143700, Reward: [-367.312 -367.312 -367.312] [52.120], Avg: [-408.274 -408.274 -408.274] (0.1000) ({r_i: None, r_t: [-818.385 -818.385 -818.385], eps: 0.1})
Step:  176100, Reward: [-375.799 -375.799 -375.799] [65.159], Avg: [-452.886 -452.886 -452.886] (0.0100) ({r_i: None, r_t: [-805.121 -805.121 -805.121], eps: 0.01})
Step:  143800, Reward: [-370.532 -370.532 -370.532] [61.524], Avg: [-408.248 -408.248 -408.248] (0.1000) ({r_i: None, r_t: [-809.507 -809.507 -809.507], eps: 0.1})
Step:  176200, Reward: [-380.288 -380.288 -380.288] [76.850], Avg: [-452.845 -452.845 -452.845] (0.0100) ({r_i: None, r_t: [-732.798 -732.798 -732.798], eps: 0.01})
Step:  143900, Reward: [-406.218 -406.218 -406.218] [60.219], Avg: [-408.246 -408.246 -408.246] (0.1000) ({r_i: None, r_t: [-797.705 -797.705 -797.705], eps: 0.1})
Step:  176300, Reward: [-394.202 -394.202 -394.202] [63.968], Avg: [-452.811 -452.811 -452.811] (0.0100) ({r_i: None, r_t: [-761.667 -761.667 -761.667], eps: 0.01})
Step:  144000, Reward: [-400.677 -400.677 -400.677] [74.566], Avg: [-408.241 -408.241 -408.241] (0.1000) ({r_i: None, r_t: [-770.269 -770.269 -770.269], eps: 0.1})
Step:  176400, Reward: [-378.118 -378.118 -378.118] [64.724], Avg: [-452.769 -452.769 -452.769] (0.0100) ({r_i: None, r_t: [-777.777 -777.777 -777.777], eps: 0.01})
Step:  144100, Reward: [-352.177 -352.177 -352.177] [45.363], Avg: [-408.202 -408.202 -408.202] (0.1000) ({r_i: None, r_t: [-809.386 -809.386 -809.386], eps: 0.1})
Step:  176500, Reward: [-397.020 -397.020 -397.020] [93.975], Avg: [-452.738 -452.738 -452.738] (0.0100) ({r_i: None, r_t: [-801.955 -801.955 -801.955], eps: 0.01})
Step:  144200, Reward: [-381.770 -381.770 -381.770] [54.283], Avg: [-408.184 -408.184 -408.184] (0.1000) ({r_i: None, r_t: [-788.233 -788.233 -788.233], eps: 0.1})
Step:  176600, Reward: [-424.670 -424.670 -424.670] [83.145], Avg: [-452.722 -452.722 -452.722] (0.0100) ({r_i: None, r_t: [-878.037 -878.037 -878.037], eps: 0.01})
Step:  144300, Reward: [-380.312 -380.312 -380.312] [72.269], Avg: [-408.165 -408.165 -408.165] (0.1000) ({r_i: None, r_t: [-815.087 -815.087 -815.087], eps: 0.1})
Step:  176700, Reward: [-369.013 -369.013 -369.013] [79.712], Avg: [-452.674 -452.674 -452.674] (0.0100) ({r_i: None, r_t: [-851.334 -851.334 -851.334], eps: 0.01})
Step:  144400, Reward: [-406.373 -406.373 -406.373] [66.681], Avg: [-408.163 -408.163 -408.163] (0.1000) ({r_i: None, r_t: [-762.615 -762.615 -762.615], eps: 0.1})
Step:  176800, Reward: [-402.888 -402.888 -402.888] [77.113], Avg: [-452.646 -452.646 -452.646] (0.0100) ({r_i: None, r_t: [-817.480 -817.480 -817.480], eps: 0.01})
Step:  144500, Reward: [-376.779 -376.779 -376.779] [58.191], Avg: [-408.142 -408.142 -408.142] (0.1000) ({r_i: None, r_t: [-824.915 -824.915 -824.915], eps: 0.1})
Step:  176900, Reward: [-397.520 -397.520 -397.520] [83.971], Avg: [-452.615 -452.615 -452.615] (0.0100) ({r_i: None, r_t: [-784.794 -784.794 -784.794], eps: 0.01})
Step:  144600, Reward: [-376.046 -376.046 -376.046] [60.471], Avg: [-408.120 -408.120 -408.120] (0.1000) ({r_i: None, r_t: [-745.175 -745.175 -745.175], eps: 0.1})
Step:  177000, Reward: [-378.457 -378.457 -378.457] [66.621], Avg: [-452.573 -452.573 -452.573] (0.0100) ({r_i: None, r_t: [-756.965 -756.965 -756.965], eps: 0.01})
Step:  144700, Reward: [-408.257 -408.257 -408.257] [82.353], Avg: [-408.120 -408.120 -408.120] (0.1000) ({r_i: None, r_t: [-760.521 -760.521 -760.521], eps: 0.1})
Step:  177100, Reward: [-379.173 -379.173 -379.173] [54.968], Avg: [-452.532 -452.532 -452.532] (0.0100) ({r_i: None, r_t: [-753.811 -753.811 -753.811], eps: 0.01})
Step:  144800, Reward: [-396.509 -396.509 -396.509] [64.029], Avg: [-408.112 -408.112 -408.112] (0.1000) ({r_i: None, r_t: [-797.721 -797.721 -797.721], eps: 0.1})
Step:  177200, Reward: [-400.872 -400.872 -400.872] [86.615], Avg: [-452.503 -452.503 -452.503] (0.0100) ({r_i: None, r_t: [-748.224 -748.224 -748.224], eps: 0.01})
Step:  144900, Reward: [-370.663 -370.663 -370.663] [47.780], Avg: [-408.086 -408.086 -408.086] (0.1000) ({r_i: None, r_t: [-733.693 -733.693 -733.693], eps: 0.1})
Step:  177300, Reward: [-391.647 -391.647 -391.647] [79.185], Avg: [-452.468 -452.468 -452.468] (0.0100) ({r_i: None, r_t: [-842.821 -842.821 -842.821], eps: 0.01})
Step:  145000, Reward: [-373.221 -373.221 -373.221] [56.407], Avg: [-408.062 -408.062 -408.062] (0.1000) ({r_i: None, r_t: [-744.772 -744.772 -744.772], eps: 0.1})
Step:  177400, Reward: [-390.985 -390.985 -390.985] [76.892], Avg: [-452.434 -452.434 -452.434] (0.0100) ({r_i: None, r_t: [-798.073 -798.073 -798.073], eps: 0.01})
Step:  145100, Reward: [-388.950 -388.950 -388.950] [54.822], Avg: [-408.049 -408.049 -408.049] (0.1000) ({r_i: None, r_t: [-765.428 -765.428 -765.428], eps: 0.1})
Step:  177500, Reward: [-392.237 -392.237 -392.237] [77.278], Avg: [-452.400 -452.400 -452.400] (0.0100) ({r_i: None, r_t: [-781.797 -781.797 -781.797], eps: 0.01})
Step:  145200, Reward: [-406.451 -406.451 -406.451] [56.561], Avg: [-408.048 -408.048 -408.048] (0.1000) ({r_i: None, r_t: [-808.323 -808.323 -808.323], eps: 0.1})
Step:  177600, Reward: [-426.140 -426.140 -426.140] [109.039], Avg: [-452.385 -452.385 -452.385] (0.0100) ({r_i: None, r_t: [-785.693 -785.693 -785.693], eps: 0.01})
Step:  145300, Reward: [-391.805 -391.805 -391.805] [61.048], Avg: [-408.036 -408.036 -408.036] (0.1000) ({r_i: None, r_t: [-771.460 -771.460 -771.460], eps: 0.1})
Step:  177700, Reward: [-421.074 -421.074 -421.074] [70.942], Avg: [-452.367 -452.367 -452.367] (0.0100) ({r_i: None, r_t: [-787.692 -787.692 -787.692], eps: 0.01})
Step:  145400, Reward: [-399.207 -399.207 -399.207] [88.851], Avg: [-408.030 -408.030 -408.030] (0.1000) ({r_i: None, r_t: [-820.131 -820.131 -820.131], eps: 0.1})
Step:  177800, Reward: [-370.712 -370.712 -370.712] [62.329], Avg: [-452.321 -452.321 -452.321] (0.0100) ({r_i: None, r_t: [-834.934 -834.934 -834.934], eps: 0.01})
Step:  145500, Reward: [-413.939 -413.939 -413.939] [78.620], Avg: [-408.034 -408.034 -408.034] (0.1000) ({r_i: None, r_t: [-793.315 -793.315 -793.315], eps: 0.1})
Step:  177900, Reward: [-384.830 -384.830 -384.830] [75.746], Avg: [-452.284 -452.284 -452.284] (0.0100) ({r_i: None, r_t: [-824.736 -824.736 -824.736], eps: 0.01})
Step:  145600, Reward: [-370.235 -370.235 -370.235] [56.256], Avg: [-408.008 -408.008 -408.008] (0.1000) ({r_i: None, r_t: [-753.713 -753.713 -753.713], eps: 0.1})
Step:  178000, Reward: [-387.150 -387.150 -387.150] [96.991], Avg: [-452.247 -452.247 -452.247] (0.0100) ({r_i: None, r_t: [-783.884 -783.884 -783.884], eps: 0.01})
Step:  145700, Reward: [-371.662 -371.662 -371.662] [51.218], Avg: [-407.983 -407.983 -407.983] (0.1000) ({r_i: None, r_t: [-784.152 -784.152 -784.152], eps: 0.1})
Step:  178100, Reward: [-420.725 -420.725 -420.725] [87.104], Avg: [-452.229 -452.229 -452.229] (0.0100) ({r_i: None, r_t: [-751.079 -751.079 -751.079], eps: 0.01})
Step:  145800, Reward: [-399.109 -399.109 -399.109] [53.040], Avg: [-407.977 -407.977 -407.977] (0.1000) ({r_i: None, r_t: [-794.314 -794.314 -794.314], eps: 0.1})
Step:  178200, Reward: [-414.878 -414.878 -414.878] [94.063], Avg: [-452.208 -452.208 -452.208] (0.0100) ({r_i: None, r_t: [-803.653 -803.653 -803.653], eps: 0.01})
Step:  145900, Reward: [-433.820 -433.820 -433.820] [82.481], Avg: [-407.995 -407.995 -407.995] (0.1000) ({r_i: None, r_t: [-795.736 -795.736 -795.736], eps: 0.1})
Step:  178300, Reward: [-385.700 -385.700 -385.700] [69.404], Avg: [-452.171 -452.171 -452.171] (0.0100) ({r_i: None, r_t: [-784.348 -784.348 -784.348], eps: 0.01})
Step:  146000, Reward: [-417.597 -417.597 -417.597] [62.094], Avg: [-408.002 -408.002 -408.002] (0.1000) ({r_i: None, r_t: [-770.735 -770.735 -770.735], eps: 0.1})
Step:  178400, Reward: [-407.596 -407.596 -407.596] [81.644], Avg: [-452.146 -452.146 -452.146] (0.0100) ({r_i: None, r_t: [-779.837 -779.837 -779.837], eps: 0.01})
Step:  146100, Reward: [-425.244 -425.244 -425.244] [88.156], Avg: [-408.013 -408.013 -408.013] (0.1000) ({r_i: None, r_t: [-815.515 -815.515 -815.515], eps: 0.1})
Step:  178500, Reward: [-394.913 -394.913 -394.913] [99.874], Avg: [-452.114 -452.114 -452.114] (0.0100) ({r_i: None, r_t: [-830.009 -830.009 -830.009], eps: 0.01})
Step:  146200, Reward: [-394.300 -394.300 -394.300] [59.844], Avg: [-408.004 -408.004 -408.004] (0.1000) ({r_i: None, r_t: [-791.244 -791.244 -791.244], eps: 0.1})
Step:  178600, Reward: [-416.431 -416.431 -416.431] [50.574], Avg: [-452.094 -452.094 -452.094] (0.0100) ({r_i: None, r_t: [-819.235 -819.235 -819.235], eps: 0.01})
Step:  146300, Reward: [-382.722 -382.722 -382.722] [83.794], Avg: [-407.987 -407.987 -407.987] (0.1000) ({r_i: None, r_t: [-841.468 -841.468 -841.468], eps: 0.1})
Step:  178700, Reward: [-409.771 -409.771 -409.771] [100.271], Avg: [-452.070 -452.070 -452.070] (0.0100) ({r_i: None, r_t: [-797.840 -797.840 -797.840], eps: 0.01})
Step:  146400, Reward: [-387.643 -387.643 -387.643] [47.310], Avg: [-407.973 -407.973 -407.973] (0.1000) ({r_i: None, r_t: [-794.562 -794.562 -794.562], eps: 0.1})
Step:  178800, Reward: [-385.695 -385.695 -385.695] [53.908], Avg: [-452.033 -452.033 -452.033] (0.0100) ({r_i: None, r_t: [-821.202 -821.202 -821.202], eps: 0.01})
Step:  146500, Reward: [-404.353 -404.353 -404.353] [66.254], Avg: [-407.970 -407.970 -407.970] (0.1000) ({r_i: None, r_t: [-791.993 -791.993 -791.993], eps: 0.1})
Step:  178900, Reward: [-379.748 -379.748 -379.748] [79.079], Avg: [-451.993 -451.993 -451.993] (0.0100) ({r_i: None, r_t: [-841.579 -841.579 -841.579], eps: 0.01})
Step:  146600, Reward: [-414.178 -414.178 -414.178] [67.652], Avg: [-407.975 -407.975 -407.975] (0.1000) ({r_i: None, r_t: [-798.595 -798.595 -798.595], eps: 0.1})
Step:  179000, Reward: [-398.968 -398.968 -398.968] [69.267], Avg: [-451.963 -451.963 -451.963] (0.0100) ({r_i: None, r_t: [-808.983 -808.983 -808.983], eps: 0.01})
Step:  146700, Reward: [-422.282 -422.282 -422.282] [68.086], Avg: [-407.984 -407.984 -407.984] (0.1000) ({r_i: None, r_t: [-809.016 -809.016 -809.016], eps: 0.1})
Step:  179100, Reward: [-378.136 -378.136 -378.136] [85.762], Avg: [-451.922 -451.922 -451.922] (0.0100) ({r_i: None, r_t: [-796.369 -796.369 -796.369], eps: 0.01})
Step:  146800, Reward: [-369.432 -369.432 -369.432] [61.825], Avg: [-407.958 -407.958 -407.958] (0.1000) ({r_i: None, r_t: [-797.028 -797.028 -797.028], eps: 0.1})
Step:  179200, Reward: [-400.215 -400.215 -400.215] [95.664], Avg: [-451.893 -451.893 -451.893] (0.0100) ({r_i: None, r_t: [-829.413 -829.413 -829.413], eps: 0.01})
Step:  146900, Reward: [-394.282 -394.282 -394.282] [101.385], Avg: [-407.949 -407.949 -407.949] (0.1000) ({r_i: None, r_t: [-776.692 -776.692 -776.692], eps: 0.1})
Step:  179300, Reward: [-365.566 -365.566 -365.566] [60.994], Avg: [-451.845 -451.845 -451.845] (0.0100) ({r_i: None, r_t: [-804.302 -804.302 -804.302], eps: 0.01})
Step:  147000, Reward: [-371.328 -371.328 -371.328] [46.075], Avg: [-407.924 -407.924 -407.924] (0.1000) ({r_i: None, r_t: [-813.886 -813.886 -813.886], eps: 0.1})
Step:  179400, Reward: [-399.322 -399.322 -399.322] [72.580], Avg: [-451.816 -451.816 -451.816] (0.0100) ({r_i: None, r_t: [-803.225 -803.225 -803.225], eps: 0.01})
Step:  147100, Reward: [-416.159 -416.159 -416.159] [54.130], Avg: [-407.930 -407.930 -407.930] (0.1000) ({r_i: None, r_t: [-806.232 -806.232 -806.232], eps: 0.1})
Step:  179500, Reward: [-399.365 -399.365 -399.365] [65.677], Avg: [-451.787 -451.787 -451.787] (0.0100) ({r_i: None, r_t: [-799.732 -799.732 -799.732], eps: 0.01})
Step:  147200, Reward: [-392.598 -392.598 -392.598] [71.301], Avg: [-407.919 -407.919 -407.919] (0.1000) ({r_i: None, r_t: [-825.297 -825.297 -825.297], eps: 0.1})
Step:  179600, Reward: [-388.970 -388.970 -388.970] [87.136], Avg: [-451.752 -451.752 -451.752] (0.0100) ({r_i: None, r_t: [-856.300 -856.300 -856.300], eps: 0.01})
Step:  147300, Reward: [-383.771 -383.771 -383.771] [78.512], Avg: [-407.903 -407.903 -407.903] (0.1000) ({r_i: None, r_t: [-805.977 -805.977 -805.977], eps: 0.1})
Step:  179700, Reward: [-408.062 -408.062 -408.062] [67.616], Avg: [-451.727 -451.727 -451.727] (0.0100) ({r_i: None, r_t: [-814.349 -814.349 -814.349], eps: 0.01})
Step:  147400, Reward: [-381.326 -381.326 -381.326] [80.864], Avg: [-407.885 -407.885 -407.885] (0.1000) ({r_i: None, r_t: [-791.301 -791.301 -791.301], eps: 0.1})
Step:  179800, Reward: [-435.179 -435.179 -435.179] [84.383], Avg: [-451.718 -451.718 -451.718] (0.0100) ({r_i: None, r_t: [-812.212 -812.212 -812.212], eps: 0.01})
Step:  147500, Reward: [-402.846 -402.846 -402.846] [75.440], Avg: [-407.881 -407.881 -407.881] (0.1000) ({r_i: None, r_t: [-751.947 -751.947 -751.947], eps: 0.1})
Step:  179900, Reward: [-415.300 -415.300 -415.300] [85.371], Avg: [-451.698 -451.698 -451.698] (0.0100) ({r_i: None, r_t: [-809.638 -809.638 -809.638], eps: 0.01})
Step:  147600, Reward: [-382.944 -382.944 -382.944] [52.304], Avg: [-407.864 -407.864 -407.864] (0.1000) ({r_i: None, r_t: [-803.361 -803.361 -803.361], eps: 0.1})
Step:  180000, Reward: [-385.467 -385.467 -385.467] [65.522], Avg: [-451.661 -451.661 -451.661] (0.0100) ({r_i: None, r_t: [-857.071 -857.071 -857.071], eps: 0.01})
Step:  147700, Reward: [-390.409 -390.409 -390.409] [53.971], Avg: [-407.853 -407.853 -407.853] (0.1000) ({r_i: None, r_t: [-823.732 -823.732 -823.732], eps: 0.1})
Step:  180100, Reward: [-374.456 -374.456 -374.456] [56.064], Avg: [-451.618 -451.618 -451.618] (0.0100) ({r_i: None, r_t: [-829.979 -829.979 -829.979], eps: 0.01})
Step:  147800, Reward: [-382.447 -382.447 -382.447] [48.404], Avg: [-407.835 -407.835 -407.835] (0.1000) ({r_i: None, r_t: [-796.354 -796.354 -796.354], eps: 0.1})
Step:  180200, Reward: [-403.831 -403.831 -403.831] [74.540], Avg: [-451.592 -451.592 -451.592] (0.0100) ({r_i: None, r_t: [-771.439 -771.439 -771.439], eps: 0.01})
Step:  147900, Reward: [-447.355 -447.355 -447.355] [74.832], Avg: [-407.862 -407.862 -407.862] (0.1000) ({r_i: None, r_t: [-800.515 -800.515 -800.515], eps: 0.1})
Step:  180300, Reward: [-382.875 -382.875 -382.875] [67.387], Avg: [-451.554 -451.554 -451.554] (0.0100) ({r_i: None, r_t: [-766.159 -766.159 -766.159], eps: 0.01})
Step:  148000, Reward: [-407.015 -407.015 -407.015] [69.859], Avg: [-407.862 -407.862 -407.862] (0.1000) ({r_i: None, r_t: [-802.517 -802.517 -802.517], eps: 0.1})
Step:  180400, Reward: [-390.898 -390.898 -390.898] [99.050], Avg: [-451.520 -451.520 -451.520] (0.0100) ({r_i: None, r_t: [-775.874 -775.874 -775.874], eps: 0.01})
Step:  148100, Reward: [-382.265 -382.265 -382.265] [54.175], Avg: [-407.844 -407.844 -407.844] (0.1000) ({r_i: None, r_t: [-755.055 -755.055 -755.055], eps: 0.1})
Step:  180500, Reward: [-389.816 -389.816 -389.816] [72.658], Avg: [-451.486 -451.486 -451.486] (0.0100) ({r_i: None, r_t: [-768.181 -768.181 -768.181], eps: 0.01})
Step:  148200, Reward: [-375.955 -375.955 -375.955] [58.845], Avg: [-407.823 -407.823 -407.823] (0.1000) ({r_i: None, r_t: [-823.793 -823.793 -823.793], eps: 0.1})
Step:  180600, Reward: [-401.450 -401.450 -401.450] [67.550], Avg: [-451.458 -451.458 -451.458] (0.0100) ({r_i: None, r_t: [-773.424 -773.424 -773.424], eps: 0.01})
Step:  148300, Reward: [-419.665 -419.665 -419.665] [80.564], Avg: [-407.831 -407.831 -407.831] (0.1000) ({r_i: None, r_t: [-761.024 -761.024 -761.024], eps: 0.1})
Step:  180700, Reward: [-409.452 -409.452 -409.452] [73.653], Avg: [-451.435 -451.435 -451.435] (0.0100) ({r_i: None, r_t: [-803.328 -803.328 -803.328], eps: 0.01})
Step:  148400, Reward: [-407.658 -407.658 -407.658] [50.984], Avg: [-407.831 -407.831 -407.831] (0.1000) ({r_i: None, r_t: [-814.998 -814.998 -814.998], eps: 0.1})
Step:  180800, Reward: [-374.835 -374.835 -374.835] [58.025], Avg: [-451.393 -451.393 -451.393] (0.0100) ({r_i: None, r_t: [-824.063 -824.063 -824.063], eps: 0.01})
Step:  148500, Reward: [-407.419 -407.419 -407.419] [69.962], Avg: [-407.830 -407.830 -407.830] (0.1000) ({r_i: None, r_t: [-779.893 -779.893 -779.893], eps: 0.1})
Step:  180900, Reward: [-362.918 -362.918 -362.918] [62.417], Avg: [-451.344 -451.344 -451.344] (0.0100) ({r_i: None, r_t: [-813.051 -813.051 -813.051], eps: 0.01})
Step:  148600, Reward: [-422.195 -422.195 -422.195] [57.672], Avg: [-407.840 -407.840 -407.840] (0.1000) ({r_i: None, r_t: [-795.611 -795.611 -795.611], eps: 0.1})
Step:  181000, Reward: [-430.373 -430.373 -430.373] [65.615], Avg: [-451.332 -451.332 -451.332] (0.0100) ({r_i: None, r_t: [-795.013 -795.013 -795.013], eps: 0.01})
Step:  148700, Reward: [-417.195 -417.195 -417.195] [83.274], Avg: [-407.846 -407.846 -407.846] (0.1000) ({r_i: None, r_t: [-803.803 -803.803 -803.803], eps: 0.1})
Step:  181100, Reward: [-404.066 -404.066 -404.066] [96.344], Avg: [-451.306 -451.306 -451.306] (0.0100) ({r_i: None, r_t: [-819.930 -819.930 -819.930], eps: 0.01})
Step:  148800, Reward: [-421.823 -421.823 -421.823] [70.411], Avg: [-407.856 -407.856 -407.856] (0.1000) ({r_i: None, r_t: [-833.104 -833.104 -833.104], eps: 0.1})
Step:  181200, Reward: [-389.326 -389.326 -389.326] [73.833], Avg: [-451.272 -451.272 -451.272] (0.0100) ({r_i: None, r_t: [-816.114 -816.114 -816.114], eps: 0.01})
Step:  148900, Reward: [-384.375 -384.375 -384.375] [67.265], Avg: [-407.840 -407.840 -407.840] (0.1000) ({r_i: None, r_t: [-843.017 -843.017 -843.017], eps: 0.1})
Step:  181300, Reward: [-391.742 -391.742 -391.742] [73.269], Avg: [-451.239 -451.239 -451.239] (0.0100) ({r_i: None, r_t: [-844.959 -844.959 -844.959], eps: 0.01})
Step:  149000, Reward: [-417.764 -417.764 -417.764] [83.200], Avg: [-407.847 -407.847 -407.847] (0.1000) ({r_i: None, r_t: [-758.695 -758.695 -758.695], eps: 0.1})
Step:  181400, Reward: [-390.048 -390.048 -390.048] [103.932], Avg: [-451.205 -451.205 -451.205] (0.0100) ({r_i: None, r_t: [-797.383 -797.383 -797.383], eps: 0.01})
Step:  149100, Reward: [-377.491 -377.491 -377.491] [59.270], Avg: [-407.826 -407.826 -407.826] (0.1000) ({r_i: None, r_t: [-808.118 -808.118 -808.118], eps: 0.1})
Step:  181500, Reward: [-385.109 -385.109 -385.109] [68.975], Avg: [-451.169 -451.169 -451.169] (0.0100) ({r_i: None, r_t: [-786.821 -786.821 -786.821], eps: 0.01})
Step:  149200, Reward: [-388.682 -388.682 -388.682] [78.126], Avg: [-407.813 -407.813 -407.813] (0.1000) ({r_i: None, r_t: [-790.717 -790.717 -790.717], eps: 0.1})
Step:  181600, Reward: [-396.673 -396.673 -396.673] [72.799], Avg: [-451.139 -451.139 -451.139] (0.0100) ({r_i: None, r_t: [-851.057 -851.057 -851.057], eps: 0.01})
Step:  149300, Reward: [-406.086 -406.086 -406.086] [84.350], Avg: [-407.812 -407.812 -407.812] (0.1000) ({r_i: None, r_t: [-825.136 -825.136 -825.136], eps: 0.1})
Step:  181700, Reward: [-411.759 -411.759 -411.759] [72.258], Avg: [-451.117 -451.117 -451.117] (0.0100) ({r_i: None, r_t: [-787.437 -787.437 -787.437], eps: 0.01})
Step:  149400, Reward: [-377.755 -377.755 -377.755] [70.877], Avg: [-407.792 -407.792 -407.792] (0.1000) ({r_i: None, r_t: [-808.322 -808.322 -808.322], eps: 0.1})
Step:  181800, Reward: [-407.188 -407.188 -407.188] [104.758], Avg: [-451.093 -451.093 -451.093] (0.0100) ({r_i: None, r_t: [-817.725 -817.725 -817.725], eps: 0.01})
Step:  149500, Reward: [-403.325 -403.325 -403.325] [81.842], Avg: [-407.789 -407.789 -407.789] (0.1000) ({r_i: None, r_t: [-795.896 -795.896 -795.896], eps: 0.1})
Step:  181900, Reward: [-389.716 -389.716 -389.716] [64.718], Avg: [-451.060 -451.060 -451.060] (0.0100) ({r_i: None, r_t: [-789.505 -789.505 -789.505], eps: 0.01})
Step:  149600, Reward: [-371.169 -371.169 -371.169] [57.513], Avg: [-407.765 -407.765 -407.765] (0.1000) ({r_i: None, r_t: [-815.587 -815.587 -815.587], eps: 0.1})
Step:  182000, Reward: [-354.212 -354.212 -354.212] [56.224], Avg: [-451.006 -451.006 -451.006] (0.0100) ({r_i: None, r_t: [-795.740 -795.740 -795.740], eps: 0.01})
Step:  149700, Reward: [-384.267 -384.267 -384.267] [68.487], Avg: [-407.749 -407.749 -407.749] (0.1000) ({r_i: None, r_t: [-746.295 -746.295 -746.295], eps: 0.1})
Step:  182100, Reward: [-389.516 -389.516 -389.516] [72.486], Avg: [-450.973 -450.973 -450.973] (0.0100) ({r_i: None, r_t: [-751.022 -751.022 -751.022], eps: 0.01})
Step:  149800, Reward: [-390.739 -390.739 -390.739] [59.673], Avg: [-407.738 -407.738 -407.738] (0.1000) ({r_i: None, r_t: [-781.311 -781.311 -781.311], eps: 0.1})
Step:  182200, Reward: [-387.890 -387.890 -387.890] [82.425], Avg: [-450.938 -450.938 -450.938] (0.0100) ({r_i: None, r_t: [-823.215 -823.215 -823.215], eps: 0.01})
Step:  149900, Reward: [-426.126 -426.126 -426.126] [101.252], Avg: [-407.750 -407.750 -407.750] (0.1000) ({r_i: None, r_t: [-845.490 -845.490 -845.490], eps: 0.1})
Step:  182300, Reward: [-379.990 -379.990 -379.990] [60.074], Avg: [-450.899 -450.899 -450.899] (0.0100) ({r_i: None, r_t: [-812.785 -812.785 -812.785], eps: 0.01})
Step:  150000, Reward: [-352.966 -352.966 -352.966] [67.944], Avg: [-407.713 -407.713 -407.713] (0.1000) ({r_i: None, r_t: [-805.918 -805.918 -805.918], eps: 0.1})
Step:  182400, Reward: [-402.986 -402.986 -402.986] [87.669], Avg: [-450.873 -450.873 -450.873] (0.0100) ({r_i: None, r_t: [-793.641 -793.641 -793.641], eps: 0.01})
Step:  150100, Reward: [-396.024 -396.024 -396.024] [77.725], Avg: [-407.706 -407.706 -407.706] (0.1000) ({r_i: None, r_t: [-782.766 -782.766 -782.766], eps: 0.1})
Step:  182500, Reward: [-394.108 -394.108 -394.108] [73.992], Avg: [-450.842 -450.842 -450.842] (0.0100) ({r_i: None, r_t: [-800.134 -800.134 -800.134], eps: 0.01})
Step:  150200, Reward: [-378.308 -378.308 -378.308] [71.351], Avg: [-407.686 -407.686 -407.686] (0.1000) ({r_i: None, r_t: [-781.226 -781.226 -781.226], eps: 0.1})
Step:  182600, Reward: [-401.980 -401.980 -401.980] [54.507], Avg: [-450.815 -450.815 -450.815] (0.0100) ({r_i: None, r_t: [-797.719 -797.719 -797.719], eps: 0.01})
Step:  150300, Reward: [-363.512 -363.512 -363.512] [48.902], Avg: [-407.657 -407.657 -407.657] (0.1000) ({r_i: None, r_t: [-812.434 -812.434 -812.434], eps: 0.1})
Step:  182700, Reward: [-384.339 -384.339 -384.339] [51.783], Avg: [-450.779 -450.779 -450.779] (0.0100) ({r_i: None, r_t: [-740.479 -740.479 -740.479], eps: 0.01})
Step:  150400, Reward: [-390.160 -390.160 -390.160] [49.089], Avg: [-407.645 -407.645 -407.645] (0.1000) ({r_i: None, r_t: [-815.706 -815.706 -815.706], eps: 0.1})
Step:  182800, Reward: [-368.600 -368.600 -368.600] [43.122], Avg: [-450.734 -450.734 -450.734] (0.0100) ({r_i: None, r_t: [-795.481 -795.481 -795.481], eps: 0.01})
Step:  150500, Reward: [-395.774 -395.774 -395.774] [47.164], Avg: [-407.637 -407.637 -407.637] (0.1000) ({r_i: None, r_t: [-777.713 -777.713 -777.713], eps: 0.1})
Step:  182900, Reward: [-412.953 -412.953 -412.953] [84.130], Avg: [-450.713 -450.713 -450.713] (0.0100) ({r_i: None, r_t: [-758.013 -758.013 -758.013], eps: 0.01})
Step:  150600, Reward: [-413.557 -413.557 -413.557] [79.364], Avg: [-407.641 -407.641 -407.641] (0.1000) ({r_i: None, r_t: [-766.207 -766.207 -766.207], eps: 0.1})
Step:  183000, Reward: [-425.131 -425.131 -425.131] [99.406], Avg: [-450.699 -450.699 -450.699] (0.0100) ({r_i: None, r_t: [-728.708 -728.708 -728.708], eps: 0.01})
Step:  150700, Reward: [-393.744 -393.744 -393.744] [67.126], Avg: [-407.632 -407.632 -407.632] (0.1000) ({r_i: None, r_t: [-810.123 -810.123 -810.123], eps: 0.1})
Step:  183100, Reward: [-369.172 -369.172 -369.172] [59.489], Avg: [-450.655 -450.655 -450.655] (0.0100) ({r_i: None, r_t: [-802.992 -802.992 -802.992], eps: 0.01})
Step:  150800, Reward: [-394.726 -394.726 -394.726] [62.933], Avg: [-407.623 -407.623 -407.623] (0.1000) ({r_i: None, r_t: [-770.263 -770.263 -770.263], eps: 0.1})
Step:  183200, Reward: [-403.306 -403.306 -403.306] [78.205], Avg: [-450.629 -450.629 -450.629] (0.0100) ({r_i: None, r_t: [-759.600 -759.600 -759.600], eps: 0.01})
Step:  150900, Reward: [-411.975 -411.975 -411.975] [57.538], Avg: [-407.626 -407.626 -407.626] (0.1000) ({r_i: None, r_t: [-754.790 -754.790 -754.790], eps: 0.1})
Step:  183300, Reward: [-383.081 -383.081 -383.081] [67.643], Avg: [-450.592 -450.592 -450.592] (0.0100) ({r_i: None, r_t: [-808.413 -808.413 -808.413], eps: 0.01})
Step:  151000, Reward: [-384.056 -384.056 -384.056] [54.581], Avg: [-407.611 -407.611 -407.611] (0.1000) ({r_i: None, r_t: [-778.933 -778.933 -778.933], eps: 0.1})
Step:  183400, Reward: [-379.634 -379.634 -379.634] [50.882], Avg: [-450.553 -450.553 -450.553] (0.0100) ({r_i: None, r_t: [-788.626 -788.626 -788.626], eps: 0.01})
Step:  151100, Reward: [-379.703 -379.703 -379.703] [45.716], Avg: [-407.592 -407.592 -407.592] (0.1000) ({r_i: None, r_t: [-790.887 -790.887 -790.887], eps: 0.1})
Step:  183500, Reward: [-420.199 -420.199 -420.199] [98.076], Avg: [-450.537 -450.537 -450.537] (0.0100) ({r_i: None, r_t: [-767.039 -767.039 -767.039], eps: 0.01})
Step:  151200, Reward: [-370.041 -370.041 -370.041] [78.508], Avg: [-407.567 -407.567 -407.567] (0.1000) ({r_i: None, r_t: [-762.105 -762.105 -762.105], eps: 0.1})
Step:  183600, Reward: [-401.685 -401.685 -401.685] [60.107], Avg: [-450.510 -450.510 -450.510] (0.0100) ({r_i: None, r_t: [-753.110 -753.110 -753.110], eps: 0.01})
Step:  151300, Reward: [-428.891 -428.891 -428.891] [81.663], Avg: [-407.582 -407.582 -407.582] (0.1000) ({r_i: None, r_t: [-811.361 -811.361 -811.361], eps: 0.1})
Step:  183700, Reward: [-379.032 -379.032 -379.032] [57.301], Avg: [-450.471 -450.471 -450.471] (0.0100) ({r_i: None, r_t: [-820.942 -820.942 -820.942], eps: 0.01})
Step:  151400, Reward: [-406.141 -406.141 -406.141] [74.532], Avg: [-407.581 -407.581 -407.581] (0.1000) ({r_i: None, r_t: [-769.677 -769.677 -769.677], eps: 0.1})
Step:  183800, Reward: [-364.970 -364.970 -364.970] [58.842], Avg: [-450.425 -450.425 -450.425] (0.0100) ({r_i: None, r_t: [-804.826 -804.826 -804.826], eps: 0.01})
Step:  151500, Reward: [-393.167 -393.167 -393.167] [71.603], Avg: [-407.571 -407.571 -407.571] (0.1000) ({r_i: None, r_t: [-749.469 -749.469 -749.469], eps: 0.1})
Step:  183900, Reward: [-414.375 -414.375 -414.375] [74.621], Avg: [-450.405 -450.405 -450.405] (0.0100) ({r_i: None, r_t: [-809.248 -809.248 -809.248], eps: 0.01})
Step:  151600, Reward: [-415.499 -415.499 -415.499] [68.316], Avg: [-407.576 -407.576 -407.576] (0.1000) ({r_i: None, r_t: [-784.269 -784.269 -784.269], eps: 0.1})
Step:  184000, Reward: [-410.122 -410.122 -410.122] [67.056], Avg: [-450.383 -450.383 -450.383] (0.0100) ({r_i: None, r_t: [-792.208 -792.208 -792.208], eps: 0.01})
Step:  151700, Reward: [-381.500 -381.500 -381.500] [80.673], Avg: [-407.559 -407.559 -407.559] (0.1000) ({r_i: None, r_t: [-789.639 -789.639 -789.639], eps: 0.1})
Step:  184100, Reward: [-358.942 -358.942 -358.942] [75.465], Avg: [-450.334 -450.334 -450.334] (0.0100) ({r_i: None, r_t: [-776.481 -776.481 -776.481], eps: 0.01})
Step:  151800, Reward: [-390.188 -390.188 -390.188] [56.114], Avg: [-407.548 -407.548 -407.548] (0.1000) ({r_i: None, r_t: [-762.051 -762.051 -762.051], eps: 0.1})
Step:  184200, Reward: [-371.224 -371.224 -371.224] [49.415], Avg: [-450.291 -450.291 -450.291] (0.0100) ({r_i: None, r_t: [-796.138 -796.138 -796.138], eps: 0.01})
Step:  151900, Reward: [-403.751 -403.751 -403.751] [82.356], Avg: [-407.545 -407.545 -407.545] (0.1000) ({r_i: None, r_t: [-785.865 -785.865 -785.865], eps: 0.1})
Step:  184300, Reward: [-419.344 -419.344 -419.344] [85.412], Avg: [-450.274 -450.274 -450.274] (0.0100) ({r_i: None, r_t: [-767.978 -767.978 -767.978], eps: 0.01})
Step:  152000, Reward: [-406.304 -406.304 -406.304] [71.564], Avg: [-407.544 -407.544 -407.544] (0.1000) ({r_i: None, r_t: [-796.562 -796.562 -796.562], eps: 0.1})
Step:  184400, Reward: [-379.902 -379.902 -379.902] [58.945], Avg: [-450.236 -450.236 -450.236] (0.0100) ({r_i: None, r_t: [-772.943 -772.943 -772.943], eps: 0.01})
Step:  152100, Reward: [-404.375 -404.375 -404.375] [63.528], Avg: [-407.542 -407.542 -407.542] (0.1000) ({r_i: None, r_t: [-764.176 -764.176 -764.176], eps: 0.1})
Step:  184500, Reward: [-398.967 -398.967 -398.967] [66.992], Avg: [-450.208 -450.208 -450.208] (0.0100) ({r_i: None, r_t: [-743.320 -743.320 -743.320], eps: 0.01})
Step:  152200, Reward: [-434.445 -434.445 -434.445] [68.653], Avg: [-407.560 -407.560 -407.560] (0.1000) ({r_i: None, r_t: [-779.302 -779.302 -779.302], eps: 0.1})
Step:  184600, Reward: [-400.089 -400.089 -400.089] [66.370], Avg: [-450.181 -450.181 -450.181] (0.0100) ({r_i: None, r_t: [-760.634 -760.634 -760.634], eps: 0.01})
Step:  152300, Reward: [-383.674 -383.674 -383.674] [94.255], Avg: [-407.544 -407.544 -407.544] (0.1000) ({r_i: None, r_t: [-734.635 -734.635 -734.635], eps: 0.1})
Step:  184700, Reward: [-386.631 -386.631 -386.631] [85.444], Avg: [-450.147 -450.147 -450.147] (0.0100) ({r_i: None, r_t: [-745.530 -745.530 -745.530], eps: 0.01})
Step:  152400, Reward: [-406.276 -406.276 -406.276] [89.734], Avg: [-407.543 -407.543 -407.543] (0.1000) ({r_i: None, r_t: [-770.454 -770.454 -770.454], eps: 0.1})
Step:  184800, Reward: [-363.333 -363.333 -363.333] [67.801], Avg: [-450.100 -450.100 -450.100] (0.0100) ({r_i: None, r_t: [-795.439 -795.439 -795.439], eps: 0.01})
Step:  152500, Reward: [-403.698 -403.698 -403.698] [98.157], Avg: [-407.541 -407.541 -407.541] (0.1000) ({r_i: None, r_t: [-799.919 -799.919 -799.919], eps: 0.1})
Step:  184900, Reward: [-365.953 -365.953 -365.953] [70.823], Avg: [-450.054 -450.054 -450.054] (0.0100) ({r_i: None, r_t: [-800.390 -800.390 -800.390], eps: 0.01})
Step:  152600, Reward: [-392.844 -392.844 -392.844] [70.198], Avg: [-407.531 -407.531 -407.531] (0.1000) ({r_i: None, r_t: [-754.977 -754.977 -754.977], eps: 0.1})
Step:  185000, Reward: [-357.092 -357.092 -357.092] [53.987], Avg: [-450.004 -450.004 -450.004] (0.0100) ({r_i: None, r_t: [-814.642 -814.642 -814.642], eps: 0.01})
Step:  152700, Reward: [-374.760 -374.760 -374.760] [60.965], Avg: [-407.510 -407.510 -407.510] (0.1000) ({r_i: None, r_t: [-813.655 -813.655 -813.655], eps: 0.1})
Step:  185100, Reward: [-395.446 -395.446 -395.446] [108.251], Avg: [-449.974 -449.974 -449.974] (0.0100) ({r_i: None, r_t: [-756.373 -756.373 -756.373], eps: 0.01})
Step:  152800, Reward: [-395.829 -395.829 -395.829] [69.903], Avg: [-407.502 -407.502 -407.502] (0.1000) ({r_i: None, r_t: [-802.024 -802.024 -802.024], eps: 0.1})
Step:  185200, Reward: [-375.849 -375.849 -375.849] [70.659], Avg: [-449.934 -449.934 -449.934] (0.0100) ({r_i: None, r_t: [-803.338 -803.338 -803.338], eps: 0.01})
Step:  152900, Reward: [-387.687 -387.687 -387.687] [74.172], Avg: [-407.489 -407.489 -407.489] (0.1000) ({r_i: None, r_t: [-766.325 -766.325 -766.325], eps: 0.1})
Step:  185300, Reward: [-430.759 -430.759 -430.759] [100.704], Avg: [-449.924 -449.924 -449.924] (0.0100) ({r_i: None, r_t: [-781.926 -781.926 -781.926], eps: 0.01})
Step:  153000, Reward: [-420.298 -420.298 -420.298] [86.166], Avg: [-407.498 -407.498 -407.498] (0.1000) ({r_i: None, r_t: [-797.758 -797.758 -797.758], eps: 0.1})
Step:  185400, Reward: [-424.818 -424.818 -424.818] [65.324], Avg: [-449.911 -449.911 -449.911] (0.0100) ({r_i: None, r_t: [-810.339 -810.339 -810.339], eps: 0.01})
Step:  153100, Reward: [-393.674 -393.674 -393.674] [92.147], Avg: [-407.489 -407.489 -407.489] (0.1000) ({r_i: None, r_t: [-837.682 -837.682 -837.682], eps: 0.1})
Step:  185500, Reward: [-404.403 -404.403 -404.403] [90.491], Avg: [-449.886 -449.886 -449.886] (0.0100) ({r_i: None, r_t: [-771.610 -771.610 -771.610], eps: 0.01})
Step:  153200, Reward: [-382.965 -382.965 -382.965] [82.896], Avg: [-407.473 -407.473 -407.473] (0.1000) ({r_i: None, r_t: [-768.053 -768.053 -768.053], eps: 0.1})
Step:  185600, Reward: [-401.811 -401.811 -401.811] [67.818], Avg: [-449.860 -449.860 -449.860] (0.0100) ({r_i: None, r_t: [-814.447 -814.447 -814.447], eps: 0.01})
Step:  153300, Reward: [-406.592 -406.592 -406.592] [89.997], Avg: [-407.472 -407.472 -407.472] (0.1000) ({r_i: None, r_t: [-790.716 -790.716 -790.716], eps: 0.1})
Step:  185700, Reward: [-418.431 -418.431 -418.431] [82.379], Avg: [-449.843 -449.843 -449.843] (0.0100) ({r_i: None, r_t: [-828.430 -828.430 -828.430], eps: 0.01})
Step:  153400, Reward: [-394.111 -394.111 -394.111] [77.517], Avg: [-407.463 -407.463 -407.463] (0.1000) ({r_i: None, r_t: [-854.459 -854.459 -854.459], eps: 0.1})
Step:  185800, Reward: [-418.526 -418.526 -418.526] [88.235], Avg: [-449.826 -449.826 -449.826] (0.0100) ({r_i: None, r_t: [-778.605 -778.605 -778.605], eps: 0.01})
Step:  153500, Reward: [-399.710 -399.710 -399.710] [63.839], Avg: [-407.458 -407.458 -407.458] (0.1000) ({r_i: None, r_t: [-833.357 -833.357 -833.357], eps: 0.1})
Step:  185900, Reward: [-372.073 -372.073 -372.073] [77.973], Avg: [-449.785 -449.785 -449.785] (0.0100) ({r_i: None, r_t: [-781.523 -781.523 -781.523], eps: 0.01})
Step:  153600, Reward: [-391.132 -391.132 -391.132] [76.570], Avg: [-407.448 -407.448 -407.448] (0.1000) ({r_i: None, r_t: [-802.287 -802.287 -802.287], eps: 0.1})
Step:  186000, Reward: [-401.376 -401.376 -401.376] [77.555], Avg: [-449.759 -449.759 -449.759] (0.0100) ({r_i: None, r_t: [-773.612 -773.612 -773.612], eps: 0.01})
Step:  153700, Reward: [-398.756 -398.756 -398.756] [74.801], Avg: [-407.442 -407.442 -407.442] (0.1000) ({r_i: None, r_t: [-823.943 -823.943 -823.943], eps: 0.1})
Step:  186100, Reward: [-378.781 -378.781 -378.781] [49.259], Avg: [-449.720 -449.720 -449.720] (0.0100) ({r_i: None, r_t: [-747.896 -747.896 -747.896], eps: 0.01})
Step:  153800, Reward: [-395.281 -395.281 -395.281] [67.886], Avg: [-407.434 -407.434 -407.434] (0.1000) ({r_i: None, r_t: [-791.253 -791.253 -791.253], eps: 0.1})
Step:  186200, Reward: [-390.317 -390.317 -390.317] [52.696], Avg: [-449.689 -449.689 -449.689] (0.0100) ({r_i: None, r_t: [-775.803 -775.803 -775.803], eps: 0.01})
Step:  153900, Reward: [-408.337 -408.337 -408.337] [73.925], Avg: [-407.435 -407.435 -407.435] (0.1000) ({r_i: None, r_t: [-808.624 -808.624 -808.624], eps: 0.1})
Step:  186300, Reward: [-400.614 -400.614 -400.614] [88.594], Avg: [-449.662 -449.662 -449.662] (0.0100) ({r_i: None, r_t: [-795.268 -795.268 -795.268], eps: 0.01})
Step:  154000, Reward: [-349.743 -349.743 -349.743] [62.057], Avg: [-407.397 -407.397 -407.397] (0.1000) ({r_i: None, r_t: [-808.532 -808.532 -808.532], eps: 0.1})
Step:  186400, Reward: [-380.677 -380.677 -380.677] [66.318], Avg: [-449.625 -449.625 -449.625] (0.0100) ({r_i: None, r_t: [-800.471 -800.471 -800.471], eps: 0.01})
Step:  154100, Reward: [-372.206 -372.206 -372.206] [80.145], Avg: [-407.374 -407.374 -407.374] (0.1000) ({r_i: None, r_t: [-803.815 -803.815 -803.815], eps: 0.1})
Step:  186500, Reward: [-387.025 -387.025 -387.025] [55.011], Avg: [-449.592 -449.592 -449.592] (0.0100) ({r_i: None, r_t: [-764.695 -764.695 -764.695], eps: 0.01})
Step:  154200, Reward: [-450.714 -450.714 -450.714] [74.694], Avg: [-407.403 -407.403 -407.403] (0.1000) ({r_i: None, r_t: [-815.086 -815.086 -815.086], eps: 0.1})
Step:  186600, Reward: [-402.603 -402.603 -402.603] [65.290], Avg: [-449.567 -449.567 -449.567] (0.0100) ({r_i: None, r_t: [-792.188 -792.188 -792.188], eps: 0.01})
Step:  154300, Reward: [-394.677 -394.677 -394.677] [79.577], Avg: [-407.394 -407.394 -407.394] (0.1000) ({r_i: None, r_t: [-803.400 -803.400 -803.400], eps: 0.1})
Step:  186700, Reward: [-391.390 -391.390 -391.390] [92.902], Avg: [-449.535 -449.535 -449.535] (0.0100) ({r_i: None, r_t: [-803.317 -803.317 -803.317], eps: 0.01})
Step:  154400, Reward: [-413.221 -413.221 -413.221] [76.701], Avg: [-407.398 -407.398 -407.398] (0.1000) ({r_i: None, r_t: [-778.446 -778.446 -778.446], eps: 0.1})
Step:  186800, Reward: [-375.742 -375.742 -375.742] [53.633], Avg: [-449.496 -449.496 -449.496] (0.0100) ({r_i: None, r_t: [-820.522 -820.522 -820.522], eps: 0.01})
Step:  154500, Reward: [-388.266 -388.266 -388.266] [75.917], Avg: [-407.386 -407.386 -407.386] (0.1000) ({r_i: None, r_t: [-796.483 -796.483 -796.483], eps: 0.1})
Step:  186900, Reward: [-380.569 -380.569 -380.569] [76.627], Avg: [-449.459 -449.459 -449.459] (0.0100) ({r_i: None, r_t: [-807.297 -807.297 -807.297], eps: 0.01})
Step:  154600, Reward: [-428.451 -428.451 -428.451] [55.539], Avg: [-407.399 -407.399 -407.399] (0.1000) ({r_i: None, r_t: [-790.390 -790.390 -790.390], eps: 0.1})
Step:  187000, Reward: [-420.566 -420.566 -420.566] [76.663], Avg: [-449.444 -449.444 -449.444] (0.0100) ({r_i: None, r_t: [-770.339 -770.339 -770.339], eps: 0.01})
Step:  154700, Reward: [-362.845 -362.845 -362.845] [37.852], Avg: [-407.371 -407.371 -407.371] (0.1000) ({r_i: None, r_t: [-801.509 -801.509 -801.509], eps: 0.1})
Step:  187100, Reward: [-392.789 -392.789 -392.789] [77.274], Avg: [-449.413 -449.413 -449.413] (0.0100) ({r_i: None, r_t: [-784.508 -784.508 -784.508], eps: 0.01})
Step:  154800, Reward: [-399.606 -399.606 -399.606] [54.097], Avg: [-407.365 -407.365 -407.365] (0.1000) ({r_i: None, r_t: [-821.075 -821.075 -821.075], eps: 0.1})
Step:  187200, Reward: [-405.374 -405.374 -405.374] [69.641], Avg: [-449.390 -449.390 -449.390] (0.0100) ({r_i: None, r_t: [-791.299 -791.299 -791.299], eps: 0.01})
Step:  154900, Reward: [-406.080 -406.080 -406.080] [80.996], Avg: [-407.365 -407.365 -407.365] (0.1000) ({r_i: None, r_t: [-811.638 -811.638 -811.638], eps: 0.1})
Step:  187300, Reward: [-389.192 -389.192 -389.192] [87.633], Avg: [-449.358 -449.358 -449.358] (0.0100) ({r_i: None, r_t: [-791.981 -791.981 -791.981], eps: 0.01})
Step:  155000, Reward: [-401.876 -401.876 -401.876] [67.781], Avg: [-407.361 -407.361 -407.361] (0.1000) ({r_i: None, r_t: [-761.538 -761.538 -761.538], eps: 0.1})
Step:  187400, Reward: [-395.791 -395.791 -395.791] [63.113], Avg: [-449.329 -449.329 -449.329] (0.0100) ({r_i: None, r_t: [-813.439 -813.439 -813.439], eps: 0.01})
Step:  155100, Reward: [-388.944 -388.944 -388.944] [66.385], Avg: [-407.349 -407.349 -407.349] (0.1000) ({r_i: None, r_t: [-783.410 -783.410 -783.410], eps: 0.1})
Step:  187500, Reward: [-378.152 -378.152 -378.152] [84.028], Avg: [-449.291 -449.291 -449.291] (0.0100) ({r_i: None, r_t: [-807.918 -807.918 -807.918], eps: 0.01})
Step:  155200, Reward: [-385.872 -385.872 -385.872] [50.812], Avg: [-407.335 -407.335 -407.335] (0.1000) ({r_i: None, r_t: [-763.634 -763.634 -763.634], eps: 0.1})
Step:  187600, Reward: [-393.968 -393.968 -393.968] [96.955], Avg: [-449.262 -449.262 -449.262] (0.0100) ({r_i: None, r_t: [-795.089 -795.089 -795.089], eps: 0.01})
Step:  155300, Reward: [-411.362 -411.362 -411.362] [76.196], Avg: [-407.338 -407.338 -407.338] (0.1000) ({r_i: None, r_t: [-815.047 -815.047 -815.047], eps: 0.1})
Step:  187700, Reward: [-411.134 -411.134 -411.134] [63.701], Avg: [-449.241 -449.241 -449.241] (0.0100) ({r_i: None, r_t: [-752.291 -752.291 -752.291], eps: 0.01})
Step:  155400, Reward: [-400.857 -400.857 -400.857] [65.277], Avg: [-407.334 -407.334 -407.334] (0.1000) ({r_i: None, r_t: [-780.785 -780.785 -780.785], eps: 0.1})
Step:  187800, Reward: [-379.458 -379.458 -379.458] [77.484], Avg: [-449.204 -449.204 -449.204] (0.0100) ({r_i: None, r_t: [-767.032 -767.032 -767.032], eps: 0.01})
Step:  155500, Reward: [-403.794 -403.794 -403.794] [93.660], Avg: [-407.332 -407.332 -407.332] (0.1000) ({r_i: None, r_t: [-798.109 -798.109 -798.109], eps: 0.1})
Step:  187900, Reward: [-358.283 -358.283 -358.283] [78.035], Avg: [-449.156 -449.156 -449.156] (0.0100) ({r_i: None, r_t: [-807.790 -807.790 -807.790], eps: 0.01})
Step:  155600, Reward: [-402.637 -402.637 -402.637] [57.916], Avg: [-407.329 -407.329 -407.329] (0.1000) ({r_i: None, r_t: [-794.266 -794.266 -794.266], eps: 0.1})
Step:  188000, Reward: [-368.885 -368.885 -368.885] [57.259], Avg: [-449.113 -449.113 -449.113] (0.0100) ({r_i: None, r_t: [-789.886 -789.886 -789.886], eps: 0.01})
Step:  155700, Reward: [-388.866 -388.866 -388.866] [63.403], Avg: [-407.317 -407.317 -407.317] (0.1000) ({r_i: None, r_t: [-807.520 -807.520 -807.520], eps: 0.1})
Step:  188100, Reward: [-397.377 -397.377 -397.377] [113.119], Avg: [-449.086 -449.086 -449.086] (0.0100) ({r_i: None, r_t: [-761.189 -761.189 -761.189], eps: 0.01})
Step:  155800, Reward: [-373.243 -373.243 -373.243] [64.551], Avg: [-407.295 -407.295 -407.295] (0.1000) ({r_i: None, r_t: [-784.505 -784.505 -784.505], eps: 0.1})
Step:  188200, Reward: [-394.470 -394.470 -394.470] [59.939], Avg: [-449.057 -449.057 -449.057] (0.0100) ({r_i: None, r_t: [-769.972 -769.972 -769.972], eps: 0.01})
Step:  155900, Reward: [-380.986 -380.986 -380.986] [56.354], Avg: [-407.278 -407.278 -407.278] (0.1000) ({r_i: None, r_t: [-806.652 -806.652 -806.652], eps: 0.1})
Step:  188300, Reward: [-392.585 -392.585 -392.585] [71.197], Avg: [-449.027 -449.027 -449.027] (0.0100) ({r_i: None, r_t: [-772.009 -772.009 -772.009], eps: 0.01})
Step:  156000, Reward: [-392.735 -392.735 -392.735] [65.803], Avg: [-407.269 -407.269 -407.269] (0.1000) ({r_i: None, r_t: [-777.932 -777.932 -777.932], eps: 0.1})
Step:  188400, Reward: [-395.207 -395.207 -395.207] [55.897], Avg: [-448.998 -448.998 -448.998] (0.0100) ({r_i: None, r_t: [-755.352 -755.352 -755.352], eps: 0.01})
Step:  156100, Reward: [-359.513 -359.513 -359.513] [38.027], Avg: [-407.238 -407.238 -407.238] (0.1000) ({r_i: None, r_t: [-728.865 -728.865 -728.865], eps: 0.1})
Step:  188500, Reward: [-367.811 -367.811 -367.811] [76.309], Avg: [-448.955 -448.955 -448.955] (0.0100) ({r_i: None, r_t: [-790.964 -790.964 -790.964], eps: 0.01})
Step:  156200, Reward: [-350.427 -350.427 -350.427] [42.996], Avg: [-407.202 -407.202 -407.202] (0.1000) ({r_i: None, r_t: [-816.237 -816.237 -816.237], eps: 0.1})
Step:  188600, Reward: [-402.680 -402.680 -402.680] [82.149], Avg: [-448.931 -448.931 -448.931] (0.0100) ({r_i: None, r_t: [-763.705 -763.705 -763.705], eps: 0.01})
Step:  156300, Reward: [-418.924 -418.924 -418.924] [61.713], Avg: [-407.209 -407.209 -407.209] (0.1000) ({r_i: None, r_t: [-770.912 -770.912 -770.912], eps: 0.1})
Step:  188700, Reward: [-408.837 -408.837 -408.837] [83.297], Avg: [-448.909 -448.909 -448.909] (0.0100) ({r_i: None, r_t: [-805.986 -805.986 -805.986], eps: 0.01})
Step:  156400, Reward: [-433.299 -433.299 -433.299] [93.882], Avg: [-407.226 -407.226 -407.226] (0.1000) ({r_i: None, r_t: [-787.523 -787.523 -787.523], eps: 0.1})
Step:  188800, Reward: [-409.913 -409.913 -409.913] [70.341], Avg: [-448.889 -448.889 -448.889] (0.0100) ({r_i: None, r_t: [-788.261 -788.261 -788.261], eps: 0.01})
Step:  156500, Reward: [-409.877 -409.877 -409.877] [75.586], Avg: [-407.228 -407.228 -407.228] (0.1000) ({r_i: None, r_t: [-797.194 -797.194 -797.194], eps: 0.1})
Step:  188900, Reward: [-372.876 -372.876 -372.876] [46.179], Avg: [-448.849 -448.849 -448.849] (0.0100) ({r_i: None, r_t: [-759.410 -759.410 -759.410], eps: 0.01})
Step:  156600, Reward: [-417.570 -417.570 -417.570] [60.542], Avg: [-407.234 -407.234 -407.234] (0.1000) ({r_i: None, r_t: [-788.909 -788.909 -788.909], eps: 0.1})
Step:  189000, Reward: [-426.036 -426.036 -426.036] [67.039], Avg: [-448.836 -448.836 -448.836] (0.0100) ({r_i: None, r_t: [-762.494 -762.494 -762.494], eps: 0.01})
Step:  156700, Reward: [-366.743 -366.743 -366.743] [65.292], Avg: [-407.208 -407.208 -407.208] (0.1000) ({r_i: None, r_t: [-743.682 -743.682 -743.682], eps: 0.1})
Step:  189100, Reward: [-395.907 -395.907 -395.907] [60.982], Avg: [-448.808 -448.808 -448.808] (0.0100) ({r_i: None, r_t: [-805.291 -805.291 -805.291], eps: 0.01})
Step:  156800, Reward: [-409.205 -409.205 -409.205] [58.331], Avg: [-407.210 -407.210 -407.210] (0.1000) ({r_i: None, r_t: [-787.045 -787.045 -787.045], eps: 0.1})
Step:  189200, Reward: [-394.720 -394.720 -394.720] [63.912], Avg: [-448.780 -448.780 -448.780] (0.0100) ({r_i: None, r_t: [-793.588 -793.588 -793.588], eps: 0.01})
Step:  156900, Reward: [-389.697 -389.697 -389.697] [85.163], Avg: [-407.199 -407.199 -407.199] (0.1000) ({r_i: None, r_t: [-831.501 -831.501 -831.501], eps: 0.1})
Step:  189300, Reward: [-409.806 -409.806 -409.806] [73.861], Avg: [-448.759 -448.759 -448.759] (0.0100) ({r_i: None, r_t: [-761.162 -761.162 -761.162], eps: 0.01})
Step:  157000, Reward: [-371.118 -371.118 -371.118] [80.963], Avg: [-407.176 -407.176 -407.176] (0.1000) ({r_i: None, r_t: [-859.607 -859.607 -859.607], eps: 0.1})
Step:  189400, Reward: [-384.557 -384.557 -384.557] [43.990], Avg: [-448.725 -448.725 -448.725] (0.0100) ({r_i: None, r_t: [-802.721 -802.721 -802.721], eps: 0.01})
Step:  157100, Reward: [-360.684 -360.684 -360.684] [49.929], Avg: [-407.146 -407.146 -407.146] (0.1000) ({r_i: None, r_t: [-783.238 -783.238 -783.238], eps: 0.1})
Step:  189500, Reward: [-411.657 -411.657 -411.657] [87.846], Avg: [-448.706 -448.706 -448.706] (0.0100) ({r_i: None, r_t: [-782.466 -782.466 -782.466], eps: 0.01})
Step:  157200, Reward: [-363.580 -363.580 -363.580] [75.056], Avg: [-407.118 -407.118 -407.118] (0.1000) ({r_i: None, r_t: [-793.171 -793.171 -793.171], eps: 0.1})
Step:  189600, Reward: [-377.925 -377.925 -377.925] [50.607], Avg: [-448.669 -448.669 -448.669] (0.0100) ({r_i: None, r_t: [-831.925 -831.925 -831.925], eps: 0.01})
Step:  157300, Reward: [-409.088 -409.088 -409.088] [69.284], Avg: [-407.120 -407.120 -407.120] (0.1000) ({r_i: None, r_t: [-778.416 -778.416 -778.416], eps: 0.1})
Step:  189700, Reward: [-406.963 -406.963 -406.963] [91.053], Avg: [-448.647 -448.647 -448.647] (0.0100) ({r_i: None, r_t: [-809.549 -809.549 -809.549], eps: 0.01})
Step:  157400, Reward: [-406.906 -406.906 -406.906] [53.413], Avg: [-407.119 -407.119 -407.119] (0.1000) ({r_i: None, r_t: [-810.995 -810.995 -810.995], eps: 0.1})
Step:  189800, Reward: [-382.545 -382.545 -382.545] [74.099], Avg: [-448.612 -448.612 -448.612] (0.0100) ({r_i: None, r_t: [-749.731 -749.731 -749.731], eps: 0.01})
Step:  157500, Reward: [-382.631 -382.631 -382.631] [77.906], Avg: [-407.104 -407.104 -407.104] (0.1000) ({r_i: None, r_t: [-806.382 -806.382 -806.382], eps: 0.1})
Step:  189900, Reward: [-408.053 -408.053 -408.053] [91.656], Avg: [-448.590 -448.590 -448.590] (0.0100) ({r_i: None, r_t: [-761.442 -761.442 -761.442], eps: 0.01})
Step:  157600, Reward: [-399.678 -399.678 -399.678] [70.581], Avg: [-407.099 -407.099 -407.099] (0.1000) ({r_i: None, r_t: [-808.241 -808.241 -808.241], eps: 0.1})
Step:  190000, Reward: [-407.662 -407.662 -407.662] [63.941], Avg: [-448.569 -448.569 -448.569] (0.0100) ({r_i: None, r_t: [-777.020 -777.020 -777.020], eps: 0.01})
Step:  157700, Reward: [-407.237 -407.237 -407.237] [57.584], Avg: [-407.099 -407.099 -407.099] (0.1000) ({r_i: None, r_t: [-832.950 -832.950 -832.950], eps: 0.1})
Step:  190100, Reward: [-353.406 -353.406 -353.406] [41.073], Avg: [-448.519 -448.519 -448.519] (0.0100) ({r_i: None, r_t: [-767.050 -767.050 -767.050], eps: 0.01})
Step:  157800, Reward: [-365.025 -365.025 -365.025] [67.776], Avg: [-407.073 -407.073 -407.073] (0.1000) ({r_i: None, r_t: [-802.185 -802.185 -802.185], eps: 0.1})
Step:  190200, Reward: [-399.421 -399.421 -399.421] [86.862], Avg: [-448.493 -448.493 -448.493] (0.0100) ({r_i: None, r_t: [-751.881 -751.881 -751.881], eps: 0.01})
Step:  157900, Reward: [-423.551 -423.551 -423.551] [55.035], Avg: [-407.083 -407.083 -407.083] (0.1000) ({r_i: None, r_t: [-786.889 -786.889 -786.889], eps: 0.1})
Step:  190300, Reward: [-400.922 -400.922 -400.922] [50.007], Avg: [-448.468 -448.468 -448.468] (0.0100) ({r_i: None, r_t: [-786.798 -786.798 -786.798], eps: 0.01})
Step:  158000, Reward: [-378.990 -378.990 -378.990] [63.730], Avg: [-407.065 -407.065 -407.065] (0.1000) ({r_i: None, r_t: [-796.082 -796.082 -796.082], eps: 0.1})
Step:  190400, Reward: [-397.062 -397.062 -397.062] [50.808], Avg: [-448.441 -448.441 -448.441] (0.0100) ({r_i: None, r_t: [-863.554 -863.554 -863.554], eps: 0.01})
Step:  158100, Reward: [-388.999 -388.999 -388.999] [91.376], Avg: [-407.054 -407.054 -407.054] (0.1000) ({r_i: None, r_t: [-814.694 -814.694 -814.694], eps: 0.1})
Step:  190500, Reward: [-401.219 -401.219 -401.219] [87.929], Avg: [-448.416 -448.416 -448.416] (0.0100) ({r_i: None, r_t: [-801.366 -801.366 -801.366], eps: 0.01})
Step:  158200, Reward: [-403.230 -403.230 -403.230] [51.614], Avg: [-407.051 -407.051 -407.051] (0.1000) ({r_i: None, r_t: [-792.736 -792.736 -792.736], eps: 0.1})
Step:  190600, Reward: [-424.407 -424.407 -424.407] [94.580], Avg: [-448.404 -448.404 -448.404] (0.0100) ({r_i: None, r_t: [-824.466 -824.466 -824.466], eps: 0.01})
Step:  158300, Reward: [-389.610 -389.610 -389.610] [77.305], Avg: [-407.040 -407.040 -407.040] (0.1000) ({r_i: None, r_t: [-782.035 -782.035 -782.035], eps: 0.1})
Step:  190700, Reward: [-398.917 -398.917 -398.917] [66.491], Avg: [-448.378 -448.378 -448.378] (0.0100) ({r_i: None, r_t: [-790.254 -790.254 -790.254], eps: 0.01})
Step:  158400, Reward: [-363.644 -363.644 -363.644] [71.460], Avg: [-407.013 -407.013 -407.013] (0.1000) ({r_i: None, r_t: [-806.906 -806.906 -806.906], eps: 0.1})
Step:  190800, Reward: [-379.795 -379.795 -379.795] [71.502], Avg: [-448.342 -448.342 -448.342] (0.0100) ({r_i: None, r_t: [-820.374 -820.374 -820.374], eps: 0.01})
Step:  158500, Reward: [-414.490 -414.490 -414.490] [70.889], Avg: [-407.018 -407.018 -407.018] (0.1000) ({r_i: None, r_t: [-805.211 -805.211 -805.211], eps: 0.1})
Step:  190900, Reward: [-425.135 -425.135 -425.135] [77.498], Avg: [-448.330 -448.330 -448.330] (0.0100) ({r_i: None, r_t: [-776.491 -776.491 -776.491], eps: 0.01})
Step:  158600, Reward: [-393.679 -393.679 -393.679] [78.427], Avg: [-407.009 -407.009 -407.009] (0.1000) ({r_i: None, r_t: [-783.976 -783.976 -783.976], eps: 0.1})
Step:  191000, Reward: [-388.972 -388.972 -388.972] [59.552], Avg: [-448.299 -448.299 -448.299] (0.0100) ({r_i: None, r_t: [-803.582 -803.582 -803.582], eps: 0.01})
Step:  158700, Reward: [-384.756 -384.756 -384.756] [66.412], Avg: [-406.995 -406.995 -406.995] (0.1000) ({r_i: None, r_t: [-779.647 -779.647 -779.647], eps: 0.1})
Step:  191100, Reward: [-428.503 -428.503 -428.503] [95.873], Avg: [-448.288 -448.288 -448.288] (0.0100) ({r_i: None, r_t: [-840.898 -840.898 -840.898], eps: 0.01})
Step:  158800, Reward: [-416.430 -416.430 -416.430] [88.201], Avg: [-407.001 -407.001 -407.001] (0.1000) ({r_i: None, r_t: [-791.316 -791.316 -791.316], eps: 0.1})
Step:  191200, Reward: [-430.574 -430.574 -430.574] [77.414], Avg: [-448.279 -448.279 -448.279] (0.0100) ({r_i: None, r_t: [-810.582 -810.582 -810.582], eps: 0.01})
Step:  158900, Reward: [-403.385 -403.385 -403.385] [81.850], Avg: [-406.999 -406.999 -406.999] (0.1000) ({r_i: None, r_t: [-809.455 -809.455 -809.455], eps: 0.1})
Step:  191300, Reward: [-374.714 -374.714 -374.714] [59.626], Avg: [-448.241 -448.241 -448.241] (0.0100) ({r_i: None, r_t: [-811.785 -811.785 -811.785], eps: 0.01})
Step:  159000, Reward: [-378.024 -378.024 -378.024] [50.452], Avg: [-406.981 -406.981 -406.981] (0.1000) ({r_i: None, r_t: [-844.023 -844.023 -844.023], eps: 0.1})
Step:  191400, Reward: [-391.803 -391.803 -391.803] [83.038], Avg: [-448.211 -448.211 -448.211] (0.0100) ({r_i: None, r_t: [-798.093 -798.093 -798.093], eps: 0.01})
Step:  159100, Reward: [-423.161 -423.161 -423.161] [59.298], Avg: [-406.991 -406.991 -406.991] (0.1000) ({r_i: None, r_t: [-755.119 -755.119 -755.119], eps: 0.1})
Step:  191500, Reward: [-432.992 -432.992 -432.992] [108.472], Avg: [-448.203 -448.203 -448.203] (0.0100) ({r_i: None, r_t: [-735.870 -735.870 -735.870], eps: 0.01})
Step:  159200, Reward: [-397.809 -397.809 -397.809] [70.299], Avg: [-406.985 -406.985 -406.985] (0.1000) ({r_i: None, r_t: [-825.227 -825.227 -825.227], eps: 0.1})
Step:  191600, Reward: [-399.094 -399.094 -399.094] [68.713], Avg: [-448.178 -448.178 -448.178] (0.0100) ({r_i: None, r_t: [-785.450 -785.450 -785.450], eps: 0.01})
Step:  159300, Reward: [-393.598 -393.598 -393.598] [77.362], Avg: [-406.977 -406.977 -406.977] (0.1000) ({r_i: None, r_t: [-828.016 -828.016 -828.016], eps: 0.1})
Step:  191700, Reward: [-403.033 -403.033 -403.033] [93.795], Avg: [-448.154 -448.154 -448.154] (0.0100) ({r_i: None, r_t: [-775.573 -775.573 -775.573], eps: 0.01})
Step:  159400, Reward: [-391.156 -391.156 -391.156] [77.732], Avg: [-406.967 -406.967 -406.967] (0.1000) ({r_i: None, r_t: [-806.280 -806.280 -806.280], eps: 0.1})
Step:  191800, Reward: [-411.310 -411.310 -411.310] [64.712], Avg: [-448.135 -448.135 -448.135] (0.0100) ({r_i: None, r_t: [-791.454 -791.454 -791.454], eps: 0.01})
Step:  159500, Reward: [-392.349 -392.349 -392.349] [53.721], Avg: [-406.958 -406.958 -406.958] (0.1000) ({r_i: None, r_t: [-822.251 -822.251 -822.251], eps: 0.1})
Step:  191900, Reward: [-412.841 -412.841 -412.841] [74.127], Avg: [-448.116 -448.116 -448.116] (0.0100) ({r_i: None, r_t: [-813.533 -813.533 -813.533], eps: 0.01})
Step:  159600, Reward: [-411.579 -411.579 -411.579] [78.292], Avg: [-406.961 -406.961 -406.961] (0.1000) ({r_i: None, r_t: [-819.796 -819.796 -819.796], eps: 0.1})
Step:  192000, Reward: [-380.732 -380.732 -380.732] [69.681], Avg: [-448.081 -448.081 -448.081] (0.0100) ({r_i: None, r_t: [-821.084 -821.084 -821.084], eps: 0.01})
Step:  159700, Reward: [-406.458 -406.458 -406.458] [66.555], Avg: [-406.960 -406.960 -406.960] (0.1000) ({r_i: None, r_t: [-780.884 -780.884 -780.884], eps: 0.1})
Step:  192100, Reward: [-359.781 -359.781 -359.781] [73.161], Avg: [-448.035 -448.035 -448.035] (0.0100) ({r_i: None, r_t: [-737.114 -737.114 -737.114], eps: 0.01})
Step:  159800, Reward: [-416.601 -416.601 -416.601] [56.762], Avg: [-406.966 -406.966 -406.966] (0.1000) ({r_i: None, r_t: [-780.469 -780.469 -780.469], eps: 0.1})
Step:  192200, Reward: [-403.656 -403.656 -403.656] [94.857], Avg: [-448.012 -448.012 -448.012] (0.0100) ({r_i: None, r_t: [-803.124 -803.124 -803.124], eps: 0.01})
Step:  159900, Reward: [-431.330 -431.330 -431.330] [99.723], Avg: [-406.982 -406.982 -406.982] (0.1000) ({r_i: None, r_t: [-816.136 -816.136 -816.136], eps: 0.1})
Step:  192300, Reward: [-377.841 -377.841 -377.841] [76.914], Avg: [-447.976 -447.976 -447.976] (0.0100) ({r_i: None, r_t: [-830.445 -830.445 -830.445], eps: 0.01})
Step:  160000, Reward: [-406.584 -406.584 -406.584] [87.429], Avg: [-406.981 -406.981 -406.981] (0.1000) ({r_i: None, r_t: [-815.776 -815.776 -815.776], eps: 0.1})
Step:  192400, Reward: [-405.903 -405.903 -405.903] [72.182], Avg: [-447.954 -447.954 -447.954] (0.0100) ({r_i: None, r_t: [-765.129 -765.129 -765.129], eps: 0.01})
Step:  160100, Reward: [-372.117 -372.117 -372.117] [45.837], Avg: [-406.960 -406.960 -406.960] (0.1000) ({r_i: None, r_t: [-768.251 -768.251 -768.251], eps: 0.1})
Step:  192500, Reward: [-386.607 -386.607 -386.607] [65.788], Avg: [-447.922 -447.922 -447.922] (0.0100) ({r_i: None, r_t: [-804.932 -804.932 -804.932], eps: 0.01})
Step:  160200, Reward: [-415.948 -415.948 -415.948] [93.686], Avg: [-406.965 -406.965 -406.965] (0.1000) ({r_i: None, r_t: [-787.425 -787.425 -787.425], eps: 0.1})
Step:  192600, Reward: [-366.339 -366.339 -366.339] [57.271], Avg: [-447.880 -447.880 -447.880] (0.0100) ({r_i: None, r_t: [-758.698 -758.698 -758.698], eps: 0.01})
Step:  160300, Reward: [-399.727 -399.727 -399.727] [73.573], Avg: [-406.961 -406.961 -406.961] (0.1000) ({r_i: None, r_t: [-782.620 -782.620 -782.620], eps: 0.1})
Step:  192700, Reward: [-403.258 -403.258 -403.258] [97.586], Avg: [-447.857 -447.857 -447.857] (0.0100) ({r_i: None, r_t: [-813.996 -813.996 -813.996], eps: 0.01})
Step:  160400, Reward: [-395.605 -395.605 -395.605] [90.013], Avg: [-406.954 -406.954 -406.954] (0.1000) ({r_i: None, r_t: [-814.431 -814.431 -814.431], eps: 0.1})
Step:  192800, Reward: [-377.793 -377.793 -377.793] [82.324], Avg: [-447.820 -447.820 -447.820] (0.0100) ({r_i: None, r_t: [-784.049 -784.049 -784.049], eps: 0.01})
Step:  160500, Reward: [-395.663 -395.663 -395.663] [51.526], Avg: [-406.947 -406.947 -406.947] (0.1000) ({r_i: None, r_t: [-828.898 -828.898 -828.898], eps: 0.1})
Step:  192900, Reward: [-382.124 -382.124 -382.124] [90.005], Avg: [-447.786 -447.786 -447.786] (0.0100) ({r_i: None, r_t: [-811.887 -811.887 -811.887], eps: 0.01})
Step:  160600, Reward: [-409.429 -409.429 -409.429] [93.860], Avg: [-406.948 -406.948 -406.948] (0.1000) ({r_i: None, r_t: [-778.052 -778.052 -778.052], eps: 0.1})
Step:  193000, Reward: [-387.552 -387.552 -387.552] [94.716], Avg: [-447.755 -447.755 -447.755] (0.0100) ({r_i: None, r_t: [-773.754 -773.754 -773.754], eps: 0.01})
Step:  160700, Reward: [-384.998 -384.998 -384.998] [59.740], Avg: [-406.934 -406.934 -406.934] (0.1000) ({r_i: None, r_t: [-820.563 -820.563 -820.563], eps: 0.1})
Step:  193100, Reward: [-387.941 -387.941 -387.941] [74.150], Avg: [-447.724 -447.724 -447.724] (0.0100) ({r_i: None, r_t: [-796.424 -796.424 -796.424], eps: 0.01})
Step:  160800, Reward: [-415.481 -415.481 -415.481] [54.242], Avg: [-406.940 -406.940 -406.940] (0.1000) ({r_i: None, r_t: [-837.334 -837.334 -837.334], eps: 0.1})
Step:  193200, Reward: [-374.315 -374.315 -374.315] [52.221], Avg: [-447.686 -447.686 -447.686] (0.0100) ({r_i: None, r_t: [-775.060 -775.060 -775.060], eps: 0.01})
Step:  160900, Reward: [-380.075 -380.075 -380.075] [69.917], Avg: [-406.923 -406.923 -406.923] (0.1000) ({r_i: None, r_t: [-807.952 -807.952 -807.952], eps: 0.1})
Step:  193300, Reward: [-348.526 -348.526 -348.526] [43.706], Avg: [-447.635 -447.635 -447.635] (0.0100) ({r_i: None, r_t: [-809.605 -809.605 -809.605], eps: 0.01})
Step:  161000, Reward: [-402.539 -402.539 -402.539] [52.420], Avg: [-406.920 -406.920 -406.920] (0.1000) ({r_i: None, r_t: [-767.636 -767.636 -767.636], eps: 0.1})
Step:  193400, Reward: [-343.336 -343.336 -343.336] [59.563], Avg: [-447.581 -447.581 -447.581] (0.0100) ({r_i: None, r_t: [-774.570 -774.570 -774.570], eps: 0.01})
Step:  161100, Reward: [-397.262 -397.262 -397.262] [57.134], Avg: [-406.914 -406.914 -406.914] (0.1000) ({r_i: None, r_t: [-757.778 -757.778 -757.778], eps: 0.1})
Step:  193500, Reward: [-387.999 -387.999 -387.999] [77.811], Avg: [-447.550 -447.550 -447.550] (0.0100) ({r_i: None, r_t: [-815.663 -815.663 -815.663], eps: 0.01})
Step:  161200, Reward: [-402.047 -402.047 -402.047] [71.497], Avg: [-406.911 -406.911 -406.911] (0.1000) ({r_i: None, r_t: [-752.201 -752.201 -752.201], eps: 0.1})
Step:  193600, Reward: [-343.652 -343.652 -343.652] [57.858], Avg: [-447.497 -447.497 -447.497] (0.0100) ({r_i: None, r_t: [-747.086 -747.086 -747.086], eps: 0.01})
Step:  161300, Reward: [-376.414 -376.414 -376.414] [71.918], Avg: [-406.892 -406.892 -406.892] (0.1000) ({r_i: None, r_t: [-817.771 -817.771 -817.771], eps: 0.1})
Step:  193700, Reward: [-404.912 -404.912 -404.912] [86.916], Avg: [-447.475 -447.475 -447.475] (0.0100) ({r_i: None, r_t: [-764.393 -764.393 -764.393], eps: 0.01})
Step:  161400, Reward: [-404.938 -404.938 -404.938] [74.549], Avg: [-406.891 -406.891 -406.891] (0.1000) ({r_i: None, r_t: [-827.291 -827.291 -827.291], eps: 0.1})
Step:  193800, Reward: [-357.478 -357.478 -357.478] [65.985], Avg: [-447.428 -447.428 -447.428] (0.0100) ({r_i: None, r_t: [-753.062 -753.062 -753.062], eps: 0.01})
Step:  161500, Reward: [-396.191 -396.191 -396.191] [72.773], Avg: [-406.885 -406.885 -406.885] (0.1000) ({r_i: None, r_t: [-842.989 -842.989 -842.989], eps: 0.1})
Step:  193900, Reward: [-388.339 -388.339 -388.339] [65.801], Avg: [-447.398 -447.398 -447.398] (0.0100) ({r_i: None, r_t: [-774.653 -774.653 -774.653], eps: 0.01})
Step:  161600, Reward: [-395.014 -395.014 -395.014] [53.539], Avg: [-406.877 -406.877 -406.877] (0.1000) ({r_i: None, r_t: [-777.445 -777.445 -777.445], eps: 0.1})
Step:  194000, Reward: [-400.784 -400.784 -400.784] [69.280], Avg: [-447.374 -447.374 -447.374] (0.0100) ({r_i: None, r_t: [-775.197 -775.197 -775.197], eps: 0.01})
Step:  161700, Reward: [-382.180 -382.180 -382.180] [54.312], Avg: [-406.862 -406.862 -406.862] (0.1000) ({r_i: None, r_t: [-794.686 -794.686 -794.686], eps: 0.1})
Step:  194100, Reward: [-381.468 -381.468 -381.468] [91.534], Avg: [-447.340 -447.340 -447.340] (0.0100) ({r_i: None, r_t: [-789.895 -789.895 -789.895], eps: 0.01})
Step:  161800, Reward: [-408.549 -408.549 -408.549] [79.387], Avg: [-406.863 -406.863 -406.863] (0.1000) ({r_i: None, r_t: [-755.870 -755.870 -755.870], eps: 0.1})
Step:  194200, Reward: [-399.412 -399.412 -399.412] [72.649], Avg: [-447.315 -447.315 -447.315] (0.0100) ({r_i: None, r_t: [-769.538 -769.538 -769.538], eps: 0.01})
Step:  161900, Reward: [-372.088 -372.088 -372.088] [54.163], Avg: [-406.842 -406.842 -406.842] (0.1000) ({r_i: None, r_t: [-805.774 -805.774 -805.774], eps: 0.1})
Step:  194300, Reward: [-371.408 -371.408 -371.408] [62.189], Avg: [-447.276 -447.276 -447.276] (0.0100) ({r_i: None, r_t: [-826.166 -826.166 -826.166], eps: 0.01})
Step:  162000, Reward: [-408.942 -408.942 -408.942] [65.900], Avg: [-406.843 -406.843 -406.843] (0.1000) ({r_i: None, r_t: [-798.043 -798.043 -798.043], eps: 0.1})
Step:  194400, Reward: [-377.847 -377.847 -377.847] [69.260], Avg: [-447.240 -447.240 -447.240] (0.0100) ({r_i: None, r_t: [-809.874 -809.874 -809.874], eps: 0.01})
Step:  162100, Reward: [-407.675 -407.675 -407.675] [53.180], Avg: [-406.843 -406.843 -406.843] (0.1000) ({r_i: None, r_t: [-756.490 -756.490 -756.490], eps: 0.1})
Step:  194500, Reward: [-357.518 -357.518 -357.518] [62.854], Avg: [-447.194 -447.194 -447.194] (0.0100) ({r_i: None, r_t: [-787.326 -787.326 -787.326], eps: 0.01})
Step:  162200, Reward: [-369.861 -369.861 -369.861] [54.482], Avg: [-406.821 -406.821 -406.821] (0.1000) ({r_i: None, r_t: [-774.762 -774.762 -774.762], eps: 0.1})
Step:  194600, Reward: [-384.010 -384.010 -384.010] [49.656], Avg: [-447.162 -447.162 -447.162] (0.0100) ({r_i: None, r_t: [-805.774 -805.774 -805.774], eps: 0.01})
Step:  162300, Reward: [-420.358 -420.358 -420.358] [38.729], Avg: [-406.829 -406.829 -406.829] (0.1000) ({r_i: None, r_t: [-766.532 -766.532 -766.532], eps: 0.1})
Step:  194700, Reward: [-395.078 -395.078 -395.078] [71.729], Avg: [-447.135 -447.135 -447.135] (0.0100) ({r_i: None, r_t: [-743.934 -743.934 -743.934], eps: 0.01})
Step:  162400, Reward: [-421.644 -421.644 -421.644] [76.157], Avg: [-406.838 -406.838 -406.838] (0.1000) ({r_i: None, r_t: [-827.293 -827.293 -827.293], eps: 0.1})
Step:  194800, Reward: [-388.522 -388.522 -388.522] [90.667], Avg: [-447.105 -447.105 -447.105] (0.0100) ({r_i: None, r_t: [-798.044 -798.044 -798.044], eps: 0.01})
Step:  162500, Reward: [-389.974 -389.974 -389.974] [57.918], Avg: [-406.828 -406.828 -406.828] (0.1000) ({r_i: None, r_t: [-789.977 -789.977 -789.977], eps: 0.1})
Step:  194900, Reward: [-376.155 -376.155 -376.155] [59.012], Avg: [-447.069 -447.069 -447.069] (0.0100) ({r_i: None, r_t: [-840.763 -840.763 -840.763], eps: 0.01})
Step:  162600, Reward: [-396.533 -396.533 -396.533] [45.541], Avg: [-406.821 -406.821 -406.821] (0.1000) ({r_i: None, r_t: [-779.682 -779.682 -779.682], eps: 0.1})
Step:  195000, Reward: [-408.944 -408.944 -408.944] [67.987], Avg: [-447.049 -447.049 -447.049] (0.0100) ({r_i: None, r_t: [-764.026 -764.026 -764.026], eps: 0.01})
Step:  162700, Reward: [-368.212 -368.212 -368.212] [67.165], Avg: [-406.798 -406.798 -406.798] (0.1000) ({r_i: None, r_t: [-812.082 -812.082 -812.082], eps: 0.1})
Step:  195100, Reward: [-359.250 -359.250 -359.250] [54.655], Avg: [-447.004 -447.004 -447.004] (0.0100) ({r_i: None, r_t: [-795.787 -795.787 -795.787], eps: 0.01})
Step:  162800, Reward: [-385.844 -385.844 -385.844] [54.594], Avg: [-406.785 -406.785 -406.785] (0.1000) ({r_i: None, r_t: [-795.727 -795.727 -795.727], eps: 0.1})
Step:  195200, Reward: [-370.368 -370.368 -370.368] [64.182], Avg: [-446.965 -446.965 -446.965] (0.0100) ({r_i: None, r_t: [-789.025 -789.025 -789.025], eps: 0.01})
Step:  162900, Reward: [-367.694 -367.694 -367.694] [46.193], Avg: [-406.761 -406.761 -406.761] (0.1000) ({r_i: None, r_t: [-763.836 -763.836 -763.836], eps: 0.1})
Step:  195300, Reward: [-373.317 -373.317 -373.317] [55.687], Avg: [-446.927 -446.927 -446.927] (0.0100) ({r_i: None, r_t: [-738.400 -738.400 -738.400], eps: 0.01})
Step:  163000, Reward: [-380.547 -380.547 -380.547] [80.649], Avg: [-406.745 -406.745 -406.745] (0.1000) ({r_i: None, r_t: [-746.617 -746.617 -746.617], eps: 0.1})
Step:  195400, Reward: [-376.791 -376.791 -376.791] [68.978], Avg: [-446.891 -446.891 -446.891] (0.0100) ({r_i: None, r_t: [-685.348 -685.348 -685.348], eps: 0.01})
Step:  163100, Reward: [-396.125 -396.125 -396.125] [87.399], Avg: [-406.738 -406.738 -406.738] (0.1000) ({r_i: None, r_t: [-776.190 -776.190 -776.190], eps: 0.1})
Step:  195500, Reward: [-374.982 -374.982 -374.982] [64.097], Avg: [-446.855 -446.855 -446.855] (0.0100) ({r_i: None, r_t: [-750.380 -750.380 -750.380], eps: 0.01})
Step:  163200, Reward: [-393.875 -393.875 -393.875] [63.200], Avg: [-406.730 -406.730 -406.730] (0.1000) ({r_i: None, r_t: [-815.141 -815.141 -815.141], eps: 0.1})
Step:  195600, Reward: [-382.828 -382.828 -382.828] [54.613], Avg: [-446.822 -446.822 -446.822] (0.0100) ({r_i: None, r_t: [-741.961 -741.961 -741.961], eps: 0.01})
Step:  163300, Reward: [-398.190 -398.190 -398.190] [39.568], Avg: [-406.725 -406.725 -406.725] (0.1000) ({r_i: None, r_t: [-788.776 -788.776 -788.776], eps: 0.1})
Step:  195700, Reward: [-382.440 -382.440 -382.440] [74.593], Avg: [-446.789 -446.789 -446.789] (0.0100) ({r_i: None, r_t: [-755.179 -755.179 -755.179], eps: 0.01})
Step:  163400, Reward: [-362.930 -362.930 -362.930] [39.962], Avg: [-406.698 -406.698 -406.698] (0.1000) ({r_i: None, r_t: [-752.616 -752.616 -752.616], eps: 0.1})
Step:  195800, Reward: [-385.300 -385.300 -385.300] [83.357], Avg: [-446.758 -446.758 -446.758] (0.0100) ({r_i: None, r_t: [-732.671 -732.671 -732.671], eps: 0.01})
Step:  163500, Reward: [-400.409 -400.409 -400.409] [85.665], Avg: [-406.694 -406.694 -406.694] (0.1000) ({r_i: None, r_t: [-769.856 -769.856 -769.856], eps: 0.1})
Step:  195900, Reward: [-373.129 -373.129 -373.129] [63.741], Avg: [-446.720 -446.720 -446.720] (0.0100) ({r_i: None, r_t: [-773.554 -773.554 -773.554], eps: 0.01})
Step:  163600, Reward: [-388.794 -388.794 -388.794] [65.480], Avg: [-406.684 -406.684 -406.684] (0.1000) ({r_i: None, r_t: [-816.775 -816.775 -816.775], eps: 0.1})
Step:  196000, Reward: [-360.878 -360.878 -360.878] [73.852], Avg: [-446.676 -446.676 -446.676] (0.0100) ({r_i: None, r_t: [-756.468 -756.468 -756.468], eps: 0.01})
Step:  163700, Reward: [-397.475 -397.475 -397.475] [86.263], Avg: [-406.678 -406.678 -406.678] (0.1000) ({r_i: None, r_t: [-754.630 -754.630 -754.630], eps: 0.1})
Step:  196100, Reward: [-383.306 -383.306 -383.306] [79.500], Avg: [-446.644 -446.644 -446.644] (0.0100) ({r_i: None, r_t: [-736.855 -736.855 -736.855], eps: 0.01})
Step:  163800, Reward: [-422.997 -422.997 -422.997] [68.824], Avg: [-406.688 -406.688 -406.688] (0.1000) ({r_i: None, r_t: [-773.212 -773.212 -773.212], eps: 0.1})
Step:  196200, Reward: [-389.924 -389.924 -389.924] [58.149], Avg: [-446.615 -446.615 -446.615] (0.0100) ({r_i: None, r_t: [-746.444 -746.444 -746.444], eps: 0.01})
Step:  163900, Reward: [-407.038 -407.038 -407.038] [87.764], Avg: [-406.688 -406.688 -406.688] (0.1000) ({r_i: None, r_t: [-761.160 -761.160 -761.160], eps: 0.1})
Step:  196300, Reward: [-394.889 -394.889 -394.889] [81.327], Avg: [-446.589 -446.589 -446.589] (0.0100) ({r_i: None, r_t: [-776.526 -776.526 -776.526], eps: 0.01})
Step:  164000, Reward: [-403.178 -403.178 -403.178] [64.761], Avg: [-406.686 -406.686 -406.686] (0.1000) ({r_i: None, r_t: [-759.290 -759.290 -759.290], eps: 0.1})
Step:  196400, Reward: [-407.920 -407.920 -407.920] [87.727], Avg: [-446.569 -446.569 -446.569] (0.0100) ({r_i: None, r_t: [-773.545 -773.545 -773.545], eps: 0.01})
Step:  164100, Reward: [-382.044 -382.044 -382.044] [75.757], Avg: [-406.671 -406.671 -406.671] (0.1000) ({r_i: None, r_t: [-749.453 -749.453 -749.453], eps: 0.1})
Step:  196500, Reward: [-365.952 -365.952 -365.952] [57.014], Avg: [-446.528 -446.528 -446.528] (0.0100) ({r_i: None, r_t: [-767.600 -767.600 -767.600], eps: 0.01})
Step:  164200, Reward: [-411.210 -411.210 -411.210] [72.274], Avg: [-406.674 -406.674 -406.674] (0.1000) ({r_i: None, r_t: [-833.718 -833.718 -833.718], eps: 0.1})
Step:  196600, Reward: [-377.016 -377.016 -377.016] [44.928], Avg: [-446.493 -446.493 -446.493] (0.0100) ({r_i: None, r_t: [-778.000 -778.000 -778.000], eps: 0.01})
Step:  164300, Reward: [-378.018 -378.018 -378.018] [57.850], Avg: [-406.656 -406.656 -406.656] (0.1000) ({r_i: None, r_t: [-783.637 -783.637 -783.637], eps: 0.1})
Step:  196700, Reward: [-393.915 -393.915 -393.915] [65.027], Avg: [-446.466 -446.466 -446.466] (0.0100) ({r_i: None, r_t: [-804.135 -804.135 -804.135], eps: 0.01})
Step:  164400, Reward: [-432.164 -432.164 -432.164] [61.349], Avg: [-406.672 -406.672 -406.672] (0.1000) ({r_i: None, r_t: [-753.229 -753.229 -753.229], eps: 0.1})
Step:  196800, Reward: [-383.817 -383.817 -383.817] [79.343], Avg: [-446.434 -446.434 -446.434] (0.0100) ({r_i: None, r_t: [-737.311 -737.311 -737.311], eps: 0.01})
Step:  164500, Reward: [-381.263 -381.263 -381.263] [65.039], Avg: [-406.656 -406.656 -406.656] (0.1000) ({r_i: None, r_t: [-763.830 -763.830 -763.830], eps: 0.1})
Step:  196900, Reward: [-377.938 -377.938 -377.938] [75.399], Avg: [-446.399 -446.399 -446.399] (0.0100) ({r_i: None, r_t: [-773.408 -773.408 -773.408], eps: 0.01})
Step:  164600, Reward: [-394.039 -394.039 -394.039] [76.250], Avg: [-406.649 -406.649 -406.649] (0.1000) ({r_i: None, r_t: [-783.099 -783.099 -783.099], eps: 0.1})
Step:  197000, Reward: [-373.431 -373.431 -373.431] [83.148], Avg: [-446.362 -446.362 -446.362] (0.0100) ({r_i: None, r_t: [-776.439 -776.439 -776.439], eps: 0.01})
Step:  164700, Reward: [-358.713 -358.713 -358.713] [53.619], Avg: [-406.620 -406.620 -406.620] (0.1000) ({r_i: None, r_t: [-803.157 -803.157 -803.157], eps: 0.1})
Step:  197100, Reward: [-433.945 -433.945 -433.945] [72.413], Avg: [-446.356 -446.356 -446.356] (0.0100) ({r_i: None, r_t: [-793.233 -793.233 -793.233], eps: 0.01})
Step:  164800, Reward: [-402.969 -402.969 -402.969] [57.154], Avg: [-406.617 -406.617 -406.617] (0.1000) ({r_i: None, r_t: [-773.452 -773.452 -773.452], eps: 0.1})
Step:  197200, Reward: [-378.768 -378.768 -378.768] [77.133], Avg: [-446.322 -446.322 -446.322] (0.0100) ({r_i: None, r_t: [-751.863 -751.863 -751.863], eps: 0.01})
Step:  164900, Reward: [-386.621 -386.621 -386.621] [73.216], Avg: [-406.605 -406.605 -406.605] (0.1000) ({r_i: None, r_t: [-789.929 -789.929 -789.929], eps: 0.1})
Step:  197300, Reward: [-399.691 -399.691 -399.691] [52.364], Avg: [-446.298 -446.298 -446.298] (0.0100) ({r_i: None, r_t: [-778.035 -778.035 -778.035], eps: 0.01})
Step:  165000, Reward: [-382.442 -382.442 -382.442] [64.910], Avg: [-406.591 -406.591 -406.591] (0.1000) ({r_i: None, r_t: [-790.290 -790.290 -790.290], eps: 0.1})
Step:  197400, Reward: [-392.742 -392.742 -392.742] [65.630], Avg: [-446.271 -446.271 -446.271] (0.0100) ({r_i: None, r_t: [-790.255 -790.255 -790.255], eps: 0.01})
Step:  165100, Reward: [-385.449 -385.449 -385.449] [52.299], Avg: [-406.578 -406.578 -406.578] (0.1000) ({r_i: None, r_t: [-767.235 -767.235 -767.235], eps: 0.1})
Step:  197500, Reward: [-351.409 -351.409 -351.409] [73.344], Avg: [-446.223 -446.223 -446.223] (0.0100) ({r_i: None, r_t: [-790.375 -790.375 -790.375], eps: 0.01})
Step:  165200, Reward: [-403.447 -403.447 -403.447] [57.799], Avg: [-406.576 -406.576 -406.576] (0.1000) ({r_i: None, r_t: [-773.695 -773.695 -773.695], eps: 0.1})
Step:  197600, Reward: [-354.005 -354.005 -354.005] [61.911], Avg: [-446.176 -446.176 -446.176] (0.0100) ({r_i: None, r_t: [-780.667 -780.667 -780.667], eps: 0.01})
Step:  165300, Reward: [-348.153 -348.153 -348.153] [68.794], Avg: [-406.541 -406.541 -406.541] (0.1000) ({r_i: None, r_t: [-819.554 -819.554 -819.554], eps: 0.1})
Step:  197700, Reward: [-389.969 -389.969 -389.969] [88.882], Avg: [-446.148 -446.148 -446.148] (0.0100) ({r_i: None, r_t: [-786.356 -786.356 -786.356], eps: 0.01})
Step:  165400, Reward: [-383.989 -383.989 -383.989] [58.355], Avg: [-406.527 -406.527 -406.527] (0.1000) ({r_i: None, r_t: [-743.081 -743.081 -743.081], eps: 0.1})
Step:  197800, Reward: [-409.211 -409.211 -409.211] [71.756], Avg: [-446.129 -446.129 -446.129] (0.0100) ({r_i: None, r_t: [-761.289 -761.289 -761.289], eps: 0.01})
Step:  165500, Reward: [-384.429 -384.429 -384.429] [61.920], Avg: [-406.514 -406.514 -406.514] (0.1000) ({r_i: None, r_t: [-784.256 -784.256 -784.256], eps: 0.1})
Step:  197900, Reward: [-403.116 -403.116 -403.116] [66.877], Avg: [-446.108 -446.108 -446.108] (0.0100) ({r_i: None, r_t: [-774.784 -774.784 -774.784], eps: 0.01})
Step:  165600, Reward: [-410.529 -410.529 -410.529] [58.680], Avg: [-406.516 -406.516 -406.516] (0.1000) ({r_i: None, r_t: [-799.815 -799.815 -799.815], eps: 0.1})
Step:  198000, Reward: [-406.736 -406.736 -406.736] [86.966], Avg: [-446.088 -446.088 -446.088] (0.0100) ({r_i: None, r_t: [-788.932 -788.932 -788.932], eps: 0.01})
Step:  165700, Reward: [-392.148 -392.148 -392.148] [62.339], Avg: [-406.507 -406.507 -406.507] (0.1000) ({r_i: None, r_t: [-780.466 -780.466 -780.466], eps: 0.1})
Step:  198100, Reward: [-374.678 -374.678 -374.678] [66.727], Avg: [-446.052 -446.052 -446.052] (0.0100) ({r_i: None, r_t: [-771.424 -771.424 -771.424], eps: 0.01})
Step:  165800, Reward: [-411.593 -411.593 -411.593] [72.256], Avg: [-406.510 -406.510 -406.510] (0.1000) ({r_i: None, r_t: [-809.503 -809.503 -809.503], eps: 0.1})
Step:  198200, Reward: [-416.900 -416.900 -416.900] [76.654], Avg: [-446.037 -446.037 -446.037] (0.0100) ({r_i: None, r_t: [-710.501 -710.501 -710.501], eps: 0.01})
Step:  165900, Reward: [-407.693 -407.693 -407.693] [53.478], Avg: [-406.511 -406.511 -406.511] (0.1000) ({r_i: None, r_t: [-789.278 -789.278 -789.278], eps: 0.1})
Step:  198300, Reward: [-366.312 -366.312 -366.312] [68.487], Avg: [-445.997 -445.997 -445.997] (0.0100) ({r_i: None, r_t: [-793.491 -793.491 -793.491], eps: 0.01})
Step:  166000, Reward: [-373.950 -373.950 -373.950] [74.322], Avg: [-406.492 -406.492 -406.492] (0.1000) ({r_i: None, r_t: [-786.256 -786.256 -786.256], eps: 0.1})
Step:  198400, Reward: [-418.810 -418.810 -418.810] [92.135], Avg: [-445.983 -445.983 -445.983] (0.0100) ({r_i: None, r_t: [-790.830 -790.830 -790.830], eps: 0.01})
Step:  166100, Reward: [-420.468 -420.468 -420.468] [50.961], Avg: [-406.500 -406.500 -406.500] (0.1000) ({r_i: None, r_t: [-767.935 -767.935 -767.935], eps: 0.1})
Step:  198500, Reward: [-378.097 -378.097 -378.097] [53.941], Avg: [-445.949 -445.949 -445.949] (0.0100) ({r_i: None, r_t: [-774.059 -774.059 -774.059], eps: 0.01})
Step:  166200, Reward: [-390.090 -390.090 -390.090] [61.659], Avg: [-406.490 -406.490 -406.490] (0.1000) ({r_i: None, r_t: [-784.029 -784.029 -784.029], eps: 0.1})
Step:  198600, Reward: [-386.812 -386.812 -386.812] [79.416], Avg: [-445.919 -445.919 -445.919] (0.0100) ({r_i: None, r_t: [-813.714 -813.714 -813.714], eps: 0.01})
Step:  166300, Reward: [-384.466 -384.466 -384.466] [56.467], Avg: [-406.477 -406.477 -406.477] (0.1000) ({r_i: None, r_t: [-751.260 -751.260 -751.260], eps: 0.1})
Step:  198700, Reward: [-373.619 -373.619 -373.619] [64.946], Avg: [-445.883 -445.883 -445.883] (0.0100) ({r_i: None, r_t: [-791.649 -791.649 -791.649], eps: 0.01})
Step:  166400, Reward: [-387.451 -387.451 -387.451] [57.399], Avg: [-406.465 -406.465 -406.465] (0.1000) ({r_i: None, r_t: [-776.296 -776.296 -776.296], eps: 0.1})
Step:  198800, Reward: [-397.411 -397.411 -397.411] [60.997], Avg: [-445.858 -445.858 -445.858] (0.0100) ({r_i: None, r_t: [-811.276 -811.276 -811.276], eps: 0.01})
Step:  166500, Reward: [-372.229 -372.229 -372.229] [57.192], Avg: [-406.445 -406.445 -406.445] (0.1000) ({r_i: None, r_t: [-747.186 -747.186 -747.186], eps: 0.1})
Step:  198900, Reward: [-393.948 -393.948 -393.948] [45.413], Avg: [-445.832 -445.832 -445.832] (0.0100) ({r_i: None, r_t: [-784.367 -784.367 -784.367], eps: 0.01})
Step:  166600, Reward: [-408.245 -408.245 -408.245] [72.106], Avg: [-406.446 -406.446 -406.446] (0.1000) ({r_i: None, r_t: [-773.898 -773.898 -773.898], eps: 0.1})
Step:  199000, Reward: [-344.248 -344.248 -344.248] [52.163], Avg: [-445.781 -445.781 -445.781] (0.0100) ({r_i: None, r_t: [-773.235 -773.235 -773.235], eps: 0.01})
Step:  166700, Reward: [-373.904 -373.904 -373.904] [61.175], Avg: [-406.426 -406.426 -406.426] (0.1000) ({r_i: None, r_t: [-778.617 -778.617 -778.617], eps: 0.1})
Step:  199100, Reward: [-373.423 -373.423 -373.423] [65.602], Avg: [-445.745 -445.745 -445.745] (0.0100) ({r_i: None, r_t: [-790.915 -790.915 -790.915], eps: 0.01})
Step:  166800, Reward: [-364.963 -364.963 -364.963] [54.924], Avg: [-406.402 -406.402 -406.402] (0.1000) ({r_i: None, r_t: [-756.660 -756.660 -756.660], eps: 0.1})
Step:  199200, Reward: [-399.484 -399.484 -399.484] [78.515], Avg: [-445.722 -445.722 -445.722] (0.0100) ({r_i: None, r_t: [-790.305 -790.305 -790.305], eps: 0.01})
Step:  166900, Reward: [-408.483 -408.483 -408.483] [75.502], Avg: [-406.403 -406.403 -406.403] (0.1000) ({r_i: None, r_t: [-749.685 -749.685 -749.685], eps: 0.1})
Step:  199300, Reward: [-399.321 -399.321 -399.321] [68.842], Avg: [-445.699 -445.699 -445.699] (0.0100) ({r_i: None, r_t: [-766.861 -766.861 -766.861], eps: 0.01})
Step:  167000, Reward: [-407.553 -407.553 -407.553] [55.939], Avg: [-406.404 -406.404 -406.404] (0.1000) ({r_i: None, r_t: [-773.408 -773.408 -773.408], eps: 0.1})
Step:  199400, Reward: [-401.258 -401.258 -401.258] [78.602], Avg: [-445.676 -445.676 -445.676] (0.0100) ({r_i: None, r_t: [-769.235 -769.235 -769.235], eps: 0.01})
Step:  167100, Reward: [-376.005 -376.005 -376.005] [65.074], Avg: [-406.385 -406.385 -406.385] (0.1000) ({r_i: None, r_t: [-784.103 -784.103 -784.103], eps: 0.1})
Step:  199500, Reward: [-369.860 -369.860 -369.860] [74.759], Avg: [-445.638 -445.638 -445.638] (0.0100) ({r_i: None, r_t: [-815.676 -815.676 -815.676], eps: 0.01})
Step:  167200, Reward: [-394.617 -394.617 -394.617] [81.571], Avg: [-406.378 -406.378 -406.378] (0.1000) ({r_i: None, r_t: [-747.659 -747.659 -747.659], eps: 0.1})
Step:  199600, Reward: [-365.123 -365.123 -365.123] [65.112], Avg: [-445.598 -445.598 -445.598] (0.0100) ({r_i: None, r_t: [-766.617 -766.617 -766.617], eps: 0.01})
Step:  167300, Reward: [-381.283 -381.283 -381.283] [59.386], Avg: [-406.363 -406.363 -406.363] (0.1000) ({r_i: None, r_t: [-760.979 -760.979 -760.979], eps: 0.1})
Step:  199700, Reward: [-377.217 -377.217 -377.217] [58.681], Avg: [-445.564 -445.564 -445.564] (0.0100) ({r_i: None, r_t: [-826.457 -826.457 -826.457], eps: 0.01})
Step:  167400, Reward: [-409.591 -409.591 -409.591] [70.613], Avg: [-406.365 -406.365 -406.365] (0.1000) ({r_i: None, r_t: [-787.206 -787.206 -787.206], eps: 0.1})
Step:  199800, Reward: [-384.009 -384.009 -384.009] [68.047], Avg: [-445.533 -445.533 -445.533] (0.0100) ({r_i: None, r_t: [-800.736 -800.736 -800.736], eps: 0.01})
Step:  167500, Reward: [-400.942 -400.942 -400.942] [68.249], Avg: [-406.362 -406.362 -406.362] (0.1000) ({r_i: None, r_t: [-747.761 -747.761 -747.761], eps: 0.1})
Step:  199900, Reward: [-436.579 -436.579 -436.579] [111.188], Avg: [-445.528 -445.528 -445.528] (0.0100) ({r_i: None, r_t: [-800.734 -800.734 -800.734], eps: 0.01})
Step:  167600, Reward: [-380.490 -380.490 -380.490] [54.419], Avg: [-406.347 -406.347 -406.347] (0.1000) ({r_i: None, r_t: [-819.042 -819.042 -819.042], eps: 0.1})
Step:  200000, Reward: [-425.731 -425.731 -425.731] [68.326], Avg: [-445.519 -445.519 -445.519] (0.0100) ({r_i: None, r_t: [-795.181 -795.181 -795.181], eps: 0.01})
Step:  167700, Reward: [-350.219 -350.219 -350.219] [66.402], Avg: [-406.313 -406.313 -406.313] (0.1000) ({r_i: None, r_t: [-800.946 -800.946 -800.946], eps: 0.1})
Step:  167800, Reward: [-388.644 -388.644 -388.644] [41.922], Avg: [-406.303 -406.303 -406.303] (0.1000) ({r_i: None, r_t: [-840.899 -840.899 -840.899], eps: 0.1})
Step:  167900, Reward: [-373.354 -373.354 -373.354] [62.559], Avg: [-406.283 -406.283 -406.283] (0.1000) ({r_i: None, r_t: [-798.013 -798.013 -798.013], eps: 0.1})
Step:  168000, Reward: [-390.851 -390.851 -390.851] [72.987], Avg: [-406.274 -406.274 -406.274] (0.1000) ({r_i: None, r_t: [-769.327 -769.327 -769.327], eps: 0.1})
Step:  168100, Reward: [-384.649 -384.649 -384.649] [79.016], Avg: [-406.261 -406.261 -406.261] (0.1000) ({r_i: None, r_t: [-763.079 -763.079 -763.079], eps: 0.1})
Step:  168200, Reward: [-400.119 -400.119 -400.119] [83.498], Avg: [-406.257 -406.257 -406.257] (0.1000) ({r_i: None, r_t: [-773.879 -773.879 -773.879], eps: 0.1})
Step:  168300, Reward: [-375.588 -375.588 -375.588] [55.909], Avg: [-406.239 -406.239 -406.239] (0.1000) ({r_i: None, r_t: [-791.344 -791.344 -791.344], eps: 0.1})
Step:  168400, Reward: [-382.915 -382.915 -382.915] [79.403], Avg: [-406.225 -406.225 -406.225] (0.1000) ({r_i: None, r_t: [-687.303 -687.303 -687.303], eps: 0.1})
Step:  168500, Reward: [-408.481 -408.481 -408.481] [74.701], Avg: [-406.227 -406.227 -406.227] (0.1000) ({r_i: None, r_t: [-756.760 -756.760 -756.760], eps: 0.1})
Step:  168600, Reward: [-381.592 -381.592 -381.592] [60.484], Avg: [-406.212 -406.212 -406.212] (0.1000) ({r_i: None, r_t: [-781.549 -781.549 -781.549], eps: 0.1})
Step:  168700, Reward: [-410.517 -410.517 -410.517] [69.408], Avg: [-406.215 -406.215 -406.215] (0.1000) ({r_i: None, r_t: [-746.009 -746.009 -746.009], eps: 0.1})
Step:  168800, Reward: [-401.279 -401.279 -401.279] [64.328], Avg: [-406.212 -406.212 -406.212] (0.1000) ({r_i: None, r_t: [-784.355 -784.355 -784.355], eps: 0.1})
Step:  168900, Reward: [-374.983 -374.983 -374.983] [76.771], Avg: [-406.193 -406.193 -406.193] (0.1000) ({r_i: None, r_t: [-774.256 -774.256 -774.256], eps: 0.1})
Step:  169000, Reward: [-386.740 -386.740 -386.740] [62.727], Avg: [-406.182 -406.182 -406.182] (0.1000) ({r_i: None, r_t: [-786.721 -786.721 -786.721], eps: 0.1})
Step:  169100, Reward: [-378.515 -378.515 -378.515] [60.759], Avg: [-406.165 -406.165 -406.165] (0.1000) ({r_i: None, r_t: [-794.219 -794.219 -794.219], eps: 0.1})
Step:  169200, Reward: [-370.528 -370.528 -370.528] [73.534], Avg: [-406.144 -406.144 -406.144] (0.1000) ({r_i: None, r_t: [-786.813 -786.813 -786.813], eps: 0.1})
Step:  169300, Reward: [-381.426 -381.426 -381.426] [49.504], Avg: [-406.130 -406.130 -406.130] (0.1000) ({r_i: None, r_t: [-728.904 -728.904 -728.904], eps: 0.1})
Step:  169400, Reward: [-394.310 -394.310 -394.310] [77.023], Avg: [-406.123 -406.123 -406.123] (0.1000) ({r_i: None, r_t: [-810.761 -810.761 -810.761], eps: 0.1})
Step:  169500, Reward: [-427.707 -427.707 -427.707] [69.532], Avg: [-406.135 -406.135 -406.135] (0.1000) ({r_i: None, r_t: [-797.724 -797.724 -797.724], eps: 0.1})
Step:  169600, Reward: [-402.142 -402.142 -402.142] [69.836], Avg: [-406.133 -406.133 -406.133] (0.1000) ({r_i: None, r_t: [-813.075 -813.075 -813.075], eps: 0.1})
Step:  169700, Reward: [-403.007 -403.007 -403.007] [83.155], Avg: [-406.131 -406.131 -406.131] (0.1000) ({r_i: None, r_t: [-776.440 -776.440 -776.440], eps: 0.1})
Step:  169800, Reward: [-403.983 -403.983 -403.983] [58.492], Avg: [-406.130 -406.130 -406.130] (0.1000) ({r_i: None, r_t: [-797.601 -797.601 -797.601], eps: 0.1})
Step:  169900, Reward: [-369.067 -369.067 -369.067] [76.557], Avg: [-406.108 -406.108 -406.108] (0.1000) ({r_i: None, r_t: [-754.960 -754.960 -754.960], eps: 0.1})
Step:  170000, Reward: [-396.210 -396.210 -396.210] [66.159], Avg: [-406.102 -406.102 -406.102] (0.1000) ({r_i: None, r_t: [-771.215 -771.215 -771.215], eps: 0.1})
Step:  170100, Reward: [-399.741 -399.741 -399.741] [60.262], Avg: [-406.099 -406.099 -406.099] (0.1000) ({r_i: None, r_t: [-778.675 -778.675 -778.675], eps: 0.1})
Step:  170200, Reward: [-392.662 -392.662 -392.662] [61.857], Avg: [-406.091 -406.091 -406.091] (0.1000) ({r_i: None, r_t: [-745.735 -745.735 -745.735], eps: 0.1})
Step:  170300, Reward: [-377.628 -377.628 -377.628] [51.865], Avg: [-406.074 -406.074 -406.074] (0.1000) ({r_i: None, r_t: [-718.712 -718.712 -718.712], eps: 0.1})
Step:  170400, Reward: [-378.413 -378.413 -378.413] [74.359], Avg: [-406.058 -406.058 -406.058] (0.1000) ({r_i: None, r_t: [-761.927 -761.927 -761.927], eps: 0.1})
Step:  170500, Reward: [-419.430 -419.430 -419.430] [50.878], Avg: [-406.066 -406.066 -406.066] (0.1000) ({r_i: None, r_t: [-768.344 -768.344 -768.344], eps: 0.1})
Step:  170600, Reward: [-388.113 -388.113 -388.113] [68.540], Avg: [-406.055 -406.055 -406.055] (0.1000) ({r_i: None, r_t: [-765.289 -765.289 -765.289], eps: 0.1})
Step:  170700, Reward: [-391.027 -391.027 -391.027] [76.735], Avg: [-406.046 -406.046 -406.046] (0.1000) ({r_i: None, r_t: [-755.167 -755.167 -755.167], eps: 0.1})
Step:  170800, Reward: [-390.923 -390.923 -390.923] [58.794], Avg: [-406.037 -406.037 -406.037] (0.1000) ({r_i: None, r_t: [-784.793 -784.793 -784.793], eps: 0.1})
Step:  170900, Reward: [-369.779 -369.779 -369.779] [51.758], Avg: [-406.016 -406.016 -406.016] (0.1000) ({r_i: None, r_t: [-723.800 -723.800 -723.800], eps: 0.1})
Step:  171000, Reward: [-406.706 -406.706 -406.706] [73.086], Avg: [-406.017 -406.017 -406.017] (0.1000) ({r_i: None, r_t: [-805.871 -805.871 -805.871], eps: 0.1})
Step:  171100, Reward: [-378.592 -378.592 -378.592] [53.591], Avg: [-406.001 -406.001 -406.001] (0.1000) ({r_i: None, r_t: [-773.670 -773.670 -773.670], eps: 0.1})
Step:  171200, Reward: [-390.812 -390.812 -390.812] [68.815], Avg: [-405.992 -405.992 -405.992] (0.1000) ({r_i: None, r_t: [-805.645 -805.645 -805.645], eps: 0.1})
Step:  171300, Reward: [-380.930 -380.930 -380.930] [63.941], Avg: [-405.977 -405.977 -405.977] (0.1000) ({r_i: None, r_t: [-797.383 -797.383 -797.383], eps: 0.1})
Step:  171400, Reward: [-411.679 -411.679 -411.679] [69.747], Avg: [-405.980 -405.980 -405.980] (0.1000) ({r_i: None, r_t: [-772.462 -772.462 -772.462], eps: 0.1})
Step:  171500, Reward: [-425.199 -425.199 -425.199] [72.192], Avg: [-405.992 -405.992 -405.992] (0.1000) ({r_i: None, r_t: [-799.778 -799.778 -799.778], eps: 0.1})
Step:  171600, Reward: [-379.237 -379.237 -379.237] [71.162], Avg: [-405.976 -405.976 -405.976] (0.1000) ({r_i: None, r_t: [-796.537 -796.537 -796.537], eps: 0.1})
Step:  171700, Reward: [-416.476 -416.476 -416.476] [81.952], Avg: [-405.982 -405.982 -405.982] (0.1000) ({r_i: None, r_t: [-838.910 -838.910 -838.910], eps: 0.1})
Step:  171800, Reward: [-378.787 -378.787 -378.787] [42.849], Avg: [-405.966 -405.966 -405.966] (0.1000) ({r_i: None, r_t: [-810.059 -810.059 -810.059], eps: 0.1})
Step:  171900, Reward: [-410.824 -410.824 -410.824] [82.423], Avg: [-405.969 -405.969 -405.969] (0.1000) ({r_i: None, r_t: [-767.070 -767.070 -767.070], eps: 0.1})
Step:  172000, Reward: [-383.540 -383.540 -383.540] [62.899], Avg: [-405.956 -405.956 -405.956] (0.1000) ({r_i: None, r_t: [-780.890 -780.890 -780.890], eps: 0.1})
Step:  172100, Reward: [-387.931 -387.931 -387.931] [75.097], Avg: [-405.946 -405.946 -405.946] (0.1000) ({r_i: None, r_t: [-776.289 -776.289 -776.289], eps: 0.1})
Step:  172200, Reward: [-364.694 -364.694 -364.694] [55.873], Avg: [-405.922 -405.922 -405.922] (0.1000) ({r_i: None, r_t: [-760.211 -760.211 -760.211], eps: 0.1})
Step:  172300, Reward: [-434.444 -434.444 -434.444] [56.613], Avg: [-405.938 -405.938 -405.938] (0.1000) ({r_i: None, r_t: [-767.531 -767.531 -767.531], eps: 0.1})
Step:  172400, Reward: [-351.016 -351.016 -351.016] [68.708], Avg: [-405.906 -405.906 -405.906] (0.1000) ({r_i: None, r_t: [-737.850 -737.850 -737.850], eps: 0.1})
Step:  172500, Reward: [-384.380 -384.380 -384.380] [87.188], Avg: [-405.894 -405.894 -405.894] (0.1000) ({r_i: None, r_t: [-820.922 -820.922 -820.922], eps: 0.1})
Step:  172600, Reward: [-398.955 -398.955 -398.955] [76.799], Avg: [-405.890 -405.890 -405.890] (0.1000) ({r_i: None, r_t: [-786.276 -786.276 -786.276], eps: 0.1})
Step:  172700, Reward: [-419.470 -419.470 -419.470] [57.709], Avg: [-405.898 -405.898 -405.898] (0.1000) ({r_i: None, r_t: [-799.765 -799.765 -799.765], eps: 0.1})
Step:  172800, Reward: [-387.544 -387.544 -387.544] [99.201], Avg: [-405.887 -405.887 -405.887] (0.1000) ({r_i: None, r_t: [-808.410 -808.410 -808.410], eps: 0.1})
Step:  172900, Reward: [-409.865 -409.865 -409.865] [73.015], Avg: [-405.890 -405.890 -405.890] (0.1000) ({r_i: None, r_t: [-801.853 -801.853 -801.853], eps: 0.1})
Step:  173000, Reward: [-397.244 -397.244 -397.244] [61.481], Avg: [-405.885 -405.885 -405.885] (0.1000) ({r_i: None, r_t: [-808.689 -808.689 -808.689], eps: 0.1})
Step:  173100, Reward: [-390.394 -390.394 -390.394] [60.901], Avg: [-405.876 -405.876 -405.876] (0.1000) ({r_i: None, r_t: [-778.824 -778.824 -778.824], eps: 0.1})
Step:  173200, Reward: [-364.756 -364.756 -364.756] [78.830], Avg: [-405.852 -405.852 -405.852] (0.1000) ({r_i: None, r_t: [-799.634 -799.634 -799.634], eps: 0.1})
Step:  173300, Reward: [-398.902 -398.902 -398.902] [61.885], Avg: [-405.848 -405.848 -405.848] (0.1000) ({r_i: None, r_t: [-789.327 -789.327 -789.327], eps: 0.1})
Step:  173400, Reward: [-398.079 -398.079 -398.079] [81.383], Avg: [-405.843 -405.843 -405.843] (0.1000) ({r_i: None, r_t: [-779.744 -779.744 -779.744], eps: 0.1})
Step:  173500, Reward: [-408.070 -408.070 -408.070] [87.968], Avg: [-405.845 -405.845 -405.845] (0.1000) ({r_i: None, r_t: [-786.851 -786.851 -786.851], eps: 0.1})
Step:  173600, Reward: [-394.137 -394.137 -394.137] [58.643], Avg: [-405.838 -405.838 -405.838] (0.1000) ({r_i: None, r_t: [-766.700 -766.700 -766.700], eps: 0.1})
Step:  173700, Reward: [-363.271 -363.271 -363.271] [57.279], Avg: [-405.813 -405.813 -405.813] (0.1000) ({r_i: None, r_t: [-766.668 -766.668 -766.668], eps: 0.1})
Step:  173800, Reward: [-393.828 -393.828 -393.828] [58.032], Avg: [-405.807 -405.807 -405.807] (0.1000) ({r_i: None, r_t: [-792.258 -792.258 -792.258], eps: 0.1})
Step:  173900, Reward: [-400.114 -400.114 -400.114] [82.372], Avg: [-405.803 -405.803 -405.803] (0.1000) ({r_i: None, r_t: [-827.204 -827.204 -827.204], eps: 0.1})
Step:  174000, Reward: [-426.490 -426.490 -426.490] [87.278], Avg: [-405.815 -405.815 -405.815] (0.1000) ({r_i: None, r_t: [-770.592 -770.592 -770.592], eps: 0.1})
Step:  174100, Reward: [-407.450 -407.450 -407.450] [69.258], Avg: [-405.816 -405.816 -405.816] (0.1000) ({r_i: None, r_t: [-791.058 -791.058 -791.058], eps: 0.1})
Step:  174200, Reward: [-399.661 -399.661 -399.661] [58.909], Avg: [-405.813 -405.813 -405.813] (0.1000) ({r_i: None, r_t: [-780.171 -780.171 -780.171], eps: 0.1})
Step:  174300, Reward: [-384.637 -384.637 -384.637] [78.868], Avg: [-405.800 -405.800 -405.800] (0.1000) ({r_i: None, r_t: [-784.383 -784.383 -784.383], eps: 0.1})
Step:  174400, Reward: [-415.326 -415.326 -415.326] [86.953], Avg: [-405.806 -405.806 -405.806] (0.1000) ({r_i: None, r_t: [-767.859 -767.859 -767.859], eps: 0.1})
Step:  174500, Reward: [-386.179 -386.179 -386.179] [77.153], Avg: [-405.795 -405.795 -405.795] (0.1000) ({r_i: None, r_t: [-773.618 -773.618 -773.618], eps: 0.1})
Step:  174600, Reward: [-417.408 -417.408 -417.408] [84.187], Avg: [-405.801 -405.801 -405.801] (0.1000) ({r_i: None, r_t: [-771.107 -771.107 -771.107], eps: 0.1})
Step:  174700, Reward: [-386.523 -386.523 -386.523] [60.310], Avg: [-405.790 -405.790 -405.790] (0.1000) ({r_i: None, r_t: [-762.296 -762.296 -762.296], eps: 0.1})
Step:  174800, Reward: [-367.797 -367.797 -367.797] [53.214], Avg: [-405.769 -405.769 -405.769] (0.1000) ({r_i: None, r_t: [-762.021 -762.021 -762.021], eps: 0.1})
Step:  174900, Reward: [-409.433 -409.433 -409.433] [67.887], Avg: [-405.771 -405.771 -405.771] (0.1000) ({r_i: None, r_t: [-828.471 -828.471 -828.471], eps: 0.1})
Step:  175000, Reward: [-380.815 -380.815 -380.815] [55.319], Avg: [-405.756 -405.756 -405.756] (0.1000) ({r_i: None, r_t: [-825.206 -825.206 -825.206], eps: 0.1})
Step:  175100, Reward: [-389.770 -389.770 -389.770] [61.925], Avg: [-405.747 -405.747 -405.747] (0.1000) ({r_i: None, r_t: [-770.499 -770.499 -770.499], eps: 0.1})
Step:  175200, Reward: [-413.214 -413.214 -413.214] [55.292], Avg: [-405.751 -405.751 -405.751] (0.1000) ({r_i: None, r_t: [-797.860 -797.860 -797.860], eps: 0.1})
Step:  175300, Reward: [-416.464 -416.464 -416.464] [80.291], Avg: [-405.758 -405.758 -405.758] (0.1000) ({r_i: None, r_t: [-775.322 -775.322 -775.322], eps: 0.1})
Step:  175400, Reward: [-402.711 -402.711 -402.711] [81.242], Avg: [-405.756 -405.756 -405.756] (0.1000) ({r_i: None, r_t: [-765.346 -765.346 -765.346], eps: 0.1})
Step:  175500, Reward: [-371.100 -371.100 -371.100] [58.188], Avg: [-405.736 -405.736 -405.736] (0.1000) ({r_i: None, r_t: [-786.145 -786.145 -786.145], eps: 0.1})
Step:  175600, Reward: [-426.418 -426.418 -426.418] [53.855], Avg: [-405.748 -405.748 -405.748] (0.1000) ({r_i: None, r_t: [-789.254 -789.254 -789.254], eps: 0.1})
Step:  175700, Reward: [-386.967 -386.967 -386.967] [72.135], Avg: [-405.737 -405.737 -405.737] (0.1000) ({r_i: None, r_t: [-766.338 -766.338 -766.338], eps: 0.1})
Step:  175800, Reward: [-383.961 -383.961 -383.961] [62.856], Avg: [-405.725 -405.725 -405.725] (0.1000) ({r_i: None, r_t: [-814.348 -814.348 -814.348], eps: 0.1})
Step:  175900, Reward: [-379.207 -379.207 -379.207] [52.783], Avg: [-405.710 -405.710 -405.710] (0.1000) ({r_i: None, r_t: [-789.938 -789.938 -789.938], eps: 0.1})
Step:  176000, Reward: [-389.260 -389.260 -389.260] [83.000], Avg: [-405.700 -405.700 -405.700] (0.1000) ({r_i: None, r_t: [-815.423 -815.423 -815.423], eps: 0.1})
Step:  176100, Reward: [-397.592 -397.592 -397.592] [52.681], Avg: [-405.696 -405.696 -405.696] (0.1000) ({r_i: None, r_t: [-776.502 -776.502 -776.502], eps: 0.1})
Step:  176200, Reward: [-392.406 -392.406 -392.406] [72.927], Avg: [-405.688 -405.688 -405.688] (0.1000) ({r_i: None, r_t: [-725.949 -725.949 -725.949], eps: 0.1})
Step:  176300, Reward: [-385.231 -385.231 -385.231] [72.754], Avg: [-405.677 -405.677 -405.677] (0.1000) ({r_i: None, r_t: [-782.315 -782.315 -782.315], eps: 0.1})
Step:  176400, Reward: [-386.987 -386.987 -386.987] [61.023], Avg: [-405.666 -405.666 -405.666] (0.1000) ({r_i: None, r_t: [-781.843 -781.843 -781.843], eps: 0.1})
Step:  176500, Reward: [-387.660 -387.660 -387.660] [51.426], Avg: [-405.656 -405.656 -405.656] (0.1000) ({r_i: None, r_t: [-770.932 -770.932 -770.932], eps: 0.1})
Step:  176600, Reward: [-390.777 -390.777 -390.777] [60.195], Avg: [-405.647 -405.647 -405.647] (0.1000) ({r_i: None, r_t: [-798.491 -798.491 -798.491], eps: 0.1})
Step:  176700, Reward: [-421.073 -421.073 -421.073] [67.054], Avg: [-405.656 -405.656 -405.656] (0.1000) ({r_i: None, r_t: [-819.633 -819.633 -819.633], eps: 0.1})
Step:  176800, Reward: [-379.185 -379.185 -379.185] [68.926], Avg: [-405.641 -405.641 -405.641] (0.1000) ({r_i: None, r_t: [-767.437 -767.437 -767.437], eps: 0.1})
Step:  176900, Reward: [-396.821 -396.821 -396.821] [70.939], Avg: [-405.636 -405.636 -405.636] (0.1000) ({r_i: None, r_t: [-785.310 -785.310 -785.310], eps: 0.1})
Step:  177000, Reward: [-405.550 -405.550 -405.550] [64.226], Avg: [-405.636 -405.636 -405.636] (0.1000) ({r_i: None, r_t: [-765.231 -765.231 -765.231], eps: 0.1})
Step:  177100, Reward: [-385.258 -385.258 -385.258] [78.350], Avg: [-405.625 -405.625 -405.625] (0.1000) ({r_i: None, r_t: [-745.091 -745.091 -745.091], eps: 0.1})
Step:  177200, Reward: [-379.721 -379.721 -379.721] [67.358], Avg: [-405.610 -405.610 -405.610] (0.1000) ({r_i: None, r_t: [-784.019 -784.019 -784.019], eps: 0.1})
Step:  177300, Reward: [-386.251 -386.251 -386.251] [56.542], Avg: [-405.599 -405.599 -405.599] (0.1000) ({r_i: None, r_t: [-796.750 -796.750 -796.750], eps: 0.1})
Step:  177400, Reward: [-376.660 -376.660 -376.660] [53.122], Avg: [-405.583 -405.583 -405.583] (0.1000) ({r_i: None, r_t: [-799.322 -799.322 -799.322], eps: 0.1})
Step:  177500, Reward: [-397.207 -397.207 -397.207] [60.123], Avg: [-405.578 -405.578 -405.578] (0.1000) ({r_i: None, r_t: [-780.134 -780.134 -780.134], eps: 0.1})
Step:  177600, Reward: [-382.922 -382.922 -382.922] [55.661], Avg: [-405.565 -405.565 -405.565] (0.1000) ({r_i: None, r_t: [-777.033 -777.033 -777.033], eps: 0.1})
Step:  177700, Reward: [-437.937 -437.937 -437.937] [56.511], Avg: [-405.584 -405.584 -405.584] (0.1000) ({r_i: None, r_t: [-799.900 -799.900 -799.900], eps: 0.1})
Step:  177800, Reward: [-411.913 -411.913 -411.913] [70.138], Avg: [-405.587 -405.587 -405.587] (0.1000) ({r_i: None, r_t: [-790.943 -790.943 -790.943], eps: 0.1})
Step:  177900, Reward: [-388.182 -388.182 -388.182] [53.956], Avg: [-405.577 -405.577 -405.577] (0.1000) ({r_i: None, r_t: [-797.186 -797.186 -797.186], eps: 0.1})
Step:  178000, Reward: [-396.000 -396.000 -396.000] [52.965], Avg: [-405.572 -405.572 -405.572] (0.1000) ({r_i: None, r_t: [-784.864 -784.864 -784.864], eps: 0.1})
Step:  178100, Reward: [-374.799 -374.799 -374.799] [60.250], Avg: [-405.555 -405.555 -405.555] (0.1000) ({r_i: None, r_t: [-746.779 -746.779 -746.779], eps: 0.1})
Step:  178200, Reward: [-408.352 -408.352 -408.352] [93.033], Avg: [-405.556 -405.556 -405.556] (0.1000) ({r_i: None, r_t: [-778.089 -778.089 -778.089], eps: 0.1})
Step:  178300, Reward: [-385.393 -385.393 -385.393] [43.973], Avg: [-405.545 -405.545 -405.545] (0.1000) ({r_i: None, r_t: [-805.746 -805.746 -805.746], eps: 0.1})
Step:  178400, Reward: [-430.340 -430.340 -430.340] [103.247], Avg: [-405.559 -405.559 -405.559] (0.1000) ({r_i: None, r_t: [-817.827 -817.827 -817.827], eps: 0.1})
Step:  178500, Reward: [-413.799 -413.799 -413.799] [68.822], Avg: [-405.564 -405.564 -405.564] (0.1000) ({r_i: None, r_t: [-795.434 -795.434 -795.434], eps: 0.1})
Step:  178600, Reward: [-385.634 -385.634 -385.634] [88.476], Avg: [-405.552 -405.552 -405.552] (0.1000) ({r_i: None, r_t: [-830.481 -830.481 -830.481], eps: 0.1})
Step:  178700, Reward: [-394.748 -394.748 -394.748] [71.330], Avg: [-405.546 -405.546 -405.546] (0.1000) ({r_i: None, r_t: [-829.590 -829.590 -829.590], eps: 0.1})
Step:  178800, Reward: [-375.770 -375.770 -375.770] [53.454], Avg: [-405.530 -405.530 -405.530] (0.1000) ({r_i: None, r_t: [-800.533 -800.533 -800.533], eps: 0.1})
Step:  178900, Reward: [-403.750 -403.750 -403.750] [77.413], Avg: [-405.529 -405.529 -405.529] (0.1000) ({r_i: None, r_t: [-773.284 -773.284 -773.284], eps: 0.1})
Step:  179000, Reward: [-397.706 -397.706 -397.706] [81.616], Avg: [-405.524 -405.524 -405.524] (0.1000) ({r_i: None, r_t: [-804.621 -804.621 -804.621], eps: 0.1})
Step:  179100, Reward: [-396.409 -396.409 -396.409] [55.918], Avg: [-405.519 -405.519 -405.519] (0.1000) ({r_i: None, r_t: [-811.502 -811.502 -811.502], eps: 0.1})
Step:  179200, Reward: [-422.853 -422.853 -422.853] [120.402], Avg: [-405.529 -405.529 -405.529] (0.1000) ({r_i: None, r_t: [-773.824 -773.824 -773.824], eps: 0.1})
Step:  179300, Reward: [-418.451 -418.451 -418.451] [78.558], Avg: [-405.536 -405.536 -405.536] (0.1000) ({r_i: None, r_t: [-802.057 -802.057 -802.057], eps: 0.1})
Step:  179400, Reward: [-420.478 -420.478 -420.478] [63.656], Avg: [-405.544 -405.544 -405.544] (0.1000) ({r_i: None, r_t: [-823.899 -823.899 -823.899], eps: 0.1})
Step:  179500, Reward: [-366.601 -366.601 -366.601] [46.706], Avg: [-405.523 -405.523 -405.523] (0.1000) ({r_i: None, r_t: [-845.448 -845.448 -845.448], eps: 0.1})
Step:  179600, Reward: [-406.829 -406.829 -406.829] [58.856], Avg: [-405.523 -405.523 -405.523] (0.1000) ({r_i: None, r_t: [-783.002 -783.002 -783.002], eps: 0.1})
Step:  179700, Reward: [-379.362 -379.362 -379.362] [58.846], Avg: [-405.509 -405.509 -405.509] (0.1000) ({r_i: None, r_t: [-805.813 -805.813 -805.813], eps: 0.1})
Step:  179800, Reward: [-412.424 -412.424 -412.424] [69.080], Avg: [-405.513 -405.513 -405.513] (0.1000) ({r_i: None, r_t: [-826.960 -826.960 -826.960], eps: 0.1})
Step:  179900, Reward: [-377.411 -377.411 -377.411] [62.326], Avg: [-405.497 -405.497 -405.497] (0.1000) ({r_i: None, r_t: [-789.489 -789.489 -789.489], eps: 0.1})
Step:  180000, Reward: [-390.211 -390.211 -390.211] [57.343], Avg: [-405.489 -405.489 -405.489] (0.1000) ({r_i: None, r_t: [-803.897 -803.897 -803.897], eps: 0.1})
Step:  180100, Reward: [-392.627 -392.627 -392.627] [52.962], Avg: [-405.482 -405.482 -405.482] (0.1000) ({r_i: None, r_t: [-757.947 -757.947 -757.947], eps: 0.1})
Step:  180200, Reward: [-405.179 -405.179 -405.179] [81.998], Avg: [-405.481 -405.481 -405.481] (0.1000) ({r_i: None, r_t: [-825.651 -825.651 -825.651], eps: 0.1})
Step:  180300, Reward: [-418.610 -418.610 -418.610] [78.339], Avg: [-405.489 -405.489 -405.489] (0.1000) ({r_i: None, r_t: [-792.153 -792.153 -792.153], eps: 0.1})
Step:  180400, Reward: [-397.790 -397.790 -397.790] [67.608], Avg: [-405.484 -405.484 -405.484] (0.1000) ({r_i: None, r_t: [-724.654 -724.654 -724.654], eps: 0.1})
Step:  180500, Reward: [-422.481 -422.481 -422.481] [71.119], Avg: [-405.494 -405.494 -405.494] (0.1000) ({r_i: None, r_t: [-809.218 -809.218 -809.218], eps: 0.1})
Step:  180600, Reward: [-390.567 -390.567 -390.567] [63.862], Avg: [-405.486 -405.486 -405.486] (0.1000) ({r_i: None, r_t: [-779.672 -779.672 -779.672], eps: 0.1})
Step:  180700, Reward: [-389.593 -389.593 -389.593] [56.709], Avg: [-405.477 -405.477 -405.477] (0.1000) ({r_i: None, r_t: [-790.560 -790.560 -790.560], eps: 0.1})
Step:  180800, Reward: [-434.861 -434.861 -434.861] [81.531], Avg: [-405.493 -405.493 -405.493] (0.1000) ({r_i: None, r_t: [-762.157 -762.157 -762.157], eps: 0.1})
Step:  180900, Reward: [-411.966 -411.966 -411.966] [80.656], Avg: [-405.497 -405.497 -405.497] (0.1000) ({r_i: None, r_t: [-786.006 -786.006 -786.006], eps: 0.1})
Step:  181000, Reward: [-391.195 -391.195 -391.195] [45.736], Avg: [-405.489 -405.489 -405.489] (0.1000) ({r_i: None, r_t: [-753.498 -753.498 -753.498], eps: 0.1})
Step:  181100, Reward: [-383.609 -383.609 -383.609] [64.627], Avg: [-405.477 -405.477 -405.477] (0.1000) ({r_i: None, r_t: [-807.370 -807.370 -807.370], eps: 0.1})
Step:  181200, Reward: [-371.991 -371.991 -371.991] [55.848], Avg: [-405.458 -405.458 -405.458] (0.1000) ({r_i: None, r_t: [-845.463 -845.463 -845.463], eps: 0.1})
Step:  181300, Reward: [-398.985 -398.985 -398.985] [58.954], Avg: [-405.455 -405.455 -405.455] (0.1000) ({r_i: None, r_t: [-805.053 -805.053 -805.053], eps: 0.1})
Step:  181400, Reward: [-391.490 -391.490 -391.490] [57.786], Avg: [-405.447 -405.447 -405.447] (0.1000) ({r_i: None, r_t: [-816.365 -816.365 -816.365], eps: 0.1})
Step:  181500, Reward: [-401.834 -401.834 -401.834] [55.697], Avg: [-405.445 -405.445 -405.445] (0.1000) ({r_i: None, r_t: [-812.885 -812.885 -812.885], eps: 0.1})
Step:  181600, Reward: [-409.907 -409.907 -409.907] [90.691], Avg: [-405.447 -405.447 -405.447] (0.1000) ({r_i: None, r_t: [-815.520 -815.520 -815.520], eps: 0.1})
Step:  181700, Reward: [-416.087 -416.087 -416.087] [68.559], Avg: [-405.453 -405.453 -405.453] (0.1000) ({r_i: None, r_t: [-831.431 -831.431 -831.431], eps: 0.1})
Step:  181800, Reward: [-391.101 -391.101 -391.101] [53.301], Avg: [-405.445 -405.445 -405.445] (0.1000) ({r_i: None, r_t: [-842.677 -842.677 -842.677], eps: 0.1})
Step:  181900, Reward: [-386.057 -386.057 -386.057] [50.770], Avg: [-405.435 -405.435 -405.435] (0.1000) ({r_i: None, r_t: [-772.631 -772.631 -772.631], eps: 0.1})
Step:  182000, Reward: [-407.189 -407.189 -407.189] [96.055], Avg: [-405.436 -405.436 -405.436] (0.1000) ({r_i: None, r_t: [-791.054 -791.054 -791.054], eps: 0.1})
Step:  182100, Reward: [-372.937 -372.937 -372.937] [58.853], Avg: [-405.418 -405.418 -405.418] (0.1000) ({r_i: None, r_t: [-766.027 -766.027 -766.027], eps: 0.1})
Step:  182200, Reward: [-419.388 -419.388 -419.388] [70.128], Avg: [-405.425 -405.425 -405.425] (0.1000) ({r_i: None, r_t: [-815.410 -815.410 -815.410], eps: 0.1})
Step:  182300, Reward: [-370.464 -370.464 -370.464] [34.618], Avg: [-405.406 -405.406 -405.406] (0.1000) ({r_i: None, r_t: [-759.826 -759.826 -759.826], eps: 0.1})
Step:  182400, Reward: [-392.429 -392.429 -392.429] [42.729], Avg: [-405.399 -405.399 -405.399] (0.1000) ({r_i: None, r_t: [-735.442 -735.442 -735.442], eps: 0.1})
Step:  182500, Reward: [-405.646 -405.646 -405.646] [65.262], Avg: [-405.399 -405.399 -405.399] (0.1000) ({r_i: None, r_t: [-764.547 -764.547 -764.547], eps: 0.1})
Step:  182600, Reward: [-380.133 -380.133 -380.133] [64.214], Avg: [-405.385 -405.385 -405.385] (0.1000) ({r_i: None, r_t: [-770.639 -770.639 -770.639], eps: 0.1})
Step:  182700, Reward: [-396.635 -396.635 -396.635] [54.979], Avg: [-405.381 -405.381 -405.381] (0.1000) ({r_i: None, r_t: [-796.307 -796.307 -796.307], eps: 0.1})
Step:  182800, Reward: [-403.250 -403.250 -403.250] [55.701], Avg: [-405.379 -405.379 -405.379] (0.1000) ({r_i: None, r_t: [-799.901 -799.901 -799.901], eps: 0.1})
Step:  182900, Reward: [-385.185 -385.185 -385.185] [69.524], Avg: [-405.368 -405.368 -405.368] (0.1000) ({r_i: None, r_t: [-766.896 -766.896 -766.896], eps: 0.1})
Step:  183000, Reward: [-401.106 -401.106 -401.106] [75.541], Avg: [-405.366 -405.366 -405.366] (0.1000) ({r_i: None, r_t: [-765.322 -765.322 -765.322], eps: 0.1})
Step:  183100, Reward: [-400.645 -400.645 -400.645] [59.167], Avg: [-405.364 -405.364 -405.364] (0.1000) ({r_i: None, r_t: [-818.592 -818.592 -818.592], eps: 0.1})
Step:  183200, Reward: [-380.811 -380.811 -380.811] [76.029], Avg: [-405.350 -405.350 -405.350] (0.1000) ({r_i: None, r_t: [-804.990 -804.990 -804.990], eps: 0.1})
Step:  183300, Reward: [-384.936 -384.936 -384.936] [59.692], Avg: [-405.339 -405.339 -405.339] (0.1000) ({r_i: None, r_t: [-775.491 -775.491 -775.491], eps: 0.1})
Step:  183400, Reward: [-396.421 -396.421 -396.421] [42.448], Avg: [-405.334 -405.334 -405.334] (0.1000) ({r_i: None, r_t: [-762.486 -762.486 -762.486], eps: 0.1})
Step:  183500, Reward: [-386.620 -386.620 -386.620] [67.299], Avg: [-405.324 -405.324 -405.324] (0.1000) ({r_i: None, r_t: [-781.564 -781.564 -781.564], eps: 0.1})
Step:  183600, Reward: [-393.450 -393.450 -393.450] [73.538], Avg: [-405.318 -405.318 -405.318] (0.1000) ({r_i: None, r_t: [-789.999 -789.999 -789.999], eps: 0.1})
Step:  183700, Reward: [-370.958 -370.958 -370.958] [75.615], Avg: [-405.299 -405.299 -405.299] (0.1000) ({r_i: None, r_t: [-778.578 -778.578 -778.578], eps: 0.1})
Step:  183800, Reward: [-369.432 -369.432 -369.432] [46.286], Avg: [-405.279 -405.279 -405.279] (0.1000) ({r_i: None, r_t: [-805.760 -805.760 -805.760], eps: 0.1})
Step:  183900, Reward: [-401.592 -401.592 -401.592] [62.147], Avg: [-405.277 -405.277 -405.277] (0.1000) ({r_i: None, r_t: [-819.408 -819.408 -819.408], eps: 0.1})
Step:  184000, Reward: [-369.091 -369.091 -369.091] [57.905], Avg: [-405.258 -405.258 -405.258] (0.1000) ({r_i: None, r_t: [-783.275 -783.275 -783.275], eps: 0.1})
Step:  184100, Reward: [-377.369 -377.369 -377.369] [52.563], Avg: [-405.243 -405.243 -405.243] (0.1000) ({r_i: None, r_t: [-792.405 -792.405 -792.405], eps: 0.1})
Step:  184200, Reward: [-402.979 -402.979 -402.979] [74.111], Avg: [-405.241 -405.241 -405.241] (0.1000) ({r_i: None, r_t: [-835.273 -835.273 -835.273], eps: 0.1})
Step:  184300, Reward: [-385.243 -385.243 -385.243] [38.209], Avg: [-405.230 -405.230 -405.230] (0.1000) ({r_i: None, r_t: [-753.674 -753.674 -753.674], eps: 0.1})
Step:  184400, Reward: [-424.003 -424.003 -424.003] [71.835], Avg: [-405.241 -405.241 -405.241] (0.1000) ({r_i: None, r_t: [-807.060 -807.060 -807.060], eps: 0.1})
Step:  184500, Reward: [-390.471 -390.471 -390.471] [79.964], Avg: [-405.233 -405.233 -405.233] (0.1000) ({r_i: None, r_t: [-742.177 -742.177 -742.177], eps: 0.1})
Step:  184600, Reward: [-401.938 -401.938 -401.938] [62.627], Avg: [-405.231 -405.231 -405.231] (0.1000) ({r_i: None, r_t: [-779.152 -779.152 -779.152], eps: 0.1})
Step:  184700, Reward: [-396.883 -396.883 -396.883] [69.627], Avg: [-405.226 -405.226 -405.226] (0.1000) ({r_i: None, r_t: [-826.909 -826.909 -826.909], eps: 0.1})
Step:  184800, Reward: [-396.349 -396.349 -396.349] [69.357], Avg: [-405.222 -405.222 -405.222] (0.1000) ({r_i: None, r_t: [-823.445 -823.445 -823.445], eps: 0.1})
Step:  184900, Reward: [-411.150 -411.150 -411.150] [64.028], Avg: [-405.225 -405.225 -405.225] (0.1000) ({r_i: None, r_t: [-810.410 -810.410 -810.410], eps: 0.1})
Step:  185000, Reward: [-418.247 -418.247 -418.247] [72.258], Avg: [-405.232 -405.232 -405.232] (0.1000) ({r_i: None, r_t: [-785.309 -785.309 -785.309], eps: 0.1})
Step:  185100, Reward: [-408.668 -408.668 -408.668] [81.904], Avg: [-405.234 -405.234 -405.234] (0.1000) ({r_i: None, r_t: [-787.033 -787.033 -787.033], eps: 0.1})
Step:  185200, Reward: [-401.728 -401.728 -401.728] [77.826], Avg: [-405.232 -405.232 -405.232] (0.1000) ({r_i: None, r_t: [-766.779 -766.779 -766.779], eps: 0.1})
Step:  185300, Reward: [-403.922 -403.922 -403.922] [83.744], Avg: [-405.231 -405.231 -405.231] (0.1000) ({r_i: None, r_t: [-805.561 -805.561 -805.561], eps: 0.1})
Step:  185400, Reward: [-405.038 -405.038 -405.038] [71.648], Avg: [-405.231 -405.231 -405.231] (0.1000) ({r_i: None, r_t: [-812.543 -812.543 -812.543], eps: 0.1})
Step:  185500, Reward: [-400.026 -400.026 -400.026] [71.818], Avg: [-405.228 -405.228 -405.228] (0.1000) ({r_i: None, r_t: [-813.617 -813.617 -813.617], eps: 0.1})
Step:  185600, Reward: [-382.493 -382.493 -382.493] [70.600], Avg: [-405.216 -405.216 -405.216] (0.1000) ({r_i: None, r_t: [-788.452 -788.452 -788.452], eps: 0.1})
Step:  185700, Reward: [-418.658 -418.658 -418.658] [100.813], Avg: [-405.223 -405.223 -405.223] (0.1000) ({r_i: None, r_t: [-795.471 -795.471 -795.471], eps: 0.1})
Step:  185800, Reward: [-426.233 -426.233 -426.233] [70.988], Avg: [-405.234 -405.234 -405.234] (0.1000) ({r_i: None, r_t: [-788.987 -788.987 -788.987], eps: 0.1})
Step:  185900, Reward: [-399.843 -399.843 -399.843] [64.966], Avg: [-405.231 -405.231 -405.231] (0.1000) ({r_i: None, r_t: [-795.177 -795.177 -795.177], eps: 0.1})
Step:  186000, Reward: [-382.377 -382.377 -382.377] [66.597], Avg: [-405.219 -405.219 -405.219] (0.1000) ({r_i: None, r_t: [-802.488 -802.488 -802.488], eps: 0.1})
Step:  186100, Reward: [-405.386 -405.386 -405.386] [59.746], Avg: [-405.219 -405.219 -405.219] (0.1000) ({r_i: None, r_t: [-798.047 -798.047 -798.047], eps: 0.1})
Step:  186200, Reward: [-363.677 -363.677 -363.677] [61.055], Avg: [-405.197 -405.197 -405.197] (0.1000) ({r_i: None, r_t: [-801.224 -801.224 -801.224], eps: 0.1})
Step:  186300, Reward: [-408.701 -408.701 -408.701] [57.418], Avg: [-405.199 -405.199 -405.199] (0.1000) ({r_i: None, r_t: [-800.220 -800.220 -800.220], eps: 0.1})
Step:  186400, Reward: [-365.837 -365.837 -365.837] [75.984], Avg: [-405.178 -405.178 -405.178] (0.1000) ({r_i: None, r_t: [-814.321 -814.321 -814.321], eps: 0.1})
Step:  186500, Reward: [-402.438 -402.438 -402.438] [72.271], Avg: [-405.176 -405.176 -405.176] (0.1000) ({r_i: None, r_t: [-783.389 -783.389 -783.389], eps: 0.1})
Step:  186600, Reward: [-413.952 -413.952 -413.952] [71.621], Avg: [-405.181 -405.181 -405.181] (0.1000) ({r_i: None, r_t: [-823.992 -823.992 -823.992], eps: 0.1})
Step:  186700, Reward: [-399.272 -399.272 -399.272] [68.095], Avg: [-405.178 -405.178 -405.178] (0.1000) ({r_i: None, r_t: [-810.620 -810.620 -810.620], eps: 0.1})
Step:  186800, Reward: [-416.876 -416.876 -416.876] [61.260], Avg: [-405.184 -405.184 -405.184] (0.1000) ({r_i: None, r_t: [-854.542 -854.542 -854.542], eps: 0.1})
Step:  186900, Reward: [-412.835 -412.835 -412.835] [38.477], Avg: [-405.188 -405.188 -405.188] (0.1000) ({r_i: None, r_t: [-766.048 -766.048 -766.048], eps: 0.1})
Step:  187000, Reward: [-391.296 -391.296 -391.296] [58.079], Avg: [-405.181 -405.181 -405.181] (0.1000) ({r_i: None, r_t: [-858.761 -858.761 -858.761], eps: 0.1})
Step:  187100, Reward: [-386.594 -386.594 -386.594] [56.835], Avg: [-405.171 -405.171 -405.171] (0.1000) ({r_i: None, r_t: [-788.519 -788.519 -788.519], eps: 0.1})
Step:  187200, Reward: [-412.293 -412.293 -412.293] [92.806], Avg: [-405.175 -405.175 -405.175] (0.1000) ({r_i: None, r_t: [-785.184 -785.184 -785.184], eps: 0.1})
Step:  187300, Reward: [-415.795 -415.795 -415.795] [98.560], Avg: [-405.180 -405.180 -405.180] (0.1000) ({r_i: None, r_t: [-761.360 -761.360 -761.360], eps: 0.1})
Step:  187400, Reward: [-426.765 -426.765 -426.765] [100.892], Avg: [-405.192 -405.192 -405.192] (0.1000) ({r_i: None, r_t: [-782.660 -782.660 -782.660], eps: 0.1})
Step:  187500, Reward: [-388.605 -388.605 -388.605] [59.510], Avg: [-405.183 -405.183 -405.183] (0.1000) ({r_i: None, r_t: [-774.415 -774.415 -774.415], eps: 0.1})
Step:  187600, Reward: [-391.514 -391.514 -391.514] [82.418], Avg: [-405.176 -405.176 -405.176] (0.1000) ({r_i: None, r_t: [-807.243 -807.243 -807.243], eps: 0.1})
Step:  187700, Reward: [-386.818 -386.818 -386.818] [75.812], Avg: [-405.166 -405.166 -405.166] (0.1000) ({r_i: None, r_t: [-845.617 -845.617 -845.617], eps: 0.1})
Step:  187800, Reward: [-383.075 -383.075 -383.075] [54.964], Avg: [-405.154 -405.154 -405.154] (0.1000) ({r_i: None, r_t: [-807.734 -807.734 -807.734], eps: 0.1})
Step:  187900, Reward: [-398.085 -398.085 -398.085] [59.630], Avg: [-405.150 -405.150 -405.150] (0.1000) ({r_i: None, r_t: [-797.781 -797.781 -797.781], eps: 0.1})
Step:  188000, Reward: [-418.584 -418.584 -418.584] [66.348], Avg: [-405.158 -405.158 -405.158] (0.1000) ({r_i: None, r_t: [-791.698 -791.698 -791.698], eps: 0.1})
Step:  188100, Reward: [-414.016 -414.016 -414.016] [58.723], Avg: [-405.162 -405.162 -405.162] (0.1000) ({r_i: None, r_t: [-776.070 -776.070 -776.070], eps: 0.1})
Step:  188200, Reward: [-411.240 -411.240 -411.240] [94.177], Avg: [-405.165 -405.165 -405.165] (0.1000) ({r_i: None, r_t: [-775.396 -775.396 -775.396], eps: 0.1})
Step:  188300, Reward: [-390.046 -390.046 -390.046] [77.405], Avg: [-405.157 -405.157 -405.157] (0.1000) ({r_i: None, r_t: [-791.220 -791.220 -791.220], eps: 0.1})
Step:  188400, Reward: [-403.288 -403.288 -403.288] [61.628], Avg: [-405.156 -405.156 -405.156] (0.1000) ({r_i: None, r_t: [-816.764 -816.764 -816.764], eps: 0.1})
Step:  188500, Reward: [-383.052 -383.052 -383.052] [58.942], Avg: [-405.145 -405.145 -405.145] (0.1000) ({r_i: None, r_t: [-799.508 -799.508 -799.508], eps: 0.1})
Step:  188600, Reward: [-407.842 -407.842 -407.842] [74.253], Avg: [-405.146 -405.146 -405.146] (0.1000) ({r_i: None, r_t: [-785.000 -785.000 -785.000], eps: 0.1})
Step:  188700, Reward: [-412.960 -412.960 -412.960] [50.257], Avg: [-405.150 -405.150 -405.150] (0.1000) ({r_i: None, r_t: [-809.549 -809.549 -809.549], eps: 0.1})
Step:  188800, Reward: [-378.772 -378.772 -378.772] [69.375], Avg: [-405.136 -405.136 -405.136] (0.1000) ({r_i: None, r_t: [-776.861 -776.861 -776.861], eps: 0.1})
Step:  188900, Reward: [-377.074 -377.074 -377.074] [53.844], Avg: [-405.122 -405.122 -405.122] (0.1000) ({r_i: None, r_t: [-806.675 -806.675 -806.675], eps: 0.1})
Step:  189000, Reward: [-389.322 -389.322 -389.322] [76.114], Avg: [-405.113 -405.113 -405.113] (0.1000) ({r_i: None, r_t: [-784.928 -784.928 -784.928], eps: 0.1})
Step:  189100, Reward: [-400.482 -400.482 -400.482] [61.495], Avg: [-405.111 -405.111 -405.111] (0.1000) ({r_i: None, r_t: [-789.137 -789.137 -789.137], eps: 0.1})
Step:  189200, Reward: [-384.113 -384.113 -384.113] [79.094], Avg: [-405.100 -405.100 -405.100] (0.1000) ({r_i: None, r_t: [-797.277 -797.277 -797.277], eps: 0.1})
Step:  189300, Reward: [-393.533 -393.533 -393.533] [73.018], Avg: [-405.094 -405.094 -405.094] (0.1000) ({r_i: None, r_t: [-788.167 -788.167 -788.167], eps: 0.1})
Step:  189400, Reward: [-403.916 -403.916 -403.916] [53.697], Avg: [-405.093 -405.093 -405.093] (0.1000) ({r_i: None, r_t: [-787.993 -787.993 -787.993], eps: 0.1})
Step:  189500, Reward: [-392.552 -392.552 -392.552] [88.330], Avg: [-405.086 -405.086 -405.086] (0.1000) ({r_i: None, r_t: [-797.432 -797.432 -797.432], eps: 0.1})
Step:  189600, Reward: [-393.891 -393.891 -393.891] [67.175], Avg: [-405.080 -405.080 -405.080] (0.1000) ({r_i: None, r_t: [-749.825 -749.825 -749.825], eps: 0.1})
Step:  189700, Reward: [-396.187 -396.187 -396.187] [74.791], Avg: [-405.076 -405.076 -405.076] (0.1000) ({r_i: None, r_t: [-832.865 -832.865 -832.865], eps: 0.1})
Step:  189800, Reward: [-445.668 -445.668 -445.668] [72.128], Avg: [-405.097 -405.097 -405.097] (0.1000) ({r_i: None, r_t: [-727.794 -727.794 -727.794], eps: 0.1})
Step:  189900, Reward: [-424.599 -424.599 -424.599] [81.051], Avg: [-405.107 -405.107 -405.107] (0.1000) ({r_i: None, r_t: [-750.642 -750.642 -750.642], eps: 0.1})
Step:  190000, Reward: [-427.802 -427.802 -427.802] [75.977], Avg: [-405.119 -405.119 -405.119] (0.1000) ({r_i: None, r_t: [-788.960 -788.960 -788.960], eps: 0.1})
Step:  190100, Reward: [-369.514 -369.514 -369.514] [83.721], Avg: [-405.101 -405.101 -405.101] (0.1000) ({r_i: None, r_t: [-796.855 -796.855 -796.855], eps: 0.1})
Step:  190200, Reward: [-401.607 -401.607 -401.607] [63.826], Avg: [-405.099 -405.099 -405.099] (0.1000) ({r_i: None, r_t: [-822.964 -822.964 -822.964], eps: 0.1})
Step:  190300, Reward: [-425.064 -425.064 -425.064] [77.668], Avg: [-405.109 -405.109 -405.109] (0.1000) ({r_i: None, r_t: [-756.006 -756.006 -756.006], eps: 0.1})
Step:  190400, Reward: [-401.979 -401.979 -401.979] [63.370], Avg: [-405.108 -405.108 -405.108] (0.1000) ({r_i: None, r_t: [-804.088 -804.088 -804.088], eps: 0.1})
Step:  190500, Reward: [-376.539 -376.539 -376.539] [68.212], Avg: [-405.093 -405.093 -405.093] (0.1000) ({r_i: None, r_t: [-801.834 -801.834 -801.834], eps: 0.1})
Step:  190600, Reward: [-408.689 -408.689 -408.689] [87.486], Avg: [-405.094 -405.094 -405.094] (0.1000) ({r_i: None, r_t: [-803.447 -803.447 -803.447], eps: 0.1})
Step:  190700, Reward: [-385.027 -385.027 -385.027] [48.040], Avg: [-405.084 -405.084 -405.084] (0.1000) ({r_i: None, r_t: [-774.373 -774.373 -774.373], eps: 0.1})
Step:  190800, Reward: [-376.878 -376.878 -376.878] [60.215], Avg: [-405.069 -405.069 -405.069] (0.1000) ({r_i: None, r_t: [-791.610 -791.610 -791.610], eps: 0.1})
Step:  190900, Reward: [-363.426 -363.426 -363.426] [41.304], Avg: [-405.047 -405.047 -405.047] (0.1000) ({r_i: None, r_t: [-835.169 -835.169 -835.169], eps: 0.1})
Step:  191000, Reward: [-400.542 -400.542 -400.542] [84.575], Avg: [-405.045 -405.045 -405.045] (0.1000) ({r_i: None, r_t: [-793.620 -793.620 -793.620], eps: 0.1})
Step:  191100, Reward: [-405.225 -405.225 -405.225] [56.447], Avg: [-405.045 -405.045 -405.045] (0.1000) ({r_i: None, r_t: [-810.490 -810.490 -810.490], eps: 0.1})
Step:  191200, Reward: [-411.825 -411.825 -411.825] [53.079], Avg: [-405.049 -405.049 -405.049] (0.1000) ({r_i: None, r_t: [-770.371 -770.371 -770.371], eps: 0.1})
Step:  191300, Reward: [-387.193 -387.193 -387.193] [52.517], Avg: [-405.039 -405.039 -405.039] (0.1000) ({r_i: None, r_t: [-800.953 -800.953 -800.953], eps: 0.1})
Step:  191400, Reward: [-372.307 -372.307 -372.307] [50.139], Avg: [-405.022 -405.022 -405.022] (0.1000) ({r_i: None, r_t: [-784.054 -784.054 -784.054], eps: 0.1})
Step:  191500, Reward: [-383.958 -383.958 -383.958] [64.298], Avg: [-405.011 -405.011 -405.011] (0.1000) ({r_i: None, r_t: [-792.107 -792.107 -792.107], eps: 0.1})
Step:  191600, Reward: [-392.660 -392.660 -392.660] [80.592], Avg: [-405.005 -405.005 -405.005] (0.1000) ({r_i: None, r_t: [-852.089 -852.089 -852.089], eps: 0.1})
Step:  191700, Reward: [-398.561 -398.561 -398.561] [72.731], Avg: [-405.001 -405.001 -405.001] (0.1000) ({r_i: None, r_t: [-823.108 -823.108 -823.108], eps: 0.1})
Step:  191800, Reward: [-425.255 -425.255 -425.255] [67.117], Avg: [-405.012 -405.012 -405.012] (0.1000) ({r_i: None, r_t: [-799.476 -799.476 -799.476], eps: 0.1})
Step:  191900, Reward: [-385.697 -385.697 -385.697] [89.210], Avg: [-405.002 -405.002 -405.002] (0.1000) ({r_i: None, r_t: [-770.573 -770.573 -770.573], eps: 0.1})
Step:  192000, Reward: [-420.742 -420.742 -420.742] [72.838], Avg: [-405.010 -405.010 -405.010] (0.1000) ({r_i: None, r_t: [-854.229 -854.229 -854.229], eps: 0.1})
Step:  192100, Reward: [-383.461 -383.461 -383.461] [76.159], Avg: [-404.999 -404.999 -404.999] (0.1000) ({r_i: None, r_t: [-839.106 -839.106 -839.106], eps: 0.1})
Step:  192200, Reward: [-436.478 -436.478 -436.478] [82.386], Avg: [-405.015 -405.015 -405.015] (0.1000) ({r_i: None, r_t: [-858.312 -858.312 -858.312], eps: 0.1})
Step:  192300, Reward: [-418.013 -418.013 -418.013] [50.043], Avg: [-405.022 -405.022 -405.022] (0.1000) ({r_i: None, r_t: [-802.193 -802.193 -802.193], eps: 0.1})
Step:  192400, Reward: [-399.837 -399.837 -399.837] [77.357], Avg: [-405.019 -405.019 -405.019] (0.1000) ({r_i: None, r_t: [-812.481 -812.481 -812.481], eps: 0.1})
Step:  192500, Reward: [-390.855 -390.855 -390.855] [90.616], Avg: [-405.012 -405.012 -405.012] (0.1000) ({r_i: None, r_t: [-806.898 -806.898 -806.898], eps: 0.1})
Step:  192600, Reward: [-413.950 -413.950 -413.950] [73.762], Avg: [-405.017 -405.017 -405.017] (0.1000) ({r_i: None, r_t: [-821.355 -821.355 -821.355], eps: 0.1})
Step:  192700, Reward: [-428.121 -428.121 -428.121] [115.928], Avg: [-405.029 -405.029 -405.029] (0.1000) ({r_i: None, r_t: [-793.462 -793.462 -793.462], eps: 0.1})
Step:  192800, Reward: [-443.829 -443.829 -443.829] [71.712], Avg: [-405.049 -405.049 -405.049] (0.1000) ({r_i: None, r_t: [-784.586 -784.586 -784.586], eps: 0.1})
Step:  192900, Reward: [-381.222 -381.222 -381.222] [64.370], Avg: [-405.036 -405.036 -405.036] (0.1000) ({r_i: None, r_t: [-831.239 -831.239 -831.239], eps: 0.1})
Step:  193000, Reward: [-397.116 -397.116 -397.116] [63.941], Avg: [-405.032 -405.032 -405.032] (0.1000) ({r_i: None, r_t: [-816.005 -816.005 -816.005], eps: 0.1})
Step:  193100, Reward: [-438.101 -438.101 -438.101] [76.789], Avg: [-405.049 -405.049 -405.049] (0.1000) ({r_i: None, r_t: [-828.739 -828.739 -828.739], eps: 0.1})
Step:  193200, Reward: [-412.479 -412.479 -412.479] [64.266], Avg: [-405.053 -405.053 -405.053] (0.1000) ({r_i: None, r_t: [-779.770 -779.770 -779.770], eps: 0.1})
Step:  193300, Reward: [-429.557 -429.557 -429.557] [69.888], Avg: [-405.066 -405.066 -405.066] (0.1000) ({r_i: None, r_t: [-788.822 -788.822 -788.822], eps: 0.1})
Step:  193400, Reward: [-428.738 -428.738 -428.738] [74.649], Avg: [-405.078 -405.078 -405.078] (0.1000) ({r_i: None, r_t: [-796.406 -796.406 -796.406], eps: 0.1})
Step:  193500, Reward: [-417.478 -417.478 -417.478] [57.791], Avg: [-405.085 -405.085 -405.085] (0.1000) ({r_i: None, r_t: [-842.278 -842.278 -842.278], eps: 0.1})
Step:  193600, Reward: [-389.280 -389.280 -389.280] [56.038], Avg: [-405.076 -405.076 -405.076] (0.1000) ({r_i: None, r_t: [-787.864 -787.864 -787.864], eps: 0.1})
Step:  193700, Reward: [-416.454 -416.454 -416.454] [89.340], Avg: [-405.082 -405.082 -405.082] (0.1000) ({r_i: None, r_t: [-783.026 -783.026 -783.026], eps: 0.1})
Step:  193800, Reward: [-392.365 -392.365 -392.365] [68.079], Avg: [-405.076 -405.076 -405.076] (0.1000) ({r_i: None, r_t: [-794.456 -794.456 -794.456], eps: 0.1})
Step:  193900, Reward: [-432.232 -432.232 -432.232] [74.487], Avg: [-405.090 -405.090 -405.090] (0.1000) ({r_i: None, r_t: [-778.031 -778.031 -778.031], eps: 0.1})
Step:  194000, Reward: [-390.970 -390.970 -390.970] [73.790], Avg: [-405.082 -405.082 -405.082] (0.1000) ({r_i: None, r_t: [-829.038 -829.038 -829.038], eps: 0.1})
Step:  194100, Reward: [-400.052 -400.052 -400.052] [85.097], Avg: [-405.080 -405.080 -405.080] (0.1000) ({r_i: None, r_t: [-828.143 -828.143 -828.143], eps: 0.1})
Step:  194200, Reward: [-396.179 -396.179 -396.179] [71.890], Avg: [-405.075 -405.075 -405.075] (0.1000) ({r_i: None, r_t: [-808.273 -808.273 -808.273], eps: 0.1})
Step:  194300, Reward: [-396.121 -396.121 -396.121] [83.520], Avg: [-405.071 -405.071 -405.071] (0.1000) ({r_i: None, r_t: [-828.350 -828.350 -828.350], eps: 0.1})
Step:  194400, Reward: [-392.995 -392.995 -392.995] [76.461], Avg: [-405.064 -405.064 -405.064] (0.1000) ({r_i: None, r_t: [-757.396 -757.396 -757.396], eps: 0.1})
Step:  194500, Reward: [-400.785 -400.785 -400.785] [62.508], Avg: [-405.062 -405.062 -405.062] (0.1000) ({r_i: None, r_t: [-790.687 -790.687 -790.687], eps: 0.1})
Step:  194600, Reward: [-382.790 -382.790 -382.790] [41.834], Avg: [-405.051 -405.051 -405.051] (0.1000) ({r_i: None, r_t: [-793.280 -793.280 -793.280], eps: 0.1})
Step:  194700, Reward: [-428.513 -428.513 -428.513] [80.370], Avg: [-405.063 -405.063 -405.063] (0.1000) ({r_i: None, r_t: [-848.060 -848.060 -848.060], eps: 0.1})
Step:  194800, Reward: [-413.355 -413.355 -413.355] [72.383], Avg: [-405.067 -405.067 -405.067] (0.1000) ({r_i: None, r_t: [-811.472 -811.472 -811.472], eps: 0.1})
Step:  194900, Reward: [-389.691 -389.691 -389.691] [59.948], Avg: [-405.059 -405.059 -405.059] (0.1000) ({r_i: None, r_t: [-797.546 -797.546 -797.546], eps: 0.1})
Step:  195000, Reward: [-412.703 -412.703 -412.703] [94.490], Avg: [-405.063 -405.063 -405.063] (0.1000) ({r_i: None, r_t: [-786.916 -786.916 -786.916], eps: 0.1})
Step:  195100, Reward: [-426.041 -426.041 -426.041] [65.469], Avg: [-405.074 -405.074 -405.074] (0.1000) ({r_i: None, r_t: [-818.167 -818.167 -818.167], eps: 0.1})
Step:  195200, Reward: [-416.657 -416.657 -416.657] [48.490], Avg: [-405.080 -405.080 -405.080] (0.1000) ({r_i: None, r_t: [-825.955 -825.955 -825.955], eps: 0.1})
Step:  195300, Reward: [-385.102 -385.102 -385.102] [45.649], Avg: [-405.070 -405.070 -405.070] (0.1000) ({r_i: None, r_t: [-762.405 -762.405 -762.405], eps: 0.1})
Step:  195400, Reward: [-389.197 -389.197 -389.197] [52.338], Avg: [-405.061 -405.061 -405.061] (0.1000) ({r_i: None, r_t: [-784.710 -784.710 -784.710], eps: 0.1})
Step:  195500, Reward: [-421.195 -421.195 -421.195] [67.339], Avg: [-405.070 -405.070 -405.070] (0.1000) ({r_i: None, r_t: [-810.841 -810.841 -810.841], eps: 0.1})
Step:  195600, Reward: [-400.653 -400.653 -400.653] [73.311], Avg: [-405.067 -405.067 -405.067] (0.1000) ({r_i: None, r_t: [-765.798 -765.798 -765.798], eps: 0.1})
Step:  195700, Reward: [-438.585 -438.585 -438.585] [90.472], Avg: [-405.085 -405.085 -405.085] (0.1000) ({r_i: None, r_t: [-775.427 -775.427 -775.427], eps: 0.1})
Step:  195800, Reward: [-372.173 -372.173 -372.173] [63.814], Avg: [-405.068 -405.068 -405.068] (0.1000) ({r_i: None, r_t: [-784.506 -784.506 -784.506], eps: 0.1})
Step:  195900, Reward: [-360.650 -360.650 -360.650] [70.531], Avg: [-405.045 -405.045 -405.045] (0.1000) ({r_i: None, r_t: [-785.163 -785.163 -785.163], eps: 0.1})
Step:  196000, Reward: [-387.059 -387.059 -387.059] [57.901], Avg: [-405.036 -405.036 -405.036] (0.1000) ({r_i: None, r_t: [-797.498 -797.498 -797.498], eps: 0.1})
Step:  196100, Reward: [-427.822 -427.822 -427.822] [69.254], Avg: [-405.048 -405.048 -405.048] (0.1000) ({r_i: None, r_t: [-776.677 -776.677 -776.677], eps: 0.1})
Step:  196200, Reward: [-390.462 -390.462 -390.462] [77.123], Avg: [-405.040 -405.040 -405.040] (0.1000) ({r_i: None, r_t: [-821.074 -821.074 -821.074], eps: 0.1})
Step:  196300, Reward: [-414.905 -414.905 -414.905] [66.411], Avg: [-405.045 -405.045 -405.045] (0.1000) ({r_i: None, r_t: [-835.142 -835.142 -835.142], eps: 0.1})
Step:  196400, Reward: [-399.504 -399.504 -399.504] [63.457], Avg: [-405.042 -405.042 -405.042] (0.1000) ({r_i: None, r_t: [-765.628 -765.628 -765.628], eps: 0.1})
Step:  196500, Reward: [-395.120 -395.120 -395.120] [68.462], Avg: [-405.037 -405.037 -405.037] (0.1000) ({r_i: None, r_t: [-754.979 -754.979 -754.979], eps: 0.1})
Step:  196600, Reward: [-380.972 -380.972 -380.972] [56.415], Avg: [-405.025 -405.025 -405.025] (0.1000) ({r_i: None, r_t: [-774.790 -774.790 -774.790], eps: 0.1})
Step:  196700, Reward: [-393.807 -393.807 -393.807] [85.272], Avg: [-405.019 -405.019 -405.019] (0.1000) ({r_i: None, r_t: [-778.862 -778.862 -778.862], eps: 0.1})
Step:  196800, Reward: [-406.798 -406.798 -406.798] [70.087], Avg: [-405.020 -405.020 -405.020] (0.1000) ({r_i: None, r_t: [-808.334 -808.334 -808.334], eps: 0.1})
Step:  196900, Reward: [-381.346 -381.346 -381.346] [53.065], Avg: [-405.008 -405.008 -405.008] (0.1000) ({r_i: None, r_t: [-806.397 -806.397 -806.397], eps: 0.1})
Step:  197000, Reward: [-399.380 -399.380 -399.380] [53.032], Avg: [-405.005 -405.005 -405.005] (0.1000) ({r_i: None, r_t: [-728.253 -728.253 -728.253], eps: 0.1})
Step:  197100, Reward: [-368.355 -368.355 -368.355] [74.138], Avg: [-404.987 -404.987 -404.987] (0.1000) ({r_i: None, r_t: [-770.034 -770.034 -770.034], eps: 0.1})
Step:  197200, Reward: [-368.782 -368.782 -368.782] [67.099], Avg: [-404.968 -404.968 -404.968] (0.1000) ({r_i: None, r_t: [-771.843 -771.843 -771.843], eps: 0.1})
Step:  197300, Reward: [-386.779 -386.779 -386.779] [66.777], Avg: [-404.959 -404.959 -404.959] (0.1000) ({r_i: None, r_t: [-811.805 -811.805 -811.805], eps: 0.1})
Step:  197400, Reward: [-395.449 -395.449 -395.449] [63.707], Avg: [-404.954 -404.954 -404.954] (0.1000) ({r_i: None, r_t: [-776.475 -776.475 -776.475], eps: 0.1})
Step:  197500, Reward: [-410.732 -410.732 -410.732] [84.930], Avg: [-404.957 -404.957 -404.957] (0.1000) ({r_i: None, r_t: [-783.973 -783.973 -783.973], eps: 0.1})
Step:  197600, Reward: [-415.058 -415.058 -415.058] [92.968], Avg: [-404.962 -404.962 -404.962] (0.1000) ({r_i: None, r_t: [-752.494 -752.494 -752.494], eps: 0.1})
Step:  197700, Reward: [-385.004 -385.004 -385.004] [52.867], Avg: [-404.952 -404.952 -404.952] (0.1000) ({r_i: None, r_t: [-761.007 -761.007 -761.007], eps: 0.1})
Step:  197800, Reward: [-389.037 -389.037 -389.037] [43.653], Avg: [-404.944 -404.944 -404.944] (0.1000) ({r_i: None, r_t: [-808.828 -808.828 -808.828], eps: 0.1})
Step:  197900, Reward: [-399.412 -399.412 -399.412] [72.278], Avg: [-404.941 -404.941 -404.941] (0.1000) ({r_i: None, r_t: [-789.474 -789.474 -789.474], eps: 0.1})
Step:  198000, Reward: [-365.944 -365.944 -365.944] [58.257], Avg: [-404.922 -404.922 -404.922] (0.1000) ({r_i: None, r_t: [-808.463 -808.463 -808.463], eps: 0.1})
Step:  198100, Reward: [-380.361 -380.361 -380.361] [58.815], Avg: [-404.909 -404.909 -404.909] (0.1000) ({r_i: None, r_t: [-789.424 -789.424 -789.424], eps: 0.1})
Step:  198200, Reward: [-381.646 -381.646 -381.646] [75.852], Avg: [-404.898 -404.898 -404.898] (0.1000) ({r_i: None, r_t: [-755.732 -755.732 -755.732], eps: 0.1})
Step:  198300, Reward: [-386.409 -386.409 -386.409] [53.620], Avg: [-404.888 -404.888 -404.888] (0.1000) ({r_i: None, r_t: [-774.678 -774.678 -774.678], eps: 0.1})
Step:  198400, Reward: [-394.008 -394.008 -394.008] [72.001], Avg: [-404.883 -404.883 -404.883] (0.1000) ({r_i: None, r_t: [-783.535 -783.535 -783.535], eps: 0.1})
Step:  198500, Reward: [-352.893 -352.893 -352.893] [52.098], Avg: [-404.857 -404.857 -404.857] (0.1000) ({r_i: None, r_t: [-760.223 -760.223 -760.223], eps: 0.1})
Step:  198600, Reward: [-384.120 -384.120 -384.120] [47.328], Avg: [-404.846 -404.846 -404.846] (0.1000) ({r_i: None, r_t: [-782.745 -782.745 -782.745], eps: 0.1})
Step:  198700, Reward: [-396.347 -396.347 -396.347] [63.464], Avg: [-404.842 -404.842 -404.842] (0.1000) ({r_i: None, r_t: [-770.958 -770.958 -770.958], eps: 0.1})
Step:  198800, Reward: [-379.699 -379.699 -379.699] [59.734], Avg: [-404.829 -404.829 -404.829] (0.1000) ({r_i: None, r_t: [-800.231 -800.231 -800.231], eps: 0.1})
Step:  198900, Reward: [-371.540 -371.540 -371.540] [55.635], Avg: [-404.813 -404.813 -404.813] (0.1000) ({r_i: None, r_t: [-743.869 -743.869 -743.869], eps: 0.1})
Step:  199000, Reward: [-390.431 -390.431 -390.431] [50.935], Avg: [-404.805 -404.805 -404.805] (0.1000) ({r_i: None, r_t: [-769.127 -769.127 -769.127], eps: 0.1})
Step:  199100, Reward: [-362.577 -362.577 -362.577] [77.647], Avg: [-404.784 -404.784 -404.784] (0.1000) ({r_i: None, r_t: [-733.403 -733.403 -733.403], eps: 0.1})
Step:  199200, Reward: [-403.211 -403.211 -403.211] [64.184], Avg: [-404.783 -404.783 -404.783] (0.1000) ({r_i: None, r_t: [-744.580 -744.580 -744.580], eps: 0.1})
Step:  199300, Reward: [-377.999 -377.999 -377.999] [60.608], Avg: [-404.770 -404.770 -404.770] (0.1000) ({r_i: None, r_t: [-751.191 -751.191 -751.191], eps: 0.1})
Step:  199400, Reward: [-402.197 -402.197 -402.197] [68.984], Avg: [-404.769 -404.769 -404.769] (0.1000) ({r_i: None, r_t: [-710.621 -710.621 -710.621], eps: 0.1})
Step:  199500, Reward: [-368.593 -368.593 -368.593] [62.725], Avg: [-404.751 -404.751 -404.751] (0.1000) ({r_i: None, r_t: [-736.217 -736.217 -736.217], eps: 0.1})
Step:  199600, Reward: [-387.973 -387.973 -387.973] [51.573], Avg: [-404.742 -404.742 -404.742] (0.1000) ({r_i: None, r_t: [-804.003 -804.003 -804.003], eps: 0.1})
Step:  199700, Reward: [-385.189 -385.189 -385.189] [60.629], Avg: [-404.732 -404.732 -404.732] (0.1000) ({r_i: None, r_t: [-782.072 -782.072 -782.072], eps: 0.1})
Step:  199800, Reward: [-415.608 -415.608 -415.608] [65.867], Avg: [-404.738 -404.738 -404.738] (0.1000) ({r_i: None, r_t: [-737.215 -737.215 -737.215], eps: 0.1})
Step:  199900, Reward: [-387.053 -387.053 -387.053] [78.621], Avg: [-404.729 -404.729 -404.729] (0.1000) ({r_i: None, r_t: [-767.985 -767.985 -767.985], eps: 0.1})
Step:  200000, Reward: [-392.955 -392.955 -392.955] [50.617], Avg: [-404.723 -404.723 -404.723] (0.1000) ({r_i: None, r_t: [-753.009 -753.009 -753.009], eps: 0.1})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		# q_value = torch.zeros_like(q_next_value)
		# for t in reversed(range(rewards.size(1))):
		# 	q_value[:, t] = self.critic_local(critic_inputs[:,t])
		# 	q_taken = torch.gather(q_value[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
		# 	critic_error = (q_taken - q_target[:, t].detach())
		# 	critic_loss = critic_error.pow(2).mean()
		# 	self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		q_value = self.critic_local(critic_inputs)
		q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		critic_error = (q_taken - q_target.detach())
		critic_loss = critic_error.pow(2).mean()
		self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-481.029 -481.029 -481.029] [83.636], Avg: [-481.029 -481.029 -481.029] (1.0000) ({r_i: None, r_t: [-9.074 -9.074 -9.074], eps: 1.0})
Step:     100, Reward: [-508.134 -508.134 -508.134] [97.317], Avg: [-494.582 -494.582 -494.582] (0.9801) ({r_i: None, r_t: [-980.575 -980.575 -980.575], eps: 0.98})
Step:     200, Reward: [-487.748 -487.748 -487.748] [103.018], Avg: [-492.304 -492.304 -492.304] (0.9606) ({r_i: None, r_t: [-994.677 -994.677 -994.677], eps: 0.961})
Step:     300, Reward: [-465.182 -465.182 -465.182] [85.826], Avg: [-485.523 -485.523 -485.523] (0.9415) ({r_i: None, r_t: [-1005.042 -1005.042 -1005.042], eps: 0.941})
Step:     400, Reward: [-470.436 -470.436 -470.436] [86.208], Avg: [-482.506 -482.506 -482.506] (0.9227) ({r_i: None, r_t: [-985.636 -985.636 -985.636], eps: 0.923})
Step:     500, Reward: [-466.870 -466.870 -466.870] [83.892], Avg: [-479.900 -479.900 -479.900] (0.9044) ({r_i: None, r_t: [-978.298 -978.298 -978.298], eps: 0.904})
Step:     600, Reward: [-475.144 -475.144 -475.144] [146.083], Avg: [-479.220 -479.220 -479.220] (0.8864) ({r_i: None, r_t: [-1003.448 -1003.448 -1003.448], eps: 0.886})
Step:     700, Reward: [-488.111 -488.111 -488.111] [98.604], Avg: [-480.332 -480.332 -480.332] (0.8687) ({r_i: None, r_t: [-965.998 -965.998 -965.998], eps: 0.869})
Step:     800, Reward: [-457.520 -457.520 -457.520] [80.665], Avg: [-477.797 -477.797 -477.797] (0.8515) ({r_i: None, r_t: [-964.483 -964.483 -964.483], eps: 0.851})
Step:     900, Reward: [-493.822 -493.822 -493.822] [74.028], Avg: [-479.400 -479.400 -479.400] (0.8345) ({r_i: None, r_t: [-948.097 -948.097 -948.097], eps: 0.835})
Step:    1000, Reward: [-521.883 -521.883 -521.883] [94.942], Avg: [-483.262 -483.262 -483.262] (0.8179) ({r_i: None, r_t: [-945.512 -945.512 -945.512], eps: 0.818})
Step:    1100, Reward: [-493.665 -493.665 -493.665] [84.095], Avg: [-484.129 -484.129 -484.129] (0.8016) ({r_i: None, r_t: [-945.635 -945.635 -945.635], eps: 0.802})
Step:    1200, Reward: [-484.508 -484.508 -484.508] [86.697], Avg: [-484.158 -484.158 -484.158] (0.7857) ({r_i: None, r_t: [-987.687 -987.687 -987.687], eps: 0.786})
Step:    1300, Reward: [-510.717 -510.717 -510.717] [94.005], Avg: [-486.055 -486.055 -486.055] (0.7700) ({r_i: None, r_t: [-979.329 -979.329 -979.329], eps: 0.77})
Step:    1400, Reward: [-507.326 -507.326 -507.326] [80.951], Avg: [-487.473 -487.473 -487.473] (0.7547) ({r_i: None, r_t: [-955.474 -955.474 -955.474], eps: 0.755})
Step:    1500, Reward: [-483.445 -483.445 -483.445] [89.839], Avg: [-487.221 -487.221 -487.221] (0.7397) ({r_i: None, r_t: [-1034.740 -1034.740 -1034.740], eps: 0.74})
Step:    1600, Reward: [-506.283 -506.283 -506.283] [159.499], Avg: [-488.343 -488.343 -488.343] (0.7250) ({r_i: None, r_t: [-999.789 -999.789 -999.789], eps: 0.725})
Step:    1700, Reward: [-510.623 -510.623 -510.623] [116.516], Avg: [-489.580 -489.580 -489.580] (0.7106) ({r_i: None, r_t: [-980.880 -980.880 -980.880], eps: 0.711})
Step:    1800, Reward: [-514.054 -514.054 -514.054] [82.987], Avg: [-490.869 -490.869 -490.869] (0.6964) ({r_i: None, r_t: [-1003.543 -1003.543 -1003.543], eps: 0.696})
Step:    1900, Reward: [-494.421 -494.421 -494.421] [74.787], Avg: [-491.046 -491.046 -491.046] (0.6826) ({r_i: None, r_t: [-983.140 -983.140 -983.140], eps: 0.683})
Step:    2000, Reward: [-460.946 -460.946 -460.946] [82.052], Avg: [-489.613 -489.613 -489.613] (0.6690) ({r_i: None, r_t: [-983.083 -983.083 -983.083], eps: 0.669})
Step:    2100, Reward: [-458.209 -458.209 -458.209] [55.572], Avg: [-488.185 -488.185 -488.185] (0.6557) ({r_i: None, r_t: [-921.267 -921.267 -921.267], eps: 0.656})
Step:    2200, Reward: [-519.558 -519.558 -519.558] [102.852], Avg: [-489.549 -489.549 -489.549] (0.6426) ({r_i: None, r_t: [-1026.551 -1026.551 -1026.551], eps: 0.643})
Step:    2300, Reward: [-499.249 -499.249 -499.249] [87.673], Avg: [-489.954 -489.954 -489.954] (0.6298) ({r_i: None, r_t: [-916.671 -916.671 -916.671], eps: 0.63})
Step:    2400, Reward: [-486.393 -486.393 -486.393] [60.006], Avg: [-489.811 -489.811 -489.811] (0.6173) ({r_i: None, r_t: [-967.624 -967.624 -967.624], eps: 0.617})
Step:    2500, Reward: [-493.004 -493.004 -493.004] [86.993], Avg: [-489.934 -489.934 -489.934] (0.6050) ({r_i: None, r_t: [-993.161 -993.161 -993.161], eps: 0.605})
Step:    2600, Reward: [-495.104 -495.104 -495.104] [81.205], Avg: [-490.125 -490.125 -490.125] (0.5930) ({r_i: None, r_t: [-969.268 -969.268 -969.268], eps: 0.593})
Step:    2700, Reward: [-467.807 -467.807 -467.807] [90.127], Avg: [-489.328 -489.328 -489.328] (0.5812) ({r_i: None, r_t: [-994.861 -994.861 -994.861], eps: 0.581})
Step:    2800, Reward: [-484.169 -484.169 -484.169] [80.934], Avg: [-489.150 -489.150 -489.150] (0.5696) ({r_i: None, r_t: [-1001.381 -1001.381 -1001.381], eps: 0.57})
Step:    2900, Reward: [-572.400 -572.400 -572.400] [136.236], Avg: [-491.925 -491.925 -491.925] (0.5583) ({r_i: None, r_t: [-1009.130 -1009.130 -1009.130], eps: 0.558})
Step:    3000, Reward: [-512.945 -512.945 -512.945] [102.083], Avg: [-492.603 -492.603 -492.603] (0.5472) ({r_i: None, r_t: [-964.000 -964.000 -964.000], eps: 0.547})
Step:    3100, Reward: [-480.640 -480.640 -480.640] [79.195], Avg: [-492.230 -492.230 -492.230] (0.5363) ({r_i: None, r_t: [-960.430 -960.430 -960.430], eps: 0.536})
Step:    3200, Reward: [-447.972 -447.972 -447.972] [69.550], Avg: [-490.888 -490.888 -490.888] (0.5256) ({r_i: None, r_t: [-1000.108 -1000.108 -1000.108], eps: 0.526})
Step:    3300, Reward: [-476.158 -476.158 -476.158] [68.125], Avg: [-490.455 -490.455 -490.455] (0.5151) ({r_i: None, r_t: [-970.251 -970.251 -970.251], eps: 0.515})
Step:    3400, Reward: [-450.594 -450.594 -450.594] [85.823], Avg: [-489.316 -489.316 -489.316] (0.5049) ({r_i: None, r_t: [-972.247 -972.247 -972.247], eps: 0.505})
Step:    3500, Reward: [-507.652 -507.652 -507.652] [127.570], Avg: [-489.826 -489.826 -489.826] (0.4948) ({r_i: None, r_t: [-915.161 -915.161 -915.161], eps: 0.495})
Step:    3600, Reward: [-507.034 -507.034 -507.034] [120.457], Avg: [-490.291 -490.291 -490.291] (0.4850) ({r_i: None, r_t: [-995.986 -995.986 -995.986], eps: 0.485})
Step:    3700, Reward: [-478.383 -478.383 -478.383] [97.091], Avg: [-489.977 -489.977 -489.977] (0.4753) ({r_i: None, r_t: [-974.731 -974.731 -974.731], eps: 0.475})
Step:    3800, Reward: [-493.035 -493.035 -493.035] [57.221], Avg: [-490.056 -490.056 -490.056] (0.4659) ({r_i: None, r_t: [-975.135 -975.135 -975.135], eps: 0.466})
Step:    3900, Reward: [-473.137 -473.137 -473.137] [70.279], Avg: [-489.633 -489.633 -489.633] (0.4566) ({r_i: None, r_t: [-953.687 -953.687 -953.687], eps: 0.457})
Step:    4000, Reward: [-443.176 -443.176 -443.176] [78.364], Avg: [-488.500 -488.500 -488.500] (0.4475) ({r_i: None, r_t: [-976.443 -976.443 -976.443], eps: 0.448})
Step:    4100, Reward: [-494.720 -494.720 -494.720] [72.507], Avg: [-488.648 -488.648 -488.648] (0.4386) ({r_i: None, r_t: [-960.992 -960.992 -960.992], eps: 0.439})
Step:    4200, Reward: [-482.974 -482.974 -482.974] [95.833], Avg: [-488.516 -488.516 -488.516] (0.4299) ({r_i: None, r_t: [-984.341 -984.341 -984.341], eps: 0.43})
Step:    4300, Reward: [-551.328 -551.328 -551.328] [91.620], Avg: [-489.943 -489.943 -489.943] (0.4213) ({r_i: None, r_t: [-1007.853 -1007.853 -1007.853], eps: 0.421})
Step:    4400, Reward: [-483.867 -483.867 -483.867] [90.910], Avg: [-489.808 -489.808 -489.808] (0.4129) ({r_i: None, r_t: [-937.368 -937.368 -937.368], eps: 0.413})
Step:    4500, Reward: [-483.124 -483.124 -483.124] [65.794], Avg: [-489.663 -489.663 -489.663] (0.4047) ({r_i: None, r_t: [-1013.897 -1013.897 -1013.897], eps: 0.405})
Step:    4600, Reward: [-496.036 -496.036 -496.036] [85.009], Avg: [-489.799 -489.799 -489.799] (0.3967) ({r_i: None, r_t: [-989.323 -989.323 -989.323], eps: 0.397})
Step:    4700, Reward: [-463.096 -463.096 -463.096] [74.965], Avg: [-489.242 -489.242 -489.242] (0.3888) ({r_i: None, r_t: [-917.667 -917.667 -917.667], eps: 0.389})
Step:    4800, Reward: [-459.970 -459.970 -459.970] [103.095], Avg: [-488.645 -488.645 -488.645] (0.3810) ({r_i: None, r_t: [-993.718 -993.718 -993.718], eps: 0.381})
Step:    4900, Reward: [-535.107 -535.107 -535.107] [80.019], Avg: [-489.574 -489.574 -489.574] (0.3735) ({r_i: None, r_t: [-1016.981 -1016.981 -1016.981], eps: 0.373})
Step:    5000, Reward: [-464.711 -464.711 -464.711] [94.744], Avg: [-489.087 -489.087 -489.087] (0.3660) ({r_i: None, r_t: [-992.315 -992.315 -992.315], eps: 0.366})
Step:    5100, Reward: [-510.697 -510.697 -510.697] [117.490], Avg: [-489.502 -489.502 -489.502] (0.3587) ({r_i: None, r_t: [-950.029 -950.029 -950.029], eps: 0.359})
Step:    5200, Reward: [-513.813 -513.813 -513.813] [140.180], Avg: [-489.961 -489.961 -489.961] (0.3516) ({r_i: None, r_t: [-964.845 -964.845 -964.845], eps: 0.352})
Step:    5300, Reward: [-512.290 -512.290 -512.290] [122.211], Avg: [-490.375 -490.375 -490.375] (0.3446) ({r_i: None, r_t: [-1045.143 -1045.143 -1045.143], eps: 0.345})
Step:    5400, Reward: [-500.896 -500.896 -500.896] [125.066], Avg: [-490.566 -490.566 -490.566] (0.3378) ({r_i: None, r_t: [-968.011 -968.011 -968.011], eps: 0.338})
Step:    5500, Reward: [-467.728 -467.728 -467.728] [78.633], Avg: [-490.158 -490.158 -490.158] (0.3310) ({r_i: None, r_t: [-998.640 -998.640 -998.640], eps: 0.331})
Step:    5600, Reward: [-507.882 -507.882 -507.882] [104.166], Avg: [-490.469 -490.469 -490.469] (0.3244) ({r_i: None, r_t: [-968.636 -968.636 -968.636], eps: 0.324})
Step:    5700, Reward: [-513.609 -513.609 -513.609] [108.201], Avg: [-490.868 -490.868 -490.868] (0.3180) ({r_i: None, r_t: [-954.459 -954.459 -954.459], eps: 0.318})
Step:    5800, Reward: [-479.766 -479.766 -479.766] [72.907], Avg: [-490.680 -490.680 -490.680] (0.3117) ({r_i: None, r_t: [-1010.980 -1010.980 -1010.980], eps: 0.312})
Step:    5900, Reward: [-496.555 -496.555 -496.555] [67.477], Avg: [-490.778 -490.778 -490.778] (0.3055) ({r_i: None, r_t: [-978.465 -978.465 -978.465], eps: 0.305})
Step:    6000, Reward: [-444.170 -444.170 -444.170] [93.551], Avg: [-490.014 -490.014 -490.014] (0.2994) ({r_i: None, r_t: [-888.424 -888.424 -888.424], eps: 0.299})
Step:    6100, Reward: [-487.405 -487.405 -487.405] [101.419], Avg: [-489.972 -489.972 -489.972] (0.2934) ({r_i: None, r_t: [-993.428 -993.428 -993.428], eps: 0.293})
Step:    6200, Reward: [-510.327 -510.327 -510.327] [89.259], Avg: [-490.295 -490.295 -490.295] (0.2876) ({r_i: None, r_t: [-980.790 -980.790 -980.790], eps: 0.288})
Step:    6300, Reward: [-478.455 -478.455 -478.455] [93.847], Avg: [-490.110 -490.110 -490.110] (0.2819) ({r_i: None, r_t: [-932.927 -932.927 -932.927], eps: 0.282})
Step:    6400, Reward: [-497.995 -497.995 -497.995] [88.019], Avg: [-490.231 -490.231 -490.231] (0.2763) ({r_i: None, r_t: [-975.632 -975.632 -975.632], eps: 0.276})
Step:    6500, Reward: [-463.662 -463.662 -463.662] [81.220], Avg: [-489.828 -489.828 -489.828] (0.2708) ({r_i: None, r_t: [-979.818 -979.818 -979.818], eps: 0.271})
Step:    6600, Reward: [-547.074 -547.074 -547.074] [109.153], Avg: [-490.683 -490.683 -490.683] (0.2654) ({r_i: None, r_t: [-960.227 -960.227 -960.227], eps: 0.265})
Step:    6700, Reward: [-440.935 -440.935 -440.935] [60.015], Avg: [-489.951 -489.951 -489.951] (0.2601) ({r_i: None, r_t: [-912.873 -912.873 -912.873], eps: 0.26})
Step:    6800, Reward: [-486.859 -486.859 -486.859] [96.108], Avg: [-489.906 -489.906 -489.906] (0.2549) ({r_i: None, r_t: [-945.286 -945.286 -945.286], eps: 0.255})
Step:    6900, Reward: [-503.673 -503.673 -503.673] [86.667], Avg: [-490.103 -490.103 -490.103] (0.2498) ({r_i: None, r_t: [-934.224 -934.224 -934.224], eps: 0.25})
Step:    7000, Reward: [-527.361 -527.361 -527.361] [97.492], Avg: [-490.628 -490.628 -490.628] (0.2449) ({r_i: None, r_t: [-971.109 -971.109 -971.109], eps: 0.245})
Step:    7100, Reward: [-520.807 -520.807 -520.807] [130.823], Avg: [-491.047 -491.047 -491.047] (0.2400) ({r_i: None, r_t: [-958.454 -958.454 -958.454], eps: 0.24})
Step:    7200, Reward: [-442.791 -442.791 -442.791] [85.323], Avg: [-490.386 -490.386 -490.386] (0.2352) ({r_i: None, r_t: [-963.108 -963.108 -963.108], eps: 0.235})
Step:    7300, Reward: [-468.763 -468.763 -468.763] [89.825], Avg: [-490.094 -490.094 -490.094] (0.2305) ({r_i: None, r_t: [-1033.592 -1033.592 -1033.592], eps: 0.231})
Step:    7400, Reward: [-523.383 -523.383 -523.383] [124.113], Avg: [-490.538 -490.538 -490.538] (0.2259) ({r_i: None, r_t: [-982.554 -982.554 -982.554], eps: 0.226})
Step:    7500, Reward: [-482.344 -482.344 -482.344] [107.123], Avg: [-490.430 -490.430 -490.430] (0.2215) ({r_i: None, r_t: [-926.284 -926.284 -926.284], eps: 0.221})
Step:    7600, Reward: [-449.129 -449.129 -449.129] [50.473], Avg: [-489.893 -489.893 -489.893] (0.2170) ({r_i: None, r_t: [-990.814 -990.814 -990.814], eps: 0.217})
Step:    7700, Reward: [-499.474 -499.474 -499.474] [72.083], Avg: [-490.016 -490.016 -490.016] (0.2127) ({r_i: None, r_t: [-934.331 -934.331 -934.331], eps: 0.213})
Step:    7800, Reward: [-501.527 -501.527 -501.527] [110.433], Avg: [-490.162 -490.162 -490.162] (0.2085) ({r_i: None, r_t: [-980.552 -980.552 -980.552], eps: 0.208})
Step:    7900, Reward: [-526.046 -526.046 -526.046] [96.506], Avg: [-490.611 -490.611 -490.611] (0.2043) ({r_i: None, r_t: [-963.787 -963.787 -963.787], eps: 0.204})
Step:    8000, Reward: [-469.174 -469.174 -469.174] [107.663], Avg: [-490.346 -490.346 -490.346] (0.2003) ({r_i: None, r_t: [-1036.217 -1036.217 -1036.217], eps: 0.2})
Step:    8100, Reward: [-495.296 -495.296 -495.296] [100.435], Avg: [-490.406 -490.406 -490.406] (0.1963) ({r_i: None, r_t: [-1024.826 -1024.826 -1024.826], eps: 0.196})
Step:    8200, Reward: [-482.916 -482.916 -482.916] [73.121], Avg: [-490.316 -490.316 -490.316] (0.1924) ({r_i: None, r_t: [-986.936 -986.936 -986.936], eps: 0.192})
Step:    8300, Reward: [-458.346 -458.346 -458.346] [68.364], Avg: [-489.935 -489.935 -489.935] (0.1886) ({r_i: None, r_t: [-1006.852 -1006.852 -1006.852], eps: 0.189})
Step:    8400, Reward: [-497.132 -497.132 -497.132] [106.964], Avg: [-490.020 -490.020 -490.020] (0.1848) ({r_i: None, r_t: [-915.498 -915.498 -915.498], eps: 0.185})
Step:    8500, Reward: [-516.557 -516.557 -516.557] [71.846], Avg: [-490.329 -490.329 -490.329] (0.1811) ({r_i: None, r_t: [-911.129 -911.129 -911.129], eps: 0.181})
Step:    8600, Reward: [-439.530 -439.530 -439.530] [94.818], Avg: [-489.745 -489.745 -489.745] (0.1775) ({r_i: None, r_t: [-917.219 -917.219 -917.219], eps: 0.178})
Step:    8700, Reward: [-479.315 -479.315 -479.315] [82.416], Avg: [-489.626 -489.626 -489.626] (0.1740) ({r_i: None, r_t: [-1049.252 -1049.252 -1049.252], eps: 0.174})
Step:    8800, Reward: [-489.384 -489.384 -489.384] [85.365], Avg: [-489.624 -489.624 -489.624] (0.1705) ({r_i: None, r_t: [-974.717 -974.717 -974.717], eps: 0.171})
Step:    8900, Reward: [-522.315 -522.315 -522.315] [112.326], Avg: [-489.987 -489.987 -489.987] (0.1671) ({r_i: None, r_t: [-979.741 -979.741 -979.741], eps: 0.167})
Step:    9000, Reward: [-502.248 -502.248 -502.248] [181.011], Avg: [-490.121 -490.121 -490.121] (0.1638) ({r_i: None, r_t: [-1000.821 -1000.821 -1000.821], eps: 0.164})
Step:    9100, Reward: [-482.252 -482.252 -482.252] [84.203], Avg: [-490.036 -490.036 -490.036] (0.1605) ({r_i: None, r_t: [-1004.643 -1004.643 -1004.643], eps: 0.161})
Step:    9200, Reward: [-460.483 -460.483 -460.483] [79.704], Avg: [-489.718 -489.718 -489.718] (0.1574) ({r_i: None, r_t: [-1018.419 -1018.419 -1018.419], eps: 0.157})
Step:    9300, Reward: [-481.732 -481.732 -481.732] [99.164], Avg: [-489.633 -489.633 -489.633] (0.1542) ({r_i: None, r_t: [-1028.989 -1028.989 -1028.989], eps: 0.154})
Step:    9400, Reward: [-556.471 -556.471 -556.471] [165.420], Avg: [-490.337 -490.337 -490.337] (0.1512) ({r_i: None, r_t: [-1013.988 -1013.988 -1013.988], eps: 0.151})
Step:    9500, Reward: [-543.351 -543.351 -543.351] [107.527], Avg: [-490.889 -490.889 -490.889] (0.1481) ({r_i: None, r_t: [-953.603 -953.603 -953.603], eps: 0.148})
Step:    9600, Reward: [-468.487 -468.487 -468.487] [67.579], Avg: [-490.658 -490.658 -490.658] (0.1452) ({r_i: None, r_t: [-1000.821 -1000.821 -1000.821], eps: 0.145})
Step:    9700, Reward: [-501.773 -501.773 -501.773] [96.848], Avg: [-490.771 -490.771 -490.771] (0.1423) ({r_i: None, r_t: [-992.643 -992.643 -992.643], eps: 0.142})
Step:    9800, Reward: [-518.059 -518.059 -518.059] [72.719], Avg: [-491.047 -491.047 -491.047] (0.1395) ({r_i: None, r_t: [-1001.016 -1001.016 -1001.016], eps: 0.139})
Step:    9900, Reward: [-470.031 -470.031 -470.031] [86.608], Avg: [-490.837 -490.837 -490.837] (0.1367) ({r_i: None, r_t: [-970.414 -970.414 -970.414], eps: 0.137})
Step:   10000, Reward: [-467.138 -467.138 -467.138] [99.709], Avg: [-490.602 -490.602 -490.602] (0.1340) ({r_i: None, r_t: [-984.383 -984.383 -984.383], eps: 0.134})
Step:   10100, Reward: [-495.869 -495.869 -495.869] [122.458], Avg: [-490.654 -490.654 -490.654] (0.1313) ({r_i: None, r_t: [-1007.658 -1007.658 -1007.658], eps: 0.131})
Step:   10200, Reward: [-457.759 -457.759 -457.759] [129.778], Avg: [-490.335 -490.335 -490.335] (0.1287) ({r_i: None, r_t: [-994.740 -994.740 -994.740], eps: 0.129})
Step:   10300, Reward: [-498.013 -498.013 -498.013] [80.094], Avg: [-490.408 -490.408 -490.408] (0.1261) ({r_i: None, r_t: [-1028.685 -1028.685 -1028.685], eps: 0.126})
Step:   10400, Reward: [-479.005 -479.005 -479.005] [104.673], Avg: [-490.300 -490.300 -490.300] (0.1236) ({r_i: None, r_t: [-938.115 -938.115 -938.115], eps: 0.124})
Step:   10500, Reward: [-528.404 -528.404 -528.404] [128.009], Avg: [-490.659 -490.659 -490.659] (0.1212) ({r_i: None, r_t: [-1069.065 -1069.065 -1069.065], eps: 0.121})
Step:   10600, Reward: [-488.692 -488.692 -488.692] [103.824], Avg: [-490.641 -490.641 -490.641] (0.1188) ({r_i: None, r_t: [-1016.301 -1016.301 -1016.301], eps: 0.119})
Step:   10700, Reward: [-434.694 -434.694 -434.694] [61.380], Avg: [-490.123 -490.123 -490.123] (0.1164) ({r_i: None, r_t: [-1028.187 -1028.187 -1028.187], eps: 0.116})
Step:   10800, Reward: [-458.417 -458.417 -458.417] [69.044], Avg: [-489.832 -489.832 -489.832] (0.1141) ({r_i: None, r_t: [-1040.669 -1040.669 -1040.669], eps: 0.114})
Step:   10900, Reward: [-543.224 -543.224 -543.224] [148.696], Avg: [-490.317 -490.317 -490.317] (0.1118) ({r_i: None, r_t: [-963.935 -963.935 -963.935], eps: 0.112})
Step:   11000, Reward: [-523.531 -523.531 -523.531] [127.559], Avg: [-490.617 -490.617 -490.617] (0.1096) ({r_i: None, r_t: [-975.247 -975.247 -975.247], eps: 0.11})
Step:   11100, Reward: [-554.766 -554.766 -554.766] [168.015], Avg: [-491.189 -491.189 -491.189] (0.1074) ({r_i: None, r_t: [-1010.752 -1010.752 -1010.752], eps: 0.107})
Step:   11200, Reward: [-532.577 -532.577 -532.577] [75.897], Avg: [-491.556 -491.556 -491.556] (0.1053) ({r_i: None, r_t: [-1026.698 -1026.698 -1026.698], eps: 0.105})
Step:   11300, Reward: [-479.370 -479.370 -479.370] [80.544], Avg: [-491.449 -491.449 -491.449] (0.1032) ({r_i: None, r_t: [-1066.245 -1066.245 -1066.245], eps: 0.103})
Step:   11400, Reward: [-495.058 -495.058 -495.058] [127.734], Avg: [-491.480 -491.480 -491.480] (0.1011) ({r_i: None, r_t: [-1071.477 -1071.477 -1071.477], eps: 0.101})
Step:   11500, Reward: [-503.784 -503.784 -503.784] [164.491], Avg: [-491.586 -491.586 -491.586] (0.1000) ({r_i: None, r_t: [-1048.574 -1048.574 -1048.574], eps: 0.1})
Step:   11600, Reward: [-544.932 -544.932 -544.932] [129.695], Avg: [-492.042 -492.042 -492.042] (0.1000) ({r_i: None, r_t: [-1036.378 -1036.378 -1036.378], eps: 0.1})
Step:   11700, Reward: [-515.055 -515.055 -515.055] [119.524], Avg: [-492.237 -492.237 -492.237] (0.1000) ({r_i: None, r_t: [-1097.824 -1097.824 -1097.824], eps: 0.1})
Step:   11800, Reward: [-467.415 -467.415 -467.415] [56.894], Avg: [-492.029 -492.029 -492.029] (0.1000) ({r_i: None, r_t: [-1054.442 -1054.442 -1054.442], eps: 0.1})
Step:   11900, Reward: [-541.702 -541.702 -541.702] [165.241], Avg: [-492.442 -492.442 -492.442] (0.1000) ({r_i: None, r_t: [-1065.724 -1065.724 -1065.724], eps: 0.1})
Step:   12000, Reward: [-543.906 -543.906 -543.906] [126.801], Avg: [-492.868 -492.868 -492.868] (0.1000) ({r_i: None, r_t: [-1097.036 -1097.036 -1097.036], eps: 0.1})
Step:   12100, Reward: [-519.839 -519.839 -519.839] [100.559], Avg: [-493.089 -493.089 -493.089] (0.1000) ({r_i: None, r_t: [-1010.209 -1010.209 -1010.209], eps: 0.1})
Step:   12200, Reward: [-538.476 -538.476 -538.476] [119.890], Avg: [-493.458 -493.458 -493.458] (0.1000) ({r_i: None, r_t: [-1036.264 -1036.264 -1036.264], eps: 0.1})
Step:   12300, Reward: [-562.871 -562.871 -562.871] [178.474], Avg: [-494.018 -494.018 -494.018] (0.1000) ({r_i: None, r_t: [-1158.570 -1158.570 -1158.570], eps: 0.1})
Step:   12400, Reward: [-512.390 -512.390 -512.390] [102.905], Avg: [-494.165 -494.165 -494.165] (0.1000) ({r_i: None, r_t: [-1045.784 -1045.784 -1045.784], eps: 0.1})
Step:   12500, Reward: [-539.130 -539.130 -539.130] [105.621], Avg: [-494.522 -494.522 -494.522] (0.1000) ({r_i: None, r_t: [-1034.178 -1034.178 -1034.178], eps: 0.1})
Step:   12600, Reward: [-583.475 -583.475 -583.475] [140.917], Avg: [-495.222 -495.222 -495.222] (0.1000) ({r_i: None, r_t: [-1029.479 -1029.479 -1029.479], eps: 0.1})
Step:   12700, Reward: [-558.648 -558.648 -558.648] [113.621], Avg: [-495.717 -495.717 -495.717] (0.1000) ({r_i: None, r_t: [-1097.126 -1097.126 -1097.126], eps: 0.1})
Step:   12800, Reward: [-574.342 -574.342 -574.342] [131.133], Avg: [-496.327 -496.327 -496.327] (0.1000) ({r_i: None, r_t: [-1132.703 -1132.703 -1132.703], eps: 0.1})
Step:   12900, Reward: [-551.812 -551.812 -551.812] [145.888], Avg: [-496.754 -496.754 -496.754] (0.1000) ({r_i: None, r_t: [-1093.769 -1093.769 -1093.769], eps: 0.1})
Step:   13000, Reward: [-552.987 -552.987 -552.987] [101.953], Avg: [-497.183 -497.183 -497.183] (0.1000) ({r_i: None, r_t: [-989.528 -989.528 -989.528], eps: 0.1})
Step:   13100, Reward: [-523.159 -523.159 -523.159] [95.189], Avg: [-497.380 -497.380 -497.380] (0.1000) ({r_i: None, r_t: [-1113.535 -1113.535 -1113.535], eps: 0.1})
Step:   13200, Reward: [-548.602 -548.602 -548.602] [106.316], Avg: [-497.765 -497.765 -497.765] (0.1000) ({r_i: None, r_t: [-1079.163 -1079.163 -1079.163], eps: 0.1})
Step:   13300, Reward: [-499.705 -499.705 -499.705] [65.332], Avg: [-497.779 -497.779 -497.779] (0.1000) ({r_i: None, r_t: [-1093.885 -1093.885 -1093.885], eps: 0.1})
Step:   13400, Reward: [-513.644 -513.644 -513.644] [103.245], Avg: [-497.897 -497.897 -497.897] (0.1000) ({r_i: None, r_t: [-975.591 -975.591 -975.591], eps: 0.1})
Step:   13500, Reward: [-537.607 -537.607 -537.607] [107.345], Avg: [-498.189 -498.189 -498.189] (0.1000) ({r_i: None, r_t: [-1143.602 -1143.602 -1143.602], eps: 0.1})
Step:   13600, Reward: [-558.989 -558.989 -558.989] [161.033], Avg: [-498.633 -498.633 -498.633] (0.1000) ({r_i: None, r_t: [-1070.572 -1070.572 -1070.572], eps: 0.1})
Step:   13700, Reward: [-558.419 -558.419 -558.419] [119.953], Avg: [-499.066 -499.066 -499.066] (0.1000) ({r_i: None, r_t: [-1084.372 -1084.372 -1084.372], eps: 0.1})
Step:   13800, Reward: [-523.921 -523.921 -523.921] [97.235], Avg: [-499.245 -499.245 -499.245] (0.1000) ({r_i: None, r_t: [-1160.920 -1160.920 -1160.920], eps: 0.1})
Step:   13900, Reward: [-567.303 -567.303 -567.303] [141.642], Avg: [-499.731 -499.731 -499.731] (0.1000) ({r_i: None, r_t: [-1056.030 -1056.030 -1056.030], eps: 0.1})
Step:   14000, Reward: [-556.475 -556.475 -556.475] [137.618], Avg: [-500.133 -500.133 -500.133] (0.1000) ({r_i: None, r_t: [-1065.016 -1065.016 -1065.016], eps: 0.1})
Step:   14100, Reward: [-533.142 -533.142 -533.142] [113.257], Avg: [-500.366 -500.366 -500.366] (0.1000) ({r_i: None, r_t: [-1049.437 -1049.437 -1049.437], eps: 0.1})
Step:   14200, Reward: [-565.589 -565.589 -565.589] [170.769], Avg: [-500.822 -500.822 -500.822] (0.1000) ({r_i: None, r_t: [-1051.789 -1051.789 -1051.789], eps: 0.1})
Step:   14300, Reward: [-582.584 -582.584 -582.584] [141.305], Avg: [-501.390 -501.390 -501.390] (0.1000) ({r_i: None, r_t: [-1147.173 -1147.173 -1147.173], eps: 0.1})
Step:   14400, Reward: [-587.839 -587.839 -587.839] [158.377], Avg: [-501.986 -501.986 -501.986] (0.1000) ({r_i: None, r_t: [-1128.419 -1128.419 -1128.419], eps: 0.1})
Step:   14500, Reward: [-508.270 -508.270 -508.270] [89.293], Avg: [-502.029 -502.029 -502.029] (0.1000) ({r_i: None, r_t: [-1086.373 -1086.373 -1086.373], eps: 0.1})
Step:   14600, Reward: [-548.244 -548.244 -548.244] [104.905], Avg: [-502.343 -502.343 -502.343] (0.1000) ({r_i: None, r_t: [-1193.133 -1193.133 -1193.133], eps: 0.1})
Step:   14700, Reward: [-573.879 -573.879 -573.879] [129.335], Avg: [-502.827 -502.827 -502.827] (0.1000) ({r_i: None, r_t: [-1026.895 -1026.895 -1026.895], eps: 0.1})
Step:   14800, Reward: [-586.028 -586.028 -586.028] [114.122], Avg: [-503.385 -503.385 -503.385] (0.1000) ({r_i: None, r_t: [-1145.890 -1145.890 -1145.890], eps: 0.1})
Step:   14900, Reward: [-589.259 -589.259 -589.259] [89.775], Avg: [-503.958 -503.958 -503.958] (0.1000) ({r_i: None, r_t: [-1100.477 -1100.477 -1100.477], eps: 0.1})
Step:   15000, Reward: [-587.482 -587.482 -587.482] [89.101], Avg: [-504.511 -504.511 -504.511] (0.1000) ({r_i: None, r_t: [-1124.046 -1124.046 -1124.046], eps: 0.1})
Step:   15100, Reward: [-622.804 -622.804 -622.804] [179.349], Avg: [-505.289 -505.289 -505.289] (0.1000) ({r_i: None, r_t: [-1060.006 -1060.006 -1060.006], eps: 0.1})
Step:   15200, Reward: [-584.246 -584.246 -584.246] [152.139], Avg: [-505.805 -505.805 -505.805] (0.1000) ({r_i: None, r_t: [-1096.888 -1096.888 -1096.888], eps: 0.1})
Step:   15300, Reward: [-560.406 -560.406 -560.406] [125.719], Avg: [-506.160 -506.160 -506.160] (0.1000) ({r_i: None, r_t: [-1146.697 -1146.697 -1146.697], eps: 0.1})
Step:   15400, Reward: [-572.181 -572.181 -572.181] [124.470], Avg: [-506.585 -506.585 -506.585] (0.1000) ({r_i: None, r_t: [-1154.284 -1154.284 -1154.284], eps: 0.1})
Step:   15500, Reward: [-595.078 -595.078 -595.078] [179.526], Avg: [-507.153 -507.153 -507.153] (0.1000) ({r_i: None, r_t: [-1083.278 -1083.278 -1083.278], eps: 0.1})
Step:   15600, Reward: [-540.844 -540.844 -540.844] [167.002], Avg: [-507.367 -507.367 -507.367] (0.1000) ({r_i: None, r_t: [-1021.843 -1021.843 -1021.843], eps: 0.1})
Step:   15700, Reward: [-568.818 -568.818 -568.818] [138.107], Avg: [-507.756 -507.756 -507.756] (0.1000) ({r_i: None, r_t: [-1111.943 -1111.943 -1111.943], eps: 0.1})
Step:   15800, Reward: [-529.159 -529.159 -529.159] [113.509], Avg: [-507.891 -507.891 -507.891] (0.1000) ({r_i: None, r_t: [-1137.236 -1137.236 -1137.236], eps: 0.1})
Step:   15900, Reward: [-524.175 -524.175 -524.175] [123.443], Avg: [-507.993 -507.993 -507.993] (0.1000) ({r_i: None, r_t: [-1097.818 -1097.818 -1097.818], eps: 0.1})
Step:   16000, Reward: [-590.146 -590.146 -590.146] [143.070], Avg: [-508.503 -508.503 -508.503] (0.1000) ({r_i: None, r_t: [-1171.718 -1171.718 -1171.718], eps: 0.1})
Step:   16100, Reward: [-558.613 -558.613 -558.613] [128.367], Avg: [-508.812 -508.812 -508.812] (0.1000) ({r_i: None, r_t: [-1038.481 -1038.481 -1038.481], eps: 0.1})
Step:   16200, Reward: [-513.261 -513.261 -513.261] [94.635], Avg: [-508.840 -508.840 -508.840] (0.1000) ({r_i: None, r_t: [-1055.829 -1055.829 -1055.829], eps: 0.1})
Step:   16300, Reward: [-487.757 -487.757 -487.757] [115.192], Avg: [-508.711 -508.711 -508.711] (0.1000) ({r_i: None, r_t: [-1127.809 -1127.809 -1127.809], eps: 0.1})
Step:   16400, Reward: [-500.059 -500.059 -500.059] [112.902], Avg: [-508.659 -508.659 -508.659] (0.1000) ({r_i: None, r_t: [-1111.320 -1111.320 -1111.320], eps: 0.1})
Step:   16500, Reward: [-569.819 -569.819 -569.819] [110.285], Avg: [-509.027 -509.027 -509.027] (0.1000) ({r_i: None, r_t: [-1137.736 -1137.736 -1137.736], eps: 0.1})
Step:   16600, Reward: [-518.395 -518.395 -518.395] [94.145], Avg: [-509.083 -509.083 -509.083] (0.1000) ({r_i: None, r_t: [-1030.645 -1030.645 -1030.645], eps: 0.1})
Step:   16700, Reward: [-527.179 -527.179 -527.179] [117.680], Avg: [-509.191 -509.191 -509.191] (0.1000) ({r_i: None, r_t: [-1064.447 -1064.447 -1064.447], eps: 0.1})
Step:   16800, Reward: [-507.434 -507.434 -507.434] [113.393], Avg: [-509.180 -509.180 -509.180] (0.1000) ({r_i: None, r_t: [-1021.406 -1021.406 -1021.406], eps: 0.1})
Step:   16900, Reward: [-521.981 -521.981 -521.981] [130.504], Avg: [-509.256 -509.256 -509.256] (0.1000) ({r_i: None, r_t: [-1009.998 -1009.998 -1009.998], eps: 0.1})
Step:   17000, Reward: [-493.681 -493.681 -493.681] [152.473], Avg: [-509.165 -509.165 -509.165] (0.1000) ({r_i: None, r_t: [-1114.978 -1114.978 -1114.978], eps: 0.1})
Step:   17100, Reward: [-558.502 -558.502 -558.502] [145.691], Avg: [-509.451 -509.451 -509.451] (0.1000) ({r_i: None, r_t: [-1008.857 -1008.857 -1008.857], eps: 0.1})
Step:   17200, Reward: [-462.563 -462.563 -462.563] [90.962], Avg: [-509.180 -509.180 -509.180] (0.1000) ({r_i: None, r_t: [-1019.831 -1019.831 -1019.831], eps: 0.1})
Step:   17300, Reward: [-522.690 -522.690 -522.690] [111.100], Avg: [-509.258 -509.258 -509.258] (0.1000) ({r_i: None, r_t: [-1066.420 -1066.420 -1066.420], eps: 0.1})
Step:   17400, Reward: [-496.078 -496.078 -496.078] [103.283], Avg: [-509.183 -509.183 -509.183] (0.1000) ({r_i: None, r_t: [-1000.366 -1000.366 -1000.366], eps: 0.1})
Step:   17500, Reward: [-501.911 -501.911 -501.911] [66.551], Avg: [-509.141 -509.141 -509.141] (0.1000) ({r_i: None, r_t: [-1024.963 -1024.963 -1024.963], eps: 0.1})
Step:   17600, Reward: [-488.190 -488.190 -488.190] [135.905], Avg: [-509.023 -509.023 -509.023] (0.1000) ({r_i: None, r_t: [-968.417 -968.417 -968.417], eps: 0.1})
Step:   17700, Reward: [-486.656 -486.656 -486.656] [56.968], Avg: [-508.897 -508.897 -508.897] (0.1000) ({r_i: None, r_t: [-939.127 -939.127 -939.127], eps: 0.1})
Step:   17800, Reward: [-572.853 -572.853 -572.853] [123.706], Avg: [-509.255 -509.255 -509.255] (0.1000) ({r_i: None, r_t: [-999.699 -999.699 -999.699], eps: 0.1})
Step:   17900, Reward: [-499.362 -499.362 -499.362] [116.693], Avg: [-509.200 -509.200 -509.200] (0.1000) ({r_i: None, r_t: [-1046.808 -1046.808 -1046.808], eps: 0.1})
Step:   18000, Reward: [-458.396 -458.396 -458.396] [75.757], Avg: [-508.919 -508.919 -508.919] (0.1000) ({r_i: None, r_t: [-1055.720 -1055.720 -1055.720], eps: 0.1})
Step:   18100, Reward: [-485.776 -485.776 -485.776] [125.723], Avg: [-508.792 -508.792 -508.792] (0.1000) ({r_i: None, r_t: [-935.879 -935.879 -935.879], eps: 0.1})
Step:   18200, Reward: [-490.191 -490.191 -490.191] [76.399], Avg: [-508.690 -508.690 -508.690] (0.1000) ({r_i: None, r_t: [-960.567 -960.567 -960.567], eps: 0.1})
Step:   18300, Reward: [-476.144 -476.144 -476.144] [96.953], Avg: [-508.513 -508.513 -508.513] (0.1000) ({r_i: None, r_t: [-981.636 -981.636 -981.636], eps: 0.1})
Step:   18400, Reward: [-477.727 -477.727 -477.727] [80.881], Avg: [-508.347 -508.347 -508.347] (0.1000) ({r_i: None, r_t: [-974.959 -974.959 -974.959], eps: 0.1})
Step:   18500, Reward: [-463.133 -463.133 -463.133] [75.188], Avg: [-508.104 -508.104 -508.104] (0.1000) ({r_i: None, r_t: [-1009.509 -1009.509 -1009.509], eps: 0.1})
Step:   18600, Reward: [-470.150 -470.150 -470.150] [78.928], Avg: [-507.901 -507.901 -507.901] (0.1000) ({r_i: None, r_t: [-992.573 -992.573 -992.573], eps: 0.1})
Step:   18700, Reward: [-442.183 -442.183 -442.183] [68.405], Avg: [-507.551 -507.551 -507.551] (0.1000) ({r_i: None, r_t: [-973.704 -973.704 -973.704], eps: 0.1})
Step:   18800, Reward: [-513.317 -513.317 -513.317] [110.384], Avg: [-507.582 -507.582 -507.582] (0.1000) ({r_i: None, r_t: [-979.881 -979.881 -979.881], eps: 0.1})
Step:   18900, Reward: [-469.373 -469.373 -469.373] [75.082], Avg: [-507.381 -507.381 -507.381] (0.1000) ({r_i: None, r_t: [-969.402 -969.402 -969.402], eps: 0.1})
Step:   19000, Reward: [-476.355 -476.355 -476.355] [68.665], Avg: [-507.218 -507.218 -507.218] (0.1000) ({r_i: None, r_t: [-996.883 -996.883 -996.883], eps: 0.1})
Step:   19100, Reward: [-478.362 -478.362 -478.362] [119.521], Avg: [-507.068 -507.068 -507.068] (0.1000) ({r_i: None, r_t: [-948.267 -948.267 -948.267], eps: 0.1})
Step:   19200, Reward: [-487.383 -487.383 -487.383] [105.112], Avg: [-506.966 -506.966 -506.966] (0.1000) ({r_i: None, r_t: [-961.332 -961.332 -961.332], eps: 0.1})
Step:   19300, Reward: [-468.833 -468.833 -468.833] [64.222], Avg: [-506.769 -506.769 -506.769] (0.1000) ({r_i: None, r_t: [-1001.688 -1001.688 -1001.688], eps: 0.1})
Step:   19400, Reward: [-469.931 -469.931 -469.931] [88.103], Avg: [-506.581 -506.581 -506.581] (0.1000) ({r_i: None, r_t: [-956.347 -956.347 -956.347], eps: 0.1})
Step:   19500, Reward: [-458.663 -458.663 -458.663] [63.453], Avg: [-506.336 -506.336 -506.336] (0.1000) ({r_i: None, r_t: [-957.920 -957.920 -957.920], eps: 0.1})
Step:   19600, Reward: [-488.982 -488.982 -488.982] [69.821], Avg: [-506.248 -506.248 -506.248] (0.1000) ({r_i: None, r_t: [-935.097 -935.097 -935.097], eps: 0.1})
Step:   19700, Reward: [-455.299 -455.299 -455.299] [76.204], Avg: [-505.991 -505.991 -505.991] (0.1000) ({r_i: None, r_t: [-940.408 -940.408 -940.408], eps: 0.1})
Step:   19800, Reward: [-488.653 -488.653 -488.653] [100.061], Avg: [-505.904 -505.904 -505.904] (0.1000) ({r_i: None, r_t: [-951.337 -951.337 -951.337], eps: 0.1})
Step:   19900, Reward: [-441.616 -441.616 -441.616] [52.029], Avg: [-505.582 -505.582 -505.582] (0.1000) ({r_i: None, r_t: [-920.805 -920.805 -920.805], eps: 0.1})
Step:   20000, Reward: [-449.855 -449.855 -449.855] [72.589], Avg: [-505.305 -505.305 -505.305] (0.1000) ({r_i: None, r_t: [-939.580 -939.580 -939.580], eps: 0.1})
Step:   20100, Reward: [-433.040 -433.040 -433.040] [53.137], Avg: [-504.947 -504.947 -504.947] (0.1000) ({r_i: None, r_t: [-976.745 -976.745 -976.745], eps: 0.1})
Step:   20200, Reward: [-507.184 -507.184 -507.184] [125.223], Avg: [-504.958 -504.958 -504.958] (0.1000) ({r_i: None, r_t: [-942.491 -942.491 -942.491], eps: 0.1})
Step:   20300, Reward: [-477.995 -477.995 -477.995] [130.415], Avg: [-504.826 -504.826 -504.826] (0.1000) ({r_i: None, r_t: [-889.650 -889.650 -889.650], eps: 0.1})
Step:   20400, Reward: [-484.572 -484.572 -484.572] [97.263], Avg: [-504.727 -504.727 -504.727] (0.1000) ({r_i: None, r_t: [-942.895 -942.895 -942.895], eps: 0.1})
Step:   20500, Reward: [-471.582 -471.582 -471.582] [77.225], Avg: [-504.566 -504.566 -504.566] (0.1000) ({r_i: None, r_t: [-886.853 -886.853 -886.853], eps: 0.1})
Step:   20600, Reward: [-433.603 -433.603 -433.603] [73.287], Avg: [-504.223 -504.223 -504.223] (0.1000) ({r_i: None, r_t: [-989.866 -989.866 -989.866], eps: 0.1})
Step:   20700, Reward: [-480.922 -480.922 -480.922] [87.008], Avg: [-504.111 -504.111 -504.111] (0.1000) ({r_i: None, r_t: [-918.888 -918.888 -918.888], eps: 0.1})
Step:   20800, Reward: [-454.685 -454.685 -454.685] [83.376], Avg: [-503.875 -503.875 -503.875] (0.1000) ({r_i: None, r_t: [-876.146 -876.146 -876.146], eps: 0.1})
Step:   20900, Reward: [-460.327 -460.327 -460.327] [76.735], Avg: [-503.668 -503.668 -503.668] (0.1000) ({r_i: None, r_t: [-920.700 -920.700 -920.700], eps: 0.1})
Step:   21000, Reward: [-473.933 -473.933 -473.933] [63.009], Avg: [-503.527 -503.527 -503.527] (0.1000) ({r_i: None, r_t: [-908.677 -908.677 -908.677], eps: 0.1})
Step:   21100, Reward: [-444.177 -444.177 -444.177] [61.227], Avg: [-503.247 -503.247 -503.247] (0.1000) ({r_i: None, r_t: [-920.167 -920.167 -920.167], eps: 0.1})
Step:   21200, Reward: [-505.498 -505.498 -505.498] [82.443], Avg: [-503.257 -503.257 -503.257] (0.1000) ({r_i: None, r_t: [-929.733 -929.733 -929.733], eps: 0.1})
Step:   21300, Reward: [-471.772 -471.772 -471.772] [88.519], Avg: [-503.110 -503.110 -503.110] (0.1000) ({r_i: None, r_t: [-975.436 -975.436 -975.436], eps: 0.1})
Step:   21400, Reward: [-460.029 -460.029 -460.029] [75.299], Avg: [-502.910 -502.910 -502.910] (0.1000) ({r_i: None, r_t: [-921.961 -921.961 -921.961], eps: 0.1})
Step:   21500, Reward: [-451.702 -451.702 -451.702] [62.357], Avg: [-502.673 -502.673 -502.673] (0.1000) ({r_i: None, r_t: [-920.816 -920.816 -920.816], eps: 0.1})
Step:   21600, Reward: [-480.501 -480.501 -480.501] [78.923], Avg: [-502.571 -502.571 -502.571] (0.1000) ({r_i: None, r_t: [-970.089 -970.089 -970.089], eps: 0.1})
Step:   21700, Reward: [-471.084 -471.084 -471.084] [88.132], Avg: [-502.426 -502.426 -502.426] (0.1000) ({r_i: None, r_t: [-914.454 -914.454 -914.454], eps: 0.1})
Step:   21800, Reward: [-434.457 -434.457 -434.457] [61.164], Avg: [-502.116 -502.116 -502.116] (0.1000) ({r_i: None, r_t: [-933.072 -933.072 -933.072], eps: 0.1})
Step:   21900, Reward: [-429.013 -429.013 -429.013] [60.875], Avg: [-501.783 -501.783 -501.783] (0.1000) ({r_i: None, r_t: [-957.416 -957.416 -957.416], eps: 0.1})
Step:   22000, Reward: [-486.657 -486.657 -486.657] [100.661], Avg: [-501.715 -501.715 -501.715] (0.1000) ({r_i: None, r_t: [-899.977 -899.977 -899.977], eps: 0.1})
Step:   22100, Reward: [-486.488 -486.488 -486.488] [82.883], Avg: [-501.646 -501.646 -501.646] (0.1000) ({r_i: None, r_t: [-964.793 -964.793 -964.793], eps: 0.1})
Step:   22200, Reward: [-463.338 -463.338 -463.338] [111.983], Avg: [-501.475 -501.475 -501.475] (0.1000) ({r_i: None, r_t: [-955.054 -955.054 -955.054], eps: 0.1})
Step:   22300, Reward: [-488.412 -488.412 -488.412] [100.502], Avg: [-501.416 -501.416 -501.416] (0.1000) ({r_i: None, r_t: [-964.952 -964.952 -964.952], eps: 0.1})
Step:   22400, Reward: [-493.690 -493.690 -493.690] [87.311], Avg: [-501.382 -501.382 -501.382] (0.1000) ({r_i: None, r_t: [-949.131 -949.131 -949.131], eps: 0.1})
Step:   22500, Reward: [-441.839 -441.839 -441.839] [82.077], Avg: [-501.118 -501.118 -501.118] (0.1000) ({r_i: None, r_t: [-889.318 -889.318 -889.318], eps: 0.1})
Step:   22600, Reward: [-445.621 -445.621 -445.621] [89.134], Avg: [-500.874 -500.874 -500.874] (0.1000) ({r_i: None, r_t: [-965.602 -965.602 -965.602], eps: 0.1})
Step:   22700, Reward: [-482.209 -482.209 -482.209] [101.580], Avg: [-500.792 -500.792 -500.792] (0.1000) ({r_i: None, r_t: [-899.790 -899.790 -899.790], eps: 0.1})
Step:   22800, Reward: [-468.672 -468.672 -468.672] [84.004], Avg: [-500.652 -500.652 -500.652] (0.1000) ({r_i: None, r_t: [-967.361 -967.361 -967.361], eps: 0.1})
Step:   22900, Reward: [-492.286 -492.286 -492.286] [78.852], Avg: [-500.616 -500.616 -500.616] (0.1000) ({r_i: None, r_t: [-897.418 -897.418 -897.418], eps: 0.1})
Step:   23000, Reward: [-497.214 -497.214 -497.214] [115.536], Avg: [-500.601 -500.601 -500.601] (0.1000) ({r_i: None, r_t: [-893.658 -893.658 -893.658], eps: 0.1})
Step:   23100, Reward: [-479.570 -479.570 -479.570] [120.863], Avg: [-500.510 -500.510 -500.510] (0.1000) ({r_i: None, r_t: [-920.468 -920.468 -920.468], eps: 0.1})
Step:   23200, Reward: [-472.350 -472.350 -472.350] [85.191], Avg: [-500.389 -500.389 -500.389] (0.1000) ({r_i: None, r_t: [-961.730 -961.730 -961.730], eps: 0.1})
Step:   23300, Reward: [-490.728 -490.728 -490.728] [93.972], Avg: [-500.348 -500.348 -500.348] (0.1000) ({r_i: None, r_t: [-981.633 -981.633 -981.633], eps: 0.1})
Step:   23400, Reward: [-499.881 -499.881 -499.881] [102.334], Avg: [-500.346 -500.346 -500.346] (0.1000) ({r_i: None, r_t: [-909.887 -909.887 -909.887], eps: 0.1})
Step:   23500, Reward: [-514.352 -514.352 -514.352] [87.998], Avg: [-500.405 -500.405 -500.405] (0.1000) ({r_i: None, r_t: [-969.491 -969.491 -969.491], eps: 0.1})
Step:   23600, Reward: [-517.035 -517.035 -517.035] [90.322], Avg: [-500.476 -500.476 -500.476] (0.1000) ({r_i: None, r_t: [-954.224 -954.224 -954.224], eps: 0.1})
Step:   23700, Reward: [-463.853 -463.853 -463.853] [95.487], Avg: [-500.322 -500.322 -500.322] (0.1000) ({r_i: None, r_t: [-949.868 -949.868 -949.868], eps: 0.1})
Step:   23800, Reward: [-512.587 -512.587 -512.587] [141.576], Avg: [-500.373 -500.373 -500.373] (0.1000) ({r_i: None, r_t: [-1018.392 -1018.392 -1018.392], eps: 0.1})
Step:   23900, Reward: [-501.575 -501.575 -501.575] [100.266], Avg: [-500.378 -500.378 -500.378] (0.1000) ({r_i: None, r_t: [-1041.142 -1041.142 -1041.142], eps: 0.1})
Step:   24000, Reward: [-477.247 -477.247 -477.247] [60.493], Avg: [-500.282 -500.282 -500.282] (0.1000) ({r_i: None, r_t: [-940.537 -940.537 -940.537], eps: 0.1})
Step:   24100, Reward: [-459.904 -459.904 -459.904] [87.927], Avg: [-500.115 -500.115 -500.115] (0.1000) ({r_i: None, r_t: [-936.815 -936.815 -936.815], eps: 0.1})
Step:   24200, Reward: [-502.889 -502.889 -502.889] [80.976], Avg: [-500.127 -500.127 -500.127] (0.1000) ({r_i: None, r_t: [-1020.720 -1020.720 -1020.720], eps: 0.1})
Step:   24300, Reward: [-502.934 -502.934 -502.934] [72.044], Avg: [-500.138 -500.138 -500.138] (0.1000) ({r_i: None, r_t: [-977.133 -977.133 -977.133], eps: 0.1})
Step:   24400, Reward: [-494.711 -494.711 -494.711] [108.053], Avg: [-500.116 -500.116 -500.116] (0.1000) ({r_i: None, r_t: [-941.430 -941.430 -941.430], eps: 0.1})
Step:   24500, Reward: [-494.582 -494.582 -494.582] [88.603], Avg: [-500.093 -500.093 -500.093] (0.1000) ({r_i: None, r_t: [-987.617 -987.617 -987.617], eps: 0.1})
Step:   24600, Reward: [-481.792 -481.792 -481.792] [88.559], Avg: [-500.019 -500.019 -500.019] (0.1000) ({r_i: None, r_t: [-964.159 -964.159 -964.159], eps: 0.1})
Step:   24700, Reward: [-464.262 -464.262 -464.262] [83.911], Avg: [-499.875 -499.875 -499.875] (0.1000) ({r_i: None, r_t: [-934.166 -934.166 -934.166], eps: 0.1})
Step:   24800, Reward: [-490.415 -490.415 -490.415] [49.925], Avg: [-499.837 -499.837 -499.837] (0.1000) ({r_i: None, r_t: [-982.628 -982.628 -982.628], eps: 0.1})
Step:   24900, Reward: [-511.074 -511.074 -511.074] [79.579], Avg: [-499.882 -499.882 -499.882] (0.1000) ({r_i: None, r_t: [-971.811 -971.811 -971.811], eps: 0.1})
Step:   25000, Reward: [-476.127 -476.127 -476.127] [94.660], Avg: [-499.787 -499.787 -499.787] (0.1000) ({r_i: None, r_t: [-924.124 -924.124 -924.124], eps: 0.1})
Step:   25100, Reward: [-470.803 -470.803 -470.803] [82.292], Avg: [-499.672 -499.672 -499.672] (0.1000) ({r_i: None, r_t: [-1007.966 -1007.966 -1007.966], eps: 0.1})
Step:   25200, Reward: [-530.226 -530.226 -530.226] [157.112], Avg: [-499.793 -499.793 -499.793] (0.1000) ({r_i: None, r_t: [-1025.708 -1025.708 -1025.708], eps: 0.1})
Step:   25300, Reward: [-485.585 -485.585 -485.585] [87.634], Avg: [-499.737 -499.737 -499.737] (0.1000) ({r_i: None, r_t: [-998.248 -998.248 -998.248], eps: 0.1})
Step:   25400, Reward: [-512.432 -512.432 -512.432] [145.927], Avg: [-499.787 -499.787 -499.787] (0.1000) ({r_i: None, r_t: [-1009.756 -1009.756 -1009.756], eps: 0.1})
Step:   25500, Reward: [-508.648 -508.648 -508.648] [103.916], Avg: [-499.822 -499.822 -499.822] (0.1000) ({r_i: None, r_t: [-1033.429 -1033.429 -1033.429], eps: 0.1})
Step:   25600, Reward: [-499.778 -499.778 -499.778] [86.702], Avg: [-499.821 -499.821 -499.821] (0.1000) ({r_i: None, r_t: [-949.405 -949.405 -949.405], eps: 0.1})
Step:   25700, Reward: [-551.822 -551.822 -551.822] [130.093], Avg: [-500.023 -500.023 -500.023] (0.1000) ({r_i: None, r_t: [-1073.780 -1073.780 -1073.780], eps: 0.1})
Step:   25800, Reward: [-524.242 -524.242 -524.242] [135.229], Avg: [-500.117 -500.117 -500.117] (0.1000) ({r_i: None, r_t: [-1017.090 -1017.090 -1017.090], eps: 0.1})
Step:   25900, Reward: [-521.160 -521.160 -521.160] [123.113], Avg: [-500.197 -500.197 -500.197] (0.1000) ({r_i: None, r_t: [-984.315 -984.315 -984.315], eps: 0.1})
Step:   26000, Reward: [-508.692 -508.692 -508.692] [94.878], Avg: [-500.230 -500.230 -500.230] (0.1000) ({r_i: None, r_t: [-984.817 -984.817 -984.817], eps: 0.1})
Step:   26100, Reward: [-495.608 -495.608 -495.608] [112.331], Avg: [-500.212 -500.212 -500.212] (0.1000) ({r_i: None, r_t: [-938.910 -938.910 -938.910], eps: 0.1})
Step:   26200, Reward: [-496.269 -496.269 -496.269] [67.887], Avg: [-500.197 -500.197 -500.197] (0.1000) ({r_i: None, r_t: [-1023.725 -1023.725 -1023.725], eps: 0.1})
Step:   26300, Reward: [-506.228 -506.228 -506.228] [95.681], Avg: [-500.220 -500.220 -500.220] (0.1000) ({r_i: None, r_t: [-1009.027 -1009.027 -1009.027], eps: 0.1})
Step:   26400, Reward: [-459.736 -459.736 -459.736] [99.881], Avg: [-500.067 -500.067 -500.067] (0.1000) ({r_i: None, r_t: [-987.601 -987.601 -987.601], eps: 0.1})
Step:   26500, Reward: [-508.819 -508.819 -508.819] [113.947], Avg: [-500.100 -500.100 -500.100] (0.1000) ({r_i: None, r_t: [-1008.942 -1008.942 -1008.942], eps: 0.1})
Step:   26600, Reward: [-453.743 -453.743 -453.743] [70.092], Avg: [-499.927 -499.927 -499.927] (0.1000) ({r_i: None, r_t: [-980.395 -980.395 -980.395], eps: 0.1})
Step:   26700, Reward: [-477.099 -477.099 -477.099] [88.074], Avg: [-499.842 -499.842 -499.842] (0.1000) ({r_i: None, r_t: [-1012.477 -1012.477 -1012.477], eps: 0.1})
Step:   26800, Reward: [-505.401 -505.401 -505.401] [147.595], Avg: [-499.862 -499.862 -499.862] (0.1000) ({r_i: None, r_t: [-1010.561 -1010.561 -1010.561], eps: 0.1})
Step:   26900, Reward: [-492.433 -492.433 -492.433] [94.037], Avg: [-499.835 -499.835 -499.835] (0.1000) ({r_i: None, r_t: [-970.350 -970.350 -970.350], eps: 0.1})
Step:   27000, Reward: [-483.580 -483.580 -483.580] [105.933], Avg: [-499.775 -499.775 -499.775] (0.1000) ({r_i: None, r_t: [-1026.932 -1026.932 -1026.932], eps: 0.1})
Step:   27100, Reward: [-478.059 -478.059 -478.059] [83.359], Avg: [-499.695 -499.695 -499.695] (0.1000) ({r_i: None, r_t: [-968.557 -968.557 -968.557], eps: 0.1})
Step:   27200, Reward: [-511.761 -511.761 -511.761] [148.868], Avg: [-499.739 -499.739 -499.739] (0.1000) ({r_i: None, r_t: [-1008.828 -1008.828 -1008.828], eps: 0.1})
Step:   27300, Reward: [-538.560 -538.560 -538.560] [70.792], Avg: [-499.881 -499.881 -499.881] (0.1000) ({r_i: None, r_t: [-1033.520 -1033.520 -1033.520], eps: 0.1})
Step:   27400, Reward: [-497.152 -497.152 -497.152] [151.661], Avg: [-499.871 -499.871 -499.871] (0.1000) ({r_i: None, r_t: [-938.326 -938.326 -938.326], eps: 0.1})
Step:   27500, Reward: [-527.507 -527.507 -527.507] [119.224], Avg: [-499.971 -499.971 -499.971] (0.1000) ({r_i: None, r_t: [-1006.816 -1006.816 -1006.816], eps: 0.1})
Step:   27600, Reward: [-520.840 -520.840 -520.840] [85.747], Avg: [-500.046 -500.046 -500.046] (0.1000) ({r_i: None, r_t: [-1007.268 -1007.268 -1007.268], eps: 0.1})
Step:   27700, Reward: [-461.525 -461.525 -461.525] [68.114], Avg: [-499.908 -499.908 -499.908] (0.1000) ({r_i: None, r_t: [-967.778 -967.778 -967.778], eps: 0.1})
Step:   27800, Reward: [-478.546 -478.546 -478.546] [104.344], Avg: [-499.831 -499.831 -499.831] (0.1000) ({r_i: None, r_t: [-1011.703 -1011.703 -1011.703], eps: 0.1})
Step:   27900, Reward: [-504.463 -504.463 -504.463] [98.531], Avg: [-499.848 -499.848 -499.848] (0.1000) ({r_i: None, r_t: [-1052.976 -1052.976 -1052.976], eps: 0.1})
Step:   28000, Reward: [-496.027 -496.027 -496.027] [87.526], Avg: [-499.834 -499.834 -499.834] (0.1000) ({r_i: None, r_t: [-943.923 -943.923 -943.923], eps: 0.1})
Step:   28100, Reward: [-475.554 -475.554 -475.554] [117.053], Avg: [-499.748 -499.748 -499.748] (0.1000) ({r_i: None, r_t: [-986.507 -986.507 -986.507], eps: 0.1})
Step:   28200, Reward: [-474.730 -474.730 -474.730] [85.693], Avg: [-499.660 -499.660 -499.660] (0.1000) ({r_i: None, r_t: [-1038.691 -1038.691 -1038.691], eps: 0.1})
Step:   28300, Reward: [-540.413 -540.413 -540.413] [119.267], Avg: [-499.803 -499.803 -499.803] (0.1000) ({r_i: None, r_t: [-1064.745 -1064.745 -1064.745], eps: 0.1})
Step:   28400, Reward: [-479.626 -479.626 -479.626] [120.013], Avg: [-499.732 -499.732 -499.732] (0.1000) ({r_i: None, r_t: [-1043.435 -1043.435 -1043.435], eps: 0.1})
Step:   28500, Reward: [-444.155 -444.155 -444.155] [65.037], Avg: [-499.538 -499.538 -499.538] (0.1000) ({r_i: None, r_t: [-993.716 -993.716 -993.716], eps: 0.1})
Step:   28600, Reward: [-501.185 -501.185 -501.185] [116.195], Avg: [-499.544 -499.544 -499.544] (0.1000) ({r_i: None, r_t: [-974.894 -974.894 -974.894], eps: 0.1})
Step:   28700, Reward: [-480.321 -480.321 -480.321] [97.535], Avg: [-499.477 -499.477 -499.477] (0.1000) ({r_i: None, r_t: [-1027.848 -1027.848 -1027.848], eps: 0.1})
Step:   28800, Reward: [-504.467 -504.467 -504.467] [111.233], Avg: [-499.494 -499.494 -499.494] (0.1000) ({r_i: None, r_t: [-927.828 -927.828 -927.828], eps: 0.1})
Step:   28900, Reward: [-492.816 -492.816 -492.816] [115.780], Avg: [-499.471 -499.471 -499.471] (0.1000) ({r_i: None, r_t: [-1002.253 -1002.253 -1002.253], eps: 0.1})
Step:   29000, Reward: [-506.398 -506.398 -506.398] [132.929], Avg: [-499.495 -499.495 -499.495] (0.1000) ({r_i: None, r_t: [-1056.562 -1056.562 -1056.562], eps: 0.1})
Step:   29100, Reward: [-493.943 -493.943 -493.943] [115.701], Avg: [-499.476 -499.476 -499.476] (0.1000) ({r_i: None, r_t: [-1067.816 -1067.816 -1067.816], eps: 0.1})
Step:   29200, Reward: [-522.885 -522.885 -522.885] [111.511], Avg: [-499.556 -499.556 -499.556] (0.1000) ({r_i: None, r_t: [-984.108 -984.108 -984.108], eps: 0.1})
Step:   29300, Reward: [-522.783 -522.783 -522.783] [93.942], Avg: [-499.635 -499.635 -499.635] (0.1000) ({r_i: None, r_t: [-1037.883 -1037.883 -1037.883], eps: 0.1})
Step:   29400, Reward: [-470.139 -470.139 -470.139] [112.002], Avg: [-499.535 -499.535 -499.535] (0.1000) ({r_i: None, r_t: [-1061.588 -1061.588 -1061.588], eps: 0.1})
Step:   29500, Reward: [-507.776 -507.776 -507.776] [136.620], Avg: [-499.563 -499.563 -499.563] (0.1000) ({r_i: None, r_t: [-1046.010 -1046.010 -1046.010], eps: 0.1})
Step:   29600, Reward: [-496.312 -496.312 -496.312] [115.738], Avg: [-499.552 -499.552 -499.552] (0.1000) ({r_i: None, r_t: [-976.606 -976.606 -976.606], eps: 0.1})
Step:   29700, Reward: [-529.806 -529.806 -529.806] [138.388], Avg: [-499.653 -499.653 -499.653] (0.1000) ({r_i: None, r_t: [-978.259 -978.259 -978.259], eps: 0.1})
Step:   29800, Reward: [-537.846 -537.846 -537.846] [174.016], Avg: [-499.781 -499.781 -499.781] (0.1000) ({r_i: None, r_t: [-1005.231 -1005.231 -1005.231], eps: 0.1})
Step:   29900, Reward: [-477.208 -477.208 -477.208] [67.442], Avg: [-499.706 -499.706 -499.706] (0.1000) ({r_i: None, r_t: [-1036.392 -1036.392 -1036.392], eps: 0.1})
Step:   30000, Reward: [-483.636 -483.636 -483.636] [72.374], Avg: [-499.652 -499.652 -499.652] (0.1000) ({r_i: None, r_t: [-969.145 -969.145 -969.145], eps: 0.1})
Step:   30100, Reward: [-475.384 -475.384 -475.384] [75.313], Avg: [-499.572 -499.572 -499.572] (0.1000) ({r_i: None, r_t: [-995.521 -995.521 -995.521], eps: 0.1})
Step:   30200, Reward: [-435.329 -435.329 -435.329] [87.415], Avg: [-499.360 -499.360 -499.360] (0.1000) ({r_i: None, r_t: [-963.218 -963.218 -963.218], eps: 0.1})
Step:   30300, Reward: [-479.583 -479.583 -479.583] [106.008], Avg: [-499.295 -499.295 -499.295] (0.1000) ({r_i: None, r_t: [-1044.259 -1044.259 -1044.259], eps: 0.1})
Step:   30400, Reward: [-475.821 -475.821 -475.821] [64.543], Avg: [-499.218 -499.218 -499.218] (0.1000) ({r_i: None, r_t: [-1053.617 -1053.617 -1053.617], eps: 0.1})
Step:   30500, Reward: [-495.017 -495.017 -495.017] [91.457], Avg: [-499.204 -499.204 -499.204] (0.1000) ({r_i: None, r_t: [-981.788 -981.788 -981.788], eps: 0.1})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		# q_value = torch.zeros_like(q_next_value)
		# for t in reversed(range(rewards.size(1))):
		# 	q_value[:, t] = self.critic_local(critic_inputs[:,t])
		# 	q_taken = torch.gather(q_value[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
		# 	critic_error = (q_taken - q_target[:, t].detach())
		# 	critic_loss = critic_error.pow(2).mean()
		# 	self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		q_value = self.critic_local(critic_inputs)
		q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		critic_error = (q_taken - q_target.detach())
		critic_loss = critic_error.pow(2).mean()
		self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-508.255 -508.255 -508.255] [114.684], Avg: [-508.255 -508.255 -508.255] (1.0000) ({r_i: None, r_t: [-9.490 -9.490 -9.490], eps: 1.0})
Step:   30600, Reward: [-527.001 -527.001 -527.001] [99.292], Avg: [-499.295 -499.295 -499.295] (0.1000) ({r_i: None, r_t: [-1005.412 -1005.412 -1005.412], eps: 0.1})
Step:     100, Reward: [-448.927 -448.927 -448.927] [77.881], Avg: [-478.591 -478.591 -478.591] (0.9900) ({r_i: None, r_t: [-1004.535 -1004.535 -1004.535], eps: 0.99})
Step:   30700, Reward: [-506.927 -506.927 -506.927] [128.143], Avg: [-499.320 -499.320 -499.320] (0.1000) ({r_i: None, r_t: [-997.121 -997.121 -997.121], eps: 0.1})
Step:     200, Reward: [-520.117 -520.117 -520.117] [95.113], Avg: [-492.433 -492.433 -492.433] (0.9801) ({r_i: None, r_t: [-1025.422 -1025.422 -1025.422], eps: 0.98})
Step:   30800, Reward: [-524.635 -524.635 -524.635] [102.947], Avg: [-499.402 -499.402 -499.402] (0.1000) ({r_i: None, r_t: [-966.676 -966.676 -966.676], eps: 0.1})
Step:     300, Reward: [-477.147 -477.147 -477.147] [82.072], Avg: [-488.612 -488.612 -488.612] (0.9704) ({r_i: None, r_t: [-928.458 -928.458 -928.458], eps: 0.97})
Step:   30900, Reward: [-541.092 -541.092 -541.092] [106.044], Avg: [-499.536 -499.536 -499.536] (0.1000) ({r_i: None, r_t: [-986.191 -986.191 -986.191], eps: 0.1})
Step:     400, Reward: [-484.244 -484.244 -484.244] [102.376], Avg: [-487.738 -487.738 -487.738] (0.9607) ({r_i: None, r_t: [-971.591 -971.591 -971.591], eps: 0.961})
Step:   31000, Reward: [-490.192 -490.192 -490.192] [94.096], Avg: [-499.506 -499.506 -499.506] (0.1000) ({r_i: None, r_t: [-1053.766 -1053.766 -1053.766], eps: 0.1})
Step:     500, Reward: [-477.955 -477.955 -477.955] [40.181], Avg: [-486.108 -486.108 -486.108] (0.9511) ({r_i: None, r_t: [-1022.568 -1022.568 -1022.568], eps: 0.951})
Step:   31100, Reward: [-455.370 -455.370 -455.370] [81.800], Avg: [-499.365 -499.365 -499.365] (0.1000) ({r_i: None, r_t: [-965.081 -965.081 -965.081], eps: 0.1})
Step:     600, Reward: [-483.059 -483.059 -483.059] [83.044], Avg: [-485.672 -485.672 -485.672] (0.9416) ({r_i: None, r_t: [-967.592 -967.592 -967.592], eps: 0.942})
Step:   31200, Reward: [-478.014 -478.014 -478.014] [145.341], Avg: [-499.296 -499.296 -499.296] (0.1000) ({r_i: None, r_t: [-1066.461 -1066.461 -1066.461], eps: 0.1})
Step:     700, Reward: [-428.631 -428.631 -428.631] [79.193], Avg: [-478.542 -478.542 -478.542] (0.9322) ({r_i: None, r_t: [-909.084 -909.084 -909.084], eps: 0.932})
Step:   31300, Reward: [-504.953 -504.953 -504.953] [77.232], Avg: [-499.314 -499.314 -499.314] (0.1000) ({r_i: None, r_t: [-914.037 -914.037 -914.037], eps: 0.1})
Step:     800, Reward: [-501.924 -501.924 -501.924] [123.823], Avg: [-481.140 -481.140 -481.140] (0.9229) ({r_i: None, r_t: [-980.366 -980.366 -980.366], eps: 0.923})
Step:   31400, Reward: [-481.900 -481.900 -481.900] [69.930], Avg: [-499.259 -499.259 -499.259] (0.1000) ({r_i: None, r_t: [-968.338 -968.338 -968.338], eps: 0.1})
Step:     900, Reward: [-516.403 -516.403 -516.403] [133.318], Avg: [-484.666 -484.666 -484.666] (0.9137) ({r_i: None, r_t: [-958.089 -958.089 -958.089], eps: 0.914})
Step:   31500, Reward: [-485.522 -485.522 -485.522] [133.210], Avg: [-499.216 -499.216 -499.216] (0.1000) ({r_i: None, r_t: [-1014.593 -1014.593 -1014.593], eps: 0.1})
Step:    1000, Reward: [-477.250 -477.250 -477.250] [89.438], Avg: [-483.992 -483.992 -483.992] (0.9046) ({r_i: None, r_t: [-1002.724 -1002.724 -1002.724], eps: 0.905})
Step:   31600, Reward: [-487.800 -487.800 -487.800] [125.143], Avg: [-499.180 -499.180 -499.180] (0.1000) ({r_i: None, r_t: [-1047.291 -1047.291 -1047.291], eps: 0.1})
Step:   31700, Reward: [-475.205 -475.205 -475.205] [120.686], Avg: [-499.104 -499.104 -499.104] (0.1000) ({r_i: None, r_t: [-1017.815 -1017.815 -1017.815], eps: 0.1})
Step:    1100, Reward: [-521.026 -521.026 -521.026] [122.628], Avg: [-487.078 -487.078 -487.078] (0.8956) ({r_i: None, r_t: [-960.260 -960.260 -960.260], eps: 0.896})
Step:   31800, Reward: [-493.701 -493.701 -493.701] [127.226], Avg: [-499.087 -499.087 -499.087] (0.1000) ({r_i: None, r_t: [-989.348 -989.348 -989.348], eps: 0.1})
Step:    1200, Reward: [-504.162 -504.162 -504.162] [88.341], Avg: [-488.392 -488.392 -488.392] (0.8867) ({r_i: None, r_t: [-972.012 -972.012 -972.012], eps: 0.887})
Step:   31900, Reward: [-504.343 -504.343 -504.343] [100.571], Avg: [-499.104 -499.104 -499.104] (0.1000) ({r_i: None, r_t: [-967.606 -967.606 -967.606], eps: 0.1})
Step:    1300, Reward: [-482.609 -482.609 -482.609] [133.423], Avg: [-487.979 -487.979 -487.979] (0.8778) ({r_i: None, r_t: [-968.194 -968.194 -968.194], eps: 0.878})
Step:   32000, Reward: [-480.739 -480.739 -480.739] [111.815], Avg: [-499.046 -499.046 -499.046] (0.1000) ({r_i: None, r_t: [-988.027 -988.027 -988.027], eps: 0.1})
Step:    1400, Reward: [-510.806 -510.806 -510.806] [102.393], Avg: [-489.501 -489.501 -489.501] (0.8691) ({r_i: None, r_t: [-1019.222 -1019.222 -1019.222], eps: 0.869})
Step:   32100, Reward: [-478.867 -478.867 -478.867] [72.837], Avg: [-498.984 -498.984 -498.984] (0.1000) ({r_i: None, r_t: [-1012.419 -1012.419 -1012.419], eps: 0.1})
Step:    1500, Reward: [-486.399 -486.399 -486.399] [90.469], Avg: [-489.307 -489.307 -489.307] (0.8604) ({r_i: None, r_t: [-1016.860 -1016.860 -1016.860], eps: 0.86})
Step:   32200, Reward: [-480.425 -480.425 -480.425] [111.634], Avg: [-498.926 -498.926 -498.926] (0.1000) ({r_i: None, r_t: [-957.022 -957.022 -957.022], eps: 0.1})
Step:    1600, Reward: [-485.425 -485.425 -485.425] [71.573], Avg: [-489.079 -489.079 -489.079] (0.8518) ({r_i: None, r_t: [-987.083 -987.083 -987.083], eps: 0.852})
Step:   32300, Reward: [-543.372 -543.372 -543.372] [131.009], Avg: [-499.064 -499.064 -499.064] (0.1000) ({r_i: None, r_t: [-938.660 -938.660 -938.660], eps: 0.1})
Step:    1700, Reward: [-524.510 -524.510 -524.510] [100.767], Avg: [-491.047 -491.047 -491.047] (0.8433) ({r_i: None, r_t: [-959.987 -959.987 -959.987], eps: 0.843})
Step:   32400, Reward: [-506.724 -506.724 -506.724] [131.155], Avg: [-499.087 -499.087 -499.087] (0.1000) ({r_i: None, r_t: [-1020.496 -1020.496 -1020.496], eps: 0.1})
Step:    1800, Reward: [-487.687 -487.687 -487.687] [119.644], Avg: [-490.870 -490.870 -490.870] (0.8349) ({r_i: None, r_t: [-966.553 -966.553 -966.553], eps: 0.835})
Step:   32500, Reward: [-516.449 -516.449 -516.449] [137.254], Avg: [-499.140 -499.140 -499.140] (0.1000) ({r_i: None, r_t: [-929.953 -929.953 -929.953], eps: 0.1})
Step:    1900, Reward: [-502.983 -502.983 -502.983] [78.189], Avg: [-491.476 -491.476 -491.476] (0.8266) ({r_i: None, r_t: [-997.280 -997.280 -997.280], eps: 0.827})
Step:   32600, Reward: [-471.304 -471.304 -471.304] [82.350], Avg: [-499.055 -499.055 -499.055] (0.1000) ({r_i: None, r_t: [-983.665 -983.665 -983.665], eps: 0.1})
Step:    2000, Reward: [-465.586 -465.586 -465.586] [79.301], Avg: [-490.243 -490.243 -490.243] (0.8183) ({r_i: None, r_t: [-976.636 -976.636 -976.636], eps: 0.818})
Step:   32700, Reward: [-463.553 -463.553 -463.553] [62.412], Avg: [-498.947 -498.947 -498.947] (0.1000) ({r_i: None, r_t: [-1010.757 -1010.757 -1010.757], eps: 0.1})
Step:    2100, Reward: [-490.623 -490.623 -490.623] [160.606], Avg: [-490.260 -490.260 -490.260] (0.8102) ({r_i: None, r_t: [-964.871 -964.871 -964.871], eps: 0.81})
Step:   32800, Reward: [-502.219 -502.219 -502.219] [116.169], Avg: [-498.957 -498.957 -498.957] (0.1000) ({r_i: None, r_t: [-915.974 -915.974 -915.974], eps: 0.1})
Step:    2200, Reward: [-539.473 -539.473 -539.473] [96.025], Avg: [-492.400 -492.400 -492.400] (0.8021) ({r_i: None, r_t: [-962.899 -962.899 -962.899], eps: 0.802})
Step:   32900, Reward: [-499.983 -499.983 -499.983] [128.938], Avg: [-498.960 -498.960 -498.960] (0.1000) ({r_i: None, r_t: [-1057.821 -1057.821 -1057.821], eps: 0.1})
Step:    2300, Reward: [-532.373 -532.373 -532.373] [181.453], Avg: [-494.066 -494.066 -494.066] (0.7941) ({r_i: None, r_t: [-934.811 -934.811 -934.811], eps: 0.794})
Step:   33000, Reward: [-464.880 -464.880 -464.880] [65.349], Avg: [-498.857 -498.857 -498.857] (0.1000) ({r_i: None, r_t: [-931.650 -931.650 -931.650], eps: 0.1})
Step:    2400, Reward: [-465.824 -465.824 -465.824] [115.344], Avg: [-492.936 -492.936 -492.936] (0.7862) ({r_i: None, r_t: [-964.059 -964.059 -964.059], eps: 0.786})
Step:   33100, Reward: [-490.187 -490.187 -490.187] [136.776], Avg: [-498.831 -498.831 -498.831] (0.1000) ({r_i: None, r_t: [-999.874 -999.874 -999.874], eps: 0.1})
Step:    2500, Reward: [-450.070 -450.070 -450.070] [94.036], Avg: [-491.287 -491.287 -491.287] (0.7783) ({r_i: None, r_t: [-958.575 -958.575 -958.575], eps: 0.778})
Step:   33200, Reward: [-464.498 -464.498 -464.498] [123.297], Avg: [-498.728 -498.728 -498.728] (0.1000) ({r_i: None, r_t: [-1071.664 -1071.664 -1071.664], eps: 0.1})
Step:    2600, Reward: [-446.722 -446.722 -446.722] [50.565], Avg: [-489.637 -489.637 -489.637] (0.7705) ({r_i: None, r_t: [-926.348 -926.348 -926.348], eps: 0.771})
Step:   33300, Reward: [-486.460 -486.460 -486.460] [145.151], Avg: [-498.691 -498.691 -498.691] (0.1000) ({r_i: None, r_t: [-992.338 -992.338 -992.338], eps: 0.1})
Step:    2700, Reward: [-439.181 -439.181 -439.181] [56.583], Avg: [-487.835 -487.835 -487.835] (0.7629) ({r_i: None, r_t: [-1037.434 -1037.434 -1037.434], eps: 0.763})
Step:   33400, Reward: [-465.810 -465.810 -465.810] [63.994], Avg: [-498.593 -498.593 -498.593] (0.1000) ({r_i: None, r_t: [-1029.294 -1029.294 -1029.294], eps: 0.1})
Step:    2800, Reward: [-472.886 -472.886 -472.886] [82.121], Avg: [-487.319 -487.319 -487.319] (0.7553) ({r_i: None, r_t: [-1003.525 -1003.525 -1003.525], eps: 0.755})
Step:   33500, Reward: [-541.166 -541.166 -541.166] [152.256], Avg: [-498.720 -498.720 -498.720] (0.1000) ({r_i: None, r_t: [-1077.730 -1077.730 -1077.730], eps: 0.1})
Step:    2900, Reward: [-479.077 -479.077 -479.077] [63.864], Avg: [-487.044 -487.044 -487.044] (0.7477) ({r_i: None, r_t: [-966.497 -966.497 -966.497], eps: 0.748})
Step:   33600, Reward: [-492.436 -492.436 -492.436] [93.304], Avg: [-498.701 -498.701 -498.701] (0.1000) ({r_i: None, r_t: [-1083.405 -1083.405 -1083.405], eps: 0.1})
Step:    3000, Reward: [-487.712 -487.712 -487.712] [105.691], Avg: [-487.066 -487.066 -487.066] (0.7403) ({r_i: None, r_t: [-956.959 -956.959 -956.959], eps: 0.74})
Step:   33700, Reward: [-592.082 -592.082 -592.082] [173.309], Avg: [-498.977 -498.977 -498.977] (0.1000) ({r_i: None, r_t: [-991.206 -991.206 -991.206], eps: 0.1})
Step:    3100, Reward: [-448.831 -448.831 -448.831] [72.435], Avg: [-485.871 -485.871 -485.871] (0.7329) ({r_i: None, r_t: [-966.063 -966.063 -966.063], eps: 0.733})
Step:   33800, Reward: [-483.467 -483.467 -483.467] [151.493], Avg: [-498.932 -498.932 -498.932] (0.1000) ({r_i: None, r_t: [-1012.167 -1012.167 -1012.167], eps: 0.1})
Step:    3200, Reward: [-465.208 -465.208 -465.208] [112.364], Avg: [-485.245 -485.245 -485.245] (0.7256) ({r_i: None, r_t: [-916.485 -916.485 -916.485], eps: 0.726})
Step:   33900, Reward: [-472.754 -472.754 -472.754] [105.112], Avg: [-498.855 -498.855 -498.855] (0.1000) ({r_i: None, r_t: [-1053.499 -1053.499 -1053.499], eps: 0.1})
Step:    3300, Reward: [-473.094 -473.094 -473.094] [84.642], Avg: [-484.888 -484.888 -484.888] (0.7183) ({r_i: None, r_t: [-965.943 -965.943 -965.943], eps: 0.718})
Step:   34000, Reward: [-499.910 -499.910 -499.910] [122.560], Avg: [-498.858 -498.858 -498.858] (0.1000) ({r_i: None, r_t: [-982.088 -982.088 -982.088], eps: 0.1})
Step:    3400, Reward: [-468.381 -468.381 -468.381] [61.442], Avg: [-484.416 -484.416 -484.416] (0.7112) ({r_i: None, r_t: [-971.028 -971.028 -971.028], eps: 0.711})
Step:   34100, Reward: [-497.739 -497.739 -497.739] [85.842], Avg: [-498.854 -498.854 -498.854] (0.1000) ({r_i: None, r_t: [-1035.188 -1035.188 -1035.188], eps: 0.1})
Step:   34200, Reward: [-550.541 -550.541 -550.541] [159.524], Avg: [-499.005 -499.005 -499.005] (0.1000) ({r_i: None, r_t: [-994.758 -994.758 -994.758], eps: 0.1})
Step:    3500, Reward: [-450.481 -450.481 -450.481] [82.913], Avg: [-483.473 -483.473 -483.473] (0.7041) ({r_i: None, r_t: [-986.471 -986.471 -986.471], eps: 0.704})
Step:   34300, Reward: [-503.012 -503.012 -503.012] [94.150], Avg: [-499.017 -499.017 -499.017] (0.1000) ({r_i: None, r_t: [-1089.817 -1089.817 -1089.817], eps: 0.1})
Step:    3600, Reward: [-448.289 -448.289 -448.289] [82.819], Avg: [-482.522 -482.522 -482.522] (0.6970) ({r_i: None, r_t: [-946.162 -946.162 -946.162], eps: 0.697})
Step:   34400, Reward: [-554.242 -554.242 -554.242] [154.280], Avg: [-499.177 -499.177 -499.177] (0.1000) ({r_i: None, r_t: [-1096.627 -1096.627 -1096.627], eps: 0.1})
Step:    3700, Reward: [-427.962 -427.962 -427.962] [72.867], Avg: [-481.087 -481.087 -481.087] (0.6901) ({r_i: None, r_t: [-954.570 -954.570 -954.570], eps: 0.69})
Step:   34500, Reward: [-509.016 -509.016 -509.016] [119.933], Avg: [-499.205 -499.205 -499.205] (0.1000) ({r_i: None, r_t: [-1018.910 -1018.910 -1018.910], eps: 0.1})
Step:    3800, Reward: [-464.550 -464.550 -464.550] [60.406], Avg: [-480.663 -480.663 -480.663] (0.6832) ({r_i: None, r_t: [-953.171 -953.171 -953.171], eps: 0.683})
Step:   34600, Reward: [-511.594 -511.594 -511.594] [107.625], Avg: [-499.241 -499.241 -499.241] (0.1000) ({r_i: None, r_t: [-1007.921 -1007.921 -1007.921], eps: 0.1})
Step:    3900, Reward: [-458.100 -458.100 -458.100] [112.527], Avg: [-480.099 -480.099 -480.099] (0.6764) ({r_i: None, r_t: [-1001.167 -1001.167 -1001.167], eps: 0.676})
Step:   34700, Reward: [-483.780 -483.780 -483.780] [106.254], Avg: [-499.197 -499.197 -499.197] (0.1000) ({r_i: None, r_t: [-1022.878 -1022.878 -1022.878], eps: 0.1})
Step:    4000, Reward: [-444.181 -444.181 -444.181] [50.967], Avg: [-479.223 -479.223 -479.223] (0.6696) ({r_i: None, r_t: [-992.815 -992.815 -992.815], eps: 0.67})
Step:   34800, Reward: [-509.792 -509.792 -509.792] [102.208], Avg: [-499.227 -499.227 -499.227] (0.1000) ({r_i: None, r_t: [-1047.867 -1047.867 -1047.867], eps: 0.1})
Step:    4100, Reward: [-456.289 -456.289 -456.289] [86.898], Avg: [-478.676 -478.676 -478.676] (0.6630) ({r_i: None, r_t: [-956.530 -956.530 -956.530], eps: 0.663})
Step:   34900, Reward: [-474.537 -474.537 -474.537] [94.156], Avg: [-499.156 -499.156 -499.156] (0.1000) ({r_i: None, r_t: [-1054.976 -1054.976 -1054.976], eps: 0.1})
Step:    4200, Reward: [-456.069 -456.069 -456.069] [63.017], Avg: [-478.151 -478.151 -478.151] (0.6564) ({r_i: None, r_t: [-968.608 -968.608 -968.608], eps: 0.656})
Step:   35000, Reward: [-492.833 -492.833 -492.833] [133.436], Avg: [-499.138 -499.138 -499.138] (0.1000) ({r_i: None, r_t: [-961.963 -961.963 -961.963], eps: 0.1})
Step:    4300, Reward: [-469.408 -469.408 -469.408] [82.143], Avg: [-477.952 -477.952 -477.952] (0.6498) ({r_i: None, r_t: [-909.430 -909.430 -909.430], eps: 0.65})
Step:   35100, Reward: [-510.246 -510.246 -510.246] [146.772], Avg: [-499.170 -499.170 -499.170] (0.1000) ({r_i: None, r_t: [-1131.792 -1131.792 -1131.792], eps: 0.1})
Step:    4400, Reward: [-489.151 -489.151 -489.151] [79.807], Avg: [-478.201 -478.201 -478.201] (0.6433) ({r_i: None, r_t: [-939.840 -939.840 -939.840], eps: 0.643})
Step:   35200, Reward: [-498.997 -498.997 -498.997] [69.266], Avg: [-499.169 -499.169 -499.169] (0.1000) ({r_i: None, r_t: [-1040.776 -1040.776 -1040.776], eps: 0.1})
Step:    4500, Reward: [-443.810 -443.810 -443.810] [69.341], Avg: [-477.453 -477.453 -477.453] (0.6369) ({r_i: None, r_t: [-908.893 -908.893 -908.893], eps: 0.637})
Step:   35300, Reward: [-575.245 -575.245 -575.245] [129.472], Avg: [-499.384 -499.384 -499.384] (0.1000) ({r_i: None, r_t: [-1018.654 -1018.654 -1018.654], eps: 0.1})
Step:    4600, Reward: [-494.640 -494.640 -494.640] [108.168], Avg: [-477.819 -477.819 -477.819] (0.6306) ({r_i: None, r_t: [-926.183 -926.183 -926.183], eps: 0.631})
Step:   35400, Reward: [-494.070 -494.070 -494.070] [104.883], Avg: [-499.369 -499.369 -499.369] (0.1000) ({r_i: None, r_t: [-1010.942 -1010.942 -1010.942], eps: 0.1})
Step:    4700, Reward: [-486.383 -486.383 -486.383] [92.074], Avg: [-477.997 -477.997 -477.997] (0.6243) ({r_i: None, r_t: [-966.656 -966.656 -966.656], eps: 0.624})
Step:   35500, Reward: [-472.704 -472.704 -472.704] [83.926], Avg: [-499.294 -499.294 -499.294] (0.1000) ({r_i: None, r_t: [-990.741 -990.741 -990.741], eps: 0.1})
Step:    4800, Reward: [-480.472 -480.472 -480.472] [82.536], Avg: [-478.048 -478.048 -478.048] (0.6180) ({r_i: None, r_t: [-946.025 -946.025 -946.025], eps: 0.618})
Step:   35600, Reward: [-512.985 -512.985 -512.985] [112.280], Avg: [-499.333 -499.333 -499.333] (0.1000) ({r_i: None, r_t: [-1091.453 -1091.453 -1091.453], eps: 0.1})
Step:    4900, Reward: [-454.856 -454.856 -454.856] [68.447], Avg: [-477.584 -477.584 -477.584] (0.6119) ({r_i: None, r_t: [-993.651 -993.651 -993.651], eps: 0.612})
Step:   35700, Reward: [-502.110 -502.110 -502.110] [91.280], Avg: [-499.341 -499.341 -499.341] (0.1000) ({r_i: None, r_t: [-972.272 -972.272 -972.272], eps: 0.1})
Step:    5000, Reward: [-477.269 -477.269 -477.269] [96.655], Avg: [-477.578 -477.578 -477.578] (0.6058) ({r_i: None, r_t: [-1002.644 -1002.644 -1002.644], eps: 0.606})
Step:   35800, Reward: [-534.061 -534.061 -534.061] [83.334], Avg: [-499.437 -499.437 -499.437] (0.1000) ({r_i: None, r_t: [-1021.773 -1021.773 -1021.773], eps: 0.1})
Step:    5100, Reward: [-499.911 -499.911 -499.911] [126.468], Avg: [-478.007 -478.007 -478.007] (0.5997) ({r_i: None, r_t: [-991.162 -991.162 -991.162], eps: 0.6})
Step:   35900, Reward: [-573.730 -573.730 -573.730] [179.633], Avg: [-499.644 -499.644 -499.644] (0.1000) ({r_i: None, r_t: [-1149.990 -1149.990 -1149.990], eps: 0.1})
Step:    5200, Reward: [-544.423 -544.423 -544.423] [109.853], Avg: [-479.260 -479.260 -479.260] (0.5937) ({r_i: None, r_t: [-993.613 -993.613 -993.613], eps: 0.594})
Step:   36000, Reward: [-539.983 -539.983 -539.983] [136.268], Avg: [-499.755 -499.755 -499.755] (0.1000) ({r_i: None, r_t: [-1043.501 -1043.501 -1043.501], eps: 0.1})
Step:    5300, Reward: [-548.747 -548.747 -548.747] [127.560], Avg: [-480.547 -480.547 -480.547] (0.5878) ({r_i: None, r_t: [-1040.856 -1040.856 -1040.856], eps: 0.588})
Step:   36100, Reward: [-575.617 -575.617 -575.617] [83.121], Avg: [-499.965 -499.965 -499.965] (0.1000) ({r_i: None, r_t: [-1028.272 -1028.272 -1028.272], eps: 0.1})
Step:    5400, Reward: [-586.822 -586.822 -586.822] [122.555], Avg: [-482.479 -482.479 -482.479] (0.5820) ({r_i: None, r_t: [-1000.892 -1000.892 -1000.892], eps: 0.582})
Step:   36200, Reward: [-531.966 -531.966 -531.966] [101.267], Avg: [-500.053 -500.053 -500.053] (0.1000) ({r_i: None, r_t: [-1103.950 -1103.950 -1103.950], eps: 0.1})
Step:    5500, Reward: [-584.038 -584.038 -584.038] [104.785], Avg: [-484.293 -484.293 -484.293] (0.5762) ({r_i: None, r_t: [-1066.545 -1066.545 -1066.545], eps: 0.576})
Step:   36300, Reward: [-485.718 -485.718 -485.718] [95.206], Avg: [-500.014 -500.014 -500.014] (0.1000) ({r_i: None, r_t: [-1125.125 -1125.125 -1125.125], eps: 0.1})
Step:    5600, Reward: [-573.684 -573.684 -573.684] [143.113], Avg: [-485.861 -485.861 -485.861] (0.5704) ({r_i: None, r_t: [-1098.514 -1098.514 -1098.514], eps: 0.57})
Step:   36400, Reward: [-535.564 -535.564 -535.564] [139.587], Avg: [-500.111 -500.111 -500.111] (0.1000) ({r_i: None, r_t: [-1078.104 -1078.104 -1078.104], eps: 0.1})
Step:    5700, Reward: [-624.190 -624.190 -624.190] [138.045], Avg: [-488.246 -488.246 -488.246] (0.5647) ({r_i: None, r_t: [-1207.530 -1207.530 -1207.530], eps: 0.565})
Step:   36500, Reward: [-521.850 -521.850 -521.850] [111.440], Avg: [-500.171 -500.171 -500.171] (0.1000) ({r_i: None, r_t: [-1072.404 -1072.404 -1072.404], eps: 0.1})
Step:    5800, Reward: [-696.660 -696.660 -696.660] [156.958], Avg: [-491.779 -491.779 -491.779] (0.5591) ({r_i: None, r_t: [-1282.640 -1282.640 -1282.640], eps: 0.559})
Step:   36600, Reward: [-576.668 -576.668 -576.668] [137.024], Avg: [-500.379 -500.379 -500.379] (0.1000) ({r_i: None, r_t: [-1105.516 -1105.516 -1105.516], eps: 0.1})
Step:    5900, Reward: [-587.734 -587.734 -587.734] [137.758], Avg: [-493.378 -493.378 -493.378] (0.5535) ({r_i: None, r_t: [-1257.541 -1257.541 -1257.541], eps: 0.554})
Step:   36700, Reward: [-546.923 -546.923 -546.923] [121.403], Avg: [-500.505 -500.505 -500.505] (0.1000) ({r_i: None, r_t: [-1086.617 -1086.617 -1086.617], eps: 0.1})
Step:    6000, Reward: [-705.511 -705.511 -705.511] [166.473], Avg: [-496.856 -496.856 -496.856] (0.5480) ({r_i: None, r_t: [-1423.263 -1423.263 -1423.263], eps: 0.548})
Step:   36800, Reward: [-550.664 -550.664 -550.664] [81.635], Avg: [-500.641 -500.641 -500.641] (0.1000) ({r_i: None, r_t: [-1099.840 -1099.840 -1099.840], eps: 0.1})
Step:    6100, Reward: [-734.526 -734.526 -734.526] [197.693], Avg: [-500.689 -500.689 -500.689] (0.5425) ({r_i: None, r_t: [-1477.268 -1477.268 -1477.268], eps: 0.543})
Step:   36900, Reward: [-541.259 -541.259 -541.259] [124.215], Avg: [-500.751 -500.751 -500.751] (0.1000) ({r_i: None, r_t: [-1096.750 -1096.750 -1096.750], eps: 0.1})
Step:    6200, Reward: [-782.338 -782.338 -782.338] [283.537], Avg: [-505.160 -505.160 -505.160] (0.5371) ({r_i: None, r_t: [-1486.346 -1486.346 -1486.346], eps: 0.537})
Step:   37000, Reward: [-504.860 -504.860 -504.860] [127.120], Avg: [-500.762 -500.762 -500.762] (0.1000) ({r_i: None, r_t: [-1143.196 -1143.196 -1143.196], eps: 0.1})
Step:    6300, Reward: [-771.138 -771.138 -771.138] [167.238], Avg: [-509.315 -509.315 -509.315] (0.5318) ({r_i: None, r_t: [-1601.034 -1601.034 -1601.034], eps: 0.532})
Step:   37100, Reward: [-529.230 -529.230 -529.230] [115.035], Avg: [-500.839 -500.839 -500.839] (0.1000) ({r_i: None, r_t: [-1152.775 -1152.775 -1152.775], eps: 0.1})
Step:   37200, Reward: [-567.774 -567.774 -567.774] [125.410], Avg: [-501.018 -501.018 -501.018] (0.1000) ({r_i: None, r_t: [-1123.783 -1123.783 -1123.783], eps: 0.1})
Step:    6400, Reward: [-806.126 -806.126 -806.126] [243.427], Avg: [-513.882 -513.882 -513.882] (0.5264) ({r_i: None, r_t: [-1696.991 -1696.991 -1696.991], eps: 0.526})
Step:   37300, Reward: [-584.494 -584.494 -584.494] [129.580], Avg: [-501.241 -501.241 -501.241] (0.1000) ({r_i: None, r_t: [-1107.702 -1107.702 -1107.702], eps: 0.1})
Step:    6500, Reward: [-825.870 -825.870 -825.870] [209.588], Avg: [-518.609 -518.609 -518.609] (0.5212) ({r_i: None, r_t: [-1837.341 -1837.341 -1837.341], eps: 0.521})
Step:   37400, Reward: [-547.906 -547.906 -547.906] [102.724], Avg: [-501.366 -501.366 -501.366] (0.1000) ({r_i: None, r_t: [-1255.560 -1255.560 -1255.560], eps: 0.1})
Step:    6600, Reward: [-896.736 -896.736 -896.736] [220.529], Avg: [-524.253 -524.253 -524.253] (0.5160) ({r_i: None, r_t: [-1801.783 -1801.783 -1801.783], eps: 0.516})
Step:   37500, Reward: [-571.815 -571.815 -571.815] [73.871], Avg: [-501.553 -501.553 -501.553] (0.1000) ({r_i: None, r_t: [-992.357 -992.357 -992.357], eps: 0.1})
Step:    6700, Reward: [-974.864 -974.864 -974.864] [174.310], Avg: [-530.879 -530.879 -530.879] (0.5108) ({r_i: None, r_t: [-1708.153 -1708.153 -1708.153], eps: 0.511})
Step:   37600, Reward: [-582.857 -582.857 -582.857] [167.645], Avg: [-501.769 -501.769 -501.769] (0.1000) ({r_i: None, r_t: [-1104.402 -1104.402 -1104.402], eps: 0.1})
Step:    6800, Reward: [-852.259 -852.259 -852.259] [224.738], Avg: [-535.537 -535.537 -535.537] (0.5058) ({r_i: None, r_t: [-1800.606 -1800.606 -1800.606], eps: 0.506})
Step:   37700, Reward: [-603.913 -603.913 -603.913] [134.168], Avg: [-502.039 -502.039 -502.039] (0.1000) ({r_i: None, r_t: [-1090.646 -1090.646 -1090.646], eps: 0.1})
Step:    6900, Reward: [-828.746 -828.746 -828.746] [177.830], Avg: [-539.726 -539.726 -539.726] (0.5007) ({r_i: None, r_t: [-1960.879 -1960.879 -1960.879], eps: 0.501})
Step:   37800, Reward: [-573.600 -573.600 -573.600] [114.600], Avg: [-502.228 -502.228 -502.228] (0.1000) ({r_i: None, r_t: [-1205.007 -1205.007 -1205.007], eps: 0.1})
Step:    7000, Reward: [-947.240 -947.240 -947.240] [273.618], Avg: [-545.465 -545.465 -545.465] (0.4957) ({r_i: None, r_t: [-1876.224 -1876.224 -1876.224], eps: 0.496})
Step:   37900, Reward: [-575.753 -575.753 -575.753] [161.855], Avg: [-502.421 -502.421 -502.421] (0.1000) ({r_i: None, r_t: [-1143.972 -1143.972 -1143.972], eps: 0.1})
Step:    7100, Reward: [-933.788 -933.788 -933.788] [235.171], Avg: [-550.859 -550.859 -550.859] (0.4908) ({r_i: None, r_t: [-1978.026 -1978.026 -1978.026], eps: 0.491})
Step:   38000, Reward: [-620.541 -620.541 -620.541] [158.462], Avg: [-502.731 -502.731 -502.731] (0.1000) ({r_i: None, r_t: [-1181.945 -1181.945 -1181.945], eps: 0.1})
Step:    7200, Reward: [-1013.183 -1013.183 -1013.183] [226.870], Avg: [-557.192 -557.192 -557.192] (0.4859) ({r_i: None, r_t: [-1990.865 -1990.865 -1990.865], eps: 0.486})
Step:   38100, Reward: [-632.498 -632.498 -632.498] [202.275], Avg: [-503.071 -503.071 -503.071] (0.1000) ({r_i: None, r_t: [-1124.291 -1124.291 -1124.291], eps: 0.1})
Step:    7300, Reward: [-1057.034 -1057.034 -1057.034] [248.505], Avg: [-563.946 -563.946 -563.946] (0.4810) ({r_i: None, r_t: [-1897.026 -1897.026 -1897.026], eps: 0.481})
Step:   38200, Reward: [-575.871 -575.871 -575.871] [130.243], Avg: [-503.261 -503.261 -503.261] (0.1000) ({r_i: None, r_t: [-1200.933 -1200.933 -1200.933], eps: 0.1})
Step:    7400, Reward: [-969.989 -969.989 -969.989] [159.942], Avg: [-569.360 -569.360 -569.360] (0.4762) ({r_i: None, r_t: [-2004.804 -2004.804 -2004.804], eps: 0.476})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		# q_value = torch.zeros_like(q_next_value)
		# for t in reversed(range(rewards.size(1))):
		# 	q_value[:, t] = self.critic_local(critic_inputs[:,t])
		# 	q_taken = torch.gather(q_value[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
		# 	critic_error = (q_taken - q_target[:, t].detach())
		# 	critic_loss = critic_error.pow(2).mean()
		# 	self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		q_value = self.critic_local(critic_inputs)
		q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		critic_error = (q_taken - q_target.detach())
		critic_loss = critic_error.pow(2).mean()
		self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-490.363 -490.363 -490.363] [86.204], Avg: [-490.363 -490.363 -490.363] (1.0000) ({r_i: None, r_t: [-8.604 -8.604 -8.604], eps: 1.0})
Step:   38300, Reward: [-603.920 -603.920 -603.920] [147.900], Avg: [-503.523 -503.523 -503.523] (0.1000) ({r_i: None, r_t: [-1236.602 -1236.602 -1236.602], eps: 0.1})
Step:    7500, Reward: [-890.692 -890.692 -890.692] [198.278], Avg: [-573.588 -573.588 -573.588] (0.4715) ({r_i: None, r_t: [-1864.419 -1864.419 -1864.419], eps: 0.471})
Step:   38400, Reward: [-571.616 -571.616 -571.616] [125.759], Avg: [-503.700 -503.700 -503.700] (0.1000) ({r_i: None, r_t: [-1289.979 -1289.979 -1289.979], eps: 0.1})
Step:    7600, Reward: [-980.196 -980.196 -980.196] [180.085], Avg: [-578.869 -578.869 -578.869] (0.4668) ({r_i: None, r_t: [-1928.230 -1928.230 -1928.230], eps: 0.467})
Step:   38500, Reward: [-634.892 -634.892 -634.892] [196.044], Avg: [-504.040 -504.040 -504.040] (0.1000) ({r_i: None, r_t: [-1211.154 -1211.154 -1211.154], eps: 0.1})
Step:    7700, Reward: [-901.225 -901.225 -901.225] [181.785], Avg: [-583.002 -583.002 -583.002] (0.4621) ({r_i: None, r_t: [-1982.465 -1982.465 -1982.465], eps: 0.462})
Step:   38600, Reward: [-612.797 -612.797 -612.797] [132.182], Avg: [-504.321 -504.321 -504.321] (0.1000) ({r_i: None, r_t: [-1288.953 -1288.953 -1288.953], eps: 0.1})
Step:    7800, Reward: [-938.279 -938.279 -938.279] [255.257], Avg: [-587.499 -587.499 -587.499] (0.4575) ({r_i: None, r_t: [-1899.893 -1899.893 -1899.893], eps: 0.458})
Step:   38700, Reward: [-620.529 -620.529 -620.529] [119.746], Avg: [-504.621 -504.621 -504.621] (0.1000) ({r_i: None, r_t: [-1167.601 -1167.601 -1167.601], eps: 0.1})
Step:    7900, Reward: [-896.331 -896.331 -896.331] [169.740], Avg: [-591.359 -591.359 -591.359] (0.4529) ({r_i: None, r_t: [-1671.850 -1671.850 -1671.850], eps: 0.453})
Step:   38800, Reward: [-621.337 -621.337 -621.337] [129.333], Avg: [-504.921 -504.921 -504.921] (0.1000) ({r_i: None, r_t: [-1274.394 -1274.394 -1274.394], eps: 0.1})
Step:    8000, Reward: [-801.402 -801.402 -801.402] [135.247], Avg: [-593.952 -593.952 -593.952] (0.4484) ({r_i: None, r_t: [-1734.723 -1734.723 -1734.723], eps: 0.448})
Step:   38900, Reward: [-619.086 -619.086 -619.086] [143.442], Avg: [-505.213 -505.213 -505.213] (0.1000) ({r_i: None, r_t: [-1284.423 -1284.423 -1284.423], eps: 0.1})
Step:    8100, Reward: [-816.995 -816.995 -816.995] [184.376], Avg: [-596.672 -596.672 -596.672] (0.4440) ({r_i: None, r_t: [-1744.589 -1744.589 -1744.589], eps: 0.444})
Step:   39000, Reward: [-771.967 -771.967 -771.967] [214.056], Avg: [-505.896 -505.896 -505.896] (0.1000) ({r_i: None, r_t: [-1252.484 -1252.484 -1252.484], eps: 0.1})
Step:    8200, Reward: [-794.397 -794.397 -794.397] [187.168], Avg: [-599.055 -599.055 -599.055] (0.4395) ({r_i: None, r_t: [-1568.483 -1568.483 -1568.483], eps: 0.44})
Step:   39100, Reward: [-676.434 -676.434 -676.434] [170.156], Avg: [-506.331 -506.331 -506.331] (0.1000) ({r_i: None, r_t: [-1254.825 -1254.825 -1254.825], eps: 0.1})
Step:    8300, Reward: [-717.718 -717.718 -717.718] [121.032], Avg: [-600.467 -600.467 -600.467] (0.4351) ({r_i: None, r_t: [-1619.748 -1619.748 -1619.748], eps: 0.435})
Step:   39200, Reward: [-723.951 -723.951 -723.951] [225.414], Avg: [-506.884 -506.884 -506.884] (0.1000) ({r_i: None, r_t: [-1244.439 -1244.439 -1244.439], eps: 0.1})
Step:    8400, Reward: [-659.478 -659.478 -659.478] [131.120], Avg: [-601.162 -601.162 -601.162] (0.4308) ({r_i: None, r_t: [-1383.363 -1383.363 -1383.363], eps: 0.431})
Step:   39300, Reward: [-636.906 -636.906 -636.906] [190.292], Avg: [-507.214 -507.214 -507.214] (0.1000) ({r_i: None, r_t: [-1256.230 -1256.230 -1256.230], eps: 0.1})
Step:    8500, Reward: [-691.314 -691.314 -691.314] [134.707], Avg: [-602.210 -602.210 -602.210] (0.4265) ({r_i: None, r_t: [-1406.522 -1406.522 -1406.522], eps: 0.427})
Step:   39400, Reward: [-698.026 -698.026 -698.026] [173.352], Avg: [-507.697 -507.697 -507.697] (0.1000) ({r_i: None, r_t: [-1247.606 -1247.606 -1247.606], eps: 0.1})
Step:    8600, Reward: [-608.309 -608.309 -608.309] [104.798], Avg: [-602.280 -602.280 -602.280] (0.4223) ({r_i: None, r_t: [-1323.304 -1323.304 -1323.304], eps: 0.422})
Step:   39500, Reward: [-644.235 -644.235 -644.235] [174.754], Avg: [-508.042 -508.042 -508.042] (0.1000) ({r_i: None, r_t: [-1387.289 -1387.289 -1387.289], eps: 0.1})
Step:    8700, Reward: [-589.336 -589.336 -589.336] [82.466], Avg: [-602.133 -602.133 -602.133] (0.4180) ({r_i: None, r_t: [-1235.239 -1235.239 -1235.239], eps: 0.418})
Step:   39600, Reward: [-639.747 -639.747 -639.747] [188.912], Avg: [-508.374 -508.374 -508.374] (0.1000) ({r_i: None, r_t: [-1237.944 -1237.944 -1237.944], eps: 0.1})
Step:   39700, Reward: [-636.642 -636.642 -636.642] [111.154], Avg: [-508.696 -508.696 -508.696] (0.1000) ({r_i: None, r_t: [-1217.397 -1217.397 -1217.397], eps: 0.1})
Step:    8800, Reward: [-622.832 -622.832 -622.832] [123.306], Avg: [-602.365 -602.365 -602.365] (0.4139) ({r_i: None, r_t: [-1248.335 -1248.335 -1248.335], eps: 0.414})
Step:   39800, Reward: [-650.244 -650.244 -650.244] [151.477], Avg: [-509.051 -509.051 -509.051] (0.1000) ({r_i: None, r_t: [-1319.899 -1319.899 -1319.899], eps: 0.1})
Step:    8900, Reward: [-528.007 -528.007 -528.007] [78.956], Avg: [-601.539 -601.539 -601.539] (0.4097) ({r_i: None, r_t: [-1134.861 -1134.861 -1134.861], eps: 0.41})
Step:   39900, Reward: [-627.538 -627.538 -627.538] [172.628], Avg: [-509.347 -509.347 -509.347] (0.1000) ({r_i: None, r_t: [-1388.105 -1388.105 -1388.105], eps: 0.1})
Step:    9000, Reward: [-583.812 -583.812 -583.812] [94.157], Avg: [-601.344 -601.344 -601.344] (0.4057) ({r_i: None, r_t: [-1092.065 -1092.065 -1092.065], eps: 0.406})
Step:   40000, Reward: [-645.213 -645.213 -645.213] [120.906], Avg: [-509.686 -509.686 -509.686] (0.1000) ({r_i: None, r_t: [-1364.141 -1364.141 -1364.141], eps: 0.1})
Step:    9100, Reward: [-498.579 -498.579 -498.579] [103.981], Avg: [-600.227 -600.227 -600.227] (0.4016) ({r_i: None, r_t: [-1074.570 -1074.570 -1074.570], eps: 0.402})
Step:   40100, Reward: [-763.212 -763.212 -763.212] [218.681], Avg: [-510.317 -510.317 -510.317] (0.1000) ({r_i: None, r_t: [-1513.220 -1513.220 -1513.220], eps: 0.1})
Step:    9200, Reward: [-516.059 -516.059 -516.059] [119.095], Avg: [-599.322 -599.322 -599.322] (0.3976) ({r_i: None, r_t: [-1002.463 -1002.463 -1002.463], eps: 0.398})
Step:   40200, Reward: [-755.296 -755.296 -755.296] [189.875], Avg: [-510.925 -510.925 -510.925] (0.1000) ({r_i: None, r_t: [-1412.580 -1412.580 -1412.580], eps: 0.1})
Step:    9300, Reward: [-491.886 -491.886 -491.886] [62.190], Avg: [-598.179 -598.179 -598.179] (0.3936) ({r_i: None, r_t: [-959.867 -959.867 -959.867], eps: 0.394})
Step:   40300, Reward: [-709.574 -709.574 -709.574] [219.207], Avg: [-511.416 -511.416 -511.416] (0.1000) ({r_i: None, r_t: [-1456.899 -1456.899 -1456.899], eps: 0.1})
Step:    9400, Reward: [-522.466 -522.466 -522.466] [116.543], Avg: [-597.382 -597.382 -597.382] (0.3897) ({r_i: None, r_t: [-1037.086 -1037.086 -1037.086], eps: 0.39})
Step:   40400, Reward: [-681.341 -681.341 -681.341] [163.675], Avg: [-511.836 -511.836 -511.836] (0.1000) ({r_i: None, r_t: [-1522.044 -1522.044 -1522.044], eps: 0.1})
Step:    9500, Reward: [-534.484 -534.484 -534.484] [139.047], Avg: [-596.727 -596.727 -596.727] (0.3858) ({r_i: None, r_t: [-920.063 -920.063 -920.063], eps: 0.386})
Step:   40500, Reward: [-787.774 -787.774 -787.774] [243.265], Avg: [-512.516 -512.516 -512.516] (0.1000) ({r_i: None, r_t: [-1324.629 -1324.629 -1324.629], eps: 0.1})
Step:    9600, Reward: [-496.145 -496.145 -496.145] [95.030], Avg: [-595.690 -595.690 -595.690] (0.3820) ({r_i: None, r_t: [-987.325 -987.325 -987.325], eps: 0.382})
Step:   40600, Reward: [-824.450 -824.450 -824.450] [229.423], Avg: [-513.282 -513.282 -513.282] (0.1000) ({r_i: None, r_t: [-1520.625 -1520.625 -1520.625], eps: 0.1})
Step:    9700, Reward: [-472.675 -472.675 -472.675] [93.446], Avg: [-594.435 -594.435 -594.435] (0.3782) ({r_i: None, r_t: [-988.877 -988.877 -988.877], eps: 0.378})
Step:   40700, Reward: [-797.004 -797.004 -797.004] [323.998], Avg: [-513.977 -513.977 -513.977] (0.1000) ({r_i: None, r_t: [-1423.041 -1423.041 -1423.041], eps: 0.1})
Step:    9800, Reward: [-418.159 -418.159 -418.159] [58.764], Avg: [-592.655 -592.655 -592.655] (0.3744) ({r_i: None, r_t: [-992.465 -992.465 -992.465], eps: 0.374})
Step:   40800, Reward: [-714.810 -714.810 -714.810] [140.462], Avg: [-514.468 -514.468 -514.468] (0.1000) ({r_i: None, r_t: [-1499.154 -1499.154 -1499.154], eps: 0.1})
Step:    9900, Reward: [-449.720 -449.720 -449.720] [67.110], Avg: [-591.225 -591.225 -591.225] (0.3707) ({r_i: None, r_t: [-920.555 -920.555 -920.555], eps: 0.371})
Step:   40900, Reward: [-900.830 -900.830 -900.830] [173.988], Avg: [-515.411 -515.411 -515.411] (0.1000) ({r_i: None, r_t: [-1648.721 -1648.721 -1648.721], eps: 0.1})
Step:   10000, Reward: [-445.204 -445.204 -445.204] [90.573], Avg: [-589.779 -589.779 -589.779] (0.3670) ({r_i: None, r_t: [-953.339 -953.339 -953.339], eps: 0.367})
Step:   41000, Reward: [-781.948 -781.948 -781.948] [180.445], Avg: [-516.059 -516.059 -516.059] (0.1000) ({r_i: None, r_t: [-1490.299 -1490.299 -1490.299], eps: 0.1})
Step:   10100, Reward: [-464.375 -464.375 -464.375] [70.622], Avg: [-588.550 -588.550 -588.550] (0.3633) ({r_i: None, r_t: [-903.699 -903.699 -903.699], eps: 0.363})
Step:   41100, Reward: [-748.626 -748.626 -748.626] [175.694], Avg: [-516.624 -516.624 -516.624] (0.1000) ({r_i: None, r_t: [-1444.354 -1444.354 -1444.354], eps: 0.1})
Step:   10200, Reward: [-469.903 -469.903 -469.903] [97.920], Avg: [-587.398 -587.398 -587.398] (0.3597) ({r_i: None, r_t: [-988.846 -988.846 -988.846], eps: 0.36})
Step:   41200, Reward: [-740.502 -740.502 -740.502] [195.046], Avg: [-517.166 -517.166 -517.166] (0.1000) ({r_i: None, r_t: [-1517.447 -1517.447 -1517.447], eps: 0.1})
Step:   10300, Reward: [-464.401 -464.401 -464.401] [81.249], Avg: [-586.215 -586.215 -586.215] (0.3561) ({r_i: None, r_t: [-953.379 -953.379 -953.379], eps: 0.356})
Step:   41300, Reward: [-772.555 -772.555 -772.555] [295.607], Avg: [-517.783 -517.783 -517.783] (0.1000) ({r_i: None, r_t: [-1515.363 -1515.363 -1515.363], eps: 0.1})
Step:   10400, Reward: [-467.570 -467.570 -467.570] [46.737], Avg: [-585.085 -585.085 -585.085] (0.3525) ({r_i: None, r_t: [-941.401 -941.401 -941.401], eps: 0.353})
Step:   41400, Reward: [-788.145 -788.145 -788.145] [204.821], Avg: [-518.434 -518.434 -518.434] (0.1000) ({r_i: None, r_t: [-1464.195 -1464.195 -1464.195], eps: 0.1})
Step:   10500, Reward: [-462.932 -462.932 -462.932] [62.292], Avg: [-583.933 -583.933 -583.933] (0.3490) ({r_i: None, r_t: [-919.554 -919.554 -919.554], eps: 0.349})
Step:   41500, Reward: [-892.406 -892.406 -892.406] [187.632], Avg: [-519.333 -519.333 -519.333] (0.1000) ({r_i: None, r_t: [-1474.312 -1474.312 -1474.312], eps: 0.1})
Step:   10600, Reward: [-451.604 -451.604 -451.604] [67.004], Avg: [-582.696 -582.696 -582.696] (0.3455) ({r_i: None, r_t: [-891.271 -891.271 -891.271], eps: 0.346})
Step:   41600, Reward: [-843.389 -843.389 -843.389] [248.932], Avg: [-520.110 -520.110 -520.110] (0.1000) ({r_i: None, r_t: [-1529.281 -1529.281 -1529.281], eps: 0.1})
Step:   10700, Reward: [-444.896 -444.896 -444.896] [89.883], Avg: [-581.420 -581.420 -581.420] (0.3421) ({r_i: None, r_t: [-927.216 -927.216 -927.216], eps: 0.342})
Step:   41700, Reward: [-737.671 -737.671 -737.671] [181.131], Avg: [-520.631 -520.631 -520.631] (0.1000) ({r_i: None, r_t: [-1468.717 -1468.717 -1468.717], eps: 0.1})
Step:   10800, Reward: [-439.605 -439.605 -439.605] [73.643], Avg: [-580.119 -580.119 -580.119] (0.3387) ({r_i: None, r_t: [-926.424 -926.424 -926.424], eps: 0.339})
Step:   41800, Reward: [-823.927 -823.927 -823.927] [271.671], Avg: [-521.355 -521.355 -521.355] (0.1000) ({r_i: None, r_t: [-1443.247 -1443.247 -1443.247], eps: 0.1})
Step:   10900, Reward: [-471.156 -471.156 -471.156] [91.310], Avg: [-579.129 -579.129 -579.129] (0.3353) ({r_i: None, r_t: [-874.679 -874.679 -874.679], eps: 0.335})
Step:   41900, Reward: [-787.136 -787.136 -787.136] [261.406], Avg: [-521.987 -521.987 -521.987] (0.1000) ({r_i: None, r_t: [-1706.979 -1706.979 -1706.979], eps: 0.1})
Step:   11000, Reward: [-462.318 -462.318 -462.318] [86.234], Avg: [-578.076 -578.076 -578.076] (0.3320) ({r_i: None, r_t: [-940.936 -940.936 -940.936], eps: 0.332})
Step:   42000, Reward: [-782.747 -782.747 -782.747] [194.873], Avg: [-522.607 -522.607 -522.607] (0.1000) ({r_i: None, r_t: [-1678.585 -1678.585 -1678.585], eps: 0.1})
Step:   11100, Reward: [-461.164 -461.164 -461.164] [74.449], Avg: [-577.033 -577.033 -577.033] (0.3286) ({r_i: None, r_t: [-921.777 -921.777 -921.777], eps: 0.329})
Step:   42100, Reward: [-806.342 -806.342 -806.342] [296.884], Avg: [-523.279 -523.279 -523.279] (0.1000) ({r_i: None, r_t: [-1562.353 -1562.353 -1562.353], eps: 0.1})
Step:   11200, Reward: [-421.664 -421.664 -421.664] [76.326], Avg: [-575.658 -575.658 -575.658] (0.3254) ({r_i: None, r_t: [-923.841 -923.841 -923.841], eps: 0.325})
Step:   42200, Reward: [-755.089 -755.089 -755.089] [251.757], Avg: [-523.827 -523.827 -523.827] (0.1000) ({r_i: None, r_t: [-1671.997 -1671.997 -1671.997], eps: 0.1})
Step:   42300, Reward: [-725.066 -725.066 -725.066] [187.803], Avg: [-524.302 -524.302 -524.302] (0.1000) ({r_i: None, r_t: [-1687.101 -1687.101 -1687.101], eps: 0.1})
Step:   11300, Reward: [-454.429 -454.429 -454.429] [81.861], Avg: [-574.594 -574.594 -574.594] (0.3221) ({r_i: None, r_t: [-914.919 -914.919 -914.919], eps: 0.322})
Step:   42400, Reward: [-775.810 -775.810 -775.810] [265.898], Avg: [-524.894 -524.894 -524.894] (0.1000) ({r_i: None, r_t: [-1648.390 -1648.390 -1648.390], eps: 0.1})
Step:   11400, Reward: [-487.405 -487.405 -487.405] [121.798], Avg: [-573.836 -573.836 -573.836] (0.3189) ({r_i: None, r_t: [-914.006 -914.006 -914.006], eps: 0.319})
Step:   42500, Reward: [-712.594 -712.594 -712.594] [201.542], Avg: [-525.334 -525.334 -525.334] (0.1000) ({r_i: None, r_t: [-1618.630 -1618.630 -1618.630], eps: 0.1})
Step:   11500, Reward: [-443.294 -443.294 -443.294] [60.631], Avg: [-572.711 -572.711 -572.711] (0.3157) ({r_i: None, r_t: [-882.794 -882.794 -882.794], eps: 0.316})
Step:   42600, Reward: [-806.683 -806.683 -806.683] [169.759], Avg: [-525.993 -525.993 -525.993] (0.1000) ({r_i: None, r_t: [-1718.157 -1718.157 -1718.157], eps: 0.1})
Step:   11600, Reward: [-453.399 -453.399 -453.399] [85.784], Avg: [-571.691 -571.691 -571.691] (0.3126) ({r_i: None, r_t: [-950.577 -950.577 -950.577], eps: 0.313})
Step:   42700, Reward: [-802.219 -802.219 -802.219] [169.101], Avg: [-526.638 -526.638 -526.638] (0.1000) ({r_i: None, r_t: [-1575.682 -1575.682 -1575.682], eps: 0.1})
Step:   11700, Reward: [-436.204 -436.204 -436.204] [64.659], Avg: [-570.543 -570.543 -570.543] (0.3095) ({r_i: None, r_t: [-905.346 -905.346 -905.346], eps: 0.309})
Step:   42800, Reward: [-871.913 -871.913 -871.913] [316.494], Avg: [-527.443 -527.443 -527.443] (0.1000) ({r_i: None, r_t: [-1590.356 -1590.356 -1590.356], eps: 0.1})
Step:   11800, Reward: [-460.884 -460.884 -460.884] [86.135], Avg: [-569.621 -569.621 -569.621] (0.3064) ({r_i: None, r_t: [-877.433 -877.433 -877.433], eps: 0.306})
Step:   42900, Reward: [-807.840 -807.840 -807.840] [218.527], Avg: [-528.095 -528.095 -528.095] (0.1000) ({r_i: None, r_t: [-1678.034 -1678.034 -1678.034], eps: 0.1})
Step:   11900, Reward: [-486.091 -486.091 -486.091] [78.882], Avg: [-568.925 -568.925 -568.925] (0.3033) ({r_i: None, r_t: [-929.095 -929.095 -929.095], eps: 0.303})
Step:   43000, Reward: [-924.784 -924.784 -924.784] [135.870], Avg: [-529.016 -529.016 -529.016] (0.1000) ({r_i: None, r_t: [-1672.748 -1672.748 -1672.748], eps: 0.1})
Step:   12000, Reward: [-472.800 -472.800 -472.800] [84.072], Avg: [-568.131 -568.131 -568.131] (0.3003) ({r_i: None, r_t: [-924.012 -924.012 -924.012], eps: 0.3})
Step:   43100, Reward: [-748.900 -748.900 -748.900] [269.726], Avg: [-529.525 -529.525 -529.525] (0.1000) ({r_i: None, r_t: [-1585.714 -1585.714 -1585.714], eps: 0.1})
Step:   12100, Reward: [-427.411 -427.411 -427.411] [67.763], Avg: [-566.977 -566.977 -566.977] (0.2973) ({r_i: None, r_t: [-928.191 -928.191 -928.191], eps: 0.297})
Step:   43200, Reward: [-758.187 -758.187 -758.187] [196.789], Avg: [-530.053 -530.053 -530.053] (0.1000) ({r_i: None, r_t: [-1483.119 -1483.119 -1483.119], eps: 0.1})
Step:   12200, Reward: [-454.296 -454.296 -454.296] [103.562], Avg: [-566.061 -566.061 -566.061] (0.2943) ({r_i: None, r_t: [-893.992 -893.992 -893.992], eps: 0.294})
Step:   43300, Reward: [-777.052 -777.052 -777.052] [189.365], Avg: [-530.622 -530.622 -530.622] (0.1000) ({r_i: None, r_t: [-1617.597 -1617.597 -1617.597], eps: 0.1})
Step:   12300, Reward: [-427.244 -427.244 -427.244] [44.219], Avg: [-564.942 -564.942 -564.942] (0.2914) ({r_i: None, r_t: [-919.818 -919.818 -919.818], eps: 0.291})
Step:   43400, Reward: [-886.886 -886.886 -886.886] [242.652], Avg: [-531.441 -531.441 -531.441] (0.1000) ({r_i: None, r_t: [-1540.956 -1540.956 -1540.956], eps: 0.1})
Step:   12400, Reward: [-429.967 -429.967 -429.967] [80.653], Avg: [-563.862 -563.862 -563.862] (0.2885) ({r_i: None, r_t: [-950.486 -950.486 -950.486], eps: 0.288})
Step:   43500, Reward: [-785.715 -785.715 -785.715] [212.614], Avg: [-532.024 -532.024 -532.024] (0.1000) ({r_i: None, r_t: [-1531.023 -1531.023 -1531.023], eps: 0.1})
Step:   12500, Reward: [-480.850 -480.850 -480.850] [75.881], Avg: [-563.203 -563.203 -563.203] (0.2856) ({r_i: None, r_t: [-947.811 -947.811 -947.811], eps: 0.286})
Step:   43600, Reward: [-804.150 -804.150 -804.150] [167.225], Avg: [-532.647 -532.647 -532.647] (0.1000) ({r_i: None, r_t: [-1563.428 -1563.428 -1563.428], eps: 0.1})
Step:   12600, Reward: [-485.661 -485.661 -485.661] [101.856], Avg: [-562.592 -562.592 -562.592] (0.2828) ({r_i: None, r_t: [-945.971 -945.971 -945.971], eps: 0.283})
Step:   43700, Reward: [-716.656 -716.656 -716.656] [204.698], Avg: [-533.067 -533.067 -533.067] (0.1000) ({r_i: None, r_t: [-1648.871 -1648.871 -1648.871], eps: 0.1})
Step:   12700, Reward: [-471.107 -471.107 -471.107] [92.370], Avg: [-561.878 -561.878 -561.878] (0.2799) ({r_i: None, r_t: [-977.661 -977.661 -977.661], eps: 0.28})
Step:   43800, Reward: [-750.942 -750.942 -750.942] [206.192], Avg: [-533.563 -533.563 -533.563] (0.1000) ({r_i: None, r_t: [-1562.000 -1562.000 -1562.000], eps: 0.1})
Step:   12800, Reward: [-494.853 -494.853 -494.853] [103.502], Avg: [-561.358 -561.358 -561.358] (0.2771) ({r_i: None, r_t: [-1010.987 -1010.987 -1010.987], eps: 0.277})
Step:   43900, Reward: [-731.687 -731.687 -731.687] [229.090], Avg: [-534.014 -534.014 -534.014] (0.1000) ({r_i: None, r_t: [-1640.664 -1640.664 -1640.664], eps: 0.1})
Step:   12900, Reward: [-454.082 -454.082 -454.082] [72.849], Avg: [-560.533 -560.533 -560.533] (0.2744) ({r_i: None, r_t: [-997.568 -997.568 -997.568], eps: 0.274})
Step:   44000, Reward: [-768.690 -768.690 -768.690] [180.373], Avg: [-534.546 -534.546 -534.546] (0.1000) ({r_i: None, r_t: [-1554.457 -1554.457 -1554.457], eps: 0.1})
Step:   13000, Reward: [-501.576 -501.576 -501.576] [119.329], Avg: [-560.083 -560.083 -560.083] (0.2716) ({r_i: None, r_t: [-958.325 -958.325 -958.325], eps: 0.272})
Step:   44100, Reward: [-746.464 -746.464 -746.464] [255.442], Avg: [-535.025 -535.025 -535.025] (0.1000) ({r_i: None, r_t: [-1578.969 -1578.969 -1578.969], eps: 0.1})
Step:   13100, Reward: [-468.563 -468.563 -468.563] [105.533], Avg: [-559.390 -559.390 -559.390] (0.2689) ({r_i: None, r_t: [-994.659 -994.659 -994.659], eps: 0.269})
Step:   44200, Reward: [-766.603 -766.603 -766.603] [239.141], Avg: [-535.548 -535.548 -535.548] (0.1000) ({r_i: None, r_t: [-1572.827 -1572.827 -1572.827], eps: 0.1})
Step:   13200, Reward: [-497.738 -497.738 -497.738] [84.224], Avg: [-558.926 -558.926 -558.926] (0.2663) ({r_i: None, r_t: [-1030.467 -1030.467 -1030.467], eps: 0.266})
Step:   44300, Reward: [-767.921 -767.921 -767.921] [221.131], Avg: [-536.071 -536.071 -536.071] (0.1000) ({r_i: None, r_t: [-1478.120 -1478.120 -1478.120], eps: 0.1})
Step:   13300, Reward: [-516.399 -516.399 -516.399] [90.500], Avg: [-558.609 -558.609 -558.609] (0.2636) ({r_i: None, r_t: [-1058.993 -1058.993 -1058.993], eps: 0.264})
Step:   44400, Reward: [-752.069 -752.069 -752.069] [207.637], Avg: [-536.557 -536.557 -536.557] (0.1000) ({r_i: None, r_t: [-1547.161 -1547.161 -1547.161], eps: 0.1})
Step:   13400, Reward: [-510.249 -510.249 -510.249] [118.431], Avg: [-558.250 -558.250 -558.250] (0.2610) ({r_i: None, r_t: [-1080.799 -1080.799 -1080.799], eps: 0.261})
Step:   44500, Reward: [-785.533 -785.533 -785.533] [180.687], Avg: [-537.115 -537.115 -537.115] (0.1000) ({r_i: None, r_t: [-1604.281 -1604.281 -1604.281], eps: 0.1})
Step:   13500, Reward: [-510.067 -510.067 -510.067] [131.820], Avg: [-557.896 -557.896 -557.896] (0.2584) ({r_i: None, r_t: [-1013.620 -1013.620 -1013.620], eps: 0.258})
Step:   44600, Reward: [-813.298 -813.298 -813.298] [222.546], Avg: [-537.733 -537.733 -537.733] (0.1000) ({r_i: None, r_t: [-1303.766 -1303.766 -1303.766], eps: 0.1})
Step:   44700, Reward: [-754.657 -754.657 -754.657] [244.089], Avg: [-538.217 -538.217 -538.217] (0.1000) ({r_i: None, r_t: [-1514.101 -1514.101 -1514.101], eps: 0.1})
Step:   13600, Reward: [-473.696 -473.696 -473.696] [69.076], Avg: [-557.282 -557.282 -557.282] (0.2558) ({r_i: None, r_t: [-974.303 -974.303 -974.303], eps: 0.256})
Step:   44800, Reward: [-781.280 -781.280 -781.280] [160.202], Avg: [-538.758 -538.758 -538.758] (0.1000) ({r_i: None, r_t: [-1457.695 -1457.695 -1457.695], eps: 0.1})
Step:   13700, Reward: [-524.826 -524.826 -524.826] [134.941], Avg: [-557.046 -557.046 -557.046] (0.2532) ({r_i: None, r_t: [-1052.587 -1052.587 -1052.587], eps: 0.253})
Step:   44900, Reward: [-757.422 -757.422 -757.422] [238.609], Avg: [-539.244 -539.244 -539.244] (0.1000) ({r_i: None, r_t: [-1340.973 -1340.973 -1340.973], eps: 0.1})
Step:   13800, Reward: [-496.452 -496.452 -496.452] [81.411], Avg: [-556.610 -556.610 -556.610] (0.2507) ({r_i: None, r_t: [-1064.358 -1064.358 -1064.358], eps: 0.251})
Step:   45000, Reward: [-690.693 -690.693 -690.693] [140.807], Avg: [-539.580 -539.580 -539.580] (0.1000) ({r_i: None, r_t: [-1316.133 -1316.133 -1316.133], eps: 0.1})
Step:   13900, Reward: [-512.404 -512.404 -512.404] [178.670], Avg: [-556.295 -556.295 -556.295] (0.2482) ({r_i: None, r_t: [-1072.310 -1072.310 -1072.310], eps: 0.248})
Step:   45100, Reward: [-659.486 -659.486 -659.486] [192.277], Avg: [-539.845 -539.845 -539.845] (0.1000) ({r_i: None, r_t: [-1366.208 -1366.208 -1366.208], eps: 0.1})
Step:   14000, Reward: [-518.892 -518.892 -518.892] [182.787], Avg: [-556.029 -556.029 -556.029] (0.2457) ({r_i: None, r_t: [-897.983 -897.983 -897.983], eps: 0.246})
Step:   45200, Reward: [-743.986 -743.986 -743.986] [189.772], Avg: [-540.296 -540.296 -540.296] (0.1000) ({r_i: None, r_t: [-1581.639 -1581.639 -1581.639], eps: 0.1})
Step:   14100, Reward: [-511.336 -511.336 -511.336] [105.327], Avg: [-555.715 -555.715 -555.715] (0.2433) ({r_i: None, r_t: [-1012.128 -1012.128 -1012.128], eps: 0.243})
Step:   45300, Reward: [-687.136 -687.136 -687.136] [206.917], Avg: [-540.619 -540.619 -540.619] (0.1000) ({r_i: None, r_t: [-1508.809 -1508.809 -1508.809], eps: 0.1})
Step:   14200, Reward: [-483.688 -483.688 -483.688] [91.027], Avg: [-555.211 -555.211 -555.211] (0.2409) ({r_i: None, r_t: [-994.712 -994.712 -994.712], eps: 0.241})
Step:   45400, Reward: [-763.207 -763.207 -763.207] [226.894], Avg: [-541.109 -541.109 -541.109] (0.1000) ({r_i: None, r_t: [-1473.400 -1473.400 -1473.400], eps: 0.1})
Step:   14300, Reward: [-502.071 -502.071 -502.071] [105.708], Avg: [-554.842 -554.842 -554.842] (0.2385) ({r_i: None, r_t: [-929.954 -929.954 -929.954], eps: 0.238})
Step:   45500, Reward: [-641.995 -641.995 -641.995] [184.072], Avg: [-541.330 -541.330 -541.330] (0.1000) ({r_i: None, r_t: [-1444.806 -1444.806 -1444.806], eps: 0.1})
Step:   14400, Reward: [-498.993 -498.993 -498.993] [98.515], Avg: [-554.457 -554.457 -554.457] (0.2361) ({r_i: None, r_t: [-1044.181 -1044.181 -1044.181], eps: 0.236})
Step:   45600, Reward: [-725.778 -725.778 -725.778] [203.971], Avg: [-541.733 -541.733 -541.733] (0.1000) ({r_i: None, r_t: [-1429.069 -1429.069 -1429.069], eps: 0.1})
Step:   14500, Reward: [-443.377 -443.377 -443.377] [78.840], Avg: [-553.696 -553.696 -553.696] (0.2337) ({r_i: None, r_t: [-1013.507 -1013.507 -1013.507], eps: 0.234})
Step:   45700, Reward: [-671.488 -671.488 -671.488] [209.272], Avg: [-542.017 -542.017 -542.017] (0.1000) ({r_i: None, r_t: [-1467.787 -1467.787 -1467.787], eps: 0.1})
Step:   14600, Reward: [-481.957 -481.957 -481.957] [83.719], Avg: [-553.208 -553.208 -553.208] (0.2314) ({r_i: None, r_t: [-965.376 -965.376 -965.376], eps: 0.231})
Step:   45800, Reward: [-674.118 -674.118 -674.118] [129.483], Avg: [-542.305 -542.305 -542.305] (0.1000) ({r_i: None, r_t: [-1468.210 -1468.210 -1468.210], eps: 0.1})
Step:   14700, Reward: [-489.601 -489.601 -489.601] [93.582], Avg: [-552.778 -552.778 -552.778] (0.2291) ({r_i: None, r_t: [-974.177 -974.177 -974.177], eps: 0.229})
Step:   45900, Reward: [-658.762 -658.762 -658.762] [178.114], Avg: [-542.558 -542.558 -542.558] (0.1000) ({r_i: None, r_t: [-1324.281 -1324.281 -1324.281], eps: 0.1})
Step:   14800, Reward: [-514.319 -514.319 -514.319] [100.091], Avg: [-552.520 -552.520 -552.520] (0.2268) ({r_i: None, r_t: [-958.281 -958.281 -958.281], eps: 0.227})
Step:   46000, Reward: [-720.329 -720.329 -720.329] [199.261], Avg: [-542.943 -542.943 -542.943] (0.1000) ({r_i: None, r_t: [-1347.194 -1347.194 -1347.194], eps: 0.1})
Step:   14900, Reward: [-522.266 -522.266 -522.266] [121.366], Avg: [-552.318 -552.318 -552.318] (0.2245) ({r_i: None, r_t: [-1033.303 -1033.303 -1033.303], eps: 0.225})
Step:   46100, Reward: [-720.413 -720.413 -720.413] [199.190], Avg: [-543.327 -543.327 -543.327] (0.1000) ({r_i: None, r_t: [-1403.547 -1403.547 -1403.547], eps: 0.1})
Step:   15000, Reward: [-520.676 -520.676 -520.676] [106.289], Avg: [-552.109 -552.109 -552.109] (0.2223) ({r_i: None, r_t: [-993.413 -993.413 -993.413], eps: 0.222})
Step:   46200, Reward: [-629.262 -629.262 -629.262] [140.044], Avg: [-543.513 -543.513 -543.513] (0.1000) ({r_i: None, r_t: [-1409.578 -1409.578 -1409.578], eps: 0.1})
Step:   15100, Reward: [-509.640 -509.640 -509.640] [91.208], Avg: [-551.829 -551.829 -551.829] (0.2201) ({r_i: None, r_t: [-1055.288 -1055.288 -1055.288], eps: 0.22})
Step:   46300, Reward: [-673.082 -673.082 -673.082] [151.683], Avg: [-543.792 -543.792 -543.792] (0.1000) ({r_i: None, r_t: [-1535.559 -1535.559 -1535.559], eps: 0.1})
Step:   15200, Reward: [-523.342 -523.342 -523.342] [95.766], Avg: [-551.643 -551.643 -551.643] (0.2179) ({r_i: None, r_t: [-1015.729 -1015.729 -1015.729], eps: 0.218})
Step:   46400, Reward: [-664.455 -664.455 -664.455] [176.428], Avg: [-544.052 -544.052 -544.052] (0.1000) ({r_i: None, r_t: [-1432.070 -1432.070 -1432.070], eps: 0.1})
Step:   15300, Reward: [-526.411 -526.411 -526.411] [113.245], Avg: [-551.479 -551.479 -551.479] (0.2157) ({r_i: None, r_t: [-1044.063 -1044.063 -1044.063], eps: 0.216})
Step:   46500, Reward: [-652.946 -652.946 -652.946] [120.108], Avg: [-544.285 -544.285 -544.285] (0.1000) ({r_i: None, r_t: [-1374.071 -1374.071 -1374.071], eps: 0.1})
Step:   15400, Reward: [-527.999 -527.999 -527.999] [90.361], Avg: [-551.328 -551.328 -551.328] (0.2136) ({r_i: None, r_t: [-1101.963 -1101.963 -1101.963], eps: 0.214})
Step:   46600, Reward: [-668.234 -668.234 -668.234] [161.928], Avg: [-544.551 -544.551 -544.551] (0.1000) ({r_i: None, r_t: [-1463.428 -1463.428 -1463.428], eps: 0.1})
Step:   15500, Reward: [-628.215 -628.215 -628.215] [103.401], Avg: [-551.821 -551.821 -551.821] (0.2114) ({r_i: None, r_t: [-1135.576 -1135.576 -1135.576], eps: 0.211})
Step:   46700, Reward: [-654.518 -654.518 -654.518] [191.015], Avg: [-544.786 -544.786 -544.786] (0.1000) ({r_i: None, r_t: [-1385.660 -1385.660 -1385.660], eps: 0.1})
Step:   15600, Reward: [-579.156 -579.156 -579.156] [100.927], Avg: [-551.995 -551.995 -551.995] (0.2093) ({r_i: None, r_t: [-1189.489 -1189.489 -1189.489], eps: 0.209})
Step:   46800, Reward: [-733.320 -733.320 -733.320] [198.688], Avg: [-545.188 -545.188 -545.188] (0.1000) ({r_i: None, r_t: [-1359.655 -1359.655 -1359.655], eps: 0.1})
Step:   15700, Reward: [-622.539 -622.539 -622.539] [130.641], Avg: [-552.441 -552.441 -552.441] (0.2072) ({r_i: None, r_t: [-1170.203 -1170.203 -1170.203], eps: 0.207})
Step:   46900, Reward: [-624.616 -624.616 -624.616] [143.210], Avg: [-545.357 -545.357 -545.357] (0.1000) ({r_i: None, r_t: [-1312.619 -1312.619 -1312.619], eps: 0.1})
Step:   15800, Reward: [-578.292 -578.292 -578.292] [97.458], Avg: [-552.604 -552.604 -552.604] (0.2052) ({r_i: None, r_t: [-1262.102 -1262.102 -1262.102], eps: 0.205})
Step:   47000, Reward: [-611.623 -611.623 -611.623] [161.497], Avg: [-545.498 -545.498 -545.498] (0.1000) ({r_i: None, r_t: [-1333.629 -1333.629 -1333.629], eps: 0.1})
Step:   15900, Reward: [-620.904 -620.904 -620.904] [103.417], Avg: [-553.031 -553.031 -553.031] (0.2031) ({r_i: None, r_t: [-1360.517 -1360.517 -1360.517], eps: 0.203})
Step:   47100, Reward: [-671.702 -671.702 -671.702] [136.452], Avg: [-545.765 -545.765 -545.765] (0.1000) ({r_i: None, r_t: [-1256.427 -1256.427 -1256.427], eps: 0.1})
Step:   16000, Reward: [-642.425 -642.425 -642.425] [108.307], Avg: [-553.586 -553.586 -553.586] (0.2011) ({r_i: None, r_t: [-1280.485 -1280.485 -1280.485], eps: 0.201})
Step:   47200, Reward: [-593.396 -593.396 -593.396] [144.353], Avg: [-545.866 -545.866 -545.866] (0.1000) ({r_i: None, r_t: [-1303.353 -1303.353 -1303.353], eps: 0.1})
Step:   16100, Reward: [-696.238 -696.238 -696.238] [120.570], Avg: [-554.467 -554.467 -554.467] (0.1991) ({r_i: None, r_t: [-1299.553 -1299.553 -1299.553], eps: 0.199})
Step:   47300, Reward: [-688.145 -688.145 -688.145] [173.730], Avg: [-546.166 -546.166 -546.166] (0.1000) ({r_i: None, r_t: [-1277.877 -1277.877 -1277.877], eps: 0.1})
Step:   16200, Reward: [-692.972 -692.972 -692.972] [143.701], Avg: [-555.316 -555.316 -555.316] (0.1971) ({r_i: None, r_t: [-1414.279 -1414.279 -1414.279], eps: 0.197})
Step:   47400, Reward: [-626.645 -626.645 -626.645] [149.916], Avg: [-546.335 -546.335 -546.335] (0.1000) ({r_i: None, r_t: [-1248.053 -1248.053 -1248.053], eps: 0.1})
Step:   47500, Reward: [-620.534 -620.534 -620.534] [128.107], Avg: [-546.491 -546.491 -546.491] (0.1000) ({r_i: None, r_t: [-1221.392 -1221.392 -1221.392], eps: 0.1})
Step:   16300, Reward: [-729.345 -729.345 -729.345] [158.551], Avg: [-556.378 -556.378 -556.378] (0.1951) ({r_i: None, r_t: [-1337.750 -1337.750 -1337.750], eps: 0.195})
Step:   47600, Reward: [-603.462 -603.462 -603.462] [185.074], Avg: [-546.611 -546.611 -546.611] (0.1000) ({r_i: None, r_t: [-1215.250 -1215.250 -1215.250], eps: 0.1})
Step:   16400, Reward: [-767.156 -767.156 -767.156] [120.166], Avg: [-557.655 -557.655 -557.655] (0.1932) ({r_i: None, r_t: [-1502.497 -1502.497 -1502.497], eps: 0.193})
Step:   47700, Reward: [-659.788 -659.788 -659.788] [150.377], Avg: [-546.847 -546.847 -546.847] (0.1000) ({r_i: None, r_t: [-1227.887 -1227.887 -1227.887], eps: 0.1})
Step:   16500, Reward: [-889.479 -889.479 -889.479] [147.950], Avg: [-559.654 -559.654 -559.654] (0.1913) ({r_i: None, r_t: [-1680.416 -1680.416 -1680.416], eps: 0.191})
Step:   47800, Reward: [-621.922 -621.922 -621.922] [88.910], Avg: [-547.004 -547.004 -547.004] (0.1000) ({r_i: None, r_t: [-1256.059 -1256.059 -1256.059], eps: 0.1})
Step:   16600, Reward: [-838.819 -838.819 -838.819] [134.701], Avg: [-561.326 -561.326 -561.326] (0.1893) ({r_i: None, r_t: [-1735.304 -1735.304 -1735.304], eps: 0.189})
Step:   47900, Reward: [-590.348 -590.348 -590.348] [156.846], Avg: [-547.094 -547.094 -547.094] (0.1000) ({r_i: None, r_t: [-1370.805 -1370.805 -1370.805], eps: 0.1})
Step:   16700, Reward: [-880.643 -880.643 -880.643] [158.089], Avg: [-563.226 -563.226 -563.226] (0.1875) ({r_i: None, r_t: [-1912.311 -1912.311 -1912.311], eps: 0.187})
Step:   48000, Reward: [-587.092 -587.092 -587.092] [128.835], Avg: [-547.178 -547.178 -547.178] (0.1000) ({r_i: None, r_t: [-1193.786 -1193.786 -1193.786], eps: 0.1})
Step:   16800, Reward: [-893.538 -893.538 -893.538] [123.345], Avg: [-565.181 -565.181 -565.181] (0.1856) ({r_i: None, r_t: [-1793.778 -1793.778 -1793.778], eps: 0.186})
Step:   48100, Reward: [-607.680 -607.680 -607.680] [153.112], Avg: [-547.303 -547.303 -547.303] (0.1000) ({r_i: None, r_t: [-1243.619 -1243.619 -1243.619], eps: 0.1})
Step:   16900, Reward: [-961.075 -961.075 -961.075] [158.525], Avg: [-567.510 -567.510 -567.510] (0.1837) ({r_i: None, r_t: [-1856.857 -1856.857 -1856.857], eps: 0.184})
Step:   48200, Reward: [-579.760 -579.760 -579.760] [162.563], Avg: [-547.370 -547.370 -547.370] (0.1000) ({r_i: None, r_t: [-1257.040 -1257.040 -1257.040], eps: 0.1})
Step:   17000, Reward: [-978.515 -978.515 -978.515] [94.575], Avg: [-569.913 -569.913 -569.913] (0.1819) ({r_i: None, r_t: [-1759.474 -1759.474 -1759.474], eps: 0.182})
Step:   48300, Reward: [-623.162 -623.162 -623.162] [157.528], Avg: [-547.527 -547.527 -547.527] (0.1000) ({r_i: None, r_t: [-1322.349 -1322.349 -1322.349], eps: 0.1})
Step:   17100, Reward: [-1017.938 -1017.938 -1017.938] [232.335], Avg: [-572.518 -572.518 -572.518] (0.1801) ({r_i: None, r_t: [-1979.422 -1979.422 -1979.422], eps: 0.18})
Step:   48400, Reward: [-681.167 -681.167 -681.167] [180.648], Avg: [-547.802 -547.802 -547.802] (0.1000) ({r_i: None, r_t: [-1133.413 -1133.413 -1133.413], eps: 0.1})
Step:   17200, Reward: [-1132.967 -1132.967 -1132.967] [236.974], Avg: [-575.757 -575.757 -575.757] (0.1783) ({r_i: None, r_t: [-1979.584 -1979.584 -1979.584], eps: 0.178})
Step:   48500, Reward: [-578.291 -578.291 -578.291] [98.833], Avg: [-547.865 -547.865 -547.865] (0.1000) ({r_i: None, r_t: [-1244.156 -1244.156 -1244.156], eps: 0.1})
Step:   17300, Reward: [-1029.705 -1029.705 -1029.705] [148.629], Avg: [-578.366 -578.366 -578.366] (0.1765) ({r_i: None, r_t: [-2089.216 -2089.216 -2089.216], eps: 0.177})
Step:   48600, Reward: [-557.320 -557.320 -557.320] [138.234], Avg: [-547.885 -547.885 -547.885] (0.1000) ({r_i: None, r_t: [-1243.724 -1243.724 -1243.724], eps: 0.1})
Step:   17400, Reward: [-1197.305 -1197.305 -1197.305] [259.299], Avg: [-581.903 -581.903 -581.903] (0.1748) ({r_i: None, r_t: [-2125.171 -2125.171 -2125.171], eps: 0.175})
Step:   48700, Reward: [-616.339 -616.339 -616.339] [127.128], Avg: [-548.025 -548.025 -548.025] (0.1000) ({r_i: None, r_t: [-1115.115 -1115.115 -1115.115], eps: 0.1})
Step:   17500, Reward: [-1092.801 -1092.801 -1092.801] [225.421], Avg: [-584.806 -584.806 -584.806] (0.1730) ({r_i: None, r_t: [-2258.571 -2258.571 -2258.571], eps: 0.173})
Step:   48800, Reward: [-541.593 -541.593 -541.593] [164.802], Avg: [-548.012 -548.012 -548.012] (0.1000) ({r_i: None, r_t: [-1247.439 -1247.439 -1247.439], eps: 0.1})
Step:   17600, Reward: [-1175.239 -1175.239 -1175.239] [247.826], Avg: [-588.142 -588.142 -588.142] (0.1713) ({r_i: None, r_t: [-2440.840 -2440.840 -2440.840], eps: 0.171})
Step:   48900, Reward: [-582.698 -582.698 -582.698] [159.367], Avg: [-548.082 -548.082 -548.082] (0.1000) ({r_i: None, r_t: [-1217.532 -1217.532 -1217.532], eps: 0.1})
Step:   17700, Reward: [-1322.684 -1322.684 -1322.684] [347.620], Avg: [-592.268 -592.268 -592.268] (0.1696) ({r_i: None, r_t: [-2511.512 -2511.512 -2511.512], eps: 0.17})
Step:   49000, Reward: [-552.887 -552.887 -552.887] [110.883], Avg: [-548.092 -548.092 -548.092] (0.1000) ({r_i: None, r_t: [-1191.852 -1191.852 -1191.852], eps: 0.1})
Step:   17800, Reward: [-1290.006 -1290.006 -1290.006] [227.275], Avg: [-596.166 -596.166 -596.166] (0.1679) ({r_i: None, r_t: [-2421.104 -2421.104 -2421.104], eps: 0.168})
Step:   49100, Reward: [-540.255 -540.255 -540.255] [135.629], Avg: [-548.076 -548.076 -548.076] (0.1000) ({r_i: None, r_t: [-1149.642 -1149.642 -1149.642], eps: 0.1})
Step:   17900, Reward: [-1306.843 -1306.843 -1306.843] [225.841], Avg: [-600.115 -600.115 -600.115] (0.1662) ({r_i: None, r_t: [-2667.276 -2667.276 -2667.276], eps: 0.166})
Step:   49200, Reward: [-569.970 -569.970 -569.970] [106.236], Avg: [-548.121 -548.121 -548.121] (0.1000) ({r_i: None, r_t: [-1152.397 -1152.397 -1152.397], eps: 0.1})
Step:   18000, Reward: [-1271.630 -1271.630 -1271.630] [130.625], Avg: [-603.825 -603.825 -603.825] (0.1646) ({r_i: None, r_t: [-2608.772 -2608.772 -2608.772], eps: 0.165})
Step:   49300, Reward: [-563.682 -563.682 -563.682] [135.923], Avg: [-548.152 -548.152 -548.152] (0.1000) ({r_i: None, r_t: [-1081.588 -1081.588 -1081.588], eps: 0.1})
Step:   18100, Reward: [-1275.064 -1275.064 -1275.064] [197.480], Avg: [-607.513 -607.513 -607.513] (0.1629) ({r_i: None, r_t: [-2527.673 -2527.673 -2527.673], eps: 0.163})
Step:   49400, Reward: [-582.287 -582.287 -582.287] [120.854], Avg: [-548.221 -548.221 -548.221] (0.1000) ({r_i: None, r_t: [-1134.596 -1134.596 -1134.596], eps: 0.1})
Step:   18200, Reward: [-1192.266 -1192.266 -1192.266] [187.967], Avg: [-610.708 -610.708 -610.708] (0.1613) ({r_i: None, r_t: [-2544.216 -2544.216 -2544.216], eps: 0.161})
Step:   49500, Reward: [-533.484 -533.484 -533.484] [105.761], Avg: [-548.191 -548.191 -548.191] (0.1000) ({r_i: None, r_t: [-1127.160 -1127.160 -1127.160], eps: 0.1})
Step:   18300, Reward: [-1227.618 -1227.618 -1227.618] [158.145], Avg: [-614.061 -614.061 -614.061] (0.1597) ({r_i: None, r_t: [-2408.230 -2408.230 -2408.230], eps: 0.16})
Step:   49600, Reward: [-540.108 -540.108 -540.108] [140.790], Avg: [-548.175 -548.175 -548.175] (0.1000) ({r_i: None, r_t: [-1041.999 -1041.999 -1041.999], eps: 0.1})
Step:   18400, Reward: [-1179.137 -1179.137 -1179.137] [206.781], Avg: [-617.115 -617.115 -617.115] (0.1581) ({r_i: None, r_t: [-2292.409 -2292.409 -2292.409], eps: 0.158})
Step:   49700, Reward: [-577.657 -577.657 -577.657] [141.556], Avg: [-548.234 -548.234 -548.234] (0.1000) ({r_i: None, r_t: [-1054.403 -1054.403 -1054.403], eps: 0.1})
Step:   18500, Reward: [-1077.695 -1077.695 -1077.695] [197.902], Avg: [-619.592 -619.592 -619.592] (0.1565) ({r_i: None, r_t: [-2303.433 -2303.433 -2303.433], eps: 0.157})
Step:   49800, Reward: [-587.775 -587.775 -587.775] [132.082], Avg: [-548.314 -548.314 -548.314] (0.1000) ({r_i: None, r_t: [-1029.893 -1029.893 -1029.893], eps: 0.1})
Step:   18600, Reward: [-1054.098 -1054.098 -1054.098] [232.347], Avg: [-621.915 -621.915 -621.915] (0.1549) ({r_i: None, r_t: [-2269.448 -2269.448 -2269.448], eps: 0.155})
Step:   49900, Reward: [-557.470 -557.470 -557.470] [160.011], Avg: [-548.332 -548.332 -548.332] (0.1000) ({r_i: None, r_t: [-1132.814 -1132.814 -1132.814], eps: 0.1})
Step:   18700, Reward: [-966.336 -966.336 -966.336] [184.028], Avg: [-623.747 -623.747 -623.747] (0.1534) ({r_i: None, r_t: [-2084.842 -2084.842 -2084.842], eps: 0.153})
Step:   50000, Reward: [-498.595 -498.595 -498.595] [94.030], Avg: [-548.233 -548.233 -548.233] (0.1000) ({r_i: None, r_t: [-1105.166 -1105.166 -1105.166], eps: 0.1})
Step:   18800, Reward: [-983.880 -983.880 -983.880] [160.402], Avg: [-625.653 -625.653 -625.653] (0.1519) ({r_i: None, r_t: [-2145.418 -2145.418 -2145.418], eps: 0.152})
Step:   50100, Reward: [-546.440 -546.440 -546.440] [86.204], Avg: [-548.229 -548.229 -548.229] (0.1000) ({r_i: None, r_t: [-1069.950 -1069.950 -1069.950], eps: 0.1})
Step:   18900, Reward: [-912.456 -912.456 -912.456] [103.186], Avg: [-627.162 -627.162 -627.162] (0.1504) ({r_i: None, r_t: [-1972.155 -1972.155 -1972.155], eps: 0.15})
Step:   50200, Reward: [-555.946 -555.946 -555.946] [112.302], Avg: [-548.244 -548.244 -548.244] (0.1000) ({r_i: None, r_t: [-1065.986 -1065.986 -1065.986], eps: 0.1})
Step:   50300, Reward: [-467.473 -467.473 -467.473] [120.479], Avg: [-548.084 -548.084 -548.084] (0.1000) ({r_i: None, r_t: [-1065.670 -1065.670 -1065.670], eps: 0.1})
Step:   19000, Reward: [-921.636 -921.636 -921.636] [175.734], Avg: [-628.704 -628.704 -628.704] (0.1489) ({r_i: None, r_t: [-1812.626 -1812.626 -1812.626], eps: 0.149})
Step:   50400, Reward: [-550.507 -550.507 -550.507] [102.937], Avg: [-548.089 -548.089 -548.089] (0.1000) ({r_i: None, r_t: [-1077.893 -1077.893 -1077.893], eps: 0.1})
Step:   19100, Reward: [-860.543 -860.543 -860.543] [137.862], Avg: [-629.911 -629.911 -629.911] (0.1474) ({r_i: None, r_t: [-1731.034 -1731.034 -1731.034], eps: 0.147})
Step:   50500, Reward: [-475.056 -475.056 -475.056] [99.744], Avg: [-547.945 -547.945 -547.945] (0.1000) ({r_i: None, r_t: [-1090.831 -1090.831 -1090.831], eps: 0.1})
Step:   19200, Reward: [-891.375 -891.375 -891.375] [149.774], Avg: [-631.266 -631.266 -631.266] (0.1459) ({r_i: None, r_t: [-1822.385 -1822.385 -1822.385], eps: 0.146})
Step:   50600, Reward: [-542.369 -542.369 -542.369] [154.226], Avg: [-547.934 -547.934 -547.934] (0.1000) ({r_i: None, r_t: [-1102.171 -1102.171 -1102.171], eps: 0.1})
Step:   19300, Reward: [-900.184 -900.184 -900.184] [208.873], Avg: [-632.652 -632.652 -632.652] (0.1444) ({r_i: None, r_t: [-1728.126 -1728.126 -1728.126], eps: 0.144})
Step:   50700, Reward: [-582.647 -582.647 -582.647] [137.846], Avg: [-548.002 -548.002 -548.002] (0.1000) ({r_i: None, r_t: [-1076.827 -1076.827 -1076.827], eps: 0.1})
Step:   19400, Reward: [-827.111 -827.111 -827.111] [141.089], Avg: [-633.650 -633.650 -633.650] (0.1430) ({r_i: None, r_t: [-1652.333 -1652.333 -1652.333], eps: 0.143})
Step:   50800, Reward: [-485.192 -485.192 -485.192] [96.725], Avg: [-547.879 -547.879 -547.879] (0.1000) ({r_i: None, r_t: [-1036.299 -1036.299 -1036.299], eps: 0.1})
Step:   19500, Reward: [-803.337 -803.337 -803.337] [131.888], Avg: [-634.515 -634.515 -634.515] (0.1416) ({r_i: None, r_t: [-1698.019 -1698.019 -1698.019], eps: 0.142})
Step:   50900, Reward: [-576.771 -576.771 -576.771] [132.766], Avg: [-547.935 -547.935 -547.935] (0.1000) ({r_i: None, r_t: [-1022.688 -1022.688 -1022.688], eps: 0.1})
Step:   19600, Reward: [-742.519 -742.519 -742.519] [134.626], Avg: [-635.063 -635.063 -635.063] (0.1402) ({r_i: None, r_t: [-1540.428 -1540.428 -1540.428], eps: 0.14})
Step:   51000, Reward: [-514.155 -514.155 -514.155] [100.702], Avg: [-547.869 -547.869 -547.869] (0.1000) ({r_i: None, r_t: [-1014.972 -1014.972 -1014.972], eps: 0.1})
Step:   19700, Reward: [-733.297 -733.297 -733.297] [104.049], Avg: [-635.560 -635.560 -635.560] (0.1388) ({r_i: None, r_t: [-1541.497 -1541.497 -1541.497], eps: 0.139})
Step:   51100, Reward: [-513.169 -513.169 -513.169] [100.463], Avg: [-547.801 -547.801 -547.801] (0.1000) ({r_i: None, r_t: [-1050.019 -1050.019 -1050.019], eps: 0.1})
Step:   19800, Reward: [-738.836 -738.836 -738.836] [111.864], Avg: [-636.079 -636.079 -636.079] (0.1374) ({r_i: None, r_t: [-1541.859 -1541.859 -1541.859], eps: 0.137})
Step:   51200, Reward: [-513.164 -513.164 -513.164] [99.249], Avg: [-547.734 -547.734 -547.734] (0.1000) ({r_i: None, r_t: [-1058.225 -1058.225 -1058.225], eps: 0.1})
Step:   19900, Reward: [-672.975 -672.975 -672.975] [94.202], Avg: [-636.263 -636.263 -636.263] (0.1360) ({r_i: None, r_t: [-1336.550 -1336.550 -1336.550], eps: 0.136})
Step:   51300, Reward: [-524.488 -524.488 -524.488] [153.981], Avg: [-547.689 -547.689 -547.689] (0.1000) ({r_i: None, r_t: [-1043.259 -1043.259 -1043.259], eps: 0.1})
Step:   20000, Reward: [-672.031 -672.031 -672.031] [107.679], Avg: [-636.441 -636.441 -636.441] (0.1347) ({r_i: None, r_t: [-1406.295 -1406.295 -1406.295], eps: 0.135})
Step:   51400, Reward: [-485.735 -485.735 -485.735] [96.362], Avg: [-547.568 -547.568 -547.568] (0.1000) ({r_i: None, r_t: [-1077.871 -1077.871 -1077.871], eps: 0.1})
Step:   20100, Reward: [-679.623 -679.623 -679.623] [124.308], Avg: [-636.655 -636.655 -636.655] (0.1333) ({r_i: None, r_t: [-1355.231 -1355.231 -1355.231], eps: 0.133})
Step:   51500, Reward: [-515.752 -515.752 -515.752] [118.598], Avg: [-547.507 -547.507 -547.507] (0.1000) ({r_i: None, r_t: [-1001.687 -1001.687 -1001.687], eps: 0.1})
Step:   20200, Reward: [-683.344 -683.344 -683.344] [114.562], Avg: [-636.885 -636.885 -636.885] (0.1320) ({r_i: None, r_t: [-1342.632 -1342.632 -1342.632], eps: 0.132})
Step:   51600, Reward: [-543.303 -543.303 -543.303] [127.973], Avg: [-547.498 -547.498 -547.498] (0.1000) ({r_i: None, r_t: [-989.028 -989.028 -989.028], eps: 0.1})
Step:   20300, Reward: [-614.485 -614.485 -614.485] [102.221], Avg: [-636.775 -636.775 -636.775] (0.1307) ({r_i: None, r_t: [-1224.075 -1224.075 -1224.075], eps: 0.131})
Step:   51700, Reward: [-496.380 -496.380 -496.380] [122.888], Avg: [-547.400 -547.400 -547.400] (0.1000) ({r_i: None, r_t: [-1003.011 -1003.011 -1003.011], eps: 0.1})
Step:   20400, Reward: [-580.784 -580.784 -580.784] [69.900], Avg: [-636.502 -636.502 -636.502] (0.1294) ({r_i: None, r_t: [-1224.691 -1224.691 -1224.691], eps: 0.129})
Step:   51800, Reward: [-522.074 -522.074 -522.074] [150.789], Avg: [-547.351 -547.351 -547.351] (0.1000) ({r_i: None, r_t: [-1003.438 -1003.438 -1003.438], eps: 0.1})
Step:   20500, Reward: [-617.735 -617.735 -617.735] [138.269], Avg: [-636.411 -636.411 -636.411] (0.1281) ({r_i: None, r_t: [-1222.898 -1222.898 -1222.898], eps: 0.128})
Step:   51900, Reward: [-502.508 -502.508 -502.508] [92.695], Avg: [-547.265 -547.265 -547.265] (0.1000) ({r_i: None, r_t: [-1043.935 -1043.935 -1043.935], eps: 0.1})
Step:   20600, Reward: [-593.032 -593.032 -593.032] [100.138], Avg: [-636.201 -636.201 -636.201] (0.1268) ({r_i: None, r_t: [-1195.159 -1195.159 -1195.159], eps: 0.127})
Step:   52000, Reward: [-522.685 -522.685 -522.685] [98.234], Avg: [-547.218 -547.218 -547.218] (0.1000) ({r_i: None, r_t: [-1023.331 -1023.331 -1023.331], eps: 0.1})
Step:   20700, Reward: [-549.843 -549.843 -549.843] [95.496], Avg: [-635.786 -635.786 -635.786] (0.1255) ({r_i: None, r_t: [-1168.507 -1168.507 -1168.507], eps: 0.126})
Step:   52100, Reward: [-573.631 -573.631 -573.631] [137.681], Avg: [-547.268 -547.268 -547.268] (0.1000) ({r_i: None, r_t: [-1004.324 -1004.324 -1004.324], eps: 0.1})
Step:   20800, Reward: [-559.783 -559.783 -559.783] [94.568], Avg: [-635.422 -635.422 -635.422] (0.1243) ({r_i: None, r_t: [-1117.422 -1117.422 -1117.422], eps: 0.124})
Step:   52200, Reward: [-473.065 -473.065 -473.065] [82.932], Avg: [-547.126 -547.126 -547.126] (0.1000) ({r_i: None, r_t: [-1036.008 -1036.008 -1036.008], eps: 0.1})
Step:   20900, Reward: [-521.422 -521.422 -521.422] [69.737], Avg: [-634.880 -634.880 -634.880] (0.1230) ({r_i: None, r_t: [-1089.778 -1089.778 -1089.778], eps: 0.123})
Step:   52300, Reward: [-479.807 -479.807 -479.807] [102.163], Avg: [-546.998 -546.998 -546.998] (0.1000) ({r_i: None, r_t: [-1062.472 -1062.472 -1062.472], eps: 0.1})
Step:   21000, Reward: [-553.437 -553.437 -553.437] [144.254], Avg: [-634.494 -634.494 -634.494] (0.1218) ({r_i: None, r_t: [-1037.838 -1037.838 -1037.838], eps: 0.122})
Step:   52400, Reward: [-493.618 -493.618 -493.618] [85.097], Avg: [-546.896 -546.896 -546.896] (0.1000) ({r_i: None, r_t: [-991.294 -991.294 -991.294], eps: 0.1})
Step:   21100, Reward: [-597.589 -597.589 -597.589] [106.688], Avg: [-634.319 -634.319 -634.319] (0.1206) ({r_i: None, r_t: [-1075.904 -1075.904 -1075.904], eps: 0.121})
Step:   52500, Reward: [-514.670 -514.670 -514.670] [133.084], Avg: [-546.835 -546.835 -546.835] (0.1000) ({r_i: None, r_t: [-1040.270 -1040.270 -1040.270], eps: 0.1})
Step:   21200, Reward: [-510.170 -510.170 -510.170] [62.285], Avg: [-633.737 -633.737 -633.737] (0.1194) ({r_i: None, r_t: [-1114.127 -1114.127 -1114.127], eps: 0.119})
Step:   52600, Reward: [-503.623 -503.623 -503.623] [114.250], Avg: [-546.753 -546.753 -546.753] (0.1000) ({r_i: None, r_t: [-1012.578 -1012.578 -1012.578], eps: 0.1})
Step:   21300, Reward: [-494.878 -494.878 -494.878] [65.607], Avg: [-633.088 -633.088 -633.088] (0.1182) ({r_i: None, r_t: [-1028.562 -1028.562 -1028.562], eps: 0.118})
Step:   52700, Reward: [-429.252 -429.252 -429.252] [61.968], Avg: [-546.530 -546.530 -546.530] (0.1000) ({r_i: None, r_t: [-1031.348 -1031.348 -1031.348], eps: 0.1})
Step:   21400, Reward: [-541.711 -541.711 -541.711] [103.884], Avg: [-632.663 -632.663 -632.663] (0.1170) ({r_i: None, r_t: [-1039.321 -1039.321 -1039.321], eps: 0.117})
Step:   52800, Reward: [-495.775 -495.775 -495.775] [96.787], Avg: [-546.434 -546.434 -546.434] (0.1000) ({r_i: None, r_t: [-948.938 -948.938 -948.938], eps: 0.1})
Step:   21500, Reward: [-489.794 -489.794 -489.794] [75.390], Avg: [-632.001 -632.001 -632.001] (0.1159) ({r_i: None, r_t: [-987.331 -987.331 -987.331], eps: 0.116})
Step:   52900, Reward: [-493.639 -493.639 -493.639] [125.813], Avg: [-546.335 -546.335 -546.335] (0.1000) ({r_i: None, r_t: [-1019.731 -1019.731 -1019.731], eps: 0.1})
Step:   21600, Reward: [-485.885 -485.885 -485.885] [65.141], Avg: [-631.328 -631.328 -631.328] (0.1147) ({r_i: None, r_t: [-1018.143 -1018.143 -1018.143], eps: 0.115})
Step:   53000, Reward: [-454.964 -454.964 -454.964] [88.846], Avg: [-546.163 -546.163 -546.163] (0.1000) ({r_i: None, r_t: [-975.745 -975.745 -975.745], eps: 0.1})
Step:   21700, Reward: [-505.311 -505.311 -505.311] [98.930], Avg: [-630.750 -630.750 -630.750] (0.1136) ({r_i: None, r_t: [-959.058 -959.058 -959.058], eps: 0.114})
Step:   53100, Reward: [-520.713 -520.713 -520.713] [140.974], Avg: [-546.115 -546.115 -546.115] (0.1000) ({r_i: None, r_t: [-1042.667 -1042.667 -1042.667], eps: 0.1})
Step:   21800, Reward: [-488.411 -488.411 -488.411] [86.224], Avg: [-630.100 -630.100 -630.100] (0.1124) ({r_i: None, r_t: [-971.932 -971.932 -971.932], eps: 0.112})
Step:   53200, Reward: [-509.368 -509.368 -509.368] [146.772], Avg: [-546.046 -546.046 -546.046] (0.1000) ({r_i: None, r_t: [-938.246 -938.246 -938.246], eps: 0.1})
Step:   21900, Reward: [-460.764 -460.764 -460.764] [63.461], Avg: [-629.330 -629.330 -629.330] (0.1113) ({r_i: None, r_t: [-975.444 -975.444 -975.444], eps: 0.111})
Step:   53300, Reward: [-468.974 -468.974 -468.974] [93.085], Avg: [-545.902 -545.902 -545.902] (0.1000) ({r_i: None, r_t: [-948.390 -948.390 -948.390], eps: 0.1})
Step:   53400, Reward: [-533.254 -533.254 -533.254] [141.735], Avg: [-545.878 -545.878 -545.878] (0.1000) ({r_i: None, r_t: [-948.864 -948.864 -948.864], eps: 0.1})
Step:   22000, Reward: [-489.380 -489.380 -489.380] [113.272], Avg: [-628.697 -628.697 -628.697] (0.1102) ({r_i: None, r_t: [-928.347 -928.347 -928.347], eps: 0.11})
Step:   53500, Reward: [-487.568 -487.568 -487.568] [76.831], Avg: [-545.769 -545.769 -545.769] (0.1000) ({r_i: None, r_t: [-1007.043 -1007.043 -1007.043], eps: 0.1})
Step:   22100, Reward: [-449.564 -449.564 -449.564] [56.902], Avg: [-627.890 -627.890 -627.890] (0.1091) ({r_i: None, r_t: [-952.972 -952.972 -952.972], eps: 0.109})
Step:   53600, Reward: [-527.052 -527.052 -527.052] [117.406], Avg: [-545.734 -545.734 -545.734] (0.1000) ({r_i: None, r_t: [-1026.900 -1026.900 -1026.900], eps: 0.1})
Step:   22200, Reward: [-449.671 -449.671 -449.671] [67.471], Avg: [-627.091 -627.091 -627.091] (0.1080) ({r_i: None, r_t: [-949.000 -949.000 -949.000], eps: 0.108})
Step:   53700, Reward: [-451.529 -451.529 -451.529] [81.946], Avg: [-545.559 -545.559 -545.559] (0.1000) ({r_i: None, r_t: [-973.623 -973.623 -973.623], eps: 0.1})
Step:   22300, Reward: [-437.344 -437.344 -437.344] [59.245], Avg: [-626.244 -626.244 -626.244] (0.1069) ({r_i: None, r_t: [-895.048 -895.048 -895.048], eps: 0.107})
Step:   53800, Reward: [-463.827 -463.827 -463.827] [94.913], Avg: [-545.408 -545.408 -545.408] (0.1000) ({r_i: None, r_t: [-978.434 -978.434 -978.434], eps: 0.1})
Step:   22400, Reward: [-478.916 -478.916 -478.916] [87.049], Avg: [-625.589 -625.589 -625.589] (0.1059) ({r_i: None, r_t: [-864.795 -864.795 -864.795], eps: 0.106})
Step:   53900, Reward: [-500.472 -500.472 -500.472] [73.420], Avg: [-545.324 -545.324 -545.324] (0.1000) ({r_i: None, r_t: [-956.901 -956.901 -956.901], eps: 0.1})
Step:   22500, Reward: [-392.352 -392.352 -392.352] [56.997], Avg: [-624.557 -624.557 -624.557] (0.1048) ({r_i: None, r_t: [-905.967 -905.967 -905.967], eps: 0.105})
Step:   54000, Reward: [-461.038 -461.038 -461.038] [71.163], Avg: [-545.169 -545.169 -545.169] (0.1000) ({r_i: None, r_t: [-972.406 -972.406 -972.406], eps: 0.1})
Step:   22600, Reward: [-417.599 -417.599 -417.599] [43.483], Avg: [-623.645 -623.645 -623.645] (0.1038) ({r_i: None, r_t: [-860.338 -860.338 -860.338], eps: 0.104})
Step:   54100, Reward: [-492.844 -492.844 -492.844] [72.539], Avg: [-545.072 -545.072 -545.072] (0.1000) ({r_i: None, r_t: [-995.467 -995.467 -995.467], eps: 0.1})
Step:   22700, Reward: [-406.140 -406.140 -406.140] [62.084], Avg: [-622.691 -622.691 -622.691] (0.1027) ({r_i: None, r_t: [-873.065 -873.065 -873.065], eps: 0.103})
Step:   54200, Reward: [-504.819 -504.819 -504.819] [125.944], Avg: [-544.998 -544.998 -544.998] (0.1000) ({r_i: None, r_t: [-987.135 -987.135 -987.135], eps: 0.1})
Step:   22800, Reward: [-440.855 -440.855 -440.855] [72.794], Avg: [-621.897 -621.897 -621.897] (0.1017) ({r_i: None, r_t: [-913.143 -913.143 -913.143], eps: 0.102})
Step:   54300, Reward: [-505.127 -505.127 -505.127] [107.044], Avg: [-544.925 -544.925 -544.925] (0.1000) ({r_i: None, r_t: [-912.933 -912.933 -912.933], eps: 0.1})
Step:   22900, Reward: [-472.841 -472.841 -472.841] [68.104], Avg: [-621.249 -621.249 -621.249] (0.1007) ({r_i: None, r_t: [-876.998 -876.998 -876.998], eps: 0.101})
Step:   54400, Reward: [-501.644 -501.644 -501.644] [65.585], Avg: [-544.845 -544.845 -544.845] (0.1000) ({r_i: None, r_t: [-987.759 -987.759 -987.759], eps: 0.1})
Step:   23000, Reward: [-447.957 -447.957 -447.957] [86.231], Avg: [-620.499 -620.499 -620.499] (0.0997) ({r_i: None, r_t: [-889.329 -889.329 -889.329], eps: 0.1})
Step:   54500, Reward: [-459.275 -459.275 -459.275] [97.268], Avg: [-544.688 -544.688 -544.688] (0.1000) ({r_i: None, r_t: [-947.448 -947.448 -947.448], eps: 0.1})
Step:   23100, Reward: [-389.250 -389.250 -389.250] [64.384], Avg: [-619.502 -619.502 -619.502] (0.0987) ({r_i: None, r_t: [-883.322 -883.322 -883.322], eps: 0.099})
Step:   54600, Reward: [-496.670 -496.670 -496.670] [123.013], Avg: [-544.601 -544.601 -544.601] (0.1000) ({r_i: None, r_t: [-996.997 -996.997 -996.997], eps: 0.1})
Step:   23200, Reward: [-430.740 -430.740 -430.740] [55.757], Avg: [-618.692 -618.692 -618.692] (0.0977) ({r_i: None, r_t: [-855.728 -855.728 -855.728], eps: 0.098})
Step:   54700, Reward: [-510.801 -510.801 -510.801] [81.260], Avg: [-544.539 -544.539 -544.539] (0.1000) ({r_i: None, r_t: [-1000.557 -1000.557 -1000.557], eps: 0.1})
Step:   23300, Reward: [-427.738 -427.738 -427.738] [100.424], Avg: [-617.876 -617.876 -617.876] (0.0967) ({r_i: None, r_t: [-872.172 -872.172 -872.172], eps: 0.097})
Step:   54800, Reward: [-456.114 -456.114 -456.114] [79.763], Avg: [-544.378 -544.378 -544.378] (0.1000) ({r_i: None, r_t: [-972.356 -972.356 -972.356], eps: 0.1})
Step:   23400, Reward: [-447.105 -447.105 -447.105] [103.561], Avg: [-617.149 -617.149 -617.149] (0.0958) ({r_i: None, r_t: [-858.379 -858.379 -858.379], eps: 0.096})
Step:   54900, Reward: [-487.161 -487.161 -487.161] [82.441], Avg: [-544.274 -544.274 -544.274] (0.1000) ({r_i: None, r_t: [-1025.915 -1025.915 -1025.915], eps: 0.1})
Step:   23500, Reward: [-426.497 -426.497 -426.497] [67.929], Avg: [-616.342 -616.342 -616.342] (0.0948) ({r_i: None, r_t: [-879.672 -879.672 -879.672], eps: 0.095})
Step:   55000, Reward: [-509.354 -509.354 -509.354] [113.517], Avg: [-544.211 -544.211 -544.211] (0.1000) ({r_i: None, r_t: [-920.716 -920.716 -920.716], eps: 0.1})
Step:   23600, Reward: [-448.783 -448.783 -448.783] [97.697], Avg: [-615.635 -615.635 -615.635] (0.0939) ({r_i: None, r_t: [-857.730 -857.730 -857.730], eps: 0.094})
Step:   55100, Reward: [-490.998 -490.998 -490.998] [87.227], Avg: [-544.114 -544.114 -544.114] (0.1000) ({r_i: None, r_t: [-944.683 -944.683 -944.683], eps: 0.1})
Step:   23700, Reward: [-410.053 -410.053 -410.053] [66.229], Avg: [-614.771 -614.771 -614.771] (0.0929) ({r_i: None, r_t: [-859.763 -859.763 -859.763], eps: 0.093})
Step:   55200, Reward: [-466.322 -466.322 -466.322] [74.307], Avg: [-543.973 -543.973 -543.973] (0.1000) ({r_i: None, r_t: [-985.762 -985.762 -985.762], eps: 0.1})
Step:   23800, Reward: [-453.955 -453.955 -453.955] [59.428], Avg: [-614.098 -614.098 -614.098] (0.0920) ({r_i: None, r_t: [-885.218 -885.218 -885.218], eps: 0.092})
Step:   55300, Reward: [-466.330 -466.330 -466.330] [79.390], Avg: [-543.833 -543.833 -543.833] (0.1000) ({r_i: None, r_t: [-982.111 -982.111 -982.111], eps: 0.1})
Step:   23900, Reward: [-459.331 -459.331 -459.331] [62.505], Avg: [-613.453 -613.453 -613.453] (0.0911) ({r_i: None, r_t: [-865.540 -865.540 -865.540], eps: 0.091})
Step:   55400, Reward: [-453.301 -453.301 -453.301] [86.403], Avg: [-543.670 -543.670 -543.670] (0.1000) ({r_i: None, r_t: [-1001.876 -1001.876 -1001.876], eps: 0.1})
Step:   24000, Reward: [-451.858 -451.858 -451.858] [104.164], Avg: [-612.782 -612.782 -612.782] (0.0902) ({r_i: None, r_t: [-868.752 -868.752 -868.752], eps: 0.09})
Step:   55500, Reward: [-499.121 -499.121 -499.121] [113.135], Avg: [-543.590 -543.590 -543.590] (0.1000) ({r_i: None, r_t: [-957.850 -957.850 -957.850], eps: 0.1})
Step:   24100, Reward: [-461.742 -461.742 -461.742] [84.180], Avg: [-612.158 -612.158 -612.158] (0.0893) ({r_i: None, r_t: [-909.963 -909.963 -909.963], eps: 0.089})
Step:   55600, Reward: [-507.774 -507.774 -507.774] [127.903], Avg: [-543.526 -543.526 -543.526] (0.1000) ({r_i: None, r_t: [-939.321 -939.321 -939.321], eps: 0.1})
Step:   24200, Reward: [-440.739 -440.739 -440.739] [69.428], Avg: [-611.453 -611.453 -611.453] (0.0884) ({r_i: None, r_t: [-903.501 -903.501 -903.501], eps: 0.088})
Step:   55700, Reward: [-505.428 -505.428 -505.428] [103.252], Avg: [-543.458 -543.458 -543.458] (0.1000) ({r_i: None, r_t: [-906.933 -906.933 -906.933], eps: 0.1})
Step:   24300, Reward: [-422.686 -422.686 -422.686] [77.288], Avg: [-610.679 -610.679 -610.679] (0.0875) ({r_i: None, r_t: [-875.612 -875.612 -875.612], eps: 0.088})
Step:   55800, Reward: [-442.234 -442.234 -442.234] [52.649], Avg: [-543.276 -543.276 -543.276] (0.1000) ({r_i: None, r_t: [-958.836 -958.836 -958.836], eps: 0.1})
Step:   24400, Reward: [-444.365 -444.365 -444.365] [80.312], Avg: [-610.000 -610.000 -610.000] (0.0866) ({r_i: None, r_t: [-877.151 -877.151 -877.151], eps: 0.087})
Step:   55900, Reward: [-454.756 -454.756 -454.756] [69.702], Avg: [-543.118 -543.118 -543.118] (0.1000) ({r_i: None, r_t: [-973.538 -973.538 -973.538], eps: 0.1})
Step:   24500, Reward: [-471.827 -471.827 -471.827] [114.199], Avg: [-609.439 -609.439 -609.439] (0.0858) ({r_i: None, r_t: [-917.171 -917.171 -917.171], eps: 0.086})
Step:   56000, Reward: [-482.380 -482.380 -482.380] [90.554], Avg: [-543.010 -543.010 -543.010] (0.1000) ({r_i: None, r_t: [-935.812 -935.812 -935.812], eps: 0.1})
Step:   56100, Reward: [-466.091 -466.091 -466.091] [82.292], Avg: [-542.873 -542.873 -542.873] (0.1000) ({r_i: None, r_t: [-955.331 -955.331 -955.331], eps: 0.1})
Step:   24600, Reward: [-397.480 -397.480 -397.480] [47.017], Avg: [-608.581 -608.581 -608.581] (0.0849) ({r_i: None, r_t: [-930.788 -930.788 -930.788], eps: 0.085})
Step:   56200, Reward: [-449.901 -449.901 -449.901] [46.159], Avg: [-542.708 -542.708 -542.708] (0.1000) ({r_i: None, r_t: [-997.115 -997.115 -997.115], eps: 0.1})
Step:   24700, Reward: [-473.178 -473.178 -473.178] [97.365], Avg: [-608.035 -608.035 -608.035] (0.0841) ({r_i: None, r_t: [-922.438 -922.438 -922.438], eps: 0.084})
Step:   56300, Reward: [-486.505 -486.505 -486.505] [85.104], Avg: [-542.608 -542.608 -542.608] (0.1000) ({r_i: None, r_t: [-904.570 -904.570 -904.570], eps: 0.1})
Step:   24800, Reward: [-467.914 -467.914 -467.914] [97.173], Avg: [-607.472 -607.472 -607.472] (0.0832) ({r_i: None, r_t: [-904.374 -904.374 -904.374], eps: 0.083})
Step:   56400, Reward: [-461.805 -461.805 -461.805] [67.785], Avg: [-542.465 -542.465 -542.465] (0.1000) ({r_i: None, r_t: [-942.265 -942.265 -942.265], eps: 0.1})
Step:   24900, Reward: [-462.381 -462.381 -462.381] [112.244], Avg: [-606.892 -606.892 -606.892] (0.0824) ({r_i: None, r_t: [-894.043 -894.043 -894.043], eps: 0.082})
Step:   56500, Reward: [-523.173 -523.173 -523.173] [80.150], Avg: [-542.431 -542.431 -542.431] (0.1000) ({r_i: None, r_t: [-934.280 -934.280 -934.280], eps: 0.1})
Step:   25000, Reward: [-453.843 -453.843 -453.843] [55.359], Avg: [-606.282 -606.282 -606.282] (0.0816) ({r_i: None, r_t: [-867.362 -867.362 -867.362], eps: 0.082})
Step:   56600, Reward: [-471.597 -471.597 -471.597] [64.929], Avg: [-542.306 -542.306 -542.306] (0.1000) ({r_i: None, r_t: [-884.668 -884.668 -884.668], eps: 0.1})
Step:   25100, Reward: [-450.087 -450.087 -450.087] [70.183], Avg: [-605.662 -605.662 -605.662] (0.0808) ({r_i: None, r_t: [-902.882 -902.882 -902.882], eps: 0.081})
Step:   56700, Reward: [-502.773 -502.773 -502.773] [128.781], Avg: [-542.237 -542.237 -542.237] (0.1000) ({r_i: None, r_t: [-904.668 -904.668 -904.668], eps: 0.1})
Step:   25200, Reward: [-460.594 -460.594 -460.594] [81.638], Avg: [-605.089 -605.089 -605.089] (0.0800) ({r_i: None, r_t: [-853.559 -853.559 -853.559], eps: 0.08})
Step:   56800, Reward: [-454.756 -454.756 -454.756] [80.729], Avg: [-542.083 -542.083 -542.083] (0.1000) ({r_i: None, r_t: [-926.241 -926.241 -926.241], eps: 0.1})
Step:   25300, Reward: [-499.246 -499.246 -499.246] [90.852], Avg: [-604.672 -604.672 -604.672] (0.0792) ({r_i: None, r_t: [-911.939 -911.939 -911.939], eps: 0.079})
Step:   56900, Reward: [-458.342 -458.342 -458.342] [97.238], Avg: [-541.936 -541.936 -541.936] (0.1000) ({r_i: None, r_t: [-953.658 -953.658 -953.658], eps: 0.1})
Step:   25400, Reward: [-518.684 -518.684 -518.684] [147.801], Avg: [-604.335 -604.335 -604.335] (0.0784) ({r_i: None, r_t: [-861.755 -861.755 -861.755], eps: 0.078})
Step:   57000, Reward: [-466.658 -466.658 -466.658] [100.670], Avg: [-541.804 -541.804 -541.804] (0.1000) ({r_i: None, r_t: [-949.783 -949.783 -949.783], eps: 0.1})
Step:   25500, Reward: [-460.434 -460.434 -460.434] [113.612], Avg: [-603.773 -603.773 -603.773] (0.0776) ({r_i: None, r_t: [-906.067 -906.067 -906.067], eps: 0.078})
Step:   57100, Reward: [-439.626 -439.626 -439.626] [58.591], Avg: [-541.626 -541.626 -541.626] (0.1000) ({r_i: None, r_t: [-970.170 -970.170 -970.170], eps: 0.1})
Step:   25600, Reward: [-429.822 -429.822 -429.822] [82.424], Avg: [-603.096 -603.096 -603.096] (0.0768) ({r_i: None, r_t: [-947.148 -947.148 -947.148], eps: 0.077})
Step:   57200, Reward: [-498.579 -498.579 -498.579] [108.054], Avg: [-541.551 -541.551 -541.551] (0.1000) ({r_i: None, r_t: [-958.902 -958.902 -958.902], eps: 0.1})
Step:   25700, Reward: [-439.860 -439.860 -439.860] [64.325], Avg: [-602.463 -602.463 -602.463] (0.0760) ({r_i: None, r_t: [-931.818 -931.818 -931.818], eps: 0.076})
Step:   57300, Reward: [-481.305 -481.305 -481.305] [90.707], Avg: [-541.446 -541.446 -541.446] (0.1000) ({r_i: None, r_t: [-927.108 -927.108 -927.108], eps: 0.1})
Step:   25800, Reward: [-421.675 -421.675 -421.675] [75.563], Avg: [-601.765 -601.765 -601.765] (0.0753) ({r_i: None, r_t: [-906.166 -906.166 -906.166], eps: 0.075})
Step:   57400, Reward: [-495.365 -495.365 -495.365] [120.743], Avg: [-541.365 -541.365 -541.365] (0.1000) ({r_i: None, r_t: [-918.805 -918.805 -918.805], eps: 0.1})
Step:   25900, Reward: [-474.099 -474.099 -474.099] [95.102], Avg: [-601.274 -601.274 -601.274] (0.0745) ({r_i: None, r_t: [-905.938 -905.938 -905.938], eps: 0.075})
Step:   57500, Reward: [-476.090 -476.090 -476.090] [60.830], Avg: [-541.252 -541.252 -541.252] (0.1000) ({r_i: None, r_t: [-1018.374 -1018.374 -1018.374], eps: 0.1})
Step:   26000, Reward: [-451.095 -451.095 -451.095] [89.890], Avg: [-600.699 -600.699 -600.699] (0.0738) ({r_i: None, r_t: [-1002.859 -1002.859 -1002.859], eps: 0.074})
Step:   57600, Reward: [-445.482 -445.482 -445.482] [55.448], Avg: [-541.086 -541.086 -541.086] (0.1000) ({r_i: None, r_t: [-925.686 -925.686 -925.686], eps: 0.1})
Step:   26100, Reward: [-483.051 -483.051 -483.051] [97.961], Avg: [-600.250 -600.250 -600.250] (0.0731) ({r_i: None, r_t: [-960.423 -960.423 -960.423], eps: 0.073})
Step:   57700, Reward: [-471.417 -471.417 -471.417] [96.510], Avg: [-540.966 -540.966 -540.966] (0.1000) ({r_i: None, r_t: [-975.251 -975.251 -975.251], eps: 0.1})
Step:   26200, Reward: [-483.269 -483.269 -483.269] [94.278], Avg: [-599.805 -599.805 -599.805] (0.0723) ({r_i: None, r_t: [-915.345 -915.345 -915.345], eps: 0.072})
Step:   57800, Reward: [-431.093 -431.093 -431.093] [83.465], Avg: [-540.776 -540.776 -540.776] (0.1000) ({r_i: None, r_t: [-910.878 -910.878 -910.878], eps: 0.1})
Step:   26300, Reward: [-481.367 -481.367 -481.367] [132.454], Avg: [-599.356 -599.356 -599.356] (0.0716) ({r_i: None, r_t: [-1004.435 -1004.435 -1004.435], eps: 0.072})
Step:   57900, Reward: [-481.982 -481.982 -481.982] [113.639], Avg: [-540.674 -540.674 -540.674] (0.1000) ({r_i: None, r_t: [-926.386 -926.386 -926.386], eps: 0.1})
Step:   26400, Reward: [-468.064 -468.064 -468.064] [68.270], Avg: [-598.861 -598.861 -598.861] (0.0709) ({r_i: None, r_t: [-959.388 -959.388 -959.388], eps: 0.071})
Step:   58000, Reward: [-467.466 -467.466 -467.466] [86.908], Avg: [-540.548 -540.548 -540.548] (0.1000) ({r_i: None, r_t: [-944.483 -944.483 -944.483], eps: 0.1})
Step:   26500, Reward: [-492.338 -492.338 -492.338] [140.865], Avg: [-598.460 -598.460 -598.460] (0.0702) ({r_i: None, r_t: [-1086.574 -1086.574 -1086.574], eps: 0.07})
Step:   58100, Reward: [-477.360 -477.360 -477.360] [87.792], Avg: [-540.440 -540.440 -540.440] (0.1000) ({r_i: None, r_t: [-904.678 -904.678 -904.678], eps: 0.1})
Step:   26600, Reward: [-555.074 -555.074 -555.074] [139.634], Avg: [-598.298 -598.298 -598.298] (0.0695) ({r_i: None, r_t: [-959.687 -959.687 -959.687], eps: 0.069})
Step:   58200, Reward: [-456.758 -456.758 -456.758] [69.911], Avg: [-540.296 -540.296 -540.296] (0.1000) ({r_i: None, r_t: [-944.430 -944.430 -944.430], eps: 0.1})
Step:   26700, Reward: [-444.974 -444.974 -444.974] [79.875], Avg: [-597.726 -597.726 -597.726] (0.0688) ({r_i: None, r_t: [-1050.030 -1050.030 -1050.030], eps: 0.069})
Step:   58300, Reward: [-480.013 -480.013 -480.013] [126.985], Avg: [-540.193 -540.193 -540.193] (0.1000) ({r_i: None, r_t: [-917.426 -917.426 -917.426], eps: 0.1})
Step:   26800, Reward: [-524.210 -524.210 -524.210] [117.909], Avg: [-597.452 -597.452 -597.452] (0.0681) ({r_i: None, r_t: [-1036.196 -1036.196 -1036.196], eps: 0.068})
Step:   58400, Reward: [-450.957 -450.957 -450.957] [106.721], Avg: [-540.041 -540.041 -540.041] (0.1000) ({r_i: None, r_t: [-926.401 -926.401 -926.401], eps: 0.1})
Step:   26900, Reward: [-526.670 -526.670 -526.670] [144.858], Avg: [-597.190 -597.190 -597.190] (0.0674) ({r_i: None, r_t: [-1004.581 -1004.581 -1004.581], eps: 0.067})
Step:   58500, Reward: [-449.490 -449.490 -449.490] [105.199], Avg: [-539.886 -539.886 -539.886] (0.1000) ({r_i: None, r_t: [-925.113 -925.113 -925.113], eps: 0.1})
Step:   58600, Reward: [-489.608 -489.608 -489.608] [72.236], Avg: [-539.800 -539.800 -539.800] (0.1000) ({r_i: None, r_t: [-927.142 -927.142 -927.142], eps: 0.1})
Step:   27000, Reward: [-445.992 -445.992 -445.992] [66.551], Avg: [-596.632 -596.632 -596.632] (0.0668) ({r_i: None, r_t: [-962.296 -962.296 -962.296], eps: 0.067})
Step:   58700, Reward: [-458.318 -458.318 -458.318] [85.312], Avg: [-539.662 -539.662 -539.662] (0.1000) ({r_i: None, r_t: [-903.388 -903.388 -903.388], eps: 0.1})
Step:   27100, Reward: [-529.694 -529.694 -529.694] [125.193], Avg: [-596.386 -596.386 -596.386] (0.0661) ({r_i: None, r_t: [-1022.570 -1022.570 -1022.570], eps: 0.066})
Step:   58800, Reward: [-480.936 -480.936 -480.936] [80.977], Avg: [-539.562 -539.562 -539.562] (0.1000) ({r_i: None, r_t: [-917.772 -917.772 -917.772], eps: 0.1})
Step:   27200, Reward: [-565.117 -565.117 -565.117] [120.319], Avg: [-596.272 -596.272 -596.272] (0.0654) ({r_i: None, r_t: [-1106.455 -1106.455 -1106.455], eps: 0.065})
Step:   58900, Reward: [-478.892 -478.892 -478.892] [95.346], Avg: [-539.459 -539.459 -539.459] (0.1000) ({r_i: None, r_t: [-977.375 -977.375 -977.375], eps: 0.1})
Step:   27300, Reward: [-530.905 -530.905 -530.905] [162.833], Avg: [-596.033 -596.033 -596.033] (0.0648) ({r_i: None, r_t: [-1089.139 -1089.139 -1089.139], eps: 0.065})
Step:   59000, Reward: [-448.422 -448.422 -448.422] [74.167], Avg: [-539.305 -539.305 -539.305] (0.1000) ({r_i: None, r_t: [-961.975 -961.975 -961.975], eps: 0.1})
Step:   27400, Reward: [-494.342 -494.342 -494.342] [89.805], Avg: [-595.663 -595.663 -595.663] (0.0641) ({r_i: None, r_t: [-1098.089 -1098.089 -1098.089], eps: 0.064})
Step:   59100, Reward: [-486.521 -486.521 -486.521] [75.023], Avg: [-539.216 -539.216 -539.216] (0.1000) ({r_i: None, r_t: [-935.429 -935.429 -935.429], eps: 0.1})
Step:   27500, Reward: [-517.806 -517.806 -517.806] [101.684], Avg: [-595.381 -595.381 -595.381] (0.0635) ({r_i: None, r_t: [-1052.474 -1052.474 -1052.474], eps: 0.063})
Step:   59200, Reward: [-461.717 -461.717 -461.717] [102.064], Avg: [-539.085 -539.085 -539.085] (0.1000) ({r_i: None, r_t: [-917.941 -917.941 -917.941], eps: 0.1})
Step:   27600, Reward: [-541.911 -541.911 -541.911] [127.168], Avg: [-595.188 -595.188 -595.188] (0.0629) ({r_i: None, r_t: [-1233.370 -1233.370 -1233.370], eps: 0.063})
Step:   59300, Reward: [-456.201 -456.201 -456.201] [96.420], Avg: [-538.946 -538.946 -538.946] (0.1000) ({r_i: None, r_t: [-1010.683 -1010.683 -1010.683], eps: 0.1})
Step:   27700, Reward: [-587.629 -587.629 -587.629] [160.064], Avg: [-595.161 -595.161 -595.161] (0.0622) ({r_i: None, r_t: [-1145.690 -1145.690 -1145.690], eps: 0.062})
Step:   59400, Reward: [-462.770 -462.770 -462.770] [83.210], Avg: [-538.818 -538.818 -538.818] (0.1000) ({r_i: None, r_t: [-929.715 -929.715 -929.715], eps: 0.1})
Step:   27800, Reward: [-638.860 -638.860 -638.860] [153.181], Avg: [-595.318 -595.318 -595.318] (0.0616) ({r_i: None, r_t: [-1120.323 -1120.323 -1120.323], eps: 0.062})
Step:   59500, Reward: [-462.179 -462.179 -462.179] [71.629], Avg: [-538.689 -538.689 -538.689] (0.1000) ({r_i: None, r_t: [-949.767 -949.767 -949.767], eps: 0.1})
Step:   27900, Reward: [-568.402 -568.402 -568.402] [86.795], Avg: [-595.221 -595.221 -595.221] (0.0610) ({r_i: None, r_t: [-1178.934 -1178.934 -1178.934], eps: 0.061})
Step:   59600, Reward: [-477.775 -477.775 -477.775] [78.413], Avg: [-538.587 -538.587 -538.587] (0.1000) ({r_i: None, r_t: [-935.378 -935.378 -935.378], eps: 0.1})
Step:   28000, Reward: [-588.898 -588.898 -588.898] [167.886], Avg: [-595.199 -595.199 -595.199] (0.0604) ({r_i: None, r_t: [-1209.168 -1209.168 -1209.168], eps: 0.06})
Step:   59700, Reward: [-475.341 -475.341 -475.341] [109.145], Avg: [-538.481 -538.481 -538.481] (0.1000) ({r_i: None, r_t: [-962.317 -962.317 -962.317], eps: 0.1})
Step:   28100, Reward: [-585.354 -585.354 -585.354] [155.037], Avg: [-595.164 -595.164 -595.164] (0.0598) ({r_i: None, r_t: [-1065.565 -1065.565 -1065.565], eps: 0.06})
Step:   59800, Reward: [-477.731 -477.731 -477.731] [82.287], Avg: [-538.380 -538.380 -538.380] (0.1000) ({r_i: None, r_t: [-944.305 -944.305 -944.305], eps: 0.1})
Step:   28200, Reward: [-617.657 -617.657 -617.657] [133.412], Avg: [-595.243 -595.243 -595.243] (0.0592) ({r_i: None, r_t: [-1052.314 -1052.314 -1052.314], eps: 0.059})
Step:   59900, Reward: [-481.811 -481.811 -481.811] [93.069], Avg: [-538.286 -538.286 -538.286] (0.1000) ({r_i: None, r_t: [-932.221 -932.221 -932.221], eps: 0.1})
Step:   28300, Reward: [-592.652 -592.652 -592.652] [154.583], Avg: [-595.234 -595.234 -595.234] (0.0586) ({r_i: None, r_t: [-1193.042 -1193.042 -1193.042], eps: 0.059})
Step:   60000, Reward: [-457.690 -457.690 -457.690] [70.392], Avg: [-538.152 -538.152 -538.152] (0.1000) ({r_i: None, r_t: [-940.327 -940.327 -940.327], eps: 0.1})
Step:   28400, Reward: [-610.093 -610.093 -610.093] [127.224], Avg: [-595.286 -595.286 -595.286] (0.0580) ({r_i: None, r_t: [-1132.146 -1132.146 -1132.146], eps: 0.058})
Step:   60100, Reward: [-515.526 -515.526 -515.526] [104.196], Avg: [-538.114 -538.114 -538.114] (0.1000) ({r_i: None, r_t: [-908.918 -908.918 -908.918], eps: 0.1})
Step:   28500, Reward: [-541.867 -541.867 -541.867] [106.542], Avg: [-595.100 -595.100 -595.100] (0.0574) ({r_i: None, r_t: [-1163.572 -1163.572 -1163.572], eps: 0.057})
Step:   60200, Reward: [-460.552 -460.552 -460.552] [72.160], Avg: [-537.985 -537.985 -537.985] (0.1000) ({r_i: None, r_t: [-978.866 -978.866 -978.866], eps: 0.1})
Step:   28600, Reward: [-584.249 -584.249 -584.249] [94.545], Avg: [-595.062 -595.062 -595.062] (0.0569) ({r_i: None, r_t: [-1259.828 -1259.828 -1259.828], eps: 0.057})
Step:   60300, Reward: [-504.219 -504.219 -504.219] [100.159], Avg: [-537.930 -537.930 -537.930] (0.1000) ({r_i: None, r_t: [-976.325 -976.325 -976.325], eps: 0.1})
Step:   28700, Reward: [-585.227 -585.227 -585.227] [135.913], Avg: [-595.028 -595.028 -595.028] (0.0563) ({r_i: None, r_t: [-1149.643 -1149.643 -1149.643], eps: 0.056})
Step:   60400, Reward: [-450.483 -450.483 -450.483] [82.589], Avg: [-537.785 -537.785 -537.785] (0.1000) ({r_i: None, r_t: [-929.635 -929.635 -929.635], eps: 0.1})
Step:   28800, Reward: [-602.042 -602.042 -602.042] [135.530], Avg: [-595.052 -595.052 -595.052] (0.0557) ({r_i: None, r_t: [-1177.328 -1177.328 -1177.328], eps: 0.056})
Step:   60500, Reward: [-466.109 -466.109 -466.109] [85.909], Avg: [-537.667 -537.667 -537.667] (0.1000) ({r_i: None, r_t: [-973.646 -973.646 -973.646], eps: 0.1})
Step:   28900, Reward: [-619.047 -619.047 -619.047] [98.021], Avg: [-595.135 -595.135 -595.135] (0.0552) ({r_i: None, r_t: [-1218.982 -1218.982 -1218.982], eps: 0.055})
Step:   60600, Reward: [-446.737 -446.737 -446.737] [83.682], Avg: [-537.517 -537.517 -537.517] (0.1000) ({r_i: None, r_t: [-978.402 -978.402 -978.402], eps: 0.1})
Step:   29000, Reward: [-531.934 -531.934 -531.934] [114.349], Avg: [-594.918 -594.918 -594.918] (0.0546) ({r_i: None, r_t: [-1208.743 -1208.743 -1208.743], eps: 0.055})
Step:   60700, Reward: [-506.590 -506.590 -506.590] [100.978], Avg: [-537.466 -537.466 -537.466] (0.1000) ({r_i: None, r_t: [-946.440 -946.440 -946.440], eps: 0.1})
Step:   29100, Reward: [-529.914 -529.914 -529.914] [104.961], Avg: [-594.695 -594.695 -594.695] (0.0541) ({r_i: None, r_t: [-1130.293 -1130.293 -1130.293], eps: 0.054})
Step:   60800, Reward: [-490.166 -490.166 -490.166] [95.013], Avg: [-537.388 -537.388 -537.388] (0.1000) ({r_i: None, r_t: [-989.473 -989.473 -989.473], eps: 0.1})
Step:   29200, Reward: [-582.392 -582.392 -582.392] [124.665], Avg: [-594.653 -594.653 -594.653] (0.0535) ({r_i: None, r_t: [-1101.832 -1101.832 -1101.832], eps: 0.054})
Step:   60900, Reward: [-482.552 -482.552 -482.552] [79.963], Avg: [-537.299 -537.299 -537.299] (0.1000) ({r_i: None, r_t: [-910.256 -910.256 -910.256], eps: 0.1})
Step:   29300, Reward: [-579.576 -579.576 -579.576] [144.373], Avg: [-594.602 -594.602 -594.602] (0.0530) ({r_i: None, r_t: [-1093.369 -1093.369 -1093.369], eps: 0.053})
Step:   61000, Reward: [-500.168 -500.168 -500.168] [89.780], Avg: [-537.238 -537.238 -537.238] (0.1000) ({r_i: None, r_t: [-919.985 -919.985 -919.985], eps: 0.1})
Step:   29400, Reward: [-555.711 -555.711 -555.711] [114.044], Avg: [-594.470 -594.470 -594.470] (0.0525) ({r_i: None, r_t: [-1114.369 -1114.369 -1114.369], eps: 0.052})
Step:   61100, Reward: [-506.896 -506.896 -506.896] [102.388], Avg: [-537.188 -537.188 -537.188] (0.1000) ({r_i: None, r_t: [-947.766 -947.766 -947.766], eps: 0.1})
Step:   29500, Reward: [-600.845 -600.845 -600.845] [165.443], Avg: [-594.491 -594.491 -594.491] (0.0520) ({r_i: None, r_t: [-1103.880 -1103.880 -1103.880], eps: 0.052})
Step:   61200, Reward: [-453.821 -453.821 -453.821] [59.414], Avg: [-537.052 -537.052 -537.052] (0.1000) ({r_i: None, r_t: [-894.549 -894.549 -894.549], eps: 0.1})
Step:   61300, Reward: [-463.170 -463.170 -463.170] [85.098], Avg: [-536.932 -536.932 -536.932] (0.1000) ({r_i: None, r_t: [-931.046 -931.046 -931.046], eps: 0.1})
Step:   29600, Reward: [-492.860 -492.860 -492.860] [77.994], Avg: [-594.149 -594.149 -594.149] (0.0514) ({r_i: None, r_t: [-1097.305 -1097.305 -1097.305], eps: 0.051})
Step:   61400, Reward: [-470.924 -470.924 -470.924] [73.392], Avg: [-536.825 -536.825 -536.825] (0.1000) ({r_i: None, r_t: [-1023.217 -1023.217 -1023.217], eps: 0.1})
Step:   29700, Reward: [-522.836 -522.836 -522.836] [93.809], Avg: [-593.910 -593.910 -593.910] (0.0509) ({r_i: None, r_t: [-1109.752 -1109.752 -1109.752], eps: 0.051})
Step:   61500, Reward: [-449.051 -449.051 -449.051] [63.858], Avg: [-536.682 -536.682 -536.682] (0.1000) ({r_i: None, r_t: [-995.547 -995.547 -995.547], eps: 0.1})
Step:   29800, Reward: [-525.373 -525.373 -525.373] [79.157], Avg: [-593.681 -593.681 -593.681] (0.0504) ({r_i: None, r_t: [-1078.359 -1078.359 -1078.359], eps: 0.05})
Step:   61600, Reward: [-479.709 -479.709 -479.709] [109.536], Avg: [-536.590 -536.590 -536.590] (0.1000) ({r_i: None, r_t: [-933.640 -933.640 -933.640], eps: 0.1})
Step:   29900, Reward: [-499.203 -499.203 -499.203] [62.507], Avg: [-593.366 -593.366 -593.366] (0.0499) ({r_i: None, r_t: [-983.767 -983.767 -983.767], eps: 0.05})
Step:   61700, Reward: [-487.430 -487.430 -487.430] [77.492], Avg: [-536.510 -536.510 -536.510] (0.1000) ({r_i: None, r_t: [-953.004 -953.004 -953.004], eps: 0.1})
Step:   30000, Reward: [-547.373 -547.373 -547.373] [156.443], Avg: [-593.213 -593.213 -593.213] (0.0494) ({r_i: None, r_t: [-1039.890 -1039.890 -1039.890], eps: 0.049})
Step:   61800, Reward: [-460.506 -460.506 -460.506] [72.467], Avg: [-536.387 -536.387 -536.387] (0.1000) ({r_i: None, r_t: [-946.408 -946.408 -946.408], eps: 0.1})
Step:   30100, Reward: [-530.110 -530.110 -530.110] [94.443], Avg: [-593.004 -593.004 -593.004] (0.0489) ({r_i: None, r_t: [-1054.340 -1054.340 -1054.340], eps: 0.049})
Step:   61900, Reward: [-445.556 -445.556 -445.556] [112.190], Avg: [-536.241 -536.241 -536.241] (0.1000) ({r_i: None, r_t: [-912.646 -912.646 -912.646], eps: 0.1})
Step:   30200, Reward: [-530.134 -530.134 -530.134] [112.742], Avg: [-592.797 -592.797 -592.797] (0.0484) ({r_i: None, r_t: [-1108.167 -1108.167 -1108.167], eps: 0.048})
Step:   62000, Reward: [-479.866 -479.866 -479.866] [86.210], Avg: [-536.150 -536.150 -536.150] (0.1000) ({r_i: None, r_t: [-905.046 -905.046 -905.046], eps: 0.1})
Step:   30300, Reward: [-506.487 -506.487 -506.487] [52.715], Avg: [-592.513 -592.513 -592.513] (0.0479) ({r_i: None, r_t: [-993.906 -993.906 -993.906], eps: 0.048})
Step:   62100, Reward: [-475.731 -475.731 -475.731] [79.748], Avg: [-536.053 -536.053 -536.053] (0.1000) ({r_i: None, r_t: [-952.201 -952.201 -952.201], eps: 0.1})
Step:   30400, Reward: [-563.741 -563.741 -563.741] [123.222], Avg: [-592.418 -592.418 -592.418] (0.0475) ({r_i: None, r_t: [-1046.546 -1046.546 -1046.546], eps: 0.047})
Step:   62200, Reward: [-510.164 -510.164 -510.164] [136.943], Avg: [-536.011 -536.011 -536.011] (0.1000) ({r_i: None, r_t: [-965.675 -965.675 -965.675], eps: 0.1})
Step:   30500, Reward: [-542.312 -542.312 -542.312] [167.236], Avg: [-592.255 -592.255 -592.255] (0.0470) ({r_i: None, r_t: [-1066.987 -1066.987 -1066.987], eps: 0.047})
Step:   62300, Reward: [-513.005 -513.005 -513.005] [113.514], Avg: [-535.974 -535.974 -535.974] (0.1000) ({r_i: None, r_t: [-893.723 -893.723 -893.723], eps: 0.1})
Step:   30600, Reward: [-525.080 -525.080 -525.080] [102.809], Avg: [-592.036 -592.036 -592.036] (0.0465) ({r_i: None, r_t: [-1006.386 -1006.386 -1006.386], eps: 0.047})
Step:   62400, Reward: [-526.670 -526.670 -526.670] [186.258], Avg: [-535.960 -535.960 -535.960] (0.1000) ({r_i: None, r_t: [-950.966 -950.966 -950.966], eps: 0.1})
Step:   30700, Reward: [-550.247 -550.247 -550.247] [106.383], Avg: [-591.900 -591.900 -591.900] (0.0461) ({r_i: None, r_t: [-996.616 -996.616 -996.616], eps: 0.046})
Step:   62500, Reward: [-464.563 -464.563 -464.563] [90.376], Avg: [-535.846 -535.846 -535.846] (0.1000) ({r_i: None, r_t: [-944.579 -944.579 -944.579], eps: 0.1})
Step:   30800, Reward: [-501.050 -501.050 -501.050] [131.779], Avg: [-591.606 -591.606 -591.606] (0.0456) ({r_i: None, r_t: [-1035.956 -1035.956 -1035.956], eps: 0.046})
Step:   62600, Reward: [-478.402 -478.402 -478.402] [67.501], Avg: [-535.754 -535.754 -535.754] (0.1000) ({r_i: None, r_t: [-1014.387 -1014.387 -1014.387], eps: 0.1})
Step:   30900, Reward: [-556.657 -556.657 -556.657] [205.466], Avg: [-591.493 -591.493 -591.493] (0.0452) ({r_i: None, r_t: [-1115.907 -1115.907 -1115.907], eps: 0.045})
Step:   62700, Reward: [-482.432 -482.432 -482.432] [89.155], Avg: [-535.669 -535.669 -535.669] (0.1000) ({r_i: None, r_t: [-991.243 -991.243 -991.243], eps: 0.1})
Step:   31000, Reward: [-622.365 -622.365 -622.365] [204.857], Avg: [-591.593 -591.593 -591.593] (0.0447) ({r_i: None, r_t: [-1154.386 -1154.386 -1154.386], eps: 0.045})
Step:   62800, Reward: [-522.723 -522.723 -522.723] [104.541], Avg: [-535.648 -535.648 -535.648] (0.1000) ({r_i: None, r_t: [-995.957 -995.957 -995.957], eps: 0.1})
Step:   31100, Reward: [-613.164 -613.164 -613.164] [96.751], Avg: [-591.662 -591.662 -591.662] (0.0443) ({r_i: None, r_t: [-1184.800 -1184.800 -1184.800], eps: 0.044})
Step:   62900, Reward: [-482.773 -482.773 -482.773] [60.637], Avg: [-535.565 -535.565 -535.565] (0.1000) ({r_i: None, r_t: [-990.889 -990.889 -990.889], eps: 0.1})
Step:   31200, Reward: [-718.160 -718.160 -718.160] [329.603], Avg: [-592.066 -592.066 -592.066] (0.0438) ({r_i: None, r_t: [-1243.629 -1243.629 -1243.629], eps: 0.044})
Step:   63000, Reward: [-470.103 -470.103 -470.103] [89.777], Avg: [-535.461 -535.461 -535.461] (0.1000) ({r_i: None, r_t: [-964.372 -964.372 -964.372], eps: 0.1})
Step:   31300, Reward: [-680.471 -680.471 -680.471] [150.856], Avg: [-592.347 -592.347 -592.347] (0.0434) ({r_i: None, r_t: [-1153.383 -1153.383 -1153.383], eps: 0.043})
Step:   63100, Reward: [-507.189 -507.189 -507.189] [138.291], Avg: [-535.416 -535.416 -535.416] (0.1000) ({r_i: None, r_t: [-1003.108 -1003.108 -1003.108], eps: 0.1})
Step:   31400, Reward: [-716.776 -716.776 -716.776] [328.024], Avg: [-592.742 -592.742 -592.742] (0.0429) ({r_i: None, r_t: [-1261.474 -1261.474 -1261.474], eps: 0.043})
Step:   63200, Reward: [-478.169 -478.169 -478.169] [102.705], Avg: [-535.326 -535.326 -535.326] (0.1000) ({r_i: None, r_t: [-1068.503 -1068.503 -1068.503], eps: 0.1})
Step:   31500, Reward: [-789.229 -789.229 -789.229] [236.876], Avg: [-593.364 -593.364 -593.364] (0.0425) ({r_i: None, r_t: [-1359.569 -1359.569 -1359.569], eps: 0.043})
Step:   63300, Reward: [-462.398 -462.398 -462.398] [106.977], Avg: [-535.211 -535.211 -535.211] (0.1000) ({r_i: None, r_t: [-953.323 -953.323 -953.323], eps: 0.1})
Step:   31600, Reward: [-803.835 -803.835 -803.835] [272.028], Avg: [-594.028 -594.028 -594.028] (0.0421) ({r_i: None, r_t: [-1380.613 -1380.613 -1380.613], eps: 0.042})
Step:   63400, Reward: [-474.917 -474.917 -474.917] [54.461], Avg: [-535.116 -535.116 -535.116] (0.1000) ({r_i: None, r_t: [-1073.463 -1073.463 -1073.463], eps: 0.1})
Step:   31700, Reward: [-865.492 -865.492 -865.492] [240.763], Avg: [-594.882 -594.882 -594.882] (0.0417) ({r_i: None, r_t: [-1580.167 -1580.167 -1580.167], eps: 0.042})
Step:   63500, Reward: [-469.693 -469.693 -469.693] [59.177], Avg: [-535.013 -535.013 -535.013] (0.1000) ({r_i: None, r_t: [-945.765 -945.765 -945.765], eps: 0.1})
Step:   31800, Reward: [-935.084 -935.084 -935.084] [251.882], Avg: [-595.948 -595.948 -595.948] (0.0413) ({r_i: None, r_t: [-1689.051 -1689.051 -1689.051], eps: 0.041})
Step:   63600, Reward: [-477.470 -477.470 -477.470] [151.535], Avg: [-534.922 -534.922 -534.922] (0.1000) ({r_i: None, r_t: [-1023.140 -1023.140 -1023.140], eps: 0.1})
Step:   31900, Reward: [-1128.231 -1128.231 -1128.231] [428.220], Avg: [-597.612 -597.612 -597.612] (0.0408) ({r_i: None, r_t: [-1997.690 -1997.690 -1997.690], eps: 0.041})
Step:   63700, Reward: [-489.320 -489.320 -489.320] [123.803], Avg: [-534.851 -534.851 -534.851] (0.1000) ({r_i: None, r_t: [-999.181 -999.181 -999.181], eps: 0.1})
Step:   32000, Reward: [-1047.662 -1047.662 -1047.662] [390.070], Avg: [-599.014 -599.014 -599.014] (0.0404) ({r_i: None, r_t: [-1864.785 -1864.785 -1864.785], eps: 0.04})
Step:   63800, Reward: [-470.150 -470.150 -470.150] [74.999], Avg: [-534.750 -534.750 -534.750] (0.1000) ({r_i: None, r_t: [-1009.857 -1009.857 -1009.857], eps: 0.1})
Step:   32100, Reward: [-1178.212 -1178.212 -1178.212] [351.949], Avg: [-600.812 -600.812 -600.812] (0.0400) ({r_i: None, r_t: [-2058.279 -2058.279 -2058.279], eps: 0.04})
Step:   63900, Reward: [-500.371 -500.371 -500.371] [106.598], Avg: [-534.696 -534.696 -534.696] (0.1000) ({r_i: None, r_t: [-986.369 -986.369 -986.369], eps: 0.1})
Step:   32200, Reward: [-1201.821 -1201.821 -1201.821] [326.025], Avg: [-602.673 -602.673 -602.673] (0.0396) ({r_i: None, r_t: [-2181.846 -2181.846 -2181.846], eps: 0.04})
Step:   64000, Reward: [-505.116 -505.116 -505.116] [135.954], Avg: [-534.650 -534.650 -534.650] (0.1000) ({r_i: None, r_t: [-983.016 -983.016 -983.016], eps: 0.1})
Step:   32300, Reward: [-1423.145 -1423.145 -1423.145] [363.271], Avg: [-605.205 -605.205 -605.205] (0.0392) ({r_i: None, r_t: [-2288.701 -2288.701 -2288.701], eps: 0.039})
Step:   64100, Reward: [-469.475 -469.475 -469.475] [77.994], Avg: [-534.548 -534.548 -534.548] (0.1000) ({r_i: None, r_t: [-959.917 -959.917 -959.917], eps: 0.1})
Step:   64200, Reward: [-501.379 -501.379 -501.379] [121.175], Avg: [-534.497 -534.497 -534.497] (0.1000) ({r_i: None, r_t: [-933.556 -933.556 -933.556], eps: 0.1})
Step:   32400, Reward: [-1201.367 -1201.367 -1201.367] [381.383], Avg: [-607.040 -607.040 -607.040] (0.0388) ({r_i: None, r_t: [-2651.356 -2651.356 -2651.356], eps: 0.039})
Step:   64300, Reward: [-498.896 -498.896 -498.896] [115.479], Avg: [-534.441 -534.441 -534.441] (0.1000) ({r_i: None, r_t: [-1004.475 -1004.475 -1004.475], eps: 0.1})
Step:   32500, Reward: [-1661.623 -1661.623 -1661.623] [348.383], Avg: [-610.275 -610.275 -610.275] (0.0385) ({r_i: None, r_t: [-2467.223 -2467.223 -2467.223], eps: 0.038})
Step:   64400, Reward: [-459.824 -459.824 -459.824] [81.492], Avg: [-534.326 -534.326 -534.326] (0.1000) ({r_i: None, r_t: [-992.720 -992.720 -992.720], eps: 0.1})
Step:   32600, Reward: [-1508.012 -1508.012 -1508.012] [330.142], Avg: [-613.020 -613.020 -613.020] (0.0381) ({r_i: None, r_t: [-2589.609 -2589.609 -2589.609], eps: 0.038})
Step:   64500, Reward: [-524.532 -524.532 -524.532] [100.754], Avg: [-534.311 -534.311 -534.311] (0.1000) ({r_i: None, r_t: [-984.893 -984.893 -984.893], eps: 0.1})
Step:   32700, Reward: [-1490.076 -1490.076 -1490.076] [318.209], Avg: [-615.694 -615.694 -615.694] (0.0377) ({r_i: None, r_t: [-3013.306 -3013.306 -3013.306], eps: 0.038})
Step:   64600, Reward: [-453.245 -453.245 -453.245] [62.809], Avg: [-534.185 -534.185 -534.185] (0.1000) ({r_i: None, r_t: [-1059.024 -1059.024 -1059.024], eps: 0.1})
Step:   32800, Reward: [-1661.543 -1661.543 -1661.543] [311.454], Avg: [-618.873 -618.873 -618.873] (0.0373) ({r_i: None, r_t: [-2969.180 -2969.180 -2969.180], eps: 0.037})
Step:   64700, Reward: [-501.403 -501.403 -501.403] [110.703], Avg: [-534.135 -534.135 -534.135] (0.1000) ({r_i: None, r_t: [-1069.963 -1069.963 -1069.963], eps: 0.1})
Step:   32900, Reward: [-1638.039 -1638.039 -1638.039] [333.113], Avg: [-621.961 -621.961 -621.961] (0.0369) ({r_i: None, r_t: [-3072.982 -3072.982 -3072.982], eps: 0.037})
Step:   64800, Reward: [-501.362 -501.362 -501.362] [74.310], Avg: [-534.084 -534.084 -534.084] (0.1000) ({r_i: None, r_t: [-977.397 -977.397 -977.397], eps: 0.1})
Step:   33000, Reward: [-1583.747 -1583.747 -1583.747] [293.850], Avg: [-624.867 -624.867 -624.867] (0.0366) ({r_i: None, r_t: [-3322.435 -3322.435 -3322.435], eps: 0.037})
Step:   64900, Reward: [-480.179 -480.179 -480.179] [70.468], Avg: [-534.001 -534.001 -534.001] (0.1000) ({r_i: None, r_t: [-962.835 -962.835 -962.835], eps: 0.1})
Step:   33100, Reward: [-1696.918 -1696.918 -1696.918] [356.392], Avg: [-628.096 -628.096 -628.096] (0.0362) ({r_i: None, r_t: [-3326.571 -3326.571 -3326.571], eps: 0.036})
Step:   65000, Reward: [-474.106 -474.106 -474.106] [74.914], Avg: [-533.909 -533.909 -533.909] (0.1000) ({r_i: None, r_t: [-965.282 -965.282 -965.282], eps: 0.1})
Step:   33200, Reward: [-1713.477 -1713.477 -1713.477] [349.354], Avg: [-631.355 -631.355 -631.355] (0.0359) ({r_i: None, r_t: [-3327.939 -3327.939 -3327.939], eps: 0.036})
Step:   65100, Reward: [-500.292 -500.292 -500.292] [71.452], Avg: [-533.858 -533.858 -533.858] (0.1000) ({r_i: None, r_t: [-997.429 -997.429 -997.429], eps: 0.1})
Step:   33300, Reward: [-1737.741 -1737.741 -1737.741] [341.922], Avg: [-634.668 -634.668 -634.668] (0.0355) ({r_i: None, r_t: [-3382.142 -3382.142 -3382.142], eps: 0.035})
Step:   65200, Reward: [-500.500 -500.500 -500.500] [125.997], Avg: [-533.807 -533.807 -533.807] (0.1000) ({r_i: None, r_t: [-1020.202 -1020.202 -1020.202], eps: 0.1})
Step:   33400, Reward: [-1899.010 -1899.010 -1899.010] [304.247], Avg: [-638.442 -638.442 -638.442] (0.0351) ({r_i: None, r_t: [-3582.257 -3582.257 -3582.257], eps: 0.035})
Step:   65300, Reward: [-507.685 -507.685 -507.685] [98.963], Avg: [-533.767 -533.767 -533.767] (0.1000) ({r_i: None, r_t: [-999.880 -999.880 -999.880], eps: 0.1})
Step:   33500, Reward: [-1829.945 -1829.945 -1829.945] [255.464], Avg: [-641.988 -641.988 -641.988] (0.0348) ({r_i: None, r_t: [-3485.627 -3485.627 -3485.627], eps: 0.035})
Step:   65400, Reward: [-487.370 -487.370 -487.370] [99.721], Avg: [-533.696 -533.696 -533.696] (0.1000) ({r_i: None, r_t: [-1000.909 -1000.909 -1000.909], eps: 0.1})
Step:   33600, Reward: [-1895.587 -1895.587 -1895.587] [228.138], Avg: [-645.708 -645.708 -645.708] (0.0344) ({r_i: None, r_t: [-3797.085 -3797.085 -3797.085], eps: 0.034})
Step:   65500, Reward: [-472.136 -472.136 -472.136] [81.346], Avg: [-533.602 -533.602 -533.602] (0.1000) ({r_i: None, r_t: [-992.298 -992.298 -992.298], eps: 0.1})
Step:   33700, Reward: [-1851.617 -1851.617 -1851.617] [198.233], Avg: [-649.276 -649.276 -649.276] (0.0341) ({r_i: None, r_t: [-3504.177 -3504.177 -3504.177], eps: 0.034})
Step:   65600, Reward: [-518.940 -518.940 -518.940] [127.972], Avg: [-533.580 -533.580 -533.580] (0.1000) ({r_i: None, r_t: [-963.456 -963.456 -963.456], eps: 0.1})
Step:   33800, Reward: [-2041.831 -2041.831 -2041.831] [360.676], Avg: [-653.384 -653.384 -653.384] (0.0338) ({r_i: None, r_t: [-3820.181 -3820.181 -3820.181], eps: 0.034})
Step:   65700, Reward: [-510.274 -510.274 -510.274] [103.855], Avg: [-533.544 -533.544 -533.544] (0.1000) ({r_i: None, r_t: [-987.265 -987.265 -987.265], eps: 0.1})
Step:   33900, Reward: [-2003.322 -2003.322 -2003.322] [247.145], Avg: [-657.354 -657.354 -657.354] (0.0334) ({r_i: None, r_t: [-3869.076 -3869.076 -3869.076], eps: 0.033})
Step:   65800, Reward: [-473.107 -473.107 -473.107] [85.615], Avg: [-533.453 -533.453 -533.453] (0.1000) ({r_i: None, r_t: [-1032.501 -1032.501 -1032.501], eps: 0.1})
Step:   34000, Reward: [-2009.690 -2009.690 -2009.690] [275.039], Avg: [-661.320 -661.320 -661.320] (0.0331) ({r_i: None, r_t: [-3773.279 -3773.279 -3773.279], eps: 0.033})
Step:   65900, Reward: [-528.154 -528.154 -528.154] [126.577], Avg: [-533.445 -533.445 -533.445] (0.1000) ({r_i: None, r_t: [-1010.516 -1010.516 -1010.516], eps: 0.1})
Step:   34100, Reward: [-1866.658 -1866.658 -1866.658] [250.070], Avg: [-664.844 -664.844 -664.844] (0.0328) ({r_i: None, r_t: [-3844.505 -3844.505 -3844.505], eps: 0.033})
Step:   66000, Reward: [-499.543 -499.543 -499.543] [60.870], Avg: [-533.393 -533.393 -533.393] (0.1000) ({r_i: None, r_t: [-1015.399 -1015.399 -1015.399], eps: 0.1})
Step:   34200, Reward: [-1928.364 -1928.364 -1928.364] [168.111], Avg: [-668.528 -668.528 -668.528] (0.0324) ({r_i: None, r_t: [-3835.921 -3835.921 -3835.921], eps: 0.032})
Step:   66100, Reward: [-467.972 -467.972 -467.972] [97.921], Avg: [-533.294 -533.294 -533.294] (0.1000) ({r_i: None, r_t: [-988.925 -988.925 -988.925], eps: 0.1})
Step:   34300, Reward: [-1889.502 -1889.502 -1889.502] [181.733], Avg: [-672.077 -672.077 -672.077] (0.0321) ({r_i: None, r_t: [-3891.589 -3891.589 -3891.589], eps: 0.032})
Step:   66200, Reward: [-491.924 -491.924 -491.924] [75.002], Avg: [-533.232 -533.232 -533.232] (0.1000) ({r_i: None, r_t: [-1011.148 -1011.148 -1011.148], eps: 0.1})
Step:   34400, Reward: [-1942.492 -1942.492 -1942.492] [198.868], Avg: [-675.760 -675.760 -675.760] (0.0318) ({r_i: None, r_t: [-3724.658 -3724.658 -3724.658], eps: 0.032})
Step:   66300, Reward: [-492.125 -492.125 -492.125] [74.847], Avg: [-533.170 -533.170 -533.170] (0.1000) ({r_i: None, r_t: [-1019.706 -1019.706 -1019.706], eps: 0.1})
Step:   34500, Reward: [-1938.875 -1938.875 -1938.875] [264.270], Avg: [-679.410 -679.410 -679.410] (0.0315) ({r_i: None, r_t: [-3813.857 -3813.857 -3813.857], eps: 0.031})
Step:   66400, Reward: [-545.571 -545.571 -545.571] [92.956], Avg: [-533.189 -533.189 -533.189] (0.1000) ({r_i: None, r_t: [-1033.620 -1033.620 -1033.620], eps: 0.1})
Step:   34600, Reward: [-1740.744 -1740.744 -1740.744] [209.537], Avg: [-682.469 -682.469 -682.469] (0.0312) ({r_i: None, r_t: [-3798.533 -3798.533 -3798.533], eps: 0.031})
Step:   66500, Reward: [-580.194 -580.194 -580.194] [141.235], Avg: [-533.259 -533.259 -533.259] (0.1000) ({r_i: None, r_t: [-1012.056 -1012.056 -1012.056], eps: 0.1})
Step:   34700, Reward: [-1877.680 -1877.680 -1877.680] [282.731], Avg: [-685.904 -685.904 -685.904] (0.0308) ({r_i: None, r_t: [-3677.061 -3677.061 -3677.061], eps: 0.031})
Step:   66600, Reward: [-510.942 -510.942 -510.942] [98.370], Avg: [-533.226 -533.226 -533.226] (0.1000) ({r_i: None, r_t: [-988.347 -988.347 -988.347], eps: 0.1})
Step:   34800, Reward: [-1917.143 -1917.143 -1917.143] [242.884], Avg: [-689.431 -689.431 -689.431] (0.0305) ({r_i: None, r_t: [-3796.295 -3796.295 -3796.295], eps: 0.031})
Step:   66700, Reward: [-507.468 -507.468 -507.468] [114.618], Avg: [-533.187 -533.187 -533.187] (0.1000) ({r_i: None, r_t: [-1046.735 -1046.735 -1046.735], eps: 0.1})
Step:   34900, Reward: [-1929.385 -1929.385 -1929.385] [327.798], Avg: [-692.974 -692.974 -692.974] (0.0302) ({r_i: None, r_t: [-3793.985 -3793.985 -3793.985], eps: 0.03})
Step:   66800, Reward: [-480.287 -480.287 -480.287] [98.059], Avg: [-533.108 -533.108 -533.108] (0.1000) ({r_i: None, r_t: [-1026.718 -1026.718 -1026.718], eps: 0.1})
Step:   35000, Reward: [-1838.508 -1838.508 -1838.508] [197.262], Avg: [-696.238 -696.238 -696.238] (0.0299) ({r_i: None, r_t: [-3610.189 -3610.189 -3610.189], eps: 0.03})
Step:   66900, Reward: [-489.555 -489.555 -489.555] [68.035], Avg: [-533.043 -533.043 -533.043] (0.1000) ({r_i: None, r_t: [-969.392 -969.392 -969.392], eps: 0.1})
Step:   35100, Reward: [-1969.757 -1969.757 -1969.757] [167.469], Avg: [-699.856 -699.856 -699.856] (0.0296) ({r_i: None, r_t: [-3675.751 -3675.751 -3675.751], eps: 0.03})
Step:   67000, Reward: [-486.750 -486.750 -486.750] [99.772], Avg: [-532.974 -532.974 -532.974] (0.1000) ({r_i: None, r_t: [-974.592 -974.592 -974.592], eps: 0.1})
Step:   35200, Reward: [-1877.488 -1877.488 -1877.488] [285.241], Avg: [-703.192 -703.192 -703.192] (0.0293) ({r_i: None, r_t: [-3624.645 -3624.645 -3624.645], eps: 0.029})
Step:   67100, Reward: [-490.327 -490.327 -490.327] [92.765], Avg: [-532.911 -532.911 -532.911] (0.1000) ({r_i: None, r_t: [-1012.572 -1012.572 -1012.572], eps: 0.1})
Step:   67200, Reward: [-505.635 -505.635 -505.635] [76.187], Avg: [-532.870 -532.870 -532.870] (0.1000) ({r_i: None, r_t: [-1062.767 -1062.767 -1062.767], eps: 0.1})
Step:   35300, Reward: [-1845.615 -1845.615 -1845.615] [201.395], Avg: [-706.419 -706.419 -706.419] (0.0290) ({r_i: None, r_t: [-3659.050 -3659.050 -3659.050], eps: 0.029})
Step:   67300, Reward: [-524.047 -524.047 -524.047] [138.523], Avg: [-532.857 -532.857 -532.857] (0.1000) ({r_i: None, r_t: [-1020.440 -1020.440 -1020.440], eps: 0.1})
Step:   35400, Reward: [-1718.684 -1718.684 -1718.684] [246.002], Avg: [-709.270 -709.270 -709.270] (0.0288) ({r_i: None, r_t: [-3732.796 -3732.796 -3732.796], eps: 0.029})
Step:   67400, Reward: [-471.847 -471.847 -471.847] [71.043], Avg: [-532.767 -532.767 -532.767] (0.1000) ({r_i: None, r_t: [-978.510 -978.510 -978.510], eps: 0.1})
Step:   35500, Reward: [-1782.701 -1782.701 -1782.701] [206.691], Avg: [-712.286 -712.286 -712.286] (0.0285) ({r_i: None, r_t: [-3606.356 -3606.356 -3606.356], eps: 0.028})
Step:   67500, Reward: [-497.339 -497.339 -497.339] [105.180], Avg: [-532.714 -532.714 -532.714] (0.1000) ({r_i: None, r_t: [-966.701 -966.701 -966.701], eps: 0.1})
Step:   35600, Reward: [-1794.534 -1794.534 -1794.534] [297.188], Avg: [-715.317 -715.317 -715.317] (0.0282) ({r_i: None, r_t: [-3562.708 -3562.708 -3562.708], eps: 0.028})
Step:   67600, Reward: [-478.195 -478.195 -478.195] [86.412], Avg: [-532.634 -532.634 -532.634] (0.1000) ({r_i: None, r_t: [-970.038 -970.038 -970.038], eps: 0.1})
Step:   35700, Reward: [-1591.780 -1591.780 -1591.780] [277.898], Avg: [-717.765 -717.765 -717.765] (0.0279) ({r_i: None, r_t: [-3300.656 -3300.656 -3300.656], eps: 0.028})
Step:   67700, Reward: [-538.210 -538.210 -538.210] [108.802], Avg: [-532.642 -532.642 -532.642] (0.1000) ({r_i: None, r_t: [-1042.643 -1042.643 -1042.643], eps: 0.1})
Step:   35800, Reward: [-1705.298 -1705.298 -1705.298] [183.799], Avg: [-720.516 -720.516 -720.516] (0.0276) ({r_i: None, r_t: [-3470.831 -3470.831 -3470.831], eps: 0.028})
Step:   67800, Reward: [-482.931 -482.931 -482.931] [90.664], Avg: [-532.569 -532.569 -532.569] (0.1000) ({r_i: None, r_t: [-1113.862 -1113.862 -1113.862], eps: 0.1})
Step:   35900, Reward: [-1624.168 -1624.168 -1624.168] [167.432], Avg: [-723.026 -723.026 -723.026] (0.0274) ({r_i: None, r_t: [-3332.839 -3332.839 -3332.839], eps: 0.027})
Step:   67900, Reward: [-501.644 -501.644 -501.644] [72.909], Avg: [-532.523 -532.523 -532.523] (0.1000) ({r_i: None, r_t: [-957.686 -957.686 -957.686], eps: 0.1})
Step:   36000, Reward: [-1602.144 -1602.144 -1602.144] [339.749], Avg: [-725.462 -725.462 -725.462] (0.0271) ({r_i: None, r_t: [-3385.151 -3385.151 -3385.151], eps: 0.027})
Step:   68000, Reward: [-493.341 -493.341 -493.341] [79.582], Avg: [-532.466 -532.466 -532.466] (0.1000) ({r_i: None, r_t: [-1061.508 -1061.508 -1061.508], eps: 0.1})
Step:   36100, Reward: [-1602.275 -1602.275 -1602.275] [173.226], Avg: [-727.884 -727.884 -727.884] (0.0268) ({r_i: None, r_t: [-3198.773 -3198.773 -3198.773], eps: 0.027})
Step:   68100, Reward: [-562.419 -562.419 -562.419] [120.639], Avg: [-532.510 -532.510 -532.510] (0.1000) ({r_i: None, r_t: [-1045.703 -1045.703 -1045.703], eps: 0.1})
Step:   36200, Reward: [-1591.956 -1591.956 -1591.956] [263.566], Avg: [-730.264 -730.264 -730.264] (0.0265) ({r_i: None, r_t: [-3165.298 -3165.298 -3165.298], eps: 0.027})
Step:   68200, Reward: [-545.536 -545.536 -545.536] [172.934], Avg: [-532.529 -532.529 -532.529] (0.1000) ({r_i: None, r_t: [-895.540 -895.540 -895.540], eps: 0.1})
Step:   36300, Reward: [-1590.366 -1590.366 -1590.366] [280.162], Avg: [-732.627 -732.627 -732.627] (0.0263) ({r_i: None, r_t: [-3143.843 -3143.843 -3143.843], eps: 0.026})
Step:   68300, Reward: [-489.253 -489.253 -489.253] [88.959], Avg: [-532.466 -532.466 -532.466] (0.1000) ({r_i: None, r_t: [-1014.449 -1014.449 -1014.449], eps: 0.1})
Step:   36400, Reward: [-1467.076 -1467.076 -1467.076] [216.919], Avg: [-734.639 -734.639 -734.639] (0.0260) ({r_i: None, r_t: [-3009.392 -3009.392 -3009.392], eps: 0.026})
Step:   68400, Reward: [-483.205 -483.205 -483.205] [105.814], Avg: [-532.394 -532.394 -532.394] (0.1000) ({r_i: None, r_t: [-980.537 -980.537 -980.537], eps: 0.1})
Step:   36500, Reward: [-1410.292 -1410.292 -1410.292] [246.292], Avg: [-736.485 -736.485 -736.485] (0.0258) ({r_i: None, r_t: [-3044.120 -3044.120 -3044.120], eps: 0.026})
Step:   68500, Reward: [-477.961 -477.961 -477.961] [86.617], Avg: [-532.314 -532.314 -532.314] (0.1000) ({r_i: None, r_t: [-1044.134 -1044.134 -1044.134], eps: 0.1})
Step:   36600, Reward: [-1431.291 -1431.291 -1431.291] [220.688], Avg: [-738.378 -738.378 -738.378] (0.0255) ({r_i: None, r_t: [-2960.076 -2960.076 -2960.076], eps: 0.025})
Step:   68600, Reward: [-502.275 -502.275 -502.275] [91.540], Avg: [-532.271 -532.271 -532.271] (0.1000) ({r_i: None, r_t: [-1014.625 -1014.625 -1014.625], eps: 0.1})
Step:   36700, Reward: [-1522.178 -1522.178 -1522.178] [269.686], Avg: [-740.508 -740.508 -740.508] (0.0252) ({r_i: None, r_t: [-2643.649 -2643.649 -2643.649], eps: 0.025})
Step:   68700, Reward: [-494.546 -494.546 -494.546] [70.997], Avg: [-532.216 -532.216 -532.216] (0.1000) ({r_i: None, r_t: [-1011.826 -1011.826 -1011.826], eps: 0.1})
Step:   36800, Reward: [-1370.881 -1370.881 -1370.881] [278.975], Avg: [-742.217 -742.217 -742.217] (0.0250) ({r_i: None, r_t: [-2817.156 -2817.156 -2817.156], eps: 0.025})
Step:   68800, Reward: [-500.946 -500.946 -500.946] [101.832], Avg: [-532.170 -532.170 -532.170] (0.1000) ({r_i: None, r_t: [-988.412 -988.412 -988.412], eps: 0.1})
Step:   36900, Reward: [-1405.712 -1405.712 -1405.712] [217.987], Avg: [-744.010 -744.010 -744.010] (0.0247) ({r_i: None, r_t: [-2715.748 -2715.748 -2715.748], eps: 0.025})
Step:   68900, Reward: [-568.162 -568.162 -568.162] [118.634], Avg: [-532.223 -532.223 -532.223] (0.1000) ({r_i: None, r_t: [-944.662 -944.662 -944.662], eps: 0.1})
Step:   37000, Reward: [-1289.160 -1289.160 -1289.160] [236.982], Avg: [-745.479 -745.479 -745.479] (0.0245) ({r_i: None, r_t: [-2794.362 -2794.362 -2794.362], eps: 0.024})
Step:   69000, Reward: [-526.685 -526.685 -526.685] [126.318], Avg: [-532.215 -532.215 -532.215] (0.1000) ({r_i: None, r_t: [-1060.312 -1060.312 -1060.312], eps: 0.1})
Step:   37100, Reward: [-1362.312 -1362.312 -1362.312] [126.333], Avg: [-747.137 -747.137 -747.137] (0.0243) ({r_i: None, r_t: [-2720.241 -2720.241 -2720.241], eps: 0.024})
Step:   69100, Reward: [-570.600 -570.600 -570.600] [175.840], Avg: [-532.270 -532.270 -532.270] (0.1000) ({r_i: None, r_t: [-970.620 -970.620 -970.620], eps: 0.1})
Step:   37200, Reward: [-1362.567 -1362.567 -1362.567] [212.300], Avg: [-748.787 -748.787 -748.787] (0.0240) ({r_i: None, r_t: [-2585.445 -2585.445 -2585.445], eps: 0.024})
Step:   69200, Reward: [-498.371 -498.371 -498.371] [99.246], Avg: [-532.221 -532.221 -532.221] (0.1000) ({r_i: None, r_t: [-1017.100 -1017.100 -1017.100], eps: 0.1})
Step:   37300, Reward: [-1347.059 -1347.059 -1347.059] [174.372], Avg: [-750.387 -750.387 -750.387] (0.0238) ({r_i: None, r_t: [-2770.776 -2770.776 -2770.776], eps: 0.024})
Step:   69300, Reward: [-521.026 -521.026 -521.026] [149.489], Avg: [-532.205 -532.205 -532.205] (0.1000) ({r_i: None, r_t: [-1028.275 -1028.275 -1028.275], eps: 0.1})
Step:   37400, Reward: [-1371.975 -1371.975 -1371.975] [193.978], Avg: [-752.045 -752.045 -752.045] (0.0235) ({r_i: None, r_t: [-2690.319 -2690.319 -2690.319], eps: 0.024})
Step:   69400, Reward: [-547.943 -547.943 -547.943] [121.381], Avg: [-532.228 -532.228 -532.228] (0.1000) ({r_i: None, r_t: [-1016.258 -1016.258 -1016.258], eps: 0.1})
Step:   37500, Reward: [-1346.273 -1346.273 -1346.273] [217.007], Avg: [-753.625 -753.625 -753.625] (0.0233) ({r_i: None, r_t: [-2642.980 -2642.980 -2642.980], eps: 0.023})
Step:   69500, Reward: [-501.487 -501.487 -501.487] [98.519], Avg: [-532.183 -532.183 -532.183] (0.1000) ({r_i: None, r_t: [-1004.037 -1004.037 -1004.037], eps: 0.1})
Step:   37600, Reward: [-1450.679 -1450.679 -1450.679] [287.829], Avg: [-755.474 -755.474 -755.474] (0.0231) ({r_i: None, r_t: [-2712.942 -2712.942 -2712.942], eps: 0.023})
Step:   69600, Reward: [-495.008 -495.008 -495.008] [131.545], Avg: [-532.130 -532.130 -532.130] (0.1000) ({r_i: None, r_t: [-1057.616 -1057.616 -1057.616], eps: 0.1})
Step:   37700, Reward: [-1383.157 -1383.157 -1383.157] [179.162], Avg: [-757.135 -757.135 -757.135] (0.0228) ({r_i: None, r_t: [-2785.393 -2785.393 -2785.393], eps: 0.023})
Step:   69700, Reward: [-519.630 -519.630 -519.630] [122.907], Avg: [-532.112 -532.112 -532.112] (0.1000) ({r_i: None, r_t: [-959.622 -959.622 -959.622], eps: 0.1})
Step:   37800, Reward: [-1435.408 -1435.408 -1435.408] [253.038], Avg: [-758.924 -758.924 -758.924] (0.0226) ({r_i: None, r_t: [-2905.289 -2905.289 -2905.289], eps: 0.023})
Step:   69800, Reward: [-468.891 -468.891 -468.891] [90.415], Avg: [-532.022 -532.022 -532.022] (0.1000) ({r_i: None, r_t: [-1092.916 -1092.916 -1092.916], eps: 0.1})
Step:   69900, Reward: [-466.891 -466.891 -466.891] [85.408], Avg: [-531.929 -531.929 -531.929] (0.1000) ({r_i: None, r_t: [-1025.460 -1025.460 -1025.460], eps: 0.1})
Step:   37900, Reward: [-1321.016 -1321.016 -1321.016] [207.821], Avg: [-760.403 -760.403 -760.403] (0.0224) ({r_i: None, r_t: [-2956.806 -2956.806 -2956.806], eps: 0.022})
Step:   70000, Reward: [-512.029 -512.029 -512.029] [114.971], Avg: [-531.900 -531.900 -531.900] (0.1000) ({r_i: None, r_t: [-1005.879 -1005.879 -1005.879], eps: 0.1})
Step:   38000, Reward: [-1508.295 -1508.295 -1508.295] [261.491], Avg: [-762.366 -762.366 -762.366] (0.0222) ({r_i: None, r_t: [-3203.886 -3203.886 -3203.886], eps: 0.022})
Step:   70100, Reward: [-505.776 -505.776 -505.776] [102.799], Avg: [-531.863 -531.863 -531.863] (0.1000) ({r_i: None, r_t: [-1043.741 -1043.741 -1043.741], eps: 0.1})
Step:   38100, Reward: [-1499.291 -1499.291 -1499.291] [249.806], Avg: [-764.295 -764.295 -764.295] (0.0219) ({r_i: None, r_t: [-3012.730 -3012.730 -3012.730], eps: 0.022})
Step:   70200, Reward: [-527.060 -527.060 -527.060] [80.416], Avg: [-531.856 -531.856 -531.856] (0.1000) ({r_i: None, r_t: [-1023.047 -1023.047 -1023.047], eps: 0.1})
Step:   38200, Reward: [-1557.543 -1557.543 -1557.543] [260.523], Avg: [-766.367 -766.367 -766.367] (0.0217) ({r_i: None, r_t: [-3200.565 -3200.565 -3200.565], eps: 0.022})
Step:   70300, Reward: [-524.653 -524.653 -524.653] [116.445], Avg: [-531.846 -531.846 -531.846] (0.1000) ({r_i: None, r_t: [-949.787 -949.787 -949.787], eps: 0.1})
Step:   38300, Reward: [-1613.345 -1613.345 -1613.345] [333.135], Avg: [-768.572 -768.572 -768.572] (0.0215) ({r_i: None, r_t: [-3048.640 -3048.640 -3048.640], eps: 0.022})
Step:   70400, Reward: [-476.214 -476.214 -476.214] [53.420], Avg: [-531.767 -531.767 -531.767] (0.1000) ({r_i: None, r_t: [-1017.546 -1017.546 -1017.546], eps: 0.1})
Step:   38400, Reward: [-1622.332 -1622.332 -1622.332] [154.688], Avg: [-770.790 -770.790 -770.790] (0.0213) ({r_i: None, r_t: [-3358.755 -3358.755 -3358.755], eps: 0.021})
Step:   70500, Reward: [-498.811 -498.811 -498.811] [88.266], Avg: [-531.720 -531.720 -531.720] (0.1000) ({r_i: None, r_t: [-930.837 -930.837 -930.837], eps: 0.1})
Step:   38500, Reward: [-1642.079 -1642.079 -1642.079] [205.395], Avg: [-773.047 -773.047 -773.047] (0.0211) ({r_i: None, r_t: [-3147.916 -3147.916 -3147.916], eps: 0.021})
Step:   70600, Reward: [-525.826 -525.826 -525.826] [83.496], Avg: [-531.712 -531.712 -531.712] (0.1000) ({r_i: None, r_t: [-924.135 -924.135 -924.135], eps: 0.1})
Step:   38600, Reward: [-1644.729 -1644.729 -1644.729] [266.140], Avg: [-775.299 -775.299 -775.299] (0.0209) ({r_i: None, r_t: [-3428.318 -3428.318 -3428.318], eps: 0.021})
Step:   70700, Reward: [-469.377 -469.377 -469.377] [107.665], Avg: [-531.624 -531.624 -531.624] (0.1000) ({r_i: None, r_t: [-996.708 -996.708 -996.708], eps: 0.1})
Step:   38700, Reward: [-1844.064 -1844.064 -1844.064] [233.184], Avg: [-778.054 -778.054 -778.054] (0.0207) ({r_i: None, r_t: [-3448.033 -3448.033 -3448.033], eps: 0.021})
Step:   70800, Reward: [-489.185 -489.185 -489.185] [95.919], Avg: [-531.564 -531.564 -531.564] (0.1000) ({r_i: None, r_t: [-1085.574 -1085.574 -1085.574], eps: 0.1})
Step:   38800, Reward: [-1800.235 -1800.235 -1800.235] [251.435], Avg: [-780.682 -780.682 -780.682] (0.0205) ({r_i: None, r_t: [-3467.876 -3467.876 -3467.876], eps: 0.02})
Step:   70900, Reward: [-520.016 -520.016 -520.016] [93.260], Avg: [-531.548 -531.548 -531.548] (0.1000) ({r_i: None, r_t: [-1062.122 -1062.122 -1062.122], eps: 0.1})
Step:   38900, Reward: [-1706.435 -1706.435 -1706.435] [195.242], Avg: [-783.055 -783.055 -783.055] (0.0202) ({r_i: None, r_t: [-3326.733 -3326.733 -3326.733], eps: 0.02})
Step:   71000, Reward: [-517.420 -517.420 -517.420] [123.945], Avg: [-531.528 -531.528 -531.528] (0.1000) ({r_i: None, r_t: [-972.597 -972.597 -972.597], eps: 0.1})
Step:   39000, Reward: [-1775.232 -1775.232 -1775.232] [231.266], Avg: [-785.593 -785.593 -785.593] (0.0200) ({r_i: None, r_t: [-3607.762 -3607.762 -3607.762], eps: 0.02})
Step:   71100, Reward: [-533.050 -533.050 -533.050] [116.007], Avg: [-531.530 -531.530 -531.530] (0.1000) ({r_i: None, r_t: [-1006.619 -1006.619 -1006.619], eps: 0.1})
Step:   39100, Reward: [-1794.842 -1794.842 -1794.842] [219.636], Avg: [-788.168 -788.168 -788.168] (0.0198) ({r_i: None, r_t: [-3646.564 -3646.564 -3646.564], eps: 0.02})
Step:   71200, Reward: [-478.787 -478.787 -478.787] [66.803], Avg: [-531.456 -531.456 -531.456] (0.1000) ({r_i: None, r_t: [-952.132 -952.132 -952.132], eps: 0.1})
Step:   39200, Reward: [-1805.798 -1805.798 -1805.798] [228.434], Avg: [-790.757 -790.757 -790.757] (0.0196) ({r_i: None, r_t: [-3671.630 -3671.630 -3671.630], eps: 0.02})
Step:   71300, Reward: [-541.365 -541.365 -541.365] [145.895], Avg: [-531.470 -531.470 -531.470] (0.1000) ({r_i: None, r_t: [-1105.757 -1105.757 -1105.757], eps: 0.1})
Step:   39300, Reward: [-1915.672 -1915.672 -1915.672] [177.722], Avg: [-793.612 -793.612 -793.612] (0.0195) ({r_i: None, r_t: [-3635.401 -3635.401 -3635.401], eps: 0.019})
Step:   71400, Reward: [-511.815 -511.815 -511.815] [129.390], Avg: [-531.443 -531.443 -531.443] (0.1000) ({r_i: None, r_t: [-1060.919 -1060.919 -1060.919], eps: 0.1})
Step:   39400, Reward: [-1897.173 -1897.173 -1897.173] [257.915], Avg: [-796.406 -796.406 -796.406] (0.0193) ({r_i: None, r_t: [-3756.491 -3756.491 -3756.491], eps: 0.019})
Step:   71500, Reward: [-520.177 -520.177 -520.177] [140.669], Avg: [-531.427 -531.427 -531.427] (0.1000) ({r_i: None, r_t: [-1020.341 -1020.341 -1020.341], eps: 0.1})
Step:   39500, Reward: [-1814.091 -1814.091 -1814.091] [272.805], Avg: [-798.976 -798.976 -798.976] (0.0191) ({r_i: None, r_t: [-3765.986 -3765.986 -3765.986], eps: 0.019})
Step:   71600, Reward: [-489.680 -489.680 -489.680] [87.670], Avg: [-531.369 -531.369 -531.369] (0.1000) ({r_i: None, r_t: [-943.608 -943.608 -943.608], eps: 0.1})
Step:   39600, Reward: [-1874.984 -1874.984 -1874.984] [302.627], Avg: [-801.686 -801.686 -801.686] (0.0189) ({r_i: None, r_t: [-3613.530 -3613.530 -3613.530], eps: 0.019})
Step:   71700, Reward: [-532.258 -532.258 -532.258] [142.545], Avg: [-531.370 -531.370 -531.370] (0.1000) ({r_i: None, r_t: [-1011.388 -1011.388 -1011.388], eps: 0.1})
Step:   39700, Reward: [-1905.831 -1905.831 -1905.831] [197.281], Avg: [-804.460 -804.460 -804.460] (0.0187) ({r_i: None, r_t: [-3698.402 -3698.402 -3698.402], eps: 0.019})
Step:   71800, Reward: [-488.094 -488.094 -488.094] [83.995], Avg: [-531.310 -531.310 -531.310] (0.1000) ({r_i: None, r_t: [-1003.559 -1003.559 -1003.559], eps: 0.1})
Step:   39800, Reward: [-1804.656 -1804.656 -1804.656] [270.829], Avg: [-806.967 -806.967 -806.967] (0.0185) ({r_i: None, r_t: [-3694.171 -3694.171 -3694.171], eps: 0.019})
Step:   71900, Reward: [-476.863 -476.863 -476.863] [92.033], Avg: [-531.234 -531.234 -531.234] (0.1000) ({r_i: None, r_t: [-1026.363 -1026.363 -1026.363], eps: 0.1})
Step:   39900, Reward: [-1800.675 -1800.675 -1800.675] [215.493], Avg: [-809.451 -809.451 -809.451] (0.0183) ({r_i: None, r_t: [-3678.809 -3678.809 -3678.809], eps: 0.018})
Step:   72000, Reward: [-484.827 -484.827 -484.827] [63.405], Avg: [-531.170 -531.170 -531.170] (0.1000) ({r_i: None, r_t: [-1050.092 -1050.092 -1050.092], eps: 0.1})
Step:   40000, Reward: [-1912.726 -1912.726 -1912.726] [204.070], Avg: [-812.203 -812.203 -812.203] (0.0181) ({r_i: None, r_t: [-3659.420 -3659.420 -3659.420], eps: 0.018})
Step:   72100, Reward: [-507.681 -507.681 -507.681] [120.934], Avg: [-531.137 -531.137 -531.137] (0.1000) ({r_i: None, r_t: [-1041.530 -1041.530 -1041.530], eps: 0.1})
Step:   40100, Reward: [-1709.700 -1709.700 -1709.700] [307.757], Avg: [-814.435 -814.435 -814.435] (0.0180) ({r_i: None, r_t: [-3563.632 -3563.632 -3563.632], eps: 0.018})
Step:   72200, Reward: [-543.662 -543.662 -543.662] [159.988], Avg: [-531.155 -531.155 -531.155] (0.1000) ({r_i: None, r_t: [-1021.764 -1021.764 -1021.764], eps: 0.1})
Step:   40200, Reward: [-1773.641 -1773.641 -1773.641] [329.897], Avg: [-816.815 -816.815 -816.815] (0.0178) ({r_i: None, r_t: [-3415.264 -3415.264 -3415.264], eps: 0.018})
Step:   72300, Reward: [-520.325 -520.325 -520.325] [119.729], Avg: [-531.140 -531.140 -531.140] (0.1000) ({r_i: None, r_t: [-1015.637 -1015.637 -1015.637], eps: 0.1})
Step:   40300, Reward: [-1697.965 -1697.965 -1697.965] [254.962], Avg: [-818.997 -818.997 -818.997] (0.0176) ({r_i: None, r_t: [-3504.585 -3504.585 -3504.585], eps: 0.018})
Step:   72400, Reward: [-546.818 -546.818 -546.818] [144.573], Avg: [-531.161 -531.161 -531.161] (0.1000) ({r_i: None, r_t: [-1180.757 -1180.757 -1180.757], eps: 0.1})
Step:   40400, Reward: [-1568.186 -1568.186 -1568.186] [310.825], Avg: [-820.846 -820.846 -820.846] (0.0174) ({r_i: None, r_t: [-3429.637 -3429.637 -3429.637], eps: 0.017})
Step:   72500, Reward: [-567.403 -567.403 -567.403] [146.266], Avg: [-531.211 -531.211 -531.211] (0.1000) ({r_i: None, r_t: [-1084.105 -1084.105 -1084.105], eps: 0.1})
Step:   40500, Reward: [-1509.611 -1509.611 -1509.611] [397.911], Avg: [-822.543 -822.543 -822.543] (0.0172) ({r_i: None, r_t: [-3210.294 -3210.294 -3210.294], eps: 0.017})
Step:   72600, Reward: [-527.545 -527.545 -527.545] [107.243], Avg: [-531.206 -531.206 -531.206] (0.1000) ({r_i: None, r_t: [-1054.841 -1054.841 -1054.841], eps: 0.1})
Step:   40600, Reward: [-1518.078 -1518.078 -1518.078] [347.044], Avg: [-824.252 -824.252 -824.252] (0.0171) ({r_i: None, r_t: [-3323.962 -3323.962 -3323.962], eps: 0.017})
Step:   72700, Reward: [-506.507 -506.507 -506.507] [127.891], Avg: [-531.172 -531.172 -531.172] (0.1000) ({r_i: None, r_t: [-1066.670 -1066.670 -1066.670], eps: 0.1})
Step:   72800, Reward: [-517.966 -517.966 -517.966] [100.351], Avg: [-531.154 -531.154 -531.154] (0.1000) ({r_i: None, r_t: [-998.398 -998.398 -998.398], eps: 0.1})
Step:   40700, Reward: [-1435.866 -1435.866 -1435.866] [289.626], Avg: [-825.751 -825.751 -825.751] (0.0169) ({r_i: None, r_t: [-3028.411 -3028.411 -3028.411], eps: 0.017})
Step:   72900, Reward: [-563.893 -563.893 -563.893] [126.975], Avg: [-531.199 -531.199 -531.199] (0.1000) ({r_i: None, r_t: [-1021.904 -1021.904 -1021.904], eps: 0.1})
Step:   40800, Reward: [-1345.988 -1345.988 -1345.988] [366.990], Avg: [-827.023 -827.023 -827.023] (0.0167) ({r_i: None, r_t: [-3099.224 -3099.224 -3099.224], eps: 0.017})
Step:   73000, Reward: [-486.098 -486.098 -486.098] [75.948], Avg: [-531.137 -531.137 -531.137] (0.1000) ({r_i: None, r_t: [-954.848 -954.848 -954.848], eps: 0.1})
Step:   40900, Reward: [-1414.241 -1414.241 -1414.241] [415.401], Avg: [-828.455 -828.455 -828.455] (0.0166) ({r_i: None, r_t: [-2748.722 -2748.722 -2748.722], eps: 0.017})
Step:   73100, Reward: [-554.953 -554.953 -554.953] [121.264], Avg: [-531.170 -531.170 -531.170] (0.1000) ({r_i: None, r_t: [-1020.589 -1020.589 -1020.589], eps: 0.1})
Step:   41000, Reward: [-1009.356 -1009.356 -1009.356] [268.490], Avg: [-828.895 -828.895 -828.895] (0.0164) ({r_i: None, r_t: [-2798.033 -2798.033 -2798.033], eps: 0.016})
Step:   73200, Reward: [-546.289 -546.289 -546.289] [135.177], Avg: [-531.190 -531.190 -531.190] (0.1000) ({r_i: None, r_t: [-992.138 -992.138 -992.138], eps: 0.1})
Step:   41100, Reward: [-1059.290 -1059.290 -1059.290] [410.357], Avg: [-829.454 -829.454 -829.454] (0.0162) ({r_i: None, r_t: [-2574.452 -2574.452 -2574.452], eps: 0.016})
Step:   73300, Reward: [-535.014 -535.014 -535.014] [147.826], Avg: [-531.196 -531.196 -531.196] (0.1000) ({r_i: None, r_t: [-1001.820 -1001.820 -1001.820], eps: 0.1})
Step:   41200, Reward: [-1181.231 -1181.231 -1181.231] [402.008], Avg: [-830.306 -830.306 -830.306] (0.0161) ({r_i: None, r_t: [-2387.519 -2387.519 -2387.519], eps: 0.016})
Step:   73400, Reward: [-552.424 -552.424 -552.424] [141.253], Avg: [-531.224 -531.224 -531.224] (0.1000) ({r_i: None, r_t: [-1032.592 -1032.592 -1032.592], eps: 0.1})
Step:   41300, Reward: [-1098.250 -1098.250 -1098.250] [441.866], Avg: [-830.953 -830.953 -830.953] (0.0159) ({r_i: None, r_t: [-2365.082 -2365.082 -2365.082], eps: 0.016})
Step:   73500, Reward: [-521.580 -521.580 -521.580] [110.861], Avg: [-531.211 -531.211 -531.211] (0.1000) ({r_i: None, r_t: [-1095.028 -1095.028 -1095.028], eps: 0.1})
Step:   41400, Reward: [-1060.027 -1060.027 -1060.027] [337.329], Avg: [-831.505 -831.505 -831.505] (0.0158) ({r_i: None, r_t: [-1838.771 -1838.771 -1838.771], eps: 0.016})
Step:   73600, Reward: [-508.094 -508.094 -508.094] [132.110], Avg: [-531.180 -531.180 -531.180] (0.1000) ({r_i: None, r_t: [-1068.301 -1068.301 -1068.301], eps: 0.1})
Step:   41500, Reward: [-781.705 -781.705 -781.705] [212.481], Avg: [-831.386 -831.386 -831.386] (0.0156) ({r_i: None, r_t: [-1989.580 -1989.580 -1989.580], eps: 0.016})
Step:   73700, Reward: [-568.500 -568.500 -568.500] [117.718], Avg: [-531.231 -531.231 -531.231] (0.1000) ({r_i: None, r_t: [-1005.669 -1005.669 -1005.669], eps: 0.1})
Step:   41600, Reward: [-719.868 -719.868 -719.868] [233.053], Avg: [-831.118 -831.118 -831.118] (0.0154) ({r_i: None, r_t: [-2056.387 -2056.387 -2056.387], eps: 0.015})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		q_value = torch.zeros_like(q_next_value)
		for t in reversed(range(rewards.size(1))):
			q_value[:, t] = self.critic_local(critic_inputs[:,t])
			q_taken = torch.gather(q_value[:, t], dim=-1, index=actions[:, t].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - q_target[:, t].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		# q_value = self.critic_local(critic_inputs)
		# q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		# critic_error = (q_taken - q_target.detach())
		# critic_loss = critic_error.pow(2).mean()
		# self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-467.241 -467.241 -467.241] [89.550], Avg: [-467.241 -467.241 -467.241] (1.0000) ({r_i: None, r_t: [-8.846 -8.846 -8.846], eps: 1.0})
Step:   73800, Reward: [-534.710 -534.710 -534.710] [78.319], Avg: [-531.235 -531.235 -531.235] (0.1000) ({r_i: None, r_t: [-1045.051 -1045.051 -1045.051], eps: 0.1})
Step:   41700, Reward: [-852.655 -852.655 -852.655] [298.686], Avg: [-831.170 -831.170 -831.170] (0.0153) ({r_i: None, r_t: [-1775.810 -1775.810 -1775.810], eps: 0.015})
Step:     100, Reward: [-508.457 -508.457 -508.457] [91.329], Avg: [-487.849 -487.849 -487.849] (0.9900) ({r_i: None, r_t: [-969.177 -969.177 -969.177], eps: 0.99})
Step:   73900, Reward: [-509.636 -509.636 -509.636] [81.048], Avg: [-531.206 -531.206 -531.206] (0.1000) ({r_i: None, r_t: [-1133.173 -1133.173 -1133.173], eps: 0.1})
Step:   41800, Reward: [-867.416 -867.416 -867.416] [364.298], Avg: [-831.256 -831.256 -831.256] (0.0151) ({r_i: None, r_t: [-1610.902 -1610.902 -1610.902], eps: 0.015})
Step:   74000, Reward: [-498.863 -498.863 -498.863] [75.089], Avg: [-531.162 -531.162 -531.162] (0.1000) ({r_i: None, r_t: [-1059.945 -1059.945 -1059.945], eps: 0.1})
Step:   41900, Reward: [-698.431 -698.431 -698.431] [262.953], Avg: [-830.940 -830.940 -830.940] (0.0150) ({r_i: None, r_t: [-1497.439 -1497.439 -1497.439], eps: 0.015})
Step:     200, Reward: [-473.028 -473.028 -473.028] [74.947], Avg: [-482.909 -482.909 -482.909] (0.9801) ({r_i: None, r_t: [-980.844 -980.844 -980.844], eps: 0.98})
Step:   74100, Reward: [-510.072 -510.072 -510.072] [97.250], Avg: [-531.134 -531.134 -531.134] (0.1000) ({r_i: None, r_t: [-979.887 -979.887 -979.887], eps: 0.1})
Step:   42000, Reward: [-647.812 -647.812 -647.812] [244.708], Avg: [-830.505 -830.505 -830.505] (0.0148) ({r_i: None, r_t: [-1199.004 -1199.004 -1199.004], eps: 0.015})
Step:     300, Reward: [-502.141 -502.141 -502.141] [81.817], Avg: [-487.717 -487.717 -487.717] (0.9704) ({r_i: None, r_t: [-968.527 -968.527 -968.527], eps: 0.97})
Step:   74200, Reward: [-520.831 -520.831 -520.831] [100.186], Avg: [-531.120 -531.120 -531.120] (0.1000) ({r_i: None, r_t: [-988.158 -988.158 -988.158], eps: 0.1})
Step:   42100, Reward: [-575.484 -575.484 -575.484] [172.577], Avg: [-829.901 -829.901 -829.901] (0.0147) ({r_i: None, r_t: [-1427.899 -1427.899 -1427.899], eps: 0.015})
Step:   74300, Reward: [-492.043 -492.043 -492.043] [77.603], Avg: [-531.068 -531.068 -531.068] (0.1000) ({r_i: None, r_t: [-1078.522 -1078.522 -1078.522], eps: 0.1})
Step:     400, Reward: [-462.215 -462.215 -462.215] [90.508], Avg: [-482.616 -482.616 -482.616] (0.9607) ({r_i: None, r_t: [-1002.153 -1002.153 -1002.153], eps: 0.961})
Step:   42200, Reward: [-625.843 -625.843 -625.843] [145.249], Avg: [-829.418 -829.418 -829.418] (0.0145) ({r_i: None, r_t: [-1250.903 -1250.903 -1250.903], eps: 0.015})
Step:   74400, Reward: [-472.463 -472.463 -472.463] [89.298], Avg: [-530.989 -530.989 -530.989] (0.1000) ({r_i: None, r_t: [-1112.744 -1112.744 -1112.744], eps: 0.1})
Step:   42300, Reward: [-573.735 -573.735 -573.735] [136.579], Avg: [-828.815 -828.815 -828.815] (0.0144) ({r_i: None, r_t: [-1330.605 -1330.605 -1330.605], eps: 0.014})
Step:     500, Reward: [-501.451 -501.451 -501.451] [85.635], Avg: [-485.755 -485.755 -485.755] (0.9511) ({r_i: None, r_t: [-884.890 -884.890 -884.890], eps: 0.951})
Step:   74500, Reward: [-518.478 -518.478 -518.478] [99.271], Avg: [-530.972 -530.972 -530.972] (0.1000) ({r_i: None, r_t: [-1148.561 -1148.561 -1148.561], eps: 0.1})
Step:   42400, Reward: [-636.821 -636.821 -636.821] [175.526], Avg: [-828.364 -828.364 -828.364] (0.0143) ({r_i: None, r_t: [-1232.246 -1232.246 -1232.246], eps: 0.014})
Step:   74600, Reward: [-489.604 -489.604 -489.604] [86.915], Avg: [-530.917 -530.917 -530.917] (0.1000) ({r_i: None, r_t: [-1103.172 -1103.172 -1103.172], eps: 0.1})
Step:     600, Reward: [-523.745 -523.745 -523.745] [85.134], Avg: [-491.182 -491.182 -491.182] (0.9416) ({r_i: None, r_t: [-974.842 -974.842 -974.842], eps: 0.942})
Step:   42500, Reward: [-565.010 -565.010 -565.010] [131.941], Avg: [-827.745 -827.745 -827.745] (0.0141) ({r_i: None, r_t: [-1100.734 -1100.734 -1100.734], eps: 0.014})
Step:   74700, Reward: [-519.456 -519.456 -519.456] [83.579], Avg: [-530.901 -530.901 -530.901] (0.1000) ({r_i: None, r_t: [-1030.495 -1030.495 -1030.495], eps: 0.1})
Step:   42600, Reward: [-545.091 -545.091 -545.091] [137.506], Avg: [-827.083 -827.083 -827.083] (0.0140) ({r_i: None, r_t: [-1124.624 -1124.624 -1124.624], eps: 0.014})
Step:   74800, Reward: [-499.210 -499.210 -499.210] [97.407], Avg: [-530.859 -530.859 -530.859] (0.1000) ({r_i: None, r_t: [-1059.612 -1059.612 -1059.612], eps: 0.1})
Step:     700, Reward: [-525.867 -525.867 -525.867] [68.326], Avg: [-495.518 -495.518 -495.518] (0.9322) ({r_i: None, r_t: [-939.890 -939.890 -939.890], eps: 0.932})
Step:   42700, Reward: [-562.078 -562.078 -562.078] [143.473], Avg: [-826.464 -826.464 -826.464] (0.0138) ({r_i: None, r_t: [-1099.222 -1099.222 -1099.222], eps: 0.014})
Step:   74900, Reward: [-522.277 -522.277 -522.277] [79.959], Avg: [-530.848 -530.848 -530.848] (0.1000) ({r_i: None, r_t: [-1139.352 -1139.352 -1139.352], eps: 0.1})
Step:     800, Reward: [-494.477 -494.477 -494.477] [83.961], Avg: [-495.402 -495.402 -495.402] (0.9229) ({r_i: None, r_t: [-968.421 -968.421 -968.421], eps: 0.923})
Step:   42800, Reward: [-548.027 -548.027 -548.027] [128.419], Avg: [-825.815 -825.815 -825.815] (0.0137) ({r_i: None, r_t: [-1106.794 -1106.794 -1106.794], eps: 0.014})
Step:   75000, Reward: [-512.327 -512.327 -512.327] [81.540], Avg: [-530.823 -530.823 -530.823] (0.1000) ({r_i: None, r_t: [-983.744 -983.744 -983.744], eps: 0.1})
Step:   42900, Reward: [-543.100 -543.100 -543.100] [105.523], Avg: [-825.158 -825.158 -825.158] (0.0136) ({r_i: None, r_t: [-1029.647 -1029.647 -1029.647], eps: 0.014})
Step:   75100, Reward: [-564.214 -564.214 -564.214] [164.417], Avg: [-530.867 -530.867 -530.867] (0.1000) ({r_i: None, r_t: [-1050.218 -1050.218 -1050.218], eps: 0.1})
Step:     900, Reward: [-450.952 -450.952 -450.952] [69.213], Avg: [-490.957 -490.957 -490.957] (0.9137) ({r_i: None, r_t: [-984.176 -984.176 -984.176], eps: 0.914})
Step:   43000, Reward: [-544.413 -544.413 -544.413] [123.313], Avg: [-824.506 -824.506 -824.506] (0.0134) ({r_i: None, r_t: [-1102.343 -1102.343 -1102.343], eps: 0.013})
Step:   75200, Reward: [-515.186 -515.186 -515.186] [85.530], Avg: [-530.847 -530.847 -530.847] (0.1000) ({r_i: None, r_t: [-1017.091 -1017.091 -1017.091], eps: 0.1})
Step:    1000, Reward: [-472.861 -472.861 -472.861] [83.686], Avg: [-489.312 -489.312 -489.312] (0.9046) ({r_i: None, r_t: [-945.523 -945.523 -945.523], eps: 0.905})
Step:   43100, Reward: [-508.956 -508.956 -508.956] [85.245], Avg: [-823.776 -823.776 -823.776] (0.0133) ({r_i: None, r_t: [-1004.377 -1004.377 -1004.377], eps: 0.013})
Step:   75300, Reward: [-523.692 -523.692 -523.692] [156.452], Avg: [-530.837 -530.837 -530.837] (0.1000) ({r_i: None, r_t: [-1039.161 -1039.161 -1039.161], eps: 0.1})
Step:   75400, Reward: [-488.535 -488.535 -488.535] [71.132], Avg: [-530.781 -530.781 -530.781] (0.1000) ({r_i: None, r_t: [-1031.734 -1031.734 -1031.734], eps: 0.1})
Step:   43200, Reward: [-557.559 -557.559 -557.559] [78.185], Avg: [-823.161 -823.161 -823.161] (0.0132) ({r_i: None, r_t: [-1119.299 -1119.299 -1119.299], eps: 0.013})
Step:    1100, Reward: [-553.471 -553.471 -553.471] [90.988], Avg: [-494.659 -494.659 -494.659] (0.8956) ({r_i: None, r_t: [-1012.842 -1012.842 -1012.842], eps: 0.896})
Step:   75500, Reward: [-511.558 -511.558 -511.558] [117.855], Avg: [-530.756 -530.756 -530.756] (0.1000) ({r_i: None, r_t: [-1153.624 -1153.624 -1153.624], eps: 0.1})
Step:   43300, Reward: [-523.443 -523.443 -523.443] [121.307], Avg: [-822.470 -822.470 -822.470] (0.0130) ({r_i: None, r_t: [-1090.417 -1090.417 -1090.417], eps: 0.013})
Step:    1200, Reward: [-519.110 -519.110 -519.110] [117.916], Avg: [-496.540 -496.540 -496.540] (0.8867) ({r_i: None, r_t: [-1017.227 -1017.227 -1017.227], eps: 0.887})
Step:   75600, Reward: [-501.776 -501.776 -501.776] [75.502], Avg: [-530.717 -530.717 -530.717] (0.1000) ({r_i: None, r_t: [-1093.414 -1093.414 -1093.414], eps: 0.1})
Step:   43400, Reward: [-539.338 -539.338 -539.338] [122.386], Avg: [-821.820 -821.820 -821.820] (0.0129) ({r_i: None, r_t: [-1053.404 -1053.404 -1053.404], eps: 0.013})
Step:   75700, Reward: [-509.767 -509.767 -509.767] [119.610], Avg: [-530.690 -530.690 -530.690] (0.1000) ({r_i: None, r_t: [-1087.887 -1087.887 -1087.887], eps: 0.1})
Step:   43500, Reward: [-504.313 -504.313 -504.313] [112.802], Avg: [-821.091 -821.091 -821.091] (0.0128) ({r_i: None, r_t: [-969.888 -969.888 -969.888], eps: 0.013})
Step:    1300, Reward: [-484.505 -484.505 -484.505] [71.335], Avg: [-495.680 -495.680 -495.680] (0.8778) ({r_i: None, r_t: [-955.719 -955.719 -955.719], eps: 0.878})
Step:   75800, Reward: [-608.907 -608.907 -608.907] [142.372], Avg: [-530.793 -530.793 -530.793] (0.1000) ({r_i: None, r_t: [-1127.505 -1127.505 -1127.505], eps: 0.1})
Step:   43600, Reward: [-477.125 -477.125 -477.125] [95.796], Avg: [-820.304 -820.304 -820.304] (0.0126) ({r_i: None, r_t: [-1055.446 -1055.446 -1055.446], eps: 0.013})
Step:    1400, Reward: [-490.454 -490.454 -490.454] [88.434], Avg: [-495.332 -495.332 -495.332] (0.8691) ({r_i: None, r_t: [-986.598 -986.598 -986.598], eps: 0.869})
Step:   75900, Reward: [-526.814 -526.814 -526.814] [97.544], Avg: [-530.788 -530.788 -530.788] (0.1000) ({r_i: None, r_t: [-1104.493 -1104.493 -1104.493], eps: 0.1})
Step:   43700, Reward: [-482.791 -482.791 -482.791] [105.971], Avg: [-819.534 -819.534 -819.534] (0.0125) ({r_i: None, r_t: [-1073.845 -1073.845 -1073.845], eps: 0.013})
Step:   76000, Reward: [-625.266 -625.266 -625.266] [161.733], Avg: [-530.912 -530.912 -530.912] (0.1000) ({r_i: None, r_t: [-1077.351 -1077.351 -1077.351], eps: 0.1})
Step:   43800, Reward: [-513.033 -513.033 -513.033] [77.460], Avg: [-818.835 -818.835 -818.835] (0.0124) ({r_i: None, r_t: [-1006.901 -1006.901 -1006.901], eps: 0.012})
Step:    1500, Reward: [-499.542 -499.542 -499.542] [91.609], Avg: [-495.595 -495.595 -495.595] (0.8604) ({r_i: None, r_t: [-1023.389 -1023.389 -1023.389], eps: 0.86})
Step:   76100, Reward: [-478.917 -478.917 -478.917] [106.549], Avg: [-530.843 -530.843 -530.843] (0.1000) ({r_i: None, r_t: [-1053.110 -1053.110 -1053.110], eps: 0.1})
Step:   43900, Reward: [-549.193 -549.193 -549.193] [96.590], Avg: [-818.223 -818.223 -818.223] (0.0123) ({r_i: None, r_t: [-1059.947 -1059.947 -1059.947], eps: 0.012})
Step:    1600, Reward: [-487.268 -487.268 -487.268] [92.092], Avg: [-495.105 -495.105 -495.105] (0.8518) ({r_i: None, r_t: [-955.158 -955.158 -955.158], eps: 0.852})
Step:   76200, Reward: [-550.226 -550.226 -550.226] [176.448], Avg: [-530.869 -530.869 -530.869] (0.1000) ({r_i: None, r_t: [-1211.939 -1211.939 -1211.939], eps: 0.1})
Step:   44000, Reward: [-477.198 -477.198 -477.198] [70.482], Avg: [-817.449 -817.449 -817.449] (0.0121) ({r_i: None, r_t: [-1006.337 -1006.337 -1006.337], eps: 0.012})
Step:   76300, Reward: [-508.569 -508.569 -508.569] [86.569], Avg: [-530.840 -530.840 -530.840] (0.1000) ({r_i: None, r_t: [-1191.597 -1191.597 -1191.597], eps: 0.1})
Step:    1700, Reward: [-476.342 -476.342 -476.342] [65.441], Avg: [-494.063 -494.063 -494.063] (0.8433) ({r_i: None, r_t: [-1007.985 -1007.985 -1007.985], eps: 0.843})
Step:   44100, Reward: [-521.703 -521.703 -521.703] [120.591], Avg: [-816.780 -816.780 -816.780] (0.0120) ({r_i: None, r_t: [-982.494 -982.494 -982.494], eps: 0.012})
Step:   76400, Reward: [-576.622 -576.622 -576.622] [199.816], Avg: [-530.900 -530.900 -530.900] (0.1000) ({r_i: None, r_t: [-1136.045 -1136.045 -1136.045], eps: 0.1})
Step:   44200, Reward: [-493.245 -493.245 -493.245] [97.860], Avg: [-816.050 -816.050 -816.050] (0.0119) ({r_i: None, r_t: [-955.428 -955.428 -955.428], eps: 0.012})
Step:    1800, Reward: [-423.725 -423.725 -423.725] [76.110], Avg: [-490.361 -490.361 -490.361] (0.8349) ({r_i: None, r_t: [-989.182 -989.182 -989.182], eps: 0.835})
Step:   76500, Reward: [-478.974 -478.974 -478.974] [106.578], Avg: [-530.832 -530.832 -530.832] (0.1000) ({r_i: None, r_t: [-1053.705 -1053.705 -1053.705], eps: 0.1})
Step:   44300, Reward: [-446.561 -446.561 -446.561] [73.924], Avg: [-815.218 -815.218 -815.218] (0.0118) ({r_i: None, r_t: [-994.062 -994.062 -994.062], eps: 0.012})
Step:   76600, Reward: [-529.005 -529.005 -529.005] [101.614], Avg: [-530.829 -530.829 -530.829] (0.1000) ({r_i: None, r_t: [-1096.117 -1096.117 -1096.117], eps: 0.1})
Step:    1900, Reward: [-504.271 -504.271 -504.271] [89.758], Avg: [-491.056 -491.056 -491.056] (0.8266) ({r_i: None, r_t: [-999.242 -999.242 -999.242], eps: 0.827})
Step:   44400, Reward: [-485.786 -485.786 -485.786] [75.957], Avg: [-814.477 -814.477 -814.477] (0.0117) ({r_i: None, r_t: [-997.630 -997.630 -997.630], eps: 0.012})
Step:   76700, Reward: [-538.598 -538.598 -538.598] [128.631], Avg: [-530.839 -530.839 -530.839] (0.1000) ({r_i: None, r_t: [-1135.550 -1135.550 -1135.550], eps: 0.1})
Step:   44500, Reward: [-508.638 -508.638 -508.638] [113.919], Avg: [-813.792 -813.792 -813.792] (0.0115) ({r_i: None, r_t: [-1085.773 -1085.773 -1085.773], eps: 0.012})
Step:    2000, Reward: [-474.929 -474.929 -474.929] [75.848], Avg: [-490.288 -490.288 -490.288] (0.8183) ({r_i: None, r_t: [-1008.585 -1008.585 -1008.585], eps: 0.818})
Step:   76800, Reward: [-481.455 -481.455 -481.455] [117.334], Avg: [-530.775 -530.775 -530.775] (0.1000) ({r_i: None, r_t: [-1101.521 -1101.521 -1101.521], eps: 0.1})
Step:   44600, Reward: [-482.606 -482.606 -482.606] [97.900], Avg: [-813.051 -813.051 -813.051] (0.0114) ({r_i: None, r_t: [-1035.228 -1035.228 -1035.228], eps: 0.011})
Step:   76900, Reward: [-568.373 -568.373 -568.373] [179.302], Avg: [-530.824 -530.824 -530.824] (0.1000) ({r_i: None, r_t: [-1129.869 -1129.869 -1129.869], eps: 0.1})
Step:    2100, Reward: [-539.272 -539.272 -539.272] [80.095], Avg: [-492.515 -492.515 -492.515] (0.8102) ({r_i: None, r_t: [-984.855 -984.855 -984.855], eps: 0.81})
Step:   44700, Reward: [-505.936 -505.936 -505.936] [114.008], Avg: [-812.365 -812.365 -812.365] (0.0113) ({r_i: None, r_t: [-962.012 -962.012 -962.012], eps: 0.011})
Step:   77000, Reward: [-529.208 -529.208 -529.208] [111.863], Avg: [-530.822 -530.822 -530.822] (0.1000) ({r_i: None, r_t: [-1112.103 -1112.103 -1112.103], eps: 0.1})
Step:   44800, Reward: [-603.144 -603.144 -603.144] [203.319], Avg: [-811.899 -811.899 -811.899] (0.0112) ({r_i: None, r_t: [-1025.227 -1025.227 -1025.227], eps: 0.011})
Step:    2200, Reward: [-445.248 -445.248 -445.248] [32.926], Avg: [-490.460 -490.460 -490.460] (0.8021) ({r_i: None, r_t: [-955.147 -955.147 -955.147], eps: 0.802})
Step:   77100, Reward: [-629.545 -629.545 -629.545] [190.069], Avg: [-530.950 -530.950 -530.950] (0.1000) ({r_i: None, r_t: [-1137.466 -1137.466 -1137.466], eps: 0.1})
Step:   44900, Reward: [-516.928 -516.928 -516.928] [156.898], Avg: [-811.244 -811.244 -811.244] (0.0111) ({r_i: None, r_t: [-1139.627 -1139.627 -1139.627], eps: 0.011})
Step:   77200, Reward: [-530.113 -530.113 -530.113] [87.168], Avg: [-530.949 -530.949 -530.949] (0.1000) ({r_i: None, r_t: [-1083.037 -1083.037 -1083.037], eps: 0.1})
Step:    2300, Reward: [-454.507 -454.507 -454.507] [71.616], Avg: [-488.962 -488.962 -488.962] (0.7941) ({r_i: None, r_t: [-1059.397 -1059.397 -1059.397], eps: 0.794})
Step:   45000, Reward: [-531.437 -531.437 -531.437] [168.356], Avg: [-810.623 -810.623 -810.623] (0.0110) ({r_i: None, r_t: [-1078.942 -1078.942 -1078.942], eps: 0.011})
Step:   77300, Reward: [-573.357 -573.357 -573.357] [136.877], Avg: [-531.004 -531.004 -531.004] (0.1000) ({r_i: None, r_t: [-1099.478 -1099.478 -1099.478], eps: 0.1})
Step:   45100, Reward: [-528.190 -528.190 -528.190] [173.596], Avg: [-809.999 -809.999 -809.999] (0.0109) ({r_i: None, r_t: [-1046.063 -1046.063 -1046.063], eps: 0.011})
Step:    2400, Reward: [-473.840 -473.840 -473.840] [77.581], Avg: [-488.357 -488.357 -488.357] (0.7862) ({r_i: None, r_t: [-1019.466 -1019.466 -1019.466], eps: 0.786})
Step:   77400, Reward: [-532.799 -532.799 -532.799] [124.458], Avg: [-531.006 -531.006 -531.006] (0.1000) ({r_i: None, r_t: [-1117.977 -1117.977 -1117.977], eps: 0.1})
Step:   45200, Reward: [-625.988 -625.988 -625.988] [279.153], Avg: [-809.592 -809.592 -809.592] (0.0108) ({r_i: None, r_t: [-1195.902 -1195.902 -1195.902], eps: 0.011})
Step:   77500, Reward: [-555.467 -555.467 -555.467] [102.653], Avg: [-531.037 -531.037 -531.037] (0.1000) ({r_i: None, r_t: [-1186.414 -1186.414 -1186.414], eps: 0.1})
Step:    2500, Reward: [-475.362 -475.362 -475.362] [112.388], Avg: [-487.857 -487.857 -487.857] (0.7783) ({r_i: None, r_t: [-973.139 -973.139 -973.139], eps: 0.778})
Step:   45300, Reward: [-555.014 -555.014 -555.014] [235.258], Avg: [-809.032 -809.032 -809.032] (0.0107) ({r_i: None, r_t: [-1377.753 -1377.753 -1377.753], eps: 0.011})
Step:   77600, Reward: [-634.170 -634.170 -634.170] [179.781], Avg: [-531.170 -531.170 -531.170] (0.1000) ({r_i: None, r_t: [-1154.236 -1154.236 -1154.236], eps: 0.1})
Step:   45400, Reward: [-583.565 -583.565 -583.565] [247.108], Avg: [-808.536 -808.536 -808.536] (0.0106) ({r_i: None, r_t: [-1316.890 -1316.890 -1316.890], eps: 0.011})
Step:    2600, Reward: [-488.121 -488.121 -488.121] [92.200], Avg: [-487.867 -487.867 -487.867] (0.7705) ({r_i: None, r_t: [-957.861 -957.861 -957.861], eps: 0.771})
Step:   77700, Reward: [-533.682 -533.682 -533.682] [110.748], Avg: [-531.173 -531.173 -531.173] (0.1000) ({r_i: None, r_t: [-1143.002 -1143.002 -1143.002], eps: 0.1})
Step:   77800, Reward: [-640.464 -640.464 -640.464] [175.776], Avg: [-531.314 -531.314 -531.314] (0.1000) ({r_i: None, r_t: [-1156.417 -1156.417 -1156.417], eps: 0.1})
Step:   45500, Reward: [-690.354 -690.354 -690.354] [287.973], Avg: [-808.277 -808.277 -808.277] (0.0104) ({r_i: None, r_t: [-1469.560 -1469.560 -1469.560], eps: 0.01})
Step:    2700, Reward: [-512.910 -512.910 -512.910] [95.930], Avg: [-488.761 -488.761 -488.761] (0.7629) ({r_i: None, r_t: [-992.920 -992.920 -992.920], eps: 0.763})
Step:   77900, Reward: [-511.664 -511.664 -511.664] [105.351], Avg: [-531.288 -531.288 -531.288] (0.1000) ({r_i: None, r_t: [-1077.435 -1077.435 -1077.435], eps: 0.1})
Step:   45600, Reward: [-758.659 -758.659 -758.659] [339.520], Avg: [-808.168 -808.168 -808.168] (0.0103) ({r_i: None, r_t: [-1727.324 -1727.324 -1727.324], eps: 0.01})
Step:   78000, Reward: [-536.739 -536.739 -536.739] [136.256], Avg: [-531.295 -531.295 -531.295] (0.1000) ({r_i: None, r_t: [-1212.210 -1212.210 -1212.210], eps: 0.1})
Step:    2800, Reward: [-517.712 -517.712 -517.712] [127.663], Avg: [-489.760 -489.760 -489.760] (0.7553) ({r_i: None, r_t: [-1020.501 -1020.501 -1020.501], eps: 0.755})
Step:   45700, Reward: [-923.618 -923.618 -923.618] [382.777], Avg: [-808.420 -808.420 -808.420] (0.0102) ({r_i: None, r_t: [-1609.820 -1609.820 -1609.820], eps: 0.01})
Step:   78100, Reward: [-647.270 -647.270 -647.270] [244.288], Avg: [-531.444 -531.444 -531.444] (0.1000) ({r_i: None, r_t: [-1109.177 -1109.177 -1109.177], eps: 0.1})
Step:   45800, Reward: [-960.470 -960.470 -960.470] [300.420], Avg: [-808.752 -808.752 -808.752] (0.0101) ({r_i: None, r_t: [-2066.833 -2066.833 -2066.833], eps: 0.01})
Step:    2900, Reward: [-497.033 -497.033 -497.033] [91.304], Avg: [-490.002 -490.002 -490.002] (0.7477) ({r_i: None, r_t: [-936.815 -936.815 -936.815], eps: 0.748})
Step:   78200, Reward: [-620.042 -620.042 -620.042] [168.081], Avg: [-531.557 -531.557 -531.557] (0.1000) ({r_i: None, r_t: [-1173.799 -1173.799 -1173.799], eps: 0.1})
Step:   45900, Reward: [-1112.454 -1112.454 -1112.454] [396.697], Avg: [-809.412 -809.412 -809.412] (0.0100) ({r_i: None, r_t: [-1803.918 -1803.918 -1803.918], eps: 0.01})
Step:   78300, Reward: [-569.741 -569.741 -569.741] [113.340], Avg: [-531.606 -531.606 -531.606] (0.1000) ({r_i: None, r_t: [-1069.345 -1069.345 -1069.345], eps: 0.1})
Step:    3000, Reward: [-467.897 -467.897 -467.897] [89.942], Avg: [-489.289 -489.289 -489.289] (0.7403) ({r_i: None, r_t: [-962.981 -962.981 -962.981], eps: 0.74})
Step:   46000, Reward: [-1048.199 -1048.199 -1048.199] [351.387], Avg: [-809.930 -809.930 -809.930] (0.0100) ({r_i: None, r_t: [-2046.155 -2046.155 -2046.155], eps: 0.01})
Step:   78400, Reward: [-557.433 -557.433 -557.433] [113.362], Avg: [-531.639 -531.639 -531.639] (0.1000) ({r_i: None, r_t: [-1156.638 -1156.638 -1156.638], eps: 0.1})
Step:   46100, Reward: [-1218.217 -1218.217 -1218.217] [400.388], Avg: [-810.814 -810.814 -810.814] (0.0100) ({r_i: None, r_t: [-2007.451 -2007.451 -2007.451], eps: 0.01})
Step:    3100, Reward: [-508.717 -508.717 -508.717] [84.985], Avg: [-489.896 -489.896 -489.896] (0.7329) ({r_i: None, r_t: [-954.051 -954.051 -954.051], eps: 0.733})
Step:   78500, Reward: [-589.659 -589.659 -589.659] [166.563], Avg: [-531.712 -531.712 -531.712] (0.1000) ({r_i: None, r_t: [-1136.379 -1136.379 -1136.379], eps: 0.1})
Step:   46200, Reward: [-1044.712 -1044.712 -1044.712] [360.072], Avg: [-811.319 -811.319 -811.319] (0.0100) ({r_i: None, r_t: [-2186.449 -2186.449 -2186.449], eps: 0.01})
Step:   78600, Reward: [-592.349 -592.349 -592.349] [142.345], Avg: [-531.789 -531.789 -531.789] (0.1000) ({r_i: None, r_t: [-1190.224 -1190.224 -1190.224], eps: 0.1})
Step:    3200, Reward: [-469.906 -469.906 -469.906] [123.599], Avg: [-489.290 -489.290 -489.290] (0.7256) ({r_i: None, r_t: [-983.108 -983.108 -983.108], eps: 0.726})
Step:   46300, Reward: [-1266.208 -1266.208 -1266.208] [301.791], Avg: [-812.299 -812.299 -812.299] (0.0100) ({r_i: None, r_t: [-2539.588 -2539.588 -2539.588], eps: 0.01})
Step:   78700, Reward: [-541.031 -541.031 -541.031] [158.531], Avg: [-531.801 -531.801 -531.801] (0.1000) ({r_i: None, r_t: [-1197.914 -1197.914 -1197.914], eps: 0.1})
Step:   46400, Reward: [-1365.255 -1365.255 -1365.255] [327.013], Avg: [-813.488 -813.488 -813.488] (0.0100) ({r_i: None, r_t: [-2736.438 -2736.438 -2736.438], eps: 0.01})
Step:    3300, Reward: [-452.883 -452.883 -452.883] [48.164], Avg: [-488.219 -488.219 -488.219] (0.7183) ({r_i: None, r_t: [-975.658 -975.658 -975.658], eps: 0.718})
Step:   78800, Reward: [-625.566 -625.566 -625.566] [176.304], Avg: [-531.920 -531.920 -531.920] (0.1000) ({r_i: None, r_t: [-1235.611 -1235.611 -1235.611], eps: 0.1})
Step:   46500, Reward: [-1463.750 -1463.750 -1463.750] [465.874], Avg: [-814.884 -814.884 -814.884] (0.0100) ({r_i: None, r_t: [-2870.937 -2870.937 -2870.937], eps: 0.01})
Step:   78900, Reward: [-583.323 -583.323 -583.323] [198.711], Avg: [-531.985 -531.985 -531.985] (0.1000) ({r_i: None, r_t: [-1160.722 -1160.722 -1160.722], eps: 0.1})
Step:    3400, Reward: [-484.338 -484.338 -484.338] [99.490], Avg: [-488.109 -488.109 -488.109] (0.7112) ({r_i: None, r_t: [-952.201 -952.201 -952.201], eps: 0.711})
Step:   46600, Reward: [-1581.923 -1581.923 -1581.923] [358.067], Avg: [-816.526 -816.526 -816.526] (0.0100) ({r_i: None, r_t: [-3010.025 -3010.025 -3010.025], eps: 0.01})
Step:   79000, Reward: [-610.967 -610.967 -610.967] [131.061], Avg: [-532.085 -532.085 -532.085] (0.1000) ({r_i: None, r_t: [-1330.315 -1330.315 -1330.315], eps: 0.1})
Step:   46700, Reward: [-1496.976 -1496.976 -1496.976] [415.036], Avg: [-817.980 -817.980 -817.980] (0.0100) ({r_i: None, r_t: [-2856.447 -2856.447 -2856.447], eps: 0.01})
Step:    3500, Reward: [-496.728 -496.728 -496.728] [84.264], Avg: [-488.348 -488.348 -488.348] (0.7041) ({r_i: None, r_t: [-962.050 -962.050 -962.050], eps: 0.704})
Step:   79100, Reward: [-649.594 -649.594 -649.594] [173.774], Avg: [-532.233 -532.233 -532.233] (0.1000) ({r_i: None, r_t: [-1159.687 -1159.687 -1159.687], eps: 0.1})
Step:   46800, Reward: [-1584.522 -1584.522 -1584.522] [305.634], Avg: [-819.615 -819.615 -819.615] (0.0100) ({r_i: None, r_t: [-3197.202 -3197.202 -3197.202], eps: 0.01})
Step:   79200, Reward: [-517.102 -517.102 -517.102] [87.370], Avg: [-532.214 -532.214 -532.214] (0.1000) ({r_i: None, r_t: [-1280.450 -1280.450 -1280.450], eps: 0.1})
Step:    3600, Reward: [-511.012 -511.012 -511.012] [101.214], Avg: [-488.961 -488.961 -488.961] (0.6970) ({r_i: None, r_t: [-1018.408 -1018.408 -1018.408], eps: 0.697})
Step:   46900, Reward: [-1555.414 -1555.414 -1555.414] [216.787], Avg: [-821.180 -821.180 -821.180] (0.0100) ({r_i: None, r_t: [-3247.369 -3247.369 -3247.369], eps: 0.01})
Step:   79300, Reward: [-605.844 -605.844 -605.844] [152.310], Avg: [-532.307 -532.307 -532.307] (0.1000) ({r_i: None, r_t: [-1111.541 -1111.541 -1111.541], eps: 0.1})
Step:   47000, Reward: [-1665.388 -1665.388 -1665.388] [248.268], Avg: [-822.972 -822.972 -822.972] (0.0100) ({r_i: None, r_t: [-3239.682 -3239.682 -3239.682], eps: 0.01})
Step:    3700, Reward: [-472.252 -472.252 -472.252] [79.502], Avg: [-488.521 -488.521 -488.521] (0.6901) ({r_i: None, r_t: [-990.623 -990.623 -990.623], eps: 0.69})
Step:   79400, Reward: [-626.910 -626.910 -626.910] [162.177], Avg: [-532.426 -532.426 -532.426] (0.1000) ({r_i: None, r_t: [-1229.648 -1229.648 -1229.648], eps: 0.1})
Step:   47100, Reward: [-1827.152 -1827.152 -1827.152] [411.314], Avg: [-825.100 -825.100 -825.100] (0.0100) ({r_i: None, r_t: [-3649.562 -3649.562 -3649.562], eps: 0.01})
Step:   79500, Reward: [-658.178 -658.178 -658.178] [234.929], Avg: [-532.584 -532.584 -532.584] (0.1000) ({r_i: None, r_t: [-1209.311 -1209.311 -1209.311], eps: 0.1})
Step:    3800, Reward: [-449.314 -449.314 -449.314] [61.272], Avg: [-487.515 -487.515 -487.515] (0.6832) ({r_i: None, r_t: [-983.568 -983.568 -983.568], eps: 0.683})
Step:   47200, Reward: [-1804.798 -1804.798 -1804.798] [288.397], Avg: [-827.171 -827.171 -827.171] (0.0100) ({r_i: None, r_t: [-3547.447 -3547.447 -3547.447], eps: 0.01})
Step:   79600, Reward: [-612.382 -612.382 -612.382] [206.846], Avg: [-532.684 -532.684 -532.684] (0.1000) ({r_i: None, r_t: [-1185.928 -1185.928 -1185.928], eps: 0.1})
Step:   47300, Reward: [-1772.501 -1772.501 -1772.501] [193.043], Avg: [-829.166 -829.166 -829.166] (0.0100) ({r_i: None, r_t: [-3737.133 -3737.133 -3737.133], eps: 0.01})
Step:    3900, Reward: [-446.618 -446.618 -446.618] [77.171], Avg: [-486.493 -486.493 -486.493] (0.6764) ({r_i: None, r_t: [-939.806 -939.806 -939.806], eps: 0.676})
Step:   79700, Reward: [-637.759 -637.759 -637.759] [211.610], Avg: [-532.816 -532.816 -532.816] (0.1000) ({r_i: None, r_t: [-1271.288 -1271.288 -1271.288], eps: 0.1})
Step:   47400, Reward: [-2056.760 -2056.760 -2056.760] [247.133], Avg: [-831.750 -831.750 -831.750] (0.0100) ({r_i: None, r_t: [-3878.022 -3878.022 -3878.022], eps: 0.01})
Step:   79800, Reward: [-730.685 -730.685 -730.685] [277.322], Avg: [-533.063 -533.063 -533.063] (0.1000) ({r_i: None, r_t: [-1286.314 -1286.314 -1286.314], eps: 0.1})
Step:    4000, Reward: [-498.588 -498.588 -498.588] [87.888], Avg: [-486.788 -486.788 -486.788] (0.6696) ({r_i: None, r_t: [-973.010 -973.010 -973.010], eps: 0.67})
Step:   47500, Reward: [-1889.464 -1889.464 -1889.464] [359.726], Avg: [-833.972 -833.972 -833.972] (0.0100) ({r_i: None, r_t: [-3718.707 -3718.707 -3718.707], eps: 0.01})
Step:   79900, Reward: [-648.180 -648.180 -648.180] [244.880], Avg: [-533.207 -533.207 -533.207] (0.1000) ({r_i: None, r_t: [-1195.250 -1195.250 -1195.250], eps: 0.1})
Step:   47600, Reward: [-2041.511 -2041.511 -2041.511] [286.328], Avg: [-836.504 -836.504 -836.504] (0.0100) ({r_i: None, r_t: [-3981.545 -3981.545 -3981.545], eps: 0.01})
Step:    4100, Reward: [-462.894 -462.894 -462.894] [74.350], Avg: [-486.219 -486.219 -486.219] (0.6630) ({r_i: None, r_t: [-995.587 -995.587 -995.587], eps: 0.663})
Step:   80000, Reward: [-631.925 -631.925 -631.925] [182.421], Avg: [-533.330 -533.330 -533.330] (0.1000) ({r_i: None, r_t: [-1231.968 -1231.968 -1231.968], eps: 0.1})
Step:   47700, Reward: [-1807.596 -1807.596 -1807.596] [195.718], Avg: [-838.535 -838.535 -838.535] (0.0100) ({r_i: None, r_t: [-3897.530 -3897.530 -3897.530], eps: 0.01})
Step:   80100, Reward: [-630.391 -630.391 -630.391] [158.137], Avg: [-533.451 -533.451 -533.451] (0.1000) ({r_i: None, r_t: [-1131.624 -1131.624 -1131.624], eps: 0.1})
Step:    4200, Reward: [-476.687 -476.687 -476.687] [64.557], Avg: [-485.997 -485.997 -485.997] (0.6564) ({r_i: None, r_t: [-929.279 -929.279 -929.279], eps: 0.656})
Step:   47800, Reward: [-1937.899 -1937.899 -1937.899] [258.380], Avg: [-840.830 -840.830 -840.830] (0.0100) ({r_i: None, r_t: [-4000.989 -4000.989 -4000.989], eps: 0.01})
Step:   80200, Reward: [-660.864 -660.864 -660.864] [282.674], Avg: [-533.610 -533.610 -533.610] (0.1000) ({r_i: None, r_t: [-1263.877 -1263.877 -1263.877], eps: 0.1})
Step:    4300, Reward: [-475.636 -475.636 -475.636] [90.957], Avg: [-485.762 -485.762 -485.762] (0.6498) ({r_i: None, r_t: [-984.524 -984.524 -984.524], eps: 0.65})
Step:   47900, Reward: [-1936.040 -1936.040 -1936.040] [224.158], Avg: [-843.112 -843.112 -843.112] (0.0100) ({r_i: None, r_t: [-3892.446 -3892.446 -3892.446], eps: 0.01})
Step:   80300, Reward: [-647.435 -647.435 -647.435] [232.579], Avg: [-533.752 -533.752 -533.752] (0.1000) ({r_i: None, r_t: [-1212.716 -1212.716 -1212.716], eps: 0.1})
Step:   80400, Reward: [-635.317 -635.317 -635.317] [171.140], Avg: [-533.878 -533.878 -533.878] (0.1000) ({r_i: None, r_t: [-1250.814 -1250.814 -1250.814], eps: 0.1})
Step:   48000, Reward: [-2004.545 -2004.545 -2004.545] [225.410], Avg: [-845.527 -845.527 -845.527] (0.0100) ({r_i: None, r_t: [-3911.199 -3911.199 -3911.199], eps: 0.01})
Step:    4400, Reward: [-498.571 -498.571 -498.571] [103.289], Avg: [-486.047 -486.047 -486.047] (0.6433) ({r_i: None, r_t: [-950.434 -950.434 -950.434], eps: 0.643})
Step:   80500, Reward: [-667.183 -667.183 -667.183] [183.238], Avg: [-534.043 -534.043 -534.043] (0.1000) ({r_i: None, r_t: [-1332.913 -1332.913 -1332.913], eps: 0.1})
Step:   48100, Reward: [-1943.695 -1943.695 -1943.695] [256.452], Avg: [-847.805 -847.805 -847.805] (0.0100) ({r_i: None, r_t: [-3811.159 -3811.159 -3811.159], eps: 0.01})
Step:    4500, Reward: [-456.931 -456.931 -456.931] [81.437], Avg: [-485.414 -485.414 -485.414] (0.6369) ({r_i: None, r_t: [-956.263 -956.263 -956.263], eps: 0.637})
Step:   80600, Reward: [-667.387 -667.387 -667.387] [190.364], Avg: [-534.209 -534.209 -534.209] (0.1000) ({r_i: None, r_t: [-1301.912 -1301.912 -1301.912], eps: 0.1})
Step:   48200, Reward: [-1999.169 -1999.169 -1999.169] [358.956], Avg: [-850.189 -850.189 -850.189] (0.0100) ({r_i: None, r_t: [-3913.334 -3913.334 -3913.334], eps: 0.01})
Step:   80700, Reward: [-677.697 -677.697 -677.697] [172.844], Avg: [-534.386 -534.386 -534.386] (0.1000) ({r_i: None, r_t: [-1275.362 -1275.362 -1275.362], eps: 0.1})
Step:   48300, Reward: [-1980.535 -1980.535 -1980.535] [212.855], Avg: [-852.524 -852.524 -852.524] (0.0100) ({r_i: None, r_t: [-3927.096 -3927.096 -3927.096], eps: 0.01})
Step:    4600, Reward: [-488.449 -488.449 -488.449] [105.450], Avg: [-485.478 -485.478 -485.478] (0.6306) ({r_i: None, r_t: [-942.531 -942.531 -942.531], eps: 0.631})
Step:   80800, Reward: [-654.504 -654.504 -654.504] [170.746], Avg: [-534.535 -534.535 -534.535] (0.1000) ({r_i: None, r_t: [-1255.371 -1255.371 -1255.371], eps: 0.1})
Step:   48400, Reward: [-2019.911 -2019.911 -2019.911] [264.813], Avg: [-854.931 -854.931 -854.931] (0.0100) ({r_i: None, r_t: [-3992.908 -3992.908 -3992.908], eps: 0.01})
Step:   80900, Reward: [-617.419 -617.419 -617.419] [148.567], Avg: [-534.637 -534.637 -534.637] (0.1000) ({r_i: None, r_t: [-1265.408 -1265.408 -1265.408], eps: 0.1})
Step:    4700, Reward: [-507.095 -507.095 -507.095] [82.225], Avg: [-485.929 -485.929 -485.929] (0.6243) ({r_i: None, r_t: [-883.900 -883.900 -883.900], eps: 0.624})
Step:   48500, Reward: [-1880.460 -1880.460 -1880.460] [237.688], Avg: [-857.041 -857.041 -857.041] (0.0100) ({r_i: None, r_t: [-3897.700 -3897.700 -3897.700], eps: 0.01})
Step:   81000, Reward: [-698.318 -698.318 -698.318] [230.412], Avg: [-534.839 -534.839 -534.839] (0.1000) ({r_i: None, r_t: [-1244.594 -1244.594 -1244.594], eps: 0.1})
Step:   48600, Reward: [-1907.777 -1907.777 -1907.777] [218.407], Avg: [-859.199 -859.199 -859.199] (0.0100) ({r_i: None, r_t: [-3913.474 -3913.474 -3913.474], eps: 0.01})
Step:    4800, Reward: [-519.730 -519.730 -519.730] [106.876], Avg: [-486.618 -486.618 -486.618] (0.6180) ({r_i: None, r_t: [-961.670 -961.670 -961.670], eps: 0.618})
Step:   81100, Reward: [-607.832 -607.832 -607.832] [182.050], Avg: [-534.929 -534.929 -534.929] (0.1000) ({r_i: None, r_t: [-1368.143 -1368.143 -1368.143], eps: 0.1})
Step:   48700, Reward: [-1922.857 -1922.857 -1922.857] [220.321], Avg: [-861.378 -861.378 -861.378] (0.0100) ({r_i: None, r_t: [-3721.461 -3721.461 -3721.461], eps: 0.01})
Step:   81200, Reward: [-737.640 -737.640 -737.640] [232.790], Avg: [-535.178 -535.178 -535.178] (0.1000) ({r_i: None, r_t: [-1256.212 -1256.212 -1256.212], eps: 0.1})
Step:    4900, Reward: [-496.777 -496.777 -496.777] [95.142], Avg: [-486.822 -486.822 -486.822] (0.6119) ({r_i: None, r_t: [-906.337 -906.337 -906.337], eps: 0.612})
Step:   48800, Reward: [-1848.910 -1848.910 -1848.910] [274.408], Avg: [-863.398 -863.398 -863.398] (0.0100) ({r_i: None, r_t: [-3905.984 -3905.984 -3905.984], eps: 0.01})
Step:   81300, Reward: [-603.542 -603.542 -603.542] [159.114], Avg: [-535.262 -535.262 -535.262] (0.1000) ({r_i: None, r_t: [-1343.905 -1343.905 -1343.905], eps: 0.1})
Step:   48900, Reward: [-1783.795 -1783.795 -1783.795] [227.152], Avg: [-865.276 -865.276 -865.276] (0.0100) ({r_i: None, r_t: [-3748.464 -3748.464 -3748.464], eps: 0.01})
Step:    5000, Reward: [-486.355 -486.355 -486.355] [71.589], Avg: [-486.812 -486.812 -486.812] (0.6058) ({r_i: None, r_t: [-1019.172 -1019.172 -1019.172], eps: 0.606})
Step:   81400, Reward: [-684.450 -684.450 -684.450] [185.517], Avg: [-535.445 -535.445 -535.445] (0.1000) ({r_i: None, r_t: [-1127.059 -1127.059 -1127.059], eps: 0.1})
Step:   49000, Reward: [-1937.879 -1937.879 -1937.879] [275.607], Avg: [-867.461 -867.461 -867.461] (0.0100) ({r_i: None, r_t: [-3636.542 -3636.542 -3636.542], eps: 0.01})
Step:   81500, Reward: [-731.706 -731.706 -731.706] [220.701], Avg: [-535.686 -535.686 -535.686] (0.1000) ({r_i: None, r_t: [-1362.528 -1362.528 -1362.528], eps: 0.1})
Step:    5100, Reward: [-475.933 -475.933 -475.933] [89.123], Avg: [-486.603 -486.603 -486.603] (0.5997) ({r_i: None, r_t: [-1034.763 -1034.763 -1034.763], eps: 0.6})
Step:   49100, Reward: [-1788.638 -1788.638 -1788.638] [231.477], Avg: [-869.333 -869.333 -869.333] (0.0100) ({r_i: None, r_t: [-3636.960 -3636.960 -3636.960], eps: 0.01})
Step:   81600, Reward: [-694.848 -694.848 -694.848] [211.306], Avg: [-535.880 -535.880 -535.880] (0.1000) ({r_i: None, r_t: [-1317.247 -1317.247 -1317.247], eps: 0.1})
Step:   49200, Reward: [-1947.757 -1947.757 -1947.757] [190.088], Avg: [-871.521 -871.521 -871.521] (0.0100) ({r_i: None, r_t: [-3778.207 -3778.207 -3778.207], eps: 0.01})
Step:    5200, Reward: [-511.408 -511.408 -511.408] [76.817], Avg: [-487.071 -487.071 -487.071] (0.5937) ({r_i: None, r_t: [-963.213 -963.213 -963.213], eps: 0.594})
Step:   81700, Reward: [-680.790 -680.790 -680.790] [199.789], Avg: [-536.057 -536.057 -536.057] (0.1000) ({r_i: None, r_t: [-1383.228 -1383.228 -1383.228], eps: 0.1})
Step:   49300, Reward: [-1879.213 -1879.213 -1879.213] [341.709], Avg: [-873.560 -873.560 -873.560] (0.0100) ({r_i: None, r_t: [-3688.109 -3688.109 -3688.109], eps: 0.01})
Step:   81800, Reward: [-875.010 -875.010 -875.010] [268.103], Avg: [-536.471 -536.471 -536.471] (0.1000) ({r_i: None, r_t: [-1323.428 -1323.428 -1323.428], eps: 0.1})
Step:    5300, Reward: [-465.276 -465.276 -465.276] [65.947], Avg: [-486.668 -486.668 -486.668] (0.5878) ({r_i: None, r_t: [-978.572 -978.572 -978.572], eps: 0.588})
Step:   49400, Reward: [-1817.630 -1817.630 -1817.630] [307.526], Avg: [-875.468 -875.468 -875.468] (0.0100) ({r_i: None, r_t: [-3711.700 -3711.700 -3711.700], eps: 0.01})
Step:   81900, Reward: [-698.508 -698.508 -698.508] [268.104], Avg: [-536.669 -536.669 -536.669] (0.1000) ({r_i: None, r_t: [-1368.113 -1368.113 -1368.113], eps: 0.1})
Step:   49500, Reward: [-1766.240 -1766.240 -1766.240] [254.208], Avg: [-877.264 -877.264 -877.264] (0.0100) ({r_i: None, r_t: [-3682.233 -3682.233 -3682.233], eps: 0.01})
Step:    5400, Reward: [-468.271 -468.271 -468.271] [100.173], Avg: [-486.333 -486.333 -486.333] (0.5820) ({r_i: None, r_t: [-932.249 -932.249 -932.249], eps: 0.582})
Step:   82000, Reward: [-670.725 -670.725 -670.725] [259.288], Avg: [-536.832 -536.832 -536.832] (0.1000) ({r_i: None, r_t: [-1418.026 -1418.026 -1418.026], eps: 0.1})
Step:   49600, Reward: [-1847.595 -1847.595 -1847.595] [263.175], Avg: [-879.216 -879.216 -879.216] (0.0100) ({r_i: None, r_t: [-3872.720 -3872.720 -3872.720], eps: 0.01})
Step:   82100, Reward: [-709.909 -709.909 -709.909] [148.010], Avg: [-537.043 -537.043 -537.043] (0.1000) ({r_i: None, r_t: [-1381.007 -1381.007 -1381.007], eps: 0.1})
Step:    5500, Reward: [-459.580 -459.580 -459.580] [72.169], Avg: [-485.855 -485.855 -485.855] (0.5762) ({r_i: None, r_t: [-906.473 -906.473 -906.473], eps: 0.576})
Step:   49700, Reward: [-1769.414 -1769.414 -1769.414] [263.814], Avg: [-881.004 -881.004 -881.004] (0.0100) ({r_i: None, r_t: [-3655.173 -3655.173 -3655.173], eps: 0.01})
Step:   82200, Reward: [-714.958 -714.958 -714.958] [214.470], Avg: [-537.259 -537.259 -537.259] (0.1000) ({r_i: None, r_t: [-1484.321 -1484.321 -1484.321], eps: 0.1})
Step:   49800, Reward: [-1754.450 -1754.450 -1754.450] [221.314], Avg: [-882.754 -882.754 -882.754] (0.0100) ({r_i: None, r_t: [-3424.136 -3424.136 -3424.136], eps: 0.01})
Step:    5600, Reward: [-463.050 -463.050 -463.050] [93.022], Avg: [-485.455 -485.455 -485.455] (0.5704) ({r_i: None, r_t: [-1033.693 -1033.693 -1033.693], eps: 0.57})
Step:   82300, Reward: [-670.875 -670.875 -670.875] [191.395], Avg: [-537.421 -537.421 -537.421] (0.1000) ({r_i: None, r_t: [-1539.775 -1539.775 -1539.775], eps: 0.1})
Step:   49900, Reward: [-1720.697 -1720.697 -1720.697] [364.644], Avg: [-884.430 -884.430 -884.430] (0.0100) ({r_i: None, r_t: [-3510.535 -3510.535 -3510.535], eps: 0.01})
Step:   82400, Reward: [-732.817 -732.817 -732.817] [248.534], Avg: [-537.658 -537.658 -537.658] (0.1000) ({r_i: None, r_t: [-1472.770 -1472.770 -1472.770], eps: 0.1})
Step:    5700, Reward: [-456.399 -456.399 -456.399] [70.567], Avg: [-484.954 -484.954 -484.954] (0.5647) ({r_i: None, r_t: [-962.567 -962.567 -962.567], eps: 0.565})
Step:   50000, Reward: [-1796.890 -1796.890 -1796.890] [301.324], Avg: [-886.251 -886.251 -886.251] (0.0100) ({r_i: None, r_t: [-3260.344 -3260.344 -3260.344], eps: 0.01})
Step:   82500, Reward: [-688.501 -688.501 -688.501] [120.858], Avg: [-537.841 -537.841 -537.841] (0.1000) ({r_i: None, r_t: [-1292.910 -1292.910 -1292.910], eps: 0.1})
Step:    5800, Reward: [-517.371 -517.371 -517.371] [105.966], Avg: [-485.504 -485.504 -485.504] (0.5591) ({r_i: None, r_t: [-992.778 -992.778 -992.778], eps: 0.559})
Step:   50100, Reward: [-1571.977 -1571.977 -1571.977] [313.112], Avg: [-887.617 -887.617 -887.617] (0.0100) ({r_i: None, r_t: [-3241.188 -3241.188 -3241.188], eps: 0.01})
Step:   82600, Reward: [-774.157 -774.157 -774.157] [255.442], Avg: [-538.126 -538.126 -538.126] (0.1000) ({r_i: None, r_t: [-1508.904 -1508.904 -1508.904], eps: 0.1})
Step:   50200, Reward: [-1636.429 -1636.429 -1636.429] [289.908], Avg: [-889.106 -889.106 -889.106] (0.0100) ({r_i: None, r_t: [-3387.152 -3387.152 -3387.152], eps: 0.01})
Step:   82700, Reward: [-697.089 -697.089 -697.089] [180.825], Avg: [-538.318 -538.318 -538.318] (0.1000) ({r_i: None, r_t: [-1433.819 -1433.819 -1433.819], eps: 0.1})
Step:    5900, Reward: [-496.675 -496.675 -496.675] [102.691], Avg: [-485.690 -485.690 -485.690] (0.5535) ({r_i: None, r_t: [-967.601 -967.601 -967.601], eps: 0.554})
Step:   82800, Reward: [-781.214 -781.214 -781.214] [221.914], Avg: [-538.611 -538.611 -538.611] (0.1000) ({r_i: None, r_t: [-1429.131 -1429.131 -1429.131], eps: 0.1})
Step:   50300, Reward: [-1586.799 -1586.799 -1586.799] [304.959], Avg: [-890.490 -890.490 -890.490] (0.0100) ({r_i: None, r_t: [-3134.953 -3134.953 -3134.953], eps: 0.01})
Step:   82900, Reward: [-636.081 -636.081 -636.081] [161.434], Avg: [-538.729 -538.729 -538.729] (0.1000) ({r_i: None, r_t: [-1398.059 -1398.059 -1398.059], eps: 0.1})
Step:    6000, Reward: [-429.862 -429.862 -429.862] [55.984], Avg: [-484.775 -484.775 -484.775] (0.5480) ({r_i: None, r_t: [-975.638 -975.638 -975.638], eps: 0.548})
Step:   50400, Reward: [-1626.271 -1626.271 -1626.271] [235.634], Avg: [-891.947 -891.947 -891.947] (0.0100) ({r_i: None, r_t: [-3033.184 -3033.184 -3033.184], eps: 0.01})
Step:   83000, Reward: [-759.127 -759.127 -759.127] [188.587], Avg: [-538.994 -538.994 -538.994] (0.1000) ({r_i: None, r_t: [-1463.266 -1463.266 -1463.266], eps: 0.1})
Step:   50500, Reward: [-1532.798 -1532.798 -1532.798] [232.448], Avg: [-893.214 -893.214 -893.214] (0.0100) ({r_i: None, r_t: [-3041.962 -3041.962 -3041.962], eps: 0.01})
Step:    6100, Reward: [-450.806 -450.806 -450.806] [34.697], Avg: [-484.227 -484.227 -484.227] (0.5425) ({r_i: None, r_t: [-928.536 -928.536 -928.536], eps: 0.543})
Step:   83100, Reward: [-818.391 -818.391 -818.391] [285.297], Avg: [-539.330 -539.330 -539.330] (0.1000) ({r_i: None, r_t: [-1511.022 -1511.022 -1511.022], eps: 0.1})
Step:   50600, Reward: [-1575.825 -1575.825 -1575.825] [299.471], Avg: [-894.560 -894.560 -894.560] (0.0100) ({r_i: None, r_t: [-2951.351 -2951.351 -2951.351], eps: 0.01})
Step:   83200, Reward: [-756.208 -756.208 -756.208] [189.483], Avg: [-539.590 -539.590 -539.590] (0.1000) ({r_i: None, r_t: [-1464.564 -1464.564 -1464.564], eps: 0.1})
Step:    6200, Reward: [-455.809 -455.809 -455.809] [110.409], Avg: [-483.776 -483.776 -483.776] (0.5371) ({r_i: None, r_t: [-931.902 -931.902 -931.902], eps: 0.537})
Step:   50700, Reward: [-1452.064 -1452.064 -1452.064] [193.620], Avg: [-895.657 -895.657 -895.657] (0.0100) ({r_i: None, r_t: [-2898.511 -2898.511 -2898.511], eps: 0.01})
Step:   83300, Reward: [-748.886 -748.886 -748.886] [200.618], Avg: [-539.841 -539.841 -539.841] (0.1000) ({r_i: None, r_t: [-1462.502 -1462.502 -1462.502], eps: 0.1})
Step:   50800, Reward: [-1501.188 -1501.188 -1501.188] [268.685], Avg: [-896.847 -896.847 -896.847] (0.0100) ({r_i: None, r_t: [-3043.815 -3043.815 -3043.815], eps: 0.01})
Step:    6300, Reward: [-478.078 -478.078 -478.078] [66.309], Avg: [-483.687 -483.687 -483.687] (0.5318) ({r_i: None, r_t: [-947.688 -947.688 -947.688], eps: 0.532})
Step:   83400, Reward: [-661.308 -661.308 -661.308] [181.226], Avg: [-539.987 -539.987 -539.987] (0.1000) ({r_i: None, r_t: [-1457.893 -1457.893 -1457.893], eps: 0.1})
Step:   50900, Reward: [-1533.830 -1533.830 -1533.830] [122.731], Avg: [-898.096 -898.096 -898.096] (0.0100) ({r_i: None, r_t: [-2991.449 -2991.449 -2991.449], eps: 0.01})
Step:   83500, Reward: [-732.308 -732.308 -732.308] [313.859], Avg: [-540.217 -540.217 -540.217] (0.1000) ({r_i: None, r_t: [-1389.450 -1389.450 -1389.450], eps: 0.1})
Step:    6400, Reward: [-465.763 -465.763 -465.763] [79.633], Avg: [-483.411 -483.411 -483.411] (0.5264) ({r_i: None, r_t: [-915.990 -915.990 -915.990], eps: 0.526})
Step:   51000, Reward: [-1411.116 -1411.116 -1411.116] [234.652], Avg: [-899.100 -899.100 -899.100] (0.0100) ({r_i: None, r_t: [-2958.776 -2958.776 -2958.776], eps: 0.01})
Step:   83600, Reward: [-747.734 -747.734 -747.734] [173.910], Avg: [-540.465 -540.465 -540.465] (0.1000) ({r_i: None, r_t: [-1619.408 -1619.408 -1619.408], eps: 0.1})
Step:   51100, Reward: [-1477.325 -1477.325 -1477.325] [263.897], Avg: [-900.229 -900.229 -900.229] (0.0100) ({r_i: None, r_t: [-2731.376 -2731.376 -2731.376], eps: 0.01})
Step:    6500, Reward: [-444.752 -444.752 -444.752] [58.225], Avg: [-482.825 -482.825 -482.825] (0.5212) ({r_i: None, r_t: [-862.713 -862.713 -862.713], eps: 0.521})
Step:   83700, Reward: [-761.827 -761.827 -761.827] [217.555], Avg: [-540.729 -540.729 -540.729] (0.1000) ({r_i: None, r_t: [-1741.464 -1741.464 -1741.464], eps: 0.1})
Step:   51200, Reward: [-1357.736 -1357.736 -1357.736] [204.967], Avg: [-901.121 -901.121 -901.121] (0.0100) ({r_i: None, r_t: [-2950.718 -2950.718 -2950.718], eps: 0.01})
Step:   83800, Reward: [-763.200 -763.200 -763.200] [307.600], Avg: [-540.994 -540.994 -540.994] (0.1000) ({r_i: None, r_t: [-1586.677 -1586.677 -1586.677], eps: 0.1})
Step:    6600, Reward: [-445.730 -445.730 -445.730] [64.779], Avg: [-482.272 -482.272 -482.272] (0.5160) ({r_i: None, r_t: [-924.664 -924.664 -924.664], eps: 0.516})
Step:   51300, Reward: [-1360.580 -1360.580 -1360.580] [234.598], Avg: [-902.015 -902.015 -902.015] (0.0100) ({r_i: None, r_t: [-2833.147 -2833.147 -2833.147], eps: 0.01})
Step:   83900, Reward: [-726.614 -726.614 -726.614] [149.309], Avg: [-541.215 -541.215 -541.215] (0.1000) ({r_i: None, r_t: [-1614.980 -1614.980 -1614.980], eps: 0.1})
Step:   51400, Reward: [-1378.997 -1378.997 -1378.997] [238.943], Avg: [-902.941 -902.941 -902.941] (0.0100) ({r_i: None, r_t: [-2910.441 -2910.441 -2910.441], eps: 0.01})
Step:    6700, Reward: [-479.367 -479.367 -479.367] [88.537], Avg: [-482.229 -482.229 -482.229] (0.5108) ({r_i: None, r_t: [-977.007 -977.007 -977.007], eps: 0.511})
Step:   84000, Reward: [-804.277 -804.277 -804.277] [179.415], Avg: [-541.528 -541.528 -541.528] (0.1000) ({r_i: None, r_t: [-1527.832 -1527.832 -1527.832], eps: 0.1})
Step:   51500, Reward: [-1296.249 -1296.249 -1296.249] [151.839], Avg: [-903.703 -903.703 -903.703] (0.0100) ({r_i: None, r_t: [-2892.186 -2892.186 -2892.186], eps: 0.01})
Step:   84100, Reward: [-806.799 -806.799 -806.799] [211.470], Avg: [-541.843 -541.843 -541.843] (0.1000) ({r_i: None, r_t: [-1567.382 -1567.382 -1567.382], eps: 0.1})
Step:    6800, Reward: [-473.722 -473.722 -473.722] [90.456], Avg: [-482.106 -482.106 -482.106] (0.5058) ({r_i: None, r_t: [-922.154 -922.154 -922.154], eps: 0.506})
Step:   51600, Reward: [-1475.137 -1475.137 -1475.137] [155.858], Avg: [-904.809 -904.809 -904.809] (0.0100) ({r_i: None, r_t: [-2697.054 -2697.054 -2697.054], eps: 0.01})
Step:   84200, Reward: [-800.603 -800.603 -800.603] [214.498], Avg: [-542.150 -542.150 -542.150] (0.1000) ({r_i: None, r_t: [-1786.260 -1786.260 -1786.260], eps: 0.1})
Step:   51700, Reward: [-1532.597 -1532.597 -1532.597] [211.748], Avg: [-906.021 -906.021 -906.021] (0.0100) ({r_i: None, r_t: [-2755.039 -2755.039 -2755.039], eps: 0.01})
Step:    6900, Reward: [-454.007 -454.007 -454.007] [69.788], Avg: [-481.704 -481.704 -481.704] (0.5007) ({r_i: None, r_t: [-965.459 -965.459 -965.459], eps: 0.501})
Step:   84300, Reward: [-814.742 -814.742 -814.742] [197.400], Avg: [-542.473 -542.473 -542.473] (0.1000) ({r_i: None, r_t: [-1784.928 -1784.928 -1784.928], eps: 0.1})
Step:   51800, Reward: [-1317.002 -1317.002 -1317.002] [212.502], Avg: [-906.813 -906.813 -906.813] (0.0100) ({r_i: None, r_t: [-2756.179 -2756.179 -2756.179], eps: 0.01})
Step:   84400, Reward: [-801.543 -801.543 -801.543] [212.128], Avg: [-542.779 -542.779 -542.779] (0.1000) ({r_i: None, r_t: [-1640.616 -1640.616 -1640.616], eps: 0.1})
Step:    7000, Reward: [-503.513 -503.513 -503.513] [125.380], Avg: [-482.011 -482.011 -482.011] (0.4957) ({r_i: None, r_t: [-945.512 -945.512 -945.512], eps: 0.496})
Step:   51900, Reward: [-1440.290 -1440.290 -1440.290] [216.767], Avg: [-907.838 -907.838 -907.838] (0.0100) ({r_i: None, r_t: [-2709.582 -2709.582 -2709.582], eps: 0.01})
Step:   84500, Reward: [-961.548 -961.548 -961.548] [242.452], Avg: [-543.274 -543.274 -543.274] (0.1000) ({r_i: None, r_t: [-1481.823 -1481.823 -1481.823], eps: 0.1})
Step:   52000, Reward: [-1438.309 -1438.309 -1438.309] [269.159], Avg: [-908.857 -908.857 -908.857] (0.0100) ({r_i: None, r_t: [-2769.248 -2769.248 -2769.248], eps: 0.01})
Step:    7100, Reward: [-448.105 -448.105 -448.105] [71.066], Avg: [-481.540 -481.540 -481.540] (0.4908) ({r_i: None, r_t: [-937.957 -937.957 -937.957], eps: 0.491})
Step:   84600, Reward: [-888.216 -888.216 -888.216] [240.066], Avg: [-543.681 -543.681 -543.681] (0.1000) ({r_i: None, r_t: [-1730.848 -1730.848 -1730.848], eps: 0.1})
Step:   52100, Reward: [-1361.688 -1361.688 -1361.688] [177.377], Avg: [-909.724 -909.724 -909.724] (0.0100) ({r_i: None, r_t: [-2781.447 -2781.447 -2781.447], eps: 0.01})
Step:   84700, Reward: [-840.668 -840.668 -840.668] [277.079], Avg: [-544.032 -544.032 -544.032] (0.1000) ({r_i: None, r_t: [-1538.501 -1538.501 -1538.501], eps: 0.1})
Step:    7200, Reward: [-475.076 -475.076 -475.076] [109.481], Avg: [-481.452 -481.452 -481.452] (0.4859) ({r_i: None, r_t: [-920.033 -920.033 -920.033], eps: 0.486})
Step:   52200, Reward: [-1424.878 -1424.878 -1424.878] [195.557], Avg: [-910.709 -910.709 -910.709] (0.0100) ({r_i: None, r_t: [-2627.908 -2627.908 -2627.908], eps: 0.01})
Step:   84800, Reward: [-855.235 -855.235 -855.235] [311.069], Avg: [-544.398 -544.398 -544.398] (0.1000) ({r_i: None, r_t: [-1732.125 -1732.125 -1732.125], eps: 0.1})
Step:    7300, Reward: [-464.014 -464.014 -464.014] [98.920], Avg: [-481.216 -481.216 -481.216] (0.4810) ({r_i: None, r_t: [-956.566 -956.566 -956.566], eps: 0.481})
Step:   52300, Reward: [-1411.877 -1411.877 -1411.877] [189.121], Avg: [-911.666 -911.666 -911.666] (0.0100) ({r_i: None, r_t: [-2738.556 -2738.556 -2738.556], eps: 0.01})
Step:   84900, Reward: [-855.840 -855.840 -855.840] [162.683], Avg: [-544.765 -544.765 -544.765] (0.1000) ({r_i: None, r_t: [-1721.085 -1721.085 -1721.085], eps: 0.1})
Step:   52400, Reward: [-1568.412 -1568.412 -1568.412] [243.192], Avg: [-912.917 -912.917 -912.917] (0.0100) ({r_i: None, r_t: [-2904.689 -2904.689 -2904.689], eps: 0.01})
Step:   85000, Reward: [-937.094 -937.094 -937.094] [335.739], Avg: [-545.226 -545.226 -545.226] (0.1000) ({r_i: None, r_t: [-1845.404 -1845.404 -1845.404], eps: 0.1})
Step:    7400, Reward: [-457.383 -457.383 -457.383] [118.819], Avg: [-480.898 -480.898 -480.898] (0.4762) ({r_i: None, r_t: [-925.940 -925.940 -925.940], eps: 0.476})
Step:   52500, Reward: [-1381.778 -1381.778 -1381.778] [149.015], Avg: [-913.808 -913.808 -913.808] (0.0100) ({r_i: None, r_t: [-2825.401 -2825.401 -2825.401], eps: 0.01})
Step:   85100, Reward: [-861.450 -861.450 -861.450] [206.067], Avg: [-545.597 -545.597 -545.597] (0.1000) ({r_i: None, r_t: [-1621.281 -1621.281 -1621.281], eps: 0.1})
Step:    7500, Reward: [-450.265 -450.265 -450.265] [84.198], Avg: [-480.495 -480.495 -480.495] (0.4715) ({r_i: None, r_t: [-948.816 -948.816 -948.816], eps: 0.471})
Step:   52600, Reward: [-1445.476 -1445.476 -1445.476] [179.450], Avg: [-914.817 -914.817 -914.817] (0.0100) ({r_i: None, r_t: [-2784.424 -2784.424 -2784.424], eps: 0.01})
Step:   85200, Reward: [-841.918 -841.918 -841.918] [322.628], Avg: [-545.944 -545.944 -545.944] (0.1000) ({r_i: None, r_t: [-1641.766 -1641.766 -1641.766], eps: 0.1})
Step:   85300, Reward: [-787.919 -787.919 -787.919] [228.250], Avg: [-546.228 -546.228 -546.228] (0.1000) ({r_i: None, r_t: [-1616.111 -1616.111 -1616.111], eps: 0.1})
Step:   52700, Reward: [-1622.240 -1622.240 -1622.240] [202.116], Avg: [-916.157 -916.157 -916.157] (0.0100) ({r_i: None, r_t: [-2847.931 -2847.931 -2847.931], eps: 0.01})
Step:    7600, Reward: [-461.859 -461.859 -461.859] [77.549], Avg: [-480.253 -480.253 -480.253] (0.4668) ({r_i: None, r_t: [-890.456 -890.456 -890.456], eps: 0.467})
Step:   85400, Reward: [-848.029 -848.029 -848.029] [239.904], Avg: [-546.581 -546.581 -546.581] (0.1000) ({r_i: None, r_t: [-1684.034 -1684.034 -1684.034], eps: 0.1})
Step:   52800, Reward: [-1505.185 -1505.185 -1505.185] [237.169], Avg: [-917.270 -917.270 -917.270] (0.0100) ({r_i: None, r_t: [-3131.951 -3131.951 -3131.951], eps: 0.01})
Step:    7700, Reward: [-426.257 -426.257 -426.257] [71.225], Avg: [-479.561 -479.561 -479.561] (0.4621) ({r_i: None, r_t: [-969.017 -969.017 -969.017], eps: 0.462})
Step:   85500, Reward: [-1012.413 -1012.413 -1012.413] [279.761], Avg: [-547.125 -547.125 -547.125] (0.1000) ({r_i: None, r_t: [-1567.528 -1567.528 -1567.528], eps: 0.1})
Step:   52900, Reward: [-1535.257 -1535.257 -1535.257] [204.944], Avg: [-918.436 -918.436 -918.436] (0.0100) ({r_i: None, r_t: [-2986.547 -2986.547 -2986.547], eps: 0.01})
Step:   85600, Reward: [-884.890 -884.890 -884.890] [296.574], Avg: [-547.519 -547.519 -547.519] (0.1000) ({r_i: None, r_t: [-1752.052 -1752.052 -1752.052], eps: 0.1})
Step:   53000, Reward: [-1662.898 -1662.898 -1662.898] [252.038], Avg: [-919.838 -919.838 -919.838] (0.0100) ({r_i: None, r_t: [-3260.592 -3260.592 -3260.592], eps: 0.01})
Step:    7800, Reward: [-445.006 -445.006 -445.006] [53.605], Avg: [-479.124 -479.124 -479.124] (0.4575) ({r_i: None, r_t: [-895.144 -895.144 -895.144], eps: 0.458})
Step:   85700, Reward: [-868.471 -868.471 -868.471] [269.242], Avg: [-547.893 -547.893 -547.893] (0.1000) ({r_i: None, r_t: [-1789.611 -1789.611 -1789.611], eps: 0.1})
Step:   53100, Reward: [-1714.799 -1714.799 -1714.799] [282.975], Avg: [-921.332 -921.332 -921.332] (0.0100) ({r_i: None, r_t: [-3252.448 -3252.448 -3252.448], eps: 0.01})
Step:   85800, Reward: [-858.769 -858.769 -858.769] [224.243], Avg: [-548.255 -548.255 -548.255] (0.1000) ({r_i: None, r_t: [-1852.234 -1852.234 -1852.234], eps: 0.1})
Step:    7900, Reward: [-419.114 -419.114 -419.114] [63.082], Avg: [-478.374 -478.374 -478.374] (0.4529) ({r_i: None, r_t: [-893.774 -893.774 -893.774], eps: 0.453})
Step:   53200, Reward: [-1791.517 -1791.517 -1791.517] [270.871], Avg: [-922.965 -922.965 -922.965] (0.0100) ({r_i: None, r_t: [-3293.281 -3293.281 -3293.281], eps: 0.01})
Step:   85900, Reward: [-1039.414 -1039.414 -1039.414] [300.626], Avg: [-548.826 -548.826 -548.826] (0.1000) ({r_i: None, r_t: [-1906.146 -1906.146 -1906.146], eps: 0.1})
Step:   53300, Reward: [-1684.138 -1684.138 -1684.138] [246.294], Avg: [-924.390 -924.390 -924.390] (0.0100) ({r_i: None, r_t: [-3364.278 -3364.278 -3364.278], eps: 0.01})
Step:    8000, Reward: [-455.305 -455.305 -455.305] [75.729], Avg: [-478.089 -478.089 -478.089] (0.4484) ({r_i: None, r_t: [-941.346 -941.346 -941.346], eps: 0.448})
Step:   86000, Reward: [-800.696 -800.696 -800.696] [228.900], Avg: [-549.118 -549.118 -549.118] (0.1000) ({r_i: None, r_t: [-1796.621 -1796.621 -1796.621], eps: 0.1})
Step:   53400, Reward: [-1793.554 -1793.554 -1793.554] [199.978], Avg: [-926.015 -926.015 -926.015] (0.0100) ({r_i: None, r_t: [-3506.515 -3506.515 -3506.515], eps: 0.01})
Step:   86100, Reward: [-1047.348 -1047.348 -1047.348] [357.189], Avg: [-549.696 -549.696 -549.696] (0.1000) ({r_i: None, r_t: [-1956.978 -1956.978 -1956.978], eps: 0.1})
Step:    8100, Reward: [-428.697 -428.697 -428.697] [66.178], Avg: [-477.486 -477.486 -477.486] (0.4440) ({r_i: None, r_t: [-930.369 -930.369 -930.369], eps: 0.444})
Step:   53500, Reward: [-1801.323 -1801.323 -1801.323] [174.926], Avg: [-927.648 -927.648 -927.648] (0.0100) ({r_i: None, r_t: [-3600.490 -3600.490 -3600.490], eps: 0.01})
Step:   86200, Reward: [-962.714 -962.714 -962.714] [362.484], Avg: [-550.175 -550.175 -550.175] (0.1000) ({r_i: None, r_t: [-1850.324 -1850.324 -1850.324], eps: 0.1})
Step:   53600, Reward: [-1816.131 -1816.131 -1816.131] [245.021], Avg: [-929.303 -929.303 -929.303] (0.0100) ({r_i: None, r_t: [-3790.236 -3790.236 -3790.236], eps: 0.01})
Step:    8200, Reward: [-459.816 -459.816 -459.816] [71.159], Avg: [-477.274 -477.274 -477.274] (0.4395) ({r_i: None, r_t: [-976.044 -976.044 -976.044], eps: 0.44})
Step:   86300, Reward: [-836.166 -836.166 -836.166] [312.419], Avg: [-550.506 -550.506 -550.506] (0.1000) ({r_i: None, r_t: [-1905.465 -1905.465 -1905.465], eps: 0.1})
Step:   53700, Reward: [-1914.633 -1914.633 -1914.633] [306.624], Avg: [-931.134 -931.134 -931.134] (0.0100) ({r_i: None, r_t: [-3693.376 -3693.376 -3693.376], eps: 0.01})
Step:   86400, Reward: [-941.917 -941.917 -941.917] [278.021], Avg: [-550.959 -550.959 -550.959] (0.1000) ({r_i: None, r_t: [-1989.750 -1989.750 -1989.750], eps: 0.1})
Step:    8300, Reward: [-447.580 -447.580 -447.580] [59.963], Avg: [-476.920 -476.920 -476.920] (0.4351) ({r_i: None, r_t: [-916.360 -916.360 -916.360], eps: 0.435})
Step:   53800, Reward: [-1963.310 -1963.310 -1963.310] [143.629], Avg: [-933.049 -933.049 -933.049] (0.0100) ({r_i: None, r_t: [-3742.705 -3742.705 -3742.705], eps: 0.01})
Step:   86500, Reward: [-926.362 -926.362 -926.362] [220.143], Avg: [-551.392 -551.392 -551.392] (0.1000) ({r_i: None, r_t: [-1848.380 -1848.380 -1848.380], eps: 0.1})
Step:   53900, Reward: [-1862.645 -1862.645 -1862.645] [223.993], Avg: [-934.770 -934.770 -934.770] (0.0100) ({r_i: None, r_t: [-3860.400 -3860.400 -3860.400], eps: 0.01})
Step:    8400, Reward: [-441.554 -441.554 -441.554] [46.651], Avg: [-476.504 -476.504 -476.504] (0.4308) ({r_i: None, r_t: [-908.430 -908.430 -908.430], eps: 0.431})
Step:   86600, Reward: [-1013.827 -1013.827 -1013.827] [408.025], Avg: [-551.925 -551.925 -551.925] (0.1000) ({r_i: None, r_t: [-2217.019 -2217.019 -2217.019], eps: 0.1})
Step:   54000, Reward: [-1860.609 -1860.609 -1860.609] [232.229], Avg: [-936.482 -936.482 -936.482] (0.0100) ({r_i: None, r_t: [-3750.697 -3750.697 -3750.697], eps: 0.01})
Step:   86700, Reward: [-957.983 -957.983 -957.983] [178.315], Avg: [-552.393 -552.393 -552.393] (0.1000) ({r_i: None, r_t: [-1825.976 -1825.976 -1825.976], eps: 0.1})
Step:    8500, Reward: [-468.410 -468.410 -468.410] [71.030], Avg: [-476.410 -476.410 -476.410] (0.4265) ({r_i: None, r_t: [-876.231 -876.231 -876.231], eps: 0.427})
Step:   54100, Reward: [-1934.645 -1934.645 -1934.645] [160.868], Avg: [-938.323 -938.323 -938.323] (0.0100) ({r_i: None, r_t: [-3998.103 -3998.103 -3998.103], eps: 0.01})
Step:   86800, Reward: [-1168.462 -1168.462 -1168.462] [453.339], Avg: [-553.102 -553.102 -553.102] (0.1000) ({r_i: None, r_t: [-1944.872 -1944.872 -1944.872], eps: 0.1})
Step:   54200, Reward: [-1976.621 -1976.621 -1976.621] [181.348], Avg: [-940.236 -940.236 -940.236] (0.0100) ({r_i: None, r_t: [-3912.204 -3912.204 -3912.204], eps: 0.01})
Step:    8600, Reward: [-454.339 -454.339 -454.339] [67.822], Avg: [-476.156 -476.156 -476.156] (0.4223) ({r_i: None, r_t: [-906.277 -906.277 -906.277], eps: 0.422})
Step:   86900, Reward: [-1058.130 -1058.130 -1058.130] [253.702], Avg: [-553.683 -553.683 -553.683] (0.1000) ({r_i: None, r_t: [-1893.415 -1893.415 -1893.415], eps: 0.1})
Step:   54300, Reward: [-2088.634 -2088.634 -2088.634] [231.559], Avg: [-942.347 -942.347 -942.347] (0.0100) ({r_i: None, r_t: [-4007.418 -4007.418 -4007.418], eps: 0.01})
Step:   87000, Reward: [-1063.515 -1063.515 -1063.515] [362.752], Avg: [-554.268 -554.268 -554.268] (0.1000) ({r_i: None, r_t: [-1857.894 -1857.894 -1857.894], eps: 0.1})
Step:    8700, Reward: [-460.526 -460.526 -460.526] [74.725], Avg: [-475.979 -475.979 -475.979] (0.4180) ({r_i: None, r_t: [-924.265 -924.265 -924.265], eps: 0.418})
Step:   54400, Reward: [-1937.895 -1937.895 -1937.895] [88.554], Avg: [-944.173 -944.173 -944.173] (0.0100) ({r_i: None, r_t: [-3753.683 -3753.683 -3753.683], eps: 0.01})
Step:   87100, Reward: [-947.977 -947.977 -947.977] [302.853], Avg: [-554.719 -554.719 -554.719] (0.1000) ({r_i: None, r_t: [-2093.828 -2093.828 -2093.828], eps: 0.1})
Step:   54500, Reward: [-2036.420 -2036.420 -2036.420] [186.035], Avg: [-946.174 -946.174 -946.174] (0.0100) ({r_i: None, r_t: [-4073.310 -4073.310 -4073.310], eps: 0.01})
Step:    8800, Reward: [-480.386 -480.386 -480.386] [70.706], Avg: [-476.028 -476.028 -476.028] (0.4139) ({r_i: None, r_t: [-909.503 -909.503 -909.503], eps: 0.414})
Step:   87200, Reward: [-983.330 -983.330 -983.330] [337.026], Avg: [-555.210 -555.210 -555.210] (0.1000) ({r_i: None, r_t: [-1978.413 -1978.413 -1978.413], eps: 0.1})
Step:   54600, Reward: [-1952.901 -1952.901 -1952.901] [154.795], Avg: [-948.014 -948.014 -948.014] (0.0100) ({r_i: None, r_t: [-3980.255 -3980.255 -3980.255], eps: 0.01})
Step:   87300, Reward: [-843.541 -843.541 -843.541] [319.996], Avg: [-555.540 -555.540 -555.540] (0.1000) ({r_i: None, r_t: [-1916.810 -1916.810 -1916.810], eps: 0.1})
Step:    8900, Reward: [-447.792 -447.792 -447.792] [80.738], Avg: [-475.714 -475.714 -475.714] (0.4097) ({r_i: None, r_t: [-873.877 -873.877 -873.877], eps: 0.41})
Step:   54700, Reward: [-1931.047 -1931.047 -1931.047] [248.425], Avg: [-949.808 -949.808 -949.808] (0.0100) ({r_i: None, r_t: [-3924.012 -3924.012 -3924.012], eps: 0.01})
Step:   87400, Reward: [-1103.494 -1103.494 -1103.494] [398.822], Avg: [-556.167 -556.167 -556.167] (0.1000) ({r_i: None, r_t: [-2164.920 -2164.920 -2164.920], eps: 0.1})
Step:    9000, Reward: [-433.414 -433.414 -433.414] [48.672], Avg: [-475.250 -475.250 -475.250] (0.4057) ({r_i: None, r_t: [-911.533 -911.533 -911.533], eps: 0.406})
Step:   54800, Reward: [-2097.455 -2097.455 -2097.455] [252.244], Avg: [-951.899 -951.899 -951.899] (0.0100) ({r_i: None, r_t: [-4210.783 -4210.783 -4210.783], eps: 0.01})
Step:   87500, Reward: [-916.196 -916.196 -916.196] [319.406], Avg: [-556.578 -556.578 -556.578] (0.1000) ({r_i: None, r_t: [-2085.059 -2085.059 -2085.059], eps: 0.1})
Step:   54900, Reward: [-1914.217 -1914.217 -1914.217] [234.823], Avg: [-953.648 -953.648 -953.648] (0.0100) ({r_i: None, r_t: [-3923.454 -3923.454 -3923.454], eps: 0.01})
Step:   87600, Reward: [-982.787 -982.787 -982.787] [243.355], Avg: [-557.064 -557.064 -557.064] (0.1000) ({r_i: None, r_t: [-1887.434 -1887.434 -1887.434], eps: 0.1})
Step:    9100, Reward: [-436.636 -436.636 -436.636] [57.135], Avg: [-474.830 -474.830 -474.830] (0.4016) ({r_i: None, r_t: [-899.861 -899.861 -899.861], eps: 0.402})
Step:   55000, Reward: [-2020.225 -2020.225 -2020.225] [178.806], Avg: [-955.584 -955.584 -955.584] (0.0100) ({r_i: None, r_t: [-3974.976 -3974.976 -3974.976], eps: 0.01})
Step:   87700, Reward: [-1189.871 -1189.871 -1189.871] [389.016], Avg: [-557.784 -557.784 -557.784] (0.1000) ({r_i: None, r_t: [-1914.984 -1914.984 -1914.984], eps: 0.1})
Step:    9200, Reward: [-493.112 -493.112 -493.112] [90.064], Avg: [-475.026 -475.026 -475.026] (0.3976) ({r_i: None, r_t: [-896.660 -896.660 -896.660], eps: 0.398})
Step:   87800, Reward: [-1072.652 -1072.652 -1072.652] [357.826], Avg: [-558.370 -558.370 -558.370] (0.1000) ({r_i: None, r_t: [-2026.868 -2026.868 -2026.868], eps: 0.1})
Step:   55100, Reward: [-1939.234 -1939.234 -1939.234] [201.352], Avg: [-957.366 -957.366 -957.366] (0.0100) ({r_i: None, r_t: [-3898.594 -3898.594 -3898.594], eps: 0.01})
Step:   87900, Reward: [-1112.814 -1112.814 -1112.814] [368.177], Avg: [-559.000 -559.000 -559.000] (0.1000) ({r_i: None, r_t: [-1940.306 -1940.306 -1940.306], eps: 0.1})
Step:   55200, Reward: [-2057.978 -2057.978 -2057.978] [224.351], Avg: [-959.356 -959.356 -959.356] (0.0100) ({r_i: None, r_t: [-4051.134 -4051.134 -4051.134], eps: 0.01})
Step:    9300, Reward: [-435.440 -435.440 -435.440] [50.009], Avg: [-474.605 -474.605 -474.605] (0.3936) ({r_i: None, r_t: [-900.034 -900.034 -900.034], eps: 0.394})
Step:   88000, Reward: [-1123.295 -1123.295 -1123.295] [267.133], Avg: [-559.641 -559.641 -559.641] (0.1000) ({r_i: None, r_t: [-2059.987 -2059.987 -2059.987], eps: 0.1})
Step:   55300, Reward: [-1908.498 -1908.498 -1908.498] [128.451], Avg: [-961.069 -961.069 -961.069] (0.0100) ({r_i: None, r_t: [-3893.655 -3893.655 -3893.655], eps: 0.01})
Step:   88100, Reward: [-1125.433 -1125.433 -1125.433] [315.039], Avg: [-560.282 -560.282 -560.282] (0.1000) ({r_i: None, r_t: [-2143.239 -2143.239 -2143.239], eps: 0.1})
Step:    9400, Reward: [-423.862 -423.862 -423.862] [55.686], Avg: [-474.071 -474.071 -474.071] (0.3897) ({r_i: None, r_t: [-874.589 -874.589 -874.589], eps: 0.39})
Step:   55400, Reward: [-2047.745 -2047.745 -2047.745] [211.041], Avg: [-963.027 -963.027 -963.027] (0.0100) ({r_i: None, r_t: [-4024.594 -4024.594 -4024.594], eps: 0.01})
Step:   88200, Reward: [-999.186 -999.186 -999.186] [213.850], Avg: [-560.779 -560.779 -560.779] (0.1000) ({r_i: None, r_t: [-2083.163 -2083.163 -2083.163], eps: 0.1})
Step:   55500, Reward: [-1998.101 -1998.101 -1998.101] [238.950], Avg: [-964.889 -964.889 -964.889] (0.0100) ({r_i: None, r_t: [-3930.754 -3930.754 -3930.754], eps: 0.01})
Step:    9500, Reward: [-453.703 -453.703 -453.703] [96.247], Avg: [-473.859 -473.859 -473.859] (0.3858) ({r_i: None, r_t: [-891.773 -891.773 -891.773], eps: 0.386})
Step:   88300, Reward: [-1108.207 -1108.207 -1108.207] [318.805], Avg: [-561.398 -561.398 -561.398] (0.1000) ({r_i: None, r_t: [-2128.799 -2128.799 -2128.799], eps: 0.1})
Step:   55600, Reward: [-1901.025 -1901.025 -1901.025] [172.319], Avg: [-966.570 -966.570 -966.570] (0.0100) ({r_i: None, r_t: [-4015.080 -4015.080 -4015.080], eps: 0.01})
Step:   88400, Reward: [-985.439 -985.439 -985.439] [264.746], Avg: [-561.878 -561.878 -561.878] (0.1000) ({r_i: None, r_t: [-2018.284 -2018.284 -2018.284], eps: 0.1})
Step:    9600, Reward: [-419.598 -419.598 -419.598] [64.815], Avg: [-473.300 -473.300 -473.300] (0.3820) ({r_i: None, r_t: [-897.711 -897.711 -897.711], eps: 0.382})
Step:   55700, Reward: [-1932.120 -1932.120 -1932.120] [190.909], Avg: [-968.300 -968.300 -968.300] (0.0100) ({r_i: None, r_t: [-3936.770 -3936.770 -3936.770], eps: 0.01})
Step:   88500, Reward: [-1137.117 -1137.117 -1137.117] [337.812], Avg: [-562.527 -562.527 -562.527] (0.1000) ({r_i: None, r_t: [-2085.116 -2085.116 -2085.116], eps: 0.1})
Step:   55800, Reward: [-1994.379 -1994.379 -1994.379] [238.236], Avg: [-970.136 -970.136 -970.136] (0.0100) ({r_i: None, r_t: [-3847.186 -3847.186 -3847.186], eps: 0.01})
Step:    9700, Reward: [-435.906 -435.906 -435.906] [60.275], Avg: [-472.918 -472.918 -472.918] (0.3782) ({r_i: None, r_t: [-882.589 -882.589 -882.589], eps: 0.378})
Step:   88600, Reward: [-914.006 -914.006 -914.006] [222.238], Avg: [-562.923 -562.923 -562.923] (0.1000) ({r_i: None, r_t: [-1991.484 -1991.484 -1991.484], eps: 0.1})
Step:   55900, Reward: [-1938.433 -1938.433 -1938.433] [158.007], Avg: [-971.865 -971.865 -971.865] (0.0100) ({r_i: None, r_t: [-4044.239 -4044.239 -4044.239], eps: 0.01})
Step:   88700, Reward: [-1102.930 -1102.930 -1102.930] [232.507], Avg: [-563.531 -563.531 -563.531] (0.1000) ({r_i: None, r_t: [-2025.982 -2025.982 -2025.982], eps: 0.1})
Step:    9800, Reward: [-478.013 -478.013 -478.013] [100.632], Avg: [-472.969 -472.969 -472.969] (0.3744) ({r_i: None, r_t: [-894.393 -894.393 -894.393], eps: 0.374})
Step:   56000, Reward: [-1991.547 -1991.547 -1991.547] [219.716], Avg: [-973.682 -973.682 -973.682] (0.0100) ({r_i: None, r_t: [-3840.120 -3840.120 -3840.120], eps: 0.01})
Step:   88800, Reward: [-993.997 -993.997 -993.997] [364.934], Avg: [-564.015 -564.015 -564.015] (0.1000) ({r_i: None, r_t: [-1957.908 -1957.908 -1957.908], eps: 0.1})
Step:   56100, Reward: [-1917.534 -1917.534 -1917.534] [238.932], Avg: [-975.362 -975.362 -975.362] (0.0100) ({r_i: None, r_t: [-3763.905 -3763.905 -3763.905], eps: 0.01})
Step:    9900, Reward: [-466.583 -466.583 -466.583] [63.561], Avg: [-472.906 -472.906 -472.906] (0.3707) ({r_i: None, r_t: [-879.535 -879.535 -879.535], eps: 0.371})
Step:   88900, Reward: [-1098.990 -1098.990 -1098.990] [300.631], Avg: [-564.616 -564.616 -564.616] (0.1000) ({r_i: None, r_t: [-1989.441 -1989.441 -1989.441], eps: 0.1})
Step:   56200, Reward: [-1769.447 -1769.447 -1769.447] [326.575], Avg: [-976.772 -976.772 -976.772] (0.0100) ({r_i: None, r_t: [-3722.024 -3722.024 -3722.024], eps: 0.01})
Step:   89000, Reward: [-1058.151 -1058.151 -1058.151] [256.693], Avg: [-565.170 -565.170 -565.170] (0.1000) ({r_i: None, r_t: [-2158.726 -2158.726 -2158.726], eps: 0.1})
Step:   10000, Reward: [-433.695 -433.695 -433.695] [60.288], Avg: [-472.517 -472.517 -472.517] (0.3670) ({r_i: None, r_t: [-872.993 -872.993 -872.993], eps: 0.367})
Step:   56300, Reward: [-1862.978 -1862.978 -1862.978] [273.751], Avg: [-978.344 -978.344 -978.344] (0.0100) ({r_i: None, r_t: [-3747.622 -3747.622 -3747.622], eps: 0.01})
Step:   89100, Reward: [-1168.227 -1168.227 -1168.227] [310.094], Avg: [-565.846 -565.846 -565.846] (0.1000) ({r_i: None, r_t: [-2134.093 -2134.093 -2134.093], eps: 0.1})
Step:   56400, Reward: [-1878.268 -1878.268 -1878.268] [247.860], Avg: [-979.936 -979.936 -979.936] (0.0100) ({r_i: None, r_t: [-3815.385 -3815.385 -3815.385], eps: 0.01})
Step:   10100, Reward: [-399.646 -399.646 -399.646] [73.870], Avg: [-471.803 -471.803 -471.803] (0.3633) ({r_i: None, r_t: [-879.532 -879.532 -879.532], eps: 0.363})
Step:   89200, Reward: [-906.242 -906.242 -906.242] [243.267], Avg: [-566.228 -566.228 -566.228] (0.1000) ({r_i: None, r_t: [-2283.796 -2283.796 -2283.796], eps: 0.1})
Step:   56500, Reward: [-1804.814 -1804.814 -1804.814] [321.296], Avg: [-981.394 -981.394 -981.394] (0.0100) ({r_i: None, r_t: [-3500.694 -3500.694 -3500.694], eps: 0.01})
Step:   89300, Reward: [-1008.133 -1008.133 -1008.133] [318.492], Avg: [-566.722 -566.722 -566.722] (0.1000) ({r_i: None, r_t: [-2002.165 -2002.165 -2002.165], eps: 0.1})
Step:   10200, Reward: [-454.677 -454.677 -454.677] [81.625], Avg: [-471.637 -471.637 -471.637] (0.3597) ({r_i: None, r_t: [-909.411 -909.411 -909.411], eps: 0.36})
Step:   56600, Reward: [-1803.843 -1803.843 -1803.843] [240.988], Avg: [-982.844 -982.844 -982.844] (0.0100) ({r_i: None, r_t: [-3375.010 -3375.010 -3375.010], eps: 0.01})
Step:   89400, Reward: [-990.085 -990.085 -990.085] [248.971], Avg: [-567.195 -567.195 -567.195] (0.1000) ({r_i: None, r_t: [-2149.744 -2149.744 -2149.744], eps: 0.1})
Step:   10300, Reward: [-436.582 -436.582 -436.582] [73.581], Avg: [-471.300 -471.300 -471.300] (0.3561) ({r_i: None, r_t: [-922.231 -922.231 -922.231], eps: 0.356})
Step:   56700, Reward: [-1695.932 -1695.932 -1695.932] [300.365], Avg: [-984.100 -984.100 -984.100] (0.0100) ({r_i: None, r_t: [-3565.957 -3565.957 -3565.957], eps: 0.01})
Step:   89500, Reward: [-1014.862 -1014.862 -1014.862] [336.599], Avg: [-567.695 -567.695 -567.695] (0.1000) ({r_i: None, r_t: [-2293.218 -2293.218 -2293.218], eps: 0.1})
Step:   56800, Reward: [-1635.306 -1635.306 -1635.306] [352.863], Avg: [-985.244 -985.244 -985.244] (0.0100) ({r_i: None, r_t: [-3314.639 -3314.639 -3314.639], eps: 0.01})
Step:   89600, Reward: [-982.680 -982.680 -982.680] [329.600], Avg: [-568.157 -568.157 -568.157] (0.1000) ({r_i: None, r_t: [-2247.120 -2247.120 -2247.120], eps: 0.1})
Step:   10400, Reward: [-430.460 -430.460 -430.460] [80.421], Avg: [-470.911 -470.911 -470.911] (0.3525) ({r_i: None, r_t: [-882.536 -882.536 -882.536], eps: 0.353})
Step:   56900, Reward: [-1608.649 -1608.649 -1608.649] [413.126], Avg: [-986.338 -986.338 -986.338] (0.0100) ({r_i: None, r_t: [-3173.571 -3173.571 -3173.571], eps: 0.01})
Step:   89700, Reward: [-1154.564 -1154.564 -1154.564] [358.625], Avg: [-568.810 -568.810 -568.810] (0.1000) ({r_i: None, r_t: [-1957.814 -1957.814 -1957.814], eps: 0.1})
Step:   10500, Reward: [-433.810 -433.810 -433.810] [58.659], Avg: [-470.561 -470.561 -470.561] (0.3490) ({r_i: None, r_t: [-879.370 -879.370 -879.370], eps: 0.349})
Step:   57000, Reward: [-1490.745 -1490.745 -1490.745] [498.041], Avg: [-987.221 -987.221 -987.221] (0.0100) ({r_i: None, r_t: [-3147.941 -3147.941 -3147.941], eps: 0.01})
Step:   89800, Reward: [-1022.941 -1022.941 -1022.941] [349.053], Avg: [-569.315 -569.315 -569.315] (0.1000) ({r_i: None, r_t: [-2190.480 -2190.480 -2190.480], eps: 0.1})
Step:   57100, Reward: [-1282.235 -1282.235 -1282.235] [439.534], Avg: [-987.737 -987.737 -987.737] (0.0100) ({r_i: None, r_t: [-2914.391 -2914.391 -2914.391], eps: 0.01})
Step:   89900, Reward: [-893.397 -893.397 -893.397] [319.828], Avg: [-569.676 -569.676 -569.676] (0.1000) ({r_i: None, r_t: [-2117.465 -2117.465 -2117.465], eps: 0.1})
Step:   10600, Reward: [-432.568 -432.568 -432.568] [69.030], Avg: [-470.206 -470.206 -470.206] (0.3455) ({r_i: None, r_t: [-843.051 -843.051 -843.051], eps: 0.346})
Step:   57200, Reward: [-1483.574 -1483.574 -1483.574] [370.929], Avg: [-988.602 -988.602 -988.602] (0.0100) ({r_i: None, r_t: [-2741.086 -2741.086 -2741.086], eps: 0.01})
Step:   90000, Reward: [-1097.476 -1097.476 -1097.476] [314.031], Avg: [-570.261 -570.261 -570.261] (0.1000) ({r_i: None, r_t: [-2088.854 -2088.854 -2088.854], eps: 0.1})
Step:   10700, Reward: [-438.800 -438.800 -438.800] [70.573], Avg: [-469.915 -469.915 -469.915] (0.3421) ({r_i: None, r_t: [-879.836 -879.836 -879.836], eps: 0.342})
Step:   90100, Reward: [-1016.354 -1016.354 -1016.354] [327.281], Avg: [-570.756 -570.756 -570.756] (0.1000) ({r_i: None, r_t: [-2033.568 -2033.568 -2033.568], eps: 0.1})
Step:   57300, Reward: [-1246.603 -1246.603 -1246.603] [469.366], Avg: [-989.052 -989.052 -989.052] (0.0100) ({r_i: None, r_t: [-2965.731 -2965.731 -2965.731], eps: 0.01})
Step:   90200, Reward: [-992.402 -992.402 -992.402] [200.350], Avg: [-571.223 -571.223 -571.223] (0.1000) ({r_i: None, r_t: [-2256.662 -2256.662 -2256.662], eps: 0.1})
Step:   57400, Reward: [-1273.829 -1273.829 -1273.829] [548.564], Avg: [-989.547 -989.547 -989.547] (0.0100) ({r_i: None, r_t: [-2592.427 -2592.427 -2592.427], eps: 0.01})
Step:   10800, Reward: [-415.308 -415.308 -415.308] [65.051], Avg: [-469.414 -469.414 -469.414] (0.3387) ({r_i: None, r_t: [-852.053 -852.053 -852.053], eps: 0.339})
Step:   90300, Reward: [-1007.680 -1007.680 -1007.680] [326.995], Avg: [-571.706 -571.706 -571.706] (0.1000) ({r_i: None, r_t: [-1978.524 -1978.524 -1978.524], eps: 0.1})
Step:   57500, Reward: [-1037.945 -1037.945 -1037.945] [323.239], Avg: [-989.631 -989.631 -989.631] (0.0100) ({r_i: None, r_t: [-2171.293 -2171.293 -2171.293], eps: 0.01})
Step:   10900, Reward: [-442.829 -442.829 -442.829] [89.220], Avg: [-469.172 -469.172 -469.172] (0.3353) ({r_i: None, r_t: [-873.615 -873.615 -873.615], eps: 0.335})
Step:   90400, Reward: [-979.288 -979.288 -979.288] [253.953], Avg: [-572.156 -572.156 -572.156] (0.1000) ({r_i: None, r_t: [-2073.878 -2073.878 -2073.878], eps: 0.1})
Step:   57600, Reward: [-1091.832 -1091.832 -1091.832] [427.308], Avg: [-989.808 -989.808 -989.808] (0.0100) ({r_i: None, r_t: [-2306.149 -2306.149 -2306.149], eps: 0.01})
Step:   90500, Reward: [-898.143 -898.143 -898.143] [241.923], Avg: [-572.516 -572.516 -572.516] (0.1000) ({r_i: None, r_t: [-2030.447 -2030.447 -2030.447], eps: 0.1})
Step:   57700, Reward: [-1004.922 -1004.922 -1004.922] [469.595], Avg: [-989.834 -989.834 -989.834] (0.0100) ({r_i: None, r_t: [-2365.987 -2365.987 -2365.987], eps: 0.01})
Step:   11000, Reward: [-433.326 -433.326 -433.326] [60.371], Avg: [-468.849 -468.849 -468.849] (0.3320) ({r_i: None, r_t: [-814.427 -814.427 -814.427], eps: 0.332})
Step:   90600, Reward: [-1009.374 -1009.374 -1009.374] [377.570], Avg: [-572.997 -572.997 -572.997] (0.1000) ({r_i: None, r_t: [-2091.273 -2091.273 -2091.273], eps: 0.1})
Step:   57800, Reward: [-872.459 -872.459 -872.459] [385.890], Avg: [-989.632 -989.632 -989.632] (0.0100) ({r_i: None, r_t: [-1814.510 -1814.510 -1814.510], eps: 0.01})
Step:   11100, Reward: [-418.428 -418.428 -418.428] [67.276], Avg: [-468.399 -468.399 -468.399] (0.3286) ({r_i: None, r_t: [-810.415 -810.415 -810.415], eps: 0.329})
Step:   90700, Reward: [-982.431 -982.431 -982.431] [249.618], Avg: [-573.448 -573.448 -573.448] (0.1000) ({r_i: None, r_t: [-2110.301 -2110.301 -2110.301], eps: 0.1})
Step:   57900, Reward: [-938.595 -938.595 -938.595] [440.940], Avg: [-989.544 -989.544 -989.544] (0.0100) ({r_i: None, r_t: [-1923.316 -1923.316 -1923.316], eps: 0.01})
Step:   90800, Reward: [-1156.376 -1156.376 -1156.376] [387.172], Avg: [-574.090 -574.090 -574.090] (0.1000) ({r_i: None, r_t: [-1949.245 -1949.245 -1949.245], eps: 0.1})
Step:   58000, Reward: [-731.393 -731.393 -731.393] [346.841], Avg: [-989.099 -989.099 -989.099] (0.0100) ({r_i: None, r_t: [-1860.205 -1860.205 -1860.205], eps: 0.01})
Step:   11200, Reward: [-454.430 -454.430 -454.430] [99.290], Avg: [-468.275 -468.275 -468.275] (0.3254) ({r_i: None, r_t: [-875.975 -875.975 -875.975], eps: 0.325})
Step:   90900, Reward: [-1048.977 -1048.977 -1048.977] [210.132], Avg: [-574.612 -574.612 -574.612] (0.1000) ({r_i: None, r_t: [-1974.487 -1974.487 -1974.487], eps: 0.1})
Step:   58100, Reward: [-903.198 -903.198 -903.198] [374.696], Avg: [-988.952 -988.952 -988.952] (0.0100) ({r_i: None, r_t: [-1865.418 -1865.418 -1865.418], eps: 0.01})
Step:   11300, Reward: [-446.143 -446.143 -446.143] [58.158], Avg: [-468.081 -468.081 -468.081] (0.3221) ({r_i: None, r_t: [-860.702 -860.702 -860.702], eps: 0.322})
Step:   91000, Reward: [-948.801 -948.801 -948.801] [235.041], Avg: [-575.022 -575.022 -575.022] (0.1000) ({r_i: None, r_t: [-2150.050 -2150.050 -2150.050], eps: 0.1})
Step:   58200, Reward: [-720.335 -720.335 -720.335] [334.675], Avg: [-988.491 -988.491 -988.491] (0.0100) ({r_i: None, r_t: [-1489.799 -1489.799 -1489.799], eps: 0.01})
Step:   91100, Reward: [-1033.339 -1033.339 -1033.339] [376.822], Avg: [-575.525 -575.525 -575.525] (0.1000) ({r_i: None, r_t: [-1960.101 -1960.101 -1960.101], eps: 0.1})
Step:   58300, Reward: [-565.065 -565.065 -565.065] [100.127], Avg: [-987.766 -987.766 -987.766] (0.0100) ({r_i: None, r_t: [-1342.773 -1342.773 -1342.773], eps: 0.01})
Step:   11400, Reward: [-435.310 -435.310 -435.310] [55.859], Avg: [-467.796 -467.796 -467.796] (0.3189) ({r_i: None, r_t: [-805.531 -805.531 -805.531], eps: 0.319})
Step:   91200, Reward: [-966.128 -966.128 -966.128] [289.583], Avg: [-575.953 -575.953 -575.953] (0.1000) ({r_i: None, r_t: [-2086.879 -2086.879 -2086.879], eps: 0.1})
Step:   58400, Reward: [-661.346 -661.346 -661.346] [336.793], Avg: [-987.208 -987.208 -987.208] (0.0100) ({r_i: None, r_t: [-1350.811 -1350.811 -1350.811], eps: 0.01})
Step:   11500, Reward: [-395.533 -395.533 -395.533] [50.725], Avg: [-467.173 -467.173 -467.173] (0.3157) ({r_i: None, r_t: [-855.736 -855.736 -855.736], eps: 0.316})
Step:   91300, Reward: [-1035.100 -1035.100 -1035.100] [344.956], Avg: [-576.455 -576.455 -576.455] (0.1000) ({r_i: None, r_t: [-2031.542 -2031.542 -2031.542], eps: 0.1})
Step:   58500, Reward: [-693.118 -693.118 -693.118] [337.178], Avg: [-986.706 -986.706 -986.706] (0.0100) ({r_i: None, r_t: [-1226.008 -1226.008 -1226.008], eps: 0.01})
Step:   91400, Reward: [-974.448 -974.448 -974.448] [255.673], Avg: [-576.890 -576.890 -576.890] (0.1000) ({r_i: None, r_t: [-2037.488 -2037.488 -2037.488], eps: 0.1})
Step:   11600, Reward: [-447.327 -447.327 -447.327] [72.145], Avg: [-467.004 -467.004 -467.004] (0.3126) ({r_i: None, r_t: [-874.235 -874.235 -874.235], eps: 0.313})
Step:   58600, Reward: [-691.507 -691.507 -691.507] [294.802], Avg: [-986.203 -986.203 -986.203] (0.0100) ({r_i: None, r_t: [-1259.187 -1259.187 -1259.187], eps: 0.01})
Step:   91500, Reward: [-983.835 -983.835 -983.835] [280.356], Avg: [-577.334 -577.334 -577.334] (0.1000) ({r_i: None, r_t: [-2216.923 -2216.923 -2216.923], eps: 0.1})
Step:   58700, Reward: [-658.654 -658.654 -658.654] [223.156], Avg: [-985.646 -985.646 -985.646] (0.0100) ({r_i: None, r_t: [-1156.580 -1156.580 -1156.580], eps: 0.01})
Step:   11700, Reward: [-405.315 -405.315 -405.315] [72.928], Avg: [-466.481 -466.481 -466.481] (0.3095) ({r_i: None, r_t: [-822.845 -822.845 -822.845], eps: 0.309})
Step:   91600, Reward: [-981.301 -981.301 -981.301] [408.552], Avg: [-577.775 -577.775 -577.775] (0.1000) ({r_i: None, r_t: [-1914.051 -1914.051 -1914.051], eps: 0.1})
Step:   58800, Reward: [-526.498 -526.498 -526.498] [116.570], Avg: [-984.867 -984.867 -984.867] (0.0100) ({r_i: None, r_t: [-1023.983 -1023.983 -1023.983], eps: 0.01})
Step:   91700, Reward: [-989.088 -989.088 -989.088] [350.814], Avg: [-578.223 -578.223 -578.223] (0.1000) ({r_i: None, r_t: [-2206.813 -2206.813 -2206.813], eps: 0.1})
Step:   11800, Reward: [-410.986 -410.986 -410.986] [55.603], Avg: [-466.015 -466.015 -466.015] (0.3064) ({r_i: None, r_t: [-833.465 -833.465 -833.465], eps: 0.306})
Step:   58900, Reward: [-612.833 -612.833 -612.833] [244.074], Avg: [-984.236 -984.236 -984.236] (0.0100) ({r_i: None, r_t: [-1131.478 -1131.478 -1131.478], eps: 0.01})
Step:   91800, Reward: [-915.442 -915.442 -915.442] [273.718], Avg: [-578.590 -578.590 -578.590] (0.1000) ({r_i: None, r_t: [-2027.564 -2027.564 -2027.564], eps: 0.1})
Step:   59000, Reward: [-551.633 -551.633 -551.633] [77.363], Avg: [-983.504 -983.504 -983.504] (0.0100) ({r_i: None, r_t: [-1129.023 -1129.023 -1129.023], eps: 0.01})
Step:   91900, Reward: [-1074.630 -1074.630 -1074.630] [360.690], Avg: [-579.129 -579.129 -579.129] (0.1000) ({r_i: None, r_t: [-2089.800 -2089.800 -2089.800], eps: 0.1})
Step:   11900, Reward: [-406.406 -406.406 -406.406] [63.308], Avg: [-465.518 -465.518 -465.518] (0.3033) ({r_i: None, r_t: [-828.724 -828.724 -828.724], eps: 0.303})
Step:   59100, Reward: [-545.716 -545.716 -545.716] [138.877], Avg: [-982.765 -982.765 -982.765] (0.0100) ({r_i: None, r_t: [-1104.305 -1104.305 -1104.305], eps: 0.01})
Step:   92000, Reward: [-1020.854 -1020.854 -1020.854] [298.608], Avg: [-579.609 -579.609 -579.609] (0.1000) ({r_i: None, r_t: [-1936.245 -1936.245 -1936.245], eps: 0.1})
Step:   12000, Reward: [-423.290 -423.290 -423.290] [61.730], Avg: [-465.169 -465.169 -465.169] (0.3003) ({r_i: None, r_t: [-804.097 -804.097 -804.097], eps: 0.3})
Step:   59200, Reward: [-476.594 -476.594 -476.594] [133.684], Avg: [-981.911 -981.911 -981.911] (0.0100) ({r_i: None, r_t: [-1131.714 -1131.714 -1131.714], eps: 0.01})
Step:   92100, Reward: [-890.316 -890.316 -890.316] [272.742], Avg: [-579.945 -579.945 -579.945] (0.1000) ({r_i: None, r_t: [-2081.316 -2081.316 -2081.316], eps: 0.1})
Step:   59300, Reward: [-486.218 -486.218 -486.218] [84.259], Avg: [-981.076 -981.076 -981.076] (0.0100) ({r_i: None, r_t: [-996.229 -996.229 -996.229], eps: 0.01})
Step:   92200, Reward: [-928.701 -928.701 -928.701] [340.311], Avg: [-580.323 -580.323 -580.323] (0.1000) ({r_i: None, r_t: [-1902.878 -1902.878 -1902.878], eps: 0.1})
Step:   12100, Reward: [-426.179 -426.179 -426.179] [52.482], Avg: [-464.849 -464.849 -464.849] (0.2973) ({r_i: None, r_t: [-867.828 -867.828 -867.828], eps: 0.297})
Step:   59400, Reward: [-570.623 -570.623 -570.623] [157.297], Avg: [-980.387 -980.387 -980.387] (0.0100) ({r_i: None, r_t: [-1040.440 -1040.440 -1040.440], eps: 0.01})
Step:   92300, Reward: [-1000.448 -1000.448 -1000.448] [200.255], Avg: [-580.778 -580.778 -580.778] (0.1000) ({r_i: None, r_t: [-1900.698 -1900.698 -1900.698], eps: 0.1})
Step:   12200, Reward: [-387.816 -387.816 -387.816] [44.836], Avg: [-464.223 -464.223 -464.223] (0.2943) ({r_i: None, r_t: [-832.671 -832.671 -832.671], eps: 0.294})
Step:   59500, Reward: [-494.128 -494.128 -494.128] [99.846], Avg: [-979.571 -979.571 -979.571] (0.0100) ({r_i: None, r_t: [-1108.157 -1108.157 -1108.157], eps: 0.01})
Step:   92400, Reward: [-1066.901 -1066.901 -1066.901] [278.508], Avg: [-581.304 -581.304 -581.304] (0.1000) ({r_i: None, r_t: [-2214.678 -2214.678 -2214.678], eps: 0.1})
Step:   59600, Reward: [-495.217 -495.217 -495.217] [121.863], Avg: [-978.759 -978.759 -978.759] (0.0100) ({r_i: None, r_t: [-944.244 -944.244 -944.244], eps: 0.01})
Step:   92500, Reward: [-1124.603 -1124.603 -1124.603] [315.396], Avg: [-581.890 -581.890 -581.890] (0.1000) ({r_i: None, r_t: [-2046.761 -2046.761 -2046.761], eps: 0.1})
Step:   12300, Reward: [-421.501 -421.501 -421.501] [57.239], Avg: [-463.878 -463.878 -463.878] (0.2914) ({r_i: None, r_t: [-838.605 -838.605 -838.605], eps: 0.291})
Step:   59700, Reward: [-475.207 -475.207 -475.207] [120.173], Avg: [-977.917 -977.917 -977.917] (0.0100) ({r_i: None, r_t: [-978.865 -978.865 -978.865], eps: 0.01})
Step:   92600, Reward: [-995.180 -995.180 -995.180] [265.261], Avg: [-582.336 -582.336 -582.336] (0.1000) ({r_i: None, r_t: [-2109.010 -2109.010 -2109.010], eps: 0.1})
Step:   12400, Reward: [-439.923 -439.923 -439.923] [85.662], Avg: [-463.687 -463.687 -463.687] (0.2885) ({r_i: None, r_t: [-849.852 -849.852 -849.852], eps: 0.288})
Step:   59800, Reward: [-456.369 -456.369 -456.369] [73.464], Avg: [-977.047 -977.047 -977.047] (0.0100) ({r_i: None, r_t: [-1069.546 -1069.546 -1069.546], eps: 0.01})
Step:   92700, Reward: [-802.902 -802.902 -802.902] [266.255], Avg: [-582.574 -582.574 -582.574] (0.1000) ({r_i: None, r_t: [-2032.527 -2032.527 -2032.527], eps: 0.1})
Step:   92800, Reward: [-908.161 -908.161 -908.161] [305.737], Avg: [-582.924 -582.924 -582.924] (0.1000) ({r_i: None, r_t: [-1951.432 -1951.432 -1951.432], eps: 0.1})
Step:   59900, Reward: [-454.853 -454.853 -454.853] [79.924], Avg: [-976.176 -976.176 -976.176] (0.0100) ({r_i: None, r_t: [-1071.250 -1071.250 -1071.250], eps: 0.01})
Step:   12500, Reward: [-417.960 -417.960 -417.960] [80.382], Avg: [-463.324 -463.324 -463.324] (0.2856) ({r_i: None, r_t: [-815.199 -815.199 -815.199], eps: 0.286})
Step:   92900, Reward: [-860.216 -860.216 -860.216] [293.287], Avg: [-583.222 -583.222 -583.222] (0.1000) ({r_i: None, r_t: [-1950.239 -1950.239 -1950.239], eps: 0.1})
Step:   60000, Reward: [-545.535 -545.535 -545.535] [139.314], Avg: [-975.460 -975.460 -975.460] (0.0100) ({r_i: None, r_t: [-944.611 -944.611 -944.611], eps: 0.01})
Step:   12600, Reward: [-439.701 -439.701 -439.701] [70.213], Avg: [-463.138 -463.138 -463.138] (0.2828) ({r_i: None, r_t: [-802.011 -802.011 -802.011], eps: 0.283})
Step:   93000, Reward: [-919.439 -919.439 -919.439] [345.506], Avg: [-583.584 -583.584 -583.584] (0.1000) ({r_i: None, r_t: [-2122.655 -2122.655 -2122.655], eps: 0.1})
Step:   60100, Reward: [-503.829 -503.829 -503.829] [121.474], Avg: [-974.676 -974.676 -974.676] (0.0100) ({r_i: None, r_t: [-1055.818 -1055.818 -1055.818], eps: 0.01})
Step:   93100, Reward: [-1024.563 -1024.563 -1024.563] [340.492], Avg: [-584.057 -584.057 -584.057] (0.1000) ({r_i: None, r_t: [-1919.365 -1919.365 -1919.365], eps: 0.1})
Step:   60200, Reward: [-493.263 -493.263 -493.263] [77.869], Avg: [-973.878 -973.878 -973.878] (0.0100) ({r_i: None, r_t: [-984.366 -984.366 -984.366], eps: 0.01})
Step:   12700, Reward: [-397.578 -397.578 -397.578] [75.801], Avg: [-462.626 -462.626 -462.626] (0.2799) ({r_i: None, r_t: [-828.201 -828.201 -828.201], eps: 0.28})
Step:   93200, Reward: [-1044.910 -1044.910 -1044.910] [287.372], Avg: [-584.551 -584.551 -584.551] (0.1000) ({r_i: None, r_t: [-1888.581 -1888.581 -1888.581], eps: 0.1})
Step:   60300, Reward: [-469.531 -469.531 -469.531] [152.482], Avg: [-973.043 -973.043 -973.043] (0.0100) ({r_i: None, r_t: [-1019.354 -1019.354 -1019.354], eps: 0.01})
Step:   12800, Reward: [-422.881 -422.881 -422.881] [58.505], Avg: [-462.318 -462.318 -462.318] (0.2771) ({r_i: None, r_t: [-823.401 -823.401 -823.401], eps: 0.277})
Step:   93300, Reward: [-943.388 -943.388 -943.388] [269.057], Avg: [-584.935 -584.935 -584.935] (0.1000) ({r_i: None, r_t: [-1870.820 -1870.820 -1870.820], eps: 0.1})
Step:   60400, Reward: [-459.872 -459.872 -459.872] [73.414], Avg: [-972.195 -972.195 -972.195] (0.0100) ({r_i: None, r_t: [-949.020 -949.020 -949.020], eps: 0.01})
Step:   93400, Reward: [-994.022 -994.022 -994.022] [293.547], Avg: [-585.372 -585.372 -585.372] (0.1000) ({r_i: None, r_t: [-2032.040 -2032.040 -2032.040], eps: 0.1})
Step:   12900, Reward: [-401.155 -401.155 -401.155] [43.749], Avg: [-461.847 -461.847 -461.847] (0.2744) ({r_i: None, r_t: [-846.688 -846.688 -846.688], eps: 0.274})
Step:   60500, Reward: [-480.383 -480.383 -480.383] [141.186], Avg: [-971.383 -971.383 -971.383] (0.0100) ({r_i: None, r_t: [-1002.372 -1002.372 -1002.372], eps: 0.01})
Step:   93500, Reward: [-890.747 -890.747 -890.747] [307.348], Avg: [-585.699 -585.699 -585.699] (0.1000) ({r_i: None, r_t: [-2036.336 -2036.336 -2036.336], eps: 0.1})
Step:   60600, Reward: [-459.447 -459.447 -459.447] [95.334], Avg: [-970.540 -970.540 -970.540] (0.0100) ({r_i: None, r_t: [-1052.628 -1052.628 -1052.628], eps: 0.01})
Step:   13000, Reward: [-399.028 -399.028 -399.028] [65.198], Avg: [-461.368 -461.368 -461.368] (0.2716) ({r_i: None, r_t: [-835.127 -835.127 -835.127], eps: 0.272})
Step:   93600, Reward: [-939.840 -939.840 -939.840] [276.618], Avg: [-586.077 -586.077 -586.077] (0.1000) ({r_i: None, r_t: [-2045.449 -2045.449 -2045.449], eps: 0.1})
Step:   60700, Reward: [-487.245 -487.245 -487.245] [155.778], Avg: [-969.745 -969.745 -969.745] (0.0100) ({r_i: None, r_t: [-875.312 -875.312 -875.312], eps: 0.01})
Step:   93700, Reward: [-974.728 -974.728 -974.728] [243.060], Avg: [-586.491 -586.491 -586.491] (0.1000) ({r_i: None, r_t: [-1734.327 -1734.327 -1734.327], eps: 0.1})
Step:   13100, Reward: [-428.154 -428.154 -428.154] [68.234], Avg: [-461.116 -461.116 -461.116] (0.2689) ({r_i: None, r_t: [-787.850 -787.850 -787.850], eps: 0.269})
Step:   60800, Reward: [-510.756 -510.756 -510.756] [101.733], Avg: [-968.991 -968.991 -968.991] (0.0100) ({r_i: None, r_t: [-897.833 -897.833 -897.833], eps: 0.01})
Step:   93800, Reward: [-835.206 -835.206 -835.206] [262.861], Avg: [-586.756 -586.756 -586.756] (0.1000) ({r_i: None, r_t: [-1811.823 -1811.823 -1811.823], eps: 0.1})
Step:   60900, Reward: [-522.390 -522.390 -522.390] [177.079], Avg: [-968.259 -968.259 -968.259] (0.0100) ({r_i: None, r_t: [-951.728 -951.728 -951.728], eps: 0.01})
Step:   13200, Reward: [-401.221 -401.221 -401.221] [61.483], Avg: [-460.666 -460.666 -460.666] (0.2663) ({r_i: None, r_t: [-806.233 -806.233 -806.233], eps: 0.266})
Step:   93900, Reward: [-935.483 -935.483 -935.483] [275.002], Avg: [-587.127 -587.127 -587.127] (0.1000) ({r_i: None, r_t: [-1804.248 -1804.248 -1804.248], eps: 0.1})
Step:   61000, Reward: [-414.355 -414.355 -414.355] [64.681], Avg: [-967.353 -967.353 -967.353] (0.0100) ({r_i: None, r_t: [-946.535 -946.535 -946.535], eps: 0.01})
Step:   94000, Reward: [-872.158 -872.158 -872.158] [252.510], Avg: [-587.430 -587.430 -587.430] (0.1000) ({r_i: None, r_t: [-1809.253 -1809.253 -1809.253], eps: 0.1})
Step:   13300, Reward: [-428.082 -428.082 -428.082] [73.481], Avg: [-460.422 -460.422 -460.422] (0.2636) ({r_i: None, r_t: [-808.604 -808.604 -808.604], eps: 0.264})
Step:   61100, Reward: [-479.964 -479.964 -479.964] [103.939], Avg: [-966.556 -966.556 -966.556] (0.0100) ({r_i: None, r_t: [-871.785 -871.785 -871.785], eps: 0.01})
Step:   94100, Reward: [-951.398 -951.398 -951.398] [275.449], Avg: [-587.816 -587.816 -587.816] (0.1000) ({r_i: None, r_t: [-1792.208 -1792.208 -1792.208], eps: 0.1})
Step:   61200, Reward: [-501.111 -501.111 -501.111] [108.971], Avg: [-965.797 -965.797 -965.797] (0.0100) ({r_i: None, r_t: [-910.842 -910.842 -910.842], eps: 0.01})
Step:   13400, Reward: [-389.772 -389.772 -389.772] [65.083], Avg: [-459.899 -459.899 -459.899] (0.2610) ({r_i: None, r_t: [-801.898 -801.898 -801.898], eps: 0.261})
Step:   94200, Reward: [-954.548 -954.548 -954.548] [247.887], Avg: [-588.205 -588.205 -588.205] (0.1000) ({r_i: None, r_t: [-1690.936 -1690.936 -1690.936], eps: 0.1})
Step:   61300, Reward: [-440.538 -440.538 -440.538] [62.812], Avg: [-964.941 -964.941 -964.941] (0.0100) ({r_i: None, r_t: [-906.365 -906.365 -906.365], eps: 0.01})
Step:   94300, Reward: [-937.937 -937.937 -937.937] [367.072], Avg: [-588.575 -588.575 -588.575] (0.1000) ({r_i: None, r_t: [-1957.801 -1957.801 -1957.801], eps: 0.1})
Step:   13500, Reward: [-407.888 -407.888 -407.888] [65.476], Avg: [-459.517 -459.517 -459.517] (0.2584) ({r_i: None, r_t: [-803.753 -803.753 -803.753], eps: 0.258})
Step:   61400, Reward: [-449.450 -449.450 -449.450] [67.873], Avg: [-964.103 -964.103 -964.103] (0.0100) ({r_i: None, r_t: [-913.530 -913.530 -913.530], eps: 0.01})
Step:   94400, Reward: [-1009.763 -1009.763 -1009.763] [305.249], Avg: [-589.021 -589.021 -589.021] (0.1000) ({r_i: None, r_t: [-1813.861 -1813.861 -1813.861], eps: 0.1})
Step:   94500, Reward: [-933.903 -933.903 -933.903] [323.112], Avg: [-589.386 -589.386 -589.386] (0.1000) ({r_i: None, r_t: [-2043.820 -2043.820 -2043.820], eps: 0.1})
Step:   61500, Reward: [-448.452 -448.452 -448.452] [105.129], Avg: [-963.266 -963.266 -963.266] (0.0100) ({r_i: None, r_t: [-903.969 -903.969 -903.969], eps: 0.01})
Step:   13600, Reward: [-380.596 -380.596 -380.596] [46.544], Avg: [-458.941 -458.941 -458.941] (0.2558) ({r_i: None, r_t: [-851.134 -851.134 -851.134], eps: 0.256})
Step:   94600, Reward: [-921.814 -921.814 -921.814] [242.769], Avg: [-589.737 -589.737 -589.737] (0.1000) ({r_i: None, r_t: [-1958.930 -1958.930 -1958.930], eps: 0.1})
Step:   61600, Reward: [-407.536 -407.536 -407.536] [59.632], Avg: [-962.365 -962.365 -962.365] (0.0100) ({r_i: None, r_t: [-843.776 -843.776 -843.776], eps: 0.01})
Step:   13700, Reward: [-387.550 -387.550 -387.550] [49.238], Avg: [-458.423 -458.423 -458.423] (0.2532) ({r_i: None, r_t: [-788.933 -788.933 -788.933], eps: 0.253})
Step:   94700, Reward: [-1025.461 -1025.461 -1025.461] [287.032], Avg: [-590.196 -590.196 -590.196] (0.1000) ({r_i: None, r_t: [-1758.490 -1758.490 -1758.490], eps: 0.1})
Step:   61700, Reward: [-458.071 -458.071 -458.071] [72.624], Avg: [-961.549 -961.549 -961.549] (0.0100) ({r_i: None, r_t: [-888.434 -888.434 -888.434], eps: 0.01})
Step:   94800, Reward: [-856.502 -856.502 -856.502] [234.737], Avg: [-590.477 -590.477 -590.477] (0.1000) ({r_i: None, r_t: [-1674.538 -1674.538 -1674.538], eps: 0.1})
Step:   61800, Reward: [-448.346 -448.346 -448.346] [70.682], Avg: [-960.720 -960.720 -960.720] (0.0100) ({r_i: None, r_t: [-880.345 -880.345 -880.345], eps: 0.01})
Step:   13800, Reward: [-445.057 -445.057 -445.057] [57.264], Avg: [-458.327 -458.327 -458.327] (0.2507) ({r_i: None, r_t: [-810.114 -810.114 -810.114], eps: 0.251})
Step:   94900, Reward: [-872.317 -872.317 -872.317] [284.276], Avg: [-590.774 -590.774 -590.774] (0.1000) ({r_i: None, r_t: [-1738.791 -1738.791 -1738.791], eps: 0.1})
Step:   61900, Reward: [-437.055 -437.055 -437.055] [53.129], Avg: [-959.876 -959.876 -959.876] (0.0100) ({r_i: None, r_t: [-892.607 -892.607 -892.607], eps: 0.01})
Step:   13900, Reward: [-394.748 -394.748 -394.748] [41.956], Avg: [-457.873 -457.873 -457.873] (0.2482) ({r_i: None, r_t: [-864.429 -864.429 -864.429], eps: 0.248})
Step:   95000, Reward: [-948.016 -948.016 -948.016] [274.408], Avg: [-591.149 -591.149 -591.149] (0.1000) ({r_i: None, r_t: [-1657.506 -1657.506 -1657.506], eps: 0.1})
Step:   62000, Reward: [-417.656 -417.656 -417.656] [73.554], Avg: [-959.003 -959.003 -959.003] (0.0100) ({r_i: None, r_t: [-881.474 -881.474 -881.474], eps: 0.01})
Step:   95100, Reward: [-1009.946 -1009.946 -1009.946] [221.415], Avg: [-591.589 -591.589 -591.589] (0.1000) ({r_i: None, r_t: [-1776.503 -1776.503 -1776.503], eps: 0.1})
Step:   14000, Reward: [-423.670 -423.670 -423.670] [64.030], Avg: [-457.630 -457.630 -457.630] (0.2457) ({r_i: None, r_t: [-797.437 -797.437 -797.437], eps: 0.246})
Step:   62100, Reward: [-433.482 -433.482 -433.482] [66.520], Avg: [-958.158 -958.158 -958.158] (0.0100) ({r_i: None, r_t: [-883.112 -883.112 -883.112], eps: 0.01})
Step:   95200, Reward: [-819.303 -819.303 -819.303] [250.584], Avg: [-591.828 -591.828 -591.828] (0.1000) ({r_i: None, r_t: [-1943.150 -1943.150 -1943.150], eps: 0.1})
Step:   62200, Reward: [-425.506 -425.506 -425.506] [64.610], Avg: [-957.303 -957.303 -957.303] (0.0100) ({r_i: None, r_t: [-885.846 -885.846 -885.846], eps: 0.01})
Step:   14100, Reward: [-407.305 -407.305 -407.305] [45.749], Avg: [-457.276 -457.276 -457.276] (0.2433) ({r_i: None, r_t: [-793.646 -793.646 -793.646], eps: 0.243})
Step:   95300, Reward: [-1009.096 -1009.096 -1009.096] [379.949], Avg: [-592.266 -592.266 -592.266] (0.1000) ({r_i: None, r_t: [-1860.579 -1860.579 -1860.579], eps: 0.1})
Step:   62300, Reward: [-425.377 -425.377 -425.377] [62.795], Avg: [-956.450 -956.450 -956.450] (0.0100) ({r_i: None, r_t: [-906.014 -906.014 -906.014], eps: 0.01})
Step:   95400, Reward: [-886.236 -886.236 -886.236] [252.517], Avg: [-592.573 -592.573 -592.573] (0.1000) ({r_i: None, r_t: [-2005.755 -2005.755 -2005.755], eps: 0.1})
Step:   14200, Reward: [-399.780 -399.780 -399.780] [67.280], Avg: [-456.874 -456.874 -456.874] (0.2409) ({r_i: None, r_t: [-812.924 -812.924 -812.924], eps: 0.241})
Step:   62400, Reward: [-429.095 -429.095 -429.095] [64.921], Avg: [-955.606 -955.606 -955.606] (0.0100) ({r_i: None, r_t: [-853.486 -853.486 -853.486], eps: 0.01})
Step:   95500, Reward: [-941.920 -941.920 -941.920] [239.938], Avg: [-592.939 -592.939 -592.939] (0.1000) ({r_i: None, r_t: [-1818.574 -1818.574 -1818.574], eps: 0.1})
Step:   62500, Reward: [-440.841 -440.841 -440.841] [94.419], Avg: [-954.784 -954.784 -954.784] (0.0100) ({r_i: None, r_t: [-884.558 -884.558 -884.558], eps: 0.01})
Step:   14300, Reward: [-404.523 -404.523 -404.523] [56.844], Avg: [-456.510 -456.510 -456.510] (0.2385) ({r_i: None, r_t: [-799.014 -799.014 -799.014], eps: 0.238})
Step:   95600, Reward: [-878.096 -878.096 -878.096] [289.014], Avg: [-593.237 -593.237 -593.237] (0.1000) ({r_i: None, r_t: [-1809.055 -1809.055 -1809.055], eps: 0.1})
Step:   62600, Reward: [-456.560 -456.560 -456.560] [81.058], Avg: [-953.990 -953.990 -953.990] (0.0100) ({r_i: None, r_t: [-912.929 -912.929 -912.929], eps: 0.01})
Step:   95700, Reward: [-931.218 -931.218 -931.218] [256.666], Avg: [-593.590 -593.590 -593.590] (0.1000) ({r_i: None, r_t: [-1816.447 -1816.447 -1816.447], eps: 0.1})
Step:   14400, Reward: [-383.073 -383.073 -383.073] [63.692], Avg: [-456.004 -456.004 -456.004] (0.2361) ({r_i: None, r_t: [-833.003 -833.003 -833.003], eps: 0.236})
Step:   62700, Reward: [-406.689 -406.689 -406.689] [77.983], Avg: [-953.118 -953.118 -953.118] (0.0100) ({r_i: None, r_t: [-864.217 -864.217 -864.217], eps: 0.01})
Step:   95800, Reward: [-850.246 -850.246 -850.246] [268.929], Avg: [-593.857 -593.857 -593.857] (0.1000) ({r_i: None, r_t: [-1539.084 -1539.084 -1539.084], eps: 0.1})
Step:   62800, Reward: [-432.628 -432.628 -432.628] [76.967], Avg: [-952.291 -952.291 -952.291] (0.0100) ({r_i: None, r_t: [-920.882 -920.882 -920.882], eps: 0.01})
Step:   14500, Reward: [-419.199 -419.199 -419.199] [61.735], Avg: [-455.752 -455.752 -455.752] (0.2337) ({r_i: None, r_t: [-784.381 -784.381 -784.381], eps: 0.234})
Step:   95900, Reward: [-919.150 -919.150 -919.150] [244.880], Avg: [-594.196 -594.196 -594.196] (0.1000) ({r_i: None, r_t: [-1700.517 -1700.517 -1700.517], eps: 0.1})
Step:   62900, Reward: [-457.804 -457.804 -457.804] [132.839], Avg: [-951.506 -951.506 -951.506] (0.0100) ({r_i: None, r_t: [-867.587 -867.587 -867.587], eps: 0.01})
Step:   96000, Reward: [-889.877 -889.877 -889.877] [202.152], Avg: [-594.504 -594.504 -594.504] (0.1000) ({r_i: None, r_t: [-1736.507 -1736.507 -1736.507], eps: 0.1})
Step:   14600, Reward: [-414.686 -414.686 -414.686] [49.823], Avg: [-455.472 -455.472 -455.472] (0.2314) ({r_i: None, r_t: [-861.658 -861.658 -861.658], eps: 0.231})
Step:   63000, Reward: [-429.039 -429.039 -429.039] [75.252], Avg: [-950.678 -950.678 -950.678] (0.0100) ({r_i: None, r_t: [-841.956 -841.956 -841.956], eps: 0.01})
Step:   96100, Reward: [-950.627 -950.627 -950.627] [384.842], Avg: [-594.874 -594.874 -594.874] (0.1000) ({r_i: None, r_t: [-1758.465 -1758.465 -1758.465], eps: 0.1})
Step:   63100, Reward: [-455.834 -455.834 -455.834] [92.781], Avg: [-949.895 -949.895 -949.895] (0.0100) ({r_i: None, r_t: [-932.703 -932.703 -932.703], eps: 0.01})
Step:   14700, Reward: [-379.037 -379.037 -379.037] [62.999], Avg: [-454.956 -454.956 -454.956] (0.2291) ({r_i: None, r_t: [-797.011 -797.011 -797.011], eps: 0.229})
Step:   96200, Reward: [-799.646 -799.646 -799.646] [246.851], Avg: [-595.087 -595.087 -595.087] (0.1000) ({r_i: None, r_t: [-1693.945 -1693.945 -1693.945], eps: 0.1})
Step:   63200, Reward: [-499.847 -499.847 -499.847] [86.856], Avg: [-949.184 -949.184 -949.184] (0.0100) ({r_i: None, r_t: [-889.848 -889.848 -889.848], eps: 0.01})
Step:   96300, Reward: [-815.763 -815.763 -815.763] [253.014], Avg: [-595.315 -595.315 -595.315] (0.1000) ({r_i: None, r_t: [-1521.623 -1521.623 -1521.623], eps: 0.1})
Step:   14800, Reward: [-405.856 -405.856 -405.856] [74.669], Avg: [-454.626 -454.626 -454.626] (0.2268) ({r_i: None, r_t: [-806.551 -806.551 -806.551], eps: 0.227})
Step:   63300, Reward: [-433.438 -433.438 -433.438] [71.357], Avg: [-948.370 -948.370 -948.370] (0.0100) ({r_i: None, r_t: [-871.862 -871.862 -871.862], eps: 0.01})
Step:   96400, Reward: [-967.642 -967.642 -967.642] [236.515], Avg: [-595.701 -595.701 -595.701] (0.1000) ({r_i: None, r_t: [-1879.287 -1879.287 -1879.287], eps: 0.1})
Step:   14900, Reward: [-420.049 -420.049 -420.049] [44.027], Avg: [-454.396 -454.396 -454.396] (0.2245) ({r_i: None, r_t: [-828.879 -828.879 -828.879], eps: 0.225})
Step:   63400, Reward: [-452.952 -452.952 -452.952] [81.635], Avg: [-947.590 -947.590 -947.590] (0.0100) ({r_i: None, r_t: [-917.775 -917.775 -917.775], eps: 0.01})
Step:   96500, Reward: [-936.155 -936.155 -936.155] [283.539], Avg: [-596.054 -596.054 -596.054] (0.1000) ({r_i: None, r_t: [-1834.789 -1834.789 -1834.789], eps: 0.1})
Step:   63500, Reward: [-442.971 -442.971 -442.971] [84.517], Avg: [-946.797 -946.797 -946.797] (0.0100) ({r_i: None, r_t: [-844.831 -844.831 -844.831], eps: 0.01})
Step:   96600, Reward: [-844.988 -844.988 -844.988] [314.514], Avg: [-596.311 -596.311 -596.311] (0.1000) ({r_i: None, r_t: [-1813.685 -1813.685 -1813.685], eps: 0.1})
Step:   15000, Reward: [-404.289 -404.289 -404.289] [46.161], Avg: [-454.064 -454.064 -454.064] (0.2223) ({r_i: None, r_t: [-815.814 -815.814 -815.814], eps: 0.222})
Step:   63600, Reward: [-457.322 -457.322 -457.322] [98.237], Avg: [-946.028 -946.028 -946.028] (0.0100) ({r_i: None, r_t: [-904.779 -904.779 -904.779], eps: 0.01})
Step:   96700, Reward: [-880.408 -880.408 -880.408] [286.447], Avg: [-596.605 -596.605 -596.605] (0.1000) ({r_i: None, r_t: [-1750.663 -1750.663 -1750.663], eps: 0.1})
Step:   15100, Reward: [-409.727 -409.727 -409.727] [63.417], Avg: [-453.772 -453.772 -453.772] (0.2201) ({r_i: None, r_t: [-828.769 -828.769 -828.769], eps: 0.22})
Step:   63700, Reward: [-459.932 -459.932 -459.932] [111.832], Avg: [-945.266 -945.266 -945.266] (0.0100) ({r_i: None, r_t: [-885.726 -885.726 -885.726], eps: 0.01})
Step:   96800, Reward: [-769.735 -769.735 -769.735] [203.462], Avg: [-596.783 -596.783 -596.783] (0.1000) ({r_i: None, r_t: [-1533.195 -1533.195 -1533.195], eps: 0.1})
Step:   96900, Reward: [-857.243 -857.243 -857.243] [346.935], Avg: [-597.052 -597.052 -597.052] (0.1000) ({r_i: None, r_t: [-1690.003 -1690.003 -1690.003], eps: 0.1})
Step:   63800, Reward: [-418.289 -418.289 -418.289] [74.136], Avg: [-944.442 -944.442 -944.442] (0.0100) ({r_i: None, r_t: [-883.445 -883.445 -883.445], eps: 0.01})
Step:   15200, Reward: [-431.875 -431.875 -431.875] [41.380], Avg: [-453.629 -453.629 -453.629] (0.2179) ({r_i: None, r_t: [-826.756 -826.756 -826.756], eps: 0.218})
Step:   97000, Reward: [-824.113 -824.113 -824.113] [308.335], Avg: [-597.286 -597.286 -597.286] (0.1000) ({r_i: None, r_t: [-1614.118 -1614.118 -1614.118], eps: 0.1})
Step:   63900, Reward: [-507.648 -507.648 -507.648] [76.487], Avg: [-943.759 -943.759 -943.759] (0.0100) ({r_i: None, r_t: [-885.014 -885.014 -885.014], eps: 0.01})
Step:   15300, Reward: [-382.739 -382.739 -382.739] [50.734], Avg: [-453.169 -453.169 -453.169] (0.2157) ({r_i: None, r_t: [-786.323 -786.323 -786.323], eps: 0.216})
Step:   97100, Reward: [-773.999 -773.999 -773.999] [293.082], Avg: [-597.468 -597.468 -597.468] (0.1000) ({r_i: None, r_t: [-1713.156 -1713.156 -1713.156], eps: 0.1})
Step:   64000, Reward: [-436.979 -436.979 -436.979] [104.178], Avg: [-942.969 -942.969 -942.969] (0.0100) ({r_i: None, r_t: [-980.394 -980.394 -980.394], eps: 0.01})
Step:   97200, Reward: [-903.704 -903.704 -903.704] [337.920], Avg: [-597.782 -597.782 -597.782] (0.1000) ({r_i: None, r_t: [-1585.202 -1585.202 -1585.202], eps: 0.1})
Step:   64100, Reward: [-475.138 -475.138 -475.138] [86.130], Avg: [-942.240 -942.240 -942.240] (0.0100) ({r_i: None, r_t: [-886.140 -886.140 -886.140], eps: 0.01})
Step:   15400, Reward: [-403.703 -403.703 -403.703] [54.408], Avg: [-452.850 -452.850 -452.850] (0.2136) ({r_i: None, r_t: [-811.926 -811.926 -811.926], eps: 0.214})
Step:   97300, Reward: [-834.262 -834.262 -834.262] [209.852], Avg: [-598.025 -598.025 -598.025] (0.1000) ({r_i: None, r_t: [-1668.493 -1668.493 -1668.493], eps: 0.1})
Step:   64200, Reward: [-462.029 -462.029 -462.029] [93.573], Avg: [-941.493 -941.493 -941.493] (0.0100) ({r_i: None, r_t: [-892.555 -892.555 -892.555], eps: 0.01})
Step:   15500, Reward: [-390.970 -390.970 -390.970] [52.342], Avg: [-452.453 -452.453 -452.453] (0.2114) ({r_i: None, r_t: [-807.377 -807.377 -807.377], eps: 0.211})
Step:   97400, Reward: [-842.332 -842.332 -842.332] [294.875], Avg: [-598.276 -598.276 -598.276] (0.1000) ({r_i: None, r_t: [-1528.501 -1528.501 -1528.501], eps: 0.1})
Step:   64300, Reward: [-419.658 -419.658 -419.658] [81.742], Avg: [-940.683 -940.683 -940.683] (0.0100) ({r_i: None, r_t: [-934.542 -934.542 -934.542], eps: 0.01})
Step:   97500, Reward: [-894.594 -894.594 -894.594] [293.516], Avg: [-598.579 -598.579 -598.579] (0.1000) ({r_i: None, r_t: [-1649.951 -1649.951 -1649.951], eps: 0.1})
Step:   64400, Reward: [-453.183 -453.183 -453.183] [92.813], Avg: [-939.927 -939.927 -939.927] (0.0100) ({r_i: None, r_t: [-880.641 -880.641 -880.641], eps: 0.01})
Step:   15600, Reward: [-368.591 -368.591 -368.591] [41.670], Avg: [-451.919 -451.919 -451.919] (0.2093) ({r_i: None, r_t: [-874.319 -874.319 -874.319], eps: 0.209})
Step:   97600, Reward: [-800.249 -800.249 -800.249] [321.653], Avg: [-598.786 -598.786 -598.786] (0.1000) ({r_i: None, r_t: [-1664.141 -1664.141 -1664.141], eps: 0.1})
Step:   64500, Reward: [-444.664 -444.664 -444.664] [96.551], Avg: [-939.160 -939.160 -939.160] (0.0100) ({r_i: None, r_t: [-925.804 -925.804 -925.804], eps: 0.01})
Step:   15700, Reward: [-422.371 -422.371 -422.371] [72.372], Avg: [-451.732 -451.732 -451.732] (0.2072) ({r_i: None, r_t: [-810.482 -810.482 -810.482], eps: 0.207})
Step:   97700, Reward: [-783.750 -783.750 -783.750] [208.893], Avg: [-598.975 -598.975 -598.975] (0.1000) ({r_i: None, r_t: [-1444.461 -1444.461 -1444.461], eps: 0.1})
Step:   64600, Reward: [-444.578 -444.578 -444.578] [104.583], Avg: [-938.396 -938.396 -938.396] (0.0100) ({r_i: None, r_t: [-925.176 -925.176 -925.176], eps: 0.01})
Step:   97800, Reward: [-858.378 -858.378 -858.378] [292.559], Avg: [-599.240 -599.240 -599.240] (0.1000) ({r_i: None, r_t: [-1550.620 -1550.620 -1550.620], eps: 0.1})
Step:   64700, Reward: [-459.832 -459.832 -459.832] [99.255], Avg: [-937.657 -937.657 -937.657] (0.0100) ({r_i: None, r_t: [-919.225 -919.225 -919.225], eps: 0.01})
Step:   15800, Reward: [-421.851 -421.851 -421.851] [63.080], Avg: [-451.544 -451.544 -451.544] (0.2052) ({r_i: None, r_t: [-797.361 -797.361 -797.361], eps: 0.205})
Step:   97900, Reward: [-778.669 -778.669 -778.669] [173.129], Avg: [-599.423 -599.423 -599.423] (0.1000) ({r_i: None, r_t: [-1643.435 -1643.435 -1643.435], eps: 0.1})
Step:   64800, Reward: [-437.296 -437.296 -437.296] [88.461], Avg: [-936.886 -936.886 -936.886] (0.0100) ({r_i: None, r_t: [-882.539 -882.539 -882.539], eps: 0.01})
Step:   98000, Reward: [-691.477 -691.477 -691.477] [198.466], Avg: [-599.517 -599.517 -599.517] (0.1000) ({r_i: None, r_t: [-1650.747 -1650.747 -1650.747], eps: 0.1})
Step:   15900, Reward: [-392.750 -392.750 -392.750] [37.563], Avg: [-451.177 -451.177 -451.177] (0.2031) ({r_i: None, r_t: [-824.359 -824.359 -824.359], eps: 0.203})
Step:   64900, Reward: [-473.877 -473.877 -473.877] [99.209], Avg: [-936.174 -936.174 -936.174] (0.0100) ({r_i: None, r_t: [-887.079 -887.079 -887.079], eps: 0.01})
Step:   98100, Reward: [-741.070 -741.070 -741.070] [203.203], Avg: [-599.661 -599.661 -599.661] (0.1000) ({r_i: None, r_t: [-1617.865 -1617.865 -1617.865], eps: 0.1})
Step:   16000, Reward: [-416.303 -416.303 -416.303] [59.720], Avg: [-450.960 -450.960 -450.960] (0.2011) ({r_i: None, r_t: [-847.211 -847.211 -847.211], eps: 0.201})
Step:   65000, Reward: [-504.579 -504.579 -504.579] [77.563], Avg: [-935.511 -935.511 -935.511] (0.0100) ({r_i: None, r_t: [-905.590 -905.590 -905.590], eps: 0.01})
Step:   98200, Reward: [-789.107 -789.107 -789.107] [217.661], Avg: [-599.854 -599.854 -599.854] (0.1000) ({r_i: None, r_t: [-1720.687 -1720.687 -1720.687], eps: 0.1})
Step:   65100, Reward: [-479.754 -479.754 -479.754] [82.964], Avg: [-934.812 -934.812 -934.812] (0.0100) ({r_i: None, r_t: [-907.242 -907.242 -907.242], eps: 0.01})
Step:   98300, Reward: [-855.879 -855.879 -855.879] [291.872], Avg: [-600.114 -600.114 -600.114] (0.1000) ({r_i: None, r_t: [-1602.886 -1602.886 -1602.886], eps: 0.1})
Step:   16100, Reward: [-394.934 -394.934 -394.934] [59.424], Avg: [-450.614 -450.614 -450.614] (0.1991) ({r_i: None, r_t: [-805.967 -805.967 -805.967], eps: 0.199})
Step:   65200, Reward: [-422.308 -422.308 -422.308] [94.962], Avg: [-934.027 -934.027 -934.027] (0.0100) ({r_i: None, r_t: [-868.964 -868.964 -868.964], eps: 0.01})
Step:   98400, Reward: [-706.601 -706.601 -706.601] [172.309], Avg: [-600.222 -600.222 -600.222] (0.1000) ({r_i: None, r_t: [-1513.147 -1513.147 -1513.147], eps: 0.1})
Step:   16200, Reward: [-406.308 -406.308 -406.308] [58.425], Avg: [-450.342 -450.342 -450.342] (0.1971) ({r_i: None, r_t: [-823.769 -823.769 -823.769], eps: 0.197})
Step:   65300, Reward: [-420.666 -420.666 -420.666] [82.055], Avg: [-933.242 -933.242 -933.242] (0.0100) ({r_i: None, r_t: [-930.048 -930.048 -930.048], eps: 0.01})
Step:   98500, Reward: [-774.860 -774.860 -774.860] [205.795], Avg: [-600.399 -600.399 -600.399] (0.1000) ({r_i: None, r_t: [-1613.713 -1613.713 -1613.713], eps: 0.1})
Step:   65400, Reward: [-454.247 -454.247 -454.247] [82.921], Avg: [-932.511 -932.511 -932.511] (0.0100) ({r_i: None, r_t: [-883.796 -883.796 -883.796], eps: 0.01})
Step:   98600, Reward: [-755.609 -755.609 -755.609] [139.455], Avg: [-600.556 -600.556 -600.556] (0.1000) ({r_i: None, r_t: [-1376.785 -1376.785 -1376.785], eps: 0.1})
Step:   16300, Reward: [-401.884 -401.884 -401.884] [56.884], Avg: [-450.047 -450.047 -450.047] (0.1951) ({r_i: None, r_t: [-815.289 -815.289 -815.289], eps: 0.195})
Step:   65500, Reward: [-437.845 -437.845 -437.845] [85.861], Avg: [-931.757 -931.757 -931.757] (0.0100) ({r_i: None, r_t: [-893.149 -893.149 -893.149], eps: 0.01})
Step:   98700, Reward: [-802.855 -802.855 -802.855] [300.761], Avg: [-600.761 -600.761 -600.761] (0.1000) ({r_i: None, r_t: [-1729.312 -1729.312 -1729.312], eps: 0.1})
Step:   16400, Reward: [-390.583 -390.583 -390.583] [43.812], Avg: [-449.686 -449.686 -449.686] (0.1932) ({r_i: None, r_t: [-788.669 -788.669 -788.669], eps: 0.193})
Step:   65600, Reward: [-439.725 -439.725 -439.725] [95.210], Avg: [-931.008 -931.008 -931.008] (0.0100) ({r_i: None, r_t: [-908.451 -908.451 -908.451], eps: 0.01})
Step:   98800, Reward: [-857.011 -857.011 -857.011] [273.250], Avg: [-601.020 -601.020 -601.020] (0.1000) ({r_i: None, r_t: [-1495.343 -1495.343 -1495.343], eps: 0.1})
Step:   65700, Reward: [-428.630 -428.630 -428.630] [62.749], Avg: [-930.244 -930.244 -930.244] (0.0100) ({r_i: None, r_t: [-929.684 -929.684 -929.684], eps: 0.01})
Step:   98900, Reward: [-748.130 -748.130 -748.130] [217.571], Avg: [-601.169 -601.169 -601.169] (0.1000) ({r_i: None, r_t: [-1318.736 -1318.736 -1318.736], eps: 0.1})
Step:   16500, Reward: [-395.530 -395.530 -395.530] [66.553], Avg: [-449.360 -449.360 -449.360] (0.1913) ({r_i: None, r_t: [-802.829 -802.829 -802.829], eps: 0.191})
Step:   65800, Reward: [-457.527 -457.527 -457.527] [75.260], Avg: [-929.527 -929.527 -929.527] (0.0100) ({r_i: None, r_t: [-898.851 -898.851 -898.851], eps: 0.01})
Step:   99000, Reward: [-757.656 -757.656 -757.656] [279.611], Avg: [-601.327 -601.327 -601.327] (0.1000) ({r_i: None, r_t: [-1451.564 -1451.564 -1451.564], eps: 0.1})
Step:   16600, Reward: [-406.116 -406.116 -406.116] [73.350], Avg: [-449.101 -449.101 -449.101] (0.1893) ({r_i: None, r_t: [-785.562 -785.562 -785.562], eps: 0.189})
Step:   65900, Reward: [-441.142 -441.142 -441.142] [88.242], Avg: [-928.787 -928.787 -928.787] (0.0100) ({r_i: None, r_t: [-914.717 -914.717 -914.717], eps: 0.01})
Step:   99100, Reward: [-731.407 -731.407 -731.407] [193.707], Avg: [-601.458 -601.458 -601.458] (0.1000) ({r_i: None, r_t: [-1649.949 -1649.949 -1649.949], eps: 0.1})
Step:   66000, Reward: [-499.041 -499.041 -499.041] [114.643], Avg: [-928.137 -928.137 -928.137] (0.0100) ({r_i: None, r_t: [-954.140 -954.140 -954.140], eps: 0.01})
Step:   99200, Reward: [-714.771 -714.771 -714.771] [215.241], Avg: [-601.572 -601.572 -601.572] (0.1000) ({r_i: None, r_t: [-1383.510 -1383.510 -1383.510], eps: 0.1})
Step:   16700, Reward: [-389.604 -389.604 -389.604] [44.155], Avg: [-448.747 -448.747 -448.747] (0.1875) ({r_i: None, r_t: [-774.780 -774.780 -774.780], eps: 0.187})
Step:   99300, Reward: [-774.918 -774.918 -774.918] [256.419], Avg: [-601.746 -601.746 -601.746] (0.1000) ({r_i: None, r_t: [-1612.176 -1612.176 -1612.176], eps: 0.1})
Step:   66100, Reward: [-398.052 -398.052 -398.052] [77.351], Avg: [-927.336 -927.336 -927.336] (0.0100) ({r_i: None, r_t: [-934.308 -934.308 -934.308], eps: 0.01})
Step:   16800, Reward: [-379.984 -379.984 -379.984] [57.339], Avg: [-448.340 -448.340 -448.340] (0.1856) ({r_i: None, r_t: [-828.029 -828.029 -828.029], eps: 0.186})
Step:   99400, Reward: [-737.153 -737.153 -737.153] [200.030], Avg: [-601.882 -601.882 -601.882] (0.1000) ({r_i: None, r_t: [-1506.176 -1506.176 -1506.176], eps: 0.1})
Step:   66200, Reward: [-489.471 -489.471 -489.471] [120.069], Avg: [-926.676 -926.676 -926.676] (0.0100) ({r_i: None, r_t: [-977.320 -977.320 -977.320], eps: 0.01})
Step:   99500, Reward: [-630.051 -630.051 -630.051] [142.661], Avg: [-601.911 -601.911 -601.911] (0.1000) ({r_i: None, r_t: [-1503.093 -1503.093 -1503.093], eps: 0.1})
Step:   66300, Reward: [-475.009 -475.009 -475.009] [120.013], Avg: [-925.996 -925.996 -925.996] (0.0100) ({r_i: None, r_t: [-943.690 -943.690 -943.690], eps: 0.01})
Step:   16900, Reward: [-438.624 -438.624 -438.624] [61.858], Avg: [-448.283 -448.283 -448.283] (0.1837) ({r_i: None, r_t: [-794.558 -794.558 -794.558], eps: 0.184})
Step:   99600, Reward: [-731.782 -731.782 -731.782] [266.521], Avg: [-602.041 -602.041 -602.041] (0.1000) ({r_i: None, r_t: [-1489.162 -1489.162 -1489.162], eps: 0.1})
Step:   66400, Reward: [-467.780 -467.780 -467.780] [99.225], Avg: [-925.307 -925.307 -925.307] (0.0100) ({r_i: None, r_t: [-934.365 -934.365 -934.365], eps: 0.01})
Step:   17000, Reward: [-399.729 -399.729 -399.729] [68.388], Avg: [-447.999 -447.999 -447.999] (0.1819) ({r_i: None, r_t: [-795.524 -795.524 -795.524], eps: 0.182})
Step:   99700, Reward: [-824.901 -824.901 -824.901] [194.090], Avg: [-602.264 -602.264 -602.264] (0.1000) ({r_i: None, r_t: [-1513.674 -1513.674 -1513.674], eps: 0.1})
Step:   66500, Reward: [-511.276 -511.276 -511.276] [117.949], Avg: [-924.685 -924.685 -924.685] (0.0100) ({r_i: None, r_t: [-963.669 -963.669 -963.669], eps: 0.01})
Step:   99800, Reward: [-781.068 -781.068 -781.068] [238.033], Avg: [-602.443 -602.443 -602.443] (0.1000) ({r_i: None, r_t: [-1411.949 -1411.949 -1411.949], eps: 0.1})
Step:   66600, Reward: [-533.994 -533.994 -533.994] [100.266], Avg: [-924.099 -924.099 -924.099] (0.0100) ({r_i: None, r_t: [-918.490 -918.490 -918.490], eps: 0.01})
Step:   17100, Reward: [-393.852 -393.852 -393.852] [64.646], Avg: [-447.684 -447.684 -447.684] (0.1801) ({r_i: None, r_t: [-779.582 -779.582 -779.582], eps: 0.18})
Step:   99900, Reward: [-704.228 -704.228 -704.228] [193.152], Avg: [-602.545 -602.545 -602.545] (0.1000) ({r_i: None, r_t: [-1461.857 -1461.857 -1461.857], eps: 0.1})
Step:   66700, Reward: [-498.278 -498.278 -498.278] [72.089], Avg: [-923.462 -923.462 -923.462] (0.0100) ({r_i: None, r_t: [-1046.889 -1046.889 -1046.889], eps: 0.01})
Step:   17200, Reward: [-406.656 -406.656 -406.656] [46.167], Avg: [-447.447 -447.447 -447.447] (0.1783) ({r_i: None, r_t: [-816.180 -816.180 -816.180], eps: 0.178})
Step:  100000, Reward: [-667.210 -667.210 -667.210] [170.720], Avg: [-602.609 -602.609 -602.609] (0.1000) ({r_i: None, r_t: [-1472.391 -1472.391 -1472.391], eps: 0.1})
Step:   66800, Reward: [-460.019 -460.019 -460.019] [92.161], Avg: [-922.769 -922.769 -922.769] (0.0100) ({r_i: None, r_t: [-991.894 -991.894 -991.894], eps: 0.01})
Step:  100100, Reward: [-668.757 -668.757 -668.757] [202.469], Avg: [-602.676 -602.676 -602.676] (0.1000) ({r_i: None, r_t: [-1390.544 -1390.544 -1390.544], eps: 0.1})
Step:   66900, Reward: [-493.506 -493.506 -493.506] [96.793], Avg: [-922.128 -922.128 -922.128] (0.0100) ({r_i: None, r_t: [-1002.209 -1002.209 -1002.209], eps: 0.01})
Step:   17300, Reward: [-398.435 -398.435 -398.435] [61.480], Avg: [-447.166 -447.166 -447.166] (0.1765) ({r_i: None, r_t: [-797.668 -797.668 -797.668], eps: 0.177})
Step:  100200, Reward: [-771.495 -771.495 -771.495] [334.478], Avg: [-602.844 -602.844 -602.844] (0.1000) ({r_i: None, r_t: [-1418.945 -1418.945 -1418.945], eps: 0.1})
Step:   67000, Reward: [-530.872 -530.872 -530.872] [122.939], Avg: [-921.545 -921.545 -921.545] (0.0100) ({r_i: None, r_t: [-975.130 -975.130 -975.130], eps: 0.01})
Step:   17400, Reward: [-394.968 -394.968 -394.968] [80.403], Avg: [-446.867 -446.867 -446.867] (0.1748) ({r_i: None, r_t: [-806.788 -806.788 -806.788], eps: 0.175})
Step:  100300, Reward: [-679.581 -679.581 -679.581] [205.495], Avg: [-602.920 -602.920 -602.920] (0.1000) ({r_i: None, r_t: [-1505.602 -1505.602 -1505.602], eps: 0.1})
Step:   67100, Reward: [-569.836 -569.836 -569.836] [140.086], Avg: [-921.022 -921.022 -921.022] (0.0100) ({r_i: None, r_t: [-1111.241 -1111.241 -1111.241], eps: 0.01})
Step:  100400, Reward: [-750.712 -750.712 -750.712] [212.647], Avg: [-603.067 -603.067 -603.067] (0.1000) ({r_i: None, r_t: [-1431.309 -1431.309 -1431.309], eps: 0.1})
Step:   67200, Reward: [-489.093 -489.093 -489.093] [72.543], Avg: [-920.380 -920.380 -920.380] (0.0100) ({r_i: None, r_t: [-1048.238 -1048.238 -1048.238], eps: 0.01})
Step:   17500, Reward: [-389.541 -389.541 -389.541] [46.218], Avg: [-446.542 -446.542 -446.542] (0.1730) ({r_i: None, r_t: [-801.817 -801.817 -801.817], eps: 0.173})
Step:  100500, Reward: [-626.674 -626.674 -626.674] [181.901], Avg: [-603.091 -603.091 -603.091] (0.1000) ({r_i: None, r_t: [-1443.446 -1443.446 -1443.446], eps: 0.1})
Step:   67300, Reward: [-496.077 -496.077 -496.077] [105.460], Avg: [-919.750 -919.750 -919.750] (0.0100) ({r_i: None, r_t: [-1000.951 -1000.951 -1000.951], eps: 0.01})
Step:   17600, Reward: [-383.614 -383.614 -383.614] [54.130], Avg: [-446.186 -446.186 -446.186] (0.1713) ({r_i: None, r_t: [-826.640 -826.640 -826.640], eps: 0.171})
Step:  100600, Reward: [-645.053 -645.053 -645.053] [148.952], Avg: [-603.132 -603.132 -603.132] (0.1000) ({r_i: None, r_t: [-1357.233 -1357.233 -1357.233], eps: 0.1})
Step:   67400, Reward: [-537.115 -537.115 -537.115] [117.246], Avg: [-919.184 -919.184 -919.184] (0.0100) ({r_i: None, r_t: [-1072.267 -1072.267 -1072.267], eps: 0.01})
Step:  100700, Reward: [-651.097 -651.097 -651.097] [165.091], Avg: [-603.180 -603.180 -603.180] (0.1000) ({r_i: None, r_t: [-1402.850 -1402.850 -1402.850], eps: 0.1})
Step:   17700, Reward: [-398.890 -398.890 -398.890] [73.829], Avg: [-445.920 -445.920 -445.920] (0.1696) ({r_i: None, r_t: [-789.367 -789.367 -789.367], eps: 0.17})
Step:   67500, Reward: [-528.888 -528.888 -528.888] [123.851], Avg: [-918.606 -918.606 -918.606] (0.0100) ({r_i: None, r_t: [-1033.382 -1033.382 -1033.382], eps: 0.01})
Step:  100800, Reward: [-700.407 -700.407 -700.407] [214.466], Avg: [-603.276 -603.276 -603.276] (0.1000) ({r_i: None, r_t: [-1463.996 -1463.996 -1463.996], eps: 0.1})
Step:   67600, Reward: [-568.642 -568.642 -568.642] [120.154], Avg: [-918.089 -918.089 -918.089] (0.0100) ({r_i: None, r_t: [-1033.086 -1033.086 -1033.086], eps: 0.01})
Step:   17800, Reward: [-415.082 -415.082 -415.082] [60.161], Avg: [-445.748 -445.748 -445.748] (0.1679) ({r_i: None, r_t: [-775.530 -775.530 -775.530], eps: 0.168})
Step:  100900, Reward: [-794.386 -794.386 -794.386] [272.332], Avg: [-603.466 -603.466 -603.466] (0.1000) ({r_i: None, r_t: [-1389.787 -1389.787 -1389.787], eps: 0.1})
Step:   67700, Reward: [-547.467 -547.467 -547.467] [162.122], Avg: [-917.543 -917.543 -917.543] (0.0100) ({r_i: None, r_t: [-1181.875 -1181.875 -1181.875], eps: 0.01})
Step:  101000, Reward: [-728.884 -728.884 -728.884] [250.004], Avg: [-603.590 -603.590 -603.590] (0.1000) ({r_i: None, r_t: [-1320.277 -1320.277 -1320.277], eps: 0.1})
Step:   17900, Reward: [-433.795 -433.795 -433.795] [58.536], Avg: [-445.682 -445.682 -445.682] (0.1662) ({r_i: None, r_t: [-794.223 -794.223 -794.223], eps: 0.166})
Step:   67800, Reward: [-570.538 -570.538 -570.538] [102.633], Avg: [-917.032 -917.032 -917.032] (0.0100) ({r_i: None, r_t: [-1082.635 -1082.635 -1082.635], eps: 0.01})
Step:  101100, Reward: [-816.680 -816.680 -816.680] [290.318], Avg: [-603.800 -603.800 -603.800] (0.1000) ({r_i: None, r_t: [-1297.050 -1297.050 -1297.050], eps: 0.1})
Step:   67900, Reward: [-600.268 -600.268 -600.268] [238.845], Avg: [-916.566 -916.566 -916.566] (0.0100) ({r_i: None, r_t: [-1165.138 -1165.138 -1165.138], eps: 0.01})
Step:   18000, Reward: [-410.104 -410.104 -410.104] [41.044], Avg: [-445.485 -445.485 -445.485] (0.1646) ({r_i: None, r_t: [-792.025 -792.025 -792.025], eps: 0.165})
Step:  101200, Reward: [-779.410 -779.410 -779.410] [269.045], Avg: [-603.974 -603.974 -603.974] (0.1000) ({r_i: None, r_t: [-1434.005 -1434.005 -1434.005], eps: 0.1})
Step:   68000, Reward: [-574.928 -574.928 -574.928] [129.808], Avg: [-916.064 -916.064 -916.064] (0.0100) ({r_i: None, r_t: [-1115.731 -1115.731 -1115.731], eps: 0.01})
Step:  101300, Reward: [-724.758 -724.758 -724.758] [246.981], Avg: [-604.093 -604.093 -604.093] (0.1000) ({r_i: None, r_t: [-1256.155 -1256.155 -1256.155], eps: 0.1})
Step:   18100, Reward: [-377.579 -377.579 -377.579] [41.577], Avg: [-445.112 -445.112 -445.112] (0.1629) ({r_i: None, r_t: [-822.210 -822.210 -822.210], eps: 0.163})
Step:   68100, Reward: [-620.790 -620.790 -620.790] [186.642], Avg: [-915.631 -915.631 -915.631] (0.0100) ({r_i: None, r_t: [-1109.550 -1109.550 -1109.550], eps: 0.01})
Step:  101400, Reward: [-758.444 -758.444 -758.444] [224.141], Avg: [-604.245 -604.245 -604.245] (0.1000) ({r_i: None, r_t: [-1316.355 -1316.355 -1316.355], eps: 0.1})
Step:   68200, Reward: [-622.427 -622.427 -622.427] [118.501], Avg: [-915.202 -915.202 -915.202] (0.0100) ({r_i: None, r_t: [-1119.640 -1119.640 -1119.640], eps: 0.01})
Step:  101500, Reward: [-661.876 -661.876 -661.876] [235.661], Avg: [-604.301 -604.301 -604.301] (0.1000) ({r_i: None, r_t: [-1487.666 -1487.666 -1487.666], eps: 0.1})
Step:   18200, Reward: [-437.560 -437.560 -437.560] [51.515], Avg: [-445.071 -445.071 -445.071] (0.1613) ({r_i: None, r_t: [-801.032 -801.032 -801.032], eps: 0.161})
Step:   68300, Reward: [-634.426 -634.426 -634.426] [188.392], Avg: [-914.791 -914.791 -914.791] (0.0100) ({r_i: None, r_t: [-1157.004 -1157.004 -1157.004], eps: 0.01})
Step:  101600, Reward: [-638.856 -638.856 -638.856] [163.757], Avg: [-604.335 -604.335 -604.335] (0.1000) ({r_i: None, r_t: [-1501.002 -1501.002 -1501.002], eps: 0.1})
Step:   18300, Reward: [-405.793 -405.793 -405.793] [48.873], Avg: [-444.857 -444.857 -444.857] (0.1597) ({r_i: None, r_t: [-821.283 -821.283 -821.283], eps: 0.16})
Step:   68400, Reward: [-591.459 -591.459 -591.459] [188.231], Avg: [-914.319 -914.319 -914.319] (0.0100) ({r_i: None, r_t: [-1227.846 -1227.846 -1227.846], eps: 0.01})
Step:  101700, Reward: [-594.825 -594.825 -594.825] [146.305], Avg: [-604.326 -604.326 -604.326] (0.1000) ({r_i: None, r_t: [-1317.209 -1317.209 -1317.209], eps: 0.1})
Step:   68500, Reward: [-640.030 -640.030 -640.030] [200.684], Avg: [-913.920 -913.920 -913.920] (0.0100) ({r_i: None, r_t: [-1348.555 -1348.555 -1348.555], eps: 0.01})
Step:  101800, Reward: [-721.634 -721.634 -721.634] [174.506], Avg: [-604.441 -604.441 -604.441] (0.1000) ({r_i: None, r_t: [-1366.818 -1366.818 -1366.818], eps: 0.1})
Step:   18400, Reward: [-364.347 -364.347 -364.347] [42.359], Avg: [-444.422 -444.422 -444.422] (0.1581) ({r_i: None, r_t: [-818.996 -818.996 -818.996], eps: 0.158})
Step:   68600, Reward: [-628.718 -628.718 -628.718] [171.938], Avg: [-913.504 -913.504 -913.504] (0.0100) ({r_i: None, r_t: [-1397.617 -1397.617 -1397.617], eps: 0.01})
Step:  101900, Reward: [-641.068 -641.068 -641.068] [155.996], Avg: [-604.477 -604.477 -604.477] (0.1000) ({r_i: None, r_t: [-1474.140 -1474.140 -1474.140], eps: 0.1})
Step:   18500, Reward: [-420.455 -420.455 -420.455] [59.283], Avg: [-444.293 -444.293 -444.293] (0.1565) ({r_i: None, r_t: [-829.002 -829.002 -829.002], eps: 0.157})
Step:   68700, Reward: [-559.343 -559.343 -559.343] [238.723], Avg: [-912.990 -912.990 -912.990] (0.0100) ({r_i: None, r_t: [-1174.086 -1174.086 -1174.086], eps: 0.01})
Step:  102000, Reward: [-730.182 -730.182 -730.182] [217.972], Avg: [-604.600 -604.600 -604.600] (0.1000) ({r_i: None, r_t: [-1495.902 -1495.902 -1495.902], eps: 0.1})
Step:   68800, Reward: [-706.204 -706.204 -706.204] [504.097], Avg: [-912.689 -912.689 -912.689] (0.0100) ({r_i: None, r_t: [-1278.799 -1278.799 -1278.799], eps: 0.01})
Step:  102100, Reward: [-623.489 -623.489 -623.489] [261.819], Avg: [-604.619 -604.619 -604.619] (0.1000) ({r_i: None, r_t: [-1293.378 -1293.378 -1293.378], eps: 0.1})
Step:   18600, Reward: [-390.820 -390.820 -390.820] [58.274], Avg: [-444.007 -444.007 -444.007] (0.1549) ({r_i: None, r_t: [-778.014 -778.014 -778.014], eps: 0.155})
Step:   68900, Reward: [-741.521 -741.521 -741.521] [338.948], Avg: [-912.441 -912.441 -912.441] (0.0100) ({r_i: None, r_t: [-1336.979 -1336.979 -1336.979], eps: 0.01})
Step:  102200, Reward: [-712.556 -712.556 -712.556] [208.727], Avg: [-604.724 -604.724 -604.724] (0.1000) ({r_i: None, r_t: [-1429.259 -1429.259 -1429.259], eps: 0.1})
Step:   18700, Reward: [-413.737 -413.737 -413.737] [54.906], Avg: [-443.846 -443.846 -443.846] (0.1534) ({r_i: None, r_t: [-811.067 -811.067 -811.067], eps: 0.153})
Step:  102300, Reward: [-714.085 -714.085 -714.085] [254.827], Avg: [-604.831 -604.831 -604.831] (0.1000) ({r_i: None, r_t: [-1423.106 -1423.106 -1423.106], eps: 0.1})
Step:   69000, Reward: [-833.686 -833.686 -833.686] [520.359], Avg: [-912.327 -912.327 -912.327] (0.0100) ({r_i: None, r_t: [-1743.556 -1743.556 -1743.556], eps: 0.01})
Step:  102400, Reward: [-662.066 -662.066 -662.066] [205.331], Avg: [-604.887 -604.887 -604.887] (0.1000) ({r_i: None, r_t: [-1290.719 -1290.719 -1290.719], eps: 0.1})
Step:   69100, Reward: [-730.179 -730.179 -730.179] [312.470], Avg: [-912.064 -912.064 -912.064] (0.0100) ({r_i: None, r_t: [-1457.262 -1457.262 -1457.262], eps: 0.01})
Step:   18800, Reward: [-418.798 -418.798 -418.798] [59.991], Avg: [-443.714 -443.714 -443.714] (0.1519) ({r_i: None, r_t: [-778.301 -778.301 -778.301], eps: 0.152})
Step:  102500, Reward: [-680.062 -680.062 -680.062] [148.594], Avg: [-604.960 -604.960 -604.960] (0.1000) ({r_i: None, r_t: [-1368.466 -1368.466 -1368.466], eps: 0.1})
Step:   69200, Reward: [-832.673 -832.673 -832.673] [470.056], Avg: [-911.950 -911.950 -911.950] (0.0100) ({r_i: None, r_t: [-1773.965 -1773.965 -1773.965], eps: 0.01})
Step:   18900, Reward: [-409.516 -409.516 -409.516] [51.794], Avg: [-443.534 -443.534 -443.534] (0.1504) ({r_i: None, r_t: [-770.163 -770.163 -770.163], eps: 0.15})
Step:  102600, Reward: [-608.659 -608.659 -608.659] [190.491], Avg: [-604.964 -604.964 -604.964] (0.1000) ({r_i: None, r_t: [-1366.263 -1366.263 -1366.263], eps: 0.1})
Step:   69300, Reward: [-780.042 -780.042 -780.042] [507.588], Avg: [-911.760 -911.760 -911.760] (0.0100) ({r_i: None, r_t: [-1344.915 -1344.915 -1344.915], eps: 0.01})
Step:  102700, Reward: [-691.629 -691.629 -691.629] [293.617], Avg: [-605.048 -605.048 -605.048] (0.1000) ({r_i: None, r_t: [-1412.704 -1412.704 -1412.704], eps: 0.1})
Step:   19000, Reward: [-416.650 -416.650 -416.650] [78.667], Avg: [-443.393 -443.393 -443.393] (0.1489) ({r_i: None, r_t: [-791.033 -791.033 -791.033], eps: 0.149})
Step:   69400, Reward: [-783.385 -783.385 -783.385] [416.063], Avg: [-911.575 -911.575 -911.575] (0.0100) ({r_i: None, r_t: [-1551.188 -1551.188 -1551.188], eps: 0.01})
Step:  102800, Reward: [-720.347 -720.347 -720.347] [216.162], Avg: [-605.160 -605.160 -605.160] (0.1000) ({r_i: None, r_t: [-1344.683 -1344.683 -1344.683], eps: 0.1})
Step:   69500, Reward: [-1004.918 -1004.918 -1004.918] [601.237], Avg: [-911.709 -911.709 -911.709] (0.0100) ({r_i: None, r_t: [-1694.988 -1694.988 -1694.988], eps: 0.01})
Step:   19100, Reward: [-412.954 -412.954 -412.954] [68.148], Avg: [-443.234 -443.234 -443.234] (0.1474) ({r_i: None, r_t: [-820.064 -820.064 -820.064], eps: 0.147})
Step:  102900, Reward: [-695.465 -695.465 -695.465] [231.080], Avg: [-605.248 -605.248 -605.248] (0.1000) ({r_i: None, r_t: [-1541.340 -1541.340 -1541.340], eps: 0.1})
Step:   69600, Reward: [-922.155 -922.155 -922.155] [419.806], Avg: [-911.724 -911.724 -911.724] (0.0100) ({r_i: None, r_t: [-2144.014 -2144.014 -2144.014], eps: 0.01})
Step:  103000, Reward: [-652.847 -652.847 -652.847] [222.351], Avg: [-605.294 -605.294 -605.294] (0.1000) ({r_i: None, r_t: [-1372.244 -1372.244 -1372.244], eps: 0.1})
Step:   69700, Reward: [-1140.118 -1140.118 -1140.118] [545.251], Avg: [-912.051 -912.051 -912.051] (0.0100) ({r_i: None, r_t: [-1747.375 -1747.375 -1747.375], eps: 0.01})
Step:   19200, Reward: [-406.937 -406.937 -406.937] [69.110], Avg: [-443.046 -443.046 -443.046] (0.1459) ({r_i: None, r_t: [-837.444 -837.444 -837.444], eps: 0.146})
Step:  103100, Reward: [-720.939 -720.939 -720.939] [269.985], Avg: [-605.406 -605.406 -605.406] (0.1000) ({r_i: None, r_t: [-1393.861 -1393.861 -1393.861], eps: 0.1})
Step:   69800, Reward: [-1138.843 -1138.843 -1138.843] [555.983], Avg: [-912.376 -912.376 -912.376] (0.0100) ({r_i: None, r_t: [-2099.633 -2099.633 -2099.633], eps: 0.01})
Step:   19300, Reward: [-406.423 -406.423 -406.423] [65.949], Avg: [-442.858 -442.858 -442.858] (0.1444) ({r_i: None, r_t: [-814.723 -814.723 -814.723], eps: 0.144})
Step:  103200, Reward: [-678.326 -678.326 -678.326] [172.112], Avg: [-605.477 -605.477 -605.477] (0.1000) ({r_i: None, r_t: [-1324.113 -1324.113 -1324.113], eps: 0.1})
Step:   69900, Reward: [-1016.013 -1016.013 -1016.013] [435.152], Avg: [-912.524 -912.524 -912.524] (0.0100) ({r_i: None, r_t: [-2568.636 -2568.636 -2568.636], eps: 0.01})
Step:  103300, Reward: [-653.813 -653.813 -653.813] [270.900], Avg: [-605.523 -605.523 -605.523] (0.1000) ({r_i: None, r_t: [-1470.775 -1470.775 -1470.775], eps: 0.1})
Step:   19400, Reward: [-398.878 -398.878 -398.878] [58.797], Avg: [-442.632 -442.632 -442.632] (0.1430) ({r_i: None, r_t: [-817.495 -817.495 -817.495], eps: 0.143})
Step:   70000, Reward: [-1198.887 -1198.887 -1198.887] [418.188], Avg: [-912.932 -912.932 -912.932] (0.0100) ({r_i: None, r_t: [-2194.474 -2194.474 -2194.474], eps: 0.01})
Step:  103400, Reward: [-716.531 -716.531 -716.531] [180.814], Avg: [-605.631 -605.631 -605.631] (0.1000) ({r_i: None, r_t: [-1423.953 -1423.953 -1423.953], eps: 0.1})
Step:   70100, Reward: [-1249.799 -1249.799 -1249.799] [603.554], Avg: [-913.412 -913.412 -913.412] (0.0100) ({r_i: None, r_t: [-2562.320 -2562.320 -2562.320], eps: 0.01})
Step:   19500, Reward: [-389.732 -389.732 -389.732] [41.399], Avg: [-442.362 -442.362 -442.362] (0.1416) ({r_i: None, r_t: [-782.599 -782.599 -782.599], eps: 0.142})
Step:  103500, Reward: [-641.863 -641.863 -641.863] [160.817], Avg: [-605.666 -605.666 -605.666] (0.1000) ({r_i: None, r_t: [-1569.613 -1569.613 -1569.613], eps: 0.1})
Step:   70200, Reward: [-1577.867 -1577.867 -1577.867] [432.623], Avg: [-914.357 -914.357 -914.357] (0.0100) ({r_i: None, r_t: [-2798.816 -2798.816 -2798.816], eps: 0.01})
Step:  103600, Reward: [-714.426 -714.426 -714.426] [202.875], Avg: [-605.770 -605.770 -605.770] (0.1000) ({r_i: None, r_t: [-1346.941 -1346.941 -1346.941], eps: 0.1})
Step:   19600, Reward: [-381.720 -381.720 -381.720] [47.351], Avg: [-442.054 -442.054 -442.054] (0.1402) ({r_i: None, r_t: [-798.706 -798.706 -798.706], eps: 0.14})
Step:   70300, Reward: [-1310.087 -1310.087 -1310.087] [454.259], Avg: [-914.919 -914.919 -914.919] (0.0100) ({r_i: None, r_t: [-2602.307 -2602.307 -2602.307], eps: 0.01})
Step:  103700, Reward: [-706.330 -706.330 -706.330] [253.782], Avg: [-605.867 -605.867 -605.867] (0.1000) ({r_i: None, r_t: [-1365.517 -1365.517 -1365.517], eps: 0.1})
Step:   70400, Reward: [-1343.310 -1343.310 -1343.310] [481.993], Avg: [-915.527 -915.527 -915.527] (0.0100) ({r_i: None, r_t: [-2752.634 -2752.634 -2752.634], eps: 0.01})
Step:   19700, Reward: [-387.488 -387.488 -387.488] [39.652], Avg: [-441.779 -441.779 -441.779] (0.1388) ({r_i: None, r_t: [-820.985 -820.985 -820.985], eps: 0.139})
Step:  103800, Reward: [-672.672 -672.672 -672.672] [227.212], Avg: [-605.932 -605.932 -605.932] (0.1000) ({r_i: None, r_t: [-1354.231 -1354.231 -1354.231], eps: 0.1})
Step:   70500, Reward: [-1419.539 -1419.539 -1419.539] [627.542], Avg: [-916.241 -916.241 -916.241] (0.0100) ({r_i: None, r_t: [-2906.759 -2906.759 -2906.759], eps: 0.01})
Step:  103900, Reward: [-723.429 -723.429 -723.429] [281.880], Avg: [-606.045 -606.045 -606.045] (0.1000) ({r_i: None, r_t: [-1288.102 -1288.102 -1288.102], eps: 0.1})
Step:   19800, Reward: [-375.305 -375.305 -375.305] [70.467], Avg: [-441.445 -441.445 -441.445] (0.1374) ({r_i: None, r_t: [-812.918 -812.918 -812.918], eps: 0.137})
Step:   70600, Reward: [-1406.937 -1406.937 -1406.937] [432.982], Avg: [-916.935 -916.935 -916.935] (0.0100) ({r_i: None, r_t: [-2718.151 -2718.151 -2718.151], eps: 0.01})
Step:  104000, Reward: [-600.906 -600.906 -600.906] [110.953], Avg: [-606.040 -606.040 -606.040] (0.1000) ({r_i: None, r_t: [-1269.287 -1269.287 -1269.287], eps: 0.1})
Step:   70700, Reward: [-1431.786 -1431.786 -1431.786] [398.224], Avg: [-917.662 -917.662 -917.662] (0.0100) ({r_i: None, r_t: [-2634.786 -2634.786 -2634.786], eps: 0.01})
Step:   19900, Reward: [-419.680 -419.680 -419.680] [51.613], Avg: [-441.336 -441.336 -441.336] (0.1360) ({r_i: None, r_t: [-800.520 -800.520 -800.520], eps: 0.136})
Step:  104100, Reward: [-718.367 -718.367 -718.367] [222.381], Avg: [-606.147 -606.147 -606.147] (0.1000) ({r_i: None, r_t: [-1380.721 -1380.721 -1380.721], eps: 0.1})
Step:   70800, Reward: [-1287.487 -1287.487 -1287.487] [322.118], Avg: [-918.184 -918.184 -918.184] (0.0100) ({r_i: None, r_t: [-2982.174 -2982.174 -2982.174], eps: 0.01})
Step:  104200, Reward: [-735.709 -735.709 -735.709] [260.031], Avg: [-606.272 -606.272 -606.272] (0.1000) ({r_i: None, r_t: [-1424.774 -1424.774 -1424.774], eps: 0.1})
Step:   20000, Reward: [-356.843 -356.843 -356.843] [22.025], Avg: [-440.915 -440.915 -440.915] (0.1347) ({r_i: None, r_t: [-801.886 -801.886 -801.886], eps: 0.135})
Step:   70900, Reward: [-1338.784 -1338.784 -1338.784] [330.682], Avg: [-918.776 -918.776 -918.776] (0.0100) ({r_i: None, r_t: [-2719.222 -2719.222 -2719.222], eps: 0.01})
Step:  104300, Reward: [-584.497 -584.497 -584.497] [145.228], Avg: [-606.251 -606.251 -606.251] (0.1000) ({r_i: None, r_t: [-1414.475 -1414.475 -1414.475], eps: 0.1})
Step:   71000, Reward: [-1358.790 -1358.790 -1358.790] [452.484], Avg: [-919.395 -919.395 -919.395] (0.0100) ({r_i: None, r_t: [-2613.074 -2613.074 -2613.074], eps: 0.01})
Step:   20100, Reward: [-391.975 -391.975 -391.975] [68.360], Avg: [-440.673 -440.673 -440.673] (0.1333) ({r_i: None, r_t: [-803.256 -803.256 -803.256], eps: 0.133})
Step:  104400, Reward: [-627.514 -627.514 -627.514] [159.916], Avg: [-606.271 -606.271 -606.271] (0.1000) ({r_i: None, r_t: [-1357.669 -1357.669 -1357.669], eps: 0.1})
Step:   71100, Reward: [-1333.918 -1333.918 -1333.918] [418.240], Avg: [-919.977 -919.977 -919.977] (0.0100) ({r_i: None, r_t: [-2487.994 -2487.994 -2487.994], eps: 0.01})
Step:  104500, Reward: [-699.177 -699.177 -699.177] [231.948], Avg: [-606.360 -606.360 -606.360] (0.1000) ({r_i: None, r_t: [-1418.511 -1418.511 -1418.511], eps: 0.1})
Step:   20200, Reward: [-392.934 -392.934 -392.934] [63.591], Avg: [-440.438 -440.438 -440.438] (0.1320) ({r_i: None, r_t: [-839.801 -839.801 -839.801], eps: 0.132})
Step:   71200, Reward: [-1225.628 -1225.628 -1225.628] [357.270], Avg: [-920.406 -920.406 -920.406] (0.0100) ({r_i: None, r_t: [-2517.593 -2517.593 -2517.593], eps: 0.01})
Step:  104600, Reward: [-762.119 -762.119 -762.119] [233.182], Avg: [-606.509 -606.509 -606.509] (0.1000) ({r_i: None, r_t: [-1481.221 -1481.221 -1481.221], eps: 0.1})
Step:   71300, Reward: [-1308.628 -1308.628 -1308.628] [404.942], Avg: [-920.950 -920.950 -920.950] (0.0100) ({r_i: None, r_t: [-2394.667 -2394.667 -2394.667], eps: 0.01})
Step:   20300, Reward: [-381.807 -381.807 -381.807] [56.766], Avg: [-440.151 -440.151 -440.151] (0.1307) ({r_i: None, r_t: [-818.917 -818.917 -818.917], eps: 0.131})
Step:  104700, Reward: [-673.786 -673.786 -673.786] [190.308], Avg: [-606.573 -606.573 -606.573] (0.1000) ({r_i: None, r_t: [-1351.962 -1351.962 -1351.962], eps: 0.1})
Step:   71400, Reward: [-1449.488 -1449.488 -1449.488] [307.440], Avg: [-921.689 -921.689 -921.689] (0.0100) ({r_i: None, r_t: [-2518.886 -2518.886 -2518.886], eps: 0.01})
Step:  104800, Reward: [-648.764 -648.764 -648.764] [223.114], Avg: [-606.613 -606.613 -606.613] (0.1000) ({r_i: None, r_t: [-1282.347 -1282.347 -1282.347], eps: 0.1})
Step:   20400, Reward: [-405.512 -405.512 -405.512] [48.334], Avg: [-439.982 -439.982 -439.982] (0.1294) ({r_i: None, r_t: [-788.113 -788.113 -788.113], eps: 0.129})
Step:   71500, Reward: [-1063.487 -1063.487 -1063.487] [267.675], Avg: [-921.887 -921.887 -921.887] (0.0100) ({r_i: None, r_t: [-2291.261 -2291.261 -2291.261], eps: 0.01})
Step:  104900, Reward: [-701.690 -701.690 -701.690] [218.327], Avg: [-606.704 -606.704 -606.704] (0.1000) ({r_i: None, r_t: [-1315.393 -1315.393 -1315.393], eps: 0.1})
Step:   71600, Reward: [-1305.902 -1305.902 -1305.902] [318.293], Avg: [-922.422 -922.422 -922.422] (0.0100) ({r_i: None, r_t: [-2429.665 -2429.665 -2429.665], eps: 0.01})
Step:  105000, Reward: [-691.575 -691.575 -691.575] [316.745], Avg: [-606.785 -606.785 -606.785] (0.1000) ({r_i: None, r_t: [-1415.550 -1415.550 -1415.550], eps: 0.1})
Step:   20500, Reward: [-383.240 -383.240 -383.240] [61.326], Avg: [-439.706 -439.706 -439.706] (0.1281) ({r_i: None, r_t: [-787.200 -787.200 -787.200], eps: 0.128})
Step:   71700, Reward: [-1216.838 -1216.838 -1216.838] [324.874], Avg: [-922.833 -922.833 -922.833] (0.0100) ({r_i: None, r_t: [-2297.503 -2297.503 -2297.503], eps: 0.01})
Step:  105100, Reward: [-679.771 -679.771 -679.771] [174.384], Avg: [-606.854 -606.854 -606.854] (0.1000) ({r_i: None, r_t: [-1541.676 -1541.676 -1541.676], eps: 0.1})
Step:   20600, Reward: [-393.245 -393.245 -393.245] [63.584], Avg: [-439.482 -439.482 -439.482] (0.1268) ({r_i: None, r_t: [-774.504 -774.504 -774.504], eps: 0.127})
Step:  105200, Reward: [-663.048 -663.048 -663.048] [226.688], Avg: [-606.907 -606.907 -606.907] (0.1000) ({r_i: None, r_t: [-1342.711 -1342.711 -1342.711], eps: 0.1})
Step:   71800, Reward: [-1061.458 -1061.458 -1061.458] [285.674], Avg: [-923.025 -923.025 -923.025] (0.0100) ({r_i: None, r_t: [-2140.773 -2140.773 -2140.773], eps: 0.01})
Step:  105300, Reward: [-671.375 -671.375 -671.375] [211.655], Avg: [-606.968 -606.968 -606.968] (0.1000) ({r_i: None, r_t: [-1359.543 -1359.543 -1359.543], eps: 0.1})
Step:   71900, Reward: [-1202.549 -1202.549 -1202.549] [369.435], Avg: [-923.414 -923.414 -923.414] (0.0100) ({r_i: None, r_t: [-2287.342 -2287.342 -2287.342], eps: 0.01})
Step:   20700, Reward: [-376.887 -376.887 -376.887] [52.165], Avg: [-439.181 -439.181 -439.181] (0.1255) ({r_i: None, r_t: [-785.460 -785.460 -785.460], eps: 0.126})
Step:  105400, Reward: [-679.974 -679.974 -679.974] [242.774], Avg: [-607.038 -607.038 -607.038] (0.1000) ({r_i: None, r_t: [-1305.293 -1305.293 -1305.293], eps: 0.1})
Step:   72000, Reward: [-1140.378 -1140.378 -1140.378] [267.824], Avg: [-923.714 -923.714 -923.714] (0.0100) ({r_i: None, r_t: [-2160.497 -2160.497 -2160.497], eps: 0.01})
Step:   20800, Reward: [-389.806 -389.806 -389.806] [61.829], Avg: [-438.945 -438.945 -438.945] (0.1243) ({r_i: None, r_t: [-822.652 -822.652 -822.652], eps: 0.124})
Step:  105500, Reward: [-681.941 -681.941 -681.941] [173.358], Avg: [-607.109 -607.109 -607.109] (0.1000) ({r_i: None, r_t: [-1227.099 -1227.099 -1227.099], eps: 0.1})
Step:   72100, Reward: [-1182.700 -1182.700 -1182.700] [402.878], Avg: [-924.073 -924.073 -924.073] (0.0100) ({r_i: None, r_t: [-2230.868 -2230.868 -2230.868], eps: 0.01})
Step:  105600, Reward: [-593.303 -593.303 -593.303] [162.146], Avg: [-607.095 -607.095 -607.095] (0.1000) ({r_i: None, r_t: [-1344.824 -1344.824 -1344.824], eps: 0.1})
Step:   72200, Reward: [-1061.044 -1061.044 -1061.044] [203.653], Avg: [-924.263 -924.263 -924.263] (0.0100) ({r_i: None, r_t: [-1978.548 -1978.548 -1978.548], eps: 0.01})
Step:   20900, Reward: [-373.583 -373.583 -373.583] [71.214], Avg: [-438.633 -438.633 -438.633] (0.1230) ({r_i: None, r_t: [-800.588 -800.588 -800.588], eps: 0.123})
Step:  105700, Reward: [-690.426 -690.426 -690.426] [160.904], Avg: [-607.174 -607.174 -607.174] (0.1000) ({r_i: None, r_t: [-1372.055 -1372.055 -1372.055], eps: 0.1})
Step:   72300, Reward: [-956.472 -956.472 -956.472] [227.558], Avg: [-924.307 -924.307 -924.307] (0.0100) ({r_i: None, r_t: [-2090.949 -2090.949 -2090.949], eps: 0.01})
Step:   21000, Reward: [-381.767 -381.767 -381.767] [65.538], Avg: [-438.364 -438.364 -438.364] (0.1218) ({r_i: None, r_t: [-837.620 -837.620 -837.620], eps: 0.122})
Step:  105800, Reward: [-774.452 -774.452 -774.452] [273.438], Avg: [-607.332 -607.332 -607.332] (0.1000) ({r_i: None, r_t: [-1359.543 -1359.543 -1359.543], eps: 0.1})
Step:   72400, Reward: [-972.774 -972.774 -972.774] [256.577], Avg: [-924.374 -924.374 -924.374] (0.0100) ({r_i: None, r_t: [-1969.875 -1969.875 -1969.875], eps: 0.01})
Step:  105900, Reward: [-665.393 -665.393 -665.393] [194.210], Avg: [-607.387 -607.387 -607.387] (0.1000) ({r_i: None, r_t: [-1405.195 -1405.195 -1405.195], eps: 0.1})
Step:   21100, Reward: [-392.820 -392.820 -392.820] [56.379], Avg: [-438.149 -438.149 -438.149] (0.1206) ({r_i: None, r_t: [-797.397 -797.397 -797.397], eps: 0.121})
Step:   72500, Reward: [-996.677 -996.677 -996.677] [219.191], Avg: [-924.474 -924.474 -924.474] (0.0100) ({r_i: None, r_t: [-2064.152 -2064.152 -2064.152], eps: 0.01})
Step:  106000, Reward: [-670.141 -670.141 -670.141] [148.583], Avg: [-607.446 -607.446 -607.446] (0.1000) ({r_i: None, r_t: [-1342.078 -1342.078 -1342.078], eps: 0.1})
Step:   72600, Reward: [-921.254 -921.254 -921.254] [195.431], Avg: [-924.469 -924.469 -924.469] (0.0100) ({r_i: None, r_t: [-1856.695 -1856.695 -1856.695], eps: 0.01})
Step:   21200, Reward: [-414.027 -414.027 -414.027] [52.369], Avg: [-438.036 -438.036 -438.036] (0.1194) ({r_i: None, r_t: [-821.767 -821.767 -821.767], eps: 0.119})
Step:  106100, Reward: [-728.069 -728.069 -728.069] [229.743], Avg: [-607.560 -607.560 -607.560] (0.1000) ({r_i: None, r_t: [-1270.120 -1270.120 -1270.120], eps: 0.1})
Step:   72700, Reward: [-986.988 -986.988 -986.988] [233.721], Avg: [-924.555 -924.555 -924.555] (0.0100) ({r_i: None, r_t: [-1934.802 -1934.802 -1934.802], eps: 0.01})
Step:  106200, Reward: [-619.873 -619.873 -619.873] [158.879], Avg: [-607.571 -607.571 -607.571] (0.1000) ({r_i: None, r_t: [-1407.205 -1407.205 -1407.205], eps: 0.1})
Step:   21300, Reward: [-398.736 -398.736 -398.736] [59.772], Avg: [-437.852 -437.852 -437.852] (0.1182) ({r_i: None, r_t: [-779.532 -779.532 -779.532], eps: 0.118})
Step:   72800, Reward: [-959.089 -959.089 -959.089] [307.171], Avg: [-924.602 -924.602 -924.602] (0.0100) ({r_i: None, r_t: [-1879.564 -1879.564 -1879.564], eps: 0.01})
Step:  106300, Reward: [-548.655 -548.655 -548.655] [97.467], Avg: [-607.516 -607.516 -607.516] (0.1000) ({r_i: None, r_t: [-1460.008 -1460.008 -1460.008], eps: 0.1})
Step:   72900, Reward: [-986.226 -986.226 -986.226] [270.114], Avg: [-924.687 -924.687 -924.687] (0.0100) ({r_i: None, r_t: [-1959.789 -1959.789 -1959.789], eps: 0.01})
Step:   21400, Reward: [-412.164 -412.164 -412.164] [61.484], Avg: [-437.733 -437.733 -437.733] (0.1170) ({r_i: None, r_t: [-793.184 -793.184 -793.184], eps: 0.117})
Step:  106400, Reward: [-822.959 -822.959 -822.959] [225.049], Avg: [-607.718 -607.718 -607.718] (0.1000) ({r_i: None, r_t: [-1209.410 -1209.410 -1209.410], eps: 0.1})
Step:   73000, Reward: [-877.249 -877.249 -877.249] [196.074], Avg: [-924.622 -924.622 -924.622] (0.0100) ({r_i: None, r_t: [-1927.488 -1927.488 -1927.488], eps: 0.01})
Step:  106500, Reward: [-690.073 -690.073 -690.073] [286.536], Avg: [-607.795 -607.795 -607.795] (0.1000) ({r_i: None, r_t: [-1428.479 -1428.479 -1428.479], eps: 0.1})
Step:   21500, Reward: [-386.131 -386.131 -386.131] [54.249], Avg: [-437.494 -437.494 -437.494] (0.1159) ({r_i: None, r_t: [-833.014 -833.014 -833.014], eps: 0.116})
Step:   73100, Reward: [-923.314 -923.314 -923.314] [238.748], Avg: [-924.620 -924.620 -924.620] (0.0100) ({r_i: None, r_t: [-1864.387 -1864.387 -1864.387], eps: 0.01})
Step:  106600, Reward: [-630.840 -630.840 -630.840] [175.025], Avg: [-607.817 -607.817 -607.817] (0.1000) ({r_i: None, r_t: [-1384.438 -1384.438 -1384.438], eps: 0.1})
Step:   73200, Reward: [-906.511 -906.511 -906.511] [274.676], Avg: [-924.595 -924.595 -924.595] (0.0100) ({r_i: None, r_t: [-1762.636 -1762.636 -1762.636], eps: 0.01})
Step:   21600, Reward: [-409.559 -409.559 -409.559] [50.112], Avg: [-437.365 -437.365 -437.365] (0.1147) ({r_i: None, r_t: [-815.484 -815.484 -815.484], eps: 0.115})
Step:  106700, Reward: [-715.953 -715.953 -715.953] [215.027], Avg: [-607.918 -607.918 -607.918] (0.1000) ({r_i: None, r_t: [-1357.031 -1357.031 -1357.031], eps: 0.1})
Step:   73300, Reward: [-865.226 -865.226 -865.226] [231.751], Avg: [-924.515 -924.515 -924.515] (0.0100) ({r_i: None, r_t: [-1848.186 -1848.186 -1848.186], eps: 0.01})
Step:  106800, Reward: [-676.746 -676.746 -676.746] [318.412], Avg: [-607.983 -607.983 -607.983] (0.1000) ({r_i: None, r_t: [-1370.840 -1370.840 -1370.840], eps: 0.1})
Step:   21700, Reward: [-403.335 -403.335 -403.335] [40.108], Avg: [-437.209 -437.209 -437.209] (0.1136) ({r_i: None, r_t: [-774.761 -774.761 -774.761], eps: 0.114})
Step:   73400, Reward: [-815.583 -815.583 -815.583] [190.328], Avg: [-924.366 -924.366 -924.366] (0.0100) ({r_i: None, r_t: [-1706.067 -1706.067 -1706.067], eps: 0.01})
Step:  106900, Reward: [-732.461 -732.461 -732.461] [254.741], Avg: [-608.099 -608.099 -608.099] (0.1000) ({r_i: None, r_t: [-1422.864 -1422.864 -1422.864], eps: 0.1})
Step:   73500, Reward: [-800.058 -800.058 -800.058] [233.769], Avg: [-924.197 -924.197 -924.197] (0.0100) ({r_i: None, r_t: [-1775.767 -1775.767 -1775.767], eps: 0.01})
Step:   21800, Reward: [-389.287 -389.287 -389.287] [56.541], Avg: [-436.990 -436.990 -436.990] (0.1124) ({r_i: None, r_t: [-820.370 -820.370 -820.370], eps: 0.112})
Step:  107000, Reward: [-724.598 -724.598 -724.598] [304.699], Avg: [-608.208 -608.208 -608.208] (0.1000) ({r_i: None, r_t: [-1357.864 -1357.864 -1357.864], eps: 0.1})
Step:   73600, Reward: [-893.824 -893.824 -893.824] [236.588], Avg: [-924.156 -924.156 -924.156] (0.0100) ({r_i: None, r_t: [-1572.192 -1572.192 -1572.192], eps: 0.01})
Step:  107100, Reward: [-730.041 -730.041 -730.041] [223.300], Avg: [-608.321 -608.321 -608.321] (0.1000) ({r_i: None, r_t: [-1599.819 -1599.819 -1599.819], eps: 0.1})
Step:   21900, Reward: [-401.756 -401.756 -401.756] [58.386], Avg: [-436.830 -436.830 -436.830] (0.1113) ({r_i: None, r_t: [-824.291 -824.291 -824.291], eps: 0.111})
Step:   73700, Reward: [-754.484 -754.484 -754.484] [204.239], Avg: [-923.926 -923.926 -923.926] (0.0100) ({r_i: None, r_t: [-1669.560 -1669.560 -1669.560], eps: 0.01})
Step:  107200, Reward: [-714.873 -714.873 -714.873] [248.690], Avg: [-608.421 -608.421 -608.421] (0.1000) ({r_i: None, r_t: [-1375.534 -1375.534 -1375.534], eps: 0.1})
Step:   73800, Reward: [-723.780 -723.780 -723.780] [183.249], Avg: [-923.655 -923.655 -923.655] (0.0100) ({r_i: None, r_t: [-1666.905 -1666.905 -1666.905], eps: 0.01})
Step:   22000, Reward: [-398.477 -398.477 -398.477] [72.026], Avg: [-436.656 -436.656 -436.656] (0.1102) ({r_i: None, r_t: [-827.679 -827.679 -827.679], eps: 0.11})
Step:  107300, Reward: [-664.345 -664.345 -664.345] [244.328], Avg: [-608.473 -608.473 -608.473] (0.1000) ({r_i: None, r_t: [-1381.663 -1381.663 -1381.663], eps: 0.1})
Step:   73900, Reward: [-770.736 -770.736 -770.736] [209.982], Avg: [-923.449 -923.449 -923.449] (0.0100) ({r_i: None, r_t: [-1503.543 -1503.543 -1503.543], eps: 0.01})
Step:  107400, Reward: [-641.766 -641.766 -641.766] [189.032], Avg: [-608.504 -608.504 -608.504] (0.1000) ({r_i: None, r_t: [-1331.633 -1331.633 -1331.633], eps: 0.1})
Step:   22100, Reward: [-406.253 -406.253 -406.253] [54.704], Avg: [-436.519 -436.519 -436.519] (0.1091) ({r_i: None, r_t: [-812.306 -812.306 -812.306], eps: 0.109})
Step:   74000, Reward: [-838.631 -838.631 -838.631] [200.989], Avg: [-923.334 -923.334 -923.334] (0.0100) ({r_i: None, r_t: [-1610.051 -1610.051 -1610.051], eps: 0.01})
Step:  107500, Reward: [-667.816 -667.816 -667.816] [190.449], Avg: [-608.559 -608.559 -608.559] (0.1000) ({r_i: None, r_t: [-1267.687 -1267.687 -1267.687], eps: 0.1})
Step:   74100, Reward: [-723.756 -723.756 -723.756] [153.428], Avg: [-923.065 -923.065 -923.065] (0.0100) ({r_i: None, r_t: [-1494.573 -1494.573 -1494.573], eps: 0.01})
Step:  107600, Reward: [-809.152 -809.152 -809.152] [293.047], Avg: [-608.745 -608.745 -608.745] (0.1000) ({r_i: None, r_t: [-1494.379 -1494.379 -1494.379], eps: 0.1})
Step:   22200, Reward: [-430.612 -430.612 -430.612] [56.904], Avg: [-436.493 -436.493 -436.493] (0.1080) ({r_i: None, r_t: [-838.035 -838.035 -838.035], eps: 0.108})
Step:   74200, Reward: [-696.017 -696.017 -696.017] [209.262], Avg: [-922.760 -922.760 -922.760] (0.0100) ({r_i: None, r_t: [-1500.947 -1500.947 -1500.947], eps: 0.01})
Step:  107700, Reward: [-736.333 -736.333 -736.333] [202.810], Avg: [-608.864 -608.864 -608.864] (0.1000) ({r_i: None, r_t: [-1233.646 -1233.646 -1233.646], eps: 0.1})
Step:   22300, Reward: [-397.642 -397.642 -397.642] [76.712], Avg: [-436.319 -436.319 -436.319] (0.1069) ({r_i: None, r_t: [-778.926 -778.926 -778.926], eps: 0.107})
Step:   74300, Reward: [-818.699 -818.699 -818.699] [192.014], Avg: [-922.620 -922.620 -922.620] (0.0100) ({r_i: None, r_t: [-1439.151 -1439.151 -1439.151], eps: 0.01})
Step:  107800, Reward: [-638.125 -638.125 -638.125] [158.980], Avg: [-608.891 -608.891 -608.891] (0.1000) ({r_i: None, r_t: [-1408.519 -1408.519 -1408.519], eps: 0.1})
Step:   74400, Reward: [-778.205 -778.205 -778.205] [185.488], Avg: [-922.426 -922.426 -922.426] (0.0100) ({r_i: None, r_t: [-1514.896 -1514.896 -1514.896], eps: 0.01})
Step:  107900, Reward: [-753.249 -753.249 -753.249] [257.377], Avg: [-609.024 -609.024 -609.024] (0.1000) ({r_i: None, r_t: [-1340.619 -1340.619 -1340.619], eps: 0.1})
Step:   22400, Reward: [-405.922 -405.922 -405.922] [75.199], Avg: [-436.184 -436.184 -436.184] (0.1059) ({r_i: None, r_t: [-793.737 -793.737 -793.737], eps: 0.106})
Step:  108000, Reward: [-733.285 -733.285 -733.285] [259.443], Avg: [-609.139 -609.139 -609.139] (0.1000) ({r_i: None, r_t: [-1271.502 -1271.502 -1271.502], eps: 0.1})
Step:   74500, Reward: [-757.952 -757.952 -757.952] [165.657], Avg: [-922.206 -922.206 -922.206] (0.0100) ({r_i: None, r_t: [-1412.072 -1412.072 -1412.072], eps: 0.01})
Step:   22500, Reward: [-408.189 -408.189 -408.189] [61.279], Avg: [-436.060 -436.060 -436.060] (0.1048) ({r_i: None, r_t: [-839.217 -839.217 -839.217], eps: 0.105})
Step:  108100, Reward: [-657.873 -657.873 -657.873] [234.177], Avg: [-609.184 -609.184 -609.184] (0.1000) ({r_i: None, r_t: [-1306.693 -1306.693 -1306.693], eps: 0.1})
Step:   74600, Reward: [-753.394 -753.394 -753.394] [226.380], Avg: [-921.980 -921.980 -921.980] (0.0100) ({r_i: None, r_t: [-1492.526 -1492.526 -1492.526], eps: 0.01})
Step:  108200, Reward: [-798.227 -798.227 -798.227] [189.891], Avg: [-609.359 -609.359 -609.359] (0.1000) ({r_i: None, r_t: [-1497.342 -1497.342 -1497.342], eps: 0.1})
Step:   74700, Reward: [-821.915 -821.915 -821.915] [264.885], Avg: [-921.846 -921.846 -921.846] (0.0100) ({r_i: None, r_t: [-1469.748 -1469.748 -1469.748], eps: 0.01})
Step:   22600, Reward: [-429.944 -429.944 -429.944] [47.698], Avg: [-436.034 -436.034 -436.034] (0.1038) ({r_i: None, r_t: [-809.357 -809.357 -809.357], eps: 0.104})
Step:  108300, Reward: [-793.155 -793.155 -793.155] [316.555], Avg: [-609.528 -609.528 -609.528] (0.1000) ({r_i: None, r_t: [-1410.339 -1410.339 -1410.339], eps: 0.1})
Step:   74800, Reward: [-729.615 -729.615 -729.615] [202.289], Avg: [-921.589 -921.589 -921.589] (0.0100) ({r_i: None, r_t: [-1572.111 -1572.111 -1572.111], eps: 0.01})
Step:   22700, Reward: [-428.571 -428.571 -428.571] [68.610], Avg: [-436.001 -436.001 -436.001] (0.1027) ({r_i: None, r_t: [-821.081 -821.081 -821.081], eps: 0.103})
Step:  108400, Reward: [-753.712 -753.712 -753.712] [216.833], Avg: [-609.661 -609.661 -609.661] (0.1000) ({r_i: None, r_t: [-1327.719 -1327.719 -1327.719], eps: 0.1})
Step:   74900, Reward: [-701.441 -701.441 -701.441] [155.680], Avg: [-921.296 -921.296 -921.296] (0.0100) ({r_i: None, r_t: [-1563.349 -1563.349 -1563.349], eps: 0.01})
Step:  108500, Reward: [-778.840 -778.840 -778.840] [233.915], Avg: [-609.817 -609.817 -609.817] (0.1000) ({r_i: None, r_t: [-1261.197 -1261.197 -1261.197], eps: 0.1})
Step:   75000, Reward: [-712.948 -712.948 -712.948] [182.594], Avg: [-921.018 -921.018 -921.018] (0.0100) ({r_i: None, r_t: [-1502.345 -1502.345 -1502.345], eps: 0.01})
Step:   22800, Reward: [-394.332 -394.332 -394.332] [71.589], Avg: [-435.819 -435.819 -435.819] (0.1017) ({r_i: None, r_t: [-823.976 -823.976 -823.976], eps: 0.102})
Step:  108600, Reward: [-679.228 -679.228 -679.228] [144.390], Avg: [-609.881 -609.881 -609.881] (0.1000) ({r_i: None, r_t: [-1365.458 -1365.458 -1365.458], eps: 0.1})
Step:   75100, Reward: [-784.855 -784.855 -784.855] [248.107], Avg: [-920.837 -920.837 -920.837] (0.0100) ({r_i: None, r_t: [-1455.356 -1455.356 -1455.356], eps: 0.01})
Step:   22900, Reward: [-419.569 -419.569 -419.569] [71.743], Avg: [-435.748 -435.748 -435.748] (0.1007) ({r_i: None, r_t: [-770.929 -770.929 -770.929], eps: 0.101})
Step:  108700, Reward: [-649.788 -649.788 -649.788] [249.249], Avg: [-609.918 -609.918 -609.918] (0.1000) ({r_i: None, r_t: [-1335.732 -1335.732 -1335.732], eps: 0.1})
Step:   75200, Reward: [-820.129 -820.129 -820.129] [252.900], Avg: [-920.703 -920.703 -920.703] (0.0100) ({r_i: None, r_t: [-1506.043 -1506.043 -1506.043], eps: 0.01})
Step:  108800, Reward: [-687.165 -687.165 -687.165] [196.200], Avg: [-609.989 -609.989 -609.989] (0.1000) ({r_i: None, r_t: [-1423.594 -1423.594 -1423.594], eps: 0.1})
Step:   23000, Reward: [-371.064 -371.064 -371.064] [63.989], Avg: [-435.468 -435.468 -435.468] (0.0997) ({r_i: None, r_t: [-835.612 -835.612 -835.612], eps: 0.1})
Step:   75300, Reward: [-1041.611 -1041.611 -1041.611] [387.195], Avg: [-920.864 -920.864 -920.864] (0.0100) ({r_i: None, r_t: [-1613.252 -1613.252 -1613.252], eps: 0.01})
Step:  108900, Reward: [-756.815 -756.815 -756.815] [298.936], Avg: [-610.123 -610.123 -610.123] (0.1000) ({r_i: None, r_t: [-1462.013 -1462.013 -1462.013], eps: 0.1})
Step:   75400, Reward: [-859.036 -859.036 -859.036] [261.823], Avg: [-920.782 -920.782 -920.782] (0.0100) ({r_i: None, r_t: [-1750.216 -1750.216 -1750.216], eps: 0.01})
Step:   23100, Reward: [-420.102 -420.102 -420.102] [60.689], Avg: [-435.402 -435.402 -435.402] (0.0987) ({r_i: None, r_t: [-807.630 -807.630 -807.630], eps: 0.099})
Step:  109000, Reward: [-745.120 -745.120 -745.120] [283.891], Avg: [-610.247 -610.247 -610.247] (0.1000) ({r_i: None, r_t: [-1503.487 -1503.487 -1503.487], eps: 0.1})
Step:   75500, Reward: [-884.243 -884.243 -884.243] [318.559], Avg: [-920.734 -920.734 -920.734] (0.0100) ({r_i: None, r_t: [-1663.097 -1663.097 -1663.097], eps: 0.01})
Step:  109100, Reward: [-744.648 -744.648 -744.648] [301.119], Avg: [-610.370 -610.370 -610.370] (0.1000) ({r_i: None, r_t: [-1384.560 -1384.560 -1384.560], eps: 0.1})
Step:   23200, Reward: [-422.421 -422.421 -422.421] [63.505], Avg: [-435.346 -435.346 -435.346] (0.0977) ({r_i: None, r_t: [-793.803 -793.803 -793.803], eps: 0.098})
Step:   75600, Reward: [-937.811 -937.811 -937.811] [350.412], Avg: [-920.756 -920.756 -920.756] (0.0100) ({r_i: None, r_t: [-1948.770 -1948.770 -1948.770], eps: 0.01})
Step:  109200, Reward: [-734.396 -734.396 -734.396] [359.999], Avg: [-610.484 -610.484 -610.484] (0.1000) ({r_i: None, r_t: [-1353.182 -1353.182 -1353.182], eps: 0.1})
Step:   75700, Reward: [-1008.335 -1008.335 -1008.335] [485.563], Avg: [-920.872 -920.872 -920.872] (0.0100) ({r_i: None, r_t: [-1981.106 -1981.106 -1981.106], eps: 0.01})
Step:  109300, Reward: [-732.139 -732.139 -732.139] [246.382], Avg: [-610.595 -610.595 -610.595] (0.1000) ({r_i: None, r_t: [-1474.218 -1474.218 -1474.218], eps: 0.1})
Step:   23300, Reward: [-405.061 -405.061 -405.061] [53.554], Avg: [-435.217 -435.217 -435.217] (0.0967) ({r_i: None, r_t: [-829.141 -829.141 -829.141], eps: 0.097})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation
TIME_BATCH = 1

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		q_value = torch.zeros_like(q_next_value)
		for t in reversed(range(0,rewards.size(1),TIME_BATCH)):
			q_value[:,t:t+TIME_BATCH] = self.critic_local(critic_inputs[:,t:t+TIME_BATCH])
			q_taken = torch.gather(q_value[:,t:t+TIME_BATCH], dim=-1, index=actions[:,t:t+TIME_BATCH].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - q_target[:,t:t+TIME_BATCH].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		# q_value = self.critic_local(critic_inputs)
		# q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		# critic_error = (q_taken - q_target.detach())
		# critic_loss = critic_error.pow(2).mean()
		# self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-531.191 -531.191 -531.191] [81.647], Avg: [-531.191 -531.191 -531.191] (1.0000) ({r_i: None, r_t: [-8.927 -8.927 -8.927], eps: 1.0})
Step:   75800, Reward: [-992.873 -992.873 -992.873] [444.699], Avg: [-920.967 -920.967 -920.967] (0.0100) ({r_i: None, r_t: [-2174.954 -2174.954 -2174.954], eps: 0.01})
Step:  109400, Reward: [-678.595 -678.595 -678.595] [244.916], Avg: [-610.657 -610.657 -610.657] (0.1000) ({r_i: None, r_t: [-1564.781 -1564.781 -1564.781], eps: 0.1})
Step:   23400, Reward: [-426.042 -426.042 -426.042] [69.540], Avg: [-435.178 -435.178 -435.178] (0.0958) ({r_i: None, r_t: [-819.693 -819.693 -819.693], eps: 0.096})
Step:   75900, Reward: [-973.056 -973.056 -973.056] [345.306], Avg: [-921.035 -921.035 -921.035] (0.0100) ({r_i: None, r_t: [-2126.573 -2126.573 -2126.573], eps: 0.01})
Step:  109500, Reward: [-743.286 -743.286 -743.286] [271.396], Avg: [-610.778 -610.778 -610.778] (0.1000) ({r_i: None, r_t: [-1468.335 -1468.335 -1468.335], eps: 0.1})
Step:   76000, Reward: [-1135.529 -1135.529 -1135.529] [442.402], Avg: [-921.317 -921.317 -921.317] (0.0100) ({r_i: None, r_t: [-1922.314 -1922.314 -1922.314], eps: 0.01})
Step:  109600, Reward: [-588.011 -588.011 -588.011] [111.294], Avg: [-610.757 -610.757 -610.757] (0.1000) ({r_i: None, r_t: [-1380.524 -1380.524 -1380.524], eps: 0.1})
Step:   23500, Reward: [-369.679 -369.679 -369.679] [43.820], Avg: [-434.900 -434.900 -434.900] (0.0948) ({r_i: None, r_t: [-786.001 -786.001 -786.001], eps: 0.095})
Step:   76100, Reward: [-1074.860 -1074.860 -1074.860] [438.216], Avg: [-921.518 -921.518 -921.518] (0.0100) ({r_i: None, r_t: [-2118.543 -2118.543 -2118.543], eps: 0.01})
Step:  109700, Reward: [-746.592 -746.592 -746.592] [305.495], Avg: [-610.881 -610.881 -610.881] (0.1000) ({r_i: None, r_t: [-1519.832 -1519.832 -1519.832], eps: 0.1})
Step:   23600, Reward: [-380.149 -380.149 -380.149] [50.264], Avg: [-434.669 -434.669 -434.669] (0.0939) ({r_i: None, r_t: [-831.710 -831.710 -831.710], eps: 0.094})
Step:   76200, Reward: [-1266.807 -1266.807 -1266.807] [494.928], Avg: [-921.971 -921.971 -921.971] (0.0100) ({r_i: None, r_t: [-2025.033 -2025.033 -2025.033], eps: 0.01})
Step:  109800, Reward: [-696.025 -696.025 -696.025] [241.186], Avg: [-610.958 -610.958 -610.958] (0.1000) ({r_i: None, r_t: [-1535.306 -1535.306 -1535.306], eps: 0.1})
Step:   76300, Reward: [-984.995 -984.995 -984.995] [372.595], Avg: [-922.053 -922.053 -922.053] (0.0100) ({r_i: None, r_t: [-2146.721 -2146.721 -2146.721], eps: 0.01})
Step:  109900, Reward: [-753.680 -753.680 -753.680] [292.352], Avg: [-611.088 -611.088 -611.088] (0.1000) ({r_i: None, r_t: [-1553.127 -1553.127 -1553.127], eps: 0.1})
Step:   23700, Reward: [-383.192 -383.192 -383.192] [56.962], Avg: [-434.453 -434.453 -434.453] (0.0929) ({r_i: None, r_t: [-786.236 -786.236 -786.236], eps: 0.093})
Step:   76400, Reward: [-1059.919 -1059.919 -1059.919] [413.503], Avg: [-922.234 -922.234 -922.234] (0.0100) ({r_i: None, r_t: [-2322.816 -2322.816 -2322.816], eps: 0.01})
Step:  110000, Reward: [-814.101 -814.101 -814.101] [246.682], Avg: [-611.272 -611.272 -611.272] (0.1000) ({r_i: None, r_t: [-1427.788 -1427.788 -1427.788], eps: 0.1})
Step:   23800, Reward: [-381.574 -381.574 -381.574] [46.889], Avg: [-434.232 -434.232 -434.232] (0.0920) ({r_i: None, r_t: [-808.677 -808.677 -808.677], eps: 0.092})
Step:   76500, Reward: [-992.550 -992.550 -992.550] [377.108], Avg: [-922.325 -922.325 -922.325] (0.0100) ({r_i: None, r_t: [-2433.189 -2433.189 -2433.189], eps: 0.01})
Step:  110100, Reward: [-641.092 -641.092 -641.092] [219.091], Avg: [-611.299 -611.299 -611.299] (0.1000) ({r_i: None, r_t: [-1550.541 -1550.541 -1550.541], eps: 0.1})
Step:   76600, Reward: [-933.764 -933.764 -933.764] [371.869], Avg: [-922.340 -922.340 -922.340] (0.0100) ({r_i: None, r_t: [-2794.144 -2794.144 -2794.144], eps: 0.01})
Step:  110200, Reward: [-818.024 -818.024 -818.024] [362.723], Avg: [-611.487 -611.487 -611.487] (0.1000) ({r_i: None, r_t: [-1391.267 -1391.267 -1391.267], eps: 0.1})
Step:   23900, Reward: [-400.566 -400.566 -400.566] [58.889], Avg: [-434.091 -434.091 -434.091] (0.0911) ({r_i: None, r_t: [-820.711 -820.711 -820.711], eps: 0.091})
Step:   76700, Reward: [-1291.390 -1291.390 -1291.390] [495.838], Avg: [-922.821 -922.821 -922.821] (0.0100) ({r_i: None, r_t: [-2605.782 -2605.782 -2605.782], eps: 0.01})
Step:  110300, Reward: [-692.996 -692.996 -692.996] [262.953], Avg: [-611.561 -611.561 -611.561] (0.1000) ({r_i: None, r_t: [-1476.527 -1476.527 -1476.527], eps: 0.1})
Step:   24000, Reward: [-387.156 -387.156 -387.156] [47.320], Avg: [-433.897 -433.897 -433.897] (0.0902) ({r_i: None, r_t: [-756.019 -756.019 -756.019], eps: 0.09})
Step:  110400, Reward: [-706.347 -706.347 -706.347] [262.169], Avg: [-611.647 -611.647 -611.647] (0.1000) ({r_i: None, r_t: [-1404.589 -1404.589 -1404.589], eps: 0.1})
Step:   76800, Reward: [-1155.306 -1155.306 -1155.306] [399.021], Avg: [-923.123 -923.123 -923.123] (0.0100) ({r_i: None, r_t: [-2491.239 -2491.239 -2491.239], eps: 0.01})
Step:   76900, Reward: [-1316.467 -1316.467 -1316.467] [447.021], Avg: [-923.634 -923.634 -923.634] (0.0100) ({r_i: None, r_t: [-2565.573 -2565.573 -2565.573], eps: 0.01})
Step:  110500, Reward: [-732.663 -732.663 -732.663] [235.930], Avg: [-611.756 -611.756 -611.756] (0.1000) ({r_i: None, r_t: [-1486.063 -1486.063 -1486.063], eps: 0.1})
Step:   24100, Reward: [-393.400 -393.400 -393.400] [59.010], Avg: [-433.729 -433.729 -433.729] (0.0893) ({r_i: None, r_t: [-779.406 -779.406 -779.406], eps: 0.089})
Step:   77000, Reward: [-1027.952 -1027.952 -1027.952] [345.407], Avg: [-923.769 -923.769 -923.769] (0.0100) ({r_i: None, r_t: [-2373.323 -2373.323 -2373.323], eps: 0.01})
Step:  110600, Reward: [-762.974 -762.974 -762.974] [259.826], Avg: [-611.893 -611.893 -611.893] (0.1000) ({r_i: None, r_t: [-1322.612 -1322.612 -1322.612], eps: 0.1})
Step:   24200, Reward: [-380.233 -380.233 -380.233] [55.177], Avg: [-433.509 -433.509 -433.509] (0.0884) ({r_i: None, r_t: [-798.299 -798.299 -798.299], eps: 0.088})
Step:   77100, Reward: [-1129.407 -1129.407 -1129.407] [498.710], Avg: [-924.036 -924.036 -924.036] (0.0100) ({r_i: None, r_t: [-2377.117 -2377.117 -2377.117], eps: 0.01})
Step:  110700, Reward: [-784.579 -784.579 -784.579] [255.225], Avg: [-612.048 -612.048 -612.048] (0.1000) ({r_i: None, r_t: [-1566.957 -1566.957 -1566.957], eps: 0.1})
Step:   77200, Reward: [-1186.572 -1186.572 -1186.572] [432.693], Avg: [-924.375 -924.375 -924.375] (0.0100) ({r_i: None, r_t: [-2168.377 -2168.377 -2168.377], eps: 0.01})
Step:  110800, Reward: [-798.983 -798.983 -798.983] [253.200], Avg: [-612.217 -612.217 -612.217] (0.1000) ({r_i: None, r_t: [-1599.996 -1599.996 -1599.996], eps: 0.1})
Step:   24300, Reward: [-377.765 -377.765 -377.765] [74.965], Avg: [-433.281 -433.281 -433.281] (0.0875) ({r_i: None, r_t: [-815.021 -815.021 -815.021], eps: 0.088})
Step:  110900, Reward: [-875.981 -875.981 -875.981] [316.454], Avg: [-612.455 -612.455 -612.455] (0.1000) ({r_i: None, r_t: [-1355.462 -1355.462 -1355.462], eps: 0.1})
Step:   77300, Reward: [-1175.461 -1175.461 -1175.461] [396.968], Avg: [-924.700 -924.700 -924.700] (0.0100) ({r_i: None, r_t: [-2452.473 -2452.473 -2452.473], eps: 0.01})
Step:   24400, Reward: [-365.552 -365.552 -365.552] [54.260], Avg: [-433.004 -433.004 -433.004] (0.0866) ({r_i: None, r_t: [-808.759 -808.759 -808.759], eps: 0.087})
Step:   77400, Reward: [-1008.619 -1008.619 -1008.619] [358.149], Avg: [-924.808 -924.808 -924.808] (0.0100) ({r_i: None, r_t: [-2268.613 -2268.613 -2268.613], eps: 0.01})
Step:  111000, Reward: [-794.854 -794.854 -794.854] [291.910], Avg: [-612.619 -612.619 -612.619] (0.1000) ({r_i: None, r_t: [-1496.887 -1496.887 -1496.887], eps: 0.1})
Step:   77500, Reward: [-949.071 -949.071 -949.071] [447.826], Avg: [-924.839 -924.839 -924.839] (0.0100) ({r_i: None, r_t: [-2039.187 -2039.187 -2039.187], eps: 0.01})
Step:   24500, Reward: [-396.085 -396.085 -396.085] [54.306], Avg: [-432.854 -432.854 -432.854] (0.0858) ({r_i: None, r_t: [-784.910 -784.910 -784.910], eps: 0.086})
Step:  111100, Reward: [-717.635 -717.635 -717.635] [224.939], Avg: [-612.713 -612.713 -612.713] (0.1000) ({r_i: None, r_t: [-1518.721 -1518.721 -1518.721], eps: 0.1})
Step:   77600, Reward: [-970.359 -970.359 -970.359] [390.454], Avg: [-924.898 -924.898 -924.898] (0.0100) ({r_i: None, r_t: [-1937.745 -1937.745 -1937.745], eps: 0.01})
Step:  111200, Reward: [-790.877 -790.877 -790.877] [250.512], Avg: [-612.873 -612.873 -612.873] (0.1000) ({r_i: None, r_t: [-1519.936 -1519.936 -1519.936], eps: 0.1})
Step:   24600, Reward: [-416.297 -416.297 -416.297] [83.083], Avg: [-432.787 -432.787 -432.787] (0.0849) ({r_i: None, r_t: [-841.673 -841.673 -841.673], eps: 0.085})
Step:   77700, Reward: [-946.904 -946.904 -946.904] [306.192], Avg: [-924.926 -924.926 -924.926] (0.0100) ({r_i: None, r_t: [-1801.662 -1801.662 -1801.662], eps: 0.01})
Step:  111300, Reward: [-973.121 -973.121 -973.121] [298.229], Avg: [-613.197 -613.197 -613.197] (0.1000) ({r_i: None, r_t: [-1418.170 -1418.170 -1418.170], eps: 0.1})
Step:   77800, Reward: [-857.577 -857.577 -857.577] [314.268], Avg: [-924.840 -924.840 -924.840] (0.0100) ({r_i: None, r_t: [-1839.607 -1839.607 -1839.607], eps: 0.01})
Step:   24700, Reward: [-366.413 -366.413 -366.413] [70.028], Avg: [-432.520 -432.520 -432.520] (0.0841) ({r_i: None, r_t: [-776.197 -776.197 -776.197], eps: 0.084})
Step:  111400, Reward: [-799.254 -799.254 -799.254] [339.686], Avg: [-613.364 -613.364 -613.364] (0.1000) ({r_i: None, r_t: [-1574.261 -1574.261 -1574.261], eps: 0.1})
Step:   77900, Reward: [-805.082 -805.082 -805.082] [222.469], Avg: [-924.686 -924.686 -924.686] (0.0100) ({r_i: None, r_t: [-1710.940 -1710.940 -1710.940], eps: 0.01})
Step:   24800, Reward: [-387.857 -387.857 -387.857] [57.295], Avg: [-432.340 -432.340 -432.340] (0.0832) ({r_i: None, r_t: [-759.389 -759.389 -759.389], eps: 0.083})
Step:  111500, Reward: [-737.717 -737.717 -737.717] [298.686], Avg: [-613.475 -613.475 -613.475] (0.1000) ({r_i: None, r_t: [-1447.949 -1447.949 -1447.949], eps: 0.1})
Step:   78000, Reward: [-698.372 -698.372 -698.372] [243.070], Avg: [-924.396 -924.396 -924.396] (0.0100) ({r_i: None, r_t: [-1657.390 -1657.390 -1657.390], eps: 0.01})
Step:  111600, Reward: [-848.351 -848.351 -848.351] [220.118], Avg: [-613.685 -613.685 -613.685] (0.1000) ({r_i: None, r_t: [-1636.231 -1636.231 -1636.231], eps: 0.1})
Step:   78100, Reward: [-723.845 -723.845 -723.845] [216.323], Avg: [-924.140 -924.140 -924.140] (0.0100) ({r_i: None, r_t: [-1566.082 -1566.082 -1566.082], eps: 0.01})
Step:   24900, Reward: [-403.847 -403.847 -403.847] [56.368], Avg: [-432.226 -432.226 -432.226] (0.0824) ({r_i: None, r_t: [-797.727 -797.727 -797.727], eps: 0.082})
Step:   78200, Reward: [-694.418 -694.418 -694.418] [167.314], Avg: [-923.847 -923.847 -923.847] (0.0100) ({r_i: None, r_t: [-1517.953 -1517.953 -1517.953], eps: 0.01})
Step:  111700, Reward: [-686.098 -686.098 -686.098] [174.167], Avg: [-613.750 -613.750 -613.750] (0.1000) ({r_i: None, r_t: [-1569.768 -1569.768 -1569.768], eps: 0.1})
Step:   25000, Reward: [-384.248 -384.248 -384.248] [60.138], Avg: [-432.035 -432.035 -432.035] (0.0816) ({r_i: None, r_t: [-771.354 -771.354 -771.354], eps: 0.082})
Step:   78300, Reward: [-759.298 -759.298 -759.298] [314.005], Avg: [-923.637 -923.637 -923.637] (0.0100) ({r_i: None, r_t: [-1287.231 -1287.231 -1287.231], eps: 0.01})
Step:  111800, Reward: [-782.566 -782.566 -782.566] [227.618], Avg: [-613.901 -613.901 -613.901] (0.1000) ({r_i: None, r_t: [-1627.562 -1627.562 -1627.562], eps: 0.1})
Step:   78400, Reward: [-622.242 -622.242 -622.242] [151.466], Avg: [-923.253 -923.253 -923.253] (0.0100) ({r_i: None, r_t: [-1362.893 -1362.893 -1362.893], eps: 0.01})
Step:   25100, Reward: [-416.990 -416.990 -416.990] [57.433], Avg: [-431.975 -431.975 -431.975] (0.0808) ({r_i: None, r_t: [-778.541 -778.541 -778.541], eps: 0.081})
Step:  111900, Reward: [-677.112 -677.112 -677.112] [194.862], Avg: [-613.957 -613.957 -613.957] (0.1000) ({r_i: None, r_t: [-1500.018 -1500.018 -1500.018], eps: 0.1})
Step:   78500, Reward: [-663.373 -663.373 -663.373] [173.769], Avg: [-922.922 -922.922 -922.922] (0.0100) ({r_i: None, r_t: [-1259.458 -1259.458 -1259.458], eps: 0.01})
Step:   25200, Reward: [-415.483 -415.483 -415.483] [47.576], Avg: [-431.910 -431.910 -431.910] (0.0800) ({r_i: None, r_t: [-792.738 -792.738 -792.738], eps: 0.08})
Step:  112000, Reward: [-778.193 -778.193 -778.193] [298.599], Avg: [-614.104 -614.104 -614.104] (0.1000) ({r_i: None, r_t: [-1561.975 -1561.975 -1561.975], eps: 0.1})
Step:   78600, Reward: [-686.575 -686.575 -686.575] [180.119], Avg: [-922.622 -922.622 -922.622] (0.0100) ({r_i: None, r_t: [-1217.094 -1217.094 -1217.094], eps: 0.01})
Step:   78700, Reward: [-578.692 -578.692 -578.692] [162.756], Avg: [-922.185 -922.185 -922.185] (0.0100) ({r_i: None, r_t: [-1177.542 -1177.542 -1177.542], eps: 0.01})
Step:  112100, Reward: [-839.267 -839.267 -839.267] [286.393], Avg: [-614.304 -614.304 -614.304] (0.1000) ({r_i: None, r_t: [-1717.800 -1717.800 -1717.800], eps: 0.1})
Step:   25300, Reward: [-424.288 -424.288 -424.288] [59.963], Avg: [-431.880 -431.880 -431.880] (0.0792) ({r_i: None, r_t: [-816.138 -816.138 -816.138], eps: 0.079})
Step:   78800, Reward: [-571.265 -571.265 -571.265] [176.427], Avg: [-921.741 -921.741 -921.741] (0.0100) ({r_i: None, r_t: [-1134.875 -1134.875 -1134.875], eps: 0.01})
Step:  112200, Reward: [-827.034 -827.034 -827.034] [351.339], Avg: [-614.494 -614.494 -614.494] (0.1000) ({r_i: None, r_t: [-1587.229 -1587.229 -1587.229], eps: 0.1})
Step:   25400, Reward: [-383.736 -383.736 -383.736] [58.465], Avg: [-431.691 -431.691 -431.691] (0.0784) ({r_i: None, r_t: [-780.063 -780.063 -780.063], eps: 0.078})
Step:   78900, Reward: [-545.139 -545.139 -545.139] [117.252], Avg: [-921.264 -921.264 -921.264] (0.0100) ({r_i: None, r_t: [-1139.125 -1139.125 -1139.125], eps: 0.01})
Step:  112300, Reward: [-688.085 -688.085 -688.085] [200.940], Avg: [-614.559 -614.559 -614.559] (0.1000) ({r_i: None, r_t: [-1567.926 -1567.926 -1567.926], eps: 0.1})
Step:   79000, Reward: [-556.948 -556.948 -556.948] [132.580], Avg: [-920.803 -920.803 -920.803] (0.0100) ({r_i: None, r_t: [-1060.531 -1060.531 -1060.531], eps: 0.01})
Step:   25500, Reward: [-397.731 -397.731 -397.731] [77.453], Avg: [-431.559 -431.559 -431.559] (0.0776) ({r_i: None, r_t: [-790.731 -790.731 -790.731], eps: 0.078})
Step:  112400, Reward: [-868.822 -868.822 -868.822] [412.642], Avg: [-614.785 -614.785 -614.785] (0.1000) ({r_i: None, r_t: [-1574.391 -1574.391 -1574.391], eps: 0.1})
Step:   79100, Reward: [-531.881 -531.881 -531.881] [104.888], Avg: [-920.312 -920.312 -920.312] (0.0100) ({r_i: None, r_t: [-1052.415 -1052.415 -1052.415], eps: 0.01})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation
TIME_BATCH = 10

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		q_value = torch.zeros_like(q_next_value)
		for t in reversed(range(0,rewards.size(1),TIME_BATCH)):
			q_value[:,t:t+TIME_BATCH] = self.critic_local(critic_inputs[:,t:t+TIME_BATCH])
			q_taken = torch.gather(q_value[:,t:t+TIME_BATCH], dim=-1, index=actions[:,t:t+TIME_BATCH].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - q_target[:,t:t+TIME_BATCH].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		# q_value = self.critic_local(critic_inputs)
		# q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		# critic_error = (q_taken - q_target.detach())
		# critic_loss = critic_error.pow(2).mean()
		# self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-527.700 -527.700 -527.700] [135.117], Avg: [-527.700 -527.700 -527.700] (1.0000) ({r_i: None, r_t: [-8.150 -8.150 -8.150], eps: 1.0})
Step:   25600, Reward: [-407.822 -407.822 -407.822] [66.458], Avg: [-431.466 -431.466 -431.466] (0.0768) ({r_i: None, r_t: [-781.404 -781.404 -781.404], eps: 0.077})
Step:  112500, Reward: [-810.231 -810.231 -810.231] [298.781], Avg: [-614.959 -614.959 -614.959] (0.1000) ({r_i: None, r_t: [-1440.840 -1440.840 -1440.840], eps: 0.1})
Step:   79200, Reward: [-528.654 -528.654 -528.654] [116.749], Avg: [-919.818 -919.818 -919.818] (0.0100) ({r_i: None, r_t: [-1141.266 -1141.266 -1141.266], eps: 0.01})
Step:  112600, Reward: [-815.499 -815.499 -815.499] [343.744], Avg: [-615.137 -615.137 -615.137] (0.1000) ({r_i: None, r_t: [-1536.024 -1536.024 -1536.024], eps: 0.1})
Step:   79300, Reward: [-562.577 -562.577 -562.577] [144.169], Avg: [-919.368 -919.368 -919.368] (0.0100) ({r_i: None, r_t: [-1116.333 -1116.333 -1116.333], eps: 0.01})
Step:   25700, Reward: [-410.127 -410.127 -410.127] [63.751], Avg: [-431.384 -431.384 -431.384] (0.0760) ({r_i: None, r_t: [-823.591 -823.591 -823.591], eps: 0.076})
Step:  112700, Reward: [-912.868 -912.868 -912.868] [369.793], Avg: [-615.401 -615.401 -615.401] (0.1000) ({r_i: None, r_t: [-1533.118 -1533.118 -1533.118], eps: 0.1})
Step:   79400, Reward: [-491.711 -491.711 -491.711] [130.346], Avg: [-918.830 -918.830 -918.830] (0.0100) ({r_i: None, r_t: [-1031.052 -1031.052 -1031.052], eps: 0.01})
Step:   25800, Reward: [-408.809 -408.809 -408.809] [61.327], Avg: [-431.296 -431.296 -431.296] (0.0753) ({r_i: None, r_t: [-763.998 -763.998 -763.998], eps: 0.075})
Step:   79500, Reward: [-552.937 -552.937 -552.937] [191.879], Avg: [-918.371 -918.371 -918.371] (0.0100) ({r_i: None, r_t: [-1068.892 -1068.892 -1068.892], eps: 0.01})
Step:  112800, Reward: [-853.480 -853.480 -853.480] [365.149], Avg: [-615.612 -615.612 -615.612] (0.1000) ({r_i: None, r_t: [-1342.741 -1342.741 -1342.741], eps: 0.1})
Step:   25900, Reward: [-393.431 -393.431 -393.431] [53.943], Avg: [-431.151 -431.151 -431.151] (0.0745) ({r_i: None, r_t: [-826.046 -826.046 -826.046], eps: 0.075})
Step:   79600, Reward: [-471.499 -471.499 -471.499] [133.032], Avg: [-917.810 -917.810 -917.810] (0.0100) ({r_i: None, r_t: [-1042.939 -1042.939 -1042.939], eps: 0.01})
Step:  112900, Reward: [-909.233 -909.233 -909.233] [433.888], Avg: [-615.872 -615.872 -615.872] (0.1000) ({r_i: None, r_t: [-1758.179 -1758.179 -1758.179], eps: 0.1})
Step:   79700, Reward: [-478.970 -478.970 -478.970] [76.633], Avg: [-917.260 -917.260 -917.260] (0.0100) ({r_i: None, r_t: [-1001.600 -1001.600 -1001.600], eps: 0.01})
Step:   26000, Reward: [-405.696 -405.696 -405.696] [54.857], Avg: [-431.053 -431.053 -431.053] (0.0738) ({r_i: None, r_t: [-782.849 -782.849 -782.849], eps: 0.074})
Step:  113000, Reward: [-968.877 -968.877 -968.877] [253.155], Avg: [-616.184 -616.184 -616.184] (0.1000) ({r_i: None, r_t: [-1536.657 -1536.657 -1536.657], eps: 0.1})
Step:   79800, Reward: [-510.685 -510.685 -510.685] [103.361], Avg: [-916.751 -916.751 -916.751] (0.0100) ({r_i: None, r_t: [-1066.474 -1066.474 -1066.474], eps: 0.01})
Step:  113100, Reward: [-794.025 -794.025 -794.025] [247.981], Avg: [-616.341 -616.341 -616.341] (0.1000) ({r_i: None, r_t: [-1855.918 -1855.918 -1855.918], eps: 0.1})
Step:   26100, Reward: [-408.885 -408.885 -408.885] [60.061], Avg: [-430.969 -430.969 -430.969] (0.0731) ({r_i: None, r_t: [-805.390 -805.390 -805.390], eps: 0.073})
Step:   79900, Reward: [-462.217 -462.217 -462.217] [102.471], Avg: [-916.183 -916.183 -916.183] (0.0100) ({r_i: None, r_t: [-1079.578 -1079.578 -1079.578], eps: 0.01})
Step:  113200, Reward: [-926.511 -926.511 -926.511] [334.863], Avg: [-616.615 -616.615 -616.615] (0.1000) ({r_i: None, r_t: [-1636.385 -1636.385 -1636.385], eps: 0.1})
Step:   80000, Reward: [-522.424 -522.424 -522.424] [124.308], Avg: [-915.692 -915.692 -915.692] (0.0100) ({r_i: None, r_t: [-1059.021 -1059.021 -1059.021], eps: 0.01})
Step:   26200, Reward: [-395.625 -395.625 -395.625] [55.220], Avg: [-430.834 -430.834 -430.834] (0.0723) ({r_i: None, r_t: [-776.742 -776.742 -776.742], eps: 0.072})
Step:   80100, Reward: [-532.082 -532.082 -532.082] [100.398], Avg: [-915.213 -915.213 -915.213] (0.0100) ({r_i: None, r_t: [-1015.143 -1015.143 -1015.143], eps: 0.01})
Step:  113300, Reward: [-816.674 -816.674 -816.674] [347.505], Avg: [-616.791 -616.791 -616.791] (0.1000) ({r_i: None, r_t: [-1651.060 -1651.060 -1651.060], eps: 0.1})
Step:   26300, Reward: [-399.791 -399.791 -399.791] [63.832], Avg: [-430.717 -430.717 -430.717] (0.0716) ({r_i: None, r_t: [-793.678 -793.678 -793.678], eps: 0.072})
Step:   80200, Reward: [-491.573 -491.573 -491.573] [98.569], Avg: [-914.686 -914.686 -914.686] (0.0100) ({r_i: None, r_t: [-1039.385 -1039.385 -1039.385], eps: 0.01})
Step:  113400, Reward: [-760.794 -760.794 -760.794] [278.885], Avg: [-616.918 -616.918 -616.918] (0.1000) ({r_i: None, r_t: [-1688.413 -1688.413 -1688.413], eps: 0.1})
Step:   80300, Reward: [-488.594 -488.594 -488.594] [62.946], Avg: [-914.156 -914.156 -914.156] (0.0100) ({r_i: None, r_t: [-1067.390 -1067.390 -1067.390], eps: 0.01})
Step:   26400, Reward: [-400.475 -400.475 -400.475] [59.571], Avg: [-430.603 -430.603 -430.603] (0.0709) ({r_i: None, r_t: [-792.828 -792.828 -792.828], eps: 0.071})
Step:  113500, Reward: [-935.570 -935.570 -935.570] [437.603], Avg: [-617.198 -617.198 -617.198] (0.1000) ({r_i: None, r_t: [-1662.088 -1662.088 -1662.088], eps: 0.1})
Step:   80400, Reward: [-459.311 -459.311 -459.311] [77.284], Avg: [-913.591 -913.591 -913.591] (0.0100) ({r_i: None, r_t: [-984.312 -984.312 -984.312], eps: 0.01})
Step:  113600, Reward: [-893.481 -893.481 -893.481] [287.611], Avg: [-617.441 -617.441 -617.441] (0.1000) ({r_i: None, r_t: [-1769.157 -1769.157 -1769.157], eps: 0.1})
Step:   26500, Reward: [-394.061 -394.061 -394.061] [63.056], Avg: [-430.465 -430.465 -430.465] (0.0702) ({r_i: None, r_t: [-802.181 -802.181 -802.181], eps: 0.07})
Step:   80500, Reward: [-501.988 -501.988 -501.988] [108.813], Avg: [-913.080 -913.080 -913.080] (0.0100) ({r_i: None, r_t: [-959.393 -959.393 -959.393], eps: 0.01})
Step:  113700, Reward: [-865.488 -865.488 -865.488] [325.166], Avg: [-617.659 -617.659 -617.659] (0.1000) ({r_i: None, r_t: [-1655.248 -1655.248 -1655.248], eps: 0.1})
Step:   80600, Reward: [-479.193 -479.193 -479.193] [97.947], Avg: [-912.542 -912.542 -912.542] (0.0100) ({r_i: None, r_t: [-1021.110 -1021.110 -1021.110], eps: 0.01})
Step:   26600, Reward: [-404.019 -404.019 -404.019] [65.206], Avg: [-430.366 -430.366 -430.366] (0.0695) ({r_i: None, r_t: [-789.882 -789.882 -789.882], eps: 0.069})
Step:  113800, Reward: [-873.846 -873.846 -873.846] [329.485], Avg: [-617.884 -617.884 -617.884] (0.1000) ({r_i: None, r_t: [-1581.965 -1581.965 -1581.965], eps: 0.1})
Step:   80700, Reward: [-456.579 -456.579 -456.579] [76.891], Avg: [-911.978 -911.978 -911.978] (0.0100) ({r_i: None, r_t: [-952.976 -952.976 -952.976], eps: 0.01})
Step:   26700, Reward: [-386.593 -386.593 -386.593] [70.771], Avg: [-430.203 -430.203 -430.203] (0.0688) ({r_i: None, r_t: [-809.469 -809.469 -809.469], eps: 0.069})
Step:  113900, Reward: [-886.581 -886.581 -886.581] [356.383], Avg: [-618.120 -618.120 -618.120] (0.1000) ({r_i: None, r_t: [-1703.050 -1703.050 -1703.050], eps: 0.1})
Step:   80800, Reward: [-454.162 -454.162 -454.162] [108.669], Avg: [-911.412 -911.412 -911.412] (0.0100) ({r_i: None, r_t: [-961.076 -961.076 -961.076], eps: 0.01})
Step:   80900, Reward: [-455.762 -455.762 -455.762] [98.660], Avg: [-910.850 -910.850 -910.850] (0.0100) ({r_i: None, r_t: [-927.475 -927.475 -927.475], eps: 0.01})
Step:  114000, Reward: [-753.964 -753.964 -753.964] [237.700], Avg: [-618.239 -618.239 -618.239] (0.1000) ({r_i: None, r_t: [-1727.978 -1727.978 -1727.978], eps: 0.1})
Step:   26800, Reward: [-408.768 -408.768 -408.768] [77.324], Avg: [-430.123 -430.123 -430.123] (0.0681) ({r_i: None, r_t: [-791.897 -791.897 -791.897], eps: 0.068})
Step:   81000, Reward: [-473.334 -473.334 -473.334] [88.277], Avg: [-910.310 -910.310 -910.310] (0.0100) ({r_i: None, r_t: [-980.201 -980.201 -980.201], eps: 0.01})
Step:  114100, Reward: [-926.758 -926.758 -926.758] [395.516], Avg: [-618.509 -618.509 -618.509] (0.1000) ({r_i: None, r_t: [-1885.607 -1885.607 -1885.607], eps: 0.1})
Step:   26900, Reward: [-385.185 -385.185 -385.185] [66.032], Avg: [-429.957 -429.957 -429.957] (0.0674) ({r_i: None, r_t: [-813.394 -813.394 -813.394], eps: 0.067})
Step:   81100, Reward: [-468.495 -468.495 -468.495] [75.144], Avg: [-909.766 -909.766 -909.766] (0.0100) ({r_i: None, r_t: [-964.095 -964.095 -964.095], eps: 0.01})
Step:  114200, Reward: [-1021.283 -1021.283 -1021.283] [369.194], Avg: [-618.862 -618.862 -618.862] (0.1000) ({r_i: None, r_t: [-1880.404 -1880.404 -1880.404], eps: 0.1})
Step:   81200, Reward: [-490.664 -490.664 -490.664] [75.663], Avg: [-909.251 -909.251 -909.251] (0.0100) ({r_i: None, r_t: [-922.505 -922.505 -922.505], eps: 0.01})
Step:   27000, Reward: [-380.331 -380.331 -380.331] [50.281], Avg: [-429.774 -429.774 -429.774] (0.0668) ({r_i: None, r_t: [-814.631 -814.631 -814.631], eps: 0.067})
Step:  114300, Reward: [-883.607 -883.607 -883.607] [410.211], Avg: [-619.093 -619.093 -619.093] (0.1000) ({r_i: None, r_t: [-1732.209 -1732.209 -1732.209], eps: 0.1})
Step:   81300, Reward: [-436.766 -436.766 -436.766] [115.097], Avg: [-908.670 -908.670 -908.670] (0.0100) ({r_i: None, r_t: [-888.204 -888.204 -888.204], eps: 0.01})
Step:   27100, Reward: [-391.434 -391.434 -391.434] [55.337], Avg: [-429.633 -429.633 -429.633] (0.0661) ({r_i: None, r_t: [-798.915 -798.915 -798.915], eps: 0.066})
Step:  114400, Reward: [-907.657 -907.657 -907.657] [460.263], Avg: [-619.345 -619.345 -619.345] (0.1000) ({r_i: None, r_t: [-1700.147 -1700.147 -1700.147], eps: 0.1})
Step:   81400, Reward: [-462.019 -462.019 -462.019] [85.222], Avg: [-908.122 -908.122 -908.122] (0.0100) ({r_i: None, r_t: [-909.577 -909.577 -909.577], eps: 0.01})
Step:  114500, Reward: [-879.299 -879.299 -879.299] [374.135], Avg: [-619.572 -619.572 -619.572] (0.1000) ({r_i: None, r_t: [-1665.969 -1665.969 -1665.969], eps: 0.1})
Step:   81500, Reward: [-461.177 -461.177 -461.177] [77.795], Avg: [-907.574 -907.574 -907.574] (0.0100) ({r_i: None, r_t: [-894.557 -894.557 -894.557], eps: 0.01})
Step:   27200, Reward: [-382.298 -382.298 -382.298] [55.734], Avg: [-429.459 -429.459 -429.459] (0.0654) ({r_i: None, r_t: [-758.601 -758.601 -758.601], eps: 0.065})
Step:  114600, Reward: [-744.911 -744.911 -744.911] [242.406], Avg: [-619.681 -619.681 -619.681] (0.1000) ({r_i: None, r_t: [-1569.362 -1569.362 -1569.362], eps: 0.1})
Step:   81600, Reward: [-436.315 -436.315 -436.315] [81.697], Avg: [-906.998 -906.998 -906.998] (0.0100) ({r_i: None, r_t: [-904.145 -904.145 -904.145], eps: 0.01})
Step:   27300, Reward: [-413.213 -413.213 -413.213] [64.270], Avg: [-429.400 -429.400 -429.400] (0.0648) ({r_i: None, r_t: [-771.747 -771.747 -771.747], eps: 0.065})
Step:  114700, Reward: [-956.532 -956.532 -956.532] [401.386], Avg: [-619.975 -619.975 -619.975] (0.1000) ({r_i: None, r_t: [-1872.339 -1872.339 -1872.339], eps: 0.1})
Step:   81700, Reward: [-519.004 -519.004 -519.004] [98.308], Avg: [-906.523 -906.523 -906.523] (0.0100) ({r_i: None, r_t: [-924.212 -924.212 -924.212], eps: 0.01})
Step:   27400, Reward: [-404.602 -404.602 -404.602] [74.150], Avg: [-429.310 -429.310 -429.310] (0.0641) ({r_i: None, r_t: [-789.135 -789.135 -789.135], eps: 0.064})
Step:   81800, Reward: [-457.144 -457.144 -457.144] [68.660], Avg: [-905.974 -905.974 -905.974] (0.0100) ({r_i: None, r_t: [-921.340 -921.340 -921.340], eps: 0.01})
Step:  114800, Reward: [-783.510 -783.510 -783.510] [288.320], Avg: [-620.117 -620.117 -620.117] (0.1000) ({r_i: None, r_t: [-1901.913 -1901.913 -1901.913], eps: 0.1})
Step:   81900, Reward: [-459.154 -459.154 -459.154] [83.857], Avg: [-905.430 -905.430 -905.430] (0.0100) ({r_i: None, r_t: [-941.202 -941.202 -941.202], eps: 0.01})
Step:  114900, Reward: [-1028.030 -1028.030 -1028.030] [375.550], Avg: [-620.472 -620.472 -620.472] (0.1000) ({r_i: None, r_t: [-1649.524 -1649.524 -1649.524], eps: 0.1})
Step:   27500, Reward: [-373.203 -373.203 -373.203] [62.003], Avg: [-429.106 -429.106 -429.106] (0.0635) ({r_i: None, r_t: [-825.962 -825.962 -825.962], eps: 0.063})
Step:   82000, Reward: [-459.369 -459.369 -459.369] [92.776], Avg: [-904.886 -904.886 -904.886] (0.0100) ({r_i: None, r_t: [-919.165 -919.165 -919.165], eps: 0.01})
Step:  115000, Reward: [-881.711 -881.711 -881.711] [296.093], Avg: [-620.699 -620.699 -620.699] (0.1000) ({r_i: None, r_t: [-1728.502 -1728.502 -1728.502], eps: 0.1})
Step:   27600, Reward: [-365.483 -365.483 -365.483] [46.942], Avg: [-428.877 -428.877 -428.877] (0.0629) ({r_i: None, r_t: [-777.165 -777.165 -777.165], eps: 0.063})
Step:   82100, Reward: [-451.137 -451.137 -451.137] [84.794], Avg: [-904.334 -904.334 -904.334] (0.0100) ({r_i: None, r_t: [-890.747 -890.747 -890.747], eps: 0.01})
Step:  115100, Reward: [-1015.942 -1015.942 -1015.942] [460.748], Avg: [-621.042 -621.042 -621.042] (0.1000) ({r_i: None, r_t: [-1872.755 -1872.755 -1872.755], eps: 0.1})
Step:   82200, Reward: [-424.008 -424.008 -424.008] [67.157], Avg: [-903.751 -903.751 -903.751] (0.0100) ({r_i: None, r_t: [-849.590 -849.590 -849.590], eps: 0.01})
Step:   27700, Reward: [-389.606 -389.606 -389.606] [55.687], Avg: [-428.736 -428.736 -428.736] (0.0622) ({r_i: None, r_t: [-720.680 -720.680 -720.680], eps: 0.062})
Step:  115200, Reward: [-1068.766 -1068.766 -1068.766] [385.062], Avg: [-621.430 -621.430 -621.430] (0.1000) ({r_i: None, r_t: [-2010.710 -2010.710 -2010.710], eps: 0.1})
Step:   82300, Reward: [-414.946 -414.946 -414.946] [97.533], Avg: [-903.157 -903.157 -903.157] (0.0100) ({r_i: None, r_t: [-864.026 -864.026 -864.026], eps: 0.01})
Step:  115300, Reward: [-925.478 -925.478 -925.478] [297.886], Avg: [-621.693 -621.693 -621.693] (0.1000) ({r_i: None, r_t: [-1585.583 -1585.583 -1585.583], eps: 0.1})
Step:   27800, Reward: [-435.059 -435.059 -435.059] [59.076], Avg: [-428.758 -428.758 -428.758] (0.0616) ({r_i: None, r_t: [-766.157 -766.157 -766.157], eps: 0.062})
Step:   82400, Reward: [-424.562 -424.562 -424.562] [85.882], Avg: [-902.577 -902.577 -902.577] (0.0100) ({r_i: None, r_t: [-911.528 -911.528 -911.528], eps: 0.01})
Step:  115400, Reward: [-1086.305 -1086.305 -1086.305] [471.633], Avg: [-622.096 -622.096 -622.096] (0.1000) ({r_i: None, r_t: [-1792.030 -1792.030 -1792.030], eps: 0.1})
Step:   82500, Reward: [-451.138 -451.138 -451.138] [120.317], Avg: [-902.031 -902.031 -902.031] (0.0100) ({r_i: None, r_t: [-904.134 -904.134 -904.134], eps: 0.01})
Step:   27900, Reward: [-412.879 -412.879 -412.879] [78.631], Avg: [-428.701 -428.701 -428.701] (0.0610) ({r_i: None, r_t: [-811.707 -811.707 -811.707], eps: 0.061})
Step:   82600, Reward: [-437.767 -437.767 -437.767] [101.449], Avg: [-901.469 -901.469 -901.469] (0.0100) ({r_i: None, r_t: [-876.247 -876.247 -876.247], eps: 0.01})
Step:  115500, Reward: [-910.675 -910.675 -910.675] [275.723], Avg: [-622.345 -622.345 -622.345] (0.1000) ({r_i: None, r_t: [-2036.613 -2036.613 -2036.613], eps: 0.1})
Step:   28000, Reward: [-380.643 -380.643 -380.643] [64.955], Avg: [-428.530 -428.530 -428.530] (0.0604) ({r_i: None, r_t: [-762.729 -762.729 -762.729], eps: 0.06})
Step:   82700, Reward: [-459.734 -459.734 -459.734] [98.771], Avg: [-900.936 -900.936 -900.936] (0.0100) ({r_i: None, r_t: [-848.974 -848.974 -848.974], eps: 0.01})
Step:  115600, Reward: [-973.029 -973.029 -973.029] [439.664], Avg: [-622.648 -622.648 -622.648] (0.1000) ({r_i: None, r_t: [-1862.524 -1862.524 -1862.524], eps: 0.1})
Step:   82800, Reward: [-445.467 -445.467 -445.467] [69.122], Avg: [-900.386 -900.386 -900.386] (0.0100) ({r_i: None, r_t: [-867.917 -867.917 -867.917], eps: 0.01})
Step:   28100, Reward: [-409.655 -409.655 -409.655] [72.050], Avg: [-428.464 -428.464 -428.464] (0.0598) ({r_i: None, r_t: [-754.688 -754.688 -754.688], eps: 0.06})
Step:  115700, Reward: [-1087.494 -1087.494 -1087.494] [333.570], Avg: [-623.050 -623.050 -623.050] (0.1000) ({r_i: None, r_t: [-1761.022 -1761.022 -1761.022], eps: 0.1})
Step:   82900, Reward: [-464.825 -464.825 -464.825] [115.755], Avg: [-899.862 -899.862 -899.862] (0.0100) ({r_i: None, r_t: [-848.256 -848.256 -848.256], eps: 0.01})
Step:  115800, Reward: [-1024.807 -1024.807 -1024.807] [339.941], Avg: [-623.396 -623.396 -623.396] (0.1000) ({r_i: None, r_t: [-1811.309 -1811.309 -1811.309], eps: 0.1})
Step:   28200, Reward: [-385.135 -385.135 -385.135] [62.386], Avg: [-428.310 -428.310 -428.310] (0.0592) ({r_i: None, r_t: [-796.637 -796.637 -796.637], eps: 0.059})
Step:   83000, Reward: [-431.682 -431.682 -431.682] [70.864], Avg: [-899.298 -899.298 -899.298] (0.0100) ({r_i: None, r_t: [-854.922 -854.922 -854.922], eps: 0.01})
Step:  115900, Reward: [-1057.381 -1057.381 -1057.381] [393.046], Avg: [-623.771 -623.771 -623.771] (0.1000) ({r_i: None, r_t: [-1850.156 -1850.156 -1850.156], eps: 0.1})
Step:   83100, Reward: [-426.471 -426.471 -426.471] [63.073], Avg: [-898.730 -898.730 -898.730] (0.0100) ({r_i: None, r_t: [-893.913 -893.913 -893.913], eps: 0.01})
Step:   28300, Reward: [-354.534 -354.534 -354.534] [48.180], Avg: [-428.051 -428.051 -428.051] (0.0586) ({r_i: None, r_t: [-807.826 -807.826 -807.826], eps: 0.059})
Step:   83200, Reward: [-409.565 -409.565 -409.565] [92.162], Avg: [-898.143 -898.143 -898.143] (0.0100) ({r_i: None, r_t: [-878.924 -878.924 -878.924], eps: 0.01})
Step:  116000, Reward: [-912.364 -912.364 -912.364] [284.671], Avg: [-624.019 -624.019 -624.019] (0.1000) ({r_i: None, r_t: [-1815.401 -1815.401 -1815.401], eps: 0.1})
Step:   28400, Reward: [-402.281 -402.281 -402.281] [69.662], Avg: [-427.960 -427.960 -427.960] (0.0580) ({r_i: None, r_t: [-815.203 -815.203 -815.203], eps: 0.058})
Step:   83300, Reward: [-409.545 -409.545 -409.545] [72.629], Avg: [-897.557 -897.557 -897.557] (0.0100) ({r_i: None, r_t: [-804.710 -804.710 -804.710], eps: 0.01})
Step:  116100, Reward: [-972.867 -972.867 -972.867] [270.275], Avg: [-624.319 -624.319 -624.319] (0.1000) ({r_i: None, r_t: [-1741.208 -1741.208 -1741.208], eps: 0.1})
Step:   83400, Reward: [-422.502 -422.502 -422.502] [96.744], Avg: [-896.988 -896.988 -896.988] (0.0100) ({r_i: None, r_t: [-828.939 -828.939 -828.939], eps: 0.01})
Step:  116200, Reward: [-940.236 -940.236 -940.236] [287.863], Avg: [-624.591 -624.591 -624.591] (0.1000) ({r_i: None, r_t: [-1823.280 -1823.280 -1823.280], eps: 0.1})
Step:   28500, Reward: [-413.290 -413.290 -413.290] [61.192], Avg: [-427.909 -427.909 -427.909] (0.0574) ({r_i: None, r_t: [-758.105 -758.105 -758.105], eps: 0.057})
Step:   83500, Reward: [-472.168 -472.168 -472.168] [98.841], Avg: [-896.480 -896.480 -896.480] (0.0100) ({r_i: None, r_t: [-874.429 -874.429 -874.429], eps: 0.01})
Step:  116300, Reward: [-1014.656 -1014.656 -1014.656] [328.905], Avg: [-624.926 -624.926 -624.926] (0.1000) ({r_i: None, r_t: [-2135.529 -2135.529 -2135.529], eps: 0.1})
Step:   28600, Reward: [-374.513 -374.513 -374.513] [41.372], Avg: [-427.723 -427.723 -427.723] (0.0569) ({r_i: None, r_t: [-796.073 -796.073 -796.073], eps: 0.057})
Step:   83600, Reward: [-439.599 -439.599 -439.599] [84.005], Avg: [-895.934 -895.934 -895.934] (0.0100) ({r_i: None, r_t: [-857.074 -857.074 -857.074], eps: 0.01})
Step:  116400, Reward: [-822.206 -822.206 -822.206] [305.060], Avg: [-625.095 -625.095 -625.095] (0.1000) ({r_i: None, r_t: [-2037.280 -2037.280 -2037.280], eps: 0.1})
Step:   83700, Reward: [-466.965 -466.965 -466.965] [84.751], Avg: [-895.422 -895.422 -895.422] (0.0100) ({r_i: None, r_t: [-823.139 -823.139 -823.139], eps: 0.01})
Step:  116500, Reward: [-934.879 -934.879 -934.879] [291.765], Avg: [-625.361 -625.361 -625.361] (0.1000) ({r_i: None, r_t: [-1957.855 -1957.855 -1957.855], eps: 0.1})
Step:   28700, Reward: [-389.411 -389.411 -389.411] [61.106], Avg: [-427.590 -427.590 -427.590] (0.0563) ({r_i: None, r_t: [-776.740 -776.740 -776.740], eps: 0.056})
Step:   83800, Reward: [-436.050 -436.050 -436.050] [103.454], Avg: [-894.875 -894.875 -894.875] (0.0100) ({r_i: None, r_t: [-904.519 -904.519 -904.519], eps: 0.01})
Step:  116600, Reward: [-1006.206 -1006.206 -1006.206] [335.561], Avg: [-625.687 -625.687 -625.687] (0.1000) ({r_i: None, r_t: [-2132.692 -2132.692 -2132.692], eps: 0.1})
Step:   28800, Reward: [-379.122 -379.122 -379.122] [62.650], Avg: [-427.422 -427.422 -427.422] (0.0557) ({r_i: None, r_t: [-774.356 -774.356 -774.356], eps: 0.056})
Step:  116700, Reward: [-868.104 -868.104 -868.104] [346.917], Avg: [-625.895 -625.895 -625.895] (0.1000) ({r_i: None, r_t: [-2064.015 -2064.015 -2064.015], eps: 0.1})
Step:   83900, Reward: [-437.105 -437.105 -437.105] [93.254], Avg: [-894.330 -894.330 -894.330] (0.0100) ({r_i: None, r_t: [-881.858 -881.858 -881.858], eps: 0.01})
Step:  116800, Reward: [-906.373 -906.373 -906.373] [345.808], Avg: [-626.135 -626.135 -626.135] (0.1000) ({r_i: None, r_t: [-1821.000 -1821.000 -1821.000], eps: 0.1})
Step:   84000, Reward: [-417.405 -417.405 -417.405] [83.668], Avg: [-893.763 -893.763 -893.763] (0.0100) ({r_i: None, r_t: [-925.607 -925.607 -925.607], eps: 0.01})
Step:   28900, Reward: [-399.334 -399.334 -399.334] [61.031], Avg: [-427.325 -427.325 -427.325] (0.0552) ({r_i: None, r_t: [-802.581 -802.581 -802.581], eps: 0.055})
Step:  116900, Reward: [-950.823 -950.823 -950.823] [383.952], Avg: [-626.412 -626.412 -626.412] (0.1000) ({r_i: None, r_t: [-2029.738 -2029.738 -2029.738], eps: 0.1})
Step:   84100, Reward: [-438.255 -438.255 -438.255] [130.215], Avg: [-893.222 -893.222 -893.222] (0.0100) ({r_i: None, r_t: [-899.656 -899.656 -899.656], eps: 0.01})
Step:   29000, Reward: [-385.122 -385.122 -385.122] [72.322], Avg: [-427.180 -427.180 -427.180] (0.0546) ({r_i: None, r_t: [-769.469 -769.469 -769.469], eps: 0.055})
Step:  117000, Reward: [-1093.507 -1093.507 -1093.507] [357.744], Avg: [-626.811 -626.811 -626.811] (0.1000) ({r_i: None, r_t: [-1771.820 -1771.820 -1771.820], eps: 0.1})
Step:   84200, Reward: [-478.446 -478.446 -478.446] [122.673], Avg: [-892.730 -892.730 -892.730] (0.0100) ({r_i: None, r_t: [-852.686 -852.686 -852.686], eps: 0.01})
Step:  117100, Reward: [-932.409 -932.409 -932.409] [348.641], Avg: [-627.072 -627.072 -627.072] (0.1000) ({r_i: None, r_t: [-2041.849 -2041.849 -2041.849], eps: 0.1})
Step:   84300, Reward: [-438.172 -438.172 -438.172] [98.941], Avg: [-892.191 -892.191 -892.191] (0.0100) ({r_i: None, r_t: [-859.619 -859.619 -859.619], eps: 0.01})
Step:   29100, Reward: [-375.937 -375.937 -375.937] [55.517], Avg: [-427.005 -427.005 -427.005] (0.0541) ({r_i: None, r_t: [-763.027 -763.027 -763.027], eps: 0.054})
Step:  117200, Reward: [-1019.046 -1019.046 -1019.046] [278.427], Avg: [-627.406 -627.406 -627.406] (0.1000) ({r_i: None, r_t: [-2205.464 -2205.464 -2205.464], eps: 0.1})
Step:   84400, Reward: [-427.529 -427.529 -427.529] [77.203], Avg: [-891.641 -891.641 -891.641] (0.0100) ({r_i: None, r_t: [-871.854 -871.854 -871.854], eps: 0.01})
Step:  117300, Reward: [-957.582 -957.582 -957.582] [314.478], Avg: [-627.688 -627.688 -627.688] (0.1000) ({r_i: None, r_t: [-2300.183 -2300.183 -2300.183], eps: 0.1})
Step:   29200, Reward: [-399.483 -399.483 -399.483] [43.222], Avg: [-426.911 -426.911 -426.911] (0.0535) ({r_i: None, r_t: [-786.081 -786.081 -786.081], eps: 0.054})
Step:   84500, Reward: [-431.454 -431.454 -431.454] [101.694], Avg: [-891.097 -891.097 -891.097] (0.0100) ({r_i: None, r_t: [-891.647 -891.647 -891.647], eps: 0.01})
Step:  117400, Reward: [-1152.855 -1152.855 -1152.855] [416.531], Avg: [-628.134 -628.134 -628.134] (0.1000) ({r_i: None, r_t: [-1847.835 -1847.835 -1847.835], eps: 0.1})
Step:   84600, Reward: [-433.192 -433.192 -433.192] [90.885], Avg: [-890.556 -890.556 -890.556] (0.0100) ({r_i: None, r_t: [-899.045 -899.045 -899.045], eps: 0.01})
Step:   29300, Reward: [-380.556 -380.556 -380.556] [49.017], Avg: [-426.753 -426.753 -426.753] (0.0530) ({r_i: None, r_t: [-768.640 -768.640 -768.640], eps: 0.053})
Step:  117500, Reward: [-1146.846 -1146.846 -1146.846] [286.458], Avg: [-628.576 -628.576 -628.576] (0.1000) ({r_i: None, r_t: [-1935.504 -1935.504 -1935.504], eps: 0.1})
Step:   84700, Reward: [-476.430 -476.430 -476.430] [99.336], Avg: [-890.068 -890.068 -890.068] (0.0100) ({r_i: None, r_t: [-854.075 -854.075 -854.075], eps: 0.01})
Step:  117600, Reward: [-1176.110 -1176.110 -1176.110] [447.361], Avg: [-629.041 -629.041 -629.041] (0.1000) ({r_i: None, r_t: [-2207.777 -2207.777 -2207.777], eps: 0.1})
Step:   29400, Reward: [-383.697 -383.697 -383.697] [49.550], Avg: [-426.607 -426.607 -426.607] (0.0525) ({r_i: None, r_t: [-763.130 -763.130 -763.130], eps: 0.052})
Step:   84800, Reward: [-416.123 -416.123 -416.123] [91.966], Avg: [-889.510 -889.510 -889.510] (0.0100) ({r_i: None, r_t: [-888.839 -888.839 -888.839], eps: 0.01})
Step:  117700, Reward: [-906.268 -906.268 -906.268] [278.012], Avg: [-629.276 -629.276 -629.276] (0.1000) ({r_i: None, r_t: [-2245.978 -2245.978 -2245.978], eps: 0.1})
Step:   84900, Reward: [-473.488 -473.488 -473.488] [82.353], Avg: [-889.020 -889.020 -889.020] (0.0100) ({r_i: None, r_t: [-889.408 -889.408 -889.408], eps: 0.01})
Step:   29500, Reward: [-408.166 -408.166 -408.166] [61.211], Avg: [-426.545 -426.545 -426.545] (0.0520) ({r_i: None, r_t: [-743.152 -743.152 -743.152], eps: 0.052})
Step:  117800, Reward: [-1064.447 -1064.447 -1064.447] [238.578], Avg: [-629.645 -629.645 -629.645] (0.1000) ({r_i: None, r_t: [-2142.520 -2142.520 -2142.520], eps: 0.1})
Step:   85000, Reward: [-452.138 -452.138 -452.138] [91.257], Avg: [-888.507 -888.507 -888.507] (0.0100) ({r_i: None, r_t: [-848.158 -848.158 -848.158], eps: 0.01})
Step:  117900, Reward: [-1301.045 -1301.045 -1301.045] [563.092], Avg: [-630.214 -630.214 -630.214] (0.1000) ({r_i: None, r_t: [-2104.399 -2104.399 -2104.399], eps: 0.1})
Step:   29600, Reward: [-378.440 -378.440 -378.440] [64.130], Avg: [-426.383 -426.383 -426.383] (0.0514) ({r_i: None, r_t: [-760.292 -760.292 -760.292], eps: 0.051})
Step:   85100, Reward: [-404.015 -404.015 -404.015] [77.540], Avg: [-887.938 -887.938 -887.938] (0.0100) ({r_i: None, r_t: [-831.878 -831.878 -831.878], eps: 0.01})
Step:  118000, Reward: [-1035.883 -1035.883 -1035.883] [437.354], Avg: [-630.558 -630.558 -630.558] (0.1000) ({r_i: None, r_t: [-2306.268 -2306.268 -2306.268], eps: 0.1})
Step:   85200, Reward: [-442.533 -442.533 -442.533] [112.999], Avg: [-887.416 -887.416 -887.416] (0.0100) ({r_i: None, r_t: [-893.666 -893.666 -893.666], eps: 0.01})
Step:   29700, Reward: [-385.874 -385.874 -385.874] [59.955], Avg: [-426.247 -426.247 -426.247] (0.0509) ({r_i: None, r_t: [-806.559 -806.559 -806.559], eps: 0.051})
Step:  118100, Reward: [-1086.123 -1086.123 -1086.123] [336.345], Avg: [-630.943 -630.943 -630.943] (0.1000) ({r_i: None, r_t: [-2186.203 -2186.203 -2186.203], eps: 0.1})
Step:   85300, Reward: [-424.617 -424.617 -424.617] [92.602], Avg: [-886.874 -886.874 -886.874] (0.0100) ({r_i: None, r_t: [-830.805 -830.805 -830.805], eps: 0.01})
Step:  118200, Reward: [-1114.217 -1114.217 -1114.217] [373.721], Avg: [-631.352 -631.352 -631.352] (0.1000) ({r_i: None, r_t: [-1883.043 -1883.043 -1883.043], eps: 0.1})
Step:   29800, Reward: [-397.728 -397.728 -397.728] [66.033], Avg: [-426.152 -426.152 -426.152] (0.0504) ({r_i: None, r_t: [-790.844 -790.844 -790.844], eps: 0.05})
Step:   85400, Reward: [-417.033 -417.033 -417.033] [73.744], Avg: [-886.325 -886.325 -886.325] (0.0100) ({r_i: None, r_t: [-898.612 -898.612 -898.612], eps: 0.01})
Step:  118300, Reward: [-1243.456 -1243.456 -1243.456] [423.047], Avg: [-631.869 -631.869 -631.869] (0.1000) ({r_i: None, r_t: [-1993.046 -1993.046 -1993.046], eps: 0.1})
Step:   29900, Reward: [-416.645 -416.645 -416.645] [73.422], Avg: [-426.120 -426.120 -426.120] (0.0499) ({r_i: None, r_t: [-819.609 -819.609 -819.609], eps: 0.05})
Step:   85500, Reward: [-451.165 -451.165 -451.165] [100.730], Avg: [-885.816 -885.816 -885.816] (0.0100) ({r_i: None, r_t: [-906.947 -906.947 -906.947], eps: 0.01})
Step:  118400, Reward: [-1124.612 -1124.612 -1124.612] [422.049], Avg: [-632.284 -632.284 -632.284] (0.1000) ({r_i: None, r_t: [-2226.077 -2226.077 -2226.077], eps: 0.1})
Step:   85600, Reward: [-475.799 -475.799 -475.799] [90.482], Avg: [-885.338 -885.338 -885.338] (0.0100) ({r_i: None, r_t: [-875.588 -875.588 -875.588], eps: 0.01})
Step:  118500, Reward: [-1062.905 -1062.905 -1062.905] [408.365], Avg: [-632.647 -632.647 -632.647] (0.1000) ({r_i: None, r_t: [-2175.210 -2175.210 -2175.210], eps: 0.1})
Step:   30000, Reward: [-403.755 -403.755 -403.755] [55.819], Avg: [-426.046 -426.046 -426.046] (0.0494) ({r_i: None, r_t: [-782.096 -782.096 -782.096], eps: 0.049})
Step:   85700, Reward: [-405.302 -405.302 -405.302] [83.379], Avg: [-884.779 -884.779 -884.779] (0.0100) ({r_i: None, r_t: [-918.662 -918.662 -918.662], eps: 0.01})
Step:  118600, Reward: [-1180.489 -1180.489 -1180.489] [414.027], Avg: [-633.109 -633.109 -633.109] (0.1000) ({r_i: None, r_t: [-2384.123 -2384.123 -2384.123], eps: 0.1})
Step:   30100, Reward: [-410.425 -410.425 -410.425] [81.433], Avg: [-425.994 -425.994 -425.994] (0.0489) ({r_i: None, r_t: [-793.559 -793.559 -793.559], eps: 0.049})
Step:   85800, Reward: [-406.349 -406.349 -406.349] [73.863], Avg: [-884.222 -884.222 -884.222] (0.0100) ({r_i: None, r_t: [-848.363 -848.363 -848.363], eps: 0.01})
Step:  118700, Reward: [-1057.388 -1057.388 -1057.388] [356.488], Avg: [-633.466 -633.466 -633.466] (0.1000) ({r_i: None, r_t: [-2243.576 -2243.576 -2243.576], eps: 0.1})
Step:   85900, Reward: [-476.411 -476.411 -476.411] [79.510], Avg: [-883.747 -883.747 -883.747] (0.0100) ({r_i: None, r_t: [-850.625 -850.625 -850.625], eps: 0.01})
Step:  118800, Reward: [-1053.124 -1053.124 -1053.124] [305.233], Avg: [-633.819 -633.819 -633.819] (0.1000) ({r_i: None, r_t: [-2179.108 -2179.108 -2179.108], eps: 0.1})
Step:   30200, Reward: [-379.578 -379.578 -379.578] [61.776], Avg: [-425.841 -425.841 -425.841] (0.0484) ({r_i: None, r_t: [-822.805 -822.805 -822.805], eps: 0.048})
Step:  118900, Reward: [-1025.149 -1025.149 -1025.149] [320.465], Avg: [-634.148 -634.148 -634.148] (0.1000) ({r_i: None, r_t: [-2133.966 -2133.966 -2133.966], eps: 0.1})
Step:   86000, Reward: [-441.434 -441.434 -441.434] [107.597], Avg: [-883.234 -883.234 -883.234] (0.0100) ({r_i: None, r_t: [-828.960 -828.960 -828.960], eps: 0.01})
Step:   30300, Reward: [-362.760 -362.760 -362.760] [50.457], Avg: [-425.633 -425.633 -425.633] (0.0479) ({r_i: None, r_t: [-763.669 -763.669 -763.669], eps: 0.048})
Step:  119000, Reward: [-1364.490 -1364.490 -1364.490] [393.407], Avg: [-634.761 -634.761 -634.761] (0.1000) ({r_i: None, r_t: [-2097.177 -2097.177 -2097.177], eps: 0.1})
Step:   86100, Reward: [-404.533 -404.533 -404.533] [71.059], Avg: [-882.678 -882.678 -882.678] (0.0100) ({r_i: None, r_t: [-888.911 -888.911 -888.911], eps: 0.01})
Step:  119100, Reward: [-1229.935 -1229.935 -1229.935] [426.859], Avg: [-635.260 -635.260 -635.260] (0.1000) ({r_i: None, r_t: [-2371.387 -2371.387 -2371.387], eps: 0.1})
Step:   86200, Reward: [-421.746 -421.746 -421.746] [66.751], Avg: [-882.144 -882.144 -882.144] (0.0100) ({r_i: None, r_t: [-853.484 -853.484 -853.484], eps: 0.01})
Step:   30400, Reward: [-402.725 -402.725 -402.725] [60.068], Avg: [-425.558 -425.558 -425.558] (0.0475) ({r_i: None, r_t: [-823.201 -823.201 -823.201], eps: 0.047})
Step:  119200, Reward: [-1022.422 -1022.422 -1022.422] [273.921], Avg: [-635.585 -635.585 -635.585] (0.1000) ({r_i: None, r_t: [-2629.921 -2629.921 -2629.921], eps: 0.1})
Step:   86300, Reward: [-445.773 -445.773 -445.773] [71.497], Avg: [-881.639 -881.639 -881.639] (0.0100) ({r_i: None, r_t: [-876.479 -876.479 -876.479], eps: 0.01})
Step:   30500, Reward: [-423.248 -423.248 -423.248] [59.171], Avg: [-425.551 -425.551 -425.551] (0.0470) ({r_i: None, r_t: [-776.131 -776.131 -776.131], eps: 0.047})
Step:  119300, Reward: [-1188.012 -1188.012 -1188.012] [342.420], Avg: [-636.048 -636.048 -636.048] (0.1000) ({r_i: None, r_t: [-2574.754 -2574.754 -2574.754], eps: 0.1})
Step:   86400, Reward: [-414.375 -414.375 -414.375] [76.994], Avg: [-881.099 -881.099 -881.099] (0.0100) ({r_i: None, r_t: [-865.548 -865.548 -865.548], eps: 0.01})
Step:  119400, Reward: [-1104.444 -1104.444 -1104.444] [369.390], Avg: [-636.440 -636.440 -636.440] (0.1000) ({r_i: None, r_t: [-2216.110 -2216.110 -2216.110], eps: 0.1})
Step:   86500, Reward: [-483.612 -483.612 -483.612] [93.782], Avg: [-880.640 -880.640 -880.640] (0.0100) ({r_i: None, r_t: [-923.280 -923.280 -923.280], eps: 0.01})
Step:   30600, Reward: [-423.517 -423.517 -423.517] [53.840], Avg: [-425.544 -425.544 -425.544] (0.0465) ({r_i: None, r_t: [-845.747 -845.747 -845.747], eps: 0.047})
Step:  119500, Reward: [-1159.115 -1159.115 -1159.115] [484.813], Avg: [-636.877 -636.877 -636.877] (0.1000) ({r_i: None, r_t: [-2586.279 -2586.279 -2586.279], eps: 0.1})
Step:   86600, Reward: [-380.180 -380.180 -380.180] [89.500], Avg: [-880.063 -880.063 -880.063] (0.0100) ({r_i: None, r_t: [-915.556 -915.556 -915.556], eps: 0.01})
Step:   30700, Reward: [-390.536 -390.536 -390.536] [49.238], Avg: [-425.430 -425.430 -425.430] (0.0461) ({r_i: None, r_t: [-739.077 -739.077 -739.077], eps: 0.046})
Step:  119600, Reward: [-1218.289 -1218.289 -1218.289] [354.801], Avg: [-637.362 -637.362 -637.362] (0.1000) ({r_i: None, r_t: [-2251.898 -2251.898 -2251.898], eps: 0.1})
Step:   86700, Reward: [-485.124 -485.124 -485.124] [114.137], Avg: [-879.608 -879.608 -879.608] (0.0100) ({r_i: None, r_t: [-900.584 -900.584 -900.584], eps: 0.01})
Step:  119700, Reward: [-1186.512 -1186.512 -1186.512] [476.796], Avg: [-637.821 -637.821 -637.821] (0.1000) ({r_i: None, r_t: [-2608.258 -2608.258 -2608.258], eps: 0.1})
Step:   86800, Reward: [-413.138 -413.138 -413.138] [88.288], Avg: [-879.071 -879.071 -879.071] (0.0100) ({r_i: None, r_t: [-862.057 -862.057 -862.057], eps: 0.01})
Step:   30800, Reward: [-407.964 -407.964 -407.964] [94.503], Avg: [-425.374 -425.374 -425.374] (0.0456) ({r_i: None, r_t: [-791.076 -791.076 -791.076], eps: 0.046})
Step:  119800, Reward: [-1218.235 -1218.235 -1218.235] [298.496], Avg: [-638.305 -638.305 -638.305] (0.1000) ({r_i: None, r_t: [-2152.130 -2152.130 -2152.130], eps: 0.1})
Step:   86900, Reward: [-452.675 -452.675 -452.675] [115.865], Avg: [-878.581 -878.581 -878.581] (0.0100) ({r_i: None, r_t: [-894.635 -894.635 -894.635], eps: 0.01})
Step:  119900, Reward: [-1132.191 -1132.191 -1132.191] [497.556], Avg: [-638.716 -638.716 -638.716] (0.1000) ({r_i: None, r_t: [-2407.980 -2407.980 -2407.980], eps: 0.1})
Step:   30900, Reward: [-404.392 -404.392 -404.392] [65.269], Avg: [-425.306 -425.306 -425.306] (0.0452) ({r_i: None, r_t: [-799.301 -799.301 -799.301], eps: 0.045})
Step:   87000, Reward: [-444.028 -444.028 -444.028] [85.233], Avg: [-878.082 -878.082 -878.082] (0.0100) ({r_i: None, r_t: [-943.652 -943.652 -943.652], eps: 0.01})
Step:  120000, Reward: [-1359.103 -1359.103 -1359.103] [375.168], Avg: [-639.316 -639.316 -639.316] (0.1000) ({r_i: None, r_t: [-2598.234 -2598.234 -2598.234], eps: 0.1})
Step:   87100, Reward: [-445.278 -445.278 -445.278] [76.232], Avg: [-877.586 -877.586 -877.586] (0.0100) ({r_i: None, r_t: [-925.818 -925.818 -925.818], eps: 0.01})
Step:   31000, Reward: [-434.837 -434.837 -434.837] [70.108], Avg: [-425.337 -425.337 -425.337] (0.0447) ({r_i: None, r_t: [-785.944 -785.944 -785.944], eps: 0.045})
Step:  120100, Reward: [-1143.256 -1143.256 -1143.256] [247.220], Avg: [-639.735 -639.735 -639.735] (0.1000) ({r_i: None, r_t: [-2338.509 -2338.509 -2338.509], eps: 0.1})
Step:   87200, Reward: [-423.547 -423.547 -423.547] [75.008], Avg: [-877.065 -877.065 -877.065] (0.0100) ({r_i: None, r_t: [-845.803 -845.803 -845.803], eps: 0.01})
Step:  120200, Reward: [-1126.968 -1126.968 -1126.968] [364.369], Avg: [-640.140 -640.140 -640.140] (0.1000) ({r_i: None, r_t: [-2255.061 -2255.061 -2255.061], eps: 0.1})
Step:   31100, Reward: [-403.836 -403.836 -403.836] [66.618], Avg: [-425.268 -425.268 -425.268] (0.0443) ({r_i: None, r_t: [-780.740 -780.740 -780.740], eps: 0.044})
Step:   87300, Reward: [-461.723 -461.723 -461.723] [60.883], Avg: [-876.590 -876.590 -876.590] (0.0100) ({r_i: None, r_t: [-884.216 -884.216 -884.216], eps: 0.01})
Step:  120300, Reward: [-1136.259 -1136.259 -1136.259] [413.329], Avg: [-640.553 -640.553 -640.553] (0.1000) ({r_i: None, r_t: [-2320.034 -2320.034 -2320.034], eps: 0.1})
Step:   87400, Reward: [-438.221 -438.221 -438.221] [99.511], Avg: [-876.089 -876.089 -876.089] (0.0100) ({r_i: None, r_t: [-884.866 -884.866 -884.866], eps: 0.01})
Step:   31200, Reward: [-378.943 -378.943 -378.943] [63.818], Avg: [-425.120 -425.120 -425.120] (0.0438) ({r_i: None, r_t: [-801.294 -801.294 -801.294], eps: 0.044})
Step:  120400, Reward: [-1262.711 -1262.711 -1262.711] [304.992], Avg: [-641.069 -641.069 -641.069] (0.1000) ({r_i: None, r_t: [-2488.054 -2488.054 -2488.054], eps: 0.1})
Step:   87500, Reward: [-414.293 -414.293 -414.293] [79.918], Avg: [-875.562 -875.562 -875.562] (0.0100) ({r_i: None, r_t: [-836.328 -836.328 -836.328], eps: 0.01})
Step:  120500, Reward: [-1153.473 -1153.473 -1153.473] [293.285], Avg: [-641.494 -641.494 -641.494] (0.1000) ({r_i: None, r_t: [-2691.764 -2691.764 -2691.764], eps: 0.1})
Step:   31300, Reward: [-396.439 -396.439 -396.439] [61.359], Avg: [-425.028 -425.028 -425.028] (0.0434) ({r_i: None, r_t: [-774.039 -774.039 -774.039], eps: 0.043})
Step:   87600, Reward: [-444.298 -444.298 -444.298] [71.873], Avg: [-875.070 -875.070 -875.070] (0.0100) ({r_i: None, r_t: [-905.662 -905.662 -905.662], eps: 0.01})
Step:  120600, Reward: [-1249.693 -1249.693 -1249.693] [450.997], Avg: [-641.998 -641.998 -641.998] (0.1000) ({r_i: None, r_t: [-2324.576 -2324.576 -2324.576], eps: 0.1})
Step:   31400, Reward: [-375.976 -375.976 -375.976] [60.044], Avg: [-424.873 -424.873 -424.873] (0.0429) ({r_i: None, r_t: [-815.598 -815.598 -815.598], eps: 0.043})
Step:   87700, Reward: [-408.590 -408.590 -408.590] [67.310], Avg: [-874.539 -874.539 -874.539] (0.0100) ({r_i: None, r_t: [-865.093 -865.093 -865.093], eps: 0.01})
Step:  120700, Reward: [-1440.116 -1440.116 -1440.116] [308.881], Avg: [-642.658 -642.658 -642.658] (0.1000) ({r_i: None, r_t: [-2561.666 -2561.666 -2561.666], eps: 0.1})
Step:   87800, Reward: [-451.801 -451.801 -451.801] [79.673], Avg: [-874.058 -874.058 -874.058] (0.0100) ({r_i: None, r_t: [-846.933 -846.933 -846.933], eps: 0.01})
Step:  120800, Reward: [-1260.291 -1260.291 -1260.291] [400.823], Avg: [-643.169 -643.169 -643.169] (0.1000) ({r_i: None, r_t: [-2474.057 -2474.057 -2474.057], eps: 0.1})
Step:   31500, Reward: [-390.722 -390.722 -390.722] [56.833], Avg: [-424.765 -424.765 -424.765] (0.0425) ({r_i: None, r_t: [-766.903 -766.903 -766.903], eps: 0.043})
Step:   87900, Reward: [-447.587 -447.587 -447.587] [91.121], Avg: [-873.574 -873.574 -873.574] (0.0100) ({r_i: None, r_t: [-931.028 -931.028 -931.028], eps: 0.01})
Step:  120900, Reward: [-1273.457 -1273.457 -1273.457] [373.334], Avg: [-643.690 -643.690 -643.690] (0.1000) ({r_i: None, r_t: [-2559.818 -2559.818 -2559.818], eps: 0.1})
Step:   31600, Reward: [-375.676 -375.676 -375.676] [46.231], Avg: [-424.610 -424.610 -424.610] (0.0421) ({r_i: None, r_t: [-799.592 -799.592 -799.592], eps: 0.042})
Step:   88000, Reward: [-460.705 -460.705 -460.705] [93.729], Avg: [-873.105 -873.105 -873.105] (0.0100) ({r_i: None, r_t: [-867.733 -867.733 -867.733], eps: 0.01})
Step:  121000, Reward: [-1295.474 -1295.474 -1295.474] [389.246], Avg: [-644.228 -644.228 -644.228] (0.1000) ({r_i: None, r_t: [-2631.393 -2631.393 -2631.393], eps: 0.1})
Step:   88100, Reward: [-441.348 -441.348 -441.348] [110.825], Avg: [-872.615 -872.615 -872.615] (0.0100) ({r_i: None, r_t: [-891.514 -891.514 -891.514], eps: 0.01})
Step:  121100, Reward: [-1275.028 -1275.028 -1275.028] [426.999], Avg: [-644.749 -644.749 -644.749] (0.1000) ({r_i: None, r_t: [-2606.067 -2606.067 -2606.067], eps: 0.1})
Step:   31700, Reward: [-402.505 -402.505 -402.505] [55.924], Avg: [-424.540 -424.540 -424.540] (0.0417) ({r_i: None, r_t: [-807.825 -807.825 -807.825], eps: 0.042})
Step:   88200, Reward: [-486.787 -486.787 -486.787] [87.042], Avg: [-872.178 -872.178 -872.178] (0.0100) ({r_i: None, r_t: [-884.206 -884.206 -884.206], eps: 0.01})
Step:  121200, Reward: [-1209.933 -1209.933 -1209.933] [379.313], Avg: [-645.215 -645.215 -645.215] (0.1000) ({r_i: None, r_t: [-2676.338 -2676.338 -2676.338], eps: 0.1})
Step:   31800, Reward: [-398.114 -398.114 -398.114] [52.816], Avg: [-424.457 -424.457 -424.457] (0.0413) ({r_i: None, r_t: [-764.142 -764.142 -764.142], eps: 0.041})
Step:   88300, Reward: [-391.695 -391.695 -391.695] [59.001], Avg: [-871.635 -871.635 -871.635] (0.0100) ({r_i: None, r_t: [-842.321 -842.321 -842.321], eps: 0.01})
Step:  121300, Reward: [-1283.965 -1283.965 -1283.965] [366.862], Avg: [-645.741 -645.741 -645.741] (0.1000) ({r_i: None, r_t: [-2307.644 -2307.644 -2307.644], eps: 0.1})
Step:   88400, Reward: [-500.462 -500.462 -500.462] [88.004], Avg: [-871.215 -871.215 -871.215] (0.0100) ({r_i: None, r_t: [-868.613 -868.613 -868.613], eps: 0.01})
Step:  121400, Reward: [-1228.411 -1228.411 -1228.411] [360.362], Avg: [-646.220 -646.220 -646.220] (0.1000) ({r_i: None, r_t: [-2298.041 -2298.041 -2298.041], eps: 0.1})
Step:   31900, Reward: [-373.315 -373.315 -373.315] [53.838], Avg: [-424.298 -424.298 -424.298] (0.0408) ({r_i: None, r_t: [-812.758 -812.758 -812.758], eps: 0.041})
Step:   88500, Reward: [-440.901 -440.901 -440.901] [88.466], Avg: [-870.730 -870.730 -870.730] (0.0100) ({r_i: None, r_t: [-847.488 -847.488 -847.488], eps: 0.01})
Step:  121500, Reward: [-1313.264 -1313.264 -1313.264] [367.670], Avg: [-646.769 -646.769 -646.769] (0.1000) ({r_i: None, r_t: [-2384.475 -2384.475 -2384.475], eps: 0.1})
Step:   32000, Reward: [-377.283 -377.283 -377.283] [49.173], Avg: [-424.151 -424.151 -424.151] (0.0404) ({r_i: None, r_t: [-757.040 -757.040 -757.040], eps: 0.04})
Step:  121600, Reward: [-1451.650 -1451.650 -1451.650] [282.676], Avg: [-647.430 -647.430 -647.430] (0.1000) ({r_i: None, r_t: [-2834.033 -2834.033 -2834.033], eps: 0.1})
Step:   88600, Reward: [-408.091 -408.091 -408.091] [103.800], Avg: [-870.208 -870.208 -870.208] (0.0100) ({r_i: None, r_t: [-832.485 -832.485 -832.485], eps: 0.01})
Step:  121700, Reward: [-1318.002 -1318.002 -1318.002] [363.330], Avg: [-647.981 -647.981 -647.981] (0.1000) ({r_i: None, r_t: [-2583.211 -2583.211 -2583.211], eps: 0.1})
Step:   88700, Reward: [-441.589 -441.589 -441.589] [108.872], Avg: [-869.726 -869.726 -869.726] (0.0100) ({r_i: None, r_t: [-829.711 -829.711 -829.711], eps: 0.01})
Step:   32100, Reward: [-376.829 -376.829 -376.829] [64.448], Avg: [-424.004 -424.004 -424.004] (0.0400) ({r_i: None, r_t: [-764.170 -764.170 -764.170], eps: 0.04})
Step:  121800, Reward: [-1340.583 -1340.583 -1340.583] [277.553], Avg: [-648.549 -648.549 -648.549] (0.1000) ({r_i: None, r_t: [-2482.264 -2482.264 -2482.264], eps: 0.1})
Step:   88800, Reward: [-404.508 -404.508 -404.508] [82.891], Avg: [-869.202 -869.202 -869.202] (0.0100) ({r_i: None, r_t: [-854.136 -854.136 -854.136], eps: 0.01})
Step:  121900, Reward: [-1273.670 -1273.670 -1273.670] [350.139], Avg: [-649.061 -649.061 -649.061] (0.1000) ({r_i: None, r_t: [-2463.932 -2463.932 -2463.932], eps: 0.1})
Step:   88900, Reward: [-414.363 -414.363 -414.363] [91.158], Avg: [-868.691 -868.691 -868.691] (0.0100) ({r_i: None, r_t: [-906.869 -906.869 -906.869], eps: 0.01})
Step:  122000, Reward: [-1237.607 -1237.607 -1237.607] [310.821], Avg: [-649.543 -649.543 -649.543] (0.1000) ({r_i: None, r_t: [-2631.767 -2631.767 -2631.767], eps: 0.1})
Step:   89000, Reward: [-438.205 -438.205 -438.205] [111.798], Avg: [-868.208 -868.208 -868.208] (0.0100) ({r_i: None, r_t: [-910.121 -910.121 -910.121], eps: 0.01})
Step:  122100, Reward: [-1378.228 -1378.228 -1378.228] [401.501], Avg: [-650.140 -650.140 -650.140] (0.1000) ({r_i: None, r_t: [-2692.091 -2692.091 -2692.091], eps: 0.1})
Step:   89100, Reward: [-460.431 -460.431 -460.431] [116.129], Avg: [-867.751 -867.751 -867.751] (0.0100) ({r_i: None, r_t: [-869.300 -869.300 -869.300], eps: 0.01})
Step:  122200, Reward: [-1413.972 -1413.972 -1413.972] [359.799], Avg: [-650.764 -650.764 -650.764] (0.1000) ({r_i: None, r_t: [-2532.901 -2532.901 -2532.901], eps: 0.1})
Step:   89200, Reward: [-438.738 -438.738 -438.738] [97.435], Avg: [-867.270 -867.270 -867.270] (0.0100) ({r_i: None, r_t: [-844.310 -844.310 -844.310], eps: 0.01})
Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, ADVANTAGE_DECAY, DISCOUNT_RATE, one_hot_from_indices

# EPS_MIN = 0.1               	# The lower limit proportion of random to greedy actions to take
# EPS_DECAY = 0.99             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# LEARN_RATE = 0.0003				# Sets how much we want to update the network weights at each training step
# TARGET_UPDATE_RATE = 0.001		# How frequently we want to copy the local network to the target network (for double DQNs)
# EPISODE_BUFFER = 64				# Sets the maximum length of the replay buffer
# REPLAY_BATCH_SIZE = 10			# Number of episodes to train on for each train step
# NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation

TARGET_UPDATE_RATE = 0.005		# How frequently we want to copy the local network to the target network (for double DQNs)
LEARNING_RATE = 0.0003
DISCOUNT_RATE = 0.99
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
EPISODE_BUFFER = 64
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
REPLAY_BATCH_SIZE = 16
NUM_STEPS = 50					# The number of steps to collect experience in sequence for each GAE calculation
TIME_BATCH = 10

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], ACTOR_HIDDEN)
		self.layer2 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])

	def forward(self, state, eps):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		action_probs = self.layer3(state).softmax(-1)
		action_probs = ((1 - eps) * action_probs + torch.ones_like(action_probs).to(state.device) * eps/action_probs.size(-1))
		action = torch.distributions.Categorical(action_probs).sample().long()
		return one_hot_from_indices(action, action_probs.size(-1)), action_probs

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], CRITIC_HIDDEN)
		self.layer2 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		q_values = self.layer3(state)
		return q_values

class COMANetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, tau=TARGET_UPDATE_RATE, gpu=True, load=""):
		self.n_agents = len(state_size)
		self.actor = COMAActor([state_size[0][-1] + action_size[0][-1] + self.n_agents], action_size[0])
		self.critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in state_size]) + 2*np.sum([np.prod(a) for a in action_size]) + state_size[0][-1] + self.n_agents], action_size[0])
		super().__init__(state_size, action_size, actor=lambda s,a: self.actor, critic=self.critic, gpu=gpu, load=load, name="coma")

	def get_action(self, inputs, eps, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_probs = self.actor_local(inputs, eps)
			return [x.cpu().numpy() if numpy else x for x in [action, action_probs]]

	def optimize(self, actions, critic_inputs, actor_inputs, rewards, dones, eps):
		q_next_value = self.critic_target(critic_inputs)
		q_target_taken = torch.gather(q_next_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		q_target_taken = torch.cat([q_target_taken, torch.zeros_like(q_target_taken[:,-1]).unsqueeze(1)], dim=1)
		q_target = self.build_td_lambda_targets(rewards, dones, q_target_taken, self.n_agents)
		q_value = torch.zeros_like(q_next_value)
		for t in reversed(range(0,rewards.size(1),TIME_BATCH)):
			q_value[:,t:t+TIME_BATCH] = self.critic_local(critic_inputs[:,t:t+TIME_BATCH])
			q_taken = torch.gather(q_value[:,t:t+TIME_BATCH], dim=-1, index=actions[:,t:t+TIME_BATCH].argmax(-1, keepdims=True)).squeeze(-1)
			critic_error = (q_taken - q_target[:,t:t+TIME_BATCH].detach())
			critic_loss = critic_error.pow(2).mean()
			self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters(), retain=t>0)
		# q_value = self.critic_local(critic_inputs)
		# q_taken = torch.gather(q_value, dim=-1, index=actions.argmax(-1, keepdims=True)).squeeze(-1)
		# critic_error = (q_taken - q_target.detach())
		# critic_loss = critic_error.pow(2).mean()
		# self.step(self.critic_optimizer, critic_loss, self.critic_local.parameters())
		self.soft_copy(self.critic_local, self.critic_target)

		mac_out = torch.stack([self.get_action(actor_inputs[:,t], eps, grad=True)[1] for t in range(actor_inputs.shape[1])], dim=1)
		q_value = q_value.reshape(-1, mac_out.shape[-1])
		pi = mac_out.view(-1, mac_out.shape[-1])
		baseline = (pi * q_value).sum(-1).detach()
		q_taken = torch.gather(q_value, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.argmax(-1).reshape(-1, 1)).squeeze(1)
		advantages = (q_taken - baseline).detach()
		actor_loss = - (advantages * pi_taken.log()).mean()
		self.step(self.actor_optimizer, actor_loss, self.actor_local.parameters())

	@staticmethod
	def build_td_lambda_targets(rewards, done, target_qs, n_agents, gamma=DISCOUNT_RATE, lamda=ADVANTAGE_DECAY):
		ret = target_qs.new_zeros(*target_qs.shape)
		ret[:,-1] = target_qs[:,-1]*(1-torch.sum(done, dim=1))
		for t in range(ret.shape[1]-2, -1, -1):
			ret[:,t] = lamda*gamma*ret[:,t+1] + (rewards[:,t]+(1-lamda)*gamma*target_qs[:,t+1]*(1-done[:,t]))
		return ret[:,0:-1]

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model(self.name, dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model(self.name, dirname, name)

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer3(EPISODE_BUFFER, state_size, action_size)
		self.n_agents = len(action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		obs = np.concatenate(state, -2)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		agent_ids = np.repeat(np.expand_dims(np.eye(self.n_agents), 0), repeats=obs.shape[0], axis=0)
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1)).float().to(self.network.device)
		self.action = self.network.get_action(inputs, eps=self.eps, numpy=True)[0]
		return np.split(self.action, len(self.action_size), axis=-2)

	def train(self, state, action, next_state, reward, done):
		self.step = 0 if not hasattr(self, "step") else self.step + 1
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]):
			states, actions, rewards, dones = map(lambda x: [np.stack(t, axis=1) for t in list(zip(*x))], zip(*self.buffer))
			self.buffer.clear()
			self.replay_buffer.add((states, actions, rewards, dones))
		if (self.step % self.update_freq)==0 and len(self.replay_buffer) >= REPLAY_BATCH_SIZE:
			sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE, lambda x: torch.Tensor(x).to(self.network.device))
			states, actions, rewards, dones = map(lambda x: torch.stack(x,2), sample)
			state = states.repeat(1,1,1,self.n_agents,1).view(*states.shape[:3],-1)
			obs = states.squeeze(-2)
			actions = actions.squeeze(-2)
			actions_joint = actions.view(*actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_mask = (1 - torch.eye(self.n_agents, device=self.network.device))
			agent_mask = agent_mask.view(-1, 1).repeat(1, self.action_size[0][-1]).view(self.n_agents, -1).unsqueeze(0).unsqueeze(0)
			action_masked = actions_joint * agent_mask
			last_actions = torch.cat([torch.zeros_like(actions[:, 0:1]), actions[:, :-1]], dim=1)
			last_actions_joint = last_actions.view(*last_actions.shape[:2],1,-1).repeat(1,1,self.n_agents,1)
			agent_inds = torch.eye(self.n_agents, device=self.network.device).unsqueeze(0).unsqueeze(0).expand(*obs.shape[:2],-1,-1)
			critic_inputs = torch.cat([state, obs, action_masked, last_actions_joint, agent_inds], dim=-1)
			actor_inputs = torch.cat([obs, last_actions, agent_inds], dim=-1)
			self.network.optimize(actions, critic_inputs, actor_inputs, rewards.mean(-1, keepdims=True), dones.mean(-1, keepdims=True), self.eps)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-457.309 -457.309 -457.309] [53.657], Avg: [-457.309 -457.309 -457.309] (1.0000) ({r_i: None, r_t: [-8.449 -8.449 -8.449], eps: 1.0})
Step:  122300, Reward: [-1424.812 -1424.812 -1424.812] [369.249], Avg: [-651.397 -651.397 -651.397] (0.1000) ({r_i: None, r_t: [-2682.516 -2682.516 -2682.516], eps: 0.1})
Step:   89300, Reward: [-422.180 -422.180 -422.180] [93.072], Avg: [-866.773 -866.773 -866.773] (0.0100) ({r_i: None, r_t: [-878.978 -878.978 -878.978], eps: 0.01})
Step:     100, Reward: [-488.815 -488.815 -488.815] [87.591], Avg: [-473.062 -473.062 -473.062] (0.9900) ({r_i: None, r_t: [-946.026 -946.026 -946.026], eps: 0.99})
Step:  122400, Reward: [-1332.148 -1332.148 -1332.148] [366.897], Avg: [-651.952 -651.952 -651.952] (0.1000) ({r_i: None, r_t: [-2639.790 -2639.790 -2639.790], eps: 0.1})
Step:   89400, Reward: [-428.790 -428.790 -428.790] [114.363], Avg: [-866.283 -866.283 -866.283] (0.0100) ({r_i: None, r_t: [-843.792 -843.792 -843.792], eps: 0.01})
Step:     200, Reward: [-549.520 -549.520 -549.520] [109.888], Avg: [-498.548 -498.548 -498.548] (0.9801) ({r_i: None, r_t: [-954.902 -954.902 -954.902], eps: 0.98})
Step:  122500, Reward: [-1422.701 -1422.701 -1422.701] [324.739], Avg: [-652.581 -652.581 -652.581] (0.1000) ({r_i: None, r_t: [-2773.040 -2773.040 -2773.040], eps: 0.1})
Step:   89500, Reward: [-409.496 -409.496 -409.496] [93.777], Avg: [-865.773 -865.773 -865.773] (0.0100) ({r_i: None, r_t: [-886.442 -886.442 -886.442], eps: 0.01})
Step:     300, Reward: [-477.363 -477.363 -477.363] [95.577], Avg: [-493.252 -493.252 -493.252] (0.9704) ({r_i: None, r_t: [-944.390 -944.390 -944.390], eps: 0.97})
Step:  122600, Reward: [-1369.537 -1369.537 -1369.537] [275.468], Avg: [-653.165 -653.165 -653.165] (0.1000) ({r_i: None, r_t: [-2916.876 -2916.876 -2916.876], eps: 0.1})
Step:   89600, Reward: [-422.948 -422.948 -422.948] [57.579], Avg: [-865.280 -865.280 -865.280] (0.0100) ({r_i: None, r_t: [-884.591 -884.591 -884.591], eps: 0.01})
Step:     400, Reward: [-539.666 -539.666 -539.666] [149.764], Avg: [-502.534 -502.534 -502.534] (0.9607) ({r_i: None, r_t: [-970.059 -970.059 -970.059], eps: 0.961})
Step:  122700, Reward: [-1471.034 -1471.034 -1471.034] [329.351], Avg: [-653.831 -653.831 -653.831] (0.1000) ({r_i: None, r_t: [-2719.033 -2719.033 -2719.033], eps: 0.1})
Step:   89700, Reward: [-458.949 -458.949 -458.949] [148.346], Avg: [-864.827 -864.827 -864.827] (0.0100) ({r_i: None, r_t: [-854.589 -854.589 -854.589], eps: 0.01})
Step:     500, Reward: [-491.294 -491.294 -491.294] [70.559], Avg: [-500.661 -500.661 -500.661] (0.9511) ({r_i: None, r_t: [-927.577 -927.577 -927.577], eps: 0.951})
Step:  122800, Reward: [-1271.632 -1271.632 -1271.632] [304.635], Avg: [-654.334 -654.334 -654.334] (0.1000) ({r_i: None, r_t: [-2635.927 -2635.927 -2635.927], eps: 0.1})
Step:   89800, Reward: [-395.919 -395.919 -395.919] [49.061], Avg: [-864.306 -864.306 -864.306] (0.0100) ({r_i: None, r_t: [-897.058 -897.058 -897.058], eps: 0.01})
Step:     600, Reward: [-464.157 -464.157 -464.157] [81.312], Avg: [-495.446 -495.446 -495.446] (0.9416) ({r_i: None, r_t: [-953.052 -953.052 -953.052], eps: 0.942})
Step:  122900, Reward: [-1377.034 -1377.034 -1377.034] [406.724], Avg: [-654.922 -654.922 -654.922] (0.1000) ({r_i: None, r_t: [-2807.464 -2807.464 -2807.464], eps: 0.1})
Step:   89900, Reward: [-474.321 -474.321 -474.321] [75.652], Avg: [-863.872 -863.872 -863.872] (0.0100) ({r_i: None, r_t: [-889.917 -889.917 -889.917], eps: 0.01})
Step:     700, Reward: [-501.636 -501.636 -501.636] [79.681], Avg: [-496.220 -496.220 -496.220] (0.9322) ({r_i: None, r_t: [-950.172 -950.172 -950.172], eps: 0.932})
Step:  123000, Reward: [-1260.237 -1260.237 -1260.237] [245.390], Avg: [-655.413 -655.413 -655.413] (0.1000) ({r_i: None, r_t: [-2812.649 -2812.649 -2812.649], eps: 0.1})
Step:   90000, Reward: [-434.054 -434.054 -434.054] [82.521], Avg: [-863.395 -863.395 -863.395] (0.0100) ({r_i: None, r_t: [-871.368 -871.368 -871.368], eps: 0.01})
Step:     800, Reward: [-442.467 -442.467 -442.467] [93.588], Avg: [-490.247 -490.247 -490.247] (0.9229) ({r_i: None, r_t: [-1042.202 -1042.202 -1042.202], eps: 0.923})
Step:  123100, Reward: [-1328.966 -1328.966 -1328.966] [278.725], Avg: [-655.960 -655.960 -655.960] (0.1000) ({r_i: None, r_t: [-2657.286 -2657.286 -2657.286], eps: 0.1})
Step:   90100, Reward: [-449.458 -449.458 -449.458] [103.371], Avg: [-862.936 -862.936 -862.936] (0.0100) ({r_i: None, r_t: [-876.653 -876.653 -876.653], eps: 0.01})
Step:     900, Reward: [-470.509 -470.509 -470.509] [83.155], Avg: [-488.274 -488.274 -488.274] (0.9137) ({r_i: None, r_t: [-1020.328 -1020.328 -1020.328], eps: 0.914})
Step:  123200, Reward: [-1358.709 -1358.709 -1358.709] [334.195], Avg: [-656.530 -656.530 -656.530] (0.1000) ({r_i: None, r_t: [-2802.678 -2802.678 -2802.678], eps: 0.1})
Step:   90200, Reward: [-441.787 -441.787 -441.787] [77.758], Avg: [-862.470 -862.470 -862.470] (0.0100) ({r_i: None, r_t: [-867.908 -867.908 -867.908], eps: 0.01})
Step:    1000, Reward: [-504.114 -504.114 -504.114] [188.497], Avg: [-489.714 -489.714 -489.714] (0.9046) ({r_i: None, r_t: [-955.113 -955.113 -955.113], eps: 0.905})
Step:  123300, Reward: [-1510.292 -1510.292 -1510.292] [393.006], Avg: [-657.222 -657.222 -657.222] (0.1000) ({r_i: None, r_t: [-2350.828 -2350.828 -2350.828], eps: 0.1})
Step:   90300, Reward: [-416.224 -416.224 -416.224] [68.210], Avg: [-861.976 -861.976 -861.976] (0.0100) ({r_i: None, r_t: [-857.538 -857.538 -857.538], eps: 0.01})
Step:    1100, Reward: [-515.007 -515.007 -515.007] [111.677], Avg: [-491.821 -491.821 -491.821] (0.8956) ({r_i: None, r_t: [-996.174 -996.174 -996.174], eps: 0.896})
Step:  123400, Reward: [-1466.535 -1466.535 -1466.535] [444.803], Avg: [-657.877 -657.877 -657.877] (0.1000) ({r_i: None, r_t: [-3041.178 -3041.178 -3041.178], eps: 0.1})
Step:   90400, Reward: [-426.034 -426.034 -426.034] [93.219], Avg: [-861.495 -861.495 -861.495] (0.0100) ({r_i: None, r_t: [-894.317 -894.317 -894.317], eps: 0.01})
Step:    1200, Reward: [-487.868 -487.868 -487.868] [67.621], Avg: [-491.517 -491.517 -491.517] (0.8867) ({r_i: None, r_t: [-1033.670 -1033.670 -1033.670], eps: 0.887})
Step:  123500, Reward: [-1534.758 -1534.758 -1534.758] [431.562], Avg: [-658.587 -658.587 -658.587] (0.1000) ({r_i: None, r_t: [-2979.849 -2979.849 -2979.849], eps: 0.1})
Step:   90500, Reward: [-435.600 -435.600 -435.600] [91.508], Avg: [-861.025 -861.025 -861.025] (0.0100) ({r_i: None, r_t: [-913.900 -913.900 -913.900], eps: 0.01})
Step:    1300, Reward: [-501.306 -501.306 -501.306] [93.420], Avg: [-492.216 -492.216 -492.216] (0.8778) ({r_i: None, r_t: [-1003.932 -1003.932 -1003.932], eps: 0.878})
Step:  123600, Reward: [-1403.501 -1403.501 -1403.501] [314.525], Avg: [-659.189 -659.189 -659.189] (0.1000) ({r_i: None, r_t: [-2692.223 -2692.223 -2692.223], eps: 0.1})
Step:   90600, Reward: [-462.284 -462.284 -462.284] [91.391], Avg: [-860.585 -860.585 -860.585] (0.0100) ({r_i: None, r_t: [-797.332 -797.332 -797.332], eps: 0.01})
Step:  123700, Reward: [-1288.556 -1288.556 -1288.556] [402.666], Avg: [-659.697 -659.697 -659.697] (0.1000) ({r_i: None, r_t: [-2679.961 -2679.961 -2679.961], eps: 0.1})
Step:    1400, Reward: [-489.119 -489.119 -489.119] [79.879], Avg: [-492.010 -492.010 -492.010] (0.8691) ({r_i: None, r_t: [-993.608 -993.608 -993.608], eps: 0.869})
Step:   90700, Reward: [-442.896 -442.896 -442.896] [92.116], Avg: [-860.125 -860.125 -860.125] (0.0100) ({r_i: None, r_t: [-856.049 -856.049 -856.049], eps: 0.01})
Step:  123800, Reward: [-1457.082 -1457.082 -1457.082] [334.751], Avg: [-660.341 -660.341 -660.341] (0.1000) ({r_i: None, r_t: [-2756.776 -2756.776 -2756.776], eps: 0.1})
Step:    1500, Reward: [-530.063 -530.063 -530.063] [114.869], Avg: [-494.388 -494.388 -494.388] (0.8604) ({r_i: None, r_t: [-1041.812 -1041.812 -1041.812], eps: 0.86})
Step:   90800, Reward: [-442.207 -442.207 -442.207] [62.709], Avg: [-859.665 -859.665 -859.665] (0.0100) ({r_i: None, r_t: [-878.497 -878.497 -878.497], eps: 0.01})
Step:  123900, Reward: [-1513.761 -1513.761 -1513.761] [423.294], Avg: [-661.029 -661.029 -661.029] (0.1000) ({r_i: None, r_t: [-2568.270 -2568.270 -2568.270], eps: 0.1})
Step:    1600, Reward: [-513.204 -513.204 -513.204] [103.500], Avg: [-495.495 -495.495 -495.495] (0.8518) ({r_i: None, r_t: [-989.077 -989.077 -989.077], eps: 0.852})
Step:   90900, Reward: [-458.826 -458.826 -458.826] [95.331], Avg: [-859.225 -859.225 -859.225] (0.0100) ({r_i: None, r_t: [-875.448 -875.448 -875.448], eps: 0.01})
Step:  124000, Reward: [-1397.174 -1397.174 -1397.174] [355.080], Avg: [-661.622 -661.622 -661.622] (0.1000) ({r_i: None, r_t: [-2825.684 -2825.684 -2825.684], eps: 0.1})
Step:    1700, Reward: [-480.490 -480.490 -480.490] [75.233], Avg: [-494.661 -494.661 -494.661] (0.8433) ({r_i: None, r_t: [-973.686 -973.686 -973.686], eps: 0.843})
Step:   91000, Reward: [-419.570 -419.570 -419.570] [64.167], Avg: [-858.742 -858.742 -858.742] (0.0100) ({r_i: None, r_t: [-930.259 -930.259 -930.259], eps: 0.01})
Step:  124100, Reward: [-1209.226 -1209.226 -1209.226] [297.569], Avg: [-662.063 -662.063 -662.063] (0.1000) ({r_i: None, r_t: [-2895.786 -2895.786 -2895.786], eps: 0.1})
Step:    1800, Reward: [-498.160 -498.160 -498.160] [93.319], Avg: [-494.846 -494.846 -494.846] (0.8349) ({r_i: None, r_t: [-918.085 -918.085 -918.085], eps: 0.835})
Step:  124200, Reward: [-1412.638 -1412.638 -1412.638] [389.768], Avg: [-662.667 -662.667 -662.667] (0.1000) ({r_i: None, r_t: [-2882.495 -2882.495 -2882.495], eps: 0.1})
Step:   91100, Reward: [-436.816 -436.816 -436.816] [85.868], Avg: [-858.279 -858.279 -858.279] (0.0100) ({r_i: None, r_t: [-895.560 -895.560 -895.560], eps: 0.01})
Step:    1900, Reward: [-501.729 -501.729 -501.729] [72.240], Avg: [-495.190 -495.190 -495.190] (0.8266) ({r_i: None, r_t: [-963.041 -963.041 -963.041], eps: 0.827})
Step:  124300, Reward: [-1398.058 -1398.058 -1398.058] [332.268], Avg: [-663.258 -663.258 -663.258] (0.1000) ({r_i: None, r_t: [-2753.411 -2753.411 -2753.411], eps: 0.1})
Step:   91200, Reward: [-434.641 -434.641 -434.641] [93.309], Avg: [-857.815 -857.815 -857.815] (0.0100) ({r_i: None, r_t: [-899.585 -899.585 -899.585], eps: 0.01})
Step:    2000, Reward: [-522.331 -522.331 -522.331] [103.184], Avg: [-496.482 -496.482 -496.482] (0.8183) ({r_i: None, r_t: [-1020.104 -1020.104 -1020.104], eps: 0.818})
Step:  124400, Reward: [-1380.453 -1380.453 -1380.453] [342.390], Avg: [-663.834 -663.834 -663.834] (0.1000) ({r_i: None, r_t: [-2937.496 -2937.496 -2937.496], eps: 0.1})
Step:   91300, Reward: [-437.328 -437.328 -437.328] [113.103], Avg: [-857.355 -857.355 -857.355] (0.0100) ({r_i: None, r_t: [-909.371 -909.371 -909.371], eps: 0.01})
Step:    2100, Reward: [-531.643 -531.643 -531.643] [115.355], Avg: [-498.080 -498.080 -498.080] (0.8102) ({r_i: None, r_t: [-999.159 -999.159 -999.159], eps: 0.81})
Step:  124500, Reward: [-1476.179 -1476.179 -1476.179] [376.425], Avg: [-664.486 -664.486 -664.486] (0.1000) ({r_i: None, r_t: [-3036.369 -3036.369 -3036.369], eps: 0.1})
Step:   91400, Reward: [-439.153 -439.153 -439.153] [81.628], Avg: [-856.898 -856.898 -856.898] (0.0100) ({r_i: None, r_t: [-871.263 -871.263 -871.263], eps: 0.01})
Step:    2200, Reward: [-476.310 -476.310 -476.310] [88.626], Avg: [-497.134 -497.134 -497.134] (0.8021) ({r_i: None, r_t: [-996.734 -996.734 -996.734], eps: 0.802})
Step:  124600, Reward: [-1571.635 -1571.635 -1571.635] [324.620], Avg: [-665.214 -665.214 -665.214] (0.1000) ({r_i: None, r_t: [-2932.667 -2932.667 -2932.667], eps: 0.1})
Step:   91500, Reward: [-445.547 -445.547 -445.547] [99.892], Avg: [-856.449 -856.449 -856.449] (0.0100) ({r_i: None, r_t: [-818.059 -818.059 -818.059], eps: 0.01})
Step:    2300, Reward: [-480.804 -480.804 -480.804] [61.210], Avg: [-496.454 -496.454 -496.454] (0.7941) ({r_i: None, r_t: [-1014.477 -1014.477 -1014.477], eps: 0.794})
Step:  124700, Reward: [-1519.606 -1519.606 -1519.606] [448.587], Avg: [-665.898 -665.898 -665.898] (0.1000) ({r_i: None, r_t: [-2750.118 -2750.118 -2750.118], eps: 0.1})
Step:   91600, Reward: [-452.394 -452.394 -452.394] [109.691], Avg: [-856.009 -856.009 -856.009] (0.0100) ({r_i: None, r_t: [-883.927 -883.927 -883.927], eps: 0.01})
Step:    2400, Reward: [-516.578 -516.578 -516.578] [106.934], Avg: [-497.259 -497.259 -497.259] (0.7862) ({r_i: None, r_t: [-1021.235 -1021.235 -1021.235], eps: 0.786})
Step:  124800, Reward: [-1482.018 -1482.018 -1482.018] [441.104], Avg: [-666.552 -666.552 -666.552] (0.1000) ({r_i: None, r_t: [-2644.677 -2644.677 -2644.677], eps: 0.1})
Step:   91700, Reward: [-487.928 -487.928 -487.928] [112.437], Avg: [-855.608 -855.608 -855.608] (0.0100) ({r_i: None, r_t: [-876.702 -876.702 -876.702], eps: 0.01})
Step:    2500, Reward: [-452.671 -452.671 -452.671] [72.687], Avg: [-495.544 -495.544 -495.544] (0.7783) ({r_i: None, r_t: [-1019.361 -1019.361 -1019.361], eps: 0.778})
Step:  124900, Reward: [-1490.742 -1490.742 -1490.742] [379.116], Avg: [-667.211 -667.211 -667.211] (0.1000) ({r_i: None, r_t: [-2741.754 -2741.754 -2741.754], eps: 0.1})
Step:   91800, Reward: [-408.581 -408.581 -408.581] [97.663], Avg: [-855.121 -855.121 -855.121] (0.0100) ({r_i: None, r_t: [-850.468 -850.468 -850.468], eps: 0.01})
Step:    2600, Reward: [-486.457 -486.457 -486.457] [70.346], Avg: [-495.207 -495.207 -495.207] (0.7705) ({r_i: None, r_t: [-973.093 -973.093 -973.093], eps: 0.771})
Step:  125000, Reward: [-1453.438 -1453.438 -1453.438] [383.865], Avg: [-667.840 -667.840 -667.840] (0.1000) ({r_i: None, r_t: [-2853.682 -2853.682 -2853.682], eps: 0.1})
Step:   91900, Reward: [-465.181 -465.181 -465.181] [89.896], Avg: [-854.697 -854.697 -854.697] (0.0100) ({r_i: None, r_t: [-892.370 -892.370 -892.370], eps: 0.01})
Step:    2700, Reward: [-521.583 -521.583 -521.583] [105.275], Avg: [-496.149 -496.149 -496.149] (0.7629) ({r_i: None, r_t: [-1043.328 -1043.328 -1043.328], eps: 0.763})
Step:  125100, Reward: [-1551.644 -1551.644 -1551.644] [306.315], Avg: [-668.545 -668.545 -668.545] (0.1000) ({r_i: None, r_t: [-2651.144 -2651.144 -2651.144], eps: 0.1})
Step:   92000, Reward: [-421.468 -421.468 -421.468] [70.350], Avg: [-854.227 -854.227 -854.227] (0.0100) ({r_i: None, r_t: [-871.421 -871.421 -871.421], eps: 0.01})
Step:    2800, Reward: [-521.170 -521.170 -521.170] [96.640], Avg: [-497.012 -497.012 -497.012] (0.7553) ({r_i: None, r_t: [-1058.488 -1058.488 -1058.488], eps: 0.755})
Step:  125200, Reward: [-1363.314 -1363.314 -1363.314] [405.100], Avg: [-669.100 -669.100 -669.100] (0.1000) ({r_i: None, r_t: [-2811.906 -2811.906 -2811.906], eps: 0.1})
Step:   92100, Reward: [-409.817 -409.817 -409.817] [103.644], Avg: [-853.745 -853.745 -853.745] (0.0100) ({r_i: None, r_t: [-849.936 -849.936 -849.936], eps: 0.01})
Step:    2900, Reward: [-461.062 -461.062 -461.062] [114.260], Avg: [-495.814 -495.814 -495.814] (0.7477) ({r_i: None, r_t: [-982.250 -982.250 -982.250], eps: 0.748})
Step:  125300, Reward: [-1190.410 -1190.410 -1190.410] [336.699], Avg: [-669.516 -669.516 -669.516] (0.1000) ({r_i: None, r_t: [-2834.088 -2834.088 -2834.088], eps: 0.1})
Step:   92200, Reward: [-391.515 -391.515 -391.515] [60.710], Avg: [-853.244 -853.244 -853.244] (0.0100) ({r_i: None, r_t: [-857.314 -857.314 -857.314], eps: 0.01})
Step:  125400, Reward: [-1434.678 -1434.678 -1434.678] [218.544], Avg: [-670.125 -670.125 -670.125] (0.1000) ({r_i: None, r_t: [-2694.046 -2694.046 -2694.046], eps: 0.1})
Step:    3000, Reward: [-567.736 -567.736 -567.736] [168.998], Avg: [-498.134 -498.134 -498.134] (0.7403) ({r_i: None, r_t: [-990.702 -990.702 -990.702], eps: 0.74})
Step:   92300, Reward: [-458.074 -458.074 -458.074] [106.249], Avg: [-852.817 -852.817 -852.817] (0.0100) ({r_i: None, r_t: [-845.866 -845.866 -845.866], eps: 0.01})
Step:  125500, Reward: [-1310.182 -1310.182 -1310.182] [369.865], Avg: [-670.635 -670.635 -670.635] (0.1000) ({r_i: None, r_t: [-2776.570 -2776.570 -2776.570], eps: 0.1})
Step:    3100, Reward: [-518.343 -518.343 -518.343] [139.754], Avg: [-498.765 -498.765 -498.765] (0.7329) ({r_i: None, r_t: [-1014.086 -1014.086 -1014.086], eps: 0.733})
Step:   92400, Reward: [-427.607 -427.607 -427.607] [110.387], Avg: [-852.357 -852.357 -852.357] (0.0100) ({r_i: None, r_t: [-839.793 -839.793 -839.793], eps: 0.01})
Step:  125600, Reward: [-1359.496 -1359.496 -1359.496] [358.343], Avg: [-671.183 -671.183 -671.183] (0.1000) ({r_i: None, r_t: [-2598.711 -2598.711 -2598.711], eps: 0.1})
Step:    3200, Reward: [-555.241 -555.241 -555.241] [111.906], Avg: [-500.477 -500.477 -500.477] (0.7256) ({r_i: None, r_t: [-995.190 -995.190 -995.190], eps: 0.726})
Step:   92500, Reward: [-424.144 -424.144 -424.144] [87.554], Avg: [-851.894 -851.894 -851.894] (0.0100) ({r_i: None, r_t: [-877.022 -877.022 -877.022], eps: 0.01})
Step:  125700, Reward: [-1477.110 -1477.110 -1477.110] [427.347], Avg: [-671.824 -671.824 -671.824] (0.1000) ({r_i: None, r_t: [-2571.349 -2571.349 -2571.349], eps: 0.1})
Step:    3300, Reward: [-543.494 -543.494 -543.494] [134.483], Avg: [-501.742 -501.742 -501.742] (0.7183) ({r_i: None, r_t: [-1015.473 -1015.473 -1015.473], eps: 0.718})
Step:   92600, Reward: [-392.500 -392.500 -392.500] [70.653], Avg: [-851.399 -851.399 -851.399] (0.0100) ({r_i: None, r_t: [-864.346 -864.346 -864.346], eps: 0.01})
Step:  125800, Reward: [-1390.377 -1390.377 -1390.377] [367.727], Avg: [-672.394 -672.394 -672.394] (0.1000) ({r_i: None, r_t: [-2734.476 -2734.476 -2734.476], eps: 0.1})
Step:    3400, Reward: [-545.334 -545.334 -545.334] [140.060], Avg: [-502.987 -502.987 -502.987] (0.7112) ({r_i: None, r_t: [-1038.847 -1038.847 -1038.847], eps: 0.711})
Step:   92700, Reward: [-415.979 -415.979 -415.979] [77.103], Avg: [-850.930 -850.930 -850.930] (0.0100) ({r_i: None, r_t: [-886.725 -886.725 -886.725], eps: 0.01})
Step:  125900, Reward: [-1384.603 -1384.603 -1384.603] [306.112], Avg: [-672.960 -672.960 -672.960] (0.1000) ({r_i: None, r_t: [-2575.864 -2575.864 -2575.864], eps: 0.1})
Step:    3500, Reward: [-504.592 -504.592 -504.592] [115.993], Avg: [-503.032 -503.032 -503.032] (0.7041) ({r_i: None, r_t: [-1037.082 -1037.082 -1037.082], eps: 0.704})
Step:   92800, Reward: [-438.284 -438.284 -438.284] [59.157], Avg: [-850.485 -850.485 -850.485] (0.0100) ({r_i: None, r_t: [-962.575 -962.575 -962.575], eps: 0.01})
Step:  126000, Reward: [-1477.847 -1477.847 -1477.847] [301.465], Avg: [-673.598 -673.598 -673.598] (0.1000) ({r_i: None, r_t: [-3087.261 -3087.261 -3087.261], eps: 0.1})
Step:    3600, Reward: [-510.927 -510.927 -510.927] [93.959], Avg: [-503.245 -503.245 -503.245] (0.6970) ({r_i: None, r_t: [-966.710 -966.710 -966.710], eps: 0.697})
Step:   92900, Reward: [-454.094 -454.094 -454.094] [102.435], Avg: [-850.059 -850.059 -850.059] (0.0100) ({r_i: None, r_t: [-922.196 -922.196 -922.196], eps: 0.01})
Step:  126100, Reward: [-1387.718 -1387.718 -1387.718] [386.137], Avg: [-674.164 -674.164 -674.164] (0.1000) ({r_i: None, r_t: [-2713.413 -2713.413 -2713.413], eps: 0.1})
Step:    3700, Reward: [-532.574 -532.574 -532.574] [132.490], Avg: [-504.017 -504.017 -504.017] (0.6901) ({r_i: None, r_t: [-1052.911 -1052.911 -1052.911], eps: 0.69})
Step:   93000, Reward: [-394.050 -394.050 -394.050] [90.710], Avg: [-849.569 -849.569 -849.569] (0.0100) ({r_i: None, r_t: [-863.852 -863.852 -863.852], eps: 0.01})
Step:  126200, Reward: [-1403.714 -1403.714 -1403.714] [415.935], Avg: [-674.741 -674.741 -674.741] (0.1000) ({r_i: None, r_t: [-2928.650 -2928.650 -2928.650], eps: 0.1})
Step:    3800, Reward: [-502.897 -502.897 -502.897] [120.477], Avg: [-503.988 -503.988 -503.988] (0.6832) ({r_i: None, r_t: [-1091.225 -1091.225 -1091.225], eps: 0.683})
Step:   93100, Reward: [-434.847 -434.847 -434.847] [76.074], Avg: [-849.124 -849.124 -849.124] (0.0100) ({r_i: None, r_t: [-893.810 -893.810 -893.810], eps: 0.01})
Step:  126300, Reward: [-1421.986 -1421.986 -1421.986] [374.082], Avg: [-675.333 -675.333 -675.333] (0.1000) ({r_i: None, r_t: [-2697.582 -2697.582 -2697.582], eps: 0.1})
Step:    3900, Reward: [-589.052 -589.052 -589.052] [124.013], Avg: [-506.115 -506.115 -506.115] (0.6764) ({r_i: None, r_t: [-1091.082 -1091.082 -1091.082], eps: 0.676})
Step:   93200, Reward: [-459.812 -459.812 -459.812] [61.572], Avg: [-848.707 -848.707 -848.707] (0.0100) ({r_i: None, r_t: [-833.643 -833.643 -833.643], eps: 0.01})
Step:  126400, Reward: [-1434.588 -1434.588 -1434.588] [381.970], Avg: [-675.933 -675.933 -675.933] (0.1000) ({r_i: None, r_t: [-2731.254 -2731.254 -2731.254], eps: 0.1})
Step:    4000, Reward: [-578.719 -578.719 -578.719] [198.292], Avg: [-507.886 -507.886 -507.886] (0.6696) ({r_i: None, r_t: [-1159.067 -1159.067 -1159.067], eps: 0.67})
Step:   93300, Reward: [-467.523 -467.523 -467.523] [94.052], Avg: [-848.299 -848.299 -848.299] (0.0100) ({r_i: None, r_t: [-889.987 -889.987 -889.987], eps: 0.01})
Step:  126500, Reward: [-1252.512 -1252.512 -1252.512] [357.861], Avg: [-676.388 -676.388 -676.388] (0.1000) ({r_i: None, r_t: [-2853.399 -2853.399 -2853.399], eps: 0.1})
Step:    4100, Reward: [-560.795 -560.795 -560.795] [170.209], Avg: [-509.145 -509.145 -509.145] (0.6630) ({r_i: None, r_t: [-1087.442 -1087.442 -1087.442], eps: 0.663})
Step:   93400, Reward: [-402.482 -402.482 -402.482] [93.886], Avg: [-847.822 -847.822 -847.822] (0.0100) ({r_i: None, r_t: [-873.765 -873.765 -873.765], eps: 0.01})
Step:  126600, Reward: [-1298.398 -1298.398 -1298.398] [322.655], Avg: [-676.879 -676.879 -676.879] (0.1000) ({r_i: None, r_t: [-2777.036 -2777.036 -2777.036], eps: 0.1})
Step:    4200, Reward: [-558.231 -558.231 -558.231] [130.344], Avg: [-510.287 -510.287 -510.287] (0.6564) ({r_i: None, r_t: [-1073.300 -1073.300 -1073.300], eps: 0.656})
Step:  126700, Reward: [-1321.482 -1321.482 -1321.482] [340.802], Avg: [-677.387 -677.387 -677.387] (0.1000) ({r_i: None, r_t: [-2956.804 -2956.804 -2956.804], eps: 0.1})
Step:   93500, Reward: [-449.318 -449.318 -449.318] [101.673], Avg: [-847.397 -847.397 -847.397] (0.0100) ({r_i: None, r_t: [-870.785 -870.785 -870.785], eps: 0.01})
Step:    4300, Reward: [-530.164 -530.164 -530.164] [107.012], Avg: [-510.739 -510.739 -510.739] (0.6498) ({r_i: None, r_t: [-1128.380 -1128.380 -1128.380], eps: 0.65})
Step:  126800, Reward: [-1222.982 -1222.982 -1222.982] [399.322], Avg: [-677.817 -677.817 -677.817] (0.1000) ({r_i: None, r_t: [-2812.215 -2812.215 -2812.215], eps: 0.1})
Step:   93600, Reward: [-454.081 -454.081 -454.081] [93.663], Avg: [-846.977 -846.977 -846.977] (0.0100) ({r_i: None, r_t: [-879.687 -879.687 -879.687], eps: 0.01})
Step:    4400, Reward: [-588.500 -588.500 -588.500] [135.841], Avg: [-512.467 -512.467 -512.467] (0.6433) ({r_i: None, r_t: [-1127.386 -1127.386 -1127.386], eps: 0.643})
Step:  126900, Reward: [-1341.958 -1341.958 -1341.958] [294.892], Avg: [-678.340 -678.340 -678.340] (0.1000) ({r_i: None, r_t: [-2618.255 -2618.255 -2618.255], eps: 0.1})
Step:   93700, Reward: [-426.573 -426.573 -426.573] [82.994], Avg: [-846.529 -846.529 -846.529] (0.0100) ({r_i: None, r_t: [-907.963 -907.963 -907.963], eps: 0.01})
Step:    4500, Reward: [-616.571 -616.571 -616.571] [177.057], Avg: [-514.730 -514.730 -514.730] (0.6369) ({r_i: None, r_t: [-1106.393 -1106.393 -1106.393], eps: 0.637})
Step:  127000, Reward: [-1268.197 -1268.197 -1268.197] [350.263], Avg: [-678.804 -678.804 -678.804] (0.1000) ({r_i: None, r_t: [-2696.408 -2696.408 -2696.408], eps: 0.1})
Step:   93800, Reward: [-429.615 -429.615 -429.615] [46.319], Avg: [-846.085 -846.085 -846.085] (0.0100) ({r_i: None, r_t: [-840.740 -840.740 -840.740], eps: 0.01})
Step:    4600, Reward: [-669.232 -669.232 -669.232] [165.994], Avg: [-518.017 -518.017 -518.017] (0.6306) ({r_i: None, r_t: [-1262.819 -1262.819 -1262.819], eps: 0.631})
Step:  127100, Reward: [-1424.454 -1424.454 -1424.454] [413.618], Avg: [-679.391 -679.391 -679.391] (0.1000) ({r_i: None, r_t: [-2477.527 -2477.527 -2477.527], eps: 0.1})
Step:   93900, Reward: [-466.464 -466.464 -466.464] [107.806], Avg: [-845.681 -845.681 -845.681] (0.0100) ({r_i: None, r_t: [-876.292 -876.292 -876.292], eps: 0.01})
Step:    4700, Reward: [-612.518 -612.518 -612.518] [121.697], Avg: [-519.986 -519.986 -519.986] (0.6243) ({r_i: None, r_t: [-1206.106 -1206.106 -1206.106], eps: 0.624})
Step:  127200, Reward: [-1510.306 -1510.306 -1510.306] [473.679], Avg: [-680.043 -680.043 -680.043] (0.1000) ({r_i: None, r_t: [-2557.486 -2557.486 -2557.486], eps: 0.1})
Step:   94000, Reward: [-448.207 -448.207 -448.207] [66.077], Avg: [-845.258 -845.258 -845.258] (0.0100) ({r_i: None, r_t: [-939.715 -939.715 -939.715], eps: 0.01})
Step:    4800, Reward: [-572.212 -572.212 -572.212] [215.227], Avg: [-521.052 -521.052 -521.052] (0.6180) ({r_i: None, r_t: [-1196.090 -1196.090 -1196.090], eps: 0.618})
Step:  127300, Reward: [-1351.526 -1351.526 -1351.526] [447.183], Avg: [-680.570 -680.570 -680.570] (0.1000) ({r_i: None, r_t: [-2785.982 -2785.982 -2785.982], eps: 0.1})
Step:   94100, Reward: [-473.786 -473.786 -473.786] [99.374], Avg: [-844.864 -844.864 -844.864] (0.0100) ({r_i: None, r_t: [-862.477 -862.477 -862.477], eps: 0.01})
Step:  127400, Reward: [-1400.927 -1400.927 -1400.927] [336.037], Avg: [-681.135 -681.135 -681.135] (0.1000) ({r_i: None, r_t: [-2600.251 -2600.251 -2600.251], eps: 0.1})
Step:    4900, Reward: [-589.272 -589.272 -589.272] [104.873], Avg: [-522.416 -522.416 -522.416] (0.6119) ({r_i: None, r_t: [-1264.605 -1264.605 -1264.605], eps: 0.612})
Step:   94200, Reward: [-433.582 -433.582 -433.582] [54.415], Avg: [-844.428 -844.428 -844.428] (0.0100) ({r_i: None, r_t: [-862.113 -862.113 -862.113], eps: 0.01})
Step:  127500, Reward: [-1041.224 -1041.224 -1041.224] [350.579], Avg: [-681.418 -681.418 -681.418] (0.1000) ({r_i: None, r_t: [-2817.275 -2817.275 -2817.275], eps: 0.1})
Step:    5000, Reward: [-700.977 -700.977 -700.977] [240.792], Avg: [-525.917 -525.917 -525.917] (0.6058) ({r_i: None, r_t: [-1202.497 -1202.497 -1202.497], eps: 0.606})
Step:   94300, Reward: [-443.794 -443.794 -443.794] [91.594], Avg: [-844.003 -844.003 -844.003] (0.0100) ({r_i: None, r_t: [-887.163 -887.163 -887.163], eps: 0.01})
Step:  127600, Reward: [-1045.792 -1045.792 -1045.792] [274.307], Avg: [-681.703 -681.703 -681.703] (0.1000) ({r_i: None, r_t: [-2706.727 -2706.727 -2706.727], eps: 0.1})
Step:    5100, Reward: [-590.405 -590.405 -590.405] [122.369], Avg: [-527.158 -527.158 -527.158] (0.5997) ({r_i: None, r_t: [-1272.770 -1272.770 -1272.770], eps: 0.6})
Step:   94400, Reward: [-429.538 -429.538 -429.538] [82.877], Avg: [-843.565 -843.565 -843.565] (0.0100) ({r_i: None, r_t: [-866.766 -866.766 -866.766], eps: 0.01})
Step:  127700, Reward: [-1427.732 -1427.732 -1427.732] [453.628], Avg: [-682.287 -682.287 -682.287] (0.1000) ({r_i: None, r_t: [-2875.918 -2875.918 -2875.918], eps: 0.1})
Step:    5200, Reward: [-632.498 -632.498 -632.498] [183.854], Avg: [-529.145 -529.145 -529.145] (0.5937) ({r_i: None, r_t: [-1263.682 -1263.682 -1263.682], eps: 0.594})
Step:   94500, Reward: [-453.163 -453.163 -453.163] [83.368], Avg: [-843.152 -843.152 -843.152] (0.0100) ({r_i: None, r_t: [-820.661 -820.661 -820.661], eps: 0.01})
Step:  127800, Reward: [-1403.239 -1403.239 -1403.239] [455.086], Avg: [-682.850 -682.850 -682.850] (0.1000) ({r_i: None, r_t: [-2453.215 -2453.215 -2453.215], eps: 0.1})
Step:    5300, Reward: [-654.590 -654.590 -654.590] [146.056], Avg: [-531.468 -531.468 -531.468] (0.5878) ({r_i: None, r_t: [-1293.695 -1293.695 -1293.695], eps: 0.588})
Step:   94600, Reward: [-433.785 -433.785 -433.785] [93.744], Avg: [-842.720 -842.720 -842.720] (0.0100) ({r_i: None, r_t: [-903.175 -903.175 -903.175], eps: 0.01})
Step:  127900, Reward: [-1353.206 -1353.206 -1353.206] [304.582], Avg: [-683.374 -683.374 -683.374] (0.1000) ({r_i: None, r_t: [-2412.544 -2412.544 -2412.544], eps: 0.1})
Step:    5400, Reward: [-624.274 -624.274 -624.274] [152.015], Avg: [-533.155 -533.155 -533.155] (0.5820) ({r_i: None, r_t: [-1328.662 -1328.662 -1328.662], eps: 0.582})
Step:   94700, Reward: [-427.972 -427.972 -427.972] [68.650], Avg: [-842.282 -842.282 -842.282] (0.0100) ({r_i: None, r_t: [-882.635 -882.635 -882.635], eps: 0.01})
Step:  128000, Reward: [-1274.297 -1274.297 -1274.297] [330.621], Avg: [-683.835 -683.835 -683.835] (0.1000) ({r_i: None, r_t: [-2696.919 -2696.919 -2696.919], eps: 0.1})
Step:    5500, Reward: [-591.631 -591.631 -591.631] [136.986], Avg: [-534.200 -534.200 -534.200] (0.5762) ({r_i: None, r_t: [-1358.536 -1358.536 -1358.536], eps: 0.576})
Step:   94800, Reward: [-464.628 -464.628 -464.628] [63.984], Avg: [-841.884 -841.884 -841.884] (0.0100) ({r_i: None, r_t: [-828.069 -828.069 -828.069], eps: 0.01})
Step:  128100, Reward: [-1300.864 -1300.864 -1300.864] [431.431], Avg: [-684.317 -684.317 -684.317] (0.1000) ({r_i: None, r_t: [-2674.620 -2674.620 -2674.620], eps: 0.1})
Step:    5600, Reward: [-644.155 -644.155 -644.155] [212.442], Avg: [-536.129 -536.129 -536.129] (0.5704) ({r_i: None, r_t: [-1223.732 -1223.732 -1223.732], eps: 0.57})
Step:   94900, Reward: [-417.894 -417.894 -417.894] [106.428], Avg: [-841.438 -841.438 -841.438] (0.0100) ({r_i: None, r_t: [-827.190 -827.190 -827.190], eps: 0.01})
Step:  128200, Reward: [-1429.815 -1429.815 -1429.815] [301.182], Avg: [-684.898 -684.898 -684.898] (0.1000) ({r_i: None, r_t: [-2726.446 -2726.446 -2726.446], eps: 0.1})
Step:    5700, Reward: [-641.101 -641.101 -641.101] [113.219], Avg: [-537.939 -537.939 -537.939] (0.5647) ({r_i: None, r_t: [-1465.222 -1465.222 -1465.222], eps: 0.565})
Step:   95000, Reward: [-394.055 -394.055 -394.055] [71.263], Avg: [-840.968 -840.968 -840.968] (0.0100) ({r_i: None, r_t: [-841.176 -841.176 -841.176], eps: 0.01})
Step:  128300, Reward: [-1484.894 -1484.894 -1484.894] [453.687], Avg: [-685.521 -685.521 -685.521] (0.1000) ({r_i: None, r_t: [-2350.857 -2350.857 -2350.857], eps: 0.1})
Step:    5800, Reward: [-654.765 -654.765 -654.765] [202.488], Avg: [-539.919 -539.919 -539.919] (0.5591) ({r_i: None, r_t: [-1244.612 -1244.612 -1244.612], eps: 0.559})
Step:   95100, Reward: [-449.245 -449.245 -449.245] [91.425], Avg: [-840.556 -840.556 -840.556] (0.0100) ({r_i: None, r_t: [-847.916 -847.916 -847.916], eps: 0.01})
Step:  128400, Reward: [-1331.909 -1331.909 -1331.909] [397.608], Avg: [-686.024 -686.024 -686.024] (0.1000) ({r_i: None, r_t: [-2560.839 -2560.839 -2560.839], eps: 0.1})
Step:    5900, Reward: [-678.444 -678.444 -678.444] [246.387], Avg: [-542.227 -542.227 -542.227] (0.5535) ({r_i: None, r_t: [-1272.791 -1272.791 -1272.791], eps: 0.554})
Step:   95200, Reward: [-413.106 -413.106 -413.106] [86.809], Avg: [-840.108 -840.108 -840.108] (0.0100) ({r_i: None, r_t: [-820.064 -820.064 -820.064], eps: 0.01})
Step:  128500, Reward: [-1215.751 -1215.751 -1215.751] [385.548], Avg: [-686.436 -686.436 -686.436] (0.1000) ({r_i: None, r_t: [-2411.838 -2411.838 -2411.838], eps: 0.1})
Step:    6000, Reward: [-632.129 -632.129 -632.129] [152.825], Avg: [-543.701 -543.701 -543.701] (0.5480) ({r_i: None, r_t: [-1282.647 -1282.647 -1282.647], eps: 0.548})
Step:   95300, Reward: [-497.076 -497.076 -497.076] [117.328], Avg: [-839.748 -839.748 -839.748] (0.0100) ({r_i: None, r_t: [-789.657 -789.657 -789.657], eps: 0.01})
Step:  128600, Reward: [-1487.738 -1487.738 -1487.738] [471.186], Avg: [-687.058 -687.058 -687.058] (0.1000) ({r_i: None, r_t: [-2610.446 -2610.446 -2610.446], eps: 0.1})
Step:    6100, Reward: [-652.596 -652.596 -652.596] [103.178], Avg: [-545.458 -545.458 -545.458] (0.5425) ({r_i: None, r_t: [-1337.622 -1337.622 -1337.622], eps: 0.543})
Step:   95400, Reward: [-458.644 -458.644 -458.644] [114.318], Avg: [-839.349 -839.349 -839.349] (0.0100) ({r_i: None, r_t: [-943.437 -943.437 -943.437], eps: 0.01})
Step:  128700, Reward: [-1310.492 -1310.492 -1310.492] [368.989], Avg: [-687.542 -687.542 -687.542] (0.1000) ({r_i: None, r_t: [-2630.578 -2630.578 -2630.578], eps: 0.1})
Step:    6200, Reward: [-616.943 -616.943 -616.943] [172.134], Avg: [-546.592 -546.592 -546.592] (0.5371) ({r_i: None, r_t: [-1251.501 -1251.501 -1251.501], eps: 0.537})
Step:   95500, Reward: [-434.664 -434.664 -434.664] [69.633], Avg: [-838.926 -838.926 -838.926] (0.0100) ({r_i: None, r_t: [-855.370 -855.370 -855.370], eps: 0.01})
Step:  128800, Reward: [-1280.925 -1280.925 -1280.925] [487.400], Avg: [-688.003 -688.003 -688.003] (0.1000) ({r_i: None, r_t: [-2548.296 -2548.296 -2548.296], eps: 0.1})
Step:    6300, Reward: [-613.133 -613.133 -613.133] [151.731], Avg: [-547.632 -547.632 -547.632] (0.5318) ({r_i: None, r_t: [-1313.142 -1313.142 -1313.142], eps: 0.532})
Step:   95600, Reward: [-450.733 -450.733 -450.733] [79.334], Avg: [-838.520 -838.520 -838.520] (0.0100) ({r_i: None, r_t: [-884.011 -884.011 -884.011], eps: 0.01})
Step:  128900, Reward: [-1368.508 -1368.508 -1368.508] [261.501], Avg: [-688.530 -688.530 -688.530] (0.1000) ({r_i: None, r_t: [-2594.722 -2594.722 -2594.722], eps: 0.1})
Step:    6400, Reward: [-608.554 -608.554 -608.554] [130.788], Avg: [-548.569 -548.569 -548.569] (0.5264) ({r_i: None, r_t: [-1139.153 -1139.153 -1139.153], eps: 0.526})
Step:  129000, Reward: [-1271.785 -1271.785 -1271.785] [366.671], Avg: [-688.982 -688.982 -688.982] (0.1000) ({r_i: None, r_t: [-2737.908 -2737.908 -2737.908], eps: 0.1})
Step:   95700, Reward: [-433.940 -433.940 -433.940] [83.540], Avg: [-838.098 -838.098 -838.098] (0.0100) ({r_i: None, r_t: [-868.797 -868.797 -868.797], eps: 0.01})
Step:    6500, Reward: [-698.758 -698.758 -698.758] [273.698], Avg: [-550.845 -550.845 -550.845] (0.5212) ({r_i: None, r_t: [-1246.631 -1246.631 -1246.631], eps: 0.521})
Step:  129100, Reward: [-1019.889 -1019.889 -1019.889] [273.953], Avg: [-689.238 -689.238 -689.238] (0.1000) ({r_i: None, r_t: [-2556.844 -2556.844 -2556.844], eps: 0.1})
Step:   95800, Reward: [-432.860 -432.860 -432.860] [67.266], Avg: [-837.675 -837.675 -837.675] (0.0100) ({r_i: None, r_t: [-951.728 -951.728 -951.728], eps: 0.01})
Step:    6600, Reward: [-652.728 -652.728 -652.728] [249.328], Avg: [-552.366 -552.366 -552.366] (0.5160) ({r_i: None, r_t: [-1328.538 -1328.538 -1328.538], eps: 0.516})
Step:  129200, Reward: [-1293.035 -1293.035 -1293.035] [395.432], Avg: [-689.705 -689.705 -689.705] (0.1000) ({r_i: None, r_t: [-2524.985 -2524.985 -2524.985], eps: 0.1})
Step:   95900, Reward: [-429.404 -429.404 -429.404] [68.660], Avg: [-837.250 -837.250 -837.250] (0.0100) ({r_i: None, r_t: [-854.865 -854.865 -854.865], eps: 0.01})
Step:    6700, Reward: [-699.146 -699.146 -699.146] [208.270], Avg: [-554.524 -554.524 -554.524] (0.5108) ({r_i: None, r_t: [-1195.278 -1195.278 -1195.278], eps: 0.511})
Step:  129300, Reward: [-1330.371 -1330.371 -1330.371] [381.688], Avg: [-690.200 -690.200 -690.200] (0.1000) ({r_i: None, r_t: [-2359.874 -2359.874 -2359.874], eps: 0.1})
Step:   96000, Reward: [-423.463 -423.463 -423.463] [83.194], Avg: [-836.819 -836.819 -836.819] (0.0100) ({r_i: None, r_t: [-844.534 -844.534 -844.534], eps: 0.01})
Step:  129400, Reward: [-1096.434 -1096.434 -1096.434] [534.087], Avg: [-690.514 -690.514 -690.514] (0.1000) ({r_i: None, r_t: [-2666.894 -2666.894 -2666.894], eps: 0.1})
Step:    6800, Reward: [-525.789 -525.789 -525.789] [92.021], Avg: [-554.108 -554.108 -554.108] (0.5058) ({r_i: None, r_t: [-1218.239 -1218.239 -1218.239], eps: 0.506})
Step:   96100, Reward: [-438.034 -438.034 -438.034] [126.697], Avg: [-836.405 -836.405 -836.405] (0.0100) ({r_i: None, r_t: [-873.980 -873.980 -873.980], eps: 0.01})
Step:  129500, Reward: [-1195.056 -1195.056 -1195.056] [412.615], Avg: [-690.903 -690.903 -690.903] (0.1000) ({r_i: None, r_t: [-2503.109 -2503.109 -2503.109], eps: 0.1})
Step:    6900, Reward: [-625.514 -625.514 -625.514] [168.618], Avg: [-555.128 -555.128 -555.128] (0.5007) ({r_i: None, r_t: [-1264.115 -1264.115 -1264.115], eps: 0.501})
Step:   96200, Reward: [-437.636 -437.636 -437.636] [90.975], Avg: [-835.991 -835.991 -835.991] (0.0100) ({r_i: None, r_t: [-905.680 -905.680 -905.680], eps: 0.01})
Step:  129600, Reward: [-1313.237 -1313.237 -1313.237] [427.344], Avg: [-691.383 -691.383 -691.383] (0.1000) ({r_i: None, r_t: [-2691.078 -2691.078 -2691.078], eps: 0.1})
Step:    7000, Reward: [-617.349 -617.349 -617.349] [156.328], Avg: [-556.004 -556.004 -556.004] (0.4957) ({r_i: None, r_t: [-1244.446 -1244.446 -1244.446], eps: 0.496})
Step:   96300, Reward: [-441.677 -441.677 -441.677] [94.144], Avg: [-835.582 -835.582 -835.582] (0.0100) ({r_i: None, r_t: [-861.392 -861.392 -861.392], eps: 0.01})
Step:  129700, Reward: [-1446.103 -1446.103 -1446.103] [377.323], Avg: [-691.965 -691.965 -691.965] (0.1000) ({r_i: None, r_t: [-2489.279 -2489.279 -2489.279], eps: 0.1})
Step:    7100, Reward: [-602.934 -602.934 -602.934] [159.399], Avg: [-556.656 -556.656 -556.656] (0.4908) ({r_i: None, r_t: [-1212.892 -1212.892 -1212.892], eps: 0.491})
Step:   96400, Reward: [-495.107 -495.107 -495.107] [86.071], Avg: [-835.229 -835.229 -835.229] (0.0100) ({r_i: None, r_t: [-861.939 -861.939 -861.939], eps: 0.01})
Step:  129800, Reward: [-1269.780 -1269.780 -1269.780] [346.932], Avg: [-692.409 -692.409 -692.409] (0.1000) ({r_i: None, r_t: [-2443.973 -2443.973 -2443.973], eps: 0.1})
Step:    7200, Reward: [-631.885 -631.885 -631.885] [104.558], Avg: [-557.686 -557.686 -557.686] (0.4859) ({r_i: None, r_t: [-1210.841 -1210.841 -1210.841], eps: 0.486})
Step:   96500, Reward: [-433.126 -433.126 -433.126] [102.728], Avg: [-834.813 -834.813 -834.813] (0.0100) ({r_i: None, r_t: [-904.195 -904.195 -904.195], eps: 0.01})
Step:  129900, Reward: [-1136.335 -1136.335 -1136.335] [325.159], Avg: [-692.751 -692.751 -692.751] (0.1000) ({r_i: None, r_t: [-2445.778 -2445.778 -2445.778], eps: 0.1})
Step:    7300, Reward: [-566.484 -566.484 -566.484] [146.505], Avg: [-557.805 -557.805 -557.805] (0.4810) ({r_i: None, r_t: [-1279.179 -1279.179 -1279.179], eps: 0.481})
Step:   96600, Reward: [-457.522 -457.522 -457.522] [136.988], Avg: [-834.422 -834.422 -834.422] (0.0100) ({r_i: None, r_t: [-902.815 -902.815 -902.815], eps: 0.01})
Step:  130000, Reward: [-1215.612 -1215.612 -1215.612] [430.521], Avg: [-693.153 -693.153 -693.153] (0.1000) ({r_i: None, r_t: [-2296.331 -2296.331 -2296.331], eps: 0.1})
Step:    7400, Reward: [-664.082 -664.082 -664.082] [287.350], Avg: [-559.222 -559.222 -559.222] (0.4762) ({r_i: None, r_t: [-1189.862 -1189.862 -1189.862], eps: 0.476})
Step:   96700, Reward: [-448.850 -448.850 -448.850] [108.511], Avg: [-834.024 -834.024 -834.024] (0.0100) ({r_i: None, r_t: [-852.760 -852.760 -852.760], eps: 0.01})
Step:  130100, Reward: [-1280.363 -1280.363 -1280.363] [388.231], Avg: [-693.604 -693.604 -693.604] (0.1000) ({r_i: None, r_t: [-2561.909 -2561.909 -2561.909], eps: 0.1})
Step:    7500, Reward: [-626.497 -626.497 -626.497] [132.182], Avg: [-560.107 -560.107 -560.107] (0.4715) ({r_i: None, r_t: [-1237.433 -1237.433 -1237.433], eps: 0.471})
Step:   96800, Reward: [-411.061 -411.061 -411.061] [53.425], Avg: [-833.588 -833.588 -833.588] (0.0100) ({r_i: None, r_t: [-840.727 -840.727 -840.727], eps: 0.01})
Step:  130200, Reward: [-1147.898 -1147.898 -1147.898] [348.365], Avg: [-693.952 -693.952 -693.952] (0.1000) ({r_i: None, r_t: [-2496.557 -2496.557 -2496.557], eps: 0.1})
Step:    7600, Reward: [-657.804 -657.804 -657.804] [281.232], Avg: [-561.376 -561.376 -561.376] (0.4668) ({r_i: None, r_t: [-1156.120 -1156.120 -1156.120], eps: 0.467})
Step:   96900, Reward: [-449.947 -449.947 -449.947] [82.637], Avg: [-833.192 -833.192 -833.192] (0.0100) ({r_i: None, r_t: [-875.599 -875.599 -875.599], eps: 0.01})
Step:  130300, Reward: [-1240.559 -1240.559 -1240.559] [474.729], Avg: [-694.372 -694.372 -694.372] (0.1000) ({r_i: None, r_t: [-2384.593 -2384.593 -2384.593], eps: 0.1})
Step:    7700, Reward: [-714.740 -714.740 -714.740] [253.956], Avg: [-563.342 -563.342 -563.342] (0.4621) ({r_i: None, r_t: [-1289.187 -1289.187 -1289.187], eps: 0.462})
Step:   97000, Reward: [-435.227 -435.227 -435.227] [131.120], Avg: [-832.782 -832.782 -832.782] (0.0100) ({r_i: None, r_t: [-812.623 -812.623 -812.623], eps: 0.01})
Step:  130400, Reward: [-1169.569 -1169.569 -1169.569] [318.105], Avg: [-694.736 -694.736 -694.736] (0.1000) ({r_i: None, r_t: [-2744.075 -2744.075 -2744.075], eps: 0.1})
Step:    7800, Reward: [-567.982 -567.982 -567.982] [165.329], Avg: [-563.401 -563.401 -563.401] (0.4575) ({r_i: None, r_t: [-1209.393 -1209.393 -1209.393], eps: 0.458})
Step:   97100, Reward: [-433.530 -433.530 -433.530] [112.043], Avg: [-832.372 -832.372 -832.372] (0.0100) ({r_i: None, r_t: [-954.348 -954.348 -954.348], eps: 0.01})
Step:  130500, Reward: [-1092.670 -1092.670 -1092.670] [327.099], Avg: [-695.040 -695.040 -695.040] (0.1000) ({r_i: None, r_t: [-2494.219 -2494.219 -2494.219], eps: 0.1})
Step:   97200, Reward: [-446.518 -446.518 -446.518] [121.371], Avg: [-831.975 -831.975 -831.975] (0.0100) ({r_i: None, r_t: [-904.884 -904.884 -904.884], eps: 0.01})
Step:    7900, Reward: [-675.648 -675.648 -675.648] [229.799], Avg: [-564.804 -564.804 -564.804] (0.4529) ({r_i: None, r_t: [-1316.846 -1316.846 -1316.846], eps: 0.453})
Step:  130600, Reward: [-1165.419 -1165.419 -1165.419] [449.721], Avg: [-695.400 -695.400 -695.400] (0.1000) ({r_i: None, r_t: [-2451.018 -2451.018 -2451.018], eps: 0.1})
Step:   97300, Reward: [-448.225 -448.225 -448.225] [73.224], Avg: [-831.581 -831.581 -831.581] (0.0100) ({r_i: None, r_t: [-847.040 -847.040 -847.040], eps: 0.01})
Step:    8000, Reward: [-701.749 -701.749 -701.749] [209.067], Avg: [-566.495 -566.495 -566.495] (0.4484) ({r_i: None, r_t: [-1304.432 -1304.432 -1304.432], eps: 0.448})
Step:  130700, Reward: [-1296.559 -1296.559 -1296.559] [459.750], Avg: [-695.860 -695.860 -695.860] (0.1000) ({r_i: None, r_t: [-2405.334 -2405.334 -2405.334], eps: 0.1})
Step:   97400, Reward: [-458.739 -458.739 -458.739] [51.588], Avg: [-831.199 -831.199 -831.199] (0.0100) ({r_i: None, r_t: [-919.843 -919.843 -919.843], eps: 0.01})
Step:    8100, Reward: [-604.230 -604.230 -604.230] [146.262], Avg: [-566.955 -566.955 -566.955] (0.4440) ({r_i: None, r_t: [-1521.572 -1521.572 -1521.572], eps: 0.444})
Step:  130800, Reward: [-1282.287 -1282.287 -1282.287] [464.817], Avg: [-696.308 -696.308 -696.308] (0.1000) ({r_i: None, r_t: [-2516.402 -2516.402 -2516.402], eps: 0.1})
Step:   97500, Reward: [-466.407 -466.407 -466.407] [125.302], Avg: [-830.825 -830.825 -830.825] (0.0100) ({r_i: None, r_t: [-891.320 -891.320 -891.320], eps: 0.01})
Step:    8200, Reward: [-611.212 -611.212 -611.212] [184.444], Avg: [-567.488 -567.488 -567.488] (0.4395) ({r_i: None, r_t: [-1462.186 -1462.186 -1462.186], eps: 0.44})
Step:  130900, Reward: [-1336.181 -1336.181 -1336.181] [435.407], Avg: [-696.796 -696.796 -696.796] (0.1000) ({r_i: None, r_t: [-2430.780 -2430.780 -2430.780], eps: 0.1})
Step:   97600, Reward: [-433.880 -433.880 -433.880] [68.932], Avg: [-830.419 -830.419 -830.419] (0.0100) ({r_i: None, r_t: [-839.243 -839.243 -839.243], eps: 0.01})
Step:    8300, Reward: [-726.188 -726.188 -726.188] [204.728], Avg: [-569.378 -569.378 -569.378] (0.4351) ({r_i: None, r_t: [-1443.982 -1443.982 -1443.982], eps: 0.435})
Step:  131000, Reward: [-1321.324 -1321.324 -1321.324] [399.805], Avg: [-697.273 -697.273 -697.273] (0.1000) ({r_i: None, r_t: [-2492.515 -2492.515 -2492.515], eps: 0.1})
Step:   97700, Reward: [-438.067 -438.067 -438.067] [121.572], Avg: [-830.017 -830.017 -830.017] (0.0100) ({r_i: None, r_t: [-831.442 -831.442 -831.442], eps: 0.01})
Step:    8400, Reward: [-673.512 -673.512 -673.512] [187.033], Avg: [-570.603 -570.603 -570.603] (0.4308) ({r_i: None, r_t: [-1383.150 -1383.150 -1383.150], eps: 0.431})
Step:  131100, Reward: [-1147.188 -1147.188 -1147.188] [478.421], Avg: [-697.616 -697.616 -697.616] (0.1000) ({r_i: None, r_t: [-1892.583 -1892.583 -1892.583], eps: 0.1})
Step:   97800, Reward: [-461.727 -461.727 -461.727] [133.399], Avg: [-829.641 -829.641 -829.641] (0.0100) ({r_i: None, r_t: [-845.081 -845.081 -845.081], eps: 0.01})
Step:    8500, Reward: [-749.504 -749.504 -749.504] [226.824], Avg: [-572.683 -572.683 -572.683] (0.4265) ({r_i: None, r_t: [-1409.189 -1409.189 -1409.189], eps: 0.427})
Step:  131200, Reward: [-1180.246 -1180.246 -1180.246] [426.546], Avg: [-697.983 -697.983 -697.983] (0.1000) ({r_i: None, r_t: [-2316.028 -2316.028 -2316.028], eps: 0.1})
Step:   97900, Reward: [-418.313 -418.313 -418.313] [87.223], Avg: [-829.221 -829.221 -829.221] (0.0100) ({r_i: None, r_t: [-862.064 -862.064 -862.064], eps: 0.01})
Step:    8600, Reward: [-783.105 -783.105 -783.105] [309.769], Avg: [-575.102 -575.102 -575.102] (0.4223) ({r_i: None, r_t: [-1457.613 -1457.613 -1457.613], eps: 0.422})
Step:  131300, Reward: [-1103.114 -1103.114 -1103.114] [345.727], Avg: [-698.292 -698.292 -698.292] (0.1000) ({r_i: None, r_t: [-2597.090 -2597.090 -2597.090], eps: 0.1})
Step:   98000, Reward: [-458.941 -458.941 -458.941] [106.074], Avg: [-828.844 -828.844 -828.844] (0.0100) ({r_i: None, r_t: [-829.876 -829.876 -829.876], eps: 0.01})
Step:    8700, Reward: [-690.724 -690.724 -690.724] [229.595], Avg: [-576.416 -576.416 -576.416] (0.4180) ({r_i: None, r_t: [-1404.903 -1404.903 -1404.903], eps: 0.418})
Step:  131400, Reward: [-1256.078 -1256.078 -1256.078] [404.160], Avg: [-698.716 -698.716 -698.716] (0.1000) ({r_i: None, r_t: [-2592.551 -2592.551 -2592.551], eps: 0.1})
Step:   98100, Reward: [-421.146 -421.146 -421.146] [68.916], Avg: [-828.429 -828.429 -828.429] (0.0100) ({r_i: None, r_t: [-880.585 -880.585 -880.585], eps: 0.01})
Step:    8800, Reward: [-780.968 -780.968 -780.968] [301.302], Avg: [-578.714 -578.714 -578.714] (0.4139) ({r_i: None, r_t: [-1563.678 -1563.678 -1563.678], eps: 0.414})
Step:  131500, Reward: [-1175.166 -1175.166 -1175.166] [329.170], Avg: [-699.078 -699.078 -699.078] (0.1000) ({r_i: None, r_t: [-2392.416 -2392.416 -2392.416], eps: 0.1})
Step:   98200, Reward: [-439.476 -439.476 -439.476] [82.083], Avg: [-828.033 -828.033 -828.033] (0.0100) ({r_i: None, r_t: [-824.050 -824.050 -824.050], eps: 0.01})
Step:    8900, Reward: [-755.415 -755.415 -755.415] [140.824], Avg: [-580.677 -580.677 -580.677] (0.4097) ({r_i: None, r_t: [-1533.191 -1533.191 -1533.191], eps: 0.41})
Step:  131600, Reward: [-945.240 -945.240 -945.240] [311.873], Avg: [-699.265 -699.265 -699.265] (0.1000) ({r_i: None, r_t: [-2342.559 -2342.559 -2342.559], eps: 0.1})
Step:   98300, Reward: [-433.842 -433.842 -433.842] [86.628], Avg: [-827.633 -827.633 -827.633] (0.0100) ({r_i: None, r_t: [-834.062 -834.062 -834.062], eps: 0.01})
Step:    9000, Reward: [-932.959 -932.959 -932.959] [279.379], Avg: [-584.548 -584.548 -584.548] (0.4057) ({r_i: None, r_t: [-1761.005 -1761.005 -1761.005], eps: 0.406})
Step:  131700, Reward: [-971.785 -971.785 -971.785] [300.833], Avg: [-699.471 -699.471 -699.471] (0.1000) ({r_i: None, r_t: [-2160.300 -2160.300 -2160.300], eps: 0.1})
Step:   98400, Reward: [-448.510 -448.510 -448.510] [85.916], Avg: [-827.248 -827.248 -827.248] (0.0100) ({r_i: None, r_t: [-906.058 -906.058 -906.058], eps: 0.01})
Step:    9100, Reward: [-829.679 -829.679 -829.679] [331.473], Avg: [-587.213 -587.213 -587.213] (0.4016) ({r_i: None, r_t: [-1580.643 -1580.643 -1580.643], eps: 0.402})
Step:  131800, Reward: [-995.463 -995.463 -995.463] [309.737], Avg: [-699.696 -699.696 -699.696] (0.1000) ({r_i: None, r_t: [-2402.428 -2402.428 -2402.428], eps: 0.1})
Step:   98500, Reward: [-452.939 -452.939 -452.939] [102.142], Avg: [-826.868 -826.868 -826.868] (0.0100) ({r_i: None, r_t: [-851.082 -851.082 -851.082], eps: 0.01})
Step:  131900, Reward: [-1063.866 -1063.866 -1063.866] [378.052], Avg: [-699.972 -699.972 -699.972] (0.1000) ({r_i: None, r_t: [-2076.407 -2076.407 -2076.407], eps: 0.1})
Step:    9200, Reward: [-813.650 -813.650 -813.650] [286.150], Avg: [-589.648 -589.648 -589.648] (0.3976) ({r_i: None, r_t: [-1561.520 -1561.520 -1561.520], eps: 0.398})
Step:   98600, Reward: [-484.152 -484.152 -484.152] [150.422], Avg: [-826.521 -826.521 -826.521] (0.0100) ({r_i: None, r_t: [-862.927 -862.927 -862.927], eps: 0.01})
Step:  132000, Reward: [-990.998 -990.998 -990.998] [369.008], Avg: [-700.192 -700.192 -700.192] (0.1000) ({r_i: None, r_t: [-2054.493 -2054.493 -2054.493], eps: 0.1})
Step:    9300, Reward: [-700.284 -700.284 -700.284] [237.524], Avg: [-590.825 -590.825 -590.825] (0.3936) ({r_i: None, r_t: [-1710.295 -1710.295 -1710.295], eps: 0.394})
Step:   98700, Reward: [-449.313 -449.313 -449.313] [77.453], Avg: [-826.139 -826.139 -826.139] (0.0100) ({r_i: None, r_t: [-926.235 -926.235 -926.235], eps: 0.01})
Step:  132100, Reward: [-1139.039 -1139.039 -1139.039] [392.650], Avg: [-700.524 -700.524 -700.524] (0.1000) ({r_i: None, r_t: [-2387.291 -2387.291 -2387.291], eps: 0.1})
Step:    9400, Reward: [-691.833 -691.833 -691.833] [203.641], Avg: [-591.888 -591.888 -591.888] (0.3897) ({r_i: None, r_t: [-1461.229 -1461.229 -1461.229], eps: 0.39})
Step:  132200, Reward: [-1134.896 -1134.896 -1134.896] [427.116], Avg: [-700.852 -700.852 -700.852] (0.1000) ({r_i: None, r_t: [-2156.676 -2156.676 -2156.676], eps: 0.1})
Step:   98800, Reward: [-442.486 -442.486 -442.486] [92.167], Avg: [-825.751 -825.751 -825.751] (0.0100) ({r_i: None, r_t: [-857.817 -857.817 -857.817], eps: 0.01})
Step:    9500, Reward: [-826.497 -826.497 -826.497] [239.012], Avg: [-594.332 -594.332 -594.332] (0.3858) ({r_i: None, r_t: [-1484.064 -1484.064 -1484.064], eps: 0.386})
Step:  132300, Reward: [-1221.520 -1221.520 -1221.520] [360.865], Avg: [-701.246 -701.246 -701.246] (0.1000) ({r_i: None, r_t: [-1976.107 -1976.107 -1976.107], eps: 0.1})
Step:   98900, Reward: [-429.774 -429.774 -429.774] [79.186], Avg: [-825.351 -825.351 -825.351] (0.0100) ({r_i: None, r_t: [-878.498 -878.498 -878.498], eps: 0.01})
Step:    9600, Reward: [-809.320 -809.320 -809.320] [345.497], Avg: [-596.548 -596.548 -596.548] (0.3820) ({r_i: None, r_t: [-1468.264 -1468.264 -1468.264], eps: 0.382})
Step:  132400, Reward: [-1147.058 -1147.058 -1147.058] [420.384], Avg: [-701.582 -701.582 -701.582] (0.1000) ({r_i: None, r_t: [-2190.744 -2190.744 -2190.744], eps: 0.1})
Step:   99000, Reward: [-415.024 -415.024 -415.024] [53.241], Avg: [-824.937 -824.937 -824.937] (0.0100) ({r_i: None, r_t: [-899.064 -899.064 -899.064], eps: 0.01})
Step:    9700, Reward: [-716.289 -716.289 -716.289] [214.635], Avg: [-597.770 -597.770 -597.770] (0.3782) ({r_i: None, r_t: [-1400.414 -1400.414 -1400.414], eps: 0.378})
Step:  132500, Reward: [-1139.224 -1139.224 -1139.224] [362.408], Avg: [-701.912 -701.912 -701.912] (0.1000) ({r_i: None, r_t: [-2112.331 -2112.331 -2112.331], eps: 0.1})
Step:   99100, Reward: [-433.903 -433.903 -433.903] [69.287], Avg: [-824.543 -824.543 -824.543] (0.0100) ({r_i: None, r_t: [-864.543 -864.543 -864.543], eps: 0.01})
Step:    9800, Reward: [-798.445 -798.445 -798.445] [312.092], Avg: [-599.797 -599.797 -599.797] (0.3744) ({r_i: None, r_t: [-1594.971 -1594.971 -1594.971], eps: 0.374})
Step:  132600, Reward: [-999.881 -999.881 -999.881] [337.401], Avg: [-702.137 -702.137 -702.137] (0.1000) ({r_i: None, r_t: [-1987.222 -1987.222 -1987.222], eps: 0.1})
Step:   99200, Reward: [-423.362 -423.362 -423.362] [90.302], Avg: [-824.139 -824.139 -824.139] (0.0100) ({r_i: None, r_t: [-889.090 -889.090 -889.090], eps: 0.01})
Step:    9900, Reward: [-611.321 -611.321 -611.321] [171.538], Avg: [-599.912 -599.912 -599.912] (0.3707) ({r_i: None, r_t: [-1427.645 -1427.645 -1427.645], eps: 0.371})
Step:  132700, Reward: [-1069.151 -1069.151 -1069.151] [425.568], Avg: [-702.413 -702.413 -702.413] (0.1000) ({r_i: None, r_t: [-2316.172 -2316.172 -2316.172], eps: 0.1})
Step:   99300, Reward: [-430.079 -430.079 -430.079] [78.171], Avg: [-823.742 -823.742 -823.742] (0.0100) ({r_i: None, r_t: [-869.002 -869.002 -869.002], eps: 0.01})
Step:   10000, Reward: [-760.142 -760.142 -760.142] [338.321], Avg: [-601.499 -601.499 -601.499] (0.3670) ({r_i: None, r_t: [-1349.010 -1349.010 -1349.010], eps: 0.367})
Step:  132800, Reward: [-1094.029 -1094.029 -1094.029] [358.029], Avg: [-702.708 -702.708 -702.708] (0.1000) ({r_i: None, r_t: [-2205.885 -2205.885 -2205.885], eps: 0.1})
Step:   99400, Reward: [-400.014 -400.014 -400.014] [73.204], Avg: [-823.317 -823.317 -823.317] (0.0100) ({r_i: None, r_t: [-946.378 -946.378 -946.378], eps: 0.01})
Step:   10100, Reward: [-712.470 -712.470 -712.470] [225.458], Avg: [-602.587 -602.587 -602.587] (0.3633) ({r_i: None, r_t: [-1225.961 -1225.961 -1225.961], eps: 0.363})
Step:  132900, Reward: [-1117.273 -1117.273 -1117.273] [415.125], Avg: [-703.019 -703.019 -703.019] (0.1000) ({r_i: None, r_t: [-2211.813 -2211.813 -2211.813], eps: 0.1})
Step:   99500, Reward: [-426.149 -426.149 -426.149] [104.876], Avg: [-822.918 -822.918 -822.918] (0.0100) ({r_i: None, r_t: [-882.456 -882.456 -882.456], eps: 0.01})
Step:   10200, Reward: [-589.649 -589.649 -589.649] [184.494], Avg: [-602.461 -602.461 -602.461] (0.3597) ({r_i: None, r_t: [-1339.058 -1339.058 -1339.058], eps: 0.36})
Step:  133000, Reward: [-1045.277 -1045.277 -1045.277] [340.493], Avg: [-703.276 -703.276 -703.276] (0.1000) ({r_i: None, r_t: [-2153.586 -2153.586 -2153.586], eps: 0.1})
Step:   99600, Reward: [-404.207 -404.207 -404.207] [68.772], Avg: [-822.498 -822.498 -822.498] (0.0100) ({r_i: None, r_t: [-917.380 -917.380 -917.380], eps: 0.01})
Step:   10300, Reward: [-666.352 -666.352 -666.352] [247.031], Avg: [-603.075 -603.075 -603.075] (0.3561) ({r_i: None, r_t: [-1245.938 -1245.938 -1245.938], eps: 0.356})
Step:  133100, Reward: [-1070.558 -1070.558 -1070.558] [382.421], Avg: [-703.552 -703.552 -703.552] (0.1000) ({r_i: None, r_t: [-2084.403 -2084.403 -2084.403], eps: 0.1})
Step:   99700, Reward: [-419.944 -419.944 -419.944] [86.981], Avg: [-822.094 -822.094 -822.094] (0.0100) ({r_i: None, r_t: [-790.543 -790.543 -790.543], eps: 0.01})
Step:   10400, Reward: [-673.363 -673.363 -673.363] [261.863], Avg: [-603.745 -603.745 -603.745] (0.3525) ({r_i: None, r_t: [-1442.496 -1442.496 -1442.496], eps: 0.353})
Step:  133200, Reward: [-1040.931 -1040.931 -1040.931] [394.524], Avg: [-703.805 -703.805 -703.805] (0.1000) ({r_i: None, r_t: [-2187.572 -2187.572 -2187.572], eps: 0.1})
Step:   99800, Reward: [-409.753 -409.753 -409.753] [66.548], Avg: [-821.682 -821.682 -821.682] (0.0100) ({r_i: None, r_t: [-903.049 -903.049 -903.049], eps: 0.01})
Step:   10500, Reward: [-693.468 -693.468 -693.468] [179.748], Avg: [-604.591 -604.591 -604.591] (0.3490) ({r_i: None, r_t: [-1337.869 -1337.869 -1337.869], eps: 0.349})
Step:  133300, Reward: [-985.623 -985.623 -985.623] [378.318], Avg: [-704.017 -704.017 -704.017] (0.1000) ({r_i: None, r_t: [-2105.946 -2105.946 -2105.946], eps: 0.1})
Step:   99900, Reward: [-465.301 -465.301 -465.301] [85.681], Avg: [-821.325 -821.325 -821.325] (0.0100) ({r_i: None, r_t: [-834.829 -834.829 -834.829], eps: 0.01})
Step:   10600, Reward: [-851.750 -851.750 -851.750] [354.309], Avg: [-606.901 -606.901 -606.901] (0.3455) ({r_i: None, r_t: [-1237.942 -1237.942 -1237.942], eps: 0.346})
Step:  133400, Reward: [-1205.472 -1205.472 -1205.472] [491.124], Avg: [-704.392 -704.392 -704.392] (0.1000) ({r_i: None, r_t: [-2276.627 -2276.627 -2276.627], eps: 0.1})
Step:  100000, Reward: [-420.348 -420.348 -420.348] [110.102], Avg: [-820.925 -820.925 -820.925] (0.0100) ({r_i: None, r_t: [-884.570 -884.570 -884.570], eps: 0.01})
Step:   10700, Reward: [-613.297 -613.297 -613.297] [132.960], Avg: [-606.960 -606.960 -606.960] (0.3421) ({r_i: None, r_t: [-1275.169 -1275.169 -1275.169], eps: 0.342})
Step:  133500, Reward: [-1022.818 -1022.818 -1022.818] [330.009], Avg: [-704.631 -704.631 -704.631] (0.1000) ({r_i: None, r_t: [-2299.489 -2299.489 -2299.489], eps: 0.1})
Step:  100100, Reward: [-419.487 -419.487 -419.487] [70.785], Avg: [-820.524 -820.524 -820.524] (0.0100) ({r_i: None, r_t: [-889.724 -889.724 -889.724], eps: 0.01})
Step:   10800, Reward: [-669.309 -669.309 -669.309] [360.641], Avg: [-607.532 -607.532 -607.532] (0.3387) ({r_i: None, r_t: [-1382.849 -1382.849 -1382.849], eps: 0.339})
Step:  133600, Reward: [-992.267 -992.267 -992.267] [451.457], Avg: [-704.846 -704.846 -704.846] (0.1000) ({r_i: None, r_t: [-1958.336 -1958.336 -1958.336], eps: 0.1})
Step:  100200, Reward: [-466.533 -466.533 -466.533] [78.804], Avg: [-820.171 -820.171 -820.171] (0.0100) ({r_i: None, r_t: [-838.015 -838.015 -838.015], eps: 0.01})
Step:   10900, Reward: [-667.785 -667.785 -667.785] [241.741], Avg: [-608.080 -608.080 -608.080] (0.3353) ({r_i: None, r_t: [-1371.989 -1371.989 -1371.989], eps: 0.335})
Step:  133700, Reward: [-990.884 -990.884 -990.884] [426.957], Avg: [-705.059 -705.059 -705.059] (0.1000) ({r_i: None, r_t: [-1888.787 -1888.787 -1888.787], eps: 0.1})
Step:  100300, Reward: [-439.204 -439.204 -439.204] [83.173], Avg: [-819.792 -819.792 -819.792] (0.0100) ({r_i: None, r_t: [-875.904 -875.904 -875.904], eps: 0.01})
Step:   11000, Reward: [-611.823 -611.823 -611.823] [230.473], Avg: [-608.114 -608.114 -608.114] (0.3320) ({r_i: None, r_t: [-1313.359 -1313.359 -1313.359], eps: 0.332})
Step:  133800, Reward: [-885.912 -885.912 -885.912] [289.548], Avg: [-705.195 -705.195 -705.195] (0.1000) ({r_i: None, r_t: [-2027.287 -2027.287 -2027.287], eps: 0.1})
Step:  100400, Reward: [-443.304 -443.304 -443.304] [76.992], Avg: [-819.417 -819.417 -819.417] (0.0100) ({r_i: None, r_t: [-860.176 -860.176 -860.176], eps: 0.01})
Step:   11100, Reward: [-605.104 -605.104 -605.104] [140.626], Avg: [-608.087 -608.087 -608.087] (0.3286) ({r_i: None, r_t: [-1261.390 -1261.390 -1261.390], eps: 0.329})
Step:  133900, Reward: [-948.933 -948.933 -948.933] [467.644], Avg: [-705.376 -705.376 -705.376] (0.1000) ({r_i: None, r_t: [-1970.442 -1970.442 -1970.442], eps: 0.1})
Step:  100500, Reward: [-447.831 -447.831 -447.831] [104.710], Avg: [-819.048 -819.048 -819.048] (0.0100) ({r_i: None, r_t: [-924.962 -924.962 -924.962], eps: 0.01})
Step:   11200, Reward: [-651.249 -651.249 -651.249] [156.669], Avg: [-608.469 -608.469 -608.469] (0.3254) ({r_i: None, r_t: [-1167.671 -1167.671 -1167.671], eps: 0.325})
Step:  134000, Reward: [-944.961 -944.961 -944.961] [282.505], Avg: [-705.555 -705.555 -705.555] (0.1000) ({r_i: None, r_t: [-1717.596 -1717.596 -1717.596], eps: 0.1})
Step:  100600, Reward: [-388.398 -388.398 -388.398] [85.456], Avg: [-818.620 -818.620 -818.620] (0.0100) ({r_i: None, r_t: [-949.263 -949.263 -949.263], eps: 0.01})
Step:  134100, Reward: [-1029.486 -1029.486 -1029.486] [476.434], Avg: [-705.796 -705.796 -705.796] (0.1000) ({r_i: None, r_t: [-2041.570 -2041.570 -2041.570], eps: 0.1})
Step:   11300, Reward: [-750.354 -750.354 -750.354] [258.346], Avg: [-609.714 -609.714 -609.714] (0.3221) ({r_i: None, r_t: [-1184.624 -1184.624 -1184.624], eps: 0.322})
Step:  100700, Reward: [-419.173 -419.173 -419.173] [75.750], Avg: [-818.224 -818.224 -818.224] (0.0100) ({r_i: None, r_t: [-854.633 -854.633 -854.633], eps: 0.01})
Step:  134200, Reward: [-1093.332 -1093.332 -1093.332] [402.496], Avg: [-706.085 -706.085 -706.085] (0.1000) ({r_i: None, r_t: [-2198.839 -2198.839 -2198.839], eps: 0.1})
Step:   11400, Reward: [-572.879 -572.879 -572.879] [191.241], Avg: [-609.393 -609.393 -609.393] (0.3189) ({r_i: None, r_t: [-1258.399 -1258.399 -1258.399], eps: 0.319})
Step:  100800, Reward: [-408.917 -408.917 -408.917] [67.470], Avg: [-817.818 -817.818 -817.818] (0.0100) ({r_i: None, r_t: [-857.443 -857.443 -857.443], eps: 0.01})
Step:  134300, Reward: [-1128.569 -1128.569 -1128.569] [533.357], Avg: [-706.399 -706.399 -706.399] (0.1000) ({r_i: None, r_t: [-1998.097 -1998.097 -1998.097], eps: 0.1})
Step:   11500, Reward: [-621.450 -621.450 -621.450] [168.419], Avg: [-609.497 -609.497 -609.497] (0.3157) ({r_i: None, r_t: [-1273.268 -1273.268 -1273.268], eps: 0.316})
Step:  100900, Reward: [-443.823 -443.823 -443.823] [95.519], Avg: [-817.448 -817.448 -817.448] (0.0100) ({r_i: None, r_t: [-848.809 -848.809 -848.809], eps: 0.01})
Step:  134400, Reward: [-954.179 -954.179 -954.179] [351.191], Avg: [-706.584 -706.584 -706.584] (0.1000) ({r_i: None, r_t: [-1868.839 -1868.839 -1868.839], eps: 0.1})
Step:   11600, Reward: [-644.989 -644.989 -644.989] [159.186], Avg: [-609.800 -609.800 -609.800] (0.3126) ({r_i: None, r_t: [-1236.379 -1236.379 -1236.379], eps: 0.313})
Step:  101000, Reward: [-413.488 -413.488 -413.488] [84.456], Avg: [-817.048 -817.048 -817.048] (0.0100) ({r_i: None, r_t: [-826.224 -826.224 -826.224], eps: 0.01})
Step:  134500, Reward: [-953.957 -953.957 -953.957] [429.574], Avg: [-706.767 -706.767 -706.767] (0.1000) ({r_i: None, r_t: [-1724.657 -1724.657 -1724.657], eps: 0.1})
Step:   11700, Reward: [-715.887 -715.887 -715.887] [229.728], Avg: [-610.700 -610.700 -610.700] (0.3095) ({r_i: None, r_t: [-1212.474 -1212.474 -1212.474], eps: 0.309})
Step:  101100, Reward: [-444.039 -444.039 -444.039] [76.414], Avg: [-816.680 -816.680 -816.680] (0.0100) ({r_i: None, r_t: [-871.816 -871.816 -871.816], eps: 0.01})
Step:  134600, Reward: [-854.089 -854.089 -854.089] [215.289], Avg: [-706.877 -706.877 -706.877] (0.1000) ({r_i: None, r_t: [-1856.878 -1856.878 -1856.878], eps: 0.1})
Step:   11800, Reward: [-608.987 -608.987 -608.987] [178.309], Avg: [-610.685 -610.685 -610.685] (0.3064) ({r_i: None, r_t: [-1358.903 -1358.903 -1358.903], eps: 0.306})
Step:  134700, Reward: [-875.586 -875.586 -875.586] [289.763], Avg: [-707.002 -707.002 -707.002] (0.1000) ({r_i: None, r_t: [-1989.489 -1989.489 -1989.489], eps: 0.1})
Step:  101200, Reward: [-427.265 -427.265 -427.265] [79.209], Avg: [-816.295 -816.295 -816.295] (0.0100) ({r_i: None, r_t: [-889.024 -889.024 -889.024], eps: 0.01})
Step:   11900, Reward: [-602.895 -602.895 -602.895] [115.499], Avg: [-610.620 -610.620 -610.620] (0.3033) ({r_i: None, r_t: [-1328.752 -1328.752 -1328.752], eps: 0.303})
Step:  134800, Reward: [-968.826 -968.826 -968.826] [484.570], Avg: [-707.196 -707.196 -707.196] (0.1000) ({r_i: None, r_t: [-1763.309 -1763.309 -1763.309], eps: 0.1})
Step:  101300, Reward: [-410.465 -410.465 -410.465] [66.160], Avg: [-815.895 -815.895 -815.895] (0.0100) ({r_i: None, r_t: [-877.043 -877.043 -877.043], eps: 0.01})
Step:   12000, Reward: [-588.323 -588.323 -588.323] [103.053], Avg: [-610.436 -610.436 -610.436] (0.3003) ({r_i: None, r_t: [-1274.072 -1274.072 -1274.072], eps: 0.3})
Step:  134900, Reward: [-945.278 -945.278 -945.278] [379.320], Avg: [-707.372 -707.372 -707.372] (0.1000) ({r_i: None, r_t: [-1606.185 -1606.185 -1606.185], eps: 0.1})
Step:  101400, Reward: [-431.685 -431.685 -431.685] [106.551], Avg: [-815.517 -815.517 -815.517] (0.0100) ({r_i: None, r_t: [-879.000 -879.000 -879.000], eps: 0.01})
Step:   12100, Reward: [-674.908 -674.908 -674.908] [267.203], Avg: [-610.964 -610.964 -610.964] (0.2973) ({r_i: None, r_t: [-1267.267 -1267.267 -1267.267], eps: 0.297})
Step:  135000, Reward: [-934.701 -934.701 -934.701] [337.364], Avg: [-707.541 -707.541 -707.541] (0.1000) ({r_i: None, r_t: [-1808.877 -1808.877 -1808.877], eps: 0.1})
Step:  101500, Reward: [-423.315 -423.315 -423.315] [63.354], Avg: [-815.131 -815.131 -815.131] (0.0100) ({r_i: None, r_t: [-876.748 -876.748 -876.748], eps: 0.01})
Step:   12200, Reward: [-638.426 -638.426 -638.426] [198.279], Avg: [-611.188 -611.188 -611.188] (0.2943) ({r_i: None, r_t: [-1291.945 -1291.945 -1291.945], eps: 0.294})
Step:  135100, Reward: [-852.647 -852.647 -852.647] [379.517], Avg: [-707.648 -707.648 -707.648] (0.1000) ({r_i: None, r_t: [-1861.897 -1861.897 -1861.897], eps: 0.1})
Step:  101600, Reward: [-456.704 -456.704 -456.704] [93.422], Avg: [-814.778 -814.778 -814.778] (0.0100) ({r_i: None, r_t: [-905.220 -905.220 -905.220], eps: 0.01})
Step:   12300, Reward: [-802.045 -802.045 -802.045] [222.977], Avg: [-612.727 -612.727 -612.727] (0.2914) ({r_i: None, r_t: [-1365.410 -1365.410 -1365.410], eps: 0.291})
Step:  135200, Reward: [-843.064 -843.064 -843.064] [281.670], Avg: [-707.748 -707.748 -707.748] (0.1000) ({r_i: None, r_t: [-1820.848 -1820.848 -1820.848], eps: 0.1})
Step:  101700, Reward: [-439.574 -439.574 -439.574] [76.152], Avg: [-814.409 -814.409 -814.409] (0.0100) ({r_i: None, r_t: [-894.115 -894.115 -894.115], eps: 0.01})
Step:   12400, Reward: [-695.227 -695.227 -695.227] [181.224], Avg: [-613.387 -613.387 -613.387] (0.2885) ({r_i: None, r_t: [-1259.117 -1259.117 -1259.117], eps: 0.288})
Step:  135300, Reward: [-941.262 -941.262 -941.262] [414.146], Avg: [-707.920 -707.920 -707.920] (0.1000) ({r_i: None, r_t: [-1831.349 -1831.349 -1831.349], eps: 0.1})
Step:  101800, Reward: [-419.892 -419.892 -419.892] [69.115], Avg: [-814.022 -814.022 -814.022] (0.0100) ({r_i: None, r_t: [-930.499 -930.499 -930.499], eps: 0.01})
Step:   12500, Reward: [-730.254 -730.254 -730.254] [166.848], Avg: [-614.314 -614.314 -614.314] (0.2856) ({r_i: None, r_t: [-1306.689 -1306.689 -1306.689], eps: 0.286})
Step:  135400, Reward: [-1006.722 -1006.722 -1006.722] [473.380], Avg: [-708.141 -708.141 -708.141] (0.1000) ({r_i: None, r_t: [-1833.531 -1833.531 -1833.531], eps: 0.1})
Step:  101900, Reward: [-419.418 -419.418 -419.418] [85.129], Avg: [-813.635 -813.635 -813.635] (0.0100) ({r_i: None, r_t: [-862.258 -862.258 -862.258], eps: 0.01})
Step:   12600, Reward: [-673.673 -673.673 -673.673] [206.319], Avg: [-614.782 -614.782 -614.782] (0.2828) ({r_i: None, r_t: [-1360.354 -1360.354 -1360.354], eps: 0.283})
Step:  135500, Reward: [-906.726 -906.726 -906.726] [342.499], Avg: [-708.287 -708.287 -708.287] (0.1000) ({r_i: None, r_t: [-1765.106 -1765.106 -1765.106], eps: 0.1})
Step:  102000, Reward: [-419.730 -419.730 -419.730] [103.344], Avg: [-813.250 -813.250 -813.250] (0.0100) ({r_i: None, r_t: [-903.377 -903.377 -903.377], eps: 0.01})
Step:   12700, Reward: [-704.175 -704.175 -704.175] [244.388], Avg: [-615.480 -615.480 -615.480] (0.2799) ({r_i: None, r_t: [-1416.272 -1416.272 -1416.272], eps: 0.28})
Step:  135600, Reward: [-897.261 -897.261 -897.261] [411.449], Avg: [-708.427 -708.427 -708.427] (0.1000) ({r_i: None, r_t: [-1801.612 -1801.612 -1801.612], eps: 0.1})
Step:  102100, Reward: [-454.536 -454.536 -454.536] [93.982], Avg: [-812.899 -812.899 -812.899] (0.0100) ({r_i: None, r_t: [-820.825 -820.825 -820.825], eps: 0.01})
Step:   12800, Reward: [-746.946 -746.946 -746.946] [270.156], Avg: [-616.499 -616.499 -616.499] (0.2771) ({r_i: None, r_t: [-1452.163 -1452.163 -1452.163], eps: 0.277})
Step:  135700, Reward: [-791.977 -791.977 -791.977] [317.733], Avg: [-708.488 -708.488 -708.488] (0.1000) ({r_i: None, r_t: [-1702.829 -1702.829 -1702.829], eps: 0.1})
Step:  102200, Reward: [-477.575 -477.575 -477.575] [101.057], Avg: [-812.571 -812.571 -812.571] (0.0100) ({r_i: None, r_t: [-863.544 -863.544 -863.544], eps: 0.01})
Step:   12900, Reward: [-687.189 -687.189 -687.189] [184.273], Avg: [-617.043 -617.043 -617.043] (0.2744) ({r_i: None, r_t: [-1512.096 -1512.096 -1512.096], eps: 0.274})
Step:  135800, Reward: [-825.947 -825.947 -825.947] [354.232], Avg: [-708.575 -708.575 -708.575] (0.1000) ({r_i: None, r_t: [-1602.011 -1602.011 -1602.011], eps: 0.1})
Step:  102300, Reward: [-433.506 -433.506 -433.506] [62.878], Avg: [-812.201 -812.201 -812.201] (0.0100) ({r_i: None, r_t: [-872.881 -872.881 -872.881], eps: 0.01})
Step:   13000, Reward: [-588.367 -588.367 -588.367] [144.321], Avg: [-616.824 -616.824 -616.824] (0.2716) ({r_i: None, r_t: [-1406.771 -1406.771 -1406.771], eps: 0.272})
Step:  135900, Reward: [-832.701 -832.701 -832.701] [327.865], Avg: [-708.666 -708.666 -708.666] (0.1000) ({r_i: None, r_t: [-1673.711 -1673.711 -1673.711], eps: 0.1})
Step:  102400, Reward: [-426.308 -426.308 -426.308] [92.611], Avg: [-811.824 -811.824 -811.824] (0.0100) ({r_i: None, r_t: [-904.329 -904.329 -904.329], eps: 0.01})
Step:  136000, Reward: [-929.126 -929.126 -929.126] [475.027], Avg: [-708.828 -708.828 -708.828] (0.1000) ({r_i: None, r_t: [-1767.413 -1767.413 -1767.413], eps: 0.1})
Step:   13100, Reward: [-731.352 -731.352 -731.352] [226.425], Avg: [-617.692 -617.692 -617.692] (0.2689) ({r_i: None, r_t: [-1368.443 -1368.443 -1368.443], eps: 0.269})
Step:  102500, Reward: [-440.235 -440.235 -440.235] [100.512], Avg: [-811.462 -811.462 -811.462] (0.0100) ({r_i: None, r_t: [-858.160 -858.160 -858.160], eps: 0.01})
Step:  136100, Reward: [-755.169 -755.169 -755.169] [223.200], Avg: [-708.862 -708.862 -708.862] (0.1000) ({r_i: None, r_t: [-1810.637 -1810.637 -1810.637], eps: 0.1})
Step:   13200, Reward: [-777.132 -777.132 -777.132] [338.653], Avg: [-618.891 -618.891 -618.891] (0.2663) ({r_i: None, r_t: [-1470.253 -1470.253 -1470.253], eps: 0.266})
Step:  102600, Reward: [-432.617 -432.617 -432.617] [103.220], Avg: [-811.093 -811.093 -811.093] (0.0100) ({r_i: None, r_t: [-813.779 -813.779 -813.779], eps: 0.01})
Step:  136200, Reward: [-790.560 -790.560 -790.560] [232.143], Avg: [-708.922 -708.922 -708.922] (0.1000) ({r_i: None, r_t: [-1466.478 -1466.478 -1466.478], eps: 0.1})
Step:   13300, Reward: [-735.470 -735.470 -735.470] [193.560], Avg: [-619.761 -619.761 -619.761] (0.2636) ({r_i: None, r_t: [-1329.684 -1329.684 -1329.684], eps: 0.264})
Step:  102700, Reward: [-490.485 -490.485 -490.485] [161.878], Avg: [-810.781 -810.781 -810.781] (0.0100) ({r_i: None, r_t: [-839.213 -839.213 -839.213], eps: 0.01})
Step:  136300, Reward: [-856.195 -856.195 -856.195] [255.303], Avg: [-709.030 -709.030 -709.030] (0.1000) ({r_i: None, r_t: [-1507.622 -1507.622 -1507.622], eps: 0.1})
Step:   13400, Reward: [-771.416 -771.416 -771.416] [320.856], Avg: [-620.884 -620.884 -620.884] (0.2610) ({r_i: None, r_t: [-1379.320 -1379.320 -1379.320], eps: 0.261})
Step:  102800, Reward: [-403.068 -403.068 -403.068] [62.401], Avg: [-810.385 -810.385 -810.385] (0.0100) ({r_i: None, r_t: [-886.501 -886.501 -886.501], eps: 0.01})
Step:  136400, Reward: [-941.927 -941.927 -941.927] [476.122], Avg: [-709.200 -709.200 -709.200] (0.1000) ({r_i: None, r_t: [-1530.884 -1530.884 -1530.884], eps: 0.1})
Step:   13500, Reward: [-663.273 -663.273 -663.273] [157.743], Avg: [-621.196 -621.196 -621.196] (0.2584) ({r_i: None, r_t: [-1495.404 -1495.404 -1495.404], eps: 0.258})
Step:  102900, Reward: [-427.200 -427.200 -427.200] [54.295], Avg: [-810.013 -810.013 -810.013] (0.0100) ({r_i: None, r_t: [-893.129 -893.129 -893.129], eps: 0.01})
Step:  136500, Reward: [-782.491 -782.491 -782.491] [276.750], Avg: [-709.254 -709.254 -709.254] (0.1000) ({r_i: None, r_t: [-1653.217 -1653.217 -1653.217], eps: 0.1})
Step:   13600, Reward: [-671.950 -671.950 -671.950] [182.212], Avg: [-621.566 -621.566 -621.566] (0.2558) ({r_i: None, r_t: [-1487.624 -1487.624 -1487.624], eps: 0.256})
Step:  103000, Reward: [-435.936 -435.936 -435.936] [112.472], Avg: [-809.650 -809.650 -809.650] (0.0100) ({r_i: None, r_t: [-867.357 -867.357 -867.357], eps: 0.01})
Step:  136600, Reward: [-826.684 -826.684 -826.684] [315.382], Avg: [-709.340 -709.340 -709.340] (0.1000) ({r_i: None, r_t: [-1466.443 -1466.443 -1466.443], eps: 0.1})
Step:   13700, Reward: [-770.488 -770.488 -770.488] [227.440], Avg: [-622.645 -622.645 -622.645] (0.2532) ({r_i: None, r_t: [-1447.002 -1447.002 -1447.002], eps: 0.253})
Step:  103100, Reward: [-425.927 -425.927 -425.927] [100.392], Avg: [-809.278 -809.278 -809.278] (0.0100) ({r_i: None, r_t: [-885.344 -885.344 -885.344], eps: 0.01})
Step:  136700, Reward: [-849.459 -849.459 -849.459] [445.920], Avg: [-709.442 -709.442 -709.442] (0.1000) ({r_i: None, r_t: [-1711.939 -1711.939 -1711.939], eps: 0.1})
Step:   13800, Reward: [-750.625 -750.625 -750.625] [217.420], Avg: [-623.566 -623.566 -623.566] (0.2507) ({r_i: None, r_t: [-1520.468 -1520.468 -1520.468], eps: 0.251})
Step:  103200, Reward: [-429.668 -429.668 -429.668] [96.245], Avg: [-808.911 -808.911 -808.911] (0.0100) ({r_i: None, r_t: [-876.302 -876.302 -876.302], eps: 0.01})
Step:  136800, Reward: [-693.599 -693.599 -693.599] [270.509], Avg: [-709.431 -709.431 -709.431] (0.1000) ({r_i: None, r_t: [-1700.417 -1700.417 -1700.417], eps: 0.1})
Step:   13900, Reward: [-771.267 -771.267 -771.267] [270.489], Avg: [-624.621 -624.621 -624.621] (0.2482) ({r_i: None, r_t: [-1543.357 -1543.357 -1543.357], eps: 0.248})
Step:  136900, Reward: [-878.005 -878.005 -878.005] [375.033], Avg: [-709.554 -709.554 -709.554] (0.1000) ({r_i: None, r_t: [-1735.841 -1735.841 -1735.841], eps: 0.1})
Step:  103300, Reward: [-440.109 -440.109 -440.109] [87.606], Avg: [-808.554 -808.554 -808.554] (0.0100) ({r_i: None, r_t: [-878.790 -878.790 -878.790], eps: 0.01})
Step:   14000, Reward: [-856.893 -856.893 -856.893] [223.977], Avg: [-626.268 -626.268 -626.268] (0.2457) ({r_i: None, r_t: [-1462.582 -1462.582 -1462.582], eps: 0.246})
Step:  137000, Reward: [-789.796 -789.796 -789.796] [343.130], Avg: [-709.612 -709.612 -709.612] (0.1000) ({r_i: None, r_t: [-1433.348 -1433.348 -1433.348], eps: 0.1})
Step:  103400, Reward: [-446.203 -446.203 -446.203] [77.228], Avg: [-808.204 -808.204 -808.204] (0.0100) ({r_i: None, r_t: [-853.675 -853.675 -853.675], eps: 0.01})
Step:   14100, Reward: [-880.791 -880.791 -880.791] [229.968], Avg: [-628.061 -628.061 -628.061] (0.2433) ({r_i: None, r_t: [-1585.994 -1585.994 -1585.994], eps: 0.243})
Step:  137100, Reward: [-763.687 -763.687 -763.687] [376.066], Avg: [-709.652 -709.652 -709.652] (0.1000) ({r_i: None, r_t: [-1440.965 -1440.965 -1440.965], eps: 0.1})
Step:  103500, Reward: [-470.785 -470.785 -470.785] [125.742], Avg: [-807.878 -807.878 -807.878] (0.0100) ({r_i: None, r_t: [-923.190 -923.190 -923.190], eps: 0.01})
Step:   14200, Reward: [-883.377 -883.377 -883.377] [366.231], Avg: [-629.846 -629.846 -629.846] (0.2409) ({r_i: None, r_t: [-1726.641 -1726.641 -1726.641], eps: 0.241})
Step:  137200, Reward: [-770.667 -770.667 -770.667] [248.635], Avg: [-709.696 -709.696 -709.696] (0.1000) ({r_i: None, r_t: [-1350.117 -1350.117 -1350.117], eps: 0.1})
Step:  103600, Reward: [-488.980 -488.980 -488.980] [95.548], Avg: [-807.571 -807.571 -807.571] (0.0100) ({r_i: None, r_t: [-874.010 -874.010 -874.010], eps: 0.01})
Step:   14300, Reward: [-929.820 -929.820 -929.820] [300.423], Avg: [-631.929 -631.929 -631.929] (0.2385) ({r_i: None, r_t: [-1746.872 -1746.872 -1746.872], eps: 0.238})
Step:  137300, Reward: [-834.012 -834.012 -834.012] [329.295], Avg: [-709.787 -709.787 -709.787] (0.1000) ({r_i: None, r_t: [-1562.102 -1562.102 -1562.102], eps: 0.1})
Step:  103700, Reward: [-445.372 -445.372 -445.372] [88.745], Avg: [-807.222 -807.222 -807.222] (0.0100) ({r_i: None, r_t: [-850.071 -850.071 -850.071], eps: 0.01})
Step:   14400, Reward: [-1005.464 -1005.464 -1005.464] [370.793], Avg: [-634.505 -634.505 -634.505] (0.2361) ({r_i: None, r_t: [-1682.472 -1682.472 -1682.472], eps: 0.236})
Step:  137400, Reward: [-972.421 -972.421 -972.421] [557.180], Avg: [-709.978 -709.978 -709.978] (0.1000) ({r_i: None, r_t: [-1587.103 -1587.103 -1587.103], eps: 0.1})
Step:  103800, Reward: [-416.080 -416.080 -416.080] [88.486], Avg: [-806.846 -806.846 -806.846] (0.0100) ({r_i: None, r_t: [-898.029 -898.029 -898.029], eps: 0.01})
Step:   14500, Reward: [-963.478 -963.478 -963.478] [315.481], Avg: [-636.759 -636.759 -636.759] (0.2337) ({r_i: None, r_t: [-2147.006 -2147.006 -2147.006], eps: 0.234})
Step:  137500, Reward: [-773.080 -773.080 -773.080] [335.652], Avg: [-710.024 -710.024 -710.024] (0.1000) ({r_i: None, r_t: [-1582.973 -1582.973 -1582.973], eps: 0.1})
Step:  103900, Reward: [-412.528 -412.528 -412.528] [64.008], Avg: [-806.466 -806.466 -806.466] (0.0100) ({r_i: None, r_t: [-877.966 -877.966 -877.966], eps: 0.01})
Step:   14600, Reward: [-1082.198 -1082.198 -1082.198] [341.540], Avg: [-639.789 -639.789 -639.789] (0.2314) ({r_i: None, r_t: [-2123.864 -2123.864 -2123.864], eps: 0.231})
Step:  137600, Reward: [-736.349 -736.349 -736.349] [180.027], Avg: [-710.043 -710.043 -710.043] (0.1000) ({r_i: None, r_t: [-1594.891 -1594.891 -1594.891], eps: 0.1})
Step:  104000, Reward: [-417.445 -417.445 -417.445] [103.764], Avg: [-806.093 -806.093 -806.093] (0.0100) ({r_i: None, r_t: [-856.167 -856.167 -856.167], eps: 0.01})
Step:   14700, Reward: [-1018.047 -1018.047 -1018.047] [271.863], Avg: [-642.345 -642.345 -642.345] (0.2291) ({r_i: None, r_t: [-1973.564 -1973.564 -1973.564], eps: 0.229})
Step:  137700, Reward: [-753.235 -753.235 -753.235] [316.822], Avg: [-710.074 -710.074 -710.074] (0.1000) ({r_i: None, r_t: [-1628.846 -1628.846 -1628.846], eps: 0.1})
Step:  104100, Reward: [-439.002 -439.002 -439.002] [66.315], Avg: [-805.740 -805.740 -805.740] (0.0100) ({r_i: None, r_t: [-846.710 -846.710 -846.710], eps: 0.01})
Step:   14800, Reward: [-945.391 -945.391 -945.391] [319.690], Avg: [-644.378 -644.378 -644.378] (0.2268) ({r_i: None, r_t: [-2175.861 -2175.861 -2175.861], eps: 0.227})
Step:  137800, Reward: [-677.528 -677.528 -677.528] [178.227], Avg: [-710.051 -710.051 -710.051] (0.1000) ({r_i: None, r_t: [-1717.140 -1717.140 -1717.140], eps: 0.1})
Step:  104200, Reward: [-460.972 -460.972 -460.972] [78.667], Avg: [-805.410 -805.410 -805.410] (0.0100) ({r_i: None, r_t: [-911.881 -911.881 -911.881], eps: 0.01})
Step:   14900, Reward: [-1064.574 -1064.574 -1064.574] [385.859], Avg: [-647.180 -647.180 -647.180] (0.2245) ({r_i: None, r_t: [-2409.950 -2409.950 -2409.950], eps: 0.225})
Step:  137900, Reward: [-847.939 -847.939 -847.939] [376.988], Avg: [-710.150 -710.150 -710.150] (0.1000) ({r_i: None, r_t: [-1468.860 -1468.860 -1468.860], eps: 0.1})
Step:  104300, Reward: [-445.159 -445.159 -445.159] [108.499], Avg: [-805.065 -805.065 -805.065] (0.0100) ({r_i: None, r_t: [-849.709 -849.709 -849.709], eps: 0.01})
Step:  138000, Reward: [-822.008 -822.008 -822.008] [372.838], Avg: [-710.231 -710.231 -710.231] (0.1000) ({r_i: None, r_t: [-1521.152 -1521.152 -1521.152], eps: 0.1})
Step:   15000, Reward: [-996.260 -996.260 -996.260] [291.126], Avg: [-649.492 -649.492 -649.492] (0.2223) ({r_i: None, r_t: [-1956.148 -1956.148 -1956.148], eps: 0.222})
Step:  104400, Reward: [-392.651 -392.651 -392.651] [82.592], Avg: [-804.670 -804.670 -804.670] (0.0100) ({r_i: None, r_t: [-861.137 -861.137 -861.137], eps: 0.01})
Step:  138100, Reward: [-833.394 -833.394 -833.394] [260.554], Avg: [-710.321 -710.321 -710.321] (0.1000) ({r_i: None, r_t: [-1459.954 -1459.954 -1459.954], eps: 0.1})
Step:   15100, Reward: [-904.156 -904.156 -904.156] [224.806], Avg: [-651.167 -651.167 -651.167] (0.2201) ({r_i: None, r_t: [-2161.893 -2161.893 -2161.893], eps: 0.22})
Step:  104500, Reward: [-477.321 -477.321 -477.321] [105.603], Avg: [-804.357 -804.357 -804.357] (0.0100) ({r_i: None, r_t: [-837.272 -837.272 -837.272], eps: 0.01})
Step:  138200, Reward: [-742.132 -742.132 -742.132] [394.738], Avg: [-710.344 -710.344 -710.344] (0.1000) ({r_i: None, r_t: [-1401.735 -1401.735 -1401.735], eps: 0.1})
Step:   15200, Reward: [-1042.640 -1042.640 -1042.640] [262.139], Avg: [-653.726 -653.726 -653.726] (0.2179) ({r_i: None, r_t: [-2459.029 -2459.029 -2459.029], eps: 0.218})
Step:  104600, Reward: [-446.800 -446.800 -446.800] [103.153], Avg: [-804.016 -804.016 -804.016] (0.0100) ({r_i: None, r_t: [-892.098 -892.098 -892.098], eps: 0.01})
Step:  138300, Reward: [-859.249 -859.249 -859.249] [461.247], Avg: [-710.451 -710.451 -710.451] (0.1000) ({r_i: None, r_t: [-1444.528 -1444.528 -1444.528], eps: 0.1})
Step:   15300, Reward: [-1087.663 -1087.663 -1087.663] [380.584], Avg: [-656.543 -656.543 -656.543] (0.2157) ({r_i: None, r_t: [-2044.376 -2044.376 -2044.376], eps: 0.216})
Step:  104700, Reward: [-402.446 -402.446 -402.446] [71.193], Avg: [-803.632 -803.632 -803.632] (0.0100) ({r_i: None, r_t: [-842.917 -842.917 -842.917], eps: 0.01})
Step:  138400, Reward: [-659.358 -659.358 -659.358] [194.603], Avg: [-710.414 -710.414 -710.414] (0.1000) ({r_i: None, r_t: [-1388.906 -1388.906 -1388.906], eps: 0.1})
Step:   15400, Reward: [-1030.213 -1030.213 -1030.213] [425.749], Avg: [-658.954 -658.954 -658.954] (0.2136) ({r_i: None, r_t: [-2117.606 -2117.606 -2117.606], eps: 0.214})
Step:  104800, Reward: [-431.224 -431.224 -431.224] [79.712], Avg: [-803.277 -803.277 -803.277] (0.0100) ({r_i: None, r_t: [-835.779 -835.779 -835.779], eps: 0.01})
Step:  138500, Reward: [-731.321 -731.321 -731.321] [308.027], Avg: [-710.429 -710.429 -710.429] (0.1000) ({r_i: None, r_t: [-1454.455 -1454.455 -1454.455], eps: 0.1})
Step:   15500, Reward: [-931.931 -931.931 -931.931] [323.982], Avg: [-660.704 -660.704 -660.704] (0.2114) ({r_i: None, r_t: [-2188.228 -2188.228 -2188.228], eps: 0.211})
Step:  104900, Reward: [-456.149 -456.149 -456.149] [70.531], Avg: [-802.947 -802.947 -802.947] (0.0100) ({r_i: None, r_t: [-881.378 -881.378 -881.378], eps: 0.01})
Step:  138600, Reward: [-825.957 -825.957 -825.957] [358.094], Avg: [-710.513 -710.513 -710.513] (0.1000) ({r_i: None, r_t: [-1582.537 -1582.537 -1582.537], eps: 0.1})
Step:   15600, Reward: [-1017.209 -1017.209 -1017.209] [359.335], Avg: [-662.975 -662.975 -662.975] (0.2093) ({r_i: None, r_t: [-2067.858 -2067.858 -2067.858], eps: 0.209})
Step:  105000, Reward: [-435.501 -435.501 -435.501] [74.480], Avg: [-802.597 -802.597 -802.597] (0.0100) ({r_i: None, r_t: [-819.696 -819.696 -819.696], eps: 0.01})
Step:  138700, Reward: [-694.867 -694.867 -694.867] [212.765], Avg: [-710.501 -710.501 -710.501] (0.1000) ({r_i: None, r_t: [-1570.614 -1570.614 -1570.614], eps: 0.1})
Step:   15700, Reward: [-800.395 -800.395 -800.395] [271.843], Avg: [-663.845 -663.845 -663.845] (0.2072) ({r_i: None, r_t: [-2219.351 -2219.351 -2219.351], eps: 0.207})
Step:  138800, Reward: [-773.470 -773.470 -773.470] [298.670], Avg: [-710.547 -710.547 -710.547] (0.1000) ({r_i: None, r_t: [-1412.339 -1412.339 -1412.339], eps: 0.1})
Step:  105100, Reward: [-415.413 -415.413 -415.413] [65.800], Avg: [-802.229 -802.229 -802.229] (0.0100) ({r_i: None, r_t: [-870.681 -870.681 -870.681], eps: 0.01})
Step:   15800, Reward: [-1074.329 -1074.329 -1074.329] [345.074], Avg: [-666.426 -666.426 -666.426] (0.2052) ({r_i: None, r_t: [-1833.175 -1833.175 -1833.175], eps: 0.205})
Step:  138900, Reward: [-762.358 -762.358 -762.358] [367.117], Avg: [-710.584 -710.584 -710.584] (0.1000) ({r_i: None, r_t: [-1308.304 -1308.304 -1308.304], eps: 0.1})
Step:  105200, Reward: [-436.516 -436.516 -436.516] [72.780], Avg: [-801.882 -801.882 -801.882] (0.0100) ({r_i: None, r_t: [-888.302 -888.302 -888.302], eps: 0.01})
Step:   15900, Reward: [-934.222 -934.222 -934.222] [363.887], Avg: [-668.100 -668.100 -668.100] (0.2031) ({r_i: None, r_t: [-1870.577 -1870.577 -1870.577], eps: 0.203})
Step:  139000, Reward: [-610.504 -610.504 -610.504] [197.022], Avg: [-710.512 -710.512 -710.512] (0.1000) ({r_i: None, r_t: [-1465.183 -1465.183 -1465.183], eps: 0.1})
Step:  105300, Reward: [-419.764 -419.764 -419.764] [92.801], Avg: [-801.519 -801.519 -801.519] (0.0100) ({r_i: None, r_t: [-900.624 -900.624 -900.624], eps: 0.01})
Step:   16000, Reward: [-927.942 -927.942 -927.942] [276.274], Avg: [-669.714 -669.714 -669.714] (0.2011) ({r_i: None, r_t: [-1983.868 -1983.868 -1983.868], eps: 0.201})
Step:  139100, Reward: [-819.918 -819.918 -819.918] [343.703], Avg: [-710.591 -710.591 -710.591] (0.1000) ({r_i: None, r_t: [-1306.087 -1306.087 -1306.087], eps: 0.1})
Step:  105400, Reward: [-433.929 -433.929 -433.929] [79.039], Avg: [-801.171 -801.171 -801.171] (0.0100) ({r_i: None, r_t: [-903.163 -903.163 -903.163], eps: 0.01})
Step:   16100, Reward: [-953.533 -953.533 -953.533] [303.639], Avg: [-671.466 -671.466 -671.466] (0.1991) ({r_i: None, r_t: [-1864.511 -1864.511 -1864.511], eps: 0.199})
Step:  139200, Reward: [-650.380 -650.380 -650.380] [161.643], Avg: [-710.547 -710.547 -710.547] (0.1000) ({r_i: None, r_t: [-1305.988 -1305.988 -1305.988], eps: 0.1})
Step:  105500, Reward: [-438.983 -438.983 -438.983] [75.828], Avg: [-800.828 -800.828 -800.828] (0.0100) ({r_i: None, r_t: [-814.327 -814.327 -814.327], eps: 0.01})
Step:   16200, Reward: [-861.031 -861.031 -861.031] [246.259], Avg: [-672.629 -672.629 -672.629] (0.1971) ({r_i: None, r_t: [-1894.825 -1894.825 -1894.825], eps: 0.197})
Step:  139300, Reward: [-771.475 -771.475 -771.475] [311.606], Avg: [-710.591 -710.591 -710.591] (0.1000) ({r_i: None, r_t: [-1242.091 -1242.091 -1242.091], eps: 0.1})
Step:  105600, Reward: [-441.688 -441.688 -441.688] [80.231], Avg: [-800.488 -800.488 -800.488] (0.0100) ({r_i: None, r_t: [-914.749 -914.749 -914.749], eps: 0.01})
Step:   16300, Reward: [-919.330 -919.330 -919.330] [357.116], Avg: [-674.133 -674.133 -674.133] (0.1951) ({r_i: None, r_t: [-1781.907 -1781.907 -1781.907], eps: 0.195})
Step:  139400, Reward: [-651.043 -651.043 -651.043] [157.608], Avg: [-710.548 -710.548 -710.548] (0.1000) ({r_i: None, r_t: [-1458.102 -1458.102 -1458.102], eps: 0.1})
Step:  105700, Reward: [-397.783 -397.783 -397.783] [57.490], Avg: [-800.108 -800.108 -800.108] (0.0100) ({r_i: None, r_t: [-917.830 -917.830 -917.830], eps: 0.01})
Step:   16400, Reward: [-804.260 -804.260 -804.260] [370.060], Avg: [-674.922 -674.922 -674.922] (0.1932) ({r_i: None, r_t: [-1930.587 -1930.587 -1930.587], eps: 0.193})
Step:  139500, Reward: [-758.907 -758.907 -758.907] [353.118], Avg: [-710.583 -710.583 -710.583] (0.1000) ({r_i: None, r_t: [-1453.166 -1453.166 -1453.166], eps: 0.1})
Step:  105800, Reward: [-437.588 -437.588 -437.588] [84.861], Avg: [-799.765 -799.765 -799.765] (0.0100) ({r_i: None, r_t: [-795.698 -795.698 -795.698], eps: 0.01})
Step:   16500, Reward: [-881.352 -881.352 -881.352] [357.385], Avg: [-676.165 -676.165 -676.165] (0.1913) ({r_i: None, r_t: [-1687.785 -1687.785 -1687.785], eps: 0.191})
Step:  139600, Reward: [-642.988 -642.988 -642.988] [209.376], Avg: [-710.535 -710.535 -710.535] (0.1000) ({r_i: None, r_t: [-1291.882 -1291.882 -1291.882], eps: 0.1})
Step:  105900, Reward: [-447.636 -447.636 -447.636] [123.304], Avg: [-799.433 -799.433 -799.433] (0.0100) ({r_i: None, r_t: [-850.143 -850.143 -850.143], eps: 0.01})
Step:   16600, Reward: [-836.807 -836.807 -836.807] [346.776], Avg: [-677.127 -677.127 -677.127] (0.1893) ({r_i: None, r_t: [-1680.304 -1680.304 -1680.304], eps: 0.189})
Step:  139700, Reward: [-596.401 -596.401 -596.401] [196.088], Avg: [-710.453 -710.453 -710.453] (0.1000) ({r_i: None, r_t: [-1296.628 -1296.628 -1296.628], eps: 0.1})
Step:  106000, Reward: [-392.690 -392.690 -392.690] [83.364], Avg: [-799.050 -799.050 -799.050] (0.0100) ({r_i: None, r_t: [-832.103 -832.103 -832.103], eps: 0.01})
Step:   16700, Reward: [-877.991 -877.991 -877.991] [368.257], Avg: [-678.323 -678.323 -678.323] (0.1875) ({r_i: None, r_t: [-1706.533 -1706.533 -1706.533], eps: 0.187})
Step:  139800, Reward: [-772.669 -772.669 -772.669] [287.182], Avg: [-710.498 -710.498 -710.498] (0.1000) ({r_i: None, r_t: [-1525.303 -1525.303 -1525.303], eps: 0.1})
Step:  106100, Reward: [-434.324 -434.324 -434.324] [119.828], Avg: [-798.706 -798.706 -798.706] (0.0100) ({r_i: None, r_t: [-856.158 -856.158 -856.158], eps: 0.01})
Step:  139900, Reward: [-670.340 -670.340 -670.340] [313.382], Avg: [-710.469 -710.469 -710.469] (0.1000) ({r_i: None, r_t: [-1385.190 -1385.190 -1385.190], eps: 0.1})
Step:   16800, Reward: [-784.759 -784.759 -784.759] [299.819], Avg: [-678.953 -678.953 -678.953] (0.1856) ({r_i: None, r_t: [-1651.938 -1651.938 -1651.938], eps: 0.186})
Step:  106200, Reward: [-445.714 -445.714 -445.714] [73.418], Avg: [-798.374 -798.374 -798.374] (0.0100) ({r_i: None, r_t: [-862.153 -862.153 -862.153], eps: 0.01})
Step:  140000, Reward: [-546.403 -546.403 -546.403] [98.537], Avg: [-710.352 -710.352 -710.352] (0.1000) ({r_i: None, r_t: [-1301.946 -1301.946 -1301.946], eps: 0.1})
Step:   16900, Reward: [-740.993 -740.993 -740.993] [198.505], Avg: [-679.318 -679.318 -679.318] (0.1837) ({r_i: None, r_t: [-1519.469 -1519.469 -1519.469], eps: 0.184})
Step:  106300, Reward: [-467.414 -467.414 -467.414] [101.141], Avg: [-798.063 -798.063 -798.063] (0.0100) ({r_i: None, r_t: [-892.216 -892.216 -892.216], eps: 0.01})
Step:  140100, Reward: [-639.711 -639.711 -639.711] [204.305], Avg: [-710.301 -710.301 -710.301] (0.1000) ({r_i: None, r_t: [-1554.132 -1554.132 -1554.132], eps: 0.1})
Step:   17000, Reward: [-813.906 -813.906 -813.906] [237.757], Avg: [-680.105 -680.105 -680.105] (0.1819) ({r_i: None, r_t: [-1530.516 -1530.516 -1530.516], eps: 0.182})
Step:  106400, Reward: [-466.739 -466.739 -466.739] [52.882], Avg: [-797.752 -797.752 -797.752] (0.0100) ({r_i: None, r_t: [-897.886 -897.886 -897.886], eps: 0.01})
Step:  140200, Reward: [-690.300 -690.300 -690.300] [216.176], Avg: [-710.287 -710.287 -710.287] (0.1000) ({r_i: None, r_t: [-1433.108 -1433.108 -1433.108], eps: 0.1})
Step:   17100, Reward: [-843.142 -843.142 -843.142] [353.018], Avg: [-681.053 -681.053 -681.053] (0.1801) ({r_i: None, r_t: [-1474.036 -1474.036 -1474.036], eps: 0.18})
Step:  106500, Reward: [-416.349 -416.349 -416.349] [94.548], Avg: [-797.394 -797.394 -797.394] (0.0100) ({r_i: None, r_t: [-876.732 -876.732 -876.732], eps: 0.01})
Step:  140300, Reward: [-685.057 -685.057 -685.057] [287.297], Avg: [-710.269 -710.269 -710.269] (0.1000) ({r_i: None, r_t: [-1301.984 -1301.984 -1301.984], eps: 0.1})
Step:   17200, Reward: [-896.246 -896.246 -896.246] [286.872], Avg: [-682.296 -682.296 -682.296] (0.1783) ({r_i: None, r_t: [-1656.795 -1656.795 -1656.795], eps: 0.178})
Step:  140400, Reward: [-619.693 -619.693 -619.693] [124.910], Avg: [-710.205 -710.205 -710.205] (0.1000) ({r_i: None, r_t: [-1329.756 -1329.756 -1329.756], eps: 0.1})
Step:  106600, Reward: [-405.925 -405.925 -405.925] [85.938], Avg: [-797.027 -797.027 -797.027] (0.0100) ({r_i: None, r_t: [-833.378 -833.378 -833.378], eps: 0.01})
Step:   17300, Reward: [-783.975 -783.975 -783.975] [251.286], Avg: [-682.881 -682.881 -682.881] (0.1765) ({r_i: None, r_t: [-1850.965 -1850.965 -1850.965], eps: 0.177})
Step:  140500, Reward: [-677.837 -677.837 -677.837] [252.752], Avg: [-710.182 -710.182 -710.182] (0.1000) ({r_i: None, r_t: [-1284.220 -1284.220 -1284.220], eps: 0.1})
Step:  106700, Reward: [-410.867 -410.867 -410.867] [65.907], Avg: [-796.666 -796.666 -796.666] (0.0100) ({r_i: None, r_t: [-895.289 -895.289 -895.289], eps: 0.01})
Step:   17400, Reward: [-948.125 -948.125 -948.125] [398.623], Avg: [-684.396 -684.396 -684.396] (0.1748) ({r_i: None, r_t: [-1478.578 -1478.578 -1478.578], eps: 0.175})
Step:  140600, Reward: [-724.034 -724.034 -724.034] [337.548], Avg: [-710.191 -710.191 -710.191] (0.1000) ({r_i: None, r_t: [-1236.251 -1236.251 -1236.251], eps: 0.1})
Step:  106800, Reward: [-401.836 -401.836 -401.836] [79.708], Avg: [-796.296 -796.296 -796.296] (0.0100) ({r_i: None, r_t: [-918.314 -918.314 -918.314], eps: 0.01})
Step:   17500, Reward: [-790.723 -790.723 -790.723] [324.137], Avg: [-685.001 -685.001 -685.001] (0.1730) ({r_i: None, r_t: [-1691.111 -1691.111 -1691.111], eps: 0.173})
Step:  140700, Reward: [-662.573 -662.573 -662.573] [178.009], Avg: [-710.158 -710.158 -710.158] (0.1000) ({r_i: None, r_t: [-1361.162 -1361.162 -1361.162], eps: 0.1})
Step:  106900, Reward: [-468.251 -468.251 -468.251] [115.160], Avg: [-795.990 -795.990 -795.990] (0.0100) ({r_i: None, r_t: [-860.105 -860.105 -860.105], eps: 0.01})
Step:   17600, Reward: [-951.079 -951.079 -951.079] [310.607], Avg: [-686.504 -686.504 -686.504] (0.1713) ({r_i: None, r_t: [-1813.137 -1813.137 -1813.137], eps: 0.171})
Step:  140800, Reward: [-633.030 -633.030 -633.030] [151.362], Avg: [-710.103 -710.103 -710.103] (0.1000) ({r_i: None, r_t: [-1269.302 -1269.302 -1269.302], eps: 0.1})
Step:  107000, Reward: [-431.664 -431.664 -431.664] [101.914], Avg: [-795.650 -795.650 -795.650] (0.0100) ({r_i: None, r_t: [-866.085 -866.085 -866.085], eps: 0.01})
Step:   17700, Reward: [-811.087 -811.087 -811.087] [311.169], Avg: [-687.204 -687.204 -687.204] (0.1696) ({r_i: None, r_t: [-1675.348 -1675.348 -1675.348], eps: 0.17})
Step:  140900, Reward: [-788.984 -788.984 -788.984] [353.355], Avg: [-710.159 -710.159 -710.159] (0.1000) ({r_i: None, r_t: [-1288.427 -1288.427 -1288.427], eps: 0.1})
Step:  107100, Reward: [-435.913 -435.913 -435.913] [106.348], Avg: [-795.314 -795.314 -795.314] (0.0100) ({r_i: None, r_t: [-839.445 -839.445 -839.445], eps: 0.01})
Step:   17800, Reward: [-908.160 -908.160 -908.160] [377.877], Avg: [-688.438 -688.438 -688.438] (0.1679) ({r_i: None, r_t: [-1649.110 -1649.110 -1649.110], eps: 0.168})
Step:  141000, Reward: [-758.099 -758.099 -758.099] [349.578], Avg: [-710.193 -710.193 -710.193] (0.1000) ({r_i: None, r_t: [-1336.628 -1336.628 -1336.628], eps: 0.1})
Step:  107200, Reward: [-403.338 -403.338 -403.338] [55.618], Avg: [-794.949 -794.949 -794.949] (0.0100) ({r_i: None, r_t: [-850.108 -850.108 -850.108], eps: 0.01})
Step:   17900, Reward: [-765.371 -765.371 -765.371] [288.895], Avg: [-688.866 -688.866 -688.866] (0.1662) ({r_i: None, r_t: [-1799.544 -1799.544 -1799.544], eps: 0.166})
Step:  141100, Reward: [-588.150 -588.150 -588.150] [138.507], Avg: [-710.106 -710.106 -710.106] (0.1000) ({r_i: None, r_t: [-1198.275 -1198.275 -1198.275], eps: 0.1})
Step:  107300, Reward: [-438.386 -438.386 -438.386] [78.201], Avg: [-794.617 -794.617 -794.617] (0.0100) ({r_i: None, r_t: [-828.829 -828.829 -828.829], eps: 0.01})
Step:   18000, Reward: [-887.291 -887.291 -887.291] [310.906], Avg: [-689.962 -689.962 -689.962] (0.1646) ({r_i: None, r_t: [-1809.100 -1809.100 -1809.100], eps: 0.165})
Step:  141200, Reward: [-620.894 -620.894 -620.894] [288.531], Avg: [-710.043 -710.043 -710.043] (0.1000) ({r_i: None, r_t: [-1313.574 -1313.574 -1313.574], eps: 0.1})
Step:  107400, Reward: [-468.728 -468.728 -468.728] [107.303], Avg: [-794.314 -794.314 -794.314] (0.0100) ({r_i: None, r_t: [-863.561 -863.561 -863.561], eps: 0.01})
Step:   18100, Reward: [-850.766 -850.766 -850.766] [248.867], Avg: [-690.845 -690.845 -690.845] (0.1629) ({r_i: None, r_t: [-1513.469 -1513.469 -1513.469], eps: 0.163})
Step:  141300, Reward: [-654.649 -654.649 -654.649] [188.649], Avg: [-710.004 -710.004 -710.004] (0.1000) ({r_i: None, r_t: [-1323.342 -1323.342 -1323.342], eps: 0.1})
Step:  107500, Reward: [-420.619 -420.619 -420.619] [68.914], Avg: [-793.966 -793.966 -793.966] (0.0100) ({r_i: None, r_t: [-851.329 -851.329 -851.329], eps: 0.01})
Step:   18200, Reward: [-905.970 -905.970 -905.970] [339.529], Avg: [-692.021 -692.021 -692.021] (0.1613) ({r_i: None, r_t: [-1780.872 -1780.872 -1780.872], eps: 0.161})
Step:  141400, Reward: [-624.421 -624.421 -624.421] [242.997], Avg: [-709.944 -709.944 -709.944] (0.1000) ({r_i: None, r_t: [-1199.154 -1199.154 -1199.154], eps: 0.1})
Step:  107600, Reward: [-413.865 -413.865 -413.865] [64.359], Avg: [-793.613 -793.613 -793.613] (0.0100) ({r_i: None, r_t: [-910.405 -910.405 -910.405], eps: 0.01})
Step:   18300, Reward: [-861.502 -861.502 -861.502] [326.122], Avg: [-692.942 -692.942 -692.942] (0.1597) ({r_i: None, r_t: [-1792.132 -1792.132 -1792.132], eps: 0.16})
Step:  141500, Reward: [-678.409 -678.409 -678.409] [280.878], Avg: [-709.921 -709.921 -709.921] (0.1000) ({r_i: None, r_t: [-1356.194 -1356.194 -1356.194], eps: 0.1})
Step:  107700, Reward: [-439.837 -439.837 -439.837] [107.615], Avg: [-793.285 -793.285 -793.285] (0.0100) ({r_i: None, r_t: [-853.538 -853.538 -853.538], eps: 0.01})
Step:   18400, Reward: [-804.848 -804.848 -804.848] [332.855], Avg: [-693.547 -693.547 -693.547] (0.1581) ({r_i: None, r_t: [-1701.947 -1701.947 -1701.947], eps: 0.158})
Step:  141600, Reward: [-631.364 -631.364 -631.364] [228.794], Avg: [-709.866 -709.866 -709.866] (0.1000) ({r_i: None, r_t: [-1226.858 -1226.858 -1226.858], eps: 0.1})
Step:  107800, Reward: [-455.105 -455.105 -455.105] [66.179], Avg: [-792.972 -792.972 -792.972] (0.0100) ({r_i: None, r_t: [-846.946 -846.946 -846.946], eps: 0.01})
Step:   18500, Reward: [-897.013 -897.013 -897.013] [307.112], Avg: [-694.641 -694.641 -694.641] (0.1565) ({r_i: None, r_t: [-1573.516 -1573.516 -1573.516], eps: 0.157})
Step:  141700, Reward: [-586.749 -586.749 -586.749] [141.485], Avg: [-709.779 -709.779 -709.779] (0.1000) ({r_i: None, r_t: [-1265.393 -1265.393 -1265.393], eps: 0.1})
Step:  107900, Reward: [-446.118 -446.118 -446.118] [96.846], Avg: [-792.651 -792.651 -792.651] (0.0100) ({r_i: None, r_t: [-861.905 -861.905 -861.905], eps: 0.01})
Step:   18600, Reward: [-722.068 -722.068 -722.068] [200.351], Avg: [-694.787 -694.787 -694.787] (0.1549) ({r_i: None, r_t: [-1867.789 -1867.789 -1867.789], eps: 0.155})
Step:  141800, Reward: [-550.914 -550.914 -550.914] [103.299], Avg: [-709.667 -709.667 -709.667] (0.1000) ({r_i: None, r_t: [-1200.941 -1200.941 -1200.941], eps: 0.1})
Step:  108000, Reward: [-426.674 -426.674 -426.674] [62.427], Avg: [-792.312 -792.312 -792.312] (0.0100) ({r_i: None, r_t: [-973.820 -973.820 -973.820], eps: 0.01})
Step:   18700, Reward: [-838.158 -838.158 -838.158] [315.631], Avg: [-695.550 -695.550 -695.550] (0.1534) ({r_i: None, r_t: [-1769.336 -1769.336 -1769.336], eps: 0.153})
Step:  141900, Reward: [-648.890 -648.890 -648.890] [252.114], Avg: [-709.624 -709.624 -709.624] (0.1000) ({r_i: None, r_t: [-1205.968 -1205.968 -1205.968], eps: 0.1})
Step:  108100, Reward: [-440.635 -440.635 -440.635] [58.850], Avg: [-791.987 -791.987 -791.987] (0.0100) ({r_i: None, r_t: [-843.650 -843.650 -843.650], eps: 0.01})
Step:   18800, Reward: [-870.413 -870.413 -870.413] [283.115], Avg: [-696.475 -696.475 -696.475] (0.1519) ({r_i: None, r_t: [-1608.845 -1608.845 -1608.845], eps: 0.152})
Step:  142000, Reward: [-611.399 -611.399 -611.399] [124.549], Avg: [-709.555 -709.555 -709.555] (0.1000) ({r_i: None, r_t: [-1112.176 -1112.176 -1112.176], eps: 0.1})
Step:  108200, Reward: [-411.626 -411.626 -411.626] [99.695], Avg: [-791.636 -791.636 -791.636] (0.0100) ({r_i: None, r_t: [-913.663 -913.663 -913.663], eps: 0.01})
Step:   18900, Reward: [-934.051 -934.051 -934.051] [333.605], Avg: [-697.726 -697.726 -697.726] (0.1504) ({r_i: None, r_t: [-1666.800 -1666.800 -1666.800], eps: 0.15})
Step:  142100, Reward: [-652.350 -652.350 -652.350] [272.686], Avg: [-709.515 -709.515 -709.515] (0.1000) ({r_i: None, r_t: [-1230.851 -1230.851 -1230.851], eps: 0.1})
Step:  108300, Reward: [-414.981 -414.981 -414.981] [64.397], Avg: [-791.288 -791.288 -791.288] (0.0100) ({r_i: None, r_t: [-925.878 -925.878 -925.878], eps: 0.01})
Step:   19000, Reward: [-988.108 -988.108 -988.108] [375.631], Avg: [-699.246 -699.246 -699.246] (0.1489) ({r_i: None, r_t: [-1712.797 -1712.797 -1712.797], eps: 0.149})
Step:  142200, Reward: [-580.266 -580.266 -580.266] [128.433], Avg: [-709.424 -709.424 -709.424] (0.1000) ({r_i: None, r_t: [-1229.866 -1229.866 -1229.866], eps: 0.1})
Step:  108400, Reward: [-432.978 -432.978 -432.978] [108.050], Avg: [-790.958 -790.958 -790.958] (0.0100) ({r_i: None, r_t: [-847.652 -847.652 -847.652], eps: 0.01})
Step:  142300, Reward: [-581.498 -581.498 -581.498] [143.443], Avg: [-709.334 -709.334 -709.334] (0.1000) ({r_i: None, r_t: [-1261.245 -1261.245 -1261.245], eps: 0.1})
Step:   19100, Reward: [-798.402 -798.402 -798.402] [260.991], Avg: [-699.762 -699.762 -699.762] (0.1474) ({r_i: None, r_t: [-1562.782 -1562.782 -1562.782], eps: 0.147})
Step:  142400, Reward: [-583.618 -583.618 -583.618] [140.704], Avg: [-709.246 -709.246 -709.246] (0.1000) ({r_i: None, r_t: [-1245.812 -1245.812 -1245.812], eps: 0.1})
Step:  108500, Reward: [-434.721 -434.721 -434.721] [135.430], Avg: [-790.630 -790.630 -790.630] (0.0100) ({r_i: None, r_t: [-887.377 -887.377 -887.377], eps: 0.01})
Step:   19200, Reward: [-785.507 -785.507 -785.507] [180.779], Avg: [-700.207 -700.207 -700.207] (0.1459) ({r_i: None, r_t: [-1543.643 -1543.643 -1543.643], eps: 0.146})
Step:  142500, Reward: [-568.207 -568.207 -568.207] [126.625], Avg: [-709.147 -709.147 -709.147] (0.1000) ({r_i: None, r_t: [-1277.512 -1277.512 -1277.512], eps: 0.1})
Step:   19300, Reward: [-736.062 -736.062 -736.062] [214.851], Avg: [-700.392 -700.392 -700.392] (0.1444) ({r_i: None, r_t: [-1671.502 -1671.502 -1671.502], eps: 0.144})
Step:  108600, Reward: [-419.436 -419.436 -419.436] [70.122], Avg: [-790.289 -790.289 -790.289] (0.0100) ({r_i: None, r_t: [-919.036 -919.036 -919.036], eps: 0.01})
Step:  142600, Reward: [-554.170 -554.170 -554.170] [107.562], Avg: [-709.039 -709.039 -709.039] (0.1000) ({r_i: None, r_t: [-1231.367 -1231.367 -1231.367], eps: 0.1})
Step:   19400, Reward: [-760.300 -760.300 -760.300] [276.437], Avg: [-700.699 -700.699 -700.699] (0.1430) ({r_i: None, r_t: [-1626.183 -1626.183 -1626.183], eps: 0.143})
Step:  108700, Reward: [-458.052 -458.052 -458.052] [107.534], Avg: [-789.983 -789.983 -789.983] (0.0100) ({r_i: None, r_t: [-864.737 -864.737 -864.737], eps: 0.01})
Step:  142700, Reward: [-617.002 -617.002 -617.002] [138.468], Avg: [-708.974 -708.974 -708.974] (0.1000) ({r_i: None, r_t: [-1148.210 -1148.210 -1148.210], eps: 0.1})
Step:   19500, Reward: [-709.365 -709.365 -709.365] [246.542], Avg: [-700.743 -700.743 -700.743] (0.1416) ({r_i: None, r_t: [-1550.706 -1550.706 -1550.706], eps: 0.142})
Step:  108800, Reward: [-435.498 -435.498 -435.498] [88.646], Avg: [-789.658 -789.658 -789.658] (0.0100) ({r_i: None, r_t: [-957.627 -957.627 -957.627], eps: 0.01})
Step:  142800, Reward: [-775.562 -775.562 -775.562] [326.336], Avg: [-709.021 -709.021 -709.021] (0.1000) ({r_i: None, r_t: [-1118.332 -1118.332 -1118.332], eps: 0.1})
Step:   19600, Reward: [-669.037 -669.037 -669.037] [252.308], Avg: [-700.582 -700.582 -700.582] (0.1402) ({r_i: None, r_t: [-1237.308 -1237.308 -1237.308], eps: 0.14})
Step:  108900, Reward: [-400.737 -400.737 -400.737] [90.907], Avg: [-789.301 -789.301 -789.301] (0.0100) ({r_i: None, r_t: [-902.152 -902.152 -902.152], eps: 0.01})
Step:  142900, Reward: [-642.098 -642.098 -642.098] [232.797], Avg: [-708.974 -708.974 -708.974] (0.1000) ({r_i: None, r_t: [-1071.646 -1071.646 -1071.646], eps: 0.1})
Step:   19700, Reward: [-784.719 -784.719 -784.719] [252.365], Avg: [-701.007 -701.007 -701.007] (0.1388) ({r_i: None, r_t: [-1364.493 -1364.493 -1364.493], eps: 0.139})
Step:  109000, Reward: [-438.492 -438.492 -438.492] [109.707], Avg: [-788.979 -788.979 -788.979] (0.0100) ({r_i: None, r_t: [-867.653 -867.653 -867.653], eps: 0.01})
Step:  143000, Reward: [-570.014 -570.014 -570.014] [106.840], Avg: [-708.877 -708.877 -708.877] (0.1000) ({r_i: None, r_t: [-1091.261 -1091.261 -1091.261], eps: 0.1})
Step:   19800, Reward: [-664.895 -664.895 -664.895] [186.625], Avg: [-700.825 -700.825 -700.825] (0.1374) ({r_i: None, r_t: [-1392.526 -1392.526 -1392.526], eps: 0.137})
Step:  109100, Reward: [-408.699 -408.699 -408.699] [85.971], Avg: [-788.631 -788.631 -788.631] (0.0100) ({r_i: None, r_t: [-872.399 -872.399 -872.399], eps: 0.01})
Step:  143100, Reward: [-546.278 -546.278 -546.278] [141.498], Avg: [-708.763 -708.763 -708.763] (0.1000) ({r_i: None, r_t: [-1204.014 -1204.014 -1204.014], eps: 0.1})
Step:   19900, Reward: [-785.603 -785.603 -785.603] [229.292], Avg: [-701.249 -701.249 -701.249] (0.1360) ({r_i: None, r_t: [-1310.536 -1310.536 -1310.536], eps: 0.136})
Step:  109200, Reward: [-406.054 -406.054 -406.054] [70.497], Avg: [-788.281 -788.281 -788.281] (0.0100) ({r_i: None, r_t: [-884.859 -884.859 -884.859], eps: 0.01})
Step:  143200, Reward: [-567.332 -567.332 -567.332] [95.315], Avg: [-708.665 -708.665 -708.665] (0.1000) ({r_i: None, r_t: [-1187.509 -1187.509 -1187.509], eps: 0.1})
Step:   20000, Reward: [-592.382 -592.382 -592.382] [178.272], Avg: [-700.708 -700.708 -700.708] (0.1347) ({r_i: None, r_t: [-1355.418 -1355.418 -1355.418], eps: 0.135})
Step:  109300, Reward: [-437.094 -437.094 -437.094] [89.101], Avg: [-787.960 -787.960 -787.960] (0.0100) ({r_i: None, r_t: [-865.505 -865.505 -865.505], eps: 0.01})
Step:  143300, Reward: [-617.948 -617.948 -617.948] [183.772], Avg: [-708.601 -708.601 -708.601] (0.1000) ({r_i: None, r_t: [-1228.669 -1228.669 -1228.669], eps: 0.1})
Step:   20100, Reward: [-681.856 -681.856 -681.856] [234.637], Avg: [-700.614 -700.614 -700.614] (0.1333) ({r_i: None, r_t: [-1402.796 -1402.796 -1402.796], eps: 0.133})
Step:  109400, Reward: [-404.071 -404.071 -404.071] [86.012], Avg: [-787.610 -787.610 -787.610] (0.0100) ({r_i: None, r_t: [-854.459 -854.459 -854.459], eps: 0.01})
Step:  143400, Reward: [-589.163 -589.163 -589.163] [106.147], Avg: [-708.518 -708.518 -708.518] (0.1000) ({r_i: None, r_t: [-1104.789 -1104.789 -1104.789], eps: 0.1})
Step:   20200, Reward: [-613.855 -613.855 -613.855] [153.198], Avg: [-700.187 -700.187 -700.187] (0.1320) ({r_i: None, r_t: [-1220.049 -1220.049 -1220.049], eps: 0.132})
Step:  109500, Reward: [-417.353 -417.353 -417.353] [97.335], Avg: [-787.272 -787.272 -787.272] (0.0100) ({r_i: None, r_t: [-816.972 -816.972 -816.972], eps: 0.01})
Step:  143500, Reward: [-556.767 -556.767 -556.767] [152.265], Avg: [-708.412 -708.412 -708.412] (0.1000) ({r_i: None, r_t: [-1117.188 -1117.188 -1117.188], eps: 0.1})
Step:   20300, Reward: [-586.970 -586.970 -586.970] [174.942], Avg: [-699.632 -699.632 -699.632] (0.1307) ({r_i: None, r_t: [-1156.573 -1156.573 -1156.573], eps: 0.131})
Step:  109600, Reward: [-439.906 -439.906 -439.906] [97.565], Avg: [-786.955 -786.955 -786.955] (0.0100) ({r_i: None, r_t: [-845.077 -845.077 -845.077], eps: 0.01})
Step:  143600, Reward: [-580.388 -580.388 -580.388] [133.686], Avg: [-708.323 -708.323 -708.323] (0.1000) ({r_i: None, r_t: [-1191.251 -1191.251 -1191.251], eps: 0.1})
Step:   20400, Reward: [-537.451 -537.451 -537.451] [84.561], Avg: [-698.841 -698.841 -698.841] (0.1294) ({r_i: None, r_t: [-1212.899 -1212.899 -1212.899], eps: 0.129})
Step:  109700, Reward: [-443.108 -443.108 -443.108] [96.605], Avg: [-786.642 -786.642 -786.642] (0.0100) ({r_i: None, r_t: [-837.152 -837.152 -837.152], eps: 0.01})
Step:  143700, Reward: [-593.960 -593.960 -593.960] [196.899], Avg: [-708.244 -708.244 -708.244] (0.1000) ({r_i: None, r_t: [-1157.748 -1157.748 -1157.748], eps: 0.1})
Step:   20500, Reward: [-627.539 -627.539 -627.539] [158.801], Avg: [-698.495 -698.495 -698.495] (0.1281) ({r_i: None, r_t: [-1242.654 -1242.654 -1242.654], eps: 0.128})
Step:  109800, Reward: [-437.556 -437.556 -437.556] [71.468], Avg: [-786.324 -786.324 -786.324] (0.0100) ({r_i: None, r_t: [-810.337 -810.337 -810.337], eps: 0.01})
Step:  143800, Reward: [-639.184 -639.184 -639.184] [144.447], Avg: [-708.196 -708.196 -708.196] (0.1000) ({r_i: None, r_t: [-1234.410 -1234.410 -1234.410], eps: 0.1})
Step:   20600, Reward: [-642.304 -642.304 -642.304] [165.846], Avg: [-698.223 -698.223 -698.223] (0.1268) ({r_i: None, r_t: [-1311.551 -1311.551 -1311.551], eps: 0.127})
Step:  109900, Reward: [-443.553 -443.553 -443.553] [68.096], Avg: [-786.013 -786.013 -786.013] (0.0100) ({r_i: None, r_t: [-906.139 -906.139 -906.139], eps: 0.01})
Step:  143900, Reward: [-576.318 -576.318 -576.318] [185.203], Avg: [-708.104 -708.104 -708.104] (0.1000) ({r_i: None, r_t: [-1142.965 -1142.965 -1142.965], eps: 0.1})
Step:   20700, Reward: [-565.211 -565.211 -565.211] [216.276], Avg: [-697.584 -697.584 -697.584] (0.1255) ({r_i: None, r_t: [-1273.359 -1273.359 -1273.359], eps: 0.126})
Step:  110000, Reward: [-425.684 -425.684 -425.684] [98.616], Avg: [-785.685 -785.685 -785.685] (0.0100) ({r_i: None, r_t: [-840.537 -840.537 -840.537], eps: 0.01})
Step:  144000, Reward: [-546.271 -546.271 -546.271] [113.171], Avg: [-707.992 -707.992 -707.992] (0.1000) ({r_i: None, r_t: [-1259.325 -1259.325 -1259.325], eps: 0.1})
Step:   20800, Reward: [-550.404 -550.404 -550.404] [111.665], Avg: [-696.880 -696.880 -696.880] (0.1243) ({r_i: None, r_t: [-1316.736 -1316.736 -1316.736], eps: 0.124})
Step:  110100, Reward: [-398.245 -398.245 -398.245] [78.447], Avg: [-785.334 -785.334 -785.334] (0.0100) ({r_i: None, r_t: [-897.437 -897.437 -897.437], eps: 0.01})
Step:  144100, Reward: [-566.007 -566.007 -566.007] [122.903], Avg: [-707.893 -707.893 -707.893] (0.1000) ({r_i: None, r_t: [-1108.287 -1108.287 -1108.287], eps: 0.1})
Step:   20900, Reward: [-664.064 -664.064 -664.064] [211.592], Avg: [-696.723 -696.723 -696.723] (0.1230) ({r_i: None, r_t: [-1265.911 -1265.911 -1265.911], eps: 0.123})
Step:  110200, Reward: [-461.472 -461.472 -461.472] [98.235], Avg: [-785.040 -785.040 -785.040] (0.0100) ({r_i: None, r_t: [-871.258 -871.258 -871.258], eps: 0.01})
Step:  144200, Reward: [-582.888 -582.888 -582.888] [121.848], Avg: [-707.807 -707.807 -707.807] (0.1000) ({r_i: None, r_t: [-1250.005 -1250.005 -1250.005], eps: 0.1})
Step:  110300, Reward: [-423.558 -423.558 -423.558] [89.256], Avg: [-784.713 -784.713 -784.713] (0.0100) ({r_i: None, r_t: [-853.996 -853.996 -853.996], eps: 0.01})
Step:   21000, Reward: [-618.936 -618.936 -618.936] [141.052], Avg: [-696.355 -696.355 -696.355] (0.1218) ({r_i: None, r_t: [-1341.199 -1341.199 -1341.199], eps: 0.122})
Step:  144300, Reward: [-527.918 -527.918 -527.918] [116.107], Avg: [-707.682 -707.682 -707.682] (0.1000) ({r_i: None, r_t: [-1097.043 -1097.043 -1097.043], eps: 0.1})
Step:  144400, Reward: [-611.854 -611.854 -611.854] [166.090], Avg: [-707.616 -707.616 -707.616] (0.1000) ({r_i: None, r_t: [-1184.187 -1184.187 -1184.187], eps: 0.1})
Step:  110400, Reward: [-474.938 -474.938 -474.938] [123.750], Avg: [-784.432 -784.432 -784.432] (0.0100) ({r_i: None, r_t: [-858.043 -858.043 -858.043], eps: 0.01})
Step:   21100, Reward: [-778.046 -778.046 -778.046] [226.483], Avg: [-696.740 -696.740 -696.740] (0.1206) ({r_i: None, r_t: [-1469.689 -1469.689 -1469.689], eps: 0.121})
Step:  144500, Reward: [-538.484 -538.484 -538.484] [117.198], Avg: [-707.499 -707.499 -707.499] (0.1000) ({r_i: None, r_t: [-1117.383 -1117.383 -1117.383], eps: 0.1})
Step:  110500, Reward: [-417.725 -417.725 -417.725] [97.244], Avg: [-784.101 -784.101 -784.101] (0.0100) ({r_i: None, r_t: [-807.850 -807.850 -807.850], eps: 0.01})
Step:   21200, Reward: [-714.669 -714.669 -714.669] [209.687], Avg: [-696.824 -696.824 -696.824] (0.1194) ({r_i: None, r_t: [-1590.671 -1590.671 -1590.671], eps: 0.119})
Step:  144600, Reward: [-591.989 -591.989 -591.989] [112.726], Avg: [-707.419 -707.419 -707.419] (0.1000) ({r_i: None, r_t: [-1137.604 -1137.604 -1137.604], eps: 0.1})
Step:  110600, Reward: [-439.600 -439.600 -439.600] [79.374], Avg: [-783.790 -783.790 -783.790] (0.0100) ({r_i: None, r_t: [-878.955 -878.955 -878.955], eps: 0.01})
Step:   21300, Reward: [-741.726 -741.726 -741.726] [247.180], Avg: [-697.034 -697.034 -697.034] (0.1182) ({r_i: None, r_t: [-1500.498 -1500.498 -1500.498], eps: 0.118})
Step:  144700, Reward: [-576.216 -576.216 -576.216] [158.316], Avg: [-707.328 -707.328 -707.328] (0.1000) ({r_i: None, r_t: [-1134.553 -1134.553 -1134.553], eps: 0.1})
Step:  110700, Reward: [-463.774 -463.774 -463.774] [127.796], Avg: [-783.501 -783.501 -783.501] (0.0100) ({r_i: None, r_t: [-890.339 -890.339 -890.339], eps: 0.01})
Step:   21400, Reward: [-791.125 -791.125 -791.125] [255.252], Avg: [-697.472 -697.472 -697.472] (0.1170) ({r_i: None, r_t: [-1704.416 -1704.416 -1704.416], eps: 0.117})
Step:  144800, Reward: [-495.725 -495.725 -495.725] [115.087], Avg: [-707.182 -707.182 -707.182] (0.1000) ({r_i: None, r_t: [-1084.367 -1084.367 -1084.367], eps: 0.1})
Step:  110800, Reward: [-404.288 -404.288 -404.288] [73.173], Avg: [-783.159 -783.159 -783.159] (0.0100) ({r_i: None, r_t: [-894.993 -894.993 -894.993], eps: 0.01})
Step:   21500, Reward: [-858.977 -858.977 -858.977] [378.741], Avg: [-698.219 -698.219 -698.219] (0.1159) ({r_i: None, r_t: [-1584.038 -1584.038 -1584.038], eps: 0.116})
Step:  144900, Reward: [-539.840 -539.840 -539.840] [121.778], Avg: [-707.067 -707.067 -707.067] (0.1000) ({r_i: None, r_t: [-1102.837 -1102.837 -1102.837], eps: 0.1})
Step:  110900, Reward: [-470.022 -470.022 -470.022] [88.114], Avg: [-782.877 -782.877 -782.877] (0.0100) ({r_i: None, r_t: [-848.267 -848.267 -848.267], eps: 0.01})
Step:   21600, Reward: [-839.187 -839.187 -839.187] [294.156], Avg: [-698.869 -698.869 -698.869] (0.1147) ({r_i: None, r_t: [-1704.681 -1704.681 -1704.681], eps: 0.115})
Step:  145000, Reward: [-539.615 -539.615 -539.615] [116.610], Avg: [-706.952 -706.952 -706.952] (0.1000) ({r_i: None, r_t: [-1207.625 -1207.625 -1207.625], eps: 0.1})
Step:  111000, Reward: [-438.623 -438.623 -438.623] [79.194], Avg: [-782.567 -782.567 -782.567] (0.0100) ({r_i: None, r_t: [-917.225 -917.225 -917.225], eps: 0.01})
Step:   21700, Reward: [-926.122 -926.122 -926.122] [358.904], Avg: [-699.911 -699.911 -699.911] (0.1136) ({r_i: None, r_t: [-1817.283 -1817.283 -1817.283], eps: 0.114})
Step:  145100, Reward: [-603.726 -603.726 -603.726] [151.168], Avg: [-706.881 -706.881 -706.881] (0.1000) ({r_i: None, r_t: [-1086.491 -1086.491 -1086.491], eps: 0.1})
Step:  111100, Reward: [-432.755 -432.755 -432.755] [71.244], Avg: [-782.252 -782.252 -782.252] (0.0100) ({r_i: None, r_t: [-912.708 -912.708 -912.708], eps: 0.01})
Step:   21800, Reward: [-926.800 -926.800 -926.800] [285.836], Avg: [-700.947 -700.947 -700.947] (0.1124) ({r_i: None, r_t: [-1614.159 -1614.159 -1614.159], eps: 0.112})
Step:  145200, Reward: [-595.349 -595.349 -595.349] [156.967], Avg: [-706.804 -706.804 -706.804] (0.1000) ({r_i: None, r_t: [-1081.977 -1081.977 -1081.977], eps: 0.1})
Step:  111200, Reward: [-435.502 -435.502 -435.502] [97.353], Avg: [-781.941 -781.941 -781.941] (0.0100) ({r_i: None, r_t: [-913.700 -913.700 -913.700], eps: 0.01})
Step:   21900, Reward: [-864.530 -864.530 -864.530] [404.904], Avg: [-701.691 -701.691 -701.691] (0.1113) ({r_i: None, r_t: [-1880.198 -1880.198 -1880.198], eps: 0.111})
Step:  145300, Reward: [-556.758 -556.758 -556.758] [114.141], Avg: [-706.701 -706.701 -706.701] (0.1000) ({r_i: None, r_t: [-1127.272 -1127.272 -1127.272], eps: 0.1})
Step:  111300, Reward: [-429.224 -429.224 -429.224] [82.907], Avg: [-781.624 -781.624 -781.624] (0.0100) ({r_i: None, r_t: [-898.140 -898.140 -898.140], eps: 0.01})
Step:   22000, Reward: [-760.005 -760.005 -760.005] [183.471], Avg: [-701.955 -701.955 -701.955] (0.1102) ({r_i: None, r_t: [-1873.862 -1873.862 -1873.862], eps: 0.11})
Step:  145400, Reward: [-559.966 -559.966 -559.966] [106.543], Avg: [-706.600 -706.600 -706.600] (0.1000) ({r_i: None, r_t: [-1192.243 -1192.243 -1192.243], eps: 0.1})
Step:  111400, Reward: [-477.639 -477.639 -477.639] [91.670], Avg: [-781.352 -781.352 -781.352] (0.0100) ({r_i: None, r_t: [-899.863 -899.863 -899.863], eps: 0.01})
Step:   22100, Reward: [-718.507 -718.507 -718.507] [178.530], Avg: [-702.029 -702.029 -702.029] (0.1091) ({r_i: None, r_t: [-1499.680 -1499.680 -1499.680], eps: 0.109})
Step:  145500, Reward: [-523.368 -523.368 -523.368] [85.610], Avg: [-706.474 -706.474 -706.474] (0.1000) ({r_i: None, r_t: [-1146.381 -1146.381 -1146.381], eps: 0.1})
Step:  111500, Reward: [-416.363 -416.363 -416.363] [78.453], Avg: [-781.024 -781.024 -781.024] (0.0100) ({r_i: None, r_t: [-864.581 -864.581 -864.581], eps: 0.01})
Step:   22200, Reward: [-684.323 -684.323 -684.323] [183.693], Avg: [-701.950 -701.950 -701.950] (0.1080) ({r_i: None, r_t: [-1657.495 -1657.495 -1657.495], eps: 0.108})
Step:  145600, Reward: [-496.682 -496.682 -496.682] [86.227], Avg: [-706.330 -706.330 -706.330] (0.1000) ({r_i: None, r_t: [-1077.625 -1077.625 -1077.625], eps: 0.1})
Step:  111600, Reward: [-421.696 -421.696 -421.696] [128.060], Avg: [-780.703 -780.703 -780.703] (0.0100) ({r_i: None, r_t: [-855.015 -855.015 -855.015], eps: 0.01})
Step:   22300, Reward: [-798.859 -798.859 -798.859] [328.316], Avg: [-702.383 -702.383 -702.383] (0.1069) ({r_i: None, r_t: [-1414.016 -1414.016 -1414.016], eps: 0.107})
Step:  145700, Reward: [-613.181 -613.181 -613.181] [127.834], Avg: [-706.266 -706.266 -706.266] (0.1000) ({r_i: None, r_t: [-1138.447 -1138.447 -1138.447], eps: 0.1})
Step:  111700, Reward: [-449.911 -449.911 -449.911] [78.424], Avg: [-780.407 -780.407 -780.407] (0.0100) ({r_i: None, r_t: [-854.781 -854.781 -854.781], eps: 0.01})
Step:   22400, Reward: [-645.549 -645.549 -645.549] [244.451], Avg: [-702.130 -702.130 -702.130] (0.1059) ({r_i: None, r_t: [-1595.777 -1595.777 -1595.777], eps: 0.106})
Step:  145800, Reward: [-550.771 -550.771 -550.771] [73.303], Avg: [-706.159 -706.159 -706.159] (0.1000) ({r_i: None, r_t: [-1098.651 -1098.651 -1098.651], eps: 0.1})
Step:  111800, Reward: [-435.829 -435.829 -435.829] [110.604], Avg: [-780.099 -780.099 -780.099] (0.0100) ({r_i: None, r_t: [-859.901 -859.901 -859.901], eps: 0.01})
Step:   22500, Reward: [-642.537 -642.537 -642.537] [285.110], Avg: [-701.866 -701.866 -701.866] (0.1048) ({r_i: None, r_t: [-1307.787 -1307.787 -1307.787], eps: 0.105})
Step:  145900, Reward: [-553.691 -553.691 -553.691] [70.553], Avg: [-706.055 -706.055 -706.055] (0.1000) ({r_i: None, r_t: [-1096.353 -1096.353 -1096.353], eps: 0.1})
Step:  111900, Reward: [-457.358 -457.358 -457.358] [91.724], Avg: [-779.811 -779.811 -779.811] (0.0100) ({r_i: None, r_t: [-807.083 -807.083 -807.083], eps: 0.01})
Step:   22600, Reward: [-667.954 -667.954 -667.954] [187.883], Avg: [-701.717 -701.717 -701.717] (0.1038) ({r_i: None, r_t: [-1230.366 -1230.366 -1230.366], eps: 0.104})
Step:  146000, Reward: [-572.121 -572.121 -572.121] [148.543], Avg: [-705.963 -705.963 -705.963] (0.1000) ({r_i: None, r_t: [-1130.640 -1130.640 -1130.640], eps: 0.1})
Step:  112000, Reward: [-440.451 -440.451 -440.451] [132.022], Avg: [-779.508 -779.508 -779.508] (0.0100) ({r_i: None, r_t: [-925.601 -925.601 -925.601], eps: 0.01})
Step:   22700, Reward: [-619.893 -619.893 -619.893] [160.403], Avg: [-701.358 -701.358 -701.358] (0.1027) ({r_i: None, r_t: [-1340.803 -1340.803 -1340.803], eps: 0.103})
Step:  146100, Reward: [-574.879 -574.879 -574.879] [135.051], Avg: [-705.874 -705.874 -705.874] (0.1000) ({r_i: None, r_t: [-1233.965 -1233.965 -1233.965], eps: 0.1})
Step:  112100, Reward: [-440.076 -440.076 -440.076] [67.580], Avg: [-779.206 -779.206 -779.206] (0.0100) ({r_i: None, r_t: [-908.801 -908.801 -908.801], eps: 0.01})
Step:   22800, Reward: [-666.724 -666.724 -666.724] [194.553], Avg: [-701.207 -701.207 -701.207] (0.1017) ({r_i: None, r_t: [-1213.423 -1213.423 -1213.423], eps: 0.102})
Step:  146200, Reward: [-546.196 -546.196 -546.196] [133.603], Avg: [-705.765 -705.765 -705.765] (0.1000) ({r_i: None, r_t: [-1207.931 -1207.931 -1207.931], eps: 0.1})
Step:  112200, Reward: [-452.291 -452.291 -452.291] [106.360], Avg: [-778.914 -778.914 -778.914] (0.0100) ({r_i: None, r_t: [-855.195 -855.195 -855.195], eps: 0.01})
Step:   22900, Reward: [-629.032 -629.032 -629.032] [235.972], Avg: [-700.893 -700.893 -700.893] (0.1007) ({r_i: None, r_t: [-1250.819 -1250.819 -1250.819], eps: 0.101})
Step:  146300, Reward: [-556.313 -556.313 -556.313] [121.586], Avg: [-705.662 -705.662 -705.662] (0.1000) ({r_i: None, r_t: [-1057.276 -1057.276 -1057.276], eps: 0.1})
Step:  112300, Reward: [-408.395 -408.395 -408.395] [87.359], Avg: [-778.585 -778.585 -778.585] (0.0100) ({r_i: None, r_t: [-852.746 -852.746 -852.746], eps: 0.01})
Step:   23000, Reward: [-606.208 -606.208 -606.208] [179.562], Avg: [-700.483 -700.483 -700.483] (0.0997) ({r_i: None, r_t: [-1388.109 -1388.109 -1388.109], eps: 0.1})
Step:  146400, Reward: [-585.128 -585.128 -585.128] [173.199], Avg: [-705.580 -705.580 -705.580] (0.1000) ({r_i: None, r_t: [-1144.074 -1144.074 -1144.074], eps: 0.1})
Step:  112400, Reward: [-455.926 -455.926 -455.926] [81.729], Avg: [-778.298 -778.298 -778.298] (0.0100) ({r_i: None, r_t: [-872.212 -872.212 -872.212], eps: 0.01})
Step:   23100, Reward: [-557.207 -557.207 -557.207] [100.525], Avg: [-699.866 -699.866 -699.866] (0.0987) ({r_i: None, r_t: [-1209.998 -1209.998 -1209.998], eps: 0.099})
Step:  146500, Reward: [-501.948 -501.948 -501.948] [112.738], Avg: [-705.441 -705.441 -705.441] (0.1000) ({r_i: None, r_t: [-1164.596 -1164.596 -1164.596], eps: 0.1})
Step:  112500, Reward: [-484.670 -484.670 -484.670] [94.501], Avg: [-778.037 -778.037 -778.037] (0.0100) ({r_i: None, r_t: [-860.554 -860.554 -860.554], eps: 0.01})
Step:  146600, Reward: [-604.767 -604.767 -604.767] [149.613], Avg: [-705.373 -705.373 -705.373] (0.1000) ({r_i: None, r_t: [-1104.624 -1104.624 -1104.624], eps: 0.1})
Step:   23200, Reward: [-641.488 -641.488 -641.488] [185.441], Avg: [-699.615 -699.615 -699.615] (0.0977) ({r_i: None, r_t: [-1160.658 -1160.658 -1160.658], eps: 0.098})
Step:  112600, Reward: [-466.833 -466.833 -466.833] [114.194], Avg: [-777.761 -777.761 -777.761] (0.0100) ({r_i: None, r_t: [-859.128 -859.128 -859.128], eps: 0.01})
Step:  146700, Reward: [-594.859 -594.859 -594.859] [94.418], Avg: [-705.297 -705.297 -705.297] (0.1000) ({r_i: None, r_t: [-1178.014 -1178.014 -1178.014], eps: 0.1})
Step:   23300, Reward: [-494.814 -494.814 -494.814] [64.174], Avg: [-698.740 -698.740 -698.740] (0.0967) ({r_i: None, r_t: [-1155.299 -1155.299 -1155.299], eps: 0.097})
Step:  146800, Reward: [-538.673 -538.673 -538.673] [118.886], Avg: [-705.184 -705.184 -705.184] (0.1000) ({r_i: None, r_t: [-1187.716 -1187.716 -1187.716], eps: 0.1})
Step:  112700, Reward: [-444.981 -444.981 -444.981] [93.723], Avg: [-777.466 -777.466 -777.466] (0.0100) ({r_i: None, r_t: [-913.369 -913.369 -913.369], eps: 0.01})
Step:   23400, Reward: [-548.608 -548.608 -548.608] [112.484], Avg: [-698.101 -698.101 -698.101] (0.0958) ({r_i: None, r_t: [-1192.895 -1192.895 -1192.895], eps: 0.096})
Step:  146900, Reward: [-618.802 -618.802 -618.802] [115.646], Avg: [-705.125 -705.125 -705.125] (0.1000) ({r_i: None, r_t: [-1131.723 -1131.723 -1131.723], eps: 0.1})
Step:  112800, Reward: [-431.192 -431.192 -431.192] [97.263], Avg: [-777.159 -777.159 -777.159] (0.0100) ({r_i: None, r_t: [-918.388 -918.388 -918.388], eps: 0.01})
Step:   23500, Reward: [-590.175 -590.175 -590.175] [178.346], Avg: [-697.644 -697.644 -697.644] (0.0948) ({r_i: None, r_t: [-1270.325 -1270.325 -1270.325], eps: 0.095})
Step:  147000, Reward: [-544.530 -544.530 -544.530] [182.271], Avg: [-705.016 -705.016 -705.016] (0.1000) ({r_i: None, r_t: [-1098.666 -1098.666 -1098.666], eps: 0.1})
Step:  112900, Reward: [-425.849 -425.849 -425.849] [86.941], Avg: [-776.848 -776.848 -776.848] (0.0100) ({r_i: None, r_t: [-882.353 -882.353 -882.353], eps: 0.01})
Step:   23600, Reward: [-538.179 -538.179 -538.179] [90.105], Avg: [-696.971 -696.971 -696.971] (0.0939) ({r_i: None, r_t: [-1166.780 -1166.780 -1166.780], eps: 0.094})
Step:  147100, Reward: [-544.349 -544.349 -544.349] [170.894], Avg: [-704.907 -704.907 -704.907] (0.1000) ({r_i: None, r_t: [-1056.214 -1056.214 -1056.214], eps: 0.1})
Step:  113000, Reward: [-419.581 -419.581 -419.581] [84.597], Avg: [-776.533 -776.533 -776.533] (0.0100) ({r_i: None, r_t: [-847.158 -847.158 -847.158], eps: 0.01})
Step:   23700, Reward: [-587.392 -587.392 -587.392] [79.604], Avg: [-696.510 -696.510 -696.510] (0.0929) ({r_i: None, r_t: [-1142.608 -1142.608 -1142.608], eps: 0.093})
Step:  147200, Reward: [-572.899 -572.899 -572.899] [126.532], Avg: [-704.817 -704.817 -704.817] (0.1000) ({r_i: None, r_t: [-1050.483 -1050.483 -1050.483], eps: 0.1})
Step:  113100, Reward: [-420.731 -420.731 -420.731] [96.085], Avg: [-776.218 -776.218 -776.218] (0.0100) ({r_i: None, r_t: [-816.487 -816.487 -816.487], eps: 0.01})
Step:   23800, Reward: [-570.766 -570.766 -570.766] [136.516], Avg: [-695.984 -695.984 -695.984] (0.0920) ({r_i: None, r_t: [-1016.403 -1016.403 -1016.403], eps: 0.092})
Step:  147300, Reward: [-622.221 -622.221 -622.221] [120.109], Avg: [-704.761 -704.761 -704.761] (0.1000) ({r_i: None, r_t: [-1131.752 -1131.752 -1131.752], eps: 0.1})
Step:  113200, Reward: [-441.373 -441.373 -441.373] [89.963], Avg: [-775.923 -775.923 -775.923] (0.0100) ({r_i: None, r_t: [-888.198 -888.198 -888.198], eps: 0.01})
Step:   23900, Reward: [-597.972 -597.972 -597.972] [149.047], Avg: [-695.576 -695.576 -695.576] (0.0911) ({r_i: None, r_t: [-1136.465 -1136.465 -1136.465], eps: 0.091})
Step:  147400, Reward: [-572.165 -572.165 -572.165] [144.969], Avg: [-704.671 -704.671 -704.671] (0.1000) ({r_i: None, r_t: [-1084.976 -1084.976 -1084.976], eps: 0.1})
Step:  113300, Reward: [-447.711 -447.711 -447.711] [104.900], Avg: [-775.633 -775.633 -775.633] (0.0100) ({r_i: None, r_t: [-863.754 -863.754 -863.754], eps: 0.01})
Step:   24000, Reward: [-563.560 -563.560 -563.560] [190.006], Avg: [-695.028 -695.028 -695.028] (0.0902) ({r_i: None, r_t: [-1062.114 -1062.114 -1062.114], eps: 0.09})
Step:  147500, Reward: [-602.953 -602.953 -602.953] [119.013], Avg: [-704.602 -704.602 -704.602] (0.1000) ({r_i: None, r_t: [-1157.436 -1157.436 -1157.436], eps: 0.1})
Step:  113400, Reward: [-382.701 -382.701 -382.701] [97.525], Avg: [-775.287 -775.287 -775.287] (0.0100) ({r_i: None, r_t: [-791.193 -791.193 -791.193], eps: 0.01})
Step:   24100, Reward: [-549.230 -549.230 -549.230] [141.786], Avg: [-694.426 -694.426 -694.426] (0.0893) ({r_i: None, r_t: [-1098.921 -1098.921 -1098.921], eps: 0.089})
Step:  147600, Reward: [-542.892 -542.892 -542.892] [66.016], Avg: [-704.493 -704.493 -704.493] (0.1000) ({r_i: None, r_t: [-1113.169 -1113.169 -1113.169], eps: 0.1})
Step:  113500, Reward: [-402.610 -402.610 -402.610] [94.990], Avg: [-774.959 -774.959 -774.959] (0.0100) ({r_i: None, r_t: [-924.571 -924.571 -924.571], eps: 0.01})
Step:   24200, Reward: [-465.838 -465.838 -465.838] [56.574], Avg: [-693.485 -693.485 -693.485] (0.0884) ({r_i: None, r_t: [-1079.146 -1079.146 -1079.146], eps: 0.088})
Step:  147700, Reward: [-554.607 -554.607 -554.607] [104.817], Avg: [-704.391 -704.391 -704.391] (0.1000) ({r_i: None, r_t: [-1190.252 -1190.252 -1190.252], eps: 0.1})
Step:  113600, Reward: [-438.597 -438.597 -438.597] [95.654], Avg: [-774.663 -774.663 -774.663] (0.0100) ({r_i: None, r_t: [-846.489 -846.489 -846.489], eps: 0.01})
Step:   24300, Reward: [-498.677 -498.677 -498.677] [87.288], Avg: [-692.687 -692.687 -692.687] (0.0875) ({r_i: None, r_t: [-1029.348 -1029.348 -1029.348], eps: 0.088})
Step:  147800, Reward: [-577.486 -577.486 -577.486] [142.311], Avg: [-704.306 -704.306 -704.306] (0.1000) ({r_i: None, r_t: [-1032.406 -1032.406 -1032.406], eps: 0.1})
Step:  113700, Reward: [-489.709 -489.709 -489.709] [128.570], Avg: [-774.413 -774.413 -774.413] (0.0100) ({r_i: None, r_t: [-848.334 -848.334 -848.334], eps: 0.01})
Step:   24400, Reward: [-489.617 -489.617 -489.617] [94.717], Avg: [-691.858 -691.858 -691.858] (0.0866) ({r_i: None, r_t: [-977.916 -977.916 -977.916], eps: 0.087})
Step:  147900, Reward: [-505.630 -505.630 -505.630] [86.959], Avg: [-704.171 -704.171 -704.171] (0.1000) ({r_i: None, r_t: [-1113.473 -1113.473 -1113.473], eps: 0.1})
Step:  113800, Reward: [-499.594 -499.594 -499.594] [92.402], Avg: [-774.172 -774.172 -774.172] (0.0100) ({r_i: None, r_t: [-855.262 -855.262 -855.262], eps: 0.01})
Step:   24500, Reward: [-502.060 -502.060 -502.060] [134.560], Avg: [-691.086 -691.086 -691.086] (0.0858) ({r_i: None, r_t: [-995.624 -995.624 -995.624], eps: 0.086})
Step:  148000, Reward: [-629.122 -629.122 -629.122] [138.980], Avg: [-704.121 -704.121 -704.121] (0.1000) ({r_i: None, r_t: [-1204.121 -1204.121 -1204.121], eps: 0.1})
Step:  113900, Reward: [-452.064 -452.064 -452.064] [63.104], Avg: [-773.889 -773.889 -773.889] (0.0100) ({r_i: None, r_t: [-861.640 -861.640 -861.640], eps: 0.01})
Step:   24600, Reward: [-489.800 -489.800 -489.800] [81.578], Avg: [-690.271 -690.271 -690.271] (0.0849) ({r_i: None, r_t: [-912.250 -912.250 -912.250], eps: 0.085})
Step:  148100, Reward: [-567.733 -567.733 -567.733] [120.683], Avg: [-704.029 -704.029 -704.029] (0.1000) ({r_i: None, r_t: [-1040.667 -1040.667 -1040.667], eps: 0.1})
Step:  114000, Reward: [-418.494 -418.494 -418.494] [81.654], Avg: [-773.578 -773.578 -773.578] (0.0100) ({r_i: None, r_t: [-919.622 -919.622 -919.622], eps: 0.01})
Step:   24700, Reward: [-525.298 -525.298 -525.298] [91.858], Avg: [-689.606 -689.606 -689.606] (0.0841) ({r_i: None, r_t: [-983.831 -983.831 -983.831], eps: 0.084})
Step:  148200, Reward: [-572.104 -572.104 -572.104] [184.938], Avg: [-703.940 -703.940 -703.940] (0.1000) ({r_i: None, r_t: [-1127.741 -1127.741 -1127.741], eps: 0.1})
Step:  114100, Reward: [-504.663 -504.663 -504.663] [78.923], Avg: [-773.342 -773.342 -773.342] (0.0100) ({r_i: None, r_t: [-838.864 -838.864 -838.864], eps: 0.01})
Step:   24800, Reward: [-491.521 -491.521 -491.521] [106.796], Avg: [-688.811 -688.811 -688.811] (0.0832) ({r_i: None, r_t: [-978.405 -978.405 -978.405], eps: 0.083})
Step:  148300, Reward: [-581.371 -581.371 -581.371] [121.646], Avg: [-703.857 -703.857 -703.857] (0.1000) ({r_i: None, r_t: [-1096.147 -1096.147 -1096.147], eps: 0.1})
Step:  114200, Reward: [-436.999 -436.999 -436.999] [116.179], Avg: [-773.048 -773.048 -773.048] (0.0100) ({r_i: None, r_t: [-918.391 -918.391 -918.391], eps: 0.01})
Step:   24900, Reward: [-484.430 -484.430 -484.430] [90.157], Avg: [-687.993 -687.993 -687.993] (0.0824) ({r_i: None, r_t: [-952.020 -952.020 -952.020], eps: 0.082})
Step:  148400, Reward: [-459.646 -459.646 -459.646] [111.357], Avg: [-703.693 -703.693 -703.693] (0.1000) ({r_i: None, r_t: [-1166.720 -1166.720 -1166.720], eps: 0.1})
Step:  114300, Reward: [-449.756 -449.756 -449.756] [112.334], Avg: [-772.765 -772.765 -772.765] (0.0100) ({r_i: None, r_t: [-925.574 -925.574 -925.574], eps: 0.01})
Step:   25000, Reward: [-475.440 -475.440 -475.440] [59.755], Avg: [-687.146 -687.146 -687.146] (0.0816) ({r_i: None, r_t: [-961.458 -961.458 -961.458], eps: 0.082})
Step:  148500, Reward: [-537.048 -537.048 -537.048] [77.936], Avg: [-703.581 -703.581 -703.581] (0.1000) ({r_i: None, r_t: [-1043.541 -1043.541 -1043.541], eps: 0.1})
Step:  114400, Reward: [-408.836 -408.836 -408.836] [86.561], Avg: [-772.447 -772.447 -772.447] (0.0100) ({r_i: None, r_t: [-868.911 -868.911 -868.911], eps: 0.01})
Step:   25100, Reward: [-477.067 -477.067 -477.067] [73.008], Avg: [-686.313 -686.313 -686.313] (0.0808) ({r_i: None, r_t: [-991.483 -991.483 -991.483], eps: 0.081})
Step:  148600, Reward: [-530.443 -530.443 -530.443] [131.282], Avg: [-703.464 -703.464 -703.464] (0.1000) ({r_i: None, r_t: [-1092.653 -1092.653 -1092.653], eps: 0.1})
Step:  114500, Reward: [-423.278 -423.278 -423.278] [104.104], Avg: [-772.143 -772.143 -772.143] (0.0100) ({r_i: None, r_t: [-879.783 -879.783 -879.783], eps: 0.01})
Step:   25200, Reward: [-459.291 -459.291 -459.291] [81.975], Avg: [-685.415 -685.415 -685.415] (0.0800) ({r_i: None, r_t: [-1038.116 -1038.116 -1038.116], eps: 0.08})
Step:  148700, Reward: [-570.473 -570.473 -570.473] [135.037], Avg: [-703.375 -703.375 -703.375] (0.1000) ({r_i: None, r_t: [-1034.962 -1034.962 -1034.962], eps: 0.1})
Step:  114600, Reward: [-444.203 -444.203 -444.203] [88.979], Avg: [-771.857 -771.857 -771.857] (0.0100) ({r_i: None, r_t: [-890.954 -890.954 -890.954], eps: 0.01})
Step:  148800, Reward: [-555.998 -555.998 -555.998] [100.164], Avg: [-703.276 -703.276 -703.276] (0.1000) ({r_i: None, r_t: [-1072.945 -1072.945 -1072.945], eps: 0.1})
Step:   25300, Reward: [-486.560 -486.560 -486.560] [115.793], Avg: [-684.632 -684.632 -684.632] (0.0792) ({r_i: None, r_t: [-912.900 -912.900 -912.900], eps: 0.079})
Step:  114700, Reward: [-414.644 -414.644 -414.644] [75.938], Avg: [-771.546 -771.546 -771.546] (0.0100) ({r_i: None, r_t: [-863.853 -863.853 -863.853], eps: 0.01})
Step:  148900, Reward: [-493.064 -493.064 -493.064] [90.677], Avg: [-703.135 -703.135 -703.135] (0.1000) ({r_i: None, r_t: [-1045.998 -1045.998 -1045.998], eps: 0.1})
Step:   25400, Reward: [-468.800 -468.800 -468.800] [70.657], Avg: [-683.786 -683.786 -683.786] (0.0784) ({r_i: None, r_t: [-942.015 -942.015 -942.015], eps: 0.078})
Step:  114800, Reward: [-398.570 -398.570 -398.570] [92.582], Avg: [-771.221 -771.221 -771.221] (0.0100) ({r_i: None, r_t: [-933.322 -933.322 -933.322], eps: 0.01})
Step:  149000, Reward: [-572.793 -572.793 -572.793] [126.852], Avg: [-703.047 -703.047 -703.047] (0.1000) ({r_i: None, r_t: [-1078.533 -1078.533 -1078.533], eps: 0.1})
Step:   25500, Reward: [-445.532 -445.532 -445.532] [72.184], Avg: [-682.855 -682.855 -682.855] (0.0776) ({r_i: None, r_t: [-952.731 -952.731 -952.731], eps: 0.078})
Step:  114900, Reward: [-424.576 -424.576 -424.576] [102.769], Avg: [-770.920 -770.920 -770.920] (0.0100) ({r_i: None, r_t: [-817.371 -817.371 -817.371], eps: 0.01})
Step:  149100, Reward: [-531.875 -531.875 -531.875] [182.126], Avg: [-702.933 -702.933 -702.933] (0.1000) ({r_i: None, r_t: [-1072.098 -1072.098 -1072.098], eps: 0.1})
Step:   25600, Reward: [-418.124 -418.124 -418.124] [42.812], Avg: [-681.825 -681.825 -681.825] (0.0768) ({r_i: None, r_t: [-886.215 -886.215 -886.215], eps: 0.077})
Step:  115000, Reward: [-432.439 -432.439 -432.439] [75.650], Avg: [-770.625 -770.625 -770.625] (0.0100) ({r_i: None, r_t: [-879.757 -879.757 -879.757], eps: 0.01})
Step:  149200, Reward: [-542.938 -542.938 -542.938] [148.537], Avg: [-702.825 -702.825 -702.825] (0.1000) ({r_i: None, r_t: [-1102.225 -1102.225 -1102.225], eps: 0.1})
Step:   25700, Reward: [-434.412 -434.412 -434.412] [60.546], Avg: [-680.866 -680.866 -680.866] (0.0760) ({r_i: None, r_t: [-830.480 -830.480 -830.480], eps: 0.076})
Step:  115100, Reward: [-438.723 -438.723 -438.723] [56.055], Avg: [-770.337 -770.337 -770.337] (0.0100) ({r_i: None, r_t: [-927.573 -927.573 -927.573], eps: 0.01})
Step:  149300, Reward: [-559.372 -559.372 -559.372] [111.656], Avg: [-702.729 -702.729 -702.729] (0.1000) ({r_i: None, r_t: [-1080.677 -1080.677 -1080.677], eps: 0.1})
Step:   25800, Reward: [-449.499 -449.499 -449.499] [57.872], Avg: [-679.973 -679.973 -679.973] (0.0753) ({r_i: None, r_t: [-878.706 -878.706 -878.706], eps: 0.075})
Step:  149400, Reward: [-533.042 -533.042 -533.042] [126.005], Avg: [-702.616 -702.616 -702.616] (0.1000) ({r_i: None, r_t: [-1186.398 -1186.398 -1186.398], eps: 0.1})
Step:  115200, Reward: [-466.431 -466.431 -466.431] [95.161], Avg: [-770.074 -770.074 -770.074] (0.0100) ({r_i: None, r_t: [-840.572 -840.572 -840.572], eps: 0.01})
Step:   25900, Reward: [-434.716 -434.716 -434.716] [70.309], Avg: [-679.030 -679.030 -679.030] (0.0745) ({r_i: None, r_t: [-937.102 -937.102 -937.102], eps: 0.075})
Step:  149500, Reward: [-582.515 -582.515 -582.515] [139.094], Avg: [-702.536 -702.536 -702.536] (0.1000) ({r_i: None, r_t: [-1136.401 -1136.401 -1136.401], eps: 0.1})
Step:  115300, Reward: [-473.313 -473.313 -473.313] [116.390], Avg: [-769.817 -769.817 -769.817] (0.0100) ({r_i: None, r_t: [-859.888 -859.888 -859.888], eps: 0.01})
Step:   26000, Reward: [-443.564 -443.564 -443.564] [86.066], Avg: [-678.127 -678.127 -678.127] (0.0738) ({r_i: None, r_t: [-867.862 -867.862 -867.862], eps: 0.074})
Step:  149600, Reward: [-517.841 -517.841 -517.841] [70.841], Avg: [-702.412 -702.412 -702.412] (0.1000) ({r_i: None, r_t: [-1099.278 -1099.278 -1099.278], eps: 0.1})
Step:  115400, Reward: [-388.261 -388.261 -388.261] [64.517], Avg: [-769.486 -769.486 -769.486] (0.0100) ({r_i: None, r_t: [-879.694 -879.694 -879.694], eps: 0.01})
Step:   26100, Reward: [-423.745 -423.745 -423.745] [50.799], Avg: [-677.156 -677.156 -677.156] (0.0731) ({r_i: None, r_t: [-891.159 -891.159 -891.159], eps: 0.073})
Step:  149700, Reward: [-585.957 -585.957 -585.957] [173.359], Avg: [-702.334 -702.334 -702.334] (0.1000) ({r_i: None, r_t: [-1179.119 -1179.119 -1179.119], eps: 0.1})
Step:  115500, Reward: [-419.936 -419.936 -419.936] [67.632], Avg: [-769.184 -769.184 -769.184] (0.0100) ({r_i: None, r_t: [-916.192 -916.192 -916.192], eps: 0.01})
Step:   26200, Reward: [-433.219 -433.219 -433.219] [45.202], Avg: [-676.229 -676.229 -676.229] (0.0723) ({r_i: None, r_t: [-840.179 -840.179 -840.179], eps: 0.072})
Step:  149800, Reward: [-544.013 -544.013 -544.013] [104.863], Avg: [-702.229 -702.229 -702.229] (0.1000) ({r_i: None, r_t: [-1195.763 -1195.763 -1195.763], eps: 0.1})
Step:  115600, Reward: [-421.726 -421.726 -421.726] [84.110], Avg: [-768.884 -768.884 -768.884] (0.0100) ({r_i: None, r_t: [-862.675 -862.675 -862.675], eps: 0.01})
Step:   26300, Reward: [-424.457 -424.457 -424.457] [62.201], Avg: [-675.275 -675.275 -675.275] (0.0716) ({r_i: None, r_t: [-850.798 -850.798 -850.798], eps: 0.072})
Step:  149900, Reward: [-590.083 -590.083 -590.083] [131.468], Avg: [-702.154 -702.154 -702.154] (0.1000) ({r_i: None, r_t: [-1126.606 -1126.606 -1126.606], eps: 0.1})
Step:  115700, Reward: [-407.773 -407.773 -407.773] [72.555], Avg: [-768.572 -768.572 -768.572] (0.0100) ({r_i: None, r_t: [-901.326 -901.326 -901.326], eps: 0.01})
Step:   26400, Reward: [-467.599 -467.599 -467.599] [89.302], Avg: [-674.492 -674.492 -674.492] (0.0709) ({r_i: None, r_t: [-866.503 -866.503 -866.503], eps: 0.071})
Step:  150000, Reward: [-496.930 -496.930 -496.930] [108.483], Avg: [-702.017 -702.017 -702.017] (0.1000) ({r_i: None, r_t: [-1082.013 -1082.013 -1082.013], eps: 0.1})
Step:  115800, Reward: [-447.984 -447.984 -447.984] [88.550], Avg: [-768.295 -768.295 -768.295] (0.0100) ({r_i: None, r_t: [-859.193 -859.193 -859.193], eps: 0.01})
Step:   26500, Reward: [-420.233 -420.233 -420.233] [71.203], Avg: [-673.536 -673.536 -673.536] (0.0702) ({r_i: None, r_t: [-844.112 -844.112 -844.112], eps: 0.07})
Step:  150100, Reward: [-534.176 -534.176 -534.176] [111.975], Avg: [-701.906 -701.906 -701.906] (0.1000) ({r_i: None, r_t: [-1152.928 -1152.928 -1152.928], eps: 0.1})
Step:  115900, Reward: [-432.792 -432.792 -432.792] [79.204], Avg: [-768.006 -768.006 -768.006] (0.0100) ({r_i: None, r_t: [-933.648 -933.648 -933.648], eps: 0.01})
Step:   26600, Reward: [-402.227 -402.227 -402.227] [64.918], Avg: [-672.520 -672.520 -672.520] (0.0695) ({r_i: None, r_t: [-897.447 -897.447 -897.447], eps: 0.069})
Step:  150200, Reward: [-536.514 -536.514 -536.514] [176.018], Avg: [-701.796 -701.796 -701.796] (0.1000) ({r_i: None, r_t: [-1144.516 -1144.516 -1144.516], eps: 0.1})
Step:  116000, Reward: [-411.925 -411.925 -411.925] [64.444], Avg: [-767.699 -767.699 -767.699] (0.0100) ({r_i: None, r_t: [-892.787 -892.787 -892.787], eps: 0.01})
Step:   26700, Reward: [-410.575 -410.575 -410.575] [48.723], Avg: [-671.542 -671.542 -671.542] (0.0688) ({r_i: None, r_t: [-847.087 -847.087 -847.087], eps: 0.069})
Step:  150300, Reward: [-558.951 -558.951 -558.951] [180.668], Avg: [-701.701 -701.701 -701.701] (0.1000) ({r_i: None, r_t: [-1158.342 -1158.342 -1158.342], eps: 0.1})
Step:  116100, Reward: [-474.879 -474.879 -474.879] [102.602], Avg: [-767.447 -767.447 -767.447] (0.0100) ({r_i: None, r_t: [-831.361 -831.361 -831.361], eps: 0.01})
Step:   26800, Reward: [-433.371 -433.371 -433.371] [54.933], Avg: [-670.657 -670.657 -670.657] (0.0681) ({r_i: None, r_t: [-811.756 -811.756 -811.756], eps: 0.068})
Step:  150400, Reward: [-537.441 -537.441 -537.441] [119.974], Avg: [-701.591 -701.591 -701.591] (0.1000) ({r_i: None, r_t: [-1152.788 -1152.788 -1152.788], eps: 0.1})
Step:  116200, Reward: [-468.874 -468.874 -468.874] [78.855], Avg: [-767.190 -767.190 -767.190] (0.0100) ({r_i: None, r_t: [-846.160 -846.160 -846.160], eps: 0.01})
Step:   26900, Reward: [-425.467 -425.467 -425.467] [55.371], Avg: [-669.749 -669.749 -669.749] (0.0674) ({r_i: None, r_t: [-822.989 -822.989 -822.989], eps: 0.067})
Step:  150500, Reward: [-521.131 -521.131 -521.131] [104.099], Avg: [-701.472 -701.472 -701.472] (0.1000) ({r_i: None, r_t: [-1155.649 -1155.649 -1155.649], eps: 0.1})
Step:  116300, Reward: [-426.063 -426.063 -426.063] [75.993], Avg: [-766.897 -766.897 -766.897] (0.0100) ({r_i: None, r_t: [-805.228 -805.228 -805.228], eps: 0.01})
Step:   27000, Reward: [-424.283 -424.283 -424.283] [97.604], Avg: [-668.843 -668.843 -668.843] (0.0668) ({r_i: None, r_t: [-853.270 -853.270 -853.270], eps: 0.067})
Step:  150600, Reward: [-534.500 -534.500 -534.500] [107.190], Avg: [-701.361 -701.361 -701.361] (0.1000) ({r_i: None, r_t: [-1152.533 -1152.533 -1152.533], eps: 0.1})
Step:  116400, Reward: [-417.990 -417.990 -417.990] [57.506], Avg: [-766.598 -766.598 -766.598] (0.0100) ({r_i: None, r_t: [-864.662 -864.662 -864.662], eps: 0.01})
Step:   27100, Reward: [-460.605 -460.605 -460.605] [77.295], Avg: [-668.077 -668.077 -668.077] (0.0661) ({r_i: None, r_t: [-857.401 -857.401 -857.401], eps: 0.066})
Step:  150700, Reward: [-584.768 -584.768 -584.768] [96.491], Avg: [-701.284 -701.284 -701.284] (0.1000) ({r_i: None, r_t: [-1056.054 -1056.054 -1056.054], eps: 0.1})
Step:  116500, Reward: [-407.573 -407.573 -407.573] [84.492], Avg: [-766.290 -766.290 -766.290] (0.0100) ({r_i: None, r_t: [-831.027 -831.027 -831.027], eps: 0.01})
Step:   27200, Reward: [-452.843 -452.843 -452.843] [76.485], Avg: [-667.289 -667.289 -667.289] (0.0654) ({r_i: None, r_t: [-863.769 -863.769 -863.769], eps: 0.065})
Step:  150800, Reward: [-546.674 -546.674 -546.674] [101.651], Avg: [-701.181 -701.181 -701.181] (0.1000) ({r_i: None, r_t: [-1102.092 -1102.092 -1102.092], eps: 0.1})
Step:  116600, Reward: [-435.835 -435.835 -435.835] [76.471], Avg: [-766.007 -766.007 -766.007] (0.0100) ({r_i: None, r_t: [-872.325 -872.325 -872.325], eps: 0.01})
Step:   27300, Reward: [-474.114 -474.114 -474.114] [79.146], Avg: [-666.584 -666.584 -666.584] (0.0648) ({r_i: None, r_t: [-926.200 -926.200 -926.200], eps: 0.065})
Step:  150900, Reward: [-545.688 -545.688 -545.688] [120.283], Avg: [-701.078 -701.078 -701.078] (0.1000) ({r_i: None, r_t: [-1052.856 -1052.856 -1052.856], eps: 0.1})
Step:  116700, Reward: [-454.032 -454.032 -454.032] [69.481], Avg: [-765.740 -765.740 -765.740] (0.0100) ({r_i: None, r_t: [-806.276 -806.276 -806.276], eps: 0.01})
Step:   27400, Reward: [-451.442 -451.442 -451.442] [86.171], Avg: [-665.802 -665.802 -665.802] (0.0641) ({r_i: None, r_t: [-907.989 -907.989 -907.989], eps: 0.064})
Step:  151000, Reward: [-557.796 -557.796 -557.796] [168.121], Avg: [-700.983 -700.983 -700.983] (0.1000) ({r_i: None, r_t: [-1044.282 -1044.282 -1044.282], eps: 0.1})
Step:  116800, Reward: [-447.015 -447.015 -447.015] [90.716], Avg: [-765.467 -765.467 -765.467] (0.0100) ({r_i: None, r_t: [-808.670 -808.670 -808.670], eps: 0.01})
Step:  151100, Reward: [-599.021 -599.021 -599.021] [164.301], Avg: [-700.916 -700.916 -700.916] (0.1000) ({r_i: None, r_t: [-1079.098 -1079.098 -1079.098], eps: 0.1})
Step:   27500, Reward: [-434.380 -434.380 -434.380] [63.811], Avg: [-664.963 -664.963 -664.963] (0.0635) ({r_i: None, r_t: [-867.286 -867.286 -867.286], eps: 0.063})
Step:  116900, Reward: [-457.866 -457.866 -457.866] [98.485], Avg: [-765.204 -765.204 -765.204] (0.0100) ({r_i: None, r_t: [-893.327 -893.327 -893.327], eps: 0.01})
Step:  151200, Reward: [-575.003 -575.003 -575.003] [130.255], Avg: [-700.833 -700.833 -700.833] (0.1000) ({r_i: None, r_t: [-1136.939 -1136.939 -1136.939], eps: 0.1})
Step:   27600, Reward: [-396.364 -396.364 -396.364] [54.907], Avg: [-663.993 -663.993 -663.993] (0.0629) ({r_i: None, r_t: [-874.306 -874.306 -874.306], eps: 0.063})
Step:  117000, Reward: [-430.851 -430.851 -430.851] [106.080], Avg: [-764.919 -764.919 -764.919] (0.0100) ({r_i: None, r_t: [-877.800 -877.800 -877.800], eps: 0.01})
Step:  151300, Reward: [-523.052 -523.052 -523.052] [92.166], Avg: [-700.715 -700.715 -700.715] (0.1000) ({r_i: None, r_t: [-1053.220 -1053.220 -1053.220], eps: 0.1})
Step:   27700, Reward: [-396.269 -396.269 -396.269] [78.170], Avg: [-663.030 -663.030 -663.030] (0.0622) ({r_i: None, r_t: [-854.586 -854.586 -854.586], eps: 0.062})
Step:  117100, Reward: [-446.356 -446.356 -446.356] [97.526], Avg: [-764.647 -764.647 -764.647] (0.0100) ({r_i: None, r_t: [-840.580 -840.580 -840.580], eps: 0.01})
Step:  151400, Reward: [-562.525 -562.525 -562.525] [134.513], Avg: [-700.624 -700.624 -700.624] (0.1000) ({r_i: None, r_t: [-1104.544 -1104.544 -1104.544], eps: 0.1})
Step:   27800, Reward: [-405.104 -405.104 -405.104] [43.527], Avg: [-662.106 -662.106 -662.106] (0.0616) ({r_i: None, r_t: [-907.731 -907.731 -907.731], eps: 0.062})
Step:  117200, Reward: [-451.565 -451.565 -451.565] [83.669], Avg: [-764.380 -764.380 -764.380] (0.0100) ({r_i: None, r_t: [-784.754 -784.754 -784.754], eps: 0.01})
Step:  151500, Reward: [-573.679 -573.679 -573.679] [148.849], Avg: [-700.540 -700.540 -700.540] (0.1000) ({r_i: None, r_t: [-1111.961 -1111.961 -1111.961], eps: 0.1})
Step:   27900, Reward: [-431.473 -431.473 -431.473] [60.984], Avg: [-661.282 -661.282 -661.282] (0.0610) ({r_i: None, r_t: [-829.196 -829.196 -829.196], eps: 0.061})
Step:  117300, Reward: [-436.495 -436.495 -436.495] [88.958], Avg: [-764.101 -764.101 -764.101] (0.0100) ({r_i: None, r_t: [-888.750 -888.750 -888.750], eps: 0.01})
Step:  151600, Reward: [-539.974 -539.974 -539.974] [133.080], Avg: [-700.434 -700.434 -700.434] (0.1000) ({r_i: None, r_t: [-1182.872 -1182.872 -1182.872], eps: 0.1})
Step:   28000, Reward: [-426.614 -426.614 -426.614] [55.235], Avg: [-660.447 -660.447 -660.447] (0.0604) ({r_i: None, r_t: [-816.189 -816.189 -816.189], eps: 0.06})
Step:  117400, Reward: [-428.887 -428.887 -428.887] [80.996], Avg: [-763.815 -763.815 -763.815] (0.0100) ({r_i: None, r_t: [-786.334 -786.334 -786.334], eps: 0.01})
Step:  151700, Reward: [-594.016 -594.016 -594.016] [143.668], Avg: [-700.364 -700.364 -700.364] (0.1000) ({r_i: None, r_t: [-1140.850 -1140.850 -1140.850], eps: 0.1})
Step:   28100, Reward: [-397.738 -397.738 -397.738] [57.472], Avg: [-659.516 -659.516 -659.516] (0.0598) ({r_i: None, r_t: [-795.107 -795.107 -795.107], eps: 0.06})
Step:  117500, Reward: [-422.382 -422.382 -422.382] [85.276], Avg: [-763.525 -763.525 -763.525] (0.0100) ({r_i: None, r_t: [-908.416 -908.416 -908.416], eps: 0.01})
Step:  151800, Reward: [-526.788 -526.788 -526.788] [117.676], Avg: [-700.250 -700.250 -700.250] (0.1000) ({r_i: None, r_t: [-1024.820 -1024.820 -1024.820], eps: 0.1})
Step:   28200, Reward: [-417.274 -417.274 -417.274] [71.414], Avg: [-658.660 -658.660 -658.660] (0.0592) ({r_i: None, r_t: [-869.980 -869.980 -869.980], eps: 0.059})
Step:  117600, Reward: [-446.169 -446.169 -446.169] [51.837], Avg: [-763.255 -763.255 -763.255] (0.0100) ({r_i: None, r_t: [-863.759 -863.759 -863.759], eps: 0.01})
Step:  151900, Reward: [-522.103 -522.103 -522.103] [98.710], Avg: [-700.133 -700.133 -700.133] (0.1000) ({r_i: None, r_t: [-1117.656 -1117.656 -1117.656], eps: 0.1})
Step:   28300, Reward: [-422.862 -422.862 -422.862] [74.420], Avg: [-657.829 -657.829 -657.829] (0.0586) ({r_i: None, r_t: [-840.961 -840.961 -840.961], eps: 0.059})
Step:  117700, Reward: [-418.397 -418.397 -418.397] [78.672], Avg: [-762.963 -762.963 -762.963] (0.0100) ({r_i: None, r_t: [-806.633 -806.633 -806.633], eps: 0.01})
Step:  152000, Reward: [-606.130 -606.130 -606.130] [84.013], Avg: [-700.071 -700.071 -700.071] (0.1000) ({r_i: None, r_t: [-1131.597 -1131.597 -1131.597], eps: 0.1})
Step:   28400, Reward: [-440.668 -440.668 -440.668] [84.574], Avg: [-657.067 -657.067 -657.067] (0.0580) ({r_i: None, r_t: [-816.354 -816.354 -816.354], eps: 0.058})
Step:  152100, Reward: [-553.695 -553.695 -553.695] [100.988], Avg: [-699.975 -699.975 -699.975] (0.1000) ({r_i: None, r_t: [-1108.667 -1108.667 -1108.667], eps: 0.1})
Step:  117800, Reward: [-419.281 -419.281 -419.281] [79.318], Avg: [-762.671 -762.671 -762.671] (0.0100) ({r_i: None, r_t: [-883.427 -883.427 -883.427], eps: 0.01})
Step:   28500, Reward: [-409.046 -409.046 -409.046] [52.112], Avg: [-656.200 -656.200 -656.200] (0.0574) ({r_i: None, r_t: [-830.464 -830.464 -830.464], eps: 0.057})
Step:  152200, Reward: [-558.260 -558.260 -558.260] [121.451], Avg: [-699.882 -699.882 -699.882] (0.1000) ({r_i: None, r_t: [-1035.701 -1035.701 -1035.701], eps: 0.1})
Step:  117900, Reward: [-422.453 -422.453 -422.453] [77.311], Avg: [-762.383 -762.383 -762.383] (0.0100) ({r_i: None, r_t: [-843.355 -843.355 -843.355], eps: 0.01})
Step:   28600, Reward: [-419.292 -419.292 -419.292] [49.756], Avg: [-655.375 -655.375 -655.375] (0.0569) ({r_i: None, r_t: [-826.565 -826.565 -826.565], eps: 0.057})
Step:  152300, Reward: [-568.369 -568.369 -568.369] [158.073], Avg: [-699.796 -699.796 -699.796] (0.1000) ({r_i: None, r_t: [-1167.301 -1167.301 -1167.301], eps: 0.1})
Step:  118000, Reward: [-452.124 -452.124 -452.124] [120.187], Avg: [-762.120 -762.120 -762.120] (0.0100) ({r_i: None, r_t: [-904.514 -904.514 -904.514], eps: 0.01})
Step:   28700, Reward: [-446.304 -446.304 -446.304] [74.158], Avg: [-654.649 -654.649 -654.649] (0.0563) ({r_i: None, r_t: [-826.470 -826.470 -826.470], eps: 0.056})
Step:  152400, Reward: [-594.837 -594.837 -594.837] [134.340], Avg: [-699.727 -699.727 -699.727] (0.1000) ({r_i: None, r_t: [-1105.540 -1105.540 -1105.540], eps: 0.1})
Step:  118100, Reward: [-492.562 -492.562 -492.562] [103.376], Avg: [-761.892 -761.892 -761.892] (0.0100) ({r_i: None, r_t: [-897.052 -897.052 -897.052], eps: 0.01})
Step:   28800, Reward: [-413.932 -413.932 -413.932] [59.456], Avg: [-653.816 -653.816 -653.816] (0.0557) ({r_i: None, r_t: [-834.161 -834.161 -834.161], eps: 0.056})
Step:  152500, Reward: [-565.779 -565.779 -565.779] [127.806], Avg: [-699.639 -699.639 -699.639] (0.1000) ({r_i: None, r_t: [-1071.891 -1071.891 -1071.891], eps: 0.1})
Step:  118200, Reward: [-485.788 -485.788 -485.788] [127.670], Avg: [-761.659 -761.659 -761.659] (0.0100) ({r_i: None, r_t: [-830.568 -830.568 -830.568], eps: 0.01})
Step:   28900, Reward: [-434.187 -434.187 -434.187] [72.910], Avg: [-653.058 -653.058 -653.058] (0.0552) ({r_i: None, r_t: [-810.244 -810.244 -810.244], eps: 0.055})
Step:  152600, Reward: [-524.570 -524.570 -524.570] [121.597], Avg: [-699.524 -699.524 -699.524] (0.1000) ({r_i: None, r_t: [-1101.114 -1101.114 -1101.114], eps: 0.1})
Step:  118300, Reward: [-435.544 -435.544 -435.544] [85.960], Avg: [-761.383 -761.383 -761.383] (0.0100) ({r_i: None, r_t: [-880.582 -880.582 -880.582], eps: 0.01})
Step:   29000, Reward: [-423.159 -423.159 -423.159] [53.768], Avg: [-652.268 -652.268 -652.268] (0.0546) ({r_i: None, r_t: [-830.719 -830.719 -830.719], eps: 0.055})
Step:  152700, Reward: [-537.453 -537.453 -537.453] [94.403], Avg: [-699.418 -699.418 -699.418] (0.1000) ({r_i: None, r_t: [-1198.565 -1198.565 -1198.565], eps: 0.1})
Step:  118400, Reward: [-424.446 -424.446 -424.446] [84.662], Avg: [-761.099 -761.099 -761.099] (0.0100) ({r_i: None, r_t: [-855.577 -855.577 -855.577], eps: 0.01})
Step:   29100, Reward: [-396.153 -396.153 -396.153] [56.228], Avg: [-651.391 -651.391 -651.391] (0.0541) ({r_i: None, r_t: [-832.353 -832.353 -832.353], eps: 0.054})
Step:  152800, Reward: [-549.734 -549.734 -549.734] [119.355], Avg: [-699.320 -699.320 -699.320] (0.1000) ({r_i: None, r_t: [-1113.325 -1113.325 -1113.325], eps: 0.1})
Step:  118500, Reward: [-411.684 -411.684 -411.684] [59.380], Avg: [-760.804 -760.804 -760.804] (0.0100) ({r_i: None, r_t: [-886.978 -886.978 -886.978], eps: 0.01})
Step:   29200, Reward: [-443.198 -443.198 -443.198] [70.912], Avg: [-650.681 -650.681 -650.681] (0.0535) ({r_i: None, r_t: [-835.399 -835.399 -835.399], eps: 0.054})
Step:  152900, Reward: [-536.510 -536.510 -536.510] [111.344], Avg: [-699.214 -699.214 -699.214] (0.1000) ({r_i: None, r_t: [-1134.505 -1134.505 -1134.505], eps: 0.1})
Step:  118600, Reward: [-415.234 -415.234 -415.234] [104.109], Avg: [-760.513 -760.513 -760.513] (0.0100) ({r_i: None, r_t: [-890.688 -890.688 -890.688], eps: 0.01})
Step:   29300, Reward: [-411.954 -411.954 -411.954] [68.055], Avg: [-649.869 -649.869 -649.869] (0.0530) ({r_i: None, r_t: [-810.921 -810.921 -810.921], eps: 0.053})
Step:  153000, Reward: [-620.699 -620.699 -620.699] [148.331], Avg: [-699.163 -699.163 -699.163] (0.1000) ({r_i: None, r_t: [-1121.866 -1121.866 -1121.866], eps: 0.1})
Step:  118700, Reward: [-427.256 -427.256 -427.256] [95.804], Avg: [-760.233 -760.233 -760.233] (0.0100) ({r_i: None, r_t: [-883.753 -883.753 -883.753], eps: 0.01})
Step:  153100, Reward: [-540.512 -540.512 -540.512] [90.799], Avg: [-699.059 -699.059 -699.059] (0.1000) ({r_i: None, r_t: [-1109.708 -1109.708 -1109.708], eps: 0.1})
Step:   29400, Reward: [-437.695 -437.695 -437.695] [85.999], Avg: [-649.149 -649.149 -649.149] (0.0525) ({r_i: None, r_t: [-826.077 -826.077 -826.077], eps: 0.052})
Step:  118800, Reward: [-407.214 -407.214 -407.214] [77.698], Avg: [-759.936 -759.936 -759.936] (0.0100) ({r_i: None, r_t: [-788.721 -788.721 -788.721], eps: 0.01})
Step:  153200, Reward: [-610.615 -610.615 -610.615] [126.690], Avg: [-699.001 -699.001 -699.001] (0.1000) ({r_i: None, r_t: [-1177.201 -1177.201 -1177.201], eps: 0.1})
Step:   29500, Reward: [-422.130 -422.130 -422.130] [63.128], Avg: [-648.383 -648.383 -648.383] (0.0520) ({r_i: None, r_t: [-844.058 -844.058 -844.058], eps: 0.052})
Step:  118900, Reward: [-430.642 -430.642 -430.642] [73.709], Avg: [-759.659 -759.659 -759.659] (0.0100) ({r_i: None, r_t: [-865.910 -865.910 -865.910], eps: 0.01})
Step:  153300, Reward: [-604.911 -604.911 -604.911] [170.707], Avg: [-698.940 -698.940 -698.940] (0.1000) ({r_i: None, r_t: [-1189.204 -1189.204 -1189.204], eps: 0.1})
Step:   29600, Reward: [-434.657 -434.657 -434.657] [86.115], Avg: [-647.663 -647.663 -647.663] (0.0514) ({r_i: None, r_t: [-835.342 -835.342 -835.342], eps: 0.051})
Step:  119000, Reward: [-411.779 -411.779 -411.779] [76.155], Avg: [-759.367 -759.367 -759.367] (0.0100) ({r_i: None, r_t: [-864.823 -864.823 -864.823], eps: 0.01})
Step:  153400, Reward: [-553.090 -553.090 -553.090] [102.903], Avg: [-698.845 -698.845 -698.845] (0.1000) ({r_i: None, r_t: [-1040.914 -1040.914 -1040.914], eps: 0.1})
Step:   29700, Reward: [-450.505 -450.505 -450.505] [100.271], Avg: [-647.001 -647.001 -647.001] (0.0509) ({r_i: None, r_t: [-841.538 -841.538 -841.538], eps: 0.051})
Step:  119100, Reward: [-424.124 -424.124 -424.124] [99.084], Avg: [-759.086 -759.086 -759.086] (0.0100) ({r_i: None, r_t: [-902.994 -902.994 -902.994], eps: 0.01})
Step:  153500, Reward: [-605.900 -605.900 -605.900] [130.877], Avg: [-698.784 -698.784 -698.784] (0.1000) ({r_i: None, r_t: [-1112.677 -1112.677 -1112.677], eps: 0.1})
Step:   29800, Reward: [-414.326 -414.326 -414.326] [67.638], Avg: [-646.223 -646.223 -646.223] (0.0504) ({r_i: None, r_t: [-853.052 -853.052 -853.052], eps: 0.05})
Step:  119200, Reward: [-429.312 -429.312 -429.312] [92.822], Avg: [-758.809 -758.809 -758.809] (0.0100) ({r_i: None, r_t: [-861.524 -861.524 -861.524], eps: 0.01})
Step:  153600, Reward: [-515.194 -515.194 -515.194] [116.265], Avg: [-698.665 -698.665 -698.665] (0.1000) ({r_i: None, r_t: [-1173.646 -1173.646 -1173.646], eps: 0.1})
Step:   29900, Reward: [-442.750 -442.750 -442.750] [69.821], Avg: [-645.545 -645.545 -645.545] (0.0499) ({r_i: None, r_t: [-809.538 -809.538 -809.538], eps: 0.05})
Step:  119300, Reward: [-395.095 -395.095 -395.095] [76.518], Avg: [-758.505 -758.505 -758.505] (0.0100) ({r_i: None, r_t: [-843.854 -843.854 -843.854], eps: 0.01})
Step:  153700, Reward: [-575.222 -575.222 -575.222] [114.007], Avg: [-698.585 -698.585 -698.585] (0.1000) ({r_i: None, r_t: [-1107.314 -1107.314 -1107.314], eps: 0.1})
Step:   30000, Reward: [-446.907 -446.907 -446.907] [96.295], Avg: [-644.885 -644.885 -644.885] (0.0494) ({r_i: None, r_t: [-848.328 -848.328 -848.328], eps: 0.049})
Step:  119400, Reward: [-426.961 -426.961 -426.961] [109.447], Avg: [-758.227 -758.227 -758.227] (0.0100) ({r_i: None, r_t: [-946.822 -946.822 -946.822], eps: 0.01})
Step:  153800, Reward: [-523.216 -523.216 -523.216] [139.772], Avg: [-698.471 -698.471 -698.471] (0.1000) ({r_i: None, r_t: [-1121.063 -1121.063 -1121.063], eps: 0.1})
Step:   30100, Reward: [-411.993 -411.993 -411.993] [58.567], Avg: [-644.114 -644.114 -644.114] (0.0489) ({r_i: None, r_t: [-845.642 -845.642 -845.642], eps: 0.049})
Step:  119500, Reward: [-432.630 -432.630 -432.630] [65.120], Avg: [-757.955 -757.955 -757.955] (0.0100) ({r_i: None, r_t: [-884.846 -884.846 -884.846], eps: 0.01})
Step:  153900, Reward: [-567.036 -567.036 -567.036] [165.585], Avg: [-698.385 -698.385 -698.385] (0.1000) ({r_i: None, r_t: [-1167.735 -1167.735 -1167.735], eps: 0.1})
Step:   30200, Reward: [-384.773 -384.773 -384.773] [51.085], Avg: [-643.258 -643.258 -643.258] (0.0484) ({r_i: None, r_t: [-794.074 -794.074 -794.074], eps: 0.048})
Step:  119600, Reward: [-422.632 -422.632 -422.632] [71.695], Avg: [-757.675 -757.675 -757.675] (0.0100) ({r_i: None, r_t: [-900.286 -900.286 -900.286], eps: 0.01})
Step:  154000, Reward: [-585.575 -585.575 -585.575] [150.351], Avg: [-698.312 -698.312 -698.312] (0.1000) ({r_i: None, r_t: [-1093.258 -1093.258 -1093.258], eps: 0.1})
Step:   30300, Reward: [-405.785 -405.785 -405.785] [55.206], Avg: [-642.477 -642.477 -642.477] (0.0479) ({r_i: None, r_t: [-802.881 -802.881 -802.881], eps: 0.048})
Step:  119700, Reward: [-461.587 -461.587 -461.587] [99.459], Avg: [-757.428 -757.428 -757.428] (0.0100) ({r_i: None, r_t: [-879.732 -879.732 -879.732], eps: 0.01})
Step:  154100, Reward: [-553.058 -553.058 -553.058] [161.169], Avg: [-698.218 -698.218 -698.218] (0.1000) ({r_i: None, r_t: [-1142.751 -1142.751 -1142.751], eps: 0.1})
Step:   30400, Reward: [-435.153 -435.153 -435.153] [59.708], Avg: [-641.797 -641.797 -641.797] (0.0475) ({r_i: None, r_t: [-833.943 -833.943 -833.943], eps: 0.047})
Step:  119800, Reward: [-396.934 -396.934 -396.934] [85.392], Avg: [-757.127 -757.127 -757.127] (0.0100) ({r_i: None, r_t: [-892.425 -892.425 -892.425], eps: 0.01})
Step:  154200, Reward: [-586.646 -586.646 -586.646] [123.238], Avg: [-698.146 -698.146 -698.146] (0.1000) ({r_i: None, r_t: [-1177.526 -1177.526 -1177.526], eps: 0.1})
Step:   30500, Reward: [-441.309 -441.309 -441.309] [120.895], Avg: [-641.142 -641.142 -641.142] (0.0470) ({r_i: None, r_t: [-847.577 -847.577 -847.577], eps: 0.047})
Step:  119900, Reward: [-408.174 -408.174 -408.174] [84.807], Avg: [-756.836 -756.836 -756.836] (0.0100) ({r_i: None, r_t: [-863.338 -863.338 -863.338], eps: 0.01})
Step:  154300, Reward: [-604.323 -604.323 -604.323] [148.816], Avg: [-698.085 -698.085 -698.085] (0.1000) ({r_i: None, r_t: [-1109.020 -1109.020 -1109.020], eps: 0.1})
Step:   30600, Reward: [-408.862 -408.862 -408.862] [58.874], Avg: [-640.385 -640.385 -640.385] (0.0465) ({r_i: None, r_t: [-817.908 -817.908 -817.908], eps: 0.047})
Step:  154400, Reward: [-609.754 -609.754 -609.754] [158.878], Avg: [-698.028 -698.028 -698.028] (0.1000) ({r_i: None, r_t: [-1264.889 -1264.889 -1264.889], eps: 0.1})
Step:  120000, Reward: [-436.202 -436.202 -436.202] [66.799], Avg: [-756.569 -756.569 -756.569] (0.0100) ({r_i: None, r_t: [-852.437 -852.437 -852.437], eps: 0.01})
Step:   30700, Reward: [-390.097 -390.097 -390.097] [91.199], Avg: [-639.573 -639.573 -639.573] (0.0461) ({r_i: None, r_t: [-835.697 -835.697 -835.697], eps: 0.046})
Step:  154500, Reward: [-573.916 -573.916 -573.916] [121.966], Avg: [-697.948 -697.948 -697.948] (0.1000) ({r_i: None, r_t: [-1211.054 -1211.054 -1211.054], eps: 0.1})
Step:  120100, Reward: [-479.670 -479.670 -479.670] [124.839], Avg: [-756.339 -756.339 -756.339] (0.0100) ({r_i: None, r_t: [-888.637 -888.637 -888.637], eps: 0.01})
Step:   30800, Reward: [-404.024 -404.024 -404.024] [67.228], Avg: [-638.810 -638.810 -638.810] (0.0456) ({r_i: None, r_t: [-772.868 -772.868 -772.868], eps: 0.046})
Step:  154600, Reward: [-617.703 -617.703 -617.703] [174.267], Avg: [-697.896 -697.896 -697.896] (0.1000) ({r_i: None, r_t: [-1186.468 -1186.468 -1186.468], eps: 0.1})
Step:  120200, Reward: [-416.678 -416.678 -416.678] [61.197], Avg: [-756.057 -756.057 -756.057] (0.0100) ({r_i: None, r_t: [-825.027 -825.027 -825.027], eps: 0.01})
Step:   30900, Reward: [-447.277 -447.277 -447.277] [56.455], Avg: [-638.192 -638.192 -638.192] (0.0452) ({r_i: None, r_t: [-813.319 -813.319 -813.319], eps: 0.045})
Step:  154700, Reward: [-630.756 -630.756 -630.756] [174.469], Avg: [-697.852 -697.852 -697.852] (0.1000) ({r_i: None, r_t: [-1153.911 -1153.911 -1153.911], eps: 0.1})
Step:  120300, Reward: [-484.789 -484.789 -484.789] [118.572], Avg: [-755.831 -755.831 -755.831] (0.0100) ({r_i: None, r_t: [-843.470 -843.470 -843.470], eps: 0.01})
Step:   31000, Reward: [-424.896 -424.896 -424.896] [73.392], Avg: [-637.507 -637.507 -637.507] (0.0447) ({r_i: None, r_t: [-785.388 -785.388 -785.388], eps: 0.045})
Step:  154800, Reward: [-636.277 -636.277 -636.277] [190.504], Avg: [-697.813 -697.813 -697.813] (0.1000) ({r_i: None, r_t: [-1113.116 -1113.116 -1113.116], eps: 0.1})
Step:  120400, Reward: [-452.654 -452.654 -452.654] [83.253], Avg: [-755.580 -755.580 -755.580] (0.0100) ({r_i: None, r_t: [-814.028 -814.028 -814.028], eps: 0.01})
Step:   31100, Reward: [-430.926 -430.926 -430.926] [61.945], Avg: [-636.844 -636.844 -636.844] (0.0443) ({r_i: None, r_t: [-828.095 -828.095 -828.095], eps: 0.044})
Step:  154900, Reward: [-530.461 -530.461 -530.461] [102.567], Avg: [-697.705 -697.705 -697.705] (0.1000) ({r_i: None, r_t: [-1184.857 -1184.857 -1184.857], eps: 0.1})
Step:  120500, Reward: [-453.279 -453.279 -453.279] [96.394], Avg: [-755.329 -755.329 -755.329] (0.0100) ({r_i: None, r_t: [-859.317 -859.317 -859.317], eps: 0.01})
Step:   31200, Reward: [-389.868 -389.868 -389.868] [42.367], Avg: [-636.055 -636.055 -636.055] (0.0438) ({r_i: None, r_t: [-858.799 -858.799 -858.799], eps: 0.044})
Step:  155000, Reward: [-599.537 -599.537 -599.537] [176.843], Avg: [-697.641 -697.641 -697.641] (0.1000) ({r_i: None, r_t: [-1059.330 -1059.330 -1059.330], eps: 0.1})
Step:  120600, Reward: [-429.627 -429.627 -429.627] [98.600], Avg: [-755.059 -755.059 -755.059] (0.0100) ({r_i: None, r_t: [-879.846 -879.846 -879.846], eps: 0.01})
Step:   31300, Reward: [-404.149 -404.149 -404.149] [59.318], Avg: [-635.317 -635.317 -635.317] (0.0434) ({r_i: None, r_t: [-863.312 -863.312 -863.312], eps: 0.043})
Step:  155100, Reward: [-624.639 -624.639 -624.639] [179.950], Avg: [-697.594 -697.594 -697.594] (0.1000) ({r_i: None, r_t: [-1182.767 -1182.767 -1182.767], eps: 0.1})
Step:  120700, Reward: [-410.148 -410.148 -410.148] [73.417], Avg: [-754.774 -754.774 -754.774] (0.0100) ({r_i: None, r_t: [-893.537 -893.537 -893.537], eps: 0.01})
Step:  155200, Reward: [-593.163 -593.163 -593.163] [104.170], Avg: [-697.527 -697.527 -697.527] (0.1000) ({r_i: None, r_t: [-1215.024 -1215.024 -1215.024], eps: 0.1})
Step:   31400, Reward: [-405.720 -405.720 -405.720] [50.561], Avg: [-634.588 -634.588 -634.588] (0.0429) ({r_i: None, r_t: [-789.969 -789.969 -789.969], eps: 0.043})
Step:  120800, Reward: [-454.813 -454.813 -454.813] [86.931], Avg: [-754.525 -754.525 -754.525] (0.0100) ({r_i: None, r_t: [-885.017 -885.017 -885.017], eps: 0.01})
Step:  155300, Reward: [-555.267 -555.267 -555.267] [98.091], Avg: [-697.435 -697.435 -697.435] (0.1000) ({r_i: None, r_t: [-1193.887 -1193.887 -1193.887], eps: 0.1})
Step:   31500, Reward: [-414.726 -414.726 -414.726] [65.076], Avg: [-633.892 -633.892 -633.892] (0.0425) ({r_i: None, r_t: [-857.554 -857.554 -857.554], eps: 0.043})
Step:  120900, Reward: [-450.358 -450.358 -450.358] [90.776], Avg: [-754.274 -754.274 -754.274] (0.0100) ({r_i: None, r_t: [-878.872 -878.872 -878.872], eps: 0.01})
Step:  155400, Reward: [-503.229 -503.229 -503.229] [86.886], Avg: [-697.311 -697.311 -697.311] (0.1000) ({r_i: None, r_t: [-1160.996 -1160.996 -1160.996], eps: 0.1})
Step:   31600, Reward: [-392.058 -392.058 -392.058] [61.281], Avg: [-633.129 -633.129 -633.129] (0.0421) ({r_i: None, r_t: [-816.923 -816.923 -816.923], eps: 0.042})
Step:  121000, Reward: [-411.873 -411.873 -411.873] [96.251], Avg: [-753.991 -753.991 -753.991] (0.0100) ({r_i: None, r_t: [-834.517 -834.517 -834.517], eps: 0.01})
Step:  155500, Reward: [-704.583 -704.583 -704.583] [240.167], Avg: [-697.315 -697.315 -697.315] (0.1000) ({r_i: None, r_t: [-1167.633 -1167.633 -1167.633], eps: 0.1})
Step:   31700, Reward: [-417.314 -417.314 -417.314] [62.704], Avg: [-632.451 -632.451 -632.451] (0.0417) ({r_i: None, r_t: [-826.347 -826.347 -826.347], eps: 0.042})
Step:  121100, Reward: [-426.989 -426.989 -426.989] [99.115], Avg: [-753.722 -753.722 -753.722] (0.0100) ({r_i: None, r_t: [-823.188 -823.188 -823.188], eps: 0.01})
Step:  155600, Reward: [-542.160 -542.160 -542.160] [123.622], Avg: [-697.216 -697.216 -697.216] (0.1000) ({r_i: None, r_t: [-1186.336 -1186.336 -1186.336], eps: 0.1})
Step:   31800, Reward: [-424.844 -424.844 -424.844] [61.815], Avg: [-631.800 -631.800 -631.800] (0.0413) ({r_i: None, r_t: [-839.778 -839.778 -839.778], eps: 0.041})
Step:  121200, Reward: [-446.137 -446.137 -446.137] [103.513], Avg: [-753.468 -753.468 -753.468] (0.0100) ({r_i: None, r_t: [-823.591 -823.591 -823.591], eps: 0.01})
Step:  155700, Reward: [-564.159 -564.159 -564.159] [125.313], Avg: [-697.130 -697.130 -697.130] (0.1000) ({r_i: None, r_t: [-1220.218 -1220.218 -1220.218], eps: 0.1})
Step:   31900, Reward: [-392.355 -392.355 -392.355] [68.553], Avg: [-631.052 -631.052 -631.052] (0.0408) ({r_i: None, r_t: [-816.748 -816.748 -816.748], eps: 0.041})
Step:  121300, Reward: [-436.710 -436.710 -436.710] [92.258], Avg: [-753.207 -753.207 -753.207] (0.0100) ({r_i: None, r_t: [-844.662 -844.662 -844.662], eps: 0.01})
Step:  155800, Reward: [-638.433 -638.433 -638.433] [180.139], Avg: [-697.093 -697.093 -697.093] (0.1000) ({r_i: None, r_t: [-1197.306 -1197.306 -1197.306], eps: 0.1})
Step:   32000, Reward: [-396.406 -396.406 -396.406] [73.602], Avg: [-630.321 -630.321 -630.321] (0.0404) ({r_i: None, r_t: [-797.177 -797.177 -797.177], eps: 0.04})
Step:  121400, Reward: [-455.759 -455.759 -455.759] [127.716], Avg: [-752.962 -752.962 -752.962] (0.0100) ({r_i: None, r_t: [-876.111 -876.111 -876.111], eps: 0.01})
Step:  155900, Reward: [-604.637 -604.637 -604.637] [146.476], Avg: [-697.033 -697.033 -697.033] (0.1000) ({r_i: None, r_t: [-1165.091 -1165.091 -1165.091], eps: 0.1})
Step:   32100, Reward: [-374.143 -374.143 -374.143] [57.436], Avg: [-629.525 -629.525 -629.525] (0.0400) ({r_i: None, r_t: [-768.995 -768.995 -768.995], eps: 0.04})
Step:  121500, Reward: [-436.321 -436.321 -436.321] [102.544], Avg: [-752.702 -752.702 -752.702] (0.0100) ({r_i: None, r_t: [-901.084 -901.084 -901.084], eps: 0.01})
Step:  156000, Reward: [-585.586 -585.586 -585.586] [166.330], Avg: [-696.962 -696.962 -696.962] (0.1000) ({r_i: None, r_t: [-1088.922 -1088.922 -1088.922], eps: 0.1})
Step:  121600, Reward: [-460.593 -460.593 -460.593] [81.411], Avg: [-752.462 -752.462 -752.462] (0.0100) ({r_i: None, r_t: [-846.375 -846.375 -846.375], eps: 0.01})
Step:   32200, Reward: [-417.975 -417.975 -417.975] [91.417], Avg: [-628.870 -628.870 -628.870] (0.0396) ({r_i: None, r_t: [-783.469 -783.469 -783.469], eps: 0.04})
Step:  156100, Reward: [-534.404 -534.404 -534.404] [114.977], Avg: [-696.858 -696.858 -696.858] (0.1000) ({r_i: None, r_t: [-1181.509 -1181.509 -1181.509], eps: 0.1})
Step:   32300, Reward: [-401.589 -401.589 -401.589] [72.525], Avg: [-628.169 -628.169 -628.169] (0.0392) ({r_i: None, r_t: [-780.107 -780.107 -780.107], eps: 0.039})
Step:  121700, Reward: [-387.189 -387.189 -387.189] [89.039], Avg: [-752.162 -752.162 -752.162] (0.0100) ({r_i: None, r_t: [-852.469 -852.469 -852.469], eps: 0.01})
Step:  156200, Reward: [-598.160 -598.160 -598.160] [138.256], Avg: [-696.795 -696.795 -696.795] (0.1000) ({r_i: None, r_t: [-1187.769 -1187.769 -1187.769], eps: 0.1})
Step:  121800, Reward: [-402.691 -402.691 -402.691] [78.719], Avg: [-751.875 -751.875 -751.875] (0.0100) ({r_i: None, r_t: [-833.587 -833.587 -833.587], eps: 0.01})
Step:   32400, Reward: [-393.989 -393.989 -393.989] [62.340], Avg: [-627.448 -627.448 -627.448] (0.0388) ({r_i: None, r_t: [-764.585 -764.585 -764.585], eps: 0.039})
Step:  156300, Reward: [-611.944 -611.944 -611.944] [120.630], Avg: [-696.740 -696.740 -696.740] (0.1000) ({r_i: None, r_t: [-1195.290 -1195.290 -1195.290], eps: 0.1})
Step:  121900, Reward: [-425.725 -425.725 -425.725] [84.728], Avg: [-751.608 -751.608 -751.608] (0.0100) ({r_i: None, r_t: [-898.813 -898.813 -898.813], eps: 0.01})
Step:   32500, Reward: [-392.384 -392.384 -392.384] [69.016], Avg: [-626.727 -626.727 -626.727] (0.0385) ({r_i: None, r_t: [-796.362 -796.362 -796.362], eps: 0.038})
Step:  156400, Reward: [-603.925 -603.925 -603.925] [141.943], Avg: [-696.681 -696.681 -696.681] (0.1000) ({r_i: None, r_t: [-1205.648 -1205.648 -1205.648], eps: 0.1})
Step:  122000, Reward: [-424.466 -424.466 -424.466] [91.151], Avg: [-751.340 -751.340 -751.340] (0.0100) ({r_i: None, r_t: [-847.443 -847.443 -847.443], eps: 0.01})
Step:   32600, Reward: [-411.620 -411.620 -411.620] [55.724], Avg: [-626.069 -626.069 -626.069] (0.0381) ({r_i: None, r_t: [-830.165 -830.165 -830.165], eps: 0.038})
Step:  156500, Reward: [-591.792 -591.792 -591.792] [239.769], Avg: [-696.614 -696.614 -696.614] (0.1000) ({r_i: None, r_t: [-1212.545 -1212.545 -1212.545], eps: 0.1})
Step:  122100, Reward: [-425.509 -425.509 -425.509] [72.723], Avg: [-751.073 -751.073 -751.073] (0.0100) ({r_i: None, r_t: [-861.229 -861.229 -861.229], eps: 0.01})
Step:   32700, Reward: [-410.340 -410.340 -410.340] [72.095], Avg: [-625.411 -625.411 -625.411] (0.0377) ({r_i: None, r_t: [-809.341 -809.341 -809.341], eps: 0.038})
Step:  156600, Reward: [-648.299 -648.299 -648.299] [139.176], Avg: [-696.583 -696.583 -696.583] (0.1000) ({r_i: None, r_t: [-1269.850 -1269.850 -1269.850], eps: 0.1})
Step:  122200, Reward: [-445.196 -445.196 -445.196] [111.887], Avg: [-750.823 -750.823 -750.823] (0.0100) ({r_i: None, r_t: [-857.769 -857.769 -857.769], eps: 0.01})
Step:   32800, Reward: [-372.872 -372.872 -372.872] [59.606], Avg: [-624.644 -624.644 -624.644] (0.0373) ({r_i: None, r_t: [-868.189 -868.189 -868.189], eps: 0.037})
Step:  156700, Reward: [-605.420 -605.420 -605.420] [160.053], Avg: [-696.525 -696.525 -696.525] (0.1000) ({r_i: None, r_t: [-1290.255 -1290.255 -1290.255], eps: 0.1})
Step:  122300, Reward: [-468.101 -468.101 -468.101] [91.118], Avg: [-750.592 -750.592 -750.592] (0.0100) ({r_i: None, r_t: [-889.816 -889.816 -889.816], eps: 0.01})
Step:   32900, Reward: [-414.020 -414.020 -414.020] [67.616], Avg: [-624.006 -624.006 -624.006] (0.0369) ({r_i: None, r_t: [-815.130 -815.130 -815.130], eps: 0.037})
Step:  156800, Reward: [-550.519 -550.519 -550.519] [99.509], Avg: [-696.432 -696.432 -696.432] (0.1000) ({r_i: None, r_t: [-1178.227 -1178.227 -1178.227], eps: 0.1})
Step:  122400, Reward: [-465.729 -465.729 -465.729] [103.396], Avg: [-750.360 -750.360 -750.360] (0.0100) ({r_i: None, r_t: [-877.414 -877.414 -877.414], eps: 0.01})
Step:   33000, Reward: [-393.823 -393.823 -393.823] [79.703], Avg: [-623.310 -623.310 -623.310] (0.0366) ({r_i: None, r_t: [-796.034 -796.034 -796.034], eps: 0.037})
Step:  156900, Reward: [-571.384 -571.384 -571.384] [129.229], Avg: [-696.352 -696.352 -696.352] (0.1000) ({r_i: None, r_t: [-1129.517 -1129.517 -1129.517], eps: 0.1})
Step:  122500, Reward: [-425.376 -425.376 -425.376] [81.116], Avg: [-750.095 -750.095 -750.095] (0.0100) ({r_i: None, r_t: [-885.362 -885.362 -885.362], eps: 0.01})
Step:   33100, Reward: [-374.132 -374.132 -374.132] [54.978], Avg: [-622.560 -622.560 -622.560] (0.0362) ({r_i: None, r_t: [-813.084 -813.084 -813.084], eps: 0.036})
Step:  157000, Reward: [-612.064 -612.064 -612.064] [150.561], Avg: [-696.299 -696.299 -696.299] (0.1000) ({r_i: None, r_t: [-1297.651 -1297.651 -1297.651], eps: 0.1})
Step:  122600, Reward: [-395.313 -395.313 -395.313] [73.955], Avg: [-749.806 -749.806 -749.806] (0.0100) ({r_i: None, r_t: [-860.148 -860.148 -860.148], eps: 0.01})
Step:   33200, Reward: [-392.096 -392.096 -392.096] [76.774], Avg: [-621.868 -621.868 -621.868] (0.0359) ({r_i: None, r_t: [-785.742 -785.742 -785.742], eps: 0.036})
Step:  157100, Reward: [-710.979 -710.979 -710.979] [275.958], Avg: [-696.308 -696.308 -696.308] (0.1000) ({r_i: None, r_t: [-1225.995 -1225.995 -1225.995], eps: 0.1})
Step:  122700, Reward: [-448.656 -448.656 -448.656] [108.284], Avg: [-749.560 -749.560 -749.560] (0.0100) ({r_i: None, r_t: [-859.132 -859.132 -859.132], eps: 0.01})
Step:   33300, Reward: [-388.918 -388.918 -388.918] [67.116], Avg: [-621.170 -621.170 -621.170] (0.0355) ({r_i: None, r_t: [-782.949 -782.949 -782.949], eps: 0.035})
Step:  157200, Reward: [-631.963 -631.963 -631.963] [176.049], Avg: [-696.267 -696.267 -696.267] (0.1000) ({r_i: None, r_t: [-1118.929 -1118.929 -1118.929], eps: 0.1})
Step:  122800, Reward: [-410.824 -410.824 -410.824] [75.560], Avg: [-749.285 -749.285 -749.285] (0.0100) ({r_i: None, r_t: [-849.923 -849.923 -849.923], eps: 0.01})
Step:   33400, Reward: [-365.113 -365.113 -365.113] [56.527], Avg: [-620.406 -620.406 -620.406] (0.0351) ({r_i: None, r_t: [-779.593 -779.593 -779.593], eps: 0.035})
Step:  157300, Reward: [-606.412 -606.412 -606.412] [159.962], Avg: [-696.210 -696.210 -696.210] (0.1000) ({r_i: None, r_t: [-1237.612 -1237.612 -1237.612], eps: 0.1})
Step:  122900, Reward: [-458.125 -458.125 -458.125] [81.721], Avg: [-749.048 -749.048 -749.048] (0.0100) ({r_i: None, r_t: [-866.017 -866.017 -866.017], eps: 0.01})
Step:  157400, Reward: [-535.905 -535.905 -535.905] [125.452], Avg: [-696.108 -696.108 -696.108] (0.1000) ({r_i: None, r_t: [-1118.223 -1118.223 -1118.223], eps: 0.1})
Step:   33500, Reward: [-376.614 -376.614 -376.614] [63.982], Avg: [-619.680 -619.680 -619.680] (0.0348) ({r_i: None, r_t: [-836.721 -836.721 -836.721], eps: 0.035})
Step:  123000, Reward: [-442.103 -442.103 -442.103] [88.978], Avg: [-748.799 -748.799 -748.799] (0.0100) ({r_i: None, r_t: [-886.352 -886.352 -886.352], eps: 0.01})
Step:  157500, Reward: [-664.010 -664.010 -664.010] [207.076], Avg: [-696.088 -696.088 -696.088] (0.1000) ({r_i: None, r_t: [-1241.617 -1241.617 -1241.617], eps: 0.1})
Step:   33600, Reward: [-427.357 -427.357 -427.357] [84.467], Avg: [-619.109 -619.109 -619.109] (0.0344) ({r_i: None, r_t: [-815.897 -815.897 -815.897], eps: 0.034})
Step:  123100, Reward: [-461.540 -461.540 -461.540] [133.923], Avg: [-748.565 -748.565 -748.565] (0.0100) ({r_i: None, r_t: [-840.824 -840.824 -840.824], eps: 0.01})
Step:  157600, Reward: [-577.222 -577.222 -577.222] [151.611], Avg: [-696.013 -696.013 -696.013] (0.1000) ({r_i: None, r_t: [-1227.645 -1227.645 -1227.645], eps: 0.1})
Step:   33700, Reward: [-406.496 -406.496 -406.496] [83.635], Avg: [-618.480 -618.480 -618.480] (0.0341) ({r_i: None, r_t: [-773.350 -773.350 -773.350], eps: 0.034})
Step:  157700, Reward: [-581.082 -581.082 -581.082] [86.899], Avg: [-695.940 -695.940 -695.940] (0.1000) ({r_i: None, r_t: [-1305.369 -1305.369 -1305.369], eps: 0.1})
Step:  123200, Reward: [-401.630 -401.630 -401.630] [87.944], Avg: [-748.284 -748.284 -748.284] (0.0100) ({r_i: None, r_t: [-855.419 -855.419 -855.419], eps: 0.01})
Step:   33800, Reward: [-430.425 -430.425 -430.425] [65.373], Avg: [-617.926 -617.926 -617.926] (0.0338) ({r_i: None, r_t: [-825.763 -825.763 -825.763], eps: 0.034})
Step:  157800, Reward: [-649.193 -649.193 -649.193] [137.069], Avg: [-695.910 -695.910 -695.910] (0.1000) ({r_i: None, r_t: [-1124.473 -1124.473 -1124.473], eps: 0.1})
Step:  123300, Reward: [-424.699 -424.699 -424.699] [67.575], Avg: [-748.022 -748.022 -748.022] (0.0100) ({r_i: None, r_t: [-819.696 -819.696 -819.696], eps: 0.01})
Step:   33900, Reward: [-366.378 -366.378 -366.378] [69.065], Avg: [-617.186 -617.186 -617.186] (0.0334) ({r_i: None, r_t: [-782.183 -782.183 -782.183], eps: 0.033})
Step:  157900, Reward: [-674.623 -674.623 -674.623] [231.248], Avg: [-695.897 -695.897 -695.897] (0.1000) ({r_i: None, r_t: [-1219.929 -1219.929 -1219.929], eps: 0.1})
Step:  123400, Reward: [-466.082 -466.082 -466.082] [100.164], Avg: [-747.794 -747.794 -747.794] (0.0100) ({r_i: None, r_t: [-864.796 -864.796 -864.796], eps: 0.01})
Step:   34000, Reward: [-384.528 -384.528 -384.528] [85.350], Avg: [-616.504 -616.504 -616.504] (0.0331) ({r_i: None, r_t: [-826.185 -826.185 -826.185], eps: 0.033})
Step:  158000, Reward: [-694.323 -694.323 -694.323] [264.342], Avg: [-695.896 -695.896 -695.896] (0.1000) ({r_i: None, r_t: [-1242.549 -1242.549 -1242.549], eps: 0.1})
Step:  123500, Reward: [-485.580 -485.580 -485.580] [105.253], Avg: [-747.581 -747.581 -747.581] (0.0100) ({r_i: None, r_t: [-891.176 -891.176 -891.176], eps: 0.01})
Step:   34100, Reward: [-367.281 -367.281 -367.281] [63.116], Avg: [-615.775 -615.775 -615.775] (0.0328) ({r_i: None, r_t: [-799.817 -799.817 -799.817], eps: 0.033})
Step:  158100, Reward: [-625.491 -625.491 -625.491] [162.991], Avg: [-695.851 -695.851 -695.851] (0.1000) ({r_i: None, r_t: [-1270.647 -1270.647 -1270.647], eps: 0.1})
Step:  123600, Reward: [-429.639 -429.639 -429.639] [92.840], Avg: [-747.324 -747.324 -747.324] (0.0100) ({r_i: None, r_t: [-845.329 -845.329 -845.329], eps: 0.01})
Step:   34200, Reward: [-387.398 -387.398 -387.398] [59.434], Avg: [-615.109 -615.109 -615.109] (0.0324) ({r_i: None, r_t: [-833.493 -833.493 -833.493], eps: 0.032})
Step:  158200, Reward: [-606.593 -606.593 -606.593] [136.829], Avg: [-695.795 -695.795 -695.795] (0.1000) ({r_i: None, r_t: [-1304.428 -1304.428 -1304.428], eps: 0.1})
Step:  123700, Reward: [-444.793 -444.793 -444.793] [87.537], Avg: [-747.080 -747.080 -747.080] (0.0100) ({r_i: None, r_t: [-821.464 -821.464 -821.464], eps: 0.01})
Step:   34300, Reward: [-371.159 -371.159 -371.159] [60.051], Avg: [-614.400 -614.400 -614.400] (0.0321) ({r_i: None, r_t: [-828.302 -828.302 -828.302], eps: 0.032})
Step:  158300, Reward: [-662.209 -662.209 -662.209] [188.717], Avg: [-695.774 -695.774 -695.774] (0.1000) ({r_i: None, r_t: [-1297.440 -1297.440 -1297.440], eps: 0.1})
Step:  123800, Reward: [-433.322 -433.322 -433.322] [110.909], Avg: [-746.827 -746.827 -746.827] (0.0100) ({r_i: None, r_t: [-899.739 -899.739 -899.739], eps: 0.01})
Step:   34400, Reward: [-407.181 -407.181 -407.181] [67.403], Avg: [-613.799 -613.799 -613.799] (0.0318) ({r_i: None, r_t: [-797.552 -797.552 -797.552], eps: 0.032})
Step:  158400, Reward: [-609.790 -609.790 -609.790] [191.725], Avg: [-695.719 -695.719 -695.719] (0.1000) ({r_i: None, r_t: [-1209.848 -1209.848 -1209.848], eps: 0.1})
Step:  123900, Reward: [-463.293 -463.293 -463.293] [92.925], Avg: [-746.598 -746.598 -746.598] (0.0100) ({r_i: None, r_t: [-877.375 -877.375 -877.375], eps: 0.01})
Step:   34500, Reward: [-377.405 -377.405 -377.405] [62.375], Avg: [-613.116 -613.116 -613.116] (0.0315) ({r_i: None, r_t: [-775.425 -775.425 -775.425], eps: 0.031})
Step:  158500, Reward: [-710.006 -710.006 -710.006] [246.076], Avg: [-695.728 -695.728 -695.728] (0.1000) ({r_i: None, r_t: [-1320.612 -1320.612 -1320.612], eps: 0.1})
Step:  124000, Reward: [-476.002 -476.002 -476.002] [96.098], Avg: [-746.380 -746.380 -746.380] (0.0100) ({r_i: None, r_t: [-842.307 -842.307 -842.307], eps: 0.01})
Step:   34600, Reward: [-382.212 -382.212 -382.212] [59.554], Avg: [-612.451 -612.451 -612.451] (0.0312) ({r_i: None, r_t: [-801.489 -801.489 -801.489], eps: 0.031})
Step:  158600, Reward: [-676.475 -676.475 -676.475] [240.396], Avg: [-695.716 -695.716 -695.716] (0.1000) ({r_i: None, r_t: [-1257.884 -1257.884 -1257.884], eps: 0.1})
Step:  124100, Reward: [-470.086 -470.086 -470.086] [83.073], Avg: [-746.158 -746.158 -746.158] (0.0100) ({r_i: None, r_t: [-875.305 -875.305 -875.305], eps: 0.01})
Step:   34700, Reward: [-407.545 -407.545 -407.545] [72.139], Avg: [-611.862 -611.862 -611.862] (0.0308) ({r_i: None, r_t: [-741.045 -741.045 -741.045], eps: 0.031})
Step:  158700, Reward: [-613.920 -613.920 -613.920] [176.068], Avg: [-695.665 -695.665 -695.665] (0.1000) ({r_i: None, r_t: [-1329.049 -1329.049 -1329.049], eps: 0.1})
Step:  124200, Reward: [-410.743 -410.743 -410.743] [92.077], Avg: [-745.888 -745.888 -745.888] (0.0100) ({r_i: None, r_t: [-850.068 -850.068 -850.068], eps: 0.01})
Step:   34800, Reward: [-407.276 -407.276 -407.276] [70.287], Avg: [-611.276 -611.276 -611.276] (0.0305) ({r_i: None, r_t: [-756.307 -756.307 -756.307], eps: 0.031})
Step:  158800, Reward: [-571.257 -571.257 -571.257] [152.736], Avg: [-695.586 -695.586 -695.586] (0.1000) ({r_i: None, r_t: [-1307.324 -1307.324 -1307.324], eps: 0.1})
Step:  124300, Reward: [-447.333 -447.333 -447.333] [96.289], Avg: [-745.648 -745.648 -745.648] (0.0100) ({r_i: None, r_t: [-880.309 -880.309 -880.309], eps: 0.01})
Step:   34900, Reward: [-386.810 -386.810 -386.810] [70.417], Avg: [-610.634 -610.634 -610.634] (0.0302) ({r_i: None, r_t: [-734.461 -734.461 -734.461], eps: 0.03})
Step:  158900, Reward: [-587.267 -587.267 -587.267] [108.723], Avg: [-695.518 -695.518 -695.518] (0.1000) ({r_i: None, r_t: [-1272.619 -1272.619 -1272.619], eps: 0.1})
Step:  124400, Reward: [-477.301 -477.301 -477.301] [103.221], Avg: [-745.432 -745.432 -745.432] (0.0100) ({r_i: None, r_t: [-853.222 -853.222 -853.222], eps: 0.01})
Step:   35000, Reward: [-397.180 -397.180 -397.180] [85.240], Avg: [-610.026 -610.026 -610.026] (0.0299) ({r_i: None, r_t: [-837.170 -837.170 -837.170], eps: 0.03})
Step:  159000, Reward: [-587.403 -587.403 -587.403] [140.589], Avg: [-695.450 -695.450 -695.450] (0.1000) ({r_i: None, r_t: [-1205.351 -1205.351 -1205.351], eps: 0.1})
Step:  124500, Reward: [-440.920 -440.920 -440.920] [96.194], Avg: [-745.188 -745.188 -745.188] (0.0100) ({r_i: None, r_t: [-868.373 -868.373 -868.373], eps: 0.01})
Step:   35100, Reward: [-395.952 -395.952 -395.952] [77.709], Avg: [-609.418 -609.418 -609.418] (0.0296) ({r_i: None, r_t: [-792.815 -792.815 -792.815], eps: 0.03})
Step:  159100, Reward: [-575.247 -575.247 -575.247] [118.278], Avg: [-695.375 -695.375 -695.375] (0.1000) ({r_i: None, r_t: [-1388.215 -1388.215 -1388.215], eps: 0.1})
Step:  124600, Reward: [-377.789 -377.789 -377.789] [77.676], Avg: [-744.893 -744.893 -744.893] (0.0100) ({r_i: None, r_t: [-863.818 -863.818 -863.818], eps: 0.01})
Step:   35200, Reward: [-406.683 -406.683 -406.683] [70.527], Avg: [-608.844 -608.844 -608.844] (0.0293) ({r_i: None, r_t: [-796.046 -796.046 -796.046], eps: 0.029})
Step:  159200, Reward: [-707.395 -707.395 -707.395] [178.469], Avg: [-695.382 -695.382 -695.382] (0.1000) ({r_i: None, r_t: [-1311.207 -1311.207 -1311.207], eps: 0.1})
Step:  124700, Reward: [-459.215 -459.215 -459.215] [83.439], Avg: [-744.664 -744.664 -744.664] (0.0100) ({r_i: None, r_t: [-906.784 -906.784 -906.784], eps: 0.01})
Step:   35300, Reward: [-430.184 -430.184 -430.184] [83.266], Avg: [-608.339 -608.339 -608.339] (0.0290) ({r_i: None, r_t: [-783.121 -783.121 -783.121], eps: 0.029})
Step:  159300, Reward: [-604.525 -604.525 -604.525] [106.899], Avg: [-695.325 -695.325 -695.325] (0.1000) ({r_i: None, r_t: [-1285.038 -1285.038 -1285.038], eps: 0.1})
Step:  124800, Reward: [-463.922 -463.922 -463.922] [76.060], Avg: [-744.440 -744.440 -744.440] (0.0100) ({r_i: None, r_t: [-881.868 -881.868 -881.868], eps: 0.01})
Step:   35400, Reward: [-413.940 -413.940 -413.940] [49.420], Avg: [-607.791 -607.791 -607.791] (0.0288) ({r_i: None, r_t: [-780.208 -780.208 -780.208], eps: 0.029})
Step:  159400, Reward: [-721.757 -721.757 -721.757] [225.771], Avg: [-695.342 -695.342 -695.342] (0.1000) ({r_i: None, r_t: [-1209.481 -1209.481 -1209.481], eps: 0.1})
Step:  124900, Reward: [-434.405 -434.405 -434.405] [90.073], Avg: [-744.191 -744.191 -744.191] (0.0100) ({r_i: None, r_t: [-858.882 -858.882 -858.882], eps: 0.01})
Step:   35500, Reward: [-380.333 -380.333 -380.333] [67.446], Avg: [-607.152 -607.152 -607.152] (0.0285) ({r_i: None, r_t: [-836.208 -836.208 -836.208], eps: 0.028})
Step:  159500, Reward: [-666.603 -666.603 -666.603] [188.994], Avg: [-695.324 -695.324 -695.324] (0.1000) ({r_i: None, r_t: [-1312.858 -1312.858 -1312.858], eps: 0.1})
Step:  125000, Reward: [-445.524 -445.524 -445.524] [86.808], Avg: [-743.953 -743.953 -743.953] (0.0100) ({r_i: None, r_t: [-850.956 -850.956 -850.956], eps: 0.01})
Step:   35600, Reward: [-428.843 -428.843 -428.843] [58.963], Avg: [-606.653 -606.653 -606.653] (0.0282) ({r_i: None, r_t: [-805.075 -805.075 -805.075], eps: 0.028})
Step:  159600, Reward: [-628.292 -628.292 -628.292] [188.546], Avg: [-695.282 -695.282 -695.282] (0.1000) ({r_i: None, r_t: [-1301.016 -1301.016 -1301.016], eps: 0.1})
Step:  125100, Reward: [-441.176 -441.176 -441.176] [102.175], Avg: [-743.711 -743.711 -743.711] (0.0100) ({r_i: None, r_t: [-864.235 -864.235 -864.235], eps: 0.01})
Step:  159700, Reward: [-702.530 -702.530 -702.530] [227.290], Avg: [-695.287 -695.287 -695.287] (0.1000) ({r_i: None, r_t: [-1286.259 -1286.259 -1286.259], eps: 0.1})
Step:   35700, Reward: [-387.733 -387.733 -387.733] [62.256], Avg: [-606.041 -606.041 -606.041] (0.0279) ({r_i: None, r_t: [-838.767 -838.767 -838.767], eps: 0.028})
Step:  125200, Reward: [-444.524 -444.524 -444.524] [101.615], Avg: [-743.472 -743.472 -743.472] (0.0100) ({r_i: None, r_t: [-818.336 -818.336 -818.336], eps: 0.01})
Step:  159800, Reward: [-659.584 -659.584 -659.584] [145.407], Avg: [-695.264 -695.264 -695.264] (0.1000) ({r_i: None, r_t: [-1438.687 -1438.687 -1438.687], eps: 0.1})
Step:   35800, Reward: [-412.028 -412.028 -412.028] [64.396], Avg: [-605.501 -605.501 -605.501] (0.0276) ({r_i: None, r_t: [-825.175 -825.175 -825.175], eps: 0.028})
Step:  125300, Reward: [-440.927 -440.927 -440.927] [131.703], Avg: [-743.231 -743.231 -743.231] (0.0100) ({r_i: None, r_t: [-915.050 -915.050 -915.050], eps: 0.01})
Step:  159900, Reward: [-631.852 -631.852 -631.852] [159.537], Avg: [-695.225 -695.225 -695.225] (0.1000) ({r_i: None, r_t: [-1340.457 -1340.457 -1340.457], eps: 0.1})
Step:   35900, Reward: [-382.545 -382.545 -382.545] [78.473], Avg: [-604.882 -604.882 -604.882] (0.0274) ({r_i: None, r_t: [-791.061 -791.061 -791.061], eps: 0.027})
Step:  125400, Reward: [-455.131 -455.131 -455.131] [93.178], Avg: [-743.001 -743.001 -743.001] (0.0100) ({r_i: None, r_t: [-880.860 -880.860 -880.860], eps: 0.01})
Step:  160000, Reward: [-693.496 -693.496 -693.496] [228.701], Avg: [-695.223 -695.223 -695.223] (0.1000) ({r_i: None, r_t: [-1184.550 -1184.550 -1184.550], eps: 0.1})
Step:   36000, Reward: [-378.425 -378.425 -378.425] [72.025], Avg: [-604.254 -604.254 -604.254] (0.0271) ({r_i: None, r_t: [-803.307 -803.307 -803.307], eps: 0.027})
Step:  125500, Reward: [-505.579 -505.579 -505.579] [80.952], Avg: [-742.812 -742.812 -742.812] (0.0100) ({r_i: None, r_t: [-875.368 -875.368 -875.368], eps: 0.01})
Step:  160100, Reward: [-670.922 -670.922 -670.922] [198.771], Avg: [-695.208 -695.208 -695.208] (0.1000) ({r_i: None, r_t: [-1195.074 -1195.074 -1195.074], eps: 0.1})
Step:   36100, Reward: [-431.300 -431.300 -431.300] [79.679], Avg: [-603.777 -603.777 -603.777] (0.0268) ({r_i: None, r_t: [-786.601 -786.601 -786.601], eps: 0.027})
Step:  125600, Reward: [-431.194 -431.194 -431.194] [91.878], Avg: [-742.564 -742.564 -742.564] (0.0100) ({r_i: None, r_t: [-866.975 -866.975 -866.975], eps: 0.01})
Step:  160200, Reward: [-728.710 -728.710 -728.710] [157.539], Avg: [-695.229 -695.229 -695.229] (0.1000) ({r_i: None, r_t: [-1206.647 -1206.647 -1206.647], eps: 0.1})
Step:   36200, Reward: [-362.271 -362.271 -362.271] [59.669], Avg: [-603.111 -603.111 -603.111] (0.0265) ({r_i: None, r_t: [-813.107 -813.107 -813.107], eps: 0.027})
Step:  125700, Reward: [-424.649 -424.649 -424.649] [63.520], Avg: [-742.312 -742.312 -742.312] (0.0100) ({r_i: None, r_t: [-878.516 -878.516 -878.516], eps: 0.01})
Step:  160300, Reward: [-670.043 -670.043 -670.043] [273.574], Avg: [-695.214 -695.214 -695.214] (0.1000) ({r_i: None, r_t: [-1304.034 -1304.034 -1304.034], eps: 0.1})
Step:   36300, Reward: [-398.632 -398.632 -398.632] [72.985], Avg: [-602.550 -602.550 -602.550] (0.0263) ({r_i: None, r_t: [-824.857 -824.857 -824.857], eps: 0.026})
Step:  125800, Reward: [-453.189 -453.189 -453.189] [99.912], Avg: [-742.082 -742.082 -742.082] (0.0100) ({r_i: None, r_t: [-873.130 -873.130 -873.130], eps: 0.01})
Step:  160400, Reward: [-651.383 -651.383 -651.383] [221.125], Avg: [-695.186 -695.186 -695.186] (0.1000) ({r_i: None, r_t: [-1277.360 -1277.360 -1277.360], eps: 0.1})
Step:   36400, Reward: [-411.907 -411.907 -411.907] [56.921], Avg: [-602.027 -602.027 -602.027] (0.0260) ({r_i: None, r_t: [-809.808 -809.808 -809.808], eps: 0.026})
Step:  160500, Reward: [-733.281 -733.281 -733.281] [313.296], Avg: [-695.210 -695.210 -695.210] (0.1000) ({r_i: None, r_t: [-1320.386 -1320.386 -1320.386], eps: 0.1})
Step:  125900, Reward: [-436.256 -436.256 -436.256] [81.831], Avg: [-741.839 -741.839 -741.839] (0.0100) ({r_i: None, r_t: [-912.431 -912.431 -912.431], eps: 0.01})
Step:   36500, Reward: [-382.452 -382.452 -382.452] [80.331], Avg: [-601.427 -601.427 -601.427] (0.0258) ({r_i: None, r_t: [-779.708 -779.708 -779.708], eps: 0.026})
Step:  160600, Reward: [-697.455 -697.455 -697.455] [205.494], Avg: [-695.211 -695.211 -695.211] (0.1000) ({r_i: None, r_t: [-1392.472 -1392.472 -1392.472], eps: 0.1})
Step:  126000, Reward: [-418.141 -418.141 -418.141] [63.898], Avg: [-741.583 -741.583 -741.583] (0.0100) ({r_i: None, r_t: [-830.897 -830.897 -830.897], eps: 0.01})
Step:   36600, Reward: [-440.965 -440.965 -440.965] [101.022], Avg: [-600.990 -600.990 -600.990] (0.0255) ({r_i: None, r_t: [-786.112 -786.112 -786.112], eps: 0.025})
Step:  160700, Reward: [-657.530 -657.530 -657.530] [179.225], Avg: [-695.188 -695.188 -695.188] (0.1000) ({r_i: None, r_t: [-1262.911 -1262.911 -1262.911], eps: 0.1})
Step:  126100, Reward: [-383.163 -383.163 -383.163] [74.857], Avg: [-741.299 -741.299 -741.299] (0.0100) ({r_i: None, r_t: [-847.651 -847.651 -847.651], eps: 0.01})
Step:   36700, Reward: [-389.490 -389.490 -389.490] [49.878], Avg: [-600.415 -600.415 -600.415] (0.0252) ({r_i: None, r_t: [-783.752 -783.752 -783.752], eps: 0.025})
Step:  160800, Reward: [-658.975 -658.975 -658.975] [195.117], Avg: [-695.165 -695.165 -695.165] (0.1000) ({r_i: None, r_t: [-1389.563 -1389.563 -1389.563], eps: 0.1})
Step:  126200, Reward: [-450.911 -450.911 -450.911] [74.290], Avg: [-741.069 -741.069 -741.069] (0.0100) ({r_i: None, r_t: [-906.217 -906.217 -906.217], eps: 0.01})
Step:   36800, Reward: [-372.332 -372.332 -372.332] [54.157], Avg: [-599.797 -599.797 -599.797] (0.0250) ({r_i: None, r_t: [-753.509 -753.509 -753.509], eps: 0.025})
Step:  160900, Reward: [-664.873 -664.873 -664.873] [232.674], Avg: [-695.147 -695.147 -695.147] (0.1000) ({r_i: None, r_t: [-1441.835 -1441.835 -1441.835], eps: 0.1})
Step:  126300, Reward: [-467.536 -467.536 -467.536] [122.140], Avg: [-740.852 -740.852 -740.852] (0.0100) ({r_i: None, r_t: [-834.023 -834.023 -834.023], eps: 0.01})
Step:   36900, Reward: [-397.186 -397.186 -397.186] [75.546], Avg: [-599.250 -599.250 -599.250] (0.0247) ({r_i: None, r_t: [-765.793 -765.793 -765.793], eps: 0.025})
Step:  161000, Reward: [-628.729 -628.729 -628.729] [153.404], Avg: [-695.105 -695.105 -695.105] (0.1000) ({r_i: None, r_t: [-1409.345 -1409.345 -1409.345], eps: 0.1})
Step:  126400, Reward: [-451.965 -451.965 -451.965] [115.051], Avg: [-740.624 -740.624 -740.624] (0.0100) ({r_i: None, r_t: [-862.014 -862.014 -862.014], eps: 0.01})
Step:   37000, Reward: [-410.858 -410.858 -410.858] [67.252], Avg: [-598.742 -598.742 -598.742] (0.0245) ({r_i: None, r_t: [-787.206 -787.206 -787.206], eps: 0.024})
Step:  161100, Reward: [-647.189 -647.189 -647.189] [207.027], Avg: [-695.076 -695.076 -695.076] (0.1000) ({r_i: None, r_t: [-1389.822 -1389.822 -1389.822], eps: 0.1})
Step:  126500, Reward: [-458.630 -458.630 -458.630] [67.013], Avg: [-740.401 -740.401 -740.401] (0.0100) ({r_i: None, r_t: [-888.121 -888.121 -888.121], eps: 0.01})
Step:   37100, Reward: [-389.174 -389.174 -389.174] [45.601], Avg: [-598.179 -598.179 -598.179] (0.0243) ({r_i: None, r_t: [-769.286 -769.286 -769.286], eps: 0.024})
Step:  161200, Reward: [-801.709 -801.709 -801.709] [296.745], Avg: [-695.142 -695.142 -695.142] (0.1000) ({r_i: None, r_t: [-1261.073 -1261.073 -1261.073], eps: 0.1})
Step:  126600, Reward: [-460.949 -460.949 -460.949] [98.719], Avg: [-740.181 -740.181 -740.181] (0.0100) ({r_i: None, r_t: [-865.907 -865.907 -865.907], eps: 0.01})
Step:   37200, Reward: [-386.309 -386.309 -386.309] [81.523], Avg: [-597.611 -597.611 -597.611] (0.0240) ({r_i: None, r_t: [-821.921 -821.921 -821.921], eps: 0.024})
Step:  161300, Reward: [-675.696 -675.696 -675.696] [174.261], Avg: [-695.130 -695.130 -695.130] (0.1000) ({r_i: None, r_t: [-1411.437 -1411.437 -1411.437], eps: 0.1})
Step:  126700, Reward: [-487.735 -487.735 -487.735] [150.439], Avg: [-739.981 -739.981 -739.981] (0.0100) ({r_i: None, r_t: [-854.031 -854.031 -854.031], eps: 0.01})
Step:   37300, Reward: [-412.406 -412.406 -412.406] [89.334], Avg: [-597.115 -597.115 -597.115] (0.0238) ({r_i: None, r_t: [-791.186 -791.186 -791.186], eps: 0.024})
Step:  161400, Reward: [-684.807 -684.807 -684.807] [199.887], Avg: [-695.123 -695.123 -695.123] (0.1000) ({r_i: None, r_t: [-1316.180 -1316.180 -1316.180], eps: 0.1})
Step:  126800, Reward: [-473.363 -473.363 -473.363] [100.936], Avg: [-739.771 -739.771 -739.771] (0.0100) ({r_i: None, r_t: [-884.003 -884.003 -884.003], eps: 0.01})
Step:   37400, Reward: [-398.871 -398.871 -398.871] [64.271], Avg: [-596.587 -596.587 -596.587] (0.0235) ({r_i: None, r_t: [-844.990 -844.990 -844.990], eps: 0.024})
Step:  161500, Reward: [-716.455 -716.455 -716.455] [320.305], Avg: [-695.136 -695.136 -695.136] (0.1000) ({r_i: None, r_t: [-1414.424 -1414.424 -1414.424], eps: 0.1})
Step:  126900, Reward: [-468.839 -468.839 -468.839] [88.654], Avg: [-739.558 -739.558 -739.558] (0.0100) ({r_i: None, r_t: [-802.486 -802.486 -802.486], eps: 0.01})
Step:   37500, Reward: [-406.665 -406.665 -406.665] [53.941], Avg: [-596.082 -596.082 -596.082] (0.0233) ({r_i: None, r_t: [-818.908 -818.908 -818.908], eps: 0.023})
Step:  161600, Reward: [-754.763 -754.763 -754.763] [247.618], Avg: [-695.173 -695.173 -695.173] (0.1000) ({r_i: None, r_t: [-1438.893 -1438.893 -1438.893], eps: 0.1})
Step:  127000, Reward: [-449.668 -449.668 -449.668] [104.759], Avg: [-739.330 -739.330 -739.330] (0.0100) ({r_i: None, r_t: [-867.483 -867.483 -867.483], eps: 0.01})
Step:  161700, Reward: [-777.573 -777.573 -777.573] [244.532], Avg: [-695.224 -695.224 -695.224] (0.1000) ({r_i: None, r_t: [-1511.341 -1511.341 -1511.341], eps: 0.1})
Step:   37600, Reward: [-379.161 -379.161 -379.161] [62.600], Avg: [-595.506 -595.506 -595.506] (0.0231) ({r_i: None, r_t: [-834.789 -834.789 -834.789], eps: 0.023})
Step:  127100, Reward: [-406.419 -406.419 -406.419] [94.114], Avg: [-739.068 -739.068 -739.068] (0.0100) ({r_i: None, r_t: [-897.687 -897.687 -897.687], eps: 0.01})
Step:  161800, Reward: [-676.010 -676.010 -676.010] [181.804], Avg: [-695.212 -695.212 -695.212] (0.1000) ({r_i: None, r_t: [-1536.099 -1536.099 -1536.099], eps: 0.1})
Step:   37700, Reward: [-390.018 -390.018 -390.018] [64.084], Avg: [-594.963 -594.963 -594.963] (0.0228) ({r_i: None, r_t: [-792.217 -792.217 -792.217], eps: 0.023})
Step:  127200, Reward: [-441.291 -441.291 -441.291] [132.481], Avg: [-738.834 -738.834 -738.834] (0.0100) ({r_i: None, r_t: [-843.487 -843.487 -843.487], eps: 0.01})
Step:  161900, Reward: [-667.226 -667.226 -667.226] [191.920], Avg: [-695.195 -695.195 -695.195] (0.1000) ({r_i: None, r_t: [-1236.329 -1236.329 -1236.329], eps: 0.1})
Step:   37800, Reward: [-374.268 -374.268 -374.268] [66.270], Avg: [-594.380 -594.380 -594.380] (0.0226) ({r_i: None, r_t: [-824.261 -824.261 -824.261], eps: 0.023})
Step:  127300, Reward: [-432.107 -432.107 -432.107] [72.478], Avg: [-738.594 -738.594 -738.594] (0.0100) ({r_i: None, r_t: [-839.798 -839.798 -839.798], eps: 0.01})
Step:  162000, Reward: [-700.387 -700.387 -700.387] [212.440], Avg: [-695.198 -695.198 -695.198] (0.1000) ({r_i: None, r_t: [-1452.976 -1452.976 -1452.976], eps: 0.1})
Step:   37900, Reward: [-407.389 -407.389 -407.389] [56.846], Avg: [-593.888 -593.888 -593.888] (0.0224) ({r_i: None, r_t: [-808.971 -808.971 -808.971], eps: 0.022})
Step:  127400, Reward: [-448.250 -448.250 -448.250] [87.234], Avg: [-738.366 -738.366 -738.366] (0.0100) ({r_i: None, r_t: [-935.282 -935.282 -935.282], eps: 0.01})
Step:  162100, Reward: [-713.710 -713.710 -713.710] [274.712], Avg: [-695.210 -695.210 -695.210] (0.1000) ({r_i: None, r_t: [-1574.352 -1574.352 -1574.352], eps: 0.1})
Step:   38000, Reward: [-400.457 -400.457 -400.457] [58.959], Avg: [-593.380 -593.380 -593.380] (0.0222) ({r_i: None, r_t: [-769.974 -769.974 -769.974], eps: 0.022})
Step:  127500, Reward: [-394.364 -394.364 -394.364] [60.325], Avg: [-738.096 -738.096 -738.096] (0.0100) ({r_i: None, r_t: [-894.439 -894.439 -894.439], eps: 0.01})
Step:  162200, Reward: [-781.112 -781.112 -781.112] [218.898], Avg: [-695.263 -695.263 -695.263] (0.1000) ({r_i: None, r_t: [-1559.986 -1559.986 -1559.986], eps: 0.1})
Step:   38100, Reward: [-390.557 -390.557 -390.557] [77.091], Avg: [-592.849 -592.849 -592.849] (0.0219) ({r_i: None, r_t: [-780.832 -780.832 -780.832], eps: 0.022})
Step:  127600, Reward: [-462.916 -462.916 -462.916] [94.589], Avg: [-737.881 -737.881 -737.881] (0.0100) ({r_i: None, r_t: [-861.239 -861.239 -861.239], eps: 0.01})
Step:  162300, Reward: [-689.898 -689.898 -689.898] [237.097], Avg: [-695.259 -695.259 -695.259] (0.1000) ({r_i: None, r_t: [-1479.598 -1479.598 -1479.598], eps: 0.1})
Step:   38200, Reward: [-398.532 -398.532 -398.532] [66.213], Avg: [-592.342 -592.342 -592.342] (0.0217) ({r_i: None, r_t: [-826.771 -826.771 -826.771], eps: 0.022})
Step:  127700, Reward: [-481.230 -481.230 -481.230] [111.068], Avg: [-737.680 -737.680 -737.680] (0.0100) ({r_i: None, r_t: [-928.903 -928.903 -928.903], eps: 0.01})
Step:  162400, Reward: [-826.899 -826.899 -826.899] [266.720], Avg: [-695.340 -695.340 -695.340] (0.1000) ({r_i: None, r_t: [-1530.329 -1530.329 -1530.329], eps: 0.1})
Step:   38300, Reward: [-414.735 -414.735 -414.735] [76.382], Avg: [-591.880 -591.880 -591.880] (0.0215) ({r_i: None, r_t: [-851.734 -851.734 -851.734], eps: 0.022})
Step:  127800, Reward: [-427.016 -427.016 -427.016] [94.324], Avg: [-737.437 -737.437 -737.437] (0.0100) ({r_i: None, r_t: [-827.451 -827.451 -827.451], eps: 0.01})
Step:  162500, Reward: [-675.311 -675.311 -675.311] [198.008], Avg: [-695.328 -695.328 -695.328] (0.1000) ({r_i: None, r_t: [-1371.194 -1371.194 -1371.194], eps: 0.1})
Step:   38400, Reward: [-404.326 -404.326 -404.326] [48.639], Avg: [-591.392 -591.392 -591.392] (0.0213) ({r_i: None, r_t: [-776.918 -776.918 -776.918], eps: 0.021})
Step:  162600, Reward: [-692.516 -692.516 -692.516] [213.696], Avg: [-695.326 -695.326 -695.326] (0.1000) ({r_i: None, r_t: [-1492.389 -1492.389 -1492.389], eps: 0.1})
Step:  127900, Reward: [-484.827 -484.827 -484.827] [125.641], Avg: [-737.240 -737.240 -737.240] (0.0100) ({r_i: None, r_t: [-902.130 -902.130 -902.130], eps: 0.01})
Step:   38500, Reward: [-427.740 -427.740 -427.740] [90.237], Avg: [-590.969 -590.969 -590.969] (0.0211) ({r_i: None, r_t: [-784.595 -784.595 -784.595], eps: 0.021})
Step:  162700, Reward: [-721.440 -721.440 -721.440] [289.941], Avg: [-695.342 -695.342 -695.342] (0.1000) ({r_i: None, r_t: [-1657.878 -1657.878 -1657.878], eps: 0.1})
Step:  128000, Reward: [-426.559 -426.559 -426.559] [104.158], Avg: [-736.997 -736.997 -736.997] (0.0100) ({r_i: None, r_t: [-846.773 -846.773 -846.773], eps: 0.01})
Step:   38600, Reward: [-421.077 -421.077 -421.077] [79.771], Avg: [-590.530 -590.530 -590.530] (0.0209) ({r_i: None, r_t: [-803.741 -803.741 -803.741], eps: 0.021})
Step:  162800, Reward: [-720.011 -720.011 -720.011] [192.692], Avg: [-695.358 -695.358 -695.358] (0.1000) ({r_i: None, r_t: [-1355.683 -1355.683 -1355.683], eps: 0.1})
Step:  128100, Reward: [-450.401 -450.401 -450.401] [114.289], Avg: [-736.774 -736.774 -736.774] (0.0100) ({r_i: None, r_t: [-961.869 -961.869 -961.869], eps: 0.01})
Step:   38700, Reward: [-376.287 -376.287 -376.287] [58.415], Avg: [-589.977 -589.977 -589.977] (0.0207) ({r_i: None, r_t: [-761.531 -761.531 -761.531], eps: 0.021})
Step:  162900, Reward: [-789.318 -789.318 -789.318] [224.290], Avg: [-695.415 -695.415 -695.415] (0.1000) ({r_i: None, r_t: [-1552.319 -1552.319 -1552.319], eps: 0.1})
Step:  128200, Reward: [-395.660 -395.660 -395.660] [110.267], Avg: [-736.508 -736.508 -736.508] (0.0100) ({r_i: None, r_t: [-899.778 -899.778 -899.778], eps: 0.01})
Step:   38800, Reward: [-389.167 -389.167 -389.167] [55.723], Avg: [-589.461 -589.461 -589.461] (0.0205) ({r_i: None, r_t: [-828.349 -828.349 -828.349], eps: 0.02})
Step:  163000, Reward: [-772.678 -772.678 -772.678] [285.251], Avg: [-695.463 -695.463 -695.463] (0.1000) ({r_i: None, r_t: [-1465.937 -1465.937 -1465.937], eps: 0.1})
Step:  128300, Reward: [-418.920 -418.920 -418.920] [92.292], Avg: [-736.260 -736.260 -736.260] (0.0100) ({r_i: None, r_t: [-881.824 -881.824 -881.824], eps: 0.01})
Step:   38900, Reward: [-394.351 -394.351 -394.351] [53.172], Avg: [-588.961 -588.961 -588.961] (0.0202) ({r_i: None, r_t: [-774.241 -774.241 -774.241], eps: 0.02})
Step:  163100, Reward: [-890.609 -890.609 -890.609] [277.887], Avg: [-695.582 -695.582 -695.582] (0.1000) ({r_i: None, r_t: [-1703.675 -1703.675 -1703.675], eps: 0.1})
Step:  128400, Reward: [-426.751 -426.751 -426.751] [84.739], Avg: [-736.020 -736.020 -736.020] (0.0100) ({r_i: None, r_t: [-913.383 -913.383 -913.383], eps: 0.01})
Step:   39000, Reward: [-412.450 -412.450 -412.450] [95.317], Avg: [-588.509 -588.509 -588.509] (0.0200) ({r_i: None, r_t: [-794.208 -794.208 -794.208], eps: 0.02})
Step:  163200, Reward: [-721.740 -721.740 -721.740] [284.106], Avg: [-695.598 -695.598 -695.598] (0.1000) ({r_i: None, r_t: [-1550.014 -1550.014 -1550.014], eps: 0.1})
Step:  128500, Reward: [-416.019 -416.019 -416.019] [84.597], Avg: [-735.771 -735.771 -735.771] (0.0100) ({r_i: None, r_t: [-848.282 -848.282 -848.282], eps: 0.01})
Step:   39100, Reward: [-380.486 -380.486 -380.486] [49.295], Avg: [-587.979 -587.979 -587.979] (0.0198) ({r_i: None, r_t: [-844.241 -844.241 -844.241], eps: 0.02})
Step:  163300, Reward: [-713.675 -713.675 -713.675] [236.404], Avg: [-695.609 -695.609 -695.609] (0.1000) ({r_i: None, r_t: [-1586.043 -1586.043 -1586.043], eps: 0.1})
Step:  128600, Reward: [-453.199 -453.199 -453.199] [102.101], Avg: [-735.551 -735.551 -735.551] (0.0100) ({r_i: None, r_t: [-867.571 -867.571 -867.571], eps: 0.01})
Step:   39200, Reward: [-391.661 -391.661 -391.661] [66.255], Avg: [-587.479 -587.479 -587.479] (0.0196) ({r_i: None, r_t: [-841.746 -841.746 -841.746], eps: 0.02})
Step:  163400, Reward: [-788.036 -788.036 -788.036] [172.738], Avg: [-695.666 -695.666 -695.666] (0.1000) ({r_i: None, r_t: [-1591.579 -1591.579 -1591.579], eps: 0.1})
Step:  128700, Reward: [-437.380 -437.380 -437.380] [88.670], Avg: [-735.320 -735.320 -735.320] (0.0100) ({r_i: None, r_t: [-845.210 -845.210 -845.210], eps: 0.01})
Step:   39300, Reward: [-379.088 -379.088 -379.088] [55.293], Avg: [-586.950 -586.950 -586.950] (0.0195) ({r_i: None, r_t: [-809.243 -809.243 -809.243], eps: 0.019})
Step:  163500, Reward: [-813.392 -813.392 -813.392] [269.886], Avg: [-695.738 -695.738 -695.738] (0.1000) ({r_i: None, r_t: [-1492.082 -1492.082 -1492.082], eps: 0.1})
Step:  128800, Reward: [-429.284 -429.284 -429.284] [100.215], Avg: [-735.082 -735.082 -735.082] (0.0100) ({r_i: None, r_t: [-851.972 -851.972 -851.972], eps: 0.01})
Step:   39400, Reward: [-433.301 -433.301 -433.301] [49.425], Avg: [-586.561 -586.561 -586.561] (0.0193) ({r_i: None, r_t: [-849.556 -849.556 -849.556], eps: 0.019})
Step:  163600, Reward: [-842.350 -842.350 -842.350] [405.773], Avg: [-695.827 -695.827 -695.827] (0.1000) ({r_i: None, r_t: [-1669.641 -1669.641 -1669.641], eps: 0.1})
Step:  128900, Reward: [-414.440 -414.440 -414.440] [98.753], Avg: [-734.834 -734.834 -734.834] (0.0100) ({r_i: None, r_t: [-833.544 -833.544 -833.544], eps: 0.01})
Step:   39500, Reward: [-469.693 -469.693 -469.693] [67.246], Avg: [-586.266 -586.266 -586.266] (0.0191) ({r_i: None, r_t: [-875.459 -875.459 -875.459], eps: 0.019})
Step:  163700, Reward: [-809.639 -809.639 -809.639] [291.836], Avg: [-695.897 -695.897 -695.897] (0.1000) ({r_i: None, r_t: [-1621.913 -1621.913 -1621.913], eps: 0.1})
Step:  129000, Reward: [-424.645 -424.645 -424.645] [102.934], Avg: [-734.593 -734.593 -734.593] (0.0100) ({r_i: None, r_t: [-822.564 -822.564 -822.564], eps: 0.01})
Step:  163800, Reward: [-831.925 -831.925 -831.925] [272.066], Avg: [-695.980 -695.980 -695.980] (0.1000) ({r_i: None, r_t: [-1805.957 -1805.957 -1805.957], eps: 0.1})
Step:   39600, Reward: [-431.521 -431.521 -431.521] [59.181], Avg: [-585.876 -585.876 -585.876] (0.0189) ({r_i: None, r_t: [-879.138 -879.138 -879.138], eps: 0.019})
Step:  129100, Reward: [-423.363 -423.363 -423.363] [72.901], Avg: [-734.353 -734.353 -734.353] (0.0100) ({r_i: None, r_t: [-894.813 -894.813 -894.813], eps: 0.01})
Step:  163900, Reward: [-720.632 -720.632 -720.632] [189.311], Avg: [-695.995 -695.995 -695.995] (0.1000) ({r_i: None, r_t: [-1604.278 -1604.278 -1604.278], eps: 0.1})
Step:   39700, Reward: [-411.605 -411.605 -411.605] [79.954], Avg: [-585.439 -585.439 -585.439] (0.0187) ({r_i: None, r_t: [-908.309 -908.309 -908.309], eps: 0.019})
Step:  129200, Reward: [-460.072 -460.072 -460.072] [134.983], Avg: [-734.140 -734.140 -734.140] (0.0100) ({r_i: None, r_t: [-863.320 -863.320 -863.320], eps: 0.01})
Step:  164000, Reward: [-813.666 -813.666 -813.666] [251.975], Avg: [-696.066 -696.066 -696.066] (0.1000) ({r_i: None, r_t: [-1476.746 -1476.746 -1476.746], eps: 0.1})
Step:   39800, Reward: [-422.823 -422.823 -422.823] [61.293], Avg: [-585.031 -585.031 -585.031] (0.0185) ({r_i: None, r_t: [-869.422 -869.422 -869.422], eps: 0.019})
Step:  129300, Reward: [-416.097 -416.097 -416.097] [92.834], Avg: [-733.895 -733.895 -733.895] (0.0100) ({r_i: None, r_t: [-924.035 -924.035 -924.035], eps: 0.01})
Step:  164100, Reward: [-882.696 -882.696 -882.696] [254.214], Avg: [-696.180 -696.180 -696.180] (0.1000) ({r_i: None, r_t: [-1635.014 -1635.014 -1635.014], eps: 0.1})
Step:   39900, Reward: [-440.490 -440.490 -440.490] [71.101], Avg: [-584.670 -584.670 -584.670] (0.0183) ({r_i: None, r_t: [-881.686 -881.686 -881.686], eps: 0.018})
Step:  129400, Reward: [-410.726 -410.726 -410.726] [61.174], Avg: [-733.645 -733.645 -733.645] (0.0100) ({r_i: None, r_t: [-834.995 -834.995 -834.995], eps: 0.01})
Step:  164200, Reward: [-876.284 -876.284 -876.284] [268.872], Avg: [-696.290 -696.290 -696.290] (0.1000) ({r_i: None, r_t: [-1630.483 -1630.483 -1630.483], eps: 0.1})
Step:   40000, Reward: [-445.829 -445.829 -445.829] [69.776], Avg: [-584.323 -584.323 -584.323] (0.0181) ({r_i: None, r_t: [-841.462 -841.462 -841.462], eps: 0.018})
Step:  129500, Reward: [-418.858 -418.858 -418.858] [101.108], Avg: [-733.402 -733.402 -733.402] (0.0100) ({r_i: None, r_t: [-833.978 -833.978 -833.978], eps: 0.01})
Step:  164300, Reward: [-714.619 -714.619 -714.619] [249.655], Avg: [-696.301 -696.301 -696.301] (0.1000) ({r_i: None, r_t: [-1531.148 -1531.148 -1531.148], eps: 0.1})
Step:   40100, Reward: [-437.476 -437.476 -437.476] [77.218], Avg: [-583.958 -583.958 -583.958] (0.0180) ({r_i: None, r_t: [-843.845 -843.845 -843.845], eps: 0.018})
Step:  129600, Reward: [-396.600 -396.600 -396.600] [105.827], Avg: [-733.142 -733.142 -733.142] (0.0100) ({r_i: None, r_t: [-889.453 -889.453 -889.453], eps: 0.01})
Step:  164400, Reward: [-905.694 -905.694 -905.694] [349.408], Avg: [-696.428 -696.428 -696.428] (0.1000) ({r_i: None, r_t: [-1472.166 -1472.166 -1472.166], eps: 0.1})
Step:   40200, Reward: [-446.327 -446.327 -446.327] [66.477], Avg: [-583.617 -583.617 -583.617] (0.0178) ({r_i: None, r_t: [-868.124 -868.124 -868.124], eps: 0.018})
Step:  129700, Reward: [-462.223 -462.223 -462.223] [67.978], Avg: [-732.934 -732.934 -732.934] (0.0100) ({r_i: None, r_t: [-866.346 -866.346 -866.346], eps: 0.01})
Step:  164500, Reward: [-859.551 -859.551 -859.551] [321.190], Avg: [-696.527 -696.527 -696.527] (0.1000) ({r_i: None, r_t: [-1580.105 -1580.105 -1580.105], eps: 0.1})
Step:   40300, Reward: [-411.057 -411.057 -411.057] [63.976], Avg: [-583.189 -583.189 -583.189] (0.0176) ({r_i: None, r_t: [-790.335 -790.335 -790.335], eps: 0.018})
Step:  129800, Reward: [-427.559 -427.559 -427.559] [89.643], Avg: [-732.699 -732.699 -732.699] (0.0100) ({r_i: None, r_t: [-887.507 -887.507 -887.507], eps: 0.01})
Step:  164600, Reward: [-832.354 -832.354 -832.354] [333.726], Avg: [-696.610 -696.610 -696.610] (0.1000) ({r_i: None, r_t: [-1629.237 -1629.237 -1629.237], eps: 0.1})
Step:   40400, Reward: [-426.526 -426.526 -426.526] [52.587], Avg: [-582.803 -582.803 -582.803] (0.0174) ({r_i: None, r_t: [-849.762 -849.762 -849.762], eps: 0.017})
Step:  129900, Reward: [-443.219 -443.219 -443.219] [85.492], Avg: [-732.476 -732.476 -732.476] (0.0100) ({r_i: None, r_t: [-838.308 -838.308 -838.308], eps: 0.01})
Step:  164700, Reward: [-764.453 -764.453 -764.453] [251.595], Avg: [-696.651 -696.651 -696.651] (0.1000) ({r_i: None, r_t: [-1630.040 -1630.040 -1630.040], eps: 0.1})
Step:   40500, Reward: [-392.777 -392.777 -392.777] [79.536], Avg: [-582.335 -582.335 -582.335] (0.0172) ({r_i: None, r_t: [-831.960 -831.960 -831.960], eps: 0.017})
Step:  130000, Reward: [-477.802 -477.802 -477.802] [98.795], Avg: [-732.280 -732.280 -732.280] (0.0100) ({r_i: None, r_t: [-864.715 -864.715 -864.715], eps: 0.01})
Step:  164800, Reward: [-940.068 -940.068 -940.068] [319.601], Avg: [-696.799 -696.799 -696.799] (0.1000) ({r_i: None, r_t: [-1609.042 -1609.042 -1609.042], eps: 0.1})
Step:   40600, Reward: [-452.301 -452.301 -452.301] [72.386], Avg: [-582.015 -582.015 -582.015] (0.0171) ({r_i: None, r_t: [-815.313 -815.313 -815.313], eps: 0.017})
Step:  164900, Reward: [-879.432 -879.432 -879.432] [311.878], Avg: [-696.909 -696.909 -696.909] (0.1000) ({r_i: None, r_t: [-1664.350 -1664.350 -1664.350], eps: 0.1})
Step:  130100, Reward: [-407.250 -407.250 -407.250] [88.160], Avg: [-732.031 -732.031 -732.031] (0.0100) ({r_i: None, r_t: [-839.443 -839.443 -839.443], eps: 0.01})
Step:   40700, Reward: [-407.219 -407.219 -407.219] [52.567], Avg: [-581.587 -581.587 -581.587] (0.0169) ({r_i: None, r_t: [-866.477 -866.477 -866.477], eps: 0.017})
Step:  165000, Reward: [-830.332 -830.332 -830.332] [344.068], Avg: [-696.990 -696.990 -696.990] (0.1000) ({r_i: None, r_t: [-1699.889 -1699.889 -1699.889], eps: 0.1})
Step:  130200, Reward: [-417.933 -417.933 -417.933] [64.463], Avg: [-731.790 -731.790 -731.790] (0.0100) ({r_i: None, r_t: [-899.446 -899.446 -899.446], eps: 0.01})
Step:   40800, Reward: [-387.695 -387.695 -387.695] [69.350], Avg: [-581.113 -581.113 -581.113] (0.0167) ({r_i: None, r_t: [-822.326 -822.326 -822.326], eps: 0.017})
Step:  165100, Reward: [-810.755 -810.755 -810.755] [262.411], Avg: [-697.059 -697.059 -697.059] (0.1000) ({r_i: None, r_t: [-1937.724 -1937.724 -1937.724], eps: 0.1})
Step:  130300, Reward: [-448.620 -448.620 -448.620] [89.768], Avg: [-731.572 -731.572 -731.572] (0.0100) ({r_i: None, r_t: [-901.907 -901.907 -901.907], eps: 0.01})
Step:   40900, Reward: [-403.213 -403.213 -403.213] [85.828], Avg: [-580.679 -580.679 -580.679] (0.0166) ({r_i: None, r_t: [-868.547 -868.547 -868.547], eps: 0.017})
Step:  165200, Reward: [-843.873 -843.873 -843.873] [328.623], Avg: [-697.148 -697.148 -697.148] (0.1000) ({r_i: None, r_t: [-2032.541 -2032.541 -2032.541], eps: 0.1})
Step:  130400, Reward: [-405.218 -405.218 -405.218] [117.302], Avg: [-731.322 -731.322 -731.322] (0.0100) ({r_i: None, r_t: [-797.652 -797.652 -797.652], eps: 0.01})
Step:   41000, Reward: [-393.570 -393.570 -393.570] [61.552], Avg: [-580.223 -580.223 -580.223] (0.0164) ({r_i: None, r_t: [-791.850 -791.850 -791.850], eps: 0.016})
Step:  165300, Reward: [-1003.046 -1003.046 -1003.046] [415.892], Avg: [-697.333 -697.333 -697.333] (0.1000) ({r_i: None, r_t: [-1776.932 -1776.932 -1776.932], eps: 0.1})
Step:  130500, Reward: [-446.483 -446.483 -446.483] [103.004], Avg: [-731.104 -731.104 -731.104] (0.0100) ({r_i: None, r_t: [-882.809 -882.809 -882.809], eps: 0.01})
Step:   41100, Reward: [-385.900 -385.900 -385.900] [75.099], Avg: [-579.752 -579.752 -579.752] (0.0162) ({r_i: None, r_t: [-784.256 -784.256 -784.256], eps: 0.016})
Step:  165400, Reward: [-803.117 -803.117 -803.117] [327.920], Avg: [-697.397 -697.397 -697.397] (0.1000) ({r_i: None, r_t: [-1659.240 -1659.240 -1659.240], eps: 0.1})
Step:  130600, Reward: [-474.411 -474.411 -474.411] [123.462], Avg: [-730.908 -730.908 -730.908] (0.0100) ({r_i: None, r_t: [-859.601 -859.601 -859.601], eps: 0.01})
Step:   41200, Reward: [-435.271 -435.271 -435.271] [78.083], Avg: [-579.402 -579.402 -579.402] (0.0161) ({r_i: None, r_t: [-779.496 -779.496 -779.496], eps: 0.016})
Step:  165500, Reward: [-953.225 -953.225 -953.225] [327.594], Avg: [-697.551 -697.551 -697.551] (0.1000) ({r_i: None, r_t: [-1648.264 -1648.264 -1648.264], eps: 0.1})
Step:  130700, Reward: [-421.420 -421.420 -421.420] [77.581], Avg: [-730.671 -730.671 -730.671] (0.0100) ({r_i: None, r_t: [-825.940 -825.940 -825.940], eps: 0.01})
Step:   41300, Reward: [-379.216 -379.216 -379.216] [67.843], Avg: [-578.918 -578.918 -578.918] (0.0159) ({r_i: None, r_t: [-797.358 -797.358 -797.358], eps: 0.016})
Step:  165600, Reward: [-901.131 -901.131 -901.131] [347.641], Avg: [-697.674 -697.674 -697.674] (0.1000) ({r_i: None, r_t: [-1709.752 -1709.752 -1709.752], eps: 0.1})
Step:  130800, Reward: [-445.436 -445.436 -445.436] [99.843], Avg: [-730.453 -730.453 -730.453] (0.0100) ({r_i: None, r_t: [-864.592 -864.592 -864.592], eps: 0.01})
Step:   41400, Reward: [-450.407 -450.407 -450.407] [107.491], Avg: [-578.609 -578.609 -578.609] (0.0158) ({r_i: None, r_t: [-786.292 -786.292 -786.292], eps: 0.016})
Step:  165700, Reward: [-789.144 -789.144 -789.144] [237.206], Avg: [-697.729 -697.729 -697.729] (0.1000) ({r_i: None, r_t: [-1492.019 -1492.019 -1492.019], eps: 0.1})
Step:  130900, Reward: [-409.284 -409.284 -409.284] [78.540], Avg: [-730.208 -730.208 -730.208] (0.0100) ({r_i: None, r_t: [-901.477 -901.477 -901.477], eps: 0.01})
Step:  165800, Reward: [-970.620 -970.620 -970.620] [392.296], Avg: [-697.894 -697.894 -697.894] (0.1000) ({r_i: None, r_t: [-1750.141 -1750.141 -1750.141], eps: 0.1})
Step:   41500, Reward: [-370.203 -370.203 -370.203] [83.131], Avg: [-578.108 -578.108 -578.108] (0.0156) ({r_i: None, r_t: [-782.417 -782.417 -782.417], eps: 0.016})
Step:  131000, Reward: [-414.081 -414.081 -414.081] [82.690], Avg: [-729.967 -729.967 -729.967] (0.0100) ({r_i: None, r_t: [-912.612 -912.612 -912.612], eps: 0.01})
Step:  165900, Reward: [-913.423 -913.423 -913.423] [419.876], Avg: [-698.023 -698.023 -698.023] (0.1000) ({r_i: None, r_t: [-1746.348 -1746.348 -1746.348], eps: 0.1})
Step:   41600, Reward: [-385.861 -385.861 -385.861] [62.082], Avg: [-577.647 -577.647 -577.647] (0.0154) ({r_i: None, r_t: [-810.352 -810.352 -810.352], eps: 0.015})
Step:  131100, Reward: [-413.840 -413.840 -413.840] [75.027], Avg: [-729.726 -729.726 -729.726] (0.0100) ({r_i: None, r_t: [-913.945 -913.945 -913.945], eps: 0.01})
Step:  166000, Reward: [-1000.938 -1000.938 -1000.938] [362.992], Avg: [-698.206 -698.206 -698.206] (0.1000) ({r_i: None, r_t: [-1830.553 -1830.553 -1830.553], eps: 0.1})
Step:   41700, Reward: [-415.897 -415.897 -415.897] [97.864], Avg: [-577.260 -577.260 -577.260] (0.0153) ({r_i: None, r_t: [-793.904 -793.904 -793.904], eps: 0.015})
Step:  131200, Reward: [-408.415 -408.415 -408.415] [78.081], Avg: [-729.481 -729.481 -729.481] (0.0100) ({r_i: None, r_t: [-963.306 -963.306 -963.306], eps: 0.01})
Step:  166100, Reward: [-977.769 -977.769 -977.769] [299.046], Avg: [-698.374 -698.374 -698.374] (0.1000) ({r_i: None, r_t: [-1839.346 -1839.346 -1839.346], eps: 0.1})
Step:   41800, Reward: [-395.844 -395.844 -395.844] [88.930], Avg: [-576.827 -576.827 -576.827] (0.0151) ({r_i: None, r_t: [-825.047 -825.047 -825.047], eps: 0.015})
Step:  131300, Reward: [-436.420 -436.420 -436.420] [79.952], Avg: [-729.258 -729.258 -729.258] (0.0100) ({r_i: None, r_t: [-874.290 -874.290 -874.290], eps: 0.01})
Step:  166200, Reward: [-699.075 -699.075 -699.075] [234.582], Avg: [-698.374 -698.374 -698.374] (0.1000) ({r_i: None, r_t: [-1673.328 -1673.328 -1673.328], eps: 0.1})
Step:   41900, Reward: [-382.326 -382.326 -382.326] [42.289], Avg: [-576.364 -576.364 -576.364] (0.0150) ({r_i: None, r_t: [-829.312 -829.312 -829.312], eps: 0.015})
Step:  131400, Reward: [-424.384 -424.384 -424.384] [90.733], Avg: [-729.026 -729.026 -729.026] (0.0100) ({r_i: None, r_t: [-882.566 -882.566 -882.566], eps: 0.01})
Step:  166300, Reward: [-795.318 -795.318 -795.318] [261.308], Avg: [-698.433 -698.433 -698.433] (0.1000) ({r_i: None, r_t: [-1515.114 -1515.114 -1515.114], eps: 0.1})
Step:   42000, Reward: [-378.715 -378.715 -378.715] [57.126], Avg: [-575.894 -575.894 -575.894] (0.0148) ({r_i: None, r_t: [-774.661 -774.661 -774.661], eps: 0.015})
Step:  131500, Reward: [-422.715 -422.715 -422.715] [79.875], Avg: [-728.794 -728.794 -728.794] (0.0100) ({r_i: None, r_t: [-886.050 -886.050 -886.050], eps: 0.01})
Step:  166400, Reward: [-931.749 -931.749 -931.749] [347.806], Avg: [-698.573 -698.573 -698.573] (0.1000) ({r_i: None, r_t: [-1645.386 -1645.386 -1645.386], eps: 0.1})
Step:   42100, Reward: [-383.927 -383.927 -383.927] [54.267], Avg: [-575.439 -575.439 -575.439] (0.0147) ({r_i: None, r_t: [-812.789 -812.789 -812.789], eps: 0.015})
Step:  131600, Reward: [-419.772 -419.772 -419.772] [91.972], Avg: [-728.559 -728.559 -728.559] (0.0100) ({r_i: None, r_t: [-903.592 -903.592 -903.592], eps: 0.01})
Step:  166500, Reward: [-934.354 -934.354 -934.354] [323.134], Avg: [-698.714 -698.714 -698.714] (0.1000) ({r_i: None, r_t: [-1775.913 -1775.913 -1775.913], eps: 0.1})
Step:   42200, Reward: [-431.538 -431.538 -431.538] [96.008], Avg: [-575.099 -575.099 -575.099] (0.0145) ({r_i: None, r_t: [-818.869 -818.869 -818.869], eps: 0.015})
Step:  131700, Reward: [-457.325 -457.325 -457.325] [86.491], Avg: [-728.353 -728.353 -728.353] (0.0100) ({r_i: None, r_t: [-885.030 -885.030 -885.030], eps: 0.01})
Step:  166600, Reward: [-886.706 -886.706 -886.706] [327.388], Avg: [-698.827 -698.827 -698.827] (0.1000) ({r_i: None, r_t: [-1813.015 -1813.015 -1813.015], eps: 0.1})
Step:   42300, Reward: [-415.146 -415.146 -415.146] [52.766], Avg: [-574.722 -574.722 -574.722] (0.0144) ({r_i: None, r_t: [-788.591 -788.591 -788.591], eps: 0.014})
Step:  131800, Reward: [-432.694 -432.694 -432.694] [66.667], Avg: [-728.129 -728.129 -728.129] (0.0100) ({r_i: None, r_t: [-815.387 -815.387 -815.387], eps: 0.01})
Step:  166700, Reward: [-828.856 -828.856 -828.856] [377.129], Avg: [-698.905 -698.905 -698.905] (0.1000) ({r_i: None, r_t: [-1720.563 -1720.563 -1720.563], eps: 0.1})
Step:   42400, Reward: [-374.533 -374.533 -374.533] [62.455], Avg: [-574.251 -574.251 -574.251] (0.0143) ({r_i: None, r_t: [-824.859 -824.859 -824.859], eps: 0.014})
Step:  131900, Reward: [-437.128 -437.128 -437.128] [100.075], Avg: [-727.909 -727.909 -727.909] (0.0100) ({r_i: None, r_t: [-858.853 -858.853 -858.853], eps: 0.01})
Step:  166800, Reward: [-875.097 -875.097 -875.097] [292.344], Avg: [-699.011 -699.011 -699.011] (0.1000) ({r_i: None, r_t: [-1719.662 -1719.662 -1719.662], eps: 0.1})
Step:   42500, Reward: [-395.113 -395.113 -395.113] [59.972], Avg: [-573.830 -573.830 -573.830] (0.0141) ({r_i: None, r_t: [-855.816 -855.816 -855.816], eps: 0.014})
Step:  132000, Reward: [-438.345 -438.345 -438.345] [87.516], Avg: [-727.689 -727.689 -727.689] (0.0100) ({r_i: None, r_t: [-848.729 -848.729 -848.729], eps: 0.01})
Step:  166900, Reward: [-1024.822 -1024.822 -1024.822] [434.428], Avg: [-699.206 -699.206 -699.206] (0.1000) ({r_i: None, r_t: [-1660.962 -1660.962 -1660.962], eps: 0.1})
Step:   42600, Reward: [-418.801 -418.801 -418.801] [82.393], Avg: [-573.467 -573.467 -573.467] (0.0140) ({r_i: None, r_t: [-782.472 -782.472 -782.472], eps: 0.014})
Step:  132100, Reward: [-462.118 -462.118 -462.118] [70.605], Avg: [-727.489 -727.489 -727.489] (0.0100) ({r_i: None, r_t: [-864.250 -864.250 -864.250], eps: 0.01})
Step:  167000, Reward: [-1000.231 -1000.231 -1000.231] [312.260], Avg: [-699.386 -699.386 -699.386] (0.1000) ({r_i: None, r_t: [-1760.852 -1760.852 -1760.852], eps: 0.1})
Step:   42700, Reward: [-393.949 -393.949 -393.949] [63.538], Avg: [-573.048 -573.048 -573.048] (0.0138) ({r_i: None, r_t: [-832.711 -832.711 -832.711], eps: 0.014})
Step:  132200, Reward: [-449.989 -449.989 -449.989] [86.273], Avg: [-727.279 -727.279 -727.279] (0.0100) ({r_i: None, r_t: [-891.629 -891.629 -891.629], eps: 0.01})
Step:  167100, Reward: [-887.764 -887.764 -887.764] [257.099], Avg: [-699.499 -699.499 -699.499] (0.1000) ({r_i: None, r_t: [-1796.011 -1796.011 -1796.011], eps: 0.1})
Step:   42800, Reward: [-437.698 -437.698 -437.698] [74.772], Avg: [-572.732 -572.732 -572.732] (0.0137) ({r_i: None, r_t: [-810.121 -810.121 -810.121], eps: 0.014})
Step:  167200, Reward: [-825.288 -825.288 -825.288] [311.856], Avg: [-699.574 -699.574 -699.574] (0.1000) ({r_i: None, r_t: [-1703.292 -1703.292 -1703.292], eps: 0.1})
Step:  132300, Reward: [-387.227 -387.227 -387.227] [92.233], Avg: [-727.022 -727.022 -727.022] (0.0100) ({r_i: None, r_t: [-906.553 -906.553 -906.553], eps: 0.01})
Step:   42900, Reward: [-415.662 -415.662 -415.662] [71.993], Avg: [-572.367 -572.367 -572.367] (0.0136) ({r_i: None, r_t: [-739.896 -739.896 -739.896], eps: 0.014})
Step:  167300, Reward: [-779.706 -779.706 -779.706] [228.732], Avg: [-699.622 -699.622 -699.622] (0.1000) ({r_i: None, r_t: [-1803.672 -1803.672 -1803.672], eps: 0.1})
Step:  132400, Reward: [-454.597 -454.597 -454.597] [133.321], Avg: [-726.816 -726.816 -726.816] (0.0100) ({r_i: None, r_t: [-876.242 -876.242 -876.242], eps: 0.01})
Step:   43000, Reward: [-417.100 -417.100 -417.100] [63.601], Avg: [-572.007 -572.007 -572.007] (0.0134) ({r_i: None, r_t: [-818.745 -818.745 -818.745], eps: 0.013})
Step:  167400, Reward: [-973.875 -973.875 -973.875] [347.409], Avg: [-699.785 -699.785 -699.785] (0.1000) ({r_i: None, r_t: [-1669.114 -1669.114 -1669.114], eps: 0.1})
Step:  132500, Reward: [-436.213 -436.213 -436.213] [86.612], Avg: [-726.597 -726.597 -726.597] (0.0100) ({r_i: None, r_t: [-873.928 -873.928 -873.928], eps: 0.01})
Step:   43100, Reward: [-407.827 -407.827 -407.827] [54.721], Avg: [-571.627 -571.627 -571.627] (0.0133) ({r_i: None, r_t: [-780.745 -780.745 -780.745], eps: 0.013})
Step:  167500, Reward: [-986.742 -986.742 -986.742] [352.276], Avg: [-699.957 -699.957 -699.957] (0.1000) ({r_i: None, r_t: [-1625.060 -1625.060 -1625.060], eps: 0.1})
Step:  132600, Reward: [-445.756 -445.756 -445.756] [89.731], Avg: [-726.386 -726.386 -726.386] (0.0100) ({r_i: None, r_t: [-889.231 -889.231 -889.231], eps: 0.01})
Step:   43200, Reward: [-396.221 -396.221 -396.221] [77.872], Avg: [-571.222 -571.222 -571.222] (0.0132) ({r_i: None, r_t: [-786.622 -786.622 -786.622], eps: 0.013})
Step:  167600, Reward: [-853.111 -853.111 -853.111] [306.976], Avg: [-700.048 -700.048 -700.048] (0.1000) ({r_i: None, r_t: [-1787.376 -1787.376 -1787.376], eps: 0.1})
Step:  132700, Reward: [-438.059 -438.059 -438.059] [93.926], Avg: [-726.168 -726.168 -726.168] (0.0100) ({r_i: None, r_t: [-818.067 -818.067 -818.067], eps: 0.01})
Step:   43300, Reward: [-368.706 -368.706 -368.706] [66.524], Avg: [-570.755 -570.755 -570.755] (0.0130) ({r_i: None, r_t: [-818.170 -818.170 -818.170], eps: 0.013})
Step:  167700, Reward: [-937.071 -937.071 -937.071] [356.930], Avg: [-700.189 -700.189 -700.189] (0.1000) ({r_i: None, r_t: [-2089.287 -2089.287 -2089.287], eps: 0.1})
Step:  132800, Reward: [-432.985 -432.985 -432.985] [106.523], Avg: [-725.948 -725.948 -725.948] (0.0100) ({r_i: None, r_t: [-861.844 -861.844 -861.844], eps: 0.01})
Step:  167800, Reward: [-858.771 -858.771 -858.771] [245.263], Avg: [-700.284 -700.284 -700.284] (0.1000) ({r_i: None, r_t: [-1936.616 -1936.616 -1936.616], eps: 0.1})
Step:   43400, Reward: [-409.737 -409.737 -409.737] [79.173], Avg: [-570.385 -570.385 -570.385] (0.0129) ({r_i: None, r_t: [-764.412 -764.412 -764.412], eps: 0.013})
Step:  132900, Reward: [-424.306 -424.306 -424.306] [92.892], Avg: [-725.721 -725.721 -725.721] (0.0100) ({r_i: None, r_t: [-817.772 -817.772 -817.772], eps: 0.01})
Step:  167900, Reward: [-1026.891 -1026.891 -1026.891] [285.245], Avg: [-700.478 -700.478 -700.478] (0.1000) ({r_i: None, r_t: [-1891.258 -1891.258 -1891.258], eps: 0.1})
Step:   43500, Reward: [-396.774 -396.774 -396.774] [72.863], Avg: [-569.987 -569.987 -569.987] (0.0128) ({r_i: None, r_t: [-823.717 -823.717 -823.717], eps: 0.013})
Step:  133000, Reward: [-436.685 -436.685 -436.685] [74.627], Avg: [-725.504 -725.504 -725.504] (0.0100) ({r_i: None, r_t: [-885.273 -885.273 -885.273], eps: 0.01})
Step:  168000, Reward: [-1022.387 -1022.387 -1022.387] [243.213], Avg: [-700.669 -700.669 -700.669] (0.1000) ({r_i: None, r_t: [-1792.442 -1792.442 -1792.442], eps: 0.1})
Step:   43600, Reward: [-381.628 -381.628 -381.628] [58.689], Avg: [-569.556 -569.556 -569.556] (0.0126) ({r_i: None, r_t: [-853.864 -853.864 -853.864], eps: 0.013})
Step:  133100, Reward: [-449.391 -449.391 -449.391] [88.362], Avg: [-725.297 -725.297 -725.297] (0.0100) ({r_i: None, r_t: [-869.593 -869.593 -869.593], eps: 0.01})
Step:  168100, Reward: [-861.360 -861.360 -861.360] [317.570], Avg: [-700.765 -700.765 -700.765] (0.1000) ({r_i: None, r_t: [-1789.257 -1789.257 -1789.257], eps: 0.1})
Step:   43700, Reward: [-404.507 -404.507 -404.507] [67.705], Avg: [-569.179 -569.179 -569.179] (0.0125) ({r_i: None, r_t: [-766.645 -766.645 -766.645], eps: 0.013})
Step:  133200, Reward: [-422.286 -422.286 -422.286] [91.562], Avg: [-725.069 -725.069 -725.069] (0.0100) ({r_i: None, r_t: [-911.217 -911.217 -911.217], eps: 0.01})
Step:  168200, Reward: [-1044.395 -1044.395 -1044.395] [353.444], Avg: [-700.969 -700.969 -700.969] (0.1000) ({r_i: None, r_t: [-2019.408 -2019.408 -2019.408], eps: 0.1})
Step:   43800, Reward: [-411.697 -411.697 -411.697] [105.535], Avg: [-568.820 -568.820 -568.820] (0.0124) ({r_i: None, r_t: [-820.585 -820.585 -820.585], eps: 0.012})
Step:  133300, Reward: [-451.671 -451.671 -451.671] [74.506], Avg: [-724.864 -724.864 -724.864] (0.0100) ({r_i: None, r_t: [-891.724 -891.724 -891.724], eps: 0.01})
Step:  168300, Reward: [-1047.150 -1047.150 -1047.150] [366.021], Avg: [-701.175 -701.175 -701.175] (0.1000) ({r_i: None, r_t: [-1849.014 -1849.014 -1849.014], eps: 0.1})
Step:   43900, Reward: [-416.858 -416.858 -416.858] [54.350], Avg: [-568.475 -568.475 -568.475] (0.0123) ({r_i: None, r_t: [-747.808 -747.808 -747.808], eps: 0.012})
Step:  133400, Reward: [-403.669 -403.669 -403.669] [79.335], Avg: [-724.624 -724.624 -724.624] (0.0100) ({r_i: None, r_t: [-877.561 -877.561 -877.561], eps: 0.01})
Step:  168400, Reward: [-928.905 -928.905 -928.905] [278.801], Avg: [-701.310 -701.310 -701.310] (0.1000) ({r_i: None, r_t: [-2019.795 -2019.795 -2019.795], eps: 0.1})
Step:   44000, Reward: [-417.132 -417.132 -417.132] [58.589], Avg: [-568.132 -568.132 -568.132] (0.0121) ({r_i: None, r_t: [-789.809 -789.809 -789.809], eps: 0.012})
Step:  133500, Reward: [-484.935 -484.935 -484.935] [145.580], Avg: [-724.444 -724.444 -724.444] (0.0100) ({r_i: None, r_t: [-880.901 -880.901 -880.901], eps: 0.01})
Step:  168500, Reward: [-968.381 -968.381 -968.381] [252.728], Avg: [-701.468 -701.468 -701.468] (0.1000) ({r_i: None, r_t: [-1779.975 -1779.975 -1779.975], eps: 0.1})
Step:   44100, Reward: [-367.021 -367.021 -367.021] [51.424], Avg: [-567.677 -567.677 -567.677] (0.0120) ({r_i: None, r_t: [-785.142 -785.142 -785.142], eps: 0.012})
Step:  133600, Reward: [-472.024 -472.024 -472.024] [90.381], Avg: [-724.256 -724.256 -724.256] (0.0100) ({r_i: None, r_t: [-846.089 -846.089 -846.089], eps: 0.01})
Step:  168600, Reward: [-963.252 -963.252 -963.252] [373.222], Avg: [-701.624 -701.624 -701.624] (0.1000) ({r_i: None, r_t: [-1840.899 -1840.899 -1840.899], eps: 0.1})
Step:   44200, Reward: [-381.173 -381.173 -381.173] [53.153], Avg: [-567.256 -567.256 -567.256] (0.0119) ({r_i: None, r_t: [-773.102 -773.102 -773.102], eps: 0.012})
Step:  133700, Reward: [-430.901 -430.901 -430.901] [76.380], Avg: [-724.036 -724.036 -724.036] (0.0100) ({r_i: None, r_t: [-874.464 -874.464 -874.464], eps: 0.01})
Step:  168700, Reward: [-1030.890 -1030.890 -1030.890] [452.388], Avg: [-701.819 -701.819 -701.819] (0.1000) ({r_i: None, r_t: [-2211.312 -2211.312 -2211.312], eps: 0.1})
Step:   44300, Reward: [-386.525 -386.525 -386.525] [63.287], Avg: [-566.849 -566.849 -566.849] (0.0118) ({r_i: None, r_t: [-772.534 -772.534 -772.534], eps: 0.012})
Step:  133800, Reward: [-412.565 -412.565 -412.565] [72.880], Avg: [-723.804 -723.804 -723.804] (0.0100) ({r_i: None, r_t: [-918.164 -918.164 -918.164], eps: 0.01})
Step:  168800, Reward: [-980.429 -980.429 -980.429] [328.387], Avg: [-701.984 -701.984 -701.984] (0.1000) ({r_i: None, r_t: [-2089.316 -2089.316 -2089.316], eps: 0.1})
Step:   44400, Reward: [-410.984 -410.984 -410.984] [68.088], Avg: [-566.498 -566.498 -566.498] (0.0117) ({r_i: None, r_t: [-823.775 -823.775 -823.775], eps: 0.012})
Step:  133900, Reward: [-428.831 -428.831 -428.831] [91.662], Avg: [-723.584 -723.584 -723.584] (0.0100) ({r_i: None, r_t: [-822.102 -822.102 -822.102], eps: 0.01})
Step:  168900, Reward: [-874.348 -874.348 -874.348] [263.923], Avg: [-702.086 -702.086 -702.086] (0.1000) ({r_i: None, r_t: [-2042.675 -2042.675 -2042.675], eps: 0.1})
Step:   44500, Reward: [-431.780 -431.780 -431.780] [104.832], Avg: [-566.196 -566.196 -566.196] (0.0115) ({r_i: None, r_t: [-811.305 -811.305 -811.305], eps: 0.012})
Step:  134000, Reward: [-449.924 -449.924 -449.924] [97.830], Avg: [-723.379 -723.379 -723.379] (0.0100) ({r_i: None, r_t: [-878.379 -878.379 -878.379], eps: 0.01})
Step:  169000, Reward: [-1096.918 -1096.918 -1096.918] [281.517], Avg: [-702.319 -702.319 -702.319] (0.1000) ({r_i: None, r_t: [-1979.310 -1979.310 -1979.310], eps: 0.1})
Step:   44600, Reward: [-433.750 -433.750 -433.750] [77.815], Avg: [-565.900 -565.900 -565.900] (0.0114) ({r_i: None, r_t: [-815.689 -815.689 -815.689], eps: 0.011})
Step:  134100, Reward: [-398.676 -398.676 -398.676] [110.677], Avg: [-723.138 -723.138 -723.138] (0.0100) ({r_i: None, r_t: [-860.579 -860.579 -860.579], eps: 0.01})
Step:  169100, Reward: [-961.919 -961.919 -961.919] [227.447], Avg: [-702.472 -702.472 -702.472] (0.1000) ({r_i: None, r_t: [-2174.374 -2174.374 -2174.374], eps: 0.1})
Step:   44700, Reward: [-400.730 -400.730 -400.730] [81.173], Avg: [-565.531 -565.531 -565.531] (0.0113) ({r_i: None, r_t: [-815.096 -815.096 -815.096], eps: 0.011})
Step:  134200, Reward: [-427.432 -427.432 -427.432] [86.720], Avg: [-722.917 -722.917 -722.917] (0.0100) ({r_i: None, r_t: [-845.775 -845.775 -845.775], eps: 0.01})
Step:  169200, Reward: [-1127.378 -1127.378 -1127.378] [280.211], Avg: [-702.723 -702.723 -702.723] (0.1000) ({r_i: None, r_t: [-2043.419 -2043.419 -2043.419], eps: 0.1})
Step:   44800, Reward: [-419.332 -419.332 -419.332] [73.449], Avg: [-565.206 -565.206 -565.206] (0.0112) ({r_i: None, r_t: [-780.835 -780.835 -780.835], eps: 0.011})
Step:  134300, Reward: [-402.350 -402.350 -402.350] [60.916], Avg: [-722.679 -722.679 -722.679] (0.0100) ({r_i: None, r_t: [-896.965 -896.965 -896.965], eps: 0.01})
Step:  169300, Reward: [-1255.849 -1255.849 -1255.849] [360.713], Avg: [-703.050 -703.050 -703.050] (0.1000) ({r_i: None, r_t: [-1973.532 -1973.532 -1973.532], eps: 0.1})
Step:   44900, Reward: [-432.699 -432.699 -432.699] [72.058], Avg: [-564.911 -564.911 -564.911] (0.0111) ({r_i: None, r_t: [-826.024 -826.024 -826.024], eps: 0.011})
Step:  134400, Reward: [-446.523 -446.523 -446.523] [92.234], Avg: [-722.474 -722.474 -722.474] (0.0100) ({r_i: None, r_t: [-919.252 -919.252 -919.252], eps: 0.01})
Step:  169400, Reward: [-1065.461 -1065.461 -1065.461] [352.911], Avg: [-703.264 -703.264 -703.264] (0.1000) ({r_i: None, r_t: [-2022.403 -2022.403 -2022.403], eps: 0.1})
Step:   45000, Reward: [-388.535 -388.535 -388.535] [71.414], Avg: [-564.520 -564.520 -564.520] (0.0110) ({r_i: None, r_t: [-834.852 -834.852 -834.852], eps: 0.011})
Step:  134500, Reward: [-482.421 -482.421 -482.421] [104.616], Avg: [-722.295 -722.295 -722.295] (0.0100) ({r_i: None, r_t: [-887.137 -887.137 -887.137], eps: 0.01})
Step:  169500, Reward: [-1050.298 -1050.298 -1050.298] [235.504], Avg: [-703.468 -703.468 -703.468] (0.1000) ({r_i: None, r_t: [-1883.602 -1883.602 -1883.602], eps: 0.1})
Step:   45100, Reward: [-433.709 -433.709 -433.709] [90.715], Avg: [-564.231 -564.231 -564.231] (0.0109) ({r_i: None, r_t: [-884.228 -884.228 -884.228], eps: 0.011})
Step:  134600, Reward: [-471.639 -471.639 -471.639] [106.539], Avg: [-722.109 -722.109 -722.109] (0.0100) ({r_i: None, r_t: [-951.257 -951.257 -951.257], eps: 0.01})
Step:  169600, Reward: [-945.219 -945.219 -945.219] [373.251], Avg: [-703.611 -703.611 -703.611] (0.1000) ({r_i: None, r_t: [-2302.154 -2302.154 -2302.154], eps: 0.1})
Step:   45200, Reward: [-397.094 -397.094 -397.094] [59.236], Avg: [-563.862 -563.862 -563.862] (0.0108) ({r_i: None, r_t: [-802.997 -802.997 -802.997], eps: 0.011})
Step:  169700, Reward: [-1137.796 -1137.796 -1137.796] [423.074], Avg: [-703.867 -703.867 -703.867] (0.1000) ({r_i: None, r_t: [-2000.762 -2000.762 -2000.762], eps: 0.1})
Step:  134700, Reward: [-470.694 -470.694 -470.694] [114.349], Avg: [-721.923 -721.923 -721.923] (0.0100) ({r_i: None, r_t: [-868.808 -868.808 -868.808], eps: 0.01})
Step:   45300, Reward: [-449.562 -449.562 -449.562] [84.249], Avg: [-563.610 -563.610 -563.610] (0.0107) ({r_i: None, r_t: [-821.975 -821.975 -821.975], eps: 0.011})
Step:  169800, Reward: [-1050.615 -1050.615 -1050.615] [447.752], Avg: [-704.071 -704.071 -704.071] (0.1000) ({r_i: None, r_t: [-1991.406 -1991.406 -1991.406], eps: 0.1})
Step:  134800, Reward: [-460.690 -460.690 -460.690] [100.810], Avg: [-721.729 -721.729 -721.729] (0.0100) ({r_i: None, r_t: [-846.777 -846.777 -846.777], eps: 0.01})
Step:   45400, Reward: [-425.917 -425.917 -425.917] [51.100], Avg: [-563.307 -563.307 -563.307] (0.0106) ({r_i: None, r_t: [-775.914 -775.914 -775.914], eps: 0.011})
Step:  169900, Reward: [-1137.798 -1137.798 -1137.798] [404.618], Avg: [-704.326 -704.326 -704.326] (0.1000) ({r_i: None, r_t: [-2009.669 -2009.669 -2009.669], eps: 0.1})
Step:  134900, Reward: [-447.971 -447.971 -447.971] [67.901], Avg: [-721.526 -721.526 -721.526] (0.0100) ({r_i: None, r_t: [-858.093 -858.093 -858.093], eps: 0.01})
Step:   45500, Reward: [-402.629 -402.629 -402.629] [68.372], Avg: [-562.955 -562.955 -562.955] (0.0104) ({r_i: None, r_t: [-848.251 -848.251 -848.251], eps: 0.01})
Step:  170000, Reward: [-1098.310 -1098.310 -1098.310] [309.891], Avg: [-704.557 -704.557 -704.557] (0.1000) ({r_i: None, r_t: [-2275.669 -2275.669 -2275.669], eps: 0.1})
Step:  135000, Reward: [-451.334 -451.334 -451.334] [61.110], Avg: [-721.326 -721.326 -721.326] (0.0100) ({r_i: None, r_t: [-892.415 -892.415 -892.415], eps: 0.01})
Step:  170100, Reward: [-1219.729 -1219.729 -1219.729] [390.827], Avg: [-704.860 -704.860 -704.860] (0.1000) ({r_i: None, r_t: [-2186.261 -2186.261 -2186.261], eps: 0.1})
Step:   45600, Reward: [-389.323 -389.323 -389.323] [64.023], Avg: [-562.575 -562.575 -562.575] (0.0103) ({r_i: None, r_t: [-801.742 -801.742 -801.742], eps: 0.01})
Step:  135100, Reward: [-419.127 -419.127 -419.127] [85.658], Avg: [-721.103 -721.103 -721.103] (0.0100) ({r_i: None, r_t: [-883.488 -883.488 -883.488], eps: 0.01})
Step:  170200, Reward: [-1007.828 -1007.828 -1007.828] [391.264], Avg: [-705.038 -705.038 -705.038] (0.1000) ({r_i: None, r_t: [-1937.085 -1937.085 -1937.085], eps: 0.1})
Step:   45700, Reward: [-441.340 -441.340 -441.340] [92.025], Avg: [-562.310 -562.310 -562.310] (0.0102) ({r_i: None, r_t: [-818.202 -818.202 -818.202], eps: 0.01})
Step:  135200, Reward: [-457.782 -457.782 -457.782] [97.409], Avg: [-720.908 -720.908 -720.908] (0.0100) ({r_i: None, r_t: [-908.516 -908.516 -908.516], eps: 0.01})
Step:  170300, Reward: [-1115.061 -1115.061 -1115.061] [272.820], Avg: [-705.279 -705.279 -705.279] (0.1000) ({r_i: None, r_t: [-2121.163 -2121.163 -2121.163], eps: 0.1})
Step:   45800, Reward: [-428.216 -428.216 -428.216] [84.777], Avg: [-562.018 -562.018 -562.018] (0.0101) ({r_i: None, r_t: [-785.204 -785.204 -785.204], eps: 0.01})
Step:  135300, Reward: [-448.981 -448.981 -448.981] [99.213], Avg: [-720.707 -720.707 -720.707] (0.0100) ({r_i: None, r_t: [-875.973 -875.973 -875.973], eps: 0.01})
Step:  170400, Reward: [-1209.613 -1209.613 -1209.613] [409.086], Avg: [-705.574 -705.574 -705.574] (0.1000) ({r_i: None, r_t: [-2391.422 -2391.422 -2391.422], eps: 0.1})
Step:   45900, Reward: [-386.658 -386.658 -386.658] [59.622], Avg: [-561.637 -561.637 -561.637] (0.0100) ({r_i: None, r_t: [-786.023 -786.023 -786.023], eps: 0.01})
Step:  135400, Reward: [-462.449 -462.449 -462.449] [89.354], Avg: [-720.517 -720.517 -720.517] (0.0100) ({r_i: None, r_t: [-852.373 -852.373 -852.373], eps: 0.01})
Step:  170500, Reward: [-1109.922 -1109.922 -1109.922] [430.710], Avg: [-705.811 -705.811 -705.811] (0.1000) ({r_i: None, r_t: [-2358.252 -2358.252 -2358.252], eps: 0.1})
Step:   46000, Reward: [-417.877 -417.877 -417.877] [101.360], Avg: [-561.325 -561.325 -561.325] (0.0100) ({r_i: None, r_t: [-765.039 -765.039 -765.039], eps: 0.01})
Step:  135500, Reward: [-432.002 -432.002 -432.002] [95.063], Avg: [-720.304 -720.304 -720.304] (0.0100) ({r_i: None, r_t: [-847.311 -847.311 -847.311], eps: 0.01})
Step:  170600, Reward: [-1073.648 -1073.648 -1073.648] [260.709], Avg: [-706.027 -706.027 -706.027] (0.1000) ({r_i: None, r_t: [-2083.724 -2083.724 -2083.724], eps: 0.1})
Step:   46100, Reward: [-432.639 -432.639 -432.639] [101.355], Avg: [-561.047 -561.047 -561.047] (0.0100) ({r_i: None, r_t: [-790.248 -790.248 -790.248], eps: 0.01})
Step:  135600, Reward: [-448.193 -448.193 -448.193] [113.904], Avg: [-720.103 -720.103 -720.103] (0.0100) ({r_i: None, r_t: [-911.552 -911.552 -911.552], eps: 0.01})
Step:  170700, Reward: [-973.780 -973.780 -973.780] [313.433], Avg: [-706.184 -706.184 -706.184] (0.1000) ({r_i: None, r_t: [-2099.080 -2099.080 -2099.080], eps: 0.1})
Step:   46200, Reward: [-357.087 -357.087 -357.087] [58.142], Avg: [-560.606 -560.606 -560.606] (0.0100) ({r_i: None, r_t: [-761.000 -761.000 -761.000], eps: 0.01})
Step:  135700, Reward: [-445.856 -445.856 -445.856] [70.404], Avg: [-719.901 -719.901 -719.901] (0.0100) ({r_i: None, r_t: [-864.924 -864.924 -864.924], eps: 0.01})
Step:  170800, Reward: [-1116.799 -1116.799 -1116.799] [449.876], Avg: [-706.424 -706.424 -706.424] (0.1000) ({r_i: None, r_t: [-2507.113 -2507.113 -2507.113], eps: 0.1})
Step:   46300, Reward: [-403.737 -403.737 -403.737] [78.050], Avg: [-560.268 -560.268 -560.268] (0.0100) ({r_i: None, r_t: [-807.067 -807.067 -807.067], eps: 0.01})
Step:  135800, Reward: [-475.394 -475.394 -475.394] [110.761], Avg: [-719.721 -719.721 -719.721] (0.0100) ({r_i: None, r_t: [-825.331 -825.331 -825.331], eps: 0.01})
Step:  170900, Reward: [-1208.690 -1208.690 -1208.690] [369.059], Avg: [-706.718 -706.718 -706.718] (0.1000) ({r_i: None, r_t: [-2156.030 -2156.030 -2156.030], eps: 0.1})
Step:   46400, Reward: [-401.961 -401.961 -401.961] [78.013], Avg: [-559.928 -559.928 -559.928] (0.0100) ({r_i: None, r_t: [-819.429 -819.429 -819.429], eps: 0.01})
Step:  135900, Reward: [-446.094 -446.094 -446.094] [105.649], Avg: [-719.520 -719.520 -719.520] (0.0100) ({r_i: None, r_t: [-894.611 -894.611 -894.611], eps: 0.01})
Step:  171000, Reward: [-1021.547 -1021.547 -1021.547] [429.890], Avg: [-706.902 -706.902 -706.902] (0.1000) ({r_i: None, r_t: [-2258.385 -2258.385 -2258.385], eps: 0.1})
Step:   46500, Reward: [-344.825 -344.825 -344.825] [49.232], Avg: [-559.466 -559.466 -559.466] (0.0100) ({r_i: None, r_t: [-826.116 -826.116 -826.116], eps: 0.01})
Step:  136000, Reward: [-431.542 -431.542 -431.542] [117.765], Avg: [-719.309 -719.309 -719.309] (0.0100) ({r_i: None, r_t: [-804.189 -804.189 -804.189], eps: 0.01})
Step:  171100, Reward: [-1169.422 -1169.422 -1169.422] [381.100], Avg: [-707.172 -707.172 -707.172] (0.1000) ({r_i: None, r_t: [-2245.779 -2245.779 -2245.779], eps: 0.1})
Step:   46600, Reward: [-396.870 -396.870 -396.870] [92.369], Avg: [-559.118 -559.118 -559.118] (0.0100) ({r_i: None, r_t: [-794.727 -794.727 -794.727], eps: 0.01})
Step:  136100, Reward: [-424.172 -424.172 -424.172] [84.053], Avg: [-719.092 -719.092 -719.092] (0.0100) ({r_i: None, r_t: [-889.834 -889.834 -889.834], eps: 0.01})
Step:  171200, Reward: [-975.871 -975.871 -975.871] [246.632], Avg: [-707.329 -707.329 -707.329] (0.1000) ({r_i: None, r_t: [-2265.487 -2265.487 -2265.487], eps: 0.1})
Step:   46700, Reward: [-406.773 -406.773 -406.773] [60.992], Avg: [-558.792 -558.792 -558.792] (0.0100) ({r_i: None, r_t: [-792.938 -792.938 -792.938], eps: 0.01})
Step:  136200, Reward: [-407.310 -407.310 -407.310] [79.226], Avg: [-718.863 -718.863 -718.863] (0.0100) ({r_i: None, r_t: [-872.699 -872.699 -872.699], eps: 0.01})
Step:  171300, Reward: [-1135.954 -1135.954 -1135.954] [319.216], Avg: [-707.579 -707.579 -707.579] (0.1000) ({r_i: None, r_t: [-2333.502 -2333.502 -2333.502], eps: 0.1})
Step:   46800, Reward: [-386.761 -386.761 -386.761] [80.296], Avg: [-558.425 -558.425 -558.425] (0.0100) ({r_i: None, r_t: [-807.850 -807.850 -807.850], eps: 0.01})
Step:  136300, Reward: [-390.288 -390.288 -390.288] [82.233], Avg: [-718.622 -718.622 -718.622] (0.0100) ({r_i: None, r_t: [-895.031 -895.031 -895.031], eps: 0.01})
Step:  171400, Reward: [-1137.395 -1137.395 -1137.395] [457.785], Avg: [-707.829 -707.829 -707.829] (0.1000) ({r_i: None, r_t: [-2311.561 -2311.561 -2311.561], eps: 0.1})
Step:   46900, Reward: [-385.861 -385.861 -385.861] [61.484], Avg: [-558.058 -558.058 -558.058] (0.0100) ({r_i: None, r_t: [-849.205 -849.205 -849.205], eps: 0.01})
Step:  136400, Reward: [-435.924 -435.924 -435.924] [115.732], Avg: [-718.415 -718.415 -718.415] (0.0100) ({r_i: None, r_t: [-898.427 -898.427 -898.427], eps: 0.01})
Step:  171500, Reward: [-1064.520 -1064.520 -1064.520] [352.107], Avg: [-708.037 -708.037 -708.037] (0.1000) ({r_i: None, r_t: [-2295.567 -2295.567 -2295.567], eps: 0.1})
Step:   47000, Reward: [-393.819 -393.819 -393.819] [90.452], Avg: [-557.710 -557.710 -557.710] (0.0100) ({r_i: None, r_t: [-840.910 -840.910 -840.910], eps: 0.01})
Step:  136500, Reward: [-447.051 -447.051 -447.051] [100.540], Avg: [-718.217 -718.217 -718.217] (0.0100) ({r_i: None, r_t: [-860.643 -860.643 -860.643], eps: 0.01})
Step:  171600, Reward: [-1135.175 -1135.175 -1135.175] [484.461], Avg: [-708.286 -708.286 -708.286] (0.1000) ({r_i: None, r_t: [-2161.315 -2161.315 -2161.315], eps: 0.1})
Step:   47100, Reward: [-418.129 -418.129 -418.129] [86.292], Avg: [-557.414 -557.414 -557.414] (0.0100) ({r_i: None, r_t: [-810.936 -810.936 -810.936], eps: 0.01})
Step:  136600, Reward: [-409.787 -409.787 -409.787] [102.548], Avg: [-717.991 -717.991 -717.991] (0.0100) ({r_i: None, r_t: [-877.466 -877.466 -877.466], eps: 0.01})
Step:  171700, Reward: [-1175.171 -1175.171 -1175.171] [489.343], Avg: [-708.558 -708.558 -708.558] (0.1000) ({r_i: None, r_t: [-2520.922 -2520.922 -2520.922], eps: 0.1})
Step:  171800, Reward: [-1002.235 -1002.235 -1002.235] [262.016], Avg: [-708.729 -708.729 -708.729] (0.1000) ({r_i: None, r_t: [-2322.064 -2322.064 -2322.064], eps: 0.1})
Step:   47200, Reward: [-417.238 -417.238 -417.238] [93.896], Avg: [-557.118 -557.118 -557.118] (0.0100) ({r_i: None, r_t: [-840.260 -840.260 -840.260], eps: 0.01})
Step:  136700, Reward: [-432.265 -432.265 -432.265] [88.800], Avg: [-717.782 -717.782 -717.782] (0.0100) ({r_i: None, r_t: [-900.581 -900.581 -900.581], eps: 0.01})
Step:  171900, Reward: [-1137.582 -1137.582 -1137.582] [265.592], Avg: [-708.978 -708.978 -708.978] (0.1000) ({r_i: None, r_t: [-2415.176 -2415.176 -2415.176], eps: 0.1})
Step:  136800, Reward: [-421.604 -421.604 -421.604] [72.606], Avg: [-717.566 -717.566 -717.566] (0.0100) ({r_i: None, r_t: [-820.119 -820.119 -820.119], eps: 0.01})
Step:   47300, Reward: [-382.737 -382.737 -382.737] [64.163], Avg: [-556.750 -556.750 -556.750] (0.0100) ({r_i: None, r_t: [-839.342 -839.342 -839.342], eps: 0.01})
Step:  172000, Reward: [-1133.833 -1133.833 -1133.833] [323.516], Avg: [-709.225 -709.225 -709.225] (0.1000) ({r_i: None, r_t: [-2300.509 -2300.509 -2300.509], eps: 0.1})
Step:  136900, Reward: [-419.695 -419.695 -419.695] [88.641], Avg: [-717.348 -717.348 -717.348] (0.0100) ({r_i: None, r_t: [-866.718 -866.718 -866.718], eps: 0.01})
Step:   47400, Reward: [-431.387 -431.387 -431.387] [114.728], Avg: [-556.486 -556.486 -556.486] (0.0100) ({r_i: None, r_t: [-837.221 -837.221 -837.221], eps: 0.01})
Step:  172100, Reward: [-1246.803 -1246.803 -1246.803] [429.918], Avg: [-709.537 -709.537 -709.537] (0.1000) ({r_i: None, r_t: [-2476.840 -2476.840 -2476.840], eps: 0.1})
Step:  137000, Reward: [-419.760 -419.760 -419.760] [105.058], Avg: [-717.131 -717.131 -717.131] (0.0100) ({r_i: None, r_t: [-865.220 -865.220 -865.220], eps: 0.01})
Step:   47500, Reward: [-363.600 -363.600 -363.600] [57.731], Avg: [-556.080 -556.080 -556.080] (0.0100) ({r_i: None, r_t: [-776.944 -776.944 -776.944], eps: 0.01})
Step:  172200, Reward: [-1229.492 -1229.492 -1229.492] [423.457], Avg: [-709.839 -709.839 -709.839] (0.1000) ({r_i: None, r_t: [-2635.278 -2635.278 -2635.278], eps: 0.1})
Step:  137100, Reward: [-439.551 -439.551 -439.551] [61.920], Avg: [-716.929 -716.929 -716.929] (0.0100) ({r_i: None, r_t: [-900.838 -900.838 -900.838], eps: 0.01})
Step:   47600, Reward: [-413.493 -413.493 -413.493] [75.774], Avg: [-555.782 -555.782 -555.782] (0.0100) ({r_i: None, r_t: [-903.893 -903.893 -903.893], eps: 0.01})
Step:  172300, Reward: [-980.089 -980.089 -980.089] [404.347], Avg: [-709.996 -709.996 -709.996] (0.1000) ({r_i: None, r_t: [-2555.133 -2555.133 -2555.133], eps: 0.1})
Step:  137200, Reward: [-430.939 -430.939 -430.939] [98.428], Avg: [-716.721 -716.721 -716.721] (0.0100) ({r_i: None, r_t: [-897.228 -897.228 -897.228], eps: 0.01})
Step:   47700, Reward: [-394.114 -394.114 -394.114] [58.372], Avg: [-555.443 -555.443 -555.443] (0.0100) ({r_i: None, r_t: [-841.173 -841.173 -841.173], eps: 0.01})
Step:  172400, Reward: [-1050.432 -1050.432 -1050.432] [384.008], Avg: [-710.193 -710.193 -710.193] (0.1000) ({r_i: None, r_t: [-2764.624 -2764.624 -2764.624], eps: 0.1})
Step:  137300, Reward: [-395.282 -395.282 -395.282] [106.202], Avg: [-716.487 -716.487 -716.487] (0.0100) ({r_i: None, r_t: [-896.015 -896.015 -896.015], eps: 0.01})
Step:   47800, Reward: [-382.421 -382.421 -382.421] [59.196], Avg: [-555.082 -555.082 -555.082] (0.0100) ({r_i: None, r_t: [-791.860 -791.860 -791.860], eps: 0.01})
Step:  172500, Reward: [-1263.153 -1263.153 -1263.153] [258.514], Avg: [-710.513 -710.513 -710.513] (0.1000) ({r_i: None, r_t: [-2156.708 -2156.708 -2156.708], eps: 0.1})
Step:  137400, Reward: [-430.401 -430.401 -430.401] [63.976], Avg: [-716.279 -716.279 -716.279] (0.0100) ({r_i: None, r_t: [-847.376 -847.376 -847.376], eps: 0.01})
Step:   47900, Reward: [-426.036 -426.036 -426.036] [84.629], Avg: [-554.813 -554.813 -554.813] (0.0100) ({r_i: None, r_t: [-849.425 -849.425 -849.425], eps: 0.01})
Step:  172600, Reward: [-1239.044 -1239.044 -1239.044] [311.532], Avg: [-710.819 -710.819 -710.819] (0.1000) ({r_i: None, r_t: [-2462.649 -2462.649 -2462.649], eps: 0.1})
Step:  137500, Reward: [-427.473 -427.473 -427.473] [116.552], Avg: [-716.069 -716.069 -716.069] (0.0100) ({r_i: None, r_t: [-803.264 -803.264 -803.264], eps: 0.01})
Step:   48000, Reward: [-409.513 -409.513 -409.513] [90.919], Avg: [-554.511 -554.511 -554.511] (0.0100) ({r_i: None, r_t: [-827.734 -827.734 -827.734], eps: 0.01})
Step:  172700, Reward: [-1121.938 -1121.938 -1121.938] [284.245], Avg: [-711.057 -711.057 -711.057] (0.1000) ({r_i: None, r_t: [-2449.058 -2449.058 -2449.058], eps: 0.1})
Step:  137600, Reward: [-409.931 -409.931 -409.931] [100.473], Avg: [-715.846 -715.846 -715.846] (0.0100) ({r_i: None, r_t: [-885.111 -885.111 -885.111], eps: 0.01})
Step:   48100, Reward: [-405.032 -405.032 -405.032] [54.783], Avg: [-554.201 -554.201 -554.201] (0.0100) ({r_i: None, r_t: [-795.090 -795.090 -795.090], eps: 0.01})
Step:  172800, Reward: [-1159.191 -1159.191 -1159.191] [354.564], Avg: [-711.316 -711.316 -711.316] (0.1000) ({r_i: None, r_t: [-2337.885 -2337.885 -2337.885], eps: 0.1})
Step:  137700, Reward: [-438.034 -438.034 -438.034] [104.700], Avg: [-715.645 -715.645 -715.645] (0.0100) ({r_i: None, r_t: [-858.180 -858.180 -858.180], eps: 0.01})
Step:   48200, Reward: [-405.910 -405.910 -405.910] [98.548], Avg: [-553.894 -553.894 -553.894] (0.0100) ({r_i: None, r_t: [-790.072 -790.072 -790.072], eps: 0.01})
Step:  172900, Reward: [-1233.332 -1233.332 -1233.332] [402.902], Avg: [-711.618 -711.618 -711.618] (0.1000) ({r_i: None, r_t: [-2356.980 -2356.980 -2356.980], eps: 0.1})
Step:  137800, Reward: [-469.720 -469.720 -469.720] [82.785], Avg: [-715.466 -715.466 -715.466] (0.0100) ({r_i: None, r_t: [-868.175 -868.175 -868.175], eps: 0.01})
Step:   48300, Reward: [-382.056 -382.056 -382.056] [83.143], Avg: [-553.539 -553.539 -553.539] (0.0100) ({r_i: None, r_t: [-794.465 -794.465 -794.465], eps: 0.01})
Step:  173000, Reward: [-1279.451 -1279.451 -1279.451] [464.166], Avg: [-711.946 -711.946 -711.946] (0.1000) ({r_i: None, r_t: [-2307.315 -2307.315 -2307.315], eps: 0.1})
Step:  137900, Reward: [-411.280 -411.280 -411.280] [89.369], Avg: [-715.246 -715.246 -715.246] (0.0100) ({r_i: None, r_t: [-875.831 -875.831 -875.831], eps: 0.01})
Step:   48400, Reward: [-402.322 -402.322 -402.322] [80.025], Avg: [-553.227 -553.227 -553.227] (0.0100) ({r_i: None, r_t: [-797.620 -797.620 -797.620], eps: 0.01})
Step:  173100, Reward: [-1084.576 -1084.576 -1084.576] [459.069], Avg: [-712.161 -712.161 -712.161] (0.1000) ({r_i: None, r_t: [-2733.550 -2733.550 -2733.550], eps: 0.1})
Step:  138000, Reward: [-395.965 -395.965 -395.965] [83.116], Avg: [-715.015 -715.015 -715.015] (0.0100) ({r_i: None, r_t: [-858.015 -858.015 -858.015], eps: 0.01})
Step:   48500, Reward: [-388.420 -388.420 -388.420] [84.569], Avg: [-552.888 -552.888 -552.888] (0.0100) ({r_i: None, r_t: [-838.564 -838.564 -838.564], eps: 0.01})
Step:  173200, Reward: [-1111.611 -1111.611 -1111.611] [338.827], Avg: [-712.392 -712.392 -712.392] (0.1000) ({r_i: None, r_t: [-2553.970 -2553.970 -2553.970], eps: 0.1})
Step:  138100, Reward: [-417.642 -417.642 -417.642] [69.877], Avg: [-714.800 -714.800 -714.800] (0.0100) ({r_i: None, r_t: [-908.303 -908.303 -908.303], eps: 0.01})
Step:   48600, Reward: [-380.856 -380.856 -380.856] [70.075], Avg: [-552.535 -552.535 -552.535] (0.0100) ({r_i: None, r_t: [-773.814 -773.814 -773.814], eps: 0.01})
Step:  173300, Reward: [-1251.670 -1251.670 -1251.670] [367.859], Avg: [-712.703 -712.703 -712.703] (0.1000) ({r_i: None, r_t: [-2491.976 -2491.976 -2491.976], eps: 0.1})
Step:  138200, Reward: [-459.551 -459.551 -459.551] [87.919], Avg: [-714.615 -714.615 -714.615] (0.0100) ({r_i: None, r_t: [-922.190 -922.190 -922.190], eps: 0.01})
Step:   48700, Reward: [-414.339 -414.339 -414.339] [74.376], Avg: [-552.252 -552.252 -552.252] (0.0100) ({r_i: None, r_t: [-817.071 -817.071 -817.071], eps: 0.01})
Step:  173400, Reward: [-1379.663 -1379.663 -1379.663] [429.259], Avg: [-713.087 -713.087 -713.087] (0.1000) ({r_i: None, r_t: [-2453.364 -2453.364 -2453.364], eps: 0.1})
Step:  138300, Reward: [-482.461 -482.461 -482.461] [82.627], Avg: [-714.447 -714.447 -714.447] (0.0100) ({r_i: None, r_t: [-868.558 -868.558 -868.558], eps: 0.01})
Step:   48800, Reward: [-407.585 -407.585 -407.585] [72.634], Avg: [-551.956 -551.956 -551.956] (0.0100) ({r_i: None, r_t: [-777.552 -777.552 -777.552], eps: 0.01})
Step:  173500, Reward: [-1217.703 -1217.703 -1217.703] [346.968], Avg: [-713.378 -713.378 -713.378] (0.1000) ({r_i: None, r_t: [-2513.243 -2513.243 -2513.243], eps: 0.1})
Step:  138400, Reward: [-446.533 -446.533 -446.533] [112.738], Avg: [-714.254 -714.254 -714.254] (0.0100) ({r_i: None, r_t: [-842.247 -842.247 -842.247], eps: 0.01})
Step:   48900, Reward: [-408.687 -408.687 -408.687] [93.537], Avg: [-551.663 -551.663 -551.663] (0.0100) ({r_i: None, r_t: [-766.653 -766.653 -766.653], eps: 0.01})
Step:  173600, Reward: [-1224.469 -1224.469 -1224.469] [415.768], Avg: [-713.672 -713.672 -713.672] (0.1000) ({r_i: None, r_t: [-2376.316 -2376.316 -2376.316], eps: 0.1})
Step:  138500, Reward: [-435.207 -435.207 -435.207] [98.306], Avg: [-714.053 -714.053 -714.053] (0.0100) ({r_i: None, r_t: [-870.206 -870.206 -870.206], eps: 0.01})
Step:  173700, Reward: [-1107.878 -1107.878 -1107.878] [349.720], Avg: [-713.899 -713.899 -713.899] (0.1000) ({r_i: None, r_t: [-2490.962 -2490.962 -2490.962], eps: 0.1})
Step:   49000, Reward: [-418.219 -418.219 -418.219] [93.289], Avg: [-551.392 -551.392 -551.392] (0.0100) ({r_i: None, r_t: [-752.082 -752.082 -752.082], eps: 0.01})
Step:  138600, Reward: [-467.914 -467.914 -467.914] [129.108], Avg: [-713.875 -713.875 -713.875] (0.0100) ({r_i: None, r_t: [-904.314 -904.314 -904.314], eps: 0.01})
Step:  173800, Reward: [-1194.393 -1194.393 -1194.393] [396.878], Avg: [-714.175 -714.175 -714.175] (0.1000) ({r_i: None, r_t: [-2709.621 -2709.621 -2709.621], eps: 0.1})
Step:   49100, Reward: [-445.711 -445.711 -445.711] [82.850], Avg: [-551.177 -551.177 -551.177] (0.0100) ({r_i: None, r_t: [-799.090 -799.090 -799.090], eps: 0.01})
Step:  138700, Reward: [-428.622 -428.622 -428.622] [102.293], Avg: [-713.670 -713.670 -713.670] (0.0100) ({r_i: None, r_t: [-874.602 -874.602 -874.602], eps: 0.01})
Step:  173900, Reward: [-1175.008 -1175.008 -1175.008] [381.286], Avg: [-714.440 -714.440 -714.440] (0.1000) ({r_i: None, r_t: [-2420.540 -2420.540 -2420.540], eps: 0.1})
Step:   49200, Reward: [-381.257 -381.257 -381.257] [59.307], Avg: [-550.832 -550.832 -550.832] (0.0100) ({r_i: None, r_t: [-840.767 -840.767 -840.767], eps: 0.01})
Step:  138800, Reward: [-441.325 -441.325 -441.325] [88.066], Avg: [-713.474 -713.474 -713.474] (0.0100) ({r_i: None, r_t: [-831.082 -831.082 -831.082], eps: 0.01})
Step:  174000, Reward: [-1363.387 -1363.387 -1363.387] [317.470], Avg: [-714.813 -714.813 -714.813] (0.1000) ({r_i: None, r_t: [-2160.055 -2160.055 -2160.055], eps: 0.1})
Step:   49300, Reward: [-418.689 -418.689 -418.689] [37.020], Avg: [-550.565 -550.565 -550.565] (0.0100) ({r_i: None, r_t: [-788.199 -788.199 -788.199], eps: 0.01})
Step:  138900, Reward: [-463.123 -463.123 -463.123] [73.675], Avg: [-713.293 -713.293 -713.293] (0.0100) ({r_i: None, r_t: [-917.973 -917.973 -917.973], eps: 0.01})
Step:  174100, Reward: [-1235.046 -1235.046 -1235.046] [340.848], Avg: [-715.111 -715.111 -715.111] (0.1000) ({r_i: None, r_t: [-2463.662 -2463.662 -2463.662], eps: 0.1})
Step:   49400, Reward: [-450.292 -450.292 -450.292] [88.725], Avg: [-550.362 -550.362 -550.362] (0.0100) ({r_i: None, r_t: [-788.738 -788.738 -788.738], eps: 0.01})
Step:  139000, Reward: [-435.973 -435.973 -435.973] [89.428], Avg: [-713.094 -713.094 -713.094] (0.0100) ({r_i: None, r_t: [-841.142 -841.142 -841.142], eps: 0.01})
Step:  174200, Reward: [-1204.858 -1204.858 -1204.858] [357.758], Avg: [-715.392 -715.392 -715.392] (0.1000) ({r_i: None, r_t: [-2627.426 -2627.426 -2627.426], eps: 0.1})
Step:   49500, Reward: [-394.147 -394.147 -394.147] [58.843], Avg: [-550.047 -550.047 -550.047] (0.0100) ({r_i: None, r_t: [-798.180 -798.180 -798.180], eps: 0.01})
Step:  174300, Reward: [-1240.929 -1240.929 -1240.929] [349.234], Avg: [-715.694 -715.694 -715.694] (0.1000) ({r_i: None, r_t: [-2748.900 -2748.900 -2748.900], eps: 0.1})
Step:  139100, Reward: [-453.889 -453.889 -453.889] [100.230], Avg: [-712.908 -712.908 -712.908] (0.0100) ({r_i: None, r_t: [-865.445 -865.445 -865.445], eps: 0.01})
Step:   49600, Reward: [-425.572 -425.572 -425.572] [56.277], Avg: [-549.797 -549.797 -549.797] (0.0100) ({r_i: None, r_t: [-817.496 -817.496 -817.496], eps: 0.01})
Step:  174400, Reward: [-1101.564 -1101.564 -1101.564] [395.787], Avg: [-715.915 -715.915 -715.915] (0.1000) ({r_i: None, r_t: [-2625.657 -2625.657 -2625.657], eps: 0.1})
Step:  139200, Reward: [-446.176 -446.176 -446.176] [129.262], Avg: [-712.716 -712.716 -712.716] (0.0100) ({r_i: None, r_t: [-872.595 -872.595 -872.595], eps: 0.01})
Step:   49700, Reward: [-397.755 -397.755 -397.755] [90.097], Avg: [-549.491 -549.491 -549.491] (0.0100) ({r_i: None, r_t: [-791.092 -791.092 -791.092], eps: 0.01})
Step:  174500, Reward: [-1205.109 -1205.109 -1205.109] [353.722], Avg: [-716.195 -716.195 -716.195] (0.1000) ({r_i: None, r_t: [-2713.726 -2713.726 -2713.726], eps: 0.1})
Step:  139300, Reward: [-458.149 -458.149 -458.149] [111.822], Avg: [-712.534 -712.534 -712.534] (0.0100) ({r_i: None, r_t: [-842.822 -842.822 -842.822], eps: 0.01})
Step:   49800, Reward: [-386.874 -386.874 -386.874] [77.461], Avg: [-549.166 -549.166 -549.166] (0.0100) ({r_i: None, r_t: [-859.410 -859.410 -859.410], eps: 0.01})
Step:  174600, Reward: [-1133.944 -1133.944 -1133.944] [408.622], Avg: [-716.434 -716.434 -716.434] (0.1000) ({r_i: None, r_t: [-2359.336 -2359.336 -2359.336], eps: 0.1})
Step:  139400, Reward: [-443.155 -443.155 -443.155] [79.788], Avg: [-712.341 -712.341 -712.341] (0.0100) ({r_i: None, r_t: [-887.058 -887.058 -887.058], eps: 0.01})
Step:   49900, Reward: [-387.406 -387.406 -387.406] [71.832], Avg: [-548.842 -548.842 -548.842] (0.0100) ({r_i: None, r_t: [-870.411 -870.411 -870.411], eps: 0.01})
Step:  174700, Reward: [-1223.738 -1223.738 -1223.738] [354.686], Avg: [-716.724 -716.724 -716.724] (0.1000) ({r_i: None, r_t: [-2616.249 -2616.249 -2616.249], eps: 0.1})
Step:  139500, Reward: [-480.146 -480.146 -480.146] [73.427], Avg: [-712.174 -712.174 -712.174] (0.0100) ({r_i: None, r_t: [-866.148 -866.148 -866.148], eps: 0.01})
Step:   50000, Reward: [-417.190 -417.190 -417.190] [65.008], Avg: [-548.579 -548.579 -548.579] (0.0100) ({r_i: None, r_t: [-805.084 -805.084 -805.084], eps: 0.01})
Step:  174800, Reward: [-1218.485 -1218.485 -1218.485] [332.270], Avg: [-717.011 -717.011 -717.011] (0.1000) ({r_i: None, r_t: [-2523.057 -2523.057 -2523.057], eps: 0.1})
Step:  139600, Reward: [-403.162 -403.162 -403.162] [78.896], Avg: [-711.953 -711.953 -711.953] (0.0100) ({r_i: None, r_t: [-880.079 -880.079 -880.079], eps: 0.01})
Step:   50100, Reward: [-438.365 -438.365 -438.365] [79.634], Avg: [-548.360 -548.360 -548.360] (0.0100) ({r_i: None, r_t: [-823.693 -823.693 -823.693], eps: 0.01})
Step:  174900, Reward: [-1241.932 -1241.932 -1241.932] [315.230], Avg: [-717.311 -717.311 -717.311] (0.1000) ({r_i: None, r_t: [-2781.512 -2781.512 -2781.512], eps: 0.1})
Step:  139700, Reward: [-452.054 -452.054 -452.054] [106.195], Avg: [-711.767 -711.767 -711.767] (0.0100) ({r_i: None, r_t: [-858.925 -858.925 -858.925], eps: 0.01})
Step:   50200, Reward: [-399.598 -399.598 -399.598] [87.207], Avg: [-548.064 -548.064 -548.064] (0.0100) ({r_i: None, r_t: [-812.893 -812.893 -812.893], eps: 0.01})
Step:  175000, Reward: [-1161.275 -1161.275 -1161.275] [401.335], Avg: [-717.565 -717.565 -717.565] (0.1000) ({r_i: None, r_t: [-2385.841 -2385.841 -2385.841], eps: 0.1})
Step:  139800, Reward: [-437.280 -437.280 -437.280] [117.643], Avg: [-711.571 -711.571 -711.571] (0.0100) ({r_i: None, r_t: [-896.534 -896.534 -896.534], eps: 0.01})
Step:   50300, Reward: [-438.646 -438.646 -438.646] [83.072], Avg: [-547.847 -547.847 -547.847] (0.0100) ({r_i: None, r_t: [-875.639 -875.639 -875.639], eps: 0.01})
Step:  175100, Reward: [-1322.195 -1322.195 -1322.195] [386.308], Avg: [-717.910 -717.910 -717.910] (0.1000) ({r_i: None, r_t: [-2338.136 -2338.136 -2338.136], eps: 0.1})
Step:  139900, Reward: [-452.263 -452.263 -452.263] [88.202], Avg: [-711.386 -711.386 -711.386] (0.0100) ({r_i: None, r_t: [-880.531 -880.531 -880.531], eps: 0.01})
Step:   50400, Reward: [-402.969 -402.969 -402.969] [86.456], Avg: [-547.560 -547.560 -547.560] (0.0100) ({r_i: None, r_t: [-782.023 -782.023 -782.023], eps: 0.01})
Step:  175200, Reward: [-1204.910 -1204.910 -1204.910] [333.206], Avg: [-718.188 -718.188 -718.188] (0.1000) ({r_i: None, r_t: [-2509.440 -2509.440 -2509.440], eps: 0.1})
Step:  140000, Reward: [-424.845 -424.845 -424.845] [70.861], Avg: [-711.181 -711.181 -711.181] (0.0100) ({r_i: None, r_t: [-884.479 -884.479 -884.479], eps: 0.01})
Step:   50500, Reward: [-392.443 -392.443 -392.443] [60.830], Avg: [-547.253 -547.253 -547.253] (0.0100) ({r_i: None, r_t: [-775.644 -775.644 -775.644], eps: 0.01})
Step:  175300, Reward: [-1148.459 -1148.459 -1148.459] [292.788], Avg: [-718.433 -718.433 -718.433] (0.1000) ({r_i: None, r_t: [-2453.953 -2453.953 -2453.953], eps: 0.1})
Step:  140100, Reward: [-375.529 -375.529 -375.529] [53.732], Avg: [-710.942 -710.942 -710.942] (0.0100) ({r_i: None, r_t: [-932.450 -932.450 -932.450], eps: 0.01})
Step:   50600, Reward: [-407.909 -407.909 -407.909] [71.323], Avg: [-546.979 -546.979 -546.979] (0.0100) ({r_i: None, r_t: [-843.155 -843.155 -843.155], eps: 0.01})
Step:  175400, Reward: [-1312.471 -1312.471 -1312.471] [436.451], Avg: [-718.772 -718.772 -718.772] (0.1000) ({r_i: None, r_t: [-2347.907 -2347.907 -2347.907], eps: 0.1})
Step:  140200, Reward: [-408.398 -408.398 -408.398] [67.226], Avg: [-710.726 -710.726 -710.726] (0.0100) ({r_i: None, r_t: [-895.251 -895.251 -895.251], eps: 0.01})
Step:   50700, Reward: [-394.711 -394.711 -394.711] [58.175], Avg: [-546.679 -546.679 -546.679] (0.0100) ({r_i: None, r_t: [-815.189 -815.189 -815.189], eps: 0.01})
Step:  175500, Reward: [-1161.147 -1161.147 -1161.147] [435.993], Avg: [-719.023 -719.023 -719.023] (0.1000) ({r_i: None, r_t: [-2831.016 -2831.016 -2831.016], eps: 0.1})
Step:  140300, Reward: [-427.102 -427.102 -427.102] [57.800], Avg: [-710.524 -710.524 -710.524] (0.0100) ({r_i: None, r_t: [-853.507 -853.507 -853.507], eps: 0.01})
Step:   50800, Reward: [-411.649 -411.649 -411.649] [75.477], Avg: [-546.414 -546.414 -546.414] (0.0100) ({r_i: None, r_t: [-844.020 -844.020 -844.020], eps: 0.01})
Step:  175600, Reward: [-1236.251 -1236.251 -1236.251] [350.300], Avg: [-719.318 -719.318 -719.318] (0.1000) ({r_i: None, r_t: [-2507.040 -2507.040 -2507.040], eps: 0.1})
Step:  140400, Reward: [-443.662 -443.662 -443.662] [104.606], Avg: [-710.334 -710.334 -710.334] (0.0100) ({r_i: None, r_t: [-920.872 -920.872 -920.872], eps: 0.01})
Step:   50900, Reward: [-401.255 -401.255 -401.255] [70.967], Avg: [-546.129 -546.129 -546.129] (0.0100) ({r_i: None, r_t: [-831.517 -831.517 -831.517], eps: 0.01})
Step:  175700, Reward: [-1401.031 -1401.031 -1401.031] [265.538], Avg: [-719.706 -719.706 -719.706] (0.1000) ({r_i: None, r_t: [-2694.360 -2694.360 -2694.360], eps: 0.1})
Step:  140500, Reward: [-413.346 -413.346 -413.346] [109.017], Avg: [-710.123 -710.123 -710.123] (0.0100) ({r_i: None, r_t: [-854.046 -854.046 -854.046], eps: 0.01})
Step:   51000, Reward: [-443.091 -443.091 -443.091] [70.697], Avg: [-545.927 -545.927 -545.927] (0.0100) ({r_i: None, r_t: [-806.333 -806.333 -806.333], eps: 0.01})
Step:  175800, Reward: [-1316.499 -1316.499 -1316.499] [404.823], Avg: [-720.045 -720.045 -720.045] (0.1000) ({r_i: None, r_t: [-2449.327 -2449.327 -2449.327], eps: 0.1})
Step:  140600, Reward: [-412.164 -412.164 -412.164] [68.951], Avg: [-709.911 -709.911 -709.911] (0.0100) ({r_i: None, r_t: [-817.097 -817.097 -817.097], eps: 0.01})
Step:   51100, Reward: [-398.105 -398.105 -398.105] [80.620], Avg: [-545.639 -545.639 -545.639] (0.0100) ({r_i: None, r_t: [-889.430 -889.430 -889.430], eps: 0.01})
Step:  175900, Reward: [-1360.231 -1360.231 -1360.231] [387.482], Avg: [-720.409 -720.409 -720.409] (0.1000) ({r_i: None, r_t: [-2473.533 -2473.533 -2473.533], eps: 0.1})
Step:  140700, Reward: [-443.684 -443.684 -443.684] [109.026], Avg: [-709.722 -709.722 -709.722] (0.0100) ({r_i: None, r_t: [-864.530 -864.530 -864.530], eps: 0.01})
Step:   51200, Reward: [-427.144 -427.144 -427.144] [86.336], Avg: [-545.408 -545.408 -545.408] (0.0100) ({r_i: None, r_t: [-831.674 -831.674 -831.674], eps: 0.01})
Step:  176000, Reward: [-1196.521 -1196.521 -1196.521] [376.407], Avg: [-720.679 -720.679 -720.679] (0.1000) ({r_i: None, r_t: [-2471.457 -2471.457 -2471.457], eps: 0.1})
Step:  140800, Reward: [-384.455 -384.455 -384.455] [84.825], Avg: [-709.491 -709.491 -709.491] (0.0100) ({r_i: None, r_t: [-860.447 -860.447 -860.447], eps: 0.01})
Step:  176100, Reward: [-1292.267 -1292.267 -1292.267] [384.191], Avg: [-721.003 -721.003 -721.003] (0.1000) ({r_i: None, r_t: [-2622.874 -2622.874 -2622.874], eps: 0.1})
Step:   51300, Reward: [-413.283 -413.283 -413.283] [54.667], Avg: [-545.151 -545.151 -545.151] (0.0100) ({r_i: None, r_t: [-852.537 -852.537 -852.537], eps: 0.01})
Step:  140900, Reward: [-411.523 -411.523 -411.523] [108.721], Avg: [-709.280 -709.280 -709.280] (0.0100) ({r_i: None, r_t: [-911.562 -911.562 -911.562], eps: 0.01})
Step:  176200, Reward: [-1288.121 -1288.121 -1288.121] [338.387], Avg: [-721.325 -721.325 -721.325] (0.1000) ({r_i: None, r_t: [-2209.189 -2209.189 -2209.189], eps: 0.1})
Step:   51400, Reward: [-396.487 -396.487 -396.487] [75.804], Avg: [-544.862 -544.862 -544.862] (0.0100) ({r_i: None, r_t: [-801.591 -801.591 -801.591], eps: 0.01})
Step:  141000, Reward: [-425.558 -425.558 -425.558] [94.980], Avg: [-709.079 -709.079 -709.079] (0.0100) ({r_i: None, r_t: [-910.396 -910.396 -910.396], eps: 0.01})
Step:  176300, Reward: [-1287.377 -1287.377 -1287.377] [385.126], Avg: [-721.646 -721.646 -721.646] (0.1000) ({r_i: None, r_t: [-2519.927 -2519.927 -2519.927], eps: 0.1})
Step:   51500, Reward: [-407.874 -407.874 -407.874] [73.993], Avg: [-544.596 -544.596 -544.596] (0.0100) ({r_i: None, r_t: [-890.876 -890.876 -890.876], eps: 0.01})
Step:  141100, Reward: [-461.569 -461.569 -461.569] [99.941], Avg: [-708.904 -708.904 -708.904] (0.0100) ({r_i: None, r_t: [-893.177 -893.177 -893.177], eps: 0.01})
Step:  176400, Reward: [-1244.983 -1244.983 -1244.983] [392.613], Avg: [-721.942 -721.942 -721.942] (0.1000) ({r_i: None, r_t: [-2320.717 -2320.717 -2320.717], eps: 0.1})
Step:   51600, Reward: [-421.199 -421.199 -421.199] [67.587], Avg: [-544.358 -544.358 -544.358] (0.0100) ({r_i: None, r_t: [-837.961 -837.961 -837.961], eps: 0.01})
Step:  141200, Reward: [-448.744 -448.744 -448.744] [123.065], Avg: [-708.720 -708.720 -708.720] (0.0100) ({r_i: None, r_t: [-862.684 -862.684 -862.684], eps: 0.01})
Step:  176500, Reward: [-1340.058 -1340.058 -1340.058] [214.726], Avg: [-722.292 -722.292 -722.292] (0.1000) ({r_i: None, r_t: [-2504.889 -2504.889 -2504.889], eps: 0.1})
Step:   51700, Reward: [-418.483 -418.483 -418.483] [60.061], Avg: [-544.115 -544.115 -544.115] (0.0100) ({r_i: None, r_t: [-827.558 -827.558 -827.558], eps: 0.01})
Step:  141300, Reward: [-468.614 -468.614 -468.614] [115.329], Avg: [-708.550 -708.550 -708.550] (0.0100) ({r_i: None, r_t: [-923.797 -923.797 -923.797], eps: 0.01})
Step:  176600, Reward: [-1386.120 -1386.120 -1386.120] [358.812], Avg: [-722.668 -722.668 -722.668] (0.1000) ({r_i: None, r_t: [-2604.361 -2604.361 -2604.361], eps: 0.1})
Step:   51800, Reward: [-407.103 -407.103 -407.103] [65.898], Avg: [-543.851 -543.851 -543.851] (0.0100) ({r_i: None, r_t: [-859.069 -859.069 -859.069], eps: 0.01})
Step:  141400, Reward: [-471.491 -471.491 -471.491] [74.354], Avg: [-708.382 -708.382 -708.382] (0.0100) ({r_i: None, r_t: [-914.348 -914.348 -914.348], eps: 0.01})
Step:  176700, Reward: [-1187.504 -1187.504 -1187.504] [268.765], Avg: [-722.931 -722.931 -722.931] (0.1000) ({r_i: None, r_t: [-2530.622 -2530.622 -2530.622], eps: 0.1})
Step:   51900, Reward: [-445.894 -445.894 -445.894] [75.843], Avg: [-543.662 -543.662 -543.662] (0.0100) ({r_i: None, r_t: [-848.877 -848.877 -848.877], eps: 0.01})
Step:  141500, Reward: [-467.245 -467.245 -467.245] [99.592], Avg: [-708.212 -708.212 -708.212] (0.0100) ({r_i: None, r_t: [-965.150 -965.150 -965.150], eps: 0.01})
Step:  176800, Reward: [-1411.650 -1411.650 -1411.650] [169.051], Avg: [-723.320 -723.320 -723.320] (0.1000) ({r_i: None, r_t: [-2593.540 -2593.540 -2593.540], eps: 0.1})
Step:   52000, Reward: [-433.987 -433.987 -433.987] [68.921], Avg: [-543.452 -543.452 -543.452] (0.0100) ({r_i: None, r_t: [-861.010 -861.010 -861.010], eps: 0.01})
Step:  141600, Reward: [-461.855 -461.855 -461.855] [56.429], Avg: [-708.038 -708.038 -708.038] (0.0100) ({r_i: None, r_t: [-889.000 -889.000 -889.000], eps: 0.01})
Step:  176900, Reward: [-1124.324 -1124.324 -1124.324] [510.292], Avg: [-723.547 -723.547 -723.547] (0.1000) ({r_i: None, r_t: [-2536.177 -2536.177 -2536.177], eps: 0.1})
Step:   52100, Reward: [-418.883 -418.883 -418.883] [96.975], Avg: [-543.213 -543.213 -543.213] (0.0100) ({r_i: None, r_t: [-856.548 -856.548 -856.548], eps: 0.01})
Step:  141700, Reward: [-453.357 -453.357 -453.357] [82.725], Avg: [-707.858 -707.858 -707.858] (0.0100) ({r_i: None, r_t: [-956.654 -956.654 -956.654], eps: 0.01})
Step:  177000, Reward: [-1302.860 -1302.860 -1302.860] [322.613], Avg: [-723.874 -723.874 -723.874] (0.1000) ({r_i: None, r_t: [-2371.029 -2371.029 -2371.029], eps: 0.1})
Step:   52200, Reward: [-421.832 -421.832 -421.832] [110.062], Avg: [-542.981 -542.981 -542.981] (0.0100) ({r_i: None, r_t: [-793.805 -793.805 -793.805], eps: 0.01})
Step:  141800, Reward: [-491.942 -491.942 -491.942] [60.240], Avg: [-707.706 -707.706 -707.706] (0.0100) ({r_i: None, r_t: [-954.375 -954.375 -954.375], eps: 0.01})
Step:  177100, Reward: [-1167.382 -1167.382 -1167.382] [366.967], Avg: [-724.124 -724.124 -724.124] (0.1000) ({r_i: None, r_t: [-2577.342 -2577.342 -2577.342], eps: 0.1})
Step:   52300, Reward: [-407.371 -407.371 -407.371] [77.361], Avg: [-542.722 -542.722 -542.722] (0.0100) ({r_i: None, r_t: [-851.782 -851.782 -851.782], eps: 0.01})
Step:  141900, Reward: [-475.926 -475.926 -475.926] [86.542], Avg: [-707.543 -707.543 -707.543] (0.0100) ({r_i: None, r_t: [-966.107 -966.107 -966.107], eps: 0.01})
Step:  177200, Reward: [-1191.196 -1191.196 -1191.196] [419.065], Avg: [-724.388 -724.388 -724.388] (0.1000) ({r_i: None, r_t: [-2754.900 -2754.900 -2754.900], eps: 0.1})
Step:   52400, Reward: [-414.550 -414.550 -414.550] [77.598], Avg: [-542.478 -542.478 -542.478] (0.0100) ({r_i: None, r_t: [-783.029 -783.029 -783.029], eps: 0.01})
Step:  142000, Reward: [-524.145 -524.145 -524.145] [91.748], Avg: [-707.414 -707.414 -707.414] (0.0100) ({r_i: None, r_t: [-996.088 -996.088 -996.088], eps: 0.01})
Step:  177300, Reward: [-1218.119 -1218.119 -1218.119] [379.966], Avg: [-724.666 -724.666 -724.666] (0.1000) ({r_i: None, r_t: [-2691.441 -2691.441 -2691.441], eps: 0.1})
Step:   52500, Reward: [-399.804 -399.804 -399.804] [71.737], Avg: [-542.207 -542.207 -542.207] (0.0100) ({r_i: None, r_t: [-832.551 -832.551 -832.551], eps: 0.01})
Step:  142100, Reward: [-464.711 -464.711 -464.711] [89.367], Avg: [-707.243 -707.243 -707.243] (0.0100) ({r_i: None, r_t: [-979.369 -979.369 -979.369], eps: 0.01})
Step:  177400, Reward: [-1346.343 -1346.343 -1346.343] [333.375], Avg: [-725.016 -725.016 -725.016] (0.1000) ({r_i: None, r_t: [-2411.419 -2411.419 -2411.419], eps: 0.1})
Step:   52600, Reward: [-377.802 -377.802 -377.802] [54.158], Avg: [-541.895 -541.895 -541.895] (0.0100) ({r_i: None, r_t: [-769.267 -769.267 -769.267], eps: 0.01})
Step:  142200, Reward: [-509.574 -509.574 -509.574] [120.787], Avg: [-707.104 -707.104 -707.104] (0.0100) ({r_i: None, r_t: [-1023.401 -1023.401 -1023.401], eps: 0.01})
Step:  177500, Reward: [-1219.857 -1219.857 -1219.857] [380.085], Avg: [-725.295 -725.295 -725.295] (0.1000) ({r_i: None, r_t: [-2352.194 -2352.194 -2352.194], eps: 0.1})
Step:   52700, Reward: [-377.844 -377.844 -377.844] [42.496], Avg: [-541.584 -541.584 -541.584] (0.0100) ({r_i: None, r_t: [-777.604 -777.604 -777.604], eps: 0.01})
Step:  177600, Reward: [-1266.806 -1266.806 -1266.806] [356.594], Avg: [-725.600 -725.600 -725.600] (0.1000) ({r_i: None, r_t: [-2341.291 -2341.291 -2341.291], eps: 0.1})
Step:  142300, Reward: [-552.896 -552.896 -552.896] [80.218], Avg: [-706.996 -706.996 -706.996] (0.0100) ({r_i: None, r_t: [-1076.146 -1076.146 -1076.146], eps: 0.01})
Step:   52800, Reward: [-397.118 -397.118 -397.118] [67.528], Avg: [-541.311 -541.311 -541.311] (0.0100) ({r_i: None, r_t: [-799.581 -799.581 -799.581], eps: 0.01})
Step:  177700, Reward: [-1399.759 -1399.759 -1399.759] [305.028], Avg: [-725.979 -725.979 -725.979] (0.1000) ({r_i: None, r_t: [-2509.614 -2509.614 -2509.614], eps: 0.1})
Step:  142400, Reward: [-503.339 -503.339 -503.339] [95.484], Avg: [-706.853 -706.853 -706.853] (0.0100) ({r_i: None, r_t: [-1088.231 -1088.231 -1088.231], eps: 0.01})
Step:   52900, Reward: [-415.682 -415.682 -415.682] [76.590], Avg: [-541.074 -541.074 -541.074] (0.0100) ({r_i: None, r_t: [-801.602 -801.602 -801.602], eps: 0.01})
Step:  177800, Reward: [-1175.537 -1175.537 -1175.537] [423.944], Avg: [-726.232 -726.232 -726.232] (0.1000) ({r_i: None, r_t: [-2390.710 -2390.710 -2390.710], eps: 0.1})
Step:  142500, Reward: [-497.523 -497.523 -497.523] [93.760], Avg: [-706.706 -706.706 -706.706] (0.0100) ({r_i: None, r_t: [-1101.162 -1101.162 -1101.162], eps: 0.01})
Step:   53000, Reward: [-395.180 -395.180 -395.180] [78.603], Avg: [-540.799 -540.799 -540.799] (0.0100) ({r_i: None, r_t: [-858.358 -858.358 -858.358], eps: 0.01})
Step:  177900, Reward: [-1253.629 -1253.629 -1253.629] [336.169], Avg: [-726.528 -726.528 -726.528] (0.1000) ({r_i: None, r_t: [-2640.607 -2640.607 -2640.607], eps: 0.1})
Step:  142600, Reward: [-519.578 -519.578 -519.578] [130.282], Avg: [-706.575 -706.575 -706.575] (0.0100) ({r_i: None, r_t: [-1084.780 -1084.780 -1084.780], eps: 0.01})
Step:   53100, Reward: [-393.251 -393.251 -393.251] [62.544], Avg: [-540.522 -540.522 -540.522] (0.0100) ({r_i: None, r_t: [-809.750 -809.750 -809.750], eps: 0.01})
