Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		self.episode_batch.update({"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy(), actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			self.episode_batch.update({"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
		self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

	def save_models(self, path):
		self.mac.save_models(path)
		torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
		torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
		torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

	def load_models(self, path):
		self.mac.load_models(path)
		self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
		self.target_critic.load_state_dict(self.critic.state_dict())
		self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
		self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def load_state(self, other_mac):
		self.agent.load_state_dict(other_mac.agent.state_dict())

	def cuda(self):
		self.agent.cuda()

	def save_models(self, path):
		torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

	def load_models(self, path):
		self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		h = self.rnn(x, h_in)
		q = self.fc2(h)
		return q, h

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.000               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(lambda: make_env(env_name), ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=True) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.0, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-534.709 -534.709 -534.709] [74.9638], Avg: [-609.672 -609.672 -609.672] (1.000)
Step: 99, Reward: [-461.552 -461.552 -461.552] [62.6257], Avg: [-566.925 -566.925 -566.925] (1.000)
Step: 149, Reward: [-482.006 -482.006 -482.006] [68.2363], Avg: [-561.364 -561.364 -561.364] (1.000)
Step: 199, Reward: [-520.545 -520.545 -520.545] [35.7545], Avg: [-560.098 -560.098 -560.098] (1.000)
Step: 249, Reward: [-455.258 -455.258 -455.258] [99.9252], Avg: [-559.115 -559.115 -559.115] (1.000)
Step: 299, Reward: [-437.283 -437.283 -437.283] [72.1241], Avg: [-550.83 -550.83 -550.83] (1.000)
Step: 349, Reward: [-558.451 -558.451 -558.451] [83.7917], Avg: [-563.889 -563.889 -563.889] (1.000)
Step: 399, Reward: [-458.777 -458.777 -458.777] [57.8094], Avg: [-557.976 -557.976 -557.976] (1.000)
Step: 449, Reward: [-522.486 -522.486 -522.486] [86.5874], Avg: [-563.654 -563.654 -563.654] (1.000)
Step: 499, Reward: [-494.818 -494.818 -494.818] [93.7429], Avg: [-566.145 -566.145 -566.145] (1.000)
Step: 549, Reward: [-476.869 -476.869 -476.869] [52.3628], Avg: [-562.789 -562.789 -562.789] (1.000)
Step: 599, Reward: [-483.793 -483.793 -483.793] [40.4360], Avg: [-559.576 -559.576 -559.576] (1.000)
Step: 649, Reward: [-433.368 -433.368 -433.368] [70.9978], Avg: [-555.329 -555.329 -555.329] (1.000)
Step: 699, Reward: [-482.152 -482.152 -482.152] [88.0864], Avg: [-556.394 -556.394 -556.394] (1.000)
Step: 749, Reward: [-529.375 -529.375 -529.375] [153.2706], Avg: [-564.81 -564.81 -564.81] (1.000)
Step: 799, Reward: [-464.632 -464.632 -464.632] [92.3868], Avg: [-564.323 -564.323 -564.323] (1.000)
Step: 849, Reward: [-569.737 -569.737 -569.737] [94.6254], Avg: [-570.208 -570.208 -570.208] (1.000)
Step: 899, Reward: [-613.692 -613.692 -613.692] [192.4171], Avg: [-583.314 -583.314 -583.314] (1.000)
Step: 949, Reward: [-503.393 -503.393 -503.393] [90.3690], Avg: [-583.864 -583.864 -583.864] (1.000)
Step: 999, Reward: [-483.4 -483.4 -483.4] [39.1650], Avg: [-580.799 -580.799 -580.799] (1.000)
Step: 1049, Reward: [-622.001 -622.001 -622.001] [142.6842], Avg: [-589.555 -589.555 -589.555] (1.000)
Step: 1099, Reward: [-553.73 -553.73 -553.73] [176.0484], Avg: [-595.929 -595.929 -595.929] (1.000)
Step: 1149, Reward: [-576.593 -576.593 -576.593] [123.5314], Avg: [-600.459 -600.459 -600.459] (1.000)
Step: 1199, Reward: [-502.963 -502.963 -502.963] [107.9823], Avg: [-600.896 -600.896 -600.896] (1.000)
Step: 1249, Reward: [-527.493 -527.493 -527.493] [70.1006], Avg: [-600.764 -600.764 -600.764] (1.000)
Step: 1299, Reward: [-581.757 -581.757 -581.757] [190.6555], Avg: [-607.366 -607.366 -607.366] (1.000)
Step: 1349, Reward: [-648.89 -648.89 -648.89] [93.7342], Avg: [-612.375 -612.375 -612.375] (1.000)
Step: 1399, Reward: [-492.368 -492.368 -492.368] [93.3105], Avg: [-611.422 -611.422 -611.422] (1.000)
Step: 1449, Reward: [-463.484 -463.484 -463.484] [83.2362], Avg: [-609.191 -609.191 -609.191] (1.000)
Step: 1499, Reward: [-436.602 -436.602 -436.602] [61.4583], Avg: [-605.487 -605.487 -605.487] (1.000)
Step: 1549, Reward: [-545.157 -545.157 -545.157] [94.6025], Avg: [-606.592 -606.592 -606.592] (1.000)
Step: 1599, Reward: [-457.32 -457.32 -457.32] [34.1742], Avg: [-602.995 -602.995 -602.995] (1.000)
Step: 1649, Reward: [-544.609 -544.609 -544.609] [66.3467], Avg: [-603.237 -603.237 -603.237] (1.000)
Step: 1699, Reward: [-591.398 -591.398 -591.398] [108.4478], Avg: [-606.078 -606.078 -606.078] (1.000)
Step: 1749, Reward: [-478.4 -478.4 -478.4] [52.1106], Avg: [-603.919 -603.919 -603.919] (1.000)
Step: 1799, Reward: [-565.506 -565.506 -565.506] [232.9477], Avg: [-609.323 -609.323 -609.323] (1.000)
Step: 1849, Reward: [-540.053 -540.053 -540.053] [67.1925], Avg: [-609.267 -609.267 -609.267] (1.000)
Step: 1899, Reward: [-524.31 -524.31 -524.31] [119.7050], Avg: [-610.181 -610.181 -610.181] (1.000)
Step: 1949, Reward: [-514.39 -514.39 -514.39] [83.5176], Avg: [-609.866 -609.866 -609.866] (1.000)
Step: 1999, Reward: [-580.022 -580.022 -580.022] [133.4731], Avg: [-612.457 -612.457 -612.457] (1.000)
Step: 2049, Reward: [-458.868 -458.868 -458.868] [70.1543], Avg: [-610.422 -610.422 -610.422] (1.000)
Step: 2099, Reward: [-494.916 -494.916 -494.916] [68.0850], Avg: [-609.293 -609.293 -609.293] (1.000)
Step: 2149, Reward: [-441.42 -441.42 -441.42] [76.4891], Avg: [-607.168 -607.168 -607.168] (1.000)
Step: 2199, Reward: [-555.95 -555.95 -555.95] [191.1294], Avg: [-610.348 -610.348 -610.348] (1.000)
Step: 2249, Reward: [-501.883 -501.883 -501.883] [95.1567], Avg: [-610.052 -610.052 -610.052] (1.000)
Step: 2299, Reward: [-501.283 -501.283 -501.283] [89.9048], Avg: [-609.642 -609.642 -609.642] (1.000)
Step: 2349, Reward: [-464.148 -464.148 -464.148] [74.1689], Avg: [-608.124 -608.124 -608.124] (1.000)
Step: 2399, Reward: [-583.382 -583.382 -583.382] [166.4283], Avg: [-611.076 -611.076 -611.076] (1.000)
Step: 2449, Reward: [-482.066 -482.066 -482.066] [66.7421], Avg: [-609.805 -609.805 -609.805] (1.000)
Step: 2499, Reward: [-510.117 -510.117 -510.117] [82.7607], Avg: [-609.467 -609.467 -609.467] (1.000)
Step: 2549, Reward: [-430.551 -430.551 -430.551] [60.1458], Avg: [-607.138 -607.138 -607.138] (1.000)
Step: 2599, Reward: [-555.379 -555.379 -555.379] [68.7240], Avg: [-607.464 -607.464 -607.464] (1.000)
Step: 2649, Reward: [-498.271 -498.271 -498.271] [117.6729], Avg: [-607.624 -607.624 -607.624] (1.000)
Step: 2699, Reward: [-633.316 -633.316 -633.316] [156.7875], Avg: [-611.003 -611.003 -611.003] (1.000)
Step: 2749, Reward: [-486.155 -486.155 -486.155] [86.7677], Avg: [-610.311 -610.311 -610.311] (1.000)
Step: 2799, Reward: [-522.658 -522.658 -522.658] [141.7197], Avg: [-611.276 -611.276 -611.276] (1.000)
Step: 2849, Reward: [-565.151 -565.151 -565.151] [67.6198], Avg: [-611.654 -611.654 -611.654] (1.000)
Step: 2899, Reward: [-531.01 -531.01 -531.01] [103.3322], Avg: [-612.045 -612.045 -612.045] (1.000)
Step: 2949, Reward: [-611.559 -611.559 -611.559] [152.7937], Avg: [-614.626 -614.626 -614.626] (1.000)
Step: 2999, Reward: [-528.219 -528.219 -528.219] [105.7576], Avg: [-614.949 -614.949 -614.949] (1.000)
Step: 3049, Reward: [-536.035 -536.035 -536.035] [173.2513], Avg: [-616.495 -616.495 -616.495] (1.000)
Step: 3099, Reward: [-552.464 -552.464 -552.464] [43.1800], Avg: [-616.159 -616.159 -616.159] (1.000)
Step: 3149, Reward: [-471.159 -471.159 -471.159] [104.2212], Avg: [-615.512 -615.512 -615.512] (1.000)
Step: 3199, Reward: [-477.187 -477.187 -477.187] [69.9949], Avg: [-614.444 -614.444 -614.444] (1.000)
Step: 3249, Reward: [-478.941 -478.941 -478.941] [94.2654], Avg: [-613.81 -613.81 -613.81] (1.000)
Step: 3299, Reward: [-462.948 -462.948 -462.948] [57.1117], Avg: [-612.389 -612.389 -612.389] (1.000)
Step: 3349, Reward: [-470.848 -470.848 -470.848] [69.6463], Avg: [-611.316 -611.316 -611.316] (1.000)
Step: 3399, Reward: [-497.476 -497.476 -497.476] [190.9859], Avg: [-612.451 -612.451 -612.451] (1.000)
Step: 3449, Reward: [-504.553 -504.553 -504.553] [63.7662], Avg: [-611.811 -611.811 -611.811] (1.000)
Step: 3499, Reward: [-469.797 -469.797 -469.797] [36.1126], Avg: [-610.298 -610.298 -610.298] (1.000)
Step: 3549, Reward: [-422.678 -422.678 -422.678] [59.9163], Avg: [-608.499 -608.499 -608.499] (1.000)
Step: 3599, Reward: [-477.804 -477.804 -477.804] [69.6297], Avg: [-607.651 -607.651 -607.651] (1.000)
Step: 3649, Reward: [-523.643 -523.643 -523.643] [63.1603], Avg: [-607.366 -607.366 -607.366] (1.000)
Step: 3699, Reward: [-429.355 -429.355 -429.355] [45.4053], Avg: [-605.574 -605.574 -605.574] (1.000)
Step: 3749, Reward: [-531.462 -531.462 -531.462] [128.4876], Avg: [-606.299 -606.299 -606.299] (1.000)
Step: 3799, Reward: [-490.947 -490.947 -490.947] [45.7191], Avg: [-605.383 -605.383 -605.383] (1.000)
Step: 3849, Reward: [-490.736 -490.736 -490.736] [78.6433], Avg: [-604.915 -604.915 -604.915] (1.000)
Step: 3899, Reward: [-433.168 -433.168 -433.168] [73.2244], Avg: [-603.652 -603.652 -603.652] (1.000)
Step: 3949, Reward: [-458.859 -458.859 -458.859] [75.7813], Avg: [-602.778 -602.778 -602.778] (1.000)
Step: 3999, Reward: [-480.164 -480.164 -480.164] [69.2737], Avg: [-602.112 -602.112 -602.112] (1.000)
Step: 4049, Reward: [-497.598 -497.598 -497.598] [81.5793], Avg: [-601.828 -601.828 -601.828] (1.000)
Step: 4099, Reward: [-526.459 -526.459 -526.459] [96.9375], Avg: [-602.091 -602.091 -602.091] (1.000)
Step: 4149, Reward: [-522.294 -522.294 -522.294] [53.3224], Avg: [-601.772 -601.772 -601.772] (1.000)
Step: 4199, Reward: [-523.194 -523.194 -523.194] [69.8159], Avg: [-601.668 -601.668 -601.668] (1.000)
Step: 4249, Reward: [-499.802 -499.802 -499.802] [122.6511], Avg: [-601.913 -601.913 -601.913] (1.000)
Step: 4299, Reward: [-591.649 -591.649 -591.649] [91.1043], Avg: [-602.853 -602.853 -602.853] (1.000)
Step: 4349, Reward: [-479.253 -479.253 -479.253] [86.4085], Avg: [-602.425 -602.425 -602.425] (1.000)
Step: 4399, Reward: [-524.991 -524.991 -524.991] [63.4734], Avg: [-602.267 -602.267 -602.267] (1.000)
Step: 4449, Reward: [-480.866 -480.866 -480.866] [69.5070], Avg: [-601.684 -601.684 -601.684] (1.000)
Step: 4499, Reward: [-491.492 -491.492 -491.492] [93.9745], Avg: [-601.503 -601.503 -601.503] (1.000)
Step: 4549, Reward: [-597.932 -597.932 -597.932] [133.8713], Avg: [-602.935 -602.935 -602.935] (1.000)
Step: 4599, Reward: [-431.331 -431.331 -431.331] [67.0583], Avg: [-601.799 -601.799 -601.799] (1.000)
Step: 4649, Reward: [-472.182 -472.182 -472.182] [44.1927], Avg: [-600.88 -600.88 -600.88] (1.000)
Step: 4699, Reward: [-463.549 -463.549 -463.549] [57.0237], Avg: [-600.026 -600.026 -600.026] (1.000)
Step: 4749, Reward: [-465.448 -465.448 -465.448] [36.9727], Avg: [-598.999 -598.999 -598.999] (1.000)
Step: 4799, Reward: [-511.013 -511.013 -511.013] [160.9404], Avg: [-599.758 -599.758 -599.758] (1.000)
Step: 4849, Reward: [-476.091 -476.091 -476.091] [56.5319], Avg: [-599.066 -599.066 -599.066] (1.000)
Step: 4899, Reward: [-410.303 -410.303 -410.303] [40.9370], Avg: [-597.558 -597.558 -597.558] (1.000)
Step: 4949, Reward: [-399.35 -399.35 -399.35] [59.4632], Avg: [-596.156 -596.156 -596.156] (1.000)
Step: 4999, Reward: [-534.812 -534.812 -534.812] [81.4639], Avg: [-596.358 -596.358 -596.358] (1.000)
Step: 5049, Reward: [-489.696 -489.696 -489.696] [87.0593], Avg: [-596.164 -596.164 -596.164] (1.000)
Step: 5099, Reward: [-473.929 -473.929 -473.929] [80.4214], Avg: [-595.754 -595.754 -595.754] (1.000)
Step: 5149, Reward: [-468.594 -468.594 -468.594] [47.3339], Avg: [-594.979 -594.979 -594.979] (1.000)
Step: 5199, Reward: [-607.756 -607.756 -607.756] [220.0807], Avg: [-597.218 -597.218 -597.218] (1.000)
Step: 5249, Reward: [-533.893 -533.893 -533.893] [80.9464], Avg: [-597.385 -597.385 -597.385] (1.000)
Step: 5299, Reward: [-585.755 -585.755 -585.755] [109.7745], Avg: [-598.311 -598.311 -598.311] (1.000)
Step: 5349, Reward: [-553.592 -553.592 -553.592] [168.8063], Avg: [-599.471 -599.471 -599.471] (1.000)
Step: 5399, Reward: [-562.983 -562.983 -562.983] [143.4075], Avg: [-600.461 -600.461 -600.461] (1.000)
Step: 5449, Reward: [-513.723 -513.723 -513.723] [66.7747], Avg: [-600.278 -600.278 -600.278] (1.000)
Step: 5499, Reward: [-467.935 -467.935 -467.935] [61.4808], Avg: [-599.634 -599.634 -599.634] (1.000)
Step: 5549, Reward: [-587.405 -587.405 -587.405] [111.2498], Avg: [-600.526 -600.526 -600.526] (1.000)
Step: 5599, Reward: [-603.451 -603.451 -603.451] [130.9097], Avg: [-601.721 -601.721 -601.721] (1.000)
Step: 5649, Reward: [-518.672 -518.672 -518.672] [77.9693], Avg: [-601.676 -601.676 -601.676] (1.000)
Step: 5699, Reward: [-551.094 -551.094 -551.094] [134.2908], Avg: [-602.41 -602.41 -602.41] (1.000)
Step: 5749, Reward: [-484.79 -484.79 -484.79] [55.1082], Avg: [-601.866 -601.866 -601.866] (1.000)
Step: 5799, Reward: [-657.144 -657.144 -657.144] [120.7491], Avg: [-603.384 -603.384 -603.384] (1.000)
Step: 5849, Reward: [-636.54 -636.54 -636.54] [130.2961], Avg: [-604.781 -604.781 -604.781] (1.000)
Step: 5899, Reward: [-722.348 -722.348 -722.348] [135.5723], Avg: [-606.926 -606.926 -606.926] (1.000)
Step: 5949, Reward: [-657.834 -657.834 -657.834] [126.7614], Avg: [-608.419 -608.419 -608.419] (1.000)
Step: 5999, Reward: [-533.056 -533.056 -533.056] [58.8229], Avg: [-608.281 -608.281 -608.281] (1.000)
Step: 6049, Reward: [-470.741 -470.741 -470.741] [30.8455], Avg: [-607.4 -607.4 -607.4] (1.000)
Step: 6099, Reward: [-520.737 -520.737 -520.737] [56.1885], Avg: [-607.15 -607.15 -607.15] (1.000)
Step: 6149, Reward: [-500.149 -500.149 -500.149] [54.4706], Avg: [-606.723 -606.723 -606.723] (1.000)
Step: 6199, Reward: [-529.8 -529.8 -529.8] [72.6454], Avg: [-606.688 -606.688 -606.688] (1.000)
Step: 6249, Reward: [-579.206 -579.206 -579.206] [103.3838], Avg: [-607.295 -607.295 -607.295] (1.000)
Step: 6299, Reward: [-550.296 -550.296 -550.296] [134.2012], Avg: [-607.908 -607.908 -607.908] (1.000)
Step: 6349, Reward: [-554.124 -554.124 -554.124] [86.7818], Avg: [-608.168 -608.168 -608.168] (1.000)
Step: 6399, Reward: [-523.529 -523.529 -523.529] [136.3644], Avg: [-608.572 -608.572 -608.572] (1.000)
Step: 6449, Reward: [-543.417 -543.417 -543.417] [38.6619], Avg: [-608.367 -608.367 -608.367] (1.000)
Step: 6499, Reward: [-508.678 -508.678 -508.678] [74.6253], Avg: [-608.174 -608.174 -608.174] (1.000)
Step: 6549, Reward: [-523.995 -523.995 -523.995] [24.7427], Avg: [-607.72 -607.72 -607.72] (1.000)
Step: 6599, Reward: [-474.152 -474.152 -474.152] [76.7005], Avg: [-607.289 -607.289 -607.289] (1.000)
Step: 6649, Reward: [-557.772 -557.772 -557.772] [92.4421], Avg: [-607.612 -607.612 -607.612] (1.000)
Step: 6699, Reward: [-502.893 -502.893 -502.893] [79.1853], Avg: [-607.422 -607.422 -607.422] (1.000)
Step: 6749, Reward: [-480.207 -480.207 -480.207] [77.2127], Avg: [-607.051 -607.051 -607.051] (1.000)
Step: 6799, Reward: [-508.746 -508.746 -508.746] [91.8126], Avg: [-607.004 -607.004 -607.004] (1.000)
Step: 6849, Reward: [-455.954 -455.954 -455.954] [92.0372], Avg: [-606.573 -606.573 -606.573] (1.000)
Step: 6899, Reward: [-552.68 -552.68 -552.68] [118.1637], Avg: [-607.038 -607.038 -607.038] (1.000)
Step: 6949, Reward: [-488.473 -488.473 -488.473] [62.8045], Avg: [-606.637 -606.637 -606.637] (1.000)
Step: 6999, Reward: [-465.482 -465.482 -465.482] [29.7816], Avg: [-605.842 -605.842 -605.842] (1.000)
Step: 7049, Reward: [-462.409 -462.409 -462.409] [79.7428], Avg: [-605.39 -605.39 -605.39] (1.000)
Step: 7099, Reward: [-557.164 -557.164 -557.164] [52.2107], Avg: [-605.418 -605.418 -605.418] (1.000)
Step: 7149, Reward: [-434.275 -434.275 -434.275] [56.5703], Avg: [-604.617 -604.617 -604.617] (1.000)
Step: 7199, Reward: [-436.514 -436.514 -436.514] [93.2494], Avg: [-604.097 -604.097 -604.097] (1.000)
Step: 7249, Reward: [-526.138 -526.138 -526.138] [104.6344], Avg: [-604.281 -604.281 -604.281] (1.000)
Step: 7299, Reward: [-501.222 -501.222 -501.222] [76.2519], Avg: [-604.097 -604.097 -604.097] (1.000)
Step: 7349, Reward: [-503.657 -503.657 -503.657] [79.8052], Avg: [-603.957 -603.957 -603.957] (1.000)
Step: 7399, Reward: [-458.791 -458.791 -458.791] [67.5547], Avg: [-603.433 -603.433 -603.433] (1.000)
Step: 7449, Reward: [-537.082 -537.082 -537.082] [97.9769], Avg: [-603.645 -603.645 -603.645] (1.000)
Step: 7499, Reward: [-515.134 -515.134 -515.134] [65.2117], Avg: [-603.49 -603.49 -603.49] (1.000)
Step: 7549, Reward: [-564.095 -564.095 -564.095] [159.4226], Avg: [-604.285 -604.285 -604.285] (1.000)
Step: 7599, Reward: [-457.934 -457.934 -457.934] [117.4406], Avg: [-604.094 -604.094 -604.094] (1.000)
Step: 7649, Reward: [-524.635 -524.635 -524.635] [80.5341], Avg: [-604.101 -604.101 -604.101] (1.000)
Step: 7699, Reward: [-576.014 -576.014 -576.014] [120.7973], Avg: [-604.703 -604.703 -604.703] (1.000)
Step: 7749, Reward: [-534.416 -534.416 -534.416] [81.7826], Avg: [-604.778 -604.778 -604.778] (1.000)
Step: 7799, Reward: [-599.416 -599.416 -599.416] [116.3747], Avg: [-605.489 -605.489 -605.489] (1.000)
Step: 7849, Reward: [-502.293 -502.293 -502.293] [76.5003], Avg: [-605.319 -605.319 -605.319] (1.000)
Step: 7899, Reward: [-571.972 -571.972 -571.972] [132.1035], Avg: [-605.944 -605.944 -605.944] (1.000)
Step: 7949, Reward: [-531.354 -531.354 -531.354] [111.9151], Avg: [-606.179 -606.179 -606.179] (1.000)
Step: 7999, Reward: [-508.137 -508.137 -508.137] [89.6201], Avg: [-606.126 -606.126 -606.126] (1.000)
Step: 8049, Reward: [-508.365 -508.365 -508.365] [152.8407], Avg: [-606.468 -606.468 -606.468] (1.000)
Step: 8099, Reward: [-442.433 -442.433 -442.433] [55.6067], Avg: [-605.799 -605.799 -605.799] (1.000)
Step: 8149, Reward: [-524.541 -524.541 -524.541] [84.4547], Avg: [-605.819 -605.819 -605.819] (1.000)
Step: 8199, Reward: [-501.282 -501.282 -501.282] [32.8105], Avg: [-605.381 -605.381 -605.381] (1.000)
Step: 8249, Reward: [-478.678 -478.678 -478.678] [93.8138], Avg: [-605.182 -605.182 -605.182] (1.000)
Step: 8299, Reward: [-470.256 -470.256 -470.256] [37.5178], Avg: [-604.595 -604.595 -604.595] (1.000)
Step: 8349, Reward: [-515.178 -515.178 -515.178] [81.7888], Avg: [-604.55 -604.55 -604.55] (1.000)
Step: 8399, Reward: [-582.413 -582.413 -582.413] [100.8121], Avg: [-605.018 -605.018 -605.018] (1.000)
Step: 8449, Reward: [-469.298 -469.298 -469.298] [74.9758], Avg: [-604.658 -604.658 -604.658] (1.000)
Step: 8499, Reward: [-521.561 -521.561 -521.561] [97.6540], Avg: [-604.744 -604.744 -604.744] (1.000)
Step: 8549, Reward: [-448.926 -448.926 -448.926] [18.9875], Avg: [-603.944 -603.944 -603.944] (1.000)
Step: 8599, Reward: [-463.138 -463.138 -463.138] [44.9265], Avg: [-603.386 -603.386 -603.386] (1.000)
Step: 8649, Reward: [-531.98 -531.98 -531.98] [62.2331], Avg: [-603.333 -603.333 -603.333] (1.000)
Step: 8699, Reward: [-445.83 -445.83 -445.83] [43.4306], Avg: [-602.678 -602.678 -602.678] (1.000)
Step: 8749, Reward: [-465.525 -465.525 -465.525] [41.6272], Avg: [-602.132 -602.132 -602.132] (1.000)
Step: 8799, Reward: [-427.413 -427.413 -427.413] [97.8315], Avg: [-601.695 -601.695 -601.695] (1.000)
Step: 8849, Reward: [-470.905 -470.905 -470.905] [59.4562], Avg: [-601.292 -601.292 -601.292] (1.000)
Step: 8899, Reward: [-510.513 -510.513 -510.513] [127.5806], Avg: [-601.499 -601.499 -601.499] (1.000)
Step: 8949, Reward: [-486.491 -486.491 -486.491] [75.6472], Avg: [-601.279 -601.279 -601.279] (1.000)
Step: 8999, Reward: [-636.114 -636.114 -636.114] [111.9530], Avg: [-602.094 -602.094 -602.094] (1.000)
Step: 9049, Reward: [-525.306 -525.306 -525.306] [73.9569], Avg: [-602.079 -602.079 -602.079] (1.000)
Step: 9099, Reward: [-486.317 -486.317 -486.317] [110.0296], Avg: [-602.047 -602.047 -602.047] (1.000)
Step: 9149, Reward: [-436.374 -436.374 -436.374] [66.8232], Avg: [-601.507 -601.507 -601.507] (1.000)
Step: 9199, Reward: [-529.997 -529.997 -529.997] [58.9135], Avg: [-601.439 -601.439 -601.439] (1.000)
Step: 9249, Reward: [-543.123 -543.123 -543.123] [113.4926], Avg: [-601.737 -601.737 -601.737] (1.000)
Step: 9299, Reward: [-456.386 -456.386 -456.386] [49.2857], Avg: [-601.22 -601.22 -601.22] (1.000)
Step: 9349, Reward: [-482.774 -482.774 -482.774] [85.2143], Avg: [-601.043 -601.043 -601.043] (1.000)
Step: 9399, Reward: [-469.311 -469.311 -469.311] [102.7601], Avg: [-600.889 -600.889 -600.889] (1.000)
Step: 9449, Reward: [-467.033 -467.033 -467.033] [15.5380], Avg: [-600.263 -600.263 -600.263] (1.000)
Step: 9499, Reward: [-515.742 -515.742 -515.742] [69.2721], Avg: [-600.182 -600.182 -600.182] (1.000)
Step: 9549, Reward: [-459.652 -459.652 -459.652] [41.2845], Avg: [-599.663 -599.663 -599.663] (1.000)
Step: 9599, Reward: [-414.857 -414.857 -414.857] [29.5652], Avg: [-598.854 -598.854 -598.854] (1.000)
Step: 9649, Reward: [-445.728 -445.728 -445.728] [66.5501], Avg: [-598.406 -598.406 -598.406] (1.000)
Step: 9699, Reward: [-510.018 -510.018 -510.018] [93.5739], Avg: [-598.432 -598.432 -598.432] (1.000)
Step: 9749, Reward: [-492.493 -492.493 -492.493] [59.7888], Avg: [-598.196 -598.196 -598.196] (1.000)
Step: 9799, Reward: [-522.638 -522.638 -522.638] [53.8589], Avg: [-598.085 -598.085 -598.085] (1.000)
Step: 9849, Reward: [-539.389 -539.389 -539.389] [23.1483], Avg: [-597.904 -597.904 -597.904] (1.000)
Step: 9899, Reward: [-534.388 -534.388 -534.388] [30.7997], Avg: [-597.739 -597.739 -597.739] (1.000)
Step: 9949, Reward: [-635.2 -635.2 -635.2] [79.3270], Avg: [-598.326 -598.326 -598.326] (1.000)
Step: 9999, Reward: [-520.954 -520.954 -520.954] [143.9370], Avg: [-598.659 -598.659 -598.659] (1.000)
Step: 10049, Reward: [-530.024 -530.024 -530.024] [61.4486], Avg: [-598.623 -598.623 -598.623] (1.000)
Step: 10099, Reward: [-632.699 -632.699 -632.699] [118.9677], Avg: [-599.381 -599.381 -599.381] (1.000)
Step: 10149, Reward: [-581.458 -581.458 -581.458] [68.7193], Avg: [-599.631 -599.631 -599.631] (1.000)
Step: 10199, Reward: [-490.602 -490.602 -490.602] [68.5945], Avg: [-599.433 -599.433 -599.433] (1.000)
Step: 10249, Reward: [-639.376 -639.376 -639.376] [126.2536], Avg: [-600.244 -600.244 -600.244] (1.000)
Step: 10299, Reward: [-573.414 -573.414 -573.414] [109.2311], Avg: [-600.644 -600.644 -600.644] (1.000)
Step: 10349, Reward: [-520.893 -520.893 -520.893] [93.4285], Avg: [-600.71 -600.71 -600.71] (1.000)
Step: 10399, Reward: [-642.615 -642.615 -642.615] [100.3757], Avg: [-601.394 -601.394 -601.394] (1.000)
Step: 10449, Reward: [-496.991 -496.991 -496.991] [98.8002], Avg: [-601.367 -601.367 -601.367] (1.000)
Step: 10499, Reward: [-485.309 -485.309 -485.309] [51.6462], Avg: [-601.06 -601.06 -601.06] (1.000)
Step: 10549, Reward: [-563.376 -563.376 -563.376] [35.7282], Avg: [-601.051 -601.051 -601.051] (1.000)
Step: 10599, Reward: [-597.122 -597.122 -597.122] [128.4491], Avg: [-601.638 -601.638 -601.638] (1.000)
Step: 10649, Reward: [-509.265 -509.265 -509.265] [29.6229], Avg: [-601.344 -601.344 -601.344] (1.000)
Step: 10699, Reward: [-569.312 -569.312 -569.312] [165.8236], Avg: [-601.969 -601.969 -601.969] (1.000)
Step: 10749, Reward: [-499.741 -499.741 -499.741] [40.4725], Avg: [-601.682 -601.682 -601.682] (1.000)
Step: 10799, Reward: [-603.545 -603.545 -603.545] [100.1552], Avg: [-602.154 -602.154 -602.154] (1.000)
Step: 10849, Reward: [-533.039 -533.039 -533.039] [72.1660], Avg: [-602.168 -602.168 -602.168] (1.000)
Step: 10899, Reward: [-531.983 -531.983 -531.983] [75.4360], Avg: [-602.192 -602.192 -602.192] (1.000)
Step: 10949, Reward: [-464.409 -464.409 -464.409] [81.1940], Avg: [-601.934 -601.934 -601.934] (1.000)
Step: 10999, Reward: [-548.913 -548.913 -548.913] [206.1882], Avg: [-602.63 -602.63 -602.63] (1.000)
Step: 11049, Reward: [-529.847 -529.847 -529.847] [91.7324], Avg: [-602.716 -602.716 -602.716] (1.000)
Step: 11099, Reward: [-564.945 -564.945 -564.945] [126.5319], Avg: [-603.115 -603.115 -603.115] (1.000)
Step: 11149, Reward: [-494.07 -494.07 -494.07] [99.3015], Avg: [-603.072 -603.072 -603.072] (1.000)
Step: 11199, Reward: [-504.741 -504.741 -504.741] [100.3153], Avg: [-603.081 -603.081 -603.081] (1.000)
Step: 11249, Reward: [-499.08 -499.08 -499.08] [55.5911], Avg: [-602.865 -602.865 -602.865] (1.000)
Step: 11299, Reward: [-589.794 -589.794 -589.794] [101.5500], Avg: [-603.257 -603.257 -603.257] (1.000)
Step: 11349, Reward: [-590.115 -590.115 -590.115] [151.6592], Avg: [-603.867 -603.867 -603.867] (1.000)
Step: 11399, Reward: [-594.075 -594.075 -594.075] [86.9580], Avg: [-604.206 -604.206 -604.206] (1.000)
Step: 11449, Reward: [-591.44 -591.44 -591.44] [120.3736], Avg: [-604.676 -604.676 -604.676] (1.000)
Step: 11499, Reward: [-592.322 -592.322 -592.322] [147.8216], Avg: [-605.265 -605.265 -605.265] (1.000)
Step: 11549, Reward: [-552.864 -552.864 -552.864] [121.8762], Avg: [-605.565 -605.565 -605.565] (1.000)
Step: 11599, Reward: [-697.325 -697.325 -697.325] [108.4766], Avg: [-606.428 -606.428 -606.428] (1.000)
Step: 11649, Reward: [-508.224 -508.224 -508.224] [59.5640], Avg: [-606.263 -606.263 -606.263] (1.000)
Step: 11699, Reward: [-446.36 -446.36 -446.36] [34.7080], Avg: [-605.728 -605.728 -605.728] (1.000)
Step: 11749, Reward: [-600.453 -600.453 -600.453] [180.9019], Avg: [-606.475 -606.475 -606.475] (1.000)
Step: 11799, Reward: [-500.294 -500.294 -500.294] [101.8483], Avg: [-606.457 -606.457 -606.457] (1.000)
Step: 11849, Reward: [-566.57 -566.57 -566.57] [109.2987], Avg: [-606.749 -606.749 -606.749] (1.000)
Step: 11899, Reward: [-555.561 -555.561 -555.561] [119.8810], Avg: [-607.038 -607.038 -607.038] (1.000)
Step: 11949, Reward: [-570.145 -570.145 -570.145] [167.8592], Avg: [-607.586 -607.586 -607.586] (1.000)
Step: 11999, Reward: [-614.409 -614.409 -614.409] [170.2918], Avg: [-608.324 -608.324 -608.324] (1.000)
Step: 12049, Reward: [-530.311 -530.311 -530.311] [101.7960], Avg: [-608.423 -608.423 -608.423] (1.000)
Step: 12099, Reward: [-499.344 -499.344 -499.344] [60.6934], Avg: [-608.223 -608.223 -608.223] (1.000)
Step: 12149, Reward: [-521.111 -521.111 -521.111] [90.6746], Avg: [-608.237 -608.237 -608.237] (1.000)
Step: 12199, Reward: [-490.042 -490.042 -490.042] [59.0505], Avg: [-607.995 -607.995 -607.995] (1.000)
Step: 12249, Reward: [-453.516 -453.516 -453.516] [40.0267], Avg: [-607.528 -607.528 -607.528] (1.000)
Step: 12299, Reward: [-472.927 -472.927 -472.927] [108.3762], Avg: [-607.421 -607.421 -607.421] (1.000)
Step: 12349, Reward: [-545.424 -545.424 -545.424] [115.0757], Avg: [-607.636 -607.636 -607.636] (1.000)
Step: 12399, Reward: [-468.427 -468.427 -468.427] [56.9611], Avg: [-607.304 -607.304 -607.304] (1.000)
Step: 12449, Reward: [-469.739 -469.739 -469.739] [75.1815], Avg: [-607.054 -607.054 -607.054] (1.000)
Step: 12499, Reward: [-503.815 -503.815 -503.815] [98.6294], Avg: [-607.035 -607.035 -607.035] (1.000)
Step: 12549, Reward: [-519.419 -519.419 -519.419] [82.4123], Avg: [-607.015 -607.015 -607.015] (1.000)
Step: 12599, Reward: [-459.571 -459.571 -459.571] [112.7309], Avg: [-606.877 -606.877 -606.877] (1.000)
Step: 12649, Reward: [-436.81 -436.81 -436.81] [70.1986], Avg: [-606.482 -606.482 -606.482] (1.000)
Step: 12699, Reward: [-449.504 -449.504 -449.504] [96.8823], Avg: [-606.246 -606.246 -606.246] (1.000)
Step: 12749, Reward: [-472.972 -472.972 -472.972] [23.8319], Avg: [-605.816 -605.816 -605.816] (1.000)
Step: 12799, Reward: [-544.487 -544.487 -544.487] [69.0093], Avg: [-605.846 -605.846 -605.846] (1.000)
Step: 12849, Reward: [-463.332 -463.332 -463.332] [38.8777], Avg: [-605.443 -605.443 -605.443] (1.000)
Step: 12899, Reward: [-548.421 -548.421 -548.421] [73.2875], Avg: [-605.506 -605.506 -605.506] (1.000)
Step: 12949, Reward: [-498.185 -498.185 -498.185] [69.5571], Avg: [-605.36 -605.36 -605.36] (1.000)
Step: 12999, Reward: [-525.959 -525.959 -525.959] [117.9651], Avg: [-605.509 -605.509 -605.509] (1.000)
Step: 13049, Reward: [-409.951 -409.951 -409.951] [59.8003], Avg: [-604.989 -604.989 -604.989] (1.000)
Step: 13099, Reward: [-468.977 -468.977 -468.977] [139.6237], Avg: [-605.002 -605.002 -605.002] (1.000)
Step: 13149, Reward: [-512.34 -512.34 -512.34] [105.6779], Avg: [-605.052 -605.052 -605.052] (1.000)
Step: 13199, Reward: [-488.842 -488.842 -488.842] [45.2669], Avg: [-604.783 -604.783 -604.783] (1.000)
Step: 13249, Reward: [-469.18 -469.18 -469.18] [93.4313], Avg: [-604.624 -604.624 -604.624] (1.000)
Step: 13299, Reward: [-463.819 -463.819 -463.819] [70.9964], Avg: [-604.362 -604.362 -604.362] (1.000)
Step: 13349, Reward: [-448.974 -448.974 -448.974] [59.2987], Avg: [-604.002 -604.002 -604.002] (1.000)
Step: 13399, Reward: [-471.36 -471.36 -471.36] [102.0729], Avg: [-603.888 -603.888 -603.888] (1.000)
Step: 13449, Reward: [-467.066 -467.066 -467.066] [101.1773], Avg: [-603.755 -603.755 -603.755] (1.000)
Step: 13499, Reward: [-509.787 -509.787 -509.787] [105.0635], Avg: [-603.796 -603.796 -603.796] (1.000)
Step: 13549, Reward: [-447.311 -447.311 -447.311] [77.7047], Avg: [-603.506 -603.506 -603.506] (1.000)
Step: 13599, Reward: [-427.942 -427.942 -427.942] [49.5257], Avg: [-603.042 -603.042 -603.042] (1.000)
Step: 13649, Reward: [-500.438 -500.438 -500.438] [88.7339], Avg: [-602.991 -602.991 -602.991] (1.000)
Step: 13699, Reward: [-476.446 -476.446 -476.446] [73.7205], Avg: [-602.799 -602.799 -602.799] (1.000)
Step: 13749, Reward: [-507.975 -507.975 -507.975] [83.1113], Avg: [-602.756 -602.756 -602.756] (1.000)
Step: 13799, Reward: [-579.115 -579.115 -579.115] [95.6050], Avg: [-603.017 -603.017 -603.017] (1.000)
Step: 13849, Reward: [-456.028 -456.028 -456.028] [87.4122], Avg: [-602.802 -602.802 -602.802] (1.000)
Step: 13899, Reward: [-475.182 -475.182 -475.182] [45.6785], Avg: [-602.507 -602.507 -602.507] (1.000)
Step: 13949, Reward: [-469.395 -469.395 -469.395] [105.7837], Avg: [-602.409 -602.409 -602.409] (1.000)
Step: 13999, Reward: [-505.505 -505.505 -505.505] [106.9480], Avg: [-602.445 -602.445 -602.445] (1.000)
Step: 14049, Reward: [-550.701 -550.701 -550.701] [133.9961], Avg: [-602.738 -602.738 -602.738] (1.000)
Step: 14099, Reward: [-540.128 -540.128 -540.128] [110.4952], Avg: [-602.907 -602.907 -602.907] (1.000)
Step: 14149, Reward: [-517.276 -517.276 -517.276] [64.9224], Avg: [-602.834 -602.834 -602.834] (1.000)
Step: 14199, Reward: [-494.102 -494.102 -494.102] [67.6694], Avg: [-602.69 -602.69 -602.69] (1.000)
Step: 14249, Reward: [-589.595 -589.595 -589.595] [166.2107], Avg: [-603.227 -603.227 -603.227] (1.000)
Step: 14299, Reward: [-511.448 -511.448 -511.448] [114.2183], Avg: [-603.305 -603.305 -603.305] (1.000)
Step: 14349, Reward: [-537.42 -537.42 -537.42] [42.9342], Avg: [-603.225 -603.225 -603.225] (1.000)
Step: 14399, Reward: [-469.541 -469.541 -469.541] [17.3196], Avg: [-602.821 -602.821 -602.821] (1.000)
Step: 14449, Reward: [-559.035 -559.035 -559.035] [121.3789], Avg: [-603.09 -603.09 -603.09] (1.000)
Step: 14499, Reward: [-576.622 -576.622 -576.622] [97.7220], Avg: [-603.335 -603.335 -603.335] (1.000)
Step: 14549, Reward: [-558.89 -558.89 -558.89] [107.1057], Avg: [-603.551 -603.551 -603.551] (1.000)
Step: 14599, Reward: [-540.253 -540.253 -540.253] [66.0333], Avg: [-603.56 -603.56 -603.56] (1.000)
Step: 14649, Reward: [-457.079 -457.079 -457.079] [41.2034], Avg: [-603.201 -603.201 -603.201] (1.000)
Step: 14699, Reward: [-464.18 -464.18 -464.18] [74.7551], Avg: [-602.982 -602.982 -602.982] (1.000)
Step: 14749, Reward: [-511.024 -511.024 -511.024] [54.3416], Avg: [-602.855 -602.855 -602.855] (1.000)
Step: 14799, Reward: [-584.182 -584.182 -584.182] [112.8903], Avg: [-603.173 -603.173 -603.173] (1.000)
Step: 14849, Reward: [-566.776 -566.776 -566.776] [54.2240], Avg: [-603.233 -603.233 -603.233] (1.000)
Step: 14899, Reward: [-508.998 -508.998 -508.998] [85.0964], Avg: [-603.202 -603.202 -603.202] (1.000)
Step: 14949, Reward: [-595.211 -595.211 -595.211] [113.0762], Avg: [-603.554 -603.554 -603.554] (1.000)
Step: 14999, Reward: [-506.307 -506.307 -506.307] [78.7228], Avg: [-603.492 -603.492 -603.492] (1.000)
Step: 15049, Reward: [-532.224 -532.224 -532.224] [102.7113], Avg: [-603.597 -603.597 -603.597] (1.000)
Step: 15099, Reward: [-548.385 -548.385 -548.385] [76.3059], Avg: [-603.666 -603.666 -603.666] (1.000)
Step: 15149, Reward: [-591.873 -591.873 -591.873] [219.2216], Avg: [-604.351 -604.351 -604.351] (1.000)
Step: 15199, Reward: [-481.751 -481.751 -481.751] [72.8500], Avg: [-604.187 -604.187 -604.187] (1.000)
Step: 15249, Reward: [-557.324 -557.324 -557.324] [150.2056], Avg: [-604.526 -604.526 -604.526] (1.000)
Step: 15299, Reward: [-539.829 -539.829 -539.829] [17.5833], Avg: [-604.372 -604.372 -604.372] (1.000)
Step: 15349, Reward: [-670.643 -670.643 -670.643] [221.3094], Avg: [-605.309 -605.309 -605.309] (1.000)
Step: 15399, Reward: [-520.183 -520.183 -520.183] [47.9826], Avg: [-605.188 -605.188 -605.188] (1.000)
Step: 15449, Reward: [-509.967 -509.967 -509.967] [53.3922], Avg: [-605.053 -605.053 -605.053] (1.000)
Step: 15499, Reward: [-547.862 -547.862 -547.862] [76.2739], Avg: [-605.115 -605.115 -605.115] (1.000)
Step: 15549, Reward: [-497.415 -497.415 -497.415] [206.4242], Avg: [-605.432 -605.432 -605.432] (1.000)
Step: 15599, Reward: [-457.127 -457.127 -457.127] [55.3035], Avg: [-605.134 -605.134 -605.134] (1.000)
Step: 15649, Reward: [-521.253 -521.253 -521.253] [71.2795], Avg: [-605.094 -605.094 -605.094] (1.000)
Step: 15699, Reward: [-557.003 -557.003 -557.003] [65.8117], Avg: [-605.15 -605.15 -605.15] (1.000)
Step: 15749, Reward: [-569.307 -569.307 -569.307] [77.0704], Avg: [-605.281 -605.281 -605.281] (1.000)
Step: 15799, Reward: [-557.362 -557.362 -557.362] [150.0911], Avg: [-605.604 -605.604 -605.604] (1.000)
Step: 15849, Reward: [-506.669 -506.669 -506.669] [74.9201], Avg: [-605.529 -605.529 -605.529] (1.000)
Step: 15899, Reward: [-584.54 -584.54 -584.54] [92.8405], Avg: [-605.754 -605.754 -605.754] (1.000)
Step: 15949, Reward: [-541.059 -541.059 -541.059] [88.7035], Avg: [-605.83 -605.83 -605.83] (1.000)
Step: 15999, Reward: [-603.988 -603.988 -603.988] [107.9944], Avg: [-606.161 -606.161 -606.161] (1.000)
Step: 16049, Reward: [-501.061 -501.061 -501.061] [92.6469], Avg: [-606.123 -606.123 -606.123] (1.000)
Step: 16099, Reward: [-532.87 -532.87 -532.87] [52.5676], Avg: [-606.058 -606.058 -606.058] (1.000)
Step: 16149, Reward: [-482.089 -482.089 -482.089] [90.3448], Avg: [-605.954 -605.954 -605.954] (1.000)
Step: 16199, Reward: [-556.625 -556.625 -556.625] [194.0983], Avg: [-606.401 -606.401 -606.401] (1.000)
Step: 16249, Reward: [-429.582 -429.582 -429.582] [56.8540], Avg: [-606.032 -606.032 -606.032] (1.000)
Step: 16299, Reward: [-495.881 -495.881 -495.881] [142.5595], Avg: [-606.131 -606.131 -606.131] (1.000)
Step: 16349, Reward: [-486.523 -486.523 -486.523] [107.9199], Avg: [-606.096 -606.096 -606.096] (1.000)
Step: 16399, Reward: [-687.044 -687.044 -687.044] [148.0379], Avg: [-606.794 -606.794 -606.794] (1.000)
Step: 16449, Reward: [-519.055 -519.055 -519.055] [79.2685], Avg: [-606.768 -606.768 -606.768] (1.000)
Step: 16499, Reward: [-473.483 -473.483 -473.483] [58.4596], Avg: [-606.541 -606.541 -606.541] (1.000)
Step: 16549, Reward: [-487.952 -487.952 -487.952] [71.4583], Avg: [-606.399 -606.399 -606.399] (1.000)
Step: 16599, Reward: [-537.925 -537.925 -537.925] [70.7829], Avg: [-606.406 -606.406 -606.406] (1.000)
Step: 16649, Reward: [-481.422 -481.422 -481.422] [49.0690], Avg: [-606.178 -606.178 -606.178] (1.000)
Step: 16699, Reward: [-559.037 -559.037 -559.037] [39.5263], Avg: [-606.155 -606.155 -606.155] (1.000)
Step: 16749, Reward: [-514.064 -514.064 -514.064] [57.2894], Avg: [-606.051 -606.051 -606.051] (1.000)
Step: 16799, Reward: [-457.308 -457.308 -457.308] [55.9753], Avg: [-605.775 -605.775 -605.775] (1.000)
Step: 16849, Reward: [-651.896 -651.896 -651.896] [129.0552], Avg: [-606.295 -606.295 -606.295] (1.000)
Step: 16899, Reward: [-528.421 -528.421 -528.421] [95.1540], Avg: [-606.346 -606.346 -606.346] (1.000)
Step: 16949, Reward: [-484.123 -484.123 -484.123] [60.9979], Avg: [-606.165 -606.165 -606.165] (1.000)
Step: 16999, Reward: [-659.61 -659.61 -659.61] [143.2189], Avg: [-606.744 -606.744 -606.744] (1.000)
Step: 17049, Reward: [-606.638 -606.638 -606.638] [119.0409], Avg: [-607.093 -607.093 -607.093] (1.000)
Step: 17099, Reward: [-533.205 -533.205 -533.205] [54.2115], Avg: [-607.035 -607.035 -607.035] (1.000)
Step: 17149, Reward: [-449.949 -449.949 -449.949] [26.4807], Avg: [-606.654 -606.654 -606.654] (1.000)
Step: 17199, Reward: [-525.831 -525.831 -525.831] [60.5181], Avg: [-606.595 -606.595 -606.595] (1.000)
Step: 17249, Reward: [-594.665 -594.665 -594.665] [84.5649], Avg: [-606.806 -606.806 -606.806] (1.000)
Step: 17299, Reward: [-540.472 -540.472 -540.472] [174.5267], Avg: [-607.119 -607.119 -607.119] (1.000)
Step: 17349, Reward: [-607.292 -607.292 -607.292] [92.4166], Avg: [-607.385 -607.385 -607.385] (1.000)
Step: 17399, Reward: [-429.359 -429.359 -429.359] [53.7461], Avg: [-607.028 -607.028 -607.028] (1.000)
Step: 17449, Reward: [-478.864 -478.864 -478.864] [96.3933], Avg: [-606.937 -606.937 -606.937] (1.000)
Step: 17499, Reward: [-489.254 -489.254 -489.254] [73.6686], Avg: [-606.811 -606.811 -606.811] (1.000)
Step: 17549, Reward: [-559.602 -559.602 -559.602] [127.3717], Avg: [-607.04 -607.04 -607.04] (1.000)
Step: 17599, Reward: [-425.052 -425.052 -425.052] [34.0269], Avg: [-606.62 -606.62 -606.62] (1.000)
Step: 17649, Reward: [-547.085 -547.085 -547.085] [37.4852], Avg: [-606.557 -606.557 -606.557] (1.000)
Step: 17699, Reward: [-541.74 -541.74 -541.74] [136.0170], Avg: [-606.758 -606.758 -606.758] (1.000)
Step: 17749, Reward: [-489.738 -489.738 -489.738] [125.8134], Avg: [-606.783 -606.783 -606.783] (1.000)
Step: 17799, Reward: [-563.385 -563.385 -563.385] [69.3734], Avg: [-606.856 -606.856 -606.856] (1.000)
Step: 17849, Reward: [-570.234 -570.234 -570.234] [119.3228], Avg: [-607.088 -607.088 -607.088] (1.000)
Step: 17899, Reward: [-510.826 -510.826 -510.826] [76.2510], Avg: [-607.032 -607.032 -607.032] (1.000)
Step: 17949, Reward: [-455.293 -455.293 -455.293] [66.3810], Avg: [-606.794 -606.794 -606.794] (1.000)
Step: 17999, Reward: [-497.961 -497.961 -497.961] [70.1226], Avg: [-606.686 -606.686 -606.686] (1.000)
Step: 18049, Reward: [-433.709 -433.709 -433.709] [28.7000], Avg: [-606.287 -606.287 -606.287] (1.000)
Step: 18099, Reward: [-533.256 -533.256 -533.256] [108.3416], Avg: [-606.384 -606.384 -606.384] (1.000)
Step: 18149, Reward: [-533.757 -533.757 -533.757] [93.9095], Avg: [-606.443 -606.443 -606.443] (1.000)
Step: 18199, Reward: [-479.078 -479.078 -479.078] [48.4185], Avg: [-606.226 -606.226 -606.226] (1.000)
Step: 18249, Reward: [-455.265 -455.265 -455.265] [82.7246], Avg: [-606.039 -606.039 -606.039] (1.000)
Step: 18299, Reward: [-457.124 -457.124 -457.124] [74.0916], Avg: [-605.835 -605.835 -605.835] (1.000)
Step: 18349, Reward: [-567.197 -567.197 -567.197] [108.6794], Avg: [-606.025 -606.025 -606.025] (1.000)
Step: 18399, Reward: [-509.333 -509.333 -509.333] [45.4425], Avg: [-605.886 -605.886 -605.886] (1.000)
Step: 18449, Reward: [-438.405 -438.405 -438.405] [72.0876], Avg: [-605.628 -605.628 -605.628] (1.000)
Step: 18499, Reward: [-564.8 -564.8 -564.8] [97.0319], Avg: [-605.78 -605.78 -605.78] (1.000)
Step: 18549, Reward: [-549.66 -549.66 -549.66] [99.0187], Avg: [-605.895 -605.895 -605.895] (1.000)
Step: 18599, Reward: [-614.803 -614.803 -614.803] [125.2989], Avg: [-606.256 -606.256 -606.256] (1.000)
Step: 18649, Reward: [-642.68 -642.68 -642.68] [146.6689], Avg: [-606.747 -606.747 -606.747] (1.000)
Step: 18699, Reward: [-537.799 -537.799 -537.799] [111.0019], Avg: [-606.859 -606.859 -606.859] (1.000)
Step: 18749, Reward: [-609.659 -609.659 -609.659] [141.4926], Avg: [-607.244 -607.244 -607.244] (1.000)
Step: 18799, Reward: [-491.574 -491.574 -491.574] [81.2018], Avg: [-607.152 -607.152 -607.152] (1.000)
Step: 18849, Reward: [-460.621 -460.621 -460.621] [45.9818], Avg: [-606.886 -606.886 -606.886] (1.000)
Step: 18899, Reward: [-544.396 -544.396 -544.396] [112.0241], Avg: [-607.017 -607.017 -607.017] (1.000)
Step: 18949, Reward: [-564.551 -564.551 -564.551] [115.7200], Avg: [-607.21 -607.21 -607.21] (1.000)
Step: 18999, Reward: [-568.659 -568.659 -568.659] [83.0909], Avg: [-607.327 -607.327 -607.327] (1.000)
Step: 19049, Reward: [-598.038 -598.038 -598.038] [142.1587], Avg: [-607.676 -607.676 -607.676] (1.000)
Step: 19099, Reward: [-504.251 -504.251 -504.251] [44.1602], Avg: [-607.521 -607.521 -607.521] (1.000)
Step: 19149, Reward: [-499.925 -499.925 -499.925] [66.8355], Avg: [-607.414 -607.414 -607.414] (1.000)
Step: 19199, Reward: [-443.798 -443.798 -443.798] [65.0574], Avg: [-607.158 -607.158 -607.158] (1.000)
Step: 19249, Reward: [-474.358 -474.358 -474.358] [65.3594], Avg: [-606.983 -606.983 -606.983] (1.000)
Step: 19299, Reward: [-547.272 -547.272 -547.272] [78.9052], Avg: [-607.032 -607.032 -607.032] (1.000)
Step: 19349, Reward: [-497.788 -497.788 -497.788] [83.9248], Avg: [-606.967 -606.967 -606.967] (1.000)
Step: 19399, Reward: [-497.393 -497.393 -497.393] [44.7333], Avg: [-606.8 -606.8 -606.8] (1.000)
Step: 19449, Reward: [-467.9 -467.9 -467.9] [77.7266], Avg: [-606.643 -606.643 -606.643] (1.000)
Step: 19499, Reward: [-529. -529. -529.] [48.9043], Avg: [-606.569 -606.569 -606.569] (1.000)
Step: 19549, Reward: [-491.766 -491.766 -491.766] [45.3525], Avg: [-606.391 -606.391 -606.391] (1.000)
Step: 19599, Reward: [-416.199 -416.199 -416.199] [21.6479], Avg: [-605.961 -605.961 -605.961] (1.000)
Step: 19649, Reward: [-424.358 -424.358 -424.358] [78.0777], Avg: [-605.698 -605.698 -605.698] (1.000)
Step: 19699, Reward: [-461.177 -461.177 -461.177] [66.8419], Avg: [-605.501 -605.501 -605.501] (1.000)
Step: 19749, Reward: [-504.616 -504.616 -504.616] [90.6954], Avg: [-605.475 -605.475 -605.475] (1.000)
Step: 19799, Reward: [-459.846 -459.846 -459.846] [87.1914], Avg: [-605.327 -605.327 -605.327] (1.000)
Step: 19849, Reward: [-507.993 -507.993 -507.993] [48.6047], Avg: [-605.205 -605.205 -605.205] (1.000)
Step: 19899, Reward: [-591.841 -591.841 -591.841] [176.2947], Avg: [-605.614 -605.614 -605.614] (1.000)
Step: 19949, Reward: [-580.432 -580.432 -580.432] [93.1127], Avg: [-605.784 -605.784 -605.784] (1.000)
Step: 19999, Reward: [-529.608 -529.608 -529.608] [131.9252], Avg: [-605.924 -605.924 -605.924] (1.000)
Step: 20049, Reward: [-506.005 -506.005 -506.005] [79.5642], Avg: [-605.873 -605.873 -605.873] (1.000)
Step: 20099, Reward: [-510.04 -510.04 -510.04] [62.0677], Avg: [-605.789 -605.789 -605.789] (1.000)
Step: 20149, Reward: [-469.113 -469.113 -469.113] [79.2600], Avg: [-605.646 -605.646 -605.646] (1.000)
Step: 20199, Reward: [-477.414 -477.414 -477.414] [134.5464], Avg: [-605.662 -605.662 -605.662] (1.000)
Step: 20249, Reward: [-438.093 -438.093 -438.093] [46.3398], Avg: [-605.363 -605.363 -605.363] (1.000)
Step: 20299, Reward: [-603.974 -603.974 -603.974] [133.9532], Avg: [-605.689 -605.689 -605.689] (1.000)
Step: 20349, Reward: [-472.861 -472.861 -472.861] [80.7430], Avg: [-605.561 -605.561 -605.561] (1.000)
Step: 20399, Reward: [-509.796 -509.796 -509.796] [106.3950], Avg: [-605.587 -605.587 -605.587] (1.000)
Step: 20449, Reward: [-489.833 -489.833 -489.833] [101.9772], Avg: [-605.554 -605.554 -605.554] (1.000)
Step: 20499, Reward: [-526.421 -526.421 -526.421] [81.3738], Avg: [-605.559 -605.559 -605.559] (1.000)
Step: 20549, Reward: [-658.909 -658.909 -658.909] [128.6555], Avg: [-606.002 -606.002 -606.002] (1.000)
Step: 20599, Reward: [-552.582 -552.582 -552.582] [78.3742], Avg: [-606.062 -606.062 -606.062] (1.000)
Step: 20649, Reward: [-511.96 -511.96 -511.96] [112.8896], Avg: [-606.108 -606.108 -606.108] (1.000)
Step: 20699, Reward: [-517.834 -517.834 -517.834] [77.4043], Avg: [-606.082 -606.082 -606.082] (1.000)
Step: 20749, Reward: [-674.003 -674.003 -674.003] [117.4661], Avg: [-606.528 -606.528 -606.528] (1.000)
Step: 20799, Reward: [-521.316 -521.316 -521.316] [70.8322], Avg: [-606.494 -606.494 -606.494] (1.000)
Step: 20849, Reward: [-588.109 -588.109 -588.109] [128.7358], Avg: [-606.758 -606.758 -606.758] (1.000)
Step: 20899, Reward: [-516.149 -516.149 -516.149] [96.6875], Avg: [-606.773 -606.773 -606.773] (1.000)
Step: 20949, Reward: [-635.099 -635.099 -635.099] [166.5481], Avg: [-607.238 -607.238 -607.238] (1.000)
Step: 20999, Reward: [-577.606 -577.606 -577.606] [103.5246], Avg: [-607.414 -607.414 -607.414] (1.000)
Step: 21049, Reward: [-538.749 -538.749 -538.749] [124.5632], Avg: [-607.547 -607.547 -607.547] (1.000)
Step: 21099, Reward: [-554.048 -554.048 -554.048] [153.1341], Avg: [-607.783 -607.783 -607.783] (1.000)
Step: 21149, Reward: [-604.476 -604.476 -604.476] [91.8024], Avg: [-607.992 -607.992 -607.992] (1.000)
Step: 21199, Reward: [-524.77 -524.77 -524.77] [37.4246], Avg: [-607.884 -607.884 -607.884] (1.000)
Step: 21249, Reward: [-522.181 -522.181 -522.181] [84.0509], Avg: [-607.88 -607.88 -607.88] (1.000)
Step: 21299, Reward: [-495.977 -495.977 -495.977] [52.5949], Avg: [-607.741 -607.741 -607.741] (1.000)
Step: 21349, Reward: [-487.094 -487.094 -487.094] [43.6995], Avg: [-607.561 -607.561 -607.561] (1.000)
Step: 21399, Reward: [-584.484 -584.484 -584.484] [154.9295], Avg: [-607.869 -607.869 -607.869] (1.000)
Step: 21449, Reward: [-519.877 -519.877 -519.877] [94.9709], Avg: [-607.885 -607.885 -607.885] (1.000)
Step: 21499, Reward: [-510.364 -510.364 -510.364] [123.6532], Avg: [-607.946 -607.946 -607.946] (1.000)
Step: 21549, Reward: [-586.916 -586.916 -586.916] [96.2041], Avg: [-608.12 -608.12 -608.12] (1.000)
Step: 21599, Reward: [-561.018 -561.018 -561.018] [84.6969], Avg: [-608.207 -608.207 -608.207] (1.000)
Step: 21649, Reward: [-643.388 -643.388 -643.388] [116.6403], Avg: [-608.558 -608.558 -608.558] (1.000)
Step: 21699, Reward: [-575.214 -575.214 -575.214] [137.8210], Avg: [-608.799 -608.799 -608.799] (1.000)
Step: 21749, Reward: [-554.679 -554.679 -554.679] [90.0750], Avg: [-608.881 -608.881 -608.881] (1.000)
Step: 21799, Reward: [-590.868 -590.868 -590.868] [115.1944], Avg: [-609.104 -609.104 -609.104] (1.000)
Step: 21849, Reward: [-578.652 -578.652 -578.652] [178.8095], Avg: [-609.444 -609.444 -609.444] (1.000)
Step: 21899, Reward: [-492.52 -492.52 -492.52] [89.8533], Avg: [-609.382 -609.382 -609.382] (1.000)
Step: 21949, Reward: [-514.706 -514.706 -514.706] [125.8497], Avg: [-609.453 -609.453 -609.453] (1.000)
Step: 21999, Reward: [-510.154 -510.154 -510.154] [55.8298], Avg: [-609.354 -609.354 -609.354] (1.000)
Step: 22049, Reward: [-515.13 -515.13 -515.13] [104.7222], Avg: [-609.378 -609.378 -609.378] (1.000)
Step: 22099, Reward: [-487.663 -487.663 -487.663] [116.3220], Avg: [-609.366 -609.366 -609.366] (1.000)
Step: 22149, Reward: [-541.819 -541.819 -541.819] [160.8621], Avg: [-609.576 -609.576 -609.576] (1.000)
Step: 22199, Reward: [-586.744 -586.744 -586.744] [104.8873], Avg: [-609.761 -609.761 -609.761] (1.000)
Step: 22249, Reward: [-471.086 -471.086 -471.086] [54.0855], Avg: [-609.571 -609.571 -609.571] (1.000)
Step: 22299, Reward: [-563.962 -563.962 -563.962] [73.7144], Avg: [-609.634 -609.634 -609.634] (1.000)
Step: 22349, Reward: [-533.635 -533.635 -533.635] [50.9638], Avg: [-609.578 -609.578 -609.578] (1.000)
Step: 22399, Reward: [-463.572 -463.572 -463.572] [87.7252], Avg: [-609.448 -609.448 -609.448] (1.000)
Step: 22449, Reward: [-634.53 -634.53 -634.53] [74.6300], Avg: [-609.67 -609.67 -609.67] (1.000)
Step: 22499, Reward: [-516.105 -516.105 -516.105] [116.0554], Avg: [-609.72 -609.72 -609.72] (1.000)
Step: 22549, Reward: [-608.762 -608.762 -608.762] [111.0488], Avg: [-609.964 -609.964 -609.964] (1.000)
Step: 22599, Reward: [-663.168 -663.168 -663.168] [103.3353], Avg: [-610.31 -610.31 -610.31] (1.000)
Step: 22649, Reward: [-491.457 -491.457 -491.457] [74.7907], Avg: [-610.213 -610.213 -610.213] (1.000)
Step: 22699, Reward: [-517.437 -517.437 -517.437] [113.2722], Avg: [-610.258 -610.258 -610.258] (1.000)
Step: 22749, Reward: [-512.183 -512.183 -512.183] [80.8269], Avg: [-610.22 -610.22 -610.22] (1.000)
Step: 22799, Reward: [-629.886 -629.886 -629.886] [116.0303], Avg: [-610.518 -610.518 -610.518] (1.000)
Step: 22849, Reward: [-516.976 -516.976 -516.976] [123.2596], Avg: [-610.583 -610.583 -610.583] (1.000)
Step: 22899, Reward: [-497.877 -497.877 -497.877] [102.6183], Avg: [-610.561 -610.561 -610.561] (1.000)
Step: 22949, Reward: [-454.321 -454.321 -454.321] [33.7475], Avg: [-610.294 -610.294 -610.294] (1.000)
Step: 22999, Reward: [-556.576 -556.576 -556.576] [76.1222], Avg: [-610.343 -610.343 -610.343] (1.000)
Step: 23049, Reward: [-571.976 -571.976 -571.976] [70.4524], Avg: [-610.412 -610.412 -610.412] (1.000)
Step: 23099, Reward: [-570.179 -570.179 -570.179] [95.3974], Avg: [-610.532 -610.532 -610.532] (1.000)
Step: 23149, Reward: [-442.325 -442.325 -442.325] [78.0504], Avg: [-610.337 -610.337 -610.337] (1.000)
Step: 23199, Reward: [-552.943 -552.943 -552.943] [130.6331], Avg: [-610.495 -610.495 -610.495] (1.000)
Step: 23249, Reward: [-539.38 -539.38 -539.38] [126.0073], Avg: [-610.613 -610.613 -610.613] (1.000)
Step: 23299, Reward: [-630.627 -630.627 -630.627] [198.8683], Avg: [-611.083 -611.083 -611.083] (1.000)
Step: 23349, Reward: [-577.345 -577.345 -577.345] [76.6222], Avg: [-611.175 -611.175 -611.175] (1.000)
Step: 23399, Reward: [-540.691 -540.691 -540.691] [70.6376], Avg: [-611.175 -611.175 -611.175] (1.000)
Step: 23449, Reward: [-533.753 -533.753 -533.753] [104.2882], Avg: [-611.232 -611.232 -611.232] (1.000)
Step: 23499, Reward: [-521.283 -521.283 -521.283] [113.1128], Avg: [-611.281 -611.281 -611.281] (1.000)
Step: 23549, Reward: [-524.657 -524.657 -524.657] [135.1025], Avg: [-611.384 -611.384 -611.384] (1.000)
Step: 23599, Reward: [-534.259 -534.259 -534.259] [145.5022], Avg: [-611.529 -611.529 -611.529] (1.000)
Step: 23649, Reward: [-548.339 -548.339 -548.339] [174.1381], Avg: [-611.764 -611.764 -611.764] (1.000)
Step: 23699, Reward: [-560.657 -560.657 -560.657] [76.9837], Avg: [-611.818 -611.818 -611.818] (1.000)
Step: 23749, Reward: [-526.828 -526.828 -526.828] [94.1542], Avg: [-611.838 -611.838 -611.838] (1.000)
Step: 23799, Reward: [-423.165 -423.165 -423.165] [38.2107], Avg: [-611.522 -611.522 -611.522] (1.000)
Step: 23849, Reward: [-441.547 -441.547 -441.547] [64.3923], Avg: [-611.3 -611.3 -611.3] (1.000)
Step: 23899, Reward: [-478.503 -478.503 -478.503] [66.3881], Avg: [-611.161 -611.161 -611.161] (1.000)
Step: 23949, Reward: [-488.93 -488.93 -488.93] [20.8031], Avg: [-610.95 -610.95 -610.95] (1.000)
Step: 23999, Reward: [-461.317 -461.317 -461.317] [96.3879], Avg: [-610.839 -610.839 -610.839] (1.000)
Step: 24049, Reward: [-582.06 -582.06 -582.06] [109.1297], Avg: [-611.006 -611.006 -611.006] (1.000)
Step: 24099, Reward: [-553.207 -553.207 -553.207] [92.7290], Avg: [-611.078 -611.078 -611.078] (1.000)
Step: 24149, Reward: [-549.898 -549.898 -549.898] [101.8771], Avg: [-611.162 -611.162 -611.162] (1.000)
Step: 24199, Reward: [-511.281 -511.281 -511.281] [88.1994], Avg: [-611.138 -611.138 -611.138] (1.000)
Step: 24249, Reward: [-492.202 -492.202 -492.202] [53.1079], Avg: [-611.003 -611.003 -611.003] (1.000)
Step: 24299, Reward: [-517.797 -517.797 -517.797] [75.2974], Avg: [-610.966 -610.966 -610.966] (1.000)
Step: 24349, Reward: [-512.163 -512.163 -512.163] [59.3278], Avg: [-610.885 -610.885 -610.885] (1.000)
Step: 24399, Reward: [-476.392 -476.392 -476.392] [53.9556], Avg: [-610.72 -610.72 -610.72] (1.000)
Step: 24449, Reward: [-471.352 -471.352 -471.352] [79.6301], Avg: [-610.597 -610.597 -610.597] (1.000)
Step: 24499, Reward: [-594.505 -594.505 -594.505] [109.9537], Avg: [-610.789 -610.789 -610.789] (1.000)
Step: 24549, Reward: [-504.193 -504.193 -504.193] [58.6370], Avg: [-610.691 -610.691 -610.691] (1.000)
Step: 24599, Reward: [-586.195 -586.195 -586.195] [188.4449], Avg: [-611.025 -611.025 -611.025] (1.000)
Step: 24649, Reward: [-685.932 -685.932 -685.932] [108.6280], Avg: [-611.397 -611.397 -611.397] (1.000)
Step: 24699, Reward: [-569.074 -569.074 -569.074] [62.6463], Avg: [-611.438 -611.438 -611.438] (1.000)
Step: 24749, Reward: [-529.852 -529.852 -529.852] [112.2010], Avg: [-611.5 -611.5 -611.5] (1.000)
Step: 24799, Reward: [-561.963 -561.963 -561.963] [129.2225], Avg: [-611.66 -611.66 -611.66] (1.000)
Step: 24849, Reward: [-522.442 -522.442 -522.442] [88.4058], Avg: [-611.659 -611.659 -611.659] (1.000)
Step: 24899, Reward: [-529.057 -529.057 -529.057] [129.1859], Avg: [-611.752 -611.752 -611.752] (1.000)
Step: 24949, Reward: [-595.863 -595.863 -595.863] [71.2518], Avg: [-611.863 -611.863 -611.863] (1.000)
Step: 24999, Reward: [-601.505 -601.505 -601.505] [136.2856], Avg: [-612.115 -612.115 -612.115] (1.000)
Step: 25049, Reward: [-569.025 -569.025 -569.025] [141.6134], Avg: [-612.312 -612.312 -612.312] (1.000)
Step: 25099, Reward: [-576.42 -576.42 -576.42] [57.7433], Avg: [-612.355 -612.355 -612.355] (1.000)
Step: 25149, Reward: [-562.894 -562.894 -562.894] [85.2799], Avg: [-612.427 -612.427 -612.427] (1.000)
Step: 25199, Reward: [-548.792 -548.792 -548.792] [77.0100], Avg: [-612.453 -612.453 -612.453] (1.000)
Step: 25249, Reward: [-600.117 -600.117 -600.117] [87.6964], Avg: [-612.602 -612.602 -612.602] (1.000)
Step: 25299, Reward: [-608.584 -608.584 -608.584] [166.2108], Avg: [-612.923 -612.923 -612.923] (1.000)
Step: 25349, Reward: [-602.562 -602.562 -602.562] [98.6601], Avg: [-613.097 -613.097 -613.097] (1.000)
Step: 25399, Reward: [-448.076 -448.076 -448.076] [49.3245], Avg: [-612.869 -612.869 -612.869] (1.000)
Step: 25449, Reward: [-542.695 -542.695 -542.695] [53.3425], Avg: [-612.836 -612.836 -612.836] (1.000)
Step: 25499, Reward: [-551.022 -551.022 -551.022] [83.3340], Avg: [-612.878 -612.878 -612.878] (1.000)
Step: 25549, Reward: [-505.989 -505.989 -505.989] [110.0565], Avg: [-612.885 -612.885 -612.885] (1.000)
Step: 25599, Reward: [-427.139 -427.139 -427.139] [25.3372], Avg: [-612.571 -612.571 -612.571] (1.000)
Step: 25649, Reward: [-447.082 -447.082 -447.082] [77.7860], Avg: [-612.4 -612.4 -612.4] (1.000)
Step: 25699, Reward: [-460.219 -460.219 -460.219] [74.0717], Avg: [-612.248 -612.248 -612.248] (1.000)
Step: 25749, Reward: [-508.765 -508.765 -508.765] [56.6946], Avg: [-612.158 -612.158 -612.158] (1.000)
Step: 25799, Reward: [-528.451 -528.451 -528.451] [177.9635], Avg: [-612.34 -612.34 -612.34] (1.000)
Step: 25849, Reward: [-576.962 -576.962 -576.962] [105.1420], Avg: [-612.475 -612.475 -612.475] (1.000)
Step: 25899, Reward: [-448.673 -448.673 -448.673] [88.2229], Avg: [-612.329 -612.329 -612.329] (1.000)
Step: 25949, Reward: [-536.862 -536.862 -536.862] [106.6494], Avg: [-612.389 -612.389 -612.389] (1.000)
Step: 25999, Reward: [-518.156 -518.156 -518.156] [79.0032], Avg: [-612.36 -612.36 -612.36] (1.000)
Step: 26049, Reward: [-504.379 -504.379 -504.379] [63.1863], Avg: [-612.274 -612.274 -612.274] (1.000)
Step: 26099, Reward: [-427.706 -427.706 -427.706] [76.5522], Avg: [-612.067 -612.067 -612.067] (1.000)
Step: 26149, Reward: [-539.63 -539.63 -539.63] [128.7231], Avg: [-612.175 -612.175 -612.175] (1.000)
Step: 26199, Reward: [-528.817 -528.817 -528.817] [77.2442], Avg: [-612.163 -612.163 -612.163] (1.000)
Step: 26249, Reward: [-511.409 -511.409 -511.409] [72.6746], Avg: [-612.11 -612.11 -612.11] (1.000)
Step: 26299, Reward: [-473.559 -473.559 -473.559] [35.4984], Avg: [-611.914 -611.914 -611.914] (1.000)
Step: 26349, Reward: [-420.751 -420.751 -420.751] [37.1891], Avg: [-611.622 -611.622 -611.622] (1.000)
Step: 26399, Reward: [-527.11 -527.11 -527.11] [126.0522], Avg: [-611.7 -611.7 -611.7] (1.000)
Step: 26449, Reward: [-477.372 -477.372 -477.372] [47.1776], Avg: [-611.535 -611.535 -611.535] (1.000)
Step: 26499, Reward: [-444.259 -444.259 -444.259] [73.5175], Avg: [-611.359 -611.359 -611.359] (1.000)
Step: 26549, Reward: [-529.524 -529.524 -529.524] [80.9944], Avg: [-611.357 -611.357 -611.357] (1.000)
Step: 26599, Reward: [-537.093 -537.093 -537.093] [132.4839], Avg: [-611.466 -611.466 -611.466] (1.000)
Step: 26649, Reward: [-497.233 -497.233 -497.233] [109.9887], Avg: [-611.458 -611.458 -611.458] (1.000)
Step: 26699, Reward: [-447.595 -447.595 -447.595] [30.8855], Avg: [-611.209 -611.209 -611.209] (1.000)
Step: 26749, Reward: [-528.357 -528.357 -528.357] [106.4935], Avg: [-611.254 -611.254 -611.254] (1.000)
Step: 26799, Reward: [-552.036 -552.036 -552.036] [114.9569], Avg: [-611.358 -611.358 -611.358] (1.000)
Step: 26849, Reward: [-444.544 -444.544 -444.544] [72.6183], Avg: [-611.182 -611.182 -611.182] (1.000)
Step: 26899, Reward: [-556.83 -556.83 -556.83] [125.5161], Avg: [-611.314 -611.314 -611.314] (1.000)
Step: 26949, Reward: [-560.423 -560.423 -560.423] [98.6245], Avg: [-611.403 -611.403 -611.403] (1.000)
Step: 26999, Reward: [-505.262 -505.262 -505.262] [64.9112], Avg: [-611.327 -611.327 -611.327] (1.000)
Step: 27049, Reward: [-500.983 -500.983 -500.983] [97.5095], Avg: [-611.303 -611.303 -611.303] (1.000)
Step: 27099, Reward: [-480.618 -480.618 -480.618] [82.7314], Avg: [-611.214 -611.214 -611.214] (1.000)
Step: 27149, Reward: [-532.286 -532.286 -532.286] [93.4643], Avg: [-611.241 -611.241 -611.241] (1.000)
Step: 27199, Reward: [-483.7 -483.7 -483.7] [82.4831], Avg: [-611.158 -611.158 -611.158] (1.000)
Step: 27249, Reward: [-542.805 -542.805 -542.805] [87.6227], Avg: [-611.194 -611.194 -611.194] (1.000)
Step: 27299, Reward: [-543.625 -543.625 -543.625] [43.4521], Avg: [-611.15 -611.15 -611.15] (1.000)
Step: 27349, Reward: [-553.192 -553.192 -553.192] [89.8320], Avg: [-611.208 -611.208 -611.208] (1.000)
Step: 27399, Reward: [-473.532 -473.532 -473.532] [31.0414], Avg: [-611.013 -611.013 -611.013] (1.000)
Step: 27449, Reward: [-568.493 -568.493 -568.493] [171.6207], Avg: [-611.248 -611.248 -611.248] (1.000)
Step: 27499, Reward: [-555.273 -555.273 -555.273] [59.3029], Avg: [-611.254 -611.254 -611.254] (1.000)
Step: 27549, Reward: [-515.558 -515.558 -515.558] [102.2521], Avg: [-611.266 -611.266 -611.266] (1.000)
Step: 27599, Reward: [-586.097 -586.097 -586.097] [84.2277], Avg: [-611.373 -611.373 -611.373] (1.000)
Step: 27649, Reward: [-489.884 -489.884 -489.884] [57.1072], Avg: [-611.257 -611.257 -611.257] (1.000)
Step: 27699, Reward: [-625.081 -625.081 -625.081] [66.0124], Avg: [-611.401 -611.401 -611.401] (1.000)
Step: 27749, Reward: [-569.702 -569.702 -569.702] [136.2717], Avg: [-611.571 -611.571 -611.571] (1.000)
Step: 27799, Reward: [-569.393 -569.393 -569.393] [111.2096], Avg: [-611.696 -611.696 -611.696] (1.000)
Step: 27849, Reward: [-592.27 -592.27 -592.27] [122.4267], Avg: [-611.881 -611.881 -611.881] (1.000)
Step: 27899, Reward: [-515.818 -515.818 -515.818] [144.6288], Avg: [-611.968 -611.968 -611.968] (1.000)
Step: 27949, Reward: [-621.121 -621.121 -621.121] [115.9230], Avg: [-612.191 -612.191 -612.191] (1.000)
Step: 27999, Reward: [-522.357 -522.357 -522.357] [42.7302], Avg: [-612.107 -612.107 -612.107] (1.000)
Step: 28049, Reward: [-508.536 -508.536 -508.536] [60.6459], Avg: [-612.031 -612.031 -612.031] (1.000)
Step: 28099, Reward: [-488.16 -488.16 -488.16] [82.9170], Avg: [-611.958 -611.958 -611.958] (1.000)
Step: 28149, Reward: [-580.609 -580.609 -580.609] [140.1915], Avg: [-612.151 -612.151 -612.151] (1.000)
Step: 28199, Reward: [-571.777 -571.777 -571.777] [60.1536], Avg: [-612.186 -612.186 -612.186] (1.000)
Step: 28249, Reward: [-562.967 -562.967 -562.967] [62.0951], Avg: [-612.209 -612.209 -612.209] (1.000)
Step: 28299, Reward: [-528.266 -528.266 -528.266] [110.3461], Avg: [-612.256 -612.256 -612.256] (1.000)
Step: 28349, Reward: [-490.977 -490.977 -490.977] [49.8824], Avg: [-612.13 -612.13 -612.13] (1.000)
Step: 28399, Reward: [-502.783 -502.783 -502.783] [33.9854], Avg: [-611.997 -611.997 -611.997] (1.000)
Step: 28449, Reward: [-516.377 -516.377 -516.377] [154.1781], Avg: [-612.1 -612.1 -612.1] (1.000)
Step: 28499, Reward: [-525.99 -525.99 -525.99] [123.5275], Avg: [-612.166 -612.166 -612.166] (1.000)
Step: 28549, Reward: [-479.516 -479.516 -479.516] [87.2880], Avg: [-612.086 -612.086 -612.086] (1.000)
Step: 28599, Reward: [-491.292 -491.292 -491.292] [56.1219], Avg: [-611.973 -611.973 -611.973] (1.000)
Step: 28649, Reward: [-476.705 -476.705 -476.705] [84.8958], Avg: [-611.885 -611.885 -611.885] (1.000)
Step: 28699, Reward: [-509.526 -509.526 -509.526] [95.0602], Avg: [-611.872 -611.872 -611.872] (1.000)
Step: 28749, Reward: [-451.451 -451.451 -451.451] [58.2677], Avg: [-611.695 -611.695 -611.695] (1.000)
Step: 28799, Reward: [-461.091 -461.091 -461.091] [34.8662], Avg: [-611.494 -611.494 -611.494] (1.000)
Step: 28849, Reward: [-422.761 -422.761 -422.761] [48.3493], Avg: [-611.251 -611.251 -611.251] (1.000)
Step: 28899, Reward: [-508.97 -508.97 -508.97] [62.2828], Avg: [-611.181 -611.181 -611.181] (1.000)
Step: 28949, Reward: [-426.91 -426.91 -426.91] [50.1970], Avg: [-610.95 -610.95 -610.95] (1.000)
Step: 28999, Reward: [-426.954 -426.954 -426.954] [86.1111], Avg: [-610.781 -610.781 -610.781] (1.000)
Step: 29049, Reward: [-487.217 -487.217 -487.217] [110.2718], Avg: [-610.758 -610.758 -610.758] (1.000)
Step: 29099, Reward: [-417.042 -417.042 -417.042] [31.8685], Avg: [-610.48 -610.48 -610.48] (1.000)
Step: 29149, Reward: [-474.791 -474.791 -474.791] [80.1658], Avg: [-610.385 -610.385 -610.385] (1.000)
Step: 29199, Reward: [-455.397 -455.397 -455.397] [30.1865], Avg: [-610.171 -610.171 -610.171] (1.000)
Step: 29249, Reward: [-494.525 -494.525 -494.525] [62.1515], Avg: [-610.08 -610.08 -610.08] (1.000)
Step: 29299, Reward: [-457.994 -457.994 -457.994] [54.5418], Avg: [-609.913 -609.913 -609.913] (1.000)
Step: 29349, Reward: [-410.72 -410.72 -410.72] [46.9668], Avg: [-609.654 -609.654 -609.654] (1.000)
Step: 29399, Reward: [-455.017 -455.017 -455.017] [76.1592], Avg: [-609.52 -609.52 -609.52] (1.000)
Step: 29449, Reward: [-447.182 -447.182 -447.182] [77.2757], Avg: [-609.376 -609.376 -609.376] (1.000)
Step: 29499, Reward: [-484.155 -484.155 -484.155] [73.6301], Avg: [-609.289 -609.289 -609.289] (1.000)
Step: 29549, Reward: [-450.605 -450.605 -450.605] [48.8435], Avg: [-609.103 -609.103 -609.103] (1.000)
Step: 29599, Reward: [-427.046 -427.046 -427.046] [48.6138], Avg: [-608.877 -608.877 -608.877] (1.000)
Step: 29649, Reward: [-499.697 -499.697 -499.697] [71.6962], Avg: [-608.814 -608.814 -608.814] (1.000)
Step: 29699, Reward: [-550.138 -550.138 -550.138] [133.3487], Avg: [-608.94 -608.94 -608.94] (1.000)
Step: 29749, Reward: [-483.225 -483.225 -483.225] [52.6742], Avg: [-608.817 -608.817 -608.817] (1.000)
Step: 29799, Reward: [-492.475 -492.475 -492.475] [107.6338], Avg: [-608.802 -608.802 -608.802] (1.000)
Step: 29849, Reward: [-564.132 -564.132 -564.132] [96.9498], Avg: [-608.89 -608.89 -608.89] (1.000)
Step: 29899, Reward: [-497.184 -497.184 -497.184] [58.5559], Avg: [-608.801 -608.801 -608.801] (1.000)
Step: 29949, Reward: [-480.232 -480.232 -480.232] [81.7637], Avg: [-608.723 -608.723 -608.723] (1.000)
Step: 29999, Reward: [-500.16 -500.16 -500.16] [158.9107], Avg: [-608.807 -608.807 -608.807] (1.000)
Step: 30049, Reward: [-672.62 -672.62 -672.62] [182.8565], Avg: [-609.217 -609.217 -609.217] (1.000)
Step: 30099, Reward: [-454.904 -454.904 -454.904] [72.3436], Avg: [-609.081 -609.081 -609.081] (1.000)
Step: 30149, Reward: [-578.957 -578.957 -578.957] [108.6352], Avg: [-609.211 -609.211 -609.211] (1.000)
Step: 30199, Reward: [-441.629 -441.629 -441.629] [118.8161], Avg: [-609.131 -609.131 -609.131] (1.000)
Step: 30249, Reward: [-514.163 -514.163 -514.163] [141.4853], Avg: [-609.208 -609.208 -609.208] (1.000)
Step: 30299, Reward: [-548.675 -548.675 -548.675] [95.8666], Avg: [-609.266 -609.266 -609.266] (1.000)
Step: 30349, Reward: [-593.085 -593.085 -593.085] [94.5850], Avg: [-609.395 -609.395 -609.395] (1.000)
Step: 30399, Reward: [-584.519 -584.519 -584.519] [111.1432], Avg: [-609.537 -609.537 -609.537] (1.000)
Step: 30449, Reward: [-518.667 -518.667 -518.667] [21.1849], Avg: [-609.423 -609.423 -609.423] (1.000)
Step: 30499, Reward: [-552.833 -552.833 -552.833] [82.4825], Avg: [-609.465 -609.465 -609.465] (1.000)
Step: 30549, Reward: [-553.915 -553.915 -553.915] [55.3799], Avg: [-609.465 -609.465 -609.465] (1.000)
Step: 30599, Reward: [-543.423 -543.423 -543.423] [82.6838], Avg: [-609.492 -609.492 -609.492] (1.000)
Step: 30649, Reward: [-472.11 -472.11 -472.11] [48.3083], Avg: [-609.347 -609.347 -609.347] (1.000)
Step: 30699, Reward: [-543.456 -543.456 -543.456] [73.0529], Avg: [-609.358 -609.358 -609.358] (1.000)
Step: 30749, Reward: [-705.19 -705.19 -705.19] [246.6108], Avg: [-609.915 -609.915 -609.915] (1.000)
Step: 30799, Reward: [-541.957 -541.957 -541.957] [186.2907], Avg: [-610.107 -610.107 -610.107] (1.000)
Step: 30849, Reward: [-510.063 -510.063 -510.063] [68.5749], Avg: [-610.056 -610.056 -610.056] (1.000)
Step: 30899, Reward: [-491.136 -491.136 -491.136] [67.7012], Avg: [-609.973 -609.973 -609.973] (1.000)
Step: 30949, Reward: [-531.234 -531.234 -531.234] [140.1964], Avg: [-610.073 -610.073 -610.073] (1.000)
Step: 30999, Reward: [-545.147 -545.147 -545.147] [182.4273], Avg: [-610.262 -610.262 -610.262] (1.000)
Step: 31049, Reward: [-505.849 -505.849 -505.849] [105.1616], Avg: [-610.263 -610.263 -610.263] (1.000)
Step: 31099, Reward: [-554.976 -554.976 -554.976] [94.2343], Avg: [-610.326 -610.326 -610.326] (1.000)
Step: 31149, Reward: [-643.747 -643.747 -643.747] [116.7360], Avg: [-610.567 -610.567 -610.567] (1.000)
Step: 31199, Reward: [-481.107 -481.107 -481.107] [64.1658], Avg: [-610.462 -610.462 -610.462] (1.000)
Step: 31249, Reward: [-576.001 -576.001 -576.001] [74.9155], Avg: [-610.527 -610.527 -610.527] (1.000)
Step: 31299, Reward: [-593.727 -593.727 -593.727] [81.8212], Avg: [-610.631 -610.631 -610.631] (1.000)
Step: 31349, Reward: [-453.185 -453.185 -453.185] [82.7496], Avg: [-610.512 -610.512 -610.512] (1.000)
Step: 31399, Reward: [-580.353 -580.353 -580.353] [144.5702], Avg: [-610.694 -610.694 -610.694] (1.000)
Step: 31449, Reward: [-498.528 -498.528 -498.528] [82.8429], Avg: [-610.647 -610.647 -610.647] (1.000)
Step: 31499, Reward: [-527.692 -527.692 -527.692] [140.7436], Avg: [-610.739 -610.739 -610.739] (1.000)
Step: 31549, Reward: [-566.177 -566.177 -566.177] [126.7644], Avg: [-610.869 -610.869 -610.869] (1.000)
Step: 31599, Reward: [-516.122 -516.122 -516.122] [58.5363], Avg: [-610.812 -610.812 -610.812] (1.000)
Step: 31649, Reward: [-585.985 -585.985 -585.985] [82.1244], Avg: [-610.903 -610.903 -610.903] (1.000)
Step: 31699, Reward: [-535.166 -535.166 -535.166] [23.8538], Avg: [-610.821 -610.821 -610.821] (1.000)
Step: 31749, Reward: [-539.978 -539.978 -539.978] [77.3900], Avg: [-610.831 -610.831 -610.831] (1.000)
Step: 31799, Reward: [-512.587 -512.587 -512.587] [70.9899], Avg: [-610.788 -610.788 -610.788] (1.000)
Step: 31849, Reward: [-584.689 -584.689 -584.689] [120.1466], Avg: [-610.936 -610.936 -610.936] (1.000)
Step: 31899, Reward: [-518.868 -518.868 -518.868] [38.5953], Avg: [-610.852 -610.852 -610.852] (1.000)
Step: 31949, Reward: [-562.048 -562.048 -562.048] [118.3516], Avg: [-610.961 -610.961 -610.961] (1.000)
Step: 31999, Reward: [-634.974 -634.974 -634.974] [102.1745], Avg: [-611.158 -611.158 -611.158] (1.000)
Step: 32049, Reward: [-542.964 -542.964 -542.964] [95.9268], Avg: [-611.201 -611.201 -611.201] (1.000)
Step: 32099, Reward: [-452.532 -452.532 -452.532] [79.8411], Avg: [-611.078 -611.078 -611.078] (1.000)
Step: 32149, Reward: [-547.839 -547.839 -547.839] [66.9967], Avg: [-611.084 -611.084 -611.084] (1.000)
Step: 32199, Reward: [-604.055 -604.055 -604.055] [57.9027], Avg: [-611.163 -611.163 -611.163] (1.000)
Step: 32249, Reward: [-517.687 -517.687 -517.687] [133.6585], Avg: [-611.226 -611.226 -611.226] (1.000)
Step: 32299, Reward: [-548.886 -548.886 -548.886] [94.8674], Avg: [-611.276 -611.276 -611.276] (1.000)
Step: 32349, Reward: [-539.974 -539.974 -539.974] [114.7709], Avg: [-611.343 -611.343 -611.343] (1.000)
Step: 32399, Reward: [-479.005 -479.005 -479.005] [116.9043], Avg: [-611.319 -611.319 -611.319] (1.000)
Step: 32449, Reward: [-537.363 -537.363 -537.363] [110.8688], Avg: [-611.376 -611.376 -611.376] (1.000)
Step: 32499, Reward: [-549.896 -549.896 -549.896] [86.8739], Avg: [-611.415 -611.415 -611.415] (1.000)
Step: 32549, Reward: [-572.726 -572.726 -572.726] [99.3506], Avg: [-611.508 -611.508 -611.508] (1.000)
Step: 32599, Reward: [-566.692 -566.692 -566.692] [82.9821], Avg: [-611.567 -611.567 -611.567] (1.000)
Step: 32649, Reward: [-611.879 -611.879 -611.879] [172.7745], Avg: [-611.832 -611.832 -611.832] (1.000)
Step: 32699, Reward: [-685.066 -685.066 -685.066] [209.0572], Avg: [-612.264 -612.264 -612.264] (1.000)
Step: 32749, Reward: [-523.801 -523.801 -523.801] [120.9587], Avg: [-612.313 -612.313 -612.313] (1.000)
Step: 32799, Reward: [-641.619 -641.619 -641.619] [99.3901], Avg: [-612.509 -612.509 -612.509] (1.000)
Step: 32849, Reward: [-545.526 -545.526 -545.526] [56.7463], Avg: [-612.494 -612.494 -612.494] (1.000)
Step: 32899, Reward: [-497.948 -497.948 -497.948] [136.5811], Avg: [-612.527 -612.527 -612.527] (1.000)
Step: 32949, Reward: [-522.285 -522.285 -522.285] [99.9446], Avg: [-612.542 -612.542 -612.542] (1.000)
Step: 32999, Reward: [-560.516 -560.516 -560.516] [89.1146], Avg: [-612.598 -612.598 -612.598] (1.000)
Step: 33049, Reward: [-568.682 -568.682 -568.682] [94.6755], Avg: [-612.675 -612.675 -612.675] (1.000)
Step: 33099, Reward: [-482.507 -482.507 -482.507] [73.8177], Avg: [-612.59 -612.59 -612.59] (1.000)
Step: 33149, Reward: [-543.114 -543.114 -543.114] [100.5632], Avg: [-612.637 -612.637 -612.637] (1.000)
Step: 33199, Reward: [-521.901 -521.901 -521.901] [33.2775], Avg: [-612.55 -612.55 -612.55] (1.000)
Step: 33249, Reward: [-563.179 -563.179 -563.179] [55.9533], Avg: [-612.56 -612.56 -612.56] (1.000)
Step: 33299, Reward: [-426.197 -426.197 -426.197] [66.5020], Avg: [-612.38 -612.38 -612.38] (1.000)
Step: 33349, Reward: [-455.567 -455.567 -455.567] [86.3932], Avg: [-612.275 -612.275 -612.275] (1.000)
Step: 33399, Reward: [-556.352 -556.352 -556.352] [103.5003], Avg: [-612.346 -612.346 -612.346] (1.000)
Step: 33449, Reward: [-479.993 -479.993 -479.993] [100.6881], Avg: [-612.299 -612.299 -612.299] (1.000)
Step: 33499, Reward: [-519.404 -519.404 -519.404] [78.6520], Avg: [-612.277 -612.277 -612.277] (1.000)
Step: 33549, Reward: [-540.469 -540.469 -540.469] [108.6226], Avg: [-612.332 -612.332 -612.332] (1.000)
Step: 33599, Reward: [-542.858 -542.858 -542.858] [173.1972], Avg: [-612.487 -612.487 -612.487] (1.000)
Step: 33649, Reward: [-472.806 -472.806 -472.806] [73.2648], Avg: [-612.388 -612.388 -612.388] (1.000)
Step: 33699, Reward: [-461.95 -461.95 -461.95] [118.5762], Avg: [-612.341 -612.341 -612.341] (1.000)
Step: 33749, Reward: [-522.516 -522.516 -522.516] [61.9105], Avg: [-612.299 -612.299 -612.299] (1.000)
Step: 33799, Reward: [-608.94 -608.94 -608.94] [149.0883], Avg: [-612.515 -612.515 -612.515] (1.000)
Step: 33849, Reward: [-493.135 -493.135 -493.135] [64.2297], Avg: [-612.433 -612.433 -612.433] (1.000)
Step: 33899, Reward: [-546.859 -546.859 -546.859] [126.3661], Avg: [-612.523 -612.523 -612.523] (1.000)
Step: 33949, Reward: [-579.122 -579.122 -579.122] [115.6045], Avg: [-612.644 -612.644 -612.644] (1.000)
Step: 33999, Reward: [-639.828 -639.828 -639.828] [153.4255], Avg: [-612.91 -612.91 -612.91] (1.000)
Step: 34049, Reward: [-728.692 -728.692 -728.692] [164.1081], Avg: [-613.321 -613.321 -613.321] (1.000)
Step: 34099, Reward: [-523.279 -523.279 -523.279] [70.9234], Avg: [-613.293 -613.293 -613.293] (1.000)
Step: 34149, Reward: [-537.566 -537.566 -537.566] [117.8044], Avg: [-613.354 -613.354 -613.354] (1.000)
Step: 34199, Reward: [-495.065 -495.065 -495.065] [108.7305], Avg: [-613.34 -613.34 -613.34] (1.000)
Step: 34249, Reward: [-555.715 -555.715 -555.715] [59.5235], Avg: [-613.343 -613.343 -613.343] (1.000)
Step: 34299, Reward: [-544.76 -544.76 -544.76] [62.7343], Avg: [-613.334 -613.334 -613.334] (1.000)
Step: 34349, Reward: [-555.912 -555.912 -555.912] [97.3331], Avg: [-613.393 -613.393 -613.393] (1.000)
Step: 34399, Reward: [-474.209 -474.209 -474.209] [113.8780], Avg: [-613.356 -613.356 -613.356] (1.000)
Step: 34449, Reward: [-509.616 -509.616 -509.616] [97.1686], Avg: [-613.346 -613.346 -613.346] (1.000)
Step: 34499, Reward: [-569.84 -569.84 -569.84] [85.2704], Avg: [-613.407 -613.407 -613.407] (1.000)
Step: 34549, Reward: [-553.447 -553.447 -553.447] [103.8741], Avg: [-613.47 -613.47 -613.47] (1.000)
Step: 34599, Reward: [-563.504 -563.504 -563.504] [111.4838], Avg: [-613.559 -613.559 -613.559] (1.000)
Step: 34649, Reward: [-578.608 -578.608 -578.608] [134.5750], Avg: [-613.703 -613.703 -613.703] (1.000)
Step: 34699, Reward: [-584.546 -584.546 -584.546] [112.0843], Avg: [-613.822 -613.822 -613.822] (1.000)
Step: 34749, Reward: [-447.6 -447.6 -447.6] [48.1417], Avg: [-613.653 -613.653 -613.653] (1.000)
Step: 34799, Reward: [-552.325 -552.325 -552.325] [137.2552], Avg: [-613.762 -613.762 -613.762] (1.000)
Step: 34849, Reward: [-476.585 -476.585 -476.585] [93.2113], Avg: [-613.699 -613.699 -613.699] (1.000)
Step: 34899, Reward: [-511.199 -511.199 -511.199] [82.3513], Avg: [-613.67 -613.67 -613.67] (1.000)
Step: 34949, Reward: [-445.477 -445.477 -445.477] [66.5005], Avg: [-613.524 -613.524 -613.524] (1.000)
Step: 34999, Reward: [-583.155 -583.155 -583.155] [83.2862], Avg: [-613.6 -613.6 -613.6] (1.000)
Step: 35049, Reward: [-580.458 -580.458 -580.458] [66.4609], Avg: [-613.647 -613.647 -613.647] (1.000)
Step: 35099, Reward: [-505.192 -505.192 -505.192] [68.3658], Avg: [-613.59 -613.59 -613.59] (1.000)
Step: 35149, Reward: [-475.985 -475.985 -475.985] [121.6534], Avg: [-613.568 -613.568 -613.568] (1.000)
Step: 35199, Reward: [-608.227 -608.227 -608.227] [99.4910], Avg: [-613.701 -613.701 -613.701] (1.000)
Step: 35249, Reward: [-584.964 -584.964 -584.964] [66.7262], Avg: [-613.755 -613.755 -613.755] (1.000)
Step: 35299, Reward: [-577.689 -577.689 -577.689] [118.4439], Avg: [-613.872 -613.872 -613.872] (1.000)
Step: 35349, Reward: [-592.225 -592.225 -592.225] [152.3977], Avg: [-614.057 -614.057 -614.057] (1.000)
Step: 35399, Reward: [-568.305 -568.305 -568.305] [38.5802], Avg: [-614.047 -614.047 -614.047] (1.000)
Step: 35449, Reward: [-498.668 -498.668 -498.668] [67.4159], Avg: [-613.979 -613.979 -613.979] (1.000)
Step: 35499, Reward: [-541.832 -541.832 -541.832] [182.4636], Avg: [-614.134 -614.134 -614.134] (1.000)
Step: 35549, Reward: [-582.055 -582.055 -582.055] [149.5901], Avg: [-614.3 -614.3 -614.3] (1.000)
Step: 35599, Reward: [-456.896 -456.896 -456.896] [53.8189], Avg: [-614.154 -614.154 -614.154] (1.000)
Step: 35649, Reward: [-619.96 -619.96 -619.96] [122.5362], Avg: [-614.334 -614.334 -614.334] (1.000)
Step: 35699, Reward: [-456.685 -456.685 -456.685] [45.5494], Avg: [-614.177 -614.177 -614.177] (1.000)
Step: 35749, Reward: [-528.474 -528.474 -528.474] [54.2697], Avg: [-614.133 -614.133 -614.133] (1.000)
Step: 35799, Reward: [-612.11 -612.11 -612.11] [151.9097], Avg: [-614.343 -614.343 -614.343] (1.000)
Step: 35849, Reward: [-536.133 -536.133 -536.133] [43.9895], Avg: [-614.295 -614.295 -614.295] (1.000)
Step: 35899, Reward: [-536.099 -536.099 -536.099] [86.6415], Avg: [-614.307 -614.307 -614.307] (1.000)
Step: 35949, Reward: [-573.737 -573.737 -573.737] [112.8488], Avg: [-614.407 -614.407 -614.407] (1.000)
Step: 35999, Reward: [-461.082 -461.082 -461.082] [80.3194], Avg: [-614.306 -614.306 -614.306] (1.000)
Step: 36049, Reward: [-546.996 -546.996 -546.996] [95.6448], Avg: [-614.345 -614.345 -614.345] (1.000)
Step: 36099, Reward: [-479.472 -479.472 -479.472] [105.6260], Avg: [-614.305 -614.305 -614.305] (1.000)
Step: 36149, Reward: [-505.93 -505.93 -505.93] [53.7784], Avg: [-614.229 -614.229 -614.229] (1.000)
Step: 36199, Reward: [-530.544 -530.544 -530.544] [59.1453], Avg: [-614.195 -614.195 -614.195] (1.000)
Step: 36249, Reward: [-494.77 -494.77 -494.77] [78.3170], Avg: [-614.138 -614.138 -614.138] (1.000)
Step: 36299, Reward: [-486.945 -486.945 -486.945] [49.6743], Avg: [-614.032 -614.032 -614.032] (1.000)
Step: 36349, Reward: [-445.365 -445.365 -445.365] [88.8901], Avg: [-613.922 -613.922 -613.922] (1.000)
Step: 36399, Reward: [-542.219 -542.219 -542.219] [120.4230], Avg: [-613.989 -613.989 -613.989] (1.000)
Step: 36449, Reward: [-403.375 -403.375 -403.375] [45.7574], Avg: [-613.763 -613.763 -613.763] (1.000)
Step: 36499, Reward: [-487.627 -487.627 -487.627] [59.7008], Avg: [-613.672 -613.672 -613.672] (1.000)
Step: 36549, Reward: [-592.528 -592.528 -592.528] [45.6274], Avg: [-613.705 -613.705 -613.705] (1.000)
Step: 36599, Reward: [-606.528 -606.528 -606.528] [51.0754], Avg: [-613.765 -613.765 -613.765] (1.000)
Step: 36649, Reward: [-452.905 -452.905 -452.905] [29.6711], Avg: [-613.586 -613.586 -613.586] (1.000)
Step: 36699, Reward: [-546.366 -546.366 -546.366] [80.4859], Avg: [-613.604 -613.604 -613.604] (1.000)
Step: 36749, Reward: [-469.228 -469.228 -469.228] [80.9956], Avg: [-613.518 -613.518 -613.518] (1.000)
Step: 36799, Reward: [-518.147 -518.147 -518.147] [53.6745], Avg: [-613.461 -613.461 -613.461] (1.000)
Step: 36849, Reward: [-492.081 -492.081 -492.081] [75.5314], Avg: [-613.399 -613.399 -613.399] (1.000)
Step: 36899, Reward: [-510.339 -510.339 -510.339] [109.3224], Avg: [-613.408 -613.408 -613.408] (1.000)
Step: 36949, Reward: [-600.305 -600.305 -600.305] [181.1498], Avg: [-613.635 -613.635 -613.635] (1.000)
Step: 36999, Reward: [-456.745 -456.745 -456.745] [34.2094], Avg: [-613.469 -613.469 -613.469] (1.000)
Step: 37049, Reward: [-566.874 -566.874 -566.874] [236.4450], Avg: [-613.725 -613.725 -613.725] (1.000)
Step: 37099, Reward: [-544.373 -544.373 -544.373] [48.2095], Avg: [-613.697 -613.697 -613.697] (1.000)
Step: 37149, Reward: [-619.381 -619.381 -619.381] [72.9408], Avg: [-613.803 -613.803 -613.803] (1.000)
Step: 37199, Reward: [-527.366 -527.366 -527.366] [84.6290], Avg: [-613.8 -613.8 -613.8] (1.000)
Step: 37249, Reward: [-501.962 -501.962 -501.962] [112.2845], Avg: [-613.801 -613.801 -613.801] (1.000)
Step: 37299, Reward: [-501.674 -501.674 -501.674] [66.3742], Avg: [-613.74 -613.74 -613.74] (1.000)
Step: 37349, Reward: [-499.481 -499.481 -499.481] [150.9576], Avg: [-613.789 -613.789 -613.789] (1.000)
Step: 37399, Reward: [-500.469 -500.469 -500.469] [81.3901], Avg: [-613.746 -613.746 -613.746] (1.000)
Step: 37449, Reward: [-583.15 -583.15 -583.15] [115.1337], Avg: [-613.859 -613.859 -613.859] (1.000)
Step: 37499, Reward: [-573.671 -573.671 -573.671] [106.6569], Avg: [-613.948 -613.948 -613.948] (1.000)
Step: 37549, Reward: [-481.852 -481.852 -481.852] [134.2869], Avg: [-613.95 -613.95 -613.95] (1.000)
Step: 37599, Reward: [-503.774 -503.774 -503.774] [118.5906], Avg: [-613.962 -613.962 -613.962] (1.000)
Step: 37649, Reward: [-487.938 -487.938 -487.938] [64.2769], Avg: [-613.88 -613.88 -613.88] (1.000)
Step: 37699, Reward: [-408.266 -408.266 -408.266] [27.0427], Avg: [-613.643 -613.643 -613.643] (1.000)
Step: 37749, Reward: [-522.135 -522.135 -522.135] [41.1235], Avg: [-613.576 -613.576 -613.576] (1.000)
Step: 37799, Reward: [-557.45 -557.45 -557.45] [166.5344], Avg: [-613.722 -613.722 -613.722] (1.000)
Step: 37849, Reward: [-560.866 -560.866 -560.866] [93.2428], Avg: [-613.776 -613.776 -613.776] (1.000)
Step: 37899, Reward: [-535.28 -535.28 -535.28] [69.3307], Avg: [-613.763 -613.763 -613.763] (1.000)
Step: 37949, Reward: [-533.286 -533.286 -533.286] [185.2943], Avg: [-613.902 -613.902 -613.902] (1.000)
Step: 37999, Reward: [-557.801 -557.801 -557.801] [117.1495], Avg: [-613.982 -613.982 -613.982] (1.000)
Step: 38049, Reward: [-592.484 -592.484 -592.484] [96.8554], Avg: [-614.081 -614.081 -614.081] (1.000)
Step: 38099, Reward: [-544.914 -544.914 -544.914] [187.2059], Avg: [-614.236 -614.236 -614.236] (1.000)
Step: 38149, Reward: [-604.469 -604.469 -604.469] [215.6228], Avg: [-614.506 -614.506 -614.506] (1.000)
Step: 38199, Reward: [-542.746 -542.746 -542.746] [30.8906], Avg: [-614.452 -614.452 -614.452] (1.000)
Step: 38249, Reward: [-612.696 -612.696 -612.696] [86.0571], Avg: [-614.562 -614.562 -614.562] (1.000)
Step: 38299, Reward: [-577.921 -577.921 -577.921] [137.2238], Avg: [-614.694 -614.694 -614.694] (1.000)
Step: 38349, Reward: [-529.876 -529.876 -529.876] [67.3734], Avg: [-614.671 -614.671 -614.671] (1.000)
Step: 38399, Reward: [-533.107 -533.107 -533.107] [89.4154], Avg: [-614.681 -614.681 -614.681] (1.000)
Step: 38449, Reward: [-536.894 -536.894 -536.894] [74.6071], Avg: [-614.677 -614.677 -614.677] (1.000)
Step: 38499, Reward: [-508.93 -508.93 -508.93] [92.1253], Avg: [-614.659 -614.659 -614.659] (1.000)
Step: 38549, Reward: [-551.56 -551.56 -551.56] [69.9179], Avg: [-614.668 -614.668 -614.668] (1.000)
Step: 38599, Reward: [-435.981 -435.981 -435.981] [88.8282], Avg: [-614.552 -614.552 -614.552] (1.000)
Step: 38649, Reward: [-478.561 -478.561 -478.561] [132.6789], Avg: [-614.547 -614.547 -614.547] (1.000)
Step: 38699, Reward: [-588.143 -588.143 -588.143] [89.0914], Avg: [-614.628 -614.628 -614.628] (1.000)
Step: 38749, Reward: [-505.707 -505.707 -505.707] [81.1807], Avg: [-614.593 -614.593 -614.593] (1.000)
Step: 38799, Reward: [-430.396 -430.396 -430.396] [34.5460], Avg: [-614.4 -614.4 -614.4] (1.000)
Step: 38849, Reward: [-428.464 -428.464 -428.464] [62.9338], Avg: [-614.241 -614.241 -614.241] (1.000)
Step: 38899, Reward: [-556.73 -556.73 -556.73] [57.0704], Avg: [-614.241 -614.241 -614.241] (1.000)
Step: 38949, Reward: [-642.267 -642.267 -642.267] [162.6154], Avg: [-614.486 -614.486 -614.486] (1.000)
Step: 38999, Reward: [-507.974 -507.974 -507.974] [79.2017], Avg: [-614.451 -614.451 -614.451] (1.000)
Step: 39049, Reward: [-462.929 -462.929 -462.929] [95.0092], Avg: [-614.378 -614.378 -614.378] (1.000)
Step: 39099, Reward: [-533.739 -533.739 -533.739] [99.6635], Avg: [-614.403 -614.403 -614.403] (1.000)
Step: 39149, Reward: [-492.26 -492.26 -492.26] [124.8094], Avg: [-614.406 -614.406 -614.406] (1.000)
Step: 39199, Reward: [-477.926 -477.926 -477.926] [77.6913], Avg: [-614.331 -614.331 -614.331] (1.000)
Step: 39249, Reward: [-572.914 -572.914 -572.914] [77.2476], Avg: [-614.377 -614.377 -614.377] (1.000)
Step: 39299, Reward: [-408.34 -408.34 -408.34] [39.2245], Avg: [-614.164 -614.164 -614.164] (1.000)
Step: 39349, Reward: [-461.72 -461.72 -461.72] [65.3846], Avg: [-614.054 -614.054 -614.054] (1.000)
Step: 39399, Reward: [-451.693 -451.693 -451.693] [76.7676], Avg: [-613.945 -613.945 -613.945] (1.000)
Step: 39449, Reward: [-458.909 -458.909 -458.909] [41.9674], Avg: [-613.802 -613.802 -613.802] (1.000)
Step: 39499, Reward: [-424.005 -424.005 -424.005] [70.8652], Avg: [-613.651 -613.651 -613.651] (1.000)
Step: 39549, Reward: [-470.726 -470.726 -470.726] [81.9246], Avg: [-613.574 -613.574 -613.574] (1.000)
Step: 39599, Reward: [-456.575 -456.575 -456.575] [99.7682], Avg: [-613.502 -613.502 -613.502] (1.000)
Step: 39649, Reward: [-485.263 -485.263 -485.263] [68.1874], Avg: [-613.426 -613.426 -613.426] (1.000)
Step: 39699, Reward: [-491.295 -491.295 -491.295] [60.7060], Avg: [-613.349 -613.349 -613.349] (1.000)
Step: 39749, Reward: [-482.979 -482.979 -482.979] [79.8878], Avg: [-613.285 -613.285 -613.285] (1.000)
Step: 39799, Reward: [-521.688 -521.688 -521.688] [114.1666], Avg: [-613.314 -613.314 -613.314] (1.000)
Step: 39849, Reward: [-476.198 -476.198 -476.198] [68.7126], Avg: [-613.228 -613.228 -613.228] (1.000)
Step: 39899, Reward: [-495.225 -495.225 -495.225] [72.1855], Avg: [-613.17 -613.17 -613.17] (1.000)
Step: 39949, Reward: [-484.839 -484.839 -484.839] [78.3680], Avg: [-613.108 -613.108 -613.108] (1.000)
Step: 39999, Reward: [-499.654 -499.654 -499.654] [89.7819], Avg: [-613.078 -613.078 -613.078] (1.000)
Step: 40049, Reward: [-406.721 -406.721 -406.721] [24.4912], Avg: [-612.851 -612.851 -612.851] (1.000)
Step: 40099, Reward: [-534.638 -534.638 -534.638] [22.3985], Avg: [-612.782 -612.782 -612.782] (1.000)
Step: 40149, Reward: [-541.771 -541.771 -541.771] [109.7667], Avg: [-612.83 -612.83 -612.83] (1.000)
Step: 40199, Reward: [-441.342 -441.342 -441.342] [28.7678], Avg: [-612.652 -612.652 -612.652] (1.000)
Step: 40249, Reward: [-512.891 -512.891 -512.891] [82.5330], Avg: [-612.631 -612.631 -612.631] (1.000)
Step: 40299, Reward: [-617.518 -617.518 -617.518] [110.1851], Avg: [-612.774 -612.774 -612.774] (1.000)
Step: 40349, Reward: [-524.884 -524.884 -524.884] [66.0725], Avg: [-612.747 -612.747 -612.747] (1.000)
Step: 40399, Reward: [-498.139 -498.139 -498.139] [66.6767], Avg: [-612.687 -612.687 -612.687] (1.000)
Step: 40449, Reward: [-513.304 -513.304 -513.304] [89.5822], Avg: [-612.675 -612.675 -612.675] (1.000)
Step: 40499, Reward: [-662.844 -662.844 -662.844] [98.1073], Avg: [-612.858 -612.858 -612.858] (1.000)
Step: 40549, Reward: [-471.101 -471.101 -471.101] [55.6263], Avg: [-612.752 -612.752 -612.752] (1.000)
Step: 40599, Reward: [-641.392 -641.392 -641.392] [91.9520], Avg: [-612.901 -612.901 -612.901] (1.000)
Step: 40649, Reward: [-462.125 -462.125 -462.125] [41.2989], Avg: [-612.766 -612.766 -612.766] (1.000)
Step: 40699, Reward: [-584.082 -584.082 -584.082] [130.0587], Avg: [-612.891 -612.891 -612.891] (1.000)
Step: 40749, Reward: [-560.014 -560.014 -560.014] [132.9353], Avg: [-612.989 -612.989 -612.989] (1.000)
Step: 40799, Reward: [-543.045 -543.045 -543.045] [87.8939], Avg: [-613.011 -613.011 -613.011] (1.000)
Step: 40849, Reward: [-530.815 -530.815 -530.815] [125.0061], Avg: [-613.063 -613.063 -613.063] (1.000)
Step: 40899, Reward: [-521.084 -521.084 -521.084] [82.6306], Avg: [-613.052 -613.052 -613.052] (1.000)
Step: 40949, Reward: [-551.88 -551.88 -551.88] [100.6923], Avg: [-613.1 -613.1 -613.1] (1.000)
Step: 40999, Reward: [-473.438 -473.438 -473.438] [84.1517], Avg: [-613.032 -613.032 -613.032] (1.000)
Step: 41049, Reward: [-528.757 -528.757 -528.757] [94.4031], Avg: [-613.045 -613.045 -613.045] (1.000)
Step: 41099, Reward: [-559.775 -559.775 -559.775] [75.5389], Avg: [-613.072 -613.072 -613.072] (1.000)
Step: 41149, Reward: [-503.351 -503.351 -503.351] [48.5084], Avg: [-612.997 -612.997 -612.997] (1.000)
Step: 41199, Reward: [-506.195 -506.195 -506.195] [77.7076], Avg: [-612.962 -612.962 -612.962] (1.000)
Step: 41249, Reward: [-595.332 -595.332 -595.332] [138.8424], Avg: [-613.109 -613.109 -613.109] (1.000)
Step: 41299, Reward: [-546.189 -546.189 -546.189] [111.7136], Avg: [-613.163 -613.163 -613.163] (1.000)
Step: 41349, Reward: [-483.876 -483.876 -483.876] [88.8674], Avg: [-613.114 -613.114 -613.114] (1.000)
Step: 41399, Reward: [-551.074 -551.074 -551.074] [143.1045], Avg: [-613.212 -613.212 -613.212] (1.000)
Step: 41449, Reward: [-456.782 -456.782 -456.782] [70.1646], Avg: [-613.108 -613.108 -613.108] (1.000)
Step: 41499, Reward: [-426.661 -426.661 -426.661] [49.6966], Avg: [-612.943 -612.943 -612.943] (1.000)
Step: 41549, Reward: [-426.454 -426.454 -426.454] [42.7493], Avg: [-612.77 -612.77 -612.77] (1.000)
Step: 41599, Reward: [-500.694 -500.694 -500.694] [104.3551], Avg: [-612.761 -612.761 -612.761] (1.000)
Step: 41649, Reward: [-495.537 -495.537 -495.537] [68.4803], Avg: [-612.703 -612.703 -612.703] (1.000)
Step: 41699, Reward: [-452.766 -452.766 -452.766] [142.2228], Avg: [-612.681 -612.681 -612.681] (1.000)
Step: 41749, Reward: [-494.946 -494.946 -494.946] [89.7908], Avg: [-612.648 -612.648 -612.648] (1.000)
Step: 41799, Reward: [-527.383 -527.383 -527.383] [119.5112], Avg: [-612.689 -612.689 -612.689] (1.000)
Step: 41849, Reward: [-532.665 -532.665 -532.665] [91.8013], Avg: [-612.703 -612.703 -612.703] (1.000)
Step: 41899, Reward: [-580.52 -580.52 -580.52] [75.0132], Avg: [-612.754 -612.754 -612.754] (1.000)
Step: 41949, Reward: [-561.223 -561.223 -561.223] [53.1844], Avg: [-612.756 -612.756 -612.756] (1.000)
Step: 41999, Reward: [-578.101 -578.101 -578.101] [154.7424], Avg: [-612.899 -612.899 -612.899] (1.000)
Step: 42049, Reward: [-558.733 -558.733 -558.733] [91.9574], Avg: [-612.944 -612.944 -612.944] (1.000)
Step: 42099, Reward: [-537.001 -537.001 -537.001] [73.7919], Avg: [-612.941 -612.941 -612.941] (1.000)
Step: 42149, Reward: [-570.979 -570.979 -570.979] [105.6938], Avg: [-613.017 -613.017 -613.017] (1.000)
Step: 42199, Reward: [-485.059 -485.059 -485.059] [115.3204], Avg: [-613.002 -613.002 -613.002] (1.000)
Step: 42249, Reward: [-568.222 -568.222 -568.222] [96.4868], Avg: [-613.063 -613.063 -613.063] (1.000)
Step: 42299, Reward: [-611.209 -611.209 -611.209] [60.0512], Avg: [-613.132 -613.132 -613.132] (1.000)
Step: 42349, Reward: [-622.033 -622.033 -622.033] [108.1882], Avg: [-613.27 -613.27 -613.27] (1.000)
Step: 42399, Reward: [-603.718 -603.718 -603.718] [132.4215], Avg: [-613.415 -613.415 -613.415] (1.000)
Step: 42449, Reward: [-495.846 -495.846 -495.846] [74.5806], Avg: [-613.365 -613.365 -613.365] (1.000)
Step: 42499, Reward: [-451.821 -451.821 -451.821] [90.2943], Avg: [-613.281 -613.281 -613.281] (1.000)
Step: 42549, Reward: [-562.007 -562.007 -562.007] [110.6243], Avg: [-613.35 -613.35 -613.35] (1.000)
Step: 42599, Reward: [-593.524 -593.524 -593.524] [116.2275], Avg: [-613.464 -613.464 -613.464] (1.000)
Step: 42649, Reward: [-486.984 -486.984 -486.984] [104.4213], Avg: [-613.438 -613.438 -613.438] (1.000)
Step: 42699, Reward: [-470.741 -470.741 -470.741] [18.7665], Avg: [-613.293 -613.293 -613.293] (1.000)
Step: 42749, Reward: [-587.557 -587.557 -587.557] [61.2323], Avg: [-613.334 -613.334 -613.334] (1.000)
Step: 42799, Reward: [-480.637 -480.637 -480.637] [79.8571], Avg: [-613.272 -613.272 -613.272] (1.000)
Step: 42849, Reward: [-619.507 -619.507 -619.507] [187.0250], Avg: [-613.498 -613.498 -613.498] (1.000)
Step: 42899, Reward: [-495.614 -495.614 -495.614] [139.4714], Avg: [-613.523 -613.523 -613.523] (1.000)
Step: 42949, Reward: [-457.472 -457.472 -457.472] [54.3768], Avg: [-613.405 -613.405 -613.405] (1.000)
Step: 42999, Reward: [-567.406 -567.406 -567.406] [72.4622], Avg: [-613.435 -613.435 -613.435] (1.000)
Step: 43049, Reward: [-614.736 -614.736 -614.736] [133.2423], Avg: [-613.592 -613.592 -613.592] (1.000)
Step: 43099, Reward: [-486.911 -486.911 -486.911] [82.8462], Avg: [-613.541 -613.541 -613.541] (1.000)
Step: 43149, Reward: [-697.663 -697.663 -697.663] [187.7032], Avg: [-613.856 -613.856 -613.856] (1.000)
Step: 43199, Reward: [-491.685 -491.685 -491.685] [91.5879], Avg: [-613.82 -613.82 -613.82] (1.000)
Step: 43249, Reward: [-541.406 -541.406 -541.406] [144.2183], Avg: [-613.903 -613.903 -613.903] (1.000)
Step: 43299, Reward: [-574.851 -574.851 -574.851] [115.3794], Avg: [-613.992 -613.992 -613.992] (1.000)
Step: 43349, Reward: [-532.401 -532.401 -532.401] [99.1512], Avg: [-614.012 -614.012 -614.012] (1.000)
Step: 43399, Reward: [-620.205 -620.205 -620.205] [72.9218], Avg: [-614.103 -614.103 -614.103] (1.000)
Step: 43449, Reward: [-495.262 -495.262 -495.262] [75.0661], Avg: [-614.053 -614.053 -614.053] (1.000)
Step: 43499, Reward: [-598.805 -598.805 -598.805] [100.5513], Avg: [-614.151 -614.151 -614.151] (1.000)
Step: 43549, Reward: [-534.634 -534.634 -534.634] [91.3955], Avg: [-614.164 -614.164 -614.164] (1.000)
Step: 43599, Reward: [-545.655 -545.655 -545.655] [90.6108], Avg: [-614.19 -614.19 -614.19] (1.000)
Step: 43649, Reward: [-590.445 -590.445 -590.445] [181.3489], Avg: [-614.37 -614.37 -614.37] (1.000)
Step: 43699, Reward: [-463.906 -463.906 -463.906] [37.6710], Avg: [-614.241 -614.241 -614.241] (1.000)
Step: 43749, Reward: [-499.323 -499.323 -499.323] [62.0575], Avg: [-614.181 -614.181 -614.181] (1.000)
Step: 43799, Reward: [-566.582 -566.582 -566.582] [153.6402], Avg: [-614.302 -614.302 -614.302] (1.000)
Step: 43849, Reward: [-552.659 -552.659 -552.659] [62.7311], Avg: [-614.303 -614.303 -614.303] (1.000)
Step: 43899, Reward: [-598.379 -598.379 -598.379] [146.3306], Avg: [-614.452 -614.452 -614.452] (1.000)
Step: 43949, Reward: [-507.611 -507.611 -507.611] [109.5911], Avg: [-614.455 -614.455 -614.455] (1.000)
Step: 43999, Reward: [-485.237 -485.237 -485.237] [68.9610], Avg: [-614.386 -614.386 -614.386] (1.000)
Step: 44049, Reward: [-573.038 -573.038 -573.038] [70.0215], Avg: [-614.419 -614.419 -614.419] (1.000)
Step: 44099, Reward: [-504.77 -504.77 -504.77] [67.3084], Avg: [-614.371 -614.371 -614.371] (1.000)
Step: 44149, Reward: [-518.194 -518.194 -518.194] [44.4611], Avg: [-614.312 -614.312 -614.312] (1.000)
Step: 44199, Reward: [-593.667 -593.667 -593.667] [179.0087], Avg: [-614.491 -614.491 -614.491] (1.000)
Step: 44249, Reward: [-501.066 -501.066 -501.066] [60.0524], Avg: [-614.431 -614.431 -614.431] (1.000)
Step: 44299, Reward: [-557.371 -557.371 -557.371] [104.7986], Avg: [-614.485 -614.485 -614.485] (1.000)
Step: 44349, Reward: [-594.058 -594.058 -594.058] [127.1191], Avg: [-614.605 -614.605 -614.605] (1.000)
Step: 44399, Reward: [-552.385 -552.385 -552.385] [106.8403], Avg: [-614.655 -614.655 -614.655] (1.000)
Step: 44449, Reward: [-587.339 -587.339 -587.339] [109.4485], Avg: [-614.748 -614.748 -614.748] (1.000)
Step: 44499, Reward: [-548.029 -548.029 -548.029] [141.1095], Avg: [-614.831 -614.831 -614.831] (1.000)
Step: 44549, Reward: [-524.397 -524.397 -524.397] [67.1928], Avg: [-614.805 -614.805 -614.805] (1.000)
Step: 44599, Reward: [-553.271 -553.271 -553.271] [64.6121], Avg: [-614.809 -614.809 -614.809] (1.000)
Step: 44649, Reward: [-456.483 -456.483 -456.483] [162.4353], Avg: [-614.813 -614.813 -614.813] (1.000)
Step: 44699, Reward: [-491.467 -491.467 -491.467] [118.8507], Avg: [-614.808 -614.808 -614.808] (1.000)
Step: 44749, Reward: [-546.413 -546.413 -546.413] [86.0736], Avg: [-614.828 -614.828 -614.828] (1.000)
Step: 44799, Reward: [-546.239 -546.239 -546.239] [44.9583], Avg: [-614.802 -614.802 -614.802] (1.000)
Step: 44849, Reward: [-556.903 -556.903 -556.903] [107.1247], Avg: [-614.857 -614.857 -614.857] (1.000)
Step: 44899, Reward: [-508.555 -508.555 -508.555] [176.9223], Avg: [-614.935 -614.935 -614.935] (1.000)
Step: 44949, Reward: [-563.451 -563.451 -563.451] [103.5393], Avg: [-614.993 -614.993 -614.993] (1.000)
Step: 44999, Reward: [-536.285 -536.285 -536.285] [132.8997], Avg: [-615.053 -615.053 -615.053] (1.000)
Step: 45049, Reward: [-574.564 -574.564 -574.564] [101.4359], Avg: [-615.121 -615.121 -615.121] (1.000)
Step: 45099, Reward: [-597.423 -597.423 -597.423] [97.0996], Avg: [-615.209 -615.209 -615.209] (1.000)
Step: 45149, Reward: [-583.546 -583.546 -583.546] [83.4137], Avg: [-615.266 -615.266 -615.266] (1.000)
Step: 45199, Reward: [-603.786 -603.786 -603.786] [149.9283], Avg: [-615.419 -615.419 -615.419] (1.000)
Step: 45249, Reward: [-593.087 -593.087 -593.087] [163.8533], Avg: [-615.576 -615.576 -615.576] (1.000)
Step: 45299, Reward: [-645.509 -645.509 -645.509] [86.9702], Avg: [-615.705 -615.705 -615.705] (1.000)
Step: 45349, Reward: [-638.871 -638.871 -638.871] [123.4183], Avg: [-615.867 -615.867 -615.867] (1.000)
Step: 45399, Reward: [-489.219 -489.219 -489.219] [100.6464], Avg: [-615.838 -615.838 -615.838] (1.000)
Step: 45449, Reward: [-526.555 -526.555 -526.555] [41.8021], Avg: [-615.786 -615.786 -615.786] (1.000)
Step: 45499, Reward: [-530.377 -530.377 -530.377] [29.6233], Avg: [-615.724 -615.724 -615.724] (1.000)
Step: 45549, Reward: [-496.232 -496.232 -496.232] [66.6845], Avg: [-615.666 -615.666 -615.666] (1.000)
Step: 45599, Reward: [-439.063 -439.063 -439.063] [68.8614], Avg: [-615.548 -615.548 -615.548] (1.000)
Step: 45649, Reward: [-441.276 -441.276 -441.276] [52.8515], Avg: [-615.415 -615.415 -615.415] (1.000)
Step: 45699, Reward: [-472.522 -472.522 -472.522] [141.4726], Avg: [-615.414 -615.414 -615.414] (1.000)
Step: 45749, Reward: [-480.426 -480.426 -480.426] [62.6846], Avg: [-615.335 -615.335 -615.335] (1.000)
Step: 45799, Reward: [-452.041 -452.041 -452.041] [33.9603], Avg: [-615.193 -615.193 -615.193] (1.000)
Step: 45849, Reward: [-446.526 -446.526 -446.526] [70.4932], Avg: [-615.086 -615.086 -615.086] (1.000)
Step: 45899, Reward: [-426.122 -426.122 -426.122] [43.4216], Avg: [-614.928 -614.928 -614.928] (1.000)
Step: 45949, Reward: [-450.341 -450.341 -450.341] [135.8623], Avg: [-614.897 -614.897 -614.897] (1.000)
Step: 45999, Reward: [-538.951 -538.951 -538.951] [132.9412], Avg: [-614.959 -614.959 -614.959] (1.000)
Step: 46049, Reward: [-502.428 -502.428 -502.428] [53.9226], Avg: [-614.895 -614.895 -614.895] (1.000)
Step: 46099, Reward: [-527.278 -527.278 -527.278] [85.0419], Avg: [-614.892 -614.892 -614.892] (1.000)
Step: 46149, Reward: [-571.475 -571.475 -571.475] [80.6574], Avg: [-614.932 -614.932 -614.932] (1.000)
Step: 46199, Reward: [-557.244 -557.244 -557.244] [102.5522], Avg: [-614.981 -614.981 -614.981] (1.000)
Step: 46249, Reward: [-469.764 -469.764 -469.764] [71.6922], Avg: [-614.902 -614.902 -614.902] (1.000)
Step: 46299, Reward: [-624.285 -624.285 -624.285] [171.1307], Avg: [-615.097 -615.097 -615.097] (1.000)
Step: 46349, Reward: [-473.387 -473.387 -473.387] [72.3461], Avg: [-615.022 -615.022 -615.022] (1.000)
Step: 46399, Reward: [-600.107 -600.107 -600.107] [141.7486], Avg: [-615.158 -615.158 -615.158] (1.000)
Step: 46449, Reward: [-611.205 -611.205 -611.205] [118.6723], Avg: [-615.282 -615.282 -615.282] (1.000)
Step: 46499, Reward: [-553.856 -553.856 -553.856] [60.7162], Avg: [-615.281 -615.281 -615.281] (1.000)
Step: 46549, Reward: [-517.874 -517.874 -517.874] [130.9919], Avg: [-615.317 -615.317 -615.317] (1.000)
Step: 46599, Reward: [-525.754 -525.754 -525.754] [45.2109], Avg: [-615.27 -615.27 -615.27] (1.000)
Step: 46649, Reward: [-575.895 -575.895 -575.895] [123.3480], Avg: [-615.36 -615.36 -615.36] (1.000)
Step: 46699, Reward: [-598.086 -598.086 -598.086] [118.3732], Avg: [-615.468 -615.468 -615.468] (1.000)
Step: 46749, Reward: [-532.108 -532.108 -532.108] [100.6268], Avg: [-615.486 -615.486 -615.486] (1.000)
Step: 46799, Reward: [-607.678 -607.678 -607.678] [90.4347], Avg: [-615.575 -615.575 -615.575] (1.000)
Step: 46849, Reward: [-650.655 -650.655 -650.655] [30.1381], Avg: [-615.644 -615.644 -615.644] (1.000)
Step: 46899, Reward: [-559.983 -559.983 -559.983] [139.9129], Avg: [-615.734 -615.734 -615.734] (1.000)
Step: 46949, Reward: [-574.327 -574.327 -574.327] [69.7471], Avg: [-615.764 -615.764 -615.764] (1.000)
Step: 46999, Reward: [-632.618 -632.618 -632.618] [148.2947], Avg: [-615.94 -615.94 -615.94] (1.000)
Step: 47049, Reward: [-475.182 -475.182 -475.182] [61.2852], Avg: [-615.855 -615.855 -615.855] (1.000)
Step: 47099, Reward: [-648.555 -648.555 -648.555] [93.2715], Avg: [-615.989 -615.989 -615.989] (1.000)
Step: 47149, Reward: [-548.427 -548.427 -548.427] [54.6197], Avg: [-615.975 -615.975 -615.975] (1.000)
Step: 47199, Reward: [-572.105 -572.105 -572.105] [146.1377], Avg: [-616.084 -616.084 -616.084] (1.000)
Step: 47249, Reward: [-507.451 -507.451 -507.451] [26.7955], Avg: [-615.997 -615.997 -615.997] (1.000)
Step: 47299, Reward: [-507.021 -507.021 -507.021] [62.5705], Avg: [-615.948 -615.948 -615.948] (1.000)
Step: 47349, Reward: [-541.41 -541.41 -541.41] [84.4632], Avg: [-615.959 -615.959 -615.959] (1.000)
Step: 47399, Reward: [-453.765 -453.765 -453.765] [62.5681], Avg: [-615.853 -615.853 -615.853] (1.000)
Step: 47449, Reward: [-510.427 -510.427 -510.427] [104.4206], Avg: [-615.852 -615.852 -615.852] (1.000)
Step: 47499, Reward: [-614.633 -614.633 -614.633] [105.7390], Avg: [-615.962 -615.962 -615.962] (1.000)
Step: 47549, Reward: [-628.735 -628.735 -628.735] [96.1470], Avg: [-616.077 -616.077 -616.077] (1.000)
Step: 47599, Reward: [-545.838 -545.838 -545.838] [62.8348], Avg: [-616.069 -616.069 -616.069] (1.000)
Step: 47649, Reward: [-596.789 -596.789 -596.789] [126.5986], Avg: [-616.182 -616.182 -616.182] (1.000)
Step: 47699, Reward: [-513.931 -513.931 -513.931] [56.3147], Avg: [-616.134 -616.134 -616.134] (1.000)
Step: 47749, Reward: [-592.556 -592.556 -592.556] [83.5008], Avg: [-616.196 -616.196 -616.196] (1.000)
Step: 47799, Reward: [-567.487 -567.487 -567.487] [70.5949], Avg: [-616.219 -616.219 -616.219] (1.000)
Step: 47849, Reward: [-496.89 -496.89 -496.89] [57.6470], Avg: [-616.155 -616.155 -616.155] (1.000)
Step: 47899, Reward: [-548.476 -548.476 -548.476] [86.4428], Avg: [-616.174 -616.174 -616.174] (1.000)
Step: 47949, Reward: [-515.344 -515.344 -515.344] [75.0580], Avg: [-616.148 -616.148 -616.148] (1.000)
Step: 47999, Reward: [-533.086 -533.086 -533.086] [102.4972], Avg: [-616.168 -616.168 -616.168] (1.000)
Step: 48049, Reward: [-511.769 -511.769 -511.769] [40.0058], Avg: [-616.101 -616.101 -616.101] (1.000)
Step: 48099, Reward: [-426.938 -426.938 -426.938] [72.3119], Avg: [-615.979 -615.979 -615.979] (1.000)
Step: 48149, Reward: [-567.805 -567.805 -567.805] [128.8081], Avg: [-616.063 -616.063 -616.063] (1.000)
Step: 48199, Reward: [-515.69 -515.69 -515.69] [128.4357], Avg: [-616.092 -616.092 -616.092] (1.000)
Step: 48249, Reward: [-550.486 -550.486 -550.486] [96.5907], Avg: [-616.124 -616.124 -616.124] (1.000)
Step: 48299, Reward: [-485.507 -485.507 -485.507] [52.1678], Avg: [-616.043 -616.043 -616.043] (1.000)
Step: 48349, Reward: [-460.69 -460.69 -460.69] [59.7092], Avg: [-615.944 -615.944 -615.944] (1.000)
Step: 48399, Reward: [-511.467 -511.467 -511.467] [87.5783], Avg: [-615.927 -615.927 -615.927] (1.000)
Step: 48449, Reward: [-505.456 -505.456 -505.456] [111.2339], Avg: [-615.927 -615.927 -615.927] (1.000)
Step: 48499, Reward: [-563.746 -563.746 -563.746] [121.8504], Avg: [-615.999 -615.999 -615.999] (1.000)
Step: 48549, Reward: [-626.665 -626.665 -626.665] [107.8672], Avg: [-616.121 -616.121 -616.121] (1.000)
Step: 48599, Reward: [-578.855 -578.855 -578.855] [104.3689], Avg: [-616.19 -616.19 -616.19] (1.000)
Step: 48649, Reward: [-485.199 -485.199 -485.199] [46.3699], Avg: [-616.103 -616.103 -616.103] (1.000)
Step: 48699, Reward: [-587.068 -587.068 -587.068] [112.5747], Avg: [-616.189 -616.189 -616.189] (1.000)
Step: 48749, Reward: [-607.666 -607.666 -607.666] [85.3031], Avg: [-616.268 -616.268 -616.268] (1.000)
Step: 48799, Reward: [-471.535 -471.535 -471.535] [75.8105], Avg: [-616.197 -616.197 -616.197] (1.000)
Step: 48849, Reward: [-518.226 -518.226 -518.226] [49.3825], Avg: [-616.148 -616.148 -616.148] (1.000)
Step: 48899, Reward: [-650.934 -650.934 -650.934] [108.4531], Avg: [-616.294 -616.294 -616.294] (1.000)
Step: 48949, Reward: [-498.065 -498.065 -498.065] [110.1533], Avg: [-616.286 -616.286 -616.286] (1.000)
Step: 48999, Reward: [-597.366 -597.366 -597.366] [181.2006], Avg: [-616.451 -616.451 -616.451] (1.000)
Step: 49049, Reward: [-547.99 -547.99 -547.99] [121.3492], Avg: [-616.505 -616.505 -616.505] (1.000)
Step: 49099, Reward: [-538.695 -538.695 -538.695] [78.8533], Avg: [-616.506 -616.506 -616.506] (1.000)
Step: 49149, Reward: [-532.356 -532.356 -532.356] [112.3748], Avg: [-616.535 -616.535 -616.535] (1.000)
Step: 49199, Reward: [-503.93 -503.93 -503.93] [127.6649], Avg: [-616.55 -616.55 -616.55] (1.000)
Step: 49249, Reward: [-478.223 -478.223 -478.223] [35.0143], Avg: [-616.446 -616.446 -616.446] (1.000)
Step: 49299, Reward: [-606.851 -606.851 -606.851] [57.7714], Avg: [-616.494 -616.494 -616.494] (1.000)
Step: 49349, Reward: [-530.267 -530.267 -530.267] [69.1429], Avg: [-616.477 -616.477 -616.477] (1.000)
Step: 49399, Reward: [-696.481 -696.481 -696.481] [117.6852], Avg: [-616.677 -616.677 -616.677] (1.000)
Step: 49449, Reward: [-585.978 -585.978 -585.978] [99.4699], Avg: [-616.747 -616.747 -616.747] (1.000)
Step: 49499, Reward: [-508.291 -508.291 -508.291] [91.4100], Avg: [-616.729 -616.729 -616.729] (1.000)
Step: 49549, Reward: [-511.917 -511.917 -511.917] [76.1708], Avg: [-616.701 -616.701 -616.701] (1.000)
Step: 49599, Reward: [-611.857 -611.857 -611.857] [71.2886], Avg: [-616.768 -616.768 -616.768] (1.000)
Step: 49649, Reward: [-496.366 -496.366 -496.366] [61.1511], Avg: [-616.708 -616.708 -616.708] (1.000)
Step: 49699, Reward: [-546.883 -546.883 -546.883] [108.1524], Avg: [-616.746 -616.746 -616.746] (1.000)
Step: 49749, Reward: [-574.679 -574.679 -574.679] [50.6795], Avg: [-616.755 -616.755 -616.755] (1.000)
Step: 49799, Reward: [-600.943 -600.943 -600.943] [129.1523], Avg: [-616.869 -616.869 -616.869] (1.000)
Step: 49849, Reward: [-489.374 -489.374 -489.374] [80.5104], Avg: [-616.822 -616.822 -616.822] (1.000)
Step: 49899, Reward: [-513.359 -513.359 -513.359] [99.7240], Avg: [-616.818 -616.818 -616.818] (1.000)
Step: 49949, Reward: [-469.313 -469.313 -469.313] [35.1162], Avg: [-616.706 -616.706 -616.706] (1.000)
Step: 49999, Reward: [-507.837 -507.837 -507.837] [56.2239], Avg: [-616.653 -616.653 -616.653] (1.000)
Step: 50049, Reward: [-590.621 -590.621 -590.621] [71.9413], Avg: [-616.699 -616.699 -616.699] (1.000)
Step: 50099, Reward: [-602.388 -602.388 -602.388] [86.8562], Avg: [-616.771 -616.771 -616.771] (1.000)
Step: 50149, Reward: [-504.296 -504.296 -504.296] [37.9031], Avg: [-616.697 -616.697 -616.697] (1.000)
Step: 50199, Reward: [-453.91 -453.91 -453.91] [84.7066], Avg: [-616.619 -616.619 -616.619] (1.000)
Step: 50249, Reward: [-552.23 -552.23 -552.23] [165.1341], Avg: [-616.719 -616.719 -616.719] (1.000)
Step: 50299, Reward: [-549.424 -549.424 -549.424] [141.4741], Avg: [-616.793 -616.793 -616.793] (1.000)
Step: 50349, Reward: [-528.034 -528.034 -528.034] [91.5821], Avg: [-616.796 -616.796 -616.796] (1.000)
Step: 50399, Reward: [-456.412 -456.412 -456.412] [96.4653], Avg: [-616.732 -616.732 -616.732] (1.000)
Step: 50449, Reward: [-505.286 -505.286 -505.286] [51.5533], Avg: [-616.673 -616.673 -616.673] (1.000)
Step: 50499, Reward: [-475.543 -475.543 -475.543] [85.2731], Avg: [-616.618 -616.618 -616.618] (1.000)
Step: 50549, Reward: [-483.956 -483.956 -483.956] [45.7329], Avg: [-616.532 -616.532 -616.532] (1.000)
Step: 50599, Reward: [-615.543 -615.543 -615.543] [184.8948], Avg: [-616.713 -616.713 -616.713] (1.000)
Step: 50649, Reward: [-483.666 -483.666 -483.666] [112.3152], Avg: [-616.693 -616.693 -616.693] (1.000)
Step: 50699, Reward: [-511.394 -511.394 -511.394] [22.5733], Avg: [-616.611 -616.611 -616.611] (1.000)
Step: 50749, Reward: [-503.421 -503.421 -503.421] [79.7837], Avg: [-616.579 -616.579 -616.579] (1.000)
Step: 50799, Reward: [-569.07 -569.07 -569.07] [67.8201], Avg: [-616.599 -616.599 -616.599] (1.000)
Step: 50849, Reward: [-550.954 -550.954 -550.954] [28.2659], Avg: [-616.562 -616.562 -616.562] (1.000)
Step: 50899, Reward: [-597.599 -597.599 -597.599] [106.7421], Avg: [-616.648 -616.648 -616.648] (1.000)
Step: 50949, Reward: [-519.323 -519.323 -519.323] [124.9885], Avg: [-616.675 -616.675 -616.675] (1.000)
Step: 50999, Reward: [-509.466 -509.466 -509.466] [128.6450], Avg: [-616.696 -616.696 -616.696] (1.000)
Step: 51049, Reward: [-532.28 -532.28 -532.28] [113.4279], Avg: [-616.725 -616.725 -616.725] (1.000)
Step: 51099, Reward: [-442.291 -442.291 -442.291] [81.1023], Avg: [-616.633 -616.633 -616.633] (1.000)
Step: 51149, Reward: [-504.856 -504.856 -504.856] [73.7777], Avg: [-616.596 -616.596 -616.596] (1.000)
Step: 51199, Reward: [-522.704 -522.704 -522.704] [127.7840], Avg: [-616.629 -616.629 -616.629] (1.000)
Step: 51249, Reward: [-500.32 -500.32 -500.32] [87.9999], Avg: [-616.602 -616.602 -616.602] (1.000)
Step: 51299, Reward: [-617.414 -617.414 -617.414] [168.7001], Avg: [-616.767 -616.767 -616.767] (1.000)
Step: 51349, Reward: [-587.38 -587.38 -587.38] [129.6364], Avg: [-616.864 -616.864 -616.864] (1.000)
Step: 51399, Reward: [-492.743 -492.743 -492.743] [39.3266], Avg: [-616.782 -616.782 -616.782] (1.000)
Step: 51449, Reward: [-474.124 -474.124 -474.124] [53.2800], Avg: [-616.695 -616.695 -616.695] (1.000)
Step: 51499, Reward: [-510.64 -510.64 -510.64] [81.7557], Avg: [-616.671 -616.671 -616.671] (1.000)
Step: 51549, Reward: [-521.396 -521.396 -521.396] [87.9994], Avg: [-616.664 -616.664 -616.664] (1.000)
Step: 51599, Reward: [-540.848 -540.848 -540.848] [110.3543], Avg: [-616.698 -616.698 -616.698] (1.000)
Step: 51649, Reward: [-485.487 -485.487 -485.487] [176.3217], Avg: [-616.742 -616.742 -616.742] (1.000)
Step: 51699, Reward: [-530.989 -530.989 -530.989] [153.3656], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 51749, Reward: [-448.835 -448.835 -448.835] [55.3967], Avg: [-616.698 -616.698 -616.698] (1.000)
Step: 51799, Reward: [-544.028 -544.028 -544.028] [93.8580], Avg: [-616.719 -616.719 -616.719] (1.000)
Step: 51849, Reward: [-511.964 -511.964 -511.964] [122.4276], Avg: [-616.736 -616.736 -616.736] (1.000)
Step: 51899, Reward: [-457.546 -457.546 -457.546] [85.3567], Avg: [-616.665 -616.665 -616.665] (1.000)
Step: 51949, Reward: [-531.634 -531.634 -531.634] [55.1228], Avg: [-616.636 -616.636 -616.636] (1.000)
Step: 51999, Reward: [-520.07 -520.07 -520.07] [89.5152], Avg: [-616.629 -616.629 -616.629] (1.000)
Step: 52049, Reward: [-493.568 -493.568 -493.568] [95.6955], Avg: [-616.603 -616.603 -616.603] (1.000)
Step: 52099, Reward: [-519.288 -519.288 -519.288] [41.7447], Avg: [-616.549 -616.549 -616.549] (1.000)
Step: 52149, Reward: [-475.329 -475.329 -475.329] [71.8713], Avg: [-616.483 -616.483 -616.483] (1.000)
Step: 52199, Reward: [-486.316 -486.316 -486.316] [31.6554], Avg: [-616.388 -616.388 -616.388] (1.000)
Step: 52249, Reward: [-538.113 -538.113 -538.113] [87.0804], Avg: [-616.397 -616.397 -616.397] (1.000)
Step: 52299, Reward: [-570.13 -570.13 -570.13] [150.6279], Avg: [-616.497 -616.497 -616.497] (1.000)
Step: 52349, Reward: [-493.571 -493.571 -493.571] [58.8685], Avg: [-616.436 -616.436 -616.436] (1.000)
Step: 52399, Reward: [-500.307 -500.307 -500.307] [56.1038], Avg: [-616.378 -616.378 -616.378] (1.000)
Step: 52449, Reward: [-476.508 -476.508 -476.508] [68.9753], Avg: [-616.311 -616.311 -616.311] (1.000)
Step: 52499, Reward: [-485.751 -485.751 -485.751] [108.2928], Avg: [-616.289 -616.289 -616.289] (1.000)
Step: 52549, Reward: [-456.218 -456.218 -456.218] [45.6277], Avg: [-616.181 -616.181 -616.181] (1.000)
Step: 52599, Reward: [-504.166 -504.166 -504.166] [118.2440], Avg: [-616.186 -616.186 -616.186] (1.000)
Step: 52649, Reward: [-458.526 -458.526 -458.526] [103.1459], Avg: [-616.135 -616.135 -616.135] (1.000)
Step: 52699, Reward: [-564.063 -564.063 -564.063] [62.0337], Avg: [-616.144 -616.144 -616.144] (1.000)
Step: 52749, Reward: [-482.545 -482.545 -482.545] [80.1758], Avg: [-616.094 -616.094 -616.094] (1.000)
Step: 52799, Reward: [-491.144 -491.144 -491.144] [62.2410], Avg: [-616.034 -616.034 -616.034] (1.000)
Step: 52849, Reward: [-501.877 -501.877 -501.877] [73.1350], Avg: [-615.995 -615.995 -615.995] (1.000)
Step: 52899, Reward: [-496.57 -496.57 -496.57] [78.9067], Avg: [-615.957 -615.957 -615.957] (1.000)
Step: 52949, Reward: [-542.362 -542.362 -542.362] [77.6355], Avg: [-615.961 -615.961 -615.961] (1.000)
Step: 52999, Reward: [-514.286 -514.286 -514.286] [75.2457], Avg: [-615.936 -615.936 -615.936] (1.000)
Step: 53049, Reward: [-557.227 -557.227 -557.227] [63.4464], Avg: [-615.94 -615.94 -615.94] (1.000)
Step: 53099, Reward: [-614.026 -614.026 -614.026] [140.4165], Avg: [-616.071 -616.071 -616.071] (1.000)
Step: 53149, Reward: [-538.932 -538.932 -538.932] [82.7520], Avg: [-616.076 -616.076 -616.076] (1.000)
Step: 53199, Reward: [-575.891 -575.891 -575.891] [84.3780], Avg: [-616.118 -616.118 -616.118] (1.000)
Step: 53249, Reward: [-608.098 -608.098 -608.098] [103.1314], Avg: [-616.207 -616.207 -616.207] (1.000)
Step: 53299, Reward: [-513.147 -513.147 -513.147] [90.6094], Avg: [-616.195 -616.195 -616.195] (1.000)
Step: 53349, Reward: [-535.415 -535.415 -535.415] [124.2360], Avg: [-616.236 -616.236 -616.236] (1.000)
Step: 53399, Reward: [-594.334 -594.334 -594.334] [119.6592], Avg: [-616.327 -616.327 -616.327] (1.000)
Step: 53449, Reward: [-472.192 -472.192 -472.192] [115.6623], Avg: [-616.301 -616.301 -616.301] (1.000)
Step: 53499, Reward: [-480.725 -480.725 -480.725] [92.8849], Avg: [-616.261 -616.261 -616.261] (1.000)
Step: 53549, Reward: [-525.917 -525.917 -525.917] [63.3287], Avg: [-616.236 -616.236 -616.236] (1.000)
Step: 53599, Reward: [-566.827 -566.827 -566.827] [104.1573], Avg: [-616.287 -616.287 -616.287] (1.000)
Step: 53649, Reward: [-552.972 -552.972 -552.972] [143.6024], Avg: [-616.362 -616.362 -616.362] (1.000)
Step: 53699, Reward: [-444.199 -444.199 -444.199] [37.5630], Avg: [-616.236 -616.236 -616.236] (1.000)
Step: 53749, Reward: [-517.512 -517.512 -517.512] [66.5975], Avg: [-616.206 -616.206 -616.206] (1.000)
Step: 53799, Reward: [-442.233 -442.233 -442.233] [43.1449], Avg: [-616.085 -616.085 -616.085] (1.000)
Step: 53849, Reward: [-455.715 -455.715 -455.715] [86.3029], Avg: [-616.016 -616.016 -616.016] (1.000)
Step: 53899, Reward: [-521.142 -521.142 -521.142] [101.3580], Avg: [-616.022 -616.022 -616.022] (1.000)
Step: 53949, Reward: [-589.211 -589.211 -589.211] [103.6605], Avg: [-616.093 -616.093 -616.093] (1.000)
Step: 53999, Reward: [-539.912 -539.912 -539.912] [153.6707], Avg: [-616.165 -616.165 -616.165] (1.000)
Step: 54049, Reward: [-509.37 -509.37 -509.37] [51.5869], Avg: [-616.114 -616.114 -616.114] (1.000)
Step: 54099, Reward: [-586.388 -586.388 -586.388] [115.0724], Avg: [-616.193 -616.193 -616.193] (1.000)
Step: 54149, Reward: [-481.719 -481.719 -481.719] [67.7923], Avg: [-616.131 -616.131 -616.131] (1.000)
Step: 54199, Reward: [-626.371 -626.371 -626.371] [130.7323], Avg: [-616.261 -616.261 -616.261] (1.000)
Step: 54249, Reward: [-516.656 -516.656 -516.656] [86.2232], Avg: [-616.249 -616.249 -616.249] (1.000)
Step: 54299, Reward: [-666.461 -666.461 -666.461] [182.4452], Avg: [-616.463 -616.463 -616.463] (1.000)
Step: 54349, Reward: [-568.149 -568.149 -568.149] [145.3581], Avg: [-616.553 -616.553 -616.553] (1.000)
Step: 54399, Reward: [-524.548 -524.548 -524.548] [75.2312], Avg: [-616.537 -616.537 -616.537] (1.000)
Step: 54449, Reward: [-590.792 -590.792 -590.792] [132.6469], Avg: [-616.635 -616.635 -616.635] (1.000)
Step: 54499, Reward: [-558.028 -558.028 -558.028] [108.7715], Avg: [-616.681 -616.681 -616.681] (1.000)
Step: 54549, Reward: [-565.041 -565.041 -565.041] [83.9529], Avg: [-616.711 -616.711 -616.711] (1.000)
Step: 54599, Reward: [-448.99 -448.99 -448.99] [46.9618], Avg: [-616.6 -616.6 -616.6] (1.000)
Step: 54649, Reward: [-590.392 -590.392 -590.392] [199.4285], Avg: [-616.759 -616.759 -616.759] (1.000)
Step: 54699, Reward: [-550.107 -550.107 -550.107] [62.0236], Avg: [-616.755 -616.755 -616.755] (1.000)
Step: 54749, Reward: [-512.521 -512.521 -512.521] [108.3797], Avg: [-616.758 -616.758 -616.758] (1.000)
Step: 54799, Reward: [-530.044 -530.044 -530.044] [92.5431], Avg: [-616.764 -616.764 -616.764] (1.000)
Step: 54849, Reward: [-573.463 -573.463 -573.463] [91.2083], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 54899, Reward: [-507.944 -507.944 -507.944] [20.3940], Avg: [-616.727 -616.727 -616.727] (1.000)
Step: 54949, Reward: [-461.47 -461.47 -461.47] [39.7833], Avg: [-616.622 -616.622 -616.622] (1.000)
Step: 54999, Reward: [-537.63 -537.63 -537.63] [186.6736], Avg: [-616.72 -616.72 -616.72] (1.000)
Step: 55049, Reward: [-603.466 -603.466 -603.466] [87.4383], Avg: [-616.787 -616.787 -616.787] (1.000)
Step: 55099, Reward: [-537.637 -537.637 -537.637] [112.5802], Avg: [-616.817 -616.817 -616.817] (1.000)
Step: 55149, Reward: [-509.713 -509.713 -509.713] [73.7757], Avg: [-616.787 -616.787 -616.787] (1.000)
Step: 55199, Reward: [-549.387 -549.387 -549.387] [126.5679], Avg: [-616.841 -616.841 -616.841] (1.000)
Step: 55249, Reward: [-522.524 -522.524 -522.524] [37.4793], Avg: [-616.789 -616.789 -616.789] (1.000)
Step: 55299, Reward: [-576.203 -576.203 -576.203] [182.2205], Avg: [-616.917 -616.917 -616.917] (1.000)
Step: 55349, Reward: [-614.536 -614.536 -614.536] [73.6586], Avg: [-616.982 -616.982 -616.982] (1.000)
Step: 55399, Reward: [-593.601 -593.601 -593.601] [181.1504], Avg: [-617.124 -617.124 -617.124] (1.000)
Step: 55449, Reward: [-536.887 -536.887 -536.887] [134.7127], Avg: [-617.173 -617.173 -617.173] (1.000)
Step: 55499, Reward: [-493.482 -493.482 -493.482] [57.5869], Avg: [-617.114 -617.114 -617.114] (1.000)
Step: 55549, Reward: [-491.907 -491.907 -491.907] [61.3638], Avg: [-617.056 -617.056 -617.056] (1.000)
Step: 55599, Reward: [-493.363 -493.363 -493.363] [72.3659], Avg: [-617.01 -617.01 -617.01] (1.000)
Step: 55649, Reward: [-608.387 -608.387 -608.387] [68.8457], Avg: [-617.064 -617.064 -617.064] (1.000)
Step: 55699, Reward: [-551.39 -551.39 -551.39] [141.1748], Avg: [-617.132 -617.132 -617.132] (1.000)
Step: 55749, Reward: [-575.253 -575.253 -575.253] [155.0021], Avg: [-617.233 -617.233 -617.233] (1.000)
Step: 55799, Reward: [-462.795 -462.795 -462.795] [56.9830], Avg: [-617.146 -617.146 -617.146] (1.000)
Step: 55849, Reward: [-539.886 -539.886 -539.886] [48.4352], Avg: [-617.12 -617.12 -617.12] (1.000)
Step: 55899, Reward: [-577.201 -577.201 -577.201] [132.3110], Avg: [-617.203 -617.203 -617.203] (1.000)
Step: 55949, Reward: [-517.82 -517.82 -517.82] [43.3966], Avg: [-617.153 -617.153 -617.153] (1.000)
Step: 55999, Reward: [-557.595 -557.595 -557.595] [81.4647], Avg: [-617.172 -617.172 -617.172] (1.000)
Step: 56049, Reward: [-523.045 -523.045 -523.045] [92.7252], Avg: [-617.171 -617.171 -617.171] (1.000)
Step: 56099, Reward: [-501.314 -501.314 -501.314] [125.6155], Avg: [-617.18 -617.18 -617.18] (1.000)
Step: 56149, Reward: [-600.546 -600.546 -600.546] [140.8363], Avg: [-617.29 -617.29 -617.29] (1.000)
Step: 56199, Reward: [-541.687 -541.687 -541.687] [86.6870], Avg: [-617.3 -617.3 -617.3] (1.000)
Step: 56249, Reward: [-524.048 -524.048 -524.048] [88.0839], Avg: [-617.296 -617.296 -617.296] (1.000)
Step: 56299, Reward: [-469.32 -469.32 -469.32] [110.7611], Avg: [-617.263 -617.263 -617.263] (1.000)
Step: 56349, Reward: [-640.928 -640.928 -640.928] [100.5779], Avg: [-617.373 -617.373 -617.373] (1.000)
Step: 56399, Reward: [-526.535 -526.535 -526.535] [71.5229], Avg: [-617.356 -617.356 -617.356] (1.000)
Step: 56449, Reward: [-652.119 -652.119 -652.119] [169.7715], Avg: [-617.537 -617.537 -617.537] (1.000)
Step: 56499, Reward: [-509.704 -509.704 -509.704] [98.2775], Avg: [-617.528 -617.528 -617.528] (1.000)
Step: 56549, Reward: [-570.282 -570.282 -570.282] [52.2368], Avg: [-617.533 -617.533 -617.533] (1.000)
Step: 56599, Reward: [-522.047 -522.047 -522.047] [36.2986], Avg: [-617.481 -617.481 -617.481] (1.000)
Step: 56649, Reward: [-546.914 -546.914 -546.914] [74.4189], Avg: [-617.484 -617.484 -617.484] (1.000)
Step: 56699, Reward: [-529.047 -529.047 -529.047] [97.4762], Avg: [-617.492 -617.492 -617.492] (1.000)
Step: 56749, Reward: [-496.303 -496.303 -496.303] [119.6156], Avg: [-617.491 -617.491 -617.491] (1.000)
Step: 56799, Reward: [-474.944 -474.944 -474.944] [80.6319], Avg: [-617.436 -617.436 -617.436] (1.000)
Step: 56849, Reward: [-492.738 -492.738 -492.738] [104.0834], Avg: [-617.418 -617.418 -617.418] (1.000)
Step: 56899, Reward: [-504.305 -504.305 -504.305] [142.4519], Avg: [-617.444 -617.444 -617.444] (1.000)
Step: 56949, Reward: [-522.267 -522.267 -522.267] [53.4605], Avg: [-617.407 -617.407 -617.407] (1.000)
Step: 56999, Reward: [-422.02 -422.02 -422.02] [19.3159], Avg: [-617.253 -617.253 -617.253] (1.000)
Step: 57049, Reward: [-473.321 -473.321 -473.321] [41.2316], Avg: [-617.163 -617.163 -617.163] (1.000)
Step: 57099, Reward: [-440.26 -440.26 -440.26] [46.2510], Avg: [-617.048 -617.048 -617.048] (1.000)
Step: 57149, Reward: [-475.332 -475.332 -475.332] [97.8242], Avg: [-617.01 -617.01 -617.01] (1.000)
Step: 57199, Reward: [-475.005 -475.005 -475.005] [127.2280], Avg: [-616.997 -616.997 -616.997] (1.000)
Step: 57249, Reward: [-485.683 -485.683 -485.683] [64.6892], Avg: [-616.939 -616.939 -616.939] (1.000)
Step: 57299, Reward: [-406.065 -406.065 -406.065] [43.5165], Avg: [-616.793 -616.793 -616.793] (1.000)
Step: 57349, Reward: [-491.666 -491.666 -491.666] [36.8504], Avg: [-616.716 -616.716 -616.716] (1.000)
Step: 57399, Reward: [-496.557 -496.557 -496.557] [66.6052], Avg: [-616.669 -616.669 -616.669] (1.000)
Step: 57449, Reward: [-490.128 -490.128 -490.128] [78.9184], Avg: [-616.628 -616.628 -616.628] (1.000)
Step: 57499, Reward: [-493.157 -493.157 -493.157] [120.5131], Avg: [-616.625 -616.625 -616.625] (1.000)
Step: 57549, Reward: [-533.224 -533.224 -533.224] [69.0954], Avg: [-616.613 -616.613 -616.613] (1.000)
Step: 57599, Reward: [-638.373 -638.373 -638.373] [63.8744], Avg: [-616.687 -616.687 -616.687] (1.000)
Step: 57649, Reward: [-524.847 -524.847 -524.847] [106.2440], Avg: [-616.699 -616.699 -616.699] (1.000)
Step: 57699, Reward: [-520.307 -520.307 -520.307] [70.1136], Avg: [-616.677 -616.677 -616.677] (1.000)
Step: 57749, Reward: [-551.306 -551.306 -551.306] [47.2706], Avg: [-616.661 -616.661 -616.661] (1.000)
Step: 57799, Reward: [-511.948 -511.948 -511.948] [94.8910], Avg: [-616.653 -616.653 -616.653] (1.000)
Step: 57849, Reward: [-632.658 -632.658 -632.658] [105.1936], Avg: [-616.757 -616.757 -616.757] (1.000)
Step: 57899, Reward: [-522.77 -522.77 -522.77] [93.9076], Avg: [-616.757 -616.757 -616.757] (1.000)
Step: 57949, Reward: [-499.328 -499.328 -499.328] [73.7775], Avg: [-616.72 -616.72 -616.72] (1.000)
Step: 57999, Reward: [-561.998 -561.998 -561.998] [197.4735], Avg: [-616.843 -616.843 -616.843] (1.000)
Step: 58049, Reward: [-526.152 -526.152 -526.152] [40.2405], Avg: [-616.799 -616.799 -616.799] (1.000)
Step: 58099, Reward: [-491.152 -491.152 -491.152] [85.2893], Avg: [-616.764 -616.764 -616.764] (1.000)
Step: 58149, Reward: [-479.248 -479.248 -479.248] [91.3790], Avg: [-616.725 -616.725 -616.725] (1.000)
Step: 58199, Reward: [-542.896 -542.896 -542.896] [82.9749], Avg: [-616.733 -616.733 -616.733] (1.000)
Step: 58249, Reward: [-516.137 -516.137 -516.137] [48.5395], Avg: [-616.688 -616.688 -616.688] (1.000)
Step: 58299, Reward: [-563.098 -563.098 -563.098] [98.6231], Avg: [-616.727 -616.727 -616.727] (1.000)
Step: 58349, Reward: [-565.906 -565.906 -565.906] [163.2503], Avg: [-616.823 -616.823 -616.823] (1.000)
Step: 58399, Reward: [-508.256 -508.256 -508.256] [68.5690], Avg: [-616.789 -616.789 -616.789] (1.000)
Step: 58449, Reward: [-463.697 -463.697 -463.697] [69.2936], Avg: [-616.717 -616.717 -616.717] (1.000)
Step: 58499, Reward: [-630.391 -630.391 -630.391] [163.6973], Avg: [-616.869 -616.869 -616.869] (1.000)
Step: 58549, Reward: [-464.1 -464.1 -464.1] [101.1114], Avg: [-616.824 -616.824 -616.824] (1.000)
Step: 58599, Reward: [-463.899 -463.899 -463.899] [50.3834], Avg: [-616.737 -616.737 -616.737] (1.000)
Step: 58649, Reward: [-471.554 -471.554 -471.554] [32.1477], Avg: [-616.641 -616.641 -616.641] (1.000)
Step: 58699, Reward: [-460.097 -460.097 -460.097] [36.3127], Avg: [-616.538 -616.538 -616.538] (1.000)
Step: 58749, Reward: [-487.621 -487.621 -487.621] [92.1169], Avg: [-616.507 -616.507 -616.507] (1.000)
Step: 58799, Reward: [-582.752 -582.752 -582.752] [76.2405], Avg: [-616.543 -616.543 -616.543] (1.000)
Step: 58849, Reward: [-503.356 -503.356 -503.356] [48.9544], Avg: [-616.488 -616.488 -616.488] (1.000)
Step: 58899, Reward: [-482.156 -482.156 -482.156] [63.6872], Avg: [-616.428 -616.428 -616.428] (1.000)
Step: 58949, Reward: [-455.22 -455.22 -455.22] [64.7513], Avg: [-616.347 -616.347 -616.347] (1.000)
Step: 58999, Reward: [-549.775 -549.775 -549.775] [63.2698], Avg: [-616.344 -616.344 -616.344] (1.000)
Step: 59049, Reward: [-495.92 -495.92 -495.92] [78.2981], Avg: [-616.308 -616.308 -616.308] (1.000)
Step: 59099, Reward: [-586.466 -586.466 -586.466] [165.5685], Avg: [-616.423 -616.423 -616.423] (1.000)
Step: 59149, Reward: [-456.424 -456.424 -456.424] [80.6602], Avg: [-616.356 -616.356 -616.356] (1.000)
Step: 59199, Reward: [-495.261 -495.261 -495.261] [23.5874], Avg: [-616.274 -616.274 -616.274] (1.000)
Step: 59249, Reward: [-543.655 -543.655 -543.655] [74.6524], Avg: [-616.275 -616.275 -616.275] (1.000)
Step: 59299, Reward: [-520.973 -520.973 -520.973] [45.3357], Avg: [-616.233 -616.233 -616.233] (1.000)
Step: 59349, Reward: [-480.393 -480.393 -480.393] [146.5157], Avg: [-616.242 -616.242 -616.242] (1.000)
Step: 59399, Reward: [-486.058 -486.058 -486.058] [116.5230], Avg: [-616.231 -616.231 -616.231] (1.000)
Step: 59449, Reward: [-584.76 -584.76 -584.76] [39.0141], Avg: [-616.237 -616.237 -616.237] (1.000)
Step: 59499, Reward: [-526.924 -526.924 -526.924] [83.8705], Avg: [-616.232 -616.232 -616.232] (1.000)
Step: 59549, Reward: [-466.405 -466.405 -466.405] [108.6666], Avg: [-616.198 -616.198 -616.198] (1.000)
Step: 59599, Reward: [-532.743 -532.743 -532.743] [59.9701], Avg: [-616.178 -616.178 -616.178] (1.000)
Step: 59649, Reward: [-437.512 -437.512 -437.512] [47.5623], Avg: [-616.068 -616.068 -616.068] (1.000)
Step: 59699, Reward: [-422.162 -422.162 -422.162] [23.6742], Avg: [-615.926 -615.926 -615.926] (1.000)
Step: 59749, Reward: [-468.895 -468.895 -468.895] [76.8905], Avg: [-615.867 -615.867 -615.867] (1.000)
Step: 59799, Reward: [-502.538 -502.538 -502.538] [44.3580], Avg: [-615.809 -615.809 -615.809] (1.000)
Step: 59849, Reward: [-526.779 -526.779 -526.779] [83.2956], Avg: [-615.805 -615.805 -615.805] (1.000)
Step: 59899, Reward: [-440.74 -440.74 -440.74] [77.8961], Avg: [-615.723 -615.723 -615.723] (1.000)
Step: 59949, Reward: [-480.44 -480.44 -480.44] [135.1409], Avg: [-615.723 -615.723 -615.723] (1.000)
Step: 59999, Reward: [-434.03 -434.03 -434.03] [63.3531], Avg: [-615.625 -615.625 -615.625] (1.000)
Step: 60049, Reward: [-479.356 -479.356 -479.356] [53.5804], Avg: [-615.556 -615.556 -615.556] (1.000)
Step: 60099, Reward: [-504.479 -504.479 -504.479] [44.1832], Avg: [-615.5 -615.5 -615.5] (1.000)
Step: 60149, Reward: [-510.499 -510.499 -510.499] [76.9830], Avg: [-615.477 -615.477 -615.477] (1.000)
Step: 60199, Reward: [-531.739 -531.739 -531.739] [82.9244], Avg: [-615.476 -615.476 -615.476] (1.000)
Step: 60249, Reward: [-521.583 -521.583 -521.583] [75.9419], Avg: [-615.461 -615.461 -615.461] (1.000)
Step: 60299, Reward: [-515.856 -515.856 -515.856] [46.5736], Avg: [-615.417 -615.417 -615.417] (1.000)
Step: 60349, Reward: [-501.268 -501.268 -501.268] [93.1918], Avg: [-615.4 -615.4 -615.4] (1.000)
Step: 60399, Reward: [-543.206 -543.206 -543.206] [104.3875], Avg: [-615.427 -615.427 -615.427] (1.000)
Step: 60449, Reward: [-503.257 -503.257 -503.257] [64.1245], Avg: [-615.387 -615.387 -615.387] (1.000)
Step: 60499, Reward: [-471.696 -471.696 -471.696] [79.6153], Avg: [-615.334 -615.334 -615.334] (1.000)
Step: 60549, Reward: [-623.878 -623.878 -623.878] [102.8889], Avg: [-615.426 -615.426 -615.426] (1.000)
Step: 60599, Reward: [-604.971 -604.971 -604.971] [118.6808], Avg: [-615.515 -615.515 -615.515] (1.000)
Step: 60649, Reward: [-490.023 -490.023 -490.023] [97.8327], Avg: [-615.492 -615.492 -615.492] (1.000)
Step: 60699, Reward: [-502.342 -502.342 -502.342] [33.4162], Avg: [-615.427 -615.427 -615.427] (1.000)
Step: 60749, Reward: [-513.191 -513.191 -513.191] [121.1245], Avg: [-615.442 -615.442 -615.442] (1.000)
Step: 60799, Reward: [-502.365 -502.365 -502.365] [63.9398], Avg: [-615.402 -615.402 -615.402] (1.000)
Step: 60849, Reward: [-519.203 -519.203 -519.203] [56.6479], Avg: [-615.369 -615.369 -615.369] (1.000)
Step: 60899, Reward: [-470.937 -470.937 -470.937] [39.9216], Avg: [-615.284 -615.284 -615.284] (1.000)
Step: 60949, Reward: [-459.556 -459.556 -459.556] [42.0358], Avg: [-615.19 -615.19 -615.19] (1.000)
Step: 60999, Reward: [-521.69 -521.69 -521.69] [32.1728], Avg: [-615.14 -615.14 -615.14] (1.000)
Step: 61049, Reward: [-463.997 -463.997 -463.997] [100.1500], Avg: [-615.098 -615.098 -615.098] (1.000)
Step: 61099, Reward: [-515.302 -515.302 -515.302] [124.2049], Avg: [-615.118 -615.118 -615.118] (1.000)
Step: 61149, Reward: [-565.56 -565.56 -565.56] [50.4100], Avg: [-615.119 -615.119 -615.119] (1.000)
Step: 61199, Reward: [-567.528 -567.528 -567.528] [103.9425], Avg: [-615.165 -615.165 -615.165] (1.000)
Step: 61249, Reward: [-590.767 -590.767 -590.767] [117.1773], Avg: [-615.241 -615.241 -615.241] (1.000)
Step: 61299, Reward: [-588.994 -588.994 -588.994] [140.6335], Avg: [-615.334 -615.334 -615.334] (1.000)
Step: 61349, Reward: [-660.652 -660.652 -660.652] [146.2122], Avg: [-615.49 -615.49 -615.49] (1.000)
Step: 61399, Reward: [-478.763 -478.763 -478.763] [83.7588], Avg: [-615.447 -615.447 -615.447] (1.000)
Step: 61449, Reward: [-527.074 -527.074 -527.074] [98.6841], Avg: [-615.455 -615.455 -615.455] (1.000)
Step: 61499, Reward: [-491.093 -491.093 -491.093] [74.8045], Avg: [-615.415 -615.415 -615.415] (1.000)
Step: 61549, Reward: [-504.052 -504.052 -504.052] [51.1507], Avg: [-615.366 -615.366 -615.366] (1.000)
Step: 61599, Reward: [-582.395 -582.395 -582.395] [75.8433], Avg: [-615.401 -615.401 -615.401] (1.000)
Step: 61649, Reward: [-502.85 -502.85 -502.85] [139.4962], Avg: [-615.423 -615.423 -615.423] (1.000)
Step: 61699, Reward: [-514.716 -514.716 -514.716] [36.8591], Avg: [-615.371 -615.371 -615.371] (1.000)
Step: 61749, Reward: [-522.854 -522.854 -522.854] [136.1835], Avg: [-615.407 -615.407 -615.407] (1.000)
Step: 61799, Reward: [-631.772 -631.772 -631.772] [99.8027], Avg: [-615.5 -615.5 -615.5] (1.000)
Step: 61849, Reward: [-630.429 -630.429 -630.429] [91.4069], Avg: [-615.586 -615.586 -615.586] (1.000)
Step: 61899, Reward: [-555.679 -555.679 -555.679] [83.5266], Avg: [-615.606 -615.606 -615.606] (1.000)
Step: 61949, Reward: [-551.203 -551.203 -551.203] [190.2161], Avg: [-615.707 -615.707 -615.707] (1.000)
Step: 61999, Reward: [-491.36 -491.36 -491.36] [61.8247], Avg: [-615.657 -615.657 -615.657] (1.000)
Step: 62049, Reward: [-474.064 -474.064 -474.064] [67.4705], Avg: [-615.597 -615.597 -615.597] (1.000)
Step: 62099, Reward: [-541.194 -541.194 -541.194] [47.9255], Avg: [-615.576 -615.576 -615.576] (1.000)
Step: 62149, Reward: [-488.504 -488.504 -488.504] [82.3600], Avg: [-615.54 -615.54 -615.54] (1.000)
Step: 62199, Reward: [-500.418 -500.418 -500.418] [89.9199], Avg: [-615.519 -615.519 -615.519] (1.000)
Step: 62249, Reward: [-412.3 -412.3 -412.3] [12.7103], Avg: [-615.366 -615.366 -615.366] (1.000)
Step: 62299, Reward: [-471.975 -471.975 -471.975] [124.6645], Avg: [-615.351 -615.351 -615.351] (1.000)
Step: 62349, Reward: [-527.698 -527.698 -527.698] [84.1471], Avg: [-615.349 -615.349 -615.349] (1.000)
Step: 62399, Reward: [-449.46 -449.46 -449.46] [80.4427], Avg: [-615.28 -615.28 -615.28] (1.000)
Step: 62449, Reward: [-446.159 -446.159 -446.159] [19.9124], Avg: [-615.161 -615.161 -615.161] (1.000)
Step: 62499, Reward: [-493.903 -493.903 -493.903] [41.3993], Avg: [-615.097 -615.097 -615.097] (1.000)
Step: 62549, Reward: [-526.724 -526.724 -526.724] [132.6956], Avg: [-615.132 -615.132 -615.132] (1.000)
Step: 62599, Reward: [-525.694 -525.694 -525.694] [131.2612], Avg: [-615.166 -615.166 -615.166] (1.000)
Step: 62649, Reward: [-438.33 -438.33 -438.33] [78.0770], Avg: [-615.087 -615.087 -615.087] (1.000)
Step: 62699, Reward: [-579.389 -579.389 -579.389] [79.0678], Avg: [-615.121 -615.121 -615.121] (1.000)
Step: 62749, Reward: [-564.757 -564.757 -564.757] [71.0505], Avg: [-615.138 -615.138 -615.138] (1.000)
Step: 62799, Reward: [-459.333 -459.333 -459.333] [55.3498], Avg: [-615.058 -615.058 -615.058] (1.000)
Step: 62849, Reward: [-534.869 -534.869 -534.869] [51.9095], Avg: [-615.035 -615.035 -615.035] (1.000)
Step: 62899, Reward: [-487.878 -487.878 -487.878] [45.5849], Avg: [-614.97 -614.97 -614.97] (1.000)
Step: 62949, Reward: [-481.006 -481.006 -481.006] [79.7958], Avg: [-614.927 -614.927 -614.927] (1.000)
Step: 62999, Reward: [-521.252 -521.252 -521.252] [130.6749], Avg: [-614.957 -614.957 -614.957] (1.000)
Step: 63049, Reward: [-546.333 -546.333 -546.333] [145.5461], Avg: [-615.018 -615.018 -615.018] (1.000)
Step: 63099, Reward: [-563.427 -563.427 -563.427] [150.4559], Avg: [-615.096 -615.096 -615.096] (1.000)
Step: 63149, Reward: [-523.281 -523.281 -523.281] [104.7866], Avg: [-615.106 -615.106 -615.106] (1.000)
Step: 63199, Reward: [-512.968 -512.968 -512.968] [74.4945], Avg: [-615.085 -615.085 -615.085] (1.000)
Step: 63249, Reward: [-505.319 -505.319 -505.319] [101.8984], Avg: [-615.078 -615.078 -615.078] (1.000)
Step: 63299, Reward: [-617.804 -617.804 -617.804] [132.0859], Avg: [-615.185 -615.185 -615.185] (1.000)
Step: 63349, Reward: [-482.385 -482.385 -482.385] [119.1519], Avg: [-615.174 -615.174 -615.174] (1.000)
Step: 63399, Reward: [-518.725 -518.725 -518.725] [61.4375], Avg: [-615.146 -615.146 -615.146] (1.000)
Step: 63449, Reward: [-535.689 -535.689 -535.689] [119.7106], Avg: [-615.178 -615.178 -615.178] (1.000)
Step: 63499, Reward: [-457.57 -457.57 -457.57] [27.3727], Avg: [-615.076 -615.076 -615.076] (1.000)
Step: 63549, Reward: [-536.268 -536.268 -536.268] [51.2121], Avg: [-615.054 -615.054 -615.054] (1.000)
Step: 63599, Reward: [-568.296 -568.296 -568.296] [77.5729], Avg: [-615.078 -615.078 -615.078] (1.000)
Step: 63649, Reward: [-631.884 -631.884 -631.884] [76.1998], Avg: [-615.151 -615.151 -615.151] (1.000)
Step: 63699, Reward: [-566.883 -566.883 -566.883] [60.0328], Avg: [-615.16 -615.16 -615.16] (1.000)
Step: 63749, Reward: [-606.895 -606.895 -606.895] [129.6511], Avg: [-615.256 -615.256 -615.256] (1.000)
Step: 63799, Reward: [-561.24 -561.24 -561.24] [149.3269], Avg: [-615.33 -615.33 -615.33] (1.000)
Step: 63849, Reward: [-514.346 -514.346 -514.346] [116.8245], Avg: [-615.343 -615.343 -615.343] (1.000)
Step: 63899, Reward: [-514.461 -514.461 -514.461] [97.3246], Avg: [-615.34 -615.34 -615.34] (1.000)
Step: 63949, Reward: [-482.153 -482.153 -482.153] [82.0633], Avg: [-615.3 -615.3 -615.3] (1.000)
Step: 63999, Reward: [-601.227 -601.227 -601.227] [80.3758], Avg: [-615.352 -615.352 -615.352] (1.000)
Step: 64049, Reward: [-559.276 -559.276 -559.276] [69.4053], Avg: [-615.362 -615.362 -615.362] (1.000)
Step: 64099, Reward: [-547.187 -547.187 -547.187] [95.4862], Avg: [-615.383 -615.383 -615.383] (1.000)
Step: 64149, Reward: [-612.192 -612.192 -612.192] [110.2950], Avg: [-615.467 -615.467 -615.467] (1.000)
Step: 64199, Reward: [-614.858 -614.858 -614.858] [93.0511], Avg: [-615.539 -615.539 -615.539] (1.000)
Step: 64249, Reward: [-540.631 -540.631 -540.631] [94.0876], Avg: [-615.554 -615.554 -615.554] (1.000)
Step: 64299, Reward: [-520.29 -520.29 -520.29] [78.3451], Avg: [-615.541 -615.541 -615.541] (1.000)
Step: 64349, Reward: [-532.163 -532.163 -532.163] [90.0850], Avg: [-615.546 -615.546 -615.546] (1.000)
Step: 64399, Reward: [-529.306 -529.306 -529.306] [96.0193], Avg: [-615.554 -615.554 -615.554] (1.000)
Step: 64449, Reward: [-612.203 -612.203 -612.203] [120.0166], Avg: [-615.644 -615.644 -615.644] (1.000)
Step: 64499, Reward: [-566.247 -566.247 -566.247] [130.8355], Avg: [-615.707 -615.707 -615.707] (1.000)
Step: 64549, Reward: [-520.84 -520.84 -520.84] [108.8906], Avg: [-615.718 -615.718 -615.718] (1.000)
Step: 64599, Reward: [-600.189 -600.189 -600.189] [71.8107], Avg: [-615.762 -615.762 -615.762] (1.000)
Step: 64649, Reward: [-553.221 -553.221 -553.221] [84.4120], Avg: [-615.778 -615.778 -615.778] (1.000)
Step: 64699, Reward: [-554.438 -554.438 -554.438] [108.8423], Avg: [-615.815 -615.815 -615.815] (1.000)
Step: 64749, Reward: [-461.211 -461.211 -461.211] [104.6153], Avg: [-615.777 -615.777 -615.777] (1.000)
Step: 64799, Reward: [-553.723 -553.723 -553.723] [71.1128], Avg: [-615.784 -615.784 -615.784] (1.000)
Step: 64849, Reward: [-553.008 -553.008 -553.008] [87.6199], Avg: [-615.803 -615.803 -615.803] (1.000)
Step: 64899, Reward: [-636.47 -636.47 -636.47] [79.3100], Avg: [-615.88 -615.88 -615.88] (1.000)
Step: 64949, Reward: [-565.18 -565.18 -565.18] [71.6888], Avg: [-615.896 -615.896 -615.896] (1.000)
Step: 64999, Reward: [-447.237 -447.237 -447.237] [65.4499], Avg: [-615.817 -615.817 -615.817] (1.000)
Step: 65049, Reward: [-488.95 -488.95 -488.95] [112.1307], Avg: [-615.805 -615.805 -615.805] (1.000)
Step: 65099, Reward: [-624.503 -624.503 -624.503] [53.6656], Avg: [-615.853 -615.853 -615.853] (1.000)
Step: 65149, Reward: [-490.55 -490.55 -490.55] [112.9395], Avg: [-615.844 -615.844 -615.844] (1.000)
Step: 65199, Reward: [-559.533 -559.533 -559.533] [25.1326], Avg: [-615.82 -615.82 -615.82] (1.000)
Step: 65249, Reward: [-519.882 -519.882 -519.882] [82.3366], Avg: [-615.809 -615.809 -615.809] (1.000)
Step: 65299, Reward: [-600.354 -600.354 -600.354] [207.6836], Avg: [-615.956 -615.956 -615.956] (1.000)
Step: 65349, Reward: [-553.349 -553.349 -553.349] [162.5918], Avg: [-616.033 -616.033 -616.033] (1.000)
Step: 65399, Reward: [-523.802 -523.802 -523.802] [85.0623], Avg: [-616.028 -616.028 -616.028] (1.000)
Step: 65449, Reward: [-454.423 -454.423 -454.423] [24.3079], Avg: [-615.923 -615.923 -615.923] (1.000)
Step: 65499, Reward: [-530.864 -530.864 -530.864] [81.7838], Avg: [-615.92 -615.92 -615.92] (1.000)
Step: 65549, Reward: [-598.109 -598.109 -598.109] [163.4137], Avg: [-616.031 -616.031 -616.031] (1.000)
Step: 65599, Reward: [-614.957 -614.957 -614.957] [177.9225], Avg: [-616.166 -616.166 -616.166] (1.000)
Step: 65649, Reward: [-479.973 -479.973 -479.973] [68.9993], Avg: [-616.115 -616.115 -616.115] (1.000)
Step: 65699, Reward: [-596.275 -596.275 -596.275] [98.0709], Avg: [-616.174 -616.174 -616.174] (1.000)
Step: 65749, Reward: [-471.123 -471.123 -471.123] [54.0640], Avg: [-616.105 -616.105 -616.105] (1.000)
Step: 65799, Reward: [-590.617 -590.617 -590.617] [80.7203], Avg: [-616.147 -616.147 -616.147] (1.000)
Step: 65849, Reward: [-572.535 -572.535 -572.535] [31.1742], Avg: [-616.138 -616.138 -616.138] (1.000)
Step: 65899, Reward: [-599.893 -599.893 -599.893] [41.8379], Avg: [-616.157 -616.157 -616.157] (1.000)
Step: 65949, Reward: [-534.795 -534.795 -534.795] [90.5145], Avg: [-616.164 -616.164 -616.164] (1.000)
Step: 65999, Reward: [-573.475 -573.475 -573.475] [96.3640], Avg: [-616.205 -616.205 -616.205] (1.000)
Step: 66049, Reward: [-429.044 -429.044 -429.044] [57.8570], Avg: [-616.107 -616.107 -616.107] (1.000)
Step: 66099, Reward: [-480.495 -480.495 -480.495] [46.5746], Avg: [-616.039 -616.039 -616.039] (1.000)
Step: 66149, Reward: [-508.425 -508.425 -508.425] [78.9967], Avg: [-616.018 -616.018 -616.018] (1.000)
Step: 66199, Reward: [-506.473 -506.473 -506.473] [64.2972], Avg: [-615.984 -615.984 -615.984] (1.000)
Step: 66249, Reward: [-541.803 -541.803 -541.803] [62.7201], Avg: [-615.975 -615.975 -615.975] (1.000)
Step: 66299, Reward: [-592.228 -592.228 -592.228] [51.4418], Avg: [-615.996 -615.996 -615.996] (1.000)
Step: 66349, Reward: [-606.112 -606.112 -606.112] [113.3635], Avg: [-616.074 -616.074 -616.074] (1.000)
Step: 66399, Reward: [-551.08 -551.08 -551.08] [50.0436], Avg: [-616.063 -616.063 -616.063] (1.000)
Step: 66449, Reward: [-597.501 -597.501 -597.501] [117.7770], Avg: [-616.137 -616.137 -616.137] (1.000)
Step: 66499, Reward: [-604.476 -604.476 -604.476] [91.7658], Avg: [-616.197 -616.197 -616.197] (1.000)
Step: 66549, Reward: [-416.793 -416.793 -416.793] [29.4503], Avg: [-616.07 -616.07 -616.07] (1.000)
Step: 66599, Reward: [-563.856 -563.856 -563.856] [153.2636], Avg: [-616.146 -616.146 -616.146] (1.000)
Step: 66649, Reward: [-490.992 -490.992 -490.992] [61.0054], Avg: [-616.098 -616.098 -616.098] (1.000)
Step: 66699, Reward: [-559.209 -559.209 -559.209] [87.2697], Avg: [-616.12 -616.12 -616.12] (1.000)
Step: 66749, Reward: [-586.497 -586.497 -586.497] [87.9263], Avg: [-616.164 -616.164 -616.164] (1.000)
Step: 66799, Reward: [-525.638 -525.638 -525.638] [141.6626], Avg: [-616.202 -616.202 -616.202] (1.000)
Step: 66849, Reward: [-445.504 -445.504 -445.504] [51.8116], Avg: [-616.113 -616.113 -616.113] (1.000)
Step: 66899, Reward: [-466.167 -466.167 -466.167] [60.7440], Avg: [-616.047 -616.047 -616.047] (1.000)
Step: 66949, Reward: [-415.981 -415.981 -415.981] [60.1847], Avg: [-615.942 -615.942 -615.942] (1.000)
Step: 66999, Reward: [-523.007 -523.007 -523.007] [109.4534], Avg: [-615.955 -615.955 -615.955] (1.000)
Step: 67049, Reward: [-562.449 -562.449 -562.449] [87.7067], Avg: [-615.98 -615.98 -615.98] (1.000)
Step: 67099, Reward: [-408.6 -408.6 -408.6] [30.5559], Avg: [-615.848 -615.848 -615.848] (1.000)
Step: 67149, Reward: [-560.303 -560.303 -560.303] [90.0720], Avg: [-615.874 -615.874 -615.874] (1.000)
Step: 67199, Reward: [-446.368 -446.368 -446.368] [85.7734], Avg: [-615.812 -615.812 -615.812] (1.000)
Step: 67249, Reward: [-483.938 -483.938 -483.938] [64.1800], Avg: [-615.761 -615.761 -615.761] (1.000)
Step: 67299, Reward: [-503.845 -503.845 -503.845] [75.3226], Avg: [-615.734 -615.734 -615.734] (1.000)
Step: 67349, Reward: [-516.486 -516.486 -516.486] [40.1913], Avg: [-615.69 -615.69 -615.69] (1.000)
Step: 67399, Reward: [-508.885 -508.885 -508.885] [81.5289], Avg: [-615.672 -615.672 -615.672] (1.000)
Step: 67449, Reward: [-548.499 -548.499 -548.499] [142.7124], Avg: [-615.728 -615.728 -615.728] (1.000)
Step: 67499, Reward: [-535.427 -535.427 -535.427] [103.9901], Avg: [-615.745 -615.745 -615.745] (1.000)
Step: 67549, Reward: [-568.501 -568.501 -568.501] [97.4804], Avg: [-615.782 -615.782 -615.782] (1.000)
Step: 67599, Reward: [-512.98 -512.98 -512.98] [95.4767], Avg: [-615.777 -615.777 -615.777] (1.000)
Step: 67649, Reward: [-571.703 -571.703 -571.703] [118.3901], Avg: [-615.832 -615.832 -615.832] (1.000)
Step: 67699, Reward: [-561.222 -561.222 -561.222] [44.1186], Avg: [-615.824 -615.824 -615.824] (1.000)
Step: 67749, Reward: [-556.527 -556.527 -556.527] [53.5946], Avg: [-615.82 -615.82 -615.82] (1.000)
Step: 67799, Reward: [-525.217 -525.217 -525.217] [64.1445], Avg: [-615.8 -615.8 -615.8] (1.000)
Step: 67849, Reward: [-562.364 -562.364 -562.364] [133.0144], Avg: [-615.859 -615.859 -615.859] (1.000)
Step: 67899, Reward: [-580.475 -580.475 -580.475] [62.4168], Avg: [-615.879 -615.879 -615.879] (1.000)
Step: 67949, Reward: [-545.627 -545.627 -545.627] [138.3100], Avg: [-615.929 -615.929 -615.929] (1.000)
Step: 67999, Reward: [-528.775 -528.775 -528.775] [77.1058], Avg: [-615.922 -615.922 -615.922] (1.000)
Step: 68049, Reward: [-443.812 -443.812 -443.812] [60.8618], Avg: [-615.84 -615.84 -615.84] (1.000)
Step: 68099, Reward: [-559.324 -559.324 -559.324] [151.7998], Avg: [-615.91 -615.91 -615.91] (1.000)
Step: 68149, Reward: [-561.696 -561.696 -561.696] [107.2994], Avg: [-615.949 -615.949 -615.949] (1.000)
Step: 68199, Reward: [-590.418 -590.418 -590.418] [91.2131], Avg: [-615.997 -615.997 -615.997] (1.000)
Step: 68249, Reward: [-546.174 -546.174 -546.174] [131.8120], Avg: [-616.042 -616.042 -616.042] (1.000)
Step: 68299, Reward: [-566.982 -566.982 -566.982] [87.8219], Avg: [-616.071 -616.071 -616.071] (1.000)
Step: 68349, Reward: [-551.363 -551.363 -551.363] [174.1000], Avg: [-616.151 -616.151 -616.151] (1.000)
Step: 68399, Reward: [-589.506 -589.506 -589.506] [113.7972], Avg: [-616.214 -616.214 -616.214] (1.000)
Step: 68449, Reward: [-581.746 -581.746 -581.746] [46.5633], Avg: [-616.223 -616.223 -616.223] (1.000)
Step: 68499, Reward: [-456.716 -456.716 -456.716] [64.1363], Avg: [-616.154 -616.154 -616.154] (1.000)
Step: 68549, Reward: [-656.775 -656.775 -656.775] [216.0297], Avg: [-616.341 -616.341 -616.341] (1.000)
Step: 68599, Reward: [-527.324 -527.324 -527.324] [63.3809], Avg: [-616.322 -616.322 -616.322] (1.000)
Step: 68649, Reward: [-628.492 -628.492 -628.492] [139.8321], Avg: [-616.433 -616.433 -616.433] (1.000)
Step: 68699, Reward: [-566.353 -566.353 -566.353] [128.8747], Avg: [-616.49 -616.49 -616.49] (1.000)
Step: 68749, Reward: [-529.187 -529.187 -529.187] [41.5810], Avg: [-616.457 -616.457 -616.457] (1.000)
Step: 68799, Reward: [-490.792 -490.792 -490.792] [38.4015], Avg: [-616.394 -616.394 -616.394] (1.000)
Step: 68849, Reward: [-534.905 -534.905 -534.905] [74.1484], Avg: [-616.388 -616.388 -616.388] (1.000)
Step: 68899, Reward: [-520.195 -520.195 -520.195] [163.1661], Avg: [-616.437 -616.437 -616.437] (1.000)
Step: 68949, Reward: [-525.97 -525.97 -525.97] [124.7690], Avg: [-616.462 -616.462 -616.462] (1.000)
Step: 68999, Reward: [-617.905 -617.905 -617.905] [136.5550], Avg: [-616.562 -616.562 -616.562] (1.000)
Step: 69049, Reward: [-543.974 -543.974 -543.974] [90.7096], Avg: [-616.575 -616.575 -616.575] (1.000)
Step: 69099, Reward: [-524.586 -524.586 -524.586] [73.0098], Avg: [-616.561 -616.561 -616.561] (1.000)
Step: 69149, Reward: [-561.036 -561.036 -561.036] [100.1675], Avg: [-616.593 -616.593 -616.593] (1.000)
Step: 69199, Reward: [-592.986 -592.986 -592.986] [112.7095], Avg: [-616.658 -616.658 -616.658] (1.000)
Step: 69249, Reward: [-529.72 -529.72 -529.72] [159.5603], Avg: [-616.71 -616.71 -616.71] (1.000)
Step: 69299, Reward: [-568.564 -568.564 -568.564] [108.3611], Avg: [-616.754 -616.754 -616.754] (1.000)
Step: 69349, Reward: [-515.718 -515.718 -515.718] [76.3583], Avg: [-616.736 -616.736 -616.736] (1.000)
Step: 69399, Reward: [-496.025 -496.025 -496.025] [66.0175], Avg: [-616.696 -616.696 -616.696] (1.000)
Step: 69449, Reward: [-481.06 -481.06 -481.06] [49.5937], Avg: [-616.634 -616.634 -616.634] (1.000)
Step: 69499, Reward: [-485.565 -485.565 -485.565] [65.0179], Avg: [-616.587 -616.587 -616.587] (1.000)
Step: 69549, Reward: [-521.026 -521.026 -521.026] [59.9917], Avg: [-616.561 -616.561 -616.561] (1.000)
Step: 69599, Reward: [-646.377 -646.377 -646.377] [110.1825], Avg: [-616.662 -616.662 -616.662] (1.000)
Step: 69649, Reward: [-465.819 -465.819 -465.819] [97.5260], Avg: [-616.624 -616.624 -616.624] (1.000)
Step: 69699, Reward: [-482.609 -482.609 -482.609] [68.2220], Avg: [-616.576 -616.576 -616.576] (1.000)
Step: 69749, Reward: [-544.808 -544.808 -544.808] [128.8969], Avg: [-616.617 -616.617 -616.617] (1.000)
Step: 69799, Reward: [-510.683 -510.683 -510.683] [137.1738], Avg: [-616.64 -616.64 -616.64] (1.000)
Step: 69849, Reward: [-503.082 -503.082 -503.082] [54.1085], Avg: [-616.597 -616.597 -616.597] (1.000)
Step: 69899, Reward: [-497.452 -497.452 -497.452] [45.4536], Avg: [-616.545 -616.545 -616.545] (1.000)
Step: 69949, Reward: [-549.351 -549.351 -549.351] [101.3151], Avg: [-616.569 -616.569 -616.569] (1.000)
Step: 69999, Reward: [-504.585 -504.585 -504.585] [112.0568], Avg: [-616.569 -616.569 -616.569] (1.000)
Step: 70049, Reward: [-575.33 -575.33 -575.33] [134.8866], Avg: [-616.636 -616.636 -616.636] (1.000)
Step: 70099, Reward: [-606.013 -606.013 -606.013] [185.9329], Avg: [-616.761 -616.761 -616.761] (1.000)
Step: 70149, Reward: [-569.468 -569.468 -569.468] [81.5305], Avg: [-616.785 -616.785 -616.785] (1.000)
Step: 70199, Reward: [-486.355 -486.355 -486.355] [133.5451], Avg: [-616.787 -616.787 -616.787] (1.000)
Step: 70249, Reward: [-534.218 -534.218 -534.218] [101.3529], Avg: [-616.801 -616.801 -616.801] (1.000)
Step: 70299, Reward: [-512.245 -512.245 -512.245] [157.8132], Avg: [-616.839 -616.839 -616.839] (1.000)
Step: 70349, Reward: [-486.124 -486.124 -486.124] [169.0017], Avg: [-616.866 -616.866 -616.866] (1.000)
Step: 70399, Reward: [-555.606 -555.606 -555.606] [157.3273], Avg: [-616.934 -616.934 -616.934] (1.000)
Step: 70449, Reward: [-521.645 -521.645 -521.645] [79.4536], Avg: [-616.923 -616.923 -616.923] (1.000)
Step: 70499, Reward: [-568.247 -568.247 -568.247] [113.8400], Avg: [-616.969 -616.969 -616.969] (1.000)
Step: 70549, Reward: [-491.125 -491.125 -491.125] [65.0714], Avg: [-616.926 -616.926 -616.926] (1.000)
Step: 70599, Reward: [-491.617 -491.617 -491.617] [63.1230], Avg: [-616.882 -616.882 -616.882] (1.000)
Step: 70649, Reward: [-529.462 -529.462 -529.462] [115.6905], Avg: [-616.902 -616.902 -616.902] (1.000)
Step: 70699, Reward: [-594.286 -594.286 -594.286] [42.3033], Avg: [-616.916 -616.916 -616.916] (1.000)
Step: 70749, Reward: [-528.619 -528.619 -528.619] [86.6017], Avg: [-616.915 -616.915 -616.915] (1.000)
Step: 70799, Reward: [-519.757 -519.757 -519.757] [83.9255], Avg: [-616.905 -616.905 -616.905] (1.000)
Step: 70849, Reward: [-583.93 -583.93 -583.93] [113.2798], Avg: [-616.962 -616.962 -616.962] (1.000)
Step: 70899, Reward: [-582.436 -582.436 -582.436] [60.7483], Avg: [-616.981 -616.981 -616.981] (1.000)
Step: 70949, Reward: [-515.907 -515.907 -515.907] [70.4166], Avg: [-616.959 -616.959 -616.959] (1.000)
Step: 70999, Reward: [-560.552 -560.552 -560.552] [65.9482], Avg: [-616.966 -616.966 -616.966] (1.000)
Step: 71049, Reward: [-489.705 -489.705 -489.705] [27.7690], Avg: [-616.896 -616.896 -616.896] (1.000)
Step: 71099, Reward: [-549.688 -549.688 -549.688] [70.6689], Avg: [-616.898 -616.898 -616.898] (1.000)
Step: 71149, Reward: [-570.478 -570.478 -570.478] [137.5711], Avg: [-616.962 -616.962 -616.962] (1.000)
Step: 71199, Reward: [-464.79 -464.79 -464.79] [60.7600], Avg: [-616.898 -616.898 -616.898] (1.000)
Step: 71249, Reward: [-501.274 -501.274 -501.274] [134.3692], Avg: [-616.911 -616.911 -616.911] (1.000)
Step: 71299, Reward: [-607.914 -607.914 -607.914] [132.1870], Avg: [-616.998 -616.998 -616.998] (1.000)
Step: 71349, Reward: [-444.188 -444.188 -444.188] [109.9652], Avg: [-616.954 -616.954 -616.954] (1.000)
Step: 71399, Reward: [-603.867 -603.867 -603.867] [137.1350], Avg: [-617.04 -617.04 -617.04] (1.000)
Step: 71449, Reward: [-519.57 -519.57 -519.57] [104.1322], Avg: [-617.045 -617.045 -617.045] (1.000)
Step: 71499, Reward: [-647.129 -647.129 -647.129] [138.2976], Avg: [-617.163 -617.163 -617.163] (1.000)
Step: 71549, Reward: [-582.026 -582.026 -582.026] [135.7562], Avg: [-617.233 -617.233 -617.233] (1.000)
Step: 71599, Reward: [-710.879 -710.879 -710.879] [163.4018], Avg: [-617.413 -617.413 -617.413] (1.000)
Step: 71649, Reward: [-621.525 -621.525 -621.525] [94.8228], Avg: [-617.482 -617.482 -617.482] (1.000)
Step: 71699, Reward: [-479.01 -479.01 -479.01] [70.4451], Avg: [-617.434 -617.434 -617.434] (1.000)
Step: 71749, Reward: [-488.643 -488.643 -488.643] [81.3041], Avg: [-617.401 -617.401 -617.401] (1.000)
Step: 71799, Reward: [-555.994 -555.994 -555.994] [32.9811], Avg: [-617.381 -617.381 -617.381] (1.000)
Step: 71849, Reward: [-529.129 -529.129 -529.129] [39.2249], Avg: [-617.347 -617.347 -617.347] (1.000)
Step: 71899, Reward: [-485.795 -485.795 -485.795] [84.9049], Avg: [-617.315 -617.315 -617.315] (1.000)
Step: 71949, Reward: [-490.615 -490.615 -490.615] [47.6744], Avg: [-617.26 -617.26 -617.26] (1.000)
Step: 71999, Reward: [-618.073 -618.073 -618.073] [152.8108], Avg: [-617.367 -617.367 -617.367] (1.000)
Step: 72049, Reward: [-512.562 -512.562 -512.562] [41.6884], Avg: [-617.323 -617.323 -617.323] (1.000)
Step: 72099, Reward: [-631.816 -631.816 -631.816] [185.7444], Avg: [-617.462 -617.462 -617.462] (1.000)
Step: 72149, Reward: [-474.994 -474.994 -474.994] [100.9946], Avg: [-617.433 -617.433 -617.433] (1.000)
Step: 72199, Reward: [-606.781 -606.781 -606.781] [40.3830], Avg: [-617.453 -617.453 -617.453] (1.000)
Step: 72249, Reward: [-607.78 -607.78 -607.78] [200.3073], Avg: [-617.585 -617.585 -617.585] (1.000)
Step: 72299, Reward: [-578.683 -578.683 -578.683] [91.8526], Avg: [-617.622 -617.622 -617.622] (1.000)
Step: 72349, Reward: [-487.315 -487.315 -487.315] [43.5922], Avg: [-617.562 -617.562 -617.562] (1.000)
Step: 72399, Reward: [-530.268 -530.268 -530.268] [106.5297], Avg: [-617.575 -617.575 -617.575] (1.000)
Step: 72449, Reward: [-514.281 -514.281 -514.281] [85.6201], Avg: [-617.563 -617.563 -617.563] (1.000)
Step: 72499, Reward: [-519.609 -519.609 -519.609] [46.2742], Avg: [-617.527 -617.527 -617.527] (1.000)
Step: 72549, Reward: [-496.843 -496.843 -496.843] [105.7711], Avg: [-617.517 -617.517 -617.517] (1.000)
Step: 72599, Reward: [-552.203 -552.203 -552.203] [80.9120], Avg: [-617.528 -617.528 -617.528] (1.000)
Step: 72649, Reward: [-547.753 -547.753 -547.753] [75.7482], Avg: [-617.532 -617.532 -617.532] (1.000)
Step: 72699, Reward: [-605.366 -605.366 -605.366] [77.1268], Avg: [-617.577 -617.577 -617.577] (1.000)
Step: 72749, Reward: [-517.844 -517.844 -517.844] [105.0392], Avg: [-617.58 -617.58 -617.58] (1.000)
Step: 72799, Reward: [-436.706 -436.706 -436.706] [94.5120], Avg: [-617.521 -617.521 -617.521] (1.000)
Step: 72849, Reward: [-610.119 -610.119 -610.119] [67.2754], Avg: [-617.562 -617.562 -617.562] (1.000)
Step: 72899, Reward: [-633.375 -633.375 -633.375] [123.5239], Avg: [-617.658 -617.658 -617.658] (1.000)
Step: 72949, Reward: [-580.044 -580.044 -580.044] [85.5865], Avg: [-617.691 -617.691 -617.691] (1.000)
Step: 72999, Reward: [-590.692 -590.692 -590.692] [88.8278], Avg: [-617.733 -617.733 -617.733] (1.000)
Step: 73049, Reward: [-497.141 -497.141 -497.141] [129.7984], Avg: [-617.739 -617.739 -617.739] (1.000)
Step: 73099, Reward: [-581.294 -581.294 -581.294] [88.4632], Avg: [-617.775 -617.775 -617.775] (1.000)
Step: 73149, Reward: [-546.507 -546.507 -546.507] [155.6857], Avg: [-617.833 -617.833 -617.833] (1.000)
Step: 73199, Reward: [-516.173 -516.173 -516.173] [50.9221], Avg: [-617.798 -617.798 -617.798] (1.000)
Step: 73249, Reward: [-583.76 -583.76 -583.76] [75.6094], Avg: [-617.826 -617.826 -617.826] (1.000)
Step: 73299, Reward: [-525.386 -525.386 -525.386] [33.6106], Avg: [-617.786 -617.786 -617.786] (1.000)
Step: 73349, Reward: [-517.926 -517.926 -517.926] [57.8531], Avg: [-617.758 -617.758 -617.758] (1.000)
Step: 73399, Reward: [-500.537 -500.537 -500.537] [113.5704], Avg: [-617.755 -617.755 -617.755] (1.000)
Step: 73449, Reward: [-548.684 -548.684 -548.684] [134.4393], Avg: [-617.8 -617.8 -617.8] (1.000)
Step: 73499, Reward: [-479.535 -479.535 -479.535] [92.9718], Avg: [-617.769 -617.769 -617.769] (1.000)
Step: 73549, Reward: [-535.682 -535.682 -535.682] [82.4917], Avg: [-617.769 -617.769 -617.769] (1.000)
Step: 73599, Reward: [-455.404 -455.404 -455.404] [46.1114], Avg: [-617.69 -617.69 -617.69] (1.000)
Step: 73649, Reward: [-551.306 -551.306 -551.306] [132.9346], Avg: [-617.735 -617.735 -617.735] (1.000)
Step: 73699, Reward: [-508.907 -508.907 -508.907] [54.6115], Avg: [-617.698 -617.698 -617.698] (1.000)
Step: 73749, Reward: [-533.903 -533.903 -533.903] [121.7904], Avg: [-617.724 -617.724 -617.724] (1.000)
Step: 73799, Reward: [-495.231 -495.231 -495.231] [103.8209], Avg: [-617.712 -617.712 -617.712] (1.000)
Step: 73849, Reward: [-507.208 -507.208 -507.208] [62.4403], Avg: [-617.679 -617.679 -617.679] (1.000)
Step: 73899, Reward: [-555.663 -555.663 -555.663] [107.0112], Avg: [-617.709 -617.709 -617.709] (1.000)
Step: 73949, Reward: [-475.558 -475.558 -475.558] [69.8928], Avg: [-617.661 -617.661 -617.661] (1.000)
Step: 73999, Reward: [-486.285 -486.285 -486.285] [74.0880], Avg: [-617.622 -617.622 -617.622] (1.000)
Step: 74049, Reward: [-567.9 -567.9 -567.9] [124.3935], Avg: [-617.672 -617.672 -617.672] (1.000)
Step: 74099, Reward: [-529.904 -529.904 -529.904] [77.9307], Avg: [-617.666 -617.666 -617.666] (1.000)
Step: 74149, Reward: [-505.45 -505.45 -505.45] [69.9704], Avg: [-617.637 -617.637 -617.637] (1.000)
Step: 74199, Reward: [-523.464 -523.464 -523.464] [111.1155], Avg: [-617.649 -617.649 -617.649] (1.000)
Step: 74249, Reward: [-569.641 -569.641 -569.641] [181.9626], Avg: [-617.739 -617.739 -617.739] (1.000)
Step: 74299, Reward: [-486.589 -486.589 -486.589] [97.2189], Avg: [-617.716 -617.716 -617.716] (1.000)
Step: 74349, Reward: [-517.145 -517.145 -517.145] [71.2360], Avg: [-617.696 -617.696 -617.696] (1.000)
Step: 74399, Reward: [-501.435 -501.435 -501.435] [41.1077], Avg: [-617.646 -617.646 -617.646] (1.000)
Step: 74449, Reward: [-528.302 -528.302 -528.302] [73.9373], Avg: [-617.635 -617.635 -617.635] (1.000)
Step: 74499, Reward: [-602.63 -602.63 -602.63] [87.4267], Avg: [-617.684 -617.684 -617.684] (1.000)
Step: 74549, Reward: [-523.992 -523.992 -523.992] [53.0993], Avg: [-617.657 -617.657 -617.657] (1.000)
Step: 74599, Reward: [-467.404 -467.404 -467.404] [107.4406], Avg: [-617.628 -617.628 -617.628] (1.000)
Step: 74649, Reward: [-472.209 -472.209 -472.209] [52.7629], Avg: [-617.566 -617.566 -617.566] (1.000)
Step: 74699, Reward: [-520.302 -520.302 -520.302] [95.6133], Avg: [-617.565 -617.565 -617.565] (1.000)
Step: 74749, Reward: [-554.855 -554.855 -554.855] [116.5972], Avg: [-617.601 -617.601 -617.601] (1.000)
Step: 74799, Reward: [-467.765 -467.765 -467.765] [37.4960], Avg: [-617.526 -617.526 -617.526] (1.000)
Step: 74849, Reward: [-573.598 -573.598 -573.598] [116.6831], Avg: [-617.574 -617.574 -617.574] (1.000)
Step: 74899, Reward: [-582.205 -582.205 -582.205] [171.2236], Avg: [-617.665 -617.665 -617.665] (1.000)
Step: 74949, Reward: [-550.2 -550.2 -550.2] [99.0802], Avg: [-617.686 -617.686 -617.686] (1.000)
Step: 74999, Reward: [-519.379 -519.379 -519.379] [73.3817], Avg: [-617.67 -617.67 -617.67] (1.000)
Step: 75049, Reward: [-529.584 -529.584 -529.584] [92.9640], Avg: [-617.673 -617.673 -617.673] (1.000)
Step: 75099, Reward: [-510.334 -510.334 -510.334] [112.6039], Avg: [-617.676 -617.676 -617.676] (1.000)
Step: 75149, Reward: [-561.636 -561.636 -561.636] [131.6817], Avg: [-617.727 -617.727 -617.727] (1.000)
Step: 75199, Reward: [-552.781 -552.781 -552.781] [67.4107], Avg: [-617.728 -617.728 -617.728] (1.000)
Step: 75249, Reward: [-557.618 -557.618 -557.618] [135.0749], Avg: [-617.778 -617.778 -617.778] (1.000)
Step: 75299, Reward: [-528.368 -528.368 -528.368] [66.4146], Avg: [-617.763 -617.763 -617.763] (1.000)
Step: 75349, Reward: [-504.206 -504.206 -504.206] [106.2804], Avg: [-617.758 -617.758 -617.758] (1.000)
Step: 75399, Reward: [-520.866 -520.866 -520.866] [73.3044], Avg: [-617.742 -617.742 -617.742] (1.000)
Step: 75449, Reward: [-547.591 -547.591 -547.591] [53.3244], Avg: [-617.731 -617.731 -617.731] (1.000)
Step: 75499, Reward: [-587.758 -587.758 -587.758] [83.8269], Avg: [-617.767 -617.767 -617.767] (1.000)
Step: 75549, Reward: [-523.873 -523.873 -523.873] [59.6113], Avg: [-617.744 -617.744 -617.744] (1.000)
Step: 75599, Reward: [-491.531 -491.531 -491.531] [72.4050], Avg: [-617.709 -617.709 -617.709] (1.000)
Step: 75649, Reward: [-453.963 -453.963 -453.963] [92.3685], Avg: [-617.661 -617.661 -617.661] (1.000)
Step: 75699, Reward: [-511.014 -511.014 -511.014] [69.5492], Avg: [-617.637 -617.637 -617.637] (1.000)
Step: 75749, Reward: [-430.295 -430.295 -430.295] [59.9240], Avg: [-617.553 -617.553 -617.553] (1.000)
Step: 75799, Reward: [-525.486 -525.486 -525.486] [110.2314], Avg: [-617.565 -617.565 -617.565] (1.000)
Step: 75849, Reward: [-530.988 -530.988 -530.988] [68.1840], Avg: [-617.553 -617.553 -617.553] (1.000)
Step: 75899, Reward: [-568.265 -568.265 -568.265] [105.5675], Avg: [-617.59 -617.59 -617.59] (1.000)
Step: 75949, Reward: [-558.908 -558.908 -558.908] [91.7249], Avg: [-617.612 -617.612 -617.612] (1.000)
Step: 75999, Reward: [-485.457 -485.457 -485.457] [58.1291], Avg: [-617.563 -617.563 -617.563] (1.000)
Step: 76049, Reward: [-545.444 -545.444 -545.444] [78.4008], Avg: [-617.567 -617.567 -617.567] (1.000)
Step: 76099, Reward: [-560.239 -560.239 -560.239] [63.4189], Avg: [-617.571 -617.571 -617.571] (1.000)
Step: 76149, Reward: [-552.81 -552.81 -552.81] [125.4812], Avg: [-617.611 -617.611 -617.611] (1.000)
Step: 76199, Reward: [-452.019 -452.019 -452.019] [86.7650], Avg: [-617.559 -617.559 -617.559] (1.000)
Step: 76249, Reward: [-450.268 -450.268 -450.268] [95.3039], Avg: [-617.512 -617.512 -617.512] (1.000)
Step: 76299, Reward: [-438.954 -438.954 -438.954] [56.2577], Avg: [-617.432 -617.432 -617.432] (1.000)
Step: 76349, Reward: [-503.482 -503.482 -503.482] [23.1491], Avg: [-617.372 -617.372 -617.372] (1.000)
Step: 76399, Reward: [-534.708 -534.708 -534.708] [154.5396], Avg: [-617.419 -617.419 -617.419] (1.000)
Step: 76449, Reward: [-502.763 -502.763 -502.763] [89.3857], Avg: [-617.403 -617.403 -617.403] (1.000)
Step: 76499, Reward: [-485.275 -485.275 -485.275] [74.4226], Avg: [-617.365 -617.365 -617.365] (1.000)
Step: 76549, Reward: [-588.371 -588.371 -588.371] [97.3436], Avg: [-617.41 -617.41 -617.41] (1.000)
Step: 76599, Reward: [-516.589 -516.589 -516.589] [132.4719], Avg: [-617.43 -617.43 -617.43] (1.000)
Step: 76649, Reward: [-496.848 -496.848 -496.848] [82.1910], Avg: [-617.405 -617.405 -617.405] (1.000)
Step: 76699, Reward: [-500.558 -500.558 -500.558] [66.0870], Avg: [-617.372 -617.372 -617.372] (1.000)
Step: 76749, Reward: [-622.131 -622.131 -622.131] [57.6330], Avg: [-617.413 -617.413 -617.413] (1.000)
Step: 76799, Reward: [-569.877 -569.877 -569.877] [158.6720], Avg: [-617.485 -617.485 -617.485] (1.000)
Step: 76849, Reward: [-542.25 -542.25 -542.25] [99.0073], Avg: [-617.501 -617.501 -617.501] (1.000)
Step: 76899, Reward: [-489.262 -489.262 -489.262] [61.8546], Avg: [-617.458 -617.458 -617.458] (1.000)
Step: 76949, Reward: [-562.394 -562.394 -562.394] [111.4540], Avg: [-617.494 -617.494 -617.494] (1.000)
Step: 76999, Reward: [-543.47 -543.47 -543.47] [145.3390], Avg: [-617.541 -617.541 -617.541] (1.000)
Step: 77049, Reward: [-555.967 -555.967 -555.967] [77.5268], Avg: [-617.551 -617.551 -617.551] (1.000)
Step: 77099, Reward: [-550.757 -550.757 -550.757] [98.4025], Avg: [-617.571 -617.571 -617.571] (1.000)
Step: 77149, Reward: [-562.727 -562.727 -562.727] [129.0635], Avg: [-617.619 -617.619 -617.619] (1.000)
Step: 77199, Reward: [-520.178 -520.178 -520.178] [45.4927], Avg: [-617.586 -617.586 -617.586] (1.000)
Step: 77249, Reward: [-653.13 -653.13 -653.13] [157.6529], Avg: [-617.711 -617.711 -617.711] (1.000)
Step: 77299, Reward: [-607.856 -607.856 -607.856] [149.1926], Avg: [-617.801 -617.801 -617.801] (1.000)
Step: 77349, Reward: [-496.816 -496.816 -496.816] [26.9262], Avg: [-617.74 -617.74 -617.74] (1.000)
Step: 77399, Reward: [-519.364 -519.364 -519.364] [177.9322], Avg: [-617.792 -617.792 -617.792] (1.000)
Step: 77449, Reward: [-552.997 -552.997 -552.997] [109.6292], Avg: [-617.821 -617.821 -617.821] (1.000)
Step: 77499, Reward: [-584.076 -584.076 -584.076] [140.9187], Avg: [-617.89 -617.89 -617.89] (1.000)
Step: 77549, Reward: [-550.76 -550.76 -550.76] [142.8589], Avg: [-617.938 -617.938 -617.938] (1.000)
Step: 77599, Reward: [-488.595 -488.595 -488.595] [25.2750], Avg: [-617.871 -617.871 -617.871] (1.000)
Step: 77649, Reward: [-572.487 -572.487 -572.487] [86.9298], Avg: [-617.898 -617.898 -617.898] (1.000)
Step: 77699, Reward: [-471.454 -471.454 -471.454] [29.9454], Avg: [-617.823 -617.823 -617.823] (1.000)
Step: 77749, Reward: [-502.678 -502.678 -502.678] [114.8203], Avg: [-617.823 -617.823 -617.823] (1.000)
Step: 77799, Reward: [-522.755 -522.755 -522.755] [131.1032], Avg: [-617.846 -617.846 -617.846] (1.000)
Step: 77849, Reward: [-505.173 -505.173 -505.173] [106.4674], Avg: [-617.842 -617.842 -617.842] (1.000)
Step: 77899, Reward: [-567.856 -567.856 -567.856] [74.5285], Avg: [-617.858 -617.858 -617.858] (1.000)
Step: 77949, Reward: [-463.696 -463.696 -463.696] [25.4308], Avg: [-617.775 -617.775 -617.775] (1.000)
Step: 77999, Reward: [-447.878 -447.878 -447.878] [62.3754], Avg: [-617.706 -617.706 -617.706] (1.000)
Step: 78049, Reward: [-485.154 -485.154 -485.154] [43.7211], Avg: [-617.65 -617.65 -617.65] (1.000)
Step: 78099, Reward: [-495.272 -495.272 -495.272] [81.0985], Avg: [-617.623 -617.623 -617.623] (1.000)
Step: 78149, Reward: [-618.157 -618.157 -618.157] [150.0741], Avg: [-617.719 -617.719 -617.719] (1.000)
Step: 78199, Reward: [-539.836 -539.836 -539.836] [75.8828], Avg: [-617.718 -617.718 -617.718] (1.000)
Step: 78249, Reward: [-475.936 -475.936 -475.936] [39.4260], Avg: [-617.653 -617.653 -617.653] (1.000)
Step: 78299, Reward: [-600.341 -600.341 -600.341] [75.9776], Avg: [-617.69 -617.69 -617.69] (1.000)
Step: 78349, Reward: [-501.898 -501.898 -501.898] [101.2319], Avg: [-617.681 -617.681 -617.681] (1.000)
Step: 78399, Reward: [-582.074 -582.074 -582.074] [97.6729], Avg: [-617.721 -617.721 -617.721] (1.000)
Step: 78449, Reward: [-538.636 -538.636 -538.636] [116.9413], Avg: [-617.745 -617.745 -617.745] (1.000)
Step: 78499, Reward: [-482.144 -482.144 -482.144] [55.6185], Avg: [-617.694 -617.694 -617.694] (1.000)
Step: 78549, Reward: [-459.443 -459.443 -459.443] [41.7718], Avg: [-617.62 -617.62 -617.62] (1.000)
Step: 78599, Reward: [-554.8 -554.8 -554.8] [70.2665], Avg: [-617.624 -617.624 -617.624] (1.000)
Step: 78649, Reward: [-487.87 -487.87 -487.87] [56.9067], Avg: [-617.578 -617.578 -617.578] (1.000)
Step: 78699, Reward: [-618.651 -618.651 -618.651] [98.7928], Avg: [-617.641 -617.641 -617.641] (1.000)
Step: 78749, Reward: [-488.343 -488.343 -488.343] [92.9684], Avg: [-617.618 -617.618 -617.618] (1.000)
Step: 78799, Reward: [-563.183 -563.183 -563.183] [112.9135], Avg: [-617.655 -617.655 -617.655] (1.000)
Step: 78849, Reward: [-517.118 -517.118 -517.118] [79.4676], Avg: [-617.642 -617.642 -617.642] (1.000)
Step: 78899, Reward: [-482.255 -482.255 -482.255] [55.9708], Avg: [-617.592 -617.592 -617.592] (1.000)
Step: 78949, Reward: [-515.316 -515.316 -515.316] [99.3577], Avg: [-617.59 -617.59 -617.59] (1.000)
Step: 78999, Reward: [-518.431 -518.431 -518.431] [119.9186], Avg: [-617.603 -617.603 -617.603] (1.000)
Step: 79049, Reward: [-562.086 -562.086 -562.086] [89.2216], Avg: [-617.624 -617.624 -617.624] (1.000)
Step: 79099, Reward: [-496.382 -496.382 -496.382] [100.3293], Avg: [-617.611 -617.611 -617.611] (1.000)
Step: 79149, Reward: [-600.833 -600.833 -600.833] [121.0889], Avg: [-617.677 -617.677 -617.677] (1.000)
Step: 79199, Reward: [-564.467 -564.467 -564.467] [129.9514], Avg: [-617.726 -617.726 -617.726] (1.000)
Step: 79249, Reward: [-495.483 -495.483 -495.483] [25.1342], Avg: [-617.664 -617.664 -617.664] (1.000)
Step: 79299, Reward: [-606.977 -606.977 -606.977] [85.0099], Avg: [-617.711 -617.711 -617.711] (1.000)
Step: 79349, Reward: [-568.092 -568.092 -568.092] [163.6391], Avg: [-617.783 -617.783 -617.783] (1.000)
Step: 79399, Reward: [-524.165 -524.165 -524.165] [53.6866], Avg: [-617.758 -617.758 -617.758] (1.000)
Step: 79449, Reward: [-487.72 -487.72 -487.72] [101.5647], Avg: [-617.74 -617.74 -617.74] (1.000)
Step: 79499, Reward: [-638.611 -638.611 -638.611] [110.9555], Avg: [-617.823 -617.823 -617.823] (1.000)
Step: 79549, Reward: [-602.336 -602.336 -602.336] [65.4810], Avg: [-617.854 -617.854 -617.854] (1.000)
Step: 79599, Reward: [-653.391 -653.391 -653.391] [190.4325], Avg: [-617.996 -617.996 -617.996] (1.000)
Step: 79649, Reward: [-696.049 -696.049 -696.049] [176.8629], Avg: [-618.156 -618.156 -618.156] (1.000)
Step: 79699, Reward: [-603.614 -603.614 -603.614] [172.7056], Avg: [-618.255 -618.255 -618.255] (1.000)
Step: 79749, Reward: [-511.103 -511.103 -511.103] [28.9617], Avg: [-618.206 -618.206 -618.206] (1.000)
Step: 79799, Reward: [-575.81 -575.81 -575.81] [148.1423], Avg: [-618.273 -618.273 -618.273] (1.000)
Step: 79849, Reward: [-660.524 -660.524 -660.524] [92.4202], Avg: [-618.357 -618.357 -618.357] (1.000)
Step: 79899, Reward: [-535.817 -535.817 -535.817] [85.8943], Avg: [-618.359 -618.359 -618.359] (1.000)
Step: 79949, Reward: [-530.91 -530.91 -530.91] [131.4504], Avg: [-618.387 -618.387 -618.387] (1.000)
Step: 79999, Reward: [-495.163 -495.163 -495.163] [51.2164], Avg: [-618.342 -618.342 -618.342] (1.000)
Step: 80049, Reward: [-662.596 -662.596 -662.596] [182.4994], Avg: [-618.483 -618.483 -618.483] (1.000)
Step: 80099, Reward: [-489.382 -489.382 -489.382] [24.4593], Avg: [-618.418 -618.418 -618.418] (1.000)
Step: 80149, Reward: [-555.671 -555.671 -555.671] [47.3647], Avg: [-618.408 -618.408 -618.408] (1.000)
Step: 80199, Reward: [-562.693 -562.693 -562.693] [146.6105], Avg: [-618.465 -618.465 -618.465] (1.000)
Step: 80249, Reward: [-536.443 -536.443 -536.443] [98.9967], Avg: [-618.476 -618.476 -618.476] (1.000)
Step: 80299, Reward: [-489.7 -489.7 -489.7] [75.8713], Avg: [-618.443 -618.443 -618.443] (1.000)
Step: 80349, Reward: [-632.319 -632.319 -632.319] [102.1525], Avg: [-618.515 -618.515 -618.515] (1.000)
Step: 80399, Reward: [-535.86 -535.86 -535.86] [188.7806], Avg: [-618.581 -618.581 -618.581] (1.000)
Step: 80449, Reward: [-518.675 -518.675 -518.675] [36.7236], Avg: [-618.542 -618.542 -618.542] (1.000)
Step: 80499, Reward: [-642.428 -642.428 -642.428] [92.8664], Avg: [-618.614 -618.614 -618.614] (1.000)
Step: 80549, Reward: [-505.76 -505.76 -505.76] [102.6259], Avg: [-618.608 -618.608 -618.608] (1.000)
Step: 80599, Reward: [-509.626 -509.626 -509.626] [131.5290], Avg: [-618.622 -618.622 -618.622] (1.000)
Step: 80649, Reward: [-611.184 -611.184 -611.184] [112.5798], Avg: [-618.687 -618.687 -618.687] (1.000)
Step: 80699, Reward: [-517.997 -517.997 -517.997] [49.8228], Avg: [-618.655 -618.655 -618.655] (1.000)
Step: 80749, Reward: [-492.017 -492.017 -492.017] [88.4176], Avg: [-618.632 -618.632 -618.632] (1.000)
Step: 80799, Reward: [-504.512 -504.512 -504.512] [68.9317], Avg: [-618.604 -618.604 -618.604] (1.000)
Step: 80849, Reward: [-461.061 -461.061 -461.061] [91.5850], Avg: [-618.563 -618.563 -618.563] (1.000)
Step: 80899, Reward: [-654.451 -654.451 -654.451] [147.9740], Avg: [-618.677 -618.677 -618.677] (1.000)
Step: 80949, Reward: [-522.053 -522.053 -522.053] [41.6313], Avg: [-618.643 -618.643 -618.643] (1.000)
Step: 80999, Reward: [-519.405 -519.405 -519.405] [79.8240], Avg: [-618.631 -618.631 -618.631] (1.000)
Step: 81049, Reward: [-487.922 -487.922 -487.922] [112.1855], Avg: [-618.619 -618.619 -618.619] (1.000)
Step: 81099, Reward: [-493.035 -493.035 -493.035] [59.4373], Avg: [-618.578 -618.578 -618.578] (1.000)
Step: 81149, Reward: [-502.922 -502.922 -502.922] [86.8476], Avg: [-618.561 -618.561 -618.561] (1.000)
Step: 81199, Reward: [-590.74 -590.74 -590.74] [164.8806], Avg: [-618.645 -618.645 -618.645] (1.000)
Step: 81249, Reward: [-554.963 -554.963 -554.963] [103.1083], Avg: [-618.669 -618.669 -618.669] (1.000)
Step: 81299, Reward: [-488.758 -488.758 -488.758] [62.5690], Avg: [-618.628 -618.628 -618.628] (1.000)
Step: 81349, Reward: [-621.692 -621.692 -621.692] [77.5854], Avg: [-618.678 -618.678 -618.678] (1.000)
Step: 81399, Reward: [-628.344 -628.344 -628.344] [113.7629], Avg: [-618.753 -618.753 -618.753] (1.000)
Step: 81449, Reward: [-528.229 -528.229 -528.229] [73.2335], Avg: [-618.743 -618.743 -618.743] (1.000)
Step: 81499, Reward: [-482.555 -482.555 -482.555] [140.7075], Avg: [-618.745 -618.745 -618.745] (1.000)
Step: 81549, Reward: [-503.165 -503.165 -503.165] [76.7241], Avg: [-618.722 -618.722 -618.722] (1.000)
Step: 81599, Reward: [-562.406 -562.406 -562.406] [70.2312], Avg: [-618.73 -618.73 -618.73] (1.000)
Step: 81649, Reward: [-515.335 -515.335 -515.335] [72.1389], Avg: [-618.711 -618.711 -618.711] (1.000)
Step: 81699, Reward: [-538.463 -538.463 -538.463] [97.6100], Avg: [-618.722 -618.722 -618.722] (1.000)
Step: 81749, Reward: [-637.437 -637.437 -637.437] [173.2586], Avg: [-618.839 -618.839 -618.839] (1.000)
Step: 81799, Reward: [-591.875 -591.875 -591.875] [127.3397], Avg: [-618.9 -618.9 -618.9] (1.000)
Step: 81849, Reward: [-534.918 -534.918 -534.918] [41.1822], Avg: [-618.874 -618.874 -618.874] (1.000)
Step: 81899, Reward: [-528.26 -528.26 -528.26] [141.1590], Avg: [-618.905 -618.905 -618.905] (1.000)
Step: 81949, Reward: [-609.882 -609.882 -609.882] [99.1730], Avg: [-618.96 -618.96 -618.96] (1.000)
Step: 81999, Reward: [-525.592 -525.592 -525.592] [41.9531], Avg: [-618.929 -618.929 -618.929] (1.000)
Step: 82049, Reward: [-524.416 -524.416 -524.416] [114.1676], Avg: [-618.941 -618.941 -618.941] (1.000)
Step: 82099, Reward: [-576.459 -576.459 -576.459] [117.6753], Avg: [-618.987 -618.987 -618.987] (1.000)
Step: 82149, Reward: [-412.978 -412.978 -412.978] [78.9151], Avg: [-618.909 -618.909 -618.909] (1.000)
Step: 82199, Reward: [-569.325 -569.325 -569.325] [88.1957], Avg: [-618.933 -618.933 -618.933] (1.000)
Step: 82249, Reward: [-552.769 -552.769 -552.769] [91.7023], Avg: [-618.948 -618.948 -618.948] (1.000)
Step: 82299, Reward: [-544.671 -544.671 -544.671] [96.2565], Avg: [-618.962 -618.962 -618.962] (1.000)
Step: 82349, Reward: [-521.157 -521.157 -521.157] [107.5235], Avg: [-618.967 -618.967 -618.967] (1.000)
Step: 82399, Reward: [-513.103 -513.103 -513.103] [108.3762], Avg: [-618.969 -618.969 -618.969] (1.000)
Step: 82449, Reward: [-520.145 -520.145 -520.145] [57.2058], Avg: [-618.944 -618.944 -618.944] (1.000)
Step: 82499, Reward: [-597.723 -597.723 -597.723] [146.9331], Avg: [-619.02 -619.02 -619.02] (1.000)
Step: 82549, Reward: [-626.476 -626.476 -626.476] [130.6083], Avg: [-619.104 -619.104 -619.104] (1.000)
Step: 82599, Reward: [-659.604 -659.604 -659.604] [111.8913], Avg: [-619.196 -619.196 -619.196] (1.000)
Step: 82649, Reward: [-498.833 -498.833 -498.833] [62.8828], Avg: [-619.161 -619.161 -619.161] (1.000)
Step: 82699, Reward: [-493.934 -493.934 -493.934] [42.9919], Avg: [-619.111 -619.111 -619.111] (1.000)
Step: 82749, Reward: [-621.696 -621.696 -621.696] [199.4378], Avg: [-619.233 -619.233 -619.233] (1.000)
Step: 82799, Reward: [-541.758 -541.758 -541.758] [75.1852], Avg: [-619.232 -619.232 -619.232] (1.000)
Step: 82849, Reward: [-548.574 -548.574 -548.574] [119.8595], Avg: [-619.262 -619.262 -619.262] (1.000)
Step: 82899, Reward: [-547.272 -547.272 -547.272] [66.6680], Avg: [-619.259 -619.259 -619.259] (1.000)
Step: 82949, Reward: [-528.914 -528.914 -528.914] [106.0681], Avg: [-619.268 -619.268 -619.268] (1.000)
Step: 82999, Reward: [-469.416 -469.416 -469.416] [112.1216], Avg: [-619.245 -619.245 -619.245] (1.000)
Step: 83049, Reward: [-448.738 -448.738 -448.738] [73.8069], Avg: [-619.187 -619.187 -619.187] (1.000)
Step: 83099, Reward: [-582.219 -582.219 -582.219] [115.9527], Avg: [-619.235 -619.235 -619.235] (1.000)
Step: 83149, Reward: [-516.277 -516.277 -516.277] [98.8725], Avg: [-619.232 -619.232 -619.232] (1.000)
Step: 83199, Reward: [-486.525 -486.525 -486.525] [54.8942], Avg: [-619.185 -619.185 -619.185] (1.000)
Step: 83249, Reward: [-481.977 -481.977 -481.977] [69.5261], Avg: [-619.145 -619.145 -619.145] (1.000)
Step: 83299, Reward: [-436.9 -436.9 -436.9] [61.1902], Avg: [-619.072 -619.072 -619.072] (1.000)
Step: 83349, Reward: [-511.807 -511.807 -511.807] [50.4798], Avg: [-619.038 -619.038 -619.038] (1.000)
Step: 83399, Reward: [-435.113 -435.113 -435.113] [59.3026], Avg: [-618.963 -618.963 -618.963] (1.000)
Step: 83449, Reward: [-423.861 -423.861 -423.861] [18.2483], Avg: [-618.857 -618.857 -618.857] (1.000)
Step: 83499, Reward: [-523.225 -523.225 -523.225] [40.7543], Avg: [-618.824 -618.824 -618.824] (1.000)
Step: 83549, Reward: [-460.182 -460.182 -460.182] [50.6525], Avg: [-618.76 -618.76 -618.76] (1.000)
Step: 83599, Reward: [-479.009 -479.009 -479.009] [56.4400], Avg: [-618.71 -618.71 -618.71] (1.000)
Step: 83649, Reward: [-463.591 -463.591 -463.591] [52.7131], Avg: [-618.649 -618.649 -618.649] (1.000)
Step: 83699, Reward: [-557.13 -557.13 -557.13] [144.6427], Avg: [-618.698 -618.698 -618.698] (1.000)
Step: 83749, Reward: [-400.569 -400.569 -400.569] [39.9412], Avg: [-618.592 -618.592 -618.592] (1.000)
Step: 83799, Reward: [-534.444 -534.444 -534.444] [78.2330], Avg: [-618.589 -618.589 -618.589] (1.000)
Step: 83849, Reward: [-580.369 -580.369 -580.369] [79.0959], Avg: [-618.613 -618.613 -618.613] (1.000)
Step: 83899, Reward: [-503.152 -503.152 -503.152] [99.6872], Avg: [-618.603 -618.603 -618.603] (1.000)
Step: 83949, Reward: [-471.361 -471.361 -471.361] [55.2578], Avg: [-618.549 -618.549 -618.549] (1.000)
Step: 83999, Reward: [-510.37 -510.37 -510.37] [113.5891], Avg: [-618.552 -618.552 -618.552] (1.000)
Step: 84049, Reward: [-531.103 -531.103 -531.103] [51.2120], Avg: [-618.53 -618.53 -618.53] (1.000)
Step: 84099, Reward: [-503.654 -503.654 -503.654] [91.6142], Avg: [-618.517 -618.517 -618.517] (1.000)
Step: 84149, Reward: [-424.97 -424.97 -424.97] [40.3045], Avg: [-618.425 -618.425 -618.425] (1.000)
Step: 84199, Reward: [-540.641 -540.641 -540.641] [110.7973], Avg: [-618.445 -618.445 -618.445] (1.000)
Step: 84249, Reward: [-460.813 -460.813 -460.813] [59.2654], Avg: [-618.387 -618.387 -618.387] (1.000)
Step: 84299, Reward: [-466.439 -466.439 -466.439] [54.0963], Avg: [-618.329 -618.329 -618.329] (1.000)
Step: 84349, Reward: [-494.074 -494.074 -494.074] [62.8032], Avg: [-618.292 -618.292 -618.292] (1.000)
Step: 84399, Reward: [-520.297 -520.297 -520.297] [100.5688], Avg: [-618.294 -618.294 -618.294] (1.000)
Step: 84449, Reward: [-554.579 -554.579 -554.579] [126.6022], Avg: [-618.331 -618.331 -618.331] (1.000)
Step: 84499, Reward: [-434.211 -434.211 -434.211] [32.2643], Avg: [-618.241 -618.241 -618.241] (1.000)
Step: 84549, Reward: [-513.492 -513.492 -513.492] [100.5515], Avg: [-618.239 -618.239 -618.239] (1.000)
Step: 84599, Reward: [-483.286 -483.286 -483.286] [106.1467], Avg: [-618.222 -618.222 -618.222] (1.000)
Step: 84649, Reward: [-545.102 -545.102 -545.102] [57.8908], Avg: [-618.213 -618.213 -618.213] (1.000)
Step: 84699, Reward: [-710.252 -710.252 -710.252] [152.9661], Avg: [-618.357 -618.357 -618.357] (1.000)
Step: 84749, Reward: [-506.751 -506.751 -506.751] [96.1519], Avg: [-618.348 -618.348 -618.348] (1.000)
Step: 84799, Reward: [-511.193 -511.193 -511.193] [65.5304], Avg: [-618.324 -618.324 -618.324] (1.000)
Step: 84849, Reward: [-516.73 -516.73 -516.73] [48.6503], Avg: [-618.292 -618.292 -618.292] (1.000)
Step: 84899, Reward: [-536.858 -536.858 -536.858] [79.3542], Avg: [-618.291 -618.291 -618.291] (1.000)
Step: 84949, Reward: [-504.062 -504.062 -504.062] [57.4658], Avg: [-618.258 -618.258 -618.258] (1.000)
Step: 84999, Reward: [-626.698 -626.698 -626.698] [162.6117], Avg: [-618.358 -618.358 -618.358] (1.000)
Step: 85049, Reward: [-555.723 -555.723 -555.723] [92.9184], Avg: [-618.376 -618.376 -618.376] (1.000)
Step: 85099, Reward: [-466.53 -466.53 -466.53] [58.1262], Avg: [-618.321 -618.321 -618.321] (1.000)
Step: 85149, Reward: [-761.683 -761.683 -761.683] [74.4250], Avg: [-618.449 -618.449 -618.449] (1.000)
Step: 85199, Reward: [-586.603 -586.603 -586.603] [81.1786], Avg: [-618.478 -618.478 -618.478] (1.000)
Step: 85249, Reward: [-689.196 -689.196 -689.196] [114.7767], Avg: [-618.587 -618.587 -618.587] (1.000)
Step: 85299, Reward: [-537.704 -537.704 -537.704] [72.8296], Avg: [-618.582 -618.582 -618.582] (1.000)
Step: 85349, Reward: [-641.642 -641.642 -641.642] [180.8043], Avg: [-618.701 -618.701 -618.701] (1.000)
Step: 85399, Reward: [-537.843 -537.843 -537.843] [79.0551], Avg: [-618.7 -618.7 -618.7] (1.000)
Step: 85449, Reward: [-465.964 -465.964 -465.964] [58.4069], Avg: [-618.645 -618.645 -618.645] (1.000)
Step: 85499, Reward: [-571.876 -571.876 -571.876] [154.8003], Avg: [-618.708 -618.708 -618.708] (1.000)
Step: 85549, Reward: [-571.63 -571.63 -571.63] [104.4254], Avg: [-618.742 -618.742 -618.742] (1.000)
Step: 85599, Reward: [-677.13 -677.13 -677.13] [107.0597], Avg: [-618.839 -618.839 -618.839] (1.000)
Step: 85649, Reward: [-581.346 -581.346 -581.346] [102.2125], Avg: [-618.876 -618.876 -618.876] (1.000)
Step: 85699, Reward: [-512.508 -512.508 -512.508] [101.9426], Avg: [-618.874 -618.874 -618.874] (1.000)
Step: 85749, Reward: [-534.54 -534.54 -534.54] [59.9914], Avg: [-618.86 -618.86 -618.86] (1.000)
Step: 85799, Reward: [-571.68 -571.68 -571.68] [148.7761], Avg: [-618.919 -618.919 -618.919] (1.000)
Step: 85849, Reward: [-505.508 -505.508 -505.508] [79.1184], Avg: [-618.899 -618.899 -618.899] (1.000)
Step: 85899, Reward: [-526.819 -526.819 -526.819] [46.4569], Avg: [-618.872 -618.872 -618.872] (1.000)
Step: 85949, Reward: [-498.868 -498.868 -498.868] [80.1805], Avg: [-618.849 -618.849 -618.849] (1.000)
Step: 85999, Reward: [-496.61 -496.61 -496.61] [91.0317], Avg: [-618.831 -618.831 -618.831] (1.000)
Step: 86049, Reward: [-587.999 -587.999 -587.999] [162.8676], Avg: [-618.908 -618.908 -618.908] (1.000)
Step: 86099, Reward: [-642.859 -642.859 -642.859] [129.9938], Avg: [-618.997 -618.997 -618.997] (1.000)
Step: 86149, Reward: [-527.171 -527.171 -527.171] [146.7256], Avg: [-619.029 -619.029 -619.029] (1.000)
Step: 86199, Reward: [-478.773 -478.773 -478.773] [65.4245], Avg: [-618.986 -618.986 -618.986] (1.000)
Step: 86249, Reward: [-467.162 -467.162 -467.162] [49.9552], Avg: [-618.926 -618.926 -618.926] (1.000)
Step: 86299, Reward: [-547.018 -547.018 -547.018] [202.4860], Avg: [-619.002 -619.002 -619.002] (1.000)
Step: 86349, Reward: [-634.938 -634.938 -634.938] [157.9751], Avg: [-619.103 -619.103 -619.103] (1.000)
Step: 86399, Reward: [-531.81 -531.81 -531.81] [136.5373], Avg: [-619.131 -619.131 -619.131] (1.000)
Step: 86449, Reward: [-552.982 -552.982 -552.982] [72.0717], Avg: [-619.135 -619.135 -619.135] (1.000)
Step: 86499, Reward: [-482.411 -482.411 -482.411] [56.7447], Avg: [-619.088 -619.088 -619.088] (1.000)
Step: 86549, Reward: [-639.402 -639.402 -639.402] [135.5013], Avg: [-619.179 -619.179 -619.179] (1.000)
Step: 86599, Reward: [-574.882 -574.882 -574.882] [116.4709], Avg: [-619.22 -619.22 -619.22] (1.000)
Step: 86649, Reward: [-574.499 -574.499 -574.499] [123.3801], Avg: [-619.266 -619.266 -619.266] (1.000)
Step: 86699, Reward: [-630.886 -630.886 -630.886] [76.6139], Avg: [-619.316 -619.316 -619.316] (1.000)
Step: 86749, Reward: [-629.859 -629.859 -629.859] [167.9727], Avg: [-619.419 -619.419 -619.419] (1.000)
Step: 86799, Reward: [-552.758 -552.758 -552.758] [124.8011], Avg: [-619.453 -619.453 -619.453] (1.000)
Step: 86849, Reward: [-521.889 -521.889 -521.889] [74.7602], Avg: [-619.44 -619.44 -619.44] (1.000)
Step: 86899, Reward: [-607.648 -607.648 -607.648] [115.4594], Avg: [-619.499 -619.499 -619.499] (1.000)
Step: 86949, Reward: [-649.76 -649.76 -649.76] [222.0531], Avg: [-619.644 -619.644 -619.644] (1.000)
Step: 86999, Reward: [-507.227 -507.227 -507.227] [78.2417], Avg: [-619.625 -619.625 -619.625] (1.000)
Step: 87049, Reward: [-602.895 -602.895 -602.895] [175.5752], Avg: [-619.716 -619.716 -619.716] (1.000)
Step: 87099, Reward: [-530.993 -530.993 -530.993] [68.2354], Avg: [-619.704 -619.704 -619.704] (1.000)
Step: 87149, Reward: [-629.681 -629.681 -629.681] [173.9652], Avg: [-619.81 -619.81 -619.81] (1.000)
Step: 87199, Reward: [-524.423 -524.423 -524.423] [103.9260], Avg: [-619.815 -619.815 -619.815] (1.000)
Step: 87249, Reward: [-534.324 -534.324 -534.324] [88.4357], Avg: [-619.816 -619.816 -619.816] (1.000)
Step: 87299, Reward: [-689.367 -689.367 -689.367] [69.7936], Avg: [-619.896 -619.896 -619.896] (1.000)
Step: 87349, Reward: [-592.242 -592.242 -592.242] [105.2051], Avg: [-619.941 -619.941 -619.941] (1.000)
Step: 87399, Reward: [-603.97 -603.97 -603.97] [87.7291], Avg: [-619.982 -619.982 -619.982] (1.000)
Step: 87449, Reward: [-440.797 -440.797 -440.797] [111.5067], Avg: [-619.943 -619.943 -619.943] (1.000)
Step: 87499, Reward: [-495.643 -495.643 -495.643] [45.1607], Avg: [-619.898 -619.898 -619.898] (1.000)
Step: 87549, Reward: [-646.902 -646.902 -646.902] [85.4381], Avg: [-619.962 -619.962 -619.962] (1.000)
Step: 87599, Reward: [-568.145 -568.145 -568.145] [133.1049], Avg: [-620.008 -620.008 -620.008] (1.000)
Step: 87649, Reward: [-546.193 -546.193 -546.193] [102.1920], Avg: [-620.025 -620.025 -620.025] (1.000)
Step: 87699, Reward: [-627.183 -627.183 -627.183] [134.7242], Avg: [-620.105 -620.105 -620.105] (1.000)
Step: 87749, Reward: [-492.787 -492.787 -492.787] [119.9033], Avg: [-620.101 -620.101 -620.101] (1.000)
Step: 87799, Reward: [-608.194 -608.194 -608.194] [157.8958], Avg: [-620.184 -620.184 -620.184] (1.000)
Step: 87849, Reward: [-599.253 -599.253 -599.253] [149.0138], Avg: [-620.257 -620.257 -620.257] (1.000)
Step: 87899, Reward: [-487.698 -487.698 -487.698] [60.8405], Avg: [-620.216 -620.216 -620.216] (1.000)
Step: 87949, Reward: [-517.802 -517.802 -517.802] [97.0570], Avg: [-620.213 -620.213 -620.213] (1.000)
Step: 87999, Reward: [-525.997 -525.997 -525.997] [147.3581], Avg: [-620.244 -620.244 -620.244] (1.000)
Step: 88049, Reward: [-593.707 -593.707 -593.707] [111.0327], Avg: [-620.292 -620.292 -620.292] (1.000)
Step: 88099, Reward: [-522.291 -522.291 -522.291] [62.6453], Avg: [-620.272 -620.272 -620.272] (1.000)
Step: 88149, Reward: [-477.488 -477.488 -477.488] [30.9012], Avg: [-620.208 -620.208 -620.208] (1.000)
Step: 88199, Reward: [-545.942 -545.942 -545.942] [120.4221], Avg: [-620.234 -620.234 -620.234] (1.000)
Step: 88249, Reward: [-534.815 -534.815 -534.815] [100.3073], Avg: [-620.243 -620.243 -620.243] (1.000)
Step: 88299, Reward: [-545.945 -545.945 -545.945] [108.2443], Avg: [-620.262 -620.262 -620.262] (1.000)
Step: 88349, Reward: [-559.43 -559.43 -559.43] [82.4307], Avg: [-620.274 -620.274 -620.274] (1.000)
Step: 88399, Reward: [-551.221 -551.221 -551.221] [141.8777], Avg: [-620.315 -620.315 -620.315] (1.000)
Step: 88449, Reward: [-453.059 -453.059 -453.059] [57.8390], Avg: [-620.253 -620.253 -620.253] (1.000)
