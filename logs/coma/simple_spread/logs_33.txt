Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 4,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 4
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 32

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
# 		del self.network
# 		n_agents = len(action_size)
# 		n_actions = action_size[0][-1]
# 		n_obs = state_size[0][-1]
# 		state_len = int(np.sum([np.prod(space) for space in state_size]))
# 		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
# 		groups = {"agents": n_agents}
# 		scheme = {
# 			"state": {"vshape": state_len},
# 			"obs": {"vshape": n_obs, "group": "agents"},
# 			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
# 			"reward": {"vshape": (1,)},
# 			"done": {"vshape": (1,), "dtype": torch.uint8},
# 		}
		
# 		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
# 		self.replay_buffer = ReplayBuffer(scheme, groups, 2000, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
# 		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
# 		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.episode_batch = self.new_episode_batch(NUM_ENVS)
# 		self.mac.init_hidden(batch_size=NUM_ENVS)
# 		self.num_envs = NUM_ENVS
# 		self.time = 0
# 		self.replay_buffer2 = MultiagentReplayBuffer(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
# 		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
# 		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
# 		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
# 		state_joint = np.concatenate(state, -1)
# 		obs = np.concatenate(state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
# 		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		self.episode_batch.update({"state": [state_joint], "obs": [obs]}, ts=self.step)
# 		actions = self.mac.select_actions(self.episode_batch, inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
# 		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
# 		return np.split(self.action, actions.size(-1), axis=-2)

# 	def train(self, state, action, next_state, reward, done):
# 		actions, rewards, dones = [list(zip(*x)) for x in [action, reward, done]]
# 		actions_one_hot = [np.argmax(a, -1) for a in actions]
# 		rewards = [np.mean(rewards, -1)]
# 		dones = [np.any(dones, -1)]
# 		obs = np.concatenate(state, -2)
# 		next_obs = np.concatenate(next_state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		actor_inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		post_transition_data = {"actions": actions_one_hot, "reward": rewards, "done": dones}
# 		self.replay_buffer2.add(state, action, next_state, reward, done)
# 		self.episode_batch.update(post_transition_data, ts=self.step)
# 		if np.any(done[0]):
# 			state_joint = np.concatenate(state, -1)
# 			self.episode_batch.update({"state": [state_joint], "obs": [next_obs]}, ts=self.step)
# 			actions = self.mac.select_actions(self.episode_batch, actor_inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 			self.episode_batch.update({"actions": actions}, ts=self.step)
# 			self.replay_buffer.insert_episode_batch(self.episode_batch)
# 			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
# 				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
# 				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, self.device)
# 				max_ep_t = episode_sample.max_t_filled()
# 				episode_sample = episode_sample[:, :max_ep_t]
# 				if episode_sample.device != self.device: episode_sample.to(self.device)
# 				self.learner.train(episode_sample)
# 			self.episode_batch = self.new_episode_batch(state[0].shape[0])
# 			self.mac.init_hidden(self.num_envs)
# 			self.time += self.step
# 			self.step = 0

# class OneHot():
# 	def __init__(self, out_dim):
# 		self.out_dim = out_dim

# 	def transform(self, tensor):
# 		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
# 		y_onehot.scatter_(-1, tensor.long(), 1)
# 		return y_onehot.float()

# 	def infer_output_info(self, vshape_in, dtype_in):
# 		return (self.out_dim,), torch.float32

# class COMALearner():
# 	def __init__(self, mac, scheme, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.last_target_update_step = 0
# 		self.mac = mac
# 		self.critic_training_steps = 0
# 		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
# 		self.critic_params = list(self.critic.parameters())
# 		self.agent_params = list(mac.parameters())
# 		self.params = self.agent_params + self.critic_params
# 		self.target_critic = copy.deepcopy(self.critic)
# 		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
# 		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

# 	def train(self, batch):
# 		# Get the relevant quantities
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length
# 		rewards = batch["reward"][:, :-1]
# 		actions = batch["actions"][:, :]
# 		done = batch["done"][:, :-1].float()
# 		mask = batch["filled"][:, :-1].float()
# 		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
# 		critic_mask = mask.clone()
# 		mask = mask.repeat(1, 1, self.n_agents).view(-1)
# 		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
# 		actions = actions[:,:-1]
# 		mac_out = []
# 		self.mac.init_hidden(batch.batch_size)
# 		for t in range(batch.max_seq_length - 1):
# 			agent_outs = self.mac.forward(batch, t=t)
# 			mac_out.append(agent_outs)
# 		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
# 		# Mask out unavailable actions, renormalise (as in action selection)
# 		q_vals = q_vals.reshape(-1, self.n_actions)
# 		pi = mac_out.view(-1, self.n_actions)
# 		baseline = (pi * q_vals).sum(-1).detach()
# 		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken[mask == 0] = 1.0
# 		log_pi_taken = torch.log(pi_taken)
# 		advantages = (q_taken - baseline).detach()
# 		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
# 		self.agent_optimiser.zero_grad()
# 		coma_loss.backward()
# 		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
# 		self.agent_optimiser.step()
# 		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
# 			self._update_targets()
# 			self.last_target_update_step = self.critic_training_steps

# 	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
# 		target_q_vals = self.target_critic(batch)[:, :]
# 		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
# 		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
# 		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
# 		for t in reversed(range(rewards.size(1))):
# 			mask_t = mask[:, t].expand(-1, self.n_agents)
# 			if mask_t.sum() == 0:
# 				continue
# 			q_t = self.critic(batch, t)
# 			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
# 			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
# 			targets_t = targets[:, t]
# 			td_error = (q_taken - targets_t.detach())
# 			# 0-out the targets that came from padded data
# 			masked_td_error = td_error * mask_t
# 			loss = (masked_td_error ** 2).sum() / mask_t.sum()
# 			self.critic_optimiser.zero_grad()
# 			loss.backward()
# 			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
# 			self.critic_optimiser.step()
# 			self.critic_training_steps += 1
# 		return q_vals

# 	def _update_targets(self):
# 		self.target_critic.load_state_dict(self.critic.state_dict())

# 	def cuda(self):
# 		self.mac.cuda()
# 		self.critic.cuda()
# 		self.target_critic.cuda()

# def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
# 	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
# 	# Initialise  last  lambda -return  for  not  done  episodes
# 	ret = target_qs.new_zeros(*target_qs.shape)
# 	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
# 	# Backwards  recursive  update  of the "forward  view"
# 	for t in range(ret.shape[1] - 2, -1,  -1):
# 		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
# 	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
# 	return ret[:, 0:-1]

# class COMACritic(torch.nn.Module):
# 	def __init__(self, scheme, n_agents, n_actions):
# 		super(COMACritic, self).__init__()
# 		self.n_actions = n_actions
# 		self.n_agents = n_agents
# 		input_shape = self._get_input_shape(scheme)
# 		self.output_type = "q"
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

# 	def forward(self, batch, t=None):
# 		inputs = self._build_inputs(batch, t=t)
# 		x = torch.relu(self.fc1(inputs))
# 		x = torch.relu(self.fc2(x))
# 		q = self.fc3(x)
# 		return q

# 	def _build_inputs(self, batch, t=None):
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length if t is None else 1
# 		ts = slice(None) if t is None else slice(t, t+1)
# 		inputs = []
# 		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
# 		inputs.append(batch["obs"][:, ts])
# 		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
# 		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
# 		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
# 		# last actions
# 		if t == 0:
# 			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		elif isinstance(t, int):
# 			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		else:
# 			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
# 			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 			inputs.append(last_actions)

# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["state"]["vshape"]
# 		input_shape += scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
# 		input_shape += self.n_agents
# 		return input_shape

# class BasicMAC:
# 	def __init__(self, scheme, groups, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
# 		self.action_selector = MultinomialActionSelector()
# 		self.hidden_states = None

# 	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
# 		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
# 		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
# 		return chosen_actions

# 	def forward(self, ep_batch, inputs, t, test_mode=False):
# 		agent_inputs = self._build_inputs(ep_batch, t)
# 		agent_outs = self.agent(agent_inputs, self.hidden_states)
# 		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
# 		if not test_mode:
# 			epsilon_action_num = agent_outs.size(-1)
# 			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
# 		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

# 	def init_hidden(self, batch_size):
# 		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

# 	def parameters(self):
# 		return self.agent.parameters()

# 	def _build_inputs(self, batch, t):
# 		bs = batch.batch_size
# 		inputs = []
# 		inputs.append(batch["obs"][:, t])  # b1av
# 		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0]
# 		input_shape += self.n_agents
# 		return input_shape

# class RNNAgent(torch.nn.Module):
# 	def __init__(self, input_shape, output_shape):
# 		super(RNNAgent, self).__init__()
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		# self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

# 	def init_hidden(self):
# 		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

# 	def forward(self, inputs, hidden_state):
# 		x = torch.relu(self.fc1(inputs))
# 		# h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
# 		# h = self.rnn(x, h_in)
# 		x = self.fc3(x).relu()
# 		q = self.fc2(x)
# 		return q

# class MultinomialActionSelector():
# 	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
# 		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
# 		self.epsilon = self.schedule.eval(0)

# 	def select_action(self, agent_inputs, t_env, test_mode=False):
# 		self.epsilon = self.schedule.eval(t_env)
# 		masked_policies = agent_inputs.clone()
# 		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
# 		return picked_actions

# class DecayThenFlatSchedule():
# 	def __init__(self, start, finish, time_length, decay="exp"):
# 		self.start = start
# 		self.finish = finish
# 		self.time_length = time_length
# 		self.delta = (self.start - self.finish) / self.time_length
# 		self.decay = decay
# 		if self.decay in ["exp"]:
# 			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

# 	def eval(self, T):
# 		if self.decay in ["linear"]:
# 			return max(self.finish, self.start - self.delta * T)
# 		elif self.decay in ["exp"]:
# 			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

# from types import SimpleNamespace as SN

# class EpisodeBatch():
# 	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
# 		self.scheme = scheme.copy()
# 		self.groups = groups
# 		self.batch_size = batch_size
# 		self.max_seq_length = max_seq_length
# 		self.preprocess = {} if preprocess is None else preprocess
# 		self.device = device

# 		if data is not None:
# 			self.data = data
# 		else:
# 			self.data = SN()
# 			self.data.transition_data = {}
# 			self.data.episode_data = {}
# 			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

# 	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
# 		if preprocess is not None:
# 			for k in preprocess:
# 				assert k in scheme
# 				new_k = preprocess[k][0]
# 				transforms = preprocess[k][1]
# 				vshape = self.scheme[k]["vshape"]
# 				dtype = self.scheme[k]["dtype"]
# 				for transform in transforms:
# 					vshape, dtype = transform.infer_output_info(vshape, dtype)
# 				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
# 				if "group" in self.scheme[k]:
# 					self.scheme[new_k]["group"] = self.scheme[k]["group"]
# 				if "episode_const" in self.scheme[k]:
# 					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

# 		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
# 		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

# 		for field_key, field_info in scheme.items():
# 			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
# 			vshape = field_info["vshape"]
# 			episode_const = field_info.get("episode_const", False)
# 			group = field_info.get("group", None)
# 			dtype = field_info.get("dtype", torch.float32)

# 			if isinstance(vshape, int):
# 				vshape = (vshape,)
# 			if group:
# 				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
# 				shape = (groups[group], *vshape)
# 			else:
# 				shape = vshape
# 			if episode_const:
# 				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
# 			else:
# 				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

# 	def extend(self, scheme, groups=None):
# 		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

# 	def to(self, device):
# 		for k, v in self.data.transition_data.items():
# 			self.data.transition_data[k] = v.to(device)
# 		for k, v in self.data.episode_data.items():
# 			self.data.episode_data[k] = v.to(device)
# 		self.device = device

# 	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
# 		slices = self._parse_slices((bs, ts))
# 		for k, v in data.items():
# 			if k in self.data.transition_data:
# 				target = self.data.transition_data
# 				if mark_filled:
# 					target["filled"][slices] = 1
# 					mark_filled = False
# 				_slices = slices
# 			elif k in self.data.episode_data:
# 				target = self.data.episode_data
# 				_slices = slices[0]
# 			else:
# 				raise KeyError("{} not found in transition or episode data".format(k))

# 			dtype = self.scheme[k].get("dtype", torch.float32)
# 			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
# 			self._check_safe_view(v, target[k][_slices])
# 			target[k][_slices] = v.view_as(target[k][_slices])

# 			if k in self.preprocess:
# 				new_k = self.preprocess[k][0]
# 				v = target[k][_slices]
# 				for transform in self.preprocess[k][1]:
# 					v = transform.transform(v)
# 				target[new_k][_slices] = v.view_as(target[new_k][_slices])

# 	def _check_safe_view(self, v, dest):
# 		idx = len(v.shape) - 1
# 		for s in dest.shape[::-1]:
# 			if v.shape[idx] != s:
# 				if s != 1:
# 					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
# 			else:
# 				idx -= 1

# 	def __getitem__(self, item):
# 		if isinstance(item, str):
# 			if item in self.data.episode_data:
# 				return self.data.episode_data[item]
# 			elif item in self.data.transition_data:
# 				return self.data.transition_data[item]
# 			else:
# 				raise ValueError
# 		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
# 			new_data = self._new_data_sn()
# 			for key in item:
# 				if key in self.data.transition_data:
# 					new_data.transition_data[key] = self.data.transition_data[key]
# 				elif key in self.data.episode_data:
# 					new_data.episode_data[key] = self.data.episode_data[key]
# 				else:
# 					raise KeyError("Unrecognised key {}".format(key))

# 			# Update the scheme to only have the requested keys
# 			new_scheme = {key: self.scheme[key] for key in item}
# 			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
# 						for key in item if "group" in self.scheme[key]}
# 			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
# 			return ret
# 		else:
# 			item = self._parse_slices(item)
# 			new_data = self._new_data_sn()
# 			for k, v in self.data.transition_data.items():
# 				new_data.transition_data[k] = v[item]
# 			for k, v in self.data.episode_data.items():
# 				new_data.episode_data[k] = v[item[0]]

# 			ret_bs = self._get_num_items(item[0], self.batch_size)
# 			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

# 			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
# 			return ret

# 	def _get_num_items(self, indexing_item, max_size):
# 		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
# 			return len(indexing_item)
# 		elif isinstance(indexing_item, slice):
# 			_range = indexing_item.indices(max_size)
# 			return 1 + (_range[1] - _range[0] - 1)//_range[2]

# 	def _new_data_sn(self):
# 		new_data = SN()
# 		new_data.transition_data = {}
# 		new_data.episode_data = {}
# 		return new_data

# 	def _parse_slices(self, items):
# 		parsed = []
# 		# Only batch slice given, add full time slice
# 		if (isinstance(items, slice)  # slice a:b
# 			or isinstance(items, int)  # int i
# 			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
# 			):
# 			items = (items, slice(None))

# 		# Need the time indexing to be contiguous
# 		if isinstance(items[1], list):
# 			raise IndexError("Indexing across Time must be contiguous")

# 		for item in items:
# 			#TODO: stronger checks to ensure only supported options get through
# 			if isinstance(item, int):
# 				# Convert single indices to slices
# 				parsed.append(slice(item, item+1))
# 			else:
# 				# Leave slices and lists as is
# 				parsed.append(item)
# 		return parsed

# 	def max_t_filled(self):
# 		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

# class ReplayBuffer(EpisodeBatch):
# 	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
# 		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
# 		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
# 		self.buffer_index = 0
# 		self.episodes_in_buffer = 0

# 	def insert_episode_batch(self, ep_batch):
# 		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
# 			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
# 			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
# 			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
# 			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
# 			self.buffer_index = self.buffer_index % self.buffer_size
# 			assert self.buffer_index < self.buffer_size
# 		else:
# 			buffer_left = self.buffer_size - self.buffer_index
# 			self.insert_episode_batch(ep_batch[0:buffer_left, :])
# 			self.insert_episode_batch(ep_batch[buffer_left:, :])

# 	def can_sample(self, batch_size):
# 		return self.episodes_in_buffer >= batch_size

# 	def sample(self, batch_size):
# 		assert self.can_sample(batch_size)
# 		if self.episodes_in_buffer == batch_size:
# 			return self[:batch_size]
# 		else:
# 			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
# 			return self[ep_ids]


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_probs = gsoftmax(self.action_probs(state), hard=False)
		return action_probs

	def init_hidden(self, batch_size=1):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		q_values = self.q_values(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

	def get_value(self, state, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean())

			hidden = model.actor_local.hidden
			# action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
			action_probs = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
			model.step(model.actor_optimizer, actor_loss.mean())
			model.actor_local.hidden = hidden

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		# state_list = self.to_tensor(state)
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
			actor_inputs.append(actor_input)
		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
		action = action_random if numpy and random.random() < eps else action_greedy
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()

			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs]]
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[4], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-492.227 -492.227 -492.227] [56.330], Avg: [-492.227 -492.227 -492.227] (1.0000) ({r_i: None, r_t: [-8.580 -8.580 -8.580], eps: 1.0})
Step:     100, Reward: [-473.461 -473.461 -473.461] [87.371], Avg: [-482.844 -482.844 -482.844] (0.9900) ({r_i: None, r_t: [-963.252 -963.252 -963.252], eps: 0.99})
Step:     200, Reward: [-467.364 -467.364 -467.364] [34.683], Avg: [-477.684 -477.684 -477.684] (0.9801) ({r_i: None, r_t: [-1039.977 -1039.977 -1039.977], eps: 0.98})
Step:     300, Reward: [-504.552 -504.552 -504.552] [43.253], Avg: [-484.401 -484.401 -484.401] (0.9704) ({r_i: None, r_t: [-1016.076 -1016.076 -1016.076], eps: 0.97})
Step:     400, Reward: [-475.851 -475.851 -475.851] [90.395], Avg: [-482.691 -482.691 -482.691] (0.9607) ({r_i: None, r_t: [-976.159 -976.159 -976.159], eps: 0.961})
Step:     500, Reward: [-467.855 -467.855 -467.855] [33.160], Avg: [-480.219 -480.219 -480.219] (0.9511) ({r_i: None, r_t: [-921.174 -921.174 -921.174], eps: 0.951})
Step:     600, Reward: [-409.216 -409.216 -409.216] [22.780], Avg: [-470.075 -470.075 -470.075] (0.9416) ({r_i: None, r_t: [-1027.260 -1027.260 -1027.260], eps: 0.942})
Step:     700, Reward: [-548.447 -548.447 -548.447] [86.953], Avg: [-479.872 -479.872 -479.872] (0.9322) ({r_i: None, r_t: [-903.819 -903.819 -903.819], eps: 0.932})
Step:     800, Reward: [-483.166 -483.166 -483.166] [60.997], Avg: [-480.238 -480.238 -480.238] (0.9229) ({r_i: None, r_t: [-1028.674 -1028.674 -1028.674], eps: 0.923})
Step:     900, Reward: [-458.176 -458.176 -458.176] [66.966], Avg: [-478.032 -478.032 -478.032] (0.9137) ({r_i: None, r_t: [-942.563 -942.563 -942.563], eps: 0.914})
Step:    1000, Reward: [-559.719 -559.719 -559.719] [131.224], Avg: [-485.458 -485.458 -485.458] (0.9046) ({r_i: None, r_t: [-954.126 -954.126 -954.126], eps: 0.905})
Step:    1100, Reward: [-450.548 -450.548 -450.548] [55.077], Avg: [-482.549 -482.549 -482.549] (0.8956) ({r_i: None, r_t: [-1046.846 -1046.846 -1046.846], eps: 0.896})
Step:    1200, Reward: [-606.075 -606.075 -606.075] [150.935], Avg: [-492.051 -492.051 -492.051] (0.8867) ({r_i: None, r_t: [-880.572 -880.572 -880.572], eps: 0.887})
Step:    1300, Reward: [-489.514 -489.514 -489.514] [89.057], Avg: [-491.869 -491.869 -491.869] (0.8778) ({r_i: None, r_t: [-981.616 -981.616 -981.616], eps: 0.878})
Step:    1400, Reward: [-543.167 -543.167 -543.167] [61.964], Avg: [-495.289 -495.289 -495.289] (0.8691) ({r_i: None, r_t: [-1054.055 -1054.055 -1054.055], eps: 0.869})
Step:    1500, Reward: [-569.480 -569.480 -569.480] [32.069], Avg: [-499.926 -499.926 -499.926] (0.8604) ({r_i: None, r_t: [-1054.013 -1054.013 -1054.013], eps: 0.86})
Step:    1600, Reward: [-530.987 -530.987 -530.987] [63.544], Avg: [-501.753 -501.753 -501.753] (0.8518) ({r_i: None, r_t: [-848.566 -848.566 -848.566], eps: 0.852})
Step:    1700, Reward: [-421.483 -421.483 -421.483] [40.953], Avg: [-497.294 -497.294 -497.294] (0.8433) ({r_i: None, r_t: [-997.333 -997.333 -997.333], eps: 0.843})
Step:    1800, Reward: [-522.226 -522.226 -522.226] [32.581], Avg: [-498.606 -498.606 -498.606] (0.8349) ({r_i: None, r_t: [-931.738 -931.738 -931.738], eps: 0.835})
Step:    1900, Reward: [-513.974 -513.974 -513.974] [81.211], Avg: [-499.374 -499.374 -499.374] (0.8266) ({r_i: None, r_t: [-996.594 -996.594 -996.594], eps: 0.827})
Step:    2000, Reward: [-477.594 -477.594 -477.594] [64.142], Avg: [-498.337 -498.337 -498.337] (0.8183) ({r_i: None, r_t: [-1061.600 -1061.600 -1061.600], eps: 0.818})
Step:    2100, Reward: [-513.863 -513.863 -513.863] [58.896], Avg: [-499.043 -499.043 -499.043] (0.8102) ({r_i: None, r_t: [-1123.124 -1123.124 -1123.124], eps: 0.81})
Step:    2200, Reward: [-499.017 -499.017 -499.017] [88.622], Avg: [-499.042 -499.042 -499.042] (0.8021) ({r_i: None, r_t: [-982.224 -982.224 -982.224], eps: 0.802})
Step:    2300, Reward: [-499.214 -499.214 -499.214] [65.889], Avg: [-499.049 -499.049 -499.049] (0.7941) ({r_i: None, r_t: [-1031.248 -1031.248 -1031.248], eps: 0.794})
Step:    2400, Reward: [-485.279 -485.279 -485.279] [57.120], Avg: [-498.498 -498.498 -498.498] (0.7862) ({r_i: None, r_t: [-920.748 -920.748 -920.748], eps: 0.786})
Step:    2500, Reward: [-501.532 -501.532 -501.532] [65.570], Avg: [-498.615 -498.615 -498.615] (0.7783) ({r_i: None, r_t: [-977.411 -977.411 -977.411], eps: 0.778})
Step:    2600, Reward: [-599.828 -599.828 -599.828] [95.832], Avg: [-502.364 -502.364 -502.364] (0.7705) ({r_i: None, r_t: [-1096.710 -1096.710 -1096.710], eps: 0.771})
Step:    2700, Reward: [-445.395 -445.395 -445.395] [62.957], Avg: [-500.329 -500.329 -500.329] (0.7629) ({r_i: None, r_t: [-1032.575 -1032.575 -1032.575], eps: 0.763})
Step:    2800, Reward: [-550.923 -550.923 -550.923] [113.713], Avg: [-502.074 -502.074 -502.074] (0.7553) ({r_i: None, r_t: [-1103.644 -1103.644 -1103.644], eps: 0.755})
Step:    2900, Reward: [-416.123 -416.123 -416.123] [68.679], Avg: [-499.209 -499.209 -499.209] (0.7477) ({r_i: None, r_t: [-1008.664 -1008.664 -1008.664], eps: 0.748})
Step:    3000, Reward: [-441.961 -441.961 -441.961] [21.761], Avg: [-497.362 -497.362 -497.362] (0.7403) ({r_i: None, r_t: [-1020.893 -1020.893 -1020.893], eps: 0.74})
Step:    3100, Reward: [-617.619 -617.619 -617.619] [145.126], Avg: [-501.120 -501.120 -501.120] (0.7329) ({r_i: None, r_t: [-983.544 -983.544 -983.544], eps: 0.733})
Step:    3200, Reward: [-519.680 -519.680 -519.680] [67.076], Avg: [-501.682 -501.682 -501.682] (0.7256) ({r_i: None, r_t: [-1142.645 -1142.645 -1142.645], eps: 0.726})
Step:    3300, Reward: [-463.282 -463.282 -463.282] [106.649], Avg: [-500.553 -500.553 -500.553] (0.7183) ({r_i: None, r_t: [-958.566 -958.566 -958.566], eps: 0.718})
Step:    3400, Reward: [-758.938 -758.938 -758.938] [85.511], Avg: [-507.935 -507.935 -507.935] (0.7112) ({r_i: None, r_t: [-1070.173 -1070.173 -1070.173], eps: 0.711})
Step:    3500, Reward: [-555.335 -555.335 -555.335] [54.728], Avg: [-509.252 -509.252 -509.252] (0.7041) ({r_i: None, r_t: [-997.698 -997.698 -997.698], eps: 0.704})
Step:    3600, Reward: [-658.625 -658.625 -658.625] [97.239], Avg: [-513.289 -513.289 -513.289] (0.6970) ({r_i: None, r_t: [-1108.548 -1108.548 -1108.548], eps: 0.697})
Step:    3700, Reward: [-504.200 -504.200 -504.200] [88.336], Avg: [-513.050 -513.050 -513.050] (0.6901) ({r_i: None, r_t: [-1115.956 -1115.956 -1115.956], eps: 0.69})
Step:    3800, Reward: [-488.748 -488.748 -488.748] [95.163], Avg: [-512.427 -512.427 -512.427] (0.6832) ({r_i: None, r_t: [-1002.819 -1002.819 -1002.819], eps: 0.683})
Step:    3900, Reward: [-542.777 -542.777 -542.777] [43.292], Avg: [-513.186 -513.186 -513.186] (0.6764) ({r_i: None, r_t: [-1003.342 -1003.342 -1003.342], eps: 0.676})
Step:    4000, Reward: [-447.234 -447.234 -447.234] [74.207], Avg: [-511.577 -511.577 -511.577] (0.6696) ({r_i: None, r_t: [-1118.692 -1118.692 -1118.692], eps: 0.67})
Step:    4100, Reward: [-521.110 -521.110 -521.110] [63.016], Avg: [-511.804 -511.804 -511.804] (0.6630) ({r_i: None, r_t: [-1041.116 -1041.116 -1041.116], eps: 0.663})
Step:    4200, Reward: [-489.562 -489.562 -489.562] [56.395], Avg: [-511.287 -511.287 -511.287] (0.6564) ({r_i: None, r_t: [-1155.709 -1155.709 -1155.709], eps: 0.656})
Step:    4300, Reward: [-479.432 -479.432 -479.432] [66.215], Avg: [-510.563 -510.563 -510.563] (0.6498) ({r_i: None, r_t: [-1083.158 -1083.158 -1083.158], eps: 0.65})
Step:    4400, Reward: [-480.704 -480.704 -480.704] [95.419], Avg: [-509.899 -509.899 -509.899] (0.6433) ({r_i: None, r_t: [-1214.202 -1214.202 -1214.202], eps: 0.643})
Step:    4500, Reward: [-595.076 -595.076 -595.076] [159.220], Avg: [-511.751 -511.751 -511.751] (0.6369) ({r_i: None, r_t: [-955.912 -955.912 -955.912], eps: 0.637})
Step:    4600, Reward: [-581.716 -581.716 -581.716] [106.788], Avg: [-513.239 -513.239 -513.239] (0.6306) ({r_i: None, r_t: [-1167.262 -1167.262 -1167.262], eps: 0.631})
Step:    4700, Reward: [-514.591 -514.591 -514.591] [27.228], Avg: [-513.268 -513.268 -513.268] (0.6243) ({r_i: None, r_t: [-1065.696 -1065.696 -1065.696], eps: 0.624})
Step:    4800, Reward: [-517.203 -517.203 -517.203] [115.992], Avg: [-513.348 -513.348 -513.348] (0.6180) ({r_i: None, r_t: [-1041.853 -1041.853 -1041.853], eps: 0.618})
Step:    4900, Reward: [-661.716 -661.716 -661.716] [96.589], Avg: [-516.315 -516.315 -516.315] (0.6119) ({r_i: None, r_t: [-1181.421 -1181.421 -1181.421], eps: 0.612})
Step:    5000, Reward: [-633.287 -633.287 -633.287] [84.978], Avg: [-518.609 -518.609 -518.609] (0.6058) ({r_i: None, r_t: [-1069.163 -1069.163 -1069.163], eps: 0.606})
Step:    5100, Reward: [-527.290 -527.290 -527.290] [42.862], Avg: [-518.776 -518.776 -518.776] (0.5997) ({r_i: None, r_t: [-1075.140 -1075.140 -1075.140], eps: 0.6})
Step:    5200, Reward: [-556.817 -556.817 -556.817] [129.347], Avg: [-519.494 -519.494 -519.494] (0.5937) ({r_i: None, r_t: [-1083.596 -1083.596 -1083.596], eps: 0.594})
Step:    5300, Reward: [-650.179 -650.179 -650.179] [58.969], Avg: [-521.914 -521.914 -521.914] (0.5878) ({r_i: None, r_t: [-963.485 -963.485 -963.485], eps: 0.588})
Step:    5400, Reward: [-668.971 -668.971 -668.971] [159.771], Avg: [-524.587 -524.587 -524.587] (0.5820) ({r_i: None, r_t: [-1149.946 -1149.946 -1149.946], eps: 0.582})
Step:    5500, Reward: [-526.623 -526.623 -526.623] [53.524], Avg: [-524.624 -524.624 -524.624] (0.5762) ({r_i: None, r_t: [-1300.566 -1300.566 -1300.566], eps: 0.576})
Step:    5600, Reward: [-583.192 -583.192 -583.192] [68.906], Avg: [-525.651 -525.651 -525.651] (0.5704) ({r_i: None, r_t: [-1264.614 -1264.614 -1264.614], eps: 0.57})
Step:    5700, Reward: [-550.157 -550.157 -550.157] [98.455], Avg: [-526.074 -526.074 -526.074] (0.5647) ({r_i: None, r_t: [-1161.239 -1161.239 -1161.239], eps: 0.565})
Step:    5800, Reward: [-507.066 -507.066 -507.066] [92.167], Avg: [-525.752 -525.752 -525.752] (0.5591) ({r_i: None, r_t: [-1134.056 -1134.056 -1134.056], eps: 0.559})
Step:    5900, Reward: [-613.839 -613.839 -613.839] [103.050], Avg: [-527.220 -527.220 -527.220] (0.5535) ({r_i: None, r_t: [-1282.765 -1282.765 -1282.765], eps: 0.554})
Step:    6000, Reward: [-476.301 -476.301 -476.301] [68.086], Avg: [-526.385 -526.385 -526.385] (0.5480) ({r_i: None, r_t: [-1264.573 -1264.573 -1264.573], eps: 0.548})
Step:    6100, Reward: [-625.939 -625.939 -625.939] [137.501], Avg: [-527.991 -527.991 -527.991] (0.5425) ({r_i: None, r_t: [-1302.741 -1302.741 -1302.741], eps: 0.543})
Step:    6200, Reward: [-524.304 -524.304 -524.304] [74.925], Avg: [-527.932 -527.932 -527.932] (0.5371) ({r_i: None, r_t: [-979.017 -979.017 -979.017], eps: 0.537})
Step:    6300, Reward: [-451.594 -451.594 -451.594] [101.890], Avg: [-526.739 -526.739 -526.739] (0.5318) ({r_i: None, r_t: [-850.267 -850.267 -850.267], eps: 0.532})
Step:    6400, Reward: [-431.389 -431.389 -431.389] [47.651], Avg: [-525.272 -525.272 -525.272] (0.5264) ({r_i: None, r_t: [-904.472 -904.472 -904.472], eps: 0.526})
Step:    6500, Reward: [-483.800 -483.800 -483.800] [93.705], Avg: [-524.644 -524.644 -524.644] (0.5212) ({r_i: None, r_t: [-1126.859 -1126.859 -1126.859], eps: 0.521})
Step:    6600, Reward: [-404.243 -404.243 -404.243] [34.375], Avg: [-522.847 -522.847 -522.847] (0.5160) ({r_i: None, r_t: [-972.014 -972.014 -972.014], eps: 0.516})
Step:    6700, Reward: [-389.573 -389.573 -389.573] [24.563], Avg: [-520.887 -520.887 -520.887] (0.5108) ({r_i: None, r_t: [-856.825 -856.825 -856.825], eps: 0.511})
Step:    6800, Reward: [-472.394 -472.394 -472.394] [60.924], Avg: [-520.184 -520.184 -520.184] (0.5058) ({r_i: None, r_t: [-961.165 -961.165 -961.165], eps: 0.506})
Step:    6900, Reward: [-484.298 -484.298 -484.298] [80.186], Avg: [-519.672 -519.672 -519.672] (0.5007) ({r_i: None, r_t: [-1057.275 -1057.275 -1057.275], eps: 0.501})
Step:    7000, Reward: [-433.589 -433.589 -433.589] [61.819], Avg: [-518.459 -518.459 -518.459] (0.4957) ({r_i: None, r_t: [-806.087 -806.087 -806.087], eps: 0.496})
Step:    7100, Reward: [-426.062 -426.062 -426.062] [91.429], Avg: [-517.176 -517.176 -517.176] (0.4908) ({r_i: None, r_t: [-933.559 -933.559 -933.559], eps: 0.491})
Step:    7200, Reward: [-492.877 -492.877 -492.877] [98.271], Avg: [-516.843 -516.843 -516.843] (0.4859) ({r_i: None, r_t: [-995.958 -995.958 -995.958], eps: 0.486})
Step:    7300, Reward: [-489.200 -489.200 -489.200] [84.713], Avg: [-516.470 -516.470 -516.470] (0.4810) ({r_i: None, r_t: [-942.556 -942.556 -942.556], eps: 0.481})
Step:    7400, Reward: [-394.609 -394.609 -394.609] [101.433], Avg: [-514.845 -514.845 -514.845] (0.4762) ({r_i: None, r_t: [-905.747 -905.747 -905.747], eps: 0.476})
Step:    7500, Reward: [-452.299 -452.299 -452.299] [33.740], Avg: [-514.022 -514.022 -514.022] (0.4715) ({r_i: None, r_t: [-885.493 -885.493 -885.493], eps: 0.471})
Step:    7600, Reward: [-415.713 -415.713 -415.713] [55.845], Avg: [-512.745 -512.745 -512.745] (0.4668) ({r_i: None, r_t: [-863.968 -863.968 -863.968], eps: 0.467})
Step:    7700, Reward: [-466.189 -466.189 -466.189] [40.527], Avg: [-512.148 -512.148 -512.148] (0.4621) ({r_i: None, r_t: [-863.593 -863.593 -863.593], eps: 0.462})
Step:    7800, Reward: [-484.258 -484.258 -484.258] [45.423], Avg: [-511.795 -511.795 -511.795] (0.4575) ({r_i: None, r_t: [-969.167 -969.167 -969.167], eps: 0.458})
Step:    7900, Reward: [-434.192 -434.192 -434.192] [30.379], Avg: [-510.825 -510.825 -510.825] (0.4529) ({r_i: None, r_t: [-922.087 -922.087 -922.087], eps: 0.453})
Step:    8000, Reward: [-438.221 -438.221 -438.221] [36.562], Avg: [-509.929 -509.929 -509.929] (0.4484) ({r_i: None, r_t: [-946.804 -946.804 -946.804], eps: 0.448})
Step:    8100, Reward: [-434.351 -434.351 -434.351] [85.989], Avg: [-509.007 -509.007 -509.007] (0.4440) ({r_i: None, r_t: [-910.754 -910.754 -910.754], eps: 0.444})
Step:    8200, Reward: [-418.494 -418.494 -418.494] [41.123], Avg: [-507.917 -507.917 -507.917] (0.4395) ({r_i: None, r_t: [-912.372 -912.372 -912.372], eps: 0.44})
Step:    8300, Reward: [-419.354 -419.354 -419.354] [63.527], Avg: [-506.862 -506.862 -506.862] (0.4351) ({r_i: None, r_t: [-1012.333 -1012.333 -1012.333], eps: 0.435})
Step:    8400, Reward: [-444.450 -444.450 -444.450] [25.929], Avg: [-506.128 -506.128 -506.128] (0.4308) ({r_i: None, r_t: [-940.001 -940.001 -940.001], eps: 0.431})
Step:    8500, Reward: [-419.945 -419.945 -419.945] [25.927], Avg: [-505.126 -505.126 -505.126] (0.4265) ({r_i: None, r_t: [-877.853 -877.853 -877.853], eps: 0.427})
Step:    8600, Reward: [-421.073 -421.073 -421.073] [70.652], Avg: [-504.160 -504.160 -504.160] (0.4223) ({r_i: None, r_t: [-817.941 -817.941 -817.941], eps: 0.422})
Step:    8700, Reward: [-415.696 -415.696 -415.696] [41.754], Avg: [-503.154 -503.154 -503.154] (0.4180) ({r_i: None, r_t: [-1027.065 -1027.065 -1027.065], eps: 0.418})
Step:    8800, Reward: [-570.525 -570.525 -570.525] [174.545], Avg: [-503.911 -503.911 -503.911] (0.4139) ({r_i: None, r_t: [-897.517 -897.517 -897.517], eps: 0.414})
Step:    8900, Reward: [-374.735 -374.735 -374.735] [37.977], Avg: [-502.476 -502.476 -502.476] (0.4097) ({r_i: None, r_t: [-1002.841 -1002.841 -1002.841], eps: 0.41})
Step:    9000, Reward: [-488.504 -488.504 -488.504] [122.575], Avg: [-502.323 -502.323 -502.323] (0.4057) ({r_i: None, r_t: [-934.347 -934.347 -934.347], eps: 0.406})
Step:    9100, Reward: [-412.741 -412.741 -412.741] [58.994], Avg: [-501.349 -501.349 -501.349] (0.4016) ({r_i: None, r_t: [-909.439 -909.439 -909.439], eps: 0.402})
Step:    9200, Reward: [-434.694 -434.694 -434.694] [89.962], Avg: [-500.632 -500.632 -500.632] (0.3976) ({r_i: None, r_t: [-931.840 -931.840 -931.840], eps: 0.398})
Step:    9300, Reward: [-537.544 -537.544 -537.544] [87.345], Avg: [-501.025 -501.025 -501.025] (0.3936) ({r_i: None, r_t: [-834.330 -834.330 -834.330], eps: 0.394})
Step:    9400, Reward: [-495.506 -495.506 -495.506] [106.989], Avg: [-500.967 -500.967 -500.967] (0.3897) ({r_i: None, r_t: [-918.970 -918.970 -918.970], eps: 0.39})
Step:    9500, Reward: [-447.895 -447.895 -447.895] [104.697], Avg: [-500.414 -500.414 -500.414] (0.3858) ({r_i: None, r_t: [-881.615 -881.615 -881.615], eps: 0.386})
Step:    9600, Reward: [-503.278 -503.278 -503.278] [83.245], Avg: [-500.443 -500.443 -500.443] (0.3820) ({r_i: None, r_t: [-850.351 -850.351 -850.351], eps: 0.382})
Step:    9700, Reward: [-490.149 -490.149 -490.149] [128.973], Avg: [-500.338 -500.338 -500.338] (0.3782) ({r_i: None, r_t: [-892.155 -892.155 -892.155], eps: 0.378})
Step:    9800, Reward: [-464.856 -464.856 -464.856] [62.930], Avg: [-499.980 -499.980 -499.980] (0.3744) ({r_i: None, r_t: [-987.343 -987.343 -987.343], eps: 0.374})
Step:    9900, Reward: [-437.326 -437.326 -437.326] [79.932], Avg: [-499.353 -499.353 -499.353] (0.3707) ({r_i: None, r_t: [-908.703 -908.703 -908.703], eps: 0.371})
Step:   10000, Reward: [-393.863 -393.863 -393.863] [53.144], Avg: [-498.309 -498.309 -498.309] (0.3670) ({r_i: None, r_t: [-1018.268 -1018.268 -1018.268], eps: 0.367})
Step:   10100, Reward: [-572.710 -572.710 -572.710] [112.186], Avg: [-499.038 -499.038 -499.038] (0.3633) ({r_i: None, r_t: [-846.013 -846.013 -846.013], eps: 0.363})
Step:   10200, Reward: [-411.304 -411.304 -411.304] [42.427], Avg: [-498.187 -498.187 -498.187] (0.3597) ({r_i: None, r_t: [-886.931 -886.931 -886.931], eps: 0.36})
Step:   10300, Reward: [-490.786 -490.786 -490.786] [92.265], Avg: [-498.115 -498.115 -498.115] (0.3561) ({r_i: None, r_t: [-970.317 -970.317 -970.317], eps: 0.356})
Step:   10400, Reward: [-453.165 -453.165 -453.165] [80.894], Avg: [-497.687 -497.687 -497.687] (0.3525) ({r_i: None, r_t: [-1005.895 -1005.895 -1005.895], eps: 0.353})
Step:   10500, Reward: [-447.831 -447.831 -447.831] [142.103], Avg: [-497.217 -497.217 -497.217] (0.3490) ({r_i: None, r_t: [-904.589 -904.589 -904.589], eps: 0.349})
Step:   10600, Reward: [-450.601 -450.601 -450.601] [155.335], Avg: [-496.781 -496.781 -496.781] (0.3455) ({r_i: None, r_t: [-837.024 -837.024 -837.024], eps: 0.346})
Step:   10700, Reward: [-439.161 -439.161 -439.161] [87.346], Avg: [-496.248 -496.248 -496.248] (0.3421) ({r_i: None, r_t: [-868.063 -868.063 -868.063], eps: 0.342})
Step:   10800, Reward: [-474.985 -474.985 -474.985] [45.231], Avg: [-496.053 -496.053 -496.053] (0.3387) ({r_i: None, r_t: [-818.404 -818.404 -818.404], eps: 0.339})
Step:   10900, Reward: [-650.642 -650.642 -650.642] [188.245], Avg: [-497.458 -497.458 -497.458] (0.3353) ({r_i: None, r_t: [-940.943 -940.943 -940.943], eps: 0.335})
Step:   11000, Reward: [-420.201 -420.201 -420.201] [46.807], Avg: [-496.762 -496.762 -496.762] (0.3320) ({r_i: None, r_t: [-935.025 -935.025 -935.025], eps: 0.332})
Step:   11100, Reward: [-417.978 -417.978 -417.978] [49.909], Avg: [-496.059 -496.059 -496.059] (0.3286) ({r_i: None, r_t: [-977.825 -977.825 -977.825], eps: 0.329})
Step:   11200, Reward: [-403.690 -403.690 -403.690] [49.071], Avg: [-495.241 -495.241 -495.241] (0.3254) ({r_i: None, r_t: [-916.521 -916.521 -916.521], eps: 0.325})
Step:   11300, Reward: [-404.281 -404.281 -404.281] [67.001], Avg: [-494.443 -494.443 -494.443] (0.3221) ({r_i: None, r_t: [-890.226 -890.226 -890.226], eps: 0.322})
Step:   11400, Reward: [-429.311 -429.311 -429.311] [29.916], Avg: [-493.877 -493.877 -493.877] (0.3189) ({r_i: None, r_t: [-963.613 -963.613 -963.613], eps: 0.319})
Step:   11500, Reward: [-475.174 -475.174 -475.174] [29.339], Avg: [-493.716 -493.716 -493.716] (0.3157) ({r_i: None, r_t: [-922.645 -922.645 -922.645], eps: 0.316})
Step:   11600, Reward: [-554.905 -554.905 -554.905] [73.439], Avg: [-494.239 -494.239 -494.239] (0.3126) ({r_i: None, r_t: [-873.581 -873.581 -873.581], eps: 0.313})
Step:   11700, Reward: [-448.476 -448.476 -448.476] [80.198], Avg: [-493.851 -493.851 -493.851] (0.3095) ({r_i: None, r_t: [-963.047 -963.047 -963.047], eps: 0.309})
Step:   11800, Reward: [-448.057 -448.057 -448.057] [64.604], Avg: [-493.466 -493.466 -493.466] (0.3064) ({r_i: None, r_t: [-866.896 -866.896 -866.896], eps: 0.306})
Step:   11900, Reward: [-483.841 -483.841 -483.841] [174.373], Avg: [-493.386 -493.386 -493.386] (0.3033) ({r_i: None, r_t: [-920.289 -920.289 -920.289], eps: 0.303})
Step:   12000, Reward: [-431.989 -431.989 -431.989] [83.966], Avg: [-492.878 -492.878 -492.878] (0.3003) ({r_i: None, r_t: [-777.443 -777.443 -777.443], eps: 0.3})
Step:   12100, Reward: [-445.761 -445.761 -445.761] [62.736], Avg: [-492.492 -492.492 -492.492] (0.2973) ({r_i: None, r_t: [-876.842 -876.842 -876.842], eps: 0.297})
Step:   12200, Reward: [-353.980 -353.980 -353.980] [36.977], Avg: [-491.366 -491.366 -491.366] (0.2943) ({r_i: None, r_t: [-842.979 -842.979 -842.979], eps: 0.294})
Step:   12300, Reward: [-440.149 -440.149 -440.149] [33.198], Avg: [-490.953 -490.953 -490.953] (0.2914) ({r_i: None, r_t: [-929.139 -929.139 -929.139], eps: 0.291})
Step:   12400, Reward: [-529.564 -529.564 -529.564] [58.082], Avg: [-491.262 -491.262 -491.262] (0.2885) ({r_i: None, r_t: [-1015.448 -1015.448 -1015.448], eps: 0.288})
Step:   12500, Reward: [-421.692 -421.692 -421.692] [37.401], Avg: [-490.710 -490.710 -490.710] (0.2856) ({r_i: None, r_t: [-834.498 -834.498 -834.498], eps: 0.286})
Step:   12600, Reward: [-548.246 -548.246 -548.246] [124.550], Avg: [-491.163 -491.163 -491.163] (0.2828) ({r_i: None, r_t: [-858.561 -858.561 -858.561], eps: 0.283})
Step:   12700, Reward: [-455.352 -455.352 -455.352] [26.748], Avg: [-490.883 -490.883 -490.883] (0.2799) ({r_i: None, r_t: [-969.123 -969.123 -969.123], eps: 0.28})
Step:   12800, Reward: [-443.729 -443.729 -443.729] [131.664], Avg: [-490.518 -490.518 -490.518] (0.2771) ({r_i: None, r_t: [-789.133 -789.133 -789.133], eps: 0.277})
Step:   12900, Reward: [-388.583 -388.583 -388.583] [26.033], Avg: [-489.733 -489.733 -489.733] (0.2744) ({r_i: None, r_t: [-929.102 -929.102 -929.102], eps: 0.274})
Step:   13000, Reward: [-503.067 -503.067 -503.067] [108.444], Avg: [-489.835 -489.835 -489.835] (0.2716) ({r_i: None, r_t: [-774.349 -774.349 -774.349], eps: 0.272})
Step:   13100, Reward: [-439.642 -439.642 -439.642] [39.771], Avg: [-489.455 -489.455 -489.455] (0.2689) ({r_i: None, r_t: [-1042.262 -1042.262 -1042.262], eps: 0.269})
Step:   13200, Reward: [-398.601 -398.601 -398.601] [75.791], Avg: [-488.772 -488.772 -488.772] (0.2663) ({r_i: None, r_t: [-934.135 -934.135 -934.135], eps: 0.266})
Step:   13300, Reward: [-423.139 -423.139 -423.139] [120.559], Avg: [-488.282 -488.282 -488.282] (0.2636) ({r_i: None, r_t: [-810.613 -810.613 -810.613], eps: 0.264})
Step:   13400, Reward: [-477.841 -477.841 -477.841] [86.494], Avg: [-488.205 -488.205 -488.205] (0.2610) ({r_i: None, r_t: [-795.262 -795.262 -795.262], eps: 0.261})
Step:   13500, Reward: [-515.679 -515.679 -515.679] [81.530], Avg: [-488.407 -488.407 -488.407] (0.2584) ({r_i: None, r_t: [-858.261 -858.261 -858.261], eps: 0.258})
Step:   13600, Reward: [-476.805 -476.805 -476.805] [37.218], Avg: [-488.322 -488.322 -488.322] (0.2558) ({r_i: None, r_t: [-930.443 -930.443 -930.443], eps: 0.256})
Step:   13700, Reward: [-391.467 -391.467 -391.467] [61.781], Avg: [-487.620 -487.620 -487.620] (0.2532) ({r_i: None, r_t: [-842.964 -842.964 -842.964], eps: 0.253})
Step:   13800, Reward: [-409.342 -409.342 -409.342] [56.313], Avg: [-487.057 -487.057 -487.057] (0.2507) ({r_i: None, r_t: [-964.655 -964.655 -964.655], eps: 0.251})
Step:   13900, Reward: [-447.030 -447.030 -447.030] [86.407], Avg: [-486.771 -486.771 -486.771] (0.2482) ({r_i: None, r_t: [-789.011 -789.011 -789.011], eps: 0.248})
Step:   14000, Reward: [-395.399 -395.399 -395.399] [92.029], Avg: [-486.123 -486.123 -486.123] (0.2457) ({r_i: None, r_t: [-960.147 -960.147 -960.147], eps: 0.246})
Step:   14100, Reward: [-444.818 -444.818 -444.818] [50.433], Avg: [-485.832 -485.832 -485.832] (0.2433) ({r_i: None, r_t: [-920.854 -920.854 -920.854], eps: 0.243})
Step:   14200, Reward: [-404.179 -404.179 -404.179] [76.276], Avg: [-485.261 -485.261 -485.261] (0.2409) ({r_i: None, r_t: [-971.914 -971.914 -971.914], eps: 0.241})
Step:   14300, Reward: [-401.308 -401.308 -401.308] [51.563], Avg: [-484.678 -484.678 -484.678] (0.2385) ({r_i: None, r_t: [-906.184 -906.184 -906.184], eps: 0.238})
Step:   14400, Reward: [-439.685 -439.685 -439.685] [51.151], Avg: [-484.368 -484.368 -484.368] (0.2361) ({r_i: None, r_t: [-847.409 -847.409 -847.409], eps: 0.236})
Step:   14500, Reward: [-459.336 -459.336 -459.336] [67.915], Avg: [-484.197 -484.197 -484.197] (0.2337) ({r_i: None, r_t: [-829.987 -829.987 -829.987], eps: 0.234})
Step:   14600, Reward: [-398.120 -398.120 -398.120] [27.586], Avg: [-483.611 -483.611 -483.611] (0.2314) ({r_i: None, r_t: [-991.536 -991.536 -991.536], eps: 0.231})
Step:   14700, Reward: [-482.574 -482.574 -482.574] [77.119], Avg: [-483.604 -483.604 -483.604] (0.2291) ({r_i: None, r_t: [-954.563 -954.563 -954.563], eps: 0.229})
Step:   14800, Reward: [-383.543 -383.543 -383.543] [55.204], Avg: [-482.932 -482.932 -482.932] (0.2268) ({r_i: None, r_t: [-938.876 -938.876 -938.876], eps: 0.227})
Step:   14900, Reward: [-447.539 -447.539 -447.539] [75.798], Avg: [-482.696 -482.696 -482.696] (0.2245) ({r_i: None, r_t: [-921.559 -921.559 -921.559], eps: 0.225})
Step:   15000, Reward: [-469.561 -469.561 -469.561] [19.433], Avg: [-482.609 -482.609 -482.609] (0.2223) ({r_i: None, r_t: [-819.202 -819.202 -819.202], eps: 0.222})
Step:   15100, Reward: [-399.596 -399.596 -399.596] [44.298], Avg: [-482.063 -482.063 -482.063] (0.2201) ({r_i: None, r_t: [-1027.266 -1027.266 -1027.266], eps: 0.22})
Step:   15200, Reward: [-441.314 -441.314 -441.314] [106.222], Avg: [-481.797 -481.797 -481.797] (0.2179) ({r_i: None, r_t: [-924.241 -924.241 -924.241], eps: 0.218})
Step:   15300, Reward: [-396.318 -396.318 -396.318] [26.999], Avg: [-481.242 -481.242 -481.242] (0.2157) ({r_i: None, r_t: [-927.945 -927.945 -927.945], eps: 0.216})
Step:   15400, Reward: [-466.319 -466.319 -466.319] [92.165], Avg: [-481.146 -481.146 -481.146] (0.2136) ({r_i: None, r_t: [-924.634 -924.634 -924.634], eps: 0.214})
Step:   15500, Reward: [-473.311 -473.311 -473.311] [69.269], Avg: [-481.095 -481.095 -481.095] (0.2114) ({r_i: None, r_t: [-923.852 -923.852 -923.852], eps: 0.211})
Step:   15600, Reward: [-521.706 -521.706 -521.706] [110.744], Avg: [-481.354 -481.354 -481.354] (0.2093) ({r_i: None, r_t: [-881.134 -881.134 -881.134], eps: 0.209})
Step:   15700, Reward: [-362.914 -362.914 -362.914] [93.807], Avg: [-480.604 -480.604 -480.604] (0.2072) ({r_i: None, r_t: [-919.287 -919.287 -919.287], eps: 0.207})
Step:   15800, Reward: [-411.857 -411.857 -411.857] [54.670], Avg: [-480.172 -480.172 -480.172] (0.2052) ({r_i: None, r_t: [-825.017 -825.017 -825.017], eps: 0.205})
Step:   15900, Reward: [-399.818 -399.818 -399.818] [75.205], Avg: [-479.670 -479.670 -479.670] (0.2031) ({r_i: None, r_t: [-781.168 -781.168 -781.168], eps: 0.203})
Step:   16000, Reward: [-405.021 -405.021 -405.021] [77.421], Avg: [-479.206 -479.206 -479.206] (0.2011) ({r_i: None, r_t: [-886.378 -886.378 -886.378], eps: 0.201})
Step:   16100, Reward: [-396.031 -396.031 -396.031] [34.698], Avg: [-478.693 -478.693 -478.693] (0.1991) ({r_i: None, r_t: [-858.399 -858.399 -858.399], eps: 0.199})
Step:   16200, Reward: [-487.694 -487.694 -487.694] [110.364], Avg: [-478.748 -478.748 -478.748] (0.1971) ({r_i: None, r_t: [-840.199 -840.199 -840.199], eps: 0.197})
Step:   16300, Reward: [-404.976 -404.976 -404.976] [41.455], Avg: [-478.298 -478.298 -478.298] (0.1951) ({r_i: None, r_t: [-905.506 -905.506 -905.506], eps: 0.195})
Step:   16400, Reward: [-491.427 -491.427 -491.427] [67.055], Avg: [-478.378 -478.378 -478.378] (0.1932) ({r_i: None, r_t: [-821.907 -821.907 -821.907], eps: 0.193})
Step:   16500, Reward: [-438.422 -438.422 -438.422] [26.144], Avg: [-478.137 -478.137 -478.137] (0.1913) ({r_i: None, r_t: [-953.702 -953.702 -953.702], eps: 0.191})
Step:   16600, Reward: [-423.996 -423.996 -423.996] [80.139], Avg: [-477.813 -477.813 -477.813] (0.1893) ({r_i: None, r_t: [-990.224 -990.224 -990.224], eps: 0.189})
Step:   16700, Reward: [-449.582 -449.582 -449.582] [80.596], Avg: [-477.645 -477.645 -477.645] (0.1875) ({r_i: None, r_t: [-944.616 -944.616 -944.616], eps: 0.187})
Step:   16800, Reward: [-401.984 -401.984 -401.984] [39.505], Avg: [-477.197 -477.197 -477.197] (0.1856) ({r_i: None, r_t: [-864.312 -864.312 -864.312], eps: 0.186})
Step:   16900, Reward: [-419.095 -419.095 -419.095] [74.560], Avg: [-476.855 -476.855 -476.855] (0.1837) ({r_i: None, r_t: [-843.094 -843.094 -843.094], eps: 0.184})
Step:   17000, Reward: [-383.238 -383.238 -383.238] [51.232], Avg: [-476.308 -476.308 -476.308] (0.1819) ({r_i: None, r_t: [-951.075 -951.075 -951.075], eps: 0.182})
Step:   17100, Reward: [-389.096 -389.096 -389.096] [64.129], Avg: [-475.801 -475.801 -475.801] (0.1801) ({r_i: None, r_t: [-890.944 -890.944 -890.944], eps: 0.18})
Step:   17200, Reward: [-486.572 -486.572 -486.572] [89.342], Avg: [-475.863 -475.863 -475.863] (0.1783) ({r_i: None, r_t: [-916.763 -916.763 -916.763], eps: 0.178})
Step:   17300, Reward: [-365.610 -365.610 -365.610] [75.968], Avg: [-475.229 -475.229 -475.229] (0.1765) ({r_i: None, r_t: [-949.452 -949.452 -949.452], eps: 0.177})
Step:   17400, Reward: [-417.597 -417.597 -417.597] [60.133], Avg: [-474.900 -474.900 -474.900] (0.1748) ({r_i: None, r_t: [-932.993 -932.993 -932.993], eps: 0.175})
Step:   17500, Reward: [-357.650 -357.650 -357.650] [31.278], Avg: [-474.234 -474.234 -474.234] (0.1730) ({r_i: None, r_t: [-923.040 -923.040 -923.040], eps: 0.173})
Step:   17600, Reward: [-389.794 -389.794 -389.794] [69.145], Avg: [-473.757 -473.757 -473.757] (0.1713) ({r_i: None, r_t: [-861.867 -861.867 -861.867], eps: 0.171})
Step:   17700, Reward: [-462.932 -462.932 -462.932] [72.306], Avg: [-473.696 -473.696 -473.696] (0.1696) ({r_i: None, r_t: [-857.410 -857.410 -857.410], eps: 0.17})
