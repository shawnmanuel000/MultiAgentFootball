Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 16
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = "cpu" if not gpu else "gpu"
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		eps = self.eps if eps is None else eps
		pre_transition_data = {"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}
		self.episode_batch.update(pre_transition_data, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=len(state[0].shape)==len(self.state_size[0]))
		return list(zip(*one_hot_from_indices(actions, self.action_size[0][-1]).data.numpy()))

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			last_data = {"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}
			self.episode_batch.update(last_data, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
    def __init__(self, out_dim):
        self.out_dim = out_dim

    def transform(self, tensor):
        y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
        y_onehot.scatter_(-1, tensor.long(), 1)
        return y_onehot.float()

    def infer_output_info(self, vshape_in, dtype_in):
        return (self.out_dim,), torch.float32

class COMALearner:
    def __init__(self, mac, scheme, n_agents, n_actions):
        self.n_agents = n_agents
        self.n_actions = n_actions
        self.last_target_update_step = 0
        self.mac = mac
        self.critic_training_steps = 0
        self.critic = COMACritic(scheme, self.n_agents, self.n_actions)
        self.critic_params = list(self.critic.parameters())
        self.agent_params = list(mac.parameters())
        self.params = self.agent_params + self.critic_params
        self.target_critic = copy.deepcopy(self.critic)
        self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
        self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

    def train(self, batch):
        # Get the relevant quantities
        bs = batch.batch_size
        max_t = batch.max_seq_length
        rewards = batch["reward"][:, :-1]
        actions = batch["actions"][:, :]
        done = batch["done"][:, :-1].float()
        mask = batch["filled"][:, :-1].float()
        mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
        critic_mask = mask.clone()
        mask = mask.repeat(1, 1, self.n_agents).view(-1)
        q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
        actions = actions[:,:-1]
        mac_out = []
        self.mac.init_hidden(batch.batch_size)
        for t in range(batch.max_seq_length - 1):
            agent_outs = self.mac.forward(batch, t=t)
            mac_out.append(agent_outs)
        mac_out = torch.stack(mac_out, dim=1)  # Concat over time
        # Mask out unavailable actions, renormalise (as in action selection)
        q_vals = q_vals.reshape(-1, self.n_actions)
        pi = mac_out.view(-1, self.n_actions)
        baseline = (pi * q_vals).sum(-1).detach()
        q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
        pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
        pi_taken[mask == 0] = 1.0
        log_pi_taken = torch.log(pi_taken)
        advantages = (q_taken - baseline).detach()
        coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
        self.agent_optimiser.zero_grad()
        coma_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
        self.agent_optimiser.step()
        if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
            self._update_targets()
            self.last_target_update_step = self.critic_training_steps

    def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
        target_q_vals = self.target_critic(batch)[:, :]
        targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
        targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
        q_vals = torch.zeros_like(target_q_vals)[:, :-1]
        for t in reversed(range(rewards.size(1))):
            mask_t = mask[:, t].expand(-1, self.n_agents)
            if mask_t.sum() == 0:
                continue
            q_t = self.critic(batch, t)
            q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
            q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
            targets_t = targets[:, t]
            td_error = (q_taken - targets_t.detach())
            # 0-out the targets that came from padded data
            masked_td_error = td_error * mask_t
            loss = (masked_td_error ** 2).sum() / mask_t.sum()
            self.critic_optimiser.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
            self.critic_optimiser.step()
            self.critic_training_steps += 1
        return q_vals

    def _update_targets(self):
        self.target_critic.load_state_dict(self.critic.state_dict())

    def cuda(self):
        self.mac.cuda()
        self.critic.cuda()
        self.target_critic.cuda()

    def save_models(self, path):
        self.mac.save_models(path)
        torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
        torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
        torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

    def load_models(self, path):
        self.mac.load_models(path)
        self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
        self.target_critic.load_state_dict(self.critic.state_dict())
        self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
        self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
    # Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
    # Initialise  last  lambda -return  for  not  done  episodes
    ret = target_qs.new_zeros(*target_qs.shape)
    ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
    # Backwards  recursive  update  of the "forward  view"
    for t in range(ret.shape[1] - 2, -1,  -1):
        ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
    # Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
    return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
    def __init__(self, scheme, n_agents, n_actions):
        super(COMACritic, self).__init__()
        self.n_actions = n_actions
        self.n_agents = n_agents
        input_shape = self._get_input_shape(scheme)
        self.output_type = "q"
        self.fc1 = torch.nn.Linear(input_shape, 128)
        self.fc2 = torch.nn.Linear(128, 128)
        self.fc3 = torch.nn.Linear(128, self.n_actions)

    def forward(self, batch, t=None):
        inputs = self._build_inputs(batch, t=t)
        x = torch.relu(self.fc1(inputs))
        x = torch.relu(self.fc2(x))
        q = self.fc3(x)
        return q

    def _build_inputs(self, batch, t=None):
        bs = batch.batch_size
        max_t = batch.max_seq_length if t is None else 1
        ts = slice(None) if t is None else slice(t, t+1)
        inputs = []
        inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
        inputs.append(batch["obs"][:, ts])
        actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
        agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
        agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
        inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
        # last actions
        if t == 0:
            inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
        elif isinstance(t, int):
            inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
        else:
            last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
            last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
            inputs.append(last_actions)

        inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
        inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
        return inputs

    def _get_input_shape(self, scheme):
        input_shape = scheme["state"]["vshape"]
        input_shape += scheme["obs"]["vshape"]
        input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
        input_shape += self.n_agents
        return input_shape

class BasicMAC:
    def __init__(self, scheme, groups, n_agents, n_actions):
        self.n_agents = n_agents
        self.n_actions = n_actions
        self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions)
        self.action_selector = MultinomialActionSelector()
        self.hidden_states = None

    def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
        agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
        chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
        return chosen_actions

    def forward(self, ep_batch, t, test_mode=False):
        agent_inputs = self._build_inputs(ep_batch, t)
        agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
        agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
        if not test_mode:
            epsilon_action_num = agent_outs.size(-1)
            agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs) * self.action_selector.epsilon/epsilon_action_num)
        return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

    def init_hidden(self, batch_size):
        self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

    def parameters(self):
        return self.agent.parameters()

    def load_state(self, other_mac):
        self.agent.load_state_dict(other_mac.agent.state_dict())

    def cuda(self):
        self.agent.cuda()

    def save_models(self, path):
        torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

    def load_models(self, path):
        self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

    def _build_inputs(self, batch, t):
        bs = batch.batch_size
        inputs = []
        inputs.append(batch["obs"][:, t])  # b1av
        inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
        inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
        inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
        return inputs

    def _get_input_shape(self, scheme):
        input_shape = scheme["obs"]["vshape"]
        input_shape += scheme["actions_onehot"]["vshape"][0]
        input_shape += self.n_agents
        return input_shape

class RNNAgent(torch.nn.Module):
    def __init__(self, input_shape, output_shape):
        super(RNNAgent, self).__init__()
        self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
        self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
        self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

    def init_hidden(self):
        return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

    def forward(self, inputs, hidden_state):
        x = torch.relu(self.fc1(inputs))
        h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
        h = self.rnn(x, h_in)
        q = self.fc2(h)
        return q, h

class MultinomialActionSelector():
    def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
        self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
        self.epsilon = self.schedule.eval(0)

    def select_action(self, agent_inputs, t_env, test_mode=False):
        self.epsilon = self.schedule.eval(t_env)
        masked_policies = agent_inputs.clone()
        picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
        return picked_actions

class DecayThenFlatSchedule():
    def __init__(self, start, finish, time_length, decay="exp"):
        self.start = start
        self.finish = finish
        self.time_length = time_length
        self.delta = (self.start - self.finish) / self.time_length
        self.decay = decay
        if self.decay in ["exp"]:
            self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

    def eval(self, T):
        if self.decay in ["linear"]:
            return max(self.finish, self.start - self.delta * T)
        elif self.decay in ["exp"]:
            return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
    def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
        self.scheme = scheme.copy()
        self.groups = groups
        self.batch_size = batch_size
        self.max_seq_length = max_seq_length
        self.preprocess = {} if preprocess is None else preprocess
        self.device = device

        if data is not None:
            self.data = data
        else:
            self.data = SN()
            self.data.transition_data = {}
            self.data.episode_data = {}
            self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

    def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
        if preprocess is not None:
            for k in preprocess:
                assert k in scheme
                new_k = preprocess[k][0]
                transforms = preprocess[k][1]
                vshape = self.scheme[k]["vshape"]
                dtype = self.scheme[k]["dtype"]
                for transform in transforms:
                    vshape, dtype = transform.infer_output_info(vshape, dtype)
                self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
                if "group" in self.scheme[k]:
                    self.scheme[new_k]["group"] = self.scheme[k]["group"]
                if "episode_const" in self.scheme[k]:
                    self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

        assert "filled" not in scheme, '"filled" is a reserved key for masking.'
        scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

        for field_key, field_info in scheme.items():
            assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
            vshape = field_info["vshape"]
            episode_const = field_info.get("episode_const", False)
            group = field_info.get("group", None)
            dtype = field_info.get("dtype", torch.float32)

            if isinstance(vshape, int):
                vshape = (vshape,)
            if group:
                assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
                shape = (groups[group], *vshape)
            else:
                shape = vshape
            if episode_const:
                self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype, device=self.device)
            else:
                self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype, device=self.device)

    def extend(self, scheme, groups=None):
        self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

    def to(self, device):
        for k, v in self.data.transition_data.items():
            self.data.transition_data[k] = v.to(device)
        for k, v in self.data.episode_data.items():
            self.data.episode_data[k] = v.to(device)
        self.device = device

    def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
        slices = self._parse_slices((bs, ts))
        for k, v in data.items():
            if k in self.data.transition_data:
                target = self.data.transition_data
                if mark_filled:
                    target["filled"][slices] = 1
                    mark_filled = False
                _slices = slices
            elif k in self.data.episode_data:
                target = self.data.episode_data
                _slices = slices[0]
            else:
                raise KeyError("{} not found in transition or episode data".format(k))

            dtype = self.scheme[k].get("dtype", torch.float32)
            v = torch.tensor(v, dtype=dtype, device=self.device)
            self._check_safe_view(v, target[k][_slices])
            target[k][_slices] = v.view_as(target[k][_slices])

            if k in self.preprocess:
                new_k = self.preprocess[k][0]
                v = target[k][_slices]
                for transform in self.preprocess[k][1]:
                    v = transform.transform(v)
                target[new_k][_slices] = v.view_as(target[new_k][_slices])

    def _check_safe_view(self, v, dest):
        idx = len(v.shape) - 1
        for s in dest.shape[::-1]:
            if v.shape[idx] != s:
                if s != 1:
                    raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
            else:
                idx -= 1

    def __getitem__(self, item):
        if isinstance(item, str):
            if item in self.data.episode_data:
                return self.data.episode_data[item]
            elif item in self.data.transition_data:
                return self.data.transition_data[item]
            else:
                raise ValueError
        elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
            new_data = self._new_data_sn()
            for key in item:
                if key in self.data.transition_data:
                    new_data.transition_data[key] = self.data.transition_data[key]
                elif key in self.data.episode_data:
                    new_data.episode_data[key] = self.data.episode_data[key]
                else:
                    raise KeyError("Unrecognised key {}".format(key))

            # Update the scheme to only have the requested keys
            new_scheme = {key: self.scheme[key] for key in item}
            new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
                          for key in item if "group" in self.scheme[key]}
            ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
            return ret
        else:
            item = self._parse_slices(item)
            new_data = self._new_data_sn()
            for k, v in self.data.transition_data.items():
                new_data.transition_data[k] = v[item]
            for k, v in self.data.episode_data.items():
                new_data.episode_data[k] = v[item[0]]

            ret_bs = self._get_num_items(item[0], self.batch_size)
            ret_max_t = self._get_num_items(item[1], self.max_seq_length)

            ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
            return ret

    def _get_num_items(self, indexing_item, max_size):
        if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
            return len(indexing_item)
        elif isinstance(indexing_item, slice):
            _range = indexing_item.indices(max_size)
            return 1 + (_range[1] - _range[0] - 1)//_range[2]

    def _new_data_sn(self):
        new_data = SN()
        new_data.transition_data = {}
        new_data.episode_data = {}
        return new_data

    def _parse_slices(self, items):
        parsed = []
        # Only batch slice given, add full time slice
        if (isinstance(items, slice)  # slice a:b
            or isinstance(items, int)  # int i
            or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
            ):
            items = (items, slice(None))

        # Need the time indexing to be contiguous
        if isinstance(items[1], list):
            raise IndexError("Indexing across Time must be contiguous")

        for item in items:
            #TODO: stronger checks to ensure only supported options get through
            if isinstance(item, int):
                # Convert single indices to slices
                parsed.append(slice(item, item+1))
            else:
                # Leave slices and lists as is
                parsed.append(item)
        return parsed

    def max_t_filled(self):
        return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
    def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
        super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
        self.buffer_size = buffer_size  # same as self.batch_size but more explicit
        self.buffer_index = 0
        self.episodes_in_buffer = 0

    def insert_episode_batch(self, ep_batch):
        if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
            self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
            self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
            self.buffer_index = (self.buffer_index + ep_batch.batch_size)
            self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
            self.buffer_index = self.buffer_index % self.buffer_size
            assert self.buffer_index < self.buffer_size
        else:
            buffer_left = self.buffer_size - self.buffer_index
            self.insert_episode_batch(ep_batch[0:buffer_left, :])
            self.insert_episode_batch(ep_batch[buffer_left:, :])

    def can_sample(self, batch_size):
        return self.episodes_in_buffer >= batch_size

    def sample(self, batch_size):
        assert self.can_sample(batch_size)
        if self.episodes_in_buffer == batch_size:
            return self[:batch_size]
        else:
            ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
            return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.950             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
# env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":0} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(make_env, ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=False, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		# envs.envs[0].render()
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = ParallelAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}")
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-1888.804 -1888.804 -1888.804] [243.1249], Avg: [-2131.929 -2131.929 -2131.929] (1.000)
Step: 99, Reward: [-2105.389 -2105.389 -2105.389] [367.8154], Avg: [-2302.566 -2302.566 -2302.566] (1.000)
Step: 149, Reward: [-2081.662 -2081.662 -2081.662] [136.5067], Avg: [-2274.434 -2274.434 -2274.434] (1.000)
Step: 199, Reward: [-1705.335 -1705.335 -1705.335] [115.3335], Avg: [-2160.993 -2160.993 -2160.993] (1.000)
Step: 249, Reward: [-1785.955 -1785.955 -1785.955] [145.9348], Avg: [-2115.172 -2115.172 -2115.172] (1.000)
Step: 299, Reward: [-1081.655 -1081.655 -1081.655] [589.8378], Avg: [-2041.226 -2041.226 -2041.226] (1.000)
Step: 349, Reward: [-662.116 -662.116 -662.116] [236.0685], Avg: [-1877.934 -1877.934 -1877.934] (1.000)
Step: 399, Reward: [-787.063 -787.063 -787.063] [381.6556], Avg: [-1789.282 -1789.282 -1789.282] (1.000)
Step: 449, Reward: [-862.885 -862.885 -862.885] [409.5653], Avg: [-1731.856 -1731.856 -1731.856] (1.000)
Step: 499, Reward: [-1967.552 -1967.552 -1967.552] [130.9933], Avg: [-1768.525 -1768.525 -1768.525] (1.000)
Step: 549, Reward: [-2109.232 -2109.232 -2109.232] [168.3335], Avg: [-1814.802 -1814.802 -1814.802] (1.000)
Step: 599, Reward: [-2131.74 -2131.74 -2131.74] [167.6041], Avg: [-1855.18 -1855.18 -1855.18] (1.000)
Step: 649, Reward: [-2034.459 -2034.459 -2034.459] [138.8383], Avg: [-1879.651 -1879.651 -1879.651] (1.000)
Step: 699, Reward: [-1754.635 -1754.635 -1754.635] [57.4898], Avg: [-1874.828 -1874.828 -1874.828] (1.000)
Step: 749, Reward: [-419.235 -419.235 -419.235] [60.6133], Avg: [-1781.829 -1781.829 -1781.829] (1.000)
Step: 799, Reward: [-550.052 -550.052 -550.052] [186.0326], Avg: [-1716.47 -1716.47 -1716.47] (1.000)
Step: 849, Reward: [-406.853 -406.853 -406.853] [70.0375], Avg: [-1643.553 -1643.553 -1643.553] (1.000)
Step: 899, Reward: [-755.609 -755.609 -755.609] [89.8570], Avg: [-1599.215 -1599.215 -1599.215] (1.000)
Step: 949, Reward: [-437.104 -437.104 -437.104] [72.5776], Avg: [-1541.871 -1541.871 -1541.871] (1.000)
Step: 999, Reward: [-410.445 -410.445 -410.445] [76.8241], Avg: [-1489.141 -1489.141 -1489.141] (1.000)
Step: 1049, Reward: [-471.163 -471.163 -471.163] [60.4211], Avg: [-1443.543 -1443.543 -1443.543] (1.000)
Step: 1099, Reward: [-989.704 -989.704 -989.704] [90.5053], Avg: [-1427.028 -1427.028 -1427.028] (1.000)
Step: 1149, Reward: [-359.21 -359.21 -359.21] [40.1964], Avg: [-1382.349 -1382.349 -1382.349] (1.000)
Step: 1199, Reward: [-541.236 -541.236 -541.236] [66.1954], Avg: [-1350.061 -1350.061 -1350.061] (1.000)
Step: 1249, Reward: [-660.752 -660.752 -660.752] [106.7819], Avg: [-1326.76 -1326.76 -1326.76] (1.000)
Step: 1299, Reward: [-1870.601 -1870.601 -1870.601] [112.2203], Avg: [-1351.993 -1351.993 -1351.993] (1.000)
Step: 1349, Reward: [-2091.561 -2091.561 -2091.561] [74.6463], Avg: [-1382.149 -1382.149 -1382.149] (1.000)
Step: 1399, Reward: [-2045.621 -2045.621 -2045.621] [131.6902], Avg: [-1410.548 -1410.548 -1410.548] (1.000)
Step: 1449, Reward: [-1385.01 -1385.01 -1385.01] [49.9864], Avg: [-1411.391 -1411.391 -1411.391] (1.000)
Step: 1499, Reward: [-1090.331 -1090.331 -1090.331] [151.9339], Avg: [-1405.753 -1405.753 -1405.753] (1.000)
Step: 1549, Reward: [-572.243 -572.243 -572.243] [40.6225], Avg: [-1380.176 -1380.176 -1380.176] (1.000)
Step: 1599, Reward: [-1473.158 -1473.158 -1473.158] [87.8636], Avg: [-1385.827 -1385.827 -1385.827] (1.000)
Step: 1649, Reward: [-1832.352 -1832.352 -1832.352] [85.6981], Avg: [-1401.955 -1401.955 -1401.955] (1.000)
Step: 1699, Reward: [-847.285 -847.285 -847.285] [129.9965], Avg: [-1389.465 -1389.465 -1389.465] (1.000)
Step: 1749, Reward: [-1098.622 -1098.622 -1098.622] [68.3514], Avg: [-1383.108 -1383.108 -1383.108] (1.000)
Step: 1799, Reward: [-908.012 -908.012 -908.012] [104.0532], Avg: [-1372.801 -1372.801 -1372.801] (1.000)
Step: 1849, Reward: [-544.839 -544.839 -544.839] [96.3375], Avg: [-1353.028 -1353.028 -1353.028] (1.000)
Step: 1899, Reward: [-418.915 -418.915 -418.915] [64.2104], Avg: [-1330.136 -1330.136 -1330.136] (1.000)
Step: 1949, Reward: [-364.264 -364.264 -364.264] [106.1584], Avg: [-1308.092 -1308.092 -1308.092] (1.000)
Step: 1999, Reward: [-333.8 -333.8 -333.8] [70.5005], Avg: [-1285.497 -1285.497 -1285.497] (1.000)
Step: 2049, Reward: [-452.369 -452.369 -452.369] [53.7969], Avg: [-1266.489 -1266.489 -1266.489] (1.000)
Step: 2099, Reward: [-408.135 -408.135 -408.135] [37.6079], Avg: [-1246.947 -1246.947 -1246.947] (1.000)
Step: 2149, Reward: [-381.39 -381.39 -381.39] [69.3155], Avg: [-1228.43 -1228.43 -1228.43] (1.000)
Step: 2199, Reward: [-435.744 -435.744 -435.744] [95.9970], Avg: [-1212.596 -1212.596 -1212.596] (1.000)
Step: 2249, Reward: [-415.188 -415.188 -415.188] [61.9826], Avg: [-1196.253 -1196.253 -1196.253] (1.000)
Step: 2299, Reward: [-547.6 -547.6 -547.6] [87.5591], Avg: [-1184.056 -1184.056 -1184.056] (1.000)
Step: 2349, Reward: [-429.564 -429.564 -429.564] [83.1930], Avg: [-1169.773 -1169.773 -1169.773] (1.000)
Step: 2399, Reward: [-439.868 -439.868 -439.868] [71.5587], Avg: [-1156.057 -1156.057 -1156.057] (1.000)
Step: 2449, Reward: [-502.333 -502.333 -502.333] [43.0566], Avg: [-1143.595 -1143.595 -1143.595] (1.000)
Step: 2499, Reward: [-448.129 -448.129 -448.129] [53.1305], Avg: [-1130.748 -1130.748 -1130.748] (1.000)
Step: 2549, Reward: [-450.997 -450.997 -450.997] [117.9335], Avg: [-1119.732 -1119.732 -1119.732] (1.000)
Step: 2599, Reward: [-533.385 -533.385 -533.385] [87.8932], Avg: [-1110.146 -1110.146 -1110.146] (1.000)
Step: 2649, Reward: [-477.301 -477.301 -477.301] [74.4840], Avg: [-1099.611 -1099.611 -1099.611] (1.000)
Step: 2699, Reward: [-451.405 -451.405 -451.405] [23.5258], Avg: [-1088.043 -1088.043 -1088.043] (1.000)
Step: 2749, Reward: [-493.381 -493.381 -493.381] [134.2901], Avg: [-1079.672 -1079.672 -1079.672] (1.000)
Step: 2799, Reward: [-523.366 -523.366 -523.366] [66.7543], Avg: [-1070.93 -1070.93 -1070.93] (1.000)
Step: 2849, Reward: [-561.716 -561.716 -561.716] [12.0999], Avg: [-1062.209 -1062.209 -1062.209] (1.000)
Step: 2899, Reward: [-1145.079 -1145.079 -1145.079] [500.0165], Avg: [-1072.259 -1072.259 -1072.259] (1.000)
Step: 2949, Reward: [-692.531 -692.531 -692.531] [42.5312], Avg: [-1066.544 -1066.544 -1066.544] (1.000)
Step: 2999, Reward: [-1514.365 -1514.365 -1514.365] [475.8817], Avg: [-1081.939 -1081.939 -1081.939] (1.000)
Step: 3049, Reward: [-1767.031 -1767.031 -1767.031] [439.7438], Avg: [-1100.379 -1100.379 -1100.379] (1.000)
Step: 3099, Reward: [-1669.086 -1669.086 -1669.086] [310.5382], Avg: [-1114.56 -1114.56 -1114.56] (1.000)
Step: 3149, Reward: [-1284.7 -1284.7 -1284.7] [340.1170], Avg: [-1122.659 -1122.659 -1122.659] (1.000)
Step: 3199, Reward: [-1402.102 -1402.102 -1402.102] [299.7335], Avg: [-1131.709 -1131.709 -1131.709] (1.000)
Step: 3249, Reward: [-1573.128 -1573.128 -1573.128] [487.2231], Avg: [-1145.996 -1145.996 -1145.996] (1.000)
Step: 3299, Reward: [-1569.437 -1569.437 -1569.437] [304.9775], Avg: [-1157.032 -1157.032 -1157.032] (1.000)
Step: 3349, Reward: [-1774.361 -1774.361 -1774.361] [164.1027], Avg: [-1168.696 -1168.696 -1168.696] (1.000)
Step: 3399, Reward: [-1781.969 -1781.969 -1781.969] [193.7921], Avg: [-1180.564 -1180.564 -1180.564] (1.000)
Step: 3449, Reward: [-2053.921 -2053.921 -2053.921] [254.0872], Avg: [-1196.904 -1196.904 -1196.904] (1.000)
Step: 3499, Reward: [-1983.125 -1983.125 -1983.125] [106.1991], Avg: [-1209.653 -1209.653 -1209.653] (1.000)
Step: 3549, Reward: [-1978.749 -1978.749 -1978.749] [94.4810], Avg: [-1221.816 -1221.816 -1221.816] (1.000)
Step: 3599, Reward: [-2017.002 -2017.002 -2017.002] [224.8115], Avg: [-1235.983 -1235.983 -1235.983] (1.000)
Step: 3649, Reward: [-1930.925 -1930.925 -1930.925] [70.9455], Avg: [-1246.474 -1246.474 -1246.474] (1.000)
Step: 3699, Reward: [-2116.075 -2116.075 -2116.075] [70.7058], Avg: [-1259.181 -1259.181 -1259.181] (1.000)
Step: 3749, Reward: [-2003.942 -2003.942 -2003.942] [46.4796], Avg: [-1269.731 -1269.731 -1269.731] (1.000)
Step: 3799, Reward: [-1939.838 -1939.838 -1939.838] [166.5092], Avg: [-1280.739 -1280.739 -1280.739] (1.000)
Step: 3849, Reward: [-1934.453 -1934.453 -1934.453] [93.0632], Avg: [-1290.437 -1290.437 -1290.437] (1.000)
Step: 3899, Reward: [-2054.352 -2054.352 -2054.352] [123.3021], Avg: [-1301.812 -1301.812 -1301.812] (1.000)
Step: 3949, Reward: [-2073.598 -2073.598 -2073.598] [272.0316], Avg: [-1315.025 -1315.025 -1315.025] (1.000)
Step: 3999, Reward: [-2023.46 -2023.46 -2023.46] [299.9458], Avg: [-1327.63 -1327.63 -1327.63] (1.000)
Step: 4049, Reward: [-2158.469 -2158.469 -2158.469] [166.9072], Avg: [-1339.947 -1339.947 -1339.947] (1.000)
Step: 4099, Reward: [-1853.275 -1853.275 -1853.275] [89.2419], Avg: [-1347.296 -1347.296 -1347.296] (1.000)
Step: 4149, Reward: [-1961.791 -1961.791 -1961.791] [124.6286], Avg: [-1356.201 -1356.201 -1356.201] (1.000)
Step: 4199, Reward: [-1950.115 -1950.115 -1950.115] [144.0938], Avg: [-1364.987 -1364.987 -1364.987] (1.000)
Step: 4249, Reward: [-2118.935 -2118.935 -2118.935] [115.0604], Avg: [-1375.21 -1375.21 -1375.21] (1.000)
Step: 4299, Reward: [-1930.828 -1930.828 -1930.828] [198.6883], Avg: [-1383.981 -1383.981 -1383.981] (1.000)
Step: 4349, Reward: [-2140.061 -2140.061 -2140.061] [269.5279], Avg: [-1395.77 -1395.77 -1395.77] (1.000)
Step: 4399, Reward: [-2034.834 -2034.834 -2034.834] [137.1051], Avg: [-1404.59 -1404.59 -1404.59] (1.000)
Step: 4449, Reward: [-1938.41 -1938.41 -1938.41] [244.9107], Avg: [-1413.34 -1413.34 -1413.34] (1.000)
Step: 4499, Reward: [-2055.283 -2055.283 -2055.283] [161.9728], Avg: [-1422.272 -1422.272 -1422.272] (1.000)
Step: 4549, Reward: [-2033.326 -2033.326 -2033.326] [121.9559], Avg: [-1430.327 -1430.327 -1430.327] (1.000)
Step: 4599, Reward: [-1977.97 -1977.97 -1977.97] [311.0143], Avg: [-1439.661 -1439.661 -1439.661] (1.000)
Step: 4649, Reward: [-2034.509 -2034.509 -2034.509] [77.7161], Avg: [-1446.892 -1446.892 -1446.892] (1.000)
Step: 4699, Reward: [-2135.798 -2135.798 -2135.798] [150.3415], Avg: [-1455.821 -1455.821 -1455.821] (1.000)
Step: 4749, Reward: [-2046.097 -2046.097 -2046.097] [144.2126], Avg: [-1463.552 -1463.552 -1463.552] (1.000)
Step: 4799, Reward: [-2173.309 -2173.309 -2173.309] [250.6903], Avg: [-1473.557 -1473.557 -1473.557] (1.000)
Step: 4849, Reward: [-1991.364 -1991.364 -1991.364] [232.9574], Avg: [-1481.297 -1481.297 -1481.297] (1.000)
Step: 4899, Reward: [-2059.681 -2059.681 -2059.681] [135.6561], Avg: [-1488.583 -1488.583 -1488.583] (1.000)
Step: 4949, Reward: [-1939.288 -1939.288 -1939.288] [92.3439], Avg: [-1494.068 -1494.068 -1494.068] (1.000)
Step: 4999, Reward: [-1918.153 -1918.153 -1918.153] [251.5237], Avg: [-1500.824 -1500.824 -1500.824] (1.000)
Step: 5049, Reward: [-924.624 -924.624 -924.624] [330.2389], Avg: [-1498.389 -1498.389 -1498.389] (1.000)
Step: 5099, Reward: [-1657.143 -1657.143 -1657.143] [343.1196], Avg: [-1503.309 -1503.309 -1503.309] (1.000)
Step: 5149, Reward: [-1159.811 -1159.811 -1159.811] [576.4321], Avg: [-1505.571 -1505.571 -1505.571] (1.000)
Step: 5199, Reward: [-1507.868 -1507.868 -1507.868] [615.4983], Avg: [-1511.511 -1511.511 -1511.511] (1.000)
Step: 5249, Reward: [-1670.164 -1670.164 -1670.164] [334.1512], Avg: [-1516.204 -1516.204 -1516.204] (1.000)
Step: 5299, Reward: [-1169.669 -1169.669 -1169.669] [723.2926], Avg: [-1519.759 -1519.759 -1519.759] (1.000)
Step: 5349, Reward: [-1088.168 -1088.168 -1088.168] [800.8276], Avg: [-1523.21 -1523.21 -1523.21] (1.000)
Step: 5399, Reward: [-1376.577 -1376.577 -1376.577] [521.6594], Avg: [-1526.682 -1526.682 -1526.682] (1.000)
Step: 5449, Reward: [-1584.497 -1584.497 -1584.497] [492.1456], Avg: [-1531.728 -1531.728 -1531.728] (1.000)
Step: 5499, Reward: [-1500.17 -1500.17 -1500.17] [342.7733], Avg: [-1534.557 -1534.557 -1534.557] (1.000)
Step: 5549, Reward: [-669.962 -669.962 -669.962] [80.0004], Avg: [-1527.488 -1527.488 -1527.488] (1.000)
Step: 5599, Reward: [-622.815 -622.815 -622.815] [198.3201], Avg: [-1521.182 -1521.182 -1521.182] (1.000)
Step: 5649, Reward: [-560.801 -560.801 -560.801] [148.0645], Avg: [-1513.993 -1513.993 -1513.993] (1.000)
Step: 5699, Reward: [-517.85 -517.85 -517.85] [81.9251], Avg: [-1505.974 -1505.974 -1505.974] (1.000)
Step: 5749, Reward: [-558.811 -558.811 -558.811] [88.0693], Avg: [-1498.503 -1498.503 -1498.503] (1.000)
Step: 5799, Reward: [-676.061 -676.061 -676.061] [249.9559], Avg: [-1493.568 -1493.568 -1493.568] (1.000)
Step: 5849, Reward: [-1559.375 -1559.375 -1559.375] [587.9762], Avg: [-1499.156 -1499.156 -1499.156] (1.000)
Step: 5899, Reward: [-1446.795 -1446.795 -1446.795] [382.3938], Avg: [-1501.953 -1501.953 -1501.953] (1.000)
Step: 5949, Reward: [-1656.006 -1656.006 -1656.006] [273.7163], Avg: [-1505.547 -1505.547 -1505.547] (1.000)
Step: 5999, Reward: [-1523.62 -1523.62 -1523.62] [180.8791], Avg: [-1507.205 -1507.205 -1507.205] (1.000)
Step: 6049, Reward: [-1165.525 -1165.525 -1165.525] [118.8684], Avg: [-1505.364 -1505.364 -1505.364] (1.000)
Step: 6099, Reward: [-779.326 -779.326 -779.326] [142.6135], Avg: [-1500.582 -1500.582 -1500.582] (1.000)
Step: 6149, Reward: [-1574.9 -1574.9 -1574.9] [248.3398], Avg: [-1503.205 -1503.205 -1503.205] (1.000)
Step: 6199, Reward: [-1135.472 -1135.472 -1135.472] [402.1152], Avg: [-1503.482 -1503.482 -1503.482] (1.000)
Step: 6249, Reward: [-591.816 -591.816 -591.816] [77.1829], Avg: [-1496.806 -1496.806 -1496.806] (1.000)
Step: 6299, Reward: [-583.49 -583.49 -583.49] [91.2058], Avg: [-1490.282 -1490.282 -1490.282] (1.000)
Step: 6349, Reward: [-540.281 -540.281 -540.281] [129.4617], Avg: [-1483.821 -1483.821 -1483.821] (1.000)
Step: 6399, Reward: [-445.799 -445.799 -445.799] [52.4075], Avg: [-1476.121 -1476.121 -1476.121] (1.000)
Step: 6449, Reward: [-400.823 -400.823 -400.823] [89.3813], Avg: [-1468.478 -1468.478 -1468.478] (1.000)
Step: 6499, Reward: [-419.833 -419.833 -419.833] [51.6717], Avg: [-1460.809 -1460.809 -1460.809] (1.000)
Step: 6549, Reward: [-404.251 -404.251 -404.251] [64.7178], Avg: [-1453.238 -1453.238 -1453.238] (1.000)
Step: 6599, Reward: [-342.774 -342.774 -342.774] [40.8665], Avg: [-1445.135 -1445.135 -1445.135] (1.000)
Step: 6649, Reward: [-395.396 -395.396 -395.396] [59.8446], Avg: [-1437.692 -1437.692 -1437.692] (1.000)
Step: 6699, Reward: [-428.251 -428.251 -428.251] [141.9478], Avg: [-1431.218 -1431.218 -1431.218] (1.000)
Step: 6749, Reward: [-455.779 -455.779 -455.779] [114.3752], Avg: [-1424.84 -1424.84 -1424.84] (1.000)
Step: 6799, Reward: [-369.437 -369.437 -369.437] [53.3665], Avg: [-1417.472 -1417.472 -1417.472] (1.000)
Step: 6849, Reward: [-408.466 -408.466 -408.466] [21.4168], Avg: [-1410.263 -1410.263 -1410.263] (1.000)
Step: 6899, Reward: [-382.766 -382.766 -382.766] [53.8838], Avg: [-1403.208 -1403.208 -1403.208] (1.000)
Step: 6949, Reward: [-489.029 -489.029 -489.029] [46.9049], Avg: [-1396.969 -1396.969 -1396.969] (1.000)
Step: 6999, Reward: [-337.22 -337.22 -337.22] [72.6331], Avg: [-1389.918 -1389.918 -1389.918] (1.000)
Step: 7049, Reward: [-372.623 -372.623 -372.623] [41.1983], Avg: [-1382.995 -1382.995 -1382.995] (1.000)
Step: 7099, Reward: [-428.954 -428.954 -428.954] [95.3827], Avg: [-1376.948 -1376.948 -1376.948] (1.000)
Step: 7149, Reward: [-394.333 -394.333 -394.333] [77.5447], Avg: [-1370.619 -1370.619 -1370.619] (1.000)
Step: 7199, Reward: [-391.463 -391.463 -391.463] [50.6662], Avg: [-1364.171 -1364.171 -1364.171] (1.000)
Step: 7249, Reward: [-414.867 -414.867 -414.867] [90.5528], Avg: [-1358.249 -1358.249 -1358.249] (1.000)
Step: 7299, Reward: [-453.754 -453.754 -453.754] [59.0109], Avg: [-1352.458 -1352.458 -1352.458] (1.000)
Step: 7349, Reward: [-480.577 -480.577 -480.577] [81.7372], Avg: [-1347.083 -1347.083 -1347.083] (1.000)
Step: 7399, Reward: [-422.874 -422.874 -422.874] [38.2162], Avg: [-1341.096 -1341.096 -1341.096] (1.000)
Step: 7449, Reward: [-431.301 -431.301 -431.301] [41.7457], Avg: [-1335.27 -1335.27 -1335.27] (1.000)
Step: 7499, Reward: [-391.261 -391.261 -391.261] [30.4366], Avg: [-1329.18 -1329.18 -1329.18] (1.000)
Step: 7549, Reward: [-424.787 -424.787 -424.787] [102.8276], Avg: [-1323.871 -1323.871 -1323.871] (1.000)
Step: 7599, Reward: [-395.599 -395.599 -395.599] [21.6805], Avg: [-1317.907 -1317.907 -1317.907] (1.000)
Step: 7649, Reward: [-380.723 -380.723 -380.723] [60.7570], Avg: [-1312.179 -1312.179 -1312.179] (1.000)
Step: 7699, Reward: [-409.055 -409.055 -409.055] [93.4382], Avg: [-1306.921 -1306.921 -1306.921] (1.000)
Step: 7749, Reward: [-408.558 -408.558 -408.558] [52.8661], Avg: [-1301.466 -1301.466 -1301.466] (1.000)
Step: 7799, Reward: [-487.722 -487.722 -487.722] [59.4678], Avg: [-1296.631 -1296.631 -1296.631] (1.000)
Step: 7849, Reward: [-477.291 -477.291 -477.291] [98.4316], Avg: [-1292.039 -1292.039 -1292.039] (1.000)
Step: 7899, Reward: [-486.234 -486.234 -486.234] [91.3932], Avg: [-1287.518 -1287.518 -1287.518] (1.000)
Step: 7949, Reward: [-651.388 -651.388 -651.388] [61.7200], Avg: [-1283.905 -1283.905 -1283.905] (1.000)
Step: 7999, Reward: [-453.892 -453.892 -453.892] [98.6001], Avg: [-1279.334 -1279.334 -1279.334] (1.000)
Step: 8049, Reward: [-445.103 -445.103 -445.103] [59.2475], Avg: [-1274.52 -1274.52 -1274.52] (1.000)
Step: 8099, Reward: [-468.891 -468.891 -468.891] [33.0573], Avg: [-1269.751 -1269.751 -1269.751] (1.000)
Step: 8149, Reward: [-407.201 -407.201 -407.201] [54.1223], Avg: [-1264.792 -1264.792 -1264.792] (1.000)
Step: 8199, Reward: [-374.463 -374.463 -374.463] [59.5150], Avg: [-1259.726 -1259.726 -1259.726] (1.000)
Step: 8249, Reward: [-359.57 -359.57 -359.57] [33.6499], Avg: [-1254.474 -1254.474 -1254.474] (1.000)
Step: 8299, Reward: [-456.589 -456.589 -456.589] [77.9462], Avg: [-1250.137 -1250.137 -1250.137] (1.000)
Step: 8349, Reward: [-438.355 -438.355 -438.355] [75.7425], Avg: [-1245.73 -1245.73 -1245.73] (1.000)
Step: 8399, Reward: [-420.483 -420.483 -420.483] [51.5610], Avg: [-1241.124 -1241.124 -1241.124] (1.000)
Step: 8449, Reward: [-470.061 -470.061 -470.061] [75.5671], Avg: [-1237.009 -1237.009 -1237.009] (1.000)
Step: 8499, Reward: [-1344.238 -1344.238 -1344.238] [622.8327], Avg: [-1241.304 -1241.304 -1241.304] (1.000)
Step: 8549, Reward: [-1489.703 -1489.703 -1489.703] [460.8461], Avg: [-1245.451 -1245.451 -1245.451] (1.000)
Step: 8599, Reward: [-1834.92 -1834.92 -1834.92] [212.9530], Avg: [-1250.116 -1250.116 -1250.116] (1.000)
Step: 8649, Reward: [-1772.431 -1772.431 -1772.431] [184.0016], Avg: [-1254.199 -1254.199 -1254.199] (1.000)
Step: 8699, Reward: [-1823.305 -1823.305 -1823.305] [118.9418], Avg: [-1258.153 -1258.153 -1258.153] (1.000)
Step: 8749, Reward: [-1845.198 -1845.198 -1845.198] [296.4312], Avg: [-1263.202 -1263.202 -1263.202] (1.000)
Step: 8799, Reward: [-1993.321 -1993.321 -1993.321] [226.0422], Avg: [-1268.635 -1268.635 -1268.635] (1.000)
Step: 8849, Reward: [-1924.045 -1924.045 -1924.045] [334.6625], Avg: [-1274.228 -1274.228 -1274.228] (1.000)
Step: 8899, Reward: [-1845.422 -1845.422 -1845.422] [249.7758], Avg: [-1278.84 -1278.84 -1278.84] (1.000)
Step: 8949, Reward: [-1867.227 -1867.227 -1867.227] [259.4409], Avg: [-1283.577 -1283.577 -1283.577] (1.000)
Step: 8999, Reward: [-1948.551 -1948.551 -1948.551] [243.9600], Avg: [-1288.627 -1288.627 -1288.627] (1.000)
Step: 9049, Reward: [-2070.688 -2070.688 -2070.688] [191.6347], Avg: [-1294.006 -1294.006 -1294.006] (1.000)
Step: 9099, Reward: [-1977.936 -1977.936 -1977.936] [187.4294], Avg: [-1298.794 -1298.794 -1298.794] (1.000)
Step: 9149, Reward: [-1798.51 -1798.51 -1798.51] [349.5899], Avg: [-1303.435 -1303.435 -1303.435] (1.000)
Step: 9199, Reward: [-1711.659 -1711.659 -1711.659] [316.9735], Avg: [-1307.376 -1307.376 -1307.376] (1.000)
Step: 9249, Reward: [-1637.7 -1637.7 -1637.7] [66.6103], Avg: [-1309.522 -1309.522 -1309.522] (1.000)
Step: 9299, Reward: [-1326.744 -1326.744 -1326.744] [510.5018], Avg: [-1312.359 -1312.359 -1312.359] (1.000)
Step: 9349, Reward: [-525.532 -525.532 -525.532] [77.4577], Avg: [-1308.566 -1308.566 -1308.566] (1.000)
Step: 9399, Reward: [-409.349 -409.349 -409.349] [34.6105], Avg: [-1303.967 -1303.967 -1303.967] (1.000)
Step: 9449, Reward: [-499.632 -499.632 -499.632] [87.0190], Avg: [-1300.171 -1300.171 -1300.171] (1.000)
Step: 9499, Reward: [-492.84 -492.84 -492.84] [125.6858], Avg: [-1296.584 -1296.584 -1296.584] (1.000)
Step: 9549, Reward: [-443.716 -443.716 -443.716] [46.3928], Avg: [-1292.361 -1292.361 -1292.361] (1.000)
Step: 9599, Reward: [-390.018 -390.018 -390.018] [46.4603], Avg: [-1287.904 -1287.904 -1287.904] (1.000)
Step: 9649, Reward: [-396.799 -396.799 -396.799] [52.0110], Avg: [-1283.556 -1283.556 -1283.556] (1.000)
Step: 9699, Reward: [-442.849 -442.849 -442.849] [72.8057], Avg: [-1279.598 -1279.598 -1279.598] (1.000)
Step: 9749, Reward: [-448.279 -448.279 -448.279] [57.3464], Avg: [-1275.629 -1275.629 -1275.629] (1.000)
Step: 9799, Reward: [-504.518 -504.518 -504.518] [86.7909], Avg: [-1272.137 -1272.137 -1272.137] (1.000)
Step: 9849, Reward: [-412.502 -412.502 -412.502] [47.3777], Avg: [-1268.014 -1268.014 -1268.014] (1.000)
Step: 9899, Reward: [-422.545 -422.545 -422.545] [44.9194], Avg: [-1263.971 -1263.971 -1263.971] (1.000)
Step: 9949, Reward: [-396.173 -396.173 -396.173] [54.1155], Avg: [-1259.882 -1259.882 -1259.882] (1.000)
Step: 9999, Reward: [-462.444 -462.444 -462.444] [76.4060], Avg: [-1256.277 -1256.277 -1256.277] (1.000)
Step: 10049, Reward: [-327.28 -327.28 -327.28] [57.6631], Avg: [-1251.942 -1251.942 -1251.942] (1.000)
Step: 10099, Reward: [-389.964 -389.964 -389.964] [62.2494], Avg: [-1247.983 -1247.983 -1247.983] (1.000)
Step: 10149, Reward: [-506.946 -506.946 -506.946] [119.7484], Avg: [-1244.922 -1244.922 -1244.922] (1.000)
Step: 10199, Reward: [-439.949 -439.949 -439.949] [112.0425], Avg: [-1241.525 -1241.525 -1241.525] (1.000)
Step: 10249, Reward: [-427.548 -427.548 -427.548] [63.8477], Avg: [-1237.866 -1237.866 -1237.866] (1.000)
Step: 10299, Reward: [-475.686 -475.686 -475.686] [152.1000], Avg: [-1234.905 -1234.905 -1234.905] (1.000)
Step: 10349, Reward: [-417.983 -417.983 -417.983] [67.1240], Avg: [-1231.283 -1231.283 -1231.283] (1.000)
Step: 10399, Reward: [-416.858 -416.858 -416.858] [52.4817], Avg: [-1227.619 -1227.619 -1227.619] (1.000)
Step: 10449, Reward: [-434.194 -434.194 -434.194] [96.4390], Avg: [-1224.284 -1224.284 -1224.284] (1.000)
Step: 10499, Reward: [-394.599 -394.599 -394.599] [70.0592], Avg: [-1220.667 -1220.667 -1220.667] (1.000)
Step: 10549, Reward: [-430.853 -430.853 -430.853] [35.2795], Avg: [-1217.091 -1217.091 -1217.091] (1.000)
Step: 10599, Reward: [-460.187 -460.187 -460.187] [73.9184], Avg: [-1213.87 -1213.87 -1213.87] (1.000)
Step: 10649, Reward: [-502.134 -502.134 -502.134] [76.0754], Avg: [-1210.885 -1210.885 -1210.885] (1.000)
Step: 10699, Reward: [-480.496 -480.496 -480.496] [122.2055], Avg: [-1208.043 -1208.043 -1208.043] (1.000)
Step: 10749, Reward: [-590.847 -590.847 -590.847] [95.4369], Avg: [-1205.617 -1205.617 -1205.617] (1.000)
Step: 10799, Reward: [-592.737 -592.737 -592.737] [109.7633], Avg: [-1203.287 -1203.287 -1203.287] (1.000)
Step: 10849, Reward: [-580.882 -580.882 -580.882] [146.0726], Avg: [-1201.092 -1201.092 -1201.092] (1.000)
Step: 10899, Reward: [-868.383 -868.383 -868.383] [321.3864], Avg: [-1201.04 -1201.04 -1201.04] (1.000)
Step: 10949, Reward: [-1631.515 -1631.515 -1631.515] [259.4467], Avg: [-1204.191 -1204.191 -1204.191] (1.000)
Step: 10999, Reward: [-790.939 -790.939 -790.939] [203.3539], Avg: [-1203.236 -1203.236 -1203.236] (1.000)
Step: 11049, Reward: [-586.133 -586.133 -586.133] [101.5383], Avg: [-1200.904 -1200.904 -1200.904] (1.000)
Step: 11099, Reward: [-491.306 -491.306 -491.306] [63.4559], Avg: [-1197.993 -1197.993 -1197.993] (1.000)
Step: 11149, Reward: [-476.842 -476.842 -476.842] [49.1189], Avg: [-1194.979 -1194.979 -1194.979] (1.000)
Step: 11199, Reward: [-454.303 -454.303 -454.303] [115.3376], Avg: [-1192.188 -1192.188 -1192.188] (1.000)
Step: 11249, Reward: [-479.083 -479.083 -479.083] [33.9393], Avg: [-1189.169 -1189.169 -1189.169] (1.000)
Step: 11299, Reward: [-423.178 -423.178 -423.178] [65.7564], Avg: [-1186.071 -1186.071 -1186.071] (1.000)
Step: 11349, Reward: [-568.406 -568.406 -568.406] [117.2253], Avg: [-1183.866 -1183.866 -1183.866] (1.000)
Step: 11399, Reward: [-686.738 -686.738 -686.738] [151.1533], Avg: [-1182.349 -1182.349 -1182.349] (1.000)
Step: 11449, Reward: [-1017.442 -1017.442 -1017.442] [87.3174], Avg: [-1182.01 -1182.01 -1182.01] (1.000)
Step: 11499, Reward: [-1842.208 -1842.208 -1842.208] [165.4158], Avg: [-1185.6 -1185.6 -1185.6] (1.000)
Step: 11549, Reward: [-706.516 -706.516 -706.516] [59.1017], Avg: [-1183.782 -1183.782 -1183.782] (1.000)
Step: 11599, Reward: [-547.516 -547.516 -547.516] [78.1803], Avg: [-1181.376 -1181.376 -1181.376] (1.000)
Step: 11649, Reward: [-518.919 -518.919 -518.919] [84.2711], Avg: [-1178.895 -1178.895 -1178.895] (1.000)
Step: 11699, Reward: [-424.214 -424.214 -424.214] [59.4055], Avg: [-1175.923 -1175.923 -1175.923] (1.000)
Step: 11749, Reward: [-466.632 -466.632 -466.632] [121.3377], Avg: [-1173.421 -1173.421 -1173.421] (1.000)
Step: 11799, Reward: [-466.984 -466.984 -466.984] [90.5333], Avg: [-1170.812 -1170.812 -1170.812] (1.000)
Step: 11849, Reward: [-451.321 -451.321 -451.321] [91.1385], Avg: [-1168.16 -1168.16 -1168.16] (1.000)
Step: 11899, Reward: [-382.783 -382.783 -382.783] [12.7363], Avg: [-1164.914 -1164.914 -1164.914] (1.000)
Step: 11949, Reward: [-361.516 -361.516 -361.516] [59.9939], Avg: [-1161.803 -1161.803 -1161.803] (1.000)
Step: 11999, Reward: [-421.179 -421.179 -421.179] [82.2860], Avg: [-1159.06 -1159.06 -1159.06] (1.000)
Step: 12049, Reward: [-398.686 -398.686 -398.686] [56.4215], Avg: [-1156.139 -1156.139 -1156.139] (1.000)
Step: 12099, Reward: [-406.259 -406.259 -406.259] [34.0388], Avg: [-1153.181 -1153.181 -1153.181] (1.000)
Step: 12149, Reward: [-457.728 -457.728 -457.728] [54.2191], Avg: [-1150.543 -1150.543 -1150.543] (1.000)
Step: 12199, Reward: [-435.812 -435.812 -435.812] [82.4531], Avg: [-1147.951 -1147.951 -1147.951] (1.000)
Step: 12249, Reward: [-447.395 -447.395 -447.395] [75.4865], Avg: [-1145.4 -1145.4 -1145.4] (1.000)
Step: 12299, Reward: [-493.832 -493.832 -493.832] [27.5132], Avg: [-1142.863 -1142.863 -1142.863] (1.000)
Step: 12349, Reward: [-465.381 -465.381 -465.381] [65.4781], Avg: [-1140.385 -1140.385 -1140.385] (1.000)
Step: 12399, Reward: [-394.038 -394.038 -394.038] [53.2306], Avg: [-1137.591 -1137.591 -1137.591] (1.000)
Step: 12449, Reward: [-467.633 -467.633 -467.633] [42.9013], Avg: [-1135.072 -1135.072 -1135.072] (1.000)
Step: 12499, Reward: [-449.396 -449.396 -449.396] [65.0218], Avg: [-1132.59 -1132.59 -1132.59] (1.000)
Step: 12549, Reward: [-501.178 -501.178 -501.178] [48.8106], Avg: [-1130.269 -1130.269 -1130.269] (1.000)
Step: 12599, Reward: [-505.753 -505.753 -505.753] [23.4319], Avg: [-1127.883 -1127.883 -1127.883] (1.000)
Step: 12649, Reward: [-545.384 -545.384 -545.384] [36.1569], Avg: [-1125.724 -1125.724 -1125.724] (1.000)
Step: 12699, Reward: [-491.392 -491.392 -491.392] [31.9443], Avg: [-1123.352 -1123.352 -1123.352] (1.000)
Step: 12749, Reward: [-430.608 -430.608 -430.608] [55.4871], Avg: [-1120.853 -1120.853 -1120.853] (1.000)
Step: 12799, Reward: [-557.623 -557.623 -557.623] [64.7339], Avg: [-1118.906 -1118.906 -1118.906] (1.000)
Step: 12849, Reward: [-524.489 -524.489 -524.489] [66.5069], Avg: [-1116.852 -1116.852 -1116.852] (1.000)
Step: 12899, Reward: [-545.429 -545.429 -545.429] [64.2566], Avg: [-1114.886 -1114.886 -1114.886] (1.000)
Step: 12949, Reward: [-559.73 -559.73 -559.73] [79.0062], Avg: [-1113.048 -1113.048 -1113.048] (1.000)
Step: 12999, Reward: [-593.041 -593.041 -593.041] [48.8029], Avg: [-1111.235 -1111.235 -1111.235] (1.000)
Step: 13049, Reward: [-609.634 -609.634 -609.634] [64.3513], Avg: [-1109.56 -1109.56 -1109.56] (1.000)
Step: 13099, Reward: [-567.462 -567.462 -567.462] [123.4799], Avg: [-1107.962 -1107.962 -1107.962] (1.000)
Step: 13149, Reward: [-528.673 -528.673 -528.673] [91.9822], Avg: [-1106.109 -1106.109 -1106.109] (1.000)
Step: 13199, Reward: [-461.154 -461.154 -461.154] [44.6720], Avg: [-1103.836 -1103.836 -1103.836] (1.000)
Step: 13249, Reward: [-481.048 -481.048 -481.048] [40.5722], Avg: [-1101.639 -1101.639 -1101.639] (1.000)
Step: 13299, Reward: [-513.054 -513.054 -513.054] [25.3160], Avg: [-1099.521 -1099.521 -1099.521] (1.000)
Step: 13349, Reward: [-445.262 -445.262 -445.262] [84.6252], Avg: [-1097.388 -1097.388 -1097.388] (1.000)
Step: 13399, Reward: [-465.493 -465.493 -465.493] [48.6306], Avg: [-1095.211 -1095.211 -1095.211] (1.000)
Step: 13449, Reward: [-558.707 -558.707 -558.707] [115.5513], Avg: [-1093.646 -1093.646 -1093.646] (1.000)
Step: 13499, Reward: [-466.532 -466.532 -466.532] [72.3274], Avg: [-1091.592 -1091.592 -1091.592] (1.000)
Step: 13549, Reward: [-477.557 -477.557 -477.557] [43.7465], Avg: [-1089.487 -1089.487 -1089.487] (1.000)
Step: 13599, Reward: [-517.538 -517.538 -517.538] [54.1689], Avg: [-1087.584 -1087.584 -1087.584] (1.000)
Step: 13649, Reward: [-513.242 -513.242 -513.242] [58.4656], Avg: [-1085.694 -1085.694 -1085.694] (1.000)
Step: 13699, Reward: [-474.158 -474.158 -474.158] [73.9952], Avg: [-1083.732 -1083.732 -1083.732] (1.000)
Step: 13749, Reward: [-538.11 -538.11 -538.11] [77.7846], Avg: [-1082.031 -1082.031 -1082.031] (1.000)
Step: 13799, Reward: [-619.31 -619.31 -619.31] [164.7642], Avg: [-1080.951 -1080.951 -1080.951] (1.000)
Step: 13849, Reward: [-604.786 -604.786 -604.786] [111.9056], Avg: [-1079.636 -1079.636 -1079.636] (1.000)
Step: 13899, Reward: [-690.54 -690.54 -690.54] [134.0987], Avg: [-1078.719 -1078.719 -1078.719] (1.000)
Step: 13949, Reward: [-582.874 -582.874 -582.874] [66.2779], Avg: [-1077.179 -1077.179 -1077.179] (1.000)
Step: 13999, Reward: [-568.471 -568.471 -568.471] [32.8675], Avg: [-1075.48 -1075.48 -1075.48] (1.000)
Step: 14049, Reward: [-694.11 -694.11 -694.11] [75.4579], Avg: [-1074.391 -1074.391 -1074.391] (1.000)
Step: 14099, Reward: [-674.56 -674.56 -674.56] [82.1778], Avg: [-1073.265 -1073.265 -1073.265] (1.000)
Step: 14149, Reward: [-739.942 -739.942 -739.942] [119.8551], Avg: [-1072.51 -1072.51 -1072.51] (1.000)
Step: 14199, Reward: [-677.801 -677.801 -677.801] [41.5880], Avg: [-1071.267 -1071.267 -1071.267] (1.000)
Step: 14249, Reward: [-678.699 -678.699 -678.699] [7.5414], Avg: [-1069.916 -1069.916 -1069.916] (1.000)
Step: 14299, Reward: [-670.79 -670.79 -670.79] [66.9917], Avg: [-1068.755 -1068.755 -1068.755] (1.000)
Step: 14349, Reward: [-683.363 -683.363 -683.363] [32.5189], Avg: [-1067.525 -1067.525 -1067.525] (1.000)
Step: 14399, Reward: [-660.878 -660.878 -660.878] [70.5123], Avg: [-1066.358 -1066.358 -1066.358] (1.000)
Step: 14449, Reward: [-623.267 -623.267 -623.267] [81.7873], Avg: [-1065.108 -1065.108 -1065.108] (1.000)
Step: 14499, Reward: [-682.088 -682.088 -682.088] [117.2385], Avg: [-1064.191 -1064.191 -1064.191] (1.000)
Step: 14549, Reward: [-625.137 -625.137 -625.137] [68.8614], Avg: [-1062.919 -1062.919 -1062.919] (1.000)
Step: 14599, Reward: [-544.036 -544.036 -544.036] [122.1317], Avg: [-1061.561 -1061.561 -1061.561] (1.000)
Step: 14649, Reward: [-548.013 -548.013 -548.013] [87.7565], Avg: [-1060.107 -1060.107 -1060.107] (1.000)
Step: 14699, Reward: [-663.15 -663.15 -663.15] [54.5541], Avg: [-1058.943 -1058.943 -1058.943] (1.000)
Step: 14749, Reward: [-609.646 -609.646 -609.646] [107.3019], Avg: [-1057.783 -1057.783 -1057.783] (1.000)
Step: 14799, Reward: [-557.247 -557.247 -557.247] [77.8844], Avg: [-1056.356 -1056.356 -1056.356] (1.000)
Step: 14849, Reward: [-588.929 -588.929 -588.929] [52.4597], Avg: [-1054.958 -1054.958 -1054.958] (1.000)
Step: 14899, Reward: [-546.097 -546.097 -546.097] [35.8528], Avg: [-1053.371 -1053.371 -1053.371] (1.000)
Step: 14949, Reward: [-508.769 -508.769 -508.769] [62.1822], Avg: [-1051.758 -1051.758 -1051.758] (1.000)
Step: 14999, Reward: [-564.041 -564.041 -564.041] [98.6225], Avg: [-1050.461 -1050.461 -1050.461] (1.000)
Step: 15049, Reward: [-427.042 -427.042 -427.042] [64.9829], Avg: [-1048.605 -1048.605 -1048.605] (1.000)
Step: 15099, Reward: [-456.032 -456.032 -456.032] [70.3148], Avg: [-1046.876 -1046.876 -1046.876] (1.000)
Step: 15149, Reward: [-459.93 -459.93 -459.93] [50.1228], Avg: [-1045.104 -1045.104 -1045.104] (1.000)
Step: 15199, Reward: [-436.089 -436.089 -436.089] [52.9999], Avg: [-1043.275 -1043.275 -1043.275] (1.000)
Step: 15249, Reward: [-394.702 -394.702 -394.702] [36.4280], Avg: [-1041.268 -1041.268 -1041.268] (1.000)
Step: 15299, Reward: [-468.656 -468.656 -468.656] [51.4656], Avg: [-1039.565 -1039.565 -1039.565] (1.000)
Step: 15349, Reward: [-413.324 -413.324 -413.324] [90.4974], Avg: [-1037.82 -1037.82 -1037.82] (1.000)
Step: 15399, Reward: [-398.458 -398.458 -398.458] [37.5091], Avg: [-1035.866 -1035.866 -1035.866] (1.000)
Step: 15449, Reward: [-441.766 -441.766 -441.766] [51.8806], Avg: [-1034.111 -1034.111 -1034.111] (1.000)
Step: 15499, Reward: [-430.47 -430.47 -430.47] [46.8863], Avg: [-1032.315 -1032.315 -1032.315] (1.000)
Step: 15549, Reward: [-471.179 -471.179 -471.179] [34.7466], Avg: [-1030.623 -1030.623 -1030.623] (1.000)
Step: 15599, Reward: [-465.935 -465.935 -465.935] [47.0251], Avg: [-1028.964 -1028.964 -1028.964] (1.000)
Step: 15649, Reward: [-438.164 -438.164 -438.164] [38.6591], Avg: [-1027.2 -1027.2 -1027.2] (1.000)
Step: 15699, Reward: [-491.242 -491.242 -491.242] [51.9218], Avg: [-1025.658 -1025.658 -1025.658] (1.000)
Step: 15749, Reward: [-460.43 -460.43 -460.43] [86.4032], Avg: [-1024.138 -1024.138 -1024.138] (1.000)
Step: 15799, Reward: [-476.012 -476.012 -476.012] [25.0254], Avg: [-1022.483 -1022.483 -1022.483] (1.000)
Step: 15849, Reward: [-523.713 -523.713 -523.713] [81.2571], Avg: [-1021.166 -1021.166 -1021.166] (1.000)
Step: 15899, Reward: [-521.952 -521.952 -521.952] [74.9669], Avg: [-1019.831 -1019.831 -1019.831] (1.000)
Step: 15949, Reward: [-465.382 -465.382 -465.382] [73.1038], Avg: [-1018.322 -1018.322 -1018.322] (1.000)
Step: 15999, Reward: [-527.845 -527.845 -527.845] [83.9585], Avg: [-1017.052 -1017.052 -1017.052] (1.000)
Step: 16049, Reward: [-467.003 -467.003 -467.003] [56.4167], Avg: [-1015.514 -1015.514 -1015.514] (1.000)
Step: 16099, Reward: [-461.921 -461.921 -461.921] [81.8365], Avg: [-1014.049 -1014.049 -1014.049] (1.000)
Step: 16149, Reward: [-462.012 -462.012 -462.012] [64.9719], Avg: [-1012.541 -1012.541 -1012.541] (1.000)
Step: 16199, Reward: [-478.717 -478.717 -478.717] [68.8180], Avg: [-1011.106 -1011.106 -1011.106] (1.000)
Step: 16249, Reward: [-463.671 -463.671 -463.671] [40.2747], Avg: [-1009.546 -1009.546 -1009.546] (1.000)
Step: 16299, Reward: [-437.971 -437.971 -437.971] [61.0170], Avg: [-1007.979 -1007.979 -1007.979] (1.000)
Step: 16349, Reward: [-428.634 -428.634 -428.634] [55.6858], Avg: [-1006.378 -1006.378 -1006.378] (1.000)
Step: 16399, Reward: [-452.826 -452.826 -452.826] [24.1148], Avg: [-1004.764 -1004.764 -1004.764] (1.000)
Step: 16449, Reward: [-423.813 -423.813 -423.813] [39.8439], Avg: [-1003.119 -1003.119 -1003.119] (1.000)
Step: 16499, Reward: [-465.415 -465.415 -465.415] [37.8524], Avg: [-1001.605 -1001.605 -1001.605] (1.000)
Step: 16549, Reward: [-459.434 -459.434 -459.434] [55.4230], Avg: [-1000.134 -1000.134 -1000.134] (1.000)
Step: 16599, Reward: [-384.966 -384.966 -384.966] [19.7770], Avg: [-998.341 -998.341 -998.341] (1.000)
Step: 16649, Reward: [-426.469 -426.469 -426.469] [68.8577], Avg: [-996.83 -996.83 -996.83] (1.000)
Step: 16699, Reward: [-410.941 -410.941 -410.941] [41.1870], Avg: [-995.199 -995.199 -995.199] (1.000)
Step: 16749, Reward: [-429.445 -429.445 -429.445] [36.0707], Avg: [-993.618 -993.618 -993.618] (1.000)
Step: 16799, Reward: [-434.175 -434.175 -434.175] [89.8563], Avg: [-992.221 -992.221 -992.221] (1.000)
Step: 16849, Reward: [-443.011 -443.011 -443.011] [35.8858], Avg: [-990.697 -990.697 -990.697] (1.000)
Step: 16899, Reward: [-453.366 -453.366 -453.366] [56.9801], Avg: [-989.276 -989.276 -989.276] (1.000)
Step: 16949, Reward: [-454.845 -454.845 -454.845] [67.2843], Avg: [-987.898 -987.898 -987.898] (1.000)
Step: 16999, Reward: [-544.581 -544.581 -544.581] [82.5971], Avg: [-986.837 -986.837 -986.837] (1.000)
Step: 17049, Reward: [-470.168 -470.168 -470.168] [84.3936], Avg: [-985.57 -985.57 -985.57] (1.000)
Step: 17099, Reward: [-508.818 -508.818 -508.818] [54.4101], Avg: [-984.335 -984.335 -984.335] (1.000)
Step: 17149, Reward: [-443.917 -443.917 -443.917] [48.9371], Avg: [-982.902 -982.902 -982.902] (1.000)
Step: 17199, Reward: [-502.915 -502.915 -502.915] [86.0835], Avg: [-981.757 -981.757 -981.757] (1.000)
Step: 17249, Reward: [-496.632 -496.632 -496.632] [59.6231], Avg: [-980.523 -980.523 -980.523] (1.000)
Step: 17299, Reward: [-514.246 -514.246 -514.246] [71.4429], Avg: [-979.382 -979.382 -979.382] (1.000)
Step: 17349, Reward: [-466.038 -466.038 -466.038] [41.6069], Avg: [-978.023 -978.023 -978.023] (1.000)
Step: 17399, Reward: [-453.27 -453.27 -453.27] [47.6528], Avg: [-976.652 -976.652 -976.652] (1.000)
Step: 17449, Reward: [-416.299 -416.299 -416.299] [65.0377], Avg: [-975.232 -975.232 -975.232] (1.000)
Step: 17499, Reward: [-436.877 -436.877 -436.877] [52.0390], Avg: [-973.843 -973.843 -973.843] (1.000)
Step: 17549, Reward: [-434.577 -434.577 -434.577] [52.3808], Avg: [-972.456 -972.456 -972.456] (1.000)
Step: 17599, Reward: [-465.021 -465.021 -465.021] [33.9863], Avg: [-971.111 -971.111 -971.111] (1.000)
Step: 17649, Reward: [-438.234 -438.234 -438.234] [25.9463], Avg: [-969.675 -969.675 -969.675] (1.000)
Step: 17699, Reward: [-445.525 -445.525 -445.525] [28.5806], Avg: [-968.275 -968.275 -968.275] (1.000)
Step: 17749, Reward: [-460.849 -460.849 -460.849] [37.9523], Avg: [-966.952 -966.952 -966.952] (1.000)
Step: 17799, Reward: [-397.316 -397.316 -397.316] [44.8041], Avg: [-965.478 -965.478 -965.478] (1.000)
Step: 17849, Reward: [-466.29 -466.29 -466.29] [87.1359], Avg: [-964.324 -964.324 -964.324] (1.000)
Step: 17899, Reward: [-431.986 -431.986 -431.986] [58.2454], Avg: [-963. -963. -963.] (1.000)
Step: 17949, Reward: [-440.566 -440.566 -440.566] [69.7516], Avg: [-961.739 -961.739 -961.739] (1.000)
Step: 17999, Reward: [-436.327 -436.327 -436.327] [58.4487], Avg: [-960.442 -960.442 -960.442] (1.000)
Step: 18049, Reward: [-422.568 -422.568 -422.568] [65.0403], Avg: [-959.132 -959.132 -959.132] (1.000)
Step: 18099, Reward: [-456.811 -456.811 -456.811] [43.2629], Avg: [-957.864 -957.864 -957.864] (1.000)
Step: 18149, Reward: [-420.702 -420.702 -420.702] [66.5133], Avg: [-956.567 -956.567 -956.567] (1.000)
Step: 18199, Reward: [-463.141 -463.141 -463.141] [73.6692], Avg: [-955.414 -955.414 -955.414] (1.000)
Step: 18249, Reward: [-485.861 -485.861 -485.861] [77.0546], Avg: [-954.339 -954.339 -954.339] (1.000)
Step: 18299, Reward: [-500.972 -500.972 -500.972] [98.8842], Avg: [-953.37 -953.37 -953.37] (1.000)
Step: 18349, Reward: [-494.003 -494.003 -494.003] [30.9056], Avg: [-952.203 -952.203 -952.203] (1.000)
Step: 18399, Reward: [-532.029 -532.029 -532.029] [70.0038], Avg: [-951.251 -951.251 -951.251] (1.000)
Step: 18449, Reward: [-627.756 -627.756 -627.756] [100.3754], Avg: [-950.646 -950.646 -950.646] (1.000)
Step: 18499, Reward: [-720.173 -720.173 -720.173] [117.7099], Avg: [-950.342 -950.342 -950.342] (1.000)
Step: 18549, Reward: [-960.414 -960.414 -960.414] [190.1140], Avg: [-950.881 -950.881 -950.881] (1.000)
Step: 18599, Reward: [-1814.792 -1814.792 -1814.792] [235.2022], Avg: [-953.836 -953.836 -953.836] (1.000)
Step: 18649, Reward: [-997.329 -997.329 -997.329] [120.6910], Avg: [-954.276 -954.276 -954.276] (1.000)
Step: 18699, Reward: [-1181.689 -1181.689 -1181.689] [116.8331], Avg: [-955.196 -955.196 -955.196] (1.000)
Step: 18749, Reward: [-926.827 -926.827 -926.827] [68.5781], Avg: [-955.304 -955.304 -955.304] (1.000)
Step: 18799, Reward: [-705.573 -705.573 -705.573] [37.4260], Avg: [-954.739 -954.739 -954.739] (1.000)
Step: 18849, Reward: [-659.909 -659.909 -659.909] [53.8627], Avg: [-954.1 -954.1 -954.1] (1.000)
Step: 18899, Reward: [-569.877 -569.877 -569.877] [80.2403], Avg: [-953.296 -953.296 -953.296] (1.000)
Step: 18949, Reward: [-576.364 -576.364 -576.364] [123.5650], Avg: [-952.627 -952.627 -952.627] (1.000)
Step: 18999, Reward: [-500.345 -500.345 -500.345] [85.2626], Avg: [-951.661 -951.661 -951.661] (1.000)
Step: 19049, Reward: [-479.137 -479.137 -479.137] [95.9342], Avg: [-950.673 -950.673 -950.673] (1.000)
Step: 19099, Reward: [-465.858 -465.858 -465.858] [22.7934], Avg: [-949.463 -949.463 -949.463] (1.000)
Step: 19149, Reward: [-473.114 -473.114 -473.114] [48.3466], Avg: [-948.346 -948.346 -948.346] (1.000)
Step: 19199, Reward: [-456.272 -456.272 -456.272] [38.3552], Avg: [-947.164 -947.164 -947.164] (1.000)
Step: 19249, Reward: [-485.52 -485.52 -485.52] [44.9165], Avg: [-946.082 -946.082 -946.082] (1.000)
Step: 19299, Reward: [-499.192 -499.192 -499.192] [43.1699], Avg: [-945.036 -945.036 -945.036] (1.000)
Step: 19349, Reward: [-469.884 -469.884 -469.884] [46.4988], Avg: [-943.928 -943.928 -943.928] (1.000)
Step: 19399, Reward: [-500.554 -500.554 -500.554] [63.6031], Avg: [-942.95 -942.95 -942.95] (1.000)
Step: 19449, Reward: [-524.436 -524.436 -524.436] [98.9708], Avg: [-942.128 -942.128 -942.128] (1.000)
Step: 19499, Reward: [-507.713 -507.713 -507.713] [49.7336], Avg: [-941.142 -941.142 -941.142] (1.000)
Step: 19549, Reward: [-524.47 -524.47 -524.47] [26.9385], Avg: [-940.145 -940.145 -940.145] (1.000)
Step: 19599, Reward: [-492.747 -492.747 -492.747] [60.6474], Avg: [-939.158 -939.158 -939.158] (1.000)
Step: 19649, Reward: [-494.231 -494.231 -494.231] [67.8421], Avg: [-938.199 -938.199 -938.199] (1.000)
Step: 19699, Reward: [-448.435 -448.435 -448.435] [44.1219], Avg: [-937.068 -937.068 -937.068] (1.000)
Step: 19749, Reward: [-527.759 -527.759 -527.759] [42.7514], Avg: [-936.14 -936.14 -936.14] (1.000)
Step: 19799, Reward: [-497.656 -497.656 -497.656] [52.8309], Avg: [-935.166 -935.166 -935.166] (1.000)
Step: 19849, Reward: [-449.575 -449.575 -449.575] [70.6887], Avg: [-934.121 -934.121 -934.121] (1.000)
Step: 19899, Reward: [-384.327 -384.327 -384.327] [44.6793], Avg: [-932.852 -932.852 -932.852] (1.000)
Step: 19949, Reward: [-358.782 -358.782 -358.782] [62.2726], Avg: [-931.569 -931.569 -931.569] (1.000)
Step: 19999, Reward: [-388.445 -388.445 -388.445] [60.7016], Avg: [-930.363 -930.363 -930.363] (1.000)
Step: 20049, Reward: [-447.525 -447.525 -447.525] [44.7290], Avg: [-929.27 -929.27 -929.27] (1.000)
Step: 20099, Reward: [-405.135 -405.135 -405.135] [60.9622], Avg: [-928.118 -928.118 -928.118] (1.000)
Step: 20149, Reward: [-495.331 -495.331 -495.331] [98.9477], Avg: [-927.29 -927.29 -927.29] (1.000)
Step: 20199, Reward: [-410.508 -410.508 -410.508] [54.4490], Avg: [-926.146 -926.146 -926.146] (1.000)
Step: 20249, Reward: [-571.263 -571.263 -571.263] [129.3995], Avg: [-925.589 -925.589 -925.589] (1.000)
Step: 20299, Reward: [-566.223 -566.223 -566.223] [96.9804], Avg: [-924.943 -924.943 -924.943] (1.000)
Step: 20349, Reward: [-701.889 -701.889 -701.889] [175.6049], Avg: [-924.826 -924.826 -924.826] (1.000)
Step: 20399, Reward: [-757.705 -757.705 -757.705] [48.4108], Avg: [-924.535 -924.535 -924.535] (1.000)
Step: 20449, Reward: [-712.991 -712.991 -712.991] [109.9574], Avg: [-924.287 -924.287 -924.287] (1.000)
Step: 20499, Reward: [-527.763 -527.763 -527.763] [81.6510], Avg: [-923.519 -923.519 -923.519] (1.000)
Step: 20549, Reward: [-520.738 -520.738 -520.738] [63.1198], Avg: [-922.692 -922.692 -922.692] (1.000)
Step: 20599, Reward: [-675.246 -675.246 -675.246] [97.5483], Avg: [-922.328 -922.328 -922.328] (1.000)
Step: 20649, Reward: [-582.156 -582.156 -582.156] [74.5865], Avg: [-921.685 -921.685 -921.685] (1.000)
Step: 20699, Reward: [-580.435 -580.435 -580.435] [175.9218], Avg: [-921.286 -921.286 -921.286] (1.000)
Step: 20749, Reward: [-474.693 -474.693 -474.693] [68.1678], Avg: [-920.374 -920.374 -920.374] (1.000)
Step: 20799, Reward: [-516.775 -516.775 -516.775] [86.3905], Avg: [-919.612 -919.612 -919.612] (1.000)
Step: 20849, Reward: [-426.976 -426.976 -426.976] [58.6323], Avg: [-918.571 -918.571 -918.571] (1.000)
Step: 20899, Reward: [-418.126 -418.126 -418.126] [65.3169], Avg: [-917.53 -917.53 -917.53] (1.000)
Step: 20949, Reward: [-472.188 -472.188 -472.188] [151.4242], Avg: [-916.828 -916.828 -916.828] (1.000)
Step: 20999, Reward: [-446.664 -446.664 -446.664] [75.7555], Avg: [-915.889 -915.889 -915.889] (1.000)
Step: 21049, Reward: [-478.873 -478.873 -478.873] [128.7415], Avg: [-915.157 -915.157 -915.157] (1.000)
Step: 21099, Reward: [-537.587 -537.587 -537.587] [134.7376], Avg: [-914.582 -914.582 -914.582] (1.000)
Step: 21149, Reward: [-425.633 -425.633 -425.633] [113.3733], Avg: [-913.694 -913.694 -913.694] (1.000)
Step: 21199, Reward: [-542.607 -542.607 -542.607] [69.1460], Avg: [-912.982 -912.982 -912.982] (1.000)
Step: 21249, Reward: [-400.205 -400.205 -400.205] [71.3844], Avg: [-911.943 -911.943 -911.943] (1.000)
Step: 21299, Reward: [-427.081 -427.081 -427.081] [45.3776], Avg: [-910.911 -910.911 -910.911] (1.000)
Step: 21349, Reward: [-525.1 -525.1 -525.1] [78.8485], Avg: [-910.192 -910.192 -910.192] (1.000)
Step: 21399, Reward: [-466.078 -466.078 -466.078] [93.4289], Avg: [-909.373 -909.373 -909.373] (1.000)
Step: 21449, Reward: [-378.893 -378.893 -378.893] [57.8705], Avg: [-908.271 -908.271 -908.271] (1.000)
Step: 21499, Reward: [-428.8 -428.8 -428.8] [58.9469], Avg: [-907.294 -907.294 -907.294] (1.000)
Step: 21549, Reward: [-403.693 -403.693 -403.693] [71.9556], Avg: [-906.292 -906.292 -906.292] (1.000)
Step: 21599, Reward: [-464.864 -464.864 -464.864] [170.1683], Avg: [-905.664 -905.664 -905.664] (1.000)
Step: 21649, Reward: [-494.724 -494.724 -494.724] [37.6825], Avg: [-904.802 -904.802 -904.802] (1.000)
Step: 21699, Reward: [-446.605 -446.605 -446.605] [79.9345], Avg: [-903.93 -903.93 -903.93] (1.000)
Step: 21749, Reward: [-415.044 -415.044 -415.044] [82.7270], Avg: [-902.997 -902.997 -902.997] (1.000)
Step: 21799, Reward: [-412.795 -412.795 -412.795] [70.4721], Avg: [-902.034 -902.034 -902.034] (1.000)
Step: 21849, Reward: [-393.463 -393.463 -393.463] [51.7549], Avg: [-900.989 -900.989 -900.989] (1.000)
Step: 21899, Reward: [-564.118 -564.118 -564.118] [100.2122], Avg: [-900.448 -900.448 -900.448] (1.000)
Step: 21949, Reward: [-429.102 -429.102 -429.102] [95.9966], Avg: [-899.593 -899.593 -899.593] (1.000)
Step: 21999, Reward: [-498.344 -498.344 -498.344] [140.8262], Avg: [-899.002 -899.002 -899.002] (1.000)
Step: 22049, Reward: [-388.145 -388.145 -388.145] [79.1888], Avg: [-898.023 -898.023 -898.023] (1.000)
Step: 22099, Reward: [-465.098 -465.098 -465.098] [56.6929], Avg: [-897.172 -897.172 -897.172] (1.000)
Step: 22149, Reward: [-466.778 -466.778 -466.778] [68.8588], Avg: [-896.355 -896.355 -896.355] (1.000)
Step: 22199, Reward: [-464.678 -464.678 -464.678] [100.5684], Avg: [-895.61 -895.61 -895.61] (1.000)
Step: 22249, Reward: [-1193.615 -1193.615 -1193.615] [407.0329], Avg: [-897.194 -897.194 -897.194] (1.000)
Step: 22299, Reward: [-1123.545 -1123.545 -1123.545] [872.3867], Avg: [-899.658 -899.658 -899.658] (1.000)
Step: 22349, Reward: [-818.439 -818.439 -818.439] [302.4177], Avg: [-900.152 -900.152 -900.152] (1.000)
Step: 22399, Reward: [-489.857 -489.857 -489.857] [99.3929], Avg: [-899.458 -899.458 -899.458] (1.000)
Step: 22449, Reward: [-462.95 -462.95 -462.95] [67.6668], Avg: [-898.637 -898.637 -898.637] (1.000)
Step: 22499, Reward: [-538.764 -538.764 -538.764] [52.5804], Avg: [-897.954 -897.954 -897.954] (1.000)
Step: 22549, Reward: [-486.297 -486.297 -486.297] [59.8886], Avg: [-897.174 -897.174 -897.174] (1.000)
Step: 22599, Reward: [-501.432 -501.432 -501.432] [84.2412], Avg: [-896.485 -896.485 -896.485] (1.000)
Step: 22649, Reward: [-504.043 -504.043 -504.043] [62.8433], Avg: [-895.757 -895.757 -895.757] (1.000)
Step: 22699, Reward: [-423.075 -423.075 -423.075] [72.6740], Avg: [-894.876 -894.876 -894.876] (1.000)
Step: 22749, Reward: [-421.986 -421.986 -421.986] [50.7377], Avg: [-893.948 -893.948 -893.948] (1.000)
Step: 22799, Reward: [-437.66 -437.66 -437.66] [112.2430], Avg: [-893.194 -893.194 -893.194] (1.000)
Step: 22849, Reward: [-464.107 -464.107 -464.107] [99.2878], Avg: [-892.472 -892.472 -892.472] (1.000)
Step: 22899, Reward: [-472.516 -472.516 -472.516] [129.9847], Avg: [-891.839 -891.839 -891.839] (1.000)
Step: 22949, Reward: [-469.519 -469.519 -469.519] [85.7332], Avg: [-891.106 -891.106 -891.106] (1.000)
Step: 22999, Reward: [-474.715 -474.715 -474.715] [90.6842], Avg: [-890.398 -890.398 -890.398] (1.000)
Step: 23049, Reward: [-504.005 -504.005 -504.005] [60.8968], Avg: [-889.692 -889.692 -889.692] (1.000)
Step: 23099, Reward: [-598.382 -598.382 -598.382] [86.1384], Avg: [-889.248 -889.248 -889.248] (1.000)
Step: 23149, Reward: [-583.88 -583.88 -583.88] [106.6389], Avg: [-888.818 -888.818 -888.818] (1.000)
Step: 23199, Reward: [-530.775 -530.775 -530.775] [119.6557], Avg: [-888.305 -888.305 -888.305] (1.000)
Step: 23249, Reward: [-636.155 -636.155 -636.155] [128.8171], Avg: [-888.039 -888.039 -888.039] (1.000)
Step: 23299, Reward: [-657.722 -657.722 -657.722] [115.2671], Avg: [-887.793 -887.793 -887.793] (1.000)
Step: 23349, Reward: [-689.692 -689.692 -689.692] [154.5166], Avg: [-887.699 -887.699 -887.699] (1.000)
Step: 23399, Reward: [-551.384 -551.384 -551.384] [87.6705], Avg: [-887.168 -887.168 -887.168] (1.000)
Step: 23449, Reward: [-718.849 -718.849 -718.849] [184.7079], Avg: [-887.203 -887.203 -887.203] (1.000)
Step: 23499, Reward: [-721.124 -721.124 -721.124] [111.0355], Avg: [-887.086 -887.086 -887.086] (1.000)
Step: 23549, Reward: [-749.055 -749.055 -749.055] [164.7356], Avg: [-887.142 -887.142 -887.142] (1.000)
Step: 23599, Reward: [-654.363 -654.363 -654.363] [158.8938], Avg: [-886.986 -886.986 -886.986] (1.000)
Step: 23649, Reward: [-623.19 -623.19 -623.19] [54.9511], Avg: [-886.544 -886.544 -886.544] (1.000)
Step: 23699, Reward: [-701.267 -701.267 -701.267] [71.1002], Avg: [-886.304 -886.304 -886.304] (1.000)
Step: 23749, Reward: [-610.593 -610.593 -610.593] [45.6939], Avg: [-885.819 -885.819 -885.819] (1.000)
Step: 23799, Reward: [-615.116 -615.116 -615.116] [40.9113], Avg: [-885.337 -885.337 -885.337] (1.000)
Step: 23849, Reward: [-620.109 -620.109 -620.109] [98.2977], Avg: [-884.987 -884.987 -884.987] (1.000)
Step: 23899, Reward: [-577.799 -577.799 -577.799] [62.8057], Avg: [-884.475 -884.475 -884.475] (1.000)
Step: 23949, Reward: [-591.927 -591.927 -591.927] [93.7824], Avg: [-884.06 -884.06 -884.06] (1.000)
Step: 23999, Reward: [-554.051 -554.051 -554.051] [78.7731], Avg: [-883.537 -883.537 -883.537] (1.000)
Step: 24049, Reward: [-536.532 -536.532 -536.532] [77.4606], Avg: [-882.977 -882.977 -882.977] (1.000)
Step: 24099, Reward: [-633.258 -633.258 -633.258] [64.6987], Avg: [-882.593 -882.593 -882.593] (1.000)
Step: 24149, Reward: [-641.422 -641.422 -641.422] [54.7429], Avg: [-882.207 -882.207 -882.207] (1.000)
Step: 24199, Reward: [-628.418 -628.418 -628.418] [26.0478], Avg: [-881.736 -881.736 -881.736] (1.000)
Step: 24249, Reward: [-598.12 -598.12 -598.12] [41.3671], Avg: [-881.237 -881.237 -881.237] (1.000)
Step: 24299, Reward: [-597.599 -597.599 -597.599] [54.4659], Avg: [-880.765 -880.765 -880.765] (1.000)
Step: 24349, Reward: [-615.898 -615.898 -615.898] [87.9974], Avg: [-880.402 -880.402 -880.402] (1.000)
Step: 24399, Reward: [-536.365 -536.365 -536.365] [49.8030], Avg: [-879.799 -879.799 -879.799] (1.000)
Step: 24449, Reward: [-554.083 -554.083 -554.083] [111.7534], Avg: [-879.361 -879.361 -879.361] (1.000)
Step: 24499, Reward: [-563.545 -563.545 -563.545] [11.9918], Avg: [-878.741 -878.741 -878.741] (1.000)
Step: 24549, Reward: [-506.386 -506.386 -506.386] [61.5556], Avg: [-878.108 -878.108 -878.108] (1.000)
Step: 24599, Reward: [-527.91 -527.91 -527.91] [49.9963], Avg: [-877.498 -877.498 -877.498] (1.000)
Step: 24649, Reward: [-519.96 -519.96 -519.96] [52.9369], Avg: [-876.88 -876.88 -876.88] (1.000)
Step: 24699, Reward: [-509.214 -509.214 -509.214] [70.3720], Avg: [-876.279 -876.279 -876.279] (1.000)
Step: 24749, Reward: [-530.741 -530.741 -530.741] [33.7623], Avg: [-875.649 -875.649 -875.649] (1.000)
Step: 24799, Reward: [-483.487 -483.487 -483.487] [65.0569], Avg: [-874.989 -874.989 -874.989] (1.000)
Step: 24849, Reward: [-571.151 -571.151 -571.151] [19.2910], Avg: [-874.417 -874.417 -874.417] (1.000)
Step: 24899, Reward: [-568.604 -568.604 -568.604] [64.8805], Avg: [-873.933 -873.933 -873.933] (1.000)
Step: 24949, Reward: [-594.935 -594.935 -594.935] [76.2819], Avg: [-873.527 -873.527 -873.527] (1.000)
Step: 24999, Reward: [-682.451 -682.451 -682.451] [36.3553], Avg: [-873.217 -873.217 -873.217] (1.000)
Step: 25049, Reward: [-798.843 -798.843 -798.843] [92.4575], Avg: [-873.253 -873.253 -873.253] (1.000)
Step: 25099, Reward: [-737.875 -737.875 -737.875] [89.0638], Avg: [-873.161 -873.161 -873.161] (1.000)
Step: 25149, Reward: [-789.539 -789.539 -789.539] [63.7570], Avg: [-873.122 -873.122 -873.122] (1.000)
Step: 25199, Reward: [-850.215 -850.215 -850.215] [114.2864], Avg: [-873.303 -873.303 -873.303] (1.000)
Step: 25249, Reward: [-1101.865 -1101.865 -1101.865] [154.6447], Avg: [-874.062 -874.062 -874.062] (1.000)
Step: 25299, Reward: [-1493.798 -1493.798 -1493.798] [363.9755], Avg: [-876.006 -876.006 -876.006] (1.000)
Step: 25349, Reward: [-1109.725 -1109.725 -1109.725] [172.6788], Avg: [-876.807 -876.807 -876.807] (1.000)
Step: 25399, Reward: [-795.874 -795.874 -795.874] [93.4769], Avg: [-876.832 -876.832 -876.832] (1.000)
Step: 25449, Reward: [-1104.275 -1104.275 -1104.275] [123.0559], Avg: [-877.521 -877.521 -877.521] (1.000)
Step: 25499, Reward: [-1026.099 -1026.099 -1026.099] [112.7443], Avg: [-878.033 -878.033 -878.033] (1.000)
Step: 25549, Reward: [-821.696 -821.696 -821.696] [140.6394], Avg: [-878.198 -878.198 -878.198] (1.000)
Step: 25599, Reward: [-747.709 -747.709 -747.709] [98.0890], Avg: [-878.135 -878.135 -878.135] (1.000)
Step: 25649, Reward: [-622.748 -622.748 -622.748] [61.4910], Avg: [-877.757 -877.757 -877.757] (1.000)
Step: 25699, Reward: [-514.668 -514.668 -514.668] [24.8149], Avg: [-877.099 -877.099 -877.099] (1.000)
Step: 25749, Reward: [-488.173 -488.173 -488.173] [47.6918], Avg: [-876.436 -876.436 -876.436] (1.000)
Step: 25799, Reward: [-506.918 -506.918 -506.918] [74.8483], Avg: [-875.865 -875.865 -875.865] (1.000)
Step: 25849, Reward: [-565.389 -565.389 -565.389] [78.3933], Avg: [-875.416 -875.416 -875.416] (1.000)
Step: 25899, Reward: [-477.121 -477.121 -477.121] [62.5608], Avg: [-874.768 -874.768 -874.768] (1.000)
Step: 25949, Reward: [-445.438 -445.438 -445.438] [25.8059], Avg: [-873.991 -873.991 -873.991] (1.000)
Step: 25999, Reward: [-453.131 -453.131 -453.131] [42.6898], Avg: [-873.263 -873.263 -873.263] (1.000)
Step: 26049, Reward: [-497. -497. -497.] [55.5385], Avg: [-872.648 -872.648 -872.648] (1.000)
Step: 26099, Reward: [-510.196 -510.196 -510.196] [50.0412], Avg: [-872.049 -872.049 -872.049] (1.000)
Step: 26149, Reward: [-526.894 -526.894 -526.894] [55.9521], Avg: [-871.496 -871.496 -871.496] (1.000)
Step: 26199, Reward: [-460.851 -460.851 -460.851] [35.8472], Avg: [-870.781 -870.781 -870.781] (1.000)
Step: 26249, Reward: [-516.533 -516.533 -516.533] [40.3736], Avg: [-870.183 -870.183 -870.183] (1.000)
Step: 26299, Reward: [-505.82 -505.82 -505.82] [59.8433], Avg: [-869.604 -869.604 -869.604] (1.000)
Step: 26349, Reward: [-509.82 -509.82 -509.82] [39.5208], Avg: [-868.996 -868.996 -868.996] (1.000)
Step: 26399, Reward: [-535.441 -535.441 -535.441] [47.8857], Avg: [-868.455 -868.455 -868.455] (1.000)
Step: 26449, Reward: [-543.901 -543.901 -543.901] [91.9822], Avg: [-868.016 -868.016 -868.016] (1.000)
Step: 26499, Reward: [-550.637 -550.637 -550.637] [39.0026], Avg: [-867.491 -867.491 -867.491] (1.000)
Step: 26549, Reward: [-569.845 -569.845 -569.845] [79.0296], Avg: [-867.079 -867.079 -867.079] (1.000)
Step: 26599, Reward: [-630.241 -630.241 -630.241] [71.2518], Avg: [-866.768 -866.768 -866.768] (1.000)
Step: 26649, Reward: [-741.253 -741.253 -741.253] [94.0892], Avg: [-866.709 -866.709 -866.709] (1.000)
Step: 26699, Reward: [-755.901 -755.901 -755.901] [129.7433], Avg: [-866.744 -866.744 -866.744] (1.000)
Step: 26749, Reward: [-652.448 -652.448 -652.448] [52.9275], Avg: [-866.442 -866.442 -866.442] (1.000)
Step: 26799, Reward: [-565.302 -565.302 -565.302] [136.4940], Avg: [-866.135 -866.135 -866.135] (1.000)
Step: 26849, Reward: [-504.725 -504.725 -504.725] [55.9017], Avg: [-865.566 -865.566 -865.566] (1.000)
Step: 26899, Reward: [-526.587 -526.587 -526.587] [54.8566], Avg: [-865.038 -865.038 -865.038] (1.000)
Step: 26949, Reward: [-476.772 -476.772 -476.772] [83.0574], Avg: [-864.472 -864.472 -864.472] (1.000)
Step: 26999, Reward: [-496.658 -496.658 -496.658] [58.3197], Avg: [-863.899 -863.899 -863.899] (1.000)
Step: 27049, Reward: [-460.392 -460.392 -460.392] [22.6065], Avg: [-863.195 -863.195 -863.195] (1.000)
Step: 27099, Reward: [-430.191 -430.191 -430.191] [59.2048], Avg: [-862.505 -862.505 -862.505] (1.000)
Step: 27149, Reward: [-455.472 -455.472 -455.472] [42.5162], Avg: [-861.834 -861.834 -861.834] (1.000)
Step: 27199, Reward: [-391.246 -391.246 -391.246] [39.2834], Avg: [-861.041 -861.041 -861.041] (1.000)
Step: 27249, Reward: [-464.15 -464.15 -464.15] [81.0953], Avg: [-860.462 -860.462 -860.462] (1.000)
Step: 27299, Reward: [-431.197 -431.197 -431.197] [38.0155], Avg: [-859.745 -859.745 -859.745] (1.000)
Step: 27349, Reward: [-411.835 -411.835 -411.835] [34.5730], Avg: [-858.989 -858.989 -858.989] (1.000)
Step: 27399, Reward: [-386.503 -386.503 -386.503] [46.9715], Avg: [-858.213 -858.213 -858.213] (1.000)
Step: 27449, Reward: [-392.961 -392.961 -392.961] [57.3854], Avg: [-857.47 -857.47 -857.47] (1.000)
Step: 27499, Reward: [-378.488 -378.488 -378.488] [39.9440], Avg: [-856.672 -856.672 -856.672] (1.000)
Step: 27549, Reward: [-404.253 -404.253 -404.253] [82.7639], Avg: [-856.001 -856.001 -856.001] (1.000)
Step: 27599, Reward: [-451.406 -451.406 -451.406] [43.9450], Avg: [-855.347 -855.347 -855.347] (1.000)
Step: 27649, Reward: [-436.45 -436.45 -436.45] [67.1001], Avg: [-854.711 -854.711 -854.711] (1.000)
Step: 27699, Reward: [-433.8 -433.8 -433.8] [28.6927], Avg: [-854.003 -854.003 -854.003] (1.000)
Step: 27749, Reward: [-460.08 -460.08 -460.08] [86.1774], Avg: [-853.449 -853.449 -853.449] (1.000)
Step: 27799, Reward: [-417.882 -417.882 -417.882] [56.6628], Avg: [-852.767 -852.767 -852.767] (1.000)
Step: 27849, Reward: [-455.023 -455.023 -455.023] [80.2694], Avg: [-852.197 -852.197 -852.197] (1.000)
Step: 27899, Reward: [-428.33 -428.33 -428.33] [63.2601], Avg: [-851.551 -851.551 -851.551] (1.000)
Step: 27949, Reward: [-460.269 -460.269 -460.269] [36.6120], Avg: [-850.917 -850.917 -850.917] (1.000)
Step: 27999, Reward: [-433.664 -433.664 -433.664] [53.3772], Avg: [-850.267 -850.267 -850.267] (1.000)
Step: 28049, Reward: [-472.533 -472.533 -472.533] [43.7011], Avg: [-849.671 -849.671 -849.671] (1.000)
Step: 28099, Reward: [-412.509 -412.509 -412.509] [36.7565], Avg: [-848.959 -848.959 -848.959] (1.000)
Step: 28149, Reward: [-462.661 -462.661 -462.661] [22.3031], Avg: [-848.312 -848.312 -848.312] (1.000)
Step: 28199, Reward: [-410.568 -410.568 -410.568] [58.2168], Avg: [-847.639 -847.639 -847.639] (1.000)
Step: 28249, Reward: [-468.124 -468.124 -468.124] [61.0909], Avg: [-847.076 -847.076 -847.076] (1.000)
Step: 28299, Reward: [-394.737 -394.737 -394.737] [47.2939], Avg: [-846.36 -846.36 -846.36] (1.000)
Step: 28349, Reward: [-376.457 -376.457 -376.457] [22.7248], Avg: [-845.572 -845.572 -845.572] (1.000)
Step: 28399, Reward: [-376.56 -376.56 -376.56] [31.0558], Avg: [-844.801 -844.801 -844.801] (1.000)
Step: 28449, Reward: [-413.806 -413.806 -413.806] [80.8016], Avg: [-844.185 -844.185 -844.185] (1.000)
Step: 28499, Reward: [-417.591 -417.591 -417.591] [47.4379], Avg: [-843.52 -843.52 -843.52] (1.000)
Step: 28549, Reward: [-394.985 -394.985 -394.985] [34.8652], Avg: [-842.795 -842.795 -842.795] (1.000)
Step: 28599, Reward: [-458.766 -458.766 -458.766] [65.3317], Avg: [-842.238 -842.238 -842.238] (1.000)
Step: 28649, Reward: [-439.247 -439.247 -439.247] [45.3983], Avg: [-841.614 -841.614 -841.614] (1.000)
Step: 28699, Reward: [-499.579 -499.579 -499.579] [45.2899], Avg: [-841.097 -841.097 -841.097] (1.000)
Step: 28749, Reward: [-412.518 -412.518 -412.518] [92.7142], Avg: [-840.513 -840.513 -840.513] (1.000)
Step: 28799, Reward: [-475.485 -475.485 -475.485] [52.5899], Avg: [-839.971 -839.971 -839.971] (1.000)
Step: 28849, Reward: [-432.106 -432.106 -432.106] [57.9558], Avg: [-839.364 -839.364 -839.364] (1.000)
Step: 28899, Reward: [-411.118 -411.118 -411.118] [52.6583], Avg: [-838.714 -838.714 -838.714] (1.000)
Step: 28949, Reward: [-423.735 -423.735 -423.735] [51.7857], Avg: [-838.087 -838.087 -838.087] (1.000)
Step: 28999, Reward: [-495.192 -495.192 -495.192] [67.5570], Avg: [-837.612 -837.612 -837.612] (1.000)
Step: 29049, Reward: [-427.872 -427.872 -427.872] [36.2225], Avg: [-836.97 -836.97 -836.97] (1.000)
Step: 29099, Reward: [-490.326 -490.326 -490.326] [51.3892], Avg: [-836.462 -836.462 -836.462] (1.000)
Step: 29149, Reward: [-466.469 -466.469 -466.469] [59.9620], Avg: [-835.93 -835.93 -835.93] (1.000)
Step: 29199, Reward: [-481.138 -481.138 -481.138] [54.0797], Avg: [-835.416 -835.416 -835.416] (1.000)
Step: 29249, Reward: [-473.766 -473.766 -473.766] [47.6423], Avg: [-834.879 -834.879 -834.879] (1.000)
Step: 29299, Reward: [-464.561 -464.561 -464.561] [92.7843], Avg: [-834.405 -834.405 -834.405] (1.000)
Step: 29349, Reward: [-521.484 -521.484 -521.484] [62.1536], Avg: [-833.978 -833.978 -833.978] (1.000)
Step: 29399, Reward: [-512.42 -512.42 -512.42] [89.0152], Avg: [-833.583 -833.583 -833.583] (1.000)
Step: 29449, Reward: [-426.518 -426.518 -426.518] [39.4115], Avg: [-832.958 -832.958 -832.958] (1.000)
Step: 29499, Reward: [-480.24 -480.24 -480.24] [142.5541], Avg: [-832.602 -832.602 -832.602] (1.000)
Step: 29549, Reward: [-481.796 -481.796 -481.796] [63.4787], Avg: [-832.116 -832.116 -832.116] (1.000)
Step: 29599, Reward: [-379.065 -379.065 -379.065] [84.5551], Avg: [-831.493 -831.493 -831.493] (1.000)
Step: 29649, Reward: [-514.206 -514.206 -514.206] [66.3482], Avg: [-831.07 -831.07 -831.07] (1.000)
Step: 29699, Reward: [-534.267 -534.267 -534.267] [96.2422], Avg: [-830.733 -830.733 -830.733] (1.000)
Step: 29749, Reward: [-517.658 -517.658 -517.658] [101.7124], Avg: [-830.377 -830.377 -830.377] (1.000)
Step: 29799, Reward: [-506.118 -506.118 -506.118] [114.4479], Avg: [-830.025 -830.025 -830.025] (1.000)
Step: 29849, Reward: [-455.799 -455.799 -455.799] [28.1156], Avg: [-829.446 -829.446 -829.446] (1.000)
Step: 29899, Reward: [-567.392 -567.392 -567.392] [24.4274], Avg: [-829.048 -829.048 -829.048] (1.000)
Step: 29949, Reward: [-554.734 -554.734 -554.734] [49.2414], Avg: [-828.673 -828.673 -828.673] (1.000)
Step: 29999, Reward: [-491.419 -491.419 -491.419] [59.9119], Avg: [-828.21 -828.21 -828.21] (1.000)
Step: 30049, Reward: [-530.038 -530.038 -530.038] [73.9754], Avg: [-827.837 -827.837 -827.837] (1.000)
Step: 30099, Reward: [-568.011 -568.011 -568.011] [41.2031], Avg: [-827.474 -827.474 -827.474] (1.000)
Step: 30149, Reward: [-523.992 -523.992 -523.992] [73.6569], Avg: [-827.093 -827.093 -827.093] (1.000)
Step: 30199, Reward: [-505.148 -505.148 -505.148] [65.3094], Avg: [-826.668 -826.668 -826.668] (1.000)
Step: 30249, Reward: [-475.688 -475.688 -475.688] [34.3905], Avg: [-826.145 -826.145 -826.145] (1.000)
Step: 30299, Reward: [-507.765 -507.765 -507.765] [49.2298], Avg: [-825.701 -825.701 -825.701] (1.000)
Step: 30349, Reward: [-468.238 -468.238 -468.238] [91.8321], Avg: [-825.263 -825.263 -825.263] (1.000)
Step: 30399, Reward: [-518.06 -518.06 -518.06] [121.7817], Avg: [-824.958 -824.958 -824.958] (1.000)
Step: 30449, Reward: [-498.825 -498.825 -498.825] [58.0707], Avg: [-824.518 -824.518 -824.518] (1.000)
Step: 30499, Reward: [-429.988 -429.988 -429.988] [100.1865], Avg: [-824.035 -824.035 -824.035] (1.000)
Step: 30549, Reward: [-467.444 -467.444 -467.444] [73.3239], Avg: [-823.572 -823.572 -823.572] (1.000)
Step: 30599, Reward: [-543.256 -543.256 -543.256] [36.0343], Avg: [-823.173 -823.173 -823.173] (1.000)
Step: 30649, Reward: [-574.51 -574.51 -574.51] [56.4274], Avg: [-822.859 -822.859 -822.859] (1.000)
Step: 30699, Reward: [-543.004 -543.004 -543.004] [29.9271], Avg: [-822.452 -822.452 -822.452] (1.000)
Step: 30749, Reward: [-538.551 -538.551 -538.551] [60.4995], Avg: [-822.089 -822.089 -822.089] (1.000)
Step: 30799, Reward: [-583.782 -583.782 -583.782] [72.6661], Avg: [-821.82 -821.82 -821.82] (1.000)
Step: 30849, Reward: [-592.247 -592.247 -592.247] [84.3320], Avg: [-821.584 -821.584 -821.584] (1.000)
Step: 30899, Reward: [-545.707 -545.707 -545.707] [18.9831], Avg: [-821.169 -821.169 -821.169] (1.000)
Step: 30949, Reward: [-588.538 -588.538 -588.538] [41.5316], Avg: [-820.86 -820.86 -820.86] (1.000)
Step: 30999, Reward: [-555.928 -555.928 -555.928] [41.9508], Avg: [-820.5 -820.5 -820.5] (1.000)
Step: 31049, Reward: [-616.641 -616.641 -616.641] [109.5554], Avg: [-820.348 -820.348 -820.348] (1.000)
Step: 31099, Reward: [-671.524 -671.524 -671.524] [49.1779], Avg: [-820.188 -820.188 -820.188] (1.000)
Step: 31149, Reward: [-526.564 -526.564 -526.564] [35.6066], Avg: [-819.774 -819.774 -819.774] (1.000)
Step: 31199, Reward: [-533.77 -533.77 -533.77] [103.2011], Avg: [-819.481 -819.481 -819.481] (1.000)
Step: 31249, Reward: [-523.069 -523.069 -523.069] [76.0141], Avg: [-819.129 -819.129 -819.129] (1.000)
Step: 31299, Reward: [-500.849 -500.849 -500.849] [85.0427], Avg: [-818.756 -818.756 -818.756] (1.000)
Step: 31349, Reward: [-515.84 -515.84 -515.84] [44.9669], Avg: [-818.345 -818.345 -818.345] (1.000)
Step: 31399, Reward: [-496.182 -496.182 -496.182] [61.7415], Avg: [-817.93 -817.93 -817.93] (1.000)
Step: 31449, Reward: [-525.293 -525.293 -525.293] [37.8692], Avg: [-817.525 -817.525 -817.525] (1.000)
Step: 31499, Reward: [-486.873 -486.873 -486.873] [73.7624], Avg: [-817.117 -817.117 -817.117] (1.000)
Step: 31549, Reward: [-453.529 -453.529 -453.529] [62.0605], Avg: [-816.639 -816.639 -816.639] (1.000)
Step: 31599, Reward: [-455.729 -455.729 -455.729] [90.6624], Avg: [-816.212 -816.212 -816.212] (1.000)
Step: 31649, Reward: [-451.033 -451.033 -451.033] [44.5852], Avg: [-815.705 -815.705 -815.705] (1.000)
Step: 31699, Reward: [-458.375 -458.375 -458.375] [78.0020], Avg: [-815.265 -815.265 -815.265] (1.000)
Step: 31749, Reward: [-496.883 -496.883 -496.883] [52.4281], Avg: [-814.846 -814.846 -814.846] (1.000)
Step: 31799, Reward: [-492.585 -492.585 -492.585] [50.5225], Avg: [-814.418 -814.418 -814.418] (1.000)
Step: 31849, Reward: [-459.731 -459.731 -459.731] [95.3477], Avg: [-814.011 -814.011 -814.011] (1.000)
Step: 31899, Reward: [-429.222 -429.222 -429.222] [50.1135], Avg: [-813.487 -813.487 -813.487] (1.000)
Step: 31949, Reward: [-420.187 -420.187 -420.187] [21.4890], Avg: [-812.905 -812.905 -812.905] (1.000)
Step: 31999, Reward: [-439.976 -439.976 -439.976] [38.8136], Avg: [-812.383 -812.383 -812.383] (1.000)
Step: 32049, Reward: [-409.773 -409.773 -409.773] [55.2805], Avg: [-811.841 -811.841 -811.841] (1.000)
Step: 32099, Reward: [-434.132 -434.132 -434.132] [70.9361], Avg: [-811.363 -811.363 -811.363] (1.000)
Step: 32149, Reward: [-433.495 -433.495 -433.495] [57.6511], Avg: [-810.865 -810.865 -810.865] (1.000)
Step: 32199, Reward: [-411.344 -411.344 -411.344] [49.3880], Avg: [-810.321 -810.321 -810.321] (1.000)
Step: 32249, Reward: [-382.255 -382.255 -382.255] [59.4577], Avg: [-809.75 -809.75 -809.75] (1.000)
Step: 32299, Reward: [-499.625 -499.625 -499.625] [36.3154], Avg: [-809.326 -809.326 -809.326] (1.000)
Step: 32349, Reward: [-521.36 -521.36 -521.36] [53.1874], Avg: [-808.963 -808.963 -808.963] (1.000)
Step: 32399, Reward: [-442.001 -442.001 -442.001] [45.7183], Avg: [-808.468 -808.468 -808.468] (1.000)
Step: 32449, Reward: [-462.596 -462.596 -462.596] [57.8226], Avg: [-808.024 -808.024 -808.024] (1.000)
Step: 32499, Reward: [-485.611 -485.611 -485.611] [80.1684], Avg: [-807.651 -807.651 -807.651] (1.000)
Step: 32549, Reward: [-423.623 -423.623 -423.623] [44.1093], Avg: [-807.129 -807.129 -807.129] (1.000)
Step: 32599, Reward: [-420.993 -420.993 -420.993] [30.3254], Avg: [-806.583 -806.583 -806.583] (1.000)
Step: 32649, Reward: [-410.997 -410.997 -410.997] [65.4779], Avg: [-806.078 -806.078 -806.078] (1.000)
Step: 32699, Reward: [-419.566 -419.566 -419.566] [56.2301], Avg: [-805.573 -805.573 -805.573] (1.000)
Step: 32749, Reward: [-422.316 -422.316 -422.316] [53.9715], Avg: [-805.07 -805.07 -805.07] (1.000)
Step: 32799, Reward: [-427.381 -427.381 -427.381] [45.1564], Avg: [-804.563 -804.563 -804.563] (1.000)
Step: 32849, Reward: [-431.617 -431.617 -431.617] [75.2319], Avg: [-804.11 -804.11 -804.11] (1.000)
Step: 32899, Reward: [-524.593 -524.593 -524.593] [86.4724], Avg: [-803.816 -803.816 -803.816] (1.000)
Step: 32949, Reward: [-483.625 -483.625 -483.625] [75.0011], Avg: [-803.444 -803.444 -803.444] (1.000)
Step: 32999, Reward: [-504.013 -504.013 -504.013] [105.8209], Avg: [-803.151 -803.151 -803.151] (1.000)
Step: 33049, Reward: [-567.284 -567.284 -567.284] [59.5359], Avg: [-802.884 -802.884 -802.884] (1.000)
Step: 33099, Reward: [-553.869 -553.869 -553.869] [118.1501], Avg: [-802.687 -802.687 -802.687] (1.000)
Step: 33149, Reward: [-551.965 -551.965 -551.965] [117.2166], Avg: [-802.485 -802.485 -802.485] (1.000)
Step: 33199, Reward: [-594.828 -594.828 -594.828] [65.2418], Avg: [-802.271 -802.271 -802.271] (1.000)
Step: 33249, Reward: [-588.348 -588.348 -588.348] [130.7745], Avg: [-802.146 -802.146 -802.146] (1.000)
Step: 33299, Reward: [-644.18 -644.18 -644.18] [146.3232], Avg: [-802.128 -802.128 -802.128] (1.000)
Step: 33349, Reward: [-624.771 -624.771 -624.771] [91.9383], Avg: [-802. -802. -802.] (1.000)
Step: 33399, Reward: [-549.126 -549.126 -549.126] [126.8932], Avg: [-801.812 -801.812 -801.812] (1.000)
Step: 33449, Reward: [-526.065 -526.065 -526.065] [89.7677], Avg: [-801.534 -801.534 -801.534] (1.000)
Step: 33499, Reward: [-462.042 -462.042 -462.042] [57.6874], Avg: [-801.113 -801.113 -801.113] (1.000)
Step: 33549, Reward: [-509.678 -509.678 -509.678] [135.3612], Avg: [-800.88 -800.88 -800.88] (1.000)
Step: 33599, Reward: [-496.82 -496.82 -496.82] [75.2915], Avg: [-800.54 -800.54 -800.54] (1.000)
Step: 33649, Reward: [-592.142 -592.142 -592.142] [188.5914], Avg: [-800.51 -800.51 -800.51] (1.000)
Step: 33699, Reward: [-592.43 -592.43 -592.43] [109.3027], Avg: [-800.364 -800.364 -800.364] (1.000)
Step: 33749, Reward: [-537.774 -537.774 -537.774] [130.4419], Avg: [-800.168 -800.168 -800.168] (1.000)
Step: 33799, Reward: [-558.905 -558.905 -558.905] [113.7278], Avg: [-799.979 -799.979 -799.979] (1.000)
Step: 33849, Reward: [-943.355 -943.355 -943.355] [369.9948], Avg: [-800.738 -800.738 -800.738] (1.000)
Step: 33899, Reward: [-812.821 -812.821 -812.821] [192.9057], Avg: [-801.04 -801.04 -801.04] (1.000)
Step: 33949, Reward: [-662.571 -662.571 -662.571] [120.9819], Avg: [-801.014 -801.014 -801.014] (1.000)
Step: 33999, Reward: [-1080.758 -1080.758 -1080.758] [295.6580], Avg: [-801.861 -801.861 -801.861] (1.000)
Step: 34049, Reward: [-745.846 -745.846 -745.846] [324.1398], Avg: [-802.254 -802.254 -802.254] (1.000)
Step: 34099, Reward: [-968.21 -968.21 -968.21] [373.3137], Avg: [-803.045 -803.045 -803.045] (1.000)
Step: 34149, Reward: [-947.091 -947.091 -947.091] [260.7373], Avg: [-803.638 -803.638 -803.638] (1.000)
Step: 34199, Reward: [-856.939 -856.939 -856.939] [244.0190], Avg: [-804.072 -804.072 -804.072] (1.000)
Step: 34249, Reward: [-738.817 -738.817 -738.817] [194.9936], Avg: [-804.262 -804.262 -804.262] (1.000)
Step: 34299, Reward: [-840.348 -840.348 -840.348] [331.3691], Avg: [-804.797 -804.797 -804.797] (1.000)
Step: 34349, Reward: [-622.235 -622.235 -622.235] [153.7020], Avg: [-804.755 -804.755 -804.755] (1.000)
Step: 34399, Reward: [-436.926 -436.926 -436.926] [59.0389], Avg: [-804.307 -804.307 -804.307] (1.000)
Step: 34449, Reward: [-454.77 -454.77 -454.77] [108.1006], Avg: [-803.956 -803.956 -803.956] (1.000)
Step: 34499, Reward: [-468.158 -468.158 -468.158] [125.8620], Avg: [-803.652 -803.652 -803.652] (1.000)
Step: 34549, Reward: [-523.511 -523.511 -523.511] [79.2037], Avg: [-803.361 -803.361 -803.361] (1.000)
Step: 34599, Reward: [-610.357 -610.357 -610.357] [102.2804], Avg: [-803.23 -803.23 -803.23] (1.000)
Step: 34649, Reward: [-602.959 -602.959 -602.959] [144.9239], Avg: [-803.15 -803.15 -803.15] (1.000)
Step: 34699, Reward: [-539.377 -539.377 -539.377] [217.0442], Avg: [-803.083 -803.083 -803.083] (1.000)
Step: 34749, Reward: [-514.649 -514.649 -514.649] [167.5373], Avg: [-802.909 -802.909 -802.909] (1.000)
Step: 34799, Reward: [-548.745 -548.745 -548.745] [119.7612], Avg: [-802.716 -802.716 -802.716] (1.000)
Step: 34849, Reward: [-981.156 -981.156 -981.156] [217.2255], Avg: [-803.283 -803.283 -803.283] (1.000)
Step: 34899, Reward: [-610.293 -610.293 -610.293] [161.4911], Avg: [-803.238 -803.238 -803.238] (1.000)
Step: 34949, Reward: [-597.784 -597.784 -597.784] [237.6236], Avg: [-803.284 -803.284 -803.284] (1.000)
Step: 34999, Reward: [-588.319 -588.319 -588.319] [93.2120], Avg: [-803.11 -803.11 -803.11] (1.000)
Step: 35049, Reward: [-584.952 -584.952 -584.952] [270.6510], Avg: [-803.185 -803.185 -803.185] (1.000)
Step: 35099, Reward: [-550.074 -550.074 -550.074] [219.7606], Avg: [-803.138 -803.138 -803.138] (1.000)
Step: 35149, Reward: [-574.938 -574.938 -574.938] [172.9453], Avg: [-803.059 -803.059 -803.059] (1.000)
Step: 35199, Reward: [-630.917 -630.917 -630.917] [87.7071], Avg: [-802.939 -802.939 -802.939] (1.000)
Step: 35249, Reward: [-595.777 -595.777 -595.777] [75.8769], Avg: [-802.753 -802.753 -802.753] (1.000)
Step: 35299, Reward: [-922.529 -922.529 -922.529] [480.0521], Avg: [-803.603 -803.603 -803.603] (1.000)
Step: 35349, Reward: [-614.717 -614.717 -614.717] [177.6697], Avg: [-803.587 -803.587 -803.587] (1.000)
Step: 35399, Reward: [-965.172 -965.172 -965.172] [460.1668], Avg: [-804.465 -804.465 -804.465] (1.000)
Step: 35449, Reward: [-587.327 -587.327 -587.327] [199.7830], Avg: [-804.44 -804.44 -804.44] (1.000)
Step: 35499, Reward: [-769.385 -769.385 -769.385] [420.1691], Avg: [-804.983 -804.983 -804.983] (1.000)
Step: 35549, Reward: [-570.054 -570.054 -570.054] [86.5360], Avg: [-804.774 -804.774 -804.774] (1.000)
Step: 35599, Reward: [-988.131 -988.131 -988.131] [618.1258], Avg: [-805.9 -805.9 -805.9] (1.000)
Step: 35649, Reward: [-634.807 -634.807 -634.807] [186.1058], Avg: [-805.921 -805.921 -805.921] (1.000)
Step: 35699, Reward: [-559.422 -559.422 -559.422] [135.2361], Avg: [-805.765 -805.765 -805.765] (1.000)
Step: 35749, Reward: [-628.846 -628.846 -628.846] [120.6097], Avg: [-805.686 -805.686 -805.686] (1.000)
Step: 35799, Reward: [-577.148 -577.148 -577.148] [120.0314], Avg: [-805.535 -805.535 -805.535] (1.000)
Step: 35849, Reward: [-616.787 -616.787 -616.787] [167.3190], Avg: [-805.505 -805.505 -805.505] (1.000)
Step: 35899, Reward: [-525.252 -525.252 -525.252] [51.5560], Avg: [-805.186 -805.186 -805.186] (1.000)
Step: 35949, Reward: [-572.339 -572.339 -572.339] [98.7636], Avg: [-805. -805. -805.] (1.000)
Step: 35999, Reward: [-484.587 -484.587 -484.587] [84.8070], Avg: [-804.673 -804.673 -804.673] (1.000)
Step: 36049, Reward: [-471.676 -471.676 -471.676] [67.2134], Avg: [-804.304 -804.304 -804.304] (1.000)
Step: 36099, Reward: [-448.613 -448.613 -448.613] [42.3197], Avg: [-803.87 -803.87 -803.87] (1.000)
Step: 36149, Reward: [-456.408 -456.408 -456.408] [38.6738], Avg: [-803.443 -803.443 -803.443] (1.000)
Step: 36199, Reward: [-404.66 -404.66 -404.66] [65.9852], Avg: [-802.983 -802.983 -802.983] (1.000)
Step: 36249, Reward: [-395.056 -395.056 -395.056] [20.9315], Avg: [-802.449 -802.449 -802.449] (1.000)
Step: 36299, Reward: [-409.721 -409.721 -409.721] [25.7860], Avg: [-801.944 -801.944 -801.944] (1.000)
Step: 36349, Reward: [-398.101 -398.101 -398.101] [22.3649], Avg: [-801.419 -801.419 -801.419] (1.000)
Step: 36399, Reward: [-447.677 -447.677 -447.677] [62.1946], Avg: [-801.019 -801.019 -801.019] (1.000)
Step: 36449, Reward: [-440.607 -440.607 -440.607] [43.5404], Avg: [-800.584 -800.584 -800.584] (1.000)
Step: 36499, Reward: [-415.925 -415.925 -415.925] [46.4671], Avg: [-800.121 -800.121 -800.121] (1.000)
Step: 36549, Reward: [-485.322 -485.322 -485.322] [77.2601], Avg: [-799.796 -799.796 -799.796] (1.000)
Step: 36599, Reward: [-409.774 -409.774 -409.774] [58.4251], Avg: [-799.343 -799.343 -799.343] (1.000)
Step: 36649, Reward: [-389.514 -389.514 -389.514] [51.2833], Avg: [-798.854 -798.854 -798.854] (1.000)
Step: 36699, Reward: [-469.541 -469.541 -469.541] [43.7548], Avg: [-798.465 -798.465 -798.465] (1.000)
Step: 36749, Reward: [-400.047 -400.047 -400.047] [57.6236], Avg: [-798.001 -798.001 -798.001] (1.000)
Step: 36799, Reward: [-409.179 -409.179 -409.179] [57.3437], Avg: [-797.551 -797.551 -797.551] (1.000)
Step: 36849, Reward: [-464.779 -464.779 -464.779] [61.1457], Avg: [-797.182 -797.182 -797.182] (1.000)
Step: 36899, Reward: [-435.449 -435.449 -435.449] [40.7757], Avg: [-796.747 -796.747 -796.747] (1.000)
Step: 36949, Reward: [-429.133 -429.133 -429.133] [46.5147], Avg: [-796.313 -796.313 -796.313] (1.000)
Step: 36999, Reward: [-505.734 -505.734 -505.734] [95.8361], Avg: [-796.05 -796.05 -796.05] (1.000)
Step: 37049, Reward: [-477.966 -477.966 -477.966] [98.0328], Avg: [-795.753 -795.753 -795.753] (1.000)
Step: 37099, Reward: [-507.845 -507.845 -507.845] [90.2855], Avg: [-795.486 -795.486 -795.486] (1.000)
Step: 37149, Reward: [-569.157 -569.157 -569.157] [75.0251], Avg: [-795.283 -795.283 -795.283] (1.000)
Step: 37199, Reward: [-461.659 -461.659 -461.659] [60.5975], Avg: [-794.916 -794.916 -794.916] (1.000)
Step: 37249, Reward: [-483.604 -483.604 -483.604] [52.2157], Avg: [-794.568 -794.568 -794.568] (1.000)
Step: 37299, Reward: [-425.812 -425.812 -425.812] [73.7823], Avg: [-794.172 -794.172 -794.172] (1.000)
Step: 37349, Reward: [-483.914 -483.914 -483.914] [86.4098], Avg: [-793.873 -793.873 -793.873] (1.000)
Step: 37399, Reward: [-469.071 -469.071 -469.071] [45.1594], Avg: [-793.499 -793.499 -793.499] (1.000)
Step: 37449, Reward: [-415.96 -415.96 -415.96] [41.7110], Avg: [-793.051 -793.051 -793.051] (1.000)
Step: 37499, Reward: [-440.064 -440.064 -440.064] [26.6663], Avg: [-792.615 -792.615 -792.615] (1.000)
Step: 37549, Reward: [-486.533 -486.533 -486.533] [80.0313], Avg: [-792.314 -792.314 -792.314] (1.000)
Step: 37599, Reward: [-428.107 -428.107 -428.107] [67.6153], Avg: [-791.92 -791.92 -791.92] (1.000)
Step: 37649, Reward: [-457.923 -457.923 -457.923] [78.2521], Avg: [-791.58 -791.58 -791.58] (1.000)
Step: 37699, Reward: [-505.403 -505.403 -505.403] [65.0021], Avg: [-791.287 -791.287 -791.287] (1.000)
Step: 37749, Reward: [-440.439 -440.439 -440.439] [80.4199], Avg: [-790.929 -790.929 -790.929] (1.000)
Step: 37799, Reward: [-476.58 -476.58 -476.58] [56.1211], Avg: [-790.587 -790.587 -790.587] (1.000)
Step: 37849, Reward: [-476.965 -476.965 -476.965] [61.6618], Avg: [-790.254 -790.254 -790.254] (1.000)
Step: 37899, Reward: [-436.986 -436.986 -436.986] [64.6057], Avg: [-789.874 -789.874 -789.874] (1.000)
Step: 37949, Reward: [-489.267 -489.267 -489.267] [100.8791], Avg: [-789.61 -789.61 -789.61] (1.000)
Step: 37999, Reward: [-432.741 -432.741 -432.741] [35.4624], Avg: [-789.188 -789.188 -789.188] (1.000)
Step: 38049, Reward: [-442.424 -442.424 -442.424] [63.4923], Avg: [-788.815 -788.815 -788.815] (1.000)
Step: 38099, Reward: [-483.512 -483.512 -483.512] [79.7484], Avg: [-788.519 -788.519 -788.519] (1.000)
Step: 38149, Reward: [-397.573 -397.573 -397.573] [74.3315], Avg: [-788.104 -788.104 -788.104] (1.000)
Step: 38199, Reward: [-396.778 -396.778 -396.778] [41.0989], Avg: [-787.646 -787.646 -787.646] (1.000)
Step: 38249, Reward: [-424.915 -424.915 -424.915] [65.6174], Avg: [-787.258 -787.258 -787.258] (1.000)
Step: 38299, Reward: [-461.715 -461.715 -461.715] [84.8836], Avg: [-786.943 -786.943 -786.943] (1.000)
Step: 38349, Reward: [-410.877 -410.877 -410.877] [52.0255], Avg: [-786.521 -786.521 -786.521] (1.000)
Step: 38399, Reward: [-463.263 -463.263 -463.263] [18.8223], Avg: [-786.125 -786.125 -786.125] (1.000)
Step: 38449, Reward: [-464.711 -464.711 -464.711] [72.9205], Avg: [-785.801 -785.801 -785.801] (1.000)
Step: 38499, Reward: [-433.651 -433.651 -433.651] [39.0049], Avg: [-785.395 -785.395 -785.395] (1.000)
Step: 38549, Reward: [-464.456 -464.456 -464.456] [34.2251], Avg: [-785.023 -785.023 -785.023] (1.000)
Step: 38599, Reward: [-445.629 -445.629 -445.629] [43.4329], Avg: [-784.639 -784.639 -784.639] (1.000)
Step: 38649, Reward: [-518.462 -518.462 -518.462] [70.6346], Avg: [-784.387 -784.387 -784.387] (1.000)
Step: 38699, Reward: [-460.461 -460.461 -460.461] [83.6939], Avg: [-784.076 -784.076 -784.076] (1.000)
Step: 38749, Reward: [-504.199 -504.199 -504.199] [99.7893], Avg: [-783.844 -783.844 -783.844] (1.000)
Step: 38799, Reward: [-531.413 -531.413 -531.413] [58.8532], Avg: [-783.594 -783.594 -783.594] (1.000)
Step: 38849, Reward: [-575.956 -575.956 -575.956] [43.3853], Avg: [-783.383 -783.383 -783.383] (1.000)
Step: 38899, Reward: [-611.855 -611.855 -611.855] [89.2051], Avg: [-783.277 -783.277 -783.277] (1.000)
Step: 38949, Reward: [-603.475 -603.475 -603.475] [110.4175], Avg: [-783.188 -783.188 -783.188] (1.000)
Step: 38999, Reward: [-725.733 -725.733 -725.733] [56.7950], Avg: [-783.187 -783.187 -783.187] (1.000)
Step: 39049, Reward: [-681.195 -681.195 -681.195] [84.7523], Avg: [-783.165 -783.165 -783.165] (1.000)
Step: 39099, Reward: [-635.437 -635.437 -635.437] [63.5458], Avg: [-783.057 -783.057 -783.057] (1.000)
Step: 39149, Reward: [-741.539 -741.539 -741.539] [75.6291], Avg: [-783.101 -783.101 -783.101] (1.000)
Step: 39199, Reward: [-904.269 -904.269 -904.269] [106.4443], Avg: [-783.391 -783.391 -783.391] (1.000)
Step: 39249, Reward: [-663.44 -663.44 -663.44] [85.5192], Avg: [-783.347 -783.347 -783.347] (1.000)
Step: 39299, Reward: [-644.432 -644.432 -644.432] [145.8988], Avg: [-783.356 -783.356 -783.356] (1.000)
Step: 39349, Reward: [-718.76 -718.76 -718.76] [84.0007], Avg: [-783.381 -783.381 -783.381] (1.000)
Step: 39399, Reward: [-790.895 -790.895 -790.895] [50.6062], Avg: [-783.455 -783.455 -783.455] (1.000)
Step: 39449, Reward: [-796.672 -796.672 -796.672] [75.0576], Avg: [-783.567 -783.567 -783.567] (1.000)
Step: 39499, Reward: [-812.847 -812.847 -812.847] [160.3047], Avg: [-783.807 -783.807 -783.807] (1.000)
Step: 39549, Reward: [-906.985 -906.985 -906.985] [192.3876], Avg: [-784.206 -784.206 -784.206] (1.000)
Step: 39599, Reward: [-736.336 -736.336 -736.336] [102.3956], Avg: [-784.274 -784.274 -784.274] (1.000)
Step: 39649, Reward: [-1121.591 -1121.591 -1121.591] [258.7471], Avg: [-785.026 -785.026 -785.026] (1.000)
Step: 39699, Reward: [-714.795 -714.795 -714.795] [134.8516], Avg: [-785.107 -785.107 -785.107] (1.000)
Step: 39749, Reward: [-723.907 -723.907 -723.907] [95.7879], Avg: [-785.151 -785.151 -785.151] (1.000)
Step: 39799, Reward: [-745.307 -745.307 -745.307] [77.4198], Avg: [-785.198 -785.198 -785.198] (1.000)
Step: 39849, Reward: [-761.019 -761.019 -761.019] [170.8789], Avg: [-785.382 -785.382 -785.382] (1.000)
Step: 39899, Reward: [-749.315 -749.315 -749.315] [163.9768], Avg: [-785.543 -785.543 -785.543] (1.000)
Step: 39949, Reward: [-627.574 -627.574 -627.574] [79.3429], Avg: [-785.444 -785.444 -785.444] (1.000)
Step: 39999, Reward: [-538.029 -538.029 -538.029] [102.3934], Avg: [-785.263 -785.263 -785.263] (1.000)
Step: 40049, Reward: [-556.671 -556.671 -556.671] [87.6866], Avg: [-785.087 -785.087 -785.087] (1.000)
Step: 40099, Reward: [-549.129 -549.129 -549.129] [23.9478], Avg: [-784.823 -784.823 -784.823] (1.000)
Step: 40149, Reward: [-563.412 -563.412 -563.412] [81.5956], Avg: [-784.648 -784.648 -784.648] (1.000)
Step: 40199, Reward: [-584.002 -584.002 -584.002] [79.8075], Avg: [-784.498 -784.498 -784.498] (1.000)
Step: 40249, Reward: [-468.66 -468.66 -468.66] [67.0000], Avg: [-784.189 -784.189 -784.189] (1.000)
Step: 40299, Reward: [-550.133 -550.133 -550.133] [99.1299], Avg: [-784.022 -784.022 -784.022] (1.000)
Step: 40349, Reward: [-456.152 -456.152 -456.152] [57.0413], Avg: [-783.686 -783.686 -783.686] (1.000)
Step: 40399, Reward: [-451.078 -451.078 -451.078] [54.8833], Avg: [-783.342 -783.342 -783.342] (1.000)
Step: 40449, Reward: [-427.49 -427.49 -427.49] [25.2170], Avg: [-782.934 -782.934 -782.934] (1.000)
Step: 40499, Reward: [-469.943 -469.943 -469.943] [72.6433], Avg: [-782.637 -782.637 -782.637] (1.000)
Step: 40549, Reward: [-461.415 -461.415 -461.415] [61.2482], Avg: [-782.316 -782.316 -782.316] (1.000)
Step: 40599, Reward: [-436.662 -436.662 -436.662] [86.7056], Avg: [-781.997 -781.997 -781.997] (1.000)
Step: 40649, Reward: [-473.898 -473.898 -473.898] [94.8497], Avg: [-781.735 -781.735 -781.735] (1.000)
Step: 40699, Reward: [-503.883 -503.883 -503.883] [118.4638], Avg: [-781.539 -781.539 -781.539] (1.000)
Step: 40749, Reward: [-484.681 -484.681 -484.681] [96.1064], Avg: [-781.293 -781.293 -781.293] (1.000)
Step: 40799, Reward: [-415.631 -415.631 -415.631] [103.5919], Avg: [-780.972 -780.972 -780.972] (1.000)
Step: 40849, Reward: [-467.764 -467.764 -467.764] [72.1396], Avg: [-780.677 -780.677 -780.677] (1.000)
Step: 40899, Reward: [-502.045 -502.045 -502.045] [40.4514], Avg: [-780.386 -780.386 -780.386] (1.000)
Step: 40949, Reward: [-473.376 -473.376 -473.376] [144.6663], Avg: [-780.187 -780.187 -780.187] (1.000)
Step: 40999, Reward: [-586.439 -586.439 -586.439] [164.2599], Avg: [-780.151 -780.151 -780.151] (1.000)
Step: 41049, Reward: [-545.22 -545.22 -545.22] [32.2738], Avg: [-779.905 -779.905 -779.905] (1.000)
Step: 41099, Reward: [-495.504 -495.504 -495.504] [52.4034], Avg: [-779.622 -779.622 -779.622] (1.000)
Step: 41149, Reward: [-436.229 -436.229 -436.229] [52.7360], Avg: [-779.269 -779.269 -779.269] (1.000)
Step: 41199, Reward: [-478.353 -478.353 -478.353] [77.4721], Avg: [-778.998 -778.998 -778.998] (1.000)
Step: 41249, Reward: [-487.415 -487.415 -487.415] [99.6056], Avg: [-778.765 -778.765 -778.765] (1.000)
Step: 41299, Reward: [-482.223 -482.223 -482.223] [55.8712], Avg: [-778.474 -778.474 -778.474] (1.000)
Step: 41349, Reward: [-656.79 -656.79 -656.79] [157.9452], Avg: [-778.518 -778.518 -778.518] (1.000)
Step: 41399, Reward: [-562.635 -562.635 -562.635] [140.6538], Avg: [-778.427 -778.427 -778.427] (1.000)
Step: 41449, Reward: [-564.522 -564.522 -564.522] [104.4697], Avg: [-778.295 -778.295 -778.295] (1.000)
Step: 41499, Reward: [-593.312 -593.312 -593.312] [110.4929], Avg: [-778.205 -778.205 -778.205] (1.000)
Step: 41549, Reward: [-657.505 -657.505 -657.505] [101.4684], Avg: [-778.182 -778.182 -778.182] (1.000)
Step: 41599, Reward: [-639.518 -639.518 -639.518] [149.9176], Avg: [-778.196 -778.196 -778.196] (1.000)
Step: 41649, Reward: [-654.632 -654.632 -654.632] [167.5753], Avg: [-778.248 -778.248 -778.248] (1.000)
Step: 41699, Reward: [-680.913 -680.913 -680.913] [208.3901], Avg: [-778.382 -778.382 -778.382] (1.000)
Step: 41749, Reward: [-686.942 -686.942 -686.942] [113.5011], Avg: [-778.408 -778.408 -778.408] (1.000)
Step: 41799, Reward: [-697.294 -697.294 -697.294] [126.8006], Avg: [-778.463 -778.463 -778.463] (1.000)
Step: 41849, Reward: [-664.423 -664.423 -664.423] [125.8218], Avg: [-778.477 -778.477 -778.477] (1.000)
Step: 41899, Reward: [-628.24 -628.24 -628.24] [68.6111], Avg: [-778.379 -778.379 -778.379] (1.000)
Step: 41949, Reward: [-653.066 -653.066 -653.066] [173.7250], Avg: [-778.437 -778.437 -778.437] (1.000)
Step: 41999, Reward: [-713.078 -713.078 -713.078] [70.8888], Avg: [-778.444 -778.444 -778.444] (1.000)
Step: 42049, Reward: [-786.625 -786.625 -786.625] [157.9039], Avg: [-778.641 -778.641 -778.641] (1.000)
Step: 42099, Reward: [-889.289 -889.289 -889.289] [139.7742], Avg: [-778.938 -778.938 -778.938] (1.000)
Step: 42149, Reward: [-652.84 -652.84 -652.84] [152.3593], Avg: [-778.97 -778.97 -778.97] (1.000)
Step: 42199, Reward: [-619.35 -619.35 -619.35] [211.9622], Avg: [-779.032 -779.032 -779.032] (1.000)
Step: 42249, Reward: [-505.463 -505.463 -505.463] [87.3051], Avg: [-778.811 -778.811 -778.811] (1.000)
Step: 42299, Reward: [-597.576 -597.576 -597.576] [207.7984], Avg: [-778.843 -778.843 -778.843] (1.000)
Step: 42349, Reward: [-515.057 -515.057 -515.057] [105.7873], Avg: [-778.656 -778.656 -778.656] (1.000)
Step: 42399, Reward: [-531.985 -531.985 -531.985] [82.4589], Avg: [-778.462 -778.462 -778.462] (1.000)
Step: 42449, Reward: [-521.452 -521.452 -521.452] [82.2999], Avg: [-778.257 -778.257 -778.257] (1.000)
Step: 42499, Reward: [-559.585 -559.585 -559.585] [53.3961], Avg: [-778.062 -778.062 -778.062] (1.000)
Step: 42549, Reward: [-482.103 -482.103 -482.103] [31.4342], Avg: [-777.751 -777.751 -777.751] (1.000)
Step: 42599, Reward: [-503.026 -503.026 -503.026] [113.6323], Avg: [-777.562 -777.562 -777.562] (1.000)
Step: 42649, Reward: [-471.562 -471.562 -471.562] [47.1369], Avg: [-777.259 -777.259 -777.259] (1.000)
Step: 42699, Reward: [-495.276 -495.276 -495.276] [39.0226], Avg: [-776.974 -776.974 -776.974] (1.000)
Step: 42749, Reward: [-581.628 -581.628 -581.628] [168.4167], Avg: [-776.943 -776.943 -776.943] (1.000)
Step: 42799, Reward: [-557.836 -557.836 -557.836] [75.6531], Avg: [-776.775 -776.775 -776.775] (1.000)
Step: 42849, Reward: [-596.242 -596.242 -596.242] [218.0959], Avg: [-776.819 -776.819 -776.819] (1.000)
Step: 42899, Reward: [-613.407 -613.407 -613.407] [212.4442], Avg: [-776.876 -776.876 -776.876] (1.000)
Step: 42949, Reward: [-426.18 -426.18 -426.18] [88.1615], Avg: [-776.571 -776.571 -776.571] (1.000)
Step: 42999, Reward: [-536.194 -536.194 -536.194] [99.9205], Avg: [-776.407 -776.407 -776.407] (1.000)
Step: 43049, Reward: [-462.976 -462.976 -462.976] [55.3921], Avg: [-776.108 -776.108 -776.108] (1.000)
Step: 43099, Reward: [-510.176 -510.176 -510.176] [59.0140], Avg: [-775.868 -775.868 -775.868] (1.000)
Step: 43149, Reward: [-490.566 -490.566 -490.566] [93.3181], Avg: [-775.645 -775.645 -775.645] (1.000)
Step: 43199, Reward: [-474.284 -474.284 -474.284] [82.1902], Avg: [-775.391 -775.391 -775.391] (1.000)
Step: 43249, Reward: [-453.366 -453.366 -453.366] [69.9907], Avg: [-775.1 -775.1 -775.1] (1.000)
Step: 43299, Reward: [-434.289 -434.289 -434.289] [72.6705], Avg: [-774.79 -774.79 -774.79] (1.000)
Step: 43349, Reward: [-440.812 -440.812 -440.812] [109.7810], Avg: [-774.532 -774.532 -774.532] (1.000)
Step: 43399, Reward: [-449.839 -449.839 -449.839] [64.8331], Avg: [-774.232 -774.232 -774.232] (1.000)
Step: 43449, Reward: [-467.818 -467.818 -467.818] [97.3690], Avg: [-773.992 -773.992 -773.992] (1.000)
Step: 43499, Reward: [-490.575 -490.575 -490.575] [58.0316], Avg: [-773.733 -773.733 -773.733] (1.000)
Step: 43549, Reward: [-495.886 -495.886 -495.886] [47.1449], Avg: [-773.468 -773.468 -773.468] (1.000)
Step: 43599, Reward: [-524.778 -524.778 -524.778] [91.4717], Avg: [-773.288 -773.288 -773.288] (1.000)
Step: 43649, Reward: [-466.264 -466.264 -466.264] [56.8195], Avg: [-773.001 -773.001 -773.001] (1.000)
Step: 43699, Reward: [-488.672 -488.672 -488.672] [74.8136], Avg: [-772.761 -772.761 -772.761] (1.000)
Step: 43749, Reward: [-466.41 -466.41 -466.41] [61.7016], Avg: [-772.482 -772.482 -772.482] (1.000)
Step: 43799, Reward: [-639.295 -639.295 -639.295] [159.2885], Avg: [-772.512 -772.512 -772.512] (1.000)
Step: 43849, Reward: [-629.982 -629.982 -629.982] [148.7759], Avg: [-772.519 -772.519 -772.519] (1.000)
Step: 43899, Reward: [-755.004 -755.004 -755.004] [184.9044], Avg: [-772.709 -772.709 -772.709] (1.000)
Step: 43949, Reward: [-809.809 -809.809 -809.809] [285.5469], Avg: [-773.076 -773.076 -773.076] (1.000)
Step: 43999, Reward: [-944.024 -944.024 -944.024] [91.9269], Avg: [-773.375 -773.375 -773.375] (1.000)
Step: 44049, Reward: [-1033.016 -1033.016 -1033.016] [69.0414], Avg: [-773.748 -773.748 -773.748] (1.000)
Step: 44099, Reward: [-886.217 -886.217 -886.217] [145.3450], Avg: [-774.04 -774.04 -774.04] (1.000)
Step: 44149, Reward: [-1074.473 -1074.473 -1074.473] [148.8638], Avg: [-774.549 -774.549 -774.549] (1.000)
Step: 44199, Reward: [-1164.776 -1164.776 -1164.776] [222.1920], Avg: [-775.242 -775.242 -775.242] (1.000)
Step: 44249, Reward: [-1290.519 -1290.519 -1290.519] [204.2445], Avg: [-776.055 -776.055 -776.055] (1.000)
Step: 44299, Reward: [-1186.576 -1186.576 -1186.576] [116.3636], Avg: [-776.65 -776.65 -776.65] (1.000)
Step: 44349, Reward: [-1133.361 -1133.361 -1133.361] [85.7730], Avg: [-777.149 -777.149 -777.149] (1.000)
Step: 44399, Reward: [-1196.945 -1196.945 -1196.945] [291.0677], Avg: [-777.949 -777.949 -777.949] (1.000)
Step: 44449, Reward: [-1107.459 -1107.459 -1107.459] [100.5424], Avg: [-778.433 -778.433 -778.433] (1.000)
Step: 44499, Reward: [-1233.727 -1233.727 -1233.727] [119.7835], Avg: [-779.079 -779.079 -779.079] (1.000)
Step: 44549, Reward: [-738.814 -738.814 -738.814] [193.2228], Avg: [-779.251 -779.251 -779.251] (1.000)
Step: 44599, Reward: [-909.268 -909.268 -909.268] [253.6264], Avg: [-779.681 -779.681 -779.681] (1.000)
Step: 44649, Reward: [-890.794 -890.794 -890.794] [189.3353], Avg: [-780.017 -780.017 -780.017] (1.000)
Step: 44699, Reward: [-615.744 -615.744 -615.744] [132.8912], Avg: [-779.982 -779.982 -779.982] (1.000)
Step: 44749, Reward: [-514.694 -514.694 -514.694] [116.3911], Avg: [-779.816 -779.816 -779.816] (1.000)
Step: 44799, Reward: [-547.515 -547.515 -547.515] [92.4229], Avg: [-779.66 -779.66 -779.66] (1.000)
Step: 44849, Reward: [-511.993 -511.993 -511.993] [89.2291], Avg: [-779.461 -779.461 -779.461] (1.000)
Step: 44899, Reward: [-468.066 -468.066 -468.066] [88.6825], Avg: [-779.213 -779.213 -779.213] (1.000)
Step: 44949, Reward: [-411.387 -411.387 -411.387] [60.0177], Avg: [-778.87 -778.87 -778.87] (1.000)
Step: 44999, Reward: [-463.037 -463.037 -463.037] [115.2999], Avg: [-778.648 -778.648 -778.648] (1.000)
Step: 45049, Reward: [-421.823 -421.823 -421.823] [50.2849], Avg: [-778.307 -778.307 -778.307] (1.000)
Step: 45099, Reward: [-427.478 -427.478 -427.478] [17.6602], Avg: [-777.938 -777.938 -777.938] (1.000)
Step: 45149, Reward: [-406.156 -406.156 -406.156] [54.0872], Avg: [-777.586 -777.586 -777.586] (1.000)
Step: 45199, Reward: [-470.7 -470.7 -470.7] [91.3966], Avg: [-777.348 -777.348 -777.348] (1.000)
Step: 45249, Reward: [-408.648 -408.648 -408.648] [26.9377], Avg: [-776.97 -776.97 -776.97] (1.000)
Step: 45299, Reward: [-468.153 -468.153 -468.153] [50.6835], Avg: [-776.685 -776.685 -776.685] (1.000)
Step: 45349, Reward: [-450.73 -450.73 -450.73] [85.3207], Avg: [-776.42 -776.42 -776.42] (1.000)
Step: 45399, Reward: [-444.547 -444.547 -444.547] [117.1769], Avg: [-776.183 -776.183 -776.183] (1.000)
Step: 45449, Reward: [-500.717 -500.717 -500.717] [81.1959], Avg: [-775.97 -775.97 -775.97] (1.000)
Step: 45499, Reward: [-463.615 -463.615 -463.615] [24.0764], Avg: [-775.653 -775.653 -775.653] (1.000)
Step: 45549, Reward: [-560.252 -560.252 -560.252] [235.4381], Avg: [-775.675 -775.675 -775.675] (1.000)
Step: 45599, Reward: [-588.84 -588.84 -588.84] [225.4802], Avg: [-775.717 -775.717 -775.717] (1.000)
Step: 45649, Reward: [-604.738 -604.738 -604.738] [255.5550], Avg: [-775.81 -775.81 -775.81] (1.000)
Step: 45699, Reward: [-446.94 -446.94 -446.94] [54.0424], Avg: [-775.509 -775.509 -775.509] (1.000)
Step: 45749, Reward: [-541.327 -541.327 -541.327] [106.0882], Avg: [-775.369 -775.369 -775.369] (1.000)
Step: 45799, Reward: [-579.007 -579.007 -579.007] [131.9542], Avg: [-775.299 -775.299 -775.299] (1.000)
Step: 45849, Reward: [-585.928 -585.928 -585.928] [69.7289], Avg: [-775.168 -775.168 -775.168] (1.000)
Step: 45899, Reward: [-522.51 -522.51 -522.51] [118.1934], Avg: [-775.022 -775.022 -775.022] (1.000)
Step: 45949, Reward: [-603.017 -603.017 -603.017] [156.1356], Avg: [-775.005 -775.005 -775.005] (1.000)
Step: 45999, Reward: [-427.618 -427.618 -427.618] [113.9692], Avg: [-774.751 -774.751 -774.751] (1.000)
Step: 46049, Reward: [-492.154 -492.154 -492.154] [122.2935], Avg: [-774.577 -774.577 -774.577] (1.000)
Step: 46099, Reward: [-444.339 -444.339 -444.339] [71.4932], Avg: [-774.296 -774.296 -774.296] (1.000)
Step: 46149, Reward: [-554.286 -554.286 -554.286] [128.6817], Avg: [-774.197 -774.197 -774.197] (1.000)
Step: 46199, Reward: [-447.028 -447.028 -447.028] [65.4831], Avg: [-773.914 -773.914 -773.914] (1.000)
Step: 46249, Reward: [-511.032 -511.032 -511.032] [114.1975], Avg: [-773.753 -773.753 -773.753] (1.000)
Step: 46299, Reward: [-439.77 -439.77 -439.77] [51.0712], Avg: [-773.448 -773.448 -773.448] (1.000)
Step: 46349, Reward: [-503.216 -503.216 -503.216] [79.6851], Avg: [-773.242 -773.242 -773.242] (1.000)
Step: 46399, Reward: [-497.555 -497.555 -497.555] [119.5399], Avg: [-773.074 -773.074 -773.074] (1.000)
Step: 46449, Reward: [-702.392 -702.392 -702.392] [85.1767], Avg: [-773.09 -773.09 -773.09] (1.000)
Step: 46499, Reward: [-764.802 -764.802 -764.802] [126.4744], Avg: [-773.217 -773.217 -773.217] (1.000)
Step: 46549, Reward: [-993.346 -993.346 -993.346] [106.7879], Avg: [-773.568 -773.568 -773.568] (1.000)
Step: 46599, Reward: [-1039.935 -1039.935 -1039.935] [195.3741], Avg: [-774.063 -774.063 -774.063] (1.000)
Step: 46649, Reward: [-1131.121 -1131.121 -1131.121] [165.6986], Avg: [-774.624 -774.624 -774.624] (1.000)
Step: 46699, Reward: [-1146.266 -1146.266 -1146.266] [176.1340], Avg: [-775.21 -775.21 -775.21] (1.000)
Step: 46749, Reward: [-1108.644 -1108.644 -1108.644] [130.3623], Avg: [-775.706 -775.706 -775.706] (1.000)
Step: 46799, Reward: [-1023.711 -1023.711 -1023.711] [74.7214], Avg: [-776.051 -776.051 -776.051] (1.000)
Step: 46849, Reward: [-984.417 -984.417 -984.417] [127.7184], Avg: [-776.41 -776.41 -776.41] (1.000)
Step: 46899, Reward: [-948.431 -948.431 -948.431] [111.2634], Avg: [-776.712 -776.712 -776.712] (1.000)
Step: 46949, Reward: [-818.465 -818.465 -818.465] [145.5193], Avg: [-776.911 -776.911 -776.911] (1.000)
Step: 46999, Reward: [-633.854 -633.854 -633.854] [67.3491], Avg: [-776.831 -776.831 -776.831] (1.000)
Step: 47049, Reward: [-587.129 -587.129 -587.129] [88.8665], Avg: [-776.723 -776.723 -776.723] (1.000)
Step: 47099, Reward: [-553.419 -553.419 -553.419] [39.2140], Avg: [-776.528 -776.528 -776.528] (1.000)
Step: 47149, Reward: [-679.613 -679.613 -679.613] [54.0515], Avg: [-776.483 -776.483 -776.483] (1.000)
Step: 47199, Reward: [-868.312 -868.312 -868.312] [74.3169], Avg: [-776.659 -776.659 -776.659] (1.000)
Step: 47249, Reward: [-705.219 -705.219 -705.219] [52.7160], Avg: [-776.639 -776.639 -776.639] (1.000)
Step: 47299, Reward: [-627.467 -627.467 -627.467] [47.3700], Avg: [-776.531 -776.531 -776.531] (1.000)
Step: 47349, Reward: [-546.602 -546.602 -546.602] [48.9069], Avg: [-776.34 -776.34 -776.34] (1.000)
Step: 47399, Reward: [-466.089 -466.089 -466.089] [43.8927], Avg: [-776.059 -776.059 -776.059] (1.000)
Step: 47449, Reward: [-482.651 -482.651 -482.651] [18.4530], Avg: [-775.769 -775.769 -775.769] (1.000)
Step: 47499, Reward: [-553.548 -553.548 -553.548] [69.6578], Avg: [-775.609 -775.609 -775.609] (1.000)
Step: 47549, Reward: [-637.84 -637.84 -637.84] [84.7502], Avg: [-775.553 -775.553 -775.553] (1.000)
Step: 47599, Reward: [-589.259 -589.259 -589.259] [46.2857], Avg: [-775.406 -775.406 -775.406] (1.000)
Step: 47649, Reward: [-487.999 -487.999 -487.999] [52.4940], Avg: [-775.159 -775.159 -775.159] (1.000)
Step: 47699, Reward: [-494.6 -494.6 -494.6] [87.0642], Avg: [-774.957 -774.957 -774.957] (1.000)
Step: 47749, Reward: [-511.638 -511.638 -511.638] [53.6802], Avg: [-774.737 -774.737 -774.737] (1.000)
Step: 47799, Reward: [-539.816 -539.816 -539.816] [60.8584], Avg: [-774.555 -774.555 -774.555] (1.000)
Step: 47849, Reward: [-706.114 -706.114 -706.114] [72.3134], Avg: [-774.559 -774.559 -774.559] (1.000)
Step: 47899, Reward: [-787.62 -787.62 -787.62] [97.2104], Avg: [-774.674 -774.674 -774.674] (1.000)
Step: 47949, Reward: [-679.912 -679.912 -679.912] [87.1986], Avg: [-774.666 -774.666 -774.666] (1.000)
Step: 47999, Reward: [-640.312 -640.312 -640.312] [49.4370], Avg: [-774.578 -774.578 -774.578] (1.000)
Step: 48049, Reward: [-620.873 -620.873 -620.873] [112.2719], Avg: [-774.535 -774.535 -774.535] (1.000)
Step: 48099, Reward: [-583.654 -583.654 -583.654] [56.9731], Avg: [-774.395 -774.395 -774.395] (1.000)
Step: 48149, Reward: [-679.685 -679.685 -679.685] [43.2590], Avg: [-774.342 -774.342 -774.342] (1.000)
Step: 48199, Reward: [-840.831 -840.831 -840.831] [92.4331], Avg: [-774.507 -774.507 -774.507] (1.000)
Step: 48249, Reward: [-611.875 -611.875 -611.875] [92.0863], Avg: [-774.434 -774.434 -774.434] (1.000)
Step: 48299, Reward: [-562.405 -562.405 -562.405] [60.1645], Avg: [-774.277 -774.277 -774.277] (1.000)
Step: 48349, Reward: [-502.554 -502.554 -502.554] [62.9449], Avg: [-774.061 -774.061 -774.061] (1.000)
Step: 48399, Reward: [-442.949 -442.949 -442.949] [57.1328], Avg: [-773.778 -773.778 -773.778] (1.000)
Step: 48449, Reward: [-459.055 -459.055 -459.055] [33.5130], Avg: [-773.487 -773.487 -773.487] (1.000)
Step: 48499, Reward: [-435.562 -435.562 -435.562] [34.0105], Avg: [-773.174 -773.174 -773.174] (1.000)
Step: 48549, Reward: [-489.962 -489.962 -489.962] [72.2877], Avg: [-772.957 -772.957 -772.957] (1.000)
Step: 48599, Reward: [-500.035 -500.035 -500.035] [50.2616], Avg: [-772.728 -772.728 -772.728] (1.000)
Step: 48649, Reward: [-424.644 -424.644 -424.644] [21.6434], Avg: [-772.392 -772.392 -772.392] (1.000)
Step: 48699, Reward: [-431.298 -431.298 -431.298] [10.8803], Avg: [-772.053 -772.053 -772.053] (1.000)
Step: 48749, Reward: [-431.075 -431.075 -431.075] [68.4485], Avg: [-771.774 -771.774 -771.774] (1.000)
Step: 48799, Reward: [-448.64 -448.64 -448.64] [19.7546], Avg: [-771.463 -771.463 -771.463] (1.000)
Step: 48849, Reward: [-464.678 -464.678 -464.678] [56.7720], Avg: [-771.207 -771.207 -771.207] (1.000)
Step: 48899, Reward: [-544.953 -544.953 -544.953] [56.8456], Avg: [-771.034 -771.034 -771.034] (1.000)
Step: 48949, Reward: [-652.364 -652.364 -652.364] [39.8227], Avg: [-770.953 -770.953 -770.953] (1.000)
Step: 48999, Reward: [-721.198 -721.198 -721.198] [64.4323], Avg: [-770.968 -770.968 -770.968] (1.000)
Step: 49049, Reward: [-806.208 -806.208 -806.208] [67.6487], Avg: [-771.073 -771.073 -771.073] (1.000)
Step: 49099, Reward: [-752.921 -752.921 -752.921] [61.6549], Avg: [-771.117 -771.117 -771.117] (1.000)
Step: 49149, Reward: [-615.268 -615.268 -615.268] [41.4662], Avg: [-771.001 -771.001 -771.001] (1.000)
Step: 49199, Reward: [-615.487 -615.487 -615.487] [53.1946], Avg: [-770.897 -770.897 -770.897] (1.000)
Step: 49249, Reward: [-472.919 -472.919 -472.919] [42.3173], Avg: [-770.637 -770.637 -770.637] (1.000)
Step: 49299, Reward: [-467.274 -467.274 -467.274] [47.7026], Avg: [-770.378 -770.378 -770.378] (1.000)
Step: 49349, Reward: [-429.188 -429.188 -429.188] [34.2464], Avg: [-770.067 -770.067 -770.067] (1.000)
Step: 49399, Reward: [-400.813 -400.813 -400.813] [36.2588], Avg: [-769.73 -769.73 -769.73] (1.000)
Step: 49449, Reward: [-399.573 -399.573 -399.573] [33.1116], Avg: [-769.389 -769.389 -769.389] (1.000)
Step: 49499, Reward: [-417.8 -417.8 -417.8] [14.6258], Avg: [-769.049 -769.049 -769.049] (1.000)
Step: 49549, Reward: [-408.674 -408.674 -408.674] [88.2879], Avg: [-768.774 -768.774 -768.774] (1.000)
Step: 49599, Reward: [-413.883 -413.883 -413.883] [60.8566], Avg: [-768.478 -768.478 -768.478] (1.000)
Step: 49649, Reward: [-439.915 -439.915 -439.915] [102.2689], Avg: [-768.25 -768.25 -768.25] (1.000)
Step: 49699, Reward: [-404.813 -404.813 -404.813] [61.6754], Avg: [-767.947 -767.947 -767.947] (1.000)
Step: 49749, Reward: [-425.793 -425.793 -425.793] [27.0512], Avg: [-767.63 -767.63 -767.63] (1.000)
Step: 49799, Reward: [-558.025 -558.025 -558.025] [236.9784], Avg: [-767.657 -767.657 -767.657] (1.000)
Step: 49849, Reward: [-2168.145 -2168.145 -2168.145] [289.8255], Avg: [-769.353 -769.353 -769.353] (1.000)
Step: 49899, Reward: [-1958.714 -1958.714 -1958.714] [267.3367], Avg: [-770.812 -770.812 -770.812] (1.000)
Step: 49949, Reward: [-2298.822 -2298.822 -2298.822] [261.5747], Avg: [-772.604 -772.604 -772.604] (1.000)
Step: 49999, Reward: [-1993.259 -1993.259 -1993.259] [61.9183], Avg: [-773.886 -773.886 -773.886] (1.000)
Step: 50049, Reward: [-1920.063 -1920.063 -1920.063] [344.6826], Avg: [-775.376 -775.376 -775.376] (1.000)
Step: 50099, Reward: [-2084.631 -2084.631 -2084.631] [128.2302], Avg: [-776.81 -776.81 -776.81] (1.000)
Step: 50149, Reward: [-1939.133 -1939.133 -1939.133] [281.0946], Avg: [-778.249 -778.249 -778.249] (1.000)
Step: 50199, Reward: [-2045.043 -2045.043 -2045.043] [287.2923], Avg: [-779.797 -779.797 -779.797] (1.000)
Step: 50249, Reward: [-1830.916 -1830.916 -1830.916] [207.2327], Avg: [-781.049 -781.049 -781.049] (1.000)
Step: 50299, Reward: [-1891.146 -1891.146 -1891.146] [186.1979], Avg: [-782.338 -782.338 -782.338] (1.000)
Step: 50349, Reward: [-1724.193 -1724.193 -1724.193] [282.5701], Avg: [-783.554 -783.554 -783.554] (1.000)
Step: 50399, Reward: [-1862.95 -1862.95 -1862.95] [348.7717], Avg: [-784.971 -784.971 -784.971] (1.000)
Step: 50449, Reward: [-1855.83 -1855.83 -1855.83] [199.6538], Avg: [-786.23 -786.23 -786.23] (1.000)
Step: 50499, Reward: [-1629.039 -1629.039 -1629.039] [266.1079], Avg: [-787.328 -787.328 -787.328] (1.000)
Step: 50549, Reward: [-1495.176 -1495.176 -1495.176] [220.4158], Avg: [-788.246 -788.246 -788.246] (1.000)
Step: 50599, Reward: [-1438.964 -1438.964 -1438.964] [92.9985], Avg: [-788.981 -788.981 -788.981] (1.000)
Step: 50649, Reward: [-1589.701 -1589.701 -1589.701] [115.4291], Avg: [-789.885 -789.885 -789.885] (1.000)
Step: 50699, Reward: [-1388.899 -1388.899 -1388.899] [58.2320], Avg: [-790.533 -790.533 -790.533] (1.000)
Step: 50749, Reward: [-1490.917 -1490.917 -1490.917] [175.5823], Avg: [-791.396 -791.396 -791.396] (1.000)
Step: 50799, Reward: [-1549.741 -1549.741 -1549.741] [130.8954], Avg: [-792.272 -792.272 -792.272] (1.000)
Step: 50849, Reward: [-1689.573 -1689.573 -1689.573] [232.5465], Avg: [-793.383 -793.383 -793.383] (1.000)
Step: 50899, Reward: [-1514.751 -1514.751 -1514.751] [199.6845], Avg: [-794.287 -794.287 -794.287] (1.000)
Step: 50949, Reward: [-1504.661 -1504.661 -1504.661] [139.3151], Avg: [-795.121 -795.121 -795.121] (1.000)
Step: 50999, Reward: [-1593.053 -1593.053 -1593.053] [168.5190], Avg: [-796.069 -796.069 -796.069] (1.000)
Step: 51049, Reward: [-1471.863 -1471.863 -1471.863] [183.1156], Avg: [-796.91 -796.91 -796.91] (1.000)
Step: 51099, Reward: [-1535.063 -1535.063 -1535.063] [234.4067], Avg: [-797.862 -797.862 -797.862] (1.000)
Step: 51149, Reward: [-1753.402 -1753.402 -1753.402] [324.5821], Avg: [-799.113 -799.113 -799.113] (1.000)
Step: 51199, Reward: [-1953.09 -1953.09 -1953.09] [500.2935], Avg: [-800.728 -800.728 -800.728] (1.000)
Step: 51249, Reward: [-1725.614 -1725.614 -1725.614] [468.6501], Avg: [-802.088 -802.088 -802.088] (1.000)
Step: 51299, Reward: [-1954.901 -1954.901 -1954.901] [261.8093], Avg: [-803.467 -803.467 -803.467] (1.000)
Step: 51349, Reward: [-2035.755 -2035.755 -2035.755] [226.5392], Avg: [-804.887 -804.887 -804.887] (1.000)
Step: 51399, Reward: [-1924.266 -1924.266 -1924.266] [259.0832], Avg: [-806.228 -806.228 -806.228] (1.000)
Step: 51449, Reward: [-1788.291 -1788.291 -1788.291] [269.6719], Avg: [-807.445 -807.445 -807.445] (1.000)
Step: 51499, Reward: [-1396.67 -1396.67 -1396.67] [178.1838], Avg: [-808.19 -808.19 -808.19] (1.000)
Step: 51549, Reward: [-887.521 -887.521 -887.521] [217.0437], Avg: [-808.477 -808.477 -808.477] (1.000)
Step: 51599, Reward: [-752.147 -752.147 -752.147] [216.1961], Avg: [-808.632 -808.632 -808.632] (1.000)
Step: 51649, Reward: [-576.413 -576.413 -576.413] [49.1227], Avg: [-808.455 -808.455 -808.455] (1.000)
Step: 51699, Reward: [-646.32 -646.32 -646.32] [43.9846], Avg: [-808.341 -808.341 -808.341] (1.000)
Step: 51749, Reward: [-1043.298 -1043.298 -1043.298] [453.6643], Avg: [-809.006 -809.006 -809.006] (1.000)
Step: 51799, Reward: [-683.495 -683.495 -683.495] [72.8064], Avg: [-808.955 -808.955 -808.955] (1.000)
Step: 51849, Reward: [-747.148 -747.148 -747.148] [255.7439], Avg: [-809.142 -809.142 -809.142] (1.000)
Step: 51899, Reward: [-872.056 -872.056 -872.056] [368.6103], Avg: [-809.558 -809.558 -809.558] (1.000)
Step: 51949, Reward: [-641.966 -641.966 -641.966] [248.4576], Avg: [-809.636 -809.636 -809.636] (1.000)
Step: 51999, Reward: [-1012.378 -1012.378 -1012.378] [378.1028], Avg: [-810.194 -810.194 -810.194] (1.000)
Step: 52049, Reward: [-957.252 -957.252 -957.252] [343.9316], Avg: [-810.666 -810.666 -810.666] (1.000)
Step: 52099, Reward: [-586.767 -586.767 -586.767] [123.8948], Avg: [-810.57 -810.57 -810.57] (1.000)
Step: 52149, Reward: [-762.753 -762.753 -762.753] [172.5784], Avg: [-810.689 -810.689 -810.689] (1.000)
Step: 52199, Reward: [-1077.419 -1077.419 -1077.419] [157.2310], Avg: [-811.096 -811.096 -811.096] (1.000)
Step: 52249, Reward: [-572.205 -572.205 -572.205] [78.2648], Avg: [-810.942 -810.942 -810.942] (1.000)
Step: 52299, Reward: [-774.08 -774.08 -774.08] [106.8044], Avg: [-811.009 -811.009 -811.009] (1.000)
Step: 52349, Reward: [-613.754 -613.754 -613.754] [157.7956], Avg: [-810.971 -810.971 -810.971] (1.000)
Step: 52399, Reward: [-828.102 -828.102 -828.102] [242.8599], Avg: [-811.219 -811.219 -811.219] (1.000)
Step: 52449, Reward: [-781.285 -781.285 -781.285] [111.3277], Avg: [-811.297 -811.297 -811.297] (1.000)
Step: 52499, Reward: [-800.359 -800.359 -800.359] [331.7048], Avg: [-811.602 -811.602 -811.602] (1.000)
Step: 52549, Reward: [-667.647 -667.647 -667.647] [119.3149], Avg: [-811.579 -811.579 -811.579] (1.000)
Step: 52599, Reward: [-704.292 -704.292 -704.292] [288.2539], Avg: [-811.751 -811.751 -811.751] (1.000)
Step: 52649, Reward: [-492.803 -492.803 -492.803] [82.8186], Avg: [-811.526 -811.526 -811.526] (1.000)
Step: 52699, Reward: [-753.049 -753.049 -753.049] [116.7982], Avg: [-811.582 -811.582 -811.582] (1.000)
Step: 52749, Reward: [-714.6 -714.6 -714.6] [166.8414], Avg: [-811.648 -811.648 -811.648] (1.000)
Step: 52799, Reward: [-584.863 -584.863 -584.863] [145.7111], Avg: [-811.571 -811.571 -811.571] (1.000)
Step: 52849, Reward: [-568.809 -568.809 -568.809] [148.8297], Avg: [-811.482 -811.482 -811.482] (1.000)
Step: 52899, Reward: [-513.234 -513.234 -513.234] [215.6804], Avg: [-811.404 -811.404 -811.404] (1.000)
Step: 52949, Reward: [-646.99 -646.99 -646.99] [179.5448], Avg: [-811.419 -811.419 -811.419] (1.000)
Step: 52999, Reward: [-728.496 -728.496 -728.496] [181.4627], Avg: [-811.512 -811.512 -811.512] (1.000)
Step: 53049, Reward: [-702.734 -702.734 -702.734] [216.4635], Avg: [-811.613 -811.613 -811.613] (1.000)
Step: 53099, Reward: [-625.531 -625.531 -625.531] [173.5508], Avg: [-811.601 -811.601 -811.601] (1.000)
Step: 53149, Reward: [-823.812 -823.812 -823.812] [374.8842], Avg: [-811.965 -811.965 -811.965] (1.000)
Step: 53199, Reward: [-962.759 -962.759 -962.759] [503.0221], Avg: [-812.58 -812.58 -812.58] (1.000)
Step: 53249, Reward: [-559.864 -559.864 -559.864] [130.8192], Avg: [-812.465 -812.465 -812.465] (1.000)
Step: 53299, Reward: [-773.758 -773.758 -773.758] [196.1597], Avg: [-812.613 -812.613 -812.613] (1.000)
Step: 53349, Reward: [-589.22 -589.22 -589.22] [86.4147], Avg: [-812.485 -812.485 -812.485] (1.000)
Step: 53399, Reward: [-690.244 -690.244 -690.244] [85.1309], Avg: [-812.45 -812.45 -812.45] (1.000)
Step: 53449, Reward: [-636.637 -636.637 -636.637] [261.9466], Avg: [-812.531 -812.531 -812.531] (1.000)
Step: 53499, Reward: [-607.875 -607.875 -607.875] [130.2559], Avg: [-812.461 -812.461 -812.461] (1.000)
Step: 53549, Reward: [-758.976 -758.976 -758.976] [241.3747], Avg: [-812.637 -812.637 -812.637] (1.000)
Step: 53599, Reward: [-735.787 -735.787 -735.787] [244.0134], Avg: [-812.792 -812.792 -812.792] (1.000)
Step: 53649, Reward: [-1141.68 -1141.68 -1141.68] [390.6292], Avg: [-813.463 -813.463 -813.463] (1.000)
Step: 53699, Reward: [-558.203 -558.203 -558.203] [103.3614], Avg: [-813.322 -813.322 -813.322] (1.000)
Step: 53749, Reward: [-785.185 -785.185 -785.185] [387.9104], Avg: [-813.656 -813.656 -813.656] (1.000)
Step: 53799, Reward: [-812.332 -812.332 -812.332] [245.0443], Avg: [-813.883 -813.883 -813.883] (1.000)
Step: 53849, Reward: [-540.716 -540.716 -540.716] [52.8132], Avg: [-813.678 -813.678 -813.678] (1.000)
Step: 53899, Reward: [-683.81 -683.81 -683.81] [182.9206], Avg: [-813.727 -813.727 -813.727] (1.000)
Step: 53949, Reward: [-787.154 -787.154 -787.154] [289.4298], Avg: [-813.971 -813.971 -813.971] (1.000)
Step: 53999, Reward: [-636.254 -636.254 -636.254] [129.5422], Avg: [-813.926 -813.926 -813.926] (1.000)
Step: 54049, Reward: [-588.319 -588.319 -588.319] [155.8141], Avg: [-813.862 -813.862 -813.862] (1.000)
Step: 54099, Reward: [-714.409 -714.409 -714.409] [186.9841], Avg: [-813.943 -813.943 -813.943] (1.000)
Step: 54149, Reward: [-892.972 -892.972 -892.972] [197.2004], Avg: [-814.198 -814.198 -814.198] (1.000)
Step: 54199, Reward: [-745.634 -745.634 -745.634] [121.8791], Avg: [-814.247 -814.247 -814.247] (1.000)
Step: 54249, Reward: [-743.578 -743.578 -743.578] [310.0058], Avg: [-814.468 -814.468 -814.468] (1.000)
Step: 54299, Reward: [-714.435 -714.435 -714.435] [145.4238], Avg: [-814.509 -814.509 -814.509] (1.000)
Step: 54349, Reward: [-690.554 -690.554 -690.554] [137.0124], Avg: [-814.521 -814.521 -814.521] (1.000)
Step: 54399, Reward: [-616.934 -616.934 -616.934] [85.0483], Avg: [-814.418 -814.418 -814.418] (1.000)
Step: 54449, Reward: [-664.736 -664.736 -664.736] [217.1161], Avg: [-814.48 -814.48 -814.48] (1.000)
Step: 54499, Reward: [-753.298 -753.298 -753.298] [205.1154], Avg: [-814.612 -814.612 -814.612] (1.000)
Step: 54549, Reward: [-1000.182 -1000.182 -1000.182] [380.8510], Avg: [-815.131 -815.131 -815.131] (1.000)
Step: 54599, Reward: [-947.873 -947.873 -947.873] [408.8977], Avg: [-815.627 -815.627 -815.627] (1.000)
Step: 54649, Reward: [-1169.335 -1169.335 -1169.335] [447.7437], Avg: [-816.36 -816.36 -816.36] (1.000)
Step: 54699, Reward: [-1059.244 -1059.244 -1059.244] [435.3734], Avg: [-816.98 -816.98 -816.98] (1.000)
Step: 54749, Reward: [-1015.904 -1015.904 -1015.904] [516.1621], Avg: [-817.633 -817.633 -817.633] (1.000)
Step: 54799, Reward: [-592.573 -592.573 -592.573] [102.0129], Avg: [-817.521 -817.521 -817.521] (1.000)
Step: 54849, Reward: [-615.395 -615.395 -615.395] [166.0364], Avg: [-817.488 -817.488 -817.488] (1.000)
Step: 54899, Reward: [-862.902 -862.902 -862.902] [236.2351], Avg: [-817.745 -817.745 -817.745] (1.000)
Step: 54949, Reward: [-633.27 -633.27 -633.27] [175.3728], Avg: [-817.736 -817.736 -817.736] (1.000)
Step: 54999, Reward: [-874.434 -874.434 -874.434] [177.1218], Avg: [-817.949 -817.949 -817.949] (1.000)
Step: 55049, Reward: [-779.555 -779.555 -779.555] [291.4898], Avg: [-818.179 -818.179 -818.179] (1.000)
Step: 55099, Reward: [-825.422 -825.422 -825.422] [181.3730], Avg: [-818.35 -818.35 -818.35] (1.000)
Step: 55149, Reward: [-881.817 -881.817 -881.817] [344.0603], Avg: [-818.72 -818.72 -818.72] (1.000)
Step: 55199, Reward: [-786.61 -786.61 -786.61] [456.6340], Avg: [-819.104 -819.104 -819.104] (1.000)
Step: 55249, Reward: [-942.269 -942.269 -942.269] [285.1724], Avg: [-819.474 -819.474 -819.474] (1.000)
Step: 55299, Reward: [-767.714 -767.714 -767.714] [245.2312], Avg: [-819.649 -819.649 -819.649] (1.000)
Step: 55349, Reward: [-836.667 -836.667 -836.667] [95.4621], Avg: [-819.75 -819.75 -819.75] (1.000)
Step: 55399, Reward: [-822.698 -822.698 -822.698] [254.1796], Avg: [-819.982 -819.982 -819.982] (1.000)
Step: 55449, Reward: [-737.208 -737.208 -737.208] [124.1674], Avg: [-820.02 -820.02 -820.02] (1.000)
Step: 55499, Reward: [-883.394 -883.394 -883.394] [317.2602], Avg: [-820.362 -820.362 -820.362] (1.000)
Step: 55549, Reward: [-720.831 -720.831 -720.831] [255.8578], Avg: [-820.503 -820.503 -820.503] (1.000)
Step: 55599, Reward: [-911.183 -911.183 -911.183] [405.3705], Avg: [-820.949 -820.949 -820.949] (1.000)
Step: 55649, Reward: [-1062.014 -1062.014 -1062.014] [380.5389], Avg: [-821.508 -821.508 -821.508] (1.000)
Step: 55699, Reward: [-974.045 -974.045 -974.045] [194.6428], Avg: [-821.819 -821.819 -821.819] (1.000)
Step: 55749, Reward: [-1222.854 -1222.854 -1222.854] [131.4155], Avg: [-822.297 -822.297 -822.297] (1.000)
Step: 55799, Reward: [-1029.658 -1029.658 -1029.658] [256.8853], Avg: [-822.713 -822.713 -822.713] (1.000)
Step: 55849, Reward: [-1113.678 -1113.678 -1113.678] [333.3210], Avg: [-823.272 -823.272 -823.272] (1.000)
Step: 55899, Reward: [-859.662 -859.662 -859.662] [223.2653], Avg: [-823.504 -823.504 -823.504] (1.000)
Step: 55949, Reward: [-1121.904 -1121.904 -1121.904] [394.9303], Avg: [-824.124 -824.124 -824.124] (1.000)
Step: 55999, Reward: [-1177.513 -1177.513 -1177.513] [177.1315], Avg: [-824.597 -824.597 -824.597] (1.000)
Step: 56049, Reward: [-1202.966 -1202.966 -1202.966] [339.3584], Avg: [-825.238 -825.238 -825.238] (1.000)
Step: 56099, Reward: [-1134.758 -1134.758 -1134.758] [110.1301], Avg: [-825.612 -825.612 -825.612] (1.000)
Step: 56149, Reward: [-1166.543 -1166.543 -1166.543] [253.7211], Avg: [-826.141 -826.141 -826.141] (1.000)
Step: 56199, Reward: [-1231.706 -1231.706 -1231.706] [225.9708], Avg: [-826.703 -826.703 -826.703] (1.000)
Step: 56249, Reward: [-1099.679 -1099.679 -1099.679] [171.6095], Avg: [-827.098 -827.098 -827.098] (1.000)
Step: 56299, Reward: [-1163.196 -1163.196 -1163.196] [191.2343], Avg: [-827.566 -827.566 -827.566] (1.000)
Step: 56349, Reward: [-1070.927 -1070.927 -1070.927] [353.9273], Avg: [-828.096 -828.096 -828.096] (1.000)
Step: 56399, Reward: [-993.834 -993.834 -993.834] [248.4969], Avg: [-828.464 -828.464 -828.464] (1.000)
Step: 56449, Reward: [-996.211 -996.211 -996.211] [172.0711], Avg: [-828.765 -828.765 -828.765] (1.000)
Step: 56499, Reward: [-936.377 -936.377 -936.377] [136.5375], Avg: [-828.981 -828.981 -828.981] (1.000)
Step: 56549, Reward: [-992.566 -992.566 -992.566] [131.4591], Avg: [-829.242 -829.242 -829.242] (1.000)
Step: 56599, Reward: [-1078.11 -1078.11 -1078.11] [167.4557], Avg: [-829.609 -829.609 -829.609] (1.000)
Step: 56649, Reward: [-1007.945 -1007.945 -1007.945] [151.7550], Avg: [-829.901 -829.901 -829.901] (1.000)
Step: 56699, Reward: [-1154.904 -1154.904 -1154.904] [268.5989], Avg: [-830.424 -830.424 -830.424] (1.000)
Step: 56749, Reward: [-1159.623 -1159.623 -1159.623] [357.5621], Avg: [-831.029 -831.029 -831.029] (1.000)
Step: 56799, Reward: [-1038.287 -1038.287 -1038.287] [247.0231], Avg: [-831.429 -831.429 -831.429] (1.000)
Step: 56849, Reward: [-1021.042 -1021.042 -1021.042] [133.4849], Avg: [-831.713 -831.713 -831.713] (1.000)
Step: 56899, Reward: [-1263.582 -1263.582 -1263.582] [172.0814], Avg: [-832.244 -832.244 -832.244] (1.000)
Step: 56949, Reward: [-1308.678 -1308.678 -1308.678] [338.7719], Avg: [-832.96 -832.96 -832.96] (1.000)
Step: 56999, Reward: [-1324.773 -1324.773 -1324.773] [319.6794], Avg: [-833.672 -833.672 -833.672] (1.000)
Step: 57049, Reward: [-1127.744 -1127.744 -1127.744] [276.4808], Avg: [-834.172 -834.172 -834.172] (1.000)
Step: 57099, Reward: [-1411.868 -1411.868 -1411.868] [129.1846], Avg: [-834.791 -834.791 -834.791] (1.000)
Step: 57149, Reward: [-1427.2 -1427.2 -1427.2] [345.4792], Avg: [-835.611 -835.611 -835.611] (1.000)
Step: 57199, Reward: [-1274.245 -1274.245 -1274.245] [175.0772], Avg: [-836.148 -836.148 -836.148] (1.000)
Step: 57249, Reward: [-1261.549 -1261.549 -1261.549] [100.3393], Avg: [-836.607 -836.607 -836.607] (1.000)
Step: 57299, Reward: [-1559.004 -1559.004 -1559.004] [396.4043], Avg: [-837.583 -837.583 -837.583] (1.000)
Step: 57349, Reward: [-1438.44 -1438.44 -1438.44] [536.0716], Avg: [-838.574 -838.574 -838.574] (1.000)
Step: 57399, Reward: [-1121.551 -1121.551 -1121.551] [403.7154], Avg: [-839.172 -839.172 -839.172] (1.000)
Step: 57449, Reward: [-1553.291 -1553.291 -1553.291] [239.7593], Avg: [-840.003 -840.003 -840.003] (1.000)
Step: 57499, Reward: [-1463.013 -1463.013 -1463.013] [307.8278], Avg: [-840.812 -840.812 -840.812] (1.000)
Step: 57549, Reward: [-1468.49 -1468.49 -1468.49] [333.6477], Avg: [-841.647 -841.647 -841.647] (1.000)
Step: 57599, Reward: [-1444.764 -1444.764 -1444.764] [296.1688], Avg: [-842.428 -842.428 -842.428] (1.000)
Step: 57649, Reward: [-1485.99 -1485.99 -1485.99] [360.1116], Avg: [-843.298 -843.298 -843.298] (1.000)
Step: 57699, Reward: [-1246.704 -1246.704 -1246.704] [193.8634], Avg: [-843.816 -843.816 -843.816] (1.000)
Step: 57749, Reward: [-1331.475 -1331.475 -1331.475] [492.5345], Avg: [-844.665 -844.665 -844.665] (1.000)
Step: 57799, Reward: [-1180.539 -1180.539 -1180.539] [566.4029], Avg: [-845.445 -845.445 -845.445] (1.000)
Step: 57849, Reward: [-1510.076 -1510.076 -1510.076] [256.7261], Avg: [-846.241 -846.241 -846.241] (1.000)
Step: 57899, Reward: [-1412.435 -1412.435 -1412.435] [474.5917], Avg: [-847.14 -847.14 -847.14] (1.000)
Step: 57949, Reward: [-1334.963 -1334.963 -1334.963] [300.6706], Avg: [-847.821 -847.821 -847.821] (1.000)
Step: 57999, Reward: [-1333.275 -1333.275 -1333.275] [371.3162], Avg: [-848.559 -848.559 -848.559] (1.000)
Step: 58049, Reward: [-1353.991 -1353.991 -1353.991] [445.5525], Avg: [-849.378 -849.378 -849.378] (1.000)
Step: 58099, Reward: [-871.623 -871.623 -871.623] [304.3557], Avg: [-849.659 -849.659 -849.659] (1.000)
Step: 58149, Reward: [-1147.601 -1147.601 -1147.601] [251.5607], Avg: [-850.132 -850.132 -850.132] (1.000)
Step: 58199, Reward: [-867.941 -867.941 -867.941] [68.3492], Avg: [-850.206 -850.206 -850.206] (1.000)
Step: 58249, Reward: [-875.342 -875.342 -875.342] [174.1278], Avg: [-850.377 -850.377 -850.377] (1.000)
Step: 58299, Reward: [-1299.162 -1299.162 -1299.162] [165.6168], Avg: [-850.904 -850.904 -850.904] (1.000)
Step: 58349, Reward: [-991.656 -991.656 -991.656] [196.9534], Avg: [-851.193 -851.193 -851.193] (1.000)
Step: 58399, Reward: [-1049.89 -1049.89 -1049.89] [263.8243], Avg: [-851.589 -851.589 -851.589] (1.000)
Step: 58449, Reward: [-1046.389 -1046.389 -1046.389] [200.3938], Avg: [-851.927 -851.927 -851.927] (1.000)
Step: 58499, Reward: [-876.886 -876.886 -876.886] [187.8190], Avg: [-852.109 -852.109 -852.109] (1.000)
Step: 58549, Reward: [-1255.878 -1255.878 -1255.878] [522.0482], Avg: [-852.9 -852.9 -852.9] (1.000)
Step: 58599, Reward: [-1185.681 -1185.681 -1185.681] [243.2356], Avg: [-853.391 -853.391 -853.391] (1.000)
Step: 58649, Reward: [-884.769 -884.769 -884.769] [350.2010], Avg: [-853.717 -853.717 -853.717] (1.000)
Step: 58699, Reward: [-975.513 -975.513 -975.513] [316.8340], Avg: [-854.09 -854.09 -854.09] (1.000)
Step: 58749, Reward: [-1197.815 -1197.815 -1197.815] [360.1020], Avg: [-854.689 -854.689 -854.689] (1.000)
Step: 58799, Reward: [-1015.289 -1015.289 -1015.289] [143.3323], Avg: [-854.948 -854.948 -854.948] (1.000)
Step: 58849, Reward: [-960.127 -960.127 -960.127] [353.1882], Avg: [-855.337 -855.337 -855.337] (1.000)
Step: 58899, Reward: [-820.988 -820.988 -820.988] [106.0426], Avg: [-855.398 -855.398 -855.398] (1.000)
Step: 58949, Reward: [-1046.525 -1046.525 -1046.525] [256.2828], Avg: [-855.777 -855.777 -855.777] (1.000)
Step: 58999, Reward: [-959.654 -959.654 -959.654] [180.8352], Avg: [-856.019 -856.019 -856.019] (1.000)
Step: 59049, Reward: [-1291.562 -1291.562 -1291.562] [528.9385], Avg: [-856.835 -856.835 -856.835] (1.000)
Step: 59099, Reward: [-1529.043 -1529.043 -1529.043] [324.5252], Avg: [-857.679 -857.679 -857.679] (1.000)
Step: 59149, Reward: [-1040.449 -1040.449 -1040.449] [211.5811], Avg: [-858.012 -858.012 -858.012] (1.000)
Step: 59199, Reward: [-902.362 -902.362 -902.362] [200.6039], Avg: [-858.219 -858.219 -858.219] (1.000)
Step: 59249, Reward: [-988.317 -988.317 -988.317] [319.5756], Avg: [-858.598 -858.598 -858.598] (1.000)
Step: 59299, Reward: [-1241.669 -1241.669 -1241.669] [264.4039], Avg: [-859.144 -859.144 -859.144] (1.000)
Step: 59349, Reward: [-1596.097 -1596.097 -1596.097] [231.6378], Avg: [-859.96 -859.96 -859.96] (1.000)
Step: 59399, Reward: [-1252.098 -1252.098 -1252.098] [271.5013], Avg: [-860.519 -860.519 -860.519] (1.000)
Step: 59449, Reward: [-1095.388 -1095.388 -1095.388] [323.5394], Avg: [-860.988 -860.988 -860.988] (1.000)
Step: 59499, Reward: [-1486.716 -1486.716 -1486.716] [234.0089], Avg: [-861.711 -861.711 -861.711] (1.000)
Step: 59549, Reward: [-1047.62 -1047.62 -1047.62] [330.1281], Avg: [-862.144 -862.144 -862.144] (1.000)
Step: 59599, Reward: [-1508.117 -1508.117 -1508.117] [408.9596], Avg: [-863.029 -863.029 -863.029] (1.000)
Step: 59649, Reward: [-1448.925 -1448.925 -1448.925] [315.3808], Avg: [-863.785 -863.785 -863.785] (1.000)
Step: 59699, Reward: [-1920.436 -1920.436 -1920.436] [164.9596], Avg: [-864.808 -864.808 -864.808] (1.000)
Step: 59749, Reward: [-1377.832 -1377.832 -1377.832] [175.2403], Avg: [-865.384 -865.384 -865.384] (1.000)
Step: 59799, Reward: [-1633.32 -1633.32 -1633.32] [281.4398], Avg: [-866.261 -866.261 -866.261] (1.000)
Step: 59849, Reward: [-1365.53 -1365.53 -1365.53] [330.8728], Avg: [-866.955 -866.955 -866.955] (1.000)
Step: 59899, Reward: [-1353.401 -1353.401 -1353.401] [568.1191], Avg: [-867.835 -867.835 -867.835] (1.000)
Step: 59949, Reward: [-1643.993 -1643.993 -1643.993] [599.6270], Avg: [-868.982 -868.982 -868.982] (1.000)
Step: 59999, Reward: [-1806.18 -1806.18 -1806.18] [448.6257], Avg: [-870.137 -870.137 -870.137] (1.000)
Step: 60049, Reward: [-1739.4 -1739.4 -1739.4] [286.7155], Avg: [-871.1 -871.1 -871.1] (1.000)
Step: 60099, Reward: [-1673.15 -1673.15 -1673.15] [363.7003], Avg: [-872.07 -872.07 -872.07] (1.000)
Step: 60149, Reward: [-1915.033 -1915.033 -1915.033] [98.1300], Avg: [-873.018 -873.018 -873.018] (1.000)
Step: 60199, Reward: [-1630.962 -1630.962 -1630.962] [366.3869], Avg: [-873.952 -873.952 -873.952] (1.000)
Step: 60249, Reward: [-1596.068 -1596.068 -1596.068] [468.6368], Avg: [-874.94 -874.94 -874.94] (1.000)
Step: 60299, Reward: [-1833.539 -1833.539 -1833.539] [335.7932], Avg: [-876.013 -876.013 -876.013] (1.000)
Step: 60349, Reward: [-1792.791 -1792.791 -1792.791] [546.3383], Avg: [-877.226 -877.226 -877.226] (1.000)
Step: 60399, Reward: [-1594.787 -1594.787 -1594.787] [430.1429], Avg: [-878.176 -878.176 -878.176] (1.000)
Step: 60449, Reward: [-1711.895 -1711.895 -1711.895] [253.3682], Avg: [-879.075 -879.075 -879.075] (1.000)
Step: 60499, Reward: [-1477.999 -1477.999 -1477.999] [344.8701], Avg: [-879.855 -879.855 -879.855] (1.000)
Step: 60549, Reward: [-1407.195 -1407.195 -1407.195] [487.6477], Avg: [-880.693 -880.693 -880.693] (1.000)
Step: 60599, Reward: [-1516.38 -1516.38 -1516.38] [385.4513], Avg: [-881.536 -881.536 -881.536] (1.000)
Step: 60649, Reward: [-1307.305 -1307.305 -1307.305] [433.6203], Avg: [-882.244 -882.244 -882.244] (1.000)
Step: 60699, Reward: [-1404.752 -1404.752 -1404.752] [233.4583], Avg: [-882.867 -882.867 -882.867] (1.000)
Step: 60749, Reward: [-1413.135 -1413.135 -1413.135] [566.7399], Avg: [-883.77 -883.77 -883.77] (1.000)
Step: 60799, Reward: [-1167.248 -1167.248 -1167.248] [392.1134], Avg: [-884.325 -884.325 -884.325] (1.000)
Step: 60849, Reward: [-1100.06 -1100.06 -1100.06] [135.6083], Avg: [-884.614 -884.614 -884.614] (1.000)
Step: 60899, Reward: [-890.9 -890.9 -890.9] [263.0666], Avg: [-884.835 -884.835 -884.835] (1.000)
Step: 60949, Reward: [-1285.794 -1285.794 -1285.794] [482.3259], Avg: [-885.56 -885.56 -885.56] (1.000)
Step: 60999, Reward: [-1000.138 -1000.138 -1000.138] [332.1716], Avg: [-885.926 -885.926 -885.926] (1.000)
Step: 61049, Reward: [-835.527 -835.527 -835.527] [116.9663], Avg: [-885.98 -885.98 -885.98] (1.000)
Step: 61099, Reward: [-788.493 -788.493 -788.493] [140.6740], Avg: [-886.016 -886.016 -886.016] (1.000)
Step: 61149, Reward: [-660.893 -660.893 -660.893] [80.0047], Avg: [-885.897 -885.897 -885.897] (1.000)
Step: 61199, Reward: [-689.456 -689.456 -689.456] [35.7449], Avg: [-885.766 -885.766 -885.766] (1.000)
Step: 61249, Reward: [-625.054 -625.054 -625.054] [94.8775], Avg: [-885.63 -885.63 -885.63] (1.000)
Step: 61299, Reward: [-685.113 -685.113 -685.113] [180.7632], Avg: [-885.614 -885.614 -885.614] (1.000)
Step: 61349, Reward: [-776.726 -776.726 -776.726] [97.1289], Avg: [-885.605 -885.605 -885.605] (1.000)
Step: 61399, Reward: [-903.835 -903.835 -903.835] [140.2576], Avg: [-885.734 -885.734 -885.734] (1.000)
Step: 61449, Reward: [-1083.405 -1083.405 -1083.405] [92.1375], Avg: [-885.97 -885.97 -885.97] (1.000)
Step: 61499, Reward: [-1162.858 -1162.858 -1162.858] [95.0946], Avg: [-886.272 -886.272 -886.272] (1.000)
Step: 61549, Reward: [-1274.494 -1274.494 -1274.494] [44.5385], Avg: [-886.624 -886.624 -886.624] (1.000)
Step: 61599, Reward: [-1513.002 -1513.002 -1513.002] [221.0266], Avg: [-887.311 -887.311 -887.311] (1.000)
Step: 61649, Reward: [-1317.473 -1317.473 -1317.473] [90.5856], Avg: [-887.734 -887.734 -887.734] (1.000)
Step: 61699, Reward: [-1110.731 -1110.731 -1110.731] [102.8186], Avg: [-887.998 -887.998 -887.998] (1.000)
Step: 61749, Reward: [-1053.833 -1053.833 -1053.833] [55.0360], Avg: [-888.177 -888.177 -888.177] (1.000)
Step: 61799, Reward: [-997.14 -997.14 -997.14] [169.3511], Avg: [-888.402 -888.402 -888.402] (1.000)
Step: 61849, Reward: [-745.928 -745.928 -745.928] [88.3200], Avg: [-888.358 -888.358 -888.358] (1.000)
Step: 61899, Reward: [-770.966 -770.966 -770.966] [134.5900], Avg: [-888.372 -888.372 -888.372] (1.000)
Step: 61949, Reward: [-744.239 -744.239 -744.239] [107.1445], Avg: [-888.342 -888.342 -888.342] (1.000)
Step: 61999, Reward: [-625.024 -625.024 -625.024] [157.6898], Avg: [-888.257 -888.257 -888.257] (1.000)
Step: 62049, Reward: [-567.107 -567.107 -567.107] [44.2370], Avg: [-888.034 -888.034 -888.034] (1.000)
Step: 62099, Reward: [-532.016 -532.016 -532.016] [75.2121], Avg: [-887.808 -887.808 -887.808] (1.000)
Step: 62149, Reward: [-583.145 -583.145 -583.145] [205.8350], Avg: [-887.728 -887.728 -887.728] (1.000)
Step: 62199, Reward: [-471.461 -471.461 -471.461] [57.6887], Avg: [-887.44 -887.44 -887.44] (1.000)
Step: 62249, Reward: [-426.568 -426.568 -426.568] [46.3336], Avg: [-887.107 -887.107 -887.107] (1.000)
Step: 62299, Reward: [-437. -437. -437.] [37.2200], Avg: [-886.776 -886.776 -886.776] (1.000)
Step: 62349, Reward: [-509.211 -509.211 -509.211] [146.5159], Avg: [-886.59 -886.59 -886.59] (1.000)
Step: 62399, Reward: [-521.555 -521.555 -521.555] [133.7243], Avg: [-886.405 -886.405 -886.405] (1.000)
Step: 62449, Reward: [-437.712 -437.712 -437.712] [53.9637], Avg: [-886.089 -886.089 -886.089] (1.000)
Step: 62499, Reward: [-457.36 -457.36 -457.36] [85.0988], Avg: [-885.814 -885.814 -885.814] (1.000)
Step: 62549, Reward: [-456.995 -456.995 -456.995] [114.5302], Avg: [-885.563 -885.563 -885.563] (1.000)
Step: 62599, Reward: [-434.958 -434.958 -434.958] [44.6847], Avg: [-885.239 -885.239 -885.239] (1.000)
Step: 62649, Reward: [-435.198 -435.198 -435.198] [34.3285], Avg: [-884.907 -884.907 -884.907] (1.000)
Step: 62699, Reward: [-513.456 -513.456 -513.456] [181.0151], Avg: [-884.755 -884.755 -884.755] (1.000)
Step: 62749, Reward: [-459.486 -459.486 -459.486] [95.0430], Avg: [-884.492 -884.492 -884.492] (1.000)
Step: 62799, Reward: [-497.72 -497.72 -497.72] [100.8044], Avg: [-884.264 -884.264 -884.264] (1.000)
Step: 62849, Reward: [-480.404 -480.404 -480.404] [93.3366], Avg: [-884.017 -884.017 -884.017] (1.000)
Step: 62899, Reward: [-473.901 -473.901 -473.901] [153.9056], Avg: [-883.813 -883.813 -883.813] (1.000)
Step: 62949, Reward: [-497.569 -497.569 -497.569] [104.1467], Avg: [-883.589 -883.589 -883.589] (1.000)
Step: 62999, Reward: [-465.997 -465.997 -465.997] [87.3990], Avg: [-883.327 -883.327 -883.327] (1.000)
Step: 63049, Reward: [-556.696 -556.696 -556.696] [140.7060], Avg: [-883.18 -883.18 -883.18] (1.000)
Step: 63099, Reward: [-496.182 -496.182 -496.182] [88.1781], Avg: [-882.943 -882.943 -882.943] (1.000)
Step: 63149, Reward: [-488.163 -488.163 -488.163] [100.5884], Avg: [-882.71 -882.71 -882.71] (1.000)
Step: 63199, Reward: [-456.224 -456.224 -456.224] [51.8194], Avg: [-882.414 -882.414 -882.414] (1.000)
Step: 63249, Reward: [-480.497 -480.497 -480.497] [102.8895], Avg: [-882.177 -882.177 -882.177] (1.000)
Step: 63299, Reward: [-554.143 -554.143 -554.143] [81.8845], Avg: [-881.983 -881.983 -881.983] (1.000)
Step: 63349, Reward: [-551.276 -551.276 -551.276] [65.6810], Avg: [-881.774 -881.774 -881.774] (1.000)
Step: 63399, Reward: [-544.309 -544.309 -544.309] [140.1958], Avg: [-881.618 -881.618 -881.618] (1.000)
Step: 63449, Reward: [-436.26 -436.26 -436.26] [82.5108], Avg: [-881.332 -881.332 -881.332] (1.000)
Step: 63499, Reward: [-535.884 -535.884 -535.884] [62.2298], Avg: [-881.109 -881.109 -881.109] (1.000)
Step: 63549, Reward: [-407.811 -407.811 -407.811] [80.4119], Avg: [-880.8 -880.8 -880.8] (1.000)
Step: 63599, Reward: [-389.918 -389.918 -389.918] [69.0462], Avg: [-880.468 -880.468 -880.468] (1.000)
Step: 63649, Reward: [-525.019 -525.019 -525.019] [113.3066], Avg: [-880.278 -880.278 -880.278] (1.000)
Step: 63699, Reward: [-463.165 -463.165 -463.165] [67.2435], Avg: [-880.004 -880.004 -880.004] (1.000)
Step: 63749, Reward: [-496.99 -496.99 -496.99] [91.4933], Avg: [-879.775 -879.775 -879.775] (1.000)
Step: 63799, Reward: [-449.917 -449.917 -449.917] [64.5924], Avg: [-879.489 -879.489 -879.489] (1.000)
Step: 63849, Reward: [-470.756 -470.756 -470.756] [114.0021], Avg: [-879.258 -879.258 -879.258] (1.000)
Step: 63899, Reward: [-436.302 -436.302 -436.302] [104.4922], Avg: [-878.993 -878.993 -878.993] (1.000)
Step: 63949, Reward: [-378.641 -378.641 -378.641] [99.3718], Avg: [-878.68 -878.68 -878.68] (1.000)
Step: 63999, Reward: [-472.934 -472.934 -472.934] [130.8621], Avg: [-878.465 -878.465 -878.465] (1.000)
Step: 64049, Reward: [-446.013 -446.013 -446.013] [131.7711], Avg: [-878.23 -878.23 -878.23] (1.000)
Step: 64099, Reward: [-454.836 -454.836 -454.836] [48.7273], Avg: [-877.938 -877.938 -877.938] (1.000)
Step: 64149, Reward: [-410.319 -410.319 -410.319] [59.2480], Avg: [-877.62 -877.62 -877.62] (1.000)
Step: 64199, Reward: [-504.886 -504.886 -504.886] [118.0170], Avg: [-877.421 -877.421 -877.421] (1.000)
Step: 64249, Reward: [-589.676 -589.676 -589.676] [135.8670], Avg: [-877.303 -877.303 -877.303] (1.000)
Step: 64299, Reward: [-576.704 -576.704 -576.704] [106.7786], Avg: [-877.152 -877.152 -877.152] (1.000)
Step: 64349, Reward: [-554.74 -554.74 -554.74] [174.0692], Avg: [-877.037 -877.037 -877.037] (1.000)
Step: 64399, Reward: [-1162.682 -1162.682 -1162.682] [456.0702], Avg: [-877.613 -877.613 -877.613] (1.000)
Step: 64449, Reward: [-1698.905 -1698.905 -1698.905] [657.4661], Avg: [-878.76 -878.76 -878.76] (1.000)
Step: 64499, Reward: [-1683.147 -1683.147 -1683.147] [609.0133], Avg: [-879.856 -879.856 -879.856] (1.000)
Step: 64549, Reward: [-1656.294 -1656.294 -1656.294] [394.1988], Avg: [-880.762 -880.762 -880.762] (1.000)
Step: 64599, Reward: [-1406.558 -1406.558 -1406.558] [792.0740], Avg: [-881.782 -881.782 -881.782] (1.000)
Step: 64649, Reward: [-1186.286 -1186.286 -1186.286] [451.0126], Avg: [-882.367 -882.367 -882.367] (1.000)
Step: 64699, Reward: [-2044.895 -2044.895 -2044.895] [210.1663], Avg: [-883.428 -883.428 -883.428] (1.000)
Step: 64749, Reward: [-1326.116 -1326.116 -1326.116] [621.7611], Avg: [-884.25 -884.25 -884.25] (1.000)
Step: 64799, Reward: [-1357.912 -1357.912 -1357.912] [597.5422], Avg: [-885.076 -885.076 -885.076] (1.000)
Step: 64849, Reward: [-984.54 -984.54 -984.54] [297.4545], Avg: [-885.382 -885.382 -885.382] (1.000)
Step: 64899, Reward: [-972.489 -972.489 -972.489] [492.2059], Avg: [-885.828 -885.828 -885.828] (1.000)
Step: 64949, Reward: [-905.748 -905.748 -905.748] [307.4729], Avg: [-886.081 -886.081 -886.081] (1.000)
Step: 64999, Reward: [-1620.221 -1620.221 -1620.221] [370.9871], Avg: [-886.931 -886.931 -886.931] (1.000)
Step: 65049, Reward: [-1226.466 -1226.466 -1226.466] [261.5801], Avg: [-887.393 -887.393 -887.393] (1.000)
Step: 65099, Reward: [-968.496 -968.496 -968.496] [469.0104], Avg: [-887.815 -887.815 -887.815] (1.000)
Step: 65149, Reward: [-1266.012 -1266.012 -1266.012] [511.9746], Avg: [-888.498 -888.498 -888.498] (1.000)
Step: 65199, Reward: [-913.797 -913.797 -913.797] [261.4729], Avg: [-888.718 -888.718 -888.718] (1.000)
Step: 65249, Reward: [-1302.311 -1302.311 -1302.311] [344.2134], Avg: [-889.299 -889.299 -889.299] (1.000)
Step: 65299, Reward: [-1648.871 -1648.871 -1648.871] [214.2190], Avg: [-890.045 -890.045 -890.045] (1.000)
Step: 65349, Reward: [-1314.517 -1314.517 -1314.517] [488.5386], Avg: [-890.743 -890.743 -890.743] (1.000)
Step: 65399, Reward: [-1189.037 -1189.037 -1189.037] [427.1447], Avg: [-891.298 -891.298 -891.298] (1.000)
Step: 65449, Reward: [-1425.403 -1425.403 -1425.403] [399.7006], Avg: [-892.011 -892.011 -892.011] (1.000)
Step: 65499, Reward: [-1129.201 -1129.201 -1129.201] [272.8471], Avg: [-892.4 -892.4 -892.4] (1.000)
Step: 65549, Reward: [-1135.614 -1135.614 -1135.614] [206.4265], Avg: [-892.743 -892.743 -892.743] (1.000)
Step: 65599, Reward: [-1241.695 -1241.695 -1241.695] [167.2654], Avg: [-893.137 -893.137 -893.137] (1.000)
Step: 65649, Reward: [-1122.279 -1122.279 -1122.279] [324.1245], Avg: [-893.558 -893.558 -893.558] (1.000)
Step: 65699, Reward: [-892.154 -892.154 -892.154] [132.1670], Avg: [-893.658 -893.658 -893.658] (1.000)
Step: 65749, Reward: [-1310.559 -1310.559 -1310.559] [335.2928], Avg: [-894.23 -894.23 -894.23] (1.000)
Step: 65799, Reward: [-1147.578 -1147.578 -1147.578] [307.8595], Avg: [-894.656 -894.656 -894.656] (1.000)
Step: 65849, Reward: [-900.415 -900.415 -900.415] [73.7213], Avg: [-894.717 -894.717 -894.717] (1.000)
Step: 65899, Reward: [-1042.252 -1042.252 -1042.252] [231.7381], Avg: [-895.004 -895.004 -895.004] (1.000)
Step: 65949, Reward: [-834.791 -834.791 -834.791] [214.9477], Avg: [-895.122 -895.122 -895.122] (1.000)
Step: 65999, Reward: [-979.153 -979.153 -979.153] [292.3331], Avg: [-895.407 -895.407 -895.407] (1.000)
Step: 66049, Reward: [-875.739 -875.739 -875.739] [219.2326], Avg: [-895.558 -895.558 -895.558] (1.000)
Step: 66099, Reward: [-1065.017 -1065.017 -1065.017] [149.1074], Avg: [-895.799 -895.799 -895.799] (1.000)
Step: 66149, Reward: [-799.976 -799.976 -799.976] [202.8056], Avg: [-895.88 -895.88 -895.88] (1.000)
Step: 66199, Reward: [-749.385 -749.385 -749.385] [137.5944], Avg: [-895.873 -895.873 -895.873] (1.000)
Step: 66249, Reward: [-926.489 -926.489 -926.489] [272.1806], Avg: [-896.101 -896.101 -896.101] (1.000)
Step: 66299, Reward: [-997.984 -997.984 -997.984] [265.4467], Avg: [-896.379 -896.379 -896.379] (1.000)
Step: 66349, Reward: [-680.788 -680.788 -680.788] [199.3174], Avg: [-896.366 -896.366 -896.366] (1.000)
Step: 66399, Reward: [-652.856 -652.856 -652.856] [197.0348], Avg: [-896.331 -896.331 -896.331] (1.000)
Step: 66449, Reward: [-678.136 -678.136 -678.136] [211.3288], Avg: [-896.326 -896.326 -896.326] (1.000)
Step: 66499, Reward: [-781.948 -781.948 -781.948] [184.5940], Avg: [-896.379 -896.379 -896.379] (1.000)
Step: 66549, Reward: [-581.263 -581.263 -581.263] [213.7301], Avg: [-896.303 -896.303 -896.303] (1.000)
Step: 66599, Reward: [-515.217 -515.217 -515.217] [107.6825], Avg: [-896.097 -896.097 -896.097] (1.000)
Step: 66649, Reward: [-748.766 -748.766 -748.766] [354.8510], Avg: [-896.253 -896.253 -896.253] (1.000)
Step: 66699, Reward: [-782.849 -782.849 -782.849] [255.7308], Avg: [-896.36 -896.36 -896.36] (1.000)
Step: 66749, Reward: [-473.842 -473.842 -473.842] [128.4376], Avg: [-896.14 -896.14 -896.14] (1.000)
Step: 66799, Reward: [-634.733 -634.733 -634.733] [177.1926], Avg: [-896.077 -896.077 -896.077] (1.000)
Step: 66849, Reward: [-549.185 -549.185 -549.185] [177.8659], Avg: [-895.95 -895.95 -895.95] (1.000)
Step: 66899, Reward: [-502.152 -502.152 -502.152] [80.0860], Avg: [-895.716 -895.716 -895.716] (1.000)
Step: 66949, Reward: [-471.768 -471.768 -471.768] [126.4218], Avg: [-895.493 -895.493 -895.493] (1.000)
Step: 66999, Reward: [-452.493 -452.493 -452.493] [98.5392], Avg: [-895.236 -895.236 -895.236] (1.000)
Step: 67049, Reward: [-428.425 -428.425 -428.425] [36.9034], Avg: [-894.916 -894.916 -894.916] (1.000)
Step: 67099, Reward: [-425.265 -425.265 -425.265] [99.4411], Avg: [-894.64 -894.64 -894.64] (1.000)
Step: 67149, Reward: [-570.675 -570.675 -570.675] [64.2434], Avg: [-894.447 -894.447 -894.447] (1.000)
Step: 67199, Reward: [-450.921 -450.921 -450.921] [107.4854], Avg: [-894.196 -894.196 -894.196] (1.000)
Step: 67249, Reward: [-429.073 -429.073 -429.073] [125.9999], Avg: [-893.944 -893.944 -893.944] (1.000)
Step: 67299, Reward: [-527.428 -527.428 -527.428] [136.5300], Avg: [-893.773 -893.773 -893.773] (1.000)
Step: 67349, Reward: [-477.299 -477.299 -477.299] [130.3498], Avg: [-893.561 -893.561 -893.561] (1.000)
Step: 67399, Reward: [-491.705 -491.705 -491.705] [128.1341], Avg: [-893.358 -893.358 -893.358] (1.000)
Step: 67449, Reward: [-447.126 -447.126 -447.126] [66.8874], Avg: [-893.077 -893.077 -893.077] (1.000)
Step: 67499, Reward: [-436.129 -436.129 -436.129] [55.8621], Avg: [-892.78 -892.78 -892.78] (1.000)
Step: 67549, Reward: [-426.415 -426.415 -426.415] [90.5049], Avg: [-892.501 -892.501 -892.501] (1.000)
Step: 67599, Reward: [-490.419 -490.419 -490.419] [102.9530], Avg: [-892.28 -892.28 -892.28] (1.000)
Step: 67649, Reward: [-546.153 -546.153 -546.153] [111.8836], Avg: [-892.107 -892.107 -892.107] (1.000)
Step: 67699, Reward: [-548.122 -548.122 -548.122] [177.9826], Avg: [-891.985 -891.985 -891.985] (1.000)
Step: 67749, Reward: [-400.069 -400.069 -400.069] [102.6852], Avg: [-891.697 -891.697 -891.697] (1.000)
Step: 67799, Reward: [-459.078 -459.078 -459.078] [61.0769], Avg: [-891.423 -891.423 -891.423] (1.000)
Step: 67849, Reward: [-719.967 -719.967 -719.967] [313.8228], Avg: [-891.528 -891.528 -891.528] (1.000)
Step: 67899, Reward: [-798.558 -798.558 -798.558] [173.4715], Avg: [-891.587 -891.587 -891.587] (1.000)
Step: 67949, Reward: [-449.679 -449.679 -449.679] [131.0964], Avg: [-891.359 -891.359 -891.359] (1.000)
Step: 67999, Reward: [-590.299 -590.299 -590.299] [186.9990], Avg: [-891.275 -891.275 -891.275] (1.000)
Step: 68049, Reward: [-573.297 -573.297 -573.297] [176.8202], Avg: [-891.171 -891.171 -891.171] (1.000)
Step: 68099, Reward: [-954.193 -954.193 -954.193] [299.8072], Avg: [-891.438 -891.438 -891.438] (1.000)
Step: 68149, Reward: [-719.311 -719.311 -719.311] [396.1028], Avg: [-891.602 -891.602 -891.602] (1.000)
Step: 68199, Reward: [-1273.099 -1273.099 -1273.099] [206.8438], Avg: [-892.033 -892.033 -892.033] (1.000)
Step: 68249, Reward: [-964.848 -964.848 -964.848] [579.8843], Avg: [-892.511 -892.511 -892.511] (1.000)
Step: 68299, Reward: [-1213.358 -1213.358 -1213.358] [291.8807], Avg: [-892.96 -892.96 -892.96] (1.000)
Step: 68349, Reward: [-846.541 -846.541 -846.541] [368.1251], Avg: [-893.195 -893.195 -893.195] (1.000)
Step: 68399, Reward: [-1028.585 -1028.585 -1028.585] [484.5481], Avg: [-893.648 -893.648 -893.648] (1.000)
Step: 68449, Reward: [-1024.388 -1024.388 -1024.388] [382.1952], Avg: [-894.023 -894.023 -894.023] (1.000)
Step: 68499, Reward: [-948.489 -948.489 -948.489] [315.0603], Avg: [-894.293 -894.293 -894.293] (1.000)
Step: 68549, Reward: [-711.159 -711.159 -711.159] [180.0953], Avg: [-894.291 -894.291 -894.291] (1.000)
Step: 68599, Reward: [-610.393 -610.393 -610.393] [268.9287], Avg: [-894.28 -894.28 -894.28] (1.000)
Step: 68649, Reward: [-1197.457 -1197.457 -1197.457] [338.2236], Avg: [-894.747 -894.747 -894.747] (1.000)
Step: 68699, Reward: [-1039.758 -1039.758 -1039.758] [503.5906], Avg: [-895.219 -895.219 -895.219] (1.000)
Step: 68749, Reward: [-899.225 -899.225 -899.225] [287.6258], Avg: [-895.431 -895.431 -895.431] (1.000)
Step: 68799, Reward: [-805.089 -805.089 -805.089] [329.3260], Avg: [-895.605 -895.605 -895.605] (1.000)
Step: 68849, Reward: [-1156.687 -1156.687 -1156.687] [473.6885], Avg: [-896.138 -896.138 -896.138] (1.000)
Step: 68899, Reward: [-1038.973 -1038.973 -1038.973] [277.1577], Avg: [-896.443 -896.443 -896.443] (1.000)
Step: 68949, Reward: [-830.119 -830.119 -830.119] [259.0054], Avg: [-896.583 -896.583 -896.583] (1.000)
Step: 68999, Reward: [-1437.449 -1437.449 -1437.449] [283.4245], Avg: [-897.18 -897.18 -897.18] (1.000)
Step: 69049, Reward: [-1225.672 -1225.672 -1225.672] [360.1584], Avg: [-897.679 -897.679 -897.679] (1.000)
Step: 69099, Reward: [-1645.159 -1645.159 -1645.159] [136.0874], Avg: [-898.318 -898.318 -898.318] (1.000)
Step: 69149, Reward: [-1157.386 -1157.386 -1157.386] [307.4532], Avg: [-898.728 -898.728 -898.728] (1.000)
Step: 69199, Reward: [-1214.107 -1214.107 -1214.107] [336.0741], Avg: [-899.198 -899.198 -899.198] (1.000)
Step: 69249, Reward: [-1699.059 -1699.059 -1699.059] [507.7851], Avg: [-900.143 -900.143 -900.143] (1.000)
Step: 69299, Reward: [-1724.117 -1724.117 -1724.117] [340.6034], Avg: [-900.983 -900.983 -900.983] (1.000)
Step: 69349, Reward: [-2093.711 -2093.711 -2093.711] [268.7266], Avg: [-902.037 -902.037 -902.037] (1.000)
Step: 69399, Reward: [-1433.919 -1433.919 -1433.919] [495.8419], Avg: [-902.777 -902.777 -902.777] (1.000)
Step: 69449, Reward: [-1293.358 -1293.358 -1293.358] [258.1611], Avg: [-903.244 -903.244 -903.244] (1.000)
Step: 69499, Reward: [-969.095 -969.095 -969.095] [314.9994], Avg: [-903.518 -903.518 -903.518] (1.000)
Step: 69549, Reward: [-819.873 -819.873 -819.873] [349.4089], Avg: [-903.709 -903.709 -903.709] (1.000)
Step: 69599, Reward: [-688.584 -688.584 -688.584] [163.1272], Avg: [-903.672 -903.672 -903.672] (1.000)
Step: 69649, Reward: [-1138.154 -1138.154 -1138.154] [292.7103], Avg: [-904.05 -904.05 -904.05] (1.000)
Step: 69699, Reward: [-992.673 -992.673 -992.673] [326.0039], Avg: [-904.348 -904.348 -904.348] (1.000)
Step: 69749, Reward: [-822.53 -822.53 -822.53] [131.5226], Avg: [-904.383 -904.383 -904.383] (1.000)
Step: 69799, Reward: [-737.961 -737.961 -737.961] [308.8391], Avg: [-904.485 -904.485 -904.485] (1.000)
Step: 69849, Reward: [-635.538 -635.538 -635.538] [276.7774], Avg: [-904.491 -904.491 -904.491] (1.000)
Step: 69899, Reward: [-924.069 -924.069 -924.069] [510.3619], Avg: [-904.87 -904.87 -904.87] (1.000)
Step: 69949, Reward: [-1058.669 -1058.669 -1058.669] [483.3843], Avg: [-905.325 -905.325 -905.325] (1.000)
Step: 69999, Reward: [-603.07 -603.07 -603.07] [164.7803], Avg: [-905.227 -905.227 -905.227] (1.000)
Step: 70049, Reward: [-1043.978 -1043.978 -1043.978] [360.1998], Avg: [-905.583 -905.583 -905.583] (1.000)
Step: 70099, Reward: [-671.074 -671.074 -671.074] [455.4654], Avg: [-905.741 -905.741 -905.741] (1.000)
Step: 70149, Reward: [-726.333 -726.333 -726.333] [379.4313], Avg: [-905.884 -905.884 -905.884] (1.000)
Step: 70199, Reward: [-455.267 -455.267 -455.267] [125.4698], Avg: [-905.652 -905.652 -905.652] (1.000)
Step: 70249, Reward: [-993.427 -993.427 -993.427] [537.1945], Avg: [-906.097 -906.097 -906.097] (1.000)
Step: 70299, Reward: [-713.678 -713.678 -713.678] [472.8380], Avg: [-906.296 -906.296 -906.296] (1.000)
Step: 70349, Reward: [-713.347 -713.347 -713.347] [249.5319], Avg: [-906.336 -906.336 -906.336] (1.000)
Step: 70399, Reward: [-668.923 -668.923 -668.923] [199.6929], Avg: [-906.31 -906.31 -906.31] (1.000)
Step: 70449, Reward: [-526.041 -526.041 -526.041] [166.5261], Avg: [-906.158 -906.158 -906.158] (1.000)
Step: 70499, Reward: [-556.346 -556.346 -556.346] [157.7684], Avg: [-906.022 -906.022 -906.022] (1.000)
Step: 70549, Reward: [-532.572 -532.572 -532.572] [117.9051], Avg: [-905.841 -905.841 -905.841] (1.000)
Step: 70599, Reward: [-700.436 -700.436 -700.436] [406.6366], Avg: [-905.983 -905.983 -905.983] (1.000)
Step: 70649, Reward: [-396.995 -396.995 -396.995] [74.1554], Avg: [-905.675 -905.675 -905.675] (1.000)
Step: 70699, Reward: [-578.85 -578.85 -578.85] [124.3786], Avg: [-905.532 -905.532 -905.532] (1.000)
Step: 70749, Reward: [-626.185 -626.185 -626.185] [213.0472], Avg: [-905.485 -905.485 -905.485] (1.000)
Step: 70799, Reward: [-510.689 -510.689 -510.689] [107.9511], Avg: [-905.283 -905.283 -905.283] (1.000)
Step: 70849, Reward: [-510.309 -510.309 -510.309] [88.3646], Avg: [-905.066 -905.066 -905.066] (1.000)
Step: 70899, Reward: [-565.635 -565.635 -565.635] [117.3077], Avg: [-904.91 -904.91 -904.91] (1.000)
Step: 70949, Reward: [-805.494 -805.494 -805.494] [178.5898], Avg: [-904.966 -904.966 -904.966] (1.000)
Step: 70999, Reward: [-574.542 -574.542 -574.542] [70.1948], Avg: [-904.782 -904.782 -904.782] (1.000)
Step: 71049, Reward: [-835.617 -835.617 -835.617] [387.5928], Avg: [-905.006 -905.006 -905.006] (1.000)
Step: 71099, Reward: [-667.898 -667.898 -667.898] [186.6436], Avg: [-904.971 -904.971 -904.971] (1.000)
Step: 71149, Reward: [-576.358 -576.358 -576.358] [44.6986], Avg: [-904.771 -904.771 -904.771] (1.000)
Step: 71199, Reward: [-902.549 -902.549 -902.549] [594.6012], Avg: [-905.187 -905.187 -905.187] (1.000)
Step: 71249, Reward: [-848.162 -848.162 -848.162] [315.4112], Avg: [-905.369 -905.369 -905.369] (1.000)
Step: 71299, Reward: [-813.448 -813.448 -813.448] [542.6234], Avg: [-905.685 -905.685 -905.685] (1.000)
Step: 71349, Reward: [-650.8 -650.8 -650.8] [194.6345], Avg: [-905.643 -905.643 -905.643] (1.000)
Step: 71399, Reward: [-614.728 -614.728 -614.728] [145.9947], Avg: [-905.541 -905.541 -905.541] (1.000)
Step: 71449, Reward: [-436.109 -436.109 -436.109] [82.0693], Avg: [-905.27 -905.27 -905.27] (1.000)
Step: 71499, Reward: [-746.017 -746.017 -746.017] [399.5147], Avg: [-905.438 -905.438 -905.438] (1.000)
Step: 71549, Reward: [-597.298 -597.298 -597.298] [194.3483], Avg: [-905.358 -905.358 -905.358] (1.000)
Step: 71599, Reward: [-566.735 -566.735 -566.735] [202.1069], Avg: [-905.263 -905.263 -905.263] (1.000)
Step: 71649, Reward: [-605.031 -605.031 -605.031] [86.8646], Avg: [-905.114 -905.114 -905.114] (1.000)
Step: 71699, Reward: [-863.225 -863.225 -863.225] [223.0969], Avg: [-905.241 -905.241 -905.241] (1.000)
Step: 71749, Reward: [-793.896 -793.896 -793.896] [425.4657], Avg: [-905.46 -905.46 -905.46] (1.000)
Step: 71799, Reward: [-528.739 -528.739 -528.739] [121.8332], Avg: [-905.282 -905.282 -905.282] (1.000)
Step: 71849, Reward: [-973.629 -973.629 -973.629] [233.9624], Avg: [-905.492 -905.492 -905.492] (1.000)
Step: 71899, Reward: [-994.219 -994.219 -994.219] [469.4195], Avg: [-905.881 -905.881 -905.881] (1.000)
Step: 71949, Reward: [-453.54 -453.54 -453.54] [78.1783], Avg: [-905.621 -905.621 -905.621] (1.000)
Step: 71999, Reward: [-633.221 -633.221 -633.221] [205.0029], Avg: [-905.574 -905.574 -905.574] (1.000)
Step: 72049, Reward: [-512.733 -512.733 -512.733] [126.3668], Avg: [-905.389 -905.389 -905.389] (1.000)
Step: 72099, Reward: [-446.129 -446.129 -446.129] [107.6753], Avg: [-905.145 -905.145 -905.145] (1.000)
Step: 72149, Reward: [-454.433 -454.433 -454.433] [82.7265], Avg: [-904.89 -904.89 -904.89] (1.000)
Step: 72199, Reward: [-529.596 -529.596 -529.596] [112.0093], Avg: [-904.708 -904.708 -904.708] (1.000)
Step: 72249, Reward: [-525.077 -525.077 -525.077] [138.6762], Avg: [-904.541 -904.541 -904.541] (1.000)
Step: 72299, Reward: [-489.632 -489.632 -489.632] [148.9808], Avg: [-904.357 -904.357 -904.357] (1.000)
Step: 72349, Reward: [-613.912 -613.912 -613.912] [160.3317], Avg: [-904.267 -904.267 -904.267] (1.000)
Step: 72399, Reward: [-614.171 -614.171 -614.171] [110.7753], Avg: [-904.143 -904.143 -904.143] (1.000)
Step: 72449, Reward: [-681.166 -681.166 -681.166] [178.2316], Avg: [-904.112 -904.112 -904.112] (1.000)
Step: 72499, Reward: [-628.289 -628.289 -628.289] [185.0983], Avg: [-904.05 -904.05 -904.05] (1.000)
Step: 72549, Reward: [-506.325 -506.325 -506.325] [64.5680], Avg: [-903.82 -903.82 -903.82] (1.000)
Step: 72599, Reward: [-496.921 -496.921 -496.921] [76.1603], Avg: [-903.592 -903.592 -903.592] (1.000)
Step: 72649, Reward: [-571.136 -571.136 -571.136] [173.7747], Avg: [-903.483 -903.483 -903.483] (1.000)
Step: 72699, Reward: [-733.574 -733.574 -733.574] [251.3695], Avg: [-903.539 -903.539 -903.539] (1.000)
Step: 72749, Reward: [-712.781 -712.781 -712.781] [153.7201], Avg: [-903.514 -903.514 -903.514] (1.000)
Step: 72799, Reward: [-753.16 -753.16 -753.16] [310.1433], Avg: [-903.623 -903.623 -903.623] (1.000)
Step: 72849, Reward: [-773.029 -773.029 -773.029] [187.6328], Avg: [-903.663 -903.663 -903.663] (1.000)
Step: 72899, Reward: [-695.136 -695.136 -695.136] [119.2699], Avg: [-903.601 -903.601 -903.601] (1.000)
Step: 72949, Reward: [-645.605 -645.605 -645.605] [135.9948], Avg: [-903.518 -903.518 -903.518] (1.000)
Step: 72999, Reward: [-647.993 -647.993 -647.993] [173.9122], Avg: [-903.462 -903.462 -903.462] (1.000)
Step: 73049, Reward: [-679.433 -679.433 -679.433] [130.4028], Avg: [-903.398 -903.398 -903.398] (1.000)
Step: 73099, Reward: [-614.886 -614.886 -614.886] [158.8009], Avg: [-903.309 -903.309 -903.309] (1.000)
Step: 73149, Reward: [-483.76 -483.76 -483.76] [97.1506], Avg: [-903.089 -903.089 -903.089] (1.000)
Step: 73199, Reward: [-602.462 -602.462 -602.462] [152.4494], Avg: [-902.988 -902.988 -902.988] (1.000)
Step: 73249, Reward: [-510.316 -510.316 -510.316] [76.9436], Avg: [-902.772 -902.772 -902.772] (1.000)
Step: 73299, Reward: [-485.688 -485.688 -485.688] [65.8446], Avg: [-902.532 -902.532 -902.532] (1.000)
Step: 73349, Reward: [-721.871 -721.871 -721.871] [176.7120], Avg: [-902.53 -902.53 -902.53] (1.000)
Step: 73399, Reward: [-790.914 -790.914 -790.914] [243.3287], Avg: [-902.619 -902.619 -902.619] (1.000)
Step: 73449, Reward: [-891.495 -891.495 -891.495] [171.7016], Avg: [-902.729 -902.729 -902.729] (1.000)
Step: 73499, Reward: [-900.222 -900.222 -900.222] [141.2942], Avg: [-902.823 -902.823 -902.823] (1.000)
Step: 73549, Reward: [-939.739 -939.739 -939.739] [157.1337], Avg: [-902.955 -902.955 -902.955] (1.000)
Step: 73599, Reward: [-876.947 -876.947 -876.947] [201.6358], Avg: [-903.074 -903.074 -903.074] (1.000)
Step: 73649, Reward: [-1015.062 -1015.062 -1015.062] [284.8532], Avg: [-903.344 -903.344 -903.344] (1.000)
Step: 73699, Reward: [-1253.666 -1253.666 -1253.666] [137.7668], Avg: [-903.675 -903.675 -903.675] (1.000)
Step: 73749, Reward: [-1132.085 -1132.085 -1132.085] [189.6081], Avg: [-903.958 -903.958 -903.958] (1.000)
Step: 73799, Reward: [-1231.951 -1231.951 -1231.951] [235.7125], Avg: [-904.34 -904.34 -904.34] (1.000)
Step: 73849, Reward: [-852.092 -852.092 -852.092] [225.7643], Avg: [-904.458 -904.458 -904.458] (1.000)
Step: 73899, Reward: [-1021.593 -1021.593 -1021.593] [213.6067], Avg: [-904.682 -904.682 -904.682] (1.000)
Step: 73949, Reward: [-879.521 -879.521 -879.521] [103.2402], Avg: [-904.734 -904.734 -904.734] (1.000)
Step: 73999, Reward: [-1091.064 -1091.064 -1091.064] [205.1091], Avg: [-904.999 -904.999 -904.999] (1.000)
Step: 74049, Reward: [-832.486 -832.486 -832.486] [239.5116], Avg: [-905.112 -905.112 -905.112] (1.000)
Step: 74099, Reward: [-1071.984 -1071.984 -1071.984] [308.9613], Avg: [-905.433 -905.433 -905.433] (1.000)
Step: 74149, Reward: [-570.777 -570.777 -570.777] [125.0265], Avg: [-905.291 -905.291 -905.291] (1.000)
Step: 74199, Reward: [-808.711 -808.711 -808.711] [201.2391], Avg: [-905.362 -905.362 -905.362] (1.000)
Step: 74249, Reward: [-1244.842 -1244.842 -1244.842] [751.5720], Avg: [-906.097 -906.097 -906.097] (1.000)
Step: 74299, Reward: [-792.949 -792.949 -792.949] [339.6879], Avg: [-906.249 -906.249 -906.249] (1.000)
Step: 74349, Reward: [-977.585 -977.585 -977.585] [601.2124], Avg: [-906.701 -906.701 -906.701] (1.000)
Step: 74399, Reward: [-549.361 -549.361 -549.361] [231.6166], Avg: [-906.617 -906.617 -906.617] (1.000)
Step: 74449, Reward: [-720.623 -720.623 -720.623] [192.3249], Avg: [-906.621 -906.621 -906.621] (1.000)
Step: 74499, Reward: [-929.64 -929.64 -929.64] [366.2366], Avg: [-906.882 -906.882 -906.882] (1.000)
Step: 74549, Reward: [-460.433 -460.433 -460.433] [45.2548], Avg: [-906.613 -906.613 -906.613] (1.000)
Step: 74599, Reward: [-569.453 -569.453 -569.453] [214.7791], Avg: [-906.531 -906.531 -906.531] (1.000)
Step: 74649, Reward: [-1287.478 -1287.478 -1287.478] [326.8746], Avg: [-907.005 -907.005 -907.005] (1.000)
Step: 74699, Reward: [-1044.162 -1044.162 -1044.162] [355.2555], Avg: [-907.335 -907.335 -907.335] (1.000)
Step: 74749, Reward: [-761.762 -761.762 -761.762] [372.1699], Avg: [-907.486 -907.486 -907.486] (1.000)
Step: 74799, Reward: [-1561.507 -1561.507 -1561.507] [103.2941], Avg: [-907.993 -907.993 -907.993] (1.000)
Step: 74849, Reward: [-1329.511 -1329.511 -1329.511] [416.4826], Avg: [-908.552 -908.552 -908.552] (1.000)
Step: 74899, Reward: [-1494.473 -1494.473 -1494.473] [117.9494], Avg: [-909.022 -909.022 -909.022] (1.000)
Step: 74949, Reward: [-1579.842 -1579.842 -1579.842] [214.6320], Avg: [-909.613 -909.613 -909.613] (1.000)
Step: 74999, Reward: [-966.426 -966.426 -966.426] [420.7093], Avg: [-909.931 -909.931 -909.931] (1.000)
Step: 75049, Reward: [-1488.086 -1488.086 -1488.086] [142.2437], Avg: [-910.411 -910.411 -910.411] (1.000)
Step: 75099, Reward: [-778.275 -778.275 -778.275] [130.8210], Avg: [-910.41 -910.41 -910.41] (1.000)
Step: 75149, Reward: [-1254.06 -1254.06 -1254.06] [270.7184], Avg: [-910.819 -910.819 -910.819] (1.000)
Step: 75199, Reward: [-1537.205 -1537.205 -1537.205] [220.7830], Avg: [-911.382 -911.382 -911.382] (1.000)
Step: 75249, Reward: [-1127.114 -1127.114 -1127.114] [251.9975], Avg: [-911.693 -911.693 -911.693] (1.000)
Step: 75299, Reward: [-1386.89 -1386.89 -1386.89] [540.3600], Avg: [-912.368 -912.368 -912.368] (1.000)
Step: 75349, Reward: [-910.72 -910.72 -910.72] [405.5462], Avg: [-912.636 -912.636 -912.636] (1.000)
Step: 75399, Reward: [-1134.059 -1134.059 -1134.059] [407.6836], Avg: [-913.053 -913.053 -913.053] (1.000)
Step: 75449, Reward: [-1562.822 -1562.822 -1562.822] [448.2932], Avg: [-913.78 -913.78 -913.78] (1.000)
Step: 75499, Reward: [-1508.781 -1508.781 -1508.781] [264.8713], Avg: [-914.35 -914.35 -914.35] (1.000)
Step: 75549, Reward: [-1571.566 -1571.566 -1571.566] [470.5435], Avg: [-915.096 -915.096 -915.096] (1.000)
Step: 75599, Reward: [-1487.028 -1487.028 -1487.028] [277.9315], Avg: [-915.658 -915.658 -915.658] (1.000)
Step: 75649, Reward: [-1181.546 -1181.546 -1181.546] [383.8981], Avg: [-916.088 -916.088 -916.088] (1.000)
Step: 75699, Reward: [-868.637 -868.637 -868.637] [221.9829], Avg: [-916.203 -916.203 -916.203] (1.000)
Step: 75749, Reward: [-836.9 -836.9 -836.9] [273.9990], Avg: [-916.332 -916.332 -916.332] (1.000)
Step: 75799, Reward: [-1073.792 -1073.792 -1073.792] [411.7764], Avg: [-916.707 -916.707 -916.707] (1.000)
Step: 75849, Reward: [-833.366 -833.366 -833.366] [214.2133], Avg: [-916.793 -916.793 -916.793] (1.000)
Step: 75899, Reward: [-1153.03 -1153.03 -1153.03] [536.0012], Avg: [-917.302 -917.302 -917.302] (1.000)
Step: 75949, Reward: [-817.505 -817.505 -817.505] [127.8905], Avg: [-917.321 -917.321 -917.321] (1.000)
Step: 75999, Reward: [-1004.106 -1004.106 -1004.106] [276.3464], Avg: [-917.559 -917.559 -917.559] (1.000)
Step: 76049, Reward: [-1003.306 -1003.306 -1003.306] [372.0413], Avg: [-917.86 -917.86 -917.86] (1.000)
Step: 76099, Reward: [-821.11 -821.11 -821.11] [372.5703], Avg: [-918.042 -918.042 -918.042] (1.000)
Step: 76149, Reward: [-777.979 -777.979 -777.979] [322.6074], Avg: [-918.162 -918.162 -918.162] (1.000)
Step: 76199, Reward: [-896.411 -896.411 -896.411] [440.8800], Avg: [-918.437 -918.437 -918.437] (1.000)
Step: 76249, Reward: [-1409.323 -1409.323 -1409.323] [364.5354], Avg: [-918.997 -918.997 -918.997] (1.000)
Step: 76299, Reward: [-1030.556 -1030.556 -1030.556] [335.4337], Avg: [-919.29 -919.29 -919.29] (1.000)
Step: 76349, Reward: [-826.233 -826.233 -826.233] [251.1047], Avg: [-919.394 -919.394 -919.394] (1.000)
Step: 76399, Reward: [-936.329 -936.329 -936.329] [422.4579], Avg: [-919.681 -919.681 -919.681] (1.000)
Step: 76449, Reward: [-1179.585 -1179.585 -1179.585] [419.5552], Avg: [-920.126 -920.126 -920.126] (1.000)
Step: 76499, Reward: [-1218.699 -1218.699 -1218.699] [539.4134], Avg: [-920.674 -920.674 -920.674] (1.000)
Step: 76549, Reward: [-1009.037 -1009.037 -1009.037] [541.5347], Avg: [-921.085 -921.085 -921.085] (1.000)
Step: 76599, Reward: [-1276.72 -1276.72 -1276.72] [564.9060], Avg: [-921.686 -921.686 -921.686] (1.000)
Step: 76649, Reward: [-977.808 -977.808 -977.808] [200.2843], Avg: [-921.853 -921.853 -921.853] (1.000)
Step: 76699, Reward: [-1316.542 -1316.542 -1316.542] [277.5709], Avg: [-922.291 -922.291 -922.291] (1.000)
Step: 76749, Reward: [-1596.775 -1596.775 -1596.775] [335.3051], Avg: [-922.949 -922.949 -922.949] (1.000)
Step: 76799, Reward: [-1451.427 -1451.427 -1451.427] [190.5692], Avg: [-923.417 -923.417 -923.417] (1.000)
Step: 76849, Reward: [-1188.302 -1188.302 -1188.302] [543.5213], Avg: [-923.943 -923.943 -923.943] (1.000)
Step: 76899, Reward: [-1309.885 -1309.885 -1309.885] [337.1198], Avg: [-924.413 -924.413 -924.413] (1.000)
Step: 76949, Reward: [-1324.539 -1324.539 -1324.539] [329.1546], Avg: [-924.887 -924.887 -924.887] (1.000)
Step: 76999, Reward: [-1425.665 -1425.665 -1425.665] [500.9803], Avg: [-925.538 -925.538 -925.538] (1.000)
Step: 77049, Reward: [-1041.178 -1041.178 -1041.178] [465.4682], Avg: [-925.915 -925.915 -925.915] (1.000)
Step: 77099, Reward: [-1705.11 -1705.11 -1705.11] [261.8400], Avg: [-926.59 -926.59 -926.59] (1.000)
Step: 77149, Reward: [-1311.062 -1311.062 -1311.062] [280.4762], Avg: [-927.021 -927.021 -927.021] (1.000)
Step: 77199, Reward: [-1313.355 -1313.355 -1313.355] [336.1677], Avg: [-927.489 -927.489 -927.489] (1.000)
Step: 77249, Reward: [-970.949 -970.949 -970.949] [94.7036], Avg: [-927.578 -927.578 -927.578] (1.000)
Step: 77299, Reward: [-1210.891 -1210.891 -1210.891] [401.4143], Avg: [-928.021 -928.021 -928.021] (1.000)
Step: 77349, Reward: [-1417.238 -1417.238 -1417.238] [375.6188], Avg: [-928.58 -928.58 -928.58] (1.000)
Step: 77399, Reward: [-1305.358 -1305.358 -1305.358] [341.4525], Avg: [-929.044 -929.044 -929.044] (1.000)
Step: 77449, Reward: [-1164.945 -1164.945 -1164.945] [311.6620], Avg: [-929.398 -929.398 -929.398] (1.000)
Step: 77499, Reward: [-1419.608 -1419.608 -1419.608] [175.4726], Avg: [-929.827 -929.827 -929.827] (1.000)
Step: 77549, Reward: [-1341.897 -1341.897 -1341.897] [202.5037], Avg: [-930.223 -930.223 -930.223] (1.000)
Step: 77599, Reward: [-669.777 -669.777 -669.777] [169.3717], Avg: [-930.165 -930.165 -930.165] (1.000)
Step: 77649, Reward: [-854.749 -854.749 -854.749] [229.0696], Avg: [-930.264 -930.264 -930.264] (1.000)
Step: 77699, Reward: [-1014.041 -1014.041 -1014.041] [300.9756], Avg: [-930.511 -930.511 -930.511] (1.000)
Step: 77749, Reward: [-973.284 -973.284 -973.284] [451.4194], Avg: [-930.829 -930.829 -930.829] (1.000)
Step: 77799, Reward: [-511.355 -511.355 -511.355] [120.8290], Avg: [-930.637 -930.637 -930.637] (1.000)
Step: 77849, Reward: [-603.319 -603.319 -603.319] [414.2708], Avg: [-930.693 -930.693 -930.693] (1.000)
Step: 77899, Reward: [-502.658 -502.658 -502.658] [183.9411], Avg: [-930.536 -930.536 -930.536] (1.000)
Step: 77949, Reward: [-566.477 -566.477 -566.477] [121.2263], Avg: [-930.381 -930.381 -930.381] (1.000)
Step: 77999, Reward: [-805.323 -805.323 -805.323] [337.8867], Avg: [-930.517 -930.517 -930.517] (1.000)
Step: 78049, Reward: [-540.99 -540.99 -540.99] [256.0155], Avg: [-930.431 -930.431 -930.431] (1.000)
Step: 78099, Reward: [-599.115 -599.115 -599.115] [199.4985], Avg: [-930.347 -930.347 -930.347] (1.000)
Step: 78149, Reward: [-521.494 -521.494 -521.494] [135.6479], Avg: [-930.172 -930.172 -930.172] (1.000)
Step: 78199, Reward: [-564.691 -564.691 -564.691] [283.0061], Avg: [-930.12 -930.12 -930.12] (1.000)
Step: 78249, Reward: [-568.734 -568.734 -568.734] [170.2098], Avg: [-929.997 -929.997 -929.997] (1.000)
Step: 78299, Reward: [-789.769 -789.769 -789.769] [237.9609], Avg: [-930.06 -930.06 -930.06] (1.000)
Step: 78349, Reward: [-582.402 -582.402 -582.402] [93.4056], Avg: [-929.898 -929.898 -929.898] (1.000)
Step: 78399, Reward: [-1269.696 -1269.696 -1269.696] [418.9196], Avg: [-930.381 -930.381 -930.381] (1.000)
Step: 78449, Reward: [-1028.867 -1028.867 -1028.867] [294.1839], Avg: [-930.632 -930.632 -930.632] (1.000)
Step: 78499, Reward: [-767.933 -767.933 -767.933] [217.4486], Avg: [-930.667 -930.667 -930.667] (1.000)
Step: 78549, Reward: [-638.898 -638.898 -638.898] [81.5998], Avg: [-930.533 -930.533 -930.533] (1.000)
Step: 78599, Reward: [-1142.342 -1142.342 -1142.342] [348.7150], Avg: [-930.889 -930.889 -930.889] (1.000)
Step: 78649, Reward: [-1097.26 -1097.26 -1097.26] [345.9755], Avg: [-931.215 -931.215 -931.215] (1.000)
Step: 78699, Reward: [-1008.335 -1008.335 -1008.335] [314.4419], Avg: [-931.464 -931.464 -931.464] (1.000)
Step: 78749, Reward: [-1080.548 -1080.548 -1080.548] [377.7012], Avg: [-931.798 -931.798 -931.798] (1.000)
Step: 78799, Reward: [-906.325 -906.325 -906.325] [344.2990], Avg: [-932.001 -932.001 -932.001] (1.000)
Step: 78849, Reward: [-1369.733 -1369.733 -1369.733] [312.1361], Avg: [-932.476 -932.476 -932.476] (1.000)
Step: 78899, Reward: [-1004.623 -1004.623 -1004.623] [236.1350], Avg: [-932.671 -932.671 -932.671] (1.000)
Step: 78949, Reward: [-750.847 -750.847 -750.847] [184.5539], Avg: [-932.673 -932.673 -932.673] (1.000)
Step: 78999, Reward: [-873.623 -873.623 -873.623] [130.0050], Avg: [-932.718 -932.718 -932.718] (1.000)
Step: 79049, Reward: [-1206.367 -1206.367 -1206.367] [348.3855], Avg: [-933.112 -933.112 -933.112] (1.000)
Step: 79099, Reward: [-989.094 -989.094 -989.094] [386.5957], Avg: [-933.391 -933.391 -933.391] (1.000)
Step: 79149, Reward: [-1058.489 -1058.489 -1058.489] [329.2022], Avg: [-933.678 -933.678 -933.678] (1.000)
Step: 79199, Reward: [-969.953 -969.953 -969.953] [326.3923], Avg: [-933.907 -933.907 -933.907] (1.000)
Step: 79249, Reward: [-694.237 -694.237 -694.237] [215.0168], Avg: [-933.892 -933.892 -933.892] (1.000)
Step: 79299, Reward: [-654.926 -654.926 -654.926] [229.8227], Avg: [-933.861 -933.861 -933.861] (1.000)
Step: 79349, Reward: [-1032.584 -1032.584 -1032.584] [137.0430], Avg: [-934.009 -934.009 -934.009] (1.000)
Step: 79399, Reward: [-600.899 -600.899 -600.899] [340.2013], Avg: [-934.014 -934.014 -934.014] (1.000)
Step: 79449, Reward: [-614.133 -614.133 -614.133] [209.0977], Avg: [-933.944 -933.944 -933.944] (1.000)
Step: 79499, Reward: [-867.462 -867.462 -867.462] [366.3628], Avg: [-934.133 -934.133 -934.133] (1.000)
Step: 79549, Reward: [-891.198 -891.198 -891.198] [328.6535], Avg: [-934.312 -934.312 -934.312] (1.000)
Step: 79599, Reward: [-1103.149 -1103.149 -1103.149] [346.4524], Avg: [-934.636 -934.636 -934.636] (1.000)
Step: 79649, Reward: [-992.107 -992.107 -992.107] [434.9805], Avg: [-934.945 -934.945 -934.945] (1.000)
Step: 79699, Reward: [-710.655 -710.655 -710.655] [228.5922], Avg: [-934.948 -934.948 -934.948] (1.000)
Step: 79749, Reward: [-943.273 -943.273 -943.273] [344.6281], Avg: [-935.169 -935.169 -935.169] (1.000)
Step: 79799, Reward: [-1017.083 -1017.083 -1017.083] [383.7386], Avg: [-935.461 -935.461 -935.461] (1.000)
Step: 79849, Reward: [-766.796 -766.796 -766.796] [348.6946], Avg: [-935.573 -935.573 -935.573] (1.000)
Step: 79899, Reward: [-598.012 -598.012 -598.012] [142.6825], Avg: [-935.452 -935.452 -935.452] (1.000)
Step: 79949, Reward: [-759.185 -759.185 -759.185] [249.0010], Avg: [-935.497 -935.497 -935.497] (1.000)
Step: 79999, Reward: [-598.881 -598.881 -598.881] [94.1214], Avg: [-935.345 -935.345 -935.345] (1.000)
Step: 80049, Reward: [-733.021 -733.021 -733.021] [224.2033], Avg: [-935.359 -935.359 -935.359] (1.000)
Step: 80099, Reward: [-794. -794. -794.] [265.0550], Avg: [-935.436 -935.436 -935.436] (1.000)
Step: 80149, Reward: [-649.532 -649.532 -649.532] [205.1260], Avg: [-935.386 -935.386 -935.386] (1.000)
Step: 80199, Reward: [-560.956 -560.956 -560.956] [107.6872], Avg: [-935.22 -935.22 -935.22] (1.000)
Step: 80249, Reward: [-705.532 -705.532 -705.532] [133.7145], Avg: [-935.16 -935.16 -935.16] (1.000)
Step: 80299, Reward: [-483.812 -483.812 -483.812] [107.5054], Avg: [-934.946 -934.946 -934.946] (1.000)
Step: 80349, Reward: [-661.782 -661.782 -661.782] [150.2971], Avg: [-934.869 -934.869 -934.869] (1.000)
Step: 80399, Reward: [-873.439 -873.439 -873.439] [154.7523], Avg: [-934.927 -934.927 -934.927] (1.000)
Step: 80449, Reward: [-624.692 -624.692 -624.692] [201.3274], Avg: [-934.86 -934.86 -934.86] (1.000)
Step: 80499, Reward: [-854.989 -854.989 -854.989] [251.9103], Avg: [-934.967 -934.967 -934.967] (1.000)
Step: 80549, Reward: [-816.252 -816.252 -816.252] [134.5744], Avg: [-934.976 -934.976 -934.976] (1.000)
Step: 80599, Reward: [-893.449 -893.449 -893.449] [369.7103], Avg: [-935.18 -935.18 -935.18] (1.000)
Step: 80649, Reward: [-1119.935 -1119.935 -1119.935] [545.2815], Avg: [-935.633 -935.633 -935.633] (1.000)
Step: 80699, Reward: [-723.816 -723.816 -723.816] [276.2482], Avg: [-935.672 -935.672 -935.672] (1.000)
Step: 80749, Reward: [-1098.531 -1098.531 -1098.531] [386.8294], Avg: [-936.013 -936.013 -936.013] (1.000)
Step: 80799, Reward: [-889.38 -889.38 -889.38] [435.8160], Avg: [-936.254 -936.254 -936.254] (1.000)
Step: 80849, Reward: [-1170.039 -1170.039 -1170.039] [313.5642], Avg: [-936.592 -936.592 -936.592] (1.000)
Step: 80899, Reward: [-566.429 -566.429 -566.429] [99.3873], Avg: [-936.425 -936.425 -936.425] (1.000)
Step: 80949, Reward: [-429.893 -429.893 -429.893] [100.2710], Avg: [-936.174 -936.174 -936.174] (1.000)
Step: 80999, Reward: [-412.878 -412.878 -412.878] [123.5767], Avg: [-935.927 -935.927 -935.927] (1.000)
Step: 81049, Reward: [-446.899 -446.899 -446.899] [84.7645], Avg: [-935.678 -935.678 -935.678] (1.000)
Step: 81099, Reward: [-645.886 -645.886 -645.886] [170.4966], Avg: [-935.604 -935.604 -935.604] (1.000)
Step: 81149, Reward: [-436.743 -436.743 -436.743] [57.4024], Avg: [-935.332 -935.332 -935.332] (1.000)
Step: 81199, Reward: [-389.457 -389.457 -389.457] [55.2587], Avg: [-935.03 -935.03 -935.03] (1.000)
Step: 81249, Reward: [-477.652 -477.652 -477.652] [108.6160], Avg: [-934.815 -934.815 -934.815] (1.000)
Step: 81299, Reward: [-792.836 -792.836 -792.836] [346.5801], Avg: [-934.941 -934.941 -934.941] (1.000)
Step: 81349, Reward: [-718.998 -718.998 -718.998] [239.5295], Avg: [-934.956 -934.956 -934.956] (1.000)
Step: 81399, Reward: [-708.714 -708.714 -708.714] [147.0700], Avg: [-934.907 -934.907 -934.907] (1.000)
Step: 81449, Reward: [-697.175 -697.175 -697.175] [135.0439], Avg: [-934.844 -934.844 -934.844] (1.000)
Step: 81499, Reward: [-910.874 -910.874 -910.874] [274.6957], Avg: [-934.998 -934.998 -934.998] (1.000)
Step: 81549, Reward: [-621.507 -621.507 -621.507] [176.9749], Avg: [-934.914 -934.914 -934.914] (1.000)
Step: 81599, Reward: [-998.279 -998.279 -998.279] [361.6275], Avg: [-935.175 -935.175 -935.175] (1.000)
Step: 81649, Reward: [-1084.869 -1084.869 -1084.869] [496.5138], Avg: [-935.57 -935.57 -935.57] (1.000)
Step: 81699, Reward: [-999.64 -999.64 -999.64] [480.0932], Avg: [-935.903 -935.903 -935.903] (1.000)
Step: 81749, Reward: [-923.603 -923.603 -923.603] [324.0242], Avg: [-936.094 -936.094 -936.094] (1.000)
Step: 81799, Reward: [-539.171 -539.171 -539.171] [203.3026], Avg: [-935.976 -935.976 -935.976] (1.000)
Step: 81849, Reward: [-611.632 -611.632 -611.632] [69.2692], Avg: [-935.82 -935.82 -935.82] (1.000)
Step: 81899, Reward: [-532.428 -532.428 -532.428] [137.6950], Avg: [-935.658 -935.658 -935.658] (1.000)
Step: 81949, Reward: [-726.074 -726.074 -726.074] [107.6757], Avg: [-935.595 -935.595 -935.595] (1.000)
Step: 81999, Reward: [-636.625 -636.625 -636.625] [153.0019], Avg: [-935.506 -935.506 -935.506] (1.000)
Step: 82049, Reward: [-469.708 -469.708 -469.708] [40.0242], Avg: [-935.247 -935.247 -935.247] (1.000)
Step: 82099, Reward: [-533.818 -533.818 -533.818] [104.1512], Avg: [-935.066 -935.066 -935.066] (1.000)
Step: 82149, Reward: [-737.27 -737.27 -737.27] [173.8126], Avg: [-935.051 -935.051 -935.051] (1.000)
Step: 82199, Reward: [-556.617 -556.617 -556.617] [155.6559], Avg: [-934.916 -934.916 -934.916] (1.000)
Step: 82249, Reward: [-738.629 -738.629 -738.629] [235.9847], Avg: [-934.94 -934.94 -934.94] (1.000)
Step: 82299, Reward: [-706.317 -706.317 -706.317] [334.3493], Avg: [-935.004 -935.004 -935.004] (1.000)
Step: 82349, Reward: [-674.048 -674.048 -674.048] [115.1934], Avg: [-934.916 -934.916 -934.916] (1.000)
Step: 82399, Reward: [-979.8 -979.8 -979.8] [369.3829], Avg: [-935.167 -935.167 -935.167] (1.000)
Step: 82449, Reward: [-755.904 -755.904 -755.904] [292.6773], Avg: [-935.236 -935.236 -935.236] (1.000)
Step: 82499, Reward: [-912.934 -912.934 -912.934] [258.1561], Avg: [-935.379 -935.379 -935.379] (1.000)
Step: 82549, Reward: [-691.535 -691.535 -691.535] [230.2422], Avg: [-935.371 -935.371 -935.371] (1.000)
Step: 82599, Reward: [-953.575 -953.575 -953.575] [437.9210], Avg: [-935.647 -935.647 -935.647] (1.000)
Step: 82649, Reward: [-763.35 -763.35 -763.35] [515.8036], Avg: [-935.855 -935.855 -935.855] (1.000)
Step: 82699, Reward: [-796.779 -796.779 -796.779] [372.7328], Avg: [-935.996 -935.996 -935.996] (1.000)
Step: 82749, Reward: [-408.538 -408.538 -408.538] [39.9946], Avg: [-935.701 -935.701 -935.701] (1.000)
Step: 82799, Reward: [-554.744 -554.744 -554.744] [62.9505], Avg: [-935.509 -935.509 -935.509] (1.000)
Step: 82849, Reward: [-524.605 -524.605 -524.605] [51.6278], Avg: [-935.292 -935.292 -935.292] (1.000)
Step: 82899, Reward: [-802.254 -802.254 -802.254] [468.7835], Avg: [-935.495 -935.495 -935.495] (1.000)
Step: 82949, Reward: [-541.842 -541.842 -541.842] [14.2970], Avg: [-935.266 -935.266 -935.266] (1.000)
Step: 82999, Reward: [-539.215 -539.215 -539.215] [77.2444], Avg: [-935.074 -935.074 -935.074] (1.000)
Step: 83049, Reward: [-577.437 -577.437 -577.437] [112.5396], Avg: [-934.927 -934.927 -934.927] (1.000)
Step: 83099, Reward: [-499.719 -499.719 -499.719] [83.4765], Avg: [-934.715 -934.715 -934.715] (1.000)
Step: 83149, Reward: [-506.443 -506.443 -506.443] [126.4525], Avg: [-934.533 -934.533 -934.533] (1.000)
Step: 83199, Reward: [-489.256 -489.256 -489.256] [124.0917], Avg: [-934.34 -934.34 -934.34] (1.000)
Step: 83249, Reward: [-526.244 -526.244 -526.244] [93.5346], Avg: [-934.152 -934.152 -934.152] (1.000)
Step: 83299, Reward: [-491.047 -491.047 -491.047] [149.0875], Avg: [-933.975 -933.975 -933.975] (1.000)
Step: 83349, Reward: [-602.842 -602.842 -602.842] [215.5330], Avg: [-933.906 -933.906 -933.906] (1.000)
Step: 83399, Reward: [-591.829 -591.829 -591.829] [190.1338], Avg: [-933.815 -933.815 -933.815] (1.000)
Step: 83449, Reward: [-731.742 -731.742 -731.742] [247.1345], Avg: [-933.842 -933.842 -933.842] (1.000)
Step: 83499, Reward: [-664.904 -664.904 -664.904] [242.1907], Avg: [-933.826 -933.826 -933.826] (1.000)
Step: 83549, Reward: [-931.791 -931.791 -931.791] [450.1716], Avg: [-934.094 -934.094 -934.094] (1.000)
Step: 83599, Reward: [-746.333 -746.333 -746.333] [123.2294], Avg: [-934.055 -934.055 -934.055] (1.000)
Step: 83649, Reward: [-771.672 -771.672 -771.672] [120.9502], Avg: [-934.03 -934.03 -934.03] (1.000)
Step: 83699, Reward: [-672.777 -672.777 -672.777] [109.0729], Avg: [-933.94 -933.94 -933.94] (1.000)
Step: 83749, Reward: [-932.542 -932.542 -932.542] [389.3266], Avg: [-934.171 -934.171 -934.171] (1.000)
Step: 83799, Reward: [-676.934 -676.934 -676.934] [154.1489], Avg: [-934.11 -934.11 -934.11] (1.000)
Step: 83849, Reward: [-710.617 -710.617 -710.617] [145.5433], Avg: [-934.063 -934.063 -934.063] (1.000)
Step: 83899, Reward: [-760.591 -760.591 -760.591] [142.9301], Avg: [-934.045 -934.045 -934.045] (1.000)
Step: 83949, Reward: [-601.442 -601.442 -601.442] [67.2615], Avg: [-933.887 -933.887 -933.887] (1.000)
Step: 83999, Reward: [-612.431 -612.431 -612.431] [92.5612], Avg: [-933.751 -933.751 -933.751] (1.000)
Step: 84049, Reward: [-743.505 -743.505 -743.505] [283.5429], Avg: [-933.806 -933.806 -933.806] (1.000)
Step: 84099, Reward: [-563.755 -563.755 -563.755] [141.4998], Avg: [-933.67 -933.67 -933.67] (1.000)
Step: 84149, Reward: [-557.548 -557.548 -557.548] [108.8400], Avg: [-933.511 -933.511 -933.511] (1.000)
Step: 84199, Reward: [-610.506 -610.506 -610.506] [180.9868], Avg: [-933.427 -933.427 -933.427] (1.000)
Step: 84249, Reward: [-732.854 -732.854 -732.854] [207.6951], Avg: [-933.431 -933.431 -933.431] (1.000)
Step: 84299, Reward: [-574.735 -574.735 -574.735] [139.1230], Avg: [-933.301 -933.301 -933.301] (1.000)
Step: 84349, Reward: [-631.542 -631.542 -631.542] [126.5504], Avg: [-933.197 -933.197 -933.197] (1.000)
Step: 84399, Reward: [-663.68 -663.68 -663.68] [71.0046], Avg: [-933.08 -933.08 -933.08] (1.000)
Step: 84449, Reward: [-722.789 -722.789 -722.789] [166.1782], Avg: [-933.054 -933.054 -933.054] (1.000)
Step: 84499, Reward: [-650.653 -650.653 -650.653] [78.2584], Avg: [-932.933 -932.933 -932.933] (1.000)
Step: 84549, Reward: [-529.982 -529.982 -529.982] [96.8857], Avg: [-932.752 -932.752 -932.752] (1.000)
Step: 84599, Reward: [-933.123 -933.123 -933.123] [420.0143], Avg: [-933. -933. -933.] (1.000)
Step: 84649, Reward: [-884.273 -884.273 -884.273] [319.2997], Avg: [-933.16 -933.16 -933.16] (1.000)
Step: 84699, Reward: [-805.138 -805.138 -805.138] [199.6588], Avg: [-933.202 -933.202 -933.202] (1.000)
Step: 84749, Reward: [-1130.641 -1130.641 -1130.641] [240.4064], Avg: [-933.461 -933.461 -933.461] (1.000)
Step: 84799, Reward: [-1260.588 -1260.588 -1260.588] [225.6786], Avg: [-933.787 -933.787 -933.787] (1.000)
Step: 84849, Reward: [-958.964 -958.964 -958.964] [372.0407], Avg: [-934.021 -934.021 -934.021] (1.000)
Step: 84899, Reward: [-1069.104 -1069.104 -1069.104] [209.9645], Avg: [-934.224 -934.224 -934.224] (1.000)
Step: 84949, Reward: [-1023.167 -1023.167 -1023.167] [142.2532], Avg: [-934.36 -934.36 -934.36] (1.000)
Step: 84999, Reward: [-957.537 -957.537 -957.537] [189.0106], Avg: [-934.485 -934.485 -934.485] (1.000)
Step: 85049, Reward: [-939.762 -939.762 -939.762] [181.9722], Avg: [-934.595 -934.595 -934.595] (1.000)
Step: 85099, Reward: [-1181.124 -1181.124 -1181.124] [53.2277], Avg: [-934.771 -934.771 -934.771] (1.000)
Step: 85149, Reward: [-1140.243 -1140.243 -1140.243] [212.4607], Avg: [-935.016 -935.016 -935.016] (1.000)
Step: 85199, Reward: [-862.649 -862.649 -862.649] [268.5773], Avg: [-935.131 -935.131 -935.131] (1.000)
Step: 85249, Reward: [-797.317 -797.317 -797.317] [224.0784], Avg: [-935.182 -935.182 -935.182] (1.000)
Step: 85299, Reward: [-653.137 -653.137 -653.137] [169.1977], Avg: [-935.116 -935.116 -935.116] (1.000)
Step: 85349, Reward: [-932.506 -932.506 -932.506] [317.4353], Avg: [-935.3 -935.3 -935.3] (1.000)
Step: 85399, Reward: [-709.306 -709.306 -709.306] [137.8274], Avg: [-935.249 -935.249 -935.249] (1.000)
Step: 85449, Reward: [-620.666 -620.666 -620.666] [65.6967], Avg: [-935.103 -935.103 -935.103] (1.000)
Step: 85499, Reward: [-777.922 -777.922 -777.922] [173.1712], Avg: [-935.112 -935.112 -935.112] (1.000)
Step: 85549, Reward: [-827.048 -827.048 -827.048] [256.3098], Avg: [-935.199 -935.199 -935.199] (1.000)
Step: 85599, Reward: [-590.212 -590.212 -590.212] [187.5349], Avg: [-935.107 -935.107 -935.107] (1.000)
Step: 85649, Reward: [-782.518 -782.518 -782.518] [282.4304], Avg: [-935.183 -935.183 -935.183] (1.000)
Step: 85699, Reward: [-678.831 -678.831 -678.831] [152.6479], Avg: [-935.122 -935.122 -935.122] (1.000)
Step: 85749, Reward: [-694.73 -694.73 -694.73] [283.8063], Avg: [-935.148 -935.148 -935.148] (1.000)
Step: 85799, Reward: [-762.518 -762.518 -762.518] [191.7926], Avg: [-935.159 -935.159 -935.159] (1.000)
Step: 85849, Reward: [-858.034 -858.034 -858.034] [348.1677], Avg: [-935.317 -935.317 -935.317] (1.000)
Step: 85899, Reward: [-898.736 -898.736 -898.736] [287.7223], Avg: [-935.463 -935.463 -935.463] (1.000)
Step: 85949, Reward: [-678.609 -678.609 -678.609] [144.3369], Avg: [-935.398 -935.398 -935.398] (1.000)
Step: 85999, Reward: [-574.734 -574.734 -574.734] [75.8333], Avg: [-935.232 -935.232 -935.232] (1.000)
Step: 86049, Reward: [-767.456 -767.456 -767.456] [69.6183], Avg: [-935.175 -935.175 -935.175] (1.000)
Step: 86099, Reward: [-550.971 -550.971 -550.971] [95.5718], Avg: [-935.007 -935.007 -935.007] (1.000)
Step: 86149, Reward: [-444.456 -444.456 -444.456] [73.1691], Avg: [-934.765 -934.765 -934.765] (1.000)
Step: 86199, Reward: [-655.386 -655.386 -655.386] [203.3942], Avg: [-934.721 -934.721 -934.721] (1.000)
Step: 86249, Reward: [-551.836 -551.836 -551.836] [132.5642], Avg: [-934.576 -934.576 -934.576] (1.000)
Step: 86299, Reward: [-515.223 -515.223 -515.223] [199.6280], Avg: [-934.449 -934.449 -934.449] (1.000)
Step: 86349, Reward: [-514.336 -514.336 -514.336] [198.1858], Avg: [-934.32 -934.32 -934.32] (1.000)
Step: 86399, Reward: [-510.471 -510.471 -510.471] [54.1691], Avg: [-934.106 -934.106 -934.106] (1.000)
Step: 86449, Reward: [-560.567 -560.567 -560.567] [93.3312], Avg: [-933.944 -933.944 -933.944] (1.000)
Step: 86499, Reward: [-527.678 -527.678 -527.678] [114.4046], Avg: [-933.775 -933.775 -933.775] (1.000)
Step: 86549, Reward: [-628.732 -628.732 -628.732] [222.3862], Avg: [-933.728 -933.728 -933.728] (1.000)
Step: 86599, Reward: [-539.615 -539.615 -539.615] [35.3755], Avg: [-933.52 -933.52 -933.52] (1.000)
Step: 86649, Reward: [-511.916 -511.916 -511.916] [67.1848], Avg: [-933.316 -933.316 -933.316] (1.000)
Step: 86699, Reward: [-565.412 -565.412 -565.412] [103.4725], Avg: [-933.163 -933.163 -933.163] (1.000)
Step: 86749, Reward: [-518.965 -518.965 -518.965] [108.6084], Avg: [-932.987 -932.987 -932.987] (1.000)
Step: 86799, Reward: [-653.491 -653.491 -653.491] [222.8538], Avg: [-932.955 -932.955 -932.955] (1.000)
Step: 86849, Reward: [-476.896 -476.896 -476.896] [129.3517], Avg: [-932.767 -932.767 -932.767] (1.000)
Step: 86899, Reward: [-579.944 -579.944 -579.944] [94.4366], Avg: [-932.618 -932.618 -932.618] (1.000)
Step: 86949, Reward: [-608.714 -608.714 -608.714] [73.7929], Avg: [-932.474 -932.474 -932.474] (1.000)
Step: 86999, Reward: [-494.752 -494.752 -494.752] [64.4019], Avg: [-932.26 -932.26 -932.26] (1.000)
Step: 87049, Reward: [-631.341 -631.341 -631.341] [192.9280], Avg: [-932.198 -932.198 -932.198] (1.000)
Step: 87099, Reward: [-513.479 -513.479 -513.479] [57.3286], Avg: [-931.99 -931.99 -931.99] (1.000)
Step: 87149, Reward: [-609.338 -609.338 -609.338] [40.7735], Avg: [-931.828 -931.828 -931.828] (1.000)
Step: 87199, Reward: [-496.257 -496.257 -496.257] [29.4422], Avg: [-931.595 -931.595 -931.595] (1.000)
Step: 87249, Reward: [-725.694 -725.694 -725.694] [241.6512], Avg: [-931.616 -931.616 -931.616] (1.000)
Step: 87299, Reward: [-671.139 -671.139 -671.139] [129.1111], Avg: [-931.541 -931.541 -931.541] (1.000)
Step: 87349, Reward: [-596.523 -596.523 -596.523] [199.9371], Avg: [-931.463 -931.463 -931.463] (1.000)
Step: 87399, Reward: [-714.356 -714.356 -714.356] [345.8814], Avg: [-931.537 -931.537 -931.537] (1.000)
Step: 87449, Reward: [-626.596 -626.596 -626.596] [47.9143], Avg: [-931.39 -931.39 -931.39] (1.000)
Step: 87499, Reward: [-594.617 -594.617 -594.617] [187.9691], Avg: [-931.305 -931.305 -931.305] (1.000)
Step: 87549, Reward: [-584.128 -584.128 -584.128] [88.0376], Avg: [-931.157 -931.157 -931.157] (1.000)
Step: 87599, Reward: [-639.251 -639.251 -639.251] [240.5685], Avg: [-931.128 -931.128 -931.128] (1.000)
Step: 87649, Reward: [-632.413 -632.413 -632.413] [171.4445], Avg: [-931.055 -931.055 -931.055] (1.000)
Step: 87699, Reward: [-518.696 -518.696 -518.696] [122.6411], Avg: [-930.89 -930.89 -930.89] (1.000)
Step: 87749, Reward: [-573.178 -573.178 -573.178] [126.3221], Avg: [-930.758 -930.758 -930.758] (1.000)
Step: 87799, Reward: [-636.125 -636.125 -636.125] [141.0004], Avg: [-930.671 -930.671 -930.671] (1.000)
Step: 87849, Reward: [-538.972 -538.972 -538.972] [71.6667], Avg: [-930.489 -930.489 -930.489] (1.000)
Step: 87899, Reward: [-524.76 -524.76 -524.76] [97.2878], Avg: [-930.313 -930.313 -930.313] (1.000)
Step: 87949, Reward: [-489.08 -489.08 -489.08] [71.9019], Avg: [-930.103 -930.103 -930.103] (1.000)
Step: 87999, Reward: [-520.132 -520.132 -520.132] [165.6217], Avg: [-929.964 -929.964 -929.964] (1.000)
Step: 88049, Reward: [-636.942 -636.942 -636.942] [74.5969], Avg: [-929.84 -929.84 -929.84] (1.000)
Step: 88099, Reward: [-536.406 -536.406 -536.406] [113.4487], Avg: [-929.681 -929.681 -929.681] (1.000)
Step: 88149, Reward: [-637.675 -637.675 -637.675] [136.9283], Avg: [-929.593 -929.593 -929.593] (1.000)
Step: 88199, Reward: [-454.7 -454.7 -454.7] [91.7910], Avg: [-929.376 -929.376 -929.376] (1.000)
Step: 88249, Reward: [-519.513 -519.513 -519.513] [121.9714], Avg: [-929.213 -929.213 -929.213] (1.000)
Step: 88299, Reward: [-648.677 -648.677 -648.677] [169.1698], Avg: [-929.15 -929.15 -929.15] (1.000)
Step: 88349, Reward: [-707.427 -707.427 -707.427] [222.8340], Avg: [-929.151 -929.151 -929.151] (1.000)
Step: 88399, Reward: [-500.481 -500.481 -500.481] [44.8231], Avg: [-928.934 -928.934 -928.934] (1.000)
Step: 88449, Reward: [-566.462 -566.462 -566.462] [74.1900], Avg: [-928.771 -928.771 -928.771] (1.000)
Step: 88499, Reward: [-554.845 -554.845 -554.845] [155.5840], Avg: [-928.647 -928.647 -928.647] (1.000)
Step: 88549, Reward: [-569.593 -569.593 -569.593] [71.2746], Avg: [-928.485 -928.485 -928.485] (1.000)
Step: 88599, Reward: [-554.477 -554.477 -554.477] [140.0187], Avg: [-928.353 -928.353 -928.353] (1.000)
Step: 88649, Reward: [-517.254 -517.254 -517.254] [149.7100], Avg: [-928.205 -928.205 -928.205] (1.000)
Step: 88699, Reward: [-488.265 -488.265 -488.265] [70.2138], Avg: [-927.997 -927.997 -927.997] (1.000)
Step: 88749, Reward: [-622.009 -622.009 -622.009] [78.8362], Avg: [-927.869 -927.869 -927.869] (1.000)
Step: 88799, Reward: [-553.352 -553.352 -553.352] [85.5939], Avg: [-927.706 -927.706 -927.706] (1.000)
Step: 88849, Reward: [-496.024 -496.024 -496.024] [105.4261], Avg: [-927.523 -927.523 -927.523] (1.000)
Step: 88899, Reward: [-554.378 -554.378 -554.378] [73.7653], Avg: [-927.354 -927.354 -927.354] (1.000)
Step: 88949, Reward: [-750.477 -750.477 -750.477] [258.2711], Avg: [-927.4 -927.4 -927.4] (1.000)
Step: 88999, Reward: [-643.244 -643.244 -643.244] [167.5392], Avg: [-927.334 -927.334 -927.334] (1.000)
Step: 89049, Reward: [-540.841 -540.841 -540.841] [91.9246], Avg: [-927.169 -927.169 -927.169] (1.000)
Step: 89099, Reward: [-528.592 -528.592 -528.592] [35.7928], Avg: [-926.965 -926.965 -926.965] (1.000)
Step: 89149, Reward: [-555.093 -555.093 -555.093] [89.1971], Avg: [-926.807 -926.807 -926.807] (1.000)
Step: 89199, Reward: [-646.703 -646.703 -646.703] [219.3493], Avg: [-926.773 -926.773 -926.773] (1.000)
Step: 89249, Reward: [-503.316 -503.316 -503.316] [95.2550], Avg: [-926.589 -926.589 -926.589] (1.000)
Step: 89299, Reward: [-649.433 -649.433 -649.433] [60.2852], Avg: [-926.468 -926.468 -926.468] (1.000)
Step: 89349, Reward: [-591.532 -591.532 -591.532] [158.6617], Avg: [-926.369 -926.369 -926.369] (1.000)
Step: 89399, Reward: [-443.321 -443.321 -443.321] [58.5119], Avg: [-926.131 -926.131 -926.131] (1.000)
Step: 89449, Reward: [-516.557 -516.557 -516.557] [69.6645], Avg: [-925.941 -925.941 -925.941] (1.000)
Step: 89499, Reward: [-456.479 -456.479 -456.479] [147.7997], Avg: [-925.762 -925.762 -925.762] (1.000)
Step: 89549, Reward: [-482.809 -482.809 -482.809] [98.3386], Avg: [-925.569 -925.569 -925.569] (1.000)
Step: 89599, Reward: [-527.743 -527.743 -527.743] [140.7563], Avg: [-925.426 -925.426 -925.426] (1.000)
Step: 89649, Reward: [-591.643 -591.643 -591.643] [251.8653], Avg: [-925.38 -925.38 -925.38] (1.000)
Step: 89699, Reward: [-574.029 -574.029 -574.029] [93.6088], Avg: [-925.237 -925.237 -925.237] (1.000)
Step: 89749, Reward: [-613.278 -613.278 -613.278] [37.1554], Avg: [-925.083 -925.083 -925.083] (1.000)
Step: 89799, Reward: [-576.242 -576.242 -576.242] [155.7915], Avg: [-924.976 -924.976 -924.976] (1.000)
Step: 89849, Reward: [-643.481 -643.481 -643.481] [64.3504], Avg: [-924.855 -924.855 -924.855] (1.000)
Step: 89899, Reward: [-742.434 -742.434 -742.434] [80.4416], Avg: [-924.798 -924.798 -924.798] (1.000)
Step: 89949, Reward: [-708.535 -708.535 -708.535] [135.0953], Avg: [-924.753 -924.753 -924.753] (1.000)
Step: 89999, Reward: [-809.292 -809.292 -809.292] [125.0469], Avg: [-924.759 -924.759 -924.759] (1.000)
Step: 90049, Reward: [-1005.473 -1005.473 -1005.473] [171.7372], Avg: [-924.899 -924.899 -924.899] (1.000)
Step: 90099, Reward: [-1015.068 -1015.068 -1015.068] [105.2498], Avg: [-925.007 -925.007 -925.007] (1.000)
Step: 90149, Reward: [-942.253 -942.253 -942.253] [164.6160], Avg: [-925.108 -925.108 -925.108] (1.000)
Step: 90199, Reward: [-978.232 -978.232 -978.232] [128.0055], Avg: [-925.209 -925.209 -925.209] (1.000)
Step: 90249, Reward: [-988.849 -988.849 -988.849] [191.7698], Avg: [-925.35 -925.35 -925.35] (1.000)
Step: 90299, Reward: [-800.255 -800.255 -800.255] [85.5544], Avg: [-925.328 -925.328 -925.328] (1.000)
Step: 90349, Reward: [-793.021 -793.021 -793.021] [155.3942], Avg: [-925.341 -925.341 -925.341] (1.000)
Step: 90399, Reward: [-631.783 -631.783 -631.783] [71.7741], Avg: [-925.218 -925.218 -925.218] (1.000)
Step: 90449, Reward: [-579.006 -579.006 -579.006] [107.8324], Avg: [-925.086 -925.086 -925.086] (1.000)
Step: 90499, Reward: [-507.772 -507.772 -507.772] [51.5367], Avg: [-924.884 -924.884 -924.884] (1.000)
Step: 90549, Reward: [-489.199 -489.199 -489.199] [82.5496], Avg: [-924.689 -924.689 -924.689] (1.000)
Step: 90599, Reward: [-454.839 -454.839 -454.839] [36.9160], Avg: [-924.45 -924.45 -924.45] (1.000)
Step: 90649, Reward: [-523.574 -523.574 -523.574] [56.9094], Avg: [-924.261 -924.261 -924.261] (1.000)
Step: 90699, Reward: [-426.089 -426.089 -426.089] [82.1163], Avg: [-924.031 -924.031 -924.031] (1.000)
Step: 90749, Reward: [-441.296 -441.296 -441.296] [134.8278], Avg: [-923.84 -923.84 -923.84] (1.000)
Step: 90799, Reward: [-457.869 -457.869 -457.869] [132.7970], Avg: [-923.656 -923.656 -923.656] (1.000)
Step: 90849, Reward: [-518.082 -518.082 -518.082] [135.2682], Avg: [-923.507 -923.507 -923.507] (1.000)
Step: 90899, Reward: [-442.392 -442.392 -442.392] [70.8395], Avg: [-923.282 -923.282 -923.282] (1.000)
Step: 90949, Reward: [-397.209 -397.209 -397.209] [67.3975], Avg: [-923.03 -923.03 -923.03] (1.000)
Step: 90999, Reward: [-437.765 -437.765 -437.765] [79.0926], Avg: [-922.806 -922.806 -922.806] (1.000)
Step: 91049, Reward: [-432.525 -432.525 -432.525] [77.4959], Avg: [-922.58 -922.58 -922.58] (1.000)
Step: 91099, Reward: [-395.322 -395.322 -395.322] [65.9954], Avg: [-922.327 -922.327 -922.327] (1.000)
Step: 91149, Reward: [-467.986 -467.986 -467.986] [56.2751], Avg: [-922.108 -922.108 -922.108] (1.000)
Step: 91199, Reward: [-422.262 -422.262 -422.262] [48.1475], Avg: [-921.861 -921.861 -921.861] (1.000)
Step: 91249, Reward: [-504.038 -504.038 -504.038] [81.9225], Avg: [-921.677 -921.677 -921.677] (1.000)
Step: 91299, Reward: [-491.683 -491.683 -491.683] [87.1850], Avg: [-921.489 -921.489 -921.489] (1.000)
Step: 91349, Reward: [-611.074 -611.074 -611.074] [69.0823], Avg: [-921.357 -921.357 -921.357] (1.000)
Step: 91399, Reward: [-487.78 -487.78 -487.78] [103.0007], Avg: [-921.176 -921.176 -921.176] (1.000)
Step: 91449, Reward: [-642.285 -642.285 -642.285] [94.0558], Avg: [-921.075 -921.075 -921.075] (1.000)
Step: 91499, Reward: [-587.854 -587.854 -587.854] [69.4382], Avg: [-920.931 -920.931 -920.931] (1.000)
Step: 91549, Reward: [-845.449 -845.449 -845.449] [89.0861], Avg: [-920.938 -920.938 -920.938] (1.000)
Step: 91599, Reward: [-1199.795 -1199.795 -1199.795] [202.7786], Avg: [-921.201 -921.201 -921.201] (1.000)
Step: 91649, Reward: [-1036.389 -1036.389 -1036.389] [168.1946], Avg: [-921.356 -921.356 -921.356] (1.000)
Step: 91699, Reward: [-1180.211 -1180.211 -1180.211] [123.3362], Avg: [-921.564 -921.564 -921.564] (1.000)
Step: 91749, Reward: [-1191.351 -1191.351 -1191.351] [139.3506], Avg: [-921.787 -921.787 -921.787] (1.000)
Step: 91799, Reward: [-1310.237 -1310.237 -1310.237] [56.7002], Avg: [-922.029 -922.029 -922.029] (1.000)
Step: 91849, Reward: [-903.534 -903.534 -903.534] [167.8699], Avg: [-922.111 -922.111 -922.111] (1.000)
Step: 91899, Reward: [-624.224 -624.224 -624.224] [141.2233], Avg: [-922.026 -922.026 -922.026] (1.000)
Step: 91949, Reward: [-600.852 -600.852 -600.852] [112.3426], Avg: [-921.912 -921.912 -921.912] (1.000)
Step: 91999, Reward: [-583.81 -583.81 -583.81] [110.8115], Avg: [-921.788 -921.788 -921.788] (1.000)
Step: 92049, Reward: [-510.859 -510.859 -510.859] [108.1010], Avg: [-921.624 -921.624 -921.624] (1.000)
Step: 92099, Reward: [-444.802 -444.802 -444.802] [94.3754], Avg: [-921.416 -921.416 -921.416] (1.000)
Step: 92149, Reward: [-460.91 -460.91 -460.91] [47.2756], Avg: [-921.192 -921.192 -921.192] (1.000)
Step: 92199, Reward: [-467.212 -467.212 -467.212] [147.7280], Avg: [-921.026 -921.026 -921.026] (1.000)
Step: 92249, Reward: [-447.168 -447.168 -447.168] [95.0092], Avg: [-920.821 -920.821 -920.821] (1.000)
Step: 92299, Reward: [-421.38 -421.38 -421.38] [40.5259], Avg: [-920.572 -920.572 -920.572] (1.000)
Step: 92349, Reward: [-463.596 -463.596 -463.596] [103.9857], Avg: [-920.381 -920.381 -920.381] (1.000)
Step: 92399, Reward: [-416.043 -416.043 -416.043] [96.7732], Avg: [-920.16 -920.16 -920.16] (1.000)
Step: 92449, Reward: [-469.209 -469.209 -469.209] [135.0099], Avg: [-919.99 -919.99 -919.99] (1.000)
Step: 92499, Reward: [-469.837 -469.837 -469.837] [47.7823], Avg: [-919.772 -919.772 -919.772] (1.000)
Step: 92549, Reward: [-437.954 -437.954 -437.954] [38.2103], Avg: [-919.532 -919.532 -919.532] (1.000)
Step: 92599, Reward: [-412.14 -412.14 -412.14] [107.4975], Avg: [-919.316 -919.316 -919.316] (1.000)
Step: 92649, Reward: [-417.138 -417.138 -417.138] [73.1750], Avg: [-919.085 -919.085 -919.085] (1.000)
Step: 92699, Reward: [-421.907 -421.907 -421.907] [68.5288], Avg: [-918.854 -918.854 -918.854] (1.000)
Step: 92749, Reward: [-471.873 -471.873 -471.873] [76.8678], Avg: [-918.654 -918.654 -918.654] (1.000)
Step: 92799, Reward: [-472.295 -472.295 -472.295] [72.1715], Avg: [-918.453 -918.453 -918.453] (1.000)
Step: 92849, Reward: [-394.426 -394.426 -394.426] [55.9606], Avg: [-918.201 -918.201 -918.201] (1.000)
Step: 92899, Reward: [-498.679 -498.679 -498.679] [50.9332], Avg: [-918.002 -918.002 -918.002] (1.000)
Step: 92949, Reward: [-457.635 -457.635 -457.635] [51.3517], Avg: [-917.782 -917.782 -917.782] (1.000)
Step: 92999, Reward: [-465.587 -465.587 -465.587] [93.3554], Avg: [-917.589 -917.589 -917.589] (1.000)
Step: 93049, Reward: [-473.382 -473.382 -473.382] [22.1792], Avg: [-917.362 -917.362 -917.362] (1.000)
Step: 93099, Reward: [-554.008 -554.008 -554.008] [50.7670], Avg: [-917.195 -917.195 -917.195] (1.000)
Step: 93149, Reward: [-707.831 -707.831 -707.831] [149.6502], Avg: [-917.163 -917.163 -917.163] (1.000)
Step: 93199, Reward: [-624.402 -624.402 -624.402] [88.9532], Avg: [-917.053 -917.053 -917.053] (1.000)
Step: 93249, Reward: [-769.318 -769.318 -769.318] [108.1691], Avg: [-917.032 -917.032 -917.032] (1.000)
Step: 93299, Reward: [-651.263 -651.263 -651.263] [75.1653], Avg: [-916.93 -916.93 -916.93] (1.000)
Step: 93349, Reward: [-856.263 -856.263 -856.263] [47.7381], Avg: [-916.923 -916.923 -916.923] (1.000)
Step: 93399, Reward: [-810.871 -810.871 -810.871] [169.1530], Avg: [-916.957 -916.957 -916.957] (1.000)
Step: 93449, Reward: [-646.363 -646.363 -646.363] [84.8063], Avg: [-916.857 -916.857 -916.857] (1.000)
Step: 93499, Reward: [-575.306 -575.306 -575.306] [77.4557], Avg: [-916.716 -916.716 -916.716] (1.000)
Step: 93549, Reward: [-661.766 -661.766 -661.766] [80.5786], Avg: [-916.623 -916.623 -916.623] (1.000)
Step: 93599, Reward: [-664.628 -664.628 -664.628] [56.3227], Avg: [-916.518 -916.518 -916.518] (1.000)
Step: 93649, Reward: [-455.834 -455.834 -455.834] [43.6000], Avg: [-916.296 -916.296 -916.296] (1.000)
Step: 93699, Reward: [-467.886 -467.886 -467.886] [85.6761], Avg: [-916.102 -916.102 -916.102] (1.000)
Step: 93749, Reward: [-434.535 -434.535 -434.535] [48.8201], Avg: [-915.871 -915.871 -915.871] (1.000)
Step: 93799, Reward: [-445.137 -445.137 -445.137] [66.2007], Avg: [-915.656 -915.656 -915.656] (1.000)
Step: 93849, Reward: [-452.882 -452.882 -452.882] [97.8100], Avg: [-915.461 -915.461 -915.461] (1.000)
Step: 93899, Reward: [-418.977 -418.977 -418.977] [29.5307], Avg: [-915.213 -915.213 -915.213] (1.000)
Step: 93949, Reward: [-500.344 -500.344 -500.344] [68.3825], Avg: [-915.028 -915.028 -915.028] (1.000)
Step: 93999, Reward: [-462.03 -462.03 -462.03] [66.4056], Avg: [-914.823 -914.823 -914.823] (1.000)
Step: 94049, Reward: [-384.283 -384.283 -384.283] [54.8868], Avg: [-914.57 -914.57 -914.57] (1.000)
Step: 94099, Reward: [-398.536 -398.536 -398.536] [58.6554], Avg: [-914.327 -914.327 -914.327] (1.000)
Step: 94149, Reward: [-440.677 -440.677 -440.677] [128.8569], Avg: [-914.144 -914.144 -914.144] (1.000)
Step: 94199, Reward: [-485.468 -485.468 -485.468] [160.2248], Avg: [-914.001 -914.001 -914.001] (1.000)
Step: 94249, Reward: [-392.502 -392.502 -392.502] [56.2846], Avg: [-913.754 -913.754 -913.754] (1.000)
Step: 94299, Reward: [-433.64 -433.64 -433.64] [30.6215], Avg: [-913.516 -913.516 -913.516] (1.000)
Step: 94349, Reward: [-459.96 -459.96 -459.96] [99.0693], Avg: [-913.328 -913.328 -913.328] (1.000)
Step: 94399, Reward: [-515.597 -515.597 -515.597] [149.1875], Avg: [-913.196 -913.196 -913.196] (1.000)
Step: 94449, Reward: [-401.294 -401.294 -401.294] [52.0789], Avg: [-912.953 -912.953 -912.953] (1.000)
Step: 94499, Reward: [-460.282 -460.282 -460.282] [104.2685], Avg: [-912.769 -912.769 -912.769] (1.000)
Step: 94549, Reward: [-531.02 -531.02 -531.02] [79.7639], Avg: [-912.609 -912.609 -912.609] (1.000)
Step: 94599, Reward: [-570.094 -570.094 -570.094] [143.9228], Avg: [-912.504 -912.504 -912.504] (1.000)
Step: 94649, Reward: [-637.339 -637.339 -637.339] [163.9286], Avg: [-912.445 -912.445 -912.445] (1.000)
Step: 94699, Reward: [-609.609 -609.609 -609.609] [121.7054], Avg: [-912.35 -912.35 -912.35] (1.000)
Step: 94749, Reward: [-490.241 -490.241 -490.241] [79.3174], Avg: [-912.169 -912.169 -912.169] (1.000)
Step: 94799, Reward: [-678.371 -678.371 -678.371] [126.1566], Avg: [-912.112 -912.112 -912.112] (1.000)
Step: 94849, Reward: [-528.608 -528.608 -528.608] [15.9772], Avg: [-911.918 -911.918 -911.918] (1.000)
Step: 94899, Reward: [-708.191 -708.191 -708.191] [146.2006], Avg: [-911.888 -911.888 -911.888] (1.000)
Step: 94949, Reward: [-592.497 -592.497 -592.497] [55.5952], Avg: [-911.749 -911.749 -911.749] (1.000)
Step: 94999, Reward: [-842.002 -842.002 -842.002] [331.5679], Avg: [-911.887 -911.887 -911.887] (1.000)
Step: 95049, Reward: [-779.483 -779.483 -779.483] [118.7553], Avg: [-911.88 -911.88 -911.88] (1.000)
Step: 95099, Reward: [-838.005 -838.005 -838.005] [344.3415], Avg: [-912.022 -912.022 -912.022] (1.000)
Step: 95149, Reward: [-581.716 -581.716 -581.716] [94.3959], Avg: [-911.898 -911.898 -911.898] (1.000)
Step: 95199, Reward: [-636.35 -636.35 -636.35] [107.8608], Avg: [-911.81 -911.81 -911.81] (1.000)
Step: 95249, Reward: [-932.05 -932.05 -932.05] [213.6149], Avg: [-911.932 -911.932 -911.932] (1.000)
Step: 95299, Reward: [-1340.211 -1340.211 -1340.211] [419.7361], Avg: [-912.377 -912.377 -912.377] (1.000)
Step: 95349, Reward: [-1217.478 -1217.478 -1217.478] [292.4030], Avg: [-912.691 -912.691 -912.691] (1.000)
Step: 95399, Reward: [-881.569 -881.569 -881.569] [74.1370], Avg: [-912.713 -912.713 -912.713] (1.000)
Step: 95449, Reward: [-696.015 -696.015 -696.015] [336.7430], Avg: [-912.776 -912.776 -912.776] (1.000)
Step: 95499, Reward: [-642.584 -642.584 -642.584] [362.5891], Avg: [-912.825 -912.825 -912.825] (1.000)
Step: 95549, Reward: [-672.517 -672.517 -672.517] [186.3568], Avg: [-912.796 -912.796 -912.796] (1.000)
Step: 95599, Reward: [-662.777 -662.777 -662.777] [262.7844], Avg: [-912.803 -912.803 -912.803] (1.000)
Step: 95649, Reward: [-694.639 -694.639 -694.639] [171.5732], Avg: [-912.779 -912.779 -912.779] (1.000)
Step: 95699, Reward: [-1019.079 -1019.079 -1019.079] [301.3968], Avg: [-912.992 -912.992 -912.992] (1.000)
Step: 95749, Reward: [-766.33 -766.33 -766.33] [187.0113], Avg: [-913.013 -913.013 -913.013] (1.000)
Step: 95799, Reward: [-786.297 -786.297 -786.297] [182.9430], Avg: [-913.042 -913.042 -913.042] (1.000)
Step: 95849, Reward: [-677.426 -677.426 -677.426] [155.5146], Avg: [-913. -913. -913.] (1.000)
Step: 95899, Reward: [-523.681 -523.681 -523.681] [98.4370], Avg: [-912.849 -912.849 -912.849] (1.000)
Step: 95949, Reward: [-609.392 -609.392 -609.392] [47.6313], Avg: [-912.715 -912.715 -912.715] (1.000)
Step: 95999, Reward: [-520.097 -520.097 -520.097] [99.0016], Avg: [-912.562 -912.562 -912.562] (1.000)
Step: 96049, Reward: [-519.596 -519.596 -519.596] [76.9762], Avg: [-912.398 -912.398 -912.398] (1.000)
Step: 96099, Reward: [-493.247 -493.247 -493.247] [81.9076], Avg: [-912.222 -912.222 -912.222] (1.000)
Step: 96149, Reward: [-450.296 -450.296 -450.296] [51.7660], Avg: [-912.009 -912.009 -912.009] (1.000)
Step: 96199, Reward: [-467.194 -467.194 -467.194] [34.5827], Avg: [-911.796 -911.796 -911.796] (1.000)
Step: 96249, Reward: [-459.168 -459.168 -459.168] [71.9456], Avg: [-911.598 -911.598 -911.598] (1.000)
Step: 96299, Reward: [-502.228 -502.228 -502.228] [99.8348], Avg: [-911.437 -911.437 -911.437] (1.000)
Step: 96349, Reward: [-448.761 -448.761 -448.761] [92.0884], Avg: [-911.245 -911.245 -911.245] (1.000)
Step: 96399, Reward: [-518.497 -518.497 -518.497] [105.1354], Avg: [-911.096 -911.096 -911.096] (1.000)
Step: 96449, Reward: [-502.673 -502.673 -502.673] [115.2266], Avg: [-910.944 -910.944 -910.944] (1.000)
Step: 96499, Reward: [-544.085 -544.085 -544.085] [85.4920], Avg: [-910.798 -910.798 -910.798] (1.000)
Step: 96549, Reward: [-504.274 -504.274 -504.274] [57.6062], Avg: [-910.617 -910.617 -910.617] (1.000)
Step: 96599, Reward: [-545.326 -545.326 -545.326] [142.1626], Avg: [-910.502 -910.502 -910.502] (1.000)
Step: 96649, Reward: [-537.036 -537.036 -537.036] [133.0013], Avg: [-910.378 -910.378 -910.378] (1.000)
Step: 96699, Reward: [-486.23 -486.23 -486.23] [54.1600], Avg: [-910.186 -910.186 -910.186] (1.000)
Step: 96749, Reward: [-524.98 -524.98 -524.98] [126.5464], Avg: [-910.053 -910.053 -910.053] (1.000)
Step: 96799, Reward: [-498.804 -498.804 -498.804] [112.9241], Avg: [-909.898 -909.898 -909.898] (1.000)
Step: 96849, Reward: [-510.084 -510.084 -510.084] [103.9457], Avg: [-909.746 -909.746 -909.746] (1.000)
Step: 96899, Reward: [-445.234 -445.234 -445.234] [65.5377], Avg: [-909.54 -909.54 -909.54] (1.000)
Step: 96949, Reward: [-516.544 -516.544 -516.544] [136.4399], Avg: [-909.408 -909.408 -909.408] (1.000)
Step: 96999, Reward: [-539.273 -539.273 -539.273] [26.5140], Avg: [-909.23 -909.23 -909.23] (1.000)
Step: 97049, Reward: [-437.248 -437.248 -437.248] [85.4305], Avg: [-909.031 -909.031 -909.031] (1.000)
Step: 97099, Reward: [-457.772 -457.772 -457.772] [90.8689], Avg: [-908.846 -908.846 -908.846] (1.000)
Step: 97149, Reward: [-487.887 -487.887 -487.887] [127.4462], Avg: [-908.695 -908.695 -908.695] (1.000)
Step: 97199, Reward: [-539.76 -539.76 -539.76] [90.1680], Avg: [-908.551 -908.551 -908.551] (1.000)
Step: 97249, Reward: [-548.187 -548.187 -548.187] [99.9325], Avg: [-908.417 -908.417 -908.417] (1.000)
Step: 97299, Reward: [-454.959 -454.959 -454.959] [39.2733], Avg: [-908.205 -908.205 -908.205] (1.000)
Step: 97349, Reward: [-616. -616. -616.] [121.5756], Avg: [-908.117 -908.117 -908.117] (1.000)
Step: 97399, Reward: [-594.267 -594.267 -594.267] [181.5313], Avg: [-908.049 -908.049 -908.049] (1.000)
Step: 97449, Reward: [-537.353 -537.353 -537.353] [45.0123], Avg: [-907.882 -907.882 -907.882] (1.000)
Step: 97499, Reward: [-580.674 -580.674 -580.674] [59.0464], Avg: [-907.744 -907.744 -907.744] (1.000)
Step: 97549, Reward: [-509.497 -509.497 -509.497] [71.8249], Avg: [-907.577 -907.577 -907.577] (1.000)
Step: 97599, Reward: [-454.661 -454.661 -454.661] [33.0867], Avg: [-907.362 -907.362 -907.362] (1.000)
Step: 97649, Reward: [-625.098 -625.098 -625.098] [118.8802], Avg: [-907.278 -907.278 -907.278] (1.000)
Step: 97699, Reward: [-526.063 -526.063 -526.063] [99.0840], Avg: [-907.134 -907.134 -907.134] (1.000)
Step: 97749, Reward: [-503.505 -503.505 -503.505] [54.1515], Avg: [-906.955 -906.955 -906.955] (1.000)
Step: 97799, Reward: [-457.359 -457.359 -457.359] [71.2651], Avg: [-906.762 -906.762 -906.762] (1.000)
Step: 97849, Reward: [-511.501 -511.501 -511.501] [88.9545], Avg: [-906.605 -906.605 -906.605] (1.000)
Step: 97899, Reward: [-485.323 -485.323 -485.323] [129.4618], Avg: [-906.456 -906.456 -906.456] (1.000)
Step: 97949, Reward: [-503.155 -503.155 -503.155] [102.4048], Avg: [-906.303 -906.303 -906.303] (1.000)
Step: 97999, Reward: [-487.337 -487.337 -487.337] [52.1698], Avg: [-906.115 -906.115 -906.115] (1.000)
Step: 98049, Reward: [-474.722 -474.722 -474.722] [31.5297], Avg: [-905.912 -905.912 -905.912] (1.000)
Step: 98099, Reward: [-453.413 -453.413 -453.413] [91.0898], Avg: [-905.727 -905.727 -905.727] (1.000)
Step: 98149, Reward: [-552.402 -552.402 -552.402] [114.3733], Avg: [-905.606 -905.606 -905.606] (1.000)
Step: 98199, Reward: [-445.909 -445.909 -445.909] [90.9154], Avg: [-905.418 -905.418 -905.418] (1.000)
Step: 98249, Reward: [-469.809 -469.809 -469.809] [83.3905], Avg: [-905.239 -905.239 -905.239] (1.000)
Step: 98299, Reward: [-459.868 -459.868 -459.868] [95.2776], Avg: [-905.06 -905.06 -905.06] (1.000)
Step: 98349, Reward: [-475.296 -475.296 -475.296] [90.1893], Avg: [-904.888 -904.888 -904.888] (1.000)
Step: 98399, Reward: [-543.145 -543.145 -543.145] [134.3444], Avg: [-904.772 -904.772 -904.772] (1.000)
Step: 98449, Reward: [-587.868 -587.868 -587.868] [132.2026], Avg: [-904.679 -904.679 -904.679] (1.000)
Step: 98499, Reward: [-591.925 -591.925 -591.925] [119.0894], Avg: [-904.58 -904.58 -904.58] (1.000)
Step: 98549, Reward: [-426.497 -426.497 -426.497] [27.4379], Avg: [-904.352 -904.352 -904.352] (1.000)
Step: 98599, Reward: [-505.643 -505.643 -505.643] [35.4420], Avg: [-904.167 -904.167 -904.167] (1.000)
Step: 98649, Reward: [-466.326 -466.326 -466.326] [42.2995], Avg: [-903.967 -903.967 -903.967] (1.000)
Step: 98699, Reward: [-497.095 -497.095 -497.095] [45.0031], Avg: [-903.784 -903.784 -903.784] (1.000)
Step: 98749, Reward: [-454.314 -454.314 -454.314] [59.8727], Avg: [-903.586 -903.586 -903.586] (1.000)
Step: 98799, Reward: [-466.815 -466.815 -466.815] [89.8214], Avg: [-903.411 -903.411 -903.411] (1.000)
Step: 98849, Reward: [-536.62 -536.62 -536.62] [153.5655], Avg: [-903.303 -903.303 -903.303] (1.000)
Step: 98899, Reward: [-471.353 -471.353 -471.353] [39.5994], Avg: [-903.105 -903.105 -903.105] (1.000)
Step: 98949, Reward: [-432.98 -432.98 -432.98] [75.4494], Avg: [-902.905 -902.905 -902.905] (1.000)
Step: 98999, Reward: [-426.98 -426.98 -426.98] [44.2946], Avg: [-902.687 -902.687 -902.687] (1.000)
Step: 99049, Reward: [-518.254 -518.254 -518.254] [74.1242], Avg: [-902.53 -902.53 -902.53] (1.000)
Step: 99099, Reward: [-451.1 -451.1 -451.1] [44.5562], Avg: [-902.325 -902.325 -902.325] (1.000)
Step: 99149, Reward: [-480.722 -480.722 -480.722] [48.7601], Avg: [-902.137 -902.137 -902.137] (1.000)
Step: 99199, Reward: [-506.539 -506.539 -506.539] [32.4369], Avg: [-901.954 -901.954 -901.954] (1.000)
Step: 99249, Reward: [-470.714 -470.714 -470.714] [88.4186], Avg: [-901.781 -901.781 -901.781] (1.000)
Step: 99299, Reward: [-433.314 -433.314 -433.314] [142.9481], Avg: [-901.617 -901.617 -901.617] (1.000)
Step: 99349, Reward: [-406.791 -406.791 -406.791] [79.0296], Avg: [-901.408 -901.408 -901.408] (1.000)
Step: 99399, Reward: [-504.876 -504.876 -504.876] [162.4799], Avg: [-901.29 -901.29 -901.29] (1.000)
Step: 99449, Reward: [-477.624 -477.624 -477.624] [77.3746], Avg: [-901.116 -901.116 -901.116] (1.000)
Step: 99499, Reward: [-492.054 -492.054 -492.054] [62.6777], Avg: [-900.942 -900.942 -900.942] (1.000)
Step: 99549, Reward: [-532.532 -532.532 -532.532] [52.0354], Avg: [-900.783 -900.783 -900.783] (1.000)
Step: 99599, Reward: [-480.479 -480.479 -480.479] [40.4618], Avg: [-900.593 -900.593 -900.593] (1.000)
Step: 99649, Reward: [-481.477 -481.477 -481.477] [92.7576], Avg: [-900.429 -900.429 -900.429] (1.000)
Step: 99699, Reward: [-464.483 -464.483 -464.483] [26.2719], Avg: [-900.224 -900.224 -900.224] (1.000)
Step: 99749, Reward: [-508.419 -508.419 -508.419] [45.0833], Avg: [-900.05 -900.05 -900.05] (1.000)
Step: 99799, Reward: [-451.011 -451.011 -451.011] [62.1737], Avg: [-899.856 -899.856 -899.856] (1.000)
Step: 99849, Reward: [-447.794 -447.794 -447.794] [75.0881], Avg: [-899.667 -899.667 -899.667] (1.000)
Step: 99899, Reward: [-417.475 -417.475 -417.475] [34.4591], Avg: [-899.443 -899.443 -899.443] (1.000)
Step: 99949, Reward: [-506.664 -506.664 -506.664] [50.6787], Avg: [-899.272 -899.272 -899.272] (1.000)
Step: 99999, Reward: [-470.991 -470.991 -470.991] [37.8489], Avg: [-899.077 -899.077 -899.077] (1.000)
