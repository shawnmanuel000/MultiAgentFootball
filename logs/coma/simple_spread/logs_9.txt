Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 512
EPS_MAX = 1.0
EPS_MIN = 0.01
EPS_DECAY = 0.999
NUM_ENVS = 16
EPISODE_LIMIT = 500
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		self.episode_batch.update({"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy(), actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			self.episode_batch.update({"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
		self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

	def save_models(self, path):
		self.mac.save_models(path)
		torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
		torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
		torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

	def load_models(self, path):
		self.mac.load_models(path)
		self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
		self.target_critic.load_state_dict(self.critic.state_dict())
		self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
		self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def load_state(self, other_mac):
		self.agent.load_state_dict(other_mac.agent.state_dict())

	def cuda(self):
		self.agent.cuda()

	def save_models(self, path):
		torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

	def load_models(self, path):
		self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		h = self.rnn(x, h_in)
		q = self.fc2(h)
		return q, h

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(lambda: make_env(env_name), ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-503.567 -503.567 -503.567] [120.4943], Avg: [-624.061 -624.061 -624.061] (1.000)
Step: 99, Reward: [-491.128 -491.128 -491.128] [93.6106], Avg: [-604.4 -604.4 -604.4] (1.000)
Step: 149, Reward: [-507.745 -507.745 -507.745] [57.2924], Avg: [-591.279 -591.279 -591.279] (1.000)
Step: 199, Reward: [-526.073 -526.073 -526.073] [97.7049], Avg: [-599.404 -599.404 -599.404] (1.000)
Step: 249, Reward: [-518.888 -518.888 -518.888] [46.5920], Avg: [-592.619 -592.619 -592.619] (1.000)
Step: 299, Reward: [-481.032 -481.032 -481.032] [80.2321], Avg: [-587.393 -587.393 -587.393] (1.000)
Step: 349, Reward: [-430.922 -430.922 -430.922] [27.7898], Avg: [-569.01 -569.01 -569.01] (1.000)
Step: 399, Reward: [-459.498 -459.498 -459.498] [74.3467], Avg: [-564.615 -564.615 -564.615] (1.000)
Step: 449, Reward: [-502.813 -502.813 -502.813] [98.0576], Avg: [-568.643 -568.643 -568.643] (1.000)
Step: 499, Reward: [-453.409 -453.409 -453.409] [97.3247], Avg: [-566.852 -566.852 -566.852] (1.000)
Step: 549, Reward: [-462.755 -462.755 -462.755] [133.9663], Avg: [-569.568 -569.568 -569.568] (1.000)
Step: 599, Reward: [-476.241 -476.241 -476.241] [34.9357], Avg: [-564.702 -564.702 -564.702] (1.000)
Step: 649, Reward: [-458.627 -458.627 -458.627] [58.4544], Avg: [-561.039 -561.039 -561.039] (1.000)
Step: 699, Reward: [-516.733 -516.733 -516.733] [40.2302], Avg: [-560.747 -560.747 -560.747] (1.000)
Step: 749, Reward: [-488.869 -488.869 -488.869] [28.9414], Avg: [-557.885 -557.885 -557.885] (1.000)
Step: 799, Reward: [-476.216 -476.216 -476.216] [76.1000], Avg: [-557.537 -557.537 -557.537] (1.000)
Step: 849, Reward: [-586.487 -586.487 -586.487] [185.2749], Avg: [-570.138 -570.138 -570.138] (1.000)
Step: 899, Reward: [-489.397 -489.397 -489.397] [83.6460], Avg: [-570.3 -570.3 -570.3] (1.000)
Step: 949, Reward: [-422.17 -422.17 -422.17] [60.0838], Avg: [-565.666 -565.666 -565.666] (1.000)
Step: 999, Reward: [-452.398 -452.398 -452.398] [92.9445], Avg: [-564.65 -564.65 -564.65] (1.000)
Step: 1049, Reward: [-485.017 -485.017 -485.017] [48.9618], Avg: [-563.189 -563.189 -563.189] (1.000)
Step: 1099, Reward: [-525.472 -525.472 -525.472] [141.3330], Avg: [-567.899 -567.899 -567.899] (1.000)
Step: 1149, Reward: [-456.416 -456.416 -456.416] [66.7531], Avg: [-565.954 -565.954 -565.954] (1.000)
Step: 1199, Reward: [-531.858 -531.858 -531.858] [144.0405], Avg: [-570.535 -570.535 -570.535] (1.000)
Step: 1249, Reward: [-490.592 -490.592 -490.592] [71.5947], Avg: [-570.201 -570.201 -570.201] (1.000)
Step: 1299, Reward: [-531.685 -531.685 -531.685] [89.7302], Avg: [-572.171 -572.171 -572.171] (1.000)
Step: 1349, Reward: [-456.474 -456.474 -456.474] [65.7881], Avg: [-570.322 -570.322 -570.322] (1.000)
Step: 1399, Reward: [-513.95 -513.95 -513.95] [50.6062], Avg: [-570.117 -570.117 -570.117] (1.000)
Step: 1449, Reward: [-447.203 -447.203 -447.203] [69.2134], Avg: [-568.265 -568.265 -568.265] (1.000)
Step: 1499, Reward: [-417.267 -417.267 -417.267] [59.3136], Avg: [-565.209 -565.209 -565.209] (1.000)
Step: 1549, Reward: [-486.025 -486.025 -486.025] [29.6378], Avg: [-563.61 -563.61 -563.61] (1.000)
Step: 1599, Reward: [-507.547 -507.547 -507.547] [117.7845], Avg: [-565.539 -565.539 -565.539] (1.000)
Step: 1649, Reward: [-699.967 -699.967 -699.967] [224.8850], Avg: [-576.427 -576.427 -576.427] (1.000)
Step: 1699, Reward: [-506.546 -506.546 -506.546] [109.9934], Avg: [-577.607 -577.607 -577.607] (1.000)
Step: 1749, Reward: [-502.947 -502.947 -502.947] [46.7665], Avg: [-576.81 -576.81 -576.81] (1.000)
Step: 1799, Reward: [-447.022 -447.022 -447.022] [37.2924], Avg: [-574.241 -574.241 -574.241] (1.000)
Step: 1849, Reward: [-475.842 -475.842 -475.842] [86.7609], Avg: [-573.926 -573.926 -573.926] (1.000)
Step: 1899, Reward: [-507.981 -507.981 -507.981] [106.2346], Avg: [-574.987 -574.987 -574.987] (1.000)
Step: 1949, Reward: [-462.533 -462.533 -462.533] [68.1356], Avg: [-573.85 -573.85 -573.85] (1.000)
Step: 1999, Reward: [-562.13 -562.13 -562.13] [180.2798], Avg: [-578.064 -578.064 -578.064] (1.000)
Step: 2049, Reward: [-469.246 -469.246 -469.246] [62.2622], Avg: [-576.929 -576.929 -576.929] (1.000)
Step: 2099, Reward: [-530.488 -530.488 -530.488] [78.1053], Avg: [-577.683 -577.683 -577.683] (1.000)
Step: 2149, Reward: [-594.875 -594.875 -594.875] [39.3092], Avg: [-578.997 -578.997 -578.997] (1.000)
Step: 2199, Reward: [-554.694 -554.694 -554.694] [50.2571], Avg: [-579.587 -579.587 -579.587] (1.000)
Step: 2249, Reward: [-487.027 -487.027 -487.027] [67.3733], Avg: [-579.027 -579.027 -579.027] (1.000)
Step: 2299, Reward: [-490.595 -490.595 -490.595] [102.7186], Avg: [-579.337 -579.337 -579.337] (1.000)
Step: 2349, Reward: [-474.148 -474.148 -474.148] [75.3694], Avg: [-578.703 -578.703 -578.703] (1.000)
Step: 2399, Reward: [-467.572 -467.572 -467.572] [53.4916], Avg: [-577.502 -577.502 -577.502] (1.000)
Step: 2449, Reward: [-601.314 -601.314 -601.314] [116.4366], Avg: [-580.364 -580.364 -580.364] (1.000)
Step: 2499, Reward: [-482.676 -482.676 -482.676] [57.3089], Avg: [-579.557 -579.557 -579.557] (1.000)
Step: 2549, Reward: [-438.221 -438.221 -438.221] [49.7106], Avg: [-577.76 -577.76 -577.76] (1.000)
Step: 2599, Reward: [-506.473 -506.473 -506.473] [123.3381], Avg: [-578.761 -578.761 -578.761] (1.000)
Step: 2649, Reward: [-475.6 -475.6 -475.6] [101.7547], Avg: [-578.735 -578.735 -578.735] (1.000)
Step: 2699, Reward: [-475.79 -475.79 -475.79] [44.0691], Avg: [-577.644 -577.644 -577.644] (1.000)
Step: 2749, Reward: [-465.432 -465.432 -465.432] [73.9351], Avg: [-576.948 -576.948 -576.948] (1.000)
Step: 2799, Reward: [-502.253 -502.253 -502.253] [70.3405], Avg: [-576.871 -576.871 -576.871] (1.000)
Step: 2849, Reward: [-450.634 -450.634 -450.634] [58.6306], Avg: [-575.685 -575.685 -575.685] (1.000)
Step: 2899, Reward: [-497.063 -497.063 -497.063] [71.6736], Avg: [-575.565 -575.565 -575.565] (1.000)
Step: 2949, Reward: [-493.652 -493.652 -493.652] [50.1309], Avg: [-575.026 -575.026 -575.026] (1.000)
Step: 2999, Reward: [-499.161 -499.161 -499.161] [106.9257], Avg: [-575.544 -575.544 -575.544] (1.000)
Step: 3049, Reward: [-451.85 -451.85 -451.85] [36.4695], Avg: [-574.114 -574.114 -574.114] (1.000)
Step: 3099, Reward: [-435.297 -435.297 -435.297] [108.7883], Avg: [-573.63 -573.63 -573.63] (1.000)
Step: 3149, Reward: [-486.844 -486.844 -486.844] [67.5244], Avg: [-573.324 -573.324 -573.324] (1.000)
Step: 3199, Reward: [-512.315 -512.315 -512.315] [31.3871], Avg: [-572.861 -572.861 -572.861] (1.000)
Step: 3249, Reward: [-556.024 -556.024 -556.024] [103.8906], Avg: [-574.2 -574.2 -574.2] (1.000)
Step: 3299, Reward: [-483.019 -483.019 -483.019] [70.7470], Avg: [-573.891 -573.891 -573.891] (1.000)
Step: 3349, Reward: [-462.444 -462.444 -462.444] [81.9055], Avg: [-573.45 -573.45 -573.45] (1.000)
Step: 3399, Reward: [-439.267 -439.267 -439.267] [44.1709], Avg: [-572.126 -572.126 -572.126] (1.000)
Step: 3449, Reward: [-501.825 -501.825 -501.825] [56.4985], Avg: [-571.926 -571.926 -571.926] (1.000)
Step: 3499, Reward: [-522.68 -522.68 -522.68] [107.6072], Avg: [-572.76 -572.76 -572.76] (1.000)
Step: 3549, Reward: [-526.714 -526.714 -526.714] [97.2598], Avg: [-573.481 -573.481 -573.481] (1.000)
Step: 3599, Reward: [-475.494 -475.494 -475.494] [42.8077], Avg: [-572.715 -572.715 -572.715] (1.000)
Step: 3649, Reward: [-482.987 -482.987 -482.987] [51.6623], Avg: [-572.193 -572.193 -572.193] (1.000)
Step: 3699, Reward: [-554.998 -554.998 -554.998] [229.9719], Avg: [-575.069 -575.069 -575.069] (1.000)
Step: 3749, Reward: [-509.146 -509.146 -509.146] [94.4737], Avg: [-575.449 -575.449 -575.449] (1.000)
Step: 3799, Reward: [-436.15 -436.15 -436.15] [30.2926], Avg: [-574.015 -574.015 -574.015] (1.000)
Step: 3849, Reward: [-478.511 -478.511 -478.511] [91.3672], Avg: [-573.961 -573.961 -573.961] (1.000)
Step: 3899, Reward: [-495.193 -495.193 -495.193] [49.4785], Avg: [-573.586 -573.586 -573.586] (1.000)
Step: 3949, Reward: [-492.873 -492.873 -492.873] [46.1733], Avg: [-573.149 -573.149 -573.149] (1.000)
Step: 3999, Reward: [-536.676 -536.676 -536.676] [61.6187], Avg: [-573.463 -573.463 -573.463] (1.000)
Step: 4049, Reward: [-467.245 -467.245 -467.245] [36.9784], Avg: [-572.608 -572.608 -572.608] (1.000)
Step: 4099, Reward: [-422.271 -422.271 -422.271] [48.2293], Avg: [-571.363 -571.363 -571.363] (1.000)
Step: 4149, Reward: [-500.08 -500.08 -500.08] [91.5483], Avg: [-571.607 -571.607 -571.607] (1.000)
Step: 4199, Reward: [-489.031 -489.031 -489.031] [52.1763], Avg: [-571.245 -571.245 -571.245] (1.000)
Step: 4249, Reward: [-542.115 -542.115 -542.115] [97.2525], Avg: [-572.047 -572.047 -572.047] (1.000)
Step: 4299, Reward: [-524.894 -524.894 -524.894] [127.5901], Avg: [-572.982 -572.982 -572.982] (1.000)
Step: 4349, Reward: [-462.08 -462.08 -462.08] [78.3643], Avg: [-572.608 -572.608 -572.608] (1.000)
Step: 4399, Reward: [-528.629 -528.629 -528.629] [93.6856], Avg: [-573.173 -573.173 -573.173] (1.000)
Step: 4449, Reward: [-541.93 -541.93 -541.93] [108.0965], Avg: [-574.036 -574.036 -574.036] (1.000)
Step: 4499, Reward: [-496.006 -496.006 -496.006] [53.9869], Avg: [-573.769 -573.769 -573.769] (1.000)
Step: 4549, Reward: [-496.611 -496.611 -496.611] [67.5618], Avg: [-573.664 -573.664 -573.664] (1.000)
Step: 4599, Reward: [-434.15 -434.15 -434.15] [22.6744], Avg: [-572.394 -572.394 -572.394] (1.000)
Step: 4649, Reward: [-464.958 -464.958 -464.958] [60.8734], Avg: [-571.893 -571.893 -571.893] (1.000)
Step: 4699, Reward: [-479.599 -479.599 -479.599] [123.6738], Avg: [-572.227 -572.227 -572.227] (1.000)
Step: 4749, Reward: [-506.467 -506.467 -506.467] [73.2024], Avg: [-572.305 -572.305 -572.305] (1.000)
Step: 4799, Reward: [-483.045 -483.045 -483.045] [129.3933], Avg: [-572.723 -572.723 -572.723] (1.000)
Step: 4849, Reward: [-468.892 -468.892 -468.892] [109.6518], Avg: [-572.783 -572.783 -572.783] (1.000)
Step: 4899, Reward: [-490.092 -490.092 -490.092] [74.6098], Avg: [-572.701 -572.701 -572.701] (1.000)
Step: 4949, Reward: [-489.362 -489.362 -489.362] [85.9706], Avg: [-572.727 -572.727 -572.727] (1.000)
Step: 4999, Reward: [-456.296 -456.296 -456.296] [62.4475], Avg: [-572.187 -572.187 -572.187] (1.000)
Step: 5049, Reward: [-454.892 -454.892 -454.892] [78.8348], Avg: [-571.807 -571.807 -571.807] (1.000)
Step: 5099, Reward: [-523.008 -523.008 -523.008] [81.6686], Avg: [-572.129 -572.129 -572.129] (1.000)
Step: 5149, Reward: [-504.859 -504.859 -504.859] [64.9938], Avg: [-572.107 -572.107 -572.107] (1.000)
Step: 5199, Reward: [-440.123 -440.123 -440.123] [71.5790], Avg: [-571.526 -571.526 -571.526] (1.000)
Step: 5249, Reward: [-479.137 -479.137 -479.137] [62.2680], Avg: [-571.239 -571.239 -571.239] (1.000)
Step: 5299, Reward: [-427.358 -427.358 -427.358] [41.1442], Avg: [-570.27 -570.27 -570.27] (1.000)
Step: 5349, Reward: [-525.106 -525.106 -525.106] [50.1086], Avg: [-570.316 -570.316 -570.316] (1.000)
Step: 5399, Reward: [-480.781 -480.781 -480.781] [84.4737], Avg: [-570.269 -570.269 -570.269] (1.000)
Step: 5449, Reward: [-573.956 -573.956 -573.956] [109.7372], Avg: [-571.31 -571.31 -571.31] (1.000)
Step: 5499, Reward: [-476.443 -476.443 -476.443] [83.6872], Avg: [-571.208 -571.208 -571.208] (1.000)
Step: 5549, Reward: [-467.539 -467.539 -467.539] [72.0846], Avg: [-570.924 -570.924 -570.924] (1.000)
Step: 5599, Reward: [-429.672 -429.672 -429.672] [53.9842], Avg: [-570.145 -570.145 -570.145] (1.000)
Step: 5649, Reward: [-527.795 -527.795 -527.795] [111.3067], Avg: [-570.755 -570.755 -570.755] (1.000)
Step: 5699, Reward: [-487.486 -487.486 -487.486] [72.2695], Avg: [-570.658 -570.658 -570.658] (1.000)
Step: 5749, Reward: [-523.988 -523.988 -523.988] [109.0798], Avg: [-571.201 -571.201 -571.201] (1.000)
Step: 5799, Reward: [-432.703 -432.703 -432.703] [52.9055], Avg: [-570.463 -570.463 -570.463] (1.000)
Step: 5849, Reward: [-500.737 -500.737 -500.737] [83.3452], Avg: [-570.579 -570.579 -570.579] (1.000)
Step: 5899, Reward: [-460.03 -460.03 -460.03] [65.6702], Avg: [-570.199 -570.199 -570.199] (1.000)
Step: 5949, Reward: [-600.895 -600.895 -600.895] [69.4872], Avg: [-571.041 -571.041 -571.041] (1.000)
Step: 5999, Reward: [-466.745 -466.745 -466.745] [38.9070], Avg: [-570.496 -570.496 -570.496] (1.000)
Step: 6049, Reward: [-547.918 -547.918 -547.918] [84.0329], Avg: [-571.004 -571.004 -571.004] (1.000)
Step: 6099, Reward: [-445.124 -445.124 -445.124] [62.1659], Avg: [-570.482 -570.482 -570.482] (1.000)
Step: 6149, Reward: [-448.642 -448.642 -448.642] [39.3816], Avg: [-569.811 -569.811 -569.811] (1.000)
Step: 6199, Reward: [-510.212 -510.212 -510.212] [77.4802], Avg: [-569.956 -569.956 -569.956] (1.000)
Step: 6249, Reward: [-466.927 -466.927 -466.927] [41.1963], Avg: [-569.461 -569.461 -569.461] (1.000)
Step: 6299, Reward: [-447.321 -447.321 -447.321] [52.6066], Avg: [-568.909 -568.909 -568.909] (1.000)
Step: 6349, Reward: [-424.317 -424.317 -424.317] [62.6605], Avg: [-568.264 -568.264 -568.264] (1.000)
Step: 6399, Reward: [-547.334 -547.334 -547.334] [82.5943], Avg: [-568.746 -568.746 -568.746] (1.000)
Step: 6449, Reward: [-523.34 -523.34 -523.34] [90.6709], Avg: [-569.097 -569.097 -569.097] (1.000)
Step: 6499, Reward: [-428.293 -428.293 -428.293] [47.0579], Avg: [-568.375 -568.375 -568.375] (1.000)
Step: 6549, Reward: [-500.996 -500.996 -500.996] [52.2143], Avg: [-568.26 -568.26 -568.26] (1.000)
Step: 6599, Reward: [-507.395 -507.395 -507.395] [51.9504], Avg: [-568.192 -568.192 -568.192] (1.000)
Step: 6649, Reward: [-591.825 -591.825 -591.825] [137.2204], Avg: [-569.402 -569.402 -569.402] (1.000)
Step: 6699, Reward: [-428.084 -428.084 -428.084] [46.8875], Avg: [-568.697 -568.697 -568.697] (1.000)
Step: 6749, Reward: [-516.548 -516.548 -516.548] [79.4405], Avg: [-568.899 -568.899 -568.899] (1.000)
Step: 6799, Reward: [-560.328 -560.328 -560.328] [54.2215], Avg: [-569.235 -569.235 -569.235] (1.000)
Step: 6849, Reward: [-513.62 -513.62 -513.62] [26.6467], Avg: [-569.023 -569.023 -569.023] (1.000)
Step: 6899, Reward: [-560.67 -560.67 -560.67] [64.0269], Avg: [-569.427 -569.427 -569.427] (1.000)
Step: 6949, Reward: [-535.438 -535.438 -535.438] [60.2238], Avg: [-569.615 -569.615 -569.615] (1.000)
Step: 6999, Reward: [-462.336 -462.336 -462.336] [77.0368], Avg: [-569.399 -569.399 -569.399] (1.000)
Step: 7049, Reward: [-470.839 -470.839 -470.839] [98.6115], Avg: [-569.4 -569.4 -569.4] (1.000)
Step: 7099, Reward: [-547.383 -547.383 -547.383] [66.5634], Avg: [-569.713 -569.713 -569.713] (1.000)
Step: 7149, Reward: [-519.05 -519.05 -519.05] [116.6565], Avg: [-570.175 -570.175 -570.175] (1.000)
Step: 7199, Reward: [-459.262 -459.262 -459.262] [74.3074], Avg: [-569.921 -569.921 -569.921] (1.000)
Step: 7249, Reward: [-457.251 -457.251 -457.251] [93.6190], Avg: [-569.789 -569.789 -569.789] (1.000)
Step: 7299, Reward: [-481.597 -481.597 -481.597] [101.9430], Avg: [-569.884 -569.884 -569.884] (1.000)
Step: 7349, Reward: [-473.919 -473.919 -473.919] [77.9636], Avg: [-569.761 -569.761 -569.761] (1.000)
Step: 7399, Reward: [-477.294 -477.294 -477.294] [77.5274], Avg: [-569.66 -569.66 -569.66] (1.000)
Step: 7449, Reward: [-473.452 -473.452 -473.452] [47.5861], Avg: [-569.334 -569.334 -569.334] (1.000)
Step: 7499, Reward: [-494.985 -494.985 -494.985] [68.9616], Avg: [-569.298 -569.298 -569.298] (1.000)
Step: 7549, Reward: [-557.3 -557.3 -557.3] [126.6506], Avg: [-570.057 -570.057 -570.057] (1.000)
Step: 7599, Reward: [-445.907 -445.907 -445.907] [48.4783], Avg: [-569.559 -569.559 -569.559] (1.000)
Step: 7649, Reward: [-527.4 -527.4 -527.4] [83.1085], Avg: [-569.827 -569.827 -569.827] (1.000)
Step: 7699, Reward: [-474.346 -474.346 -474.346] [159.2223], Avg: [-570.241 -570.241 -570.241] (1.000)
Step: 7749, Reward: [-432.219 -432.219 -432.219] [60.2548], Avg: [-569.739 -569.739 -569.739] (1.000)
Step: 7799, Reward: [-463.069 -463.069 -463.069] [46.4101], Avg: [-569.353 -569.353 -569.353] (1.000)
Step: 7849, Reward: [-528.603 -528.603 -528.603] [99.8530], Avg: [-569.729 -569.729 -569.729] (1.000)
Step: 7899, Reward: [-514.268 -514.268 -514.268] [132.9231], Avg: [-570.22 -570.22 -570.22] (1.000)
Step: 7949, Reward: [-454.376 -454.376 -454.376] [26.3351], Avg: [-569.657 -569.657 -569.657] (1.000)
Step: 7999, Reward: [-449.777 -449.777 -449.777] [50.3299], Avg: [-569.222 -569.222 -569.222] (1.000)
Step: 8049, Reward: [-466.182 -466.182 -466.182] [55.6247], Avg: [-568.927 -568.927 -568.927] (1.000)
Step: 8099, Reward: [-527.271 -527.271 -527.271] [116.3508], Avg: [-569.389 -569.389 -569.389] (1.000)
Step: 8149, Reward: [-527.696 -527.696 -527.696] [107.4805], Avg: [-569.792 -569.792 -569.792] (1.000)
Step: 8199, Reward: [-449.869 -449.869 -449.869] [78.8827], Avg: [-569.542 -569.542 -569.542] (1.000)
Step: 8249, Reward: [-451.078 -451.078 -451.078] [81.3137], Avg: [-569.317 -569.317 -569.317] (1.000)
Step: 8299, Reward: [-572.637 -572.637 -572.637] [83.9135], Avg: [-569.842 -569.842 -569.842] (1.000)
Step: 8349, Reward: [-476.47 -476.47 -476.47] [75.2181], Avg: [-569.734 -569.734 -569.734] (1.000)
Step: 8399, Reward: [-478.291 -478.291 -478.291] [33.0208], Avg: [-569.386 -569.386 -569.386] (1.000)
Step: 8449, Reward: [-565.638 -565.638 -565.638] [112.9700], Avg: [-570.032 -570.032 -570.032] (1.000)
Step: 8499, Reward: [-491.884 -491.884 -491.884] [93.3570], Avg: [-570.122 -570.122 -570.122] (1.000)
Step: 8549, Reward: [-482.271 -482.271 -482.271] [36.4355], Avg: [-569.821 -569.821 -569.821] (1.000)
Step: 8599, Reward: [-549.532 -549.532 -549.532] [125.8790], Avg: [-570.435 -570.435 -570.435] (1.000)
Step: 8649, Reward: [-451.23 -451.23 -451.23] [105.2981], Avg: [-570.354 -570.354 -570.354] (1.000)
Step: 8699, Reward: [-528. -528. -528.] [95.8093], Avg: [-570.662 -570.662 -570.662] (1.000)
Step: 8749, Reward: [-438.639 -438.639 -438.639] [68.9142], Avg: [-570.301 -570.301 -570.301] (1.000)
Step: 8799, Reward: [-542.778 -542.778 -542.778] [33.2601], Avg: [-570.334 -570.334 -570.334] (1.000)
Step: 8849, Reward: [-433.817 -433.817 -433.817] [88.3511], Avg: [-570.061 -570.061 -570.061] (1.000)
Step: 8899, Reward: [-441.634 -441.634 -441.634] [53.7231], Avg: [-569.642 -569.642 -569.642] (1.000)
Step: 8949, Reward: [-487.232 -487.232 -487.232] [74.1962], Avg: [-569.596 -569.596 -569.596] (1.000)
Step: 8999, Reward: [-414.855 -414.855 -414.855] [66.1432], Avg: [-569.104 -569.104 -569.104] (1.000)
Step: 9049, Reward: [-502.147 -502.147 -502.147] [61.0600], Avg: [-569.071 -569.071 -569.071] (1.000)
Step: 9099, Reward: [-506.401 -506.401 -506.401] [102.3215], Avg: [-569.289 -569.289 -569.289] (1.000)
Step: 9149, Reward: [-465.148 -465.148 -465.148] [59.6232], Avg: [-569.046 -569.046 -569.046] (1.000)
Step: 9199, Reward: [-551.284 -551.284 -551.284] [108.9483], Avg: [-569.541 -569.541 -569.541] (1.000)
Step: 9249, Reward: [-553.274 -553.274 -553.274] [111.4001], Avg: [-570.056 -570.056 -570.056] (1.000)
Step: 9299, Reward: [-460.174 -460.174 -460.174] [43.7949], Avg: [-569.7 -569.7 -569.7] (1.000)
Step: 9349, Reward: [-466.477 -466.477 -466.477] [42.8420], Avg: [-569.377 -569.377 -569.377] (1.000)
Step: 9399, Reward: [-456.238 -456.238 -456.238] [73.6654], Avg: [-569.167 -569.167 -569.167] (1.000)
Step: 9449, Reward: [-541.419 -541.419 -541.419] [62.0868], Avg: [-569.349 -569.349 -569.349] (1.000)
Step: 9499, Reward: [-455.565 -455.565 -455.565] [38.2511], Avg: [-568.952 -568.952 -568.952] (1.000)
Step: 9549, Reward: [-505.535 -505.535 -505.535] [125.3444], Avg: [-569.276 -569.276 -569.276] (1.000)
Step: 9599, Reward: [-474.746 -474.746 -474.746] [73.0151], Avg: [-569.164 -569.164 -569.164] (1.000)
Step: 9649, Reward: [-470.062 -470.062 -470.062] [98.4522], Avg: [-569.16 -569.16 -569.16] (1.000)
Step: 9699, Reward: [-466.846 -466.846 -466.846] [127.8145], Avg: [-569.292 -569.292 -569.292] (1.000)
Step: 9749, Reward: [-486.59 -486.59 -486.59] [50.1095], Avg: [-569.125 -569.125 -569.125] (1.000)
Step: 9799, Reward: [-456.957 -456.957 -456.957] [92.2989], Avg: [-569.023 -569.023 -569.023] (1.000)
Step: 9849, Reward: [-485.57 -485.57 -485.57] [79.0279], Avg: [-569.001 -569.001 -569.001] (1.000)
Step: 9899, Reward: [-470.804 -470.804 -470.804] [98.8740], Avg: [-569.004 -569.004 -569.004] (1.000)
Step: 9949, Reward: [-494.413 -494.413 -494.413] [119.3133], Avg: [-569.229 -569.229 -569.229] (1.000)
Step: 9999, Reward: [-476.911 -476.911 -476.911] [64.6397], Avg: [-569.091 -569.091 -569.091] (1.000)
Step: 10049, Reward: [-482.623 -482.623 -482.623] [40.6839], Avg: [-568.863 -568.863 -568.863] (1.000)
Step: 10099, Reward: [-522.775 -522.775 -522.775] [116.8094], Avg: [-569.213 -569.213 -569.213] (1.000)
Step: 10149, Reward: [-452.395 -452.395 -452.395] [41.5768], Avg: [-568.842 -568.842 -568.842] (1.000)
Step: 10199, Reward: [-469.208 -469.208 -469.208] [42.4880], Avg: [-568.562 -568.562 -568.562] (1.000)
Step: 10249, Reward: [-463.986 -463.986 -463.986] [101.4794], Avg: [-568.547 -568.547 -568.547] (1.000)
Step: 10299, Reward: [-471.21 -471.21 -471.21] [48.0838], Avg: [-568.308 -568.308 -568.308] (1.000)
Step: 10349, Reward: [-465.619 -465.619 -465.619] [61.7593], Avg: [-568.11 -568.11 -568.11] (1.000)
Step: 10399, Reward: [-594.044 -594.044 -594.044] [103.3593], Avg: [-568.732 -568.732 -568.732] (1.000)
Step: 10449, Reward: [-583.454 -583.454 -583.454] [208.2345], Avg: [-569.799 -569.799 -569.799] (1.000)
Step: 10499, Reward: [-470.635 -470.635 -470.635] [105.9130], Avg: [-569.831 -569.831 -569.831] (1.000)
Step: 10549, Reward: [-491.529 -491.529 -491.529] [97.4544], Avg: [-569.921 -569.921 -569.921] (1.000)
Step: 10599, Reward: [-503.761 -503.761 -503.761] [75.6843], Avg: [-569.966 -569.966 -569.966] (1.000)
Step: 10649, Reward: [-489.919 -489.919 -489.919] [61.8417], Avg: [-569.881 -569.881 -569.881] (1.000)
Step: 10699, Reward: [-501.071 -501.071 -501.071] [31.7785], Avg: [-569.708 -569.708 -569.708] (1.000)
Step: 10749, Reward: [-465.318 -465.318 -465.318] [118.1889], Avg: [-569.772 -569.772 -569.772] (1.000)
Step: 10799, Reward: [-474.474 -474.474 -474.474] [75.7968], Avg: [-569.682 -569.682 -569.682] (1.000)
Step: 10849, Reward: [-480.962 -480.962 -480.962] [49.3830], Avg: [-569.5 -569.5 -569.5] (1.000)
Step: 10899, Reward: [-478.867 -478.867 -478.867] [78.8561], Avg: [-569.446 -569.446 -569.446] (1.000)
Step: 10949, Reward: [-581.196 -581.196 -581.196] [91.1678], Avg: [-569.916 -569.916 -569.916] (1.000)
Step: 10999, Reward: [-497.88 -497.88 -497.88] [36.8402], Avg: [-569.756 -569.756 -569.756] (1.000)
Step: 11049, Reward: [-506.1 -506.1 -506.1] [76.1234], Avg: [-569.813 -569.813 -569.813] (1.000)
Step: 11099, Reward: [-398.4 -398.4 -398.4] [45.4787], Avg: [-569.246 -569.246 -569.246] (1.000)
Step: 11149, Reward: [-474.076 -474.076 -474.076] [37.4431], Avg: [-568.987 -568.987 -568.987] (1.000)
Step: 11199, Reward: [-481.213 -481.213 -481.213] [134.8944], Avg: [-569.197 -569.197 -569.197] (1.000)
Step: 11249, Reward: [-566.085 -566.085 -566.085] [71.5625], Avg: [-569.501 -569.501 -569.501] (1.000)
Step: 11299, Reward: [-492.463 -492.463 -492.463] [25.0203], Avg: [-569.271 -569.271 -569.271] (1.000)
Step: 11349, Reward: [-513.562 -513.562 -513.562] [41.0680], Avg: [-569.207 -569.207 -569.207] (1.000)
Step: 11399, Reward: [-524.619 -524.619 -524.619] [123.1631], Avg: [-569.551 -569.551 -569.551] (1.000)
Step: 11449, Reward: [-552.273 -552.273 -552.273] [37.6837], Avg: [-569.64 -569.64 -569.64] (1.000)
Step: 11499, Reward: [-524.033 -524.033 -524.033] [48.4736], Avg: [-569.653 -569.653 -569.653] (1.000)
Step: 11549, Reward: [-552.06 -552.06 -552.06] [118.2542], Avg: [-570.089 -570.089 -570.089] (1.000)
Step: 11599, Reward: [-545.558 -545.558 -545.558] [119.9496], Avg: [-570.5 -570.5 -570.5] (1.000)
Step: 11649, Reward: [-410.541 -410.541 -410.541] [53.8008], Avg: [-570.044 -570.044 -570.044] (1.000)
Step: 11699, Reward: [-487.696 -487.696 -487.696] [76.9333], Avg: [-570.021 -570.021 -570.021] (1.000)
Step: 11749, Reward: [-506.763 -506.763 -506.763] [73.0286], Avg: [-570.063 -570.063 -570.063] (1.000)
Step: 11799, Reward: [-468.689 -468.689 -468.689] [44.1296], Avg: [-569.82 -569.82 -569.82] (1.000)
Step: 11849, Reward: [-556.53 -556.53 -556.53] [100.6725], Avg: [-570.189 -570.189 -570.189] (1.000)
Step: 11899, Reward: [-441.919 -441.919 -441.919] [49.1901], Avg: [-569.857 -569.857 -569.857] (1.000)
Step: 11949, Reward: [-505.27 -505.27 -505.27] [103.9096], Avg: [-570.021 -570.021 -570.021] (1.000)
Step: 11999, Reward: [-401.57 -401.57 -401.57] [40.5162], Avg: [-569.488 -569.488 -569.488] (1.000)
Step: 12049, Reward: [-481.873 -481.873 -481.873] [51.1923], Avg: [-569.337 -569.337 -569.337] (1.000)
Step: 12099, Reward: [-468.612 -468.612 -468.612] [63.6786], Avg: [-569.184 -569.184 -569.184] (1.000)
Step: 12149, Reward: [-496.17 -496.17 -496.17] [96.2424], Avg: [-569.279 -569.279 -569.279] (1.000)
Step: 12199, Reward: [-531.958 -531.958 -531.958] [140.6029], Avg: [-569.703 -569.703 -569.703] (1.000)
Step: 12249, Reward: [-549.55 -549.55 -549.55] [94.4839], Avg: [-570.006 -570.006 -570.006] (1.000)
Step: 12299, Reward: [-654.462 -654.462 -654.462] [123.0267], Avg: [-570.85 -570.85 -570.85] (1.000)
Step: 12349, Reward: [-506.307 -506.307 -506.307] [84.0787], Avg: [-570.929 -570.929 -570.929] (1.000)
Step: 12399, Reward: [-471.775 -471.775 -471.775] [90.8963], Avg: [-570.895 -570.895 -570.895] (1.000)
Step: 12449, Reward: [-472.7 -472.7 -472.7] [27.9578], Avg: [-570.613 -570.613 -570.613] (1.000)
Step: 12499, Reward: [-552.775 -552.775 -552.775] [60.0017], Avg: [-570.782 -570.782 -570.782] (1.000)
Step: 12549, Reward: [-498.649 -498.649 -498.649] [56.8296], Avg: [-570.721 -570.721 -570.721] (1.000)
Step: 12599, Reward: [-450.414 -450.414 -450.414] [55.3588], Avg: [-570.463 -570.463 -570.463] (1.000)
Step: 12649, Reward: [-524.218 -524.218 -524.218] [73.7188], Avg: [-570.572 -570.572 -570.572] (1.000)
Step: 12699, Reward: [-482.844 -482.844 -482.844] [56.4759], Avg: [-570.449 -570.449 -570.449] (1.000)
Step: 12749, Reward: [-557.855 -557.855 -557.855] [55.2771], Avg: [-570.616 -570.616 -570.616] (1.000)
Step: 12799, Reward: [-517.125 -517.125 -517.125] [29.8433], Avg: [-570.524 -570.524 -570.524] (1.000)
Step: 12849, Reward: [-475.751 -475.751 -475.751] [57.1597], Avg: [-570.377 -570.377 -570.377] (1.000)
Step: 12899, Reward: [-524.845 -524.845 -524.845] [49.1978], Avg: [-570.392 -570.392 -570.392] (1.000)
Step: 12949, Reward: [-506.756 -506.756 -506.756] [83.5412], Avg: [-570.468 -570.468 -570.468] (1.000)
Step: 12999, Reward: [-508.034 -508.034 -508.034] [30.0429], Avg: [-570.344 -570.344 -570.344] (1.000)
Step: 13049, Reward: [-483.247 -483.247 -483.247] [97.7743], Avg: [-570.385 -570.385 -570.385] (1.000)
Step: 13099, Reward: [-506.565 -506.565 -506.565] [29.8865], Avg: [-570.255 -570.255 -570.255] (1.000)
Step: 13149, Reward: [-479.658 -479.658 -479.658] [74.9814], Avg: [-570.196 -570.196 -570.196] (1.000)
Step: 13199, Reward: [-499.483 -499.483 -499.483] [142.6597], Avg: [-570.468 -570.468 -570.468] (1.000)
Step: 13249, Reward: [-450.191 -450.191 -450.191] [112.2369], Avg: [-570.438 -570.438 -570.438] (1.000)
Step: 13299, Reward: [-463.849 -463.849 -463.849] [40.0282], Avg: [-570.188 -570.188 -570.188] (1.000)
Step: 13349, Reward: [-490.588 -490.588 -490.588] [85.4165], Avg: [-570.21 -570.21 -570.21] (1.000)
Step: 13399, Reward: [-443.662 -443.662 -443.662] [22.9552], Avg: [-569.823 -569.823 -569.823] (1.000)
Step: 13449, Reward: [-458.892 -458.892 -458.892] [52.2255], Avg: [-569.605 -569.605 -569.605] (1.000)
Step: 13499, Reward: [-424.952 -424.952 -424.952] [69.8953], Avg: [-569.328 -569.328 -569.328] (1.000)
Step: 13549, Reward: [-461.022 -461.022 -461.022] [90.9125], Avg: [-569.264 -569.264 -569.264] (1.000)
Step: 13599, Reward: [-452.662 -452.662 -452.662] [114.8707], Avg: [-569.257 -569.257 -569.257] (1.000)
Step: 13649, Reward: [-542.637 -542.637 -542.637] [53.2609], Avg: [-569.355 -569.355 -569.355] (1.000)
Step: 13699, Reward: [-478.528 -478.528 -478.528] [61.5205], Avg: [-569.248 -569.248 -569.248] (1.000)
Step: 13749, Reward: [-499.632 -499.632 -499.632] [63.6928], Avg: [-569.227 -569.227 -569.227] (1.000)
Step: 13799, Reward: [-535.329 -535.329 -535.329] [115.8951], Avg: [-569.524 -569.524 -569.524] (1.000)
Step: 13849, Reward: [-525.575 -525.575 -525.575] [118.4420], Avg: [-569.793 -569.793 -569.793] (1.000)
Step: 13899, Reward: [-474.56 -474.56 -474.56] [78.2791], Avg: [-569.732 -569.732 -569.732] (1.000)
Step: 13949, Reward: [-411.968 -411.968 -411.968] [80.6322], Avg: [-569.455 -569.455 -569.455] (1.000)
Step: 13999, Reward: [-503.806 -503.806 -503.806] [73.8762], Avg: [-569.484 -569.484 -569.484] (1.000)
Step: 14049, Reward: [-457.307 -457.307 -457.307] [79.3320], Avg: [-569.368 -569.368 -569.368] (1.000)
Step: 14099, Reward: [-560.556 -560.556 -560.556] [100.7991], Avg: [-569.694 -569.694 -569.694] (1.000)
Step: 14149, Reward: [-485.926 -485.926 -485.926] [88.4269], Avg: [-569.71 -569.71 -569.71] (1.000)
Step: 14199, Reward: [-528.543 -528.543 -528.543] [128.3430], Avg: [-570.017 -570.017 -570.017] (1.000)
Step: 14249, Reward: [-491.088 -491.088 -491.088] [41.4270], Avg: [-569.886 -569.886 -569.886] (1.000)
Step: 14299, Reward: [-458.501 -458.501 -458.501] [60.1450], Avg: [-569.706 -569.706 -569.706] (1.000)
Step: 14349, Reward: [-501.694 -501.694 -501.694] [14.4104], Avg: [-569.52 -569.52 -569.52] (1.000)
Step: 14399, Reward: [-500.281 -500.281 -500.281] [118.2316], Avg: [-569.69 -569.69 -569.69] (1.000)
Step: 14449, Reward: [-553.352 -553.352 -553.352] [114.2502], Avg: [-570.029 -570.029 -570.029] (1.000)
Step: 14499, Reward: [-540.678 -540.678 -540.678] [144.0065], Avg: [-570.424 -570.424 -570.424] (1.000)
Step: 14549, Reward: [-523.622 -523.622 -523.622] [49.7221], Avg: [-570.434 -570.434 -570.434] (1.000)
Step: 14599, Reward: [-495.366 -495.366 -495.366] [32.6742], Avg: [-570.289 -570.289 -570.289] (1.000)
Step: 14649, Reward: [-533.444 -533.444 -533.444] [55.5125], Avg: [-570.353 -570.353 -570.353] (1.000)
Step: 14699, Reward: [-434.349 -434.349 -434.349] [33.0516], Avg: [-570.002 -570.002 -570.002] (1.000)
Step: 14749, Reward: [-467.003 -467.003 -467.003] [68.5279], Avg: [-569.886 -569.886 -569.886] (1.000)
Step: 14799, Reward: [-438.681 -438.681 -438.681] [64.1573], Avg: [-569.659 -569.659 -569.659] (1.000)
Step: 14849, Reward: [-601.637 -601.637 -601.637] [114.0031], Avg: [-570.151 -570.151 -570.151] (1.000)
Step: 14899, Reward: [-498.825 -498.825 -498.825] [100.2734], Avg: [-570.248 -570.248 -570.248] (1.000)
Step: 14949, Reward: [-523.044 -523.044 -523.044] [104.4575], Avg: [-570.439 -570.439 -570.439] (1.000)
Step: 14999, Reward: [-491.823 -491.823 -491.823] [85.7557], Avg: [-570.463 -570.463 -570.463] (1.000)
Step: 15049, Reward: [-463.137 -463.137 -463.137] [111.8243], Avg: [-570.478 -570.478 -570.478] (1.000)
Step: 15099, Reward: [-479.71 -479.71 -479.71] [27.0715], Avg: [-570.267 -570.267 -570.267] (1.000)
Step: 15149, Reward: [-565.471 -565.471 -565.471] [100.3111], Avg: [-570.582 -570.582 -570.582] (1.000)
Step: 15199, Reward: [-541.432 -541.432 -541.432] [95.8449], Avg: [-570.802 -570.802 -570.802] (1.000)
Step: 15249, Reward: [-509.96 -509.96 -509.96] [87.8771], Avg: [-570.89 -570.89 -570.89] (1.000)
Step: 15299, Reward: [-519.551 -519.551 -519.551] [87.2076], Avg: [-571.007 -571.007 -571.007] (1.000)
Step: 15349, Reward: [-465.406 -465.406 -465.406] [49.8197], Avg: [-570.826 -570.826 -570.826] (1.000)
Step: 15399, Reward: [-504.301 -504.301 -504.301] [89.1236], Avg: [-570.899 -570.899 -570.899] (1.000)
Step: 15449, Reward: [-528.751 -528.751 -528.751] [79.9567], Avg: [-571.021 -571.021 -571.021] (1.000)
Step: 15499, Reward: [-485.357 -485.357 -485.357] [33.4324], Avg: [-570.853 -570.853 -570.853] (1.000)
Step: 15549, Reward: [-460.073 -460.073 -460.073] [50.4665], Avg: [-570.659 -570.659 -570.659] (1.000)
Step: 15599, Reward: [-499.546 -499.546 -499.546] [133.7431], Avg: [-570.86 -570.86 -570.86] (1.000)
Step: 15649, Reward: [-501.676 -501.676 -501.676] [60.3250], Avg: [-570.831 -570.831 -570.831] (1.000)
Step: 15699, Reward: [-546.282 -546.282 -546.282] [118.8547], Avg: [-571.132 -571.132 -571.132] (1.000)
Step: 15749, Reward: [-469.264 -469.264 -469.264] [76.6725], Avg: [-571.052 -571.052 -571.052] (1.000)
Step: 15799, Reward: [-492.497 -492.497 -492.497] [48.1581], Avg: [-570.956 -570.956 -570.956] (1.000)
Step: 15849, Reward: [-543.874 -543.874 -543.874] [32.9019], Avg: [-570.974 -570.974 -570.974] (1.000)
Step: 15899, Reward: [-492.492 -492.492 -492.492] [61.4886], Avg: [-570.921 -570.921 -570.921] (1.000)
Step: 15949, Reward: [-560.706 -560.706 -560.706] [89.3212], Avg: [-571.169 -571.169 -571.169] (1.000)
Step: 15999, Reward: [-511.988 -511.988 -511.988] [117.9236], Avg: [-571.352 -571.352 -571.352] (1.000)
Step: 16049, Reward: [-554.793 -554.793 -554.793] [114.8592], Avg: [-571.658 -571.658 -571.658] (1.000)
Step: 16099, Reward: [-477.084 -477.084 -477.084] [90.4467], Avg: [-571.646 -571.646 -571.646] (1.000)
Step: 16149, Reward: [-451.304 -451.304 -451.304] [92.9473], Avg: [-571.561 -571.561 -571.561] (1.000)
Step: 16199, Reward: [-463.749 -463.749 -463.749] [63.9617], Avg: [-571.425 -571.425 -571.425] (1.000)
Step: 16249, Reward: [-476.449 -476.449 -476.449] [63.4706], Avg: [-571.328 -571.328 -571.328] (1.000)
Step: 16299, Reward: [-608.614 -608.614 -608.614] [54.0216], Avg: [-571.609 -571.609 -571.609] (1.000)
Step: 16349, Reward: [-462.718 -462.718 -462.718] [38.3729], Avg: [-571.393 -571.393 -571.393] (1.000)
Step: 16399, Reward: [-469.781 -469.781 -469.781] [39.9224], Avg: [-571.205 -571.205 -571.205] (1.000)
Step: 16449, Reward: [-449.513 -449.513 -449.513] [42.9625], Avg: [-570.965 -570.965 -570.965] (1.000)
Step: 16499, Reward: [-427.997 -427.997 -427.997] [54.3500], Avg: [-570.697 -570.697 -570.697] (1.000)
Step: 16549, Reward: [-552.759 -552.759 -552.759] [93.5373], Avg: [-570.925 -570.925 -570.925] (1.000)
Step: 16599, Reward: [-518.674 -518.674 -518.674] [53.6754], Avg: [-570.93 -570.93 -570.93] (1.000)
Step: 16649, Reward: [-463.181 -463.181 -463.181] [74.4562], Avg: [-570.83 -570.83 -570.83] (1.000)
Step: 16699, Reward: [-488.846 -488.846 -488.846] [77.4868], Avg: [-570.816 -570.816 -570.816] (1.000)
Step: 16749, Reward: [-559.173 -559.173 -559.173] [100.4698], Avg: [-571.081 -571.081 -571.081] (1.000)
Step: 16799, Reward: [-584.225 -584.225 -584.225] [140.1166], Avg: [-571.537 -571.537 -571.537] (1.000)
Step: 16849, Reward: [-490.307 -490.307 -490.307] [83.7440], Avg: [-571.545 -571.545 -571.545] (1.000)
Step: 16899, Reward: [-474.722 -474.722 -474.722] [101.7338], Avg: [-571.559 -571.559 -571.559] (1.000)
Step: 16949, Reward: [-478.916 -478.916 -478.916] [78.4781], Avg: [-571.518 -571.518 -571.518] (1.000)
Step: 16999, Reward: [-509.881 -509.881 -509.881] [54.1472], Avg: [-571.496 -571.496 -571.496] (1.000)
Step: 17049, Reward: [-432.305 -432.305 -432.305] [49.5046], Avg: [-571.233 -571.233 -571.233] (1.000)
Step: 17099, Reward: [-528.919 -528.919 -528.919] [75.3943], Avg: [-571.329 -571.329 -571.329] (1.000)
Step: 17149, Reward: [-503.596 -503.596 -503.596] [92.2320], Avg: [-571.401 -571.401 -571.401] (1.000)
Step: 17199, Reward: [-465.787 -465.787 -465.787] [74.6774], Avg: [-571.311 -571.311 -571.311] (1.000)
Step: 17249, Reward: [-502.757 -502.757 -502.757] [80.2863], Avg: [-571.345 -571.345 -571.345] (1.000)
Step: 17299, Reward: [-515.249 -515.249 -515.249] [116.1159], Avg: [-571.518 -571.518 -571.518] (1.000)
Step: 17349, Reward: [-488.733 -488.733 -488.733] [98.5181], Avg: [-571.564 -571.564 -571.564] (1.000)
Step: 17399, Reward: [-462.001 -462.001 -462.001] [76.8599], Avg: [-571.47 -571.47 -571.47] (1.000)
Step: 17449, Reward: [-601.478 -601.478 -601.478] [106.9760], Avg: [-571.862 -571.862 -571.862] (1.000)
Step: 17499, Reward: [-524.33 -524.33 -524.33] [166.9651], Avg: [-572.203 -572.203 -572.203] (1.000)
Step: 17549, Reward: [-523.297 -523.297 -523.297] [102.2283], Avg: [-572.355 -572.355 -572.355] (1.000)
Step: 17599, Reward: [-483.028 -483.028 -483.028] [118.3418], Avg: [-572.438 -572.438 -572.438] (1.000)
Step: 17649, Reward: [-468.535 -468.535 -468.535] [68.3527], Avg: [-572.337 -572.337 -572.337] (1.000)
Step: 17699, Reward: [-491.507 -491.507 -491.507] [105.9187], Avg: [-572.408 -572.408 -572.408] (1.000)
Step: 17749, Reward: [-533.667 -533.667 -533.667] [56.2384], Avg: [-572.457 -572.457 -572.457] (1.000)
Step: 17799, Reward: [-528.582 -528.582 -528.582] [86.1711], Avg: [-572.576 -572.576 -572.576] (1.000)
Step: 17849, Reward: [-505.036 -505.036 -505.036] [71.6023], Avg: [-572.587 -572.587 -572.587] (1.000)
Step: 17899, Reward: [-487.318 -487.318 -487.318] [69.3283], Avg: [-572.543 -572.543 -572.543] (1.000)
Step: 17949, Reward: [-415.774 -415.774 -415.774] [61.1918], Avg: [-572.277 -572.277 -572.277] (1.000)
Step: 17999, Reward: [-511.81 -511.81 -511.81] [130.6299], Avg: [-572.472 -572.472 -572.472] (1.000)
Step: 18049, Reward: [-461.292 -461.292 -461.292] [84.9070], Avg: [-572.399 -572.399 -572.399] (1.000)
Step: 18099, Reward: [-498.176 -498.176 -498.176] [60.7585], Avg: [-572.362 -572.362 -572.362] (1.000)
Step: 18149, Reward: [-554.532 -554.532 -554.532] [99.8546], Avg: [-572.588 -572.588 -572.588] (1.000)
Step: 18199, Reward: [-466.18 -466.18 -466.18] [90.9408], Avg: [-572.545 -572.545 -572.545] (1.000)
Step: 18249, Reward: [-486.669 -486.669 -486.669] [23.9730], Avg: [-572.375 -572.375 -572.375] (1.000)
Step: 18299, Reward: [-469.635 -469.635 -469.635] [74.6676], Avg: [-572.299 -572.299 -572.299] (1.000)
Step: 18349, Reward: [-518.047 -518.047 -518.047] [70.8529], Avg: [-572.344 -572.344 -572.344] (1.000)
Step: 18399, Reward: [-468.647 -468.647 -468.647] [55.4928], Avg: [-572.213 -572.213 -572.213] (1.000)
Step: 18449, Reward: [-421.157 -421.157 -421.157] [37.9934], Avg: [-571.907 -571.907 -571.907] (1.000)
Step: 18499, Reward: [-465.144 -465.144 -465.144] [60.8581], Avg: [-571.783 -571.783 -571.783] (1.000)
Step: 18549, Reward: [-423.686 -423.686 -423.686] [68.8431], Avg: [-571.569 -571.569 -571.569] (1.000)
Step: 18599, Reward: [-486.139 -486.139 -486.139] [66.4141], Avg: [-571.518 -571.518 -571.518] (1.000)
Step: 18649, Reward: [-532.128 -532.128 -532.128] [104.3903], Avg: [-571.692 -571.692 -571.692] (1.000)
Step: 18699, Reward: [-442.348 -442.348 -442.348] [87.5085], Avg: [-571.58 -571.58 -571.58] (1.000)
Step: 18749, Reward: [-494.338 -494.338 -494.338] [76.0113], Avg: [-571.577 -571.577 -571.577] (1.000)
Step: 18799, Reward: [-506.613 -506.613 -506.613] [85.3080], Avg: [-571.631 -571.631 -571.631] (1.000)
Step: 18849, Reward: [-535.32 -535.32 -535.32] [87.5734], Avg: [-571.767 -571.767 -571.767] (1.000)
Step: 18899, Reward: [-487.674 -487.674 -487.674] [62.2137], Avg: [-571.709 -571.709 -571.709] (1.000)
Step: 18949, Reward: [-459.468 -459.468 -459.468] [100.9438], Avg: [-571.679 -571.679 -571.679] (1.000)
Step: 18999, Reward: [-489.791 -489.791 -489.791] [43.1605], Avg: [-571.577 -571.577 -571.577] (1.000)
Step: 19049, Reward: [-556.833 -556.833 -556.833] [81.8233], Avg: [-571.753 -571.753 -571.753] (1.000)
Step: 19099, Reward: [-399.451 -399.451 -399.451] [57.7108], Avg: [-571.454 -571.454 -571.454] (1.000)
Step: 19149, Reward: [-447.209 -447.209 -447.209] [36.2150], Avg: [-571.224 -571.224 -571.224] (1.000)
Step: 19199, Reward: [-506.05 -506.05 -506.05] [123.2174], Avg: [-571.375 -571.375 -571.375] (1.000)
Step: 19249, Reward: [-452.848 -452.848 -452.848] [34.1878], Avg: [-571.156 -571.156 -571.156] (1.000)
Step: 19299, Reward: [-469.464 -469.464 -469.464] [37.5833], Avg: [-570.99 -570.99 -570.99] (1.000)
Step: 19349, Reward: [-512.354 -512.354 -512.354] [105.5705], Avg: [-571.111 -571.111 -571.111] (1.000)
Step: 19399, Reward: [-515.225 -515.225 -515.225] [160.2077], Avg: [-571.38 -571.38 -571.38] (1.000)
Step: 19449, Reward: [-473.049 -473.049 -473.049] [47.5895], Avg: [-571.249 -571.249 -571.249] (1.000)
Step: 19499, Reward: [-490.593 -490.593 -490.593] [87.1231], Avg: [-571.266 -571.266 -571.266] (1.000)
Step: 19549, Reward: [-508.195 -508.195 -508.195] [94.2640], Avg: [-571.346 -571.346 -571.346] (1.000)
Step: 19599, Reward: [-520.717 -520.717 -520.717] [89.2152], Avg: [-571.444 -571.444 -571.444] (1.000)
Step: 19649, Reward: [-485.798 -485.798 -485.798] [81.5363], Avg: [-571.434 -571.434 -571.434] (1.000)
Step: 19699, Reward: [-490.662 -490.662 -490.662] [75.0025], Avg: [-571.419 -571.419 -571.419] (1.000)
Step: 19749, Reward: [-449.728 -449.728 -449.728] [96.2571], Avg: [-571.355 -571.355 -571.355] (1.000)
Step: 19799, Reward: [-567.969 -567.969 -567.969] [65.2316], Avg: [-571.511 -571.511 -571.511] (1.000)
Step: 19849, Reward: [-487.535 -487.535 -487.535] [173.2635], Avg: [-571.736 -571.736 -571.736] (1.000)
Step: 19899, Reward: [-479.58 -479.58 -479.58] [125.5395], Avg: [-571.82 -571.82 -571.82] (1.000)
Step: 19949, Reward: [-468.732 -468.732 -468.732] [65.6917], Avg: [-571.726 -571.726 -571.726] (1.000)
Step: 19999, Reward: [-500.362 -500.362 -500.362] [41.7664], Avg: [-571.652 -571.652 -571.652] (1.000)
Step: 20049, Reward: [-493.982 -493.982 -493.982] [126.6296], Avg: [-571.774 -571.774 -571.774] (1.000)
Step: 20099, Reward: [-446.053 -446.053 -446.053] [72.3264], Avg: [-571.641 -571.641 -571.641] (1.000)
Step: 20149, Reward: [-508.95 -508.95 -508.95] [106.2926], Avg: [-571.749 -571.749 -571.749] (1.000)
Step: 20199, Reward: [-454.796 -454.796 -454.796] [111.0965], Avg: [-571.735 -571.735 -571.735] (1.000)
Step: 20249, Reward: [-556.15 -556.15 -556.15] [129.9335], Avg: [-572.017 -572.017 -572.017] (1.000)
Step: 20299, Reward: [-464.569 -464.569 -464.569] [78.4223], Avg: [-571.946 -571.946 -571.946] (1.000)
Step: 20349, Reward: [-500.636 -500.636 -500.636] [59.4931], Avg: [-571.917 -571.917 -571.917] (1.000)
Step: 20399, Reward: [-550.114 -550.114 -550.114] [126.8811], Avg: [-572.174 -572.174 -572.174] (1.000)
Step: 20449, Reward: [-539.307 -539.307 -539.307] [128.7102], Avg: [-572.409 -572.409 -572.409] (1.000)
Step: 20499, Reward: [-467.936 -467.936 -467.936] [79.0952], Avg: [-572.347 -572.347 -572.347] (1.000)
Step: 20549, Reward: [-445.639 -445.639 -445.639] [35.2555], Avg: [-572.124 -572.124 -572.124] (1.000)
Step: 20599, Reward: [-472.274 -472.274 -472.274] [64.0381], Avg: [-572.037 -572.037 -572.037] (1.000)
Step: 20649, Reward: [-426.077 -426.077 -426.077] [78.4220], Avg: [-571.874 -571.874 -571.874] (1.000)
Step: 20699, Reward: [-505.237 -505.237 -505.237] [130.7059], Avg: [-572.028 -572.028 -572.028] (1.000)
Step: 20749, Reward: [-522.547 -522.547 -522.547] [105.9424], Avg: [-572.165 -572.165 -572.165] (1.000)
Step: 20799, Reward: [-515.76 -515.76 -515.76] [72.5321], Avg: [-572.203 -572.203 -572.203] (1.000)
Step: 20849, Reward: [-457.747 -457.747 -457.747] [76.0304], Avg: [-572.111 -572.111 -572.111] (1.000)
Step: 20899, Reward: [-509.087 -509.087 -509.087] [56.4264], Avg: [-572.095 -572.095 -572.095] (1.000)
Step: 20949, Reward: [-522.462 -522.462 -522.462] [100.3231], Avg: [-572.216 -572.216 -572.216] (1.000)
Step: 20999, Reward: [-557.344 -557.344 -557.344] [113.0464], Avg: [-572.45 -572.45 -572.45] (1.000)
Step: 21049, Reward: [-470.869 -470.869 -470.869] [103.1881], Avg: [-572.454 -572.454 -572.454] (1.000)
Step: 21099, Reward: [-514.325 -514.325 -514.325] [100.7175], Avg: [-572.555 -572.555 -572.555] (1.000)
Step: 21149, Reward: [-517.481 -517.481 -517.481] [139.1761], Avg: [-572.754 -572.754 -572.754] (1.000)
Step: 21199, Reward: [-577.912 -577.912 -577.912] [106.4019], Avg: [-573.017 -573.017 -573.017] (1.000)
Step: 21249, Reward: [-458.087 -458.087 -458.087] [72.4256], Avg: [-572.917 -572.917 -572.917] (1.000)
Step: 21299, Reward: [-452.844 -452.844 -452.844] [103.2381], Avg: [-572.877 -572.877 -572.877] (1.000)
Step: 21349, Reward: [-461.696 -461.696 -461.696] [44.5809], Avg: [-572.721 -572.721 -572.721] (1.000)
Step: 21399, Reward: [-488.725 -488.725 -488.725] [80.6695], Avg: [-572.713 -572.713 -572.713] (1.000)
Step: 21449, Reward: [-484.201 -484.201 -484.201] [58.7289], Avg: [-572.644 -572.644 -572.644] (1.000)
Step: 21499, Reward: [-517.331 -517.331 -517.331] [113.4182], Avg: [-572.779 -572.779 -572.779] (1.000)
Step: 21549, Reward: [-452.258 -452.258 -452.258] [35.7183], Avg: [-572.582 -572.582 -572.582] (1.000)
Step: 21599, Reward: [-486.146 -486.146 -486.146] [72.4700], Avg: [-572.55 -572.55 -572.55] (1.000)
Step: 21649, Reward: [-482.617 -482.617 -482.617] [82.0788], Avg: [-572.532 -572.532 -572.532] (1.000)
Step: 21699, Reward: [-493.513 -493.513 -493.513] [88.7322], Avg: [-572.554 -572.554 -572.554] (1.000)
Step: 21749, Reward: [-508.461 -508.461 -508.461] [88.5496], Avg: [-572.611 -572.611 -572.611] (1.000)
Step: 21799, Reward: [-419.023 -419.023 -419.023] [52.1542], Avg: [-572.378 -572.378 -572.378] (1.000)
Step: 21849, Reward: [-526.879 -526.879 -526.879] [130.7811], Avg: [-572.573 -572.573 -572.573] (1.000)
Step: 21899, Reward: [-497.196 -497.196 -497.196] [79.7735], Avg: [-572.583 -572.583 -572.583] (1.000)
Step: 21949, Reward: [-536.392 -536.392 -536.392] [67.7187], Avg: [-572.655 -572.655 -572.655] (1.000)
Step: 21999, Reward: [-429.338 -429.338 -429.338] [33.2060], Avg: [-572.405 -572.405 -572.405] (1.000)
Step: 22049, Reward: [-427.81 -427.81 -427.81] [100.1787], Avg: [-572.304 -572.304 -572.304] (1.000)
Step: 22099, Reward: [-481.971 -481.971 -481.971] [29.9554], Avg: [-572.167 -572.167 -572.167] (1.000)
Step: 22149, Reward: [-529.349 -529.349 -529.349] [38.9884], Avg: [-572.159 -572.159 -572.159] (1.000)
Step: 22199, Reward: [-547.57 -547.57 -547.57] [140.9856], Avg: [-572.421 -572.421 -572.421] (1.000)
Step: 22249, Reward: [-403.908 -403.908 -403.908] [36.8588], Avg: [-572.125 -572.125 -572.125] (1.000)
Step: 22299, Reward: [-521.009 -521.009 -521.009] [124.7399], Avg: [-572.29 -572.29 -572.29] (1.000)
Step: 22349, Reward: [-478.127 -478.127 -478.127] [60.5132], Avg: [-572.215 -572.215 -572.215] (1.000)
Step: 22399, Reward: [-476.248 -476.248 -476.248] [41.3673], Avg: [-572.093 -572.093 -572.093] (1.000)
Step: 22449, Reward: [-524.24 -524.24 -524.24] [83.8552], Avg: [-572.173 -572.173 -572.173] (1.000)
Step: 22499, Reward: [-507.717 -507.717 -507.717] [103.9512], Avg: [-572.261 -572.261 -572.261] (1.000)
Step: 22549, Reward: [-464.328 -464.328 -464.328] [82.5359], Avg: [-572.205 -572.205 -572.205] (1.000)
Step: 22599, Reward: [-512.665 -512.665 -512.665] [39.5840], Avg: [-572.16 -572.16 -572.16] (1.000)
Step: 22649, Reward: [-450.643 -450.643 -450.643] [58.9847], Avg: [-572.022 -572.022 -572.022] (1.000)
Step: 22699, Reward: [-439.873 -439.873 -439.873] [43.9571], Avg: [-571.828 -571.828 -571.828] (1.000)
Step: 22749, Reward: [-559.17 -559.17 -559.17] [108.1525], Avg: [-572.038 -572.038 -572.038] (1.000)
Step: 22799, Reward: [-413.62 -413.62 -413.62] [57.5947], Avg: [-571.817 -571.817 -571.817] (1.000)
Step: 22849, Reward: [-440.561 -440.561 -440.561] [100.7342], Avg: [-571.75 -571.75 -571.75] (1.000)
Step: 22899, Reward: [-517.108 -517.108 -517.108] [97.1712], Avg: [-571.843 -571.843 -571.843] (1.000)
Step: 22949, Reward: [-416.138 -416.138 -416.138] [41.6899], Avg: [-571.595 -571.595 -571.595] (1.000)
Step: 22999, Reward: [-509.526 -509.526 -509.526] [72.3053], Avg: [-571.617 -571.617 -571.617] (1.000)
Step: 23049, Reward: [-547.968 -547.968 -547.968] [68.0724], Avg: [-571.713 -571.713 -571.713] (1.000)
Step: 23099, Reward: [-469.709 -469.709 -469.709] [110.1904], Avg: [-571.731 -571.731 -571.731] (1.000)
Step: 23149, Reward: [-507.14 -507.14 -507.14] [54.9354], Avg: [-571.71 -571.71 -571.71] (1.000)
Step: 23199, Reward: [-492. -492. -492.] [36.6438], Avg: [-571.617 -571.617 -571.617] (1.000)
Step: 23249, Reward: [-505.336 -505.336 -505.336] [89.9428], Avg: [-571.668 -571.668 -571.668] (1.000)
Step: 23299, Reward: [-611.582 -611.582 -611.582] [150.7120], Avg: [-572.077 -572.077 -572.077] (1.000)
Step: 23349, Reward: [-458.818 -458.818 -458.818] [93.7423], Avg: [-572.035 -572.035 -572.035] (1.000)
Step: 23399, Reward: [-490.959 -490.959 -490.959] [113.5868], Avg: [-572.105 -572.105 -572.105] (1.000)
Step: 23449, Reward: [-560.792 -560.792 -560.792] [197.9875], Avg: [-572.503 -572.503 -572.503] (1.000)
Step: 23499, Reward: [-502.394 -502.394 -502.394] [43.4368], Avg: [-572.446 -572.446 -572.446] (1.000)
Step: 23549, Reward: [-479.864 -479.864 -479.864] [94.0985], Avg: [-572.449 -572.449 -572.449] (1.000)
Step: 23599, Reward: [-530.646 -530.646 -530.646] [125.4130], Avg: [-572.627 -572.627 -572.627] (1.000)
Step: 23649, Reward: [-488.399 -488.399 -488.399] [59.7953], Avg: [-572.575 -572.575 -572.575] (1.000)
Step: 23699, Reward: [-521.175 -521.175 -521.175] [46.2159], Avg: [-572.564 -572.564 -572.564] (1.000)
Step: 23749, Reward: [-470.335 -470.335 -470.335] [124.2808], Avg: [-572.61 -572.61 -572.61] (1.000)
Step: 23799, Reward: [-449.85 -449.85 -449.85] [115.1765], Avg: [-572.594 -572.594 -572.594] (1.000)
Step: 23849, Reward: [-451.98 -451.98 -451.98] [85.3447], Avg: [-572.52 -572.52 -572.52] (1.000)
Step: 23899, Reward: [-486.464 -486.464 -486.464] [139.0620], Avg: [-572.631 -572.631 -572.631] (1.000)
Step: 23949, Reward: [-537.726 -537.726 -537.726] [107.7601], Avg: [-572.783 -572.783 -572.783] (1.000)
Step: 23999, Reward: [-413.759 -413.759 -413.759] [69.7822], Avg: [-572.598 -572.598 -572.598] (1.000)
Step: 24049, Reward: [-425.126 -425.126 -425.126] [52.5812], Avg: [-572.4 -572.4 -572.4] (1.000)
Step: 24099, Reward: [-435.383 -435.383 -435.383] [100.5660], Avg: [-572.325 -572.325 -572.325] (1.000)
Step: 24149, Reward: [-452.273 -452.273 -452.273] [57.1636], Avg: [-572.194 -572.194 -572.194] (1.000)
Step: 24199, Reward: [-499.217 -499.217 -499.217] [55.0916], Avg: [-572.157 -572.157 -572.157] (1.000)
Step: 24249, Reward: [-580.065 -580.065 -580.065] [92.5738], Avg: [-572.365 -572.365 -572.365] (1.000)
Step: 24299, Reward: [-477.187 -477.187 -477.187] [53.2599], Avg: [-572.278 -572.278 -572.278] (1.000)
Step: 24349, Reward: [-463.616 -463.616 -463.616] [57.3285], Avg: [-572.173 -572.173 -572.173] (1.000)
Step: 24399, Reward: [-478.916 -478.916 -478.916] [14.0958], Avg: [-572.011 -572.011 -572.011] (1.000)
Step: 24449, Reward: [-571.056 -571.056 -571.056] [102.8430], Avg: [-572.219 -572.219 -572.219] (1.000)
Step: 24499, Reward: [-499.926 -499.926 -499.926] [60.5839], Avg: [-572.195 -572.195 -572.195] (1.000)
Step: 24549, Reward: [-429.475 -429.475 -429.475] [82.3846], Avg: [-572.072 -572.072 -572.072] (1.000)
Step: 24599, Reward: [-514.723 -514.723 -514.723] [68.4475], Avg: [-572.095 -572.095 -572.095] (1.000)
Step: 24649, Reward: [-459.87 -459.87 -459.87] [96.8082], Avg: [-572.064 -572.064 -572.064] (1.000)
Step: 24699, Reward: [-485.425 -485.425 -485.425] [68.2754], Avg: [-572.026 -572.026 -572.026] (1.000)
Step: 24749, Reward: [-513.821 -513.821 -513.821] [84.8945], Avg: [-572.08 -572.08 -572.08] (1.000)
Step: 24799, Reward: [-458.945 -458.945 -458.945] [82.0127], Avg: [-572.018 -572.018 -572.018] (1.000)
Step: 24849, Reward: [-488.06 -488.06 -488.06] [111.6919], Avg: [-572.073 -572.073 -572.073] (1.000)
Step: 24899, Reward: [-469.902 -469.902 -469.902] [26.8544], Avg: [-571.922 -571.922 -571.922] (1.000)
Step: 24949, Reward: [-462.484 -462.484 -462.484] [50.2600], Avg: [-571.804 -571.804 -571.804] (1.000)
Step: 24999, Reward: [-442.9 -442.9 -442.9] [31.9650], Avg: [-571.61 -571.61 -571.61] (1.000)
Step: 25049, Reward: [-492.273 -492.273 -492.273] [43.7254], Avg: [-571.539 -571.539 -571.539] (1.000)
Step: 25099, Reward: [-421.41 -421.41 -421.41] [66.3810], Avg: [-571.372 -571.372 -571.372] (1.000)
Step: 25149, Reward: [-449.965 -449.965 -449.965] [47.4665], Avg: [-571.225 -571.225 -571.225] (1.000)
Step: 25199, Reward: [-505.585 -505.585 -505.585] [99.4976], Avg: [-571.292 -571.292 -571.292] (1.000)
Step: 25249, Reward: [-553.743 -553.743 -553.743] [120.2511], Avg: [-571.495 -571.495 -571.495] (1.000)
Step: 25299, Reward: [-479.962 -479.962 -479.962] [44.6926], Avg: [-571.403 -571.403 -571.403] (1.000)
Step: 25349, Reward: [-521.07 -521.07 -521.07] [57.7938], Avg: [-571.418 -571.418 -571.418] (1.000)
Step: 25399, Reward: [-482.376 -482.376 -482.376] [67.5276], Avg: [-571.375 -571.375 -571.375] (1.000)
Step: 25449, Reward: [-492.15 -492.15 -492.15] [104.4149], Avg: [-571.425 -571.425 -571.425] (1.000)
Step: 25499, Reward: [-435.915 -435.915 -435.915] [110.1951], Avg: [-571.375 -571.375 -571.375] (1.000)
Step: 25549, Reward: [-507.678 -507.678 -507.678] [61.3009], Avg: [-571.37 -571.37 -571.37] (1.000)
Step: 25599, Reward: [-573.207 -573.207 -573.207] [131.0582], Avg: [-571.63 -571.63 -571.63] (1.000)
Step: 25649, Reward: [-490.057 -490.057 -490.057] [99.6648], Avg: [-571.665 -571.665 -571.665] (1.000)
Step: 25699, Reward: [-531.589 -531.589 -531.589] [145.8353], Avg: [-571.871 -571.871 -571.871] (1.000)
Step: 25749, Reward: [-506.051 -506.051 -506.051] [163.5311], Avg: [-572.061 -572.061 -572.061] (1.000)
Step: 25799, Reward: [-459.845 -459.845 -459.845] [40.6484], Avg: [-571.922 -571.922 -571.922] (1.000)
Step: 25849, Reward: [-499.951 -499.951 -499.951] [64.5281], Avg: [-571.908 -571.908 -571.908] (1.000)
Step: 25899, Reward: [-464.194 -464.194 -464.194] [54.5916], Avg: [-571.805 -571.805 -571.805] (1.000)
Step: 25949, Reward: [-463.624 -463.624 -463.624] [85.3840], Avg: [-571.761 -571.761 -571.761] (1.000)
Step: 25999, Reward: [-400.319 -400.319 -400.319] [63.6077], Avg: [-571.554 -571.554 -571.554] (1.000)
Step: 26049, Reward: [-482.768 -482.768 -482.768] [117.6982], Avg: [-571.609 -571.609 -571.609] (1.000)
Step: 26099, Reward: [-475.123 -475.123 -475.123] [102.6932], Avg: [-571.621 -571.621 -571.621] (1.000)
Step: 26149, Reward: [-499.779 -499.779 -499.779] [91.0096], Avg: [-571.658 -571.658 -571.658] (1.000)
Step: 26199, Reward: [-543.675 -543.675 -543.675] [68.1285], Avg: [-571.734 -571.734 -571.734] (1.000)
Step: 26249, Reward: [-440.067 -440.067 -440.067] [59.6494], Avg: [-571.597 -571.597 -571.597] (1.000)
Step: 26299, Reward: [-482.828 -482.828 -482.828] [71.5679], Avg: [-571.564 -571.564 -571.564] (1.000)
Step: 26349, Reward: [-469.144 -469.144 -469.144] [98.2235], Avg: [-571.557 -571.557 -571.557] (1.000)
Step: 26399, Reward: [-445.404 -445.404 -445.404] [39.1941], Avg: [-571.392 -571.392 -571.392] (1.000)
Step: 26449, Reward: [-456.999 -456.999 -456.999] [67.0120], Avg: [-571.302 -571.302 -571.302] (1.000)
Step: 26499, Reward: [-496.173 -496.173 -496.173] [38.8462], Avg: [-571.234 -571.234 -571.234] (1.000)
Step: 26549, Reward: [-474.49 -474.49 -474.49] [37.1903], Avg: [-571.122 -571.122 -571.122] (1.000)
Step: 26599, Reward: [-507.604 -507.604 -507.604] [96.0742], Avg: [-571.183 -571.183 -571.183] (1.000)
Step: 26649, Reward: [-485.558 -485.558 -485.558] [69.1604], Avg: [-571.152 -571.152 -571.152] (1.000)
Step: 26699, Reward: [-477.236 -477.236 -477.236] [95.7189], Avg: [-571.155 -571.155 -571.155] (1.000)
Step: 26749, Reward: [-530.821 -530.821 -530.821] [79.1696], Avg: [-571.228 -571.228 -571.228] (1.000)
Step: 26799, Reward: [-505.748 -505.748 -505.748] [57.4430], Avg: [-571.213 -571.213 -571.213] (1.000)
Step: 26849, Reward: [-499.094 -499.094 -499.094] [140.8747], Avg: [-571.341 -571.341 -571.341] (1.000)
Step: 26899, Reward: [-490.019 -490.019 -490.019] [79.3972], Avg: [-571.337 -571.337 -571.337] (1.000)
Step: 26949, Reward: [-551.447 -551.447 -551.447] [123.3308], Avg: [-571.529 -571.529 -571.529] (1.000)
Step: 26999, Reward: [-583.567 -583.567 -583.567] [34.4703], Avg: [-571.615 -571.615 -571.615] (1.000)
Step: 27049, Reward: [-453.242 -453.242 -453.242] [73.9939], Avg: [-571.533 -571.533 -571.533] (1.000)
Step: 27099, Reward: [-408.273 -408.273 -408.273] [45.5532], Avg: [-571.316 -571.316 -571.316] (1.000)
Step: 27149, Reward: [-596.247 -596.247 -596.247] [124.9163], Avg: [-571.592 -571.592 -571.592] (1.000)
Step: 27199, Reward: [-552.01 -552.01 -552.01] [122.9061], Avg: [-571.782 -571.782 -571.782] (1.000)
Step: 27249, Reward: [-486.659 -486.659 -486.659] [69.5728], Avg: [-571.754 -571.754 -571.754] (1.000)
Step: 27299, Reward: [-485.289 -485.289 -485.289] [67.4849], Avg: [-571.719 -571.719 -571.719] (1.000)
Step: 27349, Reward: [-571.014 -571.014 -571.014] [100.4266], Avg: [-571.901 -571.901 -571.901] (1.000)
Step: 27399, Reward: [-517.623 -517.623 -517.623] [106.4908], Avg: [-571.996 -571.996 -571.996] (1.000)
Step: 27449, Reward: [-479.552 -479.552 -479.552] [46.8595], Avg: [-571.913 -571.913 -571.913] (1.000)
Step: 27499, Reward: [-530.151 -530.151 -530.151] [96.4392], Avg: [-572.013 -572.013 -572.013] (1.000)
Step: 27549, Reward: [-498.54 -498.54 -498.54] [61.4612], Avg: [-571.991 -571.991 -571.991] (1.000)
Step: 27599, Reward: [-526.004 -526.004 -526.004] [85.2702], Avg: [-572.062 -572.062 -572.062] (1.000)
Step: 27649, Reward: [-472.441 -472.441 -472.441] [130.7003], Avg: [-572.118 -572.118 -572.118] (1.000)
Step: 27699, Reward: [-476.744 -476.744 -476.744] [91.8562], Avg: [-572.112 -572.112 -572.112] (1.000)
Step: 27749, Reward: [-523.326 -523.326 -523.326] [112.7371], Avg: [-572.227 -572.227 -572.227] (1.000)
Step: 27799, Reward: [-466.463 -466.463 -466.463] [58.3842], Avg: [-572.142 -572.142 -572.142] (1.000)
Step: 27849, Reward: [-489.792 -489.792 -489.792] [57.5713], Avg: [-572.098 -572.098 -572.098] (1.000)
Step: 27899, Reward: [-459.856 -459.856 -459.856] [52.9901], Avg: [-571.991 -571.991 -571.991] (1.000)
Step: 27949, Reward: [-487.18 -487.18 -487.18] [127.7675], Avg: [-572.068 -572.068 -572.068] (1.000)
Step: 27999, Reward: [-374.06 -374.06 -374.06] [33.9555], Avg: [-571.775 -571.775 -571.775] (1.000)
Step: 28049, Reward: [-491.244 -491.244 -491.244] [95.2624], Avg: [-571.801 -571.801 -571.801] (1.000)
Step: 28099, Reward: [-502.944 -502.944 -502.944] [79.2350], Avg: [-571.82 -571.82 -571.82] (1.000)
Step: 28149, Reward: [-481.103 -481.103 -481.103] [72.3527], Avg: [-571.787 -571.787 -571.787] (1.000)
Step: 28199, Reward: [-455.544 -455.544 -455.544] [74.2666], Avg: [-571.713 -571.713 -571.713] (1.000)
Step: 28249, Reward: [-545.515 -545.515 -545.515] [48.8992], Avg: [-571.753 -571.753 -571.753] (1.000)
Step: 28299, Reward: [-557.949 -557.949 -557.949] [73.8746], Avg: [-571.859 -571.859 -571.859] (1.000)
Step: 28349, Reward: [-476.188 -476.188 -476.188] [41.1279], Avg: [-571.763 -571.763 -571.763] (1.000)
Step: 28399, Reward: [-465.038 -465.038 -465.038] [75.5269], Avg: [-571.708 -571.708 -571.708] (1.000)
Step: 28449, Reward: [-576.762 -576.762 -576.762] [121.6866], Avg: [-571.931 -571.931 -571.931] (1.000)
Step: 28499, Reward: [-534.417 -534.417 -534.417] [111.5867], Avg: [-572.061 -572.061 -572.061] (1.000)
Step: 28549, Reward: [-509.698 -509.698 -509.698] [68.5048], Avg: [-572.072 -572.072 -572.072] (1.000)
Step: 28599, Reward: [-564.801 -564.801 -564.801] [98.3838], Avg: [-572.231 -572.231 -572.231] (1.000)
Step: 28649, Reward: [-451.055 -451.055 -451.055] [58.6915], Avg: [-572.122 -572.122 -572.122] (1.000)
Step: 28699, Reward: [-605.402 -605.402 -605.402] [200.0581], Avg: [-572.528 -572.528 -572.528] (1.000)
Step: 28749, Reward: [-434.762 -434.762 -434.762] [71.0701], Avg: [-572.412 -572.412 -572.412] (1.000)
Step: 28799, Reward: [-474.497 -474.497 -474.497] [66.8114], Avg: [-572.358 -572.358 -572.358] (1.000)
Step: 28849, Reward: [-454.643 -454.643 -454.643] [119.1443], Avg: [-572.361 -572.361 -572.361] (1.000)
Step: 28899, Reward: [-556.143 -556.143 -556.143] [95.6208], Avg: [-572.498 -572.498 -572.498] (1.000)
Step: 28949, Reward: [-497.803 -497.803 -497.803] [74.3425], Avg: [-572.498 -572.498 -572.498] (1.000)
Step: 28999, Reward: [-544.166 -544.166 -544.166] [119.5329], Avg: [-572.655 -572.655 -572.655] (1.000)
Step: 29049, Reward: [-484.738 -484.738 -484.738] [96.9062], Avg: [-572.67 -572.67 -572.67] (1.000)
Step: 29099, Reward: [-525.595 -525.595 -525.595] [107.8739], Avg: [-572.775 -572.775 -572.775] (1.000)
Step: 29149, Reward: [-531.064 -531.064 -531.064] [82.8181], Avg: [-572.845 -572.845 -572.845] (1.000)
Step: 29199, Reward: [-462.857 -462.857 -462.857] [57.9513], Avg: [-572.756 -572.756 -572.756] (1.000)
Step: 29249, Reward: [-528.693 -528.693 -528.693] [106.1274], Avg: [-572.862 -572.862 -572.862] (1.000)
Step: 29299, Reward: [-505.122 -505.122 -505.122] [71.2895], Avg: [-572.868 -572.868 -572.868] (1.000)
Step: 29349, Reward: [-577.755 -577.755 -577.755] [137.4168], Avg: [-573.111 -573.111 -573.111] (1.000)
Step: 29399, Reward: [-447.345 -447.345 -447.345] [62.2666], Avg: [-573.003 -573.003 -573.003] (1.000)
Step: 29449, Reward: [-489.009 -489.009 -489.009] [81.6952], Avg: [-572.999 -572.999 -572.999] (1.000)
Step: 29499, Reward: [-474.125 -474.125 -474.125] [95.0746], Avg: [-572.992 -572.992 -572.992] (1.000)
Step: 29549, Reward: [-477.611 -477.611 -477.611] [26.1126], Avg: [-572.875 -572.875 -572.875] (1.000)
Step: 29599, Reward: [-448.191 -448.191 -448.191] [86.7332], Avg: [-572.811 -572.811 -572.811] (1.000)
Step: 29649, Reward: [-459.285 -459.285 -459.285] [103.9380], Avg: [-572.795 -572.795 -572.795] (1.000)
Step: 29699, Reward: [-488.14 -488.14 -488.14] [106.4512], Avg: [-572.832 -572.832 -572.832] (1.000)
Step: 29749, Reward: [-474.699 -474.699 -474.699] [109.0791], Avg: [-572.85 -572.85 -572.85] (1.000)
Step: 29799, Reward: [-482.284 -482.284 -482.284] [40.6699], Avg: [-572.766 -572.766 -572.766] (1.000)
Step: 29849, Reward: [-543.81 -543.81 -543.81] [112.9934], Avg: [-572.907 -572.907 -572.907] (1.000)
Step: 29899, Reward: [-455.132 -455.132 -455.132] [60.5834], Avg: [-572.811 -572.811 -572.811] (1.000)
Step: 29949, Reward: [-519.975 -519.975 -519.975] [77.6970], Avg: [-572.853 -572.853 -572.853] (1.000)
Step: 29999, Reward: [-526.531 -526.531 -526.531] [69.6058], Avg: [-572.892 -572.892 -572.892] (1.000)
Step: 30049, Reward: [-425.918 -425.918 -425.918] [60.8295], Avg: [-572.748 -572.748 -572.748] (1.000)
Step: 30099, Reward: [-463.32 -463.32 -463.32] [45.9420], Avg: [-572.643 -572.643 -572.643] (1.000)
Step: 30149, Reward: [-496.997 -496.997 -496.997] [133.9452], Avg: [-572.74 -572.74 -572.74] (1.000)
Step: 30199, Reward: [-482.457 -482.457 -482.457] [48.3793], Avg: [-572.67 -572.67 -572.67] (1.000)
Step: 30249, Reward: [-428.084 -428.084 -428.084] [41.2330], Avg: [-572.499 -572.499 -572.499] (1.000)
Step: 30299, Reward: [-394.976 -394.976 -394.976] [49.5902], Avg: [-572.288 -572.288 -572.288] (1.000)
Step: 30349, Reward: [-527.343 -527.343 -527.343] [80.8506], Avg: [-572.347 -572.347 -572.347] (1.000)
Step: 30399, Reward: [-483.21 -483.21 -483.21] [93.1663], Avg: [-572.354 -572.354 -572.354] (1.000)
Step: 30449, Reward: [-465.143 -465.143 -465.143] [48.0353], Avg: [-572.257 -572.257 -572.257] (1.000)
Step: 30499, Reward: [-470.984 -470.984 -470.984] [99.0471], Avg: [-572.253 -572.253 -572.253] (1.000)
Step: 30549, Reward: [-490.88 -490.88 -490.88] [102.5625], Avg: [-572.288 -572.288 -572.288] (1.000)
Step: 30599, Reward: [-502.229 -502.229 -502.229] [140.2292], Avg: [-572.403 -572.403 -572.403] (1.000)
Step: 30649, Reward: [-554.27 -554.27 -554.27] [79.4620], Avg: [-572.503 -572.503 -572.503] (1.000)
Step: 30699, Reward: [-502.356 -502.356 -502.356] [63.7696], Avg: [-572.492 -572.492 -572.492] (1.000)
Step: 30749, Reward: [-485.105 -485.105 -485.105] [74.0160], Avg: [-572.47 -572.47 -572.47] (1.000)
Step: 30799, Reward: [-552.153 -552.153 -552.153] [83.4432], Avg: [-572.573 -572.573 -572.573] (1.000)
Step: 30849, Reward: [-521.602 -521.602 -521.602] [76.4807], Avg: [-572.614 -572.614 -572.614] (1.000)
Step: 30899, Reward: [-528.901 -528.901 -528.901] [67.5891], Avg: [-572.653 -572.653 -572.653] (1.000)
Step: 30949, Reward: [-503.616 -503.616 -503.616] [66.9137], Avg: [-572.65 -572.65 -572.65] (1.000)
Step: 30999, Reward: [-429.949 -429.949 -429.949] [31.8718], Avg: [-572.471 -572.471 -572.471] (1.000)
Step: 31049, Reward: [-511.993 -511.993 -511.993] [92.8346], Avg: [-572.523 -572.523 -572.523] (1.000)
Step: 31099, Reward: [-451.837 -451.837 -451.837] [65.6138], Avg: [-572.434 -572.434 -572.434] (1.000)
Step: 31149, Reward: [-473.371 -473.371 -473.371] [72.8641], Avg: [-572.392 -572.392 -572.392] (1.000)
Step: 31199, Reward: [-570.315 -570.315 -570.315] [70.0008], Avg: [-572.501 -572.501 -572.501] (1.000)
Step: 31249, Reward: [-480.017 -480.017 -480.017] [157.9905], Avg: [-572.606 -572.606 -572.606] (1.000)
Step: 31299, Reward: [-492.664 -492.664 -492.664] [36.5364], Avg: [-572.537 -572.537 -572.537] (1.000)
Step: 31349, Reward: [-472.401 -472.401 -472.401] [46.1543], Avg: [-572.451 -572.451 -572.451] (1.000)
Step: 31399, Reward: [-456.538 -456.538 -456.538] [76.4614], Avg: [-572.388 -572.388 -572.388] (1.000)
Step: 31449, Reward: [-523.864 -523.864 -523.864] [90.6433], Avg: [-572.455 -572.455 -572.455] (1.000)
Step: 31499, Reward: [-546.375 -546.375 -546.375] [86.0598], Avg: [-572.55 -572.55 -572.55] (1.000)
Step: 31549, Reward: [-486.867 -486.867 -486.867] [59.2590], Avg: [-572.508 -572.508 -572.508] (1.000)
Step: 31599, Reward: [-456.632 -456.632 -456.632] [48.4223], Avg: [-572.401 -572.401 -572.401] (1.000)
Step: 31649, Reward: [-538.25 -538.25 -538.25] [75.8366], Avg: [-572.467 -572.467 -572.467] (1.000)
Step: 31699, Reward: [-474.902 -474.902 -474.902] [86.3399], Avg: [-572.449 -572.449 -572.449] (1.000)
Step: 31749, Reward: [-522.486 -522.486 -522.486] [53.6651], Avg: [-572.455 -572.455 -572.455] (1.000)
Step: 31799, Reward: [-469.351 -469.351 -469.351] [42.7468], Avg: [-572.36 -572.36 -572.36] (1.000)
Step: 31849, Reward: [-465.342 -465.342 -465.342] [108.9385], Avg: [-572.363 -572.363 -572.363] (1.000)
Step: 31899, Reward: [-530.024 -530.024 -530.024] [91.2556], Avg: [-572.44 -572.44 -572.44] (1.000)
Step: 31949, Reward: [-464.398 -464.398 -464.398] [66.3627], Avg: [-572.375 -572.375 -572.375] (1.000)
Step: 31999, Reward: [-482.391 -482.391 -482.391] [58.9420], Avg: [-572.326 -572.326 -572.326] (1.000)
Step: 32049, Reward: [-503.605 -503.605 -503.605] [71.8530], Avg: [-572.331 -572.331 -572.331] (1.000)
Step: 32099, Reward: [-412.821 -412.821 -412.821] [46.4326], Avg: [-572.155 -572.155 -572.155] (1.000)
Step: 32149, Reward: [-466.469 -466.469 -466.469] [51.9660], Avg: [-572.071 -572.071 -572.071] (1.000)
Step: 32199, Reward: [-529.657 -529.657 -529.657] [31.9244], Avg: [-572.055 -572.055 -572.055] (1.000)
Step: 32249, Reward: [-427.326 -427.326 -427.326] [62.3201], Avg: [-571.927 -571.927 -571.927] (1.000)
Step: 32299, Reward: [-588.899 -588.899 -588.899] [113.9253], Avg: [-572.13 -572.13 -572.13] (1.000)
Step: 32349, Reward: [-529.399 -529.399 -529.399] [70.6266], Avg: [-572.173 -572.173 -572.173] (1.000)
Step: 32399, Reward: [-597.375 -597.375 -597.375] [77.6977], Avg: [-572.332 -572.332 -572.332] (1.000)
Step: 32449, Reward: [-509.298 -509.298 -509.298] [49.8142], Avg: [-572.312 -572.312 -572.312] (1.000)
Step: 32499, Reward: [-445.763 -445.763 -445.763] [70.7721], Avg: [-572.226 -572.226 -572.226] (1.000)
Step: 32549, Reward: [-455.579 -455.579 -455.579] [107.8127], Avg: [-572.212 -572.212 -572.212] (1.000)
Step: 32599, Reward: [-462.083 -462.083 -462.083] [41.1390], Avg: [-572.106 -572.106 -572.106] (1.000)
Step: 32649, Reward: [-499.566 -499.566 -499.566] [54.7440], Avg: [-572.079 -572.079 -572.079] (1.000)
Step: 32699, Reward: [-558.045 -558.045 -558.045] [124.7099], Avg: [-572.248 -572.248 -572.248] (1.000)
Step: 32749, Reward: [-482.666 -482.666 -482.666] [40.4731], Avg: [-572.173 -572.173 -572.173] (1.000)
Step: 32799, Reward: [-492.526 -492.526 -492.526] [43.8694], Avg: [-572.119 -572.119 -572.119] (1.000)
Step: 32849, Reward: [-529.11 -529.11 -529.11] [113.8698], Avg: [-572.227 -572.227 -572.227] (1.000)
Step: 32899, Reward: [-517.355 -517.355 -517.355] [57.2563], Avg: [-572.23 -572.23 -572.23] (1.000)
Step: 32949, Reward: [-459.424 -459.424 -459.424] [75.0243], Avg: [-572.173 -572.173 -572.173] (1.000)
Step: 32999, Reward: [-426.382 -426.382 -426.382] [17.9263], Avg: [-571.979 -571.979 -571.979] (1.000)
Step: 33049, Reward: [-543.878 -543.878 -543.878] [109.5200], Avg: [-572.102 -572.102 -572.102] (1.000)
Step: 33099, Reward: [-485.562 -485.562 -485.562] [75.8328], Avg: [-572.086 -572.086 -572.086] (1.000)
Step: 33149, Reward: [-504.135 -504.135 -504.135] [90.5913], Avg: [-572.12 -572.12 -572.12] (1.000)
Step: 33199, Reward: [-449.091 -449.091 -449.091] [16.8520], Avg: [-571.961 -571.961 -571.961] (1.000)
Step: 33249, Reward: [-467.012 -467.012 -467.012] [79.1775], Avg: [-571.922 -571.922 -571.922] (1.000)
Step: 33299, Reward: [-479.38 -479.38 -479.38] [94.1485], Avg: [-571.924 -571.924 -571.924] (1.000)
Step: 33349, Reward: [-498.506 -498.506 -498.506] [92.7144], Avg: [-571.953 -571.953 -571.953] (1.000)
Step: 33399, Reward: [-529.284 -529.284 -529.284] [120.7884], Avg: [-572.07 -572.07 -572.07] (1.000)
Step: 33449, Reward: [-490.831 -490.831 -490.831] [68.9033], Avg: [-572.052 -572.052 -572.052] (1.000)
Step: 33499, Reward: [-496.052 -496.052 -496.052] [65.7372], Avg: [-572.036 -572.036 -572.036] (1.000)
Step: 33549, Reward: [-444.702 -444.702 -444.702] [72.1287], Avg: [-571.954 -571.954 -571.954] (1.000)
Step: 33599, Reward: [-441.503 -441.503 -441.503] [69.6234], Avg: [-571.864 -571.864 -571.864] (1.000)
Step: 33649, Reward: [-479.493 -479.493 -479.493] [77.5477], Avg: [-571.841 -571.841 -571.841] (1.000)
Step: 33699, Reward: [-606.989 -606.989 -606.989] [102.4853], Avg: [-572.046 -572.046 -572.046] (1.000)
Step: 33749, Reward: [-507.396 -507.396 -507.396] [64.0441], Avg: [-572.045 -572.045 -572.045] (1.000)
Step: 33799, Reward: [-505.125 -505.125 -505.125] [70.6024], Avg: [-572.05 -572.05 -572.05] (1.000)
Step: 33849, Reward: [-454.161 -454.161 -454.161] [71.1954], Avg: [-571.981 -571.981 -571.981] (1.000)
Step: 33899, Reward: [-429.932 -429.932 -429.932] [35.4740], Avg: [-571.824 -571.824 -571.824] (1.000)
Step: 33949, Reward: [-473.251 -473.251 -473.251] [69.7537], Avg: [-571.782 -571.782 -571.782] (1.000)
Step: 33999, Reward: [-429.687 -429.687 -429.687] [46.8344], Avg: [-571.642 -571.642 -571.642] (1.000)
Step: 34049, Reward: [-496.294 -496.294 -496.294] [79.7678], Avg: [-571.648 -571.648 -571.648] (1.000)
Step: 34099, Reward: [-557.974 -557.974 -557.974] [83.5294], Avg: [-571.75 -571.75 -571.75] (1.000)
Step: 34149, Reward: [-646.569 -646.569 -646.569] [148.0017], Avg: [-572.077 -572.077 -572.077] (1.000)
Step: 34199, Reward: [-419.118 -419.118 -419.118] [95.3922], Avg: [-571.993 -571.993 -571.993] (1.000)
Step: 34249, Reward: [-518.329 -518.329 -518.329] [74.0777], Avg: [-572.022 -572.022 -572.022] (1.000)
Step: 34299, Reward: [-443.792 -443.792 -443.792] [57.2693], Avg: [-571.919 -571.919 -571.919] (1.000)
Step: 34349, Reward: [-460.627 -460.627 -460.627] [70.7183], Avg: [-571.86 -571.86 -571.86] (1.000)
Step: 34399, Reward: [-538.943 -538.943 -538.943] [89.9049], Avg: [-571.943 -571.943 -571.943] (1.000)
Step: 34449, Reward: [-469.314 -469.314 -469.314] [110.1767], Avg: [-571.954 -571.954 -571.954] (1.000)
Step: 34499, Reward: [-463.27 -463.27 -463.27] [66.1986], Avg: [-571.892 -571.892 -571.892] (1.000)
Step: 34549, Reward: [-424.234 -424.234 -424.234] [68.8670], Avg: [-571.778 -571.778 -571.778] (1.000)
Step: 34599, Reward: [-479.357 -479.357 -479.357] [83.9844], Avg: [-571.766 -571.766 -571.766] (1.000)
Step: 34649, Reward: [-414.289 -414.289 -414.289] [36.8682], Avg: [-571.592 -571.592 -571.592] (1.000)
Step: 34699, Reward: [-482.855 -482.855 -482.855] [70.0872], Avg: [-571.565 -571.565 -571.565] (1.000)
Step: 34749, Reward: [-456.328 -456.328 -456.328] [76.3557], Avg: [-571.509 -571.509 -571.509] (1.000)
Step: 34799, Reward: [-594.043 -594.043 -594.043] [188.3947], Avg: [-571.812 -571.812 -571.812] (1.000)
Step: 34849, Reward: [-514.081 -514.081 -514.081] [91.2170], Avg: [-571.86 -571.86 -571.86] (1.000)
Step: 34899, Reward: [-447.214 -447.214 -447.214] [54.0362], Avg: [-571.759 -571.759 -571.759] (1.000)
Step: 34949, Reward: [-485.487 -485.487 -485.487] [58.3407], Avg: [-571.719 -571.719 -571.719] (1.000)
Step: 34999, Reward: [-523.171 -523.171 -523.171] [112.6267], Avg: [-571.811 -571.811 -571.811] (1.000)
Step: 35049, Reward: [-443.861 -443.861 -443.861] [79.2786], Avg: [-571.741 -571.741 -571.741] (1.000)
Step: 35099, Reward: [-441.485 -441.485 -441.485] [52.3570], Avg: [-571.63 -571.63 -571.63] (1.000)
Step: 35149, Reward: [-501.968 -501.968 -501.968] [115.5400], Avg: [-571.695 -571.695 -571.695] (1.000)
Step: 35199, Reward: [-546.629 -546.629 -546.629] [135.0219], Avg: [-571.852 -571.852 -571.852] (1.000)
Step: 35249, Reward: [-540.345 -540.345 -540.345] [106.8955], Avg: [-571.958 -571.958 -571.958] (1.000)
Step: 35299, Reward: [-519.613 -519.613 -519.613] [129.1480], Avg: [-572.067 -572.067 -572.067] (1.000)
Step: 35349, Reward: [-611.445 -611.445 -611.445] [93.7584], Avg: [-572.256 -572.256 -572.256] (1.000)
Step: 35399, Reward: [-505.061 -505.061 -505.061] [98.0317], Avg: [-572.299 -572.299 -572.299] (1.000)
Step: 35449, Reward: [-557.954 -557.954 -557.954] [97.5892], Avg: [-572.417 -572.417 -572.417] (1.000)
Step: 35499, Reward: [-530.261 -530.261 -530.261] [76.2750], Avg: [-572.465 -572.465 -572.465] (1.000)
Step: 35549, Reward: [-456.579 -456.579 -456.579] [95.6187], Avg: [-572.436 -572.436 -572.436] (1.000)
Step: 35599, Reward: [-473.705 -473.705 -473.705] [24.7640], Avg: [-572.332 -572.332 -572.332] (1.000)
Step: 35649, Reward: [-452.329 -452.329 -452.329] [36.2684], Avg: [-572.215 -572.215 -572.215] (1.000)
Step: 35699, Reward: [-484.571 -484.571 -484.571] [93.1875], Avg: [-572.223 -572.223 -572.223] (1.000)
Step: 35749, Reward: [-497.96 -497.96 -497.96] [103.8577], Avg: [-572.264 -572.264 -572.264] (1.000)
Step: 35799, Reward: [-424.138 -424.138 -424.138] [47.5259], Avg: [-572.123 -572.123 -572.123] (1.000)
Step: 35849, Reward: [-459.385 -459.385 -459.385] [69.5724], Avg: [-572.063 -572.063 -572.063] (1.000)
Step: 35899, Reward: [-576.465 -576.465 -576.465] [111.8418], Avg: [-572.225 -572.225 -572.225] (1.000)
Step: 35949, Reward: [-556.847 -556.847 -556.847] [123.3229], Avg: [-572.375 -572.375 -572.375] (1.000)
Step: 35999, Reward: [-554.947 -554.947 -554.947] [150.6519], Avg: [-572.56 -572.56 -572.56] (1.000)
Step: 36049, Reward: [-519.28 -519.28 -519.28] [80.1831], Avg: [-572.598 -572.598 -572.598] (1.000)
Step: 36099, Reward: [-395.052 -395.052 -395.052] [10.0983], Avg: [-572.366 -572.366 -572.366] (1.000)
Step: 36149, Reward: [-515.7 -515.7 -515.7] [114.2805], Avg: [-572.445 -572.445 -572.445] (1.000)
Step: 36199, Reward: [-569.477 -569.477 -569.477] [118.7761], Avg: [-572.605 -572.605 -572.605] (1.000)
Step: 36249, Reward: [-541.432 -541.432 -541.432] [72.6279], Avg: [-572.662 -572.662 -572.662] (1.000)
Step: 36299, Reward: [-513.876 -513.876 -513.876] [98.1135], Avg: [-572.717 -572.717 -572.717] (1.000)
Step: 36349, Reward: [-514.667 -514.667 -514.667] [111.7367], Avg: [-572.791 -572.791 -572.791] (1.000)
Step: 36399, Reward: [-484.352 -484.352 -484.352] [50.9753], Avg: [-572.739 -572.739 -572.739] (1.000)
Step: 36449, Reward: [-464.366 -464.366 -464.366] [34.1581], Avg: [-572.637 -572.637 -572.637] (1.000)
Step: 36499, Reward: [-513.774 -513.774 -513.774] [140.5168], Avg: [-572.749 -572.749 -572.749] (1.000)
Step: 36549, Reward: [-453.989 -453.989 -453.989] [79.6982], Avg: [-572.696 -572.696 -572.696] (1.000)
Step: 36599, Reward: [-513.866 -513.866 -513.866] [62.0488], Avg: [-572.7 -572.7 -572.7] (1.000)
Step: 36649, Reward: [-545.815 -545.815 -545.815] [121.2227], Avg: [-572.829 -572.829 -572.829] (1.000)
Step: 36699, Reward: [-500.79 -500.79 -500.79] [100.8570], Avg: [-572.868 -572.868 -572.868] (1.000)
Step: 36749, Reward: [-506.252 -506.252 -506.252] [105.3496], Avg: [-572.921 -572.921 -572.921] (1.000)
Step: 36799, Reward: [-615.764 -615.764 -615.764] [118.3127], Avg: [-573.14 -573.14 -573.14] (1.000)
Step: 36849, Reward: [-472.679 -472.679 -472.679] [94.3627], Avg: [-573.131 -573.131 -573.131] (1.000)
Step: 36899, Reward: [-510.683 -510.683 -510.683] [81.4068], Avg: [-573.157 -573.157 -573.157] (1.000)
Step: 36949, Reward: [-485.056 -485.056 -485.056] [112.5717], Avg: [-573.19 -573.19 -573.19] (1.000)
Step: 36999, Reward: [-527.053 -527.053 -527.053] [129.1309], Avg: [-573.302 -573.302 -573.302] (1.000)
Step: 37049, Reward: [-493.74 -493.74 -493.74] [68.4186], Avg: [-573.287 -573.287 -573.287] (1.000)
Step: 37099, Reward: [-470.234 -470.234 -470.234] [95.3479], Avg: [-573.277 -573.277 -573.277] (1.000)
Step: 37149, Reward: [-435.119 -435.119 -435.119] [63.0403], Avg: [-573.176 -573.176 -573.176] (1.000)
Step: 37199, Reward: [-517.424 -517.424 -517.424] [61.0745], Avg: [-573.183 -573.183 -573.183] (1.000)
Step: 37249, Reward: [-543.772 -543.772 -543.772] [104.6014], Avg: [-573.284 -573.284 -573.284] (1.000)
Step: 37299, Reward: [-615.199 -615.199 -615.199] [104.2163], Avg: [-573.48 -573.48 -573.48] (1.000)
Step: 37349, Reward: [-511.464 -511.464 -511.464] [81.6665], Avg: [-573.506 -573.506 -573.506] (1.000)
Step: 37399, Reward: [-521.826 -521.826 -521.826] [95.4401], Avg: [-573.565 -573.565 -573.565] (1.000)
Step: 37449, Reward: [-542.843 -542.843 -542.843] [90.6969], Avg: [-573.645 -573.645 -573.645] (1.000)
Step: 37499, Reward: [-475.183 -475.183 -475.183] [67.2849], Avg: [-573.603 -573.603 -573.603] (1.000)
Step: 37549, Reward: [-518.146 -518.146 -518.146] [119.3038], Avg: [-573.688 -573.688 -573.688] (1.000)
Step: 37599, Reward: [-506.497 -506.497 -506.497] [64.8062], Avg: [-573.685 -573.685 -573.685] (1.000)
Step: 37649, Reward: [-546.048 -546.048 -546.048] [111.1672], Avg: [-573.796 -573.796 -573.796] (1.000)
Step: 37699, Reward: [-580.4 -580.4 -580.4] [161.8014], Avg: [-574.019 -574.019 -574.019] (1.000)
Step: 37749, Reward: [-552.954 -552.954 -552.954] [125.6764], Avg: [-574.158 -574.158 -574.158] (1.000)
Step: 37799, Reward: [-510.303 -510.303 -510.303] [59.2599], Avg: [-574.152 -574.152 -574.152] (1.000)
Step: 37849, Reward: [-455.751 -455.751 -455.751] [29.1986], Avg: [-574.034 -574.034 -574.034] (1.000)
Step: 37899, Reward: [-518.633 -518.633 -518.633] [124.0091], Avg: [-574.124 -574.124 -574.124] (1.000)
Step: 37949, Reward: [-487.118 -487.118 -487.118] [98.1730], Avg: [-574.139 -574.139 -574.139] (1.000)
Step: 37999, Reward: [-532.642 -532.642 -532.642] [75.9950], Avg: [-574.185 -574.185 -574.185] (1.000)
Step: 38049, Reward: [-513.125 -513.125 -513.125] [29.6512], Avg: [-574.143 -574.143 -574.143] (1.000)
Step: 38099, Reward: [-513.132 -513.132 -513.132] [102.3653], Avg: [-574.198 -574.198 -574.198] (1.000)
Step: 38149, Reward: [-482.677 -482.677 -482.677] [38.0476], Avg: [-574.127 -574.127 -574.127] (1.000)
Step: 38199, Reward: [-465.681 -465.681 -465.681] [90.3153], Avg: [-574.104 -574.104 -574.104] (1.000)
Step: 38249, Reward: [-479.753 -479.753 -479.753] [52.3198], Avg: [-574.049 -574.049 -574.049] (1.000)
Step: 38299, Reward: [-537.315 -537.315 -537.315] [118.6482], Avg: [-574.156 -574.156 -574.156] (1.000)
Step: 38349, Reward: [-491.82 -491.82 -491.82] [68.8507], Avg: [-574.138 -574.138 -574.138] (1.000)
Step: 38399, Reward: [-448.172 -448.172 -448.172] [107.4537], Avg: [-574.114 -574.114 -574.114] (1.000)
Step: 38449, Reward: [-485.731 -485.731 -485.731] [81.5449], Avg: [-574.105 -574.105 -574.105] (1.000)
Step: 38499, Reward: [-466.688 -466.688 -466.688] [82.2130], Avg: [-574.072 -574.072 -574.072] (1.000)
Step: 38549, Reward: [-513.091 -513.091 -513.091] [54.8188], Avg: [-574.064 -574.064 -574.064] (1.000)
Step: 38599, Reward: [-491.88 -491.88 -491.88] [108.8008], Avg: [-574.099 -574.099 -574.099] (1.000)
Step: 38649, Reward: [-433.317 -433.317 -433.317] [73.5530], Avg: [-574.012 -574.012 -574.012] (1.000)
Step: 38699, Reward: [-454.561 -454.561 -454.561] [107.4644], Avg: [-573.996 -573.996 -573.996] (1.000)
Step: 38749, Reward: [-483.04 -483.04 -483.04] [77.8118], Avg: [-573.979 -573.979 -573.979] (1.000)
Step: 38799, Reward: [-548.842 -548.842 -548.842] [113.9478], Avg: [-574.094 -574.094 -574.094] (1.000)
Step: 38849, Reward: [-501.047 -501.047 -501.047] [91.6105], Avg: [-574.118 -574.118 -574.118] (1.000)
Step: 38899, Reward: [-467.416 -467.416 -467.416] [41.0556], Avg: [-574.033 -574.033 -574.033] (1.000)
Step: 38949, Reward: [-551.775 -551.775 -551.775] [113.0302], Avg: [-574.15 -574.15 -574.15] (1.000)
Step: 38999, Reward: [-427.385 -427.385 -427.385] [62.7981], Avg: [-574.042 -574.042 -574.042] (1.000)
Step: 39049, Reward: [-529.997 -529.997 -529.997] [162.6659], Avg: [-574.194 -574.194 -574.194] (1.000)
Step: 39099, Reward: [-471.086 -471.086 -471.086] [60.1635], Avg: [-574.139 -574.139 -574.139] (1.000)
Step: 39149, Reward: [-503.728 -503.728 -503.728] [131.7528], Avg: [-574.218 -574.218 -574.218] (1.000)
Step: 39199, Reward: [-512.317 -512.317 -512.317] [93.3810], Avg: [-574.258 -574.258 -574.258] (1.000)
Step: 39249, Reward: [-491.944 -491.944 -491.944] [129.1088], Avg: [-574.317 -574.317 -574.317] (1.000)
Step: 39299, Reward: [-541.636 -541.636 -541.636] [73.8525], Avg: [-574.37 -574.37 -574.37] (1.000)
Step: 39349, Reward: [-472.511 -472.511 -472.511] [67.3649], Avg: [-574.326 -574.326 -574.326] (1.000)
Step: 39399, Reward: [-437.156 -437.156 -437.156] [102.2253], Avg: [-574.282 -574.282 -574.282] (1.000)
Step: 39449, Reward: [-440.381 -440.381 -440.381] [93.9364], Avg: [-574.231 -574.231 -574.231] (1.000)
Step: 39499, Reward: [-479.456 -479.456 -479.456] [85.4028], Avg: [-574.219 -574.219 -574.219] (1.000)
Step: 39549, Reward: [-470.18 -470.18 -470.18] [42.9069], Avg: [-574.142 -574.142 -574.142] (1.000)
Step: 39599, Reward: [-568.957 -568.957 -568.957] [100.6162], Avg: [-574.262 -574.262 -574.262] (1.000)
Step: 39649, Reward: [-483.269 -483.269 -483.269] [77.3032], Avg: [-574.245 -574.245 -574.245] (1.000)
Step: 39699, Reward: [-518.262 -518.262 -518.262] [94.7567], Avg: [-574.294 -574.294 -574.294] (1.000)
Step: 39749, Reward: [-417.793 -417.793 -417.793] [46.3255], Avg: [-574.155 -574.155 -574.155] (1.000)
Step: 39799, Reward: [-491.696 -491.696 -491.696] [36.9331], Avg: [-574.098 -574.098 -574.098] (1.000)
Step: 39849, Reward: [-519.618 -519.618 -519.618] [46.7536], Avg: [-574.088 -574.088 -574.088] (1.000)
Step: 39899, Reward: [-431.731 -431.731 -431.731] [84.6932], Avg: [-574.016 -574.016 -574.016] (1.000)
Step: 39949, Reward: [-456.284 -456.284 -456.284] [29.4868], Avg: [-573.906 -573.906 -573.906] (1.000)
Step: 39999, Reward: [-503.347 -503.347 -503.347] [41.0562], Avg: [-573.869 -573.869 -573.869] (1.000)
Step: 40049, Reward: [-453.498 -453.498 -453.498] [74.5827], Avg: [-573.812 -573.812 -573.812] (1.000)
Step: 40099, Reward: [-525.646 -525.646 -525.646] [80.3802], Avg: [-573.852 -573.852 -573.852] (1.000)
Step: 40149, Reward: [-525.356 -525.356 -525.356] [34.5224], Avg: [-573.834 -573.834 -573.834] (1.000)
Step: 40199, Reward: [-510.52 -510.52 -510.52] [60.2501], Avg: [-573.831 -573.831 -573.831] (1.000)
Step: 40249, Reward: [-527.377 -527.377 -527.377] [65.2384], Avg: [-573.854 -573.854 -573.854] (1.000)
Step: 40299, Reward: [-460.62 -460.62 -460.62] [85.6772], Avg: [-573.82 -573.82 -573.82] (1.000)
Step: 40349, Reward: [-482.202 -482.202 -482.202] [56.8667], Avg: [-573.777 -573.777 -573.777] (1.000)
Step: 40399, Reward: [-428.867 -428.867 -428.867] [29.8624], Avg: [-573.634 -573.634 -573.634] (1.000)
Step: 40449, Reward: [-467.813 -467.813 -467.813] [68.3317], Avg: [-573.588 -573.588 -573.588] (1.000)
Step: 40499, Reward: [-488.849 -488.849 -488.849] [68.8488], Avg: [-573.568 -573.568 -573.568] (1.000)
Step: 40549, Reward: [-504.197 -504.197 -504.197] [85.9441], Avg: [-573.589 -573.589 -573.589] (1.000)
Step: 40599, Reward: [-528.62 -528.62 -528.62] [81.5497], Avg: [-573.634 -573.634 -573.634] (1.000)
Step: 40649, Reward: [-433.515 -433.515 -433.515] [59.4908], Avg: [-573.535 -573.535 -573.535] (1.000)
Step: 40699, Reward: [-524.441 -524.441 -524.441] [82.9700], Avg: [-573.576 -573.576 -573.576] (1.000)
Step: 40749, Reward: [-481.087 -481.087 -481.087] [102.3403], Avg: [-573.588 -573.588 -573.588] (1.000)
Step: 40799, Reward: [-423.535 -423.535 -423.535] [82.8621], Avg: [-573.506 -573.506 -573.506] (1.000)
Step: 40849, Reward: [-481.586 -481.586 -481.586] [93.8811], Avg: [-573.508 -573.508 -573.508] (1.000)
Step: 40899, Reward: [-488.487 -488.487 -488.487] [46.9278], Avg: [-573.462 -573.462 -573.462] (1.000)
Step: 40949, Reward: [-515.293 -515.293 -515.293] [95.9779], Avg: [-573.508 -573.508 -573.508] (1.000)
Step: 40999, Reward: [-492.335 -492.335 -492.335] [140.5839], Avg: [-573.58 -573.58 -573.58] (1.000)
Step: 41049, Reward: [-500.924 -500.924 -500.924] [87.3631], Avg: [-573.598 -573.598 -573.598] (1.000)
Step: 41099, Reward: [-406.768 -406.768 -406.768] [70.1518], Avg: [-573.481 -573.481 -573.481] (1.000)
Step: 41149, Reward: [-516.725 -516.725 -516.725] [78.6492], Avg: [-573.507 -573.507 -573.507] (1.000)
Step: 41199, Reward: [-424.45 -424.45 -424.45] [48.7788], Avg: [-573.386 -573.386 -573.386] (1.000)
Step: 41249, Reward: [-537.237 -537.237 -537.237] [127.1581], Avg: [-573.496 -573.496 -573.496] (1.000)
Step: 41299, Reward: [-565.856 -565.856 -565.856] [65.0231], Avg: [-573.565 -573.565 -573.565] (1.000)
Step: 41349, Reward: [-487.764 -487.764 -487.764] [72.7720], Avg: [-573.55 -573.55 -573.55] (1.000)
Step: 41399, Reward: [-535.793 -535.793 -535.793] [154.9292], Avg: [-573.691 -573.691 -573.691] (1.000)
Step: 41449, Reward: [-471.137 -471.137 -471.137] [112.2798], Avg: [-573.703 -573.703 -573.703] (1.000)
Step: 41499, Reward: [-489.155 -489.155 -489.155] [52.4396], Avg: [-573.664 -573.664 -573.664] (1.000)
Step: 41549, Reward: [-468.533 -468.533 -468.533] [56.1929], Avg: [-573.605 -573.605 -573.605] (1.000)
Step: 41599, Reward: [-531.686 -531.686 -531.686] [116.4806], Avg: [-573.695 -573.695 -573.695] (1.000)
Step: 41649, Reward: [-486.741 -486.741 -486.741] [119.6084], Avg: [-573.734 -573.734 -573.734] (1.000)
Step: 41699, Reward: [-491.041 -491.041 -491.041] [105.5439], Avg: [-573.762 -573.762 -573.762] (1.000)
Step: 41749, Reward: [-415.798 -415.798 -415.798] [81.5317], Avg: [-573.67 -573.67 -573.67] (1.000)
Step: 41799, Reward: [-538.939 -538.939 -538.939] [138.3234], Avg: [-573.794 -573.794 -573.794] (1.000)
Step: 41849, Reward: [-464.561 -464.561 -464.561] [61.8757], Avg: [-573.737 -573.737 -573.737] (1.000)
Step: 41899, Reward: [-455.737 -455.737 -455.737] [78.1991], Avg: [-573.69 -573.69 -573.69] (1.000)
Step: 41949, Reward: [-493.098 -493.098 -493.098] [67.4054], Avg: [-573.674 -573.674 -573.674] (1.000)
Step: 41999, Reward: [-477.524 -477.524 -477.524] [73.9306], Avg: [-573.648 -573.648 -573.648] (1.000)
Step: 42049, Reward: [-470.573 -470.573 -470.573] [72.8329], Avg: [-573.612 -573.612 -573.612] (1.000)
Step: 42099, Reward: [-450.884 -450.884 -450.884] [6.6715], Avg: [-573.474 -573.474 -573.474] (1.000)
Step: 42149, Reward: [-425.376 -425.376 -425.376] [20.2015], Avg: [-573.322 -573.322 -573.322] (1.000)
Step: 42199, Reward: [-501.41 -501.41 -501.41] [116.2411], Avg: [-573.375 -573.375 -573.375] (1.000)
Step: 42249, Reward: [-530.423 -530.423 -530.423] [108.8241], Avg: [-573.453 -573.453 -573.453] (1.000)
Step: 42299, Reward: [-506.677 -506.677 -506.677] [122.6718], Avg: [-573.519 -573.519 -573.519] (1.000)
Step: 42349, Reward: [-607.198 -607.198 -607.198] [82.1388], Avg: [-573.655 -573.655 -573.655] (1.000)
Step: 42399, Reward: [-483.031 -483.031 -483.031] [81.0227], Avg: [-573.644 -573.644 -573.644] (1.000)
Step: 42449, Reward: [-474.9 -474.9 -474.9] [57.7056], Avg: [-573.596 -573.596 -573.596] (1.000)
Step: 42499, Reward: [-497.764 -497.764 -497.764] [104.3793], Avg: [-573.629 -573.629 -573.629] (1.000)
Step: 42549, Reward: [-524.117 -524.117 -524.117] [87.0364], Avg: [-573.673 -573.673 -573.673] (1.000)
Step: 42599, Reward: [-512.318 -512.318 -512.318] [86.4391], Avg: [-573.703 -573.703 -573.703] (1.000)
Step: 42649, Reward: [-449.526 -449.526 -449.526] [78.3419], Avg: [-573.649 -573.649 -573.649] (1.000)
Step: 42699, Reward: [-442.97 -442.97 -442.97] [68.4836], Avg: [-573.576 -573.576 -573.576] (1.000)
Step: 42749, Reward: [-489.61 -489.61 -489.61] [37.4350], Avg: [-573.522 -573.522 -573.522] (1.000)
Step: 42799, Reward: [-444.563 -444.563 -444.563] [65.4488], Avg: [-573.448 -573.448 -573.448] (1.000)
Step: 42849, Reward: [-443.718 -443.718 -443.718] [92.7449], Avg: [-573.405 -573.405 -573.405] (1.000)
Step: 42899, Reward: [-493.621 -493.621 -493.621] [26.3061], Avg: [-573.342 -573.342 -573.342] (1.000)
Step: 42949, Reward: [-525.743 -525.743 -525.743] [70.7731], Avg: [-573.369 -573.369 -573.369] (1.000)
Step: 42999, Reward: [-479.19 -479.19 -479.19] [67.5686], Avg: [-573.338 -573.338 -573.338] (1.000)
Step: 43049, Reward: [-406.728 -406.728 -406.728] [26.9324], Avg: [-573.176 -573.176 -573.176] (1.000)
Step: 43099, Reward: [-456.588 -456.588 -456.588] [57.5687], Avg: [-573.108 -573.108 -573.108] (1.000)
Step: 43149, Reward: [-452.908 -452.908 -452.908] [71.2364], Avg: [-573.051 -573.051 -573.051] (1.000)
Step: 43199, Reward: [-441.201 -441.201 -441.201] [75.8905], Avg: [-572.986 -572.986 -572.986] (1.000)
Step: 43249, Reward: [-513.473 -513.473 -513.473] [95.6535], Avg: [-573.028 -573.028 -573.028] (1.000)
Step: 43299, Reward: [-511.004 -511.004 -511.004] [96.4738], Avg: [-573.068 -573.068 -573.068] (1.000)
Step: 43349, Reward: [-522.814 -522.814 -522.814] [80.9576], Avg: [-573.103 -573.103 -573.103] (1.000)
Step: 43399, Reward: [-476.888 -476.888 -476.888] [51.4470], Avg: [-573.051 -573.051 -573.051] (1.000)
Step: 43449, Reward: [-421.992 -421.992 -421.992] [60.7888], Avg: [-572.948 -572.948 -572.948] (1.000)
Step: 43499, Reward: [-518.829 -518.829 -518.829] [86.3529], Avg: [-572.985 -572.985 -572.985] (1.000)
Step: 43549, Reward: [-535.626 -535.626 -535.626] [117.5696], Avg: [-573.077 -573.077 -573.077] (1.000)
Step: 43599, Reward: [-488.931 -488.931 -488.931] [81.5230], Avg: [-573.074 -573.074 -573.074] (1.000)
Step: 43649, Reward: [-496.21 -496.21 -496.21] [97.7960], Avg: [-573.098 -573.098 -573.098] (1.000)
Step: 43699, Reward: [-427.836 -427.836 -427.836] [42.8402], Avg: [-572.981 -572.981 -572.981] (1.000)
Step: 43749, Reward: [-501.717 -501.717 -501.717] [76.5256], Avg: [-572.987 -572.987 -572.987] (1.000)
Step: 43799, Reward: [-471.008 -471.008 -471.008] [67.3529], Avg: [-572.947 -572.947 -572.947] (1.000)
Step: 43849, Reward: [-495.422 -495.422 -495.422] [85.1862], Avg: [-572.956 -572.956 -572.956] (1.000)
Step: 43899, Reward: [-483.596 -483.596 -483.596] [71.8543], Avg: [-572.936 -572.936 -572.936] (1.000)
Step: 43949, Reward: [-511.42 -511.42 -511.42] [156.3383], Avg: [-573.044 -573.044 -573.044] (1.000)
Step: 43999, Reward: [-484.335 -484.335 -484.335] [102.6595], Avg: [-573.06 -573.06 -573.06] (1.000)
Step: 44049, Reward: [-474.843 -474.843 -474.843] [74.3025], Avg: [-573.032 -573.032 -573.032] (1.000)
Step: 44099, Reward: [-480.142 -480.142 -480.142] [55.7386], Avg: [-572.99 -572.99 -572.99] (1.000)
Step: 44149, Reward: [-483.671 -483.671 -483.671] [83.7602], Avg: [-572.984 -572.984 -572.984] (1.000)
Step: 44199, Reward: [-498.357 -498.357 -498.357] [96.1395], Avg: [-573.008 -573.008 -573.008] (1.000)
Step: 44249, Reward: [-565.801 -565.801 -565.801] [100.6855], Avg: [-573.114 -573.114 -573.114] (1.000)
Step: 44299, Reward: [-399.31 -399.31 -399.31] [46.3167], Avg: [-572.97 -572.97 -572.97] (1.000)
Step: 44349, Reward: [-516.644 -516.644 -516.644] [143.7756], Avg: [-573.069 -573.069 -573.069] (1.000)
Step: 44399, Reward: [-473.602 -473.602 -473.602] [81.3663], Avg: [-573.048 -573.048 -573.048] (1.000)
Step: 44449, Reward: [-417.106 -417.106 -417.106] [34.6682], Avg: [-572.912 -572.912 -572.912] (1.000)
Step: 44499, Reward: [-504.174 -504.174 -504.174] [71.7184], Avg: [-572.915 -572.915 -572.915] (1.000)
Step: 44549, Reward: [-440.108 -440.108 -440.108] [89.6762], Avg: [-572.867 -572.867 -572.867] (1.000)
Step: 44599, Reward: [-485.324 -485.324 -485.324] [121.4956], Avg: [-572.905 -572.905 -572.905] (1.000)
Step: 44649, Reward: [-440.417 -440.417 -440.417] [40.7143], Avg: [-572.802 -572.802 -572.802] (1.000)
Step: 44699, Reward: [-465.909 -465.909 -465.909] [83.9661], Avg: [-572.776 -572.776 -572.776] (1.000)
Step: 44749, Reward: [-509.419 -509.419 -509.419] [60.2849], Avg: [-572.773 -572.773 -572.773] (1.000)
Step: 44799, Reward: [-560.692 -560.692 -560.692] [85.1320], Avg: [-572.855 -572.855 -572.855] (1.000)
Step: 44849, Reward: [-551.503 -551.503 -551.503] [187.7185], Avg: [-573.04 -573.04 -573.04] (1.000)
Step: 44899, Reward: [-452.329 -452.329 -452.329] [59.9287], Avg: [-572.972 -572.972 -572.972] (1.000)
Step: 44949, Reward: [-460.912 -460.912 -460.912] [38.0286], Avg: [-572.89 -572.89 -572.89] (1.000)
Step: 44999, Reward: [-460.137 -460.137 -460.137] [75.1213], Avg: [-572.848 -572.848 -572.848] (1.000)
Step: 45049, Reward: [-535.4 -535.4 -535.4] [95.8811], Avg: [-572.913 -572.913 -572.913] (1.000)
Step: 45099, Reward: [-493.596 -493.596 -493.596] [67.5130], Avg: [-572.9 -572.9 -572.9] (1.000)
Step: 45149, Reward: [-531.857 -531.857 -531.857] [49.6043], Avg: [-572.909 -572.909 -572.909] (1.000)
Step: 45199, Reward: [-466.406 -466.406 -466.406] [21.3207], Avg: [-572.815 -572.815 -572.815] (1.000)
Step: 45249, Reward: [-520.071 -520.071 -520.071] [122.9677], Avg: [-572.893 -572.893 -572.893] (1.000)
Step: 45299, Reward: [-526.882 -526.882 -526.882] [69.0034], Avg: [-572.918 -572.918 -572.918] (1.000)
Step: 45349, Reward: [-491.588 -491.588 -491.588] [45.1995], Avg: [-572.878 -572.878 -572.878] (1.000)
Step: 45399, Reward: [-495.269 -495.269 -495.269] [66.7474], Avg: [-572.866 -572.866 -572.866] (1.000)
Step: 45449, Reward: [-424.253 -424.253 -424.253] [62.2976], Avg: [-572.771 -572.771 -572.771] (1.000)
Step: 45499, Reward: [-496.923 -496.923 -496.923] [64.0248], Avg: [-572.758 -572.758 -572.758] (1.000)
Step: 45549, Reward: [-461.37 -461.37 -461.37] [81.6945], Avg: [-572.726 -572.726 -572.726] (1.000)
Step: 45599, Reward: [-541.262 -541.262 -541.262] [121.0983], Avg: [-572.824 -572.824 -572.824] (1.000)
Step: 45649, Reward: [-506.425 -506.425 -506.425] [173.3740], Avg: [-572.941 -572.941 -572.941] (1.000)
Step: 45699, Reward: [-472.098 -472.098 -472.098] [41.1910], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 45749, Reward: [-476.93 -476.93 -476.93] [43.7613], Avg: [-572.819 -572.819 -572.819] (1.000)
Step: 45799, Reward: [-507.445 -507.445 -507.445] [154.5864], Avg: [-572.916 -572.916 -572.916] (1.000)
Step: 45849, Reward: [-496.043 -496.043 -496.043] [107.4497], Avg: [-572.95 -572.95 -572.95] (1.000)
Step: 45899, Reward: [-499.731 -499.731 -499.731] [92.5175], Avg: [-572.971 -572.971 -572.971] (1.000)
Step: 45949, Reward: [-452.134 -452.134 -452.134] [76.3648], Avg: [-572.922 -572.922 -572.922] (1.000)
Step: 45999, Reward: [-453.725 -453.725 -453.725] [88.2928], Avg: [-572.889 -572.889 -572.889] (1.000)
Step: 46049, Reward: [-430.708 -430.708 -430.708] [58.3632], Avg: [-572.798 -572.798 -572.798] (1.000)
Step: 46099, Reward: [-477.475 -477.475 -477.475] [70.4836], Avg: [-572.771 -572.771 -572.771] (1.000)
Step: 46149, Reward: [-492.066 -492.066 -492.066] [109.7474], Avg: [-572.802 -572.802 -572.802] (1.000)
Step: 46199, Reward: [-543.173 -543.173 -543.173] [96.6008], Avg: [-572.875 -572.875 -572.875] (1.000)
Step: 46249, Reward: [-501.033 -501.033 -501.033] [67.0367], Avg: [-572.87 -572.87 -572.87] (1.000)
Step: 46299, Reward: [-511.532 -511.532 -511.532] [92.4406], Avg: [-572.903 -572.903 -572.903] (1.000)
Step: 46349, Reward: [-485.225 -485.225 -485.225] [69.4774], Avg: [-572.883 -572.883 -572.883] (1.000)
Step: 46399, Reward: [-468.851 -468.851 -468.851] [46.5533], Avg: [-572.822 -572.822 -572.822] (1.000)
Step: 46449, Reward: [-516.038 -516.038 -516.038] [108.1375], Avg: [-572.877 -572.877 -572.877] (1.000)
Step: 46499, Reward: [-476.811 -476.811 -476.811] [32.0592], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 46549, Reward: [-511.859 -511.859 -511.859] [96.8967], Avg: [-572.847 -572.847 -572.847] (1.000)
Step: 46599, Reward: [-453.392 -453.392 -453.392] [67.3235], Avg: [-572.791 -572.791 -572.791] (1.000)
Step: 46649, Reward: [-533.974 -533.974 -533.974] [50.9327], Avg: [-572.804 -572.804 -572.804] (1.000)
Step: 46699, Reward: [-461.527 -461.527 -461.527] [77.3015], Avg: [-572.767 -572.767 -572.767] (1.000)
Step: 46749, Reward: [-463.067 -463.067 -463.067] [60.4620], Avg: [-572.715 -572.715 -572.715] (1.000)
Step: 46799, Reward: [-518.158 -518.158 -518.158] [92.6840], Avg: [-572.755 -572.755 -572.755] (1.000)
Step: 46849, Reward: [-447.82 -447.82 -447.82] [87.1167], Avg: [-572.715 -572.715 -572.715] (1.000)
Step: 46899, Reward: [-527.334 -527.334 -527.334] [95.7995], Avg: [-572.769 -572.769 -572.769] (1.000)
Step: 46949, Reward: [-508.682 -508.682 -508.682] [87.5398], Avg: [-572.794 -572.794 -572.794] (1.000)
Step: 46999, Reward: [-419.34 -419.34 -419.34] [74.5348], Avg: [-572.71 -572.71 -572.71] (1.000)
Step: 47049, Reward: [-503.477 -503.477 -503.477] [39.4090], Avg: [-572.678 -572.678 -572.678] (1.000)
Step: 47099, Reward: [-494.45 -494.45 -494.45] [33.7064], Avg: [-572.631 -572.631 -572.631] (1.000)
Step: 47149, Reward: [-432.397 -432.397 -432.397] [50.6769], Avg: [-572.536 -572.536 -572.536] (1.000)
Step: 47199, Reward: [-520.738 -520.738 -520.738] [173.7230], Avg: [-572.665 -572.665 -572.665] (1.000)
Step: 47249, Reward: [-457.688 -457.688 -457.688] [37.0771], Avg: [-572.583 -572.583 -572.583] (1.000)
Step: 47299, Reward: [-511.494 -511.494 -511.494] [63.5682], Avg: [-572.585 -572.585 -572.585] (1.000)
Step: 47349, Reward: [-535.267 -535.267 -535.267] [91.8308], Avg: [-572.643 -572.643 -572.643] (1.000)
Step: 47399, Reward: [-468.902 -468.902 -468.902] [37.7584], Avg: [-572.573 -572.573 -572.573] (1.000)
Step: 47449, Reward: [-465.579 -465.579 -465.579] [60.0425], Avg: [-572.524 -572.524 -572.524] (1.000)
Step: 47499, Reward: [-443.842 -443.842 -443.842] [49.6208], Avg: [-572.44 -572.44 -572.44] (1.000)
Step: 47549, Reward: [-463.661 -463.661 -463.661] [40.7636], Avg: [-572.369 -572.369 -572.369] (1.000)
Step: 47599, Reward: [-431.946 -431.946 -431.946] [37.5061], Avg: [-572.261 -572.261 -572.261] (1.000)
Step: 47649, Reward: [-490.119 -490.119 -490.119] [37.5728], Avg: [-572.214 -572.214 -572.214] (1.000)
Step: 47699, Reward: [-428.878 -428.878 -428.878] [56.3147], Avg: [-572.123 -572.123 -572.123] (1.000)
Step: 47749, Reward: [-537.151 -537.151 -537.151] [109.0916], Avg: [-572.2 -572.2 -572.2] (1.000)
Step: 47799, Reward: [-453.884 -453.884 -453.884] [88.8538], Avg: [-572.17 -572.17 -572.17] (1.000)
Step: 47849, Reward: [-527.493 -527.493 -527.493] [90.9482], Avg: [-572.218 -572.218 -572.218] (1.000)
Step: 47899, Reward: [-444.796 -444.796 -444.796] [58.2997], Avg: [-572.146 -572.146 -572.146] (1.000)
Step: 47949, Reward: [-562.505 -562.505 -562.505] [162.5395], Avg: [-572.305 -572.305 -572.305] (1.000)
Step: 47999, Reward: [-565.253 -565.253 -565.253] [126.1276], Avg: [-572.429 -572.429 -572.429] (1.000)
Step: 48049, Reward: [-511.784 -511.784 -511.784] [179.4132], Avg: [-572.553 -572.553 -572.553] (1.000)
Step: 48099, Reward: [-420.465 -420.465 -420.465] [37.4091], Avg: [-572.434 -572.434 -572.434] (1.000)
Step: 48149, Reward: [-443.452 -443.452 -443.452] [77.1435], Avg: [-572.38 -572.38 -572.38] (1.000)
Step: 48199, Reward: [-532.717 -532.717 -532.717] [135.6318], Avg: [-572.479 -572.479 -572.479] (1.000)
Step: 48249, Reward: [-497.53 -497.53 -497.53] [86.2546], Avg: [-572.491 -572.491 -572.491] (1.000)
Step: 48299, Reward: [-560.53 -560.53 -560.53] [131.2255], Avg: [-572.615 -572.615 -572.615] (1.000)
Step: 48349, Reward: [-435.248 -435.248 -435.248] [73.0479], Avg: [-572.548 -572.548 -572.548] (1.000)
Step: 48399, Reward: [-501.787 -501.787 -501.787] [77.2202], Avg: [-572.555 -572.555 -572.555] (1.000)
Step: 48449, Reward: [-519.521 -519.521 -519.521] [57.0724], Avg: [-572.559 -572.559 -572.559] (1.000)
Step: 48499, Reward: [-545.057 -545.057 -545.057] [140.3923], Avg: [-572.675 -572.675 -572.675] (1.000)
Step: 48549, Reward: [-520.446 -520.446 -520.446] [76.7531], Avg: [-572.701 -572.701 -572.701] (1.000)
Step: 48599, Reward: [-555.996 -555.996 -555.996] [87.7408], Avg: [-572.774 -572.774 -572.774] (1.000)
Step: 48649, Reward: [-548.041 -548.041 -548.041] [87.9278], Avg: [-572.839 -572.839 -572.839] (1.000)
Step: 48699, Reward: [-512.038 -512.038 -512.038] [57.2368], Avg: [-572.835 -572.835 -572.835] (1.000)
Step: 48749, Reward: [-513.207 -513.207 -513.207] [91.0574], Avg: [-572.867 -572.867 -572.867] (1.000)
Step: 48799, Reward: [-413.567 -413.567 -413.567] [42.7517], Avg: [-572.748 -572.748 -572.748] (1.000)
Step: 48849, Reward: [-574.187 -574.187 -574.187] [162.2111], Avg: [-572.915 -572.915 -572.915] (1.000)
Step: 48899, Reward: [-477.791 -477.791 -477.791] [56.3932], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 48949, Reward: [-474.115 -474.115 -474.115] [58.3352], Avg: [-572.834 -572.834 -572.834] (1.000)
Step: 48999, Reward: [-519.723 -519.723 -519.723] [94.2905], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 49049, Reward: [-503.481 -503.481 -503.481] [74.5055], Avg: [-572.882 -572.882 -572.882] (1.000)
Step: 49099, Reward: [-429.701 -429.701 -429.701] [112.8373], Avg: [-572.851 -572.851 -572.851] (1.000)
Step: 49149, Reward: [-487.458 -487.458 -487.458] [19.6309], Avg: [-572.784 -572.784 -572.784] (1.000)
Step: 49199, Reward: [-487.787 -487.787 -487.787] [52.4305], Avg: [-572.751 -572.751 -572.751] (1.000)
Step: 49249, Reward: [-458.319 -458.319 -458.319] [90.0550], Avg: [-572.726 -572.726 -572.726] (1.000)
Step: 49299, Reward: [-464.742 -464.742 -464.742] [37.5657], Avg: [-572.655 -572.655 -572.655] (1.000)
Step: 49349, Reward: [-482.667 -482.667 -482.667] [33.5667], Avg: [-572.597 -572.597 -572.597] (1.000)
Step: 49399, Reward: [-498.404 -498.404 -498.404] [153.2285], Avg: [-572.677 -572.677 -572.677] (1.000)
Step: 49449, Reward: [-470.172 -470.172 -470.172] [52.3559], Avg: [-572.627 -572.627 -572.627] (1.000)
Step: 49499, Reward: [-464.917 -464.917 -464.917] [37.5592], Avg: [-572.556 -572.556 -572.556] (1.000)
Step: 49549, Reward: [-524.3 -524.3 -524.3] [155.3651], Avg: [-572.664 -572.664 -572.664] (1.000)
Step: 49599, Reward: [-486.071 -486.071 -486.071] [36.0581], Avg: [-572.613 -572.613 -572.613] (1.000)
Step: 49649, Reward: [-469.918 -469.918 -469.918] [47.6159], Avg: [-572.557 -572.557 -572.557] (1.000)
Step: 49699, Reward: [-558.388 -558.388 -558.388] [155.7604], Avg: [-572.7 -572.7 -572.7] (1.000)
Step: 49749, Reward: [-430.003 -430.003 -430.003] [52.0812], Avg: [-572.609 -572.609 -572.609] (1.000)
Step: 49799, Reward: [-446.599 -446.599 -446.599] [43.8432], Avg: [-572.526 -572.526 -572.526] (1.000)
Step: 49849, Reward: [-462.387 -462.387 -462.387] [88.1370], Avg: [-572.504 -572.504 -572.504] (1.000)
Step: 49899, Reward: [-443.039 -443.039 -443.039] [42.6772], Avg: [-572.417 -572.417 -572.417] (1.000)
Step: 49949, Reward: [-427.638 -427.638 -427.638] [45.9172], Avg: [-572.318 -572.318 -572.318] (1.000)
Step: 49999, Reward: [-475.397 -475.397 -475.397] [48.5497], Avg: [-572.27 -572.27 -572.27] (1.000)
Step: 50049, Reward: [-642.351 -642.351 -642.351] [132.6889], Avg: [-572.473 -572.473 -572.473] (1.000)
Step: 50099, Reward: [-520.494 -520.494 -520.494] [70.9303], Avg: [-572.491 -572.491 -572.491] (1.000)
Step: 50149, Reward: [-464.834 -464.834 -464.834] [33.8053], Avg: [-572.418 -572.418 -572.418] (1.000)
Step: 50199, Reward: [-443.624 -443.624 -443.624] [46.2044], Avg: [-572.336 -572.336 -572.336] (1.000)
Step: 50249, Reward: [-546.131 -546.131 -546.131] [47.3291], Avg: [-572.357 -572.357 -572.357] (1.000)
Step: 50299, Reward: [-525.976 -525.976 -525.976] [61.4252], Avg: [-572.372 -572.372 -572.372] (1.000)
Step: 50349, Reward: [-550.224 -550.224 -550.224] [91.6784], Avg: [-572.441 -572.441 -572.441] (1.000)
Step: 50399, Reward: [-495.124 -495.124 -495.124] [82.4421], Avg: [-572.446 -572.446 -572.446] (1.000)
Step: 50449, Reward: [-511.854 -511.854 -511.854] [54.1401], Avg: [-572.439 -572.439 -572.439] (1.000)
Step: 50499, Reward: [-534.417 -534.417 -534.417] [109.1857], Avg: [-572.51 -572.51 -572.51] (1.000)
Step: 50549, Reward: [-513.669 -513.669 -513.669] [62.8768], Avg: [-572.514 -572.514 -572.514] (1.000)
Step: 50599, Reward: [-582.489 -582.489 -582.489] [112.3097], Avg: [-572.635 -572.635 -572.635] (1.000)
Step: 50649, Reward: [-511.898 -511.898 -511.898] [79.1175], Avg: [-572.653 -572.653 -572.653] (1.000)
Step: 50699, Reward: [-565.048 -565.048 -565.048] [104.6896], Avg: [-572.748 -572.748 -572.748] (1.000)
Step: 50749, Reward: [-512.681 -512.681 -512.681] [62.0914], Avg: [-572.75 -572.75 -572.75] (1.000)
Step: 50799, Reward: [-469.21 -469.21 -469.21] [90.3927], Avg: [-572.738 -572.738 -572.738] (1.000)
Step: 50849, Reward: [-462.486 -462.486 -462.486] [102.1254], Avg: [-572.73 -572.73 -572.73] (1.000)
Step: 50899, Reward: [-453.873 -453.873 -453.873] [23.0317], Avg: [-572.635 -572.635 -572.635] (1.000)
Step: 50949, Reward: [-473.529 -473.529 -473.529] [94.0068], Avg: [-572.63 -572.63 -572.63] (1.000)
Step: 50999, Reward: [-629.026 -629.026 -629.026] [99.8141], Avg: [-572.784 -572.784 -572.784] (1.000)
Step: 51049, Reward: [-527.119 -527.119 -527.119] [62.6452], Avg: [-572.8 -572.8 -572.8] (1.000)
Step: 51099, Reward: [-499.106 -499.106 -499.106] [44.7590], Avg: [-572.772 -572.772 -572.772] (1.000)
Step: 51149, Reward: [-496.343 -496.343 -496.343] [115.6104], Avg: [-572.81 -572.81 -572.81] (1.000)
Step: 51199, Reward: [-439.705 -439.705 -439.705] [68.7592], Avg: [-572.747 -572.747 -572.747] (1.000)
Step: 51249, Reward: [-465.093 -465.093 -465.093] [63.2084], Avg: [-572.704 -572.704 -572.704] (1.000)
Step: 51299, Reward: [-497.481 -497.481 -497.481] [44.9585], Avg: [-572.674 -572.674 -572.674] (1.000)
Step: 51349, Reward: [-513.047 -513.047 -513.047] [48.1672], Avg: [-572.663 -572.663 -572.663] (1.000)
Step: 51399, Reward: [-457.816 -457.816 -457.816] [47.8295], Avg: [-572.598 -572.598 -572.598] (1.000)
Step: 51449, Reward: [-481.061 -481.061 -481.061] [33.4351], Avg: [-572.542 -572.542 -572.542] (1.000)
Step: 51499, Reward: [-493.899 -493.899 -493.899] [118.0319], Avg: [-572.58 -572.58 -572.58] (1.000)
Step: 51549, Reward: [-459.014 -459.014 -459.014] [57.1316], Avg: [-572.525 -572.525 -572.525] (1.000)
Step: 51599, Reward: [-522.006 -522.006 -522.006] [83.2180], Avg: [-572.557 -572.557 -572.557] (1.000)
Step: 51649, Reward: [-482.041 -482.041 -482.041] [55.9906], Avg: [-572.523 -572.523 -572.523] (1.000)
Step: 51699, Reward: [-479.479 -479.479 -479.479] [27.3336], Avg: [-572.46 -572.46 -572.46] (1.000)
Step: 51749, Reward: [-420.332 -420.332 -420.332] [42.6663], Avg: [-572.354 -572.354 -572.354] (1.000)
Step: 51799, Reward: [-572.058 -572.058 -572.058] [191.7615], Avg: [-572.539 -572.539 -572.539] (1.000)
Step: 51849, Reward: [-536.205 -536.205 -536.205] [122.0331], Avg: [-572.622 -572.622 -572.622] (1.000)
Step: 51899, Reward: [-484.894 -484.894 -484.894] [47.5537], Avg: [-572.583 -572.583 -572.583] (1.000)
Step: 51949, Reward: [-483.095 -483.095 -483.095] [63.9252], Avg: [-572.558 -572.558 -572.558] (1.000)
Step: 51999, Reward: [-494.311 -494.311 -494.311] [66.4046], Avg: [-572.547 -572.547 -572.547] (1.000)
Step: 52049, Reward: [-473.154 -473.154 -473.154] [63.1669], Avg: [-572.512 -572.512 -572.512] (1.000)
Step: 52099, Reward: [-483.065 -483.065 -483.065] [65.6229], Avg: [-572.489 -572.489 -572.489] (1.000)
Step: 52149, Reward: [-456.575 -456.575 -456.575] [69.9506], Avg: [-572.445 -572.445 -572.445] (1.000)
Step: 52199, Reward: [-568.226 -568.226 -568.226] [111.9285], Avg: [-572.548 -572.548 -572.548] (1.000)
Step: 52249, Reward: [-475.574 -475.574 -475.574] [80.8787], Avg: [-572.533 -572.533 -572.533] (1.000)
Step: 52299, Reward: [-462.436 -462.436 -462.436] [80.0576], Avg: [-572.504 -572.504 -572.504] (1.000)
Step: 52349, Reward: [-526.464 -526.464 -526.464] [65.2069], Avg: [-572.522 -572.522 -572.522] (1.000)
Step: 52399, Reward: [-485.38 -485.38 -485.38] [77.7838], Avg: [-572.514 -572.514 -572.514] (1.000)
Step: 52449, Reward: [-525.491 -525.491 -525.491] [123.1109], Avg: [-572.586 -572.586 -572.586] (1.000)
Step: 52499, Reward: [-473.904 -473.904 -473.904] [30.4796], Avg: [-572.521 -572.521 -572.521] (1.000)
Step: 52549, Reward: [-500.314 -500.314 -500.314] [113.9859], Avg: [-572.561 -572.561 -572.561] (1.000)
Step: 52599, Reward: [-542.745 -542.745 -542.745] [39.5990], Avg: [-572.57 -572.57 -572.57] (1.000)
Step: 52649, Reward: [-468.796 -468.796 -468.796] [96.8828], Avg: [-572.564 -572.564 -572.564] (1.000)
Step: 52699, Reward: [-540.575 -540.575 -540.575] [96.7201], Avg: [-572.625 -572.625 -572.625] (1.000)
Step: 52749, Reward: [-453.271 -453.271 -453.271] [74.8196], Avg: [-572.583 -572.583 -572.583] (1.000)
Step: 52799, Reward: [-495.658 -495.658 -495.658] [114.9654], Avg: [-572.619 -572.619 -572.619] (1.000)
Step: 52849, Reward: [-539.901 -539.901 -539.901] [121.8913], Avg: [-572.703 -572.703 -572.703] (1.000)
Step: 52899, Reward: [-461.046 -461.046 -461.046] [58.3441], Avg: [-572.653 -572.653 -572.653] (1.000)
Step: 52949, Reward: [-579.537 -579.537 -579.537] [108.7552], Avg: [-572.762 -572.762 -572.762] (1.000)
Step: 52999, Reward: [-481.457 -481.457 -481.457] [76.2306], Avg: [-572.748 -572.748 -572.748] (1.000)
Step: 53049, Reward: [-531.583 -531.583 -531.583] [48.7751], Avg: [-572.755 -572.755 -572.755] (1.000)
Step: 53099, Reward: [-470.46 -470.46 -470.46] [56.7253], Avg: [-572.712 -572.712 -572.712] (1.000)
Step: 53149, Reward: [-546.995 -546.995 -546.995] [94.0739], Avg: [-572.776 -572.776 -572.776] (1.000)
Step: 53199, Reward: [-499.681 -499.681 -499.681] [87.4158], Avg: [-572.79 -572.79 -572.79] (1.000)
Step: 53249, Reward: [-514.372 -514.372 -514.372] [150.4792], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 53299, Reward: [-576.973 -576.973 -576.973] [134.3003], Avg: [-573.006 -573.006 -573.006] (1.000)
Step: 53349, Reward: [-439.847 -439.847 -439.847] [44.1988], Avg: [-572.923 -572.923 -572.923] (1.000)
Step: 53399, Reward: [-517.969 -517.969 -517.969] [68.0772], Avg: [-572.935 -572.935 -572.935] (1.000)
Step: 53449, Reward: [-526.12 -526.12 -526.12] [65.1068], Avg: [-572.952 -572.952 -572.952] (1.000)
Step: 53499, Reward: [-464.606 -464.606 -464.606] [62.1051], Avg: [-572.909 -572.909 -572.909] (1.000)
Step: 53549, Reward: [-552.546 -552.546 -552.546] [57.3310], Avg: [-572.943 -572.943 -572.943] (1.000)
Step: 53599, Reward: [-520.336 -520.336 -520.336] [96.8501], Avg: [-572.985 -572.985 -572.985] (1.000)
Step: 53649, Reward: [-533.128 -533.128 -533.128] [60.5838], Avg: [-573.004 -573.004 -573.004] (1.000)
Step: 53699, Reward: [-460.196 -460.196 -460.196] [82.9892], Avg: [-572.976 -572.976 -572.976] (1.000)
Step: 53749, Reward: [-448.988 -448.988 -448.988] [80.0118], Avg: [-572.935 -572.935 -572.935] (1.000)
Step: 53799, Reward: [-513.053 -513.053 -513.053] [117.0142], Avg: [-572.988 -572.988 -572.988] (1.000)
Step: 53849, Reward: [-570.926 -570.926 -570.926] [80.3726], Avg: [-573.061 -573.061 -573.061] (1.000)
Step: 53899, Reward: [-469.381 -469.381 -469.381] [86.9174], Avg: [-573.046 -573.046 -573.046] (1.000)
Step: 53949, Reward: [-497.352 -497.352 -497.352] [72.8705], Avg: [-573.043 -573.043 -573.043] (1.000)
Step: 53999, Reward: [-431.649 -431.649 -431.649] [47.5815], Avg: [-572.956 -572.956 -572.956] (1.000)
Step: 54049, Reward: [-557.453 -557.453 -557.453] [68.5994], Avg: [-573.005 -573.005 -573.005] (1.000)
Step: 54099, Reward: [-518.331 -518.331 -518.331] [69.0589], Avg: [-573.019 -573.019 -573.019] (1.000)
Step: 54149, Reward: [-455.524 -455.524 -455.524] [61.3905], Avg: [-572.967 -572.967 -572.967] (1.000)
Step: 54199, Reward: [-439.145 -439.145 -439.145] [95.7318], Avg: [-572.932 -572.932 -572.932] (1.000)
Step: 54249, Reward: [-514.612 -514.612 -514.612] [50.3404], Avg: [-572.924 -572.924 -572.924] (1.000)
Step: 54299, Reward: [-546.094 -546.094 -546.094] [103.5132], Avg: [-572.995 -572.995 -572.995] (1.000)
Step: 54349, Reward: [-481.156 -481.156 -481.156] [43.4412], Avg: [-572.95 -572.95 -572.95] (1.000)
Step: 54399, Reward: [-522.986 -522.986 -522.986] [132.5851], Avg: [-573.026 -573.026 -573.026] (1.000)
Step: 54449, Reward: [-506.062 -506.062 -506.062] [54.6841], Avg: [-573.015 -573.015 -573.015] (1.000)
Step: 54499, Reward: [-450.414 -450.414 -450.414] [52.2323], Avg: [-572.95 -572.95 -572.95] (1.000)
Step: 54549, Reward: [-482.91 -482.91 -482.91] [69.3287], Avg: [-572.931 -572.931 -572.931] (1.000)
Step: 54599, Reward: [-527.703 -527.703 -527.703] [102.0239], Avg: [-572.983 -572.983 -572.983] (1.000)
Step: 54649, Reward: [-470.254 -470.254 -470.254] [75.7278], Avg: [-572.959 -572.959 -572.959] (1.000)
Step: 54699, Reward: [-555.634 -555.634 -555.634] [160.1024], Avg: [-573.089 -573.089 -573.089] (1.000)
Step: 54749, Reward: [-492.927 -492.927 -492.927] [44.9309], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 54799, Reward: [-524.021 -524.021 -524.021] [120.6971], Avg: [-573.122 -573.122 -573.122] (1.000)
Step: 54849, Reward: [-511.677 -511.677 -511.677] [90.1653], Avg: [-573.149 -573.149 -573.149] (1.000)
Step: 54899, Reward: [-462.887 -462.887 -462.887] [103.9778], Avg: [-573.143 -573.143 -573.143] (1.000)
Step: 54949, Reward: [-578.516 -578.516 -578.516] [58.1494], Avg: [-573.201 -573.201 -573.201] (1.000)
Step: 54999, Reward: [-612.702 -612.702 -612.702] [129.9126], Avg: [-573.355 -573.355 -573.355] (1.000)
Step: 55049, Reward: [-472.477 -472.477 -472.477] [50.3428], Avg: [-573.309 -573.309 -573.309] (1.000)
Step: 55099, Reward: [-526.408 -526.408 -526.408] [95.2138], Avg: [-573.353 -573.353 -573.353] (1.000)
Step: 55149, Reward: [-492.894 -492.894 -492.894] [85.1045], Avg: [-573.357 -573.357 -573.357] (1.000)
Step: 55199, Reward: [-414.779 -414.779 -414.779] [63.5587], Avg: [-573.271 -573.271 -573.271] (1.000)
Step: 55249, Reward: [-516.571 -516.571 -516.571] [116.7833], Avg: [-573.325 -573.325 -573.325] (1.000)
Step: 55299, Reward: [-454.726 -454.726 -454.726] [68.3587], Avg: [-573.28 -573.28 -573.28] (1.000)
Step: 55349, Reward: [-487.904 -487.904 -487.904] [54.5534], Avg: [-573.252 -573.252 -573.252] (1.000)
Step: 55399, Reward: [-510.153 -510.153 -510.153] [86.3273], Avg: [-573.273 -573.273 -573.273] (1.000)
Step: 55449, Reward: [-464.913 -464.913 -464.913] [84.9377], Avg: [-573.252 -573.252 -573.252] (1.000)
Step: 55499, Reward: [-454.545 -454.545 -454.545] [83.0789], Avg: [-573.22 -573.22 -573.22] (1.000)
Step: 55549, Reward: [-469.233 -469.233 -469.233] [92.8927], Avg: [-573.21 -573.21 -573.21] (1.000)
Step: 55599, Reward: [-488.912 -488.912 -488.912] [82.1203], Avg: [-573.208 -573.208 -573.208] (1.000)
Step: 55649, Reward: [-527.265 -527.265 -527.265] [95.4122], Avg: [-573.252 -573.252 -573.252] (1.000)
Step: 55699, Reward: [-451.387 -451.387 -451.387] [29.0535], Avg: [-573.169 -573.169 -573.169] (1.000)
Step: 55749, Reward: [-511.843 -511.843 -511.843] [65.5189], Avg: [-573.173 -573.173 -573.173] (1.000)
Step: 55799, Reward: [-455.974 -455.974 -455.974] [107.1781], Avg: [-573.164 -573.164 -573.164] (1.000)
Step: 55849, Reward: [-479.608 -479.608 -479.608] [25.1485], Avg: [-573.102 -573.102 -573.102] (1.000)
Step: 55899, Reward: [-436.303 -436.303 -436.303] [59.4346], Avg: [-573.033 -573.033 -573.033] (1.000)
Step: 55949, Reward: [-513.103 -513.103 -513.103] [48.2014], Avg: [-573.023 -573.023 -573.023] (1.000)
Step: 55999, Reward: [-501.123 -501.123 -501.123] [64.3323], Avg: [-573.016 -573.016 -573.016] (1.000)
Step: 56049, Reward: [-473.184 -473.184 -473.184] [54.8745], Avg: [-572.976 -572.976 -572.976] (1.000)
Step: 56099, Reward: [-433.95 -433.95 -433.95] [98.6358], Avg: [-572.94 -572.94 -572.94] (1.000)
Step: 56149, Reward: [-484.833 -484.833 -484.833] [108.2453], Avg: [-572.958 -572.958 -572.958] (1.000)
Step: 56199, Reward: [-453.389 -453.389 -453.389] [60.2426], Avg: [-572.905 -572.905 -572.905] (1.000)
Step: 56249, Reward: [-488.822 -488.822 -488.822] [55.4018], Avg: [-572.88 -572.88 -572.88] (1.000)
Step: 56299, Reward: [-515.153 -515.153 -515.153] [47.0386], Avg: [-572.87 -572.87 -572.87] (1.000)
Step: 56349, Reward: [-456.527 -456.527 -456.527] [114.6224], Avg: [-572.869 -572.869 -572.869] (1.000)
Step: 56399, Reward: [-462.907 -462.907 -462.907] [55.0259], Avg: [-572.82 -572.82 -572.82] (1.000)
Step: 56449, Reward: [-525.704 -525.704 -525.704] [74.6248], Avg: [-572.844 -572.844 -572.844] (1.000)
Step: 56499, Reward: [-461.907 -461.907 -461.907] [17.0524], Avg: [-572.761 -572.761 -572.761] (1.000)
Step: 56549, Reward: [-454.752 -454.752 -454.752] [89.4791], Avg: [-572.736 -572.736 -572.736] (1.000)
Step: 56599, Reward: [-471.069 -471.069 -471.069] [74.7779], Avg: [-572.712 -572.712 -572.712] (1.000)
Step: 56649, Reward: [-512.659 -512.659 -512.659] [41.0291], Avg: [-572.695 -572.695 -572.695] (1.000)
Step: 56699, Reward: [-482.599 -482.599 -482.599] [15.9989], Avg: [-572.63 -572.63 -572.63] (1.000)
Step: 56749, Reward: [-462.958 -462.958 -462.958] [108.8317], Avg: [-572.629 -572.629 -572.629] (1.000)
Step: 56799, Reward: [-498.503 -498.503 -498.503] [74.1351], Avg: [-572.629 -572.629 -572.629] (1.000)
Step: 56849, Reward: [-451.628 -451.628 -451.628] [71.6263], Avg: [-572.586 -572.586 -572.586] (1.000)
Step: 56899, Reward: [-451.502 -451.502 -451.502] [68.8228], Avg: [-572.54 -572.54 -572.54] (1.000)
Step: 56949, Reward: [-452.868 -452.868 -452.868] [111.9356], Avg: [-572.533 -572.533 -572.533] (1.000)
Step: 56999, Reward: [-510.9 -510.9 -510.9] [90.8240], Avg: [-572.559 -572.559 -572.559] (1.000)
Step: 57049, Reward: [-466.103 -466.103 -466.103] [95.4872], Avg: [-572.549 -572.549 -572.549] (1.000)
Step: 57099, Reward: [-465.873 -465.873 -465.873] [48.1182], Avg: [-572.498 -572.498 -572.498] (1.000)
Step: 57149, Reward: [-474.49 -474.49 -474.49] [76.8110], Avg: [-572.479 -572.479 -572.479] (1.000)
Step: 57199, Reward: [-479.762 -479.762 -479.762] [96.4230], Avg: [-572.483 -572.483 -572.483] (1.000)
Step: 57249, Reward: [-476.506 -476.506 -476.506] [72.8557], Avg: [-572.462 -572.462 -572.462] (1.000)
Step: 57299, Reward: [-566.796 -566.796 -566.796] [56.4517], Avg: [-572.507 -572.507 -572.507] (1.000)
Step: 57349, Reward: [-491.791 -491.791 -491.791] [64.4882], Avg: [-572.492 -572.492 -572.492] (1.000)
Step: 57399, Reward: [-575.737 -575.737 -575.737] [67.9443], Avg: [-572.554 -572.554 -572.554] (1.000)
Step: 57449, Reward: [-567.735 -567.735 -567.735] [116.0028], Avg: [-572.651 -572.651 -572.651] (1.000)
Step: 57499, Reward: [-475.067 -475.067 -475.067] [106.5609], Avg: [-572.659 -572.659 -572.659] (1.000)
Step: 57549, Reward: [-604.516 -604.516 -604.516] [91.5096], Avg: [-572.766 -572.766 -572.766] (1.000)
Step: 57599, Reward: [-481.69 -481.69 -481.69] [57.8577], Avg: [-572.737 -572.737 -572.737] (1.000)
Step: 57649, Reward: [-454.871 -454.871 -454.871] [96.9715], Avg: [-572.719 -572.719 -572.719] (1.000)
Step: 57699, Reward: [-438.692 -438.692 -438.692] [66.2255], Avg: [-572.661 -572.661 -572.661] (1.000)
Step: 57749, Reward: [-454.613 -454.613 -454.613] [39.9316], Avg: [-572.593 -572.593 -572.593] (1.000)
Step: 57799, Reward: [-502.201 -502.201 -502.201] [58.6102], Avg: [-572.583 -572.583 -572.583] (1.000)
Step: 57849, Reward: [-524.17 -524.17 -524.17] [36.6781], Avg: [-572.573 -572.573 -572.573] (1.000)
Step: 57899, Reward: [-455.359 -455.359 -455.359] [57.7597], Avg: [-572.521 -572.521 -572.521] (1.000)
Step: 57949, Reward: [-433.248 -433.248 -433.248] [65.4849], Avg: [-572.458 -572.458 -572.458] (1.000)
Step: 57999, Reward: [-500.572 -500.572 -500.572] [74.0516], Avg: [-572.459 -572.459 -572.459] (1.000)
Step: 58049, Reward: [-490.518 -490.518 -490.518] [43.6238], Avg: [-572.426 -572.426 -572.426] (1.000)
Step: 58099, Reward: [-564.479 -564.479 -564.479] [141.6638], Avg: [-572.541 -572.541 -572.541] (1.000)
Step: 58149, Reward: [-455.006 -455.006 -455.006] [73.8352], Avg: [-572.504 -572.504 -572.504] (1.000)
Step: 58199, Reward: [-485.675 -485.675 -485.675] [96.8167], Avg: [-572.513 -572.513 -572.513] (1.000)
Step: 58249, Reward: [-585.118 -585.118 -585.118] [131.3218], Avg: [-572.636 -572.636 -572.636] (1.000)
Step: 58299, Reward: [-451.182 -451.182 -451.182] [90.4357], Avg: [-572.609 -572.609 -572.609] (1.000)
Step: 58349, Reward: [-478.55 -478.55 -478.55] [67.3177], Avg: [-572.587 -572.587 -572.587] (1.000)
Step: 58399, Reward: [-472.803 -472.803 -472.803] [26.5332], Avg: [-572.524 -572.524 -572.524] (1.000)
Step: 58449, Reward: [-412.199 -412.199 -412.199] [26.4499], Avg: [-572.409 -572.409 -572.409] (1.000)
Step: 58499, Reward: [-465.134 -465.134 -465.134] [53.6540], Avg: [-572.363 -572.363 -572.363] (1.000)
Step: 58549, Reward: [-455.964 -455.964 -455.964] [50.2065], Avg: [-572.307 -572.307 -572.307] (1.000)
Step: 58599, Reward: [-575.749 -575.749 -575.749] [105.3656], Avg: [-572.4 -572.4 -572.4] (1.000)
Step: 58649, Reward: [-426.174 -426.174 -426.174] [29.1664], Avg: [-572.3 -572.3 -572.3] (1.000)
Step: 58699, Reward: [-506.713 -506.713 -506.713] [66.5524], Avg: [-572.301 -572.301 -572.301] (1.000)
Step: 58749, Reward: [-483.309 -483.309 -483.309] [66.4898], Avg: [-572.282 -572.282 -572.282] (1.000)
Step: 58799, Reward: [-537.676 -537.676 -537.676] [80.1009], Avg: [-572.32 -572.32 -572.32] (1.000)
Step: 58849, Reward: [-487.842 -487.842 -487.842] [51.6940], Avg: [-572.292 -572.292 -572.292] (1.000)
Step: 58899, Reward: [-566.32 -566.32 -566.32] [77.5150], Avg: [-572.353 -572.353 -572.353] (1.000)
Step: 58949, Reward: [-471.42 -471.42 -471.42] [94.0017], Avg: [-572.347 -572.347 -572.347] (1.000)
Step: 58999, Reward: [-619.51 -619.51 -619.51] [63.2884], Avg: [-572.441 -572.441 -572.441] (1.000)
Step: 59049, Reward: [-480.053 -480.053 -480.053] [57.3822], Avg: [-572.411 -572.411 -572.411] (1.000)
Step: 59099, Reward: [-436.598 -436.598 -436.598] [77.5259], Avg: [-572.362 -572.362 -572.362] (1.000)
Step: 59149, Reward: [-419.916 -419.916 -419.916] [59.2800], Avg: [-572.283 -572.283 -572.283] (1.000)
Step: 59199, Reward: [-445.481 -445.481 -445.481] [56.1465], Avg: [-572.224 -572.224 -572.224] (1.000)
Step: 59249, Reward: [-483.007 -483.007 -483.007] [66.4208], Avg: [-572.204 -572.204 -572.204] (1.000)
Step: 59299, Reward: [-472.07 -472.07 -472.07] [70.5985], Avg: [-572.179 -572.179 -572.179] (1.000)
Step: 59349, Reward: [-466.975 -466.975 -466.975] [59.0053], Avg: [-572.14 -572.14 -572.14] (1.000)
Step: 59399, Reward: [-477.307 -477.307 -477.307] [108.6489], Avg: [-572.152 -572.152 -572.152] (1.000)
Step: 59449, Reward: [-539.467 -539.467 -539.467] [48.4808], Avg: [-572.165 -572.165 -572.165] (1.000)
Step: 59499, Reward: [-574.844 -574.844 -574.844] [113.5827], Avg: [-572.263 -572.263 -572.263] (1.000)
Step: 59549, Reward: [-446.312 -446.312 -446.312] [91.3647], Avg: [-572.234 -572.234 -572.234] (1.000)
Step: 59599, Reward: [-564.69 -564.69 -564.69] [149.9532], Avg: [-572.354 -572.354 -572.354] (1.000)
Step: 59649, Reward: [-461.786 -461.786 -461.786] [46.9066], Avg: [-572.3 -572.3 -572.3] (1.000)
Step: 59699, Reward: [-495.106 -495.106 -495.106] [25.0035], Avg: [-572.256 -572.256 -572.256] (1.000)
Step: 59749, Reward: [-518.824 -518.824 -518.824] [72.2072], Avg: [-572.272 -572.272 -572.272] (1.000)
Step: 59799, Reward: [-536.596 -536.596 -536.596] [52.8792], Avg: [-572.287 -572.287 -572.287] (1.000)
Step: 59849, Reward: [-483.673 -483.673 -483.673] [131.6858], Avg: [-572.323 -572.323 -572.323] (1.000)
Step: 59899, Reward: [-501.206 -501.206 -501.206] [40.6167], Avg: [-572.297 -572.297 -572.297] (1.000)
Step: 59949, Reward: [-488.679 -488.679 -488.679] [76.0102], Avg: [-572.291 -572.291 -572.291] (1.000)
Step: 59999, Reward: [-575.905 -575.905 -575.905] [90.4583], Avg: [-572.369 -572.369 -572.369] (1.000)
Step: 60049, Reward: [-441.315 -441.315 -441.315] [84.6123], Avg: [-572.33 -572.33 -572.33] (1.000)
Step: 60099, Reward: [-511.48 -511.48 -511.48] [139.6014], Avg: [-572.396 -572.396 -572.396] (1.000)
Step: 60149, Reward: [-407.303 -407.303 -407.303] [62.1046], Avg: [-572.31 -572.31 -572.31] (1.000)
Step: 60199, Reward: [-437.994 -437.994 -437.994] [65.7189], Avg: [-572.253 -572.253 -572.253] (1.000)
Step: 60249, Reward: [-611.851 -611.851 -611.851] [104.9412], Avg: [-572.373 -572.373 -572.373] (1.000)
Step: 60299, Reward: [-477.473 -477.473 -477.473] [24.1938], Avg: [-572.315 -572.315 -572.315] (1.000)
Step: 60349, Reward: [-519.885 -519.885 -519.885] [48.2322], Avg: [-572.311 -572.311 -572.311] (1.000)
Step: 60399, Reward: [-521.729 -521.729 -521.729] [146.2897], Avg: [-572.39 -572.39 -572.39] (1.000)
Step: 60449, Reward: [-429.754 -429.754 -429.754] [36.1945], Avg: [-572.302 -572.302 -572.302] (1.000)
Step: 60499, Reward: [-567.555 -567.555 -567.555] [83.9861], Avg: [-572.368 -572.368 -572.368] (1.000)
Step: 60549, Reward: [-524.039 -524.039 -524.039] [33.4379], Avg: [-572.356 -572.356 -572.356] (1.000)
Step: 60599, Reward: [-474.071 -474.071 -474.071] [65.6426], Avg: [-572.329 -572.329 -572.329] (1.000)
Step: 60649, Reward: [-501.315 -501.315 -501.315] [124.6616], Avg: [-572.373 -572.373 -572.373] (1.000)
Step: 60699, Reward: [-530.79 -530.79 -530.79] [102.8763], Avg: [-572.423 -572.423 -572.423] (1.000)
Step: 60749, Reward: [-522.727 -522.727 -522.727] [158.4626], Avg: [-572.513 -572.513 -572.513] (1.000)
Step: 60799, Reward: [-486.9 -486.9 -486.9] [63.9051], Avg: [-572.495 -572.495 -572.495] (1.000)
Step: 60849, Reward: [-476.166 -476.166 -476.166] [69.9976], Avg: [-572.473 -572.473 -572.473] (1.000)
Step: 60899, Reward: [-620.578 -620.578 -620.578] [110.7162], Avg: [-572.604 -572.604 -572.604] (1.000)
Step: 60949, Reward: [-513.308 -513.308 -513.308] [130.3827], Avg: [-572.662 -572.662 -572.662] (1.000)
Step: 60999, Reward: [-533.858 -533.858 -533.858] [73.7031], Avg: [-572.691 -572.691 -572.691] (1.000)
Step: 61049, Reward: [-460.67 -460.67 -460.67] [56.8911], Avg: [-572.646 -572.646 -572.646] (1.000)
Step: 61099, Reward: [-488.914 -488.914 -488.914] [83.2390], Avg: [-572.645 -572.645 -572.645] (1.000)
Step: 61149, Reward: [-484.148 -484.148 -484.148] [69.2886], Avg: [-572.629 -572.629 -572.629] (1.000)
Step: 61199, Reward: [-492.399 -492.399 -492.399] [59.8050], Avg: [-572.613 -572.613 -572.613] (1.000)
Step: 61249, Reward: [-553.625 -553.625 -553.625] [181.0728], Avg: [-572.745 -572.745 -572.745] (1.000)
Step: 61299, Reward: [-476.762 -476.762 -476.762] [83.1804], Avg: [-572.735 -572.735 -572.735] (1.000)
Step: 61349, Reward: [-492.054 -492.054 -492.054] [76.3634], Avg: [-572.731 -572.731 -572.731] (1.000)
Step: 61399, Reward: [-461.184 -461.184 -461.184] [41.6333], Avg: [-572.674 -572.674 -572.674] (1.000)
Step: 61449, Reward: [-534.124 -534.124 -534.124] [126.6952], Avg: [-572.746 -572.746 -572.746] (1.000)
Step: 61499, Reward: [-513.942 -513.942 -513.942] [67.1274], Avg: [-572.753 -572.753 -572.753] (1.000)
Step: 61549, Reward: [-508.21 -508.21 -508.21] [84.4490], Avg: [-572.769 -572.769 -572.769] (1.000)
Step: 61599, Reward: [-494.368 -494.368 -494.368] [34.9685], Avg: [-572.734 -572.734 -572.734] (1.000)
Step: 61649, Reward: [-477.45 -477.45 -477.45] [68.4622], Avg: [-572.712 -572.712 -572.712] (1.000)
Step: 61699, Reward: [-540.875 -540.875 -540.875] [176.5521], Avg: [-572.829 -572.829 -572.829] (1.000)
Step: 61749, Reward: [-420.176 -420.176 -420.176] [29.2587], Avg: [-572.729 -572.729 -572.729] (1.000)
Step: 61799, Reward: [-446.195 -446.195 -446.195] [89.6198], Avg: [-572.699 -572.699 -572.699] (1.000)
Step: 61849, Reward: [-448.499 -448.499 -448.499] [47.8239], Avg: [-572.638 -572.638 -572.638] (1.000)
Step: 61899, Reward: [-520.002 -520.002 -520.002] [137.2765], Avg: [-572.706 -572.706 -572.706] (1.000)
Step: 61949, Reward: [-569.39 -569.39 -569.39] [115.2225], Avg: [-572.796 -572.796 -572.796] (1.000)
Step: 61999, Reward: [-484.342 -484.342 -484.342] [108.8490], Avg: [-572.813 -572.813 -572.813] (1.000)
Step: 62049, Reward: [-466.247 -466.247 -466.247] [58.1157], Avg: [-572.774 -572.774 -572.774] (1.000)
Step: 62099, Reward: [-419.028 -419.028 -419.028] [62.3717], Avg: [-572.7 -572.7 -572.7] (1.000)
Step: 62149, Reward: [-565.499 -565.499 -565.499] [55.4580], Avg: [-572.739 -572.739 -572.739] (1.000)
Step: 62199, Reward: [-553.652 -553.652 -553.652] [79.5417], Avg: [-572.788 -572.788 -572.788] (1.000)
Step: 62249, Reward: [-437.383 -437.383 -437.383] [82.4105], Avg: [-572.745 -572.745 -572.745] (1.000)
Step: 62299, Reward: [-474.323 -474.323 -474.323] [81.6244], Avg: [-572.732 -572.732 -572.732] (1.000)
Step: 62349, Reward: [-560.51 -560.51 -560.51] [115.2444], Avg: [-572.814 -572.814 -572.814] (1.000)
Step: 62399, Reward: [-489.453 -489.453 -489.453] [31.4249], Avg: [-572.773 -572.773 -572.773] (1.000)
Step: 62449, Reward: [-453.082 -453.082 -453.082] [44.9466], Avg: [-572.713 -572.713 -572.713] (1.000)
Step: 62499, Reward: [-545.098 -545.098 -545.098] [101.6416], Avg: [-572.772 -572.772 -572.772] (1.000)
Step: 62549, Reward: [-509.831 -509.831 -509.831] [107.3409], Avg: [-572.807 -572.807 -572.807] (1.000)
Step: 62599, Reward: [-477.134 -477.134 -477.134] [108.5579], Avg: [-572.818 -572.818 -572.818] (1.000)
Step: 62649, Reward: [-554.258 -554.258 -554.258] [147.8468], Avg: [-572.921 -572.921 -572.921] (1.000)
Step: 62699, Reward: [-451.786 -451.786 -451.786] [64.4803], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 62749, Reward: [-591.486 -591.486 -591.486] [99.5626], Avg: [-572.97 -572.97 -572.97] (1.000)
Step: 62799, Reward: [-490.963 -490.963 -490.963] [120.8900], Avg: [-573.001 -573.001 -573.001] (1.000)
Step: 62849, Reward: [-534.664 -534.664 -534.664] [81.6581], Avg: [-573.035 -573.035 -573.035] (1.000)
Step: 62899, Reward: [-506.823 -506.823 -506.823] [106.9296], Avg: [-573.068 -573.068 -573.068] (1.000)
Step: 62949, Reward: [-476.308 -476.308 -476.308] [64.5616], Avg: [-573.042 -573.042 -573.042] (1.000)
Step: 62999, Reward: [-493.71 -493.71 -493.71] [121.9893], Avg: [-573.076 -573.076 -573.076] (1.000)
Step: 63049, Reward: [-548.983 -548.983 -548.983] [61.2224], Avg: [-573.105 -573.105 -573.105] (1.000)
Step: 63099, Reward: [-565.756 -565.756 -565.756] [116.0552], Avg: [-573.191 -573.191 -573.191] (1.000)
Step: 63149, Reward: [-528.638 -528.638 -528.638] [48.7293], Avg: [-573.195 -573.195 -573.195] (1.000)
Step: 63199, Reward: [-493.737 -493.737 -493.737] [76.1907], Avg: [-573.192 -573.192 -573.192] (1.000)
Step: 63249, Reward: [-510.606 -510.606 -510.606] [95.0011], Avg: [-573.218 -573.218 -573.218] (1.000)
Step: 63299, Reward: [-514.625 -514.625 -514.625] [79.5639], Avg: [-573.234 -573.234 -573.234] (1.000)
Step: 63349, Reward: [-416.073 -416.073 -416.073] [26.3461], Avg: [-573.131 -573.131 -573.131] (1.000)
Step: 63399, Reward: [-471.873 -471.873 -471.873] [55.8440], Avg: [-573.095 -573.095 -573.095] (1.000)
Step: 63449, Reward: [-442.709 -442.709 -442.709] [38.2485], Avg: [-573.023 -573.023 -573.023] (1.000)
Step: 63499, Reward: [-524.679 -524.679 -524.679] [65.5908], Avg: [-573.036 -573.036 -573.036] (1.000)
Step: 63549, Reward: [-501.259 -501.259 -501.259] [112.4347], Avg: [-573.068 -573.068 -573.068] (1.000)
Step: 63599, Reward: [-507.802 -507.802 -507.802] [121.9858], Avg: [-573.113 -573.113 -573.113] (1.000)
Step: 63649, Reward: [-451.193 -451.193 -451.193] [38.9298], Avg: [-573.048 -573.048 -573.048] (1.000)
Step: 63699, Reward: [-537.313 -537.313 -537.313] [75.7617], Avg: [-573.079 -573.079 -573.079] (1.000)
Step: 63749, Reward: [-498.527 -498.527 -498.527] [96.7435], Avg: [-573.097 -573.097 -573.097] (1.000)
Step: 63799, Reward: [-469.35 -469.35 -469.35] [100.8126], Avg: [-573.094 -573.094 -573.094] (1.000)
Step: 63849, Reward: [-470.334 -470.334 -470.334] [72.8595], Avg: [-573.071 -573.071 -573.071] (1.000)
Step: 63899, Reward: [-490.497 -490.497 -490.497] [53.9332], Avg: [-573.048 -573.048 -573.048] (1.000)
Step: 63949, Reward: [-461.943 -461.943 -461.943] [92.3684], Avg: [-573.034 -573.034 -573.034] (1.000)
Step: 63999, Reward: [-461.78 -461.78 -461.78] [29.4440], Avg: [-572.97 -572.97 -572.97] (1.000)
Step: 64049, Reward: [-460.618 -460.618 -460.618] [85.3490], Avg: [-572.949 -572.949 -572.949] (1.000)
Step: 64099, Reward: [-427.697 -427.697 -427.697] [37.3534], Avg: [-572.865 -572.865 -572.865] (1.000)
Step: 64149, Reward: [-485.409 -485.409 -485.409] [70.7111], Avg: [-572.852 -572.852 -572.852] (1.000)
Step: 64199, Reward: [-437.72 -437.72 -437.72] [71.3784], Avg: [-572.802 -572.802 -572.802] (1.000)
Step: 64249, Reward: [-544.101 -544.101 -544.101] [137.7450], Avg: [-572.887 -572.887 -572.887] (1.000)
Step: 64299, Reward: [-400.959 -400.959 -400.959] [35.8831], Avg: [-572.781 -572.781 -572.781] (1.000)
Step: 64349, Reward: [-495.928 -495.928 -495.928] [91.9181], Avg: [-572.793 -572.793 -572.793] (1.000)
Step: 64399, Reward: [-483.785 -483.785 -483.785] [91.5530], Avg: [-572.795 -572.795 -572.795] (1.000)
Step: 64449, Reward: [-602.182 -602.182 -602.182] [143.5284], Avg: [-572.929 -572.929 -572.929] (1.000)
Step: 64499, Reward: [-485.949 -485.949 -485.949] [77.9734], Avg: [-572.922 -572.922 -572.922] (1.000)
Step: 64549, Reward: [-447.525 -447.525 -447.525] [110.6531], Avg: [-572.91 -572.91 -572.91] (1.000)
Step: 64599, Reward: [-439.997 -439.997 -439.997] [97.0196], Avg: [-572.883 -572.883 -572.883] (1.000)
Step: 64649, Reward: [-494.68 -494.68 -494.68] [40.4428], Avg: [-572.853 -572.853 -572.853] (1.000)
Step: 64699, Reward: [-492.181 -492.181 -492.181] [149.7546], Avg: [-572.907 -572.907 -572.907] (1.000)
Step: 64749, Reward: [-460.923 -460.923 -460.923] [86.2956], Avg: [-572.887 -572.887 -572.887] (1.000)
Step: 64799, Reward: [-426.659 -426.659 -426.659] [44.5047], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 64849, Reward: [-434.226 -434.226 -434.226] [48.3113], Avg: [-572.739 -572.739 -572.739] (1.000)
Step: 64899, Reward: [-430.361 -430.361 -430.361] [54.6485], Avg: [-572.671 -572.671 -572.671] (1.000)
Step: 64949, Reward: [-614.307 -614.307 -614.307] [98.2728], Avg: [-572.779 -572.779 -572.779] (1.000)
Step: 64999, Reward: [-419.382 -419.382 -419.382] [41.6584], Avg: [-572.693 -572.693 -572.693] (1.000)
Step: 65049, Reward: [-484.453 -484.453 -484.453] [94.7129], Avg: [-572.698 -572.698 -572.698] (1.000)
Step: 65099, Reward: [-510.996 -510.996 -510.996] [130.3732], Avg: [-572.751 -572.751 -572.751] (1.000)
Step: 65149, Reward: [-486.493 -486.493 -486.493] [58.1485], Avg: [-572.729 -572.729 -572.729] (1.000)
Step: 65199, Reward: [-529.206 -529.206 -529.206] [129.2247], Avg: [-572.795 -572.795 -572.795] (1.000)
Step: 65249, Reward: [-621.647 -621.647 -621.647] [85.1835], Avg: [-572.898 -572.898 -572.898] (1.000)
Step: 65299, Reward: [-532.567 -532.567 -532.567] [86.5068], Avg: [-572.933 -572.933 -572.933] (1.000)
Step: 65349, Reward: [-474.349 -474.349 -474.349] [109.0829], Avg: [-572.941 -572.941 -572.941] (1.000)
Step: 65399, Reward: [-434.255 -434.255 -434.255] [31.7917], Avg: [-572.859 -572.859 -572.859] (1.000)
Step: 65449, Reward: [-403.272 -403.272 -403.272] [57.6925], Avg: [-572.774 -572.774 -572.774] (1.000)
Step: 65499, Reward: [-489.873 -489.873 -489.873] [70.4797], Avg: [-572.764 -572.764 -572.764] (1.000)
Step: 65549, Reward: [-490.41 -490.41 -490.41] [86.2324], Avg: [-572.767 -572.767 -572.767] (1.000)
Step: 65599, Reward: [-482.084 -482.084 -482.084] [62.1674], Avg: [-572.746 -572.746 -572.746] (1.000)
Step: 65649, Reward: [-505.895 -505.895 -505.895] [24.1125], Avg: [-572.713 -572.713 -572.713] (1.000)
Step: 65699, Reward: [-429.537 -429.537 -429.537] [81.3340], Avg: [-572.666 -572.666 -572.666] (1.000)
Step: 65749, Reward: [-482.977 -482.977 -482.977] [65.5046], Avg: [-572.648 -572.648 -572.648] (1.000)
Step: 65799, Reward: [-384.444 -384.444 -384.444] [37.7742], Avg: [-572.533 -572.533 -572.533] (1.000)
Step: 65849, Reward: [-548.468 -548.468 -548.468] [73.5844], Avg: [-572.571 -572.571 -572.571] (1.000)
Step: 65899, Reward: [-473.182 -473.182 -473.182] [77.1878], Avg: [-572.554 -572.554 -572.554] (1.000)
Step: 65949, Reward: [-462.809 -462.809 -462.809] [83.7971], Avg: [-572.534 -572.534 -572.534] (1.000)
Step: 65999, Reward: [-446.496 -446.496 -446.496] [40.4923], Avg: [-572.469 -572.469 -572.469] (1.000)
Step: 66049, Reward: [-434.715 -434.715 -434.715] [69.9719], Avg: [-572.418 -572.418 -572.418] (1.000)
Step: 66099, Reward: [-465.458 -465.458 -465.458] [118.3926], Avg: [-572.427 -572.427 -572.427] (1.000)
Step: 66149, Reward: [-418.088 -418.088 -418.088] [42.3641], Avg: [-572.342 -572.342 -572.342] (1.000)
Step: 66199, Reward: [-539.624 -539.624 -539.624] [120.8995], Avg: [-572.409 -572.409 -572.409] (1.000)
Step: 66249, Reward: [-570.318 -570.318 -570.318] [76.3282], Avg: [-572.465 -572.465 -572.465] (1.000)
Step: 66299, Reward: [-505.35 -505.35 -505.35] [90.9101], Avg: [-572.483 -572.483 -572.483] (1.000)
Step: 66349, Reward: [-531.758 -531.758 -531.758] [190.4881], Avg: [-572.596 -572.596 -572.596] (1.000)
Step: 66399, Reward: [-490.817 -490.817 -490.817] [76.1092], Avg: [-572.591 -572.591 -572.591] (1.000)
Step: 66449, Reward: [-547.38 -547.38 -547.38] [93.1931], Avg: [-572.643 -572.643 -572.643] (1.000)
Step: 66499, Reward: [-545.034 -545.034 -545.034] [146.4977], Avg: [-572.732 -572.732 -572.732] (1.000)
Step: 66549, Reward: [-577.957 -577.957 -577.957] [162.7044], Avg: [-572.858 -572.858 -572.858] (1.000)
Step: 66599, Reward: [-397.479 -397.479 -397.479] [49.2026], Avg: [-572.763 -572.763 -572.763] (1.000)
Step: 66649, Reward: [-446.273 -446.273 -446.273] [53.4235], Avg: [-572.709 -572.709 -572.709] (1.000)
Step: 66699, Reward: [-462.484 -462.484 -462.484] [74.4127], Avg: [-572.682 -572.682 -572.682] (1.000)
Step: 66749, Reward: [-570.751 -570.751 -570.751] [178.8866], Avg: [-572.814 -572.814 -572.814] (1.000)
Step: 66799, Reward: [-454.248 -454.248 -454.248] [79.1046], Avg: [-572.785 -572.785 -572.785] (1.000)
Step: 66849, Reward: [-581.313 -581.313 -581.313] [155.2451], Avg: [-572.907 -572.907 -572.907] (1.000)
Step: 66899, Reward: [-434.149 -434.149 -434.149] [40.4557], Avg: [-572.834 -572.834 -572.834] (1.000)
Step: 66949, Reward: [-485.928 -485.928 -485.928] [116.5645], Avg: [-572.856 -572.856 -572.856] (1.000)
Step: 66999, Reward: [-516.203 -516.203 -516.203] [122.2568], Avg: [-572.905 -572.905 -572.905] (1.000)
Step: 67049, Reward: [-491.345 -491.345 -491.345] [130.5644], Avg: [-572.941 -572.941 -572.941] (1.000)
Step: 67099, Reward: [-525.454 -525.454 -525.454] [59.0892], Avg: [-572.95 -572.95 -572.95] (1.000)
Step: 67149, Reward: [-491.432 -491.432 -491.432] [88.8518], Avg: [-572.955 -572.955 -572.955] (1.000)
Step: 67199, Reward: [-526.755 -526.755 -526.755] [89.8139], Avg: [-572.988 -572.988 -572.988] (1.000)
Step: 67249, Reward: [-434.394 -434.394 -434.394] [51.6845], Avg: [-572.923 -572.923 -572.923] (1.000)
Step: 67299, Reward: [-469.057 -469.057 -469.057] [129.1436], Avg: [-572.942 -572.942 -572.942] (1.000)
Step: 67349, Reward: [-508.231 -508.231 -508.231] [99.9283], Avg: [-572.968 -572.968 -572.968] (1.000)
Step: 67399, Reward: [-520.85 -520.85 -520.85] [57.0200], Avg: [-572.972 -572.972 -572.972] (1.000)
Step: 67449, Reward: [-485.745 -485.745 -485.745] [72.8442], Avg: [-572.961 -572.961 -572.961] (1.000)
Step: 67499, Reward: [-454.924 -454.924 -454.924] [47.0940], Avg: [-572.909 -572.909 -572.909] (1.000)
Step: 67549, Reward: [-445.019 -445.019 -445.019] [54.2108], Avg: [-572.854 -572.854 -572.854] (1.000)
Step: 67599, Reward: [-483.355 -483.355 -483.355] [44.2176], Avg: [-572.821 -572.821 -572.821] (1.000)
Step: 67649, Reward: [-505.455 -505.455 -505.455] [59.8092], Avg: [-572.815 -572.815 -572.815] (1.000)
Step: 67699, Reward: [-521.793 -521.793 -521.793] [190.8697], Avg: [-572.918 -572.918 -572.918] (1.000)
Step: 67749, Reward: [-452.263 -452.263 -452.263] [54.3067], Avg: [-572.869 -572.869 -572.869] (1.000)
Step: 67799, Reward: [-468.104 -468.104 -468.104] [90.1987], Avg: [-572.859 -572.859 -572.859] (1.000)
Step: 67849, Reward: [-448.921 -448.921 -448.921] [57.8739], Avg: [-572.81 -572.81 -572.81] (1.000)
Step: 67899, Reward: [-486.443 -486.443 -486.443] [95.1367], Avg: [-572.816 -572.816 -572.816] (1.000)
Step: 67949, Reward: [-489.352 -489.352 -489.352] [151.4648], Avg: [-572.866 -572.866 -572.866] (1.000)
Step: 67999, Reward: [-490.16 -490.16 -490.16] [58.7965], Avg: [-572.849 -572.849 -572.849] (1.000)
Step: 68049, Reward: [-502.945 -502.945 -502.945] [81.2137], Avg: [-572.857 -572.857 -572.857] (1.000)
Step: 68099, Reward: [-416.486 -416.486 -416.486] [44.9712], Avg: [-572.775 -572.775 -572.775] (1.000)
Step: 68149, Reward: [-540.45 -540.45 -540.45] [157.6032], Avg: [-572.867 -572.867 -572.867] (1.000)
Step: 68199, Reward: [-486.562 -486.562 -486.562] [57.6851], Avg: [-572.846 -572.846 -572.846] (1.000)
Step: 68249, Reward: [-451.885 -451.885 -451.885] [63.5796], Avg: [-572.804 -572.804 -572.804] (1.000)
Step: 68299, Reward: [-492.246 -492.246 -492.246] [57.5501], Avg: [-572.787 -572.787 -572.787] (1.000)
Step: 68349, Reward: [-542.89 -542.89 -542.89] [185.1308], Avg: [-572.901 -572.901 -572.901] (1.000)
Step: 68399, Reward: [-502.16 -502.16 -502.16] [70.2180], Avg: [-572.901 -572.901 -572.901] (1.000)
Step: 68449, Reward: [-503.919 -503.919 -503.919] [22.5959], Avg: [-572.867 -572.867 -572.867] (1.000)
Step: 68499, Reward: [-436.151 -436.151 -436.151] [65.1497], Avg: [-572.814 -572.814 -572.814] (1.000)
Step: 68549, Reward: [-505.783 -505.783 -505.783] [88.1555], Avg: [-572.83 -572.83 -572.83] (1.000)
Step: 68599, Reward: [-481.591 -481.591 -481.591] [129.5924], Avg: [-572.858 -572.858 -572.858] (1.000)
Step: 68649, Reward: [-416.022 -416.022 -416.022] [29.4958], Avg: [-572.765 -572.765 -572.765] (1.000)
Step: 68699, Reward: [-484.009 -484.009 -484.009] [93.1346], Avg: [-572.768 -572.768 -572.768] (1.000)
Step: 68749, Reward: [-556.135 -556.135 -556.135] [95.5490], Avg: [-572.826 -572.826 -572.826] (1.000)
Step: 68799, Reward: [-485.108 -485.108 -485.108] [54.2449], Avg: [-572.801 -572.801 -572.801] (1.000)
Step: 68849, Reward: [-471.811 -471.811 -471.811] [78.4470], Avg: [-572.785 -572.785 -572.785] (1.000)
Step: 68899, Reward: [-490.98 -490.98 -490.98] [42.7719], Avg: [-572.757 -572.757 -572.757] (1.000)
Step: 68949, Reward: [-488.648 -488.648 -488.648] [116.9152], Avg: [-572.78 -572.78 -572.78] (1.000)
Step: 68999, Reward: [-469.827 -469.827 -469.827] [85.7225], Avg: [-572.768 -572.768 -572.768] (1.000)
Step: 69049, Reward: [-474.81 -474.81 -474.81] [61.9091], Avg: [-572.742 -572.742 -572.742] (1.000)
Step: 69099, Reward: [-450.556 -450.556 -450.556] [47.3274], Avg: [-572.688 -572.688 -572.688] (1.000)
Step: 69149, Reward: [-450.951 -450.951 -450.951] [77.7561], Avg: [-572.656 -572.656 -572.656] (1.000)
Step: 69199, Reward: [-443.612 -443.612 -443.612] [100.1126], Avg: [-572.635 -572.635 -572.635] (1.000)
Step: 69249, Reward: [-489.725 -489.725 -489.725] [103.6478], Avg: [-572.65 -572.65 -572.65] (1.000)
Step: 69299, Reward: [-484.92 -484.92 -484.92] [84.8544], Avg: [-572.648 -572.648 -572.648] (1.000)
Step: 69349, Reward: [-411.686 -411.686 -411.686] [70.1041], Avg: [-572.582 -572.582 -572.582] (1.000)
Step: 69399, Reward: [-496.986 -496.986 -496.986] [121.4953], Avg: [-572.615 -572.615 -572.615] (1.000)
Step: 69449, Reward: [-516.13 -516.13 -516.13] [91.5872], Avg: [-572.641 -572.641 -572.641] (1.000)
Step: 69499, Reward: [-447.876 -447.876 -447.876] [101.5465], Avg: [-572.624 -572.624 -572.624] (1.000)
Step: 69549, Reward: [-473.656 -473.656 -473.656] [46.2356], Avg: [-572.586 -572.586 -572.586] (1.000)
Step: 69599, Reward: [-449.698 -449.698 -449.698] [32.4419], Avg: [-572.521 -572.521 -572.521] (1.000)
Step: 69649, Reward: [-513.473 -513.473 -513.473] [45.4110], Avg: [-572.511 -572.511 -572.511] (1.000)
Step: 69699, Reward: [-478.153 -478.153 -478.153] [50.6792], Avg: [-572.48 -572.48 -572.48] (1.000)
Step: 69749, Reward: [-502.408 -502.408 -502.408] [95.3275], Avg: [-572.498 -572.498 -572.498] (1.000)
Step: 69799, Reward: [-484.691 -484.691 -484.691] [79.8411], Avg: [-572.492 -572.492 -572.492] (1.000)
Step: 69849, Reward: [-487.825 -487.825 -487.825] [111.5600], Avg: [-572.512 -572.512 -572.512] (1.000)
Step: 69899, Reward: [-495.088 -495.088 -495.088] [148.7752], Avg: [-572.563 -572.563 -572.563] (1.000)
Step: 69949, Reward: [-443.723 -443.723 -443.723] [68.0780], Avg: [-572.519 -572.519 -572.519] (1.000)
Step: 69999, Reward: [-536.349 -536.349 -536.349] [126.5240], Avg: [-572.584 -572.584 -572.584] (1.000)
Step: 70049, Reward: [-497.259 -497.259 -497.259] [110.0403], Avg: [-572.609 -572.609 -572.609] (1.000)
Step: 70099, Reward: [-457.807 -457.807 -457.807] [72.2858], Avg: [-572.578 -572.578 -572.578] (1.000)
Step: 70149, Reward: [-494.217 -494.217 -494.217] [61.2293], Avg: [-572.566 -572.566 -572.566] (1.000)
Step: 70199, Reward: [-455.943 -455.943 -455.943] [87.3997], Avg: [-572.545 -572.545 -572.545] (1.000)
Step: 70249, Reward: [-479.075 -479.075 -479.075] [86.2175], Avg: [-572.54 -572.54 -572.54] (1.000)
Step: 70299, Reward: [-472.35 -472.35 -472.35] [71.6226], Avg: [-572.52 -572.52 -572.52] (1.000)
Step: 70349, Reward: [-515.559 -515.559 -515.559] [45.1091], Avg: [-572.511 -572.511 -572.511] (1.000)
Step: 70399, Reward: [-480.148 -480.148 -480.148] [130.2095], Avg: [-572.538 -572.538 -572.538] (1.000)
Step: 70449, Reward: [-527.856 -527.856 -527.856] [33.9889], Avg: [-572.531 -572.531 -572.531] (1.000)
Step: 70499, Reward: [-469.394 -469.394 -469.394] [85.0436], Avg: [-572.518 -572.518 -572.518] (1.000)
Step: 70549, Reward: [-477.551 -477.551 -477.551] [96.9309], Avg: [-572.519 -572.519 -572.519] (1.000)
Step: 70599, Reward: [-493.33 -493.33 -493.33] [43.9857], Avg: [-572.494 -572.494 -572.494] (1.000)
Step: 70649, Reward: [-519.681 -519.681 -519.681] [51.6562], Avg: [-572.493 -572.493 -572.493] (1.000)
Step: 70699, Reward: [-532.401 -532.401 -532.401] [58.1839], Avg: [-572.506 -572.506 -572.506] (1.000)
Step: 70749, Reward: [-526.244 -526.244 -526.244] [204.7624], Avg: [-572.618 -572.618 -572.618] (1.000)
Step: 70799, Reward: [-516.215 -516.215 -516.215] [47.0769], Avg: [-572.612 -572.612 -572.612] (1.000)
Step: 70849, Reward: [-489.133 -489.133 -489.133] [66.0192], Avg: [-572.599 -572.599 -572.599] (1.000)
Step: 70899, Reward: [-445.885 -445.885 -445.885] [64.6722], Avg: [-572.556 -572.556 -572.556] (1.000)
Step: 70949, Reward: [-458.862 -458.862 -458.862] [93.1707], Avg: [-572.541 -572.541 -572.541] (1.000)
Step: 70999, Reward: [-530.216 -530.216 -530.216] [85.0918], Avg: [-572.571 -572.571 -572.571] (1.000)
Step: 71049, Reward: [-556.514 -556.514 -556.514] [158.7228], Avg: [-572.672 -572.672 -572.672] (1.000)
Step: 71099, Reward: [-540.765 -540.765 -540.765] [70.8434], Avg: [-572.699 -572.699 -572.699] (1.000)
Step: 71149, Reward: [-420.238 -420.238 -420.238] [49.3594], Avg: [-572.627 -572.627 -572.627] (1.000)
Step: 71199, Reward: [-587.608 -587.608 -587.608] [88.0860], Avg: [-572.699 -572.699 -572.699] (1.000)
Step: 71249, Reward: [-494.748 -494.748 -494.748] [114.7488], Avg: [-572.725 -572.725 -572.725] (1.000)
Step: 71299, Reward: [-438.802 -438.802 -438.802] [55.7948], Avg: [-572.67 -572.67 -572.67] (1.000)
Step: 71349, Reward: [-456.732 -456.732 -456.732] [59.9351], Avg: [-572.631 -572.631 -572.631] (1.000)
Step: 71399, Reward: [-485.706 -485.706 -485.706] [78.6432], Avg: [-572.625 -572.625 -572.625] (1.000)
Step: 71449, Reward: [-449.593 -449.593 -449.593] [86.9739], Avg: [-572.6 -572.6 -572.6] (1.000)
Step: 71499, Reward: [-539.769 -539.769 -539.769] [125.1253], Avg: [-572.664 -572.664 -572.664] (1.000)
Step: 71549, Reward: [-486.521 -486.521 -486.521] [43.0085], Avg: [-572.634 -572.634 -572.634] (1.000)
Step: 71599, Reward: [-495.225 -495.225 -495.225] [88.9830], Avg: [-572.642 -572.642 -572.642] (1.000)
Step: 71649, Reward: [-530.553 -530.553 -530.553] [60.9962], Avg: [-572.655 -572.655 -572.655] (1.000)
Step: 71699, Reward: [-572.853 -572.853 -572.853] [122.6471], Avg: [-572.741 -572.741 -572.741] (1.000)
Step: 71749, Reward: [-526.162 -526.162 -526.162] [106.6168], Avg: [-572.783 -572.783 -572.783] (1.000)
Step: 71799, Reward: [-522.892 -522.892 -522.892] [66.5219], Avg: [-572.794 -572.794 -572.794] (1.000)
Step: 71849, Reward: [-509.039 -509.039 -509.039] [131.1306], Avg: [-572.841 -572.841 -572.841] (1.000)
Step: 71899, Reward: [-503.342 -503.342 -503.342] [85.1228], Avg: [-572.852 -572.852 -572.852] (1.000)
Step: 71949, Reward: [-519.741 -519.741 -519.741] [101.7866], Avg: [-572.886 -572.886 -572.886] (1.000)
Step: 71999, Reward: [-491.506 -491.506 -491.506] [94.4600], Avg: [-572.895 -572.895 -572.895] (1.000)
Step: 72049, Reward: [-559.019 -559.019 -559.019] [107.6344], Avg: [-572.96 -572.96 -572.96] (1.000)
Step: 72099, Reward: [-481.219 -481.219 -481.219] [64.5130], Avg: [-572.941 -572.941 -572.941] (1.000)
Step: 72149, Reward: [-501.03 -501.03 -501.03] [74.5766], Avg: [-572.943 -572.943 -572.943] (1.000)
Step: 72199, Reward: [-476.359 -476.359 -476.359] [68.5810], Avg: [-572.924 -572.924 -572.924] (1.000)
Step: 72249, Reward: [-447.986 -447.986 -447.986] [31.0165], Avg: [-572.859 -572.859 -572.859] (1.000)
Step: 72299, Reward: [-439.909 -439.909 -439.909] [36.4277], Avg: [-572.792 -572.792 -572.792] (1.000)
Step: 72349, Reward: [-533.702 -533.702 -533.702] [40.7437], Avg: [-572.793 -572.793 -572.793] (1.000)
Step: 72399, Reward: [-545.751 -545.751 -545.751] [88.4143], Avg: [-572.836 -572.836 -572.836] (1.000)
Step: 72449, Reward: [-435.015 -435.015 -435.015] [28.5389], Avg: [-572.76 -572.76 -572.76] (1.000)
Step: 72499, Reward: [-519.834 -519.834 -519.834] [134.2066], Avg: [-572.816 -572.816 -572.816] (1.000)
Step: 72549, Reward: [-494.318 -494.318 -494.318] [133.1914], Avg: [-572.854 -572.854 -572.854] (1.000)
Step: 72599, Reward: [-494.866 -494.866 -494.866] [80.7568], Avg: [-572.856 -572.856 -572.856] (1.000)
Step: 72649, Reward: [-450.722 -450.722 -450.722] [88.2110], Avg: [-572.832 -572.832 -572.832] (1.000)
Step: 72699, Reward: [-489.267 -489.267 -489.267] [48.0994], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 72749, Reward: [-560.756 -560.756 -560.756] [131.2258], Avg: [-572.89 -572.89 -572.89] (1.000)
Step: 72799, Reward: [-551.857 -551.857 -551.857] [50.9021], Avg: [-572.91 -572.91 -572.91] (1.000)
Step: 72849, Reward: [-525.839 -525.839 -525.839] [80.3203], Avg: [-572.933 -572.933 -572.933] (1.000)
Step: 72899, Reward: [-421.065 -421.065 -421.065] [43.2947], Avg: [-572.859 -572.859 -572.859] (1.000)
Step: 72949, Reward: [-585.538 -585.538 -585.538] [169.1777], Avg: [-572.983 -572.983 -572.983] (1.000)
Step: 72999, Reward: [-499.821 -499.821 -499.821] [53.5934], Avg: [-572.97 -572.97 -572.97] (1.000)
Step: 73049, Reward: [-507.374 -507.374 -507.374] [73.2242], Avg: [-572.975 -572.975 -572.975] (1.000)
Step: 73099, Reward: [-445.24 -445.24 -445.24] [51.5130], Avg: [-572.923 -572.923 -572.923] (1.000)
Step: 73149, Reward: [-455.981 -455.981 -455.981] [70.3575], Avg: [-572.891 -572.891 -572.891] (1.000)
Step: 73199, Reward: [-451.086 -451.086 -451.086] [50.3140], Avg: [-572.842 -572.842 -572.842] (1.000)
Step: 73249, Reward: [-528.539 -528.539 -528.539] [50.0688], Avg: [-572.846 -572.846 -572.846] (1.000)
Step: 73299, Reward: [-449.183 -449.183 -449.183] [67.6728], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 73349, Reward: [-479.372 -479.372 -479.372] [72.2054], Avg: [-572.794 -572.794 -572.794] (1.000)
Step: 73399, Reward: [-572.372 -572.372 -572.372] [151.6907], Avg: [-572.897 -572.897 -572.897] (1.000)
Step: 73449, Reward: [-475.193 -475.193 -475.193] [25.5463], Avg: [-572.848 -572.848 -572.848] (1.000)
Step: 73499, Reward: [-526.184 -526.184 -526.184] [166.2939], Avg: [-572.929 -572.929 -572.929] (1.000)
Step: 73549, Reward: [-450.9 -450.9 -450.9] [46.2596], Avg: [-572.878 -572.878 -572.878] (1.000)
Step: 73599, Reward: [-507.22 -507.22 -507.22] [122.4547], Avg: [-572.916 -572.916 -572.916] (1.000)
Step: 73649, Reward: [-538.866 -538.866 -538.866] [99.1217], Avg: [-572.96 -572.96 -572.96] (1.000)
Step: 73699, Reward: [-485.711 -485.711 -485.711] [65.9468], Avg: [-572.946 -572.946 -572.946] (1.000)
Step: 73749, Reward: [-451.48 -451.48 -451.48] [51.0577], Avg: [-572.898 -572.898 -572.898] (1.000)
Step: 73799, Reward: [-447.553 -447.553 -447.553] [72.9632], Avg: [-572.863 -572.863 -572.863] (1.000)
Step: 73849, Reward: [-523.943 -523.943 -523.943] [77.5634], Avg: [-572.882 -572.882 -572.882] (1.000)
Step: 73899, Reward: [-445.509 -445.509 -445.509] [70.6814], Avg: [-572.844 -572.844 -572.844] (1.000)
Step: 73949, Reward: [-513.699 -513.699 -513.699] [69.5771], Avg: [-572.851 -572.851 -572.851] (1.000)
Step: 73999, Reward: [-438.927 -438.927 -438.927] [37.5609], Avg: [-572.786 -572.786 -572.786] (1.000)
Step: 74049, Reward: [-526.789 -526.789 -526.789] [79.2339], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 74099, Reward: [-472.402 -472.402 -472.402] [50.2012], Avg: [-572.774 -572.774 -572.774] (1.000)
Step: 74149, Reward: [-513.302 -513.302 -513.302] [105.6428], Avg: [-572.805 -572.805 -572.805] (1.000)
Step: 74199, Reward: [-447.91 -447.91 -447.91] [85.2543], Avg: [-572.779 -572.779 -572.779] (1.000)
Step: 74249, Reward: [-537.556 -537.556 -537.556] [78.6511], Avg: [-572.808 -572.808 -572.808] (1.000)
Step: 74299, Reward: [-494.097 -494.097 -494.097] [110.0768], Avg: [-572.829 -572.829 -572.829] (1.000)
Step: 74349, Reward: [-452.323 -452.323 -452.323] [83.3088], Avg: [-572.804 -572.804 -572.804] (1.000)
Step: 74399, Reward: [-482.334 -482.334 -482.334] [78.0790], Avg: [-572.796 -572.796 -572.796] (1.000)
Step: 74449, Reward: [-397.484 -397.484 -397.484] [31.8362], Avg: [-572.699 -572.699 -572.699] (1.000)
Step: 74499, Reward: [-499.622 -499.622 -499.622] [92.5862], Avg: [-572.712 -572.712 -572.712] (1.000)
Step: 74549, Reward: [-486.997 -486.997 -486.997] [40.6186], Avg: [-572.682 -572.682 -572.682] (1.000)
Step: 74599, Reward: [-466.902 -466.902 -466.902] [45.4850], Avg: [-572.642 -572.642 -572.642] (1.000)
Step: 74649, Reward: [-488.759 -488.759 -488.759] [125.2537], Avg: [-572.669 -572.669 -572.669] (1.000)
Step: 74699, Reward: [-510.557 -510.557 -510.557] [95.3278], Avg: [-572.692 -572.692 -572.692] (1.000)
Step: 74749, Reward: [-521.462 -521.462 -521.462] [119.9037], Avg: [-572.738 -572.738 -572.738] (1.000)
Step: 74799, Reward: [-448.852 -448.852 -448.852] [25.6072], Avg: [-572.672 -572.672 -572.672] (1.000)
Step: 74849, Reward: [-469.315 -469.315 -469.315] [48.4243], Avg: [-572.635 -572.635 -572.635] (1.000)
Step: 74899, Reward: [-509.025 -509.025 -509.025] [82.3741], Avg: [-572.648 -572.648 -572.648] (1.000)
Step: 74949, Reward: [-529.098 -529.098 -529.098] [124.5218], Avg: [-572.702 -572.702 -572.702] (1.000)
Step: 74999, Reward: [-491.387 -491.387 -491.387] [55.8854], Avg: [-572.685 -572.685 -572.685] (1.000)
Step: 75049, Reward: [-467.983 -467.983 -467.983] [57.2160], Avg: [-572.653 -572.653 -572.653] (1.000)
Step: 75099, Reward: [-439.279 -439.279 -439.279] [48.5728], Avg: [-572.597 -572.597 -572.597] (1.000)
Step: 75149, Reward: [-495.358 -495.358 -495.358] [98.5347], Avg: [-572.611 -572.611 -572.611] (1.000)
Step: 75199, Reward: [-400.378 -400.378 -400.378] [37.1244], Avg: [-572.521 -572.521 -572.521] (1.000)
Step: 75249, Reward: [-581.736 -581.736 -581.736] [202.5036], Avg: [-572.662 -572.662 -572.662] (1.000)
Step: 75299, Reward: [-397.161 -397.161 -397.161] [39.8553], Avg: [-572.572 -572.572 -572.572] (1.000)
Step: 75349, Reward: [-521.455 -521.455 -521.455] [42.1199], Avg: [-572.566 -572.566 -572.566] (1.000)
Step: 75399, Reward: [-499.542 -499.542 -499.542] [82.0085], Avg: [-572.572 -572.572 -572.572] (1.000)
Step: 75449, Reward: [-507.489 -507.489 -507.489] [150.4653], Avg: [-572.628 -572.628 -572.628] (1.000)
Step: 75499, Reward: [-566.087 -566.087 -566.087] [175.5421], Avg: [-572.74 -572.74 -572.74] (1.000)
Step: 75549, Reward: [-497.437 -497.437 -497.437] [57.5897], Avg: [-572.728 -572.728 -572.728] (1.000)
Step: 75599, Reward: [-462.098 -462.098 -462.098] [36.1750], Avg: [-572.679 -572.679 -572.679] (1.000)
Step: 75649, Reward: [-517.574 -517.574 -517.574] [126.5597], Avg: [-572.726 -572.726 -572.726] (1.000)
Step: 75699, Reward: [-422.352 -422.352 -422.352] [34.9607], Avg: [-572.65 -572.65 -572.65] (1.000)
Step: 75749, Reward: [-507.105 -507.105 -507.105] [35.3799], Avg: [-572.63 -572.63 -572.63] (1.000)
Step: 75799, Reward: [-557.297 -557.297 -557.297] [51.6688], Avg: [-572.654 -572.654 -572.654] (1.000)
Step: 75849, Reward: [-426.217 -426.217 -426.217] [17.1079], Avg: [-572.569 -572.569 -572.569] (1.000)
Step: 75899, Reward: [-478.489 -478.489 -478.489] [133.3290], Avg: [-572.595 -572.595 -572.595] (1.000)
Step: 75949, Reward: [-499.738 -499.738 -499.738] [69.0366], Avg: [-572.592 -572.592 -572.592] (1.000)
Step: 75999, Reward: [-469.661 -469.661 -469.661] [39.8012], Avg: [-572.551 -572.551 -572.551] (1.000)
Step: 76049, Reward: [-503.701 -503.701 -503.701] [93.3903], Avg: [-572.567 -572.567 -572.567] (1.000)
Step: 76099, Reward: [-521.258 -521.258 -521.258] [57.7142], Avg: [-572.571 -572.571 -572.571] (1.000)
Step: 76149, Reward: [-488.496 -488.496 -488.496] [34.7148], Avg: [-572.539 -572.539 -572.539] (1.000)
Step: 76199, Reward: [-550.069 -550.069 -550.069] [62.3691], Avg: [-572.565 -572.565 -572.565] (1.000)
Step: 76249, Reward: [-543.694 -543.694 -543.694] [99.9891], Avg: [-572.611 -572.611 -572.611] (1.000)
Step: 76299, Reward: [-476.505 -476.505 -476.505] [72.4336], Avg: [-572.596 -572.596 -572.596] (1.000)
Step: 76349, Reward: [-431.196 -431.196 -431.196] [36.8323], Avg: [-572.527 -572.527 -572.527] (1.000)
Step: 76399, Reward: [-456.253 -456.253 -456.253] [71.0409], Avg: [-572.498 -572.498 -572.498] (1.000)
Step: 76449, Reward: [-463.714 -463.714 -463.714] [63.5888], Avg: [-572.468 -572.468 -572.468] (1.000)
Step: 76499, Reward: [-541.736 -541.736 -541.736] [52.0801], Avg: [-572.482 -572.482 -572.482] (1.000)
Step: 76549, Reward: [-444.466 -444.466 -444.466] [84.6665], Avg: [-572.454 -572.454 -572.454] (1.000)
Step: 76599, Reward: [-493.597 -493.597 -493.597] [96.4437], Avg: [-572.465 -572.465 -572.465] (1.000)
Step: 76649, Reward: [-533.609 -533.609 -533.609] [92.6385], Avg: [-572.501 -572.501 -572.501] (1.000)
Step: 76699, Reward: [-488.691 -488.691 -488.691] [57.7705], Avg: [-572.484 -572.484 -572.484] (1.000)
Step: 76749, Reward: [-460.69 -460.69 -460.69] [51.8806], Avg: [-572.444 -572.444 -572.444] (1.000)
Step: 76799, Reward: [-560.854 -560.854 -560.854] [121.1105], Avg: [-572.516 -572.516 -572.516] (1.000)
Step: 76849, Reward: [-581.389 -581.389 -581.389] [109.7380], Avg: [-572.593 -572.593 -572.593] (1.000)
Step: 76899, Reward: [-468.258 -468.258 -468.258] [62.4810], Avg: [-572.566 -572.566 -572.566] (1.000)
Step: 76949, Reward: [-456.707 -456.707 -456.707] [53.3393], Avg: [-572.525 -572.525 -572.525] (1.000)
Step: 76999, Reward: [-566.7 -566.7 -566.7] [115.0430], Avg: [-572.596 -572.596 -572.596] (1.000)
Step: 77049, Reward: [-465.57 -465.57 -465.57] [110.3900], Avg: [-572.598 -572.598 -572.598] (1.000)
Step: 77099, Reward: [-517.956 -517.956 -517.956] [86.6728], Avg: [-572.619 -572.619 -572.619] (1.000)
Step: 77149, Reward: [-464.547 -464.547 -464.547] [89.6250], Avg: [-572.607 -572.607 -572.607] (1.000)
Step: 77199, Reward: [-589.409 -589.409 -589.409] [84.5391], Avg: [-572.673 -572.673 -572.673] (1.000)
Step: 77249, Reward: [-452.979 -452.979 -452.979] [38.4621], Avg: [-572.62 -572.62 -572.62] (1.000)
Step: 77299, Reward: [-509.181 -509.181 -509.181] [181.0529], Avg: [-572.696 -572.696 -572.696] (1.000)
Step: 77349, Reward: [-512.073 -512.073 -512.073] [147.5231], Avg: [-572.752 -572.752 -572.752] (1.000)
Step: 77399, Reward: [-515.591 -515.591 -515.591] [65.2981], Avg: [-572.758 -572.758 -572.758] (1.000)
Step: 77449, Reward: [-442.942 -442.942 -442.942] [66.6199], Avg: [-572.717 -572.717 -572.717] (1.000)
Step: 77499, Reward: [-535.833 -535.833 -535.833] [38.5212], Avg: [-572.718 -572.718 -572.718] (1.000)
Step: 77549, Reward: [-504.437 -504.437 -504.437] [79.6339], Avg: [-572.725 -572.725 -572.725] (1.000)
Step: 77599, Reward: [-431.926 -431.926 -431.926] [33.9590], Avg: [-572.656 -572.656 -572.656] (1.000)
Step: 77649, Reward: [-496.966 -496.966 -496.966] [92.8443], Avg: [-572.667 -572.667 -572.667] (1.000)
Step: 77699, Reward: [-441.932 -441.932 -441.932] [25.0271], Avg: [-572.599 -572.599 -572.599] (1.000)
Step: 77749, Reward: [-525.331 -525.331 -525.331] [87.0904], Avg: [-572.625 -572.625 -572.625] (1.000)
Step: 77799, Reward: [-491.066 -491.066 -491.066] [80.1540], Avg: [-572.624 -572.624 -572.624] (1.000)
Step: 77849, Reward: [-493.085 -493.085 -493.085] [88.6507], Avg: [-572.63 -572.63 -572.63] (1.000)
Step: 77899, Reward: [-562.239 -562.239 -562.239] [87.2671], Avg: [-572.679 -572.679 -572.679] (1.000)
Step: 77949, Reward: [-548.245 -548.245 -548.245] [167.3297], Avg: [-572.771 -572.771 -572.771] (1.000)
Step: 77999, Reward: [-490.173 -490.173 -490.173] [117.8982], Avg: [-572.794 -572.794 -572.794] (1.000)
Step: 78049, Reward: [-491.702 -491.702 -491.702] [52.4142], Avg: [-572.775 -572.775 -572.775] (1.000)
Step: 78099, Reward: [-564.279 -564.279 -564.279] [136.2687], Avg: [-572.857 -572.857 -572.857] (1.000)
Step: 78149, Reward: [-522.456 -522.456 -522.456] [68.0572], Avg: [-572.868 -572.868 -572.868] (1.000)
Step: 78199, Reward: [-487.753 -487.753 -487.753] [19.6524], Avg: [-572.826 -572.826 -572.826] (1.000)
Step: 78249, Reward: [-477.833 -477.833 -477.833] [128.3669], Avg: [-572.848 -572.848 -572.848] (1.000)
Step: 78299, Reward: [-549.517 -549.517 -549.517] [90.6924], Avg: [-572.891 -572.891 -572.891] (1.000)
Step: 78349, Reward: [-405.878 -405.878 -405.878] [27.5791], Avg: [-572.802 -572.802 -572.802] (1.000)
Step: 78399, Reward: [-490.547 -490.547 -490.547] [36.4103], Avg: [-572.773 -572.773 -572.773] (1.000)
Step: 78449, Reward: [-446.985 -446.985 -446.985] [91.0675], Avg: [-572.75 -572.75 -572.75] (1.000)
Step: 78499, Reward: [-444.173 -444.173 -444.173] [42.6582], Avg: [-572.696 -572.696 -572.696] (1.000)
Step: 78549, Reward: [-435.63 -435.63 -435.63] [58.9511], Avg: [-572.646 -572.646 -572.646] (1.000)
Step: 78599, Reward: [-555.51 -555.51 -555.51] [39.7368], Avg: [-572.66 -572.66 -572.66] (1.000)
Step: 78649, Reward: [-524.628 -524.628 -524.628] [109.3890], Avg: [-572.699 -572.699 -572.699] (1.000)
Step: 78699, Reward: [-457.407 -457.407 -457.407] [62.9572], Avg: [-572.666 -572.666 -572.666] (1.000)
Step: 78749, Reward: [-559.573 -559.573 -559.573] [136.2004], Avg: [-572.744 -572.744 -572.744] (1.000)
Step: 78799, Reward: [-467.101 -467.101 -467.101] [75.9956], Avg: [-572.725 -572.725 -572.725] (1.000)
Step: 78849, Reward: [-472.574 -472.574 -472.574] [90.1980], Avg: [-572.719 -572.719 -572.719] (1.000)
Step: 78899, Reward: [-449.95 -449.95 -449.95] [38.7168], Avg: [-572.666 -572.666 -572.666] (1.000)
Step: 78949, Reward: [-441.209 -441.209 -441.209] [90.8159], Avg: [-572.64 -572.64 -572.64] (1.000)
Step: 78999, Reward: [-505.518 -505.518 -505.518] [99.9134], Avg: [-572.661 -572.661 -572.661] (1.000)
Step: 79049, Reward: [-478.649 -478.649 -478.649] [89.9598], Avg: [-572.658 -572.658 -572.658] (1.000)
Step: 79099, Reward: [-569.467 -569.467 -569.467] [69.3420], Avg: [-572.7 -572.7 -572.7] (1.000)
Step: 79149, Reward: [-497.676 -497.676 -497.676] [41.9674], Avg: [-572.679 -572.679 -572.679] (1.000)
Step: 79199, Reward: [-419.361 -419.361 -419.361] [48.3959], Avg: [-572.613 -572.613 -572.613] (1.000)
Step: 79249, Reward: [-528.996 -528.996 -528.996] [101.0912], Avg: [-572.649 -572.649 -572.649] (1.000)
Step: 79299, Reward: [-451.719 -451.719 -451.719] [59.3303], Avg: [-572.61 -572.61 -572.61] (1.000)
Step: 79349, Reward: [-517.58 -517.58 -517.58] [146.4249], Avg: [-572.668 -572.668 -572.668] (1.000)
Step: 79399, Reward: [-496.458 -496.458 -496.458] [81.0774], Avg: [-572.671 -572.671 -572.671] (1.000)
Step: 79449, Reward: [-510.428 -510.428 -510.428] [52.3009], Avg: [-572.665 -572.665 -572.665] (1.000)
Step: 79499, Reward: [-500.928 -500.928 -500.928] [100.9034], Avg: [-572.683 -572.683 -572.683] (1.000)
Step: 79549, Reward: [-528.359 -528.359 -528.359] [68.0614], Avg: [-572.698 -572.698 -572.698] (1.000)
Step: 79599, Reward: [-480.548 -480.548 -480.548] [73.1801], Avg: [-572.686 -572.686 -572.686] (1.000)
Step: 79649, Reward: [-555.624 -555.624 -555.624] [127.9883], Avg: [-572.756 -572.756 -572.756] (1.000)
Step: 79699, Reward: [-443.874 -443.874 -443.874] [93.9802], Avg: [-572.734 -572.734 -572.734] (1.000)
Step: 79749, Reward: [-502.97 -502.97 -502.97] [142.9742], Avg: [-572.78 -572.78 -572.78] (1.000)
Step: 79799, Reward: [-591.595 -591.595 -591.595] [174.2164], Avg: [-572.901 -572.901 -572.901] (1.000)
Step: 79849, Reward: [-510.242 -510.242 -510.242] [96.9045], Avg: [-572.922 -572.922 -572.922] (1.000)
Step: 79899, Reward: [-459.538 -459.538 -459.538] [93.6493], Avg: [-572.91 -572.91 -572.91] (1.000)
Step: 79949, Reward: [-494.785 -494.785 -494.785] [42.6642], Avg: [-572.888 -572.888 -572.888] (1.000)
Step: 79999, Reward: [-494.623 -494.623 -494.623] [161.3988], Avg: [-572.94 -572.94 -572.94] (1.000)
Step: 80049, Reward: [-499.475 -499.475 -499.475] [96.2480], Avg: [-572.954 -572.954 -572.954] (1.000)
Step: 80099, Reward: [-499.689 -499.689 -499.689] [59.3908], Avg: [-572.945 -572.945 -572.945] (1.000)
Step: 80149, Reward: [-549.664 -549.664 -549.664] [78.3617], Avg: [-572.98 -572.98 -572.98] (1.000)
Step: 80199, Reward: [-560.002 -560.002 -560.002] [37.3608], Avg: [-572.995 -572.995 -572.995] (1.000)
Step: 80249, Reward: [-549.556 -549.556 -549.556] [64.1714], Avg: [-573.02 -573.02 -573.02] (1.000)
Step: 80299, Reward: [-463.224 -463.224 -463.224] [47.0498], Avg: [-572.981 -572.981 -572.981] (1.000)
Step: 80349, Reward: [-450.452 -450.452 -450.452] [42.0007], Avg: [-572.931 -572.931 -572.931] (1.000)
Step: 80399, Reward: [-523.595 -523.595 -523.595] [104.1454], Avg: [-572.965 -572.965 -572.965] (1.000)
Step: 80449, Reward: [-505.531 -505.531 -505.531] [64.3744], Avg: [-572.963 -572.963 -572.963] (1.000)
Step: 80499, Reward: [-501.471 -501.471 -501.471] [115.9755], Avg: [-572.991 -572.991 -572.991] (1.000)
Step: 80549, Reward: [-543.112 -543.112 -543.112] [75.5041], Avg: [-573.019 -573.019 -573.019] (1.000)
Step: 80599, Reward: [-448.43 -448.43 -448.43] [68.1766], Avg: [-572.984 -572.984 -572.984] (1.000)
Step: 80649, Reward: [-489.404 -489.404 -489.404] [88.0021], Avg: [-572.987 -572.987 -572.987] (1.000)
Step: 80699, Reward: [-433.818 -433.818 -433.818] [42.6182], Avg: [-572.927 -572.927 -572.927] (1.000)
Step: 80749, Reward: [-488.58 -488.58 -488.58] [106.9277], Avg: [-572.941 -572.941 -572.941] (1.000)
Step: 80799, Reward: [-519.69 -519.69 -519.69] [58.8732], Avg: [-572.945 -572.945 -572.945] (1.000)
Step: 80849, Reward: [-522.831 -522.831 -522.831] [77.4044], Avg: [-572.961 -572.961 -572.961] (1.000)
Step: 80899, Reward: [-473.806 -473.806 -473.806] [71.1491], Avg: [-572.944 -572.944 -572.944] (1.000)
Step: 80949, Reward: [-415.884 -415.884 -415.884] [26.6716], Avg: [-572.864 -572.864 -572.864] (1.000)
Step: 80999, Reward: [-421.754 -421.754 -421.754] [73.5902], Avg: [-572.816 -572.816 -572.816] (1.000)
Step: 81049, Reward: [-496.147 -496.147 -496.147] [68.6400], Avg: [-572.811 -572.811 -572.811] (1.000)
Step: 81099, Reward: [-523.263 -523.263 -523.263] [38.9382], Avg: [-572.804 -572.804 -572.804] (1.000)
Step: 81149, Reward: [-505.141 -505.141 -505.141] [56.6703], Avg: [-572.797 -572.797 -572.797] (1.000)
Step: 81199, Reward: [-476.227 -476.227 -476.227] [138.9235], Avg: [-572.824 -572.824 -572.824] (1.000)
Step: 81249, Reward: [-475.509 -475.509 -475.509] [93.9187], Avg: [-572.821 -572.821 -572.821] (1.000)
Step: 81299, Reward: [-507.19 -507.19 -507.19] [145.7394], Avg: [-572.871 -572.871 -572.871] (1.000)
Step: 81349, Reward: [-474.174 -474.174 -474.174] [78.7334], Avg: [-572.858 -572.858 -572.858] (1.000)
Step: 81399, Reward: [-489.704 -489.704 -489.704] [71.7113], Avg: [-572.851 -572.851 -572.851] (1.000)
Step: 81449, Reward: [-481.449 -481.449 -481.449] [56.7753], Avg: [-572.83 -572.83 -572.83] (1.000)
Step: 81499, Reward: [-483.863 -483.863 -483.863] [87.7839], Avg: [-572.829 -572.829 -572.829] (1.000)
Step: 81549, Reward: [-435.892 -435.892 -435.892] [44.5942], Avg: [-572.773 -572.773 -572.773] (1.000)
Step: 81599, Reward: [-460.012 -460.012 -460.012] [48.9871], Avg: [-572.734 -572.734 -572.734] (1.000)
Step: 81649, Reward: [-484.957 -484.957 -484.957] [90.1276], Avg: [-572.735 -572.735 -572.735] (1.000)
Step: 81699, Reward: [-494.018 -494.018 -494.018] [65.9369], Avg: [-572.727 -572.727 -572.727] (1.000)
Step: 81749, Reward: [-515.759 -515.759 -515.759] [101.7446], Avg: [-572.755 -572.755 -572.755] (1.000)
Step: 81799, Reward: [-505.811 -505.811 -505.811] [84.4344], Avg: [-572.765 -572.765 -572.765] (1.000)
Step: 81849, Reward: [-480.656 -480.656 -480.656] [65.1860], Avg: [-572.749 -572.749 -572.749] (1.000)
Step: 81899, Reward: [-436.526 -436.526 -436.526] [39.8360], Avg: [-572.69 -572.69 -572.69] (1.000)
Step: 81949, Reward: [-571.793 -571.793 -571.793] [87.8704], Avg: [-572.743 -572.743 -572.743] (1.000)
Step: 81999, Reward: [-494.391 -494.391 -494.391] [95.2027], Avg: [-572.753 -572.753 -572.753] (1.000)
Step: 82049, Reward: [-554.529 -554.529 -554.529] [136.7251], Avg: [-572.826 -572.826 -572.826] (1.000)
Step: 82099, Reward: [-464.916 -464.916 -464.916] [53.7453], Avg: [-572.793 -572.793 -572.793] (1.000)
Step: 82149, Reward: [-429.157 -429.157 -429.157] [69.1310], Avg: [-572.747 -572.747 -572.747] (1.000)
Step: 82199, Reward: [-520.14 -520.14 -520.14] [87.7123], Avg: [-572.769 -572.769 -572.769] (1.000)
Step: 82249, Reward: [-597.953 -597.953 -597.953] [73.1766], Avg: [-572.828 -572.828 -572.828] (1.000)
Step: 82299, Reward: [-501.241 -501.241 -501.241] [112.5381], Avg: [-572.853 -572.853 -572.853] (1.000)
Step: 82349, Reward: [-536.363 -536.363 -536.363] [81.6759], Avg: [-572.881 -572.881 -572.881] (1.000)
Step: 82399, Reward: [-518.878 -518.878 -518.878] [112.3944], Avg: [-572.916 -572.916 -572.916] (1.000)
Step: 82449, Reward: [-568.43 -568.43 -568.43] [119.8208], Avg: [-572.986 -572.986 -572.986] (1.000)
Step: 82499, Reward: [-495.728 -495.728 -495.728] [102.5200], Avg: [-573.001 -573.001 -573.001] (1.000)
Step: 82549, Reward: [-504.011 -504.011 -504.011] [120.8468], Avg: [-573.033 -573.033 -573.033] (1.000)
Step: 82599, Reward: [-523.74 -523.74 -523.74] [89.0068], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 82649, Reward: [-544.51 -544.51 -544.51] [155.3320], Avg: [-573.134 -573.134 -573.134] (1.000)
Step: 82699, Reward: [-534.375 -534.375 -534.375] [55.6395], Avg: [-573.144 -573.144 -573.144] (1.000)
Step: 82749, Reward: [-434.018 -434.018 -434.018] [78.7469], Avg: [-573.107 -573.107 -573.107] (1.000)
Step: 82799, Reward: [-491.667 -491.667 -491.667] [64.9055], Avg: [-573.097 -573.097 -573.097] (1.000)
Step: 82849, Reward: [-497.501 -497.501 -497.501] [117.7957], Avg: [-573.123 -573.123 -573.123] (1.000)
Step: 82899, Reward: [-460.639 -460.639 -460.639] [72.6842], Avg: [-573.099 -573.099 -573.099] (1.000)
Step: 82949, Reward: [-504.157 -504.157 -504.157] [103.8618], Avg: [-573.12 -573.12 -573.12] (1.000)
Step: 82999, Reward: [-446.478 -446.478 -446.478] [37.1013], Avg: [-573.066 -573.066 -573.066] (1.000)
Step: 83049, Reward: [-562.902 -562.902 -562.902] [103.1786], Avg: [-573.122 -573.122 -573.122] (1.000)
Step: 83099, Reward: [-569.661 -569.661 -569.661] [167.0084], Avg: [-573.22 -573.22 -573.22] (1.000)
Step: 83149, Reward: [-494.513 -494.513 -494.513] [77.7592], Avg: [-573.22 -573.22 -573.22] (1.000)
Step: 83199, Reward: [-465.08 -465.08 -465.08] [58.5306], Avg: [-573.19 -573.19 -573.19] (1.000)
Step: 83249, Reward: [-521.073 -521.073 -521.073] [47.9396], Avg: [-573.187 -573.187 -573.187] (1.000)
Step: 83299, Reward: [-536.673 -536.673 -536.673] [35.4685], Avg: [-573.187 -573.187 -573.187] (1.000)
Step: 83349, Reward: [-543.987 -543.987 -543.987] [108.6697], Avg: [-573.234 -573.234 -573.234] (1.000)
Step: 83399, Reward: [-542.115 -542.115 -542.115] [81.9126], Avg: [-573.265 -573.265 -573.265] (1.000)
Step: 83449, Reward: [-449.162 -449.162 -449.162] [31.9020], Avg: [-573.21 -573.21 -573.21] (1.000)
Step: 83499, Reward: [-459.56 -459.56 -459.56] [45.9085], Avg: [-573.169 -573.169 -573.169] (1.000)
Step: 83549, Reward: [-431.728 -431.728 -431.728] [58.0671], Avg: [-573.119 -573.119 -573.119] (1.000)
Step: 83599, Reward: [-474.775 -474.775 -474.775] [95.0359], Avg: [-573.117 -573.117 -573.117] (1.000)
Step: 83649, Reward: [-456.777 -456.777 -456.777] [65.0971], Avg: [-573.087 -573.087 -573.087] (1.000)
Step: 83699, Reward: [-480.005 -480.005 -480.005] [43.2314], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 83749, Reward: [-485.711 -485.711 -485.711] [99.1018], Avg: [-573.064 -573.064 -573.064] (1.000)
Step: 83799, Reward: [-472.685 -472.685 -472.685] [77.4773], Avg: [-573.05 -573.05 -573.05] (1.000)
Step: 83849, Reward: [-513.575 -513.575 -513.575] [85.7672], Avg: [-573.066 -573.066 -573.066] (1.000)
Step: 83899, Reward: [-518.125 -518.125 -518.125] [86.7913], Avg: [-573.085 -573.085 -573.085] (1.000)
Step: 83949, Reward: [-561.568 -561.568 -561.568] [81.1794], Avg: [-573.126 -573.126 -573.126] (1.000)
Step: 83999, Reward: [-513.257 -513.257 -513.257] [30.0977], Avg: [-573.109 -573.109 -573.109] (1.000)
Step: 84049, Reward: [-468.441 -468.441 -468.441] [44.6655], Avg: [-573.073 -573.073 -573.073] (1.000)
Step: 84099, Reward: [-571.81 -571.81 -571.81] [93.6716], Avg: [-573.128 -573.128 -573.128] (1.000)
Step: 84149, Reward: [-544.553 -544.553 -544.553] [39.0543], Avg: [-573.134 -573.134 -573.134] (1.000)
Step: 84199, Reward: [-456.726 -456.726 -456.726] [60.6108], Avg: [-573.101 -573.101 -573.101] (1.000)
Step: 84249, Reward: [-456.522 -456.522 -456.522] [41.8931], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 84299, Reward: [-411.153 -411.153 -411.153] [45.3953], Avg: [-572.988 -572.988 -572.988] (1.000)
Step: 84349, Reward: [-490.937 -490.937 -490.937] [44.9876], Avg: [-572.966 -572.966 -572.966] (1.000)
Step: 84399, Reward: [-469.132 -469.132 -469.132] [54.5954], Avg: [-572.936 -572.936 -572.936] (1.000)
Step: 84449, Reward: [-512.618 -512.618 -512.618] [61.5768], Avg: [-572.937 -572.937 -572.937] (1.000)
Step: 84499, Reward: [-457.227 -457.227 -457.227] [32.7523], Avg: [-572.888 -572.888 -572.888] (1.000)
Step: 84549, Reward: [-491.346 -491.346 -491.346] [78.9569], Avg: [-572.887 -572.887 -572.887] (1.000)
Step: 84599, Reward: [-474.047 -474.047 -474.047] [62.0787], Avg: [-572.865 -572.865 -572.865] (1.000)
Step: 84649, Reward: [-474.546 -474.546 -474.546] [97.1169], Avg: [-572.864 -572.864 -572.864] (1.000)
Step: 84699, Reward: [-493.359 -493.359 -493.359] [104.7864], Avg: [-572.879 -572.879 -572.879] (1.000)
Step: 84749, Reward: [-424.4 -424.4 -424.4] [39.4347], Avg: [-572.815 -572.815 -572.815] (1.000)
Step: 84799, Reward: [-450.226 -450.226 -450.226] [9.3837], Avg: [-572.748 -572.748 -572.748] (1.000)
Step: 84849, Reward: [-605.406 -605.406 -605.406] [105.3938], Avg: [-572.829 -572.829 -572.829] (1.000)
Step: 84899, Reward: [-443.904 -443.904 -443.904] [64.6996], Avg: [-572.791 -572.791 -572.791] (1.000)
Step: 84949, Reward: [-531.895 -531.895 -531.895] [64.7533], Avg: [-572.806 -572.806 -572.806] (1.000)
Step: 84999, Reward: [-476.722 -476.722 -476.722] [37.1042], Avg: [-572.771 -572.771 -572.771] (1.000)
Step: 85049, Reward: [-520.305 -520.305 -520.305] [171.2084], Avg: [-572.841 -572.841 -572.841] (1.000)
Step: 85099, Reward: [-450.771 -450.771 -450.771] [49.9102], Avg: [-572.798 -572.798 -572.798] (1.000)
Step: 85149, Reward: [-581.258 -581.258 -581.258] [101.7306], Avg: [-572.863 -572.863 -572.863] (1.000)
Step: 85199, Reward: [-408.911 -408.911 -408.911] [52.5043], Avg: [-572.798 -572.798 -572.798] (1.000)
Step: 85249, Reward: [-488.712 -488.712 -488.712] [110.2074], Avg: [-572.813 -572.813 -572.813] (1.000)
Step: 85299, Reward: [-547.572 -547.572 -547.572] [174.4930], Avg: [-572.9 -572.9 -572.9] (1.000)
Step: 85349, Reward: [-566.446 -566.446 -566.446] [128.5327], Avg: [-572.972 -572.972 -572.972] (1.000)
Step: 85399, Reward: [-504.296 -504.296 -504.296] [91.2500], Avg: [-572.985 -572.985 -572.985] (1.000)
Step: 85449, Reward: [-503.155 -503.155 -503.155] [34.0833], Avg: [-572.964 -572.964 -572.964] (1.000)
Step: 85499, Reward: [-437.469 -437.469 -437.469] [70.4537], Avg: [-572.926 -572.926 -572.926] (1.000)
Step: 85549, Reward: [-493.179 -493.179 -493.179] [37.6161], Avg: [-572.901 -572.901 -572.901] (1.000)
Step: 85599, Reward: [-454.888 -454.888 -454.888] [81.0216], Avg: [-572.88 -572.88 -572.88] (1.000)
Step: 85649, Reward: [-496.2 -496.2 -496.2] [93.5904], Avg: [-572.89 -572.89 -572.89] (1.000)
Step: 85699, Reward: [-465.959 -465.959 -465.959] [82.7245], Avg: [-572.876 -572.876 -572.876] (1.000)
Step: 85749, Reward: [-455.747 -455.747 -455.747] [63.6949], Avg: [-572.844 -572.844 -572.844] (1.000)
Step: 85799, Reward: [-466.032 -466.032 -466.032] [50.4642], Avg: [-572.812 -572.812 -572.812] (1.000)
Step: 85849, Reward: [-559.964 -559.964 -559.964] [81.8703], Avg: [-572.852 -572.852 -572.852] (1.000)
Step: 85899, Reward: [-470.707 -470.707 -470.707] [73.5432], Avg: [-572.835 -572.835 -572.835] (1.000)
Step: 85949, Reward: [-476.043 -476.043 -476.043] [103.6493], Avg: [-572.839 -572.839 -572.839] (1.000)
Step: 85999, Reward: [-477.715 -477.715 -477.715] [122.6811], Avg: [-572.855 -572.855 -572.855] (1.000)
Step: 86049, Reward: [-412.684 -412.684 -412.684] [63.4440], Avg: [-572.799 -572.799 -572.799] (1.000)
Step: 86099, Reward: [-513.363 -513.363 -513.363] [101.6409], Avg: [-572.824 -572.824 -572.824] (1.000)
Step: 86149, Reward: [-450.241 -450.241 -450.241] [27.7775], Avg: [-572.768 -572.768 -572.768] (1.000)
Step: 86199, Reward: [-510.164 -510.164 -510.164] [127.4581], Avg: [-572.806 -572.806 -572.806] (1.000)
Step: 86249, Reward: [-458.991 -458.991 -458.991] [66.1073], Avg: [-572.778 -572.778 -572.778] (1.000)
Step: 86299, Reward: [-519.591 -519.591 -519.591] [69.3869], Avg: [-572.788 -572.788 -572.788] (1.000)
Step: 86349, Reward: [-473.684 -473.684 -473.684] [52.9637], Avg: [-572.761 -572.761 -572.761] (1.000)
Step: 86399, Reward: [-532.862 -532.862 -532.862] [48.4241], Avg: [-572.766 -572.766 -572.766] (1.000)
Step: 86449, Reward: [-443.285 -443.285 -443.285] [29.5607], Avg: [-572.708 -572.708 -572.708] (1.000)
Step: 86499, Reward: [-455.65 -455.65 -455.65] [36.4612], Avg: [-572.662 -572.662 -572.662] (1.000)
Step: 86549, Reward: [-474.513 -474.513 -474.513] [52.1044], Avg: [-572.635 -572.635 -572.635] (1.000)
Step: 86599, Reward: [-544.111 -544.111 -544.111] [69.9733], Avg: [-572.659 -572.659 -572.659] (1.000)
Step: 86649, Reward: [-452.85 -452.85 -452.85] [65.4330], Avg: [-572.628 -572.628 -572.628] (1.000)
Step: 86699, Reward: [-492.34 -492.34 -492.34] [95.8622], Avg: [-572.637 -572.637 -572.637] (1.000)
Step: 86749, Reward: [-475.341 -475.341 -475.341] [14.8983], Avg: [-572.589 -572.589 -572.589] (1.000)
Step: 86799, Reward: [-558.447 -558.447 -558.447] [92.4288], Avg: [-572.634 -572.634 -572.634] (1.000)
Step: 86849, Reward: [-595.955 -595.955 -595.955] [33.2343], Avg: [-572.667 -572.667 -572.667] (1.000)
Step: 86899, Reward: [-429.337 -429.337 -429.337] [58.6300], Avg: [-572.618 -572.618 -572.618] (1.000)
Step: 86949, Reward: [-469.291 -469.291 -469.291] [68.8736], Avg: [-572.598 -572.598 -572.598] (1.000)
Step: 86999, Reward: [-550.754 -550.754 -550.754] [131.5373], Avg: [-572.661 -572.661 -572.661] (1.000)
Step: 87049, Reward: [-537.992 -537.992 -537.992] [31.4238], Avg: [-572.659 -572.659 -572.659] (1.000)
Step: 87099, Reward: [-515.436 -515.436 -515.436] [101.0561], Avg: [-572.685 -572.685 -572.685] (1.000)
Step: 87149, Reward: [-541.229 -541.229 -541.229] [67.7678], Avg: [-572.705 -572.705 -572.705] (1.000)
Step: 87199, Reward: [-474.303 -474.303 -474.303] [115.9855], Avg: [-572.715 -572.715 -572.715] (1.000)
Step: 87249, Reward: [-448.573 -448.573 -448.573] [23.1629], Avg: [-572.658 -572.658 -572.658] (1.000)
Step: 87299, Reward: [-454.521 -454.521 -454.521] [87.2657], Avg: [-572.64 -572.64 -572.64] (1.000)
Step: 87349, Reward: [-539.371 -539.371 -539.371] [45.2716], Avg: [-572.647 -572.647 -572.647] (1.000)
Step: 87399, Reward: [-483.942 -483.942 -483.942] [72.2338], Avg: [-572.637 -572.637 -572.637] (1.000)
Step: 87449, Reward: [-576.704 -576.704 -576.704] [181.3012], Avg: [-572.743 -572.743 -572.743] (1.000)
Step: 87499, Reward: [-494.865 -494.865 -494.865] [53.2767], Avg: [-572.729 -572.729 -572.729] (1.000)
Step: 87549, Reward: [-563.559 -563.559 -563.559] [94.6572], Avg: [-572.778 -572.778 -572.778] (1.000)
Step: 87599, Reward: [-513.545 -513.545 -513.545] [140.0984], Avg: [-572.824 -572.824 -572.824] (1.000)
Step: 87649, Reward: [-498.634 -498.634 -498.634] [94.6084], Avg: [-572.836 -572.836 -572.836] (1.000)
Step: 87699, Reward: [-492.685 -492.685 -492.685] [141.1451], Avg: [-572.871 -572.871 -572.871] (1.000)
Step: 87749, Reward: [-500.247 -500.247 -500.247] [120.7684], Avg: [-572.898 -572.898 -572.898] (1.000)
Step: 87799, Reward: [-628.438 -628.438 -628.438] [248.4764], Avg: [-573.071 -573.071 -573.071] (1.000)
Step: 87849, Reward: [-505.497 -505.497 -505.497] [55.1705], Avg: [-573.064 -573.064 -573.064] (1.000)
Step: 87899, Reward: [-589.844 -589.844 -589.844] [131.4080], Avg: [-573.148 -573.148 -573.148] (1.000)
Step: 87949, Reward: [-500.665 -500.665 -500.665] [64.7405], Avg: [-573.144 -573.144 -573.144] (1.000)
Step: 87999, Reward: [-537.511 -537.511 -537.511] [39.3053], Avg: [-573.146 -573.146 -573.146] (1.000)
Step: 88049, Reward: [-538.021 -538.021 -538.021] [57.1803], Avg: [-573.159 -573.159 -573.159] (1.000)
Step: 88099, Reward: [-575.745 -575.745 -575.745] [88.7999], Avg: [-573.211 -573.211 -573.211] (1.000)
Step: 88149, Reward: [-524.845 -524.845 -524.845] [94.0719], Avg: [-573.236 -573.236 -573.236] (1.000)
Step: 88199, Reward: [-540.983 -540.983 -540.983] [165.2701], Avg: [-573.312 -573.312 -573.312] (1.000)
Step: 88249, Reward: [-487.2 -487.2 -487.2] [72.3287], Avg: [-573.304 -573.304 -573.304] (1.000)
Step: 88299, Reward: [-436.365 -436.365 -436.365] [44.3592], Avg: [-573.252 -573.252 -573.252] (1.000)
Step: 88349, Reward: [-455.117 -455.117 -455.117] [37.2458], Avg: [-573.206 -573.206 -573.206] (1.000)
Step: 88399, Reward: [-470.196 -470.196 -470.196] [98.8440], Avg: [-573.204 -573.204 -573.204] (1.000)
Step: 88449, Reward: [-461.709 -461.709 -461.709] [73.9390], Avg: [-573.182 -573.182 -573.182] (1.000)
Step: 88499, Reward: [-462.354 -462.354 -462.354] [64.5016], Avg: [-573.156 -573.156 -573.156] (1.000)
Step: 88549, Reward: [-526.791 -526.791 -526.791] [67.7335], Avg: [-573.168 -573.168 -573.168] (1.000)
Step: 88599, Reward: [-567.426 -567.426 -567.426] [41.5532], Avg: [-573.188 -573.188 -573.188] (1.000)
Step: 88649, Reward: [-491.352 -491.352 -491.352] [130.4154], Avg: [-573.216 -573.216 -573.216] (1.000)
Step: 88699, Reward: [-443.603 -443.603 -443.603] [61.5004], Avg: [-573.177 -573.177 -573.177] (1.000)
Step: 88749, Reward: [-463.359 -463.359 -463.359] [24.4413], Avg: [-573.129 -573.129 -573.129] (1.000)
Step: 88799, Reward: [-461.004 -461.004 -461.004] [83.1338], Avg: [-573.113 -573.113 -573.113] (1.000)
Step: 88849, Reward: [-471.509 -471.509 -471.509] [43.1588], Avg: [-573.08 -573.08 -573.08] (1.000)
Step: 88899, Reward: [-497.906 -497.906 -497.906] [92.9616], Avg: [-573.09 -573.09 -573.09] (1.000)
Step: 88949, Reward: [-453.403 -453.403 -453.403] [62.2525], Avg: [-573.058 -573.058 -573.058] (1.000)
Step: 88999, Reward: [-454.138 -454.138 -454.138] [67.0152], Avg: [-573.029 -573.029 -573.029] (1.000)
Step: 89049, Reward: [-529.209 -529.209 -529.209] [155.7879], Avg: [-573.092 -573.092 -573.092] (1.000)
Step: 89099, Reward: [-523.576 -523.576 -523.576] [129.7342], Avg: [-573.137 -573.137 -573.137] (1.000)
Step: 89149, Reward: [-477.072 -477.072 -477.072] [78.8789], Avg: [-573.127 -573.127 -573.127] (1.000)
Step: 89199, Reward: [-430.646 -430.646 -430.646] [61.1370], Avg: [-573.081 -573.081 -573.081] (1.000)
Step: 89249, Reward: [-516.879 -516.879 -516.879] [134.2042], Avg: [-573.125 -573.125 -573.125] (1.000)
Step: 89299, Reward: [-591.821 -591.821 -591.821] [149.8252], Avg: [-573.219 -573.219 -573.219] (1.000)
Step: 89349, Reward: [-451.991 -451.991 -451.991] [38.3201], Avg: [-573.173 -573.173 -573.173] (1.000)
Step: 89399, Reward: [-518.913 -518.913 -518.913] [121.1059], Avg: [-573.21 -573.21 -573.21] (1.000)
Step: 89449, Reward: [-544.407 -544.407 -544.407] [105.5992], Avg: [-573.253 -573.253 -573.253] (1.000)
Step: 89499, Reward: [-483.479 -483.479 -483.479] [65.5776], Avg: [-573.24 -573.24 -573.24] (1.000)
Step: 89549, Reward: [-427.3 -427.3 -427.3] [55.8641], Avg: [-573.189 -573.189 -573.189] (1.000)
Step: 89599, Reward: [-439.346 -439.346 -439.346] [65.6658], Avg: [-573.151 -573.151 -573.151] (1.000)
Step: 89649, Reward: [-524.463 -524.463 -524.463] [121.6836], Avg: [-573.192 -573.192 -573.192] (1.000)
Step: 89699, Reward: [-496.67 -496.67 -496.67] [77.5328], Avg: [-573.193 -573.193 -573.193] (1.000)
Step: 89749, Reward: [-576.09 -576.09 -576.09] [55.2433], Avg: [-573.225 -573.225 -573.225] (1.000)
Step: 89799, Reward: [-418.85 -418.85 -418.85] [44.5723], Avg: [-573.164 -573.164 -573.164] (1.000)
Step: 89849, Reward: [-453.123 -453.123 -453.123] [61.9208], Avg: [-573.132 -573.132 -573.132] (1.000)
Step: 89899, Reward: [-489.061 -489.061 -489.061] [63.4323], Avg: [-573.12 -573.12 -573.12] (1.000)
Step: 89949, Reward: [-524.322 -524.322 -524.322] [33.3593], Avg: [-573.112 -573.112 -573.112] (1.000)
Step: 89999, Reward: [-470.823 -470.823 -470.823] [52.5166], Avg: [-573.084 -573.084 -573.084] (1.000)
Step: 90049, Reward: [-554.54 -554.54 -554.54] [111.3240], Avg: [-573.135 -573.135 -573.135] (1.000)
Step: 90099, Reward: [-525.282 -525.282 -525.282] [57.0201], Avg: [-573.14 -573.14 -573.14] (1.000)
Step: 90149, Reward: [-426.721 -426.721 -426.721] [74.9410], Avg: [-573.101 -573.101 -573.101] (1.000)
Step: 90199, Reward: [-483.404 -483.404 -483.404] [105.2924], Avg: [-573.109 -573.109 -573.109] (1.000)
Step: 90249, Reward: [-500.37 -500.37 -500.37] [78.1773], Avg: [-573.113 -573.113 -573.113] (1.000)
Step: 90299, Reward: [-469.322 -469.322 -469.322] [13.1223], Avg: [-573.062 -573.062 -573.062] (1.000)
Step: 90349, Reward: [-457.767 -457.767 -457.767] [90.8001], Avg: [-573.049 -573.049 -573.049] (1.000)
Step: 90399, Reward: [-478.538 -478.538 -478.538] [46.6193], Avg: [-573.022 -573.022 -573.022] (1.000)
Step: 90449, Reward: [-477.578 -477.578 -477.578] [21.0650], Avg: [-572.981 -572.981 -572.981] (1.000)
Step: 90499, Reward: [-517.135 -517.135 -517.135] [112.2960], Avg: [-573.012 -573.012 -573.012] (1.000)
Step: 90549, Reward: [-486.155 -486.155 -486.155] [85.5090], Avg: [-573.012 -573.012 -573.012] (1.000)
Step: 90599, Reward: [-524.529 -524.529 -524.529] [155.9449], Avg: [-573.071 -573.071 -573.071] (1.000)
Step: 90649, Reward: [-492.496 -492.496 -492.496] [71.8277], Avg: [-573.066 -573.066 -573.066] (1.000)
Step: 90699, Reward: [-491.744 -491.744 -491.744] [42.7177], Avg: [-573.045 -573.045 -573.045] (1.000)
Step: 90749, Reward: [-505.007 -505.007 -505.007] [151.1528], Avg: [-573.091 -573.091 -573.091] (1.000)
Step: 90799, Reward: [-540.586 -540.586 -540.586] [37.9799], Avg: [-573.094 -573.094 -573.094] (1.000)
Step: 90849, Reward: [-447.102 -447.102 -447.102] [59.8489], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 90899, Reward: [-449.77 -449.77 -449.77] [116.5570], Avg: [-573.053 -573.053 -573.053] (1.000)
Step: 90949, Reward: [-455.26 -455.26 -455.26] [51.3072], Avg: [-573.017 -573.017 -573.017] (1.000)
Step: 90999, Reward: [-518.913 -518.913 -518.913] [109.6921], Avg: [-573.047 -573.047 -573.047] (1.000)
Step: 91049, Reward: [-510.919 -510.919 -510.919] [99.4115], Avg: [-573.068 -573.068 -573.068] (1.000)
Step: 91099, Reward: [-501.121 -501.121 -501.121] [80.6203], Avg: [-573.073 -573.073 -573.073] (1.000)
Step: 91149, Reward: [-431.751 -431.751 -431.751] [50.0760], Avg: [-573.023 -573.023 -573.023] (1.000)
Step: 91199, Reward: [-497.631 -497.631 -497.631] [137.6163], Avg: [-573.057 -573.057 -573.057] (1.000)
Step: 91249, Reward: [-466.862 -466.862 -466.862] [80.9428], Avg: [-573.043 -573.043 -573.043] (1.000)
Step: 91299, Reward: [-507.581 -507.581 -507.581] [130.3479], Avg: [-573.078 -573.078 -573.078] (1.000)
Step: 91349, Reward: [-485.675 -485.675 -485.675] [86.9231], Avg: [-573.078 -573.078 -573.078] (1.000)
Step: 91399, Reward: [-467.583 -467.583 -467.583] [54.3412], Avg: [-573.05 -573.05 -573.05] (1.000)
Step: 91449, Reward: [-466.152 -466.152 -466.152] [46.7089], Avg: [-573.017 -573.017 -573.017] (1.000)
Step: 91499, Reward: [-430.652 -430.652 -430.652] [82.1736], Avg: [-572.984 -572.984 -572.984] (1.000)
Step: 91549, Reward: [-573.66 -573.66 -573.66] [138.8956], Avg: [-573.061 -573.061 -573.061] (1.000)
Step: 91599, Reward: [-484.515 -484.515 -484.515] [89.8253], Avg: [-573.061 -573.061 -573.061] (1.000)
Step: 91649, Reward: [-444.424 -444.424 -444.424] [14.4420], Avg: [-572.999 -572.999 -572.999] (1.000)
Step: 91699, Reward: [-464.677 -464.677 -464.677] [86.7232], Avg: [-572.987 -572.987 -572.987] (1.000)
Step: 91749, Reward: [-518.145 -518.145 -518.145] [88.7935], Avg: [-573.006 -573.006 -573.006] (1.000)
Step: 91799, Reward: [-456.399 -456.399 -456.399] [60.3321], Avg: [-572.975 -572.975 -572.975] (1.000)
Step: 91849, Reward: [-526.674 -526.674 -526.674] [80.5299], Avg: [-572.994 -572.994 -572.994] (1.000)
Step: 91899, Reward: [-442.052 -442.052 -442.052] [28.1427], Avg: [-572.938 -572.938 -572.938] (1.000)
Step: 91949, Reward: [-433.695 -433.695 -433.695] [83.5421], Avg: [-572.908 -572.908 -572.908] (1.000)
Step: 91999, Reward: [-467.002 -467.002 -467.002] [69.7300], Avg: [-572.888 -572.888 -572.888] (1.000)
Step: 92049, Reward: [-447.079 -447.079 -447.079] [20.2439], Avg: [-572.831 -572.831 -572.831] (1.000)
Step: 92099, Reward: [-454.501 -454.501 -454.501] [65.7903], Avg: [-572.802 -572.802 -572.802] (1.000)
Step: 92149, Reward: [-460.993 -460.993 -460.993] [43.0943], Avg: [-572.765 -572.765 -572.765] (1.000)
Step: 92199, Reward: [-599.489 -599.489 -599.489] [173.2674], Avg: [-572.873 -572.873 -572.873] (1.000)
Step: 92249, Reward: [-520.285 -520.285 -520.285] [52.4246], Avg: [-572.873 -572.873 -572.873] (1.000)
Step: 92299, Reward: [-445.048 -445.048 -445.048] [64.8453], Avg: [-572.839 -572.839 -572.839] (1.000)
Step: 92349, Reward: [-537.342 -537.342 -537.342] [166.0884], Avg: [-572.91 -572.91 -572.91] (1.000)
Step: 92399, Reward: [-498.271 -498.271 -498.271] [83.5315], Avg: [-572.915 -572.915 -572.915] (1.000)
Step: 92449, Reward: [-434.117 -434.117 -434.117] [34.0901], Avg: [-572.858 -572.858 -572.858] (1.000)
Step: 92499, Reward: [-530.492 -530.492 -530.492] [88.4651], Avg: [-572.883 -572.883 -572.883] (1.000)
Step: 92549, Reward: [-436.032 -436.032 -436.032] [78.4878], Avg: [-572.851 -572.851 -572.851] (1.000)
Step: 92599, Reward: [-510.531 -510.531 -510.531] [110.6533], Avg: [-572.877 -572.877 -572.877] (1.000)
Step: 92649, Reward: [-520.666 -520.666 -520.666] [106.0771], Avg: [-572.906 -572.906 -572.906] (1.000)
Step: 92699, Reward: [-579.819 -579.819 -579.819] [257.6412], Avg: [-573.049 -573.049 -573.049] (1.000)
Step: 92749, Reward: [-524.142 -524.142 -524.142] [166.5968], Avg: [-573.113 -573.113 -573.113] (1.000)
Step: 92799, Reward: [-503.084 -503.084 -503.084] [82.5953], Avg: [-573.119 -573.119 -573.119] (1.000)
Step: 92849, Reward: [-494.646 -494.646 -494.646] [92.2568], Avg: [-573.127 -573.127 -573.127] (1.000)
Step: 92899, Reward: [-594.162 -594.162 -594.162] [144.0773], Avg: [-573.216 -573.216 -573.216] (1.000)
Step: 92949, Reward: [-486.287 -486.287 -486.287] [141.6888], Avg: [-573.245 -573.245 -573.245] (1.000)
Step: 92999, Reward: [-562.013 -562.013 -562.013] [109.5227], Avg: [-573.298 -573.298 -573.298] (1.000)
Step: 93049, Reward: [-431.677 -431.677 -431.677] [75.3510], Avg: [-573.262 -573.262 -573.262] (1.000)
Step: 93099, Reward: [-493.31 -493.31 -493.31] [31.8533], Avg: [-573.236 -573.236 -573.236] (1.000)
Step: 93149, Reward: [-488.457 -488.457 -488.457] [80.7910], Avg: [-573.234 -573.234 -573.234] (1.000)
Step: 93199, Reward: [-447.972 -447.972 -447.972] [52.1952], Avg: [-573.195 -573.195 -573.195] (1.000)
Step: 93249, Reward: [-548.448 -548.448 -548.448] [101.0308], Avg: [-573.236 -573.236 -573.236] (1.000)
Step: 93299, Reward: [-506.861 -506.861 -506.861] [71.0058], Avg: [-573.239 -573.239 -573.239] (1.000)
Step: 93349, Reward: [-555.924 -555.924 -555.924] [132.3187], Avg: [-573.3 -573.3 -573.3] (1.000)
Step: 93399, Reward: [-589.508 -589.508 -589.508] [131.0355], Avg: [-573.379 -573.379 -573.379] (1.000)
Step: 93449, Reward: [-441.161 -441.161 -441.161] [62.1816], Avg: [-573.341 -573.341 -573.341] (1.000)
Step: 93499, Reward: [-476.05 -476.05 -476.05] [119.6898], Avg: [-573.353 -573.353 -573.353] (1.000)
Step: 93549, Reward: [-521.171 -521.171 -521.171] [77.7292], Avg: [-573.367 -573.367 -573.367] (1.000)
Step: 93599, Reward: [-544.608 -544.608 -544.608] [113.3428], Avg: [-573.412 -573.412 -573.412] (1.000)
Step: 93649, Reward: [-568.207 -568.207 -568.207] [80.0176], Avg: [-573.452 -573.452 -573.452] (1.000)
Step: 93699, Reward: [-473.19 -473.19 -473.19] [47.8739], Avg: [-573.424 -573.424 -573.424] (1.000)
Step: 93749, Reward: [-527.881 -527.881 -527.881] [157.0948], Avg: [-573.484 -573.484 -573.484] (1.000)
Step: 93799, Reward: [-513.283 -513.283 -513.283] [151.5703], Avg: [-573.532 -573.532 -573.532] (1.000)
Step: 93849, Reward: [-510.425 -510.425 -510.425] [161.5028], Avg: [-573.585 -573.585 -573.585] (1.000)
Step: 93899, Reward: [-452.636 -452.636 -452.636] [30.3225], Avg: [-573.537 -573.537 -573.537] (1.000)
Step: 93949, Reward: [-564.394 -564.394 -564.394] [36.2806], Avg: [-573.551 -573.551 -573.551] (1.000)
Step: 93999, Reward: [-485.402 -485.402 -485.402] [29.7939], Avg: [-573.52 -573.52 -573.52] (1.000)
Step: 94049, Reward: [-539.594 -539.594 -539.594] [113.4610], Avg: [-573.562 -573.562 -573.562] (1.000)
Step: 94099, Reward: [-477.848 -477.848 -477.848] [80.7610], Avg: [-573.554 -573.554 -573.554] (1.000)
Step: 94149, Reward: [-463.827 -463.827 -463.827] [91.9358], Avg: [-573.545 -573.545 -573.545] (1.000)
Step: 94199, Reward: [-444.196 -444.196 -444.196] [91.1058], Avg: [-573.525 -573.525 -573.525] (1.000)
Step: 94249, Reward: [-570.989 -570.989 -570.989] [96.4632], Avg: [-573.574 -573.574 -573.574] (1.000)
Step: 94299, Reward: [-446.076 -446.076 -446.076] [56.3202], Avg: [-573.537 -573.537 -573.537] (1.000)
Step: 94349, Reward: [-586.738 -586.738 -586.738] [105.8674], Avg: [-573.6 -573.6 -573.6] (1.000)
Step: 94399, Reward: [-487.88 -487.88 -487.88] [103.5827], Avg: [-573.609 -573.609 -573.609] (1.000)
Step: 94449, Reward: [-481.854 -481.854 -481.854] [55.1015], Avg: [-573.59 -573.59 -573.59] (1.000)
Step: 94499, Reward: [-473.572 -473.572 -473.572] [50.5003], Avg: [-573.564 -573.564 -573.564] (1.000)
Step: 94549, Reward: [-560.157 -560.157 -560.157] [160.8748], Avg: [-573.642 -573.642 -573.642] (1.000)
Step: 94599, Reward: [-425.419 -425.419 -425.419] [64.9729], Avg: [-573.598 -573.598 -573.598] (1.000)
Step: 94649, Reward: [-464.734 -464.734 -464.734] [37.7275], Avg: [-573.56 -573.56 -573.56] (1.000)
Step: 94699, Reward: [-502.107 -502.107 -502.107] [85.2321], Avg: [-573.567 -573.567 -573.567] (1.000)
Step: 94749, Reward: [-498.282 -498.282 -498.282] [94.9868], Avg: [-573.578 -573.578 -573.578] (1.000)
Step: 94799, Reward: [-531.975 -531.975 -531.975] [50.6702], Avg: [-573.583 -573.583 -573.583] (1.000)
Step: 94849, Reward: [-471.626 -471.626 -471.626] [86.7371], Avg: [-573.575 -573.575 -573.575] (1.000)
Step: 94899, Reward: [-488.811 -488.811 -488.811] [80.5839], Avg: [-573.572 -573.572 -573.572] (1.000)
Step: 94949, Reward: [-520.561 -520.561 -520.561] [95.8825], Avg: [-573.595 -573.595 -573.595] (1.000)
Step: 94999, Reward: [-455.282 -455.282 -455.282] [47.8139], Avg: [-573.558 -573.558 -573.558] (1.000)
Step: 95049, Reward: [-452.216 -452.216 -452.216] [81.9450], Avg: [-573.537 -573.537 -573.537] (1.000)
Step: 95099, Reward: [-470.589 -470.589 -470.589] [63.9979], Avg: [-573.517 -573.517 -573.517] (1.000)
Step: 95149, Reward: [-443.647 -443.647 -443.647] [51.9983], Avg: [-573.476 -573.476 -573.476] (1.000)
Step: 95199, Reward: [-477.658 -477.658 -477.658] [83.5059], Avg: [-573.469 -573.469 -573.469] (1.000)
Step: 95249, Reward: [-567.309 -567.309 -567.309] [91.4852], Avg: [-573.514 -573.514 -573.514] (1.000)
Step: 95299, Reward: [-624.643 -624.643 -624.643] [240.6576], Avg: [-573.667 -573.667 -573.667] (1.000)
Step: 95349, Reward: [-491.764 -491.764 -491.764] [68.8982], Avg: [-573.66 -573.66 -573.66] (1.000)
Step: 95399, Reward: [-513.243 -513.243 -513.243] [103.7980], Avg: [-573.683 -573.683 -573.683] (1.000)
Step: 95449, Reward: [-509.663 -509.663 -509.663] [80.2016], Avg: [-573.691 -573.691 -573.691] (1.000)
Step: 95499, Reward: [-511.205 -511.205 -511.205] [55.6816], Avg: [-573.688 -573.688 -573.688] (1.000)
Step: 95549, Reward: [-497.862 -497.862 -497.862] [105.7252], Avg: [-573.704 -573.704 -573.704] (1.000)
Step: 95599, Reward: [-510.909 -510.909 -510.909] [52.5650], Avg: [-573.698 -573.698 -573.698] (1.000)
Step: 95649, Reward: [-524.276 -524.276 -524.276] [34.8764], Avg: [-573.691 -573.691 -573.691] (1.000)
Step: 95699, Reward: [-542.547 -542.547 -542.547] [96.3664], Avg: [-573.725 -573.725 -573.725] (1.000)
Step: 95749, Reward: [-517.779 -517.779 -517.779] [79.4809], Avg: [-573.737 -573.737 -573.737] (1.000)
Step: 95799, Reward: [-465.359 -465.359 -465.359] [120.5650], Avg: [-573.743 -573.743 -573.743] (1.000)
Step: 95849, Reward: [-596.096 -596.096 -596.096] [158.6022], Avg: [-573.838 -573.838 -573.838] (1.000)
Step: 95899, Reward: [-423.688 -423.688 -423.688] [58.0156], Avg: [-573.79 -573.79 -573.79] (1.000)
Step: 95949, Reward: [-482.987 -482.987 -482.987] [58.6160], Avg: [-573.773 -573.773 -573.773] (1.000)
Step: 95999, Reward: [-481.034 -481.034 -481.034] [52.7370], Avg: [-573.752 -573.752 -573.752] (1.000)
Step: 96049, Reward: [-513.906 -513.906 -513.906] [66.1913], Avg: [-573.755 -573.755 -573.755] (1.000)
Step: 96099, Reward: [-492.446 -492.446 -492.446] [92.5005], Avg: [-573.761 -573.761 -573.761] (1.000)
Step: 96149, Reward: [-447.949 -447.949 -447.949] [77.2733], Avg: [-573.736 -573.736 -573.736] (1.000)
Step: 96199, Reward: [-396.48 -396.48 -396.48] [20.4219], Avg: [-573.654 -573.654 -573.654] (1.000)
Step: 96249, Reward: [-475.642 -475.642 -475.642] [32.7578], Avg: [-573.621 -573.621 -573.621] (1.000)
Step: 96299, Reward: [-521.077 -521.077 -521.077] [33.1695], Avg: [-573.61 -573.61 -573.61] (1.000)
Step: 96349, Reward: [-487.659 -487.659 -487.659] [76.1169], Avg: [-573.605 -573.605 -573.605] (1.000)
Step: 96399, Reward: [-482.163 -482.163 -482.163] [78.0979], Avg: [-573.598 -573.598 -573.598] (1.000)
Step: 96449, Reward: [-609.03 -609.03 -609.03] [189.7616], Avg: [-573.715 -573.715 -573.715] (1.000)
Step: 96499, Reward: [-478.531 -478.531 -478.531] [42.4220], Avg: [-573.688 -573.688 -573.688] (1.000)
Step: 96549, Reward: [-506.095 -506.095 -506.095] [82.2804], Avg: [-573.695 -573.695 -573.695] (1.000)
Step: 96599, Reward: [-562.787 -562.787 -562.787] [32.8088], Avg: [-573.707 -573.707 -573.707] (1.000)
Step: 96649, Reward: [-547.831 -547.831 -547.831] [115.0607], Avg: [-573.753 -573.753 -573.753] (1.000)
Step: 96699, Reward: [-438.394 -438.394 -438.394] [49.5362], Avg: [-573.709 -573.709 -573.709] (1.000)
Step: 96749, Reward: [-483.76 -483.76 -483.76] [79.1915], Avg: [-573.703 -573.703 -573.703] (1.000)
Step: 96799, Reward: [-477.686 -477.686 -477.686] [94.5692], Avg: [-573.702 -573.702 -573.702] (1.000)
Step: 96849, Reward: [-460.916 -460.916 -460.916] [61.2235], Avg: [-573.676 -573.676 -573.676] (1.000)
Step: 96899, Reward: [-543.433 -543.433 -543.433] [87.7404], Avg: [-573.705 -573.705 -573.705] (1.000)
Step: 96949, Reward: [-464.19 -464.19 -464.19] [100.4790], Avg: [-573.701 -573.701 -573.701] (1.000)
Step: 96999, Reward: [-518.695 -518.695 -518.695] [92.8549], Avg: [-573.72 -573.72 -573.72] (1.000)
Step: 97049, Reward: [-501.45 -501.45 -501.45] [79.6185], Avg: [-573.724 -573.724 -573.724] (1.000)
Step: 97099, Reward: [-471.685 -471.685 -471.685] [131.6793], Avg: [-573.739 -573.739 -573.739] (1.000)
Step: 97149, Reward: [-436.794 -436.794 -436.794] [67.9476], Avg: [-573.704 -573.704 -573.704] (1.000)
Step: 97199, Reward: [-480.292 -480.292 -480.292] [76.4592], Avg: [-573.695 -573.695 -573.695] (1.000)
Step: 97249, Reward: [-467.91 -467.91 -467.91] [154.4042], Avg: [-573.72 -573.72 -573.72] (1.000)
Step: 97299, Reward: [-569.113 -569.113 -569.113] [123.6340], Avg: [-573.781 -573.781 -573.781] (1.000)
Step: 97349, Reward: [-467.757 -467.757 -467.757] [101.3833], Avg: [-573.779 -573.779 -573.779] (1.000)
Step: 97399, Reward: [-519.44 -519.44 -519.44] [133.2445], Avg: [-573.819 -573.819 -573.819] (1.000)
Step: 97449, Reward: [-467.98 -467.98 -467.98] [71.3312], Avg: [-573.802 -573.802 -573.802] (1.000)
Step: 97499, Reward: [-455.722 -455.722 -455.722] [44.5126], Avg: [-573.764 -573.764 -573.764] (1.000)
Step: 97549, Reward: [-471.448 -471.448 -471.448] [47.4805], Avg: [-573.736 -573.736 -573.736] (1.000)
Step: 97599, Reward: [-468.457 -468.457 -468.457] [107.8647], Avg: [-573.737 -573.737 -573.737] (1.000)
Step: 97649, Reward: [-409.399 -409.399 -409.399] [53.9151], Avg: [-573.681 -573.681 -573.681] (1.000)
Step: 97699, Reward: [-510.755 -510.755 -510.755] [43.7780], Avg: [-573.671 -573.671 -573.671] (1.000)
Step: 97749, Reward: [-447.178 -447.178 -447.178] [78.8942], Avg: [-573.646 -573.646 -573.646] (1.000)
Step: 97799, Reward: [-554.487 -554.487 -554.487] [127.3578], Avg: [-573.702 -573.702 -573.702] (1.000)
Step: 97849, Reward: [-496.727 -496.727 -496.727] [45.5744], Avg: [-573.686 -573.686 -573.686] (1.000)
Step: 97899, Reward: [-463.945 -463.945 -463.945] [58.7917], Avg: [-573.66 -573.66 -573.66] (1.000)
Step: 97949, Reward: [-481.5 -481.5 -481.5] [57.6577], Avg: [-573.642 -573.642 -573.642] (1.000)
Step: 97999, Reward: [-539.47 -539.47 -539.47] [77.4367], Avg: [-573.664 -573.664 -573.664] (1.000)
Step: 98049, Reward: [-484.777 -484.777 -484.777] [84.3212], Avg: [-573.662 -573.662 -573.662] (1.000)
Step: 98099, Reward: [-505.476 -505.476 -505.476] [80.7350], Avg: [-573.668 -573.668 -573.668] (1.000)
Step: 98149, Reward: [-556.722 -556.722 -556.722] [151.1766], Avg: [-573.737 -573.737 -573.737] (1.000)
Step: 98199, Reward: [-553.124 -553.124 -553.124] [144.3555], Avg: [-573.8 -573.8 -573.8] (1.000)
Step: 98249, Reward: [-489.007 -489.007 -489.007] [52.8322], Avg: [-573.783 -573.783 -573.783] (1.000)
Step: 98299, Reward: [-551.611 -551.611 -551.611] [41.1563], Avg: [-573.793 -573.793 -573.793] (1.000)
Step: 98349, Reward: [-540.379 -540.379 -540.379] [47.9981], Avg: [-573.8 -573.8 -573.8] (1.000)
Step: 98399, Reward: [-455.986 -455.986 -455.986] [79.6515], Avg: [-573.781 -573.781 -573.781] (1.000)
Step: 98449, Reward: [-480.16 -480.16 -480.16] [84.9482], Avg: [-573.777 -573.777 -573.777] (1.000)
Step: 98499, Reward: [-470.541 -470.541 -470.541] [125.8424], Avg: [-573.788 -573.788 -573.788] (1.000)
Step: 98549, Reward: [-535.151 -535.151 -535.151] [100.7256], Avg: [-573.82 -573.82 -573.82] (1.000)
Step: 98599, Reward: [-569.93 -569.93 -569.93] [90.0968], Avg: [-573.863 -573.863 -573.863] (1.000)
Step: 98649, Reward: [-436.986 -436.986 -436.986] [58.1001], Avg: [-573.823 -573.823 -573.823] (1.000)
Step: 98699, Reward: [-428.465 -428.465 -428.465] [72.2665], Avg: [-573.786 -573.786 -573.786] (1.000)
Step: 98749, Reward: [-525.066 -525.066 -525.066] [124.0549], Avg: [-573.824 -573.824 -573.824] (1.000)
Step: 98799, Reward: [-450.171 -450.171 -450.171] [85.0123], Avg: [-573.805 -573.805 -573.805] (1.000)
Step: 98849, Reward: [-515.49 -515.49 -515.49] [42.4580], Avg: [-573.797 -573.797 -573.797] (1.000)
Step: 98899, Reward: [-475.716 -475.716 -475.716] [64.0268], Avg: [-573.78 -573.78 -573.78] (1.000)
Step: 98949, Reward: [-554.495 -554.495 -554.495] [73.7519], Avg: [-573.807 -573.807 -573.807] (1.000)
Step: 98999, Reward: [-514.482 -514.482 -514.482] [110.5425], Avg: [-573.833 -573.833 -573.833] (1.000)
Step: 99049, Reward: [-413.849 -413.849 -413.849] [45.2926], Avg: [-573.775 -573.775 -573.775] (1.000)
Step: 99099, Reward: [-440.38 -440.38 -440.38] [83.0650], Avg: [-573.75 -573.75 -573.75] (1.000)
Step: 99149, Reward: [-420.631 -420.631 -420.631] [81.5489], Avg: [-573.714 -573.714 -573.714] (1.000)
Step: 99199, Reward: [-505.159 -505.159 -505.159] [148.6167], Avg: [-573.754 -573.754 -573.754] (1.000)
Step: 99249, Reward: [-498.106 -498.106 -498.106] [81.3698], Avg: [-573.757 -573.757 -573.757] (1.000)
Step: 99299, Reward: [-422.618 -422.618 -422.618] [25.8475], Avg: [-573.694 -573.694 -573.694] (1.000)
Step: 99349, Reward: [-522.792 -522.792 -522.792] [106.1441], Avg: [-573.722 -573.722 -573.722] (1.000)
Step: 99399, Reward: [-447.567 -447.567 -447.567] [46.6843], Avg: [-573.682 -573.682 -573.682] (1.000)
Step: 99449, Reward: [-475.812 -475.812 -475.812] [75.6922], Avg: [-573.67 -573.67 -573.67] (1.000)
Step: 99499, Reward: [-439.968 -439.968 -439.968] [92.6914], Avg: [-573.65 -573.65 -573.65] (1.000)
Step: 99549, Reward: [-522.264 -522.264 -522.264] [134.7608], Avg: [-573.692 -573.692 -573.692] (1.000)
Step: 99599, Reward: [-459.963 -459.963 -459.963] [45.0658], Avg: [-573.657 -573.657 -573.657] (1.000)
Step: 99649, Reward: [-549.153 -549.153 -549.153] [122.2726], Avg: [-573.706 -573.706 -573.706] (1.000)
Step: 99699, Reward: [-554.03 -554.03 -554.03] [56.0527], Avg: [-573.725 -573.725 -573.725] (1.000)
Step: 99749, Reward: [-483.687 -483.687 -483.687] [88.3195], Avg: [-573.724 -573.724 -573.724] (1.000)
Step: 99799, Reward: [-501.979 -501.979 -501.979] [58.4986], Avg: [-573.717 -573.717 -573.717] (1.000)
Step: 99849, Reward: [-490.364 -490.364 -490.364] [25.6937], Avg: [-573.688 -573.688 -573.688] (1.000)
Step: 99899, Reward: [-528.65 -528.65 -528.65] [120.9321], Avg: [-573.726 -573.726 -573.726] (1.000)
Step: 99949, Reward: [-438.565 -438.565 -438.565] [29.5038], Avg: [-573.673 -573.673 -573.673] (1.000)
Step: 99999, Reward: [-525.4 -525.4 -525.4] [82.9663], Avg: [-573.691 -573.691 -573.691] (1.000)
