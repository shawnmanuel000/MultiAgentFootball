Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 4,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 4
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 32

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
# 		del self.network
# 		n_agents = len(action_size)
# 		n_actions = action_size[0][-1]
# 		n_obs = state_size[0][-1]
# 		state_len = int(np.sum([np.prod(space) for space in state_size]))
# 		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
# 		groups = {"agents": n_agents}
# 		scheme = {
# 			"state": {"vshape": state_len},
# 			"obs": {"vshape": n_obs, "group": "agents"},
# 			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
# 			"reward": {"vshape": (1,)},
# 			"done": {"vshape": (1,), "dtype": torch.uint8},
# 		}
		
# 		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
# 		self.replay_buffer = ReplayBuffer(scheme, groups, 2000, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
# 		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
# 		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
# 		self.episode_batch = self.new_episode_batch(NUM_ENVS)
# 		self.mac.init_hidden(batch_size=NUM_ENVS)
# 		self.num_envs = NUM_ENVS
# 		self.time = 0
# 		self.replay_buffer2 = MultiagentReplayBuffer(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
# 		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
# 		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
# 		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
# 		state_joint = np.concatenate(state, -1)
# 		obs = np.concatenate(state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
# 		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		self.episode_batch.update({"state": [state_joint], "obs": [obs]}, ts=self.step)
# 		actions = self.mac.select_actions(self.episode_batch, inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
# 		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
# 		return np.split(self.action, actions.size(-1), axis=-2)

# 	def train(self, state, action, next_state, reward, done):
# 		actions, rewards, dones = [list(zip(*x)) for x in [action, reward, done]]
# 		actions_one_hot = [np.argmax(a, -1) for a in actions]
# 		rewards = [np.mean(rewards, -1)]
# 		dones = [np.any(dones, -1)]
# 		obs = np.concatenate(state, -2)
# 		next_obs = np.concatenate(next_state, -2)
# 		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
# 		actor_inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
# 		post_transition_data = {"actions": actions_one_hot, "reward": rewards, "done": dones}
# 		self.replay_buffer2.add(state, action, next_state, reward, done)
# 		self.episode_batch.update(post_transition_data, ts=self.step)
# 		if np.any(done[0]):
# 			state_joint = np.concatenate(state, -1)
# 			self.episode_batch.update({"state": [state_joint], "obs": [next_obs]}, ts=self.step)
# 			actions = self.mac.select_actions(self.episode_batch, actor_inputs, t_ep=self.step, t_env=self.time, test_mode=False)
# 			self.episode_batch.update({"actions": actions}, ts=self.step)
# 			self.replay_buffer.insert_episode_batch(self.episode_batch)
# 			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
# 				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
# 				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, self.device)
# 				max_ep_t = episode_sample.max_t_filled()
# 				episode_sample = episode_sample[:, :max_ep_t]
# 				if episode_sample.device != self.device: episode_sample.to(self.device)
# 				self.learner.train(episode_sample)
# 			self.episode_batch = self.new_episode_batch(state[0].shape[0])
# 			self.mac.init_hidden(self.num_envs)
# 			self.time += self.step
# 			self.step = 0

# class OneHot():
# 	def __init__(self, out_dim):
# 		self.out_dim = out_dim

# 	def transform(self, tensor):
# 		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
# 		y_onehot.scatter_(-1, tensor.long(), 1)
# 		return y_onehot.float()

# 	def infer_output_info(self, vshape_in, dtype_in):
# 		return (self.out_dim,), torch.float32

# class COMALearner():
# 	def __init__(self, mac, scheme, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.last_target_update_step = 0
# 		self.mac = mac
# 		self.critic_training_steps = 0
# 		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
# 		self.critic_params = list(self.critic.parameters())
# 		self.agent_params = list(mac.parameters())
# 		self.params = self.agent_params + self.critic_params
# 		self.target_critic = copy.deepcopy(self.critic)
# 		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
# 		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

# 	def train(self, batch):
# 		# Get the relevant quantities
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length
# 		rewards = batch["reward"][:, :-1]
# 		actions = batch["actions"][:, :]
# 		done = batch["done"][:, :-1].float()
# 		mask = batch["filled"][:, :-1].float()
# 		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
# 		critic_mask = mask.clone()
# 		mask = mask.repeat(1, 1, self.n_agents).view(-1)
# 		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
# 		actions = actions[:,:-1]
# 		mac_out = []
# 		self.mac.init_hidden(batch.batch_size)
# 		for t in range(batch.max_seq_length - 1):
# 			agent_outs = self.mac.forward(batch, t=t)
# 			mac_out.append(agent_outs)
# 		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
# 		# Mask out unavailable actions, renormalise (as in action selection)
# 		q_vals = q_vals.reshape(-1, self.n_actions)
# 		pi = mac_out.view(-1, self.n_actions)
# 		baseline = (pi * q_vals).sum(-1).detach()
# 		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
# 		pi_taken[mask == 0] = 1.0
# 		log_pi_taken = torch.log(pi_taken)
# 		advantages = (q_taken - baseline).detach()
# 		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
# 		self.agent_optimiser.zero_grad()
# 		coma_loss.backward()
# 		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
# 		self.agent_optimiser.step()
# 		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
# 			self._update_targets()
# 			self.last_target_update_step = self.critic_training_steps

# 	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
# 		target_q_vals = self.target_critic(batch)[:, :]
# 		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
# 		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
# 		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
# 		for t in reversed(range(rewards.size(1))):
# 			mask_t = mask[:, t].expand(-1, self.n_agents)
# 			if mask_t.sum() == 0:
# 				continue
# 			q_t = self.critic(batch, t)
# 			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
# 			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
# 			targets_t = targets[:, t]
# 			td_error = (q_taken - targets_t.detach())
# 			# 0-out the targets that came from padded data
# 			masked_td_error = td_error * mask_t
# 			loss = (masked_td_error ** 2).sum() / mask_t.sum()
# 			self.critic_optimiser.zero_grad()
# 			loss.backward()
# 			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
# 			self.critic_optimiser.step()
# 			self.critic_training_steps += 1
# 		return q_vals

# 	def _update_targets(self):
# 		self.target_critic.load_state_dict(self.critic.state_dict())

# 	def cuda(self):
# 		self.mac.cuda()
# 		self.critic.cuda()
# 		self.target_critic.cuda()

# def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
# 	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
# 	# Initialise  last  lambda -return  for  not  done  episodes
# 	ret = target_qs.new_zeros(*target_qs.shape)
# 	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
# 	# Backwards  recursive  update  of the "forward  view"
# 	for t in range(ret.shape[1] - 2, -1,  -1):
# 		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
# 	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
# 	return ret[:, 0:-1]

# class COMACritic(torch.nn.Module):
# 	def __init__(self, scheme, n_agents, n_actions):
# 		super(COMACritic, self).__init__()
# 		self.n_actions = n_actions
# 		self.n_agents = n_agents
# 		input_shape = self._get_input_shape(scheme)
# 		self.output_type = "q"
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

# 	def forward(self, batch, t=None):
# 		inputs = self._build_inputs(batch, t=t)
# 		x = torch.relu(self.fc1(inputs))
# 		x = torch.relu(self.fc2(x))
# 		q = self.fc3(x)
# 		return q

# 	def _build_inputs(self, batch, t=None):
# 		bs = batch.batch_size
# 		max_t = batch.max_seq_length if t is None else 1
# 		ts = slice(None) if t is None else slice(t, t+1)
# 		inputs = []
# 		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
# 		inputs.append(batch["obs"][:, ts])
# 		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
# 		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
# 		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
# 		# last actions
# 		if t == 0:
# 			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		elif isinstance(t, int):
# 			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
# 		else:
# 			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
# 			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
# 			inputs.append(last_actions)

# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["state"]["vshape"]
# 		input_shape += scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
# 		input_shape += self.n_agents
# 		return input_shape

# class BasicMAC:
# 	def __init__(self, scheme, groups, n_agents, n_actions, device):
# 		self.device = device
# 		self.n_agents = n_agents
# 		self.n_actions = n_actions
# 		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
# 		self.action_selector = MultinomialActionSelector()
# 		self.hidden_states = None

# 	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
# 		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
# 		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
# 		return chosen_actions

# 	def forward(self, ep_batch, inputs, t, test_mode=False):
# 		agent_inputs = self._build_inputs(ep_batch, t)
# 		agent_outs = self.agent(agent_inputs, self.hidden_states)
# 		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
# 		if not test_mode:
# 			epsilon_action_num = agent_outs.size(-1)
# 			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
# 		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

# 	def init_hidden(self, batch_size):
# 		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

# 	def parameters(self):
# 		return self.agent.parameters()

# 	def _build_inputs(self, batch, t):
# 		bs = batch.batch_size
# 		inputs = []
# 		inputs.append(batch["obs"][:, t])  # b1av
# 		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
# 		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
# 		inputs = torch.cat([x.reshape(bs, self.n_agents, -1) for x in inputs], dim=-1)
# 		return inputs

# 	def _get_input_shape(self, scheme):
# 		input_shape = scheme["obs"]["vshape"]
# 		input_shape += scheme["actions_onehot"]["vshape"][0]
# 		input_shape += self.n_agents
# 		return input_shape

# class RNNAgent(torch.nn.Module):
# 	def __init__(self, input_shape, output_shape):
# 		super(RNNAgent, self).__init__()
# 		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
# 		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
# 		# self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
# 		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

# 	def init_hidden(self):
# 		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

# 	def forward(self, inputs, hidden_state):
# 		x = torch.relu(self.fc1(inputs))
# 		# h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
# 		# h = self.rnn(x, h_in)
# 		x = self.fc3(x).relu()
# 		q = self.fc2(x)
# 		return q

# class MultinomialActionSelector():
# 	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
# 		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
# 		self.epsilon = self.schedule.eval(0)

# 	def select_action(self, agent_inputs, t_env, test_mode=False):
# 		self.epsilon = self.schedule.eval(t_env)
# 		masked_policies = agent_inputs.clone()
# 		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
# 		return picked_actions

# class DecayThenFlatSchedule():
# 	def __init__(self, start, finish, time_length, decay="exp"):
# 		self.start = start
# 		self.finish = finish
# 		self.time_length = time_length
# 		self.delta = (self.start - self.finish) / self.time_length
# 		self.decay = decay
# 		if self.decay in ["exp"]:
# 			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

# 	def eval(self, T):
# 		if self.decay in ["linear"]:
# 			return max(self.finish, self.start - self.delta * T)
# 		elif self.decay in ["exp"]:
# 			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

# from types import SimpleNamespace as SN

# class EpisodeBatch():
# 	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
# 		self.scheme = scheme.copy()
# 		self.groups = groups
# 		self.batch_size = batch_size
# 		self.max_seq_length = max_seq_length
# 		self.preprocess = {} if preprocess is None else preprocess
# 		self.device = device

# 		if data is not None:
# 			self.data = data
# 		else:
# 			self.data = SN()
# 			self.data.transition_data = {}
# 			self.data.episode_data = {}
# 			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

# 	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
# 		if preprocess is not None:
# 			for k in preprocess:
# 				assert k in scheme
# 				new_k = preprocess[k][0]
# 				transforms = preprocess[k][1]
# 				vshape = self.scheme[k]["vshape"]
# 				dtype = self.scheme[k]["dtype"]
# 				for transform in transforms:
# 					vshape, dtype = transform.infer_output_info(vshape, dtype)
# 				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
# 				if "group" in self.scheme[k]:
# 					self.scheme[new_k]["group"] = self.scheme[k]["group"]
# 				if "episode_const" in self.scheme[k]:
# 					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

# 		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
# 		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

# 		for field_key, field_info in scheme.items():
# 			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
# 			vshape = field_info["vshape"]
# 			episode_const = field_info.get("episode_const", False)
# 			group = field_info.get("group", None)
# 			dtype = field_info.get("dtype", torch.float32)

# 			if isinstance(vshape, int):
# 				vshape = (vshape,)
# 			if group:
# 				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
# 				shape = (groups[group], *vshape)
# 			else:
# 				shape = vshape
# 			if episode_const:
# 				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
# 			else:
# 				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

# 	def extend(self, scheme, groups=None):
# 		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

# 	def to(self, device):
# 		for k, v in self.data.transition_data.items():
# 			self.data.transition_data[k] = v.to(device)
# 		for k, v in self.data.episode_data.items():
# 			self.data.episode_data[k] = v.to(device)
# 		self.device = device

# 	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
# 		slices = self._parse_slices((bs, ts))
# 		for k, v in data.items():
# 			if k in self.data.transition_data:
# 				target = self.data.transition_data
# 				if mark_filled:
# 					target["filled"][slices] = 1
# 					mark_filled = False
# 				_slices = slices
# 			elif k in self.data.episode_data:
# 				target = self.data.episode_data
# 				_slices = slices[0]
# 			else:
# 				raise KeyError("{} not found in transition or episode data".format(k))

# 			dtype = self.scheme[k].get("dtype", torch.float32)
# 			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
# 			self._check_safe_view(v, target[k][_slices])
# 			target[k][_slices] = v.view_as(target[k][_slices])

# 			if k in self.preprocess:
# 				new_k = self.preprocess[k][0]
# 				v = target[k][_slices]
# 				for transform in self.preprocess[k][1]:
# 					v = transform.transform(v)
# 				target[new_k][_slices] = v.view_as(target[new_k][_slices])

# 	def _check_safe_view(self, v, dest):
# 		idx = len(v.shape) - 1
# 		for s in dest.shape[::-1]:
# 			if v.shape[idx] != s:
# 				if s != 1:
# 					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
# 			else:
# 				idx -= 1

# 	def __getitem__(self, item):
# 		if isinstance(item, str):
# 			if item in self.data.episode_data:
# 				return self.data.episode_data[item]
# 			elif item in self.data.transition_data:
# 				return self.data.transition_data[item]
# 			else:
# 				raise ValueError
# 		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
# 			new_data = self._new_data_sn()
# 			for key in item:
# 				if key in self.data.transition_data:
# 					new_data.transition_data[key] = self.data.transition_data[key]
# 				elif key in self.data.episode_data:
# 					new_data.episode_data[key] = self.data.episode_data[key]
# 				else:
# 					raise KeyError("Unrecognised key {}".format(key))

# 			# Update the scheme to only have the requested keys
# 			new_scheme = {key: self.scheme[key] for key in item}
# 			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
# 						for key in item if "group" in self.scheme[key]}
# 			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
# 			return ret
# 		else:
# 			item = self._parse_slices(item)
# 			new_data = self._new_data_sn()
# 			for k, v in self.data.transition_data.items():
# 				new_data.transition_data[k] = v[item]
# 			for k, v in self.data.episode_data.items():
# 				new_data.episode_data[k] = v[item[0]]

# 			ret_bs = self._get_num_items(item[0], self.batch_size)
# 			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

# 			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
# 			return ret

# 	def _get_num_items(self, indexing_item, max_size):
# 		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
# 			return len(indexing_item)
# 		elif isinstance(indexing_item, slice):
# 			_range = indexing_item.indices(max_size)
# 			return 1 + (_range[1] - _range[0] - 1)//_range[2]

# 	def _new_data_sn(self):
# 		new_data = SN()
# 		new_data.transition_data = {}
# 		new_data.episode_data = {}
# 		return new_data

# 	def _parse_slices(self, items):
# 		parsed = []
# 		# Only batch slice given, add full time slice
# 		if (isinstance(items, slice)  # slice a:b
# 			or isinstance(items, int)  # int i
# 			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
# 			):
# 			items = (items, slice(None))

# 		# Need the time indexing to be contiguous
# 		if isinstance(items[1], list):
# 			raise IndexError("Indexing across Time must be contiguous")

# 		for item in items:
# 			#TODO: stronger checks to ensure only supported options get through
# 			if isinstance(item, int):
# 				# Convert single indices to slices
# 				parsed.append(slice(item, item+1))
# 			else:
# 				# Leave slices and lists as is
# 				parsed.append(item)
# 		return parsed

# 	def max_t_filled(self):
# 		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

# class ReplayBuffer(EpisodeBatch):
# 	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
# 		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
# 		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
# 		self.buffer_index = 0
# 		self.episodes_in_buffer = 0

# 	def insert_episode_batch(self, ep_batch):
# 		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
# 			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
# 			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
# 			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
# 			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
# 			self.buffer_index = self.buffer_index % self.buffer_size
# 			assert self.buffer_index < self.buffer_size
# 		else:
# 			buffer_left = self.buffer_size - self.buffer_index
# 			self.insert_episode_batch(ep_batch[0:buffer_left, :])
# 			self.insert_episode_batch(ep_batch[buffer_left:, :])

# 	def can_sample(self, batch_size):
# 		return self.episodes_in_buffer >= batch_size

# 	def sample(self, batch_size):
# 		assert self.can_sample(batch_size)
# 		if self.episodes_in_buffer == batch_size:
# 			return self[:batch_size]
# 		else:
# 			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
# 			return self[ep_ids]


import torch
import random
import numpy as np
from utils.wrappers import ParallelAgent
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
INPUT_LAYER = 64
ACTOR_HIDDEN = 64
CRITIC_HIDDEN = 64

class COMAActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.init_hidden()

	def forward(self, state, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_probs = self.action_probs(state).softmax(-1)
		dist = torch.distributions.Categorical(action_probs)
		action_in = dist.sample()
		action = one_hot_from_indices(action_in, action_probs.size(-1))
		entropy = dist.entropy()
		return action, action_probs, entropy

	def init_hidden(self, batch_size=1):
		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

class COMACritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		q_values = self.q_values(state)
		return q_values

class COMANetwork(PTNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(gpu=gpu)
		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
		if load: self.load_model(load)
		
	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			action, action_prob, entropy = map(list, zip(*[model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]))
			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

	def get_value(self, state, grad=True, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
			return [q.cpu().numpy() for q in q_values] if numpy else q_values

	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
			q_value = model.critic_local(critic_input)
			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
			critic_loss = (q_select - q_target.detach()).pow(2)
			model.step(model.critic_optimizer, critic_loss.mean())

			_, action_probs, entropy = model.actor_local(actor_input)
			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
			advantages = (q_selected - baseline).detach()
			actor_loss = -((advantages * log_probs).sum() + 0.001*entropy.mean())
			model.step(model.actor_optimizer, actor_loss.mean())

	def save_model(self, dirname="pytorch", name="best"):
		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
	def load_model(self, dirname="pytorch", name="best"):
		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = MultiagentReplayBuffer(2000, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		eps = self.eps if eps is None else eps
		action_random = super().get_action(state)
		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
		actor_inputs = []
		state_list = state
		state_list = [state_list] if type(state_list) != list else state_list
		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
			n_agents = self.network.n_agents(s_size)
			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
			actor_inputs.append(actor_input)
		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
		if numpy: self.action = action
		return action

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, action, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()

			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
			actions_one_hot = [one_hot(a) for a in actions]
			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

			q_values = self.network.get_value(critic_inputs, grad=False)
			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
			actions, actor_inputs, critic_inputs = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs]]
			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[4], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-467.757 -467.757 -467.757] [72.536], Avg: [-467.757 -467.757 -467.757] (1.0000) ({r_i: None, r_t: [-9.279 -9.279 -9.279], eps: 1.0})
Step:     100, Reward: [-477.048 -477.048 -477.048] [70.034], Avg: [-472.403 -472.403 -472.403] (0.9900) ({r_i: None, r_t: [-1067.084 -1067.084 -1067.084], eps: 0.99})
Step:     200, Reward: [-572.546 -572.546 -572.546] [158.076], Avg: [-505.784 -505.784 -505.784] (0.9801) ({r_i: None, r_t: [-1147.207 -1147.207 -1147.207], eps: 0.98})
Step:     300, Reward: [-517.282 -517.282 -517.282] [38.060], Avg: [-508.658 -508.658 -508.658] (0.9704) ({r_i: None, r_t: [-1088.856 -1088.856 -1088.856], eps: 0.97})
Step:     400, Reward: [-720.441 -720.441 -720.441] [245.638], Avg: [-551.015 -551.015 -551.015] (0.9607) ({r_i: None, r_t: [-991.496 -991.496 -991.496], eps: 0.961})
Step:     500, Reward: [-627.017 -627.017 -627.017] [106.645], Avg: [-563.682 -563.682 -563.682] (0.9511) ({r_i: None, r_t: [-1067.111 -1067.111 -1067.111], eps: 0.951})
Step:     600, Reward: [-529.330 -529.330 -529.330] [134.841], Avg: [-558.774 -558.774 -558.774] (0.9416) ({r_i: None, r_t: [-1100.030 -1100.030 -1100.030], eps: 0.942})
Step:     700, Reward: [-490.170 -490.170 -490.170] [109.554], Avg: [-550.199 -550.199 -550.199] (0.9322) ({r_i: None, r_t: [-1059.476 -1059.476 -1059.476], eps: 0.932})
Step:     800, Reward: [-633.049 -633.049 -633.049] [116.598], Avg: [-559.404 -559.404 -559.404] (0.9229) ({r_i: None, r_t: [-1183.075 -1183.075 -1183.075], eps: 0.923})
Step:     900, Reward: [-535.874 -535.874 -535.874] [31.326], Avg: [-557.051 -557.051 -557.051] (0.9137) ({r_i: None, r_t: [-1170.619 -1170.619 -1170.619], eps: 0.914})
Step:    1000, Reward: [-576.876 -576.876 -576.876] [136.530], Avg: [-558.854 -558.854 -558.854] (0.9046) ({r_i: None, r_t: [-1121.685 -1121.685 -1121.685], eps: 0.905})
Step:    1100, Reward: [-460.172 -460.172 -460.172] [44.467], Avg: [-550.630 -550.630 -550.630] (0.8956) ({r_i: None, r_t: [-998.887 -998.887 -998.887], eps: 0.896})
Step:    1200, Reward: [-638.631 -638.631 -638.631] [107.997], Avg: [-557.399 -557.399 -557.399] (0.8867) ({r_i: None, r_t: [-1149.295 -1149.295 -1149.295], eps: 0.887})
Step:    1300, Reward: [-625.792 -625.792 -625.792] [180.860], Avg: [-562.285 -562.285 -562.285] (0.8778) ({r_i: None, r_t: [-1183.901 -1183.901 -1183.901], eps: 0.878})
Step:    1400, Reward: [-779.533 -779.533 -779.533] [162.314], Avg: [-576.768 -576.768 -576.768] (0.8691) ({r_i: None, r_t: [-1243.964 -1243.964 -1243.964], eps: 0.869})
Step:    1500, Reward: [-591.475 -591.475 -591.475] [161.191], Avg: [-577.687 -577.687 -577.687] (0.8604) ({r_i: None, r_t: [-1150.988 -1150.988 -1150.988], eps: 0.86})
Step:    1600, Reward: [-559.095 -559.095 -559.095] [38.276], Avg: [-576.593 -576.593 -576.593] (0.8518) ({r_i: None, r_t: [-1057.422 -1057.422 -1057.422], eps: 0.852})
Step:    1700, Reward: [-610.191 -610.191 -610.191] [55.153], Avg: [-578.460 -578.460 -578.460] (0.8433) ({r_i: None, r_t: [-1260.874 -1260.874 -1260.874], eps: 0.843})
Step:    1800, Reward: [-448.711 -448.711 -448.711] [60.815], Avg: [-571.631 -571.631 -571.631] (0.8349) ({r_i: None, r_t: [-1170.750 -1170.750 -1170.750], eps: 0.835})
Step:    1900, Reward: [-686.697 -686.697 -686.697] [273.413], Avg: [-577.384 -577.384 -577.384] (0.8266) ({r_i: None, r_t: [-1207.703 -1207.703 -1207.703], eps: 0.827})
Step:    2000, Reward: [-595.975 -595.975 -595.975] [96.632], Avg: [-578.270 -578.270 -578.270] (0.8183) ({r_i: None, r_t: [-1180.976 -1180.976 -1180.976], eps: 0.818})
Step:    2100, Reward: [-529.522 -529.522 -529.522] [79.615], Avg: [-576.054 -576.054 -576.054] (0.8102) ({r_i: None, r_t: [-1242.564 -1242.564 -1242.564], eps: 0.81})
Step:    2200, Reward: [-688.148 -688.148 -688.148] [209.218], Avg: [-580.927 -580.927 -580.927] (0.8021) ({r_i: None, r_t: [-1172.093 -1172.093 -1172.093], eps: 0.802})
Step:    2300, Reward: [-530.042 -530.042 -530.042] [68.806], Avg: [-578.807 -578.807 -578.807] (0.7941) ({r_i: None, r_t: [-1208.417 -1208.417 -1208.417], eps: 0.794})
Step:    2400, Reward: [-520.751 -520.751 -520.751] [53.464], Avg: [-576.485 -576.485 -576.485] (0.7862) ({r_i: None, r_t: [-1118.966 -1118.966 -1118.966], eps: 0.786})
Step:    2500, Reward: [-761.160 -761.160 -761.160] [252.245], Avg: [-583.588 -583.588 -583.588] (0.7783) ({r_i: None, r_t: [-1066.246 -1066.246 -1066.246], eps: 0.778})
Step:    2600, Reward: [-663.625 -663.625 -663.625] [142.409], Avg: [-586.552 -586.552 -586.552] (0.7705) ({r_i: None, r_t: [-1072.678 -1072.678 -1072.678], eps: 0.771})
Step:    2700, Reward: [-638.531 -638.531 -638.531] [132.081], Avg: [-588.409 -588.409 -588.409] (0.7629) ({r_i: None, r_t: [-1529.494 -1529.494 -1529.494], eps: 0.763})
Step:    2800, Reward: [-679.616 -679.616 -679.616] [180.169], Avg: [-591.554 -591.554 -591.554] (0.7553) ({r_i: None, r_t: [-1145.483 -1145.483 -1145.483], eps: 0.755})
Step:    2900, Reward: [-603.401 -603.401 -603.401] [117.221], Avg: [-591.949 -591.949 -591.949] (0.7477) ({r_i: None, r_t: [-1402.010 -1402.010 -1402.010], eps: 0.748})
Step:    3000, Reward: [-541.175 -541.175 -541.175] [169.354], Avg: [-590.311 -590.311 -590.311] (0.7403) ({r_i: None, r_t: [-1151.434 -1151.434 -1151.434], eps: 0.74})
Step:    3100, Reward: [-667.648 -667.648 -667.648] [174.505], Avg: [-592.728 -592.728 -592.728] (0.7329) ({r_i: None, r_t: [-1638.049 -1638.049 -1638.049], eps: 0.733})
Step:    3200, Reward: [-688.628 -688.628 -688.628] [166.292], Avg: [-595.634 -595.634 -595.634] (0.7256) ({r_i: None, r_t: [-1671.014 -1671.014 -1671.014], eps: 0.726})
Step:    3300, Reward: [-625.527 -625.527 -625.527] [236.060], Avg: [-596.513 -596.513 -596.513] (0.7183) ({r_i: None, r_t: [-1518.554 -1518.554 -1518.554], eps: 0.718})
Step:    3400, Reward: [-764.875 -764.875 -764.875] [236.327], Avg: [-601.323 -601.323 -601.323] (0.7112) ({r_i: None, r_t: [-1280.762 -1280.762 -1280.762], eps: 0.711})
Step:    3500, Reward: [-711.630 -711.630 -711.630] [148.650], Avg: [-604.387 -604.387 -604.387] (0.7041) ({r_i: None, r_t: [-1292.172 -1292.172 -1292.172], eps: 0.704})
Step:    3600, Reward: [-537.584 -537.584 -537.584] [164.023], Avg: [-602.582 -602.582 -602.582] (0.6970) ({r_i: None, r_t: [-1343.368 -1343.368 -1343.368], eps: 0.697})
Step:    3700, Reward: [-534.618 -534.618 -534.618] [59.964], Avg: [-600.793 -600.793 -600.793] (0.6901) ({r_i: None, r_t: [-1425.011 -1425.011 -1425.011], eps: 0.69})
Step:    3800, Reward: [-507.905 -507.905 -507.905] [77.691], Avg: [-598.412 -598.412 -598.412] (0.6832) ({r_i: None, r_t: [-1290.221 -1290.221 -1290.221], eps: 0.683})
Step:    3900, Reward: [-631.093 -631.093 -631.093] [139.311], Avg: [-599.229 -599.229 -599.229] (0.6764) ({r_i: None, r_t: [-1094.484 -1094.484 -1094.484], eps: 0.676})
Step:    4000, Reward: [-727.484 -727.484 -727.484] [247.770], Avg: [-602.357 -602.357 -602.357] (0.6696) ({r_i: None, r_t: [-1033.278 -1033.278 -1033.278], eps: 0.67})
Step:    4100, Reward: [-559.231 -559.231 -559.231] [106.817], Avg: [-601.330 -601.330 -601.330] (0.6630) ({r_i: None, r_t: [-1139.228 -1139.228 -1139.228], eps: 0.663})
Step:    4200, Reward: [-582.970 -582.970 -582.970] [61.776], Avg: [-600.903 -600.903 -600.903] (0.6564) ({r_i: None, r_t: [-1137.152 -1137.152 -1137.152], eps: 0.656})
Step:    4300, Reward: [-550.490 -550.490 -550.490] [131.214], Avg: [-599.757 -599.757 -599.757] (0.6498) ({r_i: None, r_t: [-1182.383 -1182.383 -1182.383], eps: 0.65})
Step:    4400, Reward: [-551.661 -551.661 -551.661] [108.025], Avg: [-598.688 -598.688 -598.688] (0.6433) ({r_i: None, r_t: [-1170.524 -1170.524 -1170.524], eps: 0.643})
Step:    4500, Reward: [-479.908 -479.908 -479.908] [47.090], Avg: [-596.106 -596.106 -596.106] (0.6369) ({r_i: None, r_t: [-1029.462 -1029.462 -1029.462], eps: 0.637})
Step:    4600, Reward: [-553.823 -553.823 -553.823] [92.808], Avg: [-595.207 -595.207 -595.207] (0.6306) ({r_i: None, r_t: [-1152.670 -1152.670 -1152.670], eps: 0.631})
Step:    4700, Reward: [-627.551 -627.551 -627.551] [112.066], Avg: [-595.880 -595.880 -595.880] (0.6243) ({r_i: None, r_t: [-1148.289 -1148.289 -1148.289], eps: 0.624})
Step:    4800, Reward: [-507.339 -507.339 -507.339] [104.590], Avg: [-594.073 -594.073 -594.073] (0.6180) ({r_i: None, r_t: [-925.673 -925.673 -925.673], eps: 0.618})
Step:    4900, Reward: [-552.152 -552.152 -552.152] [137.816], Avg: [-593.235 -593.235 -593.235] (0.6119) ({r_i: None, r_t: [-1121.538 -1121.538 -1121.538], eps: 0.612})
Step:    5000, Reward: [-473.227 -473.227 -473.227] [52.066], Avg: [-590.882 -590.882 -590.882] (0.6058) ({r_i: None, r_t: [-920.626 -920.626 -920.626], eps: 0.606})
Step:    5100, Reward: [-495.793 -495.793 -495.793] [57.890], Avg: [-589.053 -589.053 -589.053] (0.5997) ({r_i: None, r_t: [-1026.988 -1026.988 -1026.988], eps: 0.6})
Step:    5200, Reward: [-450.689 -450.689 -450.689] [48.249], Avg: [-586.443 -586.443 -586.443] (0.5937) ({r_i: None, r_t: [-1001.492 -1001.492 -1001.492], eps: 0.594})
Step:    5300, Reward: [-452.750 -452.750 -452.750] [64.617], Avg: [-583.967 -583.967 -583.967] (0.5878) ({r_i: None, r_t: [-954.412 -954.412 -954.412], eps: 0.588})
Step:    5400, Reward: [-488.711 -488.711 -488.711] [107.164], Avg: [-582.235 -582.235 -582.235] (0.5820) ({r_i: None, r_t: [-1017.318 -1017.318 -1017.318], eps: 0.582})
Step:    5500, Reward: [-519.718 -519.718 -519.718] [59.506], Avg: [-581.119 -581.119 -581.119] (0.5762) ({r_i: None, r_t: [-1123.243 -1123.243 -1123.243], eps: 0.576})
Step:    5600, Reward: [-594.500 -594.500 -594.500] [28.200], Avg: [-581.353 -581.353 -581.353] (0.5704) ({r_i: None, r_t: [-1000.405 -1000.405 -1000.405], eps: 0.57})
Step:    5700, Reward: [-523.254 -523.254 -523.254] [93.472], Avg: [-580.352 -580.352 -580.352] (0.5647) ({r_i: None, r_t: [-993.592 -993.592 -993.592], eps: 0.565})
Step:    5800, Reward: [-515.243 -515.243 -515.243] [70.242], Avg: [-579.248 -579.248 -579.248] (0.5591) ({r_i: None, r_t: [-955.310 -955.310 -955.310], eps: 0.559})
Step:    5900, Reward: [-529.055 -529.055 -529.055] [94.708], Avg: [-578.411 -578.411 -578.411] (0.5535) ({r_i: None, r_t: [-947.719 -947.719 -947.719], eps: 0.554})
Step:    6000, Reward: [-592.009 -592.009 -592.009] [107.807], Avg: [-578.634 -578.634 -578.634] (0.5480) ({r_i: None, r_t: [-1083.349 -1083.349 -1083.349], eps: 0.548})
Step:    6100, Reward: [-553.135 -553.135 -553.135] [90.305], Avg: [-578.223 -578.223 -578.223] (0.5425) ({r_i: None, r_t: [-986.170 -986.170 -986.170], eps: 0.543})
Step:    6200, Reward: [-372.817 -372.817 -372.817] [17.069], Avg: [-574.963 -574.963 -574.963] (0.5371) ({r_i: None, r_t: [-1010.653 -1010.653 -1010.653], eps: 0.537})
Step:    6300, Reward: [-533.546 -533.546 -533.546] [63.909], Avg: [-574.316 -574.316 -574.316] (0.5318) ({r_i: None, r_t: [-1037.055 -1037.055 -1037.055], eps: 0.532})
Step:    6400, Reward: [-596.111 -596.111 -596.111] [84.067], Avg: [-574.651 -574.651 -574.651] (0.5264) ({r_i: None, r_t: [-1081.152 -1081.152 -1081.152], eps: 0.526})
Step:    6500, Reward: [-538.589 -538.589 -538.589] [85.896], Avg: [-574.104 -574.104 -574.104] (0.5212) ({r_i: None, r_t: [-1071.391 -1071.391 -1071.391], eps: 0.521})
Step:    6600, Reward: [-519.940 -519.940 -519.940] [103.910], Avg: [-573.296 -573.296 -573.296] (0.5160) ({r_i: None, r_t: [-1112.508 -1112.508 -1112.508], eps: 0.516})
Step:    6700, Reward: [-532.907 -532.907 -532.907] [48.577], Avg: [-572.702 -572.702 -572.702] (0.5108) ({r_i: None, r_t: [-1064.518 -1064.518 -1064.518], eps: 0.511})
Step:    6800, Reward: [-534.292 -534.292 -534.292] [50.811], Avg: [-572.145 -572.145 -572.145] (0.5058) ({r_i: None, r_t: [-1042.278 -1042.278 -1042.278], eps: 0.506})
Step:    6900, Reward: [-555.285 -555.285 -555.285] [60.419], Avg: [-571.905 -571.905 -571.905] (0.5007) ({r_i: None, r_t: [-1078.546 -1078.546 -1078.546], eps: 0.501})
Step:    7000, Reward: [-511.654 -511.654 -511.654] [45.294], Avg: [-571.056 -571.056 -571.056] (0.4957) ({r_i: None, r_t: [-1151.063 -1151.063 -1151.063], eps: 0.496})
Step:    7100, Reward: [-653.257 -653.257 -653.257] [69.363], Avg: [-572.198 -572.198 -572.198] (0.4908) ({r_i: None, r_t: [-1088.252 -1088.252 -1088.252], eps: 0.491})
Step:    7200, Reward: [-591.080 -591.080 -591.080] [99.581], Avg: [-572.456 -572.456 -572.456] (0.4859) ({r_i: None, r_t: [-1179.428 -1179.428 -1179.428], eps: 0.486})
Step:    7300, Reward: [-535.393 -535.393 -535.393] [144.838], Avg: [-571.955 -571.955 -571.955] (0.4810) ({r_i: None, r_t: [-1322.459 -1322.459 -1322.459], eps: 0.481})
Step:    7400, Reward: [-491.339 -491.339 -491.339] [62.147], Avg: [-570.881 -570.881 -570.881] (0.4762) ({r_i: None, r_t: [-1505.832 -1505.832 -1505.832], eps: 0.476})
Step:    7500, Reward: [-675.753 -675.753 -675.753] [127.612], Avg: [-572.260 -572.260 -572.260] (0.4715) ({r_i: None, r_t: [-1799.477 -1799.477 -1799.477], eps: 0.471})
Step:    7600, Reward: [-978.755 -978.755 -978.755] [65.503], Avg: [-577.540 -577.540 -577.540] (0.4668) ({r_i: None, r_t: [-1723.199 -1723.199 -1723.199], eps: 0.467})
Step:    7700, Reward: [-620.820 -620.820 -620.820] [180.966], Avg: [-578.094 -578.094 -578.094] (0.4621) ({r_i: None, r_t: [-1359.175 -1359.175 -1359.175], eps: 0.462})
Step:    7800, Reward: [-926.311 -926.311 -926.311] [192.296], Avg: [-582.502 -582.502 -582.502] (0.4575) ({r_i: None, r_t: [-1380.346 -1380.346 -1380.346], eps: 0.458})
Step:    7900, Reward: [-803.889 -803.889 -803.889] [180.583], Avg: [-585.270 -585.270 -585.270] (0.4529) ({r_i: None, r_t: [-1783.457 -1783.457 -1783.457], eps: 0.453})
Step:    8000, Reward: [-648.828 -648.828 -648.828] [131.478], Avg: [-586.054 -586.054 -586.054] (0.4484) ({r_i: None, r_t: [-1582.840 -1582.840 -1582.840], eps: 0.448})
Step:    8100, Reward: [-953.599 -953.599 -953.599] [279.836], Avg: [-590.537 -590.537 -590.537] (0.4440) ({r_i: None, r_t: [-1843.143 -1843.143 -1843.143], eps: 0.444})
Step:    8200, Reward: [-1143.330 -1143.330 -1143.330] [280.796], Avg: [-597.197 -597.197 -597.197] (0.4395) ({r_i: None, r_t: [-1980.189 -1980.189 -1980.189], eps: 0.44})
Step:    8300, Reward: [-1038.276 -1038.276 -1038.276] [348.829], Avg: [-602.448 -602.448 -602.448] (0.4351) ({r_i: None, r_t: [-2099.593 -2099.593 -2099.593], eps: 0.435})
Step:    8400, Reward: [-828.584 -828.584 -828.584] [276.365], Avg: [-605.108 -605.108 -605.108] (0.4308) ({r_i: None, r_t: [-2173.174 -2173.174 -2173.174], eps: 0.431})
Step:    8500, Reward: [-837.963 -837.963 -837.963] [239.416], Avg: [-607.816 -607.816 -607.816] (0.4265) ({r_i: None, r_t: [-1949.914 -1949.914 -1949.914], eps: 0.427})
Step:    8600, Reward: [-852.159 -852.159 -852.159] [205.338], Avg: [-610.624 -610.624 -610.624] (0.4223) ({r_i: None, r_t: [-1652.984 -1652.984 -1652.984], eps: 0.422})
Step:    8700, Reward: [-1041.392 -1041.392 -1041.392] [94.521], Avg: [-615.519 -615.519 -615.519] (0.4180) ({r_i: None, r_t: [-2064.872 -2064.872 -2064.872], eps: 0.418})
Step:    8800, Reward: [-824.759 -824.759 -824.759] [436.779], Avg: [-617.870 -617.870 -617.870] (0.4139) ({r_i: None, r_t: [-1703.602 -1703.602 -1703.602], eps: 0.414})
Step:    8900, Reward: [-729.252 -729.252 -729.252] [139.745], Avg: [-619.108 -619.108 -619.108] (0.4097) ({r_i: None, r_t: [-1481.432 -1481.432 -1481.432], eps: 0.41})
Step:    9000, Reward: [-699.453 -699.453 -699.453] [129.878], Avg: [-619.991 -619.991 -619.991] (0.4057) ({r_i: None, r_t: [-1403.665 -1403.665 -1403.665], eps: 0.406})
Step:    9100, Reward: [-733.335 -733.335 -733.335] [284.303], Avg: [-621.223 -621.223 -621.223] (0.4016) ({r_i: None, r_t: [-1230.713 -1230.713 -1230.713], eps: 0.402})
Step:    9200, Reward: [-675.254 -675.254 -675.254] [158.220], Avg: [-621.804 -621.804 -621.804] (0.3976) ({r_i: None, r_t: [-1066.835 -1066.835 -1066.835], eps: 0.398})
Step:    9300, Reward: [-653.848 -653.848 -653.848] [164.585], Avg: [-622.145 -622.145 -622.145] (0.3936) ({r_i: None, r_t: [-1077.206 -1077.206 -1077.206], eps: 0.394})
Step:    9400, Reward: [-577.491 -577.491 -577.491] [53.847], Avg: [-621.675 -621.675 -621.675] (0.3897) ({r_i: None, r_t: [-1233.735 -1233.735 -1233.735], eps: 0.39})
Step:    9500, Reward: [-509.117 -509.117 -509.117] [76.678], Avg: [-620.502 -620.502 -620.502] (0.3858) ({r_i: None, r_t: [-1004.248 -1004.248 -1004.248], eps: 0.386})
Step:    9600, Reward: [-720.076 -720.076 -720.076] [173.210], Avg: [-621.529 -621.529 -621.529] (0.3820) ({r_i: None, r_t: [-1191.857 -1191.857 -1191.857], eps: 0.382})
Step:    9700, Reward: [-884.310 -884.310 -884.310] [219.686], Avg: [-624.210 -624.210 -624.210] (0.3782) ({r_i: None, r_t: [-1301.777 -1301.777 -1301.777], eps: 0.378})
Step:    9800, Reward: [-709.193 -709.193 -709.193] [218.949], Avg: [-625.069 -625.069 -625.069] (0.3744) ({r_i: None, r_t: [-1504.557 -1504.557 -1504.557], eps: 0.374})
Step:    9900, Reward: [-1022.524 -1022.524 -1022.524] [363.871], Avg: [-629.043 -629.043 -629.043] (0.3707) ({r_i: None, r_t: [-1952.250 -1952.250 -1952.250], eps: 0.371})
Step:   10000, Reward: [-972.759 -972.759 -972.759] [289.619], Avg: [-632.446 -632.446 -632.446] (0.3670) ({r_i: None, r_t: [-1739.466 -1739.466 -1739.466], eps: 0.367})
Step:   10100, Reward: [-782.167 -782.167 -782.167] [195.620], Avg: [-633.914 -633.914 -633.914] (0.3633) ({r_i: None, r_t: [-1982.328 -1982.328 -1982.328], eps: 0.363})
Step:   10200, Reward: [-996.304 -996.304 -996.304] [336.780], Avg: [-637.432 -637.432 -637.432] (0.3597) ({r_i: None, r_t: [-1846.607 -1846.607 -1846.607], eps: 0.36})
Step:   10300, Reward: [-1253.482 -1253.482 -1253.482] [432.700], Avg: [-643.356 -643.356 -643.356] (0.3561) ({r_i: None, r_t: [-1835.111 -1835.111 -1835.111], eps: 0.356})
Step:   10400, Reward: [-988.274 -988.274 -988.274] [289.561], Avg: [-646.641 -646.641 -646.641] (0.3525) ({r_i: None, r_t: [-1706.816 -1706.816 -1706.816], eps: 0.353})
Step:   10500, Reward: [-786.049 -786.049 -786.049] [280.184], Avg: [-647.956 -647.956 -647.956] (0.3490) ({r_i: None, r_t: [-1596.778 -1596.778 -1596.778], eps: 0.349})
Step:   10600, Reward: [-604.289 -604.289 -604.289] [138.663], Avg: [-647.548 -647.548 -647.548] (0.3455) ({r_i: None, r_t: [-1347.448 -1347.448 -1347.448], eps: 0.346})
Step:   10700, Reward: [-710.167 -710.167 -710.167] [222.916], Avg: [-648.128 -648.128 -648.128] (0.3421) ({r_i: None, r_t: [-1303.124 -1303.124 -1303.124], eps: 0.342})
Step:   10800, Reward: [-785.545 -785.545 -785.545] [260.564], Avg: [-649.389 -649.389 -649.389] (0.3387) ({r_i: None, r_t: [-1532.081 -1532.081 -1532.081], eps: 0.339})
Step:   10900, Reward: [-654.241 -654.241 -654.241] [105.205], Avg: [-649.433 -649.433 -649.433] (0.3353) ({r_i: None, r_t: [-1353.716 -1353.716 -1353.716], eps: 0.335})
Step:   11000, Reward: [-755.888 -755.888 -755.888] [195.182], Avg: [-650.392 -650.392 -650.392] (0.3320) ({r_i: None, r_t: [-1460.984 -1460.984 -1460.984], eps: 0.332})
Step:   11100, Reward: [-853.958 -853.958 -853.958] [190.772], Avg: [-652.209 -652.209 -652.209] (0.3286) ({r_i: None, r_t: [-1263.707 -1263.707 -1263.707], eps: 0.329})
Step:   11200, Reward: [-613.060 -613.060 -613.060] [102.997], Avg: [-651.863 -651.863 -651.863] (0.3254) ({r_i: None, r_t: [-1445.088 -1445.088 -1445.088], eps: 0.325})
Step:   11300, Reward: [-613.250 -613.250 -613.250] [154.790], Avg: [-651.524 -651.524 -651.524] (0.3221) ({r_i: None, r_t: [-1305.882 -1305.882 -1305.882], eps: 0.322})
Step:   11400, Reward: [-639.806 -639.806 -639.806] [64.888], Avg: [-651.422 -651.422 -651.422] (0.3189) ({r_i: None, r_t: [-1408.153 -1408.153 -1408.153], eps: 0.319})
Step:   11500, Reward: [-655.378 -655.378 -655.378] [111.027], Avg: [-651.456 -651.456 -651.456] (0.3157) ({r_i: None, r_t: [-1190.370 -1190.370 -1190.370], eps: 0.316})
Step:   11600, Reward: [-675.170 -675.170 -675.170] [100.295], Avg: [-651.659 -651.659 -651.659] (0.3126) ({r_i: None, r_t: [-1587.990 -1587.990 -1587.990], eps: 0.313})
Step:   11700, Reward: [-956.196 -956.196 -956.196] [249.082], Avg: [-654.240 -654.240 -654.240] (0.3095) ({r_i: None, r_t: [-1427.395 -1427.395 -1427.395], eps: 0.309})
Step:   11800, Reward: [-750.656 -750.656 -750.656] [171.549], Avg: [-655.050 -655.050 -655.050] (0.3064) ({r_i: None, r_t: [-1460.328 -1460.328 -1460.328], eps: 0.306})
Step:   11900, Reward: [-696.745 -696.745 -696.745] [213.719], Avg: [-655.397 -655.397 -655.397] (0.3033) ({r_i: None, r_t: [-1680.199 -1680.199 -1680.199], eps: 0.303})
Step:   12000, Reward: [-844.413 -844.413 -844.413] [324.978], Avg: [-656.960 -656.960 -656.960] (0.3003) ({r_i: None, r_t: [-1279.809 -1279.809 -1279.809], eps: 0.3})
Step:   12100, Reward: [-840.550 -840.550 -840.550] [69.925], Avg: [-658.464 -658.464 -658.464] (0.2973) ({r_i: None, r_t: [-1439.927 -1439.927 -1439.927], eps: 0.297})
Step:   12200, Reward: [-686.326 -686.326 -686.326] [16.603], Avg: [-658.691 -658.691 -658.691] (0.2943) ({r_i: None, r_t: [-1424.289 -1424.289 -1424.289], eps: 0.294})
Step:   12300, Reward: [-998.987 -998.987 -998.987] [218.552], Avg: [-661.435 -661.435 -661.435] (0.2914) ({r_i: None, r_t: [-1628.552 -1628.552 -1628.552], eps: 0.291})
Step:   12400, Reward: [-541.262 -541.262 -541.262] [91.735], Avg: [-660.474 -660.474 -660.474] (0.2885) ({r_i: None, r_t: [-1411.534 -1411.534 -1411.534], eps: 0.288})
Step:   12500, Reward: [-760.472 -760.472 -760.472] [129.558], Avg: [-661.268 -661.268 -661.268] (0.2856) ({r_i: None, r_t: [-1428.655 -1428.655 -1428.655], eps: 0.286})
Step:   12600, Reward: [-792.748 -792.748 -792.748] [276.138], Avg: [-662.303 -662.303 -662.303] (0.2828) ({r_i: None, r_t: [-1470.532 -1470.532 -1470.532], eps: 0.283})
Step:   12700, Reward: [-830.276 -830.276 -830.276] [164.361], Avg: [-663.615 -663.615 -663.615] (0.2799) ({r_i: None, r_t: [-1574.530 -1574.530 -1574.530], eps: 0.28})
Step:   12800, Reward: [-701.566 -701.566 -701.566] [86.176], Avg: [-663.909 -663.909 -663.909] (0.2771) ({r_i: None, r_t: [-1508.336 -1508.336 -1508.336], eps: 0.277})
Step:   12900, Reward: [-699.094 -699.094 -699.094] [129.616], Avg: [-664.180 -664.180 -664.180] (0.2744) ({r_i: None, r_t: [-1335.473 -1335.473 -1335.473], eps: 0.274})
Step:   13000, Reward: [-801.744 -801.744 -801.744] [229.382], Avg: [-665.230 -665.230 -665.230] (0.2716) ({r_i: None, r_t: [-1241.492 -1241.492 -1241.492], eps: 0.272})
Step:   13100, Reward: [-680.301 -680.301 -680.301] [134.287], Avg: [-665.344 -665.344 -665.344] (0.2689) ({r_i: None, r_t: [-1677.279 -1677.279 -1677.279], eps: 0.269})
Step:   13200, Reward: [-612.726 -612.726 -612.726] [154.205], Avg: [-664.949 -664.949 -664.949] (0.2663) ({r_i: None, r_t: [-1307.916 -1307.916 -1307.916], eps: 0.266})
Step:   13300, Reward: [-739.878 -739.878 -739.878] [172.130], Avg: [-665.508 -665.508 -665.508] (0.2636) ({r_i: None, r_t: [-1622.149 -1622.149 -1622.149], eps: 0.264})
Step:   13400, Reward: [-719.914 -719.914 -719.914] [120.341], Avg: [-665.911 -665.911 -665.911] (0.2610) ({r_i: None, r_t: [-1786.888 -1786.888 -1786.888], eps: 0.261})
Step:   13500, Reward: [-790.746 -790.746 -790.746] [220.515], Avg: [-666.829 -666.829 -666.829] (0.2584) ({r_i: None, r_t: [-1539.260 -1539.260 -1539.260], eps: 0.258})
Step:   13600, Reward: [-765.927 -765.927 -765.927] [99.370], Avg: [-667.552 -667.552 -667.552] (0.2558) ({r_i: None, r_t: [-1318.537 -1318.537 -1318.537], eps: 0.256})
Step:   13700, Reward: [-703.403 -703.403 -703.403] [178.615], Avg: [-667.812 -667.812 -667.812] (0.2532) ({r_i: None, r_t: [-1122.391 -1122.391 -1122.391], eps: 0.253})
Step:   13800, Reward: [-655.697 -655.697 -655.697] [157.389], Avg: [-667.725 -667.725 -667.725] (0.2507) ({r_i: None, r_t: [-1520.455 -1520.455 -1520.455], eps: 0.251})
Step:   13900, Reward: [-754.859 -754.859 -754.859] [162.562], Avg: [-668.347 -668.347 -668.347] (0.2482) ({r_i: None, r_t: [-1284.056 -1284.056 -1284.056], eps: 0.248})
Step:   14000, Reward: [-625.280 -625.280 -625.280] [138.331], Avg: [-668.042 -668.042 -668.042] (0.2457) ({r_i: None, r_t: [-1157.873 -1157.873 -1157.873], eps: 0.246})
Step:   14100, Reward: [-489.593 -489.593 -489.593] [48.784], Avg: [-666.785 -666.785 -666.785] (0.2433) ({r_i: None, r_t: [-1321.949 -1321.949 -1321.949], eps: 0.243})
Step:   14200, Reward: [-535.785 -535.785 -535.785] [72.129], Avg: [-665.869 -665.869 -665.869] (0.2409) ({r_i: None, r_t: [-1126.500 -1126.500 -1126.500], eps: 0.241})
Step:   14300, Reward: [-617.123 -617.123 -617.123] [119.734], Avg: [-665.530 -665.530 -665.530] (0.2385) ({r_i: None, r_t: [-1134.273 -1134.273 -1134.273], eps: 0.238})
Step:   14400, Reward: [-574.681 -574.681 -574.681] [88.670], Avg: [-664.904 -664.904 -664.904] (0.2361) ({r_i: None, r_t: [-1018.704 -1018.704 -1018.704], eps: 0.236})
Step:   14500, Reward: [-550.169 -550.169 -550.169] [111.902], Avg: [-664.118 -664.118 -664.118] (0.2337) ({r_i: None, r_t: [-1218.827 -1218.827 -1218.827], eps: 0.234})
Step:   14600, Reward: [-613.055 -613.055 -613.055] [93.248], Avg: [-663.771 -663.771 -663.771] (0.2314) ({r_i: None, r_t: [-1181.938 -1181.938 -1181.938], eps: 0.231})
Step:   14700, Reward: [-710.422 -710.422 -710.422] [114.101], Avg: [-664.086 -664.086 -664.086] (0.2291) ({r_i: None, r_t: [-1274.464 -1274.464 -1274.464], eps: 0.229})
Step:   14800, Reward: [-573.007 -573.007 -573.007] [51.871], Avg: [-663.475 -663.475 -663.475] (0.2268) ({r_i: None, r_t: [-1366.863 -1366.863 -1366.863], eps: 0.227})
Step:   14900, Reward: [-601.642 -601.642 -601.642] [130.294], Avg: [-663.062 -663.062 -663.062] (0.2245) ({r_i: None, r_t: [-1224.383 -1224.383 -1224.383], eps: 0.225})
Step:   15000, Reward: [-512.056 -512.056 -512.056] [91.238], Avg: [-662.062 -662.062 -662.062] (0.2223) ({r_i: None, r_t: [-1310.158 -1310.158 -1310.158], eps: 0.222})
Step:   15100, Reward: [-716.541 -716.541 -716.541] [93.939], Avg: [-662.421 -662.421 -662.421] (0.2201) ({r_i: None, r_t: [-1493.285 -1493.285 -1493.285], eps: 0.22})
Step:   15200, Reward: [-869.227 -869.227 -869.227] [283.788], Avg: [-663.772 -663.772 -663.772] (0.2179) ({r_i: None, r_t: [-1450.773 -1450.773 -1450.773], eps: 0.218})
Step:   15300, Reward: [-670.724 -670.724 -670.724] [154.373], Avg: [-663.817 -663.817 -663.817] (0.2157) ({r_i: None, r_t: [-1980.717 -1980.717 -1980.717], eps: 0.216})
Step:   15400, Reward: [-1167.332 -1167.332 -1167.332] [111.186], Avg: [-667.066 -667.066 -667.066] (0.2136) ({r_i: None, r_t: [-1918.906 -1918.906 -1918.906], eps: 0.214})
Step:   15500, Reward: [-1139.549 -1139.549 -1139.549] [311.834], Avg: [-670.095 -670.095 -670.095] (0.2114) ({r_i: None, r_t: [-2095.645 -2095.645 -2095.645], eps: 0.211})
Step:   15600, Reward: [-1123.862 -1123.862 -1123.862] [319.215], Avg: [-672.985 -672.985 -672.985] (0.2093) ({r_i: None, r_t: [-2306.877 -2306.877 -2306.877], eps: 0.209})
Step:   15700, Reward: [-1070.782 -1070.782 -1070.782] [146.456], Avg: [-675.503 -675.503 -675.503] (0.2072) ({r_i: None, r_t: [-2085.182 -2085.182 -2085.182], eps: 0.207})
Step:   15800, Reward: [-999.449 -999.449 -999.449] [84.087], Avg: [-677.540 -677.540 -677.540] (0.2052) ({r_i: None, r_t: [-2045.721 -2045.721 -2045.721], eps: 0.205})
Step:   15900, Reward: [-872.230 -872.230 -872.230] [126.321], Avg: [-678.757 -678.757 -678.757] (0.2031) ({r_i: None, r_t: [-1724.745 -1724.745 -1724.745], eps: 0.203})
Step:   16000, Reward: [-1000.341 -1000.341 -1000.341] [282.745], Avg: [-680.754 -680.754 -680.754] (0.2011) ({r_i: None, r_t: [-1924.005 -1924.005 -1924.005], eps: 0.201})
Step:   16100, Reward: [-1000.120 -1000.120 -1000.120] [295.372], Avg: [-682.726 -682.726 -682.726] (0.1991) ({r_i: None, r_t: [-1738.590 -1738.590 -1738.590], eps: 0.199})
Step:   16200, Reward: [-931.509 -931.509 -931.509] [274.606], Avg: [-684.252 -684.252 -684.252] (0.1971) ({r_i: None, r_t: [-1552.882 -1552.882 -1552.882], eps: 0.197})
Step:   16300, Reward: [-722.604 -722.604 -722.604] [185.553], Avg: [-684.486 -684.486 -684.486] (0.1951) ({r_i: None, r_t: [-1854.887 -1854.887 -1854.887], eps: 0.195})
Step:   16400, Reward: [-815.428 -815.428 -815.428] [188.394], Avg: [-685.279 -685.279 -685.279] (0.1932) ({r_i: None, r_t: [-1709.496 -1709.496 -1709.496], eps: 0.193})
Step:   16500, Reward: [-766.564 -766.564 -766.564] [283.123], Avg: [-685.769 -685.769 -685.769] (0.1913) ({r_i: None, r_t: [-2143.284 -2143.284 -2143.284], eps: 0.191})
Step:   16600, Reward: [-715.531 -715.531 -715.531] [213.678], Avg: [-685.947 -685.947 -685.947] (0.1893) ({r_i: None, r_t: [-1545.642 -1545.642 -1545.642], eps: 0.189})
Step:   16700, Reward: [-656.922 -656.922 -656.922] [138.482], Avg: [-685.774 -685.774 -685.774] (0.1875) ({r_i: None, r_t: [-1520.863 -1520.863 -1520.863], eps: 0.187})
Step:   16800, Reward: [-997.496 -997.496 -997.496] [254.901], Avg: [-687.619 -687.619 -687.619] (0.1856) ({r_i: None, r_t: [-1740.607 -1740.607 -1740.607], eps: 0.186})
Step:   16900, Reward: [-647.133 -647.133 -647.133] [158.592], Avg: [-687.381 -687.381 -687.381] (0.1837) ({r_i: None, r_t: [-1633.400 -1633.400 -1633.400], eps: 0.184})
Step:   17000, Reward: [-819.489 -819.489 -819.489] [204.719], Avg: [-688.153 -688.153 -688.153] (0.1819) ({r_i: None, r_t: [-1752.466 -1752.466 -1752.466], eps: 0.182})
Step:   17100, Reward: [-935.633 -935.633 -935.633] [325.203], Avg: [-689.592 -689.592 -689.592] (0.1801) ({r_i: None, r_t: [-1345.608 -1345.608 -1345.608], eps: 0.18})
Step:   17200, Reward: [-824.279 -824.279 -824.279] [147.470], Avg: [-690.371 -690.371 -690.371] (0.1783) ({r_i: None, r_t: [-1510.599 -1510.599 -1510.599], eps: 0.178})
Step:   17300, Reward: [-1021.958 -1021.958 -1021.958] [286.705], Avg: [-692.276 -692.276 -692.276] (0.1765) ({r_i: None, r_t: [-1666.269 -1666.269 -1666.269], eps: 0.177})
Step:   17400, Reward: [-1099.816 -1099.816 -1099.816] [256.677], Avg: [-694.605 -694.605 -694.605] (0.1748) ({r_i: None, r_t: [-2125.149 -2125.149 -2125.149], eps: 0.175})
Step:   17500, Reward: [-882.071 -882.071 -882.071] [73.425], Avg: [-695.670 -695.670 -695.670] (0.1730) ({r_i: None, r_t: [-2208.532 -2208.532 -2208.532], eps: 0.173})
Step:   17600, Reward: [-1017.039 -1017.039 -1017.039] [166.894], Avg: [-697.486 -697.486 -697.486] (0.1713) ({r_i: None, r_t: [-2094.706 -2094.706 -2094.706], eps: 0.171})
Step:   17700, Reward: [-1011.566 -1011.566 -1011.566] [429.030], Avg: [-699.251 -699.251 -699.251] (0.1696) ({r_i: None, r_t: [-2344.098 -2344.098 -2344.098], eps: 0.17})
Step:   17800, Reward: [-1209.039 -1209.039 -1209.039] [238.408], Avg: [-702.099 -702.099 -702.099] (0.1679) ({r_i: None, r_t: [-2142.452 -2142.452 -2142.452], eps: 0.168})
Step:   17900, Reward: [-1383.600 -1383.600 -1383.600] [255.188], Avg: [-705.885 -705.885 -705.885] (0.1662) ({r_i: None, r_t: [-2564.405 -2564.405 -2564.405], eps: 0.166})
Step:   18000, Reward: [-1327.929 -1327.929 -1327.929] [199.214], Avg: [-709.321 -709.321 -709.321] (0.1646) ({r_i: None, r_t: [-2269.441 -2269.441 -2269.441], eps: 0.165})
Step:   18100, Reward: [-1407.182 -1407.182 -1407.182] [165.760], Avg: [-713.156 -713.156 -713.156] (0.1629) ({r_i: None, r_t: [-2692.787 -2692.787 -2692.787], eps: 0.163})
Step:   18200, Reward: [-1183.136 -1183.136 -1183.136] [323.966], Avg: [-715.724 -715.724 -715.724] (0.1613) ({r_i: None, r_t: [-2444.744 -2444.744 -2444.744], eps: 0.161})
Step:   18300, Reward: [-1426.130 -1426.130 -1426.130] [316.315], Avg: [-719.585 -719.585 -719.585] (0.1597) ({r_i: None, r_t: [-2644.293 -2644.293 -2644.293], eps: 0.16})
Step:   18400, Reward: [-1506.049 -1506.049 -1506.049] [138.037], Avg: [-723.836 -723.836 -723.836] (0.1581) ({r_i: None, r_t: [-3313.544 -3313.544 -3313.544], eps: 0.158})
Step:   18500, Reward: [-1433.797 -1433.797 -1433.797] [190.850], Avg: [-727.653 -727.653 -727.653] (0.1565) ({r_i: None, r_t: [-3000.075 -3000.075 -3000.075], eps: 0.157})
Step:   18600, Reward: [-1394.206 -1394.206 -1394.206] [341.419], Avg: [-731.217 -731.217 -731.217] (0.1549) ({r_i: None, r_t: [-3101.683 -3101.683 -3101.683], eps: 0.155})
Step:   18700, Reward: [-1758.200 -1758.200 -1758.200] [91.842], Avg: [-736.680 -736.680 -736.680] (0.1534) ({r_i: None, r_t: [-3203.234 -3203.234 -3203.234], eps: 0.153})
Step:   18800, Reward: [-1600.942 -1600.942 -1600.942] [409.259], Avg: [-741.253 -741.253 -741.253] (0.1519) ({r_i: None, r_t: [-3578.291 -3578.291 -3578.291], eps: 0.152})
Step:   18900, Reward: [-1569.191 -1569.191 -1569.191] [436.418], Avg: [-745.611 -745.611 -745.611] (0.1504) ({r_i: None, r_t: [-3664.139 -3664.139 -3664.139], eps: 0.15})
Step:   19000, Reward: [-1753.121 -1753.121 -1753.121] [187.060], Avg: [-750.885 -750.885 -750.885] (0.1489) ({r_i: None, r_t: [-3338.343 -3338.343 -3338.343], eps: 0.149})
Step:   19100, Reward: [-1489.249 -1489.249 -1489.249] [271.709], Avg: [-754.731 -754.731 -754.731] (0.1474) ({r_i: None, r_t: [-3114.394 -3114.394 -3114.394], eps: 0.147})
Step:   19200, Reward: [-1735.207 -1735.207 -1735.207] [303.275], Avg: [-759.811 -759.811 -759.811] (0.1459) ({r_i: None, r_t: [-3519.936 -3519.936 -3519.936], eps: 0.146})
Step:   19300, Reward: [-1782.626 -1782.626 -1782.626] [371.045], Avg: [-765.083 -765.083 -765.083] (0.1444) ({r_i: None, r_t: [-3430.894 -3430.894 -3430.894], eps: 0.144})
Step:   19400, Reward: [-1959.226 -1959.226 -1959.226] [397.807], Avg: [-771.207 -771.207 -771.207] (0.1430) ({r_i: None, r_t: [-3602.782 -3602.782 -3602.782], eps: 0.143})
Step:   19500, Reward: [-2007.647 -2007.647 -2007.647] [266.372], Avg: [-777.516 -777.516 -777.516] (0.1416) ({r_i: None, r_t: [-3550.587 -3550.587 -3550.587], eps: 0.142})
Step:   19600, Reward: [-1770.767 -1770.767 -1770.767] [226.212], Avg: [-782.558 -782.558 -782.558] (0.1402) ({r_i: None, r_t: [-3703.749 -3703.749 -3703.749], eps: 0.14})
Step:   19700, Reward: [-1866.028 -1866.028 -1866.028] [338.562], Avg: [-788.030 -788.030 -788.030] (0.1388) ({r_i: None, r_t: [-3569.112 -3569.112 -3569.112], eps: 0.139})
Step:   19800, Reward: [-1594.891 -1594.891 -1594.891] [210.565], Avg: [-792.084 -792.084 -792.084] (0.1374) ({r_i: None, r_t: [-3808.994 -3808.994 -3808.994], eps: 0.137})
Step:   19900, Reward: [-1672.697 -1672.697 -1672.697] [159.643], Avg: [-796.487 -796.487 -796.487] (0.1360) ({r_i: None, r_t: [-3533.444 -3533.444 -3533.444], eps: 0.136})
Step:   20000, Reward: [-1688.508 -1688.508 -1688.508] [167.705], Avg: [-800.925 -800.925 -800.925] (0.1347) ({r_i: None, r_t: [-3358.105 -3358.105 -3358.105], eps: 0.135})
Step:   20100, Reward: [-1880.366 -1880.366 -1880.366] [161.590], Avg: [-806.269 -806.269 -806.269] (0.1333) ({r_i: None, r_t: [-3472.835 -3472.835 -3472.835], eps: 0.133})
Step:   20200, Reward: [-1802.164 -1802.164 -1802.164] [299.533], Avg: [-811.175 -811.175 -811.175] (0.1320) ({r_i: None, r_t: [-3614.472 -3614.472 -3614.472], eps: 0.132})
Step:   20300, Reward: [-2076.382 -2076.382 -2076.382] [259.957], Avg: [-817.377 -817.377 -817.377] (0.1307) ({r_i: None, r_t: [-3591.037 -3591.037 -3591.037], eps: 0.131})
Step:   20400, Reward: [-1615.329 -1615.329 -1615.329] [275.604], Avg: [-821.269 -821.269 -821.269] (0.1294) ({r_i: None, r_t: [-3690.567 -3690.567 -3690.567], eps: 0.129})
Step:   20500, Reward: [-1883.492 -1883.492 -1883.492] [424.696], Avg: [-826.426 -826.426 -826.426] (0.1281) ({r_i: None, r_t: [-3315.867 -3315.867 -3315.867], eps: 0.128})
Step:   20600, Reward: [-1556.824 -1556.824 -1556.824] [102.735], Avg: [-829.954 -829.954 -829.954] (0.1268) ({r_i: None, r_t: [-3074.310 -3074.310 -3074.310], eps: 0.127})
Step:   20700, Reward: [-1595.703 -1595.703 -1595.703] [305.633], Avg: [-833.636 -833.636 -833.636] (0.1255) ({r_i: None, r_t: [-3569.654 -3569.654 -3569.654], eps: 0.126})
Step:   20800, Reward: [-1519.651 -1519.651 -1519.651] [364.913], Avg: [-836.918 -836.918 -836.918] (0.1243) ({r_i: None, r_t: [-3158.669 -3158.669 -3158.669], eps: 0.124})
Step:   20900, Reward: [-1437.602 -1437.602 -1437.602] [412.485], Avg: [-839.778 -839.778 -839.778] (0.1230) ({r_i: None, r_t: [-3199.261 -3199.261 -3199.261], eps: 0.123})
Step:   21000, Reward: [-1581.014 -1581.014 -1581.014] [252.631], Avg: [-843.291 -843.291 -843.291] (0.1218) ({r_i: None, r_t: [-3095.143 -3095.143 -3095.143], eps: 0.122})
Step:   21100, Reward: [-1506.130 -1506.130 -1506.130] [359.612], Avg: [-846.418 -846.418 -846.418] (0.1206) ({r_i: None, r_t: [-2684.460 -2684.460 -2684.460], eps: 0.121})
Step:   21200, Reward: [-1635.364 -1635.364 -1635.364] [287.227], Avg: [-850.122 -850.122 -850.122] (0.1194) ({r_i: None, r_t: [-3156.731 -3156.731 -3156.731], eps: 0.119})
Step:   21300, Reward: [-1492.285 -1492.285 -1492.285] [298.277], Avg: [-853.123 -853.123 -853.123] (0.1182) ({r_i: None, r_t: [-2864.597 -2864.597 -2864.597], eps: 0.118})
Step:   21400, Reward: [-1527.360 -1527.360 -1527.360] [87.919], Avg: [-856.259 -856.259 -856.259] (0.1170) ({r_i: None, r_t: [-2799.184 -2799.184 -2799.184], eps: 0.117})
Step:   21500, Reward: [-952.248 -952.248 -952.248] [249.623], Avg: [-856.703 -856.703 -856.703] (0.1159) ({r_i: None, r_t: [-2810.723 -2810.723 -2810.723], eps: 0.116})
Step:   21600, Reward: [-1287.672 -1287.672 -1287.672] [318.693], Avg: [-858.689 -858.689 -858.689] (0.1147) ({r_i: None, r_t: [-2617.967 -2617.967 -2617.967], eps: 0.115})
Step:   21700, Reward: [-791.741 -791.741 -791.741] [277.492], Avg: [-858.382 -858.382 -858.382] (0.1136) ({r_i: None, r_t: [-1808.501 -1808.501 -1808.501], eps: 0.114})
Step:   21800, Reward: [-1018.488 -1018.488 -1018.488] [325.575], Avg: [-859.113 -859.113 -859.113] (0.1124) ({r_i: None, r_t: [-2010.486 -2010.486 -2010.486], eps: 0.112})
Step:   21900, Reward: [-881.799 -881.799 -881.799] [202.037], Avg: [-859.216 -859.216 -859.216] (0.1113) ({r_i: None, r_t: [-1675.371 -1675.371 -1675.371], eps: 0.111})
Step:   22000, Reward: [-770.271 -770.271 -770.271] [381.727], Avg: [-858.814 -858.814 -858.814] (0.1102) ({r_i: None, r_t: [-1557.046 -1557.046 -1557.046], eps: 0.11})
Step:   22100, Reward: [-661.197 -661.197 -661.197] [42.565], Avg: [-857.924 -857.924 -857.924] (0.1091) ({r_i: None, r_t: [-1291.720 -1291.720 -1291.720], eps: 0.109})
Step:   22200, Reward: [-738.689 -738.689 -738.689] [166.191], Avg: [-857.389 -857.389 -857.389] (0.1080) ({r_i: None, r_t: [-1269.787 -1269.787 -1269.787], eps: 0.108})
Step:   22300, Reward: [-680.608 -680.608 -680.608] [88.174], Avg: [-856.600 -856.600 -856.600] (0.1069) ({r_i: None, r_t: [-1230.551 -1230.551 -1230.551], eps: 0.107})
Step:   22400, Reward: [-715.557 -715.557 -715.557] [242.506], Avg: [-855.973 -855.973 -855.973] (0.1059) ({r_i: None, r_t: [-1358.352 -1358.352 -1358.352], eps: 0.106})
Step:   22500, Reward: [-645.121 -645.121 -645.121] [159.804], Avg: [-855.040 -855.040 -855.040] (0.1048) ({r_i: None, r_t: [-1311.123 -1311.123 -1311.123], eps: 0.105})
Step:   22600, Reward: [-806.347 -806.347 -806.347] [219.039], Avg: [-854.825 -854.825 -854.825] (0.1038) ({r_i: None, r_t: [-1343.867 -1343.867 -1343.867], eps: 0.104})
Step:   22700, Reward: [-837.278 -837.278 -837.278] [170.911], Avg: [-854.748 -854.748 -854.748] (0.1027) ({r_i: None, r_t: [-1571.970 -1571.970 -1571.970], eps: 0.103})
Step:   22800, Reward: [-681.924 -681.924 -681.924] [168.191], Avg: [-853.994 -853.994 -853.994] (0.1017) ({r_i: None, r_t: [-1934.889 -1934.889 -1934.889], eps: 0.102})
Step:   22900, Reward: [-975.924 -975.924 -975.924] [163.031], Avg: [-854.524 -854.524 -854.524] (0.1007) ({r_i: None, r_t: [-1967.431 -1967.431 -1967.431], eps: 0.101})
Step:   23000, Reward: [-1139.905 -1139.905 -1139.905] [205.255], Avg: [-855.759 -855.759 -855.759] (0.0997) ({r_i: None, r_t: [-2120.277 -2120.277 -2120.277], eps: 0.1})
Step:   23100, Reward: [-1284.485 -1284.485 -1284.485] [395.136], Avg: [-857.607 -857.607 -857.607] (0.0987) ({r_i: None, r_t: [-2498.969 -2498.969 -2498.969], eps: 0.099})
Step:   23200, Reward: [-1112.729 -1112.729 -1112.729] [79.784], Avg: [-858.702 -858.702 -858.702] (0.0977) ({r_i: None, r_t: [-2915.973 -2915.973 -2915.973], eps: 0.098})
Step:   23300, Reward: [-1391.148 -1391.148 -1391.148] [400.736], Avg: [-860.978 -860.978 -860.978] (0.0967) ({r_i: None, r_t: [-2294.353 -2294.353 -2294.353], eps: 0.097})
Step:   23400, Reward: [-1458.915 -1458.915 -1458.915] [359.418], Avg: [-863.522 -863.522 -863.522] (0.0958) ({r_i: None, r_t: [-3100.900 -3100.900 -3100.900], eps: 0.096})
Step:   23500, Reward: [-1601.555 -1601.555 -1601.555] [376.370], Avg: [-866.649 -866.649 -866.649] (0.0948) ({r_i: None, r_t: [-3432.833 -3432.833 -3432.833], eps: 0.095})
Step:   23600, Reward: [-1727.266 -1727.266 -1727.266] [398.653], Avg: [-870.281 -870.281 -870.281] (0.0939) ({r_i: None, r_t: [-3536.552 -3536.552 -3536.552], eps: 0.094})
Step:   23700, Reward: [-1632.942 -1632.942 -1632.942] [218.229], Avg: [-873.485 -873.485 -873.485] (0.0929) ({r_i: None, r_t: [-3455.700 -3455.700 -3455.700], eps: 0.093})
Step:   23800, Reward: [-1629.684 -1629.684 -1629.684] [410.247], Avg: [-876.649 -876.649 -876.649] (0.0920) ({r_i: None, r_t: [-3280.002 -3280.002 -3280.002], eps: 0.092})
Step:   23900, Reward: [-1848.101 -1848.101 -1848.101] [139.976], Avg: [-880.697 -880.697 -880.697] (0.0911) ({r_i: None, r_t: [-3423.899 -3423.899 -3423.899], eps: 0.091})
Step:   24000, Reward: [-1873.325 -1873.325 -1873.325] [144.243], Avg: [-884.816 -884.816 -884.816] (0.0902) ({r_i: None, r_t: [-3455.147 -3455.147 -3455.147], eps: 0.09})
Step:   24100, Reward: [-1830.307 -1830.307 -1830.307] [159.664], Avg: [-888.723 -888.723 -888.723] (0.0893) ({r_i: None, r_t: [-3620.272 -3620.272 -3620.272], eps: 0.089})
Step:   24200, Reward: [-2088.259 -2088.259 -2088.259] [193.028], Avg: [-893.659 -893.659 -893.659] (0.0884) ({r_i: None, r_t: [-4070.764 -4070.764 -4070.764], eps: 0.088})
Step:   24300, Reward: [-1994.386 -1994.386 -1994.386] [461.392], Avg: [-898.170 -898.170 -898.170] (0.0875) ({r_i: None, r_t: [-3646.159 -3646.159 -3646.159], eps: 0.088})
Step:   24400, Reward: [-1835.261 -1835.261 -1835.261] [189.203], Avg: [-901.995 -901.995 -901.995] (0.0866) ({r_i: None, r_t: [-3573.513 -3573.513 -3573.513], eps: 0.087})
Step:   24500, Reward: [-1870.029 -1870.029 -1870.029] [214.582], Avg: [-905.930 -905.930 -905.930] (0.0858) ({r_i: None, r_t: [-4064.611 -4064.611 -4064.611], eps: 0.086})
Step:   24600, Reward: [-1775.121 -1775.121 -1775.121] [200.427], Avg: [-909.449 -909.449 -909.449] (0.0849) ({r_i: None, r_t: [-4156.516 -4156.516 -4156.516], eps: 0.085})
Step:   24700, Reward: [-1926.815 -1926.815 -1926.815] [244.954], Avg: [-913.551 -913.551 -913.551] (0.0841) ({r_i: None, r_t: [-3925.277 -3925.277 -3925.277], eps: 0.084})
Step:   24800, Reward: [-1853.498 -1853.498 -1853.498] [154.580], Avg: [-917.326 -917.326 -917.326] (0.0832) ({r_i: None, r_t: [-3642.859 -3642.859 -3642.859], eps: 0.083})
Step:   24900, Reward: [-1866.749 -1866.749 -1866.749] [134.364], Avg: [-921.124 -921.124 -921.124] (0.0824) ({r_i: None, r_t: [-3785.972 -3785.972 -3785.972], eps: 0.082})
Step:   25000, Reward: [-1939.610 -1939.610 -1939.610] [271.302], Avg: [-925.182 -925.182 -925.182] (0.0816) ({r_i: None, r_t: [-3802.796 -3802.796 -3802.796], eps: 0.082})
Step:   25100, Reward: [-1873.834 -1873.834 -1873.834] [38.863], Avg: [-928.946 -928.946 -928.946] (0.0808) ({r_i: None, r_t: [-3777.800 -3777.800 -3777.800], eps: 0.081})
Step:   25200, Reward: [-2032.062 -2032.062 -2032.062] [181.069], Avg: [-933.306 -933.306 -933.306] (0.0800) ({r_i: None, r_t: [-3871.179 -3871.179 -3871.179], eps: 0.08})
Step:   25300, Reward: [-1884.661 -1884.661 -1884.661] [65.173], Avg: [-937.052 -937.052 -937.052] (0.0792) ({r_i: None, r_t: [-3629.046 -3629.046 -3629.046], eps: 0.079})
Step:   25400, Reward: [-1891.557 -1891.557 -1891.557] [73.854], Avg: [-940.795 -940.795 -940.795] (0.0784) ({r_i: None, r_t: [-4052.049 -4052.049 -4052.049], eps: 0.078})
Step:   25500, Reward: [-1990.646 -1990.646 -1990.646] [191.659], Avg: [-944.896 -944.896 -944.896] (0.0776) ({r_i: None, r_t: [-3933.719 -3933.719 -3933.719], eps: 0.078})
Step:   25600, Reward: [-1974.951 -1974.951 -1974.951] [60.388], Avg: [-948.904 -948.904 -948.904] (0.0768) ({r_i: None, r_t: [-3943.387 -3943.387 -3943.387], eps: 0.077})
Step:   25700, Reward: [-1747.349 -1747.349 -1747.349] [125.326], Avg: [-951.999 -951.999 -951.999] (0.0760) ({r_i: None, r_t: [-3715.831 -3715.831 -3715.831], eps: 0.076})
Step:   25800, Reward: [-1813.910 -1813.910 -1813.910] [91.930], Avg: [-955.326 -955.326 -955.326] (0.0753) ({r_i: None, r_t: [-3846.755 -3846.755 -3846.755], eps: 0.075})
Step:   25900, Reward: [-1927.340 -1927.340 -1927.340] [222.210], Avg: [-959.065 -959.065 -959.065] (0.0745) ({r_i: None, r_t: [-3792.401 -3792.401 -3792.401], eps: 0.075})
Step:   26000, Reward: [-2124.262 -2124.262 -2124.262] [240.229], Avg: [-963.529 -963.529 -963.529] (0.0738) ({r_i: None, r_t: [-3765.841 -3765.841 -3765.841], eps: 0.074})
Step:   26100, Reward: [-1777.173 -1777.173 -1777.173] [190.638], Avg: [-966.635 -966.635 -966.635] (0.0731) ({r_i: None, r_t: [-4196.564 -4196.564 -4196.564], eps: 0.073})
Step:   26200, Reward: [-1831.770 -1831.770 -1831.770] [190.781], Avg: [-969.924 -969.924 -969.924] (0.0723) ({r_i: None, r_t: [-3991.527 -3991.527 -3991.527], eps: 0.072})
Step:   26300, Reward: [-1946.551 -1946.551 -1946.551] [310.172], Avg: [-973.624 -973.624 -973.624] (0.0716) ({r_i: None, r_t: [-4100.635 -4100.635 -4100.635], eps: 0.072})
Step:   26400, Reward: [-1915.478 -1915.478 -1915.478] [282.342], Avg: [-977.178 -977.178 -977.178] (0.0709) ({r_i: None, r_t: [-3750.518 -3750.518 -3750.518], eps: 0.071})
Step:   26500, Reward: [-1913.219 -1913.219 -1913.219] [151.969], Avg: [-980.697 -980.697 -980.697] (0.0702) ({r_i: None, r_t: [-3750.069 -3750.069 -3750.069], eps: 0.07})
Step:   26600, Reward: [-1931.094 -1931.094 -1931.094] [361.430], Avg: [-984.256 -984.256 -984.256] (0.0695) ({r_i: None, r_t: [-4162.236 -4162.236 -4162.236], eps: 0.069})
Step:   26700, Reward: [-2083.499 -2083.499 -2083.499] [212.577], Avg: [-988.358 -988.358 -988.358] (0.0688) ({r_i: None, r_t: [-3847.519 -3847.519 -3847.519], eps: 0.069})
Step:   26800, Reward: [-1896.050 -1896.050 -1896.050] [204.582], Avg: [-991.732 -991.732 -991.732] (0.0681) ({r_i: None, r_t: [-3986.379 -3986.379 -3986.379], eps: 0.068})
Step:   26900, Reward: [-1959.508 -1959.508 -1959.508] [275.783], Avg: [-995.317 -995.317 -995.317] (0.0674) ({r_i: None, r_t: [-4155.252 -4155.252 -4155.252], eps: 0.067})
Step:   27000, Reward: [-2059.441 -2059.441 -2059.441] [242.692], Avg: [-999.243 -999.243 -999.243] (0.0668) ({r_i: None, r_t: [-3428.049 -3428.049 -3428.049], eps: 0.067})
Step:   27100, Reward: [-2046.772 -2046.772 -2046.772] [151.052], Avg: [-1003.095 -1003.095 -1003.095] (0.0661) ({r_i: None, r_t: [-3585.249 -3585.249 -3585.249], eps: 0.066})
Step:   27200, Reward: [-1908.847 -1908.847 -1908.847] [318.571], Avg: [-1006.412 -1006.412 -1006.412] (0.0654) ({r_i: None, r_t: [-3807.671 -3807.671 -3807.671], eps: 0.065})
Step:   27300, Reward: [-1800.349 -1800.349 -1800.349] [234.381], Avg: [-1009.310 -1009.310 -1009.310] (0.0648) ({r_i: None, r_t: [-3796.476 -3796.476 -3796.476], eps: 0.065})
Step:   27400, Reward: [-1879.999 -1879.999 -1879.999] [163.413], Avg: [-1012.476 -1012.476 -1012.476] (0.0641) ({r_i: None, r_t: [-3696.475 -3696.475 -3696.475], eps: 0.064})
Step:   27500, Reward: [-2075.755 -2075.755 -2075.755] [80.279], Avg: [-1016.328 -1016.328 -1016.328] (0.0635) ({r_i: None, r_t: [-3838.077 -3838.077 -3838.077], eps: 0.063})
Step:   27600, Reward: [-1871.200 -1871.200 -1871.200] [205.443], Avg: [-1019.415 -1019.415 -1019.415] (0.0629) ({r_i: None, r_t: [-3867.253 -3867.253 -3867.253], eps: 0.063})
Step:   27700, Reward: [-2045.184 -2045.184 -2045.184] [298.385], Avg: [-1023.104 -1023.104 -1023.104] (0.0622) ({r_i: None, r_t: [-3267.767 -3267.767 -3267.767], eps: 0.062})
Step:   27800, Reward: [-1908.133 -1908.133 -1908.133] [192.104], Avg: [-1026.277 -1026.277 -1026.277] (0.0616) ({r_i: None, r_t: [-3981.775 -3981.775 -3981.775], eps: 0.062})
Step:   27900, Reward: [-1885.116 -1885.116 -1885.116] [134.605], Avg: [-1029.344 -1029.344 -1029.344] (0.0610) ({r_i: None, r_t: [-3803.628 -3803.628 -3803.628], eps: 0.061})
Step:   28000, Reward: [-1866.097 -1866.097 -1866.097] [334.949], Avg: [-1032.322 -1032.322 -1032.322] (0.0604) ({r_i: None, r_t: [-3853.886 -3853.886 -3853.886], eps: 0.06})
Step:   28100, Reward: [-1651.030 -1651.030 -1651.030] [310.975], Avg: [-1034.516 -1034.516 -1034.516] (0.0598) ({r_i: None, r_t: [-3722.977 -3722.977 -3722.977], eps: 0.06})
Step:   28200, Reward: [-1669.516 -1669.516 -1669.516] [88.782], Avg: [-1036.760 -1036.760 -1036.760] (0.0592) ({r_i: None, r_t: [-3760.951 -3760.951 -3760.951], eps: 0.059})
Step:   28300, Reward: [-2218.269 -2218.269 -2218.269] [193.630], Avg: [-1040.920 -1040.920 -1040.920] (0.0586) ({r_i: None, r_t: [-3316.088 -3316.088 -3316.088], eps: 0.059})
Step:   28400, Reward: [-1803.816 -1803.816 -1803.816] [241.332], Avg: [-1043.597 -1043.597 -1043.597] (0.0580) ({r_i: None, r_t: [-3786.782 -3786.782 -3786.782], eps: 0.058})
Step:   28500, Reward: [-1924.071 -1924.071 -1924.071] [159.007], Avg: [-1046.675 -1046.675 -1046.675] (0.0574) ({r_i: None, r_t: [-3798.838 -3798.838 -3798.838], eps: 0.057})
Step:   28600, Reward: [-1824.834 -1824.834 -1824.834] [425.545], Avg: [-1049.387 -1049.387 -1049.387] (0.0569) ({r_i: None, r_t: [-3466.696 -3466.696 -3466.696], eps: 0.057})
Step:   28700, Reward: [-1616.647 -1616.647 -1616.647] [156.758], Avg: [-1051.356 -1051.356 -1051.356] (0.0563) ({r_i: None, r_t: [-3715.444 -3715.444 -3715.444], eps: 0.056})
Step:   28800, Reward: [-1717.173 -1717.173 -1717.173] [169.266], Avg: [-1053.660 -1053.660 -1053.660] (0.0557) ({r_i: None, r_t: [-3455.693 -3455.693 -3455.693], eps: 0.056})
Step:   28900, Reward: [-1494.264 -1494.264 -1494.264] [228.143], Avg: [-1055.179 -1055.179 -1055.179] (0.0552) ({r_i: None, r_t: [-3605.583 -3605.583 -3605.583], eps: 0.055})
Step:   29000, Reward: [-1494.297 -1494.297 -1494.297] [133.137], Avg: [-1056.688 -1056.688 -1056.688] (0.0546) ({r_i: None, r_t: [-2951.579 -2951.579 -2951.579], eps: 0.055})
Step:   29100, Reward: [-1336.511 -1336.511 -1336.511] [264.979], Avg: [-1057.647 -1057.647 -1057.647] (0.0541) ({r_i: None, r_t: [-2739.641 -2739.641 -2739.641], eps: 0.054})
Step:   29200, Reward: [-1154.195 -1154.195 -1154.195] [237.595], Avg: [-1057.976 -1057.976 -1057.976] (0.0535) ({r_i: None, r_t: [-2791.412 -2791.412 -2791.412], eps: 0.054})
Step:   29300, Reward: [-1078.322 -1078.322 -1078.322] [182.356], Avg: [-1058.045 -1058.045 -1058.045] (0.0530) ({r_i: None, r_t: [-2974.199 -2974.199 -2974.199], eps: 0.053})
Step:   29400, Reward: [-872.850 -872.850 -872.850] [79.989], Avg: [-1057.418 -1057.418 -1057.418] (0.0525) ({r_i: None, r_t: [-2441.092 -2441.092 -2441.092], eps: 0.052})
Step:   29500, Reward: [-889.544 -889.544 -889.544] [261.889], Avg: [-1056.850 -1056.850 -1056.850] (0.0520) ({r_i: None, r_t: [-1873.741 -1873.741 -1873.741], eps: 0.052})
Step:   29600, Reward: [-922.233 -922.233 -922.233] [248.824], Avg: [-1056.397 -1056.397 -1056.397] (0.0514) ({r_i: None, r_t: [-1609.568 -1609.568 -1609.568], eps: 0.051})
Step:   29700, Reward: [-691.666 -691.666 -691.666] [245.770], Avg: [-1055.173 -1055.173 -1055.173] (0.0509) ({r_i: None, r_t: [-1301.175 -1301.175 -1301.175], eps: 0.051})
Step:   29800, Reward: [-600.529 -600.529 -600.529] [140.791], Avg: [-1053.653 -1053.653 -1053.653] (0.0504) ({r_i: None, r_t: [-1376.371 -1376.371 -1376.371], eps: 0.05})
Step:   29900, Reward: [-565.450 -565.450 -565.450] [112.659], Avg: [-1052.025 -1052.025 -1052.025] (0.0499) ({r_i: None, r_t: [-1427.755 -1427.755 -1427.755], eps: 0.05})
Step:   30000, Reward: [-512.191 -512.191 -512.191] [88.273], Avg: [-1050.232 -1050.232 -1050.232] (0.0494) ({r_i: None, r_t: [-1213.865 -1213.865 -1213.865], eps: 0.049})
Step:   30100, Reward: [-650.099 -650.099 -650.099] [94.885], Avg: [-1048.907 -1048.907 -1048.907] (0.0489) ({r_i: None, r_t: [-1112.814 -1112.814 -1112.814], eps: 0.049})
Step:   30200, Reward: [-627.785 -627.785 -627.785] [43.536], Avg: [-1047.517 -1047.517 -1047.517] (0.0484) ({r_i: None, r_t: [-1184.665 -1184.665 -1184.665], eps: 0.048})
Step:   30300, Reward: [-471.794 -471.794 -471.794] [100.529], Avg: [-1045.623 -1045.623 -1045.623] (0.0479) ({r_i: None, r_t: [-1292.184 -1292.184 -1292.184], eps: 0.048})
