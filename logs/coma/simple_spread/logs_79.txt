Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16,
state_size: [(1, 18), (1, 18), (1, 18)],
action_size: [[1, 5], [1, 5], [1, 5]],
action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],
envs: <class 'utils.envs.EnsembleEnv'>,
reward_shape: False,
icm: False,

import copy
import torch
import numpy as np
from models.rand import MultiagentReplayBuffer3
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.0003
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 1
TARGET_UPDATE = 200

HIDDEN_SIZE = 64
EPS_MAX = 0.5
EPS_MIN = 0.01
EPS_DECAY = 0.995
NUM_ENVS = 4
EPISODE_LIMIT = 50
ENTROPY_WEIGHT = 0.001			# The weight for the entropy term of the Actor loss
MAX_BUFFER_SIZE = 192000		# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 64

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, 2000, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0
		self.replay_buffer2 = MultiagentReplayBuffer3(EPISODE_LIMIT*REPLAY_BATCH_SIZE, state_size, action_size)

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		state_joint = np.concatenate(state, -1)
		obs = np.concatenate(state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
		if not hasattr(self, "action"): self.action = np.zeros([*obs.shape[:-1], self.action_size[0][-1]])
		inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
		self.episode_batch.update({"state": [state_joint], "obs": [obs]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, inputs, t_ep=self.step, t_env=self.time, test_mode=False)
		self.action = one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy()
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(self.action, actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		actions, rewards, dones = [list(zip(*x)) for x in [action, reward, done]]
		actions_one_hot = [np.argmax(a, -1) for a in actions]
		rewards = [np.mean(rewards, -1)]
		dones = [np.any(dones, -1)]
		obs = np.concatenate(state, -2)
		next_obs = np.concatenate(next_state, -2)
		agent_ids = np.repeat(np.expand_dims(np.eye(self.learner.n_agents), 0), repeats=self.num_envs, axis=0)
		actor_inputs = torch.from_numpy(np.concatenate([obs, self.action, agent_ids], -1))
		post_transition_data = {"actions": actions_one_hot, "reward": rewards, "done": dones}
		self.replay_buffer2.add((state, action, next_state, reward, done))
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			state_joint = np.concatenate(state, -1)
			self.episode_batch.update({"state": [state_joint], "obs": [next_obs]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, actor_inputs, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				sample = self.replay_buffer2.sample(REPLAY_BATCH_SIZE, torch.Tensor)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.Adam(params=self.agent_params, lr=LEARNING_RATE)
		self.critic_optimiser = torch.optim.Adam(params=self.critic_params, lr=LEARNING_RATE)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, None, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, inputs, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, inputs, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, inputs, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		# self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		# h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		# h = self.rnn(x, h_in)
		x = self.fc3(x).relu()
		q = self.fc2(x)
		return q

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		action_probs = self.action_probs(state).softmax(-1)
# 		dist = torch.distributions.Categorical(action_probs)
# 		action_in = dist.sample()
# 		action = one_hot_from_indices(action_in, action_probs.size(-1))
# 		entropy = dist.entropy()
# 		return action, action_probs, entropy

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s,a: COMAActor([s[-1] + a[-1] + self.n_agents(s)], a)
# 		make_critic = lambda s,a: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s[-1] + self.n_agents(s)], a)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action, action_prob, entropy = map(list, zip(*[model.actor_local(s, sample) for s,model in zip(state, self.models)]))
# 			return [[a.cpu().numpy().astype(np.float32) for a in x] for x in [action, action_prob, entropy]] if numpy else (action, action_prob, entropy)

# 	def get_value(self, state, use_target=False, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s) if use_target else model.critic_target(s) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			q_value = model.critic_local(critic_input)
# 			q_select = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True)).squeeze(-1)
# 			critic_loss = (q_select - q_target.detach()).pow(2)
# 			model.step(model.critic_optimizer, critic_loss.mean(), model.critic_local.parameters())
# 			model.soft_copy(model.critic_local, model.critic_target)

# 			_, action_probs, entropy = model.actor_local(actor_input)
# 			baseline = (action_probs * q_value).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value, dim=-1, index=action.argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action.argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = -((advantages * log_probs).sum() + ENTROPY_WEIGHT*entropy.mean())
# 			model.step(model.actor_optimizer, actor_loss.mean(), model.actor_local.parameters())

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
# 		self.replay_buffer = MultiagentReplayBuffer3(MAX_BUFFER_SIZE, state_size, action_size)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		self.step = 0 if not hasattr(self, "step") else self.step + 1
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = state
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = self.to_tensor(np.concatenate([state, last_action, agent_ids], axis=-1))
# 			actor_inputs.append(actor_input)
# 		action = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)[0]
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()
# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]).to(self.network.device)
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1).to(self.network.device) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, use_target=True, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			actions, actor_inputs, critic_inputs, q_values = [[t[:-1] for t in x] for x in [actions, actor_inputs, critic_inputs, q_values]]
# 			actions, actor_inputs, critic_inputs, q_values, q_targets = [[t.reshape(-1, *t.shape[2:]).cpu().numpy() for t in x] for x in [actions, actor_inputs, critic_inputs, q_values, q_targets]]
# 			self.replay_buffer.add((actions, actor_inputs, critic_inputs, q_values, q_targets))
# 		if len(self.replay_buffer) >= REPLAY_BATCH_SIZE and self.step%self.update_freq == 0:
# 			actions, actor_inputs, critic_inputs, q_values, q_targets = self.replay_buffer.sample(REPLAY_BATCH_SIZE, cast=self.to_tensor)
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 			if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0003           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.001   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 256				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 512				# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.998			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.001               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 1000000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
# import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.sac import SACAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, SelfPlayAgent, ParticleTeamEnv, FootballTeamEnv, TrainEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker, MPI_SIZE, MPI_RANK
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-3]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False, reward_shape=False):
	if env_name in gym_envs: return TrainEnv(gym.make(env_name))
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	ballr = lambda x,y: (np.maximum if x>0 else np.minimum)(x - np.abs(y)*np.sign(x), 0.5*x)
	reward_fn = lambda obs,reward: [(ballr(o[0,88], o[0,89]) + o[0,95]-o[0,96] + 2*r)/4 for o,r in zip(obs,reward)]
	return FootballTeamEnv(ggym, env_name, reward_fn if reward_shape else None)

def run(model, steps=10000, ports=16, env_name=env_name, trial_at=100, save_at=10, checkpoint=True, save_best=False, log=True, render=False, reward_shape=False, icm=False):
	envs = (EnvManager if type(ports) == list or MPI_SIZE > 1 else EnsembleEnv)(lambda: make_env(env_name, reward_shape=reward_shape), ports)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, envs.num_envs, load="", gpu=True, agent2=RandomAgent, save_dir=env_name, icm=icm) 
	logger = Logger(model, env_name, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), reward_shape=reward_shape, icm=icm)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%trial_at == 0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.mean(rollouts, axis=-1))
			if checkpoint and len(total_rewards) % save_at==0: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.eps:.4f})", agent.get_stats())

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = (DoubleAgent if envs.env.self_play else ParallelAgent)(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent, save_dir=env_name)
	print(f"Reward: {np.mean([rollout(envs.env, agent, eps=0.0, render=True) for _ in range(5)], axis=0)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="coma", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=200000, help="Number of steps to train the agent")
	parser.add_argument("--reward_shape", action="store_true", help="Whether to shape rewards for football")
	parser.add_argument("--icm", action="store_true", help="Whether to use intrinsic motivation")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "sac":SACAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model=model, env_name=env_name, render=args.render)
	elif args.selfport is not None or MPI_RANK>0 :
		EnvWorker(self_port=args.selfport, make_env=make_env).start()
	else:
		run(model=model, steps=args.steps, ports=args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render, reward_shape=args.reward_shape, icm=args.icm)


Step:       0, Reward: [-476.573 -476.573 -476.573] [82.454], Avg: [-476.573 -476.573 -476.573] (1.0000) ({r_i: None, r_t: [-8.765 -8.765 -8.765], eps: 1.0})
Step:     100, Reward: [-520.607 -520.607 -520.607] [79.576], Avg: [-498.590 -498.590 -498.590] (1.0000) ({r_i: None, r_t: [-972.448 -972.448 -972.448], eps: 1.0})
Step:     200, Reward: [-463.096 -463.096 -463.096] [88.715], Avg: [-486.759 -486.759 -486.759] (1.0000) ({r_i: None, r_t: [-1004.220 -1004.220 -1004.220], eps: 1.0})
Step:     300, Reward: [-460.143 -460.143 -460.143] [84.953], Avg: [-480.105 -480.105 -480.105] (1.0000) ({r_i: None, r_t: [-1031.760 -1031.760 -1031.760], eps: 1.0})
Step:     400, Reward: [-507.202 -507.202 -507.202] [81.319], Avg: [-485.524 -485.524 -485.524] (1.0000) ({r_i: None, r_t: [-964.466 -964.466 -964.466], eps: 1.0})
Step:     500, Reward: [-494.498 -494.498 -494.498] [75.815], Avg: [-487.020 -487.020 -487.020] (1.0000) ({r_i: None, r_t: [-962.900 -962.900 -962.900], eps: 1.0})
Step:     600, Reward: [-485.974 -485.974 -485.974] [102.604], Avg: [-486.870 -486.870 -486.870] (1.0000) ({r_i: None, r_t: [-969.379 -969.379 -969.379], eps: 1.0})
Step:     700, Reward: [-512.105 -512.105 -512.105] [82.785], Avg: [-490.025 -490.025 -490.025] (1.0000) ({r_i: None, r_t: [-1002.453 -1002.453 -1002.453], eps: 1.0})
Step:     800, Reward: [-502.642 -502.642 -502.642] [125.465], Avg: [-491.426 -491.426 -491.426] (1.0000) ({r_i: None, r_t: [-955.018 -955.018 -955.018], eps: 1.0})
Step:     900, Reward: [-484.263 -484.263 -484.263] [108.094], Avg: [-490.710 -490.710 -490.710] (1.0000) ({r_i: None, r_t: [-979.219 -979.219 -979.219], eps: 1.0})
Step:    1000, Reward: [-485.581 -485.581 -485.581] [91.551], Avg: [-490.244 -490.244 -490.244] (1.0000) ({r_i: None, r_t: [-949.907 -949.907 -949.907], eps: 1.0})
Step:    1100, Reward: [-462.638 -462.638 -462.638] [49.044], Avg: [-487.943 -487.943 -487.943] (1.0000) ({r_i: None, r_t: [-960.448 -960.448 -960.448], eps: 1.0})
Step:    1200, Reward: [-475.655 -475.655 -475.655] [71.215], Avg: [-486.998 -486.998 -486.998] (1.0000) ({r_i: None, r_t: [-943.914 -943.914 -943.914], eps: 1.0})
Step:    1300, Reward: [-473.970 -473.970 -473.970] [85.593], Avg: [-486.067 -486.067 -486.067] (1.0000) ({r_i: None, r_t: [-972.433 -972.433 -972.433], eps: 1.0})
Step:    1400, Reward: [-508.129 -508.129 -508.129] [88.016], Avg: [-487.538 -487.538 -487.538] (1.0000) ({r_i: None, r_t: [-1009.401 -1009.401 -1009.401], eps: 1.0})
Step:    1500, Reward: [-476.110 -476.110 -476.110] [87.332], Avg: [-486.824 -486.824 -486.824] (1.0000) ({r_i: None, r_t: [-995.122 -995.122 -995.122], eps: 1.0})
Step:    1600, Reward: [-495.870 -495.870 -495.870] [91.045], Avg: [-487.356 -487.356 -487.356] (1.0000) ({r_i: None, r_t: [-997.673 -997.673 -997.673], eps: 1.0})
Step:    1700, Reward: [-451.651 -451.651 -451.651] [76.269], Avg: [-485.372 -485.372 -485.372] (1.0000) ({r_i: None, r_t: [-933.955 -933.955 -933.955], eps: 1.0})
Step:    1800, Reward: [-481.314 -481.314 -481.314] [84.496], Avg: [-485.159 -485.159 -485.159] (1.0000) ({r_i: None, r_t: [-989.867 -989.867 -989.867], eps: 1.0})
Step:    1900, Reward: [-478.568 -478.568 -478.568] [93.330], Avg: [-484.829 -484.829 -484.829] (1.0000) ({r_i: None, r_t: [-986.841 -986.841 -986.841], eps: 1.0})
Step:    2000, Reward: [-520.010 -520.010 -520.010] [108.196], Avg: [-486.505 -486.505 -486.505] (1.0000) ({r_i: None, r_t: [-960.626 -960.626 -960.626], eps: 1.0})
Step:    2100, Reward: [-531.646 -531.646 -531.646] [126.448], Avg: [-488.557 -488.557 -488.557] (1.0000) ({r_i: None, r_t: [-1013.693 -1013.693 -1013.693], eps: 1.0})
Step:    2200, Reward: [-508.669 -508.669 -508.669] [120.495], Avg: [-489.431 -489.431 -489.431] (1.0000) ({r_i: None, r_t: [-1022.710 -1022.710 -1022.710], eps: 1.0})
Step:    2300, Reward: [-487.477 -487.477 -487.477] [74.692], Avg: [-489.350 -489.350 -489.350] (1.0000) ({r_i: None, r_t: [-980.220 -980.220 -980.220], eps: 1.0})
Step:    2400, Reward: [-488.434 -488.434 -488.434] [80.094], Avg: [-489.313 -489.313 -489.313] (1.0000) ({r_i: None, r_t: [-953.021 -953.021 -953.021], eps: 1.0})
Step:    2500, Reward: [-469.642 -469.642 -469.642] [66.950], Avg: [-488.556 -488.556 -488.556] (1.0000) ({r_i: None, r_t: [-953.607 -953.607 -953.607], eps: 1.0})
Step:    2600, Reward: [-504.607 -504.607 -504.607] [74.323], Avg: [-489.151 -489.151 -489.151] (1.0000) ({r_i: None, r_t: [-953.394 -953.394 -953.394], eps: 1.0})
Step:    2700, Reward: [-506.827 -506.827 -506.827] [72.221], Avg: [-489.782 -489.782 -489.782] (1.0000) ({r_i: None, r_t: [-986.548 -986.548 -986.548], eps: 1.0})
Step:    2800, Reward: [-500.776 -500.776 -500.776] [77.256], Avg: [-490.161 -490.161 -490.161] (1.0000) ({r_i: None, r_t: [-962.376 -962.376 -962.376], eps: 1.0})
Step:    2900, Reward: [-497.416 -497.416 -497.416] [86.488], Avg: [-490.403 -490.403 -490.403] (1.0000) ({r_i: None, r_t: [-976.667 -976.667 -976.667], eps: 1.0})
Step:    3000, Reward: [-508.302 -508.302 -508.302] [101.015], Avg: [-490.980 -490.980 -490.980] (1.0000) ({r_i: None, r_t: [-1071.312 -1071.312 -1071.312], eps: 1.0})
Step:    3100, Reward: [-481.265 -481.265 -481.265] [54.077], Avg: [-490.677 -490.677 -490.677] (1.0000) ({r_i: None, r_t: [-960.351 -960.351 -960.351], eps: 1.0})
Step:    3200, Reward: [-475.615 -475.615 -475.615] [80.210], Avg: [-490.220 -490.220 -490.220] (1.0000) ({r_i: None, r_t: [-935.918 -935.918 -935.918], eps: 1.0})
Step:    3300, Reward: [-476.402 -476.402 -476.402] [101.149], Avg: [-489.814 -489.814 -489.814] (1.0000) ({r_i: None, r_t: [-1010.255 -1010.255 -1010.255], eps: 1.0})
Step:    3400, Reward: [-477.562 -477.562 -477.562] [86.164], Avg: [-489.464 -489.464 -489.464] (1.0000) ({r_i: None, r_t: [-946.432 -946.432 -946.432], eps: 1.0})
Step:    3500, Reward: [-479.803 -479.803 -479.803] [93.147], Avg: [-489.196 -489.196 -489.196] (1.0000) ({r_i: None, r_t: [-973.631 -973.631 -973.631], eps: 1.0})
Step:    3600, Reward: [-498.672 -498.672 -498.672] [91.335], Avg: [-489.452 -489.452 -489.452] (1.0000) ({r_i: None, r_t: [-978.185 -978.185 -978.185], eps: 1.0})
Step:    3700, Reward: [-516.432 -516.432 -516.432] [93.463], Avg: [-490.162 -490.162 -490.162] (1.0000) ({r_i: None, r_t: [-1018.232 -1018.232 -1018.232], eps: 1.0})
Step:    3800, Reward: [-455.980 -455.980 -455.980] [76.218], Avg: [-489.285 -489.285 -489.285] (1.0000) ({r_i: None, r_t: [-990.159 -990.159 -990.159], eps: 1.0})
Step:    3900, Reward: [-481.796 -481.796 -481.796] [78.878], Avg: [-489.098 -489.098 -489.098] (1.0000) ({r_i: None, r_t: [-978.011 -978.011 -978.011], eps: 1.0})
Step:    4000, Reward: [-499.584 -499.584 -499.584] [86.415], Avg: [-489.354 -489.354 -489.354] (1.0000) ({r_i: None, r_t: [-1053.245 -1053.245 -1053.245], eps: 1.0})
Step:    4100, Reward: [-530.752 -530.752 -530.752] [115.194], Avg: [-490.339 -490.339 -490.339] (1.0000) ({r_i: None, r_t: [-989.691 -989.691 -989.691], eps: 1.0})
Step:    4200, Reward: [-513.663 -513.663 -513.663] [132.035], Avg: [-490.882 -490.882 -490.882] (1.0000) ({r_i: None, r_t: [-1092.823 -1092.823 -1092.823], eps: 1.0})
Step:    4300, Reward: [-523.888 -523.888 -523.888] [85.221], Avg: [-491.632 -491.632 -491.632] (1.0000) ({r_i: None, r_t: [-977.729 -977.729 -977.729], eps: 1.0})
Step:    4400, Reward: [-507.535 -507.535 -507.535] [120.503], Avg: [-491.985 -491.985 -491.985] (1.0000) ({r_i: None, r_t: [-1005.799 -1005.799 -1005.799], eps: 1.0})
Step:    4500, Reward: [-481.173 -481.173 -481.173] [77.605], Avg: [-491.750 -491.750 -491.750] (1.0000) ({r_i: None, r_t: [-1029.208 -1029.208 -1029.208], eps: 1.0})
Step:    4600, Reward: [-535.602 -535.602 -535.602] [82.278], Avg: [-492.683 -492.683 -492.683] (1.0000) ({r_i: None, r_t: [-1049.270 -1049.270 -1049.270], eps: 1.0})
Step:    4700, Reward: [-529.117 -529.117 -529.117] [171.907], Avg: [-493.442 -493.442 -493.442] (1.0000) ({r_i: None, r_t: [-973.594 -973.594 -973.594], eps: 1.0})
Step:    4800, Reward: [-533.178 -533.178 -533.178] [126.681], Avg: [-494.253 -494.253 -494.253] (1.0000) ({r_i: None, r_t: [-1021.569 -1021.569 -1021.569], eps: 1.0})
Step:    4900, Reward: [-522.668 -522.668 -522.668] [139.897], Avg: [-494.822 -494.822 -494.822] (1.0000) ({r_i: None, r_t: [-1033.440 -1033.440 -1033.440], eps: 1.0})
Step:    5000, Reward: [-495.083 -495.083 -495.083] [86.061], Avg: [-494.827 -494.827 -494.827] (1.0000) ({r_i: None, r_t: [-1042.525 -1042.525 -1042.525], eps: 1.0})
Step:    5100, Reward: [-517.191 -517.191 -517.191] [116.697], Avg: [-495.257 -495.257 -495.257] (1.0000) ({r_i: None, r_t: [-1007.798 -1007.798 -1007.798], eps: 1.0})
Step:    5200, Reward: [-551.165 -551.165 -551.165] [155.800], Avg: [-496.312 -496.312 -496.312] (1.0000) ({r_i: None, r_t: [-976.688 -976.688 -976.688], eps: 1.0})
Step:    5300, Reward: [-535.195 -535.195 -535.195] [123.426], Avg: [-497.032 -497.032 -497.032] (1.0000) ({r_i: None, r_t: [-980.122 -980.122 -980.122], eps: 1.0})
Step:    5400, Reward: [-511.542 -511.542 -511.542] [114.307], Avg: [-497.296 -497.296 -497.296] (1.0000) ({r_i: None, r_t: [-1020.863 -1020.863 -1020.863], eps: 1.0})
Step:    5500, Reward: [-525.246 -525.246 -525.246] [122.884], Avg: [-497.795 -497.795 -497.795] (1.0000) ({r_i: None, r_t: [-1015.077 -1015.077 -1015.077], eps: 1.0})
Step:    5600, Reward: [-546.918 -546.918 -546.918] [165.941], Avg: [-498.656 -498.656 -498.656] (1.0000) ({r_i: None, r_t: [-1110.222 -1110.222 -1110.222], eps: 1.0})
Step:    5700, Reward: [-537.694 -537.694 -537.694] [109.620], Avg: [-499.330 -499.330 -499.330] (1.0000) ({r_i: None, r_t: [-988.962 -988.962 -988.962], eps: 1.0})
Step:    5800, Reward: [-503.865 -503.865 -503.865] [127.364], Avg: [-499.406 -499.406 -499.406] (1.0000) ({r_i: None, r_t: [-1035.557 -1035.557 -1035.557], eps: 1.0})
Step:    5900, Reward: [-506.868 -506.868 -506.868] [118.476], Avg: [-499.531 -499.531 -499.531] (1.0000) ({r_i: None, r_t: [-1074.711 -1074.711 -1074.711], eps: 1.0})
Step:    6000, Reward: [-503.338 -503.338 -503.338] [99.806], Avg: [-499.593 -499.593 -499.593] (1.0000) ({r_i: None, r_t: [-1041.779 -1041.779 -1041.779], eps: 1.0})
Step:    6100, Reward: [-542.290 -542.290 -542.290] [152.012], Avg: [-500.282 -500.282 -500.282] (1.0000) ({r_i: None, r_t: [-1038.894 -1038.894 -1038.894], eps: 1.0})
Step:    6200, Reward: [-502.247 -502.247 -502.247] [98.712], Avg: [-500.313 -500.313 -500.313] (1.0000) ({r_i: None, r_t: [-1002.180 -1002.180 -1002.180], eps: 1.0})
Step:    6300, Reward: [-533.928 -533.928 -533.928] [97.956], Avg: [-500.838 -500.838 -500.838] (1.0000) ({r_i: None, r_t: [-1029.423 -1029.423 -1029.423], eps: 1.0})
Step:    6400, Reward: [-500.464 -500.464 -500.464] [98.143], Avg: [-500.832 -500.832 -500.832] (1.0000) ({r_i: None, r_t: [-991.798 -991.798 -991.798], eps: 1.0})
Step:    6500, Reward: [-494.089 -494.089 -494.089] [77.368], Avg: [-500.730 -500.730 -500.730] (1.0000) ({r_i: None, r_t: [-1061.122 -1061.122 -1061.122], eps: 1.0})
Step:    6600, Reward: [-523.165 -523.165 -523.165] [86.327], Avg: [-501.065 -501.065 -501.065] (1.0000) ({r_i: None, r_t: [-984.985 -984.985 -984.985], eps: 1.0})
Step:    6700, Reward: [-467.580 -467.580 -467.580] [52.830], Avg: [-500.573 -500.573 -500.573] (1.0000) ({r_i: None, r_t: [-993.039 -993.039 -993.039], eps: 1.0})
Step:    6800, Reward: [-439.734 -439.734 -439.734] [83.733], Avg: [-499.691 -499.691 -499.691] (1.0000) ({r_i: None, r_t: [-920.035 -920.035 -920.035], eps: 1.0})
Step:    6900, Reward: [-478.292 -478.292 -478.292] [117.802], Avg: [-499.385 -499.385 -499.385] (1.0000) ({r_i: None, r_t: [-1006.698 -1006.698 -1006.698], eps: 1.0})
Step:    7000, Reward: [-478.567 -478.567 -478.567] [80.813], Avg: [-499.092 -499.092 -499.092] (1.0000) ({r_i: None, r_t: [-932.648 -932.648 -932.648], eps: 1.0})
Step:    7100, Reward: [-503.102 -503.102 -503.102] [76.856], Avg: [-499.148 -499.148 -499.148] (1.0000) ({r_i: None, r_t: [-986.387 -986.387 -986.387], eps: 1.0})
Step:    7200, Reward: [-442.254 -442.254 -442.254] [89.111], Avg: [-498.368 -498.368 -498.368] (1.0000) ({r_i: None, r_t: [-1008.168 -1008.168 -1008.168], eps: 1.0})
Step:    7300, Reward: [-469.490 -469.490 -469.490] [53.378], Avg: [-497.978 -497.978 -497.978] (1.0000) ({r_i: None, r_t: [-973.396 -973.396 -973.396], eps: 1.0})
Step:    7400, Reward: [-459.865 -459.865 -459.865] [60.988], Avg: [-497.470 -497.470 -497.470] (1.0000) ({r_i: None, r_t: [-924.306 -924.306 -924.306], eps: 1.0})
Step:    7500, Reward: [-489.842 -489.842 -489.842] [92.718], Avg: [-497.370 -497.370 -497.370] (1.0000) ({r_i: None, r_t: [-934.733 -934.733 -934.733], eps: 1.0})
Step:    7600, Reward: [-485.629 -485.629 -485.629] [92.959], Avg: [-497.217 -497.217 -497.217] (1.0000) ({r_i: None, r_t: [-942.369 -942.369 -942.369], eps: 1.0})
Step:    7700, Reward: [-448.085 -448.085 -448.085] [62.756], Avg: [-496.587 -496.587 -496.587] (1.0000) ({r_i: None, r_t: [-963.476 -963.476 -963.476], eps: 1.0})
Step:    7800, Reward: [-498.373 -498.373 -498.373] [117.653], Avg: [-496.610 -496.610 -496.610] (1.0000) ({r_i: None, r_t: [-924.782 -924.782 -924.782], eps: 1.0})
Step:    7900, Reward: [-435.993 -435.993 -435.993] [65.705], Avg: [-495.852 -495.852 -495.852] (1.0000) ({r_i: None, r_t: [-888.873 -888.873 -888.873], eps: 1.0})
Step:    8000, Reward: [-451.968 -451.968 -451.968] [79.168], Avg: [-495.310 -495.310 -495.310] (1.0000) ({r_i: None, r_t: [-929.110 -929.110 -929.110], eps: 1.0})
Step:    8100, Reward: [-453.132 -453.132 -453.132] [60.416], Avg: [-494.796 -494.796 -494.796] (1.0000) ({r_i: None, r_t: [-886.491 -886.491 -886.491], eps: 1.0})
Step:    8200, Reward: [-435.553 -435.553 -435.553] [55.217], Avg: [-494.082 -494.082 -494.082] (1.0000) ({r_i: None, r_t: [-925.721 -925.721 -925.721], eps: 1.0})
Step:    8300, Reward: [-494.922 -494.922 -494.922] [71.912], Avg: [-494.092 -494.092 -494.092] (1.0000) ({r_i: None, r_t: [-913.884 -913.884 -913.884], eps: 1.0})
Step:    8400, Reward: [-414.719 -414.719 -414.719] [49.911], Avg: [-493.158 -493.158 -493.158] (1.0000) ({r_i: None, r_t: [-970.920 -970.920 -970.920], eps: 1.0})
Step:    8500, Reward: [-441.500 -441.500 -441.500] [81.947], Avg: [-492.558 -492.558 -492.558] (1.0000) ({r_i: None, r_t: [-926.514 -926.514 -926.514], eps: 1.0})
Step:    8600, Reward: [-469.658 -469.658 -469.658] [62.006], Avg: [-492.295 -492.295 -492.295] (1.0000) ({r_i: None, r_t: [-901.884 -901.884 -901.884], eps: 1.0})
Step:    8700, Reward: [-446.310 -446.310 -446.310] [77.200], Avg: [-491.772 -491.772 -491.772] (1.0000) ({r_i: None, r_t: [-933.431 -933.431 -933.431], eps: 1.0})
Step:    8800, Reward: [-447.091 -447.091 -447.091] [70.364], Avg: [-491.270 -491.270 -491.270] (1.0000) ({r_i: None, r_t: [-936.212 -936.212 -936.212], eps: 1.0})
Step:    8900, Reward: [-454.137 -454.137 -454.137] [75.297], Avg: [-490.857 -490.857 -490.857] (1.0000) ({r_i: None, r_t: [-894.019 -894.019 -894.019], eps: 1.0})
Step:    9000, Reward: [-474.027 -474.027 -474.027] [77.649], Avg: [-490.672 -490.672 -490.672] (1.0000) ({r_i: None, r_t: [-925.970 -925.970 -925.970], eps: 1.0})
Step:    9100, Reward: [-460.564 -460.564 -460.564] [63.553], Avg: [-490.345 -490.345 -490.345] (1.0000) ({r_i: None, r_t: [-910.585 -910.585 -910.585], eps: 1.0})
Step:    9200, Reward: [-457.469 -457.469 -457.469] [67.128], Avg: [-489.992 -489.992 -489.992] (1.0000) ({r_i: None, r_t: [-897.341 -897.341 -897.341], eps: 1.0})
Step:    9300, Reward: [-479.045 -479.045 -479.045] [121.360], Avg: [-489.875 -489.875 -489.875] (1.0000) ({r_i: None, r_t: [-900.624 -900.624 -900.624], eps: 1.0})
Step:    9400, Reward: [-493.159 -493.159 -493.159] [98.413], Avg: [-489.910 -489.910 -489.910] (1.0000) ({r_i: None, r_t: [-936.252 -936.252 -936.252], eps: 1.0})
Step:    9500, Reward: [-454.472 -454.472 -454.472] [72.117], Avg: [-489.541 -489.541 -489.541] (1.0000) ({r_i: None, r_t: [-943.595 -943.595 -943.595], eps: 1.0})
Step:    9600, Reward: [-459.703 -459.703 -459.703] [72.795], Avg: [-489.233 -489.233 -489.233] (1.0000) ({r_i: None, r_t: [-929.422 -929.422 -929.422], eps: 1.0})
Step:    9700, Reward: [-479.259 -479.259 -479.259] [114.321], Avg: [-489.131 -489.131 -489.131] (1.0000) ({r_i: None, r_t: [-959.689 -959.689 -959.689], eps: 1.0})
Step:    9800, Reward: [-479.562 -479.562 -479.562] [74.478], Avg: [-489.035 -489.035 -489.035] (1.0000) ({r_i: None, r_t: [-882.315 -882.315 -882.315], eps: 1.0})
Step:    9900, Reward: [-473.932 -473.932 -473.932] [75.907], Avg: [-488.883 -488.883 -488.883] (1.0000) ({r_i: None, r_t: [-981.250 -981.250 -981.250], eps: 1.0})
Step:   10000, Reward: [-519.022 -519.022 -519.022] [142.739], Avg: [-489.182 -489.182 -489.182] (1.0000) ({r_i: None, r_t: [-954.236 -954.236 -954.236], eps: 1.0})
Step:   10100, Reward: [-482.540 -482.540 -482.540] [84.757], Avg: [-489.117 -489.117 -489.117] (1.0000) ({r_i: None, r_t: [-965.013 -965.013 -965.013], eps: 1.0})
Step:   10200, Reward: [-493.389 -493.389 -493.389] [87.223], Avg: [-489.158 -489.158 -489.158] (1.0000) ({r_i: None, r_t: [-1007.276 -1007.276 -1007.276], eps: 1.0})
Step:   10300, Reward: [-444.587 -444.587 -444.587] [68.559], Avg: [-488.730 -488.730 -488.730] (1.0000) ({r_i: None, r_t: [-973.782 -973.782 -973.782], eps: 1.0})
Step:   10400, Reward: [-475.876 -475.876 -475.876] [86.456], Avg: [-488.607 -488.607 -488.607] (1.0000) ({r_i: None, r_t: [-975.838 -975.838 -975.838], eps: 1.0})
Step:   10500, Reward: [-494.833 -494.833 -494.833] [74.959], Avg: [-488.666 -488.666 -488.666] (1.0000) ({r_i: None, r_t: [-978.277 -978.277 -978.277], eps: 1.0})
Step:   10600, Reward: [-494.847 -494.847 -494.847] [78.351], Avg: [-488.724 -488.724 -488.724] (1.0000) ({r_i: None, r_t: [-994.634 -994.634 -994.634], eps: 1.0})
Step:   10700, Reward: [-480.276 -480.276 -480.276] [75.842], Avg: [-488.646 -488.646 -488.646] (1.0000) ({r_i: None, r_t: [-1040.883 -1040.883 -1040.883], eps: 1.0})
Step:   10800, Reward: [-480.559 -480.559 -480.559] [67.682], Avg: [-488.571 -488.571 -488.571] (1.0000) ({r_i: None, r_t: [-1060.550 -1060.550 -1060.550], eps: 1.0})
Step:   10900, Reward: [-461.379 -461.379 -461.379] [90.445], Avg: [-488.324 -488.324 -488.324] (1.0000) ({r_i: None, r_t: [-999.567 -999.567 -999.567], eps: 1.0})
Step:   11000, Reward: [-471.848 -471.848 -471.848] [84.888], Avg: [-488.176 -488.176 -488.176] (1.0000) ({r_i: None, r_t: [-967.687 -967.687 -967.687], eps: 1.0})
Step:   11100, Reward: [-499.796 -499.796 -499.796] [91.706], Avg: [-488.279 -488.279 -488.279] (1.0000) ({r_i: None, r_t: [-1017.566 -1017.566 -1017.566], eps: 1.0})
Step:   11200, Reward: [-486.522 -486.522 -486.522] [118.900], Avg: [-488.264 -488.264 -488.264] (1.0000) ({r_i: None, r_t: [-1033.606 -1033.606 -1033.606], eps: 1.0})
Step:   11300, Reward: [-514.583 -514.583 -514.583] [127.127], Avg: [-488.495 -488.495 -488.495] (1.0000) ({r_i: None, r_t: [-917.478 -917.478 -917.478], eps: 1.0})
Step:   11400, Reward: [-474.373 -474.373 -474.373] [102.836], Avg: [-488.372 -488.372 -488.372] (1.0000) ({r_i: None, r_t: [-1027.273 -1027.273 -1027.273], eps: 1.0})
Step:   11500, Reward: [-430.149 -430.149 -430.149] [73.290], Avg: [-487.870 -487.870 -487.870] (1.0000) ({r_i: None, r_t: [-885.258 -885.258 -885.258], eps: 1.0})
Step:   11600, Reward: [-466.408 -466.408 -466.408] [101.437], Avg: [-487.687 -487.687 -487.687] (1.0000) ({r_i: None, r_t: [-940.202 -940.202 -940.202], eps: 1.0})
Step:   11700, Reward: [-476.552 -476.552 -476.552] [88.953], Avg: [-487.592 -487.592 -487.592] (1.0000) ({r_i: None, r_t: [-953.942 -953.942 -953.942], eps: 1.0})
Step:   11800, Reward: [-467.227 -467.227 -467.227] [108.072], Avg: [-487.421 -487.421 -487.421] (1.0000) ({r_i: None, r_t: [-908.390 -908.390 -908.390], eps: 1.0})
Step:   11900, Reward: [-482.157 -482.157 -482.157] [76.071], Avg: [-487.377 -487.377 -487.377] (1.0000) ({r_i: None, r_t: [-871.652 -871.652 -871.652], eps: 1.0})
Step:   12000, Reward: [-452.946 -452.946 -452.946] [119.839], Avg: [-487.093 -487.093 -487.093] (1.0000) ({r_i: None, r_t: [-879.619 -879.619 -879.619], eps: 1.0})
Step:   12100, Reward: [-431.774 -431.774 -431.774] [60.133], Avg: [-486.639 -486.639 -486.639] (1.0000) ({r_i: None, r_t: [-897.449 -897.449 -897.449], eps: 1.0})
Step:   12200, Reward: [-415.772 -415.772 -415.772] [39.154], Avg: [-486.063 -486.063 -486.063] (1.0000) ({r_i: None, r_t: [-889.159 -889.159 -889.159], eps: 1.0})
Step:   12300, Reward: [-442.613 -442.613 -442.613] [90.435], Avg: [-485.713 -485.713 -485.713] (1.0000) ({r_i: None, r_t: [-871.457 -871.457 -871.457], eps: 1.0})
Step:   12400, Reward: [-413.494 -413.494 -413.494] [57.819], Avg: [-485.135 -485.135 -485.135] (1.0000) ({r_i: None, r_t: [-827.833 -827.833 -827.833], eps: 1.0})
Step:   12500, Reward: [-414.098 -414.098 -414.098] [59.716], Avg: [-484.571 -484.571 -484.571] (1.0000) ({r_i: None, r_t: [-873.681 -873.681 -873.681], eps: 1.0})
Step:   12600, Reward: [-407.158 -407.158 -407.158] [77.668], Avg: [-483.962 -483.962 -483.962] (1.0000) ({r_i: None, r_t: [-830.507 -830.507 -830.507], eps: 1.0})
Step:   12700, Reward: [-413.483 -413.483 -413.483] [49.036], Avg: [-483.411 -483.411 -483.411] (1.0000) ({r_i: None, r_t: [-857.318 -857.318 -857.318], eps: 1.0})
Step:   12800, Reward: [-398.928 -398.928 -398.928] [56.637], Avg: [-482.756 -482.756 -482.756] (1.0000) ({r_i: None, r_t: [-803.156 -803.156 -803.156], eps: 1.0})
Step:   12900, Reward: [-413.303 -413.303 -413.303] [86.434], Avg: [-482.222 -482.222 -482.222] (1.0000) ({r_i: None, r_t: [-842.746 -842.746 -842.746], eps: 1.0})
Step:   13000, Reward: [-418.095 -418.095 -418.095] [76.522], Avg: [-481.732 -481.732 -481.732] (1.0000) ({r_i: None, r_t: [-812.938 -812.938 -812.938], eps: 1.0})
Step:   13100, Reward: [-396.528 -396.528 -396.528] [55.634], Avg: [-481.087 -481.087 -481.087] (1.0000) ({r_i: None, r_t: [-861.799 -861.799 -861.799], eps: 1.0})
Step:   13200, Reward: [-427.596 -427.596 -427.596] [46.657], Avg: [-480.685 -480.685 -480.685] (1.0000) ({r_i: None, r_t: [-865.290 -865.290 -865.290], eps: 1.0})
Step:   13300, Reward: [-393.113 -393.113 -393.113] [48.372], Avg: [-480.031 -480.031 -480.031] (1.0000) ({r_i: None, r_t: [-874.558 -874.558 -874.558], eps: 1.0})
Step:   13400, Reward: [-400.464 -400.464 -400.464] [63.348], Avg: [-479.442 -479.442 -479.442] (1.0000) ({r_i: None, r_t: [-833.794 -833.794 -833.794], eps: 1.0})
Step:   13500, Reward: [-435.312 -435.312 -435.312] [81.904], Avg: [-479.117 -479.117 -479.117] (1.0000) ({r_i: None, r_t: [-880.727 -880.727 -880.727], eps: 1.0})
Step:   13600, Reward: [-440.024 -440.024 -440.024] [63.523], Avg: [-478.832 -478.832 -478.832] (1.0000) ({r_i: None, r_t: [-814.725 -814.725 -814.725], eps: 1.0})
Step:   13700, Reward: [-413.609 -413.609 -413.609] [65.939], Avg: [-478.359 -478.359 -478.359] (1.0000) ({r_i: None, r_t: [-819.292 -819.292 -819.292], eps: 1.0})
Step:   13800, Reward: [-451.773 -451.773 -451.773] [57.746], Avg: [-478.168 -478.168 -478.168] (1.0000) ({r_i: None, r_t: [-855.262 -855.262 -855.262], eps: 1.0})
Step:   13900, Reward: [-413.902 -413.902 -413.902] [55.447], Avg: [-477.709 -477.709 -477.709] (1.0000) ({r_i: None, r_t: [-833.072 -833.072 -833.072], eps: 1.0})
Step:   14000, Reward: [-456.135 -456.135 -456.135] [76.271], Avg: [-477.556 -477.556 -477.556] (1.0000) ({r_i: None, r_t: [-860.956 -860.956 -860.956], eps: 1.0})
Step:   14100, Reward: [-430.865 -430.865 -430.865] [66.138], Avg: [-477.227 -477.227 -477.227] (1.0000) ({r_i: None, r_t: [-860.128 -860.128 -860.128], eps: 1.0})
Step:   14200, Reward: [-420.124 -420.124 -420.124] [48.187], Avg: [-476.828 -476.828 -476.828] (1.0000) ({r_i: None, r_t: [-866.026 -866.026 -866.026], eps: 1.0})
Step:   14300, Reward: [-452.271 -452.271 -452.271] [86.147], Avg: [-476.657 -476.657 -476.657] (1.0000) ({r_i: None, r_t: [-906.068 -906.068 -906.068], eps: 1.0})
Step:   14400, Reward: [-437.532 -437.532 -437.532] [39.229], Avg: [-476.388 -476.388 -476.388] (1.0000) ({r_i: None, r_t: [-887.667 -887.667 -887.667], eps: 1.0})
Step:   14500, Reward: [-440.013 -440.013 -440.013] [39.850], Avg: [-476.138 -476.138 -476.138] (1.0000) ({r_i: None, r_t: [-944.080 -944.080 -944.080], eps: 1.0})
Step:   14600, Reward: [-438.392 -438.392 -438.392] [51.960], Avg: [-475.882 -475.882 -475.882] (1.0000) ({r_i: None, r_t: [-899.002 -899.002 -899.002], eps: 1.0})
Step:   14700, Reward: [-455.039 -455.039 -455.039] [71.160], Avg: [-475.741 -475.741 -475.741] (1.0000) ({r_i: None, r_t: [-932.932 -932.932 -932.932], eps: 1.0})
Step:   14800, Reward: [-462.018 -462.018 -462.018] [83.935], Avg: [-475.649 -475.649 -475.649] (1.0000) ({r_i: None, r_t: [-967.162 -967.162 -967.162], eps: 1.0})
Step:   14900, Reward: [-479.162 -479.162 -479.162] [62.614], Avg: [-475.672 -475.672 -475.672] (1.0000) ({r_i: None, r_t: [-937.411 -937.411 -937.411], eps: 1.0})
Step:   15000, Reward: [-488.617 -488.617 -488.617] [89.703], Avg: [-475.758 -475.758 -475.758] (1.0000) ({r_i: None, r_t: [-960.019 -960.019 -960.019], eps: 1.0})
Step:   15100, Reward: [-474.514 -474.514 -474.514] [54.483], Avg: [-475.750 -475.750 -475.750] (1.0000) ({r_i: None, r_t: [-984.574 -984.574 -984.574], eps: 1.0})
Step:   15200, Reward: [-504.125 -504.125 -504.125] [80.148], Avg: [-475.935 -475.935 -475.935] (1.0000) ({r_i: None, r_t: [-1039.283 -1039.283 -1039.283], eps: 1.0})
Step:   15300, Reward: [-540.899 -540.899 -540.899] [60.343], Avg: [-476.357 -476.357 -476.357] (1.0000) ({r_i: None, r_t: [-1006.039 -1006.039 -1006.039], eps: 1.0})
Step:   15400, Reward: [-543.296 -543.296 -543.296] [71.668], Avg: [-476.789 -476.789 -476.789] (1.0000) ({r_i: None, r_t: [-1070.571 -1070.571 -1070.571], eps: 1.0})
Step:   15500, Reward: [-562.384 -562.384 -562.384] [98.799], Avg: [-477.337 -477.337 -477.337] (1.0000) ({r_i: None, r_t: [-1090.668 -1090.668 -1090.668], eps: 1.0})
Step:   15600, Reward: [-576.732 -576.732 -576.732] [84.258], Avg: [-477.971 -477.971 -477.971] (1.0000) ({r_i: None, r_t: [-1134.817 -1134.817 -1134.817], eps: 1.0})
Step:   15700, Reward: [-576.502 -576.502 -576.502] [79.559], Avg: [-478.594 -478.594 -478.594] (1.0000) ({r_i: None, r_t: [-1154.198 -1154.198 -1154.198], eps: 1.0})
Step:   15800, Reward: [-601.882 -601.882 -601.882] [86.065], Avg: [-479.370 -479.370 -479.370] (1.0000) ({r_i: None, r_t: [-1177.206 -1177.206 -1177.206], eps: 1.0})
Step:   15900, Reward: [-545.141 -545.141 -545.141] [65.840], Avg: [-479.781 -479.781 -479.781] (1.0000) ({r_i: None, r_t: [-1105.907 -1105.907 -1105.907], eps: 1.0})
Step:   16000, Reward: [-564.356 -564.356 -564.356] [70.932], Avg: [-480.306 -480.306 -480.306] (1.0000) ({r_i: None, r_t: [-1169.351 -1169.351 -1169.351], eps: 1.0})
Step:   16100, Reward: [-573.208 -573.208 -573.208] [97.860], Avg: [-480.879 -480.879 -480.879] (1.0000) ({r_i: None, r_t: [-1170.399 -1170.399 -1170.399], eps: 1.0})
Step:   16200, Reward: [-552.010 -552.010 -552.010] [71.855], Avg: [-481.316 -481.316 -481.316] (1.0000) ({r_i: None, r_t: [-1091.028 -1091.028 -1091.028], eps: 1.0})
Step:   16300, Reward: [-525.140 -525.140 -525.140] [63.110], Avg: [-481.583 -481.583 -481.583] (1.0000) ({r_i: None, r_t: [-1166.277 -1166.277 -1166.277], eps: 1.0})
Step:   16400, Reward: [-573.036 -573.036 -573.036] [78.995], Avg: [-482.137 -482.137 -482.137] (1.0000) ({r_i: None, r_t: [-1083.133 -1083.133 -1083.133], eps: 1.0})
Step:   16500, Reward: [-533.846 -533.846 -533.846] [52.739], Avg: [-482.449 -482.449 -482.449] (1.0000) ({r_i: None, r_t: [-1091.529 -1091.529 -1091.529], eps: 1.0})
Step:   16600, Reward: [-500.134 -500.134 -500.134] [80.464], Avg: [-482.555 -482.555 -482.555] (1.0000) ({r_i: None, r_t: [-1091.030 -1091.030 -1091.030], eps: 1.0})
Step:   16700, Reward: [-512.519 -512.519 -512.519] [86.411], Avg: [-482.733 -482.733 -482.733] (1.0000) ({r_i: None, r_t: [-1048.431 -1048.431 -1048.431], eps: 1.0})
Step:   16800, Reward: [-504.335 -504.335 -504.335] [76.260], Avg: [-482.861 -482.861 -482.861] (1.0000) ({r_i: None, r_t: [-978.635 -978.635 -978.635], eps: 1.0})
Step:   16900, Reward: [-488.736 -488.736 -488.736] [99.187], Avg: [-482.895 -482.895 -482.895] (1.0000) ({r_i: None, r_t: [-1003.690 -1003.690 -1003.690], eps: 1.0})
Step:   17000, Reward: [-472.111 -472.111 -472.111] [59.114], Avg: [-482.832 -482.832 -482.832] (1.0000) ({r_i: None, r_t: [-979.728 -979.728 -979.728], eps: 1.0})
Step:   17100, Reward: [-501.404 -501.404 -501.404] [107.156], Avg: [-482.940 -482.940 -482.940] (1.0000) ({r_i: None, r_t: [-915.910 -915.910 -915.910], eps: 1.0})
Step:   17200, Reward: [-470.393 -470.393 -470.393] [99.209], Avg: [-482.868 -482.868 -482.868] (1.0000) ({r_i: None, r_t: [-958.611 -958.611 -958.611], eps: 1.0})
Step:   17300, Reward: [-486.942 -486.942 -486.942] [83.552], Avg: [-482.891 -482.891 -482.891] (1.0000) ({r_i: None, r_t: [-884.190 -884.190 -884.190], eps: 1.0})
Step:   17400, Reward: [-473.364 -473.364 -473.364] [80.329], Avg: [-482.837 -482.837 -482.837] (1.0000) ({r_i: None, r_t: [-919.658 -919.658 -919.658], eps: 1.0})
Step:   17500, Reward: [-436.711 -436.711 -436.711] [90.347], Avg: [-482.575 -482.575 -482.575] (1.0000) ({r_i: None, r_t: [-919.414 -919.414 -919.414], eps: 1.0})
Step:   17600, Reward: [-447.487 -447.487 -447.487] [64.543], Avg: [-482.376 -482.376 -482.376] (1.0000) ({r_i: None, r_t: [-928.336 -928.336 -928.336], eps: 1.0})
Step:   17700, Reward: [-446.695 -446.695 -446.695] [91.173], Avg: [-482.176 -482.176 -482.176] (1.0000) ({r_i: None, r_t: [-920.409 -920.409 -920.409], eps: 1.0})
Step:   17800, Reward: [-420.119 -420.119 -420.119] [68.845], Avg: [-481.829 -481.829 -481.829] (1.0000) ({r_i: None, r_t: [-892.715 -892.715 -892.715], eps: 1.0})
Step:   17900, Reward: [-450.585 -450.585 -450.585] [91.005], Avg: [-481.656 -481.656 -481.656] (1.0000) ({r_i: None, r_t: [-900.509 -900.509 -900.509], eps: 1.0})
Step:   18000, Reward: [-467.580 -467.580 -467.580] [71.783], Avg: [-481.578 -481.578 -481.578] (1.0000) ({r_i: None, r_t: [-978.710 -978.710 -978.710], eps: 1.0})
Step:   18100, Reward: [-433.069 -433.069 -433.069] [80.318], Avg: [-481.311 -481.311 -481.311] (1.0000) ({r_i: None, r_t: [-919.596 -919.596 -919.596], eps: 1.0})
Step:   18200, Reward: [-425.241 -425.241 -425.241] [67.073], Avg: [-481.005 -481.005 -481.005] (1.0000) ({r_i: None, r_t: [-917.716 -917.716 -917.716], eps: 1.0})
Step:   18300, Reward: [-464.470 -464.470 -464.470] [77.683], Avg: [-480.915 -480.915 -480.915] (1.0000) ({r_i: None, r_t: [-948.605 -948.605 -948.605], eps: 1.0})
Step:   18400, Reward: [-479.514 -479.514 -479.514] [82.468], Avg: [-480.908 -480.908 -480.908] (1.0000) ({r_i: None, r_t: [-941.049 -941.049 -941.049], eps: 1.0})
Step:   18500, Reward: [-518.257 -518.257 -518.257] [87.990], Avg: [-481.108 -481.108 -481.108] (1.0000) ({r_i: None, r_t: [-966.931 -966.931 -966.931], eps: 1.0})
Step:   18600, Reward: [-458.099 -458.099 -458.099] [76.423], Avg: [-480.985 -480.985 -480.985] (1.0000) ({r_i: None, r_t: [-932.652 -932.652 -932.652], eps: 1.0})
Step:   18700, Reward: [-489.278 -489.278 -489.278] [95.814], Avg: [-481.029 -481.029 -481.029] (1.0000) ({r_i: None, r_t: [-1003.487 -1003.487 -1003.487], eps: 1.0})
Step:   18800, Reward: [-461.856 -461.856 -461.856] [103.374], Avg: [-480.928 -480.928 -480.928] (1.0000) ({r_i: None, r_t: [-944.712 -944.712 -944.712], eps: 1.0})
Step:   18900, Reward: [-484.061 -484.061 -484.061] [94.810], Avg: [-480.945 -480.945 -480.945] (1.0000) ({r_i: None, r_t: [-977.385 -977.385 -977.385], eps: 1.0})
Step:   19000, Reward: [-518.694 -518.694 -518.694] [186.845], Avg: [-481.142 -481.142 -481.142] (1.0000) ({r_i: None, r_t: [-1007.432 -1007.432 -1007.432], eps: 1.0})
Step:   19100, Reward: [-485.136 -485.136 -485.136] [93.734], Avg: [-481.163 -481.163 -481.163] (1.0000) ({r_i: None, r_t: [-970.225 -970.225 -970.225], eps: 1.0})
Step:   19200, Reward: [-497.029 -497.029 -497.029] [125.969], Avg: [-481.245 -481.245 -481.245] (1.0000) ({r_i: None, r_t: [-965.697 -965.697 -965.697], eps: 1.0})
Step:   19300, Reward: [-527.597 -527.597 -527.597] [117.105], Avg: [-481.484 -481.484 -481.484] (1.0000) ({r_i: None, r_t: [-953.278 -953.278 -953.278], eps: 1.0})
Step:   19400, Reward: [-537.453 -537.453 -537.453] [64.403], Avg: [-481.771 -481.771 -481.771] (1.0000) ({r_i: None, r_t: [-1065.853 -1065.853 -1065.853], eps: 1.0})
Step:   19500, Reward: [-519.349 -519.349 -519.349] [132.712], Avg: [-481.963 -481.963 -481.963] (1.0000) ({r_i: None, r_t: [-971.054 -971.054 -971.054], eps: 1.0})
Step:   19600, Reward: [-496.784 -496.784 -496.784] [121.750], Avg: [-482.038 -482.038 -482.038] (1.0000) ({r_i: None, r_t: [-1035.702 -1035.702 -1035.702], eps: 1.0})
Step:   19700, Reward: [-522.345 -522.345 -522.345] [99.697], Avg: [-482.242 -482.242 -482.242] (1.0000) ({r_i: None, r_t: [-1034.328 -1034.328 -1034.328], eps: 1.0})
Step:   19800, Reward: [-507.953 -507.953 -507.953] [118.177], Avg: [-482.371 -482.371 -482.371] (1.0000) ({r_i: None, r_t: [-1082.497 -1082.497 -1082.497], eps: 1.0})
Step:   19900, Reward: [-505.698 -505.698 -505.698] [87.766], Avg: [-482.488 -482.488 -482.488] (1.0000) ({r_i: None, r_t: [-1031.163 -1031.163 -1031.163], eps: 1.0})
Step:   20000, Reward: [-496.645 -496.645 -496.645] [112.623], Avg: [-482.558 -482.558 -482.558] (1.0000) ({r_i: None, r_t: [-1026.356 -1026.356 -1026.356], eps: 1.0})
Step:   20100, Reward: [-501.937 -501.937 -501.937] [136.246], Avg: [-482.654 -482.654 -482.654] (1.0000) ({r_i: None, r_t: [-1059.157 -1059.157 -1059.157], eps: 1.0})
Step:   20200, Reward: [-506.825 -506.825 -506.825] [112.202], Avg: [-482.773 -482.773 -482.773] (1.0000) ({r_i: None, r_t: [-965.698 -965.698 -965.698], eps: 1.0})
Step:   20300, Reward: [-522.463 -522.463 -522.463] [99.667], Avg: [-482.968 -482.968 -482.968] (1.0000) ({r_i: None, r_t: [-1022.735 -1022.735 -1022.735], eps: 1.0})
Step:   20400, Reward: [-514.065 -514.065 -514.065] [123.346], Avg: [-483.119 -483.119 -483.119] (1.0000) ({r_i: None, r_t: [-1033.681 -1033.681 -1033.681], eps: 1.0})
Step:   20500, Reward: [-492.529 -492.529 -492.529] [70.028], Avg: [-483.165 -483.165 -483.165] (1.0000) ({r_i: None, r_t: [-1080.339 -1080.339 -1080.339], eps: 1.0})
Step:   20600, Reward: [-444.271 -444.271 -444.271] [101.716], Avg: [-482.977 -482.977 -482.977] (1.0000) ({r_i: None, r_t: [-1032.921 -1032.921 -1032.921], eps: 1.0})
Step:   20700, Reward: [-496.027 -496.027 -496.027] [89.316], Avg: [-483.040 -483.040 -483.040] (1.0000) ({r_i: None, r_t: [-938.504 -938.504 -938.504], eps: 1.0})
Step:   20800, Reward: [-492.247 -492.247 -492.247] [114.104], Avg: [-483.084 -483.084 -483.084] (1.0000) ({r_i: None, r_t: [-1017.516 -1017.516 -1017.516], eps: 1.0})
Step:   20900, Reward: [-517.904 -517.904 -517.904] [105.095], Avg: [-483.250 -483.250 -483.250] (1.0000) ({r_i: None, r_t: [-994.656 -994.656 -994.656], eps: 1.0})
Step:   21000, Reward: [-449.345 -449.345 -449.345] [67.083], Avg: [-483.089 -483.089 -483.089] (1.0000) ({r_i: None, r_t: [-940.062 -940.062 -940.062], eps: 1.0})
Step:   21100, Reward: [-458.795 -458.795 -458.795] [95.600], Avg: [-482.974 -482.974 -482.974] (1.0000) ({r_i: None, r_t: [-890.291 -890.291 -890.291], eps: 1.0})
Step:   21200, Reward: [-456.733 -456.733 -456.733] [97.552], Avg: [-482.851 -482.851 -482.851] (1.0000) ({r_i: None, r_t: [-903.771 -903.771 -903.771], eps: 1.0})
Step:   21300, Reward: [-415.198 -415.198 -415.198] [62.954], Avg: [-482.535 -482.535 -482.535] (1.0000) ({r_i: None, r_t: [-898.575 -898.575 -898.575], eps: 1.0})
Step:   21400, Reward: [-444.750 -444.750 -444.750] [64.283], Avg: [-482.359 -482.359 -482.359] (1.0000) ({r_i: None, r_t: [-809.593 -809.593 -809.593], eps: 1.0})
Step:   21500, Reward: [-449.255 -449.255 -449.255] [102.532], Avg: [-482.206 -482.206 -482.206] (1.0000) ({r_i: None, r_t: [-823.782 -823.782 -823.782], eps: 1.0})
Step:   21600, Reward: [-413.244 -413.244 -413.244] [55.447], Avg: [-481.888 -481.888 -481.888] (1.0000) ({r_i: None, r_t: [-859.882 -859.882 -859.882], eps: 1.0})
Step:   21700, Reward: [-454.666 -454.666 -454.666] [109.670], Avg: [-481.763 -481.763 -481.763] (1.0000) ({r_i: None, r_t: [-838.717 -838.717 -838.717], eps: 1.0})
Step:   21800, Reward: [-405.051 -405.051 -405.051] [68.488], Avg: [-481.413 -481.413 -481.413] (1.0000) ({r_i: None, r_t: [-847.541 -847.541 -847.541], eps: 1.0})
Step:   21900, Reward: [-416.804 -416.804 -416.804] [61.896], Avg: [-481.119 -481.119 -481.119] (1.0000) ({r_i: None, r_t: [-852.757 -852.757 -852.757], eps: 1.0})
Step:   22000, Reward: [-412.683 -412.683 -412.683] [79.385], Avg: [-480.810 -480.810 -480.810] (1.0000) ({r_i: None, r_t: [-872.836 -872.836 -872.836], eps: 1.0})
Step:   22100, Reward: [-416.090 -416.090 -416.090] [56.755], Avg: [-480.518 -480.518 -480.518] (1.0000) ({r_i: None, r_t: [-780.941 -780.941 -780.941], eps: 1.0})
Step:   22200, Reward: [-421.723 -421.723 -421.723] [76.231], Avg: [-480.254 -480.254 -480.254] (1.0000) ({r_i: None, r_t: [-845.772 -845.772 -845.772], eps: 1.0})
Step:   22300, Reward: [-413.389 -413.389 -413.389] [76.560], Avg: [-479.956 -479.956 -479.956] (1.0000) ({r_i: None, r_t: [-846.786 -846.786 -846.786], eps: 1.0})
Step:   22400, Reward: [-430.272 -430.272 -430.272] [81.115], Avg: [-479.735 -479.735 -479.735] (1.0000) ({r_i: None, r_t: [-859.710 -859.710 -859.710], eps: 1.0})
Step:   22500, Reward: [-379.846 -379.846 -379.846] [68.527], Avg: [-479.293 -479.293 -479.293] (1.0000) ({r_i: None, r_t: [-782.419 -782.419 -782.419], eps: 1.0})
Step:   22600, Reward: [-408.054 -408.054 -408.054] [68.179], Avg: [-478.979 -478.979 -478.979] (1.0000) ({r_i: None, r_t: [-736.167 -736.167 -736.167], eps: 1.0})
Step:   22700, Reward: [-403.763 -403.763 -403.763] [68.870], Avg: [-478.649 -478.649 -478.649] (1.0000) ({r_i: None, r_t: [-818.139 -818.139 -818.139], eps: 1.0})
Step:   22800, Reward: [-391.995 -391.995 -391.995] [93.693], Avg: [-478.271 -478.271 -478.271] (1.0000) ({r_i: None, r_t: [-798.426 -798.426 -798.426], eps: 1.0})
Step:   22900, Reward: [-428.888 -428.888 -428.888] [78.646], Avg: [-478.056 -478.056 -478.056] (1.0000) ({r_i: None, r_t: [-784.049 -784.049 -784.049], eps: 1.0})
Step:   23000, Reward: [-416.352 -416.352 -416.352] [62.350], Avg: [-477.789 -477.789 -477.789] (1.0000) ({r_i: None, r_t: [-786.913 -786.913 -786.913], eps: 1.0})
Step:   23100, Reward: [-396.693 -396.693 -396.693] [56.027], Avg: [-477.440 -477.440 -477.440] (1.0000) ({r_i: None, r_t: [-778.949 -778.949 -778.949], eps: 1.0})
Step:   23200, Reward: [-410.049 -410.049 -410.049] [79.105], Avg: [-477.150 -477.150 -477.150] (1.0000) ({r_i: None, r_t: [-824.803 -824.803 -824.803], eps: 1.0})
Step:   23300, Reward: [-366.168 -366.168 -366.168] [61.285], Avg: [-476.676 -476.676 -476.676] (1.0000) ({r_i: None, r_t: [-766.983 -766.983 -766.983], eps: 1.0})
Step:   23400, Reward: [-412.925 -412.925 -412.925] [72.500], Avg: [-476.405 -476.405 -476.405] (1.0000) ({r_i: None, r_t: [-772.860 -772.860 -772.860], eps: 1.0})
Step:   23500, Reward: [-423.045 -423.045 -423.045] [103.060], Avg: [-476.179 -476.179 -476.179] (1.0000) ({r_i: None, r_t: [-842.881 -842.881 -842.881], eps: 1.0})
Step:   23600, Reward: [-438.134 -438.134 -438.134] [69.847], Avg: [-476.018 -476.018 -476.018] (1.0000) ({r_i: None, r_t: [-760.293 -760.293 -760.293], eps: 1.0})
Step:   23700, Reward: [-387.354 -387.354 -387.354] [54.446], Avg: [-475.646 -475.646 -475.646] (1.0000) ({r_i: None, r_t: [-801.249 -801.249 -801.249], eps: 1.0})
Step:   23800, Reward: [-419.568 -419.568 -419.568] [59.441], Avg: [-475.411 -475.411 -475.411] (1.0000) ({r_i: None, r_t: [-784.055 -784.055 -784.055], eps: 1.0})
Step:   23900, Reward: [-430.513 -430.513 -430.513] [84.340], Avg: [-475.224 -475.224 -475.224] (1.0000) ({r_i: None, r_t: [-804.206 -804.206 -804.206], eps: 1.0})
Step:   24000, Reward: [-415.640 -415.640 -415.640] [78.968], Avg: [-474.977 -474.977 -474.977] (1.0000) ({r_i: None, r_t: [-809.432 -809.432 -809.432], eps: 1.0})
Step:   24100, Reward: [-432.140 -432.140 -432.140] [66.806], Avg: [-474.800 -474.800 -474.800] (1.0000) ({r_i: None, r_t: [-810.047 -810.047 -810.047], eps: 1.0})
Step:   24200, Reward: [-449.013 -449.013 -449.013] [57.758], Avg: [-474.694 -474.694 -474.694] (1.0000) ({r_i: None, r_t: [-788.204 -788.204 -788.204], eps: 1.0})
Step:   24300, Reward: [-418.370 -418.370 -418.370] [67.516], Avg: [-474.463 -474.463 -474.463] (1.0000) ({r_i: None, r_t: [-805.477 -805.477 -805.477], eps: 1.0})
Step:   24400, Reward: [-404.463 -404.463 -404.463] [75.905], Avg: [-474.177 -474.177 -474.177] (1.0000) ({r_i: None, r_t: [-883.273 -883.273 -883.273], eps: 1.0})
Step:   24500, Reward: [-399.521 -399.521 -399.521] [69.612], Avg: [-473.874 -473.874 -473.874] (1.0000) ({r_i: None, r_t: [-810.932 -810.932 -810.932], eps: 1.0})
Step:   24600, Reward: [-414.005 -414.005 -414.005] [69.088], Avg: [-473.631 -473.631 -473.631] (1.0000) ({r_i: None, r_t: [-836.809 -836.809 -836.809], eps: 1.0})
Step:   24700, Reward: [-404.865 -404.865 -404.865] [74.856], Avg: [-473.354 -473.354 -473.354] (1.0000) ({r_i: None, r_t: [-872.516 -872.516 -872.516], eps: 1.0})
Step:   24800, Reward: [-424.724 -424.724 -424.724] [70.537], Avg: [-473.159 -473.159 -473.159] (1.0000) ({r_i: None, r_t: [-876.216 -876.216 -876.216], eps: 1.0})
Step:   24900, Reward: [-443.536 -443.536 -443.536] [63.279], Avg: [-473.040 -473.040 -473.040] (1.0000) ({r_i: None, r_t: [-892.052 -892.052 -892.052], eps: 1.0})
Step:   25000, Reward: [-413.371 -413.371 -413.371] [49.515], Avg: [-472.802 -472.802 -472.802] (1.0000) ({r_i: None, r_t: [-869.812 -869.812 -869.812], eps: 1.0})
Step:   25100, Reward: [-436.273 -436.273 -436.273] [81.048], Avg: [-472.657 -472.657 -472.657] (1.0000) ({r_i: None, r_t: [-886.250 -886.250 -886.250], eps: 1.0})
Step:   25200, Reward: [-452.334 -452.334 -452.334] [75.111], Avg: [-472.577 -472.577 -472.577] (1.0000) ({r_i: None, r_t: [-903.182 -903.182 -903.182], eps: 1.0})
Step:   25300, Reward: [-447.951 -447.951 -447.951] [63.637], Avg: [-472.480 -472.480 -472.480] (1.0000) ({r_i: None, r_t: [-922.888 -922.888 -922.888], eps: 1.0})
Step:   25400, Reward: [-457.611 -457.611 -457.611] [55.727], Avg: [-472.422 -472.422 -472.422] (1.0000) ({r_i: None, r_t: [-969.132 -969.132 -969.132], eps: 1.0})
Step:   25500, Reward: [-464.988 -464.988 -464.988] [75.416], Avg: [-472.393 -472.393 -472.393] (1.0000) ({r_i: None, r_t: [-984.245 -984.245 -984.245], eps: 1.0})
Step:   25600, Reward: [-462.850 -462.850 -462.850] [61.044], Avg: [-472.356 -472.356 -472.356] (1.0000) ({r_i: None, r_t: [-957.415 -957.415 -957.415], eps: 1.0})
Step:   25700, Reward: [-513.264 -513.264 -513.264] [60.711], Avg: [-472.514 -472.514 -472.514] (1.0000) ({r_i: None, r_t: [-1006.893 -1006.893 -1006.893], eps: 1.0})
Step:   25800, Reward: [-512.776 -512.776 -512.776] [51.051], Avg: [-472.670 -472.670 -472.670] (1.0000) ({r_i: None, r_t: [-1014.090 -1014.090 -1014.090], eps: 1.0})
Step:   25900, Reward: [-509.424 -509.424 -509.424] [47.111], Avg: [-472.811 -472.811 -472.811] (1.0000) ({r_i: None, r_t: [-1043.125 -1043.125 -1043.125], eps: 1.0})
Step:   26000, Reward: [-518.604 -518.604 -518.604] [68.351], Avg: [-472.987 -472.987 -472.987] (1.0000) ({r_i: None, r_t: [-1073.964 -1073.964 -1073.964], eps: 1.0})
Step:   26100, Reward: [-535.078 -535.078 -535.078] [73.069], Avg: [-473.224 -473.224 -473.224] (1.0000) ({r_i: None, r_t: [-1048.961 -1048.961 -1048.961], eps: 1.0})
Step:   26200, Reward: [-510.886 -510.886 -510.886] [70.596], Avg: [-473.367 -473.367 -473.367] (1.0000) ({r_i: None, r_t: [-1048.345 -1048.345 -1048.345], eps: 1.0})
Step:   26300, Reward: [-509.936 -509.936 -509.936] [40.344], Avg: [-473.505 -473.505 -473.505] (1.0000) ({r_i: None, r_t: [-1082.939 -1082.939 -1082.939], eps: 1.0})
Step:   26400, Reward: [-499.037 -499.037 -499.037] [81.415], Avg: [-473.602 -473.602 -473.602] (1.0000) ({r_i: None, r_t: [-1038.679 -1038.679 -1038.679], eps: 1.0})
Step:   26500, Reward: [-496.762 -496.762 -496.762] [97.839], Avg: [-473.689 -473.689 -473.689] (1.0000) ({r_i: None, r_t: [-1025.844 -1025.844 -1025.844], eps: 1.0})
Step:   26600, Reward: [-453.407 -453.407 -453.407] [52.271], Avg: [-473.613 -473.613 -473.613] (1.0000) ({r_i: None, r_t: [-1012.821 -1012.821 -1012.821], eps: 1.0})
Step:   26700, Reward: [-493.233 -493.233 -493.233] [64.305], Avg: [-473.686 -473.686 -473.686] (1.0000) ({r_i: None, r_t: [-1017.327 -1017.327 -1017.327], eps: 1.0})
Step:   26800, Reward: [-474.617 -474.617 -474.617] [69.105], Avg: [-473.689 -473.689 -473.689] (1.0000) ({r_i: None, r_t: [-963.993 -963.993 -963.993], eps: 1.0})
Step:   26900, Reward: [-475.292 -475.292 -475.292] [60.073], Avg: [-473.695 -473.695 -473.695] (1.0000) ({r_i: None, r_t: [-955.080 -955.080 -955.080], eps: 1.0})
Step:   27000, Reward: [-451.010 -451.010 -451.010] [74.564], Avg: [-473.612 -473.612 -473.612] (1.0000) ({r_i: None, r_t: [-915.991 -915.991 -915.991], eps: 1.0})
Step:   27100, Reward: [-443.032 -443.032 -443.032] [43.144], Avg: [-473.499 -473.499 -473.499] (1.0000) ({r_i: None, r_t: [-923.839 -923.839 -923.839], eps: 1.0})
Step:   27200, Reward: [-468.108 -468.108 -468.108] [107.209], Avg: [-473.479 -473.479 -473.479] (1.0000) ({r_i: None, r_t: [-928.935 -928.935 -928.935], eps: 1.0})
Step:   27300, Reward: [-442.402 -442.402 -442.402] [71.398], Avg: [-473.366 -473.366 -473.366] (1.0000) ({r_i: None, r_t: [-914.801 -914.801 -914.801], eps: 1.0})
Step:   27400, Reward: [-405.337 -405.337 -405.337] [56.999], Avg: [-473.119 -473.119 -473.119] (1.0000) ({r_i: None, r_t: [-907.673 -907.673 -907.673], eps: 1.0})
Step:   27500, Reward: [-422.896 -422.896 -422.896] [57.985], Avg: [-472.937 -472.937 -472.937] (1.0000) ({r_i: None, r_t: [-873.040 -873.040 -873.040], eps: 1.0})
Step:   27600, Reward: [-452.631 -452.631 -452.631] [66.679], Avg: [-472.863 -472.863 -472.863] (1.0000) ({r_i: None, r_t: [-876.733 -876.733 -876.733], eps: 1.0})
Step:   27700, Reward: [-473.220 -473.220 -473.220] [97.963], Avg: [-472.865 -472.865 -472.865] (1.0000) ({r_i: None, r_t: [-865.651 -865.651 -865.651], eps: 1.0})
Step:   27800, Reward: [-432.265 -432.265 -432.265] [57.739], Avg: [-472.719 -472.719 -472.719] (1.0000) ({r_i: None, r_t: [-857.239 -857.239 -857.239], eps: 1.0})
Step:   27900, Reward: [-410.049 -410.049 -410.049] [65.105], Avg: [-472.495 -472.495 -472.495] (1.0000) ({r_i: None, r_t: [-806.225 -806.225 -806.225], eps: 1.0})
Step:   28000, Reward: [-433.877 -433.877 -433.877] [75.504], Avg: [-472.358 -472.358 -472.358] (1.0000) ({r_i: None, r_t: [-852.898 -852.898 -852.898], eps: 1.0})
Step:   28100, Reward: [-425.979 -425.979 -425.979] [67.339], Avg: [-472.193 -472.193 -472.193] (1.0000) ({r_i: None, r_t: [-810.394 -810.394 -810.394], eps: 1.0})
Step:   28200, Reward: [-417.390 -417.390 -417.390] [72.642], Avg: [-472.000 -472.000 -472.000] (1.0000) ({r_i: None, r_t: [-821.863 -821.863 -821.863], eps: 1.0})
Step:   28300, Reward: [-425.079 -425.079 -425.079] [70.805], Avg: [-471.835 -471.835 -471.835] (1.0000) ({r_i: None, r_t: [-868.982 -868.982 -868.982], eps: 1.0})
Step:   28400, Reward: [-395.739 -395.739 -395.739] [69.714], Avg: [-471.568 -471.568 -471.568] (1.0000) ({r_i: None, r_t: [-857.508 -857.508 -857.508], eps: 1.0})
Step:   28500, Reward: [-427.527 -427.527 -427.527] [56.344], Avg: [-471.414 -471.414 -471.414] (1.0000) ({r_i: None, r_t: [-849.222 -849.222 -849.222], eps: 1.0})
Step:   28600, Reward: [-416.055 -416.055 -416.055] [63.798], Avg: [-471.221 -471.221 -471.221] (1.0000) ({r_i: None, r_t: [-793.327 -793.327 -793.327], eps: 1.0})
Step:   28700, Reward: [-400.383 -400.383 -400.383] [73.468], Avg: [-470.975 -470.975 -470.975] (1.0000) ({r_i: None, r_t: [-772.007 -772.007 -772.007], eps: 1.0})
Step:   28800, Reward: [-418.722 -418.722 -418.722] [69.706], Avg: [-470.794 -470.794 -470.794] (1.0000) ({r_i: None, r_t: [-844.769 -844.769 -844.769], eps: 1.0})
Step:   28900, Reward: [-446.876 -446.876 -446.876] [76.073], Avg: [-470.711 -470.711 -470.711] (1.0000) ({r_i: None, r_t: [-863.132 -863.132 -863.132], eps: 1.0})
Step:   29000, Reward: [-405.795 -405.795 -405.795] [64.995], Avg: [-470.488 -470.488 -470.488] (1.0000) ({r_i: None, r_t: [-813.758 -813.758 -813.758], eps: 1.0})
Step:   29100, Reward: [-392.816 -392.816 -392.816] [71.503], Avg: [-470.222 -470.222 -470.222] (1.0000) ({r_i: None, r_t: [-802.515 -802.515 -802.515], eps: 1.0})
Step:   29200, Reward: [-394.137 -394.137 -394.137] [54.494], Avg: [-469.963 -469.963 -469.963] (1.0000) ({r_i: None, r_t: [-770.756 -770.756 -770.756], eps: 1.0})
Step:   29300, Reward: [-396.859 -396.859 -396.859] [71.786], Avg: [-469.714 -469.714 -469.714] (1.0000) ({r_i: None, r_t: [-828.930 -828.930 -828.930], eps: 1.0})
Step:   29400, Reward: [-413.697 -413.697 -413.697] [50.785], Avg: [-469.524 -469.524 -469.524] (1.0000) ({r_i: None, r_t: [-827.432 -827.432 -827.432], eps: 1.0})
Step:   29500, Reward: [-399.165 -399.165 -399.165] [36.249], Avg: [-469.286 -469.286 -469.286] (1.0000) ({r_i: None, r_t: [-789.327 -789.327 -789.327], eps: 1.0})
Step:   29600, Reward: [-427.124 -427.124 -427.124] [53.719], Avg: [-469.144 -469.144 -469.144] (1.0000) ({r_i: None, r_t: [-814.115 -814.115 -814.115], eps: 1.0})
Step:   29700, Reward: [-406.544 -406.544 -406.544] [65.072], Avg: [-468.934 -468.934 -468.934] (1.0000) ({r_i: None, r_t: [-781.389 -781.389 -781.389], eps: 1.0})
Step:   29800, Reward: [-379.876 -379.876 -379.876] [51.060], Avg: [-468.637 -468.637 -468.637] (1.0000) ({r_i: None, r_t: [-765.836 -765.836 -765.836], eps: 1.0})
Step:   29900, Reward: [-403.481 -403.481 -403.481] [57.493], Avg: [-468.419 -468.419 -468.419] (1.0000) ({r_i: None, r_t: [-833.202 -833.202 -833.202], eps: 1.0})
Step:   30000, Reward: [-401.389 -401.389 -401.389] [57.546], Avg: [-468.197 -468.197 -468.197] (1.0000) ({r_i: None, r_t: [-832.426 -832.426 -832.426], eps: 1.0})
Step:   30100, Reward: [-427.508 -427.508 -427.508] [77.807], Avg: [-468.062 -468.062 -468.062] (1.0000) ({r_i: None, r_t: [-764.913 -764.913 -764.913], eps: 1.0})
Step:   30200, Reward: [-394.594 -394.594 -394.594] [63.375], Avg: [-467.819 -467.819 -467.819] (1.0000) ({r_i: None, r_t: [-808.900 -808.900 -808.900], eps: 1.0})
Step:   30300, Reward: [-364.724 -364.724 -364.724] [48.244], Avg: [-467.480 -467.480 -467.480] (1.0000) ({r_i: None, r_t: [-812.635 -812.635 -812.635], eps: 1.0})
Step:   30400, Reward: [-408.643 -408.643 -408.643] [76.597], Avg: [-467.287 -467.287 -467.287] (1.0000) ({r_i: None, r_t: [-833.695 -833.695 -833.695], eps: 1.0})
Step:   30500, Reward: [-411.767 -411.767 -411.767] [75.063], Avg: [-467.106 -467.106 -467.106] (1.0000) ({r_i: None, r_t: [-765.757 -765.757 -765.757], eps: 1.0})
Step:   30600, Reward: [-376.464 -376.464 -376.464] [82.866], Avg: [-466.811 -466.811 -466.811] (1.0000) ({r_i: None, r_t: [-793.241 -793.241 -793.241], eps: 1.0})
Step:   30700, Reward: [-388.844 -388.844 -388.844] [63.237], Avg: [-466.558 -466.558 -466.558] (1.0000) ({r_i: None, r_t: [-800.968 -800.968 -800.968], eps: 1.0})
Step:   30800, Reward: [-396.540 -396.540 -396.540] [64.591], Avg: [-466.331 -466.331 -466.331] (1.0000) ({r_i: None, r_t: [-771.575 -771.575 -771.575], eps: 1.0})
Step:   30900, Reward: [-386.927 -386.927 -386.927] [71.714], Avg: [-466.075 -466.075 -466.075] (1.0000) ({r_i: None, r_t: [-775.870 -775.870 -775.870], eps: 1.0})
Step:   31000, Reward: [-372.015 -372.015 -372.015] [67.620], Avg: [-465.772 -465.772 -465.772] (1.0000) ({r_i: None, r_t: [-778.360 -778.360 -778.360], eps: 1.0})
Step:   31100, Reward: [-414.523 -414.523 -414.523] [82.089], Avg: [-465.608 -465.608 -465.608] (1.0000) ({r_i: None, r_t: [-776.291 -776.291 -776.291], eps: 1.0})
Step:   31200, Reward: [-382.666 -382.666 -382.666] [47.384], Avg: [-465.343 -465.343 -465.343] (1.0000) ({r_i: None, r_t: [-796.511 -796.511 -796.511], eps: 1.0})
Step:   31300, Reward: [-370.869 -370.869 -370.869] [54.359], Avg: [-465.042 -465.042 -465.042] (1.0000) ({r_i: None, r_t: [-799.985 -799.985 -799.985], eps: 1.0})
Step:   31400, Reward: [-375.456 -375.456 -375.456] [65.184], Avg: [-464.758 -464.758 -464.758] (1.0000) ({r_i: None, r_t: [-786.691 -786.691 -786.691], eps: 1.0})
Step:   31500, Reward: [-387.605 -387.605 -387.605] [57.587], Avg: [-464.514 -464.514 -464.514] (1.0000) ({r_i: None, r_t: [-761.915 -761.915 -761.915], eps: 1.0})
Step:   31600, Reward: [-391.095 -391.095 -391.095] [68.609], Avg: [-464.282 -464.282 -464.282] (1.0000) ({r_i: None, r_t: [-803.088 -803.088 -803.088], eps: 1.0})
Step:   31700, Reward: [-393.304 -393.304 -393.304] [67.720], Avg: [-464.059 -464.059 -464.059] (1.0000) ({r_i: None, r_t: [-791.763 -791.763 -791.763], eps: 1.0})
Step:   31800, Reward: [-378.015 -378.015 -378.015] [50.569], Avg: [-463.789 -463.789 -463.789] (1.0000) ({r_i: None, r_t: [-834.133 -834.133 -834.133], eps: 1.0})
Step:   31900, Reward: [-438.609 -438.609 -438.609] [95.887], Avg: [-463.711 -463.711 -463.711] (1.0000) ({r_i: None, r_t: [-791.166 -791.166 -791.166], eps: 1.0})
Step:   32000, Reward: [-372.371 -372.371 -372.371] [79.174], Avg: [-463.426 -463.426 -463.426] (1.0000) ({r_i: None, r_t: [-801.958 -801.958 -801.958], eps: 1.0})
Step:   32100, Reward: [-367.672 -367.672 -367.672] [85.369], Avg: [-463.129 -463.129 -463.129] (1.0000) ({r_i: None, r_t: [-851.152 -851.152 -851.152], eps: 1.0})
Step:   32200, Reward: [-420.064 -420.064 -420.064] [84.509], Avg: [-462.995 -462.995 -462.995] (1.0000) ({r_i: None, r_t: [-858.762 -858.762 -858.762], eps: 1.0})
Step:   32300, Reward: [-404.735 -404.735 -404.735] [68.587], Avg: [-462.815 -462.815 -462.815] (1.0000) ({r_i: None, r_t: [-819.768 -819.768 -819.768], eps: 1.0})
Step:   32400, Reward: [-401.043 -401.043 -401.043] [53.986], Avg: [-462.625 -462.625 -462.625] (1.0000) ({r_i: None, r_t: [-837.445 -837.445 -837.445], eps: 1.0})
Step:   32500, Reward: [-421.661 -421.661 -421.661] [55.438], Avg: [-462.500 -462.500 -462.500] (1.0000) ({r_i: None, r_t: [-834.853 -834.853 -834.853], eps: 1.0})
Step:   32600, Reward: [-401.517 -401.517 -401.517] [48.745], Avg: [-462.313 -462.313 -462.313] (1.0000) ({r_i: None, r_t: [-846.347 -846.347 -846.347], eps: 1.0})
Step:   32700, Reward: [-426.717 -426.717 -426.717] [64.003], Avg: [-462.205 -462.205 -462.205] (1.0000) ({r_i: None, r_t: [-807.586 -807.586 -807.586], eps: 1.0})
Step:   32800, Reward: [-419.437 -419.437 -419.437] [75.546], Avg: [-462.075 -462.075 -462.075] (1.0000) ({r_i: None, r_t: [-853.118 -853.118 -853.118], eps: 1.0})
Step:   32900, Reward: [-449.166 -449.166 -449.166] [51.971], Avg: [-462.036 -462.036 -462.036] (1.0000) ({r_i: None, r_t: [-831.654 -831.654 -831.654], eps: 1.0})
Step:   33000, Reward: [-395.157 -395.157 -395.157] [55.709], Avg: [-461.834 -461.834 -461.834] (1.0000) ({r_i: None, r_t: [-844.219 -844.219 -844.219], eps: 1.0})
Step:   33100, Reward: [-411.187 -411.187 -411.187] [73.940], Avg: [-461.681 -461.681 -461.681] (1.0000) ({r_i: None, r_t: [-828.070 -828.070 -828.070], eps: 1.0})
Step:   33200, Reward: [-425.014 -425.014 -425.014] [60.751], Avg: [-461.571 -461.571 -461.571] (1.0000) ({r_i: None, r_t: [-841.475 -841.475 -841.475], eps: 1.0})
Step:   33300, Reward: [-433.362 -433.362 -433.362] [82.182], Avg: [-461.486 -461.486 -461.486] (1.0000) ({r_i: None, r_t: [-840.195 -840.195 -840.195], eps: 1.0})
Step:   33400, Reward: [-407.688 -407.688 -407.688] [73.117], Avg: [-461.326 -461.326 -461.326] (1.0000) ({r_i: None, r_t: [-874.976 -874.976 -874.976], eps: 1.0})
Step:   33500, Reward: [-461.263 -461.263 -461.263] [83.311], Avg: [-461.326 -461.326 -461.326] (1.0000) ({r_i: None, r_t: [-862.779 -862.779 -862.779], eps: 1.0})
Step:   33600, Reward: [-402.683 -402.683 -402.683] [65.841], Avg: [-461.152 -461.152 -461.152] (1.0000) ({r_i: None, r_t: [-883.988 -883.988 -883.988], eps: 1.0})
Step:   33700, Reward: [-421.848 -421.848 -421.848] [71.590], Avg: [-461.035 -461.035 -461.035] (1.0000) ({r_i: None, r_t: [-819.859 -819.859 -819.859], eps: 1.0})
Step:   33800, Reward: [-420.110 -420.110 -420.110] [68.440], Avg: [-460.915 -460.915 -460.915] (1.0000) ({r_i: None, r_t: [-816.173 -816.173 -816.173], eps: 1.0})
Step:   33900, Reward: [-434.520 -434.520 -434.520] [61.498], Avg: [-460.837 -460.837 -460.837] (1.0000) ({r_i: None, r_t: [-861.582 -861.582 -861.582], eps: 1.0})
Step:   34000, Reward: [-410.069 -410.069 -410.069] [80.396], Avg: [-460.688 -460.688 -460.688] (1.0000) ({r_i: None, r_t: [-814.312 -814.312 -814.312], eps: 1.0})
Step:   34100, Reward: [-417.240 -417.240 -417.240] [85.014], Avg: [-460.561 -460.561 -460.561] (1.0000) ({r_i: None, r_t: [-848.343 -848.343 -848.343], eps: 1.0})
Step:   34200, Reward: [-405.616 -405.616 -405.616] [66.619], Avg: [-460.401 -460.401 -460.401] (1.0000) ({r_i: None, r_t: [-844.592 -844.592 -844.592], eps: 1.0})
Step:   34300, Reward: [-430.445 -430.445 -430.445] [86.118], Avg: [-460.314 -460.314 -460.314] (1.0000) ({r_i: None, r_t: [-841.041 -841.041 -841.041], eps: 1.0})
Step:   34400, Reward: [-450.205 -450.205 -450.205] [95.799], Avg: [-460.284 -460.284 -460.284] (1.0000) ({r_i: None, r_t: [-830.852 -830.852 -830.852], eps: 1.0})
Step:   34500, Reward: [-408.057 -408.057 -408.057] [73.253], Avg: [-460.134 -460.134 -460.134] (1.0000) ({r_i: None, r_t: [-890.194 -890.194 -890.194], eps: 1.0})
Step:   34600, Reward: [-400.352 -400.352 -400.352] [95.835], Avg: [-459.961 -459.961 -459.961] (1.0000) ({r_i: None, r_t: [-800.729 -800.729 -800.729], eps: 1.0})
Step:   34700, Reward: [-364.700 -364.700 -364.700] [48.725], Avg: [-459.688 -459.688 -459.688] (1.0000) ({r_i: None, r_t: [-828.113 -828.113 -828.113], eps: 1.0})
Step:   34800, Reward: [-397.733 -397.733 -397.733] [65.333], Avg: [-459.510 -459.510 -459.510] (1.0000) ({r_i: None, r_t: [-870.433 -870.433 -870.433], eps: 1.0})
Step:   34900, Reward: [-426.457 -426.457 -426.457] [52.927], Avg: [-459.416 -459.416 -459.416] (1.0000) ({r_i: None, r_t: [-864.674 -864.674 -864.674], eps: 1.0})
Step:   35000, Reward: [-414.314 -414.314 -414.314] [49.369], Avg: [-459.287 -459.287 -459.287] (1.0000) ({r_i: None, r_t: [-785.632 -785.632 -785.632], eps: 1.0})
Step:   35100, Reward: [-396.561 -396.561 -396.561] [59.654], Avg: [-459.109 -459.109 -459.109] (1.0000) ({r_i: None, r_t: [-842.471 -842.471 -842.471], eps: 1.0})
Step:   35200, Reward: [-424.368 -424.368 -424.368] [64.654], Avg: [-459.010 -459.010 -459.010] (1.0000) ({r_i: None, r_t: [-827.701 -827.701 -827.701], eps: 1.0})
Step:   35300, Reward: [-411.798 -411.798 -411.798] [63.295], Avg: [-458.877 -458.877 -458.877] (1.0000) ({r_i: None, r_t: [-798.137 -798.137 -798.137], eps: 1.0})
Step:   35400, Reward: [-410.712 -410.712 -410.712] [88.055], Avg: [-458.741 -458.741 -458.741] (1.0000) ({r_i: None, r_t: [-828.576 -828.576 -828.576], eps: 1.0})
Step:   35500, Reward: [-393.597 -393.597 -393.597] [51.353], Avg: [-458.558 -458.558 -458.558] (1.0000) ({r_i: None, r_t: [-811.643 -811.643 -811.643], eps: 1.0})
Step:   35600, Reward: [-358.538 -358.538 -358.538] [66.571], Avg: [-458.278 -458.278 -458.278] (1.0000) ({r_i: None, r_t: [-809.433 -809.433 -809.433], eps: 1.0})
Step:   35700, Reward: [-403.107 -403.107 -403.107] [70.832], Avg: [-458.124 -458.124 -458.124] (1.0000) ({r_i: None, r_t: [-777.614 -777.614 -777.614], eps: 1.0})
Step:   35800, Reward: [-357.346 -357.346 -357.346] [65.990], Avg: [-457.843 -457.843 -457.843] (1.0000) ({r_i: None, r_t: [-860.198 -860.198 -860.198], eps: 1.0})
Step:   35900, Reward: [-376.900 -376.900 -376.900] [50.425], Avg: [-457.619 -457.619 -457.619] (1.0000) ({r_i: None, r_t: [-789.441 -789.441 -789.441], eps: 1.0})
Step:   36000, Reward: [-407.808 -407.808 -407.808] [83.345], Avg: [-457.481 -457.481 -457.481] (1.0000) ({r_i: None, r_t: [-777.767 -777.767 -777.767], eps: 1.0})
Step:   36100, Reward: [-412.287 -412.287 -412.287] [60.902], Avg: [-457.356 -457.356 -457.356] (1.0000) ({r_i: None, r_t: [-738.392 -738.392 -738.392], eps: 1.0})
Step:   36200, Reward: [-388.183 -388.183 -388.183] [60.067], Avg: [-457.165 -457.165 -457.165] (1.0000) ({r_i: None, r_t: [-787.352 -787.352 -787.352], eps: 1.0})
Step:   36300, Reward: [-373.710 -373.710 -373.710] [69.108], Avg: [-456.936 -456.936 -456.936] (1.0000) ({r_i: None, r_t: [-787.357 -787.357 -787.357], eps: 1.0})
Step:   36400, Reward: [-415.948 -415.948 -415.948] [67.499], Avg: [-456.824 -456.824 -456.824] (1.0000) ({r_i: None, r_t: [-779.179 -779.179 -779.179], eps: 1.0})
Step:   36500, Reward: [-415.139 -415.139 -415.139] [64.509], Avg: [-456.710 -456.710 -456.710] (1.0000) ({r_i: None, r_t: [-800.113 -800.113 -800.113], eps: 1.0})
Step:   36600, Reward: [-390.619 -390.619 -390.619] [71.558], Avg: [-456.530 -456.530 -456.530] (1.0000) ({r_i: None, r_t: [-793.297 -793.297 -793.297], eps: 1.0})
Step:   36700, Reward: [-369.456 -369.456 -369.456] [94.857], Avg: [-456.293 -456.293 -456.293] (1.0000) ({r_i: None, r_t: [-827.803 -827.803 -827.803], eps: 1.0})
Step:   36800, Reward: [-374.769 -374.769 -374.769] [70.008], Avg: [-456.072 -456.072 -456.072] (1.0000) ({r_i: None, r_t: [-778.907 -778.907 -778.907], eps: 1.0})
Step:   36900, Reward: [-371.895 -371.895 -371.895] [63.772], Avg: [-455.845 -455.845 -455.845] (1.0000) ({r_i: None, r_t: [-825.727 -825.727 -825.727], eps: 1.0})
Step:   37000, Reward: [-404.047 -404.047 -404.047] [75.992], Avg: [-455.705 -455.705 -455.705] (1.0000) ({r_i: None, r_t: [-793.906 -793.906 -793.906], eps: 1.0})
Step:   37100, Reward: [-367.059 -367.059 -367.059] [70.123], Avg: [-455.467 -455.467 -455.467] (1.0000) ({r_i: None, r_t: [-765.270 -765.270 -765.270], eps: 1.0})
Step:   37200, Reward: [-388.554 -388.554 -388.554] [79.292], Avg: [-455.287 -455.287 -455.287] (1.0000) ({r_i: None, r_t: [-818.146 -818.146 -818.146], eps: 1.0})
Step:   37300, Reward: [-411.099 -411.099 -411.099] [59.482], Avg: [-455.169 -455.169 -455.169] (1.0000) ({r_i: None, r_t: [-754.035 -754.035 -754.035], eps: 1.0})
Step:   37400, Reward: [-404.144 -404.144 -404.144] [68.033], Avg: [-455.033 -455.033 -455.033] (1.0000) ({r_i: None, r_t: [-836.726 -836.726 -836.726], eps: 1.0})
Step:   37500, Reward: [-381.452 -381.452 -381.452] [77.069], Avg: [-454.837 -454.837 -454.837] (1.0000) ({r_i: None, r_t: [-797.112 -797.112 -797.112], eps: 1.0})
Step:   37600, Reward: [-396.580 -396.580 -396.580] [63.499], Avg: [-454.683 -454.683 -454.683] (1.0000) ({r_i: None, r_t: [-786.084 -786.084 -786.084], eps: 1.0})
Step:   37700, Reward: [-408.376 -408.376 -408.376] [88.886], Avg: [-454.560 -454.560 -454.560] (1.0000) ({r_i: None, r_t: [-798.141 -798.141 -798.141], eps: 1.0})
Step:   37800, Reward: [-409.569 -409.569 -409.569] [97.389], Avg: [-454.442 -454.442 -454.442] (1.0000) ({r_i: None, r_t: [-787.009 -787.009 -787.009], eps: 1.0})
Step:   37900, Reward: [-390.485 -390.485 -390.485] [70.774], Avg: [-454.273 -454.273 -454.273] (1.0000) ({r_i: None, r_t: [-772.188 -772.188 -772.188], eps: 1.0})
Step:   38000, Reward: [-380.898 -380.898 -380.898] [73.474], Avg: [-454.081 -454.081 -454.081] (1.0000) ({r_i: None, r_t: [-871.446 -871.446 -871.446], eps: 1.0})
Step:   38100, Reward: [-368.291 -368.291 -368.291] [53.190], Avg: [-453.856 -453.856 -453.856] (1.0000) ({r_i: None, r_t: [-794.595 -794.595 -794.595], eps: 1.0})
Step:   38200, Reward: [-412.372 -412.372 -412.372] [69.948], Avg: [-453.748 -453.748 -453.748] (1.0000) ({r_i: None, r_t: [-788.750 -788.750 -788.750], eps: 1.0})
Step:   38300, Reward: [-381.964 -381.964 -381.964] [60.077], Avg: [-453.561 -453.561 -453.561] (1.0000) ({r_i: None, r_t: [-803.530 -803.530 -803.530], eps: 1.0})
Step:   38400, Reward: [-393.831 -393.831 -393.831] [74.891], Avg: [-453.406 -453.406 -453.406] (1.0000) ({r_i: None, r_t: [-831.186 -831.186 -831.186], eps: 1.0})
Step:   38500, Reward: [-400.697 -400.697 -400.697] [87.582], Avg: [-453.269 -453.269 -453.269] (1.0000) ({r_i: None, r_t: [-793.950 -793.950 -793.950], eps: 1.0})
Step:   38600, Reward: [-421.116 -421.116 -421.116] [75.610], Avg: [-453.186 -453.186 -453.186] (1.0000) ({r_i: None, r_t: [-766.906 -766.906 -766.906], eps: 1.0})
Step:   38700, Reward: [-396.156 -396.156 -396.156] [66.590], Avg: [-453.039 -453.039 -453.039] (1.0000) ({r_i: None, r_t: [-820.123 -820.123 -820.123], eps: 1.0})
Step:   38800, Reward: [-393.140 -393.140 -393.140] [67.054], Avg: [-452.885 -452.885 -452.885] (1.0000) ({r_i: None, r_t: [-778.937 -778.937 -778.937], eps: 1.0})
Step:   38900, Reward: [-392.716 -392.716 -392.716] [60.363], Avg: [-452.731 -452.731 -452.731] (1.0000) ({r_i: None, r_t: [-806.876 -806.876 -806.876], eps: 1.0})
Step:   39000, Reward: [-381.743 -381.743 -381.743] [77.784], Avg: [-452.549 -452.549 -452.549] (1.0000) ({r_i: None, r_t: [-777.998 -777.998 -777.998], eps: 1.0})
Step:   39100, Reward: [-381.343 -381.343 -381.343] [64.074], Avg: [-452.368 -452.368 -452.368] (1.0000) ({r_i: None, r_t: [-771.850 -771.850 -771.850], eps: 1.0})
Step:   39200, Reward: [-388.689 -388.689 -388.689] [89.331], Avg: [-452.206 -452.206 -452.206] (1.0000) ({r_i: None, r_t: [-752.671 -752.671 -752.671], eps: 1.0})
Step:   39300, Reward: [-376.052 -376.052 -376.052] [62.517], Avg: [-452.012 -452.012 -452.012] (1.0000) ({r_i: None, r_t: [-848.284 -848.284 -848.284], eps: 1.0})
Step:   39400, Reward: [-411.981 -411.981 -411.981] [92.221], Avg: [-451.911 -451.911 -451.911] (1.0000) ({r_i: None, r_t: [-819.572 -819.572 -819.572], eps: 1.0})
Step:   39500, Reward: [-395.875 -395.875 -395.875] [51.469], Avg: [-451.770 -451.770 -451.770] (1.0000) ({r_i: None, r_t: [-808.609 -808.609 -808.609], eps: 1.0})
Step:   39600, Reward: [-433.313 -433.313 -433.313] [100.042], Avg: [-451.723 -451.723 -451.723] (1.0000) ({r_i: None, r_t: [-808.130 -808.130 -808.130], eps: 1.0})
Step:   39700, Reward: [-426.901 -426.901 -426.901] [75.585], Avg: [-451.661 -451.661 -451.661] (1.0000) ({r_i: None, r_t: [-809.995 -809.995 -809.995], eps: 1.0})
Step:   39800, Reward: [-433.704 -433.704 -433.704] [80.305], Avg: [-451.616 -451.616 -451.616] (1.0000) ({r_i: None, r_t: [-811.857 -811.857 -811.857], eps: 1.0})
Step:   39900, Reward: [-449.699 -449.699 -449.699] [112.304], Avg: [-451.611 -451.611 -451.611] (1.0000) ({r_i: None, r_t: [-845.270 -845.270 -845.270], eps: 1.0})
Step:   40000, Reward: [-400.242 -400.242 -400.242] [66.900], Avg: [-451.483 -451.483 -451.483] (1.0000) ({r_i: None, r_t: [-786.911 -786.911 -786.911], eps: 1.0})
Step:   40100, Reward: [-391.193 -391.193 -391.193] [81.769], Avg: [-451.333 -451.333 -451.333] (1.0000) ({r_i: None, r_t: [-772.730 -772.730 -772.730], eps: 1.0})
Step:   40200, Reward: [-374.612 -374.612 -374.612] [54.232], Avg: [-451.142 -451.142 -451.142] (1.0000) ({r_i: None, r_t: [-804.095 -804.095 -804.095], eps: 1.0})
Step:   40300, Reward: [-380.745 -380.745 -380.745] [63.413], Avg: [-450.968 -450.968 -450.968] (1.0000) ({r_i: None, r_t: [-812.130 -812.130 -812.130], eps: 1.0})
