Model: <class 'multiagent.coma.COMAAgent'>, Dir: simple_spread
num_envs: 16, state_size: [(1, 18), (1, 18), (1, 18)], action_size: [[1, 5], [1, 5], [1, 5]], action_space: [MultiDiscrete([5]), MultiDiscrete([5]), MultiDiscrete([5])],

import copy
import torch
import numpy as np
from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot_from_indices

LEARNING_RATE = 0.001
ALPHA = 0.99
EPS = 0.00001
LAMBDA = 0.95
DISCOUNT_RATE = 0.99
GRAD_NORM = 10
TARGET_UPDATE = 200

HIDDEN_SIZE = 512
EPS_MAX = 1.0
EPS_MIN = 0.01
EPS_DECAY = 0.999
NUM_ENVS = 16
EPISODE_LIMIT = 50
REPLAY_BATCH_SIZE = 1

class COMAAgent(PTACAgent):
	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
		super().__init__(state_size, action_size, lambda *args, **kwargs: None, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		del self.network
		n_agents = len(action_size)
		n_actions = action_size[0][-1]
		n_obs = state_size[0][-1]
		state_len = int(np.sum([np.prod(space) for space in state_size]))
		preprocess = {"actions": ("actions_onehot", [OneHot(out_dim=n_actions)])}
		groups = {"agents": n_agents}
		scheme = {
			"state": {"vshape": state_len},
			"obs": {"vshape": n_obs, "group": "agents"},
			"actions": {"vshape": (1,), "group": "agents", "dtype": torch.long},
			"reward": {"vshape": (1,)},
			"done": {"vshape": (1,), "dtype": torch.uint8},
		}
		
		self.device = torch.device('cuda' if gpu and torch.cuda.is_available() else 'cpu')
		self.replay_buffer = ReplayBuffer(scheme, groups, NUM_ENVS, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.mac = BasicMAC(self.replay_buffer.scheme, groups, n_agents, n_actions, device=self.device)
		self.learner = COMALearner(self.mac, self.replay_buffer.scheme, n_agents, n_actions, device=self.device)
		self.new_episode_batch = lambda batch_size: EpisodeBatch(scheme, groups, batch_size, EPISODE_LIMIT+1, preprocess=preprocess, device=self.device)
		self.episode_batch = self.new_episode_batch(NUM_ENVS)
		self.mac.init_hidden(batch_size=NUM_ENVS)
		self.num_envs = NUM_ENVS
		self.time = 0

	def get_action(self, state, eps=None, sample=True, numpy=True):
		self.num_envs = state[0].shape[0] if len(state[0].shape) > len(self.state_size[0]) else 1
		if np.prod(self.mac.hidden_states.shape[:-1]) != self.num_envs*len(self.action_size): self.mac.init_hidden(batch_size=self.num_envs)
		if self.episode_batch.batch_size != self.num_envs: self.episode_batch = self.new_episode_batch(self.num_envs)
		self.step = 0 if not hasattr(self, "step") else (self.step + 1)%self.replay_buffer.max_seq_length
		self.episode_batch.update({"state": [np.concatenate(state, -1)], "obs": [np.concatenate(state, -2)]}, ts=self.step)
		actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
		actions = actions.view([*state[0].shape[:-len(self.state_size[0])], actions.shape[-1]])
		return np.split(one_hot_from_indices(actions, self.action_size[0][-1]).cpu().numpy(), actions.size(-1), axis=-2)

	def train(self, state, action, next_state, reward, done):
		action, reward, done = [list(zip(*x)) for x in [action, reward, done]]
		post_transition_data = {"actions": [np.argmax(a, -1) for a in action], "reward": [np.mean(reward, -1)], "done": [np.any(done, -1)]}
		self.episode_batch.update(post_transition_data, ts=self.step)
		if np.any(done[0]):
			self.episode_batch.update({"state": [np.concatenate(next_state, -1)], "obs": [np.concatenate(next_state, -2)]}, ts=self.step)
			actions = self.mac.select_actions(self.episode_batch, t_ep=self.step, t_env=self.time, test_mode=False)
			self.episode_batch.update({"actions": actions}, ts=self.step)
			self.replay_buffer.insert_episode_batch(self.episode_batch)
			if self.replay_buffer.can_sample(REPLAY_BATCH_SIZE):
				episode_sample = self.replay_buffer.sample(REPLAY_BATCH_SIZE)
				max_ep_t = episode_sample.max_t_filled()
				episode_sample = episode_sample[:, :max_ep_t]
				if episode_sample.device != self.device: episode_sample.to(self.device)
				self.learner.train(episode_sample)
			self.episode_batch = self.new_episode_batch(state[0].shape[0])
			self.mac.init_hidden(self.num_envs)
			self.time += self.step
			self.step = 0

class OneHot():
	def __init__(self, out_dim):
		self.out_dim = out_dim

	def transform(self, tensor):
		y_onehot = tensor.new(*tensor.shape[:-1], self.out_dim).zero_()
		y_onehot.scatter_(-1, tensor.long(), 1)
		return y_onehot.float()

	def infer_output_info(self, vshape_in, dtype_in):
		return (self.out_dim,), torch.float32

class COMALearner():
	def __init__(self, mac, scheme, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.last_target_update_step = 0
		self.mac = mac
		self.critic_training_steps = 0
		self.critic = COMACritic(scheme, self.n_agents, self.n_actions).to(self.device)
		self.critic_params = list(self.critic.parameters())
		self.agent_params = list(mac.parameters())
		self.params = self.agent_params + self.critic_params
		self.target_critic = copy.deepcopy(self.critic)
		self.agent_optimiser = torch.optim.RMSprop(params=self.agent_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)
		self.critic_optimiser = torch.optim.RMSprop(params=self.critic_params, lr=LEARNING_RATE, alpha=ALPHA, eps=EPS)

	def train(self, batch):
		# Get the relevant quantities
		bs = batch.batch_size
		max_t = batch.max_seq_length
		rewards = batch["reward"][:, :-1]
		actions = batch["actions"][:, :]
		done = batch["done"][:, :-1].float()
		mask = batch["filled"][:, :-1].float()
		mask[:, 1:] = mask[:, 1:] * (1 - done[:, :-1])
		critic_mask = mask.clone()
		mask = mask.repeat(1, 1, self.n_agents).view(-1)
		q_vals = self._train_critic(batch, rewards, done, actions, critic_mask, bs, max_t)
		actions = actions[:,:-1]
		mac_out = []
		self.mac.init_hidden(batch.batch_size)
		for t in range(batch.max_seq_length - 1):
			agent_outs = self.mac.forward(batch, t=t)
			mac_out.append(agent_outs)
		mac_out = torch.stack(mac_out, dim=1)  # Concat over time
		# Mask out unavailable actions, renormalise (as in action selection)
		q_vals = q_vals.reshape(-1, self.n_actions)
		pi = mac_out.view(-1, self.n_actions)
		baseline = (pi * q_vals).sum(-1).detach()
		q_taken = torch.gather(q_vals, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken = torch.gather(pi, dim=1, index=actions.reshape(-1, 1)).squeeze(1)
		pi_taken[mask == 0] = 1.0
		log_pi_taken = torch.log(pi_taken)
		advantages = (q_taken - baseline).detach()
		coma_loss = - ((advantages * log_pi_taken) * mask).sum() / mask.sum()
		self.agent_optimiser.zero_grad()
		coma_loss.backward()
		torch.nn.utils.clip_grad_norm_(self.agent_params, GRAD_NORM)
		self.agent_optimiser.step()
		if (self.critic_training_steps - self.last_target_update_step) / TARGET_UPDATE >= 1.0:
			self._update_targets()
			self.last_target_update_step = self.critic_training_steps

	def _train_critic(self, batch, rewards, done, actions, mask, bs, max_t):
		target_q_vals = self.target_critic(batch)[:, :]
		targets_taken = torch.gather(target_q_vals, dim=3, index=actions).squeeze(3)
		targets = build_td_lambda_targets(rewards, done, mask, targets_taken, self.n_agents)
		q_vals = torch.zeros_like(target_q_vals)[:, :-1]
		for t in reversed(range(rewards.size(1))):
			mask_t = mask[:, t].expand(-1, self.n_agents)
			if mask_t.sum() == 0:
				continue
			q_t = self.critic(batch, t)
			q_vals[:, t] = q_t.view(bs, self.n_agents, self.n_actions)
			q_taken = torch.gather(q_t, dim=3, index=actions[:, t:t+1]).squeeze(3).squeeze(1)
			targets_t = targets[:, t]
			td_error = (q_taken - targets_t.detach())
			# 0-out the targets that came from padded data
			masked_td_error = td_error * mask_t
			loss = (masked_td_error ** 2).sum() / mask_t.sum()
			self.critic_optimiser.zero_grad()
			loss.backward()
			torch.nn.utils.clip_grad_norm_(self.critic_params, GRAD_NORM)
			self.critic_optimiser.step()
			self.critic_training_steps += 1
		return q_vals

	def _update_targets(self):
		self.target_critic.load_state_dict(self.critic.state_dict())

	def cuda(self):
		self.mac.cuda()
		self.critic.cuda()
		self.target_critic.cuda()

	def save_models(self, path):
		self.mac.save_models(path)
		torch.save(self.critic.state_dict(), "{}/critic.torch".format(path))
		torch.save(self.agent_optimiser.state_dict(), "{}/agent_opt.torch".format(path))
		torch.save(self.critic_optimiser.state_dict(), "{}/critic_opt.torch".format(path))

	def load_models(self, path):
		self.mac.load_models(path)
		self.critic.load_state_dict(torch.load("{}/critic.torch".format(path), map_location=lambda storage, loc: storage))
		self.target_critic.load_state_dict(self.critic.state_dict())
		self.agent_optimiser.load_state_dict(torch.load("{}/agent_opt.torch".format(path), map_location=lambda storage, loc: storage))
		self.critic_optimiser.load_state_dict(torch.load("{}/critic_opt.torch".format(path), map_location=lambda storage, loc: storage))

def build_td_lambda_targets(rewards, done, mask, target_qs, n_agents, gamma=DISCOUNT_RATE, td_lambda=LAMBDA):
	# Assumes  <target_qs > in B*T*A and <reward >, <done >, <mask > in (at least) B*T-1*1
	# Initialise  last  lambda -return  for  not  done  episodes
	ret = target_qs.new_zeros(*target_qs.shape)
	ret[:, -1] = target_qs[:, -1] * (1 - torch.sum(done, dim=1))
	# Backwards  recursive  update  of the "forward  view"
	for t in range(ret.shape[1] - 2, -1,  -1):
		ret[:, t] = td_lambda * gamma * ret[:, t + 1] + mask[:, t]*(rewards[:, t] + (1 - td_lambda) * gamma * target_qs[:, t + 1] * (1 - done[:, t]))
	# Returns lambda-return from t=0 to t=T-1, i.e. in B*T-1*A
	return ret[:, 0:-1]

class COMACritic(torch.nn.Module):
	def __init__(self, scheme, n_agents, n_actions):
		super(COMACritic, self).__init__()
		self.n_actions = n_actions
		self.n_agents = n_agents
		input_shape = self._get_input_shape(scheme)
		self.output_type = "q"
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc3 = torch.nn.Linear(HIDDEN_SIZE, self.n_actions)

	def forward(self, batch, t=None):
		inputs = self._build_inputs(batch, t=t)
		x = torch.relu(self.fc1(inputs))
		x = torch.relu(self.fc2(x))
		q = self.fc3(x)
		return q

	def _build_inputs(self, batch, t=None):
		bs = batch.batch_size
		max_t = batch.max_seq_length if t is None else 1
		ts = slice(None) if t is None else slice(t, t+1)
		inputs = []
		inputs.append(batch["state"][:, ts].unsqueeze(2).repeat(1, 1, self.n_agents, 1))
		inputs.append(batch["obs"][:, ts])
		actions = batch["actions_onehot"][:, ts].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
		agent_mask = (1 - torch.eye(self.n_agents, device=batch.device))
		agent_mask = agent_mask.view(-1, 1).repeat(1, self.n_actions).view(self.n_agents, -1)
		inputs.append(actions * agent_mask.unsqueeze(0).unsqueeze(0))
		# last actions
		if t == 0:
			inputs.append(torch.zeros_like(batch["actions_onehot"][:, 0:1]).view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		elif isinstance(t, int):
			inputs.append(batch["actions_onehot"][:, slice(t-1, t)].view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1))
		else:
			last_actions = torch.cat([torch.zeros_like(batch["actions_onehot"][:, 0:1]), batch["actions_onehot"][:, :-1]], dim=1)
			last_actions = last_actions.view(bs, max_t, 1, -1).repeat(1, 1, self.n_agents, 1)
			inputs.append(last_actions)

		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).unsqueeze(0).expand(bs, max_t, -1, -1))
		inputs = torch.cat([x.reshape(bs, max_t, self.n_agents, -1) for x in inputs], dim=-1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["state"]["vshape"]
		input_shape += scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0] * self.n_agents * 2
		input_shape += self.n_agents
		return input_shape

class BasicMAC:
	def __init__(self, scheme, groups, n_agents, n_actions, device):
		self.device = device
		self.n_agents = n_agents
		self.n_actions = n_actions
		self.agent = RNNAgent(self._get_input_shape(scheme), self.n_actions).to(self.device)
		self.action_selector = MultinomialActionSelector()
		self.hidden_states = None

	def select_actions(self, ep_batch, t_ep, t_env, bs=slice(None), test_mode=False):
		agent_outputs = self.forward(ep_batch, t_ep, test_mode=test_mode)
		chosen_actions = self.action_selector.select_action(agent_outputs[bs], t_env, test_mode=test_mode)
		return chosen_actions

	def forward(self, ep_batch, t, test_mode=False):
		agent_inputs = self._build_inputs(ep_batch, t)
		agent_outs, self.hidden_states = self.agent(agent_inputs, self.hidden_states)
		agent_outs = torch.nn.functional.softmax(agent_outs, dim=-1)
		if not test_mode:
			epsilon_action_num = agent_outs.size(-1)
			agent_outs = ((1 - self.action_selector.epsilon) * agent_outs + torch.ones_like(agent_outs).to(self.device) * self.action_selector.epsilon/epsilon_action_num)
		return agent_outs.view(ep_batch.batch_size, self.n_agents, -1)

	def init_hidden(self, batch_size):
		self.hidden_states = self.agent.init_hidden().unsqueeze(0).expand(batch_size, self.n_agents, -1)  # bav

	def parameters(self):
		return self.agent.parameters()

	def load_state(self, other_mac):
		self.agent.load_state_dict(other_mac.agent.state_dict())

	def cuda(self):
		self.agent.cuda()

	def save_models(self, path):
		torch.save(self.agent.state_dict(), "{}/agent.torch".format(path))

	def load_models(self, path):
		self.agent.load_state_dict(torch.load("{}/agent.torch".format(path), map_location=lambda storage, loc: storage))

	def _build_inputs(self, batch, t):
		bs = batch.batch_size
		inputs = []
		inputs.append(batch["obs"][:, t])  # b1av
		inputs.append(torch.zeros_like(batch["actions_onehot"][:, t]) if t==0 else batch["actions_onehot"][:, t-1])
		inputs.append(torch.eye(self.n_agents, device=batch.device).unsqueeze(0).expand(bs, -1, -1))
		inputs = torch.cat([x.reshape(bs*self.n_agents, -1) for x in inputs], dim=1)
		return inputs

	def _get_input_shape(self, scheme):
		input_shape = scheme["obs"]["vshape"]
		input_shape += scheme["actions_onehot"]["vshape"][0]
		input_shape += self.n_agents
		return input_shape

class RNNAgent(torch.nn.Module):
	def __init__(self, input_shape, output_shape):
		super(RNNAgent, self).__init__()
		self.fc1 = torch.nn.Linear(input_shape, HIDDEN_SIZE)
		self.rnn = torch.nn.GRUCell(HIDDEN_SIZE, HIDDEN_SIZE)
		self.fc2 = torch.nn.Linear(HIDDEN_SIZE, output_shape)

	def init_hidden(self):
		return self.fc1.weight.new(1, HIDDEN_SIZE).zero_()

	def forward(self, inputs, hidden_state):
		x = torch.relu(self.fc1(inputs))
		h_in = hidden_state.reshape(-1, HIDDEN_SIZE)
		h = self.rnn(x, h_in)
		q = self.fc2(h)
		return q, h

class MultinomialActionSelector():
	def __init__(self, eps_start=EPS_MAX, eps_finish=EPS_MIN, eps_decay=EPS_DECAY):
		self.schedule = DecayThenFlatSchedule(eps_start, eps_finish, EPISODE_LIMIT/(1-eps_decay), decay="linear")
		self.epsilon = self.schedule.eval(0)

	def select_action(self, agent_inputs, t_env, test_mode=False):
		self.epsilon = self.schedule.eval(t_env)
		masked_policies = agent_inputs.clone()
		picked_actions = masked_policies.max(dim=2)[1] if test_mode else torch.distributions.Categorical(masked_policies).sample().long()
		return picked_actions

class DecayThenFlatSchedule():
	def __init__(self, start, finish, time_length, decay="exp"):
		self.start = start
		self.finish = finish
		self.time_length = time_length
		self.delta = (self.start - self.finish) / self.time_length
		self.decay = decay
		if self.decay in ["exp"]:
			self.exp_scaling = (-1) * self.time_length / np.log(self.finish) if self.finish > 0 else 1

	def eval(self, T):
		if self.decay in ["linear"]:
			return max(self.finish, self.start - self.delta * T)
		elif self.decay in ["exp"]:
			return min(self.start, max(self.finish, np.exp(- T / self.exp_scaling)))

from types import SimpleNamespace as SN

class EpisodeBatch():
	def __init__(self, scheme, groups, batch_size, max_seq_length, data=None, preprocess=None, device="cpu"):
		self.scheme = scheme.copy()
		self.groups = groups
		self.batch_size = batch_size
		self.max_seq_length = max_seq_length
		self.preprocess = {} if preprocess is None else preprocess
		self.device = device

		if data is not None:
			self.data = data
		else:
			self.data = SN()
			self.data.transition_data = {}
			self.data.episode_data = {}
			self._setup_data(self.scheme, self.groups, batch_size, max_seq_length, self.preprocess)

	def _setup_data(self, scheme, groups, batch_size, max_seq_length, preprocess):
		if preprocess is not None:
			for k in preprocess:
				assert k in scheme
				new_k = preprocess[k][0]
				transforms = preprocess[k][1]
				vshape = self.scheme[k]["vshape"]
				dtype = self.scheme[k]["dtype"]
				for transform in transforms:
					vshape, dtype = transform.infer_output_info(vshape, dtype)
				self.scheme[new_k] = {"vshape": vshape, "dtype": dtype}
				if "group" in self.scheme[k]:
					self.scheme[new_k]["group"] = self.scheme[k]["group"]
				if "episode_const" in self.scheme[k]:
					self.scheme[new_k]["episode_const"] = self.scheme[k]["episode_const"]

		assert "filled" not in scheme, '"filled" is a reserved key for masking.'
		scheme.update({"filled": {"vshape": (1,), "dtype": torch.long},})

		for field_key, field_info in scheme.items():
			assert "vshape" in field_info, "Scheme must define vshape for {}".format(field_key)
			vshape = field_info["vshape"]
			episode_const = field_info.get("episode_const", False)
			group = field_info.get("group", None)
			dtype = field_info.get("dtype", torch.float32)

			if isinstance(vshape, int):
				vshape = (vshape,)
			if group:
				assert group in groups, "Group {} must have its number of members defined in _groups_".format(group)
				shape = (groups[group], *vshape)
			else:
				shape = vshape
			if episode_const:
				self.data.episode_data[field_key] = torch.zeros((batch_size, *shape), dtype=dtype).to(self.device)
			else:
				self.data.transition_data[field_key] = torch.zeros((batch_size, max_seq_length, *shape), dtype=dtype).to(self.device)

	def extend(self, scheme, groups=None):
		self._setup_data(scheme, self.groups if groups is None else groups, self.batch_size, self.max_seq_length)

	def to(self, device):
		for k, v in self.data.transition_data.items():
			self.data.transition_data[k] = v.to(device)
		for k, v in self.data.episode_data.items():
			self.data.episode_data[k] = v.to(device)
		self.device = device

	def update(self, data, bs=slice(None), ts=slice(None), mark_filled=True):
		slices = self._parse_slices((bs, ts))
		for k, v in data.items():
			if k in self.data.transition_data:
				target = self.data.transition_data
				if mark_filled:
					target["filled"][slices] = 1
					mark_filled = False
				_slices = slices
			elif k in self.data.episode_data:
				target = self.data.episode_data
				_slices = slices[0]
			else:
				raise KeyError("{} not found in transition or episode data".format(k))

			dtype = self.scheme[k].get("dtype", torch.float32)
			v = v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=dtype, device=self.device)
			self._check_safe_view(v, target[k][_slices])
			target[k][_slices] = v.view_as(target[k][_slices])

			if k in self.preprocess:
				new_k = self.preprocess[k][0]
				v = target[k][_slices]
				for transform in self.preprocess[k][1]:
					v = transform.transform(v)
				target[new_k][_slices] = v.view_as(target[new_k][_slices])

	def _check_safe_view(self, v, dest):
		idx = len(v.shape) - 1
		for s in dest.shape[::-1]:
			if v.shape[idx] != s:
				if s != 1:
					raise ValueError("Unsafe reshape of {} to {}".format(v.shape, dest.shape))
			else:
				idx -= 1

	def __getitem__(self, item):
		if isinstance(item, str):
			if item in self.data.episode_data:
				return self.data.episode_data[item]
			elif item in self.data.transition_data:
				return self.data.transition_data[item]
			else:
				raise ValueError
		elif isinstance(item, tuple) and all([isinstance(it, str) for it in item]):
			new_data = self._new_data_sn()
			for key in item:
				if key in self.data.transition_data:
					new_data.transition_data[key] = self.data.transition_data[key]
				elif key in self.data.episode_data:
					new_data.episode_data[key] = self.data.episode_data[key]
				else:
					raise KeyError("Unrecognised key {}".format(key))

			# Update the scheme to only have the requested keys
			new_scheme = {key: self.scheme[key] for key in item}
			new_groups = {self.scheme[key]["group"]: self.groups[self.scheme[key]["group"]]
						for key in item if "group" in self.scheme[key]}
			ret = EpisodeBatch(new_scheme, new_groups, self.batch_size, self.max_seq_length, data=new_data, device=self.device)
			return ret
		else:
			item = self._parse_slices(item)
			new_data = self._new_data_sn()
			for k, v in self.data.transition_data.items():
				new_data.transition_data[k] = v[item]
			for k, v in self.data.episode_data.items():
				new_data.episode_data[k] = v[item[0]]

			ret_bs = self._get_num_items(item[0], self.batch_size)
			ret_max_t = self._get_num_items(item[1], self.max_seq_length)

			ret = EpisodeBatch(self.scheme, self.groups, ret_bs, ret_max_t, data=new_data, device=self.device)
			return ret

	def _get_num_items(self, indexing_item, max_size):
		if isinstance(indexing_item, list) or isinstance(indexing_item, np.ndarray):
			return len(indexing_item)
		elif isinstance(indexing_item, slice):
			_range = indexing_item.indices(max_size)
			return 1 + (_range[1] - _range[0] - 1)//_range[2]

	def _new_data_sn(self):
		new_data = SN()
		new_data.transition_data = {}
		new_data.episode_data = {}
		return new_data

	def _parse_slices(self, items):
		parsed = []
		# Only batch slice given, add full time slice
		if (isinstance(items, slice)  # slice a:b
			or isinstance(items, int)  # int i
			or (isinstance(items, (list, np.ndarray, torch.LongTensor, torch.cuda.LongTensor)))  # [a,b,c]
			):
			items = (items, slice(None))

		# Need the time indexing to be contiguous
		if isinstance(items[1], list):
			raise IndexError("Indexing across Time must be contiguous")

		for item in items:
			#TODO: stronger checks to ensure only supported options get through
			if isinstance(item, int):
				# Convert single indices to slices
				parsed.append(slice(item, item+1))
			else:
				# Leave slices and lists as is
				parsed.append(item)
		return parsed

	def max_t_filled(self):
		return torch.sum(self.data.transition_data["filled"], 1).max(0)[0]

class ReplayBuffer(EpisodeBatch):
	def __init__(self, scheme, groups, buffer_size, max_seq_length, preprocess=None, device="cpu"):
		super(ReplayBuffer, self).__init__(scheme, groups, buffer_size, max_seq_length, preprocess=preprocess, device=device)
		self.buffer_size = buffer_size  # same as self.batch_size but more explicit
		self.buffer_index = 0
		self.episodes_in_buffer = 0

	def insert_episode_batch(self, ep_batch):
		if self.buffer_index + ep_batch.batch_size <= self.buffer_size:
			self.update(ep_batch.data.transition_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size), slice(0, ep_batch.max_seq_length), mark_filled=False)
			self.update(ep_batch.data.episode_data, slice(self.buffer_index, self.buffer_index + ep_batch.batch_size))
			self.buffer_index = (self.buffer_index + ep_batch.batch_size)
			self.episodes_in_buffer = max(self.episodes_in_buffer, self.buffer_index)
			self.buffer_index = self.buffer_index % self.buffer_size
			assert self.buffer_index < self.buffer_size
		else:
			buffer_left = self.buffer_size - self.buffer_index
			self.insert_episode_batch(ep_batch[0:buffer_left, :])
			self.insert_episode_batch(ep_batch[buffer_left:, :])

	def can_sample(self, batch_size):
		return self.episodes_in_buffer >= batch_size

	def sample(self, batch_size):
		assert self.can_sample(batch_size)
		if self.episodes_in_buffer == batch_size:
			return self[:batch_size]
		else:
			ep_ids = np.random.choice(self.episodes_in_buffer, batch_size, replace=False)
			return self[ep_ids]


# import torch
# import random
# import numpy as np
# from utils.wrappers import ParallelAgent
# from utils.network import PTNetwork, PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, NUM_STEPS, EPS_MIN, one_hot, gsoftmax

# EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
# INPUT_LAYER = 64
# ACTOR_HIDDEN = 64
# CRITIC_HIDDEN = 64

# class COMAActor(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
# 		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.recurrent = torch.nn.GRUCell(ACTOR_HIDDEN, ACTOR_HIDDEN)
# 		self.action_probs = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
# 		self.init_hidden()

# 	def forward(self, state, sample=True):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		out_dims = state.size()[:-1]
# 		state = state.view(int(np.prod(out_dims)), -1)
# 		if self.hidden.size(0) != state.size(0): self.init_hidden(state.size(0))
# 		self.hidden = self.recurrent(state, self.hidden)
# 		action_probs = gsoftmax(self.action_probs(self.hidden), hard=False)
# 		action_probs = action_probs.view(*out_dims, -1)
# 		return action_probs

# 	def init_hidden(self, batch_size=1):
# 		self.hidden = torch.zeros([batch_size, ACTOR_HIDDEN])

# class COMACritic(torch.nn.Module):
# 	def __init__(self, state_size, action_size):
# 		super().__init__()
# 		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
# 		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
# 		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
# 		self.q_values = torch.nn.Linear(CRITIC_HIDDEN, action_size[-1])
# 		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

# 	def forward(self, state):
# 		state = self.layer1(state).relu()
# 		state = self.layer2(state).relu()
# 		state = self.layer3(state).relu()
# 		q_values = self.q_values(state)
# 		return q_values

# class COMANetwork(PTNetwork):
# 	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
# 		super().__init__(gpu=gpu)
# 		self.state_size = [state_size] if type(state_size[0]) in [int, np.int32] else state_size
# 		self.action_size = [action_size] if type(action_size[0]) in [int, np.int32] else action_size
# 		self.n_agents = lambda size: 1 if len(size)==1 else size[0]
# 		make_actor = lambda s_size,a_size: COMAActor([s_size[-1] + a_size[-1] + self.n_agents(s_size)], a_size)
# 		make_critic = lambda s_size,a_size: COMACritic([np.sum([np.prod(s) for s in self.state_size]) + 2*np.sum([np.prod(a) for a in self.action_size]) + s_size[-1] + self.n_agents(s_size)], a_size)
# 		self.models = [PTACNetwork(s_size, a_size, make_actor, make_critic, lr=lr, gpu=gpu, load=load) for s_size,a_size in zip(self.state_size, self.action_size)]
# 		if load: self.load_model(load)
		
# 	def get_action_probs(self, state, sample=True, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			action = [model.actor_local(s.to(self.device), sample) for s,model in zip(state, self.models)]
# 			return [a.cpu().numpy().astype(np.float32) for a in action] if numpy else action

# 	def get_value(self, state, grad=True, numpy=False):
# 		with torch.enable_grad() if grad else torch.no_grad():
# 			q_values = [model.critic_local(s.to(self.device)) for s,model in zip(state, self.models)]
# 			return [q.cpu().numpy() for q in q_values] if numpy else q_values

# 	def optimize(self, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 		for model,action,actor_input,critic_input,q_value,q_target in zip(self.models, actions, actor_inputs, critic_inputs, q_values, q_targets):
# 			for t in reversed(range(q_target.size(0))):
# 				q_value[t] = model.critic_local(critic_input[t])
# 				q_select = torch.gather(q_value[t], dim=-1, index=action[t].argmax(-1, keepdims=True)).squeeze(-1)
# 				critic_loss = (q_select - q_target[t].detach()).pow(2)
# 				model.step(model.critic_optimizer, critic_loss.mean(), retain=t>0)

# 			hidden = model.actor_local.hidden
# 			action_probs = torch.stack([model.actor_local(actor_input[t]) for t in range(q_target.size(0))], dim=0)
# 			baseline = (action_probs * q_value[:-1]).sum(-1, keepdims=True).detach()
# 			q_selected = torch.gather(q_value[:-1], dim=-1, index=action[:-1].argmax(-1, keepdims=True))
# 			log_probs = torch.gather(action_probs, dim=-1, index=action[:-1].argmax(-1, keepdims=True)).log()
# 			advantages = (q_selected - baseline).detach()
# 			actor_loss = (advantages * log_probs).sum() + 0.001*action_probs.pow(2).mean()
# 			model.step(model.actor_optimizer, actor_loss.mean())
# 			model.actor_local.hidden = hidden

# 	def save_model(self, dirname="pytorch", name="best"):
# 		[model.save_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]
		
# 	def load_model(self, dirname="pytorch", name="best"):
# 		[model.load_model("coma", dirname, f"{name}_{i}") for i,model in enumerate(self.models)]

# class COMAAgent(PTACAgent):
# 	def __init__(self, state_size, action_size, update_freq=NUM_STEPS, lr=LEARN_RATE, decay=EPS_DECAY, gpu=True, load=None):
# 		super().__init__(state_size, action_size, COMANetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)

# 	def get_action(self, state, eps=None, sample=True, numpy=True):
# 		eps = self.eps if eps is None else eps
# 		action_random = super().get_action(state)
# 		if not hasattr(self, "action"): self.action = [np.zeros_like(a) for a in action_random]
# 		actor_inputs = []
# 		state_list = self.to_tensor(state)
# 		state_list = [state_list] if type(state_list) != list else state_list
# 		for i,(state,last_a,s_size,a_size) in enumerate(zip(state_list, self.action, self.state_size, self.action_size)):
# 			n_agents = self.network.n_agents(s_size)
# 			last_action = last_a if len(state.shape)-len(s_size) == len(last_a.shape)-len(a_size) else np.zeros_like(action_random[i])
# 			agent_ids = np.eye(n_agents) if len(state.shape)==len(s_size) else np.repeat(np.expand_dims(np.eye(n_agents), 0), repeats=state.shape[0], axis=0)
# 			actor_input = torch.tensor(np.concatenate([state, last_action, agent_ids], axis=-1), device=self.network.device).float()
# 			actor_inputs.append(actor_input)
# 		action_greedy = self.network.get_action_probs(actor_inputs, sample=sample, grad=False, numpy=numpy)
# 		action = action_random if numpy and random.random() < eps else action_greedy
# 		if numpy: self.action = action
# 		return action

# 	def train(self, state, action, next_state, reward, done):
# 		self.buffer.append((state, action, reward, done))
# 		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
# 			states, actions, rewards, dones = map(self.to_tensor, zip(*self.buffer))
# 			self.buffer.clear()

# 			n_agents = [self.network.n_agents(a_size) for a_size in self.action_size]
# 			states = [torch.cat([s, ns.unsqueeze(0)], dim=0) for s,ns in zip(states, self.to_tensor(next_state))]
# 			actions = [torch.cat([a, na.unsqueeze(0)], dim=0) for a,na in zip(actions, self.get_action(next_state, numpy=False))]
# 			states_joint = torch.cat([s.view(*s.size()[:-len(s_size)], np.prod(s_size)) for s,s_size in zip(states, self.state_size)], dim=-1)
# 			actions_one_hot = [one_hot(a) for a in actions]
# 			actions_one_hot_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(actions_one_hot, self.action_size)], dim=-1)
# 			last_actions = [torch.cat([torch.zeros_like(a[0:1]), a[:-1]], dim=0) for a in actions_one_hot]
# 			last_actions_joint = torch.cat([a.view(*a.shape[:-len(a_size)], np.prod(a_size)) for a,a_size in zip(last_actions, self.action_size)], dim=-1)
# 			agent_mask = [(1-torch.eye(n_agent)).view(-1, 1).repeat(1, a_size[-1]).view(n_agent, -1) for a_size,n_agent in zip(self.action_size, n_agents)]
# 			action_mask = torch.ones([1, 1, np.sum(n_agents), np.sum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])])
# 			cols, rows = [0, *np.cumsum(n_agents)], [0, *np.cumsum([n_agent*a_size[-1] for a_size,n_agent in zip(self.action_size, n_agents)])]
# 			for i,mask in enumerate(agent_mask): action_mask[...,cols[i]:cols[i+1], rows[i]:rows[i+1]] = mask

# 			states_joint, actions_joint, last_actions_joint = [x.unsqueeze(-2).repeat_interleave(action_mask.shape[-2], dim=-2) for x in [states_joint, actions_one_hot_joint, last_actions_joint]]
# 			joint_inputs = torch.cat([states_joint, actions_joint * action_mask, last_actions_joint], dim=-1).split(n_agents, dim=-2)
# 			agent_ids = [torch.eye(self.network.n_agents(a_size)).unsqueeze(0).unsqueeze(0).expand(*a.shape[:2], -1, -1) for a_size, a in zip(self.action_size, actions)]
# 			critic_inputs = [torch.cat([joint_input, state, agent_id], dim=-1) for joint_input,state,agent_id in zip(joint_inputs, states, agent_ids)]
# 			actor_inputs = [torch.cat([state, last_action, agent_id], dim=-1) for state,last_action,agent_id in zip(states, last_actions, agent_ids)]

# 			q_values = self.network.get_value(critic_inputs, grad=False)
# 			q_selecteds = [torch.gather(q_value, dim=-1, index=a.argmax(-1, keepdims=True)).squeeze(-1) for q_value,a in zip(q_values,actions)]
# 			q_targets = [self.compute_gae(q_selected[-1], reward.unsqueeze(-1), done.unsqueeze(-1), q_selected[:-1])[0] for q_selected,reward,done in zip(q_selecteds, rewards, dones)]
# 			self.network.optimize(actions, actor_inputs, critic_inputs, q_values, q_targets)
# 		if np.any(done[0]): self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step

import gym
import argparse
import numpy as np
import particle_envs.make_env as pgym
import football.gfootball.env as ggym
from models.ppo import PPOAgent
from models.ddqn import DDQNAgent
from models.ddpg import DDPGAgent
from models.rand import RandomAgent
from multiagent.coma import COMAAgent
from multiagent.maddpg import MADDPGAgent
from multiagent.mappo import MAPPOAgent
from utils.wrappers import ParallelAgent, DoubleAgent, ParticleTeamEnv, FootballTeamEnv
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.misc import Logger, rollout
np.set_printoptions(precision=3)

gym_envs = ["CartPole-v0", "MountainCar-v0", "Acrobot-v1", "Pendulum-v0", "MountainCarContinuous-v0", "CarRacing-v0", "BipedalWalker-v2", "BipedalWalkerHardcore-v2", "LunarLander-v2", "LunarLanderContinuous-v2"]
gfb_envs = ["academy_empty_goal_close", "academy_empty_goal", "academy_run_to_score", "academy_run_to_score_with_keeper", "academy_single_goal_versus_lazy", "academy_3_vs_1_with_keeper", "1_vs_1_easy", "3_vs_3_custom", "5_vs_5", "11_vs_11_stochastic", "test_example_multiagent"]
ptc_envs = ["simple_adversary", "simple_speaker_listener", "simple_tag", "simple_spread", "simple_push"]
env_name = gym_envs[0]
env_name = gfb_envs[-4]
env_name = ptc_envs[-2]

def make_env(env_name=env_name, log=False, render=False):
	if env_name in gym_envs: return gym.make(env_name)
	if env_name in ptc_envs: return ParticleTeamEnv(pgym.make_env(env_name))
	reps = ["pixels", "pixels_gray", "extracted", "simple115"]
	multiagent_args = {"number_of_left_players_agent_controls":3, "number_of_right_players_agent_controls":3} if env_name == "3_vs_3_custom" else {}
	env = ggym.create_environment(env_name=env_name, representation=reps[3], logdir='/football/logs/', render=render, **multiagent_args)
	if log: print(f"State space: {env.observation_space.shape} \nAction space: {env.action_space}")
	return FootballTeamEnv(env)

def run(model, steps=10000, ports=16, env_name=env_name, eval_at=1000, checkpoint=True, save_best=False, log=True, render=False):
	num_envs = len(ports) if type(ports) == list else min(ports, 64)
	envs = EnvManager(lambda: make_env(env_name), ports) if type(ports) == list else EnsembleEnv(lambda: make_env(env_name), ports)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, num_envs=num_envs, gpu=True, agent2=RandomAgent) 
	logger = Logger(model, env_name, num_envs=num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space)
	states = envs.reset()
	total_rewards = []
	for s in range(steps):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if np.any(dones[0]):
			rollouts = [rollout(envs.env, agent.reset(1), render=render) for _ in range(5)]
			test_reward = np.mean(rollouts, axis=0) - np.std(rollouts, axis=0)
			total_rewards.append(test_reward)
			if checkpoint: agent.save_model(env_name, "checkpoint")
			if save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=0)): agent.save_model(env_name)
			if log: logger.log(f"Step: {s}, Reward: {test_reward+np.std(rollouts, axis=0)} [{np.std(rollouts):.4f}], Avg: {np.mean(total_rewards, axis=0)} ({agent.agent.eps:.3f})")
			agent.reset(num_envs)

def trial(model, env_name, render):
	envs = EnsembleEnv(lambda: make_env(env_name, log=True, render=render), 0)
	agent = DoubleAgent(envs.state_size, envs.action_size, model, gpu=False, load=f"{env_name}", agent2=RandomAgent)
	print(f"Reward: {rollout(envs.env, agent, eps=0.02, render=True)}")
	envs.close()

def parse_args():
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--workerports", type=int, default=[16], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--selfport", type=int, default=None, help="Which port to listen on (as a worker server)")
	parser.add_argument("--model", type=str, default="maddpg", help="Which reinforcement learning algorithm to use")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	parser.add_argument("--render", action="store_true", help="Whether to render during training")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run")
	parser.add_argument("--env", type=str, default="", help="Name of env to use")
	return parser.parse_args()

if __name__ == "__main__":
	args = parse_args()
	env_name = env_name if args.env not in [*gym_envs, *gfb_envs, *ptc_envs] else args.env
	models = {"ddpg":DDPGAgent, "ppo":PPOAgent, "ddqn":DDQNAgent, "maddpg":MADDPGAgent, "mappo":MAPPOAgent, "coma":COMAAgent, "rand":RandomAgent}
	model = models[args.model] if args.model in models else RandomAgent
	if args.trial:
		trial(model, env_name=env_name, render=args.render)
	elif args.selfport is not None:
		EnvWorker(args.selfport, make_env).start()
	else:
		run(model, args.steps, args.workerports[0] if len(args.workerports)==1 else args.workerports, env_name=env_name, render=args.render)

Step: 49, Reward: [-480.846 -480.846 -480.846] [66.9165], Avg: [-547.763 -547.763 -547.763] (1.000)
Step: 99, Reward: [-495.515 -495.515 -495.515] [29.7195], Avg: [-536.499 -536.499 -536.499] (1.000)
Step: 149, Reward: [-442.537 -442.537 -442.537] [40.3852], Avg: [-518.64 -518.64 -518.64] (1.000)
Step: 199, Reward: [-484.795 -484.795 -484.795] [75.8058], Avg: [-529.13 -529.13 -529.13] (1.000)
Step: 249, Reward: [-401.321 -401.321 -401.321] [15.3078], Avg: [-506.63 -506.63 -506.63] (1.000)
Step: 299, Reward: [-505.275 -505.275 -505.275] [35.9235], Avg: [-512.391 -512.391 -512.391] (1.000)
Step: 349, Reward: [-480.613 -480.613 -480.613] [55.5654], Avg: [-515.789 -515.789 -515.789] (1.000)
Step: 399, Reward: [-418.426 -418.426 -418.426] [49.6766], Avg: [-509.828 -509.828 -509.828] (1.000)
Step: 449, Reward: [-544.435 -544.435 -544.435] [158.7585], Avg: [-531.313 -531.313 -531.313] (1.000)
Step: 499, Reward: [-564.128 -564.128 -564.128] [121.6163], Avg: [-546.757 -546.757 -546.757] (1.000)
Step: 549, Reward: [-440.754 -440.754 -440.754] [80.4961], Avg: [-544.438 -544.438 -544.438] (1.000)
Step: 599, Reward: [-443.772 -443.772 -443.772] [37.1335], Avg: [-539.143 -539.143 -539.143] (1.000)
Step: 649, Reward: [-509.195 -509.195 -509.195] [75.7113], Avg: [-542.664 -542.664 -542.664] (1.000)
Step: 699, Reward: [-446.322 -446.322 -446.322] [58.1537], Avg: [-539.936 -539.936 -539.936] (1.000)
Step: 749, Reward: [-550.457 -550.457 -550.457] [127.4517], Avg: [-549.134 -549.134 -549.134] (1.000)
Step: 799, Reward: [-478.904 -478.904 -478.904] [77.6938], Avg: [-549.601 -549.601 -549.601] (1.000)
Step: 849, Reward: [-429.798 -429.798 -429.798] [61.4062], Avg: [-546.165 -546.165 -546.165] (1.000)
Step: 899, Reward: [-461.865 -461.865 -461.865] [107.5766], Avg: [-547.459 -547.459 -547.459] (1.000)
Step: 949, Reward: [-460.351 -460.351 -460.351] [69.4178], Avg: [-546.528 -546.528 -546.528] (1.000)
Step: 999, Reward: [-496.213 -496.213 -496.213] [35.5202], Avg: [-545.788 -545.788 -545.788] (1.000)
Step: 1049, Reward: [-437.366 -437.366 -437.366] [68.2984], Avg: [-543.877 -543.877 -543.877] (1.000)
Step: 1099, Reward: [-478.805 -478.805 -478.805] [72.4620], Avg: [-544.213 -544.213 -544.213] (1.000)
Step: 1149, Reward: [-495.508 -495.508 -495.508] [98.5047], Avg: [-546.378 -546.378 -546.378] (1.000)
Step: 1199, Reward: [-476.71 -476.71 -476.71] [79.1705], Avg: [-546.774 -546.774 -546.774] (1.000)
Step: 1249, Reward: [-451.363 -451.363 -451.363] [9.5410], Avg: [-543.339 -543.339 -543.339] (1.000)
Step: 1299, Reward: [-505.251 -505.251 -505.251] [73.7265], Avg: [-544.71 -544.71 -544.71] (1.000)
Step: 1349, Reward: [-533.289 -533.289 -533.289] [139.6162], Avg: [-549.458 -549.458 -549.458] (1.000)
Step: 1399, Reward: [-518.116 -518.116 -518.116] [86.8163], Avg: [-551.439 -551.439 -551.439] (1.000)
Step: 1449, Reward: [-510.849 -510.849 -510.849] [117.2823], Avg: [-554.084 -554.084 -554.084] (1.000)
Step: 1499, Reward: [-482.177 -482.177 -482.177] [39.3315], Avg: [-552.998 -552.998 -552.998] (1.000)
Step: 1549, Reward: [-524.264 -524.264 -524.264] [53.0812], Avg: [-553.783 -553.783 -553.783] (1.000)
Step: 1599, Reward: [-516.762 -516.762 -516.762] [63.3022], Avg: [-554.605 -554.605 -554.605] (1.000)
Step: 1649, Reward: [-496.849 -496.849 -496.849] [79.4521], Avg: [-555.262 -555.262 -555.262] (1.000)
Step: 1699, Reward: [-457.605 -457.605 -457.605] [82.0954], Avg: [-554.804 -554.804 -554.804] (1.000)
Step: 1749, Reward: [-491.95 -491.95 -491.95] [93.4511], Avg: [-555.679 -555.679 -555.679] (1.000)
Step: 1799, Reward: [-450.933 -450.933 -450.933] [68.1727], Avg: [-554.663 -554.663 -554.663] (1.000)
Step: 1849, Reward: [-472.099 -472.099 -472.099] [73.3462], Avg: [-554.414 -554.414 -554.414] (1.000)
Step: 1899, Reward: [-480.654 -480.654 -480.654] [96.5333], Avg: [-555.013 -555.013 -555.013] (1.000)
Step: 1949, Reward: [-464.178 -464.178 -464.178] [74.2430], Avg: [-554.587 -554.587 -554.587] (1.000)
Step: 1999, Reward: [-562.492 -562.492 -562.492] [117.3120], Avg: [-557.718 -557.718 -557.718] (1.000)
Step: 2049, Reward: [-478.369 -478.369 -478.369] [89.3557], Avg: [-557.962 -557.962 -557.962] (1.000)
Step: 2099, Reward: [-508.694 -508.694 -508.694] [127.3213], Avg: [-559.82 -559.82 -559.82] (1.000)
Step: 2149, Reward: [-503.971 -503.971 -503.971] [23.3415], Avg: [-559.064 -559.064 -559.064] (1.000)
Step: 2199, Reward: [-445.611 -445.611 -445.611] [73.6280], Avg: [-558.159 -558.159 -558.159] (1.000)
Step: 2249, Reward: [-611.695 -611.695 -611.695] [96.4646], Avg: [-561.493 -561.493 -561.493] (1.000)
Step: 2299, Reward: [-607.212 -607.212 -607.212] [98.6663], Avg: [-564.631 -564.631 -564.631] (1.000)
Step: 2349, Reward: [-515.303 -515.303 -515.303] [93.8576], Avg: [-565.579 -565.579 -565.579] (1.000)
Step: 2399, Reward: [-523.138 -523.138 -523.138] [72.0106], Avg: [-566.195 -566.195 -566.195] (1.000)
Step: 2449, Reward: [-461.25 -461.25 -461.25] [114.7556], Avg: [-566.395 -566.395 -566.395] (1.000)
Step: 2499, Reward: [-478.316 -478.316 -478.316] [34.3387], Avg: [-565.32 -565.32 -565.32] (1.000)
Step: 2549, Reward: [-522.972 -522.972 -522.972] [92.1401], Avg: [-566.297 -566.297 -566.297] (1.000)
Step: 2599, Reward: [-467.924 -467.924 -467.924] [98.1273], Avg: [-566.292 -566.292 -566.292] (1.000)
Step: 2649, Reward: [-511.912 -511.912 -511.912] [55.7637], Avg: [-566.318 -566.318 -566.318] (1.000)
Step: 2699, Reward: [-501.359 -501.359 -501.359] [127.4527], Avg: [-567.475 -567.475 -567.475] (1.000)
Step: 2749, Reward: [-440.335 -440.335 -440.335] [119.2679], Avg: [-567.332 -567.332 -567.332] (1.000)
Step: 2799, Reward: [-477.123 -477.123 -477.123] [99.8520], Avg: [-567.504 -567.504 -567.504] (1.000)
Step: 2849, Reward: [-567.365 -567.365 -567.365] [106.2195], Avg: [-569.365 -569.365 -569.365] (1.000)
Step: 2899, Reward: [-480.716 -480.716 -480.716] [74.0592], Avg: [-569.114 -569.114 -569.114] (1.000)
Step: 2949, Reward: [-430.526 -430.526 -430.526] [49.7415], Avg: [-567.608 -567.608 -567.608] (1.000)
Step: 2999, Reward: [-445.264 -445.264 -445.264] [70.4449], Avg: [-566.743 -566.743 -566.743] (1.000)
Step: 3049, Reward: [-480.054 -480.054 -480.054] [90.7986], Avg: [-566.81 -566.81 -566.81] (1.000)
Step: 3099, Reward: [-498.399 -498.399 -498.399] [76.0376], Avg: [-566.933 -566.933 -566.933] (1.000)
Step: 3149, Reward: [-545.867 -545.867 -545.867] [54.6666], Avg: [-567.467 -567.467 -567.467] (1.000)
Step: 3199, Reward: [-486.708 -486.708 -486.708] [38.9572], Avg: [-566.814 -566.814 -566.814] (1.000)
Step: 3249, Reward: [-496.342 -496.342 -496.342] [88.6430], Avg: [-567.093 -567.093 -567.093] (1.000)
Step: 3299, Reward: [-536.233 -536.233 -536.233] [98.5784], Avg: [-568.119 -568.119 -568.119] (1.000)
Step: 3349, Reward: [-469.166 -469.166 -469.166] [19.8789], Avg: [-566.939 -566.939 -566.939] (1.000)
Step: 3399, Reward: [-594.591 -594.591 -594.591] [113.0464], Avg: [-569.008 -569.008 -569.008] (1.000)
Step: 3449, Reward: [-485.94 -485.94 -485.94] [121.0453], Avg: [-569.558 -569.558 -569.558] (1.000)
Step: 3499, Reward: [-502.863 -502.863 -502.863] [97.6937], Avg: [-570.001 -570.001 -570.001] (1.000)
Step: 3549, Reward: [-497.639 -497.639 -497.639] [83.2900], Avg: [-570.155 -570.155 -570.155] (1.000)
Step: 3599, Reward: [-485.563 -485.563 -485.563] [105.0972], Avg: [-570.44 -570.44 -570.44] (1.000)
Step: 3649, Reward: [-486.268 -486.268 -486.268] [110.3407], Avg: [-570.798 -570.798 -570.798] (1.000)
Step: 3699, Reward: [-615.626 -615.626 -615.626] [112.6084], Avg: [-572.926 -572.926 -572.926] (1.000)
Step: 3749, Reward: [-443.453 -443.453 -443.453] [64.0686], Avg: [-572.054 -572.054 -572.054] (1.000)
Step: 3799, Reward: [-467.782 -467.782 -467.782] [66.5594], Avg: [-571.558 -571.558 -571.558] (1.000)
Step: 3849, Reward: [-515.429 -515.429 -515.429] [90.2296], Avg: [-572.001 -572.001 -572.001] (1.000)
Step: 3899, Reward: [-502.878 -502.878 -502.878] [71.9339], Avg: [-572.037 -572.037 -572.037] (1.000)
Step: 3949, Reward: [-528.851 -528.851 -528.851] [92.4575], Avg: [-572.66 -572.66 -572.66] (1.000)
Step: 3999, Reward: [-523.016 -523.016 -523.016] [36.6104], Avg: [-572.497 -572.497 -572.497] (1.000)
Step: 4049, Reward: [-586.355 -586.355 -586.355] [108.0519], Avg: [-574.002 -574.002 -574.002] (1.000)
Step: 4099, Reward: [-503.249 -503.249 -503.249] [14.7278], Avg: [-573.319 -573.319 -573.319] (1.000)
Step: 4149, Reward: [-449.674 -449.674 -449.674] [52.3341], Avg: [-572.46 -572.46 -572.46] (1.000)
Step: 4199, Reward: [-544.538 -544.538 -544.538] [92.8413], Avg: [-573.233 -573.233 -573.233] (1.000)
Step: 4249, Reward: [-485.742 -485.742 -485.742] [104.1259], Avg: [-573.429 -573.429 -573.429] (1.000)
Step: 4299, Reward: [-440.111 -440.111 -440.111] [67.7224], Avg: [-572.666 -572.666 -572.666] (1.000)
Step: 4349, Reward: [-463.19 -463.19 -463.19] [76.3206], Avg: [-572.285 -572.285 -572.285] (1.000)
Step: 4399, Reward: [-432.912 -432.912 -432.912] [91.8306], Avg: [-571.744 -571.744 -571.744] (1.000)
Step: 4449, Reward: [-494.499 -494.499 -494.499] [109.6190], Avg: [-572.108 -572.108 -572.108] (1.000)
Step: 4499, Reward: [-434.825 -434.825 -434.825] [25.0681], Avg: [-570.861 -570.861 -570.861] (1.000)
Step: 4549, Reward: [-470.651 -470.651 -470.651] [71.6724], Avg: [-570.548 -570.548 -570.548] (1.000)
Step: 4599, Reward: [-446.7 -446.7 -446.7] [52.6816], Avg: [-569.774 -569.774 -569.774] (1.000)
Step: 4649, Reward: [-473.113 -473.113 -473.113] [92.2366], Avg: [-569.727 -569.727 -569.727] (1.000)
Step: 4699, Reward: [-469.492 -469.492 -469.492] [139.7792], Avg: [-570.147 -570.147 -570.147] (1.000)
Step: 4749, Reward: [-478.444 -478.444 -478.444] [50.5803], Avg: [-569.714 -569.714 -569.714] (1.000)
Step: 4799, Reward: [-478.686 -478.686 -478.686] [65.5904], Avg: [-569.449 -569.449 -569.449] (1.000)
Step: 4849, Reward: [-578.478 -578.478 -578.478] [39.5792], Avg: [-569.951 -569.951 -569.951] (1.000)
Step: 4899, Reward: [-452.566 -452.566 -452.566] [60.9310], Avg: [-569.375 -569.375 -569.375] (1.000)
Step: 4949, Reward: [-471.534 -471.534 -471.534] [91.5087], Avg: [-569.311 -569.311 -569.311] (1.000)
Step: 4999, Reward: [-569.415 -569.415 -569.415] [107.0199], Avg: [-570.382 -570.382 -570.382] (1.000)
Step: 5049, Reward: [-550.998 -550.998 -550.998] [159.7377], Avg: [-571.771 -571.771 -571.771] (1.000)
Step: 5099, Reward: [-521. -521. -521.] [109.7695], Avg: [-572.35 -572.35 -572.35] (1.000)
Step: 5149, Reward: [-630.541 -630.541 -630.541] [180.4806], Avg: [-574.667 -574.667 -574.667] (1.000)
Step: 5199, Reward: [-551.346 -551.346 -551.346] [62.4132], Avg: [-575.043 -575.043 -575.043] (1.000)
Step: 5249, Reward: [-479.211 -479.211 -479.211] [69.9366], Avg: [-574.796 -574.796 -574.796] (1.000)
Step: 5299, Reward: [-466.545 -466.545 -466.545] [53.9118], Avg: [-574.284 -574.284 -574.284] (1.000)
Step: 5349, Reward: [-555.468 -555.468 -555.468] [172.7686], Avg: [-575.723 -575.723 -575.723] (1.000)
Step: 5399, Reward: [-473.667 -473.667 -473.667] [82.2837], Avg: [-575.539 -575.539 -575.539] (1.000)
Step: 5449, Reward: [-473.818 -473.818 -473.818] [81.9181], Avg: [-575.358 -575.358 -575.358] (1.000)
Step: 5499, Reward: [-520.116 -520.116 -520.116] [64.4590], Avg: [-575.442 -575.442 -575.442] (1.000)
Step: 5549, Reward: [-475.246 -475.246 -475.246] [26.5726], Avg: [-574.778 -574.778 -574.778] (1.000)
Step: 5599, Reward: [-483.761 -483.761 -483.761] [90.1100], Avg: [-574.77 -574.77 -574.77] (1.000)
Step: 5649, Reward: [-490.757 -490.757 -490.757] [107.9812], Avg: [-574.982 -574.982 -574.982] (1.000)
Step: 5699, Reward: [-499.604 -499.604 -499.604] [84.8950], Avg: [-575.066 -575.066 -575.066] (1.000)
Step: 5749, Reward: [-529.99 -529.99 -529.99] [91.8510], Avg: [-575.473 -575.473 -575.473] (1.000)
Step: 5799, Reward: [-441.13 -441.13 -441.13] [77.8986], Avg: [-574.986 -574.986 -574.986] (1.000)
Step: 5849, Reward: [-431.246 -431.246 -431.246] [61.8605], Avg: [-574.286 -574.286 -574.286] (1.000)
Step: 5899, Reward: [-468.348 -468.348 -468.348] [74.9853], Avg: [-574.024 -574.024 -574.024] (1.000)
Step: 5949, Reward: [-468.34 -468.34 -468.34] [82.3380], Avg: [-573.828 -573.828 -573.828] (1.000)
Step: 5999, Reward: [-438.432 -438.432 -438.432] [68.2991], Avg: [-573.268 -573.268 -573.268] (1.000)
Step: 6049, Reward: [-508.016 -508.016 -508.016] [115.5039], Avg: [-573.684 -573.684 -573.684] (1.000)
Step: 6099, Reward: [-463.254 -463.254 -463.254] [30.7097], Avg: [-573.03 -573.03 -573.03] (1.000)
Step: 6149, Reward: [-453.927 -453.927 -453.927] [43.5179], Avg: [-572.416 -572.416 -572.416] (1.000)
Step: 6199, Reward: [-436.967 -436.967 -436.967] [41.7691], Avg: [-571.66 -571.66 -571.66] (1.000)
Step: 6249, Reward: [-475.517 -475.517 -475.517] [60.9845], Avg: [-571.379 -571.379 -571.379] (1.000)
Step: 6299, Reward: [-465.088 -465.088 -465.088] [61.8733], Avg: [-571.027 -571.027 -571.027] (1.000)
Step: 6349, Reward: [-452.188 -452.188 -452.188] [37.2977], Avg: [-570.384 -570.384 -570.384] (1.000)
Step: 6399, Reward: [-579.17 -579.17 -579.17] [124.5401], Avg: [-571.426 -571.426 -571.426] (1.000)
Step: 6449, Reward: [-520.972 -520.972 -520.972] [100.7370], Avg: [-571.816 -571.816 -571.816] (1.000)
Step: 6499, Reward: [-506.883 -506.883 -506.883] [74.5792], Avg: [-571.89 -571.89 -571.89] (1.000)
Step: 6549, Reward: [-420.047 -420.047 -420.047] [57.6049], Avg: [-571.171 -571.171 -571.171] (1.000)
Step: 6599, Reward: [-457.102 -457.102 -457.102] [67.8195], Avg: [-570.82 -570.82 -570.82] (1.000)
Step: 6649, Reward: [-411.608 -411.608 -411.608] [50.7384], Avg: [-570.005 -570.005 -570.005] (1.000)
Step: 6699, Reward: [-442.14 -442.14 -442.14] [33.2052], Avg: [-569.298 -569.298 -569.298] (1.000)
Step: 6749, Reward: [-464.408 -464.408 -464.408] [32.5230], Avg: [-568.762 -568.762 -568.762] (1.000)
Step: 6799, Reward: [-484.652 -484.652 -484.652] [86.7652], Avg: [-568.782 -568.782 -568.782] (1.000)
Step: 6849, Reward: [-590.174 -590.174 -590.174] [88.0132], Avg: [-569.58 -569.58 -569.58] (1.000)
Step: 6899, Reward: [-526.845 -526.845 -526.845] [123.4011], Avg: [-570.165 -570.165 -570.165] (1.000)
Step: 6949, Reward: [-477.648 -477.648 -477.648] [126.2028], Avg: [-570.407 -570.407 -570.407] (1.000)
Step: 6999, Reward: [-446.968 -446.968 -446.968] [55.6281], Avg: [-569.923 -569.923 -569.923] (1.000)
Step: 7049, Reward: [-506.561 -506.561 -506.561] [132.2481], Avg: [-570.411 -570.411 -570.411] (1.000)
Step: 7099, Reward: [-485.931 -485.931 -485.931] [101.1201], Avg: [-570.529 -570.529 -570.529] (1.000)
Step: 7149, Reward: [-551.873 -551.873 -551.873] [92.5411], Avg: [-571.045 -571.045 -571.045] (1.000)
Step: 7199, Reward: [-528.551 -528.551 -528.551] [132.4818], Avg: [-571.67 -571.67 -571.67] (1.000)
Step: 7249, Reward: [-451.249 -451.249 -451.249] [32.6052], Avg: [-571.065 -571.065 -571.065] (1.000)
Step: 7299, Reward: [-486.837 -486.837 -486.837] [33.6215], Avg: [-570.718 -570.718 -570.718] (1.000)
Step: 7349, Reward: [-493.116 -493.116 -493.116] [66.4132], Avg: [-570.642 -570.642 -570.642] (1.000)
Step: 7399, Reward: [-564.833 -564.833 -564.833] [98.7127], Avg: [-571.27 -571.27 -571.27] (1.000)
Step: 7449, Reward: [-482.448 -482.448 -482.448] [99.1802], Avg: [-571.339 -571.339 -571.339] (1.000)
Step: 7499, Reward: [-565.591 -565.591 -565.591] [50.6768], Avg: [-571.639 -571.639 -571.639] (1.000)
Step: 7549, Reward: [-493.859 -493.859 -493.859] [38.6151], Avg: [-571.379 -571.379 -571.379] (1.000)
Step: 7599, Reward: [-530.125 -530.125 -530.125] [111.0605], Avg: [-571.839 -571.839 -571.839] (1.000)
Step: 7649, Reward: [-471.416 -471.416 -471.416] [47.0327], Avg: [-571.49 -571.49 -571.49] (1.000)
Step: 7699, Reward: [-436.575 -436.575 -436.575] [83.8128], Avg: [-571.158 -571.158 -571.158] (1.000)
Step: 7749, Reward: [-483.273 -483.273 -483.273] [80.3610], Avg: [-571.109 -571.109 -571.109] (1.000)
Step: 7799, Reward: [-496.636 -496.636 -496.636] [80.8173], Avg: [-571.15 -571.15 -571.15] (1.000)
Step: 7849, Reward: [-579.141 -579.141 -579.141] [133.4295], Avg: [-572.051 -572.051 -572.051] (1.000)
Step: 7899, Reward: [-555.972 -555.972 -555.972] [130.0412], Avg: [-572.772 -572.772 -572.772] (1.000)
Step: 7949, Reward: [-507.077 -507.077 -507.077] [111.3466], Avg: [-573.059 -573.059 -573.059] (1.000)
Step: 7999, Reward: [-452.271 -452.271 -452.271] [50.7830], Avg: [-572.622 -572.622 -572.622] (1.000)
Step: 8049, Reward: [-490.647 -490.647 -490.647] [44.7624], Avg: [-572.39 -572.39 -572.39] (1.000)
Step: 8099, Reward: [-525.449 -525.449 -525.449] [90.1692], Avg: [-572.657 -572.657 -572.657] (1.000)
Step: 8149, Reward: [-604.811 -604.811 -604.811] [109.3559], Avg: [-573.525 -573.525 -573.525] (1.000)
Step: 8199, Reward: [-548.532 -548.532 -548.532] [86.3756], Avg: [-573.9 -573.9 -573.9] (1.000)
Step: 8249, Reward: [-490.699 -490.699 -490.699] [44.0568], Avg: [-573.662 -573.662 -573.662] (1.000)
Step: 8299, Reward: [-519.498 -519.498 -519.498] [168.5400], Avg: [-574.351 -574.351 -574.351] (1.000)
Step: 8349, Reward: [-474.284 -474.284 -474.284] [49.9430], Avg: [-574.051 -574.051 -574.051] (1.000)
Step: 8399, Reward: [-533.503 -533.503 -533.503] [186.4527], Avg: [-574.92 -574.92 -574.92] (1.000)
Step: 8449, Reward: [-492.849 -492.849 -492.849] [46.7648], Avg: [-574.711 -574.711 -574.711] (1.000)
Step: 8499, Reward: [-469.135 -469.135 -469.135] [56.5391], Avg: [-574.422 -574.422 -574.422] (1.000)
Step: 8549, Reward: [-481.998 -481.998 -481.998] [69.4586], Avg: [-574.288 -574.288 -574.288] (1.000)
Step: 8599, Reward: [-567.533 -567.533 -567.533] [104.9465], Avg: [-574.859 -574.859 -574.859] (1.000)
Step: 8649, Reward: [-527.696 -527.696 -527.696] [72.8243], Avg: [-575.007 -575.007 -575.007] (1.000)
Step: 8699, Reward: [-484.21 -484.21 -484.21] [95.3796], Avg: [-575.034 -575.034 -575.034] (1.000)
Step: 8749, Reward: [-424.211 -424.211 -424.211] [65.1930], Avg: [-574.544 -574.544 -574.544] (1.000)
Step: 8799, Reward: [-563.326 -563.326 -563.326] [85.4285], Avg: [-574.966 -574.966 -574.966] (1.000)
Step: 8849, Reward: [-537.34 -537.34 -537.34] [92.1257], Avg: [-575.274 -575.274 -575.274] (1.000)
Step: 8899, Reward: [-491.408 -491.408 -491.408] [78.5172], Avg: [-575.244 -575.244 -575.244] (1.000)
Step: 8949, Reward: [-553.623 -553.623 -553.623] [133.5046], Avg: [-575.869 -575.869 -575.869] (1.000)
Step: 8999, Reward: [-443.739 -443.739 -443.739] [61.0048], Avg: [-575.474 -575.474 -575.474] (1.000)
Step: 9049, Reward: [-548.615 -548.615 -548.615] [110.1133], Avg: [-575.934 -575.934 -575.934] (1.000)
Step: 9099, Reward: [-461.273 -461.273 -461.273] [73.5649], Avg: [-575.708 -575.708 -575.708] (1.000)
Step: 9149, Reward: [-530.985 -530.985 -530.985] [97.6376], Avg: [-575.997 -575.997 -575.997] (1.000)
Step: 9199, Reward: [-504.175 -504.175 -504.175] [62.2953], Avg: [-575.945 -575.945 -575.945] (1.000)
Step: 9249, Reward: [-462.865 -462.865 -462.865] [104.1509], Avg: [-575.897 -575.897 -575.897] (1.000)
Step: 9299, Reward: [-486.165 -486.165 -486.165] [86.7853], Avg: [-575.881 -575.881 -575.881] (1.000)
Step: 9349, Reward: [-533.9 -533.9 -533.9] [109.7003], Avg: [-576.243 -576.243 -576.243] (1.000)
Step: 9399, Reward: [-463.134 -463.134 -463.134] [141.5610], Avg: [-576.395 -576.395 -576.395] (1.000)
Step: 9449, Reward: [-513.661 -513.661 -513.661] [98.8293], Avg: [-576.586 -576.586 -576.586] (1.000)
Step: 9499, Reward: [-566.897 -566.897 -566.897] [112.5885], Avg: [-577.127 -577.127 -577.127] (1.000)
Step: 9549, Reward: [-490.678 -490.678 -490.678] [23.8579], Avg: [-576.8 -576.8 -576.8] (1.000)
Step: 9599, Reward: [-464.966 -464.966 -464.966] [99.7481], Avg: [-576.737 -576.737 -576.737] (1.000)
Step: 9649, Reward: [-412.461 -412.461 -412.461] [56.4790], Avg: [-576.178 -576.178 -576.178] (1.000)
Step: 9699, Reward: [-448.535 -448.535 -448.535] [34.7320], Avg: [-575.699 -575.699 -575.699] (1.000)
Step: 9749, Reward: [-571.993 -571.993 -571.993] [175.7129], Avg: [-576.581 -576.581 -576.581] (1.000)
Step: 9799, Reward: [-542.802 -542.802 -542.802] [82.1592], Avg: [-576.828 -576.828 -576.828] (1.000)
Step: 9849, Reward: [-529.665 -529.665 -529.665] [75.7355], Avg: [-576.973 -576.973 -576.973] (1.000)
Step: 9899, Reward: [-513.883 -513.883 -513.883] [76.0263], Avg: [-577.038 -577.038 -577.038] (1.000)
Step: 9949, Reward: [-489.334 -489.334 -489.334] [58.6224], Avg: [-576.892 -576.892 -576.892] (1.000)
Step: 9999, Reward: [-473.495 -473.495 -473.495] [58.2885], Avg: [-576.667 -576.667 -576.667] (1.000)
Step: 10049, Reward: [-552.691 -552.691 -552.691] [92.6717], Avg: [-577.009 -577.009 -577.009] (1.000)
Step: 10099, Reward: [-499.372 -499.372 -499.372] [84.9117], Avg: [-577.045 -577.045 -577.045] (1.000)
Step: 10149, Reward: [-597.007 -597.007 -597.007] [142.2314], Avg: [-577.844 -577.844 -577.844] (1.000)
Step: 10199, Reward: [-517.736 -517.736 -517.736] [58.8179], Avg: [-577.837 -577.837 -577.837] (1.000)
Step: 10249, Reward: [-470.86 -470.86 -470.86] [36.5914], Avg: [-577.494 -577.494 -577.494] (1.000)
Step: 10299, Reward: [-486.106 -486.106 -486.106] [26.3386], Avg: [-577.178 -577.178 -577.178] (1.000)
Step: 10349, Reward: [-414.979 -414.979 -414.979] [48.4417], Avg: [-576.629 -576.629 -576.629] (1.000)
Step: 10399, Reward: [-601.683 -601.683 -601.683] [77.1440], Avg: [-577.12 -577.12 -577.12] (1.000)
Step: 10449, Reward: [-541.708 -541.708 -541.708] [84.5078], Avg: [-577.355 -577.355 -577.355] (1.000)
Step: 10499, Reward: [-468.9 -468.9 -468.9] [56.8973], Avg: [-577.109 -577.109 -577.109] (1.000)
Step: 10549, Reward: [-521.909 -521.909 -521.909] [81.5600], Avg: [-577.234 -577.234 -577.234] (1.000)
Step: 10599, Reward: [-501.985 -501.985 -501.985] [76.3907], Avg: [-577.24 -577.24 -577.24] (1.000)
Step: 10649, Reward: [-500.768 -500.768 -500.768] [28.9258], Avg: [-577.016 -577.016 -577.016] (1.000)
Step: 10699, Reward: [-452.395 -452.395 -452.395] [25.2232], Avg: [-576.552 -576.552 -576.552] (1.000)
Step: 10749, Reward: [-436.574 -436.574 -436.574] [43.9080], Avg: [-576.105 -576.105 -576.105] (1.000)
Step: 10799, Reward: [-516.061 -516.061 -516.061] [54.0782], Avg: [-576.077 -576.077 -576.077] (1.000)
Step: 10849, Reward: [-504.505 -504.505 -504.505] [111.4466], Avg: [-576.261 -576.261 -576.261] (1.000)
Step: 10899, Reward: [-556.481 -556.481 -556.481] [172.3498], Avg: [-576.961 -576.961 -576.961] (1.000)
Step: 10949, Reward: [-487.903 -487.903 -487.903] [73.1650], Avg: [-576.888 -576.888 -576.888] (1.000)
Step: 10999, Reward: [-481.425 -481.425 -481.425] [54.3775], Avg: [-576.702 -576.702 -576.702] (1.000)
Step: 11049, Reward: [-568.377 -568.377 -568.377] [92.9756], Avg: [-577.085 -577.085 -577.085] (1.000)
Step: 11099, Reward: [-539.404 -539.404 -539.404] [107.0761], Avg: [-577.397 -577.397 -577.397] (1.000)
Step: 11149, Reward: [-493.558 -493.558 -493.558] [69.2940], Avg: [-577.332 -577.332 -577.332] (1.000)
Step: 11199, Reward: [-489.569 -489.569 -489.569] [86.2113], Avg: [-577.325 -577.325 -577.325] (1.000)
Step: 11249, Reward: [-588.118 -588.118 -588.118] [147.7776], Avg: [-578.03 -578.03 -578.03] (1.000)
Step: 11299, Reward: [-542.82 -542.82 -542.82] [93.8850], Avg: [-578.29 -578.29 -578.29] (1.000)
Step: 11349, Reward: [-506.198 -506.198 -506.198] [107.4120], Avg: [-578.445 -578.445 -578.445] (1.000)
Step: 11399, Reward: [-509.457 -509.457 -509.457] [60.5462], Avg: [-578.408 -578.408 -578.408] (1.000)
Step: 11449, Reward: [-528.095 -528.095 -528.095] [84.0037], Avg: [-578.555 -578.555 -578.555] (1.000)
Step: 11499, Reward: [-482.868 -482.868 -482.868] [70.4555], Avg: [-578.446 -578.446 -578.446] (1.000)
Step: 11549, Reward: [-477.966 -477.966 -477.966] [55.6392], Avg: [-578.251 -578.251 -578.251] (1.000)
Step: 11599, Reward: [-544.272 -544.272 -544.272] [142.1491], Avg: [-578.718 -578.718 -578.718] (1.000)
Step: 11649, Reward: [-532.047 -532.047 -532.047] [71.9983], Avg: [-578.826 -578.826 -578.826] (1.000)
Step: 11699, Reward: [-450.444 -450.444 -450.444] [36.2465], Avg: [-578.433 -578.433 -578.433] (1.000)
Step: 11749, Reward: [-552.289 -552.289 -552.289] [37.9320], Avg: [-578.483 -578.483 -578.483] (1.000)
Step: 11799, Reward: [-497.46 -497.46 -497.46] [79.9283], Avg: [-578.478 -578.478 -578.478] (1.000)
Step: 11849, Reward: [-543.21 -543.21 -543.21] [96.7947], Avg: [-578.738 -578.738 -578.738] (1.000)
Step: 11899, Reward: [-538.593 -538.593 -538.593] [72.4563], Avg: [-578.874 -578.874 -578.874] (1.000)
Step: 11949, Reward: [-472.924 -472.924 -472.924] [29.7187], Avg: [-578.555 -578.555 -578.555] (1.000)
Step: 11999, Reward: [-577.13 -577.13 -577.13] [50.5702], Avg: [-578.759 -578.759 -578.759] (1.000)
Step: 12049, Reward: [-575.746 -575.746 -575.746] [92.2996], Avg: [-579.13 -579.13 -579.13] (1.000)
Step: 12099, Reward: [-586.425 -586.425 -586.425] [82.8613], Avg: [-579.502 -579.502 -579.502] (1.000)
Step: 12149, Reward: [-534.701 -534.701 -534.701] [63.4570], Avg: [-579.579 -579.579 -579.579] (1.000)
Step: 12199, Reward: [-515.337 -515.337 -515.337] [54.5289], Avg: [-579.539 -579.539 -579.539] (1.000)
Step: 12249, Reward: [-435.074 -435.074 -435.074] [41.9974], Avg: [-579.121 -579.121 -579.121] (1.000)
Step: 12299, Reward: [-534.121 -534.121 -534.121] [161.1964], Avg: [-579.593 -579.593 -579.593] (1.000)
Step: 12349, Reward: [-467.791 -467.791 -467.791] [89.9453], Avg: [-579.505 -579.505 -579.505] (1.000)
Step: 12399, Reward: [-513.527 -513.527 -513.527] [87.4705], Avg: [-579.592 -579.592 -579.592] (1.000)
Step: 12449, Reward: [-582.037 -582.037 -582.037] [119.3734], Avg: [-580.081 -580.081 -580.081] (1.000)
Step: 12499, Reward: [-595.136 -595.136 -595.136] [100.1567], Avg: [-580.542 -580.542 -580.542] (1.000)
Step: 12549, Reward: [-462.335 -462.335 -462.335] [55.1975], Avg: [-580.291 -580.291 -580.291] (1.000)
Step: 12599, Reward: [-523.946 -523.946 -523.946] [123.3660], Avg: [-580.557 -580.557 -580.557] (1.000)
Step: 12649, Reward: [-460.222 -460.222 -460.222] [51.7031], Avg: [-580.285 -580.285 -580.285] (1.000)
Step: 12699, Reward: [-459.842 -459.842 -459.842] [46.9378], Avg: [-579.996 -579.996 -579.996] (1.000)
Step: 12749, Reward: [-605.648 -605.648 -605.648] [170.4976], Avg: [-580.765 -580.765 -580.765] (1.000)
Step: 12799, Reward: [-517.642 -517.642 -517.642] [110.5842], Avg: [-580.951 -580.951 -580.951] (1.000)
Step: 12849, Reward: [-503.736 -503.736 -503.736] [37.2952], Avg: [-580.795 -580.795 -580.795] (1.000)
Step: 12899, Reward: [-403.944 -403.944 -403.944] [60.9366], Avg: [-580.346 -580.346 -580.346] (1.000)
Step: 12949, Reward: [-474.27 -474.27 -474.27] [35.4494], Avg: [-580.073 -580.073 -580.073] (1.000)
Step: 12999, Reward: [-430.537 -430.537 -430.537] [40.5876], Avg: [-579.654 -579.654 -579.654] (1.000)
Step: 13049, Reward: [-530.854 -530.854 -530.854] [70.6271], Avg: [-579.738 -579.738 -579.738] (1.000)
Step: 13099, Reward: [-521.973 -521.973 -521.973] [124.1347], Avg: [-579.991 -579.991 -579.991] (1.000)
Step: 13149, Reward: [-469.167 -469.167 -469.167] [54.4405], Avg: [-579.777 -579.777 -579.777] (1.000)
Step: 13199, Reward: [-475.335 -475.335 -475.335] [117.3182], Avg: [-579.826 -579.826 -579.826] (1.000)
Step: 13249, Reward: [-447.223 -447.223 -447.223] [59.3618], Avg: [-579.549 -579.549 -579.549] (1.000)
Step: 13299, Reward: [-551.94 -551.94 -551.94] [174.5301], Avg: [-580.102 -580.102 -580.102] (1.000)
Step: 13349, Reward: [-568.34 -568.34 -568.34] [32.7764], Avg: [-580.18 -580.18 -580.18] (1.000)
Step: 13399, Reward: [-508.06 -508.06 -508.06] [188.6809], Avg: [-580.615 -580.615 -580.615] (1.000)
Step: 13449, Reward: [-436.665 -436.665 -436.665] [172.6564], Avg: [-580.722 -580.722 -580.722] (1.000)
Step: 13499, Reward: [-466.721 -466.721 -466.721] [93.6378], Avg: [-580.646 -580.646 -580.646] (1.000)
Step: 13549, Reward: [-654.299 -654.299 -654.299] [187.3581], Avg: [-581.61 -581.61 -581.61] (1.000)
Step: 13599, Reward: [-481.332 -481.332 -481.332] [77.5566], Avg: [-581.526 -581.526 -581.526] (1.000)
Step: 13649, Reward: [-532.398 -532.398 -532.398] [51.8955], Avg: [-581.536 -581.536 -581.536] (1.000)
Step: 13699, Reward: [-479.508 -479.508 -479.508] [90.3249], Avg: [-581.493 -581.493 -581.493] (1.000)
Step: 13749, Reward: [-512.348 -512.348 -512.348] [43.7921], Avg: [-581.401 -581.401 -581.401] (1.000)
Step: 13799, Reward: [-520.78 -520.78 -520.78] [106.8525], Avg: [-581.569 -581.569 -581.569] (1.000)
Step: 13849, Reward: [-491.008 -491.008 -491.008] [41.5133], Avg: [-581.392 -581.392 -581.392] (1.000)
Step: 13899, Reward: [-517.787 -517.787 -517.787] [68.1103], Avg: [-581.408 -581.408 -581.408] (1.000)
Step: 13949, Reward: [-524.347 -524.347 -524.347] [119.9097], Avg: [-581.633 -581.633 -581.633] (1.000)
Step: 13999, Reward: [-529.581 -529.581 -529.581] [89.0905], Avg: [-581.765 -581.765 -581.765] (1.000)
Step: 14049, Reward: [-413.399 -413.399 -413.399] [71.5735], Avg: [-581.421 -581.421 -581.421] (1.000)
Step: 14099, Reward: [-498.981 -498.981 -498.981] [80.7620], Avg: [-581.415 -581.415 -581.415] (1.000)
Step: 14149, Reward: [-550.727 -550.727 -550.727] [77.6309], Avg: [-581.581 -581.581 -581.581] (1.000)
Step: 14199, Reward: [-581.825 -581.825 -581.825] [70.0586], Avg: [-581.828 -581.828 -581.828] (1.000)
Step: 14249, Reward: [-538.071 -538.071 -538.071] [19.0480], Avg: [-581.742 -581.742 -581.742] (1.000)
Step: 14299, Reward: [-536.011 -536.011 -536.011] [135.7258], Avg: [-582.056 -582.056 -582.056] (1.000)
Step: 14349, Reward: [-585.436 -585.436 -585.436] [127.6030], Avg: [-582.513 -582.513 -582.513] (1.000)
Step: 14399, Reward: [-496.861 -496.861 -496.861] [30.5029], Avg: [-582.321 -582.321 -582.321] (1.000)
Step: 14449, Reward: [-514.206 -514.206 -514.206] [102.6785], Avg: [-582.441 -582.441 -582.441] (1.000)
Step: 14499, Reward: [-471.843 -471.843 -471.843] [57.8994], Avg: [-582.259 -582.259 -582.259] (1.000)
Step: 14549, Reward: [-525.086 -525.086 -525.086] [110.0461], Avg: [-582.441 -582.441 -582.441] (1.000)
Step: 14599, Reward: [-480.614 -480.614 -480.614] [34.0523], Avg: [-582.209 -582.209 -582.209] (1.000)
Step: 14649, Reward: [-514.143 -514.143 -514.143] [90.1092], Avg: [-582.284 -582.284 -582.284] (1.000)
Step: 14699, Reward: [-560.961 -560.961 -560.961] [79.8461], Avg: [-582.483 -582.483 -582.483] (1.000)
Step: 14749, Reward: [-548.621 -548.621 -548.621] [112.1328], Avg: [-582.748 -582.748 -582.748] (1.000)
Step: 14799, Reward: [-509.401 -509.401 -509.401] [145.1424], Avg: [-582.991 -582.991 -582.991] (1.000)
Step: 14849, Reward: [-504.816 -504.816 -504.816] [58.4592], Avg: [-582.925 -582.925 -582.925] (1.000)
Step: 14899, Reward: [-627.238 -627.238 -627.238] [52.0527], Avg: [-583.248 -583.248 -583.248] (1.000)
Step: 14949, Reward: [-573.799 -573.799 -573.799] [87.2359], Avg: [-583.508 -583.508 -583.508] (1.000)
Step: 14999, Reward: [-558.448 -558.448 -558.448] [91.8554], Avg: [-583.731 -583.731 -583.731] (1.000)
Step: 15049, Reward: [-501.991 -501.991 -501.991] [130.2227], Avg: [-583.892 -583.892 -583.892] (1.000)
Step: 15099, Reward: [-524.142 -524.142 -524.142] [106.8151], Avg: [-584.048 -584.048 -584.048] (1.000)
Step: 15149, Reward: [-483.964 -483.964 -483.964] [30.7598], Avg: [-583.819 -583.819 -583.819] (1.000)
Step: 15199, Reward: [-538.611 -538.611 -538.611] [60.9896], Avg: [-583.871 -583.871 -583.871] (1.000)
Step: 15249, Reward: [-466.243 -466.243 -466.243] [92.4661], Avg: [-583.788 -583.788 -583.788] (1.000)
Step: 15299, Reward: [-515.983 -515.983 -515.983] [75.8267], Avg: [-583.815 -583.815 -583.815] (1.000)
Step: 15349, Reward: [-499.144 -499.144 -499.144] [59.0407], Avg: [-583.731 -583.731 -583.731] (1.000)
Step: 15399, Reward: [-522.728 -522.728 -522.728] [106.7493], Avg: [-583.88 -583.88 -583.88] (1.000)
Step: 15449, Reward: [-480.652 -480.652 -480.652] [57.5520], Avg: [-583.732 -583.732 -583.732] (1.000)
Step: 15499, Reward: [-534.699 -534.699 -534.699] [58.4852], Avg: [-583.762 -583.762 -583.762] (1.000)
Step: 15549, Reward: [-465.565 -465.565 -465.565] [31.0855], Avg: [-583.482 -583.482 -583.482] (1.000)
Step: 15599, Reward: [-465.994 -465.994 -465.994] [66.6292], Avg: [-583.319 -583.319 -583.319] (1.000)
Step: 15649, Reward: [-487.588 -487.588 -487.588] [114.2452], Avg: [-583.378 -583.378 -583.378] (1.000)
Step: 15699, Reward: [-491.679 -491.679 -491.679] [68.0875], Avg: [-583.303 -583.303 -583.303] (1.000)
Step: 15749, Reward: [-559.279 -559.279 -559.279] [181.1810], Avg: [-583.802 -583.802 -583.802] (1.000)
Step: 15799, Reward: [-448.743 -448.743 -448.743] [135.6625], Avg: [-583.804 -583.804 -583.804] (1.000)
Step: 15849, Reward: [-539.615 -539.615 -539.615] [93.7262], Avg: [-583.96 -583.96 -583.96] (1.000)
Step: 15899, Reward: [-546.248 -546.248 -546.248] [78.9146], Avg: [-584.09 -584.09 -584.09] (1.000)
Step: 15949, Reward: [-515.65 -515.65 -515.65] [174.1310], Avg: [-584.421 -584.421 -584.421] (1.000)
Step: 15999, Reward: [-518.647 -518.647 -518.647] [53.7931], Avg: [-584.384 -584.384 -584.384] (1.000)
Step: 16049, Reward: [-571.516 -571.516 -571.516] [109.3949], Avg: [-584.684 -584.684 -584.684] (1.000)
Step: 16099, Reward: [-486.642 -486.642 -486.642] [118.4557], Avg: [-584.748 -584.748 -584.748] (1.000)
Step: 16149, Reward: [-469.244 -469.244 -469.244] [65.1805], Avg: [-584.592 -584.592 -584.592] (1.000)
Step: 16199, Reward: [-451.337 -451.337 -451.337] [65.3839], Avg: [-584.382 -584.382 -584.382] (1.000)
Step: 16249, Reward: [-539.615 -539.615 -539.615] [109.8221], Avg: [-584.583 -584.583 -584.583] (1.000)
Step: 16299, Reward: [-517.19 -517.19 -517.19] [103.8885], Avg: [-584.695 -584.695 -584.695] (1.000)
Step: 16349, Reward: [-526.588 -526.588 -526.588] [86.1228], Avg: [-584.78 -584.78 -584.78] (1.000)
Step: 16399, Reward: [-514.17 -514.17 -514.17] [139.1899], Avg: [-584.989 -584.989 -584.989] (1.000)
Step: 16449, Reward: [-543.641 -543.641 -543.641] [100.3900], Avg: [-585.169 -585.169 -585.169] (1.000)
Step: 16499, Reward: [-560.776 -560.776 -560.776] [139.4320], Avg: [-585.517 -585.517 -585.517] (1.000)
Step: 16549, Reward: [-476.477 -476.477 -476.477] [63.0433], Avg: [-585.378 -585.378 -585.378] (1.000)
Step: 16599, Reward: [-494.39 -494.39 -494.39] [115.3796], Avg: [-585.452 -585.452 -585.452] (1.000)
Step: 16649, Reward: [-453.798 -453.798 -453.798] [58.5811], Avg: [-585.232 -585.232 -585.232] (1.000)
Step: 16699, Reward: [-516.711 -516.711 -516.711] [77.9598], Avg: [-585.261 -585.261 -585.261] (1.000)
Step: 16749, Reward: [-533.649 -533.649 -533.649] [148.1127], Avg: [-585.549 -585.549 -585.549] (1.000)
Step: 16799, Reward: [-475.307 -475.307 -475.307] [51.2175], Avg: [-585.373 -585.373 -585.373] (1.000)
Step: 16849, Reward: [-504.887 -504.887 -504.887] [85.9657], Avg: [-585.389 -585.389 -585.389] (1.000)
Step: 16899, Reward: [-511.858 -511.858 -511.858] [73.7291], Avg: [-585.39 -585.39 -585.39] (1.000)
Step: 16949, Reward: [-442.522 -442.522 -442.522] [41.2428], Avg: [-585.09 -585.09 -585.09] (1.000)
Step: 16999, Reward: [-522.665 -522.665 -522.665] [94.8108], Avg: [-585.185 -585.185 -585.185] (1.000)
Step: 17049, Reward: [-457.736 -457.736 -457.736] [75.6788], Avg: [-585.034 -585.034 -585.034] (1.000)
Step: 17099, Reward: [-511.106 -511.106 -511.106] [27.3715], Avg: [-584.897 -584.897 -584.897] (1.000)
Step: 17149, Reward: [-589.326 -589.326 -589.326] [129.6587], Avg: [-585.288 -585.288 -585.288] (1.000)
Step: 17199, Reward: [-600.036 -600.036 -600.036] [140.3813], Avg: [-585.739 -585.739 -585.739] (1.000)
Step: 17249, Reward: [-610.479 -610.479 -610.479] [93.8026], Avg: [-586.083 -586.083 -586.083] (1.000)
Step: 17299, Reward: [-483.488 -483.488 -483.488] [65.0922], Avg: [-585.975 -585.975 -585.975] (1.000)
Step: 17349, Reward: [-491.872 -491.872 -491.872] [32.9768], Avg: [-585.798 -585.798 -585.798] (1.000)
Step: 17399, Reward: [-560.22 -560.22 -560.22] [103.4938], Avg: [-586.022 -586.022 -586.022] (1.000)
Step: 17449, Reward: [-468.591 -468.591 -468.591] [75.9975], Avg: [-585.904 -585.904 -585.904] (1.000)
Step: 17499, Reward: [-544.983 -544.983 -544.983] [113.0197], Avg: [-586.11 -586.11 -586.11] (1.000)
Step: 17549, Reward: [-556.415 -556.415 -556.415] [106.3548], Avg: [-586.328 -586.328 -586.328] (1.000)
Step: 17599, Reward: [-563.109 -563.109 -563.109] [99.7986], Avg: [-586.546 -586.546 -586.546] (1.000)
Step: 17649, Reward: [-576.771 -576.771 -576.771] [82.5266], Avg: [-586.752 -586.752 -586.752] (1.000)
Step: 17699, Reward: [-510.297 -510.297 -510.297] [95.4552], Avg: [-586.805 -586.805 -586.805] (1.000)
Step: 17749, Reward: [-549.495 -549.495 -549.495] [146.0488], Avg: [-587.112 -587.112 -587.112] (1.000)
Step: 17799, Reward: [-500.227 -500.227 -500.227] [135.4863], Avg: [-587.248 -587.248 -587.248] (1.000)
Step: 17849, Reward: [-610.878 -610.878 -610.878] [66.9087], Avg: [-587.502 -587.502 -587.502] (1.000)
Step: 17899, Reward: [-544.684 -544.684 -544.684] [65.5519], Avg: [-587.565 -587.565 -587.565] (1.000)
Step: 17949, Reward: [-489.011 -489.011 -489.011] [93.9934], Avg: [-587.553 -587.553 -587.553] (1.000)
Step: 17999, Reward: [-443.17 -443.17 -443.17] [61.9282], Avg: [-587.324 -587.324 -587.324] (1.000)
Step: 18049, Reward: [-551.696 -551.696 -551.696] [97.3162], Avg: [-587.494 -587.494 -587.494] (1.000)
Step: 18099, Reward: [-540.476 -540.476 -540.476] [73.7417], Avg: [-587.568 -587.568 -587.568] (1.000)
Step: 18149, Reward: [-560.483 -560.483 -560.483] [132.5690], Avg: [-587.859 -587.859 -587.859] (1.000)
Step: 18199, Reward: [-479.415 -479.415 -479.415] [83.7791], Avg: [-587.791 -587.791 -587.791] (1.000)
Step: 18249, Reward: [-537.639 -537.639 -537.639] [49.5872], Avg: [-587.79 -587.79 -587.79] (1.000)
Step: 18299, Reward: [-566.895 -566.895 -566.895] [100.9424], Avg: [-588.008 -588.008 -588.008] (1.000)
Step: 18349, Reward: [-510.814 -510.814 -510.814] [100.8832], Avg: [-588.073 -588.073 -588.073] (1.000)
Step: 18399, Reward: [-511.174 -511.174 -511.174] [118.2598], Avg: [-588.185 -588.185 -588.185] (1.000)
Step: 18449, Reward: [-403.608 -403.608 -403.608] [34.5102], Avg: [-587.778 -587.778 -587.778] (1.000)
Step: 18499, Reward: [-510.554 -510.554 -510.554] [72.1512], Avg: [-587.765 -587.765 -587.765] (1.000)
Step: 18549, Reward: [-483.392 -483.392 -483.392] [95.6959], Avg: [-587.741 -587.741 -587.741] (1.000)
Step: 18599, Reward: [-488.101 -488.101 -488.101] [86.0280], Avg: [-587.705 -587.705 -587.705] (1.000)
Step: 18649, Reward: [-614.033 -614.033 -614.033] [135.1606], Avg: [-588.138 -588.138 -588.138] (1.000)
Step: 18699, Reward: [-534.196 -534.196 -534.196] [149.7076], Avg: [-588.394 -588.394 -588.394] (1.000)
Step: 18749, Reward: [-562.169 -562.169 -562.169] [111.7576], Avg: [-588.622 -588.622 -588.622] (1.000)
Step: 18799, Reward: [-473.109 -473.109 -473.109] [74.7078], Avg: [-588.513 -588.513 -588.513] (1.000)
Step: 18849, Reward: [-641.266 -641.266 -641.266] [106.8037], Avg: [-588.937 -588.937 -588.937] (1.000)
Step: 18899, Reward: [-468.33 -468.33 -468.33] [37.6728], Avg: [-588.717 -588.717 -588.717] (1.000)
Step: 18949, Reward: [-494.949 -494.949 -494.949] [46.6990], Avg: [-588.593 -588.593 -588.593] (1.000)
Step: 18999, Reward: [-566.168 -566.168 -566.168] [120.4183], Avg: [-588.851 -588.851 -588.851] (1.000)
Step: 19049, Reward: [-514.653 -514.653 -514.653] [37.3950], Avg: [-588.754 -588.754 -588.754] (1.000)
Step: 19099, Reward: [-559.538 -559.538 -559.538] [132.2676], Avg: [-589.024 -589.024 -589.024] (1.000)
Step: 19149, Reward: [-507.637 -507.637 -507.637] [99.1468], Avg: [-589.07 -589.07 -589.07] (1.000)
Step: 19199, Reward: [-576.093 -576.093 -576.093] [93.1967], Avg: [-589.279 -589.279 -589.279] (1.000)
Step: 19249, Reward: [-502.166 -502.166 -502.166] [90.6324], Avg: [-589.288 -589.288 -589.288] (1.000)
Step: 19299, Reward: [-426.499 -426.499 -426.499] [63.2934], Avg: [-589.031 -589.031 -589.031] (1.000)
Step: 19349, Reward: [-484.371 -484.371 -484.371] [53.3965], Avg: [-588.898 -588.898 -588.898] (1.000)
Step: 19399, Reward: [-501.551 -501.551 -501.551] [45.2441], Avg: [-588.79 -588.79 -588.79] (1.000)
Step: 19449, Reward: [-513.31 -513.31 -513.31] [97.6525], Avg: [-588.847 -588.847 -588.847] (1.000)
Step: 19499, Reward: [-597.418 -597.418 -597.418] [81.3270], Avg: [-589.077 -589.077 -589.077] (1.000)
Step: 19549, Reward: [-481.382 -481.382 -481.382] [52.0910], Avg: [-588.935 -588.935 -588.935] (1.000)
Step: 19599, Reward: [-544.302 -544.302 -544.302] [104.5619], Avg: [-589.088 -589.088 -589.088] (1.000)
Step: 19649, Reward: [-595.594 -595.594 -595.594] [97.0967], Avg: [-589.352 -589.352 -589.352] (1.000)
Step: 19699, Reward: [-431.159 -431.159 -431.159] [21.0490], Avg: [-589.003 -589.003 -589.003] (1.000)
Step: 19749, Reward: [-614.472 -614.472 -614.472] [154.6346], Avg: [-589.459 -589.459 -589.459] (1.000)
Step: 19799, Reward: [-485.75 -485.75 -485.75] [95.8010], Avg: [-589.439 -589.439 -589.439] (1.000)
Step: 19849, Reward: [-571.091 -571.091 -571.091] [124.7468], Avg: [-589.707 -589.707 -589.707] (1.000)
Step: 19899, Reward: [-497.681 -497.681 -497.681] [52.8679], Avg: [-589.609 -589.609 -589.609] (1.000)
Step: 19949, Reward: [-540.125 -540.125 -540.125] [68.6490], Avg: [-589.657 -589.657 -589.657] (1.000)
Step: 19999, Reward: [-560.027 -560.027 -560.027] [91.7934], Avg: [-589.812 -589.812 -589.812] (1.000)
Step: 20049, Reward: [-491.656 -491.656 -491.656] [112.6016], Avg: [-589.848 -589.848 -589.848] (1.000)
Step: 20099, Reward: [-485.251 -485.251 -485.251] [74.8708], Avg: [-589.775 -589.775 -589.775] (1.000)
Step: 20149, Reward: [-608.85 -608.85 -608.85] [163.5036], Avg: [-590.228 -590.228 -590.228] (1.000)
Step: 20199, Reward: [-541.135 -541.135 -541.135] [100.8240], Avg: [-590.356 -590.356 -590.356] (1.000)
Step: 20249, Reward: [-578.367 -578.367 -578.367] [143.6045], Avg: [-590.681 -590.681 -590.681] (1.000)
Step: 20299, Reward: [-560.763 -560.763 -560.763] [110.6999], Avg: [-590.88 -590.88 -590.88] (1.000)
Step: 20349, Reward: [-518.481 -518.481 -518.481] [125.8605], Avg: [-591.011 -591.011 -591.011] (1.000)
Step: 20399, Reward: [-495.62 -495.62 -495.62] [98.5515], Avg: [-591.019 -591.019 -591.019] (1.000)
Step: 20449, Reward: [-479.418 -479.418 -479.418] [66.8873], Avg: [-590.909 -590.909 -590.909] (1.000)
Step: 20499, Reward: [-538.614 -538.614 -538.614] [97.8558], Avg: [-591.02 -591.02 -591.02] (1.000)
Step: 20549, Reward: [-629.582 -629.582 -629.582] [113.3219], Avg: [-591.39 -591.39 -591.39] (1.000)
Step: 20599, Reward: [-507.481 -507.481 -507.481] [92.2142], Avg: [-591.41 -591.41 -591.41] (1.000)
Step: 20649, Reward: [-519.087 -519.087 -519.087] [110.0267], Avg: [-591.501 -591.501 -591.501] (1.000)
Step: 20699, Reward: [-510.683 -510.683 -510.683] [96.0019], Avg: [-591.538 -591.538 -591.538] (1.000)
Step: 20749, Reward: [-496.125 -496.125 -496.125] [78.6482], Avg: [-591.498 -591.498 -591.498] (1.000)
Step: 20799, Reward: [-568.203 -568.203 -568.203] [67.1539], Avg: [-591.603 -591.603 -591.603] (1.000)
Step: 20849, Reward: [-472.75 -472.75 -472.75] [37.4322], Avg: [-591.408 -591.408 -591.408] (1.000)
Step: 20899, Reward: [-521.903 -521.903 -521.903] [54.9794], Avg: [-591.373 -591.373 -591.373] (1.000)
Step: 20949, Reward: [-591.684 -591.684 -591.684] [68.1004], Avg: [-591.536 -591.536 -591.536] (1.000)
Step: 20999, Reward: [-563.627 -563.627 -563.627] [160.6667], Avg: [-591.853 -591.853 -591.853] (1.000)
Step: 21049, Reward: [-514.26 -514.26 -514.26] [71.1435], Avg: [-591.837 -591.837 -591.837] (1.000)
Step: 21099, Reward: [-535.18 -535.18 -535.18] [49.2736], Avg: [-591.82 -591.82 -591.82] (1.000)
Step: 21149, Reward: [-584.568 -584.568 -584.568] [125.0486], Avg: [-592.098 -592.098 -592.098] (1.000)
Step: 21199, Reward: [-476.34 -476.34 -476.34] [61.2997], Avg: [-591.97 -591.97 -591.97] (1.000)
Step: 21249, Reward: [-566.304 -566.304 -566.304] [115.7682], Avg: [-592.182 -592.182 -592.182] (1.000)
Step: 21299, Reward: [-577.607 -577.607 -577.607] [157.7638], Avg: [-592.518 -592.518 -592.518] (1.000)
Step: 21349, Reward: [-525.327 -525.327 -525.327] [65.7407], Avg: [-592.515 -592.515 -592.515] (1.000)
Step: 21399, Reward: [-497.029 -497.029 -497.029] [86.0133], Avg: [-592.492 -592.492 -592.492] (1.000)
Step: 21449, Reward: [-510.924 -510.924 -510.924] [80.4841], Avg: [-592.49 -592.49 -592.49] (1.000)
Step: 21499, Reward: [-460.375 -460.375 -460.375] [76.1356], Avg: [-592.36 -592.36 -592.36] (1.000)
Step: 21549, Reward: [-523.308 -523.308 -523.308] [113.6332], Avg: [-592.463 -592.463 -592.463] (1.000)
Step: 21599, Reward: [-489.981 -489.981 -489.981] [107.6808], Avg: [-592.475 -592.475 -592.475] (1.000)
Step: 21649, Reward: [-457.308 -457.308 -457.308] [70.2653], Avg: [-592.325 -592.325 -592.325] (1.000)
Step: 21699, Reward: [-640.92 -640.92 -640.92] [116.3931], Avg: [-592.705 -592.705 -592.705] (1.000)
Step: 21749, Reward: [-551.704 -551.704 -551.704] [142.5962], Avg: [-592.939 -592.939 -592.939] (1.000)
Step: 21799, Reward: [-615.679 -615.679 -615.679] [137.4378], Avg: [-593.306 -593.306 -593.306] (1.000)
Step: 21849, Reward: [-544.329 -544.329 -544.329] [83.5814], Avg: [-593.386 -593.386 -593.386] (1.000)
Step: 21899, Reward: [-642.033 -642.033 -642.033] [145.3794], Avg: [-593.829 -593.829 -593.829] (1.000)
Step: 21949, Reward: [-436.324 -436.324 -436.324] [38.9035], Avg: [-593.558 -593.558 -593.558] (1.000)
Step: 21999, Reward: [-564.445 -564.445 -564.445] [133.4476], Avg: [-593.795 -593.795 -593.795] (1.000)
Step: 22049, Reward: [-530.465 -530.465 -530.465] [153.0052], Avg: [-593.999 -593.999 -593.999] (1.000)
Step: 22099, Reward: [-523.189 -523.189 -523.189] [91.9988], Avg: [-594.047 -594.047 -594.047] (1.000)
Step: 22149, Reward: [-561.337 -561.337 -561.337] [119.8133], Avg: [-594.243 -594.243 -594.243] (1.000)
Step: 22199, Reward: [-528.598 -528.598 -528.598] [32.6194], Avg: [-594.169 -594.169 -594.169] (1.000)
Step: 22249, Reward: [-483.497 -483.497 -483.497] [73.0291], Avg: [-594.084 -594.084 -594.084] (1.000)
Step: 22299, Reward: [-516.436 -516.436 -516.436] [172.7884], Avg: [-594.298 -594.298 -594.298] (1.000)
Step: 22349, Reward: [-454.953 -454.953 -454.953] [45.0597], Avg: [-594.087 -594.087 -594.087] (1.000)
Step: 22399, Reward: [-529.96 -529.96 -529.96] [140.2468], Avg: [-594.257 -594.257 -594.257] (1.000)
Step: 22449, Reward: [-533.179 -533.179 -533.179] [94.4525], Avg: [-594.331 -594.331 -594.331] (1.000)
Step: 22499, Reward: [-479.664 -479.664 -479.664] [103.0460], Avg: [-594.305 -594.305 -594.305] (1.000)
Step: 22549, Reward: [-551.621 -551.621 -551.621] [124.7820], Avg: [-594.487 -594.487 -594.487] (1.000)
Step: 22599, Reward: [-707.946 -707.946 -707.946] [223.3645], Avg: [-595.232 -595.232 -595.232] (1.000)
Step: 22649, Reward: [-501.524 -501.524 -501.524] [44.7576], Avg: [-595.124 -595.124 -595.124] (1.000)
Step: 22699, Reward: [-573.106 -573.106 -573.106] [117.8817], Avg: [-595.336 -595.336 -595.336] (1.000)
Step: 22749, Reward: [-638.456 -638.456 -638.456] [111.3949], Avg: [-595.675 -595.675 -595.675] (1.000)
Step: 22799, Reward: [-562.347 -562.347 -562.347] [60.0441], Avg: [-595.734 -595.734 -595.734] (1.000)
Step: 22849, Reward: [-457.964 -457.964 -457.964] [77.1978], Avg: [-595.601 -595.601 -595.601] (1.000)
Step: 22899, Reward: [-538.42 -538.42 -538.42] [110.2933], Avg: [-595.717 -595.717 -595.717] (1.000)
Step: 22949, Reward: [-493.577 -493.577 -493.577] [133.2823], Avg: [-595.785 -595.785 -595.785] (1.000)
Step: 22999, Reward: [-572.565 -572.565 -572.565] [63.1494], Avg: [-595.872 -595.872 -595.872] (1.000)
Step: 23049, Reward: [-541.15 -541.15 -541.15] [162.0625], Avg: [-596.105 -596.105 -596.105] (1.000)
Step: 23099, Reward: [-546.036 -546.036 -546.036] [18.5039], Avg: [-596.036 -596.036 -596.036] (1.000)
Step: 23149, Reward: [-496.851 -496.851 -496.851] [88.8196], Avg: [-596.014 -596.014 -596.014] (1.000)
Step: 23199, Reward: [-540.328 -540.328 -540.328] [47.7594], Avg: [-595.997 -595.997 -595.997] (1.000)
Step: 23249, Reward: [-491.177 -491.177 -491.177] [74.0984], Avg: [-595.931 -595.931 -595.931] (1.000)
Step: 23299, Reward: [-551.263 -551.263 -551.263] [90.1340], Avg: [-596.028 -596.028 -596.028] (1.000)
Step: 23349, Reward: [-520.199 -520.199 -520.199] [57.5708], Avg: [-595.989 -595.989 -595.989] (1.000)
Step: 23399, Reward: [-537.043 -537.043 -537.043] [113.7216], Avg: [-596.106 -596.106 -596.106] (1.000)
Step: 23449, Reward: [-608.43 -608.43 -608.43] [139.3819], Avg: [-596.43 -596.43 -596.43] (1.000)
Step: 23499, Reward: [-426.146 -426.146 -426.146] [43.0987], Avg: [-596.159 -596.159 -596.159] (1.000)
Step: 23549, Reward: [-493.435 -493.435 -493.435] [66.8165], Avg: [-596.083 -596.083 -596.083] (1.000)
Step: 23599, Reward: [-460.543 -460.543 -460.543] [69.7398], Avg: [-595.944 -595.944 -595.944] (1.000)
Step: 23649, Reward: [-418.39 -418.39 -418.39] [34.9332], Avg: [-595.642 -595.642 -595.642] (1.000)
Step: 23699, Reward: [-505.797 -505.797 -505.797] [90.1845], Avg: [-595.643 -595.643 -595.643] (1.000)
Step: 23749, Reward: [-558.405 -558.405 -558.405] [60.6890], Avg: [-595.692 -595.692 -595.692] (1.000)
Step: 23799, Reward: [-474. -474. -474.] [84.1071], Avg: [-595.613 -595.613 -595.613] (1.000)
Step: 23849, Reward: [-574.214 -574.214 -574.214] [119.8661], Avg: [-595.82 -595.82 -595.82] (1.000)
Step: 23899, Reward: [-538.367 -538.367 -538.367] [81.5537], Avg: [-595.87 -595.87 -595.87] (1.000)
Step: 23949, Reward: [-440.607 -440.607 -440.607] [55.6840], Avg: [-595.662 -595.662 -595.662] (1.000)
Step: 23999, Reward: [-502.267 -502.267 -502.267] [76.2129], Avg: [-595.626 -595.626 -595.626] (1.000)
Step: 24049, Reward: [-472.061 -472.061 -472.061] [120.6599], Avg: [-595.62 -595.62 -595.62] (1.000)
Step: 24099, Reward: [-513.486 -513.486 -513.486] [71.9603], Avg: [-595.599 -595.599 -595.599] (1.000)
Step: 24149, Reward: [-527.1 -527.1 -527.1] [53.5430], Avg: [-595.568 -595.568 -595.568] (1.000)
Step: 24199, Reward: [-511.176 -511.176 -511.176] [54.3069], Avg: [-595.506 -595.506 -595.506] (1.000)
Step: 24249, Reward: [-565.324 -565.324 -565.324] [107.5155], Avg: [-595.665 -595.665 -595.665] (1.000)
Step: 24299, Reward: [-451.47 -451.47 -451.47] [44.3762], Avg: [-595.46 -595.46 -595.46] (1.000)
Step: 24349, Reward: [-449.851 -449.851 -449.851] [74.4670], Avg: [-595.314 -595.314 -595.314] (1.000)
Step: 24399, Reward: [-482.071 -482.071 -482.071] [120.0921], Avg: [-595.328 -595.328 -595.328] (1.000)
Step: 24449, Reward: [-418.267 -418.267 -418.267] [49.1974], Avg: [-595.067 -595.067 -595.067] (1.000)
Step: 24499, Reward: [-447.209 -447.209 -447.209] [65.0927], Avg: [-594.898 -594.898 -594.898] (1.000)
Step: 24549, Reward: [-481.555 -481.555 -481.555] [54.5928], Avg: [-594.778 -594.778 -594.778] (1.000)
Step: 24599, Reward: [-564.895 -564.895 -564.895] [96.3122], Avg: [-594.913 -594.913 -594.913] (1.000)
Step: 24649, Reward: [-540.469 -540.469 -540.469] [83.1511], Avg: [-594.971 -594.971 -594.971] (1.000)
Step: 24699, Reward: [-544.729 -544.729 -544.729] [71.6434], Avg: [-595.015 -595.015 -595.015] (1.000)
Step: 24749, Reward: [-446.591 -446.591 -446.591] [65.8357], Avg: [-594.848 -594.848 -594.848] (1.000)
Step: 24799, Reward: [-581.477 -581.477 -581.477] [143.0446], Avg: [-595.109 -595.109 -595.109] (1.000)
Step: 24849, Reward: [-476.873 -476.873 -476.873] [90.3732], Avg: [-595.053 -595.053 -595.053] (1.000)
Step: 24899, Reward: [-542.641 -542.641 -542.641] [58.4108], Avg: [-595.065 -595.065 -595.065] (1.000)
Step: 24949, Reward: [-498.656 -498.656 -498.656] [59.0174], Avg: [-594.99 -594.99 -594.99] (1.000)
Step: 24999, Reward: [-464.25 -464.25 -464.25] [68.1802], Avg: [-594.865 -594.865 -594.865] (1.000)
Step: 25049, Reward: [-446.302 -446.302 -446.302] [104.5238], Avg: [-594.777 -594.777 -594.777] (1.000)
Step: 25099, Reward: [-508.742 -508.742 -508.742] [80.0507], Avg: [-594.765 -594.765 -594.765] (1.000)
Step: 25149, Reward: [-556.688 -556.688 -556.688] [135.7955], Avg: [-594.96 -594.96 -594.96] (1.000)
Step: 25199, Reward: [-566.703 -566.703 -566.703] [63.3757], Avg: [-595.029 -595.029 -595.029] (1.000)
Step: 25249, Reward: [-498.544 -498.544 -498.544] [110.6236], Avg: [-595.057 -595.057 -595.057] (1.000)
Step: 25299, Reward: [-428.468 -428.468 -428.468] [55.5761], Avg: [-594.838 -594.838 -594.838] (1.000)
Step: 25349, Reward: [-631.652 -631.652 -631.652] [113.6182], Avg: [-595.135 -595.135 -595.135] (1.000)
Step: 25399, Reward: [-527.721 -527.721 -527.721] [115.9889], Avg: [-595.23 -595.23 -595.23] (1.000)
Step: 25449, Reward: [-452.108 -452.108 -452.108] [48.0523], Avg: [-595.043 -595.043 -595.043] (1.000)
Step: 25499, Reward: [-559.942 -559.942 -559.942] [152.0722], Avg: [-595.273 -595.273 -595.273] (1.000)
Step: 25549, Reward: [-504.195 -504.195 -504.195] [116.7413], Avg: [-595.323 -595.323 -595.323] (1.000)
Step: 25599, Reward: [-487.515 -487.515 -487.515] [46.7459], Avg: [-595.204 -595.204 -595.204] (1.000)
Step: 25649, Reward: [-587.65 -587.65 -587.65] [79.6676], Avg: [-595.344 -595.344 -595.344] (1.000)
Step: 25699, Reward: [-549.79 -549.79 -549.79] [96.7278], Avg: [-595.444 -595.444 -595.444] (1.000)
Step: 25749, Reward: [-644.573 -644.573 -644.573] [57.0089], Avg: [-595.65 -595.65 -595.65] (1.000)
Step: 25799, Reward: [-532.7 -532.7 -532.7] [96.4697], Avg: [-595.715 -595.715 -595.715] (1.000)
Step: 25849, Reward: [-573.261 -573.261 -573.261] [110.4006], Avg: [-595.885 -595.885 -595.885] (1.000)
Step: 25899, Reward: [-486.066 -486.066 -486.066] [69.8183], Avg: [-595.808 -595.808 -595.808] (1.000)
Step: 25949, Reward: [-597.04 -597.04 -597.04] [143.8342], Avg: [-596.087 -596.087 -596.087] (1.000)
Step: 25999, Reward: [-524.037 -524.037 -524.037] [84.5650], Avg: [-596.111 -596.111 -596.111] (1.000)
Step: 26049, Reward: [-476.451 -476.451 -476.451] [28.3429], Avg: [-595.936 -595.936 -595.936] (1.000)
Step: 26099, Reward: [-585.538 -585.538 -585.538] [126.2591], Avg: [-596.158 -596.158 -596.158] (1.000)
Step: 26149, Reward: [-532.701 -532.701 -532.701] [134.6976], Avg: [-596.294 -596.294 -596.294] (1.000)
Step: 26199, Reward: [-453.595 -453.595 -453.595] [50.7511], Avg: [-596.119 -596.119 -596.119] (1.000)
Step: 26249, Reward: [-559.983 -559.983 -559.983] [118.0280], Avg: [-596.275 -596.275 -596.275] (1.000)
Step: 26299, Reward: [-534.682 -534.682 -534.682] [50.8371], Avg: [-596.254 -596.254 -596.254] (1.000)
Step: 26349, Reward: [-481.494 -481.494 -481.494] [42.7855], Avg: [-596.118 -596.118 -596.118] (1.000)
Step: 26399, Reward: [-454.657 -454.657 -454.657] [94.1677], Avg: [-596.028 -596.028 -596.028] (1.000)
Step: 26449, Reward: [-483.419 -483.419 -483.419] [71.3854], Avg: [-595.95 -595.95 -595.95] (1.000)
Step: 26499, Reward: [-489.533 -489.533 -489.533] [53.7041], Avg: [-595.851 -595.851 -595.851] (1.000)
Step: 26549, Reward: [-529.888 -529.888 -529.888] [142.4364], Avg: [-595.995 -595.995 -595.995] (1.000)
Step: 26599, Reward: [-543.062 -543.062 -543.062] [137.4760], Avg: [-596.154 -596.154 -596.154] (1.000)
Step: 26649, Reward: [-492.688 -492.688 -492.688] [91.5239], Avg: [-596.131 -596.131 -596.131] (1.000)
Step: 26699, Reward: [-629.905 -629.905 -629.905] [118.6938], Avg: [-596.417 -596.417 -596.417] (1.000)
Step: 26749, Reward: [-507.387 -507.387 -507.387] [117.7876], Avg: [-596.471 -596.471 -596.471] (1.000)
Step: 26799, Reward: [-574.174 -574.174 -574.174] [110.4388], Avg: [-596.635 -596.635 -596.635] (1.000)
Step: 26849, Reward: [-563.57 -563.57 -563.57] [113.3543], Avg: [-596.785 -596.785 -596.785] (1.000)
Step: 26899, Reward: [-521.302 -521.302 -521.302] [66.7548], Avg: [-596.768 -596.768 -596.768] (1.000)
Step: 26949, Reward: [-613.555 -613.555 -613.555] [95.7966], Avg: [-596.977 -596.977 -596.977] (1.000)
Step: 26999, Reward: [-483.191 -483.191 -483.191] [87.5017], Avg: [-596.928 -596.928 -596.928] (1.000)
Step: 27049, Reward: [-507.274 -507.274 -507.274] [70.6231], Avg: [-596.893 -596.893 -596.893] (1.000)
Step: 27099, Reward: [-537.62 -537.62 -537.62] [60.2101], Avg: [-596.895 -596.895 -596.895] (1.000)
Step: 27149, Reward: [-535.124 -535.124 -535.124] [118.9891], Avg: [-597. -597. -597.] (1.000)
Step: 27199, Reward: [-490.886 -490.886 -490.886] [83.7126], Avg: [-596.959 -596.959 -596.959] (1.000)
Step: 27249, Reward: [-514.374 -514.374 -514.374] [124.9459], Avg: [-597.037 -597.037 -597.037] (1.000)
Step: 27299, Reward: [-548.474 -548.474 -548.474] [113.8040], Avg: [-597.156 -597.156 -597.156] (1.000)
Step: 27349, Reward: [-481.148 -481.148 -481.148] [81.1120], Avg: [-597.093 -597.093 -597.093] (1.000)
Step: 27399, Reward: [-657.69 -657.69 -657.69] [143.9563], Avg: [-597.466 -597.466 -597.466] (1.000)
Step: 27449, Reward: [-477.87 -477.87 -477.87] [67.0487], Avg: [-597.37 -597.37 -597.37] (1.000)
Step: 27499, Reward: [-595.641 -595.641 -595.641] [107.6863], Avg: [-597.563 -597.563 -597.563] (1.000)
Step: 27549, Reward: [-424.358 -424.358 -424.358] [56.6137], Avg: [-597.351 -597.351 -597.351] (1.000)
Step: 27599, Reward: [-504.792 -504.792 -504.792] [142.6894], Avg: [-597.442 -597.442 -597.442] (1.000)
Step: 27649, Reward: [-411.092 -411.092 -411.092] [53.1877], Avg: [-597.201 -597.201 -597.201] (1.000)
Step: 27699, Reward: [-524.235 -524.235 -524.235] [42.5490], Avg: [-597.146 -597.146 -597.146] (1.000)
Step: 27749, Reward: [-463.999 -463.999 -463.999] [60.5196], Avg: [-597.016 -597.016 -597.016] (1.000)
Step: 27799, Reward: [-458.298 -458.298 -458.298] [26.6681], Avg: [-596.814 -596.814 -596.814] (1.000)
Step: 27849, Reward: [-394.963 -394.963 -394.963] [38.7017], Avg: [-596.521 -596.521 -596.521] (1.000)
Step: 27899, Reward: [-508.022 -508.022 -508.022] [95.0796], Avg: [-596.533 -596.533 -596.533] (1.000)
Step: 27949, Reward: [-441.71 -441.71 -441.71] [32.9502], Avg: [-596.315 -596.315 -596.315] (1.000)
Step: 27999, Reward: [-590.665 -590.665 -590.665] [205.6851], Avg: [-596.672 -596.672 -596.672] (1.000)
Step: 28049, Reward: [-452.97 -452.97 -452.97] [62.9755], Avg: [-596.528 -596.528 -596.528] (1.000)
Step: 28099, Reward: [-488.273 -488.273 -488.273] [81.8430], Avg: [-596.481 -596.481 -596.481] (1.000)
Step: 28149, Reward: [-482.869 -482.869 -482.869] [38.6472], Avg: [-596.348 -596.348 -596.348] (1.000)
Step: 28199, Reward: [-472.259 -472.259 -472.259] [106.4323], Avg: [-596.317 -596.317 -596.317] (1.000)
Step: 28249, Reward: [-547.662 -547.662 -547.662] [99.0181], Avg: [-596.406 -596.406 -596.406] (1.000)
Step: 28299, Reward: [-448.907 -448.907 -448.907] [82.1549], Avg: [-596.29 -596.29 -596.29] (1.000)
Step: 28349, Reward: [-470.165 -470.165 -470.165] [72.3774], Avg: [-596.196 -596.196 -596.196] (1.000)
Step: 28399, Reward: [-520.652 -520.652 -520.652] [37.0814], Avg: [-596.128 -596.128 -596.128] (1.000)
Step: 28449, Reward: [-495.829 -495.829 -495.829] [111.4321], Avg: [-596.147 -596.147 -596.147] (1.000)
Step: 28499, Reward: [-570.185 -570.185 -570.185] [75.4752], Avg: [-596.234 -596.234 -596.234] (1.000)
Step: 28549, Reward: [-494.429 -494.429 -494.429] [84.1185], Avg: [-596.203 -596.203 -596.203] (1.000)
Step: 28599, Reward: [-514.658 -514.658 -514.658] [34.8926], Avg: [-596.122 -596.122 -596.122] (1.000)
Step: 28649, Reward: [-542.237 -542.237 -542.237] [152.8609], Avg: [-596.295 -596.295 -596.295] (1.000)
Step: 28699, Reward: [-583.365 -583.365 -583.365] [76.0321], Avg: [-596.404 -596.404 -596.404] (1.000)
Step: 28749, Reward: [-589.573 -589.573 -589.573] [116.6410], Avg: [-596.595 -596.595 -596.595] (1.000)
Step: 28799, Reward: [-478.022 -478.022 -478.022] [84.5539], Avg: [-596.536 -596.536 -596.536] (1.000)
Step: 28849, Reward: [-644.833 -644.833 -644.833] [180.8899], Avg: [-596.934 -596.934 -596.934] (1.000)
Step: 28899, Reward: [-530.3 -530.3 -530.3] [134.6812], Avg: [-597.051 -597.051 -597.051] (1.000)
Step: 28949, Reward: [-574.623 -574.623 -574.623] [143.5604], Avg: [-597.26 -597.26 -597.26] (1.000)
Step: 28999, Reward: [-473.002 -473.002 -473.002] [93.7791], Avg: [-597.208 -597.208 -597.208] (1.000)
Step: 29049, Reward: [-545.813 -545.813 -545.813] [113.2649], Avg: [-597.314 -597.314 -597.314] (1.000)
Step: 29099, Reward: [-563.584 -563.584 -563.584] [52.1756], Avg: [-597.346 -597.346 -597.346] (1.000)
Step: 29149, Reward: [-553.994 -553.994 -553.994] [93.9116], Avg: [-597.433 -597.433 -597.433] (1.000)
Step: 29199, Reward: [-554.378 -554.378 -554.378] [73.5868], Avg: [-597.485 -597.485 -597.485] (1.000)
Step: 29249, Reward: [-567.921 -567.921 -567.921] [117.4759], Avg: [-597.635 -597.635 -597.635] (1.000)
Step: 29299, Reward: [-501.923 -501.923 -501.923] [71.0758], Avg: [-597.593 -597.593 -597.593] (1.000)
Step: 29349, Reward: [-605.696 -605.696 -605.696] [95.3735], Avg: [-597.77 -597.77 -597.77] (1.000)
Step: 29399, Reward: [-498.974 -498.974 -498.974] [93.0739], Avg: [-597.76 -597.76 -597.76] (1.000)
Step: 29449, Reward: [-488.381 -488.381 -488.381] [53.9231], Avg: [-597.666 -597.666 -597.666] (1.000)
Step: 29499, Reward: [-596.7 -596.7 -596.7] [97.7796], Avg: [-597.83 -597.83 -597.83] (1.000)
Step: 29549, Reward: [-588.539 -588.539 -588.539] [58.1656], Avg: [-597.913 -597.913 -597.913] (1.000)
Step: 29599, Reward: [-466.421 -466.421 -466.421] [25.6644], Avg: [-597.734 -597.734 -597.734] (1.000)
Step: 29649, Reward: [-503.989 -503.989 -503.989] [30.6703], Avg: [-597.627 -597.627 -597.627] (1.000)
Step: 29699, Reward: [-443.145 -443.145 -443.145] [60.6838], Avg: [-597.47 -597.47 -597.47] (1.000)
Step: 29749, Reward: [-505.119 -505.119 -505.119] [110.8091], Avg: [-597.501 -597.501 -597.501] (1.000)
Step: 29799, Reward: [-457.526 -457.526 -457.526] [73.7794], Avg: [-597.389 -597.389 -597.389] (1.000)
Step: 29849, Reward: [-580.455 -580.455 -580.455] [154.0941], Avg: [-597.619 -597.619 -597.619] (1.000)
Step: 29899, Reward: [-470.4 -470.4 -470.4] [46.1802], Avg: [-597.484 -597.484 -597.484] (1.000)
Step: 29949, Reward: [-453.903 -453.903 -453.903] [51.2343], Avg: [-597.33 -597.33 -597.33] (1.000)
Step: 29999, Reward: [-456.083 -456.083 -456.083] [51.9562], Avg: [-597.181 -597.181 -597.181] (1.000)
Step: 30049, Reward: [-438.68 -438.68 -438.68] [52.1924], Avg: [-597.004 -597.004 -597.004] (1.000)
Step: 30099, Reward: [-444.554 -444.554 -444.554] [138.7337], Avg: [-596.981 -596.981 -596.981] (1.000)
Step: 30149, Reward: [-472.59 -472.59 -472.59] [25.5583], Avg: [-596.817 -596.817 -596.817] (1.000)
Step: 30199, Reward: [-496.454 -496.454 -496.454] [62.4252], Avg: [-596.754 -596.754 -596.754] (1.000)
Step: 30249, Reward: [-456.733 -456.733 -456.733] [60.4746], Avg: [-596.623 -596.623 -596.623] (1.000)
Step: 30299, Reward: [-415.83 -415.83 -415.83] [61.9906], Avg: [-596.427 -596.427 -596.427] (1.000)
Step: 30349, Reward: [-429.895 -429.895 -429.895] [42.8152], Avg: [-596.223 -596.223 -596.223] (1.000)
Step: 30399, Reward: [-498.108 -498.108 -498.108] [96.0457], Avg: [-596.22 -596.22 -596.22] (1.000)
Step: 30449, Reward: [-428.112 -428.112 -428.112] [100.3422], Avg: [-596.108 -596.108 -596.108] (1.000)
Step: 30499, Reward: [-501.805 -501.805 -501.805] [92.8308], Avg: [-596.106 -596.106 -596.106] (1.000)
Step: 30549, Reward: [-473.202 -473.202 -473.202] [47.4120], Avg: [-595.982 -595.982 -595.982] (1.000)
Step: 30599, Reward: [-492.693 -492.693 -492.693] [64.4711], Avg: [-595.919 -595.919 -595.919] (1.000)
Step: 30649, Reward: [-431.948 -431.948 -431.948] [61.6223], Avg: [-595.752 -595.752 -595.752] (1.000)
Step: 30699, Reward: [-593.87 -593.87 -593.87] [100.2903], Avg: [-595.912 -595.912 -595.912] (1.000)
Step: 30749, Reward: [-526.47 -526.47 -526.47] [79.7822], Avg: [-595.929 -595.929 -595.929] (1.000)
Step: 30799, Reward: [-466.618 -466.618 -466.618] [40.3853], Avg: [-595.785 -595.785 -595.785] (1.000)
Step: 30849, Reward: [-458.029 -458.029 -458.029] [61.4182], Avg: [-595.661 -595.661 -595.661] (1.000)
Step: 30899, Reward: [-578.612 -578.612 -578.612] [134.4109], Avg: [-595.851 -595.851 -595.851] (1.000)
Step: 30949, Reward: [-498.33 -498.33 -498.33] [83.1894], Avg: [-595.828 -595.828 -595.828] (1.000)
Step: 30999, Reward: [-454.14 -454.14 -454.14] [45.2558], Avg: [-595.672 -595.672 -595.672] (1.000)
Step: 31049, Reward: [-435.348 -435.348 -435.348] [71.5150], Avg: [-595.529 -595.529 -595.529] (1.000)
Step: 31099, Reward: [-473.214 -473.214 -473.214] [40.6357], Avg: [-595.398 -595.398 -595.398] (1.000)
Step: 31149, Reward: [-483.43 -483.43 -483.43] [36.4224], Avg: [-595.277 -595.277 -595.277] (1.000)
Step: 31199, Reward: [-447.334 -447.334 -447.334] [60.4865], Avg: [-595.136 -595.136 -595.136] (1.000)
Step: 31249, Reward: [-475.854 -475.854 -475.854] [66.0583], Avg: [-595.051 -595.051 -595.051] (1.000)
Step: 31299, Reward: [-522.286 -522.286 -522.286] [80.7234], Avg: [-595.064 -595.064 -595.064] (1.000)
Step: 31349, Reward: [-482.769 -482.769 -482.769] [56.3533], Avg: [-594.975 -594.975 -594.975] (1.000)
Step: 31399, Reward: [-435.882 -435.882 -435.882] [49.0668], Avg: [-594.8 -594.8 -594.8] (1.000)
Step: 31449, Reward: [-514.681 -514.681 -514.681] [104.8018], Avg: [-594.839 -594.839 -594.839] (1.000)
Step: 31499, Reward: [-423.847 -423.847 -423.847] [80.7664], Avg: [-594.696 -594.696 -594.696] (1.000)
Step: 31549, Reward: [-479.103 -479.103 -479.103] [56.9724], Avg: [-594.603 -594.603 -594.603] (1.000)
Step: 31599, Reward: [-464.683 -464.683 -464.683] [91.7120], Avg: [-594.542 -594.542 -594.542] (1.000)
Step: 31649, Reward: [-481.713 -481.713 -481.713] [73.0299], Avg: [-594.479 -594.479 -594.479] (1.000)
Step: 31699, Reward: [-456.082 -456.082 -456.082] [53.7750], Avg: [-594.346 -594.346 -594.346] (1.000)
Step: 31749, Reward: [-435.823 -435.823 -435.823] [60.7646], Avg: [-594.192 -594.192 -594.192] (1.000)
Step: 31799, Reward: [-539.767 -539.767 -539.767] [107.2849], Avg: [-594.275 -594.275 -594.275] (1.000)
Step: 31849, Reward: [-441.571 -441.571 -441.571] [60.4006], Avg: [-594.13 -594.13 -594.13] (1.000)
Step: 31899, Reward: [-466.019 -466.019 -466.019] [43.6596], Avg: [-593.998 -593.998 -593.998] (1.000)
Step: 31949, Reward: [-568.063 -568.063 -568.063] [104.9199], Avg: [-594.121 -594.121 -594.121] (1.000)
Step: 31999, Reward: [-421.513 -421.513 -421.513] [58.4420], Avg: [-593.943 -593.943 -593.943] (1.000)
Step: 32049, Reward: [-521.641 -521.641 -521.641] [56.2984], Avg: [-593.918 -593.918 -593.918] (1.000)
Step: 32099, Reward: [-365.202 -365.202 -365.202] [43.1764], Avg: [-593.629 -593.629 -593.629] (1.000)
Step: 32149, Reward: [-475.256 -475.256 -475.256] [142.1554], Avg: [-593.666 -593.666 -593.666] (1.000)
Step: 32199, Reward: [-488.226 -488.226 -488.226] [76.9187], Avg: [-593.622 -593.622 -593.622] (1.000)
Step: 32249, Reward: [-476.937 -476.937 -476.937] [42.2041], Avg: [-593.506 -593.506 -593.506] (1.000)
Step: 32299, Reward: [-479.077 -479.077 -479.077] [106.0408], Avg: [-593.493 -593.493 -593.493] (1.000)
Step: 32349, Reward: [-468.824 -468.824 -468.824] [123.3905], Avg: [-593.491 -593.491 -593.491] (1.000)
Step: 32399, Reward: [-478.701 -478.701 -478.701] [79.7265], Avg: [-593.437 -593.437 -593.437] (1.000)
Step: 32449, Reward: [-463.251 -463.251 -463.251] [66.7917], Avg: [-593.34 -593.34 -593.34] (1.000)
Step: 32499, Reward: [-495.275 -495.275 -495.275] [89.7420], Avg: [-593.327 -593.327 -593.327] (1.000)
Step: 32549, Reward: [-518.9 -518.9 -518.9] [78.8164], Avg: [-593.333 -593.333 -593.333] (1.000)
Step: 32599, Reward: [-421.896 -421.896 -421.896] [38.2695], Avg: [-593.129 -593.129 -593.129] (1.000)
Step: 32649, Reward: [-423.331 -423.331 -423.331] [62.9064], Avg: [-592.966 -592.966 -592.966] (1.000)
Step: 32699, Reward: [-473.843 -473.843 -473.843] [65.2162], Avg: [-592.883 -592.883 -592.883] (1.000)
Step: 32749, Reward: [-422.177 -422.177 -422.177] [34.2157], Avg: [-592.675 -592.675 -592.675] (1.000)
Step: 32799, Reward: [-560.491 -560.491 -560.491] [36.2507], Avg: [-592.681 -592.681 -592.681] (1.000)
Step: 32849, Reward: [-470.119 -470.119 -470.119] [69.8917], Avg: [-592.601 -592.601 -592.601] (1.000)
Step: 32899, Reward: [-472.847 -472.847 -472.847] [70.9725], Avg: [-592.527 -592.527 -592.527] (1.000)
Step: 32949, Reward: [-488.148 -488.148 -488.148] [68.1782], Avg: [-592.472 -592.472 -592.472] (1.000)
Step: 32999, Reward: [-443.05 -443.05 -443.05] [71.8821], Avg: [-592.354 -592.354 -592.354] (1.000)
Step: 33049, Reward: [-401.579 -401.579 -401.579] [59.0533], Avg: [-592.155 -592.155 -592.155] (1.000)
Step: 33099, Reward: [-444.989 -444.989 -444.989] [46.4427], Avg: [-592.003 -592.003 -592.003] (1.000)
Step: 33149, Reward: [-488.469 -488.469 -488.469] [43.2850], Avg: [-591.912 -591.912 -591.912] (1.000)
Step: 33199, Reward: [-496.212 -496.212 -496.212] [88.0265], Avg: [-591.9 -591.9 -591.9] (1.000)
Step: 33249, Reward: [-499.461 -499.461 -499.461] [105.1469], Avg: [-591.919 -591.919 -591.919] (1.000)
Step: 33299, Reward: [-426.151 -426.151 -426.151] [72.4982], Avg: [-591.779 -591.779 -591.779] (1.000)
Step: 33349, Reward: [-505.689 -505.689 -505.689] [50.3753], Avg: [-591.726 -591.726 -591.726] (1.000)
Step: 33399, Reward: [-494.443 -494.443 -494.443] [101.6780], Avg: [-591.732 -591.732 -591.732] (1.000)
Step: 33449, Reward: [-483.723 -483.723 -483.723] [101.1640], Avg: [-591.722 -591.722 -591.722] (1.000)
Step: 33499, Reward: [-508.654 -508.654 -508.654] [39.4796], Avg: [-591.657 -591.657 -591.657] (1.000)
Step: 33549, Reward: [-481.816 -481.816 -481.816] [56.6241], Avg: [-591.578 -591.578 -591.578] (1.000)
Step: 33599, Reward: [-494.998 -494.998 -494.998] [26.6380], Avg: [-591.474 -591.474 -591.474] (1.000)
Step: 33649, Reward: [-425.471 -425.471 -425.471] [77.0054], Avg: [-591.342 -591.342 -591.342] (1.000)
Step: 33699, Reward: [-535.795 -535.795 -535.795] [146.8914], Avg: [-591.477 -591.477 -591.477] (1.000)
Step: 33749, Reward: [-476.904 -476.904 -476.904] [43.1575], Avg: [-591.371 -591.371 -591.371] (1.000)
Step: 33799, Reward: [-493.015 -493.015 -493.015] [15.1774], Avg: [-591.248 -591.248 -591.248] (1.000)
Step: 33849, Reward: [-434.233 -434.233 -434.233] [38.9856], Avg: [-591.074 -591.074 -591.074] (1.000)
Step: 33899, Reward: [-536.174 -536.174 -536.174] [99.9162], Avg: [-591.14 -591.14 -591.14] (1.000)
Step: 33949, Reward: [-482.214 -482.214 -482.214] [78.8386], Avg: [-591.096 -591.096 -591.096] (1.000)
Step: 33999, Reward: [-482.297 -482.297 -482.297] [144.3032], Avg: [-591.148 -591.148 -591.148] (1.000)
Step: 34049, Reward: [-456.998 -456.998 -456.998] [70.3500], Avg: [-591.054 -591.054 -591.054] (1.000)
Step: 34099, Reward: [-486.084 -486.084 -486.084] [65.2185], Avg: [-590.996 -590.996 -590.996] (1.000)
Step: 34149, Reward: [-460.765 -460.765 -460.765] [70.1817], Avg: [-590.908 -590.908 -590.908] (1.000)
Step: 34199, Reward: [-529.123 -529.123 -529.123] [76.7948], Avg: [-590.93 -590.93 -590.93] (1.000)
Step: 34249, Reward: [-489.746 -489.746 -489.746] [100.6735], Avg: [-590.929 -590.929 -590.929] (1.000)
Step: 34299, Reward: [-540.274 -540.274 -540.274] [155.2487], Avg: [-591.082 -591.082 -591.082] (1.000)
Step: 34349, Reward: [-403.268 -403.268 -403.268] [81.9298], Avg: [-590.928 -590.928 -590.928] (1.000)
Step: 34399, Reward: [-518.586 -518.586 -518.586] [76.0796], Avg: [-590.933 -590.933 -590.933] (1.000)
Step: 34449, Reward: [-441.105 -441.105 -441.105] [83.1987], Avg: [-590.837 -590.837 -590.837] (1.000)
Step: 34499, Reward: [-562.361 -562.361 -562.361] [145.9463], Avg: [-591.007 -591.007 -591.007] (1.000)
Step: 34549, Reward: [-445.468 -445.468 -445.468] [56.0203], Avg: [-590.877 -590.877 -590.877] (1.000)
Step: 34599, Reward: [-452.15 -452.15 -452.15] [118.1377], Avg: [-590.847 -590.847 -590.847] (1.000)
Step: 34649, Reward: [-539.54 -539.54 -539.54] [107.6439], Avg: [-590.929 -590.929 -590.929] (1.000)
Step: 34699, Reward: [-491.289 -491.289 -491.289] [74.5314], Avg: [-590.893 -590.893 -590.893] (1.000)
Step: 34749, Reward: [-490.976 -490.976 -490.976] [80.0500], Avg: [-590.864 -590.864 -590.864] (1.000)
Step: 34799, Reward: [-518.534 -518.534 -518.534] [29.6969], Avg: [-590.803 -590.803 -590.803] (1.000)
Step: 34849, Reward: [-451.404 -451.404 -451.404] [88.2554], Avg: [-590.729 -590.729 -590.729] (1.000)
Step: 34899, Reward: [-482.752 -482.752 -482.752] [64.5125], Avg: [-590.667 -590.667 -590.667] (1.000)
Step: 34949, Reward: [-458.311 -458.311 -458.311] [22.7268], Avg: [-590.51 -590.51 -590.51] (1.000)
Step: 34999, Reward: [-460.145 -460.145 -460.145] [87.6995], Avg: [-590.449 -590.449 -590.449] (1.000)
Step: 35049, Reward: [-504.226 -504.226 -504.226] [103.0927], Avg: [-590.473 -590.473 -590.473] (1.000)
Step: 35099, Reward: [-475.723 -475.723 -475.723] [53.3764], Avg: [-590.386 -590.386 -590.386] (1.000)
Step: 35149, Reward: [-405.374 -405.374 -405.374] [39.8809], Avg: [-590.179 -590.179 -590.179] (1.000)
Step: 35199, Reward: [-488.687 -488.687 -488.687] [127.3345], Avg: [-590.216 -590.216 -590.216] (1.000)
Step: 35249, Reward: [-477.389 -477.389 -477.389] [53.0984], Avg: [-590.131 -590.131 -590.131] (1.000)
Step: 35299, Reward: [-422.24 -422.24 -422.24] [47.3700], Avg: [-589.961 -589.961 -589.961] (1.000)
Step: 35349, Reward: [-505.434 -505.434 -505.434] [109.1555], Avg: [-589.996 -589.996 -589.996] (1.000)
Step: 35399, Reward: [-514.063 -514.063 -514.063] [116.2650], Avg: [-590.053 -590.053 -590.053] (1.000)
Step: 35449, Reward: [-489.352 -489.352 -489.352] [69.4720], Avg: [-590.009 -590.009 -590.009] (1.000)
Step: 35499, Reward: [-437.7 -437.7 -437.7] [87.4863], Avg: [-589.917 -589.917 -589.917] (1.000)
Step: 35549, Reward: [-446.088 -446.088 -446.088] [84.1026], Avg: [-589.833 -589.833 -589.833] (1.000)
Step: 35599, Reward: [-530.005 -530.005 -530.005] [88.5063], Avg: [-589.873 -589.873 -589.873] (1.000)
Step: 35649, Reward: [-416.099 -416.099 -416.099] [66.7590], Avg: [-589.723 -589.723 -589.723] (1.000)
Step: 35699, Reward: [-499.176 -499.176 -499.176] [118.1101], Avg: [-589.762 -589.762 -589.762] (1.000)
Step: 35749, Reward: [-498.932 -498.932 -498.932] [77.0904], Avg: [-589.743 -589.743 -589.743] (1.000)
Step: 35799, Reward: [-537.855 -537.855 -537.855] [131.5885], Avg: [-589.854 -589.854 -589.854] (1.000)
Step: 35849, Reward: [-537.748 -537.748 -537.748] [117.5409], Avg: [-589.945 -589.945 -589.945] (1.000)
Step: 35899, Reward: [-456.32 -456.32 -456.32] [79.6786], Avg: [-589.87 -589.87 -589.87] (1.000)
Step: 35949, Reward: [-475.447 -475.447 -475.447] [63.4471], Avg: [-589.799 -589.799 -589.799] (1.000)
Step: 35999, Reward: [-475.91 -475.91 -475.91] [63.6125], Avg: [-589.729 -589.729 -589.729] (1.000)
Step: 36049, Reward: [-436.267 -436.267 -436.267] [28.2103], Avg: [-589.556 -589.556 -589.556] (1.000)
Step: 36099, Reward: [-487.798 -487.798 -487.798] [77.9691], Avg: [-589.523 -589.523 -589.523] (1.000)
Step: 36149, Reward: [-547.791 -547.791 -547.791] [53.6658], Avg: [-589.539 -589.539 -589.539] (1.000)
Step: 36199, Reward: [-644.871 -644.871 -644.871] [170.5237], Avg: [-589.851 -589.851 -589.851] (1.000)
Step: 36249, Reward: [-464.259 -464.259 -464.259] [69.1832], Avg: [-589.773 -589.773 -589.773] (1.000)
Step: 36299, Reward: [-509.736 -509.736 -509.736] [46.4201], Avg: [-589.727 -589.727 -589.727] (1.000)
Step: 36349, Reward: [-436.041 -436.041 -436.041] [57.0851], Avg: [-589.594 -589.594 -589.594] (1.000)
Step: 36399, Reward: [-490.4 -490.4 -490.4] [57.9501], Avg: [-589.538 -589.538 -589.538] (1.000)
Step: 36449, Reward: [-467.247 -467.247 -467.247] [68.2587], Avg: [-589.464 -589.464 -589.464] (1.000)
Step: 36499, Reward: [-472.435 -472.435 -472.435] [66.4479], Avg: [-589.394 -589.394 -589.394] (1.000)
Step: 36549, Reward: [-473.219 -473.219 -473.219] [7.9668], Avg: [-589.246 -589.246 -589.246] (1.000)
Step: 36599, Reward: [-450.673 -450.673 -450.673] [78.7320], Avg: [-589.164 -589.164 -589.164] (1.000)
Step: 36649, Reward: [-513.712 -513.712 -513.712] [55.3535], Avg: [-589.137 -589.137 -589.137] (1.000)
Step: 36699, Reward: [-538.025 -538.025 -538.025] [87.3637], Avg: [-589.186 -589.186 -589.186] (1.000)
Step: 36749, Reward: [-518.611 -518.611 -518.611] [59.4786], Avg: [-589.171 -589.171 -589.171] (1.000)
Step: 36799, Reward: [-531.704 -531.704 -531.704] [112.0007], Avg: [-589.245 -589.245 -589.245] (1.000)
Step: 36849, Reward: [-534.263 -534.263 -534.263] [102.1450], Avg: [-589.309 -589.309 -589.309] (1.000)
Step: 36899, Reward: [-514.582 -514.582 -514.582] [99.2606], Avg: [-589.343 -589.343 -589.343] (1.000)
Step: 36949, Reward: [-518.127 -518.127 -518.127] [55.5278], Avg: [-589.321 -589.321 -589.321] (1.000)
Step: 36999, Reward: [-566.262 -566.262 -566.262] [233.6409], Avg: [-589.606 -589.606 -589.606] (1.000)
Step: 37049, Reward: [-597.767 -597.767 -597.767] [88.6999], Avg: [-589.737 -589.737 -589.737] (1.000)
Step: 37099, Reward: [-537.905 -537.905 -537.905] [78.0152], Avg: [-589.772 -589.772 -589.772] (1.000)
Step: 37149, Reward: [-611.535 -611.535 -611.535] [186.7368], Avg: [-590.053 -590.053 -590.053] (1.000)
Step: 37199, Reward: [-534.232 -534.232 -534.232] [96.2473], Avg: [-590.107 -590.107 -590.107] (1.000)
Step: 37249, Reward: [-424.992 -424.992 -424.992] [73.4142], Avg: [-589.984 -589.984 -589.984] (1.000)
Step: 37299, Reward: [-464.593 -464.593 -464.593] [46.8713], Avg: [-589.879 -589.879 -589.879] (1.000)
Step: 37349, Reward: [-557.259 -557.259 -557.259] [91.8047], Avg: [-589.958 -589.958 -589.958] (1.000)
Step: 37399, Reward: [-517.646 -517.646 -517.646] [107.9148], Avg: [-590.005 -590.005 -590.005] (1.000)
Step: 37449, Reward: [-545.809 -545.809 -545.809] [84.3572], Avg: [-590.059 -590.059 -590.059] (1.000)
Step: 37499, Reward: [-562.775 -562.775 -562.775] [122.8060], Avg: [-590.186 -590.186 -590.186] (1.000)
Step: 37549, Reward: [-484.554 -484.554 -484.554] [56.6360], Avg: [-590.121 -590.121 -590.121] (1.000)
Step: 37599, Reward: [-511.898 -511.898 -511.898] [105.6624], Avg: [-590.158 -590.158 -590.158] (1.000)
Step: 37649, Reward: [-588.736 -588.736 -588.736] [127.7556], Avg: [-590.325 -590.325 -590.325] (1.000)
Step: 37699, Reward: [-576.07 -576.07 -576.07] [85.1603], Avg: [-590.419 -590.419 -590.419] (1.000)
Step: 37749, Reward: [-556.301 -556.301 -556.301] [47.6831], Avg: [-590.437 -590.437 -590.437] (1.000)
Step: 37799, Reward: [-505.59 -505.59 -505.59] [64.4907], Avg: [-590.411 -590.411 -590.411] (1.000)
Step: 37849, Reward: [-615.516 -615.516 -615.516] [114.5338], Avg: [-590.595 -590.595 -590.595] (1.000)
Step: 37899, Reward: [-631.603 -631.603 -631.603] [150.9376], Avg: [-590.848 -590.848 -590.848] (1.000)
Step: 37949, Reward: [-579.241 -579.241 -579.241] [88.7750], Avg: [-590.95 -590.95 -590.95] (1.000)
Step: 37999, Reward: [-534.942 -534.942 -534.942] [145.7114], Avg: [-591.068 -591.068 -591.068] (1.000)
Step: 38049, Reward: [-576.169 -576.169 -576.169] [82.6251], Avg: [-591.157 -591.157 -591.157] (1.000)
Step: 38099, Reward: [-478.971 -478.971 -478.971] [77.5376], Avg: [-591.111 -591.111 -591.111] (1.000)
Step: 38149, Reward: [-629.448 -629.448 -629.448] [157.9352], Avg: [-591.369 -591.369 -591.369] (1.000)
Step: 38199, Reward: [-489.972 -489.972 -489.972] [65.4430], Avg: [-591.322 -591.322 -591.322] (1.000)
Step: 38249, Reward: [-502.004 -502.004 -502.004] [69.6653], Avg: [-591.296 -591.296 -591.296] (1.000)
Step: 38299, Reward: [-560.034 -560.034 -560.034] [109.8561], Avg: [-591.399 -591.399 -591.399] (1.000)
Step: 38349, Reward: [-588.174 -588.174 -588.174] [174.1051], Avg: [-591.621 -591.621 -591.621] (1.000)
Step: 38399, Reward: [-587.268 -587.268 -587.268] [152.6898], Avg: [-591.814 -591.814 -591.814] (1.000)
Step: 38449, Reward: [-494.898 -494.898 -494.898] [69.6782], Avg: [-591.779 -591.779 -591.779] (1.000)
Step: 38499, Reward: [-534.187 -534.187 -534.187] [55.0179], Avg: [-591.776 -591.776 -591.776] (1.000)
Step: 38549, Reward: [-522.733 -522.733 -522.733] [69.4197], Avg: [-591.776 -591.776 -591.776] (1.000)
Step: 38599, Reward: [-627.029 -627.029 -627.029] [140.3375], Avg: [-592.004 -592.004 -592.004] (1.000)
Step: 38649, Reward: [-551.9 -551.9 -551.9] [101.1679], Avg: [-592.083 -592.083 -592.083] (1.000)
Step: 38699, Reward: [-649.008 -649.008 -649.008] [123.5129], Avg: [-592.316 -592.316 -592.316] (1.000)
Step: 38749, Reward: [-446.917 -446.917 -446.917] [46.8101], Avg: [-592.189 -592.189 -592.189] (1.000)
Step: 38799, Reward: [-433.351 -433.351 -433.351] [39.0749], Avg: [-592.034 -592.034 -592.034] (1.000)
Step: 38849, Reward: [-515.32 -515.32 -515.32] [34.3322], Avg: [-591.98 -591.98 -591.98] (1.000)
Step: 38899, Reward: [-558.382 -558.382 -558.382] [77.2270], Avg: [-592.036 -592.036 -592.036] (1.000)
Step: 38949, Reward: [-560.01 -560.01 -560.01] [138.6626], Avg: [-592.173 -592.173 -592.173] (1.000)
Step: 38999, Reward: [-481.797 -481.797 -481.797] [122.6851], Avg: [-592.188 -592.188 -592.188] (1.000)
Step: 39049, Reward: [-610.729 -610.729 -610.729] [181.9502], Avg: [-592.445 -592.445 -592.445] (1.000)
Step: 39099, Reward: [-556.599 -556.599 -556.599] [60.6316], Avg: [-592.477 -592.477 -592.477] (1.000)
Step: 39149, Reward: [-597.925 -597.925 -597.925] [138.2403], Avg: [-592.66 -592.66 -592.66] (1.000)
Step: 39199, Reward: [-609.138 -609.138 -609.138] [87.1436], Avg: [-592.793 -592.793 -592.793] (1.000)
Step: 39249, Reward: [-571.115 -571.115 -571.115] [60.1622], Avg: [-592.842 -592.842 -592.842] (1.000)
Step: 39299, Reward: [-522.268 -522.268 -522.268] [31.8323], Avg: [-592.792 -592.792 -592.792] (1.000)
Step: 39349, Reward: [-496.613 -496.613 -496.613] [93.1585], Avg: [-592.788 -592.788 -592.788] (1.000)
Step: 39399, Reward: [-526.647 -526.647 -526.647] [117.8608], Avg: [-592.854 -592.854 -592.854] (1.000)
Step: 39449, Reward: [-562.497 -562.497 -562.497] [59.9177], Avg: [-592.892 -592.892 -592.892] (1.000)
Step: 39499, Reward: [-558.015 -558.015 -558.015] [57.9758], Avg: [-592.921 -592.921 -592.921] (1.000)
Step: 39549, Reward: [-571.072 -571.072 -571.072] [179.9437], Avg: [-593.121 -593.121 -593.121] (1.000)
Step: 39599, Reward: [-573.747 -573.747 -573.747] [79.7355], Avg: [-593.197 -593.197 -593.197] (1.000)
Step: 39649, Reward: [-538.584 -538.584 -538.584] [118.6127], Avg: [-593.278 -593.278 -593.278] (1.000)
Step: 39699, Reward: [-598.148 -598.148 -598.148] [108.1597], Avg: [-593.42 -593.42 -593.42] (1.000)
Step: 39749, Reward: [-558.421 -558.421 -558.421] [75.2837], Avg: [-593.471 -593.471 -593.471] (1.000)
Step: 39799, Reward: [-546.81 -546.81 -546.81] [124.1595], Avg: [-593.568 -593.568 -593.568] (1.000)
Step: 39849, Reward: [-618.025 -618.025 -618.025] [77.1235], Avg: [-593.695 -593.695 -593.695] (1.000)
Step: 39899, Reward: [-528.484 -528.484 -528.484] [113.5813], Avg: [-593.756 -593.756 -593.756] (1.000)
Step: 39949, Reward: [-520.521 -520.521 -520.521] [92.4495], Avg: [-593.78 -593.78 -593.78] (1.000)
Step: 39999, Reward: [-677.811 -677.811 -677.811] [184.4202], Avg: [-594.116 -594.116 -594.116] (1.000)
Step: 40049, Reward: [-484.084 -484.084 -484.084] [98.8814], Avg: [-594.102 -594.102 -594.102] (1.000)
Step: 40099, Reward: [-554.119 -554.119 -554.119] [153.4350], Avg: [-594.243 -594.243 -594.243] (1.000)
Step: 40149, Reward: [-498.819 -498.819 -498.819] [39.5131], Avg: [-594.174 -594.174 -594.174] (1.000)
Step: 40199, Reward: [-512.849 -512.849 -512.849] [126.3323], Avg: [-594.23 -594.23 -594.23] (1.000)
Step: 40249, Reward: [-608.949 -608.949 -608.949] [156.6366], Avg: [-594.442 -594.442 -594.442] (1.000)
Step: 40299, Reward: [-471.086 -471.086 -471.086] [93.2543], Avg: [-594.405 -594.405 -594.405] (1.000)
Step: 40349, Reward: [-638.53 -638.53 -638.53] [143.4012], Avg: [-594.637 -594.637 -594.637] (1.000)
Step: 40399, Reward: [-556.224 -556.224 -556.224] [92.9817], Avg: [-594.705 -594.705 -594.705] (1.000)
Step: 40449, Reward: [-498.536 -498.536 -498.536] [63.6223], Avg: [-594.665 -594.665 -594.665] (1.000)
Step: 40499, Reward: [-637.047 -637.047 -637.047] [102.8474], Avg: [-594.844 -594.844 -594.844] (1.000)
Step: 40549, Reward: [-528.131 -528.131 -528.131] [52.5783], Avg: [-594.827 -594.827 -594.827] (1.000)
Step: 40599, Reward: [-568.446 -568.446 -568.446] [184.0593], Avg: [-595.021 -595.021 -595.021] (1.000)
Step: 40649, Reward: [-522.511 -522.511 -522.511] [79.5277], Avg: [-595.029 -595.029 -595.029] (1.000)
Step: 40699, Reward: [-564.944 -564.944 -564.944] [110.0476], Avg: [-595.128 -595.128 -595.128] (1.000)
Step: 40749, Reward: [-549.343 -549.343 -549.343] [87.7581], Avg: [-595.179 -595.179 -595.179] (1.000)
Step: 40799, Reward: [-552.191 -552.191 -552.191] [106.8923], Avg: [-595.257 -595.257 -595.257] (1.000)
Step: 40849, Reward: [-525.889 -525.889 -525.889] [112.7758], Avg: [-595.311 -595.311 -595.311] (1.000)
Step: 40899, Reward: [-488.012 -488.012 -488.012] [68.5098], Avg: [-595.263 -595.263 -595.263] (1.000)
Step: 40949, Reward: [-648.968 -648.968 -648.968] [135.6805], Avg: [-595.494 -595.494 -595.494] (1.000)
Step: 40999, Reward: [-520.595 -520.595 -520.595] [51.8310], Avg: [-595.466 -595.466 -595.466] (1.000)
Step: 41049, Reward: [-509.534 -509.534 -509.534] [88.9986], Avg: [-595.47 -595.47 -595.47] (1.000)
Step: 41099, Reward: [-661.018 -661.018 -661.018] [191.4840], Avg: [-595.783 -595.783 -595.783] (1.000)
Step: 41149, Reward: [-568.194 -568.194 -568.194] [83.6065], Avg: [-595.851 -595.851 -595.851] (1.000)
Step: 41199, Reward: [-561.356 -561.356 -561.356] [74.6490], Avg: [-595.899 -595.899 -595.899] (1.000)
Step: 41249, Reward: [-499.262 -499.262 -499.262] [81.6797], Avg: [-595.881 -595.881 -595.881] (1.000)
Step: 41299, Reward: [-467.311 -467.311 -467.311] [49.7059], Avg: [-595.786 -595.786 -595.786] (1.000)
Step: 41349, Reward: [-540.492 -540.492 -540.492] [140.8156], Avg: [-595.889 -595.889 -595.889] (1.000)
Step: 41399, Reward: [-566.842 -566.842 -566.842] [118.2414], Avg: [-595.997 -595.997 -595.997] (1.000)
Step: 41449, Reward: [-609.37 -609.37 -609.37] [96.9334], Avg: [-596.13 -596.13 -596.13] (1.000)
Step: 41499, Reward: [-629.7 -629.7 -629.7] [142.5858], Avg: [-596.342 -596.342 -596.342] (1.000)
Step: 41549, Reward: [-515.449 -515.449 -515.449] [103.3677], Avg: [-596.369 -596.369 -596.369] (1.000)
Step: 41599, Reward: [-523.955 -523.955 -523.955] [66.9973], Avg: [-596.363 -596.363 -596.363] (1.000)
Step: 41649, Reward: [-541.763 -541.763 -541.763] [112.8759], Avg: [-596.433 -596.433 -596.433] (1.000)
Step: 41699, Reward: [-560.705 -560.705 -560.705] [68.2647], Avg: [-596.472 -596.472 -596.472] (1.000)
Step: 41749, Reward: [-568.07 -568.07 -568.07] [106.7938], Avg: [-596.566 -596.566 -596.566] (1.000)
Step: 41799, Reward: [-607.678 -607.678 -607.678] [170.5732], Avg: [-596.783 -596.783 -596.783] (1.000)
Step: 41849, Reward: [-561.245 -561.245 -561.245] [176.0437], Avg: [-596.951 -596.951 -596.951] (1.000)
Step: 41899, Reward: [-606.731 -606.731 -606.731] [118.2482], Avg: [-597.104 -597.104 -597.104] (1.000)
Step: 41949, Reward: [-516.882 -516.882 -516.882] [75.8871], Avg: [-597.098 -597.098 -597.098] (1.000)
Step: 41999, Reward: [-571.642 -571.642 -571.642] [122.7497], Avg: [-597.214 -597.214 -597.214] (1.000)
Step: 42049, Reward: [-553.603 -553.603 -553.603] [63.6682], Avg: [-597.238 -597.238 -597.238] (1.000)
Step: 42099, Reward: [-520.437 -520.437 -520.437] [75.9028], Avg: [-597.237 -597.237 -597.237] (1.000)
Step: 42149, Reward: [-576.615 -576.615 -576.615] [160.9818], Avg: [-597.404 -597.404 -597.404] (1.000)
Step: 42199, Reward: [-657.537 -657.537 -657.537] [106.5724], Avg: [-597.601 -597.601 -597.601] (1.000)
Step: 42249, Reward: [-468.627 -468.627 -468.627] [79.8310], Avg: [-597.543 -597.543 -597.543] (1.000)
Step: 42299, Reward: [-546.745 -546.745 -546.745] [103.3984], Avg: [-597.605 -597.605 -597.605] (1.000)
Step: 42349, Reward: [-582.258 -582.258 -582.258] [81.9342], Avg: [-597.684 -597.684 -597.684] (1.000)
Step: 42399, Reward: [-508.348 -508.348 -508.348] [76.2890], Avg: [-597.668 -597.668 -597.668] (1.000)
Step: 42449, Reward: [-585.087 -585.087 -585.087] [110.7484], Avg: [-597.784 -597.784 -597.784] (1.000)
Step: 42499, Reward: [-483.615 -483.615 -483.615] [73.0234], Avg: [-597.736 -597.736 -597.736] (1.000)
Step: 42549, Reward: [-575.339 -575.339 -575.339] [107.3950], Avg: [-597.835 -597.835 -597.835] (1.000)
Step: 42599, Reward: [-583.573 -583.573 -583.573] [44.4700], Avg: [-597.871 -597.871 -597.871] (1.000)
Step: 42649, Reward: [-507.967 -507.967 -507.967] [90.8957], Avg: [-597.872 -597.872 -597.872] (1.000)
Step: 42699, Reward: [-568.56 -568.56 -568.56] [140.4432], Avg: [-598.002 -598.002 -598.002] (1.000)
Step: 42749, Reward: [-502.855 -502.855 -502.855] [72.6018], Avg: [-597.976 -597.976 -597.976] (1.000)
Step: 42799, Reward: [-547.764 -547.764 -547.764] [66.9224], Avg: [-597.995 -597.995 -597.995] (1.000)
Step: 42849, Reward: [-591.229 -591.229 -591.229] [135.5307], Avg: [-598.146 -598.146 -598.146] (1.000)
Step: 42899, Reward: [-533.258 -533.258 -533.258] [129.5639], Avg: [-598.221 -598.221 -598.221] (1.000)
Step: 42949, Reward: [-668.066 -668.066 -668.066] [87.7636], Avg: [-598.404 -598.404 -598.404] (1.000)
Step: 42999, Reward: [-583.751 -583.751 -583.751] [108.8828], Avg: [-598.514 -598.514 -598.514] (1.000)
Step: 43049, Reward: [-494.101 -494.101 -494.101] [45.1862], Avg: [-598.445 -598.445 -598.445] (1.000)
Step: 43099, Reward: [-621.452 -621.452 -621.452] [121.0741], Avg: [-598.612 -598.612 -598.612] (1.000)
Step: 43149, Reward: [-587.161 -587.161 -587.161] [89.1198], Avg: [-598.702 -598.702 -598.702] (1.000)
Step: 43199, Reward: [-603.835 -603.835 -603.835] [150.1296], Avg: [-598.882 -598.882 -598.882] (1.000)
Step: 43249, Reward: [-478.35 -478.35 -478.35] [53.8954], Avg: [-598.805 -598.805 -598.805] (1.000)
Step: 43299, Reward: [-517.604 -517.604 -517.604] [108.2568], Avg: [-598.836 -598.836 -598.836] (1.000)
Step: 43349, Reward: [-490.131 -490.131 -490.131] [85.7234], Avg: [-598.81 -598.81 -598.81] (1.000)
Step: 43399, Reward: [-601.624 -601.624 -601.624] [65.9716], Avg: [-598.889 -598.889 -598.889] (1.000)
Step: 43449, Reward: [-573.468 -573.468 -573.468] [158.9701], Avg: [-599.043 -599.043 -599.043] (1.000)
Step: 43499, Reward: [-478.789 -478.789 -478.789] [79.9800], Avg: [-598.996 -598.996 -598.996] (1.000)
Step: 43549, Reward: [-680.833 -680.833 -680.833] [79.3139], Avg: [-599.181 -599.181 -599.181] (1.000)
Step: 43599, Reward: [-540.735 -540.735 -540.735] [191.2594], Avg: [-599.334 -599.334 -599.334] (1.000)
Step: 43649, Reward: [-575.698 -575.698 -575.698] [153.7318], Avg: [-599.483 -599.483 -599.483] (1.000)
Step: 43699, Reward: [-562.719 -562.719 -562.719] [63.7811], Avg: [-599.514 -599.514 -599.514] (1.000)
Step: 43749, Reward: [-500.493 -500.493 -500.493] [71.4218], Avg: [-599.482 -599.482 -599.482] (1.000)
Step: 43799, Reward: [-545.322 -545.322 -545.322] [126.8889], Avg: [-599.565 -599.565 -599.565] (1.000)
Step: 43849, Reward: [-601.203 -601.203 -601.203] [187.4866], Avg: [-599.781 -599.781 -599.781] (1.000)
Step: 43899, Reward: [-587.751 -587.751 -587.751] [184.1337], Avg: [-599.977 -599.977 -599.977] (1.000)
Step: 43949, Reward: [-466.937 -466.937 -466.937] [31.4615], Avg: [-599.861 -599.861 -599.861] (1.000)
Step: 43999, Reward: [-514.889 -514.889 -514.889] [58.2090], Avg: [-599.831 -599.831 -599.831] (1.000)
Step: 44049, Reward: [-604.236 -604.236 -604.236] [151.5485], Avg: [-600.008 -600.008 -600.008] (1.000)
Step: 44099, Reward: [-536.318 -536.318 -536.318] [56.9843], Avg: [-600. -600. -600.] (1.000)
Step: 44149, Reward: [-517.958 -517.958 -517.958] [61.2930], Avg: [-599.977 -599.977 -599.977] (1.000)
Step: 44199, Reward: [-545.947 -545.947 -545.947] [85.3893], Avg: [-600.012 -600.012 -600.012] (1.000)
Step: 44249, Reward: [-656.427 -656.427 -656.427] [152.7563], Avg: [-600.249 -600.249 -600.249] (1.000)
Step: 44299, Reward: [-557.903 -557.903 -557.903] [105.8111], Avg: [-600.32 -600.32 -600.32] (1.000)
Step: 44349, Reward: [-559.376 -559.376 -559.376] [95.3951], Avg: [-600.382 -600.382 -600.382] (1.000)
Step: 44399, Reward: [-568.365 -568.365 -568.365] [170.0296], Avg: [-600.537 -600.537 -600.537] (1.000)
Step: 44449, Reward: [-548.012 -548.012 -548.012] [31.9326], Avg: [-600.514 -600.514 -600.514] (1.000)
Step: 44499, Reward: [-643.087 -643.087 -643.087] [224.1228], Avg: [-600.814 -600.814 -600.814] (1.000)
Step: 44549, Reward: [-643.216 -643.216 -643.216] [197.9696], Avg: [-601.083 -601.083 -601.083] (1.000)
Step: 44599, Reward: [-558.188 -558.188 -558.188] [188.0854], Avg: [-601.246 -601.246 -601.246] (1.000)
Step: 44649, Reward: [-614.01 -614.01 -614.01] [113.2617], Avg: [-601.387 -601.387 -601.387] (1.000)
Step: 44699, Reward: [-560.917 -560.917 -560.917] [51.4423], Avg: [-601.4 -601.4 -601.4] (1.000)
Step: 44749, Reward: [-543.459 -543.459 -543.459] [99.9226], Avg: [-601.446 -601.446 -601.446] (1.000)
Step: 44799, Reward: [-546.134 -546.134 -546.134] [45.4475], Avg: [-601.435 -601.435 -601.435] (1.000)
Step: 44849, Reward: [-543.3 -543.3 -543.3] [105.5759], Avg: [-601.488 -601.488 -601.488] (1.000)
Step: 44899, Reward: [-545.346 -545.346 -545.346] [96.6750], Avg: [-601.533 -601.533 -601.533] (1.000)
Step: 44949, Reward: [-535.468 -535.468 -535.468] [98.8238], Avg: [-601.57 -601.57 -601.57] (1.000)
Step: 44999, Reward: [-546.618 -546.618 -546.618] [20.1744], Avg: [-601.531 -601.531 -601.531] (1.000)
Step: 45049, Reward: [-621.951 -621.951 -621.951] [117.6126], Avg: [-601.684 -601.684 -601.684] (1.000)
Step: 45099, Reward: [-555.803 -555.803 -555.803] [146.9214], Avg: [-601.796 -601.796 -601.796] (1.000)
Step: 45149, Reward: [-695.124 -695.124 -695.124] [160.4751], Avg: [-602.078 -602.078 -602.078] (1.000)
Step: 45199, Reward: [-495.13 -495.13 -495.13] [67.3666], Avg: [-602.034 -602.034 -602.034] (1.000)
Step: 45249, Reward: [-594.73 -594.73 -594.73] [149.9765], Avg: [-602.191 -602.191 -602.191] (1.000)
Step: 45299, Reward: [-669.551 -669.551 -669.551] [109.0147], Avg: [-602.386 -602.386 -602.386] (1.000)
Step: 45349, Reward: [-468.891 -468.891 -468.891] [60.7662], Avg: [-602.306 -602.306 -602.306] (1.000)
Step: 45399, Reward: [-555.04 -555.04 -555.04] [77.9190], Avg: [-602.34 -602.34 -602.34] (1.000)
Step: 45449, Reward: [-560.442 -560.442 -560.442] [62.5011], Avg: [-602.362 -602.362 -602.362] (1.000)
Step: 45499, Reward: [-553.436 -553.436 -553.436] [127.1976], Avg: [-602.448 -602.448 -602.448] (1.000)
Step: 45549, Reward: [-560.707 -560.707 -560.707] [90.0158], Avg: [-602.501 -602.501 -602.501] (1.000)
Step: 45599, Reward: [-459.216 -459.216 -459.216] [120.6288], Avg: [-602.476 -602.476 -602.476] (1.000)
Step: 45649, Reward: [-596.292 -596.292 -596.292] [166.3714], Avg: [-602.652 -602.652 -602.652] (1.000)
Step: 45699, Reward: [-579.891 -579.891 -579.891] [66.1070], Avg: [-602.699 -602.699 -602.699] (1.000)
Step: 45749, Reward: [-472.858 -472.858 -472.858] [93.2107], Avg: [-602.659 -602.659 -602.659] (1.000)
Step: 45799, Reward: [-597.763 -597.763 -597.763] [141.7512], Avg: [-602.809 -602.809 -602.809] (1.000)
Step: 45849, Reward: [-522.384 -522.384 -522.384] [31.6676], Avg: [-602.756 -602.756 -602.756] (1.000)
Step: 45899, Reward: [-609.273 -609.273 -609.273] [40.0864], Avg: [-602.806 -602.806 -602.806] (1.000)
Step: 45949, Reward: [-485.5 -485.5 -485.5] [65.0831], Avg: [-602.749 -602.749 -602.749] (1.000)
Step: 45999, Reward: [-507.677 -507.677 -507.677] [84.4483], Avg: [-602.738 -602.738 -602.738] (1.000)
Step: 46049, Reward: [-489.647 -489.647 -489.647] [29.2039], Avg: [-602.647 -602.647 -602.647] (1.000)
Step: 46099, Reward: [-530.237 -530.237 -530.237] [109.1767], Avg: [-602.687 -602.687 -602.687] (1.000)
Step: 46149, Reward: [-445.098 -445.098 -445.098] [61.8499], Avg: [-602.583 -602.583 -602.583] (1.000)
Step: 46199, Reward: [-405.905 -405.905 -405.905] [32.0668], Avg: [-602.405 -602.405 -602.405] (1.000)
Step: 46249, Reward: [-463.873 -463.873 -463.873] [88.1947], Avg: [-602.35 -602.35 -602.35] (1.000)
Step: 46299, Reward: [-527.561 -527.561 -527.561] [71.7211], Avg: [-602.347 -602.347 -602.347] (1.000)
Step: 46349, Reward: [-608.988 -608.988 -608.988] [176.0947], Avg: [-602.544 -602.544 -602.544] (1.000)
Step: 46399, Reward: [-596.711 -596.711 -596.711] [124.4721], Avg: [-602.672 -602.672 -602.672] (1.000)
Step: 46449, Reward: [-560.171 -560.171 -560.171] [101.4590], Avg: [-602.736 -602.736 -602.736] (1.000)
Step: 46499, Reward: [-557.919 -557.919 -557.919] [66.8798], Avg: [-602.759 -602.759 -602.759] (1.000)
Step: 46549, Reward: [-665.626 -665.626 -665.626] [79.0217], Avg: [-602.912 -602.912 -602.912] (1.000)
Step: 46599, Reward: [-570.583 -570.583 -570.583] [114.2536], Avg: [-603. -603. -603.] (1.000)
Step: 46649, Reward: [-738.527 -738.527 -738.527] [138.2746], Avg: [-603.293 -603.293 -603.293] (1.000)
Step: 46699, Reward: [-459.451 -459.451 -459.451] [111.7384], Avg: [-603.259 -603.259 -603.259] (1.000)
Step: 46749, Reward: [-499.367 -499.367 -499.367] [96.1340], Avg: [-603.25 -603.25 -603.25] (1.000)
Step: 46799, Reward: [-607.556 -607.556 -607.556] [145.1858], Avg: [-603.41 -603.41 -603.41] (1.000)
Step: 46849, Reward: [-511.423 -511.423 -511.423] [70.0698], Avg: [-603.387 -603.387 -603.387] (1.000)
Step: 46899, Reward: [-515.861 -515.861 -515.861] [145.6083], Avg: [-603.449 -603.449 -603.449] (1.000)
Step: 46949, Reward: [-559.304 -559.304 -559.304] [52.6987], Avg: [-603.458 -603.458 -603.458] (1.000)
Step: 46999, Reward: [-609.206 -609.206 -609.206] [100.1880], Avg: [-603.57 -603.57 -603.57] (1.000)
Step: 47049, Reward: [-646.554 -646.554 -646.554] [68.2753], Avg: [-603.689 -603.689 -603.689] (1.000)
Step: 47099, Reward: [-563.205 -563.205 -563.205] [170.1715], Avg: [-603.826 -603.826 -603.826] (1.000)
Step: 47149, Reward: [-551.653 -551.653 -551.653] [124.5755], Avg: [-603.903 -603.903 -603.903] (1.000)
Step: 47199, Reward: [-519.053 -519.053 -519.053] [62.0560], Avg: [-603.879 -603.879 -603.879] (1.000)
Step: 47249, Reward: [-541.922 -541.922 -541.922] [38.3348], Avg: [-603.854 -603.854 -603.854] (1.000)
Step: 47299, Reward: [-523.065 -523.065 -523.065] [128.6492], Avg: [-603.905 -603.905 -603.905] (1.000)
Step: 47349, Reward: [-551.873 -551.873 -551.873] [93.2199], Avg: [-603.948 -603.948 -603.948] (1.000)
Step: 47399, Reward: [-527.788 -527.788 -527.788] [94.3792], Avg: [-603.967 -603.967 -603.967] (1.000)
Step: 47449, Reward: [-644.034 -644.034 -644.034] [84.0582], Avg: [-604.098 -604.098 -604.098] (1.000)
Step: 47499, Reward: [-501.722 -501.722 -501.722] [61.9109], Avg: [-604.055 -604.055 -604.055] (1.000)
Step: 47549, Reward: [-615.588 -615.588 -615.588] [88.2885], Avg: [-604.16 -604.16 -604.16] (1.000)
Step: 47599, Reward: [-555.534 -555.534 -555.534] [103.2735], Avg: [-604.218 -604.218 -604.218] (1.000)
Step: 47649, Reward: [-615.506 -615.506 -615.506] [116.3880], Avg: [-604.352 -604.352 -604.352] (1.000)
Step: 47699, Reward: [-512.007 -512.007 -512.007] [133.1091], Avg: [-604.395 -604.395 -604.395] (1.000)
Step: 47749, Reward: [-583.425 -583.425 -583.425] [134.1603], Avg: [-604.513 -604.513 -604.513] (1.000)
Step: 47799, Reward: [-565.279 -565.279 -565.279] [146.3188], Avg: [-604.625 -604.625 -604.625] (1.000)
Step: 47849, Reward: [-496.803 -496.803 -496.803] [86.9388], Avg: [-604.603 -604.603 -604.603] (1.000)
Step: 47899, Reward: [-567.701 -567.701 -567.701] [119.5682], Avg: [-604.69 -604.69 -604.69] (1.000)
Step: 47949, Reward: [-445.773 -445.773 -445.773] [94.5963], Avg: [-604.622 -604.622 -604.622] (1.000)
Step: 47999, Reward: [-599.975 -599.975 -599.975] [96.8978], Avg: [-604.719 -604.719 -604.719] (1.000)
Step: 48049, Reward: [-614.368 -614.368 -614.368] [106.4205], Avg: [-604.839 -604.839 -604.839] (1.000)
Step: 48099, Reward: [-624.178 -624.178 -624.178] [166.0806], Avg: [-605.032 -605.032 -605.032] (1.000)
Step: 48149, Reward: [-541.654 -541.654 -541.654] [130.1343], Avg: [-605.101 -605.101 -605.101] (1.000)
Step: 48199, Reward: [-694.987 -694.987 -694.987] [258.4003], Avg: [-605.463 -605.463 -605.463] (1.000)
Step: 48249, Reward: [-638.743 -638.743 -638.743] [167.8839], Avg: [-605.671 -605.671 -605.671] (1.000)
Step: 48299, Reward: [-594.071 -594.071 -594.071] [124.9603], Avg: [-605.789 -605.789 -605.789] (1.000)
Step: 48349, Reward: [-574.926 -574.926 -574.926] [117.2537], Avg: [-605.878 -605.878 -605.878] (1.000)
Step: 48399, Reward: [-602.753 -602.753 -602.753] [101.1385], Avg: [-605.979 -605.979 -605.979] (1.000)
Step: 48449, Reward: [-507.798 -507.798 -507.798] [62.9236], Avg: [-605.943 -605.943 -605.943] (1.000)
Step: 48499, Reward: [-542.433 -542.433 -542.433] [106.2224], Avg: [-605.987 -605.987 -605.987] (1.000)
Step: 48549, Reward: [-513.027 -513.027 -513.027] [122.1114], Avg: [-606.017 -606.017 -606.017] (1.000)
Step: 48599, Reward: [-619.381 -619.381 -619.381] [140.4191], Avg: [-606.175 -606.175 -606.175] (1.000)
Step: 48649, Reward: [-519.736 -519.736 -519.736] [94.6272], Avg: [-606.183 -606.183 -606.183] (1.000)
Step: 48699, Reward: [-568.096 -568.096 -568.096] [111.6101], Avg: [-606.259 -606.259 -606.259] (1.000)
Step: 48749, Reward: [-559.863 -559.863 -559.863] [129.0082], Avg: [-606.344 -606.344 -606.344] (1.000)
Step: 48799, Reward: [-631.752 -631.752 -631.752] [72.5930], Avg: [-606.444 -606.444 -606.444] (1.000)
Step: 48849, Reward: [-524.862 -524.862 -524.862] [73.1258], Avg: [-606.435 -606.435 -606.435] (1.000)
Step: 48899, Reward: [-583.231 -583.231 -583.231] [139.0435], Avg: [-606.554 -606.554 -606.554] (1.000)
Step: 48949, Reward: [-479.38 -479.38 -479.38] [97.9753], Avg: [-606.524 -606.524 -606.524] (1.000)
Step: 48999, Reward: [-550.585 -550.585 -550.585] [120.9487], Avg: [-606.59 -606.59 -606.59] (1.000)
Step: 49049, Reward: [-520.782 -520.782 -520.782] [57.1500], Avg: [-606.561 -606.561 -606.561] (1.000)
Step: 49099, Reward: [-647.26 -647.26 -647.26] [158.1496], Avg: [-606.764 -606.764 -606.764] (1.000)
Step: 49149, Reward: [-539.047 -539.047 -539.047] [86.3612], Avg: [-606.783 -606.783 -606.783] (1.000)
Step: 49199, Reward: [-646.976 -646.976 -646.976] [99.3948], Avg: [-606.924 -606.924 -606.924] (1.000)
Step: 49249, Reward: [-555.47 -555.47 -555.47] [71.7741], Avg: [-606.945 -606.945 -606.945] (1.000)
Step: 49299, Reward: [-523.314 -523.314 -523.314] [64.5590], Avg: [-606.926 -606.926 -606.926] (1.000)
Step: 49349, Reward: [-639.7 -639.7 -639.7] [134.3372], Avg: [-607.095 -607.095 -607.095] (1.000)
Step: 49399, Reward: [-576.657 -576.657 -576.657] [147.4478], Avg: [-607.213 -607.213 -607.213] (1.000)
Step: 49449, Reward: [-538.759 -538.759 -538.759] [28.3152], Avg: [-607.173 -607.173 -607.173] (1.000)
Step: 49499, Reward: [-514.871 -514.871 -514.871] [129.6608], Avg: [-607.211 -607.211 -607.211] (1.000)
Step: 49549, Reward: [-579.723 -579.723 -579.723] [174.5037], Avg: [-607.359 -607.359 -607.359] (1.000)
Step: 49599, Reward: [-527.703 -527.703 -527.703] [104.9979], Avg: [-607.385 -607.385 -607.385] (1.000)
Step: 49649, Reward: [-567.199 -567.199 -567.199] [86.4922], Avg: [-607.431 -607.431 -607.431] (1.000)
Step: 49699, Reward: [-530.891 -530.891 -530.891] [62.9849], Avg: [-607.418 -607.418 -607.418] (1.000)
Step: 49749, Reward: [-617.271 -617.271 -617.271] [102.3645], Avg: [-607.53 -607.53 -607.53] (1.000)
Step: 49799, Reward: [-551.746 -551.746 -551.746] [54.8539], Avg: [-607.529 -607.529 -607.529] (1.000)
Step: 49849, Reward: [-552.55 -552.55 -552.55] [99.9066], Avg: [-607.574 -607.574 -607.574] (1.000)
Step: 49899, Reward: [-569.058 -569.058 -569.058] [80.1120], Avg: [-607.616 -607.616 -607.616] (1.000)
Step: 49949, Reward: [-524.727 -524.727 -524.727] [123.3988], Avg: [-607.657 -607.657 -607.657] (1.000)
Step: 49999, Reward: [-469.627 -469.627 -469.627] [50.2879], Avg: [-607.569 -607.569 -607.569] (1.000)
Step: 50049, Reward: [-603.292 -603.292 -603.292] [126.4502], Avg: [-607.691 -607.691 -607.691] (1.000)
Step: 50099, Reward: [-576.902 -576.902 -576.902] [50.1238], Avg: [-607.71 -607.71 -607.71] (1.000)
Step: 50149, Reward: [-486.687 -486.687 -486.687] [48.0458], Avg: [-607.637 -607.637 -607.637] (1.000)
Step: 50199, Reward: [-565.79 -565.79 -565.79] [85.6894], Avg: [-607.681 -607.681 -607.681] (1.000)
Step: 50249, Reward: [-739.042 -739.042 -739.042] [104.7578], Avg: [-607.916 -607.916 -607.916] (1.000)
Step: 50299, Reward: [-510.444 -510.444 -510.444] [124.4735], Avg: [-607.943 -607.943 -607.943] (1.000)
Step: 50349, Reward: [-578.728 -578.728 -578.728] [121.8009], Avg: [-608.035 -608.035 -608.035] (1.000)
Step: 50399, Reward: [-666.025 -666.025 -666.025] [120.0921], Avg: [-608.212 -608.212 -608.212] (1.000)
Step: 50449, Reward: [-464.039 -464.039 -464.039] [69.9646], Avg: [-608.138 -608.138 -608.138] (1.000)
Step: 50499, Reward: [-637.476 -637.476 -637.476] [146.9681], Avg: [-608.313 -608.313 -608.313] (1.000)
Step: 50549, Reward: [-652.113 -652.113 -652.113] [104.3149], Avg: [-608.459 -608.459 -608.459] (1.000)
Step: 50599, Reward: [-563.283 -563.283 -563.283] [47.9984], Avg: [-608.462 -608.462 -608.462] (1.000)
Step: 50649, Reward: [-466.145 -466.145 -466.145] [124.3187], Avg: [-608.444 -608.444 -608.444] (1.000)
Step: 50699, Reward: [-585.535 -585.535 -585.535] [63.1988], Avg: [-608.484 -608.484 -608.484] (1.000)
Step: 50749, Reward: [-609.387 -609.387 -609.387] [235.7042], Avg: [-608.717 -608.717 -608.717] (1.000)
Step: 50799, Reward: [-517.556 -517.556 -517.556] [105.2634], Avg: [-608.731 -608.731 -608.731] (1.000)
Step: 50849, Reward: [-518.512 -518.512 -518.512] [126.4511], Avg: [-608.766 -608.766 -608.766] (1.000)
Step: 50899, Reward: [-576.125 -576.125 -576.125] [142.2961], Avg: [-608.874 -608.874 -608.874] (1.000)
Step: 50949, Reward: [-620.95 -620.95 -620.95] [133.0231], Avg: [-609.017 -609.017 -609.017] (1.000)
Step: 50999, Reward: [-575.304 -575.304 -575.304] [285.9082], Avg: [-609.264 -609.264 -609.264] (1.000)
Step: 51049, Reward: [-611.005 -611.005 -611.005] [84.2049], Avg: [-609.348 -609.348 -609.348] (1.000)
Step: 51099, Reward: [-588.018 -588.018 -588.018] [36.4151], Avg: [-609.363 -609.363 -609.363] (1.000)
Step: 51149, Reward: [-606.739 -606.739 -606.739] [156.4040], Avg: [-609.513 -609.513 -609.513] (1.000)
Step: 51199, Reward: [-469.424 -469.424 -469.424] [48.0041], Avg: [-609.423 -609.423 -609.423] (1.000)
Step: 51249, Reward: [-539.771 -539.771 -539.771] [90.6331], Avg: [-609.444 -609.444 -609.444] (1.000)
Step: 51299, Reward: [-502.852 -502.852 -502.852] [84.0401], Avg: [-609.422 -609.422 -609.422] (1.000)
Step: 51349, Reward: [-754.487 -754.487 -754.487] [44.4180], Avg: [-609.606 -609.606 -609.606] (1.000)
Step: 51399, Reward: [-526.215 -526.215 -526.215] [94.1016], Avg: [-609.617 -609.617 -609.617] (1.000)
Step: 51449, Reward: [-640.014 -640.014 -640.014] [123.1900], Avg: [-609.766 -609.766 -609.766] (1.000)
Step: 51499, Reward: [-530.539 -530.539 -530.539] [79.8976], Avg: [-609.766 -609.766 -609.766] (1.000)
Step: 51549, Reward: [-447.396 -447.396 -447.396] [42.5392], Avg: [-609.65 -609.65 -609.65] (1.000)
Step: 51599, Reward: [-438.766 -438.766 -438.766] [77.6009], Avg: [-609.56 -609.56 -609.56] (1.000)
Step: 51649, Reward: [-447.117 -447.117 -447.117] [49.4575], Avg: [-609.45 -609.45 -609.45] (1.000)
Step: 51699, Reward: [-450.776 -450.776 -450.776] [75.3530], Avg: [-609.37 -609.37 -609.37] (1.000)
Step: 51749, Reward: [-531.694 -531.694 -531.694] [118.3922], Avg: [-609.409 -609.409 -609.409] (1.000)
Step: 51799, Reward: [-537.888 -537.888 -537.888] [98.5909], Avg: [-609.435 -609.435 -609.435] (1.000)
Step: 51849, Reward: [-576.521 -576.521 -576.521] [110.8602], Avg: [-609.511 -609.511 -609.511] (1.000)
Step: 51899, Reward: [-528.536 -528.536 -528.536] [55.3943], Avg: [-609.486 -609.486 -609.486] (1.000)
Step: 51949, Reward: [-597.967 -597.967 -597.967] [81.2873], Avg: [-609.553 -609.553 -609.553] (1.000)
Step: 51999, Reward: [-550.516 -550.516 -550.516] [156.2780], Avg: [-609.647 -609.647 -609.647] (1.000)
Step: 52049, Reward: [-661.018 -661.018 -661.018] [183.9131], Avg: [-609.873 -609.873 -609.873] (1.000)
Step: 52099, Reward: [-608.707 -608.707 -608.707] [94.4013], Avg: [-609.962 -609.962 -609.962] (1.000)
Step: 52149, Reward: [-529.542 -529.542 -529.542] [88.5564], Avg: [-609.97 -609.97 -609.97] (1.000)
Step: 52199, Reward: [-479.237 -479.237 -479.237] [48.3030], Avg: [-609.891 -609.891 -609.891] (1.000)
Step: 52249, Reward: [-568.9 -568.9 -568.9] [107.4205], Avg: [-609.954 -609.954 -609.954] (1.000)
Step: 52299, Reward: [-635.533 -635.533 -635.533] [101.8533], Avg: [-610.076 -610.076 -610.076] (1.000)
Step: 52349, Reward: [-501.437 -501.437 -501.437] [143.4633], Avg: [-610.11 -610.11 -610.11] (1.000)
Step: 52399, Reward: [-498.324 -498.324 -498.324] [41.6243], Avg: [-610.043 -610.043 -610.043] (1.000)
Step: 52449, Reward: [-474.566 -474.566 -474.566] [72.9422], Avg: [-609.983 -609.983 -609.983] (1.000)
Step: 52499, Reward: [-660.595 -660.595 -660.595] [161.5024], Avg: [-610.185 -610.185 -610.185] (1.000)
Step: 52549, Reward: [-574.015 -574.015 -574.015] [70.2978], Avg: [-610.217 -610.217 -610.217] (1.000)
Step: 52599, Reward: [-576.107 -576.107 -576.107] [105.2259], Avg: [-610.285 -610.285 -610.285] (1.000)
Step: 52649, Reward: [-556.08 -556.08 -556.08] [102.2584], Avg: [-610.331 -610.331 -610.331] (1.000)
Step: 52699, Reward: [-631.164 -631.164 -631.164] [115.5824], Avg: [-610.46 -610.46 -610.46] (1.000)
Step: 52749, Reward: [-515.432 -515.432 -515.432] [44.7071], Avg: [-610.412 -610.412 -610.412] (1.000)
Step: 52799, Reward: [-541.341 -541.341 -541.341] [127.6923], Avg: [-610.468 -610.468 -610.468] (1.000)
Step: 52849, Reward: [-549.939 -549.939 -549.939] [146.1750], Avg: [-610.549 -610.549 -610.549] (1.000)
Step: 52899, Reward: [-534.169 -534.169 -534.169] [128.2643], Avg: [-610.598 -610.598 -610.598] (1.000)
Step: 52949, Reward: [-539.992 -539.992 -539.992] [67.5623], Avg: [-610.595 -610.595 -610.595] (1.000)
Step: 52999, Reward: [-486.835 -486.835 -486.835] [40.2069], Avg: [-610.516 -610.516 -610.516] (1.000)
Step: 53049, Reward: [-576.032 -576.032 -576.032] [67.8831], Avg: [-610.548 -610.548 -610.548] (1.000)
Step: 53099, Reward: [-487.847 -487.847 -487.847] [88.0926], Avg: [-610.515 -610.515 -610.515] (1.000)
Step: 53149, Reward: [-505.974 -505.974 -505.974] [45.0999], Avg: [-610.459 -610.459 -610.459] (1.000)
Step: 53199, Reward: [-536.899 -536.899 -536.899] [115.6640], Avg: [-610.499 -610.499 -610.499] (1.000)
Step: 53249, Reward: [-475.731 -475.731 -475.731] [136.0031], Avg: [-610.5 -610.5 -610.5] (1.000)
Step: 53299, Reward: [-484.375 -484.375 -484.375] [105.5122], Avg: [-610.481 -610.481 -610.481] (1.000)
Step: 53349, Reward: [-554.171 -554.171 -554.171] [114.8002], Avg: [-610.535 -610.535 -610.535] (1.000)
Step: 53399, Reward: [-538.647 -538.647 -538.647] [65.6467], Avg: [-610.53 -610.53 -610.53] (1.000)
Step: 53449, Reward: [-592.719 -592.719 -592.719] [150.0403], Avg: [-610.653 -610.653 -610.653] (1.000)
Step: 53499, Reward: [-584.147 -584.147 -584.147] [72.1565], Avg: [-610.696 -610.696 -610.696] (1.000)
Step: 53549, Reward: [-515.951 -515.951 -515.951] [75.3484], Avg: [-610.678 -610.678 -610.678] (1.000)
Step: 53599, Reward: [-646.395 -646.395 -646.395] [89.9645], Avg: [-610.795 -610.795 -610.795] (1.000)
Step: 53649, Reward: [-447.681 -447.681 -447.681] [60.8054], Avg: [-610.7 -610.7 -610.7] (1.000)
Step: 53699, Reward: [-510.979 -510.979 -510.979] [32.9704], Avg: [-610.638 -610.638 -610.638] (1.000)
Step: 53749, Reward: [-518.242 -518.242 -518.242] [48.4536], Avg: [-610.597 -610.597 -610.597] (1.000)
Step: 53799, Reward: [-619.909 -619.909 -619.909] [153.6345], Avg: [-610.748 -610.748 -610.748] (1.000)
Step: 53849, Reward: [-548.183 -548.183 -548.183] [79.2169], Avg: [-610.764 -610.764 -610.764] (1.000)
Step: 53899, Reward: [-572.986 -572.986 -572.986] [157.8208], Avg: [-610.875 -610.875 -610.875] (1.000)
Step: 53949, Reward: [-502.925 -502.925 -502.925] [39.1382], Avg: [-610.811 -610.811 -610.811] (1.000)
Step: 53999, Reward: [-585.146 -585.146 -585.146] [75.6933], Avg: [-610.858 -610.858 -610.858] (1.000)
Step: 54049, Reward: [-574.894 -574.894 -574.894] [81.3689], Avg: [-610.9 -610.9 -610.9] (1.000)
Step: 54099, Reward: [-484.047 -484.047 -484.047] [74.6977], Avg: [-610.851 -610.851 -610.851] (1.000)
Step: 54149, Reward: [-520.772 -520.772 -520.772] [43.3830], Avg: [-610.808 -610.808 -610.808] (1.000)
Step: 54199, Reward: [-572.727 -572.727 -572.727] [72.0101], Avg: [-610.84 -610.84 -610.84] (1.000)
Step: 54249, Reward: [-508.566 -508.566 -508.566] [124.3493], Avg: [-610.86 -610.86 -610.86] (1.000)
Step: 54299, Reward: [-504.941 -504.941 -504.941] [101.2016], Avg: [-610.856 -610.856 -610.856] (1.000)
Step: 54349, Reward: [-584.262 -584.262 -584.262] [149.9102], Avg: [-610.969 -610.969 -610.969] (1.000)
Step: 54399, Reward: [-444.628 -444.628 -444.628] [19.9956], Avg: [-610.834 -610.834 -610.834] (1.000)
Step: 54449, Reward: [-623.999 -623.999 -623.999] [126.3215], Avg: [-610.963 -610.963 -610.963] (1.000)
Step: 54499, Reward: [-579.739 -579.739 -579.739] [228.1550], Avg: [-611.143 -611.143 -611.143] (1.000)
Step: 54549, Reward: [-495.681 -495.681 -495.681] [73.8624], Avg: [-611.105 -611.105 -611.105] (1.000)
Step: 54599, Reward: [-527.102 -527.102 -527.102] [107.4276], Avg: [-611.127 -611.127 -611.127] (1.000)
Step: 54649, Reward: [-575.563 -575.563 -575.563] [93.9907], Avg: [-611.18 -611.18 -611.18] (1.000)
Step: 54699, Reward: [-645.782 -645.782 -645.782] [197.6412], Avg: [-611.392 -611.392 -611.392] (1.000)
Step: 54749, Reward: [-616.007 -616.007 -616.007] [141.2536], Avg: [-611.526 -611.526 -611.526] (1.000)
Step: 54799, Reward: [-520.482 -520.482 -520.482] [123.9381], Avg: [-611.556 -611.556 -611.556] (1.000)
Step: 54849, Reward: [-438.634 -438.634 -438.634] [61.4223], Avg: [-611.454 -611.454 -611.454] (1.000)
Step: 54899, Reward: [-502.564 -502.564 -502.564] [97.8743], Avg: [-611.444 -611.444 -611.444] (1.000)
Step: 54949, Reward: [-517.809 -517.809 -517.809] [115.9785], Avg: [-611.464 -611.464 -611.464] (1.000)
Step: 54999, Reward: [-569.046 -569.046 -569.046] [68.2682], Avg: [-611.488 -611.488 -611.488] (1.000)
Step: 55049, Reward: [-566.17 -566.17 -566.17] [131.8450], Avg: [-611.566 -611.566 -611.566] (1.000)
Step: 55099, Reward: [-662.212 -662.212 -662.212] [36.8225], Avg: [-611.646 -611.646 -611.646] (1.000)
Step: 55149, Reward: [-522.059 -522.059 -522.059] [31.8291], Avg: [-611.593 -611.593 -611.593] (1.000)
Step: 55199, Reward: [-572.734 -572.734 -572.734] [84.7467], Avg: [-611.635 -611.635 -611.635] (1.000)
Step: 55249, Reward: [-530.604 -530.604 -530.604] [51.8381], Avg: [-611.608 -611.608 -611.608] (1.000)
Step: 55299, Reward: [-481.617 -481.617 -481.617] [58.0766], Avg: [-611.543 -611.543 -611.543] (1.000)
Step: 55349, Reward: [-486.342 -486.342 -486.342] [119.5970], Avg: [-611.538 -611.538 -611.538] (1.000)
Step: 55399, Reward: [-588.671 -588.671 -588.671] [88.1904], Avg: [-611.597 -611.597 -611.597] (1.000)
Step: 55449, Reward: [-507.124 -507.124 -507.124] [135.2135], Avg: [-611.625 -611.625 -611.625] (1.000)
Step: 55499, Reward: [-489.212 -489.212 -489.212] [84.3152], Avg: [-611.591 -611.591 -611.591] (1.000)
Step: 55549, Reward: [-512.975 -512.975 -512.975] [79.8999], Avg: [-611.574 -611.574 -611.574] (1.000)
Step: 55599, Reward: [-566.724 -566.724 -566.724] [167.7048], Avg: [-611.684 -611.684 -611.684] (1.000)
Step: 55649, Reward: [-590.631 -590.631 -590.631] [52.7923], Avg: [-611.713 -611.713 -611.713] (1.000)
Step: 55699, Reward: [-533.932 -533.932 -533.932] [126.8085], Avg: [-611.757 -611.757 -611.757] (1.000)
Step: 55749, Reward: [-498.786 -498.786 -498.786] [50.5050], Avg: [-611.701 -611.701 -611.701] (1.000)
Step: 55799, Reward: [-428.465 -428.465 -428.465] [84.9744], Avg: [-611.613 -611.613 -611.613] (1.000)
Step: 55849, Reward: [-412.011 -412.011 -412.011] [54.3262], Avg: [-611.483 -611.483 -611.483] (1.000)
Step: 55899, Reward: [-559.067 -559.067 -559.067] [57.9786], Avg: [-611.488 -611.488 -611.488] (1.000)
Step: 55949, Reward: [-570.932 -570.932 -570.932] [127.6039], Avg: [-611.565 -611.565 -611.565] (1.000)
Step: 55999, Reward: [-546.314 -546.314 -546.314] [154.5342], Avg: [-611.645 -611.645 -611.645] (1.000)
Step: 56049, Reward: [-634.745 -634.745 -634.745] [140.2073], Avg: [-611.791 -611.791 -611.791] (1.000)
Step: 56099, Reward: [-525.247 -525.247 -525.247] [45.2321], Avg: [-611.754 -611.754 -611.754] (1.000)
Step: 56149, Reward: [-584.37 -584.37 -584.37] [97.1999], Avg: [-611.816 -611.816 -611.816] (1.000)
Step: 56199, Reward: [-496.759 -496.759 -496.759] [105.2926], Avg: [-611.808 -611.808 -611.808] (1.000)
Step: 56249, Reward: [-413.304 -413.304 -413.304] [42.3247], Avg: [-611.669 -611.669 -611.669] (1.000)
Step: 56299, Reward: [-457.614 -457.614 -457.614] [72.9723], Avg: [-611.597 -611.597 -611.597] (1.000)
Step: 56349, Reward: [-434.263 -434.263 -434.263] [81.4149], Avg: [-611.512 -611.512 -611.512] (1.000)
Step: 56399, Reward: [-504.599 -504.599 -504.599] [137.1090], Avg: [-611.538 -611.538 -611.538] (1.000)
Step: 56449, Reward: [-455.216 -455.216 -455.216] [53.7911], Avg: [-611.448 -611.448 -611.448] (1.000)
Step: 56499, Reward: [-480.53 -480.53 -480.53] [162.8622], Avg: [-611.476 -611.476 -611.476] (1.000)
Step: 56549, Reward: [-426.863 -426.863 -426.863] [93.8500], Avg: [-611.396 -611.396 -611.396] (1.000)
Step: 56599, Reward: [-546.306 -546.306 -546.306] [103.4185], Avg: [-611.429 -611.429 -611.429] (1.000)
Step: 56649, Reward: [-541.541 -541.541 -541.541] [140.1998], Avg: [-611.491 -611.491 -611.491] (1.000)
Step: 56699, Reward: [-514.354 -514.354 -514.354] [89.4788], Avg: [-611.485 -611.485 -611.485] (1.000)
Step: 56749, Reward: [-473.527 -473.527 -473.527] [81.5915], Avg: [-611.435 -611.435 -611.435] (1.000)
Step: 56799, Reward: [-631.217 -631.217 -631.217] [98.5037], Avg: [-611.539 -611.539 -611.539] (1.000)
Step: 56849, Reward: [-595.458 -595.458 -595.458] [100.8581], Avg: [-611.614 -611.614 -611.614] (1.000)
Step: 56899, Reward: [-589.233 -589.233 -589.233] [97.7983], Avg: [-611.68 -611.68 -611.68] (1.000)
Step: 56949, Reward: [-560.888 -560.888 -560.888] [82.5167], Avg: [-611.708 -611.708 -611.708] (1.000)
Step: 56999, Reward: [-569.767 -569.767 -569.767] [107.4890], Avg: [-611.765 -611.765 -611.765] (1.000)
Step: 57049, Reward: [-552.919 -552.919 -552.919] [75.8990], Avg: [-611.78 -611.78 -611.78] (1.000)
Step: 57099, Reward: [-546.487 -546.487 -546.487] [43.6509], Avg: [-611.761 -611.761 -611.761] (1.000)
Step: 57149, Reward: [-513.993 -513.993 -513.993] [59.9274], Avg: [-611.728 -611.728 -611.728] (1.000)
Step: 57199, Reward: [-492.02 -492.02 -492.02] [38.0499], Avg: [-611.657 -611.657 -611.657] (1.000)
Step: 57249, Reward: [-556.726 -556.726 -556.726] [67.1095], Avg: [-611.668 -611.668 -611.668] (1.000)
Step: 57299, Reward: [-510.169 -510.169 -510.169] [60.7048], Avg: [-611.632 -611.632 -611.632] (1.000)
Step: 57349, Reward: [-566.161 -566.161 -566.161] [64.1403], Avg: [-611.648 -611.648 -611.648] (1.000)
Step: 57399, Reward: [-546.04 -546.04 -546.04] [100.8178], Avg: [-611.679 -611.679 -611.679] (1.000)
Step: 57449, Reward: [-706.659 -706.659 -706.659] [245.0763], Avg: [-611.975 -611.975 -611.975] (1.000)
Step: 57499, Reward: [-536.87 -536.87 -536.87] [138.9775], Avg: [-612.03 -612.03 -612.03] (1.000)
Step: 57549, Reward: [-564.692 -564.692 -564.692] [44.0526], Avg: [-612.028 -612.028 -612.028] (1.000)
Step: 57599, Reward: [-428.511 -428.511 -428.511] [64.7668], Avg: [-611.924 -611.924 -611.924] (1.000)
Step: 57649, Reward: [-499.882 -499.882 -499.882] [109.4695], Avg: [-611.922 -611.922 -611.922] (1.000)
Step: 57699, Reward: [-557.597 -557.597 -557.597] [139.7820], Avg: [-611.996 -611.996 -611.996] (1.000)
Step: 57749, Reward: [-489.286 -489.286 -489.286] [39.0511], Avg: [-611.924 -611.924 -611.924] (1.000)
Step: 57799, Reward: [-519.417 -519.417 -519.417] [61.6096], Avg: [-611.897 -611.897 -611.897] (1.000)
Step: 57849, Reward: [-547.871 -547.871 -547.871] [127.4093], Avg: [-611.952 -611.952 -611.952] (1.000)
Step: 57899, Reward: [-641.84 -641.84 -641.84] [105.1308], Avg: [-612.068 -612.068 -612.068] (1.000)
Step: 57949, Reward: [-598.852 -598.852 -598.852] [116.0063], Avg: [-612.157 -612.157 -612.157] (1.000)
Step: 57999, Reward: [-624.308 -624.308 -624.308] [67.8978], Avg: [-612.226 -612.226 -612.226] (1.000)
Step: 58049, Reward: [-546.523 -546.523 -546.523] [117.9186], Avg: [-612.271 -612.271 -612.271] (1.000)
Step: 58099, Reward: [-461.659 -461.659 -461.659] [66.5050], Avg: [-612.199 -612.199 -612.199] (1.000)
Step: 58149, Reward: [-454.992 -454.992 -454.992] [71.2789], Avg: [-612.125 -612.125 -612.125] (1.000)
Step: 58199, Reward: [-520.573 -520.573 -520.573] [73.8867], Avg: [-612.11 -612.11 -612.11] (1.000)
Step: 58249, Reward: [-486.871 -486.871 -486.871] [92.1509], Avg: [-612.081 -612.081 -612.081] (1.000)
Step: 58299, Reward: [-481.177 -481.177 -481.177] [102.1239], Avg: [-612.057 -612.057 -612.057] (1.000)
Step: 58349, Reward: [-607.49 -607.49 -607.49] [214.6857], Avg: [-612.237 -612.237 -612.237] (1.000)
Step: 58399, Reward: [-474.855 -474.855 -474.855] [71.8470], Avg: [-612.181 -612.181 -612.181] (1.000)
Step: 58449, Reward: [-638.809 -638.809 -638.809] [141.4742], Avg: [-612.324 -612.324 -612.324] (1.000)
Step: 58499, Reward: [-535.361 -535.361 -535.361] [98.7563], Avg: [-612.343 -612.343 -612.343] (1.000)
Step: 58549, Reward: [-586.119 -586.119 -586.119] [56.8390], Avg: [-612.369 -612.369 -612.369] (1.000)
Step: 58599, Reward: [-529.538 -529.538 -529.538] [77.9271], Avg: [-612.365 -612.365 -612.365] (1.000)
Step: 58649, Reward: [-592.525 -592.525 -592.525] [138.6269], Avg: [-612.466 -612.466 -612.466] (1.000)
Step: 58699, Reward: [-563.745 -563.745 -563.745] [84.2423], Avg: [-612.496 -612.496 -612.496] (1.000)
Step: 58749, Reward: [-561.183 -561.183 -561.183] [91.3726], Avg: [-612.531 -612.531 -612.531] (1.000)
Step: 58799, Reward: [-629.972 -629.972 -629.972] [66.5055], Avg: [-612.602 -612.602 -612.602] (1.000)
Step: 58849, Reward: [-552.449 -552.449 -552.449] [75.4110], Avg: [-612.615 -612.615 -612.615] (1.000)
Step: 58899, Reward: [-502.231 -502.231 -502.231] [65.1289], Avg: [-612.577 -612.577 -612.577] (1.000)
Step: 58949, Reward: [-608.965 -608.965 -608.965] [81.7654], Avg: [-612.643 -612.643 -612.643] (1.000)
Step: 58999, Reward: [-607.722 -607.722 -607.722] [176.6772], Avg: [-612.788 -612.788 -612.788] (1.000)
Step: 59049, Reward: [-561.503 -561.503 -561.503] [92.8247], Avg: [-612.824 -612.824 -612.824] (1.000)
Step: 59099, Reward: [-514.645 -514.645 -514.645] [85.4905], Avg: [-612.813 -612.813 -612.813] (1.000)
Step: 59149, Reward: [-539.715 -539.715 -539.715] [55.9344], Avg: [-612.798 -612.798 -612.798] (1.000)
Step: 59199, Reward: [-529.264 -529.264 -529.264] [102.8841], Avg: [-612.815 -612.815 -612.815] (1.000)
Step: 59249, Reward: [-518.427 -518.427 -518.427] [95.2167], Avg: [-612.815 -612.815 -612.815] (1.000)
Step: 59299, Reward: [-444.526 -444.526 -444.526] [66.9629], Avg: [-612.73 -612.73 -612.73] (1.000)
Step: 59349, Reward: [-499.509 -499.509 -499.509] [124.7944], Avg: [-612.74 -612.74 -612.74] (1.000)
Step: 59399, Reward: [-513.844 -513.844 -513.844] [91.6328], Avg: [-612.734 -612.734 -612.734] (1.000)
Step: 59449, Reward: [-489.555 -489.555 -489.555] [34.0043], Avg: [-612.659 -612.659 -612.659] (1.000)
Step: 59499, Reward: [-521.963 -521.963 -521.963] [110.9145], Avg: [-612.676 -612.676 -612.676] (1.000)
Step: 59549, Reward: [-611.633 -611.633 -611.633] [154.2436], Avg: [-612.804 -612.804 -612.804] (1.000)
Step: 59599, Reward: [-568.934 -568.934 -568.934] [71.1260], Avg: [-612.827 -612.827 -612.827] (1.000)
Step: 59649, Reward: [-465.332 -465.332 -465.332] [52.7908], Avg: [-612.748 -612.748 -612.748] (1.000)
Step: 59699, Reward: [-587.893 -587.893 -587.893] [51.1588], Avg: [-612.77 -612.77 -612.77] (1.000)
Step: 59749, Reward: [-480.021 -480.021 -480.021] [72.1231], Avg: [-612.719 -612.719 -612.719] (1.000)
Step: 59799, Reward: [-581.658 -581.658 -581.658] [70.7085], Avg: [-612.752 -612.752 -612.752] (1.000)
Step: 59849, Reward: [-600.539 -600.539 -600.539] [183.0323], Avg: [-612.895 -612.895 -612.895] (1.000)
Step: 59899, Reward: [-580.801 -580.801 -580.801] [81.9584], Avg: [-612.936 -612.936 -612.936] (1.000)
Step: 59949, Reward: [-607.844 -607.844 -607.844] [103.9021], Avg: [-613.019 -613.019 -613.019] (1.000)
Step: 59999, Reward: [-552.606 -552.606 -552.606] [177.1202], Avg: [-613.116 -613.116 -613.116] (1.000)
Step: 60049, Reward: [-625.303 -625.303 -625.303] [113.1589], Avg: [-613.22 -613.22 -613.22] (1.000)
Step: 60099, Reward: [-460.383 -460.383 -460.383] [102.2901], Avg: [-613.178 -613.178 -613.178] (1.000)
Step: 60149, Reward: [-449.776 -449.776 -449.776] [58.2980], Avg: [-613.091 -613.091 -613.091] (1.000)
Step: 60199, Reward: [-554.737 -554.737 -554.737] [78.0709], Avg: [-613.107 -613.107 -613.107] (1.000)
Step: 60249, Reward: [-527.063 -527.063 -527.063] [61.8285], Avg: [-613.087 -613.087 -613.087] (1.000)
Step: 60299, Reward: [-552.995 -552.995 -552.995] [56.3780], Avg: [-613.084 -613.084 -613.084] (1.000)
Step: 60349, Reward: [-574.33 -574.33 -574.33] [120.9258], Avg: [-613.152 -613.152 -613.152] (1.000)
Step: 60399, Reward: [-537.191 -537.191 -537.191] [113.9777], Avg: [-613.184 -613.184 -613.184] (1.000)
Step: 60449, Reward: [-513.641 -513.641 -513.641] [72.3407], Avg: [-613.161 -613.161 -613.161] (1.000)
Step: 60499, Reward: [-511.151 -511.151 -511.151] [75.1450], Avg: [-613.139 -613.139 -613.139] (1.000)
Step: 60549, Reward: [-515.785 -515.785 -515.785] [36.0329], Avg: [-613.088 -613.088 -613.088] (1.000)
Step: 60599, Reward: [-492.736 -492.736 -492.736] [58.7824], Avg: [-613.038 -613.038 -613.038] (1.000)
Step: 60649, Reward: [-534.736 -534.736 -534.736] [93.5741], Avg: [-613.05 -613.05 -613.05] (1.000)
Step: 60699, Reward: [-447.595 -447.595 -447.595] [82.9242], Avg: [-612.982 -612.982 -612.982] (1.000)
Step: 60749, Reward: [-503.63 -503.63 -503.63] [73.1037], Avg: [-612.952 -612.952 -612.952] (1.000)
Step: 60799, Reward: [-394.062 -394.062 -394.062] [35.7770], Avg: [-612.802 -612.802 -612.802] (1.000)
Step: 60849, Reward: [-512.044 -512.044 -512.044] [84.9032], Avg: [-612.789 -612.789 -612.789] (1.000)
Step: 60899, Reward: [-623.313 -623.313 -623.313] [131.8556], Avg: [-612.906 -612.906 -612.906] (1.000)
Step: 60949, Reward: [-494.942 -494.942 -494.942] [57.0834], Avg: [-612.856 -612.856 -612.856] (1.000)
Step: 60999, Reward: [-587.367 -587.367 -587.367] [114.7152], Avg: [-612.929 -612.929 -612.929] (1.000)
Step: 61049, Reward: [-471.016 -471.016 -471.016] [40.1824], Avg: [-612.846 -612.846 -612.846] (1.000)
Step: 61099, Reward: [-504.06 -504.06 -504.06] [134.3002], Avg: [-612.866 -612.866 -612.866] (1.000)
Step: 61149, Reward: [-516.491 -516.491 -516.491] [184.7525], Avg: [-612.939 -612.939 -612.939] (1.000)
Step: 61199, Reward: [-493.005 -493.005 -493.005] [52.0796], Avg: [-612.883 -612.883 -612.883] (1.000)
Step: 61249, Reward: [-482.336 -482.336 -482.336] [79.3750], Avg: [-612.841 -612.841 -612.841] (1.000)
Step: 61299, Reward: [-550.103 -550.103 -550.103] [45.9188], Avg: [-612.828 -612.828 -612.828] (1.000)
Step: 61349, Reward: [-474.742 -474.742 -474.742] [85.4640], Avg: [-612.785 -612.785 -612.785] (1.000)
Step: 61399, Reward: [-497.047 -497.047 -497.047] [152.9945], Avg: [-612.815 -612.815 -612.815] (1.000)
Step: 61449, Reward: [-549.617 -549.617 -549.617] [92.0648], Avg: [-612.839 -612.839 -612.839] (1.000)
Step: 61499, Reward: [-545.224 -545.224 -545.224] [70.8798], Avg: [-612.841 -612.841 -612.841] (1.000)
Step: 61549, Reward: [-565.276 -565.276 -565.276] [103.1641], Avg: [-612.887 -612.887 -612.887] (1.000)
Step: 61599, Reward: [-568.554 -568.554 -568.554] [157.0028], Avg: [-612.978 -612.978 -612.978] (1.000)
Step: 61649, Reward: [-515.886 -515.886 -515.886] [94.8778], Avg: [-612.976 -612.976 -612.976] (1.000)
Step: 61699, Reward: [-489.742 -489.742 -489.742] [142.6267], Avg: [-612.992 -612.992 -612.992] (1.000)
Step: 61749, Reward: [-509.887 -509.887 -509.887] [102.3334], Avg: [-612.991 -612.991 -612.991] (1.000)
Step: 61799, Reward: [-543.781 -543.781 -543.781] [108.4196], Avg: [-613.023 -613.023 -613.023] (1.000)
Step: 61849, Reward: [-628.374 -628.374 -628.374] [136.0641], Avg: [-613.145 -613.145 -613.145] (1.000)
Step: 61899, Reward: [-607.273 -607.273 -607.273] [65.8816], Avg: [-613.194 -613.194 -613.194] (1.000)
Step: 61949, Reward: [-559.883 -559.883 -559.883] [112.0131], Avg: [-613.241 -613.241 -613.241] (1.000)
Step: 61999, Reward: [-653.908 -653.908 -653.908] [94.8268], Avg: [-613.351 -613.351 -613.351] (1.000)
Step: 62049, Reward: [-551.121 -551.121 -551.121] [169.5692], Avg: [-613.437 -613.437 -613.437] (1.000)
Step: 62099, Reward: [-513.307 -513.307 -513.307] [90.3929], Avg: [-613.429 -613.429 -613.429] (1.000)
Step: 62149, Reward: [-447.913 -447.913 -447.913] [57.3571], Avg: [-613.342 -613.342 -613.342] (1.000)
Step: 62199, Reward: [-548.301 -548.301 -548.301] [124.0336], Avg: [-613.39 -613.39 -613.39] (1.000)
Step: 62249, Reward: [-532.613 -532.613 -532.613] [63.2245], Avg: [-613.375 -613.375 -613.375] (1.000)
Step: 62299, Reward: [-623.449 -623.449 -623.449] [156.2880], Avg: [-613.509 -613.509 -613.509] (1.000)
Step: 62349, Reward: [-528.77 -528.77 -528.77] [97.0699], Avg: [-613.519 -613.519 -613.519] (1.000)
Step: 62399, Reward: [-489.281 -489.281 -489.281] [75.0020], Avg: [-613.479 -613.479 -613.479] (1.000)
Step: 62449, Reward: [-542.316 -542.316 -542.316] [61.7215], Avg: [-613.472 -613.472 -613.472] (1.000)
Step: 62499, Reward: [-543.621 -543.621 -543.621] [82.8252], Avg: [-613.482 -613.482 -613.482] (1.000)
Step: 62549, Reward: [-600.515 -600.515 -600.515] [153.5078], Avg: [-613.595 -613.595 -613.595] (1.000)
Step: 62599, Reward: [-501.875 -501.875 -501.875] [61.8565], Avg: [-613.555 -613.555 -613.555] (1.000)
Step: 62649, Reward: [-491.901 -491.901 -491.901] [96.8777], Avg: [-613.535 -613.535 -613.535] (1.000)
Step: 62699, Reward: [-563.998 -563.998 -563.998] [95.3209], Avg: [-613.572 -613.572 -613.572] (1.000)
Step: 62749, Reward: [-670.927 -670.927 -670.927] [166.4641], Avg: [-613.75 -613.75 -613.75] (1.000)
Step: 62799, Reward: [-558.582 -558.582 -558.582] [123.2803], Avg: [-613.804 -613.804 -613.804] (1.000)
Step: 62849, Reward: [-541.113 -541.113 -541.113] [161.6934], Avg: [-613.875 -613.875 -613.875] (1.000)
Step: 62899, Reward: [-483.034 -483.034 -483.034] [107.4391], Avg: [-613.856 -613.856 -613.856] (1.000)
Step: 62949, Reward: [-536.079 -536.079 -536.079] [154.0588], Avg: [-613.917 -613.917 -613.917] (1.000)
Step: 62999, Reward: [-520.592 -520.592 -520.592] [84.4479], Avg: [-613.91 -613.91 -613.91] (1.000)
Step: 63049, Reward: [-496.027 -496.027 -496.027] [90.5133], Avg: [-613.888 -613.888 -613.888] (1.000)
Step: 63099, Reward: [-512.88 -512.88 -512.88] [60.1702], Avg: [-613.856 -613.856 -613.856] (1.000)
Step: 63149, Reward: [-418.758 -418.758 -418.758] [84.6745], Avg: [-613.768 -613.768 -613.768] (1.000)
Step: 63199, Reward: [-404.19 -404.19 -404.19] [60.7223], Avg: [-613.651 -613.651 -613.651] (1.000)
Step: 63249, Reward: [-499.646 -499.646 -499.646] [144.6268], Avg: [-613.675 -613.675 -613.675] (1.000)
Step: 63299, Reward: [-534.961 -534.961 -534.961] [90.0258], Avg: [-613.684 -613.684 -613.684] (1.000)
Step: 63349, Reward: [-517.799 -517.799 -517.799] [54.9610], Avg: [-613.651 -613.651 -613.651] (1.000)
Step: 63399, Reward: [-545.381 -545.381 -545.381] [84.1372], Avg: [-613.664 -613.664 -613.664] (1.000)
Step: 63449, Reward: [-485.605 -485.605 -485.605] [81.3155], Avg: [-613.627 -613.627 -613.627] (1.000)
Step: 63499, Reward: [-537.333 -537.333 -537.333] [132.8038], Avg: [-613.672 -613.672 -613.672] (1.000)
Step: 63549, Reward: [-564.302 -564.302 -564.302] [145.8708], Avg: [-613.748 -613.748 -613.748] (1.000)
Step: 63599, Reward: [-574.004 -574.004 -574.004] [115.8519], Avg: [-613.807 -613.807 -613.807] (1.000)
Step: 63649, Reward: [-472.698 -472.698 -472.698] [110.6276], Avg: [-613.783 -613.783 -613.783] (1.000)
Step: 63699, Reward: [-548.579 -548.579 -548.579] [187.9848], Avg: [-613.88 -613.88 -613.88] (1.000)
Step: 63749, Reward: [-552.96 -552.96 -552.96] [149.3287], Avg: [-613.949 -613.949 -613.949] (1.000)
Step: 63799, Reward: [-656.835 -656.835 -656.835] [143.7438], Avg: [-614.095 -614.095 -614.095] (1.000)
Step: 63849, Reward: [-530.387 -530.387 -530.387] [115.6261], Avg: [-614.12 -614.12 -614.12] (1.000)
Step: 63899, Reward: [-574.2 -574.2 -574.2] [95.8280], Avg: [-614.164 -614.164 -614.164] (1.000)
Step: 63949, Reward: [-589.158 -589.158 -589.158] [57.7611], Avg: [-614.19 -614.19 -614.19] (1.000)
Step: 63999, Reward: [-587.498 -587.498 -587.498] [139.3763], Avg: [-614.278 -614.278 -614.278] (1.000)
Step: 64049, Reward: [-546.235 -546.235 -546.235] [120.0835], Avg: [-614.318 -614.318 -614.318] (1.000)
Step: 64099, Reward: [-443. -443. -443.] [49.3558], Avg: [-614.223 -614.223 -614.223] (1.000)
Step: 64149, Reward: [-607.292 -607.292 -607.292] [103.7608], Avg: [-614.299 -614.299 -614.299] (1.000)
Step: 64199, Reward: [-512.713 -512.713 -512.713] [100.6523], Avg: [-614.298 -614.298 -614.298] (1.000)
Step: 64249, Reward: [-587.988 -587.988 -587.988] [135.1758], Avg: [-614.383 -614.383 -614.383] (1.000)
Step: 64299, Reward: [-503.984 -503.984 -503.984] [144.6759], Avg: [-614.409 -614.409 -614.409] (1.000)
Step: 64349, Reward: [-486.434 -486.434 -486.434] [95.7267], Avg: [-614.384 -614.384 -614.384] (1.000)
Step: 64399, Reward: [-497.851 -497.851 -497.851] [120.7220], Avg: [-614.388 -614.388 -614.388] (1.000)
Step: 64449, Reward: [-552.427 -552.427 -552.427] [106.9266], Avg: [-614.422 -614.422 -614.422] (1.000)
Step: 64499, Reward: [-424.622 -424.622 -424.622] [17.5443], Avg: [-614.289 -614.289 -614.289] (1.000)
Step: 64549, Reward: [-437.522 -437.522 -437.522] [136.2477], Avg: [-614.258 -614.258 -614.258] (1.000)
Step: 64599, Reward: [-522.382 -522.382 -522.382] [96.9709], Avg: [-614.261 -614.261 -614.261] (1.000)
Step: 64649, Reward: [-416.076 -416.076 -416.076] [50.7875], Avg: [-614.147 -614.147 -614.147] (1.000)
Step: 64699, Reward: [-474.903 -474.903 -474.903] [68.9632], Avg: [-614.093 -614.093 -614.093] (1.000)
Step: 64749, Reward: [-413.714 -413.714 -413.714] [38.5642], Avg: [-613.968 -613.968 -613.968] (1.000)
Step: 64799, Reward: [-470.508 -470.508 -470.508] [68.2043], Avg: [-613.91 -613.91 -613.91] (1.000)
Step: 64849, Reward: [-447.397 -447.397 -447.397] [49.2405], Avg: [-613.82 -613.82 -613.82] (1.000)
Step: 64899, Reward: [-447.451 -447.451 -447.451] [110.7402], Avg: [-613.777 -613.777 -613.777] (1.000)
Step: 64949, Reward: [-466.885 -466.885 -466.885] [72.9718], Avg: [-613.72 -613.72 -613.72] (1.000)
Step: 64999, Reward: [-384.473 -384.473 -384.473] [53.7806], Avg: [-613.585 -613.585 -613.585] (1.000)
Step: 65049, Reward: [-470.805 -470.805 -470.805] [106.2833], Avg: [-613.557 -613.557 -613.557] (1.000)
Step: 65099, Reward: [-480.819 -480.819 -480.819] [47.3469], Avg: [-613.491 -613.491 -613.491] (1.000)
Step: 65149, Reward: [-579.583 -579.583 -579.583] [116.4855], Avg: [-613.555 -613.555 -613.555] (1.000)
Step: 65199, Reward: [-499.001 -499.001 -499.001] [155.0224], Avg: [-613.586 -613.586 -613.586] (1.000)
Step: 65249, Reward: [-549.76 -549.76 -549.76] [81.9908], Avg: [-613.6 -613.6 -613.6] (1.000)
Step: 65299, Reward: [-619.041 -619.041 -619.041] [115.3821], Avg: [-613.692 -613.692 -613.692] (1.000)
Step: 65349, Reward: [-577.115 -577.115 -577.115] [87.4512], Avg: [-613.731 -613.731 -613.731] (1.000)
Step: 65399, Reward: [-558.808 -558.808 -558.808] [87.2600], Avg: [-613.756 -613.756 -613.756] (1.000)
Step: 65449, Reward: [-532.692 -532.692 -532.692] [113.1424], Avg: [-613.78 -613.78 -613.78] (1.000)
Step: 65499, Reward: [-547.507 -547.507 -547.507] [127.8354], Avg: [-613.827 -613.827 -613.827] (1.000)
Step: 65549, Reward: [-624.038 -624.038 -624.038] [205.6919], Avg: [-613.992 -613.992 -613.992] (1.000)
Step: 65599, Reward: [-659.501 -659.501 -659.501] [188.8466], Avg: [-614.171 -614.171 -614.171] (1.000)
Step: 65649, Reward: [-531.493 -531.493 -531.493] [127.9683], Avg: [-614.205 -614.205 -614.205] (1.000)
Step: 65699, Reward: [-478.132 -478.132 -478.132] [105.3390], Avg: [-614.182 -614.182 -614.182] (1.000)
Step: 65749, Reward: [-493.888 -493.888 -493.888] [70.3624], Avg: [-614.144 -614.144 -614.144] (1.000)
Step: 65799, Reward: [-609.455 -609.455 -609.455] [85.9746], Avg: [-614.206 -614.206 -614.206] (1.000)
Step: 65849, Reward: [-567.748 -567.748 -567.748] [56.2971], Avg: [-614.213 -614.213 -614.213] (1.000)
Step: 65899, Reward: [-527.911 -527.911 -527.911] [81.9752], Avg: [-614.21 -614.21 -614.21] (1.000)
Step: 65949, Reward: [-504.312 -504.312 -504.312] [51.1007], Avg: [-614.165 -614.165 -614.165] (1.000)
Step: 65999, Reward: [-561.607 -561.607 -561.607] [107.8708], Avg: [-614.207 -614.207 -614.207] (1.000)
Step: 66049, Reward: [-562.985 -562.985 -562.985] [136.0791], Avg: [-614.271 -614.271 -614.271] (1.000)
Step: 66099, Reward: [-687.439 -687.439 -687.439] [217.1437], Avg: [-614.491 -614.491 -614.491] (1.000)
Step: 66149, Reward: [-573.628 -573.628 -573.628] [189.8228], Avg: [-614.604 -614.604 -614.604] (1.000)
Step: 66199, Reward: [-568.388 -568.388 -568.388] [92.9546], Avg: [-614.639 -614.639 -614.639] (1.000)
Step: 66249, Reward: [-564.167 -564.167 -564.167] [95.1271], Avg: [-614.673 -614.673 -614.673] (1.000)
Step: 66299, Reward: [-507.756 -507.756 -507.756] [116.8097], Avg: [-614.68 -614.68 -614.68] (1.000)
Step: 66349, Reward: [-611.068 -611.068 -611.068] [127.4923], Avg: [-614.773 -614.773 -614.773] (1.000)
Step: 66399, Reward: [-539.787 -539.787 -539.787] [42.2886], Avg: [-614.749 -614.749 -614.749] (1.000)
Step: 66449, Reward: [-511.986 -511.986 -511.986] [70.7717], Avg: [-614.725 -614.725 -614.725] (1.000)
Step: 66499, Reward: [-538.143 -538.143 -538.143] [86.5356], Avg: [-614.732 -614.732 -614.732] (1.000)
Step: 66549, Reward: [-509.342 -509.342 -509.342] [67.1639], Avg: [-614.703 -614.703 -614.703] (1.000)
Step: 66599, Reward: [-528.627 -528.627 -528.627] [46.5836], Avg: [-614.674 -614.674 -614.674] (1.000)
Step: 66649, Reward: [-448.697 -448.697 -448.697] [72.3307], Avg: [-614.603 -614.603 -614.603] (1.000)
Step: 66699, Reward: [-502.079 -502.079 -502.079] [74.0295], Avg: [-614.575 -614.575 -614.575] (1.000)
Step: 66749, Reward: [-506.894 -506.894 -506.894] [67.8612], Avg: [-614.545 -614.545 -614.545] (1.000)
Step: 66799, Reward: [-430.927 -430.927 -430.927] [72.8514], Avg: [-614.462 -614.462 -614.462] (1.000)
Step: 66849, Reward: [-438.785 -438.785 -438.785] [70.6858], Avg: [-614.383 -614.383 -614.383] (1.000)
Step: 66899, Reward: [-415.852 -415.852 -415.852] [90.6402], Avg: [-614.303 -614.303 -614.303] (1.000)
Step: 66949, Reward: [-528.835 -528.835 -528.835] [122.5239], Avg: [-614.33 -614.33 -614.33] (1.000)
Step: 66999, Reward: [-509.17 -509.17 -509.17] [122.6867], Avg: [-614.343 -614.343 -614.343] (1.000)
Step: 67049, Reward: [-485.618 -485.618 -485.618] [36.1852], Avg: [-614.274 -614.274 -614.274] (1.000)
Step: 67099, Reward: [-434.794 -434.794 -434.794] [68.1930], Avg: [-614.192 -614.192 -614.192] (1.000)
Step: 67149, Reward: [-532.6 -532.6 -532.6] [69.4235], Avg: [-614.182 -614.182 -614.182] (1.000)
Step: 67199, Reward: [-474.646 -474.646 -474.646] [70.5149], Avg: [-614.131 -614.131 -614.131] (1.000)
Step: 67249, Reward: [-540.611 -540.611 -540.611] [84.9715], Avg: [-614.14 -614.14 -614.14] (1.000)
Step: 67299, Reward: [-578.141 -578.141 -578.141] [104.2950], Avg: [-614.19 -614.19 -614.19] (1.000)
Step: 67349, Reward: [-540.319 -540.319 -540.319] [98.1409], Avg: [-614.208 -614.208 -614.208] (1.000)
Step: 67399, Reward: [-514.271 -514.271 -514.271] [106.1357], Avg: [-614.213 -614.213 -614.213] (1.000)
Step: 67449, Reward: [-573.249 -573.249 -573.249] [84.5009], Avg: [-614.245 -614.245 -614.245] (1.000)
Step: 67499, Reward: [-508.312 -508.312 -508.312] [100.3446], Avg: [-614.241 -614.241 -614.241] (1.000)
Step: 67549, Reward: [-573.81 -573.81 -573.81] [16.7909], Avg: [-614.224 -614.224 -614.224] (1.000)
Step: 67599, Reward: [-550.567 -550.567 -550.567] [96.4049], Avg: [-614.248 -614.248 -614.248] (1.000)
Step: 67649, Reward: [-692.525 -692.525 -692.525] [156.4916], Avg: [-614.421 -614.421 -614.421] (1.000)
Step: 67699, Reward: [-481.726 -481.726 -481.726] [79.1822], Avg: [-614.382 -614.382 -614.382] (1.000)
Step: 67749, Reward: [-554.955 -554.955 -554.955] [28.8514], Avg: [-614.359 -614.359 -614.359] (1.000)
Step: 67799, Reward: [-500.524 -500.524 -500.524] [55.8180], Avg: [-614.317 -614.317 -614.317] (1.000)
Step: 67849, Reward: [-478.524 -478.524 -478.524] [63.0341], Avg: [-614.263 -614.263 -614.263] (1.000)
Step: 67899, Reward: [-580.935 -580.935 -580.935] [123.7804], Avg: [-614.33 -614.33 -614.33] (1.000)
Step: 67949, Reward: [-557.899 -557.899 -557.899] [35.2668], Avg: [-614.314 -614.314 -614.314] (1.000)
Step: 67999, Reward: [-558.143 -558.143 -558.143] [135.9727], Avg: [-614.373 -614.373 -614.373] (1.000)
Step: 68049, Reward: [-513.635 -513.635 -513.635] [83.0818], Avg: [-614.36 -614.36 -614.36] (1.000)
Step: 68099, Reward: [-563.101 -563.101 -563.101] [158.8726], Avg: [-614.439 -614.439 -614.439] (1.000)
Step: 68149, Reward: [-539.812 -539.812 -539.812] [75.6331], Avg: [-614.439 -614.439 -614.439] (1.000)
Step: 68199, Reward: [-458.55 -458.55 -458.55] [68.4029], Avg: [-614.375 -614.375 -614.375] (1.000)
Step: 68249, Reward: [-566.054 -566.054 -566.054] [136.3104], Avg: [-614.44 -614.44 -614.44] (1.000)
Step: 68299, Reward: [-547.847 -547.847 -547.847] [114.0830], Avg: [-614.474 -614.474 -614.474] (1.000)
Step: 68349, Reward: [-669.629 -669.629 -669.629] [94.3046], Avg: [-614.584 -614.584 -614.584] (1.000)
Step: 68399, Reward: [-503.713 -503.713 -503.713] [98.2249], Avg: [-614.575 -614.575 -614.575] (1.000)
Step: 68449, Reward: [-478.532 -478.532 -478.532] [74.6488], Avg: [-614.53 -614.53 -614.53] (1.000)
Step: 68499, Reward: [-507.749 -507.749 -507.749] [106.8552], Avg: [-614.53 -614.53 -614.53] (1.000)
Step: 68549, Reward: [-513.21 -513.21 -513.21] [31.2649], Avg: [-614.479 -614.479 -614.479] (1.000)
Step: 68599, Reward: [-470.838 -470.838 -470.838] [40.0134], Avg: [-614.403 -614.403 -614.403] (1.000)
Step: 68649, Reward: [-521.048 -521.048 -521.048] [74.9874], Avg: [-614.39 -614.39 -614.39] (1.000)
Step: 68699, Reward: [-496.749 -496.749 -496.749] [67.4604], Avg: [-614.353 -614.353 -614.353] (1.000)
Step: 68749, Reward: [-573.875 -573.875 -573.875] [177.1999], Avg: [-614.453 -614.453 -614.453] (1.000)
Step: 68799, Reward: [-523.743 -523.743 -523.743] [31.9552], Avg: [-614.41 -614.41 -614.41] (1.000)
Step: 68849, Reward: [-471.886 -471.886 -471.886] [59.3589], Avg: [-614.35 -614.35 -614.35] (1.000)
Step: 68899, Reward: [-569.038 -569.038 -569.038] [86.8884], Avg: [-614.38 -614.38 -614.38] (1.000)
Step: 68949, Reward: [-517.889 -517.889 -517.889] [79.8474], Avg: [-614.368 -614.368 -614.368] (1.000)
Step: 68999, Reward: [-546.981 -546.981 -546.981] [25.7097], Avg: [-614.337 -614.337 -614.337] (1.000)
Step: 69049, Reward: [-500.151 -500.151 -500.151] [23.4423], Avg: [-614.272 -614.272 -614.272] (1.000)
Step: 69099, Reward: [-516.166 -516.166 -516.166] [69.2073], Avg: [-614.251 -614.251 -614.251] (1.000)
Step: 69149, Reward: [-649.935 -649.935 -649.935] [43.9463], Avg: [-614.308 -614.308 -614.308] (1.000)
Step: 69199, Reward: [-525.762 -525.762 -525.762] [113.7155], Avg: [-614.327 -614.327 -614.327] (1.000)
Step: 69249, Reward: [-406.274 -406.274 -406.274] [38.7869], Avg: [-614.204 -614.204 -614.204] (1.000)
Step: 69299, Reward: [-560.497 -560.497 -560.497] [97.6921], Avg: [-614.236 -614.236 -614.236] (1.000)
Step: 69349, Reward: [-563.079 -563.079 -563.079] [136.7608], Avg: [-614.298 -614.298 -614.298] (1.000)
Step: 69399, Reward: [-421.398 -421.398 -421.398] [39.3672], Avg: [-614.187 -614.187 -614.187] (1.000)
Step: 69449, Reward: [-527.099 -527.099 -527.099] [60.0905], Avg: [-614.168 -614.168 -614.168] (1.000)
Step: 69499, Reward: [-464.631 -464.631 -464.631] [86.9285], Avg: [-614.123 -614.123 -614.123] (1.000)
Step: 69549, Reward: [-515.852 -515.852 -515.852] [47.5502], Avg: [-614.086 -614.086 -614.086] (1.000)
Step: 69599, Reward: [-490.054 -490.054 -490.054] [44.1065], Avg: [-614.029 -614.029 -614.029] (1.000)
Step: 69649, Reward: [-491.504 -491.504 -491.504] [98.5793], Avg: [-614.012 -614.012 -614.012] (1.000)
Step: 69699, Reward: [-648.099 -648.099 -648.099] [267.6098], Avg: [-614.228 -614.228 -614.228] (1.000)
Step: 69749, Reward: [-634.321 -634.321 -634.321] [171.0326], Avg: [-614.365 -614.365 -614.365] (1.000)
Step: 69799, Reward: [-534.229 -534.229 -534.229] [74.4157], Avg: [-614.361 -614.361 -614.361] (1.000)
Step: 69849, Reward: [-698.633 -698.633 -698.633] [257.5966], Avg: [-614.606 -614.606 -614.606] (1.000)
Step: 69899, Reward: [-632.652 -632.652 -632.652] [99.0313], Avg: [-614.69 -614.69 -614.69] (1.000)
Step: 69949, Reward: [-452.74 -452.74 -452.74] [77.3884], Avg: [-614.629 -614.629 -614.629] (1.000)
Step: 69999, Reward: [-508.283 -508.283 -508.283] [75.0859], Avg: [-614.607 -614.607 -614.607] (1.000)
Step: 70049, Reward: [-551.597 -551.597 -551.597] [91.6448], Avg: [-614.627 -614.627 -614.627] (1.000)
Step: 70099, Reward: [-533.581 -533.581 -533.581] [118.8317], Avg: [-614.654 -614.654 -614.654] (1.000)
Step: 70149, Reward: [-597.802 -597.802 -597.802] [164.8349], Avg: [-614.76 -614.76 -614.76] (1.000)
Step: 70199, Reward: [-515.154 -515.154 -515.154] [58.5216], Avg: [-614.73 -614.73 -614.73] (1.000)
Step: 70249, Reward: [-517.337 -517.337 -517.337] [60.0746], Avg: [-614.704 -614.704 -614.704] (1.000)
Step: 70299, Reward: [-508.037 -508.037 -508.037] [86.2821], Avg: [-614.689 -614.689 -614.689] (1.000)
Step: 70349, Reward: [-514.494 -514.494 -514.494] [63.1130], Avg: [-614.663 -614.663 -614.663] (1.000)
Step: 70399, Reward: [-504.003 -504.003 -504.003] [109.3399], Avg: [-614.662 -614.662 -614.662] (1.000)
Step: 70449, Reward: [-492.945 -492.945 -492.945] [105.7519], Avg: [-614.651 -614.651 -614.651] (1.000)
Step: 70499, Reward: [-541.676 -541.676 -541.676] [59.8625], Avg: [-614.641 -614.641 -614.641] (1.000)
Step: 70549, Reward: [-503.623 -503.623 -503.623] [90.5107], Avg: [-614.627 -614.627 -614.627] (1.000)
Step: 70599, Reward: [-569.85 -569.85 -569.85] [85.9404], Avg: [-614.656 -614.656 -614.656] (1.000)
Step: 70649, Reward: [-507.798 -507.798 -507.798] [35.1693], Avg: [-614.605 -614.605 -614.605] (1.000)
Step: 70699, Reward: [-488.77 -488.77 -488.77] [61.7433], Avg: [-614.56 -614.56 -614.56] (1.000)
Step: 70749, Reward: [-506.759 -506.759 -506.759] [101.7567], Avg: [-614.556 -614.556 -614.556] (1.000)
Step: 70799, Reward: [-542.06 -542.06 -542.06] [73.5473], Avg: [-614.556 -614.556 -614.556] (1.000)
Step: 70849, Reward: [-462.701 -462.701 -462.701] [57.0404], Avg: [-614.489 -614.489 -614.489] (1.000)
Step: 70899, Reward: [-491.708 -491.708 -491.708] [126.6629], Avg: [-614.492 -614.492 -614.492] (1.000)
Step: 70949, Reward: [-546.912 -546.912 -546.912] [81.1375], Avg: [-614.502 -614.502 -614.502] (1.000)
Step: 70999, Reward: [-668.245 -668.245 -668.245] [83.4296], Avg: [-614.598 -614.598 -614.598] (1.000)
Step: 71049, Reward: [-546.729 -546.729 -546.729] [151.2631], Avg: [-614.657 -614.657 -614.657] (1.000)
Step: 71099, Reward: [-500.334 -500.334 -500.334] [97.5326], Avg: [-614.645 -614.645 -614.645] (1.000)
Step: 71149, Reward: [-596.553 -596.553 -596.553] [56.8482], Avg: [-614.672 -614.672 -614.672] (1.000)
Step: 71199, Reward: [-528.275 -528.275 -528.275] [74.9265], Avg: [-614.664 -614.664 -614.664] (1.000)
Step: 71249, Reward: [-533.34 -533.34 -533.34] [117.0657], Avg: [-614.69 -614.69 -614.69] (1.000)
Step: 71299, Reward: [-554.204 -554.204 -554.204] [122.1514], Avg: [-614.733 -614.733 -614.733] (1.000)
Step: 71349, Reward: [-551.425 -551.425 -551.425] [149.7534], Avg: [-614.793 -614.793 -614.793] (1.000)
Step: 71399, Reward: [-467.937 -467.937 -467.937] [165.8975], Avg: [-614.807 -614.807 -614.807] (1.000)
Step: 71449, Reward: [-455.683 -455.683 -455.683] [41.7321], Avg: [-614.725 -614.725 -614.725] (1.000)
Step: 71499, Reward: [-636.948 -636.948 -636.948] [118.5450], Avg: [-614.823 -614.823 -614.823] (1.000)
Step: 71549, Reward: [-452.814 -452.814 -452.814] [61.3603], Avg: [-614.753 -614.753 -614.753] (1.000)
Step: 71599, Reward: [-606.942 -606.942 -606.942] [115.2065], Avg: [-614.828 -614.828 -614.828] (1.000)
Step: 71649, Reward: [-662.917 -662.917 -662.917] [126.9425], Avg: [-614.95 -614.95 -614.95] (1.000)
Step: 71699, Reward: [-540.24 -540.24 -540.24] [66.8372], Avg: [-614.944 -614.944 -614.944] (1.000)
Step: 71749, Reward: [-515.779 -515.779 -515.779] [98.0068], Avg: [-614.943 -614.943 -614.943] (1.000)
Step: 71799, Reward: [-555.262 -555.262 -555.262] [50.2829], Avg: [-614.937 -614.937 -614.937] (1.000)
Step: 71849, Reward: [-539.001 -539.001 -539.001] [98.2802], Avg: [-614.952 -614.952 -614.952] (1.000)
Step: 71899, Reward: [-457.239 -457.239 -457.239] [62.9796], Avg: [-614.887 -614.887 -614.887] (1.000)
Step: 71949, Reward: [-545.292 -545.292 -545.292] [119.5022], Avg: [-614.921 -614.921 -614.921] (1.000)
Step: 71999, Reward: [-477.936 -477.936 -477.936] [48.2392], Avg: [-614.86 -614.86 -614.86] (1.000)
Step: 72049, Reward: [-393.008 -393.008 -393.008] [62.3257], Avg: [-614.749 -614.749 -614.749] (1.000)
Step: 72099, Reward: [-430.95 -430.95 -430.95] [83.9683], Avg: [-614.68 -614.68 -614.68] (1.000)
Step: 72149, Reward: [-611.656 -611.656 -611.656] [33.7670], Avg: [-614.701 -614.701 -614.701] (1.000)
Step: 72199, Reward: [-475.844 -475.844 -475.844] [93.7045], Avg: [-614.67 -614.67 -614.67] (1.000)
Step: 72249, Reward: [-482.893 -482.893 -482.893] [93.2369], Avg: [-614.643 -614.643 -614.643] (1.000)
Step: 72299, Reward: [-588.205 -588.205 -588.205] [103.8025], Avg: [-614.697 -614.697 -614.697] (1.000)
Step: 72349, Reward: [-460.221 -460.221 -460.221] [46.0671], Avg: [-614.622 -614.622 -614.622] (1.000)
Step: 72399, Reward: [-582.47 -582.47 -582.47] [154.3620], Avg: [-614.706 -614.706 -614.706] (1.000)
Step: 72449, Reward: [-559.855 -559.855 -559.855] [103.3406], Avg: [-614.74 -614.74 -614.74] (1.000)
Step: 72499, Reward: [-467.111 -467.111 -467.111] [56.1437], Avg: [-614.676 -614.676 -614.676] (1.000)
Step: 72549, Reward: [-574.064 -574.064 -574.064] [132.7371], Avg: [-614.74 -614.74 -614.74] (1.000)
Step: 72599, Reward: [-534.735 -534.735 -534.735] [163.4675], Avg: [-614.797 -614.797 -614.797] (1.000)
Step: 72649, Reward: [-521.474 -521.474 -521.474] [84.2579], Avg: [-614.791 -614.791 -614.791] (1.000)
Step: 72699, Reward: [-530.4 -530.4 -530.4] [57.5759], Avg: [-614.773 -614.773 -614.773] (1.000)
Step: 72749, Reward: [-589.287 -589.287 -589.287] [145.3232], Avg: [-614.855 -614.855 -614.855] (1.000)
Step: 72799, Reward: [-568.41 -568.41 -568.41] [104.3821], Avg: [-614.895 -614.895 -614.895] (1.000)
Step: 72849, Reward: [-642.522 -642.522 -642.522] [123.3546], Avg: [-614.998 -614.998 -614.998] (1.000)
Step: 72899, Reward: [-486.943 -486.943 -486.943] [78.6897], Avg: [-614.965 -614.965 -614.965] (1.000)
Step: 72949, Reward: [-588.902 -588.902 -588.902] [48.9532], Avg: [-614.98 -614.98 -614.98] (1.000)
Step: 72999, Reward: [-541.953 -541.953 -541.953] [123.3170], Avg: [-615.015 -615.015 -615.015] (1.000)
Step: 73049, Reward: [-642.621 -642.621 -642.621] [52.3547], Avg: [-615.069 -615.069 -615.069] (1.000)
Step: 73099, Reward: [-576.106 -576.106 -576.106] [52.4748], Avg: [-615.079 -615.079 -615.079] (1.000)
Step: 73149, Reward: [-614.57 -614.57 -614.57] [123.4731], Avg: [-615.163 -615.163 -615.163] (1.000)
Step: 73199, Reward: [-526.777 -526.777 -526.777] [111.9127], Avg: [-615.179 -615.179 -615.179] (1.000)
Step: 73249, Reward: [-580.388 -580.388 -580.388] [169.9734], Avg: [-615.271 -615.271 -615.271] (1.000)
Step: 73299, Reward: [-585.199 -585.199 -585.199] [92.1277], Avg: [-615.313 -615.313 -615.313] (1.000)
Step: 73349, Reward: [-498.287 -498.287 -498.287] [144.6870], Avg: [-615.332 -615.332 -615.332] (1.000)
Step: 73399, Reward: [-546.136 -546.136 -546.136] [81.3138], Avg: [-615.341 -615.341 -615.341] (1.000)
Step: 73449, Reward: [-515.744 -515.744 -515.744] [57.5533], Avg: [-615.312 -615.312 -615.312] (1.000)
Step: 73499, Reward: [-520.587 -520.587 -520.587] [95.6376], Avg: [-615.313 -615.313 -615.313] (1.000)
Step: 73549, Reward: [-469.396 -469.396 -469.396] [132.4196], Avg: [-615.303 -615.303 -615.303] (1.000)
Step: 73599, Reward: [-524.783 -524.783 -524.783] [72.8479], Avg: [-615.291 -615.291 -615.291] (1.000)
Step: 73649, Reward: [-514.711 -514.711 -514.711] [50.2606], Avg: [-615.257 -615.257 -615.257] (1.000)
Step: 73699, Reward: [-658.175 -658.175 -658.175] [101.4442], Avg: [-615.355 -615.355 -615.355] (1.000)
Step: 73749, Reward: [-666.321 -666.321 -666.321] [206.4897], Avg: [-615.53 -615.53 -615.53] (1.000)
Step: 73799, Reward: [-474.397 -474.397 -474.397] [41.3425], Avg: [-615.462 -615.462 -615.462] (1.000)
Step: 73849, Reward: [-505.597 -505.597 -505.597] [58.5309], Avg: [-615.427 -615.427 -615.427] (1.000)
Step: 73899, Reward: [-577.61 -577.61 -577.61] [157.8882], Avg: [-615.509 -615.509 -615.509] (1.000)
Step: 73949, Reward: [-574.686 -574.686 -574.686] [163.2094], Avg: [-615.591 -615.591 -615.591] (1.000)
Step: 73999, Reward: [-494.144 -494.144 -494.144] [81.4580], Avg: [-615.564 -615.564 -615.564] (1.000)
Step: 74049, Reward: [-506.776 -506.776 -506.776] [50.5444], Avg: [-615.525 -615.525 -615.525] (1.000)
Step: 74099, Reward: [-669.036 -669.036 -669.036] [215.1210], Avg: [-615.706 -615.706 -615.706] (1.000)
Step: 74149, Reward: [-553.624 -553.624 -553.624] [205.9356], Avg: [-615.803 -615.803 -615.803] (1.000)
Step: 74199, Reward: [-464.482 -464.482 -464.482] [72.8828], Avg: [-615.75 -615.75 -615.75] (1.000)
Step: 74249, Reward: [-522.022 -522.022 -522.022] [94.6761], Avg: [-615.751 -615.751 -615.751] (1.000)
Step: 74299, Reward: [-530.83 -530.83 -530.83] [141.8964], Avg: [-615.789 -615.789 -615.789] (1.000)
Step: 74349, Reward: [-472.441 -472.441 -472.441] [97.5085], Avg: [-615.759 -615.759 -615.759] (1.000)
Step: 74399, Reward: [-526.704 -526.704 -526.704] [159.9057], Avg: [-615.806 -615.806 -615.806] (1.000)
Step: 74449, Reward: [-489.254 -489.254 -489.254] [52.8918], Avg: [-615.757 -615.757 -615.757] (1.000)
Step: 74499, Reward: [-561.697 -561.697 -561.697] [76.7675], Avg: [-615.772 -615.772 -615.772] (1.000)
Step: 74549, Reward: [-459.792 -459.792 -459.792] [99.0019], Avg: [-615.734 -615.734 -615.734] (1.000)
Step: 74599, Reward: [-452.952 -452.952 -452.952] [88.8193], Avg: [-615.684 -615.684 -615.684] (1.000)
Step: 74649, Reward: [-467.399 -467.399 -467.399] [52.8826], Avg: [-615.62 -615.62 -615.62] (1.000)
Step: 74699, Reward: [-428.303 -428.303 -428.303] [101.1158], Avg: [-615.563 -615.563 -615.563] (1.000)
Step: 74749, Reward: [-455.216 -455.216 -455.216] [70.8236], Avg: [-615.503 -615.503 -615.503] (1.000)
Step: 74799, Reward: [-403.006 -403.006 -403.006] [72.6354], Avg: [-615.409 -615.409 -615.409] (1.000)
Step: 74849, Reward: [-504.902 -504.902 -504.902] [113.9605], Avg: [-615.411 -615.411 -615.411] (1.000)
Step: 74899, Reward: [-438.552 -438.552 -438.552] [121.1781], Avg: [-615.374 -615.374 -615.374] (1.000)
Step: 74949, Reward: [-567.703 -567.703 -567.703] [134.0410], Avg: [-615.432 -615.432 -615.432] (1.000)
Step: 74999, Reward: [-535.804 -535.804 -535.804] [99.6103], Avg: [-615.445 -615.445 -615.445] (1.000)
Step: 75049, Reward: [-426.659 -426.659 -426.659] [47.4572], Avg: [-615.351 -615.351 -615.351] (1.000)
Step: 75099, Reward: [-494.409 -494.409 -494.409] [40.0346], Avg: [-615.297 -615.297 -615.297] (1.000)
Step: 75149, Reward: [-546.386 -546.386 -546.386] [157.2793], Avg: [-615.356 -615.356 -615.356] (1.000)
Step: 75199, Reward: [-570.169 -570.169 -570.169] [142.4143], Avg: [-615.421 -615.421 -615.421] (1.000)
Step: 75249, Reward: [-538.242 -538.242 -538.242] [195.1146], Avg: [-615.499 -615.499 -615.499] (1.000)
Step: 75299, Reward: [-500.759 -500.759 -500.759] [99.0268], Avg: [-615.489 -615.489 -615.489] (1.000)
Step: 75349, Reward: [-530.289 -530.289 -530.289] [159.3785], Avg: [-615.538 -615.538 -615.538] (1.000)
Step: 75399, Reward: [-613.026 -613.026 -613.026] [104.4722], Avg: [-615.605 -615.605 -615.605] (1.000)
Step: 75449, Reward: [-437.663 -437.663 -437.663] [112.0273], Avg: [-615.562 -615.562 -615.562] (1.000)
Step: 75499, Reward: [-457.871 -457.871 -457.871] [64.4975], Avg: [-615.5 -615.5 -615.5] (1.000)
Step: 75549, Reward: [-550.909 -550.909 -550.909] [68.9523], Avg: [-615.503 -615.503 -615.503] (1.000)
Step: 75599, Reward: [-520.804 -520.804 -520.804] [127.7325], Avg: [-615.525 -615.525 -615.525] (1.000)
Step: 75649, Reward: [-673.458 -673.458 -673.458] [226.2294], Avg: [-615.713 -615.713 -615.713] (1.000)
Step: 75699, Reward: [-493.957 -493.957 -493.957] [123.4678], Avg: [-615.714 -615.714 -615.714] (1.000)
Step: 75749, Reward: [-420.37 -420.37 -420.37] [48.5942], Avg: [-615.617 -615.617 -615.617] (1.000)
Step: 75799, Reward: [-426.697 -426.697 -426.697] [68.4490], Avg: [-615.537 -615.537 -615.537] (1.000)
Step: 75849, Reward: [-457.461 -457.461 -457.461] [66.0545], Avg: [-615.477 -615.477 -615.477] (1.000)
Step: 75899, Reward: [-493.579 -493.579 -493.579] [36.7306], Avg: [-615.421 -615.421 -615.421] (1.000)
Step: 75949, Reward: [-447.139 -447.139 -447.139] [79.8382], Avg: [-615.362 -615.362 -615.362] (1.000)
Step: 75999, Reward: [-438.112 -438.112 -438.112] [77.3423], Avg: [-615.297 -615.297 -615.297] (1.000)
Step: 76049, Reward: [-487.328 -487.328 -487.328] [40.2785], Avg: [-615.239 -615.239 -615.239] (1.000)
Step: 76099, Reward: [-406.865 -406.865 -406.865] [57.4124], Avg: [-615.14 -615.14 -615.14] (1.000)
Step: 76149, Reward: [-447.542 -447.542 -447.542] [63.2843], Avg: [-615.071 -615.071 -615.071] (1.000)
Step: 76199, Reward: [-438.367 -438.367 -438.367] [51.1560], Avg: [-614.989 -614.989 -614.989] (1.000)
Step: 76249, Reward: [-486.43 -486.43 -486.43] [69.4813], Avg: [-614.95 -614.95 -614.95] (1.000)
Step: 76299, Reward: [-580.116 -580.116 -580.116] [122.8483], Avg: [-615.008 -615.008 -615.008] (1.000)
Step: 76349, Reward: [-556.894 -556.894 -556.894] [65.3110], Avg: [-615.013 -615.013 -615.013] (1.000)
Step: 76399, Reward: [-720.413 -720.413 -720.413] [92.6753], Avg: [-615.142 -615.142 -615.142] (1.000)
Step: 76449, Reward: [-526.841 -526.841 -526.841] [124.4768], Avg: [-615.166 -615.166 -615.166] (1.000)
Step: 76499, Reward: [-484.655 -484.655 -484.655] [67.1210], Avg: [-615.124 -615.124 -615.124] (1.000)
Step: 76549, Reward: [-514.442 -514.442 -514.442] [68.3718], Avg: [-615.103 -615.103 -615.103] (1.000)
Step: 76599, Reward: [-452.579 -452.579 -452.579] [27.3695], Avg: [-615.015 -615.015 -615.015] (1.000)
Step: 76649, Reward: [-449.039 -449.039 -449.039] [91.6228], Avg: [-614.967 -614.967 -614.967] (1.000)
Step: 76699, Reward: [-419.274 -419.274 -419.274] [59.6443], Avg: [-614.878 -614.878 -614.878] (1.000)
Step: 76749, Reward: [-388.773 -388.773 -388.773] [109.8332], Avg: [-614.802 -614.802 -614.802] (1.000)
Step: 76799, Reward: [-507.96 -507.96 -507.96] [112.4480], Avg: [-614.806 -614.806 -614.806] (1.000)
Step: 76849, Reward: [-625.205 -625.205 -625.205] [143.0694], Avg: [-614.906 -614.906 -614.906] (1.000)
Step: 76899, Reward: [-504.038 -504.038 -504.038] [70.6592], Avg: [-614.88 -614.88 -614.88] (1.000)
Step: 76949, Reward: [-518.075 -518.075 -518.075] [129.6811], Avg: [-614.901 -614.901 -614.901] (1.000)
Step: 76999, Reward: [-512.304 -512.304 -512.304] [89.8775], Avg: [-614.893 -614.893 -614.893] (1.000)
Step: 77049, Reward: [-499.314 -499.314 -499.314] [66.0806], Avg: [-614.861 -614.861 -614.861] (1.000)
Step: 77099, Reward: [-563.259 -563.259 -563.259] [84.1124], Avg: [-614.882 -614.882 -614.882] (1.000)
Step: 77149, Reward: [-585.107 -585.107 -585.107] [106.7199], Avg: [-614.931 -614.931 -614.931] (1.000)
Step: 77199, Reward: [-548.029 -548.029 -548.029] [79.7472], Avg: [-614.94 -614.94 -614.94] (1.000)
Step: 77249, Reward: [-535.175 -535.175 -535.175] [75.6074], Avg: [-614.937 -614.937 -614.937] (1.000)
Step: 77299, Reward: [-474.339 -474.339 -474.339] [83.0900], Avg: [-614.9 -614.9 -614.9] (1.000)
Step: 77349, Reward: [-527.483 -527.483 -527.483] [160.2401], Avg: [-614.947 -614.947 -614.947] (1.000)
Step: 77399, Reward: [-518.233 -518.233 -518.233] [85.0526], Avg: [-614.939 -614.939 -614.939] (1.000)
Step: 77449, Reward: [-517.298 -517.298 -517.298] [111.9626], Avg: [-614.949 -614.949 -614.949] (1.000)
Step: 77499, Reward: [-572.562 -572.562 -572.562] [44.7166], Avg: [-614.95 -614.95 -614.95] (1.000)
Step: 77549, Reward: [-536.083 -536.083 -536.083] [99.1982], Avg: [-614.963 -614.963 -614.963] (1.000)
Step: 77599, Reward: [-539.16 -539.16 -539.16] [47.2721], Avg: [-614.945 -614.945 -614.945] (1.000)
Step: 77649, Reward: [-499.925 -499.925 -499.925] [87.3460], Avg: [-614.927 -614.927 -614.927] (1.000)
Step: 77699, Reward: [-504.551 -504.551 -504.551] [61.3630], Avg: [-614.896 -614.896 -614.896] (1.000)
Step: 77749, Reward: [-506.746 -506.746 -506.746] [77.1974], Avg: [-614.876 -614.876 -614.876] (1.000)
Step: 77799, Reward: [-554.614 -554.614 -554.614] [88.8694], Avg: [-614.894 -614.894 -614.894] (1.000)
Step: 77849, Reward: [-655.189 -655.189 -655.189] [184.6521], Avg: [-615.039 -615.039 -615.039] (1.000)
Step: 77899, Reward: [-544.262 -544.262 -544.262] [217.2081], Avg: [-615.133 -615.133 -615.133] (1.000)
Step: 77949, Reward: [-459.046 -459.046 -459.046] [71.8172], Avg: [-615.078 -615.078 -615.078] (1.000)
Step: 77999, Reward: [-547.25 -547.25 -547.25] [122.5009], Avg: [-615.114 -615.114 -615.114] (1.000)
Step: 78049, Reward: [-549.067 -549.067 -549.067] [50.7195], Avg: [-615.104 -615.104 -615.104] (1.000)
Step: 78099, Reward: [-711.43 -711.43 -711.43] [85.0150], Avg: [-615.22 -615.22 -615.22] (1.000)
Step: 78149, Reward: [-610.326 -610.326 -610.326] [153.5843], Avg: [-615.315 -615.315 -615.315] (1.000)
Step: 78199, Reward: [-524.916 -524.916 -524.916] [129.9035], Avg: [-615.34 -615.34 -615.34] (1.000)
Step: 78249, Reward: [-544.027 -544.027 -544.027] [59.2416], Avg: [-615.332 -615.332 -615.332] (1.000)
Step: 78299, Reward: [-597.059 -597.059 -597.059] [155.9742], Avg: [-615.42 -615.42 -615.42] (1.000)
Step: 78349, Reward: [-497.503 -497.503 -497.503] [48.7572], Avg: [-615.376 -615.376 -615.376] (1.000)
Step: 78399, Reward: [-582.078 -582.078 -582.078] [117.0110], Avg: [-615.43 -615.43 -615.43] (1.000)
Step: 78449, Reward: [-568.893 -568.893 -568.893] [116.8387], Avg: [-615.474 -615.474 -615.474] (1.000)
Step: 78499, Reward: [-576.001 -576.001 -576.001] [71.8789], Avg: [-615.495 -615.495 -615.495] (1.000)
Step: 78549, Reward: [-605.315 -605.315 -605.315] [141.8573], Avg: [-615.579 -615.579 -615.579] (1.000)
Step: 78599, Reward: [-527.334 -527.334 -527.334] [126.2489], Avg: [-615.603 -615.603 -615.603] (1.000)
Step: 78649, Reward: [-593.588 -593.588 -593.588] [92.2888], Avg: [-615.648 -615.648 -615.648] (1.000)
Step: 78699, Reward: [-467.493 -467.493 -467.493] [58.4266], Avg: [-615.591 -615.591 -615.591] (1.000)
Step: 78749, Reward: [-645.463 -645.463 -645.463] [79.1796], Avg: [-615.66 -615.66 -615.66] (1.000)
Step: 78799, Reward: [-490.264 -490.264 -490.264] [42.3596], Avg: [-615.607 -615.607 -615.607] (1.000)
Step: 78849, Reward: [-576.887 -576.887 -576.887] [73.1571], Avg: [-615.629 -615.629 -615.629] (1.000)
Step: 78899, Reward: [-555.734 -555.734 -555.734] [45.6770], Avg: [-615.62 -615.62 -615.62] (1.000)
Step: 78949, Reward: [-562.798 -562.798 -562.798] [101.6648], Avg: [-615.651 -615.651 -615.651] (1.000)
Step: 78999, Reward: [-583.426 -583.426 -583.426] [102.6451], Avg: [-615.696 -615.696 -615.696] (1.000)
Step: 79049, Reward: [-552.237 -552.237 -552.237] [131.7457], Avg: [-615.739 -615.739 -615.739] (1.000)
Step: 79099, Reward: [-480.362 -480.362 -480.362] [95.4498], Avg: [-615.714 -615.714 -615.714] (1.000)
Step: 79149, Reward: [-504.305 -504.305 -504.305] [73.7564], Avg: [-615.69 -615.69 -615.69] (1.000)
Step: 79199, Reward: [-573.01 -573.01 -573.01] [88.5916], Avg: [-615.719 -615.719 -615.719] (1.000)
Step: 79249, Reward: [-571.966 -571.966 -571.966] [213.1442], Avg: [-615.826 -615.826 -615.826] (1.000)
Step: 79299, Reward: [-483.026 -483.026 -483.026] [61.9835], Avg: [-615.781 -615.781 -615.781] (1.000)
Step: 79349, Reward: [-545.397 -545.397 -545.397] [77.0053], Avg: [-615.785 -615.785 -615.785] (1.000)
Step: 79399, Reward: [-506.969 -506.969 -506.969] [93.9351], Avg: [-615.776 -615.776 -615.776] (1.000)
Step: 79449, Reward: [-556.515 -556.515 -556.515] [119.0620], Avg: [-615.813 -615.813 -615.813] (1.000)
Step: 79499, Reward: [-527.43 -527.43 -527.43] [65.4503], Avg: [-615.799 -615.799 -615.799] (1.000)
Step: 79549, Reward: [-572.641 -572.641 -572.641] [121.4606], Avg: [-615.848 -615.848 -615.848] (1.000)
Step: 79599, Reward: [-470.682 -470.682 -470.682] [73.5043], Avg: [-615.803 -615.803 -615.803] (1.000)
Step: 79649, Reward: [-495.519 -495.519 -495.519] [56.6786], Avg: [-615.763 -615.763 -615.763] (1.000)
Step: 79699, Reward: [-478.535 -478.535 -478.535] [90.2142], Avg: [-615.734 -615.734 -615.734] (1.000)
Step: 79749, Reward: [-522.541 -522.541 -522.541] [97.3642], Avg: [-615.736 -615.736 -615.736] (1.000)
Step: 79799, Reward: [-547.172 -547.172 -547.172] [113.9450], Avg: [-615.765 -615.765 -615.765] (1.000)
Step: 79849, Reward: [-639.275 -639.275 -639.275] [114.8851], Avg: [-615.852 -615.852 -615.852] (1.000)
Step: 79899, Reward: [-556.295 -556.295 -556.295] [91.0501], Avg: [-615.871 -615.871 -615.871] (1.000)
Step: 79949, Reward: [-521.428 -521.428 -521.428] [91.8451], Avg: [-615.87 -615.87 -615.87] (1.000)
Step: 79999, Reward: [-608.243 -608.243 -608.243] [79.7361], Avg: [-615.915 -615.915 -615.915] (1.000)
Step: 80049, Reward: [-491.534 -491.534 -491.534] [70.2280], Avg: [-615.881 -615.881 -615.881] (1.000)
Step: 80099, Reward: [-670.979 -670.979 -670.979] [111.8222], Avg: [-615.985 -615.985 -615.985] (1.000)
Step: 80149, Reward: [-620.416 -620.416 -620.416] [79.4368], Avg: [-616.037 -616.037 -616.037] (1.000)
Step: 80199, Reward: [-513.527 -513.527 -513.527] [91.9311], Avg: [-616.031 -616.031 -616.031] (1.000)
Step: 80249, Reward: [-548.33 -548.33 -548.33] [74.0644], Avg: [-616.035 -616.035 -616.035] (1.000)
Step: 80299, Reward: [-555.078 -555.078 -555.078] [76.0353], Avg: [-616.044 -616.044 -616.044] (1.000)
Step: 80349, Reward: [-546.945 -546.945 -546.945] [74.3969], Avg: [-616.047 -616.047 -616.047] (1.000)
Step: 80399, Reward: [-599.658 -599.658 -599.658] [100.7859], Avg: [-616.1 -616.1 -616.1] (1.000)
Step: 80449, Reward: [-569.88 -569.88 -569.88] [116.6624], Avg: [-616.144 -616.144 -616.144] (1.000)
Step: 80499, Reward: [-663.584 -663.584 -663.584] [155.3311], Avg: [-616.27 -616.27 -616.27] (1.000)
Step: 80549, Reward: [-449.054 -449.054 -449.054] [48.8428], Avg: [-616.196 -616.196 -616.196] (1.000)
Step: 80599, Reward: [-601.214 -601.214 -601.214] [86.9301], Avg: [-616.241 -616.241 -616.241] (1.000)
Step: 80649, Reward: [-579.905 -579.905 -579.905] [72.7829], Avg: [-616.263 -616.263 -616.263] (1.000)
Step: 80699, Reward: [-604.867 -604.867 -604.867] [119.0711], Avg: [-616.33 -616.33 -616.33] (1.000)
Step: 80749, Reward: [-569.363 -569.363 -569.363] [149.9140], Avg: [-616.394 -616.394 -616.394] (1.000)
Step: 80799, Reward: [-632.192 -632.192 -632.192] [174.9823], Avg: [-616.512 -616.512 -616.512] (1.000)
Step: 80849, Reward: [-561.125 -561.125 -561.125] [176.1714], Avg: [-616.587 -616.587 -616.587] (1.000)
Step: 80899, Reward: [-496.473 -496.473 -496.473] [101.9338], Avg: [-616.575 -616.575 -616.575] (1.000)
Step: 80949, Reward: [-542.921 -542.921 -542.921] [76.6160], Avg: [-616.577 -616.577 -616.577] (1.000)
Step: 80999, Reward: [-651.93 -651.93 -651.93] [187.3373], Avg: [-616.715 -616.715 -616.715] (1.000)
Step: 81049, Reward: [-510.517 -510.517 -510.517] [83.3802], Avg: [-616.701 -616.701 -616.701] (1.000)
Step: 81099, Reward: [-590.259 -590.259 -590.259] [72.2543], Avg: [-616.729 -616.729 -616.729] (1.000)
Step: 81149, Reward: [-581.941 -581.941 -581.941] [118.8608], Avg: [-616.781 -616.781 -616.781] (1.000)
Step: 81199, Reward: [-638.375 -638.375 -638.375] [97.4591], Avg: [-616.854 -616.854 -616.854] (1.000)
Step: 81249, Reward: [-539.174 -539.174 -539.174] [148.7433], Avg: [-616.898 -616.898 -616.898] (1.000)
Step: 81299, Reward: [-508.031 -508.031 -508.031] [75.1809], Avg: [-616.877 -616.877 -616.877] (1.000)
Step: 81349, Reward: [-464.691 -464.691 -464.691] [42.6319], Avg: [-616.81 -616.81 -616.81] (1.000)
Step: 81399, Reward: [-544.733 -544.733 -544.733] [83.3247], Avg: [-616.817 -616.817 -616.817] (1.000)
Step: 81449, Reward: [-528.828 -528.828 -528.828] [63.4659], Avg: [-616.801 -616.801 -616.801] (1.000)
Step: 81499, Reward: [-569.558 -569.558 -569.558] [57.9470], Avg: [-616.808 -616.808 -616.808] (1.000)
Step: 81549, Reward: [-508.826 -508.826 -508.826] [124.6799], Avg: [-616.818 -616.818 -616.818] (1.000)
Step: 81599, Reward: [-487.182 -487.182 -487.182] [54.2671], Avg: [-616.772 -616.772 -616.772] (1.000)
Step: 81649, Reward: [-456.807 -456.807 -456.807] [86.2344], Avg: [-616.727 -616.727 -616.727] (1.000)
Step: 81699, Reward: [-628.739 -628.739 -628.739] [119.5891], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 81749, Reward: [-547.784 -547.784 -547.784] [92.3911], Avg: [-616.822 -616.822 -616.822] (1.000)
Step: 81799, Reward: [-484.065 -484.065 -484.065] [69.5380], Avg: [-616.783 -616.783 -616.783] (1.000)
Step: 81849, Reward: [-549.165 -549.165 -549.165] [113.2096], Avg: [-616.811 -616.811 -616.811] (1.000)
Step: 81899, Reward: [-615.876 -615.876 -615.876] [95.0890], Avg: [-616.868 -616.868 -616.868] (1.000)
Step: 81949, Reward: [-529.669 -529.669 -529.669] [56.0987], Avg: [-616.849 -616.849 -616.849] (1.000)
Step: 81999, Reward: [-497.621 -497.621 -497.621] [90.2357], Avg: [-616.832 -616.832 -616.832] (1.000)
Step: 82049, Reward: [-517.559 -517.559 -517.559] [106.1332], Avg: [-616.836 -616.836 -616.836] (1.000)
Step: 82099, Reward: [-484.627 -484.627 -484.627] [83.8781], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 82149, Reward: [-589.226 -589.226 -589.226] [97.3592], Avg: [-616.849 -616.849 -616.849] (1.000)
Step: 82199, Reward: [-528.121 -528.121 -528.121] [86.0701], Avg: [-616.847 -616.847 -616.847] (1.000)
Step: 82249, Reward: [-585.426 -585.426 -585.426] [53.3982], Avg: [-616.861 -616.861 -616.861] (1.000)
Step: 82299, Reward: [-519.876 -519.876 -519.876] [152.4018], Avg: [-616.894 -616.894 -616.894] (1.000)
Step: 82349, Reward: [-598.912 -598.912 -598.912] [141.0247], Avg: [-616.969 -616.969 -616.969] (1.000)
Step: 82399, Reward: [-563.415 -563.415 -563.415] [66.7721], Avg: [-616.977 -616.977 -616.977] (1.000)
Step: 82449, Reward: [-515.254 -515.254 -515.254] [139.4399], Avg: [-617. -617. -617.] (1.000)
Step: 82499, Reward: [-542.206 -542.206 -542.206] [83.8102], Avg: [-617.005 -617.005 -617.005] (1.000)
Step: 82549, Reward: [-555.657 -555.657 -555.657] [15.0427], Avg: [-616.977 -616.977 -616.977] (1.000)
Step: 82599, Reward: [-547.837 -547.837 -547.837] [105.5725], Avg: [-616.999 -616.999 -616.999] (1.000)
Step: 82649, Reward: [-562.309 -562.309 -562.309] [99.5065], Avg: [-617.027 -617.027 -617.027] (1.000)
Step: 82699, Reward: [-525.376 -525.376 -525.376] [33.4049], Avg: [-616.991 -616.991 -616.991] (1.000)
Step: 82749, Reward: [-523.542 -523.542 -523.542] [126.2589], Avg: [-617.011 -617.011 -617.011] (1.000)
Step: 82799, Reward: [-487.415 -487.415 -487.415] [44.9903], Avg: [-616.96 -616.96 -616.96] (1.000)
Step: 82849, Reward: [-466.886 -466.886 -466.886] [66.5253], Avg: [-616.91 -616.91 -616.91] (1.000)
Step: 82899, Reward: [-457.958 -457.958 -457.958] [122.6977], Avg: [-616.888 -616.888 -616.888] (1.000)
Step: 82949, Reward: [-498.77 -498.77 -498.77] [159.1759], Avg: [-616.913 -616.913 -616.913] (1.000)
Step: 82999, Reward: [-473.816 -473.816 -473.816] [67.7637], Avg: [-616.867 -616.867 -616.867] (1.000)
Step: 83049, Reward: [-461.547 -461.547 -461.547] [73.9506], Avg: [-616.818 -616.818 -616.818] (1.000)
Step: 83099, Reward: [-421.68 -421.68 -421.68] [74.8071], Avg: [-616.746 -616.746 -616.746] (1.000)
Step: 83149, Reward: [-437.354 -437.354 -437.354] [93.6310], Avg: [-616.694 -616.694 -616.694] (1.000)
Step: 83199, Reward: [-477.67 -477.67 -477.67] [93.2132], Avg: [-616.667 -616.667 -616.667] (1.000)
Step: 83249, Reward: [-595.777 -595.777 -595.777] [111.4090], Avg: [-616.721 -616.721 -616.721] (1.000)
Step: 83299, Reward: [-454.463 -454.463 -454.463] [60.6877], Avg: [-616.66 -616.66 -616.66] (1.000)
Step: 83349, Reward: [-428.354 -428.354 -428.354] [81.2496], Avg: [-616.596 -616.596 -616.596] (1.000)
Step: 83399, Reward: [-447.826 -447.826 -447.826] [89.8702], Avg: [-616.549 -616.549 -616.549] (1.000)
Step: 83449, Reward: [-456.369 -456.369 -456.369] [62.6807], Avg: [-616.49 -616.49 -616.49] (1.000)
Step: 83499, Reward: [-441.486 -441.486 -441.486] [62.4791], Avg: [-616.423 -616.423 -616.423] (1.000)
Step: 83549, Reward: [-456.713 -456.713 -456.713] [29.6454], Avg: [-616.345 -616.345 -616.345] (1.000)
Step: 83599, Reward: [-411.153 -411.153 -411.153] [65.8877], Avg: [-616.262 -616.262 -616.262] (1.000)
Step: 83649, Reward: [-474.12 -474.12 -474.12] [128.2858], Avg: [-616.253 -616.253 -616.253] (1.000)
Step: 83699, Reward: [-407.438 -407.438 -407.438] [62.3770], Avg: [-616.166 -616.166 -616.166] (1.000)
Step: 83749, Reward: [-594.838 -594.838 -594.838] [95.0967], Avg: [-616.21 -616.21 -616.21] (1.000)
Step: 83799, Reward: [-413.041 -413.041 -413.041] [65.7101], Avg: [-616.128 -616.128 -616.128] (1.000)
Step: 83849, Reward: [-444.292 -444.292 -444.292] [94.7458], Avg: [-616.082 -616.082 -616.082] (1.000)
Step: 83899, Reward: [-504.196 -504.196 -504.196] [98.9215], Avg: [-616.074 -616.074 -616.074] (1.000)
Step: 83949, Reward: [-568.02 -568.02 -568.02] [106.8493], Avg: [-616.109 -616.109 -616.109] (1.000)
Step: 83999, Reward: [-545.52 -545.52 -545.52] [117.2486], Avg: [-616.137 -616.137 -616.137] (1.000)
Step: 84049, Reward: [-551.214 -551.214 -551.214] [48.5768], Avg: [-616.127 -616.127 -616.127] (1.000)
Step: 84099, Reward: [-575.898 -575.898 -575.898] [125.6820], Avg: [-616.178 -616.178 -616.178] (1.000)
Step: 84149, Reward: [-509.231 -509.231 -509.231] [43.1371], Avg: [-616.14 -616.14 -616.14] (1.000)
Step: 84199, Reward: [-543.507 -543.507 -543.507] [117.6903], Avg: [-616.167 -616.167 -616.167] (1.000)
Step: 84249, Reward: [-566.845 -566.845 -566.845] [100.8292], Avg: [-616.197 -616.197 -616.197] (1.000)
Step: 84299, Reward: [-565.288 -565.288 -565.288] [66.8071], Avg: [-616.207 -616.207 -616.207] (1.000)
Step: 84349, Reward: [-550.933 -550.933 -550.933] [72.8170], Avg: [-616.211 -616.211 -616.211] (1.000)
Step: 84399, Reward: [-512.065 -512.065 -512.065] [76.8440], Avg: [-616.195 -616.195 -616.195] (1.000)
Step: 84449, Reward: [-574.13 -574.13 -574.13] [76.7703], Avg: [-616.216 -616.216 -616.216] (1.000)
Step: 84499, Reward: [-550.207 -550.207 -550.207] [128.5382], Avg: [-616.253 -616.253 -616.253] (1.000)
Step: 84549, Reward: [-573.214 -573.214 -573.214] [118.8307], Avg: [-616.298 -616.298 -616.298] (1.000)
Step: 84599, Reward: [-565.039 -565.039 -565.039] [109.5506], Avg: [-616.332 -616.332 -616.332] (1.000)
Step: 84649, Reward: [-470.091 -470.091 -470.091] [78.4723], Avg: [-616.292 -616.292 -616.292] (1.000)
Step: 84699, Reward: [-512.796 -512.796 -512.796] [92.2360], Avg: [-616.285 -616.285 -616.285] (1.000)
Step: 84749, Reward: [-593.906 -593.906 -593.906] [114.0635], Avg: [-616.339 -616.339 -616.339] (1.000)
Step: 84799, Reward: [-445.304 -445.304 -445.304] [79.2858], Avg: [-616.285 -616.285 -616.285] (1.000)
Step: 84849, Reward: [-595.03 -595.03 -595.03] [119.1459], Avg: [-616.343 -616.343 -616.343] (1.000)
Step: 84899, Reward: [-496.039 -496.039 -496.039] [142.0939], Avg: [-616.356 -616.356 -616.356] (1.000)
Step: 84949, Reward: [-517.638 -517.638 -517.638] [111.8275], Avg: [-616.364 -616.364 -616.364] (1.000)
Step: 84999, Reward: [-471.382 -471.382 -471.382] [54.6621], Avg: [-616.31 -616.31 -616.31] (1.000)
Step: 85049, Reward: [-496.836 -496.836 -496.836] [81.7699], Avg: [-616.288 -616.288 -616.288] (1.000)
Step: 85099, Reward: [-524.068 -524.068 -524.068] [77.7225], Avg: [-616.28 -616.28 -616.28] (1.000)
Step: 85149, Reward: [-503.765 -503.765 -503.765] [94.6154], Avg: [-616.269 -616.269 -616.269] (1.000)
Step: 85199, Reward: [-387.447 -387.447 -387.447] [56.8730], Avg: [-616.168 -616.168 -616.168] (1.000)
Step: 85249, Reward: [-522.971 -522.971 -522.971] [107.3490], Avg: [-616.177 -616.177 -616.177] (1.000)
Step: 85299, Reward: [-467.823 -467.823 -467.823] [55.3128], Avg: [-616.122 -616.122 -616.122] (1.000)
Step: 85349, Reward: [-411.848 -411.848 -411.848] [48.2022], Avg: [-616.031 -616.031 -616.031] (1.000)
Step: 85399, Reward: [-528.337 -528.337 -528.337] [80.6446], Avg: [-616.027 -616.027 -616.027] (1.000)
Step: 85449, Reward: [-496.438 -496.438 -496.438] [60.5838], Avg: [-615.992 -615.992 -615.992] (1.000)
Step: 85499, Reward: [-597.931 -597.931 -597.931] [127.6262], Avg: [-616.056 -616.056 -616.056] (1.000)
Step: 85549, Reward: [-596.638 -596.638 -596.638] [134.4350], Avg: [-616.123 -616.123 -616.123] (1.000)
Step: 85599, Reward: [-556.478 -556.478 -556.478] [100.2146], Avg: [-616.147 -616.147 -616.147] (1.000)
Step: 85649, Reward: [-551.483 -551.483 -551.483] [84.6204], Avg: [-616.159 -616.159 -616.159] (1.000)
Step: 85699, Reward: [-459.038 -459.038 -459.038] [50.9567], Avg: [-616.097 -616.097 -616.097] (1.000)
Step: 85749, Reward: [-504.433 -504.433 -504.433] [102.8478], Avg: [-616.092 -616.092 -616.092] (1.000)
Step: 85799, Reward: [-532.92 -532.92 -532.92] [77.7153], Avg: [-616.088 -616.088 -616.088] (1.000)
Step: 85849, Reward: [-561.189 -561.189 -561.189] [108.6636], Avg: [-616.12 -616.12 -616.12] (1.000)
Step: 85899, Reward: [-656.659 -656.659 -656.659] [133.1169], Avg: [-616.221 -616.221 -616.221] (1.000)
Step: 85949, Reward: [-569.93 -569.93 -569.93] [100.0374], Avg: [-616.252 -616.252 -616.252] (1.000)
Step: 85999, Reward: [-557.382 -557.382 -557.382] [99.0273], Avg: [-616.275 -616.275 -616.275] (1.000)
Step: 86049, Reward: [-575.418 -575.418 -575.418] [114.2615], Avg: [-616.318 -616.318 -616.318] (1.000)
Step: 86099, Reward: [-510.463 -510.463 -510.463] [97.1173], Avg: [-616.313 -616.313 -616.313] (1.000)
Step: 86149, Reward: [-463.462 -463.462 -463.462] [38.8937], Avg: [-616.247 -616.247 -616.247] (1.000)
Step: 86199, Reward: [-500.871 -500.871 -500.871] [76.4793], Avg: [-616.224 -616.224 -616.224] (1.000)
Step: 86249, Reward: [-582.273 -582.273 -582.273] [114.1332], Avg: [-616.271 -616.271 -616.271] (1.000)
Step: 86299, Reward: [-573.16 -573.16 -573.16] [88.6953], Avg: [-616.297 -616.297 -616.297] (1.000)
Step: 86349, Reward: [-557.931 -557.931 -557.931] [94.4145], Avg: [-616.318 -616.318 -616.318] (1.000)
Step: 86399, Reward: [-517.113 -517.113 -517.113] [61.6622], Avg: [-616.296 -616.296 -616.296] (1.000)
Step: 86449, Reward: [-546.732 -546.732 -546.732] [76.2653], Avg: [-616.3 -616.3 -616.3] (1.000)
Step: 86499, Reward: [-556.246 -556.246 -556.246] [102.5948], Avg: [-616.325 -616.325 -616.325] (1.000)
Step: 86549, Reward: [-604.867 -604.867 -604.867] [108.8858], Avg: [-616.381 -616.381 -616.381] (1.000)
Step: 86599, Reward: [-504.527 -504.527 -504.527] [79.8052], Avg: [-616.363 -616.363 -616.363] (1.000)
Step: 86649, Reward: [-482.914 -482.914 -482.914] [102.1358], Avg: [-616.345 -616.345 -616.345] (1.000)
Step: 86699, Reward: [-579.977 -579.977 -579.977] [125.5516], Avg: [-616.396 -616.396 -616.396] (1.000)
Step: 86749, Reward: [-522.866 -522.866 -522.866] [63.1916], Avg: [-616.378 -616.378 -616.378] (1.000)
Step: 86799, Reward: [-548.646 -548.646 -548.646] [96.9782], Avg: [-616.395 -616.395 -616.395] (1.000)
Step: 86849, Reward: [-518.584 -518.584 -518.584] [117.4385], Avg: [-616.407 -616.407 -616.407] (1.000)
Step: 86899, Reward: [-577.303 -577.303 -577.303] [137.3335], Avg: [-616.463 -616.463 -616.463] (1.000)
Step: 86949, Reward: [-617.93 -617.93 -617.93] [113.2068], Avg: [-616.529 -616.529 -616.529] (1.000)
Step: 86999, Reward: [-509.566 -509.566 -509.566] [88.0754], Avg: [-616.518 -616.518 -616.518] (1.000)
Step: 87049, Reward: [-657.6 -657.6 -657.6] [106.9724], Avg: [-616.603 -616.603 -616.603] (1.000)
Step: 87099, Reward: [-541.596 -541.596 -541.596] [108.7396], Avg: [-616.623 -616.623 -616.623] (1.000)
Step: 87149, Reward: [-521.289 -521.289 -521.289] [58.4294], Avg: [-616.601 -616.601 -616.601] (1.000)
Step: 87199, Reward: [-561.322 -561.322 -561.322] [172.8080], Avg: [-616.669 -616.669 -616.669] (1.000)
Step: 87249, Reward: [-702.478 -702.478 -702.478] [36.7088], Avg: [-616.739 -616.739 -616.739] (1.000)
Step: 87299, Reward: [-504.761 -504.761 -504.761] [83.7590], Avg: [-616.723 -616.723 -616.723] (1.000)
Step: 87349, Reward: [-553.991 -553.991 -553.991] [120.4030], Avg: [-616.756 -616.756 -616.756] (1.000)
Step: 87399, Reward: [-545.264 -545.264 -545.264] [60.8364], Avg: [-616.75 -616.75 -616.75] (1.000)
Step: 87449, Reward: [-504.967 -504.967 -504.967] [63.8011], Avg: [-616.722 -616.722 -616.722] (1.000)
Step: 87499, Reward: [-509.767 -509.767 -509.767] [96.5655], Avg: [-616.716 -616.716 -616.716] (1.000)
Step: 87549, Reward: [-609.372 -609.372 -609.372] [113.1250], Avg: [-616.777 -616.777 -616.777] (1.000)
Step: 87599, Reward: [-529.289 -529.289 -529.289] [140.2519], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 87649, Reward: [-566.298 -566.298 -566.298] [91.9881], Avg: [-616.831 -616.831 -616.831] (1.000)
Step: 87699, Reward: [-518.542 -518.542 -518.542] [79.2976], Avg: [-616.82 -616.82 -616.82] (1.000)
Step: 87749, Reward: [-617.036 -617.036 -617.036] [41.4594], Avg: [-616.844 -616.844 -616.844] (1.000)
Step: 87799, Reward: [-587.394 -587.394 -587.394] [141.7834], Avg: [-616.908 -616.908 -616.908] (1.000)
Step: 87849, Reward: [-625.41 -625.41 -625.41] [144.0757], Avg: [-616.994 -616.994 -616.994] (1.000)
Step: 87899, Reward: [-528.458 -528.458 -528.458] [77.3415], Avg: [-616.988 -616.988 -616.988] (1.000)
Step: 87949, Reward: [-595.24 -595.24 -595.24] [79.2983], Avg: [-617.021 -617.021 -617.021] (1.000)
Step: 87999, Reward: [-620.032 -620.032 -620.032] [66.5229], Avg: [-617.06 -617.06 -617.06] (1.000)
Step: 88049, Reward: [-506.021 -506.021 -506.021] [46.9267], Avg: [-617.024 -617.024 -617.024] (1.000)
Step: 88099, Reward: [-618.405 -618.405 -618.405] [187.1963], Avg: [-617.131 -617.131 -617.131] (1.000)
Step: 88149, Reward: [-535.131 -535.131 -535.131] [66.9020], Avg: [-617.122 -617.122 -617.122] (1.000)
Step: 88199, Reward: [-494.597 -494.597 -494.597] [104.1808], Avg: [-617.112 -617.112 -617.112] (1.000)
Step: 88249, Reward: [-413.356 -413.356 -413.356] [51.5469], Avg: [-617.026 -617.026 -617.026] (1.000)
Step: 88299, Reward: [-525.8 -525.8 -525.8] [106.4239], Avg: [-617.034 -617.034 -617.034] (1.000)
Step: 88349, Reward: [-439.417 -439.417 -439.417] [53.1539], Avg: [-616.964 -616.964 -616.964] (1.000)
Step: 88399, Reward: [-501.787 -501.787 -501.787] [41.1133], Avg: [-616.922 -616.922 -616.922] (1.000)
Step: 88449, Reward: [-499.601 -499.601 -499.601] [85.4377], Avg: [-616.904 -616.904 -616.904] (1.000)
Step: 88499, Reward: [-512.977 -512.977 -512.977] [50.4661], Avg: [-616.874 -616.874 -616.874] (1.000)
Step: 88549, Reward: [-488.971 -488.971 -488.971] [47.1150], Avg: [-616.828 -616.828 -616.828] (1.000)
Step: 88599, Reward: [-518.814 -518.814 -518.814] [93.0216], Avg: [-616.825 -616.825 -616.825] (1.000)
Step: 88649, Reward: [-436.709 -436.709 -436.709] [33.8368], Avg: [-616.743 -616.743 -616.743] (1.000)
Step: 88699, Reward: [-433.559 -433.559 -433.559] [53.2467], Avg: [-616.669 -616.669 -616.669] (1.000)
Step: 88749, Reward: [-436.882 -436.882 -436.882] [72.1793], Avg: [-616.609 -616.609 -616.609] (1.000)
Step: 88799, Reward: [-453.987 -453.987 -453.987] [97.1114], Avg: [-616.572 -616.572 -616.572] (1.000)
Step: 88849, Reward: [-460.008 -460.008 -460.008] [59.4922], Avg: [-616.517 -616.517 -616.517] (1.000)
Step: 88899, Reward: [-435.671 -435.671 -435.671] [52.3680], Avg: [-616.445 -616.445 -616.445] (1.000)
Step: 88949, Reward: [-444.799 -444.799 -444.799] [84.7080], Avg: [-616.396 -616.396 -616.396] (1.000)
Step: 88999, Reward: [-461.779 -461.779 -461.779] [50.5725], Avg: [-616.338 -616.338 -616.338] (1.000)
Step: 89049, Reward: [-466.966 -466.966 -466.966] [80.9201], Avg: [-616.299 -616.299 -616.299] (1.000)
Step: 89099, Reward: [-497.481 -497.481 -497.481] [57.8141], Avg: [-616.265 -616.265 -616.265] (1.000)
Step: 89149, Reward: [-485.027 -485.027 -485.027] [68.3308], Avg: [-616.23 -616.23 -616.23] (1.000)
Step: 89199, Reward: [-627.19 -627.19 -627.19] [139.2187], Avg: [-616.314 -616.314 -616.314] (1.000)
Step: 89249, Reward: [-594.519 -594.519 -594.519] [174.9297], Avg: [-616.4 -616.4 -616.4] (1.000)
Step: 89299, Reward: [-514.882 -514.882 -514.882] [93.8099], Avg: [-616.395 -616.395 -616.395] (1.000)
Step: 89349, Reward: [-526.919 -526.919 -526.919] [96.7671], Avg: [-616.4 -616.4 -616.4] (1.000)
Step: 89399, Reward: [-469.483 -469.483 -469.483] [77.0405], Avg: [-616.36 -616.36 -616.36] (1.000)
Step: 89449, Reward: [-542.077 -542.077 -542.077] [91.3497], Avg: [-616.37 -616.37 -616.37] (1.000)
Step: 89499, Reward: [-537.061 -537.061 -537.061] [91.5872], Avg: [-616.377 -616.377 -616.377] (1.000)
Step: 89549, Reward: [-584.725 -584.725 -584.725] [154.2952], Avg: [-616.445 -616.445 -616.445] (1.000)
Step: 89599, Reward: [-512.204 -512.204 -512.204] [80.0034], Avg: [-616.432 -616.432 -616.432] (1.000)
Step: 89649, Reward: [-425.362 -425.362 -425.362] [34.2998], Avg: [-616.344 -616.344 -616.344] (1.000)
Step: 89699, Reward: [-517.275 -517.275 -517.275] [69.9748], Avg: [-616.328 -616.328 -616.328] (1.000)
Step: 89749, Reward: [-532.764 -532.764 -532.764] [120.8131], Avg: [-616.349 -616.349 -616.349] (1.000)
Step: 89799, Reward: [-556.607 -556.607 -556.607] [146.9573], Avg: [-616.397 -616.397 -616.397] (1.000)
Step: 89849, Reward: [-635.806 -635.806 -635.806] [112.4691], Avg: [-616.471 -616.471 -616.471] (1.000)
Step: 89899, Reward: [-570.656 -570.656 -570.656] [85.3964], Avg: [-616.493 -616.493 -616.493] (1.000)
Step: 89949, Reward: [-577.592 -577.592 -577.592] [196.3175], Avg: [-616.58 -616.58 -616.58] (1.000)
Step: 89999, Reward: [-529.575 -529.575 -529.575] [123.8956], Avg: [-616.601 -616.601 -616.601] (1.000)
Step: 90049, Reward: [-652.203 -652.203 -652.203] [101.6514], Avg: [-616.677 -616.677 -616.677] (1.000)
Step: 90099, Reward: [-487.038 -487.038 -487.038] [46.6525], Avg: [-616.631 -616.631 -616.631] (1.000)
Step: 90149, Reward: [-487.557 -487.557 -487.557] [105.4696], Avg: [-616.618 -616.618 -616.618] (1.000)
Step: 90199, Reward: [-627.031 -627.031 -627.031] [90.5858], Avg: [-616.674 -616.674 -616.674] (1.000)
Step: 90249, Reward: [-523.824 -523.824 -523.824] [131.9255], Avg: [-616.696 -616.696 -616.696] (1.000)
Step: 90299, Reward: [-572.696 -572.696 -572.696] [87.1699], Avg: [-616.719 -616.719 -616.719] (1.000)
Step: 90349, Reward: [-557.19 -557.19 -557.19] [103.5067], Avg: [-616.744 -616.744 -616.744] (1.000)
Step: 90399, Reward: [-524.449 -524.449 -524.449] [96.1987], Avg: [-616.746 -616.746 -616.746] (1.000)
Step: 90449, Reward: [-616.257 -616.257 -616.257] [110.0678], Avg: [-616.807 -616.807 -616.807] (1.000)
Step: 90499, Reward: [-492.574 -492.574 -492.574] [111.2407], Avg: [-616.799 -616.799 -616.799] (1.000)
Step: 90549, Reward: [-587.433 -587.433 -587.433] [151.4481], Avg: [-616.867 -616.867 -616.867] (1.000)
Step: 90599, Reward: [-569.333 -569.333 -569.333] [61.9947], Avg: [-616.875 -616.875 -616.875] (1.000)
Step: 90649, Reward: [-601.124 -601.124 -601.124] [124.8788], Avg: [-616.935 -616.935 -616.935] (1.000)
Step: 90699, Reward: [-541.444 -541.444 -541.444] [147.0489], Avg: [-616.974 -616.974 -616.974] (1.000)
Step: 90749, Reward: [-574.816 -574.816 -574.816] [98.6840], Avg: [-617.006 -617.006 -617.006] (1.000)
Step: 90799, Reward: [-569.806 -569.806 -569.806] [118.5464], Avg: [-617.045 -617.045 -617.045] (1.000)
Step: 90849, Reward: [-514.963 -514.963 -514.963] [55.2643], Avg: [-617.019 -617.019 -617.019] (1.000)
Step: 90899, Reward: [-571.388 -571.388 -571.388] [53.6855], Avg: [-617.024 -617.024 -617.024] (1.000)
Step: 90949, Reward: [-487.401 -487.401 -487.401] [61.7569], Avg: [-616.986 -616.986 -616.986] (1.000)
Step: 90999, Reward: [-547.219 -547.219 -547.219] [69.0412], Avg: [-616.986 -616.986 -616.986] (1.000)
Step: 91049, Reward: [-497.524 -497.524 -497.524] [57.5859], Avg: [-616.952 -616.952 -616.952] (1.000)
Step: 91099, Reward: [-517.459 -517.459 -517.459] [180.4566], Avg: [-616.996 -616.996 -616.996] (1.000)
Step: 91149, Reward: [-598.178 -598.178 -598.178] [50.7054], Avg: [-617.014 -617.014 -617.014] (1.000)
Step: 91199, Reward: [-560.99 -560.99 -560.99] [126.3859], Avg: [-617.052 -617.052 -617.052] (1.000)
Step: 91249, Reward: [-562.566 -562.566 -562.566] [104.1914], Avg: [-617.08 -617.08 -617.08] (1.000)
Step: 91299, Reward: [-552.361 -552.361 -552.361] [96.8079], Avg: [-617.097 -617.097 -617.097] (1.000)
Step: 91349, Reward: [-513.813 -513.813 -513.813] [70.4763], Avg: [-617.079 -617.079 -617.079] (1.000)
Step: 91399, Reward: [-581.621 -581.621 -581.621] [96.8934], Avg: [-617.113 -617.113 -617.113] (1.000)
Step: 91449, Reward: [-541.312 -541.312 -541.312] [83.7818], Avg: [-617.117 -617.117 -617.117] (1.000)
Step: 91499, Reward: [-511.894 -511.894 -511.894] [95.5000], Avg: [-617.112 -617.112 -617.112] (1.000)
Step: 91549, Reward: [-568.643 -568.643 -568.643] [78.2194], Avg: [-617.128 -617.128 -617.128] (1.000)
Step: 91599, Reward: [-596.915 -596.915 -596.915] [51.3434], Avg: [-617.145 -617.145 -617.145] (1.000)
Step: 91649, Reward: [-575.686 -575.686 -575.686] [174.1491], Avg: [-617.217 -617.217 -617.217] (1.000)
Step: 91699, Reward: [-455.747 -455.747 -455.747] [82.7733], Avg: [-617.175 -617.175 -617.175] (1.000)
Step: 91749, Reward: [-606.828 -606.828 -606.828] [116.8559], Avg: [-617.233 -617.233 -617.233] (1.000)
Step: 91799, Reward: [-508.321 -508.321 -508.321] [29.8880], Avg: [-617.19 -617.19 -617.19] (1.000)
Step: 91849, Reward: [-492.318 -492.318 -492.318] [97.4151], Avg: [-617.175 -617.175 -617.175] (1.000)
Step: 91899, Reward: [-489.483 -489.483 -489.483] [26.1203], Avg: [-617.119 -617.119 -617.119] (1.000)
Step: 91949, Reward: [-572.357 -572.357 -572.357] [131.5842], Avg: [-617.167 -617.167 -617.167] (1.000)
Step: 91999, Reward: [-539.177 -539.177 -539.177] [25.3597], Avg: [-617.138 -617.138 -617.138] (1.000)
Step: 92049, Reward: [-613.752 -613.752 -613.752] [145.0022], Avg: [-617.215 -617.215 -617.215] (1.000)
Step: 92099, Reward: [-494.819 -494.819 -494.819] [165.9080], Avg: [-617.239 -617.239 -617.239] (1.000)
Step: 92149, Reward: [-501.302 -501.302 -501.302] [72.8129], Avg: [-617.215 -617.215 -617.215] (1.000)
Step: 92199, Reward: [-553.908 -553.908 -553.908] [90.4168], Avg: [-617.23 -617.23 -617.23] (1.000)
Step: 92249, Reward: [-508.403 -508.403 -508.403] [35.2046], Avg: [-617.19 -617.19 -617.19] (1.000)
Step: 92299, Reward: [-623.344 -623.344 -623.344] [215.7483], Avg: [-617.31 -617.31 -617.31] (1.000)
Step: 92349, Reward: [-722.764 -722.764 -722.764] [83.0254], Avg: [-617.412 -617.412 -617.412] (1.000)
Step: 92399, Reward: [-643.856 -643.856 -643.856] [108.8669], Avg: [-617.485 -617.485 -617.485] (1.000)
Step: 92449, Reward: [-594.905 -594.905 -594.905] [108.8218], Avg: [-617.532 -617.532 -617.532] (1.000)
Step: 92499, Reward: [-611.532 -611.532 -611.532] [51.1052], Avg: [-617.556 -617.556 -617.556] (1.000)
Step: 92549, Reward: [-455.795 -455.795 -455.795] [91.5625], Avg: [-617.518 -617.518 -617.518] (1.000)
Step: 92599, Reward: [-531.35 -531.35 -531.35] [67.7421], Avg: [-617.509 -617.509 -617.509] (1.000)
Step: 92649, Reward: [-606.462 -606.462 -606.462] [115.3537], Avg: [-617.565 -617.565 -617.565] (1.000)
Step: 92699, Reward: [-516.657 -516.657 -516.657] [87.0836], Avg: [-617.557 -617.557 -617.557] (1.000)
Step: 92749, Reward: [-686.89 -686.89 -686.89] [131.6959], Avg: [-617.666 -617.666 -617.666] (1.000)
Step: 92799, Reward: [-478.844 -478.844 -478.844] [67.8705], Avg: [-617.628 -617.628 -617.628] (1.000)
Step: 92849, Reward: [-529.124 -529.124 -529.124] [47.5675], Avg: [-617.605 -617.605 -617.605] (1.000)
Step: 92899, Reward: [-617.025 -617.025 -617.025] [105.3840], Avg: [-617.662 -617.662 -617.662] (1.000)
Step: 92949, Reward: [-513.158 -513.158 -513.158] [133.5476], Avg: [-617.677 -617.677 -617.677] (1.000)
Step: 92999, Reward: [-512.292 -512.292 -512.292] [45.8915], Avg: [-617.646 -617.646 -617.646] (1.000)
Step: 93049, Reward: [-458.618 -458.618 -458.618] [75.9731], Avg: [-617.601 -617.601 -617.601] (1.000)
Step: 93099, Reward: [-537.04 -537.04 -537.04] [116.4009], Avg: [-617.62 -617.62 -617.62] (1.000)
Step: 93149, Reward: [-579.937 -579.937 -579.937] [97.0104], Avg: [-617.652 -617.652 -617.652] (1.000)
Step: 93199, Reward: [-517.445 -517.445 -517.445] [76.8301], Avg: [-617.639 -617.639 -617.639] (1.000)
Step: 93249, Reward: [-553.736 -553.736 -553.736] [98.8651], Avg: [-617.658 -617.658 -617.658] (1.000)
Step: 93299, Reward: [-476.88 -476.88 -476.88] [66.5863], Avg: [-617.618 -617.618 -617.618] (1.000)
Step: 93349, Reward: [-656.287 -656.287 -656.287] [118.8029], Avg: [-617.703 -617.703 -617.703] (1.000)
Step: 93399, Reward: [-622.776 -622.776 -622.776] [102.1189], Avg: [-617.76 -617.76 -617.76] (1.000)
Step: 93449, Reward: [-581.1 -581.1 -581.1] [108.0739], Avg: [-617.798 -617.798 -617.798] (1.000)
Step: 93499, Reward: [-456.578 -456.578 -456.578] [61.5009], Avg: [-617.745 -617.745 -617.745] (1.000)
Step: 93549, Reward: [-670.229 -670.229 -670.229] [111.9357], Avg: [-617.833 -617.833 -617.833] (1.000)
Step: 93599, Reward: [-594.15 -594.15 -594.15] [148.1302], Avg: [-617.899 -617.899 -617.899] (1.000)
Step: 93649, Reward: [-501.724 -501.724 -501.724] [61.3687], Avg: [-617.87 -617.87 -617.87] (1.000)
Step: 93699, Reward: [-526.805 -526.805 -526.805] [97.9955], Avg: [-617.874 -617.874 -617.874] (1.000)
Step: 93749, Reward: [-536.19 -536.19 -536.19] [74.6003], Avg: [-617.87 -617.87 -617.87] (1.000)
Step: 93799, Reward: [-572.193 -572.193 -572.193] [87.3810], Avg: [-617.892 -617.892 -617.892] (1.000)
Step: 93849, Reward: [-643.012 -643.012 -643.012] [143.1906], Avg: [-617.982 -617.982 -617.982] (1.000)
Step: 93899, Reward: [-619.576 -619.576 -619.576] [72.3896], Avg: [-618.021 -618.021 -618.021] (1.000)
Step: 93949, Reward: [-596.734 -596.734 -596.734] [131.1870], Avg: [-618.08 -618.08 -618.08] (1.000)
Step: 93999, Reward: [-510.389 -510.389 -510.389] [43.8343], Avg: [-618.046 -618.046 -618.046] (1.000)
Step: 94049, Reward: [-515.625 -515.625 -515.625] [54.6728], Avg: [-618.02 -618.02 -618.02] (1.000)
Step: 94099, Reward: [-609.232 -609.232 -609.232] [126.2024], Avg: [-618.083 -618.083 -618.083] (1.000)
Step: 94149, Reward: [-593.978 -593.978 -593.978] [139.8336], Avg: [-618.144 -618.144 -618.144] (1.000)
Step: 94199, Reward: [-561.875 -561.875 -561.875] [51.6192], Avg: [-618.142 -618.142 -618.142] (1.000)
Step: 94249, Reward: [-452.009 -452.009 -452.009] [83.9835], Avg: [-618.098 -618.098 -618.098] (1.000)
Step: 94299, Reward: [-521.047 -521.047 -521.047] [52.9663], Avg: [-618.075 -618.075 -618.075] (1.000)
Step: 94349, Reward: [-591.912 -591.912 -591.912] [145.6547], Avg: [-618.138 -618.138 -618.138] (1.000)
Step: 94399, Reward: [-671.323 -671.323 -671.323] [227.5226], Avg: [-618.287 -618.287 -618.287] (1.000)
Step: 94449, Reward: [-671.183 -671.183 -671.183] [184.5623], Avg: [-618.413 -618.413 -618.413] (1.000)
Step: 94499, Reward: [-553.681 -553.681 -553.681] [86.5765], Avg: [-618.424 -618.424 -618.424] (1.000)
Step: 94549, Reward: [-558.209 -558.209 -558.209] [143.9003], Avg: [-618.468 -618.468 -618.468] (1.000)
Step: 94599, Reward: [-599.405 -599.405 -599.405] [42.3489], Avg: [-618.481 -618.481 -618.481] (1.000)
Step: 94649, Reward: [-567.411 -567.411 -567.411] [71.6417], Avg: [-618.492 -618.492 -618.492] (1.000)
Step: 94699, Reward: [-527.978 -527.978 -527.978] [75.0622], Avg: [-618.483 -618.483 -618.483] (1.000)
Step: 94749, Reward: [-567.06 -567.06 -567.06] [47.8641], Avg: [-618.482 -618.482 -618.482] (1.000)
Step: 94799, Reward: [-503.686 -503.686 -503.686] [69.6985], Avg: [-618.458 -618.458 -618.458] (1.000)
Step: 94849, Reward: [-526.811 -526.811 -526.811] [102.1722], Avg: [-618.463 -618.463 -618.463] (1.000)
Step: 94899, Reward: [-550.965 -550.965 -550.965] [185.4710], Avg: [-618.525 -618.525 -618.525] (1.000)
Step: 94949, Reward: [-612.8 -612.8 -612.8] [174.5139], Avg: [-618.614 -618.614 -618.614] (1.000)
Step: 94999, Reward: [-436.959 -436.959 -436.959] [66.9267], Avg: [-618.554 -618.554 -618.554] (1.000)
Step: 95049, Reward: [-562.403 -562.403 -562.403] [91.2572], Avg: [-618.572 -618.572 -618.572] (1.000)
Step: 95099, Reward: [-580.11 -580.11 -580.11] [72.9529], Avg: [-618.591 -618.591 -618.591] (1.000)
Step: 95149, Reward: [-549.278 -549.278 -549.278] [92.6922], Avg: [-618.603 -618.603 -618.603] (1.000)
Step: 95199, Reward: [-576.525 -576.525 -576.525] [127.0628], Avg: [-618.647 -618.647 -618.647] (1.000)
Step: 95249, Reward: [-488.662 -488.662 -488.662] [101.0183], Avg: [-618.632 -618.632 -618.632] (1.000)
Step: 95299, Reward: [-595.271 -595.271 -595.271] [114.8397], Avg: [-618.68 -618.68 -618.68] (1.000)
Step: 95349, Reward: [-488.815 -488.815 -488.815] [104.4031], Avg: [-618.667 -618.667 -618.667] (1.000)
Step: 95399, Reward: [-519.882 -519.882 -519.882] [87.9054], Avg: [-618.661 -618.661 -618.661] (1.000)
Step: 95449, Reward: [-610.172 -610.172 -610.172] [164.6684], Avg: [-618.743 -618.743 -618.743] (1.000)
Step: 95499, Reward: [-519.5 -519.5 -519.5] [178.6245], Avg: [-618.785 -618.785 -618.785] (1.000)
Step: 95549, Reward: [-603.537 -603.537 -603.537] [91.8787], Avg: [-618.825 -618.825 -618.825] (1.000)
Step: 95599, Reward: [-575.139 -575.139 -575.139] [81.3250], Avg: [-618.844 -618.844 -618.844] (1.000)
Step: 95649, Reward: [-552.082 -552.082 -552.082] [140.2560], Avg: [-618.883 -618.883 -618.883] (1.000)
Step: 95699, Reward: [-566.738 -566.738 -566.738] [72.0290], Avg: [-618.893 -618.893 -618.893] (1.000)
Step: 95749, Reward: [-576.844 -576.844 -576.844] [148.8373], Avg: [-618.949 -618.949 -618.949] (1.000)
Step: 95799, Reward: [-483.062 -483.062 -483.062] [35.1859], Avg: [-618.896 -618.896 -618.896] (1.000)
Step: 95849, Reward: [-462.174 -462.174 -462.174] [55.2533], Avg: [-618.843 -618.843 -618.843] (1.000)
Step: 95899, Reward: [-452.367 -452.367 -452.367] [47.9325], Avg: [-618.782 -618.782 -618.782] (1.000)
Step: 95949, Reward: [-457.763 -457.763 -457.763] [33.8908], Avg: [-618.715 -618.715 -618.715] (1.000)
Step: 95999, Reward: [-500.556 -500.556 -500.556] [125.1169], Avg: [-618.719 -618.719 -618.719] (1.000)
Step: 96049, Reward: [-465.468 -465.468 -465.468] [128.2742], Avg: [-618.706 -618.706 -618.706] (1.000)
Step: 96099, Reward: [-442.078 -442.078 -442.078] [70.1087], Avg: [-618.651 -618.651 -618.651] (1.000)
Step: 96149, Reward: [-477.73 -477.73 -477.73] [59.1632], Avg: [-618.608 -618.608 -618.608] (1.000)
Step: 96199, Reward: [-434.575 -434.575 -434.575] [43.0333], Avg: [-618.535 -618.535 -618.535] (1.000)
Step: 96249, Reward: [-472.09 -472.09 -472.09] [129.3893], Avg: [-618.526 -618.526 -618.526] (1.000)
Step: 96299, Reward: [-464.613 -464.613 -464.613] [44.4569], Avg: [-618.469 -618.469 -618.469] (1.000)
Step: 96349, Reward: [-460.645 -460.645 -460.645] [65.0685], Avg: [-618.421 -618.421 -618.421] (1.000)
Step: 96399, Reward: [-485.726 -485.726 -485.726] [86.3655], Avg: [-618.397 -618.397 -618.397] (1.000)
Step: 96449, Reward: [-431.664 -431.664 -431.664] [88.6437], Avg: [-618.346 -618.346 -618.346] (1.000)
Step: 96499, Reward: [-553.385 -553.385 -553.385] [162.5999], Avg: [-618.397 -618.397 -618.397] (1.000)
Step: 96549, Reward: [-489.892 -489.892 -489.892] [117.1502], Avg: [-618.391 -618.391 -618.391] (1.000)
Step: 96599, Reward: [-404.735 -404.735 -404.735] [94.2370], Avg: [-618.329 -618.329 -618.329] (1.000)
Step: 96649, Reward: [-530.085 -530.085 -530.085] [81.4773], Avg: [-618.326 -618.326 -618.326] (1.000)
Step: 96699, Reward: [-450.644 -450.644 -450.644] [49.6026], Avg: [-618.264 -618.264 -618.264] (1.000)
Step: 96749, Reward: [-450.942 -450.942 -450.942] [106.5606], Avg: [-618.233 -618.233 -618.233] (1.000)
Step: 96799, Reward: [-437.694 -437.694 -437.694] [19.0227], Avg: [-618.15 -618.15 -618.15] (1.000)
Step: 96849, Reward: [-455.476 -455.476 -455.476] [57.1741], Avg: [-618.095 -618.095 -618.095] (1.000)
Step: 96899, Reward: [-453.359 -453.359 -453.359] [84.4369], Avg: [-618.054 -618.054 -618.054] (1.000)
Step: 96949, Reward: [-472.265 -472.265 -472.265] [66.0651], Avg: [-618.013 -618.013 -618.013] (1.000)
Step: 96999, Reward: [-412.277 -412.277 -412.277] [119.6090], Avg: [-617.968 -617.968 -617.968] (1.000)
Step: 97049, Reward: [-461.072 -461.072 -461.072] [57.8334], Avg: [-617.917 -617.917 -617.917] (1.000)
Step: 97099, Reward: [-451.775 -451.775 -451.775] [53.7662], Avg: [-617.859 -617.859 -617.859] (1.000)
Step: 97149, Reward: [-413.389 -413.389 -413.389] [40.9156], Avg: [-617.775 -617.775 -617.775] (1.000)
Step: 97199, Reward: [-458.727 -458.727 -458.727] [119.9937], Avg: [-617.755 -617.755 -617.755] (1.000)
Step: 97249, Reward: [-419.279 -419.279 -419.279] [38.1773], Avg: [-617.673 -617.673 -617.673] (1.000)
Step: 97299, Reward: [-439.173 -439.173 -439.173] [79.0895], Avg: [-617.622 -617.622 -617.622] (1.000)
Step: 97349, Reward: [-385.58 -385.58 -385.58] [98.6906], Avg: [-617.553 -617.553 -617.553] (1.000)
Step: 97399, Reward: [-516.4 -516.4 -516.4] [106.3661], Avg: [-617.556 -617.556 -617.556] (1.000)
Step: 97449, Reward: [-554.52 -554.52 -554.52] [37.0098], Avg: [-617.542 -617.542 -617.542] (1.000)
Step: 97499, Reward: [-438.032 -438.032 -438.032] [52.9871], Avg: [-617.477 -617.477 -617.477] (1.000)
Step: 97549, Reward: [-457.881 -457.881 -457.881] [57.9732], Avg: [-617.425 -617.425 -617.425] (1.000)
Step: 97599, Reward: [-479.531 -479.531 -479.531] [84.9248], Avg: [-617.398 -617.398 -617.398] (1.000)
Step: 97649, Reward: [-548.129 -548.129 -548.129] [130.7136], Avg: [-617.43 -617.43 -617.43] (1.000)
Step: 97699, Reward: [-476.107 -476.107 -476.107] [88.2629], Avg: [-617.403 -617.403 -617.403] (1.000)
Step: 97749, Reward: [-489.987 -489.987 -489.987] [89.3141], Avg: [-617.383 -617.383 -617.383] (1.000)
Step: 97799, Reward: [-477.33 -477.33 -477.33] [55.8125], Avg: [-617.34 -617.34 -617.34] (1.000)
Step: 97849, Reward: [-492.842 -492.842 -492.842] [85.2259], Avg: [-617.32 -617.32 -617.32] (1.000)
Step: 97899, Reward: [-566.796 -566.796 -566.796] [202.8115], Avg: [-617.398 -617.398 -617.398] (1.000)
Step: 97949, Reward: [-590.528 -590.528 -590.528] [186.6033], Avg: [-617.479 -617.479 -617.479] (1.000)
Step: 97999, Reward: [-478.944 -478.944 -478.944] [74.5121], Avg: [-617.447 -617.447 -617.447] (1.000)
Step: 98049, Reward: [-475.858 -475.858 -475.858] [61.9706], Avg: [-617.406 -617.406 -617.406] (1.000)
Step: 98099, Reward: [-422.124 -422.124 -422.124] [61.0332], Avg: [-617.338 -617.338 -617.338] (1.000)
Step: 98149, Reward: [-446.214 -446.214 -446.214] [52.7623], Avg: [-617.277 -617.277 -617.277] (1.000)
Step: 98199, Reward: [-374.713 -374.713 -374.713] [41.0358], Avg: [-617.175 -617.175 -617.175] (1.000)
Step: 98249, Reward: [-408.793 -408.793 -408.793] [70.1570], Avg: [-617.104 -617.104 -617.104] (1.000)
Step: 98299, Reward: [-466.739 -466.739 -466.739] [72.2648], Avg: [-617.065 -617.065 -617.065] (1.000)
Step: 98349, Reward: [-469.259 -469.259 -469.259] [101.5224], Avg: [-617.041 -617.041 -617.041] (1.000)
Step: 98399, Reward: [-472.734 -472.734 -472.734] [42.4979], Avg: [-616.989 -616.989 -616.989] (1.000)
Step: 98449, Reward: [-463.989 -463.989 -463.989] [51.6189], Avg: [-616.938 -616.938 -616.938] (1.000)
Step: 98499, Reward: [-552.254 -552.254 -552.254] [137.7544], Avg: [-616.975 -616.975 -616.975] (1.000)
Step: 98549, Reward: [-527.827 -527.827 -527.827] [82.9409], Avg: [-616.972 -616.972 -616.972] (1.000)
Step: 98599, Reward: [-524.595 -524.595 -524.595] [129.9620], Avg: [-616.991 -616.991 -616.991] (1.000)
Step: 98649, Reward: [-559.157 -559.157 -559.157] [154.7682], Avg: [-617.04 -617.04 -617.04] (1.000)
Step: 98699, Reward: [-509.547 -509.547 -509.547] [146.0519], Avg: [-617.06 -617.06 -617.06] (1.000)
Step: 98749, Reward: [-454.163 -454.163 -454.163] [53.7829], Avg: [-617.004 -617.004 -617.004] (1.000)
Step: 98799, Reward: [-541.929 -541.929 -541.929] [107.1488], Avg: [-617.021 -617.021 -617.021] (1.000)
Step: 98849, Reward: [-485.456 -485.456 -485.456] [23.7570], Avg: [-616.966 -616.966 -616.966] (1.000)
Step: 98899, Reward: [-596.109 -596.109 -596.109] [87.4563], Avg: [-617. -617. -617.] (1.000)
Step: 98949, Reward: [-511.6 -511.6 -511.6] [115.4950], Avg: [-617.005 -617.005 -617.005] (1.000)
Step: 98999, Reward: [-447.11 -447.11 -447.11] [85.9419], Avg: [-616.962 -616.962 -616.962] (1.000)
Step: 99049, Reward: [-452.4 -452.4 -452.4] [47.3990], Avg: [-616.903 -616.903 -616.903] (1.000)
Step: 99099, Reward: [-494.342 -494.342 -494.342] [28.7816], Avg: [-616.856 -616.856 -616.856] (1.000)
Step: 99149, Reward: [-543.691 -543.691 -543.691] [116.0622], Avg: [-616.878 -616.878 -616.878] (1.000)
Step: 99199, Reward: [-386.98 -386.98 -386.98] [37.4860], Avg: [-616.781 -616.781 -616.781] (1.000)
Step: 99249, Reward: [-451.425 -451.425 -451.425] [76.3228], Avg: [-616.736 -616.736 -616.736] (1.000)
Step: 99299, Reward: [-482.18 -482.18 -482.18] [48.4750], Avg: [-616.692 -616.692 -616.692] (1.000)
Step: 99349, Reward: [-492.48 -492.48 -492.48] [55.0210], Avg: [-616.658 -616.658 -616.658] (1.000)
Step: 99399, Reward: [-559.577 -559.577 -559.577] [159.4222], Avg: [-616.709 -616.709 -616.709] (1.000)
Step: 99449, Reward: [-599.033 -599.033 -599.033] [167.3932], Avg: [-616.784 -616.784 -616.784] (1.000)
Step: 99499, Reward: [-502.57 -502.57 -502.57] [97.9906], Avg: [-616.776 -616.776 -616.776] (1.000)
Step: 99549, Reward: [-492.154 -492.154 -492.154] [113.3594], Avg: [-616.77 -616.77 -616.77] (1.000)
Step: 99599, Reward: [-436.67 -436.67 -436.67] [43.0746], Avg: [-616.702 -616.702 -616.702] (1.000)
Step: 99649, Reward: [-450.183 -450.183 -450.183] [74.0908], Avg: [-616.655 -616.655 -616.655] (1.000)
Step: 99699, Reward: [-459.384 -459.384 -459.384] [32.7915], Avg: [-616.593 -616.593 -616.593] (1.000)
Step: 99749, Reward: [-554.915 -554.915 -554.915] [102.8606], Avg: [-616.614 -616.614 -616.614] (1.000)
Step: 99799, Reward: [-493.951 -493.951 -493.951] [63.2175], Avg: [-616.584 -616.584 -616.584] (1.000)
Step: 99849, Reward: [-530.78 -530.78 -530.78] [134.0374], Avg: [-616.608 -616.608 -616.608] (1.000)
Step: 99899, Reward: [-480.874 -480.874 -480.874] [62.1933], Avg: [-616.571 -616.571 -616.571] (1.000)
Step: 99949, Reward: [-518.16 -518.16 -518.16] [108.8965], Avg: [-616.576 -616.576 -616.576] (1.000)
Step: 99999, Reward: [-506.989 -506.989 -506.989] [102.2219], Avg: [-616.573 -616.573 -616.573] (1.000)
